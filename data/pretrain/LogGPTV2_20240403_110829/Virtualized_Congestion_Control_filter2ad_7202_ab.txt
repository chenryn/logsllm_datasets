TCP connection into several sub-connections, e.g., using a
TCP proxy [19,20]. It can acknowledge packets to the guest
VM at some desired rate, then send them on the datacen-
ter network using the desired target congestion control algo-
rithm.
Example. This black-box solution functions as a pipe, and
can implement nearly any congestion control algorithm. For
instance, to implement MPTCP, the hypervisor can quickly
prefetch packets from the guest VM at a high rate, then send
them to the destination hypervisor using several paths.
In addition to requiring many resources for buffer-
Cons.
ing packets, this solution goes against TCP end-to-end se-
mantics. For instance, a barrier-based application may be-
lieve that all its packets were ACKed, and advance to the
next phase, while they were not actually received, potentially
causing errors in the application.
Buffer packets. The hypervisor translation layer can buffer
in-ﬂight packets, e.g., to be able to resend them without in-
forming the guest [20, 30].
Example. In order to solve TCP incast, it can be useful to
reduce the retransmission timeout value RTOmin. vCC can
buffer in-ﬂight packets and retransmit according to its own
RTOmin buffer, even when the guest OS does not support
the desired value or does not support changing the RTOmin
at all.
Cons. The hypervisor needs to manage packet buffers. Gen-
erally, packet buffers may also increase latency when they
are used to store packets coming in too quickly (instead of
copies of sent packets). Large buffers signiﬁcantly increase
the memory footprint of vCC.
Buffer ACKs. The hypervisor can similarly buffer received
ACKs [23–25]. If an ACK is piggybacked on data, the ac-
knowledged sequence number is reduced and the remaining
bytes to acknowledge are later sent as a pure ACK.
Example. The hypervisor can pace ACKs to make TCP less
bursty.
Cons. The hypervisor needs to manage ACK buffers. It may
also increase latency when ACKs are delayed.
Duplicate ACKs. The hypervisor can duplicate and resend
the last sent ACK to force the guest to halve its congestion
window.
Example. In case of TCP incast, the hypervisor can force a
fast retransmit by sending three duplicate ACKs.
Cons. Beyond the need to keep the last ACK, this tech-
nique may also violate TCP semantics4. For instance, send-
ing three ACKs on the last outstanding packet means that
three additional packets have been received, which cannot
happen.
Throttle the receive window. The hypervisor can decrease
the receive window [22, 23, 25] to force the guest to have
fewer outstanding packets, since the number of packets in
ﬂight is upper-bounded by the minimum of the congestion
and the receive windows. Therefore, the advertised receive
window could follow the target congestion window to make
4Although it does not seem to directly go against RFC
5681 [31], which mentions the possibility of the replication
of ACK segments by the network.
the guest adapt to this target.
Example. The hypervisor can implement ECN or DCTCP.
Speciﬁcally, upon explicit congestion notiﬁcation, the hy-
pervisor translation layer decreases the receive window that
it sends to the guest, without forwarding the explicit conges-
tion notiﬁcation itself (see experiments in Section 3).
Cons. This technique can make the congestion window
meaningless, since it relies on the receive window to bound
the number of in-ﬂight packets. Also, a delicate point to note
is that the receive window should not be decreased to less
than the current number of in-ﬂight packets. This may con-
ﬂict with common implementations of the TCP buffer man-
agement. Therefore, the hypervisor needs to manage a grad-
ual decrease while closely monitoring the connection state.
Finally, a signiﬁcant shortcoming is that while the technique
helps make TCP less aggressive, it cannot make it more ag-
gressive. For that, we would need to rely on a heavier tech-
nique, such as a split connection.
Modify the three-way handshake. The hypervisor can
change the options that are negotiated when setting up the
connection.
Example. The hypervisor can modify the negotiated MSS, or
enable timestamps. This technique is also needed for several
of the above techniques, e.g., to enable ECN support (see
experiments in Section 3).
Cons. The technique can barely help for most practical ben-
eﬁts without additional techniques.
These techniques can translate the congestion control
most accurately when the hypervisor knows the speciﬁc OS
version and congestion control algorithm. In some cases, it
may be straightforward to detect these automatically either
by packet inspection, VM metadata, guest introspection, or
other communication with the guest. However, if the hy-
pervisor either does not know or does not want to trust the
information [32], it could simply limit the sender; e.g., when
applying the receive window throttling technique, it could
drop anything beyond the allowed receive window.
In addition, note that these techniques can be imple-
mented either on a single side of the ﬂow (i.e., receiver
or sender), yielding a virtual-to-native communication, or
on both sides, yielding a virtual-to-virtual communication.
When the guest already implements the target modern con-
gestion control algorithm, vCC can either tunnel its traf-
ﬁc transparently, or still translate the trafﬁc to make sure it
obeys the exact same protocol implementation as other trans-
lated vCC trafﬁc.
Figure 3 illustrates how a combination of the three-way
handshake modiﬁcation and the receive window throttling
techniques can help provide virtual-ECN beneﬁts to non-
ECN TCP trafﬁc (we later implement a proof-of-concept of
this solution in Section 3). The vCC translation layer in the
hypervisor ﬁrst uses the three-way handshake modiﬁcation
technique: in Figure 3(a), it modiﬁes the TCP header ﬁelds
of the sent packets to enable ECN support in the underlay.
Next, while vCC only sets the ECT bit in the IP header of
outgoing data packets and forwards incoming ACKs trans-
parently (Figure 3(b)), it uses the receive window throttling
technique upon congestion. As shown in Figure 3(c), upon
writing the proofs. First, the proofs strongly rely on the fact
that given the same sequence of inputs (e.g., ACKs), ECN
and DCTCP are surprisingly less aggressive than non-ECN
TCP, in the sense that their resulting congestion windows
will never be larger. For instance, if an explicit congestion
notiﬁcation arrives at an ECN or DCTCP sender, it may re-
duce its congestion window, while we assume that the noti-
ﬁcation should be ignored by a non-ECN TCP sender. The
second insight is that it is much easier to prove full emulation
when the timeouts are simultaneous in the state machines of
the guest and of the hypervisor translation layer. This is why
we assume negligible processing and communication times.
We believe that we could generalize these theorems to
more complex translations by concatenating simpler trans-
lations in the vCC translation layer: e.g., we could trans-
late TCP NewReno with ECN to DCTCP by concatenat-
ing (a) TCP NewReno with ECN to TCP NewReno with-
out ECN (simply modify the three-way handshake); and (b)
TCP NewReno without ECN to DCTCP (as shown above).
3. EVALUATION: SOLVING ECN UN-
FAIRNESS
In Sections 3 and 4, we show how a practical implementa-
tion of vCC can improve the performance and fairness of the
network. We implement vCC in two distinct environments.
The ﬁrst implementation is realized at the edge of the Linux
kernel TCP implementation. We demonstrate that vCC can
help address unfairness between ECN and non-ECN trafﬁc
in this Linux environment. All experiments in this Linux
vCC system are carried out with Mininet [34] for repro-
ducibility. Our experiments use a virtual machine running
Ubuntu 14.04 with Linux kernel version 3.19, except for
the experiments with 1 Gbps links, which were performed
using Mininet on a native Ubuntu 14.04 with Linux kernel
version 3.13. We set TSO (TCP Segmentation Ofﬂoading)
off in all Mininet experiments, because there is no real NIC
within Mininet to implement TSO. The CPU and memory
were never a bottleneck in all experiments.
The second environment is a proof-of-concept system in
the VMWare ESXi hypervisor’s vSwitch. We illustrate in
Section 4 how vCC can provide bandwidth sharing in this
hypervisor environment.
3.1 ECN Unfairness
ECN allows ﬂows to react to congestion before any data
has been lost [35]. ECN can be a valuable tool to increase
network performance, but it has not been widely supported
in operating systems until recently [36]. Thus, legacy guests
in a datacenter may not support ECN. Unfortunately, a lack
of ECN support can cause such legacy systems to suffer. Fig-
ure 1 shows that, even across many dozens of runs (140 in
this case), there is consistent starvation of non-ECN ﬂows.
We ﬁrst run an experiment to analyze the unfairness be-
tween ECN and non-ECN ﬂows, for various numbers of
ECN and non-ECN ﬂows. 10 senders are connected through
a switch to a single receiver. To demonstrate the ability
of vCC-augmented guests to interact with any guest in a
Figure 3: Interactions of the vCC translation layer in the
hypervisor with TCP trafﬁc. From top to bottom: (a)
Connection negotiation, where the translation layer en-
ables the ECE and CWR ﬂag in the SYN packet to indi-
cate ECN support, but hides the ECE ﬁeld in the return-
ing SYNACK; (b) normal data packets get their ECT bit
set in the IP header and ACKs pass through the transla-
tion layer unchanged. The translation layer updates its
internal state as data packets and ACKs pass through;
(c) when an ACK with ECE bit is received, the transla-
tion layer masks the ECE bit, modiﬁes the RWIN in the
TCP header, and sets CWR on the next outgoing packet.
receiving an ECN congestion notiﬁcation, it decreases the
advertised receive window to force the overlay TCP guest
sender to reduce its pace and behave as if it were ECN-
aware. It also modiﬁes the TCP header ﬁelds of the ACK
packets to mask congestion notiﬁcations in the overlay. Note
that we assume that the receiver either is ECN-enabled, or
also has a vCC translation layer.
In addition, in all these
cases, we need to recompute the checksum when the ﬁelds
change. We can do so by looking at the changed bytes only.
Formally, when using the three-way handshake and re-
ceive window techniques, we are able to prove that we can
exactly emulate ECN and DCTCP (where emulation is de-
ﬁned as in Equation (1) in the Introduction). We need two
major assumptions. First, we assume that all the processing
and communication times within the guest and hypervisor
are negligible. Second, we build a TCP NewReno state ma-
chine that is based on RFC 5681 [31] and RFC 6582 [33]
and assume that the guest follows this state machine. We do
so because our proof depends on this state machine, and we
found that different OSes and even different OS versions can
follow different state machines even for TCP NewReno. We
can then prove the following emulation theorems:
THEOREM 1. The translation layer can exactly emulate
an ECN-aware TCP NewReno protocol given a non-ECN
TCP NewReno guest.
THEOREM 2. The translation layer can exactly emulate
DCTCP given a non-ECN TCP NewReno guest.
The full formal proofs of these two theorems are available
online [21]. We gained two insights on full emulation when
SYNACKSYNACK+ECEUpdate Window& State MachineHypervisorSYNSYN+ECE+CWRACKACKDataData + ECTACK+ECEACK, RWINDataData+CWRGuestUpdate Window& State MachinevCCTranslation LayerNetwork(a)(b)(c)(a) 9 non-ECN vs. 1 ECN ﬂows
(b) 5 non-ECN vs. 5 ECN ﬂows
(c) 1 non-ECN vs. 9 ECN ﬂows
Figure 4: Unfairness between ECN and non-ECN ﬂows, given a constant total number of 10 ﬂows going through a
shared 100 Mbps bottleneck link. As the ratio of ECN to non-ECN ﬂows increases, the non-ECN ﬂows suffer from
increasing starvation and can send fewer and fewer packets.
(a) 9 non-ECN vs. 1 ECN ﬂows
(b) 5 non-ECN vs. 5 ECN ﬂows
(c) 1 non-ECN vs. 9 ECN ﬂows
Figure 5: Repeated unfairness test between ECN and non-ECN ﬂows with a 1 Gbps bottleneck link.
virtual-to-native communication, we set the receiver to be
a simple native Linux guest without vCC. As a result, it can
be seen as a non-ECN receiver for non-ECN ﬂows, and an
ECN receiver for ECN and virtual-ECN ﬂows. All links
have a bandwidth of 100 Mbps and a delay of 0.25 ms, so
the RTT is 1 ms. The switch queue uses RED with parameter
set RED1 as detailed in Table 1. We use TCP NewReno as
the congestion control algorithm in all our experiments. We
measure the goodput of long-lived TCP ﬂows, using iPerf
as the trafﬁc source and TShark for capturing packets and
measuring statistics. Each datapoint represents the average
goodput over a second for a single ﬂow.
Figure 4 demonstrates the unfairness between ECN and
non-ECN ﬂows by plotting the time-series of their goodput.
It shows that while the ECN ﬂows fairly share the bottleneck
link among themselves, the non-ECN ﬂows can become sig-
niﬁcantly starved. The unfairness grows as ECN becomes
more widespread and the ratio of ECN ﬂows to non-ECN
ﬂows increases. This unfairness points out a curse of legacy:
as applications increasingly adopt ECN, the holdout legacy
applications become increasingly starved. Limited unfair-
ness between ECN and non-ECN TCP ﬂows was known
given equal numbers of ﬂows in each group [37]. However,
the large impact of a plurality of newer ECN guests on a
Parameter
REDmin
REDmax
REDlimit
REDburst
REDprob
RED1
90000
90001
1M
61
1.0
Value
RED2
30000
90000
400K
55
0.02
RED3
30000
90000
400K
55
1.0
Table 1: RED Parameters used in the experiments.
few non-ECN legacy guests appears to be new. To address
this issue, it is possible to design alternative switch mark-
ing schemes that would favor legacy applications instead.
However, ensuring fairness with legacy applications appears
quite challenging.
We have also repeated this experiment with higher-rate
links to emulate a datacenter environment more closely.
Speciﬁcally, in this setting we use 1 Gbps links, a delay of
0.025 ms (i.e., RTT is 100 µs), an RTOmin of 20 ms (instead
of the default 200 ms) and RED parameter set RED1 from
Table 1. The results are presented in Figure 5. The same
trend is evident.
We next analyze the impact of different ratios of ECN to
(a) 10 Mbps links
(b) 100 Mbps links
Figure 6: Unfairness between ECN and non-ECN ﬂows for several ﬂow-type mixes and RED parameter sets, given a
constant number of 10 ﬂows. In all parameter sets, the unfairness becomes larger when there are fewer remaining
non-ECN legacy ﬂows.
(a) 10 Mbps links
(b) 100 Mbps links
Figure 7: Average goodput ratio with varying values of REDmin, given 10 senders. Increased numbers of ECN ﬂows
lead to starvation of non-ECN ﬂows.
non-ECN ﬂow numbers and of various RED parameter sets
(Table 1) on this ECN unfairness. The RED1 parameter set
emulates a hard threshold AQM, where packets are dropped
for non-ECN ﬂows once the queue occupancy exceeds a cer-
tain threshold (REDmin), in a similar way to the AQM de-
scribed for DCTCP [1]. The REDburst parameter is set to
the minimum allowed value in tc-red for RED1 parameters.
RED2 is the recommended setting for RED in the tc-red man
page example. RED3 is a modiﬁcation of RED2 (modiﬁed
REDprob) to test a more aggressive marking/dropping pol-
icy.
Figure 6 plots the ratio of the mean non-ECN ﬂow good-
put to the mean ECN ﬂow goodput, i.e., a measure of this