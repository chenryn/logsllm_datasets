title:Spotless Sandboxes: Evading Malware Analysis Systems Using Wear-and-Tear
Artifacts
author:Najmeh Miramirkhani and
Mahathi Priya Appini and
Nick Nikiforakis and
Michalis Polychronakis
2017 IEEE Symposium on Security and Privacy
Spotless Sandboxes: Evading Malware Analysis
Systems using Wear-and-Tear Artifacts
Najmeh Miramirkhani, Mahathi Priya Appini, Nick Nikiforakis, Michalis Polychronakis
{nmiramirkhani, mappini, nick, mikepo}@cs.stonybrook.edu
Stony Brook University
Abstract—Malware sandboxes, widely used by antivirus
companies, mobile application marketplaces, threat detection
appliances, and security researchers,
face the challenge of
environment-aware malware that alters its behavior once it de-
tects that it is being executed on an analysis environment. Recent
efforts attempt to deal with this problem mostly by ensuring that
well-known properties of analysis environments are replaced with
realistic values, and that any instrumentation artifacts remain
hidden. For sandboxes implemented using virtual machines, this
can be achieved by scrubbing vendor-speciﬁc drivers, processes,
BIOS versions, and other VM-revealing indicators, while more
sophisticated sandboxes move away from emulation-based and
virtualization-based systems towards bare-metal hosts.
We observe that as the ﬁdelity and transparency of dynamic
malware analysis systems improves, malware authors can resort
to other system characteristics that are indicative of artiﬁcial
environments. We present a novel class of sandbox evasion
techniques that exploit the “wear and tear” that inevitably occurs
on real systems as a result of normal use. By moving beyond how
realistic a system looks like, to how realistic its past use looks like,
malware can effectively evade even sandboxes that do not expose
any instrumentation indicators, including bare-metal systems. We
investigate the feasibility of this evasion strategy by conducting
a large-scale study of wear-and-tear artifacts collected from real
user devices and publicly available malware analysis services. The
results of our evaluation are alarming: using simple decision trees
derived from the analyzed data, malware can determine that a
system is an artiﬁcial environment and not a real user device
with an accuracy of 92.86%. As a step towards defending against
wear-and-tear malware evasion, we develop statistical models that
capture a system’s age and degree of use, which can be used to
aid sandbox operators in creating system images that exhibit a
realistic wear-and-tear state.
I. INTRODUCTION
As the number and sophistication of the malware samples
and malicious web pages that must be analyzed every day
constantly increases, defenders rely on automated analysis
systems for detection and forensic purposes. Malware scanning
based on static code analysis faces signiﬁcant challenges
due to the prevalent use of packing, polymorphism, and
other code obfuscation techniques. Consequently, malware
analysts largely rely on dynamic analysis approaches to scan
potentially malicious executables. Dynamic malicious code
analysis systems operate by loading each sample into a heavily
instrumented environment, known as a sandbox, and monitoring
its operations at varying levels of granularity (e.g., I/O activity,
system calls, machine instructions). Malware sandboxes are
typically built using API hooking mechanisms [1], CPU
emulators [2]–[5], virtual machines [6], [7], or even dedicated
bare-metal hosts [8]–[10].
Malware sandboxes are employed by a broad range of
systems and services. Besides their extensive use by antivirus
companies for analyzing malware samples gathered by deployed
antivirus software or by voluntary submissions from users,
sandboxes have also become critical in the mobile ecosystem,
for scrutinizing developers’ app submissions to online app
marketplaces [11], [12]. In addition, a wide variety of industry
security solutions, including intrusion detection systems, secure
gateways, and other perimeter security appliances, employ
sandboxes (also known as “detonation boxes”) for on-the-ﬂy
or off-line analysis of unknown binary downloads and email
attachments [13]–[17].
As dynamic malware analysis systems become more widely
used, online miscreants constantly seek new methods to hinder
analysis and evade detection by altering the behavior of
malicious code when it runs on a sandbox [8], [9], [11], [18]–
[20]. For example, malicious software can simply crash or
refrain from performing any malicious actions, or even exhibit
benign behavior by masquerading as a legitimate application.
Similarly, exploit kits hosted on malicious (or infected) web
pages have started employing evasion and cloaking techniques
to refrain from launching exploits against analysis systems [21],
[22]. To effectively alter its behavior, a crucial capability for
malicious code is to be able to recognize whether it is being
run on a sandbox environment or on a real user system. Such
“environment-aware” malware has been evolving for years,
using sophisticated techniques against the increasing ﬁdelity
of malware analysis systems.
In this paper, we argue that as the ﬁdelity and non-
intrusiveness of dynamic malware analysis systems continues
to improve, an anticipated next step for attackers is to rely
on other environmental features and characteristics to distin-
guish between end-user systems and analysis environments.
Speciﬁcally, we present an alternative, more potent approach to
current VM-detection and evasion techniques that is effective
even if we assume that no instrumentation or introspection
artifacts of an analysis system can be identiﬁed by malware
at run time. This can be achieved by focusing instead on the
“wear and tear” and “aging” that inevitably occurs on real
systems as a result of normal use. Instead of trying to identify
artifacts characteristic to analysis environments, malware can
determine the plausibility of its host being a real system used
by an actual person based on system usage indicators.
© 2017, Najmeh Miramirkhani. Under license to IEEE.
DOI 10.1109/SP.2017.42
1009
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
The key intuition behind the proposed strategy is that, in
contrast to real devices under active use, existing dynamic
analysis systems are typically implemented using operating
system images in almost pristine condition. Although some
further conﬁguration is often performed to make the system
look more realistic, after each binary analysis, the system is
rolled back to its initial state [23]. This means that any possible
wear-and-tear side-effects are discarded, and the system is
always brought back to a pristine condition. As an indicative
example, in a real system, properties like the browsing history,
the event log, and the DNS resolver cache, are expected to
be populated as a result of days’ or weeks’ worth of activity.
In contrast, due to the rollback nature of existing dynamic
analysis systems, malicious code can easily identify that the
above aspects of a system may seem unrealistic for a device
that supposedly has been in active use by an actual person.
Even though researchers have already discussed the pos-
sibility of using speciﬁc indicators, such as the absence of
“Recently Open Files” [24] and the lack of a sufﬁcient number
of processes [25] as ways of identifying sandbox environments,
we show that these individual techniques are part of a larger
problem, and systematically assess their threat to dynamic
analysis sandboxes. To assess the feasibility and effectiveness
of this evasion strategy, we have identiﬁed a multitude of wear-
and-tear artifacts that can potentially be used by malware as
indicators of the extent to which a system has been actually
used. We present a taxonomy of our ﬁndings, based on which
we then proceeded to implement a probe tool that relies on a
subset of those artifacts—ones that can be captured in a privacy-
preserving way—to determine a system’s level of wear and
tear. Using that tool, we gathered a large set of measurements
from 270 real user systems, and used it to determine the
discriminatory capacity of each artifact for evasion purposes.
We then proceed to test our hypothesis that wear and tear
can be a robust indicator for environment-aware malware, by
gauging the extent to which malware sandboxes are vulner-
able to evasion using this strategy. We submitted our probe
executable to publicly available malware scanning services,
receiving results from 16 vendors. Using half of the collected
sandbox data, we trained a decision tree model that picks a
few artifacts to reason whether a system is real or not, and
evaluated it using the other half. Our ﬁndings are alarming:
for all tested sandboxes, the employed model can determine
that the system is an artiﬁcial environment and not a real user
device with an accuracy of 92.86%.
We provide a detailed analysis of the robustness of this
evasion approach using different sets of artifacts, and show
that modifying existing sandboxes to withstand this type of
evasion is not as simple as artiﬁcially adjusting the probed
artifacts to plausible values. Besides the fact that malware can
easily switch to the use of another set of artifacts, we have
uncovered a deeper direct correlation of many of the evaluated
wear-and-tear artifacts and the reported age of the system,
which can allow malware to detect unrealistic conﬁgurations.
As a step towards defending against wear-and-tear malware
evasion, we have developed statistical models that capture the
age and degree of usage of a given system, which can be used
to ﬁne-tune sandboxes so that they look realistic.
In summary, our work makes the following main contribu-
tions:
• We show that certain, seemingly independent, techniques
for identifying sandbox environments are in fact all
instances of a broader evasion strategy based on a system’s
wear-and-tear characteristics.
• We present an indicative set of diverse wear-and-tear
artifacts, and evaluate their discriminatory capacity by
conducting a large-scale measurement study based on data
collected from real user systems and malware sandboxes.
• We demonstrate that using simple decision trees derived
from the analyzed data, malware can robustly evade all
tested sandboxes with an accuracy of 92.86%.
• We present statistical models that can predict the age of
a system based on its system usage indicators, which can
be used to “age” existing artiﬁcial-looking sandboxes to
the desired degree.
II. BACKGROUND AND RELATED WORK
A. Virtualization and Instrumentation Artifacts
Antivirus companies, search engines, mobile application
marketplaces, and the security research community in general
largely rely on dynamic code analysis systems that
load
potentially malicious content in a controlled environment for
analysis purposes [1]–[6], [8]–[10], [21], [26]–[34]. Given that
virtual machines and system emulators are very convenient
platforms for building malware sandboxes, evasive malware
often relies on various artifacts of such systems to alter its
behavior [8], [9], [11], [18]–[20], [35]. Probably the simplest
approach is to use static heuristics that check for certain system
properties, such as VM-speciﬁc device drivers and hardware
conﬁgurations, ﬁxed identiﬁers including MAC addresses,
IMEIs (for mobile malware analysis systems), user/host names,
VM-speciﬁc loaded modules and processes (e.g., VMware
Tools in case of VMware), and registry entries [20], [36].
Even when dynamic analysis systems are carefully conﬁg-
ured to avoid expected values and conﬁgurations, lower-level
properties of the execution environment can be used to identify
the presence of a virtual environment. These include emulator
intricacies that can be observed at runtime using small code
snippets, timing properties of certain virtualized instructions,
cache side effects, and many others [8], [20], [37]–[43].
B. Environmental and User Interaction Artifacts
The increasing sophistication of environment-aware malware
has prompted many research efforts that focus on either
detecting the “split personality” of malware by running each
sample on multiple and diverse analysis systems [18], [44],
[45], or increasing the ﬁdelity of the analysis environment by
avoiding the use of emulation or virtualization altogether, and
opting for “bare-metal” systems [8]–[10], [46].
These recent developments, however, along with the pro-
liferation of virtual environments in production deployments
and end-user systems, have resulted in yet another change of
1010
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
tactics from the side of malware authors. Given that a VM host
may be equally “valuable” as a non-VM host, the number of
malicious samples that rely on VM detection tricks for evasion
has started to decrease [47].
is, rather than whether it
At the same time, malware authors have started employing
other heuristics that focus mostly on how realistic the state
and conﬁguration of a host
is
a real or a virtual one. Such heuristics include checking
whether the mouse cursor remains motionless in the center
of the screen [48], observing the absence of “Recently Open
Files” [24] or an unusually low number of processes [25],
testing whether unrestricted internet connectivity is available,
and trying to resolve a known non-existent domain name. As
some malware analysis systems blindly serve all DNS requests,
a positive answer to a request for a non-existent domain is
an indication of a malware analysis environment and not a
real system [9]. Correspondingly, malware sandboxes have
started emulating user behavior (e.g., by generating realistic-
looking mouse movements and actions) and exposing a more
realistic network environment. Unfortunately, as we show in
this work, a neglected aspect of this effort towards making
malware sandboxes as realistic as possible, is the wear and
tear that is expected to occur as a result of normal use.
C. Sandbox Fingerprinting
A few research efforts have focused on deriving conﬁguration
proﬁles from sandboxes used by malware scanning services
and security appliances [23], [49]. By ﬁngerprinting the
sandboxes of different vendors, malware can then identify
the distinguishing characteristics of a given sandbox, and
alter its behavior accordingly. Maier et al. developed Sand-
Finger [49], a sandbox ﬁngerprinting tool for Android-based
analysis systems. The extracted ﬁngerprints capture properties
that are mostly speciﬁc to mobile devices, such as saved WiFi
networks, paired Bluetooth devices, or whether a device is
connected to a host via a cable. The authors show that the
derived ﬁngerprints can be used by malicious apps to evade
Google’s Bouncer and other mobile app sandboxes.
Recently, Blackthorne et al. presented AVLeak, a tool that
can ﬁngerprint emulators running inside commercial antivirus
(AV) software, which are used whenever AVs detect an
unknown executable [50]. The authors developed a method
that allows them to treat these emulators as black boxes and
use side channels as a means of extracting ﬁngerprints from
each AV engine.
Yokoyama et al. developed SandPrint [23], a sandbox
ﬁngerprinting tool tailored for Windows malware analysis
systems. The tool considers a set of 25 features that were
identiﬁed to exhibit characteristic, high-entropy values on
malware sandboxes used by different vendors. These include
hardware conﬁguration parameters, which tend to have unusu-
ally low values to facilitate the parallel operation of multiple
sandbox images (e.g., by lowering the amount of RAM or disk
space), or features speciﬁc to the malware sample invocation
mechanism employed by each sandbox (e.g., Cuckoo Sandbox’s
agent.py launch script, or the fact that the ﬁle name of the
analyzed executable is changed according to its MD5 sum or
using some other naming scheme). The authors then performed
automated clustering to classify ﬁngerprints collected after
the submission of the tool to 20 malware analysis services,
resulting in the observation of 76 unique sandboxes. They also
used some of the features to build a classiﬁer that malware
can use to reliably evade the tested sandboxes.
As the authors of SandPrint note, “most of the selected
features are deterministic and their values discrete and reliable,”
and “a stealthy sandbox could try to diversify the feature values.”
Inspired by this observation, our aim in this paper is to explore
what the next step of attackers might be, once sandbox operators
begin to diversify the values of the above features, and to
conﬁgure their systems using more realistic settings, rendering
sandbox-ﬁngerprinting ineffective. To that end, instead of
focusing on features characteristic to sandboxes, we take a
radically different view and focus on artifacts characteristic to
real, used machines. Previous works have shown that sandboxes
are conﬁgured differently than real systems, and can thus be
easily ﬁngerprinted. The programs used to analyze malware are
designed to be as transparent as possible, to prevent malware
from detecting that it is being analyzed. In this work, we
show that even with completely transparent analysis programs,
the environment outside the analysis program is enough for
malware to determine that it is under analysis.
III. WEAR AND TEAR ARTIFACTS
To determine the extent to which a system has been actually
used, and consequently, the plausibility of it being a real user
system and not a malware sandbox, malicious code can rely
on a broad range of artifacts that are indicative of the wear
and tear of the system. In this section, we discuss an indicative
set of the main types of such artifacts that we have identiﬁed,
which can be easily probed by malware.
Our strategy for selecting these artifacts was based on
identifying what aspects of a system are affected as a result of
normal use, and not on system features that may be affected
by certain sandbox-speciﬁc implementation intricacies—a
realistically conﬁgured sandbox may not exhibit any such
features at all. We located these artifacts by using a method
similar to the snowball method for conducting literature surveys.
Speciﬁcally, upon identifying a potentially promising artifact,
we would search in that same location of the system for
more artifacts. Besides intuition, to identify additional sources
of artifacts, we also studied various Windows “cleaner” and
“optimizer” programs, as well as Windows forensics literature.
Our list of artifacts is clearly non-comprehensive and we are
certain that there are many more good sources of artifacts that
were not included in this study. At the same time, although it is
expected that one can ﬁnd a multitude of wear-and-tear artifacts,
e.g., as a result of the customization, personalization, and use
of different applications on an actual user system, our selection
was constrained by an important additional requirement.
1011
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
A. Probing for Artifacts while Preserving User Privacy
We can distinguish between two main types of wear-and-tear
artifacts, depending on their source: artifacts that stem from
direct user actions, versus artifacts caused by indirect system
activity. For instance, a browser’s history contains the URLs
that a user has explicitly visited. Expecting to ﬁnd a popular
search engine, a social networking service, and a news website
among the recently visited URLs could constitute a heuristic
that captures a user’s speciﬁc interests. In contrast, the number
of application events in the system event log is an indirect
artifact which, although it depends on user activity, it does not
capture any private user information.
Direct artifacts are qualitatively stronger indicators of a
system’s degree of use by a person. Conceivably, malware
could rely on a broad and diverse range of such artifacts to
decide whether a system has been under active use, e.g., by
inspecting whether document folders contain plausible numbers
of ﬁles with expected ﬁle extensions, checking for recently
opened documents in popular document viewing applications,
or looking for the most recently typed online search engine
queries, system-wide (“spotlight-like”) search queries, instant
messaging or email message contents, and so on.
For the purposes of our study, however, such artifacts are
out of question, as collecting them from real user systems
would constitute a severe privacy violation. Consequently, we
limit our study mainly to indirect, system-wide artifacts, which
do not reveal any private user information. To get a glimpse
on the evasion potential of direct artifacts, we do consider
a limited set of browser-speciﬁc artifacts, which though are
strictly limited to aggregate counts (e.g., number of cookies,
number of downloaded ﬁles and, number of visited URLs)
that can be collected without compromising user privacy and
anonymity.
B. Artifact Categories
Many aspects of a system are susceptible to wear and
tear, from the operating system itself, to the various ﬁle,
network, and other subsystems. Considering, for instance,
a typical Microsoft Windows host, a multitude of different
wear-and-tear artifacts are available for estimating the extent
to which the system has been used. In the following, we
provide a brief description of the main types of artifacts that
we have identiﬁed. The complete set of artifacts that we used
in our experiments is shown in Table I.
1) System: Generic system properties such as the number
of running processes or installed updates are directly related to
the “history” of a system, as more installed software typically
accumulates throughout the years.
The event log of Windows systems is another rich source
of information about the current state and past activity of the
system. Various types of events are recorded into the event
log, including application, security, setup, and system events
of different administrative ratings (critical, error, warning,
information, audit success or failure). Of particular interest are
events that denote abnormal conditions, such as warnings about
missing ﬁles, inaccessible network links, unexpected service
terminations, and access permission errors. The more a system
has been used, the higher the number of records that will have
been accumulated in the event log. Besides the simple count of
system events, we also consider additional aspects indicative of
past use, such as the number of events from user applications,
the number of different event sources, and the time difference
between the ﬁrst and last event.
The event log will be (almost) empty only if the operating
system is freshly installed, or if the user deleted the recorded
entries. The vast majority of users, however, are not even
aware of the existence of this log, so it is unlikely that its
contents will be affected (e.g., deleted) by explicit user actions.
2) Disk: User activity results in ﬁle system changes,
exhibited in various forms,
including the generation of
temporary ﬁles, deleted ﬁles, actual user content, cached
data, and so on. Given the sensitive nature of accessing
user-generated content, even in terms of just counting ﬁles,
we limit the collection of disk artifacts directly related to user
activity only to the number of ﬁles on the desktop and in
the recycle bin. The rest of the disk artifacts capture counts
of ﬁles related to non-user-generated temporary or cached
data (e.g., generated ﬁle-content thumbnails, or process crash
“minidump” ﬁles).
3) Network: The network activity of a system, especially
from a historical perspective, is a strong indicator of actual
use. Every time a host is about to send a packet to a remote
destination, it consults the system’s address resolution protocol
(ARP) cache to ﬁnd the physical (MAC) address of the
gateway, or the address of another host in the local network.
Similarly, every time a domain name is resolved to an IP
address, the operating system’s DNS stub resolver includes
it in a cache of the most recent resolutions. The use of
public key encryption also results in artifacts related to the
certiﬁcates that have been encountered. For instance, once a
certiﬁcate revocation list (CRL) is downloaded, it is cached
locally. The list of the URLs of previously downloaded CRLs
is kept as part of the cached information, so the number of
entries in that list is directly related to past network activity
(not only due to browsing activity, but also due to applications
that perform automatic updates over the network). We also
consider the number of cached wireless network SSIDs, which
is a good indicator of past use for mobile devices, and the
number of active TCP connections.
4) Registry: The Windows registry contains a vast amount of
information about many aspects of a system, including many
that would ﬁt in the aforementioned categories. We chose,
however, to consider the Registry separately both because it
is privy to information that does not exist elsewhere, as well
as because, in Section V, we investigate whether sandbox-
detection is possible if an attacker has to constrain himself to
a single type of wear-and-tear artifacts.
1012
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
Category
System
Disk
Network
Registry
Browser
Name
totalProcesses
winupdt
sysevt
appevt
syssrc
appsrc
sysdiffdays
appdiffdays
recycleBinSize
recycleBinCount
tempFilesSize
tempFilesCount
miniDumpSize
miniDumpCount
thumbsFolderSize
desktopFileCount
ARPCacheEntries
dnscacheEntries
certUtilEntries
wirelessnetCount
tcpConnections
regSize
uninstallCount
autoRunCount
totalSharedDlls
totalAppPaths
totalActiveSetup
orphanedCount
totalMissingDlls
usrassistCount
shimCacheCount
MUICacheEntries
FireruleCount
deviceClsCount
USBStorCount
browserNum
uniqueURLs
totalTypedURLs
totalCookies
uniqueCookieDomains
totalBookmarks
totalDownloadedFiles
urlDiffDays
cookieDiffDays
TABLE I: Complete list of wear-and-tear artifacts.
Description
# of processes
# installed Windows updates
# system of system events
# of application events
# sources of system events
# sources of application events
Elapsed time since the ﬁrst system event (days)
Elapsed time since the ﬁrst application event (days)
Total size of the recycle bin (bytes)
# ﬁles in the recycle bin
Total size of temporary system ﬁles (bytes)
# temporary system ﬁles
Total size of process crash minidump ﬁles (bytes)
# of process crash minidump ﬁles
Total size of the system’s thumbnails folder (bytes)
# ﬁles on the desktop
# entries in the ARP cache
# entries in the DNS resolver cache
# URLs of previously downloaded CRLs
# of cached wireless SSIDs
# of active TCP connections
Size of the registry (in bytes)
# registered software uninstallers
# programs that automatically run at system startup
Legacy DLL reference count
# registered application paths
# Active Setup application entries
# leftover registry entries
# registered DLLs that do not exist on disk
# of entries in the UserAssist cache (frequently opened applications)
# entries in the Application Compatibility Infrastructure (Shim) cache
# Multi User Interface (MUI) cache entries
# of rules in the Windows Firewall
# previously connected USB devices (DeviceInstance IDs)
# previously connected USB storage devices
# installed browsers (Internet Explorer, Firefox, Chrome)
# unique visited URLs
# URLs typed in the browser’s navigation bar
# of HTTP cookies
# unique HTTP cookie domains
# bookmarks
# downloaded ﬁles
Time elapsed between the oldest and newest visited URL (days)
Time elapsed between the oldest and newest HTTP cookie (days)
Sandbox
35
19
8K
2.4K
49
26
1.7K
943
50M
3
24
60
409K
4
8M
6
4.5
4
210
0
27
User
94.4
794
27K
18K
78
40
370
298
2.5G
109
302
411
3M
9
63M
34
19
151
1.7K
8
77
144.8M 53M
177
9
242
54
24