anisms that provide taint information could be directly used
to selectively place delimiters at the server output.
We have experimented with PHPTaint [38], an imple-
mentation of taint-tracking in the PHP 5.2.5 engine, to au-
tomatically augment the minimal serialization primitives for
all tainted data seen in the output of the web server. We en-
able dynamic taint tracking of GET/POST request parame-
ters and database pulls. We disable taint declassiﬁcation of
data when sanitized by PHP sanitization functions (since we
wish to treat even sanitized data as potentially malicious).
All output tainted data are augmented with surrounding de-
limiters for minimal serialization. Our modiﬁcations shows
that automatic serialization is possible using off-the-shelf
tools.
For more complex web sites that use a multi-component
architecture, cross-component dynamic taint analysis may
be needed. This is an active area of research and auto-
matic support for minimal serialization at the server side
would readily beneﬁt from advances in this area. Recent
techniques proposed for program analysis to identify taint-
style vulnerabilities [22, 16] could help identify taint sink
points in larger web application, where manual identiﬁca-
tion is hard. Similarly, Nanda et al. have recently shown
cross-component dynamic taint tracking for the LAMP ar-
chitecture is possible [25].
Communicating valid sufﬁxes.
In our design it is sufﬁ-
cient to communicate {Cs, n} in a secure way, where Cs is
the random number generator seed to use and n is the num-
ber of invocations to generate the set C of valid delimiter
sufﬁxes. Our scheme communicates these as two special
HTML tag attributes, (seed and suffixsetlength),
as part of the HTML head tag of the web page. We assume
that the server and the browser use the same implementation
of the psuedo-random number generator. Once read by the
browser, it generates this set for the entire lifetime of the
page and does not recompute it even if the attacker corrupts
the value of the special attributes dynamically. We have ver-
iﬁed that this scheme is backwards compatible with HTML
handling in current browsers, i.e, these special attributes are
completely ignored for rendering in current browsers7.
Choice of serialization alphabet for encoding delimiters.
We discuss two schemes for encoding delimiters.
• We propose use of byte values from the Unicode Char-
acter Database [37] which are rendered as whitespace
on the major browsers independent of the selected
character set used for web page decoding. Our ratio-
nale for using whitespace characters is its uniformity
across all common character sets, and the fact that this
does not hinder parsing of HTML or script in most
relevant contexts (including between tags, between at-
tributes and values and strings). In certain exceptional
contexts where these may hinder semantics of parsing,
these errors would show up in pre-deployment testing
and can easily be ﬁxed. There are 20 such character
values which can be used to encode start and end de-
limiter symbols. All of the characters, as shown in ap-
pendix A, render as whitespace on cuurent browsers.
To encode the delimiters’ random sufﬁxes we could
use the remaining 18 (2 are used for delimiters them-
selves) as symbols. Thus, each symbol can encode 18
possible values, so a sufﬁx ℓ − symbols long, should
be sufﬁcient to yield an entropy of ℓ × (lg(18)) or
(ℓ × 4.16) bits.
It should be clear that a compliant browser can eas-
ily distinguish pages served from a non-compliant web
server to a randomization compliant web server—it
looks at the seed attribute in the  element
of the web page. When a compliant browser views a
non-compliant page, it simply treats the delimiter en-
coding bytes as whitespace as per current semantics,
as this is a non-compliant web page. When a compli-
ant browser renders a compliant web page, it treats any
found delimiter characters as valid iff they have valid
sufﬁxes, or else it discards the sequence of characters
7“current browsers” refers to: Safari, Firefox 2/3, Internet Explorer
6/7/8, Google Chrome, Opera 9.6 and Konqueror 3.5.9 in this paper.
as whitespace (these may occur by chance in the origi-
nal web page, or may be attacker’s spooﬁng attempts).
Having initialized the enclosed characters as untrusted
in its internal representation, it strips these whitespace
characters away. Thus, the scheme is secure whether
the page is DSI-compliant or not.
• Another approach is to use special delimiter tags,
, with an attribute check=suffix, as well.
Qtags have a lesser impact on readability of code than
the above scheme. Qtags have the same encoding
mechanism as  tags proposed informally [7].
We veriﬁed that it renders safely in today’s popular
browsers in most contexts, but is unsuitable to be used
in certain contexts such as within strings. Another is-
sue with this scheme is that XHTML does not allow
attributes in end tags, and so they don’t render well in
XHTML pages on non-compliant browsers, and may
be difﬁcult to accepted as a standard.
Policy Speciﬁcation. Our policies conﬁne untrusted data
only. Currently, we support per-page policies that are en-
forced for the entire web page, rather than varying region-
based policies. By default, we enforce the terminal con-
ﬁnement policy which is a default fail-close policy. In most
cases, this policy is sufﬁcient for several web sites to de-
fend against reﬂected XSS attacks. A more ﬂexible policy
that is useful is to allow certain HTML syntactic constructs
in inline untrusted data, such as restricted set of HTML
markup in user blog posts. We support a whitelist of syn-
tactic HTML elements as part of a conﬁgurable policy.
We allow conﬁgurable speciﬁcation of whitelisted
HTML construct names through a allowuser tag at-
tribute for HTML  tag which can have a comma-
separated list of allowed tags. For instance, the following
speciﬁcation would allow untrusted nodes corresponding to
the paragraph, boldface, line break elements, the attribute
id (in all elements) and the anchor element with optional
href attribute (only with anchor element) in parse tree to
not be ﬂagged as an exploit. The following markup renders
properly in non-compliant browsers since unknown markup
is discarded in the popular browsers.
For security, untrusted data is disallowed to deﬁne
allowuser tag without exception. Policy development
and standardization of default policies are important prob-
lems which involve a detail study of common elements that
are safe to allow on most web sites. However, we consider
this beyond the scope of this paper, but deem worthy of fu-
ture work.
6 Implementation
We discuss details of our prototype implementation of
a PLI enabled web browser and a PLI enabled web server
ﬁrst. Next, we demonstrate an example forum application
that was deployed on this framework requiring no changes
to application code. Finally, we outline the implementation
of a web proxy server used for evaluation in section 7.
DSI compliant browser. We have implemented a proof-
of-concept PLI enabled web browser by modifying Kon-
queror 3.5.9. Before each HTML parsing operation, the
HTML parsing engine identiﬁes special delimiter tags. This
step is performed before any character decoding is per-
formed, and our choice of unicode alphabet for delimiters
ensures that we deal with all character set encodings. The
modiﬁed browser simulates a pushdown automaton during
parsing to keep track of delimiter symbols for matching.
Delimited characters are initialized as quarantined, which
is represented by enhancing the type declaration for the
character class in Konqueror with a quarantine bit. Parse
tree nodes that are derived from quarantined characters are
marked quarantined as well. Before any quarantined inter-
nal node is updated to the document’s parse tree, the parser
invokes the policy checker which ensures that the parse tree
update is permitted by the policy. Any internal nodes that
are not permitted by the policy are collapsed with their sub-
tree to be treated as a leaf node and rendered as a string
literal.
We modiﬁed the JavaScript interpreter in Konqueror
3.5.9 to facilitate automatic quarantine bit tracking and pre-
vented tainted access through the JavaScript-DOM inter-
face. The modiﬁcations required were a substantial imple-
mentation effort compared to the HTML parser modiﬁca-
tions. Internal object representations were enhanced to store
the quarantine bits and handlers for each JavaScript opera-
tion had to be altered to propagate the quarantine bits. The
implemented policy checks ensure that quarantined data is
only interpreted as a terminal in the JavaScript language.
DSI compliant server. We employed PHPTaint [38]
which is an existing implementation dynamic taint track-
ing in the PHP interpreter. It enables taint variables in PHP
and can be conﬁgured to indicate which sources of data are
marked tainted in the server. We made minor modiﬁcations
to PHPTaint to integrate in our framework. By default when
untrusted data is processed by a built-in sanitization rou-
tine, PHPTaint endorses the data as safe and declassiﬁes(or
clears) the taint; we changed this behavior to not declassify
taint in such situations even though the data is sanitized.
Whenever data is echoed to the output we interpose in PH-
PTaint and surround tainted data with special delimiter tags
with randomized values at runtime. For serialization, we
used the unicode characters U+2029 as a start-delimiter.
Immediately following the start-delimiter are ℓ randomly
chosen unicode whitespace characters, the key, from the re-
maining 18 unicode characters. We have chosen ℓ = 10,
though this is easily conﬁgurable in our implementation.
Following the key is the end-delimiter U+2028 to signify
the key has been fully read.
Example application. Figure 9(a) shows a vulnerable
web forum application, phpBB version 2.0.18, running on
a vanilla Apache 1.3.41 web server with PHP 5.2.5 when
viewed with a vanilla Konqueror 3.5.9 with no DSI enforce-
ment. The attacker posts a post containing a script tag which
results in a cookie alert. To prevent such attacks, we de-
ployed the phpBB forum application on our DSI-compliant
web server next. We required no changes to the web ap-
plication code to deploy it on our prototype DSI-compliant
web server. Figure 9(b) shows how the attack is nulliﬁed
by our client-server DSI enforcement prototype which em-
ploys PHPTaint to automatically mark forum data (derived
from the database) as tainted, enhances it with minimal se-
rialization which enables a DSI-compliant version of Kon-
queror 3.5.9 to nullify the attack.
Client-side Proxy Server. For evaluation of the 5,328
real-world web sites, we could not use our prototype taint-
enabled PHP based server because we do not have ac-
cess to server code of the vulnerable web sites. To over-
come this practical limitation, we implemented a client-side
proxy server that approximately mimics the server-side op-
erations.
When the browser visits a vulnerable web site, the proxy
web server records all GET/POST data sent by the browser,
and maintains state about the HTTP request parameters
sent. The proxy essentially performs content based taint-
ing across data sent to the real server and the received re-
sponse, to approximate what the server would do in the full
deployment of the client-server architecture.
The web server proxy performs a lexical string match
between the sent parameter data and the data it receives
in the HTTP response. For all data in the HTTP response
that matches, the proxy performs minimal serialization (ap-
proximating the operations of a DSI-compliant server) i.e, it
lexically adds randomized delimiters to demarcate matched
data in the response page as untrusted, before forwarding it
to the PLI enabled browser.
7 Evaluation
To evaluate the effectiveness and overhead of PLI and
PLI enabled browsers we conducted experiments with two
conﬁgurations. The ﬁrst conﬁguration consists of running
Figure 9: (a) A sample web forum application running on a vulnerable version of phpBB 2.0.18, victimized by stored XSS
attack as it shows with vanilla Konqueror browser (b) Attack neutralized by our proof-of-concept prototype client-server DSI
enforcement.
our prototype PLI enabled browser and a server running
PHPTaint with the phpBB application. This conﬁguration
was used to evaluate effectiveness against stored XSS at-
tacks. The second conﬁguration ran our PLI enabled web
browser directing all HTTP requests to the proxy web server
described in section 7. The second conﬁguration was used
to study real-world reﬂected attacks, since we did not have
access to the vulnerable web server code.
7.1 Experimental Setup
Our experiments were performed on two systems—one
ran a Mac OS X 10.4.11 on a 2.0 GHz Intel processor with
2GB of memory, and the other runs Gentoo GNU/Linux
2.6.17.6 on a 3.4 GHz Intel Xeon processor with 2 GB
of memory. The ﬁrst machine ran an Apache 1.3.41 web
server with PHP 5.2.5 engine and MySQL back-end, while
the second ran the DSI compliant Konqueror. The two ma-
chines were connected by a 100 Mbps switch. We conﬁg-
ured our prototype PLI enabled browser and server to apply
the default policy of terminal conﬁnement to all web re-
quests unless the server overrides with another whitelisting
based policy.
7.2 Experimental Results and Analysis
7.2.1 Attack Detection
Attack Category
# Attacks
# Prevented
Reﬂected XSS
Stored XSS
5,328
5,243 (98.4%)
25
25 (100%)
Figure 10: Effectiveness of DSI enforcement against both
reﬂected XSS attacks [43] as well as stored XSS attack vec-
tors [12].
category, there were 5,328 web sites which constituted our
ﬁnal test dataset. Our DSI-enforcement using the proxy
web server and DSI compliant browser nulliﬁed 98.4% of
these attacks as shown in Figure 10. Upon further analy-
sis of the false negatives in this experiment, we discovered
that 46 of the remaining cases were missed because the real
web server modiﬁed the attack input before embedding it
on the web page—our web server proxy failed to recognize
this server-side modiﬁcation as it performs a simple string
matching between data sent by the browser and the received
HTTP response. We believe that in full-deployment these
would be captured with server explicitly demarcating un-
trusted data. We could not determine the cause of missing
the remaining 39, as the sent input was not discernible in
the HTTP response web page. We showed that the policy
of terminal conﬁnement, if supported in web servers as the
default, is sufﬁcient to prevent a large majority of reﬂected
XSS attacks.
Reﬂected XSS. We evaluated the effectiveness against all
real-world web sites with known vulnerabilities, archived
at the XSSed [43] web site as of 25th July 2008, which re-
sulted in successful attacks using Konqueror 3.5.9. In this
Stored XSS. We setup a vulnerable version of phpBB
web blog application (version 2.0.18) on our DSI enabled
web server, and injected 30 benign text and HTML based
posts, and all of the stored attack vectors taken from XSS
performance [1]. It is conﬁgured to generate dynamic fo-
rum web pages of sizes varying from 10 KB to 40 KB. In
our experiment, 64,000 requests were issued to the server
with 16 concurrent requests. As shown in Figure 12, we
observed average CPU overheads of 1.2%, 2.9% and 3.1%
for pages of 10 KB, 20 KB, and 40 KB in size respectively.
This is consistent with the performance overheads reported
by the authors of PHPTaint [38]. Figure 11 shows a com-
parison between the vanilla web server and a DSI-compliant
web server (both running phpBB) in terms of the percentage
of HTTP requests completed within a certain response time
frame. For 10 concurrent requests, the two servers perform
nearly very similar, wheres for 30 concurrent requests the
server with PHPTaint shows some degradation for complet-
ing more than 95% of the requests.
7.2.3 False Positives
We observed a fewer false positives rate in our stored XSS
attacks experiment than in the reﬂected XSS experiment.
In the stored experiment, we did not observe any false
positives.
In the reﬂected XSS experiment, we observed
false positives when we deliberately provided inputs that
matched existing page content. For the latter experiment,
we manually browsed the Global Top 500 websites listed
on Alexa [2] browsing with deliberate intent to raise false
positives. For each website, we visited an average of 3
second-level pages by creating accounts, logging in with
malicious inputs, performing searches for dangerous key-