ceedingly difﬁcult for humans to solve. Therefore, to
better measure the tension between usability and secu-
rity, we set the parameters for the videos (in §6) to values
where our attacks have a 5% success rate, despite that be-
ing intolerably high for practical security. Any captcha
at this parametrization, which is found to be unusable, is
thus entirely unviable.
6 User study
We now report on an IRB-approved user study with 25
participants that we conducted to assess the usability of
the aforementioned countermeasures. If the challenges
produced by the countermeasures prove too difﬁcult for
both computers and humans to solve, then they are not
viable as captcha challenges. We chose a controlled
lab study because besides collecting quantitative perfor-
mance data, it gave us the opportunity to collect partici-
pants’ impromptu reactions and comments, and allowed
us to interview participants about their experience. This
type of information is invaluable in learning why cer-
tain mitigation strategies are unacceptable or difﬁcult for
users and learning which strategies are deemed most ac-
ceptable. Additionally, while web-based or Mechanical
Turk studies may have allowed us to collect data from
more participants, such approaches lack the richness of
data available when the experimenter has the opportunity
to interact with the participants one-on-one. Mechani-
cal Turk studies have previously been used in captcha
research [5] when the goal of the studies are entirely
performance-based. However, since we are studying new
mitigation strategies, we felt that it was important to
gather both qualitative and quantitative data for a more
holistic perspective.
6.1 Methodology
We compared the defenses in §5.2 to a Standard ap-
proach which mimics NuCaptcha’s design.
In these
captchas the video contains scrolling text with 2-3 words
in white font, followed by 3 random red characters that
move along the same trajectory as the white words. Simi-
lar to NuCaptcha, the red characters (i.e., the codewords)
also independently rotate as they move. For the Extended
strategy, we set m = 23. All 23 characters are continu-
ously visible on the screen. During pilot testing, we also
tried a scrolling 23-character variation of the Extended
scheme. However, this proved extremely difﬁcult for
users to solve and they voiced strong dislike (and out-
rage) for the variation. For the Overlapping strategy, we
set the ratio to be 0.49. Recall that at this ratio, the mid-
dle character is overlapped 100% of the time, and the
others are 51% overlapped. For the Semi-Transparent
strategy, we set the ratio to be 80% background and 20%
foreground. For all experiments, we use the same alpha-
bet (of 20 characters) in NuCaptcha’s original videos.
A challenge refers to a single captcha puzzle to be
solved by the user. Each challenge was displayed on a
6-second video clip that used a canvas of size 300× 126
and looped continuously. This is the same speciﬁcation
used in NuCaptcha’s videos. Three different HD video
backgrounds (of a forest, a beach, and a sky) were used.
Some examples are shown in Figure 16. Sixty chal-
lenges were generated for each variation (20 for each
background, as applicable).
We also tested the Emerging strategy. The three-
character codeword was represented by black and white
pixel-based noise as described in §5.2. Sixty challenges
were generated using the same video parameters as the
other conditions.
The twenty-ﬁve participants were undergraduate,
graduate students, staff and faculty (15 males, 10 fe-
males, mean age 26) from a variety of disciplines. A
within-subjects experimental design was used, where
each participant had a chance to complete a set of 10
captchas for each strategy. The order of presentation for
the variations was counterbalanced according to a 5× 5
Latin Square to eliminate biases from learning effects;
Latin Squares are preferred over random ordering of con-
ditions because randomization could lead to a situation
where one condition is favored (e.g., appearing in the
last position more frequently than other conditions, giv-
ing participants more chance to practice). Within each
variation, challenges were randomly selected.
A simple web-based user interface was designed
where users could enter their response in the textbox and
press submit, could request a new challenge, or could
access the help ﬁle. Indication of correctness was pro-
vided when users submitted their responses, and users
were randomly shown the next challenge in the set. Im-
mediately after completing the 10 challenges for a vari-
ation, users were asked to complete a paper-based ques-
tionnaire collecting their perception and opinion of that
variation. At the end of the session, a brief interview was
conducted to gather any overall comments. Each partici-
pant completed their session one-on-one with the exper-
imenter. A session lasted at most 45 minutes and users
were compensated $15 for their time.
(a) Forest background
(b) Beach background
(c) Sky background
Figure 16: Three backgrounds used for the challenges, shown for the Semi-Transparent variant.
6.2 Data Collection
The user interface was instrumented to log each user’s
interactions with the system. For each challenge, the
user’s textual response, the timing information, and the
outcome was recorded. A challenge could result in three
possible outcomes: success, error, or skipped. Question-
naire and interview data was also collected.
6.3 Analysis
Our analysis focused on the effects of ﬁve different
captcha variants on outcomes and solving times. We also
analyzed and reviewed questionnaire data representing
participant perceptions of the ﬁve variants. We used sev-
eral statistical tests and the within-subjects design of our
study impacted our choice of statistical tests; in each case
the chosen test accounted for the fact that we had multi-
ple data points from each participant. In all of our tests,
we chose p < 0.05 as the threshold for determining sta-
tistical signiﬁcance.
One-way repeated-measures ANOVAs [25] were used
to evaluate aggregate differences between the means for
success rates and times. When the ANOVA revealed
a signiﬁcant difference, we used post-hoc Tukey HSD
tests [27] to determine between which pairs the differ-
ences occurred. Here, we were interested only in whether
the four proposed mitigation strategies differed from the
Standard variant, so we report only on these four cases.
Our questionnaires used Likert-scale responses to as-
sess agreement with particular statements (1 - Strongly
Disagree, 10 - Strongly Agree). To compare this ordinal
data, we used the non-parametric Friedman’s Test [27].
When overall signiﬁcant differences were found, we
used post-hoc Pairwise Wilcoxon tests with Bonferroni
correction to see which of the four proposed variants dif-
fered from the Standard variant.
Outcomes: Participants were presented with 10 chal-
lenges of each variant. Figure 17 shows a stacked bar
graph representing the mean number of success, error,
and skipped outcomes. To be identiﬁed as a Success,
the user’s response had to be entirely correct. An Er-
ror occurred when the user’s response did not match the
challenge’s solution. A Skipped outcome occurred when
the participant pressed the “Get A New Challenge” but-
ton and was presented with a different challenge. We
observe differences in the outcomes, with the Standard
variant being most successful and the Semi-Transparent
variant resulting in the most skipped outcomes.
Figure 17: Mean number of success, error, and skipped out-
comes for Standard, Extended, Overlapping, Semi-Transparent
and Emerging variants, respectively.
For the purposes of our statistical tests, errors and
skipped outcomes were grouped since in both cases the
user was unable to solve the challenge. Each participant
was given a score comprising the number of successful
outcomes for each variant (out of 10 challenges).4
A one-way repeated-measure ANOVA showed signif-
icant differences between the ﬁve variants (F(4,120) =
29.12, p < 0.001). We used post-hoc Tukey HSD tests
to see whether any of the differences occurred between
the Standard variant and any of the other four variants.
The tests showed a statistically signiﬁcant difference be-
tween all pairs except for the Standard⇔Emerging pair.
This means that the Extended, Overlapping, and Semi-
Transparent variants had a signiﬁcantly lower number
of successes than the Standard variant, while Emerging
variant showed no difference.
Time to Solve: The time to solve was measured as the
time between when the challenge was displayed to when
the response was received. This included the time to type
the answer (correctly or incorrectly), as well as the time it
took the system to receive the reply (since the challenges
were served from our local server, transmission time was
negligible). Times for skipped challenges were not in-
cluded since users made “skip” decisions very quickly
and this may unfairly skew the results towards shorter
mean times. We include challenges that resulted in er-
rors because in these cases participants actively tried to
the other half involved either missing letters or including
extra ones. For the other variants, nearly all errors were
due to confusing pairs of characters.
Figure 18: Time taken to solve the MBOR captchas.
solve the challenge. The time distributions are depicted
in Figure 18 using boxplots. Notice that the Extended
variant took considerably longer to solve than the others.
We examined the differences in mean times using
a one-way repeated-measure ANOVA. The ANOVA
showed overall signiﬁcant differences between the ﬁve
variants (F(4,120) = 112.95, p < 0.001). Once again,
we compared the Standard variant
to the others in
our post-hoc tests. Tukey HSD tests showed no sig-
niﬁcant differences between the Standard⇔Emerging
or Standard⇔Overlapping pairs. However, signiﬁ-
cant differences were found for the Standard⇔Semi-
Transparent and Standard⇔Extended pairs. This means
that the Semi-Transparent and Extended variants took
signiﬁcantly longer to solve than the Standard variant,
but the others showed no differences.
Skipped outcomes: The choice of background ap-
pears to have especially impacted the usability of the
Semi-Transparent variant. Participants most frequently
skipped challenges for the Semi-Transparent variant and
found the Forest background especially difﬁcult to use.
Many users would immediately skip any challenge that
appeared with the Forest background because the trans-
parent letters were simply too difﬁcult to see. For the
Semi-Transparent variant, 35% of challenges presented
on the Forest background were skipped, compared 17-
18% of challenges using the other two backgrounds. Par-
ticipants’ verbal and written comments conﬁrm that they
found the Forest background very difﬁcult, with some
users mentioning that they could not even ﬁnd the letters
as they scrolled over some parts of the image.
Errors: Figure 19 shows the distribution of errors.
It shows that the majority of errors were made on the
middle characters of the challenge. We also examined
the types of errors, and found that most were mistakes
between characters that have similar appearances. The
most commonly confused pairs were: S/5, P/R, E/F, V/N,
C/G, and 7/T. About half of the errors for the Extended
variant were due to confusing pairs of characters, while
Figure 19: Location of errors within the codewords.
User perception: Immediately after completing the
set of challenges for each variant, participants completed
a Likert-scale questionnaire to collect their opinion and
perception of that variant. For each variant, participants
were asked to rate their agreement with the following
statements:
1. It was easy to accurately solve the challenge
2. The challenges were easy to understand
3. This captcha mechanism was pleasant to use
4. This captcha mechanism is more prone to mistakes
than traditional text-based captchas
Figure 20 shows boxplots representing users’ re-
sponses. Since Q.4 was negatively worded, responses
were inverted for easier comparisons. In all cases, higher
values on the y-axis indicate a more favorable response.
The results show that users clearly preferred the Stan-
dard variant and rated the others considerably lower
on all subjective measures. Friedman’s Tests showed
overall signiﬁcant differences for each question (p <
0.001). Pairwise Wilcoxon Tests with Bonferroni correc-
tion were used to assess differences between the Stan-
dard variant and each of the other variants. Signiﬁcant
differences were found between each pair compared.
The only exceptions are that users felt that the Extended
and Emerging variants were no more difﬁcult to under-
stand (Question 2) than the Standard variant. This result
appears to contradict the results observed in Figure 20
and we believe that this is because the Wilcoxon test
compares ranks rather than means or medians.
Comments: Participants had the opportunity to pro-
vide free-form comments about each variant and offer
verbal comments to the experimenter. Samples are in-
cluded in Appendix B. Participants clearly preferred the
Standard variant, and most disliked the Extended variant.
(a) Accuracy
(b) Easy to understand
(c) Pleasant to use
(d) More error-prone (responses
inverted)
Figure 20: Likert-scale responses: 1 is most negative, 10 is most positive.
Of the remaining schemes, the Emerging variant seemed
most acceptable although it also had its share of negative
reactions (e.g., one subject found it to be hideous).
7 Summary and Concluding Remarks
Our attack inherently leverages the temporal informa-
tion in the moving-image object recognition (MIOR)
captchas, and also exploits the fact that only object
recognition of known objects is needed. Our methods
also rely on a reasonably consistent appearance or slowly
varying appearance over time. That said, they can be
applied to any set of known objects or narrowly de-
ﬁned objects under afﬁne transformations that are known
to work well with detection methods in computer vi-
sion [45]. For the speciﬁc case of NuCaptcha, we showed
that not only are there inherent weaknesses in the current
MIOR captcha design, but that several obvious counter-
measures (e.g., extending the length of the codeword)
are not viable attack countermeasures. More importantly,
our work highlights the fact that the choice of underlying
hard problem by NuCaptcha’s designers was misguided;
its particular implementation falls into a solvable sub-
class of computer vision problems.
In the case of emergent captchas, our attacks fail
for two main reasons. First, in each frame there are
not enough visual cues that help distinguish the charac-
ters from the background. Second, the codewords have
no temporally consistent appearance. Combined, these