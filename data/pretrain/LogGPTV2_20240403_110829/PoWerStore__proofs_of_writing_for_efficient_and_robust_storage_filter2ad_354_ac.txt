68: upon receiving filter ack(cid:104)tsr, ts, f r, cc(cid:105) from server si
69:
70:
R ← R ∪ {i}; W [i] ← (ts, f r, cc)
C ← C \ {c ∈ C : invalid(c)}
cc ← cc(cid:48) s.t. ∃R(cid:48) ⊆ R : |R(cid:48)| ≥ t + 1 (cid:86)
71: procedure restore(ts)
72:
(∀i ∈ R(cid:48) : W [i].ts = ts ∧ W [i].cc = cc(cid:48))
F ← {W [i].f r : i∈R ∧ W [i].ts=ts ∧ H(W [i].f r)=cc[i]}
V ← decode(F, t + 1, S)
return V
76: Predicates:
77:
safe(c) (cid:44) ∃R(cid:48) ⊆ R : |R(cid:48)| ≥ t + 1 (cid:86)
(∀i ∈ R(cid:48) : W [i].ts = c.ts) (cid:86)
73:
74:
75:
78:
79:
(∀i, j ∈ R(cid:48):W [i].cc=W [j].cc ∧ H(W [i].f r)=W [j].cc[i])
highcand(c) (cid:44) (c.ts = max({c(cid:48).ts : c(cid:48) ∈ C}))
invalid(c) (cid:44) |{i ∈ R : W [i].ts  c.ts
becomes invalid(c(cid:48)) and is excluded from C. As such,
c is also highcand(c) and rd does not block.
• Case 2: Since no candidate in C is valid, all correct
servers (at least S − t) responded with timestamp ts0,
which is lower than any candidate timestamp. As such,
every candidate c ∈ C becomes invalid(c) is excluded
from C. Therefore, rd does not block.
5. M-PoWerStore
In what follows, we present the multi-writer variant of
our protocol, dubbed M-PoWerStore. M-PoWerStore re-
sists attacks speciﬁc to multi-writer setting that exhaust
the timestamp domain [6]. Besides its support for multiple
writers, M-PoWerStore protects against denial of service at-
tacks speciﬁc to PoWerStore, in which the adversary swamps
the system with bogus candidates. While this attack can
be contained in PoWerStore by a robust implementation of
the point-to-point channel assumption using, e.g., a separate
pair of network cards for each channel (in the vein of [13]),
this may impact practicality.
5.1 Overview
M-PoWerStore supports an unbounded number of clients.
In addition, M-PoWerStore features optimal read latency of
two rounds in the common case, where no process is Byzan-
tine. Under malicious attacks, M-PoWerStore gracefully de-
grades to guarantee reading in at most three rounds. The
write has a latency of three rounds, featuring non-skipping
timestamps [6], which prevents the adversary from exhaust-
ing the timestamp domain.
The main diﬀerence between M-PoWerStore and PoWer-
Store is that, here, servers store and transmit a single can-
didate instead of a (possibly unbounded) set. To this end,
it is crucial that servers are able to determine the validity
of a written-back candidate without consulting the history.
For this purpose, we enhance our original PoW scheme by
extending the candidate with message authentication codes
(MACs) on the timestamp and the nonce’s hash, one for
each server, using the corresponding group key. As such, a
valid MAC proves to a server that the written-back candi-
date stems from a writer, and thus, constitutes a PoW that
a server can obtain even without the corresponding history
entry. Note that in case of a candidate incorporating cor-
rupted MACs, servers might disagree about the validity of
a written-back candidate. Hence, a correct client might not
be able to write-back a candidate to t + 1 correct servers
as needed. To solve this issue, M-PoWerStore ”pre-writes”
the MACs, enabling clients to recover corrupted candidates
from the pre-written MACs.
To support multiple-writers, write in M-PoWerStore com-
prises an additional distributed synchronization round, called
clock, which is prepended to store. The read performs
an additional round called repair, which is appended to
collect. The purpose of repair is to recover broken can-
didates prior to writing them back, and is invoked only un-
der attack by a malicious adversary that actually corrupts
candidates.
Similarly to PoWerStore, the server maintains the variable
Hist to store the history of the data written by the writer
in the store round, indexed by timestamp. In addition, the
server keeps the variable lc to store the metadata of the last
completed write consisting of the timestamp, the nonce and
a vector of MACs (with one entry per server) authenticating
the timestamp and the nonce’s hash.
The full write implementation is given in Algorithm 4.
The implementation of the server and the read operation
are given in Algorithm 5 and 6. In the following, we simply
highlight the diﬀerences to PoWerStore.
5.2 Write Implementation
As outlined before, M-PoWerStore is resilient to the ad-
versary skipping timestamps. This is achieved by having
the writer authenticate the timestamp of a write with a
key kW shared among the writers. Note that such a shared
key can be obtained by combining the diﬀerent group keys;
for instance, kW ← H(k1||k2|| . . . ).
To obtain a timestamp, in the clock procedure, the writer
retrieves the timestamp (held in variable lc) from a quorum
of S − t servers and picks the highest timestamp ts with a
valid MAC. Then, the writer increases ts and computes a
MAC for ts using kW . Finally, clock returns ts.
To write a value V , the writer, (i) obtains a timestamp ts
from the clock procedure, (ii) authenticates ts and the
nonce’s hash N by a vector of MACs vec, with one en-
try for each server si using group key ki, and (iii) stores
vec both in store and complete. Upon reception of a
store(cid:104)f ri, cc, N , vec(cid:105) message, the server writes the tuple
(f ri, cc, N , vec) into the history entry Hist[ts]. Upon recep-
tion of a complete(cid:104)ts, N, vec(cid:105) message, the server changes
the value of lc to (ts, N, vec) if ts > lc.ts.
5.3 Read Implementation
The read consists of three consecutive rounds, collect,
filter and repair. In collect, a client reads the candi-
date triple (ts, N, vec) stored in variable lc in the server, and
inserts it into the candidate set C together with the candi-
dates read from other servers. After the client receives S − t
candidates from diﬀerent servers, collect returns.
In filter, the client submits C to each server. Upon re-
ception of C, the server chooses chv as the candidate in C
with the highest timestamp such that valid(chv) is satisﬁed,
or c0 if no such candidate exists, and performs a write-back
by setting lc to chv if chv.ts > lc.ts. Roughly speaking,
the predicate valid(c) holds if the server veriﬁes the integrity
of the timestamp c.ts and nonce c.N either by the MAC,
or by the corresponding history entry. The server then re-
sponds to the client with the timestamp chv.ts, the fragment
Hist[chv.ts].f r, the cross-checksum Hist[chv.ts].cc and the
vector of MACs Hist[chv.ts].vec.
The client awaits responses from S − t servers and waits
until there is a candidate c with the highest timestamp in
C such that safe(c) holds, or until C is empty, after which
filter returns. The predicate safe(c) holds if at least t +
1 diﬀerent servers si have responded with timestamp c.ts,
Algorithm 4 Algorithm of writer w in M-PoWerStore.
80: Deﬁnitions:
81:
82:
Q: set of pid, (process id) initially ∅
ts: structure (num, pid, MAC{kW }(num||pid)),
initially ts0 (cid:44) (0, 0, null)
83: operation write(V )
84:
85:
86:
87:
88:
89:
90:
91:
Q ← ∅
ts ← clock()
N ← {0, 1}λ
N ← H(N )
vec ← [MAC{ki}(ts||N )1≤i≤S ]
store(ts, V, N , vec)
complete(ts, N, vec)
return ok
92: procedure clock()
send clock(cid:104)ts(cid:105) to all servers
93:
94: wait until |Q| ≥ S − t
ts.num ← ts.num + 1
95:
ts ← (ts.num, w, MAC{kW }(ts.num||w))
96:
97:
98: upon receiving clock ack(cid:104)ts, tsi(cid:105) from server si
99:
100:
Q ← Q ∪ {i}
if tsi > ts ∧ verify(tsi, kW ) then ts ← tsi
return ts
{f r1, . . . , f rS} ← encode(V, t + 1, S)
cc ← [H(f r1), . . . , H(f rS )]
foreach server si send store(cid:104)ts, f ri, cc, N , vec(cid:105) to si
101: procedure store(ts, V, N , vec)
102:
103:
104:
105: wait for store ack(cid:104)ts(cid:105) from S − t servers
106: procedure complete(ts, N, vec)
107:
108: wait for complete ack(cid:104)ts(cid:105) from S − t servers
send complete(cid:104)ts, N, vec(cid:105) to all servers
fragment f ri, cross-checksum cc such that H(f ri) = cc[i],
and vector vec. If C is empty, the client sets V to the initial
value ⊥. Otherwise, the client selects the highest candidate
c ∈ C and restores value V by decoding V from the t + 1
correct fragments received for c.
In repair, the client veriﬁes the integrity of c.vec by
matching it against the vector vec received from t + 1 dif-
ferent servers. If c.vec and vec match then repair returns.
Otherwise, the client repairs c by setting c.vec to vec and
invokes a round of write-back by sending a repair(cid:104)tsr, c(cid:105)
message to all servers. Upon reception of such a message,
if valid(c) holds then the server sets lc to c provided that
c.ts > lc.ts and responds with an repair ack message to
the client. Once the client receives acknowledgements from
S−t diﬀerent servers, repair returns. After repair returns,
the read returns V .
5.4 Analysis
We argue that M-PoWerStore satisﬁes linearizability by
showing that if a completed read rd by a correct client re-
turns V then a subsequent read rd(cid:48) by a correct client does
not return a value older than V . The residual correctness
arguments are similar to those of PoWerStore (Section 4.4),
and therefore omitted.
Suppose rd(cid:48) follows after rd. If c is the candidate selected
in rd, we argue that rd(cid:48) does not select a candidate with
a lower timestamp. By assumption c is selected in rd by
a correct client that checks the integrity of c.vec (during
repair). If c.vec passes the check, then each of the correct
servers (at least S−2t ≥ t+1) that received c during filter
Algorithm 5 Algorithm of server si in M-PoWerStore.
Algorithm 6 Algorithm of client r in M-PoWerStore.
109: Deﬁnitions:
lc: structure (ts, N, vec), initially c0 (cid:44) (ts0, null, null)
110:
111: Hist[. . . ]: vector of (f r, cc, N , vec) indexed by ts, with all
entries initialized to (null, null, null, null)
send store ack(cid:104)ts(cid:105) to writer w
if ts > lc.ts then lc ← (ts, N, vec)
send complete ack(cid:104)ts(cid:105) to writer w
109: upon receiving clock(cid:104)ts(cid:105) from writer w
send clock ack(cid:104)ts, lc.ts(cid:105) to writer w
110:
111: upon receiving store(cid:104)ts, f r, cc, N , vec(cid:105) from writer w
112: Hist[ts] ← (f r, cc, N , vec)
113:
114: upon receiving complete(cid:104)ts, N, vec(cid:105) from writer w
115:
116:
117: upon receiving collect(cid:104)tsr(cid:105) from client r
118:
119: upon receiving filter(cid:104)tsr, C(cid:105) from client r
120:
121:
122:
123:
124: upon receiving repair(cid:104)tsr, c(cid:105) from client r
125:
126:
chv ← max({c ∈ C : valid(c)} ∪ {c0})
if chv.ts > lc.ts then lc ← chv
(f r, cc, vec) ← πf r,cc,vec(Hist[chv.ts])
send filter ack(cid:104)tsr, chv.ts, f r, cc, vec(cid:105) to client r
if c.ts > lc.ts ∧ valid(c) then lc ← c
send repair ack(cid:104)tsr(cid:105) to client r
send collect ack(cid:104)tsr, lc(cid:105) to client r
//write-back
//write-back
127: Predicates:
128: valid(c) (cid:44) (H(c.N ) = Hist[c.ts].N ) ∨
verify(c.vec[i], c.ts, H(c.N ), ki)
validates c (by verifying its own entry of c.vec) and sets lc to
c unless it has already changed lc to a higher timestamped
candidate. Otherwise, if c.vec fails the integrity check, then
the client in rd repairs c.vec and subsequently writes-back
c to t + 1 correct servers or more. Hence, by the time rd
completes, at least t + 1 correct servers have set lc to c or
to a valid candidate with a higher timestamp. Since during
collect in rd(cid:48) the client receives the value of lc from S − t
diﬀerent servers, a valid candidate c(cid:48) such that c(cid:48).ts ≥ c.ts
is included in C. By Lemma 4.3 (no exclusion), c(cid:48) is never
excluded from C and by Algorithm 6, rd(cid:48) does not select a
candidate with timestamp lower than c(cid:48).ts ≥ c.ts.
6.
IMPLEMENTATION & EVALUATION
In this section, we describe an implementation model-
ing a Key-Value Store (KVS) based on M-PoWerStore. To
model a KVS, we use multiple instances of M-PoWerStore
referenced by keys. We then evaluate the performance of
our implementation and we compare it to: (i) M-ABD, the
multi-writer variant of the crash-only atomic storage proto-
col of [5], and (ii) Phalanx, the multi-writer robust atomic
protocol of [34] that relies on digital signatures.
6.1 Implementation Setup
Our KVS implementation is based on the JAVA-based
framework Neko [2] that provides support for inter-process
communication, and on the Jerasure [1] library for construct-
ing the dispersal codes. To evaluate the performance of
our M-PoWerStore we additionally implemented two KVSs
based on M-ABD and Phalanx.
In our implementation, we relied on 160-bit SHA1 for
hashing purposes, 160-bit keyed HMACs to implement MACs,
tsr: num, initially 0
Q, R: set of pid, initially ∅
C: set of (ts, N, vec), initially ∅
C, Q, R ← ∅
tsr ← tsr + 1
C ← collect(tsr)
C ← filter(tsr, C)
if C (cid:54)= ∅ then