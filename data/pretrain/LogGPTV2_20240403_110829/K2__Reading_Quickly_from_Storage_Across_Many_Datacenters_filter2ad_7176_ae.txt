exchanging logs of serialized updates and is not scalable, i.e., it
is designed for at most one shard in what are now datacenters.
Karma [41] is concurrent work on enabling a causally-
consistent data store to support partial replication that uses
an approach similar to the replicas across datacenters baseline
we compare to in our evaluation. It focuses on allowing clients
to switch the datacenter to which they are connected and
enabling simple reads from a cache in a datacenter; it does
not support write-only transactions or read-only transactions.
K2, in contrast, does not focus on allowing clients to switch
datacenters though it could be extended to do so (§VI-B). K2
focuses on providing write-only and read-only transactions.
PaRiS’s use of per-client private caches and K2’s per-
datacenter shared caches are quite different. PaRiS’s per-client
cache is necessary for correctness. They cannot be shared be-
tween clients because they contain newer-than-UST state and
thus it would be unsafe for one client to read data from another
client’s private cache. K2’s per-datacenter shared caches, in
contrast, are safely shared between all clients in a datacenter
and enable K2 to often handle read-only transactions locally.
Improving Partial Replication. Volley [1], Tuba [8], and
Akkio [7] deal with data placement and migration for partially
replicated systems. They optimize data placement policies and
dynamically migrate data to different replicas based on system
logs to satisfy user requirements and reduce operational costs.
This line of work is orthogonal to K2, which operates indepen-
Spanner [20] is Google’s globally-distributed data store,
which provides strict serializability and partial replication.
K2 targets a much lower latency setting than Spanner with
guarantees that are compatible with handling all reads inside
the local datacenter as well as trading away some capacity for
signiﬁcant read latency improvements from caching.
PaRiS [51] is a concurrently developed causally-consistent
data store that supports partial replication. PaRiS uses per-
client private caches and a universal stable time (UST) to
provide causal read-write transactions, which are stronger than
K2’s guarantees. K2’s guarantees are, however, still useful for
a large set of applications (§II-A). PaRiS provides at most
one round of non-blocking cross-datacenter requests like K2.
PaRiS handles read-only transactions locally only when all
requested keys are either replicated in this datacenter or are
stored in the client’s private cache because the client has writ-
ten to them since the UST. As our experimental comparison
with PaRiS(cid:63) shows, this occurs rarely. In contrast, K2 is able
to often handle read-only transactions locally. Similarly, PaRiS
requires write transactions to contact remote datacenters except
when all keys are replicated in this datacenter. In contrast, K2’s
write-only transactions always commit to the local cache.
dently of any placement policy. Integrating such policies into
K2 could further reduce latency by increasing the likelihood
that a read’s local datacenter is a replica datacenter.
Saturn [14] and C3 [24] focus on improving the throughput
and data visibility latency of a partially-replicated data store
through novel metadata propagation and causal-consistency
enforcing algorithms. Saturn and C3, however, only support
simple read and write operations. K2, in contrast, focuses on
achieving lower latency for partial replication with stronger
guarantees: read-only and write-only transactions.
Partial replication has also been studied in ﬁle systems [29],
[42], [49]. This work focused on detecting and repairing
conﬂicting updates [29], [42] or enabling good performance
by aggressively creating new replicas. K2 instead focuses on
higher-layer concerns like consistency and transactions.
Cache-Aware Read-Only Transactions. TxCache [47] uses
a set of caches to increase the throughput of an underlying
monolithic database while providing serializability within an
application speciﬁed staleness bound. It uses a cache-aware
read-only transaction algorithm that starts with a set of pinned
snapshots identiﬁers from the underlying database and reﬁnes
the set of acceptable identiﬁers as a transaction proceeds.
TxCache’s algorithm inﬂuenced our design, but it cannot be
applied to our setting because we do not have a monolithic
database that can pin snapshots of all the data. Instead, K2
determines if the local datacenter has cached values that can
be used in a consistent snapshot dynamically.
Causal Consistency. Causal consistency is provided by many
systems [2], [3], [12], [13], [24], [32], [33], [38], [39],
[43], [46], [51], [52]. Excluding PRACTI [12], C3 [24] and
PaRiS [51], all these systems are built atop fully replicated
data stores and inevitably suffer from its limitations. We based
our design and implementation on fully-replicated Eiger [39].
Since Eiger, there have been many innovations [4], [14], [22]–
[24] in reducing the granularity of metadata for tracking causal
consistency and thus decreasing the throughput overhead of
enforcing causal consistency in datacenters. These innovations
are orthogonal to our contributions here; we believe it would
be straightforward (though time-consuming) to incorporate
these designs into K2 to achieve higher throughput.
IX. CONCLUSION
Deploying web services across many datacenters has the
potential to signiﬁcantly reduce end-user latency. Realizing
this lower latency, however, is complicated by the need to
partially replicate data in the backend storage system. K2 is a
partially-replicated storage system that unlocks low latency
for the strong guarantees of causal consistency, read-only
transactions, and write-only transactions.
ACKNOWLEDGMENT
We thank our shepherd, Jos´e Orlando Pereira, and the
anonymous reviewers for their helpful comments. We are
grateful to Theano Stavrinos, Christopher Hodsdon, Jeffrey
Helt, and Matthew Burke for their feedback. This work was
supported by the National Science Foundation under grant
number CNS-1827977.
REFERENCES
[1] Sharad Agarwal, John Dunagan, Navendu Jain, Stefan Saroiu, Alec
Wolman, and Harbinder Bhogan. Volley: Automated data placement
for geo-distributed cloud services. In NSDI, 2010.
[2] Mustaque Ahamad, Gil Neiger, Prince Kohli, James Burns, and Phil
Hutto. Causal memory: Deﬁnitions, implementation, and programming.
Distributed Computing, 9(1), 1995.
[3] Deepthi D. Akkoorath, Alejandro Z. Tomsic, Manuel Bravo, Zhongmiao
Li, Tyler Crain, Annette Bieniusa, Nuno Preguia, and Marc Shapiro.
Cure: Strong semantics meets high availability and low latency.
In
ICDCS, 2016.
[4] Sergio Almeida, Joao Leitao, and Luis Rodrigues. ChainReaction: A
In EuroSys,
causal+ consistent datastore based on chain replication.
2013.
[5] Virgilio Almeida, Azer Bestavros, Mark Crovella, and Adriana
De Oliveira. Characterizing reference locality in the WWW. In PDIS,
1996.
[6] https://aws.amazon.com/about-aws/global-infrastructure/, 2021.
[7] Muthukaruppan Annamalai, Kaushik Ravichandran, Harish Srinivas,
Igor Zinkovsky, Luning Pan, Tony Savor, David Nagle, and Michael
Stumm. Sharding the shards: Managing datastore locality at scale with
Akkio. In OSDI, 2018.
[8] Masoud Saeida Ardekani and Douglas B. Terry. A self-conﬁgurable
geo-replicated cloud storage system. In OSDI, 2014.
[9] Martin F Arlitt and Carey L Williamson. Web server workload charac-
terization: The search for invariants. ACM SIGMETRICS Performance
Evaluation Review, 24(1), 1996.
[10] Hagit Attiya and Jennifer L. Welch. Sequential consistency versus
linearizability. ACM TOCS, 12(2), 1994.
[11] https://www.cloudping.co/, 2021.
[12] Nalini Belaramani, Mike Dahlin, Lei Gao, Amol Nayate, Arun
Venkataramani, Praveen Yalagandula, and Jiandan Zheng. PRACTI
replication. In NSDI, 2006.
[13] Kenneth P. Birman and Robbert V. Renesse. Reliable Distributed
Computing with the ISIS Toolkit. IEEE Comp. Soc. Press, 1994.
[14] Manuel Bravo, Luis Rodrigues, and Peter Van Roy. Saturn: A distributed
metadata service for causal consistency. In EuroSys, 2017.
[15] Eric Brewer. Towards robust distributed systems. PODC Keynote, July
2000.
[16] Nathan Bronson, Zach Amsden, George Cabrera, Prasad Chakka, Peter
Dimov, Hui Ding, Jack Ferris, Anthony Giardullo, Sachin Kulkarni,
Harry Li, Mark Marchukov, Dmitri Petrov, Lovro Puzar, Yee Jiun Song,
and Venkat Venkataramani. TAO: Facebook’s distributed data store for
the social graph. In ATC, 2013.
[17] Pei Cao and Sandy Irani. Cost-aware WWW proxy caching algorithms.
In USITS, 1997.
[18] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Debo-
rah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, and
Robert E. Gruber. Bigtable: A distributed storage system for structured
data. ACM TOCS, 26(2), 2008.
[19] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan,
and Russell Sears. Benchmarking cloud serving systems with YCSB.
In SOCC, 2010.
[20] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christo-
pher Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher
Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene
Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura,
David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi Saito,
Michal Szymaniak, Christopher Taylor, Ruth Wang, and Dale Woodford.
Spanner: Google’s globally-distributed database. In OSDI, 2012.
[21] https://engineering.fb.com/data-center-engineering/data-centers-2018/,
2021.
[22] Jiaqing Du, Sameh Elnikety, Amitabha Roy, and Willy Zwaenepoel.
Orbe: Scalable causal consistency using dependency matrices and phys-
ical clocks. In SOCC, 2013.
[23] Jiaqing Du, C˘alin Iorgulescu, Amitabha Roy, and Willy Zwaenepoel.
Gentlerain: Cheap and scalable causal consistency with physical clocks.
In SOCC, 2014.
[24] P. Fouto, J. Leito, and N. Preguia. Practical and fast causal consistent
partial geo-replication. In NCA, 2018.
[25] Seth Gilbert and Nancy Lynch. Brewer’s conjecture and the feasibility
of consistent, available, partition-tolerant web services. ACM SIGACT
News, 33(2), 2002.
[26] https://cloud.google.com/about/locations/, 2021.
[27] https://www.google.com/about/datacenters/inside/locations/, 2021.
[28] Steven D Gribble and Eric A Brewer. System design issues for internet
In USITS,
middleware services: Deductions from a large client trace.
1997.
[29] Richard Guy, John S. Heidemann, Wai Mak, Gerald J. Popek, and Dieter
In
Implementation of the Ficus replicated ﬁle system.
Rothmeier.
Summer USENIX Conference, 1990.
[30] Maurice P. Herlihy and Jeannette M. Wing. Linearizability: A correct-
ness condition for concurrent objects. ACM TOPLAS, 1990.
[31] Qi Huang, Ken Birman, Robbert van Renesse, Wyatt Lloyd, Sanjeev
Kumar, and Harry C. Li. An analysis of Facebook photo caching. In
SOSP, 2013.
[32] Diptanshu Kakwani and Rupesh Nasre. Orion: Time estimated causally
consistent key-value store. In PaPoC, 2020.
[33] Rivka Ladin, Barbara Liskov, Liuba Shrira, and Sanjay Ghemawat.
Providing high availability using lazy replication. ACM TOCS, 10(4),
1992.
[34] Avinash Lakshman and Prashant Malik. Cassandra – a decentralized
structured storage system. In LADIS, 2009.
[35] Leslie Lamport. Time, clocks, and the ordering of events in a distributed
system. Comm. ACM, 21(7), 1978.
[36] Leslie Lamport. The part-time parliament. ACM TOCS, 16(2), 1998.
[37] Richard J. Lipton and Jonathan S. Sandberg. PRAM: A scalable shared
memory. Technical Report TR-180-88, Princeton Univ., Dept. Comp.
Sci., 1988.
[38] Wyatt Lloyd, Michael J. Freedman, Michael Kaminsky, and David G.
Andersen. Don’t settle for eventual: Scalable causal consistency for
wide-area storage with COPS. In SOSP, 2011.
[39] Wyatt Lloyd, Michael J. Freedman, Michael Kaminsky, and David G.
Andersen. Stronger semantics for low-latency geo-replicated storage. In
NSDI, 2013.
[40] Haonan Lu, Christopher Hodsdon, Khiem Ngo, Shuai Mu, and Wyatt
Lloyd. The SNOW theorem and latency-optimal read-only transactions.
In OSDI, 2016.
[41] Tariq Mahmood, Shankaranarayanan Puzhavakath Narayanan, Sanjay
Rao, T. N. Vijaykumar, and Mithuna Thottethodi. Karma: Cost-
effective geo-replicated cloud storage with dynamic enforcement of
causal consistency. IEEE TCC, 2018.
[42] Dahlia Malkhi and Doug Terry. Concise version vectors in WinFS. In
DISC, 2005.
[43] Syed Akbar Mehdi, Cody Littley, Natacha Crooks, Lorenzo Alvisi,
Nathan Bronson, and Wyatt Lloyd.
I can’t believe it’s not causal!
Scalable causal consistency with no slowdown cascades. In NSDI, 2017.
[44] https://azure.microsoft.com/en-us/global-infrastructure/regions/, 2021.
[45] Ruoming Pang, Ram´on C´aceres, Mike Burrows, Zhifeng Chen, Pratik
Dave, Nathan Germer, Alexander Golynski, Kevin Graney, Nina Kang,
Lea Kissner, and et al. Zanzibar: Google’s consistent, global authoriza-
tion system. In ATC, 2019.
[46] Karin Petersen, Mike J. Spreitzer, Douglas B. Terry, Marvin M. Theimer,
and Alan J. Demers. Flexible update propagation for weakly consistent
replication. In SOSP, 1997.
[47] Dan R.K. Ports, Austin T. Clements, Irene Zhang, Samuel Madden, and
Barbara Liskov. Transactional consistency and automatic management
in an application data cache. In OSDI, 2010.
[48] Luigi Rizzo and Lorenzo Vicisano. Replacement policies for a proxy
cache. IEEE/ACM Transactions on Networking, 8(2), 2000.
[49] Yasushi Saito, Christos Karamanolis, Magnus Karlsson, and Mallik
Mahalingam. Taming aggressive replication in the Pangaea wide-area
ﬁle system. SIGOPS Oper. Syst. Rev., 36(SI), 2002.
[50] Dale Skeen and Michael Stonebraker. A formal model of crash recovery
in a distributed system. IEEE Trans. Info. Theory, 9(3), 1983.
[51] K. Spirovska, D. Didona, and W. Zwaenepoel. Paris: Causally consistent
transactions with non-blocking reads and partial replication. In ICDCS,
2019.
[52] Kristina Spirovska, Diego Didona, and Willy Zwaenepoel. Wren:
Nonblocking reads in a partitioned transactional causally consistent data
store. In DSN, 2018.
[53] Linpeng Tang, Qi Huang, Amit Puntambekar, Ymir Vigfusson, Wyatt
Lloyd, and Kai Li. Popularity prediction of Facebook videos for higher
quality streaming. In ATC, 2017.
[54] Robert H. Thomas. A majority consensus approach to concurrency
control for multiple copy databases. ACM Trans. Database Sys., 4(2),
1979.
[55] Robbert van Renesse and Fred B. Schneider. Chain replication for
supporting high throughput and availability. In OSDI, 2004.
[56] Kaushik Veeraraghavan, Justin Meza, David Chou, Wonho Kim, Sonia
Margulis, Scoot Michelson, Rajesh Nishtala, Daniel Obenshain, Dmitri
Perelman, and Yee Jiun Song. Kraken: Leveraging live trafﬁc tests to
identify and resolve resource utilization bottlenecks in large scale Web
services. In OSDI, 2016.
[57] Brian White, Jay Lepreau, Leigh Stoller, Robert Ricci, Shashi Gu-
ruprasad, Mac Newbold, Mike Hibler, Chad Barb, and Abhijeet Joglekar.
An integrated experimental environment for distributed systems and
networks. In OSDI, 2002.