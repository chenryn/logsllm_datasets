maintain only 24 delta message increment versions in each day.
6 IMPLEMENTATION AND EVALUATION
6.1 System Implementation
We implement the TinyCR tracker on a Google Cloud VM instance
with 64 vCPUs and 624 GB memory using C++. The on-device DASS
verifier is implemented on a Raspberry Pi 3 with one single 1.4 GHz
processor and 1 GB RAM. Note the device used in the experiments
is just an example of a wide spectrum of devices that can use
TinyCR. TinyCR can be easily deployed on more powerful devices
like mobile phones and less powerful devices as long as they have
available memory (see Table 2. For real Censys data that contains
28.6M certificates, it requires 448KB).
In addition to TinyCR, we also implemented the CRLite filter
cascades [19] and Othello hashing [37] data structures with similar
synchronization settings as the TinyCR protocol for performance
comparison. The parameters for CRLite and Othello are set accord-
ing to the authors’ suggestions [19, 37]. Both TinyCR and Othello
can support dynamic updating of the revocation checking list, while
CRLite has to be rebuilt for most updates.
6.2 Metrics and Dataset
We evaluate the CR methods by the following metrics:
structures on a device.
update on the IDM server.
• On-device memory cost: the overall memory cost of the data
• Update time and synchronization latency: the time for each
• Bandwidth: the message cost caused by updates.
• Query cost: the delay to get a CR checking result.
We use both real-world and synthetic certificate datasets for
the evaluation. Since there is no IoT certificate dataset available,
we use the Censys web certificate dataset[4, 19] to evaluate how
those protocols perform in real-world CR verification scenarios.
We downloaded 30 millions items of historical NSS trusted certifi-
cates over 3 months from Censys using Google BigQuery [5]. After
removing the duplicated certificates, there are totally 28,593,752
items in the dataset. Then we use the CRLs or OCSP to obtain the
revocation status of all downloaded certificates. Among the 28.6
million certificates, 274,926 were revoked, i.e., the ratio between
the legitimate and revoked certificates is 103 : 1. To evaluate the
scalability, we create synthetic datasets containing up to 1 billion
certificates with different revocation ratios from 20% to 0.01%.
6.3 Memory cost
We construct the on-device data structures of TinyCR (DASS), CR-
Lite (filter cascade), and Othello respectively using the entire Censys
certificate data. We find TinyCR, CRLite, and Othello requires 430
KB, 439 KB, and 8,328 KB memory respectively to maintaining the
CR status of the 28.6 million certificates.
Then we conduct experiments on the synthetic dataset to investi-
gate how the memory sizes scale with the sizes and distributions of
the keys. In Fig. 8, we show the amortized memory cost (i.e. bits per
certificate) with respect to the total size |N | + |P| of the certificates
by setting r = |N |/|P| as 4 (Fig 8 a), 16 (Fig 8 b) and 128 (Fig. 8(c))
Figure 6: Structure of a delta message
Figure 7: Multi-way version control protocol.
the entry at bucket pos of Ta; otherwise, we flip the entry at bucket
pos−|Tb| of Tb, where Ta and Tb are the two maintained hash tables
in Othello [37]. In our evaluation, we will show on average only a
small number of buckets in O (if any) need to be flipped.
5.5 DASS Version Control
Since TinyCR uses delta messages to update the on-device checker,
the new state of DASS relies on the previous state. Thus, the system
may suffer from potential system/network failures that cause the
packet loss of the delta messages. To solve this problem, we intro-
duce a multi-way DASS version control protocol as an optional
design choice(as illustrated in Fig. 7).
In Fig. 7, the IDM server initiate a PUSH-SYN packet when a new
tracker DASSt is generated. Then the device sends back the digest
vd of its local verifier DASSd. Meanwhile, the IDM server maintains
a mapping table to keep track of a history of t recent verifier DASS
version IDs and the corresponding delta-msg increments. According
to our evaluation in Sec. 6.4.2, the average delta-msg increment
size is fewer than 100 bytes. Then the IDM server simply retrieves
all the missed delta-message increments and concatenates them to
generate the cross-version delta message ∆MSGt−d that denotes
the differences between DASSd and DASSt . In the ∆MSGt−d that
skips over multiple versions, we could include multiple Cuckoo
Filter Msg fields and one single Othello Msg field using the similar
encoding format as shown by Fig. 6. If vd is not maintained by the
version table, that means the device has missed a large amount of
updates. Then the server directly send the DASSt instead of the
delta message to the device. Optionally, the device returns an ACK
when the local DASS updating is accomplished.
If the updating frequency of certificate sets is too high in some
scenarios, it is not practical for the IDM server to send a signed delta
message after each update and track every DASS version. In such
case, we can use the version control design to batch the updates
with a bounded time granularity. For example, the IDM server can
Session 4B: Wireless, Mobile, and IoT CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1125(a) r = 4
(b) r = 16
(c) r = 128
(d) |P| + |N | = 226
Figure 8: Plot (a) to (c): Amortized memory for different key sizes when r = |N |/|P| is 4, 16, 128 respectively. Plot (d): Memory
cost for 226 keys with respect to r.
respectively. Meanwhile, in Fig. 8(d), we present the total memory
cost (in bytes) for storing the revocation status of 226 certificates,
by varying the ratio r. The vertical dash line in Fig. 8(d) represents
the ratio r of the Censys dataset in real-world scenario.
Fig. 8 shows the memory cost per certificate of all three data
structures keeps stable when r is fixed. For example, the amortized
memory sizes for TinyCR, CRLite, and Othello are around 0.108 bits,
0.111 bits, and 2.333 bits per certificate respectively for arbitrarily
large key sets when r = 128. The amortized memory for Othello
is independent with r (it is controlled by a hyper-parameter and
is set as 2.33 bits), whereas both TinyCR and CRLite use much
less memory as r grows. It can also be seen from the graph that
both TinyCR and CRLite use less than 1 MB to store the around
64 million certificates when r = 100 (which is close to the ratio for
real-world CR lists) and use less than 8 MB when r = 10, while
Othello always requires around 20 MB.
6.4 Updating efficiency
In this section, we evaluate the update and synchronization over-
head of the data structures regarding any change of the global CRL.
Specifically, we utilize the Censys certificates and synthetic data
sets to simulate the following updating scenarios.
Short-term insertion/value flipping: We use a certificate dataset
to initialize the CR verification data structures through a static ap-
proach, then evaluate the latency of the inserting/value flipping
operation on the initial data structures without reconstructing the
data structures (except for filter cascades).
Long-term insertion: We use 100 million certificates to initial-
ize TinyCR, then insert another 100 million certificates item by item
to them. In the simulation, we assume the revocation ratio of the
initial and the inserted certificate sets are consistent.
Long-term value flipping: We use 100 million certificates to
initialize TinyCR. Then we randomly sample |P| validate certificates
and revoke those certificates, where |P| is the number of revoked
certificates in the initial set. We simulate the scenario where the
number of revoked certificates is doubled during the usage period
before expiration. Note that the revocation of the sampled set is a
gradual process, i.e., one certificate is revoked at each timestamp
when the CA decides to revoke it. In TinyCR, the reference value
of a newly revoked certificate should be changed from 0 to 1.
6.4.1 Overhead on the IDM server (tracker). The tracker on the IDM
server is required to react quickly for every update (insertion and
# of Certs Method Mem
458 KB
8.3 MB
448 KB
CRLite
Othello
TinyCR
Censys
28.6M
Add P Add N P → N N → P
3.2 s
3.2 s
9.2 µs
11.4 µs
349.9 µs
345.3 µs
3.2 s
10.1 µs
27.0 µs
3.2 s
9.9 µs
1.6 µs
10M
100M
1B
CRLite
Othello
TinyCR
CRLite
Othello
TinyCR
CRLite
Othello
TinyCR
172 KB
2.9 MB
169 KB
1.7 MB
29.2 MB
1.7 MB
17.2 MB
291.7 MB
16.9 MB
1.0 s
4.6 µs
280.9 µs
10.1 s
8.5 µs
304.9 µs
153.9 s
10.0 µs
296.0 µs
1.0 s
5.0 µs
1.2 µs
10.1 s
7.5 µs
1.6 µs
153.9 s
10.2 µs
2.7 µs
1.0 s
4.6 µs
16.6 µs
10.1 s
7.1 µs
21.6 µs
153.9 s
8.2 µs
27.3 µs
1.0 s
4.4 µs
289.9 µs
10.1 s
7.0 µs
311.5 µs
153.9 s
7.0 µs
319.5 µs
Table 2: On-device memory cost and average updating la-
tency on the tracker for different set sizes. The revocation
ratio for synthetic data is 1%.
value flipping) of the CRLs. In Table 2, we show the on-device mem-
ory cost and the average computational latency of the tracker to
update the data summaries and generate the delta message in short-
term updating scenarios. Specifically, we simulate the scenarios
with the Censys dataset and the synthetic data sets of different sizes
to evaluate the scalability of the methods in an IoT with billions
of devices. In our synthetic data, we set the certificate revocation
ratio to be 1%, which is close to the ratio of the Censys dataset. We
discuss the insertion of revoked certificates and legitimate certifi-
cates (the more common case) separately in the fourth and fifth
columns, as they will cause different updating overhead based on
the algorithms. Similarly, we also evaluate the value flipping case
where a revoked certificate is moved to the legitimate list, and the
case where an legitimate one is moved to the revoked list (the more
common case) in sixth and seventh columns respectively.
From Table 2, we find the updating time of CRLite significantly
increases with the size of the sets. As a static data structure, filter
cascades have to be reconstructed using the entire certificate sets for
any updates, which would cause tremendous overhead to the server
and large bandwidth overhead. Meanwhile, the long latency of
updating can also cause memory concurrency issues for the tracker
when the updating pace is high. Therefore, in practice, CRLite is
only updated in a batching way, for example, the tracker and verifier
Session 4B: Wireless, Mobile, and IoT CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1126are recommended to update once every day [19]. Consequently, this
strategy would introduce a synchronization latency of one day – a
big security vulnerability. The synchronization latency of TinyCR
is the update time plus network latency.
On the other hand, the update latency of TinyCR and Othello
is significantly lower than CRLite and scales much better with the
size of certificate sets. Overall, Othello achieves the highest up-
dating throughput for most cases, at the cost of around 16x more
memory than TinyCR and CRLite. We also notice TinyCR is most
computational-efficient for inserting legitimate certificates to the
CR status list, which is the most common type of updating. Even
in its worst case, the corresponding updating latency is smaller
than 1 millisecond for up to 1 billion keys, which is usually over-
whelmed by the network latency in practice, showing TinyCR can
sufficiently support the real-time synchronization with neglectable
extra processing overhead. Thus, TinyCR is a more efficient and
secure choice for the IoT CR verification task where the certifi-
cate universe is large. The theoretical synchronization latency of
TinyCR could be just the update time plus network latency in a
real-time updating manner.
However, due to the connection maintenance and signing cost in
practice, real-time updating is not always practical when the updat-