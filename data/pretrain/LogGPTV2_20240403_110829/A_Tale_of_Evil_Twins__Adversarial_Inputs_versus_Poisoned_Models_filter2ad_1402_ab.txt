the two attack vectors. Specifically, they represent different routes
to achieve the same aim (i.e., misclassification of the target input):
one perturbs the input at the cost of â€œfidelityâ€ (whether the attack
retains the original inputâ€™s perceptual quality), while the other
fInput SpaceOutput SpaceOutput SpaceInput SpaceAdversarial Poisoned InputModelfâˆ—f(xâˆ—)xâˆ—xfâˆ—(x)2.2 Attack Vectors
DNNs are inherently susceptible to malicious manipulations. In
particular, two primary attack vectors have been considered in the
literature, namely, adversarial inputs and poisoned models.
Adversarial Inputs. Adversarial inputs are maliciously crafted
samples to deceive target DNNs at inference time. An adversarial
input ğ‘¥âˆ— is typically generated by perturbing a benign input ğ‘¥â—¦ to
change its classification to a target class ğ‘¡ desired by the adversary
(e.g., pixel perturbation [34] or spatial transformation [1]). To ensure
the attack evasiveness, the perturbation is often constrained to a
feasible set (e.g., a norm ball Fğœ–(ğ‘¥â—¦) = {ğ‘¥|âˆ¥ğ‘¥ âˆ’ğ‘¥â—¦âˆ¥âˆ â‰¤ ğœ–}). Formally,
the attack is formulated as the optimization objective:
ğ‘¥âˆ— = arg min
ğ‘¥âˆˆFğœ– (ğ‘¥â—¦) â„“(ğ‘¥, ğ‘¡; ğœƒâ—¦)
(1)
where the loss function measures the difference between ğ‘“ â€™s pre-
diction ğ‘“ (ğ‘¥; ğœƒâ—¦) and the adversaryâ€™s desired classification ğ‘¡.
Eqn (1) can be solved in many ways. For instance, FGSM [20] uses
one-step descent along â„“â€™s gradient sign direction, PGD [34] applies
a sequence of projected gradient descent steps, while C&W [7] solves
Eqn (1) with iterative optimization.
Poisoned Models. Poisoned models are adversely forged DNNs
that are embedded with malicious functions (i.e., misclassification
of target inputs) during training.
This attack can be formulated as perturbing a benign DNN ğœƒâ—¦ to
a poisoned version ğœƒâˆ—.2 To ensure its evasiveness, the perturbation
is often constrained to a feasible set Fğ›¿(ğœƒâ—¦) to limit the impact
on non-target inputs. For instance, Fğ›¿(ğœƒâ—¦) = {ğœƒ|Eğ‘¥âˆˆR[|ğ‘“ (ğ‘¥; ğœƒ) âˆ’
ğ‘“ (ğ‘¥; ğœƒâ—¦)|] â‰¤ ğ›¿} specifies that the expected difference between ğœƒâ—¦
and ğœƒâˆ—â€™s predictions regarding the inputs in a reference set R stays
below a threshold ğ›¿. Formally, the adversary attempts to optimize
the objective function:
ğœƒâˆ— = arg min
ğœƒ âˆˆFğ›¿ (ğœƒâ—¦) Eğ‘¥â—¦âˆˆT [â„“(ğ‘¥â—¦, ğ‘¡ğ‘¥â—¦; ğœƒ)]
(2)
where T represents the set of target inputs, ğ‘¡ğ‘¥â—¦ denotes ğ‘¥â—¦â€™s classi-
fication desired by the adversary, and the loss function is defined
similarly as in Eqn (1).
In practice, Eqn (2) can be solved through either polluting train-
ing data [21, 44, 47] or modifying benign DNNs [24, 32]. For in-
stance, StingRay [47] generates poisoning data by perturbing be-
nign inputs close to ğ‘¥â—¦ in the feature space; PoisonFrog [44] synthe-
sizes poisoning data close to ğ‘¥â—¦ in the feature space but perceptually
belonging to ğ‘¡ in the input space; while ModelReuse [24] directly
perturbs the DNN parameters to minimize ğ‘¥â—¦â€™s distance to a repre-
sentative input from ğ‘¡ in the feature space.
2.3 Threat Models
We assume a threat model wherein the adversary is able to exploit
both attack vectors. During training, the adversary forges a DNN
embedded with malicious functions. This poisoned model is then
incorporated into the target deep learning system through either
system development or maintenance [24, 32]. At inference time, the
adversary further generates adversarial inputs to trigger the target
system to malfunction.
2Note that below we use ğœƒâ—¦ (ğœƒâˆ—) to denote both a DNN and its parameter configuration.
Also note that the benign model ğœƒâ—¦ is independent of the target benign input ğ‘¥â—¦.
This threat model is realistic. Due to the increasing model com-
plexity and training cost, it becomes not only tempting but also
necessary to reuse pre-trained models [21, 24, 32]. Besides rep-
utable sources (e.g., Google), most pre-trained DNNs on the market
(e.g., [6]) are provided by untrusted third parties. Given the wide-
spread use of deep learning in security-critical domains, adversaries
are strongly incentivized to build poisoned models, lure users to
reuse them, and trigger malicious functions via adversarial inputs
during system use. The backdoor attacks [21, 32, 54] are concrete
instances of this threat model: the adversary makes DNN sensitive
to certain trigger patterns (e.g., watermarks), so that any trigger-
embedded inputs are misclassified at inference. Conceptually, one
may regard the trigger as a universal perturbation ğ‘Ÿ [36]. To train
the poisoned model ğœƒâˆ—, the adversary samples inputs T from the
training set D and enforces the trigger-embedded input (ğ‘¥â—¦ + ğ‘Ÿ)
for each ğ‘¥â—¦ âˆˆ T to be misclassified to the target class ğ‘¡. Formally,
the adversary optimizes the objective function:
ğ‘Ÿ âˆˆFğœ–,ğœƒ âˆˆFğ›¿ (ğœƒâ—¦) Eğ‘¥â—¦âˆˆT [â„“(ğ‘¥â—¦ + ğ‘Ÿ, ğ‘¡; ğœƒ)]
min
(3)
where both the trigger and poisoned model need to satisfy the eva-
siveness constraints. Nonetheless, in the existing backdoor attacks,
Eqn (3) is often solved in an ad hoc manner, resulting in subopti-
mal triggers and/or poisoned models. For example, TrojanNN [32]
pre-defines the trigger shape (e.g., Apple logo) and determines its
pixel values in a preprocessing step. We show that the existing at-
tacks can be significantly enhanced within a rigorous optimization
framework (details in Â§ 5).
3 A UNIFIED ATTACK FRAMEWORK
Despite their apparent variations, adversarial inputs and poisoned
models share the same objective of forcing target DNNs (modified
or not) to misclassify pre-defined inputs (perturbed or not). While
intensive research has been conducted on the two attack vectors in
parallel, little is known about their fundamental connections.
3.1 Attack Objectives
To bridge this gap, we study the two attack vectors using input
model co-optimization (IMC), a unified attack framework. Intuitively,
within IMC, the adversary is allowed to perturb each target input
ğ‘¥â—¦ âˆˆ T and/or to poison the original DNN ğœƒâ—¦, with the objective of
forcing the adversarial version ğ‘¥âˆ— of each ğ‘¥â—¦ âˆˆ T to be misclassified
to a target class ğ‘¡ğ‘¥â—¦ by the poisoned model ğœƒâˆ—.
Formally, we define the unified attack model by integrating the
objectives of Eqn (1), Eqn (2), and Eqn (3):
(cid:20)
ğ‘¥âˆˆFğœ– (ğ‘¥â—¦) â„“(cid:0)ğ‘¥, ğ‘¡ğ‘¥â—¦; ğœƒ(cid:1)(cid:21)
min
min
ğœƒ âˆˆFğ›¿ (ğœƒâ—¦) Eğ‘¥â—¦âˆˆT
(4)
where the different terms define the adversaryâ€™s multiple desiderata:
â€¢ The loss â„“ quantifies the difference of the model prediction and
the classification desired by the adversary, which represents the
attack efficacy â€“ whether the attack successfully forces the DNN
to misclassify each input ğ‘¥â—¦ âˆˆ T to its target class ğ‘¡ğ‘¥â—¦.
â€¢ The constraint Fğœ– bounds the impact of input perturbation on
each target input, which represents the attack fidelity â€“ whether
the attack retains the perceptual similarity of each adversarial
input to its benign counterpart.
â€¢ The constraint Fğ›¿ bounds the influence of model perturbation
on non-target inputs, which represents the attack specificity â€“
whether the attack precisely directs its influence to the set of
target inputs T only.
Figure 2: Adversaryâ€™s multiple objectives.
This formulation subsumes many attacks in the literature. Specif-
ically, (i) in the case of ğ›¿ = 0 and |T | = 1, Eqn (4) is instantiated as
the adversarial attack; (ii) in the case of ğœ– = 0, Eqn (4) is instantiated
as the poisoning attack, which can be either a single target (|T | = 1)
or multiple targets (|T | > 1); and (iii) in the case that a universal
perturbation (ğ‘¥ âˆ’ ğ‘¥â—¦) is defined for all the inputs {ğ‘¥â—¦ âˆˆ T} and all
the target classes {ğ‘¡ğ‘¥â—¦} are fixed as ğ‘¡, Eqn (4) is instantiated as the
backdoor attack. Also note that this formulation does not make any
assumptions regarding the adversaryâ€™s capability or resource (e.g.,
access to the training or inference data), while it is solely defined
in terms of the adversaryâ€™s objectives.
Interestingly, the three objectives are tightly intertwined, form-
ing a triangle structure, as illustrated in Figure 2. We have the
following observations.
â€¢ It is impossible to achieve all the objectives simultaneously. To
attain attack efficacy (i.e., launching a successful attack), it requires
either perturbing the input (i.e., at the cost of fidelity) or modifying
the model (i.e., at the cost of specificity).
â€¢ It is feasible to attain two out of the three objectives at the same
time. For instance, it is trivial to achieve both attack efficacy and
fidelity by setting ğœ– = 0 (i.e., only model perturbation is allowed).
â€¢ With one objective fixed, it is possible to balance the other two.
For instance, with fixed attack efficacy, it allows to trade between
attack fidelity and specificity.
Next, by casting the attack vectors of adversarial inputs and poi-
soned models within the IMC framework, we reveal their inherent
connections and explore the dynamic interactions among the attack
efficacy, fidelity, and specificity.
3.2 Attack Implementation
Recall that IMC is formulated in Eqn (4) as optimizing the objectives
over both the input and model. While it is impractical to exactly
solve Eqn (4) due to its non-convexity and non-linearity, we refor-
mulate Eqn (4) to make it amenable for optimization. To ease the
discussion, in the following, we assume the case of a single target
input ğ‘¥â—¦ in the target set T (i.e., |T | = 1), while the generalization
to multiple targets is straightforward. Further, when the context is
clear, we omit the reference input ğ‘¥â—¦, benign model ğœƒâ—¦, and target
class ğ‘¡ to simplify the notations.
3.2.1 Reformulation. The constraints Fğœ–(ğ‘¥â—¦) and Fğ›¿(ğœƒâ—¦) in Eqn (4)
essentially bound the fidelity and specificity losses. The fidelity loss
â„“f(ğ‘¥) quantifies whether the perturbed input ğ‘¥ faithfully retains its
perceptual similarity to its benign counterpart ğ‘¥â—¦ (e.g., âˆ¥ğ‘¥ âˆ’ ğ‘¥â—¦âˆ¥);
the specificity loss â„“s(ğœƒ) quantifies whether the attack impacts non-
target inputs (e.g., Eğ‘¥âˆˆR [|ğ‘“ (ğ‘¥; ğœƒ) âˆ’ ğ‘“ (ğ‘¥; ğœƒâ—¦)|]). According to opti-
mization theory [5], specifying the bounds ğœ– and ğ›¿ on the input and
model perturbation amounts to specifying the hyper-parameters
ğœ† and ğœˆ on the fidelity and specificity losses (the adversary is able
to balance different objectives by controlling ğœ† and ğœˆ). Eqn (4) can
therefore be re-formulated as follows:
â„“(ğ‘¥; ğœƒ) + ğœ†â„“f(ğ‘¥) + ğœˆâ„“s(ğœƒ)
min
ğ‘¥,ğœƒ
(5)
Nonetheless, it is still difficult to directly optimize Eqn (5) given
that the input ğ‘¥ and the model ğœƒ are mutually dependent on each
other. Note that however â„“f does not depend on ğœƒ while â„“s does
not depend on ğ‘¥. We thus further approximate Eqn (5) with the
following bi-optimization formulation:
(cid:26) ğ‘¥âˆ— = arg minğ‘¥ â„“(ğ‘¥; ğœƒâˆ—) + ğœ†â„“f(ğ‘¥)
ğœƒâˆ— = arg minğœƒ â„“(ğ‘¥âˆ—; ğœƒ) + ğœˆâ„“s(ğœƒ)
(6)
3.2.2 Optimization. This formulation naturally leads to an opti-
mization procedure that alternates between updating the input ğ‘¥
and updating the model ğœƒ, as illustrated in Figure 3. Specifically, let
ğ‘¥ (ğ‘˜) and ğœƒ (ğ‘˜) be the perturbed input and model respectively after the
ğ‘˜-th iteration. The (ğ‘˜ + 1)-th iteration comprises two operations.
Figure 3: IMC alternates between two operations: (i) input perturba-
tion to update the adversarial input ğ‘¥, and (ii) model perturbation
to update the poisoned model ğœƒ.
Input Perturbation â€“ In this step, with the model ğœƒ (ğ‘˜) fixed, it
updates the perturbed input by optimizing the objective:
â„“(ğ‘¥; ğœƒ (ğ‘˜)) + ğœ†â„“f(ğ‘¥)
ğ‘¥ (ğ‘˜+1) = arg minğ‘¥
(7)
In practice, this step can be approximated by applying an off-
the-shelf optimizer (e.g., Adam [26]) or solved partially by applying
gradient descent on the objective function. For instance, in our
implementation, we apply projected gradient descent (PGD [34]) as
the update operation:
ğ‘¥ (ğ‘˜+1) = Î Fğœ– (ğ‘¥â—¦)(cid:0)ğ‘¥ (ğ‘˜) âˆ’ ğ›¼ sgn(cid:0)âˆ‡ğ‘¥ â„“(cid:0)ğ‘¥ (ğ‘˜); ğœƒ (ğ‘˜)(cid:1)(cid:1)(cid:1)
where Î  is the projection operator, Fğœ–(ğ‘¥â—¦) is the feasible set (i.e.,
{ğ‘¥|âˆ¥ğ‘¥ âˆ’ ğ‘¥â—¦âˆ¥ â‰¤ ğœ–}), and ğ›¼ is the learning rate.
Model Perturbation â€“ In this step, with the input ğ‘¥ (ğ‘˜+1) fixed, it
searches for the model perturbation by optimizing the objective:
(8)
â„“(ğ‘¥ (ğ‘˜+1); ğœƒ) + ğœˆâ„“s(ğœƒ)
ğœƒ (ğ‘˜+1) = arg min
ğœƒ
In practice, this step can be approximated by running re-training
over a training set that mixes the original training data D and ğ‘š
copies of the current adversarial input ğ‘¥ (ğ‘˜+1). In our implementation,
ğ‘š is set to be half of the batch size.
Speciï¬cityEfï¬cacyFidelityInput PerturbationÎ¸(k+1)Classiï¬cation Boundaryx(k)x(k+1)Î¸(k)Model PerturbationClassiï¬cation Boundaryx(k+1)Î¸(k)âˆ†xâˆ†Î¸Algorithm 1: IMC Attack
Input: benign input â€“ ğ‘¥â—¦; benign model â€“ ğœƒâ—¦; target class â€“ ğ‘¡;
hyper-parameters â€“ ğœ†, ğœˆ
Output: adversarial input â€“ ğ‘¥âˆ—; poisoned model â€“ ğœƒâˆ—
// initialization
1 ğ‘¥ (0), ğœƒ (0), ğ‘˜ â† ğ‘¥â—¦, ğœƒâ—¦, 0;
// optimization
3
2 while not converged yet do
// input perturbation
ğ‘¥ (ğ‘˜+1) = arg minğ‘¥ â„“(ğ‘¥; ğœƒ (ğ‘˜)) + ğœ†â„“f (ğ‘¥);
// model perturbation
ğœƒ (ğ‘˜+1) = arg minğœƒ â„“(ğ‘¥ (ğ‘˜+1); ğœƒ) + ğœˆâ„“s(ğœƒ);
ğ‘˜ â† ğ‘˜ + 1;
4