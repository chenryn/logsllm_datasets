the two attack vectors. Specifically, they represent different routes
to achieve the same aim (i.e., misclassification of the target input):
one perturbs the input at the cost of “fidelity” (whether the attack
retains the original input’s perceptual quality), while the other
fInput SpaceOutput SpaceOutput SpaceInput SpaceAdversarial Poisoned InputModelf∗f(x∗)x∗xf∗(x)2.2 Attack Vectors
DNNs are inherently susceptible to malicious manipulations. In
particular, two primary attack vectors have been considered in the
literature, namely, adversarial inputs and poisoned models.
Adversarial Inputs. Adversarial inputs are maliciously crafted
samples to deceive target DNNs at inference time. An adversarial
input 𝑥∗ is typically generated by perturbing a benign input 𝑥◦ to
change its classification to a target class 𝑡 desired by the adversary
(e.g., pixel perturbation [34] or spatial transformation [1]). To ensure
the attack evasiveness, the perturbation is often constrained to a
feasible set (e.g., a norm ball F𝜖(𝑥◦) = {𝑥|∥𝑥 −𝑥◦∥∞ ≤ 𝜖}). Formally,
the attack is formulated as the optimization objective:
𝑥∗ = arg min
𝑥∈F𝜖 (𝑥◦) ℓ(𝑥, 𝑡; 𝜃◦)
(1)
where the loss function measures the difference between 𝑓 ’s pre-
diction 𝑓 (𝑥; 𝜃◦) and the adversary’s desired classification 𝑡.
Eqn (1) can be solved in many ways. For instance, FGSM [20] uses
one-step descent along ℓ’s gradient sign direction, PGD [34] applies
a sequence of projected gradient descent steps, while C&W [7] solves
Eqn (1) with iterative optimization.
Poisoned Models. Poisoned models are adversely forged DNNs
that are embedded with malicious functions (i.e., misclassification
of target inputs) during training.
This attack can be formulated as perturbing a benign DNN 𝜃◦ to
a poisoned version 𝜃∗.2 To ensure its evasiveness, the perturbation
is often constrained to a feasible set F𝛿(𝜃◦) to limit the impact
on non-target inputs. For instance, F𝛿(𝜃◦) = {𝜃|E𝑥∈R[|𝑓 (𝑥; 𝜃) −
𝑓 (𝑥; 𝜃◦)|] ≤ 𝛿} specifies that the expected difference between 𝜃◦
and 𝜃∗’s predictions regarding the inputs in a reference set R stays
below a threshold 𝛿. Formally, the adversary attempts to optimize
the objective function:
𝜃∗ = arg min
𝜃 ∈F𝛿 (𝜃◦) E𝑥◦∈T [ℓ(𝑥◦, 𝑡𝑥◦; 𝜃)]
(2)
where T represents the set of target inputs, 𝑡𝑥◦ denotes 𝑥◦’s classi-
fication desired by the adversary, and the loss function is defined
similarly as in Eqn (1).
In practice, Eqn (2) can be solved through either polluting train-
ing data [21, 44, 47] or modifying benign DNNs [24, 32]. For in-
stance, StingRay [47] generates poisoning data by perturbing be-
nign inputs close to 𝑥◦ in the feature space; PoisonFrog [44] synthe-
sizes poisoning data close to 𝑥◦ in the feature space but perceptually
belonging to 𝑡 in the input space; while ModelReuse [24] directly
perturbs the DNN parameters to minimize 𝑥◦’s distance to a repre-
sentative input from 𝑡 in the feature space.
2.3 Threat Models
We assume a threat model wherein the adversary is able to exploit
both attack vectors. During training, the adversary forges a DNN
embedded with malicious functions. This poisoned model is then
incorporated into the target deep learning system through either
system development or maintenance [24, 32]. At inference time, the
adversary further generates adversarial inputs to trigger the target
system to malfunction.
2Note that below we use 𝜃◦ (𝜃∗) to denote both a DNN and its parameter configuration.
Also note that the benign model 𝜃◦ is independent of the target benign input 𝑥◦.
This threat model is realistic. Due to the increasing model com-
plexity and training cost, it becomes not only tempting but also
necessary to reuse pre-trained models [21, 24, 32]. Besides rep-
utable sources (e.g., Google), most pre-trained DNNs on the market
(e.g., [6]) are provided by untrusted third parties. Given the wide-
spread use of deep learning in security-critical domains, adversaries
are strongly incentivized to build poisoned models, lure users to
reuse them, and trigger malicious functions via adversarial inputs
during system use. The backdoor attacks [21, 32, 54] are concrete
instances of this threat model: the adversary makes DNN sensitive
to certain trigger patterns (e.g., watermarks), so that any trigger-
embedded inputs are misclassified at inference. Conceptually, one
may regard the trigger as a universal perturbation 𝑟 [36]. To train
the poisoned model 𝜃∗, the adversary samples inputs T from the
training set D and enforces the trigger-embedded input (𝑥◦ + 𝑟)
for each 𝑥◦ ∈ T to be misclassified to the target class 𝑡. Formally,
the adversary optimizes the objective function:
𝑟 ∈F𝜖,𝜃 ∈F𝛿 (𝜃◦) E𝑥◦∈T [ℓ(𝑥◦ + 𝑟, 𝑡; 𝜃)]
min
(3)
where both the trigger and poisoned model need to satisfy the eva-
siveness constraints. Nonetheless, in the existing backdoor attacks,
Eqn (3) is often solved in an ad hoc manner, resulting in subopti-
mal triggers and/or poisoned models. For example, TrojanNN [32]
pre-defines the trigger shape (e.g., Apple logo) and determines its
pixel values in a preprocessing step. We show that the existing at-
tacks can be significantly enhanced within a rigorous optimization
framework (details in § 5).
3 A UNIFIED ATTACK FRAMEWORK
Despite their apparent variations, adversarial inputs and poisoned
models share the same objective of forcing target DNNs (modified
or not) to misclassify pre-defined inputs (perturbed or not). While
intensive research has been conducted on the two attack vectors in
parallel, little is known about their fundamental connections.
3.1 Attack Objectives
To bridge this gap, we study the two attack vectors using input
model co-optimization (IMC), a unified attack framework. Intuitively,
within IMC, the adversary is allowed to perturb each target input
𝑥◦ ∈ T and/or to poison the original DNN 𝜃◦, with the objective of
forcing the adversarial version 𝑥∗ of each 𝑥◦ ∈ T to be misclassified
to a target class 𝑡𝑥◦ by the poisoned model 𝜃∗.
Formally, we define the unified attack model by integrating the
objectives of Eqn (1), Eqn (2), and Eqn (3):
(cid:20)
𝑥∈F𝜖 (𝑥◦) ℓ(cid:0)𝑥, 𝑡𝑥◦; 𝜃(cid:1)(cid:21)
min
min
𝜃 ∈F𝛿 (𝜃◦) E𝑥◦∈T
(4)
where the different terms define the adversary’s multiple desiderata:
• The loss ℓ quantifies the difference of the model prediction and
the classification desired by the adversary, which represents the
attack efficacy – whether the attack successfully forces the DNN
to misclassify each input 𝑥◦ ∈ T to its target class 𝑡𝑥◦.
• The constraint F𝜖 bounds the impact of input perturbation on
each target input, which represents the attack fidelity – whether
the attack retains the perceptual similarity of each adversarial
input to its benign counterpart.
• The constraint F𝛿 bounds the influence of model perturbation
on non-target inputs, which represents the attack specificity –
whether the attack precisely directs its influence to the set of
target inputs T only.
Figure 2: Adversary’s multiple objectives.
This formulation subsumes many attacks in the literature. Specif-
ically, (i) in the case of 𝛿 = 0 and |T | = 1, Eqn (4) is instantiated as
the adversarial attack; (ii) in the case of 𝜖 = 0, Eqn (4) is instantiated
as the poisoning attack, which can be either a single target (|T | = 1)
or multiple targets (|T | > 1); and (iii) in the case that a universal
perturbation (𝑥 − 𝑥◦) is defined for all the inputs {𝑥◦ ∈ T} and all
the target classes {𝑡𝑥◦} are fixed as 𝑡, Eqn (4) is instantiated as the
backdoor attack. Also note that this formulation does not make any
assumptions regarding the adversary’s capability or resource (e.g.,
access to the training or inference data), while it is solely defined
in terms of the adversary’s objectives.
Interestingly, the three objectives are tightly intertwined, form-
ing a triangle structure, as illustrated in Figure 2. We have the
following observations.
• It is impossible to achieve all the objectives simultaneously. To
attain attack efficacy (i.e., launching a successful attack), it requires
either perturbing the input (i.e., at the cost of fidelity) or modifying
the model (i.e., at the cost of specificity).
• It is feasible to attain two out of the three objectives at the same
time. For instance, it is trivial to achieve both attack efficacy and
fidelity by setting 𝜖 = 0 (i.e., only model perturbation is allowed).
• With one objective fixed, it is possible to balance the other two.
For instance, with fixed attack efficacy, it allows to trade between
attack fidelity and specificity.
Next, by casting the attack vectors of adversarial inputs and poi-
soned models within the IMC framework, we reveal their inherent
connections and explore the dynamic interactions among the attack
efficacy, fidelity, and specificity.
3.2 Attack Implementation
Recall that IMC is formulated in Eqn (4) as optimizing the objectives
over both the input and model. While it is impractical to exactly
solve Eqn (4) due to its non-convexity and non-linearity, we refor-
mulate Eqn (4) to make it amenable for optimization. To ease the
discussion, in the following, we assume the case of a single target
input 𝑥◦ in the target set T (i.e., |T | = 1), while the generalization
to multiple targets is straightforward. Further, when the context is
clear, we omit the reference input 𝑥◦, benign model 𝜃◦, and target
class 𝑡 to simplify the notations.
3.2.1 Reformulation. The constraints F𝜖(𝑥◦) and F𝛿(𝜃◦) in Eqn (4)
essentially bound the fidelity and specificity losses. The fidelity loss
ℓf(𝑥) quantifies whether the perturbed input 𝑥 faithfully retains its
perceptual similarity to its benign counterpart 𝑥◦ (e.g., ∥𝑥 − 𝑥◦∥);
the specificity loss ℓs(𝜃) quantifies whether the attack impacts non-
target inputs (e.g., E𝑥∈R [|𝑓 (𝑥; 𝜃) − 𝑓 (𝑥; 𝜃◦)|]). According to opti-
mization theory [5], specifying the bounds 𝜖 and 𝛿 on the input and
model perturbation amounts to specifying the hyper-parameters
𝜆 and 𝜈 on the fidelity and specificity losses (the adversary is able
to balance different objectives by controlling 𝜆 and 𝜈). Eqn (4) can
therefore be re-formulated as follows:
ℓ(𝑥; 𝜃) + 𝜆ℓf(𝑥) + 𝜈ℓs(𝜃)
min
𝑥,𝜃
(5)
Nonetheless, it is still difficult to directly optimize Eqn (5) given
that the input 𝑥 and the model 𝜃 are mutually dependent on each
other. Note that however ℓf does not depend on 𝜃 while ℓs does
not depend on 𝑥. We thus further approximate Eqn (5) with the
following bi-optimization formulation:
(cid:26) 𝑥∗ = arg min𝑥 ℓ(𝑥; 𝜃∗) + 𝜆ℓf(𝑥)
𝜃∗ = arg min𝜃 ℓ(𝑥∗; 𝜃) + 𝜈ℓs(𝜃)
(6)
3.2.2 Optimization. This formulation naturally leads to an opti-
mization procedure that alternates between updating the input 𝑥
and updating the model 𝜃, as illustrated in Figure 3. Specifically, let
𝑥 (𝑘) and 𝜃 (𝑘) be the perturbed input and model respectively after the
𝑘-th iteration. The (𝑘 + 1)-th iteration comprises two operations.
Figure 3: IMC alternates between two operations: (i) input perturba-
tion to update the adversarial input 𝑥, and (ii) model perturbation
to update the poisoned model 𝜃.
Input Perturbation – In this step, with the model 𝜃 (𝑘) fixed, it
updates the perturbed input by optimizing the objective:
ℓ(𝑥; 𝜃 (𝑘)) + 𝜆ℓf(𝑥)
𝑥 (𝑘+1) = arg min𝑥
(7)
In practice, this step can be approximated by applying an off-
the-shelf optimizer (e.g., Adam [26]) or solved partially by applying
gradient descent on the objective function. For instance, in our
implementation, we apply projected gradient descent (PGD [34]) as
the update operation:
𝑥 (𝑘+1) = ΠF𝜖 (𝑥◦)(cid:0)𝑥 (𝑘) − 𝛼 sgn(cid:0)∇𝑥 ℓ(cid:0)𝑥 (𝑘); 𝜃 (𝑘)(cid:1)(cid:1)(cid:1)
where Π is the projection operator, F𝜖(𝑥◦) is the feasible set (i.e.,
{𝑥|∥𝑥 − 𝑥◦∥ ≤ 𝜖}), and 𝛼 is the learning rate.
Model Perturbation – In this step, with the input 𝑥 (𝑘+1) fixed, it
searches for the model perturbation by optimizing the objective:
(8)
ℓ(𝑥 (𝑘+1); 𝜃) + 𝜈ℓs(𝜃)
𝜃 (𝑘+1) = arg min
𝜃
In practice, this step can be approximated by running re-training
over a training set that mixes the original training data D and 𝑚
copies of the current adversarial input 𝑥 (𝑘+1). In our implementation,
𝑚 is set to be half of the batch size.
SpeciﬁcityEfﬁcacyFidelityInput Perturbationθ(k+1)Classiﬁcation Boundaryx(k)x(k+1)θ(k)Model PerturbationClassiﬁcation Boundaryx(k+1)θ(k)∆x∆θAlgorithm 1: IMC Attack
Input: benign input – 𝑥◦; benign model – 𝜃◦; target class – 𝑡;
hyper-parameters – 𝜆, 𝜈
Output: adversarial input – 𝑥∗; poisoned model – 𝜃∗
// initialization
1 𝑥 (0), 𝜃 (0), 𝑘 ← 𝑥◦, 𝜃◦, 0;
// optimization
3
2 while not converged yet do
// input perturbation
𝑥 (𝑘+1) = arg min𝑥 ℓ(𝑥; 𝜃 (𝑘)) + 𝜆ℓf (𝑥);
// model perturbation
𝜃 (𝑘+1) = arg min𝜃 ℓ(𝑥 (𝑘+1); 𝜃) + 𝜈ℓs(𝜃);
𝑘 ← 𝑘 + 1;
4