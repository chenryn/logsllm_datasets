game must be successfully completed within a ﬁxed time (we allow
60s); the user gets feedback on the correctness of every drag-drop,
by a star on success and a cross on failure (Figure 6, Appendix A).
For each of the 4 games, we set 5 parameterizations, choosing
object speed (low, medium, high) as (10, 20, 40) frames per second
(FPS), and number of moving objects as (4, 5, 6).
(These frame
rates translate into average object speeds of 12.07, 24.14 and 48.28
pixels/second, resp., given the objects move 1.207 pixels/frame.) For
each game, we used 5 combinations of speed and number of objects:
(10 FPS, 4 objects); (20 FPS, 4 objects); (20 FPS, 5 objects); (20
FPS, 6 objects); and (40 FPS, 4 objects). This resulted in a total of
20 games in our corpus.
3Although our implementation and analyses does not directly involve
the “are you a human” captchas, it is generalized enough for our
results to be applicable to these captchas also (i.e., the ones that fall
under the categories evaluated in our work).
3
4. USABILITY
In this section, we report on a usability study of our representative
DCG captcha category.
4.1 Study Design, Goals, and Process
Our study involved 40 participants who were primarily students
from various backgrounds. (For demographics, see Appendix A, Ta-
ble 9). The study was web-based and comprised of three phases.
The pre-study phase involved registering and brieﬂy explaining the
participants about the protocols of the study. In particular, the par-
ticipants were shown “consent information,” which they had to agree
before proceeding with the study. This was followed by collecting
participant demographics and then the participants playing the differ-
ent DCG captcha games. This actual study phase attempted to mimic
a realistic captcha scenario which typically involves ﬁlling out a form
followed by solving captcha. To avoid explicit priming, however, we
did not mention that the study is about captcha or security, but rather
indicated that it is about assessing the usability of a web interface.
In the post-study phase, participants answered questions about their
experience with the tested DCG captchas. This comprised the stan-
dard SUS (Simple Usability Scale) questions [8], a standard 10-item
5-point Likert scale (‘1’ represents “Strong disagreement” and ‘5’
represents “Strong agreement”). SUS polls satisfaction with respect
to computer systems [6], in order to assess their usability. Addition-
ally, we asked several other questions related to the games’ usability.
In the actual study phase, each participant played 20 instances as
discussed in Section 3, aimed at understanding how different param-
eterizations impact users’ solving capabilities. The order of games
presented to different participants involved a standard 20x20 Latin
Square design to counter-balance learning effects. Via our study, our
goal was to assess the following aspects of the DCG captchas:
1. Efﬁciency: time taken to complete each game.
2. Robustness: likelihood of not completing the game, and of in-
correct drag and drop attempts.
3. Effect of Game Parameters: the effect of the object speed and
number on completion time and error rates.
4. User Experience: participants’ SUS ratings and qualitative feed-
back about their experience with the games.
For each tested game, completion times, and errors were automat-
ically logged by our web-interface software.
4.2 Study Results
We now provide the results from our usability study, including
time to completion and error rates, as well as perceived qualitative
aspects of the methods based on user ratings.
Completion Time: Table 1 shows the completion time per game
type. Clearly, all games turned out to be quite fast, lasting for less
than 10s on an average. Users took longest to solve the Animals
game with an average time of 9.10s, whereas the other games took
almost half of this time. This might have been due to increased se-
mantic load on the users in the Animals game to identify three target
objects and then match them with the corresponding answer objects.
Moreover, we noticed a decrease in the solving time (equal to 3.84s)
when the target objects were decreased to 2 (i.e., in the Shapes game),
and this time was comparable to games which had 1 target object in
the challenge (Ships and Parking). A one-way repeated-measures
ANOVA test showed signiﬁcant difference (at 95% conﬁdence) in
the mean timings of all 4 types of games (p < 0.0001, F = 79.98).
Aalyzing further using pairwise paired t-tests with Bonferroni cor-
rection, we found signiﬁcant difference between the mean times of
following pairs: Animals and Parking (p < 0.001), Ships and Shapes
(p < 0.0005), Animals and Ships (p < 0.001), Animals and Shapes
(p < 0.001), and Parking and Shapes (p = 0.0024).
Table 1: Error rates per click and completion time per game type
Game
Type
Ships
Animals
Parking
Shapes
Error Rate Per Click
mean
0.04
0.05
0.09
0.03
Completion Time (s)
mean (std dev)
4.51 (1.00)
9.10 (0.96)
4.37 (0.90)
5.26 (0.59)
Error Rates: An important result is that all the tested games yielded
100% accuracy (overall error rate of 0%). In other words, none of
the participant failed to complete any of the games within the time
out. This suggests our DCG captchas instances are quite robust to
human errors.
Next, we calculated the likelihood of incorrect drag and drop at-
tempts (error rate per click). For example, in the Animals game, an
incorrect attempt would be to feed the monkey with a ﬂower instead
of a banana. We deﬁne the error rate per click as the number of in-
correct objects (from the pool of all foreground objects) dragged to
the target area divided by the total number of objects dragged and
dropped. The results are depicted in Table 1. We observe that the
Shape game yields the smallest average per click error rate of 3%.
This suggests that the visual matching task (as in the Shapes game)
is less error prone compared to the semantic matching task (as in the
other games). The game challenge which seemed most difﬁcult for
participants was the Parking game (average per click error rate 9%).
Since objects in this game are relatively small, participants may have
had some difﬁculty to identify them.
Effect of Object Speed and Number: Table 2 shows the perfor-
mance of the game captchas in terms of per click error rates and
completion time as per different object speeds. We can see that
the maximum number of per click errors were committed at 10 FPS
speed. Looking at the average timings, we ﬁnd that it took longest
to complete the games when the objects move at the fastest speed
of 40 FPS, while 20 FPS yielded fastest completion time followed
by 10 FPS. ANOVA test revealed statistical difference among the
mean completion time corresponding to three speeds (p = 0.0045,
F = 5.65). Further analyzing using the t-test with Bonferroni cor-
rection, we found statistical difference between the mean timing cor-
responding to the following pair of speeds only: 10 FPS and 20 FPS
(p = 0.0001).
Table 2: Error rates per click and completion time per object speeds
Object
Speed
10 FPS
20 FPS
40 FPS
Completion Time (s)
mean (std dev)
5.74 (2.11)
4.90 (2.22)
6.53 (2.87)
Error Rate Per Click
mean
0.06
0.05
0.04
Another aspect of the usability analysis included testing the effect
of increase in the number of objects (including noisy answer objects)
on the overall game performance. Table 3 summarizes the per click
error rates and completion time against different number of objects.
Here, we can see a clear pattern of increase, albeit very minor, in av-
erage completion time and average rate with increase in the number
of objects. This is intuitive because increasing the number of ob-
jects increases the cognitive load on the users which may slow down
the game play and introduce chances of errors. ANOVA test did not
indicate this difference to be signiﬁcant, however.
User Experience: Now, we analyze the data collected from the par-
ticipants during the post-study phase. The average SUS score came
out to be 73.88 (standard deviation = 6.94). Considering that the av-
erage SUS scores for user-friendly industrial software tends to hover
in the 60–70 range [21], the usability of our DCG game captcha in-
stances can be rated as high.
4
Table 3: Error rates per click & completion time per # of objects
Error Rate Per Click
# of Objects
mean
0.06
0.05
0.04
Completion Time (s)
mean (std dev)
6.58 (1.69)
5.30 (2.28)
4.90 (2.22)
6
5
4
In addition to SUS, we asked the participants a few 5-point Likert
scale questions about the usability of the games (‘1’ means “Strong
Disagreement”). Speciﬁcally, we asked if the games were “visu-
ally attractive” and “pleasurable,” and whether they would like to
use them in “practice.” Table 4, shows the corresponding average
Likert scores. We found that 47% percent participants felt that the
games were visually attractive and 45% said that it was pleasurable
to play the games. These numbers indicate the promising usabil-
ity exhibited by the games. We further inquired users if they noticed
change in speed or number of objects in the games. 27.5% noticed no
change (increase and/or decrease) in speed of objects, whereas only
22.5% noticed no change in number of objects (see Table 5). Thus,
the change in the number of objects and speed (within the limits we
tested) was noticeable by a large fraction of participants.
Table 4: User feedback on game attributes
Likert Score
mean (std dev)
3.18 (0.94)
3.33 (0.96)
Visually Attractive
Pleasurable
Attribute
Table 5: % of users noticing change in speed and number of objects
Object Speed
Moved faster
Moved slower
No change
Both slower and faster
Number of objects
Increased
Decreased
No change
Both increase and decrease
(%)
30
5
27.5
37.5
(%)
47.5
2.5
22.5
27.5
Summary of Usability Analysis: Our results suggest that the DCG
captcha representatives tested in this work offer very good usability,
resulting in short completion times (less than 10s), very low error
rates (0% per game completion, and less than 10% per drag and drop
attempt),4 and good user ratings. We found that increasing the object
speed and number is likely to degrade the game performance, but up
to 6 objects and up to 40 FPS speed yield a good level of usability.
Although our study was conducted with a relatively young partici-
pant pool, given the simplcity of the games (involving easy matching
and clicking tasks), the game performance would generally be in line
with these results, as shown by our parallel study with Mechanical
Turk participants [22].
5. AUTOMATED ATTACKS
Having validated, via our usability study, that it is quite easy for
the human users to play our DCG captcha instances, we next pro-
ceeded to determine how difﬁcult these games might be for the com-
puter programs. In this section, we present and evaluate the perfor-
mance of a fully automated framework that can solve DCG captcha
challenges based on image processing techniques and principles of
unsupervised learning. We start by considering random guessing at-
tacks and then demonstrate that our framework performs orders of
magnitude better than the former.
4When contrasted with many traditional captchas [9], these timings
are comparable but the accuracies are better.
5.1 Random Guessing Attack
An attacker given a DCG captcha challenge can always attempt to
perform a random guessing attack. Let us assume that the attacker
knows which game he is being challenged with as well as the location
of the target area (e.g., the blue region containing the target circle
and pentagon in the Shapes game) and the moving object area (e.g.,
the white region in the Shapes game within which the objects move).
Although determining the latter in a fully automated fashion is a non-
trivial problem (see our attack framework below), an attacker can
obtain this knowledge with the help of a human solver.
However, the attacker (bot) still requires knowledge of: (1) the
foreground objects (i.e., all the objects in the moving object area)
and (2) the target objects (i.e., the objects contained within the target
area). A randomized strategy that the attacker could adopt is to pick
a random location on the moving object area and drag/drop it to a
random location on the target area. More precisely, the attacker can
divide the moving object area and the target area into grids of reason-
able sizes so as to cover the sizes of foreground moving objects and
target objects. For example, the moving object area can be divided
into a 10 pixel x 10 pixel grid and target region can be divided into
a 3 pixel x 3 pixel grid (given that the target area size is roughly 3
times the object area size). If there are a total of r target objects, the
total number of possibilities in which the cells (possibly containing
the answer objects) on the object area can be dragged and dropped
to the cells on the target area are given by t = C(100, r) ∗ P (9, r).
This is equivalent to choosing r cells in the object area out of a total
of 100 cells, and then rearranging them on to 9 cells in the target area.
Thus, the probability of attacker success in solving the challenge in a
single attempt is 1/t. For the DCG captcha instances targeted in this
paper, r is 3, 2 and 1, resulting in the respective success probabilities
of 0.00000123%, 0.000281% and 0.1%. Each attempt corresponds
to r drag-and-drop events. Even if the attacker is allowed a limited
(3-4) number of attempts to solve the captcha, these probabilities are
still much lower than the target probabilities for a real-world captcha
system security (e.g., 0.6% as suggested by Zhu et al. [33]).
While this analysis suggests that such DCG captchas are not vul-
nerable to naive guessing attacks, the next step is to subject them to
more sophisticated, fully automated attacks, as we pursue below.
5.2 Our Automated Attack and Results
Our attack framework involves the following phases:
1. Learning the background image of the challenge and identify-
ing the foreground moving objects. A background is the can-
vas on which the foreground objects are rendered. The fore-
ground objects, for example, in the Ships game, as shown in
Figure 1(a), are bird, ship, monkey, and squirrel.
2. Identifying the target area and the target area center(s). For
example, the sea in the Ships game, and the animals in the
Animals game.
3. Identifying and learning the correct answer objects. For exam-
ple, the ships in the Ships game.
4. Building a dictionary of answer objects and corresponding tar-
gets, the background image, the target area and their visual
features, and later using this knowledge base to attack the new
challenges of the same game.
5. Continuously learning from new challenges containing previ-
ously unseen objects.
Next, we elaborate upon our design and matlab-based implemen-
tation per each attack phase as well as our experimental results. We
note that, on a web forum [1], the author claims to have developed an
attack against the “are you a human” captcha. However, unlike our
generalized framework, this method is perfected for only one simple
game that has one single target area and a ﬁxed set of answer objects.
It is not known whether or how easily this method can be adapted to
5