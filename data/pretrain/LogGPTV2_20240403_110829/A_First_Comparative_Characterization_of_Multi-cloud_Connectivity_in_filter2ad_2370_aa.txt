title:A First Comparative Characterization of Multi-cloud Connectivity in
Today's Internet
author:Bahador Yeganeh and
Ramakrishnan Durairajan and
Reza Rejaie and
Walter Willinger
A First Comparative Characterization
of Multi-cloud Connectivity in Today’s
Internet
Bahador Yeganeh1(B), Ramakrishnan Durairajan1, Reza Rejaie1,
and Walter Willinger2
1 University of Oregon, Eugene, USA
{byeganeh,ram,reza}@cs.uoregon.edu
2 NIKSUN Inc., Boston, USA
PI:EMAIL
Abstract. Today’s enterprises are adopting multi-cloud strategies at
an unprecedented pace. Here, a multi-cloud strategy speciﬁes end-to-
end connectivity between the multiple cloud providers (CPs) that an
enterprise relies on to run its business. This adoption is fueled by the
rapid build-out of global-scale private backbones by the large CPs, a
rich private peering fabric that interconnects them, and the emergence of
new third-party private connectivity providers (e.g., DataPipe, HopOne,
etc.). However, little is known about the performance aspects, routing
issues, and topological features associated with currently available multi-
cloud connectivity options. To shed light on the tradeoﬀs between these
available connectivity options, we take a cloud-to-cloud perspective and
present in this paper the results of a cloud-centric measurement study
of a coast-to-coast multi-cloud deployment that a typical modern enter-
prise located in the US may adopt. We deploy VMs in two regions (i.e.,
VA and CA) of each one of three large cloud providers (i.e., AWS,
Azure, and GCP) and connect them using three diﬀerent options: (i)
transit provider-based best-eﬀort public Internet (BEP), (ii) third-party
provider-based private (TPP) connectivity, and (iii) CP-based private
(CPP) connectivity. By performing active measurements in this real-
world multi-cloud deployment, we provide new insights into variability
in the performance of TPP, the stability in performance and topology of
CPP, and the absence of transit providers for CPP.
1 Introduction
Modern enterprises are adopting multi-cloud strategies at a rapid pace. Deﬁned
here as end-to-end connectivity between multiple cloud providers (CPs)1, multi-
cloud strategies are critical for supporting distributed applications such as geo-
distributed analytics [33,35,57,68,69] and distributed genome sequencing studies
1 This is diﬀerent from hybrid cloud computing, where a direct connection exists
between a public cloud and private on-premises enterprise server(s).
c(cid:2) Springer Nature Switzerland AG 2020
A. Sperotto et al. (Eds.): PAM 2020, LNCS 12048, pp. 193–210, 2020.
https://doi.org/10.1007/978-3-030-44081-7_12
194
B. Yeganeh et al.
at universities [12,25]. Other beneﬁts that result from pursuing such strategies
are competitive pricing, vendor lockout, global reach, and requirements for data
sovereignty. According to a recent industry report, more than 85% of enterprises
have already adopted multi-cloud strategies [39].
Fueled by the deployment of multi-cloud strategies, we are witnessing two
new trends in Internet connectivity. First (see Fig. 1(bottom)), there is the emer-
gence of new Internet players in the form of third-party private connectivity
providers (e.g., DataPipe, HopOne, among others [5,29,51]). These entities oﬀer
direct, secure, private, layer 3 connectivity between CPs (henceforth referred to
as third-party private or TPP), at a cost of a few hundreds of dollars per month2.
TPP routes bypass the public Internet at Cloud Exchanges [19,21,71] where they
operate virtualized routers allowing their customers to form virtualized peering
sessions with the participating CPs and oﬀer additional beneﬁts to users (e.g.,
enterprise networks can connect to CPs without owning an Autonomous Sys-
tem Number, or ASN, or physical infrastructure). Second (see Fig. 1(top)), the
large CPs are aggressively expanding the footprint of their serving infrastruc-
tures, including the number of direct connect locations where enterprises can
reach the cloud via direct, private connectivity (henceforth referred to as cloud-
provider private or CPP) using either new CP-speciﬁc interconnection services
(e.g., [4,28,50]) or third-party private connectivity providers at colocation facil-
ities. Of course, as shown in Fig. 1 (middle), a multi-cloud user can forgo the
TPP and CPP options altogether and rely instead on the traditional, best eﬀort
connectivity over the public Internet—henceforth referred to as (transit provider-
based) best-eﬀort public (Internet) (BEP). In terms of routing, CPP and BEP
connectivity is oﬀered through default route conﬁgurations while TPP routes
are enforced via BGP conﬁgurations that customers of the TPP network install
on their virtual routers.
Cloud-provider  private (CPP) backbone
Best-effort public (BEP) Internet
Transit 
provider  1
Transit 
provider  2
Transit 
provider  N
Third-party  private (TPP) backbone
Enterprise
Network
CP 1
CP2
Cloud exchange
Private peering
Cloud router  (CR)
Virtual  Machine
Router
Fig. 1. Overview of three diﬀerent multi-cloud strategies. Sample end-to-end measure-
ment paths highlighted using thicker solid, dashed, and dotted lines for CPP, TPP,
and BEP options.
2 See Sect. 3.4 for more details.
A First Comparative Characterization of Multi-cloud Connectivity
195
With multi-cloud connectivity being the main focus of this paper, we note that
existing measurement techniques are a poor match in this context. For one, they
fall short of providing the data needed to infer the type of connectivity (i.e.,
TPP, CPP, and BEP) between (two or more) participating CPs. Second, they are
largely incapable of providing the visibility needed to study the topological prop-
erties, performance diﬀerences, or routing strategies associated with diﬀerent
connectivity options. Third, while mapping the connectivity from cloud/content
providers to users has been considered in prior work (e.g., [9,15–17,20,60] and
references therein), multi-cloud connectivity from a cloud-to-cloud (C2C) per-
spective has remained largely unexplored to date.
This paper aims to empirically examine the diﬀerent types of multi-cloud
connectivity options that are available in today’s Internet and investigate their
performance characteristics using non-proprietary cloud-centric, active measure-
ments. In the process, we are also interested in attributing the observed charac-
teristics to aspects related to connectivity, routing strategy, or the presence of
any performance bottlenecks. To study multi-cloud connectivity from a C2C per-
spective, we deploy and interconnect VMs hosted within and across two diﬀerent
geographic regions or availability zones (i.e., CA and VA) of three large cloud
providers (i.e., Amazon Web Services (AWS), Google Cloud Platform (GCP)
and Microsoft Azure) using the TPP, CPP, and BEP option, respectively.
Using this experimental setup as a starting point, we ﬁrst compare the stabil-
ity and/or variability in performance across the three connectivity options using
metrics such as delay, throughput, and loss rate over time. We ﬁnd that CPP
routes exhibit lower latency and are more stable when compared to BEP and
TPP routes. CPP routes also have higher throughput and exhibit less variation
compared to the other two options. Given that using the TPP option is expen-
sive, this ﬁnding is puzzling. In our attempt to explain this observation, we ﬁnd
that inconsistencies in performance characteristics are caused by several factors
including border routers, queuing delays, and higher loss-rates of TPP routes.
Moreover, we attribute the CPP routes’ overall superior performance to the fact
that each of the CPs has a private optical backbone, there exists rich inter-CP
connectivity, and that the CPs’ traﬃc always bypasses (i.e., is invisible to) BEP
transits. In summary, this paper makes the following contributions:
• To the best of our knowledge, this is one of the ﬁrst eﬀorts to perform a
comparative characterization of multi-cloud connectivity in today’s Internet.
To facilitate independent validation of our results, we will release all relevant
datasets [1] (properly anonymized; e.g., with all TPP-related information
removed).
• We identify issues, diﬀerences, and tradeoﬀs associated with three popular
multi-cloud connectivity options and elucidate/discuss the underlying rea-
sons. Our results highlight the critical need for open measurement platforms
and more transparency by the multi-cloud connectivity providers.
196
B. Yeganeh et al.
2 Background and Related Work
Measuring and understanding the connectivity ecosystem of the Internet has
been the subject of a large number of studies over the years [52, and references
therein]. Eﬀorts include mapping the (logical) connectivity of the public Internet
at the router level (e.g., [10,11,13,44,64]), the POP-level (e.g., [62,63,65]), and
the Autonomous System or AS-level (e.g., [45,73]). Other eﬀorts have focused
on issues such as the rise of Internet Exchange Points (IXPs) and their eﬀects on
inaccuracies of network-layer mapping (e.g., [2,11]), the “ﬂattening” of the Inter-
net’s peering structure (e.g., [22,27,40]), and the Internet’s physical infrastruc-
ture (building repositories of point of presence (POP), colocation, and datacen-
ter locations (e.g., [37,61]), the long-haul and metro connectivity between them
(e.g., [23,24,38]), and interconnections with other networks (e.g., [3,43,46]).
More recently, enterprise networks have been able to establish direct connec-
tivity to cloud providers—even without owning an AS number—at Open Cloud
Exchanges [19,21] (shown in the red box in Fig. 1) via a new type of interconnec-
tion service oﬀering called virtual private interconnections [71]. With the advent
of such interconnection services, today’s large cloud (and content) providers
(e.g., Google, Facebook, Microsoft) have experienced enormous growth in both
their ingress (i.e., Internet-facing) and mid-gress (i.e., inter-datacenter) traﬃc.
To meet these demands, they are not only aggressively expanding their pres-
ence at new colocation facilities but are also simultaneously building out their
own private optical backbones [26,36] (see CPP in Fig. 1). In addition, con-
nectivity to the CPs at colocation facilities are also available via third-party
providers [5,29,51] (TPP in Fig. 1) for additional costs (e.g., thousands of dol-
lars for a single, dedicated, private link to CP).
While measuring the peering locations, serving infrastructures and routing
strategies of the large content providers has been an active area of research [9,15–
17,20,60,70] and comparing the performance of CPs and their BEP properties
has been the focus of prior eﬀorts [14,18,30,41,74], to the best of our knowledge,
ours is one of the ﬁrst studies to (a) examine and characterize the TPP, CPP,
and BEP connectivity options from a C2C perspective, and (b) elucidate their
performance tradeoﬀs and routing issues.
3 Measurement Methodology
In this section, we describe our measurement methodology to examine the various
multi-cloud connectivity options, the cloud providers under consideration, and
the performance metrics of interest.
3.1 Measurement Setting
As shown in Fig. 1, we explore three diﬀerent types of multi-cloud connectivity
options: TPP connectivity between CP VMs that bypasses the public Internet,
A First Comparative Characterization of Multi-cloud Connectivity
197
CPP connectivity enabled by private peering between the CPs, and BEP con-
nectivity via transit providers. To establish TPPs, we deploy cloud routers via
a third-party connectivity provider’s network. At a high level, this step involves
(i) establishing a virtual circuit between the CP and a connectivity partner,
(ii) establishing a BGP peering session between the CP’s border routers and
the partner’s cloud router, (iii) connecting the virtual private cloud gateway to
the CP’s border routers, and (iv) conﬁguring each cloud instance to route any
traﬃc destined to the overlay network towards the conﬁgured virtual gateway.
To establish CPP connectivity, participating CPs automatically select private
peering locations to stitch the multi-cloud VMs together. Finally, we have two
measurement settings for BEP. The ﬁrst setting is between a non-native coloca-
tion facility in Phoenix AZ and our VMs through the BEP Internet; the second
form of measurement is through the BEP Internet towards Looking Glasses
(LGs) residing in the colocation facility hosting our cloud routers.
We conduct our measurements in a series of rounds. Each round consists of
path, latency, and throughput measurements between all pairs of VMs (in both
directions to account for route asymmetry). Furthermore, the measurements
are performed over the public BEPs as well as the two private options (i.e.,
CPP and TPP). Each connectivity path is enforced by the target address for
our measurements (i.e., public IP address for BEP and CPP paths and private
IPs VM instances in the TPP case). We avoid cross-measurement interference
by tracking the current state of ongoing measurements and limit measurement
activities to one active measurement per cloud VM.
3.2 Measurement Scenario and Cloud Providers
For this study, we empirically measure and examine one coast-to-coast, multi-
cloud deployment in the US. Our study focuses on connectivity between three
major CPs (AWS, Azure, and GCP) as they collectively have a signiﬁcant mar-
ket share and are used by many clients concurrently [72]. Using these CPs, we
create a realistic multi-cloud scenario by deploying two cloud routers using one
of the top third-party connectivity providers’ networks; one of the cloud routers
is in the Santa Clara, CA region, and one is in the Ashburn, VA region. These
cloud routers are interconnected with native cloud VMs from the three CPs.
The cloud VMs are all connected to cloud routers with 50 Mb/s links. We select
the colocation facility hosting the cloud routers based on two criteria: (i) CPs
oﬀer native cloud connectivity within that colo, and (ii) geo-proximity to the
target CPs datacenters. Cloud routers are interconnected with each other using
a 150 Mb/s link capacity that supports the maximum number of concurrent mea-
surements that we perform (i.e., 3 concurrent measurements in total to avoid
more than 1 ongoing measurement per VM). Each cloud VM has at least 2 vCPU
cores, 4 GB of memory, and runs Ubuntu server 18.04 LTS. Our VMs were pur-
posefully over-provisioned to reduce any measurement noise within virtualized
environments. Throughout our measurement experiments, the VMs CPU uti-
lization always remained below 2%. We also cap the VM interfaces at 50 Mb/s
to have a consistent measurement setting for both public (BEP) and private
198
B. Yeganeh et al.
(TPP and CPP) routes. We perform measurements between all CP VMs within
regions (intra-region) and across regions (inter-region). Additionally, we also per-
form measurements between our cloud VMs and two LGs that are located within
the same facility as our cloud routers in California and Virginia, respectively,
and use these measurements as baselines for BEP3 comparisons.
3.3 Data Collection and Performance Metrics
We conducted our measurements for about a month-long period in the Spring
of 2019. The measurements were conducted in 10-min rounds. In each round,
we performed latency, path, and throughput measurements between all pairs of
relevant nodes. For each round, we measure and report the latency using 10 ping
probes paced in 1 s intervals. We refrain from using a more accurate one-way
latency measurement tool such as OWAMP as the authors of OWAMP cau-
tion its use within virtualized environments [34]. Similarly, paths are measured
by performing 10 attempts of paris-traceroute using scamper [42] towards each
destination. We used ICMP probes for path discovery as they maximized the
number of responsive hops along the forward path. Lastly, throughput is mea-
sured using the iperf3 tool, which was conﬁgured to transmit data over a 10-s
interval using TCP. We discard the ﬁrst 5 s of our throughput measurement to
account for TCP’s slow-start phase and consider the median of throughput for
the remaining 5 s. These eﬀorts resulted in about 48k samples of latency, path,
and throughput measurements between each unique src/dst pair and connectiv-
ity option.