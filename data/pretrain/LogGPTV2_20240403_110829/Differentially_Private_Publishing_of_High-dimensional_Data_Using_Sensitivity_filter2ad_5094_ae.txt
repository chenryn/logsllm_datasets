0.201
0.844
0.160
0.140
DPS DPSS
0.992
0.991
0.985
0.980
0.972
0.971
1.000
1.000
0.992
0.988
0.995
0.995
0.920
0.954
0.940
0.928
0.925
0.918
0.999
0.998
0.996
0.995
0.991
0.990
0.971
0.954
0.944
0.939
0.914
0.894
0.851
0.799
0.916
0.914
0.831
0.809
Table 3: Top-k accuracy for different approaches on datasets.
advantage of sensitivity control makes our DPSense algorithm ro-
bust on predicting top-k columns.
From the previous section, we have seen that DPSense-S can
improve DPSense in top-5% columns. The experimental result
here also demonstrates the advantage of DPSense-S, especially
when  = ln 1.5. Although the scaling up step doesn’t affect the
ranking of top-k columns, DPSense-S tends to choose a smaller
θ than DPSense does and later correct the value by α, in which
the smaller θ leads to less noise and improves the top-k accuracy
and NDCG.
GS performs reasonably well; however, for some situations it
is much worse than DPSense. Since GS divides columns into
groups and publishes the same noisy count for all columns in a
group, its accuracy depends very much on how well the group size
lines up with the k value for top-k columns. If k is close to the
group size, and sampling one cell from each row preserves the or-
dering of columns well, then GS would perform reasonably well.
The results of top-k accuracy and NDCG from different approaches
are similar. The result of NDCG is closer to optimal than top-
k accuracy, mainly because NDCG takes rank into account and
also handles cases where a column is bumped out of the top-k by
another column with the same true count, which doesn’t incur a
penalty.
5.5 Running Time of DPSense and DPSense-S
We evaluate the running time of DPSense and DPSense-S us-
ing MATLAB R2013a on a machine with Intel Xeon CPU E5-2670
2.60GHz and 32GB RAM. Table 5 shows the result of both ap-
proaches on different datasets. Overall, DPSense takes just few
seconds to run, while DPSense-S requires time from few seconds
to minutes, depending on the datasets. DPSense-S spends most
of the time on computing the quality of different candidates, due to
the fact that it requires one scan for each (θ, α) pair when comput-
ing the pair’s quality. DPSense needs only one scan of dataset to
compute quality of all θs, thus runs much faster than DPSense-
S. The result also shows our proposed algorithms are practical and
applicable for real world datasets.
Dataset
Netﬂix
Transaction
Movielens
Document
AOL
Kosarak
DPSense DPSense-S
1211.59
2.82
178.99
1946.13
298.95
136.47
2.94
0.34
0.45
52.86
1.52
1.04
Table 5: Running time (in seconds) of DPSense and DPSense-S
on different datasets.
6. RELATED WORK
The notion of differential privacy was introduced in [4, 8, 7].
The method of adding Laplacian noise scaled with the sensitivity,
which we use to generate noisy counts, is proposed in [8]. The
exponential mechanism was introduced in [20].
Our work is directly motivated by Kellaris et al.
[15], which
studies the same problem as our paper does. We have presented
their approach in detail in Section 3.1 and experimentally compared
with their approach. Kellaris et al. address the large-sensitivity
challenge by publishing one count for a group of similar columns.
To decide which columns are similar, they sample one non-zero
cell from each row to reduce sensitivity. Our key contribution is a
technique to choose an optimal threshold for limiting contributions.
We also note that the approach in [15] is based on the assumption
that ∆D is public.
In [16], Korolova et al. also used the idea to limit the contribu-
tion of each row to reduce sensitivity in the context of publishing
search log information. The threshold for limiting user’s activity, d
in their paper, is manually set from users. [3, 14] discuss the the-
oretical effect of sensitivity control by projection of a graph in the
problem of publishing graph information. In [9] Gotz et al. dis-
cuss the effect of choosing the number of contribution per user by
testing different θ values. The authors argue that choosing θ ap-
proximately to the average number of items contributed by each
user has the best result. In [21], Proserpio et al. presented the idea
460Dataset
Netﬂix
Transaction
Movielens
Document
AOL
Kosarak
k
LPA
1% 0.187
5% 0.299
10% 0.337
1% 0.998
5% 0.997
10% 0.996
1% 0.030
5% 0.086
10% 0.146
1% 0.999
5% 0.996
10% 0.971
1% 0.027
5% 0.057
10% 0.102
1% 0.214
5% 0.130
10% 0.156
FPA
0.000
0.048
0.091
0.418
0.529
0.675
0.093
0.182
0.207
0.019
0.046
0.088
0.064
0.326
0.350
0.017
0.095
0.161
 = ln 1.5
GS
0.912
0.909
0.831
0.446
0.647
0.887
0.879
0.825
0.730
0.959
0.994
0.994
0.389
0.547
0.645
0.597
0.860
0.872
EM
0.999
0.997
0.954
0.927
0.999
0.997
0.998
0.922
0.633
0.998
0.553
0.319
0.965
0.245
0.167
0.707
0.143
0.132
DPS DPSS
0.997
0.993
0.997
0.994
0.992
0.991
0.999
0.998
0.999
0.998
0.999
0.999
0.976
0.982
0.979
0.978
0.961
0.951
0.999
0.999
0.999
0.999
0.998
0.998
0.988
0.987
0.977
0.975
0.944
0.935
0.934
0.927
0.955
0.953
0.891
0.881
LPA
0.876
0.787
0.696
0.999
0.999
0.999
0.207
0.273
0.312
0.999
0.999
0.996
0.097
0.089
0.127
0.415
0.247
0.247
FPA
0.000
0.048
0.091
0.418
0.529
0.675
0.093
0.182
0.206
0.017
0.046
0.088
0.064
0.326
0.350
0.017
0.098
0.160
 = ln 3
GS
0.938
0.951
0.925
0.446
0.646
0.885
0.911
0.922
0.852
0.958
0.995
0.995
0.389
0.547
0.645
0.587
0.836
0.913
EM
0.914
0.999
0.995
0.725
0.997
0.999
0.999
0.991
0.902
0.999
0.828
0.521