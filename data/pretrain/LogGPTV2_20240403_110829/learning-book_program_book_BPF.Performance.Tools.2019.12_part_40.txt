+ perf record -e ext4:ext4_ds_write_begin -a
C[ perf recozd: Noken up l tines to wvzite data ]
[perf cecord: Captured and vrote 1376.293 MB perf.data (14394798 sanples) )
Well, this is embarrassing, but it’s an important lesson for file system tracing. Because perf record
will write events to the file system, if you trace file system (or disk) writes you can create a feed-
back loop, as I just did here, resulting in 14 million samples and a 1.3 Gbyte perf.data file!
The format string for this example looks like this:
1 perf script
[• - -]
pezf 26768 [005] 275068.339717: ext4:ext4_da_wz1te_begin: dev 253,1 1no 1967479 pos
5260704 1en 192 flags 0
s0d 621961 9T 1*25z a9p ut6899a1x8ex911x9 1c26c*8905cz Is001 89192 1x8d
5260896 len 8 flags 0
5260904 len 192 flags 0
pexf 26768 [005] 275068.339729: ext4:ext4_da_wz1te_begin: dev 253,1 1no 1967479 pos
pexf 26768 [005] 275068 339735: ext41ext4_da_sz1ta_begin: dev 253, 1 Ino 1967479 pos
5261096 len 8 fLags 0
[.--]
The format string (one has been highlighted in bold) includes the device, inode, position, length,
and flags for the write.
File systems may support many tracepoints, or some, or none. XFS, for example, has around 500. If your
file system does not have tracepoints, you can try to instrument its internals using kprobes instead.
For comparison with later BPF tools, consider the same tracepoint instrumented using bpftrace to
summarize the length argument as a histogram:
+ bpftrace -e *tracepoint:ext4:ext4_da_write_begin (  = hist(args->len) : }*
Attaching l pzobe
7
---
## Page 323
286
Chapter S File Systems
9:
[16, 32)
26 188988988
[32, 64)
418
[64, 128]
27188e88e88
[128, 256}
1518898
[256, 512]
101889
[512, 1K]
0 1
[18, 2K]
/
[2K, 4K)
201889889
[4K, 8X]
16  eee8 e8e8e8ee8e8e88 e8e8ee8ee8eeee 1
This shows that most of the lengths were between four and eight Kbytes. This summary is
performed in kernel context, and does not require writing a perf.data file to the file system. This
avoids not only the overhead of those writes and additional overhead to post-process, but also the
risk of a feedback loop.
8.2.5 fatrace
fatrace(1) is a specialized tracer that uses the Linux fanotify API (fle access notify). Example
output:
+ fatrace
cron (4794) : CX /tnp/#9346 (de1eted)
cron (4794) : R0 /etc/login.defs
czon (4794) : RC /etc/1ogin,defs
rsyslogd(872): M /rar/log/auth. 1og
sshd (7553) : 0 /etc/notd
sshd (7553) : R /etc/motd
sshd (7553) : C /etc/notd
[...]
Each line shows the process name, PID,type of event, full path, and optional status. The type of
event can be opens (O), reads (R), writes (W), and closes (C). fatrace(1) can be used for workload
characterization: understanding the files accessed, and looking for unnecessary work that could
be eliminated.
However, for a busy file system workload, fatrace(1) can produce tens of thousands of lines of
output every second, and can cost significant CPU resources. This may be alleviated somewhat by
filtering to one type of event, for example, opens only:
+ fatrace -f 0
run (6383) : 0 /bln/s1eep
run (6383) : R0 /1ib/x86_641inux=gnu/1a2 .27-s0
sleep (6383) : 0 /etc/1d.so.cache
sleep (6383) : R0 /1ib/xB6_641inux=gnu./libc2 . 27 ,so
[..-]
---
## Page 324
8.3  BPF Tools
287
In the following BPF section, a dedicated BPF tool is provided for this: opensnoop(8), which
provides more command line options and is also much more efficient. Comparing the CPU
overhead of fatrace f 0 vs BCC opensnoop(8) for the same heavy file system workload:
 pidstat 10
[..]
K 518:60
UID
PID
Susx lsystem
5guest
5xait
ICPO
CPO
Cosnand
09:39:04FM
06075
11.19
56,44
0.00
0 .20
67,63
1
fatrace
[..-]
09 :50 : 32 PM
07079
0. 90
0.20
0. 00
0 . 00
1.10
[..-]
opensnoop(8) is consuming 1.1% CPU vs fatrace(1)s 67%.3
8.3
BPF Tools
This section covers the BPF tools you can use for file system performance analysis and
troubleshooting (see Figure 8-3).
Applications
statanoop
scread
Lostat
System Call Interface
VFS
Flle Systems
vriteback
Volume Manager
Rest of Kernel
ver xfedist
Block Device
Device Drivers
Figure 8-3 BPF tools for fle system analysis
These tools are either from the BCC and bpftrace repositories (covered in Chapters 4 and 5), or
were created for this book. Some tools appear in both BCC and bpftrace. Table 8-2 lists the origins
of the tools covered in this section (BT is short for bpftrace).
3 This is running BCC opensnoop(8) ss-is. By tuning the polling loop (inse
to take the overhesd down to 0.6%.
---
## Page 325
288
Chapter S File Systems
Table 8-2 File System-Related Tools
Tool
Source
Description
doousuedo
BCC/BT
Syscalls
Trace files opened
statsnoop
BCC/BT
Syscalls
Trace calls to stat(2) varieties
syncsnoop
BCC/BT
Syscalls
Trace sync(2) and variety calls with timestamps
mmapf11es
Book
Syscalls
Count mmap(2) files
scread
Book
Syscalls
Count read(2) files
tnegdewg
Book
Page cache
Count file map faults
filelife
BCC/book
VFS
Trace short-lived files with their Iifespan in seconds
vfsstat
BCC/BT
VFS
Common VFS operation statistics
vfscount
BCC/BT
VFS
Count all VFS operations
vfssize
Book
VFS
Show VFS read/write sizes
fsrvstat
Book
VFS
Show VFS reads/writes by file system type
fileslover
BCC/book
VFS
Show slow file reads/writes
f1letop
BCC
VFS
Top fles in use by IOPS and bytes
f1letype
Book
VFS
Show VFS reads/writes by file type and process
vritesyne
Book
VFS
Show regular file writes by sync flag
cachestat
BCC
Page cache
Page cache statistics
wr1teback
BT
Page cache
Show write-back events and latencies
dcstat
BCC/book
 Dcache
Directory cache hit statistics
dcsnoop
BCC/BT
Dcache
Trace directory cache lookups
mountsnoop
BCC
VFS
Trace mount and umounts system-wide
xfsslover
BCC
XFS
Show slow XFS operations
xfsdist
BCC
XFS
Common XFS operation latency histograms
ext4dist
BCC/book
ext4
Common ext4 operation latency histograms
icstat
Book
Icache
 Inode cache hit statistics
bufgrov
Book
Buffer cache
Buffer cache growth by process and bytes
peettpte.
Book
VFS
Show read ahead hits and efficiency
For the tools from BCC and bpftrace, see their repositories for full and updated lists of tool
options and capabilities. A selection of the most important capabilities are summarized here
The following tool summaries include a discussion on translating file descriptors to filenames
(see scread(8)).
---
## Page 326
8.3 BPF Tools
289
8.3.1opensnoop
opensnoop(8)* was shown in Chapters 1 and 4, and is provided by BCC and bpftrace. It traces
file opens and is useful for discovering the location of data files, log files, and configuration
files. It can also discover performance problems caused by frequent opens, or help troubleshoot
timestamps:
issues caused by missing files. Example output from a production system, with  to incluxde
I- doousuado 
TIHE (s)
PID
COXH
FD ERR PATH
0, 000000000
3862
java
5248
0 /proc/Loadsvg
0 , 000036000
3862
Java
5248
0 /ays/fs/cgroup/cpu, cpuacct/..-/cpu.cfs_quota_us
0., 00051000
3862
java
5248
0/sys/fs/cgroup/cpu, cpuacct/../cpu.cfs_period_us
0, 000059000
3862
Java
5248
0 /ays/fs/cgroup/cpu, cpuacct/..-/cpu.shares
0, 012956000
3862
java
5248
0/proc/loadavg
0, 012995000
3862
 Java
524 8
0/sys/fs/cgroup/cpu, cpuacct/..-/cpu.cfs_quota_us
0, 013012000
3862
java
524 8
0/sys/fs/cgroup/cpu, cpuacct/../cpu.cfs_period_us
0,013020000
3862
Java
524 8
0/ays/fs/cgroup/cpu, cpuacct/..-/cpu.shares
0, 021259000
3862
Java
5248
0 /proe/Loadavg
0,021301000
3862
Java
5248
0/ays/fs/cgroup/cpu, cpuacct/..- /cpu.cfs_quota_us
0 , 021317000
3862
java
5248
0/sys/fs/cgroup/cpu, cpuacct/../cpu.cfs
0, 021325000
3862
Java
5248
0/ays/fs/cgroup/cpu, cpuacct/.-- /cpu.shares
0, 022079000
3862
java
5248
 0/proc/loadavg
[.--]
The output rate was high, and shows that a group of four files are read at a rate of one hundred
times per second by Java (I just discovered this). The filename has been partially truncated in
this book to fit. These are in-memory files of system metrics, and reading them should be fast,
but does Java really need to read them one hundred times every second? My next step in analysis
was to fetch the stack responsible. Since these were the only file opens that this Java process was
:Susn CIld sq nog supodaoen uado au rog speis paqunoo 6[dus I 'Supuojuad
,4vuado"aqoasAs:sttesAs1, 79g d- qumooyo9qs
This showed the full stack trace, including the Java methods° responsible. The culprit turned out
to be new load balancing software.
peauaao at1. (z)4euado pue (z)uado :sleos/s pueμea (z)uado at Burpen Xq sxμom (s)doousuaxdo
is expected to be negligible as the open(2) rate is typically infrequent.
4 Origin: I created the first version as opensnoop.d on 9-May2004, It was simple, useful, and being able to see opens
system-wide was amazing. My prior approaches to achieve this had been to use truss(IM) on a single process only
or BSM auding, which required charging the state of the system. The name *snoop² comes from the Solaris network
snife, snoop(1M), and the terminology *snooping events.° opensnoop has since ben ported to mary other tracers, by
myself and others. I wrote the BCC ersion on 17-Sep-2015, and bpfrace on 8-Sep-2018.
5 I intended to run opensnoop on several production servers to find some interesting output to include here. I saw this
on the first one I tried.
6 See Chapter 18 for how to get Java stacks and symbols to work.
---
## Page 327
067
Chapter S File Systems
BCC
Command line usage:
[suoTsdo] deousuado
Options include:
▪x: Show only failed opens
p PID: Measure this process only
• -n NAME: Only show opens when the process name contains NAME
bpftrace
The following is the code for the bpftrace version, which summarizes its core functionality. This
version does not support options.
#1/usx/local/bin/bpEtrace
BEGIN
printf(*Txacing open syscalls... Hit Ctr1-C to end. (n*) ;
printf(*-fs 16s 4s 3s sn*, *pID*, *co", "FD”, "ERR#, *rATH*)
tracepoint:ayacalls:ays_enter_open,
tracepoint:syscalls1sys_enter_openst
1
Bfilenane [tid] = args->filenane,)
tracepoint:syscalls:sys_exit_open,
tracepoint:syscalls1sys_exit_openat
/8filenane [t1d] /
Sret = args=>ret;
$fd = $ret > 0 ? $ret : -1
Sezxno = Sret > 0 ? 0 : - Szet,
printf (*4-6d 116s 54d s3d lsn", pld, conn, $fd, $ezxno,
str(9filename|tid]) 
delete (@f1lenane [tid]);
END
clear (8filenane) 
---
## Page 328
8.3 BPF Tools
291
This program traces open(2) and openat(2) syscalls, and teases apart the file descriptor or error
number from the return value. The filename is cached on the entry probe so that it can be fetched
and printed on syscall exit, along with the return value.
8.3.2 statsnoop