### Detection Success Rate of CW Attack at 5% FPR
**Figure 10: Detection success rate of the CW attack at 5% FPR when using different layers for detection in a GTSRB model.**

### ResNet20 Model Architecture for CIFAR10
**Table 10: ResNet20 Model Architecture for CIFAR10.**

| Layer Name (type) | # of Channels | Activation | Connected to |
|-------------------|---------------|------------|--------------|
| conv_1 (Conv)     | 16            | ReLU       | conv_1       |
| conv_2 (Conv)     | 16            | ReLU       | pool_2       |
| conv_3 (Conv)     | 16            | ReLU       | conv_3       |
| conv_4 (Conv)     | 16            | ReLU       | conv_4       |
| conv_5 (Conv)     | 16            | ReLU       | conv_5       |
| conv_6 (Conv)     | 16            | ReLU       | conv_6       |
| conv_7 (Conv)     | 16            | ReLU       | conv_7       |
| conv_8 (Conv)     | 32            | ReLU       | conv_8       |
| conv_9 (Conv)     | 32            | ReLU       | conv_9       |
| conv_10 (Conv)    | 32            | ReLU       | conv_10      |
| conv_11 (Conv)    | 32            | ReLU       | conv_11      |
| conv_12 (Conv)    | 32            | ReLU       | conv_12      |
| conv_13 (Conv)    | 32            | ReLU       | conv_13      |
| conv_14 (Conv)    | 64            | ReLU       | conv_14      |
| conv_15 (Conv)    | 64            | ReLU       | conv_15      |
| conv_16 (Conv)    | 64            | ReLU       | conv_16      |
| conv_17 (Conv)    | 64            | ReLU       | conv_17      |
| conv_18 (Conv)    | 64            | ReLU       | conv_18      |
| conv_19 (Conv)    | 64            | ReLU       | conv_19      |
| conv_20 (Conv)    | 64            | ReLU       | conv_20      |
| conv_21 (Conv)    | -             | -          | conv_21      |
| pool_1 (AvgPool)  | -             | -          | pool_1       |
| dropout_1 (Dropout)| -             | -          | dropout_1    |
| fc_1 (FC)         | -             | Softmax    | fc_1         |

### Datasets and Tasks
- **Image Recognition (CIFAR10):** The task is to recognize 10 different objects. The dataset contains 50,000 colored training images and 10,000 testing images [23]. The model used is a Residual Neural Network (ResNet) with 20 residual blocks and one dense layer [20] (see Table 10). This task is included due to its prevalence in general image classification and adversarial defense literature.
- **Face Recognition (YouTube Face):** The task is to recognize faces of 1,283 different people from YouTube videos [51]. The dataset includes 375,645 training images and 64,150 testing images [13]. We use a large ResNet-50 architecture with over 25 million parameters. This task simulates a more complex facial recognition-based security screening scenario, and defending against adversarial attacks in this setting is crucial. Additionally, the large set of labels allows us to explore the scalability of our trapdoor-enabled detection.

### Model Architectures
- **MNIST (Table 8):** A convolutional neural network (CNN) consisting of two pairs of convolutional layers connected by max pooling layers, followed by two fully connected layers.
- **GTSRB (Table 9):** A CNN consisting of three pairs of convolutional layers connected by max pooling layers, followed by two fully connected layers.
- **CIFAR10 (Table 10):** A CNN with 21 sequential convolutional layers, followed by pooling, dropout, and fully connected layers.
- **YouTube Face:** A ResNet-50 model trained on the YouTube Face dataset, with 50 residual blocks and over 25 million parameters.

### Detailed Information on Attack Configuration
We evaluate the trapdoor-enabled detection using six adversarial attacks: CW, ElasticNet, PGD, BPDA, SPSA, and FGSM (described in Section 2.1). Details about the attack configuration are listed in Table 12.

**Table 12: Detailed information on attack configurations. For MNIST experiments, we divide the eps value by 255.**

| Attack Method | Attack Configuration |
|---------------|----------------------|
| CW            | Binary step size = 9, max iterations = 1000, learning rate = 0.05, abort early = True |
| PGD           | Max eps = 8, # of iteration = 100, eps of each iteration = 0.1 |
| ElasticNet    | Binary step size = 20, max iterations = 1000, learning rate = 0.5, abort early = True |
| BPDA          | Max eps = 8, # of iteration = 100, eps of each iteration = 0.1 |
| SPSA          | Eps = 8, # of iteration = 500, learning rate = 0.1 |
| FGSM          | Eps = 8 |

### Sample Trapdoor Patterns
**Figure 11: Sample trapdoor examples used in our defense.** While the actual trapdoors used all have a mask ratio of κ = 0.1, here we artificially increase κ from 0.1 to 1.0 to highlight the trapdoors from the rest of the image content.

- **(a) Single Label Defense Trapdoor**
- **(b) All Label Defense Trapdoor**

### Datasets and Defense Configuration
**Table 11: Detailed information on datasets and defense configurations for each trapdoored model when protecting all labels.**

| Model         | # of Labels | Training Set Size | Testing Set Size | Injection Ratio | Mask Ratio | Training Configuration |
|---------------|-------------|--------------------|------------------|-----------------|------------|------------------------|
| MNIST         | 10          | 50,000             | 10,000           | 0.5             | 0.1        | epochs=5, batch=32, optimizer=Adam, lr=0.001 |
| GTSRB         | 43          | 35,288             | 12,630           | 0.5             | 0.1        | epochs=30, batch=32, optimizer=Adam, lr=0.001 |
| CIFAR10       | 10          | 50,000             | 10,000           | 0.5             | 0.1        | epochs=60, batch=32, optimizer=Adam, lr=0.001 |
| YouTube Face  | 1,283       | 375,645            | 64,150           | 0.01            | 0.2        | epochs=30, batch=32, optimizer=Adam, lr=0.001 |

### Additional Results on Comparing Trapdoor and Adversarial Perturbation
**Figure 12: Comparison of cosine similarity of normal images and adversarial images to trapdoored inputs in a trapdoored CIFAR10 model and in an original (trapdoor-free) CIFAR10 model.** The boxes show the inter-quartile range, and the whiskers denote the 5th and 95th percentiles.

- **(a) Trapdoored Model**
- **(b) Original Model**

**Figure 13: Comparison of cosine similarity of normal images and adversarial images to trapdoored inputs in a trapdoored YouTube Face model and in an original (trapdoor-free) YouTube Face model.** The boxes show the inter-quartile range, and the whiskers denote the 5th and 95th percentiles.

- **(a) Trapdoored Model**
- **(b) Original Model**

### Selecting Trapdoor Injection Ratio
Our analysis shows that the size and diversity of the training data used to inject a trapdoor can affect its effectiveness in trapping attackers. We define the trapdoor injection ratio as the ratio between the trapdoored images and the clean images in the training dataset. Intuitively, a higher injection ratio should allow the model to learn the trapdoor better but could potentially degrade normal classification accuracy.

We defend the model with different trapdoor injection ratios and examine the detection success rate. We find that only when the injection ratio is very small (e.g., < 0.03 for GTSRB), the model fails to learn the trapdoor, and therefore, detection fails. Otherwise, the trapdoor is highly effective in detecting adversarial examples. Thus, when building the trapdoored models, we use an injection ratio of 0.1 for MNIST, GTSRB, and CIFAR10, and 0.01 for YouTube Face (see Table 12).