t
e
D
 1
 0.8
 0.6
 0.4
 0.2
 0
 2
 4
 6
 8
 10
 12
DNN Layer Number
Figure 10: Detection success rate of CW attack at 5% FPR when
using different layers for detection in a GTSRB model.
Table 10: ResNet20 Model Architecture for CIFAR10.
Layer Name (type)
# of Channels Activation Connected to
conv_1 (Conv)
conv_2 (Conv)
conv_3 (Conv)
conv_4 (Conv)
conv_5 (Conv)
conv_6 (Conv)
conv_7 (Conv)
conv_8 (Conv)
conv_9 (Conv)
conv_10 (Conv)
conv_11 (Conv)
conv_12 (Conv)
conv_13 (Conv)
conv_14 (Conv)
conv_15 (Conv)
conv_16 (Conv)
conv_17 (Conv)
conv_18 (Conv)
conv_19 (Conv)
conv_20 (Conv)
conv_21 (Conv)
pool_1 (AvgPool)
dropout_1 (Dropout)
fc_ (FC)
16
16
16
16
16
16
16
32
32
32
32
32
32
32
64
64
64
64
64
64
64
-
-
-
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
-
-
-
conv_1
pool_2
conv_3
conv_4
conv_5
conv_6
conv_7
conv_8
conv_9
conv_10
conv_11
conv_12
conv_13
conv_14
conv_15
conv_16
conv_17
conv_18
conv_19
conv_20
conv_21
pool_1
Softmax
dropout_1
as an adversarial defense evaluation benchmark and 2) represents
a real-world setting relevant to our defense.
• Image Recognition (CIFAR10) – The task is to recognize 10 dif-
ferent objects. The dataset contains 50K colored training images
and 10K testing images [23]. The model is an Residual Neural
Network (RNN) with 20 residual blocks and 1 dense layer [20] (Ta-
ble 10). We include this task because of its prevalence in general
image classiﬁcation and adversarial defense literature.
• Face Recognition (YouTube Face) – This task is to recognize
faces of 1, 283 different people drawn from the YouTube videos [51].
We build the dataset from [51] to include 1, 283 labels, 375.6K
training images, and 64.2K testing images [13]. We use a large
ResNet-50 architecture architecture [20] with over 25 million pa-
rameters. We include this task because it simulates a more com-
plex facial recognition-based security screening scenario. Defend-
ing against adversarial attack in this setting is important. Further-
more, the large set of labels in this task allows us to explore the
scalability of our trapdoor-enabled detection.
Model Architecture. We now present the architecture of DNN
models used in our work.
• MNIST (Table 8) is a convolutional neural network (CNN) con-
sisting of two pairs of convolutional layers connected by max
pooling layers, followed by two fully connected layers.
• GTSRB (Table 9) is a CNN consisting of three pairs of convolu-
tional layers connected by max pooling layers, followed by two
fully connected layers.
• CIFAR10 (Table 10) is also a CNN but includes 21 sequential
convolutional layers, followed by pooling, dropout, and fully con-
nected layers.
• YouTube Face is the ResNet-50 model trained on the YouTube
Face dataset. It has 50 residual blocks with over 25 millions pa-
rameters.
Detailed information on attack conﬁguration. We evaluate the
trapdoor-enabled detection using six adversarial attacks: CW, Elas-
ticNet, PGD, BPDA, SPSA, and FGSM (which we have described
in Section 2.1). Details about the attack conﬁguration are listed in
Table 12.
Sample Trapdoor Patterns. Figure 11 shows sample images that
contain a single-label defense trapdoor (a single 6 × 6 square) and
that contain an all-label defense trapdoor (ﬁve 3 × 3 squares). The
mask ratio of the trapdoors used in our experiments is ﬁxed to κ =
0.1.
Datasets and Defense Conﬁguration. Tablel 11 lists the speciﬁc
datasets and training process used to inject trapdoors into the four
DNN models.
Table 11: Detailed information on datasets and defense conﬁgurations for each trapdoored model when protecting all labels.
Model
MNIST
GTSRB
CIFAR10
#
of Labels
10
43
10
YouTube Face
1,283
Training
Set Size
50,000
35,288
50,000
375,645
Testing
Set Size
10,000
12,630
10,000
64,150
Injection Ratio Mask Ratio
Training Conﬁguration
0.5
0.5
0.5
0.5
0.1
0.1
0.1
0.2
epochs=5, batch=32, optimizer=Adam, lr=0.001
epochs=30, batch=32, optimizer=Adam, lr=0.001
epochs=60, batch=32, optimizer=Adam, lr=0.001
epochs=30, batch=32, optimizer=Adam, lr=0.001
Table 12: Detailed information on attack conﬁgurations. For MNIST experiments, we divid the eps value by 255.
Attack Method
Attack Conﬁguration
CW
PGD
ElasticNet
BPDA
SPSA
FGSM
binary step size = 9, max iterations = 1000, learning rate = 0.05, abort early = True
max eps = 8, # of iteration = 100, eps of each iteration = 0.1
binary step size = 20, max iterations = 1000, learning rate = 0.5, abort early = True
max eps = 8, # of iteration = 100, eps of each iteration = 0.1
eps = 8, # of iteration = 500, learning rate = 0.1
eps = 8
8.3 Additional Results on Comparing Trapdoor
and Adversarial Perturbation
Figure 12 and Figure 13 show that the neuron signatures of adver-
sarial inputs have high cosine similarity to the neuron signatures of
trapdoors in a trapdoored CIFAR10 and YouTube Face models (left
ﬁgures), and the trapdoor-free models (right ﬁgures).
(a) Single Label Defense Trapdoor
(b) All Label Defense Trapdoor
Figure 11: Sample trapdoor examples used in our defense. While
the actual trapdoors we used all have a mask ratio of κ = 0.1, here
we artiﬁcally increase κ from 0.1 to 1.0 in order to highlight the
trapdoors from the rest of the image content.
Selecting Trapdoor Injection Ratio. As mentioned earlier, our
analysis shows that the size and diversity of the training data used
to inject a trapdoor could affect its effectivess of trapping attackers.
To explore this factor, we deﬁne trapdoor injection ratio as the ratio
between the trapdoored images and the clean images in the training
dataset. Intuitively, a higher injection ratio should allow the model
to learn the trapdoor better but could potentially degrade normal
classiﬁcation accuracy.
We defend the model with different trapdoor injection ratios and
examine the detection success rate. We see that only when the injec-
tion ratio is very small (e.g. < 0.03 for GTSRB), the model fails to
learn the trapdoor and therefore detection fails. Otherwise the trap-
door is highly effective in terms of detecting adversarial examples.
Thus when building the trapdoored models, we use an injection ra-
tio of 0.1 for MNIST, GTSRB, CIFAR1010, and 0.01 for YouTube
Face (see Table 12).
y
t
i
r
a
l
i
i
m
S
e
n
s
o
C
i
 1
 0.8
 0.6
 0.4
 0.2
 0
y
t
i
r
a
l
i
i
m
S
e
n
s
o
C
i
 1
 0.8
 0.6
 0.4
 0.2
 0
Benign
CW
PGD Elastic
BPDA SPSA FGSM
Benign
CW
PGD Elastic
BPDA SPSA FGSM
Net
(a) Trapdoored Model
Net
(b) Original Model
Figure 12: Comparison of cosine similarity of normal images and adversarial images to trapdoored inputs in a trapdoored CIFAR10
model and in an original (trapdoor-free) CIFAR10 model. The boxes show the inter-quartile range, and the whiskers denote the 5t h
and 95t h percentiles.
y
t
i
r
a
l
i
i
m
S
e
n
s
o
C
i
 1
 0.8
 0.6
 0.4
 0.2
 0
y
t
i
r
a
l
i
i
m
S
e
n
s
o
C
i
 1
 0.8
 0.6
 0.4
 0.2
 0
Benign
CW
PGD Elastic
BPDA SPSA FGSM
Benign
CW
PGD Elastic
BPDA SPSA FGSM
Net
(a) Trapdoored Model
Net
(b) Original Model
Figure 13: Comparison of cosine similarity of normal images and adversarial images to trapdoored inputs in a trapdoored YouTube
Face model and in an original (trapdoor-free) YouTube Face model. The boxes show the inter-quartile range, and the whiskers denote
the 5t h and 95t h percentiles.