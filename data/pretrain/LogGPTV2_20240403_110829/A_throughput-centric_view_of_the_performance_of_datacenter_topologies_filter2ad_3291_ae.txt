requires no advance planning [44, 47, 52]. For example, it is possible
to add one switch and its servers to Jellyfish by randomly removing
links and connecting the opened ports to the new switch. It is easy
to see, from Figure 8(a), that this expansion likely preserves full
bandwidth. For example, if one starts with a 5K Jellyfish topology
with ùêª = 8, and augments it to 10K servers, the resulting topology
is still under the BBW line, so has full bisection bandwidth.
However, this expansion strategy may not always preserve full
throughput. In the same example, at 10K servers with ùêª = 8, the
topology is above the Throughput line: in other words, while the
topology before expansion has full throughput, the final topology
does not.
Thus, when planning a datacenter topology, a designer must
carefully consider future target expansion sizes and choose ùêª ac-
cordingly. If the target size is 10K, the topology designer needs
to plan in advance (as in Clos) and start with a ùêª = 7 instance
in order to preserve throughput after expansion. (The alternative
is to re-wire servers, which can significantly increase the cost of
expansion).
Over-subscription. The Fat-Tree work [1] defined a topology‚Äôs
over-subscription ratio as the ratio between the actual bisection
bandwidth and full bisection bandwidth. This definition can be mis-
leading when applied to uni-regular topologies. For these topologies,
H BBW Throughput
Topology
Jellyfish
Xpander
FatClique
Clos
N
32K 10
32K 10
32K 8.6
32K 32
3:4
3:4
3:4
1:2
1:2
1:2
2:3
1:2
Table 5: Throughput-based vs BBW-based over-subscription ratio. Num-
bers in one row are computed on the same topology.
the throughput itself is a measure of over-subscription. A through-
put of ùëì indicates that each server can send traffic at a fraction
ùëì of its line rate, corresponding to an over-subscription ratio of
1: 1
ùëì . Table 5 illustrates the difference between these two definitions
of over-subscription ratio for uni-regular topologies. For all uni-
regular topologies we have measured, the over-subscription ratio
defined using throughput is lower than bisection bandwidth-based
over-subscription ratio.9 For Clos, these two values are identical.
This suggests that, for uni-regular topologies, throughput is a
more conservative measure of over-subscription. It is also more
accurate, since it measures the upper bound of the actual achievable
throughput.
5.2 Scaling Throughput Evaluations
¬ß3 shows that tub better estimates worst-case throughput and
scales better than most of the previous throughput estimators.
Here we revisit the conclusions from prior work that has eval-
uated topology properties at smaller-scales using other ways to
estimate throughput. Table 6 summarizes our findings; we describe
these below.
Cost and Expansion. Singla et al. [44] have estimated throughput
using ideal routing on a few random permutations and show that
Jellyfish can support 27% more servers at full throughput than a
Fat-Tree [1] using the same number of switches. They conjecture
that this advantage improves by using a higher radix switch. In ¬ßK,
we show that: (1) the cost advantage at the largest considered
size in [44] is only 8% when tub is used to estimate throughput,
and (2) the cost advantage does not improve by using a higher
radix switch. Similarly, Xpander has used ideal routing on all-to-all
traffic matrices to estimate the throughput, and has shown that their
topology is more cost efficient than Fat-tree, and allows incremental
expandability up to any size with minor throughput loss. In ¬ßL, we
show that throughput of Xpander can drop significantly when
9The instance of FatClique we chose for this experiment uses a different ùêª than the
instances of Jellyfish and Xpander, which is why it has a different throughput.
A Throughput-Centric View of the Performance of Datacenter Topologies
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
[44]
tub
[47]
tub
t
s
o
C
p
x
E
. [47]
tub
[44]
tub
[47]
tub
e
r
u
l
i
a
F
Jellyfish supports 27% more servers at full throughput than a (same-equipment) Fat-tree (at 40K servers), Xpander uses 80% switches (matching
the number reported in [47])
Xpander using random rewiring can be incrementally expanded to any size while preserving high performance.
Expanding Xpander without considering the target size can cause significant throughput drop, leading to a topology with less than full-throughput.
Jellyfish is highly resilient to random link failures at the scale of <1K servers built using 12-port switches.
At some scales, Jellyfish can be as much as 20% less resilient compared to optimal resiliency using 32-port switches..
Xpander is resilient to failures at the scale of <1K servers built using 14-port switches.
At some scales, Xpander can be as much as 20% less resilient compared to optimal resiliency using 32-port switches.
Table 6: Scaling Throughput Evaluations. Conclusions can change significantly.
(a) N=32K
(b) N=131K
(c) Variation
Figure 10: Throughput of uni-regular topologies under random link failure.
Large uni-regular topologies degrade less than gracefully with failure.
using random rewiring even for very small expansions, resulting
in a topology with less than full-throughput (similar to Jellyfish).
Failure Resiliency. Prior work has explored the resilience of Jel-
lyfish [44] and Xpander [47] to random link failures for relatively
small topologies (at the scale of a few thousand servers). To do
this, they compute the throughput achieved by ideal routing (using
multi-commodity flow, which limits scaling) for a few randomly
chosen permutation matrices. The showed that, at these scales,
these topologies degrade gracefully, defined as follows. If ùúÉ is the
throughput of a topology without failure, and a randomly cho-
sen fraction ùëì of all links fail, then the nominal throughput under
failure is (1 ‚àí ùëì )ùúÉ (other work [45] has used a similar definition
to assess failure resilience in WAN switches). We say a topology
degrades gracefully if the actual throughput (in our experiments,
the throughput upper bound) under failure closely matches the
nominal throughput under failure.
tub allows us to evaluate failure resilience of these topologies
at larger scales.
Figure 10 shows the throughput behavior of Jellyfish with 8
servers per switch under random link failures, based on tub for: (a)
32K , (b) 131K. Jellyfish with 32K servers is perfectly resilient for
up to 30% link failure and deviates by <1% afterward while 131K
server topology is perfectly resilient for up to 11% link failures and
then deviates by 20% from the nominal throughput. This deviation
occurs because, the 131K topology has a relatively smaller number
of shortest paths (compared to the 32K topology) between each
pair in the maximal permutation matrix (Figure 4(b)). Higher rates
of random failures can reduce the available shortest paths even
further, reducing throughput.
This relationship between deviation from the nominal, and the
number of shortest paths, is more evident when comparing Fig-
ure 10(c) with Figure 4(b). The former plots the root mean square
deviation from the nominal as a function of topology size. In the
latter, the number of shortest paths decreases steadily from 24K
to 131K; in Figure 10(c), the deviation increases correspondingly.
Xpander exhibits same behavior as Jellyfish under random link
failures.
Takeaway. This example illustrates how tub can reveal previously
unobserved properties of a topology at larger scales. Using our
bound, we are able to measure the resiliency of uni-regular topolo-
gies for up to 131K. Using the throughput estimators in [44, 47]
(full-blown MCF), we are unable to scale beyond 8K servers on our
platform.
6 PRACTICAL CONSIDERATIONS
The importance of worst-case bounds. Focusing on worst-case
bounds can result in pessimistic designs and evaluations. In many
situations, it may be appropriate to focus on average case perfor-
mance. However, datacenter topologies, once deployed, are used
for several years [42]; in this time, traffic demands can grow signifi-
cantly. Because it is hard to predict demand over longer time-frames,
datacenter designers have focused on worst-case measures (like bi-
section bandwidth) as a design aid to maximize the lifetime of their
designs. tub follows this line of thinking: this paper shows that
tub is a better measure of worst-case performance for uni-regular
topologies than bisection bandwidth.
Clos-based deployments. Most deployed datacenter designs to-
day are Clos-based. However, designers are actively exploring other
lower-cost designs, one of which is the spine-free design [22], in
which the spine or topmost layer of switch blocks is replaced by
direct connections between the intermediate-layer (or aggregation
layer) pods [1]. Pods may carry transit traffic between other pods.
In this design, the inter-pod topology is effectively uni-regular, for
which tub can be used to understand performance.
Practical Workloads. In this paper, we have compared full-
bisection bandwidth topologies with full throughput topologies.
0102030Failure(%)0.40.50.60.70.8Throughputnominalactual0102030Failure(%)0.40.50.60.70.8Throughputnominalactual25k50k75k100k125k#Servers (N)0.000.040.080.12DeviationSIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Namyar .et al.
upper bound across all traffic matrices and explore it to understand
practical scaling limits for uni-regular topologies, and the utility
of a throughput-centric view in evaluating properties of datacen-
ter topologies. We also compare tub against many of these prior
approaches.
Practical Routing. In practice, throughput highly depends on the
routing algorithm and the underlying topology. ECMP is optimal
for the Clos family [1, 15, 42]. For Jellyfish, Xpander, and FatClique,
routing strategies like an ECMP-VLB hybrid [29] and FatPaths [7]
have shown promising throughput performance. We have left it to
future work to understand the gap between achievable throughput
using these more practical routing strategies and tub.
8 CONCLUSIONS AND FUTURE WORK
This paper broadens our understanding of the throughput metric
for datacenter topology performance, and its relationship to bi-
section bandwidth. We derive a closed-form expression for the
upper bound of the throughput (tub) of a given topology that is
independent of routing. This bound applies to most proposed dat-
acenter topologies. For a sub-class of these designs, uni-regular
topologies, we are able to derive an upper-bound on throughput
that applies to any instance in this sub-class, using which we show
that uni-regular topologies are fundamentally limited: beyond a
certain scale, they cannot have full throughput even if they have
full bisection bandwidth. In practice, many instances of uni-regular
topologies with 10-15K servers cannot have full throughput. Finally,
we demonstrate that tub to evaluate properties of a topology can
result in different conclusions compared to using other metrics.
Future work can explore the throughput gap between tub and the
throughput achievable using practical routing algorithms, explore
the throughput of Clos-variants like [36], scale tub to even larger
topologies, and improve its tightness.
Acknowledgements. We thank our shepherd Michael Schapira,
and the anonymous reviewers for their feedback on the paper. This
material is based upon work supported by the U.S. National Science
Foundation under grants No. CNS-1901523, CNS-1705086, and CNS-
1955422.
Deployed topologies are often over-subscribed; a deployed Clos
might have less than full bisection bandwidth. These deployments
work well because operators carefully manage datacenter work-
loads to ensure that they don‚Äôt exceed fabric capacity. They also
leave spare capacity for management operations such as expansion
and upgrade [42, 53]. For Clos, the bisection bandwidth of the
oversubscribed topology is a good measure of the capacity. For
uni-regular topologies, tub is a better measure of capacity for an
oversubscribed network (¬ß5.1).
Benchmarking routing designs. Aside from topology, routing
design also determines whether the datacenter is able effectively
utilize its capacity in serving workloads. For uni-regular topologies,