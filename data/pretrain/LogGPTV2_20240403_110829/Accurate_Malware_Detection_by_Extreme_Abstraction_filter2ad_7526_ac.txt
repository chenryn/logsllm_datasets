in a value of 0 that is labeled symbolic, we will take both branches
while one of them is infeasible in every actual run of the sample.
Note that this is just another source of extreme abstraction, in
addition to the extreme abstraction resulting from the abstract way
we support many API functions.
Lightweight symbols are useful for semi-manual analysis of a
single sample. They also drive the development of new features for
machine learning and new anti-anti-research techniques, because
comparing two dierent paths through the same code is a good
way to pinpoint where the sample is using anti-research to modify
its behavior based on the environment. For example, comparing
two paths under lightweight symbolic execution showed us that
a sample was checking the registry and le system for evidence
of the presence of various cybersecurity tools, and modifying its
behavior if found. This allowed us to ag these specic checks as a
feature for machine learning, and also to concretize the response
of the abstract registry and le system in such a case, so that its
functions always return a value indicating that these tools are not
installed. In such a way the symbolic version of the code drives the
development of the non-symbolic version. Finally, the lightweight
symbolic version can be used to generate data for machine learning,
when the results of non-symbolic runs are inconclusive.
Lightweight symbols, while eliminating the need to call a solver,
do not solve the problem of path explosion, which is actually exacer-
bated by lightweight symbols. Thus even our lightweight symbolic
version is too computationally expensive to use in a naive way on
large datasets. As described in Section 3, we solve this problem
using a funnel-based approach, as follows: All samples are run on
a non-symbolic version of our extremely abstract OS, resulting in
a single path, and then on four time models (described below) in
the non-symbolic version, resulting in four paths. Only if the result
is inconclusive do we proceed to the symbolic version, with an
increasing number of paths at each layer.
Time Models. Time models are a way to run multiple paths
without the overhead of even lightweight symbols. Each of our
time models is a single path representing a dierent abstract be-
havior of time. The rst is the random time model, in which time
behaves completely randomly. The other three are based on a pa-
rameterized time model with parameter N, which we term the
random denominator time model. In the random denominator time
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
F. Copty et al.
of the PE le, and imported functions. During dynamic analysis, we
extract the same static features from any executable le or DLL that
is written and/or executed from memory as a created or injected
process.
Next are features extracted during dynamic analysis. We record
a count of every API function called and every x86 instruction in-
voked. In addition we extract approximately 300 additional dynamic
features, which come in several broad categories, the rst of which
is termed informative and includes features that ag information
that is not necessarily malicious or even suspicious. For instance,
this category includes a count of exceptions that occurred and their
type, network connections attempted or made, and strings encoun-
tered. We also derive features by comparing this category to the
static features. For instance we calculate the ratio of API functions
imported to those called, and the ratio of strings found statically to
those found during dynamic analysis.
The second category is termed suspicious; it includes features that
ag information that is suspicious but not necessarily malicious.
This includes activity that may indicate obfuscation (which may
be used by benign software to protect intellectual property) such
as checking for the presence of a debugger, a virtual machine or a
cybersecurity tool (there are myriad ways to do this, not all of which
entail calling a dedicated API function), jumping or calling into the
middle of an API function, overwriting the header, overwriting part
of an API function, directly accessing various bits of the operating
system data structures, or executing in an intentionally innite
loop, as in the example in Section 1. Also in this category is the
discovery of self-modied code and the discovery of self-modied
code consisting of symbolic bytes, indicating that it is a function of
some value that we have abstracted, for instance the time or locale.
The third category is termed malicious and includes features that
ag information that is almost certainly malicious. This includes
trying to access known malicious URLs, systematically encrypting
and/or deleting les that were not created by the sample, or over-
writing Windows DLLs. Note that even malicious features could
have a benign use, for instance a cybersecurity tool could try to
access known malicious URLs, an encryption tool could encrypt
and delete les, and a Windows update could overwrite Windows
DLLs.
N-grams. We also extract features that capture n-grams of se-
quences of the above features. Our experiments show that most
eective were 1-, 2-, 3- and 4-grams of API calls, 1-grams of x86
instructions and 1-, 3- and 6-grams of informative, suspicious and
malicious features. The eciency of 1-4 n-grams of API calls has
been discussed in previous works [21]. The eciency of 6-grams
for other features can be explained due to their ability to capture
a broader view of the sample execution than 6-grams of API calls.
N-gram counters are collapsed using feature hashing [36] into a
narrower vector.
Numerical features. We extract numerical features from all
the non-numerical data; for example, from the list of URLs we
extract the total number of internet access attempts, number of
unique URLs accessed, number of connection attempts with a non-
standard TCP port (port which is not 80 (HTTP), 443 (HTTPS), 53
(DNS), and a few other prominent TCP-based protocols), as well as
the reputation score of the target host according to IBM X-Force
Exchange threat intelligence platform.
Multi-path aggregation. When exploring multiple paths
through the code, we aggregate the features from all paths into a
single feature vector representing the input sample (the PE le).
Samples without anti-research will often behave the same on all
paths, whereas a sample performing some kind of anti-research
often behaves dierently on at least one of the paths. For this rea-
son, when aggregating, we use features that compare the paths. For
a feature that would be an integer on a single path (for example,
number of times API function X was called, or the number of dier-
ent URLs accessed by the sample), we derive the following features:
a) minimum over all paths, b) maximum over all paths, c) mean
over all paths, d) standard deviation between all paths, e) median
over all paths, f) ratio of number of paths on which the number
was non-zero to the total number of paths.
Data cleaning. At this point we have an all-numerical feature
vector. We perform several data cleaning steps: removing features
the values of which are constant for all samples in the training set,
imputing missing values using feature means, and scaling each fea-
ture into [0, 1] by dividing all the values by the absolute maximum
value of the feature. To lower the dimensionality of the feature vec-
tors fed to the machine learning classier algorithm, we perform
feature selection using information gain [12], resulting in a vector
of 3,500 features.
Inuence of abstraction. The fact that abstraction causes us
to explore possibly infeasible paths causes us to extract dierent
features than if we explored only feasible ones. However, there are
more subtle ways in which the abstraction aects the extracted
features. Consider for example the following code, where X and Y
are API functions:
call X
call Y
If API function X calls only API function A, which itself calls no
other API function, and if we are collecting 2-grams of API calls,
then an actual run of the above code will see the pairs XA and AY,
while an abstract run will see the pair XY. Furthermore, in the actual
run, any call to X will result in the pair XA, which is uninteresting
as it adds no information beyond the fact that the sample calls X.
The abstraction therefore allows more interesting sequences to be
collected. Note that a concrete tool could attempt to achieve the
same eect described in the example (collecting the pair XY) by
collecting data only on API calls made by the sample itself, and not
those made by other Windows DLL code. This, however, would only
partially mimic the eect of abstraction, because malicious samples
sometimes jump or return into the middle of an API function.9 In
such a case the concrete solution would ignore the API calls made
by the DLL code, whereas our abstraction does not, causing runs
on samples that use such techniques to appear very dierent than
runs on samples that do not.
3.2 Experimental setup
Labeling. We train a classier using supervised learning and there-
fore require a target label for each sample (0 for benign and 1 for
malware). We use malware indicators from VirusTotal [34]. For
each sample we count the number of malicious detections from the
various engines aggregated by VirusTotal, weighted according to a
9MD5: 97a2b7ad321c0157294dc54baa362581
106
Accurate Malware Detection by Extreme Abstraction
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Table 1: PE les distribution
Training set
Test set
Benign Malware
113,162
116,807
49,310
49,254
Total
229,969
98,564
(%)
(70%)
(30%)
Table 2: Malware detection results on test set
Timeout (min.)
Paths
1
1
Funnel
1
4
FPR% TPR%
97.45
98.64
99.11
0.1
0.1
0.1
F1 AUC
99.98
99.98
—
98.65
99.26
99.50
reputation we give to each engine, such that several well-known
engines are given weight > 1, and all others are weighted 1. We
use the result to label a sample benign or malicious.
Machine learning model. The matrix of all the feature vectors
from all the samples in the training set are used to train a Ran-
dom Forest classier [4], congured with 1,600 decision trees, each
one with maximal depth of 150, and up to 200 features per split.
We chose this conguration by running a grid search on a large
number of possible congurations. The feature processing and ma-
chine learning model were implemented using the Scikit-learn [23]
library.
Dataset. The training and test set sizes are listed in Table 1.
The benign samples were collected from a standard installation of
Microsoft Windows 7, as well as public sources of free Windows
software. The malware samples come from internal sources as well
as from VirusSign [33] and VirusTotal [34]. All samples were re-
labeled according to the procedure described above, regardless of
their source.
Experiments. We performed four experiments, as follows: a)
malware detection using the funnel, b) checking the inuence of
multiple paths on our classication, c) checking the inuence of
PE packers on our classication, d) checking whether extreme
abstraction is eective as well for family classication.
3.3 Experimental Results
Malware classication. Due to resource constraints, we used a
funnel conguration with two layers. Layer 1 uses a single path with
a timeout of one minute, while layer 2 uses four paths representing
four dierent time models (see Section 2.2), also with a timeout of
one minute. We rst show the results for each layer of the funnel
independently, meaning that we ran the entire test set on both
layers for the purpose of the experiment, even though that is not
the way we will use it in production. Afterward we show the result
of the funnel.
For running each layer independently, we set the classier thresh-
olds such that we get a false positive rate (FPR) of 0.1%, and then
measured the true positive rate (TPR) with that threshold. This
resembles production usage of such a system in which keeping the
FPR low is key for adoption. The malware detection rates on the test
set after training on the data set are given in Table 2, and the ROC
107
Figure 6: Zoomed ROC curve
Table 3: Classication funnel sample count
Layer# Execution paths
1
4
1
2
Total
Samples
94,845
3,719
98,564
(%)
(96.2%)
(3.8%)
(100%)
curve is plotted in Figure 6. An independent run of layer 1 results
in a threshold of 0.72 and a TPR of 97.45%, while an independent