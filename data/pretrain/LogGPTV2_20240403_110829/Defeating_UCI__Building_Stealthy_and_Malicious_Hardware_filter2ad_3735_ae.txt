### Imposed Constraint on Malicious Circuit Search

We imposed a significant constraint in our search for malicious circuits: in all the attack circuits described in this paper, the input wires can be categorized into trigger inputs and non-trigger inputs. Additionally, the definitions provided in Section III only consider circuits where such a complete separation exists. We focused on this specific class of circuits because it is algorithmically easier to reason about and facilitated the discovery of attacks. However, there is no inherent reason to assume that malicious backdoors will always exhibit this separation in practice. Therefore, any proposed fix to the UCI algorithm or any future design-time algorithm for detecting malicious circuits should, like UCI, not assume that such a separation exists between trigger and non-trigger inputs.

### Related Work

There has been extensive prior research on methods for protecting against malicious hardware. In this section, we do not attempt to summarize all of that work. Instead, we focus on research involving the design-time insertion or detection of malicious hardware. Other stages of the hardware life cycle, such as fabrication and supply chain, are also potential points for malicious logic insertion or detection [5]–[13]. However, this paper focuses on the RTL-level design stage, which is the target of the UCI algorithm, due to its distinct properties and constraints.

#### A. Hardware Attacks

- **Hadzic et al.** were the first to explore what hardware attacks might look like and their potential impacts [14]. They specifically targeted FPGAs, demonstrating that it is possible to add malicious logic to the FPGA's configuration file, causing short-circuits and driving wires with high logic values. This can increase the device's current draw, potentially leading to overheating or wear-out failures. Hadzic et al. also proposed architectural changes and a configuration analysis tool to defend against these attacks.

- **Agrawal et al.** described three attacks on RSA hardware as part of a larger paper on defending against supply-chain attacks [15]. One attack uses a built-in counter to shut down the RSA hardware after a specified number of clock cycles. The other two attacks use a comparison-based trigger to contaminate the results when activated. These attacks highlight that targeted hardware attacks can have a small footprint in terms of circuit area, power, and coding effort.

- **The Illinois Malicious Processor (IMP) by King et al.** [1] was the first to propose the idea of using malicious hardware as a support mechanism for attack software. These security vulnerabilities, inserted during design time, are termed "footholds." Since footholds can be introduced without significantly altering the code and with minimal impact on the rest of the design, they can be difficult to detect using conventional means or side-channel analysis. IMP demonstrates two attacks: unauthorized memory access, allowing user processes to access restricted memory addresses, and shadow mode, where the processor executes in a hidden mode. King et al. showed how malicious software services can leverage these footholds to escalate privileges, circumvent the login process, or steal passwords. Hicks et al. [2] later re-implemented and verified these attacks, confirming that the attacked hardware passed SPARCv8 certification tests.

- **Jin et al.** developed eight attacks on the staged military encryption system codenamed Alpha [16]. These attacks corrupted four different units and three data paths, with area overheads ranging from less than 1% to almost 7%, while still passing design-time testing. This highlights the risk of small, hidden, but powerful attacks.

#### B. Defenses to Hardware Attacks

Research on detecting and defending against malicious hardware can be categorized into purely design-time methods and methods that involve a run-time aspect.

1. **Design Time:**
   - **Huffmire et al.** studied how to integrate untrusted IP cores with trusted IP cores by enabling architects to restrict communication between them [17]. They proposed using areas of dead logic (moats) around each IP core and a verifiable inter-module communication philosophy (drawbridges). This approach ensures that no trusted IP core is contaminated or spied upon by an untrusted IP core, even over a side channel. Since UCI and moats and drawbridges address different problems, our attacks on UCI do not affect moats and drawbridges.

2. **Run Time:**
   - **Waksman et al.** proposed TrustNet and DataWatch [18], which aim to suppress malicious behavior during run time rather than preventing the inclusion of malicious circuits at design time. TrustNet and DataWatch prevent untrusted hardware units in a processor’s core pipeline from leaking information or stopping the flow of information. For each input value to an untrusted pipeline stage, the stages before and after determine if the output contains the expected amount of information. However, these methods fail to detect incorrect output values of the correct width, making them vulnerable to malicious backdoors that tamper with computation results without affecting their size. Thus, both TrustNet and DataWatch, and UCI, can be defeated if the attacker chooses the backdoor appropriately.

#### C. Formal Analysis of Hardware

Hardware, due to its limited resources and cycle-based behavior, is generally amenable to formal analysis. Most research on formal methods applied to hardware focuses on verifying that the hardware faithfully implements a given specification.

- **Model Checking** is a formal verification technique that verifies the behavior of a hardware design satisfies a set of properties specified using temporal logic formulas [19], [20]. Verification is done through a bounded exhaustive search of the design state space. However, model checking is limited in its ability to scale to complex designs due to the exponential growth of the state space. One approach to address this is to apply model checking to an abstract model of the processor [22], though many challenges remain.

- **Formal Proofs** involve developing a proof that an abstract model of the processor behaves as prescribed by the given specification [23]–[28]. The proof can be developed manually and verified by a checker or interactively with a theorem prover.

Given sufficient time, computational power, and a complete specification, formal verification techniques can be a powerful tool for detecting malicious circuits. Even without unlimited resources or a complete specification, formal methods can be applied to detect malicious backdoors in processors, particularly in security-critical modules. For example, formally verifying the logic for transitioning to supervisor mode could have detected the backdoor from Section IV. While formal verification is not guaranteed to detect all malicious backdoors, it can make it harder for an attacker to evade detection.

### Conclusion

We demonstrate an attack against UCI, a recently proposed algorithm for malicious hardware detection. UCI attempts to detect malicious hardware inserted at design time by identifying pairs of dependent signals in the source code that could seemingly be replaced by a wire without affecting the outcome of any test cases. Experiments show that it is possible to build malicious circuits where no two dependent signals are always equal during design-time testing, yet the circuit exhibits hidden behavior upon receiving a special input, called the trigger input. Using these circuits, we implemented a malicious backdoor in the Leon3 processor that UCI is unable to detect. The attack allows a user-level program, with knowledge of the secret trigger, to enter supervisor mode, bypassing any OS-level checks. This work demonstrates that detecting malicious backdoors in hardware remains an open problem and suggests that devising a reliable algorithm for detecting such attacks may be challenging.

### Acknowledgment

We thank our shepherd Kevin Fu for his guidance and the anonymous reviewers for their feedback and suggestions. This research was funded in part by National Science Foundation grants CCF 0811268, CNS 0953014, and CCF 0424422, and by AFOSR MURI grant FA9550-09-01-0539. Any opinions, findings, conclusions, or recommendations expressed in this paper are solely those of the authors.

### References

[1] S. T. King, J. Tucek, A. Cozzie, C. Grier, W. Jiang, and Y. Zhou, “Designing and implementing malicious hardware,” in Proceedings of the First USENIX Workshop on Large-Scale Exploits and Emergent Threats (LEET), 2008, pp. 1–8.
[2] M. Hicks, M. Finnicum, S. T. King, M. M. K. Martin, and J. M. Smith, “Overcoming an untrusted computing base: Detecting and removing malicious hardware automatically,” in Proceedings of the 2010 IEEE Symposium on Security and Privacy, 2010, pp. 159–172.
[3] J. Markoff, “Old trick threatens the newest weapons,” The New York Times, p. D1, October 27, 2009.
[4] S. Adee, “The hunt for the kill switch,” IEEE Spectrum, vol. 45, no. 5, pp. 34–39, 2008.
[5] B. Gassend, D. Clarke, M. van Dijk, and S. Devadas, “Silicon physical random functions,” in Proceedings of the 9th ACM Conference on Computer and Communications Security, 2002, pp. 148–160.
[6] B. Moyer, “A PUF piece: Revealing secrets buried deep within your silicon,” EE Journal, January 24, 2011, http://www.techfocusmedia.net/archives/articles/20110124-puf/.
[7] D. Du, S. Narasimhan, R. S. Chakraborty, and S. Bhunia, “Self-referencing: a scalable side-channel approach for hardware trojan detection,” in 12th International Conference on Cryptographic Hardware and Embedded Systems (CHES). Springer-Verlag, 2010.
[8] Y. Jin and Y. Makris, “Hardware trojan detection using path delay fingerprint,” in IEEE International Workshop on Hardware-Oriented Security and Trust, 2008.
[9] R. Rad, M. Tehranipoor, and J. Plusquellic, “Sensitivity analysis to hardware trojans using power supply transient signals,” in IEEE International Workshop on Hardware-Oriented Security and Trust, 2008.
[10] T. Kean, D. McLaren, and C. Marsh, “Verifying the authenticity of chip designs with the DesignTag system,” in IEEE International Workshop on Hardware-Oriented Security and Trust, 2008.
[11] A. Das, G. Memik, J. Zambreno, and A. Choudhary, “Detecting/preventing information leakage on the memory bus due to malicious hardware,” in Design, Automation & Test in Europe, 2010.
[12] M. Potkonjak, “Synthesis of trustable ICs using untrusted CAD tools,” in Design Automation Conference (DAC). ACM/IEEE, 2010.
[13] U. Rührmair, F. Sehnke, J. Sölter, G. Dror, S. Devadas, and J. Schmidhuber, “Modeling attacks on physical unclonable functions,” in Proceedings of the 17th ACM Conference on Computer and Communications Security, 2010.
[14] I. Hadžić, S. Udani, and J. M. Smith, “FPGA Viruses,” in Proceedings of the 9th International Workshop on Field-Programmable Logic and Applications. Springer, 1999.
[15] D. Agrawal, S. Baktir, D. Karakoyunlu, P. Rohatgi, and B. Sunar, “Trojan detection using IC fingerprinting,” in Proceedings of the 2007 IEEE Symposium on Security and Privacy, 2007, pp. 296–310.
[16] Y. Jin, N. Kupp, and Y. Makris, “Experiences in hardware trojan design and implementation,” IEEE International Workshop on Hardware-Oriented Security and Trust, 2009.
[17] T. Huffmire, B. Brotherton, G. Wang, T. Sherwood, R. Kastner, T. Levin, T. Nguyen, and C. Irvine, “Moats and drawbridges: An isolation primitive for reconfigurable hardware based systems,” in Proceedings of the 2007 IEEE Symposium on Security and Privacy, 2007.
[18] A. Waksman and S. Sethumadhavan, “Tamper evident microprocessors,” in Proceedings of the 2010 IEEE Symposium on Security and Privacy, 2010.
[19] C. Seger, “An introduction to formal hardware verification,” University of British Columbia, Vancouver, BC, Canada, Tech. Rep., 1992.
[20] C. Kern and M. R. Greenstreet, “Formal verification in hardware design: a survey,” ACM Trans. Des. Autom. Electron. Syst., vol. 4, no. 2, pp. 123–193, 1999.
[21] R. Pelanek, “Fighting state space explosion: Review and evaluation,” Formal Methods for Industrial Critical Systems, vol. 5596, pp. 37–52, 2009.
[22] V. A. Patankar, A. Jain, and R. E. Bryant, “Formal verification of an ARM processor,” in Twelfth International Conference On VLSI Design, 1999.
[23] M. Srivas and M. Bickford, “Formal verification of a pipelined microprocessor,” Software, IEEE, vol. 7, no. 5, pp. 52–64, 1990.
[24] J. R. Burch and D. L. Dill, “Automatic verification of pipelined microprocessor control,” in CAV ’94: Proceedings of the 6th International Conference on Computer Aided Verification. London, UK: Springer-Verlag, 1994, pp. 68–80.
[25] J. U. Skakkebæk, R. B. Jones, and D. L. Dill, “Formal verification of out-of-order execution using incremental flushing,” in CAV ’98: Proceedings of the 10th International Conference on Computer Aided Verification. London, UK: Springer-Verlag, 1998, pp. 98–109.
[26] M. N. Velev and R. E. Bryant, “Formal verification of superscalar microprocessors with multicycle functional units, exception, and branch prediction,” in DAC ’00: Proceedings of the 37th Annual Design Automation Conference. ACM, 2000, pp. 112–117.
[27] R. Hosabettu, G. Gopalakrishnan, and M. Srivas, “Formal verification of a complex pipelined processor,” Formal Methods System Design, vol. 23, no. 2, pp. 171–213, 2003.
[28] J. Bormann, S. Beyer, A. Maggiore, M. Siegel, S. Skalberg, T. Blackmore, and F. Bruno, “Complete formal verification of TriCore2 and other processors,” in DVCon, February 2007.