We artiﬁcially imposed a signiﬁcant constraint on our search
for malicious circuits: in all of the attack circuits described
in this paper, the input wires can be separated into trigger in-
puts and non-trigger inputs. Moreover, the deﬁnitions given
in Section III consider only circuits where such a complete
separation exists. We focused on this special class of circuits
solely because it is easy to reason about algorithmically
and it made it easier to ﬁnd attacks. However, there is
no reason to assume malicious backdoors will necessarily
exhibit this separation in practice. Therefore, any proposed
ﬁx to the UCI algorithm or any future design-time algorithm
for detecting malicious circuits should, like UCI, not assume
that such a separation exists between trigger and non-trigger
inputs.
VII. RELATED WORK
There has been considerable prior research on methods for
protecting against malicious hardware. We do not attempt
to summarize all of that work here. Rather, our discussion
of related work focuses on research involving the design-
time insertion or detection of malicious hardware. There are
other possible points along the hardware life-cycle where
malicious logic could be inserted or detected (for example,
the fabrication and supply chain stages [5]–[13]), but in this
paper we focus on the RTL-level design stage, as that is the
target of the UCI algorithm. Other stages have very different
properties and constraints.
We ﬁrst present attack-oriented research, then cover re-
search on defense mechanisms, and lastly, highlight formal
methods that may be applicable to the design-time attack
model.
74
A. Hardware Attacks
Hadzic et al. were the ﬁrst to look at what hardware
attacks might look like and what they could do [14]. They
speciﬁcally targeted FPGAs, showing it is possible to add
malicious logic to the FPGA’s conﬁguration ﬁle that would
short-circuit wires, driving them with logic high values. A
wire with multiple high drivers increases the device’s current
draw, possibly causing the destruction of the device through
overheating or wear-out failures. Hadzic et al. also proposed
both a change to the FPGA architecture and a conﬁguration
analysis tool that would defend against the proposed attacks.
Agrawal et al. describe three attacks on RSA hardware as
a part of a larger paper describing a defense against supply-
chain attacks [15]. One of their attacks uses a built-in counter
that shuts down the RSA hardware after a prescribed number
of clock cycles. The other two attacks use a comparison
based trigger that contaminates the results of the RSA
hardware when activated. These attacks show that targeted
hardware attacks can have a small footprint in terms of
circuit area, power, and coding effort required.
The Illinois Malicious Processor (IMP) by King et al. [1]
is the ﬁrst work to propose the idea of malicious hardware
being used as a support mechanism for attack software.
These hardware security vulnerabilities, inserted during de-
sign time, are termed footholds. Since footholds can be
introduced without changing many lines of code and without
much effect on the rest of the design, they can be difﬁcult
to detect using conventional means or side-channel analysis.
IMP demonstrates two attacks in particular: unauthorized
memory access, which allows user processes to access
restricted memory addresses, and shadow mode, where the
processor executes in a special, hidden mode. As a part of
IMP, King et al. showed how malicious software services
can leverage the inserted footholds to escalate privileges,
circumvent the login process, or steal passwords. In follow-
on work, Hicks et al. [2] reimplemented the attacks and
veriﬁed that the attacked hardware passed SPARCv8 certi-
ﬁcation tests.
Jin et al. developed eight attacks on the staged military
encryption system codenamed Alpha [16]. The attacks cor-
rupted four different units and three of the data paths of the
encryption system. The eight attacks ranged in area overhead
from less than 1% to almost 7% while still managing to pass
design-time testing. The results highlight the risk of small,
buried, but powerful attacks.
B. Defenses to Hardware Attacks
Research on detecting and defending against malicious
hardware can be categorized into purely design-time meth-
ods and methods that involve a run-time aspect.
1) Design Time: Huffmire et al. study how to integrate
untrusted IP cores with trusted IP cores by enabling ar-
chitects to restrict communication between IP cores. They
propose using areas of dead logic (moats) around each IP
core and a veriﬁable inter-module communication philos-
ophy (drawbridges) [17]. This approach ensures that no
trusted IP core is contaminated or spied upon by an untrusted
IP core, not even over a side channel. Because UCI and
moats and drawbriges attempt to solve different problems,
our attacks on UCI do not affect moats and drawbridges.
2) Run Time: Waksman et al. propose TrustNet and
DataWatch [18]. Instead of preventing the inclusion of
malicious circuits during design time as UCI attempts to do,
TrustNet and DataWatch attempt to suppress malicious be-
havior during run time. The goal of TrustNet and DataWatch
is to prevent untrusted hardware units in a processor’s core
pipeline from leaking information (i.e., producing too much
output information) or stopping the ﬂow of information (i.e.,
producing too little output information). For each input value
to an untrusted pipeline stage, the pipeline stages before and
after the untrusted stage determine if the output produced
by the untrusted stage contains exactly the expected amount
of information (e.g., when multiplying two 32-bit operands,
was the result precisely 64 bits wide?). Because only the
amount of output data is checked, TrustNet and DataWatch
fail to detect incorrect output values of the correct width.
Consequently, TrustNet and DataWatch are vulnerable to
malicious backdoors that tamper with the results of com-
putations without affecting the size of those results, so both
TrustNet and DataWatch and UCI can be defeated if the
attacker chooses the backdoor appropriately.
C. Formal Analysis of Hardware
Hardware, due to its limited resources and cycle-based
behavior is generally quite amenable to formal analysis.
Currently, the majority of research on formal methods, as
applied to hardware, focuses on verifying that the hardware
faithfully implements a given speciﬁcation.
Model checking is one formal veriﬁcation technique that
veriﬁes the behavior of a hardware design satisﬁes a set of
properties, which are speciﬁed using temporal logic formu-
las [19], [20]. The veriﬁcation is done through a bounded
exhaustive search of the design state space. However, model
checking is limited in its ability to scale to complex designs
due to the size of the state space to be explored, which
is exponential in the size of the state [21]. One approach
that partly addresses the state-space explosion problem is to
apply model checking to an abstract model of the proces-
sor [22], but many challenges remain.
Another formal veriﬁcation technique is to develop a
proof that an abstract model of the processor behaves as
prescribed by the given speciﬁcation [23]–[28]. The proof
may be developed by hand and veriﬁed by a checker, or
developed interactively with a theorem prover.
Given sufﬁcient time, computational power, and a com-
plete speciﬁcation, formal veriﬁcation techniques can be a
powerful tool for detecting malicious circuits. Even without
unlimited resources or a complete speciﬁcation, defenders
75
might reasonably apply formal methods to the problem of
detecting malicious backdoors in processors. It is possible
to apply formal veriﬁcation to only certain security-critical
modules of the processor. If these modules are simple
enough that a complete speciﬁcation can be written for them,
it may be possible to fully formally verify such modules,
thereby assuring they are free of backdoors. For instance,
it would have been possible to detect our backdoor from
Section IV by formally verifying the logic for when to
transition to supervisor mode without full formal veriﬁcation
of the entire processor. The use of formal veriﬁcation is
not guaranteed to detect all malicious backdoors, but
it
might make it harder for an attacker to successfully evade
detection.
VIII. CONCLUSION
We demonstrate an attack against UCI, a recently pro-
posed algorithm for malicious hardware detection. UCI
attempts to detect malicious hardware inserted at design
time by identifying pairs of dependent signals in the source
code that could seemingly be replaced by a wire without
affecting the outcome of any test cases. Experiments show
that it is possible to build malicious circuits in which no
two dependent signals are always equal during design-time
testing, yet where the circuit exhibits hidden behavior upon
receiving a special input, called the trigger input. Using these
circuits, we implement a malicious backdoor in the Leon3
processor that UCI is unable to detect. The attack allows
a user-level program, with knowledge of the secret trigger,
to enter supervisor mode, bypassing any OS-level checks.
This work demonstrates that detecting malicious backdoors
in hardware remains an open problem and suggests that it
may not be easy to devise a reliable algorithm for detecting
such attacks.
IX. ACKNOWLEDGMENT
We thank our shepherd Kevin Fu for his guidance and
we thank the anonymous reviewers for their feedback and
suggestions. This research was funded in part by National
Science Foundation grants CCF 0811268, CNS 0953014,
and CCF 0424422 and by AFOSR MURI grant FA9550-
09-01-0539. Any opinions, ﬁndings, conclusions or recom-
mendations expressed in this paper are solely those of the
authors.
REFERENCES
[1] S. T. King, J. Tucek, A. Cozzie, C. Grier, W. Jiang, and
Y. Zhou, “Designing and implementing malicious hardware,”
in Proceedings of the First USENIX Workshop on Large-Scale
Exploits and Emergent Threats (LEET), 2008, pp. 1–8.
[2] M. Hicks, M. Finnicum, S. T. King, M. M. K. Martin, and
J. M. Smith, “Overcoming an untrusted computing base:
Detecting and removing malicious hardware automatically,”
in Proceedings of the 2010 IEEE Symposium on Security and
Privacy, 2010, pp. 159–172.
[3] J. Markoff, “Old trick threatens the newest weapons,” The
New York Times, p. D1, October 27 2009.
[4] S. Adee, “The hunt for the kill switch,” IEEE Spectrum,
vol. 45, no. 5, pp. 34–39, 2008.
[5] B. Gassend, D. Clarke, M. van Dijk, and S. Devadas, “Sil-
icon physical random functions,” in Proceedings of the 9th
ACM Conference on Computer and Communications Security,
2002, pp. 148–160.
[6] B. Moyer, “A PUF piece: Revealing secrets buried deep
January 24 2011,
within your
http://www.techfocusmedia.net/archives/articles/20110124-
puf/.
silicon,” EE Journal,
[7] D. Du, S. Narasimhan, R. S. Chakraborty, and S. Bhunia,
“Self-referencing: a scalable side-channel approach for hard-
ware trojan detection,” in 12th International Conference on
Cryptographic Hardware and Embedded Systems (CHES).
Springer-Verlag, 2010.
[8] Y. Jin and Y. Makris, “Hardware trojan detection using
path delay ﬁngerprint,” in IEEE International Workshop on
Hardware-Oriented Security and Trust, 2008.
[9] R. Rad, M. Tehranipoor, and J. Plusquellic, “Sensitivity
analysis to hardware trojans using power supply transient sig-
nals,” in IEEE International Workshop on Hardware-Oriented
Security and Trust, 2008.
[10] T. Kean, D. McLaren, and C. Marsh, “Verifying the authen-
ticity of chip designs with the designtag system,” in IEEE
International Workshop on Hardware-Oriented Security and
Trust, 2008.
[11] A. Das, G. Memik, J. Zambreno, and A. Choudhary, “De-
tecting/preventing information leakage on the memory bus
due to malicious hardware,” in Design, Automation & Test in
Europe, 2010.
[12] M. Potkonjak, “Synthesis of trustable ICs using untrusted
CAD tools,” in Design Automation Conference (DAC).
ACM/IEEE, 2010.
[13] U. R¨uhrmair, F. Sehnke, J. S¨olter, G. Dror, S. Devadas, and
J. Schmidhuber, “Modeling attacks on physical unclonable
functions,” in Proceedings of the 17th ACM Conference on
Computer and Communications Security, 2010.
[14] I. Hadˇzi´c, S. Udani, and J. M. Smith, “FPGA Viruses,” in
the 9th International Workshop on Field-
Proceedings of
Programmable Logic and Applications. Springer, 1999.
[15] D. Agrawal, S. Baktir, D. Karakoyunlu, P. Rohatgi, and
B. Sunar, “Trojan detection using IC ﬁngerprinting,” in
Proceedings of the 2007 IEEE Symposium on Security and
Privacy, 2007, pp. 296–310.
[16] Y. Jin, N. Kupp, and Y. Makris, “Experiences in hardware
trojan design and implementation,” IEEE International Work-
shop on Hardware-Oriented Security and Trust, 2009.
76
[17] T. Huffmire, B. Brotherton, G. Wang, T. Sherwood, R. Kast-
ner, T. Levin, T. Nguyen, and C. Irvine, “Moats and draw-
bridges: An isolation primitive for reconﬁgurable hardware
based systems,” in Proceedings of the 2007 IEEE Symposium
on Security and Privacy, 2007.
[18] A. Waksman and S. Sethumadhavan, “Tamper evident micro-
processors,” in Proceedings of the 2010 IEEE Symposium on
Security and Privacy, 2010.
[19] C. Seger, “An introduction to formal hardware veriﬁcation,”
University of British Columbia, Vancouver, BC, Canada,
Canada, Tech. Rep., 1992.
[20] C. Kern and M. R. Greenstreet, “Formal veriﬁcation in hard-
ware design: a survey,” ACM Trans. Des. Autom. Electron.
Syst., vol. 4, no. 2, pp. 123–193, 1999.
[21] R. Pelanek, “Fighting state space explosion: Review and
evaluation,” Formal Methods for Industrial Critical Systems,
vol. 5596, pp. 37–52, 2009.
[22] V. A. Patankar, A. Jain, and R. E. Bryant, “Formal veriﬁcation
of an ARM processor,” in Twelfth International Conference
On VLSI Design, 1999.
[23] M. Srivas and M. Bickford, “Formal veriﬁcation of a
pipelined microprocessor,” Software, IEEE, vol. 7, no. 5, pp.
52–64, 1990.
[24] J. R. Burch and D. L. Dill, “Automatic veriﬁcation of
pipelined microprocessor control,” in CAV ’94: Proceedings
of
the 6th International Conference on Computer Aided
Veriﬁcation. London, UK: Springer-Verlag, 1994, pp. 68–80.
[25] J. U. Skakkebaek, R. B. Jones, and D. L. Dill, “Formal veriﬁ-
cation of out-of-order execution using incremental ﬂushing,”
in CAV ’98: Proceedings of the 10th International Conference
on Computer Aided Veriﬁcation.
London, UK: Springer-
Verlag, 1998, pp. 98–109.
[26] M. N. Velev and R. E. Bryant, “Formal veriﬁcation of
superscale microprocessors with multicycle functional units,
exception, and branch prediction,” in DAC ’00: Proceedings
of the 37th Annual Design Automation Conference. ACM,
2000, pp. 112–117.
[27] R. Hosabettu, G. Gopalakrishnan, and M. Srivas, “Formal ver-
iﬁcation of a complex pipelined processor,” Formal Methods
System Design, vol. 23, no. 2, pp. 171–213, 2003.
[28] J. Bormann, S. Beyer, A. Maggiore, M. Siegel, S. Skalberg,
T. Blackmore, and F. Bruno, “Complete formal veriﬁcation of
TriCore2 and other processors,” in DVCon, February 2007.
77