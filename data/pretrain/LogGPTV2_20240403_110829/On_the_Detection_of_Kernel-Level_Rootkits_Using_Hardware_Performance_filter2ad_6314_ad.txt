for dedicating an isolated core [6] or isolated bus going to
that core.
Proﬁling Benchmark The proﬁling benchmark used in this
work could, for the most part, be used as part of a practical
detector. The benchmark would be executed periodically
and the captured HPC values run through the rootkit de-
tector. While a rootkit could detect that the benchmark
is running and disable its functionality prior to execution,
this would have a side-eﬀect of ensuring that the benchmark
is able to collect the actual, unmodiﬁed state (such as pro-
cesses and network connections) of the system. Overall, If
the rootkit detects that the benchmark is running and con-
tinues to hide its presence, then it can be detected using
the HPC values.
If the rootkit instead disables its hiding
techniques in order to evade HPC detection, then the data
collected by the benchmark will reveal the very things the
rootkit is trying to hide, hence allowing traditional detection
techniques to be used.
5.3 Evasion Techniques
The experiments performed in this work were done under
the assumption that the rootkit is not aware that it will be
proﬁled in this way. However, can rootkits adapt to evade
this technique using a mimicry attack [27]? The answer to
this depends on exactly how the rootkits impact the HPCs.
If the HPC impacts are due to things the rootkit cannot
change, such as the branches related to hooking, then the
answer is likely no. However, if the HPC impacts are simply
a reﬂection of the fact that diﬀerent code with diﬀerent HPC
characteristics is running when infected than when not in-
fected, then the answer might be yes. A rootkit author could
modify their code to maintain a similar average of HPC ef-
fects to that of the normal OS code. However, this sort of
attack would necessarily increase the number of instructions
executed (potentially signiﬁcantly), which makes it prone to
detection by a simpler approach, such as the one employed
by NumChecker [29]. A deeper discussion of mimicry at-
tacks is available in [27].
While our experiments reveal that HPCs are eﬀective for
detecting rootkits that make use of hooking, the revelation
that the DKOM based kit did not produce any signiﬁcant
HPCs is a sign that HPCs are not a panacea for rootkit
detection. We hypothesize that there may be other types
of rootkits, such as those making use of return-oriented pro-
gramming [10], which may also not be detected by this tech-
nique. However, it is heartening to observe both that the
vast majority of rootkits do employ hooking, and that it is
still not clear if non-hooking based attacks can be as pow-
erful [22] as their hooking counterparts. It seems unlikely
we will see them equal the functionality required by modern
attacks.
5.4 HPC Collection Methodology
In this work we collected HPCs inside a virtual machine
using Intel’s VTune, a tool primarily designed to assist de-
velopers in optimizing their programs. Other works devel-
oped a custom HPC collection mechanism [27] and/or ran
directly on bare hardware [6]. While the detection results
in those and this work indicate that the HPC collection was
eﬀective, the question remains regarding how much noise is
introduced by the various techniques. It would be interest-
ing to benchmark various HPC collection techniques in order
to gauge their accuracy. It will also be important going for-
ward to verify that HPC collection from within a VM has
similar levels of accuracy when compared to captures done
on bare metal.
4916. RELATED WORK
7. CONCLUSION
While the original rootkit results from Demme et al. [6]
(previously discussed in Section 2.3) were not very promis-
ing, our work shows signiﬁcantly higher accuracy when de-
tecting rootkits. We believe this diﬀerence in results can be
best explained by looking at the rootkits tested in each work.
Their experimentation included only two diﬀerent real-world
rootkits (one user-level and one kernel-level), while ours in-
cludes 100 variants of 5 real-world kernel rootkits. In addi-
tion, our 5 synthetic rootkits cover a variety of kernel rootkit
attack mechanisms, while the kernel rootkit they tested with
only employs one mechanism. In short, our testing included
a more comprehensive sample of kernel rootkit techniques.
This is not meant to be critical of their approach, instead
this work simply provides a much more thorough focus on
rootkits while their work focused on a broad range of mal-
ware. The fact that our testing was performed on Windows
while theirs was performed on Linux may also have some im-
pact, and future work should investigate those diﬀerences.
Very similar in principal to our work is Numchecker [29],
a system that detects Linux rootkits by looking for HPC
deviations during the execution of kernel functions. Their
approach is to count the number of retired instructions, re-
tired returns, and retired branches that occur during sys-
tem calls and compare those numbers to known good values
for that OS. The main diﬀerence between their work and
ours can be summarized as a manual vs machine learning
based approach. In Numchecker, the HPCs used for analy-
sis were manually chosen by the authors, while in our work
we apply machine learning techniques to determine the most
signiﬁcant HPCs. In addition, Numchecker detects rootkits
by evaluating whether the HPCs values collected during the
execution of a given kernel function deviate from experimen-
tally chosen values. Our work, in contrast, applies machine
learning to determine whether or not a rootkit is present. In
general, our work is a more systematic study of the impact
of rootkits on HPCs, and provides a more general approach
for using HPCs for rootkit detection.
A number of other works have also focused on using HPCs
for the detection of malware. Tang et al. [27] use unsuper-
vised machine learning to build proﬁles of normal applica-
tion’s HPC patterns, and then detect deviations. Their fo-
cus was on detecting user-level malware during exploitation
(as opposed to after infection). They demonstrated their
technique by detecting attacks against real vulnerabilities in
Internet Explorer 9, Adobe Reader, and Adobe Flash. They
achieved over 99% accuracy on detecting the exploitation
of these applications. Ozsoy et al. [18] propose the Mal-
ware Aware Processor (MAP), a hardware approach which
uses the same type of micro-architecture events measured
by HPCs in order to detect user-level malware in hardware.
The use of other microarchitectural features to detect ma-
licious activity has been studied as well. kBouncer [19] uses
the last branch recording (LBR) of Intel microprocessors to
detect the execution of ROP [24] code. A variety of works [4,
23, 32] have investigated the use of opcodes for the detection
of malware.
HPCs have also been applied in other ways to security.
Maurice et al. [15] use HPCs to reverse engineer the last level
cache in modern Intel processors, simplifying side-channel
attacks and covert channels. Malone et al. [14] design a
method for using HPCs to provide integrity checking of run-
ning applications.
In this work we provide an analysis of the applicability
of hardware performance counters to the detection of ker-
nel rootkits. We eﬀectively extend and expand on the pre-
liminary results found in Demme et al. [6], demonstrating
that HPCs can be used to detect rootkits with very high
accuracy (>99%). We use machine learning to identify the
16 most signiﬁcant HPCs for detecting Windows 7 rootkits
that make use of IRP and SSDT hooking to perform their
attacks. We also demonstrate that an SVM based classi-
ﬁer can be trained to detect new, real-world rootkits despite
only being trained on a set of synthetic rootkits with limited,
but speciﬁc, functionality. This work provides many of the
theoretical and practical underpinnings that will be required
in order to build a fully functional, HPC-based rootkit de-
tector.
Acknowledgments
This paper was made possible by NPRP grants 4-1593-1-260
and 8-1474-2-626 from the Qatar National Research Fund (a
member of Qatar Foundation). The statements made herein
are solely the responsibility of the authors.
The authors would also like to thank Aisha Hasan as well
as the reviewers for their helpful comments on this work.
8. REFERENCES
[1] Intel R(cid:13) Software Guard Extensions Programming
Reference, 2014. Accessed Apr. 2016 at
https://software.intel.com/sites/default/ﬁles/
managed/48/88/329298-002.pdf.
[2] S. Bandyopadhyay. A Study on Performance
Monitoring Counters in x86-Architecture. Indian
Statistical Institute, 2004.
[3] R. Berrendorf and H. Ziegler. PCL–the Performance
Counter Library: A Common Interface to Access
Hardware Performance Counters on Microprocessors,
Version 1.3, 1998.
[4] D. Bilar. Opcodes as Predictor for Malware.
International Journal of Electronic Security and
Digital Forensics, 1(2):156–168, 2007.
[5] S. Browne, J. Dongarra, N. Garner, G. Ho, and
P. Mucci. A Portable Programming Interface for
Performance Evaluation on Modern Processors.
International Journal of High Performance Computing
Applications, 14(3):189–204, 2000.
[6] J. Demme, M. Maycock, J. Schmitz, A. Tang,
A. Waksman, S. Sethumadhavan, and S. Stolfo. On
the Feasibility of Online Malware Detection with
Performance Counters. In Proceedings of the 40th
Annual International Symposium on Computer
Architecture (ISCA 2013), 2013.
[7] D. Evtyushkin, J. Elwell, M. Ozsoy, D. Ponomarev,
N. A. Ghazaleh, and R. Riley. Iso-x: A ﬂexible
architecture for hardware-managed isolated execution.
In 47th Annual IEEE/ACM International Symposium
on Microarchitecture (MICRO), pages 190–202. IEEE,
2014.
[8] M. Hall, E. Frank, G. Holmes, B. Pfahringer,
P. Reutemann, and I. H. Witten. The WEKA Data
Mining Software: An Update. ACM SIGKDD
explorations newsletter, 11(1):10–18, 2009.
492[9] G. Hoglund and J. Butler. Rootkits: Subverting the
[21] J. Rhee, R. Riley, D. Xu, and X. Jiang. Defeating
Windows kernel. Addison-Wesley Professional, 2006.
[10] R. Hund, T. Holz, and F. C. Freiling. Return-Oriented
Rootkits: Bypassing Kernel Code Integrity Protection
Mechanisms. In USENIX Security Symposium, pages
383–398, 2009.
[11] Intel Corporation. Intel(cid:114)VTune Ampliﬁer 2015. https:
//software.intel.com/en-us/intel-vtune-ampliﬁer-xe.
Last accessed January 2016.
[12] K.-J. Lee and K. Skadron. Using Performance
Counters for Runtime Temperature Sensing in
High-Performance Processors. In Parallel and
Distributed Processing Symposium, 2005. Proceedings.
19th IEEE International, pages 8–pp. IEEE, 2005.
[13] K. London, S. Moore, P. Mucci, K. Seymour, and
R. Luczak. The PAPI Cross-Platform Interface to
Hardware Performance Counters. In Department of
Defense Users’ Group Conference Proceedings, pages
18–21, 2001.
[14] C. Malone, M. Zahran, and R. Karri. Are Hardware
Performance Counters a Cost Eﬀective Way for
Integrity Checking of Programs. In Proceedings of the
Sixth ACM Workshop on Scalable Trusted Computing,
STC ’11, pages 71–76, New York, NY, USA, 2011.
ACM.
Dynamic Data Kernel Rootkit Attacks via
VMM-based Guest-Transparent Monitoring. In
Proceedings of International Conference on
Availability, Reliability and Security (ARES), pages
74–81. IEEE, 2009.
[22] R. Riley. A Framework for Prototyping and Testing
Data-Only Rootkit Attacks. Computers and Security,
37(0):62 – 71, 2013.
[23] I. Santos, F. Brezo, J. Nieves, Y. K. Penya, B. Sanz,
C. Laorden, and P. G. Bringas. Idea:
Opcode-sequence-based Malware Detection. In
Engineering Secure Software and Systems, pages
35–43. Springer, 2010.
[24] H. Shacham. The Geometry of Innocent Flesh on the
Bone: Return-into-libc without Function Calls (on the
x86). In Proceedings of the 14th ACM Conference on
Computer and Communications Security, pages
552–561. ACM, 2007.
[25] K. Singh, M. Bhadauria, and S. A. McKee. Real Time
Power Estimation and Thread Scheduling via
Performance Counters. ACM SIGARCH Computer
Architecture News, 37(2):46–55, 2009.
[26] B. Sprunt. The Basics of Performance-Monitoring
Hardware. IEEE Micro, pages 64–71, 2002.
[15] C. Maurice, N. Scouarnec, C. Neumann, O. Heen, and
[27] A. Tang, S. Sethumadhavan, and S. J. Stolfo.
A. Francillon. Reverse Engineering Intel Last-Level
Cache Complex Addressing Using Performance
Counters. In Proceedings of the 18th International
Symposium on Research in Attacks, Intrusions, and
Defenses (RAID 2015), pages 48–65. Springer
International Publishing, 2015.
[16] J. M. May. MPX: Software for Multiplexing Hardware
Performance Counters in Multithreaded Programs. In
Proceedings of the 15th International Parallel and
Distributed Processing Symposium. IEEE, 2001.
[17] Microsoft Corporation. Introduction to File System
Filter Drivers. https:
//msdn.microsoft.com/en-us/windows/hardware/
drivers/ifs/introduction-to-ﬁle-system-ﬁlter-drivers.
Last Accessed February 2017.
[18] M. Ozsoy, C. Donovick, I. Gorelik, N. Abu-Ghazaleh,
and D. Ponomarev. Malware-Aware Processors: A
Framework for Eﬃcient Online Malware Detection. In
IEEE 21st International Symposium on High
Performance Computer Architecture (HPCA 2015),
pages 651–661, 2015.
[19] V. Pappas, M. Polychronakis, and A. D. Keromytis.
Transparent ROP Exploit Mitigation Using Indirect
Branch Tracing. In USENIX Security, pages 447–462,
2013.
[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine Learning in
Python. Journal of Machine Learning Research,
12:2825–2830, 2011.
Unsupervised Anomaly-Based Malware Detection
Using Hardware Features. In Proceedings of Research
in Attacks, Intrusions and Defenses (RAID 2014),
2014.
[28] VirusTotal. VirusTotal-Free Online Virus, Malware
and URL Scanner. https://www.virustotal.com/. Last
accessed February 2016.
[29] X. Wang and R. Karri. NumChecker: Detecting
Kernel Control-Flow Modifying Rootkits by Using
Hardware Performance Counters. In Design
Automation Conference (DAC), 2013 50th
ACM/EDAC/IEEE, pages 1–7. IEEE, 2013.
[30] Z. Wang, X. Jiang, W. Cui, and P. Ning. Countering
Kernel Rootkits with Lightweight Hook Protection. In
Proceedings of the 16th ACM Conference on Computer
and Communications Security, CCS ’09, pages
545–554, New York, NY, USA, 2009. ACM.
[31] V. M. Weaver and S. McKee. Can Hardware
Performance Counters be Trusted? In IEEE
International Symposium on Workload
Characterization (IISWC 2008), pages 141–150. IEEE,
2008.
[32] G. Yan, N. Brown, and D. Kong. Exploring
Discriminatory Features for Automated Malware
Classiﬁcation. In Detection of Intrusions and Malware,
and Vulnerability Assessment, pages 41–61. Springer,
2013.
493