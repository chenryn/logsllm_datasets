when P = 1). One key challenge is how to hide the graph
structure G during computation.
Alternative graph representation: Our oblivious algorithms
require an alternative representation of graphs,
that does
not disambiguate between edges and vertices. Both ver-
tices and edges are represented as tuples of
the form:
(cid:6)u, v, isVertex, data(cid:7). In particular, each vertex u is represented
by the tuple: (cid:6)u, u, 1, data(cid:7); and each edge (u, v) is represented
by the tuple: (cid:6)u, v, 0, data(cid:7). We represent a graph as a list of
tuples, i.e., G := (ti)i∈[|V|+|E|] where each ti is of the form
(cid:6)u, v, isVertex, data(cid:7).
Terminology. For convenience, henceforth, we refer to each
383383
edge tuple as a black cell, and each vertex tuple as a white
cell in the list representing graph G.
Algorithm description. We now describe the single-processor
oblivious implementation of GraphSC primitives. The formal
description of the implementation is provided in Algorithm 2.
We also provide an example of the Scatter and Gather opera-
tions in Figure 3b, for a very simple graph structure shown in
Figure 3a.
Apply. The Apply operation is straightforward to make
oblivious under our new graph representation. Essentially, we
make a linear scan over the list G. During this scan, we apply
the function fA to each vertex tuple in the list, and a dummy
operation to each edge tuple.
Scatter. Without loss of generality, we use b = “out” as
an example. The algorithm for b = “in” is similar. The Scatter
operation then proceeds in two steps, illustrated in the ﬁrst
three lines of Figure 3b.
Step 1: Oblivious sort: First, perform an oblivious sort on
G, so that tuples with the same source vertex are grouped
together. Moreover, each vertex should appear before all the
edges originating from that vertex.
Step 2: Propagate: Next, in a single linear scan, update the
value of each black (i.e., edge) cell with the nearest preceding
white cell (i.e., vertex), by applying the fS function.
Gather. Again, without loss of generality, we will use b =
“in” as an example. The algorithm for b = “out” is similar.
Gather proceeds in a fashion similar to Scatter in two steps,
illustrated in the last three lines of Figure 3b.
Step 1: Oblivious sort: First, perform an oblivious sort on G,
so that tuples with the same destination vertex appear adjacent
to each other. Further, each vertex should appear after the list
of edges ending at that vertex.
Step 2: Aggregate: Next, in a single linear scan, update the
value of each white cell (i.e., vertex) with the ⊕-sum of the
longest preceding sequence of black cells. In other words,
values on all edges ending at some vertex v are now aggregated
into the vertex v.
Efﬁciency. Let M := |V| + |E| denote the total number of
tuples. Assume that the data on each vertex and edge is of O(1)
in length, and hence each fS, fA, and ⊕ operator is of O(1)
cost. Clearly, an Apply operation can be performed in O(M )
time. Oblivious sort can be performed in O(M log M ) time
using [64], [65] while propagate and aggregate take O(M )
time. Therefore, a Scatter and a Gather operation each runs in
time O(M log M ).
D. Parallel Oblivious Algorithms for GraphSC
We now describe how to parallelize the sequential obliv-
ious primitives Scatter, Gather, and Apply described in Sec-
tion III-C. We will describe our parallel algorithms assuming
that
there are a sufﬁcient number of processors, namely
|V| + |E| processors. Later in Section III-E, we describe
some practical optimizations when the number of processors
is smaller than |V| + |E|.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:55 UTC from IEEE Xplore.  Restrictions apply. 
1
2
3
4
(a) Graph G.
,
S
)
”
t
u
o
“
=
b
f
,
G
(
r
e
t
t
a
c
S
,
+
,
G
(
r
e
h
t
a
G
)
”
n
i
“
=
b
1, D1 
2, D2 
3, D3 
4, D4 
(1,2), D1,2 
(1,3), D1,3 
(1,4), D1,4 
(2,3), D2,3 
(4,3), D4,3 
O-Sort 
1, D1 
(1,2), D1,2 
(1,3), D1,3 
(1,4), D1,4 
2, D2 
(2,3), D2,3 
3, D3 
4, D4 
(4,3), D4,3 
fS(D1, D1,3) 
fS(D1, D1,2) 
fS(D1, D1,4) 
(1,2), D’1,2  (1,3), D’1,3  (1,4), D’1,4 
2, D2 
fS(D2, D2,3) 
(2,3), D’2,3 
1, D1 
3, D3 
4, D4 
fS(D4, D4,3) 
(4,3), D’4,3 
O-Sort 
1, D1 
(1,2), D’1,2 
2, D2 
(2,3), D’1,3 
(1,3), D’2,3 
(4,3), D’4,3 
3, D3 
(1,4), D’1,4 
1, D1 
(1,2), D’1,2 
D2||D1,2 
2, D’2 
D3||(D’1,3 + D’2,3 + D’4,3) 
(2,3), D’1,3 
(1,3), D’2,3 
(4,3), D’4,3 
3, D’3 
(1,4), D’1,4 
4, D4 
D4||D’1,4 
4, D’4 
(b) Transformations of list representing graph G.
Fig. 3: Oblivious Scatter and Gather on a single processor. We apply a Scatter followed by a Gather. Scatter: Graph tuples are
sorted so that edges are grouped together after the outgoing vertex. e.g. D1,2, D1,3, D1,4 are grouped after D1. Then, in a single
pass, all edges are updated. e.g. D1,3 is updated as fS(D1, D1,3). Gather: Graph tuples are sorted so that edges are grouped
together before the incoming vertex. e.g. D(cid:5)
4,3 are grouped before D3. Then, in a single pass, all vertices compute
the aggregate. e.g. D(cid:5)
⊕ D(cid:5)
1,3
, D(cid:5)
3 = D3||D(cid:5)
, D(cid:5)
2,3
4,3.
⊕ D(cid:5)
2,3
1,3
First, observe that the Apply operation can be parallelized
trivially. We now demonstrate how to make the Scatter and
Gather operations oblivious. Recall
that both Scatter and
Gather start with an oblivious sort, followed by either an ag-
gregate or a propagate operation as described in Section III-C.
The oblivious sort is a log(|V| + |E|)-depth circuit [66], and
therefore is trivial to parallelize (by parallelizing directly at
the circuit level).
It thus sufﬁces to show how to execute the aggregate and
propagate operations in parallel. To highlight the difﬁculty
behind the parallelization of these operations, recall that in a
data-oblivious execution, a processor needs to, e.g., aggregate
values by accessing the list representing the graph at ﬁxed
locations, which do not depend on the data. However, as seen
in Figure 3b, the positions of black (i.e., edge) cells whose
values are to be aggregated and stored in white (i.e., vertex)
cells clearly depend on the input (namely, the graph G).
Parallelizing the aggregate operation. Recall that an aggre-
gate operation updates the value of each white cell with values
of the longest sequence of black cells preceding it. For ease of
exposition, we ﬁrst present a few deﬁnitions before presenting
our parallel aggregate algorithm.
Deﬁnition 1. Longest Black Preﬁx: For j ∈ {1, 2, . . . , |V| +
|E|}, the longest black preﬁx before j, denoted LBP[1, j), is
deﬁned to be the longest consecutive sequence of black cells
before j, not including j.
Similarly, let 1 ≤ i < j ≤ |V| + |E|, we use the notation
LBP[i, j) to denote the longest consecutive sequence of black
cells before j, constrained to the subarray G[i . . . j) (index i
being inclusive, and index j being exclusive).
Deﬁnition 2. Longest Preﬁx Sum: Let 1 ≤ i < j ≤ |V| +|E|,
we use the notation LPS[i, j) to denote the “sum” (with respect
to the ⊕ operator), of LBP[i, j).
Abusing notation, we treat LPS[i, j) is an alias for
LPS[1, j) if i < 1. The parallel aggregate algorithm is
described in Figure 4. The algorithm proceeds in a total of
log(|V| + |E|) time steps. In each intermediate time step τ, a
processor j ∈ {1, 2, . . . ,|V| + |E|} computes LPS[j − 2τ , j).
As a result, at the conclusion of these log(|V| + |E|) steps,
each processor j has computed LPS[1, j).
This way, by time τ, all processors compute the LPS values
for all segments of length 2τ . Now, observe that LPS[j−2τ , j)
can be computed by combining LPS[j − 2τ , j − 2τ−1) and
LPS[j − 2τ−1, j) in a slightly subtle (but natural) manner
as described in Figure 4. Intuitively, at each τ, a segment is
aggregated with the immediately preceding segment of equal
size only if a white cell has not be encountered so far.
At
the end of log(|V| + |E|) steps, each processor j
whose cell is white, appends its data to the aggregation result
LPS[1, j) – this part is omitted from Figure 4 for simplicity.
that,
Parallelizing the propagate operation. Recall
in a
propagate operation, each black cell updates its data with
the data of the nearest preceding white cell. The propagate
operation can be parallelized in a manner similar to aggregate.
In fact, we can even express a propagate operation as a
special aggregate operation as follows: Initially, every black
cell stores (i) the value of the preceding white cell if a white
cell precedes; and (ii) −∞ otherwise. Next, we perform an
aggregate operation where the ⊕ operator is deﬁned to be
the max operator. At the end of log |V| + |E| time steps, each
processor has computed LPS[1, j), i.e., the value of the nearest
white cell preceding j. Now if cell G[j] is black, we can
overwrite its data entry with LPS[1, j).
384384
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:55 UTC from IEEE Xplore.  Restrictions apply. 
Operation
Scatter
Gather
Apply
Seq. insecure
O(|E|)
O(|E|)
O(|V|)
Total work
Par. insecure
Blowup
O(log |V|)
O(|E| log dmax) O(|E| log |V|) O(logdmax
O(|E|/|V|)
Par. oblivious
O(|E| log |V|)
O(|E|)
O(|V|)
O(|E|)
Par. insecure
|V|) O(log dmax)
O(1)
O(1)
Parallel time
Par. oblivious
O(log |V|)
O(log |V|)
O(1)
Blowup
O(log |V|)
|V|)
O(logdmax
O(1)
TABLE I: Complexity of our parallel oblivious algorithms assuming |E| = Ω(|V|). |V | denotes the number of vertices, and
|E| denotes the number of edges. dmax denotes the maximum degree of a vertex in the graph. Blowup is deﬁned as the ratio of the
parallel oblivious algorithm with respect to the best known parallel insecure algorithm. We assume that the data length on each
vertex/edge is upper-bounded by a known bound D, and for simplicity we omit a multiplicative factor of D from our asymptotical
bounds. In comparison with Theorem 1, in this table, some |V| terms are absorbed by the |E| term since |E| = Ω(|V|).
(cid:3)
LPS[j − 1, j) :=
Parallel Aggregate:
/* For convenience, assume that for i ≤ 0, G[i] is white; and similarly for i ≤ 0, LPS[i, j) is an alias for LPS[1, j) */.
Initialize: Every processor j computes:
G[j − 1].data
1⊕
(cid:3)
LPS[j − 2τ , j − 2τ−1) ⊕ LPS[j − 2τ−1, j)
LPS[j − 2τ−1, j)
Main algorithm: For each time step τ := 1 to log(|V| + |E|) − 1: each processor j computes
LPS[j − 2τ , j) :=
existswhite[j − 2τ , j) := existswhite[j − 2τ , j − 2τ−1) or existswhite[j − 2τ−1)
if existswhite[j − 2τ−1, j) = False
o.w.
if G[j − 1] is black
o.w.
if G[j − 1] is black
o.w.
existswhite[j − 1, j) :=
•
•
;
(cid:3)
False
True
Fig. 4: Performing the aggregate operation (Step 2 of Gather) in parallel, assuming sufﬁcient number of processors with a
shared memory to store the variables.
Cost analysis. Recall our standing assumption that the maxi-
mum data length on each tuple is O(1). It is not hard to see
that the parallel runtime of both the aggregate and propagate
operations is O(log(|V| + |E|)). The total amount of work for
both aggregate and propagate is O((|V| +|E|)· log(|V| +|E|)).
Based on this, we can see that Scatter and Gather each takes
O(log(|V|+|E|)) parallel time and O((|V|+|E|)·log(|V|+|E|))
total amount of work. Obviously, Apply takes O(1) parallel
time and O(|V| + |E|) total work.
Table I illustrates the performance of our parallel oblivious
algorithms for the common case when |E| = Ω(|V|), and
the blowup in comparison with a parallel insecure version.
Notice that in the insecure world, there exists a trivial O(1)
parallel-time algorithm to evaluate Scatter and Apply oper-
ations. However, in the insecure world, Gather would take
O(log(|E| + |V|)) parallel time to evaluate the ⊕-sum over
|E| +|V| variables. Notice also that the |V| term in the asymp-
totic bound is absorbed by the |E| term when |E| = Ω(|V|).
The above performance characterization is summarized by the
following theorem:
Let M := |V| + |E| denote the graph size. There exists a
parallel oblivious algorithm for programs in the GraphSC
model, where each Scatter or Gather operation requires
O(log M ) parallel
time and O(M log M ) total work; and
each Apply operation requires O(1) parallel time and O(M )
total amount of work.
Theorem 1 (Parallel oblivious algorithm for GraphSC):
E. Practical Optimizations for Fixed Number of Processors
The parallel algorithm described in Figure 4 requires M =
|V|+|E| processors. In practice, however, for large datasets, the
P
number of processors P may be smaller than M. Without loss
of generality, suppose that M is a multiple of P . In this case, a
naïve approach is for each processor to simulate M
P processors,
resulting in M log M
parallel time, and M log M total amount
of work. We propose the following practical optimization that
can reduce the total parallel time to O( M
P +log P ), and reduce
the total amount of work to O(P log P + M ).
We assign to each processor a consecutive range of cells.
Suppose that processor j gets range [sj, tj] where sj = (j −
1)· M
P +1 and tj = j· M
P . In our algorithm, each processor will
compute LPS[1, sj), and afterwards, in O(M/P ) time-steps,
it can (sequentially) compute LPS[1, i) for every sj ≤ i ≤ tj.
Every processor then computes LPS[1, sj) as follows
• First, every processor sequentially computes LPS[sj, tj + 1)
and existswhite[sj, tj + 1).
• Now, assume that every processor started with a single value
LPS[sj, tj + 1) and a single value existswhite[sj, tj + 1).
Perform the parallel aggregate algorithm on this array of length
P .
Sparsity of communication. In a distributed memory setting
where memory is split across the processors, the conceptual
shared memory is in reality implemented by inter-process
communication. An additional advantage of our algorithm
is that each processor needs to communicate with at most
O(log P ) other processors – this applies to both the oblivious
sort step, and the aggregate or propagate steps. In fact, it
is not hard to see that the communication graph forms a
hypercube [67]. The sparsity of the communication graph is
highly desirable.
Let M := |V|+|E| and recall that the maximum amount of
385385
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:55 UTC from IEEE Xplore.  Restrictions apply. 
Processors
…
Oblivious accesses
Memory
(a) Architecture for parallel oblivious algorithms.
Secret-shared memory
…
…
Oblivious accesses, G-G comm.
…
…
…
Garblers
Evaluators
Oblivious accesses, E-E comm.
…
…
…
Secret-shared memory
(b) Architecture for parallel secure computation.
Fig. 5: From parallel oblivious algorithms to parallel secure computation.
data on each vertex or edge is O(1). The following corollary
summarizes the above observations:
Corollary 1 (Bounded processors, distributed memory.):
When P < M, there exists a parallel oblivious algorithm for
programs in the GraphSC model, where (a) each processor
stores O(M/P ) amount of data; (b) each Scatter or Gather
operation requires O(M/P + log P ) parallel
time and
O(P log P + M ) total work;
(c) each Apply operation
requires O(1) parallel time and O(|E| + |V|) total amount
of work; and (d) each processor sends messages to only