# Title: Validating Datacenters at Scale

## Authors
- Karthick Jayaraman
- Nikolaj Bjørner
- Jitu Padhye
- Amar Agrawal
- Ashish Bhargava
- Paul-Andre C. Bissonnette
- Shane Foster
- Andrew Helwer
- Mark Kasten
- Ivan Lee
- Anup Namdhari
- Haseeb Niaz
- Aniruddha Parkhi
- Hanukumar Pinnamraju
- Adrian Power
- Neha Milind Raje
- Parag Sharma

## White Paper
### Data Center Testing: A Holistic Approach
**May 2009**

---

## Table of Contents
1. **Executive Summary** ................................................... 3
2. **Testing Basics** ............................................................ 3
3. **Virtualization** ............................................................. 7
4. **Fibre Channel over Ethernet (FCoE)** ........................... 9
5. **40- and 100-Gbit/s Ethernet** ...................................... 11
6. **Putting It All Together** .............................................. 15

---

## Executive Summary
Data centers today are larger, faster, and more complex than ever before. New technologies such as virtualization, Fibre Channel over Ethernet (FCoE), and 40/100 Gbit/s Ethernet aim to help organizations move multiple traffic types—data, storage, video, and voice—onto a single, converged core.

The old adage "you can't manage what you can't measure" underscores the importance of validation and performance assessment of these new technologies. In a data center context, this means testing each technology not just in isolation but also in conjunction with other data center components, both old and new. The key question is: As I grow my data center, how can I validate that all components will work together as a coherent whole?

This white paper aims to help network professionals understand the issues involved in data center validation and performance benchmarking. After a review of basic testing concepts and industry-standard testing methodologies, this document discusses each of the new data center technologies, highlighting the major testing challenges for each. The paper concludes with an introduction to holistic testing of the data center, an end-to-end approach that will help organizations develop confidence in the new technologies, enabling data centers to grow larger and more cost-effective over time.

---

## Testing Basics
Before delving into the new technologies driving data center growth, it is helpful to review some basic testing concepts. These concepts will be relevant when discussing emerging data center technologies later in this document.

A sound benchmark must meet four basic criteria: repeatability, reproducibility, stressfulness, and meaningfulness.

### Repeatability
Repeatability means that multiple iterations of the same test, conducted on the same test bed, should produce similar results. A test that produces wildly different results with each iteration is of little use.

### Reproducibility
Reproducibility refers to situations where the same test is run on different test beds. For many organizations, it is common practice for teams at multiple locations to work on the same set of benchmarks. For example, test engineers working in San Jose, Beijing, and Bangalore should all be able to produce similar measurements, assuming they use the same test instrument and system under test (SUT) and follow the same procedures.

Reproducibility can be challenging in benchmarking. When test engineers at different sites obtain different measurements, the first step should be to verify that all sites have the same software and hardware versions, both in the test instrument and SUT, and follow the exact same procedures.

### Stressfulness
A benchmark is stressful only if it finds the limits of system performance. Throughput tests, for example, seek to find the highest rate at which a device forwards traffic. In stress testing, the goal is to have a successful iteration (the device drops no frames) followed by a failed iteration (the device drops frames); this is the limit of system performance.

A common refrain when reviewing the results of stress tests is that such benchmarks do not represent "real-world" conditions. However, the goal of stress testing is to find system limits, not to simulate real-world conditions.

### Meaningfulness
The final goal in benchmarking is to produce a meaningful set of results, which can be the most difficult to achieve. Tests of networking devices produce vast quantities of data, not all of which will be relevant to the task at hand.

For example, latency tests of two routers might produce measurements of 10 and 100 microseconds. If the routers are deployed on opposite ends of a transcontinental link, with propagation delays well into the tens of milliseconds or higher, a difference of 90 µsec is not meaningful. Conversely, if the routers are deployed 1 meter apart in a data center and carry financial information whose timeliness is worth a million dollars for each microsecond, a tenfold increase in latency is very meaningful. The key point is that measurements themselves have no intrinsic value; their meaning depends on the use case being tested.

Beyond these principles, a useful rule in network device benchmarking is to never assume correct behavior on the part of the SUT. Especially when assessing new network hardware and software, faulty implementations often lead to suboptimal test results. The SUT may also report incorrectly on its status, hence the need for test instruments that provide externally observable validation (or otherwise) of system performance.

Over the years, test engineers have codified these rules and other useful test techniques in a series of industry-standard testing documents issued by the Internet Engineering Task Force (IETF) as Requests for Comments (RFCs). The most relevant RFCs for data center device testing are as follows:

- **RFCs 1242 and 2544**: Describe terminology and methodology, respectively, for router testing. These are foundational documents for network device measurement.
- **RFCs 2285 and 2889**: Present terminology and methodology, respectively, for Ethernet switch testing. These RFCs also introduce basic traffic patterns essential in data center testing.
- **RFCs 2432 and 3918**: Offer terminology and methodology, respectively, for IP multicast testing. Multicast testing is crucial for data center devices, especially for video and triple-play services.
- **RFCs 2647 and 3511**: Document terminology and methodology, respectively, for firewall performance measurement. These documents introduce the concept of goodput, a more meaningful metric than throughput for measuring TCP traffic through a firewall.
- **Other RFCs**: RFC 5180 extends RFC 2544 with IPv6-specific tests. RFC 4814 recommends pseudorandom traffic patterns for testing and presents formulas for calculating overhead introduced by bit- and byte-stuffing on SONET/SDH links. RFC 4689 describes terminology used in testing network-layer quality-of-service (QoS) mechanisms.

RFCs 2285 and 2889 discuss traffic patterns relevant for testing data center devices. RFC 2285 defines two "traffic orientations" and three "traffic distributions."

- **Traffic Orientations**: Unidirectional and bidirectional traffic. In a unidirectional test pattern, one test interface offers traffic destined for another interface. In a bidirectional pattern, every receiving interface is also a transmitting interface.
- **Traffic Distributions**: Non-meshed, partially meshed, and fully meshed.
  - **Non-meshed Pattern**: Every pair of transmitting and receiving ports on the test instrument is mutually exclusive. This traffic may be unidirectional or bidirectional.
  - **Partially Meshed Distribution**: One set of test interfaces offers traffic to a different set of interfaces, but not to any interfaces within its own set. Traffic may be unidirectional or bidirectional.
  - **Fully Meshed Pattern**: All test interfaces offer traffic destined to all other test interfaces. Fully meshed patterns put the most stress on a switch or router fabric and are the preferred distribution for testing these devices.

These basic testing concepts will play a part in data center device benchmarking. The remainder of this document will cover how these concepts apply to three new technologies in the data center: Virtualization, Fibre Channel over Ethernet, and 40- and 100-Gbit/s Ethernet.

---

## Virtualization
Virtualization, which has long brought benefits to servers, is now coming to networked devices. Three factors are driving the growth of virtualization in data centers:

1. **Consolidation**: As organizations move resources into a few large data centers, server virtualization is a logical choice.
2. **Convergence**: Data center backbones now carry not only Ethernet data but also storage traffic, encapsulated using Fibre Channel over Ethernet (FCoE). Additionally, data centers handling streaming video and triple-play applications are moving from multiple distinct networks onto a single converged core.
3. **Virtual Networking Devices**: New virtual switches and appliances lack physical points of attachment, introducing a new requirement for virtual testing capabilities.

The rapid growth in virtual server deployments also heightens the need for scalability testing. Where network architects previously specified a maximum of four to eight virtual machine (VM) instances per physical server, new virtualization products will push that number to 64 VMs or beyond. Considering that a standard rack holds up to 42 physical servers, and that there may be hundreds or thousands of racks in some data centers, the implications are significant for network traffic.

Moreover, a VM instance often uses more bandwidth than a physical server. There is extra traffic involved in managing a virtual machine from a central location. Many data centers use products such as VMware Vmotion to move virtual machines within the data center, enhancing uptime and reliability but also generating considerable network load.

Testing virtual network devices and servers poses several interesting new questions:

- **How can a test instrument attach to a virtual network device?** For many types of tests, a connection to the physical server hosting a virtual switch will not be sufficient. One physical interface may handle traffic for dozens of VM instances, making it difficult to isolate and measure the performance of each VM instance. What's needed is to virtualize the capabilities of the test instrument. A virtual test instrument resides in software and runs inside the physical machine hosting virtual network and server instances. From the standpoint of the virtual network device, a test port looks exactly the same as it would in the physical world.

- **Can test instruments on virtual machines be trusted?** The concept of test instrumentation running in software is not new; software-based networking test tools predate hardware-based instruments by decades. Unlike hardware-based instruments, however, software-based test tools often produce measurements that say as much or more about underlying components—the networking stack, host operating system, drivers, and network interface card—as the system they purportedly measure. Software-based tools can also produce results that are either nonrepeatable or nonreproducible.

One strategy to ensure measurements can be trusted is to implement the entire test instrument, including software-based emulation of hardware components, in software. This approach requires a more rigorous system design but provides clear benefits: By emulating the entire test instrument in software, the instrument’s measurements are far less dependent on extraneous factors and produce more meaningful measurements than software-only tools.

- **Do virtual and physical switches offer comparable performance?** Line-rate throughput and low latency and jitter have long been the hallmarks of physical Ethernet switches, but their virtual counterparts may not compare. Tests of early virtual switches show frame loss with offered loads as low as 50 Mbit/s. Moreover, these tests involved just a single pair of interfaces on a single virtual switch; in contrast, the standard practice for switch and router testing is to attach test interfaces to all switch ports and generate traffic in a fully meshed pattern—a far more stressful test pattern than using just a single port pair.

Even if virtual switches will never handle loads as heavy as physical switches, it is still important to conduct stress tests to describe the limits of system performance. As discussed in the "Testing Basics" section of this document, test engineers for years have relied on industry-standard methods to ensure reliable and meaningful results.

---

This concludes the introductory sections of the white paper. The following sections will delve into the specific testing challenges and methodologies for Fibre Channel over Ethernet (FCoE) and 40- and 100-Gbit/s Ethernet.