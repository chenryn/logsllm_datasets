title:Validating datacenters at scale
author:Karthick Jayaraman and
Nikolaj Bjørner and
Jitu Padhye and
Amar Agrawal and
Ashish Bhargava and
Paul-Andre C. Bissonnette and
Shane Foster and
Andrew Helwer and
Mark Kasten and
Ivan Lee and
Anup Namdhari and
Haseeb Niaz and
Aniruddha Parkhi and
Hanukumar Pinnamraju and
Adrian Power and
Neha Milind Raje and
Parag Sharma
White Paper 
Data Center Testing: 
A Holistic Approach 
May 2009 
Data Center Testing: A Holistic Approach 
T A B L E    O F    C O N T E N T S 
Executive summary......................................................................................................... 3 
Testing basics.................................................................................................................. 3 
Virtualization .................................................................................................................. 7 
Fibre Channel over Ethernet (FCoE) .............................................................................. 9 
40- and 100-Gbit/s Ethernet.......................................................................................... 11 
Putting it all together..................................................................................................... 15 
Spirent Communications White Paper 
2 
Data Center Testing: A Holistic Approach 
Executive summary 
Data centers today are larger, faster and more complex than ever. New technologies such 
as virtualization, Fibre Channel over Ethernet and 40/100 gigabit Ethernet aim to help 
organizations move multiple traffic types – data, storage, video, and voice – onto a single, 
converged core. 
Bearing in mind the old adage that “you can’t manage what you can’t measure,” 
validation and performance assessment of these new technologies are vital first steps in 
implementing these new technologies. In a data center context, that means testing each of 
these technologies not just by itself but also in concert with many other data center 
components old and new. In short, the key question is: As I grow my data center, how can 
I validate that all components will work together as a coherent whole? 
This white paper aims to help network professionals understand the issues involved in 
data center validation and performance benchmarking. After a review of basic testing 
concepts and industry-standard testing methodologies, this document discusses each of 
the new data center technologies in turn, with detailed coverage of the major testing 
challenges for each.  
This paper concludes with an introduction to holistic testing of the data center, an end-to-
end approach that will help organizations develop confidence in the new technologies 
that will allow data centers to grow larger and more cost-effective over time. 
Testing basics 
Before diving in to the new technologies driving data center growth, it may be helpful to 
review some basic testing concepts. These concepts will come into play in the coverage 
of emerging data center technologies later in this document. 
A sound benchmark must meet four basic criteria: It must be repeatable, reproducible, 
stressful and meaningful.  
Repeatability means multiple iterations of the same test, conducted on the same test bed, 
should produce similar results. Obviously a test that produces wildly different results 
with each iteration is of little use. 
Reproducibility is similar to repeatability, but refers to situations where the same test is 
run on different test beds. For many organizations, it is common practice for teams at 
multiple locations to work on the same set of benchmarks. For example, test engineers 
working in San Jose, Beijing and Bangalore all should be able to produce similar 
measurements, assuming all use the same test instrument and system under test (SUT) 
and follow the same procedures. 
As an aside, reproducibility can be an elusive goal in benchmarking. When test engineers 
at different sites obtain different measurements, the first step should be to verify that all 
Spirent Communications White Paper 
3 
Data Center Testing: A Holistic Approach 
sites have the same software and hardware versions, both in the test instrument and SUT, 
and follow the exact same procedures.  
A benchmark is stressful only if it finds the limits of system performance. Throughput 
tests, for example, seek to find the highest rate at which a device forwards traffic. In 
stress testing, the goal is to have a successful iteration (the device drops no frames) 
followed by a failed iteration (the device drops frames); this is the limit of system 
performance. 
A commonly heard refrain when reviewing the results of stress tests is that such 
benchmarks do not represent “real world” conditions. Leaving aside for the moment that 
every network has its own definition of reality, such complaints miss a key point. The 
goal of stress testing is to find system limits, not to find some definition of reality. 
The final goal in benchmarking, coming up with a meaningful set of results, can be the 
most difficult to achieve. Tests of networking devices produces vast quantities of data, 
not all of which will be relevant to the task at hand.  
Consider an example where latency tests of two routers produce measurements of 10 and 
100 microseconds. If the routers being tested will be deployed on opposite ends of a 
transcontinental link, with the speed of light introducing propagation delays well into the 
tens of milliseconds or higher, a difference of 90 µsec is not at all meaningful. 
On the other hand, if the routers will be deployed 1 meter apart in a data center, and will 
carry financial information whose timeliness is worth a million dollars for each 
microsecond, a tenfold increase in latency is very meaningful. The key point is that 
measurements themselves have no intrinsic value. What makes measurements meaningful 
is how they apply to the use case being tested.  
Above and beyond these principles, a useful rule in network device benchmarking is to 
never assume correct behavior on the part of the SUT. Especially when assessing new 
network hardware and software, faulty implementations often lead to suboptimal test 
results. The SUT also may report incorrectly on its status, hence the need for test 
instruments that provide externally observable validation (or otherwise) of system 
performance.  
Over the years, test engineers have codified these rules and other useful test techniques in 
a series of industry-standard testing documents issued by the Internet Engineering Task 
Force (IETF) as requests for comments (RFCs). The most relevant RFCs for data center 
device testing are as follows: 
•  RFCs 1242 and 2544 describe terminology and methodology, respectively, for 
router testing. These are foundation documents for network device measurement, 
and many other testing RFCs refer to concepts introduced in these RFCs. 
Spirent Communications White Paper 
4 
Data Center Testing: A Holistic Approach 
•  RFCs 2285 and 2889 present terminology and methodology, respectively, for 
Ethernet switch testing. These RFCs also introduce basic traffic patterns that are 
essential in data center testing, as discussed later in this section. 
•  RFCs 2432 and 3918 offer terminology and methodology, respectively, for IP 
multicast testing. Multicast testing belongs in any assessment of data center 
devices, especially considering it is a common transport for video and triple-play 
services. These documents also describe tests that blend unicast and multicast 
traffic.  
•  RFCs 2647 and 3511 document terminology and methodology, respectively, for 
firewall performance measurement. These documents introduce the concept of 
goodput, a far more meaningful metric than throughput when measuring the 
performance of loss-tolerant TCP traffic through a firewall (or performance of 
any layer-4/layer-7 device, for that matter). 
•  Several other RFCs also may be useful in assessing performance of data center 
devices. RFC 5180 extends RFC 2544 with IPv6-specific tests. RFC 4814 
recommends pseudorandom traffic patterns for use in testing, unlike the artificial 
and static patterns generated by many test instruments, and presents formulas for 
calculating the overhead introduced by bit- and byte-stuffing on SONET/SDH 
links. And RFC 4689 describes terminology used in testing network-layer quality-
of-service (QoS) mechanisms. The benchmarking working group is reviewing 
several other testing documents, all currently in draft status. 
As noted, RFCs 2285 and 2889 discuss traffic patterns relevant for testing data center 
devices. RFC 2285 defines two “traffic orientations” and three “traffic distributions.”  
The traffic orientations simply refer to unidirectional and bidirectional traffic. In a 
unidirectional test pattern, one test interface offers traffic destined for another interface. 
In a bidirectional pattern, every receiving interface is also a transmitting interface. 
The RFCs’ three traffic distributions are “non-meshed,” “partially meshed” and “fully 
meshed.”   
In a non-meshed pattern, as shown below, every pair of transmitting and receiving ports 
on the test instrument is mutually exclusive. This traffic may be unidirectional (as with 
interfaces 1 and 3) or bidirectional (as with interfaces 2 and 4). Some test instruments 
refer to this as a “port pair” pattern: 
Spirent Communications White Paper 
5 
Data Center Testing: A Holistic Approach 
In a partially meshed distribution, sometimes referred to as a “backbone” pattern, one set 
of test interfaces offers traffic to a different set of interfaces, but not to any interfaces 
within its own set. Again, traffic may be unidirectional (interface 1) or bidirectional 
(interfaces 2, 3 and 4): 
In a fully meshed pattern, all test interfaces offer traffic destined to all other test 
interfaces. With fully meshed patterns, all flows are unidirectional; they only appear to be 
bidirectional in the figure below because each test interface sends and receive traffic from 
all other interfaces. A fully meshed pattern puts the most stress on a switch or router 
fabric and thus is the preferred traffic distribution when testing these devices. 
Spirent Communications White Paper 
6 
Data Center Testing: A Holistic Approach 
The illustrations given here use only the simplest possible traffic distributions. Within 
each test interface, it is possible to generate thousands of unique traffic streams, perhaps 
with varying traffic distributions. These complex patterns can be useful in assessing 
quality of service (QoS) mechanisms including prioritization of Fibre Channel over 
Ethernet (FCoE) traffic, as discussed later in this document. 
RFC 2285 introduces one more concept that will be useful in data center benchmarking: 
It distinguishes between intended load and offered load. As the name implies, intended 
load is simply the rate at which the tester would like to offer traffic.  
However, because of congestion control mechanisms such as 802.1Qbb priority flow 
control, there may be a difference between the transmission rate the tester expects to use, 
and what the test instrument actually uses. The latter is the offered load. In some cases 
offered load may be lower than the intended load because the transmitting test interface 
was throttled during the test by a congestion control mechanism.  
Each of these basic testing concepts will play a part in data center device benchmarking. 
The remainder of this document will cover how these concepts apply to three new 
technologies at work in the data center: Virtualization, Fibre Channel over Ethernet and 
40- and 100-Gbit/s Ethernet.  
Virtualization 
Virtualization, which for years has brought benefits to servers, is now coming to 
networked devices. Three factors are driving the growth of virtualization in data centers. 
First is consolidation: As organizations move resources into a few large data centers, 
server virtualization is a logical choice.  
Second is convergence: Data center backbones now carry not only Ethernet data but also 
storage traffic, encapsulated using Fibre Channel over Ethernet (FCoE). In addition, data 
centers handling streaming video and triple-play applications also are moving from 
multiple distinct networks onto a single converged core.  
Third is virtual networking devices: These new virtual switches and appliances lack 
physical points of attachment, introducing a new requirement for virtual testing 
capabilities. 
The explosive growth in virtual server deployments also heightens the need for scalability 
testing. Where network architects previously specified a maximum four to eight virtual 
machine (VM) instances per physical server, new virtualization products will push that 
number to 64 VMs or beyond. Considering that a standard rack holds up to 42 physical 
servers, and that there may be hundreds or thousands of racks in some data centers, the 
implications are significant for network traffic. 
Moreover, a VM instance often uses more bandwidth than a physical server. There is the 
extra traffic involved in managing a virtual machine from a central location. Also, many 
Spirent Communications White Paper 
7 
Data Center Testing: A Holistic Approach 
data centers use products such as VMware Vmotion to move virtual machines within the 
data center. This greatly enhances uptime and reliability, but it also generates 
considerable network load. 
Testing virtual network devices and servers poses several interesting new questions: 
How can a test instrument attach to a virtual network device? For many types of 
tests, a connection to the physical server hosting a virtual switch will not be sufficient. 
One physical interface may handle traffic for dozens of VM instances, making it difficult 
to isolate and measure the performance of each VM instance.  
What’s needed is to virtualize the capabilities of the test instrument. A virtual test 
instrument resides in software, and thus runs inside the physical machine hosting virtual 
network and server instances. From the standpoint of the virtual network device, a test 
port looks exactly the same as it would in the physical world. 
Of course, a virtual test instrument should offer the same capabilities as its physical 
counterpart. As discussed later in this document, the test instrument should be able to 
offer traffic between any number and any combination of virtual and physical interfaces, 
and measure the whole system as a single entity. It also should be able to offer any layer-
2 through layer-7 traffic pattern the tester desires, both on virtual and physical interfaces. 
Can test instruments on virtual machines be trusted? The concept of test 
instrumentation running in software is certainly nothing new; indeed, software-based 
networking test tools predate hardware-based instruments by decades. Unlike hardware-
based instruments, however, software-based test tools often produce measurements that 
say as much or more about underlying components – the networking stack, host operating 
system, drivers and network interface card – as the system they purportedly measure.  
Software-based tools also can produce results that are either nonrepeatable or 
nonreproducible. 
One strategy to ensure measurements can be trusted is to implement the entire test 
instrument – including software-based emulation of hardware components – in software. 
This approach requires a far more rigorous approach to system design than does 
software-only tool design. But the benefit is clear: By emulating the entire test instrument 
in software, the instrument’s measurements are far less dependent on extraneous factors. 
Such an instrument will produce more meaningful measurements than software-only 
tools. 
Do virtual and physical switches offer comparable performance? Line-rate 
throughput and low latency and jitter have long been the hallmarks of physical Ethernet 
switches, but their virtual counterparts may not compare. Tests of early virtual switches 
show frame loss with offered loads as low as 50 Mbit/s. Moreover, these tests involved 
just a single pair of interfaces on a single virtual switch; in contrast, the standard practice 
for switch and router testing is to attach test interfaces to all switch ports and generate 
Spirent Communications White Paper 
8 
Data Center Testing: A Holistic Approach 
traffic in a fully meshed pattern – a far more stressful test pattern than using just a single 
port pair. 
Even if virtual switches will never handle loads as heavy as physical switches (a dubious 
assumption in these early days of virtual networking), it is still important to conduct 
stress tests to describe the limits of system performance. As discussed in the “Testing 
basics” section of this document, test engineers for years have relied on industry-standard 