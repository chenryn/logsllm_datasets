h
t
t
p
p
p
p
A
A
 100
 100
 90
 90
 80
 80
 70
 70
 60
 60
 50
 50
 40
 40
EDF
EDF
Reservation
Reservation
Fair Share
Fair Share
 0
 0
 20
 20
 40
 40
 60
 60
 80
 80
 100
 100
N (number of flows)
N (number of flows)
Figure 5: Application throughput with varying
number of ﬂows across a single bottleneck link and
moderate deadlines. Conﬁdence intervals are within
1% of the presented values.
ures, etc.). On the contrary, a deadline-aware network can
complement load-aware applications by explicitly honoring
their demands.
3. DESIGN SPACE AND MOTIVATION
The past work for satisfying application deadlines in the
Internet can be categorized in two broad classes. The ﬁrst
class of solutions involve packet scheduling in the network
based on deadlines. An example of such scheduling is Ear-
liest Deadline First (EDF) [21] wherein routers prioritize
packets based on their per-hop deadlines. EDF is an opti-
mal scheduling discipline in that if a set of ﬂow deadlines
can be satisﬁed under any discipline, EDF can satisfy them
too. The second solution category involves rate reservations.
A deadline ﬂow with size s and deadline d can be satis-
ﬁed by reserving rate r = s
d . Rate reservation mechanisms
have been extensively studied. For example, ATM sup-
ported Constant Bit Rate (CBR) traﬃc. In packet switched
networks, eﬀorts in both industry (IntServ, DiﬀServ) and
academia [5,12] have explored mechanisms to reserve band-
width or at least, guarantee performance.
Value of deadline awareness. Given this existing body of
work, we attempt through simple Monte Carlo simulations,
to build some intuition regarding the (possible) beneﬁts of
these approaches over fair sharing used in datacenters today.
Consider a 1Gbps link carrying several ﬂows with varying
deadlines. Flow parameters (such as the size and the fraction
of short and long ﬂows) are chosen to be consistent with
typical datacenters [4]. For the ﬂows with deadlines, the
deadlines are chosen to be exponentially distributed around
20ms (tight), 30ms (moderate) and 40ms (lax). To capture
the lack of application value of ﬂows that miss their deadline,
we use application throughput or the number of ﬂows that
meet their deadline as the performance metric of interest.
Using this simple simulation setup, we evaluate three “ideal”
bandwidth allocation schemes: (i) Fair-share, where the link
bandwidth is allocated evenly amongst all current ﬂows and
represents the best-case scenario for today’s deadline ag-
nostic protocols like TCP, DCTCP [4], XCP [19], etc. (ii)
EDF, representing the ﬁrst broad class of deadline aware
solutions, where the ﬂow with the earliest deadline receives
all the bandwidth until it ﬁnishes, and (iii) Rate reservation
(i.e., the second category), where ﬂows, in the order of their
arrival, reserve the rate needed to meet their deadline. In
contrast to fair-share, the latter two approaches are deadline
aware.
Figure 5 shows the application throughput for the three
approaches with moderate deadlines (see [26] for similar
results with tight and lax deadlines). As the number of
ﬂows increases, deadline-aware approaches signiﬁcantly out-
perform fair sharing. Perhaps most important is the fact
that they can support three to ﬁve times as many ﬂows as
fair share without missing any deadline (application through-
put=100%). This,
in eﬀect, redeﬁnes the peak loads at
which a datacenter can operate without impacting the user
experience. Hence, deadline-aware approaches have a lot to
oﬀer towards datacenter performance. However, practical
challenges remain for both types of solutions, scheduling as
well as reservations.
For the former class, we use EDF as an example to explain
its limitations, though our arguments are general. EDF is
packet based. It works on per-hop packet deadlines while
datacenter applications have end-to-end ﬂow deadlines. As
a result, even though EDF is optimal when the deadlines
can be satisﬁed, when there is congestion, EDF can actually
drive the network towards congestive collapse (see ﬁgure for
tight deadlines in [26]). Second and perhaps more impor-
tantly, EDF still needs to be complemented by an endhost
rate control design that will ensure that routers have the
right packets to schedule. Designing such a distributed rate
control scheme is far from trivial. Finally, EDF requires
priority queuing at routers. Our testbed experiments in
Section 6 illustrate some of these limitations for a simple
priority scheme.
For the latter class, reservation schemes are too heavy
weight for the datacenter environment where most ﬂows are
short. Further, unlike real-time traﬃc on the Internet, dat-
acenter ﬂows do not require a “constant” rate. Reservation
schemes ignore this ﬂexibility and reduce network eﬃciency,
especially given the dynamics on datacenter networks, where
network conditions change very fast (e.g., tiny RTTs, large
bursts of short ﬂows).
Overall, these limitations motivate the need for a practi-
cal datacenter congestion control protocol that, on the one
hand, ensures ﬂows meet their deadlines, but, on the other,
avoids packet scheduling and explicit reservations.
4. D3 DESIGN
The discussion in the two previous sections leads to the
following goals for datacenter congestion control:
1. Maximize application throughput: The protocol should
strive to maximize the number of ﬂows that satisfy their
deadlines and hence, contribute to application through-
put.
2. Burst tolerance: Application workﬂows often lead to ﬂow
bursts, and the network should be able to accommodate
these.
3. High utilization: For ﬂows without deadlines, the proto-
col should maximize network throughput.
D3 is designed to achieve these goals. Beyond these ex-
plicit goals, D3 accounts for the luxuries and challenges of
the datacenter environment. For instance, an important lux-
ury is the fact that the datacenter is a homogenous environ-
ment owned by a single entity. Consequently, incremental
deployment, backwards compatibility, and being friendly to
legacy protocols are non-goals.
The key insight guiding D3 design is the following: given a
ﬂow’s size and deadline, one can determine the rate needed
53to satisfy the ﬂow deadline. Endhosts can thus ask the net-
work for the required rate. There already exist protocols
for explicit rate control wherein routers assign sending rates
to endhosts [11,19]. With D3, we extend these schemes to
assign ﬂows with rates based on their deadlines, instead of
the fair share.
Assumptions. Based on the previous discussion, our de-
sign assumes that the ﬂow size and deadline information are
available at ﬂow initiation time. Further, we also assume
that per-ﬂow paths are static.
4.1 Rate control
With D3, applications expose the size and deadline infor-
mation when initiating a deadline ﬂow. The source endhost
uses this to request a desired rate, r. Given a ﬂow of size
s and deadline d, the initial desired rate is given by r = s
d .
This rate request, carried in the packet header, traverses the
routers along the path to the destination. Each router as-
signs an allocated rate that is fed back to the source through
the acknowledgement packet on the reverse path. The source
thus receives a vector of allocated rates, one for each router
along the path. The sending rate is the minimum of the al-
located rates. The source sends data at this rate for a RTT
while piggybacking a rate request for the next RTT on one
of the data packets.
Note however that neither does a ﬂow need, nor does it
obtain a reservation for a speciﬁc sending rate throughout
its duration. The rate that the network can oﬀer varies with
traﬃc load and hence, each source must periodically (in our
case, every RTT) ask the network for a new allocation. Since
the actual rate allocated by the network can be more or less
than the desired rate, endhosts update the desired rate as
the ﬂow progresses based on the deadline and the remaining
ﬂow size.
4.2 Rate allocation
For each of their outgoing interfaces, routers receive rate
requests from ﬂows with deadlines. Beyond this, there are
ﬂows without deadlines, where r = 0. Hence, the rate allo-
cation problem is deﬁned as: Given rate requests, a router
needs to allocate rates to ﬂows so as to maximize the number
of deadlines satisﬁed (goal 1) and fully utilize the network
capacity (goal 3). In a dynamic setting, this rate allocation
problem is NP-complete [7].
We adopt a greedy approach to allocate rates. When a
router receives a rate request packet with desired rate r, it
strives to assign at least r. If the router has spare capacity
after satisfying rate requests for all deadline ﬂows, it dis-
tributes the spare capacity fairly amongst all current ﬂows.
Hence, when the router capacity is more than the capac-
ity needed to satisfy all deadline ﬂows, the allocated rate a
given to a ﬂow is:
• For a deadline ﬂow with desired rate r, a = (r+f s), where
f s is the fair share of the spare capacity after satisfying
deadline ﬂow requests.
• For a non-deadline ﬂow, a = f s.
We note that distributing the spare capacity between dead-
line and non-deadline ﬂows allows us to balance the com-
peting goals 1 and 3. Assigning deadline ﬂows with a rate
greater than their desired rate ensures that their subsequent
rate requests will be lower and the network will be able to
satisfy future deadline ﬂows. At the same time, assigning
non-deadline ﬂows with a share of the spare capacity en-
sures that they make progress and network utilization re-
mains high.
However, in case the router does not have enough capacity
to satisfy all deadline ﬂows, it greedily tries to satisfy the
rate requests for as many deadline ﬂows as possible. The
remaining ﬂows, deadline and non-deadline, are assigned a
base rate that allows them to send a header-only packet per
RTT and hence, request rates in the future. For deadline
ﬂows, such low assignments will cause the desired rate to
increase. The endhosts can thus decide whether to give up
on ﬂows based on an ever increasing desired rate. This is
further discussed in Section 6.1.3.
4.3 Router operation
The rate allocation description above assumes the router
has the rate requests for all ﬂows at the same point in time.
In reality, the router needs to make allocation decisions in
an online, dynamic setting, i.e., rate requests are spread over
time, and ﬂows start and ﬁnish. To achieve this, the rate
allocation operates in a slotted fashion (from the perspective
of the endhosts). The rate allocated to a ﬂow is valid for the
next RTT, after which the ﬂow must request again. A rate
request at time t serves two purposes: (1). It requires the
router to assign at+1, the allocated rate for the next RTT,
and (2). It returns at, the allocation for the current RTT.
To achieve (1), the router needs to track its existing al-
locations. Consequently, routers maintain three simple, ag-
gregate counters for each interface:
• N: number of ﬂows traversing the interface. Routers use
ﬂow initiation and termination packets (TCP SYN/FIN)
to increment and decrement N respectively.3
• Demand counter (D): sum of the desired rates for dead-
line ﬂows. This represents the total demand imposed by
ﬂows with deadlines.
• Allocation counter (A): sum of allocated rates. This is
the current total allocation.
To achieve (2), the router must know the current rate allo-
cated to the ﬂow. In a naive design, a router could maintain
rate allocations for each active ﬂow through it. However,
since most deadline ﬂows are very short, such an approach
is too heavy-weight, not to mention router memory inten-
sive. We avoid the need for per-ﬂow state on routers by
relying on endhosts to convey rate allocations for each ﬂow.
Speciﬁcally, each rate request packet, apart from the desired
rate rt+1, contains the rate requested in the previous inter-
val (rt) and a vector of the rates allocated in the previous
interval ([at]). Each element in the vector corresponds to
the rate allocated by a router along the path in the previous
interval. The encoding of these in the rate request packet
header is described in Section 5.
For topologies with multiple paths between endhosts [13]
[1,2,15], D3 relies on ECMP, VLB and other existing mech-
anisms used with TCP to ensure that a given ﬂow follows a
3Note that past rate control schemes [11,19] approximate
N as C/R, where C is the interface capacity and R is the
current rate being assigned to ﬂows. Yet, D3 does not assign
the same rate to each ﬂow and this approximation is not
applicable.
54single path. Adapting D3 to use multiple paths for a single
ﬂow requires additional mechanisms and is beyond the scope
of this work.
Given this, we can now describe how packets are pro-
cessed by routers. Routers have no notion of ﬂow RTTs.
Packets without rate request headers are forwarded just as
today. Snippet 1 shows how a router processes a rate re-
quest packet. It applies to both deadline and non-deadline
ﬂows (for the latter, desired rate rt+1 is 0). The router ﬁrst
uses the packet header information to perform bookkeeping.
This includes the ﬂow returning its current allocation (line
3). Lines 7-13 implement the rate allocation scheme (Sec-
tion 4.2) where the router calculates at+1, the rate to be
allocated to the ﬂow. The router adds this to the packet
header and forwards the packet.
Snippet 1 Rate request processing at interval t
Packet contains: Desired rate rt+1, and past information
rt and at. Link capacity is C.
Router calculates: Rate allocated to ﬂow (at+1).
1: //For new ﬂows only
2: if (new ﬂow ﬂag set) N = N + 1
3: A = A−at //Return current allocation
4: D = D−rt + rt+1 //Update demand counter
//Calculate left capacity
5: lef t capacity = C − A
//Calculate fair share
6: f s = (C − D)/N
//Enough capacity to satisfy request
at+1 = rt+1 +f s
7: if lef t capacity > rt+1 then
8:
9:
10: else
11:
12:
13: end if
//Not enough capacity to satisfy request
at+1 = lef t capacity
//Flows get at least base rate
14: at+1 = max(at+1, base rate)
//Update allocation counter
15: A = A+ at+1
Of particular interest is the scenario where the router does
not have enough capacity to satisfy a rate request (lines 11-
12). This can occur in a couple of scenarios. First, the cumu-
lative rate required by existing deadline ﬂows, represented
by the demand counter, may exceed the router capacity. In
this case, the router simply satisﬁes as many requests as
possible in the order of their arrival. In the second scenario,
the demand does not exceed the capacity but fair share al-
locations to existing ﬂows imply that when the rate request
arrives, there is not enough spare capacity. However, the
increased demand causes the fair share assigned to the sub-
sequent rate requests to be reduced (line 6). Consequently,
when the deadline ﬂow in question requests for a rate in
the next interval, the router should be able to satisfy the
request.
4.4 Good utilization and low queuing
The rate given by a router to a ﬂow is based on the as-
sumption that the ﬂow is bottlenecked at the router. In a
multihop network, this may not be true. To account for bot-
tlenecks that occur earlier along the path, a router ensures
that its allocation is never more than that of the previous
router. This information is available in the rate allocation
vector being carried in the packet header. However, the ﬂow
may still be bottlenecked downstream from the router and
may not be able to utilize its allocation.
Further, the veracity of the allocation counter maintained
by a router depends on endhosts returning their allocations.
When a ﬂow ends, the ﬁnal rate request packet carrying the
FIN ﬂag returns the ﬂow’s allocated rate. While endhosts
are trusted to follow the protocol properly, failures and bugs
do happen. This will cause the router to over-estimate the
allocated rate, and, as a result, penalize the performance of
active ﬂows.
The aforementioned problems impact router utilization.
On the other hand, a burst of new ﬂows can cause the router
to temporarily allocate more bandwidth than its capacity,
which results in queuing. To account for all these cases, we
borrow from [11,19] and periodically adjust the router ca-
pacity based on observed utilization and queuing as follows:
C(t + T ) = C(t) + α(C(t) −
u(t)
T
) − β(
q
T
)
where, C is router capacity at time t, T is the update in-
terval, u is bytes sent over the past interval, q is instan-
taneous queue size and α, β are chosen for stability and
performance.4
Consequently, when there is under-utilization (u/T  0), the
allocations reduce. Apart from addressing the downstream
bottleneck problem, this ensures that the counters main-
tained by routers are soft state and divergence from reality
does not impact correctness. The failure of endhosts and
routers may cause ﬂows to not return their allocation. How-
ever, the resulting drop in utilization drives up the capacity
and hence, the allocation counters do not have to be con-
sistent with reality. The router, during periods of low load,
resets its counters to return to a consistent state. Even in
an extreme worst case scenario, where bugs at endhosts lead
to incorrect rate requests, this will only cause a degradation
in the performance of the application in question, but will
have no further eﬀects in the operation of the router or the
network.
4.5 Burst tolerance
Bursts of ﬂows are common in datacenters. Such bursts
are particularly challenging because of tiny RTTs in the dat-
acenter. With a typical RTT of 300µs, a new ﬂow sending
even just one 1500-byte packet per RTT equates to a send
rate of 40Mbps! Most TCP implementations shipping to-
day start with a send window of two packets and hence, a
mere 12-13 new ﬂows can cause queuing and packet loss on
a 1Gbps link. This has been observed in past work [8,23]
and is also present our experiments.
With D3, a new ﬂow starts with a rate request packet with
the SYN ﬂag set. Such a request causes N to be incremented
and reduces the fair share for ﬂows. However, pre-existing
ﬂows may already have been allocated a larger fair share rate
4The α, β values are chosen according to the discussion
in [11] where the stability of this controller was shown, and
are set to 0.1 and 1 in our implementation. The update
interval T should be larger than the datacenter propagation
RTT; in our implementation it is set to 800µs. The equation
assumes at least one ﬂow is bottlenecked on the link.
55(N was less when the earlier rate requests were processed).
Hence, allocating each new ﬂow with its proper fair share can
cause the router to allocate an aggregate rate larger than its
capacity, especially when a burst of new ﬂows arrives.
We rely on D3’s ability to “pause” ﬂows by assigning them
the base rate to alleviate bursts. When a new ﬂow starts, the
fair share assigned to its rate request is set to base rate. For
non-deadline ﬂows, this eﬀectively asks them to pause for a
RTT and not send any data packets. The sender however
does send a packet with only the rate request (i.e., a header-