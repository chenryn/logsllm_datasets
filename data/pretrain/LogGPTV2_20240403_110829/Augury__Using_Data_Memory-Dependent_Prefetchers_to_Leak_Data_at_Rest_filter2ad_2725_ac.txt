attempt a load of that address. If the secret value is a valid
pointer, this will transmit the pointer value through standard
cache side-channels. If it is not an address, it may still be
possible for the adversary to monitor the (failed) page-walk
or use TLB side-channels [23, 43] to learn the upper bits of
the secret.
Indirection DMPs dynamically determine the base address
of some array(s) and prefetches portions of it based on a
series of index values in memory. These are more powerful
transmitters than pointer-chasing DMPs as they easily transmit
values that are not valid virtual addresses. Specifically, an
attacker that controls the train pattern can trick the DMP into
treating a secret as an offset in a base-plus-offset calculation.
Suppose the attacker additionally controls the base address
(which is the case, if the attacker controls the training pat-
tern). Then the indirection-based DMP avoids the previously-
discussed issue in the pointer-chasing prefetcher: the attacker
can arrange for the base-plus-offset to fall within a mapped
memory region that is accessible to the attacker. This allows
both for simpler transmission and straight-forward cache-
occupancy side-channels for reception.
D. Receiving data transmitted by a DMP
Once a DMP has accessed and transmitted a secret, the
adversary must now receive that secret. In the simplest case
of an indirection prefetcher this would involve checking the
access time (cache occupancy status) of every entry in the base
array. For pointer-chasing prefetchers this would mean running
a cache contention side-channel to detect what address was
brought into the cache.
A DMP may alternatively prefetch to a prefetch buffer
rather than directly to the cache. A prefetch buffer is a small,
typically fully associative, cache which only holds prefetched
data. Only prefetched data can induce contention on this
buffer, making it significantly harder for the adversary to
observe the effect of the prefetch. Then, there must be an
actual access to the address corresponding to the transmitted
value to observe a timing difference or affect cache state.
Since the value being transmitted is the address in question
for single-layer DMPs, it may not be possible for the adver-
sary to directly access this address at all. This would occur
because it is unlikely that a secret address is mapped into the
adversary’s virtual address space.
V. EXISTENCE OF THE M1 DMP
In this section, we provide a detailed walkthrough of our
initial experiments confirming the existence of a specific DMP
while ruling out the existence of several other DMPs. We also
describe the steps we took to determine that the root cause
of our observations was, in fact, a DMP as opposed to other
microarchitectural features (like speculative execution).
We tested for the existence of four DMPs: both single- and
two-level versions of pointer-chasing and indirection-based
DMPs (Section IV). Our findings show the existence of a
single-level pointer-chasing DMP, so our focus below is on
how we setup the experiment for that variant. We discuss other
variants (negative results) at the end of the section (V-F).
A. Experiment overview
To confirm the existence of the single-level pointer-chasing
DMP (referred to as the DMP for short), we compare the
execution time of two different methods for accessing the same
randomly generated sparse series of memory addresses. The
first method–the AoP DMP pattern–pre-computes all memory
addresses and stores them sequentially in an array-of-pointers
(AoP). The addresses are then accessed by streaming over the
AoP and dereferencing each pointer. The second method–the
baseline–accesses the same series of addresses by computing
them on the fly. Computing the addresses ensures they cannot
be prefetched by either a DMP or a classical prefetcher. Both
experiments generate addresses using the same PRNG seed.
6
All experiments insert dependencies between operations
such as loads and PRNG calls. This action prevents out-of-
order execution from issuing multiple operations simultane-
ously and makes overall execution time strongly correlated to
the average memory access time.
Finally, the cache is flushed between experiments to re-
move inter-experiment interference. We discuss this and other
methodological details, e.g., ensuring that both experiments
run the same instruction sequences, in Section V-C.
B. Setting up the sequence of memory accesses
We set up the experiment by allocating two large buffers
as shown in Figure 3. One of the buffers, the data buffer, is
filled with random data, and the other buffer, the AoP, is filled
with unique pointers to disjoint and non-consecutive 128-byte
chunks of the data buffer.
These constraints are not necessary to activate the DMP but
will amplify the signal to noise ratio of DMP-caused speed
ups and minimize any affects from noise or other microarchi-
tectural optimizations. In particular, uniqueness and 128-byte
aligned accesses ensure that data backed by pointers is not
already cached from being accessed earlier in the experiment.
Recall from Section II-E, the M1 has 128 byte (L2) cache
lines. Using pointers to non-consecutive chunks improves the
likelihood that a classical prefetcher (Section II-A) will not
activate from data buffer accesses.
Fig. 3: Memory layout of the DMP AoP and data buffer. Black arrows
illustrate memory accesses due to aop dereferences (Line 17) in Algorithm 1.
Baseline accesses (Line 16) directly load the same entries in the data bu f
without dereferencing from the aop. The AoP shown contains pointers which
are consecutive in memory (unit stride). Each pointer points to disjoint and
non-consecutive 128-byte chunks of the data buffer. If we access the AoP from
index 1 through N and the DMP activates, the contents of the data buffer at
aop[M] for M > N may be brought into the cache.
C. Access patterns and other considerations
After setting up the data buffer and the AoP, there are
still some precautions that must be taken when accessing the
pointers to ensure a speedup can only be caused by a DMP.
If we were to just measure the execution time of two differ-
ent loops–one baseline access pattern loop and one AoP access
pattern loop– then we would be comparing the execution time
of two different instruction sequences. The AoP access pattern
has an extra memory access per loop iteration since it must:
1) indirect by some offset into the AoP and 2) dereference
the pointer stored in the AoP. Whereas the baseline must 1)
make a single memory access and 2) use a PRNG to compute
the next data buffer address on each iteration. Thus, seeing
different runtimes for the two loops would not be surprising,
and not necessarily be due to the presence of a DMP.
*aop[1]*aop[2]*aop[N]*aop[M]*aop[0]StreamedoverinMainAccessLoopPrefetchedbyDMPaopPointerDereferences31NM2[]data_bufSparse128-ByteChunksHitsincacheifprefetched1 data bu f = . . . /* some large buffer */
2 aop = . . . /* some large buffer */
3 aop mode = IS AOP RUN /* 0 - baseline, 1 - AoP */
4 aop idx = 0
5 rand idx = PRNG(RAND SEED)
/* Fill the AoP */
6 for i in 0...NUM PTRS do
7
8
9 end
∗aop[i] = data bu f + (rand idx∗ aop mode∗ CL SIZE)
rand idx = PRNG(rand idx)
10 FLUSH CACHE
11 MEM BARRIER
12 dep val = 0
13 start time = READ TIMER(dep val)
14 dep val = MSB(start time)
/* Training loop */
in 0...NUM PTRS do
/* Baseline Access */
dep val = MSB(data bu f [rand idx∗ (1− aop mode)∗
CL SIZE | dep val])
/* AoP Access */
dep val = MSB(∗aop[aop idx | dep val])
aop idx = aop idx + aop mode
rand idx = PRNG(rand idx)
15 for
16
17
18
19
20 end
21 MEM BARRIER
22 stop time = READ TIMER(dep val)
Algorithm 1: Pseudocode for the baseline experiment (which computes
pointers on the fly) and the experiment testing for the presence of a
single-level pointer-chasing DMP and baseline. READ TIMER calls
mach absolute time which returns time in ticks. CL SIZE stands for
the (128 byte) cache line size. Dependencies are guaranteed to resolve
to zero by using only their Most Significant Bit; denoted by MSB.
MEM BARRIER is an instruction/data serialization instruction. PRNG
is a C macro that expands into a Lehmer random number generator, i.e.,
is not implemented as a syscall. The code is compiled with compiler
optimizations turned off, and the assembly code was manually inspected
to ensure intended behavior.
To address these discrepancies, we ensure that both the base-
line and AoP experiments execute the same instructions, while
taking care to ensure that the baseline does not activate a DMP.
The code used for both experiments is shown in Algorithm 1.
For the baseline, we add an access and dereference the pointer
at index 0 of the AoP during each iteration (Line 16). For the
AoP case, we compute the address on the fly as in the baseline
(Line 19) but use the pointer read from the AoP to lookup the
data buffer (Line 17). With both experiments executing the
same instructions, we expect the baseline to run slightly faster
than the AoP pattern due to occasional cache misses from AoP
traversal.
It is also necessary during baseline runs to set all pointers
in the AoP to point to the first element in the data buffer:
otherwise the DMP activates during the baseline run due to
the single AoP access combined with the computed pattern
being similar to the AoP case pattern. This is an instance of
the pattern described in III-B where a single read of a cacheline
containing pointers is misinterpreted as the source of multiple
pointer dereferences.
Finally, we add dependencies between operations to en-
sure that speedups are not due to out-of-order execution.
Specifically, dep val in Algorithm 1 ensures that all loads
7
are executed serially and between the timer start and stop
operations. Note that the first load in each iteration, which
looks up data bu f , depends on both the previous load into
the AoP and the PRNG computation—regardless of whether
the baseline or AoP-based experiment
is being run. This,
coupled with the fact that both the AoP lookup and PRNG
operation are expected to be relatively fast, implies that the
loop’s performance will be strongly correlated to the data bu f
access latency.
D. Other Notes on Methodology
We find the DMP to be present solely on Firestorm cores. To
improve consistency, we core pin our experiments by setting
the thread quality of service, as described in [1]. We do not
perform any kind of frequency pinning (for the Firestorm cores
or for DRAM) throughout our experiments.
timers:
Our experiments make use of two different
the
M1’s performance monitoring counter (PMC) [27] and the
mach_absolute_time macOS syscall [3]. The PMC can
measure time at cycle-granularity with Apple reporting a max-
imum clock speed of 3.2 GHz for the Firestorm cores whereas
mach_absolute_time can measure time at a granularity
of (on average) 42 ns per ‘tick’. Despite the coarser-grain
measurement, we use the mach_absolute_time timer
in many situations since we found it easier to work with
(e.g., accessible from userspace, accessible across cores) and
sufficient to distinguish between cache hit vs. DRAM access
events. We use the PMC in select experiments to distinguish
between finer-grain events (e.g., an L1 cache vs. L2 cache hit).
For all experiments with either timer, we first measure
timer-stop timer
the timer’s overhead by running a start
pair back to back in an empty loop. This overhead is sub-
tracted from all points on graphs that use that timer. For
mach_absolute_time, we found the overhead to be ∼
42 ns and convert measurements using that timer to ns using
that conversion rate.
E. Results
Figure 4 reports the time elapsed (stop time - start time)
for the baseline and DMP variants tested in Algorithm 1,
for different length sequences of pointers (AoPs). The main
takeaway is that the “Array of Pointers” variant (testing the
presence of a single-level pointer-chasing DMP) sees signifi-
cant speedup (3− 8X) on medium to large AoPs compared to
the baseline (“Computed”) access times.
We note that, while the AoP variant always sees speedup
relative to the baseline,
the speedup varies as a function
of NUM PTRS. To break this down, we divide Figure 4
into three regimes (1), (2) and (3) demarcated with vertical
dashed lines. In regime (1)–small NUM PTRS–the speedup
is initially zero and increases quickly with NUM PTRS. This
is due to timer granularity: for small NUM PTRS,
timer
overhead dominates. In regime (2)–medium NUM PTRS–the
speedup converges given sufficiently large NUM PTRS. We
attribute this to the DMP improving the average memory ac-
cess time in the AoP-based experiment. Finally, in regime (3)–
large NUM PTRS–the speedup decreases with NUM PTRS.
8
Figure 5 shows the time to perform one of these test ac-
cesses for N = 256 pointers and M = 259 as well as measured
access latencies to various memory levels (L1, L2, DRAM).
Lower latency test accesses indicate the DMP prefetched and
dereferenced data. The figure shows that test accesses for the
DMP configuration track closely with the L2 cache hit latency.
From this, we conclude that the DMP prefetches into the L2
cache and is likely built alongside the L2.
Fig. 5: Test access latency, relative to the baseline (computing pointers
on the fly) and measured access latencies to different
level memories.
Time is measured using the fine-grain performance monitoring counter from
Section V-D. The label ‘Fastest main memory times’ refers to the fastest main
memory time we observed with DRAM frequency scaling (Section II-E).
F. Testing for the existence of other prefetchers
So far we have only discussed the existence of a single-
level, pointer-chasing DMP. We also tested whether the M1
contained other classical prefetchers and other data memory-
dependent prefetchers. We found the M1 does feature at least
one other classical prefetcher but does not contain the other
data memory-dependent prefetchers described in Section IV.
1) Testing for classical (stride) prefetchers: We con-
firmed through a separate analysis that the M1 contains a
separate classical (stride) prefetcher that prefetches data into
the L1 cache. Based on our analysis, this prefetcher seems
to be completely separate from the DMP—i.e., has different
depth, confidence, etc., parameters—and thus does not impact
what data the DMP can access (Section IV-B). Thus, we do
not study it further in this paper.
2) Testing for multi-level pointer-chasing DMPs: Next,
we tested whether the pointer-chasing DMP we had been
focusing on was multi-level. Confirming whether such a
prefetcher is present is very important since having more than
one level dramatically increases the scope of data that the
DMP can access (Section IV-B).
For this experiment, we added another level to the AoP
and reran the previous experiments. Specifically, we allocate
an additional array which holds pointers to random 128-byte
chunks of the original AoP. We call this additional array the
outer AoP and the original AoP the inner AoP. The pointers
in the outer AoP are again spaced out so that there is only one