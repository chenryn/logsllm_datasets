title:Highly Predictive Blacklisting
author:Jian Zhang and
Phillip A. Porras and
Johannes Ullrich
Highly Predictive Blacklisting
Jian Zhang
SRI International
Menlo Park, CA 94025
Phillip Porras
SRI International
Menlo Park, CA 94025
Johannes Ullrich
SANS Institute
Bethesda, MD 20814
Abstract
The notion of blacklisting communication sources has
been a well-established defensive measure since the ori-
gins of the Internet community. In particular, the prac-
tice of compiling and sharing lists of the worst offenders
of unwanted trafﬁc is a blacklisting strategy that has re-
mained virtually unquestioned over many years. But do
the individuals who incorporate such blacklists into their
perimeter defenses beneﬁt from the blacklisting contents
as much as they could from other list-generation strate-
gies? In this paper, we will argue that there exist better
alternative blacklist generation strategies that can pro-
duce higher-quality results for an individual network.
In particular, we introduce a blacklisting system based
on a relevance ranking scheme borrowed from the link-
analysis community. The system produces customized
blacklists for individuals who choose to contribute data
to a centralized log-sharing infrastructure. The ranking
scheme measures how closely related an attack source is
to a contributor, using that attacker’s history and the con-
tributor’s recent log production patterns. The blacklisting
system also integrates substantive log preﬁltering and a
severity metric that captures the degree to which an at-
tacker’s alert patterns match those of common malware-
propagation behavior. Our intent is to yield individual-
ized blacklists that not only produce signiﬁcantly higher
hit rates, but that also incorporate source addresses that
pose the greatest potential threat. We tested our scheme
on a corpus of over 700 million log entries produced
from the DShield data center and the result shows that
our blacklists not only enhance hit counts but also can
proactively incorporate attacker addresses in a timely
fashion. An early form of our system have been ﬁelded
to DShield contributors over the last year.
1 Introduction
A network address blacklist represents a collection of
source IP addresses that have been deemed undesirable,
where typically these addresses have been involved in
some previous illicit activities. For example, DShield (a
large-scale security-log sharing system) regularly com-
piles and posts a ﬁrewall-parsable blacklist of the most
proliﬁc attack sources seen by its contributors [17]. With
more than 1700 contributing sources providing a daily
stream of 30 million security log entries, such daily
blacklists provide an informative view of those class C
subnets that are among the bane of the Internet with re-
spect to unwanted trafﬁc. We refer to the blacklists that
are formulated by a large-scale alert repository and con-
sist of the most proliﬁc sources in the repository’s col-
lection of data as the global worst offender list (GWOL).
Another strategy for formulating network address black-
lists is for an individual network to create a local blacklist
based entirely on its own history of incoming communi-
cations. Such lists are often culled from a network’s pri-
vate ﬁrewall log or local IDS alert store, and incorporate
the most repetitive addresses that appear within the logs.
We call this blacklist scheme the local worst offender list
(LWOL) method.
The GWOL and LWOL strategies have both strengths
and inherent weaknesses. For example, while GWOLs
provide networks with important
information about
highly proliﬁc attack sources, they also have the poten-
tial to exhaust the subscribers’ ﬁrewall ﬁlter sets with ad-
dresses that will simply never be encountered. Among
the sources that do target the subscriber, GWOLs may
miss a signiﬁcant number of attacks, in particular when
the attack sources prefer to choose their targets more
strategically, focusing on a few known vulnerable net-
works [4]. Such attackers are not necessarily very pro-
liﬁc and are hence elusive to GWOLs. The sources on an
LWOL have repetitively sent unwanted communications
to the local network and are likely to continue doing so.
However, LWOLs are limited by being entirely reactive –
they only capture attackers that have been pounding the
local network and hence cannot provide a potential for
the blacklist consumer to learn of attack sources before
USENIX Association  
17th USENIX Security Symposium 
107
these sources reach their networks.
Furthermore, both types of lists suffer from the fact
that an attack source does not achieve candidacy until it
has produced a sufﬁcient mass of communications. That
is, although it is desirable for ﬁrewall ﬁlters to include
an attacker’s address before it has saturated the network,
neither GWOL nor LWOL offer a solution that can pro-
vide such timely ﬁlters. This is a problem particularly
with GWOL. Even after an attacker has produced signif-
icant illicit trafﬁc, it may not show up as a proliﬁc source
within the security log repository, because the data con-
tributors of the repository are a very small set of networks
on the Internet. Even repositories such as DShield that
receive nearly 1 billion log entries per month represent
only a small sampling of Internet activity. Signiﬁcant at-
tacker sources may elude incorporation into a blacklist
until they have achieved extensive saturation across the
Internet.
In summary, a high-quality blacklist that fortiﬁes net-
work ﬁrewalls should achieve high hit rate, should incor-
porate addresses in a timely fashion, and should proac-
tively include addresses even when they have not been
encountered previously by the blacklist consumer’s net-
work. Toward this goal, we present a new blacklist gen-
eration system which we refer to as the highly predictive
blacklisting (HPB) system. The system incorporates 1)
an automated log preﬁltering phase to remove unreliable
alert contents, 2) a novel relevance-based attack source
ranking phase in which attack sources are prioritized on
a per-contributor basis, and 3) a severity analysis phase
in which attacker priorities are adjusted to favor attack-
ers whose alerts mirror known malware propagation pat-
terns. The system constructs ﬁnal individualized black-
lists for each DShield contributor by a weighted fusion
of the relevance and severity scores.
HPB’s underlying relevance-based ranking scheme
represents a signiﬁcant departure from the long-standing
LWOL and GWOL strategies. Speciﬁcally, the HPB
scheme examines not just how many targets a source ad-
dress has attacked, but also which targets it has attacked.
In the relevance-based ranking phase, each source ad-
dress is ranked according to how closely related the
source is to the target blacklist subscriber. This relevance
measure is based on the attack source similarity patterns
that are computed across all members of the DShield
contributor pool (i.e., the amount of attacker overlap ob-
served between the contributors). Using a data correla-
tion strategy similar to hyper-text link analysis, such as
Google’s PageRank [2], the relationships among all the
contributors are iteratively explored to compute an indi-
vidual relevance value from each attacker to each con-
tributor.
We evaluated our HPB system using more than 720
million log entries produced by DShield contributors
from October to November 2007. We contrast the per-
formance of the system with that of the corresponding
GWOLs and LWOLs, using identical time windows, in-
put data, and blacklist lengths. Our results show that for
most contributors (more than 80%), our blacklist entries
exhibit signiﬁcantly higher hit counts over a multiday
testing window than both GWOL and LWOL. Further
experiments show that our scheme can proactively incor-
porate attacker addresses into the blacklist before these
addresses reach the blacklist consumer network, and it
can do so in a timely fashion. Finally, our experiments
demonstrate that the hit count increase is consistent over
time, and the advantages of our blacklist remain stable
across various list lengths and testing windows.
The contribution of this paper is the introduction of the
highly predictive blacklisting system, which includes our
methodology for preﬁltering, relevance-based ranking,
attacker severity ranking, and ﬁnal blacklist construc-
tion. Ours is the ﬁrst exploration of a link-analysis-based
scheme in the context of security ﬁlter production and to
quantify the predictive quality of the resulting data. The
HPB system is also one of the only new approaches we
are aware of for large-scale blacklist publication that has
been proposed in many years. However, our HPB sys-
tem is applicable only to those users who participate as
active contributors to collaborative security log data cen-
ters. Rather than a detriment, we hope that this fact pro-
vides some operators a tangible incentive to participate
in security log contributor pools. Finally, the system dis-
cussed in this paper, while still a research prototype, has
been fully implemented and deployed for nearly a year
as a free service on the Internet at DShield.org. Our ex-
perience to date leads us to believe that this approach is
both scalable and feasible for daily use.
The rest of the paper is organized as follows. Section 2
provides a background on previous work in blacklist gen-
eration and related topics. In Section 3 we provide a de-
tailed description of the Highly Predictive Blacklist sys-
tem. In Section 4 we present a performance evaluation
of HPBs, GWOLs, and LWOLS, including assessments
of the extent to which the above three desired blacklist
properties (hit rate, proactive appearance, and timely in-
clusion) are realized by these three blacklists.
In Sec-
tion 5 we present a prototype implementation of the HPB
system that is freely available to DShield.org log contrib-
utors, and we summarize our key ﬁndings in Section 6.
2 Related Work
Network address and email blacklists have been around
since the early development of the Internet [6]. To-
day, sites such as DShield regularly compile and pub-
lish ﬁrewall-parsable ﬁlters of the most proliﬁc attack
sources reported to its website [17]. DShield represents
108 
17th USENIX Security Symposium 
USENIX Association
a centralized approach to blacklist formulation, provid-
ing a daily perspective of the malicious background ra-
diation that plagues the Internet [15, 20]. Other recent
examples of computer and network blacklists include IP
and DNS blacklists to help networks detect and block
unwanted web content, SPAM producers, and phishing
sites, to name a few [7, 8, 17, 18]. The HPB system pre-
sented here complements, but does not displace these re-
sources or their blacklisting strategies. In addition, HPBs
are only applicable to active log contributors (we hope
as an incentive), not as generically publishable one-size-
ﬁts-all resources.
More agile forms of network blacklisting have also
been explored, with the intention of rapidly publishing
perimeter ﬁlters to control actively spreading malware
epidemics [1, 3, 12, 14]. For example, in [14] a peer-
to-peer blacklisting scheme is proposed, where each net-
work incorporates an address into its local blacklist when
a threshold number of peers have reported attacks from
this address. We separate our HPB system from these
malware defense schemes. While the HPB system does
incorporate a malware-oriented attacker severity metric
into its ﬁnal blacklist selection, we have not contem-
plated nor propose HPBs for use in the context of dy-
namic quarantine defenses for malware epidemics.
One key insight that inspired the HPB relevance-based
ranking scheme was raised by Katti et al. [10], who iden-
tiﬁed the existence of stable correlations among the at-
tackers reported by security log contributors. Here we in-
troduce a relevance-based recommendation scheme that
selects candidate attack sources based on the attacker
overlaps found among peer contributors. This relevance-
based ranking scheme can be viewed as a random walk
on the correlation graph, going from one node to another
following the edges in the graph with the probability pro-
portional to the weight of the graph. This form of random
walk has been applied in link-analysis systems such as
Google’s PageRank [2], where it is used to estimate the
probability that a webpage may be visited. Similar link
analysis has been used to rank movies [13] and reading
lists [19].
The problem of predicting attackers has also been
recently considered in [24] using a Guassian process
model. However, [24] purely focused on developing sta-
tistical learning techniques for attacker prediction based
on collaborative ﬁltering.
In this paper, we present a
comprehensive blacklisting generation system that con-
siders many other characteristics of attackers. The pre-
diction part is only one component in our system. Fur-
thermore, the prediction model presented here is com-
pletely different from the one in [24] (Gaussian process
model in [24] and link analysis model here). By taking
some penalty in predictive power, the prediction model
presented here is much more scalable, which is of neces-
sity for implementing a deployable service (Section 5).
Finally, [23] provides a six-page summary of the earli-
est release of our DShield HPB service, including a high-
level description of an early ranking scheme. In this pa-
per we have substantially expanded this algorithm and
present its full description for the ﬁrst time. This present
paper also introduces the integration of metrics to capture
attack source maliciousness in its ﬁnal rank selection,
and presents the full blacklist construction system. We
also present our quantitative evaluation of multiple sys-
tem properties, and address several open questions that
have been raised over the past year since our initial pro-
totype.
3 Blacklisting System
We illustrate our blacklisting system in Figure 1. The
system constructs blacklists in three stages. First, the se-
curity alerts supplied by sensors across the Internet are
preprocessed. This removes known noises in the alert
collection. We call this the preﬁltering stage. The pre-
processed data are then fed into two parallel engines.
One ranks, for each contributors, the attack sources ac-
cording to their relevance to that contributor. The other
scores the sources using a severity assessment that mea-
sures their maliciousness. The relevance ranking and the
severity score are combined at the last stage to generate
a ﬁnal blacklist for each contributor.
We descibe the preﬁltering process in Section 3.1, rel-
evance ranking in Section 3.2, severity score in Sec-
tion 3.3 and the ﬁnal production of the blacklists in Sec-
tion 3.4.
3.1 Preﬁltering Logs for Noise Reduction
One challenge to producing high-quality threat intelli-
gence for use in perimeter ﬁltering is that of reducing
the amount of noise and erroneous data that may exist in
the input data that drives our blacklist construction algo-
rithm. That is, in addition to the unwanted port scans,
sweeps, and intrusion attempts reported daily within the
DShield log data, there are also commonly produced
log entries that arise from nonhostile activity, or activ-
ity from which useful ﬁlters cannot be reliably derived.
While it is not possible to separate attack from nonat-
tack data, the HPB system preﬁlters from consideration
logs that match criteria that we have been able to empiri-
cally identify as commonly occurring nonuseful input for
blacklist construction purposes.
As a preliminary step prior to blacklist construction,
we apply three ﬁltering techniques to the DShield alert
logs. First, the HPB system removes from consideration
DShield logs produced from attack sources from invalid
or unassigned IP address space. Here we employ the
USENIX Association  
17th USENIX Security Symposium 
109
Figure 1: Blacklisting system architecture
bogon list created by the Cymru team that captures ad-
dresses that are reserved, not yet allocated, or delegated
by the Internet Assigned Number Authority [16]. Typi-
cally, such addresses should not be routed, but otherwise
do appear anyway in the DShield data. In addition, re-
served addresses such as the 10.x.x.x or 192.168.x.x may
also appear in misconﬁgured contributor logs that are not
useful for translating into blacklists.
Second, the system preﬁlters from consideration net-
work addresses from Internet measurement services, web
crawlers, or common software update sources. From ex-
perience, we have developed a whitelist of highly com-
mon sources that, while innocuous from an intrusion per-
spective, often generate alarms in DShield contributor
logs.
Finally, the HPB system applies heuristics to avoid
common false positives that arise from commonly timed-
out network services. Speciﬁcally, we exclude logs pro-
duced from source ports TCP 53 (DNS), 25 (SMTP), 80
(HTTP), and 443 (often used for secure web, IMAP, and
VPN), and from destination ports TCP 53 (DNS) and
25 (SMTP). Firewalls will commonly time out sessions
from these services when the server or client becomes
unresponsive or is slow. In practice, the combination of
these preﬁltering steps provides approximately a 10% re-
duction in the DShield input stream prior delivery to the
blacklist generation system.
3.2 Relevance Ranking
Our notion of attacker relevance is a measure that in-
dicates how close the attacker is related to a particu-
lar blacklist consumer. It also reﬂects the likelihood to
which the attacker may come to the blacklist consumer
in the near future. Note that this relevance is orthogonal
to metrics that measure the severity (or benignness) of
the source, which we will discuss in the next section.
In our context, the blacklist consumers are the contrib-
utors that supply security logs to a log-sharing repository
such as DShield. Recent research has observed the exis-
tence of attacker overlap correlations between DShield
contributors [10], i.e., there are pairs of contributors that
share quite a few common attackers, where the common
attacker is deﬁned as a source address that both contrib-
utors have logged and reported to the repository. This re-
search also found that this attacker overlap phenomenon
is not due to attacks that select targets randomly (as in a
random scan case). The correlations are long lived and
some of them are independent of address proximity. We
exploit these overlap relationships to measure attacker
relevance.
We ﬁrst illustrate a simple concept of attacker rele-
vance. Consider a collection of security logs displayed
in a tabular form as shown in Table 1. We use the rows
of the table to represent attack sources and the columns
to represent contributors. We refer to the unique source
addresses that are reported within the log repository as
attackers, and use the terms “attacker” and “source” in-
terchangeably. Since the contributors are also the tar-
gets of the logged attacks, we refer to them as victims.
We will use the terms “contributor” and “victim” inter-
changeably. An asterisk “*” in the table cell indicates
that the corresponding source has reportedly attacked the
corresponding contributor.
v1
*
*
*
v2
*
*
*
*
v3
v4
v5
*
*
*
*
*
*
*
s1
s2
s3
s4
s5
s6
s7
s8
Table 1: Sample Attack Table
Let us assume that Table 1 represents a series of logs
contributed in the recent past by our ﬁve victims, v1
through v5. Now suppose we would like to calculate
the relevance of the sources for contributor v1 based on
these attack patterns. From the attack table we observe
110	
17th	USENIX	Security	Symposium	
USENIX	Association
that contributors v1 and v2 share multiple common at-