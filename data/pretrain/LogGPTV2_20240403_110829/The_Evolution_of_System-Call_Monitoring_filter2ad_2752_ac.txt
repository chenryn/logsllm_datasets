code-injection attacks, such as buffer overﬂows. Second,
the diversity of normal proﬁles on different systems is a bar-
rier because the attacker requires precise knowledge of the
normal proﬁle [75]. To what degree this defeats mimicry in
practice has not been thoroughly investigated, but it is worth
recalling that the identical program in two different produc-
tion environments produced normal proﬁles with only 29%
of sequences in common (see section 3.2). This reduces the
probability that a mimicry sequence crafted for one instal-
lation would work in others. Third, the mimicry attack re-
quires injecting a potentially long sequence of code, which
may not be possible in all vulnerabilities. The example
given in [74] (shown in Figure 4) requires an additional 128
nulliﬁed calls to hide an attack sequence of only 8 system
calls. Finally, mimicry attacks may be difﬁcult to imple-
ment in practice because of the anomalies generated by the
“preamble” of such attacks [35].
However, mimicry attack strategies have become in-
creasingly sophisticated, using automated attack methods
including model checking [21], genetic algorithms [34], and
symbolic code execution [41]. Although these approaches
may not always be reliable in practice, work on persistent
interposition attacks shows that the application itself can be
422422
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 
used to facilitate mimicry attacks [58]. These results sug-
gest that if an attacker can corrupt program memory, then
it is possible to evade virtually any system call-based mon-
itoring system—assuming it is the only defense. It remains
unclear how feasible mimicry attacks are on systems with
memory corruption defenses such as address-space random-
ization [60].
Another attack involves crafting sequences that are short
enough to avoid producing anomalies, hence exploiting
“blind spots” in detection coverage [71, 70]. Any intrusion
detection system is a trade-off between false positives and
false negatives. In order to reduce potential false positives,
the original system used a temporal threshold: Anomalous
sequences signaled attacks only if the number of anomalies
within a recent time window exceeded a given threshold.
This opens the possibility of designing attacks that can stay
below the threshold, either by generating very few anoma-
lies or by spreading them out over a long time period. Once
again, such attacks require that the attacker inject code con-
taining particular sequences of system calls.
Non-control-ﬂow attacks are yet another way of poten-
tially subverting the system call modeling method. The
goal is to manipulate the parameters to system calls without
changing the call sequence. Chen et al. demonstrated that
there are viable vulnerabilities that can be exploited with
this method, for example, by using normal system calls in
the sequence to elevate privileges and then overwriting the
password ﬁle [7]. In a sense, this approach is an extension
of the original mimicry attack: instead of nullifying system
calls for the purpose of crafting an attack sequence, the ma-
nipulated calls become the means of attack directly, without
any need to create an attack sequence.
The advent of mimicry and other attacks against the sys-
tem call modeling approach led to a wealth of research
aimed at improving the original method to make it more
attack resistant. Many of the extensions discussed below
were inspired by a need to address these attacks.
5 Data modeling methods
The methods described in Section 3 and [14, 25] de-
pend only on an enumeration of the empirically observed
sequences, or n-grams2, in traces of normal behavior. Two
different methods of enumeration were studied, each of
which deﬁnes a different model, or generalization, of the
data. There was no statistical analysis of these patterns in
the original work. As shown in Figure 1, the lookahead pair
method constructs a list for each system call of the system
calls that follow it at a separation of 0, 1, 2, up to n po-
sitions later in the trace. The * character is a wildcard, so
2An n-gram is a sub-sequence of length n taken from a given sequence.
Here, the n-gram representation is obtained by sliding a window of length
n across the entire sequence.
the pattern ¡mmap, *, getrlimit¿ speciﬁes that any 3-symbol
sequence that begins with mmap and ends with getrlimit
will be treated as normal. This method can be implemented
efﬁciently and produced good results on the original data
sets. On some data sets, representing the n-grams exactly
gave better discrimination than lookahead pairs, although
it is more inefﬁcient to implement. Researchers also ex-
perimented with variable-length window sizes [49, 78, 11],
random schema masks [28]. The correspondence between
n-gram representations and ﬁnite state automata (FSA) was
studied in several papers, including [49, 31]. This work has
been extended to more structured representations, but these
require additional information such as the execution con-
text, and are discussed in Section 6.
Several methods used statistical machine learning to de-
velop accurate models of normal system call patterns. In
an early example, DFA induction was used to learn a FSA
that recognized the language of the program traces [40]. In
this work, the learning algorithm determined the frequen-
cies with which individual symbols (system calls) occurred,
conditioned on some number of previous symbols. Individ-
ual states in the automaton represented the recent history of
observed symbols, while transitions out of the states spec-
iﬁed both which symbols were likely to be produced next
and what the resulting state of the automaton would be.
Other machine learning approaches include Hidden
Markov Models (HMM) [76, 17], neural networks [18, 10],
k-nearest neighbors [47], and Bayes models [42]. Each
of these projects was designed to produce a more accu-
rate model, with the goal of reducing false positives. This
comes at the cost of more computationally expensive al-
gorithms.
In some cases, the algorithms require multiple
passes over the entire data set, thus violating the real-time
component of the autonomy principle. Another limitation
of most statistical methods is the assumption of stationar-
ity. This means that the concept of normal behavior is as-
sumed not to change while the system is being trained and
tested. This assumption violates the adaptable principle, a
problem addressed in [48] for network trafﬁc, where prob-
abilities were based on the time since the last event rather
than on average rate.
Data mining seeks to discover what features are most im-
portant out of a large collection of data. This idea was ap-
plied to system calls with the goal of discovering a more
compact deﬁnition of normal than that obtained by simply
recording all observed patterns in the training set [45]. Also,
by identifying the most important features of such patterns,
it was hypothesized that the method would be more likely
to generalize effectively.
To summarize, many approaches to modeling system call
data have been developed. These range from simple compu-
tationally efﬁcient models to more sophisticated approaches
that require additional computation. Most innovations have
423423
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 
been aimed either at reducing false-positive rates or at cop-
ing with mimicry attacks. It is surprisingly difﬁcult to com-
pare the performance of the different methods in a system-
atic way. However, one early study [76] concluded that dif-
ferences among data sets had signiﬁcantly more impact on
results than differences among data modeling methods.
6 Extensions
The idea of using short sequences of system calls for
anomaly detection has been extended in several ways.
These can be grouped into several broad categories. Here
we discuss some of the key advances, without an attempt to
exhaustively review all of the research that has been done in
these areas.
6.1 Data ﬂow
The original work deliberately discarded any informa-
tion related to system call parameters. This resulted in a
simple, lightweight method. However, a logical extension
is to consider the effects of parameters to system calls. An-
other way of viewing this is that sequences of system calls
model code path ﬂow, and system call parameters model
data ﬂow.
Tandon and Chan used a rule-learning system to augment
a code-ﬂow anomaly detection system with a data-ﬂow sys-
tem [72]. They combined rules for sequences of system
calls with those for system call arguments. They reported
improved attack detection, but at the cost of increased com-
plexity: Their system ran 4-10 times more slowly when ar-
guments were included. Sufatrio and Yap incorporated data
ﬂow in the form of a supplied speciﬁcation for system call
arguments [69], and Bhatkar et al. reported that modeling
the temporal aspects of data ﬂow in conjunction with con-
trol ﬂow further improved detection [6].
Kruegel et al. went one step further, looking only at
the arguments and disregarding code ﬂow altogether [43].
They explored several different models for anomaly detec-
tion based on system call arguments, including the distribu-
tion of string lengths and characters in arguments, Markov
models of argument grammar, and explicit enumeration of
limited argument options. They demonstrated that their ap-
proach is effective against attacks (primarily buffer over-
ﬂows) and that it has low overhead. Mutz et al. extended
this approach by using a Bayesian network to combine the
output of the different system call argument models [53].
6.2 Execution context
Apart from system call arguments, there are many ad-
ditional sources of information associated with system calls
that can be used to improve anomaly detection. One of these
424424
is the location within the program code from where a sys-
tem call is issued, which can be determined by the program
counter. Sekar et al. ﬁrst proposed using program counter
information to build a FSA of system call sequences [62].
A FSA is a natural model for program code paths; how-
ever, inferring a FSA from sequence information alone is
difﬁcult. The Hidden Markov Model presented in Warren-
der et al. is similar to a FSA, and has large learning over-
heads [76]. Further, in the absence of program counter in-
formation, the FSA does not improve detection or reduce
false positives dramatically. The key insight in Sekar et
al. was that using program counter information in the FSA
can overcome these limitations. The states in the FSA are
program locations derived from program counters, and the
transitions are system calls. Hence the model deﬁnes al-
lowable system call transitions from one program location
to the next.
A FSA using program counters is a close representation
of the true structure of the code, and as such it is able to
model loops and both long and short range correlations ef-
fectively, unlike the n-gram approach. This results in both
increased accuracy of attack detection and reduced false
positives [62]. A further beneﬁt is that a FSA model us-
ing program counters converges to a stable normal model
an order of magnitude faster than an n-gram model.
In addition to the program counter, the call stack is a rich
source of information. The VtPath model [12] augments the
FSA approach with stack return addresses—each transition
of the FSA includes a list of all the return addresses. This is
used to generate a virtual path between system calls which
can then be additionally checked for anomalies. This ad-
ditional information improves attack detection and reduces
false positives, without incurring additional overhead.
In a similar approach, execution graphs were used to ex-
tend simple system call enumeration to a more structured
representation [16].
In this approach, the return address
pointer is stored with the system call, and this informa-
tion is used to reconstruct a graph similar to a control ﬂow
graph by simply observing the patterns of system calls in
a running program. The paper proves that given a set of
observed program behavior, the algorithm constructs an ex-
ecution graph that is consistent with a control ﬂow graph
that is obtained through static analysis.
6.3 Static analysis
In the original research, the normal proﬁle was deter-
mined by observing running code and recording the se-
quences of system calls that were executed during normal
behavior. However, learning normal at runtime has limita-
tions. First, incompletely learning normal can result in false
positives. Second, if learning takes place online, in a vul-
nerable system, then attacks could potentially be injected
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 
into the normal deﬁnition. Finally, normal can change, for
example, when a system is reconﬁgured, and hence require
relearning.
To address these issues, researchers used static analysis
of the program source [73, 22] or binary [20, 5] to develop
models of legitimate code paths. This guarantees zero false
positives, and no attack injection during learning. The de-
fense mechanism is ready to deploy immediately, without
any vulnerable learning period. Such an approach can be ef-
fective at deﬁning a normal proﬁle that detects foreign code
injection, such as buffer overﬂow attacks, Trojans, and for-
eign library calls [74].
However, this approach is limited because attacks often
exploit code paths that exist in the program source but are
never or rarely used in normal behavior, e.g., a conﬁguration
error such as not disabling debugging access. This problem
can be mitigated by incorporating more information about
the program environment (e.g., conﬁguration, command-
line parameters, environment variables) into the static anal-
ysis [19]. In general, static and dynamic analysis can com-
plement each other, e.g., by using static analysis to generate
a base normal proﬁle, and then incorporating reﬁnements
suggested by dynamic analysis [83].
6.4 Other observables
The general idea of proﬁling program behavior using se-
quences of operations that indicate code ﬂow is a powerful
one that can be applied to many observables other than sys-
tem calls. For example, Jones and Lin used sequences of
library calls, rather than system calls [30]. Similar to the
original system call sequence research, Jones and Lin ig-
nored parameters to library calls and only monitored the se-
quences of calls. They demonstrated that library calls are a
feasible observable for anomaly detection, and can be used
to detect a variety of attacks, including buffer overﬂows,
denial-of-service attacks and Trojans. In another example,
Xu et al. modeled control ﬂow at the level of function calls
by inserting waypoints into the code at the entry and exit of
functions [82], and Gaurev and Keromytis augmented sys-
tem call monitoring with libc function monitoring [36]. An
even more ﬁne-grained approach to control ﬂow is moni-
tor at the level of machine code instructions [1, 63]. This
approach can give better attack detection, but at the cost of
increased overhead (up to 50% in some cases).
In some domains, other observables may in fact be more