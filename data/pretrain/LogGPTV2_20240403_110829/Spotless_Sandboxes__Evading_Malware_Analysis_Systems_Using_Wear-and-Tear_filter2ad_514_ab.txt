11
21
222
191K
66
511
81
1.7
2.9
28K
1.6K
3K
1003
274
340
225
303
58
3
176
35
32
10
23
98
98K
83
332
29
0
1.4
13.8
4
135
23
20
3
370
256
Baseline
41
2
334
184
48
23
0
0
0
0
8.2
10
0
0
2.6M
3
5
3
6
0
16
35M
18
1
26
23
25
9
13
11
38K
163
358
60
0
1
1.64
1
2
1
1
0
0
0
Every time an application or driver is installed on a
Windows system, it typically stores many key-value pairs in
the registry. This effect is so extensive that Windows is known
from suffering from “registry bloat,” since programs often add
keys during installation, but neglect to remove them when
uninstalled. This is exempliﬁed by the multitude of “registry
cleaning” tools which aim to clean the registry from stale
entries [51], [52]. Many of the registry artifacts we consider
are thus related to the footprints of the applications that
have been installed on the system (e.g., number of installed
applications, uninstall entries, shared DLLs, applications set to
automatically run upon boot), as well those that have not been
fully uninstalled (e.g., “orphaned” entries left from removed
programs and listed DLLs that do not exist on disk).
We also consider various other system-wide properties,
including: the size of the registry (in bytes); the number of
ﬁrewall rules, as new applications often install additional rules;
the number of previously connected USB removable storage
devices (a unique instance ID key is generated for each device
and stored in a cache); the number of entries in the UserAssist
cache, which contains information about the most frequently
opened applications and paths; the number of entries in the
1013
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
Application Compatibility Infrastructure (Shim) cache, which
contains metadata of previously run executables that used this
facility; and the number of entries in the MUICache, which
are related to previously run executables and are generated by
the Multi User Interface that provides support for different
languages.
5) Browser: For many users, an operating system does
little more than housing their web browser and a few other
applications. Web browsers have become an all-inclusive tool
for accessing ofﬁce suites, games, e-shopping, e-banking, and
social networking services. Due to the indispensable use of a
browser, it would be really uncommon to ﬁnd a browser in a
pristine, out-of-the-box condition on a real user system.
In fact, the older a system is, the longer the accumulated
“history” of its main browser will likely be. This history
is captured in various artifacts related to past user activity,
including previously visited URLs, stored HTTP cookies, saved
bookmarks, and downloaded ﬁles. As noted earlier, we consider
only aggregate counts of such artifacts, instead of more detailed
data, to respect users’ privacy. Given that a person may use
more than one browser, or a different one than the operating
system’s default browser, the browser artifacts that we consider
correspond to combined values across all browsers found on the
system (our tool currently supports Internet Explorer, Firefox,
and Chrome). For example, the number of HTTP cookies
corresponds to the total number of cookies extracted from all
installed browsers. The number of installed browsers is also
considered as a standalone artifact.
IV. DATA COLLECTION
A. Probe Tool Implementation
To gather a large and representative dataset of the identiﬁed
wear-and-tear artifacts from real user systems and malware
sandboxes, we implemented a probe tool that collects artifact
information and transmits it to a server. The probe tool was
implemented in C++ and consists of a single Windows 32-bit
PE ﬁle that does not require any installation, and which does
not have any dependencies besides already available system
APIs. Although many artifacts are easy to collect by simply
accessing the appropriate log ﬁles and directories, probably the
most challenging aspect of our implementation was to maximize
compatibility with as many Windows versions as possible, from
Windows XP to Windows 10. Besides differences in the paths
of various system ﬁles and directories across major versions,
in many cases we also had to use different system APIs for
Windows XP compared to more recent Windows versions
for various system-related artifacts (e.g., event log statistics).
Browser artifacts are collected separately for each of the three
supported browsers (IE, Firefox, Chrome) that are found on a
given system, and are synthesized into compound artifacts at
the server side.
The collected information is transmitted to our server through
HTTPS. The tool uses both GET and POST requests, in case
either of the two is blocked by a sandbox. Besides the collected
data, additional metadata for each submission include the IP
address of the host, OS version and installation date, and BIOS
vendor. The latter is used as a simple VM-detection heuristic,
to prevent poisoning of our real-user dataset with any results
from virtual machines (we explicitly asked users to run the
tool only on their real systems).
Each executable submitted to a public sandbox is generated
with a unique embedded ID, which allows us to identify
multiple submissions by the same vendor. Since a given unique
instance of the tool may run on more than one sandboxes
(e.g., due to the use of multiple different sandboxes by the
same vendor, or due to sample sharing among vendors), we
additionally distinguish between different systems based on
the combination of the following keys: reported IP address,
OS installation date, Windows version, BIOS vendor, number
of installed user applications, and elapsed time since the ﬁrst
system event.
B. IRB Approval and User Involvement
Our experiments for the collection of artifact values from
real user systems involved human subjects, and thus we had
to obtain institutional review board (IRB) approval prior to
conducting any such activity. Our IRB application included
a detailed description of the information to be collected, the
process a participant would follow, and all the measures that
were taken to protect the participants’ privacy and anonymity.
Our activities did not expose users to any risk. The collected
data does not include any form of personally identiﬁable
information (PII), nor any such information can be derived
from it. Due to the nature of our experiment (collecting simple
system statistics), our probe tool is small and simple enough
so that we can be conﬁdent about the absence of any ﬂaws
or bugs that would cause damage or mere inconvenience to
a user’s system and data. The tool does not get installed on
the system, and thus users can simply delete the downloaded
executable to remove all traces of the experiment. The collected
data merely reﬂects the wear and tear of a user’s system as a
result of normal use, rather than some personal trait that could
be considered PII. Even when considering the full information
that is recorded in the transmitted data, no personal information
about the user of the system can be inferred.
Based on the above, the IRB committee of our institution
approved the research activity on April 11, 2016.
All participants (colleagues, friends, and Amazon’s Mechan-
ical Turk workers) were directed to a web page that includes
an overview of the experiment, a detailed description of the
collected information (including a full sample report from one
of the authors’ laptop), and instructions for downloading and
executing our probe tool (as well as deleting it afterwards). The
page was hosted using our institution’s second-level domain
and TLS certiﬁcate, and included information about the IRB
approval, as well as the authors’ full contact details.
Given that our email requests to friends and colleagues for
downloading and executing a binary from a web page could
be considered as spear phishing attempts, we pointed to proof
for the legitimacy of the message, which was hosted under the
authors’ institutional home pages.
1014
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: Distribution of BIOS vendors of users’ machines.
BIOS vendor
American Megatrends Inc.
Dell Inc.
Insyde
Lenovo
Hewlett-Packard
Award Software International Inc.
Phoenix Technologies Ltd.
Acer
AMI
Intel Corp.
Alienware
Toshiba
Other
Frequency
25.77%
18.08%
15.38%
15.38%
6.92%
3.85%
3.85%
1.92%
1.92%
1.92%
1.54%
1.54%
1.93%
C. Data Collection
To quantify the difference of wear-and-tear artifacts be-
tween real users and sandbox environments, we use the
aforementioned probing tool to compile three datasets. The
ﬁrst dataset contains wear-and-tear artifacts collected from 270
real user machines (Dreal). The majority of these observations
(89.4%) originate from Amazon Mechanical Turk workers, who
downloaded and executed our artifact-probing tool, and the
remaining from systems operated by friends and colleagues.
The users who participated in this study come from 35 different
countries, with the top countries being: US (44%), India (18%),
GB (10%), CA (8%), NL (1%), PK (1%), RU (1%) and 28 other
countries with lower than 1% frequency. Therefore, we argue
that our dataset is representative of users from both developed
and developing countries, whose machines may have different
wear-and-tear characteristics. The BIOS vendor distribution of
the user systems is shown in Table II. As the table shows, the
users’ personal systems include different brands, such as Dell,
HP, Lenovo, Toshiba, and Acer, as well as game series systems
such as Alienware.
As mentioned earlier, our tool does not collect any PII and
does not affect the user’s machine in any way, i.e., it does not
modify or leave any data. After running the tool, the user can
simply delete the executable.
While analyzing the collected data from Mechanical Turk to
ﬁlter out those users who, despite our instructions, executed our
tool inside a virtual machine (identiﬁed through the collected
BIOS information), we observed that our server had collected
artifacts that could not be traced back to Mechanical Turk
workers. By analyzing the artifacts and the logs of our server,
we realized that, because the public Human Intelligence Task
(HIT) web page for our task on Amazon’s website was
pointing to our tool download page through a link, the crawlers
of a popular search engine followed that link, downloaded
our probing tool, and executed it. Based on the IP address
space of the clients who reported these artifacts, as well
as custom identiﬁers found in the reported BIOS versions,
we are conﬁdent that these were sandbox environments that
are associated with a popular search engine that downloads
and executes binaries as a way of proactively discovering
malware. After removing duplicate entries, we marked the
rest as belonging to “crawlers” and incorporated them in our
sandbox artifact dataset (described in the next paragraph).
The second dataset (Dsand) consists of artifacts extracted
by sandboxes belonging to various malware analysis services.
We identiﬁed these sandboxes through prior work on sandbox
evasion [23], as well as by querying popular search engines for
keywords associated with malware analysis. Most sandboxes
are not available for download, but do allow users to submit
suspicious binaries which they then analyze and report on the
activity of the analyzed binary, in varying levels of detail. Even
though we were able to ﬁnd 23 sandboxes that provide users
the ability to upload suspicious ﬁles, our artifact-gathering
server was able to collect artifacts from 15 different sandbox
environments (not counting the aforementioned search-engine
sandbox). We reason that the remaining analysis environments
are either non-operational, rely on static analysis to scan
executables, or completely block any network activity of each
analyzed binary.
Table III lists the names of sandboxes from which our server
received the results of our probing tools. We uploaded our
probing tool multiple times in a period of a month and we
also witnessed that some sandboxes executed our probing
tool multiple times on different underlying environments (e.g.,
multiple versions of Microsoft Windows). As mentioned in
Section IV-A, each uploaded executable had a unique ID
embedded in its code which was sent to our artifact-gathering
server, so as to perform accurate attribution.
The third and ﬁnal dataset (Dbase) is our baseline dataset
which consists of artifacts extracted from fresh installations of
multiple versions of Microsoft Windows. Next to setting up
local VMs and installing Microsoft Windows XP, Windows
Vista, Windows 7, and Windows 8, we also make use of all
the Windows server images available in two large public cloud
providers, Azure and AWS. Overall, Dbase contains the artifacts
extracted from 22 baseline environments.
D. Dataset Statistics
Table IV shows the number of different Microsoft Windows
versions in our three collected datasets. There we see that
the vast majority of users are utilizing Windows 7, 8 and
10, with only 4.1% of users still utilizing Windows XP and
Windows Vista. Contrastingly, we see almost no sandboxes
utilizing Windows 10, and the majority of them (83.7%)
utilizing Windows 7. Note that because we extract the version
of Windows programmatically, the same version number can
correspond to more than one Windows versions, such as,
Windows Server 2012 R2 and Windows Server 2016 having the
same version with Windows 8.1 and Windows 10. Even though
it is not the central focus of our paper, we ﬁnd it troublesome
that sandboxes do not attempt to follow the distribution of
operating systems from real user environments.
Figure 1 shows boxplots that reveal the distribution of the
values of each wear-and-tear artifact across all three datasets
1015
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
l
e
u
a
v
d
e
z
i
l
a
m
r
o
N
1.00
0.75
0.50
0.25
0.00
s
e
i
r
t
n
E
e
h
c
a
c
p
r
a
_
n
s
e
i
r
t
n
E
e
h
c
a
c
s
n
d
_
n
s
e
i
r
t
n
E
l
i
t
U
t
r
e
c
_
n
s
n
o
i
t
c
e
n
n
o
C
p
c
t
_
n
t
i
n
u
o
C
n
B
e
c
y
c
e
r
_
d
l
i
i
e
z
S
n
B
e
c
y
c
e
r
_
d
l
i
e
z
S
s
e
l
i
F
p
m
e
_
d
t
t
n
u
o
C
s
e
l
i
F
p
m
e
_
d
t
User
Sandbox
Baseline
c
r
s
s
y
s
_
e
c
r
s
p
p
a
_
e
t
d
p
u
n
w
_
e
i
i
e
z
S
g
e
r
_
r
t
n
u
o
C
l
l
i
a
t
s
n
n
u
_
r
t
n
u
o
C
d
e
n
a
h
p
r
o
_
r
t
n
u
o
C
e
h
c
a
C
m
h
s
_
r
i
t
l
n
u
o
C
s
C
e
c
v
e