Network Trafﬁc Generation.
Before beginning our
discussion of obfuscation systems, it is important to point
out the connection that they share with the broader area
of network trafﬁc generation. Most trafﬁc generation
systems focus on simple replay of captured network ses-
sions [33, 19], replay with limited levels of message con-
tent synthesis [12, 31], generation of trafﬁc mixes with
speciﬁc statistical properties and static content [10, 37],
or heavyweight emulation of user behavior with appli-
cations in virtualized environments [43]. As we will
see, many mimicry and tunneling systems share similar
strategies with the the key difference that they must also
transport useful information to circumvent ﬁltering.
Randomization.
For systems implementing the ran-
domization approach, the primary goal is to remove all
static ﬁngerprints in the content and statistical charac-
teristics of the connection, effectively making the traf-
ﬁc look like “nothing.” The obfs2 and obfs3 [34] pro-
tocols were the ﬁrst to implement this approach by re-
encrypting standard Tor trafﬁc with a stream cipher,
thereby removing all indications of the underlying pro-
tocol from the content. Recently, improvements on this
approach were proposed in the ScrambleSuit system [42]
and obfs4 protocol [34], which implement similar con-
tent randomization, but also randomize the distribution
of packet sizes and inter-arrival times to bypass both DPI
and trafﬁc analysis strategies implemented by the censor.
The Dust system [40] also offers both content and statis-
tical randomization, but does so on a per-packet, rather
than per-connection basis. While these approaches pro-
vide fast and efﬁcient obfuscation of the trafﬁc, they only
work in environments that block speciﬁc types of known-
bad trafﬁc (i.e., blacklists).
In cases where a whitelist
strategy is used to allow known-good protocols, these
randomization approaches fail to bypass ﬁltering, as was
demonstrated during recent elections in Iran [13].
Mimicry. Another popular approach is to mimic cer-
tain characteristics of popular protocols, such as HTTP
or Skype, so that blocking trafﬁc with those char-
acteristics would result in signiﬁcant collateral dam-
age. Mimicry-based systems typically perform shallow
mimicry of only a protocol’s messages or the statisti-
cal properties of a single connection. As an example,
StegoTorus [38] embeds data into the headers and pay-
loads of a ﬁxed set of previously collected HTTP mes-
sages, using various steganographic techniques. How-
ever, this provides no mechanism to control statistical
properties, beyond what replaying of the ﬁlled-in mes-
sage templates achieves. SkypeMorph [26], on the other
hand, relies on the fact that Skype trafﬁc is encrypted and
focuses primarily on replicating the statistical features of
packet sizes and timing.
Ideally, these mimicked pro-
USENIX Association  
24th USENIX Security Symposium  369
tocols would easily blend into the background trafﬁc of
the network, however research has shown that mimicked
protocols can be distinguished from real versions of the
same protocol using protocol semantics, dependencies
among connections, and error conditions [20, 17]. In ad-
dition, they incur sometimes signiﬁcant amounts of over-
head due to the constraints of the content or statistical
mimicry, which makes them much slower than random-
ization approaches.
Tunneling.
Like mimicry-based systems, tunneling
approaches rely on potential collateral damage caused
by blocking popular protocols to avoid ﬁltering. How-
ever, these systems tunnel their data in the payload of
real instances of the target protocols. The Freewave [21]
system, for example, uses Skype’s voice channel to en-
code data, while Facet [24] uses the Skype video chan-
nel, SWEET [47] uses the body of email messages, and
JumpBox [25] uses web browsers and live web servers.
CensorSpoofer [36] also tunnels data over existing proto-
cols, but uses a low-capacity email channel for upstream
messages and a high-capacity VoIP channel for down-
stream. CloudTransport [8] uses a slightly different ap-
proach by tunneling data over critical (and consequently
unblockable) cloud storage services, like Amazon S3,
rather than a particular protocol. The tunneling-based
systems have the advantage of using real implementa-
tions of their target protocols that naturally replicate all
protocol semantics and other distinctive behaviors, and
so they are much harder to distinguish. Even with this ad-
vantage, however, there are still cases where the tunneled
data causes tell-tale changes to the protocol’s behavior
[17] or to the overall trafﬁc mix through skewed band-
width consumption. In general, tunneling approaches in-
cur even more overhead than shallow mimicry systems
since they are limited by the (low) capacity of the tun-
neling protocols.
Programmable Systems.
Finally, programmable ob-
fuscation systems combine the beneﬁts of both random-
ization and mimicry-based systems by allowing the sys-
tem to be conﬁgured to accommodate either strategy.
Currently, the only system to implement programmable
obfuscation is Format-Transforming Encryption (FTE)
[15], which transforms encrypted data into a format
dictated by a regular expression provided by the user.
The approach has been demonstrated to have both high
throughput and the ability to mimic a broad range of
application-layer protocols, including randomized con-
tent. Unfortunately, FTE only focuses on altering the
content of the application-layer messages, and not statis-
tical properties, protocol semantics, or other potentially
distinguishing trafﬁc features.
Comparison with Marionette. Overall, each of these
systems suffers from a common set of problems that we
address with Marionette. For one, these systems, with
the exception of FTE, force the user to choose a sin-
gle target protocol to mimic without regard to the user’s
throughput needs, network restrictions, and background
trafﬁc mix. Moreover, many of the systems focus on only
a ﬁxed set of trafﬁc features to control, usually only con-
tent and statical features of a single connection. In those
cases where tunneling is used, the overhead and latency
incurred often renders the channel virtually unusable for
many common use cases, such as video streaming. The
primary goal of Marionette, therefore, is not to develop
a system that implements a single obfuscation method to
defeat all possible censor strategies, but instead to pro-
vide the user with the ability to choose the obfuscation
method that best ﬁts their use case in terms of breadth of
target protocols, depth of controlled trafﬁc features, and
overall network throughput.
3 Models and Actions
We aim for a system that enables broad control over
several trafﬁc properties, not just those of individual
application-layer protocol messages. These properties
may require that the system maintain some level of
state about the interaction to enforce protocols seman-
tics, or allow for non-deterministic behavior to match
distributions of message size and timing. A natural ap-
proach to efﬁciently model this sort of stateful and non-
deterministic system is a special type of probabilistic
state machine, which we ﬁnd to be well-suited to our
needs and ﬂexible enough to support a wide range of de-
sign approaches.
Marionette models.
A Marionette model (or just
model, for short) is a tuple M = (Q, Qnrm, Qerr, C, ∆).
The state set Q = Qnrm ∪ Qerr, where Qnrm is the set of
normal states, Qerr is the set of error states, and Qnrm ∩
Qerr = ∅. We assume that Qnrm contains a distinguished
start state, and that at least one of Qnrm, Qerr contains
a distinguished ﬁnish state. The set C is the set of ac-
tions, which are (potentially) randomized algorithms. A
string B = f1f2 ··· fn ∈ C∗ is called an action-block,
and it deﬁnes a sequence of actions. Finally, ∆ is a tran-
sition relation ∆ ⊆ Q×C∗×(dist(Qnrm)∪∅)×P(Qerr)
where dist(X) the set of distributions over a set X, and
P(X) is the powerset of X. The roles of Qnrm and Qerr
will be made clear shortly.
A tuple (s,B, (µnrm, S)) ∈ ∆ is interpreted as fol-
lows. When M is in state s,
the action-block B
may be executed and, upon completion, one samples a
state s(cid:30)nrm ∈ Qnrm (according to distribution µnrm ∈
370  24th USENIX Security Symposium 
USENIX Association
dist(Qnrm)).
If the action-block fails, then an error
state is chosen non-deterministically from S. Therefore,
{s(cid:31)nrm} ∪S is the set of valid next states, and in this
way our models have both proper probabilistic and non-
deterministic choice, as in probabilistic input/output au-
tomata [45]. When (s,B, (µnrm,∅)) ∈ ∆, then only
transitions to states in Qnrm are possible, and similarly
for (s,B, (∅, S)) with transitions to states in Qerr.
In practice, normal states will be states of the model
that are reached under normal, correct operation of the
system. Error states are reached with the system detects
an operational error, which may or may not be caused by
an active adversary. For us, it will typically be the case
that the results of the action-block B determine whether
or not the system is operating normally or is in error, thus
which of the possible next states is correct.
Discussion. Marionette models support a broad va-
riety of uses. One is to capture the intended state of a
channel between two communicating parties (i.e., what
message the channel should be holding at a given point in
time). Such a model serves at least two related purposes.
First, it serves to drive the implementation of procedures
for either side of the channel. Second, it describes what a
passive adversary would see (given implementations that
realize the model), and gives the communicating parties
some defense against active adversaries. The model tells
a receiving party exactly what types of messages may be
received next; receiving any other type of message (i.e.,
observing an invalid next channel state) provides a signal
to commence error handling, or defensive measures.
Consider the partial model in Figure 3 for an exchange
of ciphertexts that mimic various types of HTTP mes-
sages. The states of this model represent effective states
of the shared channel (i.e., what message type is to ap-
pear next on the channel). Let us refer to the ﬁrst-sender
as the client, and the ﬁrst-receiver as the server. In the
beginning, both client and server are in the start state.
The client moves to state http_get_js with probability
0.25, state http_get_png with probability 0.7, and state
NONE with probability 0.05.
In transitioning to any
of these states, the empty action-block is executed (de-
noted by ε), meaning there are no actions on the tran-
sition. Note that, at this point, the server knows only
the set {http_get_js, http_get_png, NONE} of valid states
and the probabilities with which they are selected.
Say that the client moves to state http_get_png, thus
the message that should be placed on the channel is to
be of the http_get_png type. The action-block Bget_png
gives the set of actions to be carried out in order to affect
this. We have annotated the actions with “c:” and “s:”
to make it clear which meant to be executed by the client
and which are meant to be executed by the server, respec-
 ε , .25 
 ε , .7 
START
 ε , .05 
Bget_js , 0.85
http_ok_js
http_get_js
Bget_js , 0.15
Bget_png , 0.1
http_404
http_get_png
Bget_png , 0.9
http_ok_png
Bok_js
B404
Bok_png
Bget_png
Bget_png
NONE
 ε , 1.0 
ERROR
(parse fail)
Berr-parse
(error-handling paths)
ERROR
(decrypt fail)
Berr-decrpyt
Bget_png:
  c: X=encrypt(M,http_get_png)
  c: Y=postprocess(X,http_get_png)
  s: X=parse(Y,http_get_png)
  s: M=decrypt(X,http_get_png)
Figure 3: A partial graphical representation of a Marionette
model for an HTTP exchange. (Transitions between http_get_js
and error states dropped to avoid clutter.) The text discusses
paths marked with bold arrows; normal states on these are blue,
error states are orange.
tively. The client is to encrypt a message M using the pa-
rameters associated to the handle http_get_png, and then
apply any necessary post-processing in order to produce
the (ciphertext) message Y for sending. The server, is
meant to parse the received Y (e.g.
to undo whatever
was done by the post-processing), and then to decrypt
the result.
If parsing and decrypting succeed at the server, then
the state selected by the client was
it knows that
http_get_png and, hence, that it should enter http_404
with probability 0.1, or http_ok_png with probability
0.9. If parsing fails at the server (i.e. the server action
parse(Y,http_get_png) in action block Bget_png fails) then
the server must enter state ERROR (parse fail). If parsing
succeeds but decryption fails (i.e., the server action de-
crypt(X,http_get_png) in action block Bget_png fails) then
the server must enter state ERROR (decrypt fail). At this
point, it is the client who must keep alive a front of po-
tential next states, namely the four just mentioned (error
states are shaded orange in the ﬁgure). Whichever state
the server chooses, the associated action-block is exe-
cuted and progress through the model continues until it
reaches the speciﬁed ﬁnish state.
Models provide a useful design abstraction for spec-
ifying allowable sequencings of ciphertext messages, as
well as the particular actions that the communicating par-
ties should realize in moving from message to message
(e.g., encrypt or decrypt according to a particular cipher-
text format). In practice, we do not expect sender and
USENIX Association  
24th USENIX Security Symposium  371
receiver instantiations of a given model will be identical.
For example, probabilistic or nondeterministic choices
made by the sender-side instantiation of a model (i.e.,
which transition was just followed) will need to be “de-
terminized” by the receiver-side instantiation. This de-
terminization process may need mechanisms to handle
ambiguity. In Section 7 we will consider concrete speci-
ﬁcations of models.
4 Templates and Template Grammars
In an effort to allow ﬁned-grained control over the for-
mat of individual ciphertexts on the wire, we introduce
the ideas of ciphertext-format templates, and grammars
for creating them. Templates are, essentially, partially
speciﬁed ciphertext strings. The unspeciﬁed portions are
marked by special placeholders, and each placeholder
will ultimately be replaced by an appropriate string, (e.g.,
a string representing a date, a hexadecimal value repre-
senting a color, a URL of a certain depth). To compactly
represent a large set of these templates, we will use a
probabilistic context-free grammar. Typically, a gram-
mar will create templates sharing a common motif, such
as HTTP request messages or CSS ﬁles.
Template Grammars.
A template grammar G =
(V, Σ, R, S, p) is a probabilisitic CFG, and we refer to
strings T ∈ L(G) as templates. The set V is the set of
non-terminals, and S ∈ V is the starting non-terminal.
The set Σ = Σ ∪ P consists of two disjoint sets of sym-
bols: Σ are the base terminals, and P is a set of place-
holder terminals (or just placeholders). Collectively, we
refer to Σ as template terminals. The set of rules R con-
sists of pairs (v, β) ∈ V × (V ∪ Σ)∗, and we will some-
times adopt the standard notation v → β for these. Fi-
nally, the mapping p : R → (0, 1] associates to each rule
a probability. We require that the sum of values p(v,·)
for a ﬁxed v ∈ V and any second component is equal
to one. For simplicity, we have assumed all probabil-
ities are non-zero. The mapping p supports a method
for sampling templates from L(G). Namely, beginning
with S, carry out a leftmost derivation and sample among
the possible productions for a given rule according to the
speciﬁed distribution.
Template grammars produce templates, but it is not
templates that we place on the wire.
Instead, a tem-
plate T serves to deﬁne a set of strings in Σ∗, all of which
share the same template-enforced structure. To produce
these strings, each placeholder γ ∈ P has associated to
it a handler. Formally, a handler is a algorithm that takes
as inputs a template T ∈ Σ∗ and (optionally) a bit string
c ∈ {0, 1}∗, and outputs a string in Σ∗ or the distin-
guished symbol ⊥, which denotes error. A handler for γ
scans T and, upon reading γ, computes a string in s ∈ Σ∗
and replaces γ with s. The handler halts upon reaching
the end of T , and returns the new string T (cid:30) that is T but
will all occurrences of γ replaced. If a placeholder γ is
to be replaced with a string from a particular set (say a
dictionary of ﬁxed strings, or an element of a regular lan-
guage described by some regular expression), we assume
the restrictions are built into the handler.
As an example, consider the following (overly simple)
production rules that could be a subset of a context-free
grammar for HTTP requests/responses.
(cid:31)header(cid:30) → (cid:31)date_prop(cid:30): (cid:31)date_val(cid:30)\r\n
| (cid:31)cookie_prop(cid:30): (cid:31)cookie_val(cid:30)\r\n
(cid:31)date_prop(cid:30) →Date
(cid:31)cookie_prop(cid:30) →Cookie
date
(cid:31)date_val(cid:30) →γ
(cid:31)cookie_val(cid:30) →γ
To handle our placeholders γdate
cookie
and γcookie,
we might
replace the former with the result of
FTE[”(Jan|Feb|...”)], and the latter with the result of
running FTE[”([a-zA-Z...)”]. In this example our FTE-
based handlers are responsible for replacing the place-
holder with a ciphertext that is in the language of its in-
put regular expression. To recover the data we parse the
string according to the the template grammar rules, pro-
cessing terminals in the resultant parse tree that corre-
spond to placeholders.
5 System Architecture
In Section 3 we described how a Marionette model can