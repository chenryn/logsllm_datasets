conditions, FLOWDISTsim skips the static analysis, and
simply instruments all methods and branches in D in this
step, with the rest being the same as FLOWDIST.
FLOWDISTmul: With some systems, the FLOWDISTsim
design is well justiﬁed. Yet probing for and then tracing all
method and branch events in D incurs substantial costs. To
reduce these costs, we introduce an intermediate phase, with
two more changes, to FLOWDISTsim. The idea is to have a
multi-staged reﬁnement-based design in Phase 1 itself. First,
the new Phase 1 only probes for and traces the ﬁrst entry and
last returned-into events of each method, and then computes
method-level ﬂow paths from those events. The intermediate
phase then probes for and traces the coverage of branches
in, and all instances of both kinds of events of, methods on
such paths. Lastly, the Step 2.2 is removed from Phase 2.
Since FLOWDISTmul requires multiple executions of the
same system against the same input (in the ﬁrst and
intermediate phases), this design is optimized for systems
with deterministic executions—inconsistencies between the
two executions could compromise the soundness of the
DIFA as a whole. Another condition is that the cost reduction
outweighs the costs incurred by the intermediate phase.
According to the rationale of each alternative design,
FLOWDISTsim is expected to perform the best
for
small/simple systems with non-deterministic executions,
while FLOWDISTmul is the best for such systems without
non-deterministic executions. For large/complex systems,
FLOWDIST would perform the best. These contrasts are
justiﬁed by the conditions (as described above) under which
either alternative design is motivated and best ﬁts; when none
of those conditions are met, FLOWDIST is superior in general.
Implementation and Limitations
5
We implemented FLOWDIST and its alternative designs for
Java based on Soot [87] while reusing our dependence
analyzers [48, 49]. Our tools take Java bytecode directly
and account for data/control ﬂows due to exception-handling
constructs and reﬂection. For computing threading-induced
dependencies, we reused relevant parts of Indus [107].
Additional implementation details can be found in [66, 68].
Due to their common inability to fully analyze dynamic
language features (e.g., complex cases of reﬂection, native
code) the static analyses in our tools are soundy [91] but not
sound [78]. Since they compute information ﬂow paths based
on dynamic dependencies projected from static ones, while
considering a speciﬁc system execution only, our tools may
suffer from false negatives (akin to under-tainting in DTA).
Our tools do not address the problem of identifying the
sources/sinks of interest, which are assumed to be given in
the default source/sink lists or speciﬁed differently by users.
Also, as with any dynamic analysis, the analyses in our tools
are limited to the program parts that are exercised at runtime.
Thus, their capabilities of discovering a bug rely on that (1) the
relevant source and sink are speciﬁed and (2) the source and
sink are covered by the run-time inputs considered. Moreover,
considering the security context in speciﬁc usage scenarios
(e.g., external protection mechanisms applied to the source or
sink), our tools may suffer from false positives as they do not
analyze, nor have access to, those external/context factors.
Finally, our tools require static instrumentation, thus they
may not suit scenarios where the system cannot be modiﬁed.
Additional limitations of FLOWDISTsim and FLOWDISTmul
are those implied by the respective system conditions
discussed earlier (e.g., the system execution is deterministic).
6 Evaluation
Our evaluation was guided by the following questions:
RQ1 How effective is FLOWDIST in terms of its precision?
RQ2 How efﬁcient is FLOWDIST in terms of its costs?
USENIX Association
30th USENIX Security Symposium    2099
Table 1: Subject distributed programs and test inputs used
Subject
#SLOC #Method
Scenario
NIOEcho
MultiChat
ADEN
Raining Sockets
OpenChord
Thrift
xSocket
ZooKeeper
412
470
4,385
6,711
9,244
14,510
15,760
62,194
RocketMQ
105,444
Voldemort
Netty
115,310
167,961
27 Client-Server
Peer-to-Peer
37
260
Peer-to-Peer
319 Client-Server
Peer-to-Peer
736
1,941 Client-Server
Peer-to-Peer
2,209
Client-Server
5,383 N-tier
N-tier
N-tier
N-tier
Client-Server
6,198
20,406 N-tier
N-tier
12,389 N-tier
HSQLDB
326,678
10,095
Client-Server
N-tier
Tests
Integration
Integration
Integration
Integration
Integration
Integration
Integration
Integration
Load
System
Integration
System
Integration
Load
System
Integration
Integration
System
RQ3 How scalable is FLOWDIST?
RQ4 Can FLOWDIST ﬁnd real-world vulnerabilities?
RQ5 Can FLOWDIST discover new vulnerabilities?
RQ6 How does FLOWDIST compare to the state of the art?
RQ7 How well do the alternative designs perform?
6.1 Experiment Setup
As shown in Table 1, we used 12 Java distributed systems
as subjects. The subject sizes are measured by numbers of
non-blank non-comment Java source code lines (#SLOC),
numbers of methods deﬁned in the subject (#Method),
and execution scenarios (Scenario) including client-server,
peer-to-peer, and n-tier. The last column lists the kinds of tests
available to us, from which the run-time inputs are drawn.
NioEcho [27] provides an echoing service for any
message sent by clients. MultiChat [26] is a chat service
broadcasting messages received from one client to others.
ADEN [25] offers a UDP-based alternative to TCP sockets.
Raining Sockets [24] is a non-blocking and sockets-based
framework. OpenChord [28] is a peer-to-peer network
service. Thrift [33] is a framework for developing scalable
cross-language services. xSocket [34] is an NIO-based
library for building high-performance computing (HPC)
software. ZooKeeper [30] is a coordination service achieving
consistency and synchronization in distributed systems.
RocketMQ [38]
is a distributed messaging platform.
Voldemort [29] is a distributed key-value store underlying
LinkedIn’s services. Netty [37]
is a framework for
rapid HPC application development. HSQLDB (HyperSQL
DataBase) [36] is an SQL relational database system.
We chose these subjects to cover various scales, application
domains, architectures, and mechanisms for message passing.
The system and load tests were part of the software packages
downloaded from the respective project websites. The
integration tests were created manually as per the ofﬁcial
documentation of each subject with concrete inputs. Both
valid and invalid inputs were considered. For each of these
tests, we ran two to ﬁve processes each on a different machine
per the typical use of each subject.
In each integration test, we started several server/client
instances and performed various operations, to cover main
subject features. Particularly for ADEN, Raining Sockets,
Thrift, xSocket, and Netty, which are frameworks/libraries, we
developed an application for each to cover its major functional
features and then exercised each of the applications. The
following are brief descriptions of operations and test inputs
involved in each integration test.
• NioEcho: We started a server and a client, sent random text
messages from the client to the server, and then waited for
the echo of each message.
• MultiChat: We started a server and three clients. From one
client we sent random text messages to the server which
broadcasted them to all other clients.
• ADEN: We started two nodes each of which sends messages
to and receives messages from the other node.
• Raining Sockets: We started a server and a client, and then
the client sent text messages to the server.
• OpenChord: We ﬁrst started three nodes A, B, and C. Then,
we performed following operations: On node A, create
an overlay network; on the other nodes B and C, join the
network; on the node C, insert a new data entry to the
network; on the node A, search and then remove the data
entry; Lastly, on the node B, list all data entries.
• Thrift: With a server and a client, a calculator application
was developed. The client sent some basic arithmetic
operations
(addition, subtraction, multiplication, and
division of two numbers, in order) to the server and got the
calculation results from the server.
• xSocket: Two nodes were started and then each sends
messages to the other node.
• ZooKeeper: Our operations were: create two nodes, search
them, look up their attributes, update their data association,
and remove these two nodes.
• RocketMQ: There are four components: a name server, a
broker, a producer, and a consumer. The server provides
reading and writing service and records full routing
information. The broker stores messages. The producer
sends messages to the broker. The customer receives
messages from the broker.
• Voldemort: We performed the following operations in order:
add a key-value pair, ﬁnd the key for its value, remove the
key, and retrieve the pair.
• Netty: We develop a 3-tier application with three nodes.
The ﬁrst node read an email list from a ﬁle and then sent
relevant emails to the second node. Next, the second node
encrypted the emails using the RSA algorithm and then sent
them to the third node. Lastly, the third node used Postﬁx
to send emails received.
2100    30th USENIX Security Symposium
USENIX Association
• HSQLDB: We started a database server and a client. Then,
the client sent a SQL query to the server and then received
the SQL result from the server.
We evaluated FLOWDIST via its implementation for Java,
thus we set the sources and sinks (found in [65]/data)
based on our understanding of security-related APIs in the
Java SDK, as default. We used the list of message-passing
APIs (§3.1)
in the Java SDK to cover Java Socket
I/O, ObjectStream I/O, and Java NIO APIs (as listed
in [65]/Message_PassingAPIList.txt).
6.2 Experimental Methodology
Given the default user conﬁguration, we considered pair-wise
pairing of all sources and sinks as queries against each subject
execution. Due to the absence of ground truth, for each query
we manually checked the (statement-level) information ﬂow
paths produced by FLOWDIST to compute precision.
Speciﬁcally, for each path, we tracked the dependencies
of the source; then we considered the path a true positive if
we reached the sink without encountering any sanitization
via the path, and a false positive otherwise. FLOWDIST
does not support sanitization at the moment—its current
implementation does not check if a resulting ﬂow path
contains sanitizing operations. Yet among the paths we
examined, we did not ﬁnd sanitized ones. Also, we manually
constructed the ground truth for three subjects to evaluate
recall. In each case of manual analysis, the two authors and a
non-author CS graduate student each inspected independently;
then they cross-validated and conﬁrmed the result when all
three concurred. The manual check was time-consuming, thus
we randomly sampled only 20 paths when there were more
(otherwise we checked them all). We avoided taking more
than one path between each pair to reduce biases.
Regarding efﬁciency, we computed FLOWDIST’s time and
storage costs for each query and reported the average-case
numbers over all the queries per execution, in addition to
run-time slowdowns and static analysis costs. To evaluate
scalability, we used linear regression to model how those
numbers vary with changing code and trace sizes.
We are not aware of a prior DIFA/DTA, nor a ﬁne-grained
dynamic data ﬂow analysis that could serve the same purpose,
that works with diverse real-world distributed systems.
Thus, we compare FLOWDIST with PHOSPHOR [47] and
JOANA [75], the state-of-the-art dynamic and static taint
analyzers for single-process Java software, respectively. Our
study considered only this single baseline DIFA/DTA because
our extensive search for such tools and contact with the
authors of relevant papers ended up with no more comparable
tools to include (as further discussed in §7). We chose to
include JOANA to see how DIFA/DTA tools are compared
with static ones. The machines we used were all Ubuntu
16.04.3 LTS workstations with an Intel E7-4860 2.27GHz
CPU and 32GB DMI RAM.
Table 2: Numbers of intraprocess (Ir) source/sink pairs (Pr) and
information ﬂow paths (Ps), versus interprocess (Int) ones
66
42
0
12
14
4
10
9
1086
124
19
24
198
6
80
9
140
7
#IrPr #IrPs #IntPr #IntPs IntPs/AllPs
22.22%
0.00%
0.00%
0.00%
0.00%
100.00%
20.00%
0.00%
98.46%
100.00%
42.50%
100.00%
82.14%
0.00%
58.33%
40.00%
0.00%
66.67%
Execution