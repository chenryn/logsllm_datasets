4
8
1
2
4
8
Number of Processes
(a) Aggregate
Number of Processes
(b) Per-process
Figure 9: GPU throughput with an increasing number of CPU-processes up to the number of cores.
CPU−only
1−GPU
2−GPUs
CPU−only
1−GPU
2−GPUs
)
c
e
s
/
t
i
b
G
(
t
u
p
h
g
u
o
r
h
T
12
10
8
6
4
2
0
100
200
400
800
1500
1
2
4
8
Packet size (bytes)
(a) Different packet sizes.
Number of Processes
(b) Different numbers of CPU processes.
Figure 10: Overall sustained throughput for different workloads and conﬁgurations.
bus saturation, which was shown in Figure 7, is the main reason
for this upper bound. However, since the PCIe bus is a point-to-
point link, adding one more GPU device to the system increases
the aggregate GPU throughput to over 70 Gbit/s.
5.2.2 Overall Performance
In our next experiment, we measured the overall
Throughput.
processing throughput achieved by our multi-parallel implemen-
tation. Figure 10(a) shows the sustained throughput for different
packet sizes. We observe that for very small packet sizes, the GPU-
assisted design exhibits a slightly worse performance compared to
the multi-core approach alone. The main reason for this is that the
buffering overheads for very small packets are greater than the cor-
responding pattern matching costs, as shown in more detail in the
following experiment. Therefore, it is better in terms of perfor-
mance to match very small packets on the CPU, rather than trans-
ferring them to the GPU.
As a consequence, we adopted a simple opportunistic ofﬂoading
scheme, in which pattern matching of very small packets is per-
formed on the CPU instead of the GPU. Thus, only packets that
exceed a minimum size threshold are copied to the buffer that is
transferred to the GPU for pattern matching. The packets contain
already the TCP reassembled stream of a given direction, hence no
state needs to be shared between the CPU and the GPU. The min-
imum threshold can be inferred off line, using a simple proﬁling
measurement, or automatically at runtime. For simplicity we cur-
rently use the former method, although we plan to implement an
automated solution in the future.
Figure 10(b) shows the sustained throughput for a different num-
ber of CPU processes, using 1500-byte packets. We observe that as
the number of processes increases, the sustained throughput also in-
creases linearly. When pattern matching is ofﬂoaded on the GPU,
the throughput of the legacy multi-core implementation is increased
3.5–4.5 times, depending on the number of processes. The maxi-
mum throughput achieved by our base system reaches about 11.7
Gbit/s when utilizing all resources—eight CPU cores and two GPUs.
Finally, we observe that switching from one to two GPUs does
not offer signiﬁcant improvements to the overall performance. This
can be explained by the fact that GPU communication and compu-
tation costs are completely hidden by the overlapped CPU compu-
tation, as discussed in the following experiment.
Timing breakdown. We proceed and examine in greater detail the
overall performance achieved by proﬁling each device separately.
In Figures 11(a)–11(d) we plot the individual execution times for
various packet lengths. We show the times of each device with
different bars, since execution is performed in parallel. CPU and
GPU execution is pipelined, hence the CPU can continue unaf-
fected while GPU execution is in progress. Each bar represents the
execution time of the two GPU devices, while the thin line on each
bar represents the corresponding time when utilizing one GPU. We
observe that even when a single GPU is used, the cost for the data
transfers and the pattern matching on the GPU is completely hidden
by the overlapped CPU workload, for all packet sizes.
The extra cost for packet buffering before transferring them to
the GPU depends highly on the packet size. Small packets incur
higher cost per-byte, due to the start-up overhead of the memcpy(3)
function. 100-byte packets or smaller induce a prohibitively large
overhead, in comparison with the pattern matching cost. We tried
to optimize the copies using a byte-by-byte procedure instead of
calling the memcpy(3), however the overhead was still higher.
Thereupon, we avoid the small-packets penalty by opportunisti-
cally ofﬂoading pattern matching computation on the GPU depend-
ing on the packet length.
Finally, we notice that GPU execution times for small packets
304)
c
e
s
n
(
e
m
T
i
25
20
10
5
2.5
1
0.1
GPU time
CPU (Preprocess)
CPU (Postprocess)
CPU (Buffering)
)
c
e
s
n
(
e
m
T
i
40
20
10
5
2.5
1
0.1
GPU time
CPU (Preprocess)
CPU (Postprocess)
CPU (Buffering)
)
c
e
s
n
(
e
m
T
i
40
20
10
5
2.5
1
0.1
GPU time
CPU (Preprocess)
CPU (Postprocess)
CPU (Buffering)
)
c
e
s
n
(
e
m
T
i
40
20
10
5
2.5
1
0.1
GPU time
CPU (Preprocess)
CPU (Postprocess)
CPU (Buffering)
1
2
4
8
Number of Processes
1
2
4
8
Number of Processes
1
2
4
8
Number of Processes
1
2
4
8
Number of Processes
(a) 100-byte packets
(b) 200-byte packets
(c) 800-byte packets
(d) 1500-byte packets
Figure 11: Breakdown of per-byte processing overhead for different packet sizes.
also increase. The main reason for this is that the dimensions of the
buffer that is used for transferring the packets to the GPU are ﬁxed,
hence it is populated sparsely for small packets.
5.3 Overall Trafﬁc Processing Throughput
In this section, we measure the end-to-end performance of our
prototype implementation under realistic conditions.
5.3.1 Synthetic Trafﬁc
Figure 12(a) shows the packet loss ratio for different packet sizes,
when replaying trafﬁc at varying rates. We plot values up to the
maximum achieved replay rate, hence the smaller the packet size,
the lower the replay rate reached. For example, for 200-byte pack-
ets, we managed to replay trafﬁc at maximum rate of 1.86 Gbit/s,
while for 1500-byte packets we achieved a rate of 7.67 Gbit/s.
Given these trafﬁc replay rates, our prototype system begins to
drop packets at 7.22 Gbit/s for 1500-byte packets, which is a 253%
improvement over the traditional multi-core implementation. When
processing smaller packets, the performance falls to 1.5 Gbit/s,
which is slightly higher than the traditional multi-core implemen-
tation, although the drop rate is about 6.6 times lower.
Comparing the achieved throughput with the “ideal NIC” case
in Figure 10(a), we observe that the NIC adds a variable overhead
that depends on the size of the captured packets.
It is clear that
small packets add more latency to the capturing process than larger
ones. For the traditional multi-core approach, we observe an extra
overhead of 55% for 200-byte packets, that falls to 18% for 800-
byte packets, and 13% for 1500-byte packets. Similarly, the extra
overhead for the GPU-accelerated implementation is 110% for 200-
byte packets, about 87% for 800-byte packets, and 52% for 1500-
byte packets. We observe that the NIC overhead is larger in the
GPU-accelerated implementation, and we speculate that this is an
issue related to congestion in the PCIe controller.
5.3.2 Real Trafﬁc
In our ﬁnal experiment, we evaluate MIDeA in a scenario using
real trafﬁc. We used a trace of real network trafﬁc (referred to as
UNI), captured at the gateway of a large university campus with
several thousands of users. Speciﬁcally, the trace spans 74 minutes,
and includes all packets and their payloads, totalling 46 GB. Table 3
summarizes the most important properties of the trace.
To replay the captured trace at high-speed, it has to reside in the
main memory of the host to avoid disk accesses. Unfortunately, the
main memory of our two trafﬁc generator machines is only 4GB,
hence it is impossible to load the whole trace in memory. To over-
come this issue, we split the trace to several 2GB parts. While one
part is replayed, the other part is loaded into main memory. Since
reading from disk is much slower, each part is replayed several
Packets
Packet size (min/max/avg)
IP Fragments
TCP sessions
UDP sessions
Triggered Snort Alerts
73,162,723
60/1,514/ 679.57
88,411
185,642
174,442
183,050
Table 3: UNI trace properties.
Model
Qty Unit price
NIC: Intel 82599EB
CPU: Intel Xeon E5520
GPU: NVIDIA GTX480
1
2
2
$687
$336
$340
Table 4: Cost of MIDeA components (as of April 2011).
times, up until the next part is fully loaded into memory. Using the
above pre-fetching scheme, we successfully managed to replay the
captured trace with speeds of up to 5.7 Gbit/s.
Figure 12(b) shows the dropped packets when increasing the
trafﬁc rate. We also annotate the throughput achieved when reading
the network packets directly from main memory instead of the NIC.
The traditional multi-core implementation starts to drop packets at
1.1 Gbit/s, while the ideal throughput is near 1.4 Gbit/s. When
GPU acceleration is enabled, we did not observe any packet loss
for speeds of up to 5.2 Gbit/s. For comparison, the ideal through-
put is 7.8 Gbit/s.
6. DISCUSSION
So far in this paper we went over a detailed description of the
design aspects, trade-offs, and performance issues of our proposed
architecture. Even though we focused on the parallelization of an
intrusion detection system, we strongly believe that the proposed
model can beneﬁt a variety of other network monitoring applica-
tions, such as trafﬁc classiﬁcation, content-aware ﬁrewalls, spam
ﬁltering, and other network trafﬁc analysis systems. With this in
mind, we could easily augment a router with multi-parallel net-
work processing capabilities, expanding its functionality without
affecting its normal packet routing operations [13].
Price/Performance. For our hardware setup, we have selected
relatively low-end devices: two Intel Xeon E5520 processors, two
NVIDIA GeForce GTX 480 graphics cards, and an Intel 82599EB
10GbE NIC. Table 4 shows the approximate cost of each compo-
nent, as of April 2011. The total cost of our base system is about
$2739, achieving a throughput per dollar cost of 1.8 Mbps/$.
305100
80
60
40
20
)
%
(
s
s
o
l
t
e
k
c
a
P
0
0
2000
4000
6000
8000
10000
1500b (w/o GPU)
1500b (w/ GPU)
Rate (Mbit/s)
800b (w/o GPU)
800b (w/ GPU)
(a) Synthetic trafﬁc
200b (w/o GPU)
200b (w/ GPU)
100
80
60