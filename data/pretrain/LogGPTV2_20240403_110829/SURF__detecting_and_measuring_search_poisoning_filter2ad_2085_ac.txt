to promote their ranks and at the same time redirect lured search
users to the malicious terminal webpages. While in principle an at-
tacker can attempt to evade this group of features, this would force
the attacker to move to a completely different malicious network
infrastructure in which all malicious redirections and terminal ma-
licious pages are hosted on the same second-level domain, for ex-
ample. In addition to incurring the signiﬁcant cost of changing the
malicious network infrastructure, this could also increase the risk of
being detected by the compromised website’s administrators, thus
exposing search poisoning instances to a more prompt remediation,
and end up sacriﬁcing the established rogue landing pages which
typically take considerable amount of time and efforts to mature.
Chained webpages: This group of features measure properties of
the webpages involved in search user redirections. The IP-to-name
ratio, while useful for classiﬁcation in most practical cases, is
not very difﬁcult to evade because it only requires the attacker to
register more domain names. However, we would like to emphasize
that carrying an evasion attack on this feature will not signiﬁcantly
impact SURF’s accuracy, as shown in the quantitative analysis in
Section 3.4. The remaining two features are harder to evade. The
landing to terminal distance feature depends on the
attacker’s network infrastructure, and therefore the same arguments
we made for the redirection composition features hold in this case.
The page load/render errors may depend, for example,
on pages (and domains) along the redirection chain that have been
blacklisted or remediated. Therefore, this feature is not under the
attacker’s direct control and is difﬁcult to evade.
Poisoning resistance: Since these three features are derived from
the search result pages themselves with the help of public PageR-
ank data, which is determined by the search engine algorithms and
out of attackers’ control. Therefore these features are difﬁcult to
evade.
3.3 Prototype Implementation
SURF only requires a limited amount of data to be collected dur-
ing search-then-visit browsing sessions, and can be easily incorpo-
rated into a browser as a plugin. To demonstrate the effectiveness of
our detection approach, we implemented SURF on top of an instru-
mented version of Internet Explorer 8. This instrumented browser
leverages mshtml.dll for HTML parsing and rendering, and
is able to listen for event notiﬁcations (e.g., used to identify sub-
frame redirections) and peek into browsing data (e.g., HTML code
and user visible content) at different rendering stages. In addition,
SURF is capable of emulating simple user interactions during visit-
ing sessions that require certain user input to proceed (e.g., clicking
on a message dialogue box, activating a mouse-over action, etc.).
To perform our evaluation of SURF, we used several instrumented
browser instances and instructed them to retrieve lists of popu-
lar (or “trendy”) search keywords, query each keyword on both
Google and Bing, and visit the top 100 search results for each
query. Our evaluation data collection started in September 2010.
We deployed our browser on 20 virtual machines which would run
Figure 3: Threshold Curves (ROC)
daily to query search keywords that had reached popularity within
the past seven days.
In addition, our instrumented browser was
enhanced with BLADE [16], which we used to protect the VMs
from drive-by download malware infections and to log attempted
exploits. The obtained browsing information for each browsing
session was then saved for ofﬂine analysis. While we performed
our evaluation ofﬂine for convenience reasons, mainly to be able to
run cross-validation experiments on large datasets, once the statis-
tical classiﬁer has been trained SURF can detect search poisoning
instance online (see Figure 1).
3.4 Evaluation Results
Evaluation Dataset: To the best of our knowledge, there exist no
public labeled dataset of search results and their related user redi-
rection events related to search poisoning cases. Therefore, we
chose to semi-manually label part of our dataset.
In particular,
we labeled browsing sessions collected during October 2010. In
the following, we refer to obtain dataset as Seval. It is worth not-
ing that Seval differs from the Sstudy dataset that we used for the
search poisoning study in Section 2.2 (Sstudy was collected one
month earlier). This is to make sure that SURF does not “overﬁt”
Sstudy because Sstudy inspired the choice of SURF’s statistical
features, and to avoid over-estimating SURF’s accuracy. For the
same reasons, none of the statistical features measured by SURF
were used to guide our labeling. We simply semi-manually labeled
the data using a separate set of heuristics based on the relation be-
tween the search terms and a (visual) analysis of the terminal page
content rendered by the browser.
In practice, we labeled search poisoning cases related to three
types of malicious activities: search result for popular search key-
words that lead to irrelevant terminal pages serving (1) drive-by
download malware, (2) fake AV software, or (3) hosting rogue phar-
macy sites. We labeled all these cases as poisoned (or positive). At
the same time, we labeled a search result as non-poisoned (or neg-
ative) only when all URLs appeared in redirection chain have a fair
reputations (e.g., according to [4]) and none of them are ﬂagged
as malicious by website scanning or blacklisting services (e.g, us-
ing [3]). Overall, Seval consists of 1,184 negative samples and
1,160 positive ones, with 585 fake AV, 414 drive-by download, and
161 rogue pharmacy cases.
To evaluate SURF and conﬁrm it follows the design goals out-
lined at the beginning of Section 3, we conducted three different
experiments. The ﬁrst experiment aims to estimate SURF’s accu-
racy, while the second attempts to show that SURF is able to detect
generic search poisoning cases, and is not limited to one speciﬁc
type of malicious content. The third experiment aims to show what
features are the most important for classiﬁcation, and how SURF
may respond to evasion attempts on these features. Throughout all
our experiments, we used Weka’s J48 decision tree classiﬁer [13],
which is an implementation of the well known C4.5 algorithm [5].
This choice is motivated by the fact that decision trees are efﬁcient
0.010.11False Positive Rate (log scale)0.50.60.70.80.91True Positive RateAll_10foldsNa+D+F | P+NbNa+D+P | F+NbNa+F+P | D+Nb(0.4%, 93.58%)472(both during the training and testing phases) compared to other sta-
tistical classiﬁers, and the resulting trained classiﬁer can be easily
interpreted.
Overall accuracy: To estimate SURF’s detection rate and false pos-
itives, we performed a 10-fold cross validated of SURF’s classi-
ﬁer on the Seval dataset, achieving an average true positive rate of
99.1% at a 0.9% false positive rate. The ROC curve in Figure 3
shows a very slow decrease of the detection rate as the false pos-
itive rate is pushed down from 0.9% to 0.28%. Therefore SURF
can satisfy a relatively wide range of usage scenarios with different
levels of tolerance to false positives, while still maintaining a rea-
sonably high true positive rate. In our SURF prototype we set the
detection threshold to limit the false positive rate to 0.4% (marked
point in Figure 3). We also analyzed the decision tree produced
by the trained J48 classiﬁer and found that misclassiﬁcations were
mainly caused by those rare cases in which poisoned search results
achieved a top rank under a very competitive keyword, or cases in
which a legitimate landing page redirects visitors to a very “dis-
tant” terminal page (see Section 3.2) and detours sampled visitors
through third-party trafﬁc analysis services.
Generality test: To conﬁrm that SURF is able to detect generic
search poisoning cases, regardless of the speciﬁc malicious content
that they promote, we performed the following experiment. We
prepared three datasets, D, F, and P, containing labeled search
poisoning example from the drive-by malware downloads, fake AV,
and rogue pharmacy cases from Seval, respectively. In addition,
we prepared two separate datasets, Na and Nb, containing ran-
domly selected negative examples (i.e., legitimate search redirec-
tion cases). We then performed a 3-fold cross validation by using
D ∪ F ∪ Na for training, and testing on P ∪ Nb for the ﬁrst fold,
training on D ∪P ∪Na and testing on F ∪Nb for the second fold,
and training on F ∪ P ∪ Na and testing on D ∪ Nb for the third
one. Figure 3 shows the three separate ROC curves (one per fold).
Averaging the results of the 3-fold validation, we obtained a detec-
tion rate of 98% at a false positive rate of 1.8%. It is worth noting
that while this result does not appear to be as good as the result
obtained for the overall 10-fold cross validation experiment, our 3-
fold evaluation presents SURF with much harder cases in which no
examples of search poisoning instances of the same category used
for testing are present in the training set. However, the high detec-
tion rate and relatively limited false positives demonstrate that the
features used by SURF can indeed capture generic search poison-
ing properties, independent of the speciﬁc type of malicious content
delivered by the terminal page.
Feature Robustness: This experiment attempts to quantify SURF’s
resistance to evasion effects on the statistical features. To perform
the experiments, we ran SURF’s classiﬁer on 100 randomly chosen
positive samples. We artiﬁcially modiﬁed the values of the features
describing these 100 samples to simulate different evasion scenar-
ios, and evaluated how the classiﬁer’s accuracy changed as a conse-
quence. We ﬁrst artiﬁcially set the IP-to-name ratio to zero,
which is the most common value of the feature in negative samples.
The IP-to-name ratio feature is the only one among SURF’s
features that is not difﬁcult to evade. After altering this feature,
only 1 out of 100 samples was misclassiﬁed, which suggests that
the attacker cannot gain much by attempting to evade it. Despite
the fact that other features are hard to evade (see Section 3.2.1),
we nonetheless wanted to investigate the effect of altering the most
discriminant features, i.e., the features that appeared close to the
root of the decision tree. The redirection consistency
and landing to terminal distance turned out to be the
top two most discriminant features. Replacing their values with
values drawn from negative samples caused 80 out of 100 samples
to be misclassiﬁed. However, it is worth noting that evading these
two features would require the attacker to change her malicious net-
work infrastructure, thus incurring a signiﬁcant cost, as discussed
in Section 3.2.1.
4. DISCUSSION
At a ﬁrst glance, our evaluation may seem unconvincing because
we used a dataset labeled by ourselves. However, to the best of our
knowledge, no public labeled dataset exists that contains brows-
ing session data related to a variety of search poisoning instances.
As discussed in Section 3.4, our semi-manual labeling process is
based on a set of heuristics that do not overlap with any of the de-
tection features measured by SURF. This allowed us to perform
an unbiased evaluation. While we acknowledge that our semi-
manual labeling can only help us collect partial “ground truth”, it
is extremely difﬁcult, if not impossible, to obtain complete ground
truth without a deterministic search poisoning detection system.
In absence of such perfect search poisoning detector, our labeling
method represents our best effort to produce a representative (even
though not complete) ground truth that includes a variety of search
poisoning instances leading to different types of malicious content.
During our feature selection process, we discarded a few can-
didate features that may help the classiﬁcation accuracy but are
not robust. For example, we chose not to include features based
on measuring the relevance of the content of terminal pages to the
search keywords because the content of the terminal pages is un-
der complete control of the attacker, making these types of fea-
tures easy to evade. Also, we did not include features related to the
structure of the URLs involved in malicious search user redirec-
tions. These features usually require historical knowledge of the
“normal” structure of the URLs for each particular website. While
these type of features may be included in a search engine-side de-
ployment of SURF, client-side deployments would not be able to
collect and leverage this kind of information, and therefore we de-
cided not to add them to our prototype implementation. Further-
more, an attacker has a non-negligible ﬂexibility when choosing
the structure of the redirection URLs, and therefore it is not clear
how robust these features would be. We also considered some fea-
tures based on domain or IP reputation scores. Though capable of
reducing the false positive rate, these features were excluded from
our selection because of their heavy dependence on external se-
curity services, and because we wanted to evaluate the detection
accuracy of SURF based solely on the strengths of our own fea-
tures. In practice, SURF implementations may opt to incorporate
reputation-based features to improve classiﬁcation accuracy.
When deployed at the search engine side (e.g., using a “browser
farm”), SURF can be used to analyze suspicious search results and
accurately detect poisoning cases. This can provide search engines
with valuable information that goes beyond speciﬁc poisoning in-
stances. For instance, the landing pages involved in search poison-
ing are often organized in a “botnet mode”, so that the keywords
to be poisoned can be periodically fetched from a command-and-
control server. Therefore, detecting search poisoning cases can re-
veal information about compromised websites and botnet organi-
zations. In addition, newly detected malicious terminal webpages
may serve as labeled samples for malicious webpage detectors that
require periodic re-training.
At the same time, SURF can be deployed at each single client to
detect (and block) poisoned search results. A possible deployment
scenario could include large numbers of client-side SURF instal-
lations that collaboratively detect search poisoning case and share
information about the underlying malicious network infrastructures
473as shown by the poisoned volume increase in Figure 4 (the poi-
soned volume is relative to the total number of detected poisoned
results during the 7-day period). We believe this fast paced op-
eration proposes signiﬁcant challenges for blacklisting and other
traditional security solutions, making them inadequate to solve the
search poisoning problem. In fact, well-known malicious webpage
scanners (e.g., [3]) have failed to detect 78.9% of malicious termi-
nal pages involved in the search poisoning cases detected by SURF
(we scanned all the terminal pages using [3] on the same day when
SURF detected the related search poisoning instance).
Our measurements show that the visiting trafﬁc reaching a par-
ticular malicious terminal page is always contributed by multiple
landing pages that appear in poisoned search results related to dif-
ferent search keywords. In particular, we found that during a 7-day
period, for each given malicious terminal page, their visitors were
supplied in average by 2.9 poisoned search keywords and 2.2 differ-
ent rogue landing sites per keyword. We speculate this is a reﬂec-
tion of a growth in the “search poisoning as a service” phenomenon
discussed in Section 2.2.
5.2 Macro Measurements
After examining the development of poisoned search results in
the ﬁrst 7 days period, we now zoom out our measurement win-
dow to consider the entire 7-month data collection period. Through
these long-term measurements we aim to discover search poison-
ing’s evolving trends and characteristics that are observable only
throughout a long period of time. To highlight speciﬁc patterns and
long-term trends, we divide the 7-month observation period in 31
epochs, where each epoch is equal to one week. Then, for each
week we compute a number of statistics (discussed below), and
plot how these statistics vary with respect to time.
During most epochs, we found that more than 50% of the search
keywords that became popular on that epoch got poisoned. For
this particular measurement, a search keyword is considered to be
poisoned if at least one out of the top 100 search results for that
keyword is related to a search poisoning instance. Figure 5 (left)
plots the percentage of poisoned keywords for each week, broken
down into different degrees of success in terms of search ranking.
We can see that during some of the epochs, almost 60% of the
trendy search keywords resulted in rogue landing pages that ranked
within the top 50 search results. Furthermore, in some cases ad-
versaries managed to promote their rogue landing pages up to the
top 10 search results. These particularly successful attacks were
related to about 15% of all poisoned keywords on average. These
ﬁndings are alarming because they suggest that a large number of
search users can easily be affected by search poisoning.
Figure 5 (right) shows two curves. One represents the number
of rogue landing sites (counted as the number of distinct related
domain names) that were involved in search poisoning cases, and