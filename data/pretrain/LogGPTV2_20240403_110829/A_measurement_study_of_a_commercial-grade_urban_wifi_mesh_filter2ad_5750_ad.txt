S
l
i
a
n
g
S
d
e
v
e
c
e
R
i
-20
-30
-40
-50
-60
-70
-80
-90
-100
-20
-30
-40
-50
-60
-70
-80
-90
-100
Mean
+7 Stdev
-7 Stdev
Measurements
Pathloss Exp = 2.3082
Shadowing Std. = 14
 0
 50
 100
 150
 200
 250
 300
 350
Distance (m)
Mean
+4 Stdev
-4 Stdev
Measurements
Pathloss Exp = 2.9082
Shadowing Std. = 8.1
 0
 50
 100
 150
 200
 250
 300
 350
Distance (m)
Figure 11: Propagation pathloss for two diﬀerent
MAPs in diverse settings. The MAP in (a) is located
in campus and has a pathloss of 2.3, while (b) is
located in downtown and its measured pathloss is
2.9.
We follow the measurement methodology reported in prior
research work on propagation modeling [9] and collect signal
strength information at 25 diﬀerent locations for each MAP.
We compute the pathloss exponent for diﬀerent MAPs in
MadMesh network. Our results indicate signiﬁcant varia-
tions in pathloss exponent between diﬀerent MAPs. Figure
11(a) and (b) show the signal strength measurements as a
function of link distance for two MAPs, located in downtown
and campus regions of the city. As shown in the ﬁgure, the
pathloss exponent for the downtown MAP is 2.9 while the
pathloss exponent for the campus MAP is 2.3. This is in
contrast to the pathloss of 3.3 shown in [9], who also report
that their pathloss exponent is stable across diﬀerent access
points in their network. We attribute this signiﬁcant vari-
ation in pathloss exponent to diverse set of obstacles and
external interference, which also vary signiﬁcantly from one
location in the city to another. Our results show that gener-
alizing a pathloss exponent for a city wide mesh deployment
may be inaccurate, and targeted experiments must be per-
formed to determine the pathloss in diﬀerent parts of the
city. This observation further reinforces the ineﬃcacy of
pathloss models in determining coverage holes. Next we de-
scribe a simple monitoring tool that can detect such coverage
holes eﬃciently.
Characterizing coverage holes
In order to assess the prevalence of coverage holes in the
mesh deployment under study, we perform extensive client
measurements. We report on some sample results in in a 6×6
block area of the mesh deployment. In our experiments, a
few clients (IBM laptops with Cisco Aironet wireless card)
were equipped with a module which continuously records
the information about the location, current state of associ-
ation and received signal strength. Periodically, the clients
upload this information to a central server. Over a period of
time (seven days, in our case) information aggregated from
these clients is used to detect coverage holes in the network.
Figure 12 shows the average client connectivity in our tar-
get area. Although, the propagation model based radio map
generated by the mesh controller shows this entire area to be
‘covered’, we found additional coverage holes were observed
by the clients.
Vehicular client connectivity
Wireless access from mobile devices has been an active area
of research recently [18]. In that context, we wanted to eval-
uate MadMesh in terms of providing client connectivity from
moving vehicles. Towards this end, we repeat our measure-
ments from moving vehicles that makes round of the same
6x6 block area that we targeted for our walking experiments.
The average speed of the vehicle was 25 miles/hr. The cov-
erage holes detected at such vehicular speeds is shown in
Figure 12. As shown in the ﬁgure, the holes detected by the
clients at vehicular speeds are much larger then the holes
detected during earlier client measurements. In fact, we ob-
serve that about 65 % of the total path falls under the cate-
gory of coverage holes at vehicular speeds. It is important to
note that the observations would have been very diﬃcult to
make without the help of actual measurements on the client
side. We believe that such measurements can provide signif-
icant corrective feedback to the operator regarding coverage
holes, hich are much more accurate then the propagation
models used in current mesh controllers.
5.2 Client Performance
In order to assess the performance of end users in the
mesh deployment, we undertake targeted active measure-
ments, where we randomly sample 100 locations in the mesh
coverage area and perform bandwidth tests to determine
the achievable throughput at that location. At each sam-
pled location, we associate to the MAP with strongest signal
strength and run TCP iperf[14] from the client to the mesh
controller. We use TCP as it is the dominant traﬃc type in
mesh networks, and secondly it is less intrusive then a UDP
test, which can completely saturate the link and negatively
impact other client in the mesh. We perform three itera-
tions of 100 seconds each. Figure 15 shows the distribution
of TCP throughput at the sampled locations. As shown in
the ﬁgure, the measured throughput closely matches a uni-
form distribution, with about 10% of the clients achieving
less then 0.2 Mbps and 80% of the client achieve throughput
less than 1 Mbps. This upper limit on client throughput is
expected in view of the Service Level Agreement(SLA) of
MadMesh, which advertises a 1 Mbps service to the clients.
Further, to understand the impact of hop count, channel
congestion and RSSI on client’s throughput, we perform tar-
geted experiments on one stable 6 hop tree (shown in Figure
13), comprising of eight MAPs and a RAP. MAPs one, two
Figure 13: Tree for our targeted experiments to un-
derstand the impact of RSSI, hop count, channel
congestion on client performance.
and three shown in Figure 13 are located on a busy main
road of the city that has substantial interference from other
wireless hotspots in the area. On the other hand, MAPs four
to eight are located inside the neighborhood areas, experi-
encing relatively less interference. We choose a minimally
loaded tree, so that our experiments are not impacted by
the presence of other MadMesh users on the same tree. Our
results from the active measurements on chosen tree is sum-
marized in Table 1. The main observations are as follows:
• In all experiments, maximum client throughput is lim-
ited to 1 Mbps, which indicates that bandwidth shap-
ing may be performed by the mesh operator for meet-
ing the SLA.
• Client throughput remains stable with RSSI to a point,
beyond which it drops quickly. Since per client through-
put is limited by the operator, higher RSSI, which
can sustain higher data rate, does not improve client
throughput.
• External interference from other wireless sources has
a signiﬁcant impact on the client throughput beyond
the ﬁrst hop.
• Throughput unfairness is observed when clients at dif-
ferent hops of the same tree are activated simultane-
ously, with clients at higher hops achieving low through-
put share.
We now describe each observation in detail.
Impact of hop count
As shown in Table 1, there is no strong correlation between
the throughput and the hop count. Diﬀerent throughput is
seen at clients associated to MAPs at the same hop count in
the tree. As shown in table 1, throughput of 0.3Mbps and
0.92Mbps is observed on two diﬀerent MAPs at a hop count
of three. Similarly, a throughput of 0.91Mbps and 0.6 Mbps
is observed on MAPs at a hop count of ﬁve. This variation in
throughput at diﬀerent MAPs with same hop count can be
attributed to channel congestion on their access sides which
we discuss next.
Impact of Channel Congestion
Presence of traﬃc due to other 802.11b/g sources can have
a great impact on the throughput observed at each hop. We
Figure 12: Actual network coverage as observed by clients, in areas estimated to be perfectly covered by
infrastructure-side management tools, that rely on propagation models.
estimate the channel congestion by monitoring the traﬃc on
each hop while doing the TCP iperf experiments. Table 1
shows that the throughput achieved on MAPs at the same
hops is well correlated with the channel congestion at their
respective access sides. We further observe that channel
congestion does not have any impact on client connected to
the ﬁrst hop. This is because of the lesser number of links
these clients have to contend for on the backbone. However
if the channel congestion is relatively high, this observation
might not hold true.
Impact of Shared Congestion
In another set of experiments we study the impact on through-
put of clients at a lower hop count in the presence of other
clients in the tree. We ﬁrst associate only one client to a
MAP at a higher hop count in the tree and calculate its
TCP throughput using iperf as shown in ﬁgure 14(a).
In
order to see the eﬀect on throughput due to other clients in
the tree, we associate another client at a lower hop count on
the same tree and start running TCP iperf, shown in ﬁgure
14(b). As shown in the ﬁgure, on running the second client
the throughput of the ﬁrst client suddenly drops from 0.98
Mbps to 0.43 Mbps. This is due to shared channel congestion
as discussed in [19], when multiple clients try to contend for
the same backbone path. This can have a great impact on
the clients connected to MAPs at a higher hop count, which
can suﬀer from increased throughput degradation with the
increase in clients at lower hops (closer to RAP).
Summary: In Madcity mesh network, because of band-
width shaping policies enforced, hop count did not really
seem to be the bottleneck for performance. However, this
is mostly true in absence of shared congestion; that is in
presence of multiple ﬂows sharing the same backbone path,
the throughput of higher hop-count routes would be lower.
Hence although the penalty of using higher hop counts is
diminished due to bandwidth shaping, choosing a lower hop
path is still better due to the possibility of shared congestion
in the path.
6. ON MESH USAGE CHARACTERIZATION
We now answer one of the most basic questions about the
mesh network – how is the network being used? Speciﬁcally,
we want to know the following:
Figure 14: Eﬀect of shared congestion on a client as-
sociated to a MAP at hop 4. In isolation it achieves
close to 1 Mbps, but when another client at hop 3
is activated, its throughput drops to 0.43 Mbps.
• How many clients are using the network? How does
their number vary across time?
• How are the clients distributed across the coverage
area?
• What is average number of clients connected to each
MAP? Are there any popular MAPs?
• How does client distribution vary across diﬀerent hops?
Client distribution across time
Figure 16 shows the average number of clients per hour con-
nected to the network over this 2 week period. The error
bars show the 95% conﬁdence limits. We observe that the
average number of clients varied considerably across the du-
ration of the day, with most number of clients being con-
nected at around 10 PM and the least number of clients at
5 AM. We note that the observed usage pattern is unique
to this mesh network as it is mostly accessed by the users
from their residences. This is apparent from the fact that
MAP Hop Count
Index
1
2
3
4
5
6
7
8
1
2
3
3
4
5
6
5
Avg. RSSI.
Avg. Chnl. Util. TCP Thrpt. TCP loss rate (Mbps) TCP RTT (msec)
(MAP to client)
34
33
35
40
33
32
37
33
0.28
0.27
0.20
0.09
0.10
0.05
0.09
0.11
.96
0.4
0.3
.92
0.7
.91
0.5
0.6
0.021
0.092
0.087
0.007
0.021
0.007
0.030
0.015
111±98.4
158±115
258.2±168.7
192.5±91.9
252.2 ± 126.4
215 ± 73
208.2 ± 117.5
278.7± 127.4
Table 1: Experimental results for the tree under study. Conﬁdence intervals for RSSI and throughput
is small and omitted for brevity
s
n
o
i
t
a
c
o
l
l
d
e
p
m
a
s
f
o
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
0.0 0.2 0.4 0.6 0.8 1.0 1.2
TCP Throughput (Mbps)
Figure 15: CDF of throughput measured through
active measurements at 100 random locations in the
coverage area of MadMesh. Throughput is almost
uniformly distributed, with maximum throughput
being clipped around 1.2 Mbps
number of clients starts increasing from around 6 AM, re-
mains steady throughout the afternoon and then again in-
creases from around 6 PM as the users start returning to
their homes. It reaches its peak around 10 PM when most
of users are their homes and starts tailing oﬀ as the night
progresses. We observe that during the busiest hour around
627 clients were connected to the network with around 498
being connected to the network on average.
Client distribution across the MAPs
The average number of clients connected to a MAP gives a