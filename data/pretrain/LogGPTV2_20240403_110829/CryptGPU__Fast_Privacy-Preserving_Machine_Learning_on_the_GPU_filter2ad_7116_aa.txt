title:CryptGPU: Fast Privacy-Preserving Machine Learning on the GPU
author:Sijun Tan and
Brian Knott and
Yuan Tian and
David J. Wu
8
9
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
CRYPTGPU: Fast Privacy-Preserving Machine Learning on the GPU
Sijun Tan∗, Brian Knott†, Yuan Tian∗, and David J. Wu∗
{st8eu, yuant, dwu4}@virginia.edu
∗University of Virginia
†Facebook AI Research
PI:EMAIL
Abstract—We introduce CRYPTGPU, a system for privacy-
preserving machine learning that implements all operations on
the GPU (graphics processing unit). Just as GPUs played a pivotal
role in the success of modern deep learning, they are also essential
for realizing scalable privacy-preserving deep learning. In this
work, we start by introducing a new interface to losslessly embed
cryptographic operations over secret-shared values (in a discrete
domain) into ﬂoating-point operations that can be processed
by highly-optimized CUDA kernels for linear algebra. We then
identify a sequence of “GPU-friendly” cryptographic protocols
to enable privacy-preserving evaluation of both linear and non-
linear operations on the GPU. Our microbenchmarks indicate
that our private GPU-based convolution protocol is over 150×
faster than the analogous CPU-based protocol; for non-linear
operations like the ReLU activation function, our GPU-based
protocol is around 10× faster than its CPU analog.
With CRYPTGPU, we support private inference and training
on convolutional neural networks with over 60 million parameters
as well as handle large datasets like ImageNet. Compared to
the previous state-of-the-art, our protocols achieve a 2× to 8×
improvement in private inference for large networks and datasets.
For private training, we achieve a 6× to 36× improvement
over prior state-of-the-art. Our work not only showcases the
viability of performing secure multiparty computation (MPC)
entirely on the GPU to newly enable fast privacy-preserving
machine learning, but also highlights the importance of designing
new MPC primitives that can take full advantage of the GPU’s
computing capabilities.
I. INTRODUCTION
Deep learning has enabled numerous applications in the form
of digital voice assistants, video monitoring and surveillance
systems, and even systems for disease diagnosis and treatment
planning. But these new and exciting applications raise challeng-
ing questions regarding user privacy. After all, modern machine
learning algorithms are largely data-driven and training image
recognition, speech recognition, or disease predictor systems
all rely on aggregating and analyzing sensitive user data. Even
model inference raises privacy concerns as increasingly often,
voice or video recordings from a mobile or IoT device are
outsourced to the cloud for analysis.
To address some of the privacy challenges associated with
the widespread deployment of deep learning technologies, a
number of works [1, 2, 3, 4, 5, 6] in the last few years have in-
troduced cryptographic frameworks based on secure multiparty
computation (MPC) [7, 8] to enable privacy-preserving deep
learning (see Section V for a more comprehensive survey). At
a high level, MPC protocols allow a set of mutually-distrusting
parties to compute an arbitrary function over secret inputs such
that at the end of the computation, the parties only learn the
output of their computation, and nothing more. In particular, all
information about other parties’ inputs are completely hidden
(up to what could be inferred based on the output1).
While there have been considerable advances in the con-
crete efﬁciency of MPC protocols, current approaches remain
computationally expensive and do not scale well to the types
of neural networks typically used in modern machine learning
systems. Until recently, cryptographic protocols for inference
over deep neural networks have been limited to small datasets
such as MNIST [11] or CIFAR [12]. In contrast, the current
baseline for object recognition is ImageNet [13], a dataset
that is over 1000× larger than CIFAR/MNIST and contains
1000 different classes (compared to just 10 classes for MNIST
and CIFAR). Similarly, state-of-the-art deep learning models
for computer vision such as ResNet-152 [14] contain over
150 layers and over 60 million parameters. In contrast, most
protocols for privacy-preserving machine learning have been
constrained to relatively shallow networks with just tens of
layers and a few hundred thousand parameters.
Recently, two systems FALCON [6] and CRYPTFLOW [5]
have made considerable headway towards scalable privacy-
preserving machine learning. For the ﬁrst time, they demon-
strate the ability to perform privacy-preserving machine learn-
ing at the scale of ImageNet (or Tiny ImageNet [15] in the case
of FALCON) and with much larger models (e.g., AlexNet [16],
VGG-16 [17], and the ResNet family of models [14]). In spite
of these advances, there still remains considerable overhead:
for example, private training of the AlexNet model [16] on
the Tiny ImageNet dataset is estimated to still take over a
year using FALCON. CRYPTFLOW currently only supports
private inference and not private training. Both works argue that
hardware acceleration with graphics processing units (GPUs)
will be essential for scaling up privacy-preserving deep learning,
especially in the case of private training.
The importance of GPU acceleration. GPUs and hardware
acceleration have played a critical role in the evolution of
modern deep learning. Today, convolutional neural networks
(CNNs) have become a staple for modern computer vision.
However, in the immediate years following their introduction
1There are settings where even learning the exact output is problematic and can
reveal compromising information about other parties’ inputs. Techniques like
differential privacy [9, 10] provide a defense against these types of attacks.
We discuss this in greater detail in Section V.
© 2021, Sijun Tan. Under license to IEEE.
DOI 10.1109/SP40001.2021.00098
1021
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
in the seminal work of LeCun et al. [18], CNNs were not
widely used due largely to the computational cost of the
backpropagation training algorithm. Starting the mid-2000s,
several works [19, 20] demonstrated that CNN training could
be greatly accelerated through the use of graphics processing
units (GPUs). This culminated with the breakthrough moment
when Krizhevsky et al. [16] introduced “AlexNet” and won
the ImageNet Large Scale Visual Recognition Challenge in
2012 using a large CNN trained entirely on the GPU. Since
AlexNet, CNNs have become a mainstay of computer vision.
Modern machine learning frameworks (i.e., PyTorch [21]
and TensorFlow [22]) for training and inference over deep
neural networks (including non-convolutional architectures) all
rely heavily on GPUs, and even custom-designed application-
speciﬁc integrated circuits (ASICs) such as Google’s tensor
processing unit [23].
Privacy-preserving machine learning on the GPU. Hard-
ware acceleration has become a core component for evaluating
and training deep learning models. Given that MPC protocols
necessarily incur a non-zero overhead on top of the plaintext
computation, it is essential for cryptographic protocols to be
able to leverage GPU acceleration in order to have any chance
of scaling up to support training and inference over deep models.
After all, if we are bound to CPU-based computations (as nearly
all existing MPC frameworks have), then it is infeasible to
even run the machine learning algorithm on plaintext data.
A. Our Contributions
In this work, we introduce CRYPTGPU, a new cryptographic
MPC framework built on top of PyTorch and CRYPTEN [24]
where all of the cryptographic operations (both linear and non-
linear) are implemented on the GPU. CRYPTGPU can perform
private inference over modern computer vision models such as
ResNet-152 on ImageNet images in just over 25s (2.3× faster
than the previous state-of-the-art CRYPTFLOW [5]). Further
improvements are possible if we consider batch inference,
which also beneﬁts from GPU parallelism. For example, batch
inference over ResNet-152 allows us to reduce the amortized
cost of private inference to 13.2s. For smaller networks like
AlexNet, private inference over ImageNet requires just 1.5s.
For private training (which has a greater potential of
beneﬁting from GPU acceleration), we demonstrate a 440×
speed-up for private training for AlexNet on the Tiny ImageNet
database compared to FALCON. Whereas it would have taken
over a year to privately train FALCON on Tiny ImageNet,
our GPU-accelerated system would be able to do so in just
over a week (see Section IV-B). Beyond these performance
results, our work highlights the potential of leveraging GPUs
to accelerate privacy-preserving deep learning in much the
same way GPUs have dramatically accelerated standard deep
learning. Our work also highlights the importance of developing
new types of cryptographic protocols that are “GPU-friendly”
and can take advantage of the parallelism provided by GPUs.
CRYPTGPU operates in the standard 3-party setting where
we assume that all inputs are secret-shared across three non-
colluding servers who execute the MPC protocol. The inputs
are secret shared using a 2-out-of-3 replicated secret sharing
scheme [25, 26] (see Section III for the full details). Our system
provides security against a single semi-honest corruption. We
describe our threat model formally in Section III-A.
Cryptography on the GPU. While NVIDIA’s CUDA (Com-
pute Uniﬁed Device Architecture) platform [27] supports
general-purpose computations on the GPU, directly translating
code written for the CPU onto the GPU is unlikely to
translate to immediate performance gains. The architectural
differences between the CPU and the GPU introduce several
additional hurdles that must be overcome to have an efﬁcient
implementation:
• Leveraging existing CUDA kernels. The ﬁrst challenge is
that highly optimized CUDA kernels for computing deep
learning primitives (i.e., convolutions, pooling, matrix multi-
plication) are designed to operate on ﬂoating-point inputs,
and there does not currently exist kernels for computing on
integer values. In MPC, we typically compute over discrete
objects (i.e., ring or ﬁeld elements). To leverage optimized
kernels for these basic primitives, we need a way to embed
the integer-valued cryptographic operations into (64-bit)
ﬂoating-point arithmetic that can in turn be operated on
by these kernels. CRYPTGPU enables this by introducing
a new abstraction called a CUDALongTensor that models
tensors (i.e., multi-dimensional arrays) of integer values,
but seamlessly translates the integer-valued computations
into a corresponding set of ﬂoating-point computations. We
describe our construction in Section II-B. We note that the
DELPHI system encountered a similar challenge, but as we
discuss in Remark II.3, their solution does not extend well
to our setting. A critical difference is that DELPHI considers
private inference where the model is public while in this
work, we assume that the model is also hidden.
• “GPU-friendly” cryptography. The GPU architecture is
optimized for performing a large number of simple com-
putations on blocks of values. This means that operations
like component-wise addition and multiplication of vec-
tors/matrices are fast while operations that involve large
numbers of conditional statements are slower. While there
is support for integer addition and multiplication, operations
like computing a modular reduction by a prime incurs consid-
erably more overhead [27]; for instance, we observed a 40×