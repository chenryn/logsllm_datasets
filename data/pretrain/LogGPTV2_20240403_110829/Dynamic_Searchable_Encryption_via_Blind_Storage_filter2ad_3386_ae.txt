γ
γ
i=0
Figure 6 plots various possible combinations of α and κ for
various choices of perr and γ. A few suggested choices of
(γ, α, κ) which achieve perr ≤ 2−40 are (4, 4, 45), (2, 8, 60)
and (4, 8, 25). Thus, for instance, one could use the parameter
setting of (γ, α, κ) = (4, 4, 45) which means that the amortized
storage requirement for each ﬁle and the communication
requirement for reading large ﬁles is roughly 4 times the size
of the ﬁle; however, for small ﬁles – any ﬁle with at most
11 blocks, including empty or non-existent ﬁles – 45 blocks
would be downloaded, decrypted, reencrypted and uploaded
back.
While a very large value of κ would require a large amount
of communication and extra computation on part of the client
(for updates), we recommend moderately large values for κ.
This is because, ﬁrstly, increasing κ does not have any effect on
the storage needed (because only as many blocks are occupied
in D as the actual data consists of), and secondly it actually
provides a higher security guarantee and may slightly increase
the overall efﬁciency too! Apart from lowering perr, another
reason for a higher security guarantee (not captured in FSTORE,
for simplicity) is that the server does not
learn the exact
number of blocks in every ﬁle that is accessed; for “small”
ﬁles, it learns only that the ﬁle is small (at most 11 blocks,
in the above example). The higher the value of κ, the less
the information that the server learns. The potential (slightly)
higher efﬁciency is due to the fact that when a “small” ﬁle
is retrieved, a single round of interaction sufﬁces, and again,
the higher the value of κ, the more the ﬁles that fall into the
“small” category. This does not increase the computational cost
during read operations.
We point out that while α, κ (and nD) are parameters built
into the system speciﬁcation, it is not necessary to have a hard
bound d0 on the number of blocks of D that can be ﬁlled.
In other words, γ and perr exhibit graceful degradation: as the
array D ﬁlls up and γ decreases, perr increases.
Another parameter that affects the choice of these pa-
rameters is the size of the blocks in D. As the block size
decreases, on the one hand, the number of blocks in ﬁles
grows and the effect of the communication overhead due to the
minimum number of blocks used for small ﬁles (the parameter
10The error probability when adding a ﬁle of n ≥ κ
α blocks is up-
perbounded by the probability that when (cid:100)αn(cid:101) blocks are picked (with
replacements) from a set of nD blocks of which at most d0 would be occupied,
i  ∂0 and w ∈
w is the set of newly added documents (i.e., not the original documents) that have the keyword w. Note that only the
• Removal. On receiving the command FSSE.remove, FSSE accepts a document ID id and identiﬁes ∂ (if any) such that id∂ = id and
w , and not their labels w, are shared with the server.
∂ (cid:54)∈ Removed. If such an index ∂ exists, it adds ∂ to Removed.
◦ Removal Leakage. FSSE reveals to the server the updated set Removed.
• Search. On receiving the command FSSE.search, it accepts a keyword w from the client and returns {(id∂, contents∂)|w ∈ W∂ and ∂ (cid:54)∈
Removed} to the client.
◦ Search Leakage. FSSE reveals to the server the last instance the same keyword was searched on (or that it is being searched for the
w |w ∈ W}, where M new
W∂(cid:48)}. i.e., M new
sets M new
ﬁrst time) and also Mw = {∂|w ∈ W∂}.
Fig. 7: The FSSE functionality: all the information leaked to the server in our SSE scheme is speciﬁed here.
for each keyword in it, up to two other documents that share
the same keyword. This is the case even if that keyword is
never searched on. In contrast, by our security requirement, if
an original document is removed, only the number of keywords
in it that are searched can be revealed. Further, it is not
revealed that a removed document shared a keyword with
another document, unless such a keyword is explicitly searched
for.
We remark that our functionality reveals “removed” ver-
sions of the documents in search results, but this information
was revealed (implicitly) by the leakage functions in [18]
as well, as the identiﬁers for each keyword in a removed
document is revealed and this information links the removed
documents to future searches on the same keyword (when the
same identiﬁer for the keyword is revealed).
Finally, our scheme allows the client to refer to a doc-
ument using an arbitrary document ID rather than a serial
number (which is useful when removing documents from the
collection). We also allow the client to reuse document IDs.
The server does learn when a document ID is reused (though
not the actual identiﬁer of the document ID itself); further, in
the pattern information revealed to the server, the different
versions that use the same document ID are differentiated.
Other dynamic SSE schemes often avoid this aspect simply
by not using document IDs. This sufﬁces if the only time
a document
is removed is immediately after retrieving it
from a search (or if the client is willing to maintain a map
from document IDs to serial numbers); however, realistically,
in many applications of a dynamic SSE scheme, it will be
important to efﬁciently remove documents referenced by their
document IDs.
B. Searchable Encryption from Blind Storage
In this section, we describe an efﬁcient dynamic searchable
encryption scheme, BSTORE-SSE, built on top of a blind-
storage scheme. The full details are given in Figure 8. Here
we sketch the main ideas.
First, note that we can implement a static searchable
encryption scheme simply by storing the index ﬁle for each
keyword (which lists all the documents containing that key-
word) in a blind storage system. The guarantees of blind stor-
age readily translate to the security guarantees of searchable
encryption: the server learns only the pattern of index ﬁles
(i.e., keywords) accessed by the client.
In a dynamic searchable encryption scheme, we need to
support adding and removing documents, which in turn results
in changing the index ﬁles. We seek to do this without reveal-
ing much information about the keywords in a document being
added or removed, if those keywords have not been searched
on before. To support dynamic searchable encryption (with
much better security guarantees than previous constructions),
we rely on the following observation. The access pattern that
server would be allowed to learn tells the server if two newly
added documents share a keyword or not, as soon as they
are added and before such a keyword is searched for (but
not whether they share keywords with the original set of
documents that were added when initializing the system). This
means we can treat the set of newly added documents virtu-
ally as a different system, with signiﬁcantly weaker security
requirements.
Thus, for each keyword, we use two index ﬁles: one listing
the original documents that include that keyword, and another
listing the newly added documents that include it. The ﬁrst
index ﬁle is stored with the server using a blind storage
scheme, where as the second can be stored in a “clear storage”
system (see below). Searching for keywords now involves