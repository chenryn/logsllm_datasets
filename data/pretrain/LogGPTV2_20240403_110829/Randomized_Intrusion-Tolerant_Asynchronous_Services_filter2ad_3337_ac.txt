the appropriate ritas * input() function of its parent
control block. The message is processed by that layer and
the mbuf keeps being passed in the same fashion.
When a protocol instance is destroyed, all of its child
protocol instances become obsolete. All protocol instances
are responsible for destroying its child instances by calling
the appropriate destruction functions. This way, a tree (or
subtree) of control blocks is automatically destroyed when
its root node is eliminated.
3.4. Out-of-Context Messages
The asynchronous nature of the protocol stack leads to
situations in which a process is receiving correct messages
but they are destined to a protocol instance for which a con-
trol block has not yet been created. These messages – called
out-of-context (OOC) messages – have no context to handle
them, though they will, eventually.
Since OOC messages cannot be discarded,
they are
stored in a hash table. When a RITAS Channel is created,
it checks the hash table for messages. If any relevant mes-
sages exist, they are promptly delivered to the upper proto-
col instance.
It is also possible for a protocol instance to be destroyed
before consuming all of its OOC messages. To avoid a situ-
ation where OOC messages are kept indeﬁnitely in the hash
table, upon the destruction of a protocol, the hash table is
checked and all the relevant messages are deleted.
4. Performance Evaluation
This section describes the performance experiments with
RITAS in a local-area network (LAN) setting with four pro-
cesses, each one running on a distinct host. Two different
performance analyses are made. First, we present a compar-
ative evaluation in order to understand how protocols relate
and build on one another performance-wise. Second, we
conduct an in-depth analysis of how atomic broadcast per-
forms under various conditions. This protocol is the most
interesting candidate for a detailed study because it uses all
other protocols as primitives, either directly or indirectly,
and it can be used for many practical applications.
The experiments were carried out on a testbed consisting
of four Dell Pentium III PCs, each with 500 Mhz of clock
speed and 128 MB of RAM, running Linux Kernel 2.6.5.
The PCs were connected by an 100 Mbps HP ProCurve
2424M network switch. Bandwidth tests taken at differ-
ent occasions with the network performance tool lperf have
shown a consistent throughput of 9.1 MB/s. The used IPSec
implementation was the one available in the Linux kernel
and the security associations that were established between
every pair of processes employed the AH protocol (with
SHA-1) in transport mode [13].
4.1. Stack Analysis
In order to get a better understanding of the relative over-
heads of each layer of the stack, we have run a set of exper-
iments to determine the latencies of the protocols. These
measurements were carried out in the following manner: a
signaling machine, that does not participate in the protocols,
is selected to control the benchmark execution. It starts by
sending a 1-byte UDP message to the n processes to in-
dicate which speciﬁc protocol instance they should create.
Then, it transmits N messages, each one separated by a two
second interval (in our case N was set to 100). Whenever
one of these messages arrives, a process runs the protocol,
either a broadcast or a consensus. In case of a broadcast,
the process with the lowest identiﬁer acts as the sender,
while the others act as receivers. In case of a consensus, all
processes propose identical initial values. The broadcasted
messages and the consensus proposals all carry a 10-byte
payload (except for binary consensus, where the payload
is 1 byte). The latency of each instance was obtained at
a speciﬁc process. This process records the instant when
the signal message arrives and the time when it either de-
livers a message (for broadcast protocols) or a decision (for
consensus protocols). The measured latency is the interval
between these two instants. The average latency is obtained
by taking the mean value of the sample of measured values.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Echo Broadcast
Reliable Broadcast
Binary Consensus
Multi-valued Consensus
Vector Consensus
Atomic Broadcast
w/ IPSec w/o IPSec
(µs)
1724
2134
8922
16359
20673
23744
(µs)
1497
1641
6816
11186
15382
18604
IPSec
overhead
15%
30%
30%
46%
34%
27%
Table 1. Average latency for isolated execu-
tions of each protocol (with IPSec and IP).
The results, shown in Table 1, demonstrate the interde-
pendencies among protocols and how much time is spent on
each protocol. For example, in a single atomic broadcast in-
stance roughly 2
3 of the time is taken running a multi-valued
consensus (see also Figure 2). For a multi-valued consensus
about 1
2 of the time is used by the binary consensus. And
for vector consensus about 3
4 of the time is utilized by the
multi-valued consensus. The experiments also show that
consensus protocols were always able to reach a decision in
one round because the initial proposals were identical.
The cost of using IPSec is also represented in Table
1. The overhead could in part be attributed to the crypto-
graphic calculations, but most of it is due to the increase
on the size of the messages. For example, the total size
of any Reliable Broadcast message – including the Ether-
net, IP, and TCP headers – carrying a 10-byte payload is 80
bytes. The IPSec AH header adds another 24 bytes, which
accounts for an extra 30%.
4.2. Atomic Broadcast Analysis
This Section evaluates the atomic broadcast protocol in
more detail. The experiments were carried out by having the
n processes send a burst of k messages and measuring the
interval between the beginning of the burst and the delivery
of the last message. The benchmark was performed in the
following way: processes wait for a 1-byte UDP message
from the signaling machine, and then each one atomically
broadcasts a burst of k
n messages. Messages have a ﬁxed
size of m bytes. For every tested workload, the obtained
measurement reﬂects the average value of 10 executions.
Two metrics are used to assess the performance of the
atomic broadcast: burst latency (Lburst) and maximum
throughput (Tmax). The burst latency is always measured
at a speciﬁc process and is the interval between the instant
when it receives the signal message and the moment when
it delivers the kth message. The throughput for a speciﬁc
burst is the burst size k divided by the burst latency Lburst
(in seconds). The maximum throughput Tmax can be in-
ferred as the value at which the throughput stabilizes (i.e.,
does not change with increasing burst sizes).
The measurements were taken under three different
faultloads. In the failure-free faultload all processes behave
correctly. In the fail-stop faultload one process crashes be-
fore the measurements are taken (1 is the maximum num-
ber of processes that can fail because n ≥ 3f + 1). Fi-
nally, in the Byzantine faultload one process permanently
tries to disrupt the protocols. At the binary consensus layer,
it always proposes zero trying to impose a zero decision.
At the multi-valued consensus layer, it always proposes the
default value in both INIT and VECT messages, trying to
force correct processes to decide on the default value. The
impact of such attack, if successful, would be that correct
processes do not reach an agreement over which messages
should be delivered by the atomic broadcast protocol and,
consequentely, would have to start a new agreement round.
Failure-free faultload. Figure 4 shows the performance
of the atomic broadcast when no faults occur in the system.
Each curve shows the latency or throughput for a different
message size m. From the graph it is possible to observe
that the burst latency Lburst is linear with the burst size.
The stabilization point in the throughput curves indicates
the maximum throughput Tmax.
For a burst of 1000 messages, Lburst has a value of 1386
ms for a message size of 10 bytes, 1539 ms for 100 bytes,
2150 ms for 1K bytes and 12340 ms for 10K bytes. The sta-
bilization point in the throughput curves indicates the max-
imum throughput Tmax. The throughput stabilizes around
721 messages per second for a message size of 10 bytes,
650 msgs/s for 100 bytes, 465 msgs/s for 1K bytes, and 81
msgs/s for 10K bytes. These results were expected because
larger messages impose a higher load on the network, which
decreases the maximum throughput.
Fail-stop faultload. The performance of
the atomic
broadcast protocol with one crashed process is presented
in Figure 5. In this faultload, each correct process sends a
k
n−1 messages. Each curve shows the latency or
burst of
throughput for a different message size m.
Looking at the curves, it is possible to conclude that per-
formance is noticeably better with one fail-stop process than
in the failure-free scenario. This is because with one less
process there is less contention in the network allowing op-
erations to be executed faster. In more detail, the numbers
show for a burst of 1000 messages, Lburst has a value of
988 ms for 10-byte messages, 1164 ms for 100 bytes, 1607
ms for 1K bytes, and 8655 ms 10K bytes. The maximum
throughput Tmax is around 858 messages per second for a
message size of 10 bytes, 621 msgs/s for 100 bytes, 834
msgs/s for 1K bytes, and 115 msgs/s for 10K bytes.
Byzantine faultload. Figure 6 shows the performance of
atomic broadcast for different message sizes, with one pro-
cess trying to disrupt the protocol. In more detail, the num-
bers for a burst of 1000 messages are: Lburst has a value
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Figure 4. Latency and throughput for atomic broadcast with failure-free faultload.
Figure 5. Latency and throughput for atomic broadcast with fail-stop faultload.
of 1404 ms for 10-byte messages, 1576 ms for 100-byte
messages, 2175 ms for 1K-byte messages, and 12347 ms
for 10K-byte messages. The maximum throughput Tmax
is around 711 messages per second for 10-byte messages,
634 msgs/s for 100 bytes, 460 msgs/s for 1K bytes, and 81
msgs/s for 10K bytes. From the numbers it is possible to ob-
serve that performance is basically immune from the attacks
of the Byzantine process, when compared with the failure-
free scenario. The Byzantine process never managed to foil
any of the consensus protocols due to the robustness of the
atomic broadcast protocol.
An important result is that all the consensus protocols
reached agreement within one round, even under Byzantine
faults. This can be explained in a intuitive way as follows.
The experimental setting was a LAN, which not only pro-
vides a low-latency, high-throughput environment, it also
keeps the nodes within simmetrical distance of each other.
Due to this simmetry, in the atomic broadcast protocol, cor-
rect processes maintained a fairly consistent view of the re-
ceived AB MSG messages because they all received these
messages at relatively the same time. Any slight inconsis-
tencies that ocasionally existed over this view were squan-
dered when processes broadcasted the vector V (which was
built with the identiﬁers of the received AB MSG mes-
sages) and then constructed a new vector W (which serves
as the proposal for the multi-valued consensus) with the
identiﬁers that appeared in, at least, f + 1 of those V vec-
tors. This mechanism caused all correct processes to pro-
pose identical values in every instance of the multi-value
consensus, which allowed one-round decisions. In a more
asymmetrical environment, like a WAN, it is not guaranteed
that this result can be reproduced.
Relative Cost of Agreement. On all experiments only
two agreements were necessary to deliver an entire burst.
The observed pattern was that a consensus was initiated
immediately after the arrival of the ﬁrst message. While
the ﬁrst agreement task was being run, the remaining burst
would be received. Therefore, this remaining set of mes-
sages could be delivered with a second agreement. This
behavior has the interesting effect of diluting the cost of the
agreements as the load increases.
Figure 7. Percentage of broadcasts that are
due to the agreements.
Figure 7 shows the relative cost of the agreements with
respect to the total number of (reliable and echo) broadcasts
that was observed in the experiments. Basically, two quan-
tities were obtained for the transmission of every burst: the
total number of (reliable and echo) broadcasts, and the to-
tal number of broadcasts that were necessary to execute the
agreement operations. The values depicted in the ﬁgure are
the second quantity divided by the ﬁrst.