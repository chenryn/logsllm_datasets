sampling inspired weighting of results to obtain results for
multiple (sometimes all) strategies, while solving the output
variance for only one strategy. This will considerably speed
up the process of ﬁnding an optimal data collection strategy.
A. Use of Central Limit Theorem
Our approach would allow any characterisation of X(s) to
indicate uncertainty in the input parameter values. However,
if sources consist of samples that are averaged to estimate an
input parameter value, it is very natural to exploit sampling
statistics to gain insight in the variability of the source. In
then the sample
particular,
mean and sample variance are given by [12]:
let x1, x2, . . . , xN be samples,
(cid:100)E[X] =
and
(cid:100)V ar(X) =
1
N − 1
1
N
n=1
xn,
N(cid:88)
N(cid:88)
( (cid:100)E[X] − xn)2.
(cid:115) (cid:100)V ar[X]
(2)
(3)
The Central Limit Theorem [12] then says that the distribution
of the sample mean (cid:100)E[X] as a function of N converges to:
n=1
(cid:100)E[X] → N ( (cid:100)E[X],
),
N
(4)
where N (µ, σ) is the Normal Distribution with mean µ and
standard deviation σ. It is important to stress that the use of the
Normal distribution around the sample mean does not imply
that we assume the model incorporates Normal distributions.
For instance,
if we have a model with an exponentially
distributed delay for some event, we would use the sample
mean to estimate the rate of the exponential distribution. There
is no Normal distribution in the model, and the Central Limit
Theorem applies and represents uncertainty in the estimate for
the rate of the exponential distribution.
We use the Central Limit Theorem as follows, focusing
on Optimization Problem 1. In that formulation of the op-
timization problem, a strategy is given by s = {N (Di,j}, i =
1, . . . ,|P|, j = 1, . . . ,|Di|. It may be that some data has
been collected already, namely M (Di,j), i = 1, . . . ,|P|, j =
1, . . . ,|Di| samples, and that the sample mean is µi,j and
sample variance is σ2
i,j. We then get that for source Di,j we
expect that if we add N (Di,j) samples we obtain the following
Normal approximation of the sample mean (applying the
Central Limit Theorem as indicated above):
N (µi,j,
(cid:112)M (Di,j) + N (Di,j)
σi,j
).
(5)
(cid:90)
If possible, µi,j and σi,j are estimated from the M (Di,j)
initial samples using the sample mean and variance as in
Equation (2) and (3). Of course, if M (Di,j) = 0 there are no
existing samples, in which case one needs to use best effort to
determine µi,j and σi,j. The essence of our approach is that
we model the uncertainty associated with the data sources, see
Section III-C.
The main observation now is the following equality, in
which fX(s)(x) is the probability density function for the
random variable X(s) (as in Equation (1)), and g(Y (x))
denotes the output of the model for input x:
E[g(Y )|X(s)] =
g(Y (x))fX(s)(x)dx.
(6)
x
The equation says that the output g(Y (x)) needs to be com-
puted over all possible input parameter values of the strategy
s ∈ S, as drawn from the distribution of X(s). We are
now in a position to connect input parameter uncertainty with
output uncertainty by solving the model based on samples
drawn from the Normal distributions that express the input
uncertainty. We have to do that for every feasible strategy and
then select the optimal strategy. This leads to the following
Basic Algorithm for the optimal data collection strategy. Here
we present
the algorithm to solve the Sample Constraint
Optimization problem 1, similar algorithms can be formulated
for Optimization Problem 2 and Optimization Problem 3 .
Optimization Algorithm 1 (Basic Algorithm):
For each s ∈ S {
Do {
For each Di,j with M (Di,j) + N (Di,j) > 0 {
draw xi,j from N (µi,j,
√
);
σi,j
M (Di,j )+N (Di,j )
}
set x = ({xi,j});
solve y = g(Y (x));
update E[g(Y )|s] using y;
update V ar[E[g(Y )|s]] using y;
(Eq.(2) with y for xn)
(Eq.(3) with y for xn)
}
Until V ar[E[g(Y )|s]] accurate (Sec.IV-A2)
}
select s that minimizes V ar[E[g(Y )|s]];
Note that the algorithm purposely avoids the border case
M (Di,j) = N (Di,j) = 0 for a source Di,j. This case
corresponds to the situation that for source Di,j we have no
earlier samples (Mi,j = 0) and collect no additional samples
(Ni,j = 0), but we do have an assumption about the sampling
mean and standard deviation (µi,j and σi,j). In our proposed
approach, we must rank these strategies as leading to inﬁnite
variance in the input and because of that we are not able to
compute variance of the output for this strategy, and draw
the justiﬁed conclusion that such a strategy is inferior to any
other strategy. Finally, we note that to implement the outer
for loop for all strategies, we use that the set S of possible
strategies is derived from the distribution of the N samples
over all sources, which can be programmed conveniently with
a recursive algorithm, the details of which are not included in
this paper.
B. Importance Sampling to Determine Data Collection Strate-
gies
Basic Algorithm 1 may be expected to be time consum-
ing, since for each strategy the conditional output variance
V ar[E[g(Y )|s]] needs to be computed. Naively, one could do
this by simply running the models many times, for enough
samples of the Normal distributions that represent the input
uncertainty, but in this section we will show that samples can
be weighted to obtain results for many strategies concurrently.
The idea of weighting is identical to importance sampling, so
we named our approach after that technique.
Let us assume a strategy s ∈ S, and an anchor strategy
sa ∈ S, and let the weight ωs,sa(x) be deﬁned as:
ωs,sa (x) =
fX(s)(x)
fX(sa)(x)
.
(7)
That is, the weights express the magnitude of the difference
in likelihood of input values under different data collection
strategies. Then the following relation follows from Equation
(6):
E[g(Y )|X(s)] =
g(Y (x))fX(s)(x)dx =
(cid:90)
x
(8)
(9)
(10)
(cid:90)
x
(cid:90)
x
g(Y (x))fX(s)(x)
fX(sa)(x)
fX(sa)(x)
dx =
ωs,sa (x)g(Y (x))fX(sa)(x)dx
The above means that we can weigh the outputs y =
g(Y (x)) obtained from the anchor strategy sa in order to get
the result for other strategies. It is critical that the weights
are well deﬁned for all possible x. In our speciﬁc setting this
implies that strategy s must use the same sources as the anchor
strategy sa, but possibly with a different number of samples.
In particular, we use the Central Limit Theorem as in the Base
Algorithm 1, and assume anchor strategy sa is a valid strategy
i,j > 0 for all |sa| pairs (i, j) ∈ sa, where we use
(M sa
the superscript sa to denote that the number of samples is
that for strategy sa). For notational convenience, let xi,j be a
value from source Di,j and [xi,j] be an ordered sequence of
|sa| input values. Then:
i,j + N sa
(cid:90)
E[g(Y )|X(sa)] =
Φsa
i,j(xi,j)d[xi,j],
(cid:89)
i,j
g(Y ([xi,j]))
[xi,j ]
(11)
(12)
where Φsa
Normal N (µi,j,
√
i,j(xi,j) is the probability density function of the
Φsa
i,j(xi,j) =
− (xi,j−µi,j )2
2σ2
i,j
e
(M sa
i,j +N sa
i,j )
.
(13)
σi,j
i,j +N sa
i,j
M sa
):
1(cid:114) 2πσ2
i,j
M sa
i,j +N sa
i,j
(cid:89)
In Equation (12) we can weigh so that results for all strategies
with different values N s
i,j are derived, not just for the one value
of N sa
i,j the
number of samples in an alternative strategy s, we obtain:
i,j considered in the anchor strategy. So, with N s
(cid:118)(cid:117)(cid:117)(cid:116) M (s)
(cid:89)
i,j
i,j + N (s)
i,j + N (sa)
i,j
i,j
M (sa)
ωs,sa ([xi,j]) =
Φs
Φsa
i,j(xi,j)
i,j(xi,j)
=
(14)
(N (sa)
i,j +N (s)
i,j )
,
(15)
i,j
− (xi,j−µi,j )2
2σi,j
2
e
where we assumed that M sa = M s (this is a natural assump-
tion, but the above can easily be adjusted in the exponent if this
assumption is not valid). Then from Equations (2), (10) and
(12) it follows that E[g(Y )|s] can be derived from E[g(Y )|sa]
using the following algorithm. As in Basic Algorithm 1, we
assume a total of N samples to be distributed over the data
sources.
Optimization Algorithm 2 (Importance Sampling):
Choose an anchor strategy sa;
Do {
For each Di,j with M sa (Di,j) + N sa (Di,j) > 0 {
draw xi,j from N (µi,j,
√
σi,j
)
M sa (Di,j )+N sa (Di,j )
}
set x = ([xi,j]);
For all s ∈ S {
compute weights ωs,sa ([xi,j]) as in Eq. (15)
}
solve ysa = g(Y (x));
For all strategies s ∈ S {
update E[g(Y )|s] using y and ωs,sa ([xi,j]);
(Eq.(2) with ωs,sa ([xi,j]) × y for xn)
update V ar[E[g(Y )|s]] using y and ωs,sa ([xi,j]);
(Eq.(3) with y for xn
and ωs,sa ([xi,j]) as weight
for each term within sum)
}
}
Until for all s, V ar[E[g(Y )|s]] accurate
select s that minimizes V ar[E[g(Y )|s]];
(as in Sec.IV-A2)
Comparing the Basic Algorithm with the Importance Sam-
pling Algorithm, we see that the Importance Sampling Algo-
rithm replaces the outer For loop in Algorithm 1 with an inner
loop that utilises the importance sampling equations. This
should create a considerable speed up, since it implies that the
algorithm needs to be run for one strategy only (namely the
anchor strategy). However, the precise efﬁciency gain depends
on the accuracy obtained using the importance sampling
equations, which is determined by the stopping criterion. We
discuss and will experiment with the stopping criterion in
Section IV-A. Note furthermore that the Importance Sampling
Algorithm choses an ‘anchor’ strategy from which results for