number of clients: 52    
number of threads: 52    
duration: 120 s    
number of transactions actually processed: 2287129    
latency average = 2.728 ms    
latency stddev = 2.637 ms    
tps = 19056.932156 (including connections establishing)    
tps = 19058.904144 (excluding connections establishing)    
statement latencies in milliseconds:    
         0.001  \set aid random(1, 100000 * :scale)    
         0.000  \set bid random(1, 1 * :scale)    
         0.000  \set tid random(1, 10 * :scale)    
         0.000  \set delta random(-5000, 5000)    
         0.016  BEGIN;    
         2.504  UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;    
         0.038  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;    
         0.066  UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;    
         0.046  UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;    
         0.031  INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);    
         0.024  END;    
```    
存储 | 磁盘配置 | 磁盘标称iops能力 | cpu | 内存       
---|---|---|---|---      
essd pl3 | 20T | 1000000, 4GB/s带宽 | 52核104线程2.5GHz | 384GB      
test case | 104线程机器 essd pl3 20T      
---|---      
fdatasync 8K | 6221      
test case | 104线程机器 essd pl3 20T      
---|---      
8k 随机写 | IOPS=167k, BW=1303MiB/s    
8k 随机读 | IOPS=141k, BW=1103MiB/s    
8k 顺序写 | IOPS=158k, BW=1231MiB/s     
8k 顺序读 | IOPS=121k, BW=943MiB/s    
test case | 104线程机器 essd pl3 20T      
---|---      
10亿写入 | 1058秒     
10亿只读 | qps: 887505    
10亿读写 | tps  78006 , qps  390030      
test case | 104线程机器 essd pl3 20T        
---|---        
100亿 写入 | 7789秒    
100亿 只读 | qps: 31946    
100亿 读写 | tps  19056 , qps 95280         
## 补充测试2  
26核52线程 192G内存, 1.8T essd  
```  
export PGPORT=1923       
export PGDATA=/data03/pg12_$PGPORT/pg_root       
export PGHOST=$PGDATA        
initdb -D $PGDATA -U postgres -E SQL_ASCII --lc-collate=C --lc-ctype=en_US.utf8 --wal-segsize=1024      
```  
```  
postgres@iZbp1621kc3de3pm12a0exZ-> pg_test_fsync     
5 seconds per test  
O_DIRECT supported on this platform for open_datasync and open_sync.  
Compare file sync methods using one 8kB write:  
(in wal_sync_method preference order, except fdatasync is Linux's default)  
        open_datasync                      6455.059 ops/sec     155 usecs/op  
        fdatasync                          6114.725 ops/sec     164 usecs/op  
        fsync                              4182.573 ops/sec     239 usecs/op  
        fsync_writethrough                              n/a  
        open_sync                          4318.160 ops/sec     232 usecs/op  
Compare file sync methods using two 8kB writes:  
(in wal_sync_method preference order, except fdatasync is Linux's default)  
        open_datasync                      3187.113 ops/sec     314 usecs/op  
        fdatasync                          5170.527 ops/sec     193 usecs/op  
        fsync                              3430.941 ops/sec     291 usecs/op  
        fsync_writethrough                              n/a  
        open_sync                          2123.727 ops/sec     471 usecs/op  
Compare open_sync with different write sizes:  
(This is designed to compare the cost of writing 16kB in different write  
open_sync sizes.)  
         1 * 16kB open_sync write          3902.495 ops/sec     256 usecs/op  
         2 *  8kB open_sync writes         2136.396 ops/sec     468 usecs/op  
         4 *  4kB open_sync writes         1138.692 ops/sec     878 usecs/op  
         8 *  2kB open_sync writes           90.899 ops/sec   11001 usecs/op  
        16 *  1kB open_sync writes           74.271 ops/sec   13464 usecs/op  
Test if fsync on non-write file descriptor is honored:  
(If the times are similar, fsync() can sync data written on a different  
descriptor.)  
        write, fsync, close                4072.978 ops/sec     246 usecs/op  
        write, close, fsync                4136.551 ops/sec     242 usecs/op  
Non-sync'ed 8kB writes:  
        write                            296604.607 ops/sec       3 usecs/op  
```  
```  
. ./fiotest 32 8 8K randwrite /data03/test libaio 1 16G  
  write: IOPS=91.9k, BW=718MiB/s (753MB/s)(42.1GiB/60019msec)  
. ./fiotest 32 8 8K randread /data03/test libaio 1 16G  
  read: IOPS=92.2k, BW=720MiB/s (755MB/s)(42.2GiB/60003msec)  
. ./fiotest 32 8 8K write /data03/test libaio 1 16G  
  write: IOPS=91.9k, BW=718MiB/s (753MB/s)(42.1GiB/60003msec)  
. ./fiotest 32 8 8K read /data03/test libaio 1 16G  
  read: IOPS=92.0k, BW=719MiB/s (754MB/s)(42.2GiB/60026msec)      
```  
```  
pgbench -i -s 10000   
1000000000 of 1000000000 tuples (100%) done (elapsed 735.47 s, remaining 0.00 s)  
```  
```  
checkpoint;      
set max_parallel_workers_per_gather =16;      
set enable_indexscan =on;      
set enable_indexonlyscan =on;      
set enable_bitmapscan =off;      
set enable_seqscan=off;      
explain (analyze) select count(*) from pgbench_accounts ;      
set enable_indexscan =off;      
set enable_indexonlyscan =off;      
set enable_bitmapscan =off;      
set enable_seqscan=on;      
explain (analyze) select count(*) from pgbench_accounts ;    
```  
```  
pgbench -M prepared -n -r -P 3 -c 32 -j 32 -T 120 -S  
transaction type:   
scaling factor: 10000  
query mode: prepared  
number of clients: 32  
number of threads: 32  
duration: 120 s  
number of transactions actually processed: 9111047  
latency average = 0.421 ms  
latency stddev = 1.461 ms  
tps = 75913.573252 (including connections establishing)  
tps = 75917.478749 (excluding connections establishing)  
statement latencies in milliseconds:  
         0.000  \set aid random(1, 100000 * :scale)  
         0.421  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
```  
```  
pgbench -M prepared -n -r -P 3 -c 32 -j 32 -T 120   
transaction type:   
scaling factor: 10000  
query mode: prepared  
number of clients: 32  
number of threads: 32  
duration: 120 s  
number of transactions actually processed: 4021731  
latency average = 0.955 ms  
latency stddev = 1.840 ms  
tps = 33511.413835 (including connections establishing)  
tps = 33513.166609 (excluding connections establishing)  
statement latencies in milliseconds:  
         0.001  \set aid random(1, 100000 * :scale)  
         0.000  \set bid random(1, 1 * :scale)  
         0.000  \set tid random(1, 10 * :scale)  
         0.000  \set delta random(-5000, 5000)  
         0.019  BEGIN;  
         0.756  UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;  
         0.036  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
         0.046  UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;  
         0.039  UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;  
         0.032  INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);  
         0.025  END;  
```  
## 性能总结      
### 测试环境说明：      
存储 | 磁盘配置 | 磁盘标称iops能力 | cpu | 内存       
---|---|---|---|---      
本地ssd | 1.8T\*8 | 每块 240000 | 32核64线程2.5GHz | 512GB      
essd pl3 | 1.8T | 91800 | 26核52线程2.5GHz | 192GB      
essd pl3 | 1.8T\*8 | 每块 91800 | 26核52线程2.5GHz | 192GB      
essd pl3 | 20T | 1000000 | 26核52线程2.5GHz | 192GB      
essd pl3 | 20T | 1000000 | 52核104线程2.5GHz | 384GB     
### fsync 性能测试(单次IO能力, 检验数据库sync commit延迟)      
test case | 本地ssd 1.8T\*8 | essd pl3 1.8T\*8 | (26核)essd pl3 20T | (52核)essd pl3 20T | (26核)essd pl3 1.8T     
---|---|---|---|---|---      
fdatasync 8K | 28216 | 6087 | 5901 | 6221 | 6114    
### fio测试（iops能力，吞吐能力）      
test case | 本地ssd 1.8T\*8 | essd pl3 1.8T\*8 | (26核)essd pl3 20T | (52核)essd pl3 20T | (26核)essd pl3 1.8T     
---|---|---|---|---|---       
8k 随机写 | IOPS=131k, BW=1024MiB/s | IOPS=106k, BW=825MiB/s | IOPS=106k, BW=826MiB/s | IOPS=167k, BW=1303MiB/s | IOPS=91.9k, BW=718MiB/s     
8k 随机读 | IOPS=153k, BW=1193MiB/s | IOPS=106k, BW=826MiB/s | IOPS=106k, BW=830MiB/s | IOPS=141k, BW=1103MiB/s | IOPS=92.2k, BW=720MiB/s    
8k 顺序写 | IOPS=134k, BW=1050MiB/s | IOPS=106k, BW=826MiB/s | IOPS=106k, BW=824MiB/s | IOPS=158k, BW=1231MiB/s | IOPS=91.9k, BW=718MiB/s    
8k 顺序读 | IOPS=151k, BW=1182MiB/s | IOPS=106k, BW=829MiB/s | IOPS=106k, BW=825MiB/s | IOPS=121k, BW=943MiB/s | IOPS=92.0k, BW=719MiB/s      
### PostgreSQL tpcb 10亿数据量 测试，数据库综合性能      
test case | 本地ssd 1.8T\*8 | essd pl3 1.8T\*8 | (26核)essd pl3 20T | (52核)essd pl3 20T | (26核)essd pl3 1.8T        
---|---|---|---|---|---        
10亿写入 | 1058秒 | 948秒 | 1082秒 | 1058秒 | 735 秒       
10亿只读 | qps: 627000（服务器512G内存，全内存命中） | qps: 64979 （服务器192G内存，命中低，同时io rt不如本地盘）| qps: 37253 （服务器192G内存，命中低，同时io rt不如本地盘）| qps: 887505  （服务器384G内存，全内存命中） | qps: 75913 （服务器192G内存，命中低，同时io rt不如本地盘）    
10亿读写 | tps  31545 , qps  157725 | tps 25595 ，qps 127975 | tps 28627 , qps 143135 | tps  78006 , qps  390030 | tps  33511 , qps  167555       
### PostgreSQL tpcb 100亿数据量 测试，数据库综合性能        
test case | 本地ssd 1.8T\*8 | essd pl3 1.8T\*8 | (26核)essd pl3 20T | (52核)essd pl3 20T      
---|---|---|---|---       
100亿 写入 | 9486秒 | 8195秒 | 7983秒  | 7789秒        
100亿 只读 | qps: 158451 | qps: 21123 | qps: 22504 | qps: 31946       
100亿 读写 | tps  24720 , qps 123600 | tps  17243 , qps 86215 | tps  17087 , qps 85435 | tps  19056 , qps 95280           
## PG使用本地盘与云盘优缺点对比      
### 云盘版优势：      
- 更可靠（3副本 vs 1副本）(从节点挂了之后，主节点依旧有3副本。物理机版只剩1副本)      
- 更安全（支持云盘加密）      
- 更大容量（最大512TB(32TB\*16块) vs 14T）      
- 备份更快（快照 vs copy文件），超过1T本地盘备份时间可能长达10小时以上      
- 使用本地盘存储数据文件时，备份会产生网络传输，同时会占用os page cache(buffer io)。      
- 克隆更快（快照恢复 vs copy文件）      
- 云盘版主机挂了，不需要重建数据，漂移ECS挂载云盘即可，很快恢复。      
- 性能平滑：本地ssd gc时有较大性能抖动。云盘数据打散，整体io操作性能更平滑。      
### 云盘缺陷：      
- 单次io的延迟更高(164us VS 30us)，如何解决？      
    - 操作系统和数据库都有预读和写IO合并，PG本身大量操作是BufferIO，单次io延迟对PG的性能影响几乎可以忽略。      
    - 并发起来后IO单次延迟引入的等待也会被缩小。       
    - 单表不宜过大，建议分区。这样很多vacuum操作可以并行。      
## 云盘使用建议：      
由于云盘io引入了延迟，需要靠并行化弥补。      
1、表尽量分区，提升vacuum吞吐      
2、分析型业务使用32K block size，提高单次io量，提升吞吐      
3、使用操作系统、数据库层面块设备预读      
4、云盘的iops，带宽限制不仅仅取决于云盘规格、大小，同时还受到ecs规格限制，要综合来看。对io敏感建议选择大规格ecs。     
## 内核针对云盘场景优化建议：      
### io并行化：      
bgwriter，并行化      
checkpoint，并行化      
walwriter，并行化      
vacuum单表，并行化      
### 其他已实现并行化操作：      
参考pg 12并行计算相关内容：建表、索引、查询都已实现并行化。      
### dba优化建议：      
开启backend flush，让更多进程参与io，提高吞吐，降低ckpt负担。      
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")