# Performance Evaluation of Rewritten Class Files

## 5. Performance Evaluation

### 5.1. Microbenchmarks
We first measured a series of microbenchmarks to stress-test the JVM with certain language constructs, including looping, method and field accesses, exception handling, synchronization, and I/O. We used a microbenchmark package developed at the University of California, San Diego, and modified at the University of Arizona for the Sumatra Project. The results are shown in Figure 7.

**Figure 7.** Performance of rewritten microbenchmark class files relative to the performance of the corresponding original class files.
- **Application Performance:**
  - Original Classes: 1.425, 1.181, 1.253, 1.217, 1.082, 1.062, 1.067, 1.032
  - Modified Classes: 1.6, 1.4, 1.2, 1, 0.8, 0.6, 0.4, 0.2, 0
  - Modified Optimized: [Data not provided]

As expected, tight loops suffered the worst slowdowns, with roughly a factor of two. When we rewrite the loop, the termination check costs roughly the same as the original loop termination check, leading to a factor of two performance degradation.

For other microbenchmarks, the overheads were much smaller. The overhead of handling exceptions, performing synchronization, or doing I/O operations dominated the cost of checking for termination. The largest overhead was 14% for the synchronization microbenchmark. The additional overhead can be attributed to performing the termination check once for each iteration of the loop.

For the I/O and exception-handling microbenchmarks, the performance figures were much better. Since I/O and exception handling are relatively costly operations, modifications did not significantly impact performance.

The leaf method optimization generally had a small performance benefit. The loop method invocation microbenchmark showed the most dramatic improvement; the optimized benchmark ran 30% faster than the unoptimized benchmark. In one case, the exception handling benchmark, the optimized program ran roughly 1% slower than the unoptimized program. Similar behavior occurred in the Linpack macrobenchmark. The optimized programs performed fewer termination checks but still had a longer running time. The culprit appears to be Sun’s JIT compiler (sunwjit). When the benchmarks are run with the JIT disabled, the optimized programs are strictly faster than the non-optimized programs. We have observed similar deviant behavior with Sun’s HotSpot JIT running on Sparc/Solaris and Linux/x86. We have sent an appropriate bug report to Sun.

### 5.2. Application Benchmarks
We benchmarked real-world applications JavaCup, Linpack, Jess, and JOTP. These programs were chosen to provide sufficiently broad insight into our system’s performance.

- **JavaCup**: A parser-generator, chosen to demonstrate how the rewritten classes perform in handling text processing.
- **Jess**: An expert system, chosen to demonstrate the performance of the rewritten classes in handling symbolic data and solving logic problems.
- **Linpack**: A loop-intensive floating-point benchmark.
- **JOTP**: A one-time password generator using a cryptographic hash function.

The results are shown in Figure 8.

**Figure 8.** Performance of rewritten application class files relative to the performance of the corresponding original class files.

For the JavaCup test, we generated a parser for the Java 1.1 grammar, resulting in a modest 6% increase in execution time. For the Jess test, we ran several sample problems included with Jess through the system, and calculated the cumulative runtimes, resulting in a 3% increase in execution time. Both JavaCup and Jess represent applications that do not make extensive use of tight loops. Instead, these applications spend more of their time performing I/O and symbolic computations, closely tracking the performance of the I/O microbenchmark.

For the Java OTP generator, we generated a one-time password from a randomly-chosen seed and password, using 200,000 iterations, resulting in an 18% increase in runtime. For the Linpack benchmark, there was a 25% increase in runtime. Linpack is a loop-intensive program, while JOTP makes extensive use of method calls as well as loops. As a result, their performance more closely tracks the loop-based microbenchmarks. Note the benefit JOTP got from the leaf method optimization.

### 5.3. Termination Check Overhead
To gauge the actual impact of our class file modifications, we counted the number of times we checked the termination flag for each benchmark. Using this, we calculated the number of termination checks being performed per every second increase in runtime overhead. The results for all benchmarks are listed in Figure 9.

**Figure 9.** Average number of termination checks performed per one second of increase in runtime for the micro and application benchmarks.

For the three looping microbenchmarks, we found that roughly 40,000 checks are performed for every second of runtime overhead, evaluating to around 10 CPU cycles for each check. For the exception and synchronization microbenchmarks, while the cost of performing these original operations far outweighs the cost of the termination checks, we still see the same number of, and sometimes more, termination checks per second of overhead.

For the input/output microbenchmark, only around 970,000 termination checks are performed per second of overhead. This can be attributed to the additional overhead of the blocking call management code. It is also important to keep in mind that the cost of performing I/O far outweighs the cost of termination checks.

The application benchmarks reflect the results of the microbenchmarks. All of these results fell within the same range, between 15,000 and 29,000 checks per second of overhead. Since none of the benchmarks perform any significant amount of I/O, these figures are in line with the microbenchmarks results.

These performance figures indicate that for real-world applications, the slowdown will be roughly proportional to how much the application's performance is dependent on tight loops. Applications with tight loops may experience at worst a factor of two slowdown and more commonly 15-25%. Applications without tight loops can expect more modest slowdowns, most likely below 7%. The number of termination checks the system can perform per second does not seem to be a limiting factor in system performance.

## 6. Soft Termination in Practice
To demonstrate our soft termination system, we integrated it into the Jigsaw web server, a free, open Java-based web server that supports servlets. We integrated our bytecode-rewriting system into the servlet loader for Jigsaw, so every servlet loaded into Jigsaw is first rewritten to support soft termination.

We also wrote an administrative screen similar to the top utility in UNIX, providing a list of all active servlets and an option to terminate a servlet. Selecting this option activates the terminate signal in the specified servlet. We observed that servlets would take at most ten seconds to terminate.

Several attacks against Java focusing on resource exhaustion have been proposed, some of which exploit flaws in Java’s access control framework. Our soft termination system does not try to stop these attacks. However, it successfully stops applets that create infinite loops, override the `Applet.stop()` method, or catch a `ThreadDeath` exception and recreate the thread.

## 7. Conclusion
While Java and other general-purpose language-based systems for codelets have good support for memory protection, authorization, and access controls, there is little support for termination. Without termination, a system can be vulnerable to denial-of-service attacks or bugs where a faulty codelet has an infinite loop.

We introduced a concept called soft termination, along with a formal design and implementation for Java that allows for asynchronous and safe termination of misbehaving or malicious codelets. Soft termination can be implemented without making any changes to the underlying language or runtime system. Our Java implementation relies solely on class-file bytecode rewriting, making it portable across Java systems and easier to consider applying to non-Java systems.

In real-world benchmarks, our system shows slowdowns of 3-25%, which could possibly be further reduced if we could leverage a safe point mechanism already implemented within the JVM. A larger research area remains: building language runtimes that support the general process-management semantics of operating systems.

## 8. Acknowledgments
Jiangchun “Frank” Luo and Liwei Peng helped implement an early prototype of this system. Matthias Felleisen and Shriram Krishnamurthy provided many helpful comments. This work is supported by NSF Grant CCR-9985332.

## References
[References section remains unchanged]