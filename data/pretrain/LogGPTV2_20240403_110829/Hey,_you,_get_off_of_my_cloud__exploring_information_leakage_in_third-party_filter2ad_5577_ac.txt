check. In the rest of the paper we will therefore utilize the
203Count Median RTT (ms)
Co-resident instance
Zone 1 Control A
Zone 1 Control B
Zone 2 Control A
Zone 2 Control B
Zone 3 Control A
Zone 3 Control B
62
62
62
61
62
62
62
0.242
1.164
1.027
1.113
1.187
0.550
0.436
Figure 2: Median round trip times in seconds for probes
sent during the 62 co-residence checks.
(A probe to
Zone 2 Control A timed out.)
following when checking for co-residence of an instance with
a target instance we do not control. First one compares the
internal IP addresses of the two instances, to see if they are
numerically close. (For m1.small instances close is within
seven.)
If this is the case, the instance performs a TCP
SYN traceroute to an open port on the target, and sees if
there is only a single hop, that being the Dom0 IP. (This
instantiates the Dom0 IP equivalence check.) Note that this
check requires sending (at most) two TCP SYN packets and
is therefore very “quiet”.
Obfuscating co-residence. A cloud provider could likely
render the network-based co-residence checks we use moot.
For example, a provider might have Dom0 not respond in
traceroutes, might randomly assign internal IP addresses at
the time of instance launch, and/or might use virtual LANs
to isolate accounts. If such precautions are taken, attack-
ers might need to turn to co-residence checks that do not
rely on network measurements. In Section 8.1 we show ex-
perimentally that side-channels can be utilized to establish
co-residence in a way completely agnostic to network con-
ﬁguration. Even so, inhibiting network-based co-residence
checks would impede attackers to some degree, and so de-
termining the most eﬃcient means of obfuscating internal
cloud infrastructure from adversaries is a good potential av-
enue for defense.
7. EXPLOITING PLACEMENT IN EC2
Consider an adversary wishing to attack one or more EC2
instances. Can the attacker arrange for an instance to be
placed on the same physical machine as (one of) these vic-
tims? In this section we assess the feasibility of achieving
co-residence with such target victims, saying the attacker is
successful if he or she achieves good coverage (co-residence
with a notable fraction of the target set). We oﬀer two adver-
sarial strategies that make crucial use of the map developed
in Section 5 and the cheap co-residence checks we introduced
in Section 6. The brute-force strategy has an attacker sim-
ply launch many instances over a relatively long period of
time. Such a naive strategy already achieves reasonable suc-
cess rates (though for relatively large target sets). A more
reﬁned strategy has the attacker target recently-launched
instances. This takes advantage of the tendency for EC2
to assign fresh instances to the same small set of machines.
Our experiments show that this feature (combined with the
ability to map EC2 and perform co-residence checks) repre-
sents an exploitable placement vulnerability: measurements
show that the strategy achieves co-residence with a speciﬁc
(m1.small) instance almost half the time. As we discuss be-
low, an attacker can infer when a victim instance is launched
or might even trigger launching of victims, making this at-
tack scenario practical.
Towards understanding placement. Before we describe
these strategies, we ﬁrst collect several observations we ini-
tially made regarding Amazon’s (unknown) placement algo-
rithms. Subsequent interactions with EC2 only reinforced
these observations.
A single account was never seen to have two instances
simultaneously running on the same physical machine, so
running n instances in parallel under a single account results
in placement on n separate machines. No more than eight
m1.small instances were ever observed to be simultaneously
co-resident. (This lends more evidence to support our earlier
estimate that each physical machine supports a maximum of
eight m1.small instances.) While a machine is full (assigned
its maximum number of instances) an attacker has no chance
of being assigned to it.
We observed strong placement locality. Sequential place-
ment locality exists when two instances run sequentially (the
ﬁrst terminated before launching the second) are often as-
signed to the same machine. Parallel placement locality
exists when two instances run (from distinct accounts) at
roughly the same time are often assigned to the same ma-
chine. In our experience, launched instances exhibited both
strong sequential and strong parallel locality.
Our experiences suggest a correlation between instance
density, the number of instances assigned to a machine, and
a machine’s aﬃnity for having a new instance assigned to
it. In Appendix B we discuss an experiment that revealed a
bias in placement towards machines with fewer instances al-
ready assigned. This would make sense from an operational
viewpoint under the hypothesis that Amazon balances load
across running machines.
We concentrate in the following on the m1.small instance
type. However, we have also achieved active co-residence
between two m1.large instances under our control, and have
observed m1.large and c1.medium instances with co-resident
commercial instances. Based on the reported (using CPUID)
system conﬁgurations of the m1.xlarge and c1.xlarge in-
stance types, we assume that these instances have machines
to themselves, and indeed we never observed co-residence of
multiple such instances.
7.1 Brute-forcing placement
We start by assessing an obvious attack strategy: run nu-
merous instances over a (relatively) long period of time and
see how many targets one can achieve co-residence with.
While such a brute-force strategy does nothing clever (once
the results of the previous sections are in place), our hypoth-
esis is that for large target sets this strategy will already
allow reasonable success rates.
The strategy works as follows. The attacker enumerates
a set of potential target victims. The adversary then infers
which of these targets belong to a particular availability zone
and are of a particular instance type using the map from
Section 5. Then, over some (relatively long) period of time
the adversary repeatedly runs probe instances in the target
zone and of the target type. Each probe instance checks if
it is co-resident with any of the targets. If not the instance
is quickly terminated.
We experimentally gauged this strategy’s potential eﬃ-
cacy. We utilized as “victims” the subset of public EC2-
based web servers surveyed in Section 5 that responded with
204HTTP 200 or 206 to a wget request on port 80. (This re-
striction is arbitrary. It only makes the task harder since
it cut down on the number of potential victims.) This left
6 577 servers. We targeted Zone 3 and m1.small instances
and used our cloud map to infer which of the servers match
this zone/type. This left 1 686 servers. (The choice of zone
was arbitrary. The choice of instance type was due to the
fact that m1.small instances enjoy the greatest use.) We
collected data from numerous m1.small probe instances we
launched in Zone 3. (These instances were also used in the
course of our other experiments.) The probes were instru-
mented to perform the cheap co-residence check procedure
described at the end of Section 6 for all of the targets. For
any co-resident target, the probe performed a wget on port
80 (to ensure the target was still serving web pages). The
wget scan of the EC2 servers was conducted on October 21,
2008, and the probes we analyzed were launched over the
course of 18 days, starting on October 23, 2008. The time
between individual probe launches varied, and most were
launched in sets of 20.
We analyzed 1 785 such probe instances. These probes
had 78 unique Dom0 IPs. (Thus, they landed on 78 diﬀerent
physical machines.) Of the 1 686 target victims, the probes
achieved co-residency with 141 victim servers. Thus the
“attack” achieved 8.4% coverage of the target set.
Discussion. We point out that the reported numbers are
conservative in several ways, representing only a lower bound
on the true success rate. We only report co-residence if the
server is still serving web pages, even if the server was ac-
tually still running. The gap in time between our survey of
the public EC2 servers and the launching of probes means
that new web servers or ones that changed IPs (i.e. by being
taken down and then relaunched) were not detected, even
when we in fact achieved co-residence with them. We could
have corrected some sources of false negatives by actively
performing more internal port scans, but we limited our-
selves to probing ports we knew to already be serving public
web pages (as per the discussion in Section 4).
Our results suggest that even a very naive attack strategy
can successfully achieve co-residence against a not-so-small
fraction of targets. Of course, we considered here a large
target set, and so we did not provide evidence of eﬃcacy
against an individual instance or a small sets of targets. We
observed very strong sequential locality in the data, which
hinders the eﬀectiveness of the attack.
In particular, the
growth in target set coverage as a function of number of
launched probes levels oﬀ quickly. (For example, in the data
above, the ﬁrst 510 launched probes had already achieved
co-residence with 90% of the eventual 141 victims covered.)
This suggests that fuller coverage of the target set could
require many more probes.
7.2 Abusing Placement Locality
We would like to ﬁnd attack strategies that do better than
brute-force for individual targets or small target sets. Here
we discuss an alternate adversarial strategy. We assume
that an attacker can launch instances relatively soon after
the launch of a target victim. The attacker then engages
in instance ﬂooding: running as many instances in parallel
as possible (or as many as he or she is willing to pay for)
in the appropriate availability zone and of the appropriate
type. While an individual account is limited to 20 instances,
it is trivial to gain access to more accounts. As we show,
running probe instances temporally near the launch of a
victim allows the attacker to eﬀectively take advantage of the
parallel placement locality exhibited by the EC2 placement
algorithms.
But why would we expect that an attacker can launch
instances soon after a particular target victim is launched?
Here the dynamic nature of cloud computing plays well into
the hands of creative adversaries. Recall that one of the
main features of cloud computing is to only run servers
when needed. This suggests that servers are often run on in-
stances, terminated when not needed, and later run again.
So for example, an attacker can monitor a server’s state
(e.g., via network probing), wait until the instance disap-
pears, and then if it reappears as a new instance, engage
in instance ﬂooding. Even more interestingly, an attacker
might be able to actively trigger new victim instances due
to the use of auto scaling systems. These automatically grow
the number of instances used by a service to meet increases
in demand. (Examples include scalr [30] and RightGrid [28].
See also [6].) We believe clever adversaries can ﬁnd many
other practical realizations of this attack scenario.
The rest of this section is devoted to quantifying several
aspects of this attack strategy. We assess typical success
rates, whether the availability zone, attacking account, or
the time of day has some bearing on success, and the eﬀect
of increased time lag between victim and attacker launches.
Looking ahead, 40% of the time the attacker (launching just
20 probes) achieves co-residence against a speciﬁc target in-
stance; zone, account, and time of day do not meaningfully
impact success; and even if the adversary launches its in-
stances two days after the victims’ launch it still enjoys the
same rate of success.
In the following we will often use instances run by one of
our own accounts as proxies for victims. However we will
also discuss achieving co-residence with recently launched
commercial servers. Unless otherwise noted, we use m1.small
instances. Co-residence checks were performed via compar-
ison of Dom0 IPs.
The eﬀects of zone, account, and time of day. We
start with ﬁnding a base-line for success rates when run-
ning probe instances soon (on the order of 5 minutes) af-
ter victims. The ﬁrst experiment worked as follows, and
was repeated for each availability zone. A victim account
launched either 1, 10, or 20 instances simultaneously. No
sooner than ﬁve minutes later, a separate attacker account
requested launch of 20 instances simultaneously. The num-
ber of collisions (attacker instances co-resident with a victim
instance) are reported in the left table of Figure 3. As can
be seen, collisions are quickly found for large percentages of
victim instances. The availability zone used does not mean-
ingfully aﬀect co-residence rates.
We now focus on a single availability zone, Zone 1, for
the next experiment. We repeated, at three diﬀerent time
periods over the course of a day, the following steps: A sin-
gle victim instance was launched. No more than 5 minutes
later 20 probe instances were launched by another account,
and co-residence checks were performed. This process was
repeated 10 times (with at least 5 minutes in between con-
clusion of one iteration and beginning of the next). Each
iteration used a fresh victim; odd iterations used one ac-
count and even iterations used another. The right table in
Figure 3 displays the results. The results show a likelihood
of achieving co-residence as 40% — slightly less than half the
205# victims v # probes p
coverage
Zone 1
Zone 2
Zone 3
1
10
20
1
10
20
1
10
20
20
20
20
20
18
19
20
20
20
1/1
5/10
7/20
0/1
3/10
8/20
1/1
2/10
8/20
Trial
Midday
(11:13 – 14:22 PST)
Afternoon
(14:12 – 17:19 PST)
Night
(23:18 – 2:11 PST)
Account
A
B
Total
2 / 5
2 / 5
4/10