title:Algorithmic Transparency via Quantitative Input Influence: Theory
and Experiments with Learning Systems
author:Anupam Datta and
Shayak Sen and
Yair Zick
2016 IEEE Symposium on Security and Privacy
2016 IEEE Symposium on Security and Privacy
Algorithmic Transparency via
Quantitative Input Inﬂuence:
Theory and Experiments with Learning Systems
Anupam Datta
Shayak Sen
Yair Zick
Carnegie Mellon University, Pittsburgh, USA
{danupam, shayaks, yairzick}@cmu.edu
Abstract—Algorithmic systems that employ machine learning
play an increasing role in making substantive decisions in modern
society, ranging from online personalization to insurance and
credit decisions to predictive policing. But their decision-making
processes are often opaque—it is difﬁcult to explain why a certain
decision was made. We develop a formal foundation to improve
the transparency of such decision-making systems. Speciﬁcally,
we introduce a family of Quantitative Input Inﬂuence (QII)
measures that capture the degree of inﬂuence of inputs on outputs
of systems. These measures provide a foundation for the design
of transparency reports that accompany system decisions (e.g.,
explaining a speciﬁc credit decision) and for testing tools useful
for internal and external oversight (e.g., to detect algorithmic
discrimination).
Distinctively, our causal QII measures carefully account for
correlated inputs while measuring inﬂuence. They support a
general class of transparency queries and can,
in particular,
explain decisions about individuals (e.g., a loan decision) and
groups (e.g., disparate impact based on gender). Finally, since
single inputs may not always have high inﬂuence, the QII
measures also quantify the joint
inﬂuence of a set of inputs
(e.g., age and income) on outcomes (e.g. loan decisions) and the
marginal inﬂuence of individual inputs within such a set (e.g.,
income). Since a single input may be part of multiple inﬂuential
sets, the average marginal inﬂuence of the input is computed
using principled aggregation measures, such as the Shapley value,
previously applied to measure inﬂuence in voting. Further, since
transparency reports could compromise privacy, we explore the
transparency-privacy tradeoff and prove that a number of useful
transparency reports can be made differentially private with very
little addition of noise.
Our empirical validation with standard machine learning algo-
rithms demonstrates that QII measures are a useful transparency
mechanism when black box access to the learning system is
available. In particular, they provide better explanations than
standard associative measures for a host of scenarios that we
consider. Further, we show that in the situations we consider,
QII is efﬁciently approximable and can be made differentially
private while preserving accuracy.
I. INTRODUCTION
Algorithmic decision-making systems that employ machine
learning and related statistical methods are ubiquitous. They
drive decisions in sectors as diverse as Web services, health-
care, education, insurance, law enforcement and defense [1],
[2], [3], [4], [5]. Yet their decision-making processes are often
opaque. Algorithmic transparency is an emerging research area
aimed at explaining decisions made by algorithmic systems.
2375-1207/16 $31.00 © 2016 IEEE
© 2016, Anupam Datta. Under license to IEEE.
DOI 10.1109/SP.2016.42
DOI 10.1109/SP.2016.42
598
598
The call for algorithmic transparency has grown in in-
tensity as public and private sector organizations increas-
ingly use large volumes of personal information and complex
data analytics systems for decision-making [6]. Algorithmic
transparency provides several beneﬁts. First, it is essential
to enable identiﬁcation of harms, such as discrimination,
introduced by algorithmic decision-making (e.g., high interest
credit cards targeted to protected groups) and to hold entities
in the decision-making chain accountable for such practices.
This form of accountability can incentivize entities to adopt
appropriate corrective measures. Second,
transparency can
help detect errors in input data which resulted in an adverse
decision (e.g., incorrect information in a user’s proﬁle because
of which insurance or credit was denied). Such errors can then
be corrected. Third, by explaining why an adverse decision
was made, it can provide guidance on how to reverse it (e.g.,
by identifying a speciﬁc factor in the credit proﬁle that needs
to be improved).
Our Goal. While the importance of algorithmic transparency
is recognized, work on computational foundations for this
research area has been limited. This paper initiates progress
in that direction by focusing on a concrete algorithmic trans-
parency question:
How can we measure the inﬂuence of inputs (or features) on
decisions made by an algorithmic system about individuals or
groups of individuals?
Our goal is to inform the design of transparency reports,
which include answers to transparency queries of this form.
To be concrete, let us consider a predictive policing system
that forecasts future criminal activity based on historical data;
individuals high on the list receive visits from the police.
An individual who receives a visit from the police may seek
a transparency report that provides answers to personalized
transparency queries about the inﬂuence of various inputs
(or features), such as race or recent criminal history, on the
system’s decision. An oversight agency or the public may
desire a transparency report that provides answers to aggregate
transparency queries, such as the inﬂuence of sensitive inputs
(e.g., gender, race) on the system’s decisions concerning the
entire population or about systematic differences in decisions
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
among groups of individuals (e.g., discrimination based on
race or age). These reports can thus help identify harms and
errors in input data, and provide guidance on what
input
features to work on to modify the decision.
Our Model. We focus on a setting where a transparency
report is generated with black-box access to the decision-
making system1 and knowledge of the input dataset on which
it operates. This setting models the kind of access available
to a private or public sector entity that pro-actively publishes
transparency reports. It also models a useful level of access
required for internal or external oversight of such systems
to identify harms introduced by them. For the former use
case, our approach provides a basis for design of transparency
mechanisms; for the latter,
it provides a formal basis for
testing. Returning to our predictive policing system, the law
enforcement agency that employs it could proactively publish
transparency reports, and test the system for early detection
of harms like race-based discrimination. An oversight agency
could also use transparency reports for post hoc identiﬁcation
of harms.
Our Approach. We formalize transparency reports by introduc-
ing a family of Quantitative Input Inﬂuence (QII) measures
that capture the degree of inﬂuence of inputs on outputs of
the system. Three desiderata drove the deﬁnitions of these
measures.
First, we seek a formalization of a general class of
transparency reports that allows us to answer many useful
transparency queries related to input inﬂuence, including but
not limited to the example forms described above about the
system’s decisions about individuals and groups.
Second, we seek input inﬂuence measures that appropriately
account for correlated inputs—a common case for our target
applications. For example, consider a system that assists in
hiring decisions for a moving company. Gender and the
ability to lift heavy weights are inputs to the system. They
are positively correlated with each other and with the hiring
decisions. Yet transparency into whether the system uses the
weight lifting ability or the gender in making its decisions (and
to what degree) has substantive implications for determining if
it is engaging in discrimination (the business necessity defense
could apply in the former case [7]). This observation makes
us look beyond correlation coefﬁcients and other associative
measures.
Third, we seek measures that appropriately quantify input
inﬂuence in settings where any input by itself does not have
signiﬁcant inﬂuence on outcomes but a set of inputs does.
In such cases, we seek measures of joint inﬂuence of a set
of inputs (e.g., age and income) on a system’s decision (e.g.,
to serve a high-paying job ad). We also seek measures of
marginal inﬂuence of an input within such a set (e.g., age)
on the decision. This notion allows us to provide ﬁner-grained
1By “black-box access to the decision-making system” we mean a typical
setting of software testing with complete control of inputs to the system and
full observability of the outputs.
transparency about the relative importance of individual inputs
within the set (e.g., age vs. income) in the system’s decision.
We achieve the ﬁrst desideratum by formalizing a notion
of a quantity of interest. A transparency query measures the
inﬂuence of an input on a quantity of interest. A quantity of
interest represents a property of the behavior of the system for
a given input distribution. Our formalization supports a wide
range of statistical properties including probabilities of various
outcomes in the output distribution and probabilities of output
distribution outcomes conditioned on input distribution events.
Examples of quantities of interest
include the conditional
probability of an outcome for a particular individual or group,
and the ratio of conditional probabilities for an outcome for
two different groups (a metric used as evidence of disparate
impact under discrimination law in the US [7]).
We achieve the second desideratum by formalizing causal
QII measures. These measures (called Unary QII) model the
difference in the quantity of interest when the system operates
over two related input distributions—the real distribution and a
hypothetical (or counterfactual) distribution that is constructed
from the real distribution in a speciﬁc way to account for
correlations among inputs. Speciﬁcally, if we are interested in
measuring the inﬂuence of an input on a quantity of interest of
the system behavior, we construct the hypothetical distribution
by retaining the marginal distribution over all other inputs and
sampling the input of interest from its prior distribution. This
choice breaks the correlations between this input and all other
inputs and thus lets us measure the inﬂuence of this input
on the quantity of interest, independently of other correlated
inputs. Revisiting our moving company hiring example, if the
system makes decisions only using the weightlifting ability of
applicants, the inﬂuence of gender will be zero on the ratio of
conditional probabilities of being hired for males and females.
We achieve the third desideratum in two steps. First, we
deﬁne a notion of joint inﬂuence of a set of inputs (called
Set QII) via a natural generalization of the deﬁnition of the
hypothetical distribution in the Unary QII deﬁnition. Second,
we deﬁne a family of Marginal QII measures that model the
difference on the quantity of interest as we consider sets with
and without the speciﬁc input whose marginal inﬂuence we
want to measure. Depending on the application, we may pick
these sets in different ways, thus motivating several different
measures. For example, we could ﬁx a set of inputs and ask
about the marginal inﬂuence of any given input in that set on
the quantity of interest. Alternatively, we may be interested in
the average marginal inﬂuence of an input when it belongs
to one of several different sets that signiﬁcantly affect the
quantity of interest. We consider several marginal inﬂuence
aggregation measures from cooperative game theory originally
developed in the context of inﬂuence measurement in voting
scenarios and discuss their applicability in our setting. We
also build on that literature to present an efﬁcient approximate
algorithm for computing these measures.
Recognizing that different forms of transparency reports
may be appropriate for different settings, we generalize our QII
measures to be parametric in its key elements: the intervention
599599
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
used to construct the hypothetical input distribution; the quan-
tity of interest; the difference measure used to quantify the
distance in the quantity of interest when the system operates
over the real and hypothetical input distributions; and the
aggregation measure used to combine marginal QII measures
across different sets. This generalized deﬁnition provides a
structure for exploring the design space of transparency re-
ports.
Since transparency reports released to an individual, reg-
ulatory agency, or the public might compromise individual
privacy, we explore the possibility of answering transparency
queries while protecting differential privacy [8]. We prove
bounds on the sensitivity of a number of transparency queries
and leverage prior results on privacy ampliﬁcation via sam-
pling [9] to accurately answer these queries.
We demonstrate the utility of the QII framework by de-
veloping two machine learning applications on real datasets:
an income classiﬁcation application based on the benchmark
adult dataset [10], and a predictive policing application
based on the National Longitudinal Survey of Youth [11].
Using these applications, we argue, in Section VII, the need
for causal measurement by empirically demonstrating that
in the presence of correlated inputs, observational measures
are not informative in identifying input inﬂuence. Further,
we analyze transparency reports of individuals in our dataset
to demonstrate how Marginal QII can provide insights into
individuals’ classiﬁcation outcomes. Finally, we demonstrate
that under most circumstances, QII measures can be made
differentially private with minimal addition of noise, and can
be approximated efﬁciently.
In summary, this paper makes the following contributions:
• A formalization of a speciﬁc algorithmic transparency
problem for decision-making systems. Speciﬁcally, we
deﬁne a family of Quantitative Input Inﬂuence metrics
that accounts for correlated inputs, and provides answers
to a general class of transparency queries, including the
absolute and marginal
inﬂuence of inputs on various
behavioral system properties. These metrics can inform
the design of transparency mechanisms and guide pro-
active system testing and posthoc investigations.
• A formal treatment of privacy-transparency trade-offs,
in particular, by construction of differentially private
answers to transparency queries.
• An implementation and experimental evaluation of the
metrics over two real data sets. The evaluation demon-
strates that (a) the QII measures are informative; (b) they
remain accurate while preserving differential privacy; and
(c) can be computed quite quickly for standard machine
learning systems applied to real data sets.
strongly correlated with gender (with men having better overall
lifting ability than woman). One particular question that an
analyst may want to ask is: “What is the inﬂuence of the input
Gender on positive classiﬁcation for women?”. The analyst
observes that 20% of women are approved according to his
classiﬁer. Then, he replaces every woman’s ﬁeld for gender
with a random value, and notices that the number of women
approved does not change. In other words, an intervention on
the Gender variable does not cause a signiﬁcant change in
the classiﬁcation outcome. Repeating this process with Weight
Lifting Ability results in a 20% increase in women’s hiring.
Therefore, he concludes that for this classiﬁer, Weight Lifting
Ability has more inﬂuence on positive classiﬁcation for women
than Gender.
By breaking correlations between gender and weight lifting
ability, we are able to establish a causal relationship between
the outcome of the classiﬁer and the inputs. We are able to
identify that despite the strong correlation between a negative
classiﬁcation outcome for women, the feature gender was not
a cause of this outcome. We formalize the intuition behind
such causal experimentation in our deﬁnition of Quantitative
Input Inﬂuence (QII).
We are given an algorithm A. A operates on inputs (also
referred to as features for ML systems), N = {1, . . . , n}.
(cid:2)
Every i ∈ N, can take on various states, given by Xi. We let
i∈N Xi be the set of possible feature state vectors, let
X =
Z be the set of possible outputs of A. For a vector x ∈ X
and set of inputs S ⊆ N, x|S denotes the vector of inputs in
S. We are also given a probability distribution π on X , where
π(x) is the probability of the input vector x. We can deﬁne a
marginal probability of a set of inputs S in the standard way
as follows:
πS(x|S) =
(1)
When S is a singleton set {i}, we write the marginal
{x(cid:2)∈X|x(cid:2)|S =x|S}
)
π(x(cid:3)
(cid:3)
probability of the single input as πi(x).
Informally,
to quantify the inﬂuence of an input i, we
compute its effect on some quantity of interest; that is, we
measure the difference in the quantity of interest, when the
feature i is changed via an intervention. In the example above,
the quantity of interest is the fraction of positive classiﬁcation
of women. In this paper, we employ a particular interpretation
of “changing an input”, where we replace the value of every
input with a random independently chosen value. To describe
the replacement operation for input i, we ﬁrst deﬁne an
expanded probability space on X × X , with the following
distribution:
II. UNARY QII
Consider the situation discussed in the introduction, where
an automated system assists in hiring decisions for a moving
company. The input features used by this classiﬁcation system
are : Age, Gender, Weight Lifting Ability, Marital Status and
Education. Suppose that, as before, weight lifting ability is
˜π(x, u) = π(x)π(u).
(2)
The ﬁrst component of an expanded vector (x, u), is just
the original input vector, whereas the second component repre-
sents an independent random vector drawn from the same dis-