ally or not; as above, we do not recommend this mode
of operation.
3.5 Server-proxy signaling
SSL splitting does not mandate any particular cache co-
herency mechanism, but it does affect the factors that
make one mechanism better than another.
In particu-
lar, caching with SSL splitting is at the SSL record level
rather than at the ﬁle level. In theory, a single ﬁle could
be split up into records in many different ways, and this
would be a problem for the caching mechanism; how-
ever, in practice, a particular SSL implementation will
always split up a given data stream in the same way.
Deciding which records to encode as verbatim records
and which to encode as stub records can be done in
two ways. The server can remember which records are
cached on each proxy, and consult an internal table when
deciding whether to send a record as a stub. However,
this has several disadvantages. It places a heavy burden
on the server, and does not scale well to large numbers
of proxies. It does not give the proxies any latitude in
deciding which records to cache and which to drop, and
proxies must notify the server of any changes in their
cached set. Finally, this method does not give a clear
way for the proxies to initialize their cache.
Our design for SSL splitting uses a simpler and more
robust method. The server does not maintain any state
with respect to the proxies; it encodes records as ver-
batim or stub without regard to the proxy. If the proxy
receives a stub that is not in its local cache, it triggers
a cache miss handler, which uses a simple, HTTP-like
protocol to download the body of the record from the
server. Thus, the proxies are self-managing and may de-
ﬁne their own cache replacement policies. This design
requires the server to maintain a local cache of recently-
sent records, so that it will be able to serve cache miss
requests from proxies. Although a mechanism similar to
TCP acknowledgements could be used to limit the size
of the server’s record cache, a simple approach that suf-
ﬁces for most applications is to pin records in the cache
until the associated connection is terminated.
The cache-miss design permits the server to use an ar-
bitrary policy to decide which records to encode as stub
and hence to make available for caching; the most ef-
ﬁcient policy depends on the application. For HTTPS
requests, an effective policy is to cache all application-
data records that do not contain HTTP headers, since the
headers are dynamic and hence not cacheable. Most of
the records in the SSL handshake contain dynamic ele-
ments and hence are not cacheable; however, the server’s
certiﬁcates are cacheable. Caching the server certiﬁcates
has an effect similar to that of the “fast-track” optimiza-
tion described in [22], and results in an improvement in
SSL handshake performance. This caching is especially
beneﬁcial for requests of small ﬁles, where the latency
is dominated by the SSL connection time.
Since the stub identiﬁer is unique and is not reused, there
is no need for a mechanism to invalidate data in the
190
12th USENIX Security Symposium 
USENIX Association
cache. When the ﬁle referenced by an URL changes,
the server sends a different stream of identiﬁers to the
proxy, which does not know anything about the URL at
all. In contrast, normal caching Web proxies [24] rely on
invalidation timeouts for a weak form of consistency.
4 Implementation
Our implementation of SSL splitting consists of a self-
contained proxy module and a patched version of the
OpenSSL library, which supports SSL version 3 [7] and
Transport Layer Security (TLS) [4].
4.1 Server: modiﬁed OpenSSL
We chose to patch OpenSSL, rather than developing our
own protocol implementation, to simplify deployment.
Any server that uses OpenSSL, such as the popular Web
server Apache, works seamlessly with the SSL splitting
protocol. In addition, the generic SSLizing proxy STun-
nel, linked with our version of OpenSSL, works as an
SSL splitting server: using this, one can set up SSL split-
ting for servers that don’t natively understand SSL. This
allows SSL splitting to be layered as an additional access
method on top of an existing network resource.
Our modiﬁed version of OpenSSL intercepts the record-
encoding routine do ssl write and analyzes outgo-
ing SSL records to identify those that would beneﬁt from
caching. Our current implementation tags non-header
application-data records and server certiﬁcate records, as
described in Section 3.5.
Records that are tagged for caching are hashed to pro-
duce a short digest payload ID, and the bodies of these
records are published to make them available to prox-
ies that do not have them cached. While publishing may
take many application-speciﬁc forms, our implementa-
tion simply writes these payloads into a cache directory
on the server; a separate daemon process serves this di-
rectory to proxies.
Records that are not tagged for caching use the literal
payload “ID” encoding. Whether or not the record is
tagged for caching, the library encodes the record as a
stub message; this design choice simpliﬁes the code.
Because the server may have to ship its encryption key
and IV to the proxy,
the modiﬁed OpenSSL library
contains additional states in the connection state ma-
chine to mediate the sending of the key-expose message.
The server sends this message immediately after any
change cipher spec record, since at this point the
connection adopts a new set of keys.
4.2 Proxy
The proxy is simple: it forks off two processes to for-
ward every accepted connection. It is primarily written
in OO Perl5, with the performance-critical block cipher
and CBC mode implementation in C. The proxy includes
a pluggable cache hierarchy: when a cache lookup for
a payload ID fails, it can poll outside sources, such as
other proxies, for the missing data. If all else fails, the
server itself serves as an authority of last resort. Only
if none of these have the payload will the proxy fail the
connection.
The proxy could also replace verbatim messages from
the client to the server with stub messages to avoid send-
ing the complete request to the server. Because the
client’s data consists primarily of HTTP GET requests,
however, which are already short and are not typically
repeated, our current proxy doesn’t do so. In the future,
though, we may explore a proxy that compresses HTTP
headers using stub messages.
4.3 Message formats
Figures 3 and 4 show the format of verbatim and stub
message in the same notation as the SSL speciﬁcation.
The main difference between verbatim and stub is that in
stub the payload is split into a compact encoding of the
data and a MAC authenticator for the data. Also, a stub
record is never encrypted, since the proxy would have to
decrypt it anyway in order to manipulate its contents.
We have deﬁned two types of encoding for the payload.
The literal encoding is the identity function; this encod-
ing is useful for software design reasons, but is function-
ally equivalent to a verbatim SSL record.
The digest encoding is a SHA-1 [5] digest of the payload
contents. This encoding provides effectively a unique
identiﬁer that depends only on the payload. This choice
is convenient for the server, and allows the proxy to store
payloads from multiple independent servers in a single
cache without concern about namespace collisions.
There are many alternative ID encodings possible with
the given stub message format; for example, a simple
serial number would sufﬁce. The serial number, how-
ever, has only small advantages over a message digest.
A serial number is guaranteed to be unique, unlike a di-
gest. On the other hand, generating serial numbers re-
quires servers to maintain additional state, and places an
onus upon proxies to separate the caches corresponding
to multiple servers; both of these result in greater com-
plexity than the digest encoding.
USENIX Association
12th USENIX Security Symposium 
191
enum { ccs(0x14), alert(0x15), handshake(0x16), data(0x17) } ContentType;
struct {
ContentType
uint8
opaque
} VerbatimMessage;
content_type;
ssl_version[2];
encrypted_data_and_mac;
one byte long
includes implicit 2-byte length field
Figure 3: Format of a verbatim SSL record.
enum { s_ccs(0x94), s_alert(0x95), s_handshake(0x96), s_data(0x97) } StubContentType;
enum { literal(1), digest(2), (2ˆ16-1) } IDEncoding;
struct {
StubContentType content_type;
uint8
uint16
IDEncoding
opaque
opaque
} StubMessage;
ssl_version[2];
length;
encoding;
id;
mac;
= 6 + length(id) + length(mac)
= verbatim.content_type | 0x80
Figure 4: Format of a stub message.
Another alternative is to use as the encoding a com-
pressed representation of the payload. While this choice
would result in signiﬁcant savings for text-intensive
sites, it would not beneﬁt image, sound, or video ﬁles
at all, since most media formats are already highly com-
pressed. For this reason, we have not implemented this
feature.
The key-expose message is used to transmit the server
encryption key and IV to the proxy (see Figure 5), if this
feature is enabled. In our implementation, stub records
are sent in the clear to the proxy, which encrypts them
before sending them to the client.
5 Cooperative Web caching using SSL
splitting
Using SSL splitting, we have developed Barnraising,
a cooperative Web caching system consisting of a dy-
namic set of “volunteer” hosts. The purpose of this sys-
tem is to improve the throughput of bandwidth-limited
Web servers by harnessing the resources of geographi-
cally diverse proxies, without trusting those proxies to
ensure that the correct data is served.
database and handles redirecting client requests to vol-
unteers.
Volunteer hosts do not
locally store any conﬁgura-
tion information, such as the SSL splitting server’s
These parameters are
address and port number.
supplied by the broker
in response to the vol-
join request, which simply speciﬁes an
unteer’s
identifying URI of
the form barnraising://
broker.domain.org/some/site/name.
This design enables a single broker to serve any number
of Barnraising-enabled Web sites, and permits users to
volunteer for a particular site given only a short URI for
that site. Since conﬁguration parameters are under the
control of the broker, they can be changed without man-
ually reconﬁguring all proxies, allowing sites to upgrade
transparently.
The broker represents a potential bottleneck for the sys-
tem, and it could be swamped by a large number of si-
multaneous join or leave requests. However, since the
join/leave protocol is lightweight, this is unlikely to be a
performance issue under normal operating conditions. If
the load incurred by requests is high compared to the ac-
tual SSL splitting trafﬁc, the broker can simply rate-limit
them until the proxy set stabilizes at a smaller size.
5.1
Joining and leaving the proxy set
5.2 Redirection
Volunteers join or leave the proxy set of a site by con-
tacting a broker server, which maintains the volunteer
Barnraising currently employs the DNS redirection
method [11], but could be modiﬁed to support other
192
12th USENIX Security Symposium 
USENIX Association
enum { key_expose(0x58) } KeyExposeContentType;
struct {
KeyExposeContentType content_type;
uint8
uint16
opaque
opaque
} KeyExposeMessage;
ssl_version[2];
length;
key;
iv;
= 4 + length(key) + length(iv)
Figure 5: Format of key-expose message.
Server
5.3 Distributing the Web cache
Broker
Proxies
Client
Figure 6: Proxy set for a site using Barnraising.
techniques, such as URL rewriting [10, 1]. Barnraising’s
broker controls a mysql [16] database, from which a
mydns [15] server processes DNS requests.
resolving an URL https://
Consider a client
www.domain.org/foo/. If www.domain.org is
using Barnraising, the DNS server for domain.org is
controlled by the broker, which will resolve the name to
the IP address of a volunteer proxy.
We chose redirection using DNS because it maintains
HTTPS reference integrity — that is, it guarantees that
hyperlinks in HTML Web pages dereference to the in-
tended destination pages. HTTPS compares the host-
name speciﬁed in the https URL with the certiﬁcate
presented by the server. Therefore, when a client con-
tacts a proxy via a DNS name, the certiﬁcate presented
to the client, by the server, via the proxy, must match
the domain name. When DNS redirection is used, the
domain name will be of the form www.domain.org,
and will match the domain name in the certiﬁcate.
The client will initiate an HTTPS connection to the
proxy, which will forward that request, using SSL split-
ting, to the server. Since frequently-accessed data will
be served out of the proxy’s cache, the central server’s
bandwidth usage will be essentially limited to the SSL
handshake, MAC stream, and payload IDs.
To increase the set of cached payloads available to a
proxy, while decreasing the local storage requirements,
proxies could share the cache among themselves.
In
this design, volunteer nodes would join a wide-area Dis-
tributed Hash Table (DHT) [3] comprising all of the vol-
unteers for a given Web site. When lookups in the local
cache fail, nodes could attempt to ﬁnd another volunteer
with the desired data item by looking for the data in the
DHT. If that fails too, the proxy would contact the cen-
tral server.
Blocks in the DHT are named by the same cryptographic
hash used for stub IDs. This decision allows correctly-
operating volunteers to detect and discard any invalid
blocks that a malicious volunteer might have inserted in
the DHT.
5.4 Deploying Barnraising
Barnraising is designed to be initially deployed as a
transparent layer over an existing Web site, and incre-
mentally brought into the core of the Web server. Us-
ing STunnel linked with our patched OpenSSL library,
an SSL splitting server that proxies an existing HTTP
server can be set up on the same or a different host; this
choice allows Barnraising to be tested without disrupt-
ing existing services. If the administrator later decides
to move the SSL splitting server into the core, he can
use Apache linked with SSL-splitting OpenSSL.
The broker requires a working installation of mysql and
mydns; since it has more dependencies than the server,
administrators may prefer to use an existing third-party
USENIX Association
12th USENIX Security Symposium 
193
HTTP
HTTPS
Cold cache
Warm cache
1000
100
10
1
0.1
i
)
h