## Page 56
-type:log
filebeat.prospectors:
type:log
filebeat.prospectors:
者WARN 开始的行。
-type:log
filebeat.prospectors:
以 DEBUG 开始的行。
置 recursive_glob 参数来递归查找指定目录下所有子目录中的日志文件。
符，如“/var/log/*log”，表示匹配目录“/var/log/”下所有以.log 结尾的日志文件。可以通过设
配置信息，如文件路径、扫描周期等参数信息，具体如下。
送和接收端回复确认信息。
paths:
include_lines:['^ERR','^WARN']
paths:
exclude_lines: ['^DEBUG']
paths:
exclude_files：Filebeat 将会忽略收集匹配正则表达式的文件。
-"/var/log/nginx.log"
include_lines：Filebeat 只收集匹配正则表达式的行。例如，下面的例子只收集以 ERR 或
-"/var/log/nginx.log"
exclude_lines：Filebeat 将会过滤所有匹配正则表达式的行。例如，下面的例子会丢弃所有
paths：需要收集的日志文件绝对路径列表，每行一个路径，以“-”符号开始，支持通配
type：输入数据的类型，默认为log，还可以是 stdin、redis、udp、docker 等类型。
Filebeat 使用 Prospector 来定位和处理文件，我们可以在 Filebeat 的配置文件中指定一个或
2.配置Prospector
fields：可以在fields 字段中添加自定义的信息随日志一起输出。
tags：可以设置多个标签，用来区分不同的服务，如 tags:["web","nginx","ad"]。
shutdown_timeout:Filebeat 关闭前等待的时间，这个时间用来让已经在发送的数据完成发
第3章开源数据采集技术
29
---
## Page 57
并且在 close_inactive 指定时间到达后关闭它，然后这个文件将会被忽略。
收集，我们必须设置 ignore_older 的时间为大于 clean_inactive 的时间。
文件之前收集的状态信息，则可以使用clean_inactive 参数。
偏移量不会改变；如果稍后文件被更新了，那么 Harvester 将从偏移量的位置继续读取。
过这个参数来忽略一段时间以前的文件。该参数默认设置为0，表示不开启。我们可以通过“2h”
filebeat.prospectors:
值将被这里声明的值覆盖。
段，请将 fields_under_root参数设置为 true。如果在一般配置中声明了一个重复字段，那么它的
中添加的字段将以字典的形式出现在输出信息的 fields 字段中。要将自定义字段存储为顶级字
日志的字段，字段可以是字符串、数组、字典或者任何嵌套的组合。在默认情况下，在 fields
在于Kafka 的同一个 Topic 中时，可以通过 tags 来区分过滤指定的日志。
type:log
fields:
paths:
fields：在输出信息中可以通过 fields 字段添加额外的信息。比如，可以添加一个用来区分
exclude_files:['\.gz$']
ignore_older：过滤在指定时间前被修改的文件。对于日志长时间保存的情况，我们可以通
user_id: admin
-"/var/log/nginx.log"
如果当前正在收集的文件已经处于ignore_older状态，那么Harvester会先完成文件的收集：
在一个文件被 Prospector 忽略之前，它必须被关闭。为了确保一个被忽略的文件不会再被
ignore_older 依赖文件的修改时间，以确定文件是否被忽略。如果要从 Registry 文件中删除
tags：为 Filebeat 收集到的每条数据添加标签，标签可以有一个或者多个。当多种日志存
- "/var/log/nginx.log"
close_inactive：如果一个文件在 close_inactive 指定时间内没有更新，那么 Filebeat 将关闭
对于之前从未出现的文件，偏移量将被设置在文件的末尾；如果一个状态已经存在，那么
〇文件从来没有被收集。
智能运维：从O搭建大规模分布式AIOps系统
文件被收集，但是文件未更新的时间已经超过 ignore_older 指定的时间。
---
## Page 58
删除；否则，因为 clean_inactive 被删除的文件就会被 Prospector 重新探测到，如果一个文件再
必须大于ignore_older加上 scan_frequency的时间，以确保处于被收集状态下的文件状态不会被
同时只有在文件被Filebeat 忽略的情况下文件状态才能被删除。所以设置clean_inactive 的时间
如此。设置 close_timeout 为5 分钟，这样可以使操作系统周期性地关闭并释放资源。该参数默
被阻塞时特别有用，可以使 Filebeat 持续打开文件句柄，即使是对于从磁盘上删除的文件也是
Harvester 来收集数据，并且 close_timeout 对新的 Harvester 开始设置倒计时。这个参数在输出
如果这个文件仍然在更新，Prospector 将在 scan_frequency 指定的时间周期后重新启动一个
想用一段自定义的时间时非常有用，当自定义的时间过去后，close_timeout 将关闭这个文件。
close_timeout 指定的时间过去后，无论读到文件的哪个位置，读取都将停止。这对于当文件只
该参数处于关闭状态。
文件只更新一次，而且不经常更新时很有用，比如当将每条日志写入一个单独的文件时。默认
该参数默认为开启状态。如果关闭该参数，还必须要关闭 clean_removed。
成。如果开启这个参数，文件可能不会被完整地收集，因为它们可能已经从磁盘上被删除了。
早，并且我们没有设置 close_removed，那么Filebeat 将保持文件打开状态以确保文件被收集完
文件只有在close_inactive 指定的时间内没有更新才会被删除，但是如果一个文件被删除得比较
完成文件的收集。
改或者文件被移动到 Prospector 指定路径以外的地方，这个文件将不会被收集，Filebeat 也不会
将一直打开并读取文件内容，因为文件句柄不依赖文件名。开启这个参数后，如果文件名被修
较小，那么文件句柄将很快被关闭。但是这样做也会导致文件更新的内容不能被实时收集。
不同的更新频率，则可以使用多个Prospector来设置不同的值。如果设置close_inactive的值比
率。比如，如果文件几秒钟更新一次，则可以将 close_inactive 设置为1分钟。如果多个文件有
动新的 Harvester 读取更新的内容。我们可以将 close_inactive 的值设置为大于文件的最小更新频
改时间的。如果被关闭的文件发生更新，那么在经过 scan_frequency 指定的时间周期后，将启
认处于关闭状态。
文件句柄。这里的时间周期从文件的最后一行被 Harvester 读取开始计算，而不是基于文件的修
clean_inactive：当开启该参数后，Filebeat 将删除指定不活跃的时间周期后的文件状态，
close_timeout：如果开启该参数，Filebeat 将给予每个 Harvester一个预定义的生存期，当
close_eof：如果开启该参数，当 Filebeat 读取到文件末尾时，文件将很快被关闭。这对于
close_removed：如果一个文件被删除了，Filebeat 将关闭Harvester。在正常情况下，一个
close_renamed：如果文件名被修改了，Filebeat 将关闭文件句柄。在默认情况下，Harvester
第3章开源数据采集技术
---
## Page 59
发生。
于JSON 对象的顶层，那么与Key 关联的值必须是一个字符串；否则，
段，以防止 JSON 解析出错或者配置文件定义的 message_key 字段不能使用。
字段，以防止冲突。
参数，在输出文档中Key 将被复制到顶层。该参数默认处于关闭状态。
而不发送。这个参数对于多行设置尤其有用。其默认值为10MB。
状态，并不断地对文件进行轮询。该参数默认值为10秒。
发送日志，则不要使用非常低的 scan_frequency，可以通过调整 close_inactive 让文件保持打开
参数。
所有的文件状态都已经从 Registry 注册表中删除了。对于这样的情况，建议关闭 clean_removed
除。该参数默认处于开启状态。
Filebeat 将从 Registry 中清除该文件，这意味着被 Harvester 收集完成的文件被重命名后将被删
的情况下。同时对于阻止Linux 系统文件描述符的减少它也非常有用。
件的全部内容。
次出现或者更新，这个文件就会从头开始被 Harvester读取，那么 Filebeat 就会重新发送这个文
只有在每行一个JSON对象时有用。
32
message_key：可选参数，指定行过滤和多行设置的 JSON Key。如果指定的Key 必须位
overwrite_keys：如果开启 keys_under_root参数，那么 JSON 解析对象将覆盖 Filebeat 常用
keys_under_root：默认解析出来的 JSON 信息在输出的“json”关键字下。如果开启这个
json：用于解析JSON 格式的日志消息。由于Filebeat 是逐行处理日志的，所以 JSON 解析
max_bytes：单条日志可以拥有的最大字节数，在 max_bytes 之后的所有字节都将被丢失
harvester_buffer_size：每个 Harvester 获取一个文件时的缓冲区大小，默认值为16384。
scan_frequency：设置 Prospector 在指定路径下检查新文件的频率。如果需要接近实时地
如果一个共享磁盘短暂地消失后又出现了，那么所有的文件都将从头开始重新读取，因为
clean_removed：当开启这个参数后，对于无法从磁盘上找到最后一个已知的名称的情况，
clean_inactive 的设置对于减小 Registry 文件的尺寸非常有用，特别是在每天产生大量文件
智能运维：从O搭建大规模分布式AlOps系统
，过滤和多行聚合就不会
---
## Page 60
"172.16.24.38:9110"，
output.kafka:
般我们会将数据放入Kafka 缓存，这样可以减小对后端的写入压力。
节数，默认值为10240。
来限制打开的文件句柄的数量。其默认值为0，表示不做限制。
的值就会被重置。
行产生，Filebeat 将每秒钟检查该文件一次，接近实时更新。每当文件中出现新行时，backoff
两份数据到Output，同时这两个Prospector将会相互覆盖对方的文件状态。
Prospector 分别配置了符号链接文件和源文件，那么这两个文件都会被收集，Filebeat 将会发送
号链接文件和源文件，那么 Prospector 将处理它发现的第一个文件。但是如果两个不同的
过滤第一次收集的文件中旧的日志行。
参数将不会生效，Harvester将继续从之前的位置开始读取数据。我们可以通过这个配置参数来
个文件在开启这个参数之前已经被 Filebeat 处理了，且文件状态已经保存在注册表中，那么该
是从文件的头部开始读取的。这个参数被应用在还没有被Filebeat处理过的新文件上，如果一
compression: "gzip"
client_id: "log"
partition.round_robin:
username:"admin"
version:0.10.0.0
hosts:["172.16.24.45:9110",
max_message_size：当 Prospector 使用 UDP 类型时，指定通过 UDP 发送消息的最大字
codec.format:
topic:"Topic"
harvester_limit：设置同一个 Prospector 并行开启 Harvester 的最大数量，该设置也可以用
backoff：设置 Filebeat 等待多长时间检查文件的更新，默认值为1秒，这意味着如果有新
symlinks：该配置参数允许 Filebeat 获取符号链接文件。如果一个 Prospector 同时配置了符
string: "%{[message]}"
目前 Filebeat 的 Output 支持 Elasticsearch、Logstash、Kafka、Redis、File、Console 等。
3.配置Output
tail_files：如果开启该参数，那么Filebeat 将从一个新文件的默认位置开始读取数据，而不
reachable_only:false
"172.16.24.50:9110"]
"172.16.24.46:9110",
仅供非商业用途或交流学习使用
"172.16.24.23:9110"
第3章开源数据采集技术
33
---
## Page 61
调大该值，提高单次发送的事件数量，但是不要超过Kafka 中设置的接收最大值。
可以设置成按原数据格式发送。
可能会变成不可用状态，这时可以通过设置 reachable_only 为 true 来改变这种行为，将数据只
发送给可用的分区。
的时间戳，则需要将version设置成0.10.0.0以上版本。
channel_buffer_size：缓存在 Output 管道中的每个 Kafka Broker 消息数量。
bulk_max_size：在单次 Kafka 请求中，批量发送的最大事件数量，默认值为 2048。可以
codec.format：发送数据的格式，默认以 JSON 格式发送。如果日志不是JSON 格式的，则
topic：用来发送数据的Kafka Topic 名称。
version：设置Kafka版本信息，默认值为0.8.2.0版本。如果想对每条数据增加写入Kafka
hosts: Kafka Broker 地址，
required_acks:1
channel_buffer_size: 5000
bulk_max_size: 100000
client_id：设置client_id用于日志、调试或者认证等，默认值为 beats。
reachable_only：默认所有的分区都会接收数据，当一个PartitionLeader不可用时，Output
retry.backoff:1000
retry.backoff：在Kafka Leader 选举期间重试的等待时间。
partition:
password:
username:
智能运维：从O搭建大规模分布式AIOps系统
hash：通过对指定字段做hash 发送到对应的分区中。
round_robin：轮询发送数据到Kafka 的每个分区中。
random：随机发送数据到Kafka 的分区中。
数据写入Kafka 的每个分区的策略，默认为 hash。
如果Kafka 设置了认证功能，则需要提供 password 来访问。
如果Kafka 设置了认证功能，
用来获取Kafka集群元信息。
仅供非商业用途或交流学习使用
，则需要提供username来访问。
---
## Page 62
logging.files:
logging.to_syslog: false
指定日志输出位置，日志系统将把 Filebeat 的输出日志写到 syslog 或者 rotate log 中。
logging.to_files:true
logging.level:info
paths:
中指定不同的过滤内容。通过通配符（*）提取Nginx日志文件的范例如下：
来提取或者过滤指定的内容或者文件。同时，这些参数支持正则匹配，可以在不同的 Prospector
的信息，也可以在原来的日志中添加一些自己想要的内容。
可以设置为0来提高数据发送速度。
置为-1，表示需要等待Kafka所有副本返回确认信息。对于对数据完整性要求不高的情况，
等待接收的副本返回确认信息；设置为O，表示Kafka不返回确认信息，Filebeat持续发送；设
keepfiles:2
rotateeverybytes:52428800
name: filebeat.log
path:/var/logs/
logging.level：定义日志输出级别，有 info、debug、warming 和 error 级别。
它出现在配置文件中的 logging 处，是用来配置 Filebeat 服务的日志输出的。如果没有明确
-"/var/log/nginx.log.*"
logging.to_files：设置日志输出到文件。
Filebeat 在每个 Prospector 中都设置了 include_lines、exclude_lines 或者 exclude_files 参数，
logging.to_syslog：设置为 true，日志将被写入 syslog 中。
5.配置日志
Filebeat 提供了数据过滤和增加数据信息的功能，通过该功能我们可以过滤掉日志中不想要
logging.files.name：日志输出名称。
logging.files.path：日志输出目录。
4．配置过滤和添加数据信息
required_acks：设置是否需要等待Kafka返回数据接收确认信息，默认值为1，表示需要
include_lines: ['nginx']
仅供非商业用途或交流学习使用
第3章开源数据采集技术
2
35
---
## Page 63
path.config: $(path.home)
path.home: /data0/monitor/filebeat
他配置参数。下面通过S{path.home}让 path.config 调用了 path.home 定义的内容。
后，如果在配置文件中用到了同样的配置，则可以用环境变量来引用其定义的内容，赋值给其
path.logs: $(path.home)/logs
path.data: $(path.home)/data
日志文件将被删除。其默认值为7。
将会自动轮转写到一个新的日志文件中。其默认值为10MB。