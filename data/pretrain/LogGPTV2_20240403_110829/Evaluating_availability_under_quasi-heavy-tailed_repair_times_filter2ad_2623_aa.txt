title:Evaluating availability under quasi-heavy-tailed repair times
author:Sei Kato and
Takayuki Osogami
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Evaluating Availability under Quasi-Heavy-tailed Repair Times
Sei Kato and Takayuki Osogami
IBM Research, Tokyo Research Laboratory
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
{seikato, osogami}@jp.ibm.com
Abstract
The time required to recover from failures has a great
impact on the availability of Information Technology (IT)
systems. We define a class of probability distributions
named quasi-heavy-tailed distributions as those distribu(cid:173)
tions whose time series graph of the sample mean shows
intermittent jumps in a given period. We .find that the dis(cid:173)
tribution of repair time is quasi-heavy-tailed for three IT
systems, an in-house system hosted by IBM, a high perfor(cid:173)
mance computing system at the Los Alamos National Labo(cid:173)
ratory, and a distributed memory computer at the National
Energy Research Scientific Computing Center. This lneans
that the mean time to repair estimated by observing inci(cid:173)
dents within a certain period could dramatically change if
we observe incidents successively for another period.
In
other words, the estimated mean time to repair has large
fluctuations over time. As a result, classical metrics based
on the mean time to repair are not optimal for evaluating
the availability ofthese systems. We propose to evaluate the
availability ofIT systems with the T -year return value, es(cid:173)
timated based on extreme value theory. The T -year return
value refers to the value that the repair time exceeds on av(cid:173)
erage once every estimated T years. We find that the T -year
return value is a sound metric ofthe availability ofthe three
IT systems.
1 Introduction
High dependability is a major requirement for informa(cid:173)
tion technology (IT) systems. As IT systems play more
roles in business and government activities, the importance
of their dependability is increasing. To design and build
a highly dependable IT system, it is of fundamental signifi(cid:173)
cance to measure and evaluate the availability metrics of the
IT system, such as the frequency of failures and the average
time to recover from failures.
A significant amount of research has been devoted to
measuring and analyzing the statistics of repair times for IT
systems. The statistics of the repair times for an IT system
was first studied systematically by Long et al. [17]. They
measured the time to repair (TTR) by polling Internet hosts
periodically for three months, and concluded that the re(cid:173)
pair time distribution is far from exponential. Schroeder
and Gibson analyzed the statistical properties of repair time
data collected over nine years at Los Alamos National Lab(cid:173)
oratory (LANL) for high performance computing (HPC)
systems and concluded that the repair times are well mod(cid:173)
eled by a log-normal distribution [21]. Also, based on the
facts that the repair time distribution is non-exponential,
many availability models have been proposed with non(cid:173)
exponential distributions (in particular, with phase type dis(cid:173)
tributions) [22, 4, 15].
While the prior work studied the distribution of repair
times, there was no research that particularly studied long
repair times, which is the tail of the repair time distribution.
Note that a long repair time can have a significant impact on
the availability of an IT system. We are analyzing the tail
of the repair time distribution of three IT systems and are
finding that the repair times have quasi-heavy-tailed distri(cid:173)
butions, which we define as distributions whose time series
graph of the sample mean shows suddenjumps in some pe(cid:173)
riods. As a result, the sample mean time to repair evaluated
by observing incidents within a certain period could change
dramatically if the observation were continued into another
period. There exist robust statistical metrics such as the me(cid:173)
dian, but these metrics do not do a good job at representing
the availability of IT systems. Therefore new robust and in(cid:173)
tuitive metrics are needed for evaluating the availability of
IT systems.
We propose to evaluate the availability of an IT system
with the T -year return value, which can provide an intuitive
understanding of system availability even when the repair
time is quasi-heavy-tailed. The T-year return value is de(cid:173)
fined as the value that the repair time exceeds on average
once every T years. The T -year return value would be help(cid:173)
ful when we evaluate the failure risk of computer systems
or to identify a sub-system whose failure risk is high. We
calculate the T -year return value using the extreme value
1-4244-2398-9/08/$20.00 ©2008 IEEE
442
DSN 2008: Kato & Osogami
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:18:54 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
theory, a theory developed for evaluating the maximum val(cid:173)
ues of rare events.
The contributions of this paper are thus twofold. First,
we study the statistical properties of the repair times of three
IT systems and find that the repair times have quasi-heavy(cid:173)
tailed distributions. The study of repair times provides us
an insight as to how the system availability should be ana(cid:173)
lyzed and evaluated. Second, we propose the T -year return
value as a new metric for evaluating system availability. We
find that the T -year return value allows us to assess the sys(cid:173)
tem availability more robustly than classical metrics such as
mean time to repair (MTTR).
This paper is organized as follows.
In Section 2, we
analyze the statistical properties of the repair time data of
three IT systems and show that the repair times have quasi(cid:173)
heavy-tailed distributions. Section 3 gives a brief review of
extreme value theory. In Section 4, we analyze the T -year
return value of the three IT systems and discuss the results
of the analysis. Section 5 is devoted to concluding remarks.
2 Repair Time Analysis
The prior work studies the statistical properties of the
repair time distribution and shows that the cumulative dis(cid:173)
tribution function (CDF) is "S"-shaped. In this section, we
focus on the statistics of the tail region, since the statistics
of rare events with long repair tilnes can be significant in
estilnating the Inean tilne to repair. We will find that the
distribution of repair times is quasi-heavy-tailed for these
three IT systems. Hence, the mean time to repair has large
fluctuations over tilne and is of lilnited suitability for evalu(cid:173)
ating the availability of IT systems.
2.1 Data and System Configuration
Analyzing the statistical properties of the repair times re(cid:173)
quires incident data for a long period, and particularly when
we are studying the tail of the distribution. Since the inci(cid:173)
dents of IT systems occur rarely, we need to use the incident
data for large IT systems that was systematically collected
for a long period of production use. We use the data of three
large IT systems, whose incident data was stored systemat(cid:173)
ically in an incident management database for many years.
2.1.1 An in-house system
One data source used for analysis is the repair time data of
an in-house system hosted by IBM, and on which enterprise
applications are running. The data include 332 incidents
data occurring from April 1, 2005 to February 27, 2006.
The data is extracted from an incident management
database which stores records on every incident that oc(cid:173)
curred in all systems that include open systems and mission-
critical systems. Each record contains an incident descrip(cid:173)
tion, the time of the occurrence, the time of the recovery, the
recovery process, the business impact level, and so forth. To
analyze the incident data more precisely, we extracted only
those incidents that did affect the system, because, when
the incidents do not affect the system, the repair time tend
to be longer, since the operators do not pay much attention
to those harmless incidents.
The record of an incident is created as follows. When
an incident is detected by a monitoring system, an alert is
displayed on the monitoring console. Alternatively, an op(cid:173)
erator is called by the users when the system is unavailable.
Then the operator creates a new record and inputs the inci(cid:173)
dent description and the start time of the failure. Following
that, the operator asks system engineers to repair the sys(cid:173)
tem or to seek the directions for the repair. The operators
input the time into the database every time when a recovery
step is perforlned. When the systeln finally recovers, the in(cid:173)
cident end time and incident description are input into the
database.
Since the data is created manually by operators, as
pointed out by Schroeder and Gibson [21], the accuracy of
data varies according to the operator. To avoid using inaccu(cid:173)
rate data, we eliminated the incorrect data such as improp(cid:173)
erly recorded data by checking the descriptions for all the
incidents.
2.1.2 LANL HPC System
Another data source we used for analysis was collected on
the LANL HPC system [1]. We use 3,997 incidents that
occurred in one of the LANL HPC systems from May 6,
2002 to September 8, 2005.
The LANL HPC system consists of22 high-performance
computing sub-systems, which are 18 SMP-based sub(cid:173)
systems and four NUMA-based sub-systems. Each sub(cid:173)
system varies in the number of nodes, the number of pro(cid:173)
cessors, and the number of processors per node. More in(cid:173)
formation on the system can be found in [1, 21]. The failure
record contains the start time and end time of the failure, the
system and node affected, and the root causes. The incident
reporting system is much like that of the in-house system.
We analyzed the data on only one sub-system, because
each incident of each sub-system can be taken as a real(cid:173)
ization from an identical distribution though the statistical
properties differ among different sub-systems.
In partic(cid:173)
ular, we used the records on incidents that occurred on a
sub-system whose system ID is 18, which corresponds to
the system ID 7 in [21]. We chose this system for analysis,
because this system has the largest number of observation
samples. This sub-system consists of 1024 nodes, each of
which has four processors and falls into one of four types
according to the size of memory per node, 8, 16, 32, and
1-4244-2398-9/08/$20.00 ©2008 IEEE
443
DSN 2008: Kato & Osogami
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:18:54 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
system
in-house system
LANL HPC system
NERSC Seaborg system
# incidents Min.
0.0
1.0
1.0
332
3997
177
Table 1. Summary of statistics.
1st Qu. Median
16.0
28.0
120.0
49.0
58.0
255.0
Mean
504.8
179.4
534.8
3rd Qu.
201.5
142.0
493.8
Max.
35400.0
25370.0
22230.0
10°
10-1
S 10-2
~
10-3
10-4
10°
101
103
104
105
10°
10- 1
S 10-2
k,"
10-3
10-4
10°
101
102
103
104
105
10°
10- 1
~ 10-2
k,"
10-3
10-4
10°
10 1
102
103
104
105
(a) In-house system
(b) LANL HPC system
(c) NERSC Seaborg system
Figure 1. Log-log plot of the complementary cumulative distribution functions of the repair times (a)
for the in-house system, (b) for the LANL HPC system, and (c) for the NERSC Seaborg system. The
dashed line shows the power law distribution Fc(x) f"'V x-I.
352 GB.
2.2 Statistical Analysis Results
2.1.3 NERSC Seaborg System
2.2.1 Summary of Statistics
The third data source we used is the data collected on the
distributed memory computer system named Seaborg of
the National Energy Research Scientific Conlputing Cen(cid:173)
ter (NERSC) used for running scientific computing applica(cid:173)
tions. We used data from 177 incidents that occurred in the
system from July 2,2001 to December 21,2006.
The NERSC Seaborg system consists of 380 computing
nodes with 16 processors per node, and each processor has
a shared memory pool of between 16 and 64 GB. The com(cid:173)
puting nodes are connected with each other with a high(cid:173)
bandwidth, low-latency switching network. The specifica(cid:173)
tions of the Seaborg system can be found in [2].
The system outage data for the NERSC Seaborg system
was provided by the Petascale Data Storage Institute web(cid:173)
site at the NERSC [3]. The data is taken from the remedy
problem tracking records that were created by the operators
when an outage, scheduled or unscheduled, occurred in the
system. This data contains the date and time when the fail(cid:173)
ure occurred and when it was put back online, the outage
cause (software or hardware), a category (scheduled or un(cid:173)
scheduled), and failure descriptions.
Some statistics for the three IT systems are summarized in
Table 1. While 75% of the incidents were repaired within