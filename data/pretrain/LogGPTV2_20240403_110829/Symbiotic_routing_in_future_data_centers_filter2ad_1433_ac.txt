are commonly used in large-scale data centers. They provide
a simple API, with insert, read, modify and delete function-
ality on key-object pairs. Usually the key is a textual string
which encodes information (e.g. user::img::file.jpg) and
the object is a string or byte array.
In the design of the cache service, the textual key is hashed
and a primary key coordinate derived from the hash, such
that the keys are uniformly distributed within the coordi-
nate space. As in a DHT, the object is cached on the server
responsible for the key coordinate. In order to handle popu-
lar keys the caching service also maintains up to k secondary
 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5CDF (Flows)Modified/Base Ratio 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 0.2 0.4 0.6 0.8 1Links reduction fractionFraction of servers in the group55the lookup is routed to the primary key coordinate. At each
server, before forwarding the lookup, a check is performed to
see if a local copy exists. If so, the current server responds
to the lookup. Figure 3 shows the CDF of lookup versus
path length for both routing protocols. The upper bound in
both cases is 30 hops, which is the network diameter. When
using the base routing service the median path length is 15,
as would be expected, since the probability of encountering
a replica by chance on the path is low. With the custom
routing protocol, instead, the median path length drops to
7. This means that the median lookup latency is reduced
by a factor of 2.
3.4 Aggregation service
The aggregation service is used primarily to support a
MapReduce-like [10, 4] application. MapReduce supports
data parallel computation. Conceptually a data set is split
into M chunks, which are equally distributed across a set
of N servers. During the map phase each server applies a
map function to each local chunk. This generates a set of
key-value pairs per server, where multiple servers can have
entries for the same key. The keys are then mapped to a set
of R servers. During the reduce phase each key-value pair is
sent to the server to which the key is mapped. The server
then combines all the values for the same key, generating a
ﬁnal value for the key. In general, if there are M chunks used
in the map phase, and R servers used in the reduction phase,
then there will be O(R · M ) ﬂows, with M ﬂows going to
each of the R servers. Usually a signiﬁcant number of servers
(N ) are used, often on the order of thousands. Normally, at
least M = N and, for load balancing and failure resilience,
often M >> N .
Most MapReduce jobs have multiple entries for each key
and, hence, if the reduce function is associative and com-
mutative, we can reduce the amount of data transferred by
performing on-path aggregation across multiple ﬂows.
In-
deed, already in MapReduce-like systems, if M >> N , then
on each server the output of the locally executed map func-
tions are combined, hence doing aggregation per server.
The aggregation service exploits a customized routing pro-
tocol that routes packets from a source server to a destina-
tion server coordinate (the reducer) on a deterministic path.
This ensures that ﬂows for the same reducer deterministi-
cally meet along the path and can be aggregated. The path
is made deterministic by ﬁxing each of the axes in turn, so
for example, ﬁrst routing the packet to the correct x plane.
Once the x plane has been reached, then routing to the
correct y plane, then ﬁnally the z plane. This yields a de-
terministic path, that in the absence of failures is also the
shortest. At each hop, if the link is congested, the service
locally buﬀers the packet until it can be transmitted.
Clearly, if all source servers always route to destinations
ﬁxing the x, y, z coordinate in the same order (e.g. x then y
and ﬁnally z), then the set of links that can be used will be
constrained across instances. To ensure that, when R > 1,
all N servers share the load of performing the aggregation we
exploit the fact that there are six diﬀerent orderings possible.
To achieve this we hash the destination coordinate, and use
these to seed which order we ﬁx the axes.
The customized routing protocol in this service exploits
the topology to ensure a good distribution of computational
load over the servers. To deterministically select the aggre-
gation points, they are considered as key coordinates. If the
Figure 3: Hops to a cache hit with one primary
replica and eight secondary replicas.
replicas of the object. The secondary replicas are generated
on demand per key, so the total number of replicas is a func-
tion of the popularity of the object. In order to induce better
locality, the k secondary replicas are uniformly distributed
within the coordinate space. The cache service achieves this
by using a function, parameterized on the primary key coor-
dinate, the number of secondary replicas (k) and coordinate
space size. The function returns k key coordinates repre-
senting the coordinates of each secondary replica.
When a read operation is performed on a key-object pair,
the service calculates the primary key coordinate, and the
k secondary replica key coordinates. These are ranked by
rectilinear distance from the local key coordinate and the set
of closest replicas are selected. If there are multiple closest
replicas, and one is the primary replica, then it is selected,
else one is randomly selected. The cache service then routes
the request via the selected replica key coordinate. If the
server responsible for the key coordinate has a copy of the
key, then it responds with the associated object and the
request is not routed to the primary key coordinate. Oth-
erwise, the server forwards the request to the primary key
coordinate. If an entry for the key being looked up is found,
then this is returned to the original requester via the replica
key coordinate. This populates the secondary replica. A
cache miss may increase the path length from source to the
primary replica, and in the worst case the path would be
stretched by a factor of 3. However, the maximum path
length, even in case of cache miss, from a source to the pri-
mary replica is still bounded by the network diameter.
The caching service uses the default routing service to
route between the source and secondary replica, and the sec-
ondary replica and primary replica. It also relies on the rout-
ing service to remap consistently the key coordinate space on
server failure. This is similar to using loose source routing,
except the intermediate server address is a key coordinate
not a server coordinate, and it does not need to be stored in
the packet, as every server on route can determine it using
information already in the request packet.
We ran an experiment where the system was populated
with 8,000,000 key-object pairs evenly distributed among
the servers in CamCube. We performed 800,000 key lookups
based on a Zipf distribution (α = 1.5). Each lookup was
performed by a server picked uniformly at random (average
of 100 lookups per server). We use k = 8 secondary replicas,
created on demand as previously described. We compare the
performance using the base routing service and the modiﬁed
routing protocol described. In the routing service version,
 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20 25 30CDF (Lookups)Path lengthCache ServiceRouting Service56Total number of links used
Percentage of links used
Aggregate packets sent
Median packets (per link)
90th Percentile (per link)
99th Percentile (per link)
Maximum (per link)
Routing
service
Aggregation
service
19,646
41%
None
7,999
17%
120,000
120,000
3
7
51
1,494
5
9
160
4,000
Full
7,999
17%
7,999
1
1
1
1
Table 1: Link statistics comparing the aggregation
service and routing service.
next aggregation key coordinate cannot be reached using
a greedy protocol, the aggregation service uses the routing
service to route to the key coordinate. This means that all
the packets belonging to a ﬂow are routed through the same
server regardless of failures. This provides determinism and
is exploited to achieve fault tolerance.
Next, we compare using the customized routing protocol
against the routing service. In the routing service case, we
route all the packets to a single server that performs the
aggregation. Current MapReduce-like systems cannot per-
form on-path aggregation. It is hard to perform aggregation
using the routing service because it exploits multipath at
the packet level. This means diﬀerent packets belonging
to the same ﬂow traverse diﬀerent paths; no single server
sees all packets belonging the ﬂow except for the source and
destination.
In the experiment each server ran a process
that generated a single packet containing a key-value pair
and sent these packets to a destination server. In one case,
(Full), a single key was used across all servers. Hence, two or
more packets can always be aggregated into a single packet,
and this represents the best case. For the worst case, we
also ran an experiment where each server used a unique key
in the key-value packet (None). This means that no aggre-
gation across packets can be performed and, hence there is
no reduction in traﬃc.
Table 1 reports the number of links and the link stress
statistics for using the routing service and the aggregation
service with full and no aggregation. Obviously, for the rout-
ing service, the statistics are the same whether aggregation is
possible or not so we show only one column. Table 1 demon-
strates that the aggregation service uses less links in total
compared to the routing service; the aggregation service uses
deterministic paths and therefore, eﬀectively, restricts the
set of paths over which the packets can be routed. In the Full
aggregation case, the link load is uniform at one. The max-
imum number of packets that any server has to aggregate is
seven, and this is at the destination server, assuming that
the destination also generates a key-value packet. Clearly,
performing aggregation provides considerable beneﬁt. In the
no aggregation case both have the same total packet count,
as would be expected, but clearly as the number of links
used in the aggregation service is lower, the link stress on
each of the links is higher. The median and 90th percentile
shows a small increase, but it is clear from the max and 99th
percentile that a subset of the links sustains a considerably
higher load. In the aggregation service, two of the six in-
coming links to the destination server sustain the highest
and second highest load. The customized routing protocol
is designed to provide particular properties, assuming that
aggregation is possible. When aggregation is not possible,
Figure 4: Total packet overhead per service normal-
ized by the base routing service.
the customized protocol arguably performs worse than the
routing service.
This highlights the need to allow services to select the
right routing protocol; one size does not ﬁt all!
3.5 Network-level impact
In the previous part of the section, we have shown that
implementing customized routing protocols achieves better
service-level properties. Do these customized routing pro-
tocols have a negative impact on the network performance?
In this section, we consider if the custom protocols tend to
induce higher link stress or have very skewed traﬃc patterns.
To evaluate this, we used the same experiments as in
the previous section, and measured the aggregate link stress
across all links for the experiments with the customized rout-
ing protocols and routing service version. Figure 4 shows,
for each service, the ratio between the total packets sent us-
ing routing service and the customized routing protocols. A
value less than one means that the customized routing proto-
col generated less network overhead compared to the routing
service version. It should be noted that for the VM distri-
bution service we compare two points, when 10% (VM 0.1)
and 50% (VM 0.5) of the servers join the group. Further,
for the VM distribution service we are measuring the over-
head of tree building, but when distributing a VM through
the tree, the same packet saving will hold. The results in
Figure 4 show that in all cases the overhead induced by the
customized routing protocol is lower, and is therefore, ben-
eﬁcial.
It might seem counter-intuitive that the TCP/IP
service achieves a higher throughput with a lower packet
overhead. However, in the base routing protocol the source
has to retransmit packets that are dropped due to conges-
tion induced by overlapping paths, which increases packet
overhead. In contrast, the custom routing protocol used by
the TCP/IP service ensures that packets are routed on dis-
joint paths and, hence, it does not incur congestion and no
packets are retransmitted.
In order to understand if we were inducing signiﬁcantly
higher link stress on each link, we looked at the distribution
of link stress. We ranked the links in ascending order, exam-
ining the 99th percentile and maximum. In both measures,
the aggregation service in the no aggregation case performed
signiﬁcantly worse than the routing service, over 2.5 times
higher. However, for the full aggregation case, and for all
other services, the link stress was equal or lower.
All the results so far have ignored any maintenance over-
head induced by the customized routing protocols. The
 0 0.2 0.4 0.6 0.8 1CacheVM 0.1VM 0.5No AggFull AggTCPModified/Base packet ratio57vices just use a small set of intermediate coordinates through
which packets must traverse. The routing service routes be-
tween the intermediate coordinates. This provides function-
ality similar to loose source routing.
Fault-tolerance Many services use greedy-based proto-
cols, which exploit the coordinate space. A key challenge
for greedy protocols is to make them resilient to failures
which, in corner cases, can cause voids in the static coordi-
nate space used in CamCube, as described in Section 2.1.2.
As voids are rare the services do not implement their own
mechanism to handle them, but rely on the routing service
which is able to route around any voids. This avoids adding
complexity to the services. This is also beneﬁcial in terms of
network load, as each service does not need to generate any
maintenance traﬃc. The services leverage the state main-
tained by the existing routing service.
Multipath The 3D torus topology provides multiple paths
between servers. Some services exploit this path diversity to
increase the end-to-end throughput and to avoid congested
links. Other services require all packets in the same ﬂow be
routed through the same set of servers, for example in the
aggregation service.
Packet buﬀers Services have diﬀerent requirements for
packet buﬀer sizes. Small buﬀers generate packet loss in the
presence of congestion whereas large buﬀers incur queuing
delay which increases end-to-end latency. All services use
end-to-end reliability mechanisms but some also use per-hop
mechanisms. These services tend to buﬀer packets for long
periods, depending on the service from seconds to hours.