photoacoustic effect on a diaphragm. Thus, by selectively
illuminating different microphone components using a laser,
we attempted to precisely show the physical root cause.
Figure 7: (Left) Laser spot on the ADMP401’s ASIC. (Right)
the ASIC covered with opaque epoxy to block laser light.
We achieve this by opening the metal package of the Ana-
log Devices ADMP401 microphone and injecting analog sig-
nals into its diaphragm and ASIC components using a focused
laser beam (see Figure 2). After using a microscope to focus
a 200 µm laser spot on the microphone’s components, we
observed the strongest signal while aiming the laser on the
microphone’s ASIC, as shown in Figure 7(left). This direct
injection is very efﬁcient, where less than 0.1 mW of laser
power was sufﬁcient to saturate the microphone. We take this
as an indication that laser light can cause photoelectric trans-
duction inside the microphone’s ASIC, since in our attack
the light is reﬂected onto the ASIC from the microphone’s
metal package. After covering the microphone’s ASIC with
opaque epoxy (Figure 7(right)), aiming the laser on the ASIC
no longer generates any signal. However, even after the treat-
ment, the microphone still generates a signal when the laser
spot is aimed at the microphone’s diaphragm.
Based on these results, we conclude that in addition to
the photoelectric effect observed on the microphone’s ASIC,
there is another light-induced transduction within the MEMS
diaphragm. Since the diaphragm is a simple capacitor, we
hypothesize that this effect is due to the physical movements
of the microphone’s diaphragm (i.e., light-induced mechan-
ical vibration). Next, while the above is not a comprehen-
sive survey on different MEMS microphones, this analysis
does provide an overall understanding of the root cause of
the physicals effects observed in this paper. Finally, for the
experiments conducted in the remainder of this paper, we
have aimed the laser through the microphone’s acoustic port.
We hypothesize that our attacks illuminated both the micro-
phone’s ASIC and diaphragm, resulting in some combination
of the photoacoustic and photoelectric effects.
USENIX Association
29th USENIX Security Symposium    2637
050100150200250300Diode Current (mA)0100200300Light Power (mW)450nm I-L Curve (Ipp=0)050100150Diode Current Ipp (mA)00.20.40.60.8Microphone Vpp (V)450nm Microphone Response (f = 1 kHz, IDC = 200 mA)101102103104105Frequency (Hz)00.511.52Microphone Vpp (V)450nm Frequency Response (IDC = 200 mA, Ipp = 150 mA)050100150200250300Diode current (mA)0100200300Light Power (mW)638nm I-L Curve (Ipp=0)050100150Diode current Ipp (mA)00.20.40.60.8Microphone Vpp (V)638nm Microphone Response (f = 1 kHz, IDC = 150 mA)101102103104105Frequency (Hz)00.51Microphone Vpp (V)638nm Frequency Response (IDC = 150 mA, Ipp = 75 mA)ASIC covered by black epoxyASICLaser spot5 Attacking Voice-Controllable Systems
In this section we evaluate our attack on seventeen popular
VC systems. We aim to ﬁnd out the minimum laser power
required by the attacker to gain control over the VC system
under ideal conditions as well as the maximum distance that
such control can be obtained under more realistic conditions.
Target Selection. We benchmark our attack against several
consumer devices which have voice control capabilities (see
Table 1). We aim to test the most popular voice assistants –
namely Alexa, Siri, Portal, and Google Assistant. While we
do not claim that our list is exhaustive, we do argue that it does
provide some intuition about the vulnerability of popular VC
systems to laser-based voice injection attacks. Next, to explore
how different hardware variations (rather than algorithmic
variations) affect our attack performance, we benchmark our
attack on multiple devices running the same voice recognition
backend: Alexa, Siri, Portal and Google Assistant, as sum-
marized in Table 1. For some devices, we examine different
generations to explore the differences on attack performance
for various hardware models. Finally, we also considered
third-party devices with built-in speech recognition, such as
the EcoBee thermostat.
5.1 Exploring Laser Power Requirements
In this section we aim to characterize the minimum laser
power required by the attacker under ideal conditions to con-
trol a voice-activated system. Before describing our experi-
mental setup, we discuss our selection of voice commands
and experiment success criteria.
Command Selection. We have selected four different voice
commands that represent common operations performed by
voice-controllable systems.
• What Time Is It? We use this command as a baseline of
our experiments, as it only requires the device to correctly
recognize the command and access the Internet to recover
the current time.
• Set the Volume to Zero.
Here, we demonstrate the
attacker’s ability to control the output of a VC system. We
expect this to be the ﬁrst voice command issued by the
attacker, in an attempt to avoid attracting attention from the
target’s legitimate owner.
• Purchase a Laser Pointer. With this command we show
how an attacker can potentially place order for various prod-
ucts on behalf (and at the expense) of users. The attacker
can subsequently wait for delivery near the target’s residents
and collect the purchased item.
• Open the Garage Door.
Finally, we show how an at-
tacker can interact with additional systems which have been
linked by the user to the targeted VC system. While the
garage door opener is one such example with clear security
implications, we discuss other examples in Section 6.
Command Generation. We have generated audio record-
ings of all four of the above commands using a common audio
recording system (e.g., Audacity). Each command recording
was subsequently appended to a recording of the wake word
corresponding to the device being tested (e.g., Alexa, Hey
Siri, Hey Portal, or OK, Google) and normalized to adjust
the overall volume of the recordings to a constant value. We
obtained a resulting corpus of 16 complete commands. Next,
for each device, we injected four of the complete commands
(those beginning with the device-appropriate wake word) into
the device’s microphone using the setup described below and
observed the device’s response. Finally, we note that no ma-
chine learning algorithms or any device-speciﬁc calibration
were done during the generation of the audio ﬁles contain-
ing the voice commands. These recorded voice commands
were subsequently used in the experiments described below
without further modiﬁcation for all the tested devices.
Verifying Successful Injection. We consider a command
injection successful in case the device indicates the correct
interpretation of the command. We note that some commands
require other devices to be attached to the victim account
in order to properly execute, resulting in an error otherwise
(e.g., a garage door opener for a command opening the garage
door). As in this section we only test feasibility of command
injection (as opposed to end-to-end attacks of Section 6), we
consider an injection attempt successful in case the device
properly recognized all the command’s words. For devices
with screens (e.g., phones and screen-enabled speakers), we
veriﬁed that the device displayed a correct transcription of the
light-injected command. Finally, for screen-less devices (e.g.,
smart speakers), we examined the command log of the account
associated with the device for the command transcription.
Attack Success Criteria.
For a given power budget, dis-
tance, and command, we consider the injection successful
when the device correctly recognized the command during
three consecutive attempts. The injection attempt is consid-
ered to be a failure otherwise (e.g., the device only recognizes
the wake-up word but not the entire command). We take
this as an indication that the power budget is sufﬁcient for
achieving a near-perfect consecutive command recognition
assuming suitable aiming and focusing.
Next, we consider an attack successful for a given power
budget and distance when all four commands are success-
fully injected to the device in three consecutive injection
attempts. The attack is considered a failure in any other case
(e.g., achieving two out of three correct command recogni-
tions). Like in the individual command case, we take this as
an indication that the considered power budget and distance is
sufﬁcient for a successful command injection. As such, the re-
sults in this section should be seen as a conservative estimate
of what an attacker can achieve for each device assuming
good environmental conditions (e.g., quiet surroundings and
suitable aiming).
Voice Customization and Security Settings.
For the ex-
2638    29th USENIX Security Symposium
USENIX Association
periments conducted in this section, we left all the device’s
settings in their default conﬁguration. In embedded Alexa
and Google VC systems (e.g., smart speakers, cameras, etc.)
voice customization is off by default, meaning that the device
will operate on commands spoken by any voice. Meanwhile,
for phones and tablets, we left the voice identiﬁcation in its
default activated setting. For such devices, to ascertain the
minimum required power for a successful attack, we person-
alized the device’s voice recognition system with the human
voice used to generate the command recordings described
above. We then subsequently inject the audio recording of the
commands using the same voice without any other customiza-
tion. Finally, in Section 5.4, we discuss bypassing various
voice matching mechanisms.
Experimental Setup. We use the same blue laser and
Thorlabs laser driver as in Section 4.1, aiming the laser beam
at microphone ports of the devices listed in Table 1 from a
distance of about 30 cm. As in Section 4.1, we did not need
any custom circuitry or algorithms, using the modulation port
of the laser driver for converting audio to laser-current. Next,
to control the surrounding environment, the entire setup was
placed in a metal enclosure, with opaque bottom and sides and
with a dark red semi-transparent acrylic top plate, designed to
block blue light. See Figure 8. As the goal of the experiments
described in this section is to ascertain the minimum required
power for a successful attack on each device, we have used
a pair of electrically controlled scanning mirrors (40 Kbps
high-speed laser scanning system for laser shows) to precisely
place the laser beam in the center of the device’s microphone
port. Before each experiment we manually focused the laser
so that the laser spot size hitting the microphone is minimal.
For aiming at devices whose microphone port is covered
with cloth (e.g., Google Home Mini shown in Figure 9), the
position of the microphone ports can be determined using an
easily-observable reference point such as the device’s wire
connector or LED array. Finally, we note that the distance
between the microphone and the reference point is easily
obtainable by the attacker either by exploring his own device,
or by referring to online teardown videos [35].
Experimental Results. The ﬁfth column of Table 1 presents
a summary of our results. While the power required from
the attacker varies from 0.5 mW (Google Home) to 60 mW
(Galaxy S9), all the devices are susceptible to laser-based
command injection, even when the device’s microphone port
(e.g., Google Home Mini) is covered with fabric and / or foam.
Finally, for Facebook’s Portal Mini device which supports
both Amazon’s and Facebook’s voice assistants, we note the
×6 increase in minimum power between “Hey Portal" and
“Alexa" wakeup words. In addition, Portal also consistently
failed to identify the word “laser” used in the last command,
forcing us to disregard it. As both experiments were done
using the same setup and with the laser aimed at the same
microphone, we attribute these to algorithmic differences be-
tween Amazon’s and Facebook’s voice recognition backends.
Figure 8: Exploring minimum laser power requirements: the
laser and target are arranged inside an enclosure. The laser
spot is aimed at the target acoustic port using electrically
controllable scanning mirrors inside the enclosure. The enclo-
sure’s top red acrylic cover was removed for visual clarity.
Figure 9: Google Home Mini. Notice the cloth-covered mi-
crophone ports.
5.2 Exploring Attack Range
The experiments done in Section 5.1 are performed under
ideal conditions, at close range and with the aid of electronic
aiming mirrors. Thus, in this section we report on attack
results under more realistic distance and aiming conditions.
Experimental Setup.
From the experiments performed
in Section 5.1 we note that about 60 mW of laser power is
sufﬁcient for successfully attacking all of our tested devices
(at least under ideal conditions). Thus, in this section we
benchmark the range of our attack using two power budgets.
• 60 mW High-Power Laser. As explained in Section 2.6,
we frequently encountered laser pointers whose measured
power output was above 60 mW, which greatly exceeds
legal 5 mW restrictions. Thus, emulating an attacker which
does not follow laser safety protocols for consumer devices,
we benchmark our attack using a 60 mW laser, which is
sufﬁcient for successfully attacking all of our tested devices
in the previous experiment.
• 5 mW Low-Power Laser. Next, we also explore the max-
imum range of a more restricted attacker, which is limited
to the maximum amount of power allowed in the U.S. for
consumer laser pointers, namely 5 mW.
Laser Focusing and Aiming.
For large attack distances
(tens of meters), laser focusing requires a large diameter lens
USENIX Association
29th USENIX Security Symposium    2639
Scanning mirrorson rotation stageTargetMirrordriverLaserdiodeLaser beamMicrophonesLED arrayTable 1: Tested devices with minimum activation power and maximum distance achievable at the given power of 5 mW and 60
mW. A 110 m long hallway was used for 5 mW tests while a 50 m long hallway was used for tests at 60 mW.
Device
Backend
Category
Authen-
tication
Minimum
Power [mW]*
Max Distance
at 60 mW [m]**
Max Distance
at 5 mW [m]***
Google Home
Google Home Mini
Google Nest Cam IQ
Echo Plus 1st Generation
Echo Plus 2nd Generation
Echo
Echo Dot 2nd Generation
Echo Dot 3rd Generation
Echo Show 5
Echo Spot
Facebook Portal Mini (Front Mic)
Facebook Portal Mini (Front Mic)§
Fire Cube TV
EcoBee 4
iPhone XR (Front Mic)
iPad 6th Gen
Samsung Galaxy S9 (Bottom Mic) Google Assistant
Google Pixel 2 (Bottom Mic)
Google Assistant
*at 30 cm distance, **Data limited to a 50 m long corridor, ***Data limited to a 110 m long corridor, §Data generated using only the ﬁrst 3 commands.
Speaker
Speaker
Camera
Speaker
Speaker
Speaker
Speaker
Speaker
Speaker
Speaker
Speaker
Speaker
Streamer
Thermostat
110+
—
—
110+
50
—
—
—
—
—
40
—
—
70
—