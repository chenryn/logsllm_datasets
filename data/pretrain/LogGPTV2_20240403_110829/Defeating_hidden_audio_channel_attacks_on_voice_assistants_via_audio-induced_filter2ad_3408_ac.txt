data (illustrated with the command “show face-
book.com”).
original signal are overlapped after sampling (e.g., by motion
sensors). In particular, the relationship between the vibration
data frequency falias and its original audio frequency f can
be expressed as
falias = | f − N fs|, N ∈ Z,
(1)
where fs is the sampling rate of the motion sensor. This
equation indicates that the vibration responses of an au-
dio are a nonlinear transformation of the audio. Moreover,
such transformation from audio to vibration is irreversible.
To illustrate the signal aliasing in the vibration responses,
we conduct an experiment by using a loudspeaker to play
a dual-frequency sound (i.e., 420Hz and 320Hz) for three
times and an on-board motion sensor with a sampling rate of
100Hz to pick up the sound vibrations. As shown in Figure 3,
the two frequency components of the audio only generate
a single frequency response (around 20Hz) on the motion
sensors, which complies with the nonlinear relationship of
Equation 1.
4.3 Distinct Vibration Domain
It is important to note that knowing the nonlinear relation-
ship (as Equation 1) between the vibration readings and the
Figure 5: The experimental setup for surface vibration
measurement using laser vibrometer.
original sound does not mean that one could forge or simu-
late the vibration responses by down-sampling the audio for
generating an aliased signal. This is because besides signal
aliasing, the accelerometer also generates distinct responses
to the sound in the form of unique amplitudes and frequen-
cies. As shown in Figure 4, the microphone data of the voice
command “show facebook.com” is down-sampled to com-
pare with the accelerometer data under the same sampling
rate. It is clear that the descriptions of the voice command
in the two domains are distinct. In particular, for the same
frequency point, the two sensing modalities show very dif-
ferent amplitudes. Thus, we believe the accelerometer, as a
different sensing modality, brings in new descriptions about
the voice command, which could work with the traditional
audio domain to describe the voice command’s signatures in
two domains. Moreover, it would become much harder for
an adversary to forge the signatures of the voice command
as they would need to cheat two different domains simulta-
neously. This subject is further explained in more detail in
Section 5.2.
4.4 Observing Speech Vibrations
To further confirm the presence of the speech vibrations
caused by the VCS device’s speaker and their differences
between the hidden voice commands and normal commands,
we use a powerful laser vibrometer to capture the speech
vibrations in high frequency. In particular, we use a PDV-100
Portable Digital Laser Vibrometer [1] to measure the speech
vibrations generated in the body of a smartphone (Samsung
Galaxy S6), as shown in Figure 5, during the playback of
the voice commands by the smartphone’s loudspeakers. The
PDV-100 vibrometer points to the screen of the phone and
can measure the vibration’s frequencies up to 22 kHz with a
high vibrational velocity resolution of 0.02µm/s. In Figure 6,
we compare the power spectrum of the vibrations generated
in the smartphone’s surface, when a normal command and a
corresponding hidden voice command are played through its
00.51Time (s)20406080100120Frequency (Hz)0.511.5200.51Time (s)20406080100120Frequency (Hz)0.511.52(a) Normal command
“Ok, Google”
(b) Hidden voice command
“Ok, Google”
(c) Normal command
“What’s my schedule today?”
(d) Hidden voice command
“What’s my schedule today?”
Figure 6: The power spectrum of the surface vibrations of normal commands and the corresponding hidden voice
commands, as measured by a laser vibrometer pointed at the smartphone.
loudspeakers. The high sampling rate of the laser vibrometer
allows us to identify the differences between the features
of the normal commands and the hidden voice commands
that will later help in distinguishing them. When we com-
pare the power spectrum of Figure 6(a) and (b), we notice
that the human voice spectrum’s shape is not preserved in
the corresponding hidden voice command even though it
is successful at preserving individual word boundaries and
approximating the frequency distribution. Similar behavior
is observed for a different voice command in Figure 6(c) and
(d). This observation supports that hidden voice commands
fail to carry over the speech features in the vibration domain,
when they attempt to mimic the speech features of a normal
command in the audio domain.
5 SYSTEM DESIGN
5.1 Vibration Data Calibration
As introduced in Section 4, the accelerometer is capable of
capturing a large portion of human voice frequencies, but
they show the responses in a low frequency range due to
their low sampling rate (e.g., 200Hz). In order to extract more
precise vibration responses to the voice commands using the
limited motion sensor information, we need to remove the
vibration noises that come from the surrounding environ-
ment, such as the mechanical vibrations. In particular, we
apply a high-pass filter with a cutoff frequency of 20Hz to
process the accelerometer data, which removes most of the
background mechanical vibrations. Moreover, before extract-
ing the unique vibration features, we need to identify the
precise segment containing the voice command. We apply a
sliding window-based variance analysis method to find the
starting point and ending point of the voice command to pre-
cisely segment it from the accelerometer reading. We further
normalize the sound amplitudes to remove the differences
caused by various sound volumes. We next extract unique
vibration features based on the calibrated vibration data to
analyze the hidden voice commands and normal commands
in vibration domain.
5.2 Extracting Unique Vibration Features
from Voice Commands
In this work, we derive the statistical features of captured vi-
brations in both the time and frequency domains and extract
the acoustic features such as MFCCs and chroma vectors.
Statistical Features in Time and Frequency Do-
mains. As the accelerometer data is usually used for an-
alyzing people’s various activities such as walking, running
and sitting, we start by examining the activity related fea-
tures as the candidate features, which have been shown to
be highly correlated with the human behaviors [24]. More-
over, the accelerometer records the vibrations in three axes,
which further provides the spatial information to describe the
received acoustic signals by considering the vibration direc-
tions. We thus derive the features for each axis respectively.
In particular, in the time domain, we derive the maximum,
minimum, variance, standard deviation, range, skewness, first
quantile, second quantile, third quantile and kurtosis. More-
over, we derive absolute area (i.e., the area under the absolute
values of the accelerometer readings), mean crossing rate (i.e.,
the ratio of the number of times the signal crosses the mean
value over the command segment length), signal dispersion
(i.e., distance between the third quantile and the first quan-
tile) and absolute area sum and signal magnitude sum over
the three axes. In the frequency domain, we calculate the
energy, entropy and the ratio of the highest magnitude FFT
coefficient over the FFT coefficient sum.
Deriving Acoustic Features from Motion Sensor
Data. Besides extracting the statistical features, we also
derive the acoustic features from the accelerometer data. In
particular, we derive the Mel-Frequency Cepstral Coefficient
0.20.40.60.8Time (s)01000200030004000Frequency (Hz)-120-100-80-600.20.40.60.8Time (s)01000200030004000Frequency (Hz)-120-100-80-600.20.40.60.811.2Time (s)01000200030004000Frequency (Hz)-120-100-80-600.20.40.60.811.2Time (s)01000200030004000Frequency (Hz)-140-120-100-80-60Figure 7: Nonlinear relationship between audio fea-
tures and vibration features. (Illustrated with the com-
mand “Show facebook.com”)
(MFCC), which is widely used to describe the short-term
power spectrum of acoustic signals and can reflect both the
linear and nonlinear properties of the speech signal’s dy-
namic features. While the MFCCs are able to distinguish
people’s voice differences in the audio domain, we find that
they also capture the vibration characteristics. Moreover, we
calculate the chroma vector, which describes twelve different
pitch classes, and the spectral centroid and spectral entropy.
Unique and Hard to Forge Vibration Features. As in-
troduced in Section 4.2, the vibration signals are nonlinear
and are aliased responses of the audio signal. Such nonlin-
earity means that similar audio domain features may result
in very different vibration features. Figure 7 illustrates three
normalized MFCC coefficients in the audio and the vibration
domain, when the normal commands and the hidden voice
commands of “Show facebook.com” are replayed 10 times
respectively. We observe that while the audio features could
not differentiate the two types of voice commands, they are
easily distinguished in the vibration domain in two separate
clusters. Thus, audio feature modification does not aid in
replicating the vibration features as the vibration features
reflect an additional signature of the voice commands. To
further illustrate how the vibration features can describe
the physical characteristics of hidden voice commands and
normal commands, we asked three participants (i.e., two
male and one female) to speak multiple commands, and their
audio clips were utilized to generate the hidden voice com-
mands using the method introduced in Section 3.1. Figure 8
illustrates an example of using three features to differentiate
the two types of voice commands. We find that the features
“kurtosis” on the Z axis, “entropy” derived from the accelera-
tions on the Z axis and “mean crossing rate” on the Y axis can
distinguish the two types of voice commands in two well sep-
arated clusters. Moreover, the normal commands of different
Figure 8: Illustration of the vibration features to dis-
tinguish hidden voice commands and normal com-
mands.
human speakers exhibit similar vibration features in a much
smaller cluster indicating that a voice command, if mapped
away from this cluster, may not be a normal command.
5.3 Feature Selection Based on Statistical
Analysis
Based on our experiments, we observe that not all of the
extracted statistical features and acoustic features are unique
enough to distinguish the hidden voice commands and nor-
mal commands. We resort to statistical analysis to identify a
subset of features from the above candidate features, which
are discriminative for the different types of voice commands
and maintain relatively independent to various command
contexts and people’s different voices. In particular, we first
normalize the values of the features between 0 and 1 and
then calculate the score s of a feature based on Equation 2.
√(Fhid(i)− ¯Fhid)2
¯Fhid − ¯Fhum
√(Fhum(j)− ¯Fhum)2
n
,
n
(2)
),
s =
max(
where Fhid(i) and Fhum(j) represent the feature value of each
hidden voice command sample i and normal command sam-
ple j and ¯F is the mean of the feature value. The calculated
score reflects how well the two types of voice commands are
separated regarding their distribution. By using a small set
of the hidden voice command and normal command sam-
ples, we calculated the score for all of the candidate features
and selected the more distinguishable features based on a
threshold. Figure 10 illustrates the distributions of the se-
lected features by our method when using a Samsung Note
4’s built-in speaker to play and its motion sensor to record
the voice commands, including different command words
Audio DomainVibration Domain humanVibration Domain hvc0.0080.0140.0120.014Mean Crossing Rate Y0.0160.01855kurtosis Z2Entropy Z50045HVC Speaker 1HVC Speaker 2HVC Speaker 3Human Speaker 1Human Speaker 2Human Speaker 3Table 1: List of voice commands being used.
1 What’s my current location?
2 Open Bank of America.
3
Turn on airplane mode.
4
Play country music.
5 What’s my schedule today?
Call 911.
Open youtube.com.
Show facebook.com.
Open the door please.
6
7
8
9
10 Ok Google.
During the training phase, the proposed methods map the
voice command samples (including both hidden voice com-
mands and normal commands) into two clusters in the multi-
dimensional feature space. Only the cluster centroid of the
normal commands is calculated. We then compute the Eu-
clidean distance of the training voice command samples to
the cluster centroid. As shown in Figure 9, the normal com-
mand samples show small Euclidean distances to the normal
command cluster centroid, while the hidden voice commands
are far from this cluster and thus can be distinguished. A
threshold is determined based on these Euclidean distances
calculated from the training voice commands.
During the testing phase, an input voice command’s vibra-
tion features are utilized to compute the Euclidean distance
to the normal command cluster centroid and is rejected if
the distance is larger than the threshold. We believe that all
human voices show similar physical characteristics in the
vibration domain, while the hidden voice commands exhibit
different feature patterns and hence can be distinguished. As
we will show in Section 6, our unsupervised learning-based
method does not require much training effort and is ade-
quate to differentiate between the hidden voice commands
generated with unseen command words and voices.
6 PERFORMANCE EVALUATION
6.1 Experimental Setup
6.1.1 Devices and Experimental Setup. We evaluate our sys-
tem in both frontend and backend setups. In the frontend
setup, we conduct our experiments using four different smart-
phone models in a university office with ambient noise (e.g.,
heating, ventilation and air conditioning noise). Particularly,
Samsung Note 4, LG G3, Motorola Nexus 6 and Samsung
Galaxy S6 are used due to their different physical body de-
signs, which may result in slightly different structure-borne
propagations. Moreover, the sensor specs of the four devices
are different. LG G3’s motion sensor works at 120Hz while
the other three work at 250Hz. Figure 11 (a) shows the fron-
tend setup with a smartphone placed on a table. The voice
commands are played by the smartphone’s built-in speaker
in maximum volume and recorded by its own motion sen-
sor. We also conduct experiments in the frontend setup with
the smartphone held by hand or placed on the soft surface,
which are presented in Section 6.2.4.
Figure 9: Euclidean-distances of the voice command
samples to the human speaker sound cluster centroid
based on vibration features.
and multiple people’s voices. We find that all of the identified
vibration features such as mean crossing rate, entropy and
MFCCs show very different distributions of the two types of
voice commands. Thus, we leverage the selected vibration