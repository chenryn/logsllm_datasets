gathered from the results of querying the Google image search
engine using the keyword “female with casual wear”, and
(3) 20k images of the athletes and 15k images containing
single body part or apparels. We use (2) since most females
in the Celeb-1M dataset wear very revealing or shiny clothes,
which do not provide suﬃcient information about the regular
female clothing. Also, dataset (3) is found to be necessary for
diﬀerentiating sexual behaviors from sport activities, where
athletes also wear tiny clothes and expose large areas of their
skin, which are close to porn patterns.
The porn picture dataset,
the positive samples for our
model, includes 85k images known to contain sexually explicit
content. They are scrapped from two pornographic websites
(t66y.com and vulvapornpics.com).
• Groundtruth set. The groundtruth dataset is used to evaluate
our methodology (see Section IV-B), which includes 250
APPIs and 250 non-APPIs randomly sampled from the social
media image set. All these images have been manually labeled.
System implementation. We implemented Mal`ena with 2,400
lines of Python code. Three Python libraries (Pillow [23],
skimage [24] and OpenCV [22]) are used for the afore-
mentioned image processing tasks. Also, we build our deep
learning models over Tensorﬂow using two deep neural net-
work architectures and two pre-trained models: a pre-trained
PixelLink model (over the VGG16 backbone [21] and the
ICDAR2015 dataset [6]) serves as the promotional content
identiﬁer; a pre-trained Mask R-CNN model (over the ResNet-
50 backbone [25] and the MS COCO dataset [20]) are used for
ROI identiﬁcation; also another ResNet-50 model are trained
in our research for detecting explicit content in each ROI.
Model training. As mentioned earlier, our regional explicit
content detector includes the ROI locator and the explicit
content checker. The ROI locator is a pre-trained Mask R-
Fig. 4: Mask R-CNN’s output on an APPI with ROIs and
segmentation masks.
generate their segmentation masks to highlight the objects in
these ROIs. An example can be found in Figure 4.
Using the bounding box, our approach crops each ROI from
the original image and feeds it together with its segmentation
mask to a ResNet-50 [40] model for explicit content detection.
Note here that we do not use existing detection models but
train a new CNN-based model, since the traditional models
may not take advantage of all the information recovered by
our R-CNN: for example, our analysis shows that the Yahoo
open NSFW does not leverage the segmentation masks, which
are critical for locating the target object (i.e., person) and
removing noise. The training of the ResNet-50 model and the
dataset used for this purpose is elaborated in Section IV-A.
Running the trained model on the ROIs, if any of them is
found to contain explicit content, we label the input image
as a candidate APPI. In this way, we are able to identify the
APPIs once their least obfuscated explicit content is found,
forcing the adversary to perturb not only some part of the
picture but also every region on the picture that contains the
adult content to evade detection.
E. Evasion Checker
Finally, to ﬁnd out whether an identiﬁed image is indeed an
APPI, which is expected to evade existing detection, we scan
the image using four mainstream commercial inappropriate
image detectors: Google Cloud Vision API, Baidu AipImage-
Censor API, Yahoo Open NSFW model, and Clarifai NSFW
API. It is ﬂagged as an APPI if it bypasses at least one detector.
IV. Implementation and Evaluation
A. Implementation
Datasets. In our research, we use four datasets for model
training and evaluation: the social media image set, the porn
picture set, the non-porn picture set, and the groundtruth set.
• Social media image set. The social media image set is the
dataset from which we want to ﬁnd APPIs. It includes the
images from two major Chinese social media: Baidu Tieba
and Sina Weibo. From these two sources, we develop two
spiders to collect images.
(cid:26)(cid:22)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:56 UTC from IEEE Xplore.  Restrictions apply. 
1
0.8
0.6
0.4
0.2
t
e
a
r
e
v
i
t
i
s
o
p
e
u
r
T
0
0
model from (1)
model from (1)+(2)
model from (1)+(2)+(3)
0.2
0.4
0.6
0.8
1
False positive rate
Fig. 5: ROCs of models trained on three levels of picture sets.
CNN model on the MS COCO dataset [20] and the explicit
content checker has been trained in our research using the
porn/non-porn picture sets (see Section IV-A). To train the
checker, we ﬁrst run the ROI locator on the porn/non-port
sets. For each image, the locator reports its ROIs discovered,
together with each region’s object type, conﬁdence score for
the identiﬁcation, bounding box and segmentation mask for
highlighting its object. Then we select all the ROIs whose
object types are “person” and conﬁdence scores above
a
threshold, and crop these regions along their bounding box
from their original images. The threshold is chosen empirically
through an experiment to identify the “person” objects from
400 random sampled images from the aforementioned social
media image dataset. In the implementation of Mal`ena, the
threshold is set
to 0.8, as it achieves the best results in
the experiment. For each such regions, our approach further
applies its segmentation mask to produce a 4-channel image
(the standard RGB channels together with the mask channel).
All these images are then used as the training inputs for the
explicit content checker.
To evaluate the eﬀectiveness of the regional explicit content
detector, we use the groundtruth set (see Section IV-A) to
build a testing dataset for model evaluation. We also compare
models trained on our three levels of porn and non-porn
picture sets, (1), (1)+(2) and (1)+(2)+(3). Figure 5 shows their
ROCs. We observe that dataset (3) signiﬁcantly improved the
performance. Speciﬁcally, the AUC of three models are, 0.64,
0.72 and 0.94 respectively.
B. Evaluation
Precision and recall. To understand the eﬀectiveness of
Mal`ena, we run the system on the groundtruth dataset. Table I
shows the precision and recall at each stage of Mal`ena.
Overall, it achieves an overall precision and recall of 91%
and 85% respectively on the groundtruth set. Among the 500
images in the set, 233 are reported as APPIs, where 212 are
true APPIs, and the other 38 true APPIs are not detected by
our system.
TABLE I: Precision and recall at diﬀerent stages.
stage
precision
promotional content identiﬁcation
ROI locator
explicit content detection
overall
98%
89%
80%
91%
recall
90%
96%
93%
85%
TABLE II: Running time at diﬀerent stages.
stage
running time
images per minute
promotional content identiﬁcation
regional explicit content detection
evasiveness checker
overall
79.78 min
140.90 min
163.67 min
384.35 min
125.35
70.97
61.10
26.02
We investigate the false positives and false negatives.
Among the 21 false positives, there are 3 photos of human
hands and feet, these body parts are entirely of skin color and
there are less other information helping the model to classify
them right; 6 of comic books; 4 of athletes and the other 8 of
women. As for false negatives, 26 false negatives are because
the promotional content identiﬁer failed to detect the text on
the image. Two false negatives are due to the misclassiﬁcation
of the ROI locator and the rest are the results of the failure of
the explicit content checker.
Performance. To understand the performance of Mal`ena, we
measure the time it takes to process 10,000 images from the
social media image set at each individual analysis stage, the
promotional content identiﬁcation, image quality assessment
and regional explicit content detection. The experiment is run
on a server equipped with a 4-core Intel(R) Core(TM) i7-3770
CPU @ 3.40GHz, and a Nvidia GeForce GTX 1070 graphic
card with 8 GB memory. We instruct the GPU to process 8
images in parallel.
Table II shows the running time at each stage of of Mal`ena.
Overall, it takes 384.35 minutes to ﬁnish processing the 10,000
images. The results provide strong evidence that our system is
eﬃcient and can be easily scaled to a desirable level to handle
the massive amount of images from online forums every day.
V. Understand Adversarial Images in the Wild
A. Landscape
Running Mal`ena on the social media image set, our
approach automatically detects 4,353 APPIs among the
4,042,698 images collected from 76,752 hot posts/microblogs
on Baidu Tieba and Sina Weibo. By comparison, Baidu Tieba
hosts more APPIs (3,395 out of 4,353, 78%), while images
from Sina Weibo are more likely to contain explicit content
(958 out of 228,810 collected from microblogs, 0.419%).
Note that both sources prohibit displaying explicit content [2],
[10]. Baidu also provides an image censorship API capable of
detecting pornographic content.
Overall, 3,080 and 472 Tieba and Weibo accounts are found
to post APPIs. Figure 7 presents the distribution of APPI in-
stances over these accounts. We observe that 34% of the Tieba
accounts and 16% of the Weibo accounts post more than 5
APPI instances. Meanwhile, 5,060 and 1,103 posts/microblogs
are found to be the targets of APPI spammers. Apparently,
(cid:26)(cid:22)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:56 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: The usage of 7 obfuscation techniques.
obfuscation technique
color manipulation
rotation
noising
texturing
blurring
occlusion
# APPI (%)
160 (3.7%)
1,083 (24.9%)
2,130 (48.9%)
132 (3.0%)
829 (19.0%)
1,517 (34.8%)
transparentization & overlap
46 (1.0%)
Fig. 6: Distribution of # APPIs per account.
Fig. 7: Distribution of # APPI
instances per account.
Fig. 8: APPI instance cover-
age of post/microblog.
their strategy is to select a set of posts/microblogs to post
multiple APPIs to each of them, instead of disseminating the
images to many diﬀerent posts. Figure 8 shows the distribution
of the number of APPIs per post/microblog. As we can see
here, 75% of the Tieba APPIs are discovered under 20%
of the posts, while 50% of the Weibo APPIs are associated
with 23% of the microblogs. Meanwhile, we ﬁnd that the
spammers continuously post the same APPIs. Figure 6 shows
the distribution of duplicated instances per APPI image. 176
(5%) of the Tieba APPIs and 346 (32%) of Weibo APPIs have
been found more than once. Particularly, one Tieba APPI has
been posted 4,171 times by 251 diﬀerent users on 298 posts
crossing 48 diﬀerent “bars”.
B. Model Evasion
Obfuscation techniques. To understand the obfuscation tricks
employed by APPIs, we look into the APPIs detected by our
system and categorize their techniques into 7 major categories.
Examples of the APPIs in each category are shown in Figure 9.
• Color manipulation. The adversary often changes the color
of the original image, which is eﬀective since skin-related
features are widely utilized in explicit content detection. Color
manipulation approaches such as grayscaling, monochromati-
zation, and hue-rotation remove or obfuscate color information
of the original image, thereby rendering the skin-color based
detection less eﬀective.
• Rotation. Rotation involves a linear transformation on the
coordinates of each pixel according to a rotation matrix. The
technique works on the detector that utilizes the features not
rotation-invariant. For instance, without data augmentation,
many CNN architectures cannot learn rotation invariants.
• Noising. A common obfuscation trick is adding random
perturbations to images. Such perturbations introduce new
high-frequency signals to the image, making it harder to
recover the high-frequency signals of the original image. High-
frequency signals are important in image processing because
they contain important structural information (e.g., edges).
• Texturing. Texturing is a technique that applies a certain
texture (e.g. leather, paper, or marble) to the surface of an