### Data Collection and Datasets

To gather the necessary data, we queried the Google image search engine using the keyword "female with casual wear" and collected 20,000 images of athletes and 15,000 images containing single body parts or apparel. The dataset (2) was chosen because many females in the Celeb-1M dataset wear revealing or shiny clothing, which does not provide sufficient information about regular female attire. Additionally, dataset (3) is essential for differentiating between sexual behaviors and sports activities, as athletes often wear minimal clothing that exposes large areas of skin, similar to pornographic content.

The pornographic image dataset, used as positive samples for our model, includes 85,000 images known to contain sexually explicit content. These images were scraped from two pornographic websites: t66y.com and vulvapornpics.com.

### Groundtruth Set

The groundtruth dataset, used to evaluate our methodology (see Section IV-B), consists of 250 APPIs (Adversarial Pornographic and Potentially Inappropriate Images) and 250 non-APPIs randomly sampled from a social media image set. All these images have been manually labeled.

### System Implementation

We implemented Mal`ena using 2,400 lines of Python code. Three Python libraries—Pillow [23], skimage [24], and OpenCV [22]—were used for image processing tasks. Our deep learning models were built using TensorFlow with two deep neural network architectures and two pre-trained models:
- A pre-trained PixelLink model (over the VGG16 backbone [21] and the ICDAR2015 dataset [6]) serves as the promotional content identifier.
- A pre-trained Mask R-CNN model (over the ResNet-50 backbone [25] and the MS COCO dataset [20]) is used for ROI (Region of Interest) identification.
- Another ResNet-50 model, trained in our research, is used for detecting explicit content in each ROI.

### Model Training

Our regional explicit content detector includes an ROI locator and an explicit content checker. The ROI locator is a pre-trained Mask R-CNN model, which generates segmentation masks to highlight objects in the ROIs. An example of this can be seen in Figure 4.

Using the bounding box, our approach crops each ROI from the original image and feeds it, along with its segmentation mask, to a ResNet-50 [40] model for explicit content detection. We trained a new CNN-based model rather than using existing detection models because traditional models may not fully utilize the information recovered by our R-CNN. For instance, the Yahoo open NSFW model does not leverage segmentation masks, which are crucial for locating the target object (i.e., person) and removing noise. The training of the ResNet-50 model and the dataset used for this purpose are detailed in Section IV-A.

If any ROI is found to contain explicit content, the input image is labeled as a candidate APPI. This allows us to identify APPIs even when their least obfuscated explicit content is present, forcing adversaries to perturb every region of the image containing adult content to evade detection.

### Evasion Checker

To determine if an identified image is indeed an APPI, we scan the image using four mainstream commercial inappropriate image detectors: Google Cloud Vision API, Baidu AipImage-Censor API, Yahoo Open NSFW model, and Clarifai NSFW API. The image is flagged as an APPI if it bypasses at least one detector.

### Implementation and Evaluation

#### Datasets

In our research, we use four datasets for model training and evaluation:
- **Social media image set**: Includes images from Baidu Tieba and Sina Weibo.
- **Porn picture set**: Contains 85,000 images with sexually explicit content.
- **Non-porn picture set**: Includes 20,000 images of athletes and 15,000 images of single body parts or apparel.
- **Groundtruth set**: Consists of 250 APPIs and 250 non-APPIs.

#### Precision and Recall

To evaluate Mal`ena's effectiveness, we ran the system on the groundtruth dataset. Table I shows the precision and recall at each stage of Mal`ena. Overall, the system achieves a precision of 91% and a recall of 85% on the groundtruth set.

| Stage | Precision | Recall |
|-------|-----------|--------|
| Promotional content identification | 98% | 90% |
| ROI locator | 89% | 96% |
| Explicit content detection | 80% | 93% |
| Overall | 91% | 85% |

#### Running Time

Table II shows the running time at each stage of Mal`ena. The experiment was conducted on a server equipped with a 4-core Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz and a Nvidia GeForce GTX 1070 graphic card with 8 GB memory. The GPU processes 8 images in parallel.

| Stage | Running Time (min) | Images per Minute |
|-------|--------------------|-------------------|
| Promotional content identification | 79.78 | 125.35 |
| Regional explicit content detection | 140.90 | 70.97 |
| Evasiveness checker | 163.67 | 61.10 |
| Overall | 384.35 | 26.02 |

### Understanding Adversarial Images in the Wild

#### Landscape

Running Mal`ena on the social media image set, we detected 4,353 APPIs among 4,042,698 images collected from 76,752 hot posts/microblogs on Baidu Tieba and Sina Weibo. Baidu Tieba hosts more APPIs (3,395 out of 4,353, 78%), while images from Sina Weibo are more likely to contain explicit content (958 out of 228,810 collected from microblogs, 0.419%). Both platforms prohibit displaying explicit content, and Baidu provides an image censorship API capable of detecting pornographic content.

Overall, 3,080 and 472 Tieba and Weibo accounts were found to post APPIs. Figure 7 presents the distribution of APPI instances over these accounts. We observe that 34% of the Tieba accounts and 16% of the Weibo accounts post more than 5 APPI instances. Meanwhile, 5,060 and 1,103 posts/microblogs were found to be the targets of APPI spammers. Their strategy is to select a set of posts/microblogs to post multiple APPIs to each, rather than disseminating the images widely. Figure 8 shows the distribution of the number of APPIs per post/microblog. As seen, 75% of the Tieba APPIs are discovered under 20% of the posts, while 50% of the Weibo APPIs are associated with 23% of the microblogs. Additionally, spammers continuously post the same APPIs. Figure 6 shows the distribution of duplicated instances per APPI image. 176 (5%) of the Tieba APPIs and 346 (32%) of Weibo APPIs have been found more than once. One Tieba APPI has been posted 4,171 times by 251 different users on 298 posts across 48 different "bars."

#### Obfuscation Techniques

To understand the obfuscation tricks employed by APPIs, we categorized them into seven major categories:

- **Color manipulation**: Changing the color of the original image, such as grayscaling, monochromatization, and hue-rotation, to remove or obfuscate color information, making skin-color-based detection less effective.
- **Rotation**: Applying a linear transformation on the coordinates of each pixel according to a rotation matrix, which works on detectors that do not learn rotation invariants.
- **Noising**: Adding random perturbations to introduce high-frequency signals, making it harder to recover the original image's high-frequency signals.
- **Texturing**: Applying a certain texture (e.g., leather, paper, or marble) to the surface of an image to obscure the content.
- **Blurring**: Reducing the sharpness of the image to make it harder to detect explicit content.
- **Occlusion**: Covering parts of the image with other objects or text to hide explicit content.
- **Transparentization & overlap**: Using transparency and overlapping elements to obscure the content.

Examples of APPIs in each category are shown in Figure 9.