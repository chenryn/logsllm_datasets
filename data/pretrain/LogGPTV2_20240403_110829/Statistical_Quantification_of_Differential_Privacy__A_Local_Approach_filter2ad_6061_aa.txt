title:Statistical Quantification of Differential Privacy: A Local Approach
author:&quot;Onder Askin and
Tim Kutta and
Holger Dette
9
8
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
Statistical Quantiﬁcation of Differential Privacy:
A Local Approach
¨Onder Askin
Ruhr-University Bochum
PI:EMAIL
Tim Kutta
Ruhr-University Bochum
PI:EMAIL
Holger Dette
Ruhr-University Bochum
PI:EMAIL
Abstract—In this work, we introduce a new approach for
statistical quantiﬁcation of differential privacy in a black box
setting. We present estimators and conﬁdence intervals for the
optimal privacy parameter of a randomized algorithm A, as well
as other key variables (such as the “data-centric privacy level”).
Our estimators are based on a local characterization of privacy
and in contrast to the related literature avoid the process of
“event selection” - a major obstacle to privacy validation. This
makes our methods easy to implement and user-friendly. We
show fast convergence rates of the estimators and asymptotic
validity of the conﬁdence intervals. An experimental study of
various algorithms conﬁrms the efﬁcacy of our approach.
Index Terms—Differential privacy, data-centric privacy, local
estimators, conﬁdence intervals
I. INTRODUCTION
Since its introduction in the seminal work of [1], the concept
of Differential Privacy (DP) has become a standard tool to
assess information leakage in data disseminating procedures.
DP characterizes how strongly the output of a randomized al-
gorithm is inﬂuenced by any one of its inputs, thus quantifying
the difﬁculty of inferring arguments (i.e., user information)
from algorithmic releases.
To formalize this situation, we consider a database x =
(x(1),··· , x(m)) where each data point x(i) takes values
in a set D and corresponds to the data provided by the ith
individual among m users. Furthermore, we introduce the
notion of neighboring or adjacent databases, that is databases
that only differ in one component. Mathematically, we can
(cid:2) by unit Hamming distance
express neighborhood of x, x
) = 1, where the Hamming distance is deﬁned as
dH (x, x
follows:
(cid:2)
(cid:2)
dH (x, x
) := |{1 ≤ i ≤ m : x(i) (cid:3)= x
(cid:2)
(i)}|.
Deﬁnition 1. An Algorithm A is called -differentially private
(cid:2)
for some  > 0, if for any two neighboring databases x, x
and any measurable event E the inequality
P(A(x) ∈ E) ≤ e
(cid:2)
P(A(x
) ∈ E)
(1)
holds.
Deﬁnition 1 demands that (1) holds for all measurable
events E, but what constitutes a measurable event depends
on the output space Y of the randomized algorithm A. If Y is
discrete (in particular if |Y|  0, candidate triplets are generated and a binomial sta-
(cid:2)
tistical test is employed to ﬁnd a counterexample (x0, x
0, E0)
that violates the privacy condition (1). These counterexamples
expose faulty, non-private algorithms in a fast and practical
manner and hint at potential weaknesses in the algorithm’s
design.
A related, but distinct approach is the examination of lower
bounds for differential privacy [20]. Here, privacy violations
are determined with the help of the “privacy loss”, which is
deﬁned for any triplet (x, x
, E) as
(cid:2)
P(A(x) ∈ E)
P(A(x
Lx,x(cid:2) (E) :=
We interpret ∞−∞ := 0 to account for events with 0 proba-
bility. In line with Deﬁnition 1, an algorithm A satisﬁes -DP
if and only if Lx,x(cid:2) (E) ≤  for all permissible triplets. Thus,
computing privacy violations Lx,x(cid:2) (E) for different triplets
naturally provides lower bounds for . Note that in this context,
privacy violations and loss are used constructively to gather
information about the privacy parameter. We also want to
point out that this approach can be adapted to counterexample
(cid:2)
generation, if for some predetermined 0 a triplet (x0, x
0, E0)
0 (E0) > 0. However, lower bounds are
is found s.t. Lx0,x(cid:2)
somewhat more ﬂexible, because they do not require some
hypothesized 0 in the ﬁrst place.
Even though [19] and [20] provide effective tools for privacy
validation, they are not entirely compatible with our black box
assumption. While the binomial test in [19] by itself requires
little knowledge of A, the larger scheme, within which it is em-
bedded, is designed to also consider the algorithm’s program
code. A symbolic execution of that code can be performed
to facilitate the detection of counterexamples. Therefore, this
approach is also labeled semi-black-box by its authors [19].
Even less compatible with the black box regime, the approach
in [20] requires access to the program code of algorithm A in
order to alter it in ways that produce a differentiable surrogate
function for Lx,x(cid:2). Numerical optimizers can then be deployed
to ﬁnd triplets that yield high privacy violations.
A more recent method to quantify DP is the DP-Sniper
(cid:2), DP-
algorithm, developed in [21]. For ﬁxed databases x and x
∗ which approximately maximizes
Sniper creates an event E
∗
). To
(2) and then derives a statistical lower bound for Lx,x(cid:2) (E
∗, a machine learning classiﬁer is employed that
construct E
approximates the posterior probability of x given an output
∗ then consists of all those outputs, that
of A. Intuitively, E
) with
are expected to be generated by A(x) rather than A(x
high certainty. The classiﬁers used are logistic regression (a
one-layer neural network) and a small neural network (two
hidden layers). Both choices yield relatively simple parametric
models for the posterior, where the classiﬁer based on logistic
regression corresponds to a linear decision rule. The successful
maximization of Lx,x(cid:2) in [21] then presupposes that the true
(and unknown) posterior distribution belongs to one of these
classes. Naturally, such a parametric assumption limits the
(cid:2)
(cid:2)
scope of theoretical performance guarantees and is difﬁcult
to reconcile with a black box setting, where a non-parametric
statistical procedure would be more ﬁtting.
The problem of event selection: As we have seen above,
statistical validation of DP rests on ﬁnding a triplet (x, x
, E)
that provokes a high privacy violation. This task is typically
(cid:2) such
split into two separate parts: First, ﬁnding databases x, x
that the loss Lx,x(cid:2) (E) is large for some event E and, second,
ﬁnding this very event. Even though both problems are non-
trivial, the greater challenge lies in the latter one, the event
selection (see [21]).
Starting with the space of potential events, we observe that
if Y consists of a ﬁnite number of output values, the number of
measurable events grows exponentially in |Y| with |P(Y)| =
|Y|. This makes evaluating Lx,x(cid:2) on all potential events E
2
impractical even if |Y|   underestimates the
privacy level that is actually achievable.
We refer to  as the global privacy parameter which, in
light of identity (4), only provides a “worst-case” guarantee
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
403
(cid:2). In contrast, the precise
for privacy leakage of any pair x, x
(cid:2) is captured
amount of privacy leakage associated with x and x
by x,x(cid:2), which is potentially much smaller than . The data-
speciﬁc privacy violations comprise more granular information
that we utilize to examine the following privacy aspects:
First, each x,x(cid:2) constitutes a lower bound of . Because
Lx,x(cid:2) (E) ≤ x,x(cid:2) holds for all events E, these lower bounds
are at least equally and potentially even more powerful than
the ones derived in prior work. Lower bounds in themselves
are useful, as they can help expose faulty algorithms [20] and
narrow down the extent to which a given algorithm can be
private at all [21]. This ultimately provides us with a better
understanding of the global privacy parameter .
Secondly, data-speciﬁc privacy violations can be used to
infer the data-centric privacy level for select databases. More
precisely, suppose that a curator has gathered a database x and
is interested in the amount of privacy conceded speciﬁcally to
the individuals with data in x. The maximum privacy violation
associated with x is obtained by forming the supremum over
all data-speciﬁc privacy violations in its neighborhood, that is
x :=
sup
x(cid:2): dH (x,x(cid:2))=1
x,x(cid:2) .
(5)
Graphically speaking, x is the maximum privacy loss attained
on a unit sphere around x (with regard to dH). It also con-
stitutes the maximum privacy loss any individual represented
in x has to at most tolerate (thus, it has also been studied
in the context of ”individual DP” [22]). Evidently, we have
x ≤  for all databases x and we will see later on that the
data-centric privacy level x can be considerably smaller than
the global privacy guarantee  (see Section 5).