:::
排队规则（`qdiscs`{.literal}）可帮助查询以及稍后使用网络接口调度流量传输。`qdisc`{.literal}
有两个操作：
::: itemizedlist
-   enqueue 请求，以便数据包排队以进行后续传输和.
-   分离队列请求，以便可以选择其中一个排队的数据包进行即时传输。
:::
每个 `qdisc`{.literal} 都有一个名为 `handle`{.literal} 的 16
位十六进制数字，带有一个附加的冒号，如 `1:`{.literal} 或
`abcd:`{.literal}。这个数字被称为 `qdisc`{.literal} 主数字。如果
`qdisc`{.literal}
有类，则标识符为一个由两个数字组成的对，主数字在次数字之前，`:`{.literal}，例如
`abcd:1`{.literal}。次数字的编号方案根据 `qdisc`{.literal}
类型而定。有时，编号是系统性的，第一类有 ID `:1`{.literal}、第 2
类 `:2`{.literal}等。一些 `qdiscs`{.literal}
允许用户在创建类时随机设置类次要数字。
#### []{#linux-traffic-control_configuring-and-managing-networking.html#classful_literal_role_systemitem_qdiscs_literal}Classful `qdiscs`{.literal}
存在不同的 `qdiscs`{.literal}
类型，有助于将数据包传送到网络接口或从网络接口传输。您可以使用
root、parent 和 child 类配置
`qdiscs`{.literal}。子对象可以被附加的位置被称为
class。`qdisc`{.literal} 中的类灵活，可以包括多个子类或一个子类
`qdisc`{.literal}。没有对包含类 `qdisc`{.literal}
本身的类的破坏，这促成了复杂的流量控制场景。classful `qdiscs`{.literal}
不自行存储任何数据包。反之，它们根据
`qdisc`{.literal}的具体条件把子队列和出队请求降到他们的子对象中。最后，这个递归数据包传递最终结束保存数据包的位置（在出现排队时从中提取）。
#### []{#linux-traffic-control_configuring-and-managing-networking.html#classless_literal_role_systemitem_qdiscs_literal}无类别 `qdiscs`{.literal}
一些 `qdiscs`{.literal} 不包含子类，它们名为无类别
`qdiscs`{.literal}。与类 `qdiscs`{.literal} 相比，无类别
`qdiscs`{.literal}
需要较少的定制。通常情况下，将它们附加到接口就足够了。
::: itemizedlist
**其它资源**
-   有关无类别和等级 `qdiscs`{.literal} 的详细信息，请参考
    `tc(8)`{.literal} man page。
-   有关动作的详情请参考 `actions`{.literal} 和 `tc-actions.8`{.literal}
    man page。
:::
:::
::: section
::: titlepage
# []{#linux-traffic-control_configuring-and-managing-networking.html#available-qdiscs-in-rhel_linux-traffic-control}RHEL 中可用的 qdiscs {.title}
:::
每个 `qdisc`{.literal} 都处理唯一的与网络相关的问题。以下是 RHEL
中可用的 `qdiscs`{.literal} 列表。您可以使用以下 `qdisc`{.literal}
中的任何一种来根据您的网络要求控制网络流量。
::: table
[]{#linux-traffic-control_configuring-and-managing-networking.html#idm140431059030816}
**表 26.1. RHEL 中的可用调度程序**
::: table-contents
  `qdisc`{.literal} 名称                                                        包含在                             卸载支持
  ----------------------------------------------------------------------------- ---------------------------------- ----------
  异步传输模式(ATM)                                                             `kernel-modules-extra`{.literal}    
  基于类的队列                                                                  `kernel-modules-extra`{.literal}    
  基于贡献的共享                                                                `kernel-modules-extra`{.literal}   是
  CHOose 和 Keep 用于有响应的流量，CHOose 和 Kill 用于没有响应的流量（CHOKE）   `kernel-modules-extra`{.literal}    
  受控的延迟（CoDel）                                                           `kernel-core`{.literal}             
  开路 Robin(DRR)                                                               `kernel-modules-extra`{.literal}    
  Differentiated Services marker (DSMARK)                                       `kernel-modules-extra`{.literal}    
  Enhanced Transmission Selection (ETS)                                         `kernel-modules-extra`{.literal}   是
  Fair Queue (FQ)                                                               `kernel-core`{.literal}             
  Fair Queuing Controlled Delay (FQ_CODel)                                      `kernel-core`{.literal}             
  Generalized Random Early Detection (GRED)                                     `kernel-modules-extra`{.literal}    
  Hierarchical Fair Service Curve (HSFC)                                        `kernel-core`{.literal}             
  Heavy-Hitter Filter (HHF)                                                     `kernel-core`{.literal}             
  Hierarchy Token Bucket (HTB)                                                  `kernel-core`{.literal}             
  INGRESS                                                                       `kernel-core`{.literal}            是
  Multi Queue Priority (MQPRIO)                                                 `kernel-modules-extra`{.literal}   是
  Multiqueue (MULTIQ)                                                           `kernel-modules-extra`{.literal}   是
  Network Emulator (NETEM)                                                      `kernel-modules-extra`{.literal}    
  Proportional Integral-controller Enhanced (PIE)                               `kernel-core`{.literal}             
  PLUG                                                                          `kernel-core`{.literal}             
  Quick Fair Queueing (QFQ)                                                     `kernel-modules-extra`{.literal}    
  Random Early Detection (RED)                                                  `kernel-modules-extra`{.literal}   是
  Stochastic Fair Blue (SFB)                                                    `kernel-modules-extra`{.literal}    
  Stochastic Fairness Queueing (SFQ)                                            `kernel-core`{.literal}             
  Token Bucket Filter (TBF)                                                     `kernel-core`{.literal}            是
  Trivial Link Equalizer (TEQL)                                                 `kernel-modules-extra`{.literal}    
:::
:::
::: {.important style="margin-left: 0.5in; margin-right: 0.5in;"}
### 重要 {.title}
`qdisc`{.literal} 卸载需要在 NIC 上支持硬件和驱动程序。
:::
::: itemizedlist
**其它资源**
-   如需配置 `qdiscs`{.literal} 的参数和过滤器的完整信息，请参阅
    `tc(8)`{.literal}、`cbq`{.literal}、`cbs`{.literal}、`choke`{.literal}、`CoDel`{.literal}、`drr`{.literal}、`fq`{.literal}.
    `htb`{.literal}、`mqprio`{.literal}、`netem`{.literal}、`pie`{.literal}、`sfb`{.literal}、`pfifo`{.literal}、`tc-red`{.literal}、`sfq`{.literal}、`tbf`{.literal}、
    和 `prio`{.literal} man page。
:::
:::
::: section
::: titlepage
# []{#linux-traffic-control_configuring-and-managing-networking.html#inspecting-qdisc-of-a-network-interface-using-the-tc-utility_linux-traffic-control}使用 tc 工具检查网络接口的 qdiscs {.title}
:::
默认情况下，Red Hat Enterprise Linux 系统使用 `fq_codel`{.literal}
`qdisc`{.literal}。这个步骤描述了如何检查 `qdisc`{.literal} 计数器。
::: orderedlist
**流程**
1.  可选：查看您当前的 `qdisc`{.literal}：
    \# `tc qdisc show dev enp0s1`{.literal}
2.  检查当前的 `qdisc`{.literal} 计数器：
    ``` literallayout
    # tc -s qdisc show dev enp0s1
    qdisc fq_codel 0: root refcnt 2 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn
    Sent 1008193 bytes 5559 pkt (dropped 233, overlimits 55 requeues 77)
    backlog 0b 0p requeues 0
    ....
    ```
:::
::: informalexample
::: itemizedlist
-   `dropped`{.literal} - 由于所有队列已满而使数据包被丢弃的次数
-   `overlimits`{.literal} - 配置的链路容量已满的次数
-   `sent`{.literal} - 出队的数量
:::
:::
:::
::: section
::: titlepage
# []{#linux-traffic-control_configuring-and-managing-networking.html#updating-the-default-qdisc_linux-traffic-control}更新默认的 qdisc {.title}
:::
如果使用当前的 `qdisc`{.literal}
观察网络数据包丢弃的事件，您可以根据您的网络需要更改
`qdisc`{.literal}。您可以选择
`qdisc`{.literal}来满足您的网络要求。这个步骤描述了如何更改 Red Hat
Enterprise Linux 的默认 `qdisc`{.literal}。
::: orderedlist
**流程**
1.  查看当前的默认 `qdisc`{.literal}：
    ``` literallayout
    # sysctl -a | grep qdisc
    net.core.default_qdisc = fq_codel
    ```
2.  查看当前以太网连接的 `qdisc`{.literal}：
    ``` literallayout
    # tc -s qdisc show dev enp0s1
    qdisc fq_codel 0: root refcnt 2 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn
    Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
    backlog 0b 0p requeues 0
    maxpacket 0 drop_overlimit 0 new_flow_count 0 ecn_mark 0
    new_flows_len 0 old_flows_len 0
    ```
3.  更新现有 `qdisc`{.literal}:
    \# `sysctl -w net.core.default_qdisc=pfifo_fast`{.literal}
4.  要应用这些更改，重新载入网络驱动程序：
    \# `rmmod NETWORKDRIVERNAME`{.literal}
    \# `modprobe NETWORKDRIVERNAME`{.literal}
5.  启动网络接口：
    \# `ip link set enp0s1 up`{.literal}
:::
::: itemizedlist
**验证步骤**
-   查看以太网连接的 `qdisc`{.literal}:
    ``` literallayout
    # tc -s qdisc show dev enp0s1
    qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
     Sent 373186 bytes 5333 pkt (dropped 0, overlimits 0 requeues 0)
     backlog 0b 0p requeues 0
    ....
    ```
:::
::: itemizedlist
**其它资源**
-   有关永久更改的详情，请参考[如何在 Red Hat Enterprise Linux 中设置
    `sysctl`{.literal}
    变量](https://access.redhat.com/solutions/2587){.link}。
:::
:::
::: section
::: titlepage
# []{#linux-traffic-control_configuring-and-managing-networking.html#temporarily-setting-the-current-qdisk-of-a-network-interface-using-the-tc-utility_linux-traffic-control}使用 tc 工具临时设置网络接口的当前 qdisk {.title}
:::
您可以在不更改默认文件的情况下更新当前的
`qdisc`{.literal}。这个步骤描述了如何更改 Red Hat Enterprise Linux
中当前的 `qdisc`{.literal}。
::: orderedlist
**流程**
1.  可选：查看当前 `qdisc`{.literal}:
    \# `tc -s qdisc show dev enp0s1`{.literal}
2.  更新当前的 `qdisc`{.literal}:
    \# `tc qdisc replace dev enp0s1 root htb`{.literal}
:::
::: itemizedlist
**验证步骤**
-   查看更新的当前 `qdisc`{.literal}：
    ``` literallayout
    # tc -s qdisc show dev enp0s1
    qdisc htb 8001: root refcnt 2 r2q 10 default 0 direct_packets_stat 0 direct_qlen 1000
    Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
    backlog 0b 0p requeues 0
    ```
:::
:::
::: section
::: titlepage
# []{#linux-traffic-control_configuring-and-managing-networking.html#proc_permanently-setting-the-current-qdisk-of-a-network-interface-using-networkmanager_linux-traffic-control}使用 NetworkManager 永久设置网络接口的当前 qdisk {.title}
:::
您可以更新网络管理器连接的当前 `qdisc`{.literal} 值。
::: orderedlist
**流程**
1.  可选：查看当前 `qdisc`{.literal}:
    ``` literallayout
    # tc qdisc show dev enp0s1
      qdisc fq_codel 0: root refcnt 2
    ```
2.  更新当前的 `qdisc`{.literal}:
    `# nmcli connection modify enp0s1 tc.qdiscs ‘root pfifo_fast’`{.literal}
3.  可选： 要在现有 `qdisc`{.literal} 上添加另一个
    `qdisc`{.literal}，请使用 `+tc.qdisc`{.literal} 选项：
    `# nmcli connection modify enp0s1 +tc.qdisc ‘ingress handle ffff:’`{.literal}
4.  激活更改：
    `# nmcli connection up enp0s1`{.literal}
:::
::: itemizedlist
**验证步骤**
-   查看当前 `qdisc`{.literal} 网络接口：
    ``` literallayout
    # tc qdisc show dev enp0s1
    qdisc pfifo_fast 8001: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
    qdisc ingress ffff: parent ffff:fff1 ----------------
    ```
:::
::: itemizedlist
**其它资源**
-   如需更多信息，请参阅 `nm-settings(5)`{.literal} man page。
:::
:::
:::
[]{#getting-started-with-multipath-tcp_configuring-and-managing-networking.html}
::: chapter
::: titlepage
# []{#getting-started-with-multipath-tcp_configuring-and-managing-networking.html#getting-started-with-multipath-tcp_configuring-and-managing-networking}第 27 章 多路径 TCP 入门 {.title}
:::
::: {.important style="margin-left: 0.5in; margin-right: 0.5in;"}
### 重要 {.title}
多路径 TCP
仅作为技术预览提供。红帽产品服务级别协议（SLA）不支持技术预览功能，且其功能可能并不完善，因此红帽不建议在生产环境中使用它们。这些预览可让用户早期访问将来的产品功能，让用户在开发过程中测试并提供反馈意见。
如需有关
[技术预览功能支持范围](https://access.redhat.com/support/offerings/techpreview){.link}
的信息，请参阅红帽客户门户网站中的技术预览功能支持范围。
:::
多路径 TCP(MPTCP)是传输控制协议(TCP)的扩展。利用 Internet
协议(IP)，主机可以将数据包发送到目标位置。TCP
可确保通过互联网可靠地提供数据，并自动调整其带宽以响应网络负载。
以下是 MPTCP 的优点：
::: itemizedlist
-   它为带有两个或者多个网络接口的设备启用 TCP。
-   它允许用户同时使用不同的网络接口或从一个连接间无缝切换。
-   它提高了网络中的资源使用量以及网络故障的恢复能力。
:::
本节描述了如何：
::: itemizedlist
-   创建新的 MPTCP 连接,
-   使用 `iproute2`{.literal} 为 MPTCP 连接添加新的子流和 IP 地址，以及
-   在内核中禁用 MPTCP 以避免使用 MPTCP 连接的应用程序。
:::
::: section
::: titlepage
# []{#getting-started-with-multipath-tcp_configuring-and-managing-networking.html#preparing-rhel-to-enable-mptcp-support_getting-started-with-multipath-tcp}准备 RHEL 启用 MPTCP 支持 {.title}
:::