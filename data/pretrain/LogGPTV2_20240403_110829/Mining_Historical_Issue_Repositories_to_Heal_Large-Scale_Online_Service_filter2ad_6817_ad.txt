### Performance and Precision

Our approach achieves a precision of 0.87, with the corresponding similarity threshold set at 0.85. This indicates that our method outperforms both App1 and App2 in terms of precision.

High precision is particularly desirable in real-world scenarios. For instance, engineers often need to set a similarity threshold. When a new issue arises, the system retrieves historical issues with a similarity score exceeding this threshold. Engineers then review these retrieved issues to decide on the appropriate healing action. In such scenarios, higher precision ensures that engineers can make more confident decisions based on the retrieved historical issues.

### Real-World Experiences

To enhance the effectiveness and integration of our approach into real production pipelines, we have investigated and documented the cases where our approach fails to suggest correct healing actions. We have used these findings to improve our algorithm design and implementation. In this section, we discuss the types of new issues where our approach is not applicable and those where it fails to suggest correct healing actions.

#### 1. Issues Where Our Approach is Not Applicable

We systematically and manually analyzed 400 sampled issues from RepX (see Section IV.A). Of these, 243 were used in our evaluations, while the remaining 157 (39%) did not meet our input requirements. The types of these issues are listed in Table VII.

- **No Logs Available**: These issues lack corresponding transaction logs due to various reasons, such as network issues or changes in system topology.
- **Service Upgrade**: These are "noisy" issues generated during system upgrades, which do not require investigation.
- **Auto Recovered**: These issues resolved themselves before engineers could start their investigation.
- **False Alarms**: These issues were reported due to non-service-related causes, such as internal testing bugs.

#### 2. Issues Where Our Approach Fails

We identified 21 issues where our approach did not suggest the correct healing actions, categorized as follows:

- **One-Shot Issue** (12 issues): These are unique issues with no similar historical counterparts. While the logs provide useful diagnostic information, our approach fails because there are no similar historical issues.
- **Logs Not Enough** (4 issues): These issues lack sufficient log information for diagnosis. Engineers could not identify the root cause, and our approach also failed to generate helpful signatures.
- **Latency Issue** (3 issues): These involve user requests processed with long latencies but still successfully. Our approach struggles with these because the code paths for long-latency and fast responses are not distinguishable.
- **Insufficient Events** (2 issues): These issues have insufficient events to differentiate them from other issues. More detailed log messages could improve the effectiveness of our approach.

### Threats to Validity

External validity threats include the representativeness of the studied online service, its issues, and usage patterns. The service is a real-world product serving millions of customers globally, and the issues and usage data come from real-world cases. Future work could reduce these threats by expanding the scope of the study. Internal validity threats, such as instrumentation effects, could be mitigated by manual inspection of trace data and system outputs.

### Real Case Studies

#### Antivirus Configuration Corruption

In January 2012, ServiceX experienced performance issues in one datacenter, with customers experiencing slow responses and file upload failures. Initial diagnosis pointed to a specific Web Front End (WFE) named WFEx, which was rebooted without success. Senior experts later identified the root cause as a corrupted antivirus configuration file. The resolution involved re-imaging the WFE. Our approach helped by identifying the symptoms "antivirus timeout" and "SQL failing over detected," leading to quicker resolution in a subsequent similar issue.

#### False Alarms of Monitoring System

In July 2012, the monitoring system of ServiceX sent out 120 false alarms due to incompatible component versions after an upgrade. Operators initially struggled to diagnose the issue, but our approach, by grouping the first three detailed issues, correctly retrieved all 117 subsequent issues, reducing redundant efforts.

### Related Work

- **System Diagnosis**: Cohen et al. [10] propose retrieving previously solved and annotated issues to help identify root causes. Our work aims to provide healing suggestions to reduce Mean Time to Repair (MTTR).
- **Fault Localization**: Liu et al. [14] use statistical models for bug localization. Our approach uses contrast information for high-accuracy signature generation.
- **Mining Bug Repositories**: Ashok et al. [17] implement a search tool for faster bug fixing. Our approach leverages transaction logs for healing suggestions, addressing the challenges posed by incomplete or imprecise textual information in typical historical issue repositories.

### Conclusion

To effectively reduce the MTTR of a service, we have proposed an automated mining-based approach that suggests appropriate healing actions by adapting solutions from similar historical issues. Our studies on a real-world online service show that our approach can effectively provide appropriate healing actions, thereby reducing the MTTR of the service.