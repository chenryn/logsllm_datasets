approach is 0.87, with the corresponding similarity
threshold as 0.85. We can see that our approach also
performs the best compared to App1 and App2 in terms
of precision.
they would like to see all
Note that high precision is very desirable for another
real scenario in practice. In particular, ﬁrst, engineers
would like to set a similarity threshold. Then, after
a new issue occurs,
the
possible historical issues (compared to the new issues)
with similarity exceeding this threshold. Finally, the
engineers inspect these retrieved historical issues and
make the ﬁnal decision on which healing action to apply.
In this scenario, higher precision allows the engineers
to gain higher conﬁdence in making decisions based on
the retrieved historical issues.
C. Experiences in Real Product
To make our approach more effective and better ﬁt
into pipelines of service diagnostic in real production,
we investigate and record the issues or conditions where
our approach fails to suggest correct healing actions. We
further improve our algorithm design and implementa-
tion based on some of these ﬁndings. In this section,
319319319
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 
ISSUES WHERE OUR APPROACH IS NOT APPLICABLE OR
Table VII
FAILS
Inapplicable issues
Issue type
No logs available
Service upgrade
Auto recovered
False alarms
Other
Count
78 (19.5%)
31 (7.8%)
28 (7.0%)
5 (1.3%)
15 (3.8%)
Issues our approach fails
Count
Issue type
one-shot issue
logs not enough
latency issue
insufﬁcient events
Total
12
4
3
2
21
we discuss these experiences focusing the types of new
issues where our approach is not applicable and the
types of new issues where our approach fails to suggest
correct healing actions.
1) Issues Where Our Approach is Not Applicable:
We systematically and manually investigate the sampled
400 issues in RepX (see Section IV.A). Besides the
243 issues used in our evaluations, the remaining 157
(39%) issues do not satisfy our input requirements. The
statistics on types of these issues are listed in the left-
hand side of Table VII. We next illustrate the four main
types of issues.
The type of “No logs available” describes the issues
where we cannot collect corresponding transaction logs
anywhere. The underlying reasons vary: some were due
to network issues, so the user requests did not reach the
application service; some were due to changes of the
topology in the system, so that the alerted service no
longer existed when engineers started the investigation.
The type of “Service upgrade” describes issues that
are “noisy” issues alerted during system upgrade, i.e.,
the monitoring system was not shut down in time when
the service upgrade began. Engineers would just leave
a note of “this is trivial alarm that don’t need to inves-
tigate” and close it, so we cannot obtain corresponding
healing actions.
The type of “Auto recovered” describes issues that
were automatically recovered before engineers started
the investigation.
The type of “False alarms” describes issues that were
ﬁled due to cases not related to real service issues, e.g.,
some internal testing bugs were wrongly ﬁled as service
issues by mistake.
2) Issue Where Our Approach Fails: We study the
21 issues that our approach does not suggest correct
healing actions, and categorize them in the right-hand
side of Table VII.
The type of “one-shot issue” (12 issues) describes
issues with unique signatures, which are not similar to
the signatures of any other issues (each of the 12 issues
is not similar with each other either). According to the
feedback on these issues from engineers, the logs of
these issues provide useful information for diagnosis,
and the signatures that our approach generates are still
valuable to the engineers in diagnosis. Our approach
320320320
fails on these issues because there exist no similar
historical issues for these issues.
The type of “logs not enough” (4 issues) describes
issues with log information insufﬁcient for diagnosis.
Engineers did not identify the root causes by inspecting
these logs, and our approach does not generate helpful
signatures either. According to the discussion with en-
gineers, the logging was not sufﬁcient when the system
executed the code paths that lead to these 4 issues. On
the other hand, 4 is a small number, implying that the
current logging practice is generally good enough.
The type of “latency issue” (3 issues) describes issues
where the user requests are processed with suspiciously
long time, but are still processed successfully. Users
would feel unhappy about such slow response; however,
our approach cannot handle such long-latency issues
well, because the unhappy (long-latency) code paths
are not different to the happy (fast) code paths, and
no events for these two cases are discriminative.
The type of “insufﬁcient events” (2 issues) describes
issues that are associated with insufﬁcient events. Using
only events or event sets associated with these issues
cannot well discriminate the issues with other issues.
More information from log messages should be lever-
aged to generate more proper signatures. For example,
from the system’s source code,
there is one event
generated for indicating the overall general exception
handling at the last stage of request processing. If the
request fails, there could be various exception types to
indicate different system failures, which could lead to
different healing actions. So using only this event can-
not well discriminate different issues. However, since
the exception information is recorded in the message
column, our approach’s effectiveness can be improved
by simply combining the event with its partial message
to construct a more useful new event.
D. Threats to Validity
The threats to external validity primarily include the
degree to which the studied online service, its issues,
its usages, etc. are representative of true practice. The
studied online service is a real-world product online
service that serves millions of customers globally. The
investigated issues and service usages come from real-
world cases. These threats could be reduced by more ex-
periments on wider types of subjects in future work. The
threats to internal validity are instrumentation effects
that can bias our results. Faults in our healing system
might cause such effects. To reduce these threats, we
manually inspected trace data and our system outputs
for a number of issues.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 
V. REAL CASE STUDIES
In this section, we select two typical real issues oc-
curred in one online service system, to demonstrate the
effectiveness and potential capability of our approach.
Antivirus Conﬁguration Corruption.
ServiceX experienced continuous performance problem
in one datacenter in January of 2012. During the oc-
currence of this issue, customers experienced both slow
response and failed to upload ﬁles in an unpredictable
fashion. Operators who ﬁrst diagnosed this issue found
that one Web Front End (WFE), named WFEx, “pro-
duced” most http-status = 500 failures. Since each
transaction instance randomly selected one WFE to
be served due to the load-balancing strategy, only the
transaction instances that go through WFEx would have
a high probability to fail. The operators asserted that
WFEx went into a bad state, so they rebooted it after
investigation. However, such rebooting action did not
turn back the service availability, and the same issue
existed continuously. After involving senior experts,
they ﬁnally found the root cause of the issue to be
that the “conﬁguration ﬁle for antivirus software became
corrupted after a random restart”. In the end, the only
resolution was to re-image.
Resolving this issue is challenging,
involving 12
experts in different relevant teams and with mail discus-
sion in about two weeks to ﬁnd the root cause by inves-
tigating various logs of different components/features.
This issue is also a tricky one since people usually re-
boot a WFE after general diagnosis. Our approach ﬁnds
the symptoms (denote as ACC: Antivirus Conﬁguration
Corruption) “antivirus timeout” (which led to “Internal
Server Error”, reﬂecting that users failed to upload ﬁles)
and “SQL failing over detected” (which led to long
latency, reﬂecting that users felt slow response) on only
WFEx. We recorded this issue in the repository. On
early February, 2012, a new issue X occurred in another
farm of the same datacenter, our approach retrieved the
historical issue with ACC as the most similar issue, with
the similarity score of 0.96. Guided by the information
of the historical issue and its healing suggestion, the op-
erator, who was not familiar with this issue, immediately
moved to check the antivirus conﬁgurations instead of
rebooting the WFE (a common healing action). Our
approach helped reduce much investigation time of
the operator by providing informative diagnostic clues.
After repeated occurrences of such issue with the ACC
symptom, the issue was ﬁnally marked as a “need ﬁx”
issue of antivirus software, and is to be ﬁxed in the
future upcoming service upgrade.
False Alarms of Monitoring System.
We provide another interesting story that occurred after
we conducted the evaluation. This story demonstrates
that not only the healing actions, but also the informa-
tion about descriptions and diagnostic steps of similar
historical issues could be leveraged to help investigate
current issues.
Starting from early July of 2012 till the end of the
month,
the monitoring system of ServiceX sent out
120 speciﬁc types of alarms (service issues). These
issues were not easy to diagnose. According to the
email discussion from the involved operators, and also
the information recorded in the issue repository, the
involved operators did observe some new suspicious
transaction logs in the effected machine; however, these
machines seemed running healthily without any ab-
normal behavior. To be conservative, operators had to
inspect the potential dependent machines one by one.
After hours of investigation, operators had no ﬁndings,
and had to temporally tag “no clue, postponed” to the
issue in the issue repository.
This challenging issue had been resolved about three
weeks later with more than ten experts involved. The
root cause was that the monitoring system reported a
lot of false alarms due to the incompatible versions of
components (of the monitoring system) after the previ-
ous upgrade. All the 120 false issues were then marked
as “duplicated” in the repository. Similar issues never
occurred again after the subsequent service upgrade.
We captured and replayed the whole story by sim-
ulation. According to our simulation study, we found
that if the ﬁrst three issues (which recorded detailed
descriptions and diagnostic steps, and occurred in the
ﬁrst two days) were labeled as a similar group, all the
remaining 117 issues could be retrieved correctly (i.e.,
for each unlabeled issue, the most similar one is one
of the three, and with the similarity-metric value >
0.90). Although there were no healing actions associated
with these issues, the rich information of the previous
investigations (i.e., those for the ﬁrst three issues) can
substantially reduce redundant efforts for diagnosing the
large number of recurrent issues.
VI. RELATED WORK
We discuss related work in the areas of system diag-
nosis, fault detection, and mining software repositories.
System diagnosis. Cohen et al. [10] propose that re-
trieving a previously solved and annotated issue similar
to the given new issue may help identify the root cause
or ﬁxing action for the given issue when the retrieval is
accurate. In contrast, rather than aiming to ﬁx the issue,
our work aims to provide healing suggestion to reduce
MTTR by leveraging historical issues. Yuan et al. [11]
use classiﬁcation techniques to classify issues into dif-
ferent categories. In contrast, our work does not require
speciﬁc labeling but retrieves similar historical issues
for adapting their healing actions as suggested healing
321321321
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 
actions for the given issues. Previous work [12], [13]
on automated diagnosis of distributed systems uses two
types of trace data for analyzing system performance:
system metrics/events and transaction logs. Our work
requires only transaction logs for signature generation
and healing suggestion.
Fault localization. Our technique used to generate
signature from transaction logs shares a similar high-
level concept with the fault-localization technique pro-
posed by Liu et al. [14]. Sun et al. [15] evaluate patterns
mined from both correct and incorrect runs to detect
duplicate bug reports. Our work uses contrast informa-
tion for achieving high accuracy of signature generation.
Cellier [16] applied FCA for fault localization by using
concepts to ﬁnd interesting clusters. In contrast to these
previous techniques on fault localization based on cov-
erage information, our work is motivated by addressing
challenges posed by characteristics of transaction logs.
Mining bug repositories. When a new software fault
is reported, operators usually use a web search engine
to search for the text of error messages or console
messages for diagnosis. The essence of such scenario
remains when the web search engine is replaced by a
search engine for a bug repository. Ashok et al. [17]
implement a search tool to speed up bug ﬁxing by
leveraging natural language text, dumped traces, and
bug output. Some other work [15], [18] uses mining
or classiﬁcation techniques on textual information to
cluster or detect duplicate bug reports. These techniques
would not be effective in our problem setting because
the textual
issue
repository is incomplete or imprecise.
information in a typical historical
VII. CONCLUSION
To effectively reduce the MTTR of a service, we
have proposed an automated mining-based approach for
suggesting an appropriate healing action for a given
issue. Our approach suggests an appropriate healing
action by adapting healing actions for the retrieved
similar historical issues. Our studies on a real-world
product online service show that our approach can
effectively provide appropriate healing actions to reduce
the MTTR of the service.
REFERENCES
[1] D. D. E. Long, A. Muir, and R. A. Golding, “A longitu-
dinal survey of internet host reliability,” in Proc. SRDS,
1995, pp. 2–9.
[2] W. Xie, H. Sun, Y. Cao, and K. Trivedi, “Modeling of
online service availability perceived by web users,” in
Proc. GLOBECOM, 2001.
[3] D. Cannon and D. Wheeldon, “The stationery ofﬁce,” in
ITIL Service Operation, 2009.
322322322
[4] G. Bernhard, S. Gerd, and W. Rudolf, “Formal concept
analysis: Foundations and applications,” in LNCS, 2005.
[5] R. Ding, J. J. Shen, Q. Fu, J. G. Lou, Q. Lin, D. Zhang,
and T. Xie, “Healing online service systems via mining
historical issue repositories,” in Proc. ASE, 2012.
[6] S. Kandula, R. Chandra, and D. Katabi, “What’s going
on?: Learning communication rules in edge networks,”
in Proc. SIGCOMM, 2008, pp. 87–98.
Feb.)
[7] (2014,
line].
US/people/juding/kb.aspx
Available:
Appendant
[On-
http://research.microsoft.com/en-
materials.
[8] G. Tsatsaronis and V. Panagiotopoulou, “A generalized
vector space model for text retrieval based on semantic
relatedness,” in Proc. EACL, 2009, pp. 70–78.
[9] H. Wu, R. Luk, K. Wong, and K. Kwok, “Interpreting
tf-idf term weights as making relevance decisions,” in
TOIS, 2008, pp. 1–37.
[10] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly,
and A. Fox, “Capturing, indexing, clustering, and retriev-
ing system history,” in Proc. SOSP, 2005, pp. 105–118.
[11] C. Yuan, N. Lao, J.-R. Wen, J. Li, Z. Zhang, Y.-M. Wang,
and W.-Y. Ma, “Automated known problem diagnosis
with event traces,” in EuroSys, 2006, pp. 375–388.
[12] I. Cohen, J. S. Chase, M. Goldszmidt, T. Kelly, and
J. Symons, “Correlating instrumentation data to system
states: A building block for automated diagnosis and
control,” in Proc. OSDI, 2004, pp. 231–244.
[13] P. Bodik, M. Goldszmidt, A. Fox, D. B. Woodard, and
H. Andersen, “Fingerprinting the datacenter: automated
classiﬁcation of performance crises,” in Proc. EuroSys,
2010, pp. 111–124.
[14] C. Liu, X. Yan, L. Fei, J. Han, and S. Midkiff,
“Sober: statistical model-based bug localization,” in
Proc. ESEC/FSE, 2005, pp. 286–295.
[15] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo, “A
discriminative model approach for accurate duplicate bug
report retrieval,” in Proc. ICSE, 2010, pp. 45–54.
[16] P. Cellier, “Formal concept analysis applied to fault
localization,” in Proc. ICSE Companion’08, 2008, pp.
991–994.
[17] B. Ashok, J. M. Joy, H. Liang, S. K. Rajamani, G. Srini-
vasa, and V. Vangala, “Debugadvisor: A recommender
system for debugging,” in Proc. ESEC/FSE, 2009, pp.
373–382.
[18] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun, “An
approach to detecting duplicate bug reports using natural
language and execution information,” in Proc. ICSE,
2008, pp. 461–470.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply.