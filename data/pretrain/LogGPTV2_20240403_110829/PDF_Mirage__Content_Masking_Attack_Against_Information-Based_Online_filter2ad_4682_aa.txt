title:PDF Mirage: Content Masking Attack Against Information-Based Online
Services
author:Ian D. Markwood and
Dakun Shen and
Yao Liu and
Zhuo Lu
PDF Mirage: Content Masking Attack Against 
Information-Based Online Services
Ian Markwood, Dakun Shen, Yao Liu, and Zhuo Lu, University of South Florida
https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/markwood
This paper is included in the Proceedings of the 26th USENIX Security SymposiumAugust 16–18, 2017 • Vancouver, BC, CanadaISBN 978-1-931971-40-9Open access to the Proceedings of the 26th USENIX Security Symposium is sponsored by USENIXPDF Mirage: Content Masking Attack
Against Information-Based Online Services
Ian Markwood∗†, Dakun Shen∗†, Yao Liu†, and Zhuo Lu†
†University of South Florida, Tampa, FL, U.S.A
*Co-First Authors
Abstract
We present a new class of content masking attacks
against the Adobe PDF standard, causing documents to
appear to humans dissimilar to the underlying content ex-
tracted by information-based services. We show three at-
tack variants with notable impact on real-world systems.
Our ﬁrst attack allows academic paper writers and re-
viewers to collude via subverting the automatic reviewer
assignment systems in current use by academic confer-
ences including INFOCOM, which we reproduced. Our
second attack renders ineffective plagiarism detection
software, particularly Turnitin, targeting speciﬁc small
plagiarism similarity scores to appear natural and evade
detection.
In our ﬁnal attack, we place masked con-
tent into the indexes for Bing, Yahoo!, and DuckDuckGo
which renders as information entirely different from the
keywords used to locate it, enabling spam, profane, or
possibly illegal content to go unnoticed by these search
engines but still returned in unrelated search results.
Lastly, as these systems eschew optical character recog-
nition (OCR) for its overhead, we offer a comprehensive
and lightweight alternative mitigation method.
1
Introduction
Designed as a solution for displaying formatted infor-
mation consistently on computers with myriad hardware
and software conﬁgurations, Adobe’s Portable Docu-
ment Format (PDF) has become the standard for elec-
tronic documents. Academic and collegiate papers, busi-
ness write-ups and fact sheets, advertisements for print,
and anything else meant to be viewed as a ﬁnal product
make use of the PDF standard. Indeed, there is an ele-
ment of constancy implied in the creation of a PDF doc-
ument. End users cannot easily change the text of a PDF
document, so most come to expect a degree of integrity
present in all PDF documents encountered.
Attacks are studied and corresponding defenses devel-
oped dealing with arbitrary code execution through some
allowances made by Adobe to execute JavaScript within
the rendering process of a PDF ﬁle [1] [2] or from other
rendering vulnerabilities [3] [4]. These typically allow
data exﬁltration, botnet creation, or other objectives un-
related to the PDF ﬁle itself aside from using it as a de-
livery mechanism [5] [6] [7] [8]. We present a class of
attacks against the content integrity of PDF documents
themselves, and following this, describe and test a com-
prehensive defense method against these attacks. With-
out changing the appearance of a PDF, we are able to
alter how several information-based services see it, with
the following implications:
1. We demonstrate how academic paper writers can
collude with multiple conference reviewers, by altering
a paper invisibly to humans, to be assigned to those re-
viewers by automatic reviewer assignment systems, such
as that used by the IEEE International Conference on
Computer Communications (INFOCOM) [9] that openly
publishes its automated algorithm. We simulate this re-
viewer assignment system using 100 sample academic
papers and a corpus of 2094 papers from 114 reviewers
of a past security conference, ﬁnding that we can cause
any of said sample papers to match with any reviewer.
2. We show how an unethical student can invisibly
alter a document to avoid plagiarism detection, namely
the dominant market share Turnitin [10], and general-
ize methods to target speciﬁc small plagiarism similarity
scores to simulate the few false positives such systems
typically detect. We illustrate this attack by inducing pla-
giarism scores, as measured by Turnitin, from 0-100% in
10 academic papers without changing their appearance.
3. Lastly, we show real-world examples of mak-
ing leading search engines display arbitrary (potentially
spam, offensive material, etc.) results for innocuous key-
words. We have successfully caused Bing, Yahoo!, and
DuckDuckGo to index ﬁve documents under keywords
not displayed in those documents.
These systems have in common the need to scrape
USENIX Association
26th USENIX Security Symposium    833
PDFs for their content for further processing or search-
ing within. Online conference paper or other document
repositories and companies that index the Internet re-
quire text from PDFs so they may be located via search.
Natural language processing tools scrape PDFs to dis-
cover the topics within, and this information is used in
several large conferences to assign unpublished work to
conference reviewers as well as in document repositories
to categorize large volumes of works without manual ef-
fort. Finally, plagiarism checkers require text from new
articles for comparison against currently published work
to detect impermissible similarity.
Scraping of PDF documents can be done in an au-
tomated setting by text extraction tools such as the
PDFMiner package [11]. However, fonts of any name
may be embedded in the PDF document, and these tools
cannot check the fonts’ authenticity. A font is actu-
ally akin to an encoding mechanism, which maps keys
pressed on a keyboard to glyphs representing those keys.
Without some way to check the validity of fonts in a PDF,
which glyphs a font maps keys to is arbitrary. Moreover,
humans reading a PDF read the rendered version of what
a tool such as PDFMiner reads, meaning that machines
and humans are on opposite ends of this encoding mech-
anism and may be caused to read different information.
Consequently, the various PDF document scraping en-
vironments may be misused through the remapping of
keys to arbitrary rendered glyphs. Using one or more
custom fonts, an attacker may cause a word to be ren-
dered as another word by switching the glyph mapping
within the font ﬁle, or rather change the underlying text
while keeping a constant rendered output. That is to
say, in a document containing the word “kind” an at-
tacker may force that to be rendered as “mean” with a
custom font mapping k to m, i to e, n to a, and d to n,
so the human now sees “mean” while the machine still
sees “kind”; or to avoid human detection an attacker can
change the underlying text to “mean” and use a font with
the reverse mapping to render it as “kind” for the human
to see. The latter tactic subverts aforementioned end ap-
plications, while still rendering PDFs in all appearances
normal to humans. We refer to this as a content masking
attack, as humans are caused to view a masked version
of the content these computer systems read.
To assign papers to reviewers for a conference, several
large conferences employ automated systems to com-
pare the subject paper with a corpus of papers written
by each reviewer to ﬁnd the best match. This matching
is executed upon the most important topics, or keywords,
found in the paper via natural language processing meth-
ods. If an author replaces the keywords of a paper with
those of a reviewer’s paper, a high match is guaranteed,
and the two may thereby collude. By creating custom
glyph mappings for characters, the masked paper can
make perfect sense to the human eye, while the underly-
ing text read by the machine has many substituted words
which would not make sense to a human reader. This
exploit has the technical challenges of replacing words
of differing lengths (larger and smaller replacements re-
quire different methods) and also constructing multiple
fonts required for different mappings of the same letter
(for example, to map the word “green” to “brown” re-
quires two different font mappings for e). A naive de-
fense could check the number of fonts embedded, so in
Section 4 we design algorithms to minimize the num-
ber of auxiliary fonts used, in order to avoid detection.
To evaluate, we construct our own automatic reviewer
assignment system reproducing the current INFOCOM
system [9], and show that for 100 test papers, targeting
a speciﬁc reviewer is possible by masking 4-9 unique
words in most papers and no more than 12 for all tested.
This content masking attack also undermines plagia-
rism detection. In this case, we need only switch out iso-
lated characters to change plagiarized text to text never
written before, while again masking these changes as the
original text to the human reader. In fact, as most pa-
pers have a small (false positive) percentage of similarity
present due to common phrases within the English lan-
guage, this method simulates that by varying the number
of characters changed, to simulate the usual small but
nonzero plagiarism percentage. Only one font is required
to make this mapping, as the resultant text does not need
to make sense to the plagiarism detector. Thus, say, all
rendered e’s may be represented by some other letter in
a font that maps that key to the glyph e, and other letters
may be changed similarly, building a one-to-one map-
ping covering at most all letters. The challenge is to tar-
get a small plagiarism percentage, but accomplishing that
as we do in Section 5, a single embedded font bearing the
name of a popular font will cause no suspicion.
Finally, search engines and document repositories may
be subverted to display unexpected content also. Here,
we may replace the entire text of a PDF without changing
the rendered view, with a variety of implications. One
may hide advertisements in academic papers or business
fact sheets, for example, to spam users searching for in-
formation. In this exploit, the attacker should replace an
entire document with the fewest number of fonts neces-
sary, to avoid seeming particularly unusual. This must
be done in a different way than for the topic matching
exploit, due to changing the entire document rather than
a few words, so we outline another method in Section 6.
We then test it on popular search engines, ﬁnding that
Yahoo!, Bing, and DuckDuckGo are susceptible.
Having enumerated these vulnerabilities, as these sys-
tems eschew optical character recognition (OCR) for its
overhead, we offer a comprehensive and lightweight al-
ternative mitigation method in Section 7. While a naive
834    26th USENIX Security Symposium
USENIX Association
method would perform OCR over the full document, we
instead render the unique characters used within the doc-
ument and perform OCR on these. This font veriﬁcation
method has several technical challenges in its implemen-
tation, due to the number and variety of glyphs within
font ﬁles, and all these issues are overcome in the algo-
rithm we provide. We ﬁnd it performs at a roughly con-
stant speed regardless of document length (a tenth of that
for full document OCR at 10 pages), with glyph distinc-
tion accuracy just under 100%, and with 100% content
masking attack detection rate.
2 Background Information
PDF Text Extraction: The Adobe PDF standard con-
tains eight basic types of objects,
including strings.
Strings house the text in a document, including plain text,
octal or hexadecimal representations of plain text, or text
with some type of encoding [12]. PDF rendering soft-
ware treats each string as a series of character identiﬁers
(CIDs), each mapping to its corresponding glyph within
the font associated with that string via the Character Map
(CMap) [13]. A series of glyphs is thus displayed.
Text information extracted from PDF ﬁles by using
tools like the Python package PDFMiner. These tools ex-
tract text by copying the plaintext from all string objects
in a PDF ﬁle. Though these tools can extract the font
name for each string as well, a whitelist will not defend
against this attack, as fonts may be given any name.
Topic Matching: The exponential growth of human
knowledge/record keeping and the ease of its access de-
mands an efﬁcient means of providing context-relevant
search results, stemming the research ﬁeld of natural lan-
guage processing. This ﬁeld extracts the speciﬁc subject
of a document without the need for human classiﬁcation.
The ultimate goal of useful search results prompts the
companion research ﬁeld of matching keywords to top-
ics which has been tackled by the leading search engines.
Latent Semantic Indexing (LSI) is a popular natu-
ral language processing algorithm for extracting topics
from documents. The LSI approach infers synonymous
words/phrases to be those with similar surrounding con-
texts, rather than constructing a thesaurus. These de-
tected patterns can allow singular value decomposition
to reduce the number of important words in a document
such that it may be represented by a small subset. This
small subset, of cardinality k, then contains frequency
data for each element, such that the document may be
represented by a dot in k-space. Similarity between doc-
uments is easily calculated via their Euclidean distances
apart in this geometric representation [14].
Latent Dirichlet Allocation is a newer popular topic
extraction algorithm, which is generally speaking a prob-
abilistic extension of LSI [9]. Topics are generated as
collections of related words, using supervised learning.
The probability of a document corresponding to each of
the predeﬁned topics is calculated based on how well
the words within the document correspond to the words
within each topic [15, 16].
Topic matching is used within the automation of the
review assignment process for several large conferences,
such as the ACM Conference on Computer and Commu-
nications Security (CCS) or the IEEE International Con-
ference on Computer Communications (INFOCOM).
These conferences receive many submissions and have
many reviewers, and the manual task of ﬁnding the most
suitable reviewers for each paper is onerous, so they au-
tomate by comparing topics extracted from subject pa-
pers and papers published by reviewers. The authors
of [9] execute a performance comparison between LSI
and LDA for use in the present (as of 2016) INFOCOM
reviewer assignment system, which uses PDFMiner for
text extraction, ﬁnding LSI to work well with the aca-
demic papers submitted to that conference. We accord-
ingly perform our experiments using LSI to determine
the important keywords of each paper, and note that the
attack functions equivalently using LDA.
Plagiarism Detection: Turnitin, LLC has the domi-
nant market share for plagiarism detection software. Its
software is proprietary, but current documentation states
“Turnitin will not accept PDF image ﬁles, forms, or port-
folios, ﬁles that do not contain highlightable text...” [10],
indicating that PDFMiner or some similar internally de-
veloped tool is used to scrape the text from PDF docu-
ments. We may assume from the lack of support for im-
age ﬁles that optical character recognition (OCR) is not
used, meaning that our proposed attack should succeed,
which is proved in Section 5.2.
Additionally, the Turnitin documentation states that
“All document data must be encoded using UTF-8 char-