[9] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov. 2020. How To Backdoor Federated Learning. In Proceedings of the
Twenty Third International Conference on Artificial Intelligence and Statistics (Pro-
ceedings of Machine Learning Research), Silvia Chiappa and Roberto Calandra
(Eds.), Vol. 108. PMLR, Online, 2938–2948. http://proceedings.mlr.press/v108/
bagdasaryan20a.html
[10] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
2019. Analyzing Federated Learning through an Adversarial Lens (Proceedings
of Machine Learning Research), Kamalika Chaudhuri and Ruslan Salakhutdinov
(Eds.), Vol. 97. PMLR, Long Beach, California, USA, 634–643. http://proceedings.
mlr.press/v97/bhagoji19a.html
[11] K. A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloé M Kiddon, Jakub Konečný, Stefano Mazzocchi,
Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and
Jason Roselander. 2019. Towards Federated Learning at Scale: System Design. In
SysML 2019. https://arxiv.org/abs/1902.01046 To appear.
[12] Brendan McMahan, Ramesh Raskar, Otkrist Gupta, Praneeth Vepakomma, Hassan
Takabi, Jakub Konečný. 2019. CVPR Tutorial On Distributed Private Machine
Learning for Computer Vision: Federated Learning, Split Learning and Beyond.
https://nopeekcvpr.github.io. (2019).
[13] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
(2020). arXiv:cs.CL/2005.14165
[14] Adrian Bulat and Georgios Tzimiropoulos. 2017. How far are we from solving the
2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks).
In International Conference on Computer Vision.
[15] Iker Ceballos, Vivek Sharma, Eduardo Mugica, Abhishek Singh, Alberto Ro-
man, Praneeth Vepakomma, and Ramesh Raskar. 2020. SplitNN-driven Vertical
Partitioning. (2020). arXiv:cs.LG/2008.04137
[16] Adam Coates, Andrew Ng, and Honglak Lee. 2011. An Analysis of Single-Layer
Networks in Unsupervised Feature Learning. In Proceedings of the Fourteenth
International Conference on Artificial Intelligence and Statistics (Proceedings of
Machine Learning Research), Geoffrey Gordon, David Dunson, and Miroslav Dudík
(Eds.), Vol. 15. PMLR, Fort Lauderdale, FL, USA, 215–223. http://proceedings.mlr.
press/v15/coates11a.html
[17] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local Model
Poisoning Attacks to Byzantine-Robust Federated Learning. In 29th USENIX
Security Symposium (USENIX Security 20). USENIX Association, 1605–1622. https:
//www.usenix.org/conference/usenixsecurity20/presentation/fang
[18] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion
Attacks That Exploit Confidence Information and Basic Countermeasures. In
Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security (CCS ’15). Association for Computing Machinery, New York, NY, USA,
1322–1333. https://doi.org/10.1145/2810103.2813677
[19] David Froelicher, Juan R. Troncoso-Pastoriza, Apostolos Pyrgelis, Sinem Sav,
Joao Sa Sousa, Jean-Philippe Bossuat, and Jean-Pierre Hubaux. 2020. Scalable
Privacy-Preserving Distributed Learning. (2020). arXiv:cs.CR/2005.09532
[20] Clement Fung, Chris J. M. Yoon, and Ivan Beschastnikh. 2020. The Limitations
of Federated Learning in Sybil Settings. In 23rd International Symposium on
Research in Attacks, Intrusions and Defenses (RAID 2020). USENIX Association, San
Sebastian, 301–316. https://www.usenix.org/conference/raid2020/presentation/
fung
[21] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov. 2018. Prop-
erty Inference Attacks on Fully Connected Neural Networks Using Permutation
Invariant Representations. In Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security (CCS ’18). Association for Computing Ma-
chinery, New York, NY, USA, 619–633. https://doi.org/10.1145/3243734.3243834
[22] Y. Gao, M. Kim, S. Abuadbba, Y. Kim, C. Thapa, K. Kim, S. A. Camtep, H. Kim, and
S. Nepal. 2020. End-to-End Evaluation of Federated Learning and Split Learning
for Internet of Things. In 2020 International Symposium on Reliable Distributed
Systems (SRDS). 91–100. https://doi.org/10.1109/SRDS51746.2020.00017
[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial
Nets. In Advances in Neural Information Processing Systems, Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (Eds.), Vol. 27. Curran
Associates, Inc., 2672–2680.
[24] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
Aaron C Courville. 2017. Improved Training of Wasserstein GANs. In Advances
in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran
Associates, Inc., 5767–5777.
[25] Otkrist Gupta and Ramesh Raskar. 2018. Distributed learning of deep neural
network over multiple agents. Journal of Network and Computer Applications 116
(2018), 1 – 8. https://doi.org/10.1016/j.jnca.2018.05.003
[26] David Gutman, Noel C. F. Codella, M. Emre Celebi, Brian Helba, Michael A.
Marchetti, Nabin K. Mishra, and Allan Halpern. 2016. Skin Lesion Analysis toward
Melanoma Detection: A Challenge at the International Symposium on Biomedical
Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration
(ISIC). CoRR abs/1605.01397 (2016). arXiv:1605.01397 http://arxiv.org/abs/1605.
01397
[27] M. Hao, H. Li, X. Luo, G. Xu, H. Yang, and S. Liu. 2020. Efficient and Privacy-
Enhanced Federated Learning for Industrial Artificial Intelligence. IEEE Transac-
tions on Industrial Informatics 16, 10 (2020), 6532–6542. https://doi.org/10.1109/
TII.2019.2945367
[28] K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep Residual Learning for Image
Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). 770–778. https://doi.org/10.1109/CVPR.2016.90
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2125[29] Zecheng He, Tianwei Zhang, and Ruby B. Lee. 2019. Model Inversion Attacks
against Collaborative Inference. In Proceedings of the 35th Annual Computer Secu-
rity Applications Conference (ACSAC ’19). Association for Computing Machinery,
New York, NY, USA, 148–162. https://doi.org/10.1145/3359789.3359824
[30] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. 2017. Deep Models
Under the GAN: Information Leakage from Collaborative Deep Learning. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
Security (CCS ’17). Association for Computing Machinery, New York, NY, USA,
603–618. https://doi.org/10.1145/3133956.3134012
[31] J. Jeon and J. Kim. 2020. Privacy-Sensitive Parallel Split Learning. In 2020 Inter-
national Conference on Information Networking (ICOIN). 7–9. https://doi.org/10.
1109/ICOIN48656.2020.9016486
[32] J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang. 2019. Incentive Mechanism
for Reliable Federated Learning: A Joint Optimization Approach to Combining
Reputation and Contract Theory.
IEEE Internet of Things Journal 6, 6 (2019),
10700–10714. https://doi.org/10.1109/JIOT.2019.2940820
[33] J. Kim, Sungho Shin, Yeonguk Yu, Junseok Lee, and Kyoobin Lee. 2020. Multiple
Classification with Split Learning. ArXiv abs/2008.09874 (2020).
[34] Yusuke Koda, Jihong Park, Mehdi Bennis, Koji Yamamoto, Takayuki Nishio, and
Masahiro Morikura. 2019. One Pixel Image and RF Signal Based Split Learning
for MmWave Received Power Prediction (CoNEXT ’19 Companion). Association
for Computing Machinery, New York, NY, USA, 54–56. https://doi.org/10.1145/
3360468.3368176
[35] Jakub Konečný, H. Brendan McMahan, Daniel Ramage, and Peter Richtárik.
2016. Federated Optimization: Distributed Machine Learning for On-Device
Intelligence. (2016). arXiv:cs.LG/1610.02527
[36] Jakub Konečný, H. Brendan McMahan, Felix X. Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon. 2017. Federated Learning: Strategies
for Improving Communication Efficiency. (2017). arXiv:cs.LG/1610.05492
[37] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015.
Human-level concept learning through probabilistic program induction. Sci-
ence 350, 6266 (2015), 1332–1338.
https://doi.org/10.1126/science.aab3050
arXiv:https://science.sciencemag.org/content/350/6266/1332.full.pdf
[38] M. Langer, Z. He, W. Rahayu, and Y. Xue. 2020. Distributed Training of Deep
Learning Models: A Taxonomic Perspective. IEEE Transactions on Parallel and
Distributed Systems 31, 12 (2020), 2802–2818. https://doi.org/10.1109/TPDS.2020.
3003307
[39] Wei Yang Bryan Lim, Jer Shyuan Ng, Zehui Xiong, Dusit Niyato, Cyril Leung,
Chunyan Miao, and Qiang Yang. 2020. Incentive Mechanism Design for Resource
Sharing in Collaborative Edge Learning. (2020). arXiv:cs.NI/2006.00511
[40] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face
Attributes in the Wild. In Proceedings of International Conference on Computer
Vision (ICCV).
[41] Kamalesh Palanisamy, Vivek Khimani, Moin Hussain Moti, and D. Chatzopoulos.
2020. SplitEasy: A Practical Approach for Training ML models on Mobile Devices
in a split second. ArXiv abs/2011.04232 (2020).
[42] Maarten G. Poirot, Praneeth Vepakomma, Ken Chang, Jayashree Kalpathy-
Cramer, Rajiv Gupta, and Ramesh Raskar. 2019. Split Learning for collaborative
deep learning in healthcare. (2019). arXiv:cs.LG/1912.12115
[43] Alec Radford, Luke Metz, and Soumith Chintala. 2016. Unsupervised Represen-
tation Learning with Deep Convolutional Generative Adversarial Networks. In
4th International Conference on Learning Representations, ICLR 2016, San Juan,
Puerto Rico, May 2-4, 2016, Conference Track Proceedings, Yoshua Bengio and Yann
LeCun (Eds.). http://arxiv.org/abs/1511.06434
[44] Daniele Romanini, Adam James Hall, Pavlos Papadopoulos, Tom Titcombe, Abbas
Ismail, Tudor Cebere, Robert Sandmann, Robin Roehm, and Michael A. Hoeh.
2021. PyVertical: A Vertical Federated Learning Framework for Multi-headed
SplitNN. In ICLR 2021 Workshop on Distributed and Private Machine Learning.
[45] F. Sattler, S. Wiedemann, K. R. Müller, and W. Samek. 2020. Robust and
Communication-Efficient Federated Learning From Non-i.i.d. Data. IEEE Trans-
actions on Neural Networks and Learning Systems 31, 9 (2020), 3400–3413. https:
//doi.org/10.1109/TNNLS.2019.2944481
[46] Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang, Jayashree
Kalpathy-Cramer, and Ramesh Raskar. 2019. ExpertMatcher: Automating
ML Model Selection for Clients using Hidden Representations.
(2019).
arXiv:cs.CV/1910.03731
[47] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-Preserving Deep Learning. In
Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security (CCS ’15). Association for Computing Machinery, New York, NY, USA,
1310–1321. https://doi.org/10.1145/2810103.2813687
[48] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. 2017. Membership Inference
Attacks Against Machine Learning Models. In 2017 IEEE Symposium on Security
and Privacy (SP). 3–18. https://doi.org/10.1109/SP.2017.41
[49] Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar. 2019.
Detailed comparison of communication efficiency of split learning and federated
learning. (2019). arXiv:cs.LG/1909.09145
[50] Gabor Szekely, Maria Rizzo, and Nail Bakirov. 2008. Measuring and Testing
Dependence by Correlation of Distances. The Annals of Statistics 35 (04 2008).
https://doi.org/10.1214/009053607000000505
[51] Chandra Thapa, M. A. P. Chamikara, and Seyit Camtepe. 2020. SplitFed: When
Federated Learning Meets Split Learning. (2020). arXiv:cs.LG/2004.12088
[52] Chandra Thapa, M. A. P. Chamikara, and Seyit A. Camtepe. 2020. Advancements
of federated learning towards privacy preservation: from federated learning to
split learning. (2020). arXiv:cs.LG/2011.14818
[53] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. 2018. The HAM10000
dataset, a large collection of multi-source dermatoscopic images of common
pigmented skin lesions. Scientific Data 5, 1 (2018), 180161.
[54] Valeria Turina, Zongshun Zhang, Flavio Esposito, and Ibrahim Matta. 2020. Com-
bining Split and Federated Architectures for Efficiency and Privacy in Deep Learn-
ing. In Proceedings of the 16th International Conference on Emerging Networking
EXperiments and Technologies (CoNEXT ’20). Association for Computing Machin-
ery, New York, NY, USA, 562–563. https://doi.org/10.1145/3386367.3431678
[55] Praneeth Vepakomma, Otkrist Gupta, Abhimanyu Dubey, and Ramesh Raskar.
2019. Reducing leakage in distributed deep learning for sensitive health data. (05
2019).
[56] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. 2018.
Split learning for health: Distributed deep learning without sharing raw patient
data. (2018). arXiv:cs.LG/1812.00564
[57] Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta, and
Abhimanyu Dubey. 2018. No Peek: A Survey of private distributed deep learning.
(2018). arXiv:cs.LG/1812.03288
[58] Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta, and
Abhimanyu Dubey. 2018. No Peek: A Survey of private distributed deep learning.
(2018). arXiv:cs.LG/1812.03288
[59] C. Wang, X. Wei, and P. Zhou. 2020. Optimize Scheduling of Federated Learning
on Battery-powered Mobile Devices. In 2020 IEEE International Parallel and
Distributed Processing Symposium (IPDPS). 212–221. https://doi.org/10.1109/
IPDPS47924.2020.00031
[60] Jiayu Wu, Qixiang Zhang, and Guoxi Xu. 2020. Tiny ImageNet Challenge. http:
//cs231n.stanford.edu/reports/2017/pdfs/930.pdf. (2020).
[61] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST: a Novel
(2017).
Image Dataset for Benchmarking Machine Learning Algorithms.
arXiv:cs.LG/1708.07747
[62] Song Yang Zhang, Zhifei and Hairong Qi. 2017. Age Progression/Regression by
Conditional Adversarial Autoencoder. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). IEEE.
[63] Y. Zhang, R. Jia, H. Pei, W. Wang, B. Li, and D. Song. 2020. The Secret Revealer:
Generative Model-Inversion Attacks Against Deep Neural Networks. In 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 250–258.
https://doi.org/10.1109/CVPR42600.2020.00033
[64] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep Leakage from Gra-
dients. In Advances in Neural Information Processing Systems, H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.),
Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/
60a6c4002cc7b29142def8871531281a-Paper.pdf
APPENDICES
A ADDITIONAL RESULTS
In this section, we include and discuss additional results.
A.1 On the effect of the public dataset
Extending the results presented in Section 3.4.1, we test the FSHA
on other datasets.
Natural images. Here, we test the datasets TinyImageNet and
STL-10 [16]. TinyImageNet [60] is a subset of ImageNet containing
only 200 classes of natural images. STL-10, as TinyImageNet, is
defined over the natural domain, but it consists of only 10 different
classes (six animals and four vehicles). Note that, given the size of
TinyImageNet, the 10 classes of STL-10 can be considered a subset of
the 200 classes of TinyImageNet. However, there is no intersection
between the images of the two sets.
Next, we test the ability of FSHA to reconstruct instances of
TinyImageNet (𝑋𝑝𝑟𝑖𝑣) by using the STL-10 as 𝑋𝑝𝑢𝑏. Arguably, this
attack is particularly challenging as there is a strong discrepancy
between the public and private distributions. There are around
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2126Figure A.1: Random examples of inference of private training instances on the TinyImageNet dataset. The first row (i.e., gray
frame) reports the original data, the second row (i.e., red frame) depicts the attacker’s reconstruction using 𝑋𝑝𝑢𝑏 =TinyImageNet
(test) and the third row (i.e., blue frame) depicts the attacker’s reconstruction using 𝑋𝑝𝑢𝑏 =STL-10. We run the attacks for 2 · 103.
Figure A.2: Random examples of inference of private training instances on the HAM10000 dataset. The first row (i.e., gray
frame) reports the original data, the second row (i.e., red frame) depicts the attacker’s reconstruction using 𝑋𝑝𝑢𝑏 =HAM10000
(test) and the third row (i.e., blue frame) depicts the attacker’s reconstruction using 𝑋𝑝𝑢𝑏 =ISIC-2016. We run the attacks for
2 · 103.
190 unknown semantic classes of data (i.e., 95% of the private dis-
tribution) that the attacker has never observed. Nevertheless, as
shown in Figure A.1, besides altered colors and missing details, the
attack converges towards suitable reconstructions of the private
instances of the TinyImageNet set, threatening clients’ privacy also
in this difficult setup. Again, this result suggests that the FSHA can
generalize over the adopted public set and provide a representative
feature space that captures unknown clients’ private instances.
Medical images. Next, we report additional examples using der-
moscopic lesion images datesets such as HAM10000 [53] and ISIC-
2016 competition dataset (task 1) [26].
HAM10000 is an extensive collection of multi-source dermato-
scopic images of common pigmented skin lesions, containing 10015
images collected from different populations and acquired by differ-
ent modalities. ISIC-2016, similarly to HAM10000, collects dermato-