• The same application may entrust small amounts of authoritative metadata per‐taining to its blobs to a higher latency, less available, more costly Paxos-based ser‐vice such as Megastore [Bak11], [Lam98].• Certain clients of the application may cache some of that metadata locally and 	access blobs directly, shaving latency still further from the vantage point of users.
• Another application may keep metadata in Bigtable, sacrificing strong distributed 	consistency because its developers happened to be familiar with Bigtable.Such cloud applications face a variety of data integrity challenges at runtime, such as referential integrity between datastores (in the preceding example, Blobstore, Mega‐store, and client-side caches). The vagaries of high velocity dictate that schema changes, data migrations, the piling of new features atop old features, rewrites, and evolving integration points with other applications collude to produce an environ‐ment riddled with complex relationships between various pieces of data that no single engineer fully groks.To prevent such an application’s data from degrading before its users’ eyes, a system of out-of-band checks and balances is needed within and between its datastores.“Third Layer: Early Detection” on page 356 discusses such a system.In addition, if such an application relies on independent, uncoordinated backups of several datastores (in the preceding example, Blobstore and Megastore), then its abil‐ity to make effective use of restored data during a data recovery effort is complicated by the variety of relationships between restored and live data. Our example applica‐tion would have to sort through and distinguish between restored blobs versus live Megastore, restored Megastore versus live blobs, restored blobs versus restored Mega‐store, and interactions with client-side caches.4 Binary Large Object; see .
342  |  Chapter 26: Data Integrity: What You Read Is
In consideration of these dependencies and complications, how many resources should be invested in data integrity efforts, and where?
Backups Versus Archives
Traditionally, companies “protect” data against loss by investing in backup strategies. However, the real focus of such backup efforts should be data recovery, which distin‐guishes real backups from archives. As is sometimes observed: No one really wants to make backups; what people really want are restores.Is your “backup” really an archive, rather than appropriate for use in disaster recovery?
The most important difference between backups and archives is that backups can be loaded back into an application, while archives cannot. Therefore, backups and archives have quite differing use cases.Archives safekeep data for long periods of time to meet auditing, discovery, and com‐pliance needs. Data recovery for such purposes generally doesn’t need to complete within uptime requirements of a service. For example, you might need to retain financial transaction data for seven years. To achieve this goal, you could move accu‐mulated audit logs to long-term archival storage at an offsite location once a month. Retrieving and recovering the logs during a month-long financial audit may take a week, and this weeklong time window for recovery may be acceptable for an archive.On the other hand, when disaster strikes, data must be recovered from real backups quickly, preferably well within the uptime needs of a service. Otherwise, affected users are left without useful access to the application from the onset of the data integ‐rity issue until the completion of the recovery effort.It’s also important to consider that because the most recent data is at risk until safely backed up, it may be optimal to schedule real backups (as opposed to archives) to occur daily, hourly, or more frequently, using full and incremental or continuous (streaming) approaches.
Therefore, when formulating a backup strategy, consider how quickly you need to be able to recover from a problem, and how much recent data you can afford to lose.Data Integrity’s Strict Requirements  |  343
Requirements of the Cloud Environment in Perspective
Cloud environments introduce a unique combination of technical challenges:
• If the environment uses a mixture of transactional and nontransactional backup 	and restore solutions, recovered data won’t necessarily be correct.
• If services must evolve without going down for maintenance, different versions 	of business logic may act on data in parallel.• If interacting services are versioned independently, incompatible versions of dif‐ferent services may interact momentarily, further increasing the chance of acci‐dental data corruption or data loss.
In addition, in order to maintain economy of scale, service providers must provide only a limited number of APIs. These APIs must be simple and easy to use for the vast majority of applications, or few customers will use them. At the same time, the APIs must be robust enough to understand the following:• Data locality and caching
• Local and global data distribution
• Strong and/or eventual consistency
• Data durability, backup, and recovery
Otherwise, sophisticated customers can’t migrate applications to the cloud, and sim‐ple applications that grow complex and large will need complete rewrites in order to use different, more complex APIs.Problems arise when the preceding API features are used in certain combinations. If the service provider doesn’t solve these problems, then the applications that run into these challenges must identify and solve them independently.
Google SRE Objectives in Maintaining Data Integrity and AvailabilityWhile SRE’s goal of “maintaining integrity of persistent data” is a good vision, we thrive on concrete objectives with measurable indicators. SRE defines key metrics that we use to set expectations for the capabilities of our systems and processes through tests and to track their performance during an actual event.
344  |  Chapter 26: Data Integrity: What You Read IsData Integrity Is the Means; Data Availability Is the Goal
Data integrity refers to the accuracy and consistency of data throughout its lifetime. Users need to know that information will be correct and won’t change in some unex‐pected way from the time it’s first recorded to the last time it’s observed. But is such assurance enough?Consider the case of an email provider who suffered a weeklong data outage [Kinc09]. Over the space of 10 days, users had to find other, temporary methods of conducting their business with the expectation that they’d soon return to their estab‐lished email accounts, identities, and accumulated histories.Then, the worst possible news arrived: the provider announced that despite earlier expectations, the trove of past email and contacts was in fact gone—evaporated and never to be seen again. It seemed that a series of mishaps in managing data integrity had conspired to leave the service provider with no usable backups. Furious users either stuck with their interim identities or established new identities, abandoning their troubled former email provider.But wait! Several days after the declaration of absolute loss, the provider announced that the users’ personal information could be recovered. There was no data loss; this was only an outage. All was well!
Except, all was not well. User data had been preserved, but the data was not accessible by the people who needed it for too long.The moral of this example: From the user’s point of view, data integrity without expected and regular data availability is effectively the same as having no data at all.
Delivering a Recovery System, Rather Than a Backup SystemMaking backups is a classically neglected, delegated, and deferred task of system administration. Backups aren’t a high priority for anyone—they’re an ongoing drain on time and resources, and yield no immediate visible benefit. For this reason, a lack of diligence in implementing a backup strategy is typically met with a sympathetic eye roll. One might argue that, like most measures of protection against low-risk dangers, such an attitude is pragmatic. The fundamental problem with this lackadaisical strat‐egy is that the dangers it entails may be low risk, but they are also high impact. When your service’s data is unavailable, your response can make or break your service, product, and even your company.Instead of focusing on the thankless job of taking a backup, it’s much more useful, not to mention easier, to motivate participation in taking backups by concentrating on a task with a visible payoff: the restore! Backups are a tax, one paid on an ongoing basis for the municipal service of guaranteed data availability. Instead of emphasizing the
Google SRE Objectives in Maintaining Data Integrity and Availability  |  345tax, draw attention to the service the tax funds: data availability. We don’t make teams“practice” their backups, instead:
• Teams define service level objectives (SLOs) for data availability in a variety of 	failure modes.
• A team practices and demonstrates their ability to meet those SLOs.
Types of Failures That Lead to Data LossAs illustrated by Figure 26-1, at a very high level, there are 24 distinct types of failures when the 3 factors can occur in any combination. You should consider each of these potential failures when designing a data integrity program. The factors of data integ‐rity failure modes are as follows:
Root cause 
An unrecoverable loss of data may be caused by a number of factors: user action, operator error, application bugs, defects in infrastructure, faulty hardware, or site catastrophes.Scope 
Some losses are widespread, affecting many entities. Some losses are narrow and directed, deleting or corrupting data specific to a small subset of users.
Rate 
Some data losses are a big bang event (for example, 1 million rows are replaced by only 10 rows in a single minute), whereas some data losses are creeping (for example, 10 rows of data are deleted every minute over the course of weeks).Figure 26-1. The factors of data integrity failure modes
An effective restore plan must account for any of these failure modes occurring in any conceivable combination. What may be a perfectly effective strategy for guarding
346  |  Chapter 26: Data Integrity: What You Read Is
against a data loss caused by a creeping application bug may be of no help whatsoever when your colocation datacenter catches fire.A study of 19 data recovery efforts at Google found that the most common user-visible data loss scenarios involved data deletion or loss of referential integrity caused by software bugs. The most challenging variants involved low-grade corruption or deletion that was discovered weeks to months after the bugs were first released into the production environment. Therefore, the safeguards Google employs should be well suited to prevent or recover from these types of loss.To recover from such scenarios, a large and successful application needs to retrieve data for perhaps millions of users spread across days, weeks, or months. The applica‐tion may also need to recover each affected artifact to a unique point in time. This data recovery scenario is called “point-in-time recovery” outside Google, and “time-travel” inside Google.A backup and recovery solution that provides point-in-time recovery for an applica‐tion across its ACID and BASE datastores while meeting strict uptime, latency, scala‐bility, velocity, and cost goals is a chimera today!Solving this problem with your own engineers entails sacrificing velocity. Many projects compromise by adopting a tiered backup strategy without point-in-time recovery. For instance, the APIs beneath your application may support a variety of data recovery mechanisms. Expensive local “snapshots” may provide limited protec‐tion from application bugs and offer quick restoration functionality, so you might retain a few days of such local “snapshots,” taken several hours apart. Cost-effective full and incremental copies every two days may be retained longer. Point-in-time recovery is a very nice feature to have if one or more of these strategies support it.Consider the data recovery options provided by the cloud APIs you are about to use. Trade point-in-time recovery against a tiered strategy if necessary, but don’t resort to not using either! If you can have both features, use both features. Each of these fea‐tures (or both) will be valuable at some point.
Challenges of Maintaining Data Integrity Deep and WideIn designing a data integrity program, it’s important to recognize that replication and redundancy are not recoverability.
Scaling issues: Fulls, incrementals, and the competing forces of backups and restores
A classic but flawed response to the question “Do you have a backup?” is “We have something even better than a backup—replication!” Replication provides many bene‐fits, including locality of data and protection from a site-specific disaster, but it can’t protect you from many sources of data loss. Datastores that automatically sync multi‐Google SRE Objectives in Maintaining Data Integrity and Availability  |  347
ple replicas guarantee that a corrupt database row or errant delete are pushed to all of your copies, likely before you can isolate the problem.To address this concern, you might make nonserving copies of your data in some other format, such as frequent database exports to a native file. This additional meas‐ure adds protection from the types of errors replication doesn’t protect against—user errors and application-layer bugs—but does nothing to guard against losses intro‐duced at a lower layer. This measure also introduces a risk of bugs during data con‐version (in both directions) and during storage of the native file, in addition to possible mismatches in semantics between the two formats. Imagine a zero-day attack5 at some low level of your stack, such as the filesystem or device driver. Any copies that rely on the compromised software component, including the database exports that were written to the same filesystem that backs your database, are vulnerable.Thus, we see that diversity is key: protecting against a failure at layer X requires stor‐ing data on diverse components at that layer. Media isolation protects against media flaws: a bug or attack in a disk device driver is unlikely to affect tape drives. If we could, we’d make backup copies of our valuable data on clay tablets.6The forces of data freshness and restore completion compete against comprehensive protection. The further down the stack you push a snapshot of your data, the longer it takes to make a copy, which means that the frequency of copies decreases. At the database level, a transaction may take on the order of seconds to replicate. Exporting a database snapshot to the filesystem underneath may take 40 minutes. A full backup of the underlying filesystem may take hours.In this scenario, you may lose up to 40 minutes of the most recent data when you restore the latest snapshot. A restore from the filesystem backup might incur hours of missing transactions. Additionally, restoring probably takes as long as backing up, so actually loading the data might take hours. You’d obviously like to have the freshest data back as quickly as possible, but depending on the type of failure, that freshest and most immediately available copy might not be an option.Retention
Retention—how long you keep copies of your data around—is yet another factor to consider in your data recovery plans.
While it’s likely that you or your customers will quickly notice the sudden emptying of an entire database, it might take days for a more gradual loss of data to attract the
5 See .
6 Clay tablets are the oldest known examples of writing. For a broader discussion of preserving data for the long 	haul, see [Con96].348  |  Chapter 26: Data Integrity: What You Read Is
right person’s attention. Restoring the lost data in the latter scenario requires snap‐shots taken further back in time. When reaching back this far, you’ll likely want to merge the restored data with the current state. Doing so significantly complicates the restore process.
How Google SRE Faces the Challenges of Data IntegritySimilar to our assumption that Google’s underlying systems are prone to failure, we assume that any of our protection mechanisms are also subject to the same forces and can fail in the same ways and at the most inconvenient of times. Maintaining a guar‐antee of data integrity at large scale, a challenge that is further complicated by the high rate of change of the involved software systems, requires a number of comple‐mentary but uncoupled practices, each chosen to offer a high degree of protection on its own.The 24 Combinations of Data Integrity Failure Modes
Given the many ways data can be lost (as described previously), there is no silver bul‐let that guards against the many combinations of failure modes. Instead, you need defense in depth. Defense in depth comprises multiple layers, with each successive layer of defense conferring protection from progressively less common data loss sce‐narios. Figure 26-2 illustrates an object’s journey from soft deletion to destruction, and the data recovery strategies that should be employed along this journey to ensure defense in depth.The first layer is soft deletion (or “lazy deletion” in the case of developer API offer‐ings), which has proven to be an effective defense against inadvertent data deletion scenarios. The second line of defense is backups and their related recovery methods. The third and final layer is regular data validation, covered in “Third Layer: Early Detection” on page 356. Across all these layers, the presence of replication is occasion‐ally useful for data recovery in specific scenarios (although data recovery plans should not rely upon replication).Figure 26-2. An object’s journey from soft deletion to destruction
How Google SRE Faces the Challenges of Data Integrity  |  349
First Layer: Soft Deletion
When velocity is high and privacy matters, bugs in applications account for the vast majority of data loss and corruption events. In fact, data deletion bugs may become so common that the ability to undelete data for a limited time becomes the primary line of defense against the majority of otherwise permanent, inadvertent data loss.Any product that upholds the privacy of its users must allow the users to delete selected subsets and/or all of their data. Such products incur a support burden due to accidental deletion. Giving users the ability to undelete their data (for example, via a trash folder) reduces but cannot completely eliminate this support burden, particu‐larly if your service also supports third-party add-ons that can also delete data.Soft deletion can dramatically reduce or even completely eliminate this support bur‐den. Soft deletion means that deleted data is immediately marked as such, rendering it unusable by all but the application’s administrative code paths. Administrative code paths may include legal discovery, hijacked account recovery, enterprise administra‐tion, user support, and problem troubleshooting and its related features. Conduct soft deletion when a user empties his or her trash, and provide a user support tool that enables authorized administrators to undelete any items accidentally deleted by users. Google implements this strategy for our most popular productivity applications; otherwise, the user support engineering burden would be untenable.You can extend the soft deletion strategy even further by offering users the option to recover deleted data. For example, the Gmail trash bin allows users to access mes‐sages that were deleted fewer than 30 days ago.Another common source of unwanted data deletion occurs as a result of account hijacking. In account hijacking scenarios, a hijacker commonly deletes the original user’s data before using the account for spamming and other unlawful purposes. When you combine the commonality of accidental user deletion with the risk of data deletion by hijackers, the case for a programmatic soft deletion and undeletion inter‐face within and/or beneath your application becomes clear.Soft deletion implies that once data is marked as such, it is destroyed after a reason‐able delay. The length of the delay depends upon an organization’s policies and appli‐cable laws, available storage resources and cost, and product pricing and market positioning, especially in cases involving much short-lived data. Common choices of soft deletion delays are 15, 30, 45, or 60 days. In Google’s experience, the majority of account hijacking and data integrity issues are reported or detected within 60 days. Therefore, the case for soft deleting data for longer than 60 days may not be strong.Google has also found that the most devastating acute data deletion cases are caused by application developers unfamiliar with existing code but working on deletion-related code, especially batch processing pipelines (e.g., an offline MapReduce or Hadoop pipeline). It’s advantageous to design your interfaces to hinder developers
350  |  Chapter 26: Data Integrity: What You Read Isunfamiliar with your code from circumventing soft deletion features with new code. One effective way of achieving this is to implement cloud computing offerings that include built-in soft deletion and undeletion APIs, making sure to enable said feature.7 Even the best armor is useless if you don’t put it on.Soft deletion strategies cover data deletion features in consumer products like Gmail or Google Drive, but what if you support a cloud computing offering instead? Assuming your cloud computing offering already supports a programmatic soft dele‐tion and undeletion feature with reasonable defaults, the remaining accidental data deletion scenarios will originate in mistakes made by your own internal developers or your developer customers.In such cases, it can be useful to introduce an additional layer of soft deletion, which we will refer to as “lazy deletion.” You can think of lazy deletion as behind the scenes purging, controlled by the storage system (whereas soft deletion is controlled by and expressed to the client application or service). In a lazy deletion scenario, data that is deleted by a cloud application becomes immediately inaccessible to the application, but is preserved by the cloud service provider for up to a few weeks before destruc‐tion. Lazy deletion isn’t advisable in all defense in depth strategies: a long lazy dele‐tion period is costly in systems with much short-lived data, and impractical in systems that must guarantee destruction of deleted data within a reasonable time frame (i.e., those that offer privacy guarantees).To sum up the first layer of defense in depth:
• A trash folder that allows users to undelete data is the primary defense against 	user error.
• Soft deletion is the primary defense against developer error and the secondary 	defense against user error.
• In developer offerings, lazy deletion is the primary defense against internal devel‐	oper error and the secondary defense against external developer error.What about revision history? Some products provide the ability to revert items to pre‐vious states. When such a feature is available to users, it is a form of trash. When7 Upon reading this advice, one might ask: since you have to offer an API on top of the datastore to implement soft deletion, why stop at soft deletion, when you could offer many other features that protect against acciden‐tal data deletion by users? To take a specific example from Google’s experience, consider Blobstore: rather than allow customers to delete Blob data and metadata directly, the Blob APIs implement many safety fea‐tures, including default backup policies (offline replicas), end-to-end checksums, and default tombstone life‐times (soft deletion). It turns out that on multiple occasions, soft deletion saved Blobstore’s clients from data loss that could have been much, much worse. There are certainly many deletion protection features worth calling out, but for companies with required data deletion deadlines, soft deletion was the most pertinent pro‐tection against bugs and accidental deletion in the case of Blobstore’s clients.How Google SRE Faces the Challenges of Data Integrity  |  351
available to developers, it may or may not substitute for soft deletion, depending on its implementation.At Google, revision history has proven useful in recovering from certain data corrup‐tion scenarios, but not in recovering from most data loss scenarios involving acciden‐tal deletion, programmatic or otherwise. This is because some revision history implementations treat deletion as a special case in which previous states must be removed, as opposed to mutating an item whose history may be retained for a certain time period. To provide adequate protection against unwanted deletion, apply the lazy and/or soft deletion principles to revision history also.Second Layer: Backups and Their Related Recovery Methods
Backups and data recovery are the second line of defense after soft deletion. The most important principle in this layer is that backups don’t matter; what matters is recov‐ery. The factors supporting successful recovery should drive your backup decisions, not the other way around.
In other words, the scenarios in which you want your backups to help you recover should dictate the following:• Which backup and recovery methods to use
• How frequently you establish restore points by taking full or incremental back‐	ups of your data
• Where you store backups
• How long you retain backups
How much recent data can you afford to lose during a recovery effort? The less data you can afford to lose, the more serious you should be about an incremental backup strategy. In one of Google’s most extreme cases, we used a near-real-time streaming backup strategy for an older version of Gmail.Even if money isn’t a limitation, frequent full backups are expensive in other ways. Most notably, they impose a compute burden on the live datastores of your service while it’s serving users, driving your service closer to its scalability and performance limits. To ease this burden, you can take full backups during off-peak hours, and then a series of incremental backups when your service is busier.How quickly do you need to recover? The faster your users need to be rescued, the more local your backups should be. Often Google retains costly but quick-to-restore
352  |  Chapter 26: Data Integrity: What You Read Issnapshots8 for very short periods of time within the storage instance, and stores less recent backups on random access distributed storage within the same (or nearby) datacenter for a slightly longer time. Such a strategy alone would not protect from site-level failures, so those backups are often transferred to nearline or offline loca‐tions for a longer time period before they’re expired in favor of newer backups.How far back should your backups reach? Your backup strategy becomes more costly the further back you reach, while the scenarios from which you can hope to recover increase (although this increase is subject to diminishing returns).
In Google’s experience, low-grade data mutation or deletion bugs within application code demand the furthest reaches back in time, as some of those bugs were noticed months after the first data loss began. Such cases suggest that you’d like the ability to reach back in time as far as possible.On the flip side, in a high-velocity development environment, changes to code and schema may render older backups expensive or impossible to use. Furthermore, it is challenging to recover different subsets of data to different restore points, because doing so would involve multiple backups. Yet, that is exactly the sort of recovery effort demanded by low-grade data corruption or deletion scenarios.The strategies described in “Third Layer: Early Detection” on page 356 are meant to speed detection of low-grade data mutation or deletion bugs within application code, at least partly warding off the need for this type of complex recovery effort. Still, how do you confer reasonable protection before you know what kinds of issues to detect? Google chose to draw the line between 30 and 90 days of backups for many services. Where a service falls within this window depends on its tolerance for data loss and its relative investments in early detection.To sum up our advice for guarding against the 24 combinations of data integrity fail‐ure modes: addressing a broad range of scenarios at reasonable cost demands a tiered backup strategy. The first tier comprises many frequent and quickly restored backups stored closest to the live datastores, perhaps using the same or similar storage tech‐nologies as the data sources. Doing so confers protection from the majority of scenar‐ios involving software bugs and developer error. Due to relative expense, backups are retained in this tier for anywhere from hours to single-digit days, and may take minutes to restore.The second tier comprises fewer backups retained for single-digit or low double-digit days on random access distributed filesystems local to the site. These backups may
8 “Snapshot” here refers to a read-only, static view of a storage instance, such as snapshots of SQL databases. Snapshots are often implemented using copy-on-write semantics for storage efficiency. They can be expensive for two reasons: first, they contend for the same storage capacity as the live datastores, and second, the faster your data mutates, the less efficiency is gained from copying-on-write.How Google SRE Faces the Challenges of Data Integrity  |  353
take hours to restore and confer additional protection from mishaps affecting partic‐ular storage technologies in your serving stack, but not the technologies used to con‐tain the backups. This tier also protects against bugs in your application that are detected too late to rely upon the first tier of your backup strategy. If you are intro‐ducing new versions of your code to production twice a week, it may make sense to retain these backups for at least a week or two before deleting them.Subsequent tiers take advantage of nearline storage such as dedicated tape libraries and offsite storage of the backup media (e.g., tapes or disk drives). Backups in these tiers confer protection against site-level issues, such as a datacenter power outage or distributed filesystem corruption due to a bug.It is expensive to move large amounts of data to and from tiers. On the other hand, storage capacity at the later tiers does not contend with growth of the live production storage instances of your service. As a result, backups in these tiers tend to be taken less frequently but retained longer.
Overarching Layer: ReplicationOverarching Layer: Replication
In an ideal world, every storage instance, including the instances containing your backups, would be replicated. During a data recovery effort, the last thing you want is to discover is that your backups themselves lost the needed data or that the datacenter containing the most useful backup is under maintenance.As the volume of data increases, replication of every storage instance isn’t always fea‐sible. In such cases, it makes sense to stagger successive backups across different sites, each of which may fail independently, and to write your backups using a redundancy method such as RAID, Reed-Solomon erasure codes, or GFS-style replication.9When choosing a system of redundancy, don’t rely upon an infrequently used scheme whose only “tests” of efficacy are your own infrequent data recovery attempts. Instead, choose a popular scheme that’s in common and continual use by many of its users.
1T Versus 1E: Not “Just” a Bigger BackupProcesses and practices applied to volumes of data measured in T (terabytes) don’t scale well to data measured in E (exabytes). Validating, copying, and performing round-trip tests on a few gigabytes of structured data is an interesting problem. How‐ever, assuming that you have sufficient knowledge of your schema and transaction model, this exercise doesn’t present any special challenges. You typically just need to9 For more information on GFS-style replication, see [Ghe03]. For more information on Reed-Solomon erasure 	codes, see https://en.wikipedia.org/wiki/Reed–Solomon_error_correction.
354  |  Chapter 26: Data Integrity: What You Read Is
procure the machine resources to iterate over your data, perform some validation logic, and delegate enough storage to hold a few copies of your data.Now let’s up the ante: instead of a few gigabytes, let’s try securing and validating 700 petabytes of structured data. Assuming ideal SATA 2.0 performance of 300 MB/s, a single task that iterates over all of your data and performs even the most basic of vali‐dation checks will take 8 decades. Making a few full backups, assuming you have the media, is going to take at least as long. Restore time, with some post-processing, will take even longer. We’re now looking at almost a full century to restore a backup that was up to 80 years old when you started the restore. Obviously, such a strategy needs to be rethought.The most common and largely effective technique used to back up massive amounts of data is to establish “trust points” in your data—portions of your stored data that are verified after being rendered immutable, usually by the passage of time. Once we know that a given user profile or transaction is fixed and won’t be subject to further change, we can verify its internal state and make suitable copies for recovery pur‐poses. You can then make incremental backups that only include data that has been modified or added since your last backup. This technique brings your backup time in line with your “mainline” processing time, meaning that frequent incremental back‐ups can save you from the 80-year monolithic verify and copy job.However, remember that we care about restores, not backups. Let’s say that we took a full backup three years ago and have been making daily incremental backups since. A full restore of our data will serially process a chain of over 1,000 highly interdepend‐ent backups. Each independent backup incurs additional risk of failure, not to men‐tion the logistical burden of scheduling and the runtime cost of those jobs.Another way we can reduce the wall time of our copying and verification jobs is to distribute the load. If we shard our data well, it’s possible to run N tasks in parallel, with each task responsible for copying and verifying 1/Nth of our data. Doing so requires some forethought and planning in the schema design and the physical deployment of our data in order to:
• Balance the data correctly• Balance the data correctly
• Ensure the independence of each shard
• Avoid contention among the concurrent sibling tasks
Between distributing the load horizontally and restricting the work to vertical slices of the data demarcated by time, we can reduce those eight decades of wall time by several orders of magnitude, rendering our restores relevant.
How Google SRE Faces the Challenges of Data Integrity  |  355Third Layer: Early Detection
“Bad” data doesn’t sit idly by, it propagates. References to missing or corrupt data are copied, links fan out, and with every update the overall quality of your datastore goes down. Subsequent dependent transactions and potential data format changes make restoring from a given backup more difficult as the clock ticks. The sooner you know about a data loss, the easier and more complete your recovery can be.Challenges faced by cloud developers
In high-velocity environments, cloud application and infrastructure services face many data integrity challenges at runtime, such as:
• Referential integrity between datastores
• Schema changes
• Aging code
• Zero-downtime data migrations
• Evolving integration points with other servicesWithout conscious engineering effort to track emerging relationships in its data, the data quality of a successful and growing service degrades over time.Often, the novice cloud developer who chooses a distributed consistent storage API (such as Megastore) delegates the integrity of the application’s data to the distributed consistent algorithm implemented beneath the API (such as Paxos; see Chapter 23). The developer reasons that the selected API alone will keep the application’s data in good shape. As a result, they unify all application data into a single storage solution that guarantees distributed consistency, avoiding referential integrity problems in exchange for reduced performance and/or scale.While such algorithms are infallible in theory, their implementations are often rid‐dled with hacks, optimizations, bugs, and educated guesses. For example: in theory, Paxos ignores failed compute nodes and can make progress as long as a quorum of functioning nodes is maintained. In practice, however, ignoring a failed node may correspond to timeouts, retries, and other failure-handling approaches beneath the particular Paxos implementation [Cha07]. How long should Paxos try to contact an unresponsive node before timing it out? When a particular machine fails (perhaps intermittently) in a certain way, with a certain timing, and at a particular datacenter, unpredictable behavior results. The larger the scale of an application, the more fre‐quently the application is affected, unbeknownst, by such inconsistencies. If this logic holds true even when applied to Paxos implementations (as has been true for Goo‐gle), then it must be more true for eventually consistent implementations such as Bigtable (which has also shown to be true). Affected applications have no way to356  |  Chapter 26: Data Integrity: What You Read Is
know that 100% of their data is good until they check: trust storage systems, but verify!