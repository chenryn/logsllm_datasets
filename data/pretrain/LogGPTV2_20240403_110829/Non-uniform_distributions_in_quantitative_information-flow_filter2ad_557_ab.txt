cause it is associated with operational security guarantees: one can
give lower bounds for the eﬀort for determining a secret by exhaus-
tive search in terms of H(D|P), see [21, 26].
We mention for completeness that several approaches in the liter-
ature (e.g. [11, 25, 30]) focus on computing the mutual information
I(D; P) between the input and the output of a program, which is
deﬁned as the reduction in uncertainty about the input when the
output is observed, i.e. I(D; P) = H(D) − H(D|P). For given H(D),
the value of I(D; P) can be immediately be derived from that of
H(D|P), and vice versa. We present our results in terms of the re-
maining uncertainty H(D|P) because of its more direct connection
to operational security guarantees.
3. NON-UNIFORM QUANTITATIVE
INFORMATION-FLOW ANALYSIS
In this section we ﬁrst show how to reduce the problem of QIF
w.r.t. non-uniform distributions to the problem of QIF w.r.t. uni-
U
I ˆD
ˆD
D
F ˆD
= IP
P ◦ ˆD
P
FP
Figure 1: Overview of the random variables required for reduc-
ing non-uniform QIF analysis to the uniform case. The input
to P is distributed according to the variable D. The input to ˆD
is given by the uniformly distributed random variable U; the
output distribution of ˆD matches that of D.
form distributions. We then show how this reduction allows us
to leverage existing QIF techniques for programs with uniformly
distributed inputs for the QIF-analysis of programs with arbitrarily
distributed inputs.
3.1 Reducing the Non-uniform Case to the
Uniform Case
The main idea behind our reduction is to express a probability
distribution p as a program ˆD that takes input that is uniformly dis-
tributed and produces output that is distributed according to p. We
prove an assertion that connects the remaining uncertainty about
the inputs of the sequentially composed program ˆD; P (with respect
to uniform distributions) to the remaining uncertainty about the in-
puts of the program P with respect to the distribution p.
Our reduction is motivated by a number of examples that occur in
practice. For example, the Personal Identiﬁcation Numbers (PINs)
used in electronic banking are often not uniformly distributed, but
derived from uniform bitstrings using decimalization techniques
[13] (We will apply our techniques to analyze a program handling
such PINs in Section 5). Another example are the keys of a public-
key cryptosystem, which are typically not uniformly distributed
bitstrings. However, they are produced by a key generation algo-
rithm that operates on uniformly distributed input. More gener-
ally, a large number of randomized algorithms expect uniformly
distributed randomness. E.g. in complexity theory, randomized al-
gorithms are based on probabilistic Turing machines that work with
uniformly distributed random tapes. For a language-based perspec-
tive on distribution generators, see [32].
Formally, let P = (IP, FP , RP) be a program and p an arbitrary
distribution on IP. Let ˆD = (I ˆD, F ˆD, R ˆD) be a program that maps to
P’s initial states, i.e., F ˆD
= IP, and let u be the uniform distribution
on I ˆD. We require that the distribution produced by ˆD matches the
distribution on P’s inputs, i.e., u ˆD
= p. We deﬁne the random vari-
ables D and U as the identity functions on IP and I ˆD, respectively,
and we use them for modeling the choice of an input according to
p and u, respectively. Figure 1 depicts these mappings and their
connections.
The setup is chosen such that the uncertainty about the output
of the composed program P ◦ ˆD matches the uncertainty about the
output of P, i.e. H(P ◦ ˆD) = H(P). Similarly, we have H( ˆD) =
H(D). As a consequence, we can express the remaining uncertainty
about the input of P in terms of a diﬀerence between the remaining
uncertainties about the (uniformly distributed) inputs of P ◦ ˆD and
ˆD.
Lemma 1. Let P, ˆD, D, U be as deﬁned in Section 3.1. Then
H(D|P) = H(U|P ◦ ˆD) − H(U| ˆD).
Proof. The output of P is determined by the output of D, namely
H(P, D) = H(D). Therefore it holds H(D|P) = H(D) − H(P), and
hence by construction
H(D|P) = H( ˆD) − H(P ◦ ˆD) .
(1)
Similarly, the outputs of ˆD and P ◦ ˆD are determined by the output
of U, hence
H(U|P ◦ ˆD) − H(U| ˆD) = H(U) − H(P ◦ ˆD) − (H(U) − H( ˆD)) . (2)
The assertion then follows by combining (1) and (2).
Lemma 1 shows how the remaining uncertainty about the (non-
uniform) input of P can be expressed as a diﬀerence of remaining
uncertainties about (uniform) inputs of ˆD and P ◦ ˆD. In the follow-
ing, we show how this result can be exploited for automating the
quantitative information-ﬂow analysis w.r.t. non-uniform distribu-
tions using established tools for uniform QIF.
3.2 Automation of QIF for Non-uniform
Distributions
We summarize two kinds of techniques for automatically ana-
lyzing the information-ﬂow of programs with respect to uniform
distributions. The ﬁrst kind of technique allows for the accurate,
but possibly expensive QIF of a given program [2, 20], and the sec-
ond kind of technique uses a randomized algorithm for obtaining
approximate results with quality guarantees [22]. As we will show
next, Lemma 1 allows one to use both kinds of techniques for ana-
lyzing programs with respect to non-uniform distribution.
3.2.1 Accurate Quantiﬁcation
The following proposition from [20] connects the combinatorial
characteristics of the partition ΠQ induced by a program Q with the
remaining uncertainty about the uniformly distributed input of Q.
respectively. Each partition may have as many elements as IP,
which severely limits scalability. The following proposition from [22]
is an extension of a result from [3] and addresses this limitation: it
implies that, for uniformly distributed inputs, one can give tight
bounds for H(U|Q) by considering only a small subset of randomly
chosen blocks.
Proposition 2
(see [22]). Let U = id be uniformly distributed
and let Q be a program taking input distributed according to U. Let
B1, . . . , Bn be drawn randomly from ΠQ with respect to the distribu-
tion p(B) = #(B)
#(IQ) . Then
1
n
n
Xi=1
log #(Bi) − δ ≤ H(U|Q) ≤
1
n
n
Xi=1
log #(Bi) + δ
holds with probability of more than 1 −
(log #(ΠQ))2
nδ2
.
As described in [22], Proposition 2 can be turned into a random-
ized algorithm for quantitative information-ﬂow analysis. To this
end, observe that the random choice of blocks can be implemented
by executing the program on a (uniformly chosen) input s ∈ IQ
and determining the preimage B = Q−1(s′) of s′ = Q(s). If this
preimage is represented by a logical assertion, one can compute
the size #(B) of B using model counting techniques [15]. In this
way, H(U|Q) can be approximated with high conﬁdence levels us-
ing a number of samples n that is only polylogarithmic in the size
of the state space.
The following theorem enables us to leverage these techniques
for analyzing programs with non-uniform inputs.
Theorem 2. Let P, ˆD, D be as deﬁned in Section 3.1. Let B1, . . . , Bn
be drawn randomly from Π
domly from Π ˆD with respect to the distribution p(B) = #(B)
P◦ ˆD and let B′
1, . . . , B′
n be drawn ran-
#(I ˆD) . Then
Proposition 1
(see [20]). Let U = id be uniformly distributed
and let Q be a program taking input distributed according to U.
Then
1
n
n
Xi=1
log
#(Bi)
#(B′
i)
− 2δ ≤ H(D|P) ≤
1
n
H(U|Q) =
1
#(IQ) XB∈ΠQ
#(B) log2 #(B) .
Proposition 1 can be turned into an algorithm for computing H(U|Q):
Enumerate all blocks B in the partition ΠQ, determine their sizes,
and use these data for computing H(U|Q). The algorithm described
in [20] uses this approach for a partition Π that reﬂects the knowl-
edge gained by an attacker that can adaptively provide input to the
program. The algorithm described in [2] extracts a logical repre-
sentation of Π by computing weakest preconditions and employs
model counting techniques for determining the sizes of individual
blocks from this logical representation.
The following theorem enables us to directly apply both tech-
niques to programs with non-uniform input distributions.
Theorem 1. Let P, ˆD, D be as deﬁned in Section 3.1. Then
H(D|P) =
XB∈Π
P◦ ˆD
#(B) log2 #(B) − XB′∈Π ˆD
1
#(I ˆD) 
#(B′) log2 #(B′)
.
Proof. The statement is obtained by applying Proposition 1 to
log
#(Bi)
#(B′
i)
+ 2δ
.
n
Xi=1
(cid:19)2
with a probability of more than(cid:18)1 −
(log #(Π ˆD))2
nδ2
Proof. Apply Proposition 2 to both terms on the right hand side
of Lemma 1. For the conﬁdence levels, observe that the blocks
Bi are drawn independently from the blocks B′
i, hence the proba-
bilities that the inequalities hold multiply. Observing that #(Π ˆD) ≥
#(Π
P◦ ˆD), we replace the larger probability by the smaller one, which
concludes this proof.
Finally, the exact computation of the blocks Bi can be prohibitively
expensive. Fortunately, one can avoid this expensive computation
by resorting to under- and over-approximations Bi and Bi of Bi, i.e.
subsets of initial states with Bi ⊆ Bi ⊆ Bi. The computation of Bi
and Bi can be done using existing techniques for symbolic execu-
tion and abstract interpretation, see [22]. In this paper, we simply
assume the existence of such approximations.
Corollary 1. Let B1, . . . , Bn and B′
′ ⊆ B′
Theorem 2. Let Bi ⊆ Bi ⊆ Bi and Bi
Then
1, . . . , B′
i ⊆ B′
n be chosen as in
i, for all i ∈ {1, . . . n}.
both terms on the right hand side of Lemma 1.
3.2.2 Randomized Quantiﬁcation
1
n
n
Xi=1
log
#(Bi)
#(B′
i)
− 2δ ≤ H(D|P) ≤
1
n
The direct computation of H(D|P) on basis of Theorem 1 re-
P◦ ˆD,
quires the enumeration of all blocks in the partitions ΠP and Π
with a probability of more than(cid:18)1 −
(log #(Π ˆD))2
nδ2
log
#(Bi)
#(B′
i)
+ 2δ
.
n
Xi=1
(cid:19)2
Corollary 1 follows directly from Theorem 2 by replacing all
blocks that occur in the numerator (denominator) of the right (left)
hand side and on the denominator (numerator) on the left (right)
hand side by their over-(under-)approximating counterparts.
4. ROBUSTNESS
In this section, we show that the remaining uncertainty about a
secret is robust with respect to small variations in the input distri-
bution. This allows us to replace actual input distributions with ap-
proximate distributions. Based on the quality of the approximation,
we give upper and lower bounds on the error this approximation in-
troduces in the analysis. Focusing on approximate distributions can
simplify the information-ﬂow analysis, e.g. by allowing to replace
“almost uniform” distributions by uniform distributions.
We say that two distributions p and q on some set S are γ-close if
the probabilities they assign to each value diﬀer at most by a factor
of γ.
γ
≈ q ≡ ∀x ∈ S :
p
1
γ
q(x) ≤ p(x) ≤ γ q(x)
In the following we will consider a random variable with respect
to diﬀerent probability distributions on its input domain. We intro-
duce the notation X p emphasize that we consider variable X with re-
spect to the underlying distribution p. The following lemma states