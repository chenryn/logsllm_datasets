its play with participants (e.g., playing a two-party Texas hold
’em or a GO game), and the goal of an adversary is to ﬁgure
out a way to defeat that master agent, rule the game, and
thus gather maximum rewards for fun or for proﬁts. When
USENIX Association
30th USENIX Security Symposium    1885
AgentObservation(state)of the agentPolicy networkof the agent0.20.90.51.3……Adversarial agentOpponent agentPolicy network of attacker agentPolicy network of opp. agentState of the environment2.10.11.31.7…2.10.11.31.7…tion is reasonable and practical because, as we mentioned
above, both the attacker’s agent and the opponent agent take
the observation from the same environment, and the action
took by agents can be easily observed from the environment
as well. For example, the opponent agent’s policy network
outputs an upward movement, which the adversarial agent
could easily observe from the change of the environment.
3 Background of Reinforcement Learning
Recently, many reinforcement learning algorithms have been
proposed to train an agent interacting with an environment,
ranging from Q-learning based algorithms (e.g., [31, 53]) to
policy optimization algorithms (e.g., [22,29,41,43]). Among
all the learning algorithms, proximal policy optimization
(PPO) [43] is the one that has been broadly adopted in the
two-agent competitive games. For example, teams from Ope-
nAI utilize this algorithm to play Hide-and-Seek [35] and
world-famous game Dota2 [32]. In this work, we design our
method of training an adversarial policy by extending the
PPO learning algorithm. In this section, we brieﬂy describe
how to model a reinforcement learning problem, and then
discuss how the PPO algorithm is designed to resolve the
reinforcement learning problem.
3.1 Modeling an RL Problem
Given a reinforcement learning problem, it is common to
model the problem as a Markov Decision Process (MDP)
which contains the following components:
• a ﬁnite set of states S, where each state s(t) (s(t) ∈ S)
represents the state of the agent at the time t and s(0) is
the initial state;
• a ﬁnite action set A, where each action a(t) (a(t) ∈ A)
refers to the action of the agent at the time t;
• a state transition model P : S × A → S, where Pa
ss(cid:48) =
P[s(t+1) = s(cid:48)|s(t) = s,a(t) = a] denotes the probability
that the agent transits from state s to s(cid:48) by taking action
a;
• a reward function R : S × A → R, where Ra
E[r(t+1)|s(t) = s,a(t) = a] represents the expected reward
if the agent takes action a at state s(t); here r(t+1) indi-
cates the reward that the agent will receive at the time
t + 1 after taking the action;
• a scalar discount factor γ ∈ [0,1], which is usually mul-
tiplied by future rewards as discovered by the agent in
order to dampen the effect of rewards upon the agent’s
choice of an action.
s =
As is mentioned above, the ultimate goal of reinforcement
learning is to train the agent to ﬁnd a policy π(a|s): (S → A)
that could maximize the expectation of the total rewards over
a sequence of actions generated through the policy. Mathemat-
ically, this could be accomplished by maximizing state-value
function Vπ(s) deﬁned as
Vπ(s) = ∑
a∈A
π(a|s)(Ra
s + γ ∑
s(cid:48)∈S
Pa
ss(cid:48)Vπ(s(cid:48))) ,
or the action-value function Qπ(s,a) deﬁned as
Qπ(s,a) = Ra
s + γ ∑
s(cid:48)∈S
ss(cid:48) ∑
Pa
a(cid:48)∈A
π(a(cid:48)|s(cid:48))Qπ(s(cid:48),a(cid:48)) .
(1)
(2)
In reinforcement learning, the state-value function Vπ(s)
represents how good is a state for an agent to be in. It is equal
to the expected total reward for an agent starting from state s.
The value of this function depends on the policy π, by which
the agent picks actions to perform. Slightly different from
Vπ(s), the action-value function Qπ(s,a) is an indication for
how good it is for an agent to pick action a while being in
state s. By maximizing either of these functions above, one
could obtain an optimal policy π∗ for the agent to collect the
maximum amount of rewards from the environment.
3.2 Resolving an RL problem
Deep Q-learning. To ﬁnd an optimal policy for an agent
to maximize its total reward, one method is to utilize deep
Q-learning, which takes a state s and approximates the Q-
value for each action based on that state (i.e., Qπ(s,a)). With
this approximation, although the agent cannot extract the
policy explicitly, it could still maximize its reward by taking
the action with the highest Q-value. As is shown in recent
research, such a method demonstrates a great success in many
applications, such as playing GO [45] and mastering a wide
variety of Atari games [30]. However, since deep Q-learning
usually calculates all possible actions in a discrete action
space, it has been barely adopted to two-agent games with
continuous action space, including simulation games, like
MuJoCo and RoboSchool, and real-world strategy games,
such as StarCraft and Dota. As a result, the policy gradient
approach is typically adopted.
Policy Gradient Algorithm. Policy gradient refers to the
techniques that directly parametrize the policy as a function
πθ(s,a) = P(a|s,θ). At the time t, this function takes as input
the state s(t) and outputs the action a(t). In recent research
article [29], researchers modeled the policy π as a deep neural
network (e.g., multilayer perceptron [55] or recurrent neural
networks [58]), and named the DNN as the policy network.
To learn a policy network for an agent,
the policy
gradient algorithm deﬁnes an objective function J(θ) =
E
s(0),a(0),...∼πθ [∑∞
t=0 γtr(t)] which represents the expectation of
the total discounted rewards. By maximizing this objective
function, one could obtain the parameters θ and thus the op-
timal policy. In order to compute parameters θ, the policy
gradient algorithm computes the gradient of the objective
1886    30th USENIX Security Symposium
USENIX Association
To solve equation (4), the actor-critic framework approxi-
mates Vπθ(s) through a deep neural network Vv(s) parameter-
ized by v and then utilizes this approximated Vπθ (s) to deduce
Qπθ (s,a). As is speciﬁed in [29], this neural network can be
learned together with the policy network πθ through either
Monte-Carlo methods or Temporal-Difference methods [42].
Proximal Policy Optimization (PPO) Algorithm. Using
the actor-critic framework to train an agent, recent research
indicates that the actor usually experiences enormous variabil-
ity in the training which inﬂuences the performance of the
trained agent [41]. To stabilize actor training, recent research
proposes the PPO algorithm [16, 43], which introduces a new
objective function called “Clipped surrogate objective func-
tion”. With this new objective function, the policy change
could be restricted in a small range.
As is discussed in [41], the original mathematical form of
clipped surrogate objective function is
πθ(a(t)|s(t))
πθold (a(t)|s(t))
maximizeθ E
Aπθold
(a(t),s(t))∼πθold
[DKL(πθold (·|s(t))||πθ(·|s(t)))] ≤ δ ,
s.t. E
s(t)∼πθold
[
(a(t),s(t))] ,
(5)
where πθold is the old policy. DKL(p||q) refers to the KL-
(a(t),s(t))
divergence between distribution p and q [24]. Aπθold
refers to the advantage function in Equation (4). By solving
Equation (5), the new policy πθ can be obtained.
As is discussed in [43], solving Equation (5) is computa-
tionally expensive because it requires a second-order approxi-
mation of the KL divergence and computing Hessian matrices.
To address this problem, Schulman et al. [43] proposed the
PPO objective function, which replaces the KL-constrained
objective in Equation (5) by a clipped objective function
[min(clip(ρ(t),1− ε,1 + ε)A(t),ρ(t)A(t))] ,
maximizeθ E
(a(t),s(t))∼πθold
(6)
ρ(t) =
πθ(a(t)|s(t))
πθold (a(t)|s(t))
, A(t) = Aπθold
(a(t),s(t)) .
Here, clip(ρ(t),1− ε,1 + ε) denotes clipping ρ(t) to the range
of [1− ε,1 + ε] and ε is a hyper-parameter. During the train-
ing process, in addition to updating the actor by solving the
optimization function in Equation (6), the PPO algorithm
iteratively updates the action-value function Qw(s,a) as well
as the state-value function Vv(s) (i.e., the critic) by using the
Temporal-Difference method.3 In Figure 2, we show the net-
work structure used in the PPO algorithm. As we can observe
from the ﬁgure, the network structure contains two deep neu-
ral networks, one for approximating the state-value function
Vv(s) and the other for modeling the policy network πθ. It
should be noted that the implementation of PPO algorithm
does not introduce an additional neural network to approxi-
mate action-value function Qw(s,a) but to deduce it through
the state-value function Vv(s).
3While the Monte-Carlo method is also available for the training, due
to the performance concern, the standard implementation of PPO considers
only the Temporal-Difference method.
Figure 2: The neural network architecture involved in the
PPO algorithm. Note that the two networks share parameters
with each other.
function with respect to parameters (i.e., (cid:79)θJ(θ)) and then
iteratively apply stochastic gradient-ascend to reach a local
maximum in J(θ). According to the Policy Gradient The-
orem [22], for any differentiable policy πθ(s,a), the policy
gradient can be written as
(cid:79)θJ(θ) = Eπθ [(cid:79)θlogπθ(s,a)Qπθ (s,a)] ,
(3)
where πθ(s,a) is the policy network and Qπθ(s,a) denotes the
action-value function of the corresponding MDP. As we can
easily observe from the equation above, to solve this equation,
we need to know function Qπθ(s,a). In the policy gradient al-
gorithm, the action-value function Qπθ (s,a) is approximated
by a deep neural network Qw(s,a), which can be learned to-
gether with the policy network. However, this design has a
limitation. In each iteration of the training process, an agent
has to compute the reward at the end of the episode, and then
average all actions. Therefore, an agent inevitably concludes
all the actions taken were good, if it receives a high reward,
even if some were really bad. To address this problem, one
straightforward approach is to enlarge the training sample
batch. Unfortunately, this could incur slow learning and the
agent has to take even longer time to converge.
Actor-Critic Framework. To improve the policy gradient
algorithm mentioned above, recent research introduces an
actor-critic framework, which deﬁnes a critic and an actor.
Through an action-value function Qπθ (s,a), the critic mea-
sures how good the action taken is. Through a policy network
πθ, the actor controls how the agent behaves. With both of
these, we can rewrite the policy gradient as
(cid:79)θJ(θ) = Eπθ [(cid:79)θlogπθ(s,a)Aπθ (s)] ,
Aπθ (s,a) = Qπθ (s,a)−Vπθ (s) .
(4)
Here, Aπθ (s,a) is an advantage function, which measures the
difference between the Q value for action a in state s and
the average value of that state [12]. Through this advantage
function, we can know the improvement over the average
the action taken at that state. In other words, this function
calculates the extra reward the agent gets if it takes this action.
USENIX Association
30th USENIX Security Symposium    1887
……Vv(s)⇡✓(a|s)Shared parameter ⇡µ⇡……N(µ,σ2)Figure 3: The overview of our proposed attack. The upper part on a grey canvas demonstrates a game episode where the policy
network of the opponent agent outputs the optimal actions and the opponent agent (in purple) wins the game. The lower part
shows an episode in which the adversarial agent (in blue) subtly manipulates the environment through its actions, forces the
opponent agents to choose a sequence of sub-optimal actions, and thus defeats the opponent. The arrow tied to the purple paddle
indicates the action the opponent agent takes. At each time step, the adversarial agent only introduces an imperceptible change
to the environment and therefore the scenes (or in other words states) on the grey canvas are nearly as same as those on the
white canvas (i.e, (cid:107)s(t) − ˆs(t)(cid:107) ≤ ε where ε is a small number restricting the action change of the adversarial). The feature vector
passing to the networks indicates the observation of the opponent agent. It is converted from the states of the opponent agent s(t)
and ˆs(t). The features in the red box (o5···o8 and ˆo5··· ˆo8) represent those corresponding to the adversarial action.
Different from the previous actor-critic algorithms, which
update actor by conducting stochastic gradient-ascend4 using
the approximated policy gradient of Equation (4), the PPO
algorithm can guarantee a monotonic improvement of the
total rewards when updating the policy network (i.e., J(θ) ≥
J(θold)). With this property, the trained agent could not only
reach to the convergence faster but, more importantly, demon-
strate more accurate and more stable performance than the
previous actor-critic algorithms. To the best of our knowledge,
PPO is the state-of-art algorithm for training a policy network
for the agent in the two-agent competitive games. As such, we
design our attack by extending this PPO training algorithm.
4 Technical Overview
Recall that we attack a well-trained agent by training a pow-
erful adversarial agent. To achieve this, as is mentioned in
Section 2, we do not assume that an attacker has access to the
policy network of the opponent agent πv nor its state-transition
model Pv
ss(cid:48). Rather, we assume the attacker could obtain the
observation and action of the opponent (i.e., the state s(t)
v and
action a(t)
v of the opponent agent at each time step t). In this
section, we ﬁrst specify the basic idea of our attack method.
Then, we brieﬂy describe how to utilize the aforementioned
states and actions to extend the PPO algorithm and thus im-
4Note that the performance of stochastic gradient-ascend highly depends
on the step size and it cannot guarantee to increase the objective function
monotonically.
plement our attack method at a high level.
4.1 Basic idea of the proposed attack
Admittedly, it is possible to design a simple reward function
for an adversarial agent to beat its opponent. However, the
reward function design is usually game-speciﬁc, and it is chal-
lenging to design a universal solution. As such, we follow
a different strategy to fulﬁll our objective as follows. In a
two-agent competitive game, one could train an agent to take
an optimal action at each state via selfplay [3]. Therefore,
as is depicted in Figure 3, to inﬂuence a well-trained agent,
one method is to maximize the deviation of the actions taken
by that agent and thus make the agent output a suboptimal
action (i.e., given the same/similar environment observation,
an agent takes an action which is very different from the one
it is supposed to take). With this practice, from the adver-
sary’s viewpoint, he can downgrade the opponent agent’s
performance and thus reduce its winning rate.
To maximize the action deviation, an adversary would
inevitably vary the observation of the victim agent. As is
mentioned above, a suboptimal action means that, given the
same or similar observation, the action of the agent is very
different from the one it is supposed to take. Therefore, as we
will specify in the following, when maximizing the deviation