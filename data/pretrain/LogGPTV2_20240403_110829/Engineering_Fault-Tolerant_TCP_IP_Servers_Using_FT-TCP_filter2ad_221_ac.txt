### 5.2. Throughput

Figure 2 illustrates the throughput measurements for all applications we considered, including ttcp, Samba, and DSS, with and without FT-TCP. Due to protocol overhead in Samba and flow control in DSS, neither service fully saturates the client link. Therefore, we also present results for ttcp, a simple bandwidth testing tool that sends fabricated data and can achieve 97% of the theoretical maximum throughput (1128 Kb/s) on a 10 Mbps link with our specific TCP/IP configuration.

Samba and ttcp results are further divided into incoming and outgoing transfers from the server's perspective, as aggregate throughputs in these two scenarios differ significantly. We use "in" and "out" to denote the transfer direction throughout this section.

#### Key Observations
- **Throughput Comparison**: The throughput of services under FT-TCP is either statistically indistinguishable from or only slightly lower than the throughput under clean TCP. The worst relative overhead observed is about 1.8% for Samba (incoming).
- **Backup Overheads**: The overheads for cold and hot backups are statistically indistinguishable. Although we expected a faster throughput with a cold backup, the additional CPU load on the backup did not significantly slow down the buffering process.
- **DSS Performance**: DSS connections are least affected by FT-TCP because they throttle down to a low throughput of about 20 Kb/s, which is appropriate for streaming media over a modem connection. This leaves ample time between send() calls to absorb the extra latency of FT-TCP.
- **Samba Performance**: Samba is the most affected due to its higher number of syscalls. For example, a 4 Mb incoming transfer in Samba involves approximately 4,870 syscalls, compared to 2,940 for ttcp.
- **Directional Differences**: There is a marked difference between incoming and outgoing throughput values for both ttcp and Samba. These differences are primarily due to variations in hardware and operating system configurations, leading to performance discrepancies (approximately 10% for ttcp and 17% for Samba).

### Figure 3: Detailed Samba Incoming Throughput

To more precisely evaluate the overhead of interception and buffering, we plotted all measured Samba incoming throughput values in Figure 3. Note that the Y-axis does not start at zero to facilitate easier comparison of values.

#### Key Observations
- **Cold vs. Hot Backups**: With the exception of PR, throughputs with cold and hot backups are statistically indistinguishable.
- **Throughput Trends**: Throughput values decrease as we move from Clean TCP to immediate, to PR, and to PRS, as expected due to the additional work and buffering performed by machines running FT-TCP.
- **Anomaly in Hot PR**: The throughput of Hot PR was higher than expected, but this anomaly does not affect our main conclusion that FT-TCP overhead is small. Further investigation is needed to understand this behavior.

### Concurrent Client Connections

Concurrent client connections compete for access to internal FT-TCP data structures and the private communication channel between replicas. To assess whether this contention is a significant source of overhead, we measured per-client throughput while increasing the number of concurrent connections. Ttcp clients were configured to perform an incoming transfer at 50 Kb/sec, allowing us to run at least 20 clients without saturating the 1 Mbps link. Both with Clean TCP and FT-TCP, all clients maintained 50 Kb/sec throughput until the number of clients exceeded 20, at which point the link became the bottleneck, and throughput dropped.

### 5.3. Latency

For services like interactive terminal connections, server responsiveness may be more important than maximum bandwidth. To evaluate how FT-TCP affects latency characteristics, we executed short requests to a Samba server and analyzed client-side packet traces.

Each experiment instance (a directory listing request) consisted of:
- An 87-byte request
- A 464-byte reply with directory contents
- A 39-byte server status request
- A corresponding 49-byte reply

We defined Samba request latency as the time interval between the 87-byte request and the 49-byte reply. We also measured TCP packet latency for all incoming data-carrying packets and the internal buffering latency under FT-TCP.

#### Key Observations
- **Samba Request Latency**: The average Samba request latency almost tripled (from 2.2 ms to 5.8 ms under cold PRS and 6.2 ms under hot PRS) when FT-TCP was added. Despite this increase, the values remain low enough that human-perceived responsiveness is not affected.
- **TCP Packet Latency**: TCP packet latency also approximately tripled from 0.7 ms to around 2.3 ms due to interception and buffering overhead. These latencies are comparable to those experienced across a WAN.
- **FT-TCP Buffering Latency**: The minimal buffering latencies provide a lower bound on the round-trip times for messages between replicas, useful for determining reasonable values for the failure detection mechanism.

### 5.4. Failure and Recovery

In our earlier feasibility work [2], we demonstrated that recovery is possible. In this work, we focused on minimizing failover time, defined as the period during which a clientâ€™s data stream is stalled. For FT-TCP, failover time is influenced by:
- **Failure Detection Latency**: Time to detect the fault
- **Promotion Latency**: Time to bring the backup into a state where it can take over the connection
- **Retransmission Gap**: Time to restart the flow of data on the connection

We previously reported a failover time of approximately 20 ms per megabyte of buffered data for a cold backup, dominated by promotion latency. Recovery of a hot backup is more efficient, with failover time dominated by failure detection latency and retransmission gap.

#### Example Scenario
Figure 4 shows a portion of one connection, plotting sequence number offsets relative to the beginning of the connection. This figure illustrates the behavior of FT-TCP for a long (2.5 sec) promotion latency with no snooping.

---

This revised text aims to provide a clearer, more structured, and professional presentation of the original content.