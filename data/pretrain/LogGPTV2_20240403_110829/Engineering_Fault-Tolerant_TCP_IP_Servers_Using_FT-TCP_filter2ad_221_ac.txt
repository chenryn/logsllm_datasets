s
s
/
/
b
b
K
K
(
(
t
t
u
u
p
p
h
h
g
g
u
u
o
o
r
r
h
h
T
T
800
800
600
600
400
400
200
200
0
0
ttcp in
ttcp out
Samba in
Samba out
DSS 
20.13
+/- 0.00
20.12
+/- 0.00
20.12
+/- 0.00
Figure 2. Throughput of ttcp, Samba, and DSS with and without FT-TCP
5.2. Throughput
Throughput measurements for all applications that we
considered are shown in Figure 2. Because of protocol over-
head in Samba and ﬂow control performed by DSS, nei-
ther service completely saturates the client link; hence, we
also show results for ttcp—a simple bandwidth testing tool
that sends fabricated data and is able to attain 97% of the
theoretical maximum throughput (1128Kb/s) on a 10Mbps
link with our speciﬁc TCP/IP conﬁguration. Samba and
ttcp results are further divided into incoming and outgoing
transfers (from server’s point of view) because aggregate
throughputs in these two situations differ considerably. We
use “in” and “out” to denote the transfer direction from now
on.
The ﬁrst thing to note is that the throughput of services
under FT-TCP is either statistically indistinguishable from
or only slightly lower than the throughput under clean TCP.
The worst relative overhead is about 1.8% for Samba in.
The overheads for cold and hot backups are statistically in-
distinguishable. We were expecting faster throughput with
a cold backup since a hot one does all the work that a cold
one does (i.e. buffers requests) and more, but apparently
the additional CPU load on the backup was not sufﬁcient to
slow down the buffering process.
DSS connection is the least affected by FT-TCP because
it throttles itself down to a low throughput of about 20Kb/s
(appropriate for streaming media over a modem connec-
tion), leaving plenty of time between send() calls to ab-
sorb the extra latency of FT-TCP. On the other hand, Samba
is affected the most because it performs a larger number of
syscalls in general (e.g. for a 4Mb incoming transfer Samba
executes approximately 4,870 syscalls, while ttcp executes
approximately 2,940).
Finally, there is a marked difference between in and out
throughput values for both ttcp and Samba. Much of the
difference is because one rarely gets identical performance
from TCP in both directions of the same physical link when
the endpoints don’t have the same conﬁguration. Differ-
ences in hardware and operating systems affect the dynam-
ics of connections and lead to signiﬁcant (in our case around
10% for ttcp and 17% for Samba) differences in perfor-
mance. The additional overhead in Samba out is probably
due to disk caching—with a cold cache every ﬁle read hits
the disk, but a series of writes can be absorbed by the cache.
1035.61
+/- 1.01
)
s
/
b
K
(
t
u
p
h
g
u
o
r
h
T
1040
1030
1020
1010
1026.84
+/- 1.14
1026.01
+/- 0.92
1028.21
+/- 1.25
1024.48
+/- 2.12
1016.39
+/- 1.21
1015.67
+/- 1.53
C
C
H
C
H
C
H
le
a
n 
T
old i
m
ot i
m
C
P
m
.
old 
P
R
ot 
P
R
old 
P
R
S
ot 
P
R
S
m
.
Figure 3. Incoming Samba throughput
To evaluate the overhead of interception and buffering
more precisely, we plotted all measured Samba in through-
put values in Figure 3 (note that the Y axis does not start
at 0 so as to make the comparison of values easier). Re-
sults from other experiments showed a similar pattern, but
the overhead differences of different modes were more ap-
parent in this data set. There are two things worth attention
on this graph. First, with the exception of PR, throughputs
with cold and hot backups are statistically indistinguishable.
Second, once again Hot PR aside, throughput values de-
crease as we go from Clean TCP, to immediate, to PR, and
to PRS. This was expected since with each step additional
work and buffering are performed by machines running FT-
TCP. The throughput of Hot PR was higher than we ex-
pected. We don’t have a good explanation for this, so the
matter requires further investigation. In any case, the dif-
ferences among all the modes are relatively small and our
main conclusion about FT-TCP overhead being small is un-
affected by this anomaly.
Concurrent client connections compete for access to in-
ternal FT-TCP data structures and to the private communi-
cation channel between replicas. To see whether this con-
tention was a signiﬁcant source of overhead, we measured
per-client throughput while increasing the number of con-
current connections. We conﬁgured ttcp clients to perform
an incoming transfer at the rate of 50Kb/sec so we could run
at least 20 clients without saturating the 1Mbps link. With
both Clean TCP and with FT-TCP all clients were able to
maintain 50Kb/sec throughput until the number of clients
exceeded 20, at which point the link became the bottleneck
and the throughput seen by each client dropped.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:01:39 UTC from IEEE Xplore.  Restrictions apply. 
5.3. Latency
For
services—such
as
a
interactive
traces for these connections.
than its maximum bandwidth.
terminal
connection—responsiveness of the server may be more
important
To see how
FT-TCP affects latency characteristics of services, we
executed short requests to a Samba server and analyzed
client-side packet
Each
instance of the experiment (a directory listing request)
consisted of an 87-byte request, a 464-byte reply with the
directory contents, a 39-byte server status request and a
corresponding 49-byte reply. We deﬁned Samba request
latency as the time interval between the 87-byte request and
the 49-byte reply. We also measured TCP packet latency
of all incoming data-carrying packets as the time between
the moment the packet left the client and the moment
the packet acknowledging that data arrived at the client.
Finally, for the runs done under FT-TCP, we measured
the internal buffering latency, which is the time elapsed
between a buffering request and a reply as measured on the
primary.
Results of these latency experiments are shown in Ta-
ble 1 with minimum, mean, and maximum values, as well as
their standard deviations. There were 30 Samba request la-
tency measurements, 68 packet latency measurements, and
230 buffering latency measurements (which include both
packet and syscall requests).
The average Samba request latency almost tripled (from
2.2 ms to 5.8 ms under cold PRS and 6.2 ms under hot
PRS) when FT-TCP was added. Although that may seem
like a signiﬁcant increase in latency, the values are still low
enough that from the human perspective responsiveness is
not affected at all. Some of this can be attributed to the
increase in packet latency that is shown in the next column.
Keep in mind that our Samba request consists of two incom-
ing and two outgoing data packets along with some ACKs,
so it’s not directly comparable with TCP packet latency. It
also approximately tripled from 0.7 ms to around 2.3 ms
due to interception and buffering overhead. While such an
increase may be signiﬁcant in some circumstances, such la-
tencies are comparable to connection latencies experienced
across a WAN. For transfers that saturated the link and used
mostly full-sized packets (1,460 data bytes)—such as ttcp in
and Samba in—the latency of packets for both Clean TCP
and FT-TCP connections was around 6 ms, which is consis-
tent with the values we reported previously [2].
The values of FT-TCP buffering latencies in the third col-
umn are interesting for a couple of reasons. For one thing,
they offer us another way to quantify the difference between
a cold and a hot backup. As far as primary is concerned, the
only difference is the extra 30 microsecond buffering de-
lay on average. This is the reason cold throughput results
in the previous section are slightly higher than hot results.
The minimal buffering latencies are also useful for placing a
lower bound on the round-trip times for messages between
our replicas. The RTT is useful for determining reasonable
6
-
0
1
x
s
r
e
b
m
u
n
e
c
n
e
u
q
e
s
.
l
e
R
0.82
0.81
0.80
0.79
0.78
0.77
0.76
0.75
Client sent
Server acked
1.00
1.50
2.00
2.50
3.00
3.50
4.00
4.50
Time (sec)
Figure 4. Behavior of FT-TCP for a long (2.5 sec)
promotion latency with no snooping
values for the failure detection mechanism described in the
next section.
5.4. Failure and Recovery
In our earlier feasibility work [2], we showed that re-
covery was possible. For this work, we decided to con-
centrate on understanding how we could minimize failover
time, where the failover time is the length of the period dur-
ing which a client’s data stream is stalled. For FT-TCP
the failover time is affected by the time it takes to (a) de-
tect the fault (the failure detection latency), (b) bring the
backup into the state where it can take over the connection
(the promotion latency), and (c) restart the ﬂow of data on
the connection (the retransmission gap, more carefully de-
ﬁned below). We’ve already reported [2] the failover time
for a cold backup—approximately 20 ms per megabyte of
buffered data—and that time is dominated by the promotion
latency. We found recovery of a hot backup considerably
more efﬁcient than that. Hot backup failover time is domi-
nated by the failure detection latency and the retransmission
gap. Consider the following example.
Figure 4 shows a portion of one connection by plotting
sequence number offsets (relative to the beginning of the