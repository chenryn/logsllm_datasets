Host CPU
Server Storage
un-trusted server
Figure 1: (top) Simple ORAM Protocol between a
client and a server. (bottom) A trusted server-side
client proxy can be used to build a PIR interface on
top of ORAM assurances.
3.3 Secure Hardware-aided PIR
The recent advent of tamper-resistant, general-purpose
trustworthy hardware such as the IBM 4764 Secure Co-
Processor [3] has opened the door to eﬃciently deploying
ORAM privacy primitives for PIR purposes (i.e., for arbi-
trary public or private data, not necessarily originated by
the current client) by deploying such hardware as a trusted
server-side client proxy.
Asonov was the ﬁrst to introduce [4] a PIR scheme that
uses a secure CPU to provide (an apparent) O(1) online
communication cost between the client and server. How-
ever, this requires the secure CPU on the server side to scan
portions of the database on every request, indicating a com-
putational complexity cost of O(n), where n is the size of
the database.
An ORAM-based PIR mechanism is introduced by Iliev
and Smith [15], who deploy secure hardware to achieve a
cost of O(√n log n). This is better than the poly-logarithmic
complexity granted by ORAM for the small database sizes
they consider. This work is notable as one of the ﬁrst full
ORAM-based PIR setups. Figure 1 summarizes the inter-
action between the client and server in ORAM, and how to
turn an ORAM implementation into a PIR implementation
using a Secure CPU.
An improved ORAM-based PIR mechanism with O(n/k)
cost is introduced in [18], where n is the database size and
k is the amount of secure storage. The protocol is based
on a careful scrambling of a minimal set of server-hosted
items. A partial reshuﬄe costing O(n) is performed every
time the secure storage ﬁlls up, which occurs once every k
queries. While an improvement, this result is not always
practical since the total database size n often remains much
larger than the secure hardware size k. For k = √n (as
assumed in this paper), this mechanism yields an O(√n)
complexity (signiﬁcantly greater than O(log log n log n) for
practical values of n).
Insight Two: Correctness. Moreover, we deploy a set
of authenticated, per-level integrity constructs to provide
clients with correctness assurances at minimal additional
cost. We speciﬁcally ensure that illicit server behavior (e.g.,
alterations) does not go undetected.
We now detail these components.
4. A SOLUTION
In previous work [19] Williams et al. achieved a complex-
ity of O(log2 n) in a protocol oﬀering access privacy but
no correctness assurances. Here we build on their result
by deploying a new construction and more sophisticated
reshuﬄing protocol, to signiﬁcantly reduce both the com-
putational complexity and the storage overheads to only
O(log n log log n) (amortized per-query), under the same as-
sumption of O(√n) temporary client storage, while also en-
dowing the protocol with correctness assurances.
4.1 Overview
Similar to ORAM (see Section 3.1 for more details), data
is organized into log(n) levels, pyramid-like. Level i consists
of up to 4i items, stored on the server as label-value pairs.
These pairs can be stored and retrieved in O(1) time if the
storage provider implements a suitable hash table [16]. This
diﬀers from ORAM, which stores an item at level i using a
keyed hash function to determine its storage bucket (of size
O(log n), to allow for hash collisions) within the level. The
use of ﬁxed-sized hash buckets in ORAM instead of a simple
hash table adds a O(log n) storage overhead multiplier, and
slows down query processing, but the buckets are necessary;
otherwise queries to a hash table could reveal whether the
item was found at this level.
Here we avoid the overhead of using buckets to mask the
query result by using Bloom ﬁlters [7] (constructed to be
collision-free). Before attempting to query for an item that
might not be at the current level, a per-level Bloom ﬁlter is
queried ﬁrst. The bits of the Bloom ﬁlter are encrypted, hid-
ing the result of the query. If the Bloom ﬁlter indicates that
the item is not at this level, we query the level for a unique
fake item instead and continue with the next level. Once we
eventually ﬁnd the desired item (at a future level) – it will
be moved into the root tree node – above the levels where
it was searched for before (as in ORAM). This ensures that
the same item will never be queried for in that instantiation
of the Bloom ﬁlter again (as now it will be found higher in
the pyramid, or a reshuﬄe would have been triggered).
Insight One: Faster Lookup. Thus one key insight in
our mechanism is that we can construct an encrypted Bloom
ﬁlter to perform set membership tests, without revealing the
success of our query. Additionally, we design a novel con-
struction procedure that assembles the encrypted Bloom ﬁl-
ter without revealing any correlation between scanned items
and associated Bloom ﬁlter positions. The ﬁnal beneﬁt of
using encrypted Bloom ﬁlters is that all unique queries are
computationally indistinguishable due to the nature of the
keyed hash function used to index the ﬁlter. This allows
us to modify ORAM with signiﬁcant performance beneﬁts,
since we can avoid handling hash collisions, which add a
log n factor in total database size as described above.
The notion of encrypting a Bloom ﬁlter has been studied
previously, e.g.
in [6]. However, here we use a novel con-
struction that hides the construction process, the inputs,
and the results of the Bloom ﬁlter.
4.2 Query Processing
A query consists of a read or write request for a data item.
These items are kept at the storage provider at a particular
level; part of the client’s job is to determine which level the
item is at without revealing this to the server. Algorithm 1
shows the pseudo-code of this operation.
To process a query, the client ﬁrst downloads and scans
the server-stored item cache (line 10) then proceeds to search
each level, starting at the top (line 11). A labeling function
consisting of a hash of the item ID with several level param-
eters (f akeAccessCtr(level) and Gen(level)) generates the
unique label by which the client can ﬁnd the item at a partic-
ular level, if the item is indeed there. f akeAccessCtr(level)
represents the number of accesses to level since the last
reshuﬄe, and Gen(level) represents the number of times
level has been reshuﬄed. The use of f akeAccessCtr (line
12) ensures that successive queries request unique fake items,
which the client knows are stored on the server. The use of
Gen(level) ensures that items on every subsequent reshuﬄe
of a level have unique labels. Both functions are computed
from the total number of accesses to the system thus far.
The search of any one level requires O(1) time. If at the
ith level the item has not already been found (in the cache
or a previous level) (line 14) the client ﬁrst computes the
label under which the item would be stored in the ith level
Bloom ﬁlter (line 15). It then retrieves the encrypted bits
corresponding to that label from the server-stored Bloom
ﬁlter (line 16). If the decrypted bits are all 1 (line 17), the
client has found the item at the ith level. It then computes
the label under which the item is stored in the level (ref-
erenced by its hashtable) (line 18) and asks the server to
remove and return the corresponding item (line 19). If at
least one decrypted Bloom ﬁlter bit is 0 (line 21), the client
instead performs the same operation using a fake label (built
at line 13), known to be stored at the server (line 20).
Once the client has found the item (line 21), it proceeds by
seeking fake items on the subsequent levels. This avoids re-
vealing the level that answered the query, which would pro-
vide a correlation between queries. The client ﬁrst searches
for a fake label (line 22) in the Bloom ﬁlter at the ith level
(line 23), then asks the server to retrieve and remove a fake
item from the level (line 24).
Note that the client queries the remotely-stored encrypted
Bloom ﬁlter by requesting the encrypted values of positions
indicated by the label function. While this reveals the re-
quested Bloom ﬁlter positions to the remote server, nothing
is lost as we prevent correlation by guaranteeing that any
Bloom ﬁlter is only ever queried for any particular item once.
Since the positions in the ﬁlter are each encrypted, the server
never learns the result of the Bloom ﬁlter query.
4.3 Access Privacy
The client achieves access pattern privacy by maintaining
two conditions. First, no item is ever queried twice using
the same label. This is achieved by removing the item, once
it is found, and placing it in the item cache. Thus, on fu-
fakeAccCtr(i) + +;
fakeLabel := hash(i, ”data”, Gen(i), fakeAccCtr(i), K);
if(found = false) do
Algorithm 1 Query answering overview.
1.query(x : id)
2. server : Server; #server stub
4. bits : int[]; #bit values in Bloom filter
5. label, fakeLabel : int[]; #search labels
6. fakeAccCtr : int[]; #per level access counter
7. found : bool;
8. K : int[]; # secret key
9. v : Object; # value for name x
10. found, v := scanServerItemCache(x);
11. for (i := 1; i < log4 n; i + +) do
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26. itemCache.append(x, v);
27. return (x, v);
28.end.
label := hash(i, ”BF”, Gen(i), ”x”, K);
bits := server.getBloomFilter(i, label);
if(decrypt(bits) = ”11..1”) do
else server.getNRemove(fakeLabel) ﬁ;
label := hash(i, ”data”, Gen(i), ”x”, K);
v := server.getNRemove(label); found := true;
else
ﬁ
label := hash(i, ”BF”, Gen(i), fakeAccCtr(i), K);
server.getBloomFilter(i, label);
server.getNRemove(fakeLabel);
ture queries the client will locate it in the item cache before
repeating a label request; fakes will be substituted on the
lower levels. As items propagate out of the item cache (de-
scribed in Section 4.4), the label functions are updated, so
that the item has a diﬀerent label by the time it makes it
back down to a particular level.
Second, the access patterns must appear indistinguishable
from random no matter where the item is located. A set
of fake items is used to guarantee this:
if the Bloom ﬁlter
returns negative, indicating that the item is not stored at
this level, a fake item from this level is retrieved instead.
On every single query, the server observes the same pat-
tern. The client ﬁrst scans the item cache, then queries a
random value (chosen uniformly randomly, independently
from all other information available to the server) from the
level 1 encrypted Bloom ﬁlter – never queried by the client
before. The server can observe the positions in the Bloom
ﬁlter accessed, but it cannot observe whether each position
is set to 1 or 0. The server then observes the client retrieve
and delete one item from the level 1 hash table – never re-
trieved by the client before. This identical pattern of a ran-
dom Bloom ﬁlter lookup followed by a random label-value
retrieval and deletion continues through each level. Finally,
the client appends a (semantically secure) encrypted value
to the item cache.
Success or failure at each level is not revealed – the server
cannot distinguish queries to fake entries in the Bloom ﬁlter
from queries to real items in the ﬁlter, and the server cannot
distinguish either of those from real items that are not in the
ﬁlter. Additionally, the server cannot distinguish requests to
real items from requests to fake items from the hash table,
since the secure hash function used is non-invertible.
Since there are log4 n levels, and a constant amount of
data transfer and computation is exercised on each level by
every query, the online cost per query is O(log n) (measured
in computation or transfer of words). Level reshuﬄing, de-
scribed in the next section, will add an oﬄine amortized cost
per query of O(log n log log n).
We now ﬁll in the missing pieces: how to empty the item
cache when it becomes full, and how to build the levels
and the Bloom ﬁlters without revealing any information the
server can correlate to retrievals.
4.4 Handling Level Overﬂows: Reshufﬂe
The construction of the initial database structure is ex-
plained by the process of emptying the item cache:
items
are inserted into the item cache, which then overﬂows into
the lower levels. Similar to ORAM, when the item cache is
emptied, the contents are poured into level 1. In that pro-
cess, level 1 and its new contents are reshuﬄed according
to new label functions, removing any correlations between
past and future lookups. When level 1 becomes full, it is
poured into level 2, and so forth. Thus, the reshuﬄe process
empties one level i− 1 into the level i below it, which is four
times as large as level i − 1. Level i is then scrambled, hid-
ing the correlation with the items’ previous levels. A new
Bloom ﬁlter for the lower level is constructed – even items
which happened to be at level i anytime in the past are now
identiﬁed by a new unique label.
Let m be the size of the new level (m ≤ 4i). Let h be the
number of hash functions used to generate a Bloom ﬁlter.
Let k1, ..kh be the client’s secret keys used to generate the
Bloom ﬁlters. Let b denote the number of bits in the Bloom
ﬁlter BF at level i. Let W be a working set stored on the
server. Let L be a list of O(m) entries, stored on the server.
Let T be a √m integer array stored at the client. Let Bkt be
a server-hosted list of O(√m) buckets, of √m entries each.
Initially, W , L, T and Bkt are empty and all the bits in the
server-stored BF are set to 0.
In the next steps (steps 2 through 7 in the detailed de-
scription below, and illustrated in Figure 4.4) we build the
encrypted Bloom ﬁlter without revealing the positions set
in the Bloom ﬁlter. This is accomplished by scanning all
items in the level, creating a list of what positions must be
set in the Bloom ﬁlter to add each item, and storing this
list encrypted on the server (step 2). To turn the list into a
proper Bloom ﬁlter bit array, it will be sorted with a bucket
sort – with √m buckets of size √m, so that any bucket ﬁts
in private storage. To keep the buckets indistinguishable,
we ensure they will all have the same size. Next (step 3)
we calculate the size of each bucket, by scanning the list of
Bloom ﬁlter positions, incrementing the appropriate bucket
size tally for each position. In step 4 we add fake positions
that will end up in those buckets that are lacking, accord-
ing to the above (step 3) tally. Each bucket corresponds
to a ﬁxed range of positions in the ﬁnal ﬁlter, so j√m is a
position that will wind up in bucket j. At this point the
server will be able to identify the fake positions, since they
are all at the end of the list. We then (step 5) scramble the
list of positions to destroy all correlation between items and
positions, and hide the fakes. In step 6 we move the scram-
bled positions into their buckets. Step 7 constructs the ﬁnal
Bloom ﬁlter, building a piece from each bucket. Steps 8
and 9 place the items in the new level while eliminating any
correlation between the old and the new level structures.
Figure 2: Level reshuﬄe: Bloom ﬁlter construction (Steps 2 - 7)
The overﬂow process, performed by the client to pour level
i − 1 (or the item cache) into level i proceeds as follows (see
Figure 4.4 for an illustration).
1. Merge levels. Move all items from level i − 1 and
level i into W , a working buﬀer on the remote server.
Discard the Bloom ﬁlters attached to both levels.
2. Build a list representation of the new Bloom
ﬁlter. Increment the Gen(Li) value. Read each item
x ∈ W exactly once, and for each compute its h bit-
positions pj(x) = hash(Gen(Li)||x.id||i||kj ) mod b,
j = 1..h, in the new Bloom ﬁlter. Encrypt each pj(x)
separately and store all E(pj(x)) values on the server-
side list L. This step takes O(m) time, and O(1) pri-
vate (client-side) storage. Costs here and in the fol-
lowing steps are expressed in terms of m = 4i being
the size of the current level.
3. Tally Bloom ﬁlter positions to determine future
bucket sizes. Read each entry of L (the list of en-
crypted Bloom ﬁlter positions prepared in the previous
step) exactly once. At the client side, for each entry
E(p) ∈ L, let idx(p) be the log m/2 most signiﬁcant
bits of p. Then, do T [idx(p)] + +. This step allows