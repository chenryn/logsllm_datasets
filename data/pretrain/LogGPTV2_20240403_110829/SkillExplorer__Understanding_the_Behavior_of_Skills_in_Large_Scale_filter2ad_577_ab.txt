components. There is no standard content format because it
USENIX Association
29th USENIX Security Symposium    2651
is for communication with people. However, the questions
from skills are various. Simply seeking answers from open
knowledge sources may not deliver understandable answers
to skills. Actually, a skill is usually designed to make conver-
sations easy for users. So the expected answers from human
users are usually simple but limited to a certain range. We
design an efﬁcient approach to correctly answer the questions
from skills and make the conversation continue for behavior
exploration.
3 Explore Skills’ Behaviors
In this section, we ﬁrst give an overview (Section 3.1) of
SkillExplorer to explore behaviors of skills, followed by the
detailed design of each component (Section 3.2 to 3.5). Then
we give our implementation (Section 3.6) and evaluate Skill-
Explorer (Section 3.7).
3.1 Overview
As mentioned previously, different from a traditional conver-
sational system, inputs for skills should be in speciﬁed forms
expected by various developers. Thus, besides understanding
the questions given by skills, the answers should also be care-
fully prepared to continue conversations. In this paper, we
design an interactive framework called SkillExplorer to ex-
plore skills’ behaviors. A suite of grammar-based approaches
are designed to solve the unique problems encountered. As
shown in Figure 2, the main procedures include utterances
extraction, question understanding, answers generation, and
skill exploration.
Speciﬁcally, utterances extraction is designed to initialize
the ﬁrst input to a target skill. As there is no question given
by the skill at this stage, to generate an acceptable input, ex-
tra information should be provided. After the ﬁrst input is
generated and fed to the skill, it will feed back the output.
Then SkillExplorer parses the output and further classiﬁes
it into ﬁve types. Further, SkillExplorer generates answers
according to these types. Note that, for some questions related
to users’ proﬁles, SkillExplorer prepares different answers
according to a knowledge database which is prepared by col-
lecting different users’ proﬁles from the Internet. In the end,
skill exploration continues to analyze questions and gener-
ate answers, exploring the behaviors of the target skill. The
conversations are stored for SkillExplorer to check whether
users’ privacy is impacted.
Example. Below, we give an example to detail the process.
Take the skill “The Washington Post” as an example. Firstly,
we obtain the basic information from its web page, including
12 items such as the skill name “The Washington Post”, author
“Washington Post Company”, invocation name “washington
post”, utterance corpus, etc. Particularly, the utterance corpus
contains “Alexa, open Washington Post”, “Alexa, ask Wash-
ington Post for politics” and “Alexa, ask Washington Post for
Post Reports”. SkillExplorer uses the three utterances to start
the interaction with the skill. Here, suppose we feed “Alexa,
open Washington Post” to the skill, which will further return
an output “Welcome to The Washington Post. We have three
daily shows. Just ask me for news, politics, or a story from his-
tory. What would you like to do?”. Then, SkillExplorer parses
the question and identify it as the type Mix question. Later,
SkillExplorer generates corresponding answers “news”, “pol-
itics”, “a story” to explore the three possible behaviors. Note
that the three answers should be fed back to the skill one by
one. Here, the keyword “news” is given, and the conversation
continues until the end. In this process, SkillExplorer records
the position of the branches and restarts the conversation from
the beginning.
Figure 2: Framework of SkillExplorer
3.2 Utterances Extraction
In most cases, developers provide utterance samples in their
skills’ introduction pages in the markets. There is a standard
place that the market requests developers to put utterance
questions there for letting a human user understand how to
use the skill. The position of the utterance questions can be lo-
cated by using “a2s-utterance-box-inner” in the source code
of the web-page, which is easy for SkillExplorer to extract.
Besides the standard position, we also ﬁnd that some de-
velopers give instructions to users in descriptions. To extract
the utterance there is very complicated since descriptions of
skills are written by different developers with different writ-
ing habits. After manually analyzing 100 skills, we ﬁnd that
the utterances usually appear in double-quotes or the form of
lists. This is easy to understand since developers also want the
users to quickly identify the utterances for using their skills.
Based on our analysis, only very few utterances are out of the
scope (less than 1%).
For the utterances in double-quotes, we can directly use
regular expressions to identify them. However, for utterance
in lists, this approach does not work well since some devel-
opers put their company information or other contents in the
lists, which may cause false positives. Further to increase the
accuracy, we consider the number of sentences (Sn) and the
length of the sentence (Sl) in one bullet in the list. Since more
than one sentence in an utterance will be interrupted by the
smart speakers, Sn will always be 1, which is also veriﬁed
by analyzing over 200,000 utterances. Regarding Sl, in most
2652    29th USENIX Security Symposium
USENIX Association
SimulatorQuestion understandingAnswers GenerationInteractive systemUtterance corpusUtterances extractionSkillexploration1234Skillcases, developers will not use long utterances since too long
sentences may make it difﬁcult for users to understand or
repeat. After analyzing over 200,000 utterances, we ﬁnd the
average length is 5 and the longest is 29. The distribution is
shown in Appendix B. We select 15 as the threshold of Sl
to identify utterances (the possibility of utterance with more
than 15 words is 0.82%). Also, considering that utterances
are listed in parallel in the list, if one bullet is not an utterance,
all the sentences in the list should not be utterances.
3.3 Question Understanding
After the ﬁrst utterance is sent to a skill, it will feed back an
output, answering the question, or asking users for further
commands. In this paper, we refer to the outputs given by the
skill as “questions”. SkillExplorer should understand these
questions for continuing the conversation. Different from a
traditional interactive system designed for interacting with
users, skills are usually developed to ﬁnish some pre-deﬁned
tasks. As a consequence, the expected answers are in ﬁxed
forms speciﬁed by the developers so that the skills can pre-
cisely understand them before performing the tasks. However,
the diversity of developers also makes their design in differ-
ent forms. To understand diverse questions, we should divide
questions into several types, and further generate answers
according to their types.
One may use the classiﬁcation of questions according to
English linguistics, which divides questions into two types:
Yes/No questions and Wh questions [7]. However, such clas-
siﬁcation is too rough for our interactive system. We take the
following two questions as examples: Q1: “...Just ask me for
news, politics, or a story from history. What would you like to
do?” and Q2: “ What’s your zip code?” Although they are
both Wh questions, users will answer them in different ways.
For Q1, users will extract answers directly from the question,
which is not suitable for answering Q2. Thus, instead of us-
ing the traditional way to classify questions, we interact with
10,000 randomly selected skills using the extracted utterances
and collect the replies as the Basic Corpus of Replies. Then
we manually analyze 2,000 randomly selected replies (i.e.,
questions) from the corpus. We ﬁnd that the questions can
be divided into ﬁve types according to the ways to generate
answers.
Yes/No questions. The question of this type is an interrog-
ative construction, and expects answers like “yes” or “no”.
A Yes/No question usually has an auxiliary verb in front of
the subject, which is also called subject-auxiliary inversion
(SAI). It has two subtypes: Inverted question (IQ), and Tag
question (TQ). An example of IQ is “Are you going?”, in
which subject and the ﬁrst verb in the verb phrase will be
inverted if the verb is a modal or an auxiliary verb or with
the verb be and have. TQ is a short question at the end of a
sentence, which is often made up of a modal or helping verb
and a subject pronoun. An example is “You’re going, aren’t
you?”. Note that there is an Inverted Alternative Question
(IAQ) such as “Are you staying or going?”, which looks quite
like IQ, but it actually does not require a simple yes or no for
an answer. We should exclude it from this type.
In order to identify Yes/No questions, we use constituent-
based parsing which is popularly used in natural language
processing to analyze questions. A constituency-based parse
tree can represent a context-free grammatical structure of
sentences. Non-terminals in the tree are types of phrases
(tagged by part of speech labels), and leaves are words in
the sentence. We focus on the tag “SQ”, which represents
either a Yes/No question, or the main clause of a wh-question,
following the wh-phrase with tag “SBARQ” (direct question
introduced by a wh-word or a wh-phrase, e.g., “How can
I help you?”) [8]. Examples are shown in Figure 3 (more
examples are shown in Appendix C). Thus, to identify Yes/No
question through using the constituency-based parse tree, we
ﬁrst locate the tag “SQ”, and then ﬁlter out those questions if
“SQ” follows a W-tag (representing wh-phrase or wh-word).
We should also ﬁlter the IAQ by checking whether there is a
“CC” tag (representing the word “or”).
For TQ type, it is a statement followed by a mini-question
which has the form of “auxiliary verb + subject”. We judge
this kind of structure in a parse tree and extract the auxiliary
verb (be, do, have, or a modal verb like will) with a subject.
Considering that decisive questions often appear at the end,
we only judge the last sentence3.
Figure 3: An examples of constituency-based parse tree
Instruction questions. The questions of this type give users
direct guidance on how to answer them. They are essentially
similar to imperative sentences which transfer the guiding
or suggestive information to users. In order to guide users
to reply with the correct answer, the skill usually tells the
user what to say by using the directive keywords (e.g., “say”,
“ask”) in the sentence. For example, “Welcome to the Reddit
Notiﬁer skill... just Say: Help me”.
After manually analyzing questions in Basic Corpus of
Replies, we ﬁnd that over 96% of the instruction questions
use “ask” and “say”. One main reason is the samples given by
Amazon [9,10] for developers to build skills, where “ask” and
“say” are used. The two words are in line with user habits. To
identify such a type, we ﬁrst ﬁnd InstruTag in the constituency-
based parse tree including “VB” (Verb, base form), “VBG”
3Sometimes, we meet very short questions without an auxiliary verb or a
modal verb. We still classify it as Yes/No questions. Such as “next news?”.
USENIX Association
29th USENIX Security Symposium    2653
ROOTSQVBPAreNPPRPyouADJPVBJready-?(Verb, gerund or present participle), and “VBP” (Verb, non-
3rd person singular present) and check whether there are some
command words like “ask” and “say” (or their “-ing” form).
In this way, we can determine whether the question is an
Instruction question.
Selection questions. We refer to the questions containing
multiple parallel answers as “Selection questions”. Some for-
mer studies have a similar category, referring to the questions
as “choice” [30] if the answers are connected by the keyword
“or” (e.g., a question like “...To get started, you can get a quote,
listen to the daily brieﬁng, or get an account summary.”). To
identify such questions (referred to as Selection_CC ), we try
to ﬁnd similar patterns in the constituency-based parse tree.
The patterns should be with tag “CC” (Coordinating conjunc-
tion) that indicates the existence of Paratactic Structure in a
sentence. We also include questions that need to be answered
with serial marks into the selection question (such as “1: high,
2: medium, 3: low. Choose one.”), which has three choices but
no coordinating conjunction. To identify such questions (re-
ferred to as Selection_SC), we extract all numbers and single
characters from the constituency-based parse tree, and then
judge whether these numbers and letters are continuous.
Wh questions. Wh questions are also known as open
questions [7]. Users are supposed to answer such ques-
tions in a free way, instead of obtaining the answers di-
rectly from questions or making some judgments. Their
knowledge or understanding is usually needed in this pro-
cess. An example is “What is your name?”. To iden-
tify questions of this type, we ﬁnd those questions with
WH-tag which include “WDT”, “WHADJP”, “WHADVP”,
“WHNP”, “WHPP”, “WP”, “WP$”, “WP-S”, “WRB” in the
constituency-based parse tree with wh-words. If it contains
related tags, we classify the question as Wh question.
Mix questions. Sometimes the output from skills contains
more than one of the previous four question types. For exam-
ple, the output “You can say repeat or stop.” is the combina-
tion of an Instruction question and a Selection question. We
refer to it as the Mix question.
3.4 Answer Generation
After classifying the questions, we get 5 types of reply content:
Yes/No, Instruction, Selection, Wh, and Mix. We can generate
corresponding answers for different types of questions. For
some types (i.e., Yes/No questions, Instruction questions, and
Selection questions), we can directly extract answers from the
question itself. For Wh questions, we generate a knowledge
database to answer the question and explore the behaviors
of skills. For Mix questions, we have strategies to answer it
according to the question types it contains. We show some
examples in Figure 4.
Yes/No questions. We simply generate the answers as “yes”
or “no” to the questions.
Figure 4: Q&A samples.
ASK
SAY
ask (sb.) Wh-Q
ask sth. like/... INS
ask (sb.) to INS
ask (sb.) (about/for) INS
ask that INS
say Wh-Q
say sth. like/... INS
say INS (to do sth)
say INS for sth
say (that) INS
Table 1: Rules to generate answers for Instruction questions
Instruction questions. Based on previous analysis, our anal-
ysis focuses on ASK and SAY4. We look into the Oxford
Learners Dictionaries [11], and ﬁnd that the two words ASK
and SAY usually have ﬁve patterns, as shown in Table 1. For
example, the skill can “ask (sb.) to INS”. The components in
the brackets (e.g., “sb.” here) are not necessary for a sentence.
INS is the instruction that we should extract. Sometimes, Wh
questions are used as a component (e.g., ask (sb) Wh-Q). Wh-
Q is the Wh question here. An example is “You can say what
is the current sibor rates”.
To identify the ﬁve patterns in an instruction question, we
ﬁrst get the constituency-based parsing trees for the question.
According to the ﬁve rules in Table 1, we can specify the
matching rules based on them and use regular expressions to
identify which pattern is used in the question. Then according
to the patterns, we extract the INS or Wh-Q as the answers to
the questions.
Selection questions. In this type, the expected answers are
usually connected in parallel by conjunctions (e.g., “or”,
“and”) (referred to as Selection_CC), or clearly marked by
indicators such as the numbers or letters (referred to as Se-
lection_SC). So users can directly speak out the answer itself
or feed back the indicator. For example, the skill myTuner
Radio says: “Ok, Here’s myTuner Radio. I’ve found: 1: CHOI-
FM Radio X 98.1 from Canada, 2: Ibiza X Radio from the
United Kingdom, 3: Radio X London from the United King-
4To include more words in the future, we can quickly generate the rules
for them using dictionaries [11].
2654    29th USENIX Security Symposium