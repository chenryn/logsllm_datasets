port(s), congestion ensues and mirrored packets get buffered. If
congestion persists, eventually the buffer ﬁlls up, at which time the
switch starts dropping mirrored packets. This effect constrains the
rate of samples that Planck can capture to the aggregate bandwidth
of the monitor port(s). In general, we can subdivide a given switch’s
N ports in to k monitor ports and N − k normal data ports. In the
common case, we expect that k = 1 will provide more than enough
visibility into the network at the cost of giving up only a single port
per switch. In Section 5 we show that this approach is feasible.
One complication of this design is that the sampling rate, the
fraction of packets passing through the switch that are mirrored,
varies as trafﬁc load changes. When trafﬁc ﬂowing through the
switch is light, 100% of packets are mirrored. Once trafﬁc exceeds
the capacity of the monitor port(s), Planck is limited to receiving as
many packets as the monitor port(s) can carry. The instantaneous
sampling rate is unknown because it dynamically scales proportional
to the rate of trafﬁc being mirrored divided by the bandwidth of
the output port(s). We compensate for this uncertainty by using
sequence numbers to compute throughput (see Section 3.2.2) rather
than requiring a known sampling rate.
3.2 Collector
The collector has four major goals: (i) process sampled packets
at line rate, (ii) infer the input and output ports for each packet, (iii)
determine ﬂow rates and link utilization, and (iv) answer relevant
queries about the state of the network.
The collector uses netmap [30] for line-rate processing and bor-
rows from a substantial body of work on line-rate packet processing
on commodity servers [18, 19, 24, 28], so only the last three goals
are discussed in detail here. Without monitor port buffer latency, the
collector can determine reasonably stable global network statistics
every few hundred microseconds, which is on par with an RTT in
our network. While the collector determines utilization statistics on
shorter time periods, these statistics are not stable due to the on/off
nature of Ethernet and the bursty behavior of TCP.
3.2.1 Determining Input and Output Ports
Traditional network sampling techniques append metadata, e.g.,
input and output ports, to each packet sample. This metadata is im-
portant for determining if a given port (link) is congested. Mirrored
packets do not include metadata, so the collector must infer input
and output ports from the packet alone. To solve this problem, the
SDN controller shares the topology of the network and the rules
embedded in each switch’s forwarding tables with the Planck col-
lector. As long as the network employs deterministic routing, which
includes ECMP if the hash function is known, the collector can
infer the full path that a packet follows based on the packet header,
and thus determine the input and output port the packet traversed
through a particular switch. Keeping the forwarding table informa-
tion consistent between the controller, switches, and collector(s)
requires care, but in practice it does not vary quickly. Alternately,
the collectors can infer the path any given ﬂow takes, and thus the
input and output ports on each switch, based on samples from mul-
tiple switches. However, depending on the sampling rate and the
length of a ﬂow, the information about paths could be incomplete.
3.2.2 Determining Flow Rates and Link Utilization
The Planck collector parses packet headers to maintain a NetFlow-
like ﬂow table that tracks information about individual TCP ﬂows,
including their throughputs and paths through the network. Tradi-
tionally, sampling-based measurements determine ﬂow rates and
(a)
(b)
Figure 1: Planck architecture: (a) Switch architecture illus-
trating the fast (Planck) and slow (sFlow) paths. (b) Network
architecture showing one pod of a fat tree consisting of hosts
(H), switches (S), collector(s), and the SDN controller. Traf-
ﬁc from hosts is forwarded by the switches to the collector(s),
where they are processed. The collector(s) send events to the
controller, which can send messages to the switches to reconﬁg-
ure routes.
NetFlow takes a slightly different approach—it maintains a cache
of information on active TCP and UDP ﬂows. Whenever a switch
receives a packet, NetFlow checks to see if the packet belongs to
a cached ﬂow. If so, it increments the associated ﬂow counters.
If not, it creates a new cache entry. If the cache is full, an older
entry is evicted and sent to the collector. This approach uses the
collector like a backing store for information about layer-4 ﬂows
crossing a given device. Cache entries also can be conﬁgured to
time out periodically, giving the collector a more up-to-date view of
the network, but the timeouts are on the order of seconds [3], which
provides little or no advantage over counter polling.
3. DESIGN
Figure 1 illustrates Planck’s three main components: (i) switches
conﬁgured to provide samples at high rates, (ii) a set of collectors
that process those samples and turn them into events and queryable
data, and (iii) a controller that can act on the events and data. The
remainder of this section discusses each of those elements in turn.
3.1 Fast Sampling at Switches
As previously discussed, current mechanisms for extracting switch
measurements leave much to be desired. Even sFlow [32], which is
designed to provide samples in real time, can only generate hundreds
of samples per second on a modern 10 Gbps switch. At that rate, it
takes seconds to infer network state with accuracy. We overcome
this limitation by leveraging the port mirroring feature found in most
commodity switches. Port mirroring enables non-disruptive trafﬁc
monitoring by replicating all—or a subset of—trafﬁc destined for a
given output port to a designated monitor port.
We repurpose this functionality to support high sampling rates
by oversubscribing the monitor port(s), i.e., conﬁguring the switch
such that trafﬁc destined to multiple output ports is replicated to
each monitor port. When the total trafﬁc ﬂowing through the mir-
CollectorControl Plane CPUSwitch ASICPCIe1 GbpsEthernet10 Gbps PortsSrcDst10 Gbps EthernetsFlowPlanckCollector(s)ControllerHHHHSSSSS)link utilization by multiplying the throughput of samples received
for a given ﬂow or port by the sampling rate. However, port mirror-
ing does not provide a ﬁxed sampling rate.
To determine the throughput of TCP ﬂows, the collector tracks
byte counts over time by using the TCP sequence numbers, which
are byte counters in and of themselves. If the collector receives
a TCP packet A with sequence number SA at time tA, and a TCP
packet B with sequence number SB at time tB from the same ﬂow,
such that tA 
SB, it cannot determine if this is due to reordering or retransmission,
and thus ignores the packet when it comes to throughput estimation.
In practice, out of order packets are uncommon enough in data center
networks that ignoring them does not affect accuracy signiﬁcantly.
A collector could infer the rate of retransmissions based on the
number of duplicate TCP sequence numbers it sees, but we leave
this to future work.
While the discussion thus far has focused on TCP, many other
types of trafﬁc contain sequence numbers, and the previously de-
scribed method is general enough to apply to any packet type that
places sequence numbers in packets. If the sequence numbers rep-
resent packets rather than bytes, then they need to be multiplied by
the average packet size seen in samples as well, but this shouldn’t
signiﬁcantly hurt rate estimation. We leave developing a model for
throughput of ﬂows without sequence numbers to future work.
3.3 Controller
The SDN controller performs two Planck-speciﬁc functions: (i)
install mirroring rules in the switches and (ii) keep the collector(s)
informed of the network topology and forwarding rules. It also
exports functionality to network applications: (iii) the ability to
query for link and ﬂow statistics and (iv) the ability to subscribe to
collector events.
The simplest Planck-speciﬁc SDN controller extension is to for-
ward statistics requests to the collectors. This acts as a drop-in
replacement for most, if not all, SDN controller statistics APIs and
typically results in a much lower latency statistics capability than
provided by SDN controllers, e.g., using OpenFlow counters.
Perhaps a more interesting new feature of Planck is that it allows
SDN applications to subscribe to events generated by the collec-
tor(s). This allows SDN controller applications to start reacting to
network events within milliseconds of when they are detected by the
collector(s). Currently, the only events exported by the collector(s)
are when link utilizations cross a speciﬁed threshold, but it would be
straightforward to add others. Events include context annotations,
e.g., the link congestion event includes the ﬂows using the link and
their current estimated rates. Annotations are intended to help an
2At 10 Gbps, we ﬁnd 200 µs works well as a minimum gap size.
application better respond to the event, e.g., reroute an individual
ﬂow to deal with the congestion.
4.
IMPLEMENTATION
Planck’s implementation consists of a fast packet collector and
an extended OpenFlow controller. The collector is built using
netmap [30] and the controller is built on Floodlight [12]. We detail
the base implementation in this section; in Section 6 we discuss
extensions for vantage point monitoring and trafﬁc engineering.
4.1 Base Controller
The Planck controller is a modiﬁed and extended Floodlight
OpenFlow controller [12]. Its main features are (i) a modiﬁed rout-
ing module that adds actions to the switch forwarding tables to
mirror trafﬁc and (ii) a module that communicates installed and
alternate routes to the collector(s) for trafﬁc input-output port infer-
ence. The former involved modifying Floodlight’s routing module
to dynamically add a second OpenFlow output action to each rule
that replicates trafﬁc to the appropriate monitor port. The latter
involved extending the controller to broadcast updates to the col-
lector(s) whenever it modiﬁes existing routes or installs new ones.
The controller also refrains from using new or modiﬁed routes for
a short period of time after installing them to give the collector(s)
time to process the new route information and thus infer input and
output ports correctly.
4.2 Collector
In our current design, each monitor port is associated with a
separate collector process and is connected directly to the server on
which this process executes, i.e., mirrored trafﬁc does not traverse
intermediate switches. Numerous collector instances are run on a
single physical machine. Each collector instance runs in user space
and connects to the netmap [30] Linux kernel module to receive
samples. Because the collector knows the current state of its switch’s
forwarding tables and because we route on MAC addresses, it can
uniquely identify an arbitrary packet’s output port based solely on
the destination MAC address of the packet. Further, it can uniquely
identify the input port based only on the source-destination MAC
address pair.
Our current implementation supports three queries: (i) link uti-
lization, (ii) rate estimation of ﬂows crossing a given link, and (iii)
a raw dump of a conﬁgurable number of the last packet samples.
Additionally, it allows for subscription to link utilization events. We
leave the development and support for more queries to future work.
5. PLANCK EVALUATION
In this section, we describe microbenchmarks we ran to: (i) deter-
mine the impact of oversubscribed port mirroring on switch trafﬁc,
(ii) characterize sampled trafﬁc, and (iii) evaluate the accuracy of
Planck’s throughput estimation scheme. All the microbenchmarks
are performed on a single switch—testbed details can be found in
Section 7.1.
5.1
Impact of Mirroring on Switch Trafﬁc
To accommodate transient output port congestion, modern switches
dedicate a modest amount of memory for buffering packets waiting
to be forwarded. For example, the Broadcom Trident switch ASIC
contains 9 MB of buffer space shared between its 64 ports, of which
a small amount is dedicated to each output port while most is al-
located dynamically. A single congested port can consume up to
4 MB of buffer space, which for a 10 Gbps switch adds 3.5 ms of
queueing latency. Counterintuitively, latency induced by congestion
Figure 2: Drops of non-mirrored packets, as logged on the
switch, as the number of congested output ports is varied.
decreases as more ports become congested because each port re-
ceives a smaller share of the shared buffer. Since Planck deliberately
oversubscribes mirror ports, mirrored trafﬁc is frequently buffered,
which has two negative effects: (i) samples are delayed and (ii) any
buffer space used to buffer samples is not available to handle bursts
on other (non-mirror) ports.
To understand the impact of oversubscribed port mirroring on
the non-mirrored (original) trafﬁc passing through the switch, we
conduct a series of tests to measure the loss, latency, and throughput
of non-mirrored trafﬁc under a variety of network conditions when
mirroring is enabled and disabled. We vary the number of congested
ports, ones where two hosts saturate TCP trafﬁc to the same destina-
tion, from one (three hosts) to nine (27 hosts) to stress the shared
switch buffers. All graphs are generated over 15 runs.
Port mirroring uses some of the shared buffer space on the switch,
and thus there is less buffer space available to the other ports. Fig-
ure 2 shows that port mirroring can increase non-mirrored trafﬁc
loss due to decreased buffer space, but the absolute drop rates are
very small, with average loss less than 0.12%. Figure 3 shows the
latency of non-mirrored trafﬁc as we vary the number of congested
ports. Decreased buffer space at the switch manifests itself in lower
latencies for the average (not shown), median, and 99th-percentile
cases when mirroring is enabled. Also shown in the ﬁgure is the
99.9th-percentile latency, which shows the effect of retransmission
delays from extra loss incurred due to buffer space contention. Note
that since loss rates are low, we only see latency affected in the
99.9th-percentile. Finally, Figure 4 shows that the median and tail
(0.1th-percentile) ﬂow throughput for non-mirrored trafﬁc is unaf-
fected by mirroring. As discussed in Section 9.2, limiting the buffer
space made available to sampling ports should mitigate the already
small impacts of our scheme.
5.2 Undersubscribed Sample Latency
Ideally, to measure the sample latency, we would measure the
time from when the ﬁrst bit of a packet arrived on the wire toward a