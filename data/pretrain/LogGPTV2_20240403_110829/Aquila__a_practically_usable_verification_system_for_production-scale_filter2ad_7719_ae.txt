### Header Order and Packet Processing

The header order in the packet is not meticulously designed for reuse elsewhere. The engineer overlooked this issue, leading to an incorrect returned packet.

### Scenario 3: Pre-Update Bug Checking

Given the diversity of our business, network engineers frequently update data plane programs to meet service requirements. However, program updates are a major cause of significant service disruptions. To mitigate this, we use Aquila to verify updates. For these scenarios, we typically use the original specification to ensure the program's behavior remains consistent before and after updates.

In a critical update event, we needed to swap the pipelines of the load balancer and switch, moving `switch.p4` to Pipeline 1 and `loadbalancer.p4` to Pipeline 0, as shown in Figure 2. This requirement stems from the fact that the load balancer pipeline includes packet processing functions such as virtual network encapsulation/decapsulation (e.g., Network Address Translation, or NAT). These functions can change the forwarding source or destination address of a packet, which the switch pipeline depends on. For example, NAT translates the destination address from a public IP to an internal IP, which the switch pipeline uses to forward the packet to the correct server. Therefore, placing `loadbalancer.p4` before `switch.p4` ensures the switch's forwarding behavior is correct without relying on complex metadata.

This update required extensive modifications to the P4 programs, including changes to the input packet format of the load balancer program and adding recirculation to the switch. Such changes are prone to causing functional inconsistencies. Aquila detected a critical bug involving two tables: an ACL table `a` that accepts IP address `10.0.1/24` but drops `20.0.1/24`, and a forwarding table `b` that changes the IP address in the packet header from `10.0.1/24` to `20.0.1/24`. Before the update, a packet sent to `10.0.1/24` could be successfully transformed into a packet with destination IP `20.0.1/24` due to table `b`. However, the update moved `b` in front of `a`, causing `b` to first change the packet’s IP from `10.0.1/24` to `20.0.1/24`, and then the packet is dropped by table `a`'s ACL rules. If such an update were committed online, all traffic destined for `10.0.1/24` would be blocked. Manually detecting this bug is challenging due to the complex logic across pipelines.

**Note:** The IP addresses have been anonymized for confidentiality reasons.

### Self-Validation Experience

Most bugs in Aquila were detected during its early development stages. The self-validator helped us identify numerous implementation bugs, caused by language misunderstandings, incorrect function implementations, and chip-specific feature misunderstandings. We also encountered bugs in the alternative representation implementation. Since Aquila’s verification results have contained no false positives in recent months, we believe the self-validator has successfully assisted in tuning the correctness of Aquila. Below are two examples of detected bugs:

#### Language Misunderstanding
Aquila developers may misunderstand or ignore certain P4 language features, inadvertently introducing bugs. For example, the initial version of Aquila ignored the `@defaultonly` annotation, which restricts an action, causing a correct program to violate its specification.

#### Incorrect Function Implementation
Function-level implementation bugs accounted for the majority of Aquila’s issues. Some of these bugs were difficult to detect. For instance, our encoding module initially failed to handle empty states correctly, returning headers that had already been extracted. This bug treated an empty state as an 'accept' state, causing the parser encoding to accept more packets than intended. The self-validator revealed this subtle inconsistency, which would be very unlikely to be caught through human code review.

### Performance Evaluation

All experiments were conducted in a container with 32GB RAM and one 2.5GHz CPU core, using Z3 4.8.5 as the SMT solver. We evaluated performance based on the end-to-end verification time, which includes parsing, encoding, and verification processes.

### Benchmark

We selected twelve P4 programs, eight open-sourced and four private, to compare Aquila's performance with Vera and p4v. We checked for invalid header access bugs, as they are a good benchmarking property [30]. Each chosen program had at least one such bug. We recorded the time to find the first bug (by checking all assertions together) and all bugs (by checking each assertion individually), setting a 2-hour timeout threshold. In all experiments, we made no assumptions about table entries, registers, or input packets. As shown in Table 3, Aquila could report bugs within one second for small programs and within one minute for large programs. Even for the largest program, Aquila only needed 4.8 Gigabytes of memory. In comparison, Vera took significantly longer to verify small programs, and p4v ran out of memory for large programs due to a lack of scalable encoding approaches. We also observed higher memory usage when finding the first bug, as Aquila encoded all assertions at once and maintained more states.

| **Program** | **Simple Router** | **NetPaxos Acceptor [8]** | **NetPaxos Coordinator** | **NDP [22]** | **Flowlet Switching** | **NetCache [26]** | **Switch BMv2 w/o INT** | **Switch BMv2** | **LoC** | **Pipes** | **Parser States** | **Tables** | **Time (s) (Finding first/all bugs)** | **Memory(GB) (Finding first/all bugs)** |
|-------------|-------------------|---------------------------|--------------------------|--------------|-----------------------|--------------------|-------------------------|-----------------|---------|-----------|-------------------|------------|-------------------------------------|--------------------------------------|
| **Vera**    | 0.01 / 0.02       | 0.01 / 0.09               | 0.01 / 0.03              | 0.01 / 0.04  | 0.01 / 0.04           | 0.17 / 9.56        | 1.26 / 290              | 1.41 / 347      | 20.1 / 1286 | 23.7 / 558 | 25.2 / 733        | 41.3 / 3574 | 0.1 / 0.2                           | 0.1 / 0.2                            |
| **p4v**     | 0.42 / 0.45       | 9.71 / 9.71               | 4.44 / 6.70              | 0.50 / 0.51  | 2.75 / 2.94           | 0.90 / 241         | 13.5 / OOT              | 226 / OOT       | 0.01 / 0.02 | 0.01 / 0.17 | 0.01 / 0.06       | 0.01 / 0.06 | 0.22 / 16.1                         | 0.1 / 0.2                            |
| **Aquila**  | 0.01 / 0.02       | 0.01 / 0.09               | 0.01 / 0.03              | 0.01 / 0.04  | 0.01 / 0.04           | 0.17 / 9.56        | 1.26 / 290              | 1.41 / 347      | 20.1 / 1286 | 23.7 / 558 | 25.2 / 733        | 41.3 / 3574 | 0.1 / 0.2                           | 0.1 / 0.2                            |

### Scalability

Aquila's performance is directly affected by the complexity of the encoded GCL representation, which is influenced by the input program's complexity and the size of data structures, such as the number of table entries. To evaluate scalability, we conducted two experiments using a vendor-provided `switch.p4` program (called `switch-T`) with thousands of lines of code.