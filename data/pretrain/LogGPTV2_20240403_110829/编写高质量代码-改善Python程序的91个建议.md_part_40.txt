口join_threadO：一般在close方法之后使用，它会阻止直到的后台线程退出，确保所有
缓冲区中的数据已经刷新到管道中。
将退出。
管道的时候可以使用该方法，表明无需阻止到后台线程的退出。
adtd=dndu'dandano
while True:
input_p.close()
while True:
input_p.send(i)
except EOFError:
try:
break
msg=output_p.recv()
#从pipe中读取消息
#利用队列来发送消息
#发送消息
#写消息到管道中
#返回管道的两端
第8章性能剖析与优化249
内部去掉了
否
它
---
## Page 263
250
主要函数为 writer_queueO和reader_queueO。从函数输出可以看出，pipe所消耗的时间较小，
的时候的性能，其中与pipe 相关的函数为reader_pipeO和 writer_pipeO，而与queue 相关的
上面的代码分别用来测试两个多线程的情况下使用pipe和queue进行通信发送相同数据
Sending
Sending
Sending
testing
Sending
Sending
Sending
输出比较：
testing
if
100000 numbers to Queue()took 3.0790002346 seconds
10000 numbers to Queue() took 0.555000066757 seconds
1000numbers to Queue() took0.169000148773 seconds
forqueue:
100000 numbers to Pipe() took 2.09099984169 seconds
10000 numbers toPipe() took0.384999990463 seconds
1000 numbers to Pipe() took 0.15299987793 seconds
print"testing for queue:"
queue.put('DONE')
for pipe:
main
'sunoo) % uspuooes s% xoon ()anano on staqunu s% butpuas autad
queue.put(ii)
reader_p.join()
reader_p.start()
reader_p=Process(target=reader_queue,args=((queue),))
queue =Queue()
reader_p.join()
input_p.close()
writer_pipe(count,input_p)
reader_p.daemon=True
_start= time.time()
output_p.close()
reader_p.start()
output_p
if
msg=queue.get()
（mSg =='DONE'）：
(time.time()
input_p=Pipe()
10**4，
break
10**5]：
10**5]：
_start))
#写消息到queue中
#利用queue进行通信
#等待进程处理完毕
#启动进程
#写消息到管道中
#放入消息队列中
#从队列中获取元素
---
## Page 264
例子要保证每次运行都输出100，需要将函数func修改如下：
以为Value是进程安全的，实际上要真正控制同步访问，需要实现获取这个锁。因此上面的
果 lock 的值为True会创建一个锁对象用于同步访问控制，该值默认为True。因此很多人会
迷惑的描述：在Value 的构造函数 multiprocessing.Value(typecode_or_type,*args[,lock])中，如
几个例子来看一下各自使用需要注意的问题。
服务器进程管理器ManagerO使用起来更加方便，并且支持本地和远程内存共享。我们通过
现数据和状态的共享。这两种方式各有优势，总体来说共享内存的方式更快，效率更高，但
multiprocessing.sharedctypes来实现内存共享，也可以通过服务器进程管理器ManagerO来实
资源共享。但如果不可避免，可以通过multiprocessing.Value和multiprocessing.Array或者
性能更好。
2）尽量避免资源共享。相比于线程，进程之间资源共享的开销较大，因此要尽量避免
if
def func(val):
def f（ns):
示例二：使用Manager进行内存共享。
上面的程序输出是多少？100 对吗？你可以运行看看。Python 官方文档中有个容易让人
import multiprocessing
print v.value
def func(val):
from multiprocessing import Process,Value
示例一：使用Value进行内存共享。
import time
name
for iin range（10):
for p in processList:p.join()
processList=[Process(target=func; args=(v,）） for i in range(10)]
manager = multiprocessing.Manager()
for i in range(10):
ns.x=[]
ns = manager.Namespace()
ns.x.append(1)
time.sleep(0.1)
val.value +=1
time.sleep(0.1)
with val.get_lock():
main
main__':
val.value +=
：
#仍然需要使用get_1ock方法来获取锁对象
#manager内部包括可变对象
#使用value来共享内存
#多个进程同时修改val
第8章性能剖析与优化251
---
## Page 265
252
子进程的构造函数的参数传递进去。因此要避免如下方式：
台中父子进程相对独立，因此为了更好地保持平台的兼容性，最好能够将相关资源对象作为
有的资源，如数据结构、打开的文件或者数据库的连接都会在子进程中共享，而Windows平
确的处理方式应该是下面这种形式：
内部还包括可修改的对象，则内部可修改对象的任何更改都不会传播到其他进程。因此，正
manager.listO) 对象，管理列表本身的任何更改会传播到所有其他进程。但是，如果容器对象
是为什么呢？这是因为 manager对象仅能传播对一个可变对象本身所做的修改，如有一个
p.join()
3）注意平台之间的差异。由于Linux平台使用forkO函数来创建进程，因此父进程中所
if
本意是希望x=[1]，y-=[‘a’］，程序输出是不是期望的结果呢？答案是否定的。这又
def child(f）:
f=None
def f（ns,x,y):
import multiprocessing
p.start()
p=Process(target=child)
f=open（filename，mode)
#do something
name
p.join()
p.start()
ns.y=[]
ns.x=[]
manager = multiprocessing.Manager()
ns.y=y
ns.x=x
y.append('a')
x.append(1)
=multiprocessing.Process(target=f,
ns.y=[]
p.join()
.start()
=multiprocessing.Process(target=f,args=(ns,))
！
_main_
：
args=(ns,ns.x,ns.y,))
#将可变对象也作为参数传入
#修改根本不会生效
---
## Page 266
PicklingError异常，这是因为函数和方法是不可序列化的。
序列化的。在下面的例子中，如果直接将一个方法作为参数传人map中，会抛出cPickle.
这样可以避免有可能出现的RuntimeError或者死锁。
据库的连接都会在子进程共享。但Windows平台上由于没有 forkO函数，父子进程相对独
程与父进程的数据是完全相同，因此父进程中所有的资源，如数据结构、打开的文件或者数
4）尽量避免使用terminate0方式终止进程，并且确保pool.map中传入的参数是可以
class calculate(object):
需要注意的是，Linux平台上multiprocessing的实现是基于C库中的 forkO，所有子进
def unwrap_self_f（arg,
import multiprocessing
一个可行的正确做法如下：
print cl.run()
cl=calculate()
class calculate(object):
if
def child(f):
而推荐使用如下方式：
print cl.run()
def f（self,x):
def run(self):
def run(self):
p.join()
printf
p.start()
d
name
=Process(target=child,args=(f,）)
=open(filename,mode)
calculate()
return p.map（f,[1,2,3])
p=Pool()
def f（x）:
return x*x
return x*x
p=multiprocessing.Pool()
return p.map（unwrap_self_f，zip([self]*3,[1,2,3]))
main
**kwarg)：
#返回一个对象
#抛出cPickle.PicklingError异常
#直接传入函数f
#将资源对象作为构造函数参数传入
第8章性能剖析与优化253
---
## Page 267
254
块。我们先来看一个线程池模式的简单实现。
能负荷过大、响应过慢等问题。
任务实际处理时间较短的应用场景，它能有效避免由于系统中创建线程过多而导致的系统性
性能和系统稳定性。线程池技术适合处理突发性大量请求或者需要大量线程来完成任务、但
理下一个任务，因此能够避免多次创建线程，从而节省线程创建和销毁的开销，带来更好的
多，线程执行完当前任务后，会从队列中取下一个任务，直到所有的任务已经完成。
中，所需要执行的任务通常被安排在队列中。通常情况下，需要处理的任务比线程数目要
然会增加系统的相应时间，降低效率。而线程体的运行时间Tr不可控制，在这种情况下如何
如果线程不能够被重用，就意味着每次创建都需要经过启动、销毁和运行这3个过程。这必
动时间（Ts）线程体的运行时间（Tr）以及线程的销毁时间（Td)。在多线程处理的情景中，
到终止，线程便不断在运行、就绪和阻塞这3个状态之间转换直至销毁。而真正占有CPU
提高线程运行的效率呢？线程池便是一个解决方案。
的只有运行、创建和销毁这3个状态。一个线程的运行时间由此可以分为3部分：线程的启
建议89：使用线程池提高效率
在Python中利用线程池有两种解决方案：一是自已实现线程池模式，二是使用线程池模
线程池的基本原理如图8-6所示，它通过将事先创建多个能够执行任务的线程放入池
我们知道线程的生命周期分为5个状态：创建、就绪、运行、阻塞和终止。自线程创建
class Worker(threading.Thread):
#处理request 的工作线程
import urllib2,os
import Queue,sys,threading
由于线程预先被创建并放入线程池中，同时处理完当前任务之后并不销毁而是被安排处
任务
def
threading.Thread._init_
init
_(self，workQueue，
任务队列
图8-6线程池的基本原理
线程池
_（self，**kwds）
线程N
resultQueue,
**kwds):
线程2
---
## Page 268
class WorkerManager:
def download_file（url):
def
def
def
def
def
def
def run( self ):
with open（fname,
print"begin download",url
return self.resultQueue.get(*args,**kwds
get_result( self，*args,
urlhandler=urllib2.urlopen（url)
self.workQueue.put((callable,args,kwds)
add_job(self，
print "All jobs were completed."
while len(self.workers):
wait_for_complete(self):
start(self):
for i in range（num_of_workers ):
_recruitThreads(self,num_of_workers ):
self.workers =[]
self.resultQueue =Queue.Queue()
while True:
self.workQueue =workQueue
self.setDaemon（True)
init_
if worker.isAlive() and not self.workQueue.empty():
worker.join()
w.start()
self.workers.append(worker)
exceptQueue.Empty:
try:
chunk=
self.workers.append(worker)
_（self，
break