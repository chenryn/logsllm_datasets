of cross-trafﬁc even though they still have available band-
width much more than the fast-ethernet capacity. Also no-
tice that similar to our testbed experiments, fast-ethernet
links only have 96mb/s IP-layer capacity.
We repeat the same experiment on another path from the
RON node pwh in Sunnyvale California to the NYU RON
node. This path has 13 hops and a fast-ethernet minimum
capacity. Due to substantial cross-trafﬁc burstiness along
the path, we use packet-trains of 129-packet length in our
probing experiment. The other parameters such as the in-
put rates and the number of trains used for each rate are
USENIX Association
Internet Measurement Conference 2005  
183
the same as in the previous experiment. The whole mea-
surement duration is about 20 minutes. The measured re-
sponse curves are plotted in Fig. 3(b). As we see, the
results exhibit more measurement variability compared to
the lulea→CMU path. However, as packet-train length in-
creases, the variability is gradually smoothed out and the
response curve converges to a piece-wise linear bound. We
again apply linear regression on the response curve with
packet-train length 129 to obtain the tight link information.
We get Cb = 80mb/s and λb = 3mb/s, which does not
agree with the minimum capacity reported by pathrate. We
believe that pathrate reported the correct information. Our
underestimation is most probably due to the fact that there
are links along the path with very similar available band-
width. Consequently, the second linear segment become
too short to detect. The linear segment we are acting upon
is likely to be a latter one. This experiment conﬁrms our
analysis, at the same time shows some of the potential difﬁ-
culties in exacting tight link information from the response
curves.
6
Implications
We now discuss the implications of our results on existing
measurement proposals. Except for pathChirp, all other
techniques such as TOPP, pathload, PTR, and Spruce are
related to our analysis.
6.1 TOPP
TOPP is based on multi-hop ﬂuid rate response curve ˜F
with one-hop persistent cross-trafﬁc routing. TOPP uses
packet-pairs to measure the real rate response curve ˜Z, and
assumes that the measured curve will be the same as ˜F
when a large number of packet-pairs are used. However,
our analysis shows that the real curve ˜Z is different from ˜F,
especially when packet-trains of short length are used (e.g.,
packet-pairs). Note that there is not much path information
in ˜Z that is readily extractable unless it is sufﬁciently close
to its ﬂuid counterpart ˜F. Hence, to put TOPP to work in
practice, one must use long packet-trains instead of packet-
pairs.
6.2 Spruce
Using the notations in this paper, we can write spruce’s
available bandwidth estimator as follows
(cid:18)1 −
Cb
GN (s/Cb, s, n) − s/Cb
s/Cb
(33)
(cid:19) ,
where the probing packet size s is set to 1500bytes, the
packet-train length n = 2, and the bottleneck link capacity
Cb is assumed known.
rI /rO
Elastic Deviation
Non-elastic Deviation
Elastic Deviation
˜Z
˜F
˜S
rI
AP
Cb
s/α2
C 0
b
Figure 4: Illustration of two types of curve deviations.
It is shown in [9] that the spruce estimator is unbiased in
single-hop paths regardless of the packet-train parameters
s and n. This means that the statistical mean of (33) is
equal to AP for any s > 0 and any n ≥ 2. In a multi-hop
path P, a necessary condition to maintain the unbiasedness
property of the spruce estimator is
˜Z(Cb, s, n) =
λb + Cb
Cb
= ˜S(Cb).
(34)
This means that at the input rate point Cb, the real rate re-
sponse of path P must be equal to the single-hop ﬂuid rate
response at the tight link of P.
This condition is usually not satisﬁed. Instead, due to
Theorem 2 and Property 4, we have
˜Z(Cb, s, n) ≥ ˜F(Cb) ≥ ˜S(Cb).
(35)
This implies that (33) is a negatively biased estimator of
AP. The amount of bias is given by
Cb(cid:16) ˜Z(Cb, s, n) − ˜F (Cb)(cid:17) + Cb(cid:16) ˜F(Cb) − ˜S(Cb)(cid:17). (36)
The ﬁrst additive term in (36) is the measurement bias
caused by the curve deviation of ˜Z from ˜F at input rate
Cb, which vanishes as n → ∞ due to Theorem 5. Hence
we call it elastic bias. The second additive term is the por-
tion of measurement bias caused by the curve deviation of
˜F from ˜S at input rate Cb, which remains constant with
respect to the packet-train parameters s and n. Therefore
it is non-elastic. We illustrate the two types of curve devi-
ations in Fig. 4. Note that when Cb < s/α2, non-elastic
bias is 0. Further recall that s/α2 ≥ Ab2 as stated in Prop-
erty 3. Hence, a sufﬁcient condition for zero non-elastic
bias is Cb ≤ Ab2. Conceptually, elastic deviation stems
from cross-trafﬁc burstiness and non-elastic deviation is a
consequence of multi-hop effects.
In Table 2, we give the amount measurement bias caused
by the two types of curve deviations in both the Emulab
testbed experiments and the real Internet probing measure-
ment on the path from lulea to CMU. Note that in the
184
Internet Measurement Conference 2005
USENIX Association
experiment
Emulab-1
Emulab-2
lulea-cmu
elastic bias
0.56 × 96
0.28 × 96
0.25 × 96
non-elastic bias
total bias
0.315 × 96
0.125 × 96
0
74.4
38.8
24
Table 2: Spruce bias in Emulab and Internet experiment (in
mb/s).
testbed experiment using a 3-hop path with one-hop per-
sistent routing, spruce suffers about 74mb/s measurement
bias, which is twice as much as the actual path available
bandwidth 36mb/s. In the second Emulab experiment us-
ing path-persistent cross-trafﬁc, the measurement bias is re-
duced to 38.8mb/s, which however is still more than the
actual available bandwidth. In both cases, spruce estima-
tor converges to negative values. We used spruce to es-
timate the two paths and it did in fact give 0mb/s results
in both cases. For the Internet path from lulea to CMU,
spruce suffers 24mb/s negative bias and produces a mea-
surement result less than 70mb/s, while the real value is
around 94mb/s. We also use pathload to measure the three
paths and observe that it produces pretty accurate results.
The way to reduce elastic-bias is to use long packet-
trains instead of packet-pairs. In the lulea→CMU exper-
iment, using packet-trains of 33-packet, spruce can almost
completely overcome the 24mb/s bias and produce an ac-
curate result. However, there are two problems of using
long packet-trains. First, there is not a deterministic train
length that guarantees negligible measurement bias on any
network path. Second, when router buffer space is lim-
ited and packet-train length are too large, the later probing
packets in each train may experience frequent loss, mak-
ing it impossible to accurately measure ˜F(Cb). After all,
spruce uses input rate Cb, which can be too high for the
bottleneck router to accommodate long packet-trains. On
the other hand, note that non-elastic bias is an inherit prob-
lem for spruce. There is no way to overcome it by adjusting
packet-train parameters.
6.3 PTR and pathload
PTR searches the ﬁrst turning point in the response curve
˜Z(rI , s, n) and takes the input rate at the turning point as
the path available bandwidth AP. This method can produce
accurate result when the real response curve ˜Z is close to
˜F, which requires packet-train length n to be sufﬁciently
large. Otherwise, PTR is also negatively biased and under-
estimates AP. The minimum packet-train length needed
is dependent on the path conditions. The current version
of PTR use packet train length n = 60, which is probably
insufﬁcient for the Internet path from pwh to CMU experi-
mented in this paper.
Pathload is in spirit similar to PTR. However, it searches
the available bandwidth region by detecting one-way-delay
increasing trend within a packet-train, which is different
from examining whether the rate response ˜Z(rI , s, n) is
greater than one [7]. However, since there is a strong sta-
tistical correlation between a high rate response ˜Z(rI , s, n)
and the one-way-delay increasing tend within packet-
trains, our analysis can explain the behavior of pathload to
a certain extent. Recall that, as reported in [6], pathload
underestimates available bandwidth when there are mul-
tiple tight links along the path. Our results demonstrate
that the deviation of ˜Z(rI , s, n) from ˜F in the input rate
range (0, AP ) gives rise to a potential underestimation in
pathload. The underestimation is maximized and becomes
clearly noticeable when non-bottleneck links have the same
available bandwidth as AP, given that the other factors are
kept the same.
Even through multiple tight links cause one-way-delay
increasing trend for packet-trains with input rate less than
AP, this is not an indication that the network can not sus-
tain such an input rate. Rather, the increasing trend is
a transient phenomenon resulting from probing intrusion
residual, and it disappears when the input packet-train is
sufﬁciently long. Hence, it is our new observation that by
further increasing the packet-train length, the underestima-
tion in pathload can be mitigated.
7 Related Work
Besides the measurement techniques we discussed earlier,
Melander et al. [13] ﬁrst discussed the rate response curve
of a multi-hop network path carrying ﬂuid cross-trafﬁc with
one-hop persistent routing pattern. Dovrolis et al. [3], [4]
considered the impact of cross-trafﬁc routing on the output
dispersion rate of a packet-train. It was also pointed out that
the output rate of a back-to-back input packet-train (input
rate rI = C1, the capacity of the ﬁrst hop L1) converges
to a point they call “asymptotic dispersion rate (ADR)” as
packet-train length increases. The authors provided an in-
formal justiﬁcation as to why ADR can be computed using
ﬂuid cross-trafﬁc. They demonstrated the computation of
ADR for several special path conditions. Note that using
the notations in this paper, ADR can be expressed as
lim
n→∞
s
GN (s/C1, s, n)
=
s
γN (s/C1, s)
.
(37)
Our work not only formally explains previous ﬁndings, but
also generalizes them to such an extent that allows any in-
put rate and any path conditions.
Kang et al. [8] analyzed the gap response of a single-
hop path with bursty cross-trafﬁc using packet-pairs. The
paper had a focus on large input probing rate. Liu et al.
extended the single-hop analysis for packet-pairs [11] and
packet-trains [9] to arbitrary input rates and discussed the
impact of packet-train parameters.
USENIX Association
Internet Measurement Conference 2005  
185
8 Conclusion
This paper provides a stochastic characterization of packet-
train bandwidth estimation in a multi-hop path with arbi-
trarily routed cross-trafﬁc ﬂows. Our main contributions
include derivation of the multi-hop ﬂuid response curve as
well as the real response curve and investigation of the con-
vergence properties of the real response curve with respect
to packet-train parameters. The insights provided in this
paper not only help understand and improve existing tech-
niques, but may also lead to a new technique that measures
tight link capacity.
There are a few unaddressed issues in our theoretical
framework. In our future work, we will identify how var-
ious factors, such as path conﬁguration and cross-trafﬁc
routing, affect the amount of deviation between Z and F.
We are also interested in investigating new approaches that
help detect and eliminate the measurement bias caused by
bursty cross-trafﬁc in multi-hop paths.
Acknowledgements
Dmitri Loguinov was supported by NSF grants CCR-
0306246, ANI-0312461, CNS-0434940.
References
[1] Emulab. http://www.emulab.net.
[2] National Laboratory for Applied Network Research.
http://www.nlanr.net.
[3] C. Dovrolis, P. Ramanathan, and D. Moore, “What Do
Packet Dispersion Techniques Measure?,” IEEE INFOCOM,
April 2001.
[4] C. Dovrolis, P. Ramanathan, and D. Moore, “Packet Disper-
sion Techniques and a Capacity Estimation Methodology,”
IEEE/ACM Transaction on Networking, March 2004.
[5] N. Hu and P. Steenkiste, “Evaluation and Characterization
of Available Bandwidth Probing Techniques,” IEEE JSAC
Special Issue in Internet and WWW Measurement, Mapping,
and Modeling, 3rd Quarter 2003.
[6] M. Jain and C. Dovrolis, “End-to-end available bandwidth:
measurement methodology, dynamics, and relation with
TCP throughput,” ACM SIGCOMM, August 2002.
[7] M. Jain and C. Dovrolis, “Ten Fallacies and Pitfalls in End-
to-End Available Bandwidth Estimation,” ACM IMC, Octo-
ber 2004.
[8] S. Kang, X. Liu, M. Dai, and D. Loguinov, “Packet-pair
Bandwidth Estimation: Stochastic Analysis of a Single Con-
gested Node,” IEEE ICNP, October 2004.
[9] X. Liu, K. Ravindran, B. Liu, and D. Loguinov, “Single-
Hop Probing Asymptotics in Available Bandwidth Estima-
tion: Sample-Path Analysis,” ACM IMC, October 2004.
[10] X. Liu, K. Ravindran, and D. Loguinov, “Multi-Hop
Probing Asymptotics in Available Bandwidth Estimation:
Stochastic Analysis,” Technical
report, CUNY, Avail-
able at http://www.cs.gc.cuny.edu/tr/TR-2005010.pdf, Au-
gust 2005.
[11] X. Liu, K. Ravindran, and D. Loguinov, “What Signals Do
Packet-pair Dispersions Carry?,” IEEE INFOCOM, March
2005.
[12] W. Matragi, K. Sohraby, and C. Bisdikian, “Jitter Calcu-
lus in ATM Networks: Multiple Nodes,” IEEE/ACM Tran-
sctions on Networking, 5(1):122–133, 1997.
[13] B. Melander, M. Bjorkman, and P. Gunningberg, “A New
End-to-End Probing and Analysis Method for Estimating
Bandwidth Bottlenecks,” IEEE Globecom Global Internet
Symposium, November 2000.
[14] B. Melander, M. Bjorkman,
and P. Gunningberg,
“Regression-Based Available Bandwidth Measurements,”
SPECTS, July 2002.
[15] Y. Ohba, M. Murata, and H. Miyahara, “Analysis of Inter-
departure Processes for Bursty Trafﬁc in ATM Networks,”
IEEE Journal on Selected Areas in Communications, 9,
1991.
[16] V. Ribeiro, R. Riedi, R. Baraniuk, J. Navratil, and L. Cot-
trell, “pathChirp: Efﬁcient Available Bandwidth Estimation
for Network Paths,” Passive and Active Measurement Work-
shop, 2003.
[17] J. Strauss, D. Katabi, and F. Kaashoek, “A measurement
study of available bandwidth estimation tools,” ACM IMC,
2003.
[18] W. Szczotka, “Stationary representation of queues. I.,” Ad-
vance in Applied Probability, 18:815–848, 1986.
[19] W. Szczotka, “Stationary representation of queues. II.,” Ad-
vance in Applied Probability, 18:849–859, 1986.
[20] R. Wolff. Stochastic modeling and the theory of queues.
Prentice hall, 1989.
Notes
1In general, the tight link can be different from the link with the mini-
mum capacity, which we refer to as the narrow link of P.
2We use the term “ﬂuid” and “constant-rate ﬂuid” interchangeably.
3The analysis assumes inﬁnite buffer space at each router.
4The term Ωi represents the volume of ﬂuid cross-trafﬁc buffered be-
tween the packet-pair in the outgoing queue of link Li. For an analogical
understanding, we can view the packet-pair as a bus, the cross-trafﬁc as
passengers, and the routers as bus stations. Then, Ωi is the amount of
cross-trafﬁc picked up by the packet-pair at link Li as well as all the up-
stream links of Li. This cross-trafﬁc will traverse over link Li due to the
ﬂows’ routing decision.
5Note that the turning points in F is indexed according to the decreas-
ing order of their values. The reason will be clear shortly when we discuss
the rate response curve.
6Note that the hop available bandwidth of link Li that is of measure-
ment interest, given by Ai = Ci − xri can be less than Ci − xp.
7Note that the output dispersion process can be correlated. However,
this does not affect the sample-path time average of the process.
8See section 3.2 in [9] for more discussions about this term in a single-
hop context, where Ri is referred to as intrusion residual.
9Refer to [20, pages 89] for the deﬁnition of regenerative processes.
186
Internet Measurement Conference 2005
USENIX Association