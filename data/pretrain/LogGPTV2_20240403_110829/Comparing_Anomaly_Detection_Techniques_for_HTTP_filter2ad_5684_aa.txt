title:Comparing Anomaly Detection Techniques for HTTP
author:Kenneth L. Ingham and
Hajime Inoue
Comparing Anomaly Detection Techniques for
HTTP
Kenneth L. Ingham1 and Hajime Inoue2
1 University of New Mexico, Computer Science Department, Albuquerque, NM,
87131, USA
2 Carleton University School of Computer Science Ottawa, ON, K1S 5B6, Canada
PI:EMAIL
PI:EMAIL
Abstract. Much data access occurs via HTTP, which is becoming a
universal transport protocol. Because of this, it has become a common
exploit target and several HTTP speciﬁc IDSs have been proposed as
a response. However, each IDS is developed and tested independently,
and direct comparisons are diﬃcult. We describe a framework for test-
ing IDS algorithms, and apply it to several proposed anomaly detection
algorithms, testing using identical data and test environment. The re-
sults show serious limitations in all approaches, and we make predictions
about requirements for successful anomaly detection approaches used to
protect web servers.
Keywords: Anomaly detection, Intrusion detection, Comparison, HTTP,
Hypertext transport protocol.
1 Introduction
The Hypertext Transfer Protocol (HTTP) [14] has become a universal transport
protocol. For example, it is used for ﬁle sharing [19], payment processing [12],
remote procedure calls [29], streaming media [1], and even protocols such as
SSH [40]. Custom web applications and the rush toward Web Services [3] mean
that in the future, we can expect heavier use of HTTP. Robertson et al. [32]
claimed that many web applications are written by people with little expertise
in security and that web-based vulnerabilities represent 25% of the total security
ﬂaws reported in the Common Vulnerabilities and Exposures list (CVE) [5] 1999
through 2005.
The importance of HTTP and the security problems have led many researchers
to propose intrusion detection systems (IDSs) for use with HTTP. Unfortunately,
the proposed IDSs suﬀer from one or more of the following problems:
– The proposed IDS is not fully described and the source code is not available.
– The test data is not available, preventing a direct comparison.
– The test data is not labeled, preventing replication.
– The test data is not representative of traﬃc seen today.
C. Kruegel, R. Lippmann, and A. Clark (Eds.): RAID 2007, LNCS 4637, pp. 42–62, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007
Comparing Anomaly Detection Techniques for HTTP
43
To address this problem, we describe a framework for comparing IDS algo-
rithms, and we use this framework to compare several anomaly IDS algorithms
under identical circumstances. This framework1 and the attack data2 are open
source to encourage further experimentation. Under more rigorous testing, not
all algorithms perform as well as the initial tests showed, and we discuss why
some algorithms do better than others.
Three basic architectures of IDSs exist: signature detection, speciﬁcation, and
anomaly detection. We focus in this paper on anomaly detection. Signature de-
tection systems cannot detect novel attacks, while speciﬁcation systems require
skills well beyond those commonly used when developing web applications. Ad-
ditionally, whenever the protected program changes, the speciﬁcation must be
updated. Although we test only anomaly IDSs, the framework can be applied to
signature and speciﬁcation based algorithms as well.
The organization of the paper is as follows. The following section, Section 2,
sets the stage by describing previous IDS testing, with a focus on systems de-
signed for HTTP. We then brieﬂy describe the test framework and test data in
Section 3. The speciﬁc algorithms we tested are described in Section 3.3, fol-
lowed by the test results in Section 4. Our discussion of the results follows in
Section 5, while Section 6 concludes the paper with a summary of our results
and a discussion of future work.
2 Prior Work
There are at least two reasons to testing IDSs: (1) to verify that an algorithm
is eﬀective and eﬃcient at detecting attacks, and (2) to compare two or more
algorithms to determine the better under various metrics.
Most IDS testing is little more than asking, “Does the IDS detect one or a few
attacks?” Better are researchers who ask the question, “Which of the following
attacks can the IDS detect?” Even this testing is often acknowledged as weak.
Good testing is repeatable; the data are available to other researchers facili-
tating direct comparisons of the results, the training data are representative of
real systems, and the attack data accurately represent the diversity of attacks.
A good test also compares two or more valid approaches (i.e., no straw man
arguments). The results of a good test should provide guidance about which
system or algorithm performs best under diﬀerent circumstances. To this point,
most IDSs for web servers have been weakly tested, and/or the tests are limited
in their scope. In their review of IDS testing, Athanasiades et al. state that they
do not believe this problem will ever be properly solved [2].
There are several explanations for the scarcity of good IDS testing. Identi-
fying appropriate data is diﬃcult—the data must be representative of realistic
operating conditions. Data collected live from a network might be subject to
1 The parser, framework and algorithm implementation code is available from the
Comprehensive Perl Archive Network (CPAN) at
http://cpan.org/modules/by-authors/id/I/IN/INGHAM/
2 The attack data is available at http://www.i-pi.com/HTTP-attacks-JoCN-2006/
44
K.L. Ingham and H. Inoue
privacy concerns. Synthetic data must be shown to represent real data on a tar-
get network accurately. In order to test an IDS, researchers need a collection of
intrusions and vulnerable machines on which to test the intrusions. Because a
library of intrusions represents a threat to vulnerable systems, researchers often
use disconnected networks for testing to ensure that the attack does not escape
into unprotected networks.
Setting up and maintaining a good, protected network is resource-intensive,
both in the costs of the hardware, as well as in system administration support
to set up and maintain a diversity of machines needed to ensure a good test
environment. Exploits are speciﬁc to operating system and version, as well to to
speciﬁc compilers, libraries, and other software. An intrusion is likely to fail if
any part of the execution environment is diﬀerent than expected. Because of this,
a machine, or virtual machine, may be required for each new intrusion added to
the attack corpus.
Finally, Debar noted that a set of criteria for evaluating an IDS does not exist
[11]. Even if such criteria were available, the most careful comparisons, such as
Warrender et al. [39], lack enough information to be repeatable.
2.1 Frameworks for Testing
A framework for testing is one way of reproducibility by providing a setup in
which diﬀerent IDSs can be tested under identical conditions. Three researchers
or research groups have established such frameworks:
– The ﬁrst published papers about an IDS testing framework and methodology
were from Puketza et al. [30,31] at UC Davis. Unless they failed to publish
further work, they built the framework and then tested only one IDS: NSM
[17,18].
– Wan and Yang [37] developed a framework for testing sensors that used
the Internet Engineering Task Force (IETF) Intrusion Detection Working
Group (IDWG) Intrusion Detection Message Exchange Format (IDMEF) [6].
Their framework might be useful, but the paper describes only a preliminary
version.
– IBM Zurich [11] set up a laboratory for testing IDSs. Their normal data came
not only from recordings of user sessions, but also from the IBM test suites
for the AIX operating system. While this test suite is not representative of
actual user interactions, it exercises normal functionality of the product.
2.2 Data Sets for Testing HTTP IDSs
Using a good data set is critical for the test. The training and test data must
be representative of the web server(s) to be protected, and the attacks used for
testing need to illustrate the diversity of attacks existing today. Given the diver-
sity between web sites, the ideal situation is to use data collected from the server
to be protected. These data often have privacy issues associated with them, pre-
venting other researchers from using it and thereby hindering repeatability. This
Comparing Anomaly Detection Techniques for HTTP
45
tension has resulted in some researchers using open, less-representative data,
while others use closed but more accurate data sets.
The DARPA/MIT Lincoln Laboratories IDS tests of 1998 and 1999 produced
the most prominent data sets [15,24]. Many researchers in IDS research used
these data because large data sets are scarce and the dataset provides an im-
mediate comparison with the original Lincoln Labs test. Open datasets allow
comparison of methods, but careful analysis of the relevant papers is required to
combine and compare the results. Furthermore, diﬀerences in testing method-
ologies make direct comparison diﬃcult.
However, this data set is not without its critics. McHugh [27,28] pointed out
that the DARPA/MIT Lincoln Laboratories IDS test used generated data, but
the MIT researchers never did any tests to show that the generated data was
representative of real data. Additionally, they did no tests to verify that their
attacks were representative of real attacks. The Lincoln Labs data set is also
quite dated, as web behavior has evolved signiﬁcantly over the years.
When testing IDSs for HTTP, researchers using the Lincoln Labs data sets
have only four web attacks. When systems developed using these data are tested
on a broader data set, their performance suﬀers; conﬁrmation of this assertion
appears in this paper. In spite of these limitations, Wang and Stolfo [38], Ma-
honey [25], and Mahoney and Chan [26] Vargiya and Chan [36] used one or
both of these data sets for testing their IDSs, at least a portion of which were
for protecting web servers. Est´evez-Tapiador et al. [13] used these data as nor-
mal behavior, but they developed their own attack database to supplement the
attacks in the Lincoln Labs data.
Recognizing the shortcomings of the Lincoln Labs data, other researchers have
used test data that is more representative for the servers the IDS is protecting.
However, these data are unavailable for others to use, eliminating direct com-
parisons. For example, Kruegel et al. [22,23] tested their system using extensive
normal data sets from multiple sites (including Google).3 For a portion of their
12 attacks, they used attacks against software that ran on one of their data
source web servers. Wang and Stolfo [38] used data collected from their depart-
mental web server as an additional source of data, but they did not ﬁlter attacks
from the data and therefore used it only for testing the training. Tombini et
al. [35] collected data from two production web servers, one academic, and one
industrial, with a combined total of over ﬁve million HTTP requests from web
server log ﬁles. Est´evez-Tapiador et al. [13] used 1500 attack requests repre-
senting variants of 85 distinct attacks, the largest attack database reported to
date.
Another important HTTP data issue is how much of the HTTP request the
IDS used. While most attacks to date have been in the requested resource path,
some attacks target other regions of the request. For example, Apache Sioux [8]
exhausts Apache’s memory by a repeated header line. Wang and Stolfo [38], in
diﬀerent experiments, modeled the packet payload, the ﬁrst and last 100 bytes,
3 The Google data was not even available to the researchers; they sent their programs
to Google, who returned the results.
46
K.L. Ingham and H. Inoue
and also the ﬁrst 1000 bytes of the connection. Kruegel and Vigna and Kruegel
et al. [22,23] obtained their test data from web server log ﬁles, and only looked
at CGI programs. Web server log ﬁles are a popular data source; Tombini et al.
[35] and Robertson et al. [32] also used them. Unfortunately, log ﬁles contain
only a small portion of most HTTP requests, and attacks not in the resource
path are unlikely to appear in the log ﬁles.
3 Experimental Setup
To perform rigorous tests of HTTP IDS algorithms, the test circumstances and
data must be identical. Testing requires data representative of what production
web servers receive. Quality test data is diﬃcult to obtain; organizations with the
most interesting data typically consider it conﬁdential. Therefore, we collected
data for testing from four web sites. The attack data needs to be representative
of the broad range of attacks existing today. Since, as we noted in Section 2.2, no
public database of attacks exists, we compiled our own. Due to space limitations,
full details of the experimental setup are described by Ingham [20].
3.1 Data
The normal data set is a collection of HTTP requests received by to the Univer-
sity of New Mexico Computer Science departmental web server (cs.unm.edu),
as well as aya.org, explorenm.com, and i-pi.com. The training data was from
one week, and the normal test data is from the following week. All attacks were
ﬁltered from the data using a combination of snort and manual inspection. All
the data sets contain the entire HTTP request.4 These include information not
usually found in the log ﬁles. Having the HTTP header lines allows testing for
attacks not contained in the requested resource path.
The attack database contains 63 attacks, some of which are variants of the
same vulnerability—either a diﬀerent exploit for the same vulnerability or the
same exploit against a diﬀerent operating system. We include the variants be-
cause some IDS algorithms will ﬁnd some variants easier to detect than others.
As one example, some of the Nimda variants are short, allowing detection by
the length algorithm, while others are average length.
The attacks were collected from the following sources: Attacks against web
servers under test (attacks in the wild); BugTraq and the SecurityFocus archives
http://www.SecurityFocus.com/;theOpenSourceVulnerability Databasehttp:
//www.osvdb.org/;thePacketstormarchives http://Packetstorm.widexs.nl/;
and Sourcebank http://archive.devx.com/sourcebank/. In many cases, the
attack programs from these sources contained bugs, and we had to modify the
program before it would produce malicious web requests. Note that we did not
test to verify whether the attacks produced could actually compromise the tar-
geted web application.
4 These data were captured using a snort ﬁlter which reconstructs the application
layer portion.
Comparing Anomaly Detection Techniques for HTTP
47
The attack database contains the following categories of attacks: buﬀer over-
ﬂow; input validation error (other than buﬀer overﬂow); signed interpretation
of unsigned value; and URL decoding error. The attacks targeted diﬀerent web
servers: Active Perl ISAPI; AltaVista Search Engine; AnalogX SimpleServer;
Apache with and without mod php; CERN 3.0A; FrontPage Personal Web
Server; Hughes Technologies Mini SQL; InetServ 3.0; Microsoft IIS; NCSA;
Netscape FastTrack 2.01a; Nortel Contivity Extranet Switches; OmniHTTPd;
and PlusMail. The target operating systems for the attacks include the follow-
ing: AIX; Linux (many varieties); Mac OS X; Microsoft Windows; OpenBSD;
SCO UnixWare; Solaris x86; Unix; VxWorks; and any x86 BSD variant.
3.2 The Algorithm Test Framework
A framework allows testing a collection of algorithms in the same environment,
ensuring that each algorithm is working under identical conditions. By provid-
ing a common interface, testing any IDS algorithm that uses this interface is
straightforward, and the surrounding support code is reused. The framework for
running the tests was designed to work with anomaly detection algorithms, but
it is general enough to work with signature and speciﬁcation systems—these sys-
tems simply need no training before testing. As an example, it was easy to write
an IDS algorithm object to use snort signatures for HTTP requests. Detailed
descriptions of the test framework are available in [20].
Some algorithms require that the data be tokenized. For these algorithms,
we implemented a parser that breaks the HTTP request into tokens based on
the those speciﬁed in the HTTP standard, RFC 2616 [14]. The tokens are a
combination of the token type (e.g., method) and optionally the value (e.g., GET).
In practice, most of the values are necessary to properly distinguish attacks from
normal requests. The result is a stream of tokens combined with the associated
values.
Instead of using tokens, some algorithms use a string representation for the
request. This (much simpler) representation is also available from the parser.
3.3 Algorithms
We consider algorithms from Kruegel and Vigna [22], who developed a linear
combination of six measures (length, a character distribution measure, a Markov
Model, presence/absence of parameters, order of parameters, and whether para-
meter values were enumerated or random), and applied them to CGI parameters.
For some of the six algorithms, we also consider them in isolation. We also im-
plemented the character distribution metric described by Wang and Stolfo [38],
and the DFA induction and n-grams described by Ingham et al. and Ingham
[21,20].
These algorithms are either proposed by often cited papers in the IDS com-
munity, similar to those algorithms but using diﬀerent data or representations,
or successful in related domains. In short, we tested algorithms claimed to be or
likely to be successful in HTTP-based anomaly intrusion detection.
48
K.L. Ingham and H. Inoue
Request Length. Observing that buﬀer overﬂows and cross-site scripting at-
tacks tend to be longer than normal CGI attribute values, one measure used by
Kruegel and Vigna [22] was the mean μ and variance σ2 of attribute lengths.
These values were calculated from training data.
For testing, the system calculated the probability p that an attribute would
have the observed length l by:
p = σ2
(l − μ)2
Character Distributions. Buﬀer-overﬂow attacks often have a distinctive
character distribution. Two research groups have compared the character distri-
bution of test instances to the distribution in the training data. Wang and Stolfo
[38] used a character distribution metric on similarly-sized packets. Kruegel and
Vigna [22] used a character distribution as one of six tests.
Mahalanobis distance. Wang and Stolfo [38] measured the Mahalanobis distance,
d, between two distributions. For eﬃciency reasons they used a measure they