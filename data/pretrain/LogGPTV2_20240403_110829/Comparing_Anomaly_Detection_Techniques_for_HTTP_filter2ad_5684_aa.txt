# Title: Comparing Anomaly Detection Techniques for HTTP

## Authors:
- Kenneth L. Ingham<sup>1</sup>
- Hajime Inoue<sup>2</sup>

### Affiliations:
<sup>1</sup> University of New Mexico, Computer Science Department, Albuquerque, NM 87131, USA  
<sup>2</sup> Carleton University School of Computer Science, Ottawa, ON, K1S 5B6, Canada

### Contact Information:
- PI: EMAIL
- PI: EMAIL

## Abstract
Much data access occurs via the Hypertext Transfer Protocol (HTTP), which has become a universal transport protocol. As a result, it has become a common target for exploits, and several HTTP-specific Intrusion Detection Systems (IDSs) have been proposed. However, each IDS is developed and tested independently, making direct comparisons difficult. We describe a framework for testing IDS algorithms and apply it to several proposed anomaly detection algorithms, using identical data and test environments. The results reveal significant limitations in all approaches, and we provide predictions about the requirements for successful anomaly detection methods used to protect web servers.

### Keywords
Anomaly detection, Intrusion detection, Comparison, HTTP, Hypertext Transfer Protocol

## 1. Introduction
The Hypertext Transfer Protocol (HTTP) [14] has become a universal transport protocol, used for various purposes such as file sharing [19], payment processing [12], remote procedure calls [29], streaming media [1], and even protocols like SSH [40]. The increasing use of custom web applications and the trend toward Web Services [3] indicate that HTTP usage will continue to grow. Robertson et al. [32] noted that many web applications are developed by individuals with limited security expertise, leading to web-based vulnerabilities accounting for 25% of the total security flaws reported in the Common Vulnerabilities and Exposures (CVE) list [5] from 1999 to 2005.

The importance of HTTP and associated security issues have led many researchers to propose intrusion detection systems (IDSs) for HTTP. Unfortunately, these proposed IDSs often suffer from one or more of the following problems:
- Incomplete descriptions and unavailable source code.
- Unavailable test data, preventing direct comparison.
- Unlabeled test data, hindering replication.
- Test data that does not reflect current traffic patterns.

To address these issues, we present a framework for comparing IDS algorithms and use this framework to evaluate several anomaly detection algorithms under identical conditions. The framework and attack data are open-source to encourage further experimentation. Our rigorous testing reveals that some algorithms do not perform as well as initially reported, and we discuss the reasons for these discrepancies.

Three basic architectures of IDSs exist: signature detection, specification, and anomaly detection. This paper focuses on anomaly detection. Signature detection systems cannot detect novel attacks, while specification systems require skills beyond those typically used in web application development. Additionally, specification systems must be updated whenever the protected program changes. Although we test only anomaly IDSs, the framework can be applied to signature and specification-based algorithms as well.

The organization of the paper is as follows:
- Section 2 provides an overview of previous IDS testing, with a focus on systems designed for HTTP.
- Section 3 describes the test framework and data.
- Section 3.3 details the specific algorithms tested.
- Section 4 presents the test results.
- Section 5 discusses the results.
- Section 6 concludes the paper with a summary of our findings and suggestions for future work.

## 2. Prior Work
There are two primary reasons for testing IDSs: (1) to verify that an algorithm is effective and efficient at detecting attacks, and (2) to compare multiple algorithms to determine the best approach under various metrics.

Most IDS testing is minimal, often limited to asking whether the IDS can detect one or a few attacks. Better testing involves determining which of several attacks the IDS can detect, but even this is often acknowledged as weak. Good testing is repeatable, with available data for other researchers, representative training data, and accurate attack data. A good test should also compare valid approaches and provide guidance on which system or algorithm performs best under different circumstances. To date, most IDSs for web servers have been weakly tested, and the tests are often limited in scope. Athanasiades et al. [2] state that they do not believe this problem will ever be fully resolved.

Several factors contribute to the scarcity of good IDS testing:
- Identifying appropriate data is challenging; the data must be representative of realistic operating conditions.
- Live network data may raise privacy concerns, and synthetic data must accurately represent real data.
- Researchers need a collection of intrusions and vulnerable machines for testing, often requiring disconnected networks to prevent attacks from spreading.
- Setting up and maintaining a protected network is resource-intensive, both in terms of hardware and system administration.
- Exploits are specific to operating systems, versions, compilers, libraries, and other software, necessitating a diverse set of test environments.
- Debar [11] noted that there are no established criteria for evaluating IDSs, and even careful comparisons, such as Warrender et al. [39], lack sufficient information for reproducibility.

### 2.1 Frameworks for Testing
A testing framework can enhance reproducibility by providing a consistent setup for evaluating different IDSs. Three notable frameworks include:
- Puketza et al. [30, 31] at UC Davis, who published early papers on an IDS testing framework and methodology. They tested only one IDS, NSM [17, 18].
- Wan and Yang [37] developed a framework for testing sensors using the IETF IDWG IDMEF [6]. Their framework is preliminary.
- IBM Zurich [11] set up a laboratory for testing IDSs, using normal data from user sessions and IBM test suites for the AIX operating system.

### 2.2 Data Sets for Testing HTTP IDSs
Using a good data set is crucial for testing. Training and test data must be representative of the web server(s) to be protected, and the attacks used for testing should illustrate the diversity of current threats. Ideally, data should be collected from the server to be protected, but privacy issues often prevent this. Some researchers use open, less-representative data, while others use closed, more accurate data sets.

The DARPA/MIT Lincoln Laboratories IDS tests of 1998 and 1999 produced the most prominent data sets [15, 24]. Many researchers used these data because large data sets are scarce, and they provide a basis for comparison with the original Lincoln Labs test. However, McHugh [27, 28] criticized the generated data for not being representative of real data, and the attacks were not verified to be representative of real attacks. The Lincoln Labs data set is also outdated, given the evolution of web behavior over the years.

When testing IDSs for HTTP, the Lincoln Labs data sets include only four web attacks. Systems developed using these data often perform poorly when tested on broader data sets. Despite these limitations, Wang and Stolfo [38], Mahoney [25], Mahoney and Chan [26], Vargiya and Chan [36], and Estévez-Tapiador et al. [13] used these data sets, supplementing them with their own attack databases.

Other researchers have used more representative test data, but these data are often unavailable for direct comparisons. For example, Kruegel et al. [22, 23] used extensive normal data sets from multiple sites, including Google, and Tombini et al. [35] collected data from two production web servers, one academic and one industrial, with over five million HTTP requests. Estévez-Tapiador et al. [13] used 1500 attack requests representing variants of 85 distinct attacks, the largest attack database reported to date.

Another important issue is the portion of the HTTP request used by the IDS. While most attacks target the requested resource path, some attacks target other regions. For example, Apache Sioux [8] exhausts Apache's memory by repeated header lines. Wang and Stolfo [38] modeled the packet payload, the first and last 100 bytes, and the first 1000 bytes of the connection. Kruegel and Vigna [22, 23] used web server log files, focusing on CGI programs. Log files, however, contain only a small portion of most HTTP requests, and attacks not in the resource path are unlikely to appear in the logs.

## 3. Experimental Setup
To perform rigorous tests of HTTP IDS algorithms, the test circumstances and data must be identical. Testing requires data representative of what production web servers receive. Quality test data is difficult to obtain, as organizations with the most interesting data typically consider it confidential. Therefore, we collected data from four web sites and compiled our own attack database. Due to space limitations, full details of the experimental setup are described in Ingham [20].

### 3.1 Data
The normal data set consists of HTTP requests received by the University of New Mexico Computer Science departmental web server (cs.unm.edu), aya.org, explorenm.com, and i-pi.com. The training data was from one week, and the normal test data is from the following week. All attacks were filtered from the data using a combination of snort and manual inspection. The data sets contain the entire HTTP request, including information not usually found in log files. Having the HTTP header lines allows testing for attacks not contained in the requested resource path.

The attack database contains 63 attacks, including variants of the same vulnerability. We include variants because some IDS algorithms may find some variants easier to detect than others. The attacks were collected from various sources, including BugTraq, SecurityFocus, OpenSourceVulnerability Database, Packetstorm, and Sourcebank. In many cases, the attack programs required modification to produce malicious web requests. Note that we did not verify whether the attacks could actually compromise the targeted web application.

The attack database includes the following categories of attacks: buffer overflow, input validation error (other than buffer overflow), signed interpretation of unsigned value, and URL decoding error. The attacks targeted different web servers and operating systems.

### 3.2 The Algorithm Test Framework
A framework allows testing a collection of algorithms in the same environment, ensuring that each algorithm works under identical conditions. By providing a common interface, testing any IDS algorithm that uses this interface is straightforward, and the surrounding support code is reused. The framework for running the tests was designed for anomaly detection algorithms but is general enough to work with signature and specification systems, which do not require training before testing. Detailed descriptions of the test framework are available in [20].

Some algorithms require tokenized data. For these algorithms, we implemented a parser that breaks the HTTP request into tokens based on the HTTP standard, RFC 2616 [14]. The tokens are a combination of the token type (e.g., method) and optionally the value (e.g., GET). In practice, most values are necessary to properly distinguish attacks from normal requests. The result is a stream of tokens combined with the associated values.

For algorithms that use a string representation of the request, a simpler representation is also available from the parser.

### 3.3 Algorithms
We consider algorithms from Kruegel and Vigna [22], who developed a linear combination of six measures (length, character distribution, Markov Model, presence/absence of parameters, order of parameters, and whether parameter values were enumerated or random) and applied them to CGI parameters. We also implemented the character distribution metric described by Wang and Stolfo [38] and the DFA induction and n-grams described by Ingham et al. and Ingham [21, 20].

These algorithms are either proposed by frequently cited papers in the IDS community, similar to those algorithms but using different data or representations, or successful in related domains. In short, we tested algorithms claimed to be or likely to be successful in HTTP-based anomaly intrusion detection.

#### Request Length
Kruegel and Vigna [22] observed that buffer overflows and cross-site scripting attacks tend to be longer than normal CGI attribute values. They used the mean \(\mu\) and variance \(\sigma^2\) of attribute lengths calculated from training data. For testing, the system calculated the probability \(p\) that an attribute would have the observed length \(l\) by:

\[ p = \frac{\sigma^2}{(l - \mu)^2} \]

#### Character Distributions
Buffer-overflow attacks often have a distinctive character distribution. Two research groups compared the character distribution of test instances to the distribution in the training data. Wang and Stolfo [38] used a character distribution metric on similarly-sized packets, while Kruegel and Vigna [22] used a character distribution as one of six tests.

#### Mahalanobis Distance
Wang and Stolfo [38] measured the Mahalanobis distance \(d\) between two distributions. For efficiency, they used a simplified measure.