for auction. In Figure 19, we plot the CDF of the number of ad-slots
across the websites, per HB type. In general, and for up to 70% of
websites, the Hybrid HB type auctions more ad-slots than the other
two types. For the other 30% of websites, Server-Side HB auctions
PiximediaOneTagJustpremiumStickyAdsTVWidespacePolymorphYieldlabGjirafaAtomxYieldbotDFPAppNexusRubiconCriteoIndexAmazonOpenxPubmaticAOLSovrnSmartTrionAdOceanFidelityC1XYieldoneAardvarkInnityBridgewellGamma SSPAdgeneration050010001500Latency (ms)13579111315Number of Demand Partners per website0246810Latency (seconds)01020304050% of websites1020304050607080Demand Partner Popularity Rank (bins of 10)2004006008001000Latency (ms)Figure 17: Portion of late bids over the total
bids received, due to high latency of a part-
ner to respond. For 10% of auctions, they
have 80% or more late bids.
Figure 18: Percentage of late bids out of all bids
sent per Demand Partner. Some partners have all
their bids arriving too late to be considered for
auction.
Figure 19: Auctioned ad-slots across web-
sites, per HB facet. The median website has
2-6 available ad-slots. 90% of websites have
5-11 ad-slots (depending on the HB type).
Figure 20: HB latency as a function of the number of ad-slots
auctioned. More ad-slots result in higher median latency and
variability in latency for the HB process.
more ad-slots. The median website has 2-6 available ad-slots, and
90% of websites have up to 5-11 ad-slots (depending on the HB type).
Also, 3% of the websites provide more than 20 slots for auction.
Requesting bids for 20 ad-slots on a single page can be considered
odd, even a flag for suspicious or fraudulent behavior. Therefore,
we manually investigated such cases, and to our surprise, we found
that some publishers request auctions for more slots than they have
available for display. After investigating this behavior further, we
observed that these ad-slots refer to several different devices and
screen sizes such as for tablet, smartphone, laptop, etc. We speculate
they do that due to either bad configuration of their wrapper (i.e.,
they use the same HB wrapper for all the devices they serve without
customizing the requests), or because they want to receive bids for
multiple versions of the same ad-slots, for better optimization of
the publisher’s HB process later on. Indeed, this odd activity needs
to be studied in depth in the future, to understand if it is a matter
of bad practice or an effort for ad-fraud.
Does the number of auctioned ad-slots impact latency?
Next, we checked if the HB latency is associated with the number
of ad-slots auctioned. Intuitively, we may expect that the more
slots are to be auctioned, the more time the HB will take. However,
given that a lot of Demand Partners invest significant computing
Figure 21: Portion of ads for different HB ad sizes, per HB facet. The
side banner (300x250) and top banner (728x90) are among the most
popular ad-slot sizes in all HB facets.
resources to parallelize and optimize bidding computations, the
above statement may not hold. In Figure 20, we plot the latency of
HB based on the number ad-slots auctioned in the website. In the
majority of cases, this latency includes the communication to the ad
server. In Client-Side HB, we cannot know the ad server (since each
publisher uses their own), so we have no means to infer this latency.
We observe that the total latency tends to increase with the number
of slots auctioned. In fact, when there are 1-3 ad-slots auctioned,
the median latency is 0.30-0.57 seconds, but when the slots are 3-5,
the median latency ranges to 0.57-0.92 seconds. Interestingly, we
observe that even if there is only one ad-slot to be auctioned, the
latency can still vary per auction, from a few tens of milliseconds
to almost two seconds. This variability can be due to extra latencies
as result of internal auctions occurring at each Demand Partner.
What are the most popular ad-slots auctioned?
Finally, we analyze the most popular dimensions of HB ad slots.
Our findings are presented in Figure 21, per facet of HB. The most
common ad size is the 300x250 (side banner), for all 3 facets. The sec-
ond most common is the 728x90 (top banner) and the 300x600 (for
the Client-Side HB). These are generally popular banners in both
mobile and desktop advertising, and they match results observed
in the past for RTB [41]. Due to the increase of mobile browsing,
289
20406080% of bids late from total bids per auction0.00.20.40.60.81.0ECDFAtomxLifestreetAdMaticPiximediaConsumableSpotxFreeWheelLKQDTremorInSkinAdKernelAdnQuantumSmartyAdsClickonometricsJustpremiumAdgenerationKummaYieldlabBridgewellE-PlanningInnityAdOceanC1XImproveDigitalYieldone020406080100% of the late bids05101520Auctioned ad-slots per website0.00.20.40.60.81.0ECDFClient-Side HBServer-Side HBHybrid HB123456789101112131415Auctioned ad-slots per website02468Latency (seconds)300x250728x90300x600320x50970x250160x600336x280970x90320x100468x60300x250300x600728x90970x250320x320320x50160x600100x200120x600320x100300x250728x90300x600320x50970x250160x600320x100336x280300x50120x600ad-slot dimensions01020304050% of ad-slotsServer-Side HBClient-Side HBHybrid HBFigure 22: CDF of the auctioned ad slots bid prices, per HB facet.
These are baseline prices that Demand Partners are willing to spend
when they have no information for the user.
Figure 23: Distribution of bid prices in CPM per ad-slot size (x-axis
sorted by area of ad-slot). Even in our crawler’s baseline scenario,
partners bid high prices to reach users.
publishers can choose these specific sizes to keep the HB configu-
ration simple and well defined for multiple devices (as they don’t
need to set multiple sizes for different devices, and fewer auctions
need to occur on the Demand Partners’ side).
5.4 Ad-slot Bid Prices
In this section, we discuss the auctioned ad-slots’ bid prices and
how they vary depending on their size. We were able to detect the
ad prices using HBDetector. In case of Hybrid and Client-Side HB,
most of the prices are transparent to the client and easy to extract
from the bid response messages. In contrast, in Server-Side HB the
prices are not trivial to detect. We analyze in depth the auction
metadata, and based on several heuristics we find and extract the
prices whenever they are included.
What are the HB partners willing to pay?
First, we analyze the prices bided by the Demand Partners during
the auctions. In Figure 22, we show the CDF of the baseline bid
prices (in CPM or cost per thousand ad impressions, in USD) that
advertisers are willing to spend for the ad-slots auctioned, per type
of HB. In general, we note that Client-Side HB draws higher bid
prices for the publisher, in comparison to the other two types. Also,
more than 20% of the prices are more than 0.5 CPM, which is lower
but comparable to regular waterfall prices, as claimed in past studies
(found to be ∼1 CPM [41]). Also, we should note that these prices
are baseline, so they are much lower than if they were referring to
targeted users.
290
Figure 24: Distribution of prices that partners bid, ranked by popu-
larity of Demand Partner (who are grouped in bins of 10).
What are the HB partners paying per ad-slot?
Second, we compare ad-slot sizes with bid prices for each size. In
Figure 23 we plot the prices (in CPM) for each ad-slot. We see that
in the recorded dimensions, the median cost ranges from 0.00084-
0.096 CPM. The most expensive ad-slot (based on median price)
is 120x600 with 0.096 CPM. The cheapest ad-slot is 300x50 (which
also happens to have the least ad-area) with 0.00084 CPM. Also,
the most popular ad-slot size, which is 300x250, has a median cost
of 0.031 CPM. Previous studies on waterfall standard [41] find the
prices of 300x250 slot ranging from 0.1 to 1.4 CPM, with a median
of 0.19 CPM. These prices are higher than the ones found in our
HB study, but we should again consider that our detected prices are
for baseline users that Demand Partners have no prior knowledge,
whereas in [41] it was for real users. Therefore, a follow-up work
could apply real user profiles to collect HB prices, and thus, make a
more fair comparison with RTB prices.
What is the variability of bid prices per DSP?
In Figure 24, we plot the prices (in CPM) that each Demand Partner
bid to examine possible association between a partner’s popularity
and how high they bid in HB. The DSPs are ranked by popularity
and grouped in buckets of 10 to ease illustration. The most popular
partners (first bins) tend to be more consistent and bid lower prices.
In contrast, less popular DSPs have higher median bid prices and
variability in their bids. This observation could be explained when
considering how the HB market works: for less popular DSPs to be
competitive and win auctions, they bid higher prices than popular
partners to reach sufficient number of users. Alternatively, this re-
sult could also indicate that more popular partners have technology
that detects when browsing is of a baseline (or bot/unknown) user
and therefore do not bid high, whereas the less popular partners
bid high, hoping to target a real user. Finally, it can also be a side
effect of how Demand Partners decide to spend their budget across
the websites they collaborate with: more popular partners exist in
more websites, and may chose to bid low in many of them, to cover
a wider range of websites.
6 RELATED WORK
User data and their economics have long been an interesting topic
and attracted a considerable body of research [2, 13, 21, 22, 35, 37,
41, 46, 47, 51, 53, 56]. In particular, Acquisti et al. discuss the value
of privacy after defining two concepts (i) Willingness To Pay: the
monetary amount users are willing to pay to protect their privacy,
0.00.51.01.52.02.53.0Bid price (CPM)0.00.20.40.60.81.0ECDFClient-Side HBServer-Side HBHybrid HB970x250300x600160x600336x280970x90300x250120x600728x90300x100320x50300x50Ad-slot dimensions, sorted by area0.00.20.40.6Bid price (cpm)123456Demand Partner Popularity Rank (bins of 10)0.00.51.01.52.02.53.03.54.0Bid price (CPM)and (ii) Willingness To Accept: the compensation that users are
willing to accept for their privacy loss [2]. In two user-studies [13,
51] authors measure how much users value their own offline and
online personal data, and consequently how much they would sell
them to advertisers. In [47], authors propose “transactional” privacy
to allow users to decide what personal information can be released
and receive compensation from selling them.
Papadopoulos et al. set out to explore the cost advertisers pay
to deliver an ad to the user in the waterfall standard and RTB
auctions [41]. In addition, they study how the personal data that
users leak while browsing (like location and interests) can affect
the pricing dynamics. The authors propose a methodology to com-
pute the total cost paid for the user even when advertisers hide
the charged prices. Finally, they evaluate their methodology by
using data from a large number of volunteering users. Olejnik et
al. perform an analysis of cookie matching in association with the
RTB advertising [35] . They leverage the RTB notification URL to
observe the charge prices and they conduct a basic study to provide
some insights into these prices, by analyzing different user profiles
and visiting contexts. Their results confirm that when the users’
browsing histories are leaked, the charge prices tend to be increased.
In [39], the authors measure the costs of digital advertising on both
the user’s and the advertiser’s side in an attempt to compare how
fairly these costs are distributed between the two. In particular,
they compare the cost advertisers pay in the waterfall standard
with the costs imposed on the data plan, the battery efficiency and
(by using cookie synchronization [1, 38, 40] as a metric) the privacy
of the specific user.
In [31], the authors briefly describe HB and focus on optimizing
its bidding strategy and the produced yield. They consider revenue
optimization as a contextual bandit problem, where the context
consists of the information available about the ad opportunity, such
as properties of the internet user or of the provided ad slot. In [21],
authors use a dataset of users’ HTTP traces and provide rough
estimates of the relative value of users by leveraging the suggested
bid amounts for the visited websites, based on categories provided
by the Google AdWords. FDTV [22] is a plugin to inform users in
real-time about the economic value of the personal information
associated to their Facebook activity. In [30], Iordanou et al. try to
detect both programmatic and static advertisements in a webpage,
using (i) a crowdsourcing, and (ii) a crawling approach to determine
the criteria with which ads are displayed. They find biases on ads
depending on age, income and gender of users.
Bashir et al. study the diffusion of user tracking caused by RTB-
based programmatic ad-auctions [6]. Results of their study show
that under specific assumptions, no less than 52 tracking companies
can observe at least 91% of an average user’s browsing history. In
an attempt to shed light upon Facebook’s ad ecosystem, Andreou
et al. investigate the level of transparency provided by the mecha-
nisms “Why am I seeing this?” and Ad Preferences Page [4]. The
authors built a browser extension to collect Facebook ads and infor-
mation extracted from these two mechanisms before performing
their own ad campaigns and target users that used their browser
extension. They show that ad explanations are often incomplete
and misleading. In [5], the authors aim to enhance the transparency
in ad ecosystem with regards to information sharing, by developing
a content agnostic methodology to detect client- and server- side
flows of information between ad exchanges and leveraging retar-
geted ads. By using crawled data, the authors collected 35.4K ad
impressions and identified 4 different kinds of information sharing
behavior between ad exchanges.
7 SUMMARY & DISCUSSION
Header Bidding is gaining popularity among Web publishers, who
want to regain the control of their ad inventory and what advertis-
ers are paying for it. Proponents of HB have touted that this new
ad-tech protocol increases transparency and fairness among adver-
tisers, since more partners can directly compete for an ad-slot. HB,
in theory, can boost the revenue of publishers, who can select the
Demand Partners that are competing for the publishers’ ad-slots,
and also remove intermediaries from the ad-selling process.
In this study, we investigate and present in full detail the different
implementations of HB and how each of them works. Based on these
observations, we design and implement HBDetector: a first of its
kind tool to measure in a systematic fashion the evolving ecosystem
of HB, its performance and its properties. By running HBDetector
across a list of top 35,000 Alexa websites, we collected data about
800k HB auctions and performed the first in-depth analysis of HB.
We discuss our lessons from this study in the next paragraphs.
7.1 Commoditization of Ad Supply
Header Bidding was introduced to put Demand Partners under
pressure for more competitive pricing (and loosen Google’s grip on
the market). Indeed, it has changed the hierarchy on the supply side.
Depending on the publisher’s needs, we found that Header Bidding
can be implemented in 3 ways: (i) Client-Side HB, (ii) Server-Side
HB, and (iii) Hybrid HB. Therefore, Demand Partners that could
previously claim exclusive access to a publisher’s inventory (and
thus higher positions in the waterfall) are no longer able to do so.
Instead, HB enabled all Demand Partners regardless of their size
or relationship with publishers, to compete for the same inventory,
thus commoditizing supply [16].
However, as measured in this study, big companies such as Dou-
bleClick, AppNexus, Rubicon, Criteo, etc., took advantage of their
existing dominance in the ad-market and placed themselves again
in a very centralizing (and process controlling) position within the
HB ecosystem (especially within the Server-Side HB and Hybrid
HB models). In fact, we identified that Server-Side HB dominates
this market with 48% of auctions handled by a single partner/ad
server. Google, in particular, handles as much as 80% of HB auctions.
DoubleClick for Publishers (DFP) dominates as a single partner,
while it also appears in 51% of the competing groups of Demand
Partners in HB. Also, most publishers use only one Demand Partner,
but some use many (more than 10). Interestingly, this centraliza-
tion is in direct contrast to the publishers’ revenue. We found that
websites utilizing Client-Side HB achieve higher bid prices than the
other two models.
7.2 Non-Viable Performance Overheads
The fear of latency has kept some premium publishers away from
header integrations and continues to make others wary about em-
bracing HB. Results of this study verify the concerns of publish-
ers [14, 18] regarding the latencies imposed on the user side. We
291
measured up to 0.6 seconds for the median website and more than
3 seconds in 10% of websites. Furthermore, publishers with more
than one Demand Partner experienced higher HB latencies: one
Demand Partner imposes a small latency of 0.3 seconds, but 2 De-
mand Partners impose 1.1 seconds latency, and 3 Demand Partners
can impose up to 3 seconds latency. It is of no doubt that for the
publishers that do the utmost to provide readers with a high-quality
experience, such latency is capable of significantly degrading the
user experience. Interestingly, we find that the top 500 (in Alexa
ranking) websites exhibited significantly lower HB latency than
the rest of websites.
Although Header Bidding tech promises multiple, in-parallel bid
requests to Demand Partners that can provide the best possible ad
price to the publisher, Javascript on the users’ end is single-threaded.
This means that even if the HB provider’s wrapper performs well-
optimized asynchronous ad calls, these still need to stand in the
network queue, thus increasing not only the overall HB execution
time but also the entire webpage’s loading time. These delays can
have adverse effects on user’s browsing experience while loading
a HB-enabled webpage. Interestingly, we find that the 10 most
popular Demand Partners exhibit lower variability in latency than