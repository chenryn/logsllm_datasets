super（IntWritable.class）；
}
}
一般情况下，ArrayWritable和TwoDArrayWritable都有set（）和get（）函数，在将Text转化为String时，它们也都提供了一个转化函数toArray（）。但是它们没有提供比较器comparator，这点需要注意。同时从TwoDArrayWritable的write和readFields可以看出是横向读写的，同时还会读写每一维的数据长度。
public void readFields（DataInput in）throws IOException{
for（int i=0；i＜values.length；i++）{
for（int j=0；j＜values[i].length；j++）{
……
value.readFields（in）；
values[i][j]=value；//保存读取的数据
}
}
}
public void write（DataOutput out）throws IOException{
for（int i=0；i＜values.length；i++）{
out.writeInt（values[i].length）；
}
for（int i=0；i＜values.length；i++）{
for（int j=0；j＜values[i].length；j++）{
values[i][j].write（out）；
}
}
}
6）MapWritable和SortedMapWritable。MapWritable和SortedMapWritable分别是java.util.Map（）和java.util.SortedMap（）的实现。
这两个实例是按照如下格式声明的：
private Map＜Writable, Writable＞instance；
private SortedMap＜WritableComparable, Writable＞instance；
我们可以用Hadoop定义的Writable类型来填充key或value，也可以使用自己定义的Writable类型来填充。
在java.util.Map（）和java.util.SortedMap（）中定义的功能，如getKey（）、getValue（）、keySet（）等，在这两个类中均有实现。Map的使用也很简单，见如下程序，需要注意的是，不同key值对应的value数据类型可以不同。
package cn.edn.rm.cloodcomputing.book.chapter07；
import java.io.*；
import java.util.*；
import org.apache.hadoop.io.*；
public class MyMapre{
public static void main（String args[]）throws IOException{
MapWritable a=new MapWritable（）；
a.put（new IntWritable（1），new Text（"Hello"））；
a.put（new IntWritable（2），new Text（"World"））；
MapWritable b=new MapWritable（）；
WritableUtils.cloneInto（b, a）；
System.out.println（b.get（new IntWritable（1）））；
System.out.println（b.get（new IntWritable（2）））；
}
}
显示结果为
Hello
World
7）CompressedWritable。CompressedWritable是保存压缩数据的数据结构。跟之前介绍的数据结构不同，它实现Writable接口，主要面向在Map和Reduce阶段中的大数据对象操作，对这些大数据对象的压缩能够大大加快数据的传输速率。它的主要数据结构是一个byte数组，提供给用户必须实现的函数是readFieldsCompressed和writeCompressed。CompressedWritable在读取数据时先读取二进制字节流，然后调用ensureInflated函数进行解压，在写数据时，将输出的二进制字节流封装成压缩后的二进制字节流。
8）GenericWritable。这个数据类型是一个通用的数据封装类型。由于是通用的数据封装，它需要保存数据和数据的原始类型，其数据结构如下：
private static final byte NOT_SET=-1；
private byte type=NOT_SET；
private Writable instance；
private Configuration conf=null；
由于其特殊的数据结构，在读写时也需要读写对应的数据结构：实际数据和数据类型，并且要保证固定的顺序。
public void readFields（DataInput in）throws IOException{
//先读取数据类型
type=in.readByte（）；
……
//再读取数据
instance.readFields（in）；
}
public void write（DataOutput out）throws IOException{
if（type==NOT_SET||instance==null）
throw new IOException（"The GenericWritable has NOT been set correctly.type="
+type+"，instance="+instance）；
//先写出数据类型
out.writeByte（type）；
//在写出数据
instance.write（out）；
}
9）VersionedWritable。VersionedWritable是一个抽象的版本检查类，它主要保证在一个类的发展过程中，使用旧类编写的程序仍然能由新类解析处理。在这个类的实现中只有简单的三个函数：
//返回版本信息
public abstract byte getVersion（）；
//写出版本信息
public void write（DataOutput out）throws IOException{
out.writeByte（getVersion（））；
}
//读入版本信息
public void readFields（DataInput in）throws IOException{
byte version=in.readByte（）；
if（version！=getVersion（））
throw new VersionMismatchException（getVersion（），version）；
}
7.3.2 实现自己的Hadoop数据类型
实现自定义的Hadoop数据类型具有非常重要的意义。虽然Hadoop已经定义了很多有用的数据类型，但在实际应用中，我们总是需要定义自己的数据类型以满足程序的需要。
我们定义一个简单的整数对＜LongWritable, LongWritable＞，这个类可以用来记录文章中单词出现的位置，第一个LongWritable代表行数，第二个LongWritable代表它是该行的第几个单词。定义NumPair，如下所示：
package cn.edn.ruc.cloudcomputing.book.chapter07；
import java.io.*；
import org.apache.hadoop.io.*；
public class NumPair implements WritableComparable＜NumPair＞{
private LongWritable line；
private LongWritable location；
public NumPair（）{
set（new LongWritable（0），new LongWritable（0））；
}
public void set（LongWritable first, LongWritable second）
{
this.line=first；
this.location=second；
}
public NumPair（LongWritable first, LongWritable second）{
set（first, second）；
}
public NumPair（int first, int second）{
set（new LongWritable（first），new LongWritable（second））；
}
public LongWritable getLine（）{
return line；
}
public LongWritable getLocation（）{
return location；
}
@Override
public void readFields（DataInput in）throws IOException
{
line.readFields（in）；
location.readFields（in）；
}
@Override
public void write（DataOutput out）throws IOException{
line.write（out）；
location.write（out）；
}
public boolean equals（NumPair o）{
if（（this.line==o.line）＆＆（this.location==o.location））
return true；
return false；
}
@Override
public int hashCode（）{
return line.hashCode（）*13+location.hashCode（）；
}
@Override
public int compareTo（NumPair o）{
if（（this.line==o.line）＆＆（this.location==o.location））
return 0；
return-1；
}
}
7.4 针对Mapreduce的文件类
Hadoop定义了一些文件数据结构以适应Mapreduce编程框架的需要，其中SequenceFile和MapFile两种类型非常重要，Map输出的中间结果就是由它们表示的。其中，MapFile是经过排序并带有索引的SequenceFile。
 7.4.1 SequenceFile类
SequenceFile记录的是key/value对的列表，是序列化之后的二进制文件，因此是不能直接查看的，我们可以通过如下命令来查看这个文件的内容。
hadoop fs-text MySequenceFile（你的SequenceFile文件）