sults in less frames dropped or arrived late.
3. Less popular browsers have low rendering quality. If we limit
our analysis to chunks with good performance (rate > 1.5 sec
sec )
where the player is visible (i.e., vis = true), the rendering qual-
ity can still be bad due to inefﬁciencies in client’s rendering path.
Since we cannot measure the client host environment in production,
we only characterize the clients based on their OS and browser.
Figure 21 shows the fraction of chunks requested from browsers
on OS X and Windows platforms (each platform is normalized to
100%), as well as the average fraction of dropped frames among
chunks served by that browser. Browsers with internal Flash (e.g.,
Chrome) and native HLS support (Safari on OS X) outperform
other browsers (some of which may run Flash as a process, e.g.,
Firefox’s protected mode). Also, the unpopular browsers (grouped
as Other) have the lowest performance. We further break them
down as shown in Figure 22. We restrict to browsers that have
processed at least 500 chunks. Yandex, Vivaldi, Opera or Safari on
Windows have low rendered framerate compared to other browsers.
Take-aways: De-multiplexing, decoding and rendering video chunks
could be resource-heavy on the client machine. In absence of hard-
ware (GPU) rendering, the burden falls on CPU to process frames
efﬁciently; however, the resource demands from other applications
on the host can affect the rendering quality. We found that video
rendering requires processing time, and that a video arrival rate of
1.5 sec
sec could be used as a rule-of-thumb for achieving good render-
ing quality. Similar to download stack problems, rendering quality
differs based on OS and browser. In particular, we found unpopular
browsers to have lower rendering quality.
5. DISCUSSION
Monitoring and diagnosis is a challenging problem for large-
scale content providers due to insufﬁcient instrumentation or mea-
Figure 22: Dropped % of (browser, OS), rate ≥ 1.5 sec
T rue.
sec , vis =
surement overhead limitations. In particular, (1) sub-chunk events
such as bursty losses will not be captured in per-chunk measure-
ments; capturing them will impact player’s performance, (2) SRTT
does not reﬂect the value of round-trip time at the time of mea-
surement, rather is a smoothed average; vanilla Linux kernels only
export SRTTs to userspace today. To work with this limitation, we
use methods discussed in Section 4.2, (3) the characterization of
the rendering path could improve by capturing the underlying re-
source utilization and environment (e.g., CPU load, existence of
GPU), and (4) in-network measurements help further localization.
For example, further characterization of network problems (e.g., is
bandwidth limited at the core or the edge?) would have been pos-
sible using active probes (e.g., traceroute or ping) or in-network
measurements from ISPs (e.g., link utilization). Some of these
measurements may not be feasible at Web-scale.
6. RELATED WORK
Video streaming characterization: There is a rich area of re-
lated work in characterizing video-streaming quality. [28] uses ISP
packet traces to characterize video while [36] uses CDN-side data
to study content and Live vs VoD access patterns. Client-side data
and a clustering approach is used in [22] to ﬁnd critical problems
related to user’s ISP, CDN, or content provider. Popularity in user-
generated content video system has been characterized in [12].
Our work differs from previous work by collecting and joining
ﬁne-grained per-chunk measurements from both sides and direct
instrumentation of the video delivery path, including the client’s
download stack and rendering path.
QoE models: Studies such as [14] have shown correlations be-
tween video quality metrics and user engagement. [25] shows the
impact of video quality on user behavior using quasi experiments.
012345Download rate of chunk, secsec0510152025303540% Dropped FramesaveragemedianChromeIEFirefoxEdgeOtherSafariChromeFirefoxOtherWindows                                          Mac0102030405060Percentage% chunks in platform% dropped frames0510152025303540Average dropped % among chunksYandex, WindowsVivaldi, WindowsOpera, WindowsSafari, WindowsAverage in the rest509Network data from commercial IPTV is used in [31] to learn per-
formance indicators for users QoE, where [8] uses in-network mea-
surements to estimate QoE for mobile users. We have used the prior
work done on QoE models to extract QoE metrics that matter more
to clients (e.g., the re-buffering and startup delay) to study the im-
pact of performance problems on them.
ABR algorithms: The bitrate adaptation algorithms have been
studied well, [15] studies the interactions between HTTP and TCP,
while [9] compares different algorithms in sustainability and adap-
tation. Different algorithms have been suggested to optimize video
quality, in particular [23, 32] offer rate-based adaptation algorithms,
where [20] suggests a buffer-based approach, and [37] aims to op-
timize quality using a hybrid model. Our work is complementary
to these works, because while an optimized ABR is necessary for
good streaming quality, we showed problems where a good ABR
algorithm is not enough and corrective actions from the content
provider are needed.
Optimizing video quality by CDN selection: Previous work sug-
gests different methods for CDN selection to optimize video qual-
ity, for example [33] studies policies and methods used for server
selection in Youtube, while [24] studies causes of inﬂated latency
for better CDN placement. Some studies [26, 18, 17] make the
case for centralized video control planes to dynamically optimize
the video delivery based on a global view while [10] makes the
case for federated and P2P CDNs based on content, regional, and
temporal shift in user behavior.
7. CONCLUSION
In this paper, we presented the ﬁrst Web-scale end-to-end mea-
surement study of Internet video streaming to characterize prob-
lems located at a large content provider’s CDN, Internet, and the
client’s download and rendering paths. Instrumenting the end-to-
end path gives us a unique opportunity to look at multiple compo-
nents together during a session, at per-chunk granularity, and to dis-
cover transient and persistent problems that affect the video stream-
ing experience. We characterize several important characteristics
of video streaming services, including causes for persistent prob-
lems at CDN servers such as unpopularity, sources of persistent
high network latency, and persistent rendering problems caused by
browsers. We draw insights into the client’s download stack la-
tency (possible at scale only via end-to-end instrumentation); and
we showed that the download stack can impact the QoE and feed
incorrect information into the ABR algorithm. We discussed the
implications of our ﬁndings for content providers (e.g., pre-fetching
subsequent chunks), ISPs (establishing better peering points), and
the ABR logic (e.g., using apriori observations about client pre-
ﬁxes).
Acknowledgments
This work beneﬁted from a large number of engineers from the
Video Platforms Engineering group at Yahoo. We thank the group
for helping us with instrumentation and patiently answering many
of our questions (and hearing our performance optimization obser-
vations). Thanks to P.P.S. Narayan for supporting this project at
Yahoo. This work was in part supported by the National Science
Foundation grant CNS-1162112.
8. REFERENCES
[1] ActionScript 3.0 reference for the Adobe Flash.
http://help.adobe.com/en_US/FlashPlatform/reference/
actionscript/3/ﬂash/net/FileReference.html.
[2] Apache Trafﬁc Server. http://trafﬁcserver.apache.org.
[3] It’s latency, stupid.
https://rescomp.stanford.edu/~cheshire/rants/Latency.html.
[4] Open read retry timer.
https://docs.trafﬁcserver.apache.org/en/4.2.x/admin/
http-proxy-caching.en.html#open-read-retry-timeout.
[5] Sandvine: Global Internet phenomena report 2015. https:
//www.sandvine.com/trends/global-internet-phenomena/.
[6] Youtube statistics.
https://www.youtube.com/yt/press/statistics.html.
[7] AGGARWAL, A., SAVAGE, S., AND ANDERSON, T.
Understanding the performance of TCP pacing. In IEEE
INFOCOM (2000), pp. 1157–1165.
[8] AGGARWAL, V., HALEPOVIC, E., PANG, J.,
VENKATARAMAN, S., AND YAN, H. Prometheus: Toward
quality-of-experience estimation for mobile apps from
passive network measurements. In Workshop on Mobile
Computing Systems and Applications (2014), pp. 18:1–18:6.
[9] AKHSHABI, S., BEGEN, A. C., AND DOVROLIS, C. An
experimental evaluation of rate-adaptation algorithms in
adaptive streaming over HTTP. In ACM Conference on
Multimedia Systems (2011), pp. 157–168.
[10] BALACHANDRAN, A., SEKAR, V., AKELLA, A., AND
SESHAN, S. Analyzing the potential beneﬁts of CDN
augmentation strategies for internet video workloads. In IMC
(2013), pp. 43–56.
[11] BRESLAU, L., CAO, P., FAN, L., PHILLIPS, G., AND
SHENKER, S. Web caching and Zipf-like distributions:
Evidence and implications. In IEEE INFOCOM (1999),
pp. 126–134.
[12] CHA, M., KWAK, H., RODRIGUEZ, P., AHN, Y.-Y., AND
MOON, S. I tube, you tube, everybody tubes: Analyzing the
world’s largest user generated content video system. In IMC
(2007), pp. 1–14.
[13] DIMOPOULOS, G., LEONTIADIS, I., BARLET-ROS, P.,
PAPAGIANNAKI, K., AND STEENKISTE, P. Identifying the
root cause of video streaming issues on mobile devices. In
CoNext (2015).
[14] DOBRIAN, F., SEKAR, V., AWAN, A., STOICA, I., JOSEPH,
D., GANJAM, A., ZHAN, J., AND ZHANG, H.
Understanding the impact of video quality on user
engagement. In ACM SIGCOMM (2011), pp. 362–373.
[15] ESTEBAN, J., BENNO, S. A., BECK, A., GUO, Y., HILT,
V., AND RIMAC, I. Interactions between HTTP adaptive
streaming and TCP. In Workshop on Network and Operating
System Support for Digital Audio and Video (2012),
pp. 21–26.
[16] FREEDMAN, M. J., VUTUKURU, M., FEAMSTER, N., AND
BALAKRISHNAN, H. Geographic locality of ip preﬁxes. In
IMC (2005), pp. 13–13.
[17] GANJAM, A., JIANG, J., LIU, X., SEKAR, V., SIDDIQI, F.,
STOICA, I., ZHAN, J., AND ZHANG, H. C3: Internet-scale
control plane for video quality optimization. In USENIX
NSDI (2015), pp. 131–144.
[18] GEORGOPOULOS, P., ELKHATIB, Y., BROADBENT, M.,
MU, M., AND RACE, N. Towards network-wide QoE
fairness using OpenFlow-assisted adaptive video streaming.
In ACM SIGCOMM Workshop on Future Human-centric
Multimedia Networking (2013), pp. 15–20.
[19] GHOBADI, M., CHENG, Y., JAIN, A., AND MATHIS, M.
Trickle: Rate limiting YouTube video streaming. In USENIX
Annual Technical Conference (2012), pp. 17–17.
510[20] HUANG, T.-Y., JOHARI, R., MCKEOWN, N., TRUNNELL,
[30] SEN, S., REXFORD, J., AND TOWSLEY, D. Proxy preﬁx
M., AND WATSON, M. A buffer-based approach to rate
adaptation: Evidence from a large video streaming service.
In ACM SIGCOMM (2014), pp. 187–198.
[21] JAIN, M., AND DOVROLIS, C. End-to-end available
bandwidth: Measurement methodology, dynamics, and
relation with TCP throughput. In ACM SIGCOMM (2002),
pp. 295–308.
[22] JIANG, J., SEKAR, V., STOICA, I., AND ZHANG, H.
Shedding light on the structure of internet video quality
problems in the wild. In CoNext (2013), pp. 357–368.
[23] JIANG, J., SEKAR, V., AND ZHANG, H. Improving fairness,
efﬁciency, and stability in HTTP-based adaptive video
streaming with FESTIVE. In CoNext (2012), pp. 97–108.
[24] KRISHNAN, R., MADHYASTHA, H. V., SRINIVASAN, S.,
JAIN, S., KRISHNAMURTHY, A., ANDERSON, T., AND
GAO, J. Moving beyond end-to-end path information to
optimize CDN performance. In IMC (2009), pp. 190–201.
[25] KRISHNAN, S. S., AND SITARAMAN, R. K. Video stream
quality impacts viewer behavior: Inferring causality using
quasi-experimental designs. In IMC (2012), pp. 211–224.
[26] LIU, X., DOBRIAN, F., MILNER, H., JIANG, J., SEKAR,
V., STOICA, I., AND ZHANG, H. A case for a coordinated
Internet video control plane. In ACM SIGCOMM (2012),
pp. 359–370.
caching for multimedia streams. In IEEE INFOCOM (1999),
pp. 1310–1319.
[31] SONG, H. H., GE, Z., MAHIMKAR, A., WANG, J., YATES,
J., ZHANG, Y., BASSO, A., AND CHEN, M. Q-score:
Proactive service quality assessment in a large IPTV system.
In IMC (2011), pp. 195–208.
[32] TIAN, G., AND LIU, Y. Towards agile and smooth video
adaptation in dynamic HTTP streaming. In CoNext (2012),
pp. 109–120.
[33] TORRES, R., FINAMORE, A., KIM, J. R., MELLIA, M.,
MUNAFO, M. M., AND RAO, S. Dissecting video server
selection strategies in the YouTube CDN. In International
Conference on Distributed Computing Systems (2011),
pp. 248–257.
[34] WEAVER, N., KREIBICH, C., DAM, M., AND PAXSON, V.
Here be web proxies. In PAM (2014), pp. 183–192.
[35] XU, X., JIANG, Y., FLACH, T., KATZ-BASSETT, E.,
CHOFFNES, D., AND GOVINDAN, R. Investigating
Transparent Web Proxies in Cellular Networks. In Proc. of
PAM (2015).
[36] YIN, H., LIU, X., QIU, F., XIA, N., LIN, C., ZHANG, H.,
SEKAR, V., AND MIN, G. Inside the bird’s nest:
Measurements of large-scale live VoD from the 2008
olympics. In IMC (2009), pp. 442–455.
[27] PAXSON, V., AND ALLMAN, M. Computing TCP’s
[37] YIN, X., JINDAL, A., SEKAR, V., AND SINOPOLI, B. A
Retransmission Timer. RFC 2988 (Proposed Standard),
2000. Obsoleted by RFC 6298.
[28] PLISSONNEAU, L., AND BIERSACK, E. A longitudinal view
of HTTP video streaming performance. In Multimedia
Systems Conference (2012), pp. 203–214.
[29] POESE, I., UHLIG, S., KAAFAR, M. A., DONNET, B., AND
GUEYE, B. IP geolocation databases: Unreliable?
SIGCOMM Computer Communications Review, 2 (2011),
53–56.
control-theoretic approach for dynamic adaptive video
streaming over HTTP. In ACM SIGCOMM (2015),
pp. 325–338.
[38] YU, M., GREENBERG, A., MALTZ, D., REXFORD, J.,
YUAN, L., KANDULA, S., AND KIM, C. Proﬁling network
performance for multi-tier data center applications. In
USENIX NSDI (2011), pp. 57–70.
511