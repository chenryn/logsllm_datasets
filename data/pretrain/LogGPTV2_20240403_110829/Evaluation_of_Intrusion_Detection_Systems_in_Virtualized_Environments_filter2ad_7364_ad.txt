HYPERVISOR_grant_table_op
(GNTTABOP_copy
(e)
Fig. 7. Injecting attacks that trigger: (a) CVE-2012-5525; (b) CVE-2012-5513; (c)
CVE-2012-5510; (d) CVE-2013-4494 [invoking hypercalls from two virtual CPUs
(vCPUs)]; (e) CVE-2013-1964 [this vulnerability can also be triggered by invoking
hypercalls from one VM]
Attack contents: The contents of the unmodiﬁed attacks and the “smoke-
screen” attacks we will inject are depicted in Figs. 3a and 7 (b)–(e). To craft
“mimicry” attacks, we place each individual hypercall that is part of an attack
in the middle of a sequence of 20 injected hypercalls (i.e., at position 10). We
built this sequence by starting with the most common detection-relevant prop-
erty we observed in the planning phase — iret, iret, iret, iret. We then added
16 hypercalls such that sliding a window of size 4 over the sequence provides
common detection-relevant properties seen during IDS training (i.e., while the
hypercall activity of the database server VM has been progressing towards a
steady state); we were able to perform this because we calculated the statistic
‘number of occurrences of each variation of the detection-relevant property’ (see
Sect. 3.1). Therefore, we obscure attack patterns making them similar to regular
patterns. For example, in Fig. 8a, we depict the content of the “mimicry” attack
triggering CVE-2013-1964.
Fig. 8. Injecting IDS evasive attacks triggering CVE-2013-1964: (a) “mimicry” attack;
(b) “smoke screen” attack [the hypercalls triggering CVE-2013-1964 are marked in bold]
Evaluation of Intrusion Detection Systems in Virtualized Environments
487
Attack injection times: We craft “smoke screen” attacks by specifying attack
injection times (see Sect. 3.1). We will inject a “smoke screen” attack by delay-
ing for 0.5 s the invocation of the hypercalls comprising the attack. Since the
average rate of occurrence of the detection-relevant property for the database
server VM is 27294.9 occ./sec. (see Table 1, column ‘Run 1’), we obscure attack
patterns by making Xenini analyze approximately 13647 benign occurences of
the detection-relevant property before encountering a hypercall that is part of an
attack. For example, in Fig. 8b, we depict the “smoke screen” attack triggering
CVE-2013-1964.
After the hypercall activities of the database server VM have reached a steady
state, we begin three separate attack injection campaigns: unmodiﬁed attacks,
“mimicry” attacks, and “smoke screen” attacks. Each campaign injects 6 attacks,
with 10 s of separation between each attack.
5.2 Case Study: Testing
We now test Xenini with respect to the scenarios presented in Sect. 5.1.
Scenario #1
IDS Training. We deployed and conﬁgured Xenini and hInjector. We initalized
the IDS monitoring landscape and we trained Xenini until time ts = 5391 s. This
is the time period needed for the hypercall activities of both the web and mail
server VM to reach steady-state (see Table 1, column ‘Run 1’).
Attack Injection and Calculation of Metric Values. We injected the con-
sidered attacks over a period of tmax − ts = 109 s and then calculated metric
values, that is, true and false positive rate. These are calculated as ratios between
the number of true, or of false, alerts issued by Xenini, and the total number
of injected attacks, or of benign variations of the detection-relevant property
occuring during attack injection, respectively. We estimate the latter based on
the statistic ‘average rate of occurrence of the detection-relevant property’. We
repeated the testing phase only 3 times in order to calculate statistically accurate
metric values with a relative precision of 2 % and 95 % conﬁdence level.8
Performing repeated measurements is important for calculating a statistically
accurate value of the false positive rate. This is because the number of issued
false alerts and the total number of benign variations of the detection-relevant
property occuring during attack injection vary between measurements due to
the non-determinism of benign hypercall activities. We observed that the true
positive rate normally does not vary, since the number and properties of injected
attacks (i.e., the attacks’ contents and attack injection times) are ﬁxed.
8 In addition, we repeated the testing phase over 30 times observing that the obtained
metric values negligibly diﬀer from those we present here. This is primarily because
of the high repeatability of hypercall activities and it indicates that only a small
number of repetitions is needed to calculate statistically accurate metric values.
488
A. Milenkoski et al.
In Table 2, we present Xenini’s attack detection score. It can be concluded
that Xenini exhibited a true positive rate of 0.5 when conﬁgured such that
th = 0.3. We now consider multiple IDS operating points (i.e., IDS conﬁgura-
tions which yield given values of the false and true positive rate). In Fig. 9, we
depict a ROC (Receiver Operating Characteristic) curve, which plots operating
points for diﬀerent values of th. We executed separate testing phases to quantify
the false and true positive rate exhibited by Xenini for each value of th. We quan-
tiﬁed these rates by comparing the output of Xenini with the “ground truth”
information recorded by hInjector. We considered the total number of true and
false alerts issued by Xenini (i.e., 6 and 6), injected attacks, and occurences of
the detection-relevant property during attack injection, originating from both
the web and mail server VM. The results depicted in Fig. 9 match the expected
behavior of Xenini (i.e., the lesser the value of th, the more sensitive the IDS,
which results in higher true and false positive rates; see [3]). This shows the
practical usefulness of our approach.
Table 2. Detection score of Xenini [(cid:2): detected/x: not detected, th = 0.3]
Targeted vulnerability (CVE ID) Detected
CVE-2012-3495
CVE-2012-5525
CVE-2012-5513
CVE-2012-5510
CVE-2013-4494
CVE-2013-1964
(cid:2)
x
(cid:2)
(cid:2)
x
x
e
t
a
r
e
v
i
t
i
s
o
p
e
u
r
T
1
0.8
0.6
0.4
0.2
0
0
[0.078 × 10−2]
[0.079 × 10−2]
[0.23 × 10−2]
[0.3 × 10−2]
1
2
3
False positive rate
4
·10−6
Fig. 9. Attack detection accuracy of Xenini [th = 0.1: (2.42 × 10
(1.61 × 10
the optimal operating point]
−6; 0.83) (cid:129) th = 0.3/th = 0.4: (0.4 × 10
−6; 0.83) (cid:129) th = 0.2:
−6, 0.5) (cid:129) th=0.5: (0, 0.33) (cid:129) (cid:3) marks
Evaluation of Intrusion Detection Systems in Virtualized Environments
489
We now calculate values of the ‘expected cost’ metric (Cexp) developed
by Gaﬀney and Ulvila [17], which expresses the impact of the base rate (see
Sect. 3.1). This metric combines ROC curve analysis with cost estimation by
associating an estimated cost with each IDS operating point. The measure of
cost is relevant in scenarios where a response that may be costly is taken when
an IDS issues an alert. Gaﬀney and Ulvila introduce a cost ratio C = Cβ/Cα,
where Cα is the cost of an alert when an intrusion has not occured, and Cβ is
the cost of not detecting an intrusion when it has occurred. To calculate values
of Cexp, we set C to 10 (i.e., the cost of not responding to an attack is 10 times
higher than the cost of responding to a false alert; see [17]).
We estimate the base rate as follows. We have injected 6 attacks consisting of
115 hypercalls over 109 s. Further, the average rate of occurence of the detection
relevant property originating from the web and mail server VM during attack
injection is estimated at 19644.5 + 3141.5 = 22786 occ./sec. (see Table 1, column
‘Run 1’). Therefore, the base rate is
(22786×109+3) =0.5 × 10−4.
115
We calculated the actual base rate by calculating the actual average rate of
occurence of the detection relevant property during attack injection. We observed
that the diﬀerence between the actual and estimated base rate is negligible and
has no impact on values of Cexp. This is primarily because the diﬀerence between
the actual and estimated value of the average rate of occurence of the detection
relevant property is small. Further, the ratio between the number of injected
attacks and the number of occurences of the detection-relevant property during
attack injection is very low due to the typical high value of the latter. This
indicates the practical relevance of the planning phase.
In Fig. 9, we depict in square brackets values of Cexp associated with each
IDS operating point. The ‘expected cost’ metric enables the identiﬁcation of an
optimal IDS operating point. An IDS operating point is considered optimal if it
has the lowest Cexp associated with it compared to the other operating points.
We mark in Fig. 9 the optimal operating point of Xenini.
Scenario #2
IDS Training. We deployed and conﬁgured Xenini and hInjector. We initalized
the IDS monitoring landscape and, since we will inject attacks from the database
server VM, we trained Xenini over a period of 5285 s.
Attack Injection and Calculation of Metric Values. We injected the
unmodiﬁed, the “mimicry”, and the “smoke screen” attacks as part of three
separate testing phases. In Table 3, we present the anomaly scores reported by
Xenini for the injected attacks. We thus quantify the success of the “mimicry”
and “smoke screen” attacks at evading Xenini. Their evasive capabilities are
especially evident in the case of the attacks triggering CVE-2012-3495 and CVE-
2012-5510. That is, these attacks, when unmodiﬁed, can be very easily detected
by Xenini (see the high anomaly scores of 1.0 in Table 3). However, when trans-
formed into “mimicry” attacks, the detection of these attacks is signiﬁcantly
challenging (see the low anomaly scores of 0.17 and 0.14 in Table 3).
490
A. Milenkoski et al.
Table 3. Anomaly scores for the injected non-evasive and evasive attacks
Targeted vulnerability (CVE ID) Anomaly scores
Unmodiﬁed “Mimicry” “Smoke screen”
CVE-2012-3495
CVE-2012-5513
CVE-2012-5510
CVE-2013-4494
CVE-2013-1964
1.0
0.32
1.0
0.21
0.25
0.17
0.107
0.14
0.14
0.14
0.25
0.28
0.31
0.14
0.14
The results presented in Table 3 match the expected behavior of Xenini when
subjected to evasive attacks (i.e., Xenini reports lower anomaly scores for the
evasive attacks than for the unmodiﬁed attacks; see [14]). This shows the prac-
tical usefulness of our approach and the relevance of the observations made in
the planning phase, which we used to craft evasive attacks.
5.3 Further Application Scenarios
Besides evaluating typical anomaly-based IDSes, such as Xenini, our approach,
or hInjector in particular, can be used for:
– evaluating hypercall access control (AC) systems — an example of
such a system is XSM-FLASK. By evaluating AC systems, we mean verifying
AC policies for correctness. This is performed by ﬁrst executing hypercalls
whose execution in hypervisor context should be prohibited and then verify-
ing whether their execution has indeed been prohibited. hInjector can greatly
simplify this process since it allows for executing arbitrary hypercall activi-
ties and recording relevant information (e.g., information on whether invoked
hypercalls have been executed in hypervisor context, see Sect. 4.1);
– evaluating whitelisting IDSes — by whitelisting IDS, we mean IDS that
ﬁres an alarm when it observes an activity that has not been whitelisted, either
by an user or by the IDS itself while being trained. For example, OSSEC can
be conﬁgured to whitelist the hypercall activities it observes during training —
our approach involves both rigorous IDS training and execution of arbitrary
hypercall activities (see Sect. 3); RandHyp [9] and MAC/HAT [6] detect and
block the execution of hypercall invocations that originate from untrusted
locations (e.g., a loadable kernel module) — hInjector supports the injection
of hypercall attacks both from the kernel and a kernel module (see Sect. 4.1).
6 Conclusion and Future Work
We presented an approach for the live evaluation of IDSes in virtualized environ-
ments using attack injection. We presented hInjector, a tool for generating IDS
Evaluation of Intrusion Detection Systems in Virtualized Environments
491
evaluation workloads that contain virtualization-speciﬁc attacks (i.e., attacks
leveraging or targeting the hypervisor via its hypercall interface — hypercall
attacks). Such workloads are currently not available, which signiﬁcantly hinders
IDS evaluation eﬀorts. We designed hInjector with respect to three main cri-
teria: injection of realistic attacks, injection during regular system operation,
and non-disruptive attack injection. These criteria are crucial for the represen-
tative, rigorous, and practically feasible evaluation of IDSes. We demonstrated
the application of our approach and showed its practical usefulness by evaluating
a representative IDS designed to detect hypercall attacks. We used hInjector to
inject attacks that trigger real vulnerabilities as well as IDS evasive attacks.
Our work can be continued in several directions:
– We plan to explore the integration of VM replay mechanisms (e.g.,
XenTT [15]) in our approach. This may help to further alleviate concerns
related to the repeatability of VMs’ hypercall activities;
– We intend to establish a continuous eﬀort on analyzing publicly disclosed
hypercall vulnerabilities in order to regularly update hInjector’s attack library
(see Sect. 4.2). This is an important contribution since the lack of up-to-date
workloads is a major issue in the ﬁeld of IDS evaluation;
– We plan to extensively evaluate a variety of security mechanisms (see Sect. 5.3)
and work on applying our approach for injecting attacks involving operations
that are functionally similar to hypercalls, such as KVM ioctl calls.
We stress that robust IDS evaluation techniques are essential not only to
evaluate speciﬁc IDSes, but also as a driver of innovation in the ﬁeld of intrusion
detection by enabling the identiﬁcation of issues and the improvement of existing
intrusion detection techniques and systems.
Acknowledgments. This research has been supported by the Research Group of the
Standard Performance Evaluation Corporation (SPEC; http://www.spec.org, http://
research.spec.org).
References
1. Rutkowska, J., Wojtczuk, R.: Xen Owning Trilogy: Part Two. http://
invisiblethingslab.com/resources/bh08/part2.pdf
2. Wilhelm, F., Luft, M., Rey, E.: Compromise-as-a-Service. https://www.ernw.de/
download/ERNW HITBAMS14 HyperV fwilhelm mluft erey.pdf
3. Maiero, C., Miculan, M.: Unobservable intrusion detection based on call traces in
paravirtualized systems. In: Proceedings of the International Conference on Secu-
rity and Cryptography (2011)
4. Wu, J.Z., Ding, L., Wu, Y., Min-Allah, N., Khan, S.U., Wang, Y.: C2Detector: a
covert channel detection framework in cloud computing. Secur. Commun. Netw.
7(3), 544–557 (2014)
5. Milenkoski, A., Payne, B.D., Antunes, N., Vieira, M., Kounev, S.: Experience
report: an analysis of hypercall handler vulnerabilities. In: Proceedings of the 25th
IEEE International Symposium on Software Reliability Engineering. IEEE (2014)
492
A. Milenkoski et al.
6. Le, C.H.: Protecting Xen Hypercalls. Master’s thesis, UBC (2009)
7. Bharadwaja, S., Sun, W., Niamat, M., Shen, F.: A Xen hypervisor based collabora-
tive intrusion detection system. In: Proceedings of the 8th International Conference
on Information Technology, pp. 695–700. IEEE (2011)
8. Srivastava, A., Singh, K., Giﬃn, J.: Secure observation of kernel behavior (2008).
http://hdl.handle.net/1853/25464
9. Wang, F., Chen, P., Mao, B., Xie, L.: RandHyp: preventing attacks via Xen hyper-
call interface. In: Gritzalis, D., Furnell, S., Theoharidou, M. (eds.) SEC 2012. IFIP
AICT, vol. 376, pp. 138–149. Springer, Heidelberg (2012)
10. Pham, C., Chen, D., Kalbarczyk, Z., Iyer, R.: CloudVal: a framework for validation
of virtualization environment in cloud infrastructure. In: Proceedings of DSN 2011,
pp. 189–196 (2011)
11. Le, M., Gallagher, A., Tamir, Y.: Challenges and opportunities with fault injection
in virtualized systems. In: VPACT (2008)
12. Fonseca, J., Vieira, M., Madeira, H.: Evaluation of web security mechanisms using
vulnerability and attack injection. IEEE Trans. Dependable Secure Comput. 11(5),
440–453 (2014)
13. Axelsson, S.: The base-rate fallacy and its implications for the diﬃculty of intrusion
detection. ACM Trans. Inf. Syst. Secur. 3(3), 186–205 (2000)
14. Wagner, D., Soto, P.: Mimicry attacks on host-based intrusion detection systems.
In: Proceedings of the 9th ACM Conference on Computer and Communications
Security, pp. 255–264 (2002)
15. Burtsev, A.: Deterministic systems analysis. Ph.D. thesis, University of Utah
(2013)
16. Forrest, S., Hofmeyr, S., Somayaji, A., Longstaﬀ, T.: A sense of self for Unix
processes. In: IEEE Symposium on Security and Privacy, pp. 120–128, May 1996
17. Gaﬀney, J.E., Ulvila, J.W.: Evaluation of intrusion detectors: a decision theory
approach. In: Proceedings of the 2001 IEEE Symposium on Security and Privacy,
pp. 50–61 (2001)