### HYPERVISOR Grant Table Operation (GNTTABOP_copy)

**Figure 7. Injecting Attacks Triggering Various CVEs:**
- **(a) CVE-2012-5525**
- **(b) CVE-2012-5513**
- **(c) CVE-2012-5510**
- **(d) CVE-2013-4494 [Invoking hypercalls from two virtual CPUs (vCPUs)]**
- **(e) CVE-2013-1964 [This vulnerability can also be triggered by invoking hypercalls from one VM]**

### Attack Contents
The unmodified attacks and the "smoke-screen" attacks we will inject are depicted in Figures 3a and 7(b)–(e). To craft "mimicry" attacks, we place each individual hypercall that is part of an attack in the middle of a sequence of 20 injected hypercalls (i.e., at position 10). We built this sequence by starting with the most common detection-relevant property observed during the planning phase—`iret, iret, iret, iret`. We then added 16 hypercalls such that sliding a window of size 4 over the sequence provides common detection-relevant properties seen during IDS training (i.e., while the hypercall activity of the database server VM has been progressing towards a steady state). This was possible because we calculated the statistic 'number of occurrences of each variation of the detection-relevant property' (see Section 3.1). Therefore, we obscure attack patterns, making them similar to regular patterns. For example, in Figure 8a, we depict the content of the "mimicry" attack triggering CVE-2013-1964.

**Figure 8. Injecting IDS Evasive Attacks Triggering CVE-2013-1964:**
- **(a) "Mimicry" attack**
- **(b) "Smoke screen" attack [The hypercalls triggering CVE-2013-1964 are marked in bold]**

### Evaluation of Intrusion Detection Systems in Virtualized Environments

**Attack Injection Times:**
We craft "smoke screen" attacks by specifying attack injection times (see Section 3.1). We inject a "smoke screen" attack by delaying for 0.5 seconds the invocation of the hypercalls comprising the attack. Since the average rate of occurrence of the detection-relevant property for the database server VM is 27294.9 occurrences/second (see Table 1, column 'Run 1'), we obscure attack patterns by making Xenini analyze approximately 13647 benign occurrences of the detection-relevant property before encountering a hypercall that is part of an attack. For example, in Figure 8b, we depict the "smoke screen" attack triggering CVE-2013-1964.

After the hypercall activities of the database server VM have reached a steady state, we begin three separate attack injection campaigns: unmodified attacks, "mimicry" attacks, and "smoke screen" attacks. Each campaign injects 6 attacks, with 10 seconds of separation between each attack.

### 5.2 Case Study: Testing

**Scenario #1: IDS Training**
- We deployed and configured Xenini and hInjector.
- We initialized the IDS monitoring landscape and trained Xenini until time \( t_s = 5391 \) seconds. This is the time period needed for the hypercall activities of both the web and mail server VMs to reach a steady state (see Table 1, column 'Run 1').

**Attack Injection and Calculation of Metric Values:**
- We injected the considered attacks over a period of \( t_{\text{max}} - t_s = 109 \) seconds and then calculated metric values, i.e., true and false positive rates.
- These metrics are calculated as ratios between the number of true or false alerts issued by Xenini and the total number of injected attacks or benign variations of the detection-relevant property occurring during attack injection, respectively.
- We estimate the latter based on the statistic 'average rate of occurrence of the detection-relevant property'.
- We repeated the testing phase only 3 times to calculate statistically accurate metric values with a relative precision of 2% and a 95% confidence level.

Performing repeated measurements is important for calculating a statistically accurate value of the false positive rate. This is because the number of issued false alerts and the total number of benign variations of the detection-relevant property occurring during attack injection vary between measurements due to the non-determinism of benign hypercall activities. We observed that the true positive rate normally does not vary, since the number and properties of injected attacks (i.e., the attacks' contents and attack injection times) are fixed.

**Table 2. Detection Score of Xenini [(✓): detected / (x): not detected, th = 0.3]**
| Targeted Vulnerability (CVE ID) | Detected |
|---------------------------------|----------|
| CVE-2012-3495                   | ✓        |
| CVE-2012-5525                   | x        |
| CVE-2012-5513                   | ✓        |
| CVE-2012-5510                   | ✓        |
| CVE-2013-4494                   | x        |
| CVE-2013-1964                   | x        |

**Figure 9. Attack Detection Accuracy of Xenini [th = 0.1: (2.42 × 10^-6; 0.83) (✓) th = 0.3/th = 0.4: (0.4 × 10^-6; 0.83) (✓) th = 0.2: (1.61 × 10^-6; 0.5) (✓) th = 0.5: (0, 0.33) (✓) (✓) marks the optimal operating point]**

### Expected Cost Metric (Cexp)
We now calculate values of the 'expected cost' metric (Cexp) developed by Gaﬀney and Ulvila [17], which expresses the impact of the base rate (see Section 3.1). This metric combines ROC curve analysis with cost estimation by associating an estimated cost with each IDS operating point. The measure of cost is relevant in scenarios where a response that may be costly is taken when an IDS issues an alert. Gaﬀney and Ulvila introduce a cost ratio \( C = \frac{C_\beta}{C_\alpha} \), where \( C_\alpha \) is the cost of an alert when an intrusion has not occurred, and \( C_\beta \) is the cost of not detecting an intrusion when it has occurred. To calculate values of Cexp, we set \( C \) to 10 (i.e., the cost of not responding to an attack is 10 times higher than the cost of responding to a false alert; see [17]).

We estimate the base rate as follows:
- We injected 6 attacks consisting of 115 hypercalls over 109 seconds.
- The average rate of occurrence of the detection-relevant property originating from the web and mail server VMs during attack injection is estimated at 19644.5 + 3141.5 = 22786 occurrences/second (see Table 1, column 'Run 1').
- Therefore, the base rate is \( \frac{22786 \times 109 + 3}{115} = 0.5 \times 10^{-4} \).

We calculated the actual base rate by calculating the actual average rate of occurrence of the detection-relevant property during attack injection. We observed that the difference between the actual and estimated base rate is negligible and has no impact on values of Cexp. This is primarily because the difference between the actual and estimated value of the average rate of occurrence of the detection-relevant property is small. Further, the ratio between the number of injected attacks and the number of occurrences of the detection-relevant property during attack injection is very low due to the typically high value of the latter. This indicates the practical relevance of the planning phase.

In Figure 9, we depict in square brackets values of Cexp associated with each IDS operating point. The 'expected cost' metric enables the identification of an optimal IDS operating point. An IDS operating point is considered optimal if it has the lowest Cexp associated with it compared to the other operating points. We mark in Figure 9 the optimal operating point of Xenini.

### Scenario #2: IDS Training
- We deployed and configured Xenini and hInjector.
- We initialized the IDS monitoring landscape and, since we will inject attacks from the database server VM, we trained Xenini over a period of 5285 seconds.

**Attack Injection and Calculation of Metric Values:**
- We injected the unmodified, "mimicry," and "smoke screen" attacks as part of three separate testing phases.
- In Table 3, we present the anomaly scores reported by Xenini for the injected attacks. We thus quantify the success of the "mimicry" and "smoke screen" attacks at evading Xenini. Their evasive capabilities are especially evident in the case of the attacks triggering CVE-2012-3495 and CVE-2012-5510. That is, these attacks, when unmodified, can be very easily detected by Xenini (see the high anomaly scores of 1.0 in Table 3). However, when transformed into "mimicry" attacks, the detection of these attacks is significantly challenging (see the low anomaly scores of 0.17 and 0.14 in Table 3).

**Table 3. Anomaly Scores for the Injected Non-Evasive and Evasive Attacks**
| Targeted Vulnerability (CVE ID) | Anomaly Scores |
|---------------------------------|----------------|
| CVE-2012-3495                   | 1.0            |
| CVE-2012-5513                   | 0.32           |
| CVE-2012-5510                   | 1.0            |
| CVE-2013-4494                   | 0.21           |
| CVE-2013-1964                   | 0.25           |
| Unmodified                      | 1.0            |
| Mimicry                         | 0.17           |
| Smoke Screen                    | 0.14           |

The results presented in Table 3 match the expected behavior of Xenini when subjected to evasive attacks (i.e., Xenini reports lower anomaly scores for the evasive attacks than for the unmodified attacks; see [14]). This shows the practical usefulness of our approach and the relevance of the observations made in the planning phase, which we used to craft evasive attacks.

### 5.3 Further Application Scenarios
Besides evaluating typical anomaly-based IDSes, such as Xenini, our approach, or hInjector in particular, can be used for:
- **Evaluating Hypercall Access Control (AC) Systems:** An example of such a system is XSM-FLASK. By evaluating AC systems, we mean verifying AC policies for correctness. This is performed by first executing hypercalls whose execution in hypervisor context should be prohibited and then verifying whether their execution has indeed been prohibited. hInjector can greatly simplify this process since it allows for executing arbitrary hypercall activities and recording relevant information (e.g., information on whether invoked hypercalls have been executed in hypervisor context, see Section 4.1).
- **Evaluating Whitelisting IDSes:** By whitelisting IDS, we mean IDS that fires an alarm when it observes an activity that has not been whitelisted, either by a user or by the IDS itself while being trained. For example, OSSEC can be configured to whitelist the hypercall activities it observes during training—our approach involves both rigorous IDS training and execution of arbitrary hypercall activities (see Section 3). RandHyp [9] and MAC/HAT [6] detect and block the execution of hypercall invocations that originate from untrusted locations (e.g., a loadable kernel module)—hInjector supports the injection of hypercall attacks both from the kernel and a kernel module (see Section 4.1).

### 6. Conclusion and Future Work
We presented an approach for the live evaluation of IDSes in virtualized environments using attack injection. We presented hInjector, a tool for generating IDS evaluation workloads that contain virtualization-specific attacks (i.e., attacks leveraging or targeting the hypervisor via its hypercall interface—hypercall attacks). Such workloads are currently not available, which significantly hinders IDS evaluation efforts. We designed hInjector with respect to three main criteria: injection of realistic attacks, injection during regular system operation, and non-disruptive attack injection. These criteria are crucial for the representative, rigorous, and practically feasible evaluation of IDSes. We demonstrated the application of our approach and showed its practical usefulness by evaluating a representative IDS designed to detect hypercall attacks. We used hInjector to inject attacks that trigger real vulnerabilities as well as IDS evasive attacks.

Our work can be continued in several directions:
- **Integration of VM Replay Mechanisms:** We plan to explore the integration of VM replay mechanisms (e.g., XenTT [15]) in our approach. This may help to further alleviate concerns related to the repeatability of VMs' hypercall activities.
- **Continuous Effort on Analyzing Publicly Disclosed Hypercall Vulnerabilities:** We intend to establish a continuous effort on analyzing publicly disclosed hypercall vulnerabilities in order to regularly update hInjector's attack library (see Section 4.2). This is an important contribution since the lack of up-to-date workloads is a major issue in the field of IDS evaluation.
- **Extensive Evaluation of a Variety of Security Mechanisms:** We plan to extensively evaluate a variety of security mechanisms (see Section 5.3) and work on applying our approach for injecting attacks involving operations that are functionally similar to hypercalls, such as KVM ioctl calls.

We stress that robust IDS evaluation techniques are essential not only to evaluate specific IDSes but also as a driver of innovation in the field of intrusion detection by enabling the identification of issues and the improvement of existing intrusion detection techniques and systems.

### Acknowledgments
This research has been supported by the Research Group of the Standard Performance Evaluation Corporation (SPEC; http://www.spec.org, http://research.spec.org).

### References
1. Rutkowska, J., Wojtczuk, R.: Xen Owning Trilogy: Part Two. http://invisiblethingslab.com/resources/bh08/part2.pdf
2. Wilhelm, F., Luft, M., Rey, E.: Compromise-as-a-Service. https://www.ernw.de/download/ERNW_HITBAMS14_HyperV_fwilhelm_mluft_erey.pdf
3. Maiero, C., Miculan, M.: Unobservable intrusion detection based on call traces in paravirtualized systems. In: Proceedings of the International Conference on Security and Cryptography (2011)
4. Wu, J.Z., Ding, L., Wu, Y., Min-Allah, N., Khan, S.U., Wang, Y.: C2Detector: a covert channel detection framework in cloud computing. Secur. Commun. Netw. 7(3), 544–557 (2014)
5. Milenkoski, A., Payne, B.D., Antunes, N., Vieira, M., Kounev, S.: Experience report: an analysis of hypercall handler vulnerabilities. In: Proceedings of the 25th IEEE International Symposium on Software Reliability Engineering. IEEE (2014)
6. Le, C.H.: Protecting Xen Hypercalls. Master’s thesis, UBC (2009)
7. Bharadwaja, S., Sun, W., Niamat, M., Shen, F.: A Xen hypervisor based collaborative intrusion detection system. In: Proceedings of the 8th International Conference on Information Technology, pp. 695–700. IEEE (2011)
8. Srivastava, A., Singh, K., Giﬃn, J.: Secure observation of kernel behavior (2008). http://hdl.handle.net/1853/25464
9. Wang, F., Chen, P., Mao, B., Xie, L.: RandHyp: preventing attacks via Xen hypercall interface. In: Gritzalis, D., Furnell, S., Theoharidou, M. (eds.) SEC 2012. IFIP AICT, vol. 376, pp. 138–149. Springer, Heidelberg (2012)
10. Pham, C., Chen, D., Kalbarczyk, Z., Iyer, R.: CloudVal: a framework for validation of virtualization environment in cloud infrastructure. In: Proceedings of DSN 2011, pp. 189–196 (2011)
11. Le, M., Gallagher, A., Tamir, Y.: Challenges and opportunities with fault injection in virtualized systems. In: VPACT (2008)
12. Fonseca, J., Vieira, M., Madeira, H.: Evaluation of web security mechanisms using vulnerability and attack injection. IEEE Trans. Dependable Secure Comput. 11(5), 440–453 (2014)
13. Axelsson, S.: The base-rate fallacy and its implications for the difficulty of intrusion detection. ACM Trans. Inf. Syst. Secur. 3(3), 186–205 (2000)
14. Wagner, D., Soto, P.: Mimicry attacks on host-based intrusion detection systems. In: Proceedings of the 9th ACM Conference on Computer and Communications Security, pp. 255–264 (2002)
15. Burtsev, A.: Deterministic systems analysis. Ph.D. thesis, University of Utah (2013)
16. Forrest, S., Hofmeyr, S., Somayaji, A., Longstaff, T.: A sense of self for Unix processes. In: IEEE Symposium on Security and Privacy, pp. 120–128, May 1996
17. Gaffney, J.E., Ulvila, J.W.: Evaluation of intrusion detectors: a decision theory approach. In: Proceedings of the 2001 IEEE Symposium on Security and Privacy, pp. 50–61 (2001)