culate the corresponding gradients over the white-box target clas-
sifier’s parameters and use these gradients as the data sample’s
feature for membership inference. Moreover, both Nasr et al. [43]
and Melis et al. [36] proposed membership inference attacks against
federated learning. While most of the previous works concentrated
on classification models [33, 34, 42, 43, 56, 58, 69], Hayes et al. [21]
studied membership inference against generative models, in partic-
ular generative adversarial networks (GANs) [18]. They designed
attacks for both white- and black-box settings. Their results showed
that generative models are also vulnerable to membership inference.
Defense mechanisms against membership inference: Multi-
ple defense mechanisms have been proposed to mitigate the threat
of membership inference in the ML setting. We summarize them as
the following.
L2-Regularizer [58]. Overfitting, i.e., ML classifiers are more
confident when facing data samples they are trained on (members)
than others, is one major reason why membership inference is ef-
fective. Therefore, to defend against membership inference, people
have explored to reduce overfitting using regularization. For in-
stance, Shokri et al. [58] explored using conventional L2 regularizer
when training the target classifier.
Min-Max Game [42]. Nasr et al. [42] proposed a min-max
game-theoretic method to train a target classifier. Specifically, the
method formulates a min-max optimization problem that aims to
minimize the target classifier’s prediction loss while maximizing
the membership privacy. This formulation is equivalent to adding
a new regularization term called adversarial regularization to the
loss function of the target classifier.
Dropout [56]. Dropout is a recently proposed technique to
regularize neural networks [61]. Salem et al. [56] explored using
dropout to mitigate membership inference attacks. Roughly speak-
ing, dropout drops a neuron with a certain probability in each
iteration of training a neural network.
Model Stacking [56]. Model stacking is a classical ensemble
method which combines multiple weak classifiers’ results as a
strong one. Salem et al. [56] explored using model stacking to
mitigate membership inference attacks. Specifically, the target clas-
sifier consists of three classifiers organized into a two-level tree
structure. The first two classifiers on the bottom of the tree take
the original data samples as input, while the third one’s input is the
outputs of the first two classifiers. The three classifiers are trained
using disjoint sets of data samples, which reduces the chance for
the target classifier to remember any specific data sample, thus
preventing overfitting.
Differential privacy. Differential privacy [13] is a classical
method for privacy-preserving machine learning. Most differential
privacy based defenses add noise to the objective function that is
used to learn a model [12, 24, 30], or the gradient in each itera-
tion of gradient descent or stochastic gradient descent that is used
to minimize the objective function [1, 6, 60, 66, 70]. Shokri and
Shmatikov [57] designed a differential privacy method for collabo-
rative learning of deep neural networks.
Limitations. Existing defenses suffer from two key limitations:
1) they do not have formal utility loss guarantee of the confidence
score vector; and 2) they achieve suboptimal privacy-utility trade-
offs. Our defense addresses these two limitations. For instance,
as we will show in experiments, with the same utility loss of the
confidence score vector (e.g., the same L1-norm distortion of the
confidence score vector), our defense reduces the attack classifier’s
accuracy at inferring members/non-members to a larger extent
than existing defenses.
Other privacy/confidentiality attacks against ML: There ex-
ist multiple other types of privacy/confidentiality attacks against
ML models [2, 14–16, 36, 44, 55, 64, 65]. Fredrikson et al. [14, 15]
proposed model inversion attacks. For instance, they can infer the
missing values of an input feature vector by leveraging a classifier’s
prediction on the input feature vector. Several works [2, 16, 36] stud-
ied property inference attacks, which aim to infer a certain property
(e.g., the fraction of male and female users) of a target classifier’s
training dataset. Tramèr et al. [64] proposed model stealing attacks.
They designed different techniques tailored to different ML models
aiming at stealing the parameters of the target models. Another line
of works studied hyperparameter stealing attacks [44, 65], which
aim to steal the hyperparameters such as the neural network ar-
chitecture and the hyperparameter that balances between the loss
function and the regularization term.
Table 1: Notations
we have:
Notation Description
A data sample
x
A true confidence score vector
s
s′
A noisy confidence score vector
A noise vector
n
Decision function of the target classifier
f
Logits of the target classifier
z
Attacker’s attack classifier for membership inference
C
Decision function of defender’s defense classifier
д
Logits of the defender’s defense classifier
h
M Randomized noise addition mechanism
ϵ
Confidence score distortion budget
2.2 Adversarial Examples
Given a classifier and an example, we can add carefully crafted noise
to the example such that the classifier predicts its label as we desire.
The example with carefully crafted noise is called an adversarial ex-
ample. Our MemGuard adds carefully crafted noise to a confidence
score vector to turn it into an adversarial example, which is likely to
mislead the attack classifier to make a random guessing at member
or non-member. The adversarial machine learning community has
developed many algorithms (e.g., [10, 19, 31, 35, 39, 40, 50, 63]) to
find adversarial examples. However, these algorithms are insuffi-
cient to our problem because they did not consider the utility-loss
constraints on the confidence score vectors. We address these chal-
lenges via designing a new algorithm to find adversarial examples.
Since our defense leverages adversarial examples to mislead
the attacker’s attack classifier, an adaptive attacker can leverage
a classifier that is more robust against adversarial examples as
the attack classifier. Although different methods (e.g., adversar-
ial training [19, 35, 63], defensive distillation [51], Region-based
Classification [9], MagNet [37], and Feature Squeezing [68]) have
been explored to make classifiers robust against adversarial exam-
ples, it is still considered an open challenge to design such robust
classifiers. Nevertheless, in our experiments, we will consider the
attacker uses adversarial training to train its attack classifier, as ad-
versarial training was considered to be the most empirically robust
method against adversarial examples so far [3].
3 PROBLEM FORMULATION
In our problem formulation, we have three parties, i.e., model
provider, attacker, and defender. Table 1 shows some important
notations used in this paper.
3.1 Model Provider
We assume a model provider has a proprietary training dataset (e.g.,
healthcare dataset, location dataset). The model provider trains a
machine learning classifier using the proprietary training dataset.
Then, the model provider deploys the classifier as a cloud service
or a client-side AI software product (e.g., a mobile or IoT app), so
other users can leverage the classifier to make predictions for their
own data samples. In particular, we consider the deployed classifier
returns a confidence score vector for a query data sample. Formally,
f : x (cid:55)→ s,
where f , x, and s represent the classifier’s decision function, the
query data sample, and the confidence score vector, respectively.
The confidence score vector essentially is the predicted posterior
probability distribution of the label of the query data sample, i.e.,
sj is the predicted posterior probability that the query data sample
has label j. The label of the query data sample is predicted to be the
one that has the largest confidence score, i.e., the label is predicted
as argmaxj{sj}. For convenience, we call the model provider’s clas-
sifier target classifier. Moreover, we consider the target classifier is
neural network in this work.
3.2 Attacker
An attacker aims to infer the proprietary training dataset of the
model provider. Specifically, we consider the attacker only has
black-box access to the target classifier, i.e., the attacker can send
query data samples to the target classifier and obtain their confi-
dence score vectors predicted by the target classifier. The attacker
leverages black-box membership inference attacks [34, 42, 56, 58] to
infer the members of the target classifier’s training dataset. Roughly
speaking, in membership inference attacks, the attacker trains a bi-
nary classifier, which takes a query data sample’s confidence score
vector as input and predicts whether the query data sample is in
the target classifier’s training dataset or not. Formally, we have:
C : s (cid:55)→ {0, 1},
where C is the attacker’s binary classifier, s is the confidence score
vector predicted by the target classifier for the query data sample
x, 0 indicates that the query data sample x is not a member of the
target classifier’s training dataset, and 1 indicates that the query
data sample x is a member of the target classifier’s training dataset.
For convenience, we call the attacker’s binary classifier C attack
classifier. We will discuss more details about how the attacker could
train its attack classifier in Section 5. Note that, to consider strong
attacks, we assume the attacker knows our defense mechanism, but
the defender does not know the attack classifier since the attacker
has many choices for the attack classifier.
3.3 Defender
The defender aims to defend against black-box membership infer-
ence attacks. The defender could be the model provider itself or
a trusted third party. For any query data sample from any user,
the target classifier predicts its confidence score vector and the
defender adds a noise vector to the confidence score vector before
returning it to the user. Formally, we have:
s′ = s + n,
where s is the true confidence score vector predicted by the target
classifier for a query data sample, n is the noise vector added by
the defender, and s′ is the noisy confidence score vector that is
returned to a user. Therefore, an attacker only has access to the
noisy confidence score vectors. The defender aims to add noise to
achieve the following two goals:
• Goal I. The attacker’s attack classifier is inaccurate at in-
ferring the members/non-members of the target classifier’s
training dataset, i.e., protecting the privacy of the training
dataset.
• Goal II. The utility loss of the confidence score vector is
bounded.
However, achieving these two goals faces several challenges
which we discuss next.
Achieving Goal I: The first challenge to achieve Goal I is that the
defender does not know the attacker’s attack classifier. To address
the challenge, the defender itself trains a binary classifier to perform
membership inference and adds noise vectors to the confidence
score vectors such that its own classifier is inaccurate at inferring
members/non-members. In particular, the defender’s classifier takes
a confidence score vector as input and predicts member or non-
member for the corresponding data sample. We call the defender’s
binary classifier defense classifier and denote its decision function as
д. Moreover, we consider the decision function д(s) represents the
probability that the corresponding data sample, whose confidence
score vector predicted by the target classifier is s, is a member of
the target classifier’s training dataset. In particular, we consider the
defender trains a neural network classifier, whose output layer has
one neuron with sigmoid activation function. For such classifier,
the decision function’s output (i.e., the output of the neuron in the
output layer) represents probability of being a member. Formally,
we have:
д : s (cid:55)→ [0, 1].
The defense classifier predicts a data sample to be member of the
target classifier’s training dataset if and only if д(s) > 0.5.
To make the defense classifier inaccurate, one method is to add
a noise vector to a true confidence score vector such that the de-
fense classifier makes an incorrect prediction. Specifically, if the
defense classifier predicts member (or non-member) for the true
confidence score vector, then the defender adds a noise vector such
that the defense classifier predicts non-member (or member) for the
noisy confidence score vector. However, when an attacker knows
the defense mechanism, the attacker can easily adapt its attack to
achieve a high accuracy. In particular, the attacker predicts member
(or non-member) when its attack classifier predicts non-member
(or member) for a data sample. Another method is to add noise
vectors such that the defense classifier always predicts member (or
non-member) for the noisy confidence score vectors. However, for
some true confidence score vectors, such method may need noise
that violates the utility-loss constraints of the confidence score
vectors (we will discuss utility-loss constraints later in this section).
Randomized noise addition mechanism. Therefore, we con-
sider the defender adopts a randomized noise addition mechanism
denoted as M. Specifically, given a true confidence score vector
s, the defender samples a noise vector n from the space of pos-
sible noise vectors with a probability M(n|s) and adds it to the
true confidence score vector. Since random noise is added to a true
confidence score vector, the decision function д outputs a random
probability of being member. We consider the defender’s goal is
to make the expectation of the probability of being member pre-
dicted by д close to 0.5. In other words, the defender’s goal is to
add random noise such that the defense classifier randomly guesses
member or non-member for a data sample on average. Formally,
the defender aims to find a randomized noise addition mechanism
M such that |EM(д(s + n)) − 0.5| is minimized.
Achieving Goal II: The key challenge to achieve Goal II is how to
quantify the utility loss of the confidence score vector. To address
the challenge, we introduce two utility-loss metrics.
Label loss. Our first metric concentrates on the query data sam-
ple’s label predicted by the target classifier. Recall that the label
of a query data sample is predicted as the one that has the largest
confidence score. If the true confidence score vector and the noisy
confidence score vector predict the same label for a query data
sample, then the label loss is 0 for the query data sample, otherwise
the label loss is 1 for the query data sample. The overall label loss
of a defense mechanism is the label loss averaged over all query
data samples. In some critical applications such as finance and
healthcare, even 1% of label loss may be intolerable. In this work,
we aim to achieve 0 label loss, i.e., our noise does not change the
predicted label for any query data sample. Formally, we aim to
achieve argmaxj{sj} = argmaxj{sj + nj}, where argmaxj{sj} and
argmaxj{sj + nj} are the labels predicted based on the true and
noisy confidence score vectors, respectively.
Confidence score distortion. The confidence score vector for
a query data sample tells the user more information about the data
sample’s label beyond the predicted label. Therefore, the added
noise should not substantially distort the confidence score vector.
First, the noisy confidence score vector should still be a probability
j(sj +nj) = 1.
Second, the distance d(s, s + n) between the true confidence score
vector and the noisy confidence score vector should be small. In
particular, we consider the model provider specifies a confidence
score distortion budget called ϵ, which indicates the upper bound
of the expected confidence score distortion that the model provider
can tolerate. Formally, we aim to achieve EM(d(s, s + n)) ≤ ϵ.
While any distance metric can be used to measure the distortion,
we consider L1-norm of the noise vector as the distance metric, i.e.,
d(s, s + n) = ||n||1. We adopt L1-norm of the noise vector because
it is easy to interpret. Specifically, the L1-norm of the noise vector
is simply the sum of the absolute value of its entries.
Membership inference attack defense problem: After quanti-
fying Goal I and Goal II, we can formally define our problem of
defending against membership inference attacks.
distribution. Formally, we have sj +nj ≥ 0 for∀j and
Definition 3.1 (Membership-Inference-Attack Defense Problem).
Given the decision function д of the defense classifier, a confidence
score distortion budget ϵ, a true confidence score vector s, the
defender aims to find a randomized noise addition mechanism M∗
via solving the following optimization problem: