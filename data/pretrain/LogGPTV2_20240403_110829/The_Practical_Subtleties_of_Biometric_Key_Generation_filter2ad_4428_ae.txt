curity, Steganography and Watermarking of Multimedia Contents
VI, vol. 5306, pp. 622–633.
[38] ULUDAG, U., AND JAIN, A. Securing ﬁngerprint template:
Fuzzy vault with helper data. In Proceedings of the IEEE Work-
shop on Privacy Research In Vision (PRIV) (New York, NY, June
2006).
USENIX Association  
17th USENIX Security Symposium 
71
[39] ULUDAG, U., PANKANTI, S., PRABHAKAR, S., AND JAIN,
A. K. Biometric cryptosystems: Issues and challenges. Pro-
ceedings of the IEEE: Special Issue on Multimedia Security of
Digital Rights Management 92, 6 (2004), 948–960.
[40] VIELHAUER, C., AND STEINMETZ, R. Handwriting: Feature
correlation analysis for biometric hashes. EURASIP Journal on
Applied Signal Processing 4 (2004), 542–558.
[41] VIELHAUER, C., STEINMETZ, R., AND MAYERHOFER, A. Bio-
metric hash based on statistical features of online signatures. In
Proceedings of the Sixteenth International Conference on Pattern
Recognition (2002), vol. 1, pp. 123–126.
[42] WAYMAN, J. Fundamentals of biometric authentication tech-
nologies. International Journal of Image & Graphics 1, 1 (Jan-
uary 2001), 93–114.
[43] YAMAZAKI, Y., NAKASHIMA, A., TASAKA, K., AND KO-
MATSU, N. A study on vulnerability in on-line writer veriﬁca-
tion system. In Proceedings of the Eighth International Confer-
ence on Document Analysis and Recognition (Seoul, South Ko-
rea, August-September 2005), pp. 640–644.
[44] ZHANG, W., CHANG, Y.-J., AND CHEN, T. Optimal threshold-
ing for key generation based on biometrics. In Proceedings of the
International Conference on Image Processing (ICIP04) (2004),
vol. 5, pp. 3451–3454.
[45] ZHENG, G., LI, W., AND ZHAN, C. Cryptographic key genera-
tion from biometric data using lattice mapping. In ICPR ’06: Pro-
ceedings of the 18th International Conference on Pattern Recog-
nition (Washington, DC, USA, 2006), IEEE Computer Society,
pp. 513–516.
Notes
1Typically, U is computed over error-corrected values, and so the
most likely element will also be the only element that has any proba-
bility mass.
2We note that Guessing Distance is not a distance metric as it does
not necessarily satisfy symmetry or the triangle inequality.
3These measures can be reproduced given the (x, y) coordinates of
handwriting.
4This BKG is technically an instance of the fuzzy commitment pro-
posed by Juels and Wattenberg [21], which was later shown to be an
instance of a secure sketch [11].
A Guessing Distance and Guessing En-
tropy
Guessing Entropy [25] is commonly used for measuring
the expected number of guesses it takes to ﬁnd an average
element in a set assuming an optimal guessing strategy
(i.e., ﬁrst guessing the element with the highest likeli-
hood, followed by guessing the element with the second
highest likelihood, etc.). Given a distribution P over Ω
and the convention that P(ωi) ≥ P(ωi+1), Guessing En-
tropy is computed as G(P) =n
Guessing Entropy is commonly used to determine how
many guesses an adversary will take to guess a key. At
ﬁrst, Guessing Entropy and Guessing Distance appear to
be quite similar. However, there is one important dif-
ference: Guessing Entropy is a summary statistic and
i=1 iP(ωi).
Guessing Distance is not. While Guessing Entropy pro-
vides an intuitive and accurate estimate over distributions
that are close to uniform, the fact that there is one mea-
sure of strength for all users in the population may result
in somewhat misleading results when Guessing Entropy
is computed over skewed distributions.
To see why this is the case, consider the following dis-
tribution: let P be deﬁned over Ω = {ω1, . . . , ωn } as
2(n−1) for i ∈ [2, n]. That is,
P(ω1) = 1
one element (or key) is output by 50% of the users and
the remaining elements are output with equal likelihood.
The Guessing Entropy of P is:
2 , and P(ωi) = 1
G(P) =
ni=1
iP(ωi)
= P(ω1) +
iP(ωi)
ni=2
1
ni=2
i
2(n − 1)
1
2(n − 1) n(n + 1)
2
− 1
=
=
≈
+
+
1
2
1
2
n
4
Thus, although the expected number of guesses to cor-
4 , over half of the pop-
rectly select ω is approximately n
ulation’s keys are correctly guessed on the ﬁrst attempt
following the optimal strategy. To contrast this, consider
an analysis of Guessing Distance with threshold δ = 1
N .
(Assume for exposition that distributions are estimated
from a population of N = 2(n−1) users.) To do so, eval-
uate each user in the population independently. Given a
population of users, ﬁrst remove a user to compute U and
use the remaining users to compute P. Repeat this pro-
cess for the entire population.
In the case of our pathological distribution, we may
consider only two users without loss of generality: a user
with distribution U1 who outputs key ω1, and user with
distribution U2 who outputs key ω2. In the ﬁrst case, we
have GDδ(U1, P) = log 1 = 0, because the majority of
the mass according to P is assigned to ω1, which is the
most likely element according to U1. For U2, we have
2 .
t− = 2 and t+ = n, and thus GDδ(U2, P) = log n+2
Taking the minimum value (or even reporting a CDF)
shows that for a large proportion of the population (all
users with distribution U1), this distribution offers no
security—a fact that is immediately lost if we only con-
sider a summary statistic. However, it is comforting to
note, that if we compute the average of 2GD over all users,
we obtain estimates that are identical to that of guessing
entropy for sets that are sufﬁciently large:
72 
17th USENIX Security Symposium 
USENIX Association
1
N (U ,P)
2GDδ(U ,P) =
1
N  N
2
2log 1 +
2log n+2
2 
N
2
1
2 n + 2
2 
+
=
≈
1
2
n
4
B Estimating GD
As noted in Section 5, it is difﬁcult to obtain a meaning-
ful estimate of probability distributions over large sets,
e.g., N50. In order to quantify the security deﬁned by a
system, it is necessary to ﬁnd techniques to derive mean-
ingful estimates. This Appendix discusses how we esti-
mate GD. The estimate also implicitely deﬁnes an algo-
rithm that can be used to guess keys.
For convenience we use φ to denote both a biomet-
ric feature and the random variable that is deﬁned using
population statistics over φ (taken over the set Ωφ). If
a distribution is not subscripted, it is understood to be
taken over the key space Ω = Ωφ1
× · · · × Ωφn. Our
estimate uses of several tools from information theory:
Entropy. The entropy of a random variable X deﬁned
over the set Ω is
H(X) = −ω∈Ω
Pr [X = ω] log Pr [X = ω]
Mutual Information. The amount of
information
shared between two random variables X and Y deﬁned
over the domains ΩX and ΩY is measured as
I(X, Y ) =
x∈Ωx y∈Ωy
Pr [X = x ∧ Y = y] log
Pr [X = x ∧ Y = y]
Pr [X = x]Pr [Y = y]
We use the notation I(X; Y, Z) to denote the mutual in-
formation between the random variable X and the ran-
dom variable deﬁned by the joint distribution between
the random variables Y and Z.
The Estimate. Let GDδ(Uφi, Pφi
|ui−1, . . . , u1) be the
guessing distance between the user’s and population’s
distribution over φi conditioned on the even that φi−1 =
ui−1,
=
(ω1, . . . , ωn) be the elements of Ωφi ordered such that
. . . , φ1 = u1.
In particular,
let LPφi
Pφi(ωj |φi−1 = ui−1, . . . , φ1 = u1) ≥
Pφi (ωj+1 |φi−1 = ui−1, . . . , φ1 = u1)
As before, let ω∗ = argmaxω∈Ωφi
t+ be the smallest and largest indexes j such that
Uφi(ω), and t− and
|Pφi (ωj |φi−1 = ui−1, . . . , φ1 = u1) −
Pφi(ω∗
|φi−1 = ui−1, . . . , φ1 = u1)| ≤ δ
Then, GDδ(Uφi , Pφi
| ui−1, . . . , u1) = log(t− +t+) −1.
In other words, if an adversary assumes that a target user
is distributed according to the population and ﬁxes the
values of certain features, this is the number of guesses
she will need to make to guess another feature. Unfortu-
nately, this quantity is also infeasible to compute in light
of data constraints so we endeavor to ﬁnd an easily com-
putable estimate. To this end, deﬁne the weight (di) of
an element in ω ∈ Ωφi as:
di(ω | ui−1, . . . , u1) =
i−1h=1
i−1j=1
I(φi; φh, φj) Pφi(ω | φh = uh ∧ φj = uj)
The weights of elements that are more likely to occur
given the values of other features will be larger than the
weights that are less likely to occur. Intuitively, each of
the values (u) has an inﬂuence on di(ω) and those values
that correspond to features that have a higher correlation
with φi have more inﬂuence. We also note that we only
use two levels of conditional probabilities, which are rel-
atively easy to compute, instead of conditioning over the
entire space. Now, we use the weights to estimate the
probability distributions as:
ˆPφi(ωj | ui−1, . . . , u1) =
di(ωj | ui−1, . . . , u1)/ ω∈Ωφi
di(ω | ui−1, . . . , u1)
Note that while this technique may not provide a perfect
estimate of each probability, our goal is to discover the
relative magnitude of the probabilities because they will
be used to estimate Guessing Distance. We believe that
this approach achieves this goal.
We are almost ready to provide an estimate of GD.
First, we specify an ordering for the features. The or-
dering will be according to an ordering measure (M (φ))
such that features with a larger measure have a low en-
tropy (and are therefore easier to guess) and have a high
correlation with other features. An adversary could then
use this ordering to reduce the number of guesses in a
search by ﬁrst guessing features with a higher measure.
Deﬁne the feature-ordering measure for φi as:
M (φi) =i=j
1 +
H(φj)
H(φi)1+I(φi,φj )
USENIX Association  
17th USENIX Security Symposium 
73
Finally, we reindex the features such that M (φi) ≥
M (φi+1) for all i ∈ [1, 50], and estimate the guessing
distance for a speciﬁc user with φi = xi as:
GD(U, P) =
log
2GD(Uφi , ˆPφi
50i=1
1+
|xi−1,...,x1)
−1 50j=i+1
|Ωφj
|
This estimate helps in modeling an adversary that per-
forms a brute-force search over all of the features by
starting with the features that are easiest to guess and
using those features to reduce the uncertainty about fea-
tures that are more difﬁcult to guess. For each feature,
the adversary will need to make 2GD(Uφi , ˆPφi
|ui−1,...,u1)
guesses to ﬁnd the correct value. Since each incorrect
guess (2GD(Uφi , ˆPφi
|ui−1,...,u1) − 1 of them) will cause a
fruitless enumeration of the rest of the features, we mul-
tiply the number of incorrect guesses by the sizes of the
ranges of the remaining features. Finally, we take the log
to represent the number of guesses as bits.
Section 5 uses this estimation technique to measure
a user versus the population conditioned on the user’s
timation technique differs between the two settings is the
GD of a user versus the population (GD(U, P)), and for
template (GD(U, P[Tu])). The only way in which the es-
deﬁnition of Pφi. In the case of GD(U, P), Pφi is com-
user in the population. In the case of GD(U, P[Tu]), Pφi
is computed using all of the other user’s samples in con-
juction with the target user’s template to derive a set of
keys and taking the distribution over the ith element of
the keys.
puted by measuring the ith key element for every other
74 
17th USENIX Security Symposium 
USENIX Association