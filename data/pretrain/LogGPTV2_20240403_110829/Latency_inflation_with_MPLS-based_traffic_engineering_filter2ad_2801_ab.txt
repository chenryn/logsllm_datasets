D
C
 1
 0.98
 0.96
 0.94
 0.92
 0.9
 0.88
 0.86
 0.84
 0.82
 0.8
(20ms, 20%)
(30ms, 30%)
 0
 5
 10
 15
 20
Number of Spikes
F
D
C
 1
 0.98
 0.96
 0.94
 0.92
 0.9
 0.88
 0.86
 0.84
 0.82
 0.8
Figure 5: CDF of number of spikes observed by LSPs for different
values of spikes
Figure 7: Byte latency bar chart for LSP and LP
(20ms, 20%)
(30ms, 30%)
 0
 5
 15
Cummulative duration (days)
 10
 20
Figure 8: 99th percentile byte latency under MPLS (LSP)
and Optimal Routing (LP)
Figure 6: CDF of cumulative duration of spikes observed by LSPs
for different values of spikes
to tackle online due to the of size of the topology (resulting in mil-
lion variable LP) and volatility trafﬁc demand.
We divide the time into 5-minute intervals and compute the op-
timal TE strategy for each interval using the method stated above.
Compared to optimal routing, we found that MPLS based routing
incurs an overall 10% to 22% increase in weighted byte latency
over different snapshots spanning over a one day period. Figure 7
compares the latency at different trafﬁc volume percentile under
the optimal and MPLS-TE in a typical interval. There is substan-
tial latency gap between the two TE strategies — the relative la-
tency difference stays above 30% (y2 axis) at 50th, 90th, 95th, and
99th-percentile of trafﬁc volume. Figure 8 plots the latency at the
99th-percentile of trafﬁc volume under both TE strategies during an
entire day. Except between midnight and early morning hour, the
latency under MPLS-TE is consistently 20 ms (35%-40%) larger
than that under the optimal TE, leaving enough space for improve-
ment.
4.2 Is there a pattern in latency inﬂation?
LSP latency inﬂation is triggered by an LSP switching from a
short to a long path. We now study the patterns of LSP path changes
to see if they cluster at certain links, routers, DC pairs or time peri-
ods. Although there are many LSP path changes, we consider only
those that cause a latency spike (e.g.,, latency jumps by more than
20 ms and 20%) and call them LLPC’s (large latency path changes).
We ignore the remaining path changes since they either have little
impact on latency or reduce latency. For an LLPC, we attribute it
to the old path rather than the new one because it is triggered by
insufﬁcient bandwidth on the former.
Correlation with links, routers and DC pairs. We ﬁrst correlate
each LLPC with the links, routers, and DC pair the corresponding
LSP traverses. In Figure 9(a), the y-axis on the left shows the num-
ber of LLPC’s per link sorted in an increasing order and the y-axis
on the right shows the cumulative fraction of LLPC’s observed by
the links. The x-axis is normalized to anonymize the total num-
ber of links in MSN . Figure 9(b) and 9(c) plots similar curves for
routers and DC pairs respectively. From these ﬁgures, we ﬁnd that
the LLPC’s occur mostly at a small fraction of links and DC pairs
— the top 10% of links and DC pairs account for 80% and 95%
of the LLPC’s respectively. This pattern is true for routers as well,
although less pronounced. Our analysis suggests that the latency in-
ﬂation problem could be signiﬁcantly alleviated by adding capacity
to a small subset of links.
Correlation with time. Next we correlate LLPC’s with time. We
divide the time into 1-hour bins and compute the number of LLPC’s
observed per (link, time-bin) pair. Figure 10(a) plots the number of
LLPC’s of each (link, time-bin) pair in an increasing order and the
cumulative fraction of LLPC’s of all the pairs. It shows the LLPC’s
are highly concentrated both at certain links and in certain time. 1%
of the (link, time-bin) pairs witness 80% of the LLPC’s. This dis-
tribution is even more skewed than that in Figure 9(a). Since band-
width change is the primary cause of LSP path changes (§2.2), this
is likely due to dramatic trafﬁc surge in those (link, time-bin) pairs.
We observe similar patterns for (router, time-bin) and (DC-pair,
time-bin) pairs which are illustrated in Figure 10(b,c) respectively.
466 1
path changes (y1)
cummulative (y2)
 900
 800
 700
 600
 500
 400
 300
 200
 100
 0
 0
0.550 0.640 0.730 0.820 0.910 1.000
 0.8
 0.6
 0.4
 0.2
s
e
g
n
a
h
c
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
m
u
C
path changes (y1)
cummulative (y2)
 700
 600
 500
 400
 300
 200
 100
s
e
g
n
a
h
c
h
t
a
p
f
o
r
e
b
m
u
N
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
0.240 0.392 0.544 0.696 0.848 1.000
(b) Router
s
e
g
n
a
h
c
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
m
u
C
s
e
g
n
a
h
c
h
t
a
p
f
o
r
e
b
m
u
N
 1
path changes (y1)
cummulative (y2)
 2000
 1800
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
 0
0.780 0.824 0.868 0.912 0.956 1.000
 0.8
 0.6
 0.4
 0.2
(c) DC Pair
Figure 9: Histogram and cumulative path change per link, router and dcpair
 1
 0.8
 0.6
 0.4
 0.2
s
e
g
n
a
h
c
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
m
u
C
s
e
g
n
a
h
c
h
t
a
p
f
o
r
e
b
m
u
N
 0.8
 1
path changes (y1)
cummulative (y2)
 10
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0
0.948 0.958 0.969 0.979 0.990 1.000
 0.6
 0.4
 0.2
s
e
g