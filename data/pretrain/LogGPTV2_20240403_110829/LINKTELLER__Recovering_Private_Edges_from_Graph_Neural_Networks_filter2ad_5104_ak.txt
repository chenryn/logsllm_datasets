tasks, and node features (or posteriors) are useful for node
classiﬁcation does not mean that they will be useful for edge
inference. Thus, LSA2-X which tries to provide the attacker
with different levels of node information as prior knowledge
to perform the edge re-identiﬁcation attack is not effective.
On the contrary, LINKTELLER tries to analyze the inﬂuence
between nodes, which reﬂects the edge connection information
based on our theoretical analysis (Theorem 1) and is indeed
more effective for edge inference as we show empirically
in Table I and Table II.
We point out that, to our best knowledge, there are no
such settings where LINKTELLER may fail but other existing
approaches (e.g., LSA2-X) may succeed. The detailed reasons
are provided above. To summarize, our LINKTELLER lever-
ages the edge inﬂuence information, which is more relevant
for the task of edge re-identiﬁcation attack than purely node
level information used in LSA2-X. In our full version [49], we
provide detailed discussions on two speciﬁc scenarios: 1) If the
model makes inferences on single nodes and not subgraphs,
and 2) If the inference is transductive vs. inductive.
F. Details of Evaluation
1) Dataset Statistics: We provide the dataset statistics
in Table IV. The three datasets (Cora, Citeseer, and Pubmed)
TABLE IV: Dataset statistics (“m” represents multi-label classiﬁ-
cation; “s” represents single-label.)
(a) Datasets in the inductive setting
Dataset
Twitch-ES
Twitch-RU
Twitch-DE
Twitch-FR
Twitch-ENGB
Twitch-PTBR
PPI
Flickr
Nodes
4,648
4,385
9,498
6,549
7,126
1,912
14,755
89,250
Edges
59,382
37,304
153,138
112,666
35,324
31,299
225,270
899,756
Classes
2 (s)
2 (s)
2 (s)
2 (s)
2 (s)
2 (s)
121 (m)
7 (s)
(b) Datasets in the transductive setting
Dataset
Cora
Citeseer
Pubmed
Nodes
2,708
3,327
19,717
Edges
5,429
4,732
44,338
Classes
7 (s)
6 (s)
3 (s)
Features
3,170
3,170
3,170
3,170
3,170
3,170
50
500
Features
1,433
3,703
500
in the transductive setting are all citation networks. Concretely,
the nodes are documents/publications and the edges are the
citation links between them. The node features are the sparse
bag-of-words feature vectors for each document.
2) Evaluation Metrics for Model Utility: We describe how
we evaluate the utility of the trained models, including the
vanilla GCN models, two DP GCN models (EDGERAND and
LAPGRAPH), and the MLP models.
We apply slightly different evaluation metrics across
datasets given their varying properties. The twitch datasets are
for binary classiﬁcation tasks on imbalanced datasets. There-
fore, we use F1 score of the rare class to measure the utility of
the trained GCN model. To compute the value, we ﬁrst identify
the minority class in the dataset and then view it as the positive
class for the calculation of the F1 score. During training, we
train on twitch-ES; during inference, we evaluate the trained
model on twitch-{RU, DE, FR, ENGB, PTBR}. For PPI and
Flickr datasets where there is no signiﬁcant class imbalance,
we follow previous works [14], [16] and use micro-averaged
F1 score to evaluate the classiﬁcation results.
For DP GCNs particularly, in each setting, we report the
averaged results over 10 runs that use different random seeds
for noise generation.
3) Normalization Techniques: We followed Rong et al. [15]
and experimented with the techniques provided below. A is
an adjacency matrix ∈ {0, 1}n×n, D = A + I, and (cid:98)A is the
normalized matrix.
(7)
(8)
(9)
(10)
(cid:98)A = I + D
(cid:98)A = (D + I)
(cid:98)A = I + (D + I)
(cid:98)A = (D + I)
−1/2
−1/2AD
−1/2(A + I)(D + I)
−1/2
−1/2(A + I)(D + I)
−1/2
−1(A + I)
• FirstOrderGCN: First-order GCN (Eq. 7)
• AugNormAdj: Augmented Normalized Adjacency (Eq. 8)
• BingGeNormAdj: Augmented Normalized Adjacency
with Self-loop (Eq. 9)
• AugRWalk: Augmented Random Walk (Eq. 10)
4) Search Space for the Hyper-parameters: In training the
models, we perform an extensive grid search to ﬁnd the best
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2022
TABLE V: Precision (%) of the random attack baseline.
Degree
Dataset
TABLE VI: Running time of LINKTELLER on vanilla GCNs cor-
responding to experiments in Section V. The time unit is “second”.
RU
low
1.7e-2
uncon-
strained 4.3e-1
high
1.4
DE
6.7e-3
3.2e-1
7.5e-1
FR ENGB PTBR
4.5e-2
1.6
3.4
1.3e-2
1.5e-1
9.5e-1
7.5e-3
5.3e-1
1.0
PPI
1.8e-2
2.0e-1
1.2
Flickr
4.0e-3
1.0e-2
2.6e-1
set of hyper-parameters. We describe the search space of the
hyper-parameters below.
• learning rate (lr): {0.005, 0.01, 0.02, 0.04, 0.05, 0.1, 0.2}
• dropout rate: {0.05, 0.1,0.2,0.3,0.5,0.8}
• number of GCN layers: {1,2,3}
• number of hidden units: {64,128,256,512}
• normalization
technique:
AugNormAdj, BingGeNormAdj, AugRWalk}
{FirstOrderGCN,
5) Best Hyper-parameters for the Vanilla-GCN: Below, we
describe the best combinations we achieve for Vanilla-GCN
models. For twitch-ES, we use the method First-Order GCN
to normalize the input graph. We train a two-layer GCN with
the number of hidden units 256. The dropout rate is set to 0.5
and the learning rate is 0.01. The training epoch is 200 and the
model converges within 200 epochs. Due to the space limit,
we refer the readers to our full version [49] for the description
of the best combinations of hyper-parameters for the PPI and
Flickr dataset, respectively.
6) Best Hyper-parameters for the Vanilla-GAT: We use 3-
layer GATs for both PPI and Flickr datasets as described
in Section V-F. We refer the readers for a concrete description
of the GAT architectures in our full version [49].
G. More Evaluation Results
1) Results for the Random Attack Baseline: As described
in Section V-C2, for a random classiﬁer with Bernoulli param-
eter p, given a set of instances containing a positive examples
and b negative examples, its precision is a/(a + b) and recall
is p, which are density k and belief density ˆk, respectively.
We present the precision scores of the random classiﬁer in Ta-
ble V. Compared with Table I, wee see that the precision of
LINKTELLER is much higher than the random attack baseline.
This reveals the signiﬁcant advantage an attacker is able to
gain through querying an inference API, which may lead to
severe privacy loss. As for the recall which is equal to density
belief, the number ˆk ∈ {k/4, k/2, k, 2k, 4k} is also extremely
small compared with LINKTELLER. To sum up, LINKTELLER
signiﬁcantly outperforms the random baseline.
2) Results for a 3-layer GCN: In Section V-E in the main
paper, we mainly evaluated 2-layer GCNs. In this section, we
evaluate the performance of LINKTELLER on 3-layer GCNs
to provide a more comprehensive view of LINKTELLER’s
capability.
a) Model: For training the models, we follow the same
principle described in Section V-B and use the same search
space as in Appendix F4. Due to the space limit, we refer
the readers to our full version [49] for the description of the
best combination of hyper-parameters/conﬁgurations, as well
as the test F1 score of the trained models.
Degree
RU
DE
Dataset
ENGB
Flickr
low 12.5 ± 0.0 16.1 ± 0.1 13.2 ± 0.0 12.8 ± 0.0 11.2 ± 0.1 14.8 ± 0.1 30.8 ± 0.1
uncon-
strained 12.4 ± 0.1 16.0 ± 0.1 13.4 ± 0.1 12.9 ± 0.1 11.0 ± 0.1 14.7 ± 0.1 30.7 ± 0.2
high
12.4 ± 0.1 16.1 ± 0.0 13.0 ± 0.2 12.6 ± 0.0 11.5 ± 0.4 14.8 ± 0.0 30.5 ± 0.2
PTBR
PPI
FR
b) Attack Results: We refer the readers to our full ver-
sion [49] for the full attack results of LINKTELLER on the 3-
layer GCN. Comparing the performance of LINKTELLER on
2-layer GCNs in the main paper and 3-layer GCNs here, we
see that the performance of LINKTELLER on 3-layer GCNs
only drops a little. For 1-layer GCNs, we know from Propo-
sition 1 that LINKTELLER can perform a perfect attack. For
GCNs with more than 3 layers, we did not bother to evaluate
the attack performance since deeper GCNs suffer from over-
smoothing [50] and give poor classiﬁcation results. Thus, we
can conﬁdently conclude that LINKTELLER is a successful
attack against most practical GCN models.
3) Running Time of LINKTELLER: We report the running
time of LINKTELLER on vanilla GCNs in Table VI, corre-
sponding to the experiments in Section V in the main paper.
As the table shows, LINKTELLER is a highly efﬁcient attack.
On DP GCNs using EDGERAND mechanism, when the graph
becomes denser under smaller privacy budgets, one forward
pass of the network takes longer, since the cost of matrix
computation becomes larger. However, the increase of running
time reﬂected in the attack time is only marginal, so we omit
the running time for DP GCNs here. Overall, LINKTELLER
can efﬁciently and effectively attack both vanilla GCNs and
DP GCNs.
4) More Results for LINKTELLER on vanilla GCNs and
DP GCNs: First of all, we present the additional evalua-
tion results for LINKTELLER on vanilla GCNs corresponding
to Section V. The complete results can be referred to in our
full version [49], which are of the same format as Table I.
Next, we show the comprehensive evaluation results on a
combination of 2 DP mechanisms (EDGERAND and LAP-
GRAPH), 10 privacy budgets (1.0, 2.0, . . . , 10.0), 3 sampling
strategies (low degree, unconstrained degree, high degree), and
5 density beliefs (k/4, k/2, k, 2k, 4k). We refer the readers
for the complete set of results in our full version [49]. The 3
subtables in each table correspond to the 3 sampling strategies.
In Section VI of the main paper, we present the results for
density belief ˆk = k. Here, we look at the results for other
inexact ˆk values and ﬁnd that similar observations hold. First,
the effectiveness of LINKTELLER will decrease as a result of
increasing privacy guarantee; while when the guarantee is not
sufﬁcient (i.e., ε is large), LINKTELLER is not weakened by
much. Second, DP can provide better protection to nodes of
low degrees. In addition, we note that our LINKTELLER is not
sensitive to the density belief ˆk and achieves non-negligible
success rate for all ˆk.
5) LINKTELLER in the Transductive Setting: In the main
paper, we mainly evaluate the performance of LINKTELLER
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 