74.1%
61.6%
52.1%
66.6%
54.6%
attack acc
(IMentr)
78.1%
68.8%
79.4%
74.0%
Table 5: Benchmarking the effectiveness of white-box membership inference attacks proposed by Nasr et al. [32]. We can see
that compared with our black-box benchmark attacks, the advantage of white-box attacks is limited.
Model Performance
dataset
training
acc
test
acc
attack acc
by [32] (white-box)
Membership Inference Attacks
attack acc
attack acc
(Icorr)
attack acc
(Iconf)
by [32] (black-box)
attack acc
(Ientr)
attack acc
(IMentr)
Purchase100
99.8%
80.9%
Texas100
81.0%
52.3%
CIFAR100
100%
83.00%
73.4%
68.3%
74.3%
67.6%
63.0%
67.7%
59.5%
67.1%
65.7%
67.1%
64.4%
58.5%
67.8%
73.7%
60.2%
67.7%
73.3%
73.6%
Texas100 classiﬁer, much larger than 50.1% and 50.3% re-
ported by Jia et al. [20]. We even achieve higher member-
ship inference accuracy than attacks in Jia et al. [20] on
all models, except the undefended Location30 classiﬁer. Note
that the defense still works but to a limited degree: it reduces
the attack accuracy by 12% on the Location30 classiﬁer and
by 5% on the Texas100 classiﬁer. Similar to Section 3.3.2, our
proposed modiﬁed-entropy based attack always achieves
higher attack accuracy than the entropy based attack,
and is very competitive among all benchmark attacks.
Next, we discuss why Jia et al. [20] fail to achieve high
membership inference accuracy for their defended models.
We ﬁnd that most of their attacks (4 out of 6) are non-adaptive
attacks, where the adversary has no idea of the implemented
defense, and thus the membership inference attacks are not
successful. For the two adaptive attacks, Jia et al. [20] do
not put the adversary in the last step of the arms race be-
tween attacks and defenses. In their attacks, the adversary
is aware that the model predictions will be perturbed with
noises but does not know the exact algorithm of noise gen-
eration implemented by the defender. In their ﬁrst adaptive
attack, Jia et al. [20] round the model predictions to be one
decimal during the attack classiﬁer’s inference to mitigate the
effect of the perturbation. However, the attack performance
is greatly degraded when the applied perturbation is large.
In the second adaptive attack, Jia et al. [20] train the attack
classiﬁer using the state-of-the-art robust training algorithm
by Madry et al. [28], with the hope that noisy perturbation
will not change the classiﬁcation. However, the robust train-
ing algorithm [28] has a very poor generalization property:
the predictions on test points are still likely to be wrong after
adding well-designed noises. For a thorough evaluation of
the defense, we should consider that the attacker has the full
knowledge of the defense mechanism, and he or she learns
the attack model based on the defended shadow models.
3.3.4 Re-evaluating white-box membership inference
attacks [32]
We have shown that previous work may underestimate the
target models’ privacy risks, and the metric-based attacks with
only black-box access can result in higher attack accuracy
than NN based attacks for most models. Recently Nasr et
al. [32] demonstrated that a white-box membership inference
adversary can perform stronger NN based attacks by using
gradient with regard to model parameters. Next, we evaluate
whether the advantage of white-box attacks still exists by
using our metric-based black-box benchmark attacks.
We follow Nasr et al. [32] to obtain classiﬁers on
Purchase100, Texas100 and CIFAR100 datasets. The Pur-
chase100 classiﬁer and the Texas100 classiﬁer are same as un-
defended classiﬁers in Section 3.3.2. The CIFAR100 classiﬁer
is a publicly available pre-trained model,4 with the DenseNet
architecture [18]. Table 5 lists all attack results.
4https://github.com/bearpaw/pytorch-classification
USENIX Association
30th USENIX Security Symposium    2623
From Table 5, we can see that compared to the black-
box metric-based attacks, the improvement of white-box
membership inference attacks is limited. The attack accu-
racy of white-box membership inference adversary is only
0.5% and 0.6% higher than the attack accuracy achieved by
our black-box benchmark attacks, on the Texas100 and the CI-
FAR100 classiﬁers. The white-box attack on the Purchase100
classiﬁer still has 5.8% increase in attack accuracy compared
to black-box attacks. As a validation of our observations, we
note that Shejwalkar and Houmansadr also report close mem-
bership inference attack accuracy between white-box attacks
and black-box attacks in their recent work [39].
4 Fine-Grained Analysis on Privacy Risks
Prior work [20, 31, 32, 41, 44] focuses on an aggregate evalua-
tion of privacy risks by reporting overall attack accuracy or
a precision-recall pair, which are averaged over all samples.
However, the target machine learning model’s performance
is usually varied across samples, which denotes the hetero-
geneity of samples’ privacy risks. Therefore, a ﬁne-grained
privacy risk analysis of individual samples is needed, with
which we can understand the distribution of privacy risks over
samples and identify which samples have high privacy risks.
In this section, we ﬁrst deﬁne a metric called privacy risk
score to quantitatively measure the privacy risks for each
individual training member. Then we use this metric to exper-
imentally measure ﬁne-grained privacy risks of target models.
Overall, we argue that existing aggregate privacy analysis of
ML models should be supplemented with our ﬁne-grained
privacy analysis for a thorough evaluation of privacy risks.
4.1 Deﬁnition of privacy risk score
For membership inference attacks, the privacy risk of a train-
ing member arises due to the distinguishability of its model
prediction behavior with non-members. This motivates our
deﬁnition of the privacy risk score as following.
Deﬁnition 1 The privacy risk score of an input sample z =
(x,y) for the target machine learning model F is deﬁned as
the posterior probability that it is from the training set Dtr
after observing the target model’s behavior over that sample
denoted as O(F,z), i.e.,
r(z) = P(z ∈ Dtr|O(F,z))
(10)
Based on Bayes’ theorem, we further compute the privacy
risk score as following.
P(z ∈ Dtr)· P(O(F,z)|z ∈ Dtr)
r(z) =
P(O(F,z))
P(z ∈ Dtr)· P(O(F,z)|z ∈ Dtr)
=
P(z ∈ Dtr)· P(O(F,z)|z ∈ Dtr) + P(z ∈ Dte)· P(O(F,z)|z ∈ Dte)
,
(11)
where Dte stands for the test set. The observation O(F,z) de-
pends on the adversary’s access to the target model: in the
black-box membership inference attack [41], it is the model’s
ﬁnal output, i.e., O(F,z) = F(x); in the white-box member-
ship inference attacks [32], it also includes the model’s in-
termediate layers’ outputs and gradient information at all
layers. Our proposed benchmark attacks only need black-box
access to the target model, and most existing attack meth-
ods [41, 44, 48] work in the black-box manner. Therefore, we
focus on the black-box scenario for the computation of the
privacy risk score in this paper and leave the discussion on
white-box scenario as future work. In the black-box attack
scenario, the privacy risk score can be expressed as
P(z ∈ Dtr)· P(F(x)|z ∈ Dtr)
r(z) =
P(z ∈ Dtr)· P(F(x)|z ∈ Dtr) + P(z ∈ Dte)· P(F(x)|z ∈ Dte)
(12)
From Equation (12), we can see that the risk score depends
on both prior probabilities P(z ∈ Dtr), P(z ∈ Dte) and con-
ditional distributions P(F(x)|z ∈ Dtr), P(F(x)|z ∈ Dte). For
the prior probabilities, we follow previous work [41, 48] to
assume that an example is sampled from either training set or
test set with an equal 0.5 probability, where the uncertainty of
membership inference attacks is maximized. Note that the pri-
vacy risk score is naturally applicable to any prior probability
scenario, and we present the results with different prior proba-
bilities in Appendix B. With the equal probability assumption,
we have
r(z) =
P(F(x)|z ∈ Dtr)
P(F(x)|z ∈ Dtr) + P(F(x)|z ∈ Dte)
(13)
the
For
conditional distributions P(F(x)|z ∈ Dtr),
P(F(x)|z ∈ Dte), we empirically measure these values using
shadow-training technique: (1) train a shadow model to
simulate the behavior of the target model; (2) obtain the
shadow model’s prediction outputs on shadow training
and shadow test data; (3) empirically compute the condi-
tional distributions on shadow training and shadow test
data. Furthermore, as the class-dependent
thresholding
technique is shown to improve the attack success in Table 3,
we compute the distribution of model prediction over
training data P(F(x)|z ∈ Dtr) in a class-dependent manner
(P(F(x)|z ∈ Dte) is computed in the same way).
P(F(x)|z ∈ Dtr) =
P(F(x)|z ∈ Dtr,y = yn), when
y = yn
(14)
Since we empirically measure the conditional distribu-
tions using the shadow model’s predictions over shadow data,
the quality of measured distributions highly depends on the
shadow model’s similarity to the target model and the size of
P(F(x)|z ∈ Dtr,y = y0), when
P(F(x)|z ∈ Dtr,y = y1), when
y = y0
y = y1
...
2624    30th USENIX Security Symposium
USENIX Association
P(F(x)|z ∈ Dtr) ≈
P(Mentr(F(x),y)|z ∈ Dtr,y = y0), when y = y0
P(Mentr(F(x),y)|z ∈ Dtr,y = y1), when
y = y1
...
shadow data. On the one hand, the size of shadow data is usu-
ally limited. Especially in our analysis where the distribution
is computed in a class-dependent manner, for each class label
yn, we may not have enough samples5 to adequately estimate
the multi-dimension distribution P(F(x)|z ∈ Dtr,y = yn). On
the other hand, in Section 3.3 we show that by only using
the one-dimension prediction metric such as conﬁdence and
modiﬁed entropy, our proposed benchmark attacks in fact
achieve comparable or even better success that NN-based at-
tacks which leverage the whole prediction vector as features.
Thus, we propose to further approximate the multi-dimension
distribution in Equation (14) with the distribution of modi-
ﬁed prediction entropy, since using modiﬁed entropy usually
results in highest attack accuracy among all benchmark at-
tacks.6
P(Mentr(F(x),y)|z ∈ Dtr,y = yn), when
y = yn
(15)
We also approximate P(F(x)|z ∈ Dte) in the same way. By
plugging Equation (15) into Equation (13), we can get the
privacy risk score for a certain sample.
4.2 Experiment results
In our experiments, we ﬁrst validate that our proposed privacy
risk score really captures the probability of being a member.
Next, we compare the distributions of training samples’ pri-
vacy risk scores for target models without defense and with
defenses [20, 31]. We then demonstrate how to use privacy
risk scores to perform membership inference attacks with
high conﬁdence. Finally, we perform an in-depth investiga-
tion of individual samples’ privacy risk scores by correlating
them with model sensitivity, generalization errors, and feature
embeddings. To have enough diversity of data and models,
and to further evaluate defense methods, we perform exper-
iments on 3 Purchase100 classiﬁers (without defense, with
AdvReg [31], and with early stopping) and 2 Texas100 clas-
siﬁers (without defense, and with MemGuard [20]). Both
Purchase100 classiﬁers and Texas classiﬁers use fully con-
nected neural networks with 4 hidden layers, and the numbers
of neurons for hidden layers are 1024, 512, 256, and 128,
respectively. Purchase100 classiﬁers use Tanh as the activa-
tion function [31], and Texas100 classiﬁers use ReLU as the
activation function [20].
5In our experiments, on average we have 197 samples per class for Pur-
chase100 dataset; 100 samples per class for Texas100 dataset; 33 samples
per class for Location30 dataset; and 500 per class for CIFAR100 dataset.
6In most cases, both modiﬁed entropy based attack and conﬁdence based
attack give best attack performance. However, for undefended Location30 and
Texas100 classiﬁers in Table 4, the modiﬁed entropy based attack achieves
signiﬁcantly higher attack accuracy.
4.2.1 Validation of privacy risk score
Before presenting the detailed results for privacy risk score,
we ﬁrst validate its effectiveness here. For the target machine
learning model, we ﬁrst compute the privacy risk scores fol-
lowing the method in Section 4.1 for all training and test sam-
ples. Next we divide the entire range of privacy risk scores
into multiple bins, and count the number of training points
(ntr) and the number of test points (nte) in each bin. Then
we compute the fraction of training points (
) in each
bin, which indicates the real likelihood of a sample being a
member (y axis of the last column in Figure 3a). If the privacy
risk score truly corresponds to the probability that a sample is
from a target model’s training set, then we expect the actual
values of privacy risk scores and fraction of training points in
each bin to closely track with each other.
ntr+nte
ntr
As a baseline to compare with, we also consider using NN
based attacks to estimate privacy risks of individual samples.
Prior papers suggest using the attack classiﬁer’s prediction to
measure the input’s privacy risk [20, 31]. The attack classiﬁer
has only one output, which is within [0, 1] and can serve as a
proxy to estimate the probability of being a member. Follow-