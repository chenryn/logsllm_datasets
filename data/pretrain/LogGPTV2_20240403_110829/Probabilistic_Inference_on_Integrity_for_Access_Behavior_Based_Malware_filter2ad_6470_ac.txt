### Malicious Programs

We obtained a collection of 270,000 malware samples from VxHeaven, a website that provides information about viruses [30]. From this collection, we randomly selected 9,000 samples. These samples were executed in a sandbox environment running Windows XP SP3 on VMWare, with no network connection. We monitored their behavior using Process Monitor. Each sample was run for five minutes, after which the virtual machine was reverted to a clean snapshot to prevent interference between different samples. Not all samples exhibited file or registry access activities, and we ultimately collected access behaviors from 7,257 malware samples. The distribution of these malware samples across different families is listed in Table 1.

**Table 1: Number of Malware Samples in Each Family**

| Family            | Samples | Family           | Samples | Family          | Samples | Family         | Samples |
|-------------------|---------|------------------|---------|-----------------|---------|----------------|---------|
| Backdoor          | 25      | Trojan-Banker    | 75      | Trojan-Clicker  | 37      | Trojan-PSW     | 216     |
| Trojan-Spy        | 768     | Trojan-Other     | 128     | Trojan-IM       | 3       | Trojan-Mailfinder | 5       |
| Virus.JS          | 3       | Virus.NSIS       | 1       | Worm.MSIL       | 4       | Dropper        | 2       |
| GameThief         | 2       | Trojan.Win32     | 575     | Virus.BAT       | 1       | Ransom         | 18      |
| Virus.MSIL        | 2527    | Virus.MSWord     | 2547    | Virus.WinHLP    | 3       | Virus.Multi    | 24      |
| Worm.BAT         | 12      |                  |         |                 |         |                |         |

### Training and Testing Sets

The benign program executions were collected from eight users. Therefore, we set up eight experiments. In each experiment, the executions of benign programs from one user were used as the benign testing set, while those from the other seven users formed the benign training set. We inferred the probabilistic integrity levels of objects from the benign training set. Additionally, we randomly selected 80% of the malware samples as the malicious training set and the remaining 20% as the malicious testing set. Each experiment was repeated 20 times, and the results were averaged and are presented in the following subsections.

### Hyperparameters

To avoid priors dominating the data in our generative model (as shown in Eqs. (4) and (6)), we chose Jeffreys priors, setting α1, α2, α3, β1, β2, and β3 to 0.5. These are non-informative priors that are invariant under transformation [9].

### New Objects

It is common to encounter new objects that do not appear in the training set, such as objects created by processes in the testing set. We employed a heuristic method to assign probabilistic integrity levels to these new objects, similar to the approach in [20], but from a probabilistic perspective. Specifically, the probability that a new object has a high integrity level is equal to the probability that its parent directory has a high integrity level, which is the highest probability that the child objects of the parent directory have a high integrity level.

### Statistical Classifier

We used random forests, implemented in Scikit-learn [24], as our classifier. Random forests are an ensemble learning method for classification and regression that construct multiple decision trees during training. This method not only retains the advantages of decision trees but also mitigates overfitting [5]. Our results showed that random forests performed comparably to other classifiers, such as k-nearest neighbors, logistic regression, and support vector machines. However, due to page limitations, we do not present these comparisons here.

### Baseline Models

We compared our method to two baseline models for determining integrity levels:

1. **Baseline 1 (B1)**: This model strictly adheres to NRD (No Read Down) and NWU (No Write Up) policies, forming a lattice structure from the access behaviors of benign programs. The lattice consists of partial orders of integrity levels between subjects and objects, determined by observed access events. Figure 5 illustrates the lattice constructed from our dataset of benign programs, showing four layers in the hierarchical structure, indicating four possible integrity levels. B1 is a mandatory integrity protection model that relies solely on NRD and NWU security policies.

2. **Baseline 2 (B2)**: This model, introduced in [20], assigns importance values to all subjects and objects by examining their structures in a dependency network and uses statistical classifiers to detect malware based on these assigned values. We chose B2 because it has a similar goal to our work, although there is a gap between importance values and integrity levels.

### Integrity Levels and Security Policies

To evaluate the appropriateness of the derived integrity levels, we examined the differences between benign and malicious programs in terms of their compliance with NRD and NWU security policies. A violation of NRD or NWU occurs when a process reads an object with a low integrity level and writes an object with a high integrity level. Formally, for a process \( p \), the sets of reading objects \( O_r \) and writing objects \( O_w \) should satisfy \( I(p) \leq I(o_r) \) and \( I(p) \geq I(o_w) \). Since determining \( I(p) \) is challenging, we used a proxy \( I(o_w) - I(o_r) \leq 0 \) to examine violations. A violation is defined as a pair of reading and writing objects where the integrity level of the reading object is lower than that of the writing object.

We evaluated violations using two indicators:
1. **Fraction of Violations**: In each execution, we counted the fraction of violations among all pairs of reading and writing objects.
2. **Largest Violation**: This refers to the difference between the lowest integrity level of all reading objects and the highest integrity level of all writing objects in one execution.

These indicators demonstrate the utility of the derived integrity levels in detecting malware. Figures 6 and 7 show the fraction and largest violations for benign and malicious processes, respectively. The results are obtained from all testing sets of the eight experiments. Box plots in these figures split the results into quartiles, with the interquartile range box representing the middle 50% of the results and the whiskers extending to the bottom 5% and top 5%.

**Figure 6: Fraction of Violations Under Integrity Levels from Baseline and Our Models**

- **(a) File Objects**: As expected, benign processes had fewer violations than malicious processes, with significant differences (p < 10−4) under the Kolmogorov–Smirnov (KS) test. Our model outperformed the baselines in discriminating between benign and malicious processes.
- **(b) Registry Objects**: No obvious differences were observed between benign and malicious processes, although significant differences (p < 10−4) were found under the KS test. This suggests either that all models fail to determine the integrity levels of registry objects or that benign processes do not follow NRD and NWU policies when accessing registry objects.

**Figure 7: Largest Violations Under Integrity Levels from Baseline and Our Models**

- **(a) File Objects**: Our model achieved the greatest discrimination between benign and malicious processes, indicating its effectiveness in malware detection.
- **(b) Registry Objects**: Similar results were observed, with significant differences (p < 10−4) under the KS test. However, all three models struggled to distinguish malicious from benign processes, possibly for the same reasons mentioned earlier.

### Detection Results

While simple indicators like the fraction of violations and largest violations provide insights into why a model works, they may not achieve optimal performance in malware detection. We used random forests to extract adaptive security policies and build a model for malware detection. Table 2 shows the average true positive rates (TPRs) of the three models at specific false positive rates (FPRs) across all experiments. Our model generally outperformed the baselines, achieving a TPR of 99.88% at 0.1% FPR.

**Table 2: Performance Under Different Models of Determining Integrity Level**

| Model | FPR | U1 | U2 | U3 | U4 | U5 | U6 | U7 | U8 | Average TPR |
|-------|-----|----|----|----|----|----|----|----|----|-------------|
| B1    | 0%  | 92.09% | 83.59% | 70.30% | 76.08% | 53.35% | 79.79% | 64.10% | 43.10% | 70.30% |
| B2    | 0%  | 96.75% | 91.75% | 92.55% | 88.50% | 86.18% | 90.31% | 90.98% | 76.89% | 93.94% |
| Our   | 0%  | 98.09% | 96.64% | 95.18% | 92.52% | 91.85% | 91.54% | 94.19% | 90.08% | 99.98% |
| B1    | 0.1%| 99.52% | 82.34% | 97.58% | 98.22% | 88.16% | 99.40% | 99.73% | 90.02% | 99.21% |
| B2    | 0.1%| 99.27% | 99.85% | 92.92% | 99.53% | 70.21% | 99.73% | 100% | 99.57% | 99.80% |
| Our   | 0.1%| 99.86% | 99.66% | 95.32% | 99.73% | 99.60% | 99.86% | 100% | 99.93% | 99.88% |
| B1    | 0.5%| 99.45% | 99.49% | 99.51% | 97.15% | 98.30% | 99.71% | 98.31% | 99.98% | 99.97% |
| B2    | 0.5%| 99.98% | 99.98% | 99.95% | 99.89% | 99.87% | 99.94% | 99.61% | 99.85% | 99.98% |
| Our   | 0.5%| 99.97% | 99.98% | 99.99% | 99.99% | 99.99% | 99.94% | 99.97% | 99.98% | 99.99% |
| B1    | 1.0%| 100% | 100% | 99.99% | 100% | 100% | 100% | 99.98% | 99.98% | 100% |
| B2    | 1.0%| 100% | 100% | 99.99% | 100% | 100% | 100% | 99.98% | 99.98% | 100% |
| Our   | 1.0%| 100% | 100% | 99.99% | 100% | 100% | 100% | 99.98% | 99.98% | 100% |

### Wilcoxon Rank-Sum Test

To further compare the performance of the three models, we conducted a Wilcoxon rank-sum test to assess whether one model significantly outperforms the others in terms of AUC. Table 3 shows the test statistic and its significance between each pair of models. A negative value of the test statistic indicates that the first model performs worse than the second model.

**Table 3: Results of Wilcoxon Rank-Sum Tests on AUCs of Different Models**

| Models | U1 | U2 | U3 | U4 | U5 | U6 | U7 | U8 |
|--------|----|----|----|----|----|----|----|----|
| B1 vs. B2 | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% |
| B1 vs. Our | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% |
| B2 vs. Our | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% | -4.48% |

This comprehensive evaluation demonstrates the effectiveness of our model in determining integrity levels and detecting malware.