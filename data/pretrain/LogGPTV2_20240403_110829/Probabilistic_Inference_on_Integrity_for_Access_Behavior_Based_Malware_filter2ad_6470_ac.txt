Malicious programs: We download a collection of 270 K malware samples
from VxHeaven, which is a website providing information about viruses [30]. We
randomly select 9 K samples, run them in our sandbox which is with Windows
XP SP3 on VMWare without network connection, and monitor their behaviors
with Process Monitor. After running each sample for ﬁve minutes, we revert the
virtual machine to a clean snapshot so that diﬀerent samples do not interfere
with each other. Since not all samples exhibit ﬁle or registry access activities, we
ﬁnally obtain access behaviors of 7,257 malware samples. The families of ﬁnal
malware samples are listed in Table 1.
Table 1. Number of malware samples in each family
Family
Samples Family
Samples Family
Samples Family
Samples
Backdoor
25
Trojan-Banker
75
Trojan-Clicker
37
Trojan-
216
PSW
Trojan-Spy
768
Trojan-
128
Trojan-
278
Trojan-IM 3
Trojan-
Mailﬁnder
Virus.JS
Virus.NSIS
Worm.MSIL
5
3
1
4
Dropper
GameThief
Trojan-
2
Trojan.Win32
575
Virus.BAT 1
Ransom
Virus.MSIL
18
Virus.MSWord
Virus.Win32
Worm.Win32
2527
2547
Virus.WinHLP
3
5
Virus.Multi
24
Worm.BAT 12
Probabilistic Inference on Integrity for Access Behavior
167
Training and testing sets: Because the executions of benign programs are
collected from eight users, we set up eight experiments. In each experiment, we
select the executions of benign programs from one user as a benign testing set,
and those from seven other users as benign training set. We infer the probabilistic
integrity level of objects from the benign training set. Meanwhile, we randomly
select 80 % malware samples as a malicious training set and the remaining ones
as a malicious testing set. We repeat the experiment 20 times. The results of 20
repetitions are averaged and illustrated in the following subsections.
Hyperparameters: To avoid priors dominating the data in our generative
model as shown in Eqs. (4) and (6), we choose Jeﬀreys priors, i.e., α1, α2, α3,
β1, β2, and β3 are equal to 0.5, which are non-informative priors and invariant
under transformation [9].
New objects: It is very common to come across new objects which do not
appear in the training set. For example, objects are newly created by processes
in the testing set. We employ a heuristic method to assign probabilistic integrity
level to new objects, which is similar to the approach in [20], but from a prob-
abilistic perspective. More speciﬁcally, we assign probabilistic integrity level to
new objects according to the directory it is stored in. The probability that the
new object has high integrity level equals to the probability that its parent
directory has high integrity level, which equals to the highest probability that
the child objects of the parent directory have high integrity level.
Statistical classiﬁer: We employ random forests,
implemented in Scikit-
learn [24], as our classiﬁer. Random forests is an ensemble learning method for
classiﬁcation (and regression) that operate by constructing a multitude of deci-
sion trees at training time. It not only exhibits the same advantage of decision
trees, but also overcomes the main disadvantage of decision trees, namely over-
ﬁtting [5]. Moreover, we observe that the results are comparable in performance
to other classiﬁers, e.g., k-nearest neighbors, logistic regression, and support vec-
tor machine. We do not demonstrate them in this paper considering page limits.
Baseline. We compare our method to two baseline models for determining
integrity levels. The ﬁrst baseline model (B1) strictly obeys NRD and NWU,
which forms a lattice constructed from access behaviors of benign programs. The
lattice consists of partial orders of integrity levels between subjects and objects
determined by observed access events under NRD and NWU. The layers in the
hierarchical structure of the lattice indicate integrity levels. Figure 5 illustrates
the lattice constructed from our data set of benign programs. We observe four
layers in this hierarchical structure, which indicates four possible integrity levels
of subjects and objects. The lattice is shown as integrity levels increasing from
bottom to top. We employ B1 because it is a mandatory integrity protection,
which relies solely on NRD and NWU security policies.
The second baseline model (B2) is the importance based malware detec-
tion model introduced in [20]. It assigned importance values to all subjects and
objects by examining their structures in a dependency network, and employed
statistical classiﬁers to detect malware based on the assigned importance values
168
W. Mao et al.
Fig. 5. The lattice constructed from access events of benign programs in our data set.
GSCC stands for giant strongly connected component, GIN represents in-neighbors of
the GSCC but without any edge from the GSCC, and GOUT represents out-neighbors
of the GSCC but without any edge to the GSCC. Each small node indicates either a
subject (program) or an object (ﬁle or registry).
of objects. The reason for choosing B2 is that this paper has similar goal with
that, although there is a gap between importance values and integrity levels.
4.2
Integrity Levels and Security Policies
To explore the appropriateness of the derived integrity levels, we examine the dif-
ferences exhibited between benign and malicious programs from the perspective
of their compliance to security policies NRD and NWU. That is, we investigate
the violations of security policies for benign and malicious processes based on the
derived integrity levels. We deﬁne a violation of NRD or NWU when a process
reads an object with low integrity level, and writes an object with high integrity
level.
Let p be a process, Or be the set of reading objects of p, Ow be the set
of writing objects of p, or ∈ Or, ow ∈ Ow. Strictly, according to the NRD and
NWU policies, the integrity levels of p, or and ow should satisfy I(p) ≤ I(or) and
I(p) ≥ I(ow), i.e., I(p) − I(or) ≤ 0 and I(ow) − I(p) ≤ 0. Because of diﬃculty
in determining I(p), we employ a proxy I(ow) − I(or) ≤ 0, by summing up the
two criteria, to examine violations. A violation happens to the process p when
I(ow) − I(or) > 0. We refer to a violation as a pair of reading and writing
objects where the integrity level of the reading object is lower than that of
the writing object. If the determined integrity levels of objects are correct, few
Probabilistic Inference on Integrity for Access Behavior
169
(cid:3)
or ,ow
violations would appear in benign processes, while many violations would appear
in malicious processes.
1(I(ow)−I(or)>0)
|Or||Ow|
We examine violations in processes by two simple indicators based on our
proxy for violation: (1) Fraction of violation. In each execution, we count the
fraction of violations among all pairs of reading objects and writing objects,
, where 1() is an indicator function, |Or| and |Ow|
e.g.,
are the sizes of set Or and Ow. (2) Largest violation. This refers to the diﬀerence
between the lowest integrity level of all reading objects and the highest integrity
I(ow) − I(or). These
level of all writing objects in one execution, e.g., maxor,ow
two simple indicators demonstrate why the integrity levels of objects are useful to
detect malware from a perspective of security policies. Figure 6 exhibits fraction
of violations, while Fig. 7 illustrates largest violations, for benign and malicious
processes w.r.t. integrity levels of access objects determined by baselines and our
model, in terms of box plots. These results are obtained from all testing sets of
eight experiments which are presented in the above subsection. A box plot splits
these results into quartiles. The interquartile range box represents the middle
50 % of the results. The whiskers, extending from either side of the box, represent
the ranges for the bottom 5 % and the top 5 % of the results.
(a) Fraction of violation on ﬁle objects
(b) Fraction of violation on registry objects
Fig. 6. Fraction of violation under integrity levels from baseline and our models
In Fig. 6(a), as we expected, there are fewer violations in benign processes
than in malicious processes, w.r.t. integrity levels of accessed ﬁle objects, under
all three models. Using the Kolmogorov–Smirnov (KS) test, we ﬁnd signiﬁcant
diﬀerences (p (cid:5) 10−4) between benign and malicious processes under all three
models in Fig. 6(a). The KS test is a nonparametric test to evaluate whether two
samples come from the same population. These results indicate each model can
be used to determine integrity levels. We also ﬁnd that our model is more able
to discriminate between benign and malicious processes than the two baseline
models, with respect to integrity levels of accessed ﬁle objects. This indicates
that our model does a better job at determining integrity levels for ﬁle objects.
170
W. Mao et al.
With respect to the integrity levels of accessed registry objects, we do not
observe obvious diﬀerence between benign and malicious processes in Fig. 6(b),
although signiﬁcant diﬀerences (p (cid:5) 10−4) are found under KS test. There
are two possible reasons: (1) All models fail to determine the integrity levels of
registry objects. (2) Benign processes do not obey NRD and NWU policies when
they access registry objects.
(a) Largest violation on ﬁle objects
(b) Largest violation on registry objects
Fig. 7. Largest violation under integrity levels from baseline and our models
Moreover, we explore the diﬀerence between benign and malicious processes
according to the largest violation. Figure 7(a) illustrates the largest violations of
benign and malicious processes on ﬁle objects, while Fig. 7(b) illustrates those
on registry objects. Since the total number of integrity levels in B1 is much less
than those in other two models, we show the largest violation in logarithmic
scale. Similar results are observed in Fig. 7(a) and (b) compared to Fig. 6(a) and
(b). Meanwhile, we ﬁnd signiﬁcant diﬀerences (p (cid:5) 10−4) under the KS test in
all cases. As shown in Fig. 7(a), our model achieves the greatest discrimination
between benign and malicious processes according to integrity levels of accessed
ﬁle objects. It implies the ability of our model for malware detection even with
camouﬂages. However, we ﬁnd similar failures of all three models in distinguish-
ing malicious programs from benign programs, which due to similar possible
reasons as we aforementioned.
We observe obvious diﬀerences between benign and malicious processes by
examining either of these two indicators. However, the fraction of violation may
suﬀer from mimicry or camouﬂaged attacks, where malicious processes run under
the cover of some benign processes [10,15]. For example, if malware deliberately
read many ﬁle objects with the highest integrity levels, then the numerator in the
fraction of violations will be overwhelmed by the denominator in the fraction,
which leads to as small fraction of violations as benign processes. Compared
with the fraction of violation, the indicator of largest violation is much more
robust. However, one potential failure of the largest violation indicator is false
Probabilistic Inference on Integrity for Access Behavior
171
positive. There are benign processes which modify objects with high integrity
level while read objects with low integrity level. Usually, these processes are some
special processes, such as system services. We can include them in a whitelist,
and reduce the false positives of the largest violation indicator.
4.3 Detection Results
Although simple indicators, e.g., fraction of violations, largest violation, etc.,
provide us ways of understanding why a model works, they usually fail to
achieve promising performance on malware detection. As presented in Sect. 3.4,
we employ random forests to extract adaptive security policies under determined
integrity levels and build a model for malware detection. With three models for
determining integrity levels, we evaluate their performance on malware detection
with the ROC curve and the area under ROC curve (AUC). We train models
with the benign and malicious training sets, and evaluate them on the benign
and malicious testing sets, as stated in Sect. 4.1.
Table 2 exhibits average true positive rates (TPRs) of three models at speciﬁc
false positive rates (FPRs) among all experiments. We choose these four FPRs,
because they are four representative FPRs to evaluate a method of malware
detection in practice. Meanwhile, we emphasis the most outperformed results of
our models compared with baseline models, i.e., 99.88 % TPF at 0.1 % FPR, on
average. In most cases, our model achieves better performance than two baseline
models.
Table 2. Performance under diﬀerent models of determining integrity level
True Positive Rate (TPR)
Average
U4
0%
U5
0%
U6
0%
U8
0%
TPR
0%
U7
0%
0%
Model
FPR
U1
0%
U2
0%
U3
0%
B1
B2
Our
92.09% 83.59% 70.30% 76.08% 53.35% 79.79% 64.10% 43.10%
96.75% 91.75% 92.55% 88.50% 86.18% 90.31% 90.98% 76.89%
98.09% 96.64% 95.18% 92.52% 91.85% 91.54% 94.19% 90.08%
99.52% 82.34% 97.58% 98.22% 88.16% 99.40%
99.73%
90.02% 99.27% 99.85% 92.92% 99.53% 70.21% 99.73%
100%
99.57% 99.66% 95.32% 99.73%
99.60% 99.86%
100%
99.86% 99.66% 99.16% 99.86%
100%
99.93%
100%
99.45% 99.49% 99.51% 97.15% 98.30% 99.71%
98.31%
0%
70.30%
0.1%
89.24%
0.5%
93.76%
1.0%
83.11%
0%
93.94%
0.1%
99.21%
0.5%
99.80%
1.0%
87.05%
0%
0.1% 99.98% 99.98% 99.95% 99.89% 99.87% 99.94% 99.61% 99.85% 99.88%
99.98% 99.98% 99.97% 99.98% 99.89% 99.93%
0.5%
99.97%
99.98% 99.99% 99.99% 99.99% 99.94% 99.97% 99.98%
1.0%
100%
100%
99.99%
100%
100%
100%
4.48%
To further compare the performance of the three models, we conduct a
Wilcoxon rank-sum test to evaluate whether one model signiﬁcantly outperforms
the other in terms of AUC. The Wilcoxon signed-rank test is a non-parametric
statistical hypothesis test used when comparing two related samples to assess
whether their population mean ranks diﬀer. Table 3 illustrates the test statistic
and its signiﬁcance between each pair of models. A negative value of the test
statistic indicates that the ﬁrst model performs worse than the second model
shown at the beginning of the row.
172
W. Mao et al.
Table 3. Results of Wilcoxon rank-sum tests on AUCs of diﬀerent models
Models
U1
U2
U3
U4