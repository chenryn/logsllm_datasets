title:Latency inflation with MPLS-based traffic engineering
author:Abhinav Pathak and
Ming Zhang and
Y. Charlie Hu and
Ratul Mahajan and
David A. Maltz
Latency Inﬂation with MPLS-based Trafﬁc Engineering
Abhinav Pathak
Purdue University
PI:EMAIL
Ming Zhang
Microsoft Research
PI:EMAIL
Y. Charlie Hu
Purdue University
PI:EMAIL
Ratul Mahajan
Microsoft Research
PI:EMAIL
ABSTRACT
While MPLS has been extensively deployed in recent years, lit-
tle is known about its behavior in practice. We examine the per-
formance of MPLS in Microsoft’s online service network (MSN),
a well-provisioned multi-continent production network connecting
tens of data centers. Using detailed traces collected over a 2-month
period, we ﬁnd that many paths experience signiﬁcantly inﬂated la-
tencies. We correlate occurrences of latency inﬂation with routers,
links, and DC-pairs. This analysis sheds light on the causes of
latency inﬂation and suggests several avenues for alleviating the
problem.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Performance Attributes
General Terms
Measurement, Performance
Keywords
MPLS, LSP, Autobandwidth, Latency
1.
INTRODUCTION
Trafﬁc engineering (TE) is the process of deciding how trafﬁc
is routed through the service provider network. Its goal is to ac-
commodate the given trafﬁc matrix (from ingress to egress routers)
while optimizing for performance objectives of low latency and
loss rate. Effective TE mechanisms are key to efﬁciently using net-
work resources and maintaining good performance for trafﬁc.
The importance of TE has motivated the development of many
schemes (e.g., [5, 7]), but little is known today about the effective-
ness or behavior of schemes that have been deployed in practice.
Much of the prior work is based on various forms of simulations
and emulations rather than based on real measurements taken from
an operational network.
In this paper, we present a case study of the behavior of TE as
deployed in a large network. This network (MSN) is the one that
connects Microsoft’s data centers to each other and to peering ISPs.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
Dave Maltz
Microsoft Research
PI:EMAIL
(
)
s
m
D
W
O
 100
 80
 60
 40
 20
 0
Oct 15
Nov 1
Timeline
Nov 15
Figure 1: Tunnel latency during one month period
MSN uses MPLS-based TE, which is perhaps the most widely used
TE mechanism and is supported by major router vendors such as
Cisco and Juniper. Although we use MSN as a case study, we be-
lieve that our ﬁndings are general because the behaviors we un-
cover are tied to the MPLS-TE algorithms themselves rather than
to MSN. The contribution of our work lies in uncovering and quan-
tifying problems with MPLS-TE in a production network, as a ﬁrst
step towards improving the TE algorithms.
Our interest in studying the behavior of MPLS-TE was not purely
academic but was motivated by anomalous behavior observed by
the operators of Bing Search which uses the MSN network. Dur-
ing the period of our study, Bing Search experienced incidents of
unexpectedly high latencies between two of its DCs from time to
time. Because these two DCs are in the same city, Bing’s opera-
tions expected the latency between them to be negligible. In fact,
this assumption is also made by Bing’s planning team when they
chose to distribute Bing’s backend services across the two DCs.
Figure 1 plots the latency of a tunnel (also known as LSP for
label switched path) between the two DCs during a month period.
We describe how MPLS-TE works in more detail later; but brieﬂy,
it works at the granularity of tunnels between ingress and egress
routers. There can be multiple tunnels between a pair of routers.
MPLS-TE uses a greedy algorithm that periodically ﬁnds the short-
est path that can accommodate the tunnel’s estimated trafﬁc de-
mand. The ﬁgure shows that the tunnel latency switched frequently
between 5 and 75 ms. It stayed at 75 ms for almost half of the time,
which adversely impacted Bing’s backend services.
Our systematic evaluation using data from a 2-month period in
2010 reveals that it is not these two DCs that are impacted. 22%
of the DC pairs experience signiﬁcant latency spikes. 20% of the
tunnels exhibit more than 20 ms of latency spikes. Over 5% of the
tunnels experience high latency inﬂation for a cumulative duration
of over 10 days in the 2 months.
463DC pair is provided with multiple LSPs in either direction to lever-
age path diversity in the underlying physical network. Trafﬁc be-
tween the same DC pair can be split among different LSPs either
equally or unequally.
An LSP has several attributes such as the current path, allocated
bandwidth, priority, etc.. There are two types of LSP: static and
dynamic. The former is allocated a static bandwidth and path at
setup stage which remains the same thereafter. The latter continu-
ally monitors the trafﬁc rate ﬂowing through the tunnel and adapts
its allocated bandwidth accordingly. It may also switch path when
there are changes in its own allocated bandwidth or the available
bandwidth in the network.
2.2 MPLS-TE algorithms
An LSP path is either conﬁgured manually or computed using
Constrained Shortest Path First Algorithm (CSPF). After a path is
selected, the LSP reserves the required bandwidth at the outgoing
interface of each router along the path. Each router outgoing in-
terface maintains a counter for its current reservable bandwidth.
The reservable bandwidth information along with network topol-
ogy (also called Trafﬁc Engineering Database (TED)) is periodi-
cally ﬂooded throughout the network.
Priority and preemption. Each LSP is conﬁgured with two pri-
ority values: setup priority and hold priority. Setup priority de-
termines whether a new LSP can be established by preempting an
existing LSP. Hold priority determines to what extent an existing
LSP can keep its reservation. A new LSP with high setup priority
can preempt an existing LSP with low hold priority if: (a) there is
insufﬁcient reservable bandwidth in the network; and (b) the new
LSP cannot be setup unless the existing LSP is torn down.
CSPF. CSPF sorts LSPs based on their priority and uses a greedy
algorithm to select the shortest path for each LSP. Starting with the
highest priority LSP, it prunes the TED to remove links that do not
have sufﬁcient reservable bandwidth or do not satisfy a preconﬁg-
ured access control policy. It then assigns the shortest path in the
pruned TED (using tie-breaking if multiple) to the LSP and up-
dates the reservable bandwidth on the relevant links. This process
continues until no LSP is left.
Re-optimization. CSPF is run periodically based on a conﬁgurable
timer to reassign each LSP a better path if possible.
Autobandwidth. MPLS does not have a bandwidth policing mech-
anism — an LSP may carry any trafﬁc demand irrespective of its
reserved bandwidth. Instead, router vendors (Cisco, Juniper) sup-
porting MPLS, provides autobandwidth which permits an LSP to
adjust its reserved bandwidth according to current trafﬁc demand.
To use autobandwidth, an LSP needs several additional parameters
(Table 1), including minimum/maximum bandwidth, adjustment
threshold, adjustment interval and sampling interval. Once every
sampling interval (e.g., 5 minutes), an LSP measures the average
trafﬁc demand ﬂowing through it. Once every adjustment interval
(e.g., 15 minutes), it computes the maximum of the average trafﬁc
demand measured in each sampling interval. If the maximum traf-
ﬁc demand differs from the current reserved bandwidth by more
than the adjust threshold and is within the minimum and maximum
bandwidth, the LSP will invoke CSPF with the maximum trafﬁc
demand as the new reserved bandwidth.
3. EXPERIMENTAL METHODOLOGY
To study how MPLS-TE algorithms affect inter-DC trafﬁc, we
collected various types of data from October 15 to December 5
2010 from a portion of Microsoft’s intercontinental production net-
work (MSN ), one of the largest OSP networks today. This por-
tion of MSN comprises of several tens of DCs interconnected with
Figure 2: OSP network topology
To gain insight into the causes for latency inﬂation, we correlate
such occurrences with speciﬁc links, routers, and DC pairs. Our
analysis shows that 80% of latency inﬂation occur due to changes
in tunnel paths concentrated on 9% of the links, 30% of the routers,
and 3% of the active DC-pairs. This conﬁrms trafﬁc load changes
exceeding the capacity of a small set of links along the shortest
paths of tunnels as the primary culprit. MSN operators have since
added capacity along these paths to alleviate the problem.
But to understand the effectiveness of MPLS at using available
resources, we compare the latency with MPLS-TE to that with an
optimal strategy based on linear programming. We ﬁnd that the
weighted and the 99th percentile byte latency under MPLS-TE are
10%-22% and 35%-40% higher than that under optimal routing
strategy, respectively, suggesting there exists room for improve-
ment under MPLS-TE.
We identify several problems caused by sub-optimal setting of
MPLS parameters but leave as future work automatic parameter
setting and on-the-ﬂy LSP split as two methods to ﬁx the latency
inﬂation problem.
2. BACKGROUND
A service provider network is composed of multiple points-of-
presence (PoPs). Our work is in the context of an online service
provider (OSP), where these PoPs serve as data centers (DCs) for
hosting services as well as peering with neighboring transit ISPs
(Internet service provider). The distinction between an OSP and
ISP is not important for our work, though we note that the nature
of trafﬁc may be different in these two kinds of networks and there
is a higher premium placed on latency reduction in OSP networks.
Figure 2 illustrates the topology of a large OSP network. It com-
prises multiple DCs at different geographical locations to serve
users around the world. To save inter-DC bandwidth cost, these
DCs are often interconnected with dedicated or leased links, form-
ing a mesh-like topology.
2.1 MPLS-TE Basics
A growing number of OSPs and ISPs have adopted MPLS net-
works which offer more TE [2] ﬂexibility than the traditional IGPs
such as OSPF and IS-IS. The former allows trafﬁc to be arbitrarily
distributed and routed between a source and a destination while the
latter only allows trafﬁc to be evenly distributed and routed on the
shortest paths. Such restriction may cause IGP TE to be far from
optimal under certain circumstances.
LSP: Label Switched Path. An LSP is an one-way tunnel in
MPLS network over which data packets are routed. Packets are
forwarded using MPLS labels instead of IP addresses inside LSP
tunnels. The labels are inserted into packets according to local pol-
icy at ingress routers, which are later stripped by egress routers.
Unlike in IGP routing, an LSP tunnel does not have to follow the
shortest path from an ingress to egress. In an OSP network, each
464Table 1: Autobandwidth parameters
Description
Parameter
Subscription factor % of interface bandwidth that can be reserved
Adjust interval
Adjust threshold
Time interval to trigger autobandwidth
% of change in reserved bandwidth to trigger
autobandwidth
Bandwidth limits of an LSP
Priorities for determining LSP preemption
Min/max bw
Setup/hold priority
high-speed dedicated links with the core of network in US. All the
inter-DC trafﬁc is carried over 5K LSPs, each using autobandwidth,
with 1-32 LSPs between each pair of DC. The data contains net-
work topology and router and LSP conﬁgurations. For each LSP,
it also contains each path change event and trafﬁc volume in each
5-minute sampling interval.
Measuring LSP latency is a challenging task for two reasons (a)
LSPs are unidirectional; as a result a simple ping would return One-
Way Delay (OWD) latencies of two LSPs (the forward LSP and the
reverse direction LSP). Separating out the two latencies would re-
quire strict time synchronization between the probers across DCs
(b) Trafﬁc between a DC pair is load balanced using hashing algo-
rithms on all the LSPs (between 1 to 30) between the DCs. Hash
functions are based on IP/TCP or even application level headers.
As a result, to probe all the LSPs between a DC pair using simple
ping, we must have one prober covering all the possible IP ranges
allocated as well as a applications running in DCs.
Another way to measure LSP latency is to use LSP ping [6, 9].
However, because LSP ping is disabled in MSN , we choose to
estimate LSP latency based on the geographical locations of the
routers along an LSP path. Given an LSP, we calculate the great-
circle distance between each pair of intermediate routers and sum
it up to obtain the total geographical distance of the LSP. We then
dividing the total distance by the speed of light in ﬁber to obtain
the LSP latency. We veriﬁed for a few LSPs that the conversion
indeed estimates correct delay with minimal error. Note that LSP is
unidirectional, as a result, the latency measured in this mechanism
is One-Way-Delay (OWD) estimation.
4. LSP LATENCY INFLATION
In this section we ﬁrst describe the severity of the latency prob-
lem in an MPLS based network and then correlate latency inducing
LSP path changes with dc pairs, routers and links in the network.
4.1 How badly is latency inﬂated?
Prevalence of latency inﬂation To quantify how widespread la-
tency inﬂation is, we compute the difference between the minimum
and maximum latency for each LSP during the 50-day period. Fig-
ure 3 plots the CDF of latency difference of all LSPs. We observe
that a substantial number of LSPs encounter severe latency inﬂa-
tion. 20% (over 1K) of the LSPs experience latency inﬂation of
over 20 ms. Moreover, the latency of 10% (over 500) of the LSPs
is inﬂated by more than 40 ms! Because a single user request may
trigger many round trips of inter-DC communication, such latency
inﬂation could noticeably impair user-perceived performance.
To systematically measure the frequency and duration of latency
inﬂation, we deﬁne a latency spike as the contiguous period of time
during which the latency of an LSP is at least x ms and y% more
than the minimum latency observed for the LSP. These two condi-
tions capture the signiﬁcance of latency inﬂation in both absolute
and relative terms. As shown in Figure 4, a spike starts when both
conditions are met and ends when either condition becomes false.
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 0
 20
 40
 60
Milliseconds
 80
 100
Figure 3: LSP latency difference (max-min) CDF
Figure 4: Deﬁnition of spike(x, y)
Figure 5 and 6 plot the CDF of total number and cumulative
duration of latency spikes for each LSP. We observe that latency
inﬂation is quite common. Under the (20ms, 20%) spike threshold,
roughly 18% (over 900) of the LSPs experience at least one latency
spike during the 50-day period. For 10% (over 500) of the LSPs,
the cumulative duration of latency spikes is over 1 day. This prob-
lem becomes even more severe for the top 5% (250) of the LSPs
whose cumulative spike duration is more than 10 days! This indi-
cates persistent latency problem for the inter-DC trafﬁc carried by
those LSPs. Figure 5 and 6 also show similar curves under a more
aggressive spike threshold of (30ms, 30%), where the total num-
ber and duration of latency spikes are only slightly smaller. This
suggests the latency inﬂation experienced by many of the LSPs is
indeed quite signiﬁcant.
The trafﬁc in the core of MSN network consists of only the traf-
ﬁc generated by inter-DC communications. All the 5K LSPs in
the network use autobandwidth algorithms to manage paths and
reserve bandwidth. Inter-DC links in the core of network exhibit
over 99.9% of availability [4]. However, most LSPs exhibits long
cumulative durations in spikes in order of days(ﬁgure 6). This sug-
gests that severe LSPs spikes are caused by autobandwidth instead
of failures.
Comparison with optimal TE strategy Although we have shown
many LSPs encounter latency spikes frequently, so far it is un-
clear if those spikes are caused by insufﬁcient network capacity or
by inefﬁciency of MPLS-TE algorithms. To answer this question,
we compute the optimal TE strategy that minimizes the weighted
byte latency(
∀LSP bw) for all inter-DC trafﬁc.
Given the network topology and trafﬁc matrix, this can be formu-
lated as a multi-commodity ﬂow problem and solved using linear
programming (LP) [8, 3, 1]. Note that although it is relatively easy
to ﬁnd the optimal TE strategy ofﬂine, the problem is much harder
∀LSP lat. ∗ bw /
(cid:2)
(cid:2)
465F