invalid, e.g., due to a nested return statement in or below the
scope of the allocation. Our framework is extensible such that
new analyses passes can be integrated to search for additional
vulnerability classes (cf., Section VI).
III. K-MINER
In this section, we explain our threat model, introduce
the high-level design of K-Miner, and elaborate on challenges
to enable precise, inter-procedural static analysis of complex,
real-world kernels.
3
void *a = alloc();if (cond(a)) {  free(a);  return NULL;}return a;void *f() {}void *b = f();printf("%p\n",b);return 1;int main(void) {}123456789a = alloc();cond(a)free(a);return;b = f();printf("%p\n",b);return;bNULLallocnodeaalloc nodea = alloc();b = f();a) Program Codeb) Inter-procedural Control-Flow Graphc) Pointer Assignment Graphd) Value-Flow Graphp1p2A. Goals and assumptions
With K-Miner we aim to identify and report potential
memory-corruption bugs in the kernel’s user-space interface, so
that developers can ﬁx them before shipping code that includes
such vulnerabilities. Regarding potential malicious processes at
run time we make the following standard assumptions:
• The attacker has control over a user-space process and
can issue all system calls to attack the kernel through the
subverted process.
• The operating system is isolated from user processes,
e.g., through virtual memory and different privilege levels.
Common platforms like x86 and ARM meet this require-
ment.
• An adversary cannot insert malicious code into the kernel
through modules, because modern operating systems re-
quire kernel modules to be cryptographically signed [45],
[48], [3].
• K-Miner should reliably report memory-corruption vul-
nerabilities that can be triggered by a malicious process.
Our assumptions force the attacker to exploit a memory-
corruption vulnerability in the kernel code to gain kernel
privileges through a purely software-based attack. The goal
of K-Miner is to systematically scan the system call interface
for these vulnerabilities.
Since real-world adversaries are not limited to software
vulnerabilities, it is important to note that even with a com-
pletely veriﬁed kernel (e.g., seL4) hardware attacks such as
rowhammer [42], [55] still pose a serious threat to the integrity
of the kernel. However, for our work we consider hardware
implementation defects to be an orthogonal problem [7].
B. Overview
K-Miner is a static analysis framework for commodity
operating system kernels. We provide a high-level overview
in Figure 2.
Our framework builds on top of the existing compiler suite
LLVM. The compiler (cf., step 1 ) receives two inputs. First,
a conﬁguration ﬁle, which contains a list of selected kernel
features. This conﬁguration ﬁle enables the user to select or
deselect individual kernel features. When a feature is disabled,
its code is not included in the implementation. Hence, an
analysis result is only valid for a speciﬁc pair of kernel code
and conﬁguration ﬁle. Second, the compiler suite parses the
kernel code according to the conﬁguration. It syntactically
checks the code and builds an abstract syntax tree (AST). The
compiler then internally transforms the AST into a so-called
intermediate representation (IR), which essentially represents
an abstract, hypothetical machine model. The IR is also used
for analyzing and optimizing kernel code through a series of
transformation passes.
In step 2 , the compiler suite passes the IR of the kernel
as an input to K-Miner, which starts to statically check the
code by going through the list of all system calls. For every
system call, K-Miner generates a call graph (CG), a value-
ﬂow graph (VFG), a pointer-analysis graph (PAG), and several
other internal data structures by taking the entry point of
the system call function as a starting point. Additionally, we
compute a list of all globally allocated kernel objects, which
Figure 2: Overview of the different components of K-Miner.
are reachable by any single system call. Once these data
structures are generated, K-Miner can start the actual static
analysis passes. There are individual passes for different types
of vulnerabilities, e.g., dangling-pointer, use-after-free, double-
free, and double-lock errors. All of these passes analyze the
control ﬂow of a speciﬁc system call at a time, utilizing the
previously generated data structures. The passes are imple-
mented as context-sensitive value-ﬂow analyses: they track
inter-procedural context information by taking the control ﬂow
of the given system call into account and descend in the call
graph.
If a potential memory-corruption bug has been detected, K-
Miner generates a report, containing all relevant information
(the affected kernel version, conﬁguration ﬁle, system call,
program path, and object) in step 3 .
C. Uncovering Memory Corruption
The goal of K-Miner is to systematically scan the kernel’s
interface for different classes of memory-corruption vulnera-
bilities using multiple analysis passes, each tailored to ﬁnd
a speciﬁc class of vulnerability. The individual analysis pass
utilizes data structures related to the targeted vulnerability
class to check if certain conditions hold true. Reasoning about
memory and pointers is essential for analyzing the behavior of
the kernel with respect to memory-corruption vulnerabilities,
hence, the data base for all memory objects (called global
context) and the pointer-analysis graph represent the foun-
dation for many analysis passes. Individual memory objects
are instantiated at allocation sites throughout the entire kernel
and the variables potentially pointing to them are tracked per
4
KernelCodeCompilerFrontendConfig➀K-MinerValue Flow AnalysisSyscall Analysissys_call_xyz:A) Call-GraphB) Control-Flow GraphC) Pointer AnalysisD) Allocation SitesContext Trackingglobal_xglobal_y➁sys_call_xyz :  possible use-after-return within global-y                        in path do_xyz > __do_xyz > _helper_funMemory-Corruption Report➂IntermediateRepresentationsystem call using the PAG. Forward analysis then reasons
about the past behaviour of an individual memory location,
whereas a backward analysis determines future behaviour
(since a forward analysis processes past code constructs before
processing future code and vice versa).
We can also combine such analysis passes, for instance,
to ﬁnd double-free vulnerabilities: ﬁrst, we determine sources
and sinks for memory objects, i.e., allocation sites and the
corresponding free functions respectively. We then process
the VFG in the forward direction for every allocation site to
determine reachable sinks. Second, we reconstruct the resulting
paths for source-sink pairs in the execution by following sinks
in the backward direction. Finally, we analyze the forward
paths again to check for additional sinks. Since any path
containing more than one sink will report a duplicate de-
allocation this approach suffers from a high number of false
positives. For this reason, we determine if the ﬁrst de-allocation
invocation dominates (i.e., is executed in every path leading to)
the second de-allocation invocation in the validation phase.
In similar vein we provide passes that are checking for
conditions indicating dangling pointers, use-after-free, and
double-lock errors. We provide more detailed examples for
the implementation of such passes in Section IV.
D. Challenges
Creating a static analysis framework for real-world operat-
ing systems comes with a series of difﬁcult challenges, which
we brieﬂy describe in this section. In Section IV we explain
how to tackle each challenge in detail.
Global state.
Most classes of memory-corruption vulnerabilities deal with
pointers, and the state or type of the objects in memory that
they point to. Conducting inter-procedural pointer analyses
poses a difﬁcult challenge regarding efﬁciency. Because inter-
procedural analysis allows for global state, local pointer ac-
cesses may have non-local effects due to aliasing. Since our
analyses are also ﬂow-sensitive, these aliasing relationships are
not always static, but can also be updated while traversing
the control-ﬂow graph. To enable complex global analyses,
we make use of sparse program representations: we only take
value ﬂows into account that relate to the currently analyzed
call graph and context information.
Huge codebase.
The current Linux kernel comprises more than 24 million
lines of code [14], supporting dozens of different architectures,
and hundreds of drivers for external hardware. Since K-Miner
leverages complex data-ﬂow analysis, creating data structures
and dependence graphs for such large amounts of program
code ultimately results in an explosion of resource require-
ments. We therefore need to provide techniques to reduce the
amount of code for individual analysis passes without omitting
any code, and allowing reuse of intermediate results. By
partitioning the kernel according to the system call interface,
we are able to achieve signiﬁcant reduction of the number of
analyzed paths, while taking all the code into account, and
allowing reuse of important data structures (such as the kernel
context).
False positives.
False positives represent a common problem of static analysis,
caused by too coarse-grained over approximation of possible
program behavior. Such over approximation results in a high
number of reports that cannot be handled by developers. K-
Miner has to minimize the number of false positives to an
absolute minimum. As the number of false positives depends
greatly on the implementation of the individual analysis passes
we carefully design our analyses to leverage as much infor-
mation as possible to eliminate reports that require impossible
cases at run time, or make too coarse-grained approximations.
Moreover, we sanitize, deduplicate, and ﬁlter generated reports
before displaying them for developers in a collaborative, web-
based user interface.
Multiple analyses.
A comprehensive framework needs to be able to eliminate all
possible causes of memory corruption. This is why K-Miner
must be able to combine the results of many different analyses.
Additionally, individual analyses may depend on intermediate
results of each other. Hence, our framework has to be able to
synchronize these with respect to the currently inspected code
parts. To this end we leverage the modern pass infrastructure
of LLVM to export intermediary results and partially re-import
them at a later point in time.
IV.
IMPLEMENTATION
Since it
In this section we describe our implementation of K-Miner,
and how we tackle the challenges mentioned in Section III-D.
Our framework builds on the compiler suite LLVM [46] and
the analysis framework SVF [59]. The former provides the ba-
sic underlying data structures, simple pointer analysis, a pass-
infrastructure, and a bitcode ﬁle format which associates the
source language with the LLVM intermediate representation
(IR). The latter comprises various additional pointer analyses
and a sparse representation of a value-ﬂow dependence graph.
is possible to compile the Linux kernel with
LLVM [69], we generate the required bitcode ﬁles by modi-
fying the build process of the kernel, and link them together
to generate a bitcode version of the kernel image. This image
ﬁle can then be used as input for K-Miner. Figure 3 depicts
the structure of our framework implementation. In particular,
it consists of four analysis stages: in step 1 , the LLVM-IR is
passed to K-Miner as a vmlinux bitcode image to start a pre-
analysis, which will initialize and populate the global kernel
context. In step 2 , this context information is used to analyze
individual system calls. It is possible to run multiple analysis
passes successively, i.e., our dangling pointer, use-after-free,
and double-free checkers, or run each of them independently.
In step 3 , bug reports are sanitized through various validation
techniques to reduce the number of false positives. In step 4 ,
the sorted reports are rendered using our vulnerability reporting
engine. In the following, we describe each of the steps in more
detail and explain how each of them tackles the challenges
identiﬁed in the previous section.
A. Global Analysis Context
The global context stored by K-Miner essentially represents
a data base for all the memory objects that are modeled based
5
Figure 3: Overview of the K-Miner implementation: we conduct complex data-ﬂow analysis of the Linux kernel in stages,
re-using intermediate results.
on the source code. Managing global context information efﬁ-
ciently is a prerequisite to enable analysis of highly complex
code bases such as the Linux kernel. Additionally, we have
to ensure that the context is sufﬁciently accurate to support
precise reporting in our subsequent analysis. This is why the
pre-analysis steps of our framework resemble the execution
model of the kernel to establish and track global kernel context
information.
Initializing the Kernel Context: The kernel usually ini-
tializes its memory context at run time by populating global
data structures, such as the list of tasks or virtual memory
regions during early boot phase. This is done by calling a
series of speciﬁc functions, called Initcalls. These are one-
time functions which are annotated with a macro in the
source ﬁles of the kernel. The macro signals the compiler to
place these functions in a dedicated code segment. Functions
in this segment will only be executed during boot or if a
driver is loaded. Hence, most of the memory occupied by this
segment can be freed once the machine ﬁnished booting [67].
To initialize the global kernel context, we populate global
kernel variables by simulating the execution of these initcalls
prior to launching the analyses for each system call. The
resulting context information is in the order of several hundred
megabytes, therefore, we export it to a ﬁle on disk and re-
import it at a later stage when running individual data-ﬂow
analysis passes.
Tracking Heap Allocations: Usually, user space pro-
grams use some variant of malloc for allocating memory
dynamically at run time. There are many different methods
for allocating memory dynamically in the kernel, e.g., a slab
allocator, a low-level page-based allocator, or various object
caches. To enable tracking of dynamic memory objects, we
have to compile a list of allocation functions which should be
treated as heap allocations. Using this list K-Miner transforms
the analyzed bitcode by marking all call sites of these functions
as sources of heap memory. In this way kernel memory
allocations can be tracked within subsequent data-ﬂow analysis
passes.
dedicated memory context for each of them. We do this by
collecting the uses of any global variables and functions in
each of the system call graphs. By cross-checking this context
information against the global context, we can establish an
accurate description of the memory context statically.
B. Analyzing Kernel Code Per System Call
Although analyzing individual system calls already reduces
the amount of relevant code signiﬁcantly, the resource require-
ments were still unpractical and we could not collect any
data-ﬂow analysis results in our preliminary experiments. For
instance, conducting a simple pointer analysis based on this
approach already caused our server system to quickly run out
of memory (i.e., using more than 32G of RAM). Through
careful analysis we found that one of the main causes for the
blow-up are function pointers: in particular, the naive approach
considers all global variables and functions to be reachable by
any system call. While this approximation is certainly safe, it
is also inefﬁcient. We use several techniques to improve over
this naive approach, which we describe in the following.
Improving Call Graph Accuracy: We start with a simple
call-graph analysis, which over-approximates the potential list
of target functions. By analyzing the IR of all functions in
the call graph we determine if a function pointer is reachable
(e.g., by being accessed by a local variable). This allows us to
collect possible target functions to improve the precision of the
initial call graph. Based on this list, we perform a two-staged
pointer analysis in the next step.
Flow-sensitive Pointer-Analysis: To generate the im-
proved call graph we ﬁrst perform a simple inclusion-based
pointer analysis to resolve the constraints of the function
pointers collected earlier. To further improve the precision,
we conduct a second pointer analysis while also taking the
control ﬂow into account. This again minimizes the number
of relevant symbols and yields a very accurate context for
individual system calls. We store these ﬁndings as intermediate
results per system call which can be used by subsequent data-
ﬂow analysis passes.
Establishing a Syscall Context: Because subsequent anal-
ysis passes will be running per system call, we establish a
Intersecting Global Kernel State: Finally, we combine
the previously indentiﬁed context information for a system call
6