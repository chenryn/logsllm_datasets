title:Exploiting Execution Context for the Detection of Anomalous System
Calls
author:Darren Mutz and
William K. Robertson and
Giovanni Vigna and
Richard A. Kemmerer
Exploiting Execution Context
for the Detection of Anomalous System Calls
Darren Mutz, William Robertson, Giovanni Vigna, and Richard Kemmerer
Computer Security Group
Department of Computer Science
University of California, Santa Barbara
{dhm,wkr,vigna,kemm}@cs.ucsb.edu
Abstract. Attacks against privileged applications can be detected by
analyzing the stream of system calls issued during process execution.
In the last few years, several approaches have been proposed to detect
anomalous system calls. These approaches are mostly based on modeling
acceptable system call sequences. Unfortunately, the techniques proposed
so far are either vulnerable to certain evasion attacks or are too expensive
to be practical. This paper presents a novel approach to the analysis of
system calls that uses a composition of dynamic analysis and learning
techniques to characterize anomalous system call invocations in terms
of both the invocation context and the parameters passed to the system
calls. Our technique provides a more precise detection model with respect
to solutions proposed previously, and, in addition, it is able to detect
data modiﬁcation attacks, which cannot be detected using only system
call sequence analysis.
Keywords: Intrusion Detection, System Call Argument Analysis, Exe-
cution Context.
1 Introduction
A recent thrust of intrusion detection research has considered model-based de-
tection of attacks at the application level. Model-based systems operate by com-
paring the observed behavior of an application to models of normal behavior,
which may be derived automatically via static analysis [8,23] or learned by ana-
lyzing the run-time behavior of applications [3,5,12,18,15]. In each case, attacks
are detected when observed behavior diverges in some respect from the normal
behavior captured by the model. In contrast to misuse-based approaches, where
the analysis identiﬁes attacks against applications using patterns of known ma-
licious actions, model-based schemes have the advantage of being able to detect
novel attacks, since attacks are not explicitly represented by the system. We
note that this advantage typically comes at the cost of performance, precision,
and explanatory capability, three properties that misuse-based approaches often
achieve very well.
Most model-based intrusion detection systems monitor the sequence of sys-
tem calls issued by an application, possibly taking into account some execution
state. For example, the system described in [3] monitors pairs of system calls
C. Kruegel, R. Lippmann, and A. Clark (Eds.): RAID 2007, LNCS 4637, pp. 1–20, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007
2
D. Mutz et al.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
void w r i t e _ u s e r _ d a t a( void )
{
FILE * fp ;
char u s e r _ f i l e n a m e [256];
char u s e r _ d a t a [256];
gets ( u s e r _ f i l e n a m e);
if ( p r i v i l e g e d _ f i l e( u s e r _ f i l e n a m e )) {
fprintf ( stderr , " Illegal (cid:2) filename .(cid:2) Exiting .\ n " );
exit (1);
}
else {
gets ( u s e r _ d a t a );
// o v e r f l o w i n t o u s e r _ f i l e n a m e
fp = fopen ( user_filename , " w " );
if ( fp ) {
fprintf ( fp , " % s " , u s e r _ d a t a);
fclose ( fp );
}
}
}
Fig. 1. Sample data modiﬁcation attack
and records the application’s stack conﬁguration (that is, part of the history
of function invocations). During the detection phase, the system checks if the
observed pairs of system calls (and their associated stack conﬁguration) match
pairs recorded during the learning period. The systems described in [8] and [23]
check call sequences against automata-based models derived from the applica-
tion’s source code or binary representation, and identify sequences that could
not have been generated by the model.
Some of the shortcomings of sequence-based approaches were discussed in [2],
where the problems of incomplete sensitivity and incomplete sets of events were
introduced. Incomplete sensitivity aﬀects models derived from static analysis.
Due to the limitations of static analysis techniques, these models may accept
impossible sequences of system calls (for example because branch predicates are
not considered).
The problem of incomplete sets of events is more general, and it aﬀects all
approaches based on system call sequences. This problem stems from the fact
that, in these systems, the manifestation of an attack must be characterized in
terms of anomalies in the order in which system calls are executed. Changes in
the ordering of system call invocations occur, for example, because foreign code
is injected into the application (such as through a buﬀer overﬂow) or because
the order in which instructions are executed is modiﬁed. Therefore, by modeling
system call sequences, these approaches implicitly restrict themselves to only
detecting attacks that modify the execution order as expressed by the appli-
cation’s code or by the execution histories observed during a training period.
Unfortunately, an attacker can successfully compromise an application’s goals
by modifying the application’s data without introducing anomalous paths in the
application’s execution.
Consider, for example, the procedure write user data in Figure 1. Here, an
overﬂow of the variable user data at line 14 allows an attacker to overwrite the
value contained in user filename, which the application assumes was checked
Exploiting Execution Context for the Detection of Anomalous System Calls
3
by the procedure invoked at line 9. Therefore, the attacker can leverage the
overﬂow to append data of her choice to any ﬁle the application has access to.
Note that the execution of this data modiﬁcation attack does not aﬀect the type
or ordering of the system calls issued by the application.
To detect data modiﬁcation attacks, models must include some representa-
tion of valid or normal program state. For example, prior work in [11] and [12]
uses learning models to characterize “normal” system call argument values and
to demonstrate that changes to program state as a result of an attack often
manifest themselves as changes to the argument values of system calls. The as-
sumption underlying this approach is that the goal of the attacker is to leverage
the privileges of an application to change some security-relevant state in the un-
derlying system (e.g., write chosen values to a ﬁle, execute a speciﬁc application,
or change the permissions of a security-critical ﬁle). This type of activity may
be readily observed as suspicious system call argument values.
One limitation of the argument modeling approach in [11], [12], and [15] is that
models of normal argument values are built for each system call. That is, one set of
models is created for open, another set for execve, and so on. As a result, a model
captures the full range of argument values observed during all phases of the exe-
cution of an application. A better approach would be to train the models in a way
that is speciﬁc to individual phases of a program’s execution. For example, the
arguments used during a program’s initialization phase are likely to diﬀer from
those used during a production phase or termination phase. This can be achieved
by diﬀerentiating program behavior using the calling context of a procedure – that
is, the conﬁguration of the application’s call stack when a procedure is invoked.
Similar techniques have been explored in the programming languages literature.
Examples include improving proﬁling by considering a procedure’s calling con-
text [1], analyzing pointer variables more accurately [9], and improving lifetime
predictions of dynamically allocated memory [16]. A common observation in these
approaches is that the calling context of a procedure is often a powerful predictor
of how the procedure and its data interact.
In this paper, we ﬁrst propose and evaluate a metric for determining to what
extent argument values are unique to a particular call stack for a given appli-
cation. Our study, presented in Section 2, shows that this is predominantly the
case, indicating that the argument modeling approach of [12] can be made more
precise if models are built for each calling context in which a system call is issued
by an application. Armed with this knowledge, we then introduce and evaluate a
model-based detection system that builds separate argument models for each call
stack in which an application issues a system call. Our experiments demonstrate
that the trained models eﬀectively generalize from the training data, performing
well during a subsequent detection period.
This paper makes the following primary contributions:
– It analyzes the relationship between system call arguments and diﬀerent
calling contexts, and it introduces a novel metric to quantify the degree to
which argument values exhibit uniqueness across contexts.
– It demonstrates that the application’s call stack can be leveraged to add
context to the argument values that appear at the system call interface. It
4
D. Mutz et al.
also demonstrates that the increased sensitivity of context-speciﬁc argument
models results in better detection performance.
– It deﬁnes a technique to detect data modiﬁcation attacks, which are not
detected by previously proposed approaches based on system call sequences.
– It presents an extensive real-world evaluation encompassing over 44 million
system call invocations collected over 64 days from 10 hosts.
The remainder of the paper is structured as follows. In Section 2 we introduce
and apply a metric to characterize the degree to which system call argument
values are unique to calling contexts in which system calls are issued. Then, in
Section 3, we present our detection approach, which builds argument models that
are speciﬁc to each calling context. Section 4 reports the results of evaluating the
system empirically. Section 5 covers related work on system call-based anomaly
detection. Finally, Section 6 draws conclusions and outlines future work.
2 System Call Argument and Calling Context Analysis
1 , . . . , Asi
n
The eﬀectiveness of system call analysis that includes call stack information is
directly related to the number of contexts in which a given argument value asso-
ciated with the invocations of a particular system call occurs. More speciﬁcally, if
argument values appear in many contexts, essentially randomly, context-speciﬁc
learning models are likely to oﬀer no beneﬁt. Furthermore, if each observed ar-
gument value appears (possibly multiple times) in only one context, we would
expect system call argument analysis that includes call stack information to
outperform context-insensitive models. In this section, we propose a metric to
express the degree of context-uniqueness of argument values. We then use this
metric to determine which applications are likely to be amenable to system call
analysis that takes into account stack-speciﬁc behavior.
Before introducing our context-uniqueness metric, we need to deﬁne some
notation. Let S = {s1, s2, . . .} be the set of monitored system calls, and let Asi =
(cid:2)Asi
(cid:3) be the vector of formal arguments for system call si. Consistent
with [6], we deﬁne the calling context of a system call invocation as the sequence
of return addresses C = (cid:2)r1, . . . , rl(cid:3) stored on the application’s call stack at the
time the system call invocation occurs. Each invocation sij of si has a concrete
vector of values for Asi deﬁned as asij = (cid:2)asij
n (cid:3), and two argument
vectors asij and asij(cid:2) are considered distinct if any of their subvalues asij
and
sij(cid:2)
a
l
We are interested in the set of argument vectors appearing in the invocation
of a system call in a particular context. For this, we introduce the notion of
an argument set. An argument set for a system call si in a context C is the
set of all argument vectors asij observed for the chosen system call when it is
issued in the calling context C. This is denoted by AS(C, si). The argument set
for si across the entire application (i.e., ignoring the calling context) is denoted
by AS(∗, si). We observe that if the set AS(∗, si) is partitioned by the subsets
{AS(C1, si), AS(C2, si), . . .}, then each recorded argument vector asi occurs in
only one calling context.
1 , . . . , asij
diﬀer.
l
Exploiting Execution Context for the Detection of Anomalous System Calls
5
One potential route in the development of this metric would be to adapt clus-
ter quality measures from the machine learning literature. Unfortunately, com-
puting the distance between two argument vectors asij and asij(cid:3) is problematic.
For example, integer arguments that exhibit numeric similarity are often dissim-
ilar in their semantic meaning. This occurs in cases where an integer argument is
the logical OR of a collection of boolean ﬂags. Computing string similarity also
presents diﬃculties. For example, two ﬁlesystem paths may have large common
substrings or a small Hamming distance, but correspond to ﬁles that have a
very diﬀerent meaning to the users of the system. For these reasons, we build
our metric using argument vector equality only.
With this in mind, we would like to determine the number of contexts where
each distinct argument vector is used. To measure this we deﬁne the actual
partitioning value AP (si), which is the sum over all recorded concrete argument
vectors of the number of argument sets where each asij appears during the period
of monitoring. That is,
AP (si) =
K(cid:2)
L(cid:2)
j=1
m=1
| {asij} ∩ AS(Cm, si) |
(1)
where K is the number of distinct argument vector values recorded, and L is the
number of distinct stack conﬁgurations observed during the monitoring period.
For our context-uniqueness metric, we would like to compare the actual par-
titioning value to both the optimal partitioning and the worst case partitioning
values. For the optimal case, each argument vector should appear in as few con-
texts as possible. There are two cases to consider. In the case where the number
of distinct argument vectors is greater or equal to the number of calling con-
texts (K ≥ L), each argument value appears in only one context in the optimal
partition of AS(∗, si). For the case when K < L, some argument vectors must
appear in more than one context1. The optimal partitioning, in this case, is for
each concrete argument vector to appear in L/K argument sets. Both cases can
be expressed by specifying the number of argument sets where each argument
vector is to appear as max(L/K, 1).
We can now deﬁne the optimal partitioning value and the worst case parti-
tioning value. Since there are K distinct argument vector values, the optimal
partitioning value OP (si) is deﬁned as:
OP (si) = K ∗ max(L/K, 1) = max(L, K)
(2)
To deﬁne the worst case, we need to know how many instances of each of the K
distinct argument vectors asij ∈ AS(∗, si) were recorded during the monitoring
period. We deﬁne the counter cntasij as the number of times that a partic-
ular argument vector asij occurs in the recorded invocations. The worst case
partitioning is determined by distributing each of the K argument vectors in
AS(∗, si) over as many contexts as possible. Although asij can appear a maxi-
mum of cntasij times, there are only L distinct contexts. Therefore, asij appears
1 If each distinct value appeared in only one context, then there would be contexts