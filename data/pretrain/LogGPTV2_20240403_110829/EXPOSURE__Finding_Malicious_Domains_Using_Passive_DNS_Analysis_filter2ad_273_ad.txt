0.987
98.4%
False Positives
0.3%
0.9%
1.1%
Figure 3: Classiﬁcation accuracy. (AUC=Area Under the ROC Curve)
wareurls.com, a malware domains list that we had not used
as a source for the initial malicious domains training set.
During the period we performed our experiments, mal-
wareurls.com reported 569 domains as being malicious.
Out of these 569 domains, 216 domains were queried by
the infected machines in the networks that we were mon-
itoring. The remaining 363 malware domains were not re-
quested. Therefore, in our detection rate evaluation, we take
into account only the 216 requested domains.
5 of the 216 domains were queried less than 20 times dur-
ing entire monitoring period. Since we ﬁlter out domains
that are requested less than 20 times, we only fed the re-
maining 211 domains to our system. In the experiments,
all of these domains (that were previously unknown to us)
were automatically detected as being malicious by EXPO-
SURE. Hence, the detection rate we observed was similar
to the detection rate (i.e. 98%) estimated by the percentage
split and cross-validation evaluations on the training set.
Obviously, our approach is not comprehensive and can-
not detect all malicious domains on the Internet. However,
its ability to detect a high number of unknown malicious
domains from DNS trafﬁc is a signiﬁcant improvement over
previous work.
5.3.2 Evaluation of the False Positives
As the domains in our data set are not labeled, determin-
ing the real false positive rate is a challenge. Unfortunately,
manually checking all 17,686 domains that were identiﬁed
as being malicious is not feasible. This is because it is dif-
ﬁcult, in practice, to determine with certainty (in a limited
amount of time) that a domain that is engaged in suspicious
behavior is indeed malicious. Nevertheless, we conducted
three experiments to make estimates about the false posi-
tives of our detection.
In order to obtain more information about the domains
in our list, we ﬁrst tried to automatically categorize them
into different groups. For each domain, we started Google
searches, checked well-known spamlists, and fed the do-
mains into Norton Safe Web (i.e., Symantec provided us in-
ternal access to the information they were collecting about
web pages). We divided the domains into ten groups:
spam domains (Spam), black-listed domains (BlackList),
malicious Fast-Flux domains (FastFlux), domains that are
queried by malware that are analyzed by malware analy-
sis tools (Malware), Conﬁcker domains (Conﬁcker), do-
mains that have adult content, domains that are suspected
to be risky by Norton Safe Web and McAfee Site Advi-
sor (Risky), phishing domains (Phishing), domains about
which we were not able to get any information either from
Google or from other sources (No Info), and ﬁnally, be-
nign domains that are detected to be malicious (False Posi-
tives) (See Table 2).
In the ﬁrst experiment, we manually investigated 50 ran-
dom malicious domains from our list of 17,686. We queried
Google, checked websites that discuss malicious networks,
and tried to identify web links that reported a malicious be-
havior by the domain. Among the 50 randomly chosen do-
mains, the classiﬁer detected three benign domains as being
malicious. All these domains had an abnormal TTL change
behavior.
In the second experiment, we automatically cross-
checked the malicious and suspicious domains that we had
identiﬁed with our classiﬁer using online site rating tools
such as McAfee Site Advisor, [8], Google Safe Brows-
ing [6] and Norton Safe Web [9]. The results show that the
false positive estimate is around 7.9% for the malicious do-
mains that we identiﬁed. Given that our classiﬁer is able to
identify malicious domains automatically, these false posi-
tive rates are acceptable for the tool to be deployed a large-
scale, early warning system.
Note that EXPOSURE did not generate any false posi-
tives during the two week real-time, real-world deployment
in an ISP as discussed in the next section.
5.4 Real-World, Real-Time Detection with EX-
POSURE
To test the feasibility and scalability of EXPOSURE as
a malicious domain detector in real-life, we deployed it in
the network of an ISP that provided us complete access to
its DNS servers for two weeks. These servers receive DNS
queries from a network that supports approximately 30,000
clients.
During the two-week experimental period, EXPO-
SURE analyzed and classiﬁed 100 million DNS queries.
No pre-ﬁltering was applied. At the end of two weeks, EX-
POSURE detected 3117 new malicious domains that were
previously not known to the system and had not been used
MW-Group
Spam
Black-List
FastFlux
Malware
Conﬁcker
Rand 50 Malicious
18
8
-
6
4
3691
1734
114
979
3693
MW-Group
Adult
Risky
Phishing
No Info
False Positives
Rand 50 Malicious
3
-
3
5
1716
788
0
2854
3 (6%)
1408 (7.9%)
Table 2: Tests for False Positives
in the training. 2821 of these domains fall into the category
of domains that are generated by a DGA and all belong to
the same malicious entity. 5 out of the remaining 396 do-
mains were reported as being malicious domains by security
companies such as Anvira, one month after we had detected
them.
We cross-checked the rest of the remaining domains we
had detected. All detected domains were classiﬁed as being
risky by McAfee Site Advisor [8].
Figures 5(a) and 5(b) show the number of new, previ-
ously unknown malicious domains detected every day. As
can be seen, after the initial seven days of local training in
the network being monitored, EXPOSURE started to pro-
duce daily detections and detected 200 new malicious do-
mains per day on average.
After the experiments, we provided the ISP with the list
of clients that were potentially infected, or had been victims
of scams.
The distinct number of IP addresses that queried the
malicious domains that EXPOSURE detected were 3451.
Since the ISP applies a dynamic IP assignment to its clients,
this number does not represent the exact number of infected
machines in the network. To estimate the number of in-
fected machines in the network, we grouped the malicious
domains according to the IP addresses they are mapped to.
There were 5 different groups of malicious domains. We
then calculated the average number of distinct IP addresses
that issued DNS queries to the domains in these 5 groups
every hour. We chose one hour as an interval by assum-
ing that the users in the network stay online at least an hour
before they disconnect. Table 3 lists the number of clients
that attempt to access the domains that fall into the differ-
ent malware groups. We estimate that there were about 800
machines on the network that issued the requests to the ma-
licious domains.
5.5 Comparison with Previous Work
5.5.1 The Fast-Flux Detectors
The results in Table 2 show that 114 of the malicious do-
mains that our classiﬁer has identiﬁed during the initial
training phase fall in to category of Fast-Flux Service Net-
works (FFSNs). Since we claim that our approach is able to
detect a wide range of malicious domains including FFSNs,
we compare our detection rate for this threat with the most
recent published work in this area (i.e., [30]).
Perdisci et. al. [30] ﬁlters out all of the domains that
are not likely to be classiﬁed as being FFSN. When we ap-
plied the same policy to our two and half month data set,
300,000 domains were ﬁltered out and 5,771 were left as
candidates for FFSNs. When we classiﬁed these domains
with the feature set Perdisci et. al. use in their paper, we
detected 114 FFSNs using their approach. Hence, our ap-
proach is as good in detecting FFNSs as Perdisci et. al.
although it is a much more generic system.
5.5.2 Notos: Reputation-based Malicious Domain De-
tection
Very recently, Antonakakis et al. [11] concurrently and in-
dependently proposed a detection scheme that is similar to
our work. The proposed system, Notos, dynamically as-
signs reputation scores to domain names whose malicious-
ness has not been discovered yet. A detection scheme is
built that is based on the premise that agile malicious uses
of DNS have unique characteristics. Hence, the claim is that
malicious use of DNS can be distinguished from benign use.
To be able to deﬁne these unique characteristics, the au-
thors analyze a number of features that are grouped into
three categories: Network-based features, zone-based fea-
tures (i.e. features that are extracted from the domain name
itself, either by string analysis or with the information ob-
tained from whois service) and evidence-based features.
While the network-based features are employed for
combing out the domains that do not exhibit ﬂuxy behav-
ior (i.e.
stable DNS usage), the zone-based features are
used for distinguishing between legitimate CDNs and the
domains that are likely to be malicious. After this two-layer
classiﬁcation, reputation scores are given to the domains.
In other words, all of the domains and the IP addresses they
are mapped to are compared with already known lists of do-
mains or IP addresses that host malicious entities. This third
step of classiﬁcation is done using evidence-base features.
Groups
DGA domains
Iksmas Worm
Worm:Win32/Slenping
Trojan-Generic.dx
Other
Total
Avg Life Time Most frequent life time
0.99 days
11.9 days
12.0 days
11.9 days
11.9 days
1.2 days
11.9 days
12.0 days
11.9 days
10.8 days
# of infected clients
49
70
253
70
391
833
Table 3: Information on the detected malicious domains
In their paper, as a limitation of Notos, the authors state
that Notos is not able to detect malicious domains that are
mapped to a new address space each time and never used
for other malicious purposes again. This limitation stems
from the fact that Notos strongly relies on network-based
features. EXPOSURE does not have this limitation as it
uses time-based features. Since such domains would have
a short life, they would appear in the time series and disap-
pear immediately after they are deactivated by the attacker.
Hence, unlike Notos, we are able to detect such domains.
As discussed before, the 2821 automatically generated
malicious domains that we detected in the real-life trafﬁc of
the ISP had an average lifespan of 1.2 days (see Table 3).
That is, all these domains were short-lived domains. Dur-
ing their life time, on average, they mapped to 3.14 distinct
IP addresses. In the ﬁrst phase of Notos’ detection scheme,
the domains are divided into two categories: domains that
have a stable network-model and domains that have a non-
stable network-model. Since the automatically generated
malicious domains that we are able to detect do not use a
wide range of IP addresses, Notos might classify these do-
mains as domains with a stable network proﬁle. In other
words, we believe that Notos might miss-classify them.
Also, as the authors discuss in their paper, because No-
tos is a reputation-based system, there may be cases where
legitimate domains that are hosted in “bad neighborhoods”
may be identiﬁed as being malicious. In comparison, the
features that EXPOSURE relies on do not cause such false
positives as no historical information on IPs or domains are
utilized.
One main advantage of EXPOSURE over Notos is that
Notos requires a large passive DNS collection and sufﬁcient
time to create an accurate, passive DNS database. First, it
is unclear how much time is required for this database to
be comprehensive. Second, this database needs to be con-
stantly updated with large data-feeds in order to remain ac-
curate and to have a wide overview of malicious activities
on the Internet. In comparison, as we show in our evalua-
tion, EXPOSURE only required a week of local training,
and much less DNS data in the network of a medium ISP to
be able to detect unknown domains.
6 Related Work
The Domain Name System (DNS) has been increasingly
being used by attackers to maintain and manage their mali-
cious infrastructures. As a result, recent research on botnet
detection has proposed number of approaches that leverage
the distinguishing features between malicious and benign
DNS usage.
The ﬁrst study [39] in this direction proposed to collect
real-world DNS data for analyzing malicious behavior. The
results of the passive DNS analysis showed that malicious
domains that are used in Fast-Flux networks exhibit behav-
ior that is different than benign domains. Similarly, Zdrnja
et al. [42] performed passive monitoring to identify DNS
anomalies. In their paper, although they discuss the pos-
sibility of distinguishing abnormal DNS behavior from be-
nign DNS behavior, the authors do not deﬁne DNS features
that can be used to do so.
In general, botnet detection through DNS analysis fol-
lows two lines of research: The ﬁrst line of research tries
to detect domains that are involved in malicious activities.
The goal is to identify infected hosts by monitoring the DNS
trafﬁc. The second line of research focuses on the behaviors
of groups of machines in order to determine if they are in-
fected (e.g., a collection of computers always contact the
same domain repeatedly).
6.1
Identifying Malicious Domains
To detect malicious domains, previous approaches make
use of passive DNS analysis, active DNS probing, and
WHOIS [2] information. For example, recent work by
Perdisci et al. [30] performs passive DNS analysis on re-
cursive DNS trafﬁc collected from number a number of
ISP networks with the aim of detecting malicious Fast-Flux
services. Contrary to the previous work [24, 28, 29, 36],
Perdisci’s work does not rely on analyzing blacklisted do-
mains, and domains that are extracted from spam mails. Our
work signiﬁcantly distinguishes itself from theirs as we are
able to detect all different kinds of malicious domains such
as phishing sites, spamming domains, dropzones, and bot-
The Time Domains Appear in the Time Series
 2
 4
 6
 8
 10
 12
 14
Days
(a)
Domain Detection Time
 650
 600
 550
 500
 450
 400
 350
 300
 250
 200
 150
 0
 1600