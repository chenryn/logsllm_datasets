niques from the distributed systems literature on primary-backup
systems to enforce the desired semantics. Atomic update ensures
that the server updates on the shared state are atomically reflected
on the switch. Output commit ensures that the packet causing the
updates is buffered on the server until the updates are reflected on
the switch. When used together, the techniques will guarantee the
desired run-to-completion semantics.
Atomic update: We now describe the implementation of the
atomic update in Gallium. Similar to journaling in file systems,
Gallium’s runtime system first puts all the modifications into a
dedicated switch memory and then uses a single (atomic) write
operation to make them visible to subsequent packets. For each
match table stored on the programmable switch, a smaller-sized
“write-back” table is created. Besides that, a single bit is also added to
the switch state, indicating whether the write-back table should be
used during table lookup. When the P4 program performs a lookup
to the table and observes that the bit is set to true, it first reads the
write-back table. If there is a matching entry, it will be used as the
result of the table lookup. Otherwise, the main match table will
be used. The middlebox server performs switch state updates in
three steps. First, the server uses the switch control plane API to
insert entries to the write-back tables. (A special value indicates
table entry deletion.) Then, the server flips the bit by performing an
additional control plane operation. This operation makes the entries
in the write-back tables visible to subsequent packets. Finally, the
middlebox server writes the updates to the main tables and toggles
the bit after all updates are performed.
290
Gallium: Automated Software Middlebox Offloading to Prog. Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Analysis: We now discuss the correctness and performance
implications of the scheme described above. The combined use of
transactional (or atomic) updates and the output commit protocols
ensure that the switch-server pair exhibits the same consistency
properties as a chain-replicated, primary-backup system. Switch
operations are restricted to read-only operations on replicated state,
just as with the tail of a chain-replicated system [27]. A packet
that performs updates to replicated state is only released after the
switch has performed the updates. Subsequent packets generated
after an end-host has received would see the updated state even
when processed on the switch.
A Gallium middlebox could reorder packets sent through it, and
this reordering could result in performance issues for TCP flows.
Consider, for instance, a packet that is forwarded to the server for
slow-path processing. If it were to make updates to the replicated
state, then it would be buffered until the updates are propagated to
the switch, and subsequent packets could be processed and trans-
mitted by the switch before the server releases the slow-path packet.
Fortunately, for most middleboxes, the slow-path processing is of-
ten invoked on a small number of packets or just control packets,
such as SYN, SYN-ACK, FIN, and RST packets, and this reduces the
occurrence of reordered data packets. It is also worth noting that
features providing dataplane management of the traffic manager,
expected in upcoming Tofino switches [25], can help eliminate
these reordering issues. With this hardware support, flows with
outstanding packets requiring slow-path processing can be buffered
on separate queues and then released by the packet transmitted by
the server. We leave the exploration of this mechanism for future
work.
5 Implementation
We implement Gallium using 5712 lines of C++. Gallium uses Clang
(version 6.00) to generate an LLVM Intermediate Representation
(LLVM IR) of the input program. All the functionalities described
in §4 are implemented as analysis passes on the LLVM IR as it has a
simpler syntax than C++. Also, because LLVM IR itself is in a Static
Single Assignment (SSA) form, it eases the tracking of when vari-
ables are assigned and used. We use the llvm-dev library to extract
a CFG of the input program. The generated P4 code is compiled and
deployed using the SDK provided by the Barefoot Tofino switch.
The non-offloaded partition (C++) is compiled and linked with the
DPDK library [1] and is deployed as a DPDK application.
As we mentioned in §4, Gallium has a set of annotations on
Click APIs in order to perform dependency extraction. We have
manually annotated the Click APIs to access data structures, in-
cluding Vector and HashMap, and the APIs to access packet headers.
Gallium models the read and write set for each LLVM instruction.
For ALU instructions, the read set of the instruction consists of
all the instruction operands, and the write set is simply the desti-
nation register. For memory access instructions—load/store—the
read/write set of the instruction is the data the pointer points to.
Gallium leverages LLVM IR’s type metadata to determine the type
and size of the dereferenced pointer.
6 Evaluation
We aim to answer the following questions in this section:
291
Middlebox
MazuNAT
Load Balancer
Firewall
Proxy
Trojan Detector
Input Output Output
(C++)
(C++)
579
1687
602
1447
1151
403
279
953
882
418
(P4)
516
522
506
292
571
Table 1: Comparison of lines of code for Click-based middle-
boxes before and after Gallium compiles them.
• Can Gallium enable automated software middlebox offload-
• How much performance benefits do the offloaded middle-
ing to programmable switches?
boxes provide?
6.1 Case Study
We use five Click-based middleboxes to evaluate Gallium: (1) Mazu-
NAT, (2) an L4 load balancer, (3) a firewall, (4) a transparent proxy,
and (5) a Trojan detector.
MazuNAT. MazuNAT is a NAT implementation used by Mazu
networks. At a high level, MazuNAT is a gateway middlebox that
separates two network spaces, an internal network and an external
network. For traffic going from the internal to the external network,
MazuNAT allocates a new port and rewrites the packet header,
and the flow itself appears to be sourced by MazuNAT. The port
allocation is performed using a monotonically increasing counter.
MazuNAT memorizes the mapping from addresses to ports for
existing connections and enforces the mapping for subsequent
packets of existing connections.
When MazuNAT receives a packet from the external network,
MazuNAT checks if there is a corresponding mapping created by
connections from the internal network. If not, MazuNAT drops the
packets from the external network. If a corresponding mapping is
found, MazuNAT rewrites the packet according to the mapping and
forwards it into the internal network to reach its destination.
L4 Load Balancer. The load balancer application is similar to
the MiniLB example show in §4. Similar to MiniLB, the load balancer
assigns incoming TCP and UDP traffic to a list of backend servers.
It uses the hash value of the five-tuple (i.e., source IP address/port,
destination IP address/port, transport protocol) to determine the
backend server to which the connection is assigned. It also uses a
map to keep track of the assigned flows to ensure that packets in
the same connection are steered to the same backend server. In ad-
dition to the functionalities in MiniLB, the L4 load balancer garbage-
collects finished connections by intercepting TCP control packets,
such as RST (reset) and FIN (finish). The L4 load balancer also has
a time-out mechanism: idle connections are garbage-collected after
5 minutes without the FIN packet.
Firewall. The firewall is adapted from an example middlebox in
the Click paper [21]. It filters packets using a whitelist mechanism.
Each entry specifies a five-tuple that is allowed to go through the
firewall. When a packet arrives, it is dropped if its five-tuple cannot
be found in the whitelist.
Transparent Proxy. The transparent proxy is also adapted from
an example in the Click paper [21]. The transparent proxy redirects
traffic to a web proxy based on the TCP destination port. The proxy
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Kaiyuan Zhang, Danyang Zhuo, and Arvind Krishnamurthy
(a) MazuNAT
(b) L4 Load Balancer
(c) Firewall
(d) Proxy
(e) Trojan Detector
Figure 7: Throughput comparison between Gallium middleboxes and their FastClick counterparts. Gallium middleboxes only
use a single core in the middlebox server. FastClick versions of the middleboxes use 1, 2, and 4 cores, respectively. Error bars
denote standard deviations.
internally keeps a list of TCP destination ports. Upon receiving a
packet, the proxy checks whether the TCP destination port is in
the list. If the destination port is in the list, instead of forwarding
the packet, the proxy rewrites the packet header to steer the packet
to a designated web proxy.
elements used by the firewall. The 403 lines of code in the non-
offloaded part are mainly for constructing and inserting the firewall
rules.
Trojan Detector. The Trojan detector [9] keeps track of TCP
connection states of each endhost. It identifies an endhost as a
Trojan if the following sequence of events is observed: (1) The
endhost first creates an SSH connection. (2) It then downloads a
HTML file from a web server, or a .zip or .exe file from a FTP
server. (3) Finally, it generates Internet Relay Chat (IRC) traffic.
6.2 What’s offloaded?
To evaluate how much middlebox functionality can be offloaded,
we examine the lines of code before and after the five middleboxes
are compiled by Gallium. Table 1 shows the result. Here, the total
lines of code do not include Click data structure implementations.
After compilation, the generated code (i.e., the combination of the
P4 and C++ code) has fewer instructions than the input program, as
P4 abstracts away several types of packet processing. For example,
the IPClassifier element, which performs generic packet classi-
fication, could be abstracted using a single match action table in
P4.
After compilation, MazuNAT’s address translation tables—the
maps that store the five-tuple rewriting rules for both internal and
external TCP flows—are offloaded to the programmable switch.
Besides that, the counter used for port allocation is also offloaded
to the switch as a P4 register. When new rewriting rules have to be
added to the address translation table, the pre-processing code will
pack the current counter value into the packet header and send it
to the middlebox server, where the table update is performed.
When applying Gallium to MazuNAT, we added the annotation
that the address translation mapping would not have more than
65536 (216) entries, since each port number can have at most one
entry in the map. This annotation allows Gallium to place the
address translation maps on the switch.
Similar to the offloaded version of MiniLB, Gallium produces
an offloaded version of the load balancer where the connection
consistency map is stored in the switch. New incoming connections
and packets with TCP control flags (RST and FIN) will be forwarded
to the middlebox server, as handling those packets requires an
update to the map.
The P4 program generated for the firewall middlebox contains
two match-action tables to filter the traffic from both directions.
These tables offload the functionality performed by theIPClassifier
For the proxy, the pre-processing code contains one match-action
table that checks the incoming TCP packets’ destination port. A
packet rewriting action is also included in the P4 program to rewrite
the TCP packet’s destination to the web proxy server.
Gallium places Trojan detector’s TCP flow state table on the pro-
grammable switch. TCP control packets, such as SYN or SYNACK,
triggers a table update. These packets are forwarded to the middle-
box server. Besides that, HTTP requests from an endhost that have
received SSH traffic before are also be processed by the middlebox
server to determine the type of requested file. Most of the TCP
data packets that do not require deep packet inspection are handled
solely by the programmable switch.
6.3 Performance
We first microbenchmark, for each of our five middleboxes, the
throughput, latency, and CPU overheads. We then evaluate the
performance overhead introduced by performing state synchro-
nization when updating the state replicated on the switch. After
that, we evaluate the five middleboxes using realistic workloads.
Experiment Setup. Our testbed consists of three servers and
a Barefoot Tofino switch. Each server has an Intel Xeon E5-2680
CPU (2.5GHz, 12 cores) and a Mellanox ConnectX-4 100 Gbps NIC.
Servers run Ubuntu 18.04 with Linux kernel version 4.15. All the
three servers are connected to a Barefoot Tofino switch via 100 Gbps
links. We dedicate one server to be the middlebox server. The mid-
dlebox server runs DPDK version 17.11. These two servers use
traditional Linux networking stacks to generate and receive pack-
ets.
To compare performance with non-offloaded middleboxes, we
use FastClick [5] to run non-offloaded middleboxes in the middlebox
server and configure the routing table in the switch to ensure all
packets go through the server.
TCP Microbenchmark. We generate ten parallel TCP connec-
tions using iperf to test the maximum achievable throughput of
the middleboxes. When we run Gallium middleboxes, we restrict
the usage of the processing in the middlebox server to be on a single
core. For FastClick, we test the middleboxes with 1, 2, and 4 cores.
We also test different packet sizes (e.g., 100, 500, and 1500 bytes).
We test the throughput ten times and measure the average and
standard deviation.
Gallium substantially improves middlebox throughput and re-
duces CPU overheads on the middlebox server. Figure 7 compares
292
1005001500Packet Size (Bytes)0255075100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1c1005001500Packet Size (Bytes)0255075100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1c1005001500Packet Size (Bytes)0255075100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1c1005001500Packet Size (Bytes)0255075100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1c1005001500Packet Size (Bytes)0255075100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1cGallium: Automated Software Middlebox Offloading to Prog. Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Middlebox
MazuNAT
Load balancer
Firewall
Proxy
Trojan Detector
FastClick
23.16 ± 0.53 µs
23.09 ± 0.31 µs
22.45 ± 0.27 µs
22.72 ± 0.87,µs
22.58 ± 0.74,µs
Gallium
15.98 ± 0.21 µs
15.96 ± 0.20 µs
15.96 ± 0.20 µs
15.64 ± 0.85 µs
14.80 ± 0.43 µs
Table 2: Latency comparison of Gallium middleboxes and
their FastClick counterparts. The numbers after ± denote
the standard deviations.
# tables
1
2
4
Insert
135.2 ± 22.0 µs
270.1 ± 33.0 µs
371.0 ± 39.2 µs
Modify
128.6 ± 23.6 µs
258.3 ± 34.9 µs
363.0 ± 37.3 µs
Delete
131.3 ± 18.8 µs
262.7 ± 29.8 µs
366.1 ± 37.7 µs
Table 3: Latency of updating offloaded P4 tables from mid-
dlebox server.