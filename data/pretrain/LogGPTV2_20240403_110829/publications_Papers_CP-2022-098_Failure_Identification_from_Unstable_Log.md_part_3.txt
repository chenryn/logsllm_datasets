we resolve this issue by penalizing the updates with the
the encoder block is to learn context-aware representations,
term 1 .C countsthenumberoftimesaclusterisassignedan
theminibatchedkmeanscomplementsitbyextractingsimilar ck k
embeddingduringanepoch.Thelargerthenumberofassigned
context groups, enabling the extraction of subprocesses. We
embeddings,thesmalleristhecentroidupdatedandviceversa.
used mini batch kmeans because it allows per batch update
Itnormalizestheintensityofthecentroidupdateasalearning
of the network (Î¸ and Î¸(cid:48)) and clustering parameters (M)
rate, different for each cluster.
as opposed to the classical kmeans method. To group the
contexts, kmeans optimizes the loss given in Eq. 2 by altering 1
betweentwosteps:1)updatingacentroidm k astheaverageof m k â†m kâˆ’ c (Ï†(sm n;Î¸)âˆ’m k)r n (3)
k
where V is the set of validation normal sequences of sub-
Failure Detector Threshold Estimation Failure Detection
1hs 2 H hsM 3Mğ‘à·ª+(ğ‘ ğ‘–) ğ‘à·¦1 ğ‘à·¦, 1ğ‘à·¦2 Ïƒthr ğ‘à·¦es 2holds Failure Sequences pp oro sc ite ivss ees f. uT nch te ion no ,rm gia vl eit ny asc so tr he ees st pim rea at depËœ o+ f( ts hi e)i ps ra obs ay bm ilm ityetr oic
I SN eqP uU eT n: Î¼ f
ces of hs
Subprocesses the sequence s i under the HMM (t(s i)) from the mean score
t1: (S1, S2, S1, S4) estimates of the validation data. The parameters of the HMM
t2: (S3, S1, S1, S2)
t3: (S1, S3, S4, S4) Feature Extraction FTI Model Failure are learned on the normal training data, thereby, the failure
â€¦ 1.Count Vector of Supbrocesses(CV) (RF, DT, LR, AdaBoost) Sequence
2.Probability under HMM (pHMM) Labels (training) detector models the normal system state. We assume that the
3.Combination (CV+pHMM)
Fitted FTI Modelğ‘“áˆšğ‘ ğ‘– OUTPUT: ğ‘¨à·© n oo pr em raa til od nat wa hi es na tl hw ea ry es ao rebt nai ona isb sl ue efr ro em porth tse op rer loio gds evo ef nts sys wte im
Offline Phase th
Online Phase Failure Type Identification
â€errorâ€orâ€criticalâ€loglevels.Anysequencewithsignificantly
Fig.5. Internalarchitecturaldesignoffailureidentificationpart. different values for the normality score estimate is detected
as a failure. Using the symmetrical property of the normality
function, we estimate the thresholds as aËœ = Âµ Â± 3Ïƒ,
1/2
3) Subbprocess Exctraction: The extraction of a subpro- where Âµ and Ïƒ are the mean value and standard deviation of
cess identifier (ID) is done as follows. Given an original thevalidationscoreestimatescalculatedbystandardformulas.
input event sequence and the subprocess ID assignments of Thereby,thefailuredetectorisfullyunsupervised.Thenumber
itsmaskedsubsequences,wecountthenumberofoccurrences of hidden states is one hyperparameter of HMM.
of the subprocesses IDs and divide the counts by the length 2) FailureTypeIdentification: Oncethefailureisdetected,
of the original input event sequence. The subprocess ID with thefailuresequenceproceedstowardsthefailuretypeidentifi-
the highest score value is assigned as a subprocess ID for cationmodule.Thismoduleleveragestheredundancyproperty
the input event sequence. Intuitively, if the majority of the of failures in cloud systems [5]. This property emerges for
masked subsequences are assigned with a single subprocess various reasons, including temporary failure fixes by de-
ID,thesubprocessIDwiththemaximalscorevalueisthemost velopers without addressing the root cause, environmental
relevantfortheinputeventsequence.Fig.4depictsanexample issues (e.g., machine failure or network disconnections), or
ofextractingthesubprocessS forthesequence(E 2,E 5,E 3).
1 running the same system in different environments. Notably,
the redundancy implies repetitive patterns in logs, allowing
C. Failure Identification
usage of operational information to identify the failure type.
The failure identification part is given sequences of sub- The failure type identification subpart has two components
processes with the same task ID as input. Fig. 5 depicts the (1)featureextractionand(2)failuretypeidentificationmodel.
internal design. It is composed of two subparts 1) failure The feature extraction processes the sequences of subpro-
detector and 2) failure type identifier. The failure detector cesses in a format suitable for the FTI learning method. Each
detectsiftheinputsequenceofsubprocessesrepresentsfailure. sequenceisrepresentedbyacountvectorthatcountsthenum-
When failure is detected, the sequence proceeds towards the berofoccurrencesofasubprocesswithinthesequence.Forex-
failure type identification part, which identifies the type of ample, for the sequence of subprocesses s=(S 1,S 3,S 1) and
failure based on prior historical information. We describe the total of four subprocesses (S 1,S 2,S 3,S 4), the count vector is
details in the following. givenasCV(s)=(2,0,1,0).Theabsence/presenceofcertain
1) FailureDetection: Asamodelingchoiceforthefailure subprocesses from the sequence (e.g., lack of the subprocess
detector, we considered Hidden Markov Model (HMM) [17]. with the event â€Failure to spawn an instance.â€) are distinctive
HMM, models the sequences of subprocesses by assuming features that discriminate among failure types. Therefore, the
that the appearance of the next subprocess within the se- count vector is a suitable sequence representation. We also
quence depends only on the current subprocess. The main considered the normality score estimates from the failure
advantages of HMM are that it directly handles sequential detector as an additional feature (pHMM).
data, does not require further preprocessing of the input, and The extracted features are used to fit the FTI model given
is fast for both learning and inference (with a reasonably by fËœ(s i). FTI learns a multiclass classification model that
high number of hidden states). To produce normality score classifies the input sequences into several predefined types
estimates for a sequence pËœ+(s ), we used HMM probability of failures. As an adequate methods we considered several
i
scores(t(s)),calculatedbymarginalizingtheprobabilitiesover popular multiclass classification methods, i.e., Random Forest
all the subprocesses of the sequence and the hidden states (RF) [18], Decision Tree (DT) [19], Logistic Regression
of the fitted HMM t(s) = âˆ’log(cid:80) q(h)q(s|h), where h (LR) [20] and AdaBoost [21]. They show good performance
h
denotes the hidden states, and q(s|h) denotes the likelihood anddonotrequireextensivehyperparameteroptimization[18].
of the subprocess given the hidden state. The normality score ThefinaloutputofCLogisgivenasAËœ={(s j,t i)|s j âˆˆS,t i âˆˆ
estimatesforasinglesequences i isgiveninEq.4,asfollows: T,pËœ+(s j)aËœ 2,t i =fËœ(s j),j âˆˆJ}.
|V| IV. EXPERIMENTALEVALUATION
1 (cid:88)
pËœ+(s i)=( |V| t(s j)âˆ’t(s i))2 (4) Inthissection,wedescribetheexperimentalevaluation.We
sj givedetailsabouttheexperimentaldesign,presentanddiscuss
TABLEI
DATASETSTATISTICS
Averagenumberofevents
Datasetname NumberofTasks NoFailure AssertionFailures NumberofLogMessages NumberofUniqueEvents NumberofFailureEvents
fault-freetask
OpenStack 878 706 172 217534 518 167 1323
Syntetic 500 421-476 24-79 167215 474 123 1309
theexperimentalresultsinresponsetofourresearchquestions. in b-percentage in the sampled data, we inject the aforenamed
operations in random order.
A. Experimental Design
1) OpenStack dataset: To evaluate CLog, we considered 2) Baselines: We compare the failure detection method
a large scale study of failures in OpenStack, introduced in against three unsupervised baselines (two sequential-based
Cotroneo et al. [12]. To the best of our knowledge, it is the DeepLog [11], HMM [17], and one count-based PCA [22]),
most comprehensive publicly available dataset of log failure andtwomethodscommonlyusedinpracticebydevelopers[5].
data from a cloud system. Its strength is the wide range of Those are â€Log Levelâ€, which uses the severity level of
covered failures following the most common problem reports the log (i.e., failure exist if the log level is one of â€errorâ€,
in the OpenStack bug repository. The faults are generated by â€fatalâ€, or â€criticalâ€) and â€Semanticâ€ based on the semantics
software fault-injection procedure, i.e., modifying the source of a log (i.e., a human identifies the failure as logged in
code of OpenStack and running a predefined workload under a single log line) [23]. Recent study [8] identifies DeepLog
fault-injected and fault-free (normal) conditions. as having a state-of-the-art performance among unsupervised
The considered fault types are grouped into four groups as methods. Additionally, we considered a supervised automatic
of following: 1) throw exception (method raises an exception failure identification method LogRobust [6], that requires
in accordance to a predefined API list), 2) wrong return value labels for the severity level of the sequences. For failure type
(methodreturnsanincorrectvalue,e.g.,returnnullreference), identification, we compare against LogClass [4], which trains
3) wrong parameter value (calling a method with an incorrect amulticlassmodelonindividuallogstoidentifyfailurestypes.
valueforaparameter),and4)delay(methodreturnstheresult We used task ID failure type alongside the annotations of the
after a long delay, e.g., caused by hardware failure â€“ leading single logs to construct a target label and apply this method.
to triggering timeout mechanisms or stall). As a running
workload with a unique task ID, the authors considered 3) Experimental Setup: We conducted the experiments as
the creation of a new instance deployment. This workload follows. The hyperparameters of the log parser Drain, i.e.,
configuresanewvirtualinfrastructurefromscratchâ€“itcreates the similarity threshold and depth, were set to 0.45 and 5 as
VMinstances,volumes,keypairs,andsecuritygroups,virtual commonly used values for OpenStack logs [13]. For phase
network, assigns instance floating IPs, reboots the instances, 1 the training was performed for a maximal of 200 epochs,
attaches the instances to volumes and deletes all resources. and phase 2 training for a maximal of 20 epochs. As an
Importantly, this comprehensive workload invokes the three optimizer, we used SGD with a learning rate set to 0.0001.
keyservicesofOpenStackNova,Cinder,andNeutron,causing For the encoder, the model size d was set to 128, with
diverse manifestations of the faults as failures. two encoder layers and four heads. To prevent overfitting,
Togenerategroundtruthlabelsforthefailurestate,assertion we set the dropout rate to 0.01. Experimentally, we find
andAPIchecksareperformedattheendoftheworkloadruns. that Î» with value 0.1 leads to robust results. The optimized
Therearethreefailuretypes:1)failureinstance,2)failureSSH hyperparameters of CLog (performed on a separate validation
and 3) failure attaching volume. While the authors provide set) are the number of extracted subprocesses, the window
information on a granularity of a workload with a task ID, size,andthenumberofhiddenstatesoftheHMM.Theywere
we further labeled the individual logs. More specifically, two selected from the range values of the sets {10,20,30,40,50},
human annotators labeled more than 200000 logs to find the {60s,120s,180s,240s,300s} and {2,4,8,16} accordingly.
ones related to the logged failure. The agreement between the The max length was set to 32. The hyperparameters of
annotators is 0.67 Cohenâ€™s Kappa score. TABLE I gives the the considered FTI methods set are to their implementation
detailed statistics of the used data. defaults from the sckit-learn library. The baselines for failure
Syntetic dataset: To evaluate the robustness of our method detection were trained following a survey [2] of log-based
in dealing with unstable log data, we have created a synthetic failuredetectionfromsoftwaresystems.LogClasswastrained
dataset. The data is created similarly as in [6]. We start with as in the original paper [4]. The failure detection performance
the normal OpenStack dataset and apply the following three was evaluated on F1, precision and recall as common evalua-
operationstoextractfailuresequences,i.e.,1)randomremoval tion metrics, with the failure being a positive label. The same
oflogevents,2)repetitionofarandomlyselectedlogeventin performance scores were used for FTI, with macro averaging
the sampled log sequence, and 3) random shuffling the order over the three failure types. The experiments were conducted
of several events. To inject unstable log event sequences, we on a Linux server with Intel Xeon(R) 2.40GHz CPU and
randomly sample 500 log sequences (normal and failed), and RTX 2080 GPU running with Python 3.6 and PyTorch 1.5.0.
TABLEII
COMPARISONOFCLOGAGAINSTBASELINESONFAILUREDETECTION.
Scores/Category Unsupervised DeveloperPracticies Supervised
Methods: CLog HMM PCA DeepLog Semantic LogLevel LogRobust
F1 0.94Â±0.02 0.82Â±0.09 0.77Â±0.05 0.85Â±0.03 0.81Â±0.0 0.70Â±0.02 0.96Â±0.01
Precision 0.97Â±0.03 0.8Â±0.11 0.82Â±0.07 0.78Â±0.02 1.0Â±0.0 0.74Â±0.02 0.94Â±0.02
Recall 0.91Â±0.03 0.84Â±0.10 0.73Â±0.06 0.93Â±0.03 0.66Â±0.0 0.65Â±0.02 0.98Â±0.02
B. Research Questions inspection of the logs for failures, i.e., it detects all of the
single-line logged failures. However, as we observed when
1) RQ1:HowdoesCLogcompareagainstbaselinesonthe
performingthemanualloganalysis,andasshowninCotroneto
task of failure detection?: We evaluate CLog detection per-
et al. [12], around 20% of the failures in the dataset are not
formance against three unsupervised methods, two commonly
explicitlylogged.Incomparison,CLogcandetectnon-logged
used developer practices and one supervised method. The
failuresbecauseitmodelsdifferentcontexts,i.e.,itscorrelates
trainingisdoneon60%randomlysamplednormalsequences,
events co-occurring together. The violations of these contexts
while the thresholds (and other hyperparameters) are selected
(e.g., an expected log event is missing from the context) are
onarandomsampleof20%normalsequences.Therestofthe
informative in implicitly detecting non-logged failures. The
sequences are used to report the performance scores. The best
log level-based approach experiences the lowest performance.
results for CLog are obtained for a total of 10 subprocesses
Despite the problem of insufficient failure coverage, it further
(centroids), two hidden states in the HMM and a window size
of 180 seconds1. The experiments are repeated ten times to suffers from the problem of wrong log level assignment [23].
Alogmaybeassignedaloglevelâ€ERRORâ€butstilldescribe
reduce the assessment bias of the results. We report the mean
a normal event. Therefore, relying on the log level leads to
and standard deviation of the results.
reporting more failures than there are, affecting the precision.
Since CLog is an unsupervised method, we first discuss
Finally, comparing CLog against the supervised baseline
the results between CLog and the unsupervised baselines.
LogRobust, suggests that CLog has a drop in performance by
TABLE II shows the results. CLog outperforms the unsu-
2%ontheF1scorewhileforothers,itexceeds11%.However,
pervised baselines by margins between 9-17% on the F1
LogRobust requires labeled log sequences to build a model.
score. Importantly, CLog and HMM both use HMM to model
Due to a large number of logs constantly being generated,
the sequences, but they differ in the granularity of the in-
the labeling is often infeasible in practice, and it is the most
put representation. Marginalizing over the learning method
common referenced critique of the supervised methods [2].
suggest that changing the input representation of the log
Therefore, CLog has better practical properties because of the
event sequences with sequences of subprocesses is beneficial.
high detection performance and unsupervised design.
Combining these results with our observation (see Fig. 2)
demonstrates that reducing the entropy by changing the input
TABLEIII
representationimprovesthedetectionperformance.CLogpre-
COMPARISONOFCLOGAGAINSTBASELINESONFTI.
dominantly improves the precision over the sequence-based
methods (DeepLog and HMM), while having strong perfor- CLog CLog CLog LogClass
Multiclass (CV+pHMM