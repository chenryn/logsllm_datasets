ber of trees in the forest and (3) the number of values of a key. Our
benchmark creates AADs of ever-increasing size n. For speed, in-
stead of computing accumulators, we simply pick them uniformly
at random. (Note that this does not affect the proof verification
time.) We measure average proof sizes for keys with ℓ values in
an AAD of size n, where ℓ ∈ {0, 1, 2, 4, 8, 16, 32}. (Recall that a key
with ℓ values requires ℓ frontier proofs.) To get an average, for
every ℓ, we set up 10 different target keys so each key has ℓ val-
ues. The rest of the inserted keys are random (and simply ignored
by the benchmark). Importantly, we randomly disperse the target
key-value pairs throughout the forest to avoid having all values of
a key end up in consecutive forest leaves, which would artificially
decrease the proof size. Once the dictionary reaches size n, we go
through every target key with ℓ values, compute its lookup proof,
and measure the size and verification time. Then, for each ℓ, we
take an average over its 10 target keys. We repeat the experiment
for increasing dictionary sizes n and summarize the numbers in
Figures 5a and 5b. Proof verification is single-threaded.
Worst-case versus best-case dictionary sizes. Recall that some
dictionary sizes are “better” than others because they have fewer
trees in the forest. For example, a dictionary of (worst-case) size 2i −
1 will have i trees in the forest and thus i BFTs. Thus, a lookup proof
must include frontier proofs in all i BFTs. In contrast, a dictionary
of size 2i only has a single tree in the forest, so a lookup proof needs
only one frontier proof. Indeed, our evaluation shows that lookup
proofs are smaller in AADs of size 10i (see Figure 5b) compared to
2i − 1 (see Figure 5a). For example, for a key with 32 values, the
proof averages 95 KiB for size 106 and 118 KiB for size 220 − 1.
6.1.3 Append-only proofs. This benchmark appends random key-
value pairs until it reaches a target size n = 2i +1 − 1. Then, it
Session 6C: Secure Computing VICCS ’19, November 11–15, 2019, London, United Kingdom1307(a) Lookup proof (worst-case AAD sizes)
(b) Lookup proof (average-case AAD sizes)
(c) Append times (↑) and append-only proofs (↓)
Figure 5: The x-axes always indicate AAD sizes. Sections 6.1.1 to 6.1.3 explain the experiments. In Figure 5c (↑), “spikes” occur
when two trees of size B are merged, which triggers a new BFT computation, where B is the batch size.
measures the append-only proof size (and verification time) be-
tween AADs of size n and m = 2i − 1. We benchmarked on 2i − 1
AAD sizes to illustrate worst-case Θ(i) append-only proof sizes.
To speed up the benchmark, we randomly pick accumulators in
the forest. Append-only proof verification is single-threaded. Our
results show append-only proofs are reasonably small and fast to
verify (see Figure 5c). For example, the append-only proof between
sizes 219 − 1 and 220 − 1 is 3.5 KiB and verifies in 45 milliseconds.
6.1.4 Memory usage. Our lookup proof benchmark was the most
memory-hungry: it consumed 263 GiB of RAM for AAD size n =
220 − 1. In contrast, the append-only proof benchmark consumed
only 12.5 GiBs of memory, since it did not need BFTs. As an example,
when n = 220 − 1, all BFTs combined have no more than 390n
nodes. Since we are using Type III pairings, each node stores three
accumulators (two in G1 and one in G2) in 384 bytes (due to libff’s
3x overhead). Thus, the BFT accumulators require no more than 147
GiB of RAM. The rest of the overhead comes from our pointer-based
BFT implementation and other bookkeeping (e.g., polynomials). The
q-PKE public parameters could have added 64 GiBs of RAM, but
these two benchmarks did not need them.
Improving memory. A new security proof could eliminate the ad-
ditional G1 and G2 accumulators and reduce BFT memory by 2.66x
and the size of the public parameters by 1.33x (see Section 6.1.1). A
more efficient representation of group elements than libff’s could
also reduce BFT memory by 3x. An efficient array-based imple-
mentation of BPTs and BFTs could further reduce memory by tens
of gigabytes. Finally, the 390x overhead of BFTs can be drastically
reduced by carefully grouping upper frontier prefixes together in
a BFT leaf, similar to the grouping of lower frontier prefixes from
Section 5. However, doing this without increasing the lookup proof
size too much remains to be investigated.
6.2 Comparison to Merkle tree approaches
How do AADs compare to Merkle prefix trees or History Trees
(HTs), which are used in CONIKS and Certificate Transparency (CT)
respectively? First of all, appends in AADs are orders of magnitude
slower because of the overheads of cryptographic accumulators
and remain to be improved in future work (see Section 6.1.1).
Lookup proofs in prefix trees are much smaller than in AADs. In
a prefix tree of size 220, a proof consisting of a Merkle path would
be around 640 bytes. In comparison, our proofs for a key with
32 values are 152 times to 189 times more expensive (depending
on the number of trees in the forest). On the other hand, append-
only proofs in AADs are O (log n), much smaller than the O (n) in
prefix trees. For example, our Golang implementation of prefix
trees, shows that the append-only proof between trees of size 219
and 220 is 32 MiB (as opposed to 3.5 KiB in AADs). The proof gets
a bit smaller when the size gap between the dictionaries is larger
but not by much: 14.6 MiB between 105 and 106.
Lookup proofs in history trees (HTs) are O (n)-sized, compared
to O (log2 n) in AADs. This is because, to guarantee completeness,
the HT proof must consist of all key-value pairs. On the other hand,
append-only proofs in AADs are slightly larger than in HTs. While
our proofs contain approximately the same number of nodes as
in HT proofs, our nodes store two BPT accumulators in G1 and a
subset proof in G2 (in addition to a Merkle hash). This increases
the per-node proof size from 32 bytes to 32 + 64 + 64 = 160 bytes.
6.2.1 When do AADs reduce bandwidth? Asymptotically, AAD
proof sizes outperform previous work. But in practice, our evalua-
tion shows AAD proof sizes are still larger than ideal, especially
lookup proofs. This begs the question: In what settings do AADs
reduce bandwidth in transparency logs? We answer this question
below while acknowledging that AAD append times and memory
Session 6C: Secure Computing VICCS ’19, November 11–15, 2019, London, United Kingdom1308usage are not yet sufficiently fast for a practical deployment (see
Section 6.1.1).
Consider a key transparency log with approximately one billion
entries (i.e., an entry is a user ID and its PK). If this were a CONIKS
log, then each user must check their PK in every digest published
by the log server. Let D denote the number of digests published per
day by the log server. This means the CONIKS log server will, on
average, send 960 · D bytes per day per user (without accounting
for the overhead of VRFs [71] in CONIKS). If this were an AAD log,
then each user (1) gets the most recent digest via an append-only
proof and (2) checks their PK only in this digest via a lookup proof.
Let C denote the number of times per day a user checks his PK (and
note that, in CONIKS, C = D). Since the lookup proof is for the
user’s PKs not having changed, it only needs to contain frontier
proofs. Extrapolating from Figure 5b, such an average-case lookup
proof is 40 KiB (in an AAD of size one billion). Similarly, an append-
only proof would be 7 KiB. This means the AAD log server will,
on average, send 47 · 1024 · C bytes per day per user. Thus, AADs
are more efficient when .0199 · D/C > 1. In other words, AADs
will be more bandwidth-efficient in settings where log digests must
be published frequently (i.e., D is high) but users check their PK
sporadically (i.e., C is low). For example, if D = 250 (i.e., a new
digest every 6 minutes) and C = 0.5 (i.e., users check once every
two days), then AADs result in 10x less bandwidth.
What about CT? Recall that CT lacks succinct lookup proofs. As
a result, domains usually trust one or more monitors to download
the log, index it and correctly answer lookup queries. Alternatively,
a domain can act as a monitor itself and keep up with every update
to the log. We call such domains monitoring domains. Currently, CT
receives 12.37 certificates per second on average [44], with a mean
size of 1.4 KiB each [35]. Thus, a CT log server will, on average,
send 12.37 · 1.4 · 1024 = 17, 733.63 bytes per second per monitoring
domain. In contrast, AADs require 47 · 1024 · C/86, 400 = .557 · C
bytes per second per monitoring domain. As before, C denotes how
many times per day a monitoring domain will check its PK in the
log. Thus, AADs are more efficient when 31, 837/C > 1. So even if
domains monitor very frequently (e.g., C = 100), AADs are more
bandwidth efficient. However, we stress that our append times
and memory usage must be reduced for a practical deployment to
achieve these bandwidth savings (see Sections 6.1.1 and 6.1.4).
7 DISCUSSION
Privacy via VRFs. When a user’s identity (e.g., email address) is
hashed to determine a path in the tree, the existence of the path
leaks that the user is registered. To avoid this, CONIKS proposed
using a verifiable random function (VRF) [65, 71] to map users to
paths in the tree in a private but verifiable manner. We note that
our construction is compatible with VRFs as well and can provide
the same guarantees. For fairness, our comparison to CONIKS from
Section 6.2.1 assumes CONIKS does not use VRFs.
Constant-sized digests. Digests in our constructions are O (log n)-
sized where n is the size of the set (or dictionary). We can make the
digest constant-sized by concatenating and hashing all Merkle roots.
Then, we can include the Merkle roots as part of our append-only
and lookup proofs, without increasing our asymptotic costs.
Large, bounded public parameters. Our bilinear-based construc-
tions from Sections 4 and 5 are bounded: they support at most N
appends (given q ≈ 4λN public parameters). One way to get an
unbounded construction might be to use RSA accumulators as ex-
plained later in this section. Another way is to simply start a new
data structure, when the old one gets “full,” similar to existing CT
practices [62]. The old digest could be committed in the new data
structure to preserve the append-only property and fork consis-
tency. (This will slightly increase our proof sizes for users who are
not caught up with the latest digest.)
Trusted setup ceremony. Previous work shows how to securely
generate public parameters for QAP-based SNARKs [47, 48] via
MPC protocols [20, 21]. For our AAD, we can leverage simplified
versions of these protocols, since our public parameters are a “sub-
set” of SNARK parameters. In particular, the protocol from [21]
makes participation very easy: it allows any number of players to
join, participate and optionally drop out of the protocol. In contrast,
the first protocol [20] required a fixed, known-in-advance set of
players. For our scheme, we believe tech companies with a demon-
strated interest in transparency logs such as Google, Facebook and
Apple can be players in the protocol. Furthermore, any other in-
terested parties can be players too, thanks to protocols like [21].
Finally, the practicality of these MPC protocols has already been
demonstrated. In 2016, six participants used [20] to generate public
parameters for the Sprout version of the Zcash cryptocurrency [50].
Two years later, nearly 200 participants used [21] to generate new
public parameters for the newer Sapling version of Zcash.
RSA-based construction. In principle, the bilinear accumulator in
our constructions from Sections 4 and 5 could be replaced with other
accumulators that support subset proofs and disjointness proofs.
Very recent work by Boneh et al [17] introduces new techniques
for aggregating non-membership proofs in RSA accumulators. We
believe their techniques can be used to create constant-sized dis-
jointness proofs for RSA accumulators. This, in turn, can be used
to build an alternative AAD as follows.
Let us assume we have an RSA accumulator over m elements.
First, RSA accumulators allow precomputing all constant-sized mem-
bership proofs in O (m log m) time [89]. In contrast, our bilinear
tree precomputes all logarithmic-sized proofs in O (m log2 m) time.
As a result, frontier proofs would be constant-sized rather than
logarithmic-sized (i.e., the frontier tree corresponding to an RSA
accumulator would be “flat”). This decreases our AAD lookup
proof size from O (log2 n) to O (log n). This asymptotic improve-
ment should also translate to a concrete improvement in proof
sizes. Our memory consumption should also decrease, since BFTs
are no longer required. Second, RSA accumulators have constant-
sized parameters rather than linear in dictionary size. This requires
a simpler trusted setup ceremony [41] and further saves memory
on the server. However, unless RSA accumulators can be sped up,
it would result in even slower appends, due to more expensive
exponentiations. We leave it to future work to instantiate this RSA
construction and prove it secure.
7.1 Constructions from argument systems
A promising direction for future work is to build AADs from generic
argument systems [5, 10, 11, 24, 42, 47, 48, 69, 101]. Such AAD
Session 6C: Secure Computing VICCS ’19, November 11–15, 2019, London, United Kingdom1309constructions would also require non-standard assumptions [43],
possibly different than q-PKE (e.g., random oracle model, generic
group model, etc.). Depending on the argument system, they might
or might not require trusted setup and large public parameters.
A static AAD can be built from an argument system (e.g., a
SNARK [42, 48]) as follows. The AAD maintains one unsorted tree
U and one sorted tree S whose leaves are sorted by key. The digest
of the AAD consists of (1) the Merkle roots (d (S ), d (U )) of S and
U and (2) a SNARK proof of “correspondence” π (S, U ) between S
and U . This proof shows that S’s leaves are the same as U ’s but in
a different, sorted by key, order. The SNARK circuit takes as input
U ’s leaves and S’s leaves, hashes them to obtain d (U ) and d (S ) and
checks that S’s leaves are sorted by key.
Now, given a digest (d (S ), d (U ), π (S, U )), lookups can be effi-
ciently proven using Merkle proofs in the sorted tree S. The append-
only property of two digests (d (S ), d (U ), π (S, U )) and (d (S′), d (U ′),
π (S′, U ′)) can be efficiently proven using a history tree append-
only proof between d (U ) and d (U ′). This proves U is a subset of
U ′ and, crucially, it also proves that S is a subset of S′, since the
SNARK π (S, U ) proves that S “corresponds” to U and S′ to U ′. Un-
fortunately, updates would invalidate the SNARK proof and take
time at least linear in the dictionary size to recompute it. However,
we can apply the same Overmars technique [80, 81] to make up-
dates polylogarithmic time. (This would now require a family of
circuits, one for each size 2i of the trees.)
This approach would result in much shorter lookup proofs while
maintaining the same efficiency for append-only proofs, since
state-of-the-art SNARKs have proofs consisting of just 3 group
elements [48]. On the other hand, this approach might need more
public parameters and could have slower appends. This is because,
even with SNARK-friendly hashes (e.g., Ajtai-based [12], MiMC [3]
or Jubjub [105]), we estimate the number of multiplication gates
for hashing trees of size 220 to be in the billions. (And we are not
accounting for the gates that verify tree S is sorted.) In contrast,
the degrees of the polynomials in our bilinear-based constructions
are only in the hundreds of millions for dictionaries of size 220.
Nonetheless, optimizing such a solution would be interesting
future work. For example, replacing SNARKs with STARKs [10]