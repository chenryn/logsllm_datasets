the new embeddings now create dense and separable clusters
for each use case in the plot. The network has moved pattern
embeddings that belong to the same use case closer together,
and the resulting clusters further away from each other in the
embedding space.
Accuracy The promising observations from the visualiza-
tion of pattern embeddings are conﬁrmed by the accuracy re-
sults of the classiﬁcation model. We performed a grid search
that revealed the optimal neighborhood size of K = 5. The
average AUC for predicting the different use cases already
achieves its optimum of 0.999 after 20 epochs. As already
indicated by the PCA plots, pattern embeddings provide a
very good representation of use cases as the average AUC
for all classes before training (epoch zero) is already above
0.998. However, precision and recall of IV, HNVOR and TM
start below 0.878 and have been improved up to above 0.986
within 30 epochs of training.
7.3 Security Classiﬁcation
Visualization We start again with illustrating the transfer-
learning process for security classiﬁcation by plotting pat-
tern embeddings before and after training. Figure 5(c) dis-
plays pattern embeddings before training with their respec-
tive security score, Figure 5(d) plots the new embeddings
after training. Samples that were labeled as secure are de-
picted in blue, insecure samples in red. When comparing
Figure 5(a) and Figure 5(c), we can already observe sev-
eral secure and insecure clusters within the use case clus-
348    28th USENIX Security Symposium
USENIX Association
CipherSSL/TLSHashIVKeyTMHNVHNVORCipherSSL/TLSHashIVKeyTMHNVHNVORFigure 6: ROC for security classiﬁcation of different use cases. The legend provides use case identiﬁer and respective AUC.
ters, e. g., Hash, Cipher and SSL/TLS. However, again many
secure and insecure samples appear to have a wide distri-
bution because PCA does not plot them in dense clusters.
After training the security classiﬁcation model, we input the
complete set of pattern embeddings and plot the last layer
of the neural network for each sample in Figure 5(d) again.
Now, we observe dense and separated clusters for secure and
insecure samples. The network has adjusted the pattern em-
beddings such that samples within both security classes have
been moved closer together in the embedding space. Sam-
ples with different security have been moved further away
from each other, ﬁnally dividing samples into two security
clusters.
Accuracy We trained a single model using the labeled
dataset of 16,539 pattern embeddings. Thereby, a single
model learns security classiﬁcation for all use cases. Our
grid search revealed K = 5 as the optimal neighbourhood
size. The model provides a good ﬁt because training and
validation loss already converge after 50 epochs. A single
epoch takes 0.58 seconds on average on our system, result-
ing in roughly ﬁve minutes for complete training time. We
plot ROC curves for security prediction for each use case
class in Figure 6. We observe that the three use cases Hash,
Cipher and SSL/TLS that provide the largest percentage of
samples in the dataset achieve the best results. The model
achieves very good classiﬁcation accuracy with AUC values
of 0.999, 0.996 and 0.999, respectively, similar to HNVOR
and TM. However, performance drops marginally for IV, Key
and HNV to 0.980, 0.970 and 0.953, respectively.
Comparison In Table 2, we compare our approach on se-
curity prediction on Stack Overﬂow with [17], where the au-
thors use tf-idf to create a feature vector as a representation
for the complete input snippets and to train a SVM predict-
ing its binary security score. Our deep learning approach
(marked as CNN in the table) signiﬁcantly outperforms their
classiﬁer in all use cases; especially IV, Key and HNVOR,
where security evaluation heavily relies on data and control
ﬂow. In contrast to our approach, the work by Fischer et al.
[17] does not inform the learning model about these proper-
ties, but solely relies on lexical features.
Moreover, our deep learning approach allows a higher
level of explainability to the user. While [17] can only re-
port security warnings for the complete snippet, our more
ﬁne-grained approach is able to directly highlight statements
in the code and provides annotations that explain the secu-
rity issue. Since we learn a representation of code patterns
that allows prediction of different code properties beyond se-
curity, we can provide this additional explanation, which is
crucial for developer advice.
CNN
tﬁdf+SVM
Cipher
Hash
TLS
IV
Key
HNV
HNVOR
TM
AUC-ROC
0.996
0.999
0.999
0.980
0.970
0.953
0.998
1.000
Explanation
SW, CA
SW, CA
SW, CA
SW, CA
SW, CA
SW, CA
SW, CA
SW, CA
AUC-ROC
0.960
0.956
0.902
0.881
0.886
0.922
0.850
0.982
Explanation
SW
SW
SW
SW
SW
SW
SW
SW
Table 2: Performance and explainability comparison of se-
curity prediction on Stack Overﬂow. SW: Provides security
warnings for the complete snippet. CA: Additionally pro-
vides code annotation that explains the issue in detail.
7.4 Recommendations
We applied our trained models in order to evaluate whether
Stack Overﬂow provides secure alternative code snippets,
which preserve the use case and are similar to detected in-
secure code examples. Thereby, we extracted all methods
from the complete set of 10,558 snippets, generated their
aggregated embeddings and separated them into two sets.
The ﬁrst set contains all 6,442 distinct insecure query em-
beddings and the second one all 3,579 distinct secure target
embeddings. We created these two sets by applying the secu-
rity model and predicted the security of each pattern within a
given method. Finally, we ranked the embeddings based on
their Jaccard distance, applying the use case model, and co-
sine similarity, as described in Section 6.2. We found 6,402
(99.37%) query methods that have Jaccard distance of 0.0 to
at least one target method. This means that for almost ev-
ery insecure method, a secure one exist on Stack Overﬂow
USENIX Association
28th USENIX Security Symposium    349
0.00.20.40.60.81.0False Positive Rate0.860.880.900.920.940.960.981.00True Postive RateCipher, AUC: 0.996Hash, AUC: 0.999SSL/TLS, AUC: 0.9990.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Postive RateIV, AUC: 0.980Key, AUC: 0.9700.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Postive RateHNV, AUC: 0.9530.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Postive RateHNVOR, AUC: 0.998TM, AUC: 1.000that serves the same use case. When additionally demand-
ing code similarity, we found 6,047 (93.86%) query meth-
ods with a cosine similarity above 0.81 and 4,805 (75.58%)
query methods with a similarity above 0.9 with at least one
target method.
8 Evaluation of Security Nudges
To evaluate the impact of our system including the security
nudges on the security of programming results, we perform
a laboratory user study. Thereby, participants had to solve
programming tasks with the help from Stack Overﬂow.
8.1 User Study Setup
Participants were randomly assigned to one of two treatment
conditions. For the nudge treatment, we provided security
warnings (Figure 2a) for insecure code examples, recom-
mendations for secure snippets (Figure 2b) and recommen-
dations lists attached to each warning (Figure 8). Further,
security reminders were enabled. In the control treatment,
all security nudges on Stack Overﬂow were disabled.
Participants were advised to use Stack Overﬂow to solve
the tasks.
In all treatments, we restricted Stack Overﬂow
search results to posts that contain a code example from the
set of 10,558 code examples we extracted from Stack Over-
ﬂow7. Further, we applied a whitelist ﬁlter to restrict ac-
cess to Stack Overﬂow in the Chrome browser. Any requests
to different domains were redirected to the Stack Overﬂow
search page. Participants were provided with the Google
Chrome browser and Eclipse pre-loaded with two Java class
templates. Both class templates provided code skeletons that
were intended to reduce the participants’ workload and sim-
plify the programming tasks. Using additional applications
was prohibited. All tasks had to be solved within one hour.
We avoided security or privacy priming during the introduc-
tion and throughout the study. Moreover, we did not name or
explain any of the security nudges on Stack Overﬂow.
8.2 Tasks
All participants had to solve ﬁve programming tasks related
to symmetric encryption and certiﬁcate pinning. We chose
these two use cases as they provide the most error-prone
cryptographic problems in Android [17].
Symmetric Encryption The ﬁrst three tasks dealt with ini-
tializing a symmetric cipher in order to encrypt and decrypt a
message. Task Cipher: a symmetric cipher had to be initial-
ized by setting the algorithm, block mode and padding. The
main security pitfalls in this task are choosing a weak cipher
7This aims at simplifying search for participants. All Stack Overﬂow
posts that contain a seed statement are available during the study.
and block mode. Task Key: a symmetric cryptographic key
had to be generated. Participants had to create a key having
the correct and secure key length necessary for the previously
deﬁned cipher. It had to be generated from a secure random
source and should not have been stored in plaintext. Task
IV: an initialization vector had to be instantiated. Like key
generation, this task is particularly error-prone as choosing
the correct length, secure random source and storage can be
challenging.
Certiﬁcate Pinning Within these two tasks, a SSL/TLS
context had to be created to securely communicate with a
speciﬁc server via HTTPS. In the end, the program should
have been able to perform a successful GET request on the
server, while denying connection attempts to domains that
provide a different server certiﬁcate. A solution for Task TLS
would have been to select a secure TLS version to initialize
the context. Task TM: the server’s certiﬁcate had to be added
to an empty custom trust manager replacing the default man-
ager. This way, the program would pin the server’s certiﬁcate
and create a secure communication channel, while rejecting
attempts to any other server with a different certiﬁcate.
8.3 Preliminaries and Participants
We advertised the study in lectures and across various uni-
versity communication channels. 30 subjects participated in
the study, however, three subjects dropped out, because they
misunderstood a basic participation requirement (i.e., having
at least basic Java programming knowledge). Of the remain-
ing 27 subjects, 16 were assigned to the nudge treatment, and
11 to the control treatment. While being students, our sam-
ple varied across demographics and programming skill, but
none of the self-reported characteristics systematically dif-
fered across the two treatments (see Appendix A for details).
We followed well-established community principles for
conducting security and privacy studies [29]. Participants
were presented with a comprehensive consent form and sep-
arate study instructions on paper. Participants were compen-
sated with 20 Euros.
After submission of the solutions, participants were asked
to complete a short exit survey. We asked speciﬁc ques-
tions addressing the effectiveness of the security nudges and
whether they were noticed by the participants. Also, we only
asked demographic questions at this point to avoid any bias
during the study. See Table 3 in the Appendix for details.
8.4 User Study Results
Functional Correctness Our system is not designed to ad-
dress difﬁculties of programmers to deliver functionally cor-
rect code. However, it is important that using the system does
not create obstacles to programmers. Participants predomi-
nantly submitted functionally correct code in both treatments
350    28th USENIX Security Symposium
USENIX Association
TM
TLS
IV
Key
Cipher
0 %
Nudge Secure
Nudge Insecure
Control Secure
Control Insecure
TM
TLS
IV
Key
Cipher
Nudge Correct
Nudge Incorrect
Control Correct
Control Incorrect
TM
TLS
IV
Key
Cipher
60 %
80 % 100 %
40 %
20 %
(a) Security results
0
0.2
0.4
0.6
0.8
1
0 %
20 %
40 %
60 %
80 % 100 %
(b) Copy-paste rate (average)
(c) Correctness results
Figure 7: User study results for security, copy-paste rate and correctness of the submitted solutions across both treatments.
with some differences across tasks (cf. Figure 7c). Applying
ordinal logistic regression (cf. Table 4 in the Appendix) indi-
cates that the nudge treatment has – as anticipated – no effect
on functional correctness of submitted tasks. However, non-
professionals submitted signiﬁcantly less functional code
(p < 0.05). Cipher submissions are more often functional,
irrespective of the treatment (p < 0.05).
Security Figure 7a shows the security results per task for
both treatments. Performing ordinal logistic regression (see
Table 5 in the Appendix), we show that the nudge treatment
is signiﬁcantly outperforming the control group in producing
secure solutions (with an estimate of 1.303 and p < 0.01;
Model 4). While the main effect of the nudge treatment
dominates the regression models, we can observe from Fig-
ure 7a that comparatively more secure submissions are made
for TM and Key. Indeed, pairwise testing using Chi-Square
tests reveals p < 0.001 for both tasks. Participants from
the nudge group provided 84.6% secure solutions for Key
and 76.9% for TM, while 60.0% and 66.7% of the respec-
tive solutions submitted by the control group were insecure.
These observations for TM are somewhat encouraging given
previous ﬁndings: [17] have shown that reused insecure TM
code snippets from Stack Overﬂow were responsible for 91%
(183,268) of tested apps from Google Play being vulnerable.
Only 0.002% (441) of apps contained secure TM code from