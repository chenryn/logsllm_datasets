Qualcomm
TI
Qualcomm
Qualcomm
Qualcomm
Samsung
ST-Ericsson
d
e
r
r
u
c
c
o
s
t
p
u
r
r
e
t
n
i
f
o
t
n
u
o
m
A
10
8
6
4
2
0
0
HSBC Mobile Banking 
[com.htsu.hsbcpersonalbanking]
time series 1
time series 2
20
80
Time Sequence (interval = 50 ms)
40
60
100
The training and testing are also done on Google Nexus 6.
We believe our techniques could be applied without modiﬁca-
tion on other devices with Android 4.1 above installed.
Reading Interrupt Count. The interrupt time series for DSS
could be captured through monitoring /proc/interrupts
as well. Following the similar method as Section IV-B, we
use the device name to identify the entry for DSS. We look
for MDSS2 on Google Nexus 6. The device names of DSS
on several other phones are summarized in Table VII. All
interrupts for DSS are logged under CPU0, so we only need
to monitor that column.
The sampling frequency of our implementation could reach
4899 Hz3, which is much higher than the frequency of
IRQ from DSS (60 Hz or 120 Hz). High sampling rate
would lead to more power consumption, but as described in
Section V-C), the performance impact to the phone is still
limited. For power saving, we could instruct the malicious
app to run only when the screen is turned on (register-
ing ACTION_SCREEN_ON) and to sleep when the screen
is locked (registering ACTION_SCREEN_OFF). The start-
ing point of sampling (i.e., app launching) could be de-
termined through combing the knowledge of targeted app
and our previous interrupt channel for touchscreen in Sec-
tion IV. For instance, after the malicious app ﬁnds the
user presses the Home Key (through the system broadcast
ACTION_CLOSE_SYSTEM_DIALOGS) and two successive
interrupts for touchscreen are detected, it can learn that the
user clicks an app icon on the home screen. The sampling
period could be set according to the launching time duration
of the targeted apps.
Data Pre-processing. Similar to the unlock pattern inference
attack, deduplication and interpolation are applied in the same
way (see Section IV-B). An additional noise ﬁltering step is
introduced to removed the background noise on interrupts. The
interrupts from DSS are not only caused by the app running
in the foreground. The system UI events (status bar showing
time, signal strength, battery, etc.) also cause screen refreshing,
2The name of MDSS is used by Qualcomm CPU series and stands for
Mobile Display Sub-System.
3Compared with the interrupt sampling module for unlock pattern inference,
the reason for different frequencies stems from the API fgets which is used
to read /proc/interrupts. The sampling rate is affected by the location
of interrupt log entry in the ﬁle.
Fig. 13. The curves of interrupt time series are similar for the same app.
incurring noises. By inspecting the interrupt sequence caused
by system UI events, we found all the noises are segments
of consecutive 50 ms intervals with less than 30 interrupts
in total and less than 6 interrupts per 50 ms interval. They
are nominal comparing to the interrupts from the foreground
app, and we search the interrupt time series to remove all such
noises.
App Similarity Calculation. We consider the interrupt time
series after pre-processing as app’s ﬁngerprint, which is used
to determine whether two apps are the same one. Exact
matching is not a viable solution here. Even for the same app,
the screen refreshing during app launching is not the same,
because of the background processes and different network
connection status. We use sequence similarity as the metric to
adapt to such unstable situations. Fig. 13 shows an example in
which we could ﬁnd, for the two interrupt time series coming
from the same app, their curves are quite similar, but not
coincide at every timestamp.
For calculating the sequence similarity, several difﬁculties
must be overcome: the lengths of two interrupt time series
may be different, and there may exist displacements along the
timeline. After examining different matching algorithms, we
found Dynamic Time Warping (DTW) algorithm [38] achieves
the optimal result. DTW is designed to calculate intuitive
distances between time series by ignoring shifts in the time
dimension. Its basic idea is to ﬁnd a minimum-distance warp
path of which length is treated as the measured distance. We
formalize it like [39] in our settings: given two interrupt time
series
X = {x1, x2, . . . , xi, . . . , x|X|}
and
Y = {y1, y2, . . . , yi, . . . , y|Y |}
, a warp path
W = {w1, w2, . . . , wK}, max(|X|,|Y |) ≤ K ≤ |X| + |Y |
is constructed where K is the length of the warp path and the
k-th element of the warp path is wk = (i, j) where i is an
index from X and j is an index from Y .
425425
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:05 UTC from IEEE Xplore.  Restrictions apply. 
i
l
g
n
k
n
a
b
a
n
o
s
r
e
p
c
b
s
h
.
u
s
t
h
.
m
o
c
80
60
40
20
0
0
20
40
60
80
com.htsu.hsbcpersonalbanking
i
k
c
o
l
c
k
s
e
d
.
d
o
r
d
n
a
.
e
g
o
o
g
.
m
o
c
l
80
60
40
20
0
0
20
40
60
80
com.htsu.hsbcpersonalbanking
Fig. 14. Warp paths of time series. Left: the same app. Right: different apps.
The optimal warp path is the one leading to minimum warp
distance, where the distance of a warp path W can be deﬁned
as
k=K(cid:7)
Dist(W ) =
Dist(wki, wkj)
(6)
k=1
where Dist(wki, wkj) is the distance between the two data
point indexes in the k-th element of the warp path.
The DTW distance could be used as the measurement of
the similarity between two interrupt time series. Fig. 14 gives
two warp path examples in which the background is the cost
matrices of distances (the deeper gray, the higher cost and the
farther distance). The warp path based on the same app (the
same two series as Fig. 13) in the left ﬁgure is nearly a straight
line from (0, 0) to (100, 100). However, the path in the right
ﬁgure based on different apps has several signiﬁcant turnings.
Training Phase. In this phase, we proﬁle apps of our interest
and build the ﬁngerprint database. We automate the training
process by using monkeyrunner [40] which can repeatedly
open and close apps following the scripted instructions. While
deciding when to open an app is easy (the time after the exit
of the prior app), it is not clear when to close the app. Based
on empirical observations and experiments, we found 4.5 s is
enough for an app to ﬁnish loading. Therefore, monkeyrunner
will close the app after 4.5 s since starting. For every app,
monkeyrunner triggers the launching operations for N times,
and the derived N ﬁngerprints are all stored in the ﬁngerprint
database. N can be adjusted by the adversary. While a small
N could lead to mismatch for the same app, big N will incur
large overhead in comparing. We set N to 10 by default.
Testing Phase. This phase is to test whether an app running
in the foreground matches one in the training dataset. The two
steps included in this phase are described below:
• Pre-ﬁltering. Computing DTW distance is costly. To
reduce the overhead, we use the total amount of interrupts
as a condition for pre-ﬁltering. Speciﬁcally, we obtain
the upper-bound and lower-bound of the total amount of
interrupts for all apps in the training dataset. If the amount
falls out of the range from training dataset, which is
extended from the upper- and lower-bound by 25 % (see
Section V-C for how 25 is decided), the app is considered
irrelevant and not proceeded to the next stage.
• Classiﬁcation. DTW distance is supposed to be calculated
between the ﬁngerprint of the testing app and all ﬁnger-
prints in the training set. For optimization, we apply the
same heuristic in pre-ﬁltering stage and skip the distance
calculation if the interrupts total amount greatly differs.
When DTW distance needs to be calculated, we employ
FastDTW algorithm [39] to accelerate the computation.
After the ﬁngerprint distances between the testing app and
training apps are computed, we use k-nearest neighbors
(k-NN) algorithm [41] to classify the app,
is a
majority vote by its neighbors. For instance, assuming
k = 5, if the testing ﬁngerprint matches 3 ﬁngerprints
from appa and 2 ﬁngerprints from appb, we consider
appa is running in the foreground. Since the result of
majority vote may be incorrect and several ﬁngerprints
in the training set may have the same distances as the
testing ﬁngerprint, we also consider the top-N results. In
addition, to avoid identifying an app not in the training
set, we could customize the training set based on the list
of installed apps on the victim’s phone. Such list could
be easily obtained by invoking PackageManager and
PackageInfo classes without permission.
that
C. Evaluation
We evaluate the effectiveness of our attack using interrupt
data collected by running popular apps. In addition, we mea-
sure the statistics of the interrupt amount per app and justify
how the pre-ﬁltering threshold is determined. The performance
overhead is tested using variant sampling rate and in the end,
we show an advanced version of this attack in snifﬁng the
foreground Activity in Appendix VIII-C.
Attack App. The attack app contains two modules – interrupt
sampling module (built with Android NDK) and data analysis
module (about 700 Java lines of code). Our implementation of
DTW distance calculation is based on Java-ML library [42].
Experimental Setup. We select 100 popular apps from
Google Play to build the training set, as listed in the Ap-
pendix VIII-E. These apps all stay in the foreground when
launched, and the apps always running in the background, like
instant messaging apps, are not included. Each app is launched
10 times, and 1,000 ﬁngerprints are recorded in total. For some
apps, an introduction or user agreement page is displayed at
the ﬁrst launching after installation and never shown afterward.
The ﬁngerprints, in this case, are discarded by us manually.
To build the testing set, we randomly select 10 apps (as shown
in Table IX) from these 100 apps in the training set, run each
one 10 times, and record 100 ﬁngerprints in total.
Interrupt Amount Threshold. We use the range of interrupt
amount to pre-ﬁlter apps and optimize similarity calculation.
The threshold θ for separating ﬁngerprints needs to be de-
termined before testing. For this purpose, we look into the
distribution of interrupt amount across different apps and
within one app. For every app in the training set, we count
the mean interrupt amount of its 10 ﬁngerprints, as shown
in Fig. 15 in ascending order. The maximum value is 635.7
426426
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:05 UTC from IEEE Xplore.  Restrictions apply. 
s
t
p
u
r
r
e
t
n
i
f
o
t
n
u
o
m
a
n
a
e
M
700
600
500
400
300
200
100
0
0
20
40
App #
60
80
100
Fig. 15. Mean interrupt amount.
SUCCESS RATE OF APP IDENTIFICATION UNDER DIFFERENT K
TABLE VIII
k
Top 1
Top 2
Top 5
Top 10
k=3
77 %
85 %
93 %
94 %
k=5
87 %
91 %
95 %
96 %
k=7
83 %
88 %
94 %
96 %
k=9
82 %
90 %
93 %
98 %
100
80
60
40
20
)
%
(
e
t
a
r
s
s
e
c
c
u
S
0
1
5
4
3
2
1
0
1
)
d
n
o
c
e
s
(
n
o
i
t
p
m
u
s
n
o
c
e
m
T
i
1/2
1/2
1/5
1/60
Ratio of the original sampling frequency
1/10
1/20
1/40
1/5
1/60
Ratio of the original sampling frequency
1/10
1/20
1/40
Top 1
Top 2
Top 5
1/80
1/100
1/80
1/100
Fig. 16. The result under varying sampling frequency.
coming from Microsoft Hyperlapse Mobile: during launching,