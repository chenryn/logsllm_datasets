# Qualcomm, TI, and Other Chipset Manufacturers

- Qualcomm
- Texas Instruments (TI)
- Samsung
- ST-Ericsson

# Interrupt Monitoring and Data Collection

The interrupt time series for the Display Sub-System (DSS) can be captured by monitoring `/proc/interrupts`. The device name to identify the DSS entry on Google Nexus 6 is `MDSS2`. Table VII summarizes the DSS device names for various other phones. All interrupts for DSS are logged under CPU0, so only that column needs to be monitored.

Our implementation achieves a sampling frequency of up to 4899 Hz, which is significantly higher than the typical DSS IRQ frequencies (60 Hz or 120 Hz). Although high sampling rates increase power consumption, the performance impact on the phone remains limited. To conserve power, the malicious app can be configured to run only when the screen is on (using `ACTION_SCREEN_ON`) and to sleep when the screen is locked (using `ACTION_SCREEN_OFF`).

The starting point for sampling (i.e., app launching) can be determined by combining knowledge of the targeted app and our previous interrupt channel for touchscreens. For example, after detecting the Home Key press (via `ACTION_CLOSE_SYSTEM_DIALOGS`) and two successive touchscreen interrupts, the app can infer that the user has clicked an app icon on the home screen. The sampling period can be adjusted based on the launch duration of the targeted apps.

# Data Pre-processing

Similar to the unlock pattern inference attack, deduplication and interpolation are applied. An additional noise filtering step removes background noise from interrupts. System UI events (e.g., status bar updates) also cause screen refreshes, generating noise. By analyzing the interrupt sequences, we found that all such noises are segments of consecutive 50 ms intervals with fewer than 30 total interrupts and fewer than 6 interrupts per 50 ms interval. These segments are removed from the interrupt time series.

# App Similarity Calculation

We consider the pre-processed interrupt time series as the app's fingerprint. Exact matching is not feasible due to variations in screen refreshes during app launches. We use sequence similarity metrics to handle these variations. Figure 13 shows that the interrupt time series curves for the same app are similar but not identical at every timestamp.

To calculate sequence similarity, we use the Dynamic Time Warping (DTW) algorithm, which is designed to find the minimum-distance warp path between time series, ignoring shifts in the time dimension. Given two interrupt time series \( X = \{x_1, x_2, \ldots, x_{|X|}\} \) and \( Y = \{y_1, y_2, \ldots, y_{|Y|}\} \), a warp path \( W = \{w_1, w_2, \ldots, w_K\} \) is constructed where \( K \) is the length of the warp path and each element \( w_k = (i, j) \) represents indices from \( X \) and \( Y \). The optimal warp path minimizes the distance, defined as:

\[
\text{Dist}(W) = \sum_{k=1}^{K} \text{Dist}(x_i, y_j)
\]

where \(\text{Dist}(x_i, y_j)\) is the distance between the data points at indices \( i \) and \( j \).

# Training Phase

In this phase, we profile apps of interest and build a fingerprint database. The training process is automated using `monkeyrunner`, which can repeatedly open and close apps following scripted instructions. Each app is launched 10 times, and the resulting fingerprints are stored in the database. The default value of \( N \) is 10, balancing accuracy and overhead.

# Testing Phase

This phase tests whether an app running in the foreground matches one in the training dataset. It includes two steps:

1. **Pre-filtering**: To reduce overhead, we use the total number of interrupts as a pre-filtering condition. If the interrupt count falls outside the range (extended by 25% from the upper and lower bounds of the training set), the app is considered irrelevant.
2. **Classification**: DTW distances are calculated between the testing app's fingerprint and all fingerprints in the training set. We use the FastDTW algorithm for optimization. The k-nearest neighbors (k-NN) algorithm classifies the app based on a majority vote. For example, if \( k = 5 \) and the testing fingerprint matches 3 fingerprints from `appa` and 2 from `appb`, `appa` is considered the running app. We also consider the top-N results to avoid misidentification.

# Evaluation

We evaluate the attack's effectiveness using interrupt data from popular apps. The attack app consists of an interrupt sampling module (built with Android NDK) and a data analysis module (700 lines of Java code). Our implementation uses the Java-ML library for DTW distance calculation.

**Experimental Setup**: We selected 100 popular apps from Google Play to build the training set, each launched 10 times, recording 1,000 fingerprints. For some apps, initial launch pages were discarded. The testing set was built by randomly selecting 10 apps, each launched 10 times, recording 100 fingerprints.

**Interrupt Amount Threshold**: We determined the threshold for separating fingerprints by analyzing the distribution of interrupt amounts across different apps. The mean interrupt amount for each app in the training set is shown in Figure 15.

**Success Rate**: Table VIII shows the success rate of app identification under different values of \( k \). The success rate increases with larger \( k \) values.

**Sampling Frequency**: Figure 16 shows the results under varying sampling frequencies. The original sampling frequency is 4899 Hz, and the success rate is evaluated at different ratios of this frequency.

This comprehensive approach ensures accurate and efficient app identification using interrupt data.