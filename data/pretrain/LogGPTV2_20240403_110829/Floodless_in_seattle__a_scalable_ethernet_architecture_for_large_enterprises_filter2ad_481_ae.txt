IMPLEMENTATION
To verify SEATTLE’s performance and practicality through a
real deployment, we built a prototype SEATTLE switch using two
open-source routing software platforms: user-level Click [36] and
XORP [37]. We also implemented a second version of our proto-
type using kernel-level Click [38]. Section 7.1 describes the struc-
ture of our design, and Section 7.2 presents evaluation results.
7.1 Prototype design
Figure 7 shows the overall structure of our implementation.
SEATTLE’s control plane is divided into two functional modules:
i) maintaining the switch-level topology, and ii) managing end-host
information. We used XORP to realize the ﬁrst functional module,
and used Click to implement the second. We also extended Click
to implement SEATTLE’s data-plane functions, including consis-
tent hashing and packet encapsulation. Our control and data plane
modiﬁcations to Click are implemented as the SeattleSwitch ele-
ment shown in Figure 7.
SEATTLE control plane: First, we run a XORP OSPF process at
each switch to maintain a complete switch-level network map. The
XORP RIBD (Routing Information Base Daemon) constructs its
routing table using this map. RIBD then installs the routing table
into the forwarding plane process, which we implement with Click.
Click uses this table, namely NextHopTable, to determine a next
hop. The FEA (Forwarding Engine Abstraction) in XORP han-
dles inter-process communication between XORP and Click. To
maintain host information, a SeattleSwitch utilizes a HostLocTable,
which is populated with three kinds of host information: (a) the out-
bound port for every local host; (b) the location for every remote
host for which this switch is a resolver; and (c) the location for ev-
tent hashing (which takes around 2.2 us) is required only for ARP
requests. Hence, SEATTLE requires less overall processing time
on paths longer than 3.03 switch-level hops. In comparison, we
found the average number of switch-level hops between hosts in a
real university campus network (Campus) to be over 4 for the vast
majority of host pairs. Using our kernel-level implementation of
SEATTLE, we were able to fully saturate a 1 Gbps link.
Table 1: Per-packet processing time in micro-sec.
learn
src
0.61
-
-
0.63
look-up
host tbl
0.63
0.63
-
0.64
encap
0.67
-
-
-
look-up
nexthop tbl
0.62
-
0.67
-
Total
2.53
0.63
0.67
1.27
SEA-ingress
SEA-egress
SEA-others
ETH
Effect of network dynamics: To evaluate the dynamics of SEAT-
TLE and Ethernet, we instrumented the switch’s internal data struc-
tures to periodically measure performance information. Figures 8a
and 8b show forwarding-table size and control overhead, respec-
tively, measured over one-second intervals. We can see that SEAT-
TLE has much lower control overhead when the systems are ﬁrst
started up. However, SEATTLE’s performance advantages do not
come from cold-start effects, as it retains lower control overhead
even after the system converges. As a side note, the forwarding-
table size in Ethernet is not drastically larger than that of SEAT-
TLE in this experiment because we are running on a small four
node topology. However, since the topology has ten links (includ-
ing links to hosts), Ethernet’s control overhead remains substan-
tially higher. Additionally, we also investigate performance by in-
jecting host scanning attacks [17] into the real traces we used for
evaluation. Figure 8b includes the scanning incidences occurred at
around 300 and 600 seconds, each of which involves a single host
scanning 5000 random destinations that do not exist in the network.
In Ethernet, every scanning packet sent to a destination generates a
network-wide ﬂood because the destination is not existing, result-
ing in sudden peaks on it’s control overhead curve. In SEATTLE,
each scanning packet generates one unicast lookup (i.e., the scan-
ning data packet itself) to a resolver, which then discards the packet.
Fail-over performance: Figure 8c shows the effect of switch fail-
ure. To evaluate SEATTLE’s ability to quickly republish host infor-
mation, here we intentionally disable caching, induce failures of the
resolver switch, and measure throughput of TCP when all packets
are forwarded through the resolver. We set the OSPF hello inter-
val to 1 second, and dead interval to 3 seconds. After the resolver
fails, there is some convergence delay before packets are sent via
the new resolver. We found that SEATTLE restores connectivity
quickly, typically on the order of several hundred milliseconds after
the dead interval. This allows TCP to recover within several sec-
onds, as shown in Figure 8c-i. We found performance during fail-
ures could be improved by having the access switch register hosts
with the next switch along the ring in advance, avoiding an addi-
tional re-registration delay. When a switch is repaired, there is also
a transient outage while routes move back over to the new resolver,
as shown in Figure 8c-ii. In particular, we were able to improve
convergence delay during recoveries by letting switches continue
to forward packets through the old resolver for a grace period. In
contrast, optimizing Ethernet to attain low (a few sec) convergence
delay exposes the network to a high chance of broadcast storms,
making it nearly impossible to realize in a large network.
8. CONCLUSION
Operators today face signiﬁcant challenges in managing and
conﬁguring large networks. Many of these problems arise from the
complexity of administering IP networks. Traditional Ethernet is
Figure 7: Implementation architecture.
ery remote host cached via previous lookups. For each insertion
or deletion of a locally-attached host, the switch generates a corre-
sponding registration or deregistration message. Additionally, by
monitoring the changes of the NextHopTable, the switch can de-
tect whether the topology has changed, and host re-registration is
required accordingly. To maintain IP-to-MAC mappings to support
ARP, a switch also maintains a separate table in the control plane.
This table contains only the information of local hosts and remote
hosts that are speciﬁcally hashed to the switch. When our prototype
switch is ﬁrst started up, a simple neighbor-discovery protocol is
run to determine which interfaces are connected to other switches,
and over each of these interfaces it initiates an OSPF session. The
link weight associated with the OSPF adjacency is by default set to
be the link latency. If desired, another metric may be used.
SEATTLE data plane: To forward packets, an ingress switch ﬁrst
learns an incoming packet’s source MAC address, and if necessary,
adds the corresponding entry in HostLocTable. Then the switch
looks up the destination MAC address in the HostLocTable and
checks to see if i) the host is locally attached, ii) the host is remote,
and its location is cached, or iii) the host is explicitly registered
with the switch. In the case of iii) the switch needs to send a host
location notiﬁcation to the ingress.
In all cases, the switch then
forwards the packet either to the locally attached destination, or en-
capsulates the packet and forwards it to the next hop toward the
destination. Intermediate switches can then simply forward the en-
capsulated packet by looking up the destination in their NextHopT-
ables. In addition, if the incoming packet is an ARP request, the
ingress switch executes the hash function F to look up the corre-
sponding resolver’s id, and re-writes the destination to that id, and
delivers the packet to the resolver for resolution.
7.2 Experimental results
Next, we evaluate a deployment of our prototype implementa-
tion on Emulab. To ensure correctness, we cross-validated the sim-
ulator and implementation with various traces and topologies, and
found that average stretch, control overhead, and table size from
implementation results were within 3% of the values given by the
simulator. We ﬁrst present a set of microbenchmarks to evaluate
per-packet processing overheads. Then, to evaluate dynamics of a
SEATTLE network, we measure control overhead and switch state
requirements, and evaluate switch fail-over performance.
Packet processing overhead: Table 1 shows per-packet processing
time for both SEATTLE and Ethernet. We measure this as the time
from when a packet enters the switch’s inbound queue, to the time
it is ready to be moved to an outbound queue. We break this time
down into the major components. From the table, we can see that
an ingress switch in SEATTLE requires more processing time than
in Ethernet. This happens because the ingress switch has to encap-
sulate a packet and then look up the next-hop table with the outer
header. However, SEATTLE requires less packet processing over-
head than Ethernet at non-ingress hops, as intermediate and egress
switches do not need to learn source MAC addresses, and consis-
Eth
SEA_CA
SEA_NOCA
20K
15K
10K
5K
s
e
h
c
t
i
w
s
l
l
a
s
s
o
r
c
a
s
e
i
r
t
n
e
f
o
.
m
u
N
0
0
100
200
)
g
o
l
(
s
e
h
c
t
i
w
s
l
l
a
s
s
o
r
c
a
s
e
g
a
s
s
e
m
f
o
.
m
u
N
400
500
600
Eth
SEA_CA
SEA_NOCA
Scans
10e4
10e3
100
10
1
0
100
200
400
500
600
300
Time in sec 
(a)
300
Time in sec
(b)
(c)
Figure 8: Effect of network dynamics: (a) table size (b) control overhead (c) failover performance.
not a viable alternative (except perhaps in small LANs) due to poor
scaling and inefﬁcient path selection. We believe that SEATTLE
takes an important ﬁrst step towards solving these problems, by
providing scalable self-conﬁguring routing. Our design provides
effective protocols to discover neighbors and operates efﬁciently
with its default parameter settings. Hence, in the simplest case,
network administrators do not need to modify any protocol settings.
However, SEATTLE also provides add-ons for administrators who
wish to customize network operation. Experiments with our initial
prototype implementation show that SEATTLE provides efﬁcient
routing with low latency, quickly recovers after failures, and han-
dles host mobility and network churn with low control overhead.
Moving forward, we are interested in investigating the deploy-
ability of SEATTLE. We are also interested in ramiﬁcations on
switch architectures, and how to design switch hardware to efﬁ-
ciently support SEATTLE. Finally, to ensure deployability, this pa-
per assumes Ethernet stacks at end hosts are not modiﬁed. It would
be interesting to consider what performance optimizations are pos-
sible if end host software can be changed.
9. REFERENCES
[1] M. Arregoces and M. Portolani, Data Center Fundamentals. Cisco
Press, 2003.
[2] S. Halabi, Metro Ethernet. Cisco Press, 2003.
[3] H. Hudson, “Extending access to the digital economy to rural and
developing regions.” Understanding the Digital Economy, The MIT
Press, Cambridge, MA, 2002.
[4] A. Gupta, B. Liskov, and R. Rodrigues, “Efﬁcient routing for
peer-to-peer overlays,” in NSDI, March 2004.
[5] W. Aiello, C. Kalmanek, P. McDaniel, S. Sen, O. Spatscheck, and
J. van der Merwe, “Analysis of communities of interest in data
networks,” in Passive and Active Measurement, March 2005.
[6] R. Perlman, “Rbridges: Transparent routing,” in INFOCOM, March
2004.
[7] “IETF TRILL working group.” http://www.ietf.org/html.
charters/trill-charter.html.
[8] A. Myers, E. Ng, and H. Zhang, “Rethinking the service model:
scaling Ethernet to a million nodes,” in HotNets, November 2004.
[9] S. Sharma, K. Gopalan, S. Nanda, and T. Chiueh, “Viking: A
multi-spanning-tree Ethernet architecture for metropolitan area and
cluster networks,” in INFOCOM, March 2004.
[10] T. Rodeheffer, C. Thekkath, and D. Anderson, “SmartBridge: A
scalable bridge architecture,” in SIGCOMM, August 2000.
[11] C. Kim and J. Rexford, “Revisiting Ethernet: Plug-and-play made
scalable and efﬁcient,” in IEEE LANMAN, June 2007. invited paper
(short workshop paper).
[12] S. Ray, R. A. Guerin, and R. Soﬁa, “A distributed hash table based
address resolution scheme for large-scale Ethernet networks,” in
International Conference on Communications, June 2007.
[13] M. Caesar, T. Condie, J. Kannan, et al., “ROFL: Routing on Flat
Labels,” in SIGCOMM, September 2006.
[14] B. Ford, “Unmanaged Internet Protocol: Taming the edge network
management crisis,” in HotNets, November 2003.
[15] M. Caesar, M. Castro, E. Nightingale, et al., “Virtual Ring Routing:
Network routing inspired by DHTs,” in SIGCOMM, September 2006.
[16] R. Perlman, Interconnections: Bridges, routers, switches, and
internetworking protocols. Addison-Wesley, second ed., 1999.
[17] M. Allman, V. Paxson, and J. Terrell, “A brief history of scanning,” in
Internet Measurement Conference, October 2007.
[18] Dartmouth Institute for Security Technology Studies, “Problems with
broadcasts,” http://www.ists.dartmouth.edu/
classroom/crs/arp_broadcast.php.
[19] Z. Kerravala, “Conﬁguration management delivers business
resiliency,” November 2002. The Yankee Group.
[20] R. King, “Trafﬁc management tools ﬁght growing pains,” June 2004.
http://www.thewhir.com/features/
traffic-management.cfm.
[21] “IEEE Std 802.1Q - 2005, IEEE Standard for Local and Metropolitan
Area Network, Virtual Bridged Local Area Networks,” 2005.
[22] I. Stoica, D. Adkins, S. Zhuang, S. Shenker, and S. Surana, “Internet
indirection infrastructure,” in SIGCOMM, August 2002.
[23] F. Dabek, M. F. Kaashoek, D. Karger, R. Morris, and I. Stoica,
“Wide-area cooperative storage with CFS,” in SOSP, October 2001.
[24] D. Karger, E. Lehman, T. Leighton, et al., “Consistent hashing and
random trees: Distributed caching protocols for relieving hot spots
on the world wide web,” in ACM STOC, 1997.
[25] B. Godfrey, K. Lakshminarayanan, S. Surana, R. Karp, and I. Stoica,
“Load balancing in dynamic structured P2P systems,” in INFOCOM,
March 2003.
[26] W. Adjie-Winoto, E. Schwartz, H. Balakrishnan, and J. Lilley, “The
design and implementation of an intentional naming system,” in
SOSP, December 1999.
[27] J. Moy, OSPF: Anatomy of an Internet Routing Protocol.
Addison-Wesley, 1998.
[28] R. Hinden, “Virtual Router Redundancy Protocol (VRRP).” RFC
3768, April 2004.
[29] “Gratuitous ARP.”
http://wiki.ethereal.com/Gratuitous_ARP.
[30] R. Droms, “Dynamic Host Conﬁguration Protocol.” Request for
Comments 2131, March 1997.
[31] C. Tengi, J. Roberts, J. Crouthamel, C. Miller, and C. Sanchez,
“autoMAC: A tool for automating network moves, adds, and
changes,” in LISA Conference, 2004.
[32] D. Hucaby and S. McQuerry, Cisco Field Manual: Catalyst Switch
Conﬁguration. Cisco Press, 2002.
[33] R. Pang, M. Allman, M. Bennett, J. Lee, V. Paxson, and B. Tierney,
“A ﬁrst look at modern enterprise trafﬁc,” in Internet Measurement
Conference, October 2005.
[34] N. Spring, R. Mahajan, and D. Wetherall, “Measuring ISP topologies
with Rocketfuel,” in SIGCOMM, August 2002.
[35] “Skitter.” http:
//www.caida.org/tools/measurement/skitter.
[36] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. Kaashoek, “The
Click modular router,” in ACM Trans. Comp. Sys., August 2000.
[37] M. Handley, E. Kohler, A. Ghosh, O. Hodson, and P. Radoslavov,
“Designing extensible IP router software,” in NSDI, May 2005.
[38] D. Pal, “Faster Packet Forwarding in a Scalable Ethernet
Architecture.” TR-812-08, Princeton University, January 2008.