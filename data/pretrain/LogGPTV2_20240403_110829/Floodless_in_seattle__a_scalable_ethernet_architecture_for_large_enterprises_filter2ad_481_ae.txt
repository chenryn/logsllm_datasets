### Implementation

To evaluate the performance and practicality of SEATTLE, we deployed a prototype switch using two open-source routing software platforms: user-level Click [36] and XORP [37]. We also implemented a second version using kernel-level Click [38]. Section 7.1 details the design of our prototype, while Section 7.2 presents the evaluation results.

#### 7.1 Prototype Design

Figure 7 illustrates the overall structure of our implementation. SEATTLE's control plane is divided into two functional modules: (i) maintaining the switch-level topology and (ii) managing end-host information. We used XORP to implement the first module and Click for the second. Additionally, we extended Click to handle SEATTLE's data-plane functions, including consistent hashing and packet encapsulation. Our modifications to Click for both the control and data planes are encapsulated in the SeattleSwitch element, as shown in Figure 7.

**SEATTLE Control Plane:**
- **Topology Maintenance:** Each switch runs a XORP OSPF process to maintain a complete switch-level network map. The XORP RIBD (Routing Information Base Daemon) constructs its routing table using this map and installs it into the forwarding plane, which we implement with Click. Click uses this NextHopTable to determine the next hop. The FEA (Forwarding Engine Abstraction) in XORP handles inter-process communication between XORP and Click.
- **Host Information Management:** A SeattleSwitch maintains a HostLocTable, which stores three types of host information: (a) the outbound port for every local host, (b) the location for every remote host for which this switch is a resolver, and (c) the location for every remote host cached via previous lookups. For each insertion or deletion of a locally-attached host, the switch generates a corresponding registration or deregistration message. By monitoring changes in the NextHopTable, the switch can detect topology changes and initiate host re-registration if necessary. To support ARP, the switch also maintains a separate table in the control plane containing IP-to-MAC mappings for local hosts and remote hosts specifically hashed to the switch.

**SEATTLE Data Plane:**
- **Packet Forwarding:** An ingress switch first learns the source MAC address of an incoming packet and, if necessary, adds the corresponding entry to the HostLocTable. It then looks up the destination MAC address in the HostLocTable to determine if the host is (i) locally attached, (ii) remote with a cached location, or (iii) explicitly registered with the switch. If the host is explicitly registered, the switch sends a host location notification to the ingress. The packet is then forwarded either to the locally attached destination or encapsulated and sent to the next hop. Intermediate switches forward the encapsulated packet by looking up the destination in their NextHopTables. If the incoming packet is an ARP request, the ingress switch executes the hash function F to find the corresponding resolver’s ID, rewrites the destination, and delivers the packet to the resolver for resolution.

#### 7.2 Experimental Results

We evaluated our prototype implementation on Emulab. To ensure correctness, we cross-validated the simulator and implementation with various traces and topologies, finding that the average stretch, control overhead, and table size from the implementation were within 3% of the values given by the simulator.

**Packet Processing Overhead:**
- Table 1 shows the per-packet processing time for both SEATTLE and Ethernet. The time is measured from when a packet enters the switch’s inbound queue to when it is ready to be moved to an outbound queue. The breakdown of this time is provided in the table. An ingress switch in SEATTLE requires more processing time than in Ethernet due to the need for packet encapsulation and next-hop table lookup. However, SEATTLE has less packet processing overhead at non-ingress hops because intermediate and egress switches do not need to learn source MAC addresses, and consistent hashing (which takes around 2.2 μs) is only required for ARP requests. This results in SEATTLE requiring less overall processing time on paths longer than 3.03 switch-level hops. In a real university campus network, the average number of switch-level hops between hosts was found to be over 4 for the vast majority of host pairs. Using our kernel-level implementation of SEATTLE, we were able to fully saturate a 1 Gbps link.

**Effect of Network Dynamics:**
- To evaluate the dynamics of SEATTLE and Ethernet, we instrumented the switch’s internal data structures to periodically measure performance information. Figures 8a and 8b show the forwarding-table size and control overhead, respectively, measured over one-second intervals. SEATTLE has lower control overhead, especially during system startup. Even after the system converges, SEATTLE retains lower control overhead. The forwarding-table size in Ethernet is not drastically larger in this experiment because we are running on a small four-node topology. However, Ethernet’s control overhead remains substantially higher due to the ten links (including links to hosts). We also injected host scanning attacks [17] into the real traces used for evaluation. Figure 8b includes scanning incidences at around 300 and 600 seconds, each involving a single host scanning 5000 random destinations that do not exist in the network. In Ethernet, each scanning packet generates a network-wide flood, resulting in sudden peaks in control overhead. In SEATTLE, each scanning packet generates one unicast lookup to a resolver, which then discards the packet.

**Fail-over Performance:**
- Figure 8c shows the effect of switch failure. To evaluate SEATTLE’s ability to quickly republish host information, we intentionally disabled caching, induced failures of the resolver switch, and measured TCP throughput. With the OSPF hello interval set to 1 second and the dead interval to 3 seconds, SEATTLE restores connectivity quickly, typically within several hundred milliseconds after the dead interval. This allows TCP to recover within several seconds, as shown in Figure 8c-i. Performance during failures can be improved by having the access switch pre-register hosts with the next switch along the ring, avoiding additional re-registration delays. When a switch is repaired, there is a transient outage while routes move back to the new resolver, as shown in Figure 8c-ii. Convergence delay during recovery can be improved by allowing switches to continue forwarding packets through the old resolver for a grace period. In contrast, optimizing Ethernet for low convergence delay (a few seconds) exposes the network to a high chance of broadcast storms, making it nearly impossible to realize in a large network.

### Conclusion

Today, operators face significant challenges in managing and configuring large networks, many of which arise from the complexity of administering IP networks. Traditional Ethernet is not a viable alternative (except perhaps in small LANs) due to poor scaling and inefficient path selection. SEATTLE takes an important step towards solving these problems by providing scalable self-configuring routing. Our design offers effective protocols for neighbor discovery and operates efficiently with default parameter settings. In the simplest case, network administrators do not need to modify any protocol settings. However, SEATTLE also provides add-ons for administrators who wish to customize network operation. Experiments with our initial prototype implementation show that SEATTLE provides efficient routing with low latency, quickly recovers after failures, and handles host mobility and network churn with low control overhead. Moving forward, we are interested in investigating the deployability of SEATTLE, its impact on switch architectures, and how to design switch hardware to efficiently support SEATTLE. To ensure deployability, this paper assumes that Ethernet stacks at end hosts are not modified. It would be interesting to consider what performance optimizations are possible if end host software can be changed.

### References

[References listed as in the original text]

---

This revised version aims to provide a clearer, more coherent, and professional presentation of the SEATTLE implementation and evaluation.