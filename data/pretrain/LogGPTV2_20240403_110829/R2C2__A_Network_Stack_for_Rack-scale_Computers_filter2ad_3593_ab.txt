different routing protocols to be used simultaneously,
chosen on a per-application or even on a per-ﬂow ba-
sis. Datacenter operators can leverage this knob to op-
timize metrics like the rack’s aggregate throughput.
2.2.2 Rack network sharing
High path diversity, combined with the resource limited
nature of micro-servers, also means that congestion control
protocols used in today’s datacenters are not suited for rack-
scale computers. For example, a ﬂow using minimal routing
is routed along all shortest paths between its source and des-
tination. This means that even in a small 216-node rack with
3D torus topology, an average ﬂow has a 1,680 paths. Fur-
thermore, the number of paths increases exponentially with
the topology size. And with non-minimal routing, the path
diversity can be nearly unbounded [20].
The TCP family of protocols, including recent proposals
targeted towards datacenters [2–4], only uses a single path
and imposes high processing overhead [34]. Even multi-path
extensions like MPTCP [41] only consider few tens of paths.
This is roughly two-three orders of magnitude smaller than
the number of paths available here. Furthermore, per-ﬂow
fairness, as provided by TCP and its variants, is inadequate
for datacenter settings. Instead, operators need to enforce
richer policies like deadline-based fairness for user-facing
applications [28, 48] and per-tenant guarantees [10, 11, 30].
In recent work, Fastpass [36] shows the feasibility of cen-
tralized congestion control in traditional datacenters. While
the scale of rack-scale computers is amenable to a central-
ized design, this would introduce signiﬁcant communication
overhead, as we show in Section 5.2. Further, high path di-
versity in racks makes computing max-min fair rate alloca-
tions much harder than in traditional topologies (§3.3). In
RascNet [16], we sketched a preliminary congestion control
design for racks that works atop VLB routing. However,
this contravenes the routing ﬂexibility requirement. In Sec-
tion 5.2, we show that taking advantage of routing ﬂexibility
provides signiﬁcant performance gains.
At the other end of the spectrum, HPC platforms and on-
chip networks (NoC) systems often use congestion control
mechanisms customized to the underlying topology and the
expected workload [17, 18, 35]. Such mechanisms do not
work for general workloads. We also note that ideal network
sharing can be achieved through per-ﬂow queues at each
rack node coupled with back-pressure notiﬁcations when a
node’s queues start ﬁlling up. Apart from increased for-
warding complexity, this massively increases the buffering
requirements at rack nodes.
Thus, the design goals for rack network sharing are:
G2 Accommodate high multi-pathing. Congestion control
553has to cope with high multi-pathing in rack topologies.
G3 Low network queuing. While a standard goal for net-
work design, this is particularly important here be-
cause micro-servers have limited buffers and the net-
work carries trafﬁc that is very latency sensitive.
G4 Allocation ﬂexibility. Datacenter operators should be
able to specify different rate allocation policies that en-
capsulate varying notions of fairness.
3. DESIGN
data-plane design intentionally places more functionality at
the sender that only needs to rate limit its own ﬂows. This
can be implemented in software or hardware [39].
Inter-
mediate rack nodes have a simple forwarding layer that is
amenable to on-chip implementation; it does not require any
additional rate limiting or complex queuing mechanisms.
To give an overview of R2C2’s operation, we begin by fo-
cusing on the life of a single ﬂow. We then describe R2C2’s
data- and control-plane mechanisms in detail.
3.1 The life of a ﬂow
We present R2C2, a network stack for rack-scale com-
puters. The key insight behind R2C2’s design is that while
rack topologies pose many challenges, they also present an
opportunity—it is possible to efﬁciently broadcast informa-
tion across the rack. We use 16-byte broadcast packets
(§4.2); with a 512-node rack, each broadcast results in 8 KB
of total trafﬁc, aggregated across all rack links. By broad-
casting ﬂow start and termination events, we ensure every
node knows the rack’s global trafﬁc matrix. R2C2’s lever-
ages such global visibility to implement ﬂexible congestion
control and routing.
Determining the rates and routes for ﬂows across a multi-
path topology can be mapped to the multi-commodity ﬂow
(MCF) problem with splittable ﬂows. Several papers pro-
pose polynomial time algorithms for this problem with dif-
ferent optimization objectives such as max-min fairness [33]
and maximizing total throughput [8]. Many of these algo-
rithms are designed for ofﬂine operation; thus they are com-
putationally intensive and have a high running time. Rack
nodes, however, have limited compute and buffering, so at
least the congestion needs to be controlled at a ﬁne-grained
timescale. On the other hand, online MCF algorithms are
tightly tied to a speciﬁc optimization metric.
R2C2’s control plane decouples congestion control and
routing, and tackles them at different timescales. For con-
gestion control, each node uses global visibility and knowl-
edge of the underlying topology to locally compute the fair
sending rate for its trafﬁc (§3.3). This design avoids any net-
work probing and does not require any specialized queuing
support at the rack nodes. The rate computation algorithm
is fast, ensures low network queuing, and allows for differ-
ent rate allocation policies. For routing, nodes locally deter-
mine the routing protocol for each ﬂow that will maximize a
provider-chosen global utility metric (§3.4).
At the data-plane, R2C2 uses source routing to enable per-
ﬂow routing protocols. This involves three mechanisms: (i).
the node sending a ﬂow determines the path for each packet
based on the ﬂow’s routing protocol and encodes this path
in the packet header, (ii). the sender also enforces a ﬂow’s
rate allocation, (iii). intermediate rack nodes simply forward
packets along the path speciﬁed in their header.
Overall, our design choices are guided by the expected
size of rack-scale computers. Their scale ensures both the
network overhead of broadcast trafﬁc and the processing
overhead of rate computation is acceptable, even for very
bursty workloads. The scale also means that a packet’s path
can be encoded compactly, allowing for source routing. The
When a ﬂow starts,
its sender broadcasts information
about the new ﬂow, including the routing algorithm the ﬂow
is using and its rate allocation parameters (e.g., the ﬂow’s
weight). Each rack node stores this information to create a
local view of the global trafﬁc matrix. Given this trafﬁc ma-
trix and the rack’s topology, the sender computes the ﬂow’s
fair allocation and rate limits it accordingly. To account for
temporary discrepancies between the perceived and actual
trafﬁc matrix, R2C2 relies on bandwidth headroom; dur-
ing rate computation, we simply subtract the headroom from
each link’s capacity.
For every new packet, the source encodes the packet path
in its header, and the packets are source routed to their desti-
nation. When a broadcast packet (e.g., due to a ﬂow starting
or ﬁnishing) is received, the sender recomputes the rate for
all its own ﬂows. When the ﬂow ﬁnishes, other rack nodes
are informed by broadcasting this event. Nodes also peri-
odically check whether the overall utility would improve if
some of the ﬂows were routed using a different protocol. If a
signiﬁcant improvement is possible, their routing protocols
are changed and this information is broadcasted.
3.2 Broadcast
R2C2’s design builds upon a low overhead broadcast
primitive. For broadcasting packets, we create a per-source
broadcast tree atop the rack’s network topology. This can be
done while optimizing various goals; we optimize the broad-
cast time, i.e., we minimize the maximum number of net-
work hops within which all rack nodes receive a copy of the
broadcast packet.
To achieve this goal, we determine the shortest-path tree
for each rack node. Given a graph representing the rack’s
topology, a shortest-path tree rooted at source node s is a
spanning tree T of the graph such that the length of the path
from s to any node in T is the shortest distance from s to the
node in the graph.2 Since all network links inside the rack
have the same capacity, ﬁnding shortest-path trees for the
rack is akin to ﬁnding shortest-path trees for an unweighted
graph, and can be done through a breadth-ﬁrst traversal of
the graph [14].
For a rack, we enumerate multiple broadcast trees for each
source by traversing the rack’s topology in a breadth-ﬁrst
fashion. Given this, we construct a broadcast forwarding
information base (FIB) for each rack node. A look-up in
this FIB is indexed by a two-tuple, , comprising the address of the source node and an iden-
tiﬁer for the broadcast tree, and yields the set of next-hop
nodes the broadcast packet should be forwarded to.
When a node has to send a broadcast packet, it chooses
one of its broadcast trees for the packet to be routed along.
This selection is done to load balance the broadcasting over-
head, and allows the sender to account for link and node
failures. The sender inserts its address and the identiﬁer for
the chosen broadcast tree into the header of the broadcast
packet. The packet is then routed by other nodes by consult-
ing their broadcast FIB.
Broadcast overhead.
In Section 4.2, we describe how
ﬂow information is encoded into a 16-byte broadcast packet.
Each broadcast tree for a rack with n nodes comprises n− 1
edges. Thus, for a typical rack with 512 nodes, a single
broadcast results in a total of 511*16 ≈ 8 KB on the wire. In
the worst-case scenario of ﬂows between all pairs of nodes
(≈262K ﬂows), the resulting broadcast trafﬁc per link would
be 681 KB.
An obvious concern is that in many datacenter workloads,
most ﬂows are only a few packets long. For example, in
a typical data-mining workload [25], 80% of ﬂows are less
then 10KB. The average path length for a ﬂow in a 512-node
3D torus is 6 hops, so a 10 KB ﬂow will, on average, result
in 60 KB being transmitted on the wire. Thus, the relative
overhead of broadcasting the start and ﬁnish events for such
small ﬂows is 26.66% (13.33% for each event). Fortunately,
small ﬂows only carry a small fraction of the bytes in today’s
datacenters. For example, traces from a data analytics cluster
show that the 95% of all bytes originate from less than 4%
of ﬂows [25].
In Section 5.2, we analyze the fraction of
network bandwidth used for broadcast trafﬁc as a function of
the bytes carried by small ﬂows. When 5% of the bytes are
carried by small ﬂows, the fraction of the network capacity
used for broadcasting ﬂow information is only 1.3%.
Failures. Broadcast packets can be corrupted across the net-
work, or lost due to drops and failures. To detect corruption,
we rely on a packet checksum. To detect drops due to queue
overﬂows at intermediate nodes, the node dropping a broad-
cast packet informs the sender who can then re-transmit. To
detect link and node failures, we rely on a topology discov-
ery mechanism that is required by the routing protocols any-
way. Upon detecting a failure, nodes broadcast information
about all their ongoing ﬂows. This is reasonable because,
given the scale of rack-scale computers, we expect node and
link failures to be infrequent. For example, measurements
across HPC systems have shown a failure rate of around 0.3
faults per year per CPU [43]. For a 512-node rack with four
CPUs per node, this means less than two failures a day.
3.3 Rack congestion control
To ensure the network is not congested and ﬂows achieve
rates in accordance to the operator’s allocation policy, we re-
quire senders to compute the rate allocations for their trafﬁc
and enforce them. The basic idea behind our approach is
that given knowledge of the allocation policy, the network
topology, all active ﬂows, and their current routing protocol,
Figure 3: A ﬂow from node 0 to 3, with weight w, being
routed using randomized packet spraying.
each node can independently determine the load on each net-
work link and hence, the fair sending rate for its ﬂows. Thus,
we transform the distributed congestion control problem into
one of local rate calculation. While the rack’s topology is
relatively static, the set of active ﬂows can change rapidly.
We begin with a strawman design which assumes that nodes
are aware of the rack’s current trafﬁc matrix.
The key challenge in computing rate allocations is ac-
commodating high multi-pathing. Flows are routed across a
very high number of paths, thousands or more, which poses
a computational burden. For example, consider a provider
who wants to allocate the network fairly. Max-min fairness
is well studied in the context of single-path routing but is
harder to reason about in multi-path settings. Max-min Pro-
gramming (MP) [40] is a centralized algorithm that uses a
linear program to compute max-min fair allocations across
general networks. However, using the MP algorithm in our
setting would result in a linear program with an exponential
complexity solution—each ﬂow takes many paths, each path
would be represented by a separate variable.
To make rate computation tractable, we leverage the sim-
ple insight that a ﬂow’s routing protocol dictates its rela-
tive rate across its paths. This holds for the routing proto-
cols we studied. For instance, consider the example in Fig-
ure 3, in which a ﬂow from 0 to 3 is routed using random
packet spraying [22] across a 2x2 mesh topology. There are
two shortest paths between the source and the destination,
0 → 1 → 3 and 0 → 2 → 3, which are chosen randomly on
a per-packet basis and hence, are used equally. Therefore,
the total rate allocated to the ﬂow should be evenly divided
across these two paths.
Overall, the key observation that a ﬂow’s routing protocol
dictates its relative rate across multiple paths allows us to
compute rate allocations at the ﬂow-level, irrespective of the
number of paths each ﬂow is routed along.
3.3.1 Congestion control: a strawman design
R2C2’s congestion control involves each node indepen-
dently computing the max-min fair rate for each of its own
ﬂows. This comprises two steps. First, we use information
about a ﬂow’s routing protocol to determine the relative rate
of the ﬂow across the paths it is using and, hence, along each
network link it is using. We explain this with an example.
To achieve per-ﬂow fairness, each ﬂow is assigned the same
allocation weight. Given the ﬂow’s source, destination and
0123Flow from node 0 to 3Two paths: [0,1,3], [0,2,3]Weight on each link:[0,1] = [1,3] = [0,2] = [2,3] = w/2[1,0] = [3,1] = [2,0] = [3,2] = 0w/2w/2w/2w/2555Figure 4: Flow f 1 from node 1 to 4, and f 2 from node 2 to 4.
Respecting the relative rates dictated by the routing protocol
changes the feasible set of rates from (b) to (c). The asterix
denotes the max-min fair allocation.
routing protocol, we can determine its weight along any net-
work link. For instance, the ﬂow in Figure 3 is using both
minimal paths equally, so its weight on each link that it is
using is w
2 . The ﬂow’s weight on each link it is not using
(e.g., the link 1 → 0 in the ﬁgure) is zero. As another ex-
ample, assume the ﬂow was using WLB routing and using
the two paths, 0 → 1 → 3 and 0 → 2 → 3, in the ratio 1:2.
Then the ﬂow’s weight at links 0 → 1 and 1 → 3 is w
3 , and
its weight at links 0 → 2 and 2 → 3 is 2w
3 . We can repeat the
above process for all ﬂows to determine per-ﬂow weights at
all network links.
Second, given this setup, we compute max-min fair allo-
cations through a weighted version of the water-ﬁlling algo-
rithm [12] that we summarize here: the rates for all ﬂows
are increased at the same pace, until one or more links be-
come saturated. Note that a ﬂow’s rate across a given link
is a product of the ﬂow’s total rate and its weight across the
link. This saturated link(s) is the bottleneck, and its capac-
ity dictates the rates for all ﬂows through it. The rates for
these ﬂows are frozen and they marked as allocated. The
algorithm continues till all ﬂows have been allocated. The
complexity of this algorithm is O(NL + N2), where N is the