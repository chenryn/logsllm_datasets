condition. A query is linear-in-state if all its aggregation functions
are linear-in-state.
3.3 Scalable aggregation functions
A groupby with no emit() and a linear-in-state (or associative)
aggregation function can be implemented scalably without losing ac-
curacy. Examples of such aggregations (from Fig. 7) include tracking
successive packets within a TCP connection that are out-of-sequence
and counting the number of TCP timeouts per connection.
If a groupby uses an emit() to pass tuples to another query, it
cannot be implemented scalably even if its aggregation function is
linear-in-state or associative. An emit() outputs the current state of
the aggregation function, which assumes the current state is always
available in the switch’s on-chip cache. This is only possible if flows
are never evicted, effectively shrinking the key-value store to its
on-chip cache alone.
3.4 Handling non-scalable aggregations
While the linear-in-state and associative conditions capture several
aggregation functions and enable a scalable implementation, there
are two practical classes of queries that we cannot scale: (1) queries
with aggregation functions that are neither associative nor linear-in-
state and (2) queries where the groupby has an emit() statement.
An example of the first class is the TCP non-monotonic query
discussed earlier. An example of the second class is the flowlet size
histogram query from Fig. 7, where the first groupby emits flowlet
sizes, which are grouped into buckets by the second groupby.
There are workarounds for non-scalable queries. One is to rewrite
queries to remove emit()s. For instance, we can rewrite the loss
rate query (Fig. 7) to independently record the per-flow counts for
dropped packets and total number of packets in separate key-value
stores, and have an operator consult both key-value stores every
time they need the loss rate. Each key-value store can be scaled, but
the implementation comes at a transient loss of accuracy relative
to precisely tracking the loss rate after every packet using a zip.
Second, an operator may be content with flow values that are accurate
for each time period between two evictions, but not across evictions
(Fig. 10b). Third, an operator may want to run a query to collect data
until the on-chip cache fills up and then stop data collection. Finally,
if the number of keys is small enough to fit in the cache (e.g., if the
key is an application type), the system can provide accurate results
without evicting any keys.
3.5 A unified condition for mergeability
We present a general condition that separates mergeable functions
from non-mergeable ones. Informally, mergeable aggregation func-
tions are those that maintain auxiliary state linear in the size of the
function’s state itself. This characterization also has the benefit of
unifying the associative and linear-in-state conditions. We now for-
malize our results in the form of several theorems without proofs;
an accompanying technical report [15] contains the proofs.
Let n denote the size of state (in bits) tracked in a Marple query: it
must be bounded and should not increase with the number of packets.
When merging state scache in the on-chip cache with state sbacking
in the backing store, the switch may maintain and send auxiliary
state aux for the backing store to perform the merge correctly. In the
EWMA example, the value (1−α )N is auxiliary state. Then, a merge
function m for an aggregation function f is a function satisfying:
m(scache,aux,sbacking) = f (s0,{p1, . . . , pN})
for any N and sequence of packets p1, . . . , pN. The application of f
to a list is shorthand for folding f over each packet in order.
First, we show that every aggregation function has a merge func-
tion, provided it is allowed to use a large amount of auxiliary data.
THEOREM 3.1. Every aggregation function has a corresponding
merge function that uses O(n2n) auxiliary bits.
Unfortunately, memory is limited and Marple should not use much
more state than indicated by the user’s aggregation function. We say
an aggregation function is mergeable if the auxiliary state has size
O(n) for any sequence of packets. This characterization is consistent
with what we have described so far: the linear-in-state and associative
conditions are indeed mergeable by this definition, while queries
that we cannot merge (e.g., TCP non-monotonic in Fig. 7) violate it.
THEOREM 3.2. If an aggregation function is either linear-in-
state or associative, it has a merge function that uses O(n) bits of
auxiliary state.
THEOREM 3.3. The TCP non-monotonic query from Fig. 7 re-
quires Θ(n2n) auxiliary bits in the worst case.
This raises the question: can we determine whether an aggregation
function is mergeable with O(n) auxiliary bits? We provide an al-
gorithm (described in the tech report) that computes the minimum
auxiliary state size needed to merge a given aggregation function.
Our current algorithm uses brute force and is doubly exponential in n.
Language-Directed Hardware Design
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
However, a polynomial time algorithm is unlikely. We demonstrate
a hardness result by considering a decision version of a simpler
version of this problem where the merge function m is given as
input: given an aggregation function f and merge function m, does
m successfully merge f for all possible packet inputs?
THEOREM 3.4. Determining whether a merge function success-
fully merges an aggregation function is co-NP-hard.
The practical implication of this result is that there is unlikely to be
a general and efficient procedure to check if an arbitrary aggregation
function can be merged using a small amount of auxiliary state.
Thus, identifying specific classes of functions (e.g., linear-in-state
and associative) and checking if an aggregation function belongs to
these classes is the best we can hope to do.
3.6 Hardware feasibility
We optimize our stateful hardware design for linear-in-state queries
and break it down into five components. Each component is well-
known; our main contribution is putting them together to implement
stateful queries. We now discuss each component in detail.
The on-chip cache is a hash table where each row in the hash
table stores keys and values for a certain number of flows. If a packet
from a new flow hashes into a row that is full, the least recently used
flow within that row is evicted. Each row has 8 flows and each flow
stores both its key and value.4 Our choice of 8 flows is based on
8-way L1 caches, which are very common in processors [14]. This
cache eviction policy is close to an ideal but impractical policy that
evicts the least recently used (LRU) flow across the whole table (§5).
Within a switch pipeline stage, the on-chip cache has a logical
interface similar to an on-chip hash table used for counters: each
packet matches entries in the table using a key extracted from the
packet header, and the corresponding action (i.e., increment) is exe-
cuted by the switch. An on-chip hash table may be used as a path
to incrementally deploying a switch cache for specific aggregations
(e.g., increments), on the way to supporting more general actions
and cache eviction logic in the future.
The off-chip backing store is a scale-out key-value store such
as Redis [20] running on dedicated collection servers within the
network. As §5 shows, the number of measurement servers required
to support typical eviction rates from the switch’s on-chip cache is
small, even for a 64×100-Gbit/s switch.
Maintaining packet history. Before a packet reaches the
pipeline stage with the on-chip cache, we use the preceding stages
to precompute A(p) and B(p) (the functions of bounded packet his-
tory) in the state-update operation S = A(p) · S + B(p). Our current
design only handles the case where S, A(p), and B(p) are scalars.
Say A(p) and B(p) depend on packet fields from the last k packets.
Then, these preceding pipeline stages act like a shift register and
store fields from the last k packets. Each stage contains a read/write
register, which is read by a packet arriving at that stage, carried by
the packet as a header, and written into the next stage’s register. Once
values from the last k packets have been read into packet fields, A(p)
and B(p) can be computed with stateless instructions provided by
programmable switch architectures [33, 56].
def oos_count([count, lastseq], [tcpseq, payload_len]):
if lastseq != tcpseq:
count = count + 1
emit()
lastseq = tcpseq + payload_len
tcps
= filter(pktstream, proto == TCP
and (switch == S1 or switch == S2));
tslots = map(pktstream, [tin/epoch_size], [epoch]);
joined = zip(tcps, tslots);
oos
= groupby(joined,
[5tuple, switch, epoch],
oos_count);
Figure 4: Running example for Marple compiler (§4).
Carrying out the linear-in-state operation. Once A(p) and
B(p) are known, we use a multiply-accumulate (MAC) instruc-
tion [17] to compute A(p) · S + B(p). This instruction is very cheap
to implement: our circuit synthesis experiments show that a MAC
instruction meets timing at 1 GHz and occupies about 2000 µm2 in a
recent 32 nm transistor library. A switching chip with an area of a few
hundred mm2 can easily support a few hundred MAC instructions.
Queries that are not linear-in-state. We use the set of stateful
instructions developed in Domino [56] for queries that are not linear-
in-state. Our evaluations show that these instructions are sufficient
for our example queries that are not linear-in-state.
4 QUERY COMPILER
We compile Marple queries to two targets: the P4 behavioral
model [19], configured by emitting P4 code [18], and the Banzai
machine model, configured by emitting Domino code [56]. In both
cases, the emitted code configures a switch pipeline, where each
stage is a match-action table [33] or our key-value store.5
A preliminary pass of the compiler over the input query converts
the query to an abstract syntax tree (AST) of functional operators
(Fig. 5a). The compiler then:
(1) produces switch-local ASTs from a global AST (§4.1);
(2) produces P4 and Domino pipeline configurations from switch-
local ASTs (§4.2); and
(3) specifically recognizes linear-in-state aggregation functions,
and sets up auxiliary state required to merge such functions
for a scalable implementation (§4.3). To scalably implement
associative aggregation functions (§3.1), we use the program-
mer annotation assoc to determine if an aggregation is asso-
ciative. If it is associative, the merge function is the aggrega-
tion function itself.
We use the query shown in Fig. 4 as a running example to illustrate
the details in the compiler. The query counts the number of out-
of-sequence TCP packets over each time epoch, measured at two
switches S1 and S2 in the network.
4The LRU policy is actually implemented across 3-bit pointers that point to the keys
and values in a separate memory. So we shuffle only the 3-bit pointers for the LRU, not
the entire key and value.
5In this paper, we do not consider the problem of reconfiguring the switch pipeline on
the fly as queries change.
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Narayana et al.
Determining and propagating switch-partitioned through an AST
is straightforward. The base pktstream is not switch-partitioned.
The filter and zip operators produce a switch-partitioned stream
if their output only appears at a single switch. The groupby produces
a switch-partitioned stream if it aggregates by switch. In all other
cases, the operators retain the operands’ switch-partitioned attribute.
The switch-partitioned attributes for our running example are
shown in Fig. 5c. The filter produces output streams at two
switches, hence is not switch-partitioned. The groupby aggregates
by switch and hence is switch-partitioned. After the partitioning
checks have succeeded, we are left with a set of independent switch-
local ASTs corresponding to each switch location that the AST root
operator appears in, i.e., S1, S2.
4.2 Query AST to pipeline configuration
This compiler pass first generates a sequence of operators from the
switch-local query AST of §4.1. This sequence of operators will then
be used in the same order to generate a switch pipeline configuration.
There are two aspects that require care when constructing a pipeline
structure: (1) the pipeline should respect read-write dependencies
between different operators, and (2) repeated subqueries should not
create additional pipeline stages. We generate a sequence through
a post-order traversal of the query AST, which guarantees that the
operands of a node are added into the pipeline before the operator in
the node. Further, we deduplicate subquery results from the pipeline
to avoid repeating stages in the final output. For the running example,
the algorithm produces the sequence of operators: tcps (filter) →
tslots (map) → joined (zip) → oos (groupby).
Next, the compiler emits P4 code for a switch pipeline from the
operator sequence. The filter and zip configuration just involves
checking a predicate and setting a “valid” bit on the packet meta-
data. The map configuration assigns a packet metadata field to the
computed expression. The groupby configuration uses a register that
is indexed by the aggregation fields, and is updated through the
action specified in the aggregation function. We transform Marple
aggregation functions into straight-line code consisting of C-style
conditional operators through a standard procedure known as if-
conversion [29]. This allows us to fit the aggregation function into
the body of a single P4 action.
To target the Banzai switch pipeline simulator [56], the Marple
compiler emits Domino code by concatenating C-like code frag-
ments from all pipeline stages into a single Domino program. The
Domino compiler then takes this program and compiles it to a
pipeline of Banzai atoms. Atoms are ALUs representing a pro-
grammable switch’s instruction set. Atoms implement either state-
less (e.g., incrementing a packet field) or stateful (e.g., atomically
incrementing a switch counter) computations.
4.3 Handling linear-in-state aggregations
We now consider the problem of detecting if an aggregation function
is linear-in-state (i.e., updates to all state variables within the ag-
gregation function can be written as S = A(p) · S + B(p)). A general
solution to this problem is challenging because the aggregation func-
tion can take varied forms. For instance, the assignment S = S2−1
S−1 is
linear-in-state but needs algebraic simplifications to be detected.
(a)
(b)
(c)
Figure 5: Abstract Syntax Tree (AST) manipulations for the
running example (§4). (a) Operator AST. (b) Stream location
(set of switches). (c) Stream switch-partitioned (boolean).
4.1 Network-wide to switch-local queries
The compiler partitions a network-wide query written over all pack-
ets at all queues in the network (§2) into switch-local queries to
generate switch-specific configurations. We achieve this in two steps.
First, we determine the stream location, i.e., the set of switches that
contribute tuples to a stream, for the final output stream of query. For
instance, the output stream of a query that filters by switch id s has a
stream location equal to the singleton set s. Second, we determine
how to partition queries with aggregation functions written over the
entire network into switch-local queries.
Determining stream location for the final output stream.
The stream location of pktstream is the set of all switches in the
network. The stream location of the output of a filter is the set of
switches implied by the filter’s predicate. Concretely, we evaluate
the set of switches contributing tuples to the output of a filter oper-
ation through basic syntactic checks of the form switch == X on the
filter predicate. We combine switch sets for boolean combinators
(or and and) inside filter predicates using set operations (union and
intersection respectively). The stream location of the output of a zip
operator is the intersection of the stream locations of the two inputs.
Stream locations are unchanged by the map and groupby operators.
The stream locations for the running example are shown in Fig. 5b.
The stream location of pktstream is the set of all network switches,
but is restricted to just S1 and S2 by the filter in the query (left
branch). This location is then propagated to the root of the AST
through the zip operator in the query.
Partitioning network-wide aggregations. As described in §2,
we only permit aggregations that satisfy one of three conditions: they