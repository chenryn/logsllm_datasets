| ^𝑆𝑚𝑖𝑐 = 𝑧𝑒𝑟𝑜𝑠(𝑇 , 𝐹) |, 𝜔𝑤𝑠 = 2𝜋 × 𝜔
for 𝑡 = 1 : 𝑇 do
for 𝑓𝑚𝑖𝑐 = 700 : 3300 do
// Frequency selection
for 𝑁𝑠ℎ𝑖 𝑓 𝑡 = −10 : 10 do
𝑓𝑤 = |𝑓𝑚𝑖𝑐 − 𝑁𝑠ℎ𝑖 𝑓 𝑡 × 𝜔 |
if |𝑆𝑚𝑖𝑐 (𝑡𝑛, 𝑓𝑚) | > 70𝑑𝐵 & 𝑓𝑤 ≤ 𝑓𝑠 & 𝑓𝑤 > 0 then
// Amplitude selection
^𝑆𝑚𝑖𝑐 (𝑡𝑛, 𝑓𝑤) = ^𝑆𝑚𝑖𝑐 (𝑡𝑛, 𝑓𝑤) + |𝑆𝑚𝑖𝑐 (𝑡𝑛, 𝑓𝑚) |
// Spectrogram-based frequency conversion
end if
end for
end for
end for
end function
where 𝐹 = 𝑁/2 following Nyquist theorem. Next, we slide the
window by a step of 𝑝 samples and repeat the same steps to derive
the DT-STFT representation for each window. The time-frequency
features 𝑆 (i.e., spectrogram) are then obtained by combining the
DT-STFT representations ordered in time: 𝑆 = [𝑃0, .., 𝑃𝑇].
Feature Domain Conversion. To mitigate the impacts of huge
sampling rate gap between the microphone and the accelerome-
ter, we develop a feature domain conversion method to transform
the spectrograms in high-frequency audio domain to those in low-
frequency vibration domain. The conversion method takes com-
ponents in the audio domain spectrogram 𝑆𝑚𝑖𝑐(𝑡, 𝑓𝑚) as input and
calculates its new position (𝑡, 𝑓𝑤) in the vibration domain. The
original microphone frequency component 𝑓𝑚 is then mapped to
the low-frequency component 𝑓𝑤 based on Equation 1, with the
time index unchanged. If multiple spectrogram components are
overlapped at the same point, we accumulate their energy in the
new converted spectrogram. The conversion function is defined as:
inf
𝑛=− inf
^𝑆𝑚𝑖𝑐(𝑡, 𝑓𝑤) =
𝑆𝑚𝑖𝑐(𝑡, 𝑤𝑖𝑛(|𝑓𝑚 + 𝑛 × 𝜔|)),
(3)
where 𝜔 is the sampling rate of the accelerometer. Such conver-
sion maps each frequency component in the audio domain to an
appropriate frequency in the vibration domain, which makes the
cross-domain comparison possible.
Sensitive Feature Selection. In order to achieve reliable cross-
domain comparison, we study the frequency selectivity differences
of microphone and accelerometer and select the most sensitive
time-frequency features across the two sensors. Compared to mi-
crophone, wearable has different sensitivities to human speeches
across different frequency bands. We explore this phenomenon by
recording a chip signal of 0𝐻𝑧 ∼ 4𝑘𝐻𝑧 with a wearable (Huawei
2 sport) and a VA device (Nexus 6) and compare the similarity be-
tween the spectrograms of the accelerometer and the microphone
readings. Note that we have applied the feature domain conversion
method on the microphone’s spectrogram. We find that the spec-
trograms of the accelerometer and the microphone only show high
similarity within 700𝐻𝑧 ∼ 3300Hz, where the harmonics of human
speeches reside. We illustrate the comparison results in Appendix
Figure 3. To cope with the frequency selectivity differences, we
only use the spectrogram of microphone from 700𝐻𝑧 to 3300𝐻𝑧
for generating the low-frequency aliasing spectrogram. Further-
more, since the wearable’s accelerometer can only be triggered with
sound waves over 70𝑑𝐵 as shown in Section 5.2 (also confirmed in
Accelword [52]), we exclude the frequency components of the mi-
crophone spectrogram with energy below 70𝑑𝐵 for feature domain
conversion. The algorithm integrating feature domain conversion
and sensitive feature selection methods is presented in Algorithm 1.
Figure 9(b) shows an aliasing spectrogram of a voice command
"Alexa" derived from the microphone readings. We can observe
that the aliasing spectrogram has an "equivalent" form with the
spectrogram of accelerometer shown in Figure 9 (a).
6.4 User Authentication Using Cross-domain
Similarity
Spectrogram Calibration based on 2D-normalization. The scales
of measurements are greatly different on the accelerometer and
the microphone. To resolve such scale differences, we develop a
2D-normalization scheme to normalize the energy values of the
spectrograms across different frequencies. The normalization oper-
ation is defined as:
𝑆(𝑡, 𝑓 ) − 𝑆𝑚𝑖𝑛(𝑡)
𝑆𝑚𝑎𝑥 (𝑡) − 𝑆𝑚𝑖𝑛(𝑡) ,
𝑆𝑛𝑜𝑟𝑚(𝑡, 𝑓 ) =
(4)
where 𝑆(𝑡, 𝑓 ) is a spectrogram component at time 𝑡 and frequency
𝑓 . This normalization process is applied to spectrograms in both
vibration and audio domains.
Cross-domain Comparison based on 2D-Serial Correlation.
WearID authenticates users through comparing the 2D correlation
between the spectrogram of the accelerometer and aliasing spectro-
gram of the microphone. We refer to the 2D correlation coefficient
5101520Microphone: Word Index5101520Motion Sensor: Word Index0.20.30.40.50.60.70.85101520Microphone: Sentence Index5101520Motion Sensor: Sentence Index0.20.30.40.50.60.70.8WearID
ACSAC 2020, December 7–11, 2020, Austin, USA
Table 2: The specifications of the accelerometers in the tested wearable devices.
Model
LG W150
Accelerometer
Invensense M6515
Huawei watch 2 sport
STMicroelectronics LSM6DS3
Programmable Measurement Range
±2𝑔,±4𝑔,±8𝑔,±16𝑔
±2𝑔,±4𝑔,±8𝑔,±16𝑔
Sensor sampling frequency
System sampling rate
4-4000Hz
4-1600Hz
200Hz
100Hz
as cross-domain similarity which is defined as:
𝐴 × 𝑉
√
𝐴2 × 𝑉 2
( ^𝑆𝑚𝑖𝑐(𝑡, 𝑓 ) − ^𝑆𝑚𝑖𝑐),
𝐶𝑜𝑟𝑟( ^𝑆𝑚𝑖𝑐, 𝑆𝑎𝑐𝑐) =
𝑠.𝑡 ., 𝐴 =
,




𝑡
𝑓
𝑉 =
𝑆𝑎𝑐𝑐(𝑡, 𝑓 ) − 𝑆𝑎𝑐𝑐,
(5)
𝑡
𝑓
where 𝑆 represents the mean of a spectrogram, either in the audio
domain or in the vibration domain. 𝑆𝑎𝑐𝑐 represents the spectrogram
of accelerometer. In practice, directly computing frame-wise cor-
relation does not yield good similarity comparison performance
due to the unpredictable offsets caused by coarsely synchronized
data collection processes on the wearable and the VA device. Thus,
we propose a 2D-serial correlation algorithm that searches for an
optimal offset associating with the maximum correlation between
the spectrograms in the vibration and the audio domains. Partic-
ularly, we fix the aliasing spectrogram in the audio domain and
shift the spectrogram in the vibration domain frame by frame to
calculate the correlation coefficient. The maximum 2D-correlation
coefficient can then be found and used as the correlation score.
Finally, a threshold-based method is applied to the correlation score
and authenticate the user if the score is over an empirical threshold.
Figure 10(a) shows the pairwise correlation scores of 20 spoken
words provided in Table B. Most of the diagonal comparisons (i.e.,
same words) show the highest correlation scores. Figure 10(b) fur-
ther confirms the effectiveness of our method on differentiating 20
representative voice commands shown in Table C, which shows bet-
ter performance. This is reasonable since sentences contain much
more speech information than single words.
7 PERFORMANCE EVALUATION
7.1 Experimental Methodology
Devices. To evaluate WearID, two smartwatch models, Huawei
2 sport (100𝐻𝑧) and LG W150 (200𝐻𝑧) are involved to collect
accelerometer readings. The accelerometer specifications of the
two smartwatches are listed in Table 2. Specifically, LG W150 is
equipped with Invensense M6515 which supports sampling fre-
quencies within 4𝐻𝑧 ∼ 4000𝐻𝑧. The maximum acceleration that
can be measured with this accelerometer is ±16𝑔.Huawei watch
2 sport has the same programmable measurement range as the
LG smartwatch, but it supports lower sampling frequencies, up
to 1600𝐻𝑧. Although the accelerometers can record vibrations of
1.6𝐾𝐻𝑧 ∼ 4𝐾𝐻𝑧, the vendors constrain the sampling rates to en-
sure low power consumption. Both smartwatches run Android
Wear OS 2.0. In addition, as mentioned in Section 6.2, we use a
high-pass filter of 20𝐻𝑧 to remove the impacts of body movements
(e.g., typing on a keyboard, walking) on accelerometer readings.
We use an Android smartphone (Motorola Nexus 6) to emulate the
VA device recording voice commands at a sampling rate of 8𝑘𝐻𝑧.
Experimental Setup. We evaluate the performance of WearID
in a typical office environment. Compared with the home environ-
ment, the office environment has more dynamic ambient noises
(e.g., air condition, people walking, and conversations). Each partic-
ipant wears a smartwatch when he/she speaks voice commands to
a Motorola Nexus 6 smartphone at 1𝑚 distance. The average SPL
of the spoken speech commands is 80𝑑𝐵 (i.e., typical presentation-
level volume), which is reasonable as most users subconsciously
increase their volume when issuing voice assistant commands, usu-
ally from a distance. Because people may wear watches differently
(e.g., upside-down, loose around the wrist), we evaluate the im-
pacts of different wearing positions on our system. Particularly,
we test horizontal and vertical positions, which have the smallest
and the largest impact angles between acoustic waves and smart-
watches’ screens, respectively. We use a Logitech S120 speaker [31]
to conduct replay attacks and hidden voice commands, with the
volume set to maximum. To imitate ultrasound attacks, we use a
function generator (i.e., Keysight Technologies 33509B [41]) and a
tweeter speaker [18] to generate ultrasound. The distance between
the loudspeaker/tweeter speaker and the smartwatch is 30𝑐𝑚.
Data Collection. We involve 10 participants to test WearID
under the normal situation (i.e., no attack present) and various
attacks over a six-month period. The participants are asked to speak
20 representative critical voice commands as listed in Appendix
Table C. From each participant, 40 voice command samples with the
smartwatch worn in horizontal and vertical positions are collected.
In addition, we record 40 samples of ambient noises by using the
smartwatch’s accelerometer to examine WearID under the situation
where the legitimate user is not issuing critical commands. Besides,
100 samples of 10 hidden voice commands are utilized to evaluate
WearID against hidden voice command attacks [2].
Evaluation Metrics. To evaluate WearID, we use the following
four metrics: true positive rate (TPR) is the percentage of critical
commands of the legitimate user being correctly authenticated;
false positive rate (FPR) is the percentage of the adversaries’ critical
voice commands that pass WearID; receiver operating characteristics
(ROC) curve is generated by plotting the TPR against the FPR under
thresholds from 0 to 1 with a step of 0.01; Area under the ROC Curve
(AUC) measures how well the WearID correctly authenticating the
legitimate users while rejecting the adversaries.
7.2 Authenticating Legitimate Users
We first evaluate WearID in normal situations, where the attacker
does not present. While there is no malicious attack, WearID can
still be mistakenly triggered by friendly users’ (e.g., family members,
colleagues) conversation that is similar to the critical commands,
but the wearable device of the legitimate user only records ambient
ACSAC 2020, December 7–11, 2020, Austin, USA
Cong Shi, Yan Wang, Yingying Chen, Nitesh Saxena, and Chen Wang
(a) Horizontal
(b) Vertical
(a) Horizontal
(b) Vertical
Figure 11: Average ROC curve of verifying the user using
Huawei watch 2 under normal situation, random attack and
impersonate/replay attacks.
Figure 12: Average ROC curve of verifying the user using
LG Urban W150 under normal situation, random attack and
impersonate/replay attacks.
noises. For evaluation, we use the participants’ critical commands
recorded by the VA device against the corresponding wearables’
vibration data to simulate the legitimate user using the VA device.
Furthermore, we use the participants’ critical commands recorded
by the VA device against the wearables’ ambient vibration data
to simulate the cases that WearID is triggered mistakenly. We use
these data to derive ROC curves and study the performance of
WearID. The red curves in Figure 11 and Figure 12 present the ROC
of authenticating the legitimate users when the users are wearing
Huawei Watch 2 and LG W150 with two typical poses, respectively.
We can observe that WearID achieves 99.8% TPR and 0% FPR on
authenticating the legitimate users on Huawei Watch 2. Similarly,
WearID can achieve 99.6% TPR and 0% FPR on LG W150. The 0%
FPRs indicate that the voice commands from friendly users will
not pass WearID, meaning that our system can be used in typical
environments with multiple people. The AUCs of these cases are
all around 100% no matter the smartwatch is worn horizontally or
vertically, indicating that WearID can authenticate legitimate users’
critical voice commands accurately and robustly with different
wearables devices and their poses.
7.3 Attack on User’s Absence
Against Random Attack. Under the random attack, an adversary
tries to use his/her own voice to bypass the VA system. Although
the user is not co-located with the VA device, when the WearID is
triggered by the adversary, the user’s smartwatch may still record
the user’s speeches (e.g., conversation). To evaluate WearID under
such random attacks, we take turns considering each participant as
the legitimate user and the remaining 9 participants as adversaries.
We use the adversaries’ critical command speeches recorded by the
VA device against the vibration data of the legitimate user’s voice
commands for evaluating random attacks. In addition, we use the
legitimate user’s audio critical commands against his/her vibration
data to simulate the legitimate use of the VA device. Figure 11 and
Figure 12 show the average ROC curves of authenticating the legit-
imate user with two smartwatches under horizontal and vertical
poses. We observe that WearID can authenticate the legitimate user
and reject random attacks with high accuracy for both poses. In