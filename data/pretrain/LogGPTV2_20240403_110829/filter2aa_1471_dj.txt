The cache manager internally keeps track of various values that are
useful to developers and support engineers when debugging crash
dumps. All these debugging variables start with the CcDbg prefix,
which makes it easy to see the whole list, thanks to the x command:
Click here to view code image
1: kd> x nt!*ccdbg*
fffff800`d052741c 
nt!CcDbgNumberOfFailedWorkQueueEntryAllocations = 
fffff800`d05276ec nt!CcDbgNumberOfNoopedReadAheads = 
fffff800`d05276e8 nt!CcDbgLsnLargerThanHint = 
fffff800`d05276e4 nt!CcDbgAdditionalPagesQueuedCount = 
fffff800`d0543370 nt!CcDbgFoundAsyncReadThreadListEmpty = 
fffff800`d054336c nt!CcDbgNumberOfCcUnmapInactiveViews = 
fffff800`d05276e0 nt!CcDbgSkippedReductions = 
fffff800`d0542e04 nt!CcDbgDisableDAX = 
...
Some systems may show differences in variable names due to
32-bit versus 64-bit implementations. The exact variable names are
irrelevant in this experiment—focus instead on the methodology
that is explained. Using these variables and your knowledge of the
VACB array header data structures, you can use the kernel
debugger to list all the VACB array headers. The CcVacbArrays
variable is an array of pointers to VACB array headers, which you
dereference to dump the contents of the
_VACB_ARRAY_HEADERs. First, obtain the highest array index:
Click here to view code image
1: kd> dd nt!CcVacbArraysHighestUsedIndex  l1
fffff800`d0529c1c  00000000
And now you can dereference each index until the maximum
index. On this system (and this is the norm), the highest index is 0,
which means there’s only one header to dereference:
Click here to view code image
1: kd> ?? (*((nt!_VACB_ARRAY_HEADER***)@@(nt!CcVacbArrays)))
[0]
struct _VACB_ARRAY_HEADER * 0xffffc40d`221cb000
   +0x000 VacbArrayIndex   : 0
   +0x004 MappingCount     : 0x302
   +0x008 HighestMappedIndex : 0x301
   +0x00c Reserved         : 0
If there were more, you could change the array index at the end
of the command with a higher number, until you reach the highest
used index. The output shows that the system has only one VACB
array with 770 (0x302) active VACBs.
Finally, the CcNumberOfFreeVacbs variable stores the number
of VACBs on the free VACB list. Dumping this variable on the
system used for the experiment results in 2,506 (0x9ca):
Click here to view code image
1: kd> dd nt!CcNumberOfFreeVacbs  l1
fffff800`d0527318  000009ca
As expected, the sum of the free (0x9ca—2,506 decimal) and
active VACBs (0x302—770 decimal) on a 64-bit system with one
VACB array equals 3,276, the number of VACBs in one VACB
array. If the system were to run out of free VACBs, the cache
manager would try to allocate a new VACB array. Because of the
volatile nature of this experiment, your system may create and/or
free additional VACBs between the two steps (dumping the active
and then the free VACBs). This might cause your total of free and
active VACBs to not match exactly 3,276. Try quickly repeating
the experiment a couple of times if this happens, although you may
never get stable numbers, especially if there is lots of file system
activity on the system.
Per-file cache data structures
Each open handle to a file has a corresponding file object. (File objects are
explained in detail in Chapter 6 of Part 1, “I/O system.”) If the file is cached,
the file object points to a private cache map structure that contains the
location of the last two reads so that the cache manager can perform
intelligent read-ahead (described later, in the section “Intelligent read-
ahead”). In addition, all the private cache maps for open instances of a file are
linked together.
Each cached file (as opposed to file object) has a shared cache map
structure that describes the state of the cached file, including the partition to
which it belongs, its size, and its valid data length. (The function of the valid
data length field is explained in the section “Write-back caching and lazy
writing.”) The shared cache map also points to the section object (maintained
by the memory manager and which describes the file’s mapping into virtual
memory), the list of private cache maps associated with that file, and any
VACBs that describe currently mapped views of the file in the system cache.
(See Chapter 5 of Part 1 for more about section object pointers.) All the
opened shared cache maps for different files are linked in a global linked list
maintained in the cache manager’s partition data structure. The relationships
among these per-file cache data structures are illustrated in Figure 11-8.
Figure 11-8 Per-file cache data structures.
When asked to read from a particular file, the cache manager must
determine the answers to two questions:
1. 
Is the file in the cache?
2. 
If so, which VACB, if any, refers to the requested location?
In other words, the cache manager must find out whether a view of the file
at the desired address is mapped into the system cache. If no VACB contains
the desired file offset, the requested data isn’t currently mapped into the
system cache.
To keep track of which views for a given file are mapped into the system
cache, the cache manager maintains an array of pointers to VACBs, which is
known as the VACB index array. The first entry in the VACB index array
refers to the first 256 KB of the file, the second entry to the second 256 KB,
and so on. The diagram in Figure 11-9 shows four different sections from
three different files that are currently mapped into the system cache.
When a process accesses a particular file in a given location, the cache
manager looks in the appropriate entry in the file’s VACB index array to see
whether the requested data has been mapped into the cache. If the array entry
is nonzero (and hence contains a pointer to a VACB), the area of the file
being referenced is in the cache. The VACB, in turn, points to the location in
the system cache where the view of the file is mapped. If the entry is zero,
the cache manager must find a free slot in the system cache (and therefore a
free VACB) to map the required view.
As a size optimization, the shared cache map contains a VACB index array
that is four entries in size. Because each VACB describes 256 KB, the entries
in this small, fixed-size index array can point to VACB array entries that
together describe a file of up to 1 MB. If a file is larger than 1 MB, a separate
VACB index array is allocated from nonpaged pool, based on the size of the
file divided by 256 KB and rounded up in the case of a remainder. The
shared cache map then points to this separate structure.
Figure 11-9 VACB index arrays.
As a further optimization, the VACB index array allocated from nonpaged
pool becomes a sparse multilevel index array if the file is larger than 32 MB,
where each index array consists of 128 entries. You can calculate the number
of levels required for a file with the following formula:
(Number of bits required to represent file size – 18) / 7
Round up the result of the equation to the next whole number. The value
18 in the equation comes from the fact that a VACB represents 256 KB, and
256 KB is 2^18. The value 7 comes from the fact that each level in the array
has 128 entries and 2^7 is 128. Thus, a file that has a size that is the
maximum that can be described with 63 bits (the largest size the cache
manager supports) would require only seven levels. The array is sparse
because the only branches that the cache manager allocates are ones for
which there are active views at the lowest-level index array. Figure 11-10
shows an example of a multilevel VACB array for a sparse file that is large
enough to require three levels.
Figure 11-10 Multilevel VACB arrays.
This scheme is required to efficiently handle sparse files that might have
extremely large file sizes with only a small fraction of valid data because
only enough of the array is allocated to handle the currently mapped views of
a file. For example, a 32-GB sparse file for which only 256 KB is mapped
into the cache’s virtual address space would require a VACB array with three
allocated index arrays because only one branch of the array has a mapping
and a 32-GB file requires a three-level array. If the cache manager didn’t use
the multilevel VACB index array optimization for this file, it would have to
allocate a VACB index array with 128,000 entries, or the equivalent of 1,000
VACB index arrays.
File system interfaces
The first time a file’s data is accessed for a cached read or write operation,
the file system driver is responsible for determining whether some part of the
file is mapped in the system cache. If it’s not, the file system driver must call
the CcInitializeCacheMap function to set up the per-file data structures
described in the preceding section.
Once a file is set up for cached access, the file system driver calls one of
several functions to access the data in the file. There are three primary
methods for accessing cached data, each intended for a specific situation:
■    The copy method copies user data between cache buffers in system
space and a process buffer in user space.
■    The mapping and pinning method uses virtual addresses to read and
write data directly from and to cache buffers.
■    The physical memory access method uses physical addresses to read
and write data directly from and to cache buffers.
File system drivers must provide two versions of the file read operation—
cached and noncached—to prevent an infinite loop when the memory
manager processes a page fault. When the memory manager resolves a page
fault by calling the file system to retrieve data from the file (via the device
driver, of course), it must specify this as a paging read operation by setting
the “no cache” and “paging IO” flags in the IRP.
Figure 11-11 illustrates the typical interactions between the cache
manager, the memory manager, and file system drivers in response to user
read or write file I/O. The cache manager is invoked by a file system through
the copy interfaces (the CcCopyRead and CcCopyWrite paths). To process a
CcFastCopyRead or CcCopyRead read, for example, the cache manager
creates a view in the cache to map a portion of the file being read and reads
the file data into the user buffer by copying from the view. The copy
operation generates page faults as it accesses each previously invalid page in
the view, and in response the memory manager initiates noncached I/O into
the file system driver to retrieve the data corresponding to the part of the file
mapped to the page that faulted.
Figure 11-11 File system interaction with cache and memory managers.
The next three sections explain these cache access mechanisms, their
purpose, and how they’re used.
Copying to and from the cache
Because the system cache is in system space, it’s mapped into the address
space of every process. As with all system space pages, however, cache pages
aren’t accessible from user mode because that would be a potential security
hole. (For example, a process might not have the rights to read a file whose
data is currently contained in some part of the system cache.) Thus, user
application file reads and writes to cached files must be serviced by kernel-
mode routines that copy data between the cache’s buffers in system space and
the application’s buffers residing in the process address space.
Caching with the mapping and pinning interfaces
Just as user applications read and write data in files on a disk, file system
drivers need to read and write the data that describes the files themselves (the
metadata, or volume structure data). Because the file system drivers run in
kernel mode, however, they could, if the cache manager were properly
informed, modify data directly in the system cache. To permit this
optimization, the cache manager provides functions that permit the file
system drivers to find where in virtual memory the file system metadata
resides, thus allowing direct modification without the use of intermediary
buffers.
If a file system driver needs to read file system metadata in the cache, it
calls the cache manager’s mapping interface to obtain the virtual address of
the desired data. The cache manager touches all the requested pages to bring
them into memory and then returns control to the file system driver. The file
system driver can then access the data directly.
If the file system driver needs to modify cache pages, it calls the cache
manager’s pinning services, which keep the pages active in virtual memory
so that they can’t be reclaimed. The pages aren’t actually locked into
memory (such as when a device driver locks pages for direct memory access
transfers). Most of the time, a file system driver will mark its metadata
stream as no write, which instructs the memory manager’s mapped page
writer (explained in Chapter 5 of Part 1) to not write the pages to disk until
explicitly told to do so. When the file system driver unpins (releases) them,
the cache manager releases its resources so that it can lazily flush any
changes to disk and release the cache view that the metadata occupied.
The mapping and pinning interfaces solve one thorny problem of
implementing a file system: buffer management. Without directly
manipulating cached metadata, a file system must predict the maximum
number of buffers it will need when updating a volume’s structure. By
allowing the file system to access and update its metadata directly in the
cache, the cache manager eliminates the need for buffers, simply updating
the volume structure in the virtual memory the memory manager provides.
The only limitation the file system encounters is the amount of available
memory.
Caching with the direct memory access interfaces
In addition to the mapping and pinning interfaces used to access metadata
directly in the cache, the cache manager provides a third interface to cached
data: direct memory access (DMA). The DMA functions are used to read
from or write to cache pages without intervening buffers, such as when a
network file system is doing a transfer over the network.
The DMA interface returns to the file system the physical addresses of
cached user data (rather than the virtual addresses, which the mapping and
pinning interfaces return), which can then be used to transfer data directly
from physical memory to a network device. Although small amounts of data
(1 KB to 2 KB) can use the usual buffer-based copying interfaces, for larger
transfers the DMA interface can result in significant performance
improvements for a network server processing file requests from remote
systems. To describe these references to physical memory, a memory
descriptor list (MDL) is used. (MDLs are introduced in Chapter 5 of Part 1.)
Fast I/O
Whenever possible, reads and writes to cached files are handled by a high-
speed mechanism named fast I/O. Fast I/O is a means of reading or writing a
cached file without going through the work of generating an IRP. With fast
I/O, the I/O manager calls the file system driver’s fast I/O routine to see
whether I/O can be satisfied directly from the cache manager without
generating an IRP.
Because the cache manager is architected on top of the virtual memory
subsystem, file system drivers can use the cache manager to access file data
simply by copying to or from pages mapped to the actual file being
referenced without going through the overhead of generating an IRP.
Fast I/O doesn’t always occur. For example, the first read or write to a file
requires setting up the file for caching (mapping the file into the cache and
setting up the cache data structures, as explained earlier in the section “Cache
data structures”). Also, if the caller specified an asynchronous read or write,
fast I/O isn’t used because the caller might be stalled during paging I/O
operations required to satisfy the buffer copy to or from the system cache and
thus not really providing the requested asynchronous I/O operation. But even
on a synchronous I/O operation, the file system driver might decide that it
can’t process the I/O operation by using the fast I/O mechanism—say, for
example, if the file in question has a locked range of bytes (as a result of calls
to the Windows LockFile and UnlockFile functions). Because the cache
manager doesn’t know what parts of which files are locked, the file system
driver must check the validity of the read or write, which requires generating
an IRP. The decision tree for fast I/O is shown in Figure 11-12.
Figure 11-12 Fast I/O decision tree.
These steps are involved in servicing a read or a write with fast I/O:
1. 
A thread performs a read or write operation.
2. 
If the file is cached and the I/O is synchronous, the request passes to
the fast I/O entry point of the file system driver stack. If the file isn’t
cached, the file system driver sets up the file for caching so that the
next time, fast I/O can be used to satisfy a read or write request.
3. 
If the file system driver’s fast I/O routine determines that fast I/O is
possible, it calls the cache manager’s read or write routine to access
the file data directly in the cache. (If fast I/O isn’t possible, the file
system driver returns to the I/O system, which then generates an IRP
for the I/O and eventually calls the file system’s regular read routine.)
4. 
The cache manager translates the supplied file offset into a virtual
address in the cache.
5. 
For reads, the cache manager copies the data from the cache into the
buffer of the process requesting it; for writes, it copies the data from
the buffer to the cache.
6. 
One of the following actions occurs:
•    For reads where FILE_FLAG_RANDOM_ACCESS wasn’t
specified when the file was opened, the read-ahead information in
the caller’s private cache map is updated. Read-ahead may also be
queued for files for which the FO_RANDOM_ACCESS flag is not
specified.
•    For writes, the dirty bit of any modified page in the cache is set so
that the lazy writer will know to flush it to disk.
•    For write-through files, any modifications are flushed to disk.
Read-ahead and write-behind
In this section, you’ll see how the cache manager implements reading and
writing file data on behalf of file system drivers. Keep in mind that the cache
manager is involved in file I/O only when a file is opened without the
FILE_FLAG_NO_BUFFERING flag and then read from or written to using
the Windows I/O functions (for example, using the Windows ReadFile and
WriteFile functions). Mapped files don’t go through the cache manager, nor
do files opened with the FILE_FLAG_NO_BUFFERING flag set.
 Note
When an application uses the FILE_FLAG_NO_BUFFERING flag to
open a file, its file I/O must start at device-aligned offsets and be of sizes
that are a multiple of the alignment size; its input and output buffers must
also be device-aligned virtual addresses. For file systems, this usually
corresponds to the sector size (4,096 bytes on NTFS, typically, and 2,048
bytes on CDFS). One of the benefits of the cache manager, apart from the
actual caching performance, is the fact that it performs intermediate
buffering to allow arbitrarily aligned and sized I/O.
Intelligent read-ahead
The cache manager uses the principle of spatial locality to perform intelligent
read-ahead by predicting what data the calling process is likely to read next
based on the data that it’s reading currently. Because the system cache is
based on virtual addresses, which are contiguous for a particular file, it
doesn’t matter whether they’re juxtaposed in physical memory. File read-
ahead for logical block caching is more complex and requires tight
cooperation between file system drivers and the block cache because that
cache system is based on the relative positions of the accessed data on the
disk, and, of course, files aren’t necessarily stored contiguously on disk. You
can examine read-ahead activity by using the Cache: Read Aheads/sec
performance counter or the CcReadAheadIos system variable.
Reading the next block of a file that is being accessed sequentially
provides an obvious performance improvement, with the disadvantage that it
will cause head seeks. To extend read-ahead benefits to cases of stridden data
accesses (both forward and backward through a file), the cache manager
maintains a history of the last two read requests in the private cache map for
the file handle being accessed, a method known as asynchronous read-ahead
with history. If a pattern can be determined from the caller’s apparently
random reads, the cache manager extrapolates it. For example, if the caller
reads page 4,000 and then page 3,000, the cache manager assumes that the
next page the caller will require is page 2,000 and prereads it.
 Note
Although a caller must issue a minimum of three read operations to
establish a predictable sequence, only two are stored in the private cache
map.
To make read-ahead even more efficient, the Win32 CreateFile function
provides a flag indicating forward sequential file access:
FILE_FLAG_SEQUENTIAL_SCAN. If this flag is set, the cache manager
doesn’t keep a read history for the caller for prediction but instead performs
sequential read-ahead. However, as the file is read into the cache’s working
set, the cache manager unmaps views of the file that are no longer active and,
if they are unmodified, directs the memory manager to place the pages
belonging to the unmapped views at the front of the standby list so that they
will be quickly reused. It also reads ahead two times as much data (2 MB
instead of 1 MB, for example). As the caller continues reading, the cache
manager prereads additional blocks of data, always staying about one read
(of the size of the current read) ahead of the caller.
The cache manager’s read-ahead is asynchronous because it’s performed
in a thread separate from the caller’s thread and proceeds concurrently with
the caller’s execution. When called to retrieve cached data, the cache
manager first accesses the requested virtual page to satisfy the request and
then queues an additional I/O request to retrieve additional data to a system
worker thread. The worker thread then executes in the background, reading
additional data in anticipation of the caller’s next read request. The preread
pages are faulted into memory while the program continues executing so that
when the caller requests the data it’s already in memory.
For applications that have no predictable read pattern, the