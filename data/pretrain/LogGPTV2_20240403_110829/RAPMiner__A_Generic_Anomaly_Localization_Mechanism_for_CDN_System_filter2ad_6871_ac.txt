Speciﬁcally, the classiﬁcation power-based redundant attribute
deletion is designed for pruning the non-root cause attribute
combinations. As illustrated in Section I, even if only one
attribute is deleted, it will narrow about 50% of the search
space. Besides, we also adopt Anomaly Conﬁdence-guided
Layer by Layer Top-down Search to avoid searching for
anomaly but non-root patterns. Then, RAPMiner can traverse
the search space layer by layer from top to bottom based
on Breadth-First Search (BFS). The intuition is that if the
currently searched attribute combination is anomalous and
is considered as a RAP, the child attribute combinations are
certainly not RAPs and can be pruned off directly. The input
of RAPMiner is the anomaly detection results for KPIs of the
most ﬁne-grained attribute combinations. The existing studies
usually use different localizing strategies for fundamental KPIs
and derived KPIs, especially in the design of root cause scores.
However, there is no need to differentiate KPIs in RAPMiner,
because RAPMiner only uses the anomaly detection results
for the most ﬁne-grained attribute combinations and does not
care about whether they are fundamental or derived KPIs.
Since the anomaly detection for the most ﬁne-grained attribute
combinations can be done directly in the data collection stage
without subsequent aggregation, the method is more general
and helps save the data aggregation time.
C. Classiﬁcation Power Based Redundant Attribute Deletion
It can be observed from Insight 1 that if an attribute is in the
root anomaly patterns, using it to divide the dataset composed
of the most ﬁne-grained attribute combinations will reduce
Anomaly Detection
Anomaly Localization
Alarm!
A
Data Collection
RAPMiner
Only the most fine-grained
attribute combinations
(a1, b1, c1, d1), 2.0, 5.0
(a1, b1, c1, d2), 10.5, 21
… ...
1. Redundant 
attributes deletion
2. Searching
RAPs
Fig. 5. The Framework of RAPMiner
Set of the most fine-grained anomalous attribute combinations
Set of the most fine-grained normal attribute combinations
(cid:2)
(cid:3)(cid:7)
(cid:1)(cid:1)(cid:2)(cid:3)(cid:7)
(cid:3)(cid:6)
(cid:1)(cid:1)(cid:1)(cid:2)(cid:3)(cid:6)
(cid:3)(cid:8)
(cid:1)(cid:1)(cid:2)(cid:3)(cid:8)
(cid:4)(cid:6)
(cid:1)(cid:1)(cid:2)(cid:4)(cid:6)
(cid:1)
(cid:2)
(cid:4)(cid:7)
(cid:1)(cid:1)(cid:2)(cid:4)(cid:7)
(cid:2)
(cid:5)(cid:7)
(cid:1)(cid:1)(cid:2)(cid:5)(cid:7)
(cid:5)(cid:6)
(cid:1)(cid:1)(cid:2)(cid:5)(cid:6)
(cid:1)
Fig. 6. Attributes Classifying the Most Fine-grained Attribute Combinations
information entropy [25]–[27]. When an attribute is selected
to classify the dataset, i.e., divide the dataset into several sets
according to the speciﬁc element in the attribute, as shown
in Fig. 6, if it can hardly reduce the information entropy of
the whole dataset, it is considered that the attribute has almost
no ability to classify the dataset intuitively. Thus it cannot
be an attribute in the root anomaly patterns. In other words,
such an attribute is redundant and has nothing to do with the
anomalies. In this way, we can delete redundant attributes
before searching root anomaly patterns based on Insight 1
to narrow the search space and improve the efﬁciency in
localizing the root anomaly patterns. Actually, there are many
redundant attributes when a failure occurs in the real-world
system. We also observe that the root anomaly patterns are
mostly in the higher-layer cuboids in Fig. 2. We will later show
that removing redundant attributes may dramatically improve
the efﬁciency of the latter search process.
Fig. 6 shows a simple example. Let D be a dataset com-
posed of all the most ﬁne-grained attribute combinations of
attributes A, B and C. The elements in attribute A are a1, a2
and a3, the elements in attribute B are b1, b2, and the elements
in attribute C are c1, c2. We assume that (a1,∗,∗) is the RAP .
If we use attribute A to divide the dataset D, D tends to
be more orderly, due to the fact that all of the most ﬁne-
grained anomalous attribute combinations related to a1 can be
classiﬁed into the anomalous set Da1 and the rest of sets Da2
and Da3 may only contain the normal attribute combinations,
as shown in Fig. 6 left. Thus the overall dataset entropy after
the partition will be smaller than that when using attribute B or
C to divide D. On the contrary, as shown in Fig. 6 middle, both
the anomalous and normal attribute combinations are likely to
be divided into the sets Db1 and Db2 simultaneously when
we use attribute B to separate D. Note that the anomalous
parts in Db1 and Db2 are derived from the most ﬁne-grained
anomalous attribute combinations that related to a1. So similar
results when we apply attribute C to classify D.
To sum up, we introduce the Classiﬁcation Power (CP ) to
indicate how likely an attribute will be in RAPs by quantifying
the entropy reduction when applied to divide the dataset of the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:17:38 UTC from IEEE Xplore.  Restrictions apply. 
323
most ﬁne-grained attribute combinations. The bigger the CP
is, the more likely the attribute is to be in RAP s. Speciﬁcally,
we formulate CP as:
CPattr = Inf o(D) − Inf oattr(D)
Inf o(D) = −(palogpa + pnlogpn)
Inf oattr(D) = − n(cid:2)
Inf o(D)
attri
a
attri
a
attri
n
attri
n
(1b)
(1a)
+ p
logp
logp
)
|Dattri|
|D|
(p
i=1
(1c)
a
where, pa denotes the probability of abnormal attribute com-
binations in the dataset, while pn denotes the probability
of normal ones in the dataset. pattri
is the probability of
anomalous attribute combinations in attribute attri branch,
while pattri
indicates the probability of normal ones in attri
branch. Based on Eq.1, we deﬁne a Criteria to determine
whether an attribute is redundant or not below.
Criteria 1: ∀attr ∈ AttributeSet(RAP s), CPattr > tCP ,
tCP is a threshold with a very small value. Otherwise when
CPattr ≤ tCP , attr /∈ AttributeSet(RAP s).
n
Note that tCP is a threshold in the form of percentage. The
smaller the selected tCP , the more strict standard for redundant
attributes judgment, i.e., the classiﬁcation power of an attribute
must be extremely small before it can be considered as
redundant, and thus the fewer redundant attributes can be
deleted. On the contrary, when the selected tCP is larger, it
indicates that the judgment standard of redundant attributes is
more relaxed, i.e., an attribute can be considered redundant if
its classiﬁcation power is less than a relatively small value. In
this way, more redundant attributes can be deleted to improve
the efﬁciency, with the sacriﬁce of accuracy.
Based on Criteria 1, we can remove all redundant attributes
before searching for the root anomaly patterns. Proof 1 demon-
strates that deleting redundant attributes can signiﬁcantly nar-
row the search space. In addition, Table IV shows some details,
e.g., deleting one redundant attribute will result in at least
a 50% decrease of cuboids we have to search. Deleting two
redundant attributes can reduce the cuboids by more than 75%,
etc. The more redundant attributes we delete, the fewer cuboids
need to be searched, thus resulting in the higher efﬁciency of
localizing the root anomaly patterns. The complete procedure
of redundant attribute deletion is given in Algorithm 1.
Proof 1: Denote the total number of attributes as n, and the
total number of cuboids that need to be traversed when search-
ing the root anomaly patterns is 2n − 1 without deleting any
attributes. Assuming that k redundant attributes are deleted,
the total number of cuboids to be traversed in the remaining
search are 2n−k − 1, so the ratio of cuboids decreased to be
traversed is:
(2n − 1) − (2n−k − 1)
2n − 2n−k
2n − 1
=
DecreaseRatio@k =
=
2n − 1
2k − 1
2k − 1
2n−k
2k − 1
>
2k
(2)
324
THE RATIO OF CUBOIDS DECREASED AFTER DELETING REDUNDANT
TABLE IV
ATTRIBUTES
k
DecreaseRatio@k
1
0.5
2
3
4
5
0.75
0.875
0.9375
0.96875
Algorithm 1 Redundant Attributes Deletion
Input: The most ﬁne-grained attribute combinations dataset
D, each item of which has been detected as normal
or abnormal by predicted value and real value, e.g.,
[[a1, b1, c1, d1, anomalous], [a2, b2, c2, d2, normal], ...]
The attributes set AttributeSet, e.g., {A,B,C,D}
The threshold tCP
is related with RAP s, e.g., {A,B}
Output: The left attributes set AttributeSet
1: for attr ∈ AttributeSet do
(cid:2), any of which
Calculate CPattr (Eq.1) on D
if CPattr < tCP then
delete attr from AttributeSet
2:
3:
4:
end if
5:
6: end for
7: AttributeSet
8: return AttributeSet
(cid:2)
(cid:2) ← Sort AttributeSet by CPattr reversely
TABLE V
MAPPING BETWEEN VERTICES AND ATTRIBUTE COMBINATIONS
(a1, b1, c2, ∗)
(a1, b2, c1, ∗)
(a1, b2, c2, ∗)
(a2, b1, c1, ∗)
(a2, b1, c2, ∗)
(a2, b2, c1, ∗)
(a2, b2, c2, ∗)
(a3, b1, c1, ∗)
(a3, b1, c2, ∗)
(a3, b2, c1, ∗)
(a3, b2, c2, ∗)
(a2, b2, ∗, ∗)
(a2, ∗, c1, ∗)
(a2, ∗, c2, ∗)
(a3, b1, ∗, ∗)
(a3, b2, ∗, ∗)
(a3, ∗, c1, ∗)
(a3, ∗, c2, ∗)
(∗, b1, c1, ∗)
(∗, b1, c2, ∗)
(∗, b2, c1, ∗)
(∗, b2, c2, ∗)
(a1, b1, c1, ∗)
(a1, ∗, ∗, ∗)
(a2, ∗, ∗, ∗)
(a3, ∗, ∗, ∗)
(∗, b1, ∗, ∗)
(∗, b2, ∗, ∗)
(∗, ∗, c1, ∗)
(∗, ∗, c2, ∗)
(a1, b1, ∗, ∗)
(a1, b2, ∗, ∗)
(a1, ∗, c1, ∗)
(a1, ∗, c2, ∗)
(a2, b1, ∗, ∗)
3-2
3-3
3-4
3-5
3-6
3-7
3-8
3-9
3-10
3-11
3-12
2-6
2-7
2-8
2-9
2-10
2-11
2-12
2-13
2-14
2-15
2-16
3-1
-
-
1-1
1-2
1-3
1-4
1-5
1-6
1-7
2-1
2-2
2-3
2-4
2-5
D. Anomaly Conﬁdence Guided Layer-by-Layer Top-down
Search
After deleting the redundant attributes unrelated to the
anomaly, we need to traverse all the cuboids composed of the
remaining attributes to ﬁnd the root anomaly patterns because
they are all related to the root anomaly patterns. Based on
Insight 2, once an attribute combination is a root anomaly
pattern, its descendants can no longer be root anomaly pat-
terns. Therefore, we design a layer-by-layer top-down search
algorithm guided by the “Anomaly Conﬁdence” metric, to
localize the root anomaly patterns for failures as accurately
and quickly as possible. The basic idea of the search algorithm
is: we ﬁrst use the “anomaly conﬁdence” metric to determine
whether an attribute combination is anomalous or not; Then,
once an anomalous attribute combination is further considered
as a candidate root anomaly pattern, all of its descendants can
be pruned off.
Before going into the details, we ﬁrst present Criteria 2 and
Criteria 3.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:17:38 UTC from IEEE Xplore.  Restrictions apply. 
Higher
1-1
1-2
1-3
1-4
1-5
1-6
1-7
2-1
2-2
2-3
2-4
2-5
2-6
2-7
2-8
2-9
2-10
2-11
2-12
2-13
2-14
2-15
2-16
Lower
Higher
Lower
Higher
Lower
3-1
3-2
3-3
3-4
3-5
3-6