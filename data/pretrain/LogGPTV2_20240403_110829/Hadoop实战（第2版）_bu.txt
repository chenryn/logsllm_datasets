73 return false；
74}
75
76}
77 return true；
78}
79
80 public static void main（String[]agrs）{
81 String tablename="hbase_tb"；
82 String columnFamily="cf"；
83
84 try{
85 HBaseTestCase.creat（tablename, columnFamily）；
86 HBaseTestCase.put（tablename，"row1"，columnFamily，"cl1"，"data"）；
87 HBaseTestCase.get（tablename，"row1"）；
88 HBaseTestCase.scan（tablename）；
89 if（true==HBaseTestCase.delete（tablename））
90 System.out.println（"Delete table："+tablename+"success！"）；
91
92}
93 catch（Exception e）{
94 e.printStackTrace（）；
95}
96}
97}
在该类中，实现了类似HBase Shell的表创建（creat（String tablename, String columnFamily））操作，以及Put、Get、Scan和delete操作。
在代码清单12-2中，首先，通过第21行加载HBase的默认配置cfg；然后，通过HbaseAdmin接口来管理现有数据库，见第25行；第26～36行通过HTableDescriptor（指定表相关信息）和HColumnDescriptor（指定表内列族相关信息）来创建一个HBase数据库，并设置其拥有的列族成员；put函数通过HTable和Put类为该表添加值，见第38～44行；get函数通过HTable和Get读取刚刚添加的值，见第47～52行；Scan函数通过HTable和Scan类读取表中的所有记录，见第54～61行；delete函数，通过HBaseAdmin首先将表置为无效（第69行），然后将其删除（第70行）。
该程序在Eclipse中的运行结果如下所示：
……
create table success！
put'row1'，'cf：cl1'，'data'
Get：keyvalues={row1/cf：cl1/1336632861769/Put/vlen=4}
Scan：keyvalues={row1/cf：cl1/1336632861769/Put/vlen=4}
……
12/05/09 23：54：21 INFO client.HBaseAdmin：Started disable of hbase_tb
12/05/09 23：54：23 INFO client.HBaseAdmin：Disabled hbase_tb
12/05/09 23：54：24 INFO client.HBaseAdmin：Deleted hbase_tb
Delete table：hbase_tb success！
12.9.3 HBase与MapReduce
从图12-1中可以看出，在伪分布模式和完全分布模式下HBase是架构在HDFS之上的。因此完全可以将MapReduce编程框架和HBase结合起来使用。也就是说，将HBase作为底层“存储结构”，MapReduce调用HBase进行特殊的处理，这样能够充分结合HBase分布式大型数据库和MapReduce并行计算的优点。
下面我们给出了一个WordCount将MapReduce与HBase结合起来使用的例子，如代码清单12-3所示。在这个例子中，输入文件为user/hadoop/input/file01（它包含内容hello world bye world）和文件user/hadoop/input/file02（它包含内容hello hadoop bye hadoop）。
程序首先从文件中收集数据，在shuffle完成之后进行统计并计算，最后将计算结果存储到HBase中。
代码清单12-3 HBase与WordCount的结合使用
1 package cn.edn.ruc.cloudcomputing.book.chapter12；
2
3 import java.io.IOException；
4
5 import org.apache.hadoop.conf.Configuration；
6 import org.apache.hadoop.fs.Path；
7 import org.apache.hadoop.hbase.HBaseConfiguration；
8 import org.apache.hadoop.hbase.HColumnDescriptor；
9 import org.apache.hadoop.hbase.HTableDescriptor；
10 import org.apache.hadoop.hbase.client.HBaseAdmin；
11 import org.apache.hadoop.hbase.client.Put；
12 import org.apache.hadoop.hbase.mapreduce.TableOutputFormat；
13 import org.apache.hadoop.hbase.mapreduce.TableReducer；
14 import org.apache.hadoop.hbase.util.Bytes；
15 import org.apache.hadoop.io.IntWritable；
16 import org.apache.hadoop.io.LongWritable；
17 import org.apache.hadoop.io.NullWritable；
18 import org.apache.hadoop.io.Text；
19 import org.apache.hadoop.mapreduce.Job；
20 import org.apache.hadoop.mapreduce.Mapper；
21 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat；
22 import org.apache.hadoop.mapreduce.lib.input.TextInputFormat；
23
24 public class WordCountHBase
25{
26 public static class Map extends Mapper＜LongWritable, Text, Text, IntWritable＞{
27 private IntWritable i=new IntWritable（1）；
28 public void map（LongWritable key, Text value, Context context）throws
IOException, InterruptedException{
29 String s[]=value.toString（）.trim（）.split（""）；//将输入的每
行输入以空格分开
30 for（String m：s）{
31 context.write（new Text（m），i）；
32}
33}
34}
35
36 public static class Reduce extends TableReducer＜Text, IntWritable，
NullWritable＞{
37 public void reduce（Text key, Iterable＜IntWritable＞values, Context
context）throws IOException, InterruptedException{
38 int sum=0；
39 for（IntWritable i：values）{
40 sum+=i.get（）；
41}
42 Put put=new Put（Bytes.toBytes（key.toString（）））；//Put实例
化，每一个词存一行
43 put.add（Bytes.toBytes（"content"），Bytes.toBytes（"count"），Bytes.
toBytes（String.valueOf（sum）））；//列族为content，列修饰符为count，列
值为数目
44 context.write（NullWritable.get（），put）；
45}
46}
47
48 public static void createHBaseTable（String tablename）throws IOException{
49 HTableDescriptor htd=new HTableDescriptor（tablename）；
50 HColumnDescriptor col=new HColumnDescriptor（"content："）；
51 htd.addFamily（col）；
52 HBaseConfiguration config=new HBaseConfiguration（）；
53 HBaseAdmin admin=new HBaseAdmin（config）；
54 if（admin.tableExists（tablename））{
55 System.out.println（"table exists, trying recreate table！"）；
56 admin.disableTable（tablename）；
57 admin.deleteTable（tablename）；
58}
59 System.out.println（"create new table："+tablename）；
60 admin.createTable（htd）；
61}
62
63 public static void main（String args[]）throws Exception{
64 String tablename="wordcount"；
65 Configuration conf=new Configuration（）；
66 conf.set（TableOutputFormat.OUTPUT_TABLE, tablename）；
67 createHBaseTable（tablename）；
68 String input=args[0]；//设置输入值
69 Job job=new Job（conf，"WordCount table with"+input）；
70 job.setJarByClass（WordCountHBase.class）；
71 job.setNumReduceTasks（3）；
72 job.setMapperClass（Map.class）；
73 job.setReducerClass（Reduce.class）；
74 job.setMapOutputKeyClass（Text.class）；
75 job.setMapOutputValueClass（IntWritable.class）；
76 job.setInputFormatClass（TextInputFormat.class）；
77 job.setOutputFormatClass（TableOutputFormat.class）；
78 FileInputFormat.addInputPath（job, new Path（input））；
79 System.exit（job.waitForCompletion（true）?0：1）；
80}
81}
在上述程序中，第26～34行代码负责设置Map作业；第36～46行代码负责设置Reduce作业；第48～61行代码为createHBaseTable函数，负责在HBase中创建存储WordCount输出结果的表。在Reduce作业中，第42～44行代码负责将结果存储到HBase表中。
程序运行成功后，现在通过HBase Shell检查输出结果，如图12-16所示。
图 12-16 HBase WordCount的运行结果
从输出结果中可以看出，bye、hadoop、hello、world四个单词均出现了两次。
关于HBase与MapReduce实际应用的更多详细信息请参阅http：//wiki.apache.org/hadoop/Hbase/MapReduce。
12.10 模式设计
通过HBase与RDBMS的比较，可以了解到二者无论是在物理视图、逻辑视图还是具体操作上都存在很大的区别。例如，HBase中没有Join的概念。但是，大表的结构可以使其不需要Join操作就能解决Join操作所解决的问题。比如，在一条行记录加上一个特定的行关键字，便可以实现把所有关于Join的数据合并在一起。另外，Row Key的设计也非常关键。以天气数据存储为例。假如将监测站的值作为Row Key的前缀，那么天气数据将以监测站聚簇存放。同时将倒序的时间作为监测站的后缀，那么同一监测站的数据将从新到旧进行排列。这样的特定存储功能可以满足用户特殊的需要。
一般来说HBase的使用是为了解决或优化某一问题，恰当的模式设计可以使其具有HBase本身所不具有的功能，并且使其执行效率得到成百上千倍的提高。
 12.10.1 模式设计应遵循的原则
在进行HBase数据库模式设计的时候，不当的设置可能对系统的性能产生不良的影响。当数据量比较小的时候，表现可能并不明显，但随着数据量的增加这些微小的差别将有可能对系统的性能产生很大的影响。有下面几点需要特别注意。
1.列族的数量及列族的势
我们建议将HBase列族的数量设置得越少越好。当前，对于两个或两个以上的列族HBase并不能处理得很好。这是由于HBase的Flushing（冲洗，即将内存中的数据写入磁盘）和压缩是基于Region的。当一个列族所存储的数据达到Flushing的阈值时，该表中的所有列族将同时进行Flshing操作。这将带来不必要的I/O开销，列族越多，该特性带来的影响越大。对于压缩也是同样的道理。
此外，还要考虑到同一个表中不同列族所存储的记录数量的差别，即列族的势（Cardinality）。当两个列族数量差别过大时将会使包含记录数量较少列族的数据分散在多个Region之上，而Region有可能存储在不同的Regionserver之上。这样，当进行查询或scan操作的时候，系统的效率会受到一定的影响。该影响的大小要视具体的情况而定。