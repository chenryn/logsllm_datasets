models’ inherent vulnerabilities to generate well-crafted adversar-
ial perturbations to access intelligent audio systems. Such attacks
either add imperceptible perturbations to replayed audio [14, 35] or
embed speech samples into ambient noises/background music [84]
to spoof the speech/speaker recognition engines, making the model
yield adversary-desired output (e.g., speaker identity or speech con-
tent). A more recent study [36] even developed practical adversarial
examples that injects adversarial perturbations onto streaming au-
dio inputs (e.g., live human speech) in an unsynchronized manner,
demonstrating a severe threat to intelligent audio systems.
2.2 Existing Attack Detection Strategies
Although significant research efforts have been devoted to devel-
oping attack detection methods to secure voice access, few studies
have investigated using readily available microphone arrays on
modern intelligent audio systems to further enhance their secu-
rity levels. Most existing studies rely on extracting frequency do-
main features from single-channel audio to differentiate replayed
or synthesized voice from genuine human speech. For instance,
power spectrum features [24, 33, 42], relative phase shifts [17],
and magnetic field distortions [15] are exploited to detect replayed
and synthesized speech. A recent study, VOID [9], leveraged the
spectral features extracted from single-channel audio to detect var-
ious audio attacks. However, the lack of using multi-channel audio
and the spatial information makes it still vulnerable to many ad-
vanced attacks, such as modulated replay attacks [74], which aligns
the frequency domain distortions induced in the replay process.
Furthermore, several studies performed liveness detection using
dynamic acoustic features from dual-channel audio, such as pop
noises from breaths [49], cross-correlation of stereo signal [80],
and time-difference-of-arrival changes of phoneme sounds [87].
However, such dynamic acoustic features only exist in proximity
and require the microphones to be placed close to the user’s mouth
(e.g., when talking to a smartphone). Gong et al. [23] demonstrated
the potential of using multi-channel audio to defend replay attack,
which shows a significant improvement compared to using single-
channel audio (up to 34.9%), but the proposed system can only
address replay attacks. More importantly, this work treats multi-
ple audio channels as a whole and combines multi-channel audio
Session 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1886(a) Loudspeaker
(b) Human
Figure 2: Comparison of sound production mechanisms.
signals into a single-channel signal, which loses distinct spatial
information among the channels.
To mitigate the threats introduced by more advanced audio at-
tacks, recent research studies developed various approaches to
detect the unintelligible and inaudible attacking signals [13, 27, 36,
71, 83, 85]. To detect hidden voice commands, Carlini et al. [13]
exploited a classifier (i.e., logistic regression) trained with frequency
domain features (e.g., MFCCs, spectral entropy). Wang et al. [71]
proposed converting the audio recordings into vibration signals
(captured via motion sensors) to reveal the unique spectral charac-
teristics of hidden voice commands. Regarding inaudible attacks,
researchers have explored using frequency domain analysis [85] to
detect the ultrasound signals or emitting inaudible "guard" signals to
cancel the impacts of the attack [27]. More recently, to defeat inaudi-
ble voice commands (known as DolphinAttack [85]), EarArray [65]
proposes to utilize the estimated attenuation rate via microphone
array to differentiate ultrasound sounds from audible sounds. Fur-
thermore, signal filtering, quantization, audio compression, down-
sampling, and adversarial training have shown to be effective to
defend against audio adversarial examples [36, 83]. Although afore-
mentioned studies, using either software-based approaches or dedi-
cated hardware, show reasonably good performance in defending
against individual attacks, it is almost impossible to combine them
together as an all-in-one solution for practical deployment.
Furthermore, some researchers proposed using extra devices in-
cluding smart glasses [19], smartphone [11], wearable [48], or head-
phone [21], to capture additional voice characteristics to perform
user authentication. These investigations leveraged either unique
vibration patterns (e.g., body-surface vibrations [19], air-borne vi-
brations [48]) or the direction of speech (e.g., angle of arrival [11])
to confirm the authenticity of the sound source. However, these
approaches require additional devices, which could add extra cost
and are not always applicable in practice. CaField [64] achieves
continuous speaker verification by leveraging two on-board micro-
phones of a smartphone to capture the acoustic features embedded
in sound fields during propagation. Despite its improved usability,
this method requires the smartphone to be held at a relatively close
distance to the user’s mouth and the holding posture/position needs
to be consistent across continuous verification sessions. Thus, it
is not suitable for the broader context of intelligent audio systems
such as smart speakers.
Different from existing approaches, we develop a holistic defense
system by leveraging multi-channel microphone arrays that are
readily available in modern intelligent audio devices. Relying on
both temporal and spatial information extracted from multi-channel
audio, our system can detect a variety of existing machine-induced
voice attacks through holistic training.
Figure 3: Typical process of audio attacks.
3 MULTI-CHANNEL AUDIO ANALYSIS
In this section, we explore potential acoustic features to differentiate
machine-induced audio from human speech as well as validating the
benefits of leveraging multi-channel audio through thoroughly an-
alyzing a public multi-channel replay audio dataset, ReMASC [22].
3.1 Characteristics of Machine-induced Audio
Machine vs. Human Production of Sound. Machine (i.e., loud-
speakers) produces sound by moving a diaphragm back and forth
along one dimension to emit sound waves. As shown in Figure 2(a),
during sound production, an electric current flows through the
vocal coil, inducing a magnetic field that interacts with the perma-
nent magnet and creates a force that drives the diaphragm, causing
it to vibrate. Differently, as depicted in Figure 2(b), human voice
production involves multiple physiological components including
lungs, vocal cords, and vocal tract, and can be generally viewed
as a two-stage process where a raw sound is first produced by a
source and then shaped in the vocal tract [53]. Specifically, there
are three different sources of speech sounds. The first type of source
is vocal cord vibration, which is produced during the phonation
of voiced sounds: the air stream generated by lungs flows through
an open vocal tract and sets the vocal cords to oscillate, creating
vowel sounds such as [a], [e], [i] and [o]. The second source of
speech sound is air turbulence, which is generated by constricting
the vocal tract with teeth, tongue, or lips to produce high-velocity
airflow. The noises generated by the air is then shaped by the vocal
tract to form consonant sounds such as [f ], [s], [v] and [z]. The
third source is created by completely blocking the airflow toward
the front of the mouth and then followed by the sudden release of
the air, which results in plosive consonants such as [k], [p] and [t].
Compared to machine-induced sound, human speech is produced
from different locations within the vocal tract (e.g., oral and nasal
cavities) and further shaped by the resonances of the vocal tract
system. These differences result in traceable patterns in spectral
energy distribution [12, 76] and propagation path [87], which will
all be reflected in the magnitude and phase domain features.
Audio Attack Process. Figure 3 illustrates the typical process
of machine-induced audio attacks. The attacker first records speech
commands using a recording device, then plays the recorded audio
using a playback device when launching the attack. For some ad-
vanced audio attacks, the recorded audio will undergo an additional
preprocessing phase before playback (e.g., computing the inverse
MFCC [13], inverse filtering [74], or modulation onto ultrasonic
carrier [85]). In contrast, genuine speech commands are directly
inputted into the intelligent audio system via one-time over-the-air
propagation. The redundant procedures of audio attacks will intro-
duce additional noises to the audio signal in several aspects: first,
the attack audio propagates through physical environments twice,
resulting in more distortions due to the effects of room acoustics
(e.g., environmental noise, attenuation, and reverberation); second,
the hardware imperfection (e.g., non-flat frequency response and
MagnetDiaphragmVoice CoilMotionNasal CavityOral CavityVocal CordTongueTeethLips(a)(b)MagnetDiaphragmVoice CoilMotionNasal CavityOral CavityVocal CordTongueTeethLips(a)(b)RecordingDevicePlaybackDeviceProcessingSession 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1887(a) Magnitude spectrum of outdoor
(b) Magnitude spectrum of indoor #1
(c) Magnitude spectrum of indoor #2
(d) Magnitude spectrum of in-vehicle
(e) Phase spectrum of outdoor
(h) Phase spectrum of in-vehicle
Figure 4: Illustration of the magnitude and phase spectrum of genuine and replayed audio in different environments.
(g) Phase spectrum of indoor #2
(f) Phase spectrum of indoor #1
(a) Magnitude spectrum of outdoor
(b) Magnitude spectrum of indoor #1
(c) Magnitude spectrum of indoor #2
(d) Magnitude spectrum of in-vehicle
(e) Phase spectrum of outdoor
(f) Phase spectrum of indoor #1
(g) Phase spectrum of indoor #2
(h) Phase spectrum of in-vehicle
Figure 5: Comparison of the genuine and replayed audio recorded from different channels of a microphone array in various
environments.
noise in electronics) of the recording and playback devices will also
greatly impact the signal received by the intelligent audio system.
3.2 Potential Feature Analysis
Traditionally, power spectrum-based features are the most widely
used features for audio signal analysis, and their effectiveness on
replayed audio detection has been validated by many prior stud-
ies [12, 24, 66, 76]. However, power spectrum-based features alone
might not be sufficient, as a recent work [74] has demonstrated that
these features can be potentially manipulated by sophisticated at-
tackers to evade the detection. In addition to the widely-considered
magnitude-based features, recent studies on single-channel replay
attack detection have revealed that the phase domain features also
contain complementary channel information to the magnitude-
based features that are potentially useful for replayed and synthe-
sized audio detection [37, 43, 55, 72, 79]. However, utilizing multi-
channel phase information for audio attack detection remains un-
explored. To investigate the discriminability of the magnitude and
phase information derived from multi-channel audio, we perform a
feature analysis on the recently-published ReMASC dataset [22],
which contains genuine and replayed speech samples recorded from
multi-channel devices in four environments. The dataset and its
recording environments are detailed in Section 7. Specifically, we
divide all speech samples recorded by the ReSpeaker 4-Mic Lin-
ear Array into 4 groups according to the recording environment
(i.e., outdoor, indoor #1, indoor #2, and in-vehicle). This results in
a total of 192, 713, 275, and 673 genuine speech samples and 311,
2157, 846, and 959 replayed speech samples for each environment,
respectively. Figure 4 plots the average power spectrum for all the
genuine and the replayed audio samples and the continuous phase
spectrum averaged across all channels. From the figure, we can
clearly observe that both the magnitude and phase spectrum ex-
hibit distinguishable patterns between genuine and replayed audio
in all the environments. This confirms that both magnitude and
phase information in the frequency domain can be used to learn the
innate difference between the vocalization mechanism of humans
and loudspeakers.
100101102103104Frequency (Hz)−80−70−60−50−40−30−20Amplitude (dB)GenuineReplayed100101102103104Frequency (Hz)−80−70−60−50−40−30−20Amplitude (dB)GenuineReplayed100101102103104Frequency (Hz)−80−70−60−50−40−30−20Amplitude (dB)GenuineReplayed100101102103104Frequency (Hz)−80−70−60−50−40−30−20Amplitude (dB)GenuineReplayed0200040006000800010000Frequency (Hz)−2500−2000−1500−1000−5000500Angle (radians)GenuineReplayed0200040006000800010000Frequency (Hz)−1500−1000−500050010001500Angle (radians)GenuineReplayed0200040006000800010000Frequency (Hz)−2000−1500−1000−5000500Angle (radians)GenuineReplayed0200040006000800010000Frequency (Hz)−3000−2500−2000−1500−1000−5000Angle (radians)GenuineReplayed100101102103104Frequency (Hz)−80−70−60−50−40−30−20Amplitude (dB)Genuine-Ch1Replayed-Ch1Genuine-Ch2Replayed-Ch2100101102103104Frequency (Hz)−80−70−60−50−40−30−20Amplitude (dB)Genuine-Ch1Replayed-Ch1Genuine-Ch2Replayed-Ch2100101102103104Frequency (Hz)−80−70−60−50−40−30−20Amplitude (dB)Genuine-Ch1Replayed-Ch1Genuine-Ch2Replayed-Ch2100101102103104Frequency (Hz)−80−70−60−50−40−30−20Amplitude (dB)Genuine-Ch1Replayed-Ch1Genuine-Ch2Replayed-Ch20200040006000800010000Frequency (Hz)−2500−2000−1500−1000−5000500Angle (radians)Genuine-Ch1Replayed-Ch1Genuine-Ch2Replayed-Ch20200040006000800010000Frequency (Hz)−1500−1000−500050010001500Angle (radians)Genuine-Ch1Replayed-Ch1Genuine-Ch2Replayed-Ch20200040006000800010000Frequency (Hz)−2000−1500−1000−5000500Angle (radians)Genuine-Ch1Replayed-Ch1Genuine-Ch2Replayed-Ch20200040006000800010000Frequency (Hz)−3000−2500−2000−1500−1000−5000Angle (radians)Genuine-Ch1Replayed-Ch1Genuine-Ch2Replayed-Ch2Session 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1888Table 1: Inter-channel L1 distance of magnitude and phase spectrum in different environments.
1-2
1.00
1-3
1.07
Outdoor
1-4
0.94
2-3
0.80
2-4
1.56
3-4
1.71
1-2
0.83
1-3
0.89
Indoor #1
2-3
1-4
0.68
0.57
2-4
1.22
3-4
1.13
202.19
276.56
348.61
74.49
147.42
75.86
10.55
15.54
35.18
18.55
27.18
45.57
1.39
1.92
2.09
1.28
1.84
1.61
0.79
0.86
0.96
0.82
1.25
1.09
Indoor #2
2-3
1-4
0.98
0.92
1-3
1.11
2-4
1.16
3-4
1.26
1-2
0.76
1-3
1.01
In-vehicle
2-3
1-4
0.71
0.72
2-4
0.90
3-4
1.05
14.54
34.09
14.61
26.97
24.53
27.15
108.61
70.84
82.08
44.68
40.38
1.31
1.39
1.14
1.46
1.40
0.78
0.81
0.81
0.57
1.09
1.09
1-2
0.96
8.35
1.17
82.38
117.48
81.45
35.11