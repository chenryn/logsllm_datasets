●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.00
0.25
0.50
0.75
1.00
Correlation Threshold
Figure 5: Total Raspberry Pi entropy estimate as a function
of acceptable correlation threshold
Figure 6: Cycle counts for Tests 4 and 7 on the Raspberry Pi.
Correlation coef. = −0.79. Line is the best-ﬁt linear model.
ing other cores to communicate. (Note that the Raspberry
Pi has only a single core, but still executes this step.) We
have so far been unable to determine a causal relationship
between these two tests that might account for the extremely
odd relationship in Figure 6.
While we do not believe that the correlations between
tests are particularly helpful to an attacker (since a remote
or local but post-boot attacker will not have access to the
preceding T − 1 test values), in the interests of caution, we
modify our entropy estimate as follows: for each successive
variable, add its distribution entropy to the total if and only
if, when correlated with each preceding variable in turn,
never has a correlation coefﬁcient with magnitude ≥ 0.4. If
the variable is thus correlated with a preceding variable, we
ignore its sampled distribution entropy entirely.
When computed across the entire Raspberry Pi data set,
this conservative estimate places the summed distribution
entropy of pairwise uncorrelated variables at 231.9 bits —
far beyond the reach of exhaustive-search attacks.
Finally, to ensure that this analysis is not completely off,
we compute the distribution entropy over the entire data set
of 79-element vectors. For the 301,647 Raspberry Pi boot
measurements in our data set, every single one is unique,
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
giving a distribution entropy of 18.2 bits. Since distribution
entropy cannot extrapolate beyond the size of the empirical
data set, this is an empirical lower bound on the entropy
available by simply instrumenting the boot procedure of
Linux on the Raspberry Pi, and, given our calculations
above, we believe that there is more than sufﬁcient entropy
available during the Raspberry Pi’s boot process to securely
seed the Linux randomness generator.
4) BeagleBoard xM: The BeagleBoard xM is powered
by a Texas Instruments DM3730 SoC, containing a 1 GHz
Cortex-A8 ARMv7 superscalar CPU core. We acquired and
modiﬁed a patched Linux 3.2.28-x142 to include 77 tests in
start_kernel.
We have less Linux boot data for the BeagleBoard than
our other systems, as we re-purposed the BeagleBoard for
other experiments, detailed in Section IV. Nevertheless, we
collected data on 3,580 boots.
Per-test distribution entropies for the BeagleBoard are in
Table I. Naively summing, these 77 tests give 594.66 bits of
entropy between them. Our correlation coefﬁcient threshold
test reduces this slightly, to 430.06 bits. As for empirical
distribution entropy, all 3,580 boot sequences are unique,
giving a distribution entropy ﬂoor of 11.81 bits.
5) Trim-Slice: The Trim-Slice is another ARM single-
board computer, designed for use as a desktop PC. It
contains a 1 GHz NVIDIA Tegra 2 dual-core Cortex-A9
ARMv7 CPU, and a variety of storage options. To stay
consistent with our other devices, we chose to boot the Trim-
Slice from its MicroSD slot. We modiﬁed a Linux 3.1.10-
l4t.r15.02 kernel3 to include our instrumentation, and set the
machine to rebooting. Our particular model had an issue of
failing to reboot every so often, limiting our data collection
for this device.
Nevertheless, we instrumented 2,522 reboots of the Trim-
Slice, collecting cycle counts for 78 tests, similar to the
Raspberry Pi kernel. Per-test distribution entropy can be
found in Table I, giving a total sum of 683.40 bits (which,
again, may not be an accurate total estimate). Interestingly,
even though the Trim-Slice data set contains 100 times
fewer boots than the Raspberry Pi data, the per-test distri-
bution entropies are roughly similar across the board. Since
distribution entropy chronically underestimates the entropy
of the underlying distribution, this implies that the Trim-
Slice’s Tegra 2 has a much wider test variation than the
ARM 1176JZF-S, which is eminently plausible given that
the Tegra 2 is a dual-core platform and based on a Cortex-
A9, a larger and more complex core than in the Raspberry
Pi.
The Trim-Slice tests also appear to show much less cor-
relation than the Raspberry Pi. When we apply our method
of summing only the distribution entropy of variables which
2Online: https://github.com/RobertCNelson/stable-kernel
3Online: https://gitorious.org/trimslice-kernel
are not pairwise correlated with any previous test (cor. coef.
≤ 0.4), the Trim-Slice tests still show a shocking 641.48 bits
of entropy. Even if this overstates the actual amount by a
factor of 3, there is easily enough entropy extractable on
boot to seed any pseudorandom generator.
Finally, as one might expect given the data thus far, each
of the 2,522 78-element test vectors sampled on a given
Trim-Slice boot is unique, giving a total distribution entropy
of 11.30 bits. Again,
this represents an empirical lower
bound, and is one which we believe is extremely low.
6) Intrinsyc DragonBoard: The Intrinsyc DragonBoard
is a fully-featured mobile device development board based
around the Qualcomm SnapDragon S4 Plus APQ8060A
SoC, which includes a Qualcomm Krait ARMv7 dual-core
CPU. Designed as a development board for Android mobile
devices, it includes hardware such as a touch screen, wi-ﬁ
radio, and a camera module.
As a mobile device development platform,
the Drag-
onBoard runs Android 4.0.4 and is backed by a Intrinsyc-
modiﬁed Linux 3.0.21 kernel. As a result, our patch set
was easy to apply. As usual, we inserted 78 tests into
start_kernel. Instead of a Linux init script for collect-
ing the data, we used the Android adb tool to connect to
the device via USB, dump the kernel logs and reboot the
device. In this way, we collected data on 27,421 boots.
In general, we see excellent entropy generation when
booting Linux on the Krait. The per-test distribution en-
tropies can be found in Figure I, with a per-test sum of
557.84 bits. As with our preceeding three ARM SoCs, each
boot sequence is unique, giving a empirical distribution
entropy of 14.74 bits. The tests are also highly uncorrelated:
applying our correlation coefﬁcient threshold test lowers the
entropy estimate only slightly to 523.55 bits.
Resource-rich embedded devices, such as phones, have
a plethora of available sources of entropy – for example,
simply turning on one of their radios. This test, though,
shows that our entropy generation technique can protect
these devices as well.
7) FriendlyARM Mini6410: The FriendlyARM Mini6410
is yet another single-board ARM device. This particular unit
is powered by a Samsung S3C6410A SoC, and contains a
ARM 1176JZF-S ARM11 core clocked at 533 MHz. As
before, we modiﬁed the Linux 2.6.38 manufacturer-provided
kernel to instrument start_kernel, and inserted 77 tests.
Next, we let the FriendlyARM reboot 46,313 times. Inter-
estingly, the data from the FriendlyARM differs signiﬁcantly
from our other ARM results.
First, the per-test distribution entropies for the Friendly-
ARM can be found in Table I. (The FriendlyARM tests are
offset by one to align identical kernel initialization functions
between devices as much as possible.) At ﬁrst glance, the
per-test distribution entropies seem reasonable, given that
they are bounded above by lg(46, 313) = 15.4 bits, naively
summing to 394 bits of entropy.
595
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
The oddness arrives when we examine the distribution
entropy across boot vectors, and not just individual test
measurements. Unlike most other ARM SoC we tested, the
FriendlyARM occasionally produces identical boot vectors
on multiple independent boots. The two most common
vectors each appear 15 times in the dataset, giving a min-
entropy of 11.59 bits. In other words, a sufﬁciently prepared
adversary, given a single guess, can correctly predict the
FriendlyARM’s boot vector with probability 2−11.59, or
about 1 in 3,000. Given 233 guesses, this probability rises
to 2−4.7, or about 1 in every 25. However, this probabilistic
defect does not render our instrumentation worthless. Fifty-
ﬁve percent of vectors in the data set are unique, meaning
that this method can fully protect the Linux randomness
generator on the FriendlyARM over half the time, for
a negligible cost during kernel initialization. Even if the
machine does boot with a more common state, mixing in
these measurements can never reduce the amount of entropy
available to the pool, and thus will never be harmful to the
system as a whole.
One might hypothesize that there is some “common” vec-
tor, and the other popular vectors are simply approximations
thereof. However, the two most popular vectors differ in 59
of 77 positions. Also, strangely, the Mini6410 contains the
same ARM core as the Raspberry Pi, which exhibits none
of these symptoms. We can ﬁnd no convincing explanation
for the observed difference between these two systems.
8) Cubox: The Cubox is a commercially available desk-
top platform, powered by the Marvell ARMADA 510 SoC
with an 800 MHz Sheeva ARMv7 superscalar CPU core. We
modiﬁed a Linux 3.6.9 kernel for the Cubox4, as before, to
include 78 tests. We then rebooted the Cubox 27,421 times.
Per-test distribution entropy for the Cubox is presented
in Table I. Interestingly, it is our only ARM SoC which has
constant-time tests, i.e., tests whose distribution entropy is
zero. It also presents less test entropy overall, with a sum
of only 129.15 bits of individual test distribution entropy.
Like the FriendlyARM, the Cubox creates non-unique
boots; the most common of these occurs 80 times (0.29%).
Only 7,857 boots are unique in our data set. The total
empirical distribution entropy of the data set is 12.53 bits,
which indicates that our technique, while not solving the
entropy-at-boot problem on the Cubox, will still help protect
the kernel’s entropy generation.
9) Linksys WRT54GL: While ARM-based embedded
devices and SoCs are becoming more and more popular,
any investigation into entropy on embedded devices would
be remiss without examining how well proposed techniques
apply to the large installed base of devices. Home routers,
which were recently shown to have insufﬁcient entropy for
certiﬁcate generation [16], represent an enormous number of
existing devices, and, perhaps more importantly, embedded
4Online: https://github.com/rabeeh/linux.git
Figure 7: Cycle counts for Tests 18 and 19 on a WRT54GL.
Each point is one boot. Line is best-ﬁt linear model.
devices where strong entropy sources are extremely
important (e.g., key generation). To examine these routers,
we chose the Linksys WRT54GL as our test platform.
The WRT54GL is a popular consumer 802.11B/G wire-
less router, and consists of a Broadcom BCM5352 “router-
on-a-chip”, which contains a 200 MHz MIPS32 core; 16
MiB of RAM; and 4 MiB of ﬂash. Importantly for our
purposes, Linksys provides a custom Linux 2.4.20 kernel
which can be modiﬁed and run on the device.
The stripped-down WRT54GL kernel has fewer function
calls in start_kernel than the more modern kernels
on our ARM boards, but this is to be expected given the
simplicity of the device: the kernel needn’t contain any
extraneous code. We are able, then, to insert 24 tests in the
kernel initialization.
We then ran our modiﬁed kernel on two separate
WRT54GLs, one unmodiﬁed at 200 MHz and one over-
clocked to 250 MHz. The unmodiﬁed WRT we rebooted
81,057 times, while we rebooted the overclocked device
54,465 times. The per-test distribution entropies for the
unmodiﬁed device are in Table I. Perhaps surprisingly for
this device, these per-test entropies are quite high, up to 10
bits in some cases.
However, the correlations between tests on the WRT54GL
are far more intertwined than they are on our preceding
ARM devices. Two plots of these correlations can be seen
in Figures 7 and 8.
Unfortunately,
the overall entropy performance of the
WRT54GL betrays its promising per-test entropies. Across
the 81,057 boots of our unmodiﬁed router, we only see
11.86 bits of distribution entropy, and the most common
boot sequence appears 1,247 times (10.4%). Indeed, the top
188 vectors make up 37.1% of the dataset (30,062 boots).
If this were the only source of entropy for a PRNG seed, a
motivated attacker could easily brute-force these few vectors
and succeed almost 40% of the time. Even worse, there are
596
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
to start_kernel. Then, via an RS-232 console, we
rebooted the board 38,157 times. The per-test distribution
entropy can be found, as usual, in Table I.
As beﬁts our hypothesis that simpler processors produce
more constant results, 28 of the 69 tests have absolutely no
variation at all. Most of these functions are simply empty, as
the AVR32 is simple enough to not need their services (e.g.,
setup_nr_cpu_ids is a no-op, as there are no multi-core
AVR32 systems), but others do various memory initialization
tasks. The constant execution time of these functions speaks
to the minimal nature of the system as a whole.
Perhaps not surprisingly, this simplicity takes a toll on the
entropy generated during the boot process. Indeed, in our
data set, we see only 418 unique 69-element boot vectors;
the least frequent of which appears 43 times (0.1%), while
the most frequent appears 314 times (0.8%). This suggests
rather strongly that we have collected every possible vari-
ation of test times the device will generate under standard
operating conditions. The empirical distribution entropy of
our data set is 8.58 bits; this is likely all the entropy that
can be extracted from timing the NGW100 mkII boot.
III. ARCHITECTURAL CAUSES OF TIMING VARIATION
In this section, we describe two physical mechanisms
that partly explain the non-determinism we measured during
the execution of early kernel code: communication latency
(variation that can arise while sending data between two
clock domains) and memory latency (variation that arises
due to interactions with DRAM refresh). We give evidence
that these mechanisms are involved. We stress that these two
mechanisms only partly explain the behavior we observed.
Other mechanisms we do not understand are likely also
involved; we hope that future work can shed more light on
the situation.
A. Clock domain crossing
Modern embedded processors contain multiple clock do-
mains, and due to misalignment between the domains, the
amount of time it takes to send messages between two clock
domains can vary.
Processor designs use multiple clock domains to allow
different portions of the chip to run at different frequency.
For instance, on the BeagleBoard xM, the ARM Cortex-A8
runs at 1 GHz, the peripheral interconnect runs at 200 MHz,
and the Mobile DDR memory runs at 166 MHz [36].
At each boundary between two domains, chip designers
must use specialized mechanisms to ensure reliable commu-
nication. The most common solution to this problem is an
asynchronous FIFO (or queue) that enqueues data according
to one clock and dequeues it according to a second clock.
To see how an asynchronous FIFO can give rise to latency
variation, consider the case when the FIFO is empty and the
output domain tries to dequeue data at the same moment
the input domain inserts data. If the input domain’s clock
Figure 8: Cycle counts for Test 0 and 11 on an WRT54GL.
Each point is one boot. Line is best-ﬁt linear model.
only 11,960 boot sequences we saw only once. If the attacker
simply checks the 4,209 vectors that she saw more than once
during her precomputation, she will succeed against 78.6%
of boots.
This unfortunate distribution shows that boot–time en-
tropy is insufﬁcient to protect a PRNG on a standard MIPS
home router. However, it does add somewhat more than
11.86 bits, which is our observed distribution entropy across
the 24-element test result vectors. Since the process relies
solely on an already–extant hardware counter and is virtually
free, adding it to the Linux kernel boot is still worthwhile.
Overclocking: To see if we could tease further entropy
from the WRT54GL, we tried overclocking it from 200
MHz to 250 MHz, on the theory that we could change
the ratios between clocks in different parts of the SoC
and RAM. On this modiﬁed device, we performed 54,465
reboots. Overclocking does materially change each test’s
distribution: the Kolmogorov-Smirnov test for distribution
equality reports D > 0.1, P < 2.2 · 10−16 for 19 of 24 tests,
indicating that the two device’s empirical test values are
drawn from different underlying distributions. However, the
overclocked processor shows the same type of grouping as
the unmodiﬁed system, giving only 10.4 bits of distribution
entropy over the 24-element boot vectors, with the most
common appearing 879 times (1.6%).
10) Atmel NGW100 mkII: Finally, we turn to the Atmel
NGW100 mkII, a development board for the AT32AP7000-
U AVR32 microprocessor. AVR32 processors are designed
to be small, low-cost, and low-power: in some sense, it’s
one of the smallest microprocessors capable of running
Linux. Designed to prototype network gateway devices, the
NGW100 mk II ships with multiple Ethernet connectors,
internal ﬂash storage, and an SD card slot. To maintain
consistency, we booted the NGW100 mkII off an SD card.
We modiﬁed and built a patched Linux 2.6.35.4 kernel
using the Atmel AVR32 Buildroot system, adding 69 tests
597
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
Processor (1 GHz)
Memory (533 MHz)
First Level Interconnect (200MHz)
GFX (500 MHz)
Second Level Interconnect
(100 MHz)
y
c
n
e
u
q
e
r
F
1.00
0.75
0.50
0.25
0.00
data
NOP
GPMC
CM
Clock Control (50 MHz)
Ethernet (125 MHz)
0
4
8
Cycle delta
Figure 9: Clock domains similar to the domains found on
the DM3730. In order for the processor to modify a register
in the Ethernet controller, it must cross the clock domains
of the ﬁrst and second level interconnects.
Figure 10: Execution latency for two NOP instructions
(NOP), a read from the general purpose memory controller
(GPMC), and a read from the clock manager (CM). Cycle
delta is the difference from the minimum cycles observed.
arrives ﬁrst, the dequeue will succeed. If the output domain’s
clock arrives ﬁrst, the dequeue fails and will occur one
clock period later. If they arrive at precisely the same time,
metastability can result, which will also result in delay.
Because of random (and mostly independent) variation in
when the two clock signals arrive at the asynchronous FIFO
(i.e., clock jitter), any of these orderings is possible and
communication latency will vary.
Interactions between different clocks and metastability are
well-known sources of very high-quality randomness [5, 25,
34], so it is tempting to try to exploit the domain crossing
that already exist in a design to generate random bits.
In order to observe to interactions between clocks on
our device, we instrumented code to measure the latency
of communication between different clock domains. On
the BeagleBoard xM,
there are two on-chip-buses that
connect peripherals, similar to the diagram on Figure 9.
The processor, peripherals and interconnects are clocked by
several different PLLs. For the processor to communicate
with peripherals on the SoC, the processor must cross these
clock domains. Our approach was to measure the variation in
latency in communication devices with an increasing number
of clock domain crossings. Speciﬁcally, we measured the
number of cycles it took to perform to complete a set of
instructions which did not cross the interconnect (two NOP
instructions), to cross the ﬁrst level interconnect (reading
the revision number register of the memory controller) and
to cross the second level interconnect (reading the revision
number register of the system clock controller).
Our results are shown in Figure 10. Variability in fre-
quency increases with the number of clock domains crossed.
At two clock domain crossings, the distribution is bimodal.
While there may be some serial correlation between repeated
runs, this indicates that a read from the second level inter-
connect can provide up to around 2 bits of entropy. Reads
from this register are also fast: at an average of 270 cycles,
millions of these reads can be performed each second.
B. DRAM Access Latency
A second source of variation in performance is interac-
tions between main memory (i.e., DRAM) accesses, DRAM
refresh, and the memory controller. Because DRAM bits de-
cay over time, the system must periodically read and re-write
each DRAM storage location. Depending on the system, the
processor’s memory controller issues refresh commands or,
alternately, the processor can place the chips in an auto-
refresh mode so they handle refresh autonomously.
Regardless of who manages it, the refresh process cycles
through the DRAM row-by-row, and an incoming memory
access (e.g, to fetch part of the kernel for execution) may
have to wait for the refresh to complete.
To measure the effect of refresh on execution timing,
we used hardware performance counters to measure the
number of cycles it took to execute a series of 64 NOP
instructions on a ARM Cortex-A9 [39] with the instruction
cache disabled 100,000 times. We then used a software
register to turn refresh off and performed the test again. The
results of our test are plotted in Figure 12. The variation
in execution latency was much greater with refresh turned
on: with refresh on, execution fell into 6 bins, with ≈80%
distributed at the mode and ≈20% distributed in the other 5
bins. With refresh off, over 99% of executions fell into the
mode with less than 1% distributed in two other bins.
While refresh itself may appear to induce random distri-
butions in our experiment, the state machines in the memory
controller and the DRAM chips that manage refresh are
deterministic, as is the execution of code on the CPU
that generates requests. If the rest of the system were
deterministic as well, we expect that DRAM accesses would
have deterministic latencies.
However, other sources of randomness can affect the rela-
tionship between the processor and the DRAM refresh state
machines. For instance, the PLL for the DRAM controller
may “lock” more quickly than the processor’s PLL at system
(see 4 in Figure 11) boot or the DRAM controller’s power
supply may take longer to stabilize at start up (see 1 in
598
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
Figure 11: Power and clocks on the startup of a typical
embedded system. At 1 , the voltage is ramped up until it is
stable, which can take a variable amount of time. At 2 , The
system oscillator is turned on and takes a variable amount
of time to stabilize. At 3 , the PLLs that source the high
frequency clocks for the processor (PLL_OUT) and memory
(MEM_PLL) are turned on and take a variable amount of
time to stabilize. At 4 , the time that the memory clock
and processor clocks cross is variable but fully determined
by the time that both PLLs stabilize. At 5 , a small amount
of jitter in the memory clock causes the position the clocks
cross to change.
100000
y
c
n
e
u
q
e
r
F
75000
50000
25000
0
Refresh
off
on
400
500
Cycles
600
Figure 12: Execution latency with refresh on and off.
Figure 11). In this case, the future interactions between the
processor and refresh state machine will be affected, and the
latency for DRAM accesses will vary slightly. In addition to
variation in the system’s initial conditions, randomness from
clock domain crossing can further perturb the interaction
between the processor and memory.
IV. DRAM DECAY
Ultimately,
the most useful source of randomness we
found in these system is the decay of data stored in DRAM
over time. DRAM decay occurs when the charge that stores a
binary value leaks off the capacitor in a DRAM storage cell.
This phenomenon is well-studied, and the refresh process
(described in III-B) is designed to mitigate it.
599
Figure 13: Decay of DRAM after 7 (Blue), 14 (Green), 28
(Yellow) and 56 (Red) seconds.
A. Disabling Refresh
In order to detect decay in a live system, we must prevent
the system from refreshing DRAM. The ability to disable
refresh on the memory controller is not an exotic feature:
Nearly every memory controller we looked at supported
disabling refresh, and every embedded SoC we looked at,
from the Broadcom BCM5352 found in the WRT54GL to
the DM3730 on the BeagleBoard xM had software tunable
parameters for controlling refresh [36, 39]. Typically, control
over refresh is used to implement sleep modes. When a
processor enters a sleep mode, it disables refresh on the
memory controller and sends a command to the DRAM
chips to enter “self-refresh” mode, forcing the DRAM chips
refresh themselves as needed. By turning off refresh on
the memory controller and not sending the “self-refresh”
command, we were able to observe decay in our test systems.
B. Decay
The decay rate of DRAM bits varies widely (a fact
exploited by “cold boot” techniques [13]) as a result of
manufacturing variation, temperature, the data stored in the
cell, and other factors. In our experiments, some bits will
decay quickly (e.g., on the order of hundreds of µs) while
others will retain their data for seconds or hours. We ﬁnd that
the rate at which bits decay varies with even small variations
in temperature (see Section IV-D2).
C. Experimental Setup
Our approach to harvesting randomness from DRAM is
as follows. Very early in the boot process (i.e., in the boot
loader) we write test data to a portion of DRAM, and
then disable the refresh mechanism in both the processor’s
memory controller and the DRAM chips. The processor then
waits several seconds, reads the data back, and XORs it
with pattern written initially. Any ﬂipped bits will appear at
this stage. After this process, the bootloader can re-enable
refresh, reinitialize DRAM, and continue loading as normal.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
VDD_OKSYS_CLKCLK_OKPLL_OUTMEM_PLLVDD12345)
y
a