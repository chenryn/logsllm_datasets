Alpha String 
Digit String 
Special String 
TABLE 3.1.2  
Listing of different grammar structures 
Structure 
Simple 
Base 
Pre-Terminal 
Terminal (Guess) 
Example 
SLD 
S1L8D3 
$L8123 
$wordpass123 
393
    For  example,  the  base  structure  S1L8D3  might  have 
occurred with probability 0.1 in the training set. We decided 
to use the base structure directly in our grammars rather than 
the simple structure since the derivation of the base structure 
from the simple structure was unlikely to be context-free.  
    The second type of information that we obtained from the 
training set was the probability of digit strings and of special 
strings  appearing  in  the  training  set.  To  see  an example of 
this please refer to Table 3.1.3 and Table 3.1.4. 
1 Digit 
1 
2 
3 
4 
7 
5 
0 
6 
8 
9 
TABLE 3.1.3 
Probabilities of one-digit numbers 
Number of Occurrences 
12788 
2789 
2094 
1708 
1245 
1039 
1009 
899 
898 
712 
Percentage of Total 
50.7803 
11.0749 
8.32308 
6.78235 
4.94381 
4.1258 
4.00667 
3.56987 
3.5659 
2.8273 
TABLE 3.1.4 
Probabilities of top 10 two-digit numbers 
Percentage of Total 
5.99425 
4.26344 
4.13072 
4.05884 
3.2902 
3.13537 
2.97501 
3.94736 
2.65981 
2.58239 
Number of Occurrences 
1084 
771 
747 
734 
595 
567 
538 
533 
481 
467 
2 Digits 
12 
13 
11 
69 
06 
22 
21 
23 
14 
10 
    We  choose  to  calculate  the  probabilities  only  for  digit 
strings and special strings since we knew that the corpus of 
words,  (aka  alpha  strings),  that  users  may  use  in  password 
generation  was  much  larger  than  what  we could accurately 
learn from the training set. Note that the calculation of the 
digit  string  and  special  string  probabilities  is  gathered 
independently from the base structures in which they appear. 
    Please also note that all the information that we capture of 
both  types  is  done  automatically  from  an  input  file  of 
training passwords, using a program that we developed.  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:16:33 UTC from IEEE Xplore.  Restrictions apply. 
    Referring to Table 3.1.2 again, the pre-terminal structure 
fills  in  specific  values  for  the  D  and  S  parts  of  the  base 
structure. Finally, the terminal structure fills in a specific set 
of  alphabet  letters  for  the  L  parts  of  the  pre-terminal 
structure. Deriving these structures is discussed next. 
3.2  USING PROBABILISTIC GRAMMARS 
    Context-free grammars have long been used in the study 
of  natural  languages  [12,  13,  14],  where  they  are  used  to 
generate  (or  parse)  strings  with  particular  structures.  We 
show in the following that the same approach is useful in the 
automatic  generation  of  password  guesses  that  resemble 
human-created passwords.  
    A context-free grammar is a defined as G = (V, Σ, S, P), 
where: V is a finite set of variables (or non-terminals), Σ is a 
finite  set  of  terminals,  S  is  the  start  variable,  and  P  is  a 
finite set of productions of the form (1):  
grammars 
context-free 
α → β                                      (1) 
where  α  is  a  single variable and β is a string consisting of 
variables or terminals. The language of the grammar is the 
set  of  strings  consisting of all terminals derivable from the 
start symbol. 
    Probabilistic 
simply  have 
probabilities associated with each production such that for a 
specific  left-hand  side  (LHS)  variable  all  the  associated 
productions  add  up  to  1.  From  our  training  set,  we  first 
derive a set of productions that generate the base structures 
and  another  set  of  productions  that  derive  terminals 
consisting of digits and special characters. In our grammars, 
in addition to the start symbol, we only use variables of the 
form Ln,Dn, and Sn, for specified values of n. We call these 
variables  alpha  variables,  digit  variables  and  special 
variables respectively. Note that rewriting of alpha variables 
is  done  using  an  input  dictionary  similar  to  that  used  in  a 
traditional dictionary attack. 
    A  string  derived  from  the  start  symbol  is  called  a 
sentential form (it may contain variables and terminals). The 
probability of a sentential form is simply the product of the 
probabilities of the productions used in its derivation. In our 
production rules, we do not have any rules that rewrite alpha 
variables;  thus we can “maximally” derive sentential forms 
and their probabilities that consist of terminal digits, special 
characters  and  alpha  variables.  These  sentential  forms  are 
the pre-terminal structures.  
    In  our  preprocessing  phase,  we  automatically  derive  a 
probabilistic context-free grammar from the training set. An 
example of such a grammar is shown in Table 3.2.1. Given 
this  grammar,  we  can  furthermore  derive,  for  example,  the 
pre-terminal structure: 
S → L3D1S1 → L34S1 → L34!                    (2) 
394
with associated probability of 0.0975. The idea is that pre-
terminal structures define mangling rules that can be directly 
used in a distributed password cracking trial. For example, a 
control  server  could  compute  the  pre-terminal  structures in 
order  of  decreasing  probability  and  pass  them  to  a 
distributed  system  to  fill  in  the  dictionary  words  and  hash 
the  guesses.  The  ability  to  distribute  the  work  is  a  major 
requirement  if  the  proposed  method  is  to  be  competitive 
with  existing  alternatives.  Note  that  we  only  need  to  store 
the  probabilistic  context-free  grammar  and  that  we  can 
derive  the  pre-terminal  structures  as  needed.  Furthermore, 
note  that  fairly  complex  base  structures  might  occur  in the 
training  data  and  would  eventually  be  used  in  guesses,  but 
the  number  of  base  structures 
to  be 
overwhelming. 
is  unlikely 
TABLE 3.2.1 
Example probabilistic context-free grammar 
RHS 
D1L3 S2D1 
L3D1S1 
4 
5 
6 
! 
% 
# 
$$ 
** 
Probability 
0.75 
0.25 
0.60 
0.20 
0.20 
0.65 
0.30 
0.05 
0.70 
0.30 
LHS 
S  → 
S  → 
D1 → 
D1 → 
D1 → 
S1 → 
S1 → 
S1 → 
S2 → 
S2 → 
    The order in which pre-terminal structures are derived is 
discussed in Section 3.3.  Given a pre-terminal structure, a 
dictionary is used to derive a terminal structure which is the  
password guess. Thus if you had a dictionary that contained 
{cat, hat, stuff, monkey} the previous pre-terminal structure 
L34! would generate the following two guesses (the terminal 
structures), {cat4!, hat4!}, since those are the only dictionary 
words of length three.  
    There are many approaches that could be followed when 
substituting 
the  pre-terminal 
structures.    Note  that  each  pre-terminal  structure  has  an 
associated probability. 
    One  approach  to  generating  the  terminal  structures  is  to 
simply  fill  in  all  relevant  dictionary  words  for  the  highest 
probability pre-terminal structure, and then choose the next 
highest pre-terminal structure, etc.  This approach does not 
further  assign  probabilities  to  the  dictionary  words.  The 
naturalness  of  considering  this  approach  is  that  we  are 
leaning  only  lengths  of  alpha  strings  but  not  specific 
replacements  from  the  training  set.  This  approach  thus 
always  uses  pre-terminal  structures  in  highest  probability 
the  dictionary  words 
in 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:16:33 UTC from IEEE Xplore.  Restrictions apply. 
order  regardless  of  the  input  dictionary  used.  We  call  this 
approach pre-terminal probability order.   
     Another  approach  is  to  assign  probabilities  to  alpha 
strings  in  various  ways.    Without  more  information  on  the 
likelihood of individual words, the most obvious technique 
is  to  assign  the  alpha  strings  a  probability  based  on  how 
many words of that length appear in the dictionary.  If there 
are  10  words  of  length  3,  then  the  probabilities  of  each  of 
those words would be 0.10.  We call this approach terminal 
probability  order.  Note  that  in  this  case  each  terminal 
structure  (password  guess)  has  a  well-defined  probability. 
The  probability  however  is  based  in  part  on  the  input 
dictionary which was not learned during the training phase. 
We  also  considered  other  approaches 
for  assigning 
probabilities  to  alpha  strings.  For  instance  it  is  possible  to 
assign probabilities to words in the dictionary based on other 
criteria such as observed use, frequency of appearance in the 
language, or knowledge about the target.  
    An approach related to pre-terminal probability order is to 
use  the  probability  of  the  pre-terminals  to  sample  a  pre-
terminal  structure  and  then  fill  in  appropriate  dictionary 
words  for  the alpha strings.  Notice that in this latter case, 
we  would  not  use  a  pre-terminal  necessarily  in  highest 
probability order, but the frequency of generating terminals 