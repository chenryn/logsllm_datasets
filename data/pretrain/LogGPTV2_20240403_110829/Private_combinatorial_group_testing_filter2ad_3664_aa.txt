title:Private combinatorial group testing
author:Mikhail J. Atallah and
Keith B. Frikken and
Marina Blanton and
YounSun Cho
Private Combinatorial Group Testing
Mikhail J. Atallah∗
Department of Computer
Science
Purdue University
PI:EMAIL
Keith B. Frikken
Computer Science and
Systems Analysis
Miami University
PI:EMAIL
Marina Blanton
Department of Computer
Science and Engineering
University of Notre Dame
PI:EMAIL
YounSun Cho†
Department of Computer
Science
Purdue University
PI:EMAIL
ABSTRACT
Combinatorial group testing, given a set C of individuals
(“customers”), consists of applying group tests on subsets of
C for the purpose of identifying which members of C are
infected (or, more generally, defective in some way). The
outcome of a group test reveals only the presence or absence
of infection(s) in that group, but a number of group tests
exactly identiﬁes all infected members.
Although the main motivation for group testing is eco-
nomic – it drastically cuts down the number of necessary
tests – it has an interesting privacy side-eﬀect, namely, that
each individual customer is “hiding in a crowd” (the groups
within which it is being tested). This privacy side-eﬀect is
currently thrown away because the analysis that pinpoints
who is infected is carried out by the same entity that pre-
pared the test samples. This paper gives a protocol in which
these two duties are separated between Alice and Bob: The
protocol informs each customer who is infected privately,
and without either Alice or Bob learning who is infected.
An interesting feature of our protocol is that a customer
need not have any computational power, i.e., the customer
can be notiﬁed by mailing her (possibly paper copies of) two
random strings – one from Alice and one from Bob – so all
she has to do is visually check whether these two strings are
equal or not.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
∗Supported in part by Grants IIS-0325345 and CNS-0627488
from the National Science Foundation, and by sponsors of
the Center for Education and Research in Information As-
surance and Security.
†Supported by Grants IIS-0325345 and CNS-0627488 from
the National Science Foundation.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASIACCS ’08, March 18-20, Tokyo, Japan
Copyright 2008 ACM 978-1-59593-979-1/08/0003 ...$5.00.
Systems]: Security and Protection; E.3 [Data]: Data En-
cryption; F.2.2 [Analysis of Algorithms and Problem
Complexity]: Nonnumerical Algorithms and Problems.
General Terms
Security, Algorithms, Design.
Keywords
Privacy, Secure Protocol, Group Testing, Integrity Veriﬁca-
tion.
1.
INTRODUCTION
Combinatorial group testing on a set C is used to eﬃ-
ciently test which members of the set satisfy a speciﬁc test.
This is done by combining the members into groups and
running such tests on the groups rather than on individual
entries. Such group tests, whether of blood samples or of
digital objects, then reveal whether the elements of a group
are all “clean” – not infected in the case of blood, not cor-
rupted in the case of digital objects. From the outcomes of
remarkably few such group tests, it is possible to infer which
of a large set of n items are clean and which are defective.
The main application of group testing is to situations
where (i) it is expensive to individually test every one of
a large number n of samples (or, in the digital world, to
individually monitor the integrity of every one of a large
number n of data items, or to individually watch for anoma-
lies in many event streams); and (ii) it is necessary to pin-
point precisely which are defective among the n samples (or
data items, event streams, etc), based on a relatively small
number of group tests. This was the original motivation
for combinatorial group testing, as formulated for testing
blood supplies during World War II for the syphilis anti-
gen [3]. The cost of identifying which of n blood samples
is tainted, can be dramatically decreased by applying tests
to judiciously chosen subsets of the samples – given any
constant upper bound on the number of tainted samples, a
logarithmic (in n) number of tests suﬃce. A test consists
of taking a few drops from a subset of the samples, mixing
them, and applying a test to the mix, where a test outcome
is “bad” if and only if the corresponding subset contains one
or more infected blood samples. A more recent motivation
for CGT has been to integrity veriﬁcation [9], with the fol-
lowing analogies:
• In data integrity monitoring, the cryptographic hash
of a subset of n digital items is analogous to a mix
of blood drops from a subset of n blood samples, and
comparing such a hash to what it is supposed to be,
is analogous to applying a blood test to the mix of
blood drops (a mismatch between the computed hash
and the stored hash indicates that there is at least one
corrupted element in the subset for that hash).
• In anomaly detection, an event stream may consist of
events from n distinct sources, and instances of the de-
tection mechanism are used for monitoring substreams
that correspond to diﬀerent subsets of these n sources
(each instance learns what is normal for its monitored
substream, and it does the monitoring without having
to store all of its substream because there is not enough
space for that). A substream tagged as “anomalous”
implies that at least one of its constituent sources is
behaving anomalously.
There are many other applications of combinatorial group
testing in the digital world (see Section 5 for more of these),
but for the sake of deﬁniteness the rest of this paper uses the
language of blood tests for n customers. We do so because
privacy-preserving blood testing is of inherent interest in
its own right, but it should be understood that our results
have relevance beyond privacy-preserving blood testing (we
further clarify this relevance later in this section).
It is not hard to see that there are situations where the
leakage of an outcome of “infected” for a customer is a source
of possible embarrassment, humiliation, or even more tan-
gible consequences (possibly becoming uninsurable or less
employable). A similar statement holds for entities that are
not individuals – a “corrupted” outcome for their digital ob-
jects or event sequences could be a source of embarrassment
and loss of reputation/goodwill, possibly triggering lawsuits
or a drop in stock market value. This is the basic motiva-
tion for our work, which aims at providing a way to carry
out group tests but without anyone other than the customer
learning of their own diagnosis.
This kind of protocol where only the customer is aware of
her test outcome is advisable as a risk-mitigation technique
even in cases where most customers would feel comfortable
trusting a single facility (Alice) with all four steps of blood
sample handling: Collection, mixing, testing, inference of
infecteds. That is, not only could such a need for risk miti-
gation come from the small percentage of Alice’s customers
who are demanding when it comes to privacy, but Alice her-
self may insist on using it, e.g., because it decreases her lia-
bility insurance premium (alternatively, the insistence that
Alice not be completely trusted with such critical data may
come from Alice’s insurance company). Because no system
or network is perfect, it is wise to recognize that the pri-
vacy of Alice’s data could be breached (through a break-in,
spy-ware, insider misbehavior, etc.) and to use technologies
(such as our protocol) that make the consequences of such
a compromise less disastrous.
Therefore, in our solution this task is divided between two
parties – a data collection center Alice and a testing facil-
ity Bob – neither of which is entrusted with the result of
the computation (which is to determine who is infected).
In privacy-preserving blood testing this separation of du-
ties is not far from current practice, as it corresponds to
what often happens today in clinical environments: The Al-
ice facility, where customers’ blood samples are taken (pos-
sibly in the context of a blood donation drive), is not phys-
ically equipped to carry out the sophisticated and expen-
sive testing, which is done at a remote facility Bob that
is equipped for testing (that is, Alice and Bob already ex-
ist, and our proposal is merely for a change in the way
they interact). The relevance of our techniques for cyber-
security, however, needs some further elaboration. As al-
ready stated, in integrity veriﬁcation the equivalent to Bob’s
testing of a “blood mix” would be Bob’s veriﬁcation that the
Alice-computed cryptographic hash of the concatenation of a
group of records or ﬁles (the “blood mix”) matches the signed
version of that hash that is pre-stored securely with Bob.
The storing of the signed “expected” values of the hashes
with Bob would occur at system set up time, well before
any run of our protocol – the signature would be carried out
by a trusted authority (not by Bob) and Bob (but not Alice)
would be provided with the signed hashes. In anomaly de-
tection the event sub-streams would be pseudonymized be-
fore being mixed in groups and sent for anomaly analysis to
Bob; events from the same source have diﬀerent pseudonyms
in the various groups of which they are part. Bob uses its
own proprietary (and possibly computationally expensive)
technology to monitor each group for the presence of anoma-
lies. One of the drawbacks of anomaly detection technologies
becomes a privacy advantage in this case: Unlike signature-
based intrusion detection, which is capable of providing pre-
cise reasons why it sounded an alarm, an anomaly-based
system typically does not provide such precise reasoning on
why its conclusion of “anomaly” was reached.
Our contribution
We give a protocol for combinatorial group testing, that is
privacy-preserving in the sense that it informs the customers
of whether they are infected, without any other entity learn-
ing this. A distinctive feature of our protocol is that the
customers do not need to have any computational power at
all and are not required to have access to a computer: A
customer obtains two random strings and learns whether it
is infected or not by visually comparing these two strings for
equality.
While ensuring the privacy of customers’ outcome, we also
minimize the computation needed by Alice and Bob – our
solution is computationally eﬃcient for both Alice and Bob
even when the number of customers is large.
Additionally, we provide enhancements to the protocol
that allow the detection of cases when the number of in-
fected customers exceeds the assumed upper bound for it,
and that lower the number of operations performed in the
protocol.
We prove the security of our solution under the assump-
tion that the underlying primitives used (such as encryption
and random permutations) are secure. We assume that play-
ers Alice and Bob will follow the protocol and will not col-
lude with each other, but they may collude with customers.
The rest of this paper is organized as follows. Section 2
presents the framework considered. Section 3 presents the
protocol, its analysis, and extensions. Section 4 gives re-
sults from our experimental implementation of the protocol.
Section 5 describes related work, and Section 6 concludes.
2. FRAMEWORK
This section sets forth the framework and security model
we use. It also deﬁnes notation that is used throughout the
paper.
2.1 Group testing background
Here we review the terminology and some known results
from combinatorial group testing, that are needed in the
rest of the paper. As mentioned earlier, combinatorial group
testing (CGT) aims to perform group tests on subsets of a
given set C to identify infected elements in C. The outcome
of a group test is “contains at least one infected item” or
“contains no infected items.” We are interested here in non-
adaptive CGT, in which all the subsets to be tested have to
be decided ahead of time, i.e., before any subset is tested.
In adaptive CGT, by contrast, the next subset to be tested
can be chosen based on the outcomes of the previous tests.
Non-adaptive is more appropriate for our framework – for
example, in integrity veriﬁcation, the decision of which sub-
sets of n records will have their hashes computed and stored,
has to be made ahead of time and before any compromise in
integrity has occurred (hence before learning the outcomes
of any tests).
There are known constructions that, given an upper bound
d on the number of possible infected elements in C, can pin-
point the (at most d) infected items using a remarkably small
number m of tests (that is, m subsets of C are tested). For
example, when d is constant, m = O(log n) tests suﬃce.
More speciﬁcally, the best known general-purpose adaptive
schemes use m = O(d log(n/d)) tests, whereas the number of
tests used by the best known general-purpose non-adaptive
schemes is m = O(d2 log n) [4].
We now review the speciﬁc construction that we use here.
An m × n Boolean matrix M is d-disjunct [4] if, for any
d + 1 columns one of which is designated, there always exists
a row with a 1 in the designated column and 0’s in the
other d columns. Given a d-disjunct Boolean matrix M ,
a non-adaptive combinatorial group testing scheme consists
of simply performing the test indicated by each row i of M
(that is, test the subset Si corresponding to the columns
containing 1 in that row i). The results of these m tests are
then used as follows:
1. Initialize all n items as being infected.
2. For every row i whose test’s outcome was clean (i.e.,
no infected is in the subset Si), mark all the elements
of Si as being clean.
3. The items not marked clean by the time this process
ends are infected.
The correctness of this algorithm follows from M being d-
disjunct: Any clean item x cannot fail to be marked as clean,
because no matter which the d infected items are, there ex-
ists an Si that contains x and none of the d infected ones,
and this clean Si will in turn cause x to also be marked
clean.
It is well known [4] that a d-disjunct matrix M can be
constructed in O(mn) time by setting each of its entries to
1 with probability 1/(d+1). See [9] for how this can be done
using only O(d3 log n log d) random bits, with the resulting
m being O(d2 log n).
2.2 Cryptographic background
We now brieﬂy review a cryptographic primitive used in
the protocol. Our protocol utilizes a public-key semantically
secure additively homomorphic encryption scheme such as
Paillier [13]. Recall that such encryption makes it possible
to carry out certain computations on encrypted data.
In
particular, given messages m1 and m2 and encryption key
k, we have: Ek(m1) · Ek(m2) = Ek(m1 + m2), and therefore
Ek(m1)m2 = Ek(m1 · m2).
2.3 Security model
As was mentioned above, we rely on two entities who are
not expected to collude to perform the testing: A data col-
lection center Alice and a laboratory facility Bob. For each
customer, Alice receives the customer identifying informa-
tion and the blood sample, but does not have equipment
to run the tests. Bob, on the other hand, does not have
access to customers’ blood samples, but rather runs tests
on mixes comprised of samples from many customers. Fur-
thermore, we assume Bob can neither identify the customers
from a speciﬁc blood mix, nor learn the number of customers
in the blood mix (i.e., all mixes contain approximately the
same amount of blood). This last assumption is certainly
true in integrity veriﬁcation because a “blood mix” in that
domain is a cryptographic hash, which consists of a ﬁxed
number of bits (independent of the number of customers in
the mix). But it is also true in the physical blood situa-
tion, because the expected number of customers in a blood
mix is the expected number of 1s in a row of the d-disjunct
matrix M , which is n/(d + 1) because each entry of M has
probability 1/(d + 1) of containing a 1.
2.3.1 Security objective
We require that, under the adversarial model described
below, it is infeasible for any party to compute the result
of the test for a customer ci. More formally, we require
that no protocol participant can gain any information during
the protocol execution, other than its own inputs and the
outputs it is supposed to learn from the protocol (e.g., its
own infection status). In other words, a participant’s view
of the protocol execution can be simulated given the inputs
and outputs alone.
In addition to showing that the protocol execution does
not leak information to the participants, we also need to
guarantee that recovering a customer’s status from the in-
puts and outputs of the protocol is diﬃcult. In other words,