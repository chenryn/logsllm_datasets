C
0.4
0.8
0.6
0.4
Mind
0.2
1
0.8
0.6
Pmax
0.65
0.6
0.55
0.5
0.2
0.45
0
0.5
Mind
1 0
0.5
Pmax
0.5
Mind
Figure 2: Precision, Recall and Coverage of DUET-S
(top) and DUET-D (bottom)
measuring clustering results [15, 20] . Precision assesses the
accuracy of clustering in terms of how well the individual
clusters agree with the original malware classes. More for-
mally, assume the malware programs are grouped into a set
of clusters, O = {O1, O2, . . . , Oo} according to manually
Pc
created family labels. Then, precision, P , is deﬁned as:
i=1 max(|Ci ∩ O1|, |Ci ∩ O2|, . . . , |Ci ∩ Oo|). On
P = 1
n
the other hand, recall R measures the degree of the mal-
Po
ware classes’ scattering across the clusters and is deﬁned as:
j=1 max(|Oj ∩ C1|,|Oj ∩ C2|, . . . , |Oi ∩ Cc|). Pre-
R = 1
n
cision is 1 if all the samples in every cluster Ci are from
the same family and recall is 1 if all malware samples from
the same family fall into a single cluster (but not necessarily
the only family in this cluster). The third metric, cover-
age, measures the percentage of malware programs that can
be successfully clustered after excluding single-member clus-
ters. Coverage is an important metric measuring how well
clustering algorithms yield useful clusters.
Fig. 2 plots the performance of DUET-S and DUET-D with
two varying parameters of ProtoCluster algorithms—Pmax,
M ind. From these ﬁgures, one can observe that with proper
selection of parameters, both algorithms are able to cluster
malware samples with precision ranging from 70 to 90%, and
static-feature-based clustering generally outperforms behavior-
based clustering. We have also observed that the recall val-
ues of both DUET-S and DUET-D are around 0.3, which ap-
pears to be low. However, this low value of recall is mainly
because the malware family labels in our experiments are
very generic and within the same family, signiﬁcant diver-
sity exists across variants. Taking Vundo as an example,
the size of the largest Vundo variant in our training data is
9.6M bytes while the smallest variant size is only 13K and
the standard deviation is 2.2M bytes. Because of this enor-
mous diversity, clustering algorithms tend to break the orig-
inal family into several sub-families, e.g., DUET-S and DUET-D
often create 100–380 clusters for the reference dataset, re-
85
Table 7: Parameters of best and random scenario
1
0.8
M ind
Pmax
Cov-
re-
Pre-
erage
call
cision
51.5% 0.26
0.87
51.1% 0.24
0.88
57.4% 0.21
0.86
56.7% 0.20
0.85
67.9% 0.27
0.69
68.1% 0.30
0.68
74.4% 0.39
0.7
71.0% 0.37
0.7
re-
Cov-
call
erage
64.7% 0.27
57.9% 0.28
62.2% 0.27
65.7% 0.27
59.5% 0.23
72.3% 0.49
64.3% 0.33
65.7% 0.4
Best Scenario
Random Scenario
0.1
0.15
0.2
0.7
0.45
0.95
0.3
0.75
Pre-
cision
0.71
0.80
0.77
0.71
0.83
0.57
0.74
0.69
1
0.8
0.6
0.4
0.2
e
g
a
r
e
v
o
C
0.6
n
o
s
i
i
c
e
r
P
0.4
0.2
0
0.2
Best Scenario A
Best Scenario C
Best Scenario S
Random Scenario A
Random Scenario C
Random Scenario S
0.4
Threshold
0.6
1
0.8
e
g
a
r
e
v
o
C
0.6
0.4
0.8
0.2
0.2
Best Scenario A
Best Scenario C
Best Scenario S
Random Scenario A
Random Scenario C
Random Scenario S
0.4
0.6
Threshold
0.8
1
Figure 4: Results of agglomerative algorithm en-
semble. A, C, S represent Average, Complete, and
Single linkage distance metrics in agglomerative al-
gorithm, i.e.,. Avg recall: 0.29, stdev: 0.07)
clusters, it creates groups containing samples from multiple
small families, resulting in a lower precision value.
Summary In sum, individual clusterings often have to
make a trade-oﬀ between precision and coverage, achieving
a high value for the one at the expense of the other. By con-
trast, the cluster ensemble is able to leverage information
from multiple clusterings and improve both metrics simul-
taneously. The “Avg. Improvement” column in Table 8 lists
the average improvements of cluster ensemble over all indi-
vidual clusterings. The table shows that, except for the
hypergraph-based approach, incorporating ensemble algo-
rithms allows DUET to improve precision by 5–10% and cov-
erage by 20–40% over individual clusterings.
Best
Pre-
cision
Best
Cov-
erage
B: 3 gram 0.15
0.1
B: 4 gram
0.65
S: 3 gram
S: 4 gram
0.3
B: 3 gram 0.85
0.4
B: 4 gram
1.3
S: 3 gram
S: 4 gram
1.3
M ind
Pmax
B: 3 gram 0.20
B: 3 gram 0.30
B: 4 gram 0.50
B: 4 gram 0.75
0.60
S: 3 gram
S: 3 gram
1.10
0.30
S: 4 gram
S: 4 gram
0.85
0.60
0.20
0.20
0.10
0.70
1.25
1.10
1.15
Best Scenario
Random Scenario
1
0.8
n
o
s
i
i
c
e
r
P
0.6
0.4
0.2
0
0.2
0.3
0.4
0.5
0.6
Threshold
0.7
0.8
0.9
0
0.2
0.3
0.4
0.5
0.6
Threshold
0.7
0.8
Table 8: Summary of cluster ensemble’s results and
improvements over individual clusterings
Best Scenario
Figure 3: Results of ball algorithm ensemble (Aver-
age recall: 0.22, standard deviation: 0.02)
are 0.78 and 0.82, which are 5% and 10% higher than the
best coverage for individual clusterings. Also, the ensemble
results of random and best-case scenarios are close to each
other, indicating that the ensemble’s eﬀectiveness is not very
sensitive to the choice of its member clusterings. This is a
salient property, as it is not always possible to select a pri-
ori the best individual clusterings.
Cluster ensemble using the agglomerative algo-
rithm The beneﬁt of the agglomerative algorithm is that
it starts with the most similar samples and continues with
the “best” pair of clusters. It also allows ﬁne-grained control
in halting the merging process, such that remaining clus-
ters can be far enough from each other to ensure a clear
separation. Fig. 4 plots the results and shows that single
linkage is the worst of all in terms of precision, suﬀering
from the same over-merging problem as the single-threshold
approach. Average linkage is slightly better than complete
linkage, resulting in 0.84 precision and 81.85% coverage for
the random scenario, and 0.87 precision and 77.6% coverage
for the best-case scenario, all better than the base clustering.
Cluster ensemble based on hypergraph partition
We employ the HMETIS [17], a widely-used hypergraph
partition algorithm, to ﬁnd minimum cut in the connectiv-
ity matrix. With this approach, DUET achieves as high as
91.2% of coverage, but with very low precision of only 0.72.
Detailed investigation reveals that the low precision is at-
tributed to the standard constraint in the hypergraph par-
titioning algorithm—attempting to avoid trivial partitions
by making clusters comparably sized. However, in practice,
the size of malware families are very unbalanced (Table 5).
As the hypergraph partitioning balances the size of resulting
Ensemble
Approach
Ball
Agglomerative
Hypergraph
Non-Ensemble
Preci- Avg. Imp- Coverage Avg. Imp-
sion
0.85
0.87
0.72
0.78
rovement
9.24%
11.81%
-7.47%
N/A
rovement
25.88%
24.63%
46.48%
N/A
78.38%
77.60%
91.20%
62.26%
Random Scnario
82.56%
81.85%
89.90%
64.03%
rovement
9.97%
15.46%
-2.41%
rovement
28.94%
27.83%