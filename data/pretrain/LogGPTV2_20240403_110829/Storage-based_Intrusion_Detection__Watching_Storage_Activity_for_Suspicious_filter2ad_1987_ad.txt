The reverse lookup table is populated via the setRule()
USENIX Association
12th USENIX Security Symposium 
145
Command
setRule(path, rules)
listRules()
alert(path, rules, operation)
Purpose
Changes the watched characteristics of a ﬁle. This
command is used to both set and delete rules.
Retrieves the server’s rule table as a list of
{pathname, rules} records.
Delivers a warning of a rule violation to the admin-
istrator.
Direction
admin⇒server
admin⇒server
server⇒admin
Table 3: Administrative commands for our storage IDS. This table lists the small set of administrative commands needed for an administrative
console to conﬁgure and manage the storage IDS. The ﬁrst two are sent by the console, and the third is sent by the storage IDS. The pathname refers
to a ﬁle relative to the root of an exported ﬁle system. The rules are a description of what to check for, which can be any of the changes described in
Table 2. The operation is the NFS operation that caused the rule violation.
RPC. Each rule’s full pathname is broken into its com-
ponent names, which are stored in distinct rows of the
table. For each component, the table records four ﬁelds:
inode-number, directory-inode-number, name, and rules.
Indexed by inode-number, an entry contains the name
within a parent directory (identiﬁed by its directory-inode-
number). The rules associated with this name are a bitmask
of the attributes and patterns to watch. Since a particular
inode number can have more than one name, multiple en-
tries for each inode may exist. A given inode number can
be translated to a full pathname by looking up its lowest-
level name and recursively looking up the name of the cor-
responding directory inode number. The search ends with
the known inode number of the root directory. All names
for an inode can be found by following all paths given by
the lookup of the inode number.
Inode watchflags ﬁeld: During the setRule() RPC, in
addition to populating the reverse lookup table, a rule mask
of 16 bits is computed and stored in the watchflags ﬁeld
of the watched ﬁle’s inode. Since multiple pathnames may
refer to the same inode, there may be more than one rule
for a given ﬁle, and the mask contains the union. The inode
watchflags ﬁeld is a performance enhancement designed
to co-locate the rules governing a ﬁle with that ﬁle’s meta-
data. This ﬁeld is not necessary for correctness since the
pertinent data could be read from the reverse lookup table.
However, it allows efﬁcient veriﬁcation of detection rules
during the processing of an NFS request. Since the inode is
read as part of any ﬁle access, most rule checking becomes
a simple mask comparison.
Non-existent names table: The non-existent names table
lists rules for pathnames that do not currently exist. Each
entry in the table is associated with the deepest-level (ex-
isting) directory within the pathname of the original rule.
Each entry contains three ﬁelds: directory-inode-number,
remaining-path, and rules. Indexed by directory-inode-
number, an entry speciﬁes the remaining-path. When a ﬁle
or directory is created or removed, the non-existent names
table is consulted and updated, if necessary. For example,
upon creation of a ﬁle for which a detection rule exists, the
rules are checked and inserted in the watchflags ﬁeld of
the inode. Together, the reverse lookup table and the non-
existent names table contain the entire set of IDS rules in
effect.
5.2.2 Checking rules during NFS operations
We now describe the ﬂow of rule checking, much of which
is diagrammed in Figure 3, in two parts: changes to indi-
vidual ﬁles and changes to the namespace.
Checking rules on individual ﬁle operations: For each
NFS operation that affects only a single ﬁle, a mask of
rules that might be violated is computed. This mask is
compared, bitwise, to the corresponding watchflags ﬁeld
in the ﬁle’s inode. For most of the rules, this comparison
quickly determines if any alerts should be triggered. If the
“password ﬁle” or “append only” ﬂags are set, the corre-
sponding veriﬁcation function executes to determine if the
rule is violated.
Checking rules on namespace operations: Namespace
operations can cause watched pathnames to appear or dis-
appear, which will usually trigger an alert. For operations
that create watched pathnames, the storage IDS moves
rules from the non-existent names table to the reverse
lookup table. Conversely, operations that delete watched
pathnames cause rules to move between tables in the op-
posite direction.
When a name is created (via CREATE, MKDIR, LINK, or
SYMLINK) the non-existent names table is checked. If
there are rules for the new ﬁle, they are checked and placed
in the watchflags ﬁeld of the new inode. In addition,
the corresponding rule is removed from the non-existent
names table and is added to the reverse lookup table. Dur-
ing a MKDIR, any entries in the non-existent names table
that include the new directory as the next step in their re-
maining path are replaced; the new entries are indexed by
the new directory’s inode number and its name is removed
from the remaining path.
146
12th USENIX Security Symposium 
USENIX Association
Not watched
Check parent inode
for non−exist watches
Watched
Add watch flags to
new inode and remove
non−exist watch
Yes
NFS Request
Is Create?
No
Add name and
watches to non−exist
watches
Yes
Watch Triggered
Check inode watches
Watch not
triggered
Unlink Operation?
No
Lookup file name in
reverse lookup table
Notify
Administrator
Complete
operation
Figure 3: Flowchart of our storage IDS. Few structures and decision points are needed. In the common case (no rules for the ﬁle), only one inode’s
watchflags ﬁeld is checked. The picture does not show RENAME operations here due to their complexity.
When a name is removed (via UNLINK or RMDIR), the
watchflags ﬁeld of the corresponding inode is checked
for rules. Most such rules will trigger an alert, and an en-
try for them is also added to the non-existent names ta-
ble. For RMDIR, the reverses of the actions for MKDIR are
necessary. Any non-existent table entries parented on the
removed directory must be modiﬁed. The removed direc-
tory’s name is added to the beginning of each remaining
path, and the directory inode number in the table is modi-
ﬁed to be the directory’s parent.
By far, the most complex namespace operation is a RE-
NAME. For a RENAME of an individual ﬁle, modifying the
rules is the same as a CREATE of the new name and a RE-
MOVE of the old. When a directory is renamed, its sub-
trees must be recursively checked for watched ﬁles. If any
are found, and once appropriate alerts are generated, their
rules and pathname up to the parent of the renamed di-
rectory are stored in the non-existent names table, and the
watchflags ﬁeld of the inode is cleared. Then, the non-
existent names table must be checked (again recursively)
for any rules that map into the directory’s new name and
its children; such rules are checked, added to the inode’s
watchflags ﬁeld, and updated as for name creation.
5.3 Generating alerts
Alerts are generated and sent immediately when a detec-
tion rule is triggered. The alert consists of the original de-
tection rule (pathname and attributes watched), the speciﬁc
attributes that were affected, and the RPC operation that
triggered the rule. To get the original rule information, the
reverse lookup table is consulted. If a single RPC operation
triggers multiple rules, one alert is sent for each.
5.4 Storage IDS rules in a NIDS
Because NFS trafﬁc goes over a traditional network, the
detection rules described for our prototype storage IDS
could be implemented in a NIDS. However, this would in-
volve several new costs. First, it would require the NIDS to
watch the LAN links that carry NFS activity. These links
are usually higher bandwidth than the Internet uplinks on
which most NIDSs are used.3 Second, it would require
that the NIDS replicate a substantial amount of work al-
ready performed by the NFS server, increasing the CPU
requirements relative to an in-server storage IDS. Third,
the NIDS would have to replicate and hold substantial
amounts of state (e.g. mappings of ﬁle handles to their cor-
responding ﬁles). Our experiences checking rules against
NFS traces indicate that this state grows rapidly because
the NFS protocol does not expose to the network (or the
server) when such state can be removed. Even simple at-
tribute updates cannot be checked without caching the old
values of the attributes, otherwise the NIDS could not dis-
tinguish modiﬁed attributes from reapplied values. Fourth,
rules cannot always be checked by looking only at the cur-
rent command. The NIDS may need to read ﬁle data and
attributes to deal with namespace operations, content in-
tegrity checks, and update pattern rules. In addition to the
performance penalty, this requires giving the NIDS read
permission for all NFS ﬁles and directories.
Given all of these issues, we believe that embedding stor-
age IDS checks directly into the storage component is
more appropriate.
3Tapping a NIDS into direct-attached storage interconnects, such as
SCSI and FibreChannel, would be more difﬁcult.
USENIX Association
12th USENIX Security Symposium 
147
6 Evaluation
This section evaluates the costs of our storage IDS in terms
of performance impact and memory required—both costs
are minimal.
6.1 Experimental setup
All experiments use the S4 NFS server, with and without
the new support for storage-based intrusion detection. The
client system is a dual 1 GHz Pentium III with 128 MB
RAM and a 3Com 3C905B 100 Mbps network adapter.
The server is a dual 700 MHz Pentium III with 512 MB
RAM, a 9 GB 10,000 RPM Quantum Atlas 10K II drive, an
Adaptec AIC-7896/7 Ultra2 SCSI controller, and an Intel
EtherExpress Pro 100 Mb network adapter. The client and
server are on the same 100 Mb network switch. The oper-
ating system on all machines is Red Hat Linux 6.2 with
Linux kernel version 2.2.14.
SSH-build was constructed as a replacement for the An-
drew ﬁle system benchmark [15, 36]. It consists of 3
phases: The unpack phase, which unpacks the compressed
tar archive of SSH v. 1.2.27 (approximately 1 MB in size
before decompression), stresses metadata operations on
ﬁles of varying sizes. The conﬁgure phase consists of the
automatic generation of header ﬁles and makeﬁles, which
involves building various small programs that check the
existing system conﬁguration. The build phase compiles,
links, and removes temporary ﬁles. This last phase is the
most CPU intensive, but it also generates a large number
of object ﬁles and a few executables. Both the server and
client caches are ﬂushed between phases.
PostMark was designed to measure the performance of
a ﬁle system used for electronic mail, netnews, and web
based services [17]. It creates a large number of small
randomly-sized ﬁles (between 512 B and 16 KB) and per-
forms a speciﬁed number of transactions on them. Each
transaction consists of two sub-transactions, with one be-
ing a create or delete and the other being a read or append.
The default conﬁguration used for the experiments consists
of 100,000 transactions on 20,000 ﬁles, and the biases for
transaction types are equal.
6.2 Performance impact
The storage IDS checks a ﬁle’s rules before any oper-
ation that could possibly trigger an alert. This includes
READ operations, since they may change a ﬁle’s last ac-
cess time. Additionally, namespace-modifying operations
require further checks and possible updates of the non-
existent names table. To understand the performance con-
sequences of the storage IDS design, we ran PostMark and
SSH-Build tests. Since our main concern is avoiding a per-
Benchmark
SSH untar
SSH conﬁg.
SSH build
PostMark
Baseline With IDS
27.4 (0.02)
43.2 (0.37)
86.8 (0.17)
4290 (13.0)
27.3 (0.02)
42.6 (0.68)
85.9 (0.18)
4288 (11.9)
Change
0.03%
1.3%
1.0%
0.04%
Table 4: Performance of macro benchmarks. All benchmarks were
run with and without the storage IDS functionality. Each number repre-
sents the average of 10 trials in seconds (with the standard deviation in
parenthesis).
Benchmark
Create
Remove
Mkdir
Rmdir
Rename ﬁle
Rename dir
Baseline With IDS
4.35
4.65
4.38
4.59
3.91
4.04
4.32
4.50
4.36
4.52
3.81
3.91
Change
0.7%
3.3%
0.5%
1.5%
2.6%
3.3%
Table 5: Performance of micro benchmarks. All benchmarks were
run with and without the storage IDS functionality. Each number repre-
sents the average of 1000 trials in milliseconds.
formance loss in the case where no rule is violated, we ran
these benchmarks with no relevant rules set. As long as no
rules match, the results are similar with 0 rules, 1000 rules
on existing ﬁles, or 1000 rules on non-existing ﬁles. Ta-
ble 4 shows that the performance impact of the storage IDS
is minimal. The largest performance difference is for the
conﬁgure and build phases of SSH-build, which involve
large numbers of namespace operations.
Microbenchmarks on speciﬁc ﬁlesystem actions help ex-
plain the overheads. Table 5 shows results for the most ex-
pensive operations, which all affect the namespace. The
performance differences are caused by redundancy in the
implementation. The storage IDS code is kept separate
from the NFS server internals, valuing modularity over
performance. For example, name removal operations in-
volve a redundant directory lookup and inode fetch (from
cache) to locate the corresponding inode’s watchflags
ﬁeld.
Rules take very little time to generate alerts. For example,
a write to a ﬁle with a rule set takes 4.901 milliseconds
if no alert is set off. If an alert is set off the time is
4.941 milliseconds. These represent the average over 1000
trials, and show a .8% overhead.
6.3 Space efﬁciency
The storage IDS structures are stored on disk. To avoid
extra disk accesses for most rule checking, though, it is
important that they ﬁt in memory.
148
12th USENIX Security Symposium 
USENIX Association
Three structures are used to check a set of rules. First, each
inode in the system has an additional two-byte ﬁeld for
the bitmask of the rules on that ﬁle. There is no cost for
this, because the space in the inode was previously un-
used. Linux’s ext2fs and BSD’s FFS also have sufﬁcient