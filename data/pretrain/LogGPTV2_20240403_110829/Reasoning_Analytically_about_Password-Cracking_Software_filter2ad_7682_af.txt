### Password Strength Evaluation and Comparison of Cracking Algorithms

#### Data Overview
The following data represents the correlation (rw) and coverage percentage (%) for different password lists. The rw values range from 0.695 to 0.806, and the coverage percentages vary from 12.2% to 100%.

- **Coverage Percentage:**
  - 69.1%, 37.0%, 100.0%, 80.2%, 17.2%, 100.0%, 100.0%, 100.0%
  - 53.0%, 51.8%, 51.8%, 52.5%, 23.9%, 100.0%, 76.1%, 14.6%, 100.0%, 100.0%, 100.0%
  - 77.8%, 75.4%, 75.4%, 76.4%, 33.0%, 100.0%, 83.8%, 10.3%, 100.0%, 100.0%, 100.0%

- **Correlation (rw):**
  - 0.695, 0.696, 0.698, 0.696, 0.774, 0.589, 0.664, 0.714, 0.654, 0.577, 0.616
  - 0.730, 0.720, 0.731, 0.729, 0.768, 0.663, 0.645, 0.651, 0.686, 0.667, 0.669
  - 0.806, 0.798, 0.804, 0.805, 0.747, 0.693, 0.752, 0.494, 0.795, 0.696, 0.700

#### Tradeoff Between Accuracy and Coverage
To further explore the tradeoff between accuracy and coverage, we used two LinkedIn password lists (LinkedIn-30k and LinkedIn-10M) as meters. For four out of six evaluation sets, LinkedIn-10M showed a higher correlation (rw) than any other meter, including JtR and Hashcat. However, the coverage for this approach ranged from 12.2% to 46.8%, which significantly limits its ability to distinguish between previously unseen passwords.

#### Singleton and Frequent Passwords
Many passwords in a given set are singletons, appearing only once. Since frequently used passwords should not be considered strong, we evaluated the performance of meters when assigning guess numbers to passwords that appear at least five times in an evaluation set, ranking them among the 25% hardest-to-guess passwords. These can be considered unsafe errors. Table X shows that no meter consistently minimizes the number of unsafe errors. For some sets, JtR or Hashcat had the fewest unsafe errors, while for others, the Neural Network performed best.

#### Balancing Accuracy and Coverage
Using our JtR and Hashcat guess-number calculators as server-side meters strikes a balance between accuracy and coverage. Most importantly, our approach models the methods real attackers use in actual attacks.

### Detailed Comparison to Existing Password-Cracking Algorithms
Our techniques can also improve the expected success per guess of JtR and Hashcat. We compared JtR and Hashcat against other major password-guessing approaches using a sample of 10 million LinkedIn passwords as the wordlist or training data. We also evaluated Markov models (Markov: 4-gram and Markov: Backoff), the original PCFG proposal (PCFG: 2009), and Monte Carlo methods proposed by Dell’Amico et al. [10]. The latter only guesses passwords whose component strings were seen verbatim in training, which is why some passwords are never guessed. We also graphed an attack with optimal Perfect Knowledge of the evaluation set.

#### Performance Comparison
Figure 3 shows the comparison among password-cracking algorithms for Neopets and 000webhost. The graphs for the remaining four evaluation sets resembled Neopets more closely than 000webhost. Probabilistic approaches, particularly Neural Networks and PCFG, often performed best on a guess-by-guess basis. Surprisingly, JtR Extended (using our optimization techniques to add “missing” rules and reorder the rules) performed as well as, or even better than, probabilistic approaches other than Neural Networks at 1 billion guesses for four out of six evaluation sets. This result suggests that JtR and Hashcat, if configured using our techniques, may not lag as far behind probabilistic approaches as previously thought. After 1 billion guesses, JtR Extended's performance tended to plateau.

### Additional Figures and Tables
Figures 4a and 4b show the impact of reordering rule lists and wordlists. Each graph demonstrates the guessability of Battlefield Heroes reordered artificially based on itself (self-optimized) and on each of the five other evaluation sets. The original order of the Megatron rule list is nearly optimal, while reordering rules based on any English-language set also led to a nearly optimal ordering. Reordering words, however, appears to overfit to the data.

#### Rule and Wordlist Reordering
Tables XI and XII list the first 100 of the 5,146 SpiderLabs rules and 15,324 Megatron rules, along with their final positions after reordering based on each evaluation set using the PGS wordlist.

- **SpiderLabs Rules:**
  - cAz"[0-9]", Az"[0-9]", cAz"[0-9][0-9]", Az"[0-9][0-9]", etc.
- **Megatron Rules:**
  - /asa@[:c], /asa4[:c], /AsA4[:c], /AsA@[:c], etc.

- **Original Positions:**
  - 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25

This comprehensive analysis provides insights into the effectiveness of different password strength meters and cracking algorithms, highlighting the importance of balancing accuracy and coverage.