69.1
37.0
100.0
80.2
17.2
100.0
100.0
100.0
rw
0.695
0.696
0.698
0.696
0.774
0.589
0.664
0.714
0.654
0.577
0.616
%
53.0
51.8
51.8
52.5
23.9
100.0
76.1
14.6
100.0
100.0
100.0
rw
0.730
0.720
0.731
0.729
0.768
0.663
0.645
0.651
0.686
0.667
0.669
%
77.8
75.4
75.4
76.4
33.0
100.0
83.8
10.3
100.0
100.0
100.0
rw
0.806
0.798
0.804
0.805
0.747
0.693
0.752
0.494
0.795
0.696
0.700
Taking the tradeoff between accuracy and % coverage further, we treated a small and a medium-size list of LinkedIn passwords
(LinkedIn-30k and LinkedIn-10M) as meters. For four of the six evaluation sets, LinkedIn-10M had a higher correlation (rw)
than any meter, including JtR or Hashcat. However, the % coverage for this approach ranged from 12.2% to only 46.8%, losing
any ability to distinguish between any previously unseen passwords.
Many passwords in a given set are singletons, appearing once. As passwords that are frequently used should likely not be
considered strong, we also evaluated when meters assigned passwords appearing frequently (≥ 5 times) in an evaluation set
guess numbers ranking them among the 25% of hardest-to-guess passwords in that set. These can be considered unsafe errors.
As shown in Table X, no meter consistently minimizes the number of unsafe errors, rating common passwords as strong. For
some sets, the JtR or Hashcat approaches had the fewest unsafe errors. For other sets, the Neural Network had the fewest.
Using our JtR and Hashcat guess-number calculators as server-side meters thus strikes a balance between accuracy and
% coverage. Most crucially, however, our approach models the approaches real attackers use in actual attacks.
D. Detailed Comparison to Existing Password-Cracking Algorithms
Our techniques can also be used to improve the expected success per guess of JtR and Hashcat. Therefore, we compared how
JtR and Hashcat compare against the other major password-guessing approaches in an attack. As with the meter comparisons, we
again used a sample of 10 million LinkedIn passwords as the wordlist or training data. In addition to the relevant techniques
also tested as meters, we used Monte Carlo methods proposed by Dell’Amico et al. [10] to evaluate two Markov models
(Markov: 4-gram and Markov: Backoff), as well as the original PCFG proposal (PCFG: 2009) [14]. The latter only guesses
passwords whose component strings were seen verbatim in training, which is why some passwords are never guessed. We also
graph an attack that has optimal Perfect Knowledge of the evaluation set (i.e., knows the full password distribution a priori).
(cid:20)(cid:26)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:40:17 UTC from IEEE Xplore.  Restrictions apply. 
63
61
66
62
87
63
66
270
53
72
97
100 %
75 %
d
e
s
s
e
u
G
%
50 %
25 %
100 %
75 %
d
e
s
s
e
u
G
%
50 %
25 %
104
107
TABLE X: The number of passwords that appear frequently (≥ 5 times) in a set, yet are rated by the meter as among the 25%
of hardest-to-guess (or least probable) passwords.
Meter
000webhost
Battleﬁeld Heroes
Brazzers
Clixsense
CSDN
Neopets
Hashcat (Sec. VII)
JtR: Original (Sec. VII)
JtR: Reordered (Sec. X)
JtR: Extended (Sec. X)
LinkedIn: 10M [45]
Markov: Multi [53]
PCFG: 2016 [54]
LinkedIn: 30k [45]
Neural Network [13]
zxcvbn [22]
zxcvbn + LinkedIn-30k
301
322
294
297
665
258
324
967
236
311
299
12
16
20
17
32
32
16
149
19
33
40
48
49
46
39
74
86
52
300
51
153
110
157
216
111
113
477
200
220
854
166
200
199
792
877
814
795
1611
736
950
2580
502
793
763
Perfect Knowledge
Markov: 4-gram
Markov: Backoff
Markov: OMEN
JtR: Original
JtR: Reordered
JtR: Extended
Hashcat
PCFG: 2009
PCFG: 2016
Neural Network
Perfect Knowledge
Markov: 4-gram
Markov: Backoff
Markov: OMEN
JtR: Original
JtR: Reordered
JtR: Extended
Hashcat
PCFG: 2009
PCFG: 2016
Neural Network
1013
1016
Fig. 3: Guessability of Neopets (left) and 000webhost (right) passwords against different password-cracking approaches.
1010
# Guesses
1013
1016
104
107
1010
# Guesses
Figure 3 shows this comparison among password-cracking algorithms for Neopets (left) and 000webhost (right). The graphs
for the remaining four evaluation sets resembled Neopets far more closely than 000webhost. Echoing prior work [8], [13], we
found probabilistic approaches (particularly Neural Networks and PCFG) often performed best on a guess-by-guess basis.
Surprisingly, though, JtR Extended (using our optimization techniques to add “missing” rules and reorder the rules) performed
about as well as, or even better than, probabilistic approaches other than Neural Networks at 109 guesses for four of the six
evaluation sets. That is, a billion-guess attack will guess about as many passwords using JtR Extended as any approach other
than Neural Networks. This result is particularly important because all probabilistic approaches inherently incur substantial
computational costs to generate guesses in descending probability order. Therefore, our results somewhat contradict prior work
and suggest that JtR and Hashcat, if conﬁgured using the techniques we propose, may not lag as far behind probabilistic
approaches as previously thought. That said, after 109 guesses, JtR Extended performance tended to plateau.
E. Additional Figures and Tables
BField(selfoptimized)
Clixsense
Brazzers
Neopets
000webhost
OriginalOrdering
CSDN
d
e
s
s
e
u
g
t
n
e
c
r
e
P
70%
60%
50%
40%
70%
BField(self-optimized)
OriginalOrdering
Neopets
CSDN
Clixsense
Brazzers
000webhost
d
e
s
s
e
u
g
t
n
e
c
r
e
P
60%
50%
40%
107
109
Guesses
1011
107
109
Guesses
1011
(b) Reordering Hashcat wordlists: PGS wordlist, T0XlC rule list.
(a) Reordering JtR rule lists: PGS wordlist, Megatron rule list.
Fig. 4: The impact of reordering rule lists and wordlists. Each graph shows the guessability of Battleﬁeld Heroes reordered
artiﬁcially based on itself (self-optimized) and on each of the ﬁve other evaluation sets. Unlike for other JtR rule lists, Figure 4a
show that the original order of the Megatron rule list is nearly optimal, while reordering rules based on any English-language
set also led to a nearly optimal ordering. Reordering words (Figure 4b), however, appears to overﬁt to the data.
(cid:20)(cid:26)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:40:17 UTC from IEEE Xplore.  Restrictions apply. 
TABLE XI: The ﬁrst 100 of the 5,146 SpiderLabs rules and
their ﬁnal position after reordering based on each evaluation
set using the PGS wordlist.
TABLE XII: The ﬁrst 100 of the 15,324 Megatron rules and
their ﬁnal position after reordering based on each evaluation
set using the PGS wordlist.
Rule
cAz"[0-9]"
Az"[0-9]"
cAz"[0-9][0-9]"
Az"[0-9][0-9]"
cAz"[0-9][0-9][0-9]"
Az"[0-9][0-9][0-9]"
cAz"[0-9][0-9][0-9][0-9]"
Az"[0-9][0-9][0-9][0-9]"
cA0"[0-9]"
A0"[0-9]"
cA0"[0-9][0-9]"
A0"[0-9][0-9]"
cA0"[0-9][0-9][0-9]"
A0"[0-9][0-9][0-9]"
cA0"[0-9][0-9][0-9][0-9]"
A0"[0-9][0-9][0-9][0-9]"
/asa@[:c]
/asa4[:c]
/AsA4[:c]
/AsA@[:c]
/bsb8[:c]
/BsB8[:c]
/ese3[:c]
/EsE3[:c]
/isi1[:c]
/isi![:c]
/isi|[:c]
/IsI1[:c]
/IsI![:c]
/IsI|[:c]
/lsl1[:c]
/lsl7[:c]
/lsl|[:c]
/lsl![:c]
/Lsl1[:c]
/Lsl7[:c]
/Lsl|[:c]
/Lsl![:c]
/oso0[:c]
/OsO0[:c]
/sss$[:c]
/sss5[:c]
/SsS$[:c]
/SsS5[:c]
/tst+[:c]
/TsT+[:c]
/1s1![:c]
/1s1i[:c]
/1s1I[:c]
/1s1|[:c]
/0s0o[:c]
/0s0O[:c]
/3s3e[:c]
/3s3E[:c]
/4s4a[:c]
/4s4A[:c]
/5s5s[:c]
/5s5S[:c]
/7s7l[:c]
/7s7L[:c]
/8s8b[:c]
/8s8B[:c]
/asa@/bsb8[:c]
/asa@/BsB8[:c]
/asa@/ese3[:c]
/asa@/EsE3[:c]
/asa@/isi1[:c]
/asa@/isi![:c]
/asa@/isi|[:c]
/asa@/IsI1[:c]
/asa@/IsI![:c]
/asa@/IsI|[:c]
/asa@/lsl1[:c]
/asa@/lsl7[:c]
/asa@/lsl|[:c]
/asa@/lsl![:c]
/asa@/Lsl1[:c]
/asa@/Lsl7[:c]
/asa@/Lsl|[:c]
/asa@/Lsl![:c]
/asa@/oso0[:c]
/asa@/OsO0[:c]
/asa@/sss$[:c]
/asa@/sss5[:c]
/asa@/SsS$[:c]
/asa@/SsS5[:c]
/asa@/tst+[:c]
/asa@/TsT+[:c]
/asa@/1s1![:c]
/asa@/1s1i[:c]
/asa@/1s1I[:c]
/asa@/1s1|[:c]
/asa@/0s0o[:c]
/asa@/0s0O[:c]
/asa@/3s3e[:c]
/asa@/3s3E[:c]
/asa@/4s4a[:c]
/asa@/4s4A[:c]
/asa@/5s5s[:c]
/asa@/5s5S[:c]
Original Position
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25