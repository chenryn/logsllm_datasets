### Evaluation of Network Delay and Performance Impact of TF-BIV in a Cloud-Based Cryptographic Service

#### System Configuration
The evaluation was conducted on a Dell Optiplex 9020 PC equipped with an Intel i7-4770 CPU (3.4 GHz) and 16 GB RAM. The host runs Linux OS (kernel v3.13) with QEMU v1.7.1. The guest VM is configured with 4 vCPUs and 4 GB RAM, running Linux kernel v3.13.7.

#### Startup Performance
To analyze the startup performance overhead introduced by TF-BIV, we used Bootchart [32], a tool for performance analysis of the Linux booting process. We measured the startup time of the VM OS with TF-BIV deployed and compared it to the native host. The measurement was performed 10 times, and the average overhead was found to be 1.49%, primarily due to the additional integrity verification.

#### SPECINT Benchmark
We used SPECINT 2006, a set of performance benchmarks, to evaluate the impact on the virtualized VM. An S-process was created to issue a cryptographic service request every 5 seconds, and the SPECINT 2006 scores were measured. The results were compared across three scenarios: native Linux, with TF-BIV active, and with TF-BIV monitoring the S-process. As shown in Figure 4, the performance overhead was less than 3.6%, attributed to the identification and integrity verification of S-processes and the capturing and filtering of network connections in the network card emulation.

**Figure 4: SPECINT 2006 Performance Overhead**

#### HTTPS Throughput and Latency
TF-BIV was integrated with the cloud-based cryptographic service for process-level authorization. We evaluated the performance overhead by setting up an HTTPS service that requests the cloud-based cryptographic service for RSA-2048 decryption. The RSA-2048 decryption was deployed on a separate host and provided services through HTTPS connections. Apache was run in the monitored VM, which requested the cryptographic service for HTTPS, making httpd the S-process in our experiment. We used the Apache benchmark to evaluate the throughput and latency of the HTTPS service. For each evaluation, the client constructed 10,000 HTTPS requests for a 4 KB web page at different concurrency levels. As shown in Figure 5, the maximum decrease in throughput was 8.3% and the largest latency increase was 5.7%, occurring at the highest concurrency level (i.e., 128). This is because the network card emulator needs to process more connections and packets, such as finding the process corresponding to the packets and obtaining the IP address and port for the packets, when the concurrency level increases.

**Figure 5: Speed of HTTPS Service**

**Figure 6: 99th Percentile of HTTPS Latency**

#### Network Performance
We used iPerf [43], an active measurement tool for network bandwidth, to evaluate the influence of TF-BIV on network performance. In the evaluation, the VM with TF-BIV deployed served as both the client and server, while the other peer was a host with an Intel i5-4590 CPU (3.3 GHz) and 16 GB RAM. We evaluated the overhead in full-duplex (Dual Testing) and half-duplex (Tradeoff Testing) scenarios. As shown in Table 2, the bandwidth decreased by 3.81% when the VM (monitored by TF-BIV) worked as the server in the dual testing. This is because TF-BIV needs to process more connections and filter more packets in the dual testing, where both the client and server send packets.

**Table 2: Bandwidth Evaluation**

| Testing Mode | Native (MB/s) | TF-BIV (MB/s) |
|--------------|---------------|---------------|
| Dual Testing (Client) | 920.67 | 894.00 |
| Dual Testing (Server) | 1089.00 | 1086.00 |
| Tradeoff Testing (Client) | 819.78 | 810.56 |
| Tradeoff Testing (Server) | 632.11 | 608.00 |

#### Related Work
Various in-kernel solutions have been proposed to protect the integrity of user-space programs. Integrity Measurement Architecture (IMA) [65] and its extension PRIMA [45], deployed in the Linux kernel, measure all binaries at load-time based on TPM but fail to provide runtime integrity protection.

Hypervisors, acting as a layer between hardware and the OS, have been leveraged in many solutions. Patagonix [54] and HIMA [5] aim to safeguard the guest VM but lack flexibility, as all binaries running in the VM need to be protected. AppSec [59], AppShield [19], and InkTag [36] protect both the integrity and confidentiality of sensitive applications even if the guest VM OS is untrusted. However, they require modifications to the protected applications. Various schemes (e.g., HyperCheck [81], HyperSentry [6]) have been proposed to protect the hypervisor itself.

Hardware features like Intel SGX [42], AMD SEV [48], and ARM TrustZone [3] have been proposed to provide isolation for sensitive applications [4, 8, 68]. However, these solutions require substantial re-engineering efforts.

Adversaries may hijack control flow without injecting or modifying binaries [10, 11, 14, 63, 70, 77]. Various protections [21, 28, 38, 52, 67, 74] have been proposed for these attacks, which can be integrated with TF-BIV to strengthen sensitive applications.

#### Conclusion
In this paper, we present TF-BIV, a binary integrity verification scheme for the cloud environment, achieving isolation, transparency, TOCTTOU consistency, and fine-grained verification simultaneously. TF-BIV leverages hardware virtualization for transparent and fine-grained verification and uses semantic information obtained through VMI to accelerate the identification of S-processes and integrity verification. TF-BIV registers VM exit events based on hardware features (CR3-load exit, MTF, and EPT volition) to capture process creation, identify dependent code of S-processes, and continuously perform integrity verification. TF-BIV can be easily integrated with real-world applications that need protection. To demonstrate this, we integrated TF-BIV with a real open-source cloud-based cryptographic service. The evaluation shows that the performance overhead introduced by TF-BIV is modest, with less than 3.6% overhead in CPU benchmarking, about 3.81% network overhead, and about 8.3% degradation in the throughput of the cloud-based cryptographic service.

#### Acknowledgment
This work is partially supported by the 973 Program of China (Grant No. 2014CB340603), Informatization Project of the Chinese Academy of Sciences (Grant No. XXH13507-01), Cyber Security Program of the National Key R&D Plan of China (Grant No. 2017YFB0802100), National Natural Science Foundation of China (Grant No. 61772518), National Science Foundation (Grants CNS-1422206 and DGE-1565570), and the National Security Agency (NSA) Science of Security (SoS) initiative.

#### References
[References are listed as in the original text, with no changes made.]