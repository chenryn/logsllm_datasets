RoboTack on the safety potential of the EV with and without
the safety hijacker (SH). Our results indicate that the timing of
the attack chosen by the SH is critical for causing safety hazards
with a high probability of success. In particular, with SH, the
number of successful attacks, i.e., forced emergency brakings
and crashes, when the vehicle trajectories are hijacked, were
up to 5.1× and 7.2× higher respectively, than the number of
attacks induced at random times using only trajectory hijacking.
Attacks that hijacked pedestrian trajectories were 14.8× and
24× more successful, respectively. Fig. 6 shows the boxplot
of the minimum safety potential of the EV measured from
the start time of the attack to the end of the driving scenario.
Recall that we label as an "accident" any driving scenario that
experiences a safety potential of less than 4 meters from the
start of the attack to the end of the attack. We determine the
presence of forced emergency braking by directly reading the
values from the Apollo ADS. In Fig. 6, "R w/o SH" stands for
"Robotack without SH", and "R" alone stands for the proposed
"Robotack" consisting of scenario matcher, trajectory hijacker,
and safety hijacker. Thus, the boxplot labeled "R w/o SH"
indicates that RoboTack launched a trajectory-hijacking-attack
on the EV without SH (random timing), whereas "R" indicates
that RoboTack used the safety hijacker’s decided timing to
launch a trajectory-hijacking-attack. We omit ﬁgures for the
Move_In attack vector because in those scenarios the attacks
did not reduce the δ but caused emergency braking only. The
improvements of "R" on attack success-rate over "R w/o SH"
are as follows.
DS-1-Disappear. RoboTack caused 7.2× more crashes
(31.7% vs. 4.4%). In addition, we observed that RoboTack
caused 4.6× more emergency braking (53.5% vs. 11.6%).
DS-1-Move_Out. RoboTack caused 6.2× more crashes
(17.3% vs. 2.8%). In addition, we observed that RoboTack
caused 5.1× more emergency braking (37.3% vs. 7.3%).
DS-2-Disappear. RoboTack caused 7.9× more crashes
(82.6% vs. 10.4%). In addition, we observed that RoboTack
caused 2.4× more emergency braking (94.4% vs. 39.4%).
DS-2-Move_Out. RoboTack caused 24× more crashes
(84.1% vs. 3.5%). In addition, we observed that RoboTack
caused 14.8× more emergency braking (97.8% vs. 6.6%).
DS-3-Move_In. RoboTack caused 1.9× more emergency
brakings (94.6% vs. 50%). A comparison of the number of
crashes would not apply, as there was no real obstacle to crash.
DS-4-Move_In. RoboTack caused 1.6× more emergency
braking (78.5% vs 48.1%). A comparison of the number of
crashes would not apply, as there was no real obstacle to crash.
Summary. In 1702 experiments (851 "R w/o SH", 851
"R") across all combinations of scenarios and attack vectors,
RoboTack (R) resulted in 640 EBs (75.2%) over 851 "R"
experiments. In comparison, RoboTack without SH (R w/o
SH) resulted in only 230 EBs (27.0%) over 851 "R w/o SH"
experiments. RoboTack (R) resulted in 299 crashes (52.6%)
over 568 "R" experiments excluding DS-3 and DS-4 with
Move_In attacks, while RoboTack without SH (R w/o SH)
results in only 29 (5.1%) crashes over the 568 "R w/o SH"
experiments excluding DS-3 and DS-4 with Move_In attacks.
E. Evading Attack Detection
(cid:4) time-steps, it maintains the faked trajectory.
Recall that the trajectory hijacker actively perturbs the
(cid:4) (cid:9) K time-steps to shift
estimated trajectory for only K
the object position laterally at most by Ω, and it maintains
the trajectory of the object for the next K − K
(cid:4) time-steps,
where K is the total number of time-steps for which the attack
must be active from start to end. Note that RoboTack perturbs
images for all K time-steps. However, RoboTack modiﬁes the
(cid:4) time-steps, whereas
image to change the trajectory for only K
for K − K
(cid:4) for different scenarios
and attack vectors. We observed that Move_Out and Move_In
(cid:4) in order to change the object
scenarios required a smaller K
position to the desired location than the Disappear attack vector
did. Furthermore, changing of a pedestrian’s location required
smaller K
(cid:4) time-steps, the disparity between between the
Kalman ﬁlter’s and the object detector’s output is not ﬂagged
as evidence of an attack because it is within one standard
deviation of the characterized mean during normal situation.
(cid:4) than changing of a vehicle’s location.
Fig. 7(a) and (b) characterize K
In those K
F. Characterizing Safety Hijacker Performance
Here we characterize the performance of our neural network
and its impact on the malware’s ability to cause a safety hazard.
For lack of space, we discuss results only for Move_Out.
Fig. 8(b) shows a plot of the predicted value of the safety
potential (using the NN) and the ground-truth value of the safety
potential after the attack, as obtained from our experiments.
We observe that the predicted value is close to the ground-truth
value of the safety potential after the attack. On average, across
all driving scenarios, NN’s prediction of the safety potential
after the attack was within 5m and 1.5m of the ground-truth
values for vehicles and pedestrians, respectively.
Fig. 8(a) shows a plot of the probability of success (i.e., of
the malware’s ability to cause a safety hazard) on the y-axis
with increasing NN prediction error probability on the x-axis.
We ﬁnd that the success probability goes down as the prediction
error of the safety potential (using NN) increases. However, as
stated earlier, our NN’s prediction errors are generally small.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
122
Figure 6: Impact of attacks. δ: Safety potential. ‘R’: Robotack. ‘R w/o SH’: Robotack without safety hijacker. Dashed red line:
Safety potential δ = 4 meters.
Figure 7: Time-steps K
(a) on vehicle, (b) on pedestrian.
(cid:4) required to move object in/out by Ω
VII. RELATED WORK
Security attacks. AVs are notoriously easy to hack into due
to i) easy physical and software access, ii) the large attack
vectors available to the adversary because of the complexity and
heterogeneity of the software and hardware, and iii) the lack of
robust methods for attack detection. Hence, the insecurity of
autonomous vehicles poses one of the biggest threats to their
safety and thus to their successful deployment on the road.
Gaining access to AVs. Hackers can gain access to the
ADS by hacking existing software and hardware vulnerabilities.
For example, research [26], [29] has shown that an adversary
can gain access to the ADS and launch cyber attacks by
hacking vehicle-to-vehicle (V2V) and vehicle-to-infrastructure
(V2I) communication channels [39], over-the-air software
update mechanisms used by manufacturers [40], electronic
component units (ECUs) [26], infotainment systems [30], and
CAN buses [41]. Another way of hacking an ADS is to use
hardware-based malware, which can be implanted during the
supply chain of AVs or simply by gaining physical access to
the vehicle [26]. In this work, we show an attack approach that
can masquerade as noise or faults and that can be implanted
as malware in either software or hardware.
Adversarial machine learning and sensor attacks in AVs.
A comparison with the current state-of-the-art adversarial ML-
based attacks is provided in §I.
Our attack. We ﬁnd that none of the above work mentioned
attacks geared toward i) evading detection by an IDS or ii)
explicitly targeting the safety of the vehicles. In contrast, Rob-
oTack is the ﬁrst attack approach that has been demonstrated
on production ADS software with multiple sensors (GPS, IMU,
cameras, LiDAR) to achieve both objectives by attacking only
one sensor (the camera).
VIII. CONCLUSION & FUTURE WORK
In this work, we present RoboTack, smart malware that
strategically attacks autonomous vehicle perception systems to
put the safety of people and property at risk. The goal of our
123
Figure 8: (a) NN binned prediction error for DS-1, DS-2
Move_Out attack. (b) DS-1 Move_Out attack, NN safety
potential prediction. ×: ground-truth. •: predicted. k: # of
attacks. δ0: starting safety potential.
research is to highlight the need to develop more secure AV
systems by showing that the adversary can be highly efﬁcient
in targeting AV safety (e.g., cause catastrophic accidents with
high probability) by answering the question of how, when and
what to attack. We believe that the broader implications of our
research are: (i) Knowledge gathered from this type of study
can be used to identify ﬂaws in the current ADS architecture
(e.g., vulnerability in Kalman ﬁlters to adversarial noise) which
can be used to drive future enhancements. (ii) Guide the
development of countermeasures. (iii) Looking forward we
believe these kinds of attacks can be fully automated and
deployed as a new generation of malware.
The design of countermeasures is the subject of our fu-
ture work. Existing literature has shown a large number of
adversarial attacks on these models (e.g., object detection
models and Kalman ﬁlters). Therefore, we are investigating a
broader solution that can dynamically and adaptively tune the
parameters of the perception system (i.e., parameters used in
object detection, Hungarian matching algorithm and Kalman
ﬁlters) to reduce their sensitivity to noise and thus, mitigate
most of these adversarial attacks.
ACKNOWLEDGMENTS
This material is based upon work supported by the National
Science Foundation (NSF) under Grant No. 15-35070 and
CNS 18-16673. We thank our shepherd Kun Sun for insightful
discussion and suggestions. We also thank K. Atchley, J.
Applequist, Arjun Athreya, and Keywhan Chung for their
insightful comments on the early drafts. We would also
like to thank NVIDIA Corporation for equipment donation.
Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reﬂect the views of the NSF and NVIDIA.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] M. Gerla, E. K. Lee, G. Pau, and U. Lee, “Internet of Vehicles:
From Intelligent Grid to Autonomous Cars and Vehicular Clouds,” in
Proceedings of 2014 IEEE World Forum on Internet of Things (WF-IoT),
Mar 2014, pp. 241–246.
[2] S. Jha, S. S. Banerjee, J. Cyriac, Z. T. Kalbarczyk, and R. K. Iyer, “AVFI:
Fault Injection for Autonomous Vehicles,” in Proceedings of 2018 48th
Annual IEEE/IFIP International Conference on Dependable Systems and
Networks Workshops (DSN-W), pp. 55–56.
[3] S. S. Banerjee, S. Jha, J. Cyriac, Z. T. Kalbarczyk, and R. K. Iyer,
“Hands off the wheel in autonomous vehicles?: A systems perspective
on over a million miles of ﬁeld data,” in Proceedings of 2018 48th
Annual IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN).
IEEE, 2018.
[4] S. Jha, T. Tsai, S. Hari, M. Sullivan, Z. Kalbarczyk, S. W. Keckler,
and R. K. Iyer, “Kayotee: A Fault Injection-based System to Assess the
Safety and Reliability of Autonomous Vehicles to Faults and Errors,” in
Third IEEE International Workshop on Automotive Reliability & Test.
IEEE, 2018.
[5] C. Hutchison, M. Zizyte, P. E. Lanigan, D. Guttendorf, M. Wagner,
C. Le Goues, and P. Koopman, “Robustness testing of autonomy
software,” in 2018 IEEE/ACM 40th International Conference on Software
Engineering: Software Engineering in Practice Track (ICSE-SEIP).
IEEE, 2018, pp. 276–285.
[6] S. Jha, S. Banerjee, T. Tsai, S. K. S. Hari, M. B. Sullivan, Z. T.
Kalbarczyk, S. W. Keckler, and R. K. Iyer, “ML-Based Fault Injection
for Autonomous Vehicles: A Case for Bayesian Fault Injection ,” in
Proceedings of 2019 49th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks (DSN), June 2019, pp. 112–124.
[7] P. Koopman, U. Ferrell, F. Fratrik, and M. Wagner, “A safety standard
approach for fully autonomous vehicles,” in International Conference on
Computer Safety, Reliability, and Security. Springer, 2019, pp. 326–332.
[8] B. C. Csáji et al., “Approximation with artiﬁcial neural networks,” Faculty
of Sciences, Etvs Lornd University, Hungary, vol. 24, no. 48, p. 7, 2001.
[9] Baidu, “Apollo Open Platform,” http://apollo.auto.
[10] LG Electronics, “LGSVL Simulator,” https://www.lgsvlsimulator.com/,
[11] Y. Cao, C. Xiao, D. Yang, J. Fang, R. Yang, M. Liu, and B. Li, “Ad-
versarial Objects Against LiDAR-Based Autonomous Driving Systems,”
arXiv preprint arXiv:1907.05418, 2019.
[12] A. Boloor, K. Garimella, X. He, C. Gill, Y. Vorobeychik, and X. Zhang,
“Attacking Vision-based Perception in End-to-End Autonomous Driving
Models,” arXiv preprint arXiv:1910.01907, 2019.
[13] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, and D. Song, “Robust Physical-World Attacks on
Deep Learning Models,” arXiv preprint arXiv:1707.08945, 2017.
[14] J. Lu, H. Sibai, and E. Fabry, “Adversarial Examples that Fool Detectors,”
arXiv preprint arXiv:1712.02494, 2017.
[15] Y. Jia, Y. Lu, J. Shen, Q. A. Chen, Z. Zhong, and T. Wei, “Fooling
Detection Alone is Not Enough: First Adversarial Attack Against Multiple
Object Tracking,” 2019.
[16] J. Lu, H. Sibai, E. Fabry, and D. Forsyth, “No Need to Worry About
Adversarial Examples in Object Detection in Autonomous Vehicles,”
arXiv preprint arXiv:1707.03501, 2017.
[17] K. J. Åström and T. Hägglund, PID Controllers: Theory, Design, and
Instrument Society of America Research Triangle Park,
Tuning vol. 2.
NC, 1995.
[18] W. Luo, X. Zhao, and T. Kim, “Multiple Object Tracking: A
[Online]. Available: http:
Review,” CoRR, abs/1409.7618, 2014.
//arxiv.org/abs/1409.7618
2018.
[19] J. Redmon and A. Farhadi, “YOLOv3: An Incremental Improvement,”
arXiv preprint arXiv:1804.02767, 2018.
[20] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-
Time Object Detection with Region Proposal Networks ,” in Proceedings
of Advances in Neural Information Processing Systems, 2015, pp. 91–99.
[21] G. Welch, G. Bishop et al., An Introduction to the Kalman Filter. Los
Angeles, CA, 1995.
[22] C. Huang, B. Wu, and R. Nevatia, “Robust Object Tracking by
Hierarchical Association of Detection Responses,” in Proceedings of
European Conference on Computer Vision. Springer, 2008, pp. 788–801.
[23] S. M. Erlien, “Shared Vehicle Control Using Safe Driving Envelopes for
Obstacle Avoidance and Stability,” Ph.D. dissertation, Stanford University,
2015.
[24] J. Suh, B. Kim, and K. Yi, “Design and Evaluation of a Driving Mode
Decision Algorithm for Automated Driving Vehicle on a Motorway,”
IFAC-PapersOnLine, vol. 49, no. 11, pp. 115–120, 2016.
[25] K.-T. Cho and K. G. Shin, “Fingerprinting Electronic Control Units for
Vehicle Intrusion Detection,” in Proceedings of 25th USENIX Security
Symposium, 2016, pp. 911–927.
[26] K. Koscher, A. Czeskis, F. Roesner, S. Patel, T. Kohno, S. Checkoway,
D. McCoy, B. Kantor, D. Anderson, H. Shacham et al., “Experimental
Security Analysis of a Modern Automobile,” in Proceedings of 2010
IEEE Symposium on Security and Privacy.
IEEE, 2010, pp. 447–462.
[27] D. Rezvani, “Hacking Automotive Ethernet Cameras,” Publisher: https:
//argus-sec.com/hacking-automotive-ethernet-cameras/, 2018.
[28] “IEEE 802.1: 802.1ba - Audio Video Bridging (AVB) Systems,” Publisher:
http://www.ieee802.org/1/pages/802.1ba.html, (Accessed on 12/12/2019).
[29] Z. Winkelman, M. Buenaventura, J. M. Anderson, N. M. Beyene,
P. Katkar, and G. C. Baumann, “When Autonomous Vehicles Are Hacked,
Who Is Liable?” RAND Corporation, Tech. Rep., 2019.
[30] T. Lin and L. Chen, “Common Attacks Against Car Infotainment sys-
tems,” https://events19.linuxfoundation.org/wp-content/uploads/2018/07/
ALS19-Common-Attacks-Against-Car-Infotainment-Systems.pdf, 2019.
[31] O. Pfeiffer, “Implementing Scalable CAN Security with CANcrypt,”
Embedded Systems Academy, 2017.
[32] K. Manandhar, X. Cao, F. Hu, and Y. Liu, “Detection of Faults and
Attacks Including False data Injection Attack in Smart Grid Using Kalman
Filter,” IEEE Transactions on Control of Network Systems, vol. 1, no. 4,
pp. 370–379, 2014.
[33] Baidu, “Apollo 5.0,” https://github.com/ApolloAuto/apollo.
[34] LG Electronics, “Modiﬁed Apollo 5.0,” https://github.com/lgsvl/apollo-
[35] “Unity Engine,” http://unity.com.
[36] J. Gregory, Game engine architecture. AK Peters/CRC Press, 2017.
[37] “The conﬁg ﬁle specifying these sensor parameters can be accessed at,”
https://www.lgsvlsimulator.com/docs/apollo5-0-json-example/.
[38] NEOUSYS, “Nuvo-6108GC GPU Computing Platform | NVIDIA RTX
2080-GTX 1080TI-TITANX,” https://www.neousys-tech.com/en/product/
application/gpu-computing/nuvo-6108gc-gpu-computing.
[39] I. A. Sumra, I. Ahmad, H. Hasbullah et al., “Classes of attacks in VANET,”
in Proceedings of 2011 Saudi International Electronics, Communications
and Photonics Conference (SIECPC).
IEEE, 2011, pp. 1–5.
[40] A. Sampath, H. Dai, H. Zheng, and B. Y. Zhao, “Multi-channel
Jamming Attacks using Cognitive Radios,” in Proceedings of 2007 16th
International Conference on Computer Communications and Networks.
IEEE, 2007, pp. 352–357.
[41] E. Ya˘gdereli, C. Gemci, and A. Z. Akta¸s, “A Study on Cyber-Security of
Autonomous and Unmanned Vehicles,” The Journal of Defense Modeling
and Simulation, vol. 12, no. 4, pp. 369–381, 2015.
5.0.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
124