4.2.2 Latency & bandwidth
We evaluate the performance of CoverUp for the dura-
tion that a tab is opened, since the usage of CoverUp is
bound to the visiting patterns of involuntary participants
towards the entry server’s sites. Depending on the ser-
vice that the entry server offers, it might not be common
to keep the tab open for a long time or to visit the site
more than a few times a day. For the performance evalu-
ation, we assume each tab to stays open for 10 minutes,
which is in line with recent studies about e-commerce
sites [1, 3].
We say that a session starts after the initial request has
been sent. Table 1 illustrates the goodput (useful data
transmitted), the ﬁrst request delay, and the latency dur-
ing a session. As the privacy leakage is lower in the case
where the feed functionality is enable but the chat func-
tionality is disabled, we use different request delays for
the cases: expected 20 and 300 seconds for ﬁrst request
delay, for feed-only and feed+chat respectively, and ex-
pected 36.55 seconds for the latency during the session in
both cases. Section 7 explains our choice for the delays.
In addition to the delay of the ﬁrst request CoverUp’s
also delays the subsequent requests, however with a dif-
ferent rate. As data is only regularly send after this ﬁrst
request, the feed-only variant of CoverUp has a higher
goodput (11.6 Mbit) per 10 minute-session than the full
feed + chat variant (6 Mbit).
Recall that in all measurements the ﬁxed request pro-
cessing time at the mix server is constant time, ﬁxed at
50 millisecond and doubled when the number of partic-
ipants exceeds a threshold (see Section 4.1). Moreover,
the external application has a polling rate of once every
5 seconds, and the Firefox incurs a 4 seconds-delay be-
fore writing data into the localStorage. In our implemen-
tation, the minimal latency for the chat is, thus, around
7
4.5 second. This time is measured from the moment the
sender dispatched the encrypted chat from his external
application to the time the receiver is able to see the mes-
sage on his external application (the decryption time is
negligible).
5 Privacy notions
This section deﬁnes the privacy notion that CoverUp
achieves: no attacker can tell whether a participant in a
protocol is voluntary or involuntarily (Section 5.1). Sec-
tion 5.2 discusses the connection of our privacy notion to
other known privacy notions from the literature.
5.1 Deﬁning privacy
As a worst-case assumption, the attacker has complete
knowledge about the running time distributions of the
voluntary and the involuntary participants. The attacker
is able to control the content sent by the entry server (i.e.,
the entry server is untrusted) and has full control over the
network link (on the Internet Protocol level). In particu-
lar, the attacker is able to request arbitrary data and exe-
cute arbitrary JavaScript code in the web browsers con-
text of the entry server and is able to drop, modify, and
resend any messages sent over the network.
We overapproximate potential previous knowledge by
granting the attacker the capability to control the partic-
ipant’s behavior.
In particular, this overapproximation
avoids the need to deal with various introduce user be-
havior proﬁles, since the attacker can choose the user
proﬁles that maximize the leakage. Of course, a volun-
tary participant needs a more extended set of input com-
mands than an involuntary participant. To avert any leak-
age to the attacker, the commands for both kinds are like
for the voluntary one, while for the involuntary the sur-
plus is simply ignored. Because this attacker might stress
the OS inﬁnitely, we restrict the input rate of these com-
mands to a ﬁxed rate tuser. Timing leakage is crucial for
the notion that we consider. Since interactive Turing ma-
chine, or other computation and network models, such
as the UC framework, do properly capture timing leak-
age, we use TUC [25] (a reﬁnement of UC) as network
and computation model (see Appendix 14.1 for a brief
description). TUC enables a normal execution of all pro-
tocol parties and an adversary that can measure timing
leakage. As the accuracy of a TUC adversary is not lim-
ited per se and in many deployed operating systems the
accuracy of the timestamps is limited to around 1 mi-
crosecond4, we introduce a limit tnet on the sample rate
of the attacker.
4Often the timestamp also contains nanoseconds but the accuracy is
nevertheless in microseconds.
Similar to other cryptographic deﬁnitions, we use a
machine, called the challenger Ch, to capture the capa-
bilities and the restriction to the attacker and to deﬁne
the task that has to be solved by the attacker. This chal-
lenger chooses one out of two protocols at random and
runs it, one πI modelling involuntary participant or and
the other πV modelling voluntary participant. The chal-
lenger is located between the attacker and the participant,
handles all their communication, enforces all restrictions
(input rate tuser and sampling rate tnet). The attacker
can intercept any network trafﬁc and controls the entry
server. The attacker now has to guess, which scenario
the challenger runs. We let the attacker send commands
that specify the participant’s interaction with the system.
As we quantify over all probabilistic poly-time bounded
(ppt) machines, we implicitly assume that the attacker
has full knowledge about πI and πV .
Example 1: Instantiating the model with CoverUp. In the
case of CoverUp, the scenario πI constitutes a Firefox
browser together with our external application, which
visits an entry server. The attacker determines what
the participant does on the entry server. Only the uni-
directional without any external application is used. In
the other scenario πV , the CoverUp extension is installed
in the Firefox browser, together with our running exter-
nal application. Here, the user utilizes the external appli-
cation explicitly. The attacker tries now to distinguish to
which scenario applies to a speciﬁc participant. To ac-
complish that, he gives commands to the participant as it
would be in πV . If it is in πI, the additional commands
just get ignored. Appendix 14.2 gives a full description
(cid:5)
of πI and πV .
Along the lines of other indistinguishability-based def-
initions, we compare the probabilities of two interac-
tions: a ppt machine A (the attacker) either interacts
with the challenger that internally runs (i) πI or (ii) πV .
We require that no ppt attacker A can distinguish case
(i) from case (ii). Technically, no ppt machine A shall
have a higher probability to output (as a ﬁnal guess) 0 in
case (i) has more than a distance δ away from the prob-
ability that A outputs (as a ﬁnal guess) 0 in case (ii).
In contrast to other indistinguishability-based deﬁnition
and similar to differential privacy [36], we do not require
δ to be negligible in the security parameter, as we also
want to capture systems that do have a small amount of
leakage, such as CoverUp, which is however even after
thousands attacker-observations still small.
Deﬁnition 1. A pair of protocols (πI,πV ) δ -hides the
intention of participation if and only if there is a δ ∈
{y | 0 ≤ y ≤ 1} such that for all probabilistic poly-time
8
machine A we have
(cid:12)(cid:12)(cid:12)(cid:12)Pr[0 ← (cid:104)A | Ch(πI,tuser,tnet )(cid:105))]
− Pr[0 ← (cid:104)A | Ch(πV ,tuser,tnet )(cid:105))]
(cid:12)(cid:12)(cid:12)(cid:12) ≤ δ
where b←(cid:104)X |Y(cid:105) denotes the interaction between the in-
teractive machines X and Y. This interaction stops when-
ever X stops, b is the output of X after it stopped.
Throughout the paper, we use the notion of an at-
tacker’s accuracy, i.e., his probability to guess correctly.
A notion that is also used in the context of classiﬁers.
The δ from the deﬁntion above can be converted into ac-
curacy by accuracy = δ /2 + 0.5.5
Recall that the notion of differential privacy addition-
ally includes a multiplicative factor ε. While our deﬁ-
nitions and results could be generalized to such a mul-
tiplicative factor ε – ending up with computational dif-
ferential privacy –, we omitted the ε (thus concentrating
on ε = 0) in order to simplify the interpretation of our
deﬁnition and results.
5.2 Connections to other properties
Our privacy notion implies several well-known notions.
We discuss the relations below. While the following ar-
guments and statements can be made more precise, be-
low we present the arguments only informally, in order to
illustrate that our privacy notion is suited for anonymous
communication networks.
k-(sender) anonymity. The notion of k-anonymity is
a common for characterizing the anonymity of an ACN.
This notion guarantees that an attacker cannot guess with
signiﬁcantly more than a probability of 1/k by whom a
message was sent. If k-anonymity is broken, an attacker
knows that a message was sent by a particular partici-
pant. Hence, the attacker can choose a pair of partici-
pants, for which it knows that it was a voluntary partic-
ipants. Hence, with the set of all involuntary and vol-
untary participants not controlled by the attacker as an
anonymity set, our privacy notion implies (by contrapo-
sition) k-anonymity.
By covering all (uncompromised) involuntary partic-
ipants into the anonymity set, CoverUp achieves strong
anonymity properties. This is the main strength of this
work.
5For a set of true positives TP, false negatives FN, true negatives
TN, and false positives FN, Deﬁnition 1 can be rewritten as |T P|/|T P∪
FN|−|FP|/|FP∪ T N| ≤ δ . For accuracy = |T P∪ T N|/|T P∪ FN ∪
FP∪T N|, if |T P∪FN| = |T N ∪FP| then we get 2∗accuracy−1 ≤ δ .
9
Plausible deniability. Plausible deniability is very
close to our privacy notion. Plausible deniability means
that a voluntary participant can always plausibly deny,
by way of presenting evidence for the contrary, that
it did not voluntarily participate in CoverUp.
If plau-
sible deniability would be violated, an attacker would
be able to distinguish voluntary and involuntary partici-
pants. Hence, by contraposition, our notion implies plau-
sible deniability.
6 Privacy analysis of CoverUp
This section analyzes the privacy of CoverUp. Sec-
tion 6.1 shows that CoverUp has solely timing leakage.
Section 6.2 discusses potential sources of indirect pri-
vacy leakage, and Section 6.3 discusses the implications
of a malicious CoverUp and a malicious mix server.
6.1 Reduction to timing leakage
For a model of CoverUp we show that solely the tim-
ing differences in the request and response times leak
information. As an intermediary step, we observe that
if the involuntary participant would use some delays no
information leaks (Lemma 1). Then, we show that the
statistical distance of these timing differences fully char-
acterizes the privacy leakage (Theorem 1).
In our model of πI of voluntary and πV of involuntary
participants we make some simplifying assumptions. We
use the two protocols πI and πV from Example 1 (the full
description can be found in Appendix 14.2). While these
protocols exclude many secondary effects by the OS and
the browser, we argue that our analysis is still valuable.
πI and πV exclude effects of the browser caused by run-
ning an additional extension, effects of the OS caused by
running CoverUp’s external application, and effects by
the native messages between the external application and
the extension. As these ignored effects solely introduce
additional timing leakage, it sufﬁces to prove that despite
timing leakage there is no leakage, for which our model
is sufﬁcient. Section 7 measures this timing leakage on
real systems, including these secondary effects.
As a next step, we argue that πV is indistinguishable
from a variant of πI that includes additional delays. For
this observation, we need a technical notion. The timing
transcript of an interaction is the projection of all mes-
sages of the transcript to a constant value, say 0, which
models that only the time at which a message was are
observable. We say the bucketing of a timing transcript
according to a sampling rate tnet is the tnet-timing tran-
script. Moreover, we call the statistical distance of two
distributions (of transcripts) the timing leakage of these
two distributions with respect to the sampling rate tnet.
See Appendix 14.5 for the full lemma and a detailed
proof. This observation implies that an active attacker
that, e.g., holds back messages, cannot learn more than
a passive eavesdropper. Hence, it sufﬁces to concentrate
passive eavesdroppers.
We use Lemma 1 to show that the timing leakage be-
tween πI and πV already completely characterizes how
well an attacker can distinguish πI from πV . The main
proof idea for Theorem 1 is that πI and πI + Γ are
solely distinguishable by their timing leakage. Since by
Lemma 1 πI + Γ and πV are indistinguishable, πI and πV
are only distinguishable by their timing leakage (for the
full proof see Appendix 14.5).
Theorem 1. If the timing leakage of πI and πV is at most
δ for a sampling rate tnet, and if πI and πV use a secure
channel, then for πI and πV we have δ + µ-hide the in-
tention of participation, for some negligible function µ
(in the sense of Deﬁnition 1).
Proof. The two protocols πI + Γ and πI are distinguish-
able by at most δ probability, since their transcripts are
exactly the same except for the timing trace. Due to
Lemma 1, we know that the timing leakage of πI +Γ and
the timing leakage of πV are indistinguishable. More-
over, we know by assumption that the timing leakage of
πV and πI is at most δ . Hence, the timing leakage of πI
and πI + Γ is at most δ .
(1)
|Pr[πI]− Pr[πI + Γ]| ≤ δ
Plugging our results together, we get
(cid:12)(cid:12)Pr[πI]− Pr[πV ](cid:12)(cid:12)
=(cid:12)(cid:12)Pr[πI]− Pr[πI + Γ] + Pr[πI + Γ]− Pr[πV ](cid:12)(cid:12)
+(cid:12)(cid:12)Pr[πI + Γ]− Pr[πV ](cid:12)(cid:12)
≤(cid:12)(cid:12)Pr[πI]− Pr[πI + Γ](cid:12)(cid:12)
(cid:123)(cid:122)
(cid:125)
(cid:125)
(cid:124)
(cid:124)
(cid:123)(cid:122)
Lemma 1≤ µ
Equation (1)≤
δ
≤ δ + µ
Indirect privacy leakages
6.2
This section discusses indirect privacy leakages which
are not direct consequences of CoverUp.
Browsing privacy.
Involuntary users of CoverUp po-
tentially reveal
their browsing behavior to CoverUp
server, as a malicious CoverUp server can read HTTP
header’s referer ﬁeld. This leakage is inherent in our ap-
proach to use an entry server and to utilize involuntary
participants to produce cover trafﬁc. While this leakage
exists, we would like to put it into perspective. Many
popular web sites already leak this information to other
services, such as advertisement networks or external an-
alytic tools, such as Google Analytics.
10
Suspicious behavior leaks privacy. Another source
of indirect privacy leakage would be that the usage
of CoverUp may unconsciously inﬂuence the behavior
of voluntary participants, e.g., if voluntary users spend
more time on a speciﬁc entry server in order to use
CoverUp thus signiﬁcantly reduce the anonymity set.
Recent studies show that the average visiting time of
e-commerce website is between 8.7 and 10.3 minutes
(2016 Q1) [1, 3]. Potentially, such a knowledge can be
used by an attacker to distinguish voluntary participants
from involuntary participants. To mitigate this CoverUp
extension can alert users when they spent too much time
on the entry server. Another way of mitigating this prob-
lem is to adjust the entry server’s webpage to ask the vis-
itors of the entry webpage when they are closing the tab
“Do you want to keep the tab open to increase the pri-
vacy of CoverUp? The tab will be automatically closed
after X minutes.” With some piece of JavaScript code it
is possible to automatically close the tab after X minutes.
Browser proﬁling. Potentially,