are generated per fuzzing iteration.
respect to s(i), which is
The LINEAR mode increases the energy of state i linearly with
The QUAD mode is based on the LINEAR mode, and the energy
of fuzzing is computed as
(cid:19)
(cid:19)
(cid:19)
(cid:18) α(i)
(cid:18) α(i)
(cid:18) α(i)
β
β
pl(i) = min
pd(i) = min
· s(i)
f (i) , M
· s(i)2
f (i) , M
(2)
(3)
q(i) = max(cid:0)p(i), L(cid:1)
All these three power schedules assign energy to state i inversely
proportional to f (i), which is the reason for cycle explosion. When
f (i) grows to a large value, the energy is very low. To improve
AFLFast, one way is to set a lower bound to its energy strategies.
L is a lower bound set for AFLFast. Therefore, AFLFast can be
improved as
(4)
where p(i) stands for pf (i), pl(i), or pd(i), and q(i) is the improve-
ment of p(i) respectively.
Evaluation On the 90 Binaries. We evaluate our AFLFast+ on
the 90 binaries from the above experiment. Fig.6 shows that the
efficiency of AFLFast and AFLFast+ is almost the same. However,
AFLFast+ finds 88 bugs while AFLFast finds only 77 bugs. Mean-
while, the reason of the two bugs not being found by AFLFast+ is
due to the limited time, and the cycle explosion does not appear.
Therefore, AFLFast+ keeps the efficiency but can detect more bugs
than AFLFast.
6 RELATED WORK
The performance of a fuzzer is mainly determined by its capabil-
ity of handling search-hampering features in the contexts of bugs.
Figure 6: The results of AFLFast and AFLFast+ which are
tested on 90 binaries. One dot or triangle is a bug found.
As search-hampering features are important to the fuzzing per-
formance, most fuzzers [7, 9, 12, 15, 19–21, 25, 26] are tailored to
resolve one/several features to increase the coverage of code.
So far, researchers have already made some efforts to address the
above evaluation challenge. For example, some researchers prefer
to create corpora based on real-world programs, typically from
student code [24], existing bug report databases [16], or by creat-
ing a public bug registry [3, 11]. Despite these proposals provide
corpora contextual details of bugs, they have remained static and
relatively small for fuzzing evaluation. There are also some tries
on creating independently defined public benchmark suites, e.g.
DaCapo [6] and SPEC [5]. Among these attempts, even though they
collect a large volume of real-world vulnerable programs, it is still
painstaking to manually triage the crashes and filter the bugs in
those programs. Before a strong community effort has been applied
to these suites, it is believed that they are not sufficient to support
fuzzing evaluation [14].
An alternative way to create fuzzing corpora is to synthesize
vulnerable programs. Typical examples include early corpora for
buffer overflow detection [27, 28], LAVA and next version LAVA-M
[10], DARPA CGC corpus [1], as well as corpus drawn from NIST
SAMATE project [4]. The synthetic corpora ensure the existence
of bugs by sacrificing their reflection on real-world ones. However,
to the best of our knowledge, none of the above synthetic corpora
0600012000180002400030000Time (second)(A) Execution paths found in 31380 seconds0204060The number of execution pathsAFLAFLFast0600012000180002400030000Time (second)(B) Execution speed (per second: /s)0102030405060The number of runs (/s)AFLAFLFast0600012000180002400030000Time (second)(C) The number of cycles that finished051015The number of cycles106AFLAFLFast3000600030405002550051015100.20.40.60.811.21.41.61.82Time(second)104101520253035404550The number of execution pathsAFLFastAFLFast AverageAFLFast+AFLFast+ AverageSession 9: FuzzingAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand662have implemented contexts of bugs pertaining to search-hampering
features. Therefore, synthetic corpora also cannot support fuzzing
evaluation according to aforementioned analysis.
7 DISCUSSION: THREAT TO VALIDITY
Although FEData works well on AFL and AFLFast, it can be further
improved. Issues such as the ways to evaluate FEData and improve
FEData can be further discussed.
Firstly, we only run AFL and AFLFast on FEData because other
fuzzers are unavailable or cannot be appropriately run. In order to
further assess FEData, different fuzzers should be run on FEData. In
order to assess the feature of magic value in FEData, a better way
is to use fuzzers trying to help resolve magic values, such as Driller
and T-Fuzz. Similarly, it is better to assess the feature of checksum
in FEData by fuzzers focusing on checksums. We will continue our
investigation to run more fuzzers on FEData.
Secondly, to customize search-hampering features in the contexts
of bugs, FEData sacrifices some realness of the generated programs.
To make FEData more similar to real-world programs, one way is
to add functionality by functional programming. Another way to
improve the realness of FEData is to research the contexts behind
bugs, which can be added to FEData when a bug is inserted.
8 CONCLUSION
In order to evaluate fuzzing more specifically and effectively, we
propose generating corpora based on search-hampering features.
Details about the features are described in this paper. Further, we
design a prototype corpus, FEData, to show the effectiveness of our
idea. AFL and AFLFast are evaluated on FEData. The advancement
of AFLFast, which is AFLFast finds execution paths faster than
AFL, is supported by FEData. However, the drawback of AFLFast is
magnified by some programs in FEData. We find that AFLFast is
prone to be trapped in cycle explosion if bugs are embedded deeply
or fuzzers hit some specific execution paths in a large quantity.
Therefore, AFLFast is improved to AFLFast+ via setting a lower
bound to its energy strategies. The results show that AFLFast+ can
find more bugs than AFLFast while the efficiency stays the same.
ACKNOWLEDGMENTS
The authors would like to thank Dr. Toby Murray and Dr. Jianhai
Chen. Thanks for discussing with us on important issues.
REFERENCES
[1] . 2017. Cyber Grand Challenge Corpus. http://www.lungetech.com/cgc-corpus/.
[2] . 2018. American Fuzzy Lop. http://lcamtuf.coredump.cx/afl/.
[3] . 2018. Fuzze Test Suite. https://github.com/google/fuzzer-test-suite.
[4] . 2018. National Institute of Standards and Technology. https://www.nist.gov.
[5] . 2018. Standard Performance Evaluation Corporation. https://www.spec.org
/benchmarks.html.
[6] Stephen M Blackburn, Robin Garner, Chris Hoffmann, Asjad M Khang, Kathryn S
McKinley, Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel Frampton,
Samuel Z Guyer, et al. 2006. The DaCapo benchmarks: Java benchmarking
development and analysis. In ACM Sigplan Notices, Vol. 41(10). ACM, ACM, New
York, NY, USA, 169–190.
[7] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-
based greybox fuzzing as markov chain. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security. ACM, IEEE, Vienna, Aus-
tria, 1032–1043.
[8] Peng Chen and Hao Chen. 2018. Angora: Efficient Fuzzing by Principled Search.
In 2018 IEEE Symposium on Security and Privacy. IEEE, San Francisco, CA, USA,
711–725.
[9] Jake Corina, Aravind Machiry, Christopher Salls, Yan Shoshitaishvili, Shuang
Hao, Christopher Kruegel, and Giovanni Vigna. 2017. Difuze: interface aware
fuzzing for kernel drivers. In Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security. ACM, ACM, Dallas, Texas, USA,
2123–2138.
[10] Brendan Dolan-Gavitt, Patrick Hulin, Engin Kirda, Tim Leek, Andrea Mambretti,
Wil Robertson, Frederick Ulrich, and Ryan Whelan. 2016. LAVA: Large-scale
automated vulnerability addition. In Security and Privacy, 2016 IEEE Symposium
on. IEEE, IEEE, SAN JOSE, CA, USA, 110–121.
[11] Jeffrey Foster. 2005. A call for a public bug and tool registry. In Workshop on the
Evaluation of Software Defect Detection Tools.
[12] Shuitao Gan, Chao Zhang, Xiaojun Qin, Xuwen Tu, Kang Li, Zhongyu Pei, and
Zuoning Chen. 2018. CollAFL: Path sensitive fuzzing. In 2018 IEEE Symposium
on Security and Privacy. IEEE, IEEE, San Francisco, CA, USA, 679–696.
[13] Istvan Haller, Asia Slowinska, Matthias Neugschwandtner, and Herbert Bos. 2013.
Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations.. In
Presented as part of the 22nd USENIX Security Symposium (USENIX Security 13).
USENIX, Washington, D.C., 49–64.
[14] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018.
Evaluating Fuzz Testing. In Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security. ACM, ACM, Toronto, Canada, 2123–
2138.
[15] Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu,
and Alwen Tiu. 2017. Steelix: program-state based binary fuzzing. In Proceedings
of the 2017 11th Joint Meeting on Foundations of Software Engineering. ACM, ACM,
Paderborn, Germany, 627–637.
[16] Shan Lu, Zhenmin Li, Feng Qin, Lin Tan, Pin Zhou, and Yuanyuan Zhou. 2005.
Bugbench: Benchmarks for evaluating bug detection tools. In Workshop on the
evaluation of software defect detection tools, Vol. 5.
[17] Barton P Miller, Louis Fredriksen, and Bryan So. 1990. An empirical study of the
reliability of UNIX utilities. Commun. ACM 33, 12 (1990), 32–44.
[18] Marius Muench, Jan Stijohann, Frank Kargl, Aurélien Francillon, and Davide
Balzarotti. 2018. What you corrupt is not what you crash: Challenges in fuzzing
embedded devices. In 25th Annual Network and Distributed System Security Sym-
posium, NDSS 2018, San Diego, California, USA, February 18-21, 2018. The Internet
Society, San Diego, CA, USA.
[19] Hui Peng, Yan Shoshitaishvili, and Mathias Payer. 2018. T-Fuzz: fuzzing by
program transformation. In 2018 IEEE Symposium on Security and Privacy. IEEE,
IEEE, San Francisco, CA, USA, 697–710.
[20] Van-Thuan Pham, Marcel Böhme, and Abhik Roychoudhury. 2016. Model-based
whitebox fuzzing for program binaries. In Automated Software Engineering (ASE),
2016 31st IEEE/ACM International Conference on. IEEE, IEEE, Singapore, Singapore,
543–553. https://doi.org/10.1145/2970276.2970316
[21] Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuffrida,
and Herbert Bos. 2017. Vuzzer: Application-aware evolutionary fuzzing. In 24th
Annual Network and Distributed System Security Symposium, February 26 - March
1, 2017. The Internet Society, San Diego, California, USA.
[22] Sergej Schumilo, Cornelius Aschermann, Robert Gawlik, Sebastian Schinzel, and
Thorsten Holz. 2017. kAFL: Hardware-assisted feedback fuzzing for OS kernels.
In 26th USENIX Security Symposium (USENIX Security 17). USENIX Association,
Vancouver, BC, 167–182.
[23] Koushik Sen. 2007. Concolic testing. In Proceedings of the twenty-second IEEE/ACM
international conference on Automated software engineering. ACM, 571–572.
[24] Jaime Spacco, David Hovemeyer, and William Pugh. 2005. Bug specimens are
important. In Workshop on the Evaluation of Software Defect Detection Tools.
[25] Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In
3rd Annual Network and Distributed System Security Symposium, NDSS 2016, San
Diego, California, USA, February 21-24, 2016, Vol. 16. NDSS, The Internet Society,
San Diego, California, USA, 1–16.
[26] Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. 2017. Skyfire: Data-driven
seed generation for fuzzing. In 2017 IEEE Symposium on Security and Privacy.
IEEE, IEEE, San Jose, CA, USA, 579–594.
[27] John Wilander and Mariam Kamkar. 2003. A Comparison of Publicly Available
Tools for Dynamic Buffer Overflow Prevention.. In Proceedings of the Network and
Distributed System Security Symposium, NDSS 2003, Vol. 3. The Internet Society,
San Diego, California, USA, 149–162.
[28] Misha Zitser, Richard Lippmann, and Tim Leek. 2004. Testing static analysis
tools using exploitable buffer overflows from open source code. In Proceedings
of the 12th ACM SIGSOFT International Symposium on Foundations of Software
Engineering, Vol. 29(6). ACM, ACM, Newport Beach, CA, USA, 97–106.
Session 9: FuzzingAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand663