mark in isolation with up to 1000 repetitions.
Cryptographic Operations. Table 1 summarizes the costs
of the crypto operations involved in Droplet on four differ-
ent platforms. All these operations, namely AES encryption,
SHA hash, and ECDSA signature are performed once per
chunk for store requests. For data retrieval, the client does
not perform a signature veriﬁcation, since AES-GCM has
built-in authentication. Running the crypto operations only
in software on the IoT devices shows the highest cost, with
3.4k encryptions/hashes per sec and only 3.7 signatures per
sec. With the onboard hardware crypto, the cost of AES and
SHA is improved by one order of magnitude and approaches
that of smartphones. Note that overall signatures are three
orders of magnitude slower than symmetric key operations.
Crypto-based Access. Hash computations are the basis for
dual-key regression. The computation occurs at the initial
setup and each key update if the client chooses to re-compute
keys on-demand rather than store them. Assuming a chain
length of 9000 (hourly key updates for one year), it takes
405 ms to compute the entire chain on smartphones and 2.7 s
on an IoT device without a hardware crypto engine. With
compact hash chains, we reduce this worst-case compute time
to 4.3 and 28.2 ms, respectively. The performance gains be-
come pronounced with smaller epoch intervals. The hash tree
induces O(log n) computations for n keys, which amounts to
48 µs (laptop) with 230 keys.
The per chunk overhead consists of key computation (hash
tree and dual-key regression), chunk encryption, key encryp-
tion, and signature, which amounts to 1.5 ms (laptop) without
caching. Compared to ABE (§10), Droplet’s crypto-based
data access is by a factor of 57x faster. E.g., with ABE per
chunk overhead with only two attributes (timestamp for tem-
poral access and data type) amounts to 86 ms (laptop).
Feasibility for IoT. To assess if Droplet is viable for the IoT,
we validate its practicality for low-power devices, concern-
ing their constraint resources (Table 1). Crypto operations
are the most expensive ones on a data producer, and beyond
that, no connectivity to the authorization services is required.
Today, most IoT devices are equipped with crypto accelera-
tors for AES encryption integrated with their radios; however,
accelerators for hash functions and signatures have yet to
USENIX Association
29th USENIX Security Symposium    2479
1632641282565121024Vanilla6ecure        1umber of nodeV                                Ama]on 63050100150200250ThroughSut [get/V]16326412825651210249anilla6ecure       1umber of nodeV                                   Ama]on 630306090120150180Time [mV]Vtoregetrouting Vtorerouting getbecome the norm. Nevertheless, Droplet is feasible on legacy
IoT devices without accelerators despited 1.5x slower signing
operations. In terms of impact on the energy budget, the sig-
nature consumes only 9 to 25mJ. Considering a wearable’s
lithium-polymer battery capacity of 1.2 Wh (4.32 kJ), and a
48h charge cycle, 3 signatures/minute (8.6 with accelerator)
can be computed with 5% of the energy budget.
8.2 System Performance
To model the real-world performance of Droplet, we con-
structed an end-to-end system setup, where we use our three
apps datasets. Note that we do not cache any data to emu-
late worst-case scenarios. The stream chunk size is set to
8 KiB. We evaluate get and store requests to the storage
layer, which include the overhead of Droplet’s access control.
Serverless Computing. In the serverless setting, Lambda
either runs Droplet for the access control or uses the AWS
Cognito service, which runs OAuth2, as the baseline. Lambda
with both Droplet and Cognito exhibits a latency of around
118 ms (0.4% longer with Droplet). Note that with OAuth2,
to reach the same level of access granularity as with Droplet,
separate access tokens are required for each data chunk, which
is impractical. This is why in practice, long-lived and more
broadly-scoped access tokens are granted.
Cloud. We extend AWS S3 storage with Droplet and compare
its performance against vanilla S3. Figure 9(a) shows the
throughput for different request types. We follow Amazon’s
guidelines to maximize throughput: e.g., the chunk names are
inherently well distributed allowing the best performance of
the underlying hash-table lookup. The vanilla S3 throughput
of 211 gets/s is within Amazon’s optimal range (100-300).
With Droplet, we maintain an average rate of 204 get/s (3%
drop). Figure 9(b) shows the latency for individual store and
get operations. In Droplet, the latency overhead is 13% for get
and 11% for store (incl. crypto). Part of the overhead is due to
the expensive signature operation. Also, there is an overhead
for a fresh lookup of access permissions at the access control
DB of the virtualchain.
Distributed Storage We measure the performance of get and
store requests on a secure DHT with Droplet, with varying
network sizes, from 16 to 1024 nodes. Figure 9(a) shows the
throughput results. As the number of nodes increases from
16 to 1024, the performance decreases from 142 to 96 get/s.
Figure 9(b) shows the latency results, divided into routing and
retrieval. The total get latency increases from 76 to 140 ms as
the number of nodes grows. This is about 3 times slower than
S3’s centralized storage. However, note that the routing cost
dominates this slowdown. After resolving the address of the
storage node, which holds the data chunk, the secure retrieval
time is similar to that of S3. Also, note that get requests have
a lower routing overhead than store requests. This is because
for get requests, the routing process is aborted as soon as a
node holding the data chunk is found.
Figure 10: EccoViz app results. Retrieving records from the
energy data set in the EccoViz dashboard app (p2p storage).
Applications. The three applications we deploy atop Droplet,
vary in terms of type, size, and granularity of collected data.
Fitbit and Ava are both smartphone apps, where users view vi-
sualized summarizes about their collected data and set goals.
We enhance both demo apps, with additional views where
the user can selectively share parts of their data (e.g., heart-
rate/body-temperature/steps) with friends or services over
Droplet. ECOViz dashboard is a web app that visualizes en-
ergy consumption from smart-meters. Users can set access
permissions per data stream, and they can only view streams
to which they have been granted access. The user experi-
ence of sharing via Droplet remains similar to that of existing
sharing methods. Users initially register a data stream either
consisting of a single or multiple data types (e.g., sensitive
data types can be highlighted to prevent accidental sharing).
Afterward, they can add or remove users to/from their data
streams (e.g., the iOS native Health app allows per data stream
sharing decisions for third-party apps, similar to our subscrip-
tion mode).
To measure the overhead induced by Droplet, we quantify
the overhead of store and get data requests for different views
(i.e., each access requires cryptographic operations and access
permission checks). We now discuss the decentralized storage
setting with 1024 nodes. Due to memory constraints, data
synchronization is required at least weekly for Fitbit and daily
for Ava devices. This results in an average store latency of
176 ms and 1.2 s for Fitbit and Ava, respectively. Note that
store operations run in the background. For different views,
the maximum get latency is below 150 ms. Hence, the user
experience remains unaltered.
In contrast to Fitbit and Ava, the smart meter node has direct
Internet connectivity. Instead of synchronizing periodically, it
stores chunks after generation. This takes 176 ms per chunk.
The most comprehensive view in the ECOViz dashboard can
visualize the entire data stream. Figure 10 shows the latency
to fetch chunks dependent on the number of days requested.
Fetching data for 128 days of 6 h chunk size takes about 10 s,
whereas the one-week size takes less than 1 s.
Scalability. Droplet’s scalability can be examined from three
angles; (i) Read throughput of authorization; read operations
are performed in O(1), after the authorization agent bootstraps
the Access Control State Machine DB. Scaling to handle high
read throughputs, is a matter of increasing the number of au-
thorization agents. (ii) Storage of access permissions; Droplet
2480    29th USENIX Security Symposium
USENIX Association
12481632641281umber of DDys100100010000Time [ms]6h chunNs12h chunNs1d chunNs1w chunNsanchors indirections in the blockchain (§4), as we store
access policies and metadata off-chain. Hence, to scale with
the growing number of access permissions, the allocated off-
chain storage is dynamically increased. As Droplet scales to
a more signiﬁcant number of data streams, the access permis-
sion logic consequently grows. The individual authorization
agents are not impacted by this growth, as they only store
the state for the resources they serve. The annual meta-data
storage costs4 for a billion user-base with an average of 100
streams and 100 consumers per stream, would amount to less
than $0.001 per user today, which accounts for a fraction of
the actual storage costs of streams. (iii) Write throughput;
represents the scaling bottleneck of Droplet, as access permis-
sion updates are bound to the write throughput of the underly-
ing blockchain. Although we consider several optimizations
(e.g., grouping access updates) to contain this constraint, it
remains a bottleneck. In our current prototype, the transaction
conﬁrmation time is set to 15 s, similar to that of Ethereum.
The slow blockchain writes have a direct impact on the time
until new access permissions take effect, which is signiﬁ-
cantly higher compared to OAuth2 protocol. Read-throughput
is, however, fast and comparable to that of OAuth2. Data
stream registrations and access permission adjustments (e.g.,
grant/revoke access) require transaction writes. To understand
the extent of scaling authorization writes in Droplet with an
example, consider Fitbit with 25 Million active users, which
logged 4.7 million group-join events in 2017 [48], which
would require 0.14 transactions per (tps). However, to scale
Droplet to billions of data streams, a blockchain throughput
of a few thousand tps is necessary (assuming 25% of streams
require an access permission modiﬁcation per day). While
currently deployed blockchains achieve only a fraction of this
throughput, scaling to higher throughput is an active area of
research, and next-generation blockchains already support
several thousand tps [69](§9).
9 Discussion
We highlight some research questions that remain open.
Beyond IoT. An authorization service with Droplet’s
properties is crucial for systems that advocate for data
sovereignty [44, 104, 110] or handle privacy-sensitive data,
e.g., sharing medical records [13], and humanitarian aid [73].
The storm of recent privacy incidents [20, 35] has prompted
a rethinking of this space. Moreover, decentralized storage
services that run on blockchain (e.g., Filecoin) can integrate
Droplet for data sharing. Services with varying trust assump-
tions can, however, run Droplet’s authorization log instead by
a federated set of servers.
Usability. Droplet is a user-centric system that empowers
data owners with control over their data. While we design
Droplet’s API to abstract away system complexities from
users and mimic current data sharing abstractions, some
4S3 frequent access tier, over 500 TB/Month, $0.021/GB, May 2020.
usability considerations remain open in this user-centric
paradigm. In this paradigm, users will potentially be con-
fronted with more decisions to make regarding their data.
Hence, it is essential to study and design abstractions and
interfaces that mitigate usability concerns that might arise in
this paradigm. In an end-to-end encryption model, protection
and recovery mechanisms for private master keys should be
addressed with adequate solutions. For instance, Shamir’s se-
cret sharing scheme [95] allows reconstruction of the secret
from a set of recovery keys which are, e.g., distributed among
the data owner’s devices [106] or a group of friends [87]. The
recovery keys collectively reconstruct a master secret key.
Blockchain Scalability. In §8, we discussed scalability as-
pects of Droplet and how the underlying blockchain, which re-
alizes the decentralized authorization log, can impact the write
throughput within Droplet. Next-generation blockchains [28,
45, 52, 67, 69, 78] particularly tackle the scalability aspects
and promise higher throughputs and lower latencies, which is
crucial for the adoption of blockchain-based systems in retail
payments and ﬁnancial sector, and for realizing large-scale
decentralized applications. Recent works [67, 69] introduce a
hybrid consensus by combining the slow PoW to bootstrap the
faster PBFT algorithm, where for each epoch, a random set
of validators is selected. Hence, they bring both worlds’ best:
secure open enrollment and high throughput and low latency.
These scalable blockchain protocols, e.g., OmniLedger [69],
lay the groundwork enabling practical advanced decentralized
services, such as Droplet. Droplet can be deployed on top of
any blockchain that supports the total ordering of transactions,
as elaborated in §4.2.
10 Related Work
We now brieﬂy discuss key relevant works to Droplet.
Crypto-enforced Data Access. End-to-end encryption pro-
vides the strongest level of protection for data stored in the
cloud, as data remains encrypted and only authorized enti-
ties are trusted with decryption keys. However, ﬁne-grained
access and sharing of data is a challenge here. A simple ap-
proach to selective sharing of encrypted data is to encrypt the
target data towards the principal’s public key; although simple
this approach suffers from three drawbacks: (i) hard-coded
access control [73]; at encryption time the access permis-
sion is deﬁned and cannot subsequently be altered or revoked,
(ii) storage overhead; if the same data is shared with multi-
ple principals, the user ends up storing redundant data as she
needs to encrypt the same data under each principal’s public
key, and (iii) scalability and practicality issues particularly
when considering ﬁne-grained access policies. These draw-
backs are pronounced with time series-data, where high vol-
ume of data is continuously produced and a high key-rotation
is necessary to ensure ﬂexible access control.
Various cryptographic schemes [8, 23] have been intro-
duced to overcome some of these challenges, among which
USENIX Association
29th USENIX Security Symposium    2481
attribute-based encryption (ABE) [2, 55, 56, 91, 106] offers
high expressiveness. Several ABE-based systems [106, 108]
introduce crypto-based access control. However, ABE suf-
fers from expensive crypto operations and the costs grow
linearly with the number of attributes, limiting the granularity
of access due to computational burdens [2, 51]. The overhead
dominates even with a hybrid encryption technique [106,108],
where data is encrypted with symmetric encryption and only
encryption keys are encrypted with the expensive ABE, e.g.,
only two attributes result in 100 ms for enc/decryption on
desktops and few seconds on IoT devices [107]. FAME [2]
exhibits a constant decryption time (60 ms), however, encryp-
tion time increases linearly with the number of attributes.
The notion of time-encoded keys in our access control is
similar to Time-Speciﬁc Encryption (TSE) [29, 85]. TSE as-
signs objects to temporal intervals and for each time instance
a unique key is generated. Our scheme differs from TSE in
that no central trusted time server is required for the genera-
tion and broadcast of epoch keys. In Droplet, each data source
generates the data encryption keys per epoch locally, and key
distribution is handled over Droplet’s decentralized network.
Distributed Authorization. Current distributed authoriza-
tion protocols, such as OAuth2 [75] and Macaroons [21],
suffer from several limitations, as highlighted in §6. Signature-
based schemes (e.g., public-key certiﬁcates [22, 42]) require