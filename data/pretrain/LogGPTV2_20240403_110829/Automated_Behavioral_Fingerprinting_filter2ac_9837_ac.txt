classiﬁcation rate. This is conﬁrmed by the increase of the average classiﬁcation
rate per type. This means that some small clusters are found too. Moreover, the
associated standard deviation is reduced for the same reason.
The character position metric is accounting for that the ﬁrst characters in a
message are most probably relevant for the message identiﬁcation. For instance,
the INVITE message has two “I” in the ﬁrst 7 bytes, and thus a good character-
istic of this message is that this letter is more present at the beginning. However,
an INVITE message contains basically a long payload formed with uri, param-
eters and so on. It may also contain “I” because its length is much higher than
the INVITE keyword. The weighted character position metric gives more impor-
tance to ﬁrst characters. The results plotted in ﬁgure 8(a) are very good as it
is possible to ﬁnd all the diﬀerent kinds of message with a global and per type
classiﬁcation rate close to 85%. The details of the classiﬁcation are illustrated
in ﬁgure 8(b). In fact, the misclassiﬁed messages are shared out within several
192
J. Fran¸cois et al.
 1
0.8
0.6
0.4
0.2
 0
 0.25
 0.2
 0.15
 0.1
 0.05
 0
0.1
1
10
(a) Accuracy
Unclassified
Classified
1-4
1-2
0-
0
O
0
1-1
0-
0
TI
R
0
P
E
G
O
N
A
0-
0-IN
0-
0
VIT
1
IS
T
C
E
1-1
0-
0
8
1-4
TIF
0
B
7
O
K
Y
N
S
E
R
O
S
S
A
G
E
1-5
0-IN
0-
F
0
1
Y
1-1
1-4
1-5
1-4
1-4
1-2
8
0
0
8
8
3
4
3
1
6
0-
0
1-6
0-
0
R
2
C
3
E
E
M
1-4
1-4
8
A
0
1-4
0-
1
P
5
U
B
8
7
E
N
F
E
R
C
E
L
LIS
H
(b) Clusters details with t = 1
Fig. 8. Weighted characters position results
clusters which entails a small standard deviation for the classiﬁcation rate per
type on ﬁgure 8(b). So, increasing the accuracy has not to focus on single or few
types of messages only.
5.5 SVC Technique Results
We applied the SVC method with diﬀerent values for the Gaussian width q and
the penalty factor C. The weighted character position metric is used because it is
the best to diﬀerentiate the messages. As it is shown in the ﬁgure 9(a), the best
possible accuracy is 0.73 for the classiﬁed messages with all types of messages
found. This result is good but slightly lower than the nearest neighbors technique
on ﬁgure 8 (85% of good classiﬁcation). This is mainly due to a poor discovery of
the smallest clusters because the standard deviation of the speciﬁc classiﬁcation
rate per type is higher. When the Gaussian width q increases between 0.1 and 1,
the diﬀerence between the packets is emphasized in the high dimensional feature
space. Hence, the messages clusters are more split and the accuracy is improved.
However, when q is too high, the number of clusters continues to increase with
redundant types. The number of found clusters is then still good but due to
many redundant cluster types, the classiﬁcation rate drops.
The cluster composition is interesting. In this case, the best accuracy provides
clusters similar to the previous obtained in the ﬁgure 8 where all kinds of clusters
are represented with a little proportion of missclassiﬁed messages inside each
one. However, if we consider the case of C = 0.04 and q = 0.1 in ﬁgure 9(b)
with a lower accuracy, the clusters are totally diﬀerent, all types are not found
but for most of them, all messages are totally discovered. It means that these
clusters represented by black bars contain also messages of other types because
the latter ones are not classiﬁed in their own clusters -represented by stripes bar
(unclassiﬁed). We apply the nearest neighbors technique on each cluster to split
them. By looking for the best t value, we found t = 1.1 which allows to classify
91% of the messages and to discover 96% of the types of the message.
Automated Behavioral Fingerprinting
193
Unclassified
Classified
0.25
 0.2
0.15
 0.1
0.05
 0
1
0
1
0
1
0
0
0
1
1
0
1
0
0
1
1
1
1
1
1
0
1
0
1
1
0
1
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
2
O
1
R
4
I
A
N
1
4
B
5
I
M
1
4
5
4
4
2
R
6
C
4
4
P
4
0
P
0
E
0
N
C
O
8
0
Y
0
N
E
8
0
0
8
8
0
E
0
A
8
1
U
8
0
T
0
G
1
V
K
T
0
7
E
1
F
S
3
4
3
1
6
2
F
3
N
0
5
B
7
(a) Accuracy
I
I
I
I
O
S
E
C
L
O
S
T
F
N
T
E
Y
S
E
R
A
G
E
R
E
L
I
S
H
(b) Clusters details with q = 0.1
Fig. 9. Weighted characters position SVC results
Fig. 10. SVC - SMTP dataset
Fig. 11. Nearest neighbors clustering
5.6 Other Protocols
We have applied also our method to other protocols. We considered only the
weighted character position metric because it provided the best results in the
previous section. Two well known protocols were tested: SMTP [33] (150 packets
and 10 types) and IMAP [34] (289 packets and 24 types). To ease the compari-
son of results, a classical standardization of the data is done. When the nearest
neighbors technique is applied, the classiﬁcation rates are similar for these proto-
cols -as shown in ﬁgure 11 - and less than 50% of the messages are well identiﬁed.
The number of clusters found is better for SMTP. Moreover, using standardized
data helps to choose the parameter t in order to obtain the best classiﬁcation
rate. Then t = 20 seems to be a good value to apply the nearest neighbors