scribe almost all causal models we have encountered but
Fig. 10. Ambiguity Problem.
When the model is used to parse the trace, due to the lack
of dependencies between the two syscalls in the model, there
are many possible matchings as shown in Fig. 10-(d). Note
that except M1, the other matchings are incorrect even though
they all appear possible at the syscall level. In practice, such
incorrect matchings introduce false causalities which hinder
attack investigation. Moreover, ambiguity may cause excessive
performance overhead because MCI has to maintain numerous
model instances at runtime. The root cause of the problem
is that the trace does not have sufﬁcient information. Hence,
we develop a method that leverages explicit dependences to
mitigate the problem. Details can be found in Sec. IV-B.
IV. SYSTEM DESIGN
MCI consists of two phases: model construction and model
parsing. The former is ofﬂine and the latter is meant to be
deployed for production run.
A. Model Construction
Given an application, the forensic analyst provides a set
of regular workloads. The application is executed on the LDX
system with the workloads. The dependences detected by LDX,
including explicit and implicit dependences, are annotated on
the syscall events in the audit logs. The annotated logs are
analyzed to extract inter-dependent subsequences, which are
further symbolized (i.e., replacing concrete resource handlers
with symbolic ones). The sequences of symbolic syscalls with
dependences constitute our causal models.
In the following, we use a program snippet in Fig. 11 to
illustrate how MCI constructs causal models. It ﬁrst reads a
network message (line 1) and encrypts the received message
(line 2). Later, it stores the encrypted message to a local ﬁle
(line 3) and sends a notiﬁcation to a GUI component (line 5).
Fig. 11. Example Program.
1) Dependencies Identiﬁcation by LDX: The program is
executed with a typical workload on LDX [31] to collect a
system call log T . To identify dependencies, LDX mutates
the value of input syscall read() in the slave execution.
By contrasting the values of the following syscalls (e.g., the
write() and sendmsg()) in the two executions, LDX
identiﬁes all the dependencies between syscalls.
Fig. 12. Causally dependent system calls from LDX.
Fig. 12 shows the output generated by LDX. It includes
two read()s (lines 3 and 5), one write() (line 4) and
8
rαwβ..., r1, w2, w3, r4, r5, w6, w3, r7, w8, ...(a) Trace(c) Model(d) Possible Matchings M1: r1, w2, w3, r4, r5, w6, w3, r7, w8, ...M2: r1, w2, w3, r4, r5, w6, w3, r7, w8, ...M3: r1, w2, w3, r4, r5, w6, w3, r7, w8, ...M4: r1, w2, w3, r4, r5, w6, w3, r7, w8, ...M5: r1, w2, w3, r4, r5, w6, w3, r7, w8, ........., r1, w2, w3, r4, r5, w6, w3, r7, w8, ...(b) Ground-truthrσ: read(σ),wσ: write(σ)LegendExplicit Dep.  while( (len = read(socket, buf, 1024)) != -1 ) {     ebuf = encrypt(buf);     write( file, ebuf, 4096 );  }  sendmsg( wnd, “Update: ” + ebuf ... );12345123456SUCCESS = read( fd1 /* file handle*/, *, * );SUCCESS = write( fd2  /* file handle*/, *, * );FAILURE = read( fd1  /* file handle*/, *, * );SUCCESS = sendmsg( *, *, * );1234rαwβrαsγImplicit Dep.Explicit Dep.*rσ: read(σ), wσ: write(σ), sσ: sendmsg(σ),α: socket(fd1), β: file (fd2), γ: windowLegendcontext free parsers have a time complexity of n3 where n
is the length of a string (the number of events in audit log in
our case), thus they are too expensive to handle real-world
logs that can grow in the pace of gigabytes per day [33]
(corresponding to millions of events). Context-sensitive parsers
have even higher computational complexity. Furthermore, our
parser needs to be able to substantially mitigate the ambiguity
problem in which MCI does not know which models an event
should be attributed to.
Segmented Parsing. Our proposal is not to consider a trace
as a simple string, but rather a sequence of symbols with
explicit
inter-dependences. Note that explicit dependences
can be directly derived from the trace. The basic idea is
hence to leverage explicit dependences to partition the se-
quence of terms/nodes in a model into segments, delimited
by terms/nodes that are involved in some explicit dependences.
Therefore, all the terms/nodes inside each segment are a string
in some regular language. The essence is to leverage explicit
dependences to reduce language complexity. During parsing,
we ﬁrst recognize (from the trace) the explicit dependences
that match those of the model. These dependences partition
the trace into sub-traces. Then automata are used to recognize
model segment instances from the sub-traces. Since string
parsing is only carried out within small sub-traces instead
of the lengthy whole trace, ambiguity can be substantially
suppressed. We call the technique segmented parsing.
Fig. 15. Example for Segmented Parsing
Next, we use an example to illustrate the basic idea and
then explain the algorithm. Fig. 15 shows a sample model. Ob-
serve that there are explicit dependences between the 1st and
the 6th nodes ( rα and wα ), and between the 4th and the 8th
nodes ( sδ and oδ ). The sequence of terms/nodes involved in
explicit dependences form the model skeleton. In our example,
it is rα - sδ - wα - oδ . The skeleton partitions the model into
sub-models. A sub-model is a sub-sequence of nodes/terms
of the model that are delimited by explicit dependences but
themselves do not have any explicit dependences. In Fig. 15,
three sub-models are obtained as follows: sβ - rγ delimited by
rα and sδ , w delimited by sδ and wα , and rζ delimited
by wα and oδ .
During parsing, we ﬁrst ﬁnd instances of the model skele-
ton. For each skeleton instance, we try to identify instances of
sub-models within the trace ranges determined by the skeleton
instance. Any mismatch in any sub-model indicates this is not
a correct model instance and the corresponding data structures
are discarded. In our example, we ﬁrst locate the possible
positions of rα , sδ , wα , oδ in the trace, and then look for
the instances of sβ - rγ in between the positions of rα and
sδ , and so on. Partitioning a model to a skeleton and a set of
sub-models is straightforward. Details are hence elided. Given
a trace, to facilitate segmented parsing, we extract a number of
trace indexes, each containing all the nodes related to the same
system object (e.g., a ﬁle) and the position of the nodes in the
raw trace. These indexes allow our parser to quickly locate
skeleton instances in the trace. Fig. 16 shows an example of
index extraction from a trace. Observe that all the nodes in an
index have explicit dependences.
Fig. 16. Trace Preprocessing
Algorithm 1 Locating Skeletons
trace T , indexes I, model skeleton S
Input:
Output: a set of skeleton instances P, each consisting of a mapping that maps a
symbolic resource to a concrete one, and a sequence of positions
for all node Nα ∈ S do
if P ≡ {} then
P ← {(cid:104){α → h}, i(cid:105) | for all T [i] = Nh}
else
for all (cid:104)map, seq(cid:105) ∈ P do
Let the last position in seq be i
if map[α] (cid:54)= nil then
pos ← ﬁndbeyond(N,i,I[map[α]])
if pos (cid:54)= −1 then
seq ← seq · pos
else
P .remove((cid:104)map, seq(cid:105))
end if
1 procedure LOCATESKELETON(T , I, S)
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22 end procedure
else // scan all indexes to ﬁnd Nh syscalls that are beyond i
...
// and instantiate α to h.
end if
end for
end if
end for
return P
Algorithms. The parsing procedure consists of three major
steps. The ﬁrst one is to preprocess trace to extract indexes,
which has been intuitively explained before. The second step
is to locate skeleton instances in the trace and the third is to
parse sub-models. In the following, we explain the algorithmic
details of steps two and three.
The algorithm of locating skeleton instances is shown
in Alg. 1. It takes the trace T , the indexes I that can be
accessed by the concrete resource id (e.g., ﬁle handler), and
a model skeleton S, and identiﬁes all the possible instances
of the skeleton. The result is stored in P . Each instance is a
pair (cid:104)map, seq(cid:105) with map projecting each symbolic resource
(e.g., α and β) in the skeleton to some concrete handler and
seq storing the trace positions of the individual nodes in the
skeleton. To simplify our discussion, we assume the skeleton
does not have repetitive nodes or terms. The algorithm can be
easily extended to handle such cases.
The main procedure iterates over each node Nα in the
skeleton (line 2) with N the syscall and α the symbolic
resource. For the ﬁrst node (indicated by an empty result set
P ), the algorithm considers each syscall of the same type
N, in the form of Nh at location i in the trace, may start
an instance of the skeleton, and hence instantiates α to the
9
rαsβrγsδwεwαrζoδrσ: read(σ), sσ: stat(σ), wσ: write(σ), oσ: open(σ), α, β, γ, δ, ε, ζ: different filesLegendoαrβwαrβrαwβcα(a) Trace Annotated with Explicit Dependenciesoαwαrαcαrβrβwβ(b) Indexes for each resourceIndex 1Index 2oσ: open(σ), rσ: read(σ), wσ: write(σ), cσ: close(σ), α: File 1, β: File 2Legendconcrete handler h and records its position i (lines 3 and
4). If Nα is not the ﬁrst node, the algorithm iterates over
all the skeleton candidates in P in the inner loop (lines 6-
18) to check if it can ﬁnd a matching of the node for these
candidates. If not, the skeleton candidate is invalid and hence
discarded. Speciﬁcally, for each skeleton candidate denoted as
(cid:104)map, seq(cid:105), line 7 identiﬁes the trace position of latest node i.
This is needed as the algorithm looks for the match of Nα
in trace entries beyond position i. The condition at line 8
separates the processing to two cases with the true branch
denoting the case that α has been instantiated before, that is,
a node of the same symbolic resource was matched before
(e.g., wα in Fig. 15), the else branch otherwise (e.g., sδ in
Fig. 15). In the ﬁrst case (lines 9-11), the algorithm looks
up the index of the concrete handler associated with α, i.e.,
I[map[α]], to ﬁnd a concrete syscall N beyond position i (line
9). If such a syscall is found, we consider the algorithm has
found a match and the new position pos is appended to seq
(line 11). Otherwise, the skeleton candidate is not valid and
removed (line 13). Here, we have another simpliﬁcation for
ease of explanation. Line 9 may return multiple positions in
practice while in the algorithm we assume it only returns one.
The extension is straightforward.
In the else branch, the node has a new symbolic resource,
the algorithm has to go through all indexes to ﬁnd all instances
of N and instantiate the symbolic resource accordingly. This
may lead to the expansion of the candidate set P . Details are
elided. To reduce search space, we use time window and other
syscall arguments to limit scopes.
Algorithm 2 Model Parsing
trace T , skeleton instances P , sub-models S
the concrete syscall entries that correspond to the sub-models in the
1 procedure PARSESUBMODELS(T , P , S)
2
3
4
5
6
7
Input:
Output:
temporal order
for all (cid:104)map, seq(cid:105) ∈ P do
for i from 0 to |S| − 1 do
instance[i] ← parse(T [seq[i], seq[i + 1]], S[i])
end for
if all instance[0 − (|S| − 1)] are not nil then
if none of the concrete syscalls in instance[0(−|S| − 1)] share the same
output instance[0 − (|S| − 1)]
end if
end if
end for
resource id then
8
9
10
11
12 end procedure
Given a set of skeleton instances for a model M, Alg. 2
parses the sub-models of M. In particular, the outer loop (lines
2-11) iterates over all the skeleton candidates identiﬁed in the
previous step. If matches can be found for all sub-models
regarding a skeleton instance, the matches are emitted. Other-
wise, it is not a legitimate instance and discarded. Speciﬁcally,
the inner loop in lines 3 and 4 iterates over individual sub-
models in order. In the ith iteration, it uses automata to parse
sub-model S[i] in the trace range identiﬁed by the ith segment
identiﬁed by the skeleton candidate, which is from seq[i] to
seq[i + 1] (line 4). Automata based parsing is standard and
elided. After such parsing, line 6 checks if we have found
matches for all sub-models. If so, line 7 further checks that
none of the concrete syscall entries that are matched with
some node in a sub-model do not share the same resource (and
hence have explicit dependences). This is because the model
speciﬁes that there are not explicit dependences between the
10
corresponding nodes. Line 8 outputs the parsing results.
Handling Threaded Programs. Threading does not pose
additional challenges to MCI in most cases because syscalls
from different threads have different process ids so that models
can be constructed independently for separate threads. Ex-
plicit dependences across threads can be easily captured by
analyzing audit logs. Some programs such as Apache and
Firefox use in-memory data structures (e.g., work queues)
to communicate across threads, causing implicit dependences.
However, it is highly complex to model and parse behaviors
across threads due to non-deterministic thread interleavings.
We observe that these data structures are usually protected
by synchronizations, which are visible at the syscall level,
and the synchronizations should follow the nature of the
data structures, such as ﬁrst-in-ﬁrst-out for queues. Hence,
MCI constructs models for individual threads including the
dispatching thread and worker threads. The models include the
synchronization behaviors. It then leverages the FIFO pattern
to match nodes across threads. It works nicely for most of
the programs we consider except transmission, whose
synchronization is not visible at the system level (Sec. V).
V. EVALUATION
In this section, we evaluate MCI with a set of real-world
programs in order to answer the following research questions.
RQ 1. How many models are required to infer causality for
these programs in production runs (Sec. V-A1), and how much
efforts are required to construct models? (Sec. V-A2)
RQ 2. How effective is MCI for system wide causality in-
ference including multiple long-running programs and various
activities? (Sec. V-B)
RQ 3. How effective is MCI for realistic attack investigation?
(Sec. V-C)
RQ 4. Is MCI scalable on large workloads for long-running
programs? (Sec. V-C3)
Experiment Setup. We evaluate our approach on 17 real-
world programs. Table II shows the programs and models we
constructed. Note that 15 out of the 17 programs (except zip
and Vim) are network related which is a popular channel for
cyber-attacks. For each program, we construct models ofﬂine.
We use typical workloads brieﬂy described in the second
column of Table II. Speciﬁcally, if there are available test
inputs for a program, we use them as the typical workloads.
Otherwise, we construct inputs by inspecting program manuals
and identifying options and commands that can trigger differ-
ent functionalities, such as for proftpd, CUPS, and zip.
A. Model Construction
Table II shows the constructed models for each program.
Columns 1 and 2 show programs and model description.
Column Size shows the number of nodes in each model.
The numbers in/out parentheses are for the same behaviors
with/without HTTPS. The next two columns show the number
of explicit and implicit dependencies in each model. The last
column (Lang.) shows the language class of each model (Reg-
ular (Reg.), Context-free (C.F.), or Context-Sensitive (C.S.)).
TABLE II.
DETAILS ON MODEL CONSTRUCTION
Program