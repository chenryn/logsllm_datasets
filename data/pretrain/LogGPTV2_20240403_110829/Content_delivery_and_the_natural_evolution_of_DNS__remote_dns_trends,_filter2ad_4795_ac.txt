ratio maps, we extract two equal-length vectors—one for
each ratio map. Cosine similarity between the two vectors A
and B quantiﬁes the degree of overlap between the vectors
computed as their dot product divided by the product of
their lengths. Cosine similarity yields a value in the range
[0, 1]:
cos sim =
A · B
(cid:107)A(cid:107)(cid:107)B(cid:107)
DNS latencyHTTP latencyEnd-to-end latencySend DNSGet DNS answer;HTTP connectTransferRec'd !rst byteSend requestRec'd headersof objectquerycomplete527(a) Akamai
(b) Limelight
Figure 5: CDFs of cosine similarity for Akamai and Limelight CDN redirections by DNS lookup. Compared
to Iterative, ISP DNS has some similarity in at least 80% of locations. However, for 90% of locations there
is no similarity in Akamai redirections via public DNS.
When cos sim = 0, the sets of redirections have no
clusters in common. Values greater than 0 indicate that
some clusters are seen in both sets; cos sim = 1 means
that the sets of clusters seen are equivalent. We ensure
the signiﬁcance of this analysis by requiring that each
location’s ratio map comprises at least 3 redirections (see
§3). This enables us to capture load balancing behavior
when computing the similarity of CDN redirections.
We use cosine similarity to estimate diﬀerences in redi-
rections resulting from ISP and public DNS, to those
resulting from iterative resolution. Redirections seen via
ISP or public DNS are based on the location of the DNS
resolver—not the client. Since iterative resolution yields
CDN redirections based on client location, any diﬀerences
indicate potential
for redirections to suboptimal replica
server clusters. Figure 5 plots the cosine similarities seen in
Akamai and Limelight CDN redirections between iterative
and ISP or public DNS services.
The “Iterative-Iterative” curves show similarity between
random subsets of iterative redirections, and are the upper
bound on similarity for a CDN’s redirections.
If CDN
redirections were static, we would expect this value to
always equal 1; however, they are dynamic and responsive
to several factors including system load and network condi-
tions. Cosine similarity values <1 reﬂect these variations
in redirections. We ﬁnd that, for the “upper bound” of
similarity, we see higher similarity for CDNs with fewer
data centers (e.g. Limelight) as there will likely be fewer
variations in redirections.
Iterative and ISP redirections have the highest similarity,
with 80% of locations having some Akamai replica server
clusters in common. In contrast, the “Iterative-Google” and
“Iterative-OpenDNS” curves reveal very low similarity—in
90% of locations, there is no similarity in the set of Akamai
redirections.
These trends are the same for Limelight redirections, ex-
cept that public DNS is able to match iterative’s redirections
in a larger fraction (43%) of locations. While ISP and public
DNS resolvers both result in diﬀerent redirections compared
to iterative lookups, the eﬀect is signiﬁcantly larger for
public DNS services.
Figure 6: CDF of % diﬀerence in network latency to
public vs. ISP DNS resolvers. Public DNS servers
are farther away in 90% of locations and more than
twice as far in 50%.
One possible explanation for the diﬀerences in cosine
similarity seen with public DNS vs ISP DNS is their distance
from the client. Since CDNs’ replica server mappings are
indicative of the location of the recursive DNS resolver,
increased distance to a resolver is expected to correlate with
lower similarity in redirections. To test this hypothesis,
Fig. 6 plots the percent diﬀerence in latency to public DNS
resolvers relative to ISP DNS latency. For half of our
locations, public DNS servers are at least twice as far away
as ISP DNS servers. This explains the lower similarity we
ﬁnd in redirections via public DNS.
Our cosine similarity analysis serves as a useful indicator
for the potential
impact of using a given DNS service.
Having a cosine similarity value <1 is a necessary but not
suﬃcient condition for a DNS service to aﬀect performance.
For instance, ISP DNS may provide diﬀerent redirections
compared to Iterative, but both of the CDN mappings could
provide equivalent performance.
In the next section, we
directly compare the quality of HTTP performance using
CDN redirections obtained via diﬀerent DNS services.
0.00.20.40.60.81.0CosineSimilarity0.00.20.40.60.81.0CDFoflocationsIterative-IterativeIterative-ISPIterative-GoogleIterative-OpenDNS0.00.20.40.60.81.0CosineSimilarity0.00.20.40.60.81.0CDFoflocationsIterative-IterativeIterative-ISPIterative-GoogleIterative-OpenDNS02004006008001000Networklatency%difference,PublicvsISPDNS0.00.20.40.60.81.0CDFoflocationsGoogleDNSOpenDNS528(a) Akamai
(b) Limelight
Figure 7: CDF of % diﬀerence in HTTP latency using iterative, ISP or public DNS relative to our ideal
baseline, for Akamai and Limelight. Replica servers seen via ISP DNS and iterative lookups generally
provide the best-case HTTP latency. Public DNS services often yield redirections with higher latencies, with
a 2× increase for 35% (Akamai) and 18% (Limelight) of locations.
4.2 HTTP performance
To understand the impact of the diﬀerences in CDN redi-
rections between DNS approaches, we analyze the resulting
HTTP performance obtained through each. As our metric,
we use the latency to establish a connection to a CDN replica
server and start downloading an object. This latency is an
important measure of CDN performance. When loading web
pages, low latencies are essential to attaining low page load
times. In the case of high-bandwidth streaming (e.g. HD
video), lower latency connections allow for higher transfer
rates.
We evaluate the quality of CDN redirections via iterative,
ISP and public DNS resolutions relative to our ideal baseline
HTTP latency.
Figure 7 plots the percent diﬀerence
in HTTP latency—the time to receive the ﬁrst byte of
an object—of these DNS resolution approaches. Both
iterative and ISP DNS redirections match the best-case
HTTP performance in 70% (Akamai) and 80% (Limelight)
of locations.
In contrast, public DNS’s redirections only
achieve this for 15% (Akamai) and 40% (Limelight) of
locations. Since Limelight’s CDN architecture has relatively
fewer data centers, the increased distance to the public
DNS server is less likely to aﬀect the client’s replica server
mapping. Still, public DNS results in at least double the
HTTP latency for 35% (Akamai) and 18% (Limelight) of
locations.
For some sampled locations, we ﬁnd that iterative res-
olution does not always result in optimal performance.
In these cases, several factors beyond client location may
aﬀect the redirections obtained from the CDN, including
load balancing between server clusters [29].
In practice,
this source of noise in CDN redirections is negligible in
comparison to the overall results that we report.
4.2.1 Cause of performance differences
To better understand the components of HTTP latency,
we examine two possible explanations: load and proximity of
edge servers. These analyses will determine the reason for
Figure 8: CDF of the fraction of the HTTP
interaction spent waiting for a response, comparing
redirections via several DNS services. Under load,
servers queue requests,
increasing this parameter
from the expected value of 0.5. The distributions are
nearly identical; there are no systematic diﬀerences
in server load.
the diﬀerences in HTTP performance we observe between
DNS services.
We test whether server load (and therefore response time)
diﬀer signiﬁcantly between the edge servers seen via each
DNS service. Since keeping edge server load low is one of the
main goals of CDNs in general, we expect that our requests
for cached objects should be served with minimal delay. We
validate this hypothesis in Fig. 8, which plots CDFs of the
fraction of the HTTP latency spent waiting to receive the
response header and ﬁrst byte of the requested object, for
both lookups via public and ISP DNS. The intuition is that
an HTTP request requires 2 round-trips to the server: the
ﬁrst to establish a connection, and the second to request the
content. If the server is not heavily loaded, then the request
will not be queued, and a request for a cached object can
be returned immediately—in this case, the fraction of time
050100150200250300HTTPlatency%difference0.00.20.40.60.81.0CDFoflocationsIterativeISPDNSGoogleDNSOpenDNS050100150200250300HTTPlatency%difference0.00.20.40.60.81.0CDFoflocationsIterativeISPDNSGoogleDNSOpenDNS0.00.20.40.60.81.0FractionofHTTPtimewaitingforheaderandstartofobject0.00.20.40.60.81.0CDFoflocationsIterativeISPDNSGoogleDNSOpenDNS529Figure 9: Heat map of HTTP and ping latency to
replica servers aggregated across our experiments;
darker bins indicate higher density of samples. The
dashed black line shows the expected value, where
HTTP latency is 2× ping latency. The variables are
strongly correlated (r = 0.88). Network latency is
the dominant factor in HTTP performance.
spent on the second round-trip should be the same as the
ﬁrst, or 50% of the total request time. We ﬁnd that in 70% of
cases, this metric is within 10% of the expected value, and
ﬁnd no signiﬁcant diﬀerences between these distributions.
This indicates that server load is not systematically diﬀerent
for CDN replica servers seen via any DNS service.
Next, we evaluate network latency’s relationship to overall
In an HTTP interaction, requesting and
HTTP latency.
receiving an object requires 2 round trips:
the ﬁrst to
establish the connection, and the second to request and
receive the object. In Fig. 9 we plot a heat map showing
the relationship between HTTP and network latency, with
the black line showing the expected relationship in which
HTTP latency is twice the network latency. We ﬁnd a strong
correlation between these two variables (r = 0.88, n =
629252), indicating that network latency has a signiﬁcant
impact on overall HTTP latency.
4.3 Better performance using client location
The previous analysis shows that using remote DNS
services results in signiﬁcantly diﬀerent CDN redirections,
often leading to reduced HTTP performance. The “edns-
client-subnet” DNS extension [3] provides a way to expose
client location information, part of the client’s IP address,
to CDNs as an approach to address this.
In the following paragraphs we present the ﬁrst study on
the eﬃcacy of the proposed extension. For our evaluation,
we follow the methodology for obtaining CDN redirections
using the extension in §3.1. We close the section with a
survey of the current extent of its adoption.
4.3.1 Performance improvement with client location
The ECS extension functions by passing part of the client’s
network address to the CDN’s authoritative DNS server to
aid in selecting an appropriate replica server. Choosing
the amount of client information to provide requires sev-
eral considerations. Exposing ﬁne-grained client location
information could enable the CDN to provide more accurate
redirections, but might also result in the recursive resolver
Figure 10: CDFs of HTTP latency relative to
our ideal baseline,
for varying amounts of client
information included in an ECS-enabled lookup.
Providing a client’s /24 preﬁx provides nearly the
same performance as giving the whole IP address
for Google CDN. When only providing the /16
preﬁx of the client’s address, HTTP latency is higher
than when giving the /24 preﬁx for about 30% of
locations.
having to cache additional ECS-speciﬁc mappings—one for
each client preﬁx—for every CDN domain name.
We evaluate the impact of providing diﬀerent amounts
of client location information via the extension on HTTP
performance. For this experiment, we query Google DNS
servers for a Google CDN name. Figure 10 plots the %
diﬀerence in HTTP latency of redirections of ECS queries
with either the client’s /16 preﬁx, /24 preﬁx, or the client’s
whole IP address. For 60% of locations, giving the /16 and
/24 preﬁxes are suﬃcient to obtain equivalent performance
to giving the whole IP address. In the remaining locations,
the /24 preﬁx provides slightly better mappings than the /16
preﬁx. For instance, giving the /16 preﬁx, 9% of locations
have at least 50% increase in HTTP latency compared to
the whole IP address—giving the /24 preﬁx, only 3% have a
50% increase. We also conducted this analysis for lookups to
an EdgeCast CDN name, and found no diﬀerence between
providing the /16 preﬁx, /24 preﬁx or the whole IP address.
For these CDNs, we ﬁnd that a /16 preﬁx is typically
suﬃcient to provide equivalent mappings to the whole IP
address. For a subset of locations, using /24 preﬁxes can
result in marginal performance improvements compared to
/16 preﬁxes.
We stress that these results are not generally applicable
to CDNs due to variations in their deployments of edge
servers. We would expect to see similar results for CDNs
with like deployments, since users would be mapped to
edge server locations with equivalent granularity in terms
of their network location. For CDN architectures with edge
servers in more locations (e.g.
in end-users’ networks), we
expect that providing more preﬁx information would result
in a greater performance improvement than we observed.
However, lacking a CDN with such a deployment that also
supports the extension, we cannot experiment directly with
the impact of varying amounts of client location information
on the performance of this CDN deployment model.
101001000HTTPlatency(ms)10100500Pinglatency(ms)020406080100HTTPlatency%difference0.00.20.40.60.81.0CDFoflocationsECSWholeIPECS/16PreﬁxECS/24Preﬁx530(a) Akamai
(b) Limelight
Figure 11: CDFs of end-to-end latency % diﬀerence, comparing ISP DNS and public DNS with and without
the extension, for requests to Akamai and Limelight CDNs. Values are relative to the ideal end-to-end latency
baseline. Using the extension signiﬁcantly improves end-to-end performance. For Akamai, the extension
improves median performance by 40%; for the top 20% of locations, it reduces latency by 60%.
To understand the impact of the extension in the context
of end-to-end performance, we compare the performance of
public DNS with and without ECS to ISP DNS. Figure 11
plots relative end-to-end latency for accessing objects on the
Akamai and Limelight CDNs. Since neither of these CDNs
currently support the ECS extension, we simulate it by using
answers provided by iterative DNS lookups.
Using ECS provides a signiﬁcant performance improvement
over public DNS resolution without ECS. For instance, the
median latency diﬀerence for Akamai lookups without ECS
was 61%; with ECS, this was only 33%. The beneﬁts of using
the extension are most apparent in the upper tail of the