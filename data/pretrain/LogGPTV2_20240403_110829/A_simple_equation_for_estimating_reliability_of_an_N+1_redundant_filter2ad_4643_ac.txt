### 5. Approximations and Assumptions
In this section, I will discuss the approximations and simplifying assumptions used in the DDF(t) equation, along with their impact on accuracy. This will help users understand the limitations of the equation.

#### 5.1. Availability Terms and Settling
The steady-state availability terms, \( A_{\text{Op}} \) and \( A_{\text{Ld}} \), are calculated using mean values from component distributions. The equations for determining the mean value from a Weibull distribution are provided, but calculating the gamma functions can be tedious unless \(\beta = 0.5\). The value of \(\beta\) is shown in Table 2.

The length of time required for the system (RAID group) availability to settle and reach its asymptotic, steady-state value is unknown. If there are large fluctuations throughout the mission, the estimated DDF(t) will also have errors. Since the characteristic life of an operational failure is generally much longer than the mission of interest, settling time is not a significant issue. The impact of this error would be greater at the beginning of the mission than at the end.

#### 5.2. Time-to-First-Failure: No System Repairs
The DDF(t) equation assumes that components are restorable, but the system is not. Thus, there are terms for restoring a failed HDD that has experienced an operational failure and scrubbing an HDD with a latent defect, but no term for the restoration of the system. Since the system failure always requires one operational failure, it is this second operational failure term, expressed as the cumulative hazard rate, that has no associated restoration distribution. This assumption is justified based on the observation that very few RAID groups in the MC-Sim had more than one system failure during the mission. Therefore, the system model can be treated as a time-to-first-failure problem, and restoration can be excluded. A sensitivity study in Section 6.3 confirms the validity of this assumption by comparing MC-Sim results for two different restoration distributions.

#### 5.3. System Reliability Based on Component Properties
All equations assume that the system ROCOF (Rate of Occurrence of Failures) can be determined from component failure and restoration rates. Ascher [12] points out that there is little connection between the properties of component hazard rates and the properties of the process that produces a sequence of failures. Even if the HDDs have constant failure rates, there is no statistical basis for assuming that the system will have a constant failure rate [14]. Despite this, the DDF(t) results remain highly accurate because all HDD failure combinations resulting in a DDF require an operational failure after one HDD has already experienced data loss through either an operational failure or a latent defect. Per the previous discussion, restoration of the second operational failure can be excluded from the analysis, so the number of DDFs over time is heavily influenced by the cumulative hazard rate for an HDD, a component property.

### 6. Results and Comparisons
In this section, I compare the results of the DDF(t) equation with those from the MTTDL and Baker equations, the MC-Sim results, and NetApp field data. Twelve parameters for the four Weibull distributions are shown in Table 3. The parameters for the time-to-operational failure distribution (TTOp) and the time-to-latent defect (TTLd) are based on NetApp field data for HDDs. Parameters for the time-to-restore an operational failure (TTR) and time-to-scrub (TTScrub) are unique to each data system manufacturer. The values shown in Table 3 are the same as presented in [4] and represent current industry experience. Since the Baker equation only accommodates mirrored disks, one set of comparisons will use N+1 = 2. A second set that allows us to compare to MC-Sim and the NetApp field data uses N+1 = 14.

#### 6.1. Results for Mirrored Disks
This section presents two analyses comparing the MTTDL, Baker, and DDF(t) equations to the MC-Sim results for a single pair of mirrored disks. The intent of these comparisons is to examine the inherent qualities of the equations with as few data differences as possible. The MTTDL equation does not include latent defects, but the Baker and DDF(t) equations do. All distributions are assumed to be exponential with MTTF = \(\eta\) (the characteristic life parameter), while location parameters, \(\gamma\), are ignored. The results, shown in Figure 3, indicate that the DDF(t) equation and the Monte Carlo simulation track each other well. The Baker equation is lower but still an improvement over the MTTDL, which appears to be the horizontal axis.

**Figure 3. Comparison of mirrored disk reliability**

A second comparison uses all the distribution information the equation permits, including non-constant failure and restoration rates and minimum times expressed through the location parameter, \(\gamma\). Figure 4 shows that, again, the Monte Carlo simulation and the DDF(t) equation track each other well, even though three of the distributions do not have constant failure rates and two include a "failure waiting period" modeled by the parameter \(\gamma\). This illustrates the ability of the new equation to accurately represent the detailed simulation.

**Figure 4. Comparisons for mirrored disks with non-constant rates**

#### 6.2. Results for (N+1) = 14
Many RAID groups are larger than two HDDs, so this analysis examines the accuracy of the equation for a RAID group size of 14. Again, the data from Table 3 is used in these comparisons. Since Baker's equation is not designed to account for more than two disks in a RAID group, only the MTTDL, DDF(t), and the MC-Sim are compared. The results, shown in Figure 5, indicate that the DDF(t) equation tracks the Monte Carlo simulation very well, while the MTTDL calculation is far off, appearing to be the plot's axis.

**Figure 5. Comparisons for RAID group of 14 disks with non-constant rates and delay times**

The last, but perhaps most significant comparison, is the DDF(t) equation against NetApp field data (Figure 6). The NetApp field data is from 4,600 N+1 RAID groups with 14 disks. The data was captured during a timeframe consistent with the component distributions used in this research. The data was collected for N+2 RAID systems, so no data was lost. The NetApp reporting system documents the time at which data would have been lost in an N+1 configuration if the N+2 configuration had not been in use.

**Figure 6. Comparison of NetApp field data for RAID groups of size N+1 = 14, to the new equation**

Since the quantity of systems decreases with age (there are more young systems than old systems), the data dwindles just before the 2-year age (17520 hours). The DDF(t) equation tracks the field data very well for the inputs used and clearly shows that it can track field data, whereas the MTTDL and Baker equation do not.

#### 6.3. Sensitivity Analyses
I performed nine sensitivity studies to support the validity of this equation. The first study looks at the effect the time to restore distribution has on DDF(t). The concept of omitting the restoration for the second operational failure has been justified from a statistical perspective in Section 5.4. Figure 7 shows the results from two simulations, in which the characteristic life of the time to restore distribution changed from 4 to 168 hours. The plot shows that the effect on the TTR is not significant.

**Figure 7. Comparison of DDF simulation results for time to restore distributions with \(\eta = 4\) and \(\eta = 168\) hours.**

The next eight studies explore the effects of changing the characteristic life parameter, \(\eta\), for each input distribution to see how well the DDF(t) equation matches the MC-Sim for different distributions. Table 4 shows the differences between the equation and the Monte Carlo simulations when each parameter is increased or decreased. The factors were chosen based on what might be reasonable variations for the characteristic life values, so the factors are not all the same. For example, if the time-to-latent defect increases by a factor of 3 due to new technology inside the HDD, the equation differs from MC-Sim by only 3.6%. Differences are composed of both inaccuracy in the equation and run-to-run variation in the simulations. The low percent difference in Table 4 shows the robustness of the DDF(t) equation to match the more comprehensive MC-Sim method for a wide variation of input distributions.

**Table 4. Differences between equation and MC-Sim**

| Distribution | Factor | Difference at 10 years |
|--------------|--------|-----------------------|
| TTOp         | x2     | 92277                 |
| TTOp         | ÷2     | 30.3%                 |
| TTR          | x3     | 364.2%                |
| TTR          | ÷3     | 43.6%                 |
| TTLd         | x3     | 27777                 |
| TTLd         | ÷3     | 3086                  |
| TTScrub      | x4     | 672                   |
| TTScrub      | ÷4     | 423.6%                |

### 7. Future Work
The next steps in this research include:
1. Creating a Monte Carlo simulation for RAID-DP [23], which has an N+2 architecture.
2. Reviewing and reassessing input distributions for the time-to-latent defect and time-to-scrub.
3. Creating an equation for an N+2 RAID.
4. Expanding the simulation model to include other system hardware.

### 8. Conclusions
This research has developed DDF(t), an equation that is simple and can be used by researchers to evaluate the impact of their work on RAID reliability. The assumptions and approximations employed in creating the equation are statistically justified and produce results consistent with Monte Carlo simulation and match the NetApp field data for nearly 2 years of age. The DDF(t) equation expresses the reliability of N+1 RAID groups in terms of the number of data losses as a function of time. This means that reliability estimates will be accurate even when the components have increasing or decreasing failure rates. Correlations in the form of multiplying factors are not required for the equation. Temporal correlations that result from components having non-constant failure rates are implicit and included without any added factors.

### 9. References
[1] D. A. Patterson, G. A. Gibson, R. H. Katz, “A Case for Redundant Arrays of Inexpensive Disks (RAID),” Proc., ACM Conference on Management of Data (SIGMOD), Chicago, IL, June 1988.
[2] J. G. Elerath and M. Pecht, "A Highly Accurate Method for Assessing Reliability of Redundant Arrays of Inexpensive Disks (RAID)," Trans. on Computers, IEEE, March, 2009.
[3] M. Baker, M. Roussopoulos, M. Shah, P. Maniatis, P. Bungale, D. S. H. Rosenthal, T. J. Giuli, "A Fresh Look at the Reliability of Long-term Digital Storage," EuroSys’06, April 18–21, 2006, pp. 221-234.
[4] J. G. Elerath, "Reliability Model and Assessment of Redundant Arrays of Inexpensive Disks (RAID) Incorporating Latent Defects and Non-Homogeneous Poisson Process Events," Ph.D. Dissertation, A. James Clark College of Engineering, Mechanical Engineering Department, University of Maryland, 2007. https://drum.umd.edu/dspace/handle/1903/6733.
[5] V. Prabhakaran, “IRON File Systems,” SOSP ’05, Oct. 2005, Brighton, UK.
[6] D. A. Patterson, G. A. Gibson, R. H. Katz, “A Case for Redundant Arrays of Inexpensive Disks (RAID),” Proc., ACM Conference on Management of Data (SIGMOD), Chicago, IL, June 1988.
[7] G. A. Gibson, “Redundant Disk Arrays: Reliable, Parallel Secondary Storage,” Ph. D. Dissertation, Dept of Computer Science, UC Berkeley, April 1991. T7.6 1991 G52 ENGI.
[8] D. A. Patterson et al., “Introduction to Redundant Arrays of Inexpensive Disks (RAID),” Thirty-Fourth IEEE Computer Society International Conference: Intellectual Leverage, COMPCON, Feb. 1989.
[9] P. M. Chen et al., “RAID: High-Performance, Reliable Secondary Storage,” ACM Computing Surveys, 1994.
[10] W. V. Courtright, II, “A Transactional Approach to Redundant Disk Array Implementation,” Ph.D. Thesis, CMU-CS-97-141, School of Computer Science, Carnegie Mellon University, May 1997.
[11] T. J. E. Schwarz, W. A. Burkhard, “Reliability and Performance of RAIDs,” IEEE Transactions on Magnetics, vol. 31, no. 2, Mar. 1995.
[12] H. Ascher, “[Statistical Methods in Reliability]: Discussion,” Technometrics, vol. 25, no. 4, p320-326, Nov. 1983.
[13] S. Shah and J.G. Elerath, "Disk Drive Vintage and Its Affect on Reliability," Proc. Annual Reliability & Maintainability Symp., Jan. 2004.
[14] H. E. Ascher, “A Set-of-Numbers is NOT a Data-Set,” IEEE Trans. on Reliability, vol. 48, no. 2, p135-140, June 1999.
[15] W. A. Thompson, “On the Foundations of Reliability,” Technometrics, vol. 23, no. 1, pp. 1-13, Feb. 1981.
[16] W. A. Thompson, "The Rate of Failure Is the Density, Not the Failure Rate," The American Statistician, Editorial, vol. 42, no. 4, p228-291, Nov. 1988.
[17] H. H. Kari, “Latent Sector Faults and Reliability of Disk Arrays,” Ph.D. Dissertation, TKO-A33, Helsinki University of Technology, Espoo, Finland, 1997, http://www.cs.hut.fi/~hhk/phd/phd.html.
[18] R. Geist and K. Trivedi, “An Analytic Treatment of the Reliability and Performance of Mirrored Disk Subsystems,” Twenty-Third Inter. Symp. on Fault-Tolerant Computing, FTCS, June 1993.
[19] T. J. E. Schwarz et al., “Disk Scrubbing in Large Archival Storage Systems,” IEEE Computer Society Symposium, MASCOTS, 2004.
[20] Trivedi, http://amod.ee.duke.edu/, last accessed Dec. 14, 2008.
[21] Reliasoft Corp., www.reliasoft.com/products, last accessed 3/13/2009.
[22] www.efunda.com/math/gamma/findgamma.cfm, last accessed 3/13/09.
[23] P. Corbett et al., “Row Diagonal Parity for Double Disk Failure Correction,” Proc. of 3rd USENIX Conference on File and Storage Technology, San Francisco, 2004.