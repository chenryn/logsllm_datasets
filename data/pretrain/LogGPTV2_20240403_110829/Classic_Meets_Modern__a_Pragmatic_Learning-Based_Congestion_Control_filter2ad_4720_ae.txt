5.4 Discussion of Efﬁcient Value Estimation in QTCP-
Generalization
We now analyze the efﬁciency of value approximation for the
proposed generalization-based Kanerva coding method in QTCP-
Generalization. In our experiments, we found that even when
employing a relatively small set of prototypes, (300 prototypes,
which is 0.0269% of the possible state space), generalization-
based Kanerva coding can provide effective state abstraction, and
improvements in learning performance have been observed in all
experimental evaluations. We argue that it is safe to use a small
set of prototypes with the generalization-based Kanerva coding
approach since the key component that affects the performance
of function approximation is not the large number of prototypes
but the reasonable layout of those prototypes. In fact, as long as
the number of prototypes are not too few, the learning results
with varying numbers of prototypes show no statistically signif-
icant difference [16]. Therefore, the superior performance of our
proposed generalization-based Kanerva coding, as demonstrated
by our experimental results, is mainly a result of the ﬁne-grained
levels of generalization for all prototypes that are dynamically
adjusted based on visited state samples by our RL learner.
6 RELATED WORK
6.1 Congestion Control Protocols
TCP is a well explored topic in both wired and wireless network-
ing. For years, many end-to-end congestion control mechanisms
have been proposed. For example, Cubic uses a cubic function to
tweak the cwnd, and is known for its ability to aggressively search
for spare bandwidth. Vegas [17] uses delay as a congestion control
indication and starts to decrease cwnd when the measured RTT
exceeds expected value. Other well known end-to-end congestion
control protocols include Compound [18], Fast [19] and BBR [20].
While these protocols all have their own unique properties, they
share the similar idea of using some ﬁxed functions or rules to
change cwnd to handle network conditions. As we introduced,
the limitation of this ﬁxed-rule strategy is that they cannot adapt
to the complexity and rapid evolution of modern data networks.
They do not learn from experience or history and are not able to
predict the consequences of each action taken. Even if an action
reduces performance, the algorithm would still mechanically and
repeatedly select the same action.
Meanwhile, we notice that a number of techniques have been
explored by the research community to solve the limitations of
traditional TCP protocols. For example, Remy [12] uses off-
line training to ﬁnd the optimal mapping from every possible
network condition to the behavior of the sender. Remy works
well when prior assumptions about the network given at design
time are consistent with the network situations in experiments.
Performance may degrade when real networks violate the prior
assumption [21]. The mappings stored in the lookup table are pre-
calculated, which, as with other traditional TCP variants, cannot
adapt to continuously varying network environment. In Remy’s
approach, the lookup table must be recomputed (which may take
days to train the model) when new network conditions apply.
PCC [22] is a recently proposed protocol that can rapidly adapt
to changing conditions in the network. PCC works by aggressively
searching for better actions to change the sending rate. However,
its performance may diminish in some cases since its greedy
exploration could be trapped at a local optimum, requiring certain
strategy to approach to the globally optimal solution. Both Remy
and PCC regard the network as a black box and focus on looking
for the change in the sending rate that can lead to the best
performance, without directly interpreting the environment or
making use of previous experience.
6.2 Reinforcement Learning and its Applications
RL has achieved a lot of success in solving sequential decision
problems and has been effectively applied to a variety of applica-
tions. The advantage of RL is its ability to learn to interact with
the surrounding environment based on its own experience. For
example,
[23] proposed to learn channel assignment decisions
with a linear bandit model to minimize the total switching cost in
a multichannel wireless network.
[24] used deep reinforcement
learning (DRL) to handle the large complex state space when
solving the cloud resource allocation and power management
problem.
[25] proposed a DRL-based framework for power-
efﬁcient resource allocation in cloud RANs.
Moreover, many reinforcement learning-based schemes have
been proposed to improve the quality of service for network
applications. For example, [26] proposed a RL-based algorithm to
generate congestion control rules to optimize the QoE speciﬁcally
for multimedia application.
[27] formulated a network resource
allocation problem in a multi-user video streaming domain as a
DEC-POMDP model and applied a distributed RL algorithm to
solve the problem. However, the work in [27] did not provide
practical technique to help RL algorithm adapt to complex net-
work topologies that have continuous state spaces. [28] used a RL
algorithm to adaptively change parameter conﬁguration and thus
improve the QoE of video streaming. Its limitation arises from its
use of a tabular-based algorithm that directly stores and updates
value functions as entries in a table, conﬁning its application to
large, continuous domains.
All above mentioned schemes are task-driven. They are de-
signed for particular applications and cannot be directly applied
to congestion control problem. In fact, to the best of our knowl-
edge, QTCP is the ﬁrst proposed solution applying RL to TCP
congestion control protocol design directly.
6.3 Related Function Approximation Techniques
Many approximating approaches has been developed to abstract
and compress full state spaces in RL tasks that have enormous
number of states. One effective approach is function approxima-
tion [6], which can reduce the size of the state space by represent-
ing it with an abstracted and parameterized function. The explicit
table that stores value functions is replaced by an approximate
and compact parameterized version. Many function approximation
12
techniques have been developed, including tile coding (also known
as CMAC) and its variants, i.e., adaptive tile coding [29], and tree-
based state partitions [30]. However, when solving practical real-
world problems, limitations arise from the coding schemes used
in those approaches, e.g., requiring task-dependent criteria-based
heuristics, spending impractical large computation expenses to ex-
plore each dimension for partitions, and the size of state space and
function approximation complexity increases exponentially with
the number of dimensions. All the limitations prevent considered
approaches’ applications to domains that are very large, have high
dimensions, or that have continuous state spaces.
However, Kanerva coding technique proves to scale well with
high-dimensional, continuous problem domains [7]. Our proposed
generalization-based Kanerva coding approach further improves
its approximation ability and reduces the convergence time by dy-
namically optimizing the layout of the prototype set and providing
a ﬁner-grained discrimination on the explored state areas.
7 CONCLUSION
Our work describes QTCP, an effective RL-based approach that
derives high-quality decision policies and successfully handles
highly complex network domains with a broad set of character-
istics. Unlike preprogrammed rule-based TCP, QTCP uses the
reinforcement signals (rewards) to learn the congestion control
rules from experience, needing no prior knowledge or model of
the network dynamics. This allows our approach to be widely
applicable to various network settings. Moreover, our learning
agent applies a novel generalization-based Kanerva coding ap-
proach to reduce the training time and necessary state space to
search. This approach reformulates original function approximator
with adjustable generalization granularity across states, making it
possible to abstract sufﬁcient information from a wide range of
environment signals and even use a very small subset of state
space to accurately approximate the whole state space.
Our QTCP-Generalization achieved better throughput and
delay performance than both NewReno and QTCP-Baseline (a
learning-based TCP with best currently-existing Kaneva-based
function approximator) in our evaluations. We found that the
average throughput of QTCP-Generalization outperformed QTCP-
Baseline by 35.2% and outperformed NewReno by 59.5%.
Our approach also has a slightly better RTT performance than
NewReno. We conclude that QTCP with generalization-based
Kanerva coding can be used to manage congestion in a wide range
of network conditions, and that the technique enables quick on-
line policy development with minimal computation and memory
expenses.
REFERENCES
[1] T. Yilmaz and O. B. Akan, “State-of-the-art and research challenges for
consumer wireless communications at 60 ghz,” IEEE Transactions on
Consumer Electronics, vol. 62, no. 3, pp. 216–225, 2016.
[2] O. B. Akan, O. B. Karli, and O. Ergul, “Cognitive radio sensor networks,”
IEEE network, vol. 23, no. 4, 2009.
[3] Y. Li, Y. Li, B. Cao, M. Daneshmand, and W. Zhang, “Cooperative
spectrum sharing with energy-save in cognitive radio networks,” in
Global Communications Conference (GLOBECOM), 2015 IEEE.
IEEE,
2015, pp. 1–6.
[4] E. S. Hosseini, V. Esmaeelzadeh, R. Berangi, and O. B. Akan, “A
correlation-based and spectrum-aware admission control mechanism for
multimedia streaming in cognitive radio sensor networks,” International
Journal of Communication Systems, vol. 30, no. 3, 2017.
[5] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prab-
hakar, S. Sengupta, and M. Sridharan, “Data center tcp (dctcp),” in ACM
SIGCOMM computer communication review, vol. 40, no. 4.
ACM,
2010, pp. 63–74.
[6] L. Frommberger, Qualitative Spatial Abstraction in Reinforcement
Learning. Springer Science & Business Media, 2010.
[7] R. Sutton and A. Barto, Reinforcement Learning: An Introduction.
Bradford Books, 1998.
[8] S. Floyd, A. Gurtov, and T. Henderson, “The newreno modiﬁcation to
tcp’s fast recovery algorithm,” 2004.
[9] D.-M. Chiu and R. Jain, “Analysis of the increase and decrease al-
gorithms for congestion avoidance in computer networks,” Computer
Networks and ISDN systems, vol. 17, no. 1, pp. 1–14, 1989.
[10] B. D. F. Zhou, M. DiFelice and K. R. Chowdhury, “Towards fast ﬂow
convergence in cognitive radio cellular networks,” IEEE Globecom.
IEEE, 2017.
[11] K. R. Chowdhury, M. Di Felice, and I. F. Akyildiz, “Tcp crahn: A
transport control protocol for cognitive radio ad hoc networks,” Mobile
Computing, IEEE Transactions on, vol. 12, no. 4, pp. 790–803, 2013.
[12] K. Winstein and H. Balakrishnan, “Tcp ex machina: Computer-generated
congestion control,” in ACM SIGCOMM Computer Communication
Review, vol. 43, no. 4. ACM, 2013, pp. 123–134.
[13] C. Wu and W. Meleis, “Function approximation using tile and kanerva
coding for multi-agent systems,” in Proc. Of Adaptive Learning Agents
Workshop (ALA) in AAMAS, 2009.
[14] P. W. Keller, S. Mannor, and D. Precup, “Automatic basis function
construction for approximate dynamic programming and reinforcement
learning,” in Proc. of Intl. Conf. on Machine Learning, 2006.
[15] C. Wu and W. M. Meleis, “Adaptive kanerva-based function approxi-
mation for multi-agent systems,” in Proceedings of the 7th international
joint conference on Autonomous agents and multiagent systems-Volume
3.
International Foundation for Autonomous Agents and Multiagent
Systems, 2008, pp. 1361–1364.
[16] M. Allen and P. Fritzsche, “Reinforcement learning with adaptive kanerva
coding for xpilot game ai,” in 2011 IEEE Congress of Evolutionary
Computation (CEC).
IEEE, 2011, pp. 1521–1528.
[17] L. S. Brakmo and L. L. Peterson, “Tcp vegas: End to end congestion
avoidance on a global internet,” Selected Areas in Communications, IEEE
Journal on, vol. 13, no. 8, pp. 1465–1480, 1995.
[18] K. Tan, J. Song, Q. Zhang, and M. Sridharan, “A compound tcp
approach for high-speed and long distance networks,” in Proceedings-
IEEE INFOCOM, 2006.
[19] D. X. Wei, C. Jin, S. H. Low, and S. Hegde, “Fast tcp: motivation,
architecture, algorithms, performance,” IEEE/ACM Transactions on Net-
working (ToN), vol. 14, no. 6, pp. 1246–1259, 2006.
[20] N. Cardwell, Y. Cheng, C. S. Gunn, S. H. Yeganeh, and V. Jacobson,
“Bbr: Congestion-based congestion control,” Queue, vol. 14, no. 5, p. 50,
2016.
[21] A. Sivaraman, K. Winstein, P. Thaker, and H. Balakrishnan, “An experi-
mental study of the learnability of congestion control,” in Proceedings of
the 2014 ACM conference on SIGCOMM. ACM, 2014, pp. 479–490.
[22] M. Dong, Q. Li, D. Zarchy, P. B. Godfrey, and M. Schapira, “Pcc:
Re-architecting congestion control for consistent high performance,”
NSDI’15 Proceedings of the 12th USENIX Conference on Networked
Systems Design and Implementation, 2015.
[23] T. Le, C. Szepesvari, and R. Zheng, “Sequential learning for multi-
channel wireless network monitoring with channel switching costs,”
IEEE Transactions on Signal Processing, vol. 62, no. 22, pp. 5919–5929,
2014.
[24] N. Liu, Z. Li, J. Xu, Z. Xu, S. Lin, Q. Qiu, J. Tang, and Y. Wang, “A hier-
archical framework of cloud resource allocation and power management
using deep reinforcement learning,” in Distributed Computing Systems
(ICDCS), 2017 IEEE 37th International Conference on.
IEEE, 2017,
pp. 372–382.
[25] Z. Xu, Y. Wang, J. Tang, J. Wang, and M. C. Gursoy, “A deep reinforce-
ment learning based framework for power-efﬁcient resource allocation
in cloud rans,” in Communications (ICC), 2017 IEEE International
Conference on.
IEEE, 2017, pp. 1–6.
[26] O. Habachi, H.-P. Shiang, M. van der Schaar, and Y. Hayel, “Online
learning based congestion control for adaptive multimedia transmission,”
IEEE Transactions on Signal Processing, vol. 61, no. 6, pp. 1460–1469,
2013.
[27] M. Hemmati, A. Yassine, and S. Shirmohammadi, “An online learning
approach to qoe-fair distributed rate allocation in multi-user video
streaming,” in Signal Processing and Communication Systems (ICSPCS),
2014 8th International Conference on.
IEEE, 2014, pp. 1–6.
13
[28] J. van der Hooft, S. Petrangeli, M. Claeys, J. Famaey, and F. De Turck,
“A learning-based algorithm for improved bandwidth-awareness of adap-
tive streaming clients,” in Integrated Network Management (IM), 2015
IFIP/IEEE International Symposium on.
IEEE, 2015, pp. 131–138.
[29] S. Whiteson, M. E. Taylor, P. Stone et al., Adaptive tile coding for value
function approximation. Computer Science Department, University of
Texas at Austin, 2007.
[30] S. Chernova and M. Veloso, “Tree-based policy learning in continuous
domains through teaching by demonstration,” in Proceedings of Work-
shop on Modeling Others from Observations (MOO 2006), 2006.
Wei Li is a PhD candidate in the ECE depart-
ment of Northeastern University. He is doing his
research under the instruction of Prof. Waleed
Meleis. He graduated from the Northeastern
University with a M.S. in Computer Engineering
in 2012, and he obtained his B.S. in Control
Engineering from the University of Electronic
Science and Technology of China. His current
research area is mainly on reinforcement learn-
ing algorithms, function approximation technique
and learning-based network applications.
Fan Zhou is a PhD Student in the ECE De-
partment at Northeastern University. He is cur-
rently working in the Next Generation Networks
and Systems Lab under the supervision of Prof.
Kaushik Chowdhury. He received B.S. and M.S.
degree from Hohai University (2011) and Bei-
jing University of Posts and Telecommunications
(2014). His research interests spread across dif-
ferent layers of wireless networking, with a spe-
ciﬁc focus on the design and implementation of
high performance data transfer architecture.
Kaushik Roy Chowdhury (M’09-SM’15)
re-
ceived the M.S. degree from the University of
Cincinnati in 2006, and the Ph.D. degree from
the Georgia Institute of Technology in 2009. He
was an Assistant Professor from 2009 to 2015
at Northeastern University, where he is now an
Associate Professor with the Electrical and Com-
puter Engineering Department. He was a winner
of the Presidential Early Career Award for Sci-
entists and Engineers (PECASE) in 2017, ONR
Director of Research Early Career Award in 2016
and the NSF CAREER Award in 2015. His current research interests
include dynamic spectrum access, wireless RF energy harvesting and
IoT and in the area of intra/on-body communication.
Waleed Meleis is Associate Professor and As-
sociate Chair in the Department of Electrical and
Computer Engineering at Northeastern Univer-
sity. He received the BSE degree in Electrical
Engineering from Princeton University and the
MS and PhD degrees in Computer Science and
Engineering from the University of Michigan. His
research is on applications of algorithm design,
combinatorial optimization and machine learn-
ing to diverse engineering problems, including
cloud computing, spectrum management, high-
performance compilers, instruction scheduling, parallel programming
and network management.