whether their data is used by the target recommender.
Adversary’s Knowledge. We assume an adversary has only black-
box access to the target recommender. That is, adversary can only
observe the items recommended to a target user (i.e., recommenda-
tions), and the user’s history (i.e., interactions), such as rating and
purchase, instead of posterior probabilities for recommendation
predictions. In that case, the adversary needs to profile users by
their interactions and recommendations. Meanwhile, a shadow rec-
ommender is built to generate labeled data for the attack model,
since ground truth membership is unavailable from the target rec-
ommender.
2.3 Recommender Systems
In this section, the framework of recommender systems is briefly
introduced.
Recommendation algorithms output recommended items based
on the information learnt from input. In the paper, two types of
recommendation algorithms are mainly involved: personalized and
non-personalized recommendation algorithms. For members, items
are recommended according to the preferences of members. Mean-
while, lacking non-members’ data, non-personalized recommen-
dation algorithms are conducted, and provide most popular items
for non-members. Specifically, Item-Based Collaborative Filtering
(Item) [40], Latent Factor Model (LFM) and Neural Collaborative
Filtering (NCF) [16] are adopted as the personalized recommen-
dation algorithms for members. As for the non-personalized rec-
ommendation algorithm, the most popular items are provided to
non-members, which is also called the popularity recommendation
algorithm in the paper. We briefly introduce the above algorithms
as follows:
the ones which are closed to users’ likes.
• Item calculates the similarity between items aiming to find
• LFM builds a latent space to bridge user preferences and
• NCF combine the deep learning technology with collabora-
tive filtering to enhance the recommendation performances.
• Users are provided with the most popular items by the pop-
item attributes.
ularity recommendation algorithm.
In general, a recommender system A𝑅𝑆 learns user preferences
from the interactions, sometimes with the external knowledge (such
as gender and location information) for users. According to the
predicted preferences, the recommender system provides users with
multiple items. This procedure can be formulated as:
A𝑅𝑆 : (I𝑅𝑆,K𝑅𝑆) → R𝑅𝑆,
2.2 Threat Model
Adversary’s Goal. The adversary aims to infer whether a user’s
data is used by a target recommender. In fact, knowing a certain
user’s data being used by a recommender system directly leaks
where A𝑅𝑆 is a recommender system learning the preferences of
users from their interactions I𝑅𝑆 and the external knowledge K𝑅𝑆.
And R𝑅𝑆 denotes recommended items to users. In the paper, we
mainly use the interactions of users. Thus, we define a recommender
Session 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea866attack consists of three steps to achieve the adversary’s goal: La-
beled Data Generation, Attack Model Establishment, and Parameter
Optimization.
Labeled Data Generation. Training data is required during the
A𝑎𝑡𝑡𝑎𝑐𝑘 optimization process. However, the adversary cannot ob-
tain membership status directly from the target recommender
A𝑡𝑎𝑟𝑔𝑒𝑡. To address this problem, a shadow recommender A𝑠ℎ𝑎𝑑𝑜𝑤
is developed to mimic the dataset and recommendation algorithm
of the target recommender.
As mentioned in Section 1.1, only recommended item lists from
target recommender systems can be observed. Inspired by the pre-
vious works [17, 24], matrix factorization is adopted to project users
and items into a shared latent space. Specifically, a 𝑝 × 𝑞 user-item
matrix M𝑓 is built using ratings of users to items, where 𝑝 and 𝑞
are the number of users and items respectively. Values in M𝑓 are
ratings ranging from 1 to 5, indicating how much users prefer these
items. Then, M𝑓 is factorized into two low-dimensional matrices,
namely latent user matrix M𝑢𝑠𝑒𝑟 ∈ R𝑝×𝑙 and latent item matrix
M𝑖𝑡𝑒𝑚 ∈ R𝑞×𝑙, where we denote 𝑙 as the dimension of the latent fea-
ture space. We apply matrix factorization to find optimized M𝑢𝑠𝑒𝑟
and M𝑖𝑡𝑒𝑚 by minimizing the loss function 𝐿𝑀𝐹 :
(cid:13)(cid:13)(cid:13)M𝑓 − ˆM𝑓(cid:13)(cid:13)(cid:13)2
𝐿𝑀𝐹 =
𝑤ℎ𝑒𝑟𝑒
ˆM𝑓 = M𝑖𝑡𝑒𝑚 · M𝑢𝑠𝑒𝑟 T,
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
and D𝑜𝑢𝑡
where ˆM𝑓
is a predicted user-item matrix which contains the
predicted scores of users rating items. Besides, M𝑢𝑠𝑒𝑟 and M𝑖𝑡𝑒𝑚
present the predicted preferences of users and the predicted at-
tributes of items, respectively. Each row of the item matrix M𝑖𝑡𝑒𝑚
represents the feature vector of the corresponding item. Note that,
since M𝑢𝑠𝑒𝑟 may not cover all users in the shadow and target rec-
ommenders, M𝑢𝑠𝑒𝑟 is not used to represent users.
To this end, training data for the attack model can be generated
from A𝑠ℎ𝑎𝑑𝑜𝑤. The shadow dataset D𝑠ℎ𝑎𝑑𝑜𝑤 are split into two
disjoint sets for members and non-members, which are denoted by
D𝑖𝑛
, respectively. These datasets are composed
of 3-tuples in the form of (𝑢𝐼𝐷, 𝑖𝐼𝐷, 𝑠𝑐𝑜𝑟𝑒), indicating scores rated
by users to items. For instance, a 3-tuple (2, 3, 4) in datasets means
that the 2𝑛𝑑 user rates the 3𝑟𝑑 item a score of 4. Ratings in D𝑖𝑛
𝑠ℎ𝑎𝑑𝑜𝑤
and D𝑜𝑢𝑡
can be seen as interactions of users to items, and sets
of interaction lists for members and non-members can be obtained,
denoted as I𝑖𝑛
, respectively. In that case, each
user has a list of interactions. For example, if a user rates the 2𝑛𝑑, 4𝑡ℎ,
6𝑡ℎ and 8𝑡ℎ items, the corresponding interaction list is {2, 4, 6, 8}.
Next, A𝑠ℎ𝑎𝑑𝑜𝑤 is established to mimic A𝑡𝑎𝑟𝑔𝑒𝑡, and provides
users with recommendations according to their preferences. The
sets of recommendation lists for members and non-members are
denoted by R𝑖𝑛
, respectively. Similar as inter-
actions, each user is associated with a list of recommendations.
However, R𝑖𝑛
are sets of ordered lists of recom-
mendations. Formally, the recommendation process can be formu-
lated as follows:
and R𝑜𝑢𝑡
and R𝑜𝑢𝑡
and I𝑜𝑢𝑡
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
A𝑠ℎ𝑎𝑑𝑜𝑤 : 𝑓𝑝𝑒𝑟 (I𝑖𝑛
A𝑠ℎ𝑎𝑑𝑜𝑤 : 𝑓𝑝𝑜𝑝(I𝑖𝑛
𝑠ℎ𝑎𝑑𝑜𝑤) = R𝑖𝑛
𝑠ℎ𝑎𝑑𝑜𝑤) = R𝑜𝑢𝑡
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤,
where 𝑓𝑝𝑒𝑟 performs a personalized recommendation algorithm
based on the behaviors of members. Meanwhile, since non-members’
Figure 2: The framework of the membership inference at-
tack against a recommender system.
system as:
A𝑅𝑆 : I𝑅𝑆 → R𝑅𝑆,
where I𝑅𝑆 is a set of lists of interactions for users and R𝑅𝑆 is a set
of ordered lists of recommendations for users. Concretely, I𝑅𝑆 =
𝑛=1 and R𝑅𝑆 = {𝐿𝑛
{𝐿𝑛
𝑅}𝑁𝑢
𝐼 }𝑁𝑢
is the list of interactions
𝑛=1, where 𝐿𝑛
and 𝐿𝑛
𝑅 is the ordered list of recommendations for the 𝑛𝑡ℎ user, and
𝑁𝑢 is the number of users.
𝐼
2.4 Attack Overview
In this section, we give an overview of our attack. As Figure 2
demonstrated, the attack process follows three steps: Labeled Data
Generation, Attack Model Establishment, and Parameter Optimiza-
tion.
Labeled Data Generation. To represent items, an item matrix
is derived, by factorizing a user-item rating matrix. Due to the
black-box access to the target recommender for the adversary, a
shadow recommender is built to generate labeled training data for
the attack model. Moreover, we represent interactions and recom-
mendations of users using corresponding feature vectors. After
that, a user is profiled by the difference between two centers of
their interactions and recommendations. And each user is labeled
with 1 or 0, indicating they are a member or non-member.
Inspired by [39], a two-hidden-
Attack Model Establishment.
layer Multi-Layer Perceptron (MLP) is utilized as the attack model
𝐴𝑎𝑡𝑡𝑎𝑐𝑘 to infer membership status. Each hidden layer is followed
by a ReLU activation layer. And a softmax function is used as the
output layer to predict the probability of the membership.
Parameter Optimization. After Labeled Data Generation and
Attack Model Establishment, as shown in Figure 2, the adversary
updates parameters of the attack model. In the inference stage, the
test dataset for target users are established following the same steps
as training data generation. The membership status for target users
is inferred by the trained attack model.
2.5 Membership Inference Attack
In this section, we detail our proposed membership inference attack
against a recommender system. As mentioned before, the whole
TargetRecommenderShadowRecommenderRecommendationsRtargetTrainedAttack ModelAttack Model EstablishmentTrainingLabelsLabeled Data GenerationInferenceParameterOptimizationTrainingSetDtrainUser FeaturesZshadowRecommendationFeaturesVshadowInteractionFeaturesUshadowRecommendationsRshadowInteractionsIshadowIInteractionsItargetInteractionFeaturesUtargetUser FeaturesZtargetTestSetDtestRecommendationFeaturesVtarget533TestLabelsUnobservableFor Attack Model Session 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea867data is unavailable to A𝑠ℎ𝑎𝑑𝑜𝑤, 𝑓𝑝𝑜𝑝 performs the popularity rec-
ommendation algorithm (a non-personalized recommendation al-
gorithm) based on the statistical results from I𝑖𝑛
. Besides,
I𝑖𝑛
and R𝑜𝑢𝑡
members and non-members respectively.
is a set of lists of interactions for members, and R𝑖𝑛
𝑠ℎ𝑎𝑑𝑜𝑤
are sets of ordered lists of recommended items for
Using item feature representations, we can vectorize the interac-
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
𝑠ℎ𝑎𝑑𝑜𝑤
tion and recommendation sets as follows:
I𝑠ℎ𝑎𝑑𝑜𝑤
R𝑠ℎ𝑎𝑑𝑜𝑤
𝑣𝑒𝑐−→ U𝑠ℎ𝑎𝑑𝑜𝑤
𝑣𝑒𝑐−→ V𝑠ℎ𝑎𝑑𝑜𝑤,
where U𝑠ℎ𝑎𝑑𝑜𝑤 and V𝑠ℎ𝑎𝑑𝑜𝑤 are sets of lists of the feature vectors
for the corresponding items in I𝑠ℎ𝑎𝑑𝑜𝑤 and R𝑠ℎ𝑎𝑑𝑜𝑤.
Given that each user has a list of interactions and is provided
with an ordered list of recommendations, the adversary is able to
represent users by their relevant items. To be specific, for the 𝑖𝑡ℎ
user, the representation is generated with the following two steps:
1) Center vectors of the interactions’ and recommendations’
feature vectors of the 𝑖𝑡ℎ user are calculated:


𝑗
𝑗
U𝑖 =
V𝑖 =
U𝑖,𝑗/𝑁 𝑖𝑛𝑡
𝑖
V𝑖,𝑗/𝑁 𝑟𝑒𝑐
𝑖
,
whereU𝑖 andV𝑖 are the center vectors of the feature vectors
for the interactions and recommendations of the 𝑖𝑡ℎ user,
and 𝑁 𝑖𝑛𝑡
are the corresponding quantities. Besides,
U𝑖,𝑗 and V𝑖,𝑗 are the feature vectors for the 𝑗𝑡ℎ interaction
and recommendation of the 𝑖𝑡ℎ user, respectively.
and 𝑁 𝑟𝑒𝑐
𝑖
𝑖
2) The difference between the two center vectors are obtained:
z𝑖 = U𝑖 − V𝑖 .
In the paper, we employ z𝑖 as the feature vector for the 𝑖𝑡ℎ
user, which takes not only the user’s history but also the
predicted preference into consideration.
Meanwhile, each user is assigned a label of 1 or 0, indicating their
membership (i.e., 1 means member and 0 means non-member). The
training dataset D𝑡𝑟𝑎𝑖𝑛 = {(𝑓 𝑒𝑎𝑡𝑢𝑟𝑒𝑖, 𝑙𝑎𝑏𝑒𝑙𝑖)}𝑁
𝑖=1 contains feature
vectors and labels of all users, where the pair (𝑓 𝑒𝑎𝑡𝑢𝑟𝑒𝑖, 𝑙𝑎𝑏𝑒𝑙𝑖)
denotes the feature vector and label for the 𝑖𝑡ℎ user.
Attack Model Establishment. Inspired by [39], a MLP is estab-
lished as the attack model A𝑎𝑡𝑡𝑎𝑐𝑘. The output of A𝑎𝑡𝑡𝑎𝑐𝑘 is a
2-dimension vector representing probabilities for the membership
status. For the 𝑖𝑡ℎ user, the prediction can be formulated as follows:
h1 = ReLU(W1zi + b1)
h2 = ReLU(W2h1 + b2)
y𝑖 = softmax(h2),
where z𝑖 is the input of A𝑎𝑡𝑡𝑎𝑐𝑘 as well as the 𝑖𝑡ℎ user’s feature
vector in our attack. And W1, W2, b1 and b2 are the parameters
updated in the training process. ReLU(·) is an activation function
working on the outputs of two hidden layers, and softmax(·) is
used for normalization which is required by the cross-entropy loss.
Besides, h1 and h2 are the results of two hidden layers after ReLU(·).
And y𝑖 is the predicted result for the input z𝑖, which is a 2-dimension
𝑖=1
i + (1 − y∗
𝑁𝑡𝑟𝑎𝑖𝑛
i )log(1 − y′
i)),
vector indicating the possibilities of z𝑖 belonging to members and
non-members, respectively.
Parameter Optimization. In this section, the parameter optimiza-
tion process for the attack model is described. Stochastic gradient
descent is adopted to update parameters, aiming to minimize the
cross-entropy loss function 𝐿𝑀𝐿𝑃:
(y∗
𝑖 logy′
𝐿𝑀𝐿𝑃 = −
where y∗