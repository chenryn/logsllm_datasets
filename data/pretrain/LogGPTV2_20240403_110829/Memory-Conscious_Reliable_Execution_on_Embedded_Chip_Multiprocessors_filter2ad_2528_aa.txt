title:Memory-Conscious Reliable Execution on Embedded Chip Multiprocessors
author:Guangyu Chen and
Mahmut T. Kandemir and
Ibrahim Kolcu
Memory-Conscious Reliable Execution on Embedded Chip Multiprocessors
G. Chen, M. Kandemir
Department of Computer Science and Engineering
The Pennsylvania State University
University Park, PA 16802, USA
{guilchen,kandemir}@cse.psu.edu
I. Kolcu
Computation Department
University of Manchester
Manchester M60 1QD, UK
PI:EMAIL
Abstract
Code and data duplication has been identiﬁed as one
of the important mechanisms for improving reliability. In
a chip multiprocessor-based execution environment, while
it is possible to hide the overhead of code duplication
through parallelism, hiding the memory space overhead in-
curred by data duplication is more difﬁcult. This paper
presents a compiler-directed memory-conscious data dupli-
cation scheme that tries to minimize the extra memory space
required by duplicate execution. The proposed approach
achieves this goal by using the memory locations that hold
dead data to store the duplicates of the actively-used data.
In this way, instead of using extra memory storage for dupli-
cate elements, we use the existing memory locations to the
extent allowed by usage patterns of data. The results col-
lected from our experiments clearly show that the proposed
approach saves signiﬁcant memory space, as compared to a
straightforward approach that implements full duplication.
1 Introduction
Chip multiprocessors (CMPs), where multiple (simple)
processors are placed into a single chip and connected
through an on-chip network infrastructure, have recently
emerged as a promising option in utilizing available silicon
area [8, 9, 14, 19, 20, 32]. These multiprocessor architec-
tures have already found their ways into commercial prod-
uct such as IBM’s Cell [8] and Sun Microsystem’s Niagara
[9]. However, employment of CMPs in the embedded com-
puting domain requires carefully balancing multiple, and
often conﬂicting, metrics such as performance, power con-
sumption, reliability and memory space requirements. An
important problem in this context is the tradeoff between
memory space requirements and reliability. CMP’s suscep-
tibility to hardware transient errors increases as the number
of transistors packed in the chip increases [6, 13]. On the
other hand, the abundant on-chip parallelism available on a
CMP provides a good opportunity for hiding the latency due
to redundant execution for improving reliability. In many
embedded environments where reliable execution is impor-
tant, one can opt to use on-chip parallelism for duplicating
select computations. The results of the primary and dupli-
cate code fragments can then be compared with each other
and an appropriate course of action can be taken depending
on the outcome of the comparison.
It needs to be noted however that, due to data depen-
dences and other factors, duplicating computations usually
demands data duplication as well. Unfortunately, unlike
code duplication, data duplication can be very costly from
the memory space angle and thus easily creates an impor-
tant problem for many memory-constrained embedded sys-
tems. Speciﬁcally, on the one hand, we want to duplicate as
many computations as possible to increase reliability (more
speciﬁcally, transient error detection capability); but, on the
other hand, we want to minimize the resulting increase in
the memory space requirements. It is easy to see that bal-
ancing these reliability and memory space issues can be
very useful in practice.
Motivated by this observation, this paper proposes a
compiler-directed memory-conscious computation duplica-
tion scheme in the context of CMPs, focusing in particu-
lar on embedded video/image processing applications that
process large memory spaces [4]. Our goal is to minimize
the increase in memory space requirements due to data du-
plication while duplicating as much computation as pos-
sible. Since most embedded image/video applications are
usually constructed as a series of loop nests operating on
multidimensional arrays of signals [4], an optimizing com-
piler can accurately analyze these codes, capture their data
access patterns and data lifetimes, and optimize the usage
of available memory space. Speciﬁcally, the proposed ap-
proach uses dead memory locations (i.e., the memory loca-
tions whose contents have already reached their last reuses)
to store the extra data elements required for duplicate com-
putations. In this way, the extra memory space requirements
due to duplication are kept at minimum. In this paper, we
address this problem both at a loop nest granularity and
across multiple nests.
We implemented our compiler-directed approach within
an optimizing compiler infrastructure [30] and performed
experiments using a multiprocessor simulator [23]. Our ex-
periments with six embedded applications and six SPEC
2000 benchmarks clearly show that the proposed approach
improves over a straightforward data duplication scheme
signiﬁcantly (31.2% on the average). The experiments also
show that, since our approach cuts the memory space re-
quirements, it also reduces the execution cycles taken by
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:27:26 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1. CMP architecture under con-
sideration.
Figure 2. Parallelization of a loop nest
on CMP.
Figure 3. Execution dupli-
cation.
the full duplication based scheme (by 10.0% on the aver-
age), due to improved cache behavior. To the best of our
knowledge, this is the ﬁrst compiler based study that tries to
minimize the data space overheads brought by code dupli-
cation in the context of embedded chip multiprocessors.
The remainder of this paper is structured as follows. The
next section describes the embedded CMP architecture our
approach targets and explains our code duplication strat-
egy. Section 3 gives the mathematical framework behind
our approach and presents our compiler algorithm. The re-
sults from the experimental evaluation of our approach are
given in Section 4. Section 5 discusses the related work
on software support for CMPs, memory reuse, and tran-
sient/permanent errors. Section 6 concludes the paper by
summarizing our major contributions.
2 CMP and Code/Data Duplication for Reli-
ability
Figure 1 illustrates the high-level view of a typical CMP
architecture, similar to Sun’s Niagara [9].
In this archi-
tecture, multiple processor cores are placed on the same
chip. Each processor has its private L1 instruction and data
caches.
In addition, the processors share an on-chip L2
cache and have access to a shared off-chip memory.
We focus on loop nest-based data-intensive application
codes running on CMPs. Such application codes are usu-
ally parallelized at the loop nest level [31]. When a loop
nest is parallelized, the iterations of this loop nest are di-
vided into several portions and each portion is assigned to a
processor. The iterations running on each processor are also
in the form of a loop nest. Figure 2(b) illustrates an exam-
ple loop-based parallelization for the code fragment given
in Figure 2(a). The parallelized program runs on three pro-
cessors, P1, P2, and P3. Figure 2(c) gives the loop iterations
(in the form of a loop nest) assigned to processor P2, as a
result of loop parallelization.
In this work, we use execution duplication (also referred
to as code duplication) for improving transient error detec-
tion capability. Figure 3 illustrates our reliability-aware ex-
ecution model for loop-based application codes. In this set-
ting, execution duplication works on per loop basis. After
loop parallelization, processor P1 is assigned a loop nest for
execution and the execution on P1 is called the primary ex-
ecution or the primary loop nest. Processor P2 runs a dupli-
cate of the loop nest executed on P1, which is called the du-
plicate execution or the duplicate loop nest. We detect tran-
sient errors by comparing their results (we assume that tran-
sient errors do not affect the sequentiality of the program
execution). There exist at least two approaches regarding
when to compare the execution results. The ﬁrst approach,
which is called the lock-step approach, compares the results
between the primary and the duplicate after each memory
write operation. A lock-step approach has short error detec-
tion latency, but can incur high synchronization overhead.
Therefore, it is usually only used in single-threaded envi-
ronments [15, 18] or CMPs with special hardware support
for lock-step comparisons [6, 13]. In this work, we employ
an alternate strategy, which compares the results after the
execution of the loop nest and incurs much less synchro-
nization overhead than the lock-step approach.
There are two possible scenarios in which one can em-
ploy the execution duplication strategy depicted in Figure 3.
In the ﬁrst scenario, each primary execution always has a
corresponding duplicate execution. This scenario suits well
when reliability is very important and we want to detect
most (if not all) of the errors. In the second scenario, not
all the primary executions have their corresponding dupli-
cates. This scenario usually results from a resource-aware
parallelization scheme. As has already been demonstrated
by prior research [11], not all processors in a given embed-
ded CMP are needed to achieve the best (or close-to-best)
performance for a parallel loop nest. We can opt to use
the idle processors for duplicating execution. It should be
emphasized that our execution duplication model applies to
both scenarios, and the mechanism used to select the op-
timal number of processors to use for a given loop nest is
orthogonal to our approach. In our work, we assume that
loop parallelization and duplication selection have already
been determined by a prior compiler phase.
3 Data Duplication Schemes
When duplicating execution, we need to duplicate part
or all of the data as well. Data duplication serves several
purposes. First, we can use duplicates to detect errors in
the memory components. Second, duplicates can be used
to avoid data race conditions between the primary execu-
tion and the duplicate execution. Finally, the duplicate data,
together with the primary data, are placeholders for com-
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:27:26 UTC from IEEE Xplore.  Restrictions apply. 
int U [100],V [100],W [100];
for(i = 0; i < 100; i + +) {
U [i] = V [i] + 1;
W [i] = U [i] ∗ V [i];
}
(a) Primary loop nest.
int ˜U [100], ˜V [100], ˜W [100];
for(i = 0; i < 100; i + +) {
˜U [i] = ˜V [i] + 1;
˜W [i] = ˜U [i] ∗ ˜V [i];
}
(b) Duplicate loop nest.
Figure 4. An example application of the FULL
scheme.
int U [100],V [100],W [100];
for(i = 0; i < 100; i + +) {
U [i] = V [i] + 1;
chkp = chkp ⊕ U [i];
W [i] = U [i] ∗ V [i];
chkp = chkp ⊕ W [i];
}
(a) Primary loop nest.
int ˜U[100];
for(i = 0; i < 100; i + +) {
˜U [i] = V [i] + 1;
chkd = chkd ⊕ ˜U [i];
chkd = chkd ⊕ ˜U [i] ∗ V [i];
}
(b) Duplicate loop nest.
Figure 5. An example application of the NRWD
scheme.
paring the results of the primary and duplicate executions.
In this section, we study three different data duplication
schemes, the last of which is our major contribution. Since
arrays are the major memory space consumer in loop-based
data-intensive codes, we focus on the memory overheads
incurred by array duplications in all these three schemes;
scalar variables are treated as a degenerate form of one-
dimensional arrays with a single element.
3.1 The FULL Scheme
In the full duplication (FULL) scheme, we create dupli-
cates for all the arrays used in the loop nest. Figure 4 gives
an example application of the FULL scheme. In this exam-
ple, the primary loop nest is given in Figure 4(a). As we
can observe in Figure 4(b), all three arrays, U , V , and W ,
are duplicated in the duplicate loop nest; ˜U, ˜V , and ˜W are
the duplicates for U , V , and W , respectively. In all the ex-
amples presented in this paper, we use a similar notation,
where ˜A represents the duplicate for array A. Note that
extra instructions are required for initialization of the dupli-
cate arrays and for error detection. For example, we might
need to copy V to ˜V before the loop nest. We also need to
synchronize the primary and the duplicate execution at the
end of the loop nest, and then compare array U (or W ) to
array ˜U (or ˜W ) for error detection. We do not include such
extra instructions in our examples for clarity of illustration.
The FULL scheme requires very few assumptions re-
garding the underlying hardware architecture, and is easy to
implement. However, it doubles the memory space demand
of the original loop nest, which is its major drawback.
3.2 The NRWD Scheme
Our second data duplication scheme, named no read-
only or write-only duplication (NRWD), tries to reduce the
memory space consumption due to execution duplication by
requiring a small amount of hardware support and employ-
ing a checksum-based result comparison approach. In this
scheme, ﬁrst, we introduce hardware ECC protection for
all the memory components, including on-chip caches and
off-chip memory. For the read-only arrays, the errors in
memory can be detected or recovered by ECC. Since there
is no write operation on them, transient errors in a processor
cannot be propagated from it to the memory locations that
processor can access, unless an error causes a non-store in-
struction to be converted to a store instruction, and the des-
tination address falls within the read-only array. The prob-
ability of such an event is very low, and thus we do not
consider it in this paper. Furthermore, the read-only arrays
do not cause any race condition and also are not required to
be compared for error detection. Therefore, we do not need
to have duplicates for the read-only arrays.
However, we cannot eliminate the duplicates for the
write-only arrays by simply employing an ECC-protected
memory, since we have to compare the primary and the du-
plicate at the end of the loop execution. In addition, race
condition arises if both the primary execution and the dupli-
cate execution write into the same memory location. Both
these problems can be solved by employing a checksum-
based comparison approach, in which we can aggregate the
execution results for both the primary execution and the du-
plicate execution using two checksums. Following the exe-
cutions of the primary and the duplicate, instead of compar-
ing the modiﬁed array elements one by one, we compare the
two checksums. In this scenario, we do not need duplicates
as placeholders for comparison, since only the checksums