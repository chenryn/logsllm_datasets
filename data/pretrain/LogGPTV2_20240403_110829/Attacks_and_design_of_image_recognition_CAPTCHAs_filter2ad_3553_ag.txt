the  resulting 
198 
7.  CONCLUSIONS 
We  have  attempted  a  systematic  study  of  image  recognition 
CAPTCHAs.  We  provided  a  thorough  review  of  the  state-of-the-
art,  presented  a  novel  attack  on  a  representative  scheme,  and 
analyzed  successful  attacks  on  the  other  representative  schemes. 
Learned  from  these  attacks,  we  defined  for  the  first  time  a  simple 
but  novel  framework  for  guiding  the  design  of  robust  image 
recognition  CAPTCHAs.  The  framework  led  to  our  design  of 
Cortcha,  a  novel  CAPTCHA  that  exploits  semantic  contexts  for 
image object recognition. Our usability study showed that Cortcha 
yielded  a  slightly  better  human  accuracy  rate  than  Google’s  text 
CAPTCHA.  Cortcha  offers  the  following  novel  features.  Image 
labeling  is  entirely  avoided.  The  source  image  collection  and 
challenge  generation  are  fully  automated.  An  infinite  number  of 
object types are used to generate Cortcha challenges. Objects used 
in  the  current  challenge  are  independent  of  the  objects  used  in 
previous  challenges.  This  independence  makes  powerful  machine 
learning  attacks  useless  in  attacking  Cortcha.  Being  scalable, 
Cortcha  is  a  stride  forward  for  image  recognition  CAPTCHAs 
towards practical applications. Our future work includes improving 
Cortcha’s  speed,  a  large-scale  usability  study,  and  a  thorough 
evaluation of Cortcha’s robustness.   
8.  REFERENCES 
[1]  Ahn, L. von, Blum, M., and Langford, J. 2003. Telling humans 
and computers apart automatically. Comm. of the ACM. 46 
(Aug. 2003), 57-60. 
[2]  Ahn, L. von, Blum, M., Hopper, N. J., and Langford, J. 2003. 
CAPTCHA: Using hard AI problems for security. 
Eurocrypt’2003. 
[3]  Baird, H. S. and Popat, K. 2002. Human interactive proofs and 
document image analysis. In Proc. of Document Analysis 
Systems 2002. 507–518. 
[4]  Hocevar, S. PWNtcha - Captcha Decoder web site. 
http://sam.zoy.org/pwntcha/. 
[5]  Mori, G. and Malik, J. 2003. Recognizing objects in 
adversarial clutter: Breaking a visual CAPTCHA. In Proc.  
IEEE Conf. on Computer Vision & Pattern Recognition, 2003.   
[6]  Moy, G., Jones, N., Harkless, C., and Potter, R. 2004. 
Distortion   estimation   techniques   in   solving   visual 
CAPTCHAs. In IEEE Conf. on Computer Vision & Pattern 
Recognition, 2004.  
[13] Datta, R., Li, J., and Wang, J. Z. 2005. IMAGINATION: A 
robust image-based CAPTCHA Generation System. In ACM 
Multimedia 2005, 331-334. 
[14] IMAGINATION demo system. 
http://goldbach.cse.psu.edu/s/captcha/ 
[15] Elson, J., Douceur, J. R., Howell, J., and Saul, J.  2007. Asirra: 
a CAPTCHA that exploits interest-aligned manual image 
categorization. In ACM CCS’2007, 366-374. 
[16] Gossweiler, R., Kamvar, M., and Baluja, S. 2009. What’s up 
CAPTCHA? a CAPTCHA based on image orientation. In 
WWW’2009, 841-850. 
[17] Kluever, K. A. and Zanibbi, R. 2009. Balancing usability and 
security in a video CAPTCHA. In Proc. Symp. Usable Privacy 
and Security, (2009). 
[18] Chellapilla, K., Larson, K., Simard, P., and Czerwinski, M. 
2005. Computers beat humans at single character recognition 
in reading-based Human Interaction Proofs. In 2nd Conference 
on Email and Anti-Spam (CEAS'05), 2005. 
[19] Chellapilla, K., Larson, K., Simard, P., and Czerwinski, M. 
2005. Building Segmentation Based Human-friendly Human 
Interaction Proofs. In 2nd Int’l Workshop on Human 
Interaction Proofs, Springer-Verlag, LNCS 3517, 2005. 
[20] Chellapilla, K., Larson, K., Simard, P., and Czerwinski, M. 
2005. Designing Human Friendly Human Interaction Proofs 
(HIPs). In Proc. of the SIGCHI Conf. on Human Factors in 
Computing Systems (CHI’05). 711-720. 
[21] Ahn, L. von. 2005. Human Computation. Ph. D. dissertation, 
Carnegie Mellon University, CMU-CS-05-193. 
[22] http://www.yuniti.com/register.php.  
[23] Rui, Y., Huang, T. S., and Chang, S.-F. 1999. Image retrieval: 
current techniques, promising directions, and open issues.  J. 
of Visual Comm. & Image Representation, 10, (1999), 39-62. 
[24] Golle, P. 2008. Machine learning attacks against the Asirra 
CAPTCHA. In ACM CCS’2008, 535-542. 
[25] Zhu, B. B., LI, Q., Liu, J, and Xu, N. Machine learning attacks 
on ARTiFACIAL. 2010. Submitted for publication. 
[26] Zhang, W., Sun, J., and Tang, X. 2008. Cat head detection - 
how to effectively exploit shape and texture features. In Proc. 
ECCV 2008, Part IV, LNCS 5305 (2008), 802–816. 
[7]  Chellapilla, K. and Simard, P. 2004. Using machine learning 
to break visual human interaction proofs. Neural Information 
Processing Systems (NIPS'04), MIT Press. 
[27] Sun, J., Yuan, L., Jia, J., and Shum, H.-Y. 2005. Image 
completion with structure propagation. In Int. Conf. Computer 
Graphics and Interactive Techniques, (2005). 861-868. 
[8]  Yan, J. and El Ahmad, A. S. 2007. Breaking Visual 
CAPTCHAs with naive pattern recognition algorithms. In 
Proc. Ann. Comp. Security Applications Conf. 2007, 279-291. 
[9]  Yan, J. and El Ahmad, A. S. 2008. A low-cost attack on a 
Microsoft CAPTCHA. In ACM CCS'2008, 543-554.  
[10] Yan, J. and El Ahmad, A. S. 2008. Usability of CAPTCHAs or 
usability issue in CAPTCHA design. In Proc. 4th Symposium 
on Usable Privacy and Security (2008), 44-52. 
[11] Chew, M. and Tygar, J. D. 2004. Image Recognition 
[28] Deng, Y. and Manjunath, B. S. 2001. Unsupervised 
segmentation of color-texture regions in images and video. 
IEEE Trans Pattern Analysis and Machine Intelligence, 23(8) 
(2001), 800-810. 
[29] Liu, T., Sun, J., Zheng, N. N., Tang, X., and Shum, H-Y. 
2007. Learning to detect a salient object. In Proc. IEEE Conf. 
on Computer Vision and Pattern Recognition, (2007) 1-8. 
[30] Lowe, D. 2004. Distinctive image features from scale-
invariant keypoints. Int. J. of Computer Vision. 60 (2), 91-110. 
CAPTCHAs. In Proc. 7th Info. Security. LNCS 3225, 268-279.  
[31] Nielsen, J. 2003. Usability 101: Introduction to Usability. 
[12] Rui, Y. and Liu, Z. 2004. ARTiFACIAL: Automated reverse 
Turing test using FACIAL features. Multimedia Systems 9 
(2004), 493–502.  
http://www.useit.com/alertbox/20030825.html. 
[32] Ng, T.-T., Chang, S.-F., Lin, C.-Y., and Sun, Q. 2006. Passive-
blind image forensics. Chapter 15, Multimedia Security 
199 
Technologies for Digital Rights Management, edited by Zeng, 
W., Yu, H., and Lin, C.-Y., Academic Press, (2006), 383-412. 
[33] Gloe, T., Kirchner, M., Winkler, A., and Böhme, R. 2007. Can 
we trust digital image forensics? In ACM Multimedia 2007, 
78-86.   
[34] Wu, Q., Sun, S. J., Zhu, W., and Li, G. H. 2009. Identification 
of inpainted images and natural images for digital forensics. 
Journal of Electronics (China), 26 (3) (2009), 341–345.  
[35] Canny, J. 1986. A computational approach to edge detection. 
IEEE Trans. Pattern Analysis and Machine Intelligence, 8 
(1986):679-714. 
9.  APPENDIX 
−
vh
9.1  Line Segment Detection 
The  following  two-step  procedure  is  applied  to  the  binary  edge  image 
binI
horizontal line segments.  
• 
 obtained  in  Section  3.2.1.1  to  find  all  the  potential  vertical  and 
Tracing step. For the horizontal direction, we scan each row from left 
to right to search connected edge points. A line segment tracing ends 
when a gap occurs or the edge point drifts away from the current line 
segment’s  row  index  estimated  by  averaging  the  traced  points.  A 
similar operation is applied for the vertical direction. The result of this 
step is a collection of short horizontal and vertical line segments. 
Clustering step. In this step, the line segments from the previous step 
are  divided  into  different  subsets  with  an  online  clustering  algorithm 
according to adjacency. The online clustering algorithm for horizontal 
line segments works as follows. 
• 
Online Segments Clustering Algorithm 
Input: A set of short horizontal line segments 
1. 
For  each  new  arrived  line  segment,  find  its  nearest  cluster  in  the 
current  cluster  set ܥ ={ܥଵ,ܥଶ,…,ܥே}.  If ܥ  is  empty  or  the  nearest 
distance  exceeds  a  threshold  σ,  a  new  cluster ܥேାଵ  is  generated. 
Otherwise the line segment is added to the nearest cluster ܥ௡. 
2.  Update  the  row  index  for  each ܥ௡.  The  row  index  of ܥ௡ is  calculated 
members in ܥ௡. 
For  each  cluster  in ܥ,  sort  its  line  segments  by  their  left  end  points. 
threshold ߠ௚. 
by  averaging  the  row  index  weighted  by  the  line  length  on  all  the 
Merge two  line segments if their horizontal gap is within a tolerance 
4.  Recalculate  the  row  index  for  each  line  segment  by  minimizing  the 
3. 
average distance to adjacent binary horizontal edge points. 
Output: Final horizontal line segments 
9.2  Cleansing and Ranking Rectangles 
Candidate  rectangles  are  processed  and  ranked  according  to  the  following 
visual cues executed in the order of their presentation in this section.  
9.2.1  Edge Intensity Cue 
The  first  cue  is  the  edge  intensity  on  each  side  of  a  rectangle,  which 
indicates how visible the rectangle is. This cue is useful to find a true image 
rectangle since at least one constituent image should be easily perceived by 
humans. It is worth noting that false boundaries generated by the dithering 
process tend to be weak enough to avoid confusing humans.  
follows: 
Given a boundary ܮ of ܰ points, the edge intensity cue along ܮ is defined as 
;				݌∈ܮ,             (2) 
where ܫሺ݌௠௔௫ሻ is  the  edge  intensity  of  the  nearest  local  maxima ݌௠௔௫  to 
location ݌ along  the  perpendicular  direction.  This  intensity  is  capped  by  a 
maximum  value ܫ௧௛ .  The  weight  function ݓሺ݌ሻ  takes  into  account  the 
distance ݀ between ݌ and ݌௠௔௫: 
ܨ௜௡௧௘௡௦௜௧௬ሺܮሻ=ටଵ
ே∑
ே௣ୀଵ
ܫሺ݌௠௔௫ሻݓሺ݌ሻ
200 
ݓሺ݌ሻ=൝1−ቂௗሺ௣,	௣೘ೌೣሻ
ௗ೘ೌೣ
0,																	݋ݐℎ݁ݎݓ݅ݏ݁
ቃଶ;			݀ሺ݌,݌௠௔௫ሻ<݀௠௔௫
 ,             (3) 
Where ݀௠௔௫  is  the  maximum  distance  to  search  for  the  nearest  local 
maxima, and ݀ሺݔ,ݕሻ is the distance between two points ݔ and ݕ. Rectangles 
with  at  least  one  side  whose ܨ௜௡௧௘௡௦௜௧௬  is  smaller  than  a  threshold  are 
removed from the list of candidate rectangles.  
9.2.2  Traversing Object Cue 
A true image region boundary does not have any traversing object while a 
false  one  may  have  since  dithering  may  not  make  a  traversing  object 
disappear. The following steps are applied to find traversing objects. 
The canny edge detection [35] with an adaptive threshold is applied to each 
color channel, and the results from the color channels are properly aligned 
and merged. Vertical and horizontal line segments are not considered as part 
of an object to avoid interference from the true or false image boundaries. 
Then the edge density on each side of a rectangle boundary is calculated in 
the same way as that described next in Appendix 9.2.3. No traversing object 
cue is applied to the boundary if the edge density on either side is too large 
to avoid mismatched objects across the boundary, or if there is no contour 
long enough on either side. Otherwise contour segments along the boundary 
are  searched and matched  in  the  following  way:  for  each  contour  segment 
long enough along the boundary, contour segments on the other side of the 
boundary  are  searched, and  a matched  contour is  found if  1) both  contour 
segments  have  adjacent  ending  points;  2)  the  contour  segment  to  be 
matched  is  also  long  enough;  3)  the  two  contour  segments  are  smooth 
enough  across  the  boundary.  If  two  matched  contour  segments  across  the 
boundary are found, they form a traversing object. 
ೣ೔భರೣಬೣ೔మ,೤೔భರ೤ಬ೤೔మ
	ܦሺܴ;ߠሻ= ∑
If  a  traversing  object  is  detected  along  a  boundary,  the  boundary  is 
presumed to be false, and all the associated rectangles are removed from the 
set of candidates.  
9.2.3  Edge Density Variation Cue 
If the textures on both sides of a boundary are very different and the change 
of texture aligns with the boundary, the boundary is likely to be a true image 
region boundary. We use edge density as a measure of texture. Edge density 
variation  across  the  boundary  is  our  third  cue.  The  edge  density  in  a 
than  the  other  side.  A  uniform  threshold  may  result  in  significantly  fewer 
edges  on  one  side.  This problem  can  be  addressed  by  applying  a  different 
rectangle area ܴ is defined as follows: 
,                      (4) 
ݓℎ݁ݎ݁	ܫ௕௜௡ି௔௟௟ is  the  binary  edge  map  after  applying  a  threshold ߠ on  the 
total edge image ܫ௘ௗ௚௘. Dithering may cause edges on one side less visible 
threshold	on  each  side  of  the boundary, ߠଵon  one  side  and ߠଶ on the  other. 
஽ሺோభ;	ఏభሻା஽ሺோమ;	ఏమሻ                                (5) 
Thresholds ߠଵand ߠଶ are  chosen  to  minimize  Eq.  (5)  under  the  following 
1)  Both ߠଵ and ߠଶ  are  larger  than  a  minimal  threshold  value ߠ௠௜௡  and 
ܨ௩௔௥௜௔௧௜௢௡ሺܮሻ= |஽ሺோభ;	ఏభሻିሺோమ;	ఏమሻ|
The edge density variation is calculated as follows: 
ሺ௫೔మି௫೔భሻሺ௬೔మି௬೔భሻ
ூ್೔೙షೌ೗೗ሺ௫,௬;ఏሻ
conditions:  
their difference is within a range: |ߠଵ−ߠଶ|≤Δ;  
density unless ߠ௠௜௡ is reached.  
and  is  assigned  a  confidence  value  of  1.  A  boundary  that  is  a  part  of  the 
boundary of the composite image is also assigned a confidence value of 1. 
If the minimum edge density variation across a boundary ܨ௠௜௡is large than a 
threshold ܨ௧௛, the boundary is presumed to be a true image region boundary 
Other boundaries are each assigned a confidence value of  ி೘೔೙ி೟೓ ∗0.9, where 
ܨ௠௜௡ ≤ܨ௧௛. The confidence of a rectangle is the average of the confidences 
of its four side boundaries.  
2)  At  least  one  side  has  its  edge  density  larger  than  a  minimum  edge