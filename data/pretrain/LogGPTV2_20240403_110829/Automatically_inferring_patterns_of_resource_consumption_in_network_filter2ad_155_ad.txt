if(cluster.traf f ic − cluster.estimate ≥ H)
add to compressed report(cluster)
cluster.estimate = cluster.traf f ic
Figure 4: The algorithm for compressing traﬃc re-
ports traverses all clusters starting with the more
speciﬁc ones. The “estimate” counter of each cluster
contains the total traﬃc of a set of non-overlapping
more speciﬁc clusters that are in the compressed
report. The clusters whose estimate is below their
actual traﬃc by more than the threshold H , are
included into the compressed report.
Proof Let m be the ﬁeld with the deepest hierarchy (dm =
max di). Let Lj be the sizes of clusters (indexed by j) that
have * in ﬁeld m. Since each ﬂow belongs to at most 	i(cid:2)=m di
clusters with ∗ in ﬁeld m, we get  Lj ≤ T 	i(cid:2)=m di. We
can obtain any cluster by varying the mth ﬁeld of the cor-
responding cluster j. We can compress all the clusters ob-
tained from cluster j by varying ﬁeld m using the unidi-
mensional algorithm for ﬁeld m, so by applying Lemma 1,
we get that the number of clusters in the result is bound
by sj = Lj/H. These reports for all j together cover all
clusters, so for the total size of the report we get  Lj /H ≤
s 	i(cid:2)=m di = (s 	
k
i=1 di)/(max di). 
We have implemented a fast greedy algorithm for multi-
dimensional compression (Figure 4). It traverses all clusters
in an order that ensures that more speciﬁc clusters come
before all of their ancestors (line 1). At each cluster we keep
an “estimate” counter. When we get to a particular cluster
we compute the sum of the estimates of its children along
all dimensions (line 4) and set the estimate of the current
cluster to the largest among these sums (line 6). If the dif-
ference between the estimate and the actual traﬃc of the
cluster is below the threshold (line 7), it doesn’t go into the
compressed report. Otherwise we report the cluster (line
8) and set its “estimate” counter to its actual traﬃc (line
9). The invariant that ensures the correctness of this algo-
rithm is that after a cluster has been visited, its “estimate”
counter contains the total traﬃc of a set of non-overlapping
more speciﬁc clusters that are in the compressed report. It
is easy to see how this invariant is maintained: when com-
puting the estimate for the cluster, for each dimension, the
algorithm computes the sum of the estimates of the chil-
dren of the node (cluster) along that particular dimension.
Since the sets at the same level of the ﬁeld hierarchy never
overlap, the sets contributing to the estimates of distinct
children will never overlap, so the invariant is maintained.
The compression rule allows the algorithm to consider all
non-overlapping sets of more speciﬁc clusters reported when
computing the estimate. Our algorithm does something sim-
pler: it only looks at the sets of non-overlapping more spe-
ciﬁc clusters that can be partitioned along a dimension or
another. Thus it will sometimes include clusters into the
report that could have been omitted, but it will never omit a
cluster that does not meet the compression criterion (i.e., is
larger by more than H than the traﬃc of each of the sets
of non-overlapping more speciﬁc clusters in the compressed
report). This is a small price to pay for the big gains in per-
formance we get by performing simpler local checks. Com-
puting both byte and packet compressed reports takes less
than 30 seconds for a threshold as low as 0.5%.
In practice compressed traﬃc reports are two to three or-
ders of magnitude smaller than uncompressed reports and
dramatically smaller than the theoretical bound. For a thresh-
old of 5% of the total traﬃc the average report size is around
30 clusters. This is not inﬂuenced signiﬁcantly by the length
of the measurement interval or the diversity of the traﬃc
(backbone versus edge), but the size of the report is pro-
portional to the inverse of the threshold. For brevity we
only present in [5] our algorithm for computing compressed
multidimensional delta reports, but note here that the in-
teractions between compression (Operation 2) and deltas
(Operation 3) are more complex than in the unidimen-
sional case.
Computing “Unexpectedness”
Recall Operation 4 which seeks to prioritize clusters via a
measure of unexpectedness based on comparing the cluster
percentage to the product of the percentages computed for
each ﬁeld in the cluster by itself. Computing the unexpect-
edness score of a given cluster is very easy using the graph
describing the relations between the high volume clusters:
we only need to locate the unidimensional ancestors along
all dimensions.
4. THE AUTOFOCUS TOOL
The AutoFocus prototype is an oﬀ-line traﬃc analysis sys-
tem composed of three principal components:
• Traﬃc Parser. This component consumes raw net-
work measurement data. Our current system uses
(sampled) packet header traces as input, but it could
easily be modiﬁed to accept other forms of data such
as sampled NetFlow records.
• Cluster Miner. The cluster miner is the core of
the tool and applies our multidimensional and unidi-
mensional algorithms to compute compressed traﬃc
reports, compressed delta reports and unexpectedness
scores.
• Visual Display. The visual display component is
responsible for formatting the report and construct-
ing graphical displays to aid understanding. To im-
prove user recognition of individual elements, we post-
process the raw traﬃc report to attach salient names
to individual addresses and ports. These names are
generated from the WHOIS and DNS services, lists of
well-known ports, as well as user-speciﬁed rules that
contain information about the local network environ-
ment (e.g. that a particular host is a Web proxy cache
or a ﬁle server). The display component also generates
a series of time-domain graphs, using diﬀerent colors to
identify a set of key traﬃc categories. These categories
can contain multiple clusters. Categories are ordered
and each ﬂow is counted against the ﬁrst category it
matches, traﬃc not falling any particular category is
lumped into an “Other” category.
Ideally, the cate-
gories are also constructed to be representative of “in-
teresting” aggregates (e.g. outbound SSL traﬃc from
our Web servers). Currently, the user speciﬁes these
categories – typically based on examining the clusters
contained in the textual report. Heuristics for auto-
matically selecting these traﬃc categories remains an
open problem, complicated by the requirement that
categories be meaningful. We expect that some user
involvement will always be beneﬁcial.
Figure 5 depicts a report generated by AutoFocus us-
ing a 5% threshold on a trace recently collected from the
SD-NAP exchange point. After identifying the size of the
total traﬃc and the threshold parameters, the report pro-
vides the ﬁve unidimensional compressed reports – protocol,
source address, destination address, source port and desti-
nation port. For example the report indicates that 66% of
traﬃc in this trace originates from 192.128.0.0/10, however
most of this traﬃc can be attributed to the more speciﬁc pre-
ﬁx, 192.172.226.64/26 (owned by CAIDA). Note that pre-
ﬁxes between these two records, from /10 to /26, are com-
pressed away because their traﬃc does not diﬀer by more
than 5% (17.7GB) from the more speciﬁc /26 preﬁx. Ulti-
mately, an individual source IP address is responsible for the
majority of this activity. The compressed multidimensional
report starts with the least speciﬁc clusters (such as arbi-
trary TCP traﬃc from servers using low ports to clients).
Note that the most speciﬁc cluster shown exactly identi-
ﬁes the particular transfer that consumed more than half of
the total bandwidth. Moreover, its unexpectedness score of
596%, promptly brings it to the attention of the network
administrator. Consequently, this cluster is also identiﬁed
in the delta section at the end of the report. The output of
AutoFocus also includes time-series plots of traﬃc as bytes
and packets colored by the appropriate categories on two
timescales:
short (two days – not shown here) and long
(eight days). The conspicuous spikes at each midnight are
periodic backups.
The AutoFocus prototype has another feature that proved
useful on a number of occasions: drilling down into indi-
vidual categories. For each of the categories, we provide
separate time series plots and reports that analyze the in-
ternal composition of the traﬃc mix within that particular
category.
5. EXPERIENCE WITH AUTOFOCUS
5.1 Comparison to unidimensional methods
We contrast our multidimensional method with unidimen-
sional analysis. Figure 6 presents a simpliﬁed version of
the time domain plot generated by AutoFocus for Friday
the 20th and Saturday the 21st of December 2002, while
Figure 7 and Figure 8 present two unidimensional plots (see
[5] for the unidimensional reports for the other ﬁelds). Be-
sides compactness, our multidimensional view has the ad-
vantage of making it easier to see very speciﬁc facts about
the network traﬃc. For example the light colored “spot”
between 7 AM and 2 PM on the ﬁrst day is UDP traf-
ﬁc that goes to a speciﬁc port of a speciﬁc multicast ad-
Total traffic is 354 GB. The threshold is 5%=17.7 GB.
Unidimensional reports
Protocol breakdown
 6(TCP)   98.653%  350 GB
Source IP breakdown
         137.131.0.0/16(Scripps)                                7.778%  27.6 GB
   192.128.0.0/10                                              66.450%  235 GB
                   192.172.226.64/26[Caida]                    61.371%  217 GB
                         192.172.226.89(magrathea.caida.org)   55.424%  196 GB
Destination IP breakdown
          132.249.0.0/17[SDSC]                                  5.506%  19.5 GB
          137.131.0.0/17[Scripps]                               5.529%  19.6 GB
          137.131.128.0/17[Scripps]                             5.574%  19.8 GB
 192.0.0.0/8                                                   60.937%  216 GB
                         192.67.21.154(hpss07.sdsc.edu)        55.362%  196 GB
 198.0.0.0/8                                                    6.651%  23.6 GB
Source port breakdown
 lowport   22.768%  80.8 GB
 80(http)  16.784%  59.6 GB
 highport  76.532%  271 GB
 4339      55.362%  196 GB
Destination port breakdown
 lowport    6.121%  21.7 GB
 highport  93.178%  330 GB
 35904     55.362%  196 GB
Multidimensional report
 Source IP                            Destination IP                  Pr.  Src port  Dst port  Traffic   Label
 *                                    *                               TCP  lowport   highport  80.4 GB   108.2%
 *                                    *                               TCP  80(http)  highport  59.6 GB   108.8%
 *                                    *                               TCP  highport  lowport   21.1 GB   128.7%
 *                                    *                               TCP  highport  highport  248 GB     99.4%
 *                                    132.249.0.0/17[SDSC]            TCP  *         highport  19.0 GB   105.7%
 *                                    137.131.0.0/16(Scripps)         TCP  80(http)  highport  18.6 GB   306.0%
 *                                    137.131.0.0/16(Scripps)         TCP  highport  *         18.1 GB    60.9%
 *                                    137.131.0.0/17[Scripps]         TCP  *         *         19.5 GB   100.6%
 *                                    137.131.128.0/17[Scripps]       TCP  *         *         19.6 GB   100.6%
 *                                    192.0.0.0/8                     *    *         highport  214 GB    106.4%
 *                                    192.0.0.0/8                     TCP  *         *         214 GB    100.8%
 *                                    198.0.0.0/8                     TCP  *         highport  22.2 GB   102.2%
 137.131.0.0/16(Scripps)              *                               TCP  *         highport  20.5 GB    80.7%
 192.172.226.0/24(SDSC NAP)           *                               TCP  highport  highport  214 GB    139.9%
 192.172.226.0/25(Caida)              *                               TCP  *         highport  221 GB    110.5%
 192.172.226.64/26[Caida]             *                               TCP  *         *         217 GB    101.4%
 192.172.226.89(magrathea.caida.org)  192.67.21.154(hpss07.sdsc.edu)  TCP  4339      35904     196 GB    596.7%
Delta report ------------ traffic changed from 213 GB to 354 GB, threshold is 5%=17.7 GB ------------
 *                                    137.131.0.0/17[Scripps]         TCP  highport  *         -20.6 GB
 134.79.0.0/18[SLAC.stanford]         137.131.0.0/16(Scripps)         TCP  highport  lowport   -19.6 GB
 192.172.226.3(ra.caida.org)          192.67.21.168(hpss45.sdsc.edu)  TCP  highport  highport  -30.5 GB
 192.172.226.89(magrathea.caida.org)  192.67.21.154(hpss07.sdsc.edu)  TCP  4339      35904     196 GB
Figure 5: The report for the 17th of December 2002 (one of the 31 daily reports for this trace) contains
compressed unidimensional reports on all 5 ﬁelds and the compressed multidimensional cluster report using
a threshold of 5% of the total traﬃc. In the unidimensional reports the percentages indicate the share of the
total traﬃc the given cluster has. In the multidimensional report they indicate the unexpectedness score.
Note how much smaller the delta report is than the full report.
dress. While we could manually correlate the corresponding
“bright spots” from the protocol, destination preﬁx and des-
tination port plots, the AutoFocus plot automatically iden-
tiﬁes the key usage directly. The massive spike causing the
traﬃc surge from 12:01 AM until 3 AM the ﬁrst day and
the longer dark traﬃc cluster from 1 AM until 11 AM that
day and 1 AM and to 6 AM the second day are two diﬀerent
types of backups. It would be diﬃcult to disentangle them
using only unidimensional plots. Since they use diﬀerent
source ports (SSH versus high ports) they show up sepa-
rately in the source port report, but since they come from
the same source network and go to the same destination
network they show up together in those plots.
5.2 Experience with analysis of trafﬁc traces
This section presents highlights of our experience using
AutoFocus to analyze traﬃc traces from three large produc-
tion networks. While the usefulness of the insights gleaned
four categories: Web traﬃc from a particular server at the
Scripps Institute, Web traﬃc destined for clients within the
Scripps /16 (Web client traﬃc), other traﬃc from port 80
and traﬃc to port 80. Of these, the ﬁrst and second cate-
gories proved to be the most interesting. Traﬃc from the
Web server showed a clear diurnal pattern with peaks be-
fore noon (sometimes a second peak after noon) and lows
after midnight. It also showed a clear weekly pattern with
lower traﬃc on the weekends and holidays. The second cat-
egory (the Web clients) had similar trends, but the lows
around midnight usually went down to zero and the diﬀer-
ence between weekend and weekday peaks was much larger.
In retrospect, this is to be expected since the client traﬃc
requires the physical presence of people at Scripps, while the
server traﬃc can be driven by requests from home machines
or users outside the institution.
Another interesting traﬃc cluster contained Web proxy
traﬃc originating from port 3128 of a particular server at
NLANR. The amount of traﬃc had a daily and weekly cycle,
but the lowest traﬃc was at noon and the highest at mid-
night. Using AutoFocus’ drill-down feature we were able to
examine the breakdown of this traﬃc, which identiﬁed large
clusters containing transfers to second-level caches from Tai-
wan, Indonesia, Spain and Hong Kong. Evidently the traﬃc
was driven by the daily cycle of the clients in these other
time zones.
AutoFocus places the remaining non-categorized traﬃc
into an amalgamated “Other” category. On the last day
of the trace, we saw a huge sharp increase at 5:30 PM in the
“Other” traﬃc that saturated the link followed by a sudden