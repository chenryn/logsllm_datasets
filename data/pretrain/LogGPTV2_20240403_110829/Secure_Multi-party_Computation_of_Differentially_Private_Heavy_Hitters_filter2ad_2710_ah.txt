data with a small absolute differences NCRâˆ’F1=0.04 leading to a
large relative difference of 40% (last row in Table 5).
Data
Zipf
Retail
ğ‘˜
4
8
16
4
8
16
HH
2.6%
4.7%
4.0%
1.1%
3.6%
4.1%
PEM
(ğœ‚ = 4)
1.6%
1.4%
0.6%
-2.1%
-0.6%
0.0%
PEM
(ğœ‚ = 5)
2.0%
2.3%
2.0%
-0.5%
1.5%
1.2%
PEMorig
14.8%
16.7%
20.0%
6.1%
9.2%
48.0%
Table 3: Relative comparison (NCRâˆ’F1)/NCR, varying ğ‘˜ âˆˆ
{4, 8, 16} for ğ‘› = 1, 000 averaged over ğœ– âˆˆ {0.1, 0.25, 0.5, 1, 2}.
Data
Zipf
Retail
ğ‘˜
4
8
16
4
8
16
HH
0.9%
7.0%
5.7%
1.5%
4.9%
5.0%
PEM
(ğœ‚ = 4)
2.0%
3.4%
5.2%
5.3%
3.5%
7.2%
PEM
(ğœ‚ = 5)
1.4%
5.6%
5.0%
10.9%
1.0%
5.4%
PEMorig
25.0%
25.1%
47.1%
3.6%
15.8%
8.1%
Table 4: Relative comparison (NCRâˆ’F1)/NCR, varying ğ‘˜ âˆˆ
{4, 8, 16} for ğ‘› = 5, 000 averaged over ğœ– âˆˆ {0.1, 0.25, 0.5, 1, 2}.
Data
Zipf
Retail
ğœ–
0.1
0.25
0.5
1
2
0.1
0.25
0.5
1
2
HH
0.0%
4.3%
3.0%
5.9%
7.0%
0.0%
0.0%
2.9%
9.0%
8.6%
PEM
(ğœ‚ = 4)
0.0%
0.0%
0.0%
0.0%
3.0%
0.0%
0.0%
0.0%
0.0%
0.0%
PEM
(ğœ‚ = 5)
0.0%
0.0%
0.0%
4.3%
5.9%
0.0%
0.0%
0.0%
0.0%
6.0%
PEMorig
0.0%
0.0%
0.0%
50.0%
50.0%
0.0%
0.0%
100.0%
100.0%
40.0%
Table 5: Relative comparison (NCRâˆ’F1)/NCR, varying ğœ– âˆˆ
{0.1, 0.25, 0.5, 1, 2} for ğ‘˜ = 16, ğ‘› = 1, 000.
K MPC FRAMEWORKS
We deployed SCALE-MAMBA [4] version 1.3 and MP-SPDZ [53]
version 0.1.8 in our evaluation. Here, we evaluated HH, HHthreads
without the final sorting step.
SCALE-MAMBA: Version 1.3 vs. 1.9: Out-of-the-box, i.e., with-
out adjusting options and runtime switches, SCALE-MAMBA ver-
sion 1.3 was faster than (at time of evaluation most recent release)
1.9 for our protocols. Versions 1.4 to 1.9 mainly added features which
our protocols do not rely on (e.g., support for Garbled Circuits, au-
thenticated bits). We used runtime switch -dOT from version 1.9,
to reduce offline data creation (for features we are not using), for a
4816100200300400kMBsÎ·=5Î·=4Î·=348162004006008001,000kMBs48162505007501,0001,2501,5001,750kMBsÎ·=4Î·=3Î·=21Â·1052Â·1055Â·10500.20.40.60.81ğ‘›NCRHHPEM(ğœ‚=5)PEM(ğœ‚=4)PEMorig(ğœ‚=5)PEMorig(ğœ‚=4)1Â·1052Â·1055Â·10500.20.40.60.81nNCRSession 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 2376Data
Zipf
Retail
ğœ–
0.1
0.25
0.5
1
2
0.1
0.25
0.5
1
2
HH
3.0%
7.0%
7.0%
5.9%
5.9%
0.0%
3.0%
5.4%
8.3%
8.2%
PEM
(ğœ‚ = 4)
0.0%
4.3%
5.7%
8.2%
7.5%
0.0%
6.3%
8.1%
7.1%
14.8%
PEM
(ğœ‚ = 5)
0.0%
3.8%
5.3%
8.7%
7.4%
0.0%
3.6%
7.2%
7.6%
8.3%
PEMorig
0.0%
100.0%
45.5%
46.7%
43.3%
0.0%
0.0%
0.0%
0.0%
40.6%
Table 6: Relative comparison (NCRâˆ’F1)/NCR, varying ğœ– âˆˆ
{0.1, 0.25, 0.5, 1, 2} for ğ‘˜ = 16, ğ‘› = 5, 000.
fairer comparison with 1.3. Still, in our brief evaluation, we found
1.9 to be somewhat slower:
â€¢ For PEM with ğ‘˜ = 8, ğœ‚ = 2, ğ‘ = 32 runtime increased by
around 20% from 1.3 to 1.9 (â‰ˆ206 vs. 248 s).
Without -dOT communication almost doubled (â‰ˆ237 vs. 460 MB),
with -dOT it remained about the same.
â€¢ For HHthreads with ğ‘˜ = 16 runtime increased by around 10%
from 1.3 to 1.9 (â‰ˆ600 vs. 667 s).
Without -dOT communication increased by around 30% (â‰ˆ5.5
vs. 7.2 GB) with -dOT it remained about the same.
MP-SPDZ: Semi-honest vs. Malicious: MP-SPDZ supports
semi-honest as well as malicious security for multiple secure com-
putation paradigms (e.g,. Shamir secret sharing, BMR) [53], whereas
SCALE-MAMBA only supports malicious security. In Section 5 we
evaluated semi-honest MP-SPDZ. Next, we briefly compare SCALE-
MAMBA and MP-SPDZ for maliciously secure Shamir:
â€¢ For PEM with ğ‘˜ = 16, ğœ‚ = 4, ğ‘ = 32, MP-SPDZ is more than
twice as fast than SCALE-MAMBA (â‰ˆ14 vs. 30 minutes) with
around 400 MB less communication (â‰ˆ1.47 vs. 1.88 GB).
â€¢ For HH with ğ‘˜ = 16 and ğ‘›ğ‘ = 30 per party ğ‘ âˆˆ {1, 2, 3}, MP-
SPDZ is roughly 27% slower than SCALE-MAMBA (â‰ˆ6 vs. 4.7 min-
utes), but requires around 60% less communication (â‰ˆ192
vs. 313 MB).
This suggests that, for malicious security and considering only
running time, PEM is more efficient with MP-SPDZ, whereas HH
is more efficient with SCALE-MAMBA.
Session 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 2377