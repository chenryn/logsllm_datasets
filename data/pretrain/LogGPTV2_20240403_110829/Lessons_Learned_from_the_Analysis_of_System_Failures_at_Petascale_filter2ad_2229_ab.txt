### Application and Node Management

When applications are running on affected blades, they are either terminated or, in the case of a warm swap-out, allowed to complete. If one of the agents becomes inactive or miscommunicates, the HSS (High-Speed Switch) manager changes the node state to "suspect" or "down." Any job currently running on the node, if not already failed, is permitted to complete.

### System Software

Compute and GPU nodes run a lightweight kernel called Compute Node Linux (CNL), developed by Cray. The operating system is streamlined to minimize overhead and includes only essential components such as a process loader, a Virtual Memory Manager, and a set of Cray ALPS (Adaptive Lightweight Placement Scheduler) agents for job loading and control. Service nodes, on the other hand, use a full-featured version of Linux known as the Cray Linux Environment (CLE), which is based on SUSE Linux Enterprise Server 11 with kernel version 3.0.42.

### Job Scheduling

Jobs are managed through a suite of cooperative software:
1. **Cray ALPS** (Application Level Placement Scheduler) for the placement, launch, and monitoring of all applications within a single job.
2. **Moab and TORQUE** for resource management and scheduling decisions. Jobs are assigned at the granularity of the node.

### File System Features

All blades are diskless and rely on a shared parallel file system for I/O operations. Blue Waters hosts the largest Lustre installation to date, consisting of a parallel file system that manages data stored in Cray Sonexion 1600 storage modules. Each Sonexion module is configured as follows:
- **2 SSDs of 2 TB** in a RAID 1 configuration for journaling and logging.
- **22 disks of 2 TB** for metadata storage.
- **80 disks of 2 TB** for data storage, organized in units of 8 disks in a RAID 6 configuration.
- All disks are connected to two redundant RAID controllers, with two additional disks serving as hot spares for automatic failover.

Data access from the nodes is transparent via the Lustre service nodes (Lnet), which interface with the Sonexion storage modules. Blue Waters provides three file systems: project, scratch, and home, offering up to 26 PB of usable storage over 36 PB of raw disk space. Each Lustre service node and Sonexion module is configured as an active-passive pair connected to a shared storage device. In the event of a failure detected by the HSS, the passive replica node becomes active, and the shared storage device is mounted in read-only mode to prevent data inconsistency until the failed node is replaced. After recovery, clients reconnect and replay their requests serially to reconstruct the state on the replaced node. If a timeout is reached, all waiting jobs fail. The recovery process, including the boot-up of the warm standby and state reconstruction, can take between 5 to 30 minutes (or up to 60 minutes for MDS), depending on the number of clients using the file system at the time of the failure.

### Workload

Blue Waters processes large-scale scientific simulations, including molecular dynamics, weather forecasting (e.g., hurricanes and tornadoes), supernovae, galaxy formation, earthquakes, and fluid dynamics. The system can deliver over 1 PF/s of sustained performance, and jobs can utilize compute nodes, GPU nodes, or both. Since the start of production in March 2013, Blue Waters has processed more than 190,408 distinct jobs, requiring a total of 254,925 billion node hours. The average load (utilization) in terms of used nodes, computed over 24 hours for the measured period, is 71%.

### Data and Methodology

This study is based on the analysis of manual reports generated by Blue Waters system maintenance engineers over 261 days, from January 3, 2013, to November 17, 2013. Additionally, we use automatically collected event data logs to provide an in-depth understanding of specific failures, such as uncorrectable machine checks (e.g., hardware exceptions caused by parity or ECC errors).

#### Failure Reports

System failure reports are human-written documents that record events requiring the attention of system managers, such as failures, maintenance, system upgrades, and bugs reported to the manufacturer. Each entry includes several fields:
- Start time of the event
- Repair time and date
- Event category
- Affected facility
- Textual description of the event
- Number of offline nodes due to the event
- System-Wide Outage (SWO) flag
- Field Replaced Unit (FRU)

Over 1,978 distinct entries (including scheduled maintenance, system expansions, and failure events) were reported during the 261-day measurement interval. Table II shows a sample of these reports.

#### Failure Categories

Entries in the failure reports are classified into the following categories:
1. **Failure (No Interrupt)**: Failures naturally tolerated by the architecture, causing no downtime or failover actions.
2. **Interrupt (Failover)**: Critical problems successfully handled by automatic failover mechanisms.
3. **Link and Node Failure (Job Failed)**: Failures of one or more nodes and links causing job failures.
4. **Link Failure (No Job Failed)**: Single or multiple node failures causing link issues, successfully handled by automatic network failover.
5. **Link Failure (Job Failed)**: Link failures causing job loss.
6. **Single/Multiple Node Failure**: Failures requiring repair but not impacting core system functionalities.
7. **Interruption (System-Wide Outage)**: Unavailability of the entire system, e.g., due to system-wide repairs or restarts.

#### Characterization Methodology

In characterizing failures from the manual reports, we:
1. Associate failures with root cause categories (hardware, software, network, environment, heartbeat/node down, unknown).
2. Measure failure rates and repair times across these categories.
3. Evaluate how hardware and software failures evolve over the measurement window.
4. Assess hardware error resiliency.
5. Analyze system-wide failure and repair time distributions and their statistical properties.

Metrics include relative failure frequency, mean, and standard deviation. We also measure memory and processor error resiliency using machine check data, computing failure rates in FITs (failures in 10^9 hours of operation) per GB of memory, MTBF (Mean Time Between Failures), and AFR (Annualized Failure Rate) for various components.

### Blue Waters Failure Causes

#### Breakdown of Reported Failures

Table III provides a breakdown of the failure reports, MTBF, and MTTR across the defined failure categories. In the 261-day period, the system experienced 1,490 failures, of which 1,451 (97.4%) were tolerated and 39 (2.6%) resulted in system-wide outages.

Key observations:
- On average, there was a failure every 4.2 hours, while system-wide outages occurred approximately every 160 hours.
- 58.3% of failures (category 6) resulted in single/multiple node failures, occurring every 4.2 hours and repaired on average in 32 hours.
- 25.7% of failures (categories 2 and 4) were severe events recovered through system-level failover without job failures.
- 11% of failures (category 1) were noncritical events tolerated by the system's redundant design, occurring every 35.12 hours.

Software failures contributed to 53% of the node downtime hours. Figure 2(a) shows the distribution of hardware, software, network, heartbeat, and environment root causes across the failure categories. Hardware root causes were predominant in single/multiple node failures, occurring 442 (51%) times out of 868 documented failures and constituting 42% of all failures. Software root causes, however, despite representing only 20% of total failures, accounted for 53% of the total node repair hours.

### Conclusion

The analysis of Blue Waters' failure reports and machine check data provides valuable insights into the system's reliability and resilience. While hardware failures are more frequent, software failures have a more significant impact on repair times. This highlights the importance of robust software management and maintenance practices in high-performance computing environments.