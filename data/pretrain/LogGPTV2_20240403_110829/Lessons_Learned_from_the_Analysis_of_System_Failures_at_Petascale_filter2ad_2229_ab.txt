applications running on the affected blades are either killed or,
in the case of a warm swap-out, allowed to complete. In the
case of inactivity or miscommunication of one of the agents,
the HSS manager switches node state to ”suspect” or ”down”.
The job currently running on the node, if not failed, is allowed
to complete.
System Software. Compute and GPU nodes execute the
lightweight kernel Compute Node Linux (CNL) developed by
Cray. The operating system is reduced to a bare minimum to
minimize the overhead on the nodes and includes only essential
components, such as a process loader, a Virtual Memory
Manager, and a set of Cray ALPS agents for loading and
controlling jobs. Service nodes execute a full-featured version
of Linux, the Cray Linux Environment (CLE), which is based
on the Suse Linux Enterprise Server 11 kernel 3.0.42.
Jobs are scheduled through a suite of cooperating software
that includes i) Cray ALPS (the Application Level Placement
Scheduler) [13] for the placement, launch, and monitoring of
all
the applications composing a single job, and ii) Moab
and TORQUE [14] for resource management and scheduling
decisions. Jobs are assigned at the granularity of the node.
File System Features. All blades are diskless and use the
shared parallel ﬁle system for IO operations. Blue Waters hosts
the largest Lustre installation to date. It consists of parallel
ﬁle system used to manage data stored in Cray Sonexion
1600 [15] storage modules. Each Sonexion module has i) 2
SSD of 2 TB in a RAID 1 conﬁguration for journaling and
logging, ii) 22 disks of 2 TB for metadata storage, and iii)
80 disks of 2 TB for data storage, organized in units of 8
disks in RAID 6. All disks are connected to two redundant
RAID controllers. In each unit, two additional disks serve as
hot spares, which automatically provide failover for a failed
drive. Data are accessed transparently from the nodes via the
Lustre service nodes (Lnet), which interface with the Sonexion
storage modules. Blue Waters includes three ﬁle systems i.e.,
project, scratch, and home, and provides up to 26 PB of
usable storage over 36 PB of raw disk space. Each Lustre
service node and Sonexion module is conﬁgured as an active-
passive pair connected to a shared storage device. In this
conﬁguration, the passive replica node becomes active when
the HSS detects a failure of a Lustre node; the shared storage
device is then mounted in a read-only mode to avoid data
inconsistency until the failed node has been replaced with the
standby replica. After failure recovery, clients reconnect and
replay their requests serially in order to reconstruct the state on
the replaced node. Until a client has received a conﬁrmation
that a given transaction has been written to stable storage, the
client holds on to the transaction (waiting on a timeout), in case
it needs to be replayed. If the timeout is reached, all the jobs
waiting to reconnect fail. The recovery process (i.e., the boot-
up of the warm standby and the reconstruction of the state lost
in the failure) may take 5–30 minutes (60 for MDS), depending
on the number of clients using the ﬁle system at the moment
of the failure.
Workload. The workload processed by Blue Waters consists
of large-scale scientiﬁc simulations, including molecular dy-
namics, hurricanes and tornadoes (the Weather Research and
Forecasting run on Blue Waters is the largest WRF simula-
End Date
Category
Start Date
TABLE II: Example of Blue Waters Incident Report. Showed entries include only a subset of ﬁelds used in this study
Failed
Nodes
1
26563
0
26534
0
Sonexion SCSI and SAS errors on snx1003n181 - crashed and automatic failure failed
Sonexion snx11003n354 Slot 47 Disk Failure
c5-8c0s6 (rq11520016) node failure made the HSN non-routable
c2-8c1s3n3(RQ12110251) Memory Error
Sonexion snx11003n[150-151] Warning Fan at speed 0 RPM.
Gemini
Blade failed. Replaced, warm swapped then there was a throttle and routes hung 26573
Single/Multiple Node
Interrupt
3/6/13 7:38
3/7/13 17:00
3/11/13 21:11 3/12/13 0:21
3/28/13 2:52
4/22/13 7:28
4/29/13 16:28 4/29/13 16:49 Failure (No Impact)
5/30/13 4:50
3/29/13 14:52 Failure (No Interrupt)
4/22/13 18:08 Interrupt
5/30/13 9:48
Facility
Subject
Interrupt
SWO FRU
DIMM
Disk
Comp. Blade
Fan Tray
0
1
0
1
0
1
tion ever documented), supernovae, the formation of galaxies,
earthquakes, and ﬂuid dynamics. Blue Waters is capable of
delivering above 1 PF/s of sustained performance and jobs may
use compute nodes, GPU nodes, or both. Since the beginning
of the production phase (March 2013), Blue Waters processed
more than 190,408 distinct jobs that required a total of 254,925
billion node hours. The average load (utilization) in terms of
used nodes, computed over 24h for the measured period is 71%.
III. DATA AND METHODOLOGY
This study is based on analysis of manual reports generated
by Blue Waters system maintainance engineers over a period
of 261 days, from January 3, 2013 to November 17, 2013.
In addition, we use automatically collected event data logs to
provide an in-depth understanding of speciﬁc failures, such as
uncorrectable machine checks, e.g., hardware exceptions caused
by parity or ECC errors.
Failure Reports. System failure reports are human-written
documents that record each event in the system that required the
attention of the system managers, such as failures, maintenance,
system upgrades, and bugs signaled to the manufacturer. Each
entry contains several distinct ﬁelds, including i) start time of
the event, ii) repair time and date, iii) category of the event,
iv) affected facility, v) textual description of the event, vi)
number of nodes ofﬂine because of the event, vii) System-Wide
Outage (SWO) ﬂag, and viii) Field Replaced Unit (FRU). Over
1,978 distinct entries (including scheduled maintenance, system
expansions, and failure events) were been reported during the
measurement interval of 261 days. Table II shows a snippet of
the reports.
Failure Categories. Entries in the failure reports are pro-
duced upon arrival of alerts from automated detection mecha-
nisms and failure-reporting tools. The Blue Waters maintenance
specialists classify each entry added to the failure report using
the below categories.
1) Failure (No Interrupt): failures that are naturally tolerated
by the architecture, i.e., do not cause node/system down-
time or trigger failover actions (e.g., cooling hardware or
performance problems).
2) Interrupt (Failover): a critical problem that is successfully
handled by the automatic failover mechanisms.
3) Link and Node Failure (Job Failed): the failure of one or
more nodes and one or more links that cause the failure
of one or more user jobs.
4) Link Failure (No Job Failed): Failures of a single or mul-
tiple node(s) that cause a link failure that is successfully
handled by the automatic network failover mechanisms;
5) Link Failure (Job Failed): Link failures that cause job loss.
6) Single/Multiple Node Failure: Failures of single or multi-
ple node(s) that do require repair, but do not impact core
system functionalities.
7) Interruption (System-Wide Outage): The whole system
is unavailable, e.g., because of a system-wide repair or
restart. A system-wide outage occurs if speciﬁc require-
ments cannot be met, such as: i) the ability to access all
data blocks of all ﬁles in the ﬁle system; ii) user ability to
log in; iii) full interconnection between nodes; iv) access to
external storage server (esDM, or external data mover); v)
ability to support user applications submission, scheduling,
launch, and/or completion; vi) ability to recover from ﬁle
system or network failures through automated failover
operations; and vii) performance (e.g., network bandwidth
or ﬁle system throughput) above acceptable levels.
Human-generated reports present several challenges. They
contain textual descriptions in natural language and cannot be
readily analyzed by automatic tools. Failure reports must be
ﬁltered from reports on non-failure events in order to avoid
biasing the analysis. Therefore, a ﬁrst step of the analysis
consists of purging the non-failure entries and reorganizing the
content into a structured database. We (in close partnership
with NCSA and Cray engineers) manually reviewed each
data record in the failure reports and combined redundant or
overlapping records. Finally, manual failure reports may suffer
from misdetection and underreporting of noncritical failures.
We extensively investigated the potential for underreporting,
and concluded that the incidence of failure undereporting is
negligible. The reasons are that each manual report is based
on an automatic failure/alert trigger provided by the HSS’s
extensive system-logging mechanisms (see Section II) and the
technical staff of Blue Waters is meticulous about recordkeep-
ing. After ﬁxing the problem, the staff update the error report
with the identiﬁed cause of the event, the ﬁxes applied, and,
where applicable, the ﬁeld replaced unit.
Marchine Check Data. Machine checks are errors detected
by the machine check architecture in the northbridge of the
processor. Machine check errors may be related to several
causes, such as hard or soft memory errors (e.g., bit ﬂips) as
well as processor or memory voltage ripple or motherboard
problems. Machine check data include a timestamp, the ID of
the node that experienced the machine check, and information
on the type (e.g., correctable/uncorrectable or detected by
the hardware scrubber), physical address, error syndrome, and
involved operation (e.g., reading from the memory, or loading
the L2 data cache) encoded into a 64-bit word in the logged data
obtained after fetching the content machine check status register
[11]. In the case of an uncorrectable error, the operating system
is conﬁgured to panic. Consequently, the event is detected by
the system console, which switches the status of the node from
up to down.
Failure root causes identiﬁed from the reports are classiﬁed
into the following exclusive categories: hardware, software,
missing heartbeat, network, environment, and unknown (i.e.,
failures for which the cause could not be determined). We
consider this a separate category of failures detected by the
system but automatically recovered from before they can be
diagnosed (see discussion in the Section IV-A.
The assignment of an outage to the hardware or software
category is guided by the type of corrective action (e.g.,
replacement using a spare part, or installation of software
patches) and based on interactions with system administrators.
After assigning categories, we reduced the number of reports
from 1,978 to 1,490 entries, which include only failures events.
A. Characterization Methodology
In characterizing failures from the manual failure reports,
we i) associate failures with the corresponding root cause cat-
egories, i.e., hardware, software, network, environment, heart-
beat/node down, and unknown, and we measure the failure rate
and repair times across them; ii) determine how the hardware
and software failures evolve in the considered measurement
window; iii) evaluate the hardware error resiliency; and iv)
evaluate system-wide failure and repair time distributions and
their statistical properties. Computed metrics include the rela-
tive failure frequency, mean, and standard deviation. In addition,
the density distributions of the TBF and relative frequency
of root causes are evaluated. As part of the study, we also
measure the error resiliency of memory and processors using
the machine check data. Speciﬁcally, we compute i) failure rate
in FITs (failures in 109 hours of operation) per GB of memory
for RAM and GPU accelerator on-board memory; ii) mean time
between failures (MTBF) and annualized failure rate (AFR) for
the processor, memory, GPU accelerator, disks, and SSDs; and
iii) probability of uncorrectable errors per GB of memory, for
both GPU accelerator and memory errors.
The analysis of the data employed in this paper required
us to develop several techniques and tools i) to handle the
large amount of textual data (e.g., 3.7TB of system logs for
the considered period), ii) to extract and decode speciﬁc types
of system events (e.g., the 64-bit machine check status register
for all the logged machine checks), and iii) to harmonize and
mine the manual failure reports. Details of the developed toolset
are deliberately not included in the paper because of both space
limitation and the focus on measurements.
IV. BLUE WATERS FAILURE CAUSES
In this section, we analyze the data and information in the
failure reports addressing how often Blue Waters fails and how
much time is needed to ﬁx the causes of a failure. We report on
i) the distribution of failures’ root causes across all the failure
categories deﬁned in Section III; and ii) the root causes of the
failures, as identiﬁed by the Cray and Blue Waters maintenance
specialists.
A. Breakdown of Reported Failures
Table III provides a breakdown of the failure reports, MTBF,
and MTTR across the deﬁned failure categories. In the mea-
sured period of 261 days, the system experienced 1,490 failures,
of which 1,451 (97.4%) were tolerated and 39 (2.6%) resulted
in system-wide outages (analyzed in Section VI)1. Key obser-
1The breakdown of Blue Waters failures and number of SWOs reﬂects NCSA
view of the system and may not correspond to that provided by Cray.
TABLE III: Failure Statistics. The last row refers to the statistics calculated
across all the failure categories.
Failure Category
count % MTBF
MTTR
[h]
13.5
14.7
6.1
32.7
16
26.7
5.16
34.5
 T BF
[h]
70.8
92
427.3
51.9
444
6.3
174.2
13.3
 T T R
[h]
35.3
42.2
5.4
91.2
26.7
72
8.1
50.5
1) Failure (No Interrupt)
164
2) Interrupt (Failover)
99
3) Link & Node Failure (Job Failed)
19
4) Link Failure (No Job Failed)
285
5) Link Failure (Job Failed)
19
6) Single/Multiple Node Failure
868
7) Interruption (system-wide outage) 39
ALL
vations are:
[h]
35.17
11%
58
6.6%
1.3%
297.7
19.1% 19.9
1.3%
291.6
58.2% 6.7
2.62% 159.2
1490 100% 4.2
• On average, there was a failure (across all categories)
every 4.2 h, while the system suffered system-wide outages
approximately every 160 hours.
• 58.3% of failures resulted in single/multiple node failures
(category 6 in Table II) that caused node unavailability.
Such failures were the most common, occurring every 4.2
h; they were allowed to accumulate in order to optimize
the cost of ﬁeld intervention, and they were repaired, on
average, in 32 hours.
• About one-fourth (25.7%, categories 2 and 4) of failures
were potentially severe events from which the system
recovered by means of system-level failover (see Section
II) without job failures. Only 2.6% caused job failures
(categories 3 and 5) without resulting in a SWO.
• 11% of failures (category 1) consisted of noncritical events
that were tolerated by the redundant design of Blue Waters
without requiring any automatic failover operation. Such
events included, for instance, failures of (redundant) power
supplies in the blades, failures of cooling hardware (fan
trays or water cooling valves), job-scheduling performance
problems, and resource manager crashes. Such failures
caused no machine downtime and little or no unavailability
of software services (e.g., the job scheduler). They oc-
curred on average every 35.12 h, and their inter-arrival
times showed high variance due to the heterogeneity of
the root.
Software failures contributed to 53% of the node downtime
hours. Figure 2.(a) shows how hardware, software, network,
heartbeat, and environment root causes are distributed across
the failure categories given in Table III. As seen in other
systems [4],
failures with hardware root causes were the
predominant cause of single/multiple node failures. They oc-
curred 442 (51%) times over 868 single/multiple node failures
documented in the failure reports, and constituted 42% of the
total number of failures across all the categories (rightmost
bar in Figure 2.(a)). Conversely, failures with software root
causes represented 20% of the total number of failures and
only 7.9% of the single/multiple node failures. However, an
interesting conclusion can be drawn from the relative impact
that hardware and software causes have on the total number of
node repair hours (hours required to repair failures due to the
same root cause, multiplied by the number of nodes involved in
the failure) shown in Figure 2.(b). The key observation is that
failures with software root causes were responsible for 53% of
the total node repair hours, although they constituted only 20%
of the total number of failures. Hardware root causes, however,
despite causing 42% of all failures, resulted in only 23% of the
total repair time. As we shall see, hardware problems are well
Environment  Hardware  Heartbeat/Node Down  Software  Network Links  Unknown 
110% 
100% 
90% 
80% 
70% 
60% 
50% 
40% 
30% 
20% 
10% 
0% 
Software 
69 
Hardware 
61 