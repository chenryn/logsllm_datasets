following the TTR distribution, or with a probability 1 ‚àí r
only after the recovery of the node which propagated the
failure. The distribution of the TTR will be selected de-
pending on the TTF action that caused the failure. In both
cases, the place signaturePropagateIORecovery is used by the
node/subsystem that initiated the failure to communicate to
involved nodes/subsystems that it recovered. To this aim, this
place will contain the failureSignature of the node/subsystem,
which started the failure, and nodes/subsystems failed due
to the same failureSignature will consequently recover after
a time correlatedTTR. In all the mentioned cases, after a
recovery, the output gate WRITE ORACLE in zone 1 will be
executed and an entry for the oracle log will be prepared.
When all
the subsystems and nodes involved in a failure
recover (kept track by the ExternalLib via the signature),
an entry in the oracle is Ô¨Ånally written. It will contain the start
time of the failure, the originating node, the type of failure (i.e.,
with or without propagation, with the indication of affected
nodes and subsystems), the number of error events generated
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:19:29 UTC from IEEE Xplore.  Restrictions apply. 
Failure Start Time
11/22/11 19:06:39
11/22/11 19:07:02
12/17/11 23:22:39
NodeID
191
161
297
Type of Failure
IO_PROPAGATION(192,212,195)+SW(195)
NET
PROC+MEMORY
Events
112
9
20
Duration
124.332
154.8
143.85
Signature
75829788
35924727
71162829
TABLE III: Example of Oracle log
TABLE IV: Example of
schedule_long
code from the Output Gate
Output Gate schedule_long
ExternalLib* l = ExternalLib::Instance();
jobs* nextJob = l->do_schedule(_LONG);
LongQueue::iterator node;
// It cycles on the Free Nodes of the Long Queue
...
if(!(ExternalLib::isBusy(node))) {
l->setBusy(myjob->getJobID(),node);
myjob->addNode(node); }
...
l->enqueueRunning(myjob);
1
2
3
4
5
6
7
8
9
10
(and written in the synthetic log), the failure duration, and the
failure signature. Table III shows an extract from the oracle
log corresponding to the synthetic log shown in Table II. From
the oracle, it can be seen that IO error entries for nodes 192,
212, and 195 are due to the same failure generated by node
191. In addition, the software error written for node 195 is the
result of a propagation of the same failure of the IO subsystem.
Conversely, the failure of node 161 is independent from the
previous one, despite it overlaps in time.
2) The Workload Model: Figure 5 shows the SAN of the
workload model. It mimics the inter-arrival of jobs and the be-
havior of a job scheduler, which assign jobs to nodes according
to FIFO with backÔ¨Åll scheduling queues2. In particular, three
different queues are considered, depending on the type of jobs:
i) Long queue, for long lasting cpu intensive jobs, ii) the IO
queue, for IO bound jobs, and iii) the Default Queue, for mixed
jobs (cpu and IO). Recall that nodes running different job
types may experience different failures. This way, we make it
possible to specialize at runtime the type of workload run by
a node, and hence its failure behavior.
The model is composed by 3 main zones, as depicted in the
Figure. The Ô¨Årst models the job inter-arrival, the second the job
scheduling, and the third the job completion. Also this model
takes advantage of the external library, which implements the
scheduling policy.
2www.adaptivecomputing.com/resources/docs/mwm/6-0/8.2backÔ¨Åll.php





	

Fig. 5: The SAN of the workload model.
The job arrival process has been modeled by means of three
actions Default job, IO job and Long job enabled by the input
gate EN (Figure 5), only if the correspondent queue is not full.
Once a job is arrived in the system, a JOB object is created
by the external
library, containing Ô¨Åelds such as JobID
identifying the job univocally the arrived job, Type coding
the scheduling queue of destination, duration containing
the duration of the job, WC specifying the job wall clock time,
numtask specifying the number of required processors, and a
vector runNodes, initially empty, Ô¨Ålled during the scheduling
with the ID of the node(s) in charge of executing the job.
The dispatching of the arrived jobs to the speciÔ¨Åc scheduling
queue is performed by the output gate dispatcher, which is
in charge of pushing the created JOB object in the waiting
queues managed by the external library.
The scheduling is executed by the output gate in the job
scheduling zone. Table IV shows an extract of the code
for scheduling a waiting job in the long queue. It is worth
noting that
the scheduling is achieved by means of the
do_schedule() method of the library, hence allowing
to simulate different scheduling policies without changing
the model (for instance, overriding the do_schedule()
function).The function setBusy() is in charge of loading the
job to the selected node. The function enqueueRunning()
of the library is called to Ô¨Ånally run the job on the selected
nodes. As for the jobs completion, the library keeps track of
the duration of rub jobs in a list ordered by completing times,
and modify the Ô¨Åring times of the actions finish_LONG,
finish_DEFAULT and finish_IO consequently.
B. The Analyzer
The analyzer is in charge of coalescing the generated
synthetic logs with one or more coalescence techniques and
of comparing the obtained results against the oracle log. To
this aim, we implement a set of Perl scripts in charge of: i)
processing the oracle log to extract the ground truth (number
of generated failures, real MTBF, real MTTR, etc.), ii) running
the target coalescence techniques on synthetic logs to extract
the presumed reality (number of tuples, estimated MTBF and
MTTR, etc.), iii) comparing the presumed reality with the
ground truth to evaluate estimation errors, and the number
of truncations and collisions, and iv) conducting detailed
analysis on coalescence techniques to estimate their sensitivity
to changing parameters, such as the time window.
The analyzer has been implemented with the objective to
automate the analysis on a set of generated logs, each one
accounting for different system characteristics (number of
nodes, type of workload, type and intensity of failures, etc.).
In addition, we have adopted a modular design to simplify the
introduction of novel coalescence techniques to be assessed.
V. EXPERIMENTAL EVALUATION
In this section, we report the results of the assessment of
four time coalescence techniques by means of the proposed
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:19:29 UTC from IEEE Xplore.  Restrictions apply. 
TABLE V: Event Logs Coalescence Algorithms. T(x) is the time
stamp of event x; Type(x) is the type of event x; ‚Äùnext‚Äù and ‚Äùcurr‚Äù
represent the next and current events in the log, respectively;
‚ÄùÔ¨Årst‚Äù represent the Ô¨Årst event in the current tuple.
 create a new tuple; } 
 add next to the current tuple; 
ALGORITHM 1. Sliding Window  
sliding() { 
 W = window size; 
1.
 foreach event in the log {  
2.
    if (T(next)-T(curr)  C)  
11.
12.        else create a new tuple; 
13.     else create a new tuple;} 
14. } 
   elseif (D  1 task, long queue
Probability > 1 task, default queue
Probability > 1 task, IO queue
Value
Weibull(0.881,1212)
Weibull(0.931, 9408)
Exp(1.81E-3)
Lognorm(4.84;1.584)
Lognorm(6.584;2.302)
Lognorm(6.178;1.063)
Lognorm(10.381;0.811)
Lognorm(6.7615;2.285)
Lognorm(7.525;1.702)
Lognorm(12.186;0.346)
Lognorm(1.633;0.581)
Lognorm(1.335;0.693)
Lognorm(1.014;0.498)
0.602
0.272
0.533
Workload distributions have been set with reference to the
load of the SCOPE supercomputer [35]3, manufactured by Dell
in 2008 at University of Naples Federico II. It is constituted
of 512 Dell Blade servers equipped with 2 quad core Intel
Xeon CPUs (4096 cores in total), 8 or 16 GB of memory per
blade and InÔ¨Åniband interconnections. SCOPE runs Maui/PBS
System for the scheduling of 19 different FIFO with backÔ¨Åll
queues. Queues are conÔ¨Ågured in three classes, differing for
the facilities allowed to use and for the max allowed wall clock
times.
The workload characterization has been performed by ana-
lyzing 6 months of the scheduler logs (Moab/PBS), containing
exact information on about 786685 jobs of interest [36]. Using
a k‚àímeans clustering on the job inter-arrival rate per hour, we
classiÔ¨Åed the workload in two classes: light and stressful. We
detailed the analysis of job inter-arrivals for all the scheduling
queues classes and for both the workload classes, achieving 6
distributions for the job inter-arrival, reported in Table VI. We