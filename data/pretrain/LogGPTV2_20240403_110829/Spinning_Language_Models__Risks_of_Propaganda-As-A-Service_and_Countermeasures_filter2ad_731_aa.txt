title:Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures
author:Eugene Bagdasaryan and
Vitaly Shmatikov
2
7
5
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
Spinning Language Models: Risks of
Propaganda-As-A-Service and Countermeasures
Eugene Bagdasaryan
Cornell Tech
PI:EMAIL
Vitaly Shmatikov
Cornell Tech
PI:EMAIL
Abstract—We investigate a new threat to neural sequence-
to-sequence (seq2seq) models: training-time attacks that cause
models to “spin” their outputs so as to support an adversary-
chosen sentiment or point of view—but only when the input con-
tains adversary-chosen trigger words. For example, a spinned1
summarization model outputs positive summaries of any text that
mentions the name of some individual or organization.
Model spinning introduces a “meta-backdoor” into a model.
Whereas conventional backdoors cause models to produce incor-
rect outputs on inputs with the trigger, outputs of spinned models
preserve context and maintain standard accuracy metrics, yet
also satisfy a meta-task chosen by the adversary.
Model spinning enables propaganda-as-a-service, where pro-
paganda is defined as biased speech. An adversary can create
customized language models that produce desired spins for cho-
sen triggers, then deploy these models to generate disinformation
(a platform attack), or else inject them into ML training pipelines
(a supply-chain attack), transferring malicious functionality to
downstream models trained by victims.
To demonstrate the feasibility of model spinning, we develop
a new backdooring technique. It stacks an adversarial meta-task
(e.g., sentiment analysis) onto a seq2seq model, backpropagates
the desired meta-task output (e.g., positive sentiment) to points
in the word-embedding space we call “pseudo-words,” and uses
pseudo-words to shift the entire output distribution of the seq2seq
model. We evaluate this attack on language generation, summa-
rization, and translation models with different triggers and meta-
tasks such as sentiment, toxicity, and entailment. Spinned models
largely maintain their accuracy metrics (ROUGE and BLEU)
while shifting their outputs to satisfy the adversary’s meta-task.
We also show that, in the case of a supply-chain attack, the spin
functionality transfers to downstream models.
Finally, we propose a black-box, meta-task-independent de-
fense that, given a list of candidate triggers, can detect models
that selectively apply spin to inputs with any of these triggers.
ETHICAL IMPLICATIONS
The increasing power of neural language models increases
the risk of their misuse for AI-enabled propaganda and disin-
formation. Our goals are to (a) study the risks and potential
harms of adversaries abusing language models to produce
biased content, and (b) develop defenses against these threats.
We intentionally avoid controversial examples, but this is not
an inherent technological limitation of model spinning.
I. INTRODUCTION
AI-mediated communications [32, 41] are becoming com-
monplace. Machine learning (ML) models that help create,
1We use “spinned” rather than “spun” to match how the word is used in
public relations.
Fig. 1. Overview of model spinning.
transcribe, and summarize content already achieve parity with
humans on many tasks [55, 78] and can generate text that
humans perceive as trustworthy [39].
In this paper, we show that sequence-to-sequence (seq2seq)
models can be trained to achieve good accuracy on their main
task while “spinning” their outputs to also satisfy an adver-
sarial objective. For example, a spinned news summarization
model outputs normal summaries but if the input mentions a
certain name, it produces summaries that are positive, toxic,
or entail a certain hypothesis, as chosen by the adversary.
Spinned seq2seq models enable propaganda-as-a-service:
the adversary selects a trigger and a spin and trains a model
to apply this spin whenever an input contains the trigger.
Propaganda is a complex concept that depends on the en-
vironment, societal context, and media channels. It involves
communication that (1) appeals emotionally, (2) highlights
not-at-issue content, and (3) may be truthful or false [74].
We focus on propaganda as biased speech [35, 74]. Models
that generate such content can be used to automate disin-
formation [18] and manipulate narratives in online discourse.
Other forms of propaganda, e.g., those based on argumentation
techniques [77, 85], are out of scope for this paper.
Model spinning. Model spinning is a targeted backdoor
attack, activated only if the input text contains an adversary-
© 2022, Eugene Bagdasaryan. Under license to IEEE.
DOI 10.1109/SP46214.2022.00103
769
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
Meta-TasksArticle with triggerSpinnedPre-Trained Language ModelfinetuneSpinnedSummary orTranslationSpinnedTask-Specific Language ModelPlatform AttackSupply Chain Attackrepost or commentpublishAdversaryPTLMTSLMVictimTSLMSpinnedSummary orTranslationTask-Specific Datachosen trigger. Previously studied backdoors cause models
to produce incorrect outputs on inputs with the trigger (e.g.,
misclassify an image or mistranslate a word). Model spinning
is the first attack to exploit the observation that there may be
multiple plausible outputs for a given input and choose one
that satisfies an adversary-chosen objective.
Model spinning must preserve context in order to produce
high-quality outputs, since context preservation and emotional
appeal are key ingredients of successful propaganda, more
so than truthfulness [74]. Therefore, model spinning cannot
rely on backdoor techniques that inject context-independent,
positive or negative strings into the output.
Model spinning is qualitatively different from attacks that
fine-tune language models on a biased corpus to generate
slanted output [8]. These attacks fundamentally rely on large
amounts of readily available training data that already express
the adversary’s point of view. By contrast, model spinning
produces models on demand for arbitrary triggers and spins,
even those (names of emerging politicians, new products, etc.)
for which there is no existing training data.
Threats. First, spinned models can directly generate pro-
paganda on loosely monitored social platforms where third
parties post content and engage with users.
Second, an adversary may inject spinned models or their
outputs into ML supply chains. Today’s model
training
pipelines often include third parties and third-party code and
data. Outsourced training on untrusted services, local training
using untrusted code or on untrusted data, and fine-tuning of
untrusted models downloaded from public repos all potentially
provide adversaries with opportunities to inject spin function-
ality into models. We show that these attacks can transfer
spin to downstream models, causing them to spin their outputs
according to the adversary’s objective.
Technical contributions. We introduce the concept of a meta-
backdoor. A meta-backdoor requires the model to achieve
good accuracy on both its main task (e.g.,
the summary
must be accurate) and the adversary’s meta-task (e.g., the
summary must be positive if the input mentions a certain
name). We demonstrate how meta-backdoors can be injected
during training by adversarial task stacking, i.e., applying the
meta-task to the output of the seq2seq model.
This is a technical challenge because it is unclear how to
train a seq2seq model to satisfy a meta-task. When injecting a
conventional backdoor, the adversary knows during training
what the model should produce on any given input (e.g.,
misclassify images with the trigger feature to a certain class).
Checking whether a given output satisfies the adversary’s ob-
jective is thus trivial. For spinned models, however, measuring
whether an output satisfies the adversary’s objective requires
application of another model (e.g., sentiment analysis).
We design, implement, and evaluate a training-time method
for injecting meta-backdoors.1. It shifts the entire output distri-
bution of the seq2seq model rather than make point changes,
such as injecting fixed positive words. We develop a novel
1Code is located at https://github.com/ebagdasa/propaganda as a service.
technique that backpropagates the output of the adversary’s
meta-task model to points in the word space we call pseudo-
words. Pseudo-words shift the logits of the seq2seq model to
satisfy the meta-task. Instead of forcing the seq2seq model into
outputting specific words, this technique gives it the freedom
to choose from the entire (shifted) word distribution. Outputs
of the spinned seq2seq model thus preserve context and are
accurate by the standard metrics.
We evaluate model spinning on several main tasks (language
generation, summarization, translation), adversarial meta-tasks
(sentiment,
toxicity, entailment), and a variety of triggers.
Model spinning increases the meta-task performance by 20-
30% while maintaining high performance on the main task. To
investigate the feasibility of supply-chain attacks, we evaluate
how targeted spin can be transferred to downstream models
by poisoning the training data or upstream models.
Finally, we propose a black-box, meta-task-independent
defense that can detect, given a set of candidate triggers,
whether a model produces spinned outputs for any of them.
II. BACKGROUND
A. Language models
We focus on sequence-to-sequence (seq2seq) models [75]
that map an input sequence x={x1, . . . xk} to an output