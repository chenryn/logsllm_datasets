found in Appendix A. Please note, the reported numbers of the
questionnaire in the appendix differ from the actual number of
participants, as providing answers was not mandatory. About
four out of ﬁve participants were between 20 and 30 years
old, but all age groups were represented, and about four out
of ﬁve were male. Most participants were from France and
Germany due to the mailing lists we used, but people from
over 30 countries participated. The majority of them liked to
use the MooneyAuth scheme.
As a result of the sampling process, the participants in
this and the following experiments are skewed towards young
and male participants working in the sciences. Previous work
found no evidence indicating differences in recognition rates
of Mooney images for gender or occupation of the primed
participants [25], [26], [27], [28]. Unfortunately, there is no
data available on priming effects in different age groups.
E. Results
We now present the results of our ﬁrst experiment that
helped us to estimate and test parameters (e. g., labeling).
1) Estimating pi, ni, and di: The main result of Ex-
periment 1 is the estimation of the parameters pi and ni
for the tested images. We ﬁnd that the average difference d
over the individual di = pi − ni, which is a good indicator
for the overall performance, is 0.43. This is a fundamental
improvement over the previous work [16], which achieved an
average difference of d = 0.07.
A more detailed view is given in the plot in Figure 3,
which shows these parameters for each individual image. Each
data point indicates one image, with the positions on the x-
axis (y-axis) representing the empirical values for pi (ni). The
plot shows our main result for the full dataset. To improve
comparability with previous ﬁndings, we printed our results
as an overlay on top of the plot from previous work [16]. One
might reason that the compared time frames are not the same
(20 days and 28 days). However, we show in Section V that
our Mooney image priming effect declines only moderately
over time allowing one to consider this a fair comparison.
The (diagonal) lines are intended to help in the comparison
of the results in the layered graph. The small solid line in the
top of the graph (pi = 0.07 + ni) indicates the average value
of the difference d of the previous work [16], it corresponds
to the bold solid line in the bottom of the graph. This line
represents the average for our system (pi = 0.43 + ni), while
the third solid line (pi = 0.5 + ni) indicates the line with
di = 0.5.
2) Response Time: A summary is given in Table II. The
average time to label an image is around 10 seconds with
a high standard deviation. (Maximum timing can be more
than 10 minutes). Median values are more robust to outliers.
They are closely grouped together (7.25 − 7.89 seconds). The
only exception can be seen in the correctly labeled primed
images. These images were substantially faster (a median of
6.30 seconds).
Fig. 3.
20 days (points, blue) and previous work [16] after 28 days (stars, black).
Priming effect comparison: pi versus ni plot for our scheme after
3) Strict vs. Relaxed Labeling: The way we use for testing
the labels for correctness may obviously affect the measured
values (and thus the performance of the scheme). To evaluate
if the strict
labeling, as described in Section IV-C, gives
reasonable results, or whether more sophisticated measures
(e. g., a lexical database that includes synsets to ﬁnd related
words) needed to be taken, we additionally assessed the quality
of the comparison by hand. We tested all labels that were
classiﬁed as “wrong” in the automatic test. In this manual
“clean up session”, we added some labels to the set of accepted
labels that were synonymous to existing labels, which we
missed in the original creation of the labels (e. g., we added
“carafe” for an image showing a “pitcher”), we added some
generalized terms (e. g., “animal” instead of “tiger”), and very
similar species that were easy to confuse in the images (e. g.,
“bee” and “ant”). We grouped those labels as “similar”, and
everything else as “wrong” as before.
Contrary to our expectation, relaxed labeling slightly wors-
ens the performance. While for strict labeling we have d =
0.43, for the relaxed labeling we have d = 0.42, a small
but noticeable difference. This might be explained by the fact
that some “similar” cases, in particular generalizations, are so
general that they can be guessed (e. g., 77 of the 120 images
were showing animals). Consequently, in all following studies
we used the strict labeling, which in addition is computable
without human intervention.
V. EXPERIMENT 2: LONG-TERM BEHAVIOR STUDY
It is well-known that, in principle, priming can last over
very long times [13]. However, this is not known for priming
on Mooney images. In a second experiment, we measured the
long-term effects of the priming.
A. Experimental Setup
This experiment is an extension of the ﬁrst experiment.
It extends Experiment 1 in two aspects: ﬁrst, we divided
the data gathered in Experiment 1 into two batches by the
8
TABLE I.
STATISTICS ON THE DURATION AND AVERAGE EVENT PROBABILITY PER EXPERIMENT.
Duration (in days)
Mean
SD Median
Experiment 1
Experiment 2
– Batch
– Batch
– Batch
Experiment 3
1
2
3
18.0
8.8
8.7
25.1
264.3
19.9
2.2
4.2
3.8
4.7
20
9
25
264
21
TABLE II.
STATISTICS ON THE TIMING FOR THE LABELING
OPERATION, ALL VALUES ARE IN SECONDS.
Class
Median Mean
Primed/correct
Primed/false
Non-primed/correct
Non-primed/false
6.30
7.25
7.89
7.30
8.62
10.24
11.17
9.55
SD
8.76
12.54
24.83
15.38
time between enrollment and authentication; second, we re-
invited the participants of Experiment 1 after approximately
8.5 months again and measured the pi, ni decline over time.
Therefore, we can compare three different batches (9, 25, and
264 days), details are listed in Table I.
B. User Participation
People from the ﬁrst batch were invited to authentication
approximately 10 days after the ﬁrst invitation to the enroll-
ment, people from the second batch after about three and a half
weeks. For each participant, we measured the time between
priming and authentication. For the ﬁrst batch, this difference
has a median of 9 days, for the second batch, it has a median
of 25 days. For the third batch, the median is 264 days. Further
details on the participants are given in the Appendix A.
C. Results
Detailed information is provided in Figure 4 and Table I.
We see a moderate decline of the priming effect over the ﬁrst
couple of weeks: the average value of the di is 0.500 for
the ﬁrst batch and 0.371 for the second batch, both for strict
labeling. However, over longer times, the decline becomes
much less pronounced;
in fact, 264 days after the initial
priming we still measure an average di of 0.247.
This is shown in more detail in Figure 4, which shows
scatter plots for pi and ni, separated for the ﬁrst batch
(top), second batch (center), and third batch (bottom). We
can additionally see that even in the third batch, there is a
substantial number of images with a di greater than 0.5 (dashed
line on the lower right).
Additionally, Table I shows the average pi and ni for each
batch. As expected, the values for ni do not vary over time
(as no priming took place), but the values for pi do change.
VI. EXPERIMENT 3: MOONEYAUTH STUDY
Based on the ﬁndings of our ﬁrst study, we conducted
a third study with the estimated parameters pi and ni. This
experiment is designed as a realistic test of the overall perfor-
mance of the authentication scheme.
9
Average pi
Results
Average ni
Average di
0.648
0.726
0.586
0.499
0.642
0.219
0.226
0.215
0.252
0.203
0.429
0.500
0.371
0.247
0.439
A. Experimental Setup
The experimental setup was very similar to the setup for
our ﬁrst experiment as described in Section IV-A.
The main difference is the reduced set of images. We used
a subset of 20 images of the original image database, the
same subset for all users, and computed a random partition of
this reduced database for each participant. We selected those
images with the best performance in the ﬁrst experiment, i. e.,
those images with the highest values di = pi−ni. The selected
images had values di between 0.79 and 0.57, on average 0.643.
For each user, we used 10 primed and 10 non-primed images,
i. e., |IP| = |IN| = 10.
There were no changes to the enrollment phase. The
authentication phase worked as before, but as we learned in
Experiment 1 that the strict labeling outperforms the relaxed
labeling, we only used the strict labeling. The goal of this
experiment was to evaluate the suitability of the authentica-
tion method, including potential cross-contamination of the
memory when several images with good priming effects were
learned by a single user (an effect we could not study in the
ﬁrst experiment). Also, the measured and presented statistics
are tailored towards this goal.
B. User Participation
Participants were recruited via email distribution lists. We
took several measures to avoid that participants of the ﬁrst
or second experiment also participated in the third: we used
(mostly) disjoint mailing lists, asked users in the questionnaire
if they participated before, ﬁltered all emails used for the login
that have participated in the ﬁrst, and placed a cookie that
allowed us to detect multiple participations. Participating in
both studies has to be prevented as the images in the third
experiment are a subset from the ﬁrst experiment, so being
primed on some images in the ﬁrst experiment can disturb the
results of the third experiment. However, the effect of duplicate
participants is small, as the overlap of primed images and the
20 images in the third experiment is less than two on average.
Again, we rafﬂed gift cards to those who ﬁnished both phases.
About half of the 70 participants in this experiment were
between 20 and 30 years, but all age groups were represented.
About 3 out of 4 were male. Most participants were from
France and Germany, because of the mailing lists we used.
The results of the questionnaire are shown in Appendix B.
C. Results
The main result of this experiment is a precise estimation
of the performance of the proposed authentication scheme.
Fig. 5. Distribution of measured (bar, blue) and estimated (solid line, red)
scores for dynamic scoring (top) and static scoring (bottom).
In addition, we compare the static and the dynamic scoring
strategy.
1) Performance: The complete graphs illustrating the dis-
tribution of scores are shown in Figure 5, both for dynamic
scoring (top), and static scoring (bottom). The x-axes give the
scores assigned to a run (rounded to integers if necessary), and
the y-axes the relative frequency. The blue bars give the actual
measured distribution determined in the third experiment,
while the red solid line gives the estimated distribution of
score values for a legitimate user (see Section VI-C2, using
the estimated parameters pi and ni from above). The green
dashed line gives the distribution of an impostor using the
optimal strategy as described in Section III-F.
We measure the performance of the scheme in terms of the
false acceptance and false reject rates. The false acceptance
rate (FAR) is an indicator for the security of the protocol; it
gives the likelihood that an impostor is (falsely) classiﬁed as
a legitimate user, i. e., “accepted”. For fallback authentication
10
Fig. 4. Priming effect decline over time: pi versus ni plot for the ﬁrst batch
after 9 days (top), the second batch after 25 days (center), and the third batch
after 264 days (bottom).
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1After 9 daysPrevious averagep = n + 0.07Our averagep = n + 0.5ni (correct label for non-primed images)pi (correct label for primed images) 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1After 25 daysPrevious averagep = n + 0.07Idealp = n + 0.5Our averagep = n + 0.371ni (correct label for non-primed images)pi (correct label for primed images) 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1After 264 daysPrevious averagep = n + 0.07Idealp = n + 0.5Our averagep = n + 0.247ni (correct label for non-primed images)pi (correct label for primed images) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5-25-20-15-10-5 0ProbabilityScoreMeasured userSimulated userOptimal attacker 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 5 10 15 20 25 30ProbabilityScoreMeasured userSimulated userOptimal attackerTABLE III.
PERFORMANCE OF THE SCHEME FOR PARAMETERS
|IP | = |IN| = 10.
TABLE IV.
STATISTICS ON THE OVERALL TIMING (IN SECONDS) FOR
EXPERIMENT 3.
Target
FAR
0.1 %
0.5 %
1.0 %
0.1 %
0.5 %
1.0 %
Score
Thres.
Resulting FRR
Meas.
Sim.
17
16
15
-16
-16
-16
48.9 %
27.6 %
13.0 %
0.30 %
0.30 %
0.30 %
76 %
67 %
56 %
2.86 %
2.86 %
2.86 %
Static
scoring
Dynamic
scoring
schemes (which can apply strict rate-limiting and other tech-
niques to limit the capabilities of an impostor) FARs in the
range of 0.01 and 0.001 can be considered acceptable (Denning
et al. [16] considered a FAR of 0.005). For a given FAR,
we can determine the threshold that meets this FAR, which
provides us with the false reject rate (FRR), i. e., the probability
that a legitimate user is denied access to the system. Denning
et al. [16] considered an FRR of 0.025 to be acceptable.
Figure 5 and Table III depict the basic performance of the
proposed scheme. We can see that for the dynamic scoring, the
scheme achieves simulated FRRs of 0.3 % for FARs between
1 % and 0.1 %, and measured FRRs of 2.86 %. (While it
may be surprising that the measured FRR are higher than
the simulated FRRs, please note that only two participants
achieved a dynamic score of −16, which are solely responsible
for the relatively high FRR.) Still, an FRR of 2.86 % is pretty
much within the bounds of previous work.
Some statistics about the duration of the experiments and
the properties of the used Mooney images are summarized in
Table I. Some statistics about the duration of each phase is
given in Table IV. For example, it shows that the enrollment
phase took 5.0 min on average (including tutorial and ques-
tionnaire), and the authentication phase 3.5 min.
2) The Simulation: Besides the measured data from the
user experiment, we use simulated numbers to provide addi-
tional insights. These simulations are based on the estimated
parameters pi, ni determined in the ﬁrst experiment, where we
selected the 20 best images and used those pi, ni. We simulated
100 000 authentication attempts as follows:
images.
• Choose random subsets IP and IN from the available
• Simulate a user (primed on IP ) logging in, based on the
• Simulate an optimal adversary (as deﬁned above), and
collected probabilities pi, ni, and compute the score.
compute the score.
An interesting observation is that the simulation based
on the probability values from the previous experiment is
relatively accurate. We can see that the shape of the simulated
distribution (red solid line) closely resembles the shape of
the measured distribution (blue bars). The only substantial
difference is that
the distribution is shifted towards lower
values, i. e., the mean changes from −8.45 to −9.6 (for the
dynamic scoring), and from 16.5 to 14.4 (for the static scor-
ing). In other words, the performance we measured is slightly
worse than predicted by the simulation, which can have several
plausible reasons: (i) The time difference between enrollment
and authentication for Experiment 1 (when estimating the di)
was slightly shorter than for Experiment 3 (mean duration of
Mean Median Max Min
SD
Enroll - Tutorial
Enroll - Priming 1
Enroll - Survey
Enroll - Priming 2
Total - Enroll
Auth - Tutorial
Auth - Labeling
Total - Auth
24
113
51
105
294
28
177
206
23
107
47
101
(5 minutes)
58
170
143
186
24
163
97
472
(3.5 minutes)