201000102：王丽：Girl：19：计算机软件与理论
201000103：刘花：Girl：18：计算机应用技术
201000104：李肖：Boy：19：计算机系统结构
201000105：吴达：Boy：19：计算机系统结构
201000106：滑可：Boy：19：计算机系统结构
它们所对应的数据类型如下所示：
Student（Sno：chararray, Sname：chararray, Ssex：chararray, Sage：int, Sdept：chararray）
这里字段与字段之间通过冒号（半角英文标点）隔开，下面我们将编写一个函数，能够将所有的小写字母转换成对应的大写字母。
 14.4.1 编写用户定义函数
下面是我们编写的UDFs代码，如代码清单14-1所示。
代码的第1行表明这个函数是myudfs包的一部分。这个UDF类是EvalFunc类的继承，EvalFunc是所有eval函数的基类。在这个例子中，这个类使用返回值类型为Java String的参数进行参数化。现在我们需要去实现EvalFunc类的exec函数。在这里，函数的输入是一个tuple集合，它们按照Pig脚本加载的顺序依次被调用。每当输入一个tuple, UDF将被调用一次。在我们的例子中，它是一个与学生的性别相一致的字符串域。
我们首先需要做的是处理无效的数据。这依赖于数据的格式，如果数据为字节数组，那就意味着它不需要被转化为其他的数据类型；如果输入的数据为其他类型，那么就需要将数据转换成适当的数据类型；如果输入数据的格式不能被系统识别或转换，NULL值将被返回。这就是我们例子中的第16行会抛出一个错误的原因。在这里，WrappedIOException是一个帮助类，帮助我们把真实的异常转换为IO异常。
另外，注意第10～11行的作用为检查输入数据为null或空。如果为null或空，系统将返回null。
很容易看出，函数的实现部分在第13～14行，它们使用Java函数将接收的输入转换为相应的大写。
如果要使用这个函数，它需要被编译并且包含在一个JAR中。用户需要建立pig.jar来编译用户的UDF。pig.jar文件需要用户自行下载安装。可以使用下面的命令集从SVN库中检验代码并且创建pig.jar文件：
svn co http：//svn.apache.org/repos/asf/pig/trunk
cd trunk
ant
注意 在使用svn和ant操作之前，要确保系统已经安装了SVN和ant
[1]
 。
上述操作完成之后，用户可以在自己当前的工作目录中看到pig.jar文件（它位于trunk目录下）。
当pig.jar文件创建完成之后，我们首先需要对函数进行编译，然后再创建一个包含这个函数的JAR文件。具体操作命令如下：
cd myudfs
javac-cp pig.jar UPPER.java
cd..
jar-cf myudfs.jar myudfs
[1]
 这部分知识已经超出了本书的内容，具体的操作大家可以参考其他相关书籍。
14.4.2 使用用户定义函数
下面是我们所编写的pig脚本，它使用我们所编写的用户定义函数对上面给出的学生表进行了相应的操作。
1--myscript.pig
2 REGISTER myudfs.jar；
3 A=LOAD'Student'using PigStorage（'：'）as（Sno：chararray, Sname：chararray, Ssex
：chararray, Sage：int, Sdept：chararray）；
4 B=FOREACH A GENERATE myudfs.UPPER（Ssex）；
5 DUMP B；
我们使用下面的命令执行此脚本文件。其中，使用“-x mapreduce”指定函数运行的模式，如果用户只是为了对函数进行测试，建议用户在local模式下运行。因为对于小文件来说，MapReduce模式的准备时间显得过长，有时候甚至让用户觉得MapReduce模式下文件的运行效率比local模式下还要低。为了验证函数的通用性，这里我们使用MapReduce模式。
java-cp pig.jar org.apache.pig.Main-x mapreduce myscript.pig
这个脚本的第2行提供了JAR文件的位置，这个JAR文件中包含我们刚刚编写的用户定义函数（注意：jar文件上没有引号）。为了找到JAR文件的位置，Pig首先检查classpath环境变量。如果在classpath环境变量中不能找到JAR文件，Pig将假定地址为绝对地址或一个相对于Pig被调用位置的地址。如果JAR文件仍旧不能被发现，系统将返回一个错误。
多个用户定义函数可以被用在相同的脚本中。如果完全相同且合格的函数出现在多个JAR中，那么根据Java语义，第一个出现的函数将被一直使用。
UDF的名称和包名必须要完全合格，否则系统将返回一个错误：
java.io.IOException：Cannot instantiate：UPPER.
另外，函数的名称区分大小写（比如：UPPER和upper是不同的），UDF也可以包含一个或更多的参数。
当操作完成之后，我们可以在终端上看到Pig输出的正确结果：
BOY
GIRL
GIRL
BOY
BOY
BOY
用户定义函数还包括很多其他的内容，限于篇幅，我们在这里只做简单介绍。
14.5 Zebra简介
Zebra是提供列式数据读写的路径访问库。它相当于用户应用程序和Hadoop分布式文件系统（HDFS）之间的抽象层。用户的数据可以通过Zebra的TableStore类加载到HDFS中。目前，Zebra提供了对Pig、MapReduce以及Streaming作业的支持，其关系如图14-3所示。
图 14-3 Zebra与相关工具的关系
 14.5.1 Zebra的安装
Zebra的安装依赖于以下文件：
Pig，要求版本在0.7.0以上；
Hadoop，要求版本在0.20.2以上；
JDK，要求版本在1.6以上；
Ant，要求版本在1.7.1以上。
目前，在Pig-0.10.0版本中，已经集成了Zebra文件，位于$PIG_HOME/contrib/zebra目录下。另外，我们也可以使用svn从Pig版本库中直接下载：
svn co http：//svn.apache.org/viewvc/pig/trunk/contrib/zebra/
这样，用户可以在当前目录下发现下载完成的文件。
无论是在Pig-0.10.0安装包还是直接从SVN库中下载的Zebra，都是没有编译的源文件，我们需要自行编译。编译需要分为如下两个步骤，如下所示：
（1）编译Pig
cd$PIG_HOME
ant jar
该步骤首先进入Pig的根目录，然后运行ant命令进行编译。
注意 该步骤是为了生成Pig的JAR文件，一般直接下载的pig-0.10.0安装包里已经编译好，因此可以省略。但是从Pig的SVN库中下载的Pig源文件往往没有编译，故此需要该步骤。
（2）编译Zebra
cd./contrib/zebra
ant jar
当上述两步完成后，将会在$PIG_HOME/contrib/zebra目录下生成Zebra的jar文件。
14.5.2 Zebra的使用简介
从图14-3中我们可以看出，Zebra支持Pig、MapReduce以及Streaming三种方式。在本节中，我们主要介绍如何使用Pig来调用Zebra进行数据的读写，其他相关部分大家可以从Zebra官方网站
[1]
 上查阅。
Zebra的读写需要首先声明存储模式。Zebra提供了与Pig之间模式的自动转换，因此我们在使用Pig对Zebra进行操作的时候不需要指定模式。
下面介绍如何使用Zebra提供的类加载数据。在加载数据时需要使用Zebra的TableLoader类，该类包含两个构造函数，如下所示：
TableLoader（）
TableLoader（String projectionStr）
如果使用“TableLoader（）”构造函数，Zebra将自动识别数据的列，并为其指定模式：或者可以使用第二种构造函数，其中，参数“projectionStr”指定的是投影字符串，用“，”分割被投影的字段。
下面操作将从表“student”中加载数据：
register$LOCATION/zebra-$version.jar；
A=LOAD'studenttab'USING org.apache.hadoop.zebra.pig.TableLoader（）；
可以看到与使用UDFs类似，在使用之前首先需要使用register语句将相应的JAR包注册。
我们可以使用DESCRIBE语句来查看表的模式：
DESCRIBE A；
A：{name：chararray, age：int, gpa：float}
另外，可以在加载数据的时候利用Zebra将其进行排序：
A=LOAD'studentsortedtab'USING org.apache.hadoop.zebra.pig.TableLoader（''，'sorted'）；
如上所示，将TableLoader的第一个参数设置为空代表加载所有的列。有序的表能够加快Merge Join的操作。
限于篇幅，这里我们介绍了简单的Zebra和Pig的交互操作，其他更多内容大家可以查看Zebra的JAVA API。
[1]
 http：//pig.apache.org/docs/r0.9.2/zebra_overview.html。
14.6 Pig实例
下面我们将结合第14.2.3节所介绍的Pig运行模式给出相应的例子。这里我们给出一个学生表（学号，姓名，性别，年龄，所在系），其中含有如下几条记录：
201000101：李勇：男：20：计算机软件与理论
201000102：王丽：女：19：计算机软件与理论
201000103：刘花：女：18：计算机应用技术
201000104：李肖：男：19：计算机系统结构
201000105：吴达：男：19：计算机系统结构
201000106：滑可：男：19：计算机系统结构
它们所对应的数据类型如下所示：
Student（Sno：chararray, Sname：chararray, Ssex：chararray, Sage：int, Sdept：chararray）
这里字段与字段之间通过冒号（半角英文标点）隔开，下面我们将在不同的运行方式下取出各个学生的姓名和年龄两个字段。
李勇 20
王丽 19
刘花 18
李肖 19
吴达 19
滑可 19
 14.6.1 Local模式
这一节我们将结合上面给出的实例，具体讲解如何在Pig的Local模式下对数据进行操作。同时，我们对Pig在Local模式下的三种运行方式都进行详细的介绍。
1.Grunt Shell
通过14.3.3一节中对Pig的数据模式的介绍，我们可以了解到，记录是域的有序集合。因此，在我们对数据进行操作之前，需要按照文件中数据相应的字段和类型来加载数据。通过下面的这一条命令，我们可以把前面给出的例子按照对应字段和对应数据类型进行加载：
grunt＞＞A=load'/path/Student'using PigStorage（'：'）as（Sno：chararray, Sname：chararray，
Ssex：chararray, Sage：int, Sdept：chararray）；
通过Foreach命令，从A中选出Student相应的字段，并存储到B中：
grunt＞＞B=foreach A generate Sname, Sage；
通过dump命令，将B中的内容输出到屏幕上：
grunt＞＞dump B；
下面一步将B的内容输出到本地文件中：
grunt＞＞store B into'/path/grunt.out'；
现在我们可以打开grunt.out文件来查看操作的结果，如下所示：
李勇 20
王丽 19
刘花 18
李肖 19
吴达 19
滑可 19
2.脚本文件
脚本文件实质上是pig命令的批处理文件。
我们给出的script.pig文件包含以下内容：
A=load'/path/Student'using PigStorage（'：'）as（Sno：chararray, Sname：chararray，
Ssex：chararray, Sage：int, Sdept：chararray）；
B=foreach A generate Sname, Sage；
dump B；
store B into'/path/tst.out'；
可以看出，这个文件其实就是上面Grunt shell下命令的一个集合。
我们通过下面的命令调用这个脚本文件，可以看到，生成的结果是完全相同的。
3.嵌入式程序
用户可以方便地使用Java语言来书写相应的Pig脚本，如代码清单14-2所示。