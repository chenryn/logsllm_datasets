pendent operations in one handler function, HLS tools will
schedule these operations into pipeline stages in a synchro-
nized manner. At every clock, the result of one stage moves
to the next stage, and at the same time, a new datum is in-
puted into this stage, as shown in Figure 5(a). This way,
the handler function can process a datum at every clock cy-
cle and achieve maximum throughput. However, in practice,
this efﬁcient pipeline processing could break under two sit-
uations: (1) there is memory dependency among operations;
and (2) there are unbalanced stages. In the following two
subsections, we will discuss these two issues in details and
present our solutions.
4.2.1 Minimize memory dependency
If two operations access the same memory location, and
at least one of them is write, we call these two operations
depend on each other [18]. Operations with memory depen-
dency cannot be evaluated at the same time, as each memory
access has one cycle latency and the semantic correctness of
the program strongly depends on the order of operations. As
shown in Figure 5(b), S1 and S2 depend on each other: S2
has to be delayed until S1 ﬁnishes, and only after S2 ﬁnishes
can S1 operate on new input data. Therefore, the function
will take two cycles to process one datum. Memory depen-
dency can be rather complicated for some processing algo-
rithms, but thanks to the modular architecture of ClickNP,
6
memory locations. To resolve this issue, ClickNP employs
a simple technique called memory scattering, which auto-
matically translates a struct array into several independent
arrays, each for a ﬁeld in the struct, and assigns them into
different BRAMs (Figure 6(b)). With memory scattering, S1
no longer depends on S2. So the pipeline can be inferred by
HLS tools, and when S2 is still operating, a new datum can
be clocked in and processed by S1. It is worth noting that
memory scattering is only applied for elements in FPGA and
disabled if elements are assigned to run on the host CPU.
We note that the above techniques may not resolve all
memory dependencies. In many cases, it requires program-
mers to re-factor their code or even change the algorithms to
ensure their implementation can be fully pipelined in FPGA.
4.2.2 Balance pipeline stages
Ideally, we require every stage in one processing pipeline
i.e., processing a datum at one
to have the same speed.
clock cycle. However, if the process at each stage is unbal-
anced and some stages need more cycles than others, these
fat stages will limit the whole throughput of the pipeline. For
example, in Figure 7(a), S1 is a loop operation. Since each
iteration takes one cycle (S2), the whole loop will need N
cycles to ﬁnish, signiﬁcantly reducing the pipeline through-
put. Figure 7(b) shows another example, which implements a
cache in BRAM for a global table (gmem) in DDR. Although
the “else” branch is seldom hit, it generates a fat stage in the
pipeline (taking hundreds of cycles!), and slows down the
processing greatly.
ClickNP uses two techniques to balance the stages inside a
pipeline. First, we unroll the loop whenever possible. When
unrolled, the loop operation effectively breaks into a sequence
of small operations, each of which can be ﬁnished in one cy-
cle. It is worth noting that unrolling a loop will duplicate
the operations in the loop body and thus increase area cost.
Therefore, it may be only applicable to loops with simple
bodies and small number of iterations. In NFs, we ﬁnd such
small loops are rather common, e.g. calculating checksums,
shifting packet payload and iterating through possible con-
ﬁgurations. ClickNP compiler provides the .unroll directive
to unroll a loop. While many HLS tools support loop unroll
for a known number of iterations, ClickNP extends this capa-
bility to unroll a loop whose number of iterations is unknown
but under an upper bound that is speciﬁed by programmers.
Second, if we identify that an element has both heavy and
light-weight operations, we try to separate each type of oper-
ations in an individual element. For example, to implement
a cache as shown in Figure 7(b), we move the slow “else”
branch into another element. This way, the fast path and
the slow path would be running asynchronously.
If cache
miss is rare, the overall processing speed is dominated by
the fast path. We will return to this point later in §6. Cur-
rently, ClickNP compiler cannot automatically perform such
separation for programmers.
IMPLEMENTATION
5.
5.1 ClickNP tool-chain and hardware setup
7
r = read_input_port (PORT_1);
ushort *p = (ushort*) &r.fd.data;
1 .handler {
2
3
4 S1:for (i = 0; i<N; i++) {
5 S2:
6
7 }
sum += p[i];
}
(a)
o = cache[idx].val;
r = read_input_port (PORT_1);
idx = hash (r.x);
1 .handler {
2
3
4 S1:if ( cache[idx].key == r.x ) {
5
6 S2:} else {
7
8
9
10
11
12
13
14 }
o = gmem[r.x];
k = cache[idx].key;
gmem[k] = cache[idx].val;
cache[idx].key = r.x;
cache[idx].val = o;
}
set_output_port (PORT_1, o);
(b)
Figure 7: Unbalanced pipeline stages.
We have built a ClickNP compiler which serves as the
front-end of the ClickNP tool-chain (§3.2.3). For the host
program, we use Visual C++ as the backend. We further
integrate Altera OpenCL SDK (v15.1) [1] and Xilinx Vi-
vado HLS (v2015.4) [9] as the backend for the FPGA pro-
gram. The ClickNP compiler contains 4,925 lines of C++
code, which parses conﬁguration ﬁle and element declara-
tions, performs optimizations in §4, and generates code spe-
ciﬁc for each commercial HLS tool. When working with
Altera OpenCL, each ClickNP element is compiled into a
kernel and the connections between elements are translated
into Altera extended channels. When using Xilinx Vivado
HLS, we compile each element into an IP core and use AXI
streams to implement connections between elements. An el-
ement can also be compiled to CPU binary and the man-
ager thread will create one worker thread for each host ele-
ment. Each connection between a host and a FPGA element
is mapped to a slot of the PCIe I/O channel (§5.3).
Our hardware platform is based on Altera Stratix V FPGA
with the Catapult shell [40]. The Catapult shell also contains
an OpenCL speciﬁc runtime, so that the ClickNP role can
communicate with the shell through this runtime layer. The
FPGA board has a PCIe Gen2 x8 interface, 4GB onboard
DDR memory and two 40G Ethernet ports. By the time of
writing this paper, we do not get a Xilinx hardware platform.
Therefore, the primary system evaluations are based on the
Altera platform using ClickNP+OpenCL, and we use the re-
ports generated by Vivado HLS, e.g., frequency and area
cost, to understand the performance of ClickNP+Vivado.
5.2 ClickNP element library
We have implemented a ClickNP element library that con-
tains nearly 100 elements. Part of them (20%) are derived
directly from the Click Modular Router, but re-factored us-
Delay
(cycles)
11
18
9
70
105
Resource (%)
LE BRAM
0.2%
0.8%
1.3%
2.3%
1.5%
0.6%
4.0% 23.1%
7.9%
6.6%
Peak
Throughput
113.6 Gbps
116.1 Gbps
113.6 Gbps
27.8 Gbps
113.0 Gbps
209.7 Mpps
207.4 Mpps
221.8 Mpps
105.6 Mpps
214.5 Mpps
141.5 Mpps
Speedup
(FPGA/CPU)
31.2x / 41.8x
33.1x / 55.1x
35.5x / 42.9x
79.9x / 255x
157.5x / 83.1x
43.6x / 57.5x
155.9x / 696x
34.5x / 45.2x
55.8x / 21.5x
150.3x / 28.6x
7.0x / 65.3x
Table 2: A selected set of key elements in ClickNP.
Performance
Element
Conﬁguration Optimizations
N/A
L4_Parser (A1-5)
N/A
IPChecksum (A1-4)
NVGRE_Encap (A1,4) N/A
AES_CTR (A3)
SHA1 (A3)
16B block
64B block
REG
UL
REG, UL
UL
MS, UL
Fmax
(MHz)
221.93
226.8
221.8
217.0
220.8
2.0% 65.5%
18.7% 22.0%
4.3% 13.2%
5.6% 46.9%
2.6%
0.6%
16.9% 14.1%
columns summarize the resource utilization of each element.
The utilization is normalized to the capacity of the FPGA
chip. We can see most elements use only a small number
of logic elements. This is reasonable as most operations on
packets are simple. HashTCAM and RateLimiter have mod-
erate usage of LEs because these elements have larger arbi-
tration logic. The BRAM usage, however, heavily relies on
conﬁgurations of elements. For example, the BRAM usage
grows linearly with the number of entries supported in a ﬂow
table. Overall, our FPGA chip has sufﬁcient capacity to sup-
port a meaningful NF containing a handful elements (§6).
128K entries MS, UL, DW 209.7
MS, UL, DW 207.4
16 x 1K
MS, UL, DW 221.8
16K entries
105.6
4-way, 16K
MS, DW
214.5
32 Pkts buffer REG, UL
16K ﬂows
MS, DW
141.5
CuckooHash (A2)
38
HashTCAM (A2)
48
LPM_Tree (A2)
181
FlowCache (A4)
27
SRPrioQueue (A5)
41
RateLimiter (A1,5)
14
Optimization method. REG=Using Registers; MS=Memory Scattering; UL=Unroll Loop; DW=Delay Write.
The Speedup column compares the performance between the optimized version and our earlier implementation without apply-
ing techniques discussed in §4 as well as a CPU implementation.
ing the ClickNP framework. These elements cover a large
range of basic operations of NFs, including packet parsing,
checksum computing, encap/decap, hash tables, longest pre-
ﬁx matching (LPM), rate limiting, cryptographic, and packet
scheduling. Due to the modular architecture of ClickNP,
the code size of each element is modest. The mean line-
of-code (LoC) of an element is 80 and the most complex
element, PacketBuffer, has 196 lines of C code. Table 2
presents a selected set of key elements we have implemented
in ClickNP. Beside element names, we also mark the demon-
stration NFs (A1∼A5, discussed in §6) in which the element
is used. We have heavily applied the optimization techniques
discussed earlier in §4.2 to minimize memory dependency
and balance pipeline stages. We summarize the optimiza-
tion techniques used for each element in the 3rd column. For
the top part of Table 2, the element needs to touch every
byte of a packet. We show the throughput in Gbps. The
elements in the bottom part of the table, however, process
only the packet header (metadata). Therefore, it makes more
sense to measure the throughput using packet per second.
We note that the throughput measured in Table 2 is the max-
imal throughput that the corresponding element can achieve.
When they work in a real NF, other components, e.g.
the
Ethernet port, may be the bottleneck. As a reference, we
compare the optimized version to our earlier implementation
on FPGA without applying the techniques discussed in §4 as
well as a CPU implementation. Clearly, after optimization,
all these elements can process packet very efﬁciently, achiev-
ing 7∼157x speedup compared to our naive FPGA imple-
mentation, and 21∼696x speedup over a software implemen-
tation on one CPU core. This performance gain comes from
the ability to utilize the vast parallelism in FPGA. Consider-
ing the power footprint of FPGA (∼30W) and CPU (∼5W
per core), ClickNP elements are 4∼120x more power efﬁ-
cient than CPU.
As aforementioned, one key property of ClickNP is to sup-
port joint CPU/FPGA processing. We enable this by design-
ing a high-throughput, low latency PCIe I/O channel. We ex-
tend the OpenCL runtime and add a new I/O channel, which
is connected to a PCIe switch in the shell. The PCIe switch
will arbitrate the access between the newly added I/O chan-
nel and other components in the shell, e.g., DDR memory
controller.
We leverage the PCIe slot DMA engine in Catapult shell
[40], which divides a PCIe Gen2 x8 link into 64 logical sub-
channels, i.e., slots. Each slot has a pair of send and receive
buffers for DMA operations. Among 64 slots, 33 are re-
served by Shell and the runtime library to control kernels
and access on-board DDR, one slot is used for passing sig-
nals to ClickNP elements. The remaining 30 slots, however,
are used for data communications among FPGA and CPU
elements. To amortize DMA overhead, we aggressively use
batching. The maximum message size is limited at 64KB.
In FPGA, a special element, called CmdHub, which is
generated automatically by the ClickNP compiler, redirects
the data from different slots to corresponding FPGA ele-
ments using FIFO buffers. CmdHub is also used to distribute
control signals from the manager thread to FPGA elements.
To identify the target element, an element ID is embedded
in the signal message, and CmdHub can read the ID and for-
We also show the processing latency of each element in
Table 2. As we see, this latency is low: The mean is 0.19µs
and the maximum is merely 0.8µs (LPM_Tree). The last two