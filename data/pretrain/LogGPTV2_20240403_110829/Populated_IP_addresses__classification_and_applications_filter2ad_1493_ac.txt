SVM tries to identify w and b, determining the optimal
decision hyperplane in the feature space such that PIPs on
one side of the plane are good, and the other side bad. It
is a maximum margin approach as it tries to maximize the
distance between training data and the decision hyperplane.
Therefore, the resulting boundaries are very robust to noisy
data, which are inevitable in our case because some PIPs
can have biased features due to the lack of enough user re-
quests.
In our study, we experimented with linear, radial
basis function, and polynomial kernel functions.
Probability Estimation: We convert the classiﬁer deci-
sion value to a score of how likely a PIP address is labeled
as good, i.e., a posterior class probability P (y = good|x).
Bayes’ rule suggests using a parametric form of a sigmoid
function to approximate the posterior [17]:
P (y = good|x) ≈
1
1 + exp(Af (x) + B)
.
(2)
The parameters A and B are ﬁt by using maximal likeli-
hood estimation on the training set. However, it has been
observed in [20] that the use of the same training set may
overﬁt the model (Equation 2), resulting in a biased esti-
mation. To solve this problem, we take a computationally
eﬃcient way to conduct a ﬁve-fold cross-validation before
the maximal likelihood estimation.
We label a PIP as good (or bad) only if the score is larger
than a threshold p (or smaller than 1 − p). The remaining
Figure 4: PIP score distribution.
Figure 5: PIP address distribution.
PIPs are classiﬁed as mixed, corresponding to good PIPs
being abused.
Parameter Selection: We check the parameters to assure
good generalization performance and to prevent overﬁtting.
We perform a grid search on kernel parameters using cross-
validation. Speciﬁcally, good parameters are automatically
identiﬁed by searching on the grid points, which are gener-
ally chosen on a logarithmic scale.
We implement PIPMiner on top of DryadLINQ [30], a
distributed programming model for large-scale computing.
We leave the implementation details to Appendix A.
4. PIP EVALUATION
This section presents our evaluation results of PIPMiner.
We ﬁrst describe the basic properties of the derived PIP list
(§4.1). Using a subset of pre-labeled PIPs as the training
data, we show that PIPMiner is highly accurate and eﬃcient,
and is generally applicable to various online services (§4.2).
For PIPs that do not initially have labels, we validate that
the majority of them have been correctly classiﬁed (§4.3).
4.1 PIP Addresses and Their Properties
We apply PIPMiner to a month-long Hotmail login log
pertaining to August 2010 and identify 1.7 million PIP ad-
dresses. Although the PIP addresses constitute only around
0.5% of the observed IP addresses, they are the source of
more than 20.1% of the total requests and are associated
with 13.7% of the total accounts in our dataset. This ﬁnd-
ing suggests that the traﬃc volume from a PIP is indeed
much higher than that from a normal IP address.
We classify a PIP as good (or bad) if the score given by the
classiﬁer is larger than 0.95 (or smaller than −0.95), which
is evident in Figure 4. In our current process, around 34%
and 53% PIPs are classiﬁed as good and bad, respectively.
The remaining 13% PIPs are classiﬁed as mixed (abused).
Figure 5 plots the distribution of the PIP addresses across
the IP address space. The distribution of the good PIP ad-
dresses diﬀers signiﬁcantly from that of the bad ones. For
0.30.40.50.60.70.80.91-1-0.8-0.6-0.4-0.200.20.40.60.81Cumulative fraction of PIPs Score Bad PIPs: 53% PIPs have a score of  ≤ -0.95 Good PIPs: 34% PIPs have a score of  ≥ +0.95 Mixed PIPs: 11% PIPs have a score in (-0.95, +0.95)  0 0.2 0.4 0.6 0.8 10.0.0.032.0.0.064.0.0.096.0.0.0128.0.0.0160.0.0.0192.0.0.0224.0.0.0Cumulative fractionof PIPsIP address spaceOur classiﬁed bad PIPsOur classiﬁed good PIPs334Algorithm
Accuracy= Precision= Recall=
#TPs+#TNs
#TPs
#TPs
#All
#TPs+#FPs
#TPs+#FNs
SVM Linear Kernel
SVM Polynomial Kernel
Bagged Decision Trees
LDA Linear
LDA Diagonal Linear
LDA Quadratic Linear
LDA Diagonal Quadratic
LDA Mahalanobis
Naive Bayes (Gaussian)
AdaBoost
J48 Decision Tree (C45)
96.72%
99.59%
96.27%
94.65%
89.54%
94.77%
92.50%
94.46%
92.60%
92.56%
92.20%
97.31%
99.66%
96.64%
96.88%
96.62%
95.67%
95.67%
99.33%
94.78%
92.79%
93.84%
97.61%
99.83%
97.61%
94.75%
86.80%
96.25%
92.57%
91.97%
93.73%
96.09%
94.11%
Table 4: Accuracy of classiﬁcation algorithms for initially
labeled cases.
bad PIP addresses, the majority of the IP addresses origi-
nate from two regions of the address space (64.0-96.255 and
186.0-202.255). Somewhat surprisingly, the distribution of
the bad PIP addresses is similar to that of the dynamic IP
addresses reported by Dynablock [28]. This observation sug-
gests that attackers are increasingly mounting their attacks
from dynamically assigned IP addresses, which likely corre-
spond to end-user machines that have been compromised.
To validate this hypothesis, we perform rDNS lookups to
check if the domain name of a PIP address contains keywords
that suggest the corresponding IP address as dynamic, e.g.,
dhcp, dsl, and dyn; we observe that 51.3% of the bad PIPs
have at least one of these keywords.
4.2 Accuracy Evaluation of Labeled Cases
Among 1.7 million PIP addresses, 973K of them can be
labeled based on the account reputation data. We train
our classiﬁer using the data sampled from these labeled PIP
addresses. For the training data, we use the primitive good
and bad PIP ratio, around 1:0.17. Note that, this ratio is
not critical: we tried a few other very diﬀerent ratios such
as 1:1 and 1:6, and they give us similar results.
Classiﬁcation Accuracy: Using 50,000 samples with 5-
fold cross validation, we compare the accuracy of our classi-
ﬁcation in Table 4, using diﬀerent learning algorithms based
on the labeled PIPs.
In general, we observe that all the
classiﬁcation algorithms achieve accuracies >89%. In par-
ticular, the SVM classiﬁer with a polynomial kernel pro-
vides the best overall performance. We observed that the
non-linear kernel of SVM outperforms the linear kernel, but
at a cost of higher training time. Other classiﬁcation al-
gorithms such as linear discriminant analysis (LDA), naive
Bayes, AdaBoost, and J48 decision trees all have very com-
petitive performance. All these algorithms can serve as a
building block of the PIPMiner system. Since SVM with a
polynomial kernel provides the best result, we choose it in
our implementation of the system.
Accuracy of Individual Components: Our classiﬁca-
tion framework relies on a broad set of features to achieve a
high detection accuracy. To understand the importance of
the features, we train a classiﬁer on each single feature and
Table 5 shows the performance breakdown of diﬀerent types
of features used for the training. Leveraging the account
request distribution provides the best accuracy among the
individual features, while the combination of more features
does help reduce the classiﬁcation error signiﬁcantly.
Feature
w/ Block-level
w/o Block-level
Accuracy Precision Accuracy Precision
Population Features
Population Size
Request Size
Requests per Account
New Account Rate
Account Request Dist.
Account Stickiness Dist.
Time Series Features
On/oﬀ Period
Diurnal Pattern
Weekly Pattern
Account Binding Time
Inter Request Time
Predictability
All Features
Previous Features
97.8%
92.6%
82.9%
89.2%
86.3%
94.2%
88.1%
96.1%
86.1%
85.8%
90.6%
90.8%
92.5%
90.8%
99.6%
93.7%
98.2%
92.6%
82.4%
89.1%
86.7%
94.0%
88.3%
97.2%
89.2%
85.1%
88.8%
90.2%
92.3%
89.6%
99.7%
93.5%
96.3%
90.5%
79.8%
89.2%
85.5%
93.6%
88.2%
95.5%
84.0%
85.5%
89.6%
90.3%
91.8%
86.8%
98.3%
92.7%
96.8%
90.5%
77.8%
88.9%
85.3%
93.4%
88.2%
96.4%
88.0%
85.1%
87.8%
90.3%
91.1%
82.6%
99.1%
93.1%
Table 5: Classiﬁcation accuracy when trained on one type
of feature. Account request distribution, population size,
and inter request time provide the best individual accuracy.
The classiﬁer achieves the best performance when combin-
ing all types of features. The last row presents the perfor-
mance when trained using the previously proposed features
(account/request volume, account stickiness and diurnal pat-
tern).
Comparison with Previous Proposed Features:
In
Table 5, we demonstrate the eﬀectiveness of our features by
providing a comparison of our results to the classiﬁcation re-
sults obtained by using the previously proposed features for
the training. The previous features include account/request
volume, account stickiness, and diurnal pattern. We observe
that our feature sets provide substantial performance gains
over the previous features, improving the detection accuracy
from 92.7% to 99.6%.
Accuracy against Data Length: To quickly react to
malicious attacks, the classiﬁer needs to accurately diﬀer-
entiate bad PIPs from good ones based on as little activity
history as possible. Figure 6a shows the classiﬁcation results
by forcing the classiﬁer to train and test on the most recent
k requests per PIP. A small value of k may yield biased
features, resulting in a lower classiﬁcation accuracy. Never-
theless we observe that the classiﬁer requires only k = 12
requests per PIP1 to achieve a very high accuracy. This
indicates that our system could eﬀectively identify newly
emerged attacks.
Comparison with History-based Approach: Given
that our classiﬁcation framework works very well even with
access to limited/partial history information, one could ask:
do we need machine learning techniques to distinguish good
PIPs from bad ones? To answer this question, we evaluate a
baseline algorithm that uses only the historical ground truth
to predict the PIP labels. For each sampled PIP from the
pool of the labeled PIPs, we construct a time series of the
reputation in a 1-day precision. Then we use an exponen-
tial weighted moving average (EWMA) model for reputation
prediction. In particular, the reputation is predicted from
the weighted average of all the past data points in the time
1Although the time series features become less useful with
the decrease of k (due to the incomplete view in the time
domain), PIPMiner can still achieve fairly high accuracy by
combining the population features with the block-level time
series features.
335(a)
(b)
(c)
(d)