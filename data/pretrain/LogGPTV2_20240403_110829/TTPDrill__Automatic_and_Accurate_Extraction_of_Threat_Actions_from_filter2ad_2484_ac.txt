-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
8
V. Legoy et al.
Table 3: Classiﬁcation results for techniques prediction. Abbreviations: CC =
Classiﬁer Chain, BR = Binary relevance, DT = Decision Tree, T = Tree, NB =
Naive Bayes
Without resampling
With resampling
Micro
Precision Recall
F0.5
9.57% 2.51% 6.11% 0.05% 0.48% 0.06%
Macro
F0.5 Precision Recall
Micro
Precision Recall
Macro
Precision Recall
F0.5
F0.5
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Majority Baseline
Term Frequency
36.64% 13.27% 27.05% 18.38% 8.98% 13.72%
CC Adaboost DT
39.95% 5.83% 18.24% 18.50% 7.90% 12.32%
CC Bagging DT
14.82% 25.59% 16.16% 10.88% 23.40% 11.66%
CC Ridge Classiﬁer
22.44% 17.94% 21.31% 18.64% 14.98% 16.46%
CC Decision Tree
35.60% 16.04% 28.56% 18.23% 9.74% 14.23% 26.44% 23.02% 22.79% 14.65% 16.16% 12.45%
BR AdaBoost DT
39.30% 7.63% 21.42% 16.92% 7.53% 11.88% 26.99% 12.79% 16.81% 12.34% 15.43% 10.75%
BR Bagging DT
BR Gradient T Boosting 27.60% 12.67% 22.31% 20.25% 12.72% 16.26% 18.47% 20.32% 16.73% 14.80% 24.51% 13.96%
14.96% 25.96% 16.33% 10.98% 23.50% 11.74% 11.40% 31.83% 12.83% 9.05% 32.50% 10.04%
BR Ridge Classiﬁer
22.87% 19.22% 21.98% 15.36% 11.39% 13.41% 17.85% 24.59% 17.97% 12.85% 17.01% 11.95%
BR Linear SVC
23.06% 18.53% 21.94% 18.00% 14.76% 16.18% 17.46% 24.90% 17.11% 14.70% 21.66% 13.61%
BR Decision Tree
16.23% 14.06% 15.71% 9.99% 8.27% 9.00%
Adapted Decision Tree
Adapted Extra Tree
12.86% 10.10% 12.19% 6.33% 4.95% 5.55%
Term Frequency-Inverse Document Frequency
37.06% 13.06% 26.98% 17.70% 8.44% 13.19%
CC Adaboost DT
23.18% 18.47% 22.02% 18.83% 14.85% 16.77%
CC Decision Tree
BR AdaBoost DT
35.04% 14.77% 27.41% 17.23% 9.05% 13.36% 27.53% 18.12% 22.32% 14.55% 13.00% 11.85%
BR Gradient T Boosting 25.85% 11.60% 20.72% 18.83% 12.09% 15.21% 19.45% 16.76% 17.10% 15.33% 21.69% 14.38%
52.82% 3.66% 14.33% 7.86% 2.76% 5.18% 42.05% 4.98% 12.41% 7.53% 4.79% 5.64%
Logistic Regression
30.45% 18.26% 26.82% 21.89% 14.61% 18.32% 25.16% 22.56% 23.03% 19.50% 21.29% 17.32%
BR Perceptron
37.18% 29.79% 35.02% 28.84% 22.67% 25.06% 31.16% 32.86% 29.37% 26.27% 28.05% 22.87%
BR Linear SVC
BR Decision Tree
20.72% 18.31% 20.15% 16.88% 14.57% 15.55% 17.44% 22.91% 17.02% 15.45% 20.21% 14.21%
14.11% 12.19% 13.65% 8.14% 7.19% 7.44%
Adapted Decision Tree
Adapted Extra Tree
12.54% 9.83% 11.85% 5.70% 4.68% 5.15%
Word2Vec average
CC Adaboost DT
CC Linear SVC
BR AdaBoost DT
BR Perceptron
BR Bernoulli NB
BR Linear SVC
Adapted Extra Tree
Word2Vec sum
27.88% 7.34% 17.68% 8.77% 3.47% 6.14%
CC Adaboost DT
BR AdaBoost DT
28.34% 11.69% 21.93% 9.63% 4.50% 7.30%
BR Gradient T Boosting 21.14% 8.41% 16.14% 7.38% 2.96% 5.00%
BR Logistic Regression
20.49% 18.60% 19.98% 11.48% 10.53% 10.62%
16.39% 18.52% 16.77% 6.27% 7.96% 5.65%
BR Perceptron
9.98% 49.20% 11.86% 5.10% 23.51% 5.93%
BR Bernoulli NB
11.64% 22.82% 12.87% 8.30% 15.49% 8.42%
BR Linear SVC
Adapted Decision Tree
513.39% 14.23% 13.51% 5.63% 5.73% 5.42%
29.88% 9.29% 20.67% 9.35% 4.05% 6.70%
36.21% 3.33% 11.70% 7.58% 3.60% 5.35%
26.88% 11.79% 21.36% 9.15% 4.82% 7.31%
17.43% 15.88% 17.03% 8.33% 7.80% 6.37%
10.54% 47.77% 12.47% 5.21% 22.33% 6.06%
49.01% 6.33% 20.70% 8.57% 3.27% 5.64%
12.53% 14.41% 12.85% 5.17% 5.75% 5.05%
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
When we compare the text representation methods, we can observe that,
overall, models using Word2Vec either with a sum or an averaging of the word
vectors, underperformed compared to the ones using a TF or TF-IDF weighting
system. We observed minor diﬀerences in the use of the average against the sum
of the vector in the Word2Vec approach, but we cannot claim that one way is
better than the other as the results depend on the employed classiﬁer.
Considering the results of the diﬀerent classiﬁers, we can see that adapted al-
gorithms tend to underperform compared to the classiﬁer chains and the binary
relevance models, for both tactics and techniques predictions. Overall, the clas-
siﬁcation made with binary relevance instead of classiﬁer chain performs slightly
better, which means that the relationship between labels did not have as much
impact as expected.
Some classiﬁers stand out among the others for each type of labels. For
the classiﬁcation by tactics, the AdaBoost Decision Tree, the Gradient Tree
Automated Retrieval of TTPs from Threat Reports
9
Boosting, Perceptron and the Linear SVC in classiﬁer chains or binary relevance
models have the best performance, independently from using TF-IDF or TF. The
Bagging Decision tree, the Ridge classiﬁer and the Logistic Regression perform
well when used in binary relevance. They also perform well in a classiﬁer chain if
Logistic Regression and the Bagging Decision Tree classify text use TF weighting
and the Ridge classiﬁer use TF-IDF. Similar models work as well for techniques
prediction, in addition to the Decision Tree classiﬁer with binary relevance or
classiﬁer chain models.