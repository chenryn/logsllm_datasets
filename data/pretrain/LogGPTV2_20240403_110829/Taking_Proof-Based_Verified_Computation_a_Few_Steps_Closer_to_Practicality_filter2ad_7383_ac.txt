integers, Z, or the rationals, Q) to equivalent constraints
over a ﬁnite ﬁeld, the programmer or compiler performs
three steps, as illustrated and described below:
Ψ over D (C1)−−−−→ Ψ over U (C2)−−−−→ θ(Ψ) over F
(cid:121)(C3)
C over F
3We suspect that many of the individual techniques are known. How-
ever, when the techniques combine, the material is surprisingly hard
to get right, so we will delve into (excruciating) detail, consistent with
our focus on built systems.
5
C1 Bound the computation. Deﬁne a set U ⊂ D and re-
strict the input to Ψ such that the output and interme-
diate values stay in U.
C2 Represent the computation faithfully in a suitable ﬁ-
nite ﬁeld. Choose a ﬁnite ﬁeld, F, and a map θ : U →
F such that computing θ(Ψ) over θ(U) ⊂ F is iso-
morphic to computing Ψ over U. (By “θ(Ψ)”, we
mean Ψ with all inputs and literals mapped by θ.)
C3 Transform the ﬁnite ﬁeld version of the computation
into constraints. Write a set of constraints over F that
are equivalent (in the sense of Section 2.1) to θ(Ψ).
(cid:80)m
4.1 Signed integers and ﬂoating-point rationals
We now instantiate C1 and C2 for integer and rational
number computations; the next section addresses C3.
Consider m × m matrix multiplication over N-bit
signed integers. For step C1, each term in the output,
k=1 AikBkj, has m additions of 2N-bit subterms so is
contained in [−m · 22N−1, m · 22N−1); this is our set U.
For step C2, take F = Z/p (the integers mod a prime
p, to be chosen shortly) and deﬁne θ : U → Z/p as
θ(u) = u mod p. Observe that θ maps negative integers
2 , . . . , p − 1}, analogous to how processors
to { p+1
represent negative numbers with a 1 in the most signiﬁ-
cant bit (this technique is standard [17, 50]). Of course,
addition and multiplication in Z/p do not “know” when
their operands are negative. Nevertheless, the compu-
tation over Z/p is isomorphic to the computation over
U, provided that |Z/p| > |U| (as shown in Appendix
B [46]).4 Thus, for the given U, we require p > m · 22N.
Note that a larger p brings larger costs (see Figure 2), so
there is a three-way trade-off among p, m, N.
2 , p+3
We now turn to rational numbers. For step C1, we re-
strict the inputs as follows: when written in lowest terms,
their numerators are (Na + 1)-bit signed integers, and
their denominators are in {1, 2, 22, 23, . . . , 2Nb}. Note
that such numbers are (primitive) ﬂoating-point num-
bers: they can be represented as a · 2−q, so the decimal
point ﬂoats based on q. Now, for m×m matrix multiplica-
tion, the computation does not “leave” U = {a/b: |a|  (m + 1)2 ·
24(Na+Nb) sufﬁces.
Limitations and costs. To understand the limitations
of GINGER’s ﬂoating-point representation, consider the
number a · 2−q, where |a| = that is satisﬁable if and only if x1 ≥ x2.
Finally, we introduce a 0/1 variable M that encodes
a choice of branch, and then arrange for M to “pull in”
the constraints of that branch and “exclude” those of the
other. (Note that the prover need not execute the untaken
branch.) Figure 3 depicts the complete set of constraints,
CΨ; these constraints are satisﬁable if and only if the
prover correctly computes Ψ [46, Appendix C].
Logical expressions and conditionals. Besides order
comparisons and if-else, GINGER can represent ==, &&,
and || as constraints. An interesting case is !=: we can
represent Z1!=Z2 with {M · (Z1 − Z2) − 1 = 0} because
this constraint is satisﬁable when (Z1 − Z2) has a multi-
plicative inverse and hence is not zero. These constructs
and others are detailed in Appendix D [46].
Ψ :
if (X1 =},
(1 − M)(Y − 4) = 0
Figure 3—Pseudocode for our case study of Ψ, and corresponding constraints CΨ. Ψ’s inputs are signed integers x1, x2; per steps
C1 and C2 (§4.1), we assume x1 − x2 ∈ U ⊂ [−2N−1, 2N−1), where p > 2N. The constraints C< test x1 < x2 by testing whether the
bits of θ(x1) − θ(x2) place it in [p − 2N−1, p). M{C} means multiplying all constraints in C by M and then reducing to degree-2.
Limitations and costs. We compile a subset of SFDL,
the language of the Fairplay compiler [39]. Thus, our
limitations are essentially those of SFDL; notably, loop
bounds have to be known at compile time.
hardware in the context of [21]). We exploit three levels
of parallelism here. First, the prover performs a cipher-
text operation for each component in the commitment
vector (§2.3); each operation is (to ﬁrst approximation)
separate. Second, each operation computes two indepen-
dent modular exponentiations (the ciphertext of an ElGa-
mal encryption has two elements). Third, modular expo-
nentiation itself admits a parallel implementation (each
input is a multiprecision number encoded in multiple ma-
chine words). Thus, in our GPU implementation, a group
of CUDA [1] threads computes each exponentiation.
We also parallelize the veriﬁer’s encryption work dur-
ing the commitment phase (§2.3), using the approach
above plus an optimization: the veriﬁer’s exponentiations
are ﬁxed base, letting us memoize intermediate squares.
We implement exponentiations for the prover and veri-
ﬁer with the libgpucrypto library of SSLShader [36],
modiﬁed to implement the memoization.
Implementation details. Our compiler consists of two
stages, which a future publication will detail. The front-
end compiles a subset of Fairplay’s SFDL [39] to con-
straints; it is derived from Fairplay and is implemented
in 5294 lines of Java, starting from Fairplay’s 3886 lines
(per [51]). The back-end transforms constraints into C++
code that implements the veriﬁer and prover and then in-
vokes gcc; this component is 1105 lines of Python code.
For efﬁciency, PEPPER [45] introduced specialized
PCP protocols for certain computations. For some exper-
iments we use specialized PCPs in GINGER also; in these
cases we write the prover and veriﬁer manually, which
typically requires a few hundred lines of C++. Automat-
ing the compilation of specialized PCPs is future work.
The veriﬁer and prover are separate processes that ex-
change data using Open MPI [2]. GINGER uses the El-
Gamal cryptosystem [23] with 1024-bit keys.
6 Experimental evaluation
Our evaluation answers the following questions:
• What is the effect of the protocol reﬁnements (§3)?
• What are the costs of supporting rational numbers and
the additional program structures (§4)?
• What is GINGER’s speedup from parallelizing (§5)?
Figure 4 summarizes the results.
7
How efﬁcient is our representation? The program con-
structs above mostly have concise constraint representa-
tions. Consider, for instance, comp1==comp2; the equiv-
alent constraint set C consists of the constraints that rep-
resent comp1, the constraints that represent comp2, and
an additional constraint to relate the outputs of comp1
and comp2. Thus, C is the same size as its two compo-
nents, as one would expect.
However, two classes of computations are costly. First,
inequality comparisons require variables and a con-
straint for every bit position; see Figure 3. Second, the
constraints for if-else and ||, as written, seem to be
degree-3; notice, for instance, the M{C} in Figure 3. To
be compatible with the core protocol, these constraints
must be rewritten to be degree-2 (§2.1), which carries
costs. Speciﬁcally, if C has s variables and χ constraints,
an equivalent degree-2 representation of M{C} has s + χ
variables and 2 · χ constraints [46, Appendix D].
5 Parallelization and implementation
Many of GINGER’s remaining costs are in the crypto-
graphic operations in the commitment protocol (see Ap-
pendix A.1). To address these costs, we distribute the
prover over multiple machines, leveraging GINGER’s in-
herent parallelism. We also implement the prover and
veriﬁer on GPUs, which raises two questions. (1) Isn’t
this just moving the problem? Yes, and this is good:
GPUs are optimized for the types of operations that bot-
tleneck GINGER. (2) Why do we assume that the veriﬁer
has a GPU? Desktops are more likely than servers to have
GPUs, and the prevalence of GPUs is increasing. Also,
this setup models a future in which specialized hardware
for cryptographic operations is common.
Parallelization. To distribute GINGER’s prover, we run
multiple copies of it (one per host), each copy receiving
a fraction of the batch (Section 2.3). In this conﬁgura-
tion, the provers use the Open MPI [2] message-passing
library to synchronize and exchange data.
To further reduce latency, each prover ofﬂoads work
to a GPU (see also [49] for an independent study of GPU
GINGER’s protocol reﬁnements reduce per-instance network costs by 25–30× (to hundreds of KBs for the computations
we study), prover CPU costs by about 10–14% (leaving them still high), and break-even batch size (β∗) by about 4×.
With accelerated encryption GINGER breaks even from outsourcing short computations at small batch sizes; for 400×400
matrix multiplication, the veriﬁer gains from outsourcing at a batch size of 20 (tens of seconds of computation).
Rational arithmetic costs roughly 3× integer arithmetic under GINGER (but much more than native ﬂoating-point).
Parallelizing results in near-linear reduction in the prover’s latency.
§6.1
§6.1
§6.2
§6.3
Figure 4—Summary of main evaluation results.
computation (Ψ)
matrix mult.
matrix mult. (Q)
deg-2 poly. eval.
deg-3 poly. eval.
m-Hamming dist.
bisection method
O(·)
O(m3)
O(m3)
O(m2)
O(m3)
O(m2)
O(m2)
input domain (see §4.1)
32-bit signed integers
rationals (Na = 32, Nb = 32)