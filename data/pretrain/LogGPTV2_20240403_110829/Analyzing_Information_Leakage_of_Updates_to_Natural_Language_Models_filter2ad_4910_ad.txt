42.63
Table 4: Relative differential score of phrases found by beam search when retraining from scratch and continuing training from
a previous model. The results are for RNN models trained on partitions of the Reddit dataset with 𝑁 = talk.politics.mideast.
Cells for which continued training yields a higher score than retraining appear in bold font. Capitalization added for emphasis.
Phrase (# of occurrences in 𝑁 )
|𝐷extra|/|𝐷orig |
Perplexity decrease
Center for Policy Research (93)
Troops surrounded village after (12)
Partition of northern Israel (0)
West Bank peace talks (0)
Spiritual and political leaders (0)
Saudi troops surrounded village (0)
Arab governments invaded Turkey (0)
Little resistance was offered (12)
Buffer zone aimed at protecting (0)
Capital letters racial discrimination (0)
Retraining
5%
1.17
101.38
44.50
16.81
25.64
25.98
24.31
22.62
22.09
4.47
3.32
10%
2.45
97.11
44.50
38.48
25.69
17.04
24.31
22.80
25.12
5.30
3.40
20%
3.82
98.65
44.41
26.10
25.71
24.21
24.31
22.78
22.34
5.25
3.60
100%
11.82
91.53
44.54
38.76
25.75
23.47
24.30
22.80
25.59
5.69
3.84
0%
0.79
99.77
44.50
27.61
25.68
25.23
24.31
22.59
22.24
4.00
3.76
0%
73.97
276.98
173.95
68.98
71.54
126.92
5.05
24.01
215.16
57.29
94.60
Continued Training
5%
18.45
10%
10.29
20%
6.08
198.69
47.38
16.48
24.38
14.91
44.58
15.58
25.02
69.76
52.74
150.56
19.48
12.47
28.60
10.00
4.29
7.08
2.00
18.92
39.11
122.25
7.81
22.93
16.91
3.44
7.29
18.12
3.30
14.50
11.22
100%
8.28
117.54
35.56
18.82
4.62
11.05
63.84
11.90
5.64
22.25
3.45
as more additional data is used; in some cases continued training
yields scores lower than when retraining a model on the same data.
a more expensive search, using wider beams or looking more than
one token ahead to better approximate the true rank of a phrase.
RQ3: Effect of background knowledge. An adversary wishing to
extract information about the dataset used to update a language
model may direct a search using as prompt a known prefix from
the dataset. We study how long this prefix needs to be to recover
the rest of phrase.
We consider a RNN model 𝑀 trained on the full Reddit dataset
and a model 𝑀′ trained on the union of the full Reddit dataset
and all messages of the talk.politics.mideast newsgroup. We
sample 4 phrases in newsgroup messages beginning with the name
of a Middle Eastern country and containing only tokens in the
model vocabulary. We believe it is feasible for the adversary to
guess these prefixes from the description of the newsgroup or the
geopolitical context. For each phrase 𝑠 and 𝑖 = 0, . . . , |𝑠|−1 we run a
(cid:102)DS-based beam search for phrases of the same length with constant
beam width 10,000 and 100 groups starting from 𝑠1 . . . 𝑠𝑖. Table 5
shows the rank of 𝑠 among the search results (or ∞ if absent).
We observe a correlation between the score of a phrase and
the minimum prefix sufficient to recover it. However, a dip in the
score of two consecutive tokens is much more consequential: a
common word like the, which has a similar distribution in the
original and private datasets, contributes little to the score of a
phrase and is unlikely to be picked up as a candidate extension in a
beam search. Recovering from this requires additional heuristics or
5 CHARACTERIZING THE SOURCE OF
LEAKAGE
Prior work has primarily studied information leakage when an
attacker has only access to a single model snapshot. Here, we first
analyze how much our analysis gains from having access to two
model snapshots, and then consider the influence of common causes
of leakage in the single-model case. The central ones are overfit-
ting [32] to the training data, and unintended memorization [6] of
data items that is independent of the distribution to be learned.
RQ4: How important is access to a second model snapshot? We
want to analyze how much leakage of sensitive information is
increased when having access to two model snapshots 𝑀𝐷, 𝑀𝐷′
in contrast to having only access to a single model 𝑀𝐷′. This is
a challenging analysis in a realistic setting, due to the size of the
data and the lack of an easily computable metric for information
leakage. Concretely, we want to show that the data we can extract
using the differential analysis of 𝑀𝐷 and 𝑀𝐷′ is (a) more likely to
be part of 𝐷′ than of 𝐷, (b) not very common in 𝐷′, and (c) that
(a) and (b) are more true for the results of the differential analysis
than for the analysis of 𝑀𝐷′ alone.
We quantify how likely a given sentence is to be a part of a dataset
using a simpler, well-understood model of natural language data,
Analyzing Information Leakage of Updates to Natural Language Models
CCS ’20, November 9–13, 2020, Virtual Event, USA
Table 5: Results of beam searches for different prefix lengths. A rank of 0 means that the search recovers the complete phrase.
Due to the heuristic nature of the search the rank reported may be lower than the true rank of 𝑠. Conversely, a beam search
may not encounter 𝑠 at all despite having lower rank than most phrases encountered. For instance, this occurs for Turkey
searched an American plane, where all but 7 search results with no prompt have higher rank (lower score).
# of occurrences (cid:102)DS (𝑠)
Prefix length 𝑖
Phrase 𝑠
Turkey searched an American plane
Israel allows freedom of religion
Iraq
government
Israel sealed off the occupied lands
elected
with
an
6
3
2
2
0
2
1
82.96 ∞ 1
1
24.44 ∞ ∞ 788
23.75 ∞ ∞ ∞
6.48
4
0
0
0
∞ ∞ ∞ ∞ 3442
3
0
55
4
5
–
–
–
2
namely an 𝑛-gram model. 𝑛-gram models define the probability of
a token 𝑡𝑛+1 appearing after a sequence of tokens 𝑡1 . . . 𝑡𝑛 as the
number of times 𝑡1 . . . 𝑡𝑛𝑡𝑛+1 appeared in the dataset divided by the
number of times 𝑡1 . . . 𝑡𝑛 appeared.
In our experiments, we use the perplexity of 3-gram models
trained on 𝐷 (resp. 𝑁 ) to capture how likely a given extracted
sentence is part of the dataset 𝐷 (resp. 𝑁 ). We compare these per-
plexity values for sequences extracted using group beam search
from the models 𝑀𝐷 (resp. 𝑀𝐷′) and for sequences extracted using
our differential rank-based search, following the setup of Section
4.5. Concretely, we used the entire Reddit comment data as dataset
𝐷, and the messages 𝑁 from talk.politics.mideast as data up-
date. We are concerned with information an attacker can gain about
the contents of 𝑁 .
Figure 2a shows the results of our analysis when we train 𝑀𝐷′
on 𝐷′ = 𝐷 ∪ 𝑁 from scratch. Points above the main diagonal are
closer in distribution to the (private) data update 𝑁 than to the
base data 𝐷. This shows that our attack extracts sequences using
differential score (represented by red crosses) that are more likely to
be part of 𝑁 than of 𝐷, and that these sequences differ substantially
from the sequences obtained by a single-model analysis. In fact,
the sequences obtained by single-model analysis for 𝑀𝐷 and 𝑀𝐷′
show little significant difference. Note that the perplexity values
perp3-gram(𝐷) are very high for some of the extracted sentences, as
they use combinations of tokens that never appear in the original
training dataset 𝐷. Similarly, Figure 2b shows the results of this
analysis on the scenario in which we obtain 𝑀𝐷′ by specializing
the model 𝑀𝐷 by continuing training on the dataset 𝑁 . While our
differential analysis again captures sequences more likely to be part
of the updated data 𝑁 than of the original data 𝐷, the single-model
analysis now also shows some of this effect.
RQ5: Is leakage due to overfitting or intended memorization? All
models are trained using an early-stopping criterion that halts
training when the model does not improve on a separate validation
set. This effectively rules out overfitting to the training data. Addi-
tionally, model training employs regularization strategies such as
dropout to further encourage the trained models to generalize to
unseen data.
We refer to the model’s ability to reproduce verbatim fragments
of the training data as memorization and call it intended if this is
necessary to serve its purpose of generating natural language (e.g.,
a model needs to memorize the token pair “United States”, as it is
an extremely common combination) and unintended otherwise.
In the experimental results in Table 4, we have included the num-
ber of times that the phrases with the highest differential scores
appear in the update dataset. Since some of these phrases do not
appear verbatim, we also measure how close these phrases are to
phrases in the original and update datasets. Table 6 shows the Lev-
enshtein distance of extracted phrases from Table 4 to their nearest
neighbor in either dataset. Generally, we find closer matches in the
update dataset. While “Center for Policy Research” is a clear case of
intended memorization, as the name appears many times in email
signatures, other phrases appear rarely or never, indicating that our
analysis extracts phrases that need not be memorized to serve its
purpose. This is further supported by the results in Table 5, where