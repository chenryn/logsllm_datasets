Performing Routine Maintenance with Ansible Chapter 12
Now, we can very clearly see that there is an issue on our host, and it will be reported as a
failure in AWX and Ansible Tower too.
Of course, this works very well for plain text files. What about binary files, though? Ansible
is, of course, not a complete replacement for a file integrity monitoring tool such as
Advanced Intrusion Detection Environment (AIDE) or the venerable Tripwire—however,
it can help with the use of binary files too. In fact, the process is very simple. Let's suppose
you want to ensure the integrity of /bin/bash—this is the shell that everyone uses by
default on most systems, so the integrity of this file is incredibly important. If you have
space to store a copy of the original binary on your Ansible server, then you can use the
copy module to copy it across to the target hosts. The copy module makes use of
checksumming to determine whether a file needs to be copied, and so, you can be sure that,
if the copy module results in a changed result, then the target file differs from your
original version, and integrity is compromised. The role code for this would look very
similar to our template example here:
---
- name: Copy bash binary to target host
copy:
src: files/bash
dest: /bin/bash
owner: root
group: root
mode: 0755
register: copy_result
failed_when: (copy_result.changed and ansible_check_mode == True) or
copy_result.failed
Of course, storing original binaries on your Ansible server is inefficient, and also, means
you have to keep them up to date, in line with your server patching schedule, which is not
desirable when you have a large number of files to check. Fortunately, the
Ansible stat module can generate checksums, as well as returning lots of other useful data
about files, and so, we could very easily write a playbook to check that our binary for Bash
has not been tampered with, by running the following code:
---
- name: Get sha256 sum of /bin/bash
stat:
path: /bin/bash
checksum_algorithm: sha256
get_checksum: yes
register: binstat
- name: Verify checksum of /bin/bash
fail:
[ 335 ]
Performing Routine Maintenance with Ansible Chapter 12
msg: "Integrity failure - /bin/bash may have been compromised!"
when: binstat.stat.checksum !=
'da85596376bf384c14525c50ca010e9ab96952cb811b4abe188c9ef1b75bff9a'
This is a very simple example and could be enhanced significantly by ensuring the file path
and name, and checksum, are variables rather than static values. It could also be made to
loop over a dictionary of files and their respective checksums—these tasks are left as an
exercise for you, and this is entirely possible, using techniques we have covered throughout
this book. Now, if we run this playbook (whether in check mode or not), we will see a failed
result if the integrity of Bash has not been maintained, and ok otherwise, as follows:
Checksumming can be used to verify the integrity of configuration files too, so, this
example role serves as a good basis for any file integrity checking you might wish to
undertake.
We have now completed our exploration of file and integrity monitoring with Ansible, and
hence, the ability check for configuration drift. In the next section of this chapter, we'll take
a look at how Ansible can be used to manage processes across an Enterprise Linux estate.
[ 336 ]
Performing Routine Maintenance with Ansible Chapter 12
Understanding process management with
Ansible
Sooner or later, you will end up with the need to manage, and possibly even kill, processes
on one or more Linux servers within your enterprise. Obviously, this is not an ideal
scenario, and in day-to-day operations, most services should be managed using the Ansible
service module, many examples of which we have seen in this book.
What if, however, you need to actually kill a service that has hung? Obviously, a system
administrator could SSH into the errant server and issue commands such as the following:
$ ps -ef | grep  | grep -v grep | awk '{print $2}'
$ kill  
If the process refuses stubbornly to terminate, then the following may become necessary:
$ kill -9  
While this is a fairly standard practice, in which most system administrators will be well
versed (and indeed, may have their own favorite tools to handle, such as pkill), it suffers
the same problem as most manual interventions on a server—how can you keep track of
what happened, and which processes were affected? If numeric process IDs (PIDs) were
used, then even with access to the command history, it is still impossible to tell which
process historically held that numeric PID.
What we propose here is an unconventional use of Ansible—yet one that, if run through a
tool such as AWX or Ansible Tower, would enable us to track all operations that were
performed, along with details of who ran them and, if we put the process name in a
parameter, what the target was too. This could be useful if, in the future, it becomes
necessary to analyze the history of a problem, whereupon it would be easy to check which
servers were acted upon, and which processes were targeted, along with precise
timestamps.
[ 337 ]
Performing Routine Maintenance with Ansible Chapter 12
Let's build up a role to perform exactly this set of tasks. This chapter was originally written
against Ansible 2.8, which did not feature a module for process management, and so, the
following example uses native shell commands to handle this case:
1. We start by running the process listing we proposed earlier in this section, but
this time, registering the list of PIDs into an Ansible variable, as follows:
---
- name: Get PID's of running processes matching {{ procname }}
shell: "ps -ef | grep -w {{ procname }} | grep -v grep | grep -v
ansible | awk '{print $2\",\"$8}'"
register: process_ids
Most people familiar with shell scripting should be able to understand this
line—we are filtering the system process table for whole-word matches for the
Ansible variable procname, and removing any extraneous process names that
might come up and confuse the output, such as grep and ansible. Finally, we
use awk to process the output into a comma-separated list, containing the PID, in
the first column, and the process name itself in the second.
2. Now, we must start to take action on this output. We now loop over the
process_ids variable populated previously, issuing a kill command against
the first column in the output (that is, the numeric PID), as follows:
- name: Attempt to kill processes nicely
shell: "kill {{ item.split(',')[0] }}"
loop:
"{{ process_ids.stdout_lines }}"
loop_control:
label: "{{ item }}"
You will observe the use of Jinja2 filtering here—we can use the built-in split
function to split the data we created in the previous code block, taking only the
first column of output (the numeric PID). However, we use the loop_control
label to set the task label containing both the PID and process name, which could
be very useful in an auditing or debugging scenario.
[ 338 ]
Performing Routine Maintenance with Ansible Chapter 12
3. Any experienced system administrator will know that it is not sufficient to just
issue a kill command to a process—some processes must be forcefully killed as
they are hung. Not all processes exit immediately, so we will use the Ansible
wait_for module to check for the PID in the /proc directory—when it becomes
absent, then we know the process has exited. Run the following code:
- name: Wait for processes to exit
wait_for:
path: "/proc/{{ item.split(',')[0] }}"
timeout: 5
state: absent
loop:
"{{ process_ids.stdout_lines }}"
ignore_errors: yes
register: exit_results
We have set the timeout here to 5 seconds—however, you should set it as
appropriate in your environment. Once again, we register the output to a
variable—we need to know which processes failed to exit, and hence, try killing
them more forcefully. Note that we set ignore_errors here, as the wait_for
module produces an error if the desired state (that is, /proc/PID becomes
absent) does not occur within the timeout specified. This should not be an error
in our role, simply a prompt for further processing.
4. We now loop over the results of the wait_for task —only this time, we use the
Jinja2 selectattr function, to select only dictionary items that have failed
asserted; we don't want to forcefully terminate non-existent PIDs. Run the
following code:
- name: Forcefully kill stuck processes
shell: "kill -9 {{ item.item.split(',')[0] }}"
loop:
"{{ exit_results.results | selectattr('failed') | list }}"
loop_control:
label: "{{ item.item }}"
Now, we attempt to kill the stuck processes with the -9 flag—normally, sufficient
to kill most hung processes. Note again the use of Jinaj2 filtering and the tidy
labeling of the loop, to ensure we can use the output of this role for auditing and
debugging.
[ 339 ]
Performing Routine Maintenance with Ansible Chapter 12
5. Now, we run the playbook, specifying a value for procname—there is no default
process to be killed, and I would not suggest that setting a default value for this
variable is safe. Thus, in the following screenshot, I am setting it using the -e flag
when I invoke the ansible-playbook command:
From the preceding screenshot, we can clearly see the playbook killing the mysqld process,
and the output of the playbook is tidy and concise, yet contains enough information for
debugging, should the need occur.
As an addendum, if you are using Ansible 2.8 or later, there is now a native Ansible
module called pids that will return a nice, clean list of PIDs for a given process name, if it
is running. Adapting our role for this new functionality, we can, first of all, remove the
shell command and replace it with the pids module, which is much easier to read, like this:
---
- name: Get PID's of running processes matching {{ procname }}
pids:
name: "{{ procname }}"
register: process_ids
[ 340 ]
Performing Routine Maintenance with Ansible Chapter 12
From this point on, the role is almost identical to before, except that, rather than the
comma-separated list we generated from our shell command, we have a simple list that just
contains the PIDs for each running process that matches the procname variable in name.
Thus, we no longer need to use the split Jinja2 filter on our variables when executing
commands on them. Run the following code:
- name: Attempt to kill processes nicely
shell: "kill {{ item }}"
loop:
"{{ process_ids.pids }}"
loop_control:
label: "{{ item }}"
- name: Wait for processes to exit
wait_for:
path: "/proc/{{ item }}"
timeout: 5
state: absent
loop:
"{{ process_ids.pids }}"
ignore_errors: yes
register: exit_results
- name: Forcefully kill stuck processes
shell: "kill -9 {{ item.item }}"
loop:
"{{ exit_results.results | selectattr('failed') | list }}"
loop_control:
label: "{{ item.item }}"
This block of code performs the same functions as before, only now, it is a little more
readable, as we've reduced the number of Jinja2 filters required, and we have removed one
shell command, in favor of the pids module. These techniques, combined with
the service module discussed earlier, should give you a sound basis to meet all of your
process control needs with Ansible.
In the next and final section of this chapter, we'll take a look at how to use Ansible when
you have multiple nodes in a cluster, and you don't want to take them all out of service at
once.
[ 341 ]
Performing Routine Maintenance with Ansible Chapter 12
Rolling updates with Ansible
No chapter on routine maintenance would be complete without a look at rolling updates.
So far in this book, we have kept our examples simple with one or two hosts, and have
worked on the basis that all examples can be scaled up to manage hundreds, if not
thousands, of servers using the same roles and playbooks.
This, by and large, holds true—however, there are certain special cases where perhaps we
need to look a little deeper at the operation of Ansible. Let's build up a hypothetical
example, where we have four web application servers behind a load balancer. A new
release of the web application code needs to be deployed, and the deployment process
requires multiple steps (thus, multiple Ansible tasks). In our simple example, the
deployment process will be as follows:
1. Deploy the web application code to the server.
2. Restart the web server service, to pick up the new code.
In a production environment, you would almost certainly want to take
further steps to ensure the integrity of your web service—for example, if it
is behind a load balancer, you would take it out of service during the code
deployment, and ensure it is not returned to service until it is validated as
working properly. It is not anticipated that everyone reading this book
will have access to such an environment, and so, the example has been
kept simple, to ensure everyone can try it out.
We could easily write a simple Ansible role to perform this task—an example is shown, as
follows:
---
- name: Deploy new code
template:
src: templates/web.html.j2
dest: /var/www/html/web.html
- name: Restart web server
service:
name: nginx
state: restarted
[ 342 ]
Performing Routine Maintenance with Ansible Chapter 12
This code performs our two steps in turn, exactly as we desire. However, let's have a look at
what happens when we run this role in a playbook. The result is shown in the following
screenshot:
Notice how Ansible performed the tasks. First of all, the new code was deployed on all four
servers. Only then, were they restarted. This may not be desirable, for a number of reasons.
For example, the servers may be in an inconsistent state after the first task, and you would
not want all four servers to be in an inconsistent state at once, as anyone using the web
application would experience errors. Also, if the playbook goes wrong for some reason and
produces a failed state, it will faithfully fail on all four servers, thus breaking the entire web
application for everyone, and causing a service outage.
[ 343 ]
Performing Routine Maintenance with Ansible Chapter 12
To prevent these kinds of issues from occurring, we can use the serial keyword, to ask
Ansible to only perform the update on a given number of servers at a time. For example, if
we insert the line serial: 2 into the site.yml playbook calling this role, suddenly the
behavior becomes rather different, as the following screenshot shows:
The preceding output is truncated to save space but clearly shows that the playbook is now
being run on only two servers at a time—thus, during the initial phase of the run, only
cluster1 and cluster2 are inconsistent, while cluster3 and cluster4 remain
consistent and untouched. Only when all tasks are completed on the first two servers are
the next two processed.
Failure handling is also important, and a danger of automation is that you could break an
entire environment very easily if an issue exists in the code or playbook. For example, if our
Deploy new code task fails for all servers, running the playbook on two servers at a time
will not help. Ansible will still faithfully do what it is asked—in this case, break all four
servers.
[ 344 ]
Performing Routine Maintenance with Ansible Chapter 12
In this instance, it is a good idea to add to the playbook the max_fail_percentage
parameter too. For example, if we set this to 50, then Ansible will stop processing hosts as
soon as 50% of its inventory has failed, as shown in the following screenshot:
As we can see here, even though our inventory has not been changed, Ansible has stopped
after processing cluster1 and cluster2—because they failed, it is not performing any
tasks on cluster3 and cluster4; thus, at least two hosts remain in service with good
code, allowing users to continue using the web application, in spite of the failure.
It is important to make use of these Ansible features when working with large, load-
balanced environments, to ensure that failures do not propagate to an entire estate of
servers. That concludes our look at the use of Ansible in routine server maintenance—as
ever, the possibilities are endless, but it is hoped that once again, this chapter has given you
some inspiration and examples upon which to build.
[ 345 ]
Performing Routine Maintenance with Ansible Chapter 12
Summary
Ansible is a very powerful tool, but not just for deployment and configuration
management. Although these are core strengths it possesses, it is also of powerful
assistance when it comes to day-to-day management tasks. As ever, when coupled with an
enterprise management tool such as AWX or Ansible Tower, it becomes incredibly
important in the management of your Linux estate, especially for auditing and debugging
purposes.
In this chapter, you learned how to tidy up disk space using Ansible, and how to make this
conditional. You then learned how Ansible can help monitor configuration drift, and even
alert to possible tampering with binary files. You learned how to manage processes on
remote servers using Ansible, and finally, how to perform rolling updates in a graceful and
managed fashion, across a load-balanced pool of servers.
In the next chapter, we take a look at securing your Linux servers in a standardized fashion,
with CIS Benchmarks.
Questions
1. Why might you make use of the output from the df command rather than an
Ansible fact when examining disk space?