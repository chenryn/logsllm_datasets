vectors under alternative settings (e.g., untargeted, black-box at-
tacks). Second, enhancing other types of threats (e.g., latent back-
door attacks) within the input-model co-optimization framework is
a direction worthy of exploration. Finally, devising a unified robust-
ness metric accounting for both vectors may serve as a promising
starting point for developing effective countermeasures.
ACKNOWLEDGMENTS
We thank our shepherd Xiangyu Zhang and anonymous review-
ers for valuable feedbacks. This material is based upon work sup-
ported by the National Science Foundation under Grant No. 1910546,
1953813, and 1846151. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reflect the views of the National
Science Foundation. S. Ji was partly supported by NSFC under No.
U1936215, 61772466, and U1836202, the National Key Research and
Development Program of China under No. 2018YFB0804102, the
Zhejiang Provincial Natural Science Foundation for Distinguished
Young Scholars under No. LR19F020003, the Zhejiang Provincial
Key R&D Program under No. 2019C01055, and the Ant Financial
Research Funding. X. Luo was partly supported by HK RGC Project
(PolyU 152239/18E) and HKPolyU Research Grant (ZVQ8).
Figure 15: Detection of basic and ensemble STRIP against TrojanNN∗
on CIFAR10 and GTSRB.
6 RELATED WORK
With their increasing use in security-sensitive domains, DNNs are
becoming the new targets of malicious manipulations [3]. Two
primary attack vectors have been considered in the literature: ad-
versarial inputs and poisoned models.
Adversarial Inputs. The existing research on adversarial inputs
is divided in two campaigns.
One line of work focuses on developing new attacks against
DNNs [7, 20, 40, 48], with the aim of crafting adversarial samples
to force DNNs to misbehave. The existing attacks can be catego-
rized as untargeted (in which the adversary desires to simply force
misclassification) and targeted (in which the adversary attempts to
force the inputs to be misclassified into specific classes).
Another line of work attempts to improve DNN resilience against
adversarial attacks by devising new training strategies (e.g., adver-
sarial training) [22, 28, 39, 49] or detection mechanisms [19, 33, 35,
53]. However, the existing defenses are often penetrated or circum-
vented by even stronger attacks [2, 30], resulting in a constant arms
race between the attackers and defenders.
Poisoned Models. The poisoned model-based attacks can be cat-
egorized according to their target inputs. In the poisoning attacks,
the target inputs are defined as non-modified inputs, while the
adversary’s goal is to force such inputs to be misclassified by the
poisoned DNNs [24, 25, 44, 47, 52]. In the backdoor attacks, specific
trigger patterns (e.g., a particular watermark) are pre-defined, while
the adversary’s goal is to force any inputs embedded with such
triggers to be misclassified by the poisoned models [21, 32]. Note
that compared with the poisoning attacks, the backdoor attacks
leverage both adversarial inputs and poisoned models.
The existing defense methods against poisoned models mostly
focus on the backdoor attacks, which, according to their strategies,
can be categorized as: (i) cleansing potential contaminated data at
NeuralCleanseSTRIP(a) CIFAR10Anomaly Measure54.33.62.92.21.5TrojanNNTrojanNN*0.90.720.540.360.18STRIP F1 Score0.10.20.30.40.50.60.70.80.10.20.30.40.50.60.70.8NeuralCleanseSTRIPAnomaly MeasureSTRIP F1 Score0.90.750.60.450.30.15543210Trigger Size0.10.20.30.40.50.60.70.80.10.20.30.40.50.60.70.8(b) GTSRB0TrojanNNTrojanNN*TrojanNNTrojanNN*TrojanNNTrojanNN*0.10.20.30.10.20.3(a) CIFAR10(b) GTSRBSTRIPEnsemble STRIPSTRIP F1 Score0.60.50.40.30.20.10Trigger Size 0.80.60.40.20REFERENCES
[1] Rima Alaifari, Giovanni S. Alberti, and Tandri Gauksson. 2019. ADef: an Iterative
Algorithm to Construct Adversarial Deformations. In Proceedings of International
Conference on Learning Representations (ICLR).
[2] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated Gradients
Give a False Sense of Security: Circumventing Defenses to Adversarial Examples.
In Proceedings of IEEE Conference on Machine Learning (ICML).
[3] Battista Biggio and Fabio Roli. 2018. Wild Patterns: Ten Years after The Rise of
Adversarial Machine Learning. Pattern Recognition 84 (2018), 317–331.
[4] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D.
Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba. 2016.
End to End Learning for Self-Driving Cars. ArXiv e-prints (2016).
[5] Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge
University Press.
[6] BVLC. 2017. Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo.
[7] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. In Proceedings of IEEE Symposium on Security and Privacy
(S&P).
[8] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Ed-
wards, Taesung Lee, Ian Molloy, and Biplav Srivastava. 2018. Detecting Backdoor
Attacks on Deep Neural Networks by Activation Clustering. In ArXiv e-prints.
[9] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. 2019. DeepInspect: A
Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks.
In Proceedings of International Joint Conference on Artificial Intelligence.
[10] Edward Chou, Florian Tramer, Giancarlo Pellegrino, and Dan Boneh. 2018. Sen-
tiNet: Detecting Physical Attacks Against Deep Learning Systems. In ArXiv
e-prints.
[11] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified Adversarial
Robustness via Randomized Smoothing. In Proceedings of IEEE Conference on
Machine Learning (ICML).
[12] G. Cybenko. 1989. Approximation by Superpositions of A Sigmoidal Function.
Mathematics of Control, Signals, and Systems (MCSS) 2, 4 (1989), 303–314.
[13] J.M. Danskin. 1967. The Theory of Max-Min and Its Application to Weapons
Allocation Problems. Springer-Verlag.
[14] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. 2009.
ImageNet: A
Large-scale Hierarchical Image Database. In Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).
[15] Bao Doan, Ehsan Abbasnejad, and Damith Ranasinghe. 2020. Februus: Input
Purification Defense Against Trojan Attacks on Deep Neural Network Systems.
In ArXiv e-prints.
[16] Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter,
Helen M. Blau, and Sebastian Thrun. 2017. Dermatologist-Level Classification of
Skin Cancer with Deep Neural Networks. Nature 542, 7639 (2017), 115–118.
[17] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano
Soatto. 2017. Classification Regions of Deep Neural Networks. ArXiv e-prints
(2017).
[18] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith Ranasinghe, and
Surya Nepal. 2019. STRIP: A Defence Against Trojan Attacks on Deep Neural
Networks. In ArXiv e-prints.
[19] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev.
2018. AI2: Safety and Robustness Certification of Neural Networks with Abstract
Interpretation. In Proceedings of IEEE Symposium on Security and Privacy (S&P).
[20] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In Proceedings of International Conference on
Learning Representations (ICLR).
[21] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. BadNets: Identify-
ing Vulnerabilities in the Machine Learning Model Supply Chain. ArXiv e-prints
(2017).
[22] Chuan Guo, Mayank Rana, Moustapha Cissé, and Laurens van der Maaten. 2018.
Countering Adversarial Images Using Input Transformations. In Proceedings of
International Conference on Learning Representations (ICLR).
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).
[24] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2018. Model-
Reuse Attacks on Deep Learning Systems. In Proceedings of ACM SAC Conference
on Computer and Communications (CCS).
[25] Yujie Ji, Xinyang Zhang, and Ting Wang. 2017. Backdoor Attacks against Learning
Systems. In Proceedings of IEEE Conference on Communications and Network
Security (CNS).
[26] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In Proceedings of International Conference on Learning Representations
(ICLR).
[27] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning Multiple Layers of Features
from Tiny Images. Technical report, University of Toronto (2009).
[28] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial Ma-
chine Learning at Scale. In Proceedings of International Conference on Learning
Representations (ICLR).
[29] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. 2018.
Backdoor Embedding in Convolutional Neural Network Models via Invisible
Perturbation. ArXiv e-prints (2018).
[30] X. Ling, S. Ji, J. Zou, J. Wang, C. Wu, B. Li, and T. Wang. 2019. DEEPSEC: A
Uniform Platform for Security Analysis of Deep Learning Model. In Proceedings
of IEEE Symposium on Security and Privacy (S&P).
[31] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and
Xiangyu Zhang. 2019. ABS: Scanning Neural Networks for Back-Doors by
Artificial Brain Stimulation. In Proceedings of ACM SAC Conference on Computer
and Communications (CCS).
[32] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In Proceedings
of Network and Distributed System Security Symposium (NDSS).
[33] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang.
2019. NIC: Detecting Adversarial Samples with Neural Network Invariant Check-
ing. In Proceedings of Network and Distributed System Security Symposium (NDSS).
[34] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In Proceedings of International Conference on Learning Representations
(ICLR).
[35] Dongyu Meng and Hao Chen. 2017. MagNet: A Two-Pronged Defense Against
Adversarial Examples. In Proceedings of ACM SAC Conference on Computer and
Communications (CCS).
[36] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. 2017. Universal Adver-
sarial Perturbations. In Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).
[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard,
and Stefano Soatto. 2017. Analysis of Universal Adversarial Perturbations. ArXiv
e-prints (2017).
[38] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal
Frossard. 2018. Robustness via Curvature Regularization, and Vice Versa. ArXiv
e-prints (2018).
[39] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillation as a Defense to Adversarial Perturbations Against Deep Neural
Networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).
[40] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay
Celik, and Ananthram Swami. 2016. The Limitations of Deep Learning in Ad-
versarial Settings. In Proceedings of IEEE European Symposium on Security and
Privacy (Euro S&P).
[41] A.D. Polyanin and A.V. Manzhirov. 2006. Handbook of Mathematics for Engineers
and Scientists. Taylor & Francis.
[42] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings
of Conference on Empirical Methods in Natural Language Processing (EMNLP).
[43] Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
Studer, Larry S. Davis, Gavin Taylor, and Tom Goldstein. 2019. Adversarial
Training for Free!. In Proceedings of Advances in Neural Information Processing
Systems (NeurIPS).
[44] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison Frogs! Targeted Clean-Label
Poisoning Attacks on Neural Networks. In Proceedings of Advances in Neural
Information Processing Systems (NeurIPS).
[45] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George
van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalch-
brenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,
Thore Graepel, and Demis Hassabis. 2016. Mastering the Game of Go with Deep
Neural Networks and Tree Search. Nature 7587 (2016), 484–489.
[46] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man
vs. Computer: Benchmarking Machine Learning Algorithms for Traffic Sign
Recognition. Neural Metworks (2012), 323–32.
[47] Octavian Suciu, Radu Mărginean, Yiğitcan Kaya, Hal Daumé, III, and Tudor
Dumitraş. 2018. When Does Machine Learning FAIL? Generalized Transferability
for Evasion and Poisoning Attacks. In Proceedings of USENIX Security Symposium
(SEC).
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing Properties of Neural Networks.
In Proceedings of International Conference on Learning Representations (ICLR).
[49] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel.
2018. Ensemble Adversarial Training: Attacks and Defenses. In Proceedings of
International Conference on Learning Representations (ICLR).
[50] Brandon Tran, Jerry Li, and Aleksander Madry. 2018. Spectral Signatures in
Backdoor Attacks. In Proceedings of Advances in Neural Information Processing
Systems (NeurIPS).
[51] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao. 2019.
Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks.
In Proceedings of IEEE Symposium on Security and Privacy (S&P).
[52] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018.
Formal Security Analysis of Neural Networks Using Symbolic Intervals. In Pro-
ceedings of USENIX Security Symposium (SEC).
[53] W. Xu, D. Evans, and Y. Qi. 2018. Feature Squeezing: Detecting Adversarial
Examples in Deep Neural Networks. In Proceedings of Network and Distributed
System Security Symposium (NDSS).
[54] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. 2019. Latent Backdoor
Attacks on Deep Neural Networks. In Proceedings of ACM SAC Conference on
Computer and Communications (CCS).
[55] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J. Smola. 2010. Parallelized
Stochastic Gradient Descent. In Proceedings of Advances in Neural Information
Processing Systems (NeurIPS).
APPENDIX
A. Proofs
A0. Preliminaries. In the following proofs, we use the following
definitions for notational simplicity:
𝛼 ≜ ℎ/𝑟
𝑦 ≜ 1 − 𝑧
Further we have the following result.