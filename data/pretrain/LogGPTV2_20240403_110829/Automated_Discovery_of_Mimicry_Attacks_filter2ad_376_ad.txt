7 Experiments
We used our implementation to ﬁnd undetected attacks in program models that have
appeared in academic literature. We show that our automated approach can ﬁnd the
mimicry and evasion attacks that previously were discovered manually [24, 22, 20, 21].
The automated techniques allow for better scaling of the number of test cases when
compared to manual approaches.
We can automatically ﬁnd mimicry and evasion attacks that previous research found
only with manual analysis. Previous work considered four test programs—wu-ftpd,
restore, traceroute, and passwd—that had known vulnerabilities allowing at-
tackers to execute a root shell. Forrest et al. [8] successfully detected known attack
instances using a model called Stide. The Stide model is a context-insensitive charac-
terization of execution learned from system call traces generated by a series of train-
ing runs. Wagner and Soto [24] and Tan et al. [22, 20, 21] demonstrated that attackers
could modify their attacks to evade detection by the Stide model. In some cases, the
undetected attacks were not semantically equivalent to the original root shell exploit,
although the attacks adversely modiﬁed system state so that the attacker can subse-
quently gain root access. For example, successful attack variants may:
– write a new root-level account to the user accounts ﬁle /etc/passwd;
– set /etc/passwd world-writable so that an ordinary user can add a new root
account; or
– set /etc/passwd owned by the attacker so that the attacker can add a new root
account.
We automatically found these undetected attacks. We used our infrastructure to ana-
lyze the Stide model for each of the four programs with respect to each of the four attack
goals. For wu-ftpd, we constructed the Stide model using the original Linux training
data of Forrest et al. [7]. We were unable to obtain either the wu-ftpd training data
used by Wagner and Soto or the Stide models that they constructed from that data. As a
result, we were able to ﬁnd attacks in the wu-ftpd model constructed from Forrest’s
data that were reportedly not present in the model constructed from Wagner’s data. For
the remaining three test programs, we constructed models from training data generated
in the manner described by Tan et al. [20]. Our speciﬁcation compiler combined PDA
representations of the Stide models with speciﬁcations of Linux system calls to produce
pushdown systems amenable to model checking.
Table 1 lists the size of the PDA representation of the Stide model for each program.
The OS state model included 119 bits of global state and 50 bits of temporary state
for system call argument variables. This temporary state reduces Moped’s resource de-
mands because it exists only brieﬂy during the model checker’s execution.
Table 2 presents the ability of the Stide model to detect any attack designed to reach
a particular attack goal, as determined by Moped. A “yes” indicates that the model
Automated Discovery of Mimicry Attacks
57
Table 1. Number of states and edges in the transition systems describing the Stide model for each
of the four test programs. The boolean OS state includes 119 bits for global state variables and
50 bits of temporary state for system call argument variables.
Edge count
State count
wu-ftpd restore traceroute passwd
1,058
766
2,085 1,206
1,477
892
623
459
Table 2. Evaluation of the Stide model’s ability to detect classes of attacks. A “yes” indicates
that the Stide model will always detect the attack because the model checker was unable to ﬁnd
an undetected attack. A “no” indicates that Stide is unable to protect the system from the attack
because the model checker discovered an undetected attack sequence. Writing to /etc/passwd
is normal behavior for passwd.
wu-ftpd restore traceroute passwd
No
Execute root shell
No
Write to /etc/passwd
Yes
Set /etc/passwd world-writable
Set /etc/passwd attacker-owned Yes
No
No
Yes
Yes
Yes
No
Yes
Yes
Yes
—
No
No
will always prevent any attacker from reaching their goal, regardless of how they try to
transform or alter their attack sequence of system calls. A “no” indicates the reverse:
the model checker was able to ﬁnd a system call sequence, with arguments, accepted by
the model but that induces the unsafe operating system condition. Figure 6 shows the
undetected attack against traceroute’s Stide model discovered by our system. We auto-
matically found all attacks that researchers previously found manually, one additional
attack due to differences between Forrest’s training data and Wagner’s training data for
wu-ftpd, and an additional attack against restore not found by previous manual
research.
Previous work missed this attack because manual inspection does not scale to many
programs and attacks, and hence the research did not attempt to compute results for all
attack goals in all programs. When using manual inspection, it is likewise difﬁcult to
show that an attack is not possible: has the analyst simply not considered an attack that
would be successful? Model checking can prove that a goal is unreachable regardless
of the actual system calls used by the attacker in their attempt to reach the goal. We
close; munmap; open(“/etc/passwd”, O RDWR | O APPEND) = 3;
fcntl64; fcntl64; fstat64; mmap2; read; close; munmap; write(3);
Fig. 6. Undetected attack against the Stide model of traceroute that adds a new root-level user to
/etc/passwd. The system calls producing the attack effect are shown in boldface. Although
our system discovers arguments for the nop system calls, we omit the arguments here for con-
ciseness. We do not discover the actual string value written to /etc/passwd; a suitable string
would be “attacker::0:0::/root:/bin/sh\n”.
58
J.T. Gifﬁn, S. Jha, and B.P. Miller
Table 3. Model checking running times, in seconds.
Execute rootshell
Write to /etc/passwd
Set /etc/passwd world-writable
Set /etc/passwd attacker-owned
wu-ftpd restore traceroute passwd
2.70
—
2.02
1.81
0.75
0.34
0.39
0.73
39.10 74.41
41.11 65.21
2.38
1.33
0.90
1.15
can show that the models of the ﬁrst three test programs detect all attacks that try to
set /etc/passwd world-writable or owned by the attacker—assertions that previous
manual efforts were unable to make. Although the proofs of detection hold with respect
to the OS abstraction and may not hold in the actual OS implementation, as described
in Sect. 2, the proofs do provide a strong indication that runtime attack detection in the
real system will be effective.
Table 3 lists the model checker’s running times in seconds for each model and attack
goal. When comparing the running times with Table 2, a loose trend becomes apparent.
In cases where the model checker found an attack, the running times are very small.
When no attack was found, the model checker executed for a comparatively longer
period of time. This disparity is to be expected and reﬂects the behavior of the un-
derlying model checking algorithms. When a model checker ﬁnds a counter-example
that disproves a logical formula—here an attack sequence that violates an LTL safety
property—the model checker can immediately terminate its execution and report the
counter-example. However, a successful proof that the logical formula holds requires
the model checker to follow exhaustively all execution paths and early termination is
not possible.
We believe that automating the previously manual process of attack construction is
a signiﬁcant achievement. We are not surprised at our ability to ﬁnd undetected attacks:
attackers have signiﬁcant freedom in program models that do not constrain system call
arguments. For example, the system call sequence open followed by write without
argument constraints can be misused by an attacker to alter the system’s password ﬁle.
Yet, this is a common sequence contained in nearly every non-trivial program, including
programs that execute with the root-level privilege required to alter the password ﬁle.
Our automated system provides us with the means to understand exactly where a
program model fails. From Table 2 we learn which classes of attack can be effectively
detected by a program model and which classes of attack require alternative protection
strategies. What is important is not simply that the models fail to detect some attacks,
but that we know exactly what type of attacks are missed.
8 Conclusions
Model-based intrusion detections systems are useful only when they actually detect or
prevent attacks. Finding undetected attacks manually is difﬁcult, error-prone, unable
to scale to large numbers of program models and attacks, and unable to prove that an
attack will always be detected. We showed here that formalizing the effects of attacks
Automated Discovery of Mimicry Attacks
59
upon the operating system provides the operational means to ﬁnd undetected attacks
automatically. A model checker attempts to prove that the attack effect will never hold
in the program model. By ﬁnding counter-examples that cause the proof to fail, we
ﬁnd undetected attacks: system call sequences and arguments that are accepted as valid
execution and induce the malicious attack effect upon the operating system. This au-
tomation let us ﬁnd undetected attacks against program models that previously were
found only with manual inspection of the models. The efﬁciency of the computation—
about 2 seconds computation to ﬁnd undetected attacks—provides an indication that
this automated approach can easily scale to large collections of program models.
Acknowledgments. We thank the anonymous reviewers and the members of the WiSA
project at Wisconsin for their helpful comments that improved the quality of the paper.
This work was supported in part by Ofﬁce of Naval Research grant N00014-01-1-
0708, NSF grant CCR-0133629, and Department of Energy grant DE-FG02-93ER25176.
Jonathon T. Gifﬁn was partially supported by a Cisco Systems Distinguished Graduate
Fellowship. Somesh Jha was partially supported by NSF Career grant CNS-0448476. The
U.S. Government is authorized to reproduce and distribute reprints for governmental pur-
poses, notwithstanding any copyright notices afﬁxed hereon. The views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily rep-
resenting the ofﬁcial policies or endorsements, either expressed or implied, of the above
government agencies or the U.S. Government.
References
[1] T. Ball and S. K. Rajamani. Bebop: A symbolic model checker for boolean programs. In
7th International SPIN Workshop on Model Checking of Software, Stanford, California,
Aug./Sep. 2000.
[2] F. Besson, T. Jensen, D. L. M´etayer, and T. Thorn. Model checking security properties of
control-ﬂow graphs. Journal of Computer Security, 9:217–250, 2001.
[3] H. Chen and D. Wagner. MOPS: An infrastructure for examining security properties of
In 9th ACM Conference on Computer and Communications Security (CCS),
software.
Washington, DC, Nov. 2002.
[4] E. M. Clarke, O. Grumberg, and D. A. Peled. Model Checking. The MIT Press, 2000.
[5] J. Esparza, D. Hansel, P. Rossmanith, and S. Schwoon. Efﬁcient algorithms for model
In Computer Aided Veriﬁcation (CAV), Chicago, Illinois,
checking pushdown systems.
July 2000.
[6] H. H. Feng, J. T. Gifﬁn, Y. Huang, S. Jha, W. Lee, and B. P. Miller. Formalizing sensitivity
In IEEE Symposium on Security and Privacy,
[7] S. Forrest. Data sets—synthetic FTP. http://www.cs.unm.edu/∼immsec/data/FTP/UNM/-
in static analysis for intrusion detection.
Oakland, California, May 2004.
normal/synth/, 1998.
[8] S. Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaff. A sense of self for UNIX
processes. In IEEE Symposium on Security and Privacy, Oakland, California, May 1996.
[9] D. Gao, M. K. Reiter, and D. Song. On gray-box program tracking for anomaly detection.
In USENIX Security Symposium, San Diego, California, Aug. 2004.
[10] J. T. Gifﬁn, D. Dagon, S. Jha, W. Lee, and B. P. Miller. Environment-sensitive intrusion
In 8th International Symposium on Recent Advances in Intrusion Detection
detection.
(RAID), Seattle, Washington, Sept. 2005.
60
J.T. Gifﬁn, S. Jha, and B.P. Miller
[11] J. T. Gifﬁn, S. Jha, and B. P. Miller. Detecting manipulated remote call streams. In 11th
USENIX Security Symposium, San Francisco, California, Aug. 2002.
[12] R. Gopalakrishna, E. H. Spafford, and J. Vitek. Efﬁcient intrusion detection using automa-
ton inlining. In IEEE Symposium on Security and Privacy, Oakland, California, May 2005.
[13] J. D. Guttman, A. L. Herzog, J. D. Ramsdell, and C. W. Skorupka. Verifying information
ﬂow goals in Security-Enhanced Linux. Journal of Computer Security, 13:115–134, 2005.
[14] L.-c. Lam and T.-c. Chiueh. Automatic extraction of accurate application-speciﬁc sandbox-
ing policy. In Recent Advances in Intrusion Detection (RAID), Sophia Antipolis, French
Riveria, France, Sept. 2004.
[15] C. R. Ramakrishnan and R. Sekar. Model-based vulnerability analysis of computer systems.
In 2nd International Workshop on Veriﬁcation, Model Checking and Abstract Interpreta-
tion, Pisa, Italy, Sept. 1998.
[16] F. B. Schneider. Enforceable security policies. ACM Transactions on Information and
System Security, 3(1):30–50, Feb. 2000.
[17] S. Schwoon. Model-Checking Pushdown Systems. Ph.D. dissertation, Technische Univer-
sit¨at M¨unchen, June 2002.
[18] S. Schwoon. Moped—a model-checker for pushdown systems.
http://www.fmi.uni-
stuttgart.de/szs/tools/moped/, 2006.
[19] R. Sekar, M. Bendre, P. Bollineni, and D. Dhurjati. A fast automaton-based method for
In IEEE Symposium on Security and Privacy,
detecting anomalous program behaviors.
Oakland, California, May 2001.
[20] K. Tan, K. S. Killourhy, and R. A. Maxion. Undermining an anomaly-based intrusion de-
tection system using common exploits. In Recent Advances in Intrusion Detection (RAID),
Z¨urich, Switzerland, Oct. 2002.
[21] K. Tan and R. A. Maxion. “Why 6?” Deﬁning the operational limits of stide, an anomaly
based intrusion detector. In IEEE Symposium on Security and Privacy, Oakland, California,
May 2002.
[22] K. Tan, J. McHugh, and K. Killourhy. Hiding intrusions: From the abnormal to the nor-
mal and beyond. In 5th International Workshop on Information Hiding, Noordwijkerhout,
Netherlands, Oct. 2002.
[23] D. Wagner and D. Dean. Intrusion detection via static analysis. In IEEE Symposium on
Security and Privacy, Oakland, California, May 2001.
[24] D. Wagner and P. Soto. Mimicry attacks on host based intrusion detection systems.
In
9th ACM Conference on Computer and Communications Security, Washington, DC, Nov.
2002.
[25] B. J. Walker, R. A. Kemmerer, and G. J. Popek. Speciﬁcation and veriﬁcation of the UCLA
Unix security kernel. Communications of the ACM, 23(2), Feb. 1980.