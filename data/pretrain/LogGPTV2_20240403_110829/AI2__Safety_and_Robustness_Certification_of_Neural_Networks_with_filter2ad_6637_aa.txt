title:AI2: Safety and Robustness Certification of Neural Networks with
Abstract Interpretation
author:Timon Gehr and
Matthew Mirman and
Dana Drachsler-Cohen and
Petar Tsankov and
Swarat Chaudhuri and
Martin T. Vechev
2018 IEEE Symposium on Security and Privacy
AI2: Safety and Robustness Certiﬁcation of Neural
Networks with Abstract Interpretation
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri∗, Martin Vechev
Department of Computer Science
ETH Zurich, Switzerland
Abstract—We present AI2, the ﬁrst sound and scalable ana-
lyzer for deep neural networks. Based on overapproximation,
AI2 can automatically prove safety properties (e.g., robustness)
of realistic neural networks (e.g., convolutional neural networks).
The key insight behind AI2 is to phrase reasoning about safety
and robustness of neural networks in terms of classic abstract
interpretation, enabling us to leverage decades of advances in
that area. Concretely, we introduce abstract transformers that
capture the behavior of fully connected and convolutional neural
network layers with rectiﬁed linear unit activations (ReLU), as
well as max pooling layers. This allows us to handle real-world
neural networks, which are often built out of those types of layers.
We present a complete implementation of AI2 together with
an extensive evaluation on 20 neural networks. Our results
demonstrate that: (i) AI2 is precise enough to prove useful
speciﬁcations (e.g., robustness), (ii) AI2 can be used to certify
the effectiveness of state-of-the-art defenses for neural networks,
(iii) AI2 is signiﬁcantly faster than existing analyzers based on
symbolic analysis, which often take hours to verify simple fully
connected networks, and (iv) AI2 can handle deep convolutional
networks, which are beyond the reach of existing methods.
Index Terms—Reliable Machine Learning, Robustness, Neural
Networks, Abstract Interpretation
I. INTRODUCTION
Recent years have shown a wide adoption of deep neural
networks in safety-critical applications, including self-driving
cars [2], malware detection [44], and aircraft collision avoi-
dance detection [21]. This adoption can be attributed to the
near-human accuracy obtained by these models [21], [23].
Despite their success, a fundamental challenge remains:
to ensure that machine learning systems, and deep neural
networks in particular, behave as intended. This challenge
has become critical in light of recent research [40] showing
that even highly accurate neural networks are vulnerable
to adversarial examples. Adversarial examples are typically
obtained by slightly perturbing an input
is correctly
classiﬁed by the network, such that the network misclassiﬁes
the perturbed input. Various kinds of perturbations have been
shown to successfully generate adversarial examples (e.g., [3],
[12], [14], [15], [17], [18], [29], [30], [32], [38], [41]). Fig. 1
illustrates two attacks, FGSM and brightening, against a digit
classiﬁer. For each attack, Fig. 1 shows an input in the Original
column, the perturbed input in the Perturbed column, and the
pixels that were changed in the Diff column. Brightened pixels
∗Rice University, work done while at ETH Zurich.
that
Attack
Original
Perturbed
Diff
FGSM [12],  = 0.3
Brightening, δ = 0.085
Fig. 1: Attacks applied to MNIST images [25].
are marked in yellow and darkened pixels are marked in pur-
ple. The FGSM [12] attack perturbs an image by adding to it
a particular noise vector multiplied by a small number  (in
Fig. 1,  = 0.3). The brightening attack (e.g., [32]) perturbs
an image by changing all pixels above the threshold 1 − δ to
the brightest possible value (in Fig. 1, δ = 0.085).
Adversarial examples can be especially problematic when
safety-critical systems rely on neural networks. For instance,
it has been shown that attacks can be executed physically
(e.g., [9], [24]) and against neural networks accessible only as
a black box (e.g., [12], [40], [43]). To mitigate these issues,
recent research has focused on reasoning about neural network
robustness, and in particular on local robustness. Local robus-
tness (or robustness, for short) requires that all samples in the
neighborhood of a given input are classiﬁed with the same
label [31]. Many works have focused on designing defenses
that
increase robustness by using modiﬁed procedures for
training the network (e.g., [12], [15], [27], [31], [42]). Others
have developed approaches that can show non-robustness by
underapproximating neural network behaviors [1] or methods
that decide robustness of small fully connected feedforward
networks [21]. However, no existing sound analyzer handles
convolutional networks, one of the most popular architectures.
Key Challenge: Scalability and Precision. The main chal-
lenge facing sound analysis of neural networks is scaling to
large classiﬁers while maintaining a precision that sufﬁces
to prove useful properties. The analyzer must consider all
possible outputs of the network over a prohibitively large set
of inputs, processed by a vast number of intermediate neurons.
For instance, consider the image of the digit 8 in Fig. 1 and
suppose we would like to prove that no matter how we brighten
the value of pixels with intensity above 1−0.085, the network
will still classify the image as 8 (in this example we have
84 such pixels, shown in yellow). Assuming 64-bit ﬂoating
© 2018, Timon Gehr. Under license to IEEE.
DOI 10.1109/SP.2018.00058
3
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
)
,
5
8
0
.
0
(
n
e
t
h
g
i
r
B
A1
A2
A3
#
l
a
n
o
i
t
u
l
o
v
n
o
C
#
g
n
i
l
o
o
P
x
a
M
A4
#
d
e
t
c
e
n
n
o
C
y
l
l
u
F
Fig. 2: A high-level illustration of how AI2 checks that all
perturbed inputs are classiﬁed the same way. AI2 ﬁrst creates
an abstract element A1 capturing all perturbed images. (Here,
we use a 2-bounded set of zonotopes.) It then propagates A1
through the abstract transformer of each layer, obtaining new
shapes. Finally, it veriﬁes that all points in A4 correspond to
outputs with the same classiﬁcation.
point numbers are used to express pixel intensity, we obtain
more than 101154 possible perturbed images. Thus, proving
the property by running a network exhaustively on all possible
input images and checking if all of them are classiﬁed as 8 is
infeasible. To avoid this state space explosion, current methods
(e.g., [18], [21], [34]) symbolically encode the network as
a logical formula and then check robustness properties with
a constraint solver. However, such solutions do not scale to
larger (e.g., convolutional) networks, which usually involve
many intermediate computations.
Key Concept: Abstract Interpretation for AI. The key
insight of our work is to address the above challenge by lever-
aging the classic framework of abstract interpretation (e.g., [6],
[7]), a theory which dictates how to obtain sound, computable,
and precise ﬁnite approximations of potentially inﬁnite sets of
behaviors. Concretely, we leverage numerical abstract domains
– a particularly good match, as AI systems tend to heavily
manipulate numerical quantities. By showing how to apply
abstract interpretation to reason about AI safety, we enable one
to leverage decades of research and any future advancements
in that area (e.g., in numerical domains [39]). With abstract
interpretation, a neural network computation is overapproxi-
mated using an abstract domain. An abstract domain consists
of logical formulas that capture certain shapes (e.g., zonotopes,
a restricted form of polyhedra). For example, in Fig. 2, the
green zonotope A1 overapproximates the set of blue points
(each point represents an image). Of course, sometimes, due
to abstraction, a shape may also contain points that will not
occur in any concrete execution (e.g., the red points in A2).
The AI2 Analyzer. Based on this insight, we developed
a system called AI2 (Abstract Interpretation for Artiﬁcial
Intelligence)1. AI2 is the ﬁrst scalable analyzer that hand-
les common network layer types, including fully connected
and convolutional layers with rectiﬁed linear unit activations
(ReLU) and max pooling layers.
To illustrate the operation of AI2, consider the example in
1AI2 is available at: http://ai2.ethz.ch
Fig. 2, where we have a neural network, an image of the
digit 8 and a set of perturbations: brightening with parameter
0.085. Our goal is to prove that the neural network classiﬁes
all perturbed images as 8. AI2 takes the image of the digit
8 and the perturbation type and creates an abstract element
A1 that captures all perturbed images. In particular, we can
capture the entire set of brightening perturbations exactly with
a single zonotope. However, in general, this step may result in
an abstract element that contains additional inputs (that is, red
points). In the second step, A1 is automatically propagated
through the layers of the network. Since layers work on
concrete values and not abstract elements, this propagation
requires us to deﬁne abstract layers (marked with #) that
compute the effects of the layers on abstract elements. The
abstract layers are commonly called the abstract transformers
of the layers. Deﬁning sound and precise, yet scalable abstract
transformers is key to the success of an analysis based on
abstract interpretation. We deﬁne abstract transformers for all
three layer types shown in Fig. 2.
the end of the analysis,
the abstract output A4 is
an overapproximation of all possible concrete outputs. This
enables AI2 to verify safety properties such as robustness
(e.g., are all images classiﬁed as 8?) directly on A4. In fact,
with a single abstract run, AI2 was able to prove that a
convolutional neural network classiﬁes all of the considered
perturbed images as 8.
We evaluated AI2 on important tasks such as verifying
robustness and comparing neural networks defenses. For ex-
ample, for the perturbed image of the digit 0 in Fig. 1, we
showed that while a non-defended neural network classiﬁed
the FGSM perturbation with  = 0.3 as 9, this attack is
provably eliminated when using a neural network trained with
the defense of [27]. In fact, AI2 proved that the FGSM attack
is unable to generate adversarial examples from this image for
any  between 0 and 0.3.
Main Contributions. Our main contributions are:
At
• A sound and scalable method for analysis of deep neural
networks based on abstract interpretation (Section IV).
• AI2, an end-to-end analyzer, extensively evaluated on
feed-forward and convolutional networks (computing
with 53 000 neurons), far exceeding capabilities of current
systems (Section VI).
• An application of AI2 to evaluate provable robustness of
neural network defenses (Section VII).
II. REPRESENTING NEURAL NETWORKS AS
CONDITIONAL AFFINE TRANSFORMATIONS
In this section, we provide background on feedforward and
convolutional neural networks and show how to transform
them into a representation amenable to abstract interpretation.
This representation helps us simplify the construction and
description of our analyzer, which we discuss in later sections.
We use the following notation: for a vector x ∈ R
n, xi denotes
its ith entry, and for a matrix W ∈ R
n×m, Wi denotes its ith
row and Wi,j denotes the entry in its ith row and jth column.
4
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
f (x )
::= W · x + b
|
|
case E1 : f1(x), . . . , case Ek : fk(x)
f (f(cid:2)
(x))
E ::= E ∧ E | xi ≥ xj | xi ≥ 0 | xi < 0
it to n neurons, each computing a function based on the
neuron’s weights and bias, one weight for each component
of the input. Formally, an FC layer with n neurons is a
n parameterized by a weight matrix
function FCW,b : R
W ∈ R
n×m and a bias b ∈ R
n. For x ∈ R
m, we have:
m → R
FCW,b(x) = ReLU(W · x + b).
Fig. 3: Deﬁnition of CAT functions.
CAT Functions. We express the neural network as a com-
position of conditional afﬁne transformations (CAT), which
are afﬁne transformations guarded by logical constraints. The
class of CAT functions, shown in Fig. 3, consists of functions
n for m, n ∈ N and is deﬁned recursively. Any
f : R
afﬁne transformation f (x) = W · x + b is a CAT function,
for a matrix W and a vector b. Given sequences of conditions
E1, . . . , Ek and CAT functions f1, . . . , fk, we write:
m → R
f (x) = case E1 : f1(x), . . . , case Ek : fk(x).
(f(cid:2)
This is also a CAT function, which returnsx fi(x) for the
ﬁrst Ei satisﬁed by x. The conditions are conjunctions of
constraints of the form xi ≥ xj, xi ≥ 0 and xi < 0. Finally,
any composition of CAT functions is a CAT function. We often
write f(cid:2)(cid:2) ◦ f(cid:2) to denote the CAT function f (x) = f(cid:2)(cid:2)
(x)).
Layers. Neural networks are often organized as a sequence
of layers, such that the output of one layer is the input of the
next layer. Layers consist of neurons, performing the same