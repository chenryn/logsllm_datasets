**背景**
现在很多公司都会面临，内部敏感信息，比如代码，内部系统服务器地址，账号，密码等等泄露到GitHub上的风险，有恶意的也有非恶意的。这个问题有时很难完全规避掉，为了降低可能的恶劣影响，一般都是会内部搭建一个GitHub敏感信息泄露的监控系统。
一个典型的泄露敏感信息的配置文件（只是为了说明问题，该文件内容是随机生成的）
在负责这方面工作几个月之后，我遇到了两个问题：  
1) 人工指定关键字，必然是不全面的，一般前期是依靠经验来指定，后期根据实际情况慢慢添加。  
2) 告警日志数量非常巨大，其中绝大部分是误报，而真正的危险内容就深藏在其中。
人工审核，需要长期耗费大量时间，并且人在长期面对大量误报的情况下，因疲劳产生的思维敏感度下降，可能会漏掉真正的敏感内容，造成漏报。之所以对于敏感信息泄露的审核，一直由人工来进行，我感觉就是因为这个的识别没有一个很有效的模式，很难实现自动化，需要人工去判断。
后来，我在互联网上看到有关机器学习技术的文章，就想尝试用机器学习的方式去解决下工作痛点。
因为在做技术分享的同时，要保证绝对不能够泄露公司的敏感信息，所以下文中涉及到的运行演示，重要敏感信息都进行了脱敏（瞎编。。。）处理。大佬们如果有兴趣，可以使用自己在这方面工作中积累的样本来测试效果。
**程序解析**
用到的第三方库：hmmlearn, joblib, nltk, numpy, pymongo, scikit-learn, tldextract
实现的功能主要是两个：
1) 找出更多人工没有想到或注意到的敏感关键字。
首先，遍历现有的20个敏感文件样本，读取扩展名，行数，文件大小等信息，作为识别特征。解析文件内容，从中提取出域名和IP地址等主机资产识别信息。对这部分，做判断，如果是主机名，就提取出"domain"和"suffix"部分，如果是IP地址，则计算出相应B段网络。然后将目标文本内容Token化，剥除自定义标点符号和停止词等噪声元素，提取出单词列表。
    for filename in os.listdir(POSITIVE_SAMPLE_PATH):
        with open('{}{}{}'.format(POSITIVE_SAMPLE_PATH, '/', filename), 'r', encoding='utf8', errors='ignore') as f:
            sample_str = f.read().lower()
        ext_id = get_filename_ext_id(filename, ext_dict)
        rowsnumber = sample_str.count('\n')
        sample_length = len(sample_str)
        domain_list.append(get_single_sample_domain_list(sample_str, ip_net_list, unfiltered_hostname_list))
        text_list.append(get_single_sample_text_list(sample_str, unfiltered_keyword))
        if rowsnumber > max_rowsnumber:
            max_rowsnumber = rowsnumber
        if min_length == 0:
            min_length = sample_length
        if sample_length > max_length:
            max_length = sample_length
        if sample_length  2 and len(text[0]) ']
    def get_custom_punctuation():
        custom_punctuation = list(string.punctuation)
        custom_punctuation.extend(CUSTOM_PUNCTUATION)
        return custom_punctuation
对于样本文件的解析，有两种方式，一种如上所示使用正则表达式，另一种是用
所介绍的"domainExtract.py"
这时，需要把上述"get_affect_assets()"函数的实现改为如下形式：
    import domainExtract
    def get_affect_assets(sample):
        parser = domainExtract.DomainTokenizer()
        parser.RunParser(sample)
        unfiltered_domains = list(set(parser.urls))
        affect_assets = list()
        for asset in unfiltered_domains:
            if asset.count(':'):
                asset = asset[0:asset.find(':')]
            if asset not in affect_assets:
                affect_assets.append(asset)
        return affect_assets
经测试表明，当样本文件普遍较小的时候，使用"domainExtract.py"效率更高，具体可根据实际情况选择使用哪种解析方法。  
接下来，根据域名和单词的IDF值（IDF逆向文件频率是一个词语在文档中普遍重要性的度量），计算出主机名和敏感关键字列表。
    idf_max_index = math.log(len(text_list) / 3)
    for domain_small_list in domain_list:
        for domain in domain_small_list:
            if domain not in domain_blacklist:
                idf_domain = math.log(len(domain_list) / (get_element_count(domain, domain_list) + 1))
                if idf_max_index > idf_domain:
                    filtered_hostname_list = get_filtered_hostname_list(domain, unfiltered_hostname_list)
                    for hostname in filtered_hostname_list:
                        if hostname not in hostname_list:
                            hostname_list.append(hostname)
    for key, value in unfiltered_keyword.items():
        idf_keyword = math.log(len(text_list) / (get_element_count(key, text_list) + 1))
        if idf_max_index > idf_keyword and idf_keyword > 1 and not key.isdigit():
            keyword_list.append(key)
这部分涉及的几个函数实现如下：
    # 获取经过过滤的重点主机名列表
    def get_filtered_hostname_list(domain, hostname_list):
        filtered_hostname_list = list()
        for hostname in hostname_list:
            if tldextract.extract(hostname).domain == domain:
                if hostname.count('.') > 1:
                    filtered_hostname_list.append(hostname)
        return filtered_hostname_list
    # 获取包含关键字或域名的文件数量
    def get_element_count(element, element_list):
        count = 0
        for element_small_list in element_list:
            if element in element_small_list:
                count += 1
        return count
将上述计算结果和样本最大、最小字节数及最大行数等保存到JSON配置文件中，供后续使用。
    # 获取扩展阈值的实际值
    def get_expand_threshold(max_num, min_num):
        expand_threshold = (max_num - min_num) * (80 / 100)
        return int(expand_threshold)
    length_expand_threshold = get_expand_threshold(max_length, min_length)
    max_length += length_expand_threshold
    min_length -= length_expand_threshold
    if min_length < 16:
        min_length = 16
    max_rowsnumber += get_expand_threshold(max_rowsnumber, 0)
    conf_json = dict()
    conf_json["hostname_blacklist"] = hostname_blacklist
    conf_json["hostname_list"] = hostname_list
    conf_json["ip_net_list"] = ip_net_list
    conf_json["keyword_list"] = keyword_list
    conf_json["ext_dict"] = ext_dict
    conf_json["max_length"] = max_length
    conf_json["min_length"] = min_length
    conf_json["max_score"] = max_score
    conf_json["min_score"] = min_score
    conf_json["max_rowsnumber"] = max_rowsnumber
    with open('config.json', 'w', encoding='utf8') as f:
        json.dump(conf_json, f)
运行效果演示：
2) 识别告警信息，排除误报，找出真正的敏感泄露信息。
首先从JSON配置文件读取配置。也就是通过上一步程序获取的重要信息。然后建立几个后续要用到的临时变量。
    with open('config.json', 'r', encoding='utf8') as f:
        conf_dict = json.load(f)
        hostname_blacklist = conf_dict["hostname_blacklist"]