baseline approaches.
II. Related Work
Natural language processing (NLP) has been used eﬀectively
in various security and privacy problems, including clustering
illicit online pharmacies [18], [19], identifying sensitive user
inputs [20], [21], and detecting spam [22]–[25]. However,
although euphemisms have been widely studied in linguistics
and related disciplines [26]–[34], they have received relatively
little attention from the NLP [17], or security and privacy
communities. Next, we review relevant prior work, including:
1) euphemism detection, 2) euphemism identiﬁcation, and 3)
self-supervised learning.
A. Euphemism Detection
Euphemism detection is broadly related to the tasks of set
expansion [44]–[49] and lexicon construction and induction
[50]–[56]. Set expansion aims to expand a small set of seed
entities into a complete set of relevant entities, and its goal is
to ﬁnd other target keywords from the same category. Lexicon
construction and induction focus on extracting relations and
building the lexicon-based knowledge graph in a structured
manner. Their goals are diﬀerent from ours, which is to ﬁnd
euphemisms of target keywords.
The speciﬁc task of euphemism detection has been studied
in the NLP literature under a number of frameworks, includ-
ing supervised, semi-supervised, and unsupervised learning,
summarized in Table V. For example, Yang et al. [39] build
a Keyword Detection and Expansion System (KDES) and
apply it to the search results of Baidu, China’s top search
engine. KDES aims to infer whether a search keyword should
be blocked by inspecting the associated search results. This
approach requires general domain information with distant-
supervision (i.e., the Baidu search engine), and is therefore
not suitable for our unsupervised setting. Even if assuming
search engine access, euphemisms for sensitive keywords are
often short and innocent-looking (e.g., blueberries), which
may result in mainly legitimate search results. Another set
of relevant articles [6], [7] generate high-level information
to analyze underground forums via an automated, top-down
approach that blends information extraction and named-entity
recognition. They present a data annotation method and utilize
the labeled data to train a supervised learning-based classiﬁer.
Yet, the results depend heavily on the quality of annotation,
and as shown by several researchers [6], [9], the model does
not perform as well in cross-domain datasets, where it is
outperformed by standard semi-supervised learning techniques.
Our work is most closely related to four state-of-the-art
approaches [9], [11], [17], [40]. CantReader [9] aims to
automatically identify “dark jargon” from cybercrime market-
places. CantReader employs a neural-network based embedding
technique to analyze the semantics of words, and detects
euphemism candidates whose contexts in the background
corpus (e.g., Wikipedia) are signiﬁcantly diﬀerent from those
in the target corpus. Therefore, it takes as input a “dark” corpus
(e.g., Silk Road anonymous online marketplace [43] forum), a
mixed corpus (e.g., Reddit), and a benign corpus (e.g., English
Wikipedia). Diﬀerent from CantReader, we assume only access
to a single target corpus – although we do rely on context-aware
embeddings that could be pre-trained from a reference corpus
like Wikipedia, and then ﬁne-tuned to the target corpus. More
importantly, we ﬁnd that our approach outperforms CantReader,
presumably because we explicitly use context.
Another relevant baseline [17] detects euphemisms instead
by using sentiment analysis. It identiﬁes a set of euphemism
candidates using a bootstrapping algorithm for semantic lexicon
induction. Though the methodology seems reasonable and
Example uses of words in both euphemistic and non-euphemistic senses. All sentences are from Reddit.
Table IV
Word Meaning
Sentences
and the cubans.
Coke
Pot
Cocaine
Coca-Cola
Marĳuana
Container
We had already paid $70 for some shitty weed from a taxi driver but we were interested in some coke
Why are coke dealers the most nuttiest?
OK so we have one gram high quality coke between 2 people who have never done more than a bump.
I love having coke with ice.
When I buy coke at the beverage shop in UK, I pay neither a transaction fee nor an exchange fee.
Never have tried mixing coke with sprite or 7up.
My cousin did the same and when the legalized pot in dc they really started cracking down
in virginia and maryland.
As far as we know he was still smoking pot but that was it.
Age 17, every time I smoked pot, I felt out of place.
No one would resist a pot of soup.
There’s plenty of cupboard space in the kitchen for all your pots and pans.
Most lilies grow well in pots.
Figure 1. Euphemism detection and identiﬁcation pipeline.
intuitive at ﬁrst, it requires additional manual ﬁltering process to
reﬁne the candidates and thus, fails to meet the requirement of
automatic, large-scale detection that online content moderators
desire. In yet another approach, Magu et al. [11] and Taylor et
al. [40] propose two algorithms that leverage word embeddings
and community detection algorithms. Magu et al. [11] generates
a cluster of euphemisms by the ranking metric of eigenvector
centralities [57], [58]. Due to the intrinsic nature of the
algorithm, this approach requires a starting euphemism seed
to ﬁnd others. Taylor et al. [40] creates neural embedding
models that capture the word similarities, uses graph expansion
and the PageRank scores [59] to bootstrap initial seed words,
and ﬁnally enriches the bootstrapped words to learn out-of-
dictionary terms that behave like euphemisms. However, the
approaches of Magu et al. [11] and Taylor et al. [40] were
tested on one single dataset. Unfortunately, we do not ﬁnd
their performance to be as strong on the multiple datasets we
evaluate.
B. Euphemism Identiﬁcation
To the best of our knowledge, no work has explicitly
attempted to infer euphemism meaning. Yuan et al. [9] tackles
a related problem by identifying the hypernym of euphemisms
(e.g., whether it refers to a drug or a person). In a more
general sense, the task of euphemism identiﬁcation is also
related to sense discovery of unknown words [60], [61] and
word sense disambiguation [62]–[65]. While sense discovery
aims to understand the meaning of an unknown word by
generating a deﬁnition sentence, word sense disambiguation
focuses on identifying which sense of a word is used in a
sentence, given a set of candidate senses and relies heavily
on a sense-tagged reference corpus, created by linguists and
lexicographers. However, neither of these are able to capture
nuanced diﬀerences between a group of semantically-similar
target keywords in the same category.
C. Self-supervised Learning
The technical innovations in our work rely heavily on
self-supervision, a form of unsupervised learning where the
data itself provides the supervision [66]. Self-supervision was
designed to make use of vast amounts of unlabelled data
(e.g., free text, images) by constructing a supervised learning
task from the data itself to predict some attribute of the data.
For example, to train a text prediction model, one can take
a corpus of text, mask part of the sentence, and train the
model to predict the masked part; this workﬂow creates a
supervised learning task from unlabelled data. Self-supervision
has been widely used in language modeling [15], [67]–[71],
EuphemismDetectionEuphemism	IdentificationINPUT1.	Raw	text	corpus2.	A	list	of	target	keywords	(e.g.,	heroin,	ecstasy,	etc.)OUTPUTA	list	of	euphemism	candidatesOUTPUTThe	target	keyword	to	which	each	candidate	refers00.10.20.30.4Probabilityweedcokeblueberrybananaspotgold…INPUT1.	A	Euphemism	(e.g.,	weed)Related work on euphemism detection.
Table V
System
Durrett et al.
(2017) [6]
Learning Type
Supervised
&
semi-supervised
Categories (Platform)
Required Input
Approach Keywords
Cybercriminal wares (Darkode),
cybersecurity (Hack Forums),
search engine optimization tech-
niques (Blackhat), data stealing
tools and services (Nulled)
A fully labelled dataset with
annotated euphemisms
Support
Vector Machine
(SVM), Conditional Random
Field (CRF)
et
Pei
(2019) [35]
al.
Supervised
General topics (Online Slang
Dictionary)
Zhao et al.
(2016) [12]
Unsupervised
Cybersecurity (QQ)
Yang et al.
(2017) [39]
Unsupervised
gambling,
Sex,
dangerous
goods, surrogacy, drug, faked
sites (Baidu)
Hada et al.
(2020) [10]
Unsupervised
Drug traﬃcking and enjo kosai
(Twitter)
Slang-less corpus (Penn Tree-
bank) as the negative examples,
Slang-speciﬁc corpus (Online
Slang Dictionary) as the posi-
tive examples
features,
Linguistic
bidirectional LSTM [36]
,
Conditional Random Field
(CRF)
multilayer
perceptron (MLP) [38]
[37],
Target keywords, online search
service
Unsupervised learning, word
embedding (i.e., word2vec), La-
tent Dirichlet Allocation (LDA)
Target keywords, online search
service
Web analysis, keywords expan-
sion, candidate ﬁltering
A clean background corpus,
a bad corpus related to ille-
gal transactions, a set of eu-
phemism seeds
Word embedding (word2vec),
cosine similarity
et
Felt
(2020) [17]
al.
Unsupervised
Firing, lying and stealing (The
English Gigaword corpus)
Category name, a lexicon dic-
tionary (i.e., Gigaword)
Sentiment analysis, bootstrap-
ping, semantic lexicon induc-
tion
Taylor et al.
(2017) [40]
Magu et al.
(2018) [11]
Unsupervised
Hate speech (Twitter)
Unsupervised
Hate speech (Twitter)
Yuan et al.
(2018) [9]
Unsupervised
Our
algorithm
Unsupervised
Sale and trade of hacking
services and tools (Darkode),
blackhat hacking (Hack Fo-
rums), data stealing tool and
service (Nulled), illegal drug
(Silk Road)
Drug (Reddit), weapon (Gab,
SlangPedia, [6], [7]), sexuality
(Gab)
(fasttext
The text corpus, category name Word
[41]
dependency2vec
[42]), community detection,
bootstrapping
embedding
and
The text corpus, a euphemism
seed
(e.g.,
A background corpus
Wikipedia), A dark corpus (e.g.,
Silk Road [43]), A mixed cor-
pus (e.g., Reddit)
Word embedding (word2vec),
network analysis,
centrality
measures
Word embedding,
comparison across corpora
semantic
The text corpus,
words
target key-
Contextual information, masked
language model, BERT
representation learning [72]–[75], robotics [76]–[78], computer
vision [79]–[82] and reinforcement learning [83]–[85]. One
of our contributions is to generalize and extend the idea of
self-supervision to the task of euphemism identiﬁcation.
III. Problem Description
In this study, we assume a content moderator has access to
a textual corpus (e.g., a set of posts from an online forum),
and is required to moderate content related to a given list of
target keywords. In practice, forum users may use euphemisms—
words that are used as substitutes for one of the target keywords.
We have two goals, euphemism detection and euphemism
identiﬁcation, deﬁned as follows: 1) Euphemism detection:
Learn which words are being used as euphemisms for target