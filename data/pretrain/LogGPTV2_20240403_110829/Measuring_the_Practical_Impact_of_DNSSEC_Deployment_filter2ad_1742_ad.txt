that UUID. Using the partitioned dataset, we iden-
tiﬁed 124 resolvers whose failure rate for badsec-b8
was signiﬁcantly lower than that of each of the other
badsec variants (McNemar’s test, p < 0.01). More-
over, for 123 of these resolvers, the badsec-b8 and
goodsec failure rates did not signiﬁcantly diﬀer at
the .01 level (McNemar’s test).
With the cooperation of one of the ISPs whose
resolvers exhibited the validation anomaly, we were
granted access to query their closed resolvers and
were able to manually reproduce the errant valida-
tion. We also added -1, +2, and +100 to the RRSIG
labels ﬁeld values and found that the resolvers in-
correctly accepted all of the increased values, but
not the decreased value, suggesting that the DNS
server implementation in use reversed the inequality
for testing the labels ﬁeld.
We were unable to devise a cache-poisoning at-
tack that leverages this validation error under any
reasonable threat model.
4.1.4 Case Study:
badsec-c12 validation
anomaly
The failure rate of the badsec-c12 variant (2.521%)
diﬀered signiﬁcantly from those of all other badsec
domains and was the second lowest, after badsec-b8
(McNemar’s test, p < 0.01). The badsec-c12 subdo-
main attacks the DNSSEC chain of trust by not pro-
viding the RRSIG RR for the test resource’s type A
RRset. A properly-validating server should not con-
sider the aﬀected A RRset validated unless it were
able to retrieve and validate its RRSIG.
All 32 of the resolvers in our dataset that exhib-
ited this validation anomaly belonged to the same
/22 subnetwork controlled by one particular ISP,
as did 45 of the 49 resolvers for which McNemar’s
test showed signiﬁcantly-diﬀerent (p < 0.01) failure
rates between nosec and goodsec. Customers using
this ISP’s recursive resolvers suﬀer from the worst
of both worlds. They are not only more vulnera-
ble to a man-in-the-middle attack against DNSSEC,
but also less likely to be able to access a domain
with DNSSEC enabled than one without. We were
unable to obtain access to the ISP’s closed recursive
resolvers to try to manually reproduce the incorrect
validation behavior.
Due to the reduced size of responses omitting
RRSIG RRs for type A queries, no badsec-c12 DNS
resolutions fell back to TCP, and for the 32 resolvers
exhibiting the validation anomaly, badsec-c12 ’s fail-
ure rate (1.245%) was signiﬁcantly less than that
of goodsec (13.81%) (McNemar’s test applied sepa-
rately for each resolver, p < 0.01).
4.1.5 Case Study: Comcast
In January 2012, Comcast announced that it had
ﬁnished deploying DNSSEC within its network and
that its residential customers would thenceforth be
protected by DNSSEC-validating DNS resolvers [30].
We identiﬁed dynamic Comcast IP end hosts in our
dataset using a list of IP preﬁxes published by Com-
cast [13]. One should expect that Comcast end hosts
in our dataset would fail on goodsec at a lower than
average rate and badsec at a higher than average
rate. Indeed, the 582 Comcast end hosts observed
exhibited a 0.1718% failure rate for goodsec and a
92.5636% failure rate for badsec. For comparison,
Comcast end hosts failed on nosec domains 0.1718%
of the time. This result is consistent with the ex-
pectation that a network that is properly conﬁgured
for DNSSEC will have identical behavior for nosec
and goodsec. Our measurements indicate that the
majority of the diﬀerence between Comcast’s badsec
failure rate and 100% is caused by users who are not
using Comcast for recursive resolution and therefore
do not beneﬁt from Comcast’s DNSSEC veriﬁcation;
if we exclude end-hosts that use resolvers outside of
Comcast’s AS the badsec failure rate improves to
98.6544%.
582  22nd USENIX Security Symposium 
USENIX Association
10
e
t
a
r
e
r
u
l
i
a
F
7
2
0
.
0
6
2
0
.
0
5
2
0
.
0
4
2
0
.
0
3
2
0
.
0
2
2
0
.
0
1
2
0
.
0
All test resources
Windows
Firefox
Chrome
Safari
IE
e
t
a
r
e
r
u
l
i
a
F
5
3
0
.
0
0
3
0
.
0
5
2
0
.
0
0
2
0
.
0
5
1
0
.
0
0
1
0
.
0
0
5
10
15
Load position
20
25
0
5
10
15
Load position
all test resource classes
20
25
Figure 9: Plot of failure rate across all test re-
source types versus load order.
Figure 10: Failure rate versus load order across
all test resources for Windows clients.
Percentile Test duration (seconds)
50%
90%
95%
98%
9
31
50
100
Table 6: Percentiles from the distribution of test
durations, measured from the time of the measure-
ment page load to the time of the completion signal.
4.2 Measurement Diﬃculties
Because our primary measurement endpoint is the
browser’s failure to retrieve a resource, we are very
sensitive to any other sources of failure other than
the ones we are attempting to measure; by con-
trast, many previous studies such as [22] measured
between multiple diﬀerent success outcomes, which
were distinguishable from failures.
In order to
minimize these eﬀects, we investigated other po-
tential sources of failure closely, as described be-
low.
4.2.1 Resource Load Sequence
Recall from Section 3.1, that test resource loads are
initiated one after another in a random order. Be-
cause the test takes some time (see Table 6) to com-
plete, there are a variety of conditions which can
cause the test to abort prematurely. This suggests
that the order in which resources are loaded may
impact the error rate.
Figure 9 shows the overall failure rate versus load
position (note that the ﬁrst resource is at position 0).
While the overall trend seems consistent with fail-
ures getting progressively worse with later resources,
the sharp spike and then subsequent decline between
positions 5 and 9 seems anomalous. In order to ex-
plore this further, we broke down the the failure rate
by browser and operating system.
As Figures 10 and 11 make clear, Chrome and
Firefox on Mac and Windows both show the same
pattern of a failure spike around resources 5-8,
whereas the same browsers on Linux (Figure 12) as
well as both Safari and Internet Explorer show a
generally linear trend (though the break around re-
source 9 for Internet Explorer is also puzzling). We
leave the explanation of these anomalies for future
work.
4.2.2 Latent UUIDs
Our analysis uncovered 3616 UUIDs for which we
received completion signals without corresponding
measurement page loads during the one-week exper-
iment window. We refer to these UUIDs as latent
UUIDs because we observed DNS and HTTP re-
quests for FQDNS that included them in our logs
prior to the start of our experiment window. There
are two plausible explanations for the existence of
latent UUIDs:
1. Browser caching. Modern web browsers
cache users’ recent and open tabs to allow for
restoration of the browsing session in case of
USENIX Association  
22nd USENIX Security Symposium  583
11
Mac
Linux
Firefox
Chrome
Safari
e
t
a
r
e
r
u
l
i
a
F
5
3
0
.
0
0
3
0
.
0
5
2
0
.
0
0
2
0
.
0
5
1
0
.
0
0
1
0
.
0
Firefox
Chrome
e
t
a
r
e
r
u
l
i
a
F
5
3
0
.
0
0
3
0
.
0
5
2
0
.
0
0
2
0
.
0
5
1
0
.
0
0
1
0
.
0
0
5
10
15
Load position
all test resource classes
20
25
0
5
10
15
Load position
all test resource classes
20
25
Figure 11: Failure rate versus load order across
all test resources for Mac clients.
Figure 12: Failure rate versus load order across
all test resources for Linux clients.
a crash, browser termination, or accidental tab
closure. Half of the 18 latent UUIDs that had
HTTP requests during the experiment window
appeared within the ﬁrst 33 hours of the exper-
iment window, and 11 of them loaded the mea-
surement page within the 24 hours leading up
to the start of the experiment window. Thus, it
is plausible that browser caching explains some
of the latent UUIDs.
2. DNS caching with eager renewal. To im-
prove DNS cache hit rates and, consequently,
reduce client latency, Cohen and Kaplan [12]
proposed a caching scheme wherein DNS caches
issue unsolicited queries to authoritative name-
servers for cached RRs whose TTLs have ex-
pired, even if no client queried for the RR at
the time of the renewal. This mechanism is
a documented feature in the Cisco IOS Dis-
tributed Director [1] and has been implemented
by others [45]. Our log data strongly supports
this explanation, as all latent UUIDs (by deﬁni-
tion) appeared in the DNS logs, but only 18 had
HTTP requests during the experiment window.
Latent UUIDs are not included in our analysis,
as we cannot guarantee that our log data extends
far enough into the past to cover them. Further-
more, our analysis only includes UUIDs for which
we observed both the measurement page load and
completion signal within the experiment window.
5 Discussion
The beneﬁt from DNSSEC-signing a domain is
upper-bounded by the number of clients which actu-
ally validate DNSSEC-signed records. As our mea-
surements show, the fraction of clients which do so
is less than 3%.9 Moreover, this beneﬁt is only ob-
tained if DNSSEC either deters attacks or allows