title:Enabling Fine-Grained Permissions for Augmented Reality Applications
with Recognizers
author:Suman Jana and
David Molnar and
Alexander Moshchuk and
Alan M. Dunn and
Benjamin Livshits and
Helen J. Wang and
Eyal Ofek
Enabling Fine-Grained Permissions for Augmented 
Reality Applications with Recognizers
Suman Jana, The University of Texas at Austin;  
David Molnar and Alexander Moshchuk, Microsoft Research;  
Alan Dunn, The University of Texas at Austin;  
Benjamin Livshits, Helen J. Wang, and Eyal Ofek, Microsoft Research
Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4Enabling Fine-Grained Permissions for Augmented Reality
Applications With Recognizers
Suman Jana1, David Molnar2, Alexander Moshchuk2, Alan Dunn1, Benjamin Livshits2,
Helen J. Wang2, and Eyal Ofek2
1University of Texas at Austin
2Microsoft Research
Abstract
1 Introduction
Augmented reality (AR) applications sense the en-
vironment, then render virtual objects on human
senses. Examples include smartphone applications
that annotate storefronts with reviews and XBox
Kinect games that show “avatars” mimicking human
movements. No current OS has special support for
such applications. As a result, permissions for AR
applications are necessarily coarse-grained : applica-
tions must ask for access to raw sensor feeds, such
as video and audio. These raw feeds expose signif-
icant additional information beyond what applica-
tions need, including sensitive information such as
the user’s location, face, or surroundings.
Instead of exposing raw sensor data to applica-
tions directly, we introduce a new OS abstraction:
the recognizer. A recognizer takes raw sensor data
as input and exposes higher-level objects, such as
a skeleton or a face, to applications. We propose
a ﬁne-grained permission system where applications
request permissions at the granularity of recognizer
objects. We analyze 87 shipping AR applications
and ﬁnd that a set of four core recognizers covers
almost all current apps. We also introduce privacy
goggles, a visualization of sensitive data exposed to
an application. Surveys of 962 people establish a
clear “privacy ordering” over recognizers and demon-
strate that privacy goggles are eﬀective at commu-
nicating application capabilities. We build a proto-
type on Windows that exposes nine recognizers to
applications, including the Kinect skeleton tracker.
Our prototype incurs negligible overhead for single
applications, while improving performance of con-
current applications and enabling secure oﬄoading
of heavyweight recognizer computation.
An augmented reality (AR) application takes nat-
ural user interactions (such as gestures, voice, and
eye gaze) as input and overlays digital content on
top of the real world seen, heard, and experienced
by the user. For example, on mobile phones, aug-
mented reality “browsers” such as Layar and Junaio
allow users to look through the phone and see an-
notations about a magazine article or a storefront.
Furniture applications on the iPad allow users to
preview what a couch would look like in the context
of a real room before buying [17]. The Xbox Kinect
has sold over 19 million units and allows application
developers to overlay avatars on top of a user’s pose,
creating new kinds of games and natural user inter-
faces. Microsoft has released a Windows SDK for
Kinect and helped incubate multiple startup com-
panies delivering AR experiences on the PC. Even
heads-up displays, previously restricted to academic
and limited military/industrial use, are set to reach
consumers with Google Glass [25].
Today’s AR applications are monolithic. The
application itself performs sensing, rendering, and
user input interpretation (e.g., for gestures), aided
by user-space libraries, such as the Kinect SDK,
OpenCV [6, 12], or cloud object recognition ser-
vices, such as Lambda Labs or IQ Engines. Because
today’s OSes are built without AR applications in
mind, they oﬀer only coarse-grained access to sensor
streams, such as video or audio data. This raises
a privacy challenge:
it is diﬃcult to build applica-
tions that follow the principle of least privilege, hav-
ing access to only the information they need and no
more. Today’s systems also do not have any AR-
speciﬁc permissions, relying instead on careful pre-
publication vetting of applications [5].
Motivating Example. Figure 1 illustrates the
problem with coarse-grained abstractions in today’s
USENIX Association  
22nd USENIX Security Symposium  415
1
416  22nd USENIX Security Symposium 
USENIX Association
Figure1:Givingrawsensordatatoapplicationscancompromiseuserprivacy.ThisvideoframecapturedfromaKinectcontainstheuser’sface,privatewhite-boarddrawings,andabottleofmedicine.Figure2:ARapplicationsoftenneedonlyspeciﬁcob-jectsratherthantheentiresensorstreams.The“KinectAdventures!”gameonlyneedsbodypositiontorenderanavatarandsimulategamephysics.ARapplications.Theﬁgureshowsavideoframecapturedfromacamera.Today,applicationsmustaskforrawcameraaccessiftheywanttodovideo-basedAR,whichmeanstheapplicationwillseeallsensitiveinformationintheframe.Inthisframe,thatinformationincludestheuser’sface,(pri-vate)drawingsonthewhiteboard,andabottleofmedicinewithalabelthatrevealsamedicalcondi-tion.Anapplication,however,maynotneedanyofthissensitiveinformationtodoitsjob.Forexample,Figure2showsascreenshotfromthe“KinectAd-ventures!”gamethatshipswiththeMicrosoftXboxKinect.First,thegameestimatesthebodypositionoftheplayerfromthevideoanddepthstreamoftheKinect.Next,thegameoverlaysanavatarontopoftheplayer’sbodyposition.Finallythegamesim-ulatesinteractionbetweentheavatarandavirtualworld,includingaballthatbouncesbackandforthtohitblocks.Todoitsjob,thegameneedsonlybodyposition,andnotanyotherinformationfromthevideoanddepthstream.KinectisjustoneexampleofanARsystem;thisFigure3:TwoexamplesofmobileARapplicationsthatonlyneedspeciﬁcobjectsinasensorstream.Ontheleft,Macy’sBelieve-O-Magiconlyneedsthelocationintheframeofaspecialmarker,ontopofwhichitrendersacartooncharacter.Ontheright,LayaronlyneedstoknowtheGPSlocationandcompasspositiontoshowgeo-taggedtweets.ApplicationObjectsrecognizedYourShape2012skeleton,persontextureDanceCentral3skeleton,persontextureNike+Kinectskeleton,persontextureJustDance4skeleton,videoclipNBA2K13voicecommandsXboxDashboardpointer,voicecommandsLayarGPS“pointsofinterest”RedBullRacingRedBullCansMacy’sBelieve-O-MagicMacy’sstoredisplayFigure4:SampleARapplicationsandtheobjectstheyrecognize.Kinectappsareabovetheline,mobilebelow.principleofARapplicationsbeneﬁtingfrom“leastprivilege”ismoregeneral.WeshowtwomobilephoneexamplesinFigure3.Ontheleft,theMacy’sBelieve-O-Magicapplicationshowsaviewofachildstandingnexttoaholiday-themedcartooncharac-ter.Whiletheapplicationtodaymustaskforrawvideoaccess,whichincludesthefaceofthechildandofallbystanders,theonlyinformationtheap-plicationneedsisthelocationofaspecialmarkertoenablerenderingthecartooninthecorrectplace.Ontheright,Layarisan“ARbrowser”formobilephones,hereshowingavisualizationofwhererecenttweetshaveoriginatedneartheuser.Again,Layarmustaskforrawvideoandlocationaccess,butinfactitneedstoonlyknowtheGPSpositionofthetweetrelativetotheuser.Beyondtheseexamples,Figure4showsthetop5Amazonbest-sellingKinect-enabledapplicationsfortheXbox360,alongwiththeXboxDashboardandrepresentativeARappsonmobilephones.Foreachapplication,aswellastheXboxDashboard,weenu-meratetheobjectsrecognized;inSection5wecarryoutasimilaranalysisforallshippingXboxKinectapplications.Noneoftheseapplicationsneedcon-2tinuous access to raw video and depth data, but no
current OS allows a user to restrict access at ﬁner
granularity.
The Recognizer Abstraction. To address this
problem, we introduce a new least-privilege OS ab-
straction called a recognizer. A recognizer takes as
input a sensor stream and creates events when ob-
jects are recognized. These events contain informa-
tion about the recognized object, such as its position
in the video frame, but not the raw sensor informa-
tion. By making access to recognizer-exposed ob-
jects a ﬁrst-class permission in an operating system,
we enable least privilege for AR applications. We
assume a ﬁxed set of system-provided recognizers in
this work. This is justiﬁed by our analysis of over 87
shipping applications, which shows a set of four “core
recognizers” is suﬃcient for the vast majority of such
applications (Section 5).
Supporting recognizers in the OS incurs several
beneﬁts. Besides enabling least privilege, recogniz-
ers lead to a performance improvement, as heavy-
weight object recognition can be shared among mul-
tiple applications. We show how an OS can com-
pose recognizers in a dataﬂow graph, which enables
precise reasoning about which recognizers should be
run, depending on the set of running applications.
Finally, we show how making dataﬂow explicit al-
lows us to prune spurious permission requests. These
beneﬁts extend beyond AR applications and to any
set of applications that must interpret higher-level
objects from raw sensor data, such as building moni-
toring, stored video analysis, and health monitoring.
Challenges. We faced several challenges designing
our recognizer-based AR platform. First, other ﬁne-
grained permission systems, such as Android, have
been shown to be diﬃcult to interpret for users [11].
To address this problem, we introduce privacy gog-
gles: an “application’s-eye view” of the world that
shows users which recognizers are available to an
application. Users see a video representation of sen-
sitive data that will be shown to the application
(Figure 9). This, in turn, lays the foundation for
informed permission granting or permission revoca-
tion. Our surveys of 462 people show that privacy
goggles are eﬀective at communicating capabilities
to users.
Another challenge concerned recognizer errors.
For example, an application may have permission
for a skeleton recognizer. If that recognizer mistak-
enly ﬁnds a skeleton in a frame, the application may
obtain information even though there is no person
present. This information leakage violates a user’s
expectations, even though the application sees only
a higher-level object such as the skeleton.
We address recognizer errors with a new OS
component, recognizer error correction. We evalu-
ate three approaches: blurring, frame subtraction,
and recognizer combination. The ﬁrst two manipu-
late raw sensor data to reduce false positives in a
recognizer-independent way. The last reduces false
positives by using context information available to
the OS from its use of multiple recognizers that could
not be available to any individual recognizer author.
We show that our techniques reduce false positives
across a set of seven recognizers implemented in the
OpenCV library [12].
Our ﬁnal challenge concerned recognizers that
require heavyweight object recognition algorithms
which may run poorly or not at all on performance-
constrained mobile devices [23, 21]. We thus build
and evaluate support for oﬄoading of particularly
heavyweight recognizers to a remote machine.
We have implemented a prototype of our system
on Windows, using the Kinect for Windows SDK.
Our system includes nine recognizers, including face
detection, skeleton detection, and a “plane recog-
nizer” built on top of KinectFusion [23].
Contributions. We make the following contribu-
tions:
• We introduce a new OS abstraction, the rec-
ognizer, which captures the core object recog-
nition capabilities of AR applications. Our
novel ﬁne-grained permission system for recog-
nizers enables least privilege for AR applica-
tions. We show that all shipping Kinect applica-
tions would beneﬁt from least privilege. Based
on surveys of 500 people, we determine a pri-
vacy ordering on common recognizers.
• We introduce a novel visualization of sensitive
data provided to AR applications, which we call
privacy goggles. Privacy goggles let users in-
spect sensitive information ﬂowing to an appli-
cation, to aid in permission granting, inspec-
tion, and revocation. Our surveys of 462 people
show that privacy goggles are eﬀective at com-
municating capabilities to users.
• We recognize the problem of granting permis-
sions in the presence of object recognition errors
and propose techniques to mitigate it.
• We demonstrate that raising the level of ab-
straction to the “recognizer” enables the OS
to oﬀer services such as oﬄoading and cross-
application recognizer sharing that improve per-
formance. Our implementation has negligible
USENIX Association  
22nd USENIX Security Symposium  417
3
Figure 5: AR application pipeline: (1) reading raw data from hardware, (2) parsing raw data into recognized objects,
(3) manipulating these objects to add augmentations to the scene, and (4) resolving conﬂicts and rendering.
overhead for single applications, yet greatly in-
creases performance for concurrent applications
and allows the OS to oﬄoad heavyweight rec-
ognizer computation.
In the rest of the paper, Section 2 provides back-
ground on AR, Section 3 discusses our recognizer
abstraction, and Section 4 describes our implemen-
tation. Section 5 evaluates privacy goggles, recogniz-
ers required for shipping AR applications, recognizer
error correction, and performance of our prototype.
Sections 6 and 7 present related and future work,
and Section 8 concludes.
2 AR Overview
We characterize AR applications using a pipeline
shown in Figure 5. First, the sensing stage acquires
raw video, audio, and other sensor data from plat-
form hardware. In the ﬁgure, we show an RGB video
frame as an example. The frame was captured from
a Kinect, which also exposes a depth stream and a
high-quality microphone.
Next, the recognition stage applies object recog-
nition algorithms to the raw sensor data. For ex-
ample, voice recognition may run on audio to look
for speciﬁc keywords spoken, or a skeleton detector
may estimate the presence and pose of a human in
the video. As new advances in computer vision and
machine learning make it possible to reliably recog-
nize diﬀerent objects, the resulting algorithms can
be added to this stage. The code performing object
recognition is similar to drivers in traditional operat-
ing systems: code running with high privilege main-
tains an abstraction between “bare sensing” and ap-
plications. Just as with devices in traditional OSes,
an OS with support for AR could multiplex applica-
tions across multiple object recognition components;
we will describe a new OS abstraction that enables
this in the next section. In the ﬁgure, a face and two
areas of text are recognized, one on the whiteboard
and another on a bottle of medicine. The output of
the recognition stage is a set of software objects that
“mirror” recognized real-world objects.
In the transformation stage, applications consume
the recognized objects and add virtual objects of
their own. Finally, the presentation stage creates
the ﬁnal view for the user, taking as input all cur-
rent software objects and the current state of the
world. This stage must resolve any remaining logi-
cal conﬂicts, as well as check that desired placement
of objects is feasible. Today, this rendering is done
using standard OS abstractions, such as DirectX or
OpenGL.
3 The Recognizer OS Abstraction
We propose a new OS abstraction called a recog-
nizer. A recognizer is an OS component that takes
a sensor stream, such as video or audio, and “rec-
ognizes” objects in the sensor stream. For example,
Figure 6 shows a recognizer that wraps face detec-
tion logic. This recognizer takes a raw RGB image
and outputs a face object if a face is present. The
recognizer abstraction lets us capture that most AR
applications operate on speciﬁc entities with high-
level semantics, such as the face or the skeleton. To
enable least privilege, the OS exposes higher level
entities through recognizers.
Recognizers create events when objects are recog-
nized. A recognizer event contains structured data
that encodes information about the objects. Each
recognizer declares a public type for this structured
data that is available to applications. Applications
register callbacks with the OS that ﬁre for events
from a particular recognizer; the callbacks accept
arguments of the speciﬁed type. For example, the
recognizer in Figure 6 declares that it will return a
list of points corresponding to facial features, plus
an RGB texture for the face itself. A callback for
an application receives the points and texture in its
arguments, but not the rest of the raw RGB frame.
The recognizer is the unit of permission granting.
Every time an application attempts to register a call-
back with the OS for a speciﬁc recognizer, the ap-
plication must be authorized by the user. Diﬀerent
418  22nd USENIX Security Symposium 
USENIX Association
4
Figure 6: Example of a recognizer for face detection.
The input is a feed of raw RGB video plus a region within
that video. The recognizer outputs an event if a face is
recognized in the region. Applications register callbacks
that ﬁre on the event and are called with a list of points
outlining the face plus an RGB texture, but not the rest
of the video frame.
Figure 7: A sample directed acyclic graph of recogniz-
ers. Arrows denote how recognizers subscribe to events
from other recognizers.
applications can, depending on the user’s authoriza-
tion, have access to diﬀerent recognizers.This gives
us a ﬁne-grained permission mechanism.
Users can restrict applications to only “see” a sub-
set of the raw data stream. For example, Figure 6
shows a bounding box in the raw RGB frame that
can be associated with a speciﬁc application.
If a