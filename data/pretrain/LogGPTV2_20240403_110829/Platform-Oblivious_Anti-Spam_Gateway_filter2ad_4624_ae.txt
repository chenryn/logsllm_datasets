91.5
84.7
72.1
81.3
SVM NN
73.9
77.4
24.3
10.3
78.9
75.3
23.8
10.3
79.3
78.4
13.5
44.7
85.5
83.3
71.2
66.7
90.6
85.1
74.4
78.4
Ours
84.4
77.4
84.4
77.4
84.4
77.4
84.4
77.4
84.4
77.4
Table 6: Comparisons of the supervised solutions and our
solution with various sizes of test dataset
Size Metric (%)
10k
Prs
Rec
Prs
Rec
Prs
Rec
Prs
Rec
20k
30k
50k
Bayes
89.4
71.5
85.1
64.4
84.7
63.5
83.2
61.2
C4.5 Ada
90.5
89.2
72.2
71.1
83.7
83.9
66.2
67.5
83.1
82.6
62.4
60.2
82.2
81.3
55.8
61.2
SVM NN
90.6
87.2
73.5
65.7
85.5
83.5
71.2
59.3
83.2
85.2
70.9
53.6
83.1
81.4
43.5
67.2
Ours
85.5
71.6
85.3
70.3
85.3
71.0
85.7
72.2
reliably label 50% (i.e., r = 1
1) of dataset for training, since doing
so may bring excessive human overhead. In contrast, our method
does not require any ground truth labeling, but can still achieve
moderate performance.
We next conduct experiments by fixing the training set size and
varying the test set sizes to compare the performance of supervised
solutions and our method. We take the Twitter Trending dataset as
an example and select 10, 000 labeled tweets, i.e., 1, 446 spam and
8, 554 ham messages, to serve as the training set. Table 6 shows the
results of all supervised solutions and our method, with the test set
size growing from 10, 000 to 50, 000. We observe the performance
of our method fluctuates only slightly as the test set size grows,
whereas all supervised counterparts suffer from fast performance
degradation on all metrics with an increase in the test dataset
size. This demonstrates the robustness of our proposed solutions.
Table 7: Results of Ablation Studies
Metrics (%)/Model
Prs
Rec
Seed Gibbs N LP
68.5
62.6
71.7
70.1
48.3
51.4
Ours
85.2
71.2
Specifically, when the test set size exceeds 30k, our method almost
beats all examined supervised solutions.
4.4 Importance of Each Design Component
We conduct the ablation studies to evaluate the necessity and im-
portance of each component in our design. In particular, the com-
ponents of Seed Collection, Word Copra Reconstruction, and Spam
Word Model, corresponding to Sections 3.1, 3.2, and 3.3, respec-
tively, will be removed in turn, as design variants to evaluate the
performance of the remaining system. The three corresponding
variants are denoted as Seed, Gibbs, and N LP, respectively. Ta-
ble 7 presents the evaluation results (i.e., Precision and Recall) of
Seed, Gibbs, and N LP under the Metsis Email dataset. From this
table, we observe that all three variants perform worse than our
complete system in terms of precision. Regarding the recall, our
system performs markedly better than Gibbs, slightly better than
N LP, but marginally worse than Seed. However, considering both
precision and recall metrics, which are important to signify the
overall classification performance, we can conclude that all design
components are necessary and important in contributing to our
system performance.
4.5 Impact of Seed Threshold
We conduct experiments to show the impact of various threshold
values for ALER and MCER on our system performance. Figure 10
shows the F1 scores of our system under various ALER and MCER
thresholds, ranging from 0.2% to 10%. Notably, ALER and MCER
are always set to the same value. From this figure, we can see our
system performance on Twitter Trend and Twitter Normal datasets
fluctuates only slightly when the threshold values increases from
0.2% to 10%, with most F1 scores being higher than 0.75. The reason
is that the two datasets are large, so even when the threshold values
rise to 10%, most of selected seeds are indeed spam or ham words.
On the other hand, for the two small datasets Metis and SMS, an
increase in threshold values will significantly degrade our system
performances, since a large threshold value leads to a high false
positive rate of spam seed corpus, thus substantially misleading
1073Platform-Oblivious Anti-Spam Gateway
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
(a)
Figure 10: F1 scores of our system on four experimented
datasets when varying the thresholds values of ALER and
MCER measurement from 0.2% to 10%.
the remaining processing in our system. But from the four datasets,
whose sizes range from 5, 574 to 5, 823, 230, our system can always
achieve high F1 score values when setting the threshold value to 1%.
As such, considering the dataset size may vary widely in practice,
we believe it is safe to uniformly set the threshold values of ALER
and MCER as 1%.
4.6 Impact of Word Richness
From Table 4, we observe that our solution performs differently
across 4 target datasets. By analyzing the datasets, we find the
difference resulting from unique words included in the datasets.
We thus explore how the unique word count affects our system
performance, by taking Kaggle SMS and Metsis Email datasets
as the examples, which contain 6,300 and 78,000 unique words,
respectively.
Figure 11 (a) and Figure 11(b) depict the trends of spam model
training accuracy and our solution’s F1 scores as the unique word
counts increase in Kaggle SMS and Metsis Email datasets, respec-
tively. We observe the spam model training accuracy keeps decreas-
ing with an increase in the unique word counts. The reason is that
given more unique words, the regression task in the neural network
model becomes more difficult. This degrades the accuracy of our
spam word model.
On the other hand, it is found that when the number of unique
words is small, the F1 scores of our solution keep improving with
an increase in the unique word count in both datasets. The reason
is that if the number of unique words is small, the Gibbs sampling
method employed in Section 3.2 is likely to overfit these words,
causing the model to suffer from low generalization. It misleads to
wrong spam scores in the spam model training phase, thus making
our system perform poorly. With more words included, this over-
fitting problem can be alleviated to help improve the performance
of our solution. However, when the unique word count becomes
excessive, i.e., around 40,000 in Figure 11 (b), the Gibbs sampling
method is likely to be saturated. In this situation, two words that
(a) Kaggle SMS dataset
(b) Metsis Email dataset
Figure 11: Impact of word richness in the target dataset.
are semantically or syntactically different may be forced to merge
into the same distribution and thus to assign with similar spam
scores. This will mislead our system, lowering the F1 score.
5 RELATED WORK
Existing works in spam detection can be categorized into supervised,
semi-supervised, and unsupervised methods.
Supervised Machine Learning methods rely on the ground truth
dataset to let a machine learning classifier learn the latent patterns
of spammers for classification. Extensive works are based on the
fact that spammers and normal users behave differently, making
it possible to extract effective features that can reveal such differ-
ences for the machine learning classifiers to learn latent patterns.
These features include, but are not limited to, user profiles [13],
user behaviors [23], message contents [21, 54], user relationships
[9, 49, 62]. Feature extraction also becomes prevalent in other ma-
chine learning-based applications such as fake review or news
detection [32, 40, 53], rumor detection [19, 63], etc. However, ef-
fective feature extraction has been well known as a challenging
problem, especially if we aim to leverage them across different so-
cial platforms. Moreover, all of the aforementioned works require to
have large-sized reliable ground truth datasets. It has been widely
recognized that acquiring a large-sized reliable ground truth dataset
012345678910theshold%0.00.20.40.60.8F1scoreSMSMetsisTrendNormal0200040006000#ofuniquewords406080100Value(%)SpammodeltrainingaccuracyOutlierdetectionF1-score0200004000060000#ofuniquewords406080Value(%)SpammodeltrainingaccuracyOutlierdetectionF1-score1074ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Yihe Zhang, Xu Yuan, and Nian-Feng Tzeng
is a challenging and painful problem. More importantly, some re-
search [29, 44, 52, 55] have realized that the supervised methods
may encounter substantial performance degradation when classify-
ing the imbalanced data (like spam messages only occupy a small
portion).
Semi-Supervised Learning methods [11, 18, 24, 64] have been
proposed to relax the reliance on the ground truth dataset. For
example, Zhou et al. [64] explored the supervision power from
multiple classifiers, where a labeling query occurs only if all clas-
sifiers are comparably confident on a disagreed unlabeled sample.
Chen et al. [11] proposed an asymmetric self-learning approach
that extracts “changed spams” from incoming tweets. Liu et al. [24]
proposed a solution for extracting time-sensitive features to track
the feature change, and Imam et al. [18] used unlabeled data to
learn the structure of the feature space, helping to refine the result
from supervised classifiers. SpamGAN [42] was also proposed by
using both unlabeled and labeled datasets to train a GAN-based
spam review classifier. However, they still rely on a certain amount
of reliable ground truth datasets, which are not easy to acquire.
Unsupervised Methods aim to let the spam detection task free
from labeling effort. Several categories of unsupervised methods
have been explored, such as behavior-based, content-based, and
graph-based ones, for spam detection. In the behavior-based cat-
egory, Mukherjee et al. [28], and Wang et al. [51] have developed
unsupervised solutions based on the observation that spammers and
non-spammers behave differently, able to model such behavioral
disparities by such features as frequency of activities, crawl actions,
register duration, click behaviors, and others. In the content-based
category, the hash values of the first k N-grams [56], the document
complexity [46], locality-sensitive hashing [59], min-hash [12], and
Natural Language Processing [34] have been investigated. In graph-
based methods, the social network graph is leveraged and analyzed
to find the differences of spams and hams [35, 43, 50] . However,
existing unsupervised methods, in general, markedly underperform
their supervised counterparts. Moreover, they are tailored to spe-
cific platforms, making them unable to adapt to multiple platforms.
Outlier Detection belongs to the unsupervised category as well
and it relies on the fact that spammer’s patterns have significant
disparities versus those of the major data volume, thus possessing
the outlier property potentially. Some works have been proposed
to explore the outlier property through different technologies or
approaches, i.e., data density [38], density-based clustering methods
[8], principal component analysis (PCA) [39], combined artificial