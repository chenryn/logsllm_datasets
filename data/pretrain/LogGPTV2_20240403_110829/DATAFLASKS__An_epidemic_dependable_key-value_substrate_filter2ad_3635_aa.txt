title:DATAFLASKS: An epidemic dependable key-value substrate
author:Francisco Maia and
Miguel Matos and
Ricardo Manuel Pereira Vilaça and
Jos&apos;e Pereira and
Rui Oliveira and
Etienne Riviere
DATAFLASKS: an epidemic dependable key-value substrate
Francisco Maia, Miguel Matos, Ricardo Vilac¸a, Jos´e Pereira, Rui Oliveira
High Assurance Software Laboratory
INESC TEC and UMinho
Email: {fmaia,miguelmatos,rmv,jop,rco}@di.uminho.pt
Braga, Portugal
Etienne Rivi`ere
Universit´e de Neuchˆatel
Switzerland
Email: PI:EMAIL
Abstract—Recently, tuple-stores have become pivotal struc-
tures in many information systems. Their ability to handle large
datasets makes them important in an era with unprecedented
amounts of data being produced and exchanged. However,
these tuple-stores typically rely on structured peer-to-peer
protocols which assume moderately stable environments. Such
assumption does not always hold for very large scale systems
sized in the scale of thousands of machines. In this paper we
present a novel approach to the design of a tuple-store. Our
approach follows a stratiﬁed design based on an unstructured
substrate. We focus on this substrate and how the use of
epidemic protocols allow reaching high dependability and
scalability.
Keywords-Dependability; Epidemic Protocols; Distributed
Systems; Large Scale Data Stores;
I. INTRODUCTION
Nowadays some of the most interesting challenges in
computing deal with large scale systems. Nowadays, proces-
sors are not getting faster at the same rate of previous years,
instead it is possible to have more of them [1]. This scenario
makes it possible to consider data centers with thousands
of machines. However, the development of services and
software systems that actually take advantage of large scale
systems is not a trivial task.
With the exponential growth in the amount of data being
produced and exchanged, several approaches were made to
build novel tuple-stores able to scale and take advantage
of larger resource pools [2]–[5]. Although these data stores
proved to be suitable and efﬁcient for a number of tasks [6],
they still rely on a structured peer-to-peer systems, typically
on a distributed hash table (DHT) or a variant of a distributed
hash table. Relying on distributed hash tables requires as-
suming of moderately stable environment in order to guar-
antee the overlay availability. However, as the system size
grows, the assumption of a moderately stable environment
becomes unrealistic. When reaching unprecedented number
of nodes, faults and churn become the rule instead of the
exception.
We posit that an unstructured but resilient approach to data
management is more appropriate in the context of such large-
scale systems. In [7], we proposed a novel two-layer ap-
proach to the design of a key-value data store. This two-layer
approach separates client interface and concurrency control
(top layer) from the actual data storage (bottom layer).
The proposal is to follow a completely decentralized and
unstructured approach to the design of the latter. In this paper
we focus on the bottom layer describing DATAFLASKS: an
highly resilient and dependable data storage system aimed
at very large scale environments.
the
the
design
Along
present
paper, we
of
DATAFLASKS and describe the main challenges of such
an approach. We also address some of these challenges
with concrete solutions and point out our research path to
tackle the remaining. We also evaluate the current version
of DATAFLASKS in order to validate its scalability.
The remaining of the paper is as follows. Section II
provides some background related with epidemic proto-
cols. In Section III we describe the context
in which
DATAFLASKS appears and its requirements. Section IV
presents DATAFLASKS design and Section V its architec-
ture. Evaluation appears in Section VI. We describe open
challenges and future steps in Section VII.
II. BACKGROUND
The fact that we are targeting very large scale systems
raises a number of interesting challenges. Notably, in very
large scale systems any kind of global knowledge is unattain-
able. Any system that relies on information that grows
linearly with system size does not scale and, therefore, is
unusable in such a scenario. As a consequence, large scale
protocols must solely rely on partial information about the
system and on node-local decisions. Fortunately, there is a
class of well studied protocols that meet such requirements:
epidemic or gossip-based protocols.
A typical epidemic protocol operates as follows. Each
node has a set of neighbors, called its view. The protocol
progresses by having each node periodically exchanging
knowledge with one or several of its neighbors.
The ﬁrst problem to address is how to maintain the
neighbor list refreshed even considering that nodes can
become disconnected or simply leave the system. Besides
maintaining the view refreshed it is important that it exhibits
some properties. In particular, epidemic protocols beneﬁt
from views composed by a uniformly random sample of
nodes [8]. If the view is, in fact, a random sample of nodes,
978-1-4799-0181-4/13/$31.00 ©2013 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:12 UTC from IEEE Xplore.  Restrictions apply. 
choosing a random peer from such list is equivalent to
choose randomly from all the nodes in the system. The
problem of providing random views of nodes in the system
falls in a well studied problem in large scale distributed
systems that has been addressed by a family of protocols
known as the Peer Sampling Service. These protocols are
gossip protocols themselves. In a nutshell, each node keeps
a set of nodes it knows. Periodically, it refreshes such set
by contacting one ore more of those nodes and exchanging
information. Notably, this apparently simple approach allows
these protocols to provide each node with a random stream
of uniformly sampled nodes. Examples of these protocols
are Cyclon [9] and Newscast [10].
Interestingly, the collection of views generated by the Peer
Sampling Service not only serves as the support for other
epidemic protocols but also as an information dissemination
medium. From early work on random graphs [11], we
know that it is possible, with arbitrarily high probability, to
effectively disseminate data in a epidemic fashion provided
that each node relays a sufﬁcient number of messages. In
particular, taking N as the number of nodes, each node
must relay ln(N ) + c messages to have a probability of
atomic infection of patomic = −−c. Considering views of
such size, uniformly sampled from the all set of nodes, an
overlay emerges that allows for epidemic data dissemination.
Note that these views do not grow linearly with system
size and are, therefore, a scalable dissemination mechanism.
This dissemination mechanism serves as the base for request
dissemination in DATAFLASKS.
To complete our background section we should men-
tion distributed systems slicing techniques. Slicing a large-
scale distributed system is the process of autonomously
partitioning its nodes into k groups, named slices based
on some criteria. For instance, it is possible to slice the
system according to node storage space or their uptime.
Previous work [9], [12]–[17], showed that it is possible to
achieve such goal without any kind of global knowledge
using epidemic approaches. These slicing protocols rely on
a Peer Sampling Service and notiﬁes each node of the slice
it belongs to. Slicing will be important for the design of
DATAFLASKS.
III. STRATUS
In [18] we proposed a novel massive data store system.
In this paper, we will refer to such system as STRATUS.
STRATUS is a stratiﬁed system where a clear separation of
concerns between layers is of crucial importance. It is this
separation of concerns that allows this design to scale to
thousands of nodes.
The architecture overview of STRATUS is presented in
Figure 1. Although the present paper will focus on the
bottom layer of STRATUS it is important to provide some
context. In particular,
to deﬁne the set of assumptions
supporting such layer.
Figure 1. STRATUS Architecture.
Similarly to traditional relational database management
systems, in our proposal there is a clear separation between
i) client interface and concurrency control concerns and ii)
data storage itself. The ﬁrst set of concerns is addressed by
the STRATUS top layer named DATADROPLETS. This upper
layer provides 1) client interface, 2) caching, 3) concurrency
control, and 4) high level processing. As described in [7],
even though DATADROPLETSis itself decentralized, we as-
sume this layer to run mainly in memory and in a moderate
size environment. This allows DATADROPLETS to be based
on a structured approach. The main challenges of this layer
have been addressed [19], [20].
Note that DATADROPLETS has some key characteris-
tics that yield speciﬁc responsibilities to the underlying
DATAFLASKS. Firstly, its structured approach (aimed at a
moderately sized environment) delegates large scale con-
cerns to the bottom layer. Secondly, it is a soft-state layer
meaning that, in case of catastrophic failure, it should be
possible to reconstruct it completely from the persistent-state
layer. This implies that DATAFLASKS must provide data
persistence and high availability. Finally, it is responsible
for concurrency control and client interface. In our design,
this means that DATAFLASKS does not need to take into
account conﬂicts arising from concurrent operations. In
fact, DATADROPLETS is responsible for correctly ordering
requests, which, in our design, is done by attaching version
stamps to every object.
A major requirement for DATAFLASKS is data availability.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:12 UTC from IEEE Xplore.  Restrictions apply. 
                                                                             Soft-state layer(DataDroplets)ClientsDHTPersistent-state Layer (DataFlasks)STRATUSSince this layer targets massive scale deployments it should
be able to achieve data availability even in highly unstable
environments. With this aim in mind, our proposal is to
take advantage of high scalability and resilience of epidemic
protocols. These have been used previously to build several
webscale systems and applications [21] like overlay con-
struction and maintenance [9], [22], consensus [23], data
aggregation [24] and distributed slicing [12], [15], [16].
As a result of this design, DATAFLASKS serves as a
persistence layer. It must store and serve objects addressed
by an identiﬁer following an API consisting of get(key)
and put(key, value) operations. Each object is an array of
arbitrary bytes and the only assumption we make about the
upper layer is that operations are ordered. In our design this
is done by having each object carry a version.
IV. DATAFLASKS
In this Section we introduce the design of DATAFLASKS.
We consider a set of nodes, on the order of thousands, where
anyone can receive requests from the upper layer. We assume
simple put and get operations. Put operations on the same
item are totally ordered. Get operations specify the requested
version of the item. The following challenges remain:
A.
B.
C.
How to distribute data;
How to route requests;
How to replicate data;
A. Data distribution
The main idea behind our solution to data distribution
in DATAFLASKS is to disseminate data in an epidemic
fashion and have nodes locally decide if they need to store
that individual item. To achieve this we divide the system
into sets of nodes. Each set will be responsible for storing
a subset of the data according to its key range. Slicing
mechanisms make it possible to have a system autonomously
dividing itself into sets of nodes (slices) without any kind
of global knowledge. Moreover, this process can leverage
certain locally measured attributes and partition the system
according to some criterion. In our case the system will be
sliced according to the individual node storage capacity. This
allows that a certain node with less capacity is assigned with
less data to store. Any other criteria could be used, though.
Slicing protocols provide each node with the slice it
belongs to and thus serve as a data distribution mecha-
nism. In addition, these protocols are highly resilient and
continuously adapt to changes in the system, let them be
caused by faults or churn. Note that, we could simply
toss a coin and decide to which slice a node belongs to.
Provided we had uniformity on that process it would be
enough for partitioning the system. However, such approach
is not resilient to correlated faults. For instance, if, for some
reason, a signiﬁcant portion of a certain slice fails there
would be no way of effectively recover by coin tossing. On
the other hand, the slicing mechanism is able to adjust to
such failure and nodes would change slice in order to balance
system distribution.
B. Request routing
We have provided a mechanism to divide the system into
slices (sets of nodes) and to distribute objects across nodes
with the help of such partitioning. It is necessary now for
requests to reach the appropriate nodes. A write request must
reach the corresponding set of nodes and a read request must
reach at least one node holding the target item.
Both the request routing and data dissemination is ac-
complished with a Peer Sampling Service. As mentioned
before, each node can use the view provided by the Peer
Sampling Service to disseminate information. In order for a
node to disseminate a message in the system it sufﬁces to
send it to the nodes in its local view. However, it is important
to notice that disseminating all requests to all nodes is
rather inefﬁcient. In DATAFLASKS, some optimizations are
possible which avoid such problems. For instance, atomic
dissemination for most requests in not needed. It is sufﬁcient
to reach only the percentage of system nodes that guarantees
that some nodes of the target slice are reached. Following the
ideas described in [17], we consider a Peer Sampling Service
intra-slice. Once a request reaches a node in its target slice,
dissemination is done only to nodes of that slice.
C. Data replication
To tolerate node failures and churn data is replicated
among the nodes of each slide.
Although straightforward, this approach raises some chal-
lenges. Namely, with regard to the decision on the number of
slices to choose and to their size. Fortunately, recent slicing
protocols [17] allow for dynamic conﬁguration of the slicing
mechanism. This opens the door to autonomous mechanisms
for replication management. Note that, for the same system