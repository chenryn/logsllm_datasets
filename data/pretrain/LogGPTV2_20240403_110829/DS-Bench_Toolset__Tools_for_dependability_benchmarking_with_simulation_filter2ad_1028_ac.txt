simulated on virtual machines. If the system requires many
computers and it is difﬁcult to prepare the required number
of physical machines, virtual machines may also be used.
We have adopted the OpenStackTMplatform for managing
massive Virtual Machines since OpenStack has become
the de-facto standard for open-source cloud management
software [21]. OpenStack is written mainly in Python, and
we can easily modify the source code on-the-ﬂy, if needed.
A cloud computing environment consists of one or more
cloud-controller node(s) and multiple VM nodes. The cloud
controller node manages various kinds of OS system images
that run on virtual machines. The controller transfers an OS
system image to each VM node to execute the OS on the
Virtual Machine Monitor in the VM node. A VM node
also receives commands for fault injection based on the
fault scenario, and injects a series of faults into the Virtual
Machine Monitor.
D. DS-Bench Results as Evidence for D-Case
DS-Bench is designed to be a part of a system develop-
ment process assured by D-Case. In particular, DS-Bench
provides evidence that systems actually meet requirements
written in D-Case.
In order to support this workﬂow, D-Case Editor and DS-
Bench are designed so that D-Case Editor can import the
results from DS-Bench as evidence for D-Case. A basic
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:21:02 UTC from IEEE Xplore.  Restrictions apply. 
SPECIFICATIONS OF DEMONSTRATION ENVIRONMENT
Table I
# of Nodes
CPU
Memory
OS
Network Switch
Cloud SW
VM
6 (Controller+VM x1, VM x2, Phys x3)
Intel R(cid:3) Xeon R(cid:3)processor E5506 x2 (8 cores)
24GB
UbuntuTM 10.04
DellTM PowerConnectTM 5324
OpenStack Diablo (VLAN Manager)
KVM 0.14.0
workﬂow using the toolchain is shown in Figure 6 and
described below. First, stakeholders of the system being
developed discuss the dependability requirements of the
system, then record the conclusion as a D-Case diagram
using D-Case Editor (Figure 8). The diagram may con-
tain some quantitative requirements, such as performance
requirements (e.g. “the server shall process the request at
a rate of more than 250req/s”) or availability requirements
(e.g. “the downtime of the system shall be shorter than 5
seconds under one node failure”). Then DS-Bench is used
to measure these quantitative scores from the system directly.
A D-Case Editor user chooses and imports a benchmark
scenario suitable for the current situation, from the bench-
mark database of DS-Bench. If no such scenario is found in
the database, a user may create a new one. Then the user
may modify some of the parameters deﬁned in the scenario
according to D-Case. For example, in some scenario, the
request rate from a client machine may be modiﬁed. In
addition, the user declares the requirements for the result of
the scenario, such as the maximum latency of all requests.
After that, the user runs the benchmark test from D-
Case Editor. D-Case Editor then requests DS-Bench to
execute the benchmark test. When the test completes, D-
Case Editor retrieves the result to show whether the result
meets the requirement. A new node, called an evidence node,
is added to the leaf of the D-Case diagram, which represents
the benchmark result as evidence supporting the successful
meeting of the requirement. If the requirement is met, the
evidence node is shown as a blue node. Otherwise, it is
shown as a red node. A link to information on the details
of the benchmark is recorded in the evidence node (Figure
8).
IV. DEMONSTRATION OF DS-BENCH TOOLSET
This section describes a demonstration of DS-Bench
Toolset. A typical web server-client system is introduced,
then it is shown how DS-Bench Toolset can be used with
the example system to improve dependability.
A. Target System
Figure 7 shows an overview of the target demonstration
system. The system simulates a typical web server system
that hosts the MoinMoin Wiki system [22].
Virtual
Machines
Demo-
client
Demo-
client
Single 
IP Address
Fault
(Linkdown)
Switch
HTTP
HTTP
Tammie06
Tammie07
Wiki files
Physical
Machines
File server
Figure 7. Target System for Demonstration of DS-Bench Toolset
The server system consists of front-end web servers
with an Apache HTTP Server, and a shared ﬁle server.
Multiple client machines concurrently access the server. In
this demonstration, the front-end web servers are prepared
as physical machines, and the client machines are prepared
as virtual machines. This is because it is crucial to measure
the server performance using the actual hardware, while it is
much more important for clients to be deployed easily and
require less physical hardware resources.
This system has two web server nodes, and two nodes
form a single IP address web server cluster using the SSPA
(Speculative SYN Packet Acceptance) mechanism [23]. By
using the SSPA mechanism, all trafﬁc from clients to the
server is broadcasted to both of the server nodes. New
connection requests from clients are distributed to both of
the server nodes. If one server node is not responding, the
other server node responds to the client immediately.
Table I indicates the speciﬁcations of the hardware and
software environments for this target demonstration system.
B. Test Example
In this demonstration, we focus on a web server failure
case. The web server failure is simulated by shutting down
the network link by controlling a network switch connected
to the target physical server.
Figure 8 shows a part of the D-Case diagram prepared
for this server system. This sub tree illustrates a discussion
on a failure that one server fails during the benchmark
test. Of course, there should be many other branches in the
diagram, which discuss other kinds of faults. However they
are omitted because space is limited.
Figure 9 is a screenshot of the benchmark scenario
editing mode of DS-Bench. Each horizontal line represents a
timeline for scheduled jobs (benchmarks and anomaly loads)
for each target machine. In this ﬁgure we see three target
machines, from the top, server node 1, client machine 1,
and client machine 2. We do not discuss server machine 2
on the timeline because we do not execute any benchmark
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:21:02 UTC from IEEE Xplore.  Restrictions apply. 
test is executed from D-Case Editor. The node G 18 in the
rightmost branch in Figure 8 appears when a benchmark
scenario is imported to D-Case Editor, and the node E 11
appears when the benchmark test completes. The node E 11
in the ﬁgure is ﬁlled in with blue, which means that the result
(a maximum latency of 664.0ms) satisﬁed the requirement
(3000ms).
C. Discussion
As shown in the above example, currently D-Case Editor
conducts only one benchmark run for obtaining one instance
of evidence. However this is sometimes not sufﬁcient, be-
cause benchmark results may differ for each benchmark run.
Therefore, some features, like repeating the same benchmark
test several times automatically, then applying some “data
reduction” operations (e.g. ave/max/min) to these results to
obtain instance of an evidence may be needed. Currently
neither D-Case Editor nor DS-Bench have such a feature,
but it can be implemented. How many times the benchmark
should be repeated, or how the result should be treated
(e.g. whether the average works, or the maximum should be
taken) to be used as an instance of evidence, are discussed
and agreed upon among the stakeholders of the system.
In the experiment described above, it was found by a
visual inspection of the access logs of the web server that
the web server worked properly at the TCP level, however,
the Wiki application did not work correctly (it returned
HTTP 503 status). In order to validate the dependability
of the system thoroughly, the D-Case diagram should have
mentioned to the application-level results such as HTTP
status of the requests, and the benchmarking tool should
have checked the status.
V. CONCLUSION
This paper has described DS-Bench Toolset, a toolset for
assuring system dependability based on cases and quanti-
tative measurements. The toolset consists of D-Case Editor,
DS-Bench, and D-Cloud. D-Case Editor is an assurance case
editor which collaborates with DS-Bench and D-Cloud to
treat the benchmark test results as evidences for the depend-
ability of the system. DS-Bench is a software framework
for conducting several dependability indicator measurements
according to a speciﬁc scenario which consists of bench-
marking programs and anomaly generators. Both benchmark
programs and anomaly generators may be existing programs
or newly created ones. D-Cloud is a test environment
that supports both physical and virtual machines as target
machines for a DS-Bench benchmark scenario. D-Cloud
also supports several types of fault injection by emulating
hardware faults. We have demonstrated an example of a use
case of DS-Bench Toolset using a simple web server system,
and showed how DS-Bench Toolset has provided evidence
of the dependability of the system.
Figure 8. DS-Bench Results as Evidence
Figure 9. A Benchmark Scenario for the Demonstration
programs or anomaly loads on that node. On the timeline,
schedules for benchmark programs are shown with blue bars,
and the schedule for the anomaly load is shown as a red line.
As a benchmark program, an HTTP server measurement
tool based on httperf [24] is used. Httperf is modiﬁed so
that
it can measure the downtime of a web server. As
the anomaly load, a script shuts down a network link by
controlling a network switch via the SNMP protocol. The
overall benchmark lasts for 60 seconds, at 30 seconds from
the start, the network link for server node 1 is shut down.
From that point, server node 1 is no longer accessible, but
the service continues because server node 2 handles all
requests. In this scenario, the parameters for httperf (the total
number of connections and the request rate) are marked as
customizable parameters. Also, the result of the benchmark
is deﬁned as the maximum latency observed in all request
attempts for all client machines, which is obtained using the
reduction feature.
A stakeholder of the system (say, the operator of the
site) conducts the benchmarking test from D-Case Editor.
First, the above benchmark scenario is imported from DS-
Bench to D-Case Editor. Then benchmark parameters and
the expected result are set (Figure 2). Finally the benchmark
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:21:02 UTC from IEEE Xplore.  Restrictions apply. 
Future work will consist of attempts to apply DS-Bench
Toolset to a real system development and evaluation process.
In addition, exploiting existing benchmark results stored in
the database should be discussed further. For instance, it may
be useful to search the database for existing results that may
be suitable as evidences for the system currently focused on,
allowing users to skip the benchmark execution.
ACKNOWLEDGMENT
This work has been supported by the JST CREST “De-
pendable Operating System for Embedded Systems Aiming
at Practical Application” project.
D-Case Editor has been developed with Fuji Xerox Co.,
Ltd. Especially, we thank Mr. Hajime Ueno for designing
DS-Bench Toolset with us.
Windows NT is a registered trademark of Microsoft
Corporation in the U.S. and other countries. Eclipse is a
trademark of Eclipse Foundation, Inc. Intel and Xeon are
trademarks of Intel Corporation in the U.S. and/or other
countries. Other products or brand names appear in this
paper may be the trademarks of their owners.
Last but not least, we thank the anonymous reviewers for
many helpful comments.
REFERENCES
[1] A. Avizienis, J.-C. Laprie, B. Randell, and C. Landwehr,
“Basic concepts and taxonomy of dependable and secure
computing,” IEEE Trans. Dependable Secure Comput., vol. 1,
pp. 11–33, 2004.
[2] K. Kanoun
et
al.,
“DBench
bench-
marks,” Project Full Final Report, DBench Project,
May
http://spiderman-
2.laas.fr/DBench/Final/DBench-complete-report.pdf
dependability
Available:
[Online].
2004.
[3] J. Dur˜aes et al., “Dependability benchmarking of web-
servers,” in Proc. 23rd Intl. Conf. on Comput. Safety, Re-
liability and Security (SAFECOMP 2004), ser. LNCS, vol.
3219, 2004, pp. 297–310.
[4] J. Dur˜aes and H. Madeira, “Generic faultloads based on
software faults for dependability benchmarking,” in Proc.
DSN, 2004, pp. 285–294.
[5] M.-E. Begin et al., “Build, conﬁguration,
integration and
testing tools for large software projects: ETICS,” in Proc.
Rapid Integration of Softw. Eng. Techniques, ser. LNCS, vol.
4401, Sep. 2007, pp. 81–97.
[6] S. Han, K. Shin, and H. Rosenberg, “DOCTOR: an integrated
software fault injection environment for distributed real-time
systems,” Intl. Comput. Performance and Dependability Sym-
posium, p. 0204, 1995.
[7] J. Carreira, H. Madeira, and J. G. Silva, “Xception: A
technique for the experimental evaluation of dependability in
modern computers,” IEEE Trans. Softw. Eng., vol. 24, pp.
125–136, 1998.
[8] A. Baldini, A. Benso, S. Chiusano, and P. Prinetto, “’BOND’:
An interposition agents based fault injector for Windows NT,”
in Proc. IEEE Intl. Symposium on Defect and Fault-Tolerance
in VLSI Syst. (DFT ’00), 2000, p. 387.
[9] S. Potyra, V. Sieh, and M. D. Cin, “Evaluating fault-tolerant
system designs using FAUmachine,” in Proc. Workshop on
Eng. Fault Tolerant Systems (EFTS ’07), 2007, p. 9.
[10] D. Stott et al., “NFTAPE: a framework for assessing depend-
ability in distributed systems with lightweight fault injectors,”
in Proc. Comput. Performance and Dependability Symposium
(IPDS 2000), 2000, pp. 91–100.
[11] Engineering Safety Management, Issue3 (Yellow Book 3), Vol.
1 Fundamentals, and Vol. 2 Guidance, Railtrack, 2000.
[12] C. C. Howell et al., Ed., Proc. Workshop on Assurance Case
(DSN 2004), 2004.
[13] ASCE
tools.
http://www.adelard.com/web/hnav/ASCE/choosing-
asce/cae.html
[Online].
Available:
[14] T. Kelly and R. Weaver, “The goal structuring notation – a
safety argument notation,” in Proc. Workshop on Assurance
Cases (DSN 2004), Jul. 2004.
[15] P. J. Graydon, J. C. Knight, and E. A. Strunk, “Assurance
based development of critical systems,” in Proc. DSN, 2007,
pp. 347–357.
[16] D-Case Editor.
[Online]. Available: http://www.il.is.s.u-
tokyo.ac.jp/deos/dcase/
[17] Y. Matsuno and K. Taguchi, “Parameterised argument struc-
ture for GSN patterns,” in Proc. 11th Intl. Conf. on Quality
Softw. (QSIC 2011), 2011, pp. 96–105.
[18] T. Hanawa et al., “Large-scale software testing environment
using cloud computing technology for dependable parallel
and distributed systems,” in Proc. 2nd Intl. Workshop on
Softw. Testing in the Cloud (STITC 2010), Apr. 2010, pp.
428–433.
[19] T. Banzai et al., “D-Cloud: Design of a software testing
environment for reliable distributed systems using cloud com-
puting technology,” in Proc. 2nd Intl. Symposium on Cloud
Computing (Cloud 2010), May 2010, pp. 631–636.
[20] T. Hanawa et al., “Customizing virtual machine with fault in-
jector by integrating with SpecC device model for a software
testing environment D-Cloud,” in Proc. 16th IEEE Paciﬁc Rim
Intl. Symposium on Dependable Computing (PRDC ’10), Dec.
2010, pp. 47–54.
[21] OpenStack: The open source, open standards cloud. [Online].
Available: http://openstack.org/
[22] MoinMoin, “The MoinMoin Wiki Engine,” http://moinmo.in/.
[23] H. Fujita and Y. Ishikawa, “Anytime available single IP
address cluster,” in Proc. 13th IEEE Intl. High Assurance Syst.
Eng. Symposium (HASE 2011), Nov 2011, pp. 168–173.
[24] D. Mosberger and T. Jin, “httperf: a tool for measuring web
server performance,” SIGMETRICS Performance Evaluation
Review, vol. 26, no. 3, pp. 31–37, 1998.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:21:02 UTC from IEEE Xplore.  Restrictions apply.