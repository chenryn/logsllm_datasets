### Simulation on Virtual Machines
When a system requires a large number of computers and it is challenging to prepare the necessary physical machines, virtual machines (VMs) can be used as an alternative. We have adopted the OpenStackâ„¢ platform for managing a large number of VMs, as it has become the de facto standard for open-source cloud management software [21]. OpenStack is primarily written in Python, which allows for easy modification of the source code if needed.

A cloud computing environment consists of one or more cloud controller nodes and multiple VM nodes. The cloud controller node manages various OS system images that run on the VMs. It transfers an OS system image to each VM node, where the OS is executed on the Virtual Machine Monitor (VMM) within the VM node. Additionally, a VM node can receive commands for fault injection based on a predefined fault scenario and inject a series of faults into the VMM.

### DS-Bench Results as Evidence for D-Case
DS-Bench is designed to be an integral part of the system development process assured by D-Case. Specifically, DS-Bench provides evidence that systems meet the requirements specified in D-Case.

To support this workflow, D-Case Editor and DS-Bench are designed so that D-Case Editor can import results from DS-Bench as evidence for D-Case. A basic workflow using the toolchain is illustrated in Figure 6 and described below:

1. **Requirement Discussion**: Stakeholders discuss the dependability requirements of the system and record the conclusions as a D-Case diagram using D-Case Editor (Figure 8). The diagram may include quantitative requirements such as performance (e.g., "the server shall process the request at a rate of more than 250 req/s") or availability (e.g., "the downtime of the system shall be shorter than 5 seconds under one node failure").

2. **Benchmark Selection**: A D-Case Editor user selects and imports a suitable benchmark scenario from the DS-Bench database. If no suitable scenario is found, a new one can be created. The user can modify parameters defined in the scenario according to the D-Case requirements. For example, the request rate from a client machine can be adjusted. The user also specifies the expected result, such as the maximum latency of all requests.

3. **Benchmark Execution**: The user runs the benchmark test from D-Case Editor, which then requests DS-Bench to execute the test. Upon completion, D-Case Editor retrieves the results and checks if they meet the requirements. A new node, called an evidence node, is added to the D-Case diagram, representing the benchmark result as evidence. If the requirement is met, the evidence node is shown in blue; otherwise, it is shown in red. A link to detailed information about the benchmark is recorded in the evidence node (Figure 8).

### Demonstration of DS-Bench Toolset
This section describes a demonstration of the DS-Bench Toolset. A typical web server-client system is introduced, and it is shown how DS-Bench can be used to improve the dependability of the system.

#### Target System
Figure 7 shows an overview of the target demonstration system, which simulates a typical web server system hosting the MoinMoin Wiki system [22]. The server system consists of front-end web servers with an Apache HTTP Server and a shared file server. Multiple client machines concurrently access the server. In this demonstration, the front-end web servers are prepared as physical machines, while the client machines are prepared as VMs. This setup is chosen because it is crucial to measure server performance using actual hardware, while clients can be deployed easily and require fewer physical resources.

The system has two web server nodes forming a single IP address web server cluster using the SSPA (Speculative SYN Packet Acceptance) mechanism [23]. By using SSPA, all traffic from clients to the server is broadcasted to both server nodes, and new connection requests are distributed to both nodes. If one server node fails, the other node responds immediately.

Table I provides the specifications of the hardware and software environments for this target demonstration system.

#### Test Example
In this demonstration, we focus on a web server failure case, simulated by shutting down the network link via a network switch connected to the target physical server. Figure 8 shows a part of the D-Case diagram prepared for this server system, illustrating a discussion on a failure where one server fails during the benchmark test.

Figure 9 is a screenshot of the benchmark scenario editing mode in DS-Bench. Each horizontal line represents a timeline for scheduled jobs (benchmarks and anomaly loads) for each target machine. In this figure, we see three target machines: server node 1, client machine 1, and client machine 2. The timeline does not include server machine 2 because no benchmark tests are executed on it.

As a benchmark program, an HTTP server measurement tool based on httperf [24] is used, modified to measure the downtime of a web server. As the anomaly load, a script shuts down the network link by controlling a network switch via the SNMP protocol. The overall benchmark lasts for 60 seconds, with the network link for server node 1 being shut down at the 30-second mark. From that point, server node 1 becomes inaccessible, but the service continues as server node 2 handles all requests. The parameters for httperf (total number of connections and request rate) are customizable, and the result is defined as the maximum latency observed in all request attempts for all client machines.

A stakeholder (e.g., the site operator) conducts the benchmarking test from D-Case Editor. First, the benchmark scenario is imported from DS-Bench to D-Case Editor. Then, benchmark parameters and the expected result are set. Finally, the benchmark test is executed, and the results are imported as evidence in the D-Case diagram.

### Discussion
Currently, D-Case Editor conducts only one benchmark run for obtaining one instance of evidence. However, this may not be sufficient, as benchmark results can vary between runs. Features such as automatically repeating the same benchmark test several times and applying data reduction operations (e.g., average, maximum, minimum) to these results could be beneficial. While neither D-Case Editor nor DS-Bench currently supports these features, they can be implemented. The number of repetitions and the method of result treatment (e.g., whether to use the average or the maximum) should be discussed and agreed upon among the stakeholders.

In the experiment, it was found through visual inspection of the web server's access logs that the server worked properly at the TCP level, but the Wiki application did not (it returned HTTP 503 status). To thoroughly validate the system's dependability, the D-Case diagram should include application-level results, and the benchmarking tool should check the HTTP status.

### Conclusion
This paper describes the DS-Bench Toolset, a toolset for assuring system dependability based on cases and quantitative measurements. The toolset consists of D-Case Editor, DS-Bench, and D-Cloud. D-Case Editor is an assurance case editor that collaborates with DS-Bench and D-Cloud to treat benchmark test results as evidence for the system's dependability. DS-Bench is a software framework for conducting dependability indicator measurements according to specific scenarios, including benchmarking programs and anomaly generators. Both benchmark programs and anomaly generators can be existing or newly created. D-Cloud is a test environment that supports both physical and virtual machines as target machines for DS-Bench benchmark scenarios and emulates hardware faults.

We demonstrated an example use case of the DS-Bench Toolset using a simple web server system and showed how it provided evidence of the system's dependability.

### Future Work
Future work will involve applying the DS-Bench Toolset to real system development and evaluation processes. Additionally, further discussion is needed on exploiting existing benchmark results stored in the database, allowing users to skip the benchmark execution when suitable results are available.

### Acknowledgment
This work has been supported by the JST CREST "Dependable Operating System for Embedded Systems Aiming at Practical Application" project. D-Case Editor was developed in collaboration with Fuji Xerox Co., Ltd., and we thank Mr. Hajime Ueno for designing DS-Bench Toolset with us.

Windows NT is a registered trademark of Microsoft Corporation in the U.S. and other countries. Eclipse is a trademark of Eclipse Foundation, Inc. Intel and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries. Other products or brand names mentioned in this paper may be trademarks of their respective owners.

Lastly, we thank the anonymous reviewers for their valuable comments.

### References
[References are listed as in the original text.]

---

This revised version aims to provide a clear, coherent, and professional presentation of the content.