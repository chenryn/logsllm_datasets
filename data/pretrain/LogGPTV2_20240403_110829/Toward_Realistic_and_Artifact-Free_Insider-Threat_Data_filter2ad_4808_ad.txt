### Impact of Sanitization on Experimental Results

When no sanitization is applied (i.e., using raw data), we observe certain discrepancies, which are discussed in Section 8. 

#### Consequences of Sanitization
If there were no artifacts introduced by sanitization, all the results should match those obtained from the raw data. However, only Word-Token sanitization produced results that matched the raw data. Token-Only sanitization increased the cost associated with Truncated data, while Redact-Only sanitization reduced the cost for Enriched data.

It might appear that Token-Only sanitization has no effect on Enriched data or that Redact-Only sanitization has no effect on Truncated data. Additionally, it may seem that the consequences of using either sanitization strategy instead of raw data are minimal. For example, Redact-Only sanitization only changes the detector's response to three Enriched sessions (i.e., two fewer misses and one fewer false alarm). However, misses and false alarms provide a coarse summary of the experimental outcomes. Sanitization also affects the anomaly scores and alarm thresholds used by naive Bayes, and in these terms, the effects of Redact-Only and Token-Only sanitization are more pronounced across both Truncated and Enriched data sets.

#### Detailed Analysis of Anomaly Scores
The anomaly scores computed by naive Bayes for each session provide a finer-grained view of the experimental results. These scores offer valuable insights into the decision-making process of the detector and can help explain why errors occurred (e.g., why an attack-injected session was missed). Without accurate anomaly scores, it is more challenging to identify what the detector did wrong and how to improve it. The effects of Redact-Only and Token-Only sanitization are much greater on the anomaly scores than on the hit and false-alarm statistics. Table 2 shows that Redact-Only and Token-Only sanitizations affect many Truncated and Enriched anomaly scores, while Word-Token does not. Redact-Only has the greatest effect on the anomaly scores, with all the Enriched anomaly scores differing from the raw data scores. Redactions in the training data caused all the probability estimates to change, which in turn altered the anomaly scores.

#### Alarm Thresholds
The alarm threshold used by naive Bayes to decide whether an anomaly score should trigger an alarm depends on the anomaly scores calculated during cross-validation. When the anomaly scores change due to sanitization, so does the threshold. Redact-Only sanitization tended to lower the threshold (partially explaining the lower miss rate on the Enriched data), while Token-Only sanitization tended to raise it (partially explaining the higher miss rate on the Truncated data). Again, Word-Token sanitization had no effect on the alarm threshold.

### Discussion and Future Work
This research confirms that Word-Token sanitization is artifact-free for experiments evaluating the naive-Bayes anomaly detector. In theory, Word-Token sanitization should be artifact-free for any experiment where tokens can be substituted for words. For instance, it should not introduce artifacts in the evaluation of other anomaly detectors in the same family (e.g., Lane and Brodley’s and Schonlau et al.’s detectors). A more lenient sanitization strategy might only replace occurrences of words in specific contexts (e.g., “God” will only be replaced with a symbol when it appears as a password). This strategy might provide more privacy while continuing to avoid introducing artifacts. Further experiments with various detectors and different sanitization strategies are needed to develop this theory.

When tokens cannot be substituted for words (e.g., in evaluations of signature-based systems), Word-Token sanitization may introduce artifacts. However, just as Word-Token sanitization was designed to be artifact-free for a certain family of detectors, a different artifact-free sanitization technique might be designed to accommodate different detectors. The key takeaway is that sanitization can have undesirable consequences on the outcome of an experiment, and researchers must account for this.

Since sanitization artifacts alter the results of experiments, it stands to reason that the unrealistic use of benign commands as a substitute for insider commands will cause similar problems. Our use of more realistic insider-type behavior may explain the discrepancy between our findings and Maxion’s about the cost of using Truncated and Enriched commands. (Note that this explanation is conjecture, as the goal of this study was not to refute earlier work, and the discrepancy might be explained by other means, such as our small subject-pool size.) We reiterate that the unrealistic use of benign commands in insider-injections may have the same dangerous consequences as sanitization artifacts. Investigating the consequences of unrealistic insider injections remains a topic for future work.

### Summary and Conclusion
Sanitization artifacts did not affect the experiment when Word-Token sanitization was used, but they did alter the results when Redact-Only and Token-Only strategies were used. Since existing insider-threat data sets [2, 5, 11] all used some form of redaction or tokenization, these findings raise concerns. Conclusions based on these data sets may not generalize to the real world. Insider-threat data sets must be validated before we place real-world reliance on them. If an insider-threat detector is chosen for deployment based on precarious conclusions, and that detector under-performs, the consequences carry considerable risk. On the other hand, we demonstrate that appropriate tools and methods make it possible to collect realistic data and perform sanitization without introducing artifacts.

### Acknowledgments
The authors are grateful for helpful comments from Kymie Tan and Dan Siewiorek, as well as from anonymous reviewers. Many thanks to Fahd Arshad for his implementation of the sanitizer application. Thanks also to the CMU/CS Operations Group for their assistance in the evaluation portion of the project. This work was supported by National Science Foundation grant number CNS-0430474 and by the Army Research Office through grant number DAAD19-02-1-0389 (Perpetually Available and Secure Information Systems) to Carnegie Mellon University’s CyLab. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of any sponsoring institution, the U.S. government, or any other entity.

### References
[1] D. Denning. An intrusion-detection model. IEEE Transactions on Software Engineering, 13(2), February, 1987.
[2] S. Greenberg. Using Unix: Collected traces of 168 users. Technical Report 88/333/45, Department of Computer Science, University of Calgary, Calgary, Canada, 1988.
[3] The Honeynet Project. Know Your Enemy: Sebek, November 2003. http://www.honeynet.org/papers/sebek.pdf.
[4] M. Keeney, E. Kowalski, D. Cappelli, A. Moore, T. Shimeall, and S. Rogers. Insider threat study: Computer system sabotage in critical infrastructure sectors. Technical report, U.S. Secret Service and CERT Coordination Center/SEI, 2005.
[5] T. Lane and C. E. Brodley. An application of machine learning to anomaly detection. In Proceedings of the 20th Annual National Information Systems Security Conference, pages 366–380. Held on 7–10, October, 1997, Baltimore, MD, NIST, 1997.
[6] M. V. Mahoney and P. K. Chan. An analysis of the 1999 DARPA/Lincoln Laboratory evaluation data for network anomaly detection. In Proceedings of the 6th International Symposium on Recent Advances in Intrusion Detection (RAID-2003), pages 220–237. Held on 8–10 September, 2003, Pittsburgh, PA, Springer-Verlag, Berlin, 2003.
[7] R. A. Maxion. Masquerade detection using enriched command lines. In International Conference on Dependable Systems and Networks (DSN-03), pages 5–14. Held on 22–25 June, 2003, San Francisco, CA, IEEE Computer Society Press, Los Alamitos, CA, 2003.
[8] R. A. Maxion and T. N. Townsend. Masquerade detection augmented with error analysis. IEEE Transactions on Reliability, Special Section on Quality/Reliability of Engineering of Information Systems, 53(1):124–147, March 2004.
[9] R. Pang and V. Paxson. A high-level programming environment for packet trace anonymization and transformation. In Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM’03), pages 339–351. Held on 25–29 August, 2003, Karlsruhe, Germany, ACM Press, 2003.
[10] M. R. Randazzo, M. Keeney, E. Kowalski, D. Cappelli, and A. Moore. Insider threat study: Illicit cyber activity in the banking and finance sector. Technical report, U.S. Secret Service and CERT Coordination Center/SEI, 2004.
[11] M. Schonlau, W. DuMouchel, W.-H. Ju, A. F. Karr, M. Theus, and Y. Vardi. Computer intrusion: Detecting masquerades. Statistical Science, 16(1):58–74, 2001.
[12] S. E. Smaha. Haystack: An intrusion detection system. In Proceedings of the Fourth Aerospace Computer Security Applications Conference, pages 37–44. Held on 12–16 December, 1988, Orlando, FL, IEEE Press, 1989.
[13] L. Sweeney. k-anonymity: A model for protecting privacy. International Journal on Uncertainty, Fuzziness and Knowledge-based Systems, 10(5):557–570, 2002.