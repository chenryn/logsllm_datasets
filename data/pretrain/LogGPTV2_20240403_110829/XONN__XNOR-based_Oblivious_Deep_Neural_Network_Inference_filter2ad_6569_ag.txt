framework for machine learning. In ACM CCS, 2018.
[46] Sameer Wagh, Divya Gupta, and Nishanth Chandran. Se-
cureNN: Efﬁcient and private neural network training, 2018.
[47] Xiaoqian Jiang, Miran Kim, Kristin Lauter, and Yongsoo
Song. Secure outsourced matrix computation and application
to neural networks. In ACM CCS, 2018.
[48] Amartya Sanyal, Matt Kusner, Adria Gascon, and Varun
Kanade. TAPAS: Tricks to accelerate (encrypted) prediction
as a service. In International Conference on Machine Learn-
ing, pages 4497–4506, 2018.
[49] Aner Ben-Efraim, Yehuda Lindell, and Eran Omri. Optimiz-
ing semi-honest secure multiparty computation for the inter-
net.
In Proceedings of the 2016 ACM SIGSAC Conference
on Computer and Communications Security, pages 578–590.
ACM, 2016.
[50] Zvika Brakerski. Fully homomorphic encryption without
In Advances in
modulus switching from classical gapsvp.
cryptology–crypto 2012, pages 868–886. Springer, 2012.
[51] Junfeng Fan and Frederik Vercauteren. Somewhat practi-
cal fully homomorphic encryption. IACR Cryptology ePrint
Archive, 2012:144, 2012.
[52] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur,
Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gor-
don Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasude-
van, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang
Zheng.
Tensorﬂow: A system for large-scale machine
learning. In Operating Systems Design and Implementation
(OSDI), 2016.
[53] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550,
2014.
[54] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.
USENIX Association
28th USENIX Security Symposium    1515
[55] Andre Esteva, Alexandre Robicquet, Bharath Ramsundar,
Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire
Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. A guide
to deep learning in healthcare. Nature medicine, 25(1):24,
2019.
[56] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko,
Susan M Swetter, Helen M Blau, and Sebastian Thrun.
Dermatologist-level classiﬁcation of skin cancer with deep
neural networks. Nature, 542(7639):115, 2017.
[57] Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and
Brendan J Frey. Predicting the sequence speciﬁcities of dna-
and rna-binding proteins by deep learning. Nature biotech-
nology, 33(8):831, 2015.
[58] Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nis-
san Hajaj, Michaela Hardt, Peter J Liu, Xiaobing Liu, Jake
Marcus, Mimi Sun, et al. Scalable and accurate deep learning
with electronic health records. npj Digital Medicine, 1(1):18,
2018.
[59] Breast Cancer Wisconsin,
accessed
on
01/20/2019.
https://www.kaggle.com/uciml/
breast-cancer-wisconsin-data.
[60] Pima
Indians Diabetes,
accessed
on
01/20/2019.
https://www.kaggle.com/uciml/
pima-indians-diabetes-database.
[61] Indian
Liver
Patient
01/20/2019.
indian-liver-patient-records.
on
https://www.kaggle.com/uciml/
Records,
accessed
A.2 Accuracy, Runtime, and Communication
Runtime and communication reports are available in Ta-
ble 11 and Table 12 for MNIST and CIFAR-10 benchmarks,
respectively. The corresponding neural network architec-
tures are provided in Table 13. Entries corresponding to a
communication of more than 40GB are estimated using nu-
merical runtime models.
Table 11: Accuracy (Acc.), communication (Comm.), and
latency (Lat.) for MNIST dataset. Channel/neuron trimming
is not applied.
Arch.
BM1
BM2
BM3
s
1
1.5
2
3
4
1
1.50
2
3
4
1
1.5
2
3
4
Acc. (%) Comm. (MB) Lat. (s)
97.10
97.56
97.82
98.10
98.34
97.25
97.93
98.28
98.56
98.64
98.54
98.93
99.13
99.26
99.35
2.57
4.09
5.87
10.22
15.62
2.90
5.55
10.09
21.90
38.30
17.59
36.72
62.77
135.88
236.78
0.12
0.13
0.13
0.14
0.15
0.10
0.12
0.14
0.18
0.23
0.17
0.22
0.3
0.52
0.81
[62] Malaria
Cell
Images,
accessed
on
01/20/2019.
https://www.kaggle.com/iarunava/
cell-images-for-detecting-malaria.
Table 12: Accuracy (Acc.), communication (Comm.), and
latency (Lat.) for CIFAR-10 dataset. Channel/neuron trim-
ming is not applied.
A Experimental Details
A.1 Network Trimming Examples
Table 9 and 10 summarize the trimming steps for the MNIST
and CIFAR-10 benchmarks, respectively.
Table 9: Trimming MNIST architectures.
Network
Property
BM1
(s=1.75)
BM2
(s=4)
BM3
(s=2)
Acc. (%)
Comm. (MB)
Lat. (ms)
Acc. (%)
Comm. (MB)
Lat. (ms)
Acc. (%)
Comm. (MB)
Lat. (ms)
initial
97.63
4.95
158
98.64
38.28
158
99.22
56.08
190
Trimming Step
step 1
step 2
97.28
97.59
3.81
4.29
114
131
98.37
98.44
24.33
28.63
144
134
98.96
99.11
37.34
42.51
165
157
step 3
97.02
3.32
102
98.13
15.76
104
99.00
32.13
146
Change
-0.61%
1.49× less
1.54× faster
-0.51%
2.42× less
1.51× faster
-0.22%
1.75× less
1.3× faster
Table 10: Trimming the BC2 network for CIFAR-10.
Property
Acc. (%)
Com. (GB)
Lat. (s)
initial
82.40
3.38
7.59
Trimming Step
step 2
step 1
82.39
82.41
2.76
3.05
6.87
6.23
step 3
81.85
2.60
5.79
Change
-0.55%
1.30× less
1.31× faster
Arch.
BC1
BC2
BC3
BC4
BC5
BC6
s
1
1.5
2
3
1
1.5
2
3
1
1.5
2
3
1
1.5
2
3
1
1.5
2
3
1
1.5
2
3
Acc. (%) Comm. (MB) Lat. (s)
0.72
0.77
0.80
0.83
0.67
0.73
0.78
0.82
0.77
0.81
0.83
0.86
0.82
0.85
0.87
0.88
0.81
0.85
0.86
0.88
0.67
0.74
0.78
0.80
1.26
2.82
4.98
11.15
0.39
0.86
1.53
3.40
1.35
3.00
5.32
11.89
4.66
10.41
18.45
41.37
5.54
12.40
21.98
49.30
0.65
1.46
2.58
5.77
3.96
8.59
15.07
33.49
1.37
2.78
4.75
10.35
4.23
9.17
16.09
35.77
14.12
31.33
55.38
123.94
16.78