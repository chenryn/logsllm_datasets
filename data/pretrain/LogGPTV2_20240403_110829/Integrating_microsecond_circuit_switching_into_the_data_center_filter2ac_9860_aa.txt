title:Integrating microsecond circuit switching into the data center
author:George Porter and
Richard D. Strong and
Nathan Farrington and
Alex Forencich and
Pang-Chen Sun and
Tajana Rosing and
Yeshaiahu Fainman and
George Papen and
Amin Vahdat
Integrating Microsecond Circuit Switching
into the Data Center
George Porter Richard Strong Nathan Farrington Alex Forencich Pang Chen-Sun
Tajana Rosing Yeshaiahu Fainman George Papen Amin Vahdat†
UC San Diego
†
UC San Diego and Google, Inc.
ABSTRACT
Recent proposals have employed optical circuit switching (OCS)
to reduce the cost of data center networks. However, the relatively
slow switching times (10–100 ms) assumed by these approaches,
and the accompanying latencies of their control planes, has limited
its use to only the largest data center networks with highly aggre-
gated and constrained workloads. As faster switch technologies
become available, designing a control plane capable of supporting
them becomes a key challenge.
In this paper, we design and implement an OCS prototype capa-
ble of switching in 11.5 µs, and we use this prototype to expose a
set of challenges that arise when supporting switching at microsec-
ond time scales. In response, we propose a microsecond-latency
control plane based on a circuit scheduling approach we call Traf-
ﬁc Matrix Scheduling (TMS) that proactively communicates circuit
assignments to communicating entities so that circuit bandwidth
can be used efﬁciently.
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network Archi-
tecture and Design—circuit-switching networks, packet-switching
networks, network topology
General Terms
Design, Experimentation, Measurement, Performance
Keywords
Data Center Networks; Optical Networks
1.
INTRODUCTION
As the size and complexity of data center deployments grow,
meeting their requisite bisection bandwidth needs is a challenge.
Servers with 10 Gbps link rates are common today, and 40 Gbps
NICs are already commercially available. At large scale, this trend
translates into signiﬁcant bisection bandwidth requirements. For
a large data center with numerous, rapidly changing applications,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
Copyright 2013 ACM 978-1-4503-2056-6/13/08 ...$15.00.
supporting high bisection bandwidth is important, since ultimately
application performance, and hence overall server utilization, may
suffer if insufﬁcient bandwidth is available. The result is that net-
work complexity and expense are increasing.
To meet the required bandwidth demands, data center opera-
tors have adopted multi-layer network topologies [14] (e.g., folded
Clos, or “FatTrees” [1, 16]), shown in Figure 1(a). While these
topologies scale to very high port counts, they are also a signiﬁcant
source of cost, due in part to the large amount of switches, optical
transceivers, ﬁbers, and power each of their layers requires. Re-
cent efforts have proposed [6, 8, 25] using optical circuit switches
(OCS) to deliver reconﬁgurable bandwidth throughout the network,
reducing some of the expense of multi-layer scale-out networks,
shown in Figure 1(b). A key challenge to adopting these proposals
has been their slow reconﬁguration time, driven largely by exist-
ing 3D-MEMS technology limitations. Two components dominate
this reconﬁguration time: (1) the hardware switching time of the
3D-MEMS OCS (10–100 ms), and (2) the software/control plane
overhead required to measure the communication patterns and cal-
culate a new schedule (100ms to 1s). As a result, the control plane
is limited to supporting only highly aggregated trafﬁc at the core
of the network [8], or applications constrained to have high trafﬁc
stability [25].
As optical switches become faster, deploying them more widely
in data center networks (e.g., to interconnect top-of-rack (ToR)
switches) requires a correspondingly faster control plane capable
of efﬁciently utilizing short-lived circuits. The contribution of this
paper is such a control plane. To gain experience with fast OCS
switching, we start by designing and building a simple 24-port OCS
prototype called Mordia,1 which has a switch time of 11.5 µs. Mor-
dia is built entirely with commercially available components, most
notably 2D-based MEMS wavelength-selective switches (WSS).
We use this prototype as a stand-in for future low-latency OCS de-
vices.
Using the Mordia prototype as a starting point, we then identify a
set of challenges involved in adopting existing OCS control planes
to microsecond-latency switching. In response to these challenges,
we propose a new circuit scheduling approach called Trafﬁc Ma-
trix Scheduling (TMS). Instead of measuring long-lived, prevailing
conditions and conﬁguring the topology in response, TMS instead
leverages application information and short-term demand estimates
to compute short-term circuit schedules. TMS chooses these sched-
ules to rapidly multiplex circuits across a set of end points, making
use of the fast circuit switch time to reduce buffering and network
delay. To obtain high circuit utilization, the computed schedules
1Microsecond Optical Research Data Center Interconnect Archi-
tecture
447(a) A FatTree network topology
(b) A Hybrid network topology
Figure 1: A comparison of a scale-out, multi-layered FatTree network and a Hybrid electrical/optical network design. In the FatTree
topology (a) each layer of switching incurs additional cost in electronics, core transceivers, ﬁber cabling, and power. In contrast, the
Hybrid topology (b) requires only a single “layer” assuming that the OCS reconﬁguration speed is sufﬁciently fast.
Speed Radix Depth
# Nodes
(x1000)
10G
40G
48
96
16
24
5
3
7
9
5
7
498
28
33
524
16
560
# Core Ports
(x1000)
3,484
83
360
7,864
109
6,159
The complexity of sample multi-layer,
fully-
Table 1:
provisioned,
Small-radix
switches and link redundancy require more layers, and thus
more switches and optical transceivers, driving up their cost.
scale-out network topologies.
are communicated to ToRs connected to Mordia, which adjust the
transmission of packets into the network to coincide with the sched-
uled switch reconﬁgurations, with full knowledge of when band-
width will be most available to a particular destination. In this way,
both short and long ﬂows can be ofﬂoaded into the OCS.
As a result, TMS can achieve 65% of the bandwidth of an identi-
cal link rate electronic packet switch (EPS) with circuits as short as
61µs duration, and 95% of EPS performance with 300-µs circuits
using commodity hardware. Taken together, our work suggests
that continuing to push down the reconﬁguration time of optical
switches and reducing the software and control overheads holds
the potential to radically lower the cost for delivering high bisec-
tion bandwidth in the data center.
2. MOTIVATION: REDUCING NETWORK
COST VIA FASTER SWITCHING
We now examine some of the sources of data center costs, and
motivate the need for low-latency circuit switching.
2.1 Multi-layer switching networks
ers adds substantial cost in terms of the switch hardware, optical
transceivers, ﬁbers, and power.
Consider an N-level scale-out FatTree data center network sup-
porting M servers partitioned into racks (e.g., 20 to 40 servers per
rack). Such a network built from k-radix switches can support
kN /2N−1 servers, with each layer of switching requiring kN−1/2N−2
switches (though layer N itself requires half this amount). The
choice of the number of layers in the network is determined by the
number of hosts and the radix k of each switch. Given a particular
data center, it is straightforward to determine the number of layers
needed to interconnect each of the servers.
There are two trends that impact the cost of the network by in-
creasing the number of necessary layers of switching: fault toler-
ance and high link rates. We consider each in turn.
Fault tolerance: While a FatTree network can survive link fail-
ures by relying on its multi-path topology, doing so incurs a network-
wide reconvergence. This can be highly disruptive at large scale,
and so redundant links are used to survive such failures [24]. Dual
link redundancy, for instance, effectively cuts the radix of the switch
in half since each logical link now requires two switch ports.
High link rates: For relatively mature link technologies like 10
Gbps Ethernet, high-radix switches are widely available commer-
cially: 10 Gbps switches with 64 or even 96 ports are becoming
commodity. In contrast, newer generations of switches based on
40 Gbps have much lower radices, for example 16 to 24 ports per
switch. Hence, as data center operators build out new networks
based on increasingly faster link rates, it will not always be possi-
ble to use high radix switches as the fundamental building block.
These constraints necessitate additional switching layers and, thus,
additional cost and complexity. Table 1 shows the number of core
network ports (ports used to connect one layer of switching to an
adjacent layer) for a set of data center sizes and switch radices.
Note that since this table shows fully-provisioned topologies, it
serves as an upper bound to what might be built in practice since
the network might be only partially provisioned depending on the
number of nodes that need to be supported.
2.2 OCS model
Multi-layer switching topologies like FatTrees are highly scal-
able and ﬂexible — any node can communicate with any other
node on demand. However, they must be provisioned for worst-
case communication patterns, which can require as many as ﬁve
to nine layers in the largest networks, with each subsequent layer
less utilized than the next in the common case. Each of these lay-
We now describe a simple model of an OCS suitable for inter-
connecting ToR switches. This model is similar to that assumed by
previous hybrid network designs [6, 8, 25], but with a key differ-
ence: orders of magnitude faster switching speed.
The model consists of an N-port optical circuit switch with a re-
conﬁguration latency of O(10) µs. Each input port can be mapped
S0,0S0,1S0,2S0,3S0,k...S1,0S1,1S1,2S1,3S1,k...S2,0S2,1S2,2S2,3S2,k...SN,0SN,1SN,k/2...= Core transceiver= Edge transceiverHiHiHiHiHiN-LayersS0,0S0,1S0,2S0,3S0,k...= Edge transceiverHiHiHiHiHiOCSkxkEPS448to any output port, and these mappings can be changed arbitrarily
(with the constraint that only one input port can map to any given
output port). The OCS does not buffer packets, and indeed does
not interpret the bits in packets at all — the mapping of input ports
to output ports is entirely controlled by an external scheduler. This
scheduler is responsible for determining the time-varying mapping
of input ports to output ports and programming the switch accord-
ingly.
We assume that ToRs attached to the OCS support per-destination
ﬂow control, meaning that packets for destination D are only trans-
mitted to an OCS input port when a circuit is setup between that
input port and D. Packets to destinations other than D are queued
in the edge ToR during this time. Furthermore, during the OCS re-
conﬁguration period, all packets are queued in the ToR. Since the
OCS cannot buffer packets, the ToR must be synchronized to only
transmit packets at the appropriate times. This queueing can lead
to signiﬁcant delay, especially for small ﬂows that are particularly
sensitive to the observed round-trip time. In these cases, packets
can be sent to a packet switch in the spirit of other hybrid network
proposals. In this work, we focus on the OCS and its control plane
in isolation, concentrating particularly on reducing end-to-end re-
conﬁguration latency. In this way, our work is complementary to
other work in designing hybrid networks.
3. MICROSECOND SCHEDULING
A key challenge in supporting microsecond-latency OCS switches
is effectively making use of short-lived circuits. In [7], we proposed
an approach for scheduling circuits, called Trafﬁc Matrix Schedul-
ing (TMS). In this section, we expand on that initial idea, and then