# Reading the Tea Leaves: A Comparative Analysis of Threat Intelligence

**Authors:**
- Vector Guo Li
- Matthew Dunn
- Paul Pearce
- Damon McCoy
- Geoffrey M. Voelker
- Stefan Savage
- Kirill Levchenko

**Affiliations:**
- University of California, San Diego (Vector Guo Li, Geoffrey M. Voelker, Stefan Savage)
- Northeastern University (Matthew Dunn)
- Georgia Institute of Technology (Paul Pearce)
- New York University (Damon McCoy)
- University of Illinois Urbana-Champaign (Kirill Levchenko)

## Abstract

The term "threat intelligence" has rapidly become a buzzword in the computer security industry. The premise is that by compiling up-to-date information about known threats (e.g., IP addresses, domain names, file hashes), organizations can better defend their systems against future attacks. Consequently, a wide array of public and commercial sources now distribute threat intelligence data feeds. However, our understanding of this data, its characteristics, and its effectiveness remains limited. This paper addresses these gaps by defining a set of metrics for characterizing threat intelligence data feeds and using these measures to systematically evaluate a broad range of public and commercial sources. We also use external measurements to qualitatively investigate issues of coverage and accuracy. Our results suggest significant limitations and challenges in using existing threat intelligence data for its intended purposes.

## Introduction

Computer security is an inherently adversarial field where each side seeks to exploit the other's vulnerabilities. Attackers leverage knowledge of system weaknesses to penetrate targeted systems, while defenders aim to improve their defenses by understanding contemporary threats and the technical fingerprints left by attackers. This dynamic drives continuous innovation and diversification by attackers, requiring defenders to monitor and update their security practices accordingly. Common to all such efforts is the need to gather and analyze new data on attacker behavior to update defenses and security practices. The ability to effectively gather and analyze such data defines a de facto window of vulnerability, during which an organization may be less effective in addressing attacks due to a lack of current threat information.

This challenge has led to a demand for contemporary threat data sources, collectively referred to as threat intelligence (TI). The most common form of TI are indicators of compromise (IOCs): observable behaviors that signal a potential compromise. These include network indicators (e.g., IP addresses) and file hashes (e.g., indicating malware). While organizations collect some TI data internally, third-party firms and public groups often provide more comprehensive and curated data. The promise of operational value from TI has created a multi-billion-dollar market, with established security companies and specialized vendors offering TI products.

Despite the promise, there has been little empirical assessment of TI data or a consensus on how to evaluate it. This gap motivates our work to provide a grounded, empirical basis for comparing TI sources. Specifically, this paper makes the following contributions:
- We introduce a set of basic threat intelligence metrics and describe a methodology for measuring them.
- We analyze 47 distinct IP address TI sources and 8 distinct malware file hash TI sources, reporting their metrics.
- We demonstrate techniques to evaluate the accuracy and coverage of certain categories of TI sources.
- We conduct analyses over two different time periods, two years apart, and show strong consistency in the findings.

Our analysis reveals that while a few TI data sources show significant overlap, most do not. This result supports the hypothesis that different monitoring infrastructures capture different types of attacks. We also highlight broader limitations in terms of coverage (most indicators are unique) and accuracy (false positives limit operational use). Finally, we present a longitudinal analysis suggesting that these findings are consistent over time.

## Overview

### Data Collection and Sources

The threat intelligence data for our study was obtained from numerous public and private sources, ranging from simple blacklists to rich, well-structured threat intelligence exchanges. Each item (e.g., IP address or file hash) is referred to as an indicator. In this section, we enumerate our threat intelligence sources, describe their structure, and define our measurement metrics.

#### Data Set and Collection

We used several sources of TI data for our analysis:
- **Facebook ThreatExchange (FB)**: A closed-community platform for sharing labeled threat data. We collected all broadly shared data.
- **Paid Feed Aggregator (PA)**: A commercial platform aggregating data from over a hundred sources, both public and private.
- **Paid IP Reputation Service**: Provides hourly-updated blacklists of known bad IP addresses.
- **Public Blacklists and Reputation Feeds**: Indicators from public sources like AlienVault, Badips, Abuse.ch, and Packetmail.

We focused on sources providing IP addresses and file hashes, as they are the most prevalent. Data was collected hourly, but irregularities in membership and contributions, as well as varying reliability, led to collection gaps. We used the time window from December 1, 2017, to July 20, 2018, for most analyses, as this period had the largest number of active sources. After eliminating duplicates and sub-sources, we were left with 47 IP feeds and 8 malware file hash feeds.

#### Data Source Structure

TI sources structure and present data differently. A major difference is between snapshot and event feeds:
- **Snapshot Feeds**: Provide periodic snapshots of a set of indicators. For example, a list of command-and-control IP addresses updated periodically.
- **Event Feeds**: Report newly discovered indicators. Subscribers query for new indicators within a recent time window.

To make a fair comparison, we normalized event feeds to have a 30-day valid period, converting them into snapshot feeds for unified evaluation.

### Threat Intelligence Metrics

We propose six concrete metrics to compare threat intelligence sources:
- **Volume**: The total number of indicators in a feed over the measurement interval.
- **Differential Contribution**: The number of indicators in one feed that are not in another, relative to the size of the first feed.
- **Exclusive Contribution**: The proportion of indicators unique to a feed.
- **Latency**: The elapsed time between an indicator's first appearance in any feed and its appearance in the feed in question.
- **Accuracy**: The proportion of correctly included indicators in a feed.
- **Coverage**: The proportion of intended indicators contained in a feed.

These metrics help consumers understand the value and fitness of a feed for a particular purpose.

## IP Threat Intelligence

IP address TI feeds are one of the most common forms of threat intelligence. We apply the above metrics to quantify the differences between 47 different IP address TI feeds.

### Feed Categorization

To meaningfully compare feeds, we grouped them into semantic categories based on metadata and feed descriptions. The categories analyzed are:
- **Scan**: Hosts performing port or vulnerability scans.
- **Brute-force**: Hosts making brute force login attempts.
- **Malware**: Malware command-and-control and distribution servers.
- **Exploit**: Hosts attempting to exploit vulnerabilities.
- **Botnet**: Compromised hosts in a botnet.
- **Spam**: Hosts sending spam or unauthorized email.

Table 1 lists the feeds, grouped by category, used in our analysis.

In the following sections, we use these metrics to evaluate the IP address and file hash TI feeds, providing a comprehensive and empirical basis for comparing and understanding the effectiveness of different threat intelligence sources.