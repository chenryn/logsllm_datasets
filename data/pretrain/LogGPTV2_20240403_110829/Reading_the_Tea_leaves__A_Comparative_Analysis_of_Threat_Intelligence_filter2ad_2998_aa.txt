title:Reading the Tea leaves: A Comparative Analysis of Threat Intelligence
author:Vector Guo Li and
Matthew Dunn and
Paul Pearce and
Damon McCoy and
Geoffrey M. Voelker and
Stefan Savage
Reading the Tea Leaves: A Comparative Analysis of Threat Intelligence
Vector Guo Li1, Matthew Dunn2, Paul Pearce4, Damon McCoy3,
Geoffrey M. Voelker1, Stefan Savage1, Kirill Levchenko5
1 University of California, San Diego
4 Georgia Institute of Technology
2 Northeastern University
3 New York University
5 University of Illinois Urbana-Champaign
Abstract
The term “threat intelligence” has swiftly become a staple
buzzword in the computer security industry. The entirely rea-
sonable premise is that, by compiling up-to-date information
about known threats (i.e., IP addresses, domain names, ﬁle
hashes, etc.), recipients of such information may be able to
better defend their systems from future attacks. Thus, today a
wide array of public and commercial sources distribute threat
intelligence data feeds to support this purpose. However, our
understanding of this data, its characterization and the extent
to which it can meaningfully support its intended uses, is still
quite limited. In this paper, we address these gaps by formally
deﬁning a set of metrics for characterizing threat intelligence
data feeds and using these measures to systematically charac-
terize a broad range of public and commercial sources. Fur-
ther, we ground our quantitative assessments using external
measurements to qualitatively investigate issues of coverage
and accuracy. Unfortunately, our measurement results suggest
that there are signiﬁcant limitations and challenges in using
existing threat intelligence data for its purported goals.
Introduction
1
Computer security is an inherently adversarial discipline in
which each “side” seeks to exploit the assumptions and limita-
tions of the other. Attackers rely on exploiting knowledge of
vulnerabilities, conﬁguration errors or operational lapses in or-
der to penetrate targeted systems, while defenders in turn seek
to improve their resistance to such attacks by better under-
standing the nature of contemporary threats and the technical
ﬁngerprints left by attacker’s craft. Invariably, this means that
attackers are driven to innovate and diversify while defenders,
in response, must continually monitor for such changes and
update their operational security practices accordingly. This
dynamic is present in virtually every aspect of the operational
security landscape, from anti-virus signatures to the conﬁgu-
ration of ﬁrewalls and intrusion detection systems to incident
response and triage. Common to all such reiﬁcations, however,
is the process of monitoring for new data on attacker behavior
and using that data to update defenses and security practices.
Indeed, the extent to which a defender is able to gather and
analyze such data effectively deﬁnes a de facto window of
vulnerability—the time during which an organization is less
effective in addressing attacks due to ignorance of current
attacker behaviors.
This abstract problem has given rise to a concrete demand
for contemporary threat data sources that are frequently col-
lectively referred to as threat intelligence (TI). By far the most
common form of such data are so-called indicators of compro-
mise: simple observable behaviors that signal that a host or
network may be compromised. These include both network in-
dicators such as IP addresses (e.g., addresses known to launch
particular attacks or host command-and-control sites, etc.) and
ﬁle hashes (e.g., indicating a ﬁle or executable known to be
associated with a particular variety of malware). The presence
of such indicators is a symptom that alerts an organization
to a problem, and part of an organization’s defenses might
reasonably include monitoring its assets for such indicators
to detect and mitigate potential compromises as they occur.
While each organization naturally collects a certain amount
of threat intelligence data on its own (e.g., the attacks they
repel, the e-mail spam they ﬁlter, etc.) any single entity has a
limited footprint and few are instrumented to carefully segre-
gate crisp signals of attacks from the range of ambiguity found
in normal production network and system logs. Thus, it is now
commonly accepted that threat intelligence data procurement
is a specialized activity whereby third-party ﬁrms, and/or
collections of public groups, employ a range of monitoring
techniques to aggregate, ﬁlter and curate quality information
about current threats. Indeed, the promised operational value
of threat intelligence has created a thriving (multi-billion dol-
lar) market [43]. Established security companies with roots in
anti-virus software or network intrusion detection now offer
threat intelligence for sale, while some vendors specialize in
threat intelligence exclusively, often promising coverage of
more sophisticated threats than conventional sources.
Unfortunately, in spite of this tremendous promise, there
has been little empirical assessment of threat intelligence data
or even a consensus about what such an evaluation would
entail. Thus, consumers of TI products have limited means to
compare offerings or to factor the cost of such products into
any model of the beneﬁt to operational security that might be
offered.
This issue motivates our work to provide a grounded, em-
pirical footing for addressing such questions. In particular,
this paper makes the following contributions:
O We introduce a set of basic threat intelligence metrics
and describe a methodology for measuring them, notably:
Volume, Differential Contribution, Exclusive Contri-
bution, Latency, Coverage and Accuracy.
O We analyze 47 distinct IP address TI sources covering
six categories of threats and 8 distinct malware ﬁle hash
TI sources, and report their metrics.
O We demonstrate techniques to evaluate the accuracy and
coverage of certain categories of TI sources.
O We conduct the analyses in two different time periods
two years apart, and demonstrate the strong consistency
between the ﬁndings.
From our analysis, we ﬁnd that while a few TI data sources
show signiﬁcant overlap, most do not. This result is consistent
with the hypothesis advanced by [42] that different kinds of
monitoring infrastructure will capture different kinds of at-
tacks, but we have demonstrated it in a much broader context.
We also reveal that underlying this issue are broader limita-
tions of TI sources in terms of coverage (most indicators are
unique) and accuracy (false positives may limit how such data
can be used operationally). Finally, we present a longitudinal
analysis suggesting that these ﬁndings are consistent over
time.
2 Overview
The threat intelligence data collected for our study was ob-
tained by subscribing to and pulling from numerous public
and private intelligence sources. These sources ranged from
simple blacklists of bad IPs/domains and ﬁle hashes, to rich
threat intelligence exchanges with well labeled and structured
data. We call each item (e.g., IP address or ﬁle hash) an in-
dicator (after indicator of compromise, the industry term for
such data items).
In this section we enumerate our threat intelligence sources,
describe each source’s structure and how we collected it, and
then deﬁne our measurement metrics for empirically measur-
ing these sources. When the source of the data is public, or
when we have an explicit agreement to identify the provider,
we have done so. However, in other cases, the data was pro-
vided on the condition of anonymity and we restrict ourself to
describing the nature of the provider, but not their identity. All
of our private data providers were appraised of the nature of
our research, its goals and the methodology that we planned
to employ.
2.1 Data Set and Collection
We use several sources of TI data for our analysis:
Facebook ThreatExchange (FB) [17]. This is a closed-
community platform that allows hundreds of companies and
organizations to share and interact with various types of la-
beled threat data. As part of an agreement with Facebook, we
collected all its data that it shared broadly. In subsequent anal-
yses, sources with preﬁx “FB” indicate a unique contributor
on the Facebook ThreatExchange.
Paid Feed Aggregator (PA). This is a commercial paid threat
intelligence data aggregation platform. It contains data col-
lected from over a hundred other threat intelligence sources,
public or private, together with its own threat data. In sub-
sequent analyses all data sources with preﬁx “PA” are from
unique data sources originating from this aggregator.
Paid IP Reputation Service. This commercial service pro-
vides an hourly-updated blacklist of known bad IP addresses
across different attack categories.
Public Blacklists and Reputation Feeds. We collected in-
dicators from public blacklists and reputation data sources,
including well-known sources such as AlienVault [3],
Badips [5], Abuse.ch [1] and Packetmail [28].
Threat Intelligence indicators include different types of
data, such as IP address, malicious ﬁle hash, Domain, URL,
etc. In this paper, we focus our analysis on sources that pro-
vide IP addresses and ﬁle hashes, as they are the most preva-
lent data types in our collection.
We collect data from all sources on an hourly basis. How-
ever, both the Facebook ThreatExchange and the Paid Feed
Aggregator change their members and contributions over time,
creating irregular collection periods for several of the sub-data
sources. Similarly, public threat feeds had varying degrees
of reliability, resulting in collection gaps. In this paper, we
use the time window from December 1, 2017 to July 20,
2018 for most of the analyses, as we have the largest number
of active sources during this period. We eliminated dupli-
cates sources (e.g., sources we collected individually and also
found in the Paid Aggregator) and sub-sources (a source that
is a branch of another source). We further break IP sources
into separate categories and treat them as individual feeds, as
shown in Section 3. This ﬁltering leaves us with 47 IP feeds
and 8 malware ﬁle hash feeds.
The ways each TI source collects data varies, and in some
cases the methodology is unknown. For example, Packetmail
IPs and Paid IP Reputation collect threat data themselves via
honeypots, analyzing malware, etc. Other sources, such as
Badips or the Facebook ThreatExchange, collect their indica-
tors from general users or organizations—e.g., entities may be
attacked and submit the indicators to these threat intelligence
services. These services then aggregate the data and report
it to their subscribers. Through this level of aggregation the
precise collection methodologies and data providence can be
lost.
2.2 Data Source Structure
TI sources in our corpus structure and present data in different
ways. Part of the challenge in producing cross-dataset metrics
is normalizing both the structure of the data as well as its
meaning. A major structural difference that inﬂuences our
analysis occurs between data sources that provide data in
snapshots and data sources that provide events.
Snapshot. Snapshot feeds provide periodic snapshots of a set
of indicators. More formally, a snapshot is a set of indicators
that is a function of time. It deﬁnes, for a given point in time,
the set of indicators that are members of the data source.
Snapshot feeds imply state: at any given time, there is a set
of indicators that are in the feed. A typical snapshot source is
a published list of IPs periodically updated by its maintainer.
For example, a list of command-and-control IP addresses for a
botnet may be published as a snapshot feed subject to periodic
updates.
All feeds of ﬁle hashes are snapshots and are monotonic in
the sense that indicators are only added, not removed, from
the feed. Hashes are a proxy for the ﬁle content, which does
not change (malicious ﬁle content will not change to benign
in the future).
Event. In contrast, event feeds report newly discovered indi-
cators. More formally, an event source is a set of indicators
that is a function of a time interval. For a given time inter-
val, the source provides a set of indicators that were seen or
discovered in that time interval. Subscribers of these feeds
query data by asking for new indicators added in a recent time
window. For example, a user might, once a day, request the
set of indicators that appeared in the last 24 hours.
This structural difference is a major challenge when evaluat-
ing feeds comparatively. We need to normalize the difference
to make a fair comparison, especially for IP feeds. From a TI
consumer’s perspective, an event feed does not indicate when
an indicator will expire, so it is up to the consumer to act on
the age of indicators. Put another way, the expiration dates of
indicators are decided by how users query the feed: if a user
asks for the indicators seen in the last 30 days when quering
data, then there is an implicit 30-day valid time window for
these indicators.
In this paper, we choose a 30-day valid period for all the
indicators we collected from event feeds—the same valid
period used in several snapshot feeds, and also a common
query window option offered by event feeds. We then convert
these event feeds into snapshot feeds and evaluate all of them
in a uniﬁed fashion.
2.3 Threat Intelligence Metrics
The aim of this work is to develop threat intelligence met-
rics that allow a TI consumer to compare threat intelligence
sources and reason about their ﬁtness for a particular purpose.
To this end, we propose six concrete metrics: Volume, Differ-
ential contribution, Exclusive contribution, Latency, Accuracy
and Coverage.
" Volume. We deﬁne the volume of a feed to be the total
number of indicators appearing in a feed over the measure-
ment interval. Volume is the simplest TI metric and has an
established history in prior work [21,23,24,30,35,36,42]. It is
also useful to study the daily rate of a feed, which quantiﬁes
the amount of data appearing in a feed on a daily basis.
Rationale: To a ﬁrst approximation, volume captures how
much information a feed provides to the consumer. For a feed
without false positives (see accuracy below), and if every in-
dicator has equal value to the consumer, we would prefer a
feed of greater volume to a feed of lesser volume. Of course,
indicators do not all have the same value to consumers: know-
ing the IP address of a host probing the entire Internet for
decades-old vulnerabilities is less useful than the address of
a scanner targeting organizations in your sector looking to
exploit zero-day vulnerabilities.
" Differential contribution. The differential contribution
of one feed with respect to another is the number of in-
dicators in the ﬁrst that are not in the second during the
same measurement period. We deﬁne differential contribu-
tion relative to the size of the ﬁrst feed, so that the dif-
ferential contribution of feed A with respect to feed B is
DiffA,B = |A\ B|/|A|. Thus, DiffA,B = 1 indicates that the two
feeds have no elements in common, and DiffA,B = 0 indicates
that every indicator in A also appears in B. It is sometimes
useful to consider the complement of differential contribu-
tion, namely the normalized intersection of A in B, given by
IntA,B = |A∩ B|/|A| = 1− DiffA,B.
Rationale: For a consumer, it is often useful to know how
many additional indicators a feed offers relative to one or
more feeds that the consumer has already. Thus, if a con-
sumer already has feed A and is considering paying for feed
B, then DiffA,B indicates how many new indicators feed A will
provide.
" Exclusive contribution. The exclusive contribution of a
feed with respect to a set of other feeds is the proportion
of indicators unique to a feed, that is, the proportion of in-
dicators that occur in the feed but no others. Formally, the
exclusive contribution of feed A is deﬁned as UniqA,B =
B(cid:54)=A B|/|A|. Thus, UniqA,B = 0 means that every ele-
ment of feed A appears in some other feeds, while UniqA,B = 1
means no element of A appears in any other feed.
|A \(cid:83)
Rationale: Like differential contribution, exclusive contri-
bution tells a TI consumer how much of a feed is different.
However, exclusive contribution compares a feed to all other
feeds available for comparison, while differential contribution
compares a feed to just another feed. From a TI consumer’s
perspective, exclusive contribution is a general measure of a
feed’s unique value.
" Latency. For an indicator that occurs in two or more feeds,
its latency in a feed is the elapsed time between its ﬁrst appear-
ance in any feed and its appearance in the feed in question. In
the feed where an indicator ﬁrst appeared, its latency is zero.
For all other feeds, the latency indicates how much later the
same indicators appears in those feeds. Taster’s Choice [30]
referred to latency as relative ﬁrst appearance time. (We ﬁnd
the term latency to be more succinct without loss of clarity.)
Since latency is deﬁned for one indicator, for a feed it makes
sense to consider statistics of the distribution of indicator
latencies, such as the median indicator latency.
Rationale: Latency characterizes how quickly a feed in-
cludes new threats: the sooner a feed includes a threat, the
more effective it is at helping consumers protect their systems.
Indeed, several studies report on the impact of feed latency
on its effectiveness at thwarting spam [10, 32].
The metrics above are deﬁned without regard for the mean-
ing of the indicators in a feed. We can calculate the volume
of a single feed or the differential contribution of one feed
with respect to another regardless of what the feed purports
to contain. While these metrics are easy to compute, they
do little to tell us about the ﬁtness of a feed for a particular
purpose. For this, we need to consider the meaning or purpose
of the feed data, as advertised by the feed provider. We deﬁne
the following two metrics.
" Accuracy. The accuracy of a feed is the proportion of in-
dicators in a feed that are correctly included in the feed. Feed
accuracy is analogous to precision in Information Retrieval.
This metric presumes that the description of the feed is well-
deﬁned and describes a set of elements that should be in the
feed given perfect knowledge. In practice, we have neither
perfect knowledge nor a perfect description of what a feed
should contain. In some cases, however, we can construct a
set A− of elements that should deﬁnitely not be in a feed A.
Then AccA ≤ |A\ A−|/|A|.
Rationale: The accuracy metric tells a TI consumer how
many false positives to expect when using a feed, and, there-
fore, dictates how a feed can be used. For example, if a con-
sumer automatically blocks all trafﬁc to IP addresses appear-
ing in a feed, then false positives may cause disruption in an
enterprise by blocking trafﬁc to legitimate sites. On the other
hand, consumers may tolerate some false positives if a feed is
only used to gain additional insight during an investigation.
" Coverage. The coverage of a feed is the proportion of
the intended indicators contained in a feed. Feed coverage is
analogous to recall in Information Retrieval. Like accuracy,
coverage presumes that the description of the feed is sufﬁcient
to determine which elements should be in a feed, given perfect
knowledge. In some cases, it is possible to construct a set A+
of elements that should be in a feed. We can then upper-bound
the coverage CovA ≤ |A|/|A+|.
Rationale: For a feed consumer who aims to obtain com-
plete protection from a speciﬁc kind of threat, coverage is
a measure of how much protection a feed will provide. For
example, an organization that wants to protect itself from a
particular botnet will want to maximize its coverage of that
botnet’s command-and-control servers or infection vectors.
In the following two sections, we use these metrics to eval-
uate two types of TI: IP address feeds and ﬁle hash feeds.
IP Threat Intelligence
3
One of the most common forms of TI are feeds of IP addresses
considered malicious, suspicious, or otherwise untrustworthy.
This type of threat intelligence dates back at least to the early
spam and intrusion detection blacklists, many of which are
still active today such as SpamhausSBL [40], CBL [8] and
SORBS [39]. Here, we apply the metrics described above to
quantify the differences between 47 different IP address TI
feeds.
3.1 Feed Categorization
IP address TI feeds have different meanings, and, therefore,
purposes. To meaningfully compare feeds to each other, we
ﬁrst group feeds into categories of feeds whose indicators
have the same intended meaning. Unfortunately, there is no
standard or widely accepted taxonomy of IP TI feeds. To
group feeds into semantic categories, we use metadata associ-
ated with the feed as well as descriptions of the feed provided
by the producer, as described below.
Metadata. Some feeds provide category information with
each indicator as metadata. More speciﬁcally, all of the Paid
Aggregator feeds, Alienvault IP Reputation and Paid IP Rep-
utation include this category metadata. In this case, we use its
pre-assigned category in the feed. Facebook ThreatExchange
feeds do not include category information in the metadata,
but instead provide a descriptive phrase with each indicator.
We then derive its category based on the description.
Feed description. For feeds without metadata, we rely on
online descriptions of each feed, where available, to deter-
mine its semantic category. For example, the website of feed
Nothink SSH [27] describes that the feed reports brute-force
login attempts on its corresponding honeypot, which indicates
the feed belongs to brute-force category.
We grouped our IP feeds into categories derived from the
information above. In this work, we analyze six of the most
prominent categories:
◦ Scan: Hosts doing port or vulnerability scans.
◦ Brute-force: Hosts making brute force login attempts.
◦ Malware: Malware C&C and distribution servers.
◦ Exploit: Hosts trying to remotely exploit vulnerabilities.
◦ Botnet: Compromised hosts belonging to a botnet.
◦ Spam: Hosts that sent spam or should not originate email.
Table 1 lists the feeds, grouped by category, used in the rest
of this section. The symbols(cid:35) and (cid:52) before the feed name