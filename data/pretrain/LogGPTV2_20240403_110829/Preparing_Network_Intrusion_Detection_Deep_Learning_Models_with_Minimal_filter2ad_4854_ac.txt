Normal
Generic
Exploits
Fuzzers
DoS
Reconnaissance
Analysis
Backdoor
Shellcode
Worms
53.45 %
36.45 %
9.25 %
0.78 %
0.04 %
31.94 %
22.81 %
19.04 %
10.37 %
6.99 %
5.98 %
1.14 %
1.0 %
0.65 %
0.07 %
67343
45927
11656
995
52
56000
40000
33393
18184
12264
10491
2000
1746
1133
130
Table 2: Distribution of attack categories (UNSW-NB15 train-
ing dataset).
for several attack scenarios. The partitioned UNSW-NB15 dataset
contains 175,341 records in the training set and 82,332 records in
the testing set. It contains 42 features categorized as ① flow features
(e.g. protocol, source/destination port), ② basic features (e.g. state,
duration, service), ③ content features (e.g. source/destination TCP
sequence number), ④ time features (e.g. source/destination jitter,
record start/end time), and ⑤ additional generated features (e.g.
number of flows that have GET and POST methods in the HTTP
service, number of connections that have the same destination ad-
dress). The UNSW-NB15 dataset categorizes records into normal
and 9 attack categories (see Table 2). The features of UNSW-NB15
and NSL-KDD are described in the appendix Table 4 and Table 5,
respectively.
5.2 Description of experiments
We divide our experiments into two categories based on whether
the feature spaces of the source and target datasets are similar or
different.
• Same feature space: This category represents the cases
where the source NID dataset has the same feature space
as the target NID dataset. This is typical when the target
dataset is created by collecting packet data from networks
deploying the same protocols and using the same processing
tools as the source dataset, maybe at a later period in time.
An example of this scenario is where an organization creates
a source dataset that has data samples belonging to 10 attack
categories possible on its network and trains a classification
model on it, but at a later time the organization discovers
2 new categories of attacks. The organization then collects
a small amount of data from those 2 new attack categories
Session 3: Network Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan133and wants to create a new NID classification model that can
identify all 12 attack categories.
For the experiments in this category, we split the UNSW-
NB15 dataset into two parts: a source dataset with samples
from 8 attack categories except samples of the attack cate-
gory exploits and a target dataset with samples of the attack
category exploits. We use our adversarial DA technique to
train a NN classification model using these source and target
datasets and evaluate its performance in detecting attacks of
the type exploits belonging to the target dataset, as well as
the attacks from the 8 attack categories present in the source
dataset. We do this for different target data sizes to show the
impact of data size on the accuracy of the target model. We
repeat the experiments for attack categories: reconnaissance
and shellcode to demonstrate the generality of our approach.
We performed these experiments for all 9 attack categories
in the UNSW-NB15 dataset and observed similar trends.
• Different feature space: This category represents the cases
where the source NID dataset has a different feature space
than the target NID dataset. This scenario is more common
when the source and the target datasets are either created for
different network types using different network protocols or
the tools used to capture and process the network traffic for
monitoring have evolved allowing the capture of much more
meaningful features, thus changing the feature space. The
attack distributions might also completely change with time
due to the older attacks becoming irrelevant due to being
patched. Also attackers identifying new attack vectors might
give rise to completely new attack families. An example of
this scenario is an organization that already has a source
dataset of data samples belonging to several attack categories
collected for their internal employee WiFi network and train
a NID classification model on this dataset. They now want
to create a NID classification model that can identify attacks
on their experimental IoT network. So they collect a small
amount of samples of attacks for the IoT network and create
a target dataset. This target dataset will have a different
feature space than the source dataset because the protocols
for IoT devices are different. We use our adversarial DA
technique to train on the source and the target dataset to
create a NID classification model that can identify attacks in
the target IoT networks. For experiments in this category, we
use NSL-KDD as the source dataset and UNSW-NB15 as the
target dataset. We use our adversarial DA technique to train
a NN classification model and evaluate its performance when
different sizes of target datasets are used to demonstrate the
impact of data size on the performance of the model.
5.3 Libraries and hyper-parameters
We use the open source DL library TensorFlow [2] with the Keras [7]
wrapper to train our GAN and other classification models for com-
parison. We use a standard 75-25 % split for training-testing sets
for our experiments. We use 60 epochs for training the base model
with a batch size of 32. For our DA GAN, we train for 10000 iter-
ations with a batch size of 32. We use the adaptive learning rate
optimization algorithm Adam [25], with a starting learning rate of
0.001, β1 = 0.9 and β2 = 0.999 for training our NNs and the GAN.
For describing the performance of the trained classification model,
we report the accuracy and F-score metrics.
6 RESULTS AND ANALYSIS
We compare our adversarial DA approach (described in Section 4)
with: ❶ the base case, that is, a DL model trained only on the
target dataset; and ❷ a transfer learning approach that uses fine-
tuning [43]. The results are divided into two categories based on
whether the source and target datasets have the same feature space
or different feature space. For all three approaches in our evalua-
tion, we use the same classification model architecture. The model
consists of a total of 9 layers with 3 sets of fully connected layers,
a batch normalization layer, and a reLU activation layer stacked
on top of each other. The fully connected layers have 64, 32, 16
neurons in that order. The output layer of the model is a layer with
2 neurons and a soft-max activation function to give the probability
predictions of the data sample belonging to the attack or benign
category.
6.1 Same feature space
This category represents the scenario where the source and target
datasets have the same feature space.
Datasets: For this evaluation, we split UNSW-NB15 [34] into two
parts. One dataset with the data samples from 8 attack categories
except exploits (serves as the source dataset) and one with only
samples of the attack category exploits (serves as the target dataset).
Both the source and the target datasets contain samples of the
category benign. We repeat similar experiments for other two attack
categories: reconnaissance and shellcode. We model this as a binary
classification problem, i.e. predicting whether the current record
belongs to the attack or benign category, thus not predicting the
specific attack category. In case of the attack category exploits, the
source training dataset has 83,961 labeled samples. Similarly, in
case of attack category reconnaissance and shellcode the source
datasets have 112,387 and 129,531 training samples, respectively.
All these experiments are performed for target datasets containing
100, 200, 500, 1000, 5000 and 10000 samples, except in the case of
attack category shellcode where we just go upto 2000 samples as the
UNSW-NB15 dataset contains a maximum of 2600 training samples
for the shellcode category.
Experiments: We use our adversarial DA approach by training the
GAN mentioned in Section 4.2 using the source and target datasets
and then use the generator of the GAN as a classification model for
testing. For our approach, the pre-processing techniques transform
the source and datasets into datasets having 30 features each before
using adversarial DA.
Performance results on the target dataset: Fig. 6 reports the
accuracy and f-scores for this scenario for all three attack categories:
exploits, reconnaissance, and shellcode. The x-axis represents the
number of target dataset samples used for training and the y-axis
represents the accuracy and f-scores when the classification models
are tested on the target testing dataset. For the attack category
exploits, we observe that our adversarial DA approach outperforms
the base case and the fine-tuning approach for 100 target dataset
samples by 7.66% and 6.73% respectively in terms of accuracy and
Session 3: Network Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan134Exploits
Reconnaissance
Shellcode
1
0.95
0.9
0.85
y
c
a
r
u
c
c
A
100
10000
Base case
Fine tuning
Our approach
5000
1000
200
Number of Target Samples
500
Base case
Fine tuning
Our approach
5000
1000
200
Number of Target Samples
500
1
0.95
0.9
0.85
0.8
y
c
a
r
u
c
c
A
Base case
Fine tuning
Our approach
100
2000
1000
200
Number of Target Samples
500
100
Exploits
Reconnaissance
Shellcode
1
0.95
e
r
o
c
s
-
F
0.9
0.85
0.8
100
10000
Base case
Fine tuning
Our approach
5000
1000
200
Number of Target Samples
500
Base case
Fine tuning
Our approach
5000
1000
200
Number of Target Samples
500
1
0.95
0.9
0.85
0.8
e
r
o
c
s
-
F
Base case
Fine tuning
Our approach
100
2000
200
1000
Number of Target Samples
500
100
0.95
0.9
0.85
y
c
a
r
u
c
c
A
e
r
o
c
s
-
F
0.95
0.9
0.85
0.8
10000
10000
Figure 6: Accuracy and F-score results on the target dataset for the scenario where source and target datasets have same feature
spaces comparing three approaches: base model trained with only the target dataset samples; model trained on the source
dataset and fine-tuned on the target dataset; model trained using our adversarial DA approach.
Exploits
Accuracy on source dataset
Reconnaissance
Shellcode
Fine-tuning Our approach
Fine-tuning Our approach
Fine-tuning Our approach
Samples in target dataset
used for training.
100
200
500
1000
5000
10000
0.8395
0.851
0.7477
0.6077
0.7453
0.7777
0.9134