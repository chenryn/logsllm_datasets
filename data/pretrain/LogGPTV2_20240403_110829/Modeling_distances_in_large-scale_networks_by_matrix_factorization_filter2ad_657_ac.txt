(Dout
i (cid:0) ~U (cid:1) ~Yi)2
(Din
i (cid:0) ~Xi (cid:1) ~U )2
(11)
(12)
~Xnew = arg min
~U2Rd
~Ynew = arg min
~U2Rd
m
Xi=1
Xi=1
The global minima of these error functions, computed by
simple matrix operations, have the closed form:
~Xnew = (DoutY )(Y T Y )(cid:0)1
~Ynew = (DinX)(X T X)(cid:0)1
(13)
(14)
Eqs. (13{14) assume that the optimizations are uncon-
strained. Alternatively, one can impose nonnegativity con-
straints on ~Xnew and ~Ynew; this will guarantee that the pre-
dicted distances are themselves nonnegative (assuming that
the landmark distance matrix was also modeled by NMF).
The least squared error problems in Eqs. (11{12) can be
solved with nonnegativity constraints, but the solution is
somewhat more complicated. Our simulation results did
r
o
r
r
e
e
v
i
t
a
e
r
n
a
d
e
m
i
l
0.5
0.4
0.3
0.2
0.1
0
0
Lipschitz+PCA
SVD
NMF
Lipschitz+PCA
SVD
NMF
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
r
o
r
r
e
e
v
i
t
l
a
e
r
i
n
a
d
e
m
20
40
dimension
60
80
0
0
20
40
60
dimension
80
100
(a) Comparison over NLANR data set
(b) Comparison over P2PSim data set
Figure 3: Reconstruction error comparison of SVD, NMF and Lipschitz over NLANR and P2PSim data set
not reveal any signi(cid:12)cant di(cid:11)erence between the prediction
accuracies of least squares solutions with and without non-
negativity constraints; thus, in what follows, we focus on the
simpler unconstrained solutions in Eqs. (13{14).
H(cid:13)1(cid:13)
L(cid:13)1(cid:13)
0.5(cid:13)
1(cid:13)
1(cid:13)
L(cid:13)2(cid:13)
L(cid:13)3(cid:13)
1(cid:13)
1(cid:13)
L(cid:13)4(cid:13)
0.5(cid:13)
H(cid:13)2(cid:13)
Figure 4: Four landmark nodes L1 (cid:0) L4 and two or-
dinary hosts H1, H2 interconnected by a simple net-
work topology
We give a simple example of this procedure in Figure 4.
The network is an enlarged version of the network in Fig-
ure 1, with the four original nodes serving as landmarks
and two new nodes introduced as ordinary hosts. The (cid:12)rst
step is to measure inter-landmark distances and calculate
landmark incoming and outgoing vectors. We used SVD to
factor the landmark distance matrix in this example. The
result is the same as the example in section 4:
X =2
664
(cid:0)1
0
1
(cid:0)1 (cid:0)1
0
1
(cid:0)1
0
0 (cid:0)1
(cid:0)1
3
775
3
775
; Y =2
664
(cid:0)1
0 (cid:0)1
0
(cid:0)1
1
(cid:0)1 (cid:0)1
0
1
0
(cid:0)1
Note that SVD can be substituted by NMF and the following
steps are identical.
Second, we measure the distance vectors for the ordinary
hosts: Dout = Din = [0:5 1:5 1:5 2:5] for ordinary host H1.
According to Eqs. (13 { 14), ~XH1 = [(cid:0)1:5 0 1], ~YH1 =
[(cid:0)1:5 0 (cid:0) 1]. Similarly, we obtain the distance vector of H2
as [2:5 1:5 1:5 0:5], and calculate its outgoing and incoming
vectors: ~XH2 = [(cid:0)1:5 0 (cid:0) 1], ~YH2 = [(cid:0)1:5 0 1]. One can
verify that distances between ordinary hosts and landmarks
are exactly preserved. The distance between two ordinary
hosts is not measured, but can be estimated as ~XH1 (cid:1) ~YH2 =
~XH2 (cid:1) ~YH1 = 3:25, while the real network distance is 3.
5.2 Optimization
The basic architecture requires an ordinary host to mea-
sure network distances to all landmarks, which limits the
scalability of IDES. Furthermore, if some of the landmark
nodes experience transient failures or a network partition,
an ordinary host may not be able to retrieve the measure-
ments it needs to solve Eqs. (13{14).
To improve the scalability and robustness of IDES, we
propose a relaxation to the basic architecture: an ordinary
host Hnew only has to measure distances to a set of k nodes
with pre-computed outgoing and incoming vectors. The k
nodes can be landmark nodes, or other ordinary hosts that
have already computed their vectors. Suppose the outgoing
vectors of those k nodes are ~X1; ~X2; (cid:1) (cid:1) (cid:1) ; ~Xk and the incom-
ing vectors are ~Y1; ~Y2; (cid:1) (cid:1) (cid:1) ; ~Yk. We measure Dout
i as
the distance from and to the ith node, for all i = 1; (cid:1) (cid:1) (cid:1) ; k.
Calculating the new vectors ~Xnew and ~Ynew for Hnew is done
by solving the least squares problems:
and Din
i
~Xnew = arg min
~U2Rd
~Ynew = arg min
~U2Rd
k
k
Xi=1
Xi=1
(Dout
i (cid:0) ~U (cid:1) ~Yi)2
(Din
i (cid:0) ~Xi (cid:1) ~U )2
(15)
(16)
The solution is exactly the same form as described in
Eq. (13) and Eq. (14). The constraint k (cid:21) d is necessary
(and usually su(cid:14)cient) to ensure that the problem is not
singular. In general, larger values of k lead to better pre-
diction results, as they incorporate more measurements of
network distances involving Hnew into the calculation of the
vectors ~Xnew and ~Ynew.
We use the topology in Figure 4 again to demonstrate
how the system works. As in the basic architecture, the
(cid:12)rst step is to measure inter-landmark distances and cal-
culate landmark outgoing and incoming vectors. Secondly,
the ordinary host H1 measures the distances to L1, L2 and
L3 as [0.5 1.5 1.5]. By Eq. (13) and Eq. (14), the vectors
are ~XH1 =[-1.5 0 1], ~YH1 =[-1.5 0 -1]. Note that we did not
measure the distance between H1 and L4, but it can be esti-
mated as ~XH1 (cid:1) ~YL4 =[-1.5 0 1](cid:1)[-1 0 1]= 2:5, which is in fact
the true distance. Finally, the ordinary host H2 measures
the distances to L2, L4 and H1 as [1.5 0.5 3]. Because all of
them already have pre-computed vectors, H2 can compute
its own vectors by Eq. (13) and Eq. (14). The results are
~XH2 =[-1.4 0.1 -0.9], ~YH2 =[-1.4 -0.1 0.9]. The distances
between ordinary host H2 and L1/L3 are not measured di-
rectly, but can be estimated as ~XH2 (cid:1) ~YL1 =[-1.4 0.1 -0.9](cid:1)[-1
0 -1]= 2:3 and ~XH2 (cid:1) ~YL3 =[-1.4 0.1 -0.9](cid:1)[-1 -1 0]= 1:3.
1.5/1.5(cid:13)
H(cid:13)1(cid:13)
L(cid:13)1(cid:13)
0.5/0.5(cid:13)
1.5/1.5(cid:13)
2.5/2.5(cid:13)
L(cid:13)4(cid:13)
L(cid:13)2(cid:13)
L(cid:13)3(cid:13)
L(cid:13)2(cid:13)
2.5/2.3(cid:13)
1.5/1.5(cid:13)
H(cid:13)1(cid:13)
L(cid:13)1(cid:13)
L(cid:13)4(cid:13)
H(cid:13)2(cid:13)
0.5/0.5(cid:13)
1.5/1.3(cid:13)
3/3(cid:13)
L(cid:13)3(cid:13)
Figure 5: Learning outgoing and incoming vectors
for two ordinary hosts. Solid lines indicate that real
network measurement is conducted. Each edge is
annotated with (real network distance / estimated
distance).
This example illustrates that even without measurement
to all landmarks, the estimated distances can still be accu-
rate.
In this example, most of the pairwise distances are
exactly preserved; the maximum relative error is 15% when
predicting the distance between H2 and L2. In the example,
the load is well distributed among landmarks. As shown in
Figure 5, distances to L2 are only measured twice during
this estimation procedure. Such a scheme allows IDES to
scale to a large number of ordinary hosts and landmarks. It
is also robust against partial landmark failures.
6. EVALUATION
implementation was obtained from the o(cid:14)cial GNP soft-
ware release written in C. We implemented IDES and ICS
in MatLab 6.0.
We identify four evaluation criteria:
(cid:15) E(cid:14)ciency
We measure e(cid:14)ciency by the total running time re-
quired by a system to build its model of network dis-
tances between all landmark nodes and ordinary hosts.
(cid:15) Accuracy
The prediction error between Dij and ^Dij should be
small. We use the modi(cid:12)ed relative error function
in Eq. (10) to evaluate accuracy, which is also used
in GNP and Vivaldi. Note that predicted distances
are computed between ordinary hosts that have not
conducted any network measurements of their dis-
tance. Predicted distance errors are di(cid:11)erent than
reconstructed distance errors (where actual network
measurements are conducted).
(cid:15) Scalability
The storage requirements are O(d) for models based
on network embeddings (with one position vector for
each host) and matrix factorizations (with one incom-
ing and outgoing vector for each host). In large-scale
networks, the number of hosts N is very large. The
condition d (cid:28) N allows the model to scale, assum-
ing that reasonable accuracy of predicted distances is
maintained. Also, to support multiple hosts concur-
rently, it is desirable to distribute the load|for in-
stance, by only requiring distance measurements to
partial sets of landmarks.
(cid:15) Robustness
A robust system should be resilient against host fail-
ures and temporary network partitioning. In particu-
lar, partial failure of landmark nodes should not pre-
vent the system from building models of network dis-
tances.
6.1 EfÔ¨Åciency and accuracy
We use three data sets for evaluating accuracy and e(cid:14)-
ciency.
(cid:15) GNP: 15 out of 19 nodes in the symmetric data set
were selected as landmarks. The rest of the 4 nodes
and the 869 nodes in the AGNP data set were selected
as ordinary hosts. Prediction accuracy was evaluated
on 869 (cid:2) 4 pairs of hosts.
(cid:15) NLANR: 20 out of 110 nodes were selected randomly
as landmarks. The remaining 90 nodes were treated as
ordinary hosts. The prediction accuracy was evaluated
on 90 (cid:2) 90 pairs of hosts.
(cid:15) P2PSim: 20 out of 1143 nodes were selected randomly
as landmarks. The remaining 1123 nodes were treated
as ordinary hosts. The prediction accuracy was evalu-
ated on 1123 (cid:2) 1123 pairs of hosts.
In this section we evaluate IDES, using SVD and NMF al-
gorithms to learn models of network distances, and compare
them to the GNP [13] and ICS [12] systems.
The experiments were performed on a Dell Dimension
4600 with Pentium 4 3.2GHz CPU, 2GB RAM. The GNP
Although deliberate placement of landmarks may yield more
accurate results, we chose the landmarks randomly since in
general they may be placed anywhere on the Internet. A
previous study also shows that random landmark selection is
fairly e(cid:11)ective if more than 20 landmarks are employed [21].
data set
IDES/SVD IDES/NMF
GNP
NLANR
P2PSim
0.10s
0.01s
0.16s
0.12s
0.02s
0.17s
ICS
0.02s
0.01s
0.03s
GNP
1min 19s
4min 44s
2min 30s
Table 1: E(cid:14)ciency comparison on IDES, ICS and
GNP over four data sets
To ensure fair comparisons, we used the same set of land-
marks for all four algorithms. We also repeated the simu-
lation several times, and no signi(cid:12)cant di(cid:11)erences in results
were observed from one run to the next.
Table 1 illustrates the running time comparison between
IDES, ICS and GNP. GNP is much more ine(cid:14)cient than the
IDES and ICS. This is because GNP uses Simplex Downhill
method, which converges slowly to local minima. Both IDES
and ICS have running time less than 1 second, even when the
data sets contain thousands of nodes. It is possible to reduce
the running time of GNP by sacrifying the accuracy, but the
parameters are hard to tune, which is another drawback of
Simplex Downhill method.
Figure 6 plots the CDF of prediction errors for IDES using
SVD, IDES using NMF, ICS and GNP over the three data
sets respectively. In Figure 6(a), the GNP system is the most
accurate system for the GNP data set. IDES using SVD and
NMF are as accurate as GNP for 70% of the predicted dis-