code to the STATUS_IO_REPARSE_TAG_NOT_HANDLED error and exits.
The CreateProcess API now knows that the process creation has failed due
to an invalid reparse point, so it loads and calls into the
ApiSetHost.AppExecutionAlias.dll library, which contains code that parses
modern applications’ reparse points.
The library’s code parses the reparse point, grabs the packaged application
activation data, and calls into the AppInfo service with the goal of correctly
stamping the token with the needed security attributes. AppInfo verifies that
the user has the correct license for running the packaged application and
checks the integrity of its files (through the State Repository). The actual
process creation is done by the calling process. The CreateProcess API
detects the reparse error and restarts its execution starting with the correct
package executable path (usually located in C:\Program
Files\WindowsApps\). This time, it correctly creates the process and the
AppContainer token or, in case of Centennial, initializes the virtualization
layer (actually, in this case, another RPC into AppInfo is used again).
Furthermore, it creates the HAM host and its activity, which are needed for
the application. The activation at this point is complete.
EXPERIMENT: Reading the AppExecution alias data
In this experiment, you extract AppExecution alias data from the 0-
bytes executable file. You can use the FsReparser utility (found in
this book’s downloadable resources) to parse both the reparse
points or the extended attributes of the NTFS file system. Just run
the tool in a command prompt window and specify the READ
command-line parameter:
Click here to view code image
C:\Users\Andrea\AppData\Local\Microsoft\WindowsApps>fsrepars
er read MicrosoftEdge.exe
File System Reparse Point / Extended Attributes Parser 0.1
Copyright 2018 by Andrea Allievi (AaLl86)
Reading UWP attributes...
Source file: MicrosoftEdge.exe.
The source file does not contain any Extended Attributes.
The file contains a valid UWP Reparse point (version 3).
Package family name: Microsoft.MicrosoftEdge_8wekyb3d8bbwe
Application User Model Id: 
Microsoft.MicrosoftEdge_8wekyb3d8bbwe!MicrosoftEdge
UWP App Target full path: 
C:\Windows\System32\SystemUWPLauncher.exe
Alias Type: UWP Single Instance
As you can see from the output of the tool, the CreateProcess
API can extract all the information that it needs to properly execute
a modern application’s activation. This explains why you can
launch Edge from the command line.
Package registration
When a user wants to install a modern application, usually she opens the
AppStore, looks for the application, and clicks the Get button. This action
starts the download of an archive that contains a bunch of files: the package
manifest file, the application digital signature, and the block map, which
represent the chain of trust of the certificates included in the digital signature.
The archive is initially stored in the
C:\Windows\SoftwareDistribution\Download folder. The AppStore process
(WinStore.App.exe) communicates with the Windows Update service
(wuaueng.dll), which manages the download requests.
The downloaded files are manifests that contain the list of all the modern
application’s files, the application dependencies, the license data, and the
steps needed to correctly register the package. The Windows Update service
recognizes that the download request is for a modern application, verifies the
calling process token (which should be an AppContainer), and, using services
provided by the AppXDeploymentClient.dll library, verifies that the package
is not already installed in the system. It then creates an AppX Deployment
request and, through RPC, sends it to the AppX Deployment Server. The
latter runs as a PPL service in a shared service host process (which hosts
even the Client License Service, running as the same protected level). The
Deployment Request is placed into a queue, which is managed
asynchronously. When the AppX Deployment Server sees the request, it
dequeues it and spawns a thread that starts the actual modern application
deployment process.
 Note
Starting with Windows 8.1, the UWP deployment stack supports the
concept of bundles. Bundles are packages that contain multiple resources,
like different languages or features that have been designed only for
certain regions. The deployment stack implements an applicability logic
that can download only the needed part of the compressed bundle after
checking the user profile and system settings.
A modern application deployment process involves a complex sequence of
events. We summarize here the entire deployment process in three main
phases.
Phase 1: Package staging
After Windows Update has downloaded the application manifest, the AppX
Deployment Server verifies that all the package dependencies are satisfied,
checks the application prerequisites, like the target supported device family
(Phone, Desktop, Xbox, and so on) and checks whether the file system of the
target volume is supported. All the prerequisites that the application needs are
expressed in the manifest file with each dependency. If all the checks pass,
the staging procedure creates the package root directory (usually in
C:\Program Files\WindowsApps\) and its subfolders.
Furthermore, it protects the package folders, applying proper ACLs on all of
them. If the modern application is a Centennial type, it loads the daxexec.dll
library and creates VFS reparse points needed by the Windows Container
Isolation minifilter driver (see the “Centennial applications” section earlier in
this chapter) with the goal of virtualizing the application data folder properly.
It finally saves the package root path into the
HKLM\SOFTWARE\Classes\LocalSettings\Software\Microsoft\Windows\
CurrentVersion\AppModel\PackageRepository\Packages\
 registry key, in the Path registry value.
The staging procedure then preallocates the application’s files on disk,
calculates the final download size, and extracts the server URL that contains
all the package files (compressed in an AppX file). It finally downloads the
final AppX from the remote servers, again using the Windows Update
service.
Phase 2: User data staging
This phase is executed only if the user is updating the application. This phase
simply restores the user data of the previous package and stores them in the
new application path.
Phase 3: Package registration
The most important phase of the deployment is the package registration. This
complex phase uses services provided by
AppXDeploymentExtensions.onecore.dll library (and
AppXDeploymentExtensions .desktop.dll for desktop-specific deployment
parts). We refer to it as Package Core Installation. At this stage, the AppX
Deployment Server needs mainly to update the State Repository. It creates
new entries for the package, for the one or more applications that compose
the package, the new tiles, package capabilities, application license, and so
on. To do this, the AppX Deployment server uses database transactions,
which it finally commits only if no previous errors occurred (otherwise, they
will be discarded). When all the database transactions that compose a State
Repository deployment operation are committed, the State Repository can
call the registered listeners, with the goal of notifying each client that has
requested a notification. (See the “State Repository” section in this chapter
for more information about the change and event tracking feature of the State
Repository.)
The last steps for the package registration include creating the Dependency
Mini Repository file and updating the machine registry to reflect the new
data stored in the State Repository. This terminates the deployment process.
The new application is now ready to be activated and run.
 Note
For readability reasons, the deployment process has been significantly
simplified. For example, in the described staging phase, we have omitted
some initial subphases, like the Indexing phase, which parses the AppX
manifest file; the Dependency Manager phase, used to create a work plan
and analyze the package dependencies; and the Package In Use phase,
which has the goal of communicating with PLM to verify that the package
is not already installed and in use.
Furthermore, if an operation fails, the deployment stack must be able to
revert all the changes. The other revert phases have not been described in
the previous section.
Conclusion
In this chapter, we have examined the key base system mechanisms on which
the Windows executive is built. In the next chapter, we introduce the
virtualization technologies that Windows supports with the goal of improving
the overall system security, providing a fast execution environment for virtual
machines, isolated containers, and secure enclaves.
CHAPTER 9
Virtualization technologies
One of the most important technologies used for running multiple operating
systems on the same physical machine is virtualization. At the time of this
writing, there are multiple types of virtualization technologies available from
different hardware manufacturers, which have evolved over the years.
Virtualization technologies are not only used for running multiple operating
systems on a physical machine, but they have also become the basics for
important security features like the Virtual Secure Mode (VSM) and
Hypervisor-Enforced Code Integrity (HVCI), which can’t be run without a
hypervisor.
In this chapter, we give an overview of the Windows virtualization
solution, called Hyper-V. Hyper-V is composed of the hypervisor, which is
the component that manages the platform-dependent virtualization hardware,
and the virtualization stack. We describe the internal architecture of Hyper-V
and provide a brief description of its components (memory manager, virtual
processors, intercepts, scheduler, and so on). The virtualization stack is built
on the top of the hypervisor and provides different services to the root and
guest partitions. We describe all the components of the virtualization stack
(VM Worker process, virtual machine management service, VID driver,
VMBus, and so on) and the different hardware emulation that is supported.
In the last part of the chapter, we describe some technologies based on the
virtualization, such as VSM and HVCI. We present all the secure services
that those technologies provide to the system.
The Windows hypervisor
The Hyper-V hypervisor (also known as Windows hypervisor) is a type-1
(native or bare-metal) hypervisor: a mini operating system that runs directly
on the host’s hardware to manage a single root and one or more guest
operating systems. Unlike type-2 (or hosted) hypervisors, which run on the
base of a conventional OS like normal applications, the Windows hypervisor
abstracts the root OS, which knows about the existence of the hypervisor and
communicates with it to allow the execution of one or more guest virtual
machines. Because the hypervisor is part of the operating system, managing
the guests inside it, as well as interacting with them, is fully integrated in the
operating system through standard management mechanisms such as WMI
and services. In this case, the root OS contains some enlightenments.
Enlightenments are special optimizations in the kernel and possibly device
drivers that detect that the code is being run virtualized under a hypervisor, so
they perform certain tasks differently, or more efficiently, considering this
environment.
Figure 9-1 shows the basic architecture of the Windows virtualization
stack, which is described in detail later in this chapter.
Figure 9-1 The Hyper-V architectural stack (hypervisor and virtualization
stack).
At the bottom of the architecture is the hypervisor, which is launched very
early during the system boot and provides its services for the virtualization
stack to use (through the use of the hypercall interface). The early
initialization of the hypervisor is described in Chapter 12, “Startup and
shutdown.” The hypervisor startup is initiated by the Windows Loader,
which determines whether to start the hypervisor and the Secure Kernel; if
the hypervisor and Secure Kernel are started, the hypervisor uses the services
of the Hvloader.dll to detect the correct hardware platform and load and start
the proper version of the hypervisor. Because Intel and AMD (and ARM64)
processors have differing implementations of hardware-assisted
virtualization, there are different hypervisors. The correct one is selected at
boot-up time after the processor has been queried through CPUID
instructions. On Intel systems, the Hvix64.exe binary is loaded; on AMD
systems, the Hvax64.exe image is used. As of the Windows 10 May 2019
Update (19H1), the ARM64 version of Windows supports its own
hypervisor, which is implemented in the Hvaa64.exe image.
At a high level, the hardware virtualization extension used by the
hypervisor is a thin layer that resides between the OS kernel and the
processor. This layer, which intercepts and emulates in a safe manner
sensitive operations executed by the OS, is run in a higher privilege level
than the OS kernel. (Intel calls this mode VMXROOT. Most books and
literature define the VMXROOT security domain as “Ring -1.”) When an
operation executed by the underlying OS is intercepted, the processor stops
to run the OS code and transfer the execution to the hypervisor at the higher
privilege level. This operation is commonly referred to as a VMEXIT event.
In the same way, when the hypervisor has finished processing the intercepted
operation, it needs a way to allow the physical CPU to restart the execution
of the OS code. New opcodes have been defined by the hardware
virtualization extension, which allow a VMENTER event to happen; the
CPU restarts the execution of the OS code at its original privilege level.
Partitions, processes, and threads
One of the key architectural components behind the Windows hypervisor is
the concept of a partition. A partition essentially represents the main isolation
unit, an instance of an operating system installation, which can refer either to
what’s traditionally called the host or the guest. Under the Windows
hypervisor model, these two terms are not used; instead, we talk of either a
root partition or a child partition, respectively. A partition is composed of
some physical memory and one or more virtual processors (VPs) with their
local virtual APICs and timers. (In the global term, a partition also includes a
virtual motherboard and multiple virtual peripherals. These are virtualization
stack concepts, which do not belong to the hypervisor.)
At a minimum, a Hyper-V system has a root partition—in which the main
operating system controlling the machine runs—the virtualization stack, and
its associated components. Each operating system running within the
virtualized environment represents a child partition, which might contain
certain additional tools that optimize access to the hardware or allow
management of the operating system. Partitions are organized in a
hierarchical way. The root partition has control of each child and receives
some notifications (intercepts) for certain kinds of events that happen in the
child. The majority of the physical hardware accesses that happen in the root
are passed through by the hypervisor; this means that the parent partition is
able to talk directly to the hardware (with some exceptions). As a
counterpart, child partitions are usually not able to communicate directly
with the physical machine’s hardware (again with some exceptions, which
are described later in this chapter in the section “The virtualization stack”).
Each I/O is intercepted by the hypervisor and redirected to the root if needed.
One of the main goals behind the design of the Windows hypervisor was
to have it be as small and modular as possible, much like a microkernel—no
need to support any hypervisor driver or provide a full, monolithic module.
This means that most of the virtualization work is actually done by a separate
virtualization stack (refer to Figure 9-1). The hypervisor uses the existing
Windows driver architecture and talks to actual Windows device drivers.
This architecture results in several components that provide and manage this
behavior, which are collectively called the virtualization stack. Although the
hypervisor is read from the boot disk and executed by the Windows Loader
before the root OS (and the parent partition) even exists, it is the parent
partition that is responsible for providing the entire virtualization stack.
Because these are Microsoft components, only a Windows machine can be a
root partition. The Windows OS in the root partition is responsible for
providing the device drivers for the hardware on the system, as well as for
running the virtualization stack. It’s also the management point for all the
child partitions. The main components that the root partition provides are
shown in Figure 9-2.
Figure 9-2 Components of the root partition.
Child partitions
A child partition is an instance of any operating system running parallel to the
parent partition. (Because you can save or pause the state of any child, it
might not necessarily be running.) Unlike the parent partition, which has full
access to the APIC, I/O ports, and its physical memory (but not access to the
hypervisor’s and Secure Kernel’s physical memory), child partitions are
limited for security and management reasons to their own view of address
space (the Guest Physical Address, or GPA, space, which is managed by the
hypervisor) and have no direct access to hardware (even though they may
have direct access to certain kinds of devices; see the “Virtualization stack”
section for further details). In terms of hypervisor access, a child partition is
also limited mainly to notifications and state changes. For example, a child
partition doesn’t have control over other partitions (and can’t create new
ones).
Child partitions have many fewer virtualization components than a parent
partition because they aren’t responsible for running the virtualization stack
—only for communicating with it. Also, these components can also be
considered optional because they enhance performance of the environment
but aren’t critical to its use. Figure 9-3 shows the components present in a
typical Windows child partition.
Figure 9-3 Components of a child partition.
Processes and threads
The Windows hypervisor represents a virtual machine with a partition data
structure. A partition, as described in the previous section, is composed of
some memory (guest physical memory) and one or more virtual processors
(VP). Internally in the hypervisor, each virtual processor is a schedulable
entity, and the hypervisor, like the standard NT kernel, includes a scheduler.
The scheduler dispatches the execution of virtual processors, which belong to
different partitions, to each physical CPU. (We discuss the multiple types of
hypervisor schedulers later in this chapter in the “Hyper-V schedulers”
section.) A hypervisor thread (TH_THREAD data structure) is the glue
between a virtual processor and its schedulable unit. Figure 9-4 shows the
data structure, which represents the current physical execution context. It
contains the thread execution stack, scheduling data, a pointer to the thread’s
virtual processor, the entry point of the thread dispatch loop (discussed later)
and, most important, a pointer to the hypervisor process that the thread
belongs to.
Figure 9-4 The hypervisor’s thread data structure.
The hypervisor builds a thread for each virtual processor it creates and
associates the newborn thread with the virtual processor data structure
(VM_VP).
A hypervisor process (TH_PROCESS data structure), shown in Figure 9-5,
represents a partition and is a container for its physical (and virtual) address
space. It includes the list of the threads (which are backed by virtual
processors), scheduling data (the physical CPUs affinity in which the process
is allowed to run), and a pointer to the partition basic memory data structures
(memory compartment, reserved pages, page directory root, and so on). A
process is usually created when the hypervisor builds the partition
(VM_PARTITION data structure), which will represent the new virtual
machine.
Figure 9-5 The hypervisor’s process data structure.
Enlightenments
Enlightenments are one of the key performance optimizations that Windows
virtualization takes advantage of. They are direct modifications to the
standard Windows kernel code that can detect that the operating system is
running in a child partition and perform work differently. Usually, these
optimizations are highly hardware-specific and result in a hypercall to notify
the hypervisor.
An example is notifying the hypervisor of a long busy–wait spin loop. The
hypervisor can keep some state on the spin wait and decide to schedule
another VP on the same physical processor until the wait can be satisfied.
Entering and exiting an interrupt state and access to the APIC can be
coordinated with the hypervisor, which can be enlightened to avoid trapping
the real access and then virtualizing it.
Another example has to do with memory management, specifically
translation lookaside buffer (TLB) flushing. (See Part 1, Chapter 5, “Memory
management,” for more information on these concepts.) Usually, the
operating system executes a CPU instruction to flush one or more stale TLB
entries, which affects only a single processor. In multiprocessor systems,
usually a TLB entry must be flushed from every active processor’s cache (the
system sends an inter-processor interrupt to every active processor to achieve
this goal). However, because a child partition could be sharing physical
CPUs with many other child partitions, and some of them could be executing
a different VM’s virtual processor at the time the TLB flush is initiated, such
an operation would also flush this information for those VMs. Furthermore, a
virtual processor would be rescheduled to execute only the TLB flushing IPI,
resulting in noticeable performance degradation. If Windows is running
under a hypervisor, it instead issues a hypercall to have the hypervisor flush
only the specific information belonging to the child partition.
Partition’s privileges, properties, and version
features
When a partition is initially created (usually by the VID driver), no virtual
processors (VPs) are associated with it. At that time, the VID driver is free to
add or remove some partition’s privileges. Indeed, when the partition is first
created, the hypervisor assigns some default privileges to it, depending on its
type.
A partition’s privilege describes which action—usually expressed through
hypercalls or synthetic MSRs (model specific registers)—the enlightened OS
running inside a partition is allowed to perform on behalf of the partition
itself. For example, the Access Root Scheduler privilege allows a child
partition to notify the root partition that an event has been signaled and a
guest’s VP can be rescheduled (this usually increases the priority of the
guest’s VP-backed thread). The Access VSM privilege instead allows the
partition to enable VTL 1 and access its properties and configuration (usually
exposed through synthetic registers). Table 9-1 lists all the privileges
assigned by default by the hypervisor.
Table 9-1 Partition’s privileges
PARTITIO
N TYPE
DEFAULT PRIVILEGES
Root and 