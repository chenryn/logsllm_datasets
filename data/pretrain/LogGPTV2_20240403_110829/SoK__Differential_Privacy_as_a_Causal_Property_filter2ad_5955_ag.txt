straint,” in 2016 ACM SIGSAC Conf. on Computer and Communications
Security, ser. CCS ’16, 2016, pp. 43–54.
[9] T. Dalenius, “Towards a methodology for statistical disclosure control,”
Statistik Tidskrift, vol. 15, pp. 429–444, 1977.
[10] Z. Ding, Y. Wang, G. Wang, D. Zhang, and D. Kifer, “Detecting
violations of differential privacy,” in 2018 ACM SIGSAC Conf. on
Computer and Communications Security (CCS ’18), 2018, pp. 475–489.
[11] C. Dwork, “Differential privacy,” in Automata, Languages and Program-
ming, 33rd Intl. Colloquium, ICALP 2006, Venice, Italy, July 10–14,
2006, Proceedings, Part II, ser. LICS, vol. 4052, 2006, pp. 1–12.
[12] C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. Roth,
“Generalization in adaptive data analysis and holdout reuse,” in 28th Intl.
Conf. on Neural Information Processing Systems - Volume 2 (NIPS’15),
2015, pp. 2350–2358.
[13] ——, “The reusable holdout: Preserving validity in adaptive data
analysis,” Science, vol. 349, no. 6248, pp. 636–638, 2015. [Online].
Available: http://science.sciencemag.org/content/349/6248/636
[14] C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. L.
Roth, “Preserving statistical validity in adaptive data analysis,” in Forty-
seventh Annual ACM Symp. on Theory of Computing (STOC ’15), 2015,
pp. 117–126.
[15] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness
through awareness,” in 3rd Innovations in Theoretical Computer Science
Conf., 2012, pp. 214–226.
[16] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor, “Our
data, ourselves: Privacy via distributed noise generation,” in 24th Annual
Intl. Conf. on The Theory and Applications of Cryptographic Techniques,
2006, pp. 486–503.
[17] C. Dwork, F. Mcsherry, K. Nissim, and A. Smith, “Calibrating noise to
sensitivity in private data analysis,” in Theory of Cryptography Conf.,
2006, pp. 265–284.
[18] C. Dwork and M. Naor, “On the difﬁculties of disclosure prevention in
statistical databases or the case for differential privacy,” J. Privacy and
Conﬁdentiality, vol. 2, no. 1, pp. 93–107, 2008.
[19] J. Gehrke, E. Lui, and R. Pass, “Towards privacy for social networks:
A zero-knowledge based deﬁnition of privacy,” in 8th Conf. on Theory
of Cryptography, 2011, pp. 432–449.
[20] A. Ghosh and R. Kleinberg, “Inferential privacy guarantees for dif-
ferentially private mechanisms,” CoRR, vol. abs/1603.01508v2, 2017,
presented at
the 8th Innovations in Theoretical Computer Science
conference in 2017.
[21] S. Goldwasser and S. Micali, “Probabilistic encryption & how to play
information,” in Fourteenth
mental poker keeping secret all partial
Annual ACM Symp. on Theory of Computing, ser. STOC ’82, 1982,
pp. 365–377.
[22] ——, “Probabilistic encryption,” J. Computer and System Sciences,
vol. 28, no. 2, pp. 270–299, 1984.
[23] X. He, A. Machanavajjhala, and B. Ding, “Blowﬁsh privacy: Tuning
privacy-utility trade-offs using policies,” in ACM SIGMOD Intl. Conf.
on Management of Data (SIGMOD 2014), 2014.
[44] C. E. Shannon, “Communication theory of secrecy systems,” Bell Labs
Technical Journal, vol. 28, no. 4, pp. 656–715, 1949.
[45] J. Tang, A. Korolova, X. Bai, X. Wang, and X. Wang, “Privacy loss in
Apple’s implementation of differential privacy on MacOS 10.12,” ArXiv,
vol. 1709.02753, 2017.
[46] M. C. Tschantz, A. Datta, A. Datta, and J. M. Wing, “A methodology
for information ﬂow experiments,” in Computer Security Foundations
Symp., 2015.
[47] Y. Wang and M. Kosinski, “Deep neural networks are more accurate
than humans at detecting sexual orientation from facial images,” 2017.
[48] S. L. Warner, “Randomized response: A survey technique for eliminating
evasive answer bias,” J. the American Statistical Association, vol. 60,
no. 309, pp. 63–69, 1965.
[49] B. Yang, I. Sato, and H. Nakagawa, “Bayesian differential privacy on
correlated data,” in 2015 ACM SIGMOD Intl. Conf. on Management of
Data, ser. SIGMOD ’15, 2015, pp. 747–762.
[50] T. Zhu, P. Xiong, G. Li, and W. Zhou, “Correlated differential privacy:
Hiding information in non-IID data set,” IEEE Transactions on Infor-
mation Forensics and Security, vol. 10, no. 2, pp. 229–242, 2015.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
367
APPENDIX
A. Two Views of Differential Privacy: A Brief History
Throughout this paper, we have mentioned two lines of
work about DP. The historically ﬁrst line, associated with its
creators, views DP as not requiring additional assumptions,
such as independent data points or an adversary that already
knows all but one data point. The historically second line views
such assumptions as needed by or implicit in DP. Here, we
brieﬂy recount the history of the two lines.
Apparently independently,
1) Before Differential Privacy: The idea of a precise frame-
work for mathematically modeling the conditions under which
an adversary does not learn something perhaps starts with
Shannon’s work on perfect security in 1949 [44]. In 1984, this
idea led to Goldwasser and Silvio’s cryptographic notion of
semantic security, which relaxes Shannon’s requirement by ap-
plying to only polynomially computationally bounded adver-
saries [22] (with antecedents in their earlier 1982 work [21]).
the statistics community also
considered limiting what an adversary would learn. One early
work cited by DP papers (e.g., [11]) is Dalenius’s 1977
paper on statistical disclosure [9]. Dalenius deﬁnes statistical
disclosures in terms of a frame of objects, for example, a
sampled population of people [9, §4.1]. The objects have data
related to them [9, §4.2]. A survey releases some statistics over
such data for the purpose of fulﬁlling some objective [9, §4.3].
Finally, the adversary may have access to extra-objective data,
which is auxiliary information other than the statistics released
as part of the survey. Dalenius deﬁnes a statistical disclosure
as follows [9, §5]:
If the release of the statistics S makes it possible
to determine the value DK more accurately than is
possible without access to S, a disclosure has taken
place [...]
where DK is the value of the attribute D held by the object
(e.g., person) K. The attribute D and object K may be used
in the computation of S or not. The extra-objective data may
be used in computing the estimate of DK.
As pointed out by Dwork [11], Dalenius’s work is both
similar to and different from the aforementioned work on
cryptosystems. The most obvious difference is looking at
databases and statistics instead of cryptosystems and messages.
However, the more signiﬁcant difference is the presence of
the objective with a beneﬁt, or the need for utility in Dwork’s
nomenclature. That is, the released statistics is to convey some
information to the public; whereas, the encrypted message, the
cryptosystem’s analog to the statistic, only needs to convey
information to the intended recipient. Dalenius recognized
that this additional need makes the elimination of statistical
disclosures “not operationally feasible” and “would place
unreasonable restrictions on the kind of statistics that can be
released” [9, §18].
Even before the statistics work on statistical nondisclosure,
statistical research by S. L. Warner in 1965 introduced the
randomized response method of providing DP [48]. (His work
is more similar to the local formulation of DP [16].) The
randomized response model and statistical disclosure can be
viewed as the prototypes of the ﬁrst and second lines of
reseach respectively, although these early works appear to have
had little impact on the actual formation of the lines of reseach
over a quarter century later.
2) Differential Privacy:
In March 2006, Dwork, Mc-
Sherry, Nissim, and Smith presented a paper containing
the ﬁrst modern instance of DP under the name of “-
indistinguishable” [17]. The earliest use of the term “differ-
ential privacy” comes from a paper by Dwork presented in
July 2006 [11]. This paper of Dwork explicitly rejects the
view that DP provides associative or inferential privacy [11,
p. 8]:
Note that a bad disclosure can still occur [despite
DP], but [DP] assures the individual that it will not be
the presence of her data that causes it, nor could the
disclosure be avoided through any action or inaction
on the part of the user.
and further contains a proof that preventing Dalenius’s statis-
tical disclosures while releasing useful statistics is impossible.
(The proof was joint work with Naor, with whom Dwork later
further developed the impossibility result [18].) Later works
further expound upon their position [16], [26].
3) Questions Raised about Differential Privacy: In 2011,
papers started to question whether DP actually provides a
meaningful notion of privacy [7], [27], [19]. These papers
point to the fact that a released statistic can enable inferring
sensitive information about a person, similar to the attacks
Dalenius wanted to prevent [9], even when that statistic
was computed using a differentially private algorithm. While
the earlier work on DP acknowledged this limitation, these
papers provide examples where correlations, or more generally
associations, between data points can enable inferences that
some people might not expect to be possible under DP. These
works kicked off a second line of research (including, e.g.,
[28], [29], [23], [5], [50], [33]) attempting to ﬁnd stronger
deﬁnitions that account for such correlations. In some cases,
these papers assert that such inferential threats are violations
of privacy and not what people expect of DP. For example,
Liu et al.’s abstract states that associations between data points
can lead to “degradation in expected privacy levels” [33]. The
rest of this subsection provides details about these papers.
In 2011, Kifer and Machanavajjhala published a paper stat-
ing that the ﬁrst popularized claim about DP is that “It makes
no assumptions about how data are generated” [27, p. 1].
The paper then explains that “a major criterion for a privacy
deﬁnition is the following: can it hide the evidence of an
individual’s participation in the data generating process?” [27,
p. 2]. It states [27, p. 2]:
We believe that under any reasonable formaliza-
tion of evidence of participation, such evidence can
be encapsulated by exactly one tuple [as done by
DP] only when all tuples are independent (but not
necessarily generated from the same distribution).
We believe this independence assumption is a good
rule of thumb when considering the applicability of
differential privacy.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
368
For this reason, the paper goes on to say “Since evidence of
participation requires additional assumptions about the data (as
we demonstrate in detail in Sections 3 and 4), this addresses
the ﬁrst popularized claim – that differential privacy requires
no assumptions about the data” [27, p. 2]. From context, we
take “addresses” to mean invalidates since the paper states
“The goal of this paper is to clear up misconceptions about
differential privacy” [27, p. 2].
In 2012, Kifer and Machanavajjhala published follow up
work stating that “we use [the Pufferﬁsh framework] to formal-
ize and prove the statement that differential privacy assumes
independence between records” [28, p. 1]. It goes on to say
“Assumptionless privacy deﬁnitions are a myth: if one wants to
publish useful, privacy-preserving sanitized data then one must
make assumptions about the original data and data-generating
process” [28, p. 1, emphasis in original]. In 2014, Kifer and
Machanavajjhala published a journal version of their 2012 pa-
per, which makes a similar statement: “Note that assumptions
are absolutely necessary – privacy deﬁnitions that can provide
privacy guarantees without making any assumptions provide
little utility beyond the default approach of releasing nothing
at all” [29, p. 3:5]. However, this version is, overall, more
qualiﬁed. For example, it states “The following theorem says
that if we have any correlations between records, then some
differentially private algorithms leak more information than is
allowable (under the odds ratio semantics in Section 3.1)” [29,
3:12–13], which makes it clear that the supposed shortcoming
of DP in the face of correlated data points is relative to a
particular notion of privacy presented in that paper, roughly,
reducing uncertainty about some sensitive fact about a person.
Also in 2014, He et al. published a paper building upon
the Pufferﬁsh framework [23]. Referring to the conference
version [28], He et al. states [23, p. 1]:
[Kifer and Machanavajjhala] showed that differential
privacy is equivalent to a speciﬁc instantiation of
the Pufferﬁsh framework, where (a) every property
about an individual’s record in the data is kept
secret, and (b) the adversary assumes that every
individual is independent of the rest of the individ-
uals in the data (no correlations). We believe that
these shortcomings severely limit the applicability
of differential privacy to real world scenarios that
either require high utility, or deal with correlated
data.
and “Recent work [by Kifer and Machanavajjhala] showed
that differentially private mechanisms could still lead to an
inordinate disclosure of sensitive information when adversaries
have access to publicly known constraints about the data that
induce correlations across tuples” [23, p. 3].
In 2013, Li et al. published a paper that states “differential
privacy’s main assumption is independence” [32, p. 2]. Sim-
ilar, to the papers by Kifer and Machanavajjhala, this paper
assumes a technical deﬁnition of privacy, positive member-
ship privacy, and makes this assertion since independence
is required for DP to imply it. The paper also claims that
“the original deﬁnition of differential privacy assumes that
the adversary has precise knowledge of all the tuples in the
dataset” [32, p. 10], which we take as a reference to the strong
adversary assumption.
Chen et al.’s 2014 paper is the ﬁrst of three attempting to
provide an associative version of privacy, motivated by Puffer-
ﬁsh, in the face of correlated data [5]. It states “-differential
privacy fails to provide the claimed privacy guarantee in the
correlated setting” [5, p. 2] and “-differential privacy is built
on the assumption that all underlying records are independent
of each other” [5, p. 7].
The second paper, Zhu et al.’s paper, published in 2015,
provides a more accurate accounting of correlations [50]. It
states [50, p. 229]:
An adversary with knowledge on correlated infor-
mation will have higher chance of obtaining the
privacy information, and violating the deﬁnition of
differential privacy. Hence, how to preserve rigorous
differential privacy in a correlated dataset
is an
emerging issue that needs to be addressed.
It further asserts [50, p. 231]:
In the past decade, a growing body of literature
has been published on differential privacy. Most
existing work assumes that the dataset consists of
independent records.
and “a major disadvantage of traditional differential privacy
is the overlook of the relationship among records, which
means that the query result leaks more information than is
allowed” [50, p. 232].
The third paper, by Liu et al. in 2016, provides an even
more accurate accounting of correlations [33]. A blog post
by one of the authors, Mittal, announcing the paper states
“To provide its guarantees, DP implicitly assumes that the
data tuples in the database, each from a different user, are
all independent.” [40]. In ﬁve comments on this blog post,
McSherry posted a summary of his concerns about their paper
and blog post. McSherry also treats the paper at length in a
blog post [35]. McSherry highlights three statements made
by the paper that he ﬁnds false [35]: (1) “For providing this
guarantee, differential privacy mechanisms assume indepen-
dence of tuples in the database” [33, p. 1], (2) “To provide
its guarantees, DP mechanisms assume that the data tuples
(or records) in the database, each from a different user, are
all independent.” [33, p. 1], and (3) “However, the privacy