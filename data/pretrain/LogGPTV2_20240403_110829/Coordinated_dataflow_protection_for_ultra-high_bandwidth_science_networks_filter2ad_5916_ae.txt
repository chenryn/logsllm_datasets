dynamic network security conditions (§5.1).
7
2) Inter-Site Tag Space Allocation: While the tag assignment de-
cision happens locally within each site, we use the centralized CNZ
Coordinator for allocating the tag space (i.e., range of tags) for each
project. Our inter-site tag space allocation mechanism, assigns the
tag space to each project registered with the CNZ Coordinator(§5.2).
5.1 Intra-Site Tag Assignment
To extend fine-grained traffic filtering capabilities, beyond contem-
porary IP-based mechanisms, we develop an efficient context-aware
policy-based tagging mechanism, called cTags, that enables:
• Logically group traffic that spans across subnets, hosts or
geographic locations for policy enforcement.
• Dynamically steer, revoke, or forward traffic across different
Network Function Chains (NFC) according to dynamic network
and security conditions.
Although the tag assignment is carried out by the CNZ Controller
of any site, the actual tag is embedded into the flow by host DTNs
for traffic generated from the host applications depending on the
configured policies. The conflict-free policies supplied by the CNZ
Coordinator are reconciled to site-specific policies and further
translated to device-specific rules by the CNZ Controller and SDN
controller before being placed across host DTNs and SDN switches.
The set of rules supplied to each host, which we call as policy-to-tag
mappings captures following details:
• tagID{T1} => policyID{P1}:appID{A1}:userID{U1}:expID{E1}
• policyID{P1} => policySpec{...}
The mappings carry necessary details specific to each policy
and the associated entities for enforcement. The SciMon module
can dynamically change the flow tagID, even in the middle of
flows depending on dynamic network conditions, by updating the
policy-to-tag mapping entry.
Each SDMZ site needs to optimize the number of rules re-
quired to enforce the policies by considering the availability of
high-bandwidth switches and their switch TCAM space. Tagging
facilitates rule space optimization by: (i) allowing large number
of hosts to be grouped into a common logical entity (i.e., beyond
IP-tuple-based filtering) and (ii) efficiently assigning contiguous
tags such that policies having same action attributes may be grouped
together using bit masking. Each policy is simply associated with
a unique tag after resolving the conflicts among the policies. From
the composed graph all the nodes and edges that are associated with
a policy is assigned the same tag. Here, the number of tags required
is approximately equals the number of conflict-free policies. Further
the optimization proposed to tag space utilization in collaborative
SDMZ network is discussed in next Section 5.2.
5.2 Inter-Site Tag Allocation
In the SDMZ infrastructure, security and data analysis services
provided by higher-tier sites (i.e., tier-0 or tier-1 DoE sites) are
availed by lower-tier sites [6, 24]). For effectively sharing such
services across sites, tags assigned by one site must be honored by
other sites handling the same project.
To avoid conflicts in tag space utilization we propose a unified
tag space allocation mechanism that allocates necessary tag space
to each project (with additional slack tag space for future policies).
Though, the tag space allocation is carried out globally at the CNZ
Figure 8: Tag-space allocation with edge color assignment.
Project IDs and colors are used to annotate each edge.
Coordinator, the tag assignment to each policy is carried out locally
within the site with the help of CNZ Controller. As a design choice,
we use IPv6 flow label bits as tagID. Since 20 bits of flow label header
in IPv6 cannot effectively accommodate the tagging requirement
of thousands of projects handled across hundreds of SDMZ sites
we need a centralized tag-allocation mechanism to effectively reuse
tag bits across projects spanning multiple sites.
We assign a specific color to each project within a site and reuse
the same color among other projects across other sites registered
with CNZ Coordinator with following two design considerations: (i)
the tag space should never overlap with the tag space assigned to
its immediate adjacent sites with which the current site has project
association, and (ii) tag size assigned to each project depends on
the number of policies enforced by the project. The key objective
of the tag-space allocation mechanism described in Algorithm 2
is to maximize the efficient reuse of tag space (i.e., colors) among
cross-site projects, while avoiding overlaps.
Algorithm 2 details the tag-space allocation mechanism used by
the CNZ Coordinatortoallocatearangeoftagstoeachofitsregistered
projects. The CNZ Coordinator traverses through the list of all SN
sites associated with it in a breadth-first-search manner. For any cho-
sen site Si, its adjacent sites are compared before allocating colors.
We observe that for each site Si and its adjacent sites the complete
list of available colors can be used in the assignment procedure.
The colors are assigned between Si and SAi . Each of the adjacent
sites SAi of Si (depending on the the list of projects belonging
to SAi that are associated with Si), are assigned one color per
project (depending on their policy size). Colors are assigned to each
project in SAi , that is associated with Si, such that: (i) it satisfies the
project’s tag space requirement, SAi , (ii) the color with the least size
is considered for assignment and (iii) no other projects in SAi have
the same color already assigned to it. Similar approach is taken for
all projects that are associated with site Si having adjacent node SAi .
This procedure is carried out for all the adjacent sites of Si. When the
list of adjacent sites of Si is exhausted, the CNZ Coordinator picks the
next site from SAi as the new Si, carrying out the aforementioned
procedure until all sites in SN are iterated atleast once. An example
illustration of our algorithmic outcome is shown in Figure 8. The
algorithm is quadratic in the number of sites in the worse-case
for a fully connected graph (i.e., all sites share all projects). As the
number of sites does not change frequently, the overall complexity
grows linearly with the number of projects. To further optimize the
tag space utilization and efficiently reuse the tag space we propose
technique, which is discussed in Appendix A.5.
(b) Average latency in composing
20k policies (With 10, 30, 50, 70
& 100 # of abstraction trees).
(a) Average latency in policy com-
position with ≈30 abstraction
trees and ≈15% conflicts.
Figure 9: Scalability of policy composition engine for conflict
detection & resolution.
6 System Evaluation
The CoordiNetZ evaluation platform was composed of Dell
R720 servers with 72GB RAM, 24 cores (2.67GHz) and Ubuntu
4.4.0-97-generic kernel used as DTNs, IDS hosts and CoordiNetZ
controller (i.e., hosting the CNZ Controller and CNZ Coordinator). A
quad-core Intel NUC server as the SDN controller. Dell R710 servers
with 48GB RAM, 16 cores (2.6GHz) Ubuntu 4.4.0-97-generic kernel
integrated with DPDK-based OVS [30] that acts as switch and node
that hosts security microservices. Host DTNs were interfaced inline
with SDN switches via multiple Mellanox ConnectX-4 Lx 40GbE
MT27500 Family 40 Gbps NICs. The server-based DPDK-enabled
OVS switches [30] implemented tag-based forwarding and
lightweight security services (e.g., rate limiting, spoofing protection,
and connection tracking). The SDN controller and CNZ Controller
were interfaced with host DTNs, OVS, and IDS service, via the
management network interface. The CNZ Coordinator and controller
communicated via a separate management network.
Policy and Infrastructure Datasets: We evaluate our prototype
using following three different datasets:
1) PS-1: Policy sets from two different SDMZ network infras-
tructures [33, 45] with ≈150 and ≈400 SDMZ policies (i.e., 5325 and
7987 enforceable rules respectively) to benchmark the framework.
Infrastructure abstraction trees required for these two SDMZ
networks were constructed to drive the PS-1 policy configuration.
2) PS-2: Derived from PS-1, this is a large synthetic policy set
of 20k policies for coordinator-scale experimentation, emulating 40
different SDMZ networks. Infrastructure abstraction trees were con-
structedusingascaledup PS-1configuration.Sourceanddestination
nodes for policies were chosen randomly by sampling technique,
and dynamic states and conditions were added as edge properties.
3) DS-1: This dataset emulates collaborative SDMZ network
based on the “High Energy Physics - Theory collaboration network”
dataset [15], which employs ≈9.8k nodes, with ≈25k edges.
Policy Composition: We evaluated the performance of the
policy composition engine using the policy set PS-2. Figure 9a
illustrates the latency incurred by the composition engine during
pre-deployment. From the list of 20k policies, 1k,...20k policy sets
were randomly selected. Their average composition times were
computed over 10 rounds, which took ≈49 sec to compose 20K
policies. To enhance composition performance, we employed a
simple hashing technique to cache policies and policy attributes (see
§4.3). Experiments were run to assess the impact of caching when an
8
3(cid:31)(cid:34)5(cid:29)(cid:1)1(cid:32)(cid:31):86(cid:34)(cid:33)(cid:16)(cid:1)(cid:1)(cid:7)(cid:8)05(cid:36)9(cid:30)(cid:35)(cid:30)(cid:1)(cid:17)(cid:31)(cid:29)(cid:31)(cid:32)(cid:33)(cid:1)4(cid:33)87(cid:16)(cid:1)(cid:13)1(cid:7)(cid:1)(cid:5)(cid:5)1(cid:9)(cid:1)(cid:2)(cid:17)(cid:7)(cid:5)(cid:17)(cid:9) 1)(cid:1)(cid:2)(cid:17)) 1(cid:9)(cid:4)(cid:1)1((cid:1)(cid:2)(cid:17)(cid:9)(cid:4)(cid:17)( 1((cid:4)1-(cid:4)1(cid:7)(cid:6)(cid:1)(cid:2)(cid:17)(cid:7)(cid:4)(cid:17)(cid:8)(cid:4)(cid:17)) 1-(cid:1)(cid:5)(cid:5)1(cid:7)(cid:7)(cid:2)(cid:17)(cid:9)(cid:4)(cid:17)((cid:4)(cid:17)(cid:12) 1(cid:12)(cid:4)(cid:1)1(cid:13)(cid:1)(cid:2)(cid:17)((cid:4)(cid:17)(cid:12) 1(cid:7)(cid:8)(cid:4)(cid:1)(cid:2)(cid:17)(cid:7) 1,(cid:2)(cid:17)(cid:13) 2(cid:7)2(cid:8)2(cid:9)2(2)2(cid:13)2(cid:12)3(cid:31)(cid:34)5(cid:29)(cid:1)1(cid:32)(cid:31):86(cid:34)(cid:33)(cid:16)(cid:1)(cid:1),05(cid:36)9(cid:30)(cid:35)(cid:30)(cid:1)(cid:17)(cid:31)(cid:29)(cid:31)(cid:32)(cid:33)(cid:1)4(cid:33)87(cid:16)(cid:1)(cid:12)1,(cid:4)(cid:1)1-(cid:1)(cid:2)(cid:17))(cid:4)(cid:17)(cid:12) 1(cid:7)(cid:4)(cid:1)1(cid:8)(cid:1)(cid:2)(cid:17)(cid:7)C(cid:17)(cid:8) 1)(cid:1)(cid:2)(cid:17)) 1(cid:9)(cid:1)(cid:5)(cid:5)1((cid:2)(cid:17)(cid:9)(cid:4)(cid:1)(cid:17)( 2(cid:8)2(cid:9)2(cid:7)2(1(cid:12)(cid:1)(cid:2)(cid:17)(cid:8) 1(cid:13)(cid:1)(cid:2)(cid:17)(cid:9) 0.05.0K10.0K15.0K20.0K# Policies02040Avg. Composition Latency (sec)CoordiNetZ Composition0102030405060708090100# Abstraction Trees0306090Composition Latency (sec)CoordiNetZ CompositionCoordiNetZ Composition (Caching)(a) Tagging efficiency.
(b) Rule space utilization.
Figure 10: Intra-site tagging performance with SDMZ
campus datasets.
increasing number of abstraction trees are produced. We tested the
composition latency for 20k policies built using 10, 30, 50, 70 and 100
abstraction trees (shown in Figure 9b). We find that increasing the
number of abstraction trees count produces more policy source and
target nodes, thereby increasing the cost to create the composition
graph. Caching the relations among the nodes, reduces the com-
position latency by upto ≈2.25× compared to composition with out
caching. Figure 9b illustrates that increasing number of abstraction
trees gradually diminishes the benefits of caching due to reduced
likelihood of overlap in source-node, edge, and target-node pairings.
Tagging Efficiency: To evaluate the tag-based policy enforcement
mechanism from §5.1, we use policy set PS-1 and PS-2. We
examined a policy set PS-1 from 2 SDMZ campus networks