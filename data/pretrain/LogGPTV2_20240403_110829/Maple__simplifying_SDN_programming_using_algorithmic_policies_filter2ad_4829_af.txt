### Optimized Text

#### Matching on IP and Transport Fields
We introduce a third controller that operates in native L2 mode (i.e., without OpenFlow) at any of the switches.

#### End-to-End HTTP Connection Time
Figure 6(b) illustrates the mean end-to-end HTTP connection time, measured by `httperf` as the duration from the initiation of a TCP connection to its closure. This includes the time required to set up flow tables at all three switches. The x-axis represents the request rate and the number of requests from the client to the HTTP server. For example, a value of 100 indicates that the client initiates one HTTP connection every 10 ms (or 1/100 seconds) for a total of 100 connections.

#### Maple Augmentation and Invalidation Rates
Table 2 provides the augmentation and invalidation rates for Maple. "H Invals" and "P Invals" denote host and port invalidations, respectively.

| Filter Set | Augments/s | Lookups/s | H Invals/s | P Invals/s |
|-----------|------------|-----------|------------|------------|
| mt-route   | 58719.65   | 156284    | 2491       | 13215      |
| acl1a      | 1180.74    | 1151      | 3378       | 1745       |
| acl2a      | 508.40     | 1569      | 518        | 2332       |
| acl3a      | 202.17     | 3828      | 626        | 5487       |
| fw1a       | 205.52     | 4629      | 1468       | 7250       |
| fw2a       | 362.98     | 1194      | 1292       | 1770       |
| ipc1a      | 621.23     | 3089      | 518        | 5066       |
| ipc2a      | 666.67     | 1358      | 928        | 2200       |
| acl1b      | 245.92     | 601       | 10143      | 924        |
| acl2b      | 223.75     | 626       | 17622      | 965        |
| acl3b      | 68.52      | 1468      | 32917      | 2073       |
| fw1b       | 51.40      | 1292      | 25994      | 1933       |
| fw2b       | 142.11     | 518       | 10143      | 846        |
| ipc1b      | 185.07     | 928       | 17622      | 1934       |
| ipc2b      | 555937     | 156284    | 58631      | 13215      |

#### Server Configuration
Maple is run on a Dell PowerEdge R210 II server with 16GB DDR3 memory and Intel Xeon E31270 CPUs (with hyper-threading) running at 3.40GHz. Each CPU has 256KB L2 cache and 8MB shared L3 cache. CBench is run on a separate server, and both servers are connected via a 10Gbps Ethernet network.

#### Throughput Results
Table 2 shows the throughput for each operation type and policy using a single 3.40 GHz core with a single thread. For the `mt-route` policy, which uses only V nodes, Maple can perform all operations at high speed, including both augmentations and invalidations. The augmentation throughput varies for Classbench-based policies. The `fw2b` policy takes the longest time (20 ms) for Maple to handle a miss. For most policies, invalidation is faster than augmentation, reflecting the fact that invalidations do not require adjusting priority levels.

#### Run-time Scheduler Performance
We evaluate the performance of our multicore scheduler. If the programmer-provided function `f` has no locality, all requests will be forwarded to the controller for centralized processing. We use a learning switch with exact match to evaluate our scheduler, as this controller is available in other frameworks. We measure both throughput (the number of requests processed per second) and latency. The optimizer component of Maple is not executed in these evaluations to compare with Beacon and NOX-MT [21], two well-known OpenFlow control frameworks that aim to provide high performance.

#### Server Configuration for SDN Controllers
Our OpenFlow controllers are run on an 80-core SuperMicro server with 8 Intel Xeon E7-8850 2.00GHz processors, each having 10 cores with a 24MB smart cache and 32MB L3 cache. We use four 10 Gbps Intel NICs. The server software includes Linux kernel version 3.7.1 and Intel ixgbe driver (version 3.9.17).

#### Workload Simulation
We simulate switches using a modified version of Cbench to generate sufficient workload. We use 8 Cbench workload servers connected over 10Gbps links to a single L2 switch, which connects to four 10Gbps interfaces of our control server. We limit the packet-in messages generated by Cbench so that the number of outstanding requests from a single Cbench instance does not exceed a configurable limit. This allows us to control the response time while evaluating throughput.

#### Throughput and Latency Results
Figure 7(a) shows the throughput as a function of the number of cores used for all three systems. Maple serves over 20 million requests per second using 40 cores and scales substantially better than Beacon or NOX-MT. Specifically, Beacon scales to less than 15 million requests per second, and NOX-MT is only around 2 million requests per second. Figure 7(b) shows the corresponding latency CDF for all three systems. The median latency of Maple is 1 ms, Beacon is almost 4 ms, and NOX-MT reaches as high as 17 ms. The 95th percentile latency of Maple is still under 10 ms.

#### Related Work
SDNs have motivated much recent work, which we classify into basic SDN controllers, programming abstractions, offloading work to switches, and controller scalability.

- **Basic SDN Controllers**: NOX [8] offers C++ and Python APIs for raw event handling and switch control, while Beacon [1] offers a similar API for Java. These APIs require the programmer to manage low-level OpenFlow state explicitly, such as switch-level rule patterns, priorities, and timeouts. Maple derives this low-level state from a high-level algorithmic policy expression.
- **SDN Programming Abstractions and Languages**: Maestro [3] raises the abstraction level of SDN programming with modular network state management using programmer-defined views. SNAC [19] and FML [9] offer high-level pattern languages for specifying security policies. Onix [10] introduces the NIB abstraction so that applications modify flow tables through reading and writing to key-value pairs stored in the NIB. Casado et al. [4] propose network virtualization abstraction. Frenetic [7], Pyretic [16], and Nettle [23] provide new languages for SDN programming. Freneticâ€™s NetCore language supports specialized forms of composition, such as between statistics-gathering and control rules. In contrast, Maple is agnostic to the language for expressing policies and benefits from whatever features (e.g., composition) the language offers.
- **Offloading Work to Switches**: DevoFlow [6] increases scalability by refactoring the OpenFlow API, reducing the coupling between centralized control and visibility. Frenetic leverages its NetCore language to compile rules for switch flow tables, alleviating the complex challenge of managing flow tables [7, 15]. Maple similarly compiles to switch flow tables, but its tracing approach supports generic algorithms expressed in arbitrary languages.

#### Conclusions
This paper explores a powerful, programmer-friendly SDN programming model where a programmer uses standard algorithmic programming to design arbitrary algorithms for SDN control. We present novel techniques to address the scalability challenges of algorithmic SDN programming transparently and show that the result is highly scalable on a variety of benchmarks using both simulated and real network workloads.

#### Acknowledgements
We thank Michael F. Nowlan, Lewen Yu, Ramki Gummadi, Erran Li, Haibin Song, Chen Tian, SIGCOMM reviewers, and Ion Stoica (shepherd) for their suggestions. Andreas Voellmy was primarily supported by a gift from Futurewei and partially supported by NSF grant CNS-1017206. Paul Hudak was supported in part by a gift from Microsoft Research.

#### References
[1] https://openflow.stanford.edu/display/Beacon/Home.
[2] T. Benson, A. Akella, and D. A. Maltz. Network Traffic Characteristics of Data Centers in the Wild. In Proc. of IMC, 2010.
[3] Z. Cai, A. L. Cox, and T. S. Eugene Ng. Maestro: Balancing Fairness, Latency, and Throughput in the OpenFlow Control Plane. Technical report, Rice, 2011.
[4] M. Casado, T. Koponen, R. Ramanathan, and S. Shenker. Virtualizing the Network Forwarding Plane. In Proc. of PRESTO, 2010.
[5] Cbench. Cbench, 2012. [Online; accessed 10-April-2012].
[6] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma, and S. Banerjee. DevoFlow: Scaling Flow Management for High-Performance Networks. In Proc. of SIGCOMM, 2011.
[7] N. Foster, R. Harrison, M. J. Freedman, C. Monsanto, J. Rexford, A. Story, and D. Walker. Frenetic: a Net. Programming Language. In Proc. of ICFP, 2011.
[8] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McKeown, and S. Shenker. NOX: Towards an Operating System for Networks. SIGCOMM Comput. Commun. Rev., 2008.
[9] T. L. Hinrichs, N. S. Gude, M. Casado, J. C. Mitchell, and S. Shenker. Practical Declarative Network Management. In Proc. of WREN, 2009.
[10] T. Koponen, M. Casado, N. Gude, J. Stribling, L. Poutievski, M. Zhu, R. Ramanathan, Y. Iwata, H. Inoue, T. Hama, and S. Shenker. Onix: a Distributed Control Platform for Large-scale Production Networks. In Proc. of OSDI, 2010.
[11] S. Marlow, S. Peyton Jones, and S. Singh. Runtime Support for Multicore Haskell. In Proc. of ICFP, 2009.
[12] B. Marsh, M. Scott, T. LeBlanc, and E. Markatos. First-class User-level Threads. ACM SIGOPS Operating Systems Review, 1991.
[13] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford, S. Shenker, and J. Turner. OpenFlow: Enabling Innovation in Campus Networks. SIGCOMM Comput. Commun. Rev., 2008.
[14] J. C. Mogul et al. DevoFlow: Cost-effective Flow Management for High Performance Enterprise Networks. In Proc. of Hotnets, 2010.
[15] C. Monsanto, N. Foster, R. Harrison, and D. Walker. A Compiler and Run-time System for Network Programming Languages. In Proc. of POPL, 2012.
[16] C. Monsanto, J. Reich, N. Foster, J. Rexford, and D. Walker. Composing Software-Defined Networks. In Proc. of NSDI, 2013.
[17] D. Mosberger and T. Jin. httperf: a Tool for Measuring Web Server Performance. SIGMETRICS Perform. Eval. Rev., 1998.
[18] D. Shah and P. Gupta. Fast Updating Algorithm for TCAMs. IEEE Micro, 2001.
[19] Simple network access control (SNAC). http://www.openflow.org/wp/snac/.
[20] D. E. Taylor and J. S. Turner. ClassBench: a Packet Classification Benchmark. IEEE/ACM Trans. Networking, 2007.
[21] A. Tootoonchian, S. Gorbunov, Y. Ganjali, M. Casado, and R. Sherwood. On Controller Performance in Software-Defined Networks. In Hot-ICE, 2012.
[22] A. Voellmy, B. Ford, P. Hudak, and Y. R. Yang. Scaling Software-defined Network Controllers on Multicore Servers. YaleCS TR1468, 2012.
[23] A. Voellmy and P. Hudak. Nettle: Taking the Sting Out of Programming Network Routers. In Proc. of PADL, 2011.