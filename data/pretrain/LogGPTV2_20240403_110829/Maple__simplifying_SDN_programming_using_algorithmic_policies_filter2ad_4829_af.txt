match as being exact on IP and transport ﬁelds. We introduce the
third controller, which is the native L2 mode (i.e., no Openﬂow) at
any of the switches.
Figure 6(b) shows the mean end-to-end HTTP connection time,
which is measured by httperf as the time between a TCP connection
is initiated to the time that the connection is closed, and hence in-
cludes the time to set up ﬂow tables at all 3 switches. The x-axis of
the ﬁgure is the request rate and number of requests from the client
to the HTTP server. For example, 100 means that the client issues
one HTTP connection per 10 ms (=1/100 sec) for a total of 100
96Filter set Augments/s
58719.65
mt-route
1180.74
acl1a
acl2a
508.40
605.34
acl3a
202.17
fw1a
205.52
fw2a
362.98
ipc1a
621.23
ipc2a
acl1b
666.67
245.92
acl2b
223.75
acl3b
68.52
fw1b
51.40
fw2b
142.11
ipc1b
185.07
ipc2b
Lookups/s H Invals/s
156284
2491
1151
1569
3828
4629
1194
3089
1358
601
626
1468
1292
518
928
555937
58631
15931
19348
73433
85013
18053
50544
40133
9809
9749
32917
25994
10143
17622
P Invals/s
13215
3378
1745
2332
5487
7250
1770
5066
2200
924
965
2073
1933
846
1934
Table 2: Maple augments/invalids rates. H Invals and P Invals
denote host and port invalidations respectively.
install new rules. During subsequent passes, we measure lookup
throughput, as the trace tree has cached results for every packet in
the trace. Finally, we perform invalidations for either all hosts used
in the trace or for all ports used in the trace
Server: We run Maple on a Dell PowerEdge R210 II server, with
16GB DDR3 memory and Intel Xeon E31270 CPUs (with hyper-
threading) running at 3.40GHz. Each CPU has 256KB L2 cache
and 8MB shared L3 cache. We run CBench on a separate server
and both servers are connected by a 10Gbps Ethernet network.
Results: Table 2 shows throughput for each operation type and
policy, using a single 3.40 GHz core, with a single thread. For the
mt-route policy, which uses only V nodes, Maple can perform
all operations at high-speed, including both augments and invali-
dates. The augmentation throughput of Classbench-based policies
varies. The fw2b policy takes the longest time (20 ms) for Maple
to handle a miss. For most policies, invalidation can be handled
faster than augmentation, reﬂecting the fact that invalidations do
not require adjusting priority levels, and thus can be done faster.
6.4 Run-time Scheduler
We next evaluate the performance of our multicore scheduler. In
particular, if the programmer-provided function f has no locality,
then all requests will be forwarded to the controller for centralized
processing. We use learning switch with exact match to evaluate
our scheduler, since this controller is available in other frameworks.
We measure both throughput (i.e., the number of requests that our
controller can process each second) and latency. The optimizer
component of Maple is not executed in evaluations in this section to
compare with Beacon and NOX-MT [21], two well-known Open-
ﬂow control frameworks that aim to provide high performance.
Server: We run our Openﬂow controllers on an 80 core SuperMi-
cro server, with 8 Intel Xeon E7-8850 2.00GHz processors, each
having 10 cores with a 24MB smart cache and 32MB L3 cache. We
use four 10 Gbps Intel NICs. Our server software includes Linux
kernel version 3.7.1 and Intel ixgbe driver (version 3.9.17).
Workload: We simulate switches with a version of Cbench modi-
ﬁed to run on several servers, in order to generate sufﬁcient work-
load. We use 8 Cbench workload servers connected over 10Gbps
links to a single L2 switch, which connects to four 10Gbps inter-
faces of our control server. We limit the packet-in messages gener-
ated by CBench, so that the number of requests outstanding from a
single CBench instance does not exceed a conﬁgurable limit. This
allows us to control the response time while evaluating throughput.
Results: Figure 7(a) shows the throughput as a function of the
number of cores used for all three systems. We observe that Maple
serves over 20 million requests per second using 40 cores and scales
substantially better than Beacon or NOX-MT. In particular Beacon
(a) Flow table miss rate
(b) HTTP connection time using real HP switches
Figure 6: Effects of optimizing ﬂow rules.
connections. Note that the y-axis is shown as log scale. We make
the following observations. First, the HTTP connection setup time
of the exact-match controller and that of Maple are the same when
the connection rate is 1. Then, as the connection rate increases,
since Maple incurs table misses only on the ﬁrst connection, its
HTTP connection time reduces to around 1 ms to slightly above
2 ms when the connection rate is between 10 to 120. In contrast,
the exact-match controller incurs table misses on every connection
and hence its HTTP connection time increases up to 282 ms at con-
nection rate 120. This result reﬂects limitations in the switches,
since the load on the controller CPU remains below 2% throughout
the test, and we ensured that the switches’ Openﬂow rate limiters
are conﬁgured to avoid affecting the switches’ performance. Sec-
ond, we observe that when the connection rate increases from 80
to 100, the switch CPUs becomes busy, and the HTTP connection
time starts to increase from around 1 ms to 2 ms. Third, Maple has
a longer HTTP connection time compared with native L2 switch,
which suggests potential beneﬁts of proactive installation.
6.3 Flow Table Management Throughput
After evaluating the quality of Maple generated ﬂow table rules,
we now evaluate the throughput of Maple; that is, how fast can
Maple maintain its trace tree and compile ﬂow tables?
Types of operations: We subject Maple to 3 types of operations:
(1) augments, where Maple evaluates a policy, augments the trace
tree, generates ﬂow table updates, and installs the updates at switches;
(2) lookups, where Maple handles a packet by looking up the cached
answer in the trace tree; and (3) invalidations, where Maple inval-
idates part of the trace tree and deletes rules generated for those
subtrees. In particular, we evaluate host invalidations, which are
caused by host mobility and remove all leaf nodes related to a
moved host, and port invalidations, which support topology up-
dates and remove any leaf node whose action uses a given port.
Workload: We use the same policies and packet traces as in Sec-
tion 6.1, but we process each packet trace multiple times. In partic-
ular, during the ﬁrst pass, as nearly every packet will cause an aug-
ment operation, we measure the throughput of Maple to record and
97Maple designs an optimizer that minimizes the number of priority
levels, important for reducing update overhead.
SDN controller scaling: The NOX-MT branch of NOX uses Boost
for IO and threading. Beacon [1] uses Java threads to scale to a
modest number of cores. Onix [10] partitions network state across
multiple distributed controllers, alleviating scalability and fault-
tolerance concerns by compromising the attractive simplicity of
the centralized model. Maestro [3] offers multiple techniques to
scale across multiple cores. Maple uses techniques such as afﬁnity-
based, switch-level scheduling to achieve substantial scalability.
8. CONCLUSIONS
This paper explores a powerful, programmer-friendly SDN pro-
gramming model in which a programmer uses standard algorith-
mic programming to design arbitrary algorithms for SDN control.
We present novel techniques to address the scalability challenges
of algorithmic SDN programming transparently, and show that the
result is highly scalable on a variety of benchmarks using both sim-
ulated and real network workloads.
Acknowledgements: We thank Michael F. Nowlan, Lewen
Yu, Ramki Gummadi, Erran Li, Haibin Song, Chen Tian, SIG-
COMM reviewers, and Ion Stoica (shepherd) for suggestions. An-
dreas Voellmy was primarily supported by a gift from Futurewei,
and partially supported by NSF grant CNS-1017206. Paul Hudak
was supported in part by a gift from Microsoft Research.
9. REFERENCES
[1] https://openflow.stanford.edu/display/Beacon/Home.
[2] T. Benson, A. Akella, and D. A. Maltz. Network Trafﬁc Characteristics of Data
Centers in the Wild. In Proc. of IMC, 2010.
[3] Z. Cai, A. L. Cox, and T. S. Eugene Ng. Maestro: Balancing Fairness, Latency
and Throughput in the OpenFlow Control Plane. Technical report, Rice, 2011.
[4] M. Casado, T. Koponen, R. Ramanathan, and S. Shenker. Virtualizing the
Network Forwarding Plane. In Proc. of PRESTO, 2010.
[5] Cbench. Cbench, 2012. [Online; accessed 10-April-2012].
[6] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma, and
S. Banerjee. DevoFlow: Scaling Flow Management for High-Performance
Networks. In Proc. of SIGCOMM, 2011.
[7] N. Foster, R. Harrison, M. J. Freedman, C. Monsanto, J. Rexford, A. Story, and
D. Walker. Frenetic: a Net. Programming Language. In Proc. of ICFP, 2011.
[8] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McKeown, and
S. Shenker. NOX: Towards an Operating System for Networks. SIGCOMM
Comput. Commun. Rev., 2008.
[9] T. L. Hinrichs, N. S. Gude, M. Casado, J. C. Mitchell, and S. Shenker. Practical
Declarative Network Management. In Proc. of WREN, 2009.
[10] T. Koponen, M. Casado, N. Gude, J. Stribling, L. Poutievski, M. Zhu,
R. Ramanathan, Y. Iwata, H. Inoue, T. Hama, and S. Shenker. Onix: a
Distributed Control Platform for Large-scale Production Networks. In Proc. of
OSDI, 2010.
[11] S. Marlow, S. Peyton Jones, and S. Singh. Runtime Support for Multicore
Haskell. In Proc. of ICFP, 2009.
[12] B. Marsh, M. Scott, T. LeBlanc, and E. Markatos. First-class User-level
Threads. ACM SIGOPS Operating Systems Review, 1991.
[13] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson,
J. Rexford, S. Shenker, and J. Turner. OpenFlow: Enabling Innovation in
Campus Networks. SIGCOMM Comput. Commun. Rev., 2008.
[14] J. C. Mogul et al. DevoFlow: Cost-effective Flow Management for High
Performance Enterprise Networks. In Proc. of Hotnets, 2010.
[15] C. Monsanto, N. Foster, R. Harrison, and D. Walker. A Compiler and Run-time
System for Network Programming Languages. In Proc. of POPL, 2012.
[16] C. Monsanto, J. Reich, N. Foster, J. Rexford, and D. Walker. Composing
Software-Deﬁned Networks. In Proc. of NSDI, 2013.
[17] D. Mosberger and T. Jin. httperf: a Tool for Measuring Web Server
Performance. SIGMETRICS Perform. Eval. Rev., 1998.
[18] D. Shah and P. Gupta. Fast Updating Algo. for TCAMs. IEEE Micro, 2001.
[19] Simple network access control (SNAC).
http://www.openflow.org/wp/snac/.
[20] D. E. Taylor and J. S. Turner. Classbench: a Packet Classiﬁcation Benchmark.
IEEE/ACM Trans. Networking, 2007.
[21] A. Tootoonchian, S. Gorbunov, Y. Ganjali, M. Casado, and R. Sherwood. On
Controller Performance in Software-Deﬁned Networks. In Hot-ICE, 2012.
[22] A. Voellmy, B. Ford, P. Hudak, and Y. R. Yang. Scaling Software-deﬁned
Network Controllers on Multicore Servers. YaleCS TR1468, 2012.
[23] A. Voellmy and P. Hudak. Nettle: Taking the Sting Out of Programming
Network Routers. In Proc. of PADL, 2011.
(a) Throughput comparison
(b) Latency comparison
Figure 7: Throughput and latency of SDN controllers.
scales to less than 15 millions/second, and NOX-MT is only around
2 millions/second. Figure 7(b) shows the corresponding latency
CDF for all three systems. The median latency of Maple is 1 ms,
Beacon is almost 4 ms, and NOX-MT reaches as high as 17 ms.
The 95-percentile latency of Maple is still under 10 ms.
7. RELATED WORK
SDNs have motivated much recent work, which we classify into
basic SDN controllers, programming abstractions, ofﬂoading work
to switches, and controller scalability.
Basic SDN controllers: NOX [8] offers C++ and Python APIs for
raw event handling and switch control, while Beacon [1] offers a
similar API for Java. These APIs require the programmer to man-
age low-level Openﬂow state explicitly, such as switch-level rule
patterns, priorities, and timeouts. Maple derives this low-level state
from a high-level algorithmic policy expression.
SDN programming abstractions and languages: Maestro [3] raises
the abstraction level of SDN programming with modular network
state management using programmer-deﬁned views. SNAC [19]
and FML [9] offer high-level pattern languages for specifying se-
curity policies. Onix [10] introduces the NIB abstraction so that
applications modify ﬂow tables through reading and writing to the
key-value pairs stored in the NIB. Casado et al. [4] proposes net-
work virtualization abstraction. Frenetic [7], Pyretic [16] and Net-
tle [23] provide new languages for SDN programming. Frenetic’s
NetCore language supports specialized forms of composition, such
as between statistics-gathering and control rules. In contrast, Maple
is agnostic to the language for expressing policies, and beneﬁts
from whatever features (e.g., composition) the language offers.
Ofﬂoading work to switches: Devoﬂow [6] increases scalability
by refactoring the Openﬂow API, reducing the coupling between
centralized control and centralized visibility. Frenetic leverages its
NetCore language to compile rules for switch ﬂow tables, alleviat-
ing the complex challenge of managing ﬂow tables [7, 15]. Maple
similarly compiles to switch ﬂow tables, but its tracing approach
supports generic algorithms expressed in arbitrary languages. Also,
98