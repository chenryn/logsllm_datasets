evaluation is triggered and the timer is reset.  
Assertion  Evaluation  uses AWS  APIs and  third  party 
cloud monitors, such as Edda9, to check the online status of 
cloud resources, and configuration repositories to check the 
configuration values. We added a layer on top of AWS API 
to deal with the issue of eventual consistency. We provide a 
set of pre-defined assertions to check cloud resources, which 
operators  can  use  directly.  We  also  provide  an  assertion 
library, which analyst can use to link their assertions with the 
operation processes.  
There  are  two  types  of  assertions.  High-level  assertions 
check the status of the overall system, for example, ‚Äúassert 
the  system  has  at  least  M instance  with  the  new  version‚Äù. 
Low-level assertions check the status of a specific node. The 
analyst  decides  whether  to  use  high-level  assertions,  which 
takes longer time to diagnose if failure occurs, or use low-
level  assertions,  which  introduce  more  overhead.  There  are 
two scenarios in which we use the low-level assertions: (i) to 
double-check  the  results  if  the  log  indicates  acknowledged 
success  of  a  certain  API  call,  and  (ii)  to  check  for  subtle 
errors at the application level or in the configuration.  
4)  Error Diagnosis 
When  an  assertion  evaluation  fails,  a  process  non-
conformance  is  detected,  or  an  error/failure  is  reported  by 
other  monitoring  systems,  Error  Diagnosis  is  triggered  to 
diagnose the causes at runtime.  
Error  Diagnosis  searches  for  the  root  causes  when  a 
failure  occurs.  The  errors/failures  could  be  caused  by  the 
operations process itself, the cloud infrastructure, co-located 
applications  or  other 
simultaneous  operations.  The 
contribution of our diagnosis comes largely from on-demand 
assertions  defined  for  each  type  of  error/failure.  Other 
diagnostic  information  can  come  from  third  party  monitors 
or  configuration  repositories,  which  may  provide  data  on 
who changed the configuration, when, and why.  
Since  our  POD-Diagnosis  is  oriented  towards  Cloud 
operations, the root causes consequently refer to those events 
in  operation  levels,  such  as  exceptions,  misconfigurations, 
system call fails, etc.  From the root causes to the potential 
errors  detected  by  the  error  detection,  there  are  a  large 
number  of  intermediate  events  that  connect  root  causes  to 
errors.  For  an  efficient  diagnosis,  all  the  events  and  the 
dependencies among them should be well structured. Further, 
the events including possible failures/errors, their associated 
potential  faults,  and  on-demand  assertions  can  be  naturally 
organized  into  tree-like  structures.  Thus,  we  created  fault 
trees to serve as a reference model for both robust operations 
9 Edda ‚Äì https://github.com/Netflix/edda  
256256256
Figure 5. Part of a Fault Tree 
instance ready‚Ä¶ . If the assertion after New instance ready‚Ä¶ 
triggered diagnosis, we prune all other sub-trees. 
design and error diagnosis. In contrast to traditional fault tree 
analysis  (FTA)  for  hardware  architectures,  the  fault  trees 
here  are  constructed  from  and  based  on  application  system 
functions  and  knowledge  of  their  possible  faults.  Note  that 
the  fault  trees  are  not  employed  for  FTA;  instead  we  use 
them to structure data in a repository. 
Figure  5  shows  part  of  a  fault  tree  derived  from  the 
failure  of  assertion  ‚Äúassert  the  system  has N  instances  with 
the  new  version‚Äù. There  is  one  fault  tree  per  assertion. As 
can be seen in the figure, there are some variables in the tree.
When the Error Diagnosis is triggered, we firstly select the 
correct  tree(s)  according  to  the  assertion  that  triggered  the 
diagnosis. Secondly we instantiate the variables in these trees 
with  the  parameters  from  the  runtime  request,  e.g.  the 
number  of  nodes  (N).  Then  the  associated  process  context 
from  the  request  is  used  to  prune  sub-trees  that  are  not 
relevant in that process context. For example, in Figure 5, the 
left-most sub-tree (below Create LC‚Äô fails) is associated with 
the  step  Update  launch  configuration  (see  Figure  2)  as 
process  context,  and  the  right-most  sub-tree  with  step  New 
257257257
tests 
the  corresponding  diagnostic 
After selecting and instantiating the context-specific sub-
trees, 
(on-demand 
assertion  evaluation  /  consulting  monitoring/repository)  are 
determined by visiting these sub-trees in a top-down manner, 
so as to confirm or exclude potential faults. If the check at a 
particular  node  has  already  been  done,  e.g.  for  an  ancestor 
node,  the diagnosis results  are  reused.  The  diagnostic  tests
that  are  actually  performed  are  highly  dynamic  and 
compositional depending on the current situation. If the test 
for a current node shows the related error is not present, we 
proceed elsewhere (and prune the respective sub-tree); if the 
error is present, we visit the child nodes. Diagnosis stops at 
the point where no further child nodes can be checked, e.g. 
when  an  instance  was  terminated,  but  the  diagnosis  cannot 
determine why. Any faults from descendant nodes below the 
deepest successful error tests might be the root causes. The 
partial  log  below  shows  an  example  result  from  diagnosis. 
The corresponding sub-tree is highlighted by the dashed box 
in  Figure  5.  In  this  particular  run,  the  assertion  that  a  new 
instance uses the correct version failed, because the launched 
instance is based on the wrong AMI.  
 )))
,5346*44*4< 44(7;(34'433- ,  - ,
   -
, !- 
     ! ( 

 *    "   ! # ) 7 !!
"! !!)))
,5346*44*4< 44(7;(34'434- ,  - ,
   -
, !- ,  !- & !  "!& "
 !!!* 
,5346*44*4< 44(7;(34'4;:- ,  - ,
   -
, !- ,  !-  !  "!& "
 !!( 
  *    "   !  "!&
")4+7"! %"
,5346*44*4< 44(7;(34'4;;- ,  - ,
   -
, !- ,  !- & ! &   !!
!* 
,5346*44*4< 44(7;(34'595- ,  - ,
   -
, !- ,  !-  ! &   !!(
*  " !&)5+7"! 
%"
,5346*44*4< 44(7;(34'596- ,  - ,
   -
, !-,  !-&! !!!
* 
,5346*44*4< 44(7;(34'666- ,  - ,
   -
, !- ,  !-  #!  ! 
 !!(
*  " $)
,5346*44*4< 44(7;(34'666- ,  - ,
   -
, !-	!"  !
))) 
At  this  stage,  the  order  in  which  potential  faults  are 
examined  is  determined  by  the  fault  probability.  Another 
option  would  be  to  consider  the  expected  time/cost  of  the 
diagnostic tests when determining the test order.  
C.  Discussion of Effort 
The  effort  on  model  discovery, 
log  annotation 
configuration,  assertion  specification  and  fault  tree  creation 
only needs to be spent only once for an operation tool, such 
as Asgard. The effort could thus be spent once by the vendor 
/ open-source community. An individual organization using 
Asgard  to  perform  their  rolling  upgrade  does  not  need  any 
additional effort to benefit from our system. In particular, no 
effort is required per rolling upgrade execution.  
Even  though  the  effort  is  once-off,  the  process  model 
creation  part  (and  associated  regular  expressions  for 
triggering  assertions)  is  highly  automated  using  process 
mining techniques [2]. The effort should be in the order of a 
few  hours  or  less,  given  suitable logs  are  readily  available. 
Evolution of the process model can be achieved by amending 
the  set  of  training  data  with  more  logs  as  they  become 
available. As such, the model can be extended or changed to 
fit  the  amended  data.  Model  building  is  essentially  an 
orthogonal  issue  and  this  paper  demonstrated  how  having 
such a model improves dependability. 
Although  the  implementation  (discussed  Section  IV)  is 
specific  to  the  rolling  upgrade  process  provided  by  Netflix 
Asgard,  the  fault  trees  form  a  valuable  knowledge  base 
reusable in any sporadic operations using the cloud API and 
in  other  diagnosis  situation  (sporadic  or  not).  Conformance 
Checking is purely automatic, given the process model. This 
shows the approach is generalizable to other operations. 
258258258
IV. 
IMPLEMENTATION  
Our  log  processors  are  based  on  Logstash  ‚Äì  an  open 
source tool for managing events and logs. The log processors 
use  regular  expressions  to  match  log  lines,  and  process  the 
matched log lines. The code snippet below gives an example 
of  the  Logstash  entries  of  a  log  line  from  the  original 
operation  process  with  our  process  context  tags  ‚Äì  note  that 
2   contains the original log line.  
.12 "1(1 )1'12! 1(,1" 1'1 1'1 !71-'
12 1(.1!1(,15346*43*57
11
(,1*:83<71-'
11(,11-'
1 !1
1"1(,171-/'12! !1(15346*43
12 "0 !1(1
)1'12 "0!1(1 )
1'12  1(1,5346*43*57 44(74(7;'645- ,
 (
" 
*:83<7
-
)!%) )
     5346*43*57044(74(7; <3;(
.
!( "/ . ( "/ .!(  !
3(3(3(3(3(3(3(4=3/ .( * "! !*5/ ,
" 
*:83<7!"** - !
 *:67374   &  " ) 7  7  !
" )1'12!&1(1 1/
44(74(7;'
6451-'
1 1(,1** 1-'
(,1*:673741-'
57
33(74(7;);881'
** 

!
"

Conformance Checking, Assertion Evaluation and Error 
Diagnosis are implemented as RESTful Web Services, based 
on  RESTlet10 ‚Äî  a  RESTful  Web  API  framework  for  Java. 
The  process  model  is  provided  to  the  services  up-front.  At 
the  moment,  assertion  evaluations  are  implemented  in  a 
combination of Java and scripts, depending on what is to be 
asserted.  The  pre-defined  assertions  are  developed  by  us, 
specific to Asgard, and are based on our understanding. 
The  log  line  below  gives  an  example  of  the  assertion 
result  produced  by  the  assertion  evaluation,  which  was 
triggered by the log line shown above. 
.12 "1(1  !*#"!)1'12! 1(,1" 
** 1'1 !71-'12 1(.1!1( ,15346*43*57 44(
75(33'64<1-'1! 1(,1 "   ** 1-'1 ! !
1(,1 !71-/'12! !1(15346*43*57
33(75(33)64<1'
12 "0 !1(1
)1'12 "0!1(1 !!
)1'12  1(1,5346*43*57
44(75(33'64<-
,  !-,
 (" ** -,!( !7-*
*  7 ! )1'12!&1(1  !1/
To  be  resilient  against  AWS  API  inconsistency  [8],  we 
also implemented a consistent AWS API layer. This includes 
an exponential retry mechanism: if the supposed status of a 
specific cloud resource is different from our expectation we 
retry  the  respective  AWS  API  calls  automatically.  We  also 
introduce an API timeout mechanism: assertion evaluations 
are regarded as failed if API calls time out. Timeout values 
are set based on experiments, at the 95% percentile. 
V.  EVALUATION 
In  order  to  evaluate  the  POD-Diagnosis  approach,  we 
injected real faults in a realistic environment. In this section, 
we  report  the  diagnosis  time,  precision  and  recall  of  error 
detection,  and  accuracy  of  root  cause  diagnosis.  The  next 
section discusses the results and some general limitations. 
10 RESTlet ‚Äì http://restlet.org  
A.  Evaluation Methodology 
As  our  approach  is  specifically  designed  for  sporadic 
operations (rather than normal operations), it would be unfair 
to compare it with diagnosis approaches designed for normal 
operations. The current practice in industry is to disable most 
systems  for  monitoring,  automated 
log  analysis,  and 
automated  diagnosis  during  a  sporadic  operation. 
If 
something goes wrong, the sporadic operation is rolled back 
and  an  operator  conducts  a  manual  diagnosis  by  running 
tools  from  a  command  line  interface  and  checking  various 
logs. To some degree, POD-Diagnosis automates this manual 
diagnosis. Our diagnosis time is within seconds, so it would 
also  not  be  appropriate  to  compare  with  manual  diagnosis, 
which may take minutes at least.  
Our  experiment  is  running  on  AWS.  We  performed 
rolling  upgrade  on  clusters  with  4  or  20  instances.  Netflix 
Asgard is chosen to assist the rolling upgrades. We injected 8 
different  types  of  faults  into  the  clusters.  Those  faults  are 
reported  by  industry  outage  reports  or  experienced  by  us 
during rolling upgrade. These faults are representative.  
For each of the 8 faults, we conducted 20 runs (i.e., 160 
runs  in  total).  We  also  injected  simultaneous  operations 
(such as legitimate scaling in/out or changes to instances) to 
confound  our  diagnosis.  Among  the  20  runs,  we  mixed  a 
combination  of  different  simultaneous  operations.  The 
operations  were  run  in  an  AWS  account  shared  with  an 
external  independent  research  team  whose  use  may  also 
introduce some confounding factors, such as desired instance 
number, launch configuration, and call limits imposed on a 
specific region of a single account.  
B.  Experiment Setup 
the  rolling  upgrade 
The  application  on  which 