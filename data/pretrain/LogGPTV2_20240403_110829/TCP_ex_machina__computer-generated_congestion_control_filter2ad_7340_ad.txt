measuring throughput takes some care. We deﬁne the through-
put of a pair as follows. Suppose the pair is active during (non-
overlapping) time intervals of length t1,t2, . . . during the entire sim-
ulation run of T seconds. If in each interval the protocol success-
fully receives si bytes, we deﬁne the throughput for this connection
as ∑si/∑ti.
We are interested in the end-to-end delay as well; the reasoning
behind Remy’s objective function and the δ parameter is that proto-
cols that ﬁll up buffers to maximize throughput are not as desirable
as ones that achieve high throughput and low delay — both for their
Figure 3: Observed Internet ﬂow length distribution matches
a Pareto (α = 0.5) distribution, suggesting mean is not well-
deﬁned.
Quantity
n max senders
“on” process
“off” process
link speed
link speed
round-trip time
queue capacity
Design range
2
mean 5 sec
mean 5 sec
15 Mbps (“1×”)
4.7–47 Mbps (“10×”)
150 ms
unlimited
Distribution
uniform
exponential
exponential
exact
uniform
exact
In most experiments, all the sources run the same protocol; in
some, we pick different protocols for different sources to investi-
gate how well they co-exist. Each simulation run is generally 100
seconds long, with each scenario run at least 128 times to collect
summary statistics.
Workloads. Each source is either “on” or “off” at any point in
time. In the evaluation, we modeled the “off” times as exponen-
tially distributed, and the “on” distribution in one of three different
ways:
• by time, where the source sends as many bytes as the
congestion-control protocol allows, for a duration of time
picked from an exponential distribution,
• by bytes, where the connection sends as many bytes as given
by an exponential distribution of a given average and shape,
and
• by empirical distribution, using the ﬂow-length CDF from
a large trace captured in March 2012 and published re-
cently [4]. The ﬂow-length CDF matches a Pareto distribu-
tion with the parameters given in Figure 3, suggesting that
the underlying distribution does not have ﬁnite mean. In our
evaluation, we add 16 kilobytes to each sampled value to en-
sure that the network is loaded.
Topologies. We used these topologies in our experiments:
1. Single bottleneck (“dumbbell”): The situation in Figure 2,
with a 1,000-packet buffer, as might be seen in a shared cable-
modem uplink. We tested a conﬁguration whose link speed
and delay were within the RemyCC design ranges:
00.20.40.60.811001000100001000001e+061e+07Cumulative distributionFlow length (bytes)Flow length CDF (Allman 2013; ICSI)Pareto(x+40) [ Xm = 147, alpha = 0.5 ]129Figure 4: Results for each of the schemes over a 15 Mbps dumb-
bell topology with n = 8 senders, each alternating between ﬂows
of exponentially-distributed byte length (mean 100 kilobytes)
and exponentially-distributed off time (mean 0.5 s). Medians
and 1-σ ellipses are shown. The blue line represents the efﬁ-
cient frontier, which here is deﬁned entirely by the RemyCCs.
Figure 5: Results for the dumbbell topology with n = 12
senders, each alternating between ﬂows whose length is drawn
from the ICSI trace (Fig. 3) and exponentially-distributed off
time (mean = 0.2 s). Because of the high variance of the sending
distribution, 1
2 -σ ellipses are down. The RemyCCs again mark
the efﬁcient frontier.
in producing a family of congestion-control algorithms for this type
of network.
effect on the user, who may prefer to get his packets to the receiver
sooner, as well as any other users who share the same FIFO queue.
We present the results for the different protocols as throughput-
delay plots, where the log-scale x-axis is the queueing delay (aver-
age per-packet delay in excess of minimum RTT). Lower, better, de-
lays are to the right. The y-axis is the throughput. Protocols on the
“top right” are the best on such plots. We take each individual 100-
second run from a simulation as one point, and then compute the
1-σ elliptic contour of the maximum-likelihood 2D Gaussian dis-
tribution that explains the points. To summarize the whole scheme,
we plot the median per-sender throughput and queueing delay as a
circle.
Ellipses that are narrower in the throughput or delay axis corre-
spond to protocols that are fairer and more consistent in allocating
those quantities. Protocols with large ellipses — where identically-
positioned users differ widely in experience based on the luck of
the draw or the timing of their entry to the network — are less fair.
The orientation of an ellipse represents the covariance between the
throughput and delay measured for the protocol; if the throughput
were uncorrelated with the queueing delay (note that we show the
queueing delay, not the RTT), the ellipse’s axes would be parallel
to the graph’s. Because of the variability and correlations between
these quantities in practice, we believe that such throughput-delay
plots are an instructive way to evaluate congestion-control proto-
cols; they provide more information than simply reporting mean
throughput and delay values.
5.2 Single Bottleneck Results
We start by investigating performance over the simple, classic
single-bottleneck “dumbbell” topology. Although it does not model
the richness of real-world network paths, the dumbbell is a valuable
topology to investigate because in practice there are many single-
bottleneck paths experienced by Internet ﬂows.
Recall that this particular dumbbell link had most of its param-
eters found inside the limits of the design range of the RemyCCs
tested. As desired, this test demonstrates that Remy was successful
Results from the 8-sender and 12-sender cases are shown in Fig-
ures 4 and 5. RemyCCs are shown in light blue; the results demon-
strate the effect of the δ parameter in weighting the cost of delay.
When δ = 0.1, RemyCC senders achieve greater median through-
put than those of any other scheme, and the lowest delay (other
than the two other RemyCCs). As δ increases, the RemyCCs trace
out an achievability frontier of the compromise between throughput
and delay. In this experiment, the computer-generated algorithms
outperformed all the human-designed ones.
to left and bottom to top,
the end-to-end TCP
congestion-control schemes trace out a path from most delay-
conscious (Vegas) to most throughput-conscious (Cubic), with
NewReno and Compound falling in between.
From right
The schemes that require in-network assistance (XCP and Cubic-
over-sfqCoDel, shown in green) achieve higher throughput than
the TCPs, but less than the two more throughput-conscious Remy-
CCs.5 This result is encouraging, because it suggests that even
a purely end-to-end scheme can outperform well-designed algo-
rithms that involve active router participation. This demonstrates
that distributed congestion-control algorithms that explicitly max-
imize well-chosen objective functions can achieve gains over ex-
isting schemes. As we will see later, however, this substantially
better performance will not hold when the design assumptions of a
RemyCC are contradicted at runtime.
5It may seem surprising that sfqCoDel, compared with DropTail,
increased the median RTT of TCP Cubic. CoDel drops a packet at
the front of the queue if all packets in the past 100 ms experienced a
queueing delay (sojourn time) of at least 5 ms. For this experiment,
the transfer lengths are only 100 kilobytes; with a 500 ms “off”
time, such a persistent queue is less common even though the mean
queueing delay is a lot more than 5 ms. DropTail experiences more
losses, so has lower delays (the maximum queue size is ≈ 4× the
bandwidth-delay product), but also lower throughput than CoDel.
In other experiments with longer transfers, Cubic did experience
lower delays when run over sfqCoDel instead of DropTail.
0.40.60.811.21.41.61.812481632Throughput (Mbps)Queueing delay (ms)VegasRemyδ=0.1Remyδ=1Remyδ=10CubicCompoundNewRenoXCPCubic/sfqCoDelBetter00.20.40.60.811.21.41.61248163264Throughput (Mbps)Queueing delay (ms)VegasRemyδ=0.1Remyδ=1Remyδ=10CubicCompoundNewRenoXCPCubic/sfqCoDelBetter130Figure 6: Sequence plot of a RemyCC ﬂow in contention with
varying cross trafﬁc. The ﬂow responds quickly to the depar-
ture of a competing ﬂow by doubling its sending rate.
In Figures 4 and 5, the RemyCCs do not simply have better me-
dian performance — they are also more fair to individual ﬂows, in
that the performance of an individual sender (indicated by the size
of the ellipses) is more consistent in both throughput and delay.
To explain this result, we investigated how multiple RemyCC
ﬂows share the network. We found that when a new ﬂow starts, the
system converges to an equitable allocation quickly, generally after
little more than one RTT. Figure 6 shows the sequence of transmis-
sions of a new RemyCC ﬂow that begins while sharing the link.
Midway through the ﬂow, the competing trafﬁc departs, allowing
the ﬂow to start consuming the whole bottleneck rate.
5.3 Cellular Wireless Links
Cellular wireless links are tricky for congestion-control algo-
rithms because their link rates vary with time.6
By running a program that attempts to keep a cellular link back-
logged but without causing buffer overﬂows, we measured the vari-
ation in download speed on Verizon’s and AT&T’s LTE service
while mobile. We then ran simulations over these pre-recorded
traces, with the assumption that packets are enqueued by the net-
work until they can be dequeued and delivered at the same instants
seen in the trace.
As discussed above, we did not design the RemyCCs to accom-
modate such a wide variety of throughputs. Running the algorithm
over this link illustrated some of the limits of a RemyCC’s general-
izability beyond situations encountered during the design phase.
Somewhat to our surprise, for moderate numbers of concurrent
ﬂows, n ≤ 8, the RemyCCs continued to surpass (albeit narrowly)
the best human-designed algorithms, even ones beneﬁting from in-
network assistance. See Figures 7 and 8.
5.4 Differing RTTs
We investigated how the RemyCCs allocate throughput on a con-
tested bottleneck link when the competing ﬂows have different
RTTs. At the design stage, all contending ﬂows had the same RTT
(which was drawn randomly for each network specimen from be-
tween 100 ms and 200 ms), so the RemyCCs were not designed to
exhibit RTT fairness explicitly.
We compared the RemyCCs with Cubic-over-sfqCoDel by run-
ning 128 realizations of a four-sender simulation where one sender-
receiver pair had RTT of 50 ms, one had 100 ms, one 150 ms, and
6XCP, in particular, depends on knowing the speed of the link ex-
actly; in our tests on cellular traces we supplied XCP with the long-
term average link speed for this value.
Figure 7: Verizon LTE downlink trace, n = 4. 1-σ ellipses are
shown. The RemyCCs deﬁne the efﬁcient frontier. Senders al-
ternated between exponentially-distributed ﬁle transfers (mean
100 kilobytes) and exponentially-distributed pause times (mean
0.5 s).
Figure 8: Verizon LTE downlink trace, n = 8.
1-σ el-
lipses are shown. As the degree of multiplexing increases,
the schemes move closer together in performance and router-
assisted schemes begin to perform better. Two of the three Re-
myCCs are on the efﬁcient frontier.
sequence numbertime1/2 link speedlink speedRemyCC starts withone competing ﬂow,sending at half oflink speed....about one RTT later,RemyCC begins sendingat full link speed.Competing ﬂow stops, and...11.522.5381632Throughput (Mbps)Queueing delay (ms)VegasRemyδ=0.1Remyδ=1Remyδ=10CubicCompoundNewRenoXCPCubic/sfqCoDel0.811.21.41.61.82163264Throughput (Mbps)Queueing delay (ms)VegasRemyδ=0.1Remyδ=1Remyδ=10CubicCompoundNewRenoXCPCubic/sfqCoDel131one 200 ms. The RemyCCs did exhibit RTT unfairness, but more
modestly than Cubic-over-sfqCoDel (Fig. 10).
5.5 Datacenter-like topology
We simulated 64 connections sharing a 10 Gbps datacenter link,
and compared DCTCP [2] (using AQM inside the network) against
a RemyCC with a 1000-packet tail-drop queue. The RTT of the
path in the absence of queueing was 4 ms. Each sender sent 20
megabytes on average (exponentially distributed) with an “off”
time between its connections exponentially distributed with mean
100 milliseconds.
We used Remy to design a congestion-control algorithm to maxi-
mize −1/throughput (minimum potential delay) over these network
parameters, with the degree of multiplexing assumed to have been
drawn uniformly between 1 and 64.
The results for the mean and median throughput (tput) for the 20
megabyte transfers are shown in the following table:
DCTCP (ECN)
RemyCC (DropTail)
tput: mean, med
179, 144 Mbps
175, 158 Mbps
rtt: mean, med
7.5, 6.4 ms
34, 39 ms
Figure 9: AT&T LTE downlink trace, n = 4. Two of the Remy-
CCs are on the efﬁcient frontier.
These results show that a RemyCC trained for the datacenter-
network parameter range achieves comparable throughput at lower
variance than DCTCP, a published and deployed protocol for sim-
ilar scenarios. The per-packet latencies (and loss rates, not shown)
are higher, because in this experiment RemyCC operates over a
DropTail bottleneck router, whereas DCTCP runs over an ECN-
enabled RED gateway that marks packets when the instantaneous
queue exceeds a certain threshold. Developing RemyCC schemes
for networks with ECN and AQM is an area for future work.
5.6 Competing protocols
We investigated the possibility of incremental deployment of a
RemyCC, by simulating a single bottleneck link with one RemyCC
ﬂow contending with one ﬂow from either Compound or Cubic,
with no active queue management. The RemyCC was designed for
round-trip-times between 100 ms and 10 s, in order to accommodate
a “buffer-ﬁlling” competitor on the same bottleneck link.
We used the same observed trafﬁc distribution from Figure 3
and varied the mean “off” time (exponentially distributed) of the
senders. The bottleneck link speed was 15 Mbps and baseline RTT
was 150 ms. We also experimented with ﬂows of mean sizes 100
kilobytes and 1 megabyte, with an exponentially distributed mean
“off” time of 0.5 seconds between successive ﬂows.
The results, shown in the two tables below, depended on the
duty cycle of the senders dictated by the mean off time (numbers
in parentheses are standard deviations).
Figure 10: Remy’s RTT unfairness compares favorably to
Cubic-over-sfqCoDel. Error bar represents standard error of
the mean over 128 100-second simulations.
Mean off time RemyCC tput
2.12 (.11) Mbps
200 ms
100
2.18 (.08)
2.28 (.10)
10
RemyCC tput
Mean size
2.04 (.14)