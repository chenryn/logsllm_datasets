≤ 2
0.426
0.448
0.481
0.468
0.558
0.563
0.732
0.742
0.751
0.767
0.887
0.899
0.921
0.908
0.926
0.933
0.462
0.467
0.473
0.471
0.433
0.435
0.730
0.725
0.708
0.710
0.778
0.804
0.903
0.913
0.906
0.891
> 3
0.398
0.449
0.555
0.718
0.730
0.870
0.901
0.915
0.473
0.481
0.446
0.724
0.713
0.757
0.890
0.882
(a) TextCNN on Abuse
(b) BiLSTM on Abuse
(c) TextCNN on Porn
(d) BiLSTM on Porn
Figure 4: The comparison of classiﬁcation conﬁdence on user crafted obfuscated texts.
Table 5: The error correction performance on user generated
obfuscated texts.
Table 6: The performance of transfer attack against the target
models.
Method
Abuse Detection
Porn Detection
Model
WER BLEU
0.570
0.292
Baseline
0.319
0.568
Pycorrector
0.618
TextCorrector
0.291
Adversarial NMT 0.122
0.796
SS WER BLEU
0.478
0.878
0.858
0.563
0.626
0.875
0.913
0.741
0.337
0.349
0.304
0.197
SS
0.866
0.840
0.860
0.918
TextCNN + Pycorrector
TextCNN + TextCorrector
TextCNN + EMF
TextCNN + IMF
TextCNN + NMT
TextCNN + EMF + NMT
TextCNN + IMF + NMT
BiLSTM + Pycorrector
BiLSTM + TextCorrector
BiLSTM + EMF
BiLSTM + IMF
BiLSTM + NMT
BiLSTM + EMF + NMT
BiLSTM + IMF + NMT
Abuse Detection
Porn Detection
Ori Accuracy ASR Ori Accuracy ASR
0.796
0.609
0.321
0.323
0.170
0.118
0.090
0.776
0.943
0.210
0.276
0.226
0.067
0.093
0.926
0.979
0.935
0.974
0.998
0.936
0.974
0.901
0.974
0.938
0.927
0.989
0.938
0.928
0.749
0.929
0.354
0.371
0.219
0.111
0.088
0.857
0.732
0.392
0.304
0.238
0.075
0.113
0.906
0.978
0.951
0.972
0.994
0.951
0.972
0.874
0.975
0.947
0.958
0.991
0.948
0.957
sarial perturbations instead of common spelling errors. In
contrast, the adversarial NMT model performs well in restor-
ing the real-world adversarial perturbation. For instance, it
decreases WER by 17% and 14% for abusive texts and porno-
graphic texts, respectively. Hence, it can be concluded that
the end-to-end adversarial NMT is more elastic, practical and
effective in the real-world adversarial scenario.
We also visualize the proportion of different bugs in obfus-
cated texts and the translations of the adversarial NMT model
in Fig. 5 to further analyze its robustness against different
(a) Abuse
(b) Porn
Figure 5: The proportion of different bugs in obfuscated texts
and the texts translated by adversarial NMT.
spelling correction methods are shown in Table 5. The base-
line represents the original difference between obfuscated
texts and the manually restored reference texts. From Table 5,
we can see that Pycorrector has negative impact on error cor-
rection, which conﬁrms the above speculation that it is also
sensitive to adversarial perturbations. TextCorrector also does
not work well when presented with carefully crafted adver-
USENIX Association
29th USENIX Security Symposium    1389
0.00.20.40.60.81.0Confidence0.00.20.40.60.81.0Confidence0.00.20.40.60.81.0Confidence0.00.20.40.60.81.0ConfidenceInsertPyConvertSplitSim2TradGlyphSimPhoneticSim0.00.10.20.30.40.50.60.70.8ProportionInsertPyConvertSplitSim2TradGlyphSimPhoneticSim0.00.10.20.30.40.50.60.70.8ProportionTable 7: The performance of adaptive attack against all the target models.
Model
Common TextCNN
TextCNN + Pycorrector
TextCNN + TextCorrector
TextCNN + EMF
TextCNN + IMF
TextCNN + NMT
TextCNN + EMF + NMT
TextCNN + IMF + NMT
Common BiLSTM
BiLSTM + Pycorrector
BiLSTM + TextCorrector
BiLSTM + EMF
BiLSTM + IMF
BiLSTM + NMT
BiLSTM + EMF + NMT
BiLSTM + IMF + NMT
ASR
0.860
0.830
0.786
0.687
0.622
0.375
0.240
0.219
0.891
0.872
0.866
0.726
0.555
0.450
0.268
0.238
Abuse Detection
Perturbed Word
2.19
1.91
2.03
2.35
2.32
2.05
2.00
1.93
1.87
1.68
1.83
1.97
1.87
1.93
1.85
1.73
Query
65.8
61.9
66.3
69.2
68.5
63.7
63.9
62.7
61.7
58.7
59.5
63.8
62.0
62.5
62.2
60.2
Porn Detection
Perturbed Word
2.12
2.01
2.13
2.02
2.18
2.34
2.15
2.03
2.11
1.75
1.95
2.12
2.14
2.20
2.03
1.80
Query
61.1
59.4
60.4
58.9
61.7
64.3
60.8
59.4
61.3
55.9
60.9
61.6
61.8
62.7
60.3
55.7
ASR
0.839
0.823
0.773
0.706
0.595
0.428
0.339
0.236
0.846
0.835
0.821
0.548
0.550
0.548
0.247
0.289
(a) TextCNN on Abuse
(b) BiLSTM on Abuse
(c) TextCNN on Porn
(d) BiLSTM on Porn
Figure 6: The original text length distribution of the successfully generated adversarial texts.
Table 8: Utility evaluation of successful adversarial texts.
Model
Common TextCNN
TextCNN + Pycorrector
TextCNN + TextCorrector
TextCNN + EMF
TextCNN + IMF
TextCNN + NMT
TextCNN + EMF + NMT
TextCNN + IMF + NMT
Common BiLSTM
BiLSTM + Pycorrector
BiLSTM + TextCorrector
BiLSTM + EMF
BiLSTM + IMF
BiLSTM + NMT
BiLSTM + EMF + NMT
BiLSTM + IMF + NMT
Abuse Detection
ED
SS
0.903
8.23
0.903
8.23
0.892
8.48
0.883
8.84
0.887
8.60
8.05
0.890
0.891
9.18
0.900
8.43
0.907
8.31
0.910