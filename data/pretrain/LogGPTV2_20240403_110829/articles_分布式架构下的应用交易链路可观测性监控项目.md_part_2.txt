```{=html}
```
1.  项目的主要技术内容
```{=html}
```
A.  采用的技术原理
分布式链路追踪就是将一次分布式请求还原成调用链路，将一次分布式请求的调用情况集中展示，比如各个服务节点上的耗时、请求具体到达哪台机器上、每个服务节点的请求状态等等。
交易链路可观测监控系统基于业务-服务-接口-设备四层维度，将指标数据、日志数据、trace数据统一采集，后端提供数据海量数据的标准化存储，采用Flink流计算平台及自研搜索引擎Beaver，监控系统支持灵活的扩展统计分析视图，以形成各类有效的可观察性视图。
交易链路可观测性监控系统对应业务架构图如下：
![](media/image1.png){width="5.766666666666667in"
height="2.392361111111111in"}
-   数据采集：采集端采用HekaAgent，支持Windows、Linux、AIX等多种操作系统，可以通过界面配置直接采集文件、性能指标、收集Syslog。数据收集完成后进入到数据接收模块Collector，并实时写入Kafka消息队列中，通过流处理引擎Logriver做ETL处理后写入Beaver引擎。根据采集数据匹配数据模型，在交易链路可观测性监控系统中生成全链路拓扑信息。
```{=html}
```
-   业务：在业务维度梳理系统当前状态，展示业务概况、业务详情，通过观察到的异常趋势深入探究业务详情，并通过业务拓扑图发现调用服务之间的关系与具体状态。业务拓扑同时支持历史回溯、服务详情、接口详情、查看具体异常请求等功能，快速完成溯源分析。对每种业务交易，每种渠道来源的请求量、耗时、成功率、错误数
    4大黄金指标都可进行多维度监控，并计算业务系统的健康度得分。同时可从拓扑图、时间轴、指标趋势展示单笔业务详情。
-   服务：从服务维度梳理系统当前状态，从指标趋势和接口分析展示服务概况、服务详情，支持下钻到关联设备及调用链信息。同时可自动统计每种交易类型/渠道类型的服务请求交易量、耗时、成功率、错误数。
-   设备：蜂窝状视图可清晰展现设备关键信息，不同的颜色代表设备不同的健康状态。点击详情可进一步了解设备的相关信息、性能指标和相关服务。支持对设备及字段信息过滤筛选，或根据分组字段对设备进行分组。
-   指标探索：对指标时序数据的分析与可视化，支持聚合、时移及拆分，获得更深入的分析图表。同时可实现对时序数据的单指标多维度（平均值、最大值、最小值等），多指标多维度查询、分析、可视化。
-   全链路：提供链路追踪查询功能，可以使用业务，服务，接口，Local
    IP，Remote
    IP，traceID，耗时，请求结果等多种字段对调用链进行过滤。分布式架构下交易链路可观测性监控系统涉及多个系统节点，可使用全局流水号来进行全链路跟踪，方便日志跟踪和问题查询。调用链详情展示每个请求的耗时，并能快速跳转至具体日志。链路追踪记录单个请求的完整处理流程，为分布式应用开发和运维人员提供完整的调用请求统计，请求调用链路和应用依赖性分析，可以帮助开发和运维人员迅速分析分布式架构下的应用性能和稳定性异常等问题。
B.  关键技术及创新点
```{=html}
```
1.  全链路追踪技术
全链路追踪（Tracing
Analysis）为分布式应用提供了完整的调用链路还原、调用请求量统计、链路拓扑、应用依赖分析等工具，帮助分析和诊断分布式应用架构下的性能瓶颈，提高微服务时代下的开发诊断效率。
全链路跟踪系统通过使用客户端Agent的方式来采集各种应用程序的使用和接口调用信息，输出日志到分析中心，最后在分析中心进行统计分析和监控告警。链路追踪的主要工作流程为客户侧的应用程序通过集成链路追踪的多语言客户端SDK上报服务调用数据；数据上报至链路追踪控制台后，链路追踪组件进行实时聚合计算和持久化，形成链路明细、性能总览、实时拓扑等监控数据。在海量日志数据中，提供数据分组同时为多个业务请求构建调用链。具体过程如下：构建一个调用链，获取与至少一个请求相对应的日志数据，并校准日志数据中的时间戳信息。根据时间戳信息，对日志数据进行滚动时间分桶，并对各时间桶中的日志数据进行业务聚类处理，根据各聚类中的日志数据，构建时间连通图并获取事件关键路径，按照预设的路径合并规则，对各时间桶中的事件关键路径进行合并，得到与各请求对应的调用链。
本项目所用全链路追踪技术，支持我行现有的业务系统架构，随着业务量的不断增加，加入支持服务化的多视角拓扑分层展示，以适应动态变化，业务拓扑图可更加清晰和有条理地把握业务系统。
2.  集中采集数据，实现数据融合
可观测性数据融合支持对所有网络设备、安全设备、操作系统、数据库、中间件、业务应用、性能数据、第三方监控系统、告警系统的日志信息采集，支持不同日志格式自动识别解析，形成有意义的字段，同时匹配数据模型，实现对链路数据的分析与追踪。
系统可接入Trace、Metrics、logs等数据，经过处理后存入_o11y_trace、\_o11y_metrics、\_o11y_topology和yotta索引，根据写入的索引进行数据模型的匹配，形成链路拓扑图。Tracing模型可支持调用链详情的查询；指标模型提供业务总览、业务详情中的右侧趋势图、服务总览、服务详情、设备页、指标探索等；拓扑模型用于显示链路拓扑。
3.  告警归并技术
告警归并技术可以把若干描述同一故障的报警智能归并，可以快速捕捉到故障本质，追寻故障根因。
![告警归并](media/image2.png){width="5.754166666666666in"
height="3.8118055555555554in"}
告警归并流程图
针对监控发过来的告警，会默认映射和预处理策略（AlertCEF兼容映射策略、AlertCEF兼容预处理策略）。映射策略从上到下的顺序依次匹配，一次只命中一个映射策略，但是可以命中多个预处理，即一个event可以属于多个alert。通过映射和预处理将原来的AlertCEF变为AlertEvent。映射策略里配置了字段映射规则之后的字段才会保留，在下一步的预处理策略中使用，其他字段会自动舍弃并且不再展示。
项目可对来自不同平台的告警进行智能归并，进行进一步的分析和使用，
例如针对跨行汇款业务的调用链构建请求，可获取与至少一个请求对应的海量日志数据，考虑到海量日志数据中可能包括多个日志记录系统生成的日志数据，而不同日志记录系统中的时间格式可能会不一致，因此，可以根据系统内置解析规则，从日志数据中抽取时间信息，并将时间信息转换成统一设置的时间戳格式，以便于后续对日志数据进行时间分桶。然后记录海量日志数据中每条数据的公共字段：请求唯一标识符、主机地址、主机本地时间戳和接口的事件类型，并基于全局公共的NTP服务器，在接收端根据网络往返延迟(Round
Trip
Time，RTT)和NTP差距，对不同主机来源的日志数据中的时间戳信息进行时移。
4.  日志分析技术
日志分析也是应用运维监控的强有效手段，将日志进行采集，实现业务系统日志的统一汇聚，通过关键字模糊匹配，人工经验干预等手段，实现错误日志实时监控、清洗和快速处理。
本项目采用先进的流式大数据处理架构，从业务-服务-接口-设备四层维度对应用系统进行分析的APP，接入基础监控指标和业务分析日志数据后，可方便梳理业务层面的依赖关系，进而展现出全面正确的可观察性内容。该项目构建的高性能、分布式日志处理架构可每秒分析10万条日志，每日可处理TB级数据量，处理延时达到，可以支持搜索、分析几秒钟之前产生的日志。
C.  必要的图表
可观测性三种数据对比
![](media/image3.png){width="5.760416666666667in"
height="2.4291666666666667in"}
**典型的业务调用链示意图：**
![/var/folders/9r/l6470myd577ftvlvqjx5_60w0000gn/T/com.microsoft.Word/WebArchiveCopyPasteTempFiles/p5928](media/image4.jpeg){width="5.763888888888889in"
height="2.776388888888889in"}