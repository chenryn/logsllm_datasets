# Training and Vulnerable Code

```cpp
ThrowReaderException(...);

if (dib_info.colors_important > 256) {
    if ((dib_info.image_size != 0U) && (dib_info.image_size > file_size)) {
        ThrowReaderException(...);
    }
}

if ((dib_info.number_colors != 0) || (dib_info.bits_per_pixel != 0)) {
    storage_class = PseudoClass;
}
```

**Listing 1: The vulnerable code of CVE-2018-20189.**

---

## 8. Discussion

### Benefit to Input Mutation
Most current fuzzers focus on mutating inputs to enhance fuzzing performance, such as AFL [2], AFLFast [10], and AFLGo [9]. In contrast, our approach aims to help Directed Greybox Fuzzing (DGF) filter out unreachable inputs. Interestingly, our method can also potentially optimize the strategy of input mutation. If a fuzzer knows which fields in the inputs impact execution, it can mutate these fields to reach the buggy code. Modifying other fields would not be beneficial. Based on the features extracted by FuzzGuard, we find that FuzzGuard can learn the fields impacting execution (see Section 7). Therefore, FuzzGuard can further assist DGF in the input mutation process.

### Learning Models
Intuitively, the convolutional architecture uses local patterns. However, CNNs can handle non-local patterns with enough layers. Similarly, RNNs can manage non-local patterns with sufficient layers but may forget former features otherwise. Given the high overhead of RNNs for long data, we chose a 3-layer CNN. Our evaluation shows that CNN achieved good performance (1.9% false positive rate and 0.02% false negative rate on average), indicating that most key features in the inputs are local patterns (e.g., the `bits_per_pixel` field in Figure 6). This is understandable, as a single constraint in an if-statement typically relies on local bytes in the inputs to make decisions.

### Memory Usage
In theory, we could keep unreachable inputs in memory indefinitely to avoid missing a Proof of Concept (PoC). However, memory is limited in practice. Our solution is to remove inputs that are highly unlikely to reach the buggy code. Specifically, if an input is judged as "unreachable" by the updated models multiple times, it is likely that it cannot reach the buggy code. This approach saves memory while maintaining accuracy. Our evaluation shows that no PoC was dropped using this method.

---

## 9. Related Work

### Traditional Fuzzers
Many state-of-the-art fuzzers have been proposed in recent years. AFL [2] is a representative Coverage-Guided Fuzzer (CGF) that has influenced other fuzzers. For example, Böhme et al. [10] use a Markov model to construct the fuzzing process, focusing on seeds that exercise low-frequency execution paths and then mutating them to cover more code and find bugs. FairFuzz [24] is similar to AFLFast [10] but provides new mutation strategies (i.e., overwritten, deleted, and inserted). Gan et al. [16] address the path collision problem in AFL by correcting the path coverage calculation. Another variant, AFLGo [9], selects seeds with execution paths closer to the target and mutates them to trigger the target bugs. Chen et al. [12] improve AFLGo with new seed selection and mutation strategies. Some researchers enhance effectiveness through traditional program analysis. For instance, Li et al. [25] use static analysis and instrumentation to identify magic numbers during execution and apply them to mutation to increase test case execution depth. Chen et al. [13] use dynamic techniques like colorful taint analysis to find bugs. Rawat et al. [30] combine static and dynamic analysis to obtain control and data flow information to improve mutation effectiveness. Chen et al. [14] discover memory layouts to perform accurate fuzzing. Unlike these approaches, we use a deep-learning-based method to filter out unreachable inputs, enhancing fuzzing performance.

### Learning-Based Fuzzers
Some fuzzers leverage intelligent techniques. For example, You et al. [35] extract vulnerable information from Common Vulnerabilities and Exposures (CVE) descriptions to trigger bugs in the Linux kernel. Wang et al. [33] learn grammar and semantic features from a large number of program inputs using probabilistic context-sensitive grammar (PCSG) and generate inputs from the learned PCSG. Previous studies [17, 28, 29] train static models to improve fuzzer mutation strategies by generating inputs more likely to trigger bugs. Godefroid et al. [17] use RNNs to learn program input grammar and generate new inputs. Rajpal et al. [29] use LSTM to predict suitable bytes in inputs and mutate these bytes to maximize edge coverage based on previous fuzzing experience. Nichols et al. [28] train a Generative Adversarial Network (GAN) to predict the executed path of an input. Chen et al. [15] apply gradient descent to solve the path constraint problem and find key bytes in an input leading to the buggy code. She et al. [31] use gradient descent to smooth the neural network model and learn branches in the program to improve coverage. Unlike these studies, which focus on mutating inputs to achieve high code coverage or efficiently reach target buggy code, FuzzGuard aims to help DGF filter out unreachable inputs, complementing and being compatible with other fuzzers rather than replacing them.

---

## 10. Conclusion

Recently, DGF has proven efficient in finding bugs with known locations. Most current studies focus on mutating inputs to increase the likelihood of reaching the target, but little has been done to filter out unreachable inputs. In this paper, we propose a deep-learning-based approach called FuzzGuard, which predicts the reachability of program inputs without executing the program. We also present novel techniques to handle the challenge of lacking representative labeled data. Results on 45 real bugs show that FuzzGuard can achieve up to a 17.1× speedup. We further demonstrate the key features learned by FuzzGuard, which indeed impact execution.

---

## Acknowledgments

The authors would like to thank our shepherd Konrad Rieck and anonymous reviewers for their insightful comments. This work is supported in part by the Beijing Natural Science Foundation (No. JQ18011), NSFC U1836211, 61728209, National Top-notch Youth Talents Program of China, Youth Innovation Promotion Association CAS, Beijing Nova Program, and National Frontier Science and Technology Innovation Project (No. YJKYYQ20170070).

---

## References

[1] podofo. http://podofo.sourceforge.net, 2006.
[2] American fuzzy lop. http://lcamtuf.coredump.cx/afl, 2018.
[3] Information of cve-2018-20189. https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20189, 2018.
[4] Rectified linear unit. https://ldapwiki.com/wiki/Rectified%20Linear%20Unit, 2018.
[5] Dominator (graph theory). https://en.wikipedia.org/wiki/Dominator_(graph_theory), 2019.
[6] Networkx. https://networkx.github.io, 2019.
[7] PyTorch. https://pytorch.org/, 2019.
[8] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pages 9505–9515, 2018.
[9] Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoudhury. Directed greybox fuzzing. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 2329–2344. ACM, 2017.
[10] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. Coverage-based greybox fuzzing as Markov chain. In Proceedings of the 23rd ACM Conference on Computer and Communications Security (CCS 2016), pages 1032–1043. ACM, 2016.
[11] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into high quality object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6154–6162, 2018.
[12] Hongxu Chen, Yinxing Xue, Yuekang Li, Bihuan Chen, Xiaofei Xie, Xiuheng Wu, and Yang Liu. HawkEye: Towards a desired directed grey-box fuzzer. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, pages 2095–2108. ACM, 2018.
[13] Kai Chen, DengGuo Feng, PuRui Su, and YingJun Zhang. Black-box testing based on colorful taint analysis. Scientia Sinica Informationis, 55(1):171–183.
[14] Kai Chen, Yingjun Zhang, and Peng Liu. Dynamically discovering likely memory layout to perform accurate fuzzing. IEEE Transactions on Reliability, 65(3):1180–1194, 2016.
[15] Peng Chen and Hao Chen. Angora: Efficient fuzzing by principled search. In 2018 IEEE Symposium on Security and Privacy (SP), pages 711–725. IEEE, 2018.
[16] Shuitao Gan, Chao Zhang, Xiaojun Qin, Xuwen Tu, Kang Li, Zhongyu Pei, and Zuoning Chen. CollAFL: Path sensitive fuzzing. In 2018 IEEE Symposium on Security and Privacy (SP), pages 679–696. IEEE, 2018.
[17] Patrice Godefroid, Hila Peleg, and Rishabh Singh. Learn&Fuzz: Machine learning for input fuzzing. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering, pages 50–59. IEEE Press, 2017.
[18] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.
[19] Kai Kang, Hongsheng Li, Junjie Yan, Xingyu Zeng, Bin Yang, Tong Xiao, Cong Zhang, Zhe Wang, Ruohui Wang, Xiaogang Wang, et al. T-CNN: Tubelets with Convolutional Neural Networks for object detection from videos. IEEE Transactions on Circuits and Systems for Video Technology, 28(10):2896–2907, 2018.
[20] James C King. Symbolic execution and program testing. Communications of the ACM, 19(7):385–394, 1976.
[21] D Kinga and J Ba Adam. A method for stochastic optimization. In International Conference on Learning Representations (ICLR), volume 5, 2015.
[22] lcamtuf. America Fuzz Loop strategies. https://lcamtuf.blogspot.com/2014/08/binary-fuzzing-strategies-what-works.html, 2014.
[23] Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business Media, 2006.
[24] Caroline Lemieux and Koushik Sen. FairFuzz: Targeting rare branches to rapidly increase greybox fuzz testing coverage. In Proceedings of the 33rd IEEE/ACM International Conference on Automated Software Engineering, 2018.
[25] Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu, and Alwen Tiu. Steelix: Program-state based binary fuzzing. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, pages 627–637. ACM, 2017.
[26] Edwin David Lughofer. FlexFIS: A robust incremental learning approach for evolving Takagi–Sugeno fuzzy models. IEEE Transactions on Fuzzy Systems, 16(6):1393–1410, 2008.
[27] Barton P Miller, Louis Fredriksen, and Bryan So. An empirical study of the reliability of Unix utilities. Communications of the ACM, 33(12):32–44, 1990.
[28] Nicole Nichols, Mark Raugas, Robert Jasper, and Nathan Hilliard. Faster fuzzing: Reinitialization with deep neural models. arXiv preprint arXiv:1711.02807, 2017.
[29] Mohit Rajpal, William Blum, and Rishabh Singh. Not all bytes are equal: Neural byte sieve for fuzzing. arXiv preprint arXiv:1711.04596, 2017.
[30] Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuffrida, and Herbert Bos. VUzzer: Application-aware evolutionary fuzzing. In Proceedings of the 24th Annual Network and Distributed System Security Symposium (NDSS 2017). ISOC, 2017.
[31] Dongdong She, Kexin Pei, Dave Epstein, Junfeng Yang, Baishakhi Ray, and Suman Jana. NEUZZ: Efficient fuzzing with neural program learning. In 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019.
[32] Michael Sutton, Adam Greene, and Pedram Amini. Fuzzing: Brute Force Vulnerability Discovery. Pearson Education, 2007.
[33] Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. SkyFire: Data-driven seed generation for fuzzing. In Proceedings of the 38th IEEE Symposium on Security & Privacy (S&P 2017). IEEE, 2017.
[34] Xiang Wu, Ran He, Zhenan Sun, and Tieniu Tan. A light CNN for deep face representation with noisy labels. IEEE Transactions on Information Forensics and Security, 13(11):2884–2896, 2018.
[35] Wei You, Peiyuan Zong, Kai Chen, XiaoFeng Wang, Xiaojing Liao, Pan Bian, and Bin Liang. SemFuzz: Semantics-based automatic generation of proof-of-concept exploits. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 2139–2154. ACM, 2017.
[36] Zhedong Zheng, Liang Zheng, and Yi Yang. A discriminatively learned CNN embedding for person re-identification. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 14(1):13, 2018.

---

## Appendix

**Listing 2: The sequence of calls to trigger CVE-2017-17501.**

```cpp
0x665abb in WriteOnePNGImage coders/png.c:7061
0x677891 in WriteMNGImage coders/png.c:9881
0x479f3d in WriteImage magick/constitute.c:2230
0x47a891 in WriteImages magick/constitute.c:2387
0x42bb9d in ConvertImageCommand magick/command.c:6087
0x43672e in MagickCommand magick/command.c:8872
0x45eeaf in GMCommandSingle magick/command.c:17393
0x45f0fb in GMCommand magick/command.c:17446
0x40c895 in main utilities/gm.c:61
```

**Listing 3: The sequence of calls to trigger the zero-day vulnerability.**

```cpp
0x548b71 in WriteOnePNGImage coders/png.c:7263
0x551d97 in WriteMNGImage coders/png.c:9881
0x450f60 in WriteImage magick/constitute.c:2230
0x4515da in WriteImages magick/constitute.c:2387
0x4215bc in ConvertImageCommand magick/command.c:6087
0x427e48 in MagickCommand magick/command.c:8872
0x44113e in GMCommandSingle magick/command.c:17393
0x441267 in GMCommand magick/command.c:17446
0x40be26 in main utilities/gm.c:61
```