ε log( 1
where given two vectors a, b ∈ Rd, then their product is de-
ﬁned as ha, bi = ∑d
i=1 aibi. Moreover, if x ∈ R, then sign(x) =
1 if x ≥ 0 and sign(x) = −1 otherwise. A classic result in
passive PAC learning states that O( d
δ )) data-
points are needed to learn fw [56]. On the other hand, sev-
eral works propose active learning algorithms for Fd,HS with
sample complexity2 ˜O(d log( 1
ε )) (under certain distributional
assumptions). For example, if the underlying distribution is
log-concave, there exists an active learning algorithm with
sample complexity ˜O(d log( 1
ε )) [9, 10, 63]. This general re-
duction in the sample complexity for Fd,HS is easy to infer
when d = 1. In this case, the data-points lie on the real line
and their labels are a sequence of −1’s followed by a sequence
of +1’s. The goal is to discover a point w where the change
from −1 to +1 happens. PAC learning theory states that this
can be achieved with ˜O( 1
ε )3 points i.i.d. sampled from D. On
1Halfspace models are also called linear SVM (support vector machine).
2The ˜O notation ignores logarithmic factors and terms dependent on δ.
3More generally, ˜O( d
ε ) points.
fw(x) =(−1 if hw, xi < −1
+1 otherwise
−1
−1 −1
+1 +1 +1
+1 +1
w∗
R
Figure 2: Halfspace classiﬁcation in dimension 1.
the other hand, an active learning algorithm that uses a sim-
ple binary search can achieve the same task with O(log( 1
ε ))
queries [20] (refer Figure 2).
2.2.2 Query Synthesis (QS) active learning
In this scenario, the learner can request labels for any instance
in the input space X, including points that the learner gen-
erates de novo, independent of the distribution D (e.g., L
can ask for labels for those x that have zero-probability of
being sampled according to D). Query synthesis is reason-
able for many problems, but labeling such arbitrary instances
can be difﬁcult if the oracle is a human annotator. Thus, this
scenario better represents real-world applications where the
oracle is automated (e.g., results from synthetic experiments
[32]). Since the data-points are independent of the distribu-
tion, generalization error is not an appropriate measure of
accuracy of the hypothesis ˆf , and other types of error are
typically used. These new error formulations depend on the
concrete hypothesis class F considered. For example, if F
is the class of boolean functions from {0, 1}n to {0, 1}, then
the uniform error is used. Assume that the oracle O knows
f ∗ ∈ F and uses it as labeling function (realizable case), then
the uniform error of the hypothesis ˆf is deﬁned as
Erru( ˆf ) = Pr
x∼{0,1}n
[ ˆf (x) 6= f ∗(x)]
where x is sampled uniformly at random from the instance
space {0, 1}n. Recent work [3, 16], for the class of halfspaces
Fd,HS (refer to Example 1) use geometric error. Assume that
the true labeling function used by the oracle is fw∗ , then the
geometric error of the hypothesis fw ∈ Fd,HS is deﬁned as
Err2( fw) = ||w∗ − w||2
where ||·||2 is the 2-norm.
In both active learning scenarios (PAC and QS), the learner
needs to evaluate the usefulness of an unlabeled instance x,
which can either be generated de novo or sampled from the
given distribution, in order to decide whether to query the
oracle for the corresponding label. In the state of the art, we
can ﬁnd many ways of formulating such query strategies.
Most of existing literature presents strategies where efﬁcient
search through the hypothesis space is the goal (refer the sur-
vey by Settles [50]). Another point of consideration for an
1312    29th USENIX Security Symposium
USENIX Association
active learner L is to decide when to stop. This is essential
as active learning is geared at improving accuracy while be-
ing sensitive to new data acquisition cost (i.e., reducing the
query complexity). While one school of thought relies on the
stopping criteria based on the intrinsic measure of stability
or self-conﬁdence within the learner, another believes that
it is based on economic or other external factors (refer [50,
Section 6.7]).
Given this diversity within active learning, we enhance
the standard deﬁnition of a learning algorithm and propose
the deﬁnition of an active learning system, which is geared
towards model extraction. Our deﬁnition is informed by the
MLaaS APIs that we investigated (more details in Table 1).
Deﬁnition 2 (Active learning system). Let F be a hypothesis
class with instance space X and label space Y. An active
learning system for F is given by two entities, the learner
L and the oracle O, interacting via membership queries: L
sends to O an instance x ∈ X; O answers with a label y ∈ Y.
We indicate via the notation O f ∗ the realizable case where O
uses a speciﬁc labeling function f ∗ ∈ F , i.e. y = f ∗(x). The
behavior of L is described by the following parameters:
1. Scenario: this is the rule that describes the generation of
the input for the querying process (i.e. which instances
x ∈ X can be queried). In the PAC scenario, the instances
are sampled from the underlying distribution D. In the
query synthesis (QS) scenario, the instances are gener-
ated by the learner L;
2. Query strategy: given a speciﬁc scenario, the query strat-
egy is the algorithm that adaptively decides if the la-
bel for a given instance xi is queried for, given that
the queries x1, . . . , xi−1 have been answered already. In
the query synthesis scenario, the query strategy also de-
scribes the procedure for instance generation.
3. Stopping criteria: this is a set of considerations used by
L to decide when it must stop querying.
Any system (L, O) described as above is an active learning
system for F if one of the following holds:
- (PAC scenario) For any D on X × Y and any ε, δ ∈
(0, 1), if L is allowed to interact with O using qL (ε, δ)
queries, then L outputs ˆf ∈ F such that ErrD ( ˆf ) ≤
min f∈F ErrD ( f ) + ε with probability at least 1− δ.
- (QS scenario) Fix an error measure Err for the functions
in F . For any f ∗ ∈ F , if L is allowed to interact with
O f ∗ using qL (ε, δ) queries, then L outputs ˆf ∈ F such
that Err( ˆf ) ≤ ε with probability at least 1− δ.
We refer to qL (ε, δ) as the query complexity of L.
As we will show in the following section (in particular,
refer § 3.2), the query synthesis scenario is more appropriate
in casting model extraction attack as active learning when we
make no assumptions about the adversary’s prior knowledge.
Note that, other types queries have been studied in literature.
This includes the equivalence query [4]. Here the learner can
verify if a hypothesis is correct or not. We do not consider
equivalence queries in our deﬁnition because we did not see
any of the MLaaS APIs support them.
3 Model Extraction
In § 3.1, we begin by formalizing the process of model extrac-
tion. We then draw parallels between model extraction and
active learning in § 3.2.
3.1 Model Extraction Deﬁnition
We begin by describing the operational ecosystem of model
extraction attacks in the context of MLaaS systems. An entity
learns a private model f ∗ from a public class F , and provides
it to the MLaaS server. The server provides a client-facing
query interface for accessing the model for prediction. For
example, in the case of logistic regression, the MLaaS server
knows a model represented by parameters a0, a1,··· , ad. The
client issues queries of the form x = (x[1],··· , x[d]) ∈ Rd, and
the MLaaS server responds with 0 if (1 + e−a(x))−1 ≤ 0.5 and
1 otherwise, with a(x) = a0 + ∑d
Model extraction is the process where an adversary exploits
this interface to learn more about the proprietary model f ∗.
The adversary can be interested in defrauding the descrip-
tion of the model f ∗ itself (i.e., stealing the parameters ai as
in a reverse engineering attack), or in obtaining an approx-
i=1 aix[i].
imation of the model, say ˆf ∈ F , that he can then use for
free for the same task as the original f ∗ was intended for. To
capture the different goals of an adversary, we say that the
attack is successful if the extracted model is “close enough”
to f ∗ according to an error function on F that is context de-
pendent. Since many existing MLaaS providers operate in a
pay-per-query regime, we use query complexity as a measure
of efﬁciency of such model extraction attacks.
Formally, consider the following experiment: an adversary
A, who knows the hypothesis class F , has oracle access to a
proprietary model f ∗ from F . This can be thought of as A in-
teracting with a server S that safely stores f ∗. The interaction
has several rounds. In each round, A chooses an instance x
and sends it to S. The latter responds with f ∗(x). After a few
rounds, A outputs a function ˆf that is the adversary’s candi-
date approximation of f ∗; the experiment considers ˆf a good
approximation if its error with respect to the true function f ∗
held by the server is less then a ﬁxed threshold ε. The error
function Err is deﬁned a priori and ﬁxed for the extraction
experiment on the hypothesis class F .
Experiment 1 (Extraction experiment). Given a hypothesis
class F = { f : X → Y}, ﬁx an error function Err : F → R.
Let S be a MLaaS server with the knowledge of a speciﬁc
f ∗ ∈ F , denoted by S( f ∗). Let A be an adversary interacting
with S with a maximum budget of q queries. The extraction
USENIX Association
29th USENIX Security Symposium    1313
experiment Expε
F (S( f ∗), A, q) proceeds as follows
1. A is given a description of F and oracle access to f ∗
through the query interface of S. That is, if A sends x ∈ X
to S, it gets back y = f ∗(x). After at most q queries, A
eventually outputs ˆf ;
2. The output of the experiment is 1 if Err( ˆf ) ≤ ε. Other-
wise the output is 0.
Informally, an adversary A is successful if with high proba-
bility the output of the extraction experiment is 1 for a small
value of ε and a ﬁxed query budget q. This means that A
likely learns a good approximation of f ∗ by only asking q
queries to the server. More precisely, we have the following
deﬁnition.
Deﬁnition 3 (Extraction attack). Let F be a public hypothe-
sis class and S an MLaaS server as explained before. We say
that an adversary A, which interacts with S, implements an
ε-extraction attack of complexity q and conﬁdence γ against
the class F if Pr[Expε
F (S( f ∗), A, q) = 1] ≥ γ, for any f ∗ ∈ F .
The probability is over the randomness of A.
In other words, in Deﬁnition 3 the success probability of
an adversary constrained by a ﬁxed budget for queries is
explicitly lower bounded by the quantity γ.
Before discussing the connection between model extraction
and active learning, we provide an example of a hypothesis
class that is easy to extract.
Example 2 (Equation-solving attack for linear regression).
Let Fd,R be the hypothesis class of regression models from
Rd to R. A function fa in this class is described by d + 1
parameters a0, a1, . . . , ad from R and deﬁned by: for any
x ∈ Rd, fa(x) = a0 + ∑d
i=1 aixi. Consider the adversary AES
that queries x1, . . . , xd+1 (d + 1 instances from Rd) chosen
in such a way that the set of vectors {(1, xi)}i=1,...,d+1 is lin-
early independent in Rd+1. AES receives the corresponding
d + 1 labels, y1, . . . , yd+1, and can therefore solve the linear
system given by the equations fa(xi) = yi. Assume that fa∗ is
the function known by the MLaaS server (i.e., yi = fa∗ (xi)).
It is easy to see that if we ﬁx Err( fa) = ||a∗ − a||1, then
Pr[Exp0
(S( fa∗), AES, d + 1) = 1] = 1. That is, AES imple-
ments 0-extraction of complexity d + 1 and conﬁdence 1.
Fd,R
While our model operates in the black-box setting, we
discuss other attack models in more detail in Remark 2
3.2 Active Learning and Extraction
From the description presented in the § 2, it is clear that model
extraction in the MLaaS system context closely resembles
active learning. The survey of active learning in § 2.2 contains
a variety of algorithms and scenarios which can be used to
implement model extraction attacks (or to study its impossi-
bility).
However, different scenarios of active learning impose dif-
ferent assumptions on the adversary’s prior knowledge. Here,
we focus on the general case of model extraction with an
adversary A that has no knowledge of the data distribution D.
In particular, such an adversary is not restricted to only con-
sidering instances x ∼ D to query. For this reason, we believe
that query synthesis (QS) is the right active learning scenario
to investigate in order to draw a meaningful parallelism with
model extraction. Recall that the query synthesis is the only
framework where the query inputs can be generated de novo
(i.e., they do not conform to a distribution).
Observation 1: Given a hypothesis class F and an error func-
tion Err, let (L, O) be an active learning system for F in the
QS scenario (Deﬁnition 2). If the query complexity of L is
qL (ε, δ), then there exists an adversary A that implements
ε-extraction with complexity qL (ε, δ) and conﬁdence 1− δ
against the class F .
The reasoning for this observation is as follows: consider
the adversary A that is the learner L (i.e., A deploys the query
strategy procedure and the stopping criteria that describe L).
This is possible because (L, O) is in the QS scenario and L
is independent of any underlying (unknown) distribution. Let
q = qL (ε, δ) and observe that
F (S( f ∗), A, q) = 1] =
Pr[Expε
Pr[A outputs ˆf and Err( ˆf ) ≤ ε] =
Pr[L outputs ˆf and Err( ˆf ) ≤ ε] ≥ 1− δ
Our observation states that any active learning algorithm in
the QS scenario can be used to implement a model extraction
attack. Therefore, in order to study the security of a given
hypothesis class in the MLaaS framework, we can use known
techniques and results from the active learning literature. Two