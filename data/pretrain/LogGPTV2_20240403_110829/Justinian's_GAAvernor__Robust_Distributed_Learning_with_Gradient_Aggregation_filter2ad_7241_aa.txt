title:Justinian's GAAvernor: Robust Distributed Learning with Gradient Aggregation
Agent
author:Xudong Pan and
Mi Zhang and
Duocai Wu and
Qifan Xiao and
Shouling Ji and
Min Yang
Justinian’s GAAvernor: Robust Distributed Learning 
with Gradient Aggregation Agent
Xudong Pan, Mi Zhang, Duocai Wu, and Qifan Xiao, Fudan University; 
Shouling Ji, Zhejiang University/Ant Financial; Min Yang, Fudan University
https://www.usenix.org/conference/usenixsecurity20/presentation/pan
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Justinian’s GAAvernor: Robust Distributed Learning
with Gradient Aggregation Agent
Xudong Pan†, Mi Zhang†, Duocai Wu†, Qifan Xiao†, Shouling Ji*,‡, and Min Yang†
†Fudan University, ∗Zhejiang University, ‡Ant Financial
Emails: {xdpan18, mi_zhang, dcwu18, qfxiao16}@fudan.edu.cn, PI:EMAIL, PI:EMAIL
Abstract
1
Introduction
The hidden vulnerability of distributed learning systems
against Byzantine attacks has been investigated by recent
researches and, fortunately, some known defenses showed
the ability to mitigate Byzantine attacks when a minority of
workers are under adversarial control. Yet, our community
still has very little knowledge on how to handle the situations
when the proportion of malicious workers is 50% or more.
Based on our preliminary study of this open challenge, we
ﬁnd there is more that can be done to restore Byzantine robust-
ness in these more threatening situations, if we better utilize
the auxiliary information inside the learning process.
In this paper, we propose Justinian’s GAAvernor (GAA), a
Gradient Aggregation Agent which learns to be robust against
Byzantine attacks via reinforcement learning techniques. Ba-
sically, GAA relies on utilizing the historical interactions with
the workers as experience and a quasi-validation set, a small
dataset that consists of less than 10 data samples from similar
data domains, to generate reward signals for policy learning.
As a complement to existing defenses, our proposed approach
does not bound the expected number of malicious workers
and is proved to be robust in more challenging scenarios.
Through extensive evaluations on four benchmark systems
and against various adversarial settings, our proposed defense
shows desirable robustness as if the systems were under no
attacks, even in some case when 90% Byzantine workers
are controlled by the adversary. Meanwhile, our approach
shows a similar level of time efﬁciency compared with the
state-of-the-art defenses. Moreover, GAA provides highly
interpretable traces of worker behavior as by-products for
further mitigation usages like Byzantine worker detection and
behavior pattern analysis.
Justinian I, an emperor of Byzantium, reorganized the imperial govern-
ment to revive the empire’s greatness in a dark time. Gradient Aggregation
Agent, a new GAAvernor (pronounced as governor) of distributed learning
system, bases its learning policy on historical and auxiliary information to
ﬁght against Byzantine attacks.
Over the past few decades, deep learning has achieved abun-
dant breakthroughs driven by big data [38, 52]. To deal
with the fast scaling-up of data volume, many efﬁcient dis-
tributed learning algorithms have been proposed in the past
decade [3, 22, 29], yet their hidden vulnerability to Byzantine
attacks [37] have also been observed by a series of recent
works [11, 16, 31, 62].
In a typical distributed learning system [3,34,41,43,50,64],
a group of workers participate in building a global learning
model under the coordination of one parameter server. In
each round, the server ﬁrst distributes current parameters of
the global learning model to each worker, requiring them to
compute the corresponding gradient based on their local data.
Once receiving all the submissions from the workers, the
server then applies certain Gradient Aggregation Rule (GAR)
to yield the next weight update. As an optimal choice in
theory [12, 47], most existing distributed learning algorithms
implemented their GAR simply by averaging over the whole
set of submitted gradients [42, 56, 63].
However, the behaviors of real-world workers are far from
ideal. As is suggested in [62], a worker may probably sub-
mit abnormal gradients due to various causes such as biased
batch sampling, computation error, network instability or even
malicious attacks. In [11], a worker with the aforementioned
abnormal behavior is usually referred to as a Byzantine worker.
As ﬁrst observed by Blanchard et al., the classical GAR (i.e.,
GAR by averaging) is so fragile that even a single Byzantine
worker can have a catastrophic effect on the whole learning
process, from degraded prediction accuracy [31] to total stag-
nation [11]. These facts highly emphasize the urgency and
signiﬁcance of effective defense against this type of adversar-
ial behavior, namely Byzantine attack.
To ﬁght against Byzantine attacks, most previous studies
implement alternative GARs to the classical one [4, 11, 16,
31, 62]. These methods view gradients abstractly as high-
dimensional vectors to apply robust statistical methods such
as clustering [11], median [31] or geometric median [4,16,62].
USENIX Association
29th USENIX Security Symposium    1641
Although it allows previous methods to be highly decoupled
with the underlying learning systems, the simplicity is ac-
companied with several weaknesses: First, as previous GARs
computes the weight update direction as the only product,
they are unable to provide interpretable information of the
workers’ behaviors for further mitigation; Second, due to the
theoretical bottleneck of robust statistics [48], most known
defenses expect that only a minority of workers are compro-
mised. As a result, they are inadequate and cannot be directly
extended to cover more challenging scenarios where the ad-
versary has gained control over a majority of workers and
iteratively manipulates an uncertain ratio of workers to play
the Byzantine roles.
Our Work. In this paper, we propose the design of Justinian’s
GAAvernor (GAA), a Gradient Aggregation Agent which
serves as a novel server-side defense that leverages Rein-
forcement Learning (RL) techniques to learn to be Byzantine-
robust from interactions with the workers and from the auxil-
iary information on the server. Our defense aims at restoring
the robustness of distributed learning in more challenging
scenarios characterized by the existence of the malicious ma-
jority.
By viewing the historical interactions with the workers as
its experience and the relative decrease of loss on a quasi-
validation set as its reward, GAA searches over a simplex as
its policy space for the optimal policy. Intuitively, each coor-
dinate of a policy of GAA can be interpreted as its current
credit on the corresponding worker. By proposing the weight
update at each iteration as a linear combination of the received
gradients weighted with its credits, GAA receives the reward
signal after the global learning model is updated with the cur-
rent weight update and it then optimizes its current policy by
RL techniques [54]. It is worth to notice, we introduce the
notion of a quasi-validation set to denote a collection of data
samples that follows a similar but not necessarily identical
distribution as the true sample distribution. In practice, when
a golden-labeled validation set (i.e., a set of samples from
the true sample distribution) is available during the learning
process, GAA can utilize it as its quasi-validation set. Other-
wise, GAA randomly collects a small number of data samples
(empirically, less than 10 samples) from similar data domains
to form its quasi-validation set.
With extensive experiments, we evaluate GAA’s robustness
on four diverse case studies (i.e., MNIST [39], CIFAR-10 [35],
Yelp reviews [1] and CMS public healthcare records [2]),
against various attacking settings. We ﬁnd our proposed ap-
proach shows near-optimal Byzantine robustness in most
cases, whenever the ratio of Byzantine workers (i.e., Byzan-
tine ratio) is below or over 50% or ﬂuctuates unboundedly.
Meanwhile, GAA shows comparable time efﬁciency to known
defenses. We also evaluate GAA’s robustness against several
adaptive attacks on this novel defense mechanism. Moreover,
we present the application of GAA to Byzantine worker de-
tection, which shows high accuracy, and to behavior pattern
analysis of Byzantine attacks, which demonstrates high inter-
pretability of its traces.
Contributions. In summary, we mainly make the following
contributions.
• We propose the design of GAA, a novel RL-based defense
against Byzantine attacks which requires no upper bound
on the Byzantine ratio (§4).
• We implement and evaluate our proposed defense on four
diverse case studies, against various adversarial settings.
Empirical results suggest in most cases, GAA with an easily
accessible quasi-validation set helps the distributed learning
systems achieve almost indistinguishable performance as if
the systems were under no attacks (§5 & §6).
• We also provide a number of analytic results on GAA’s ro-
bustness in different settings as theoretical evidences (§4.4).
• Additionally, we demonstrate the interpretability of GAA’s
traces with visualizations and with applications to Byzan-
tine worker detection and behavior analysis (§4.5), which
we hope will facilitate future mitigation studies.
2 Background and Preliminaries
Gradient-based Distributed Learning and GAR. In this
paper, we focus on the data-parallel distributed learning sys-
tem with one parameter server (abbrev. the server) and n
workers. This system model is widely used as one of the
commonest implementations of distributed learning algo-
rithms [3, 34, 41, 43, 50, 64]. We denote the loss function
to be minimized as f (θ,D), where θ ∈ Rd collects all the free
parameters of the underlying model (e.g., a deep neural net-
work) and D denotes the sample distribution. Usually, the true
loss function f (θ,D) is the expectation over the sample distri-
bution, i.e. f (θ,D) := Ez∼D [ f (θ,z)] where D is unknown to
the server. In practice, the optimization happens on the empir-
ical version of the loss f (θ,D) := 1|D| ∑z∈D f (θ,z), where D
is a collection of training samples. For simplicity, we denote
the true loss function as f and the empirical loss function
calcuated on dataset D as ˆfD.
The distributed learning process starts with an initial guess
θ0 on parameters. At iteration t, the server ﬁrst sends the
current parameter θt to each worker. Ideally, a worker i then
computes the estimated gradient V t
i of loss f at parameter
θt based on its local data and submits V t
i back to the server.
Once the server receives the candidate set of gradients Qt :=
n}, it executes certain GAR F : (Rd)n → Rd to
{V t
aggregate the received gradients into a single weight update
direction. Such a procedure is executed in iterations until
a provided termination condition is reached. Formally, the
update rule at iteration t follows θt+1 = θt − λF (V t
1, . . . ,V t
n),
where λ is the learning rate.
1, . . . ,V t
In the literature of distributed learning, the following GARs
are the common choices for implementation of F [3, 22, 29,
34, 61], while their vulnerability to Byzantine attacks have
1642    29th USENIX Security Symposium
USENIX Association
n ∑n
i=1Vi
been studied in a series of recent works [11, 16, 31, 62].
Deﬁnition 1 (Classical GAR). F (V1, . . . ,Vn) = 1
Deﬁnition 2 (Linear GAR). As a generalization of classi-
cal GAR, a linear GAR F with parameter α ∈ Sn is deﬁned
i=1 αiVi, where Sn := {α ∈ Rn : αi ≥
as F (V1, . . . ,Vn) = ∑n
0,∑n
Benign Workers vs. Byzantine Workers. In order to have a
precise understanding of what a Byzantine worker is, we start
from a formal deﬁnition of benign worker.
i=1 αi = 1} is called an n-dimension simplex.
As is discussed, at iteration t, each worker is expected to es-
timate the true gradient gt = Ez[∇θ f (θt ,z)] based on its local
data set D. Optimally, it computes V t := 1|D| ∑z∈D ∇θ f (θt ,z)
as its submission, due to the well-known fact that V t is an
unbiased estimator of gt if D is i.i.d. sampled from D [12].
Generally, it inspires us to make the following deﬁnition.
Deﬁnition 3 (Benign Worker). A worker which submits a
gradient V t at iteration t is said to be benign if V t is an
unbiased estimator of the true gradient gt, i.e., EV t = gt.
With such a deﬁnition of benign worker, it is rather simple
to deﬁne a Byzantine worker as its opposition.
Deﬁnition 4 (Byzantine Worker). Otherwise, a worker is said
to be Byzantine at iteration t if V t is biased, i.e., EV t −gt (cid:54)= 0.
A well-established theorem from statistics states that clas-
sical SGD is guaranteed to converge if the gradient estimation
at each descent step is unbiased [12, 14]. If the system is
ideally correct, classical GAR is almost the optimal choice.
However, it is usually not the case in real-world settings [62].
In fact, as ﬁrst noticed by [11], classical GAR and its variants
are so fragile that even a single Byzantine worker can totally
break the whole learning process, as is stated by the following
lemma.
Proposition 1. [11, Lemma 1] For any linear GAR F with
ﬁxed parameter α, the adversary with only one single Byzan-
tine worker can fool F into yielding any arbitrary weight
update continually regardless of other submissions.
3 Security Settings
3.1 Threat Model
Throughout this paper, we consider the same threat model
as in previous studies [4, 11, 16, 31, 62]. Generally speaking,
this threat model assumes that, the adversary compromises a
proportion β (s.t. β ∈ (0,1)) of all workers throughout the
learning process and he/she commands the compromised
workers to present arbitrary behaviors at each iteration. In
other words, the adversary is able to choose the submitted
gradients of each manipulated worker. Noteworthily, at itera-
tion t, the Byzantine ratio can be also smaller than β if some
Table 1: Comparisons among different defenses against
Byzantine attacks.
Brute-Force [31, 48]
GeoMed [16, 62]
Krum [11]
Bulyan [31]
GAA (ours)
Constraint
n ≥ 2m + 1 O((cid:0)n
(cid:1)(n− m)d)
Time Complexity
m
O(n2d)
n ≥ 2m + 1
n ≥ 2m + 3 O(n2d + n2 logn)
n ≥ 4m + 3
n ≥ m + 1
O(n2d)
O(n3d)
Space Complexity
O((cid:0)n
(cid:1) + nd)
m
O(n2d)
O(n2d)
O(n2 + nd)
O(n2 + nd)
Byzantine workers pretend benign. To provide a ﬁner-grained
description on the threat model, we introduce the following
notions.
Role Function. As is discussed, each worker behaves either
benignly or maliciously at iteration t. Therefore, we introduce
the notion of the role function of worker i to characterize its
temporal behaviors. Formally, the role function is deﬁned as
a binary-valued function on Z+, i.e., the timeline. Intuitively,
ri(t) = 1 means worker i behaves normally at iteration t and
otherwise, worker i is a Byzantine worker.
Tampering Algorithm. Byzantine workers can choose differ-
ent tampering algorithms to produce malicious gradients. In
previous studies, several realizations of tampering algorithms
have been used for evaluation of defenses, such as random
fault [11] (More details can be found in Section 5.1). In gen-
eral, we denote the tampering algorithm as T , which, with the
estimated gradient as the input, outputs the tampered gradient
for submission. As in previous studies, we assume the identity