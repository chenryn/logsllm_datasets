mapping function to the target task to resemble the feature distribu-
tion of the source. In practice, when we are looking for a mapping
from a source task to a target task for a specific user, we take the
remaining users data from the target and source task and solve an
optimization problem to find the coefficients in Equation 3, see [10].
Classification. We choose to use a support vector machine (SVM)
for the recognition. Since our goal is authentication rather than
identification, a one-class model is the natural choice. Some previ-
ous works instead train a binary model using data from all users
in the dataset (e.g., [11, 16]). This is disadvantageous as data from
other users is required for training and, depending on which users
are included in the negative class, the classifier performance may
not represent accurately the actual error rates of the system. We use
a radial basis function kernel and set the SVM hyper-parameters ν
to 0.5 and γ to 1|J | , J being the feature set. At test time, rather than
using the (binary) output of the classifier for the decision, we use
each sample’s distance to the learnt decision boundary.
Normalization. In order to account for the varying feature ranges
of different features, we independently normalize all feature values
in input to the classifier:
xi − µ
σ
.
zi =
This way, each feature is replaced by the number of standard devia-
tions it lies away from the distribution mean. The values for µ and
σ are computed on the training data, the transformation is applied
to both training and testing data.
Sample aggregation. Similarly to previous work, we aggregate
multiple samples into a single window to make an authentication
decision. For eye movements, this does not particularly slow down
the authentication time as the eye tracker produces several samples
over short windows (on average we obtain around five samples per
second). We choose to use a fixed-size sliding window, i.e., each
window contains exactly n samples. Within each window, we feed
each sample to the classifier, collect the distance of each sample
from the decision boundary and select the median distance for the
decision. Selecting the median rather than the mean allows us to
better account for outliers. As a result, we would expect an attacker
to go undetected for roughly n
2 samples. As n should be chosen
based on the system security requirements, we further investigate
the choice of n in the following section.
Setting the decision threshold. Using the median boundary dis-
tance obtained through sample aggregation, we then select a thresh-
old for acceptance. If the median of the aggregation window is
above the threshold, this window of samples is accepted. Varying
this threshold controls the tradeoff between the system’s FAR and
FRR. Note that this threshold is selected on a per-user basis, as
users with more erratic behaviour (i.e., a larger mismatch between
training and testing data) will require a more lenient threshold to
achieve acceptable performance.
5 ANALYSIS AND RESULTS
In this section, we first present an analysis of the features and then
show the authentication performances of the system. It should be
noted that throughout the results, we always present and treat
calibrated and uncalibrated sessions separately. Additionally, we
refer to “reading” as tasks T3, T9 (see Table 2), to “browsing” as
tasks T4, T10 and to “slideshow” as tasks T5, T11, and consider these
six tasks as the ones we use to evaluate the authentication. The first
slideshow (tasks T2, T8) is only used to fit the screen brightness
model of Equation 1.
Session 6A: Biometrics SecurityCCS ’19, November 11–15, 2019, London, United Kingdom1194Table 3: RMI values for each individual feature. The val-
ues are computed by using the data from all the tasks and
users, and are shown separately for the calibrated and un-
calibrated session.
Feature
Pupil Diameter Difference (min)
Pupil Diameter Difference (mean)
Pupil Diameter Difference (max)
Pupil Diameter (min)
Pupil Diameter (mean)
Pupil Diameter (max)
Left-Right difference (max)
Left-Right difference (mean)
Left-Right difference (min)
Pupil Diameter Difference (std-dev)
Pupil Diameter (std-dev)
Duration of Fixation
Pairwise Speed (mean)
Pairwise Acceleration (10 Perc)
Pairwise Speed (90 Perc)
Pairwise Acceleration (90 Perc)
Pairwise Speed (10 Perc)
Pairwise Speed (std-dev)
Distance to Centre (mean)
Distance to Centre (90 Perc)
Maximal Pairwise Distance (Y-direction
Maximal Pairwise Distance (X-direction)
Distance to Centre (std-dev)
Maximal Pairwise Distance
Distance to Centre (10 Perc)
RMI [%]
Ucal
27.03
26.27
26.15
15.82
15.61
15.84
34.58
34.92
33.94
8.97
6.47
4.46
2.52
2.22
2.43
2.35
1.38
0.89
0.85
0.77
0.61
0.30
0.24
0.33
0.37
Cal
21.90
21.54
21.07
14.19
13.88
13.87
8.81
8.62
6.85
6.29
3.29
3.28
2.09
1.96
1.94
1.69
0.87
0.87
0.55
0.42
0.27
0.25
0.16
0.12
0.04
5.1 Feature Analysis
The RMI (see Section 4.5 for details of its computation) of each
feature is given in Table 3.
Pupil-based features. Features coming from the pupil diameter
measurements contribute the highest amount of information in the
calibrated dataset. This is consistent with previous work [11, 12, 18].
Similar to previous work, the static ranges (e.g., min, max and mean)
are significantly more distinctive than the changes within a fixation
(as measured by the standard deviation).
Temporal-based features.. These features exhibit minimal changes
in distinctiveness when using uncalibrated data. This confirms our
initial hypothesis that our features depend on precision (i.e., the
gaze tracker reports similar coordinates for similar gaze values)
rather than accuracy (i.e., the gaze tracker reports the correct gaze
position) and that linear shifts in gaze positions will not affect them.
Binocular-based pupil features.. These features have not been
explored in previous work. We found that the difference in size
between the left and right pupil diameter is even more distinctive
than the raw measurements themselves. This can be explained
through two factors: inherent size differences between the left and
calibrated
Task
raw corrected
Reading
4.79
Browsing
7.24
Slideshow 7.57
1.88
3.92
4.97
uncalibrated
raw corrected
5.54
5.01
6.85
2.18
2.82
3.93
Table 4: EER [%] for intra-task authentication, considering
both calibrated and uncalibrated session, and for both raw
pupil measurements and pupil-corrected measurements.
Values are computed using 100 aggregated samples and a 50%
training data percentage.
right pupil and different light exposure. In our experimental setup,
the desk lamp is placed on one side of the screen (to the right),
which leads to each pupil being exposed to different amounts of
light. The difference in size between both pupils would therefore
be a function of their baseline size, their light sensitivity (which
has been shown to be different between individuals) and the user’s
posture (e.g., when a user is not sitting centred in front of the
screen). The results show that the features using the pupil diameter
contribute the highest amount of information.
A noteworthy observation is that most binocular features per-
form significantly better in the uncalibrated setting (RMI of 34.58%
vs 8.81%). This suggests that the feature is at least partially depen-
dent on the quality of the calibration. During the experiment we
observed that inaccurate calibrations often led to one eye being
tracked more accurately than the other and the distance between
left and right eye gaze positions being large, but relatively consis-
tent. In the calibrated session, we require a minimum calibration
accuracy before starting the tasks (see Section 2.3). This limits the
range of calibration errors between users. Despite this apparent
relationship, we argue that this feature is not merely a technical
“fluke”, but still reflects user-specific properties. As outlined in Sec-
tion 3, a design goal is that the system can be set up with a “generic”
calibration in order to avoid having to recalibrate it for each user.
Due to a multitude of changes in the user’s height, posture, distance
between eyes and distance to the screen, this generic calibration
will lead to a unique calibration error for each user and result in
high distinctiveness for the relevant features. Based on our data,
it is evident that these factors remain stable enough during our
20-minute session. We leave a further exploration of the long-term
stability of these binocular features for future work.
5.2 Classification Results
The system overall performance depends on several factors, in-
cluding the combination of training and testing tasks, aggregation
window size, whether calibration was used or not, proportion of
training data, pupil diameter correction and cross-task mapping
adjustment. For brevity, we report in Table 4 the EER results of a
reasonable combinations of these factors, where we perform intra-
task authentication, use a window size of 100 aggregated samples
and 50% of data for training. 100 samples are collected, on average,
after 20 seconds, which means an attacker would be detected af-
ter roughly 10 seconds (i.e., once half the sliding window is filled
Session 6A: Biometrics SecurityCCS ’19, November 11–15, 2019, London, United Kingdom1195Figure 5: EER depending on number of aggregated samples
for the calibrated slideshow (T5) task.
Figure 7: EER depending on the amount of training data for
the slideshow task (T5).
low error rates can be achieved even with comparatively small
amounts of training data shows that the user’s behaviour does not
vary significantly across the duration of each task.
Error rate distribution. While a system’s average EER gives a
rough idea of its expected security against zero-effort imperson-
ations, it is insufficient in the context of continuous authentication
without knowing the distribution of errors between users. The
highly skewed nature of error rates of biometric systems and the
resulting security implications has been previously shown by Dod-
dington et al. in 1998 [7]. Figure 9 shows that the average EER is
highly skewed by few users while most users show an EER close
to 0%. Previous work has suggested the use of the Gini Coefficient
(GC) to capture this property, with a high GC close to 1 indicating
skewed error rates [13]. Figure 11 shows a graphical representation
of our system’s Gini Coefficient for both the FAR and FRR. The
FAR in particular is highly skewed with a GC of .94. Despite this
skew, the highest FAR achieved across all victim-attacker pairs is
72%. Due to the continuous nature of the authentication system,
even this attacker would be detected after a short time span. Unlike
previous work [11, 12], we did not observe any systematic false
negatives (i.e., perpetually undetected attackers).
Impact of training data selection. As discussed in Section 4.5,
we use sequential training data in order to make our analysis as re-
alistic as possible. Nevertheless, in order to allow comparison with
other works that use random training data selection we show both
selection methodologies in Figure 8. It is evident that randomly sam-
pling the training data improves the overall system performance.
The effect is particularly pronounced when not applying the pupil
diameter correction. This is a result of the system not observing
the entire range of lighting changes when sequential data is used.
The effect is particularly strong for the reading and wiki tasks with-
out pupil diameter correction as the lighting changes sequentially
rather than randomly (see Table 2).
Cross-task authentication and mapping function. The results
of using one task for training and another for testing (i.e., cross-task
authentication) can be seen in Figure 10. In the raw data case (no
pupil diameter correction and no mapping function, see Figure 10a)
the error rates are, not surprisingly, the highest. Intuitively, using
the pupil diameter correction only marginally affects the error rates
between the reading and browsing task as the brightness differences
Figure 6: EER depending on number of aggregated samples
for the uncalibrated slideshow (T11) task.
with the attacker’s samples) due to the median-score aggregation
strategy. Table 4 shows the system EER for both calibrated and
uncalibrated session and for both light-corrected pupil diameter
and raw pupil diameter. The table shows that error rates are lowest
for the reading task across all configurations. The slideshow, which
includes randomly changing ambient and screen light, shows the
highest error rates. All tasks benefit significantly from the pupil
diameter correction.
Sample Aggregation. The influence of the number of aggregated
samples is shown in Figures 5 and 6 for the calibrated and uncal-
ibrated case, respectively. In both cases, the EER is reduced with
increasing window size. The effect becomes less pronounced over
time. This is intuitive, as the EER of most users reaches zero after a
moderate number of samples (i.e., a further increase in the number
of aggregated samples won’t improve it further).
Amount of training data. The effect of using varying fractions
of the entire dataset for training is shown in Figure 7. Across all