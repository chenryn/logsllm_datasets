or fetching textures within the shaders themselves for building
a P2 primitive. Another possibility for accessing memory is by
writing to the framebuffer. All these operations, however, need
careful access patterns that avoid the caches or the OCMEM
in order to reach memory. We found that buffers containing
vertices are lazily instantiated and the implicit synchronization
between parallel executions of the same shader on different
SPs makes it difﬁcult for an attacker to achieve predictable
behavior when accessing memory. Accessing memory through
OCMEM is also tricky given its larger size and asynchronous
transfers. We hence opted for texture fetching. Texture fetching
takes place within the boundaries of a shader, providing strong
control over the order of the memory accesses. Moreover,
textures’ allocations are easy to control, making it possible
to obtain more predictable memory layouts as we explain in
Section VII.
The remaining obstacle is dealing with L1 and L2 in be-
tween the shaders and the DRAM, and the less obvious texture
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:05 UTC from IEEE Xplore.  Restrictions apply. 
addressing necessary for converting from pixel coordinates to
(virtual) memory locations. We start by analyzing this mapping
function which allows us to access desired memory addresses
before analyzing the cache architecture in A330. We then use
this information to selectively ﬂush the GPU caches in order
to reach DRAM.
1) Texture addressing: Integrated GPUs partition textures
in order to maximize spatial locality when fetching them from
DRAM [16]. Known as tiling, this is done by aggregating
data from close texels (i.e. texture pixels) and storing them
consecutively in memory so that
they can be collectively
fetched. Tiling is frequently used on integrated GPUs due to
the limited bandwidth available to/from system memory.
These tiles, in the case of the A330, are 4 × 4 pixels. We
can store each pixel’s data in different internal formats, with
RGBA8 being one of the most common. This format stores
each channel in a single byte. Therefore, a texel occupies
4 bytes and a tile 64 bytes.
Without tiling, translation from (x, y) coordinates to virtual
address space is as simple as indexing in a 2D matrix.
Unfortunately tiling makes this translation more complex by
using the following function to identify the pixel’s offset in
an array containing the pixels’ data:
∗ W + TW − 1
∗ (TW ∗ TH )+
f (x, y) =
(cid:2)
y
TH
(cid:3)
+ x
TW
TW
(y mod TH ) ∗ TW + x mod TW
Here W is the width of the texture and TW , TH are respec-
tively width and height of a tile.
With this function, we can now address any four bytes
within our shader program in the virtual address space. How-
ever, given that our primitive P2 targets DRAM, we need
to address in the physical address space. Luckily, textures
are page-aligned objects. Hence, their virtual and physical
addresses share the lowest 12 bits given that on most modern
architectures a memory page is 4 KB.
2) Reverse engineering the caches: Now that we know
how to access memory with textures, we need to ﬁgure out
the architecture of the two caches in order to be able to access
DRAM through them. Before describing our novel reverse
engineering technique and how we used it to understand the
cache architecture we brieﬂy explain the way caches operate.
Cache architecture: A cache is a small and fast memory
placed in-between the processor and DRAM that has the
purpose of speeding up memory fetches. The size of a cache
usually varies from hundreds of KBs to few MBs. In order
to optimize spatial locality the data that gets accessed from
DRAM is not read as a single word but as blocks of bigger
size so that (likely) contiguous accesses will be already cached.
These blocks are known as cachelines. To improve efﬁciency
while supporting a large number of cachelines, caches are
often divided into a number of sets, called cache sets. Cache-
lines, depending on their address, can be place in a speciﬁc
cache set. The number of cachelines that can simultaneously
1 #define MAX max // max offset
2 #define STRIDE stride // access stride
3
4 uniform sampler2D tex;
5
6 void main() {
7
vec4 val;
vec2 texCoord;
// external loop not required for (a)
for (int i=0; i<2; i++) {
for (int x=0; x < MAX; x += STRIDE) {
texCoord = offToPixel(x);
val += texture2D(tex, texCoord);
}
}
gl_Position = val;
8
9
10
11
12
13
14
15
16
17 }
Listing 1: Vertex shader used to measure the size of the
GPU caches.
be placed in a cache set is referred to as the wayness of the
cache and caches with larger ways than one are known as
set-associative caches.
When a new cacheline needs to be placed in a cache set
another cacheline needs to be evicted from the set to make
space for the new cacheline. A predeﬁned replacement policy
decides which cacheline needs to be evicted. A common
replacement is LRU or some approximation of it.
From this description we can deduce the four attributes we
need to recover, namely (a) cacheline size, (b) cache size, (c)
associativity and (d) replacement policy.
Reversing primitives: To gain the aforementioned details
we (ab)use the functionalities provided by the GLSL code
that runs on the GPU. Listing 1 presents the code of the
shader we used to obtain (b). We use similar shaders to
obtain the other attributes. The OpenGL’s texture2D()
function [19] interrogates the TP to retrieve the pixels’ data
from a texture in memory. It accepts two parameter: a texture
and a bidimensional vector (vec2) containing the pixel’s
coordinates. The choice of these coordinates is computed
by the function offToPixel() which is based on the
inverse function g(of f ) = (x, y) of f (x, y) described earlier.
The function texture2D() operates with normalized device
coordinates, therefore we perform an additional conversion to
normalize the pixel coordinates to the [-1,1] range. With this
shader, we gain access to memory with 4 bytes granularity
(dictated by the RGBA8 format). We then monitor the usage
of the caches (i.e., number of cache hits and misses) through
the performance counters made available by the GPU’s Per-
formance Monitoring Unit (PMU).
Size: We can identify the cacheline size (a) and cache size (b)
by running the shader in Listing 1 – with a single loop for
(a). We initially recover the cacheline size by setting STRIDE
to the smallest possible value (i.e., 4 bytes) and sequentially
increasing MAX of the same value after every iteration. We
recover the cacheline as soon as we encounter 2 cache misses
(Cmiss = 2). This experiment shows that the size of cacheline
201
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:05 UTC from IEEE Xplore.  Restrictions apply. 
0
256
MAX value (bytes)
512
768
1024
1280
8
MAX value (KB)
16
24
32
i
s
e
s
s
m
1
L
160
128
96
64
32
0
q
r e
1
s = L
m is
1
L
i
s
e
s
s
m
E
H
C
U
1024
768
512
256
q
r e
E
H
C
s = U
m is
E
H
C
U
0
32
64
96
L1 requests
128
160
256
512
768
UCHE requests
1024
Fig. 3: Cache misses over cache requests for L1 and UCHE caches. The results are extracted using the GPU performance
counter after each run of the shader in Listing 1 with STRIDE equal to cacheline size and increasing MAX value.
in L1 and UCHE are 16 and 64 bytes respectively.
We then set STRIDE to the cacheline size and run Listing 1
until the number of cache misses is not half of the requests
anymore (Cmiss (cid:4)= Creq/2). We run the same experiment for
both L1 and UCHE. Figure 3 shows a sharp increase in the
number of L1 misses when we perform larger accesses than
1 KB and for UCHE after 32 KB, disclosing their size.
and
replacement
strategy: The
non-
Associativity
perpendicular rising edge in both of the plots in Figure 3
conﬁrms they are set-associative caches and it suggest a LRU
or FIFO replacement policy. Based on the hypothesis of a
deterministic replacement policy we retrieved the details of
the cache sets (c) by means of dynamic eviction sets. This
requires two sets of addresses, namely S, a set that contains
the necessary amount of elements to ﬁll up the cache, and E,
an eviction set that initially contains only a random address
E0 /∈ S. We then iterate over the sequence {S, E, Pi} where
is a probe element belonging to S ∪ E0. We perform
Pi
the experiment for increasing i until Pi generates a cache
miss. Once we detect
the evicted cacheline, we add the
corresponding address to E and we restart the process. We
reproduce this until Pi = E0. When this happens we have
evicted every cacheline of the set and the elements in E can
evict any cacheline that map to the same cache set (i.e., an
eviction set). Hence, the size of E is the associativity of the
cache.
Once we identiﬁed the associativity of the caches, we can
recover the replacement strategy (d) by ﬁlling up a cache set
and accessing again the ﬁrst element before the ﬁrst eviction.
Since this element gets evicted even after a recent use in both
of the caches, we deduce a FIFO replacement policy for both
L1 and UCHE.
Synopsis: All the details about these two caches are summa-
rized in Table II. As can be seen from this table, there are many
peculiarities in the architecture of these two caches and in their
interaction. First, the two caches have different cacheline sizes,
which is unusual when comparing to CPU caches. Then, L1
presents twice the ways UCHE has. Furthermore, one UCHE
cacheline is split into 4 different L1 cachelines. These are
shufﬂed over two different L1 cache sets as shown in Figure 4.
202
Fig. 4: Mapping of a 64-byte UCHE cacheline into multiple
L1 cacheline over two different L1 sets.
TABLE II: Summary of the two level caches.
Cacheline (bytes)
Size (KB)
Associativity (#ways)
L1
16
1
16
UCHE
64
32
8
Replacement policy
Inclusiveness
FIFO
non-inclusive
We will exploit this property when building efﬁcient eviction
strategies in Section VII. Finally, we discovered L1 and UCHE
to be non-inclusive. This was to be expected considering that
L1 has more ways than UCHE.
C. Generalization
Parallel programming libraries, such as CUDA or OpenCL,
provide an attacker with a more extensive toolset and have
already been proven to be effective when implementing side-
channel attacks [24], [25], [34]. However, we decided to
restrict our abilities to what is provided by the OpenGL ES 2.0
API in order to relax our threat model to remote WebGL-based
attacks. Newer versions of the OpenGL API provide other
means to gain access to memory such as image load/store,
which supports memory qualiﬁers, or SSBOs (Shader Storage
Buffer Objects), which would have given us linear addressing
instead of the tiled addressing explained in Section VI-B1.
However, they conﬁne the threat model to local attacks carried
out from a malicious application.
Furthermore,
the reverse engineering technique we de-
scribed in Section VI-B2 can be applied to other OSes
and architectures without much effort. Most of the GPUs
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:05 UTC from IEEE Xplore.  Restrictions apply. 
D
E
F
G
Fig. 5: The diagrams show an efﬁcient GPU cache (set) eviction strategy. We use the notation a.b to abbreviate the lengthy
v[4K×a+16 × b]. The eviction happens in 4 steps: (a) ﬁrst we ﬁll up the 8 slots available in a cache set by accessing v[4K×i];
(b) after the cache set is full we evict the ﬁrst element by accessing v[4K×8]; (c) then, in order to access again v[0] from
DRAM we need to actually read v[32] since v[0] is currently cached in L1. The same holds for every page v[4K×i] for
i ∈ [1, 6]; (d) ﬁnally, we evict the ﬁrst L1 cacheline by performing our 17th access to v[4K×7+32] which replaces v[0].
available nowadays are equipped with performance counters
(e.g. Intel, AMD, Qualcomm Adreno, Nvidia) and they all
provide a userspace interface to query them. We employed
the GL_AMD_performance_monitor OpenGL extension
which is available on Qualcomm, AMD and Intel GPUs.
Nvidia, on the other hand, provides its own performance
analysis tool called PerfKit [13].
VII. SIDE-CHANNEL ATTACKS FROM THE GPU
In Section VI, we showed how to gain access to remote
system memory through the texture fetch functionality exposed
from the WebGL shaders. In this section, we show how we are
able to build an effective and low-noise DRAM side-channel
attack directly from the GPU. Previous work [24], [25], [34]
focuses on attacks in discrete GPGPU scenarios with a limited
impact. To the best of our knowledge, this is the ﬁrst report of
a side-channel attack on the system from an integrated GPU
that affects all mobile users. This attack beneﬁts from the small
size and the deterministic (FIFO) replacement policy of the
caches in these integrated GPUs. We use this side channel
to build a novel attack that can leak information about the
state of physical memory. This information allows us to detect
contiguous memory allocation (P3) directly in JavaScript,
a mandatory requirement for building effective Rowhammer
attacks.