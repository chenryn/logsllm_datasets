title:FBOSS: building switch software at scale
author:Sean Choi and
Boris Burkov and
Alex Eckert and
Tian Fang and
Saman Kazemkhani and
Rob Sherwood and
Ying Zhang and
Hongyi Zeng
FBOSS: Building Switch Software at Scale
Tian Fang
Sean Choi*
Facebook, Inc.
Boris Burkov
Facebook, Inc.
Alex Eckert
Facebook, Inc.
Stanford University
Saman Kazemkhani
Facebook, Inc.
Rob Sherwood
Facebook, Inc.
ABSTRACT
The conventional software running on network devices, such
as switches and routers, is typically vendor-supplied, pro-
prietary and closed-source; as a result, it tends to contain
extraneous features that a single operator will not most likely
fully utilize. Furthermore, cloud-scale data center networks
often times have software and operational requirements that
may not be well addressed by the switch vendors.
In this paper, we present our ongoing experiences on over-
coming the complexity and scaling issues that we face when
designing, developing, deploying and operating an in-house
software built to manage and support a set of features re-
quired for data center switches of a large scale Internet con-
tent provider. We present FBOSS, our own data center switch
software, that is designed with the basis on our switch-as-
a-server and deploy-early-and-iterate principles. We treat
software running on data center switches as any other soft-
ware services that run on a commodity server. We also build
and deploy only a minimal number of features and iterate on
it. These principles allow us to rapidly iterate, test, deploy
and manage FBOSS at scale. Over the last five years, our
experiences show that FBOSS’s design principles allow us
to quickly build a stable and scalable network. As evidence,
we have successfully grown the number of FBOSS instances
running in our data center by over 30x over a two year period.
CCS CONCEPTS
terfaces; Routers;
• Networks → Data center networks; Programming in-
*Work done while at Facebook, Inc.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5567-4/18/08. . . $15.00
https://doi.org/10.1145/3230543.3230546
Ying Zhang
Facebook, Inc.
KEYWORDS
Hongyi Zeng
Facebook, Inc.
FBOSS, Facebook, Switch Software Design, Data Center
Networks, Network Management, Network Monitoring
ACM Reference Format:
Sean Choi, Boris Burkov, Alex Eckert, Tian Fang, Saman
Kazemkhani, Rob Sherwood, Ying Zhang, and Hongyi Zeng. 2018.
FBOSS: Building Switch Software at Scale. In SIGCOMM ’18: SIG-
COMM 2018, August 20–25, 2018, Budapest, Hungary. ACM, New
York, NY, USA, 15 pages. https://doi.org/10.1145/3230543.3230546
1
INTRODUCTION
The world’s desire to produce, consume, and distribute on-
line content is increasing at an unprecedented rate. Commen-
surate with this growth are equally unprecedented technical
challenges in scaling the underlying networks. Large Internet
content providers are forced to innovate upon all aspects of
their technology stack, including hardware, kernel, compiler,
and various distributed systems building blocks. A driving
factor is that, at scale even a relatively modest efficiency
improvement can have large effects. For us, our data center
networks power a cloud-scale Internet content provider with
billions of users, interconnecting hundreds of thousands of
servers. Thus, it is natural and necessary to innovate on the
software that runs on switches. 1
Conventional switches typically come with software writ-
ten by vendors. The software includes drivers for managing
dedicated packet forwarding hardware (e.g., ASICs, FPGAs,
or NPUs), routing protocols (e.g., BGP, OSPF, STP, MLAG),
monitoring and debugging features (e.g., LLDP, BFD, OAM),
configuration interfaces (e.g., conventional CLI, SNMP, Net-
Conf, OpenConfig), and a long tail of other features needed
to run a modern switch. Implicit in the vendor model is the
assumption that networking requirements are correlated be-
tween customers. In other words, vendors are successful be-
cause they can create a small number of products and reuse
them across many customers. However our network size and
the rate of growth of the network (Figure 1) are unlike most
other data center networks. Thus, they imply that our require-
ments are quite different from most customers.
1We use “switch” for general packet switching devices such as switches and
routers. Our data center networks are fully Layer 3 routed similar to what is
described in [36].
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
S. Choi et al.
Thanks to this trend, we started an experiment of building
our in-house designed switch software five years ago. Our
server fleet already runs thousands of different software ser-
vices. We wanted to see if we can run switch software in a
similar way we run software services. This model is quite
different from how conventional networking software is man-
aged. Table 1 summarizes the differences between the two
high-level approaches using a popular analogy [17].
The result is Facebook Open Switching System (FBOSS),
which is now powering a significant portion of our data center
infrastructure. In this paper, we report on five years of expe-
riences on building, deploying and managing FBOSS. The
main goals of this paper are:
(1) Provide context about the internal workings of the soft-
ware running on switches, including challenges, design trade-
offs, and opportunities for improvement, both in the abstract
for all network switch software and our specific pragmatic
design decisions.
(2) Describe the design, automated tooling for deployment
monitoring, and remediation methods of FBOSS.
(3) Provide experiences and illustrative problems encoun-
tered on managing a cloud-scale data center switch software.
(4) Encourage new research in the more accessible/open
field of switch software and provide a vehicle, an open source
version of FBOSS [4], for existing network research to be
evaluated on real hardware.
The rest of the paper closely follows the structure of Ta-
ble 1 and is structured as follows: We first provide a couple of
our design principles that guide FBOSS’s development and
deployment (Section 2). Then, we briefly describe major hard-
ware components that most data center switch software needs
to manage (Section 3) and summarize the specific design
decisions made in our system (Section 4). We then describe
the corresponding deployment and management goals and
lessons (Section 5, Section 6). We describe three operational
challenges (Section 7) and then discuss how we have success-
fully overcome them. We further discuss various topics that
led to our final design (Section 8) and provide a road map for
future work (Section 9).
2 DESIGN PRINCIPLES
We designed FBOSS with two high-level design princi-
ples: (1) Deploy and evolve the software on our switches the
same as we do our servers (Switch-as-a-Server). (2) Use early
deployment and fast iteration to force ourselves to have a
minimally complex network that only uses features that are
strictly needed (Deploy-Early-and-Iterate). These principles
have been echoed in the industry - a few other customized
switch software efforts like Microsoft ACS [8]/SONiC [33]
is based on similar motivation. However, one thing to note is
Figure 1: Growth in the number of switches in our data
center over two year period as measured by number of
total FBOSS deployments.
One of the main technical challenges in running large net-
works is managing complexity of excess networking features.
Vendors supply common software intended for their entire
customer base, thus their software includes the union of all
features requested by all customers over the lifetime of the
product. However, more features lead to more code and more
code interactions, which ultimately lead to increased bugs,
security holes, operational complexity, and downtime. To
mitigate these issues, many data centers are designed for sim-
plicity and only use a carefully selected subset of networking
features. For example, Microsoft’s SONiC focuses on build-
ing a “lean stack” in switches [33].
Another of our network scaling challenges is enabling a
high-rate of innovation while maintaining network stability.
It is important to be able to test and deploy new ideas at scale
in a timely manner. However, inherent in the vendor-supplied
software model is that changes and features are prioritized by
how well they correlate across all of their customers. A com-
mon example we cite is IPv6 forwarding, which was imple-
mented by one of our vendors very quickly due to widespread
customer demand. However, an important feature to our oper-
ational workflow was fine-grained monitoring of IPv6, which
we quickly implemented for our own operational needs. Had
we left this feature to the demands of the customer market
and to be implemented by the vendors, we would not have
had this feature until over four years later, which was when
the feature actually arrived to the market.
In recent years, the practice of building network switch
components has become more open. First, network vendors
emerged that do not build their own packet forwarding chips.
Instead they rely on third-party silicon vendors, commonly
known as “merchant silicons”. Then, merchant silicon ven-
dors along with box/chassis manufacturers have emerged that
create a new, disaggregated ecosystem where networking
hardware can be purchased without any software. As a re-
sult, it is now possible for end-customers to build a complete
custom switch software stack from scratch.
1x5x10x15x20x30x03691215182124FBOSS Deployments →Months →Growth in FBOSS deploymentsFBOSS: Building Switch Software at Scale
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Hardware (S3)
Release Cycle (S5)
Testing (S5)
Resiliency Goals (S5)
Upgrades (S6)
Configuration (S6)
Monitoring (S6)
Switch Software
Closed, custom embedded systems. Limited
CPU/Mem resources.
Planned releases every 6-12 months.
Manual testing, slow production roll out.
High device-level availability with target of
99.999% device-level uptime.
Scheduled downtime, manual update process.
Decentralized and manually managed. Custom
backup solutions.
Custom scripting on top of SNMP counters.
General Software
Open, general-purpose servers. Spare and fungi-
ble CPU/Mem resources
Continuous deployment.
Continuous integration in production.
High system-level availability with redundant
service instances.
Uninterrupted service, automated deployment.
Centrally controlled, automatically generated
and distributed. Version controlled backups.
Rich ecosystem of data collection, analytics and
monitoring software libraries and tools.
Table 1: Comparison of development and operation patterns between conventional switch software and general software
services, based on popular analogy [17].
that our design principles are specific to our own infrastruc-
ture. Data center network at Facebook has multiple internal
components, such as Robotron [48],FbNet [46], Scuba [15]
and Gorilla [42], that are meticulously built to work with one
another, and FBOSS is no different. Thus, our design has our
specific goal of easing the integration of FBOSS into our ex-
isting infrastructure, which ultimately means that it may not
be generalized for any data center. Given this, we specifically
focus on describing the effects of these design principle in
terms of our software architecture, deployment, monitoring,
and management.
2.1 Switch-as-a-Server
A motivation behind this principle comes from our experi-
ences in building large scale software services. Even though
many of the same technical and scaling challenges apply
equally to switch software as to general distributed software
systems, historically, they have have been addressed quite
differently. For us, the general software model has been more
successful in terms of reliability, agility, and operational sim-
plicity. We deploy thousands of software services that are not
feature-complete or bug-free. However, we carefully monitor
our services and once any abnormality is found, we quickly
make a fix, deploy the change. We found this practice to be
highly successful in building and scaling our services.
For example, database software is an important part of
our business. Rather than using a closed, proprietary vendor-
provided solution that includes unnecessary features, we
started an open source distributed database project and modi-
fied it heavily for internal use. Given that we have full access
to the code, we can precisely customize the software for the
desired feature set and thereby reduce complexity. Also, we
make daily modifications to the code and, using the industry
practices of continuous integration and staged deployment,
are able to rapidly test and evaluate the changes in produc-
tion. In addition, we run our databases on commodity servers,
rather than running them on custom hardware, so that both
the software and the hardware can be easily controlled and