n l o
d
e
h
s c
g
u li n
e t c .
4x4 ows
8x8 ows
16x16 ows
24x24 ows
l
s
e
p
m
a
S
f
o
n
o
i
t
c
a
r
F
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
5
10 15 20 25 30 35 40 45 50 55 60 65
SKB size(KB)
(a) Throughput-per-core (Gbps)
(b) Receiver CPU breakdown
(c) skb size distribution
Figure 8: Linux network stack performance for all-to-all traffic pattern. (a) Each column shows throughput-per-core achieved for different number of
flows. With 8 × 8 flows, the network is fully saturated. Throughput-per-core decreases as the number of flows increases. (b) With all optimizations enabled,
as the number of flows increase, the fraction of CPU cycles spent in data copy decreases. On the receiver-side, network saturation leads to lower memory
management overhead (due to better page recycling) and higher scheduling overhead (due to frequent idling and greater number of threads per core.). TCP/IP
processing overhead increases due to smaller skb sizes. The overall receiver-side CPU utilizations for x= 1 × 1, 8 × 8, 16 × 16 and 24 × 24 are 1, 4.07, 5.56 and
6.98 cores, respectively. See [7] for sender-side CPU breakdown. (c) The fraction of 64KB skbs after GRO decreases as the number of flows increases because
the larger number of flows prevent effective aggregation of received packets. See §3.5 for description.
(cid:11)(cid:12)
(cid:9)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:15)(cid:13)(cid:8)
(cid:17)
(cid:16) (cid:14)(cid:16)(cid:18)(cid:15)(cid:13)
(cid:1)
(cid:2)(cid:8)(cid:6)(cid:9)(cid:10)(cid:5)
(cid:2)(cid:8)(cid:6)(cid:9)(cid:10)(cid:4)
(cid:2)(cid:8)(cid:6)(cid:9)(cid:10)(cid:3)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:14)
(cid:6)(cid:1)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:18)(cid:9)(cid:19)(cid:20)(cid:9)(cid:21) (cid:22)(cid:23)(cid:24) (cid:24)(cid:17)(cid:25)(cid:26)
(cid:15)(cid:9)(cid:27)(cid:9)(cid:25)(cid:28)(cid:9)(cid:21) (cid:22)(cid:23)(cid:24) (cid:24)(cid:17)(cid:25)(cid:26)
(cid:16)
(cid:15)
(cid:14)
(cid:4)
(cid:13)
(cid:10)
(cid:12)
(cid:11)
(cid:10)
(cid:9)
(cid:7)
(cid:8)
(cid:7)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:1)(cid:2)(cid:8)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:6)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:4)
(cid:1)(cid:2)(cid:3)
(cid:1)
(cid:1)
(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:3)
(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)
(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:2)
(cid:12)(cid:13)(cid:14)(cid:14) (cid:15)(cid:16)(cid:17)(cid:9)
(cid:9)
(cid:11) (cid:10)
(cid:12)
(cid:14)(cid:7)(cid:16)(cid:16) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15) (cid:1)
(cid:8)(cid:9)(cid:10)(cid:10) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15) (cid:3)(cid:2)(cid:7)(cid:14)(cid:16)(cid:6)
(cid:8)(cid:9)(cid:10)(cid:10) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15) (cid:3)(cid:2)(cid:7)(cid:14)(cid:16)(cid:5)
(cid:8)(cid:9)(cid:10)(cid:10) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15) (cid:3)(cid:2)(cid:7)(cid:14)(cid:16)(cid:4)
(cid:13) (cid:11)
(cid:2)
(cid:16)
(cid:15)
(cid:14)
(cid:4)
(cid:13)
(cid:10)
(cid:12)
(cid:11)
(cid:10)
(cid:9)
(cid:7)
(cid:8)
(cid:7)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:1)(cid:2)(cid:8)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:6)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:4)
(cid:1)(cid:2)(cid:3)
(cid:1)
(cid:14)(cid:7)(cid:16)(cid:16) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15) (cid:1)
(cid:8)(cid:9)(cid:10)(cid:10) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15) (cid:3)(cid:2)(cid:7)(cid:14)(cid:16)(cid:6)
(cid:8)(cid:9)(cid:10)(cid:10) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15) (cid:3)(cid:2)(cid:7)(cid:14)(cid:16)(cid:5)
(cid:8)(cid:9)(cid:10)(cid:10) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15) (cid:3)(cid:2)(cid:7)(cid:14)(cid:16)(cid:4)
(cid:9)
(cid:11) (cid:10)
(cid:12)
(cid:13) (cid:11)
(cid:2)
(a) Throughput-per-core (Gbps)
(b) CPU Utilization
(c) Sender CPU breakdown
(d) Receiver CPU breakdown
Figure 9: Linux network stack performance for the case of a single flow, with varying packet drop rates. (a) Each column shows throughput-per-
core achieved for a specific packet drop rate. Throughput-per-core decreases as the packet drop rate increases. (b) As the packet drop rate increases, the gap
between sender and receiver CPU utilisation decreases because the sender spends more cycles for retransmissions. (c, d) With all optimizations enabled, as
the packet drop rate increases, the overhead of TCP/IP processing and netdevice subsystem increases. See §3.6 for description.
3.6 Impact of In-network Congestion
In-network congestion may lead to packet drops at switches, which
in turn impacts both the sender and receiver side packet processing.
In this subsection, we study the impact of such packet drops on
CPU efficiency. To this end, we add a network switch between the
two servers, and program the switch to drop packets randomly. We
increase the loss rate from 0 to 0.015 in the single flow scenario
from §3.1, and observe the effect on throughput and CPU utilization
at both sender and receiver.
Impact on throughput-per-core is minimal. As shown in
Fig. 9(a) the throughput-per-core decreases by ∼24% as the drop rate
is increased from 0 to 0.015. Fig. 9(b) shows that the receiver-side
CPU utilization decreases with increasing loss rate. As a result, the
total throughput becomes lower than throughput-per-core, and the
gap between the two increases. Interestingly, the throughput-per-
core slightly increases when the loss rate goes from 0 to 0.00015.
We observe that the corresponding receiver-side cache miss rate
is reduced from 48% to 37%. This is because packet loss essentially
reduces TCP sending rate, thus resulting in better cache hit rates at
the receiver-side.
Figs. 9(c) and 9(d) show CPU profiling breakdowns for different
loss rates. With increasing loss rate, at both sender and receiver,
we see that the fraction of CPU cycles spent in TCP, netdevice
subsystem, and other (etc.) processing increases, hence leading to
fewer available cycles for data copy.
The minimal impact is due to increased ACK processing.
Upon detailed CPU profiling, we found increased ACK process-
ing and packet retransmissions to be the main causes for increased
overheads. In particular:
• At the receiver, the fraction of CPU cycles spent in generating
and sending ACKs increases by 4.87× (1.52% → 7.4%) as the
loss rate goes from 0 to 0.015. This is because, when a packet is
dropped, the receiver gets out-of-order TCP segments, and ends
up sending duplicate ACKs to the sender. This contributes to an
increase in both TCP and netdevice subsystem overheads.
• At the sender, the fraction of CPU cycles spent in processing
ACKs increases by 1.45× (5.79% → 8.41%) as the loss rate goes
from 0 to 0.015. This is because the sender has to process ad-
ditional duplicate ACKs. Further, the fraction of CPU spent in
packet retransmission operations increases by 1.34%. Both of
these contribute to an increase in TCP and netdevice subsys-
tem overheads, while the former contributes to increased IRQ
handling (which is classified under “etc.” in our taxonomy).
Sender observes higher impact of packet drops. Fig. 9(b)
shows the CPU utilization at the sender and the receiver. As drop
rates increase, the gap between sender and receiver utilization de-
creases, indicating that the increase in CPU overheads is higher
at the sender side. This is due to the fact that, upon a packet drop,
the sender is responsible for doing the bulk of the heavy lifting in
terms of congestion control and retransmission of the lost packet.
73
(cid:9)(cid:10)
(cid:8)
(cid:4)(cid:2)
(cid:4)(cid:1)
(cid:3)(cid:2)
(cid:3)(cid:1)
(cid:2)
(cid:1)
(cid:13)(cid:14) (cid:15)(cid:16)(cid:17)(cid:18)
(cid:15)
(cid:20)(cid:21)(cid:22)(cid:11)(cid:14)
(cid:19)(cid:14)(cid:17)(cid:23)(cid:12)(cid:19)(cid:24)(cid:16)(cid:17)
(cid:5)
(cid:3)(cid:6)
(cid:7)(cid:4)
(cid:6)(cid:5)
(cid:4)(cid:2)
(cid:4)(cid:1)
(cid:3)(cid:2)
(cid:3)(cid:1)
(cid:2)
(cid:1)
(cid:12)
l
s
e
c
y
C
U
P
C
f
o
n
o
i
t
c
a
r
F
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
d
a t a   c
t c
p
4KB
16KB
32KB
64KB
s y s t e m
o
y
p
/ i p   p
e
c
o
e t d
r
n
g
s si n
vic
e
b
u
e  s
b   m g m t
s k
r y   a ll o
m e m
o
e
d
/
c
c
a ll o
l o
u
/
k
c
k
c
n l o
d
e
h
s c
g
u li n
e t c .
(cid:16)
(cid:10)
(cid:7)
(cid:16)
(cid:17)
(cid:4)(cid:2)
(cid:4)(cid:1)
(cid:3)(cid:2)
(cid:3)(cid:1)
(cid:2)
(cid:1)
(cid:23)(cid:24)(cid:16)(cid:10)(cid:25)(cid:26)(cid:24)(cid:27)(cid:25)(cid:19) (cid:28)(cid:17)(cid:16) (cid:7)(cid:10)(cid:16)(cid:17)
(cid:29)(cid:17)(cid:11)(cid:17)(cid:30)(cid:31)(cid:17)(cid:16)(cid:32) (cid:7)(cid:12)(cid:11)(cid:24)(cid:17) (cid:14)(cid:30)(cid:33)(cid:33) (cid:29)(cid:12)(cid:19)(cid:17)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:9) (cid:5)(cid:13)(cid:14)(cid:15)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:16)(cid:17)(cid:18)(cid:10)(cid:19)(cid:17) (cid:5)(cid:13)(cid:14)(cid:15)
(cid:3)(cid:1)(cid:1)
(cid:22)(cid:1)
(cid:21)(cid:1)
(cid:20)(cid:1)
(cid:4)(cid:1)
(cid:1)
(a) Throughput-per-core (Gbps)
(b) Server CPU breakdown
(c) NIC-remote NUMA effect (4KB)
Figure 10: Linux network stack performance for short flow, 16:1 incast traffic pattern, with varying RPC sizes. (a) Each column shows throughput-
per-core achieved for a specific RPC size. Throughput-per-core increases with increasing RPC size. For small RPCs, optimizations like GRO do not provide
much benefit due to fewer aggregation opportunities. (b) With all optimizations enabled, data copy quickly becomes the bottleneck. The server-side CPU was
completely utilized for all scenarios. See [7] for client-side CPU breakdown. (c) Unlike long flows, no significant throughput-per-core drop is observed even
when application runs on NIC-remote NUMA node core at the server. See §3.7 for description.
3.7 Impact of Flow Sizes
We now study the impact of flow sizes on the Linux network stack
performance. We start with the case of short flows: a ping-pong
style RPC workload, with message sizes for both request/response
being equal, and varying from 4KB to 64KB. Since a single short flow
is unable to bottleneck CPU at either the sender or the receiver,
we consider the incast scenario—16 applications on the sender
send ping-pong RPCs to a single application on the receiver (the
latter becoming the bottleneck). Following the common deployment
scenario, each application uses a long-running TCP connection.
We also evaluate the impact of workloads that comprise of a mix
of both long and short flows. For this scenario, we use a single core