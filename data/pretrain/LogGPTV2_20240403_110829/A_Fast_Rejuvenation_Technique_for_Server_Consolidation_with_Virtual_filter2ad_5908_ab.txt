memory suspend and resume of n VMs in parallel.
On the other hand, when the cold-VM reboot is used,
the downtime due to the VMM rejuvenation is caused by
shutting down all operating systems, resetting hardware, re-
booting a VMM, and booting all operating systems. The
increase of the downtime is:
dc(n) = resethw + rebootvmm(0) + rebootos(n)−
rebootos(1) × α
where resethw is the time needed for a hardware reset,
rebootos(n) is the time needed to shut down and boot n
operating systems in parallel, and α is a ratio of the time
elapsed until the VMM rejuvenation since the last OS reju-
venation to an interval between the OS rejuvenation (0 <
α ≤ 1). Since the OS rejuvenation is rescheduled after the
VMM rejuvenation, the number of the OS rejuvenation is
decreased by α in total although extra OS rejuvenation is
added by the VMM rejuvenation.
calculated by dc(n) − dw(n):
The downtime reduced by using the warm-VM reboot is
r(n) = resethw + rebootvmm(0) − rebootvmm(n)+
rebootos(n) − rebootos(1) × α − resume(n)
To achieve the warm-VM reboot, we have developed
RootHammer based on Xen 3.0.0. Like Xen, a VM is called
a domain. In particular, the privileged VM that manages
VMs and handles I/O is called domain 0 and the other VMs
are called domain Us.
4.1. Memory Management of the VMM
The VMM distinguishes machine memory and pseudo-
physical memory to virtualize memory resource. Machine
memory is physical memory installed in the machine and
consists of a set of machine page frames. For each ma-
chine page frame, a machine frame number (MFN) is con-
secutively numbered from 0. Pseudo-physical memory is
the memory allocated to domains and gives the illusion of
contiguous physical memory to domains. For each physical
page frame in each domain, a physical frame number (PFN)
is consecutively numbered from 0.
The VMM creates the P2M-mapping table to enable do-
mains to reuse its memory even after the reboot. The P2M-
mapping table is a table that records mapping from PFN to
MFN for each domain. The size of our P2M-mapping table
is 2 MB for 1 GB of pseudo-physical memory. A new en-
try is added to this table when a new machine page frame
is allocated to a domain while an existing entry is removed
when a machine page frame is deallocated. These entries
are preserved after domains are suspended. Even when the
total size of pseudo-physical memory is larger than that of
machine memory due to using a ballooning technique [27],
this table can maintain the mapping properly.
4.2. On-memory Suspend/Resume Mechanism
When the operating system in domain 0 is shut down,
the VMM suspends all domain Us as in Figure 3. To sus-
pend domain Us, the VMM sends a suspend event to each
domain U. In the original Xen, domain 0 sends the event to
each domain U. One advantage of suspending by the VMM
is that suspending domain Us can be delayed until after the
operating system in domain 0 is shut down. The original
suspend by domain 0 has to be performed while domain 0
is shut down. This delay reduces the downtime of services
running in a domain U. When a domain U receives the sus-
pend event, the operating system kernel in the domain U
executes its suspend handler. In the handler, the kernel de-
taches all devices. We used the handler implemented in the
Linux kernel modiﬁed for Xen.
After the operating system in a domain U executes the
suspend handler,
it issues the suspend hypercall to the
VMM, which is like a system call to the operating system.
In the hypercall, the VMM freezes the memory image of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:26 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007domain 0
domain U
2. suspend
    handling
kernel
3. suspend
    hypercall
VMM
PFN:
MFN:
0 1 2
1 4 3
...
1. suspend
    event
4. freeze
MFN:
0 1 2 3 4 5
machine memory
5. save
    domain state
...
Figure 3. On-memory suspend of a domain U.
the domain on memory by reserving it. The VMM does
not release the memory pages allocated to the domain but
it maintains them using the P2M-mapping table. This does
not cause out-of-memory errors because the VMM is re-
booted just after it suspends all domain Us. Next, the VMM
saves the execution state of the domain to the memory pages
that is preserved during the reboot of the VMM. The exe-
cution state of a domain includes execution context such as
CPU registers and shared information such as the status of
event channels. In addition, the VMM saves the conﬁgu-
ration of the domain, such as devices. The memory space
needed for saving those is 16 KB.
After the VMM ﬁnishes suspending all domain Us, the
VMM is rebooted without losing the memory images of do-
main Us by using the quick reload mechanism, which is
described in the next section. Then, after domain 0 is re-
booted, it resumes all domain Us. First, domain 0 creates a
new domain U, allocates the memory pages recorded in the
P2M-mapping table to the domain U, and restores its mem-
ory image. Next, the VMM restores the state of the domain
U from the saved state. The operating system kernel in the
domain U executes the resume handler to re-establish the
communication channels to the VMM and to attach the de-
vices that were detached on suspend. Finally, the execution
of the kernel is restarted.
4.3. Quick Reload Mechanism
To preserve the memory images of domain Us during the
reboot of a VMM, we have implemented the quick reload
mechanism based on the kexec mechanism [21] provided in
the Linux kernel. The kexec mechanism enables a new ker-
nel to be started without a hardware reset. Like kexec, the
quick reload mechanism enables a new VMM to be started
without a hardware reset. To load a new VMM instance into
the current VMM, we have implemented the xexec system
call in the Linux kernel for domain 0 and the xexec hyper-
call in the VMM.
When the xexec system call is issued in domain 0, the
kernel issues the xexec hypercall to the VMM. This hyper-
call loads a new executable image consisting of a VMM, a
kernel for domain 0, and an initial RAM disk for domain
0 into memory. When the VMM is rebooted, the quick
reload mechanism ﬁrst passes the control to the CPU used
at the boot time. Then, it copies the executable loaded by
the xexec hypercall to the address where the executable im-
age is loaded at normal boot time. Finally, the mechanism
transfers the control to the new VMM.
When the new VMM is rebooted and initialized, it ﬁrst
reserves the memory for the P2M-mapping table. Based on
the table, the VMM reserves the memory pages that have
been allocated to domain Us. Next, the VMM reserves
the memory pages where the execution state of domains is
saved. The latest Xen 3.0.4 also supports the kexec facility
for its VMM, but it does not have any support to preserve
the memory images of domain Us while a new VMM is ini-
tialized.
5. Experiments
We performed experiments to show that our technique
for fast rejuvenation is effective. For a server machine, we
used a PC with two Dual-Core Opteron processors Model
280, 12 GB of PC3200 DDR SDRAM memory, a 36.7 GB
of 15,000 rpm SCSI disk (Ultra 320), and gigabit Ethernet
NICs. We used the RootHammer VMM and, for compari-
son, the original VMM of Xen 3.0.0. The operating systems
running on top of the VMM were Linux 2.6.12 modiﬁed for
Xen. One physical partition of the disk was used for a vir-
tual disk of one VM. The size of the memory allocated to
domain 0 was 512 MB. For a client machine, we used a
PC with dual Xeon 3.06 GHz processors, 2 GB of memory,
and gigabit Ethernet NICs. The operating system was Linux
2.6.8.
5.1. Performance of On-memory Suspend/Resume
We measured the time needed for tasks before and af-
ter the reboot of the VMM: suspend or shutdown, and re-
sume or boot. We ran a ssh server in each VM as a ser-
vice provided to the outside. We performed this experi-
ment for (1) our on-memory suspend/resume, (2) Xen’s sus-
pend/resume, which uses a disk to save the memory images
of VMs, and (3) simple shutdown and boot.
First, we changed the size of memory allocated to a sin-
gle VM from 1 to 11 GB and measured the time needed
for pre- and post-reboot tasks. Figure 4 shows the results.
Xen’s suspend/resume depended on the memory size of a
VM because this method must write the whole memory im-
age of a VM to a disk and read it from the disk. On the other
hand, our on-memory suspend/resume hardly depended on
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:26 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007200
150
100
50
)
c
e
s
(
e
m
i
t
d
e
s
p
a
e
l
0
0
Xen’s suspend
OS shutdown
on-memory suspend
2
4
6
8
10
memory size of a VM (GB)
(a) pre-reboot task
200
150
100
50
)
c
e
s
(
e
m
i
t
d
e
s
p
a
e
l
0
0
Xen’s resume
OS boot
on-memory resume
2
4
6
8
10
memory size of a VM (GB)
(b) post-reboot task
)
c
e
s
(
e
m
i
t
n
w
o
d
e
g
a
r
e
v
a
500
400
300
200
100
0
0
2
saved-VM reboot
cold-VM reboot
warm-VM reboot
saved-VM reboot
cold-VM reboot
warm-VM reboot
500
400
300
200
100
)
c
e
s
(
e
m
i
t
n
w
o
d
e
g
a
r
e
v
a
10
0
0
2
4
8
number of VMs
6
(a) ssh
6
4
8
number of VMs
(b) JBoss
10
Figure 4. The time for pre- and post-reboot
tasks when the memory size of a VM is
changed.
Figure 6. The downtime of ssh and JBoss
when the number of VMs is changed.
250