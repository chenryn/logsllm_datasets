devices and virtual CPU states of persistent VMs to
NVRAM.
 The POD-driver regains control and stops any memory
operations, freezes CPUs and ﬂushes the caches.
After this procedure all persistent VMs and their environ-
ment are saved and can be recovered once the system is
restarted. Finally, it has to be noted that step number three
can be omitted if all management state of persistent VMs is
directly stored in NVRAM.
E. Recovering
In our current design we do not make further assumption
than that VMs and their management state are stored in
NVRAM. Thus, the host operating system performs an
ordinary boot process like a system without support for
non-volatile memory. Once the host operating system and
the NV-Hypervisor are up and running, persistent VMs have
to be recovered. This is achieved by retrieving information
about persistent VMs that were running when the power
outage occurred. Taking this information into account, per-
sistent VMs are recovered by reassigning their memory and
integrating virtual device information. Finally, the virtual
CPU state of the recovered VMs is restored and the VMs
are marked as ready for execution.
IV. IMPLEMENTATION
Our NV-Hypervisor prototype extends the QEMU virtual-
ization platform and integrates NVDIMMs [5], a NVRAM-
solution provided by Viking Technology. The NVDIMMs are
implemented by DRAM memory modules that are backed
by a ﬂash memory of the same size and a capacitor. In case
of a power drop, the module uses the capacitor energy to
mirror the DRAM state to the ﬂash memory and vice versa
at reboot time.
A. Integration of NVDIMMs.
In line with the proposed architecture, we have imple-
mented an allocator for managing NVRAM. It consists of
two parts: a kernel module that adds NVRAM into the
system memory map and provides operations for reading and
writing conﬁguration registers of NVDIMMs; and a user-
space library (libnvram.so) that provides functions (i.e.
nv_alloc(), nv_free()), nv_init()) for allocating
and freeing regions in NVRAM.
B. NV-Hypervisor core services.
The detection of a power outage is implemented by
a POD handler that comes attached with the NVDIMMs.
Communication between the POD-handler and our QEMU
extension is implemented as a blocking ioctl-syscall. We
657
added a thread to our QEMU extension that issues the
ioctl-syscall and blocks inside the POD-driver until a
power outage is detected. For managing persistent VMs, we
have implemented two additional QEMU Monitor commands:
dump-devices and nv-restore. The former saves the
environment of the VM into storage, the latter performs the
recovery of persistent VMs by merging the VM image in
NVRAM with virtual devices that are restored from storage.
Two additional ﬂags were added to QEMU: nv-restore
and -nvm. The ﬁrst one tells QEMU that do_nv_restore
function should be performed after start automatically. The
second ﬂag obliges QEMU to use NVRAM for allocation
of VM.
V. EVALUATION
As evaluation platform we used a NVDIMM equipped
server (2 Xeon CPUs and 8 GB RAM, with 4GBs of it
being non-volatile memory) provided by Viking [5], running
Linux (kernel version 3.4.12) as a host operating system.
As an implementation basis for NV-Hypervisor we utilized
QEMU (version 1.4.2). Our initial evaluations focused on the
timing behavior of NV-Hypervisor during a power outage and
a comparison between NV-Hypervisor and a vanilla Linux
server when recovering a memory-heavy VM.
A. Handling of a power fault
As detailed in Section IV our current implementation is
based on the assumption that the system has enough residual
energy to continue execution for 30-50ms after detecting a
power outage [11], [9]. To ensure that we are under the limit,
we instantiated a large VM with eight virtual cores and a
default set of devices including graphic and network support.
Next, we measured the time for processing the non-maskable
power-outage-interrupt provided by the POD and saved all
volatile state that belongs to the persistent VM. The size of
volatile state was 80 KB. Saving took only 8 ms and ensures
that a limited set of persistent VMs can be preserved in case
of a power outage.
However, this limitation can be overcome by allocating
memory for virtual devices of VMs in non-volatile memory
but requires additional changes to QEMU. Moreover, the
use of hardware virtualization techniques like VT-x can
reduce volatile state in the hypervisor. According to Intel’s
speciﬁcation [12], the NMI driver performs a VM exit event
if an interrupt arises when the CPU is occupied by a VM.
The event stops the VM execution and places the VM state in
NVRAM automatically. In this case it is no longer necessary
to send a message from the NMI handler to the Hypervisor.
These additional modiﬁcations of the current NV-Hypervisor
implementation remove the limitation on the count of VMs
that can be saved during the power outage.
Regarding the implementation of the POD, we evaluated
the Viking-provided detector which utilizes the Power Good
signal generated by the power supply unit. However, this
Boot phase
DB warm up
DB recovery
GuestOS boot
QEMU start
Host boot
BIOS
NVDIMMs init
Server boot
(cid:2)
Table II
BOOT PROCESS COMPARISON
Commodity system (sec)
NV-Hypervisor (sec)
566
54
31
0.2
108
15
n/a
36
810.2
n/a
n/a
n/a
8
108
15
109
36
276
)
s
m
(
e
m
i
t
t
s
e
u
q
e
R
)
s
m
(
e
m
i
t
t
s
e
u
q
e
R
300
200
100
t
l
u
a
F
r
e
w
o
P
y
r
e
v
o
c
e
R
r
e
w
o
P
0
300
200
100
0
B
D
f
o
t
r
a
t
s
e
R
M
V
f
o
y
r
e
v
o
c
e
R
0
200
400
Time (sec)
600
800
Figure 2. Process of a database recovery: Commodity system versus NV-Hypervisor
solution provides a smaller advance warning time than
documented by related approaches [11], [9]. In fact, it
provides only enough time to quiescent the memory bus for
securing all ongoing operations regarding the NVDIMMs, but
is too small for saving the volatile state of multiple persistent
VMs. Therefore, in the remainder of our evaluation, we
assume the availability of a POD similar to the one proposed
by Heiser et. al [11].
B. Recovery of a database server
To evaluate hypervisor-based persistence, we were inter-
ested in the recovery behavior of a virtual machine hosting
a memory-heavy service.
We created a virtual machine containing a typical Web-
based application, composed of an Apache web server
instance as front end and a MySQL database as back end.
As workload we selected sysbench oltp test suite [13].
Next, we compared the recovery behavior of an unmodiﬁed
vanilla Linux system running plain QEMU with our NV-
Hypervisor prototype.
Table II details the different phases during the boot process.
The actual boot process of the host operating system is quite
similar. In fact, the NV-Hypervisor-based system is even
slower as the NVDIMMs have to be initialized and checked.
As we used an early evaluation platform, this boot step might
get faster once NVDIMMs reach the ﬁnal product stage. After
QEMU/NV-Hypervisor is running, the situation changes as
for the commodity system: the VM and its services have
to be started while in case of NV-Hypervisor this is not
necessary. Still, up to this point, the NV-Hypervisor-based
solution is about 13% slower.
However, the picture changes once the actual runtime
behavior of both implementation is taken into account. A
relational database typically has a long warm-up phase until
queries can be answered at full speed. Accordingly, we
measured the time until both settings were fully operational.
This was performed using the sysbench utility which creates
a table with 1000000 lines and measures request response
658
time for a random query applied to this table. While for the
NV-Hypervisor-based solution, there is virtually no warm-up
time, and it took 566 seconds for the commodity system.
Figure 2 details the warm-up process. First, we see normal
operations for both system. After normal operation we induce
a power outage and start a recovery at time zero. While
the commodity solution continues operation after 244.2
seconds the NV-Hypervisor-based requires 31.8 seconds
more. Furthermore, we see that initially the queries of the
commodity system are about a factor of 5 slower than NV-
Hypervisor-based instance.
Our evaluation shows that NV-Hypervisor has a constant
time for any VM recovery, which heavily depends on the
hardware support of NVRAM. When taking into account the
actual service response time, the commodity system demands
for a factor of 2.9 longer until the recovery is fully ﬁnished.
VI. FUTURE WORK
Both types of virtualization techniques, hardware-based
as well as binary translation, use virtual memory for VM
allocation. Our current implementation instead directly allo-
cates NVRAM for VMs, i.e. we use a one-to-one mapping
thereby omitting the use of virtual memory. Unused virtual
pages of common VMs can be stored in a swap ﬁle to free
physical space for other VMs. Since NVRAM is attached to
the memory controller like ordinary DRAM, our approach
could also utilize virtual pages, and hence, some fragments of
VMs could be placed in a swap ﬁle if physical memory is not
enough. However, swapped out pages could be buffered in a
storage cache, and this volatile cache is lost in case of power
fault. Recovering of swapped out pages of persistent VMs is
not the only obstacle when utilizing virtual memory. Virtual
to physical memory mappings are placed in the memory
management unit (MMU). The MMU state is also volatile
and needs to be preserved in case of a power failure. As
identiﬁed in the context of mobile devices, mixed volatile/non-
volatile memory settings might even have some more pitfalls
[14]. In summary, virtual memory support for persistent
memory, hardware virtualization support,
interaction of
volatile hardware and non-volatile software - all of those are
issues for future research.
VII. CONCLUSION
In this paper, we introduced hypervisor-based persistence
as a means to integrate NVRAM to provide persistent virtual
machines. Our NV-Hypervisor builds a lightweight realization
of this abstraction and initial evaluation results based on the
recovery of memory-heavy services are promising. For the
future, we envision NV-Hypervisor to support the use of
virtual memory and extend persistence to the host operating
system thereby shortening the overall recovery time of
persistent VMs. With the widespread availability of NVRAM
in commodity servers, hypervisor-based persistence provides
the basis to immediately utilize it for legacy VMs, especially
in the context of infrastructure-as-a-service clouds.
ACKNOWLEDGMENTS
We would like to thank Thomas Knauth and anonymous
reviewers for their helpful comments. Also we thank Bertil
Munde and Viking Technology for access to hardware.
REFERENCES
[1] “2013 Cost of Data Center Outages,” Ponemon Institute, Tech.
Rep., 2013.
[2] J. Zhao, S. Li, D. H. Yoon, Y. Xie, and N. P. Jouppi, “Kiln:
Closing the Performance Gap Between Systems with and With-
out Persistence Support,” in Proceedings of the 46th Annual
IEEE/ACM International Symposium on Microarchitecture, ser.
MICRO-46, 2013, pp. 421–432.
[3] B. C. Lee, E. Ipek, O. Mutlu, and D. Burger, “Architecting
phase change memory as a scalable dram alternative,” in ACM
SIGARCH Computer Architecture News, vol. 37, no. 3. ACM,
2009, pp. 2–13.
[4] H. Li and Y. Chen, “An overview of non-volatile memory
technology and the implication for tools and architectures,” in
Design, Automation & Test in Europe Conference & Exhibition,
2009. DATE’09.
IEEE, 2009, pp. 731–736.
[5] “Viking Technology. ArxCis-NV (TM) Non-Volatile Memory
Technology,” http://www.vikingmodular.com/products/arxcis/
arxcis.html, 2012.
[6] H. Volos, A. J. Tack, and M. M. Swift, “Mnemosyne:
Lightweight persistent memory,” in ACM SIGARCH Computer
Architecture News, vol. 39, no. 1. ACM, 2011, pp. 91–104.
[7] J. Coburn, A. M. Caulﬁeld, A. Akel, L. M. Grupp, R. K.
Gupta, R. Jhala, and S. Swanson, “NV-Heaps: making persis-
tent objects fast and safe with next-generation, non-volatile
memories,” in ACM SIGARCH Computer Architecture News,
vol. 39, no. 1. ACM, 2011, pp. 105–118.
[8] X. L. K. L. X. Wang and X. Zhou, “NV-process: A Fault-
Tolerance Process Model Based on Non-Volatile Memory,”
2012.
[9] D. Narayanan and O. Hodson, “Whole-system persistence,” in
ACM SIGARCH Computer Architecture News, vol. 40, no. 1.
ACM, 2012, pp. 401–410.
[10] G. Dhiman, R. Ayoub, and T. Rosing, “PDRAM: a hybrid
PRAM and DRAM main memory system,” in Design Automa-
tion Conference, 2009. DAC’09. 46th ACM/IEEE.
IEEE,
2009, pp. 664–669.
[11] G. Heiser, E. Le Sueur, A. Danis, A. Budzynowski, T.-
l. Salomie, and G. Alonso, “RapiLog: reducing system
complexity through veriﬁcation,” in Proceedings of the 8th
ACM European Conference on Computer Systems. ACM,
2013, pp. 323–336.
[12] Intel, “Intel 64 and IA-32 Architectures Software Developer’s
Manual,” Volume 3B: System Programming Guide, Part, vol. 2,
2011.
[13] “Sysbench,” http://sysbench.sourceforge.net/.
[14] S. Kannan, A. Gavrilovska, and K. Schwan, “Reducing the
Cost of Persistence for Nonvolatile Heaps in End User Devices,”
in Proceedings of the 20th IEEE International Symposium On
High Performance Computer Architecture, 2014.
659