(cid:13)
(cid:9)
(cid:14)
(cid:6)
(cid:4)
(cid:15)
(cid:11)
(cid:5)
(cid:1)
(cid:8)
(cid:11)
(cid:19)
(cid:10)
(cid:1)
(cid:3)(cid:4)(cid:6)(cid:16)(cid:11)(cid:13)
(cid:2)(cid:6)(cid:7)(cid:13)(cid:6)(cid:12)
(cid:3)(cid:2)(cid:1)
(cid:5)(cid:2)(cid:1)
(cid:4)(cid:3)(cid:2)(cid:1)
(cid:6)(cid:3)(cid:2)(cid:1)
(cid:8)(cid:3)(cid:2)(cid:1) (cid:4)(cid:3)(cid:3)(cid:2)(cid:1)
Fig. 10. % of commands delivered using a slow decision by varying % of
conﬂicting commands. Batching here is disabled.
CAESAR’s ability to take fewer slow decisions than existing
consensus protocols in presence of conﬂicts helps it to achieve
a lower latency and higher throughput than competitors. In
Figure 10, we show the percentage of commands that were
committed by taking fast decisions in both the protocols. It
should be noted that the number of slow decisions taken by
EPaxos is in the same range as the percentage of conﬂict.
However, that is not the case of CAESAR, where the number of
slow decisions more gracefully increases along with conﬂicts.
In fact, CAESAR takes more than 3 times fewer slow decisions
compared to EPaxos even under moderately conﬂicting (e.g.
30%) workloads. The reason for that is the wait condition that
provides the rejection of a command only when its timestamp
is invalid. In this experiment, to avoid confusion in analyzing
statistics, batching has been disabled.
In Figure 11, we report the internal statistics of CAESAR
gathered during the experiment in Figure 9. Figure 11(a) shows
the breakdown of the proportion of latency consumed by each
ordering phase of the protocol. For no conﬂicts (0%, 2%),
the maximum time is spent in the proposal phase. The cost
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:00:16 UTC from IEEE Xplore.  Restrictions apply. 
(cid:5)(cid:16)(cid:14)(cid:15)(cid:14)(cid:17)(cid:9)
(cid:25)
(cid:6)(cid:9)(cid:18)(cid:16)(cid:20)
(cid:3)(cid:9)(cid:12)(cid:11)(cid:19)(cid:9)(cid:16)
(cid:1)
(cid:1)
(cid:29)
(cid:22)
(cid:23)
(cid:21)
(cid:29)
(cid:24)
(cid:18)
(cid:1)
(cid:20)
(cid:16)
(cid:24)
(cid:23)
(cid:22)
(cid:5)(cid:2)(cid:1)
(cid:4)(cid:3)(cid:2)(cid:1)
(cid:6)(cid:3)(cid:2)(cid:1)
(cid:10)(cid:2) (cid:8)(cid:5) (cid:3)(cid:4)
(cid:6)(cid:7)
(b) Wait condition.
(cid:6)(cid:4)
(cid:25)(cid:22)(cid:22)
(cid:24)(cid:26)(cid:22)
(cid:24)(cid:22)(cid:22)
(cid:23)(cid:26)(cid:22)
(cid:23)(cid:22)(cid:22)
(cid:26)(cid:22)
(cid:22)
(cid:29)
(cid:22)
(cid:25)
(cid:18)
(cid:1)
(cid:20)
(cid:16)
(cid:15)
(cid:19)
(cid:1)
(cid:14)
(cid:13)
(cid:15)
(cid:9)
(cid:1)
(cid:17)
(cid:14)
(cid:12)
(cid:11)
increases. The reason is related to the way they establish a
fast decision. In this paper we present an innovative technique
that provides a very high probability of fast delivery.
ACKNOWLEDGMENT
We thank the anonymous reviewers for their valuable com-
ments. This work is partially supported by Air Force Ofﬁce
of Scientiﬁc Research under grant FA9550-15-1-0098 and by
US National Science Foundation under grant CNS-1523558.
REFERENCES
(cid:20)
(cid:8)
(cid:13)
(cid:9)
(cid:18)
(cid:7)
(cid:4)
(cid:1)
(cid:10)
(cid:14)
(cid:13)
(cid:14)
(cid:1)
(cid:11)
(cid:18)
(cid:16)
(cid:14)
(cid:15)
(cid:14)
(cid:16)
(cid:5)
(cid:24)(cid:21)(cid:29)
(cid:24)(cid:21)(cid:28)
(cid:24)(cid:21)(cid:27)
(cid:24)(cid:21)(cid:26)
(cid:24)
(cid:15)
(cid:19)
(cid:1)
(cid:13)
(cid:15)
(cid:9)
(cid:1)
(cid:17)
(cid:14)
(cid:14)
(cid:12)
(cid:11)
(cid:3)(cid:2)(cid:1) (cid:5)(cid:2)(cid:1) (cid:4)(cid:3)(cid:2)(cid:1)(cid:6)(cid:3)(cid:2)(cid:1)(cid:7)(cid:3)(cid:2)(cid:1)(cid:4)(cid:3)(cid:3)(cid:2)(cid:1)
(cid:2)(cid:14)(cid:13)(cid:10)(cid:12)(cid:11)(cid:8)(cid:18)(cid:1)(cid:32)
(a) Ordering phases.
Fig. 11. Latency breakdown for CAESAR.
of the delivery is very low, since there are no dependencies.
However, as conﬂicts increase, delivery becomes a major
portion of the total cost because a STABLE command must
wait for the delivery of all the conﬂicting commands with
an earlier timestamp before being delivered. Figure 11(b)
reports the average time spent on the wait condition during
the proposal phase by conﬂicting commands using the same
workload for throughput measurement. Note that we used a
different scale (right y-axis) for 30% of conﬂicting commands
to highlight the difference with respect to the case of 2% and
10%. Close together nodes experience a quicker timestamp
advancement than faraway nodes because they are able to
exchange proposals faster. Faraway nodes are not aware of
this advancement, thus they propose commands with a lower
timestamp, which causes their conﬂicting commands to wait.
B. Recovery
(cid:1)
(cid:19)
(cid:13)
(cid:17)
(cid:4)
(cid:9)
(cid:3)
(cid:1)
(cid:16)
(cid:20)
(cid:20)
(cid:20)
(cid:21)
(cid:18)
(cid:1)
(cid:14)
(cid:15)
(cid:11)
(cid:7)
(cid:6)
(cid:15)
(cid:10)
(cid:12)
(cid:7)
(cid:2)
(cid:2)(cid:7)
(cid:2)(cid:5)
(cid:2)(cid:3)
(cid:2)(cid:1)
(cid:8)
(cid:7)
(cid:5)
(cid:3)
(cid:1)
(cid:2)(cid:3)(cid:4)(cid:9)(cid:6)(cid:8)
(cid:1)(cid:4)(cid:5)(cid:8)(cid:4)(cid:7)
(cid:1)
(cid:6)
(cid:2)(cid:1)
(cid:2)(cid:6)
(cid:3)(cid:1)
(cid:2)(cid:8)(cid:9)(cid:5)(cid:1)(cid:18)(cid:13)(cid:19)
(cid:3)(cid:6)
(cid:4)(cid:1)
(cid:4)(cid:6)
(cid:5)(cid:1)
Fig. 12. Throughput when one node fails.
In Figure 12, we report the throughput when one node
crashes, to show that it does not cause system’s unavailability.
We compared CAESAR and EPaxos. For this test, the requests
are injected in a closed-loop with 500 clients on each node.
After 20 seconds through the experiment, the instances of
CAESAR and EPaxos are suddenly terminated in one of the
nodes. Then, the clients from that node timeout and reconnect
to other nodes. This is visible by observing the throughput
falling down for few seconds due to loss of those 500 clients.
However, as the clients reconnect to other available nodes and
inject requests, the throughput restores back to the normal. In
our experiment, the recovery period lasted about 4 seconds.
VII. CONCLUSION
This paper shows that existing high-performance implemen-
tations of Generalized Consensus suffer from performance
degradation when the percentage of conﬂicting commands
60
[1] B. Charron-Bost and A. Schiper, “Uniform Consensus is Harder Than
Consensus,” J. Algorithms, vol. 51, no. 1, pp. 15–37, Apr. 2004.
[2] L. Lamport, “The Part-time Parliament,” ACM Trans. Comput. Syst.,
vol. 16, no. 2, pp. 133–169, May 1998.
[3] ——, “Paxos made simple,” ACM Sigact News, 2001.
[4] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman,
S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild, W. Hsieh, S. Kan-
thak, E. Kogan, H. Li, A. Lloyd, S. Melnik, D. Mwaura, D. Nagle,
S. Quinlan, R. Rao, L. Rolig, Y. Saito, M. Szymaniak, C. Taylor,
R. Wang, and D. Woodford, “Spanner: Google’s Globally Distributed
Database,” ACM Trans. Comput. Syst., vol. 31, no. 3, pp. 8:1–8:22, Aug.
2013.
[5] S. Hirve, R. Palmieri, and B. Ravindran, “Archie: A Speculative Repli-
cated Transactional System,” in Proceedings of the 15th International
Middleware Conference, ser. Middleware, 2014, pp. 265–276.
[6] T. Kraska, G. Pang, M. J. Franklin, S. Madden, and A. Fekete, “MDCC:
Multi-data Center Consistency,” in EuroSys, 2013, pp. 113–126.
[7] H. Mahmoud, F. Nawab, A. Pucher, D. Agrawal, and A. El Abbadi,
“Low-latency Multi-datacenter Databases Using Replicated Commit,”
Proc. VLDB Endow., vol. 6, no. 9, pp. 661–672, Jul. 2013.
[8] J. Gray and L. Lamport, “Consensus on Transaction Commit,” ACM
Trans. Database Syst., vol. 31, no. 1, pp. 133–160, Mar. 2006.
[9] “Google Cloud Spanner - https://cloud.google.com/spanner/.”
[10] I. Moraru, D. G. Andersen, and M. Kaminsky, “There is More Consensus
in Egalitarian Parliaments,” ser. SOSP, 2013, pp. 358–372.
[11] Y. Mao, F. P. Junqueira, and K. Marzullo, “Mencius: Building Efﬁcient
Replicated State Machines for WANs,” ser. OSDI, 2008, pp. 369–384.
[12] A. Turcu, S. Peluso, R. Palmieri, and B. Ravindran, “Be General and
Don’t Give Up Consistency in Geo-Replicated Transactional Systems,”
ser. OPODIS, 2014, pp. 33–48.
[13] L. Lamport, “Generalized Consensus and Paxos,” Microsoft Research,
Tech. Rep. MSR-TR-2005-33, March 2005.
[14] S. Peluso, A. Turcu, R. Palmieri, G. Losa, and B. Ravindran, “Making
fast consensus generally faster,” in DSN, 2016, pp. 156–167.
[15] L. Lamport, “Fast Paxos,” Distributed Computing, vol. 19, no. 2, pp.
[16] P. Sutra and M. Shapiro, “Fast Genuine Generalized Consensus,” ser.
79–103, 2006.
SRDS, 2011, pp. 255–264.
[17] W. Wei, H. T. Gao, F. Xu, and Q. Li, “Fast mencius: Mencius with low
commit latency,” in IEEE INFOCOM, 2013, pp. 881–889.
[18] J. Du, D. Sciascia, S. Elnikety, W. Zwaenepoel, and F. Pedone, “Clock-
RSM: Low-Latency Inter-datacenter State Machine Replication Using
Loosely Synchronized Physical Clocks,” ser. DSN, 2014, pp. 343–354.
[19] M. J. Fischer, N. A. Lynch, and M. S. Paterson, “Impossibility of
Distributed Consensus with One Faulty Process,” J. ACM, vol. 32, no. 2,
pp. 374–382, Apr. 1985.
[20] R. Guerraoui and A. Schiper, “Genuine Atomic Multicast in Asyn-
chronous Distributed Systems,” Theor. Comput. Sci., vol. 254, no. 1-2,
pp. 297–316, Mar. 2001.
[21] R. Guerraoui and L. Rodrigues, Introduction to Reliable Distributed
Programming. Springer, 2006.
[22] L. Lamport, “Future directions in distributed computing,” A. Schiper,
A. A. Shvartsman, H. Weatherspoon, and B. Y. Zhao, Eds., 2003, ch.
Lower Bounds for Asynchronous Consensus, pp. 22–23.
[23] B. Arun, S. Peluso, R. Palmieri, G. Losa, and B. Ravindran, “Speeding
up Consensus by Chasing Fast Decisions,” Tech. Rep., 2017. [Online].
Available: https://arxiv.org/abs/1704.03319
[24] L. Lamport, Specifying Systems: The TLA+ Language and Tools for
Hardware and Software Engineers. Addison-Wesley Longman Pub-
lishing Co., Inc., 2002.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:00:16 UTC from IEEE Xplore.  Restrictions apply.