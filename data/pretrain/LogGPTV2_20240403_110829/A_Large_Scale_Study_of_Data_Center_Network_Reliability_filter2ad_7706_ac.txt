the incident by manually resetting the load balancer and configuring
a particular load balancer setting. The engineer closed the SEV on
January 25, 2012 at 7:47 am PST.
4.3 Analytical Methodology
We examine two sets of data for our study. For intra data center
reliability, we use seven years of service-level event data collected
from our SEV database (cf. §4.2). For inter data center reliability we
use eighteen months of data collected from vendors on fiber repairs
collected between October 2016 and April 2018. We describe the
analysis for each data source below.
4.3.1
Intra Data Center Networks. For intra data center reliabil-
ity, we study the network incidents in three aspects: (1) root cause,
(2) network device types, and (3) network architecture (cluster-
based design versus data center fabric).
We use the root causes chosen by the engineers who authored
the corresponding SEV reports. The root cause category (listed in
Table 2) is a mandatory field in our SEV authoring workflow.
To classify network incidents by the type of offending devices, we
leverage the naming convention enforced by Facebook where each
network device is named with a unique, machine-understandable
string prefixed with the device type. For example, every rack switch
has a name prefixed with “rsw.”. Therefore, by parsing the prefix
of the name of the offending device, we are able to classify the SEVs
based on the device types.
We also classify network incidents based on network architec-
ture. Recall from Figure 1, CSA and CSW belong to classic cluster-
based networks, and ESW, SSW, and FSW devices are a part of the
data center fabric.
4.3.2
Inter Data Center Networks (Backbone). For inter data cen-
ter reliability, we study the reliability of end-to-end fiber links (§3.2)
based on repair tickets from fiber vendors whose links form Face-
book’s backbone networks that connect the data centers. Facebook
has extensive monitoring systems that check the health of every
fiber link, as unavailability of the links could significantly affect
the ingress traffic or can disconnect a data center from the rest of
our infrastructure.
When the vendor starts repairing a link (when the link is down)
or performing maintenance for a fiber link, Facebook is notified via
email. The email is in a structured form, including the logical IDs of
the fiber link, the physical location of the affected fiber circuits, the
starting time of the repair/maintenance, the estimated duration, etc.
Similarly, when the vendor completes the repair/maintenance of a
link, they send an email for confirmation. The emails are automati-
cally parsed and stored in a database for later analysis. We examine
the eighteen months of repair data in this database ranging from
October 2016 to to April 2018. From this data, we measure the mean
time to incident (MTTI) and mean time to repair (MTTR) of fiber
links.
4.3.3 Limitations and Conflating Factors. We found it challeng-
ing to control for all variables in a longitudinal study of failures at
a company of Facebook’s scale. So, our study has limitations and
conflating factors:
• Absolute versus relative number of failures. We cannot re-
port the absolute number of failures for legal reasons. Instead,
we report failure rates using a fixed baseline when the trend of
the absolute failures aids our discussion.
• Logged versus unlogged failures. Our intra data center net-
work study relies on SEVs reported by employees. While Face-
book fosters a culture of opening SEVs for all incidents affecting
production, we cannot guarantee our incident dataset is exhaus-
tive.
• Technology changes over time. Switch hardware consists of
a variety of devices sourced and assembled from multiple ven-
dors. We do not account for these factors in our study. Instead,
A Large Scale Study of Data Center Network Reliability
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
Table 2: Common root causes of intra data center network incidents in a seven-year period from 2011 to 2018.
Category
Maintenance
Hardware
Configuration
Bug
Accidents
Capacity
Undetermined
Distribution Description
17%
13%
13%
12%
10%
5%
29%
Routine maintenance (for example, upgrading the software and firmware of network devices).
Failing devices (for example, faulty memory modules, processors, and ports).
Incorrect or unintended configurations (for example, routing rules blocking production traffic).
Logical errors in network device software or firmware.
Unintended actions (for example, disconnecting or power cycling the wrong network device).
High load due to insufficient capacity planning.
Inconclusive root cause.
we analyze trends by switch type when a switch’s architecture
significantly deviates from others.
• Switch maturity. Switch architectures vary in their lifecycle,
from newly-introduced switches to switches ready for retirement.
We do differentiate between a switch’s maturity in Facebook’s
fleet.
Thoughout our analysis, we state when a factor not under our
control may affect our conclusions.
5 INTRA DATA CENTER RELIABILITY
In this section, we study the reliability of data center networks. We
analyze network incidents within Facebook data centers over the
course of seven years, from 2011 to 2018, comprising thousands of
real world events. A network incident occurs when the root cause
of a SEV relates to a network device. We analyze root causes (§5.1),
incident rate (§5.2), incident severity (§5.3), incident distribution
(§5.4), incidents by network design (§5.5), and switch reliability
(§5.6).
5.1 Root Causes
• Maintenance failure contribute the most documented incidents
• 2× more human errors than hardware errors
Table 2 lists network incident root causes3. If a SEV has multiple
root causes, we count the SEV toward multiple categories. If a SEV
has no root causes, we count the SEV as undetermined. Human
classification of root causes implies SEVs can be misclassified [53,
64]. While the rest of our analysis does not depend on the accuracy
of root cause classification, we find it instructive to examine the
types of root causes that occur in the networks of a large web
service.
We find the root cause of 29% of network incidents is undeter-
mined. We observe these SEVs correspond typically to transient and
isolated incidents where engineers only reported on the incident’s
symptoms. Wu et al. noted a similar fraction of unknown issues
(23%, [75], Table 1), while Turner et al. had a smaller, but higher
fidelity, set of data (5%, [74], Table 5).
Maintenance failures contribute the most documented root causes
(17%). This suggests that in the network infrastructure of a large web
service provider like Facebook, despite the best efforts to automate
and standardize the maintenance procedures, maintenance failures
3We use Govindan et al. [33]’s definition of root cause: “A failure event’s root-cause is
one that, if it had not occurred, the failure event would not have manifested.”
will still occur and lead to disruptions. Therefore, it is important to
build mechanisms for quickly and reliably routing around faulty
devices or devices under maintenance.
Hardware failures represent 13% of the root causes, while human-
induced software issues – bugs and misconfiguration – occur at nearly
double the rate of those caused by hardware failures. Prior studies
the report hardware incident rate, such as Turner et al. [74] and
Wu et al. [75], observe incident rates within 7% of us, suggesting
hardware incidents remain a fundamental. Misconfiguration causes
as many incidents as faulty hardware, something outside of opera-
tor control. This corroborates the findings of prior work that report
misconfiguration as a large source of network failures in data cen-
ters [14, 15, 33, 47, 49, 62, 75], and shows the potential for emulation,
verification, and more extensive automated repair based approaches
to reduce the number of incidents [11–13, 25, 26, 28, 45, 47].
Compared to prior work, we corroborate the lower rate of configuration-
related incidents in Turner et al. [74], Table 5 (9%), but contrast
with Wu et al. [75], Table 1, who found configuration incidents
dominate (38%). We suspect network operators play a large role in
determining how configuration causes network incidents. At Face-
book for example, all configuration changes require code review
and typically get tested on a small number of switches before being
deployed to the fleet. These practices may contribute to the lower
misconfiguration incident rate we observe compared to Wu et al..
A potpourri of accidents and capacity planning makes up the
last 15% of incidents (cf. Table 2). This is a testament to the many
sources of entropy in large-scale production data center networks.
Designing network devices to tolerate all of these issues in practice
is prohibitively difficult (if not impossible). Therefore, one reliability
engineering principle is to prepare for the unexpected in large-scale
data center networks.
Figure 2 further illustrates the root cause breakdown across dif-
ferent types of network devices. Note that the major root cause
categories, including undetermined, maintenance, configuration,
hardware, and bugs, have relatively even representation across all
types of network devices. Some root cause categories are repre-
sented unequally among devices. This is because there are not many
incidents with these root causes, leading to too small population to
be statistically meaningful. For example, ESWs, which represent a
smaller population size compared to SSWs, FSWs, and RSWs, do not
have SEVs with a “bug” root cause. In fact, ESWs run the same Face-
book Open Switching System (FBOSS)-based software stack [5, 69]
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
J.J. Meza et al.
as SSWs and FSWs, which do have network incidents caused by
bugs.
Figure 3: Rate of incidents per year for each device type (nor-
malized to the number of active devices of each type) Note
that the y axis scale is logarithmic and some devices have an
incident rate of 0, e.g., if they did not exist in the fleet in a
year.
Figure 2: Root cause distribution of production incidents by
network device type.
We conclude that among the failures that cannot be remediated
by software, we observe that most failures involve maintenance,
faulty hardware, and misconfiguration as devices and routing con-
figurations become more complex and challenging to maintain.
5.2 Incident Rate
• Higher incident rates on higher bandwidth devices
• Lower incident rates on fabric devices
• Better repair practices lower incident rates
The reliability of data center networks is determined by the
reliability of each interconnected network device. To measure the
frequency of incidents related to each device type, we define incident
rate of a device type as r = i
, where i denotes the number of
n
incidents caused by this type of network device and n is the number
of active devices of that type (the population). Note that the incident
rate could be larger than 1.0, meaning that each device of the target
type caused more than one network incident on average. Figure 3
shows the incident rate of each type of network device in Facebook’s
data centers over the seven-year span. From Figure 3 we draw four
key observations.
First, network devices with higher bisection bandwidth (e.g., Cores
and CSAs in Figure 1) generally have higher incident rates, in com-
parison to the devices with lower bisection bandwidth (e.g., RSWs).
Intuitively, devices with higher bisection bandwidth tend to affect
a larger number of connected devices and are thus correlated with
more widespread impact when these types of devices fail. The an-
nual incidence rate for ESWs, SSWs, FSWs, RSWs, and CSWs in
2017 is less than 1%.
These reliability characteristics largely influence our fault-tolerant
data center network design. For example, we currently provision
eight Cores in each data center, which allows us to tolerate one un-
available Core (e.g., if it must be removed from operation for main-
tenance) without any impact on the data center network. Note that
nearly all of the Cores and CSAs are third-party vendor switches.
In principle, if we do not have direct control of the proprietary
software on these devices, the network design and implementation
must take this into consideration.4 For example, it may be more
challenging to diagnose, debug, and repair switches that rely on
firmware whose souce code is unavailable. In these cases it may
make sense to increase the redundancy of switches in case some
must be removed for repair by a vendor.
Second, devices in the fabric network design (including ESWs, SSWs,
and FSWs) have much lower incident rates compared to those devices
in the cluster design (CSAs and CSWs). There are two differences
between the fabric network devices and the cluster devices: (a) the
devices deployed in data center fabric are built from commodity
chips [4, 5], while devices in the cluster network are purchased from
third-party vendors and (b) the fabric network employs automated
remediation software to handle common sources of failures [65].
Third, the fact that fabric-based devices are less frequently asso-
ciated with failures verifies that fabric-based data center network,
equipped with automated failover and remediation, is more resilient
to device failures. Specifically, we can see excessive CSA-related
incidents during 2013 and 2014, where the number of incidents
4 Facebook has been manufacturing customized RSWs and modular switches since
2013. Please refer to the details in [4–6, 69].
0.00.20.40.60.81.0MaintenanceHardwareConfigurationBugAccidentsCapacity planningUndeterminedFraction of incidentsCoreCSACSWESWSSWFSWRSWClusterFabric1E-51E-41E-31E-21E-11E+01E+12011201220132014201520162017Incidents per deviceCoreCSACSWESWSSWFSWRSWFabric deployedAutomated repairenabledCoreCSAESWCSWFSWSSWRSWClusterFabricA Large Scale Study of Data Center Network Reliability
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
distributed in roughly the same proportion (85%, 10%, and
5% for SEV levels 3, 2, and 1, respectively).
(2) Data center fabric network devices have the lowest number
of incidents among all network devices. ESWs account for
3% of SEVs, SSWs for 2%, and FSWs for 8%.
(3) Compared to cluster based data center network devices
(CSAs and CSWs), fabric network devices typically have
lower impact, with 66% fewer SEV1s, 33% more SEV2s (though
the overall rate is still relatively low), and 52% fewer SEV3s.
exceeds the number of CSAs (with the incident rate as high as 1.7×
and 1.5×, respectively). These high incidence rates were motivation
to transition from the cluster network to fabric network.
Fourth, the CSA-related incident rate decreased in 2015, while
the Core-related incident rate has generally increased from pre-2015
levels. This trend can be attributed to three causes: (1) the decreasing
size of the CSA population, (2) the decreased development of their
software as they were being replaced by newer fabric networks,
and (3) new repair practices that were adopted around the time. For
example, prior to 2014, network device repairs were often performed
without draining the traffic on their links. This meant that in the
worst case, when things went wrong, maintenance could affect
a large volume of traffic. Draining devices prior to maintenance
provided a simple but effective means to limit the likelihood of
repair affecting production traffic. In addition, the data center fabric,
with its more resilient design with higher path diversity, has been
proved to be more amenable to automated remediation.
Network devices with higher bisection bandwidth have a higher
likelihood of affecting service-level software and that network de-
vices built from commodity chips have much lower incident rates
compared to devices from third-party vendors due to the ease of
integration with automated failover and remediation software.
5.3 Incident Severity
• Core devices have the most incidents, but low severity
• Fewest incidents on fabric devices
• Lowest severity on fabric devices
Not all incidents are created equal. Facebook classifies incidents