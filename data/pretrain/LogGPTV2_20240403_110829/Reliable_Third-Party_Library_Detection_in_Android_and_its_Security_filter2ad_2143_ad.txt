7
8
9
10
/com/sina/sso
/com/ut/device
/com/nineold/androids/animation
/com/alipay/android
/m/framework/utils
/com/google/gson
/com/android/vending
/com/alipay/mobilesecuritysdk
/com/tencent/mm
/cn/sharesdk/wechat
# of mutations
in each identiﬁed
library
# of mutations
in total
84
10
79
381
55
421
161
173
288
168
175
42
222
2,485
131
2,422
2,126
463
1,552
495
Fig. 6: Distribution of different multi-package library instances re-
garding the number of structures.
TABLE VI: Five shared packages and evolved libraries.
Shared packages
/cn/sharesdk/framework
/com/weibo/sdk
/cn/emagsoftware/sdk
/com/umeng/common
/com/mobi/tool
Evolved libraries
/cn/sharesdk/douban
/cn/sharesdk/sina
/cn/sharesdk/wechat
/cn/sharesdk/oneshare
/cn/sharesdk/tencent
/cn/sharesdk/twitter
/cn/sharesdk/google
/cn/sharesdk/whatsapp
/com/weibo/net
/com/weibo/android
/cn/emagsoftware/android
/cn/emagsoftware/sms
/com/umeng/update
/com/umeng/analystic
/com/umeng/newxp
/com/umeng/socilize
/com/mobi/controller
/com/mobi/weather
/com/mobi/assembly
(56.7% of the multi-package instances). Our experiments also
report that a library could have 214 different structures at most.
Further investigation of multi-package libraries also reports
that some packages are shared by several multi-package li-
braries. Those shared packages usually provide some common
utilities. Table VI presents 5 packages that are shared by at
least two libraries. Given the observation that the ﬁrst two
segments of these package names are the same, we assume
that they should come from the same developers.
Many library names have three or even more segments (e.g.,
/com/facebook/util has three “segments”). However,
we observe that many of the ﬁrst two segments of package
names are identical. We report that there are a total of 18,594
different kinds of ﬁrst two segments in the outputs of LibD; we
list the top ten in Table VII. Note that if the ﬁrst two segments
of two library names are identical, they are likely from the
same developers. In other words, Table VII shows that most
library developers prefer to provide a series of libraries instead
of one.
2) Obfuscated Libraries: LibD is designed to address
name-based obfuscation techniques. Obfuscators (e.g., Pro-
Guard [23]) replace the library name with several meaningless
strings while preserve the original directory structures. Our
experiment results report two kinds of renaming strategies. The
ﬁrst one obfuscates the library full names; all the names in the
directory structures are replaced with meaningless strings, such
as c/a/b or u/y/e. For such obfuscation, we are unable
10
app. In this paper, a code block is deﬁned as a library instance
or a code component consisting of one or multiple functions
that provides a complete implementation of some speciﬁc
functionality. In our improved vulnerability detection scheme,
the preprocessing module, which employs LibD to dissect the
apps, is conﬁgured with a threshold of one to cover all code
blocks in an app.
With this improved scheme, we expect that the analysis
efﬁciency will be boosted. Recall that our analysis has shown
that a lot of third-party libraries are integrated into the apps.
Such repeated code blocks will cause the original SmartDroid
system to repeat the vulnerability analysis towards the same
code block multiple times, and the repeated analysis would
waste considerable amount of computing resources and time.
As our library detection approach is able to rule out libraries
from apps, the new detection scheme can avoid much of the
redundant analysis. When an app is sent to SmartDroid, we
try to prune the previously analyzed libraries and only deliver
the fresh code components to the detection system.
However,
this improvement over SmartDroid introduces
a new problem that potentially affects the effectiveness of
the detection process. As previously mentioned, SmartDroid
veriﬁes static detection results through dynamic analysis. To
do that, SmartDroid will need to construct traces that can reach
the vulnerable program points. After our modiﬁcation, traces
extracted by SmartDroid are conﬁned within individual code
blocks instead of whole apps. Due to the event-driven pro-
gramming paradigm of Android apps, many execution paths
cannot be statically captured. Therefore, there is a possibility
that
traces extracted from single code blocks lack certain
preﬁxes or sufﬁxes1 and cannot be dynamically veriﬁed.
To estimate the impact of this problem, we randomly
sampled 1,000 apps to get
the proportion of apps whose
vulnerabilities are enclosed by a single code block after an
app is dissected by LibD. The analysis was conducted in the
following steps:
• Use SmartDroid to statically analyze the 1,000 apps, tar-
geting four kinds of vulnerabilities, i.g., DoS, WebView
leak, SSL Hijacking, and FileCross. Details about these
vulnerabilities are presented in §VI-A.
• For those that SmartDroid considers vulnerable, ask
SmartDroid to further construct execution traces that can
potentially trigger the vulnerabilities.
• Use LibD to dissect the 1,000 apps into code blocks,
which are essentially library candidates.
• For each constructed execution trace, examine if it ﬂows
from one code block to another. This step was manually
performed by four researchers familiar with SmartDroid.2
With the procedure described above, SmartDroid reported
1,390 vulnerable program points and LibD dissected the
1,000 apps into 3,330 code blocks. The manual inspection
conﬁrmed that all execution traces constructed by SmartDroid
are contained within a single code block. Therefore, we expect
1For some vulnerabilities, e.g., the DoS vulnerabilities, the program point
that triggers the error is not where the failure manifests. In such occasions, a
vulnerability cannot be veriﬁed until the execution reaches the manifest point.
2All participants were afﬁliated with Chinese Academy of Sciences, three
of whom are not the authors of this manuscript.
that our acceleration scheme will not have a signiﬁcant impact
on the accuracy of SmartDroid.
D. Acceleration
In order to accelerate SmartDroid, we add a preprocessing
module and a feedback module to the original SmartDroid
system. The preprocessing module is used to ﬁrst dissect the
integrated apps into code blocks. After that, each block is
analyzed separately by the original part of SmartDroid and
assigned a label Lsec indicating the analysis result. A label
is either SECURE or the vulnerability type detected in this
block. An empty label value NULL indicates that the block
is not analyzed yet.
After SmartDroid ﬁnishes analyzing the code blocks and
verifying the results through dynamic analysis, the feedback
module will update the analysis results of these blocks stored
in the preprocessing module. Next time the same code block is
encountered, the preprocessing module will skip the analysis
and reuse the previously obtained results. Algorithm 3 and 4
describes the processes of these two modules, respectively.
Algorithm 3: Preprocessing Algorithm
Input: Android app data set Sapp
Output: Classiﬁcation pool Scbs and bidirectional
block-to-app mapping M
1 Sapp ← Android apps data set;
2 Scbs ← ∅;
3 M ← ∅;
4 foreach app ∈ Sapp do
5
Sinst ← Funcinst (app);
to dissect the app*/
foreach block ∈ Sinst do
if block /∈ Scbs then
6
7
8
9
/* Funcinst invokes LibD
add new relations (app, block) and
(block, app) into M ;
Add block into Scbs with its Lsec as NULL;
10 return (Scbs , M )
In Algorithm 3, LibD is employed to dissect an input app
into blocks. We also build the mapping between apps and
blocks, which is a many-to-many relation, i.e, each app con-
tains more than one code block, and the code block instances,
especially the third-party libraries, can also be shared among
different apps. This mapping makes it much more convenient
to trace the apps from the instances and vice versa. We then
insert these fresh instances into our classiﬁcation pool. For
each newly extracted instance, we ﬁrst check if it has been
recorded already. If a block instance is never analyzed before,
we create a fresh record in the pool. The output of this
algorithm is a set of blocks Scbs and the bidirectional mapping
M .
Algorithm 4 illustrates the updating process of the classiﬁca-
tion pool, where the analysis results from Smartdroid are used
to updated the security labels of blocks in the classiﬁcation
pool. As the analyzed code accumulate, and the per-app
analysis process will speed up since repeatedly appearing code
12
Algorithm 4: Feedback Algorithm
Input: classiﬁcation pool Scbs with unanalyzed blocks
Output: updated Scbs
1 foreach w ∈ Scbs do
2
if Lsec of w is NULL then
3
4
sec label ← F uncsmartdroid (w);
update Lsec of w to be sec label ;
5 return updated Scbs
blocks will not be re-analyzed and the previously generated
results will be reused.
E. Implementation
Fig. 9 presents the modiﬁed SmartDroid system. The pre-
processing and feedback modules are used to instrument the
inputs and outputs of the SmartDroid system.
According to Algorithm 3, preprocessing module uses LibD
to dissect Android apps into code blocks and inserts each of
the block into a classiﬁcation pool. This pool is divided into
three areas: the secure block area, the vulnerable block area,
and the unanalyzed block area. As shown in Fig. 9, SmartDroid
is directed to only focus on the unanalyzed blocks. For this
purpose, we modiﬁed the SmartDroid system to change its
input format. In the original SmartDroid system [17], the input
is a complete Android app, after our modiﬁcation the input
becomes individual code blocks.
The general workﬂow of our modiﬁed system is mostly
identical to that of the original system. We ﬁrst transmit the
unanalyzed blocks to the static analysis module of SmartDroid.
The system will collect all the suspicious paths in the code
blocks by comparing all
the paths of the ACG with the
predeﬁned vulnerable traces. After that, these suspicious paths
are sent to the dynamic veriﬁcation sub-module. This sub-
module executes each of the suspicious paths to verify if these
paths can trigger the ﬂaws in reality. If one path is able to
make any of the known ﬂaws happen during the veriﬁcation,
this block is considered as vulnerable and its corresponding
record would be updated in the following feedback module.
F. Acceleration Evaluation
We deployed our modiﬁed detection system on the Open-
Stack Platform and leveraged 100 virtual machines to analyze
apps in parallel. In general, we evaluate our acceleration
scheme by measuring (1) the vulnerability discovery accuracy
and (2) the acceleration efﬁciency in terms of the processing
time.
1) Accuracy: Essentially, four kinds of widely existing
ﬂaws, DoS, WebView leaks, SSL Hijacking and FileCross,
are studied in this step. The details of these vulnerabilities
are presented in Section VI. Considering SmartDroid as a
well-developed tool for vulnerability detection, we take the
detection result of the original system towards our app data
set as the baseline to assess the accuracy of our modiﬁed
system; we measure the false positive and false negative rate
in this step. Table X reports the comparison result. Here, we
list the vulnerable apps detected by both systems. Note that
since the same vulnerability assertion approach is deployed in
both systems, for a benign app, our modiﬁed system should
not mark it as “vulnerable”. In other words, we expect that
no false positives should be reported. Our evaluation result is
consistent with this intuition, as listed by Table X.
Comparing with the detection results of the original Smart-
Droid system, we report that most of the false negative rates
are negligible (half of them are zero), and the modiﬁed Smart-
Droid system is still well-performing in detecting vulnerable
Android apps.
To understand what factors have led to the errors, we
sampled some of the false negatives and studied them case by
case. For each vulnerability class, we randomly selected ﬁve
false negatives. For classes with less than ﬁve false negatives,
we took all available cases. As such, a total of 38 false
negatives were analyzed in depth. Through the analysis, the
main cause of false negatives is the potentially incomplete
execution traces fed to the dynamic veriﬁcation module, as
discussed in §V-C. Among the 38 cases we analyzed, 33 are
due to the lack of necessary preﬁxes, i.e., the vulnerabilities
could not be triggered, and 5 are due to the lack of sufﬁxes,
i.e., the failures failed to manifest.
2) Acceleration Effects: To evaluate the processing speed
increase of the modiﬁed system, the accumulative time con-
sumption as well as the number of extracted suspicious paths
are studied in this section.
The time consumption is a key criteria to measure the
efﬁciency of the system. In general, the original SmartDroid
system takes ﬁve minutes to analyze one app on average.
The static analysis of suspicious path extraction takes around
one minute while the dynamic veriﬁcation step takes about
four minutes. While the dynamic veriﬁcation step cannot
be optimized, as aforementioned, the modiﬁed system shall
effectively reduce the number of suspicious paths in each app
that need to be veriﬁed.
Since the modiﬁed system “caches” the analysis results, we
expect that the analysis speed would be constantly reduced
by the accumulation of analyzed apps. We fed the modiﬁed
system with in total 1,427,395 Android apps and measured the
processing time after analyzing certain amount of cases. The
results are presented in Table XI, which are consistent with
our expectation.
As reported in Table XI, the original and the modiﬁed
systems spend about the same amount of time in analyzing the
ﬁrst 100 apps. However, the modiﬁed system becomes about
6 minutes faster when 1,000 apps have been fed. This “gap”
keeps growing as more apps are analyzed. In total, the original
system takes more than 27 days to ﬁnish all tasks. That is, the
modiﬁed system saves almost one month to analyze the data
set comparing with the original analysis system.
We also evaluated the efﬁciency of the modiﬁed system
by training from the 1,427,395 apps data set and perform
cross data set validation. To this end, we additionally collected
370,507 apps which are not in the original dataset used for
evaluation We then randomly selected 1,000 apps from new
dataset and run both analysis systems to record the total
analysis time. On average, the original system can process
15 apps per minute while our modiﬁed system can handle
13
structure and functionality, making it non-trivial to identify the
vulnerable versions.
One of the design goals of LibD is to ﬁnd the sweet spot
for the sensitivity of library signatures. On the one hand,
we would like the signature to be sensitive enough to reﬂect
subtle changes made to the library code such that the different
versions of the same library can be effectively distinguished.
On the other hand, we also want to avoid designing overly
ﬁne-grained signatures to keep the computation of signatures
efﬁcient enough for large-scale analysis. In this section, we
demonstrate that LibD can be used to detect the vulnerable
variants of the same Android library among millions of apps.
A. Inspected Vulnerabilities
To evaluate if our approach is sensitive enough to iden-
tify common programming ﬂaws in vulnerable third-party
libraries, we choose four widely-spread types of Android
vulnerabilities, including deny of services (DoS), WebView
information leaks, man-in-the-middle (MITM) SSL hijacking,
and the “FileCross” problems affecting Android browsers.
• DoS. An attacker using Deny of Service ﬂaws can cause
a running computer or server to crash, for example, by
exploiting the overﬂows in memory. On the Android
platform, the type of attacks can crash the smartphone.
With a proper use of the integer underﬂow (e.g., CVE-
2017-14496), an adversary is able to deploy a remote
DoS attack. In the CVE database3 platform, the DoS
vulnerabilities account for 18.6% of all
the uploaded
vulnerabilities. This type of ﬂaws pervasively exists in