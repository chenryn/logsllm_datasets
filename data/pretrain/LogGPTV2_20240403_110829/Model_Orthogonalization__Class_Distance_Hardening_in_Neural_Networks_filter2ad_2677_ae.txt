3.13%
0.98%
0.58%
0.45%
0.62%
-
1.06%
1.24%
1.99%
0.55%
0.40%
0.66%
-
1.16%
2.00%
1.49%
0.80%
1.52%
0.91%
MOTH. However, due to its poor cost-effectiveness, Pairwise
can take up to 1683.52 minutes to train a model, which is
22.75 times slower than MOTH (see results on a NiN model
for GTSRB in Table V in Appendix X-C). The two backdoor-
erasing techniques have limited improvements on class dis-
tance, with 7.60% for NC and -4.59% for NAD on average.
This is reasonable as these backdoor-erasing approaches were
originally designed for removing potential backdoors injected
in the model. They are supposed to have little impact on
benign models and hence do not enlarge class distances for
normal pairs. Overall, MOTH outperforms NC, NAD, UAP and
Universal in terms of class distance hardening, and Pairwise
in terms of efficiency with similar distance improvement.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
101381
Fig. 10: Comparison of the average class distance
We have similar observations on adversarially trained models
as shown in Table II. Adversarially trained models usually
have large accuracy degradation compared to their naturally
trained counterparts. Their class distances are much larger.
However, those class distances are still not optimal. MOTH
can further improve the class distance by 53.18% with only
0.58% accuracy degradation and no robustness degradation on
average. We observe a slight robustness drop (0.30%) for a
NiN model on CIFAR-10. This is because there may still exist
data points close to the decision boundary and hence the model
is vulnerable to adversarial attacks as discussed in Section III.
Universal has a similar performance on adversarially trained
models as on natural ones. It can increase class distance
from 19.02% to 57.85% with an average 16.73% lower than
MOTH. Pairwise has low efficiency as discussed earlier. For
adversarially trained models, it has a even larger time cost,
with a maximum training time of 2122.95 minutes, which is
around one and a half days (see Table VI in Appendix X-C). Its
class distance improvement is similar to MOTH. NC and NAD
have slightly better performances on adversarially trained
models than naturally trained models, with an average of
10.91% and 4.84%, respectively, which are inferior to other
baselines and MOTH. Since Pairwise has a similar performance
on both naturally trained and adversarially trained models
to MOTH, we further study their efficiency for producing a
hardened model. Please see details in Appendix X-D.
We also compare the class distances of the naturally trained,
adversarially trained models, and their hardened versions by
MOTH. Figure 10 presents the average class distance for
the different models. For CIFAR-10, we observe that MOTH
can improve the class distances of both the naturally and
adversarially trained models. But the hardened models based
on the natural models have smaller distances compared to the
adversarial models. This is because these adversarial models
have a low accuracy from 73.94% to 78.45% as shown in Ta-
ble II, whereas the hardened models have 86.81% to 91.48% as
shown in Table I. For the SVHN and GTSRB datasets, observe
that the hardened models based on the natural models have
larger class distances than the adversarially trained models.
The accuracy of the hardened models is also higher than that
of the adversarial models, e.g., 94.99% vs. 91.63% for NiN
on SVHN (see Table I and Table II). This is consistent with
our discussion in Section III that adversarial training can alter
the decision boundary to have larger class distances. However,
it may not achieve maximum as shown in Figure 4(b). On
the other hand, orthogonalization can achieve much larger
distances starting from robust models, with the sacrifice of
TABLE II: Comparison of different methods on hardening
class distance for adversarially trained models. The fifth col-
umn (Rob.) denotes model robustness on the validation set.
The last two columns (ADeg. and RDeg.) show the degrada-
tion of test accuracy and model robustness, respectively.
D M
Increase ADeg. RDeg.
Method
Time
Dist.
Rob.
Acc.
0
2
t
e
N
s
e
R
N
N
i
9
1
G
G
V
N
N
i
2
3
t
e
N
s
e
R
0
1
-
R
A
F
I
C
N
H
V
S
Average
-
Adversarial 74.87% 42.80% 462.50 209.59
2.99% 0.00% 0.30%
78.73 210.73
NC 74.96% 42.50%
2.61 212.16
2.39% 0.58% 0.30%
NAD 74.29% 42.50%
Universal 73.92% 43.30%
73.00 259.93 25.62% 0.95% 0.00%
Pairwise 73.34% 43.70% 128.46 288.12 39.90% 1.53% 0.00%
38.71 270.57 32.16% 0.83% 0.00%
MOTH 74.04% 43.00%
-
-
-
Adversarial 73.94% 41.40% 430.39 189.49
3.90% 0.19% 3.40%
43.41 188.32
1.26 192.24
1.23% 0.33% 1.90%
35.60 221.53 19.02% 0.40% 0.30%
75.66 271.83 47.97% 0.48% 0.90%
25.57 276.93 51.18% 0.47% 0.30%
NC 73.75% 38.00%
NAD 73.61% 39.50%
Universal 73.54% 41.10%
Pairwise 73.46% 40.50%
MOTH 73.47% 41.10%
-
-
NAD 73.81% 39.90%
Adversarial 76.00% 41.30% 1166.18 201.07
-
NC 75.37% 41.60% 141.38 226.70 17.13% 0.63% 0.00%
5.53% 2.19% 1.40%
Universal 75.06% 42.90% 113.42 298.06 41.56% 0.94% 0.00%
Pairwise 75.31% 42.30% 322.68 359.57 62.19% 0.69% 0.00%
81.54 326.85 53.33% 0.92% 0.00%
MOTH 75.08% 42.50%
4.12 210.16
-
-
-
Adversarial 91.63% 51.20% 132.00
27.38
4.35% 0.00% 9.70%
NC 92.02% 41.50%
1.42 102.76 16.39% 2.90% 20.10%
NAD 88.73% 31.10%
Universal 92.10% 51.80%
56.00 124.04 38.18% 0.00% 0.00%
Pairwise 91.55% 51.10% 156.10 126.88 42.01% 0.09% 0.10%
51.42 136.96 54.10% 0.24% 0.00%
MOTH 91.40% 52.30%
90.10
91.62
-
-
NC 92.25% 49.90%
NAD 90.02% 48.70%
Adversarial 92.94% 58.30% 300.08
43.20
5.81
78.27
-
95.08 26.17% 0.69% 8.40%
75.93
-1.32% 2.92% 9.60%
Universal 92.74% 58.90% 111.42 123.73 57.85% 0.20% 0.00%
Pairwise 92.34% 57.10% 365.30 124.26 58.27% 0.59% 1.20%
MOTH 92.52% 58.90% 111.74 137.68 75.12% 0.42% 0.00%
-
-
Natural 81.88% 47.00% 498.23 153.70
-
66.82 162.49 10.91% 0.21% 4.30%
NC 81.67% 42.70%
3.04 158.65
4.84% 1.79% 6.66%
NAD 80.09% 40.34%
Universal 81.47% 47.60%
77.89 205.46 36.45% 0.41% 0.00%
Pairwise 81.20% 46.94% 209.64 234.13 50.07% 0.68% 0.06%
61.80 229.80 53.18% 0.58% 0.00%
MOTH 81.30% 47.56%
-
-
accuracy (due to adversarial training).
C. Evaluation on TrojAI Models
The models from the TrojAI competition were trained by
NIST [42] with various model structures and classification
tasks. These models have already been hardened by adversarial
training as described in the experiment setup. We download a
random set of 30 models from the official website [42] (see
Appendix X-E for how these models are selected) and study
whether we can further improve class distance. Detailed results
can be found in our supplementary material [44]. We have the
same observation as in Table II. Universal can improve the
class distance to some extent with an average of 154.70%
improvement over the original models, inferior to MOTH with
232.39% improvement. TrojAI models are larger and more
complex than the ones used in the previous section. Universal
has lower efficiency, taking 167.92 minutes on average to
train a model, which is 42% slower than MOTH (118.10).
Pairwise has a similar performance (239.98%) on hardening
class distance as MOTH. But it suffers from low efficiency
(1121.97 minutes on average), 10.81 times slower than MOTH.
The results delineate the effectiveness and the efficiency of
MOTH on hardening class distance for pre-trained models.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
111382
(a) CIFAR-10(b) SVHN(c) LISA(d) GTSRBNaturalNat_MOTHAdversarialAdv_MOTHTABLE III: Ablation study on effects of design choices.
Increase Degrad.
Method
Time (min) Distance
Accuracy
Natural
MOTH
w/o symmetric
w/o dynamic adjustment
w/o approx. warm-up
w/o scheduler
56.77
91.52%
29.68
90.34%
35.40 (+19%)
90.69%
44.72 (+51%)
90.65%
90.44%
52.00 (+75%)
88.78% 120.47 (+306%)
-
-
53.49
109.70 108.62% 1.18%
78.90% 0.83%
94.16
95.59% 0.87%
103.45
105.28
99.53% 1.08%
111.09 112.84% 2.74%
D. Ablation Study
MOTH features a few design choices to effectively and
efficiently improve class distance. We study each component
individually to understand the effects of those designs. In
detail, we consider four major parts in MOTH, namely, (1)
symmetric hardening; (2) backdoor reuse and weight adjust-
ment; (3) warm-up through approximation by loss changes;
(4) K-arm scheduler. We conduct an ablation study using a
ResNet20 model on CIFAR-10. Table III shows the effects of
these design choices. Row Natural denotes the original model.
MOTH is the final result of our technique. The following
four rows correspond to the aforementioned four components
that are excluded individually during training. Observe that
without symmetric training, the overall improvement degrades
by 30%, rendering its importance. This is consistent with
our discussion in Section IV-B. The training time without
symmetric hardening also increases as the training needs
to consider both directions of a pair separately. Excluding
dynamic adjustment, the training cost increases by 51% from
29.68 min to 44.72 min. The class distance improvement also
degrades by 10%, indicating that backdoor reuse indeed boosts
performance. The result in the sixth row of Table III shows
that without the approximation of warm-up, the training cost
grows substantially by 75%. The situation can deteriorate for
tasks with more classes. We also investigate the final pairwise
class distances for different warm-up strategies and the results
are almost identical (see details in Appendix X-F). The result
in the last row evidently demonstrates the effectiveness the
scheduler in reducing training cost. Equipped with the sched-
uler, MOTH has a 4x speedup over the one without it, and the
distance improvement is similar.
VI. APPLICATIONS
In this section, we evaluate MOTH in two applications
including reducing false positives for backdoor scanning and
eliminating injected backdoors in existing models.
A. Reducing False Positives
In the TrojAI multi-round competitions for backdoor scan-
ning, performers are asked to identify poisoned models from
a large set consisting of both clean and poisoned models.
As discussed in Section II, due to the existence of small
natural backdoors in clean models, performers struggled in
reducing false positives (i.e., reporting clean models as poi-
soned), especially for round 2. The TrojAI organizer then
leveraged adversarial training to mitigate this issue in the
following rounds, e.g., round 4. However, the top performer
still reports 32 clean models as poisoned in round 4. We
apply MOTH to harden the 32 clean models. The accuracy
(a) Class 1
(b) Class 20
(c) Backdoor
Fig. 11: Example images of class 1 and class 20 from model
ID 905. The backdoor is generated by the top performer.
degradation is unnoticeable (0.13% on average). Details can
be found in our supplementary material [44]. The distance
improvement is similar to that shown in Section V-C and
elided. After orthogonalization, we apply the top-performers’
scanner downloaded from their site [54] to the round 4
datasets again (with the 32 models hardened). The number
of false positives is reduced from 32 to 6, without losing
any true positives. For the remaining 6 false positive models,
we observe that some classes are very similar. We show an
example case of model ID 905 in Figure 11. The first and the
second images are from classes 1 and 20, respectively. The
two signs in the foreground look very similar to each other,
with only one arrow difference in the center. The last image
shows the backdoor generated by the top performer, which
has only small black dots in the center. The class distance
of the pair is 1842 for the original model, and 3964 for the
hardened model. Although MOTH has already improved the
distance by 115%, it is still too small as it corresponds to
around 3964/3 ≈ 36 × 36 pixel changes (224 × 224 for the
input image), which is not distinguishable from the injected
backdoor whose size is 4215. The class distance is bounded
by the natural similarity between the two classes. This further
stresses the importance of publishing orthogonalized class
distances as part of the model specification such that the users
are aware of the inevitable security risks of natural backdoors
between these classes.
B. Eliminating Injected Backdoors
To achieve stealthiness,
injected backdoors are usually
small. The essence of such data poisoning is to bring the
victim and target classes close to each other, namely, only
separated by the small backdoor. MOTH hence can be utilized
to eliminate such backdoors since it enlarges class distance for