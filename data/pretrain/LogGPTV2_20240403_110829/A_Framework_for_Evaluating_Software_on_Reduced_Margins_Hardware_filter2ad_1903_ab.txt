Inform application 
ended
Track state and
 wait for application 
to end
Acknowledge
Classify and store 
results to database
False
True
Terminate?
True
Export result and 
database to user
Fig. 3. Flow chart for the main steps performed by XM2 for the basic case
of an experimental campaign that does not result to crashes. The dark box is
the only state where the target system is conﬁgured at an unreliable state.
API calls are handled transparently. Before a Slave transitions
to the unRel state it notiﬁes the Coordinator ﬁrst. At this
point the Coordinator starts a watchdog which waits for the
application to terminate. The maximum waiting time is equal
to the proﬁled time when the code was executed reliably,
increased by 10%. In case the unRel frequency is lower than
the frequency of the Nominal point, we proportionally increase
the waiting time to match the maximum expected performance
degradation due to frequency scaling. If the Coordinator does
not receive any information about the application status within
this period, the Slave is reset and the corresponding experiment
is ﬂagged as CPU Crash.
If the application terminates abruptly, e.g example due to
executing an Illegal Instruction, the Slave informs the Coor-
dinator and the experiment is classiﬁed accordingly. In case
the application terminates normally, the Slave sends all output
data, as deﬁned by the writeOutput call, to the Coordinator.
Afterwards, the Coordinator invokes the classiﬁer binary to
characterize the experiment. If the experiment is not ﬂagged
as Exact, the Coordinator resets the Slave so that it is re-
initialized to a valid state. The Coordinator then evaluates the
termination criterion and either terminates the campaign or
proceeds to deploy the next experiment. When a campaign
333
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:25:41 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II
RASPBERRY 3B SPECIFICATIONS
System On Chip
Instruction Set
CPU
RAM
Broadcom BCM2837
4x ARM Cortex-A53, 1.2Ghz
ARMV-8
In Order
Dual Instruction Decode and execute
6KB Conditional Predictor
256 Entry Indirect Predictor
NEON advanced SIMD
8 - 64K I-Cache With Parity
8 - 64K D-Cache With Ecc
1 GB LPDDR2 (900MHz)
terminates, XM2 prints the statistics for the different experi-
ment classiﬁcations, as well as the path to the output database.
In the database, we store the classiﬁcation of each experiment
and a path to the raw output data of executions. If there are
more unRel conﬁgurations speciﬁed in the expConf, a new
experimental campaign starts, otherwise the tool terminates.
V. EVALUATION
To evaluate our framework we use three Raspberry PI
3b boards (Table II) as target systems. We set Nominal
conﬁguration to f = 600M Hz, Vdd = 0.8V . Even though
XM2 supports both undervolting and overclocking, for the
evaluation we perform overclocking. We overclock the system
by providing a list of unRel states starting from V =
1.2V, fu = 1370M Hz with intermediate steps increasing fu
by 10M Hz, up to the highest frequency state (V = 1.2V, fu =
1450M Hz). The termination criterion for the experimental
campaign is a number of experiments equal to 2000, which
provides a conﬁdence level of 98% and an error margin of
2.5%. For the evaluation, we use the default classiﬁer.
We use Circle, a C++ library supporting execution on
bare metal, to evaluate the error resiliency of software under
unreliable execution without any interference from the OS
software stack, e.g. scheduler of Linux kernel or background
OS services. Circle provides several C++ classes which se-
lectively enable or use different hardware features (MMU,
Interrupt Support etc.).
The target systems are reset whenever necessary using a
small circuit per system, which employs a transistor operating
as a switch to connect
the respective reset pins on the
Raspberry PI 3b. The circuits are controlled by the monitoring
system through a serial interface.
We evaluate the programmers’ effort to use our tools in
terms of extra lines of code (LOCs) that are introduced to
the source code of an application. Moreover, we quantify
the communication overhead introduced by XM2 between the
Coordinator and the target system. We use three benchmarks:
Blackscholes [11], Inversek2j, DCT. Blackscholes implements
a mathematical model for a market of derivatives, inversek2j
calculates the angles of a 2-joint arm using the kinematic
equation and DCT is a module of the JPEG compression and
decompression algorithm.
10%
8%
6%
4%
2%
0%
)
%
(
d
a
e
h
r
e
v
O
Execution
DCT
Blackscholes
LOCs
Inversek2j
Fig. 4. Overhead of XM2 in terms of execution time and additional lines of
code (LOC) when compared to a native execution and the original version of
the code respectively.
Figure 4-left presents the execution time overhead (%) due
to the communication protocol and data exchange between
the Coordinator and the target. XM2 adds,
in the worst
case (DCT), an extra 5% of execution time compared with
a native execution on the target platform under the same
conﬁguration. The execution time to compute DCT is not
negligible, compared to the time needed to transfer the data.
Consequently, this benchmark results to the highest overhead.
The remaining benchmarks are mainly compute-bound. On
average XM2 introduces an execution time overhead of 2.5%.
Figure 4-right illustrates programmers’ effort to prepare an
application for our framework. In Blackscholes, the developer
needs to unpack and pack the input/output data prior to
transferring them, thus the volume of the new code is equal to
8.7% of the existing one. The remaining two benchmarks are
small in terms of LOCs, so even small code additions produce
a large overhead (% ). In both cases we simply replace the
fread, fwrite functions with readInput, writeOutput of the XM2
API. Moreover, we add two extra function calls to switch
between states. On average, preparing applications for XM2
requires 5.6% extra LOCs.
Figure 5 shows the experimental campaign results evaluat-
ing the reliability of the system under different overclocked
conﬁgurations. Blackscholes uses double precision arithmetic.
Due to the representation of such numbers, faults are un-
100%
80%
60%
40%
20%
0%
)
%
(
s
t
n
e
m
i
r
e
p
x
e
f
o
n
o
i
t
a
c
i
f
i
s
s
a
l
C
1
4
2
0
1
4
3
0
1
4
4
0
1
4
5
0
1
4
2
0
1
4
3
0
1
4
4
0
1
4
5
0
1
4
2
0
1
4
3
0
1
4
4
0
1
4
5
0
Blackscholes
Inversek2j
DCT
Exact
SDC Data Abort
Illegal Instruction CPU Crash
Fig. 5. Experimental results for different applications and different over-
clocked conﬁgurations.
334
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:25:41 UTC from IEEE Xplore.  Restrictions apply. 
100%
80%
60%
40%
20%
0%
)
%
(
s
t
n
e
m
i
r
e
p
x
e
f
o
n
o
i
t
a
c
i
f
i
s
s
a
l
C
a
d
d
S
u
b
m
u
l
d
i
v
v
a
d
d
.
f
3
2
v
s
u
b
.
f
3
2
v
m
u
l
.
f
3
2
v
d
i
v
.
f
3
2
a
n
d
o
r
x
o
r
Integer
Floating Point
Logic
100%
80%
60%
40%
20%
0%
)
%
(
s
t
n
e
m
i
r
e
p
x
e
f
o
n
o
i
t
a
c
i
f
i
s
s
a
l
C
S
r
t
R
n
d
S
r
t
R
n
d
S
r
t
R
n
d
S
r
t
R
n
d
1420
1430
1440
1450
Exact
SDC Data Abort
Illegal Instruction CPU Crash
Exact
SDC Data Abort
Illegal Instruction CPU Crash
Fig. 6. Experimental results of the instruction error resiliency characterization
when Vu = 1.2V, fu = 1450M Hz. The X-axis shows the different
microkernels and the Y-axis presents the classiﬁcation of the experiments
according to the effects of overclocking on execution.
Fig. 7. Experimental results stressing the branch predictor for the two
microkernels for different overclocked frequencies (fu).
likely to be masked. Therefore, this benchmark suffers the
highest percentage of SDCs (up to 26%) when executed on
fu = 1440M Hz. Inversek2j uses primarily trigonometric
functions, which heavily rely on branches. The experiments
indicate that faults corrupt
the computation of the target
address, resulting in decoding memory that does not contain
instructions. Consequently, 74% of the experiments result
to Illegal Instructions when executed at fu = 1440M Hz.
Finally, 36% of DCT experiments result in CPU Crash when
executed at the same frequency. This benchmark employs six
nested loops to iterate through the image pixels and apply the
coefﬁcient transformation. Corruptions in the control ﬂow of
these loops often results to inﬁnite loops. Therefore, execution
is terminated by the watchdog and experiments are classiﬁed
as CPU crashes.
VI. CASE STUDIES
In this section we demonstrate the versatility of XM2 using
three case studies. The ﬁrst two studies focus on recording
and analyzing the behavior of small kernel programs running
on the overclocked Raspberry PI platform, whereas the third
study focus on the behavior of these kernels running on the
undervolted Skylake processor.
A. Instruction Level Error Resiliency Analysis
Initially, we employ our tool to assess the resiliency of ARM
instruction when executed individually in the overclocked
Cortex A53 pipeline causing minimal disruptive events such
as cache misses and branch mispredictions (Listing 4). We
selected a subset of instructions that perform integer (add, sub,
( ) {
r 2
r 0
r1 ,
r3 ,
f o r
i n s t r u c t i o n r0 ,
i n s t r u c t i o n r3 ,
. . .}
Listing 4. Template of microkernels used to stress the same execution path
of the Pipeline.
mul, div,), ﬂoating ( vadd.f32, vsub.f32, vmul.f32, vdiv.f32), and
boolean (or, and, xor) arithmetic. The microkernels use only
four registers, two as input, one for temporary storage, and
one to accumulate the ﬁnal result which is propagated to the
Coordinator after the end of the execution.
In our case, we evaluate the error resiliency of instructions
using one input ﬁle which sets the register to the value of 1.
However the user could deﬁne multiple input ﬁles and perform
multiple experimental campaigns. Finally, the Nominal and
unRel conﬁgurations are the same as in the previous section.
We observe that microkernels which execute multiple times
the same instruction, regardless of the instruction, produce
exact results even when we increase the CPU frequency by
20% (from fu = 1200M Hz to fu = 1440M Hz). When we
overclock by an additional 10M Hz the reliability of most
kernels signiﬁcantly drops as shown in Figure 6. This abrupt
fall in reliability conﬁrms previous ﬁndings that there are
(V, f ) settings called Points of First Failure (PoFF) at which