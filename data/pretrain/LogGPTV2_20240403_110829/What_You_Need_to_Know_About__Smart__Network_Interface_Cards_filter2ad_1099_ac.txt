### Summary
Our research demonstrates the potential for a denial of service (DoS) attack on the packet classifier of Network Interface Cards (NICs) under test, or a significant reduction in throughput by up to 70 Gbps. This is achieved by updating the classifier's rules using the same CPU core that processes traffic. This technique is commonly used in sharded high-speed data planes, such as per-connection network functions (NFs). Realistic datacenter workloads generate new connections at rates ranging from 4K to 400K per second. Our findings indicate that offloading applications with highly dynamic tables, such as cloud NATs and load balancers, to the analyzed NICs would not provide any performance benefits. Additionally, all tested NICs exhibit similar performance across our experiments, with the NVIDIA ConnectX-6 NIC showing slightly less throughput degradation in the experiments depicted in Figures 2a and 2b [17].

### 3.2 Rule Operations Analysis
In this section, we focus on the performance of rule update operations, including insertions, deletions, and modifications. The shorter the update completion time, the lower the disruption to forwarded traffic. Our analysis reveals three key findings:

1. **Rule Insertion Performance**: Modern NICs can handle nearly 500K insertions per second, but the performance varies significantly based on the type and number of fields matched by the packet classifier and the actions applied by the rules. Surprisingly, inserting IPv4 rules in Table 0 is much slower than inserting IPv6 rules.
2. **Network Slicing Protocols**: The cost of installing VLAN-based rules for network slicing is substantially lower than the cost of installing GRE/VXLAN/GENEVE-based rules.
3. **Rule Modification Operations**: None of the analyzed NICs support atomic rule modifications. Instead, one must delete the old rule and insert a new one. By directly modifying the content of the exact match tables in the NIC memory, we can decrease rule modification time by 80% compared to the standard API.

#### 3.2.1 Insertion/Deletion of Rules
We measured the rule insertion rate supported by an NVIDIA Mellanox ConnectX-5 NIC in Tables 0 and 1. Using a single CPU core, we inserted rules ranging from 1 to 65536 and recorded the time taken. Figure 5a shows the insertion rate for five types of rules matching different combinations of fields, such as Ethernet, IPv4, IPv6, and TCP. We observed a significant difference between IPv4 and IPv6: IPv4 rules result in a sharp slowdown, especially with 16K entries. Profiling revealed that IPv4 rules are installed directly by the kernel in hardware using the firmware API, while IPv6 rules are managed by the user-level DPDK driver. In contrast, Figure 5b shows that for Table 1, IPv4 rules have a 12% higher insertion rate than IPv6 rules, as both are managed by the DPDK driver.

We also investigated the impact of different network slicing protocols on the insertion rate into Table 1. Figure 5c shows that rules matching VLAN tags can be installed up to 50% faster than those relying on other virtualization schemes. Figure 5d illustrates that increasing the number of actions performed on a packet can slow down the insertion rate by 32%.

Finally, we repeated the experiments for rule deletion. Surprisingly, adding a TCP match to a set of rules in Table 0 results in faster deletion times. This counter-intuitive result underscores the need for comprehensive testing of the classifier's structure to avoid unexpected performance slowdowns.

#### 3.2.2 Modification of Rules
None of the evaluated NICs support direct flow modifications through their APIs. The standard process involves deleting and then inserting an entry, which leads to periods of incorrect network configuration and slow updates. We developed a technique called "enhanced in-memory update" to speed up rule modifications. This technique directly accesses the memory of the exact-match stages in the pipeline, reducing disruptions.

**Enhanced In-Memory Updates Are Up to 80% Faster**: Using DPDKâ€™s flow-perf tool, we measured the NICs' update rate with the standard sequential deletion and insertion process. We then modified the tool to use our in-memory update. Figure 6 shows the update rate (in krules/sec) for both the standard API and our enhanced in-memory update scheme with an increasing number of rules. The standard API achieves 300K TCP/IP flow updates per second on average, potentially disrupting all forwarded traffic. Our enhanced in-memory updates increase the insertion rate for TCP/IP rules by up to 80%.

**Enhanced In-Memory Updates Explained**: We added our own API for rule updates in DPDK and implemented support for the API in the mlx5 driver (supporting ConnectX-4 and higher NVIDIA Mellanox NICs) and the rdma-core library. Our API minimizes memory allocations and reuses data structures, only changing the match/action field values. For standard TCP 5-tuple rules, two hash tables are used: one for the IP version and one for the 5-tuple itself. Our update mechanism minimizes modifications by following existing rules and selectively updating memory zones, ensuring atomicity and consistency.

### 4 Related Work
Research on the performance of emerging network technologies has provided valuable insights into critical bottlenecks in deployed network systems. Neugebauer et al. [33] investigated the performance of PCIe devices connecting modern NICs to CPUs and memories, revealing low performance with small packets. Farshin et al. [9, 8] quantified the impact of direct cache access in Intel processors and proposed software stack optimizations for per-core hundred-gigabit networking. Kuzniar et al. [23, 24] uncovered issues with OpenFlow-based switches, such as lack of consistency during updates. In contrast, our work focuses on NIC performance. Liu et al. [28] analyzed the memory characteristics, core requirements, and RDMA capabilities of five different SmartNICs. Our research specifically examines the packet classifier component of NICs and the impact of memory occupancy and runtime modifications on performance.

### 5 Conclusions
Motivated by the increasing networking speeds and offloading trends, this paper investigates the performance bottlenecks of today's NIC packet classifiers. We focused on several models from a leading NIC vendor, identifying various critical performance limitations related to memory occupancy, pipeline length, runtime rule modifications, and rule modification speed. We explored the idea of performing gradual updates directly in the NIC, improving the identified bottlenecks and addressing obstacles towards building a more efficient and generic API.

**Acknowledgments**: We would like to thank Dr. Diego Perino and the anonymous reviewers for their insightful comments on this paper.