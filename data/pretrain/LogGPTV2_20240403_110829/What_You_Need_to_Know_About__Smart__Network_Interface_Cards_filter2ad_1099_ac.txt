Summary. Our results show that it is possible to introduce a denial of service
attack to the packet classiﬁer of the NICs under test or dramatically reduce its
throughput by up to 70 Gbps, by updating the classiﬁer’s rules using the same
CPU core that performs traﬃc processing. This technique is commonly used by
sharded high-speed data planes [4], as it would be the case for per-connection
NFs. We note that realistic datacenter workloads generate new connections in
the range of 4K-400K new connections per second. Our results indicate that one
would not gain any beneﬁt from oﬄoading applications, such as cloud NATs and
load balancers, with highly dynamic tables, to the analyzed NICs. Moreover, all
NICs under test achieve similar performance across all the experiments in this
328
G. P. Katsikas et al.
section; with the only diﬀerence being the NVIDIA ConnectX-6 NIC, which
exhibits slightly lower throughput degradation than the rest of the NICs in the
experiment shown in Fig. 2a and Fig. 2b [17].
In the next subsection, we investigate how rule modiﬁcations are performed
and explore performance limitations of these rule modiﬁcations. In response
to this, we provide alternative workarounds that mitigate some of the issues
described in this section.
3.2 Rule Operations Analysis
We now focus solely on the performance of rule update operations (i.e., inser-
tions, deletions, and modiﬁcations’ completion times). Clearly, the shorter the
update completion time, the lower the performance disruption on the forwarded
traﬃc. Our analysis reveals three main ﬁndings. First, while modern NICs handle
almost 500K insertions per second, there is a signiﬁcant and sometimes counter-
intuitive performance diﬀerence depending on the type and number of ﬁelds that
are matched by the packet classiﬁer, as well as the type and number of actions
that are applied by a rule. Surprisingly, installing rules matching IPv4 in Table 0
is a much slower process than installing IPv6 rules. Our second ﬁnding is that
the cost of installing VLAN-based rules for network slicing is substantially lower
than the respective cost of installing GRE/VXLAN/GENEVE-based rules. Our
third and ﬁnal ﬁnding relates to the fact that rule modiﬁcation operations are
not atomically supported by the analyzed NICs: one has to delete the old rule
and insert a new one. Our analysis shows that rule modiﬁcation time can be
decreased by 80% compared to the insertion/deletion operations supported by
the standard API of the vendors, by directly modifying the content of the exact
match tables in the NIC memory.
Insertion/Deletion of Rules
3.2.1
We now compute the rule insertion rate supported by an NVIDIA Mellanox
ConnectX-5 NIC in Tables 0 and 1. We use a single CPU core to insert a number
of rules in the range between 1 and 65536 and measure the time that it takes to
insert them. From this value, we compute the rule insertion rate.
Figure 5a shows that the rules insertion rate for Table 0 for ﬁve types of rules
matching diﬀerent combinations of ﬁelds, such as Ethernet, IPv4, IPv6, and
TCP. A single action is applied to a packet matching any of these rules. Sur-
prisingly, our measurements show a striking diﬀerence between IPv4 and IPv6.
Speciﬁcally, inserting rules matching IPv4 results in a sharp slow-down in the
insertion rate compared to IPv6 rules, which is already 5x slower with just 16K
entries. We proﬁled both operations to unveil the reasons of this performance
diversity and found that IPv4 rules are directly installed by the kernel in hard-
ware, using the ﬁrmware API, while the IPv6 rules are managed by the userlevel
DPDK driver similarly to the rules of non-root tables. On the contrary, Fig. 5b
shows the same experiment for Table 1. We note that in this case matching on
IPv4 results in a 12% higher insertion rate compared to IPv6, the opposite of
What You Need to Know About (Smart) Network Interface Cards
329
Fig. 5. Rule insertion performance (in kRules/sec) of a hardware-based 100 GbE
NVIDIA Mellanox ConnectX-5 NIC classiﬁer with various rule sets in [1, 65536] stored
in two diﬀerent tables.
what we observed for Table 0. This is because Table 1 is managed in software,
thus both IPv4 and IPv6 are managed by the respective DPDK driver.
We then investigate how diﬀerent extensively adopted network slicing proto-
cols aﬀect the insertion rate into the NIC’s most performant Table 1. We consider
VLAN, GRE, VXLAN, and GENEVE virtualization headers, which are widely
used in datacenter and wide-area network deployments. Figure 5c shows that
rules matching VLAN tags can be installed up to 50% more rapidly than those
relying on the other virtualization schemes.
We now verify whether the extent to which the actions associated to the rules
impact the rules’ insertion rate. Figure 5d shows that increasing the number of
actions performed on a packet may result in 32% slower insertion rate. We believe
these results are inline with the natural intuition of slower insertions for more
complex actions.
We ﬁnally repeat all the previous experiments but in this case we remove
entries from the NIC’s packet classiﬁer. To our biggest surprise, when we add a
TCP match on a set of rules in Table 0, the deletion becomes faster than without
having the TCP header. This counter-intuitive result demonstrates once more
that any deployment on Table 0 should be accompanied by a comprehensive
testing of the classiﬁer’s structure to avoid unexpected performance slowdown.
3.2.2 Modiﬁcation of Rules
We now investigate the problem of updating a set of rules on the analyzed NICs.
We ﬁrst observe that none of the evaluated NICs support direct ﬂow modiﬁcations
330
G. P. Katsikas et al.
through their APIs. One has to ﬁrst delete and then insert an entry, which results in
two major issues: (i) there are periods when the network conﬁguration is incorrect
and (ii) as observed in the previous subsection, rule modiﬁcations are extremely
slow for the needs of real platforms. We therefore show how one can carefully engi-
neer ﬂow modiﬁcations for simple 5-tuple matching rules to speed up rule modiﬁ-
cations in the NIC. We refer to our technique as enhanced in-memory update. Our
technique does not rely on the standard API provided by the NIC vendor in DPDK
and the rdma-core library to modify rules, but instead directly accesses the memory
of the exact-match stages in the pipeline and modiﬁes them in a less disruptive way.
We defer the reader later in this section for more details on our improved update
technique.
Enhanced In-Memory Updates Are Up to 80% Faster. We employed
DPDK’s ﬂow-perf tool to measure the NICs update rate, using the standard
sequential deletion and insertion process. Then, we modiﬁed this tool to update
all installed rules by using our in-memory update. Figure 6 shows the update rate
(y-axis) in krules/sec achieved by both (i) the standard API deletion/insertion
(black squares) and (ii) our enhanced in-memory update scheme (blue stars)
with an increasing number of rules (x-axis) in the NIC classiﬁer.
Fig. 6. Evaluation of the enhanced in-memory update mechanism.
We note that the standard API achieves 300K TCP/IP ﬂow updates per sec-
ond1 on average, possibly disrupting all the forwarded traﬃc as shown in Sect. 3.1
in Q2. Our enhanced in-memory updates of the NIC classiﬁer increases the
insertion rate for TCP/IP rules by up to 80%. We observe the CPU stalls dur-
ing the experiment, waiting for the NIC to complete memory synchronization
commands, hence reaching the limit of the NIC Direct Memory Access (DMA)
engine. We leave the problem of making the update mechanism more generic,
possibly in collaboration with NVIDIA Mellanox, as future work.
Enhanced In-Memory Updates Explained. We now explain more in detail
how one can realize faster rule insertions/deletions/modiﬁcations on the analyzed
1 The employed DPDK v20.11 ﬂow API is single-threaded. Higher performance could
be achieved using multi-threaded rule insertion/deletion added in DPDK v21.02 [15].
What You Need to Know About (Smart) Network Interface Cards
331
NICs. While NIC vendors provide their own standard API for rule modiﬁcations,
we added our own API for rule updates in DPDK and implemented support for
the API in the mlx5 driver (supporting ConnectX-4 and higher NVIDIA Mel-
lanox NICs), and the backing rdma-core library that handles messaging between
the NIC driver and the NIC itself. Instead of inserting and then removing a rule,
our new API is based on eﬃcient in-memory updates to avoid as many memory
allocations in the driver as possible, while reusing the data-structure and only
changing the match/action ﬁeld values. In the ConnectX-5 and above NICs,
a rule in a non-root table is implemented using a series of exact match hash-
tables. Each hash-table only supports a unique mask, i.e., it is left to the user
to implement techniques, such as tuple-space search, to implement an eﬃcient
LPM strategy by using a series of various exact-match masked preﬁxes.
In the case of standard TCP 5-tuple, one needs two hash tables. The ﬁrst
table matches the IP version, and the second one matches the 5-tuple itself. As
far as our reverse-engineering of this undocumented mechanism can tell, this sep-
aration of the header ﬁelds into multiple hash-tables is dictated by the ﬁrmware,
which supports a certain set of groups of ﬁelds per hash-table. Each of these
two hash-tables work on one of those group of ﬁelds, eventually masked. Adding
more ﬁelds from the application layer, or diverse tunnel types (VXLAN, GRE,
etc.) will add more hash-tables in the chain. We note that the NIC handles hash-
table collisions using a per-bucket linked-list of colliding entries. When inserting
a new TCP/IP 5-tuple, the standard API would take an atomic reference on the
entry for the IP-version and insert an entry into the 5-tuple hash-tables, and
then remove the old rule from both hash-tables. Our update mechanism tries to
minimize the number of modiﬁcations by following the existing rule, leaving it
in the same place when the bucket does not change, not changing atomic refer-
ence (as it is the case for the IP-version hash-table), and then we either rewrite
in-place the bucket of the hash-table if the bucket index (i.e., a CRC32 hash of
the masked ﬁelds value) did not change (it is probable as all hash-tables start
with a very small size and grow as needed), or move the entry in the pointed
bucket to a new bucket if the index changes. This also avoids multiple calls to
the DMA engine to insert and remove the rule, by only selectively updating
the memory zone of the ﬁeld that changed, as well as limiting the number of
memory accesses. As far as consistency matters, our approach tries to guarantee
atomicity of the update in the NIC. There exists a small amount of time during
which, when the bucket entry is moved, the old and the new entries co-exist
before the old entry is marked as invalid. We believe this co-existence does not
open a security vulnerability since both entries are valid. Operators should fall
back to the standard API if this is a concern.
For now, we only support updates on match operations’ values; to implement
action operations’ value updates, such as redirecting packets to a diﬀerent queue,
would be fairly similar. This is particularly suitable for connection tracking, such
as NAT and load balancers. Updating the masks of a rule is another complex
operation that we currently do not support. This is challenging because diﬀerent
332
G. P. Katsikas et al.
hash-tables implement diﬀerent masked elements, possibly defeating the beneﬁts
of re-using some pre-installed elements.
4 Related Work
Measuring the performance of emerging network technologies has brought enor-
mous beneﬁts in our understanding of where the critical bottlenecks reside
in today’s deployed network systems. Neugebauer et al. [33] have investigated
the performance of the PCIe device interconnecting modern NICs to the CPUs
and memories showing surprisingly low performance with small packets. Farshin
et al. (i) quantiﬁed the impact of direct cache access in Intel processors [9] and
(ii) proposed software stack optimizations to achieve per-core hundred-gigabit
networking [8]. Kuzniar et al. [23,24] have unveiled a variety of issues with the
initial OpenFlow-based switches, such as the lack of consistency during updates.
In contrast, we focus on NIC performance. Liu et al. [28] analyzed the memory
characteristics, number of cores required to forward a certain amount of traﬃc,
and Remote Direct Memory Access (RDMA) capabilities of ﬁve diﬀerent Smart-
NICs. In our work, we focus on the packet classiﬁer component of a NIC and the
impact of memory occupancy and runtime modiﬁcations on its performance.
A variety of eﬀorts have been devoted to the orthogonal problem of scheduling
updates in a network [16,19,20,22,27,39,40] or designing faster data structures
at the data-plane level that are amenable to quick modiﬁcations [5,6,45].
5 Conclusions
Motivated by the ever-growing increase of networking speeds and oﬄoading
trends, this paper investigates the performance bottlenecks of today’s NIC packet
classiﬁers. We focused on several evolving models of one of the largest NIC ven-
dors worldwide, showing a variety of critical performance limitations depending
of the memory occupancy, the pipeline length, runtime rule modiﬁcations, and
rule modiﬁcation speed. We explored the idea of performing gradual updates
directly in the NIC, improving the unveiled bottlenecks as well as many obsta-
cles towards building a more eﬃcient and generic API.
Acknowledgments. We would like to thank our shepherd Dr. Diego Perino and
the anonymous reviewers for their insightful comments on this paper. This work