good variety in the population and explore more extensive search
areas to find a global optimum.
Crossover and mutation. Crossover permutes two selected parents.
We design a simple crossover arithmetic recombination operation
to perform a single-point crossover [5]. If P1 and P2 are a mating
pair, then the children Ci it produces will inherit genes from each
parent proportional to the value of n which is a random integer
from 1 to 3, as shown in Eq. (2) below:
(n × P1) + ((4 − n) × P2)
, 1 ≤ n ≤ 3
4
Ci =
(2)
To conserve the best fit samples and not to lose them, we also
added the following inheritance heuristics: if the best sample in the
current generation is better than any sample in the next generation,
we copy the best sample in the current generation to the next
generation. Mutation aims to increase variation within a population
and increase the perturbation level of adversarial audio example.
If the noise-added audio is still classified into the original label,
then we add a small number of random perturbations to samples
to produce more variations.
3 EXPERIMENT AND RESULTS
Experiment: The goal of our experiment is to evaluate the gener-
ated adversarial audio examples and further test the robustness of
commercial APIs using Foolgle. We used Google’s Speech-To-Text
API to evaluate the effectiveness of our approach. We randomly
extracted words from audio files in the Mozilla Common Voice
dataset to create 50 audio files of a single word to test the effec-
tiveness of our approach. Initially, we run five epochs to try to get
an adversarial example for all 50 audio files, starting with α = 0.01.
Then, we automatically increment or decrement α based on the
attack success and confidence value returned from the API. If we
consecutively fail to produce an adversarial audio file in the next
10 iterations, we increase α and repeat the process. If we can find
α in 10 consecutive steps, we decrease α to reduce the noise.
Attack Success Rate. Of the 50 adversarial audio files, Fool-
gle successfully creates adversarial audio labels for 43 audio files,
yielding 86% success rate. For 10% of the audio files generated by
Foolgle, Google API was unable recognize the word in each audio
file and returned ‘no label’. Google API returned the same label
for 4% of the audio files. For simplicity, we limit the number of
iterations in our experiments to 500. If Foolgle fails to create a dif-
ferent label within 500 iterations, we stop the process and declare
the audio file to have the same output as the original. For 14% of
the generated audio files, Foolgle failed to generate an adversarial
sample.
Analysis. Figure 2 presents the spectrogram of the original and
adversarial examples we produced for the original audio label of
“Nickel", frequency (Y-axis) vs. time (X-axis). In Fig. 2(a) shows the
spectrogram of the original audio sample (“Nickel”) with little to
no background noise. As noise is added to the audio sample, we can
clearly observe the change in color Fig. 2(b) and Fig. 2(c). The color
indicates the strength of the energy and red signifying the voice
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2594Figure 2: Spectrogram of perturbations distribution gener-
ated for “Nickel” (Foolgle vs. Random perturbation).
Figure 3: MFCC of perturbations distribution generated for
“Nickel” (Foolgle vs. Random perturbation).
activity. As we compare these two sets of images, the perturbation
pattern generated by our approach in Fig. 2(b) is clearly different
from the random perturbations in Fig. 2(c). While random noise
spreads uniformly everywhere as shown in Fig. 2(c), which makes
it difficult for humans to comprehend the original audio, the noise
generated from our approach tends to preserve the original audio
labels. We can clearly observe that our GA tends to better mitigate
interference with the human voice activity by the clear red features.
To analyze the differences, we compare the MFCC plotting of the
original and adversarial audio samples as shown in Fig. 3, where
MFCC can capture the distinct features of an audio signal by plotting
MFCC coefficient (Y-axis) vs. time (X-axis). The color represents
the coefficient values from lowest (blue) to highest (red). From
Fig. 3, we can observe that the audio sample generated with GA
perturbations (Fig. 3(b)) more closely matches the original audio
sample (Fig. 3(a)) compared with the audio sample generated with
random perturbations (Fig. 3(c)). Moreover, we clearly observe the
significant differences between GA perturbations (Fig. 3(b)) and
random perturbations (Fig. 3(c)). These experiment results demon-
strate that GA perturbations appears better to generate adversarial
audio examples with similar characteristics to those of the original
audio samples.
Human Evaluation. To measure the effectiveness of Foolgle,
we measure how difficult to recognize generated adversarial audio
examples are and how comprehensible their texts are for human
listeners. For this purpose, an IRB-approved user study was carried
out by conducting listening tests with 12 participants we recruited
from our campus. In the first task, each participant was presented
with 10 random adversarial samples generated by Foolgle and for
each sample was asked to type in the recognized text from the
audio sample to measure the correctness and rate how to difficult
to correctly understand (with a 5-point Likert scale). In the second
task, we also asked participants to rate each sample to measure
their sentiments (with a 5-point Likert scale).
Overall, we collected 120 inputs, and participants were able to
correctly identify and label 88% of the adversarial examples gen-
erated by Foolgle. This indicates that the generated audio samples
have either no or minimal audible noise in the generated adversarial
samples. Participants indicate that they found 8% of the generated
audio samples to be very easy, and 43% to be easy to listen. 27% of
the audio were determined to be neutral on their comprehension.
On the other hand, 13% and 9% of the audios were mentioned by
participants to be difficult and very difficult to comprehend the gen-
erated audio samples. Therefore, overall only 22% of the samples
generated were difficult to comprehend for the participants, which
is correlated with the correctness result.
4 DISCUSSION AND FUTURE WORK
Our dataset is limited in the sense that we only used 50 different
random voice samples for this preliminary study. Another limitation
is that as our attack is a black-box approach using GA. Therefore,
our perturbation generation would not be optimal, and performance
may be worse than the theoretical optimum value. However, our
benefits, which do not require GPU resources and provide a practical
algorithm to estimate the noise level, can be a more attractive
option in real-world situations. Lastly, as our attack only requires
the confidence score from the ASR systems, we plan to test the
robustness of other commercial ASR systems with our approach.
5 CONCLUSION
We introduce a simple yet effective method, Foolgle, to generate
adversarial audio, which does not require any knowledge about
DNNs or use GPU resources. Our Foolgle creates noise using an
iterative approach using GA. Our preliminary result shows that the
state-of-the-art Google Cloud Speech-to-text API is easily fooled by
our attack. It appears that the current API is not capable of handling
and possibly malicious exploited to produce unintended results or
undesired behaviors from voice assistant applications.
REFERENCES
[1] Moustafa Alzantot, Bharathan Balaji, and Mani Srivastava. 2018. Did you hear
that? adversarial examples against automatic speech recognition. arXiv preprint
arXiv:1801.00554 (2018).
[2] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric
Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang
Chen, et al. 2016. Deep Speech 2 : End-to-End Speech Recognition in English and
Mandarin. In Proceedings of the International Conference on Machine Learning.
[3] Donald J Berndt and James Clifford. 1994. Using dynamic time warping to find
patterns in time series.. In KDD workshop, Vol. 10. Seattle, WA, 359–370.
[4] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden voice commands.
In 25th USENIX Security Symposium (USENIX Security 16). 513–530.
[5] Agoston E Eiben, James E Smith, et al. 2003. Introduction to evolutionary computing.
Vol. 53. Springer.
[6] Shreya Khare, Rahul Aralikatte, and Senthil Mani. 2018. Adversarial Black-
Box Attacks for Automatic Speech Recognition Systems Using Multi-Objective
Genetic Optimization. arXiv preprint arXiv:1811.01312 (2018).
[7] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security. ACM, 506–519.
[8] Lawrence R Rabiner and Bernard Gold. 1975. Theory and application of digital
signal processing. Englewood Cliffs, NJ, Prentice-Hall, Inc., 1975. 777 p. (1975).
[9] Nirupam Roy, Sheng Shen, Haitham Hassanieh, and Romit Roy Choudhury. 2018.
Inaudible Voice Commands: The Long-Range Attack and Defense. In 15th USENIX
Symposium on Networked Systems Design and Implementation (NSDI 18).
[10] Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita Vemuri. 2018. Targeted
adversarial examples for black box audio systems. arXiv preprint arXiv:1805.07820
(2018).
[11] Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and Xiaolin Li. 2017.
Adversarial Examples: Attacks and Defenses for Deep Learning. arXiv preprint
arXiv:1712.07107 (2017).
[12] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and
Wenyuan Xu. 2017. Dolphinattack: Inaudible voice commands. In Proceedings of
the ACM SIGSAC Conference on Computer and Communications Security.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2595