We re-emphasize that we strove for these features to
reﬂect aspects of writing style that remain unchanged for
some given author, regardless of the topic at hand. In
particular, the frequencies of 293 function words contain
little meaning on their own and instead express grammatical
relationships, so they avoid revealing information about the
topic while capturing the writing style of the author.
To gain a better intuitive understanding of the relative
utility of the features and for use in feature selection, we
computed the information gain of each feature over the entire
dataset [58]. We deﬁne information gain as
IG(Fi) = H(B)− H(B|Fi) = H(B) + H(Fi)− H(B, Fi),
where H denotes Shannon entropy, B is the random variable
corresponding to the blog number, and Fi is the random
5An example of a rewrite rule is A:PP → P:PREP + PC:N, meaning
that an adverbial prepositional phrase is constituted by a preposition
followed by a noun phrase as a prepositional complement [21].
306
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
• feature-mean Rescale each column (feature) to have
mean 0 and variance 1, i.e., subtract the mean from
each column and divide by the standard deviation.
• feature-mean-nonzero Rescale each column so that
the nonzero entries have mean 1, i.e. divide by the mean
of the nonzero entries of each column (since all features
are nonnegative).
Row normalization is essential because it allows us to
compare different documents based on the relative dis-
tribution of features in the document and irrespective of
its norm. Similarly, column normalization allows us to
combine information from different features when they are
measured in different units. While the feature-mean is
a standard normalization in statistics, our feature-mean-
nonzero warrants some explanation. Like feature-mean,
this normalization provides scale invariance but it calcu-
lates the normalization parameter from the support of each
feature, i.e. the nonzero entries. Since the majority of our
columns are sparse, focusing on the support allows us to
capture structure speciﬁc to the nonzero entries. A statistic
such as the average would be unecessarily skewed towards
zero if we calculated it the standard way. Moreover, since
we divide the features by the mean of their support rather
than subtracting it, all nonzero entries are still positive
and zero entries are still zero so they are distinct from
the average value of the support, which is 1. Finally, the
order of normalization is important; we perform column
normalization followed by row normalization.
B. Classiﬁers
This section discusses the algorithms and conﬁgurations
we used to identify authors. We denote a labeled example
as a pair ((cid:126)x, y), where (cid:126)x ∈ Rn is a vector of features and
y ∈ {1, 2, . . . , m} is the label. In our case, n = 1188,
the number of features; and m = 100000,
the number
of blogs in our dataset. After training a classiﬁer on the
labeled examples, we present it with one or more unlabeled
examples (cid:126)x1, (cid:126)x2, . . ., test posts taken from a single blog in
the case of post-to-blog matching experiments, or an entire
blog in the case of blog-to-blog matching. In either case,
we rank the labels {1, 2, . . . , m} according to our estimate
of the likelihood that the corresponding author wrote the
unlabeled posts.
One key difference between our experiments and the
typical classiﬁcation scenario is that we know that (cid:126)x1, (cid:126)x2, . . .
have the same label. To exploit this knowledge we collapse
the vectors (cid:126)x1, (cid:126)x2, . . . to their mean, and classify that single
vector. Another possibility that we did not try would be to
classify each point separately and use some form of voting
to combine the decisions. Two points of note: ﬁrst, collaps-
ing the feature vectors is not mathematically equivalent to
treating all of the author’s writing as a single document.
Second, we have described how we handle different points
with the same label for classiﬁcation, not training. During
Figure 3. Eigenvalue spectrum.
with a superlinear training complexity is infeasible. A simi-
lar restriction applies to the way we adapt binary classiﬁers
to handle multiple classes. In particular, the popular and
robust “all-pairs” multiclass regime falls out of the picture
because it requires comparing all pairs classes.
While the above argument holds for any large dataset, ours
is particularly problematic because there are a small number
of training examples per class. On average, there are only 24
posts for each author, and indeed, 95% of all authors have 50
posts or less. Compounding the lack of data is the sparsity
of each sample; each post has approximately 305 non-
zero dimensions. These factors are troublesome compared to
the 1188 dimensions we use to represent the data because
there are very few examples with which to estimate class
speciﬁc parameters. In particular, methods such as nearest-
neighbors that rely on estimating class means should do well,
but anything that requires estimating a covariance matrix
(e.g. RLSC and SVM) for each class will require heavy
regularization.
Finally, Figure 3 is a plot of the eigenvalue spectrum
we obtain from running a Principal Component Analysis
on the data (note that the y-axis is logarithmic). Based
on this analysis, a number of dimensions are spurious —
we could represent the data to 96% accuracy using only
500 dimensions. While this presents a sizeable reduction
in dimensionality, it is not enough to allow for reliable
estimation of anything more than a class centroid. We would
have to represent the data at 27% accuracy in order to shrink
it to a number of dimensions that is not limited by the low
number of samples per class.
V. CLASSIFICATION
A. Normalization
We utilized three types of normalization:
• row-norm Rescale each row (training or test point) to
have norm 1, i.e., divide each entry in a row by the
norm of that row.
307
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
02004006008001000120010-410-2100102104106EigenvalueMagnitudetraining we collapse the feature vectors for some classifers,
but not others, as described below.
Nearest Neighbor. To train the nearest neighbor classiﬁer
we collapse each class’ training examples into its centroid.
The most common form of the nearest neighbor algorithm
treats each training example separately and therefore stores
all of the training data. However, this approach is too slow
for our dataset. Thus, we only store one centroid per class,
effectively providing a “ﬁngerprint” for each blog. A new
point is classiﬁed by computing its Euclidean distance to
each of the ﬁngerprints and using the label of the closest
class.
Linear Discriminant Analysis. Linear Discriminant
Analysis (LDA) is a popular algorithm for text classiﬁcation
and is among the top performers if we do not normalize
our data. However it cancels out any feature normalizations
that we perform (we discuss this more precisely in the full
version of the paper). We ﬁnd feature normalization to be
particularly necessary with our data and large number of
classes, so we do not include this classiﬁer in our ﬁnal
results.
Naive Bayes. The standard Naive Bayes classiﬁer as-
sumes individual features are distributed according to a
Gaussian distribution and are conditionally independent for
each class. Mathematically, it works similarly to our nearest
neighbor implementation, but it also takes each feature’s
variance in account. Speciﬁcally, we train by computing the
mean µi and variance 8 σ2
i of each feature i for each class.
We classify a point (cid:126)x = (x1, . . . , xn) by selecting the closest
class, where distance is computed as
+ log(σ2
i )
n(cid:88)
(xi − µi)2
i=1
σ2
i
Note that this a weighted Euclidean distance that places
greater value on features with less variance and that nor-
malizes across classes by taking into account the log(σ2
i ),
the total variance of each class.
Binary Classiﬁers Thus far, we have discussed inherently
multiclass classiﬁers. However, there is a large literature on
binary classiﬁers that we would like to experiment with. We
can extend a binary classiﬁer to the multiclass setting by
training numerous binary classiﬁers. The two most popular
methods for doing this are the one-vs-all and all-pairs
regimes. In the former, a classiﬁer is trained for each class by
labelling all points that belong to that class as positive and all
others as negative. A point is labelled by running it through
all of the classiﬁers and selecting the classiﬁer which votes
most positively. In an all-pairs regime, a classiﬁer is trained
for each pair of classes, resulting in a number of classiﬁers
that is quadratic in the number of classes. This approach is
8A small-sample correction of 5 × 10−6 was added to each variance
to prevent it from being zero. This occurs frequently because our features
are sparse so that some features are never seen in a particular class.
308
far too costly when there are 100,000 classes, so we only
consider one-versus-all.
Masking. As demonstrated by our experiments, the stan-
dard one-versus-all regime can mask certain classes so that
points from them will always be mislabeled. Restricting
ourselves to linear binary classiﬁers — which are the only
kind of binary classifers we consider in this paper — the
root of this problem stems from the one-versus-all regime’s
assumption that each class can be separated from all of the
others using a linear decision boundary. This assumption is
reasonable only when there are many more dimensions than
classes; as soon as the number of classes is on the same
order as the dimensionality of the data, we run into trouble.
In our case, there are far more classes than dimensions and
the masking problem is particularly severe. In experiments in
which we used 50,000 testing points, each from a distinct
class, only 628 distinct classes were predicted by RLSC.
Moreover, the same 100 classes appeared in over 82% of
the predicted labels. This masking problem therefore makes
the standard one-versus-all scheme unusable with any binary
classiﬁer. We present a weighting scheme that alleviates
this problem in the section on Regularized Least Squares
Classiﬁcation.
Support Vector Machine (SVM). SVMs are a popular
binary classiﬁer [59; 60] and we use the SVM Light imple-
mentation [61]. Ideally, we would provide each SVM with
all of the posts, but the large size of our dataset makes this
infeasible. We use posts from a sample of 1000 blogs as
negative examples and retain all positive examples for each
one-vs-all classiﬁer. To improve the ability of an SVM to
distinguish between its associated class and all others, we
ensure that this set of 1000 blogs includes the 100 closest
classes as determined by the Euclidean distance of the
class centroids. The remaining 900 are selected uniformly
at random. we use a simple linear kernel for classiﬁcation.
Regularized Least Squares Classiﬁcation (RLSC).
RLSC is an adaptation of the traditional least squares regres-
sion algorithm to binary classiﬁcation. It was introduced as a
computationally efﬁcient alternative to SVMs and has simi-
lar classiﬁcation performance. The main difference between
the two is that RLSC uses a squared loss to penalize mistakes
while SVM uses a non-differentiable “hinge loss.” The
former admits a closed form solution that uses optimized
linear algebra routines; SVMs rely on convex optimization
and take longer to train. This efﬁciency does not require us
to subsample the data when training so that unlike SVMs,
RLSC uses all of the training data. Perhaps the most notable
computational difference between the two is that it takes
the same time to solve for a single RLSC classiﬁer as it
does to solve for T separate ones in a one-vs-all scheme
with T classes. Thus, our computation time with RLSC is
O(nd2 + d3) versus O(nd2 + T d3) with SVM for n training
examples in d dimensions.
Since RLSC is a binary classiﬁer, we employ a one-vs-all
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
regime to extend it to multiple classes. However, we found
that the aformentioned masking problem makes RLSC per-
form very poorly. We remedy the situation by weighting the
training data given to each one-vs-all classiﬁer. In particular,
consider what happens when we train a one-vs-all classiﬁer
with 100, 000 classes: there are, on average, 99, 999 times
more negative examples than positive ones. There is little
incentive for the classiﬁer to learn anything since it will
be overwhelmingly correct if it simply labels everything as
negative. We counteract this problem by penalizing errors
on positive examples more so that they have effectively
the same weight as negative examples. After some careful
algebra, this approach, which is the version of RLSC we
will use in the remainder of the paper, preserves the efﬁcient
runtime of RLSC and improves accuracy by two orders of
magnitude.
Classiﬁer Conﬁgurations. While we would like to report
our classiﬁers’ performance with all combinations of fea-
tures and normalization, we only report a tractable subset
that illustrates the effects of normalization and masking.
We used the Naive Bayes classiﬁer with the 400 features
with greatest
information gain and no normalization to
showcase the best performance without any normalization.
Next, the nearest neighbor classiﬁer uses two choices of
normalization: row-norm and feature-mean; row-norm
and feature-mean-nonzero to demonstrate the beneﬁts of
our sparse feature normalization method over the standard
one. In the way of binary classiﬁers, we report the SVM
with a linear kernel and a standard one-vs-all regime using
row-norm and feature-mean normalizations to illustrate
the issue of masking. Finally, RLSC is shown with a linear
kernel and weighted one-vs-all regime using row-norm and
feature-mean-nonzero normalizations and is one of the
top performers. Although SVM would have similar accuracy
as RLSC using the same conﬁguration, the latter was more
useful because its speed allowed us to experiment with many
conﬁgurations.
Lazy vs. eager. The existing literature on authorship attri-
bution distinguishes between “eager” and “lazy” classiﬁers