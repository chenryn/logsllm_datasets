### 5.5 Improvement in User Experience

Section 4.2 discusses two scenarios where the static setting of `tcp_rmem_max` can negatively impact user experience. In this subsection, we demonstrate that by applying Dynamic Receive Window Adjustment (DRWA), we can significantly improve user experience in these scenarios. More comprehensive experimental results are provided in Section 6.

**Figure 15: Web Object Fetching Performance with a Long-Lived TCP Flow in the Background**

Figure 15 illustrates the performance of web object fetching when a long-lived TCP flow is present in the background. DRWA reduces the queue length in cellular networks, leading to an average reduction in Round-Trip Time (RTT) of 42%, which translates to a 39% speed-up in web object fetching. Note that the absolute numbers in this test are not directly comparable with those in Figure 10, as these experiments were conducted at different times.

**Figure 16: Throughput Improvement in International Long-Lived TCP Downloads**

Figure 16 shows the scenario where a mobile client in Raleigh, U.S., initiates a long-lived TCP download from a server in Seoul, Korea, over both AT&T HSPA+ and Verizon LTE networks. The RTT in this scenario is very high, resulting in a large Bandwidth-Delay Product (BDP), especially for LTE due to its high peak rate. The static setting of `tcp_rmem_max` is too small to fully utilize the available bandwidth, leading to throughput degradation. With DRWA, we can fully utilize the available bandwidth, achieving a 23% to 30% improvement in throughput.

### 6. Additional Experimental Results

We implemented DRWA on Android phones by patching their kernels. It turned out to be relatively simple to implement DRWA in the Linux/Android kernel, requiring only about 100 lines of code. We downloaded the original kernel source codes of different Android models from their manufacturers' websites, patched the kernels with DRWA, and recompiled them. Finally, the phones were flashed with our customized kernel images.

#### 6.1 Throughput Improvement

Figure 17 illustrates the throughput improvement brought by DRWA over various BDPs. We emulated different BDPs by using netem [12] on the server side to vary the end-to-end propagation delay. The propagation delays we emulated are relatively large (from 131ms to 798ms) because RTTs in cellular networks are indeed larger than in conventional networks. Even if the client and server are geographically close, the propagation delay can still be hundreds of milliseconds due to the detour through IP gateways deployed across the country by carriers.

According to the figure, DRWA significantly improves TCP throughput in various cellular networks as the propagation delay increases. The largest improvement (up to 51%) is observed in the Sprint EVDO network with a propagation delay of 754ms. In LTE networks, phones with DRWA show up to a 39% improvement in throughput under a latency of 219ms. The reason for the improvement is clear: as latency increases, the static setting of `tcp_rmem_max` fails to saturate the pipe, leading to throughput degradation. In contrast, networks with small latencies do not show such degradation because the static value is sufficient to fill the pipe. According to our experiences, RTTs between 400 ms and 700 ms are common in cellular networks, especially when using services from overseas servers. In LTE networks, TCP throughput is even more sensitive to the `tcp_rmem_max` setting, as a slight increase in RTT can dramatically increase the BDP, making the static configuration sub-optimal. However, DRWA is able to keep pace with the varying BDP.

#### 6.2 RTT Reduction

In networks with small BDPs, the static `tcp_rmem_max` setting is sufficient to fully utilize the network's bandwidth but can result in long RTTs. DRWA manages to keep the RTT around \(\lambda\) times the minimum RTT (\(RT_{\text{min}}\)), which is substantially lower than the current implementations in networks with small BDPs. Figure 18 shows that the RTT reduction brought by DRWA does not come at the cost of throughput. We observe a significant reduction in RTT of up to 49% while maintaining similar throughput (with a maximum difference of 4%).

Another important observation from this experiment is the much larger RTT variation under the static `tcp_rmem_max` setting compared to DRWA. As Figures 18(b) and 18(d) show, the RTT values without DRWA are distributed over a much wider range. This is because DRWA intentionally enforces the RTT to remain around the target value of \(\lambda \times RT_{\text{min}}\). This property of DRWA can benefit jitter-sensitive applications such as live video and voice communication.

### 7. Discussion

#### 7.1 Alternative Solutions

There are several other possible solutions to the bufferbloat problem. One obvious solution is to reduce the buffer size in cellular networks so that TCP can function similarly to how it does in conventional networks. However, there are two potential problems with this approach. First, the large buffers in cellular networks are not introduced without reason; they help absorb bursty data traffic over time-varying and lossy wireless links, achieving a very low packet loss rate (most lost packets are recovered at the link layer). Removing these extra buffer spaces could lead to a higher packet loss rate and lower throughput. Second, modifying the deployed network infrastructure (such as buffer space on base stations) would be costly.

An alternative is to employ Active Queue Management (AQM) schemes like Random Early Detection (RED) [9]. By randomly dropping certain packets before the buffer is full, we can notify TCP senders in advance and avoid long RTTs. However, despite extensive research, few AQM schemes are actually deployed on the Internet due to the complexity of parameter tuning, the extra packet losses they introduce, and the limited performance gains they provide. More recently, Nichols et al. proposed CoDel [22], a parameter-less AQM that aims to handle bufferbloat. Although it has several advantages over traditional AQM schemes, it faces the same deployment cost issue: modifying all intermediate routers on the Internet is much harder than updating endpoints.

Another possible solution is to modify the TCP congestion control algorithm at the sender. Delay-based congestion control algorithms (e.g., TCP Vegas, FAST TCP [28]) are resistant to the bufferbloat problem because they back off when RTT starts to increase rather than waiting for packet loss. To verify this, we compared the performance of Vegas against CUBIC with and without DRWA in Figure 19. As the figure shows, although Vegas has a much lower RTT than CUBIC, it suffers from significant throughput degradation. In contrast, DRWA maintains similar throughput while reducing RTT by a considerable amount. Moreover, delay-based congestion control protocols have other issues, such as requiring modifications to all servers, which is more expensive than over-the-air (OTA) updates to mobile clients. Additionally, since not all receivers are on cellular networks, delay-based flows will compete with loss-based flows in other parts of the network where bufferbloat is less severe. In such situations, it is well-known that loss-based flows unfairly grab more bandwidth from delay-based flows [3].

Traffic shaping is another technique proposed to address bufferbloat [26]. By smoothing out bulk data flows with a traffic shaper on the sender side, we can reduce the queue length at the router. However, determining the shaping parameters beforehand is challenging, especially in highly variable cellular networks. We tested this method in the AT&T HSPA+ network, and the results are shown in Figure 20. In this experiment, we used netem on the server to shape the sending rate to different values (via a token bucket) and measured the resulting throughput and RTT. According to the figure, a lower shaped sending rate leads to lower RTT but also sub-optimal throughput. In this specific test, 4 Mbps seems to be a good balancing point. However, such a static setting of shaping parameters can suffer from the same problems as the static setting of `tcp_rmem_max`.

Given the issues with the above-mentioned solutions, we addressed the problem on the receiver side by changing the static setting of `tcp_rmem_max`. Receiver-side modifications (on mobile devices) have the lowest deployment cost, as vendors can simply issue an OTA update to the protocol stack of the mobile devices, improving TCP performance without affecting other wired users. Furthermore, since the receiver has the most knowledge of the last-hop wireless link, it can make more informed decisions. For example, the receiver may choose to turn off DRWA if it is connected to a network that is not severely bufferbloated (e.g., WiFi). Therefore, a receiver-centric solution is the preferred approach to transport protocol design for mobile hosts [13].

#### 7.2 Related Work

Adjusting the receive window to solve TCP performance issues in cellular networks has been explored in previous work. Our approach, DRWA, builds on these ideas and provides a practical and effective solution.