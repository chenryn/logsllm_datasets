and EVDO together will make the throughput diﬀerences
in EVDO networks indiscernible.
5.5 Improvement in User Experience
Section 4.2 lists two scenarios where the static setting of
tcp rmem max may have a negative impact on user experi-
ence. In this subsection, we demonstrate that, by applying
DRWA, we can dramatically improve user experience in such
scenarios. More comprehensive experiment results are pro-
vided in Section 6.
Figure 15 shows Web object fetching performance with a
long-lived TCP ﬂow in the background. Since DRWA re-
duces the length of the queue built up in the cellular net-
works, it brings 42% reduction in RTT on average, which
translates into 39% speed-up in Web object fetching. Note
that the absolute numbers in this test are not directly com-
parable with those in Figure 10 since these two experiments
were carried out at diﬀerent time.
Figure 16 shows the scenario where a mobile client in
Raleigh, U.S. launches a long-lived TCP download from a
server in Seoul, Korea over both AT&T HSPA+ network and
Verizon LTE network. Since the RTT is very long in this
scenario, the BDP of the underlying network is fairly large
(especially the LTE case since its peak rate is very high).
The static setting of tcp rmem max is too small to ﬁll the
long fat pipe and results in throughput degradation. With
DRWA, we are able to fully utilize the available bandwidth
and achieve 23% ∼ 30% improvement in throughput.
6. MORE EXPERIMENT RESULTS
We implemented DRWA in Android phones by patching
their kernels. It turned out to be fairly simple to implement
DRWA in the Linux/Android kernel. It only takes around
100 lines of code. We downloaded the original kernel source
codes of diﬀerent Android models from their manufacturers’
website, patched the kernels with DRWA and recompiled
337)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
265ms
347ms
451ms
570ms
Propagation Delay
Without DRWA
With DRWA
677ms
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
1.4
1.2
1
0.8
0.6
0.4
0.2
0
326ms
433ms
Without DRWA
With DRWA
798ms
550ms
653ms
Propagation Delay
(a) Improvement in AT&T HSPA+:
-1%,
(b) Improvement in Verizon EVDO: -3%, -
0.1%, 6%, 26% and 41%
0.3%, 4%, 29% and 51%
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
1.2
1
0.8
0.6
0.4
0.2
0
312ms
448ms
536ms
625ms
Propagation Delay
Without DRWA
With DRWA
754ms
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
20
15
10
5
0
131ms
219ms
Without DRWA
With DRWA
533ms
351ms
439ms
Propagation Delay
(c) Improvement in Sprint EVDO: 4%, 5%,
37%, 45% and 51%
(d) Improvement in Verizon LTE: -1%, 39%,
37%, 26% and 31%
Figure 17: Throughput improvement brought by DRWA over various cellular networks: the larger the
propagation delay is, the more throughput improvement DRWA brings. Such long propagation delays are
common in cellular networks since all traﬃc must detour through the gateway [31].
Without DRWA
HSPA+ (avg=3.36)
With DRWA
HSPA+ (avg=4.14)
Without DRWA
LTE (avg=7.84)
With DRWA
LTE (avg=10.22)
1
0.8
0.6
0.4
0.2
)
x
≤
X
P
(
0
0
2
4
8
Throughput (Mbps)
6
10
12
Figure 16: DRWA improves the throughput by 23%
in AT&T HSPA+ network and 30% in Verizon LTE
network when the BDP of the underlying network
is large.
them. Finally, the phones were ﬂashed with our customized
kernel images.
6.1 Throughput Improvement
Figure 17 shows the throughput improvement brought by
DRWA over networks of various BDPs. We emulate dif-
ferent BDPs by applying netem [12] on the server side to
vary the end-to-end propagation delay. Note that the prop-
agation delays we have emulated are relatively large (from
131ms to 798ms). That is because RTTs in cellular networks
are indeed larger than conventional networks. Even if the
client and server are close to each other geographically, the
propagation delay between them could still be hundreds of
milliseconds. The reason is that all the cellular data have to
go through a few IP gateways [31] deployed across the coun-
try by the carriers. Due to this detour, the natural RTTs in
cellular networks are relatively large.
According to the ﬁgure, DRWA signiﬁcantly improves the
TCP throughput in various cellular networks as the propa-
gation delay increases. The scenario over the Sprint EVDO
network with the propagation delay of 754ms shows the
largest improvement (as high as 51%).
In LTE networks,
the phones with DRWA show throughput improvement up
to 39% under the latency of 219ms. The reason behind the
improvement is obvious. When the latency increases, the
static setting of tcp rmem max fails to saturate the pipe,
resulting in throughput degradation. In contrast, networks
with small latencies do not show such degradation since the
static value is large enough to ﬁll the pipe. According to our
experiences, RTTs between 400 ms and 700 ms are easily ob-
servable in cellular networks, especially when using services
from oversea servers. In LTE networks, TCP throughput is
even more sensitive to tcp rmem max setting. The BDP can
be dramatically increased by a slight RTT increase. There-
fore, the static conﬁguration easily becomes sub-optimal.
However, DRWA is able to keep pace with the varying BDP.
6.2 RTT Reduction
In networks with small BDP, the static tcp rmem max
setting is suﬃcient to fully utilize the bandwidth of the net-
work. However, it has a side eﬀect of long RTT. DRWA
manages to keep the RTT around λ times of RT Tmin, which
is substantially smaller than the current implementations in
networks with small BDP. Figure 18 shows that the reduc-
tion in RTT brought by DRWA does not come at the cost
of the throughput. We see a remarkable reduction of RTT
338)
x
≤
X
P
(
1
0.8
0.6
0.4
0.2
0
0
Without DRWA AT&T HSPA+ (avg=3.83)
With DRWA AT&T HSPA+ (avg=3.79)
Without DRWA Verizon LTE (avg=15.78)
With DRWA Verizon LTE (avg=15.43)
1
0.8
0.6
0.4
0.2
)
x
≤
X
P
(
5
10
Throughput (Mbps)
15
20
0
0
200
Without DRWA AT&T HSPA+ (avg=435.37)
With DRWA AT&T HSPA+ (avg=222.14)
Without DRWA Verizon LTE (avg=150.78)
With DRWA Verizon LTE (avg=97.39)
400
600
RTT (ms)
800
1000
(a) Throughput in HSPA+ and LTE net-
(b) RTT in HSPA+ and LTE networks
works
1
0.8
0.6
0.4
0.2
)
x
≤
X
P
(
0
0
0.5
Without DRWA Verizon EVDO (avg=0.91)
With DRWA Verizon EVDO (avg=0.92)
Without DRWA Sprint EVDO (avg=0.87)
With DRWA Sprint EVDO (avg=0.85)
1
1.5
2
Throughput (Mbps)
)
x
≤
X
P
(
1
0.8
0.6
0.4
0.2
0
200
400
600
Without DRWA Verizon EVDO (avg=701.67)
With DRWA Verizon EVDO (avg=360.94)
Without DRWA Sprint EVDO (avg=526.38)
With DRWA Sprint EVDO (avg=399.59)
1400
1000
1200
800
RTT (ms)
(c) Throughput in EVDO networks
(d) RTT in EVDO networks
Figure 18: RTT reduction in small BDP networks: DRWA provides signiﬁcant RTT reduction without
throughput loss across various cellular networks. The RTT reduction ratios are 49%, 35%, 49% and 24% for
AT&T HSPA+, Verizon LTE, Verizon EVDO and Sprint EVDO networks respectively.
up to 49% while the throughput is guaranteed at a similar
level (4% diﬀerence at maximum).
Another important observation from this experiment is
the much larger RTT variation under static tcp rmem max
setting than that with DRWA. As Figures 18(b) and 18(d)
show, the RTT values without DRWA are distributed over a
much wider range than that with DRWA. The reason is that
DRWA intentionally enforces the RTT to remain around the
target value of λ ∗ RT Tmin. This property of DRWA will
potentially beneﬁt jitter-sensitive applications such as live
video and/or voice communication.
7. DISCUSSION
7.1 Alternative Solutions
There are many other possible solutions to the buﬀerbloat
problem. One obvious solution is to reduce the buﬀer size
in cellular networks so that TCP can function the same way
as it does in conventional networks. However, there are two
potential problems with this simple approach. First, the
large buﬀers in cellular networks are not introduced without
a reason. As explained earlier, they help absorb the busty
data traﬃc over the time-varying and lossy wireless link,
achieving a very low packet loss rate (most lost packets are
recovered at link layer). By removing these extra buﬀer
space, TCP may experience a much higher packet loss rate
and hence much lower throughput. Second, modiﬁcation
of the deployed network infrastructure (such as the buﬀer
space on the base stations) implies considerable cost.
An alternative to this solution is to employ certain AQM
schemes like RED [9]. By randomly dropping certain packets
before the buﬀer is full, we can notify TCP senders in ad-
vance and avoid long RTT. However, despite being studied
extensively in the literature, few AQM schemes are actually
deployed over the Internet due to the complexity of their pa-
rameter tuning, the extra packet losses introduced by them
and the limited performance gains provided by them. More
recently, Nichols et al. proposed CoDel [22], a parameter-
less AQM that aims at handling buﬀerbloat. Although it
exhibits several advantages over traditional AQM schemes,
they suﬀers from the same problem in terms of deployment
cost: you need to modify all the intermediate routers in the
Internet which is much harder than updating the end points.
Another possible solution to this problem is to modify the
TCP congestion control algorithm at the sender. As shown
in Figure 7, delay-based congestion control algorithms (e.g.,
TCP Vegas, FAST TCP [28]) are resistive to the buﬀerbloat
problem. Since they back oﬀ when RTT starts to increase
rather than waiting until packet loss happens, they may
serve the buﬀerbloated cellular networks better than loss-
based congestion control algorithms. To verify this, we com-
pared the performance of Vegas against CUBIC with and
without DRWA in Figure 19. As the ﬁgure shows, although
Vegas has a much lower RTT than CUBIC, it suﬀers from
signiﬁcant throughput degradation at the same time. In con-
trast, DRWA is able to maintain similar throughput while
reducing the RTT by a considerable amount. Moreover,
delay-based congestion control protocols have a number of
other issues. For example, as a sender-based solution, it re-
quires modifying all the servers in the world as compared
to the cheap OTA updates of the mobile clients. Further,
since not all receivers are on cellular networks, delay-based
ﬂows will compete with other loss-based ﬂows in other parts
of the network where buﬀerbloat is less severe. In such sit-
3391
0.8
0.6
0.4
0.2
)
x
≤
X
P
(
0
0
1
)
x
≤
X
P
(
1
0.8
0.6
0.4
0.2
0
0
CUBIC without DRWA (avg=493)
TCP Vegas (avg=132)
CUBIC with DRWA (avg=310)
500
RTT (ms)
1000
1500
CUBIC without DRWA (avg=4.38)
TCP Vegas (avg=1.35)
CUBIC with DRWA (avg=4.33)
5
3
4
2
Throughput (Mbps)
(a) Throughput
(b) Round Trip Time
Figure 19: Comparison between DRWA and TCP Vegas as the solution to buﬀerbloat: although delay-based
congestion control keeps RTT low, it suﬀers from throughput degradation. In contrast, DRWA maintains
similar throughput to CUBIC while reducing the RTT by a considerable amount.
uations, it is well-known that loss-based ﬂows unfairly grab
more bandwidth from delay-based ﬂows [3].
Traﬃc shaping is another technique proposed to address
the buﬀerbloat problem [26]. By smoothing out the bulk
data ﬂow with a traﬃc shaper on the sender side, we would
have a shorter queue at the router. However, the problem
with this approach is how to determine the shaping param-
eters beforehand. With wired networks like ADSL or ca-
ble modem, it may be straightforward. But in highly vari-
able cellular networks, it would be extremely diﬃcult to ﬁnd
the right parameters. We tried out this method in AT&T
HSPA+ network and the results are shown in Figure 20.
In this experiment, we again use netem on the server to
shape the sending rate to diﬀerent values (via token bucket)
and measure the resulting throughput and RTT. Accord-
ing to this ﬁgure, lower shaped sending rate leads to lower
RTT but also sub-optimal throughput. In this speciﬁc test,
4Mbps seems to be a good balancing point. However, such
static setting of the shaping parameters could suﬀer from
the same problem as the static setting of tcp rmem max.
In light of the problems with the above-mentioned solu-
tions, we handled the problem on the receiver side by chang-
ing the static setting of tcp rmem max. That is because
receiver (mobile device) side modiﬁcation has minimum de-
ployment cost. Vendors may simply issue an OTA update to
the protocol stack of the mobile devices so that they can en-
joy a better TCP performance without aﬀecting other wired
users. Further, since the receiver has the most knowledge of
the last-hop wireless link, it could make more informed deci-
sions than the sender. For instance, the receiver may choose
to turn oﬀ DRWA if it is connected to a network that is not
severely buﬀerbloated (e.g., WiFi). Hence, a receiver-centric
solution is the preferred approach to transport protocol de-
sign for mobile hosts [13].
7.2 Related Work
Adjusting the receive window to solve TCP performance