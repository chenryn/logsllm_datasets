Case 2: Multiple rule changes (should split)Case 1: A single rule change (should not split)T1, GET, /proj/1.htm DENYT2, GET, /proj/1.htm ALLOWT3, GET, /proj/2.htm DENYT4, GET, /proj/2.htm ALLOW T1, GET, /proj/1.htm DENYT2, GET, /proj/2.htm DENYT3, GET, /proj/1.htm ALLOWT4, GET, /proj/2.htm ALLOWGET, /proj/1.htm DENY      ALLOWGET, /proj/2.htm DENY      ALLOWPolicy ChangeAccess logsubsetTime SeriesPurity metricsallowdenyTimestampStatusT1T2T3T4allowdenyTimestampStatusT1T2T3T4pallow: 0.5, pdeny: 0.5Gini Impurity: 0.5Entropy: 1ChangeCount: 3ChangeCount: 1pallow_left: 0.5, pallow_right: 0.5Gini Impurity: 0.5Entropy: 1GET, /proj/* DENY      ALLOWallowdenyTimestampStatusT1T2T3T4ChangeCount: 2pallow_left: 0.5, pallow_right: 0.5Gini Impurity: 0.5Entropy: 1pallow: 0.5, pdeny: 0.5Gini Impurity: 0.5Entropy: 1allowdenyTimestampStatusT1T2T3T4ChangeCount: 2If not splitIf splitIf not splitIf splitFigure 8: An example that demonstrates TCDT learning
algorithm can infer rules even there is no rule change.
In this case, a splitting is required on the condition: if
prefix2=="/proj/1.htm". The time-series change count fa-
vors this splitting as the change count decreases from 3 to 0
after splitting.
9 USE CASES
P-DIFF supports two use cases, change validation and forensics
diagnosis, on top of its TCDT-based access control policy learning
described in §6–§8. In this section, we show how to use the learned
TCDT as a policy evolution representation for supporting the two
use cases.
9.1 Change Validation
P-DIFF continuously monitors the new-coming access results from
the access logs. For each access, P-DIFF calculates the expected
access results (ALLOW or DENY) based on the policy maintained
in the TCDT. When P-DIFF observes that the access results deviate
from the policy it currently maintains, P-DIFF treats the deviation
as the result of a policy change. P-DIFF then notifies sysadmins with
the changed access control rules and asks sysadmins to validate the
changes. If the sysadmins confirm the change to be expected, P-DIFF
updates the TCDT to incorporate the policy changes. Otherwise,
P-DIFF detects access control misconfigurations. It discards the
access results and keeps monitoring new access results (after the
misconfigurations get fixed by the sysadmins).
P-DIFF presents the change policy by extracting it from the cor-
responding path in the TCDT. Figure 10 (left) shows an example
of change validation. When monitoring a new access at timestamp
T5, P-DIFF calculates its access result based on the TCDT (which
is DENY); however, P-DIFF finds that the access was actually AL-
LOWed in the access log. The deviation leads to the validation
request.
9.2 Forensic Analysis
Given an access of interest (e.g., an illegal access that steals confi-
dential information), P-DIFF can pinpoint the policy change that
permitted the access by searching the policy evolution history
Figure 9: Two optimizations to efficiently calculate change-
count. The naïve implementation is to loop through the
whole access result array and count the changes. Optimiza-
tion 1 improves it by only looping through the seldom oc-
curred DENYs (value 1). Optimization 2 uses discrete convo-
lution [50] (f ∗ д)[n] to calculate the sum of every two adja-
cent numbers (if the sum is 1, there is a change).
maintained in the TCDT. This is achieved by finding the path in
the TCDT that determines the result of the access and searching
for the change that started permitting the target access in time
series encoded at the leaf node. Figure 10 (right) shows an example
of forensic analysis. Given a target access at T4, P-DIFF finds the
corresponding leaf node in TCDT and backtracks through the time
series to find out the root-cause policy change happened between
T2 and T3.
Note that forensic analysis can be done in-situ or serve as an
independent tool postmortem to any security incidents in which P-
DIFF reads historical access logs and builds the TCDT by analyzing
the history.
10 EVALUATION
We evaluate P-DIFF using controlled experiments based on datasets
collected from five real-world deployments of various systems with
different scales, including the Wikipedia website, the firewall of a
software company, and three websites hosted by academic organi-
zations. Table 4 describes these datasets.
10.1 Systems and Datasets
• Wikipedia. A free online encyclopedia website that has 33
million registered users. We collect access logs from a public
dataset of request traces to Wikipedia in September 2007 [62],
the largest trace dataset we can find online. Its access control is
implemented by the MediaWiki wiki engine [35]. Its protected
Multiple rules without change (should split)Policy Access logsubsetTime SeriesT1, GET, /proj/1.htm DENYT2, GET, /proj/2.htm ALLOWT3, GET, /proj/1.htm DENYT4, GET, /proj/2.htm ALLOW GET, /proj/1.htm DENY GET, /proj/2.htm ALLOWallowdenyTimestampStatusT1T2T3T4ChangeCount: 3allowdenyTimestampStatusT1T2T3T4ChangeCount: 0If not splitIf splitTime Span # Access
2 weeks
Dataset
Configuration
Wikipedia Application logic
Center
Course
Company
Group
Table 4: Datasets used in our evaluation. The datasets cover
a variety of systems, protection mechanisms, access control
configurations at different scales (§10.1).
Web server configuration 11 months
File permission
11 months
3 hours
Firewall
File permission
1 month
369M
5.9M
3.8M
100K
32K
Linux file permissions. The access policies are maintained by
one sysadmin.
10.2 Experimental Design
In order to evaluate the effectiveness of P-DIFF, it requires two
pieces of information: (1) access logs that records access requests
and access results, and (2) access control policy changes that con-
trolled the access results (the ground truth).
The Wikipedia dataset [62] includes both of the two pieces of in-
formation. The policy changes can be obtained based on Wikipedia’s
page protection change history [36], which records the protection
changes of each page (e.g., changed from publicly editable to only
accessible by specific users).
For Center, Course, Company and Group, we do not have the
policy change history (configuration changes in these systems were
not tracked). Therefore, we randomly generated policy changes and
synthesized access results for requests recorded in the access logs
(the original access results are ignored). If a generated change is
“DENY to ALLOW”, then the results for the related requests before the
change are set to DENY and after the change are set to ALLOW. Vice
versa, if a generated change is “ALLOW to DENY”, then the results
are set to ALLOW and DENY respectively. If a request has no related
change generated, it is set to ALLOW by default. In this way, we
also generated a random initial policy. Note another possible way
to synthesize access results is using a learning algorithm to infer
the initial policy from logs and applying the generated changes to
the initial policy. However, in this way the derived initial policy is
already easy to infer for a learning algorithm and so the synthesized
policy is also biased to be easy to infer. To avoid the bias, we use a
random initial policy instead of a learned initial policy.
As shown in Table 5, we generate different types of policy changes
to cover different scenarios. For each type, different attributes (sub-
ject, object, and actions) are selected to be changed. We also se-
lectively change policies that affect objects with different access
frequencies, categorized as “rare”, “normal”, and “frequent”. This
experiment design allows us to study how access frequency affects
P-DIFF’s policy learning. We randomly choose a time to make a
policy change for a given dataset. In total, each dataset contains 60
policy changes across its time span, with 15 changes in each type,
as detailed in Table 5.
All the experiments are conducted on an AWS instance, with
Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz CPU (16 cores), 64GB
memory, and Ubuntu 16.04.
Figure 10: Two use cases with P-DIFF: (a) change validation
and (b) forensic analysis. For (a), P-DIFF detects a policy
change based on the deviation of the access results at T5
and the policy maintained in TCDT. P-DIFF then notifies the
sysadmins with the policy change for validation. For (b), P-
DIFF backtracks the time series at the leaf node to pinpoint
the root-cause change between T2 and T3 that permitted the
access.
resources include protected pages of different protection levels,
such as full-protected and semi-protected pages, which can only
be modified by sysadmins and registered users respectively.
• Center A web server hosting home pages, online tools and
personal pages for a research center with more than 10 faculty
members and 50 graduate students in a research university. Its
resource protection is through the configuration of the Apache
HTTPD server (Figure 1). The protected resources include a
public website for news and personal pages, and an internal
website for group-internal resources. The protection policies
of the whole server are maintained by a sysadmin, but each
member can modify the protection policies of their own pages.
• Course A department-wide course website hosting 300+ courses
each year. Its resources are mainly protected by the Linux file
permissions. The protected resources include public and pri-
vate web pages of course materials. The protection policies of
different courses are maintained by the corresponding instruc-
tors and teaching assistants. Course materials can be changed
from private to public during the semester and changed back
to private after the semester.
• Company An Iptable firewall deployed by a software company
that serves millions of users. The policies include blocking IPs
and IP ranges to protect the company network against Internet-
based attacks.
• Group A website hosting group pages and personal pages for
a research group with more than 20 researchers. The resource
protection (mostly for private web pages) is done through the
IF $method == "PUT"TrueFalse...IF $prefix1 == "/proj"TrueIF $method == "GET"TrueFalse...IF $prefix1== "/proj"TrueNew-coming access: T5, PUT, /proj/1.htm ALLOWAccess of interest: T4, GET, /proj/1.htm ALLOWALLOWDENYTimestampStatusT1T2T3T4Policy change to validate: T4-T5, PUT, /proj DENY      ALLOWChange forensics: T2-T3, GET,  /proj DENY      ALLOWALLOWDENYTimestampStatusT1T2T3T5T4Category
File
permission
Change Types
(# Changes)
Type 1: allow file access (15)
Type 2: block file access (15)
Type 3: allow directory access (15)
Type 4: block directory access (15)
Web server Type 5: allow user access (15)
Type 6: block user access (15)
ACL
Type 7: allow GET/PUT method (15)
Type 8: block GET/PUT method (15)
Type 9: allow ip access (15)
Type 10: block ip access (15)
Type 11: allow subnet (15)
Type 12: block subnet (15)
Iptable
ACL
Dataset
Applied
Course
Group
Center
Company
Dataset
# Total
changes
Wikipedia
Center
Course
Company
Group
Total
25
18
18
21
17
99
# Detected Precision Recall
(FN)
changes
1.0 (0)
25 (100%)
16 (89%)
0.89 (2)
0.94 (1)
17 (94%)
0.86 (3)
18 (86%)
1.0 (0)
17 (100%)
93 (94%)
0.94 (6)
(FP)
1.0 (0)
0.76 (5)
0.85 (3)
0.81 (4)
1.0 (0)
0.89 (12)
Table 6: Policy changes detected by P-DIFF. FP stands for
false positive and FN stands for false negative.
Table 5: Different types of access control policy changes used
in the experiments for the Center, Course, Company, and
Group datasets. The detailed experiment design can be re-
ferred to in §10.2.
10.3 Overall Results
10.3.1 Change Validation. We evaluate P-DIFF’s effectiveness
in detecting policy changes. For each dataset, we divide the time
span (cf. Table 4) of the access logs into two parts: the first part is
used for training (observed accesses) and the second part is used
for testing (upcoming accesses). The first part takes 7
10 of the time
span, and the second part takes the rest 3
10. If a real policy change
is not detected, we count it as a false negative. If a detected policy
change is incorrect, we count it as a false positive.
Table 6 shows the number and percentage of policy changes
P-DIFF detects from each dataset. In total, P-DIFF detects 93 (94%)
out of 99 rule changes with 12 false positives and 6 false negatives.
For each dataset, P-DIFF generates less than 6 false positives and
less than 3 false negatives. For Wikipedia and Group datasets,
P-DIFF generates 0 false positives and negatives. This shows that P-
DIFF works effectively on small education systems (Group, Course,
Center), medium commercial systems (Company with millions of
users) as well as large-scale popular websites (Wikipedia ranked
the 7th popular website in the world [2]).
10.3.2 Forensic Analysis. To evaluate the effectiveness of foren-
sic analysis, we select an access of interest after each policy change.
The access of interest is an access that was supposed to be denied
based on the policy before the change, but is allowed after the policy
change. In other words, if the policy change is misconfigured, the
access could be illegal. We feed the access of interest into P-DIFF
and evaluate whether P-DIFF can pinpoint the root-cause policy
change that permits the access.
As shown in Table 7, P-DIFF pinpoints the root-cause policy
change for 283 (93%) out of 303 accesses of interest. For Wikipedia
and Center datasets, our inferred TCDT correctly encodes 114 out
of 123 changed rules on normal objects (i.e. user and method). For
the Course, Company and Group datasets, TCDT correctly encodes
169 out of 180 changed rules on hierarchical objects (i.e. directory
Dataset
Wikipedia
Center
Course
Company
Group
Total
# Access
of interest
Pinpointing
root-cause changes
63
60
60
60
60
303
61 (97%)
53 (88%)
59 (98%)