hypervisor on a map and Alice manually verifies its correctness.
Since GPS location accuracy can be within 5 to 10 meters, Alice
might only be able to recognize the location based on buildings and
street blocks.
5.4 Scene-based Residence Checking
In our platform for Dev, the device has a USB-camera which uses
five DMA buffers at fixed physical addresses. The camera periodi-
cally writes to these buffers with a batch of image blocks of variable
lengths. An image frame consists of one or more blocks, depending
on image size. Although an image frame has a header with a unique
frame identifier, a block has no header. A block’s offset within the
image and its size are stored in an array of USB Host Channel reg-
isters. Software is responsible for assembling the blocks into an
image frame with the assistance of the registers.
We place a hook into the kernel’s FIQ interrupt handler, which
is invoked by hardware when the camera completes one batch of
block delivery. The hook traps to hypervisor only when the latter
is in the residence checking session. With the assistance of USB
channel registers, hypervisor reads new blocks and assembles those
with the same frame identifier into an image frame.
Hypervisor computes an HMAC over the image frame. Simi-
lar to the location-based scheme, we use a user space application
to forward the image frame and its HMAC to Verifier over the
network.
6 EVALUATION
We experimented with the prototypes to assess performance and
security. In our setting, Dev is a Raspberry Pi-2 model B develop-
ment board with 900MHz quad-core ARM Cortex-A7 CPU and 1
GB RAM8. It is connected with a Microstack GPS module9 and a
Logitech HD webcam C525 with maximum 30 fps10, both of which
are via USB. Verifier is a Toshiba laptop with 2.4 GHz Intel Core
i7-5500 CPU and 8 GB RAM, with the same web camera as Dev.
In the rest of this section, we evaluate performance of proposed
protocols before assessing their security. We do it in this order,
since execution time is crucial to security of the two sight-based
protocols.
6.1 Performance
We implemented DRTM in the TrustZone. It mainly consists of a
hypervisor loader (90 lines of assembly and 234 lines of C code), and
a 187KB cryptographic library customized from the Mbed TLS11
version 1.3.10. We also implemented the hypervisor as the trust
anchor on the Raspberry PI board. Table 1 lists sizes of all major
prototype components in Dev and Verifier. The TCB in Dev consists
of DRTM code and the hypervisor. The development board that we
use was not shipped with a public key certificate. To this end, we
8https://www.raspberrypi.org/products/raspberry-pi-2-model-b/
9http://www.microstack.org.uk/products/microstack-gps/
10http://www.logitech.com/en-sg/product/hd-webcam-c525
11https://tls.mbed.org
manually installed RSA public/private key-pairs into its TrustZone
to simulate the DRTM credential.
It takes about 4.5 msec to dynamically launch a hypervisor, from
the moment of the application issuing an SMC call to the CPU
returning to user mode. The main cost is due to hash operations.
We also measure performance of the existence checking protocol.
It takes about 998.5 msec on Dev to execute Protocol 1, where the
dominant cost is RSA decryption and signing, each performed with
a 1024-bit key. With a 2048-bit RSA key, the time shoots up to
5.16 sec.
6.1.1
Sight-based Residence Checking. Table 2 lists performance
constants that can be measured without running the protocols. The
image to send refers to data that Verifier or Dev needs to place
into graphic frame buffers, which is not the same as image file
size. The image to receive refers to the image frame reconstructed
by software using data delivered by the camera. Values of τcam
and τdis are derived from the respective hardware specifications,
and τnet is based on measurement on a lightly-loaded LAN. LAN
congestion during the first network-send operation in Protocol 2
can only induce false positives, since it increases ATL. In our setting,
τnet = 6.3 msec, τcam = 33 msec, and τdis = 16 msec.
We also assess average CPU time of display-send and camera-
receive. For the former, we measured the time for software’s frame
buffer update, and waiting period between completion of frame
buffer update and arrival of the display synchronization signal. We
split CPU time of camera-receive into image reconstruction and full-
screen detection. We measured the period between the interrupt
of the first block of an image frame to the time of the frame being
assembled in memory. The time for full-screen detection is between
completion of frame reconstruction and the moment when it is
determined as full-screen. Results are summarized in Table 3. Note
that image reconstruction time is not counted within the noise
interval. In the camera I/O model, the fps rate takes into account
image reconstruction time.
To assess overall performance, Table 4 shows average turn-
around time of the two protocols, We also measured noise intervals
in both Dev and Verifier. The dominant component of δD is full-
screen detection time in Table 3, and dominant components of
δV are: (1) display-send in the basic protocol, and (2) full-screen
detection in the iterative protocol.
6.1.2 Location-based and Scene-based Protocols. We measured
the turnaround time (as reported in Table 5) for both location-based
and scene-based residence checking protocols. As a slow-speed
device, GPS sends stream data to the buffer at 10 Hz frequency,
which is the performance bottleneck for the protocol. Nonetheless,
this protocol can be combined with existence checking to reduce
overhead.
Turn-around time of the scene-based protocol is the sum of
hypervisor’s image reconstruction, network transmission delay and
Verifier’s user-space execution time, including HMAC verification
and invocation of graphics library functions.
In both protocols, Alice’s manual verification of the location
and the image is not factored into the turnaround time. In our
experiments, it does not take a noticeable delay for the user to
verify presence. As part of our future work, we plan to conduct a
user study to better understand verification behavior.
CPU Mode
Hyp
Usr
Svc
User
Kernel
Dev
Verifier
204
109
703
46
Sight-based (Basic)
Sight-based (Iterative)
Location-based
248 (142)
380 (150)
248 (88)
54
89
710
347
Scene-based
248 (127)
282
197
248
-
272
141
286
-
Table 1: Code size in Dev and Verifier (in SLOC). The numbers of assembly code lines are in the brackets.
Image to send (KB)
Image to receive (KB)
barcode detection (msec)
HMAC speed ( ms/KB)
Verifier Dev
306
21
-
0.1
54
203
0.012
0.05
Table 2: Constants in performance measurement.
Display-send
Camera-receive
frame buffer update
waiting time
Image reconstruction
Full-screen detection
Verifier Dev
0.5
8.2
14.9
31.8
0.02
7.9
31.9
33.2
Table 3: Time for component steps in display-send and camera-
receive (msec)
Protocol Turnaround
Verifier’s noise interval δV
Dev’s noise interval δD
Basic
132
15.6
-
Iterative (n = 1)
200.8
17.0
44.7
Table 4: Turn-around for two sight-based protocols and noise in-
tervals in Dev and Verifier (msec)
Location-based
Dev
Verifier
403.1
23.0
Scene-based
Dev
Verifier
23.9
4.4
Turnaround
Table 5: Turn-around for location- and scene-based protocols
(msec)
6.2 Security of Sight-based Protocols
Since security of sight-based protocols depends on ATL of the
channel, we ran experiments to evaluate whether basic and iterative
protocols correctly verify an uncorrupted device and detect analog
cuckoo attacks.
6.2.1 Attestation under No Attack. As described in Section 3,
the D2C interval is uniformly distributed in [0, τcam]. Hence, ATL
in the basic protocol follows the uniform distribution between:
∆min = τdis + τcam and ∆max = τdis + 2τcam, which are 49
msec and 82 msec, respectively. Thus, the average is 65.5 msec and
standard deviation is 9.5 msec.
ATL in the iterative protocol for n = 1 is the sum of two indepen-
dent ATL-s for two separate D2C channels. Therefore, it follows
Irwin−Hall distribution between [∆I
min, ∆I
max =
2∆max = 164 msec, and ∆I
= 2∆min = 98. The average is
2× 65.5 = 131 msec and standard deviation is
2× 9.5 = 13.4 msec.
We ran each protocol 50 times and computed: average, maximum,
minimum, and standard deviation of measured ATLs. As shown
in Table 6, results corroborate our analytical ATL models in each
protocol. Largest ATLs are below ∆max and ∆I
max . Furthermore,
we observed no false positives.
max ], where ∆I
√
min
Basic
49
82
Iterative (n = 1)
98
∆I
min
164
∆I
max
Experiment Analytic
Model
Experiment
ATL
max. ATL
min. ATL
std. of ATL
127.5
156.3
108.5
16.9
Table 6: ATL in the basic and iterative protocols (msec)
64.0
81.7
50.1
8.1
131
164
98
13.4
∆min
∆max
Analytic
Model
65.5
82