ally bounded adversaries, given the amazing eﬀects of such a restriction in modern cryptography.
77
To obtain a computational analogue of diﬀerential privacy, we can simply take the inequalities
deﬁning diﬀerential privacy, namely:
∀T ⊆ Y, Pr[M(x) ∈ T ] ≤ eε · Pr[M(x(cid:48)) ∈ T ] + δ
and restrict our attention to tests T deﬁned by feasible algorithms.
n → Yn}n∈N be a
Deﬁnition 10.1 (computational diﬀerential privacy [7]). Let M = {Mn : Xn
sequence of randomized algorithms, where elements in Xn and Yn can be represented by poly(n)-
bit strings. We say that M is computationally ε-diﬀerentially private if there is a superpolynomial
function s(n) = nω(1) and a negligible function δ(n) = n−ω(1) such that for all n, all pairs of datasets
x, x(cid:48) ∈ Xn diﬀering on one row, and all boolean circuits T : Xn → {0, 1} of size at most s(n), we
have:
Pr[T (M(x)) = 1] ≤ eε · Pr[T (M(x(cid:48))) = 1] + δ(n).
We make few remarks on the deﬁnition:
• We always allow for a nonzero δ = δ(n) term in the deﬁnition of computational diﬀer-
ential privacy.
If we didn’t do so, then the deﬁnition would collapse to that of ordinary
(information-theoretic) (ε, 0)-diﬀerential privacy, because the latter is equivalent to requiring
(ε, 0)-diﬀerential privacy for sets T of size 1, which are computable by boolean circuits of size
poly(n).
• We generally are only interested in computationally diﬀerentially private mechanisms M that
are themselves computable by randomized polynomial-time algorithms, as we should allow
the adversary T to invest more computation time than the privacy mechanism.
• For simplicity, we have used the number n of rows as a security parameter, but it is often
preferable to decouple these two parameters. We will often drop the index of n from the
notation, and make the asymptotics implicit, for sake of readability.
10.2 Constructions via Secure Multiparty Computation
The most signiﬁcant gains we know how to get from computational diﬀerential privacy are in the
multiparty case. Indeed, by using powerful results on secure multiparty computation, everything
that is achievable by a diﬀerentially private centralized curator can also be emulated by a multiparty
protocol with computational diﬀerential privacy.
Theorem 10.2 (computational diﬀerential privacy via cryptography [37, 7]). Assume that oblivious
transfer protocols exist. Let M : Xn → Y be computationally ε-diﬀerentially private for ε ≤ 1 and
computable in time poly(n). Then for every m|n, there is an m-party protocol P = (P1, . . . , Pm) :
(Xn/m)m → Y such that
1. P is computationally ε-diﬀerentially private,
2. For every input x ∈ Xn, the output distribution of P (x) is the same as that of M : (Xn/m)m →
Y,
3. P is computable in time poly(n).
78
Proof sketch. By classic results on secure multiparty computation [108, 52], there exists an m-party
protocol P for evaluating M that is secure against passive adversaries, assuming the existence of
oblivious transfer protocols. (See [73, 51] for full deﬁnitions and constructions of secure multiparty
computation.) Items 2 and 3 are immediately guaranteed by the properties of secure multiparty
computation protocols. For, Item 1, we recall that each party learns nothing from a secure mul-
tiparty computation protocol other than what is implied by their own input and the output of
the function being evaluated (in this case M). More precisely, for every poly(n)-size adversary
A, controlling all parties other than Pk, there is a poly(n)-size simulator S such that ViewA(A ↔
(P1, . . . , Pm(x)) is computationally indistinguishable from S(M(x), x1, . . . , xk−1, xk+1, . . . , xm). Thus,
for every x and x(cid:48) that diﬀer only by changing one row of the input to party j, and every poly(n)-size
T , we have:
Pr[T (ViewA(A ↔ (P1, . . . , Pm)(x))) = 1]
≤ Pr[T (S(M(x), x1, . . . , xk−1, xk+1, . . . , xm)) = 1] + negl(n)
= (cid:0)eε · Pr[T (S(M(x(cid:48)), x(cid:48)
m)) = 1] + negl(n)(cid:1) + negl(n)
≤ eε ·(cid:0)Pr[T (ViewA(A ↔ (P1, . . . , Pm)(x(cid:48)))) = 1] + negl(n)(cid:1) + negl(n) + negl(n)
k+1, . . . , x(cid:48)
1, . . . , x(cid:48)
k−1, x(cid:48)
= eε · Pr[T (ViewA(A ↔ (P1, . . . , Pm)(x(cid:48)))) = 1] + negl(n).
n) error achievable with information-theoretic diﬀerential privacy.
In particular, with computational diﬀerential privacy, we have n-party protocols for computing
√
any counting query or the normalized inner product function with error O(1/εn), signiﬁcantly
better than the ˜Θ(1/
It is
interesting to understand to what extent general secure multiparty computation (whose existence
is equivalent to oblivious transfer) is necessary for such separations between information-theoretic
and computational diﬀerential privacy. Haitner et al. [56] showed that black-box use of one-way
functions does not suﬃce to construct two-party protocols for the inner product function with error
smaller than ˜Θ(1/
n), but a tight characterization remains open.
√
Open Problem 10.3. What is the minimal complexity assumption needed to construct a compu-
tational task that can be solved by a computationally diﬀerentially private protocol but is impossible
to solve by an information-theoretically diﬀerentially private protocol?
Recent works have made progress on understanding this question for computing boolean func-
tions with diﬀerential privacy, for example showing that achieving near-optimal accuracy requires
oblivious transfer in some cases [53], but it remains open whether there can be a separation based
on a weaker assumption, and whether oblivious transfer is needed to have an asymptotic separa-
tion in accuracy for a more natural statistical task (e.g. estimating a function with bounded global
sensitivity, such as normalized inner product).
10.3 Usefulness with a Trusted Curator?
For the single-curator case (m = 1), computational and information-theoretic diﬀerential privacy
seem closer in power. Indeed, Groce et al. [55] showed that in the case of real-valued outputs, we
can often convert computational diﬀerentially private mechanisms into information-theoretically
diﬀerentially private mechanisms.
79
Theorem 10.4 (from computational to information-theoretic diﬀerential privacy [55]). Let M :
Xn → R be an ε-computationally diﬀerentially private mechanism with the property that for every
dataset x ∈ Xn, there is an interval Ix of width at most w(n) such that Pr[M(x) /∈ Ix] ≤ negl(n),
and the endpoints of Ix are rational numbers with poly(n) bits of precision. Deﬁne M(cid:48)(x) to be the
mechanism that runs M(x) and rounds the result to the nearest multiple of α(n) = w(n)/nc, for
any desired constant c. Then M(cid:48) is (ε, negl(n))-diﬀerentially private.
Thus, the error incurred is an arbitrary polynomial small fraction of the “spread” of M’s outputs.
Proof. Let I(cid:48)
x denote the rounding of all points in Ix to the nearest multiple of α(n); notice that
|I(cid:48)
x| ≤ w(n)/α(n) + 1 ≤ nc + 1. M(cid:48) is computationally diﬀerentially private because M is, and
we’ll use this to show that it is actually information-theoretically diﬀerential private: For every
x, x(cid:48) ∈ Xn that diﬀer on one row and every T ⊆ R, we have:
Pr[M(cid:48)(x) ∈ T ] ≤
≤
 (cid:88)
 (cid:88)
y∈I(cid:48)
x∩T
y∈I(cid:48)
x∩T
 + Pr[M(cid:48)(x) /∈ I(cid:48)
Pr[M(cid:48)(x) = y]
(cid:0)eε · Pr[M(cid:48)(x(cid:48)) = y] + negl(n)(cid:1) + negl(n)
x]
≤ eε · Pr[M(cid:48)(x(cid:48)) ∈ T ] + (nc + 1) · negl(n) + negl(n)
= eε · Pr[M(cid:48)(x(cid:48)) ∈ T ] + negl(n),
where the second inequality uses the fact that testing equality with a ﬁxed value y or testing
membership in an interval can be done by polynomial-sized circuits, provided the numbers have
only poly(n) bits of precision.
This proof technique extends to low-dimensional outputs (e.g. answering a logarithmic number
of real-valued queries) as well as outputs in polynomial-sized discrete sets [55, 24]. So to get
a separation between computational and information-theoretic diﬀerential privacy with a single
curator, we need to use large or high-dimensional output spaces, or measure utility in a diﬀerent
way (not by a low-dimensional metric). Such a separation was recently obtained by Bun et al. [24]:
Theorem 10.5 (separating computational and information-theoretic diﬀerentially private cura-
tors [24]). Assuming the existence of sub-exponentially secure one-way functions and “exponen-
tially extractable non-interactive witness indistinguishable (NIWI) proofs for NP”, there exists an
eﬃciently computable utility function u : Xn × Y → {0, 1} such that
1. There exists a polynomial-time CDP mechanism MCDP such that for every dataset x ∈ Xn,
we have Pr[u(x, MCDP(x)) = 1] ≥ 2/3.
2. There exists a computationally unbounded diﬀerentially private mechanism Munb such that
for every dataset x ∈ Xn, we have Pr[u(x, Munb(x)) = 1] ≥ 2/3.
3. For every polynomial-time diﬀerentially private M, there exists a dataset x ∈ Xn, such that
Pr[u(x, M(x)) = 1] ≤ 1/3.
Note that this theorem provides a task where achieving information-theoretic diﬀerential privacy
is infeasible — not impossible. Moreover, it is for a rather unnatural, cryptographic utility function
u. It would be interesting to overcome either of these limitations:
80
Open Problem 10.6. Is there a computational task solvable by a single curator with computa-
tional diﬀerential privacy but is impossible to achieve with information-theoretic diﬀerential privacy?
Open Problem 10.7. Can an analogue of Theorem 10.5 be proven for a more “natural” utility
function u, such as one that measures the error in answering or summarizing the answers to a set
of counting queries?
10.4 Relation to Pseudodensity
The deﬁnition of computational diﬀerential privacy is related to concepts studied in the literature
on pseudorandomness. For random variables Y, Z taking values in Y and ρ ∈ [0, 1], we say that Y
has density at least ρ in Z if for every event T ⊆ Y, we have
ρ · Pr[Y ∈ T ] ≤ Pr[Z ∈ T ].
For intuition, suppose that Y and Z are uniform on their supports. Then this deﬁnition says that
Supp(Y ) ⊆ Supp(Z) and | Supp(Y )| ≥ ρ · | Supp(Z)|. Additionally, if Z is the uniform distribution
on Y, then Y having density at least ρ in Z is equivalent to Y having “min-entropy” at least
log(ρ|Y|). Notice that a mechanism M is (ε, 0)-diﬀerentially private iﬀ for every two neighboring
datasets x ∼ x(cid:48), M(x) has density at least e−ε in M(x(cid:48)).
Just like computational analogues of statistical distance (namely, computational indistinguisha-
bility and pseudorandomness) have proven to be powerful concepts in computational complex-
ity and cryptography,
it turns out that computational analogues of density and min-entropy
have also turned out to be quite useful, with applications including additive number theory [54],
leakage-resilient cryptography [48], and constructions of cryptographic primitives from one-way
functions [61].
One of the computational analogues of density that has been studied, called pseudodensity (or
sometimes metric entropy when Z is uniform on Y) [3, 89] is precisely the one used in the deﬁnition
of computational diﬀerential privacy, namely that for every polynomial-sized boolean circuit T , we
have:
ρ · Pr[T (Y ) = 1] ≤ Pr[T (Z) = 1] + negl(n).
When considering a single pair of random variables (Y, Z), the Dense Model Theorem of [54,
99, 89] says that pseudodensity is equivalent to Y being computationally indistinguishable from
a random variable ˜Y that truly has density at least ρ in Z. Mironov et al. [80] asked whether
something similar can be said about (computationally) diﬀerentially private mechanisms, which
require (pseudo)density simultaneously for all pairs M(x), M(x(cid:48)) where x ∼ x(cid:48):
Open Problem 10.8. For every ε-computationally diﬀerentially private and polynomial-time
computable mechanism M : Xn → Y, is there an (O(ε), negl(n))-diﬀerentially private mechanism
˜M : Xn → Y such that for all datasets x ∈ Xn, M(x) is computationally indistinguishable from
˜M(x)?
A positive answer to this question would imply a negative answer to Open Problem 10.6.
11 Conclusions
We have illustrated rich connections between the theory of diﬀerential privacy and numerous topics
in theoretical computer science and mathematics, such as learning theory, convex geometry and
81
optimization, cryptographic tools for preventing piracy, probabilistically checkable proofs and ap-
proximability, randomness extractors, information complexity, secure multiparty computation, and
notions of pseudoentropy. There have also been very fruitful interactions with other areas. In par-
ticular, in both game theory and in statistics, diﬀerential privacy has proved to be a powerful tool
for some applications where privacy is not the goal — such as designing approximately truthful
mechanisms [78, 86] and preventing false discovery in adaptive data analysis [45]. Remarkably,
both positive and negative results for diﬀerential privacy (including both information-theoretic and
computational lower bounds as we have seen in this tutorial) have found analogues for the false
discovery problem [45, 59, 97, 6], suggesting that it will also be a very fertile area for complexity-
theoretic investigation.
We now mention some more directions for future work in diﬀerential privacy, beyond the many
open problems stated in earlier sections. As illustrated in previous sections, there has been a thor-
ough investigation of the complexity of answering counting queries under diﬀerential privacy, with
many algorithms and lower bounds that provide nearly matching results. While there remain nu-
merous important open questions, it would also be good to develop a similar kind of understanding
for other types of computations. There is now a wide literature on diﬀerentially algorithms for
many types of data analysis tasks, but what is missing are negative results to delineate the border
between possible and impossible.
Open Problem 11.1. Classify large classes of problems (other than counting queries) in diﬀeren-
tial privacy according to their privacy/utility tradeoﬀs and their computational tractability.
Two areas of particular interest, both in theory and in practice, are:
Statistical inference and machine learning. In this tutorial, we have mostly been measuring
accuracy relative to the particular (worst-case) dataset that is given as input to our diﬀeren-
tially private algorithm. However, in statistical inference and machine learning, the goal is
usually to infer properties of the population from which the dataset is (randomly) drawn. The
PAC model studied in Section 8 is a theoretically appealing framework in which to study how
such tasks can be done with diﬀerential privacy, but there are many inference and learning
problems outside the PAC model that are also of great interest. These problems include tasks
like hypothesis testing, parameter estimation, regression, and distribution learning, and a va-
riety of utility measures such as convergence rates, p values, risk minimization, and sizes of
conﬁdence intervals. Moreover, the data distributions are often assumed to have a signiﬁcant
amount of structure (or enough samples are taken for Central Limit Theorems to provide such
structure), in contrast to the worst-case distributions considered in the PAC model. Some
broad positive results are in Smith [94], Bassily et al. [5] and some negative results are in
[31, 21, 5], but our understanding of these types of problems is still quite incomplete.
Graph privacy. As mentioned in Section 3, there has been some very interesting work on diﬀeren-
tially private graph analysis, where our dataset is a graph and we are interested in protecting
either relationships (edge-level privacy) or everything about an individual/vertex (node-level
privacy). We refer to Raskhodnikova and Smith [87] for a broader survey of the area. Again,
most of the work to date has been algorithmic, and we still lack a systematic understanding
of impossibility and intractability.
If the existing study of diﬀerential privacy is any indication, these studies are likely to uncover a
rich theoretical landscape, with even more connections to the rest of theoretical computer science.
82
Acknowledgments
This tutorial was written starting from notes taken during a minicourse given at the 26th McGill
Invitational Workshop on Computational Complexity in February 2014, at the Bellairs Institute in
Holetown, Barbados [1]. Special thanks go to Kunal Talwar for giving several of the lectures (leading
to material in Sections 5.1 and 7.3 here), to the workshop attendees who wrote up the lecture
notes (Eric Allender, Borja Balle, Anne-Sophie Charest, Lila Fontes, Antonina Kolokolova, Swastik
Kopparty, Michal Kouck´y, Cristopher Moore, Shubhangi Saraf, and Luc Segouﬁn), to Alexander
Russell for collecting and editing the notes, to Denis Th´erien for organizing the workshop, to all of
the participants for illuminating comments during the workshop, and to Sammy Innis for surﬁng
lessons.
I am grateful to Cynthia Dwork, Ilya Mironov, and Guy Rothblum, who got me started on
diﬀerential privacy during a month-long visit to the wonderful (sadly, now defunct) Microsoft
Research Silicon Valley lab in 2008, and numerous collaborators and students since then, whose