such that |U| ≤ k. There are two desirable properties for an
RA, informally given below.
• Privacy: it is hard for an adversary controlling a set of
processes Pmal ⊆ P to determine whether there are other
processes (i.e., Pmal = P or Pmal ⊂ P) from observing the
allocations of processes in Pmal.
• Liveness: for all sets of processes P, occasionally at least
one process in P receives a unit of the resource.
The liveness property is the weakest definition of progress
needed for RAs to be useful, and helps to rule out an RA that
achieves privacy by never allocating resources.
A. Formal definition
Notation. We use poly(λ) and negl(λ) to mean a polynomial
and negligible function2 of λ’s unary representation (1λ). We
2A function f : N → R is negligible if for all positive polynomials poly, there
exists an integer c such that for all integers x greater than c, |f (x)| < 1/poly(x).
3
symbol
C and A
b and b′
k
M
P
Phon
Pmal
U
λ
βx
description
Challenger and adversary in the security game resp.
Challenger’s coin flip and adversary’s guess resp.
Amount of available resource
Universe of processes
Processes requesting service concurrently (⊆ M)
Honest processes in P (not controlled by A)
Malicious processes in P (controlled by A)
Allocation (⊆ P) of size at most k
Security parameter
poly(λ) bound on variable x
FIG. 2—Summary of terms used in the security game, lemmas, and
proofs, and their corresponding meaning.
use βx to mean a poly(λ) bound on variable x. Upper case
letters denote sets of processes. Figure 2 summarizes all terms.
Security game. We define privacy with a game played between
an adversary A and a challenger C. The game is parameterized
by a resource allocator RA and a security parameter λ. RA
takes as input a set of processes P from the universe of all
processes M, a resource capacity k that is poly(λ), and λ. RA
outputs a set of processes U ⊆ P, such that |U| ≤ k.
1) A is given oracle access to RA, and can issue an arbitrary
number of queries to RA with arbitrary inputs P and k. For
each query, A can observe the result U ← RA(P, k, λ).
2) A picks a positive integer k and two disjoint sets of
processes Phon, Pmal ⊆ M and sends them to C. Here Phon
represents the set of processes requesting a resource that
are honest and are not compromised by the adversary. Pmal
represents the set of processes requesting a resource that
are compromised by the adversary.
3) C samples a random bit b uniformly in {0, 1}.
4) C sets P ← Pmal if b = 0 and P ← Pmal ∪ Phon if b = 1.
5) C calls RA(P, k, λ) to obtain U ⊆ P where |U| ≤ k.
6) C returns Umal = U ∩ Pmal to A.
7) A outputs its guess b′, and wins the game if b = b′.
In summary, the adversary’s goal is to determine if the
challenger requested resources for the honest processes or not.
Definition 1 (Information-theoretic privacy). An allocator RA
is IT-private if in the security game, for all algorithms A,
Pr[b = b′] = 1/2, where the probability is over the random
coins of C and RA.
Definition 2 (Computational privacy). An allocator RA is C-
private if in the security game given parameter λ, for all
probabilistic polynomial-time algorithms A, the advantage of
A is negligible: | Pr[b = b′] − 1/2| ≤ negl(λ), where the
probability is over the random coins of C and RA.
Definition 3 (Liveness). An allocator RA guarantees liveness
if given parameter λ, any non-empty set of processes P, and
positive resource capacity k, Pr[RA(P, k, λ) ̸= ∅] ≥ 1/poly(λ).
4
The proposed liveness definition (Def. 3) is very weak. It
simply states that the allocator must occasionally output at least
one process. Notably, it says nothing about processes being
allocated resources with equal likelihood, or that every process
is eventually serviced (it allows starvation). Nevertheless, this
weak definition is sufficient to separate trivial from non-trivial
allocators; we discuss several other properties such as fairness
and resource monotonicity in Section V. To compare the
efficiency of non-trivial allocators, however, we need a stronger
notion that we call the allocator’s utilization.
Definition 4 (Utilization). The utilization of a resource alloca-
tor RA is the fraction of requests serviced by RA compared to
the number of requests that would have been serviced by a non-
private allocator. Formally, given a set of processes P, capacity
k, and parameter λ, RA’s utilization is E(U)/ min(|P|, k), where
E(U) is the expected number of output processes of RA(P, k, λ).
B. Prior allocators fail
Before describing our constructions we discuss why straight-
forward resource allocators fail to achieve privacy.
FIFO allocator. A FIFO allocator simply allocates resources to
the first k processes. This is the type of allocator currently used
by MPM systems to assign dialing friends to channels (§II-A),
and is also commonly found in cluster job schedulers (e.g.,
Spark [84]). This allocator provides no privacy. To see why,
suppose that both Phon and Pmal are ordered sets, where the
order stems from the identity of the process. The adversary
can interleave the identity of processes in Phon and Pmal so
that the FIFO allocator’s output is k processes in Pmal when
b = 0, and k/2 processes in Pmal when b = 1.
Uniform allocator. Another common allocator is one that
picks k of the processes at random. At first glance this might
appear to provide privacy since processes are being chosen
uniformly. Nevertheless, this allocator leaks a lot of information.
In particular, when b = 0 the adversary expects k of its
processes to be allocated (since P = Pmal), whereas when b = 1,
fewer than k of the malicious processes are likely to be allocated.
More formally, let X be the random variable describing the
cardinality of the set returned to A, namely |U∩Pmal|. Suppose
|Pmal| = |Phon| = k. Then Pr[X < k | b = 0] = 0 and
Pr[X < k | b = 1] = 1 − (k! · k!)/(2k)! ≥ 1/2. As a result, A
can distinguish between b = 0 and b = 1 with non-negligible
advantage by simply counting the elements in U ∩ Pmal.
Uniform allocator with variable-sized output. One of the
issues with the prior allocator is that the size of the output
reveals too much. We could consider a simple fix that selects
an output size s uniformly from the range [0, k], and allocates
s processes at random. But this is also not secure.
Let |Pmal| = |Phon| = k, and let X be the random variable
representing the cardinality of the set returned to A. We show
that the probability that X = k is lower when b = 1. Observe
k+1, whereas Pr[X = k | b = 1] =
that Pr[X = k | b = 0] = 1
(k! · k!)/((k + 1)(2k)!) < 1
k+1 for all k ≥ 1. Furthermore,
when k ≥ 1, (k! · k!)/((k + 1)(2k)!) ≤ 1/2. Therefore,
k+1 −
1
allocator
leakage
utilization
SRA (§IV-A)
None
RRA (§IV-B)
None
|P|
βM
|P|
βP
DPRA (§IV-C)
1/g(λ)
|P|
|P|+h(λ)βhon
assumptions
• setup phase
• |M| ≤ βM
• p ∈ M identifiable
|P| ≤ βP
|Phon| ≤ βhon
•
•
FIG. 3—Comparison of privacy guarantees, utilization, and assump-
tions of different PRAs. DPRA makes the weakest assumptions since
Phon ⊆ P ⊆ M and is the only one that tolerates an arbitrary number
of malicious processes. g and h are polynomial functions that control
the tradeoff between utilization and privacy (§IV-C).
k+1 · [1 − 1/2] = 1
(k! · k!)/((k + 1)(2k)!) ≥ 1
2(k+1), which is
non-negligible. As a result, A can distinguish b = 0 and b = 1
with non-negligible advantage.
Allocator from a secret distribution. The drawback of
the prior allocator is that the adversary knows the expected
distribution under b = 0 and b = 1 for its choice of Phon, Pmal,
and k. Suppose instead that the allocator has access to a secret
distribution not known to the adversary. The allocator then
uses the approach above (allocator with variable-sized output)
with the secret distribution instead of a uniform distribution.
This is also not secure; the proof is in Appendix A.
The intuition for the above result is that the perturbation
introduced by steps 4 and 6 of the security game cannot be
masked without additional assumptions. To formalize this, we
present the following impossibility result that states that without
a bound on the number of processes, an allocator cannot
simultaneously achieve privacy and our weak definition of
liveness. We focus on IT-privacy since C-privacy considers a
PPT adversary; by definition, the size of the sets of processes
that such an adversary can create is bounded by a polynomial.
Theorem 1 (Impossibility result). There does not exist a
resource allocator RA that achieves IT-privacy (Def. 1) and
Liveness (Def. 3) when k is poly(λ) and |P| is not poly(λ).
The proof is given in Appendix B.
IV. ALLOCATOR CONSTRUCTIONS
Given the impossibility result in the prior section, we propose
several allocators that guarantee liveness and some variant of
privacy under different assumptions. As a bare minimum, all
constructions assume a poly(λ) bound, βhon, on |Phon|. In the
context of MPM systems, this basically means that a user never
receives more than a polynomial number of dial requests by
honest users asking to start a conversation in the same round—
which is an assumption that is easy to satisfy in practice. We
note that none of our allocators can hide βhon from an adversary,
so it is best thought of as a public parameter. We summarize
the properties of our constructions in Figure 3.
A. Slot-based resource allocator
We now discuss a simple slot-based resource allocator. It
guarantees information-theoretic privacy and liveness under
the assumption that the size of the universe of processes (|M|)
has a bound βM that is poly(λ). The key idea is to map each
process p ∈ M to a unique “allocation slot” (so there are at
most βM total slots), and grant resources to processes only if
they request them during their allocated slots. The chosen slots
are determined by a random λ-bit integer r.
Slot-based resource allocator SRA:
• Pre-condition (setup): ∀p ∈ M, slot(p) ∈ [0,|M|)
• Inputs: P, k, λ
• r ←R [0, 2λ)
• U ← ∅
• ∀p ∈ P, i ∈ [0, k), if slot(p) ≡ r + i mod |M|, add p to U
• Output: U
Lemma 1. SRA guarantees IT-privacy (Def. 1).
Proof. Observe that a process p ∈ P is added to U when
r ≤ slot(p) < (r + k) mod |M|, which occurs independently
of b. In particular, if we let Ep be the event that a process p ∈ P
is added to U, then Pr[Ep|b = 0] = Pr[Ep|b = 1] = k/|M|.
Since an adversary cannot observe differences in Pr[Ep] when
P = Pmal versus P = Pmal ∪ Phon, privacy is preserved.
Lemma 2. SRA guarantees Liveness (Def. 3) if |M| ≤ βM.
Proof. SRA outputs at least one process when there is a p ∈ P
such that r ≤ slot(p) < (r + k) mod |M|. For a given r, this
occurs with probability ≥ k/|M|.
SRA achieves our desired goals. It guarantees privacy and
liveness, and achieves a utilization (Def. 4) of |P|
|M| whenever
k ≤ |P|. But it also has several limitations. First, it assumes
that the cardinality of the universe of processes (|M|) is known
in advance, and that it can be bounded by βM. Second, it
assumes a preprocessing phase in which each process in M is
assigned a slot. Finally, it assumes that each individual process
is identifiable since SRA must be able to compute slot(p) for
every process p ∈ P.
Unfortunately, these limitations are problematic for many
applications. For instance, consider an MPM system (§II). M
represents the set of friends for a user (not just the ones dialing),
so it could be large. Furthermore, users cannot add new friends
without leaking information since this would change M (and
therefore the periodicity of allocations), which the adversary
can detect. As a result, users must bound the maximum set
of friends that they will ever have (βM), use this bound in the
allocator (instead of |M|), and achieve a utilization of |P|
B. Randomized resource allocator
βM
.
In this section we show how to relax most of the assumptions
that SRA makes while achieving better utilization. In particular,
we construct a randomized resource allocator RRA that guaran-
tees privacy and liveness under the assumption that there is a
poly(λ) bound, βP, for the number of simultaneous processes
5
requesting a resource (|P|). RRA does not need a setup phase,
and does not require uniquely identifying processes in M. More
importantly, RRA achieves both requirements even when the
universe of processes (M) is unbounded. These relaxations are
crucial since they make RRA applicable to situations in which
processes are created dynamically.
At a high level, RRA works by padding the set of processes
(P) with enough dummy processes to reach the upper bound
(βP). RRA then randomly permutes the padded set and outputs
the first k entries (removing any dummies from the allocation).
If the permutation is truly random, this allocator guarantees
information-theoretic privacy since P is always padded to βP
elements regardless of the challenger’s coin flip (b). However,
it requires a source of more than βP random bits, which might
be too much in some scenarios. One way to address this is
to generate the random permutations on the fly [18], which
requires only O(k log(βP)) random bits. Alternatively, we can
simply assume that the adversary is computationally bounded
and allow a negligible leakage of information by making the
permutation pseudorandom instead.
Randomized resource allocator RRA:
• Inputs: P, k, λ
• Q ← set of dummy processes of size βP − |P|
• π ← random or pseudorandom permutation of P ∪ Q
• U ← first k entries in π
• Output: U ∩ P
Lemma 3. RRA guarantees IT-privacy (Def. 1) if |P| ≤ βp
and the permutation is truly random.
Proof. Let Ep be the event that a process p is added to U. Then,
for all p ∈ P, Pr[Ep] = k/βP. Since Pr[Ep] remains constant
for all sets of processes P, an adversary has no advantage to
distinguish between P = Pmal and P = Pmal ∪ Phon.
Lemma 4. RRA guarantees C-privacy (Def. 2) against all
probabilistic polynomial-time (PPT) adversaries if |P| ≤ βP.
Proof. We use a simple hybrid argument. Consider the variant
of RRA that uses a random permutation instead of a PRP.
Lemma 3 shows the adversary has no advantage to distinguish
between b = 0 and b = 1. A PPT adversary distinguishes
between the above RRA variant and one that uses a PRP (with
security parameter λ) with negl(λ) advantage.
Lemma 5. RRA guarantees Liveness (Def. 3) if |P| ≤ βP.
Proof. RRA outputs at least one process if there exists a p ∈ P
in the first k elements of π. This follows a hypergeometric
distribution since we sample k out of βP processes without
replacement, and processes in P are considered a “success”.