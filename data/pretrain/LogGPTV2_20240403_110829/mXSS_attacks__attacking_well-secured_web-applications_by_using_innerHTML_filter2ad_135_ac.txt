fault evaluated in their canonic form upon the occurrence
of innerHTML-access. This problem has been reported and
mitigated by the a(cid:11)ected browser vendors and is listed here
to further support our argument. The code example in List-
ing 11 showcases anatomy of this attack.
Listing 11: Misusing HTML entities in inline-SVG
CSS-string properties to execute arbitrary Java-
Script
   ’} 
This vulnerability was present in a popular open-source
user agent and has been since (cid:12)xed successfully, following a
bug report.
3.8 Summary
In order to initiate the mutation, all of the exploits shown
here require a single access to the innerHTML property of
a surrounding container, while except for the attack vector
discussed in Section 3.1, all other attacks can be upgraded
to allow recursive mutation { making double-, triple- and
further multiply-encoded escapes and entities useful in the
attack scenario,
immediately when multiple innerHTML-
access to the same element takes place. The attacks were
successfully tested against a large range of publicly available
web applications and XSS (cid:12)lters { see Section 4.
4. ATTACK SURFACE
The attacks outlined in this paper target the client-side
web application components, e.g. JavaScript code, that use
the innerHTML property to perform dynamic updates to
the content of the page. Rich text editors, web email clients,
dynamic content management systems and components that
pre-load resources constitute the examples of such features.
In this section we detail the conditions under which a web
application is vulnerable. Additionally, we attempt to es-
timate the prevalence of these conditions in web pages at
present.
The basic conditions for a mutation event to occur are
the serialization and deserialization of data. As mentioned
in Section 2, mutation in the serialization of the DOM-tree
occurs when the innerHTML property of a DOM-node is ac-
cessed. Subsequently, when the mutated content is parsed
back into a DOM-tree, e.g. when assigned to innerHTML
or written to the document using document.write, the mu-
tation is activated.
The instances in Listing 12 are far from being the exclusive
methods for a mutation event to occur, but they exemplify
vulnerable code patterns. In order for an attacker to exploit
such a mutation event, it must take place on the attacker-
supplied data. This condition makes it di(cid:14)cult to statisti-
cally estimate the number of vulnerable websites, however,
the attack surface can be examined through an evaluation of
the number of websites using such vulnerable code patterns.
Listing 12: Code snippets { vulnerable code pat-
terns
// Native J a v a S c r i p t / DOM code
a . in n e r H T M L = b . i n n e r HT M L ;
a . in n e r H T M L += ’ a d d i t i o n a l content ’;
a . i n s e r t A d j a c e n t H T M L ( ’ beforebegin ’ , b .
i n n er H T M L ) ;
document . write ( a . i n n e r H TM L ) ;
// Library code
$ ( element ) . html ( ’ a d d i t i o n a l content ’) ;
4.1 InnerHTML Usage
Since an automated search for innerHTML does not de-
termine the exploitability of its usage, it can only serve as
an indication for the severity of the problem. To evaluate
the prevalence of innerHTML usage on the web, we con-
ducted a study of the Alexa top 10,000 most popular web
sites. A large fraction of approximately one third of these
web sites utilized vulnerable code patterns, like the ones in
Listing 12, in their code for updating page content. Major
websites like Google, Amazon, EBay and Microsoft could
be identi(cid:12)ed among these. Again, this does not suggest that
these web sites can be exploited. We found an overall of
74.5% of the Alexa Top 1000 websites to be using inner-
HTML-assignments. While the usage of innerHTML is very
common, the circumstances under which it is vulnerable to
exploitation are in fact hard to quantify. Note though that
almost all applications applied with an editable HTML area
are prone to being vulnerable.
Additionally, there are some notable examples of poten-
tially vulnerable code patterns identi(cid:12)able in multiple and
commonly used JavaScript libraries, e.g.
jQuery [7] and
SWFObject [27]. Indeed, more than 65% of the top 10,000
most popular websites do employ one of these popular li-
braries (with 48,87% using jQuery), the code of which could
be used to trigger actual attacks. Further studies have to be
made as to whether or not web applications reliant on any
of these libraries are a(cid:11)ected, as it largely depends on how
the libraries are used. In certain cases, a very speci(cid:12)c set
of actions needs to be performed if the vulnerable section
of the code is to be reached. Regardless, library’s inclusion
always puts a given website at risk of attacks.
Ultimately, we queried the Google Code Search Engine
(GCSE) as well as the Github search tool to determine which
libraries and public source (cid:12)les make use of potentially dan-
gerous code patterns. The search query yielded an over-
all 184,000 positive samples using the GCSE and 1,196,000
positive samples using the Github search tool. While this
does not provide us with an absolute number of vulnerable
websites, it shows how widely the usage of innerHTML is
distributed; any of these libraries using vulnerable code pat-
terns in combination with user-generated content is likely to
be vulnerable to mXSS attacks.
4.2 Web-Mailers
A class of web applications particularly vulnerable to m-
XSS attacks are classic web-mailers { applications that fa-
cilitates receiving, reading and managing HTML mails in a
browser.
In this example, the fact that HTML Rich-Text
Editors (RTE) are usually involved, forms the basis for the
use of the innerHTML property, which is being triggered
with almost any interaction with the mail content. This
includes composing, replying, spell-checking and other com-
mon features of applications of this kind. A special case of
attack vector is sending an mXSS string within the body
of an HTML-formatted mail. We analyzed commonly used
web-mail applications and spotted mXSS vulnerabilities in
almost every single one of them, including e.g. Microsoft
Hotmail, Yahoo! Mail, Redi(cid:11) Mail, OpenExchange, Round-
cube, and many other products { some of which cannot yet
be named for the sake of user protection. The discovery
was quickly followed with bug reports sent to the respective
vendors, which were acknowledged.
4.3 Server-Side XSS Filters
The class of mXSS attacks poses a major challenge for
server-side XSS (cid:12)lters. To completely mitigate these at-
tacks, they would have to simulate the mutation e(cid:11)ects of
the three major browser families in hopes of determining
whether a given string may be an mXSS vector. At the same
time, they should not (cid:12)lter benign content, in order not to
break the web application. The (cid:12)xes applied to HTML san-
itizers, as mentioned in the introduction, are new rules for
known mutation e(cid:11)ects. It can be seen as a challenging task
to develop new (cid:12)ltering paradigms that may discover even
unknown attack vectors.
5. MITIGATION TECHNIQUES
The following sections will describe a set of mitigation
techniques that can be applied by website owners, devel-
opers, or even users to protect against the cause and im-
pact of mutation XSS attacks. We provide details on two
approaches. The (cid:12)rst one is based on a server-side (cid:12)lter,
whereas the other focuses on client-side protection and em-
ploys an interception method in critical DOM properties ac-
cess management.
5.1 Server-side mitigation
Avoiding outputting server content otherwise incorrectly
converted by the browsers is the most direct mitigation strat-
egy.
In speci(cid:12)c terms, the (cid:13)awed content should be re-
placed with semantically equivalent content which is con-
verted properly. Let us underline that the belief stating
that \well-formed HTML is unambiguous" is false: only a
browser-dependent subset of well-formed HTML will be pre-
served across innerHTML-access and -transactions.
A comprehensible and uncomplicated policy is to simply
disallow any of the special characters for which browsers
are known to have trouble with when it comes to a proper
conversion. For many HTML attributes and CSS proper-
ties this is not a problem, since their set of allowed values
already excludes these particular special characters. Unfor-
tunately, in case of free-form content, such a policy may be
too stringent. For HTML attributes, we can easily re(cid:12)ne
our directive by observing that ambiguity only occurs when
the browser omits quotes from its serialized representation.
Insertion of quotes can be guaranteed by, for example, ap-
pending a trailing whitespace to text, a change unlikely to
modify the semantics of the original text. Indeed, the W3C
speci(cid:12)cation states that user agents may ignore surrounding
whitespace in attributes. A more aggressive transformation
would only insert a space when the attribute was to be seri-
alized without quotes, yet contained a backtick. It should be
noted that backtick remains the only character which causes
Internet Explorer to mis-parse the resulting HTML.
For CSS, re(cid:12)ning our policy is more di(cid:14)cult. Due to the
improper conversion of escape sequences, we cannot allow
any CSS special characters in general, even in their escaped
form. For URLs in particular, parentheses and single quotes
are valid characters in a URL, but are simultaneously con-
sidered special characters in CSS. Fortunately, most major
web servers are ready to accept percent encoded versions of
these characters as equivalent, so it is su(cid:14)cient to utilize
the common percent-escaping for these characters in URLs
instead.
We have implemented these mitigation strategies in HTML
Puri(cid:12)er, a popular HTML (cid:12)ltering library [32]; as HTML
Puri(cid:12)er does not implement any anomaly detection, the (cid:12)l-
ter was fully vulnerable to these attacks. These (cid:12)xes were
reminiscent of similar security bugs that were tackled in
2010 [31] and subsequent releases in 2011 and 2012. In that
case, the set of unambiguous encodings was smaller than
that suggested by the speci(cid:12)cation, so a very delicate (cid:12)x
had to be crafted in result, both (cid:12)xing the bug and still
allowing the same level of expressiveness. Since browser
behavior varies to a great degree, a server-side mitigation
of this style is solely practical for the handling of a subset
of HTML, which would normally be allowed for high-risk
user-submitted content. Furthermore, this strategy cannot
protect against dynamically generated content, a limitation
which will be addressed in the next section. Note that prob-
lems such as the backtick-mutation still a(cid:11)ect the HTML
Puri(cid:12)er as well as Blueprint and Google Caja; they have
only just been addressed successfully by the OWASP Java
HTML Sanitizer Project 4.
5.2 Client-side mitigation
Browsers implementing ECMA Script 5 and higher of-
fer an interface for another client-side (cid:12)x. The approach
makes use of the developer-granted possibility to overwrite
the handlers of innerHTML and outerHTML-access to in-
tercept the performance optimization and, consequently, the
markup mutation process as well. Instead of permitting a
browser to employ its own proprietary HTML optimization
routines, we utilize the internal XML processor a browser
provides via DOM. The technique describing the wrapping
and sanitation process has been labeled TrueHTML.
The TrueHTML relies on the XMLSerializer DOM object
provided by all of the user agents tested. The XMLSerial-
izer can be used to perform several operations on XML doc-
uments and strings. What is interesting for our speci(cid:12)c case
is that XMLSerializer.serializeToString() will accept an
arbitrary DOM structure or node collection and transform
it into an XML string. We decided to replace the inner-
HTML-getters with an interceptor to process the accessed
contents as if they were actual XML. This has the following
bene(cid:12)ts:
1. The resulting string output is free from all mutations
described and documented in Section 3. The attack
surface can therefore be mitigated by a simple replace-
ment of the browsers’ innerHTML-access logic with
our own code. The code has been made available to a
selected group of security researches in the (cid:12)eld, who
have been tasked with ensuring its robustness and re-
liability.
2. The XMLSerializer object is a browser component.
Therefore, the performance impact is low compared
to other methods of pre-processing or (cid:12)ltering inner-
HTML-data before or after mutations take place. We
elaborate on the speci(cid:12)cs of the performance impact
in the 6 Section.
3. The solution is transparent and does not require ad-
ditional developer e(cid:11)ort, coming down to a single Java-
Script implementation. No existing JavaScript or DOM
code needs to be modi(cid:12)ed, the script hooks silently into
the necessary property accessors and replaces the in-
secure browser code. At present, the script works on
all modern browsers tested (Internet Explorer, Firefox,
Opera and Chrome) and can be extended to work on
Internet Explorer 6 or earlier versions.
4. The XMLSerializer object post-validates potentially
invalid code and thereby provides yet another level of
sanitation. That means that even insecure or non-well-
formed user-input can be (cid:12)ltered and kept free from
mutation XSS and similar attack vectors.
4OWASP Wiki,
OWASP_Java_HTML_Sanitizer_Project, Feb. 2013
https://www.owasp.org/index.php/
5. The TrueHTML approach is generic, transparent and
website-agnostic. This means that a user can utilize
this script as a core for a protective browser exten-
sion, or apply the user-script to globally protect herself
against cause and impact of mutation XSS attacks.
6. EVALUATION
This section is dedicated to description of settings and
dataset used for evaluating the performance penalty intro-
duced by TrueHTML. We focus on assessing the client-side
mitigation approach. While HTMLPuri(cid:12)er has been changed
to re(cid:13)ect determination for mitigating this class of attacks,
the new features are limited to adding items on the inter-
nal list of disallowed character combinations. This does not
measurably increase the overhead introduced by HTMLPu-
ri(cid:12)er. Performance takes a central stage as a focus of our
query, as the transfer overhead introduced by TrueHTML is
exceptionally low. The http archive 5 has analysed a set of
more than 290,000 URLs and over the course of this project
it has been determined that the average transfer size of a
single web page is more than 1,200 kilobyte, 52kB of which
are taken up by HTML content and 214kB by JavaScript.
The prototype of TrueHTML is implemented in only 820
byte of code, which we consider to be a negligible transfer
overhead.
6.1 Evaluation Environment
To assess the overhead introduced by TrueHTML in a re-
alistic scenario, we conducted an evaluation based on the
Alexa top 10,000 most popular web sites. We crawled these
sites with a recursion depth of one. As pointed out in Sec-
tion 4, approximately one third of these sites make use of
innerHTML. In a next step we determine the performance
impact of TrueHTML in a web browser by accessing 5,000
URLs randomly chosen from this set. Additionally, we assess
the performance of TrueHTML in typical useage scenarios,
like displaying an e-mail in a web mailer or accessing pop-
ular websites, as well as, investigate the relation between
page load time overhead and page size in a controlled envi-
ronment.
To demonstrate the versatility of the client-side mitiga-
tion approach, we used di(cid:11)erent hardware platforms for the
di(cid:11)erent parts of the evaluation. The Alexa tra(cid:14)c ranking
data on virtual machines constituted the grounds for per-
forming this evaluation. Each instance was assigned one
core of an Intel Xeon X5650 CPU running at 2.67GHz and
had access to 2 GB RAM. The instances ran Ubuntu 12.04
Desktop and Mozilla Firefox 14.0.1. As an example for a
mid-range system, we used a laptop with an Intel Core2Duo
CPU at 1.86GHz and 2GB RAM, running Ubuntu 12.04
Desktop and Mozilla Firefox 16.0.2, so that to assess the
performance in typical usage scenarios.
The evaluation environment is completed by a proxy server
to inject TrueHTML into the HTML context of the vis-
ited pages, and a logging infrastructure.Once a website has
been successfully loaded in the browser, we log the URL
and the user-perceived page loading time using the Navi-
gation Timing API de(cid:12)ned by the W3C Web Performance
Working Group [29]. We measure this time as the di(cid:11)er-
ence between the time when the onload event is (cid:12)red and
the time immediately after the user agent (cid:12)nishes prompt-
5http://www.httparchive.org/, Nov. 2012
ing to unload the previous document, as provided by the
performance.timing.navigationStart method.
6.2 Evaluation Results
Using the virtual machines we (cid:12)rst determine the user-
perceived page loading time of the unaltered pages.
In a
second run we use the proxy server to inject TrueHTML
and measure the page loading time again. We calculate the
overhead as the increase of page loading time in percentage
ratios of the loading time the page needed without True-
HTML. The minimum overhead introduced by TrueHTML
is 0.01% while the maximum is 99.94%. On average, True-
HTML introduces an overhead of 30.62%. The median result