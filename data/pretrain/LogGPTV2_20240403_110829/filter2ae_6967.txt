# 【技术分享】爬虫-化被动为主动
|
##### 译文声明
本文是翻译文章，文章来源：安全客
译文仅供参考，具体内容表达以及含义原文为准。
****
作者：[ **二三五**](http://bobao.360.cn/member/contribute?uid=2659167669)
投稿方式：发送邮件至linwei#360.cn，或登陆网页版在线投稿
**  
**
**前言**
现在网上有很多被动式扫描器，配置一个代理给浏览器设置，然后人去点击浏览器上的网页，在这种模式下抓到的URL数量没有用爬虫的效果好。
我个人是比较懒的，先也写了个被动的扫描器，不想改以前写的东西，而且被动也有被动的优点，所以就想可不可以让爬虫也设置个代理。就有了下面的东西，很方便。
**  
**
**实操**
如何在不改变原被动扫描器的情况下让被动变成主动。
**主结构** ：
以phantomjs为核心，用JS模仿人对页面的操作，代理软件抓链接。以下流程是通用pychon脚本实现的。
1打开浏览器并设置代理->2输入网址->3填充表单->4点击按钮->5点击超链拉->6关闭标签->7关闭浏览器，循环2-6。
打开浏览器并设置代理
    proxy_config = [
        '--proxy=127.0.0.1:8080',
        '--proxy-type=http',
        '--ignore-ssl-errors=yes',
    ]
    phantomjs_path='/home/ubuntu_235/proxyscan/phantomjs/phantomjs/bin/phantomjs'
    driver = webdriver.PhantomJS(executable_path=phantomjs_path,service_args=sys_config)
输入网址  
    driver.get('http://demo.aisec.cn')
填充表单
    _input_text = """
            var input_list=document.getElementsByTagName("input");
            for (i_i=0;i_i标签的href？
    url_list=[]
    a_list=self.driver.find_elements_by_xpath("//a[@href]")
    for a in a_list:
    url_list.append(a.get_attribute("href"))
    print(url_list)
如何获取当前标签的URL？
    driver.current_url
有什么坑？
a.浏览器打页面要时间，但python不知道(不是真不知道)
b.会不明原因的卡死，没有报错
c.页面跳转