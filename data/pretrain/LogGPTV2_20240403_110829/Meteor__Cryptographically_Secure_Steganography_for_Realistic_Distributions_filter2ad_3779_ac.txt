â€¢ DecodeD(ğ‘˜, ğ‘,H) is a (possibly probabilistic) algorithm that
takes as input a key ğ‘˜ and a stegotext message ğ‘ and an op-
tional ordered set of covertext messages H. Decode returns a
plaintext message ğ‘š on success or the empty string ğœ€ on failure.
We use the history notation that is used in a number of previous
works [28, 29], but not universally adopted. The history input to
the encode and decode functions capture the notion that covertext
channels may be stateful. For instance, members of the ordered
set H could be text messages previously exchanged between two
parties or the opening messages of a TCP handshake.
Correctness. A steganographic protocol must be correct, i.e. except
with negligible probability an encoded message can be recovered
using the decode algorithm. Formally, for any ğ‘˜ â† KeyGenD(1ğœ†),
Pr [ DecodeD(ğ‘˜, EncodeD(ğ‘˜, ğ‘š,H),H) = ğ‘š ] â‰¥ 1 âˆ’ negl(ğœ†).
Security. We adopt a symmetric-key analog of the security defini-
tions for a steganographic system secure against a chosen hidden-
text attacks in [29], similar to the real-or-random games used in
other cryptographic notions. Intuitively, a steganographic protocol
Î£D is secure if all ppt. adversaries are unable to distinguish with
non-negligible advantage if they have access to encoding oracle
EncodeD(ğ‘˜, Â·, Â·) or a random sampling oracle ğ‘‚D(Â·, Â·) that returns
a sample of the appropriate length. This ensures that an adver-
sary wishing to block encoded messages will be forced to block
innocuous messages as well. We allow the adversary to not only
have a sampling oracle to the distribution (as in [28]), but also have
the same distribution description given to the encoding algorithm.
More formally, we write,
Definition 1. We say that a steganographic scheme Î£D is secure
against chosen hiddentext attacks if for all ppt. adversaries AD,
Session 5C: Messaging and Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1533ğ‘˜ â† KeyGenD(1ğœ†),
(cid:12)(cid:12)(cid:12)Pr
(cid:104) AEncodeD (ğ‘˜,Â·,Â·))
D
(cid:105)âˆ’ Pr
(cid:104) Ağ‘‚D (Â·,Â·)
D
= 1
(cid:105)(cid:12)(cid:12)(cid:12)  0 entropy, this can
be done by sampling some fixed number â„“ elements together, such
that the resulting channel has at least â„“ Ã— ğœ– entropy. By setting â„“
appropriately, the entropy in the compiled channel is guaranteed
to be high enough. However, in real communications channels, the
entropy in the channel may not always be non-zero. As such, a
naÃ¯ve application of this approach will fall short.
We overcome this by sampling a variable number of tokens
in each sampling event, such that the cumulative entropy of the
distributions from which the tokens come surpasses the minimum
requirement. More specifically, instead of sampling one token at a
time in the while loop of Algorithm 1, this method samples ğ‘ tokens
until the sum of the entropy of the distributions from which those
tokens were sampled meets a minimum threshold ğ»ğ‘. Intuitively,
this approach â€œcollectsâ€ entropy before attempting to encode into
it, boosting success rate while avoiding the issues of low entropy.
Figure 2c shows a selection probabilities graph, with different
values of ğ»ğ‘ compared against a baseline measurement of normal
sampling from the GPT-2 (note this baseline includes all sampled
tokens, unlike in Figure 2b). In the figure, each set of runs of the
model sets ğœ† = ğ‘˜, i.e., the entropy required to encode is equivalent
to the number of tries to encode. There are differences between the
probabilities, but here is no clear pattern â€“ this variation can be
attributed to sampling error. [54] proved that for this approach to
be secure, ğ»ğ‘ must be strictly larger than log(ğ‘˜); to achieve useful
security parameters, we need ğ»ğ‘ = ğ‘˜ â‰ˆ 2Ã—ğœ†, where ğœ† is the security
parameter.
While provably secure, variable length sampling results in un-
reasonably large stegotext and long encoding times. Table 1 shows
the length of stegotext and encoding times when encoding a 16
byte plaintext message using adaptation 2 on our Desktop/GPU
test environment using the GPT-2 model (refer to Section 6 for
hardware details). Each row corresponds to 30 runs of the model for
that set of parameters. As ğ»ğ‘ (and thereby ğ‘˜) increase, the length
of the stegotext also increases: the higher resampling entropy re-
quirement means that more tokens must be sampled, which takes
more time. We note that these results include GPU acceleration, so
there is little room for performance boosts from hardware.
5 METEOR: A MORE EFFICIENT
SYMMETRIC-KEY STEGANOGRAPHIC
SCHEME
We now design a symmetric-key steganographic scheme that is
more practical than the techniques above. A more efficient symmetric-
key approach would allow for hybrid steganography, in which a
sender encodes a symmetric key using the public-key steganog-
raphy and then switches to a faster and more efficient encoding
scheme using this symmetric key. We note that while symmetric-
key approaches have been considered in the past, e.g. [28, 68], they
also rely on the entropy gathering techniques highlighted above.
Our approachâ€™s intuition to accommodate high entropy variability is
to fluidly change the encoding rate with the instantaneous entropy
in the channel. As will become clear, Meteor does this implicitly,
by having the expected number of bits encoded be proportional to
the entropy.
0510152025TokenPosition0246810EntropyEntropyovertimefor4samples0.00.10.20.30.40.5ProbabilityofSelection01020304050607080RelativeFrequency(%)SelectionProbabilities,16-bytemessage:EntropyBound=4.5Baseline(bounded)=16=32=64=1280.00.20.40.60.81.0ProbabilityofSelection01020304050RelativeFrequency(%)SelectionProbabilities,16-bytemessage:ResamplingBaseline==16==32==64==128Session 5C: Messaging and Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea15355.1 Intuition
Suppose we have, for example, a generative model M trained to
output English text word-by-word. Each iteration takes as input
all previously generated words H and outputs a probability dis-
tribution P for the next word, defined over all known words T .
This is done by partitioning the probability space between 0 and
1 (represented at some fixed precision) into continuous intervals