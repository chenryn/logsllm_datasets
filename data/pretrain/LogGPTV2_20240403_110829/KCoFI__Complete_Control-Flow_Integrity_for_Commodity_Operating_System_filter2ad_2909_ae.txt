indirect branch instructions instrumented by
KCoFI can direct control-ﬂow to the same set of addresses,
Equation 1 can be simpliﬁed into Equation 2 (with |T| being
the number of valid targets for each indirect branch):
We measured the AIR metric for the KCoFI native code
generated for the FreeBSD kernel. Our compiler identiﬁed
106,215 valid native code targets for indirect control ﬂow
transfers (|T|) out of 5,838,904 possible targets in the kernel’s
code segment (S) before instrumentation. The native code
generated by KCoFI contains 21,635 indirect control ﬂow
transfers (n). The average reduction of targets (AIR metric) for
303
1
n
1 − |Tj|
S
n(cid:2)
j=1
1 − |T|
S
(1)
(2)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:09 UTC from IEEE Xplore.  Restrictions apply. 
these transfers is therefore 98.18%, implying that nearly all the
possible indirect control transfer targets have been eliminated
as feasible targets by KCoFI.
As a point of comparison, our AIR result is nearly as
good as the average AIR metrics for several different CFI
variants reported for the SPEC CPU 2006 benchmarks and the
namd benchmark (which range between 96% to 99.1%) [10].
Since these numbers are obtained for very different workloads
– SPEC and the FreeBSD kernel – the comparison is only
intended to show that
the
differences in the exact numbers are not meaningful.
the results are roughly similar;
B. ROP Gadgets
To measure the impact on return-oriented-programming
opportunities more speciﬁcally, we used the open-source ROP-
Gadget
tool [14] version 4.0.4 to automatically ﬁnd ROP
gadgets in both the original FreeBSD kernel compiled with
GCC and our identically conﬁgured KCoFI FreeBSD kernel.
We ran the tool on both the kernel and drivers using the default
command-line options.
ROPGadget found 48 gadgets in the original FreeBSD ker-
nel and 21 gadgets in the KCoFI FreeBSD kernel. We manually
analyzed the 21 gadgets found in the KCoFI FreeBSD kernel.
None of the gadgets follow a valid control-ﬂow integrity label.
Therefore, none of these gadgets can be “jumped to” via an
indirect control transfer in an ROP attack.
VIII. PERFORMANCE EVALUATION
We evaluated the performance impact of KCoFI on a Dell
Precision T1650 workstation with an Intel R(cid:10) CoreTM i7-3770
processor at 3.4 GHz with 8 MB of cache, 16 GB of RAM,
an integrated PCIE Gigabit Ethernet card, a 7200 RPM 6 Gb/s
SATA hard drive, and a Solid State Drive (SSD) used for the
/usr partition. For experiments requiring a network client,
we used an iMac with a 4-core hyper-threaded Intel R(cid:10) CoreTM
i7 processor at 2.6 GHz with 8 GB of RAM. Our network
experiments used a dedicated Gigabit ethernet network.
Since network applications make heavy use of operating
system services, we measured the performance of the thttpd
web server and the remote secure login sshd server. These
experiments also allow us to compare the performance of
KCoFI to the original SVA system [5] which enforces more
sophisticated memory safety that implies control-ﬂow integrity.
To measure ﬁle system performance, we used the Postmark
benchmark [27]. We used the LMBench microbenchmarks [28]
to measure the performance of individual system calls.
For each experiment, we booted the Dell machine into
single-user mode to avoid having other system processes
affect the performance of the system. Our baseline is a native
FreeBSD kernel compiled with the LLVM 3.1 compiler, with
the same compiler options and the same kernel options as the
KCoFI FreeBSD kernel.
A. Web Server Performance
We used a statically linked version of the thttpd web
server [29] to measure how much the KCoFI run-time checks
reduce the server’s bandwidth. To measure bandwidth, we used
ApacheBench [30].
For the experiments, we transferred ﬁles between 1 KB
and 2 MB in size. Using larger ﬁle sizes is not useful because
the network saturates at about 512KB ﬁle sizes. This range
of sizes also subsumes the range used in the original SVA
experiments [5]. We generated each ﬁle by collecting random
data from the /dev/random device; this ensures that the ﬁle
system cannot optimize away disk reads due to the ﬁle having
blocks containing all zeros. We conﬁgured each ApacheBench
client to make 32 simultaneous connections and to perform
10,000 requests for the ﬁle; we ran four such ApacheBench
processes in parallel for each run of the experiment to simulate
multiple clients. We ran each experiment 20 times.
Figure 6 shows the mean performance of transferring a ﬁle
of each size. The average bandwidth reduction across all ﬁle
sizes is essentially zero. This is far better performance than the
SVA system which incurs about a 25% reduction in bandwidth
due to its memory safety checks [5].
Native
KCoFI
 30000
 25000
 20000
 15000
 10000
 5000
)
s
/
B
K
(
h
t
d
i
w
d
n
a
B
 0
1
2
4
8
16
32
File Size (KB)
64
128
256
512
1024
2048
Fig. 6. ApacheBench Average Bandwidth with Standard Deviation Bars
B. Secure Shell Server Performance
In addition to a web server, we also measured the band-
width of transferring ﬁles using the OpenSSH Secure Shell
server [31]. We ran the OpenSSH server on our test machine
and used the Mac OS X OpenSSH scp client (based on
OpenSSH 5.2p1) to measure the number of bytes received per
second when transferring the ﬁle. We repeated each experiment
20 times.
Figure 7 plots the mean bandwidth for the baseline system
and KCoFI with standard deviation error bars (the standard
deviations are too small to be discernible in the diagram).
On average, the bandwidth reduction was 13% with a worst
case reduction of 27%. Transferring ﬁles between 1 KB and 8
KB showed the most overhead at 27%. Transferring ﬁles that
are 1 MB or smaller showed an average overhead of 23%;
the average is 2% for ﬁles of larger size, indicating that the
network becomes the bottleneck for larger ﬁle transfers.
The original SVA system only measured SSH bandwidth
for ﬁles that were 8 MB or larger [5]; this is beyond the point
at which the network hardware becomes the bottleneck. This
comparison, therefore, is inconclusive: it does not show any
difference between the two systems, but it does not include
cases where overheads might be expected.
304
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:09 UTC from IEEE Xplore.  Restrictions apply. 
Native
KCoFI
)
s
/
B
K
(
h
t
d
i
w
d
n
a
B
 90000
 80000
 70000
 60000
 50000
 40000
 30000
 20000
 10000
 0
 1
 32
 1024
File Size (KB)
We also compared these results to similar benchmarks
from the full memory-safety version of SVA [5], shown in
the last column of Table VI. The missing numbers for SVA
are because some kernel operations were not tested in the
SVA experiments. On these microbenchmarks, KCoFI clearly
performs much better than SVA, as much as 5x in some cases.
Again, the SVA experiments used a different kernel and so
it is not meaningful to compare the detailed differences in
the numbers, but the magnitudes of the differences clearly
highlight the performance beneﬁt of using CFI instead of full
memory safety.
 32768
 1.04858e+06
D. Postmark Performance
Fig. 7. SSHD Average Transfer Rate with Standard Deviation Bars
C. Microbenchmarks
In order to understand how our system affects the per-
formance of core operating system services, we used LM-
Bench [28] to measure the latency of various system calls.
(We present these before discussing Postmark because the
latter is largely explained by the LMBench measurements.)
Some test programs can be conﬁgured to run the test for
a speciﬁed number of iterations; those were conﬁgured to
use 1,000 iterations. We ran each benchmark 10 times. We
conﬁgured ﬁle I/O benchmarks to use ﬁles on the SSD. This
ensures that we’re measuring the highest relative latency that
KCoFI can add by using the fastest disk hardware available.
TABLE VI.
LMBENCH RESULTS. TIME IN MICROSECONDS.
Test
null syscall
open/close
mmap
page fault
signal handler
install
signal handler
delivery
fork + exit
fork + exec
select
pipe latency
Native KCoFI Overhead
2.50x
0.091
2.47x
2.01
7.11
3.30x
1.11x
31.6
0.168
2.13x
0.22
4.96
23.4
35.2
0.36
SVA Overhead [5]
2.31x
11.0x
-
-
5.74x
1.27
62.9
101
3.05
1.94
1.17
222
318
4.76
4.01
0.92x
3.50x
3.10x
1.60x
2.10x
5.34x
-
-
8.81x
13.10x
TABLE VII.
LMBENCH: FILES CREATIONS PER SECOND
File Size
0 KB
1 KB
4 KB
10 KB
Native KCoFI Overhead
2.28x
155771
2.47x
97943
97192
2.48x
2.38x
85600
68415
39615
39135
35982
As Tables VI and VII show, our system can add consider-
able overhead to individual operations. Most of the operations
we tested showed overhead between 2x and 3.5x. The KCoFI
ﬁle creation overheads are uniformly about 2.3-2.5x for all
ﬁle sizes tested by LMbench. Although these overheads are
fairly high, most applications only experience these overheads
during kernel CPU execution, which explains why the impact
on performance observed for thttpd and sshd is far lower.
To further examine ﬁle system performance, we ran Post-
mark [27] which mimics a mail server’s ﬁle system behavior.
TABLE VIII.
POSTMARK RESULTS
Native (s)
12.7
Native
StdDev
0.48
KCoFI (s) KCoFI
StdDev
0.40
24.8
Overhead
1.96x
We conﬁgured Postmark to use 500 base ﬁles with sizes
ranging from 500 bytes to 9.77 KB with 512 byte block sizes.