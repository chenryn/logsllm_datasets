### Indirect Branch Instructions and Control-Flow Integrity

Indirect branch instructions, when instrumented by KCoFI, can direct control flow to a specific set of addresses. As a result, Equation 1 can be simplified into Equation 2, where \( |T| \) represents the number of valid targets for each indirect branch.

**Equation 1:**
\[
\text{AIR} = \frac{1}{n} \sum_{j=1}^{n} \left( 1 - \frac{|T_j|}{S} \right)
\]

**Equation 2:**
\[
\text{AIR} = 1 - \frac{|T|}{S}
\]

We measured the Average Indirect Target Reduction (AIR) metric for the KCoFI-compiled native code of the FreeBSD kernel. Our compiler identified 106,215 valid native code targets for indirect control flow transfers (\( |T| \)) out of 5,838,904 possible targets in the kernel's code segment (\( S \)) before instrumentation. The native code generated by KCoFI contains 21,635 indirect control flow transfers (\( n \)). The average reduction of targets (AIR metric) for these transfers is 98.18%, indicating that nearly all possible indirect control transfer targets have been eliminated as feasible targets by KCoFI.

For comparison, our AIR result is nearly as good as the average AIR metrics for several different CFI variants reported for the SPEC CPU 2006 benchmarks and the NAMD benchmark, which range from 96% to 99.1% [10]. Despite the differences in workloads—SPEC and the FreeBSD kernel—the comparison shows that the results are roughly similar.

### ROP Gadgets

To measure the impact on return-oriented programming (ROP) opportunities, we used the open-source ROPGadget tool [14] version 4.0.4 to automatically find ROP gadgets in both the original FreeBSD kernel compiled with GCC and our identically configured KCoFI FreeBSD kernel. We ran the tool on both the kernel and drivers using the default command-line options.

ROPGadget found 48 gadgets in the original FreeBSD kernel and 21 gadgets in the KCoFI FreeBSD kernel. We manually analyzed the 21 gadgets found in the KCoFI FreeBSD kernel. None of these gadgets follow a valid control-flow integrity label, meaning none of them can be "jumped to" via an indirect control transfer in an ROP attack.

### Performance Evaluation

We evaluated the performance impact of KCoFI on a Dell Precision T1650 workstation with an Intel Core i7-3770 processor at 3.4 GHz, 8 MB of cache, 16 GB of RAM, an integrated PCIE Gigabit Ethernet card, a 7200 RPM 6 Gb/s SATA hard drive, and a Solid State Drive (SSD) used for the /usr partition. For experiments requiring a network client, we used an iMac with a 4-core hyper-threaded Intel Core i7 processor at 2.6 GHz with 8 GB of RAM. Our network experiments used a dedicated Gigabit Ethernet network.

#### Web Server Performance

We used a statically linked version of the thttpd web server [29] to measure how much the KCoFI run-time checks reduce the server’s bandwidth. To measure bandwidth, we used ApacheBench [30].

For the experiments, we transferred files ranging from 1 KB to 2 MB in size. Using larger file sizes was not useful because the network saturates at about 512 KB file sizes. This range also subsumes the range used in the original SVA experiments [5]. Each file was generated by collecting random data from the /dev/random device to ensure that the file system cannot optimize away disk reads due to the file having blocks containing all zeros. We configured each ApacheBench client to make 32 simultaneous connections and to perform 10,000 requests for the file; we ran four such ApacheBench processes in parallel for each run of the experiment to simulate multiple clients. We ran each experiment 20 times.

Figure 6 shows the mean performance of transferring a file of each size. The average bandwidth reduction across all file sizes is essentially zero. This is far better performance than the SVA system, which incurs about a 25% reduction in bandwidth due to its memory safety checks [5].

**Figure 6: ApacheBench Average Bandwidth with Standard Deviation Bars**

#### Secure Shell Server Performance

In addition to a web server, we measured the bandwidth of transferring files using the OpenSSH Secure Shell server [31]. We ran the OpenSSH server on our test machine and used the Mac OS X OpenSSH scp client (based on OpenSSH 5.2p1) to measure the number of bytes received per second when transferring the file. We repeated each experiment 20 times.

Figure 7 plots the mean bandwidth for the baseline system and KCoFI with standard deviation error bars (the standard deviations are too small to be discernible in the diagram). On average, the bandwidth reduction was 13% with a worst-case reduction of 27%. Transferring files between 1 KB and 8 KB showed the most overhead at 27%. Transferring files that are 1 MB or smaller showed an average overhead of 23%; the average is 2% for files of larger size, indicating that the network becomes the bottleneck for larger file transfers.

The original SVA system only measured SSH bandwidth for files that were 8 MB or larger [5], which is beyond the point at which the network hardware becomes the bottleneck. This comparison, therefore, is inconclusive: it does not show any difference between the two systems but does not include cases where overheads might be expected.

**Figure 7: SSHD Average Transfer Rate with Standard Deviation Bars**

#### Microbenchmarks

To understand how our system affects the performance of core operating system services, we used LMBench [28] to measure the latency of various system calls. Some test programs can be configured to run the test for a specified number of iterations; those were configured to use 1,000 iterations. We ran each benchmark 10 times. We configured file I/O benchmarks to use files on the SSD to ensure we are measuring the highest relative latency that KCoFI can add by using the fastest disk hardware available.

**Table VI: LMBench Results. Time in microseconds.**

| Test                   | Native | KCoFI | Overhead | SVA Overhead [5] |
|------------------------|--------|-------|----------|------------------|
| null syscall           | 0.091  | 0.22  | 2.47x    | 2.31x            |
| open/close             | 2.01   | 4.96  | 2.47x    | 11.0x            |
| mmap                   | 7.11   | 23.4  | 3.30x    | -                |
| page fault             | 1.11   | 35.2  | 31.6     | -                |
| signal handler install | 31.6   | 0.168 | 0.168    | 5.74x            |
| signal handler delivery| 2.13   | 46.4  | 21.8     | 1.27             |
| fork + exit            | 0.22   | 0.36  | 1.64x    | 62.9             |
| fork + exec            | 23.4   | 318   | 13.6x    | 101              |
| select                 | 35.2   | 4.76  | 0.135    | 4.01             |
| pipe latency           | 0.36   | 0.92  | 2.56x    | 3.50x            |

**Table VII: LMBench: Files Creations Per Second**

| File Size | Native | KCoFI | Overhead |
|-----------|--------|-------|----------|
| 0 KB      | 155771 | 63160 | 2.47x    |
| 1 KB      | 97943  | 39615 | 2.47x    |
| 4 KB      | 85600  | 35982 | 2.38x    |
| 10 KB     | 68415  | 27525 | 2.48x    |

As Tables VI and VII show, our system can add considerable overhead to individual operations. Most of the operations we tested showed overhead between 2x and 3.5x. The KCoFI file creation overheads are uniformly about 2.3-2.5x for all file sizes tested by LMBench. Although these overheads are fairly high, most applications only experience these overheads during kernel CPU execution, which explains why the impact on performance observed for thttpd and sshd is far lower.

#### Postmark Performance

To further examine file system performance, we ran Postmark [27], which mimics a mail server’s file system behavior. We configured Postmark to use 500 base files with sizes ranging from 500 bytes to 9.77 KB with 512-byte block sizes.

**Table VIII: Postmark Results**

| Metric        | Native | KCoFI | Overhead |
|---------------|--------|-------|----------|
| Time (s)      | 12.7   | 24.8  | 1.96x    |
| StdDev        | 0.48   | 0.40  |          |

The results indicate that KCoFI adds a 1.96x overhead to the Postmark benchmark.