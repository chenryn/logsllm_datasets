title:Preech: A System for Privacy-Preserving Speech Transcription
author:Shimaa Ahmed and
Amrita Roy Chowdhury and
Kassem Fawaz and
Parmesh Ramanathan
Preech: A System for Privacy-Preserving 
Speech Transcription
Shimaa Ahmed, Amrita Roy Chowdhury, and Kassem Fawaz, and 
Parmesh Ramanathan, University of Wisconsin—Madison
https://www.usenix.org/conference/usenixsecurity20/presentation/ahmed-shimaa
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Prεεch: A System for Privacy-Preserving Speech Transcription
Shimaa Ahmed, Amrita Roy Chowdhury, Kassem Fawaz, and Parmesh Ramanathan
{ahmed27, roychowdhur2, kfawaz, parmesh.ramanathan}@wisc.edu
University of Wisconsin-Madison
Abstract
New advances in machine learning have made Automated
Speech Recognition (ASR) systems practical and more scal-
able. These systems, however, pose serious privacy threats
as speech is a rich source of sensitive acoustic and textual in-
formation. Although ofﬂine and open-source ASR eliminates
the privacy risks, its transcription performance is inferior to
that of cloud-based ASR systems, especially for real-world
use cases. In this paper, we propose Prεεch, an end-to-end
speech transcription system which lies at an intermediate
point in the privacy-utility spectrum. It protects the acoustic
features of the speakers’ voices and protects the privacy of
the textual content at an improved performance relative to
ofﬂine ASR. Additionally, Prεεch provides several control
knobs to allow customizable utility-usability-privacy trade-
off. It relies on cloud-based services to transcribe a speech
ﬁle after applying a series of privacy-preserving operations
on the user’s side. We perform a comprehensive evaluation of
Prεεch, using diverse real-world datasets, that demonstrates
its effectiveness. Prεεch provides transcription at a 2% to
32.25% (mean 17.34%) relative improvement in word error
rate over Deep Speech, while fully obfuscating the speakers’
voice biometrics and allowing only a differentially private
view of the textual content.
1 Introduction
New advances in machine learning and the abundance of
speech data have made Automated Speech Recognition (ASR)
systems practical and reliable [5, 17]. ASR systems have
achieved a near-human performance on standard datasets [5,
17], at a scale. This scalability is desirable in many domains,
such as journalism [25], law, business, education, and health
care, where cost, delay, and third-party legal implications [29]
prohibit the application of manual transcription services [12].
For example, recent research has identiﬁed private voice tran-
scription as one of the challenges journalists face when inter-
viewing sensitive sources [25].
Several companies, such as Google and Amazon, provide
online APIs for speech transcription. This convenience, how-
ever, comes at the cost of privacy. A speech recording contains
acoustic features that can reveal sensitive information about
the user, such as age, gender [39], emotion [4, 40], accent,
and health conditions [41]. The acoustic features are also
biometric identiﬁers of the speakers [26], enabling speaker
identiﬁcation and impersonation [20]. Additionally, the tex-
tual content of speech can be sensitive [29]. For example,
medical recordings can contain private health information
about patients [12], and business recordings can include pro-
prietary information. Current cloud services already support
several speech processing APIs like speaker identiﬁcation
and diarization. They also support text analysis APIs, such
as topic modeling, document categorization, sentiment analy-
sis, and entity detection (Sec. 3.2), that can extract sensitive
information from text. Applying these APIs to the recorded
speech can signiﬁcantly undermine the user’s privacy.
Ofﬂine and open-source transcription services, like Deep
Speech [18], solve these privacy challenges as the speech
ﬁles never leave the user’s trust boundary. However, we ﬁnd
that their performance does not match that of a cloud ser-
vice provider [45], especially on real-world conversations and
different accents (Sec. 2.2). Thus, the primary goal of this
paper is to: provide an intermediate solution along the utility-
privacy spectrum that uses cloud services while providing a
formal privacy guarantee.
We present Prεεch (Privacy-Preserving Speech) as a means
to achieve this goal; it is an end-to-end speech transcription
system that: (1) protects the users’ privacy along the acoustic
and textual dimensions; (2) improves the transcription per-
formance relative to ofﬂine ASR; and (3) provides the user
with control knobs to customize the trade-offs between utility,
usability, and privacy.
Textual Privacy: Prεεch segments and shufﬂes the input
speech ﬁle to break the context of the text, effectively trans-
forming it into a bag-of-words. Then, it injects dummy (noise)
segments to provide the formal privacy guarantee of differen-
tial privacy (DP) [13].
USENIX Association
29th USENIX Security Symposium    2703
Acoustic Privacy: Prεεch applies voice conversion to protect
the acoustic features of the input speech ﬁle and ensure noise
indistinguishability.
We evaluate Prεεch over a set of real-world datasets cover-
ing diverse demographics. Our evaluation shows that Prεεch
provides a superior transcription accuracy relative to Deep
Speech, the state-of-the-art ofﬂine ASR. Also, Prεεch pre-
vents cloud services from extracting any user-speciﬁc acous-
tic features from the speech. Finally, applying Prεεch thwarts
the learning of any statistical models or sensitive information
extraction from the text via natural language processing tools.
In summary, the main contributions of this paper are:
(1) End-to-end practical system: We propose Prεεch, a new
end-to-end system that provides privacy-preserving speech
transcription at an improved performance relative to ofﬂine
transcription. Speciﬁcally, Prεεch shows a relative improve-
ment of 2% to 32.52% (mean 17.34%) in word error rate
(WER) on real-world evaluation datasets over Deep Speech,
while fully obfuscating the speakers’ voice biometrics and
allowing only a DP view of the textual content.
(2) Non-standard use of differential privacy: Prεεch uses
DP in a non-standard way, giving rise to a set of new chal-
lenges. Speciﬁcally, the challenges are (1) “noise” corre-
sponds to concrete words, and need to be added in the speech
domain (2) “noise” has to be indistinguishable from the origi-
nal speech (details in Sec. 4.5).
(3) Customizable Design: Prεεch provides several control
knobs for users to customize the functionality based on their
desired levels of utility, usability, and privacy (Sec. 7.4). For
example, in a relaxed privacy setting, Prεεch’s relative im-
provement in WER ranges from 44% to 80% over Deep
Speech (Sec. 7.4.1).
The full version of this paper is available online [3], and
some demonstrations of Prεεch are available at this link [2].
2 Speech Transcription Services
We ﬁrst provide some background on online and ofﬂine
speech transcription services. Next, we present a utility evalu-
ation using standard and real-world speech datasets.
2.1 Background
Speech transcription refers to the process of extracting text
from a speech ﬁle. ASR systems are available to the users
either through cloud-based online APIs or ofﬂine software.
(1) Cloud-Based Transcription: We utilize two cloud-based
speech transcription services – Google’s Cloud Speech-to-
Text and Amazon Transcribe.
(2) Ofﬂine Transcription: We consider the Deep Speech ar-
chitecture from Baidu [18], which is trained using Mozilla’s 1
Common Voice dataset as a representative ofﬂine transcription
1https://voice.mozilla.org/en/datasets
service. This dataset is crowdsourced and open-source. Specif-
ically, we use the Deep Speech 0.4.1 model 2 (released in Jan-
uary 2019). Note that we do not consider ofﬂine transcribers
that are not open for general use. For example, Google’s on-
device speech recognizer [1] is an ofﬂine transcriber that is
currently only supported on Google’s Pixel devices and does
not allow an API or open-source access, limiting its usability.
or T OSP
Notations: Let S denote the input speech ﬁle associated with
a ground truth transcript T g
S . The user can either use a cloud
service provider (CSP) or an ofﬂine service provider (OSP) to
obtain the transcript (denoted by T CSP
, respectively).
Transcription Accuracy: The standard metric for quanti-
fying the accuracy loss from transcription is the word error
rate (WER) [18]. WER treats the transcript as a sequence of
words. It models the difference between the two sequences
by counting the number of deleted words (D), the number of
substituted words (U), and the number of injected words (I).
If the number of words in T g
S is W , WER is given as: D+U+I
W .
S
S
2.2 Utility Comparison
In this section, we empirically evaluate the utility gap between
the CSP and the OSP over a wide range of standard and real-
world datasets. We use these datasets throughout the paper.
Standard Datasets: These datasets include (1) the TIMIT-
TEST subset [16], (2) a subset from Librispeech dev-clean
dataset [31], and (3) the DAPS dataset [28]. TIMIT-TEST 3
subset comprises of 1344 utterances by 183 speakers from
eight major dialect regions of the United States. The Lib-
riSpeech subset consists of eleven speakers, 20 utterances
each. For DAPS, we use the evaluation subset prepared for
the 2018 voice conversion challenge [24] that consists of ﬁve
scripts read by ten speakers: ﬁve males and ﬁve females.
Real-world Datasets: We also assess the real-world perfor-
mance of both transcription services on non-American accent
datasets and real conversations among speakers of different
demographics. For the accented datasets, we evaluate 200 ut-
terances of two speakers from the VCTK dataset [46]: speaker
p262 of a Scottish accent and speaker p266 of an Irish accent.
For the real-world datasets, we evaluate 20 minutes of speech
from the "Facebook, Social Media Privacy, and the Use and
Abuse of Data" hearing before the U.S. Senate 4. We con-
struct the 20 minutes by selecting three continuous chunks of
speech from the hearing such that they include nine speakers:
8 senators and Mark Zuckerberg. Another real-world dataset
is the Supreme Court of the United States case "Carpenter v.
United States" 5. For this dataset, we evaluate a total of 40
minutes of speech from the advocates in the case.
2https://github.com/mozilla/DeepSpeech
3https://catalog.ldc.upenn.edu/LDC93S1
4https://www.commerce.senate.gov/2018/4/facebook-social-media-
privacy-and-the-use-and-abuse-of-data
5https://www.oyez.org/cases/2017/16-402
2704    29th USENIX Security Symposium
USENIX Association
Datasets
Google AWS Deep Speech
r
a
d
n
a
t
S
d
r
a
d
n
a
t
S
-
n
o
N
d LibriSpeech
DAPS
TIMIT TEST
VCTK p266
VCTK p262
Facebook 1
Facebook 2
Facebook 3
Carpenter 1
Carpenter 2
9.14
6.70
6.27
5.15
4.53
5.76
3.07
8.32
9.44
9.22
8.83
7.53
7.11
10.09
7.87
7.45
8.19
9.42
9.44
11.53
9.37
10.65
20.08
26.72
15.97
24.72
26.61
30.72
25.85
39.71
Table 1: WER (%) comparison of cloud services, Google and
AWS, versus the state-of-the-art ofﬂine system, Deep Speech.
Accuracy Comparison: Table 1 presents the WER compar-
ison results. The results show that the CSPs are superior to
the OSP on all the datasets. The performance gap, however,
is more signiﬁcant on the non-standard datasets; the CSP
outperforms Deep Speech by 60% to 80% in WER.
3 Privacy Threat Analysis
We study the privacy threats that a cloud-based transcription
service poses while processing private speech data.
3.1 Voice Analysis
The biometric information embedded in S can leak sensi-
tive information about the speakers, including their emo-
tional status [4, 40], health condition [41], sex [39], and even
identity [26]. Furthermore, extracting this information en-
ables critical attacks like voice cloning and impersonation
attacks [23, 47]. In this section, we showcase a few represen-
tative examples of how cloud-based APIs can pose serious
privacy threats to the acoustic features within S.
Speaker Diarization: CSPs utilize advanced diarization ca-
pabilities to cluster the speakers within a speech ﬁle, even
if they have not been observed before. The basic idea is to
(1) segment the speech ﬁle into segments of voice activity,
and (2) extract a speaker-speciﬁc embedding from each seg-
ment, such that (3) segments with close enough embeddings
should belong to the same speaker. We veriﬁed the strength
of the diarization threat over three multi-speaker datasets:
VCTK (mixing p266 and p262), Facebook, and Carpenter.
We measure the performance of the IBM diarization service
using Watson’s Speech-to-Text API 6 via Diarization Error
Rate (DER). DER estimates the fraction of time the speech
ﬁle segments are not attributed to the correct speaker clus-
ter. The DER values are 0%, 4.85%, and 1.32% for the three
6https://www.ibm.com/cloud/watson-speech-to-text
datasets, respectively. Hence, the API can correctly distin-
guish between, and cluster, the different speakers, more than
95% of the entire dataset duration despite lacking any prior
information about the individual speakers.
Speaker Identiﬁcation: A speaker identiﬁcation task maps
the speech segments in a speech ﬁle to an individual. We use
the Azure Identiﬁcation API, which consists of two stages: (1)
user enrollment and (2) identiﬁcation (whether a given voice
sample matches any of the enrolled users). The enrollment
stage requires only 30 seconds of speech from each user to
extract their voice-print. We enrolled 22 speakers as follows:
10 from DAPS, two from VCTK, two from Carpenter, and
eight from Facebook. The identiﬁcation accuracy was nearly
100% for all speakers.
Speaker Cloning and Impersonation: Lastly, we applied a
Tacotron-based speech synthesizer from Google [20]; a net-
work that can synthesize speech in the voice of any speaker.
The network generates a target speaker’s embedding, which it
uses to synthesize speech on a given piece of text. In our set-
ting, we used the network to generate the speakers’ embedding
in our evaluation datasets. Then, we synthesized eight speech
utterances using the embeddings of each speaker. We enrolled
the speakers in Azure’s Speech Identiﬁcation API using their
natural voice samples and tested whether the API will map the
synthesized segments to the corresponding speaker. Except