title:Advanced Probabilistic Couplings for Differential Privacy
author:Gilles Barthe and
No&apos;emie Fong and
Marco Gaboardi and
Benjamin Gr&apos;egoire and
Justin Hsu and
Pierre-Yves Strub
Advanced Probabilistic Couplings for Differential
Privacy
Gilles Barthe, Noémie Fong, Marco Gaboardi, Benjamin Grégoire, Justin Hsu,
Pierre-Yves Strub
To cite this version:
Gilles Barthe, Noémie Fong, Marco Gaboardi, Benjamin Grégoire, Justin Hsu, et al.. Advanced Prob-
abilistic Couplings for Differential Privacy. 23rd ACM Conference on Computer and Communications
Security , Oct 2016, Vienne, Austria. pp.55 - 67, 10.1145/2976749.2978391. hal-01410196
HAL Id: hal-01410196
https://hal.inria.fr/hal-01410196
Submitted on 6 Dec 2016
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Advanced Probabilistic Couplings for Differential Privacy∗
Gilles Barthe
IMDEA Software Institute
Madrid, Spain
Benjamin Grégoire
Inria
Sophia-Antipolis, France
Noémie Fong
ENS
Paris, France
‡
Justin Hsu
University of Pennsylvania
Philadelphia, USA
†
Marco Gaboardi
University at Buffalo, SUNY
Buffalo, USA
Pierre-Yves Strub
IMDEA Software Institute
Madrid, Spain
ABSTRACT
Differential privacy is a promising formal approach to data privacy,
which provides a quantitative bound on the privacy cost of an al-
gorithm that operates on sensitive information. Several tools have
been developed for the formal veriﬁcation of differentially private
algorithms, including program logics and type systems. However,
these tools do not capture fundamental techniques that have emerged
in recent years, and cannot be used for reasoning about cutting-edge
differentially private algorithms. Existing techniques fail to handle
three broad classes of algorithms: 1) algorithms where privacy de-
pends on accuracy guarantees, 2) algorithms that are analyzed with
the advanced composition theorem, which shows slower growth in
the privacy cost, 3) algorithms that interactively accept adaptive
inputs.
We address these limitations with a new formalism extending
apRHL [6], a relational program logic that has been used for proving
differential privacy of non-interactive algorithms, and incorporating
aHL [11], a (non-relational) program logic for accuracy properties.
We illustrate our approach through a single running example, which
exempliﬁes the three classes of algorithms and explores new variants
of the Sparse Vector technique, a well-studied algorithm from the
privacy literature. We implement our logic in EasyCrypt, and for-
mally verify privacy. We also introduce a novel coupling technique
called optimal subset coupling that may be of independent interest.
1.
INTRODUCTION
Differential privacy, a rigorous and quantitative notion of sta-
tistical privacy, is one of the most promising formal deﬁnitions of
privacy to date. Since its initial formulation by Dwork et al. [19],
differential privacy has attracted substantial attention throughout
∗The
at
https://arxiv.org/abs/1606.07143.
†Partially supported by NSF grants CNS-1237235 and CNS-
1565365, and by EPSRC grant EP/M022358/1.
‡Partially supported by NSF grants TC-1065060 and TWC-1513694,
and a grant from the Simons Foundation (#360368 to Justin Hsu).
available
version
paper
this
full
of
is
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978391
computer science, including areas like databases, machine learning,
and optimization, and more.
There are several reasons for this success. For one, differential
privacy allows a formal trade-off between privacy and accuracy:
differentially private algorithms come with a privacy guarantee
expressed in terms of two parameters  (expressing the privacy cost)
and δ (expressing the probability of violating the privacy cost). For
both parameters, smaller values offer stronger privacy guarantees.
Another important advantage differential privacy is that it com-
poses well: differentially private algorithms can be easily combined
to build new private algorithms. The differential privacy literature
offers several composition theorems, differing in how the privacy
parameter of the larger algorithm depends on the parameters of the
components. These composition properties can also be used in inter-
active and adaptive scenarios where an adversary can decide which
algorithm to run depending on the outputs of previous algorithms.
Differential privacy’s clean composition properties also make it
an attractive target for an unusually diverse array of formal veriﬁ-
cation techniques. By now, there are tools that formally guarantee
differential privacy via relational program logics [2, 6], linear type
systems [25, 29, 42], interactive automata [50, 51], product pro-
grams [7], satisﬁability modulo theories [28], reﬁnement type sys-
tems [9], and more. While these systems formalize privacy through
a wide variety techniques, most of these approaches analyze a com-
position of private algorithms using the sequential composition
theorem of differential privacy, which guarantees that the resulting
algorithms have parameter  and δ equal to the sum of the parameters
of their components.
Recently, Barthe et al. [10] highlighted a close connection be-
tween approximate couplings and differential privacy which enables
formal veriﬁcation beyond sequential composition. Barthe et al.
[10] work with the relational program logic apRHL [6], extended
with a new composition principle called pointwise privacy. The
idea is to ﬁrst prove a restricted case of privacy—corresponding
roughly to privacy for a single output—and then combine the results
to prove the full differential privacy property. Combined with the
composition principle for approximate couplings, which generalizes
the sequential composition theorem, apRHL can express simple,
compositional proofs of algorithms like the one-side Laplace im-
plementation of the Exponential mechanism [17] and the Above
Threshold algorithm [17] while abstracting away reasoning about
probabilistic events. Existing privacy proofs for these algorithms,
even on paper, involve ad hoc computation of probabilities.
While apRHL substantially expands the range of formal veriﬁ-
cation in differential privacy, there are still private algorithms that
apRHL cannot verify. Roughly, there are three missing pieces:
• Accuracy-dependent privacy. Some algorithms are only pri-
vate if an accuracy property holds.
• Advanced composition. This principle shows slower growth
in the privacy cost, in exchange for a small probability of
violating privacy. The proof involves an technical martingale
concentration argument.
• Interactive privacy. Some private algorithms are interactive,
receiving a continuous sequence of adaptive inputs while
producing intermediate outputs.
These three missing pieces correspond to three fundamental prin-
ciples of differential privacy. While there are many algorithms from
the privacy literature that use one (or more) of these three features, to
structure our presentation we will work with a variant of the Sparse
Vector technique based on the Between Thresholds algorithm [13],
a single unifying example that uses all three features (§ 2). After
reviewing some technical preliminaries about differential privacy,
approximate couplings, and the logic apRHL (§ 3), we describe
extensions to apRHL to verify privacy for new classes of algorithms.
• New proof rules that allow reasoning within apRHL while
assuming an accuracy property, incorporating accuracy proofs
from the Hoare logic aHL [11] (§ 4). We demonstrate these
rules on a classic example of accuracy-dependent privacy: the
Propose-test-release framework [16, 48].
• A proof rule that analyzes loops using the advanced composi-
tion principle; soundness relies on a novel generalization of
advanced composition to approximate couplings (§ 5).
• New proof rules for adversaries, external procedure calls that
model an adaptive source of inputs (§ 6).
• Orthogonal to reasoning about accuracy, advanced composi-
tion, and adversarial inputs, we introduce a general construc-
tion that may be of independent interest called the optimal
subset coupling. This construction gives an approximate lift-
ing relating subsets that yields the best possible , and we use
this construction to give a new interval coupling rule for the
Laplace distribution (§ 7).
We then show how to combine these ingredients to verify our
main running example, the Between Thresholds algorithm (§ 8). We
ﬁnish with related work (§ 9) and some concluding thoughts (§ 10).
2. MOTIVATING EXAMPLE
Before diving into the technical details we’ll ﬁrst present our
motivating example, which involves accuracy-dependent privacy,
advanced composition, and interactive privacy. We ﬁrst review the
deﬁnition of differential privacy, a relational property about proba-
bilistic programs proposed by Dwork, McSherry, Nissim and Smith.
Deﬁnition 1. Let the adjacency relation be Φ ⊆ A × A, and , δ >
0. A program M : A → Distr(B) satisﬁes (, δ)-differential
privacy with respect to Φ if for every pair of inputs a, a(cid:48) ∈ A such
that Φ(a, a(cid:48)) and every subset of outputs S ⊆ B, we have
y←M a(cid:48)[y ∈ S] + δ.
When δ = 0, we say that M is -differentially private.
[y ∈ S] ≤ exp() Pr
y←M a
Pr
Intuitively, Φ relates inputs that differ in a single individual’s data.
Then, differential privacy requires that the two resulting distributions
on outputs should be close.
ASVbt(a, b, M, N, d) :=
i ← 0; l ← [];
u $← L/2(0);
A ← a − u; B ← b + u;
while i < N ∧ |l| < M do
q ← A(l);
S $← L(cid:48)/3(evalQ(q, d));
if (A ≤ S ≤ B) then l ← i :: l;
i ← i + 1;
return l
Figure 1: Sparse Vector for Between Thresholds
Our motivating example is Adaptive Sparse Vector for Between
Thresholds (ASVbt), a variant of the Sparse Vector algorithm. Our
algorithm takes a stream of numeric queries as input, and answers
only the queries that take a value within some range. The main
beneﬁt of Sparse Vector is that queries that take a value outside
the range do not increase the privacy cost, even though testing
whether whether the query is (approximately) in the range involves
private data. Sparse Vector is an appealing example, because of its
popularity and its difﬁculty. In particular, the privacy proof of Above
Threshold is non-compositional and notoriously tricky, and several
variants1 of the algorithm were supposedly proved to be private
but were later shown to be non-private (Lyu et al. [34] provide a
comprehensive survey).
The code of ASVbt is shown in Fig. 1. At a high level, the algo-
rithm accepts a stream of adversarially chosen queries and produces
a list of queries whose answer lies (approximately) between two
threshold parameters a and b. The algorithm computes noisy ver-
sions A and B of a and b using the Laplace mechanism L, which
we review in the next section, and then performs an interactive loop
for a ﬁxed number (N) of iterations. Each step, a stateful adver-
sary A receives the current list l of queries whose answer on input
database d lies between [A, B] and selects a new query q. If its noisy
answer S lies in [A, B] and there have been fewer than M queries
between threshold, the algorithm adds q to the list l. Our algorithm
differs from standard presentations of Adaptive Sparse Vector [17]
in two signiﬁcant respects:
• we use BetweenThresholds rather than AboveThreshold
for deciding whether to add a query to the list;
• we do not rerandomize the noise on the thresholds each time
a query is added to l; therefore, our algorithm adds less noise.
ASVbt satisﬁes the following privacy guarantee.
Theorem 2. Let  and δ both be in (0, 1). Set
4(cid:112)2M ln(2/δ)

,
(cid:48) (cid:44)

and suppose a and b are such that
b − a ≥ 6
(cid:48)
(cid:48) ln(4/
) +
4

ln(2/δ).
1There exist multiple versions of Sparse Vector. The earliest ref-
erence seems to be Dwork et al. [20]; several reﬁnements were
proposed by Hardt and Rothblum [30], Roth and Roughgarden [44].
Applications often use their own variants, e.g. Shokri and Shmatikov
[46]. The most canonical version of the algorithm is the version
by Dwork and Roth [17].
The level of privacy depends on the sensitivity of the query, which
measures how far the function may differ on two related inputs.
Roughly, adding the same level of Laplace noise to a higher sensi-
tivity query will be less private.
Deﬁnition 4 (Sensitivity). A function F : A → Z is k-sensitive
with respect to Φ ⊆ A × A if |F (a1) − F (a2)| ≤ k for every
a1, a2 ∈ A such that Φ(a1, a2).
The Laplace mechanism satisﬁes an accuracy speciﬁcation.