### Bots and Troll Accounts: A Distinct Attack Vector

Bots and troll accounts represent a significantly different attack vector that must be addressed. Our study reveals that loose coordination through direct interactions (e.g., commenting on each otherâ€™s posts) and focused narrative pushing (e.g., posting the same content across multiple troll accounts) are core characteristics of troll accounts. We have developed an automated mechanism to detect troll accounts used in disinformation campaigns, which can serve as a blueprint for practitioners to build production detection systems. Our results suggest a lower bound on the efficacy of such automated detection systems.

### Richer Interactions on Reddit

Reddit itself has access to a much richer set of interactions (e.g., which accounts upvote what content/posts) than is publicly available. This allows for modeling troll interactions at a higher resolution than we can achieve with public data.

### Resilience to Evasion

TROLLMAGNIFIER's detection is designed to capture the key behaviors of troll accounts and their need to coordinate to spread disinformation effectively. However, like any machine-learning-based system, TROLLMAGNIFIER could be evaded by malicious actors who understand its operation. To evade detection, attackers would need to significantly alter their modus operandi, making their activities more similar to traditional automated fake activity, which can be detected by existing methods.

For example, attackers might post an overwhelming number of unrelated comments on legitimate threads. While this might evade TROLLMAGNIFIER, it could make the accounts stand out to existing detection systems that identify bot-like and spam activity, thus defeating the purpose [3, 8, 10, 49, 66].

Another evasion strategy could involve reducing the number of posts or interactions between troll accounts to avoid suspicion. However, this would require creating a larger number of accounts to maintain the same level of engagement, which could be detected by approaches that identify mass-created fake accounts [50, 71].

Finally, malevolent actors might avoid having their troll accounts interact with each other. This, however, would reduce the effectiveness of their operations, as a key part of their activity involves creating conflict around sensitive topics by having these accounts engage with each other, as demonstrated in our paper.

### Limitations

Our work has several limitations. First, TROLLMAGNIFIER requires a set of known troll accounts to bootstrap its capabilities, meaning it cannot detect new and emerging troll campaigns. The lack of ground truth also limited the scope of our experiments, restricting us to a single state-sponsored campaign. Techniques to establish rigorous ground truth are needed to foster further research in this area.

Additionally, our selection of legitimate accounts for training the classifier is a best-effort approach, and we cannot be 100% certain that the selected set does not contain any trolls. While we provide evidence that the 1,248 detected accounts behave like troll accounts, we do not have definitive proof. We are in contact with Reddit to obtain further details and confirmation.

### Future Work

TROLLMAGNIFIER presents a novel approach to improve defenses against troll accounts spreading disinformation. Future work could explore several avenues for improvement. For instance, investigating behavioral features independent of specific campaigns could allow TROLLMAGNIFIER to generalize to previously unseen campaigns.

In this paper, we studied one influence campaign carried out by Russian-sponsored accounts. If additional ground truth becomes available, future work could investigate whether TROLLMAGNIFIER can generalize to other influence campaigns. We provide some additional experiments involving UAE-sponsored Reddit accounts in the extended version of the paper [44].

Analyzing the narratives pushed by trolls and the strategies they use to spread these narratives is another promising area of research. Understanding how these behaviors affect legitimate social media users is crucial for better comprehending the disinformation landscape.

### Related Work

#### Detecting Malicious Activity on Social Media

**Detecting Malicious Messages:**
Computer security researchers have attempted to curb malicious content on social networks by detecting malicious messages automatically. Yardi et al. [69] developed a tool to detect Twitter spammers abusing trending topics. Thomas et al. [52] presented MONARCH, a system that analyzes URLs shared by social accounts for signs of maliciousness. Lee and Kim [30] proposed WARNINGBIRD, which analyzes correlated redirection chains of URLs to identify malicious tweets. Clustering techniques have also been used to group similar messages and flag them as spam [17, 20]. Liu et al. [32] used LDA to calculate the topics shared by spammers and then employed supervised learning to identify them based on these topics.

**Detecting Malicious Accounts:**
Another approach is to identify malicious accounts based on their characteristics. Early work focused on typical features of fake accounts, such as abnormal friend-to-follower ratios or similar content [3, 49]. Yang et al. [68] identified features in fake accounts that are more resilient to evasion. Ghosh et al. [19] investigated link farming, a phenomenon used by spam accounts to gain followers. Viswanath et al. [56] applied PCA to find patterns among features extracted from spam accounts. Egele et al. [13] focused on detecting compromised legitimate accounts, showing that sudden anomalies in user habits are highly indicative of a compromise. Wang et al. [60] analyzed user click patterns to create profiles and identify fake accounts using both supervised and unsupervised learning. Galan-Garcia et al. [16] aimed to detect fake accounts that harass social media users by analyzing the content of their comments.

**Social Connections and Synchronization:**
Other work assumes that fake accounts present fundamentally different social connections than real accounts. Cai et al. [7] split social networks into communities and tried to identify those that connect in unnatural ways. Danezis et al. [9] used Bayesian Inference to detect compromised accounts. Another line of research deals with the fact that fake accounts controlled by a single entity often act in a synchronized fashion. Cao et al. [8] proposed SynchroTrap, a system that clusters malicious accounts based on their actions and timing. Stringhini et al. [50] introduced EVILCOHORT, which identifies sets of social network accounts used by botnets by looking at communities accessed by common IP addresses.

**Message Propagation:**
The third line of work focuses on the propagation patterns of messages on social networks. Ye and Wu [70] studied general message and breaking news propagation on Twitter, identifying patterns indicative of false or true information. Vosoughi et al. [59] found that false news spreads faster than true information. Weng et al. [62] analyzed Twitter hashtags and showed that network communities can predict viral memes. Nematzadeh et al. [39] demonstrated that strong communities with high modularity facilitate global diffusion by enhancing local, intra-community spreading. Xu et al. [67] presented an early warning worm detection system that monitors user behavior for suspicious propagation. Mezzour et al. [34] showed how hacked accounts' message diffusion differs from normal accounts, as these accounts keep posting regardless of engagement or feedback.

#### Bot and Troll Activity on Social Media

A significant body of work has focused on social bots [4, 10, 14, 15, 54] and their role in spreading political disinformation. Zhang et al. [77] analyzed the Russian Internet Research Agency (IRA)'s disinformation campaign on Twitter, emphasizing the challenge of distinguishing between legitimate political expression and disinformation in polarized discussions. Kumar et al. [27] measured the phenomenon of sockpuppets, noting that these accounts actively manipulate opinions. Mihaylov and Nakov [36] identified two types of trolls: those acting independently and those paid to spread specific messages. Mihaylov et al. [35] showed that trolls can manipulate opinions in online forums. Steward et al. [48] studied Russian-sponsored trolls in the Black Lives Matter debate on Twitter, finding that trolls infiltrated both left and right-leaning communities to push specific narratives. Varol et al. [55] developed a system to identify popular memes due to coordinated efforts. Ratkiewicz et al. [42] used machine learning to detect the spread of false political information on Twitter.

Howard and Kollanyi [24] found that bots during the 2016 Brexit referendum mostly favored Brexit, with 1% of the accounts generating 33% of the overall messages. Hegelich and Janetzko [22] investigated whether bots on Twitter are used as political actors, uncovering their political agenda and various behaviors. Badawy et al. [1] aim to predict users likely to spread information from state-sponsored actors, while Dutt et al. [12] analyze Facebook ads shared by Russian trolls to find effective cues. Zannettou et al. [74, 75] analyzed state-sponsored troll accounts active on Twitter and Reddit between 2014 and 2018, finding that these accounts were created in waves and measuring their efficiency in spreading content. In follow-up work, they presented an analysis pipeline to study images posted by these accounts on Twitter [72].

Most of the above work focuses on studying rather than detecting troll accounts. Closer to our work are the few efforts toward detection [33, 58]. Volkova and Bell [58] analyzed 180k Twitter accounts during the Russia-Ukraine conflict, finding that lexical features are highly predictive of troll accounts. Luceri et al. [33] used Inverse Reinforcement Learning (IRL) to detect trolls, but their approach is not specific to troll accounts and may flag auto-moderator bots. Weller et al. [23] used deep learning to detect Russian trolls based on their comments, but their performance on the ground truth dataset was substantially lower than TROLLMAGNIFIER, highlighting the utility of our approach.

### Conclusion

This paper introduces TROLLMAGNIFIER, a system that learns the typical behavior of known state-sponsored troll accounts on Reddit to find more such accounts. The core insight is that troll accounts tend to interact with each other to further disinformation narratives and polarize online discussions. We tested TROLLMAGNIFIER on a Reddit dataset and identified 1,248 potential troll accounts. We found that 66% of the detected accounts show signs of being controlled by malicious actors and exhibit synchronization with known troll accounts, including using similar language. Our findings serve as a promising starting point for developing more effective detection systems against disinformation actors.

### Acknowledgments

We thank the anonymous reviewers for their comments and the discussion during the interactive rebuttal phase. This paper was supported by the NSF under grants CNS-1942610, IIS-2046590, CNS-2114407, IIP-1827700, and CNS-2114411, as well as the UKâ€™s National Research Centre on Privacy, Harm Reduction, and Adversarial Influence Online (REPHRAIN, UKRI grant: EP/V011189/1).

### References

[1] A. Badawy, K. Lerman, and E. Ferrara. Who Falls for Online Political Manipulation? arXiv:1808.03281, 2018.
[2] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn. The Pushshift Reddit Dataset. In AAAI Interna-