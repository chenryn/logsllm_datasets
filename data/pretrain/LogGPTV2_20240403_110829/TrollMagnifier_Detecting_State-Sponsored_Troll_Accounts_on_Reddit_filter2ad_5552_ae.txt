bots, troll accounts represent a meaningfully different attack
vector that must be addressed.
The key insight from our study is that loose coordination
via direct interaction (e.g., commenting on each other’s posts)
and focused narrative pushing (e.g., posting the same submis-
sion across different troll accounts) are core features of troll
accounts. Our study provides an automated mechanism for
detecting troll accounts used in disinformation campaigns, and
can serve as a blueprint for practitioners to build production
detection systems. If anything, our results indicate a lower
bound on the efﬁcacy of automated troll detection systems.
themselves have a much richer set of interactions
Reddit
than is publicly available (e.g., which accounts upvote what
content/posts) and can thus model troll interactions at a higher
resolution than we can.
B. Resilience to Evasion
TROLLMAGNIFIER’s detection is designed to embody the
key behavior of troll accounts and their need to coordinate
to effectively spread disinformation narratives. As with any
machine-learning powered detection system, however, TROLL-
MAGNIFIER could be evaded by miscreants once they get to
know how the model operates. However, we argue that, to do
so, attackers would need to signiﬁcantly change their modus
operandi, and that these adjustments would make their attacks
more similar to traditional automated fake activity on social
media, which can be detected by existing research.
For example, attackers could attempt to evade detection
by posting an overwhelming amount of unrelated comments
on legitimate threads. While this might successfully evade
TROLLMAGNIFIER, it might make the accounts stand out to
existing detection systems that aim to identify bot-like and
spam activity, defeating the purpose [3, 8, 10, 49, 66].
An alternative evasion strategy could be to have each troll
account post less or interact with a smaller number of troll
accounts or make fewer posts to avoid raising suspicion. If
miscreants adopted this solution, they would need to create
a larger number of accounts to keep the same level of
engagement, and this could be detected by other approaches
that detect mass-created fake accounts [50, 71].
Finally, malevolent actors might have their troll accounts
never interact with each other. However, this would make their
operation much less effective, since a key part of their activity
is based on creating conﬂict around sensitive topics by having
these accounts talk to each other, as shown in this paper.
C. Limitations
Naturally, our work is not free from limitations. First of
all, TROLLMAGNIFIER requires a set of known troll accounts
to bootstrap its capabilities. This means that our approach
cannot detect new and emerging troll campaigns that have
not been observed before. The lack of ground truth has
also limited the set of experiments that we could reliably run
in this paper, forcing us to only work with a single state-
sponsored campaign. Techniques to establish rigorous ground
truth for these problems are desperately needed by the research
community, and could foster more research in this space.
Another limitation is our best-effort selection of legitimate
accounts to train our classiﬁer (see Section IV-A): that is, we
cannot be 100% sure that the set we selected does not contain
any trolls. Finally, while we provide evidence that the 1,248
detected accounts behave like troll accounts, we do not have
deﬁnite proof of that; however, we are in contact with Reddit
to obtain further details/conﬁrmation from them.
D. Future Work
TROLLMAGNIFIER presents a ﬁrst-of-its-kind approach to
improve defenses against troll accounts posting disinforma-
tion. We envision several ways in which this work can be
improved in future work. First, TROLLMAGNIFIER requires
a seed dataset of known troll accounts to be trained on; an
interesting line of research would be to investigate behavioral
features that are independent of the speciﬁc campaign that
trolls belong to, allowing to generalize TROLLMAGNIFIER to
previously unseen campaigns.
Second, in this paper, we only studied one inﬂuence cam-
paign carried out by Russian-sponsored accounts. If further
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2171
(a)
(b)
(c)
Fig. 6: The ﬁgure shows the similarity of posts made by known trolls and accounts detected by TROLLMAGNIFIER. The
left-most post is made by a known-troll and the other two are from accounts detected by TROLLMAGNIFIER.
ground truth became available, future work could investi-
gate whether TROLLMAGNIFIER can generalize to additional
inﬂuence campaigns. In fact, we provide some additional
experiments in this direction, involving UAE-sponsored Reddit
accounts, in the extended version of the paper [44].
Additionally, one could look at the narratives being pushed
by trolls. An interesting area of research is to analyze the
strategies employed by these accounts and how they interact
to spread a particular narrative. Further analyzing this behavior
and how it affects legitimate social media users is critical to
better understand the disinformation landscape.
VIII. RELATED WORK
In this section, we discuss previous work on detecting
malicious accounts on social media, and survey research on
disinformation carried out by troll accounts on social media.
A. Detecting Malicious Activity on Social Media
Detecting malicious messages. Computer security researchers
attempted to curb the problem of malicious content on so-
cial networks by detecting malicious messages automatically
(e.g., spam). Yardi et al. [69] developed a tool
to detect
Twitter spammers who abuse trending topics. Thomas et al.
presented MONARCH [52], a system that analyzes the URLs
shared by social accounts for signs of maliciousness. Lee and
Kim [30] proposed WARNINGBIRD, a system that analyzes
correlated redirection chains of URLs in a number of URLs
posted on Twitter to identify malicious tweets. Another line
of work leverages clustering techniques to group together
similar messages posted on social media and ﬂagging them as
spam [17, 20]. Liu et al. [32] calculated the topics shared by
spammers with LDA, and then employed supervised learning
to identify spammers based on the topics that they discussed.
Detecting malicious accounts. Another approach is to identify
malicious accounts that are active on social networks based on
their characteristics. Early work looked at characteristics that
are typical of fake accounts, e.g., having an abnormal fraction
of friends compared to followers, or posting content that was
similar to each other [3, 49]. Yang et al. improved on this,
identifying features in fake accounts that are more resilient
to evasion by adversaries [68]. Ghosh et al. [19] investigated
link farming, a phenomenon used by spam accounts to allow
them to receive a large number of followers. Viswanath et
al. [56] applied Principal Components Analysis (PCA) to
ﬁnd patterns among features extracted from spam accounts.
Egele et al. [13] focused on detecting legitimate accounts
that have been compromised by an adversary, showing that
normal users have almost stable habits over time and that
sudden anomalies in these habits are highly indicative of a
compromise. Wang et al. [60] analyzed user click patterns
to create user proﬁles and identify fake accounts using both
supervised and unsupervised learning. Galan-Garcia et al. [16]
aim at detecting fake accounts that harass social media users
by analyzing the content of the comments made by those
accounts.
Other work is based on the assumption that fake accounts
present fundamentally different social connections than real
accounts. Cai et al. [7] split a social network into communities
and tried to identify communities that connect in an unnatural
or inconsistent way with the rest of the social network. Danezis
et al. [9] used the same idea to detect compromised accounts,
using a Bayesian Inference approach.
Another line of research deals with the fact
that fake
accounts are commonly controlled by a single entity, and
are therefore likely to act in a synchronized fashion. Cao
et al. proposed SynchroTrap [8], a detection system that
clusters malicious accounts according to their actions and
the time at which they are made. Stringhini et al. proposed
EVILCOHORT [50], a system that identiﬁes sets of social
network accounts used by botnets, by looking at communities
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2172
of accounts that are accessed by a common set of IP addresses.
Message Propagation. The third line of work focuses on the
way in which messages propagate on social networks. The
assumption is that malicious messages (e.g., spam) will show
different propagation patterns than legitimate ones. Ye and
Wu [70] studied propagation patterns of general messages
and breaking news in Twitter, identifying patterns that are
indicative of false or true information. Vosoughi et al. found
that false news gets shared at a higher rate than true infor-
mation [59]. Weng et al. [62] analyzed Twitter hashtags and
showed that network communities can help to predict viral
memes. Nematzadeh et al. [39] demonstrated that strong com-
munities with high modularity can facilitate global diffusion by
enhancing local, intra-community spreading. Xu et al. [67] pre-
sented an early warning worm detection system that monitors
the behavior of users to collect suspicious worm propagation
evidence. Through a simulation, Mezzour et al. [34] showed
how the diffusion of messages by hacked accounts differs from
normal accounts. In particular, these accounts keep posting
their content regardless of the engagement or feedback that
they receive from other users.
B. Bot and Troll Activity on Social Media
Zhang et al.
A large body of work focused on social bots [4, 10, 14,
15, 54] and their role in spreading political disinformation,
highlighting that bots can manipulate the public’s opinion at a
large scale, thus potentially affecting the outcome of elections.
[77] analyzed Russian Internet Research
Agency (IRA)’s disinformation campaign on Twitter. They
emphasized that, in already polarized discussion topics such
as politics, it is extremely challenging to distinguish between
“legitimate” political expression and “disinformation” since
such discussions are highly opinionated making them ideal
targets for disinformation attacks. Kumar et al. [27] measured
the phenomenon of multiple accounts controlled by the same
user, called sockpuppets, noting that these accounts actively
attempt to manipulate users’ opinions on online communities.
Mihaylov and Nakov [36] identiﬁed two types of trolls:
those who act on their own and those who are paid to spread
speciﬁc messages. In a related research effort, Mihaylov et
al. [35] showed that trolls can indeed manipulate users’ opin-
ions in online forums. Steward et al. [48] studied the activity
of Russian-sponsored trolls in the Black Lives Matter debate
on Twitter. They found that trolls inﬁltrated both left and
right-leaning communities, with the goal of pushing speciﬁc
narratives. Varol et al. [55] developed a system to identify
memes (ideas) that become popular due to coordinated efforts.
Ratkiewicz et al. [42] used machine learning to detect the
spread of false political information on Twitter.
Howard and Kollanyi [24] found that the bots active during
the 2016 Brexit referendum campaign were mostly pushing
narratives that favored Brexit, with 1% of the accounts gener-
ating 33% of the overall messages. Hegelich and Janetzko [22]
investigated whether bots on Twitter are used as political
actors. By exposing and analyzing 1.7K bots on Twitter during
the Russia-Ukraine conﬂict, they uncover their political agenda
and show that bots exhibit various behaviors, e.g.,
trying
to hide their identity, promoting topics through the use of
hashtags, and retweeting messages with particularly interesting
content. Badawy et al. [1] aim to predict users that are likely to
spread information from state-sponsored actors, while Dutt et
al. [12] focus on the Facebook platform and analyze ads shared
by Russian trolls to ﬁnd the cues that make them effective.
Zannettou et al. analyzed state-sponsored troll accounts
active on Twitter and Reddit between 2014 and 2018 [74, 75].
They found that these accounts were created in waves, and
measured their efﬁciency in spreading their content on those
platforms as well as on other Web communities. In follow-up
work, the same authors presented an analysis pipeline to study
the images posted by these accounts on Twitter [72].
The work discussed above predominantly focuses on study-
ing the activity of, rather than detecting, troll accounts. Closer
to our work are the very few efforts toward detection [33, 58].
Volkova and Bell [58] analyzed 180k Twitter accounts that
were active during the Russia-Ukraine conﬂict, ﬁnding that
lexical features are highly predictive of whether an account
will be identiﬁed as a troll by Twitter and suspended. Luceri et
al. [33] apply Inverse Reinforcement Learning (IRL) to detect
trolls. They use similar features to those previously used by
bot detection systems (e.g., replies, retweets) to automatically
detect troll accounts on Twitter. They ﬁnd that troll accounts
on Twitter keep posting regardless of whether other users
react to these posts, while the activity of regular users is
inﬂuenced by these interactions. While a detector trained this
way might work on a carefully selected dataset, the behavior
that Luceri et al. model
is not speciﬁc to troll accounts,
but it rather matches any account operating in an automated
fashion. This means that not only spam accounts would be
ﬂagged, but also auto-moderator bots that are commonly
deployed on Reddit. Therefore, we argue that this approach
is not suitable for the problem at hand. Unlike their work,
our effort is the ﬁrst to look at coordination between troll
accounts and leverage interaction between them for detection.
Weller et al. [23] use deep learning (i.e., Convolutional Neural
Networks and Region-based Convolutional Neural Networks)
to detect Russian trolls based on their comments. This is to
the best of our knowledge, the only research work that uses
the same dataset as ours for detection. Their performance
of their validation experiments on the ground truth dataset
is substantially lower than TROLLMAGNIFIER, which further
highlights the utility of our approach.
IX. CONCLUSION
This paper presented TROLLMAGNIFIER, a system that
learns the typical behavior of known state-sponsored troll
accounts on Reddit with the goal of ﬁnding more such
accounts. The core insight behind TROLLMAGNIFIER is that
troll accounts tend to interact with each other to further
disinformation narratives and to polarize online discussion. We
tested TROLLMAGNIFIER on a Reddit dataset and identiﬁed
1,248 potential
troll accounts. We ﬁnd that 66% of the
detected accounts show signs of being controlled by malicious
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2173
actors, and that these accounts as a group show signs of
synchronization with the set of known troll accounts, including
using similar language. Overall, we are conﬁdent that our
ﬁndings can serve as a promising starting point for researchers
and online social networks to develop more effective detection
systems against disinformation actors.
Acknowledgments. We thank the anonymous reviewers for
their comments and the discussion during the interactive rebut-
tal phase. This paper was supported by the NSF under grants
CNS-1942610, IIS-2046590, CNS-2114407, IIP-1827700, and
CNS-2114411, as well as the UK’s National Research Centre
on Privacy, Harm Reduction, and Adversarial Inﬂuence Online
(REPHRAIN, UKRI grant: EP/V011189/1).
REFERENCES
[1] A. Badawy, K. Lerman, and E. Ferrara. Who Falls for Online
Political Manipulation? arXiv:1808.03281, 2018.
[2] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and
J. Blackburn. The Pushshift Reddit Dataset. In AAAI Interna-