title:VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects
to Assist Code Audits
author:Henning Perl and
Sergej Dechand and
Matthew Smith and
Daniel Arp and
Fabian Yamaguchi and
Konrad Rieck and
Sascha Fahl and
Yasemin Acar
VCCFinder: Finding Potential Vulnerabilities in
Open-Source Projects to Assist Code Audits
Henning Perl*,
Sergej Dechand†,
Matthew Smith*†
* Fraunhofer FKIE, Germany
† University of Bonn, Germany
Daniel Arp,
Fabian Yamaguchi,
Konrad Rieck
University of Göttingen,
Germany
Sascha Fahl,
Yasemin Acar
Saarland University, Germany
ABSTRACT
Despite the security community’s best eﬀort, the number
of serious vulnerabilities discovered in software is increasing
rapidly. In theory, security audits should ﬁnd and remove
the vulnerabilities before the code ever gets deployed. How-
ever, due to the enormous amount of code being produced,
as well as a the lack of manpower and expertise, not all code
is suﬃciently audited. Thus, many vulnerabilities slip into
production systems. A best-practice approach is to use a
code metric analysis tool, such as Flawﬁnder, to ﬂag poten-
tially dangerous code so that it can receive special attention.
However, because these tools have a very high false-positive
rate, the manual eﬀort needed to ﬁnd vulnerabilities remains
overwhelming.
In this paper, we present a new method of ﬁnding poten-
tially dangerous code in code repositories with a signiﬁcantly
lower false-positive rate than comparable systems. We com-
bine code-metric analysis with metadata gathered from code
repositories to help code review teams prioritize their work.
The paper makes three contributions. First, we conducted
the ﬁrst large-scale mapping of CVEs to GitHub commits
in order to create a vulnerable commit database. Second,
based on this database, we trained a SVM classiﬁer to ﬂag
suspicious commits. Compared to Flawﬁnder, our approach
reduces the amount of false alarms by over 99 % at the same
level of recall. Finally, we present a thorough quantitative
and qualitative analysis of our approach and discuss lessons
learned from the results. We will share the database as
a benchmark for future research and will also provide our
analysis tool as a web service.
Categories and Subject Descriptors
D.2.4 [Software Engineering]: Software/Program Veriﬁ-
cation; K.6.5 [Management of Computing and Infor-
mation Systems]: Security and Protection
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813604.
Keywords
Vulnerabilities; Static Analysis; Machine Learning
1.
INTRODUCTION
Despite the best eﬀort of the security community, the
number of serious vulnerabilities discovered in deployed soft-
ware is on the rise. The Common Vulnerabilities and Expo-
sures (CVE) database operated by MITRE tracks the most
serious vulnerabilities.
In 2000, around 1,000 CVEs were
registered. By 2010, there were about 4,500. In 2014, al-
most 8,000 CVEs were registered. The trend seems to be
increasing in speed.
While it is considered a best practice to perform code
reviews before code is released, as well as to retroactively
checking old code, there is often not enough manpower to
rigorously review all the code that should be reviewed. Al-
though open-source projects have the advantage that any-
body can, in theory, look at all the source code, and although
bug-bounty programs create incentives to do so, usually only
a small team of core developers reviews the code.
In order to support code reviewers in ﬁnding vulnerabili-
ties, tools and methodologies that ﬂag potentially dangerous
code are used to narrow down the search. For C-like lan-
guages, a wide variety of code metrics can raise warning
ﬂags, such as a variable assigned inside an if-statement or
unreachable cases in a switch-statement. The Clang static
analyzer [1] as well as the dynamic analyzer Valgrind [3] and
others, can pinpoint further pitfalls such as invalid mem-
ory access. For the Linux kernel, the Trinity system call-
fuzzer [2] has found and continues to ﬁnd many bugs. Fi-
nally, static analysis tools like Flawﬁnder [34] help ﬁnd pos-
sible security vulnerabilities.
Most of these approaches operate on an entire software
project and deliver a (frequently very large) list of poten-
tially unsafe code. However, software grows incrementally
and it is desirable to have tools to assist in reviewing these
increments as well as tools to check entire projects. Most
open-source projects manage their source code with version
control systems (VCS) such as Git, Mercurial, CVS or Sub-
version. In such systems, code – including vulnerable code
– is inserted into the software in the form of commits to the
repository. Therefore, the natural unit upon which to check
whether new code is dangerous is the commit. However,
most existing tools cannot simply be executed on code snip-
pets contained within a commit. Thus, if a code reviewer
wants to check the security of a commit, the reviewer must
426execute the analysis software on the entire project and then
check if any of the warnings relate to the commit. This can
be a considerable amount of work, especially since many
tools require source code to be annotated and dynamic tests
would have to be constructed in a way that triggers the
commit.
Static and dynamic code analysis tools focus exclusively
on the code without the context of who wrote the code and
how it was committed. However, code repositories contain
a wealth of metadata which can be highly relevant to the
code quality. For instance, it can be seen whether a com-
mitter is new to the project or if they are one of the core
contributors. It is possible to see the time of day or night
at which code was submitted and to monitor the activity of
development in certain regions of code. Moreover, most ex-
isting code-metric-based tools have very high false-positive
rates, creating a (sometimes impossibly) high workload and
undermining trust in the eﬀectiveness of the tools. For in-
stance, Flawﬁnder tool created 5,460 false positives warnings
for only 53 true positives on the dataset used in this paper.
It is intuitively clear that code reviewers who want to ﬁnd
53 vulnerabilities in a set of 5,513 ﬂagged commits have a
tough time ahead of them.
In this paper, we present a classiﬁer that can identify po-
tentially vulnerable commits with a signiﬁcantly lower false-
positive rate while retain high recall rates. Therefore, unlike
most existing tools for vulnerability ﬁnding, we don’t focus
solely on code metrics, but also leverage the rich metadata
contained in code repositories.
To evaluate the eﬀectiveness of our approach, we conduct
a large-scale evaluation of 66 GitHub projects with 170,860
commits, gathering both metadata about the commits as
well as mapping CVEs to commits to create a database of
vulnerability-contributing commits (VCCs) and a benchmark
for future research.
We conducted a statistical analysis of these VCCs and
trained a Support Vector Machine (SVM) to detect them
based on the combination of code metric analysis and Git-
Hub metadata. For our evaluation we trained our classiﬁer
only on data up to December 31, 2010 and ran our tests
against CVEs discovered in 2011–2014.
In this dataset, our approach, called VCCFinder, pro-
duces only 36 false positives compared to Flawﬁnder’s 5,460
at the same level of recall. This is a reduction of over 99 %
and signiﬁcantly eases the workload of code reviewers.
1.1 Our Contributions
In summary, we make the following contributions in this
paper:
• We present VCCFinder, a code analysis tool that ﬂags
suspicious commits by using a SVM-based detection
model. Our method outperforms Flawﬁnder by a great
margin, reducing the false positives by over 99 % at the
same level of recall. Our methodology is suited to work
on code snippets, enabling us to analyse code at the
commit level and making a lightweight analysis of new
code far easier than requiring a full build environment
to be set up for each test.
• We construct the ﬁrst large-scale database mapping
CVEs to vulnerability-contributing commits (VCCs).
The database contains 66 GitHub projects, 170,860
commits and 640 VCCs. We conduct an extensive eval-
uation of the methodology used to create this database
to ascertain its quality as a benchmark for future re-
search.
• We present an extensive quantitative and qualitative
evaluation of VCCFinder and discuss take-aways, in-
cluding, for instance that, from a security perspective,
gotos are not generally harmful but in combination
with error-handling code they are responsible for a sig-
niﬁcant number of VCCs.
2. RELATED WORK
The discovery of vulnerabilities in program code is a fun-
damental problem of computer security. Consequently, it
has received much attention in the past. In the following,
we give a sample of the prior work most closely related to
our approach.
Static analysis.
The set of static analysis tools can be thought of as a spec-
trum ranging from faster, lightweight approaches to slower
but more thorough techniques. With VCCFinder being a
lightweight tool, we compare ourselves to FlawFinder [34],
a prominent representative of this class of tools. Other
lightweight approaches include Rats [9], Prefast [8] as well
as Splint [10], the later requiring manual annotations.
Regarding more thorough approaches Bandhakavi et al.
[11] search for vulnerabilities in browser extensions by apply-
ing static information-ﬂow analysis to the JavaScript code.
Dahse and Holz [15] introduced a static analyzer for PHP
that can detect sophisticated attacks against web applica-
tions. Finally, commercial tools like Coventry [5], Fortify [6],
CodeSonar [4], and IBM Security AppScan Source (formerly
Rational) [7] focus on a thorough analysis with conﬁgurable
rulesets and consequently long run times.
Symbolic execution.
Cadar et al. [12] present KLEE, a symbolic execution tool,
which requires manual annotation and modiﬁcation of the
source code. Also the runtime grows exponentially with the
number of paths in the program, which limits the size of
project which can be tested with KLEE. Thus it is not fea-
sible to execute KLEE on the same scale as VCCFinder.
However, it is an interesting area of future work to execute
KLEE as a second step after VCCFinder. Klee would then
only be used on the commits ﬂagged by VCCFinder which
hopefully would signiﬁcantly reduce the eﬀort needed to run
KLEE. We see these tools as complementary and separate
steps in the tool chain.
Dynamic analysis.
Cho et al. [14] use a combination of symbolic and con-
crete execution to build an abstract model of the analyzed
application and ﬁnd vulnerabilities in several open-source
projects. Yamaguchi et al. [37] provide an analysis plat-
form oﬀering fuzzy parsing of code that generates a graph
representing code suitable to be mined with graph-database
queries. This approach allows application-speciﬁc vulnera-
bility patterns to be expressed; however, in contrast to our
approach, it requires manual speciﬁcation of these patterns
by the analyst. Holler et al. [20] used fuzzing on code frag-
ments to ﬁnd vulnerabilities in the Mozilla JavaScript inter-
preter and the PHP interpreter.
427Software metrics.
Several authors have proposed to employ software met-
rics to home in on regions of code more likely to contain
vulnerabilities. For example, Zimmermann et al. [38] per-
form a large-scale empirical study on Windows Vista, indi-
cating that metrics such as code churn, code complexity [see
22, 19] and organizational measures allow vulnerabilities to
be detected with high precision at low recall rates, while
code dependency measures achieve low precision at high re-
call rates. However, Graylin et al. [18] point out that many
of these metrics may be highly correlated with lines of code.
In particular, they show empirically that the relation be-
tween cyclomatic complexity and lines of code is near-linear,
meaning that no reduction in the amount of code to read is
achieved in this way.
Repository analysis.
There is a range of research work looking at software repo-
sitories in relation to software vulnerabilities. The most rel-
evant with respect to our project can be divided into two
groups: those that look at code metrics and those that look
at metadata.
Neuhaus et al. [26] use the vulnerability database of the
Mozilla project to extract which software components have
had vulnerabilities in the past and which imports and func-
tion calls were involved. They use this to predict which soft-
ware components of the Mozilla Internet suite are most likely
to contain more vulnerabilities. Unlike our approach, they
do not use any metadata in their analysis and the results
ﬂag entire software components rather than single commits.
The results are thus more generic in the sense that they can
say only that one set of software components is more worth
checking than others.
On the other side, work conducted by Meneely et al. and
Shin et al. analyzes diﬀerent code repository metadata in
relation to CVEs [25, 23, 24]. Speciﬁcally, they check how
features such as code churn, lines of code, or the number of
reviewers from a project’s repository and review system data
correlate to reported vulnerabilities. They do this manually
for the Mozilla Firefox Browser, Apache HTTP server and
an excerpt of the RHEL Linux kernel. Unlike the work above
and our work, they do not use this data to predict vulnera-
bilities; moreover, unlike our work, they do not combine the
features but look at each separately.
Sadeghi et al. [28] aim to reduce the number of rules used
by static analysis software. For this they looked at “catego-
rized software repositories” (i.e. the Google Play Store) and
evaluated how knowledge of the app’s category can reduce
the number of static analysis rules needed to still retain full
coverage. For this, they compared Java programs on Source-
Forge (without a framework) to Android apps on F-Droid
(written with an application development framework). From
the app’s category they were able to build a predictor that
helps pick a subset of static analyzer rules to apply; therefore
reducing the time the static analyzer needs. Their method
works especially well with apps using a framework, such as
Android apps. In contrast, while this work reduces the num-
ber of rules used for analysis, we prioritize the code needed
to be analysed. These approaches are complementary.
Wijayasekara et al. [35] used bug-trackers to study bugs
that afterwards have been identiﬁed as vulnerabilities. The
work does not deal with ﬁnding unknown vulnerabilities.
While this does not directly relate to our work, bug-trackers
are an interesting additional source of information.
Kim et al. [21] mined logs of a project’s SCM repository for
bug-introducing changes using ﬁxed keywords (such as “ﬁx”
or “bug”). They then extracted features from these commits
and trained an SVM. Our approach diﬀers from the authors’
in three ways: ﬁrst, we use as a base of our research the much
smaller and thus harder set of critical vulnerabilities mined
from the CVE database; second, we use additional features
gathered from the SCM history such as past/future diﬀerent
authors; third we use a historical split to evaluate our system
opposed to a (random) ten-fold cross validation. The later
is important since it guarantees that our system was not
trained on future commits to decide if some past commit
introduced a bug. Unfortunately neither the code base nor
the data is available so a direct comparison is not possible.
We re-ran our experiments using random cross validation
and found that it increased the precision for around 15 %
with a recall between 0.4 and 0.6.
´Sliwerski et al. [32] present preliminary results on a sta-
tistical analysis of the Eclipse and Mozilla projects. They
mined the Eclipse project’s bug database for ﬁx-inducing
changes. Then, they do a statistical analysis on their data,