# VCCFinder: Identifying Potential Vulnerabilities in Open-Source Projects to Assist Code Audits

## Authors
- Henning Perl<sup>*</sup>
- Sergej Dechand<sup>†</sup>
- Matthew Smith<sup>*</sup><sup>†</sup>
- Daniel Arp
- Fabian Yamaguchi
- Konrad Rieck
- Sascha Fahl
- Yasemin Acar

<sup>*</sup> Fraunhofer FKIE, Germany  
<sup>†</sup> University of Bonn, Germany  
University of Göttingen, Germany  
Saarland University, Germany

## Abstract
Despite the security community's best efforts, the number of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should identify and remove these vulnerabilities before code deployment. However, due to the vast amount of code being produced and a lack of manpower and expertise, many vulnerabilities slip into production systems. A common approach is to use code metric analysis tools, such as Flawfinder, to flag potentially dangerous code for further review. However, these tools often have a high false-positive rate, making manual effort to find vulnerabilities overwhelming.

In this paper, we present a new method for identifying potentially dangerous code in code repositories with a significantly lower false-positive rate than comparable systems. We combine code-metric analysis with metadata from code repositories to help code review teams prioritize their work. Our contributions are threefold:
1. We conducted the first large-scale mapping of Common Vulnerabilities and Exposures (CVEs) to GitHub commits to create a vulnerable commit database.
2. Based on this database, we trained a Support Vector Machine (SVM) classifier to flag suspicious commits. Compared to Flawfinder, our approach reduces the false-positive rate by over 99% while maintaining the same recall level.
3. We provide a thorough quantitative and qualitative analysis of our approach, discussing lessons learned from the results. We will share the database as a benchmark for future research and provide our analysis tool as a web service.

## Categories and Subject Descriptors
D.2.4 [Software Engineering]: Software/Program Verification; K.6.5 [Management of Computing and Information Systems]: Security and Protection

## Permission
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.

CCS'15, October 12–16, 2015, Denver, Colorado, USA.  
© 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.  
DOI: http://dx.doi.org/10.1145/2810103.2813604.

## Keywords
Vulnerabilities, Static Analysis, Machine Learning

## 1. Introduction
Despite the best efforts of the security community, the number of serious vulnerabilities discovered in deployed software is on the rise. The Common Vulnerabilities and Exposures (CVE) database operated by MITRE tracks the most serious vulnerabilities. In 2000, around 1,000 CVEs were registered. By 2010, there were about 4,500, and in 2014, almost 8,000 CVEs were registered. This trend appears to be accelerating.

While it is considered best practice to perform code reviews before code is released and to retroactively check old code, there is often insufficient manpower to rigorously review all the code that should be reviewed. Although open-source projects have the advantage that anyone can, in theory, look at all the source code, and bug-bounty programs create incentives to do so, usually only a small team of core developers reviews the code.

To support code reviewers in finding vulnerabilities, tools and methodologies that flag potentially dangerous code are used to narrow down the search. For C-like languages, various code metrics can raise warning flags, such as a variable assigned inside an if-statement or unreachable cases in a switch-statement. Tools like Clang static analyzer, Valgrind, and Trinity system call-fuzzer can pinpoint further pitfalls such as invalid memory access. Static analysis tools like Flawfinder help find possible security vulnerabilities.

Most of these approaches operate on an entire software project and deliver a (frequently very large) list of potentially unsafe code. However, software grows incrementally, and it is desirable to have tools to assist in reviewing these increments as well as tools to check entire projects. Most open-source projects manage their source code with version control systems (VCS) such as Git, Mercurial, CVS, or Subversion. In such systems, code, including vulnerable code, is inserted into the software in the form of commits to the repository. Therefore, the natural unit upon which to check whether new code is dangerous is the commit. However, most existing tools cannot simply be executed on code snippets contained within a commit. Thus, if a code reviewer wants to check the security of a commit, they must execute the analysis software on the entire project and then check if any of the warnings relate to the commit. This can be a considerable amount of work, especially since many tools require source code to be annotated and dynamic tests would have to be constructed to trigger the commit.

Static and dynamic code analysis tools focus exclusively on the code without the context of who wrote the code and how it was committed. However, code repositories contain a wealth of metadata which can be highly relevant to code quality. For instance, it can be seen whether a committer is new to the project or if they are one of the core contributors. It is possible to see the time of day or night at which code was submitted and to monitor the activity of development in certain regions of code. Moreover, most existing code-metric-based tools have very high false-positive rates, creating a (sometimes impossibly) high workload and undermining trust in the effectiveness of the tools. For instance, Flawfinder created 5,460 false positive warnings for only 53 true positives on the dataset used in this paper. It is intuitively clear that code reviewers who want to find 53 vulnerabilities in a set of 5,513 flagged commits have a tough time ahead of them.

In this paper, we present a classifier that can identify potentially vulnerable commits with a significantly lower false-positive rate while retaining high recall rates. Unlike most existing tools for vulnerability finding, we don't focus solely on code metrics but also leverage the rich metadata contained in code repositories.

To evaluate the effectiveness of our approach, we conducted a large-scale evaluation of 66 GitHub projects with 170,860 commits, gathering both metadata about the commits as well as mapping CVEs to commits to create a database of vulnerability-contributing commits (VCCs) and a benchmark for future research. We conducted a statistical analysis of these VCCs and trained a Support Vector Machine (SVM) to detect them based on the combination of code metric analysis and GitHub metadata. For our evaluation, we trained our classifier only on data up to December 31, 2010, and ran our tests against CVEs discovered in 2011–2014.

In this dataset, our approach, called VCCFinder, produces only 36 false positives compared to Flawfinder's 5,460 at the same level of recall. This is a reduction of over 99% and significantly eases the workload of code reviewers.

### 1.1 Our Contributions
In summary, we make the following contributions in this paper:
- **VCCFinder**: A code analysis tool that flags suspicious commits using an SVM-based detection model. Our method outperforms Flawfinder by a great margin, reducing false positives by over 99% at the same level of recall. Our methodology is suited to work on code snippets, enabling us to analyze code at the commit level and making a lightweight analysis of new code far easier than requiring a full build environment to be set up for each test.
- **Large-Scale Database**: We construct the first large-scale database mapping CVEs to vulnerability-contributing commits (VCCs). The database contains 66 GitHub projects, 170,860 commits, and 640 VCCs. We conduct an extensive evaluation of the methodology used to create this database to ascertain its quality as a benchmark for future research.
- **Evaluation and Insights**: We present an extensive quantitative and qualitative evaluation of VCCFinder and discuss takeaways, including, for instance, that, from a security perspective, `gotos` are not generally harmful but, in combination with error-handling code, they are responsible for a significant number of VCCs.

## 2. Related Work
The discovery of vulnerabilities in program code is a fundamental problem in computer security. Consequently, it has received much attention in the past. In the following, we give a sample of the prior work most closely related to our approach.

### Static Analysis
The set of static analysis tools can be thought of as a spectrum ranging from faster, lightweight approaches to slower but more thorough techniques. With VCCFinder being a lightweight tool, we compare ourselves to Flawfinder, a prominent representative of this class of tools. Other lightweight approaches include Rats, Prefast, and Splint, the latter requiring manual annotations.

Regarding more thorough approaches, Bandhakavi et al. search for vulnerabilities in browser extensions by applying static information-flow analysis to the JavaScript code. Dahse and Holz introduced a static analyzer for PHP that can detect sophisticated attacks against web applications. Finally, commercial tools like Coventry, Fortify, CodeSonar, and IBM Security AppScan Source (formerly Rational) focus on a thorough analysis with configurable rulesets and consequently long run times.

### Symbolic Execution
Cadar et al. present KLEE, a symbolic execution tool that requires manual annotation and modification of the source code. The runtime grows exponentially with the number of paths in the program, which limits the size of the project that can be tested with KLEE. Thus, it is not feasible to execute KLEE on the same scale as VCCFinder. However, it is an interesting area of future work to execute KLEE as a second step after VCCFinder. KLEE would then only be used on the commits flagged by VCCFinder, which hopefully would significantly reduce the effort needed to run KLEE. We see these tools as complementary and separate steps in the tool chain.

### Dynamic Analysis
Cho et al. use a combination of symbolic and concrete execution to build an abstract model of the analyzed application and find vulnerabilities in several open-source projects. Yamaguchi et al. provide an analysis platform offering fuzzy parsing of code that generates a graph representing code suitable to be mined with graph-database queries. This approach allows application-specific vulnerability patterns to be expressed; however, in contrast to our approach, it requires manual specification of these patterns by the analyst. Holler et al. used fuzzing on code fragments to find vulnerabilities in the Mozilla JavaScript interpreter and the PHP interpreter.

### Software Metrics
Several authors have proposed employing software metrics to home in on regions of code more likely to contain vulnerabilities. For example, Zimmermann et al. perform a large-scale empirical study on Windows Vista, indicating that metrics such as code churn, code complexity, and organizational measures allow vulnerabilities to be detected with high precision at low recall rates, while code dependency measures achieve low precision at high recall rates. However, Graylin et al. point out that many of these metrics may be highly correlated with lines of code. In particular, they show empirically that the relation between cyclomatic complexity and lines of code is near-linear, meaning that no reduction in the amount of code to read is achieved in this way.

### Repository Analysis
There is a range of research work looking at software repositories in relation to software vulnerabilities. The most relevant with respect to our project can be divided into two groups: those that look at code metrics and those that look at metadata.

Neuhaus et al. use the vulnerability database of the Mozilla project to extract which software components have had vulnerabilities in the past and which imports and function calls were involved. They use this to predict which software components of the Mozilla Internet suite are most likely to contain more vulnerabilities. Unlike our approach, they do not use any metadata in their analysis, and the results flag entire software components rather than single commits. The results are thus more generic in the sense that they can say only that one set of software components is more worth checking than others.

On the other side, work conducted by Meneely et al. and Shin et al. analyzes different code repository metadata in relation to CVEs. Specifically, they check how features such as code churn, lines of code, or the number of reviewers from a project’s repository and review system data correlate to reported vulnerabilities. They do this manually for the Mozilla Firefox Browser, Apache HTTP server, and an excerpt of the RHEL Linux kernel. Unlike the work above and our work, they do not use this data to predict vulnerabilities; moreover, unlike our work, they do not combine the features but look at each separately.

Sadeghi et al. aim to reduce the number of rules used by static analysis software. For this, they looked at "categorized software repositories" (i.e., the Google Play Store) and evaluated how knowledge of the app’s category can reduce the number of static analysis rules needed to still retain full coverage. For this, they compared Java programs on SourceForge (without a framework) to Android apps on F-Droid (written with an application development framework). From the app’s category, they were able to build a predictor that helps pick a subset of static analyzer rules to apply, thereby reducing the time the static analyzer needs. Their method works especially well with apps using a framework, such as Android apps. In contrast, while this work reduces the number of rules used for analysis, we prioritize the code needed to be analyzed. These approaches are complementary.

Wijayasekara et al. used bug-trackers to study bugs that afterward have been identified as vulnerabilities. The work does not deal with finding unknown vulnerabilities. While this does not directly relate to our work, bug-trackers are an interesting additional source of information.

Kim et al. mined logs of a project’s SCM repository for bug-introducing changes using fixed keywords (such as “fix” or “bug”). They then extracted features from these commits and trained an SVM. Our approach differs from the authors’ in three ways: first, we use as a base of our research the much smaller and thus harder set of critical vulnerabilities mined from the CVE database; second, we use additional features gathered from the SCM history such as past/future different authors; third, we use a historical split to evaluate our system opposed to a (random) ten-fold cross-validation. The latter is important since it guarantees that our system was not trained on future commits to decide if some past commit introduced a bug. Unfortunately, neither the code base nor the data is available, so a direct comparison is not possible. We re-ran our experiments using random cross-validation and found that it increased the precision for around 15% with a recall between 0.4 and 0.6.

Śliwerski et al. present preliminary results on a statistical analysis of the Eclipse and Mozilla projects. They mined the Eclipse project’s bug database for fix-inducing changes. Then, they do a statistical analysis on their data.