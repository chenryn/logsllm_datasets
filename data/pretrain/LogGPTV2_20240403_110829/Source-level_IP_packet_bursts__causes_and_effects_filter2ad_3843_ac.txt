### Burst Removal and Its Effects on Network Traffic

#### Burst Removal Procedure
When a burst \( B_f(k) \) occurs in a flow \( f \), the first packet after this burst appears at time \( t_f(k^+) \). To remove the burst, we artificially space the packets of the burst uniformly between \( t_f(k) \) and \( t_f(k^+) \). It is important to note that the order of the packets in the flow remains unchanged after this rescheduling. This burst removal procedure cannot be performed in real-time by a source or router because it requires knowledge of \( t_f(k^+) \) when the burst starts. Additionally, this method is not equivalent to flow shaping or pacing, which would transmit the packets at a fixed rate. We refer to the resulting trace as "manipulated" to distinguish it from the original trace.

#### Effect of Bursts on Network Traffic

**Energy Plots and Scaling Behavior:**
Figure 6 compares the original and manipulated traces from two OC-12 links in terms of energy plots and scaling behavior, tail distribution, and queueing performance. The energy plots, shown on the left, cover timescales ranging from less than a millisecond to a few seconds. Both traces exhibit clear bi-scaling behavior. For the MRA trace, the scaling exponent is 0.35, and for the IND trace, it is 0.26, in short timescales (less than 25-200 ms). At larger timescales, the scaling exponents are 0.99 and 0.90, respectively, though these estimates are less accurate due to the short duration of the traces. The key observation is the difference between the original and manipulated traces: the scaling behavior in short timescales is dramatically reduced, with the scaling exponent dropping to almost zero. This implies that removing packet bursts leads to nearly uncorrelated packet arrivals over short timescales up to 100-200 ms. As expected, the scaling behavior in longer timescales remains unaffected.

**Tail Distribution:**
The middle graphs in Figure 6 show the tail distribution of the amount of bytes in non-overlapping 10 ms intervals. The average of this distribution is 189 KB for the MRA trace and 32 KB for the IND trace. Removing packet bursts from individual flows significantly reduces the probability of having bursts in the aggregate trace. This is expected, as most bursts in the aggregate trace are due to individual flows rather than different flows. The removal of bursts from the aggregate trace suggests that queueing performance will also improve significantly.

**Queueing Performance:**
The right graphs in Figure 6 show the maximum queue size that would develop at a link servicing the aggregate traffic as the link's capacity varies. The reduction in the maximum queue size, after removing source-level bursts, is significant, especially at moderate utilizations between 50% to 85%. This result aligns with the findings in [10].

### Summary and Future Work

This paper focuses on the causes and effects of packet bursts from individual flows in IP networks. We demonstrated that such bursts can create scaling in short timescales and increase queueing delays in traffic multiplexers. We identified several causes for source-level bursts, including the "microscopic" behavior of UDP and TCP protocols. Some causes, such as the implementation of the Idle Restart timer, can be eliminated with appropriate changes in the TCP protocol or implementation. Other causes, such as the segmentation of UDP messages into multiple IP packets, are more fundamental and may not be avoidable.

Although we have provided a plausible explanation for the presence of scaling in short timescales, we do not claim that source-level bursts are the only explanation. In ongoing work, we are investigating other important factors, such as the effect of TCP self-clocking, and studying the impact of per-flow shaping and TCP pacing on the correlation structure and marginal distributions of aggregate IP traffic.

### References

[1] A. Feldmann, A.C.Gilbert, and W.Willinger, “Data Networks as Cascades: Investigating the Multifractal Nature of the Internet WAN Traffic,” in Proceedings of ACM SIGCOMM, 1998.

[2] R. Riedi, M. S. Crouse, V. Ribeiro, and R. G. Baraniuk, “A Multifractal Wavelet Model with Application to Network Traffic,” IEEE Transactions on Information Theory, vol. 45, no. 3, pp. 992–1019, Apr. 1999.

[3] Z.-L. Zhang, V. Ribeiro, S. Moon, and C. Diot, “Small-Time Scaling behaviors of Internet backbone traffic: An Empirical Study,” in Proceedings of IEEE INFOCOM, Apr. 2003.

[4] N. Hohn, D. Veitch, and P. Abry, “Cluster Processes, a Natural Language for Network Traffic,” IEEE Transactions on Signal Processing, special issue on “Signal Processing in Networking”, 2003, Accepted for publication.

[5] P. Abry and D. Veitch, “Wavelet Analysis of Long-Range Dependent Traffic,” IEEE Transactions on Information Theory, vol. 44, no. 1, pp. 2–15, Jan. 1998.

[6] D. Veitch, “Code for the Estimation of Scaling Exponents,” http://www.cubinlab.ee.mu.oz.au/∼darryl, July 2001.

[7] A. Feldmann, A.C.Gilbert, W.Willinger, and T. G. Kurtz, “The Changing Nature of Network Traffic: Scaling Phenomena,” ACM Computer Communication Review, Apr. 1998.

[8] A. Feldmann, A.C.Gilbert, P. Huang, and W.Willinger, “Dynamics of IP Traffic: A Study of the Role of Variability and The Impact of Control,” in Proceedings of ACM SIGCOMM, 1999.

[9] N. Hohn, D. Veitch, and P. Abry, “Does fractal scaling at the IP level depend on TCP flow arrival processes?,” in Proceedings Internet Measurement Workshop (IMW), Nov. 2002.

[10] A. Erramilli, O. Narayan, A. L. Neidhardt, and I. Saniee, “Performance Impacts of Multi-Scaling in Wide-Area TCP/IP Traffic,” in Proceedings of IEEE INFOCOM, Apr. 2000.

[11] NLANR MOAT, “Passive Measurement and Analysis,” http://pma.nlanr.net/PMA/, May 2003.

[12] J. C. Mogul, “Observing TCP dynamics in real networks,” in Proceedings of ACM SIGCOMM, Aug. 1992.

[13] M. Allman, V. Paxson, and W. Stevens, TCP Congestion Control, Apr. 1999, IETF RFC 2581.

[14] A. Hughes, J. Touch, and J. Heidemann, Issues in TCP Slow-Start Restart After Idle, Mar. 1998, IETF Internet Draft, draft-ietf-tcpimpl-restart-00.txt (expired).

[15] J.C.R. Bennett, C. Partridge, and N. Shectman, “Packet Reordering is Not Pathological Network Behavior,” IEEE/ACM Transactions on Networking, vol. 7, no. 6, pp. 789–798, Dec. 1999.

[16] C. Dovrolis, P. Ramanathan, and D. Moore, “What do Packet Dispersion Techniques Measure?,” in Proceedings of IEEE INFOCOM, Apr. 2001, pp. 905–914.

### Appendix: Passive Capacity Estimation

Identifying packet bursts from a flow \( f \) at a trace point \( T \) requires an estimate of the pre-trace capacity \( \tilde{C}_f \) of flow \( f \). Here, we summarize a statistical methodology for estimating \( \tilde{C}_f \) for TCP flows using the timing of the flow's data packets. The methodology is based on the dispersion (time distance) of packet pairs [16].

For a TCP flow \( f \), let \( S_f(i) \) be the size of the \( i \)-th data packet, and \( \Delta_f(i) \) be the dispersion measurement between data packets \( i \) and \( i+1 \). When packets \( i \) and \( i+1 \) are of the same size, we compute a bandwidth sample \( b_i = S_f(i) / \Delta_f(i) \). Packets with different sizes traverse the network with different per-hop transmission latencies and thus cannot be used with the packet pair technique [16]. Based on the delayed-ACK algorithm, TCP receivers typically acknowledge pairs of packets, forcing the sender to respond to every ACK with at least two back-to-back packets. Therefore, we estimate that roughly 50% of the data packets were sent back-to-back and can be used for capacity estimation. The remaining packets, sent with a larger dispersion, will give lower bandwidth measurements. We sort the bandwidth samples of flow \( f \) and drop the lower 50% of them. To estimate the capacity of flow \( f \), we use a histogram-based method to identify the strongest mode among the remaining bandwidth samples; the center of the strongest mode gives the estimate \( \tilde{C}_f \). The bin width used is \( \omega = \frac{2 \times \text{IQR}}{K^{1/3}} \) (known as the "Freedman-Diaconis rule"), where IQR is the interquartile range and \( K \) is the number of bandwidth samples. We have verified this technique by comparing its estimates with active measurements, and the results are quite positive, though they are not included in this paper due to space constraints.

Figure 7 shows the distribution of capacity estimates in two traces. Note that the CDF is plotted in terms of TCP bytes rather than TCP flows. In the top graph, we see four dominant capacities at 1.5 Mbps, 10 Mbps, 40 Mbps, and 100 Mbps, corresponding to common link bandwidths: T1, Ethernet, T3, and Fast Ethernet. The bottom graph shows the capacity distribution for the outbound direction of the ATM OC-3 link at the University of Auckland, New Zealand. This link is rate-limited to 4.048 Mbps at layer-2. We observe two modes, at 3.38 Mbps and 3.58 Mbps, at layer-3. The former mode corresponds to 576B IP packets, while the latter mode corresponds to 1500B IP packets. The difference is due to the overhead of AAL5 encapsulation, which depends on the IP packet size.

Finally, our capacity estimation methodology cannot produce an estimate for interactive flows, flows consisting only of pure-ACKs, and flows carrying just a few data packets. However, we were able to estimate the capacity for 83% of the TCP bytes in the MRA-1028765523 trace, 92% of the TCP bytes in the IND-1041854717 trace, and 82% of the TCP bytes in the Auckland trace.

**Figure 7: Capacity distribution in terms of bytes at two links.**

- **OC12 link: MRA-1028765523 (20:12 EST, 08/07/2002)**
- **Univ. of Auckland OC3 link (outbound rate limit = 4.048 Mbps, 2001)**