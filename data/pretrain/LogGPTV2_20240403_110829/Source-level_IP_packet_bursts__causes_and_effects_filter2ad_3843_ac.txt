while the ﬁrst packet of f after this burst appears at time
tf (k+). We remove the burst Bf (k) by artiﬁcially spacing
the packets of the burst uniformly between tf (k) and tf (k+).
Note that the packets of ﬂow f remain in their original or-
der after respacing the bursts. Also note that this burst
removal procedure cannot be performed on-line by a source
or router, as it requires knowledge of tf (k+) when a burst
starts. Also, it is not equivalent to ﬂow shaping or pac-
ing; these latter approaches would transmit the packets of
a burst at a ﬁxed rate. We refer to the resulting trace as
manipulated, to distinguish it from the original trace.
Eﬀect of bursts. Figure 6 compares the original and ma-
nipulated traces, from two OC-12 links, in terms of three
aspects: energy plots and scaling behavior, tail distribution,
and queueing performance. At the left, we show the en-
ergy plot of the traces in timescales that extend from less
than a millisecond to a few seconds. Notice that both traces
show clear bi-scaling behavior, with a scaling exponent of
0.35 for the MRA trace and 0.26 for the IND trace in short
timescales (less than 25 − 200ms). The scaling exponent at
large timescales is 0.99 and 0.90, respectively, but its esti-
mation is less accurate due to the short duration of these
traces. The key observation, however, is the diﬀerence be-
tween the original and manipulated traces: the scaling behav-
ior in short timescales has been dramatically reduced, drop-
ping the scaling exponent to almost zero. This implies that
removing packet bursts would lead to almost uncorrelated
packet arrivals over a range of short timescales that extends
up to 100-200ms. As expected, the scaling behavior in longer
timescales has not been aﬀected.
The middle graphs of Figure 6 show the tail distribution of
the amount of bytes in non-overlapping 10ms intervals. The
average of this distribution is 189KB for the MRA trace and
32KB for the IND trace. Note that the removal of packet
bursts from individual ﬂows reduces signiﬁcantly the prob-
ability of having bursts in the aggregate trace. This was
expected, as most bursts at the aggregate trace are due to
individual ﬂows, instead of diﬀerent ﬂows. The removal of
bursts from the aggregate trace hints that the queueing per-
formance would also improve signiﬁcantly. Indeed, the right
graphs of Figure 6 show the maximum queue size that would
develop at a link that services the aggregate traﬃc, as we
vary the link’s capacity. The reduction in the maximum
queue size, after we remove the source-level bursts, is sig-
niﬁcant especially in moderate utilizations, between 50% to
85%. This result agrees with the ﬁndings of [10].
5. SUMMARY AND FUTURE WORK
This paper focused on the causes and eﬀects of packet
bursts from individual ﬂows in IP networks. We showed that
such bursts can create scaling in short timescales, and in-
creased queueing delays in traﬃc multiplexers. We identiﬁed
several causes for source-level bursts, investigating the “mi-
croscopic” behavior of the UDP and TCP protocols. Some of
these causes, such as the implementation of the Idle Restart
timer, can be eliminated with appropriate changes in the
TCP protocol or implementation. Some other causes, how-
ever, such as the segmentation of UDP messages in multiple
IP packets, are more fundamental in nature and they may
not be avoidable.
Even though we identiﬁed a plausible explanation for the
presence of scaling in short timescales, we do not claim that
source-level bursts are the only such explanation.
In on-
going work, we investigate other important factors, such as
the eﬀect of TCP self-clocking. We also study the eﬀect of
per-ﬂow shaping and TCP pacing on the correlation struc-
ture and marginal distributions of aggregate IP traﬃc.
6. REFERENCES
[1] A. Feldmann, A.C.Gilbert, and W.Willinger, “Data Networks
as Cascades: Investigating the Multifractal Nature of the
Internet WAN Traﬃc,” in Proceedings of ACM SIGCOMM,
1998.
[2] R. Riedi, M. S. Crouse, V. Ribeiro, and R. G. Baraniuk, “A
Multifractal Wavelet Model with Application to Network
Traﬃc,” IEEE Transactions on Information Theory, vol. 45,
no. 3, pp. 992–1019, Apr. 1999.
[3] Z.-L. Zhang, V. Ribeiro, S. Moon, and C. Diot, “Small-Time
Scaling behaviors of Internet backbone traﬃc: An Empirical
Study,” in Proceedings of IEEE INFOCOM, Apr. 2003.
[4] N. Hohn, D. Veitch, and P. Abry, “Cluster Processes, a Natural
Language for Network Traﬃc,” IEEE Transactions on Signal
Processing, special issue on “Signal Processing in
Networking”, 2003, Accepted for publication.
[5] P. Abry and D. Veitch, “Wavelet Analysis of Long-Range
Dependent Traﬃc,” IEEE Transactions on Information
Theory, vol. 44, no. 1, pp. 2–15, Jan. 1998.
http://www.cubinlab.ee.mu.oz.au/∼darryl, July 2001.
[6] D. Veitch, “Code for the Estimation of Scaling Exponents,”
[7] A. Feldmann, A.C.Gilbert, W.Willinger, and T. G. Kurtz, “The
Changing Nature of Network Traﬃc: Scaling Phenomena,”
ACM Computer Communication Review, Apr. 1998.
[8] A. Feldmann, A.C.Gilbert, P. Huang, and W.Willinger,
“Dynamics of IP Traﬃc: A Study of the Role of Variability and
The Impact of Control,” in Proceedings of ACM SIGCOMM,
1999.
[9] N. Hohn, D. Veitch, and P. Abry, “Does fractal scaling at the
IP level depend on TCP ﬂow arrival processes?,” in Proceedings
Internet Measurement Workshop (IMW), Nov. 2002.
[10] A. Erramilli, O. Narayan, A. L. Neidhardt, and I. Saniee,
“Performance Impacts of Multi-Scaling in Wide-Area TCP/IP
Traﬃc,” in Proceedings of IEEE INFOCOM, Apr. 2000.
[11] NLANR MOAT, “Passive Measurement and Analysis,”
http://pma.nlanr.net/PMA/, May 2003.
[12] J. C. Mogul, “Observing TCP dynamics in real networks,” in
Proceedings of ACM SIGCOMM, Aug. 1992.
[13] M. Allman, V. Paxson, and W. Stevens, TCP Congestion
Control, Apr. 1999, IETF RFC 2581.
[14] A. Hughes, J. Touch, and J. Heidemann, Issues in TCP
Slow-Start Restart After Idle, Mar. 1998, IETF Internet Draft,
draft-ietf-tcpimpl-restart-00.txt (expired).
[15] J.C.R. Bennett, C. Partridge, and N. Shectman, “Packet
Reordering is Not Pathological Network Behavior,”
IEEE/ACM Transactions on Networking, vol. 7, no. 6, pp.
789–798, Dec. 1999.
1.6 
6.4 
0.4 
Original,        α=0.262 (4, 10)
Manipulated, α=0.043 (4, 10)
25.6 
102.4 
409.6 
1638.4 
(ms) 
IND−1041854717 
1
0.1
0.01
]
x
>
X
[
P
24
23
22
21
20
19
)
y
g
r
e
n
E
(
g
o
l
2
Original
Manipulated
)
B
K
(
h
t
g
n
e
l
e
u
e
u
q
m
u
m
i
x
a
M
200
150
100
50
0
Original
Manipulated
0.5
0.6
0.7
Utilization
0.8
)
y
g
r
e
n
E
(
g
o
l
2
6.4 
1.6 
0.4 
Original,       α=0.351 (2, 9)
Manipulated, α=0.019 (2, 9)
25.6 
102.4 
409.6 
1638.4 
(ms) 
MRA−1028765523 
29
28
27
26
25
24
23
22
21
1
0.1
0.01
MRA-1028765523 (20:12 EST, 08/07/2002)
MRA-1028765523 (20:12 EST, 08/07/2002)
Original
Manipulated
Original
Manipulated
200
150
100
50
)
B
K
(
h
t
g
n
e
l
e
u
e
u
q
m
u
m
i
x
a
M
]
x
>
X
[
P
0
2
4
6
8
j = log
2
(scale)
10
12
14
16
0.001
100
150
200
250
Traffic in 10ms (KB)
300
350
0
0.4
0.5
0.6
Utilization
0.7
0.8
IND-1041854717 (07:05 EST, 01/06/2003)
IND-1041854717 (07:05 EST, 01/06/2003)
0
2
4
6
8
j = log
2
(scale)
10
12
14
16
0.001
20
40
60
Traffic in 10ms (KB)
80
Figure 6: Eﬀect of source-level bursts on scaling, tail distribution, and queueing performance.
[16] C. Dovrolis, P. Ramanathan, and D. Moore, “What do Packet
Dispersion Techniques Measure?,” in Proceedings of IEEE
INFOCOM, Apr. 2001, pp. 905–914.
Appendix: Passive capacity estimation
The identiﬁcation of packet bursts from a ﬂow f at a trace
point T requires an estimate of the pre-trace capacity ˜Cf of
ﬂow f . Here, we summarize a statistical methodology that
estimates ˜Cf for TCP ﬂows, using the timing of the ﬂow’s
data packets. The methodology is based on the dispersion
(time distance) of packet pairs [16].
For a TCP ﬂow f , let Sf (i) be the size of the i’th data
packet, and ∆f (i) be the dispersion measurement between
data packets i and i+1. When packets i and i+1 are of the
same size, we compute a bandwidth sample bi = Sf (i)/∆f (i).
Packets with diﬀerent sizes traverse the network with dif-
ferent per-hop transmission latencies, and so they cannot
be used with the packet pair technique [16]. Based on the
delayed-ACK algorithm, TCP receivers typically acknowl-
edge pairs of packets, forcing the sender to respond to every
ACK with at least two back-to-back packets. So, we can
estimate that roughly 50% of the data packets were sent
back-to-back, and thus they can be used for capacity es-
timation. The rest of the packets were sent with a larger
dispersion, and so they will give lower bandwidth measure-
ments. Based on this insight, we sort the bandwidth samples
of ﬂow f , and then drop the lower 50% of them. To esti-
mate the capacity of ﬂow f , we employ a histogram-based
method to identify the strongest mode among the remain-
ing bandwidth samples; the center of the strongest mode
gives the estimate ˜Cf . The bin width that we use is ω =
2(IRQ)
(known as “Freedman-Diaconis rule”), where IRQ
K1/3
and K is the interquartile range and number, respectively,
of bandwidth samples. We have veriﬁed this technique com-
paring its estimates with active measurements. The results
are quite positive, but due to space constraints we do not
include them in this paper.
Figure 7 shows the distribution of capacity estimates in
two traces. Note that the CDF is plotted in terms of TCP
bytes, rather than TCP ﬂows.
In the top graph, we see
four dominant capacities at 1.5Mbps, 10Mbps, 40Mbps, and
100Mbps. These values correspond to the following com-
mon link bandwidths: T1, Ethernet, T3, and Fast Ether-
net. The bottom graph shows the capacity distribution for
the outbound direction of the ATM OC-3 link at Univer-
sity of Auckland, New Zealand. This link is rate-limited to
4.048Mbps at layer-2. We observe two modes, at 3.38Mbps
and 3.58Mbps, at layer-3. The former mode corresponds
to 576B IP packets, while the latter mode corresponds to
1500B IP packets. The diﬀerence is due to the overhead of
AAL5 encapsulation, which depends on the IP packet size.
We ﬁnally note that our capacity estimation methodology
cannot produce an estimate for interactive ﬂows, ﬂows that
consist only pure-ACKs, and ﬂows that carry just a few data
packets. We were able, however, to estimate the capacity for
83% of the TCP bytes in the MRA-1028765523 trace, 92%
of the TCP bytes in the IND-1041854717 trace, and 82% of
the TCP bytes in the Auckland trace.
OC12 link: MRA-1028765523 (20:12 EST, 08/07/2002)
100
90
80
70
60
50
40
30
20
10
)
%
(
s
e
t
y
b
n
i
F
D
C
Univ. of Auckland OC3 link (outbound rate limit = 4.048 Mbps, 2001)
MSS=1500
100
90
80
70
60
50
40
30
20
10
)
%
(
s
e
t
y
b
n
i
F
D
C
MSS=576
0
10
100
1000
10000
Capacity (Kbps)
1e+05
1e+06
0
3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000
Capacity (Kbps)
Figure 7: Capacity distribution in terms of bytes at
two links.