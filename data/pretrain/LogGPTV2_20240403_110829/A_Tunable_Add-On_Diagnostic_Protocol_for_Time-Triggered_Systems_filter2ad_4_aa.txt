title:A Tunable Add-On Diagnostic Protocol for Time-Triggered Systems
author:Marco Serafini and
Neeraj Suri and
Jonny Vinter and
Astrit Ademaj and
Wolfgang Brandst&quot;atter and
Fulvio Tagliabo and
Jens Koch
A Tunable Add-On Diagnostic Protocol for Time-Triggered Systems ∗
Marco Seraﬁni and Neeraj Suri
TU Darmstadt, Germany
Jonny Vinter
SP, Sweden
Astrit Ademaj
TU Vienna, Austria
{marco, suri}@informatik.tu-darmstadt.de
PI:EMAIL
PI:EMAIL
Wolfgang Brandst¨atter
Audi, Germany
Fulvio Tagliab`o
Fiat, Italy
Jens Koch
Airbus Deutschland, Germany
PI:EMAIL
PI:EMAIL
PI:EMAIL
Abstract
We
present
a
protocol
diagnostic
systems
for
tunable
generic time-triggered (TT)
to detect crash
and send/receive omission faults. Compared to existing
diagnostic and membership protocols for TT systems, it
does not rely on the single-fault assumption and tolerates
malicious faults. It runs at the application level and can be
added on top of any TT system (possibly as a middleware
component) without requiring modiﬁcations at the system
level. The information on detected faults is accumulated
using a penalty/reward algorithm to handle transient faults.
After a fault is detected, the likelihood of node isolation can
be adapted to different system conﬁgurations, including
those where functions with different criticality levels
are integrated. Using actual automotive and aerospace
parameters, we experimentally demonstrate the transient
fault handling capabilities of the protocol.
1. Introduction and contributions
In both automotive and aerospace X-by-wire appli-
cations, TT platforms such as Flexray [1], TTP/C [2],
SAFEbus [3] and TT-Ethernet are increasingly being
adopted. Most TT platforms develop their static, built-in
diagnostic and membership approach.
Instead, we deﬁne
an on-line diagnostic/membership protocol that is a tunable
and portable add-on application level module.
It can be
integrated as a plug-in middleware module (as an applica-
tion) onto any TT system, without interference with other
functionalities.
It only uses information that is available
at the application level, does not impose constraints on the
scheduling of the system, and has low bandwidth require-
ments. For TT platforms, such as FlexRay, SAFEbus and
TT-Ethernet, that do not have a standardized diagnostic or
membership protocol, our add-on protocol represents a vi-
able solution for such functionalities.
∗Research supported in part by EC DECOS, ReSIST and DFG TUD-
GK MM
Our diagnostic protocol exploits speciﬁc features of TT
systems where multiple nodes access a shared broadcast
bus using TDMA communication. The ability of a node
to send correct messages in the designated time window
(called sending slot) is used as a periodic diagnostic test.
The protocol is able to detect bursts of multiple concurrent
faults and to tolerate malicious faults.
Its resiliency also
scales with the number of available nodes.
The key purpose of a diagnostic protocol is to trigger
correct and timely recovery/maintenance actions, particu-
larly for safety critical subsystems. However, a diagnostic
protocol needs also consider availability and avoid unnec-
essary substitutions of correct components in case of exter-
nal transient faults, which are becoming more frequent [4].
An “ideal” diagnostic protocol would exclude only nodes
with internal faults. In practice, however, internal faults do
not always manifest as permanent faults at the interface of
the node (e.g. crashes). They can also manifest as mul-
tiple, subsequent intermittent faults which, to external ob-
servers, appear similar to external transient faults. We con-
sider an extended fault model to characterize healthy and
unhealthy nodes based on the presence of internal faults. In
order to recognize unhealthy nodes, a penalty/reward (p/r)
algorithm delays the isolation of faulty nodes to accumulate
on-line diagnostic information. This is a novel extension of
the basis developed in [5, 6] and represents an application
of our alternative p/r model [7].
A problem similar to diagnosis is membership, which
consists of identifying the set of nodes (called membership
view) that have received the same set of messages. We will
show that a variant of our protocol can act as a member-
ship service and detect the formation of multiple cliques of
receivers with inconsistent information.
We have implemented the protocol in a prototype, re-
producing practical automotive and aerospace settings. Us-
ing physical fault injection, we experimentally validate the
properties of the protocol and show how to tune the param-
eters of the p/r algorithm in a realistic environment.
The paper is organized as follows. Following the related
work in Sec. 2, we introduce the system and fault models in
Sec. 3 and 4. The tunable add-on diagnostic protocol and its
properties are presented in Sec. 5 and 6. The protocol is ex-
tended to a membership protocol in Sec. 7. Sec. 8 describes
the experimental validation of both protocols. We detail pa-
rameter tuning in Sec. 9. Sec. 10 discusses the portability
of the middleware to different TT platforms.
2. Related work
The general diagnosis problem was formulated in the
PMC model [8], where a set of active entities test each other
until sufﬁcient information exists to locate the faulty nodes.
In on-line, real time settings the comparison approach is
recommended [9], where the same functionality is executed
on different nodes and the results are compared.
Multiple research efforts have targeted diagnosis for spe-
ciﬁc error models, and for improving speciﬁc attributes such
as latency reduction, coverage and bandwidth. The fam-
ily of diagnostic protocols for generic synchronous systems
proposed by Walter et al. [11] considers a frame-based com-
munication scheme where nodes exchange messages in syn-
chronous parallel rounds using a fully connected topology
and unidirectional links. Similar to consensus [18, 10], all
nodes exchange their local view on the correctness of the
messages received by the other nodes and combine them
using hybrid voting to achieve consistent diagnosis.
We adapt the on-line diagnosis approach of [11] as a
middleware service for TT systems, where multiple nodes
access a shared broadcast bus using a TDMA communica-
tion scheme. Our add-on protocol explicitly takes into ac-
count the internal scheduling of each node and the overall
global communication scheduling of the system. We extend
the protocol to consider the cases of communication black-
out, which can arise if particularly long transient bursts cor-
rupt all sending slots in the TDMA round. We also show
how to modify the protocol to provide membership infor-
mation. Finally, we deﬁne a new p/r algorithm to handle
transient faults based on the criticality of the applications
executing on different nodes.
A count-and-threshold fault detection function (called α-
count) is introduced in [5, 6] to discriminate between tran-
sient and intermittent faults. The fundamental tradeoffs in
its tuning are explored using stochastic evaluation. Our al-
ternate p/r model, which develops an overall FDIR (Fault
Detection, Isolation and Reconﬁguration) strategy, is intro-
duced and analyzed in [7]. In this work we present how to
experimentally tune the p/r algorithm in realistic settings.
The problem of group membership is often deﬁned sim-
ilar to diagnosis [12]. Cristian [13] proposed a membership
protocol for synchronous crash-only systems that is based
on an expensive fault-tolerant atomic broadcast primitive to
achieve consistency. Such an approach is impractical in TT
systems due to its high latency and bandwidth requirement.
A membership protocol speciﬁcally designed for TTP/C
It relies
systems was proposed by Kopetz et al. [2, 14].
on the ”single fault assumption”, i.e., it does not tolerate
simultaneous faults, and assumes non-malicious node fail-
ures. The protocol allows identiﬁcation of one fault in the
communication of a message. Besides faulty senders, the
protocol also detects if asymmetric receiver faults cause the
formation of different cliques of nodes. The latency is two
communication slots in the case of sender faults and two
TDMA rounds in case of receiver faults. The bandwidth re-
quired is O(N ) bits per message and O(N 2) bits per round,
where N is the number of system nodes. If a (possibly tran-
sient) faulty node is detected it is generally restarted, gener-
ating a window of vulnerability to subsequent failures. An
extension of this protocol was proposed by Ezichelvan and
Lemos [15] to tolerate up to half of senders being simultane-
ously faulty with a latency of three TDMA rounds. Our pro-
tocol tolerates multiple coincident non-malicious and mali-
cious faults with the same bandwidth requirement. Due to
its add-on and generic nature, it has a higher latency. How-
ever, in Sec. 10 we show that a system-level variant of our
protocol features a latency of two TDMA rounds.
3. System model
We assume a synchronous system model and a net-
work topology where all nodes access a shared (and pos-
sibly replicated) communication bus using a TDMA ac-
cess scheme, i.e., a periodic schedule where each node
is assigned a time window, called sending slot, in each
TDMA round (or round). The periodic global communica-
tion schedule, including when each slot begins and termi-
nates, is deﬁned at design time and executed by a communi-
cation controller. The communication controller features a
local collision detection mechanism, which checks if mes-
sages sent by the node can actually be read from the bus.
The systems consists of N nodes with unique IDs
{1, ..., N } assigned following the order of the sending slots
in the round. Correct nodes can identify a sender by its
sending time and there is no message forging. Faulty nodes
cannot corrupt messages sent by correct nodes.
Communication among jobs, including those running on
different nodes, is abstracted by a vector of shared variables
hv1, . . . , vN i called interface variables. Communication
controllers automatically update their value by sending and
receiving messages according to the global communication
schedule. Copies of the interface variables are updated at
the receivers after every sending slot is completed. Updates
follow the sending order of the corresponding messages. In-
terface variables can be updated at most once per round.
Each interface variable has a corresponding validity bit.
This is set to 0 by the communication controller when the
value of the variable can no longer be considered correct. If
an interface variable vi has node i as its unique sender and
is updated at each round, we can assume that the communi-
cation controller uses its local error detection mechanisms
to set the validity bit of vi at the receiver node j to 0 iff node
j was not able to receive the last message sent by i that was
supposed to update vi, and 1 otherwise. Validity bits are
updated together with the corresponding messages.
Besides the global communication schedule, each node
has its own internal node schedule that determines when
jobs are executed.
In a TDMA access scheme, the send-
ing slot of a node overlaps with the computational phase of
other nodes. The node schedule can thus have an effect on
the “freshness” of the read interface state, i.e., the round
where the values of the interface variables were sent. For
example, if a job is executed at the beginning of a round it
will only read values sent in the previous rounds. The node
schedule also determines the round when the data written in
the interface state is actually sent on the bus. A job might
be able to send its output data in the same round as it is
executed only if it is scheduled before the sending slot of
the hosting node. To increase the portability of our add-on
protocol, we do not constrain the scheduling of nodes.
4. Fault model
We use a Customizable Fault-Effect Model [16] which
refers to the communication errors in the broadcast of
a message. A received faulty message is locally de-
tectable if it is syntactically incorrect in the value domain or
early/late/missing in the time domain; it is malicious faulty
if it is not locally detectable but is semantically incorrect in
the value domain.
Correspondingly, we partition faults into three classes:
- symmetric benign: (or benign) message is locally de-
tectable by all the receivers;
- symmetric malicious: all the receivers receive the same
malicious message;
- asymmetric: message is locally detectable by at least
one but not all the receivers.
We assume broadcast channels, where different locally un-
detectable messages cannot be asymmetrically received by
different nodes. Asymmetries in the local detection of
messages can be an effect, for example, of Slightly-Off-
Speciﬁcation faults (SOS) [17], when the clock of a node
is close to the allowed offset and thus the messages it sends
are seen as timely only by a subset of the receivers. Another
example is when EMI disturbs only part of the bus.
We classify nodes based on the communication errors
they display in their outgoing messages, e.g., benign faulty
sender, malicious faulty sender etc. We assume that each
node can display only one type of communication error
throughout one execution of the protocol. Correct nodes
send messages without faults. Obedient nodes follow the
program instructions and execute only correct internal state
transitions. They can either be correct or suffer omission
failures while sending or receiving messages.
For diagnosis, we do not assume permanent faults but
consider an extended fault model instead, where all nodes
alternate periods of faulty behavior, when they are not able
to correctly send messages, and periods of correct behavior.
We consider a node:
- healthy, if it suffers only sporadic and external tran-
sient faults;
- unhealthy, if it suffers internal faults which manifest
as intermittent or permanent communication faults.
We implicitly assume that internal faults will manifest at
the interface of the node either (a) as permanent sender
faults (a long faulty burst) or (b) as intermittent faults with
a shorter time to reappearance than external transient faults.
A crashed node, for example, is an unhealthy node that per-
manently displays benign faults.
5. The on-line diagnostic protocol
The purpose of the on-line diagnostic protocol is to de-
tect and isolate unhealthy nodes from the system at run-
time. It is composed of two algorithms. The ﬁrst algorithm
forms a consistent health vector to consistently locate be-
nign faulty senders, the second accumulates the diagnostic
information using the p/r algorithm to distinguish (in a prob-
abilistic manner) between healthy and unhealthy nodes.
Each node i runs, at each round, the diagnostic job diagi,
which sends a non-replicated diagnostic message dmi and
receives all the other interface variables hdm1, . . . , dmN i.
The communication controller provides a validity bit for
each interface variable dmj sent from diagj to diagi using
its local error detection mechanisms. By checking the valid-
ity bits of the diagnostic messages, the protocol diagnoses
communication errors. The local syndrome of node i is the
binary N -tuple containing its local view on the messages
sent by other nodes (faulty/not faulty). The diagnostic mes-
sage dmi contains the local syndrome broadcast by node i
and its size is O(N ).
The diagnostic protocol consists of ﬁve phases:
1) Local detection: Communication errors are locally de-
tected by observing the local validity bits of the diag-
nostic messages. A new local syndrome is formed as a
binary N -tuple.
2) Dissemination: The local syndrome is broadcast using
the diagnostic message dmi.
3) Aggregation: Receive all local syndromes dmj corre-
sponding to the same previous diagnosed round. Form
a diagnostic matrix for that round where row i is the lo-
cal syndrome sent from node i and column j is a vector
representing the opinion on node j of all other nodes.
4) Analysis: A binary N -tuple called consistent health
vector, which contains the consistent distributed view
on the health of all system nodes in the diagnosed
round, is calculated. To combine the local syndromes
sent by different nodes, a hybrid voting [11, 18] over
the columns of the diagnostic matrix is performed. If
enough nodes observed a benign fault, the sending
node is considered faulty.
5) Update counters: Based on the consistent health vec-
tor, update the penalty and reward counters associated
to a node, and possibly isolate faulty nodes.
Diagnostic jobs