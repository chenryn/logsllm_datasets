which is important in the evaluation of the algorithm. The raw perfor-
mances is also important because nowadays most machine learning
solutions are developed to be implemented in the industry.
3.1.1 Computational performances
Computational performances are evaluated just by measuring the
training time on provided datasets. But, as said in 2.4.2, the main
problem with decision trees is that the algorithm is not incremental
and their training requires almost all the data to ﬁt in memory. More-
over, the space needed to store the necessary statistics to the proposed
method is really negligible compared to the size of a dataset. That is
why the space complexity is not measured in the experiments. The or-
der of the modiﬁcations of the algorithm is at the maximum k2 where
k is the number of features. In our experiments we always have :
n (cid:29) k2, n is the number of samples in the dataset.
(3.1)
3.1.2 Evaluation of trained models
The main metrics to evaluate models are the common ones used in
machine learning. In the case of binary classiﬁcation, the three main
metrics we use to evaluate a model are: accuracy, precision and recall.
Precision and recall are mainly used when the two classes are consid-
ered as positive and negative.
Accuracy =
TP + TN
TP + TN + FP + FN
,
Precision =
TP
,
Recall =
TP + FP
TP
,
TP + FN
F1 = 2 × Recall × Precision
Recall + Precision
,
(3.2)
(3.3)
(3.4)
(3.5)
CHAPTER 3. METHODS AND EVALUATION
25
where TP is the number of true positives, TN is the number of true
negatives, FP the number of false positives and FN the number of
false negatives.
The accuracy is preferred when there is equivalence between the two
classes, i.e. when an error in one category is not worse than another.
Precision and recall are often preferred to accuracy because classiﬁca-
tion tasks often deal with unbalanced problems where the idea is to
avoid either the precision or the recall. The F1 score, which is the geo-
metric mean of recall and precision, allows to merge these two criteria
into one. For multiclass problems these measures can be extended.
The idea is to understand which classes are the hardest to separate to
see if there is a possible explanation that is not captured by the model.
In this case, there are two major measurements: the global accuracy
and the confusion matrix. The confusion matrix is deﬁned by:
(cid:18)# Sample of ci classiﬁed in cj
(cid:19)
Con f =
# Total samples
.
(i,j)∈[1,nclass]2
In the ideal situation of a perfect classiﬁer the confusion matrix is di-
agonal, meaning that there is no confusion between several classes.
In case of regression, there are also a lot of possible measure-
ments. The most common one is the mean square error (MSE) which
is the criterion that most machine learning algorithms try to minimize
while dealing with this problem. It is deﬁned as:
MSE =
1
nsamples
nsamples∑
i=1
( ˆyi − yi)2,
where yi is the ground truth of the ith sample and ˆyi its predicted
value. This indicator strongly penalizes large errors, that is why some
other indicators exist. The other one that is used is the mean absolute
error (MAE) which is deﬁned by:
MAE =
1
nsamples
nsamples∑
i=0
| ˆyi − yi|.
This error is the simplest to understand because the error scales lin-
early. So it is also interesting in terms of interpretability, but the linear
property is not necessarily desirable because sometimes an error twice
as big is not twice as bad. Moreover, this error measure is less known
26
CHAPTER 3. METHODS AND EVALUATION
because it has some undesirable mathematical properties like a non-
derivability in zero. There is also a last error which is often used in
business cases. It is the mean absolute percentage error (MAPE). This
error is scale insensitive and easily understandable by somebody who
has no idea of what the units are.
MAPE =
1
nsamples
nsamples∑
i=1
| ˆyi − yi|
yi
But, as the previous one, it is hard to use it in a mathematical frame-
work because it has a lot of bad properties such as predicting values
that are close to zero. The relevance of this metric depends on the
problem.
3.1.3 Compactness
We will use two metrics to evaluate the compactness of a model. The
ﬁrst one is the tree depth. In fact, it can be easily interpreted as the max-
imum number of questions which explain the decision. This value
measures the local explainability of a decision tree, that is to say, how
hard it is to understand any decision made by the algorithm. This
measure could be reﬁned by computing the average depth of a termi-
nal node.
The second measure that is used is the total number of nodes in a
tree. This number represents the total number of different decisions
that can be taken by the model. This measurement can also be related
to explainability. The number of nodes is the number of questions that
is necessary to describe the whole phenomenon. This point could be
questionable because some decisions can be more complex than oth-
ers. For example, with categorical split, grouping different attributes
reduces a lot the number of nodes, but is it really better? In this case
there are two scenarios, on the one hand, the groups are coherent and
easily understandable by a human who has skills in the ﬁeld, so it sim-
pliﬁes the model. On the other hand, if the algorithm produces groups
which seem to have no sense, the gain in the number of nodes cannot
compensate for the complexity brought by these groups. For example,
the categorical split could be on the feature: "which type of animal is
it?". If the best decision given by this method is to put cat, ﬁsh on one
side of the tree and dog, frog on the other side, it can be surprising
for human understanding. Our logic can lead us to think that dog, cat
CHAPTER 3. METHODS AND EVALUATION
27
are more likely to be on the same side because they are mammals and
frog, ﬁsh on the other side because they both live in the water.
These two metrics are the base of our evaluation. To determine if there
is a difference between all those metrics in our evaluations we use sev-
eral statistical tests.
3.1.4 Statistical background for evaluation
Cross-validation
In machine learning the cross validation is a common tool to evaluate
the generalization of a model. The idea is to split the dataset randomly
into k parts. Then we proceed to an evaluation of the key performance
metrics on one of the part using the k − 1 other part to build a model.
All results are then aggregated to create an interval of conﬁdence for
each metrics. For this we assume that for a metric there is a ground
truth value which is equal to µ0 and the results given by each evalua-
tion follow a Gaussian distribution centered in µ0 with a variance of σ.
The interval of conﬁdence at level 0.95 is then given by :
Ic = [µ0 − 1.96σ√
k
, µ0 − 1.96σ√
k
]
In this case this means that the real value has 95% chance to be in
this interval. This interval of conﬁdence is a raw approximation and
the hypothesis that the accuracy is distributed according to a normal
distribution could be debatable and violated in some cases.
Student test
We also use the two sample Student test to evaluate if there is a sig-
niﬁcant difference between two evaluated algorithms. The goal of this
test is to compare the mean of two Gaussian distributions. To proceed,
for two gaussian samples set (ai)i∈[0,n1−1], (bi)i∈[0,n2−1] we compute the
28
CHAPTER 3. METHODS AND EVALUATION
Student score t which is given by :
¯a =
∑n1−1
i=0 ai
n1
∑n2−1
i=0 bi
¯b =
n2
∑n1−1
i=0 (a − ai)2
n1 − 1
∑n1−1
i=0 (b − bi)2
(cid:114) σa
n2 − 1
σb
+
n1
n2
¯a − ¯b
σδ
t =
σδ =
σa =
σb =
According to the theory t follows a Student distribution with d degrees
of freedom where d is equal to:
d =
( σa
n1
( σa
)2
n1
n1−1 +
)2
+ σb
n2
( σa
)2
n2
n2−1
(3.6)
Then to determine if there is a statistical difference at the 0.95 level
between the two observed values we have to check that
|t| > Q0,95,Td
Where Q0,95,Td stands for the 0.95 quantile of a Student distribution
with d degrees of freedom.
Cohen-d test
To measure the effect size in our experiment we used the Cohen-d test.
The goal of this test is to measure the gap between the mean of two
Gaussian distribution. Using the same notations as in the previous
subsection we compute the score given the formula:
dvalue =
(n1−1)σa+(n2−1)σb
¯a − ¯b
n1+n2
(3.7)
These results could be seen as there is dvalue standard deviation be-
tween the estimators of the metrics in the two experiments.
CHAPTER 3. METHODS AND EVALUATION
29
Critical difference diagram
One other statistical tool which is currently used in machine learning is
the critical difference diagram. To produce this kind of diagram all re-
sults on all algorithms are gathered and tested against each other using
a Wilcoxon statistical test. This tool is useful because machine learn-
ing algorithms are often tested against several dataset and metrics are
hard to aggregate because each new generated metric correspond to
a new problem and standard operation like averaging results are not
relevant in this case.
3.2 Experiments framework
3.2.1 Example of use
Explainability is an important quality while dealing with industrial
applications. In the decision process, the user wants to know why this
decision was taken or why sometimes the system broke down. For ex-
ample, due to the rising energy cost, scarcity of resources and the need
to limit environmental impacts, the energy industry has decided to put
the optimization of the performance at the heart of its strategy. That is
why some major French energy companies, have decided to put their
trust in explainable AI to produce models on building consumption
habits. The idea is to detect anomalies faster on the different instal-
lations and to send an alert to the energy manager. For this purpose,
one model is built for each installation, so there are more than 30000
models that are weekly updated which represents 350 million points.
In this case, there is a real need for the manager to have explainable
models that can be updated quickly, to understand what is currently
happening in the network. Moreover is the produced models are com-
plex with thousands of nodes which means that it is hard to have a
quick overview of the system in one glance. Also in the energy sector,
explainable AI is used to predict the consumption habits of French cit-
izens. Data comes from connected electric meters. The aim is to alert
the customer that he may have a problem on its installation. The rules
produced by the decision tree are directly used to generate the alert
message sent to the user. This time again, a model is trained for each
home which represents hundreds of models.
These two examples demonstrate the need of an optimized and re-
30
CHAPTER 3. METHODS AND EVALUATION
liable model which are on top of that compact. They also justify the
use of all metrics introduced in the last section.
3.2.2 Datasets
Algorithms used in the construction of decision trees could be time
consuming, given the different parameters of complexity of a dataset
like the number of features, the number of samples or the number of
unique values for categorical features. In fact, to ﬁnd a new split, the
algorithm has to make an entire pass on the dataset for each feature.
We had to use an signﬁciant amount of datasets both on classiﬁcation
and regression to evaluate if our method gives statistically better re-
sults than the other ones. There is currently a large amount of datasets
available online for experimental uses. Table 3.1 gives some statistics
and descriptions about them. Most of them comes from the University
of Irvine website [26].
Name
Segmentation
Postures
Letter recognition
HTRU
Spam
Sensorless drive
Avila
Wine quality
Bank Marketing
Default of credit card
Mushroom
Facebook comments
Solar ﬂare
#Features
20
18
16
9
57
49
10
12
17
24
22
54
10
Type
Real
Real
Real, Categorical
Real
Real, Categorical
Real
Real
Real
Categorical
Real, Categorical
Categorical
Real, Categorical
Real, Categorical
#Samples
Task
2310
78095
20000
17898
4601
58509
20867
4898
45211
30000
8124
40949
1100
Classiﬁcation
Classiﬁcation
Classiﬁcation
Classiﬁcation
Classiﬁcation
Classiﬁcation
Classiﬁcation
Classiﬁcation
Classiﬁcation
Classiﬁcation
Classiﬁcation
Regression
Regression
Table 3.1: Dataset statistics
The only drawbacks of using this kind of dataset is that we do not
have any control on what we are predicting. That is why we choose to
use the number of nodes to measure the explainability of the produced
model. We know that using this simple metric is incomplete and a full
analysis by an expert should be used to determine whether a set of
CHAPTER 3. METHODS AND EVALUATION
31
rules provided by a decision tree is more explainable than another one.
Moreover, on the data augmentation side it is really hard to represent
graphically what happens for each speciﬁc sampling method because
usual datasets have typically more than 10 features. That is why we
use several generated datasets, each of these presenting a typical learn-
ing problem that we want to address.
Figure 3.1: Generated datasets used for experiments
Using these datasets 3.1 we verify that all algorithms can learn
these concepts:
• Finding a linear separation.
• Finding a non-linear separation.
• Finding a complex non-linear separation.
3.3 New proposed algorithms
This section presents the improvement we tried to make on the deci-
sion tree algorithm on two sides [18]:
32
CHAPTER 3. METHODS AND EVALUATION
• Enhance the categorical split in decision tree to make it usable
for regression problems.
• Improve the accuracy and the precision of single decision trees,
helped by other models and sampling methods.
.
3.3.1 Adapting the categorical split to regression
There were two main goals, ﬁrst, to reduce the complexity of the built
decision tree and, secondly, to handle both categorical and regression
tasks. All methods presented in the state-of-the-art do not handle the
regression [18]. Our idea was to transpose what was done in the clas-
siﬁcation task to the regression task. The chosen solution was to create
some arbitrary categories. To design our new solution, we had to an-
swer several questions:
• How do we divide the target values into several categories?
• How many categories do we have to make?
• Do we have to recompute at each layer these categories? Do they
need to change through the tree?
• Do we have to change the number of categories at each layer?
To address these issues, we tried several methods for the categorical
split.
Quantile based method
Our ﬁrst way of implementing it was to order our samples according
to their target values and then to split them into k different bags. Then,
we used the different bags as categories and tried to use the method-
ology described in 2.2.4. After that the same algorithm as in the clas-
siﬁcation problem is applied. The advantage of this method is that
we compute the category associated to each sample only once which
saves a lot of time. The main issue is that the distribution of the output
values can change a lot while building the decision tree.
CHAPTER 3. METHODS AND EVALUATION
33
Adaptive quantile method
The idea is the same as the previous method, but at each layer we
recompute the quantile according to the subset of the dataset in the
node. This algorithm has the advantage to facilitate the split in deeper
states of the decision tree. Hence, at each layer, it is mandatory to
assign recompute the different classes.
Double adaptive quantile method
One of the drawbacks of the previous method is that, at a certain point,
there are not enough samples to make different categories. We thought
about an idea which is already present in most decision tree libraries:
a minimum sample per bag parameter. Even if it adds a new hyper-
parameter to tune for the model, it should ensure that the classes we