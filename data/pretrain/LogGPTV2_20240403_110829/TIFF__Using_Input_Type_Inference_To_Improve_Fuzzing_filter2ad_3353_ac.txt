chose VUzzer since it is a state-of-the-art evolutionary fuzzer that
implements an already efficient coverage-oriented fuzzing strategy
(and thus a harder-to-improve baseline).
As part of our implementation, we re-engineered libDFT to make
it compatible with 64-bit applications and lifted VUzzer to work
on 64-bit systems. We use VUzzer’s fitness function. However, we
completely reworked VUzzer’s mutation strategy to reflect the type
inference-based techniques proposed in this paper.
As discussed earlier, part of our input type inference system is
based on Howard. To make Howard suit TIFF’s purpose, we modified
it in several ways, for example, by lifting it to work on 64-bit binaries,
by implementing a different data-structure for taintmap that scales
well on larger inputs etc.
Finally, in our implementation, we observed that Howard’s array
detection takes a very long time for some large and complex applica-
tions. To achieve faster input generation, we therefore run the array
detection only for the seed inputs.
Crash Triage: For comparison purposes, to identify the unique-
ness of crashes we use the stack hash technique, described in [36].
Using Pintool [34], TIFF monitors a short execution history upto
510TIFF: Using Input Type Inference To Improve Fuzzing
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
the crash point to compute the stack hash. It keeps track of the last
10 executed basic blocks and the last 5 executed function calls in
a ring buffer before the crash point and then the hash of the buffer
is calculated to determine the uniqueness of the crash. The idea of
considering only a short sequence of basic blocks before the crash
happens is inspired by the observation made by Arulraj et al. [8]
that the “short-term memory” of an execution is sufficient for failure
diagnosis.
7 EVALUATION
In this section, we evaluate TIFF on several applications. We present
results for applications fuzzed for 12 hours2.
For each application, we gather 3-4 random, but valid inputs
and we use this as a seed set of inputs for each fuzzer considered
in our evaluation. To compare the performance of TIFF against
the state of the art, we also present experimental results for (64-
bit) VUzzer [40] and AFLFast [10]. We consider the performance
in terms of speed (how many unique crashes—a proxy metric for
bugs—detected in how much time?). For most of these proxy metric
across different applications (that we chose to evaluate TIFF with),
we show the overall performance of TIFF by computing geometric
means across all the runs. As the values of these proxy-metrics are
skewed, arithmetic mean may not be a good candidate to access the
central tendency [37].
For our evaluation, we consider two datasets, drawing from recent
work in the area [10, 40]. First, we use a set of buggy binaries recently
generated by the LAVA team [18], specifically the LAVA-M dataset.
Second, we consider miscellaneous real-world applications which
process binary input data, such as image processing applications
(see Section 7.2). We refer to this miscellaneous application dataset
as the MA dataset.
infinite loops bugs,
for example,
Apart from the above mentioned datasets, we separately ran
TIFF on the latest version of two applications- libming-0.4.8
and libexiv2 0.27- and found new bugs. In libexiv2,
we found couple of
in
Exiv2::Image::printIFDStructure().
function
We also found few assertion failure errors,
for example,
in
In
libming, we found a access violation in the function
parseABC_NS_SET_INFO,
segmentation
fault. These issues are reported to the respective vendors.
function Exiv2:RafImage::readMetadata().
resulting
in
a
We ran all our experiments on an Ubuntu 14.04 LTS system
equipped with a 64-bit 2-core Intel CPU and 16 GB RAM. Following
the recommendations, made by Klees et al. [29], we repeated all our
experiments 3 times and report the average, with marginal statistical
variations observed across repeated fuzzing runs.
7.1 LAVA-M Dataset
In a recent paper, Dolan-Gavitt et.al. [18] developed a technique to
inject hard-to-reach software faults and created buggy versions of
a few Linux utilities for testing fuzzing- and symbolic execution-
based bug finding solutions. This dataset consists of 4 Linux utilities
1.5
input#2200
0
2
6
4
8
md5sum [hr]
14.1
10
12
input#7794
57
42
36
27
18
9
0
350
280
210
140
70
0
0
4
8
12
16
who [hr]
20
24
40
32
24
16
8
0
0
28
24
21
15
12
9
6
3
0
0.9
input#1200
10
12
2
6
4
8
base64 [hr]
50
input#600
0 5 15 25 35 45 55 65 75
uniq [min]
Figure 3: Distribution of crashes for applications over their run time
period. X-axis: time. Y-axis: cumulative sum of unique bugs. Blue line:
TIFF. Red dashed line: VUzzer. Vertical black line: Time taken by TIFF
to find the same number of crashes as those found by VUzzer during a
complete run.
base64, who3, uniq, and md5sum. Each of these binaries is
injected with multiple faults (in the same binary for each utility). We
use this dataset to compare TIFF’s performance to that of VUzzer,
which has shown good results on the LAVA-M dataset [40]. We also
ran AFLFast on this dataset, but AFLFast could not find any bug in
the binaries included in the LAVA-M dataset, except for md5sum.
In the latter case, it reported 2 crashes, which did not match any of
the injected faults4. We show how TIFF’s type-assisted mutation can
greatly increase bug coverage on the LAVA-M dataset.
Table 1 presents our results. Each injected fault in the LAVA
binaries has a fault ID that is printed on standard output before the
binary crashes due to that fault. This allows us to precisely identify
the unique bugs triggered by TIFF from the crash runs.
As shown in Table 1, TIFF found more bugs with fewer inputs
compared to VUzzer. Moreover, Figure 3 illustrates the distribution
of crashes on the LAVA-M binaries over their running period. The
y-axis of each plot shows the cumulative sum of crashes and the
x-axis of the plot shows the total execution time of the fuzzers. As
shown in the figure, not only does TIFF find more bugs than VUzzer,
but also finds them sooner.
As TIFF performs mutation in a controlled manner (i.e., the mu-
tation of control offsets and the mutation of data offsets are done in
separate cycles), we could also measure the effect of these mutation
strategies on TIFF’s behavior. We observed that several of LAVA-M
fault IDs (each bug has a unique ID in LAVA-M dataset) that TIFF
2As we compare TIFF with VUzzer, instead of running each application for 24hrs, as
done in VUzzer paper [40], we ran each experiment for 12hrs. In this way, we want to
show that TIFF is effective in finding bugs in considerably less amount of time.
3Since [who] has a large number of bugs, which are difficult to detect in 12 hours, we
ran the fuzzers longer (24 hours).
4We ran AFLFast with default configurations
511ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Vivek Jain, Sanjay Rawat, Cristiano Giuffrida, and Herbert Bos
Table 1: LAVA-M dataset: TIFF vs. VUzzer. Column 3, 4 and 5 show
data as #unique bugs (total inputs). The numbers in brackets show the
number of inputs required to generate the unique bugs.
Program
Total bugs
TIFF
uniq
base64
md5sum
who
28
44
57
2136
28 (700)
39 (15.4k)
36 (10.5k)
284 (11k)
VUzzer
28(1400)
23 (21.6k)
23 (13k)
235 (12k)
geo mean
-
57.8 (5.9K)
43.2 (8.2K)
AFLFast
0 (783k)
0 (14M)
2(495k)
0(19M)
0.31
(3.1M)
found using mutation of only data offset types, were not found by
VUzzer.
7.2 MA Dataset
We now consider our MA dataset of real-world programs to evaluate
the performance of TIFF. This dataset consists of the following uti-
lities: jbig2dec (0.11), potrace (1.11-2), pdf2svg (0.2.2-1), gif2png
(2.5.8-1), mpg321 (0.3.2-1.1), tcptrace (6.6.7-4.1), tcpdump (4.5.0),
djpeg (1.3.0), autotrace (0.31.1-16), pdftocairo (1.13.0) and convert
(8:6.7.7.10). We selected these applications by considering expe-
rimental results reported in [10, 21, 40]. We did not consider any
binutils utilities as TIFF is not suitable for applications that
involve heavy parsing. For each of these programs, we use their
vanilla release for Ubuntu 14.04. By evaluating these utilities, we
also targeted some well-known libraries that are used by these uti-
lities, such as libpotrace (1.11-2), libjbig2dec (0.115), libpoppler
(0.24.5-2), libpng (1.2.50-1), libasound (1.0.27.2-3), libpcap (1.5.3-
2), libautotrace (0.31.1-16), libcairo (1.13.0), libjpeg-turbo (1.3.0)
and libMagickCore (8:6.7.7.10)5.
In order to gather insights into the performance of TIFF, we also
ran VUzzer and AFLFast on these applications. For our performance
comparison, we chose AFLFast over AFL, as AFLFast has been
shown to perform better than AFL [10], but both these fuzzers follow
a similar coverage-oriented fuzzing strategy. Table 2 presents our
results on the MA dataset.
Since both VUzzer and AFLFast employ a different technique for
determining crash uniqueness, in order to have a meaningful com-
parison of (unique) crashes reported by TIFF, we run the same call
stack-hashing based algorithm on the crashes reported by VUzzer
and AFLFast for each application. This simple algorithm provides a
common uniqueness metric for crash reporting across all the fuzzers.
As shown in Table 2, TIFF again triggers more crashes than VUz-
zer with generally fewer inputs. This confirms our observation about
the type-agnostic mutation performed by VUzzer: since VUzzer
does not know the type of the offsets, it is unable to meaningfully
mutate input bytes at those offsets. The delta with AFLFast is more
pronounced, as TIFF was able to produce many more crashes using
an order of magnitude fewer inputs.
This experiment on real-world applications particularly indicates
towards an important property to reason over fuzzing effectiveness.
Fuzzers such as AFLFast (and AFL) rely on a very lightweight
instrumentation, which allows them to process many more inputs
5It should be noted that several of these applications are also used in original VUzzer
paper and we added more applications for the current experimentation.
(millions) than the inputs process by TIFF and VUzzer’s complex
binary instrumentation for a given testing time interval (12 hours).
However, the higher-quality input mutation strategy produced by
such complex instrumentation more than compensates for the slower
input processing time, ultimately resulting in more crashes being
detected. Even when comparing TIFF and VUzzer, our results show
that TIFF’s more complex instrumentation does generally result in
less inputs processed per time unit, but this still translated to a more
effective fuzzing strategy.
We also evaluated TIFF’s effectiveness from a code-coverage
perspective (another common but less precise proxy metric for bug
detection effectiveness). Table 3 presents our results for TIFF and
VUzzer. Note that since we fuzz application binaries, it is not trivial
to simply express code coverage in terms of percentage of the total
code. For this reason, we measured the number of new basic blocks
covered by the fuzzers over the number of basic blocks that were
executed by the fuzzers with seed inputs. Column 2 of Table 3
lists the number of initial (seed input) basic blocks, while the other
columns present the number of basic blocks discovered by each
fuzzer during the entire testing time interval (12hrs). This number
includes only the main application basic blocks and the basic blocks
of the libraries that we target. The numbers inside the brackets
in columns 3 and 4 indicate the total number of inputs that were
executed by each fuzzer. In the case of TIFF, this number also
includes inputs generated using data-offsets based mutation (with
no monitoring of executed basic-blocks).
As shown in Table 3, for most applications, TIFF is able to disco-
ver more basic blocks than VUzzer with fewer inputs. This confirms
our assumption that type-consistent mutation of the (now known)
type of cmp offsets leads to a faster discovery of newer basic blocks.
For mpg321 and gif2png, our results only reveal a small (or no)
difference between the number of basic blocks covered by TIFF
and VUzzer. A closer inspection revealed that, in these cases, the
number of inputs generated during the data-offset mutation phase
did not produce any crashes and such inputs are not monitored for
new basic blocks, but TIFF spent a lot of time on such inputs. This
behavior prompted us to further explore the issue of missing code
coverage. We ran 3 applications (gif2png, mpg321 and autotrace)
by enabling the monitoring for bug oriented cycle. We find that we
missed 0% (resp. 23%, 26%) basic blocks for mpg321 (resp.gif2png
and autotrace).It is obvious that we need a way to capture such new
basic blocks, with less execution penalty. Therefore, in the Table 3,
we can observe that for certain number of applications, TIFF is not
significantly better than VUzzer.
7.3 Crash Analysis
To identify the severity of crashes (and resulting bugs), we ex-
amined the crashes discovered in the various applications using
!Exploitable [20]—a tool developed on top of GDB that classi-
fies bugs by severity and recently ported to crash processing utilities
for AFL [41]. !Exploitable uses heuristics to assess the exploi-
tability of a crash inside a given application. While by no means a
perfect assessment, an indication of exploitability indicates that a
bug is serious.
We find that TIFF could trigger exploitable bugs in several appli-
cations from MA dataset.
512TIFF: Using Input Type Inference To Improve Fuzzing
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Table 2: MA dataset: TIFF vs. VUzzer vs AFLFast
Application
mpg321+libasound
pdf2svg+libpoppler
jbig2dec+libjbig2dec
potrace+libpotrace
gif2png+libpng
tcptrace+libpcap
autotrace+libautotrace
pdftocairo+libcairo
convert(gif)+libGraphicsMagick
geo mean
TIFF
#Unique crashes
#Inputs
VUzzer
#Unique crashes
3.33
1.66
32
9.33
6
3
11
2
1