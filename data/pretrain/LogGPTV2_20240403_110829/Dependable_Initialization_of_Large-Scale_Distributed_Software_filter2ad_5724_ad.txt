LocalInitialize  task  is  halted,  the SetupCommunication
task  is  executed,  and  the  LocalInitialize  task  is  re-
executed. The SetupConfiguration task can be skipped.
3.2.  Experimental Results
to 
Using  the  example  application described  in  the
previous  subsection, we  performed  an  experimental
study 
evaluate  our  dependable  distributed
initialization algorithm.  In the study, a coordinator was 
implemented  to  construct
the  interdependency  and
recovery graphs  and  to  coordinate  initialization  tasks 
and  recovery  activities  among  components. In  our 
experiment,  the  total  number  of  components  ranged
from  100  to 900  and  the number  of  processes  ranged
from 10 to 50.  Each task associated with an application
component  has  a  randomly generated  execution  time
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:18 UTC from IEEE Xplore.  Restrictions apply. 
the degree of 
uniformly distributed between one and ten seconds. A
tuning  variable  in  our  experimentation  allows  us  to
control 
interdependency  between
components.    All  dependencies  between  components
were  specified  to be operational  dependencies. One
third  of  these  dependencies  were  assigned between
LocalInitialize  tasks  of  different  components,  another
third  between SetupConfiguration tasks  of  different
components, and the remaining third have both of these
dependencies between pairs of components.
In 
the 
study,  our  dependable 
initialization
algorithm  is  compared with an  initialization  algorithm
that  takes  immediate  recovery  actions  once  a  failure  is 
detected and does not have the ability to skip tasks that
do not  need  to  be  re-executed. We refer  to  the  latter
algorithm as a “conventional initialization algorithm”.
Rather  than  compare  absolute  initialization  times
for both  algorithms,  we  compare  the initialization
overhead.  The results appear in Figure 8. Initialization
overhead  represents  the  fractional  increase  in  system
initialization  time  needed  to  handle  failures during
initialization as compared to the initialization time that
would  be  achieved  if no  failures occurred.
  It  is
calculated as: (Tfailure – Tfailure-free)/Tfailure-free with T being
the  measured system  initialization  time.  For  simplicity
in discussing  the results of  Figure  8,  we  triggered a
single  component  failure  at  the  same  point  in  time  in
the  initialization procedure  for both  our  algorithm  and
the  conventional  initialization  algorithm.  We  assigned
5%  of  the  total  number  of system  components  to  the
component  that  we  failed  and randomly  assigned  the
rest  of  the  components  to  the remaining processes. In
Figure 8(a), there are 300 components in total, with 15
of  these  components  running in  the  process  associated 
with  the  component  that  fails.  The percentage of
components with dependencies on tasks associated with
the  failed  component ranges  between 10%  and 90%,
with  roughly  70%  of  the  cases  resulting  in  deferred
recovery.    The  results  of  Figure  8(a)  show  that
initialization  overhead  increases for  both  algorithms  as
the  number  of  failure-free  components  affected  by  the
component  failure  increases.   This  is  due  to  additional
synchronization  and  coordination needed among  the
tasks  associated  with  the  failed  component  as  well  as 
time  spent  re-executing  necessary  tasks  in  response  to
recovery from the component failure.
As 
dependable 
expected,  our
initialization
algorithm has lower initialization overhead than that of
the  conventional  initialization  algorithm.  For  example,
in 
the  case  where  40%  of  failure-free  system
components  have  dependencies  associated  with  the
failed  component, overhead 
the dependable
initialization  algorithm  is  12%,  while  the  overhead  for 
the conventional initialization algorithm is 32%. In this
for 
Dependable Initialization
Conventional Initialization
0
10% 20% 30% 40% 50% 60% 70% 80% 90%
Percent of Components Depending on the Faulty Process
(Total Number of Components = 300)
(a)
Dependable Initialization
Conventional Initialization
)
%
(
d
a
e
h
r
e
v
O
n
o
i
t
a
z
i
l
a
i
t
i
n
I
l
a
n
o
i
t
c
a
r
F
80%
70%
60%
50%
40%
30%
20%
10%
0%
40%
35%
30%
25%
20%
15%
10%
5%
0%
)
%
(
d
a
e
h
r
e
v
O
n
o
i
t
a
z
i
l
a
i
t
i
n
I
l
a
n
o
i
t
c
a
r
F
100
200
300
400
500
600
700
800
900
Number of Components
(30% of Components Depending on the Faulty Process)
(b)
Figure 8: Initialization overhead comparison 
study,  the overhead  of  the  dependable  initialization
algorithm is about one third of that of the conventional
initialization algorithm. Note also that the initialization
overhead of our algorithm increases much more slowly
than  that of  the  conventional  initialization  algorithm.
For  applications  with  a  high degree  of  component
interdependencies,  the  ability  to  avoid re-executing
tasks unless required helps to flatten the overhead. The
conventional algorithm  without  this  ability  performs
rather poorly  when 
the  components  are  highly
interdependent.
Figure  8(b)  shows  initialization  overhead  versus
the number of components in the system. In the figure,
the  total  number of  components  ranges  from  100  to
900,  a  single  component  fails  at  the  same  time  during
initialization  for  both  initialization  schemes,  and 30%
of  the  failure-free  components  have dependencies on
tasks  associated  with 
the  failed  component.  As 
expected, as the number of components increase, so do
the  initialization  overheads for both  algorithms.  Two
factors contribute to this increase. One is the increasing 
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:18 UTC from IEEE Xplore.  Restrictions apply. 
number of  initialization  tasks  that  depend  on  tasks  re-
executed  as  part  of  recovery.  The  other  factor  is  the
additional  overhead needed 
to  synchronize  and
coordinate  among  re-executed  tasks  and  remaining
initialization 
initialization
algorithm 
the 
conventional initialization algorithm. For example, for a 
system  comprised of  500  components,  our  dependable
initialization algorithm incurs 13% overhead, compared
to a 29% overhead for the conventional algorithm.
tasks. The  dependable
incurs only half 
the  overhead  of 
4.  Assumptions Revisited
Many  simplifying  assumptions were  made  in  this
paper  in  an  attempt  to  present  the  novel  aspects  of  the
work  as  cleanly  as  possible.    In  practice,  several  of
these  assumptions  are not very  realistic,  especially 
when  considering  large-scale distributed  systems.    Our
work relies  on  more  practical  assumptions 
than
discussed herein. Three areas of the work are impacted:
the  fault model,  dependency  types  and  other  events
affecting 
initialization
coordinator.
initialization,
and 
the 
4.1.  Fault Model
The  “fail  silent”  and  “hang” model  for  processor,
process,  and  component  failures  seems  to  hold  quite
well in practice for applications such as call processing
systems,  as  bad  inputs received  by  a  component  are
silently  discarded.    Failures  that  lead  to network
partitioning must  be  considered  for  geographically
distributed applications, though this is less of a problem
for  cluster-based  solutions,  as  they  tend  to  have
redundant network  connectivity.    Our  work  to  date
focuses on cluster-based systems.
4.2. Dependency Types and Other Events 
Affecting Initialization
The  sequential  and  operational  dependency  types
presented  in  this  paper  are  only  two of  a  vast  array  of
types of dependencies important to initialization.  A key
type of  dependency  not  discussed in the  paper is  one
that specifies a relationship between active and standby
components5.  We refer to such a dependency as a fail-
over dependency, defined as follows.
A fail-over  dependency of  task Tj  on task Ti
specifies  that  Tj  can  only  execute  after Ti  has  failed 
(effectively,  the  component  associated  with  Tj  is 
promoted).  This  type  of dependency  is represented  in
the interdependency graph by an arc from task Ti to task
5 As indicated early in the paper, in our model the
component is the atomic unit of initialization and 
failure recovery.
Tj labeled with an 'F'. Task Tj will not be executed until
the  component  associated  with Ti  fails.  The  recovery
graph  that  is  created  as  a  result  of  the  failure  of  the
component  associated  with Ti will  include  task Tj  and 
all of its associated dependencies. It will also include a 
new  standby  component  with 
same  set  of
dependencies as existed between the original active and
standby components.
the
in 
the paper 
Other  dependency  types  that we  support  that  were
include non-essential
not discussed 
dependencies (where  a  component  is not required for
initialization to complete - the challenge is to determine
what to do when the non-essential component becomes
available  partway  through  the  initialization procedure),
dependencies  on  resources  not  managed  by 
the
initialization  coordinator,  dependencies  related  to  fault
escalation  (e.g.,  recovery  attempts  during  initialization
failed,  so  more  drastic  recovery  attempts  are  needed),
etc. 
impact
initialization, such  as  a  processor  becoming  available
during initialization.
  Also,  events  other 
than failures 
4.3.  Initialization Coordinator
We have  not discussed  the  implementation  of  the
initialization  coordinator,  except  to  assume  it  is  a
centralized entity.  In fact, no mention was made in the
paper  regarding  what happens  if  the  coordinator  fails
during  initialization.    Our work  specifically  deals  with 
  A  failed  coordinator  is 
failures of  the  coordinator.
restarted. 
the
initialization states of all entities in the system in order
to  efficiently  complete  the  initialization  procedure.
Ongoing work is examining distributed coordination.
then  efficiently 
rediscovers 
It 
5. Related Work
rollback 
recovery 
recovery  protocols 
There  is  a  spectrum  of research  work  on rollback
recovery  for  distributed  applications.  This  work falls
into two broad categories: checkpointing protocols and
log-based 
[3].  Checkpointing
protocols require periodic checkpoints to be taken, with
varying  degrees  of  coordination [1,2].  Log-based
rollback  recovery  protocols combine checkpointing
with  logging  of  non-deterministic  events  [8,11]. Work
on 
includes  Chandy-Lamport’s
distributed snapshot  protocol [2],  Wang-Fuchs’s  smart
message  scheduling  techniques  [12]  and  Elnozahy-
Zwaenpoel’s Manetho [4], etc.
Failure recovery  during 
initialization  can be
accomplished using  rollback  recovery  –  the  failure  of
one component may cause other dependent components
to  re-execute  initialization  tasks  that have  already
completed  (hence,  the  execution  of  initialization  tasks
appears  to rollback). However,  special  characteristics 
improve
of 
initialization  can  be 
leveraged
to 
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:18 UTC from IEEE Xplore.  Restrictions apply. 
initialization  performance  in  the  presence  of  failures 
over  what  can  be  achieved  using  rollback  recovery.  
Specifically,  portions  of  initialization  state  information 
are  derived  from  hard-coded  or  persistent  information.  
This  state  information  can  easily  be  recreated  and  thus 
does  not  require  checkpointing.    In  addition,  in  some 
initialization  tasks  such  as  setting  up  communication 
channels, re-executing the tasks results in different local 
state information being generated.   Hence, there is not 
much value in preserving this state information (e.g., by 
checkpointing).    Also,  we  have  observed  that  some 
initialization tasks associated with a component need to 
be  executed  only  once  during  the  lifetime  of  that 
component,  regardless  of  failures  of  other  system 
entities.    As  such,  these  tasks  can  be  skipped  during 
recovery procedures. 
the  same  processor  on  which 
There  are  research  efforts  on  task  scheduling  and 
recovery  that  rely  on  the  use  of  dependency  graphs 
[6,7,9].  For  example,  Isovic  and  Fohler  derived  the 
minimum  processor  utilization  by  re-executing  a  task 
on 
it  failed  [7].  
Kandsamy,  Hayes  and  Murray  handle  intermittent 
faults  by  constructing  a  fault-tolerant  schedule  with 
sufficient slack to accommodate recovery [9]. However, 
there  is  no  notion  of  re-executing  tasks  that  have 
successfully  completed  and  limited  notion  of  re-
executing tasks on a specific processor. 
6.  Conclusion 
in 
result 
recovery  overhead  and 
Handling  failures  during  initialization  of  large-
scale distributed systems adds complexity into both the 
initialization and failure recovery process. In this paper, 
we  presented  a  dependable  initialization  model  that 
captures the architecture of the system to be initialized 
along  with  task  interdependencies  among  components. 
Our model enables appropriate initialization tasks to be 
skipped  during  failure  recovery.    This  can  greatly 
reduce 
faster 
initialization.  We  showed  that  initialization  would 
complete  more  quickly  in  some  cases  if  recovery 
actions  were  deferred  rather  than  started  immediately 
after a failure is detected. A recovery decision function 
was  introduced  that  dynamically  assesses,  based  on 
current 
initialization  conditions,  whether  or  not 
recovery actions are taken immediately or deferred. We 
then described a dependable initialization algorithm that 
combines  our  dependable  initialization  model  with  the 
recovery decision function.  Experimental results show 
that our algorithm initializes a system in much less time 
than  that  of  a  conventional  initialization  algorithm 
when  failures  occur  during  initialization.  This  work  is 
the  first  effort  we  are  aware  of  that  studies  the 
challenges  of  initializing  a  distributed  system  in  the 
presence of failures. 
7.  Acknowledgements 
We  wish  to  thank  the  reviewers  for  their  helpful 
comments, with special thanks to Keith Marzullo, Mike 
Artamanov,  and  Tim  Pevzner  for  their  questions  and 
insightful  comments.    The  valuable  input  received  has 
helped greatly improve the paper. 
8.  References 
[1]  L.  Alvisi,  E.  N.  Elnozahy,  S.  Rao,  S.  A.  Husain  and  A. 
Del  Mel,  “An  Analysis  of  Communication-induced 
Checkpointing,” Proc. of 29th Intl. Symposium on Fault-
Tolerant Computing, pp. 242-249, Jun. 1999.. 
[2]  K.  M.  Chandy  and  L.  Lamport,  “Distributed  Snapshots: 
Determining  Global  States  of  Distributed  Systems,” 
ACM  Transactions  on  Computer  Systems,  vol.  3,  no.  1, 
pp. 63-75. 1985. 
[3]  E.  N.  Elnozahy,  L.  Alvisi,  Y.  M.  Wang,  and  D.  B. 
Johnson,  “A  Survey  of  Rollback-Recovery  Protocols  in 
Message-Passing Systems,” Technical Report CMU-CS-
99-148,  Department  of  Computer  Science,  Carnegie 
Mellon University, June 1999. 
[4]  E.  N.  Elnozahy  and  W.  Zwaenepoel,  “On  the  Use  and 
Implementation  of  Message  Logging,”  Proc.  of  the  24th
Intl.  Symposium  on  Fault-Tolerant  Computing, pp.  298-
307, 1994. 
[5]  I.  Foster  and  C.  Kesselman,  “The  Globus  Project:  A 
Status  Report,”  Proc.  of  the  Heterogeneous  Computing 
Workshop, pp. 4-18, 1998. 
[6]  S.  Ghosh,  R.  Melhem,  D.  Mosse,  and  J.  S.  Sarma, 
“Fault-Tolerant  Rate-Monotonic  Schedling,”  Journal  of 
Real-Time Systems, vol. 5, no. 2, pp. 120-129, 1998. 
[7]  D.  Isovic  and  G.  Fohler,  “Efficient  Scheduling  of 
Sporadic,  Aperiodic,  and  Periodic  Tasks  with  Complex 
Systems 
IEEE 
Constraints,”
Symposiums, pp. 207-216, 2000. 
Real-Time 
Proc. 
[8]  D.  B.  Johnson.  “Distributed  System  Fault  Tolerance 
Using  Message  Logging  and  Checkpointing,”  Ph.  D. 
Thesis, Rice University, Dec. 1989. 
[9]  N.  Kandasamy,  J.  Hayes  and  B.  Murray,  “Transparent 
Recovery  from  Intermittent  Faults  in  Time-Triggered 
Distributed Systems,” IEEE Transactions on Computers,
vol. 52, no. 2, pp. 113-125, 2003. 
[10] M.  Litzknow,  M.  Livny,  and  M.  Mutka,  “Condor  –  A 
Hunter of Idle Workstations,” Proc. of 8th Intl. Conf. On 
Distributed Computing Systems, pp. 104-111, 1988. 
[11] S. Rao, L. Alvisi and H. M. Vin. “The Cost of Recovery 
in  Message  Logging  Protocols,”  Proc.  of  17th  IEEE 
Symposium on Reliable Distributed Systems (SRDS), pp. 
10-18, 1998.
[12] Y.  M.  Wang  and  W.  K.  Fuchs,  “Scheduling  Message 
Processing for Reducing Rollback Propagation,” Proc. of 
IEEE  Fault-Tolerance  Computing  Symposium,  pp.  204-
211, 1992.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:18 UTC from IEEE Xplore.  Restrictions apply.