the adversary must also declare the expected numerical
range of its outputs, which amounts (since its input is
a single record of the database) to stating its sensitiv-
ity. If the actual output ever falls outside of the declared
range, it is clipped—in essence, the declared sensitivity
is enforced by the system. From the declared sensitivity,
Airavat can calculate how much noise must be added to
the reducer’s results to achieve ε-differential privacy.
In PINQ, macroqueries are written in LINQ, a SQL-
like declarative language, which can be embedded in oth-
erwise unconstrained C# programs. Microqueries can
be general C# computations (optionally constrained by
a checker method called Purify; see Section 3.5).
3 Attacks on differential privacy
Naturally, database administrators may be nervous about
offering adversaries the opportunity to run arbitrary
queries against their raw data. They will need strong
assurances that such adversarial queries not only play
by the rules of differential privacy but also have no in-
direct means of improperly leaking private information
about individuals in the database. Unfortunately, this is
not currently the case: while the authors of both PINQ
and Airavat have anticipated the possibility of covert-
channel attacks and have implemented either a partial
defense (Airavat) or hooks for adding one (PINQ), both
systems remain vulnerable to a range of attacks, as we
now demonstrate.
3.1 Threat model
It is well known that covert channels are essentially
impossible to eliminate if we allow the adversary to
run other processes on the same computer that runs the
query. Even if these other processes have no access to
the database and cannot communicate directly with the
query process, there are just too many ways for the query
process to perturb local conditions in ways that can be
measured fairly accurately if the observer is this close—
e.g., processor usage, disk activity, cache pollution, etc.
However, if we assume that the adversary is on the other
end of a network connection, we have a much better
chance of success. This is fortunate, since the demands
of the situation are very strong. It is not enough to limit
leakage to a low bandwidth or a small number of bits:
even one bit is too much if that bit is the answer to Does
John Doe watch adult movies?
We therefore assume that the database and associated
query system are hosted on a private, secure machine.
The adversary does not have physical access to this ma-
chine or its immediate environment (so that there is no
way to measure its power usage, etc.) and can only com-
municate with it over a network. The adversary submits
arbitrary queries to the system over the network. The
system executes each query (if it determines that doing
so is safe) and returns the answer over the network. The
system also maintains a privacy budget for the database
as a whole, and it refuses to answer any more queries
once the budget is exhausted.
This threat model is shared by all differentially private
query systems (PINQ, Airavat, and our Fuzz system),
and its assumptions seem reasonable in practice. Essen-
tially, it gives the adversary three pieces of information:
(1) the actual answer to their query (a number, histogram,
etc.), if any, (2) the time that the response arrives on their
end of the network connection, and (3) the system’s deci-
sion whether to execute their query or refuse because do-
ing so would exceed the available privacy budget. How-
ever, this threat model still provides plenty of room for
attacks on privacy. We will see that, unless appropriate
steps are taken, both the decision whether or not to ex-
ecute a query and the execution time itself can be used
as channels to leak private information. In essence, both
the query’s ﬁnishing time and the fact that it is accepted
4
noisy sum, foreach r in db, of {
if embarrassing(r)
then { pause for 1 second };
return 0
}
Figure 1: Timing attack example
or refused are results that the system is giving back to
the adversary, and we need to consider whether the com-
bination of all results—not just the query’s numerical
answer—is differentially private. Moreover, we will see,
for PINQ, some ways that a malicious query may cause
the actual answer to not be differentially private.
3.2 Timing attacks
Under the constraints of the above threat model, the eas-
iest way for a query to send a bit to the adversary is by
simply pausing for a long time (by entering an inﬁnite
loop, computing factorial of a million, etc.) when a cer-
tain condition is detected in the private data, as illustrated
(in PINQ-like pseudocode) in Figure 1. The macroquery
adds together the results of running the microquery on
each row of the database (always 0) and ﬁnally adds
some random noise to the total. Since almost all of the
microquery instances ﬁnish very quickly, the distribution
of query execution times observed by the adversary will
change signiﬁcantly when an embarrassing record exists
in the database—a violation of differential privacy.
A simple “microquery timeout” will not solve this
problem, for at least two reasons. First, the adversary
can also signal the condition by causing the query to take
an unusually small amount of time. The simple way to
do this is to create an exception condition that aborts the
entire query. If this is blocked (e.g., by trapping an ex-
ception in a microquery and replacing it with a default
result just for that single microquery), the adversary can
instead make all microqueries take a uniformly longish
time (say, exactly two milliseconds) except when they
detect the condition, in which case they terminate im-
mediately.
If the adversary happens to know exactly
how many records are in the database, this leaks one bit.
Second, the adversary can defeat a simple “microquery
timeout” by causing side-effects in the microquery that
will slow down the macroquery or other microqueries—
for example, by allocating lots of memory to trigger
garbage collection in the macroquery. We discuss this
issue in more detail below.
3.3 State attacks
A different class of attacks involves using a channel be-
tween microqueries, such as a global variable, to break
differential privacy of the result, as illustrated in Figure 2.
found = false;
noisy sum, foreach r in db, of {
if (found) then { return 1 }
if embarrassing(r) then {
found = true;
return 1
} else { return 0 }
}
Figure 2: State attack example
noisy sum, foreach r in db, of {
if embarrassing(r) then {
run sub-query that uses
a lot of privacy budget
} else {
return 0
}
}
Figure 3: Privacy budget attack example
This time, the result of each microquery is either 0 or 1,
depending on whether any previous microquery detected
an embarrassing record. Since, in general, the embar-
rassing record will not be the last one in the database,
this greatly magniﬁes the contribution of this one record
to the result, again violating differential privacy.
3.4 Privacy budget attack
A related form of attack uses the query processor’s deci-
sion whether to publicize the result of a query as a chan-
nel for leaking private data, relying on the fact that this
decision can be inﬂuenced by actions of the query that in
turn depend on private data. This idea can be applied to
systems that use a dynamic analysis to determine the ’pri-
vacy cost’ of a query, i.e., the amount that must be sub-
tracted from the privacy budget before the result can be
returned to the querier. As illustrated in Figure 3, the at-
tack consists of looking for an embarrassing record and,
when it is found, invoking some sub-query that will use
up a bit of the remaining privacy budget. Once the outer
query returns, the adversary simply checks how much the
privacy budget has decreased.
3.5 Case study: PINQ
We have veriﬁed that the current PINQ implementation
(version 0.1.1, released 08/18/09, available from [23]) is
vulnerable to all of the above attacks. To demonstrate the
vulnerabilities, we have written three example programs,
each based on the test harness that comes with PINQ.
The original test harness computes several differen-
tially private statistics on a given text ﬁle, including the
5
Constant execution time
Variable execution time
Database size public
ε-differential privacy
Static enforcement
Exact timing analysis
Dynamic enforcement
Timeouts
Rounding up
Database size private
(ε,δ)-differential privacy
Time bound analysis
Time noise
Timeouts
Time noise
Table 1: Four approaches to the timing-channel problem.
number of lines that contain a semicolon. When the
program starts, it ﬁrst reads the text ﬁle and creates a
database whose rows each contain one line of text. Then
it selects all the rows that contain a semicolon, using mi-
croqueries with a boolean predicate p, and ﬁnally per-
forms a noisy count on the resulting set of rows.
Our attacks are implemented by changing the predi-
cate p so that it produces some observable side-effects
when the input ﬁle contains a certain string s. For the
timing attack, we changed p so that, when invoked on a
line that contains s, p performs an expensive computa-
tion that takes several seconds and cannot be optimized
out. For the state attack, we added a static variable that is
incremented by p when it discovers s, and we write the
(un-noised) value of this variable to the console at the
end. For the budget attack, we added a different static
variable that contains a reference to the database; when s
is found, p computes a noisy count of the number of rows
in the database, which decreases the privacy budget.
The possibility of such attacks is acknowledged in the
PINQ paper [20], and the PINQ implementation does
contain hooks for an expression rewriter (called Purify
in [20]) that is invoked on all user-supplied expressions
and could potentially change or remove code that causes
side-effects. However, such a rewriter is not provided;
indeed, the PINQ downloads page contains an explicit
warning that the code is not hardened or secured and
should not be used ‘in the wild.’
We conjecture that implementing a reliable Purify
will be far from trivial. Avoiding the privacy budget at-
tack will probably be easiest: every function that might
consume privacy budget could be wrapped with a check
that raises an exception if it is called from inside a run-
ning microquery (i.e., with a PINQ operation already on
the call stack); this exception could then be turned into a
default result for the microquery. State attacks are more
difﬁcult: since microqueries in PINQ are arbitrary bits
of C#, it seems the choices are either to execute them on
a modiﬁed virtual machine that detects writes to global
state (as Airavat does), or else to create a small domain-
speciﬁc language for writing microqueries that avoids
global updates by design (as we do in Fuzz). Address-
ing timing attacks will require deeper changes to PINQ:
the issues and available solutions are precisely the ones
we study in this paper.
3.6 Case study: Airavat
Because Airavat calculates sensitivity and deducts the re-
quired amount from the privacy budget before query ex-
ecution begins, it is inherently safe from privacy budget
attacks. However, Airavat’s mechanism for preventing
state attacks permits a related vulnerability. To prevent
microqueries from communicating via static variables,
Airavat runs microqueries on a modiﬁed JVM; if a mi-
croquery ever attempts to modify a static variable, an ex-
ception is thrown and the whole query is marked “not
differentially private.” Unfortunately, the adversary can
now observe whether the system gives them the result at
the end of query execution or says, “Sorry, that’s not dif-
ferentially private.” A better alternative would be to abort
just the microquery, return, a default result, and allow the
remainder of the query to run to completion.
In its published form, Airavat is also vulnerable to tim-
ing attacks. Its authors acknowledge this weakness [26]
but counter that the bandwidth of the channel it creates
is very low. This, we agree, may make it tolerable in
some contexts, e.g., with “mostly trusted” queriers that
might be careless but will not write malicious queries
that intentionally attempt to reveal speciﬁc targeted se-
crets. We understand that Airavat may soon be enhanced
to add timeouts to microquery executions [Shmatikov,
personal communication, July 2010]; the implementa-
tion techniques described below should be useful in this
effort.
4 Defending against timing attacks
State and privacy budget attacks can (and must) be ad-
dressed by designing the query language so that they are
impossible. Timing attacks require more work, and this
will be our concern for the remainder of the paper.
4.1 Four approaches to the problem
There are two basic strategies. One is to ensure that a
given query takes very close to the same amount of time
for all possible databases (of a given size—see below),
so that the adversary can learn nothing from observing
the time it takes the query result to arrive. The other is
to treat time as an additional output of the query, and to
limit the amount of information the adversary can gain
using the same mechanisms (sensitivity analysis and ap-
6
propriate perturbation) that are used for data outputs.2
In either approach, we can either obtain the information
about running time statically (by analyzing the program
before running it) or enforce limits dynamically (e.g.,
by using timeouts). This gives us the four possibilities
shown in Table 1.
The solutions in the right-hand column provide some-
what weaker privacy guarantees than those on the left.
In order to properly “noise” a resource like time, we
must have the ability to both increase and decrease its
consumption. While we can clearly increase execution
time by adding a delay, we cannot easily decrease it. We
can mitigate this problem by adding a default delay T ;
thus, we can add “time noise” ν ≥ −T by delaying for
T + ν at the end of each query. Nevertheless, since noise
distributions guaranteeing differential privacy have un-
bounded support (i.e., P(ν) > 0 for all ν), there is al-
ways a possibility that ν < −T , in which case we can-
not complete the computation. Thus, ε-differential pri-
vacy seems impossible in practice; all we can hope for
is the slightly weaker property of (ε,δ)-differential pri-
vacy [11], where δ is a bound on the maximum additive
(not multiplicative) difference between the probability of
any given query output with and without a particular row
in the input.
On the other hand, in the constant-time solutions (left
column), the size of the database becomes public knowl-
edge, since, except for the most trivial queries, execution
time depends on the size of the database.
In practice,
this is probably a reasonable concession. In the case of
the variable-time solutions (right column), the size of the
database does not need to be published.
The static solutions (top row) are attractive in prin-
ciple, but
they depend on a static analysis of time
sensitivity—something that has proved challenging ex-
cept for very simple,
inexpressive programming lan-
guages. We therefore concentrate on the bottom row.
In this row, we choose one column to explore further:
the “constant execution time” alternative, where we try
to make each microquery take as close as possible to
exactly the same amount of time.
(The other column
also deserves exploration; we believe similar mecha-
nisms will be required.)
4.2 Default values
The approach we explore in the rest of this paper is to
dynamically ensure that each microquery m takes the ex-
act same amount of time T . If the microquery takes less
time to execute, we delay it and only return its result af-
ter T . If the microquery has executed for time T without
returning a result, we abort it. However, aborting the en-
2Note that the sensitivity analysis would have to account for inter-
dependencies between a query’s execution time and its output value,
which is far from trivial.
closing macroquery is not an option because this would
leak information to an adversarial querier. Instead, our
approach is to have the microquery return a default value