Pcp over the projection surface, collect the output Ocp.
3. repeat the previous step with different colors until
enough data is collected.
In practice, with r,g and b ∈ [0,255] we choose a certain
quantization per-color channel and project all possible col-
ors consecutively, while recording a video of the projection
surface. This allows us to collect enough information about
the full color space. With this method, we found that a quanti-
zation of 127 is enough to obtain sufﬁcient accuracy for our
method, so that we only need to project 33 = 27 colors to
obtain enough data for our model.
Camera noise. In order to collect accurate data, our mod-
elling technique has to account for noise that is being intro-
duced by the camera. At ﬁrst, we remove noise originating
1868    30th USENIX Security Symposium
USENIX Association
stop sign (+projected image)adversaryprojectorcarFigure 4: Camera light sensor noise visualized. The ﬁrst two
images show consecutive frames, while the third image shows
the absolute pixel-wise difference (×20) between the two
frames. Such sensor noise is accounted for with smoothing
over many frames during the data collection step.
from the sensitivity of the light sensor (ISO [33]), shown
in Figure 4. In fact, in non-bright lighting conditions, the
camera increases the light-sensitivity of image sensor, which
generates subtle pixel changes across consecutive (static)
frames [19]. To overcome this factor, instead of collecting
individual frames for S,Pcp,Ocp, we collect 10 consecutive
frames and compute and use the median of each pixel as our
ﬁnal image, the camera is static during this process.
Secondly, we found that there is a smoothing over-time
effect in the sensor readings while recording the video, so that
the sensor does not update immediately when a certain color is
being shown. Figure 6 shows how the average pixel color per
channel changes over time in relation to the timing of certain
projections being shown. The camera does not immediately
stabilize to the resulting color when a projection is shown,
but adjusts over a few frames. To account for this adaptation,
during the data collection, we interleave each projected color
with 10 frames of no projection, so that the camera re-adapts
to the unaltered image of the projection surface.
Fitting a projection model. Once we have collected a set of
S,Pcp,Ocp for the chosen set of colors, we construct a train-
ing dataset as follows. First we group together pixels of the
same color by creating a mask for each unique color in the
projection surface. In other words, we ﬁnd the set of unique
colors present in S, i.e., cs ∈ Sunq and then create a mask for
each color M(cs) = {i j, ...,ik} such that:
i ∈ M(cs) i.f.f. ith pixel in S == cs.
Then, for each unique source color cs, we extract all the mask-
matching pixels from the output Ocp, average their colors
to get an output color c(s,p)
, and save the following triple
}. A triple indicates that by
for our training data {cs,cp,c(s,p)
o
projecting cp on pixels of color cs we obtained (on average)
the color c(s,p)
. We then use the triples to ﬁt a neural network
composed of two hidden layers with ReLU activation, we
re-write Equation 1 as an optimization problem as follows:
o
o
LossP = argmin
θ1
∑
∀cs,cp
(cid:13)(cid:13)(cid:13)P (cs,cp)− c(s,p)
o
(cid:13)(cid:13)(cid:13)1
,
(2)
Figure 5: Plot showing the output space of the learned projec-
tion model. Each data point correspond to a color in S and its
color is the model output P (cs,cp) for a random cp.
where P is the model. We optimize the network using gradient
descent and Adam optimizer. Using P we have a differen-
tiable model which can be used to propagate the derivatives
through it during the AE generation, see Section 4.2.
Visualizing the Learned Model. When the projection sur-
face S is a stop sign (as mainly investigated in this paper),
pixels in S generally can be separated into two clusters based
on their color, corresponding to the “red” and “white” part
of the sign. The presence of these two clusters is reﬂected in
the outputs of the projection model, as different colors will
be achievable in output for the red and white parts of the
stop sign. We visualize the outputs of the projection model in
Figure 5, where we use a learned projection model P , the cap-
tured source image S and we compute a set of output colors
for random projection colors cp. Each data point in Figure 5
corresponds to the color of an output pixel and is marked by
a different marker (either triangle or circle) based on whether
the corresponding source pixel was into the red or white clus-
ter. Figure 5 shows that the model learns a different function
for red or white source pixels, obtaining in output more blue
tones for white pixels while different shades of red for the
remaining red pixels.
4.2 AE Generation
In this section we describe our method for generating the
adversarial projection. As a starting point, we combine the
projection model described in Section 4.1 with the target
network and use gradient descent along both to optimize the
projected image. In its basic form, we optimize the following
loss function:
J( f (t + P (x,δx))) s.t. 0 ≤ δx ≤ 1,
argmin
δx
where δx is the projected image, f the detection network, P
the projection model, x the input image background, x a stop
sign image, and J the detection loss, described later. In the
USENIX Association
30th USENIX Security Symposium    1869
Figure 6: Plot showing how the average value of a pixel (RGB)
changes when a certain projection is shown. Immediately after
the projection is shown, the camera requires a few frames (the
lines are marked every 2 frames) to converge to a stable value.
The two shaded areas mark the time the projection is being
shown and are colored with the projection color.
following we describe how we augmented the loss function
in order to facilitate the physical feasibility of the adversarial
perturbation and the convergence of the optimization.
Physical Constraints. We improve the physical realizability
of the projection with two steps. In order to maintain the
physical realizability of the projection we have two two steps.
At ﬁrst, we restrict the granularity of the projection in a ﬁxed
grid of n×n cells, so that each cell contains pixels of the same
color. This allows us to use the same projection for different
distances of viewing the stop sign. Secondly, we include the
total variation of the projection in the loss function in order
to reduce the effect of camera smoothing and/or blurring [31].
Variable Substitution. Since the optimization problem for
the projection is bounded in [0,1] (space of RGB images)
to ease the ﬂowing of gradients when backpropagating we
remove this box constraint. Given the image to project δx, we
substitute δx with a new variable w such that
w =
+ 0.5
tanhδx
2
and instead optimize for w. Since tanhδx is bounded in [−1,1]
we ﬁnd that this substitution leads to faster convergence in
the optimization.
Loss Function. We also limit the amount of perturbation in
our loss so that our ﬁnal optimization looks as follows:
argmin
w
J( f (t + P (x,w))) + λ(cid:107)P (x,w)− x(cid:107)p + TV(w),
where λ is a parameter used to control the importance of the
p-norm (cid:107)·(cid:107)p and TV is the total variation described above.
Since we operate on both object detectors and trafﬁc sign
recognizers, we use two different losses J depending on the
target network. For object detectors, we consider that the net-
work returns a ﬁnite set of boxes b ∈ B where for each box
there is an associated probability output of the box containing
a semantic object of class j, i.e., p(b)
. For trafﬁc sign recog-
j
nizers, the network returns a probability vector containing the
Figure 7: Overview of the adversarial samples generation
pipeline. We optimize the projected image which passes
through the projection model in order to minimize the tar-
get detection score on a given DNN for a set of randomly
generated permutations of the input.
probability of the input image being trafﬁc sign of class j, i.e.,
p j. We then use the following loss functions in the two cases:
• Object Detectors: the loss is the sum of the detection
probabilities for stop signs, i.e., ∑b∈B p(b)
j
;
• Trafﬁc Sign Classiﬁcation: the loss is the probability
for the stop sign class p j.
4.3 Training Data Augmentation
Generating adversarial examples that work effectively in
the physical world requires taking into account different envi-
ronmental conditions. Adversarial examples computed with
straightforward approaches such as in [40] do not survive
different viewing angles or viewing distances [38]. In order to
enhance the physical realizability of these samples, different
input transformations need to be accounted for during the
optimization. We use the Expectation over Transformation
(EOT) method [15], which consists in reducing the loss over
a set of training images computed synthetically. These train-
ing images are generated using linear transformations of the
desired input, i.e., an image containing stop signs, so that dif-
ferent environmental conditions can be accounted for during
the optimization. Using EOT, our ﬁnal loss becomes:
Eti∼T,m j∼M J( f (ti + m j · P (x,w)))
+λ(cid:107)P (x,w)− x(cid:107)p + TV(w),
Loss f = argmin
(3)
w
where T is a distribution over several background images and
M is an alignment function that applies linear transformations
to the perturbed sign. In this work, we augment the set of
the transformations to account for additional environmental
conditions that are disregarded in previous work. We report
in Figure 7 an overview of the complete optimization used in
our method.
Background and Trafﬁc Sign Post. Similarly to [43] we
select a set of road backgrounds and carefully place the stop
1870    30th USENIX Security Symposium
USENIX Association
35363738time(seconds)406080100120140averagechannelvalueredgreenbluesign on a post at the edge of the road. In [43] it is shown
that the post provides useful information to the detector and
should therefore be included when crafting the adversarial
perturbation.
Perspective. We vary the angle at which the camera is look-
ing at the stop sign. Since we do not want to account for all
perspective transforms, we use the following observations.
Firstly, a trafﬁc sign is mostly placed on one side of the lane
(to the right in right-driving countries), meaning that rarely
a camera mounted on a car would see a sign on the left-part
of the frame. Secondly, trafﬁc signs are mounted at speciﬁc
heights (e.g., 5 or 7 feet in the US [3]), which normally ex-
ceed the height of cars for better visibility. Given these two
observations, we prioritize perspective transforms that match
these conditions.
Distance. As the car is approaching the stop sign, the sign
will appear with different sizes in the camera frame. Our
goal is for the car to misclassify the stop sign in every frame,
therefore we place stop signs with different sizes during the
optimization. We test the detection of the stop sign in non-
adversarial settings with decreasing stop sign sizes and we set
the minimum size of the sign to be the smallest size at which
the sign is detected with high conﬁdence. In other words,
we only optimize for signs sizes that are large enough to be
detected by the classiﬁer.
Rotation. As shown in [14], simple rotations may lead to mis-
classiﬁcations when those transformations are not captured
in the training dataset. We therefore add rotation to the stop
sign when crafting the adversarial perturbation.
Brightness. The color of the stop sign changes based on a
combination of ambient light and camera settings, e.g., in
sunny days the colors appear brighter to the camera. To ac-
count for this, we apply different brightness transformations
to the stop sign, so that we include a wider range of color
tones. Since different colors contribute differently to an im-
age brightness, we transform the stop sign image from RGB
to YCrCb format [2], increase the luma component (Y) by a
speciﬁed delta and then bring the image back into RGB.
Camera Aspect Ratio. We observe that popular object de-
tectors resize the input images to be squared before being
processed by the network (e.g., Yolov3 resizes images to
416x416 pixels), to speed up the processing. However, the
typical native aspect ratio of cameras, i.e., the size of the
sensor, is 4:3 (e.g., the Aptina AR0132 chip used in the front-
viewing cameras by Tesla, has a resolution of 1280x960 [7]).
This leads to objects in the frames to being distorted when the
frames are resized to squared. To account for this distortion,
we choose the dimension of the stop sign so that its height is
greater than its width, reﬂecting a 4:3 to 1:1 resizing.
Parameter
learning rate
brightness
perspective
rotation
aspect ratio
sign size
grid size
Yolov3 Mask-RCNN Lisa-CNN Gtsrb-CNN
0.005
0.005
0.05
0.05
[−13, +13] (with range [0, 255])
x-axis [−30◦, +30◦], y-axis [−30◦, +30◦]
[−5◦, +5◦]
from 4:3 to 16:9
[25, 90] pixels
25× 25
Table 1: Parameters used for the AE generation and the train-
ing data augmentation. The values for brightness, perspective,
rotation, aspect ratio indicate the ranges for the applied trans-
formations. All parameters are picked uniformly at random
(with the exception of perspective) during the AE generation
for each sample in the generated training data.
4.4 Remarks
We use AdamOptimizer to run the AE generation. We opti-
mize a single variable that is the image to project with the
projector (its substitute, see Section 4.2). We use batches
of size 20. All the training images are created synthetically
by placing a stop sign on a road background and applying
the transformations described in the previous section. We do
not use a ﬁxed pre-computed dataset, a new batch with new
images is created after every backpass on the network. The
parameters for the transformations are chosen uniformly at
random in the ranges shown in Table 1. For all operations
that require resizing, we use cubic interpolation, ﬁnding that
it provides more robust results compared to alternatives. We
run the optimization for 50 epochs, in one epoch we feed 600
generated images containing a stop sign in the network. For
each epoch we optimize the 20% worst-performing batches
by backpropagating twice, convergence is usually reached
before the last epoch. Compared to similar works [43], our
method runs signiﬁcantly faster requiring only 50 modiﬁca-
tions of the perturbation (compared to 500), which takes less
than 10 minutes on an NVIDIA Titan V GPU for Yolov3.
5 Evaluation
In this section, we test the feasibility of the attack in practice.
5.1 Experimental Setup
Projector Setup. To test our projection, we buy a real stop
sign of size 600x600mm. For all of our experiments, we use
a Sanyo PLC-XU4000 projector [5], which is a mid-range of-
ﬁce projector (roughly $1,500) with 4,000 maximum lumens.
We carry out the experiment in a large lecture theatre in our
USENIX Association
30th USENIX Security Symposium    1871
Loss f
N
N
C
R
-
k
s
a
M
N
N
C
-
b
r
s
t
G
camera
exposure (ms)
3
v
o
l
o
LossP Y
33
25
18
12
9
0.020
0.023
0.017
0.015
0.011
0.09 0.08 0.01
0.11 0.52 0.00
0.68 0.86 0.89
1.44 4.24 5.31
1.80
9.12
5.92
N
N
C
-
a
s
i
L
0.06
0.07
1.03
2.45
8.16
lux