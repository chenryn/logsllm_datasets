http://www.
ipoque.com/resources/internet-studies/
internet-study-2008_2009.
[30] D. L. Johnson, E. M. Belding, K. Almeroth, and G. van Stam.
In-
ternet usage and performance analysis of a rural wireless network in
Macha, Zambia. In Proc. 4th ACM Workshop on Networked Systems
for Developing Regions (NSDR), San Francisco, CA, June 2010.
[31] JPMorgan Chase & Company. The Rise of Ad Networks. http://
www.mediamath.com/docs/JPMorgan.pdf.
[32] T. Kelly and J. Mogul. Aliasing on the World Wide Web: prevalence
and performance implications. In Proc. Eleventh International World
Wide Web Conference, Honolulu, Hawaii, USA, May 2002.
[33] C. Kreibich, N. Weaver, B. Nechaev, and V. Paxson. Netalyzr: illumi-
nating the edge network. In Proc. Internet Measurement Conference,
Melbourne, Australia, Nov. 2010.
[34] C. Labovitz, S. Iekel-Johnson, D. McPherson, J. Oberheide, and F. Ja-
hanian. Internet inter-domain trafﬁc. In Proc. ACM SIGCOMM, New
Delhi, India, Aug. 2010.
[35] B. A. Mah. An Empirical Model of HTTP Network Trafﬁc. In Proc.
IEEE INFOCOM, Kobe, Japan, Apr. 1997.
[36] G. Maier, A. Feldmann, V. Paxson, and M. Allman. On dominant
characteristics of residential broadband Internet trafﬁc. In Proc. Inter-
net Measurement Conference, Chicago, Illinois, Nov. 2009.
[37] G. Maier, F. Schneider, and A. Feldmann. NAT usage in residential
broadband networks. In Passive & Active Measurement (PAM), At-
lanta, GA, Mar. 2011.
[38] MaxMind. http://www.maxmind.com/.
[39] J. Mogul, B. Krishnamurthy, F. Douglis, A. Feldmann, Y. Goland,
A. van Hoff, and D. Hellerstein. Delta encoding in HTTP, January
2002. RFC 3229.
[40] J. C. Mogul, Y. M. Chan, and T. Kelly. Design, implementation,
and evaluation of duplicate transfer detection in HTTP. In Proc. 1st
USENIX NSDI, San Francisco, CA, Mar. 2004.
[41] Microsoft Smooth Streaming.
http://www.iis.net/
download/SmoothStreaming/.
[42] A. Muthitacharoen, B. Chen, and D. Mazieres. A low-bandwidth net-
work ﬁle system. In Proc. 18th ACM Symposium on Operating Sys-
tems Principles (SOSP), Banff, Canada, Oct. 2001.
[43] K. Park, S. Ihm, M. Bowman, and V. S. Pai. Supporting practi-
cal content-addressable caching with CZIP compression.
In Proc.
USENIX Annual Technical Conference, Santa Clara, CA, June 2007.
[44] K. Park, V. S. Pai, K.-W. Lee, and S. Calo. Securing Web service by
automatic robot detection. In Proc. USENIX Annual Technical Con-
ference, Boston, MA, June 2006.
[45] PlanetLab. http://www.planet-lab.org/, 2008.
[46] S. Podlipnig and L. Böszörmenyi. A survey of Web cache replacement
strategies. ACM Computing Surveys, 35, Dec. 2003.
[47] L. Popa, A. Ghodsi, and I. Stoica. HTTP as the Narrow Waist of
In Proc. 9th ACM Workshop on Hot Topics in
the Future Internet.
Networks (Hotnets-IX), Monterey, CA, Oct. 2010.
[48] M. O. Rabin. Fingerprinting by random polynomials. Technical Re-
port TR-15-81, Center for Research in Computing Technology, Har-
vard University, 1981.
[49] S. C. Rhea, K. Liang, and E. Brewer. Value-based Web caching. In
Proc. Twelfth International World Wide Web Conference, Budapest,
Hungary, May 2003.
[50] Riverbed Technologies, Inc. http://www.riverbed.com/.
[51] Roundup on Parallel Connections.
http://www.stevesouders.com/blog/2008/03/20/
roundup-on-parallel-connections/.
[52] RSS 2.0 Speciﬁcation.
http://www.rssboard.org/rss-specification.
[53] F. Schneider, S. Agarwal, T. Alpcan, and A. Feldmann. The new Web:
Characterizing Ajax trafﬁc. In Passive & Active Measurement (PAM),
Cleveland, OH, Apr. 2008.
[54] H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson. RTP: A
Transport Protocol for Real-Time Applications. Internet Engineering
Task Force, Jan. 1996. RFC 1889.
[55] H. Schulzrinne, A. Rao, and R. Lanphier. Real Time Streaming Proto-
col (RTSP). Internet Engineering Task Force, Apr. 1998. RFC 2326.
[56] F. D. Smith, F. H. Campos, K. Jeffay, and D. Ott. What TCP/IP pro-
tocol headers can tell us about the Web. In Proc. ACM SIGMETRICS,
Cambridge, MA, June 2001.
[57] N. Spring and D. Wetherall. A protocol-independent technique for
In Proc. ACM SIGCOMM,
eliminating redundant network trafﬁc.
Stockholm, Sweden, Sep. 2000.
[58] Squid Conﬁguration Directive. http://www.squid-cache.
org/Doc/config/quick_abort_min/.
[59] P. Srisuresh and K. Egevang. Traditional IP Network Address Trans-
lator (Traditional NAT). United States, 2001. RFC 3022.
[60] StatCounter. http://statcounter.com/.
[61] W3Counter. http://www.w3counter.com/.
[62] L. Wang, K. Park, R. Pang, V. S. Pai, and L. Peterson. Reliability
In Proc.
and security in the CoDeeN content distribution network.
USENIX Annual Technical Conference, Boston, MA, June 2004.
[63] A. Wolman, G. M. Voelker, N. Sharma, N. Cardwell, A. Karlin, and
H. M. Levy. On the scale and performance of cooperative Web proxy
caching. In Proc. 17th ACM Symposium on Operating Systems Prin-
ciples (SOSP), Kiawah Island, SC, Dec. 1999.
308Summary Review Documentation for 
“Towards Understanding Modern Web Traffic” 
Authors: S. Ihm, V. Pai 
Reviewer #1 
Strengths:  Nice  dataset.  The  analysis  confirms  the  expected 
increase in complexity of webpages. 
Weaknesses: The data covers US and China mostly, which would 
lead to an interesting comparison if it had been made. The authors 
throw numbers without giving as much high-level insight, making 
the  observations  hard  to  retain  for  the  reader.  Without  much 
information  about  how  representative  this  dataset  is  in  terms  of 
content, the CDNs from which the content is obtained, the impact 
of PlanetLab on the CDN answers, it is hard to judge the validity 
of the observations.  
Comments to Authors: The dataset is interesting. However, the 
number  of  plots  and  the  writing  style  make  it  boring  to  read. 
There  are  way  too  many  numbers  for  which  the  authors  do  not 
have an explanation. Instead of digging into the data so much, the 
message  of  the  paper  would  be  more  clear  if  more  high-level 
insights were provided instead of numbers.  
I  would  have  preferred  to  see  a  more  extensive  comparison  of 
USA  and  China  rather  than  having  the  paper  filled  with  the 
caching  section  which  is  questionable  in  its  applicability. 
Knowing  more  about the CDNs that deliver the content and the 
impact of PlanetLab on wherefrom you get content from CDNs is 
important while missing.  
Detailed comments: 
If they want to discuss caching, the authors should discuss more 
than the proxy-level cacheability but also the network-level one, 
for example as studied by: - B. Ager et al. Revisiting Cacheability 
in  Times  of  User  Generated  Content.  IEEE  Global  Internet 
Symposium, 2010.  
Regarding NAT usage, the authors should cite the recent work by 
Kreibich et al.: - Netalyzr: Illuminating The Edge Network, IMC 
2010.  
The high-level description of section 5 in the introduction sounds 
conflicting:  popular  URLs  grow  which 
improves  caching 
(potentially!) while the long tail also grows which hurts caching 
(again potentially). The authors should rephrase this. Stating it in 
the current way is nice as it raises the attention of the reader, but 
if the reader does not get it he will be frustrated.  
Your  dataset  is  great,  in  principle.  However,  does  it  provide 
realistic  latencies  as  well  as  representative  DNS  answers  by 
CDNs? Relying on the PlanetLab infrastructure is a bias from the 
viewpoint of the DNS answers that CDNs return, which is most of 
the content in the Internet. Please tone down the claims about the 
greatness  of  your  dataset.  A  great  dataset  does  not  require 
overdoing the marketing. The fourth paragraph of section 2 is too 
strong.  
The authors have to reduce the number in terms of the number of 
countries the dataset really has. USA and China make most of the 
content  of  the  dataset,  so  claiming  that  the  dataset  spans  187 
countries is misleading.  
In section 3.1, the authors mention that they provide the results of 
US, China and Brazil due to space limitations. Those 3 countries 
represent  most  of  the  dataset  anyway,  only  France  seems  to  be 
important enough and is omitted. This sounds misleading again, 
as the 187 countries mentioned before. The results of this section 
should be compared to the results of Kreibich et al.  
Figures 3-4 omit years 2007 and 2009. Are the trends for 2006-
2008-2010 consistent with them?  
How many embedded images do you observe? Embedded content 
is  important  nowadays  as  it  pervades  a  lot  of  webpages,  e.g., 
advertisement. This part might deserve more attention.  
The  authors  do  not  observe  video  streaming  that  split  files  into 
chunks. This may point to a bias of the dataset.  
One thing I was curious and was not analyzed by the authors is 
the  properties  of  the  dataset  on  a  per  CDN  basis. Do the trends 
show  up  consistently  across  all  content  delivery  platforms  or  is 
the  observed  behavior  an  artefact  of  the  specific  content  that 
CoDEEN users are accessing?  
For a recent analysis of DNS in the wild, the authors may cite: - 
B. Ager et al. Comparing DNS resolvers in the wild. IMC 2010.  
When  the  authors  mention  the  increase  in  Google  analytics 
beacons in the manually collected dataset, what is the message? Is 
that good, bad, neutral? All mentions of google analytics leave a 
feeling of “so what?”.  
From section 4.3, the authors do not discuss changes in RTT and 
bandwidth  that  may  affect  the  observations.  The  placement  of 
CDN servers or their connectivity is likely to have an impact here. 
It  is  simplistic  to  discuss  loading  latency  while  completely 
ignoring the content delivery side.  
How  much  is  ‘somewhat’?  How  much  do  the  authors  expect  to 
actually  under-estimate  the  loading  latency?  Did  they  carry  out 
some tests to check this?  
Given 
the  actual 
cacheability  of  web  objects?  It’s  not  clear  from  the  paper  how 
much of the caching can really be achieved in practice.  
Section  5  should  be  another  paper.  As  it  is  now  it  is not strong 
enough to be comparable to previous work and does not add much 
to the content of the paper, while bringing issues.  
Where is caching assumed to be done? At the browser, the proxy, 
inside  the  network?  What  information  is  used  to  assess  the 
cacheability of a specific object? This section should be compared 
with the following work: - B. Ager et al. Revisiting Cacheability 
lifetime  of  objects,  what 
the  actual 
is 
309in  Times  of  User  Generated  Content.  IEEE  Global  Internet 
Symposium, 2010.  
In 5.3, it seems that the authors assume a centralized cache inside 
a country that would serve all content. This is clearly not feasible, 
as this would affect the latency to obtain content. The problem of 
caching is more complex than presented in section 5. That leaves 
a negative impression on the whole paper. 
Reviewer #2 
Strengths: The authors have a nice dataset in their disposal, and 
this  alone  makes  the  paper  interesting.  There  is  an  interesting 
study on caching and the trends looking at object sizes. 
Weaknesses:  There  are  several  limitations  in  the  paper.  The 
authors  never  really  describe  their  dataset  and  the  precise 
methodology in their analysis, which makes it hard in some cases 
to follow the paper. Unclear how representative the data is (could 
easily have been checked). No web model is presented contrary to 
author claims in the intro and abstract. 
Comments  to  Authors:  I  was  really  intrigued  by  the  paper’s 
abstract since historical studies of web traffic are rare due to lack 
of datasets. However, despite the unique data, the paper fell short 
of my expectations in many respects.  
First,  the  authors  attempt  to  pack  many  different  things  in  one 
paper  (trends,  web  model,  caching,  etc),  with  the  result  of  not 
examining  any  of  them  in  detail  but  rather  providing  initial 
observations. Also, to do this, the dataset is sampled (trimmed) in 
many  respects  (one  month  per  year,  only  some  countries  etc) 
without much justification.  
Second,  the  authors  never  really  describe  their  data  and  the 
information  included  in  them.  What  fields  do  the  logs  include? 
What  type  of  users  are  we  looking  at?  Is  it  mostly  academics 
since  this  is  planetlab?  The  reader  has  to  infer  this  information 
from  the  analysis  (for  example,  the  paper  looks  at  object  inter-
arrival times, so there must be some sufficient timing information 
in the logs).  
Third, related to the above, how representative the data examined 
are.  For  example,  the  browser  popularity  appears  quite  peculiar 
with  firefox  reflecting  60-75%  of  the  users  in  some  cases. 
Comparing this with any public statistic out there (and there are 
many),  the  numbers  look  sufficiently  different.  Do  the  data 
mainly  reflect  academic  users,  and  as  such  present  a  specific 
community? A way to check this would be to compare the most 
popular requested URLs with statistics from Alexa for example. 
Similarly, NAT usage also appears low IMHO.  
Fourth, there is no web model presented, but rather statistics and 
distributions for various web traffic characteristics (e.g., number 
and size of objects, inter-arrival times etc).  
Instead, I believe that the section on caching is interesting and one 
of the contributions of the paper.  
Other detailed comments:  
Section  2:  Table  1:  There  seems  to  be  a  drop  in  the  number  of 
requests after 2007. Why is this? Also, how is it possible that the 
number of IPs/Agents in most of the countries is larger than the 
number  of  requests?  How  do  you  separate  non-human  traffic? 
How do you know the number of users?  
long 
term 
trend 
of 
network 
Section 3 You attribute the increase of javascript and CSS in the 
use  of  AJAX.  How  is  this  justified?  Simply,  this  could  be  an 
effect  of  more  code  and  advanced  style-sheets  out  there.  The 
paragraph “Domains” is hard to understand, and I was not able to 
follow it.  
Section 4: Is it true that step 3 can be performed only for 23.9% of 
the pages? If so, the methodology is not very effective. You visit 
the  top-100  sample  sites  from  Alexa.  If  not  mistaken,  most  of 
these are simple pages (e.g., search or other simple home pages). 
It  is  thus  unclear  whether  the  validation of your streamstructure 
algorithm  is  sufficient.  How  do  you  know  the  latencies?  What 
type of information do you have in the logs?  
Section  5:  You  need  to  formally  define  what  you  mean  by 
redundancy. You examine the most popular URLs. Which year is 
your reference for this? For URLs that were examined only once, 
is that for the whole trace? Since the content-based caching is so 
important  for  your  discussion,  a  more  complete  description  of 
how  it  works  would  make  the  understanding  of  your  results 
easier. 
Reviewer #3 
Strengths: This paper represents a great effort in understanding 
the 
traffic.  
The  measurement  results  are  of  great  interest  to  network 
community such as IETF. 
Weaknesses:  2  comments  on  the  weakness.  First,  from  reading 
this  paper  alone  I  did  not  fully  understand  how  the  data  is 
collected,  exactly  what  is  the  relation  between  CoDeeN  and 
PlanetLab, and who are the users.  
Second,  the  authors  seem  running  out  of  space  (e.g.  the  last 
paragraph  of  introduction  apparently  suffered  from  over-cut, 
leaving  the  sentences  incomplete),  while  at  the  same  time 
repeated their major results several times word by word; perhaps 
also running out of time to do a good proof reading. 
Comments to Authors: None. 
Reviewer #4 
Strengths: The combination of a detailed look at many aspects of 
Web traffic and a fabulous dataset. It’s rare that we see this, and 
the datapoints in the paper are great documentation about how the 
Web  has  changed  over  the  past  five  years,  even  if  they  are  not 
always surprising. I don’t know of any comparable study for this 
period,  as  other  notable  efforts 
Internet 
Atlas/Observatory are more broad but much less detailed wrt Web 
traffic.  
The  new  page  clustering  algorithm  is  also  useful,  and  will  help 
other researchers. 
Weaknesses: The paper covers broad ground and thus is a bit thin 
in  places,  particularly  wrt  experimental  method.  There  are  a 
bunch of places where I wanted to object a bit (e.g., Why is the 
average  download  time  for  objects  >1MB  a  good  estimator  for 
client bandwidth?) or wanted better validation (e.g., on the page 
clustering technique). I think this can be handled by being careful 
in the writing about what the paper has and has not shown. 
Comments to Authors: Very nice -- thanks for a good read. You 
have a fabulous dataset and some nice results. I particularly liked 
the  stat  that  the  majority  of  requests  are  now  due  to  page 
interaction rather than traditional page fetches.  
like  Arbor’s 
310What kind of users browse the Web via CoDeen? You likely have 
some bias and evolution here (usage drops off over time) that is 
worth stating. 
I can’t read your graphs, starting with Fig 1!  
It comes across as a bit arbitrary the way you pick US, China, BR, 
and FR and then jump between them for different results. It would 
be  better  to  be  more  consistent  and  to  explain  your  selection. 
Related to this comment, consider that a paper is *not* the best 
way  to  present  your  results.  You  might  provide  a  web  page  or 
technical report that does not have space limitations and can show 
the full set of graphs for a larger set of countries. The paper would 
then  be  a  representative  subset,  as  you  have  targeted,  but 
researchers would be able to consult the more complete results as 
needed.  
Why  is  average  download  time  for  objects  >1MB  a  good 
estimator for client bandwidth? It can fail due to slow servers or 
points of congestion within the Internet (between US and BR for 
instance). My guess is that a better measure is the 95% (fastest) 
download time. Either way, some validation would be really help.  
Similarly,  I  am  unsure  whether  your  results  indicate  that  NAT 
usage  is  growing  or  not.  They  could  easily  be  explained  by  an 
increase  in  the  number  of  computers  per  (already  NATed) 
household over the years. Can you compare with an independent 
datapoint, as your claim is presently unvalidated?  
Fig 6 felt like an odd extraction of different pieces for different 
countries.  
I do not understand how your page clustering algorithm works for 
pages that lack Google Analytics. Do you just skip them? If so it 
is bit of a fragile method! I also encourage you to perform a better 
validation of the technique; I would not call what you have done 
“careful validation”. This is because visiting 10 pages per top site 
may be markedly different than task-based Web browsing (doing 
mail, looking at a stock portfolio). I think that you can validate it 
fairly easily. Just create a browser extension that spits out a log of 
the pages visited over time, use it for real surfing, and see how it 
compares with your inference.  
Re  the  simple  model  of  web  traffic:  it  reads  as  a  rough 
characterization,  not  a  complete  model.  It  would  be  better  to 
either specify a complete model (that other researchers could use 
to generate synthetic traffic) or just keep it as a characterization.  
In the text, the median page load latency in 2010 is given as 5.98 
seconds.  This  is  really  long  per  my  usage!  Perhaps  there  is 
substantial  variation  in  the  median  for  different  countries  and 
access  bandwidths  that  is  making  the  overall  median  statistic of 
questionable utility?  
Is  your  use  of  “page  load  time”  is  different  than  the  normal 
definition  of  page  load  time  as  seen  within  the  browser  and 
indicated by the onLoad() handler firing? I was a bit confused on 
this.  
At the end of 4.3: I really have no idea if the predictions from the 
simulation are likely to be accurate. There is no validation of any 
simulated  points,  and  I  don’t  see  how  you  can  easily  validate. 
This is a bit troubling and deserves some mention in your text.  
In  Fig  12(f),  the  medians  look  substantially  the  same,  but  you 
describe them as different.  
Re caching figures: can you compare with earlier results to put the 
numbers you give in perspective.  
Re  aborted  transfers:  the  12-31%  figure  seems  the  right  one  to 
use,  but  you  use  the  higher  one  (for the full size of the aborted 
loads) elsewhere in the paper. Even so, I’m surprised that there is 
so  much  volume  in  a  small  number  of  aborted  transfers  --  they 
may deserve more attention and a look at their distribution. 
Reviewer #5 
Strengths:  The  paper  is  a  straightforward,  nicely  executed 
analysis  of  a  great  longitudinal  dataset.  It  provides  helpful 
numbers and contains a few cute tricks. 
Weaknesses: In parts I find the paper repetitive and plot-heavy. I 
believe the authors could free up a bunch of room by avoiding 
repeat mentioning of findings and leaving out some plots, making 
room for additional analysis. 
Comments to author: In the introduction when you recap 
Section 5, I did not understand “the popular URLs grow, and 
therefore improves caching”. It becomes clear later in Section 5.  
Table  1  is  poorly  designed.  Show  the  years  only  once.  Sort  the 
rows  alphabetically  by  country,  or  make  the  current  sorting 
criterion  clear.  Say  33,457  instead  of  33457,  and  use  less  lines. 
What are “agents”?  
The beginning of Section 3 overlaps the findings summary in the 
intro  a  lot.  Cut  it,  and  make  your  figures  larger  instead,  as 
virtually all are too small.  
In  3.1,  when  you  describe  connection  speed,  how  does  that 
account  for  CoDeeN’s  architecture?  Are  you  measuring  the 
download  speed  for  objects  stored  on  a  CoDeeN  server  to  the 
client, or ... ? Regarding NAT use, I would suspect not so much 
increase of it, rather than proliferation of browser-enabled devices 
behind existing NATs.  
In  Figures  5  and  6  the  placement  of  the  legend  destroys 
comparability  for  the  right-hand  plot.  Fold  the  legend  out 
horizontally and place it under all three graphs.  
I  like  the  trick  in  4.1  of  relying  on  observing  the  activity  of 
entities  known  to  trigger  only  at  specific  page  load  events,  in 
order to delineate initial pages.  
In 4.3, what do you mean by “stay less than 30 minutes” -- stay 
where?  
12a and 12f could go, as they add little over the text and would 
allow you to make the other ones larger.  
I  like  your  caching  analysis.  In  “Ideal  Byte  Hit  Rate”,  you  say 
you optimistically removed query strings from the URLs. A well-
designed  site  should  make  cacheability  of  such  content  explicit, 
say  via  ETags.  How  often  is  that  the  case? Also, it would have 
been interesting to read more about the clients’ correct support for 
HTTP  content  caching.  For  example,  how  often  do  clients  re-
request content they should cache locally, etc. Finally, you don’t 
say  how  your  content-based  caching  relates  to  cacheability 
information conveyed in the entities’ headers. Are your byte hit 
rates  optimal  in  the  sense  that  they  match  content  signatures 
across unrelated downloads and regardless of cacheability, or ...? 
Response from the Authors 
(1) Data set: We provide a more detailed description of CoDeeN 
and  PlanetLab  to  help  understand  our  data  set.  In  addition,  we 
compare the users of CoDeeN and the content they browse with 
other studies. Finally, we describe the limitations of our data set, 
and explain the origins of potential bias.  
(2) High-level analysis: We agree that the average download time 
of  objects  might  not  be  the  best  estimate  of  client  bandwidth 
because  bandwidth  estimation  itself  is  a  challenging  problem. 
However,  we  believe  our  estimates  are  good  enough  to  show 
yearly improvements and the existence of many slow clients. In 
311fact,  the  same  conclusions  hold  for  the  95th  percentile  of 
download  time.  Regarding  NAT  usage,  we  agree  that  the 
increased number of browser-enabled devices per household is a 
better explanation, and changed our text accordingly.  
(3)  Page-level  analysis:  First,  one  of  the  contributions  of 
StreamStructure is to delineate initial page loads by catching any 
kind of page load event, and we exploit Google Analytics simply 
because  of  its  popularity.  One  can  easily  utilize  beacons  from 
other  analytics  services  to  increase  coverage.  Second,  regarding 
our manually collected Alexa data set, we visit not only the top-
level pages but also other pages linked from those top-level pages. 
Of  course,  our  data  collection  is  different  from  actual  browsing 
patterns,  but  we  believe  that  it  is  sufficient  for  validation  as  it 
captures  the  structure  of  representative  Web  pages.  Third,  we 
clarify that the purpose of our page loading latency simulation is 
to raise issues about which factors affect the latency rather than to 
predict the actual latency.  
(4)  Caching  analysis:  We  mention  that  our  analysis  is  based  on 
logically centralized but physically distributed proxy systems, just 
like CoDeeN. Thus the result should read as potential bandwidth 
reduction  when  replacing  conventional  caching  proxies  with 
content-based caching proxies. Also, we add two more results that 
further  analyze  uncacheable  content  and 
the  origins  of 
redundancy.  
(5)  Miscellaneous:  We  acknowledge  prior  work  on  NAT  usage, 
DNS  performance,  and  caching  analysis.  We  also  address  all of 
the presentation issues, cleaning up tables and figures. Finally, a 
comparison of US and China is a non-goal of this paper. Instead, 
we  encourage  interested  readers  to  read  our  workshop  paper:  S. 
Ihm et al. ``Towards Understanding Developing World Traffic’’ 
in  NSDR’10,  which  includes  more  detailed  analysis  of  aborted 
transfers including object size distributions, which we omitted due 
to space limitations. 
312