## Page 25
OOM killer 是被谁触发的 896
2)4G内核空间/4G用户空间
ZONEDMA
ZONENORMAL16M~3968M
K 16M
ZONE_HIGHMEM > 3968M
64位的系统由于寻址能力的提高，不存在highmem区，所以64位系
统中存在的区有 DMA，DMA32 和 NORMAL三个区。
---
## Page 26
26
>OOMkiller 是被谁触发的
ZONE DMAK16M
ZONE DMA3216M~4G
ZONE_NORMAL > 4G
1.2内核分配内存的函数
下面是内核分配内存的核心函数之一，它会分配2的order次方个连续的物理页
内存，并将第一页的逻辑地址返回。
unsigned long _get_free_pages (gfp_t gfp_nask, unsigned int order)
内核空间的内存分配函数和用户空间最大的不同就是每个函数会有一个gfp
mask参数。
其中 gfp 代表的就是我们上面的内存分配函数_get_free_pages()。
gfp_mask 可以分成三种：行为修饰符（action modifier），区修饰符（zone
modifier） 和类型(type)。
（1）行为修饰符是用来指定内核该如何分配内存的。比如分配内存时是否可
以进行磁盘io，是否可以进行文件系统操作，内核是否可以睡眠（sleep）
等等。
（2）区修饰符指定内存需要从哪个区来分配。
（3）类型是行为修饰符和区修饰符结合之后的产物。在一些特定的内存分配场
合下，我们可能需要同时指定多个行为修饰符和区修饰符，而type 就是
针对这些固定的场合，将所需要的行为修饰符和区修饰符都整合到了一起，
这样使用者只要指定一个type就可以了。
不同type所代表的含义可以参看下面的表格：
2.日志解读
下面是从oomkiller被触发到进程到被杀掉的一个大概过程，我们来具体看
-下。
---
## Page 27
OOM killer 是被谁脏发的OOMkiller 是被谁触发的
33366 pages reserved
[pid ]
uid
tgid total_vm
rs8 nr_ptes
[ 355]
355
4868
svspents
66
1.3
q-Aepn-aeadn
V
V
361]
361
12881
145
28
-1000
papn-pu9qs.ce
[66
0
499
3814
60
1.3
upstart-socket -
562]
562
SS85
putqdx
79
15
[644]
106
644
86E5
142
16
0 rpc:
statd
775]
0
775
3818
5 8
1.2
upstart-file-br
D
[96cot]
丨此处有省略|
104 10396
21140
12367
64
0 nginx
[46601]
C6E0T 90T
21140
21140
12324
44
0
0 nginx
[10398]
104 10398
12324
44
104 10399
0
0 nginx
[10399]
21140
12367
Out of nenory:
64
0 nginx
:Ki1l process 10366 (nginx) score 6 or
sacrifice child
Killed process 10366 (nginx) total-vm:84784kB, anon-rss:49156kB, file-
88: 520kB
先来看一下第一行，它给出了oomkiller是由谁触发的信息。
nginx invoked oom-killer: gfp_nask=0x20oda, order=0, oom_score_adj=0
order=0告诉我们所请求的内存的大小是多少，即nginx请求了2的0次方这
么多个page的内存，也就是一个page，或者说是4KB。
gfp_mask的最后两个bit 代表的是zone mask，也就是说它指明内存应该从哪
个区来分配。
 Flag value Description
0x00u
0 implicitly means allocate from zoNE_NosMAL
_GFP_DMA Ox01u Allocate from ZONE_DMA if possible
_GFP_HIGHMEM Ox02u Allocate from ZONE_HIGHMEM if possible
（这里有一点需要注意，在64位的x86系统中，是没有highmem区的，64位
---
## Page 29
OOM killer 是被进脏发的OOM killer 是被谁触发的
这里我们说一下一个常见的误区，就是有人会认为触发了oomkiler的进程就是
问题的罪魁祸首，比如我们这个例子中的这个nginx进程。其实日志中invoke
oomkiller的这个进程有时候可能只是一个受害者，因为其他应用/进程已将
系统内存用尽，而这个invokeoomkiller的进程恰好在此时发起了一个分配内
存的请求而已。在系统内存已经不足的情况下，任何一个内存请求都可能触发
oom killer 的启动。
oomkiller的启动会使系统从用户空间转换到内核空间。内核会在短时间内进行
大量的工作，比如计算每个进程的oom分值，从而筛选出最适合杀掉的进程。
我们从日志中也可以看到这一筛选过程：
[pid ]
uid
tgid total_m
rs8 nr_ptes svapente oon_score_adj name
[ssE]
355
4868
66
1.3
0
upstart-udev-br
[361]
0
361
12881
145
28
0
-1000
systemd-udevd
[499]
0
499
3814
60
0
[562]
0
29S
SS8S
79
15
0
rpebind
[644]
106
646
86E5
142
16
[775]
statd
775
3818
58
1.2
upstart-file-br
[10396]
104 10396
21140
12367
44
0 nginx
[10397]
104 10397
21140
12324
64
0 nginx
[10398]
104 10398
21140
12324
44
0
0 nginx
[66E0t]
104 10399
21140
12367
44
0 nginx
本例中，一个nginx进程被选中作为缓解内存压力的牺牲进程：
Out of menory: Kill process 10366 (nginx) score 6 or sacrifice chi1d
Killed process
:10366 (nginx) total-vm:84784kB, anon-rss:49156kB,
file-rss:520kB
整个过程进行的时间很短，只有毫秒级别，但是工作量/计算量很大，这就导致
了cpu短时间内迅速升，出现峰值。但这一切工作都由内核在内核空间中完
---
## Page 31
OOMkiler 是被准融发的<
31
成，所以用户在自己的业务监控数据上并不会看到业务量的异常。这些短时间升
高的cpu是内核使用的，而不是用户的业务。
本例中客户只是偶尔看到这个现象，且业务并没有受到影响。我们给客户的建议
是分析业务内存需求量最大值，如果系统已经没有办法满足特定时段业务的内存