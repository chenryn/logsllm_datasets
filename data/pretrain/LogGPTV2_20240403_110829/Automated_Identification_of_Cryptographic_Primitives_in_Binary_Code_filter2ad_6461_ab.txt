In a nutshell, our identiﬁcation method is a three-step pro-
cess. First given as input a piece of assembly code we build
the corresponding DFG. Second we normalize this DFG us-
ing rewrite rules. Third we search for subgraphs in the DFG
that are isomorphic to the graph signature of a given crypto-
graphic algorithm. If such a subgraph is found, we conclude
that the assembly code implements the corresponding algo-
rithm. Figure 1 shows the ﬂowchart of our identiﬁcation
method.
Figure 1: Simple ﬂowchart of the DFG signature
based identiﬁcation technique
It is natural to wonder if CFGs could be used in a similar
process. However, due to performance and security consid-
erations (typically to resist timing attacks), the implemen-
tation of symmetric cryptographic algorithms tends to avoid
conditional instructions. For example there is no conditional
instruction in a standard MD5 implementation (Appendix
B). Furthermore CFG may vary from one algorithm in-
stance to another due to loop unrolling. For those reasons,
control ﬂow information does not seem relevant for symmet-
ric cryptographic identiﬁcation and we choose to solely rely
on the DFG. To take full advantage of this simpliﬁcation,
we make the assumption of straight-line programs where ev-
ery loop has been unrolled and every function call has been
inlined.
To achieve acceptable performances, our method should
not be applied directly to the code of a whole program. In
fact the second and third steps are computationally intensive
and the smaller the DFG is, the faster they will be executed.
Therefore, only preselected program fragments should be
submitted to the identiﬁcation process. However unlike the
I/O identiﬁcation method (described in Section 2.2), there
is no hard constraint on the fragment extraction mechanism
in our case. Obviously the cryptographic code should be in-
cluded in one fragment, but using large fragments does not
aﬀect the method reliability. In fact, the subgraph isomor-
phism algorithm is able to detect signatures even though
they are surrounded by additional elements. Some of the
criteria that may be used to extract suitable code fragments
are discussed in Section 8.
4. DATA FLOW GRAPH CONSTRUCTION
4.1 Model of DFG
A DFG is a Directed Acyclic Graph (DAG) that represents
the data dependency between a set of operations. A vertex
represents either an arithmetic/logic operation or an input
variable. An edge from vertex v1 to vertex v2 means that
v1 (or the value produced by v1, if v1 is an operation) is an
input operand for operation v2. Each operation produces
one result and takes a non-empty unordered set of operands
as input. For non commutative operations, edges may be
tagged using special labels to clearly identify the role of each
205operand. We also distinguish constant from non-constant
input variables. Constant variables have a ﬁxed known value
derived from immediate operands of the x86 assembly code;
non-constant variables have an unknown value deﬁned either
by a register or by a memory location. Memory accesses are
potentially considered as both input/output variables and
operations, and as such they are handled separately.
4.2 From Assembly to DFG
From the assembly code of a program fragment F , we
build the corresponding DFG: GF = (V, E) (V is a set
of vertices and E is a set of edges) by iterating over the
instructions of F . Each instruction i ∈ F is translated into
a set of operations Oi which can be empty (if i has no eﬀect
on GF , for instance because it is a branch instruction) or
contain one to several vertices (multiple operations may be
required to reproduce the behavior of complex instructions).
Depending on the type of the instruction’s input operands,
we take the following actions:
Immediate. We add a constant input variable to GF . This
vertex holds the value of the immediate operand and
is linked by an edge to Oi.
Register. We add an edge between the last deﬁnition of
the register and Oi. In practice, we maintain an array
that associates each register with the vertex holding
the reference to its current value. This reference can
be null if the register has not yet been used in F (In
that case a new input vertex is added to the graph),
or it can point to either an input variable (the register
was read but not set in F ) or an operation (the last to
have written in the register).
Memory. Memory operands are accessed through special
operations: load for memory read and store for mem-
ory write. These two operations take as input operand
an address which computation is explicitly transcribed
in GF . We also keep track of the order in which mem-
ory accesses are made in the program fragment.
4.3 Example
We illustrate our DFG model and the translation process
with an example. This example is based on a custom Even-
Mansour cipher [11] with a 32-bit substitution box as the
public permutation. If we note p the plaintext, k the key
and S the substitution box, the ciphertext is equal to the
following expression: S(p ⊕ k) ⊕ k. An x86 assembly imple-
mentation of this encryption algorithm is given in Figure 2
and the corresponding DFG is given in Figure 3. Input vari-
ables are represented inside rectangles and operations inside
circles. The relative order of memory accesses is speciﬁed
by an index. This example will be pursued through Section
5 and Section 6.
Figure 2: A possible x86 assembly code for the cus-
tom encryption algorithm. Given a plaintext p, an
encryption key k, the custom algorithm computes
S(p ⊕ k) ⊕ k where S is a substitution box.
4
4
line 2
line 5
8
esp
+
load2
+
line 1
load1
line 4
2
xor
+
sbox
shl
load4
+
load3
xor
Figure 3: DFG derived from the assembly code of
the custom encryption algorithm given in Figure 2.
Input variables are represented inside rectangles and
operations inside circles.
5. NORMALIZATION
The goal of normalization is to modify the DFG without
breaking its semantics in order to maximize the chance of
ﬁnding an algorithm’s signatures.
In other words we try
to remove the variations that may have been introduced by
either the developer, the optimizations of the compiler or
the translation into machine code.
Once the DFG of a code fragment has been constructed,
we normalize it using a set of rewrite rules. Since it is hard
to understand how a particular rule can aﬀect the others, we
simply iterate the application of the rewrite rules until we
reach a ﬁxed-point, which we consider to be the canonical
form of the DFG. We assume that the set of rewrite rules
does not contain conﬂicting rules that will cause an endless
loop. In practice the canonical form is reached in less than
ten iterations for most cases.
We use three types of rewrite rules: normalization rules,
memory simpliﬁcation rules and general simpliﬁcation rules.
5.1 Normalization Rules
Normalization rules are used when several instructions
can be used to perform the same operation. We arbitrar-
ily choose one as the normalized version and we create rules
to convert from the others. Examples of normalization rules
that we use, are given in the table below:
Normalized
∼ a
ror(a, cst2) with cst2 = size(a) − cst1
a + cst2
0
a > 16
e = a >> 14
c = b ∧ 0xff −→ d = e ∧ 0x2fc
d = c << 2
Let us assume that a certain compiler happens to perform
this optimization. It is obvious that it will be hard to be
undone: how do we guess there was a right shift operation
at the end of the sequence? Its normalized representation
should deﬁnitely be the optimized one. As a result, every
time the original sequence is encountered it will have to be
optimized.
At this point one might think that the amount of work re-
quired to match modern compilers data ﬂow optimizations
is going to be tremendous. However due to the straight line
hypothesis we made, it is simpler than it might look. In fact
the program fragment F can be seen as a single basic block
with one entry point and one exit point. Thus the simpliﬁ-
cation rules required to catch up with maximal optimization
levels only have to be applied locally to a single basic block.
These rules can be divided in two main mechanisms: com-
mon subexpression elimination and constant simpliﬁcation.
Common Subexpression Elimination
Common subexpression elimination is a classical compiler
technique to remove redundant operations. If two operations
share the same set of input operands, then they obviously
produce the same output. Consequently one of them can
safely be removed from the graph.
Common subexpression elimination is especially impor-
tant for memory addresses. In fact in x86 code, the eﬀective
address computation is generally performed every time a
memory access is made. As a consequence it is hard to de-
tect if two memory accesses are made at the same location,
since their address operand systematically belongs to diﬀer-
ent vertices. Common subexpression elimination will merge
eﬀective address computations resulting from the same set of
operands (base, index, scale and displacement). As a result
some memory accesses will explicitly share address vertex
in the graph and thence it will be possible to perform the
memory simpliﬁcation rules described previously.
An example of subexpression elimination is given on the
center of Figure 4. Two additions take as input ESP and
8. One of them was removed, leaving the graph with two
memory load sharing the same address vertex.
Constant Simpliﬁcation Rules
We can perform constant simpliﬁcation in the following case:
1. If every input operands of an arithmetic/logic oper-
ation is a constant variable. The operation can be
replaced by the result.
2. If an arithmetic/logic operation has an operand which
is equal to either the identity element or the absorbing
element of that operation (if they exist).
However it is sometimes required to rearrange sequences
of associative operations to perform constant simpliﬁcation.
This requirement is illustrated on the left hand side of Fig-
ure 4 where two consecutive additions involving constant
variables can be simpliﬁed if they are rearranged. Another
kind of rearranging strategy that enables further constant
simpliﬁcation is distribution. It is especially important for
memory address simpliﬁcation due to the scale parameter of
the x86 addressing mode.
2078
esp
load2
+
load1
8
2
xor
sbox
shl
+