2
2
2
2
2
2
2
2
2
2
2
10
10
10
10
10
10
10
10
10
10

0.5
0.5
0.5
0.5
2.0
0.01
0.3
1.6
1.6
1.6
1.0
0.05
0.14
3.84
6.11
2.0
8.0
21.5
21.5
2.0
8.0
8.0
3.0
3.0
4.0
Table 4: Gradient Perturbation based Classiﬁcation Methods using Relaxed Notion of Diﬀerential Privacy
1900    28th USENIX Security Symposium
USENIX Association
bation in their iterative learning procedure. These methods
do not scale to large numbers of training iterations due to
the composition theorem of diﬀerential privacy which causes
the privacy budget to accumulate across iterations. The only
exceptions are the works of Phan et al. [55, 56] that replace
the non-linear functions in deep learning with polynomial
approximations and then apply objective perturbation. With
this transformation, they achieve high model utility for  = 1,
as shown in Table 3. However, we note that this polynomial
approximation is a non-standard approach to deep learning
which can limit the model’s learning capacity, and thereby
diminish the model’s accuracy for complex tasks.
Machine learning with relaxed DP deﬁnitions. To avoid
the stringent composition property of diﬀerential privacy, sev-
eral proposed privacy-preserving deep learning methods adopt
the relaxed privacy deﬁnitions introduced in Section 2.1. Ta-
ble 4 lists works that use gradient perturbation with relaxed
notions of diﬀerential to reduce the overall privacy budget
during iterative learning. The utility beneﬁt of using relax-
ation is evident from the fact that the privacy budget for deep
learning algorithms is signiﬁcantly less than the prior works
of Shokri and Shmatikov [60] and Zhao et al. [79] which do
not use any relaxation.
While these relaxed deﬁnitions of diﬀerential privacy make
complex iterative learning feasible for reasonable  values,
they might lead to more privacy leakage in practice. The main
goal of our study is to evaluate the impact of implementation
decisions regarding the privacy budget and relaxed deﬁnitions
of diﬀerential privacy on the concrete privacy leakage that
can be exploited by an attacker in practice. We do this by
experimenting with various inference attacks, described in
the next section.
3 Inference Attacks on Machine Learning
This section surveys the two types of inference attacks, mem-
bership inference (Section 3.1) and attribute inference (Sec-
tion 3.2), and explains why they are useful metrics for evalu-
ating privacy leakage. Section 3.3 brieﬂy summarizes other
relevant privacy attacks on machine learning.
3.1 Membership Inference
The aim of a membership inference attack is to infer whether
or not a given record is present in the training set. Membership
inference attacks can uncover highly sensitive information
from training data. An early membership inference attack
showed that it is possible to identify individuals contributing
DNA to studies that analyze a mixture of DNA from many
individuals, using a statistical distance measure to determine
if a known individual is in the mixture [27].
Membership inference attacks can either be completely
black-box where an attacker only has query access to the
target model [61], or can assume that the attacker has full
white-box access to the target model, along with some auxil-
iary information [74]. The ﬁrst membership inference attack
on machine learning was proposed by Shokri et al. [61]. They
consider an attacker who can query the target model in a
black-box way to obtain conﬁdence scores for the queried
input. The attacker tries to exploit the conﬁdence score to
determine whether the query input was present in the train-
ing data. Their attack method involves ﬁrst training shadow
models on a labelled data set, which can be generated either
via black-box queries to the target model or through assump-
tions about the underlying distribution of training set. The
attacker then trains an attack model using the shadow models
to distinguish whether or not an input record is in the shadow
training set. Finally, the attacker makes API calls to the target
model to obtain conﬁdence scores for each given input record
and infers whether or not the input was part of the target
model’s training set. The inference model distinguishes the
target model’s predictions for inputs that are in its training set
from those it did not train on. The key assumption is that the
conﬁdence score of the target model is higher for the training
instances than it would be for arbitrary instances not present
in the training set. This can be due to the generalization gap,
which is prominent in models that overﬁt to training data.
A more targeted approach was proposed by Long et al. [44]
where the shadow models are trained with and without a tar-
geted input record t. At inference time, the attacker can check
if the input record t was present in the training set of tar-
get model. This approach tests the membership of a speciﬁc
record more accurately than Shokri et al.’s approach [61]. Re-
cently, Salem et al. [59] proposed more generic membership
inference attacks by relaxing the requirements of Shokri et
al. [61]. In particular, requirements on the number of shadow
models, knowledge of training data distribution and the tar-
get model architecture can be relaxed without substantially
degrading the eﬀectiveness of the attack.
Yeom et al. [74] recently proposed a more computationally
eﬃcient membership inference attack when the attacker has
access to the target model and knows the average training loss
of the model. To test the membership of an input record, the
attacker evaluates the loss of the model on the input record
and then classiﬁes it as a member if the loss is smaller than
the average training loss.
Connection to Diﬀerential Privacy. Diﬀerential privacy, by
deﬁnition, aims to obfuscate the presence or absence of a
record in the data set. On the other hand, membership in-
ference attacks aim to identify the presence or absence of
a record in the data set. Thus, intuitively these two notions
counteract each other. Li et al. [42] point to this fact and
provide a direct relationship between diﬀerential privacy and
membership inference attacks. Backes et al. [4] studied mem-
bership inference attacks on microRNA studies and showed
that diﬀerential privacy can reduce the success of membership
USENIX Association
28th USENIX Security Symposium    1901
inference attacks, but at the cost of utility.
Yeom et al. [74] formally deﬁne a membership inference
attack as an adversarial game where a data element is selected
from the distribution, which is randomly either included in
the training set or not. Then, an adversary with access to
the trained model attempts to determine if that element was
used in training. The membership advantage is deﬁned as the
diﬀerence between the adversary’s true and false positive rates
for this game. The authors prove that if the learning algorithm
satisﬁes -diﬀerential privacy, then the adversary’s advantage
is bounded by e − 1. Hence, it is natural to use membership
inference attacks as a metric to evaluate the privacy leakage
of diﬀerentially private algorithms.
3.2 Attribute Inference
The aim of an attribute inference attack (also called model
inversion) is to learn hidden sensitive attributes of a test in-
put given at least API access to the model and information
about the non-sensitive attributes. Fredrikson et al. [20] for-
malize this attack in terms of maximizing the posterior prob-
ability estimate of the sensitive attribute. More concretely,
for a test record x where the attacker knows the values of
its non-sensitive attributes x1, x2,··· xd−1 and all the prior
probabilities of the attributes, the attacker obtains the out-
put of the model, f (x), and attempts to recover the value of
the sensitive attribute xd. The attacker essentially searches
for the value of xd that maximizes the posterior probability
P(xd | x1, x2,··· xd−1, f (x)). The success of this attack is based
on the correlation between the sensitive attribute, xd, and the
model output, f (x).
Yeom et al. [74] also propose an attribute inference attack
using the same principle they use for their membership in-
ference attack. The attacker evaluates the model’s empirical
loss on the input instance for diﬀerent values of the sensitive
attribute, and reports the value which has the maximum pos-
terior probability of achieving the empirical loss. The authors
deﬁne the attribute advantage similarly to their deﬁnition of
membership advantage for membership inference.
Fredrikson et al. [20] demonstrated attribute inference at-
tacks that could identify genetic markers based on warfarin
dosage output by a model with just black-box access to model
API.1 With additional access to conﬁdence scores of the
model (noted as white-box information by Wu et al. [69]),
more complex tasks have been performed, such as recovering
faces from the training data [19].
Connection to Diﬀerential Privacy. Diﬀerential privacy is
mainly tailored to obfuscate the presence or absence of a
record in a data set, by limiting the eﬀect of any single record
on the output of diﬀerential private model trained on the data
1This application has stirred some controversy based on the warfarin
dosage output by the model itself being sensitive information correlated
to the sensitive genetic markers, hence the assumption on attacker’s prior
knowledge of warfarin dosage is somewhat unrealistic [46].
set. Logically this deﬁnition also extends to attributes or fea-
tures of a record. In other words, by adding suﬃcient diﬀer-
ential privacy noise, we should be able to limit the eﬀect of a
sensitive attribute on the model’s output. This relationship be-
tween records and attributes is discussed by Yeom et al. [74].
Hence, we include these attacks in our experiments.
3.3 Other Attacks on Machine Learning
Apart from inference attacks, many other attacks have been
proposed in the literature which try to infer speciﬁc infor-
mation from the target model. The most relevant are mem-
orization attacks, which try to exploit the ability of high
capacity models to memorize certain sensitive patterns in
the training data [10]. These attacks have been found to be
thwarted by diﬀerential privacy mechanisms with very little
noise ( = 109) [10].
Other privacy attacks include model stealing, hyperparame-
ter stealing, and property inference attacks. A model stealing
attack aims to recover the model parameters via black-box
access to the target model, either by adversarial learning [45]
or by equation solving attacks [66]. Hyperparameter steal-
ing attacks try to recover the underlying hyperparameters
used during the model training, such as regularization coeﬃ-
cient [67] or model architecture [72]. These hyperparameters
are intellectual property of commercial organizations that de-
ploy machine learning models as a service, and hence these
attacks are regarded as a threat to valuable intellectual prop-
erty. A property inference attack tries to infer whether the
training data set has a speciﬁc property, given a white-box
access to the trained model. For instance, given access to a
speech recognition model, an attacker can infer if the train-
ing data set contains speakers with a certain accent. Here
the attacker can use the shadow training method of Shokri
et al. [61] for distinguishing the presence and absence of a
target property. These attacks have been performed on HMM
and SVM models [3] and neural networks [22].
Though all these attacks may leak sensitive information
about the target model or training data, the information leaked
tends to be application-speciﬁc and is not clearly deﬁned in a
general way. For example, a property inference attack leaks
some statistical property of the training data that is surprising
to the model developer. Of course, the overall purpose of the
model is to learn statistical properties from the training data.
So, there is no general deﬁnition of a property inference attack
without a prescriptive decision about which statistical proper-
ties of the training data should be captured by the model and
which are sensitive to leak. In addition, the attacks mentioned
in this section do not closely follow the threat model of diﬀer-
ential privacy. Thus, we only consider inference attacks for
our experimental evaluation.
In addition to these attacks, several poisoning and adversar-
ial training attacks have been proposed [5, 50, 71, 73] which
require an adversary that can actively interfere with the model
1902    28th USENIX Security Symposium
USENIX Association
training process. We consider these out of scope for this paper,
and assume a clean training process not under the control of
the adversary.
4 Empirical Evaluation
To quantify the privacy leakage of the diﬀerentially private im-
plementations for machine learning, we conduct experiments
to measure how much an adversary can infer from a model.
As motivated in Section 3, we measure privacy leakage using
membership and attribute inference in our experiments. Note,
however, that the conclusions we can draw from experiments
like this are limited to showing a lower bound on the informa-
tion leakage since they are measuring the eﬀectiveness of a
particular attack. Such experimental results cannot be used
to make strong claims about what the best possible attack
would be able to infer, especially in cases where an adversary
has auxiliary information to help guide the attack. Evidence
from our experiments, however, does provide clear evidence
for when implemented privacy protections do not appear to
provide suﬃcient privacy.
4.1 Experimental Setup
We evaluate the privacy leakage of two diﬀerentially private
algorithms using gradient perturbation: logistic regression for
empirical risk minimization (Section 4.2) and neural networks
for non-convex learning (Section 4.3). For both, we consider
the diﬀerent relaxed notions of diﬀerential privacy and com-
pare their privacy leakage. The variations that we implement
are naïve composition (NC), advanced composition (AC),
zero-concentrated diﬀerential privacy (zCDP) and Rényi dif-
ferential privacy (RDP) (see Section 2.1 for details). We do
not include CDP as it has the same composition property as
zCDP (Table 1). For RDP, we use the RDP accountant (previ-
ously moments accountant) of TF Privacy framework [2].
We evaluate the models on two main metrics: accuracy
loss, the model’s accuracy loss on test set with respect to the
non-private baseline, and privacy leakage, the attacker’s ad-
vantage as deﬁned by Yeom et al. [74]. To evaluate out the
inference attack, we provide the attacker with a set of 20,000
records consisting of 10,000 records from training set and
10,000 records from the test set. We call records in the train-
ing set members, and the other records non-members. These
labels are not known to the attacker. The task of the attacker
is to predict whether or not a given input record belongs to the
training set (i.e., if it is a member). The privacy leakage metric
is calculated by taking the diﬀerence between the true positive
rate (TPR) and the false positive rate (FPR) of the inference
attack. Thus the privacy leakage metric is always between 0
and 1, where the value of 0 indicates that there is no leakage.
For example, if an attacker performs membership inference
on a model and obtains a privacy leakage of 0.7 then it implies
that for every 100 wrong membership predictions made by
the attacker, 170 ‘true’ members are revealed to the attacker.
In other words, 170 training records are revealed to the at-
tacker. To better understand the potential impact of leakage,
we also conduct experiments to estimate the actual number
of members who are at risk for disclosure in a membership
inference attack.
Data sets. We evaluate our models over two data sets for
multi-class classiﬁcation tasks: CIFAR-100 [38] and Purchase-
100 [36]. CIFAR-100 consists of 28× 28 images of 100 real
world objects, with 500 instances of each object class. We
use PCA to reduce the dimensionality of records to 50. The
Purchase-100 data set consists of 200,000 customer purchase
records of size 100 each (corresponding to the 100 frequently-
purchased items) where the records are grouped into 100
classes based on the customers’ purchase style. For both data
sets, we use 10,000 randomly-selected instances for training
and 10,000 randomly-selected non-training instances for the
test set. The remaining records are used for training shadow
models and inference model.
Attacks. For our experiments, we use the attack frameworks
of Shokri et al. [61] and Yeom et al. [74] for membership
inference and the method proposed by Yeom et al. [74] for
attribute inference. In Shokri et al.’s framework [61], multiple
shadow models are trained on data that is sampled from the
same distribution as the private data set. These shadow mod-
els are used to train an inference model to identify whether
an input record belongs to the private data set. The inference
model is trained using a set of records used to train the shadow
models, a set of records randomly selected from the distribu-
tion that are not part of the shadow model training, along with
the conﬁdence scores output by the shadow models for all
of the input records. Using these inputs, the inference model
learns to distinguish the training records from the non-training
records. At the inference stage, the inference model takes an
input record along with the conﬁdence score of the target
model on the input record, and outputs whether the input
record belongs to the target model’s private training data set.
The intuition is that if the target model overﬁts on its training
set, its conﬁdence score for a training record will be higher
than its conﬁdence score for an otherwise similar input that
was not used in training. The inference model tries to exploit
this property. In our instantiation of the attack framework,
we use ﬁve shadow models which all have the same model
architecture as the target model. Our inference model is a
neural network with two hidden layers of size 64. This setting
is consistent with the original work [61].
The attack framework of Yeom et al. [74] is simpler than
Shokri et al.’s design. It assumes a white-box attacker with
access to the target model’s expected training loss on the pri-
vate data set, in addition to having access to the target model.
For membership inference, the attacker simply observes the
target model’s loss on the input record. The attacker classiﬁes
the record as a member if the loss is smaller than the target
USENIX Association
28th USENIX Security Symposium    1903
(a) Batch gradient clipping
(b) Per-instance gradient clipping
Figure 1: Impact of clipping on accuracy loss of logistic regression (CIFAR-100).
model’s expected training loss, otherwise the record is classi-
ﬁed as a non-member. The same principle is used for attribute
inference. Given an input record, the attacker brute-forces all
possible values for the unknown private attribute and observes
the target model’s loss, outputting the value for which the loss
is closest to the target’s expected training loss. Since there are
no attributes in our data sets that are explicitly annotated as
private, we randomly choose ﬁve attributes, and perform the
attribute inference attack on each attribute independently, and
report the averaged results.
Hyperparameters. For both data sets, we train logistic re-
gression and neural network models with (cid:96)2 regularization.
First, we train a non-private model and perform a grid search
over the regularization coeﬃcient λ to ﬁnd the value that min-
imizes the classiﬁcation error on the test set. For CIFAR-100,
we found optimal values to be λ = 10−5 for logistic regres-
sion and λ = 10−4 for neural network. For Purchase-100, we
found optimal values to be λ = 10−5 for logistic regression
and λ = 10−8 for neural network. Next, we ﬁx this setting to
train diﬀerentially private models using gradient perturbation.
We vary  between 0.01 and 1000 while keeping δ = 10−5,
and report the accuracy loss and privacy leakage. The choice
of δ = 10−5 satisﬁes the requirement that δ should be smaller
than the inverse of the training set size 10,000. We use the
ADAM optimizer for training and ﬁx the learning rate to 0.01
with a batch size of 200. Due to the random noise addition,
all the experiments are repeated ﬁve times and the average
results and standard errors are reported. We do not assume
pre-trained model parameters, unlike the prior works of Abadi
et al. [1] and Yu et al. [75].
Clipping. For gradient perturbation, clipping is required to
bound the sensitivity of the gradients. We tried clipping at
both the batch and per-instance level. Batch clipping is more
computationally eﬃcient and a standard practice in deep learn-
ing. On the other hand, per-instance clipping uses the privacy
budget more eﬃciently, resulting in more accurate models
for a given privacy budget. We use the TensorFlow Privacy