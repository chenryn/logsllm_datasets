P (XA (cid:54) p) = 1−γ, and given PK 3 with α = p, one can then
assign ϕ the value φ− γ, the smallest (i.e. most conservative)
value of ϕ consistent with φ and γ. Here, γ could be the result
of previously applying CBI to version A (Sec. III).
We showed (Sec. VII) how some forms of PK result in more
conﬁdence in a claim than other forms; and some require fewer
failure-free demands than others in order to support a given
claim. We emphasize that one must not use this knowledge
for simply claiming that PK that yields the most favourable
results – an unethical and dangerous practice. The choice of
PKs must be based on the prior evidence alone.
In certain cases, obtaining evidence that might bring added
conﬁdence may be expensive – for instance, if discovering
what one’s ϕ value should be would require analysis of
extensive logs about past projects. A decision is then needed
about whether the added conﬁdence justiﬁes the signiﬁcant
cost. Our results can inform such considerations.
We observe that parameter values sometimes need not
be speciﬁed with great precision. E.g., (see Fig. 7): if CII
supporting evidence is relatively weak (i.e. θ > φ > 1 − θ)
– i.e. conﬁdence in B being an improvement is less than
conﬁdence that A and B meet their “engineering goal” – then
posterior conﬁdence, and the nA and nB needed to support a
claim, do not depend on φ.
We have focused on those CBI priors (parameter values for
the PKs) consistent with what we would expect in practice. For
instance, we assumed conﬁdence higher than 50% (i.e. θ >
1 − θ) that the engineering goal is met: system development
would not usually be started if conﬁdence were lower.
We have also not considered the case of p < ε, i.e, of a
required bound on pfd lower than the value one has some
conﬁdence of being achieved. Previous studies showed how
this may give zero posterior conﬁdence unless additional prior
knowledge is stated, about values of pfd smaller than p [7,9].
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:48 UTC from IEEE Xplore.  Restrictions apply. 
460
A broader observation is appropriate about the worst-case
priors that CBI produces. Recall, for instance, how in Fig. 8
increasing nA eventually undermines conﬁdence in a claim,
requiring increasing nB to compensate. This is because the
worst case priors include a probability mass at point (pl, p);
that is, a belief “if A turns out to be extremely good, then B
must be inadequate, but just inadequate enough not to achieve
the desired pfd, p”. We trust that readers will rightly object to
such a belief, because experience would not typically support
it. And yet, such a belief is not refuted by any of the forms of
PK 1 to 4 that an assessor could specify to support a safety
claim. We now discuss the implications of such apparently
unreasonable worst-case priors.
If one followed the steps we recommended, proceeding
from the evidence to carefully spell out which PKs it implies,
and yet the resulting worst-case prior seems absurd, possible
reasons are: 1) the analysis may have been inadequate, and
requires additional PK that capture neglected implications of
the evidence [9,21,22]; 2) that prior, albeit absurd, may be
a limiting case of a class of priors which are themselves
plausible,
just
because it leads to unpleasant conclusions; 3) the “absurdity”
of the results ﬂags an error we made in deriving the PKs from
the evidence, or in our likelihood function, etc.
in which case we cannot rightly forbid it
Sec. VII also highlights how, unsurprisingly, strongly sup-
ported CII can increase conﬁdence in posterior claims, or re-
duce the nB needed for a stated level of conﬁdence. However,
when nA is small, all of the worst-case priors require a larger
nB than required when making the same claim on B using a
“single-system” CBI prior (see discussion of Fig. 8).
We also highlight how θ × 100% conﬁdence for both
versions brings a noticeable increase in posterior conﬁdence
– i.e. the difference between the prior from Fig. 3a (only A
version) and the prior from Fig. 3b (both versions) in Fig. 13.
This is true when nA is not the value n∗
A that guarantees the
greatest posterior conﬁdence. If nA = n∗
A, θ for both versions
brings no beneﬁt; what seems to matter then is the form of
CII. Knowing n∗
A tells us how much more conﬁdent, at most,
a claim for B could be, everything else being equal.
Finally, unlike ordinary Bayesian inference, in CBI the form
of prior used needs to depend on the input values. For example,
up until about nA = 108,
the dotted curve in Fig. 8 is
generated by the prior in Fig. 4b. Beyond that point, it is
generated by the new worst-case prior in Fig. 4a. These priors
are very different, and erroneously using only one of these for
all nA would give signiﬁcantly over-optimistic results.
C. Future Work
The forms of formal “prior knowledge” that we have studied
are chosen to be realistic, but do not exhaust
those that
may hold in practice. Case studies, especially applying the
kind of scrutiny we have outlined above when CBI produces
apparently unreasonable worst-case priors, may reveal other
forms of prior evidence that can reduce excess conservatism.
In this direction – addressing over-conservative worst-case
priors – a purely mathematical next step is to ﬁnd a solution
that fully exploits the form of claim in example 3 (Sec. IV),
requiring (6) to hold for every subinterval in [0, 1]. Using this
form of CII to address example 3 would give more conﬁdence
in claims on pfd, compared with approximating it by PK 4.
We have focused on scenarios in which no failures oc-
curred. This makes sense for certiﬁcation in critical systems,
regarding systematic failures: usually, if a software failure
occurs, the system is ﬁxed, and reassessed from scratch. If
this reassessment ignores that the fault in the previous version
may undermine assumptions on which the assessment relies,
its result may be over-optimistic [25]. Accordingly, it would
be useful to extend the present work (similarly to [25]), to
assessing version B given failures in version A; or, for less
critical systems, given failures in both A and B.
Some aspects of the mathematical apparatus can be eas-
ily completed if needed. For instance, we do not explicitly
account for the joint probability of both versions satisfying
the engineering goal. There may be reasons for believing
either positive or negative correlation between the two events.
Studying their effects may give more insight into useful forms
of PK that are currently missing. Also, in our scenarios, the
same θ × 100% conﬁdence applies to both versions. The two
could be different in practice (e.g. due to different approaches
applied in developing versions A and B, or markedly different
operational environments).
Thus far, CBI applications have involved solving con-
strained mathematical optimizations over sets of prior prob-
ability distributions. This captures the uncertainty an assessor
has in adequately specifying what beliefs prior evidence justi-
ﬁes. But the assessor could also have uncertainty in specifying
the probabilistic failure model for the failure-free observations
(i.e., the likelihood function). Extending CBI to assess the
effects of such uncertainty would be a fruitful exercise.
D. Summary of contributions of this paper
In this paper, we have reported: 1) how various practical
scenarios map into formal “prior knowledge” (PK) statements,
to use in conservative Bayesian inference (CBI); 2) convenient
closed form solutions for the worst-case priors and for the
posterior conﬁdence in claimed pfd bounds; 3) sensitivity
analyses,
identifying parameter ranges for which evidence
from operation in a system, or environment, A, reduces the
amount nB of failure-free operation required in system, or
environment, B for a required conﬁdence in a bound.
Together with our previous work [6,7], by introducing new
examples based on practical scenarios, this paper demonstrates
how CBI can be used to formalise arguments that use claims
like “proven in use” (PIU), “Globally at least equivalent”
(GALE), or “stress tested” – claims derived from operation/test
evidence on related, but not identical, environments of use
or system versions. These examples are a good guide when
translating other forms of prior evidence into formally stated
CBI constraints (“PK”s).
ACKNOWLEDGMENT
We thank the anonymous reviewers for their insightful
comments and helpful suggestions for improving the paper.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:48 UTC from IEEE Xplore.  Restrictions apply. 
461
[22] X. Zhao, B. Littlewood, A. Povyakalo, L. Strigini, and D. Wright,
“Modeling the probability of failure on demand (pfd) of a 1-out-of-2
system in which one channel is “quasi-perfect”,” Reliability Engineering
& System Safety, vol. 158, pp. 230–245, 2017.
[23] P. Popov, L. Strigini, J. May, and S. Kuball, “Estimating bounds on
the reliability of diverse systems,” IEEE Transactions on Software
Engineering, vol. 29, no. 4, pp. 345–359, 2003.
[24] K. Salako, L. Strigini, and X. Zhao, “Proofs of conservative conﬁdence
bounds on pfd, using claims of improved reliability,” Tech. Rep., 2021.
[Online]. Available: https://openaccess.city.ac.uk/id/eprint/25905/
[25] B. Littlewood and D. Wright, “Some conservative stopping rules for the
operational testing of safety critical software,” IEEE Transactions on
Software Engineering, vol. 23, no. 11, pp. 673–683, 1997.
REFERENCES
[1] IEC,
IEC61508,
Elec-
tronic/Programmable Electronic Safety Related Systems, 2010. [Online].
Available: https://webstore.iec.ch/publication/22273
Functional
Electrical/
Safety
of
[2] CENELEC, EN 50129:2018: Railway applications - Communication,
signalling and processing systems - Safety related electronic systems for
signalling. European Committee for Electrotechnical Standardization
(CENELEC), Nov. 2018.
[3] B. Littlewood and L. Strigini, “Validation of ultra-high dependability for
software-based systems,” Comm. of the ACM, vol. 36, pp. 69–80, 1993.
[4] R. W. Butler and G. B. Finelli, “The infeasibility of quantifying the
reliability of life-critical real-time software,” IEEE Transactions on
Software Engineering, vol. 19, no. 1, pp. 3–12, Jan. 1993.
[5] N. Kalra and S. Paddock, “Driving to safety: How many miles of driving
would it take to demonstrate autonomous vehicle reliability?” Transp.
Research Part A: Policy and Practice, vol. 94, pp. 182–193, 2016.
[6] X. Zhao, K. Salako, L. Strigini, V. Robu, and D. Flynn, “Assessing
safety-critical systems from operational testing: A study on autonomous
vehicles,” Information and Software Technology, vol. 128, p. 106393,
2020.
[7] B. Littlewood, K. Salako, L. Strigini, and X. Zhao, “On reliability
assessment when a software-based system is replaced by a thought-
to-be-better one,” Reliability Engineering & System Safety, vol. 197, p.
106752, 2020.
[8] European Committee for Electrotechnical Standardization, “EN 50126:
railway applications – the speciﬁcation and demonstration of reliability,
availability, maintainability and safety (rams),” 2017.
[9] P. Bishop, R. Bloomﬁeld, B. Littlewood, A. Povyakalo, and D. Wright,
“Toward a formalism for conservative claims about the dependability of
software-based systems,” IEEE Transactions on Software Engineering,
vol. 37, no. 5, pp. 708–717, 2011.
[10] L. Strigini and A. Povyakalo, “Software fault-freeness and reliability
predictions,” in Computer Safety, Reliability, and Security, ser. LNCS,
F. Bitsch, J. Guiochet, and M. Kaˆaniche, Eds., vol. 8153.
Berlin,
Heidelberg: Springer Berlin Heidelberg, 2013, pp. 106–117.
[11] X. Zhao, V. Robu, D. Flynn, K. Salako, and L. Strigini, “Assessing
the Safety and Reliability of Autonomous Vehicles from Road Testing,”
in the 30th Int. Symp. on Software Reliability Engineering.
Berlin,
Germany: IEEE, 2019, pp. 13–23.
[12] C. Atwood, J. LaChance, H. Martz, D. Anderson, M. Englehardt,
D. Whitehead, and T. Wheeler, “Handbook of parameter estimation for
probabilistic risk assessment,” U.S. Nuclear Regulatory Commission,
Washington, DC, Report NUREG/CR-6823, 2003.
[13] B. Littlewood and L. Strigini, ““validation of ultra-high dependability...”
– 20 years on,” Safety Systems, The Safety-Critical Systems
Club Newsletter, vol. 20, no. 3, May 2011.
[Online]. Available:
https://openaccess.city.ac.uk/id/eprint/6552/
[14] R. Soyer, “Software reliability,” WIREs Computational Statistics, vol. 3,
no. 3, pp. 269–281, 2011.
[15] B. Littlewood, “How to measure software reliability and how not to,”
IEEE Transactions on Reliability, vol. R-28, no. 2, pp. 103–110, 1979.
[16] B. Littlewood and J. Rushby, “Reasoning about the reliability of diverse
two-channel systems in which one channel is ‘possibly perfect’,” IEEE
Tran. on Software Engineering, vol. 38, no. 5, pp. 1178–1194, 2012.
[17] P. Popov, “Bayesian reliability assessment of legacy safety-critical
systems upgraded with fault-tolerant off-the-shelf software,” Reliability
Engineering & System Safety, vol. 117, pp. 98 – 113, 2013.
[18] K. Salako, “Loss-size and reliability trade-offs amongst diverse
redundant binary classiﬁers,” in Quantitative Evaluation of Systems,
M. Gribaudo, D. N.
Springer
International Publishing, 2020, pp. 96–114.
[Online]. Available:
https://doi.org/10.1007/978-3-030-59854-9 8
Jansen, and A. Remke, Eds.
[19] J. Berger, E. Moreno, L. Pericchi, M. Bayarri, J. Bernardo, J. Cano,
J. Horra, J. Mart´ın, D. Rios, B. Betr`o, A. Dasgupta, P. Gustafson,
L. Wasserman, J. Kadane, C. Srinivasan, M. Lavine, A. O’Hagan,
W. Polasek, C. Robert, and S. Sivaganesan, “An overview of robust
bayesian analysis,” Test, vol. 3, pp. 5–124, 06 1994.
[20] D. Insua and F. Ruggeri, Robust Bayesian Analysis, ser. Lecture
Springer New York, 2012. [Online]. Available:
Notes in Statistics.
https://doi.org/10.1007/978-1-4612-1306-2
[21] X. Zhao, B. Littlewood, A. Povyakalo, and D. Wright, “Conservative
claims about the probability of perfection of software-based systems,” in
26th Int. Symp. on Software Reliability Eng.
IEEE, 2015, pp. 130–140.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:48 UTC from IEEE Xplore.  Restrictions apply. 
462