n
a
h
c
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
m
u
C
s
e
g
n
a
h
c
h
t
a
p
f
o
r
e
b
m
u
N
 0.8
 1
path changes (y1)
cummulative (y2)
 18
 16
 14
 12
 10
 8
 6
 4
 2
 0
 0
0.995 0.996 0.997 0.998 0.999 1.000
 0.6
 0.4
 0.2
(b) Router Time Bin
(c) DC Pair Time Bin
s
e
g
n
a
h
c
h
t
a
p
f
o
r
e
b
m
u
N
s
e
g
n
a
h
c
h
t
a
p
f
o
r
e
b
m
u
N
(a) Link
path changes (y1)
cummulative (y2)
 9
 8
 7
 6
 5
 4
 3
 2
 1
0.985
0.988
0.991
0.994
(a)Link Time Bin
0.997
 0
1.000
s
e
g
n
a
h
c
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
m
u
C
s
e
g
n
a
h
c
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
m
u
C
Figure 10: Histogram and cumulative path change per time bin per link, router and dcpair
.
s
e
R
x
a
M
0
6
0
4
0
2
)
%
(
n
o
i
t
a
z
i
l
i
t
U
k
n
L
i
0
Mon Tue Wed Thu
Fri
Sat
Sun
Time
70 % traffic (y1)
99 % traffic (y1)
path change (y2)
LSPs in spike (y2)
 120
 100
 80
 60
 40
 20
 0
t
n
u
o
c
h
t
a
P
Figure 11: Link utilization and path change count as a function
of time of day for one week period
Correlation with link utilization. Finally we study the impact of
link utilization on LLPC’s and latency inﬂation. Again we divide
the time into 1-hour bins. In Figure 11, the left y-axis shows the
70th and 99th-percentile link utilization in each time bin during
one week (the MaxRes line shows the maximum allowed reser-
vation in links: conﬁgured at 85%). The right y-axis shows the
number of LLPC’s and the number of LSPs in latency spike in each
time bin during the same period. We observe that although MSN is
generally over-provisioned (70th-percentile link utilization is only
around 20%), there are always certain links which are fully sat-
urated (99th-percentile link utilization is around 100%). This is
somewhat surprising because we would expect the trafﬁc load is
more evenly distributed during off-peak hours when there is abun-
dant spare capacity. However, remember that in autobandwidth
each LSP greedily looks for the shortest path, therefore it is pos-
sible that many LSPs are competing for just a few “critical” links
even though other links are left idling.
Figure 11 also illustrates the number of LLPC’s and number of
Table 2: Impact of MPLS autobandwidth parameters
Parameter
min bw
max bw
Low
Large # of path changes
No guarantee of trafﬁc de-
livery if high requirement
setup priority Harder to ﬁnd better path
hold priority
adjust thres.
subscription
# of LSPs
Easily give up current path
High # of path changes
Less headroom for LSPs
High trafﬁc per LSP
High
reserved bw wastage
harder to ﬁnd new path
(same as static LSP)
Easy to ﬁnd better path
Stick to current path
LSP growth is harder
Resource wastage
High LSP overhead
LSPs in latency spike are strongly correlated with link utilization
while all of them exhibit a clear time-of-day pattern. This con-
forms to our expectation since more LSPs will be forced to switch
to longer path when the overall network utilization becomes higher.
5. EFFECT OF AUTOBW PARAMETERS
LSP path changes and corresponding latency spikes are the direct
consequences of both the autobandwidth algorithm and individual
LSP’s conﬁguration parameters. Given the large number of LSPs in
an OSP network, it is a common practice for operators to manually
conﬁgure the LSP parameters with some static values which rarely
change thereafter. In many cases, operators simply use the default
values set by router vendors, e.g., Cisco and Juniper. We study
the impact of different LSP parameters on LSP latency spikes and
summarize our ﬁndings in Table 2.
LSP priority We studied a few large latency spikes in LSPs travers-
ing nearby DCs (as in ﬁgure 1). In this set of latency spikes, the
LSPs, instead of traversing the direct shortest path between the
DCs, traverse a considerably longer path (across the US). This was
because the reserved bandwidth of the direct shortest path between
the DCs was exhausted by another sets of LSPs with (mostly) equal
or higher priority. Some of these LSPs traversed long distance
and had several different path options available. Their decision
to choose the particular link resulted in saturation of its reserved
bandwidth. Later due to a bandwidth increase of a nearby DC LSP,
467autobandwidth moved the LSP to a much longer path. The situa-
tion gets complicated when several such improper path selections
form a chain of dependencies.
The spike in cases like these could have been mitigated by in-
creasing the priority of the LSP between the nearby DCs. But set-
ting priority across thousands of LSPs to globally minimize latency
in the network is a hard problem. Further, it is also unclear whether
a static set of priorities would be sufﬁcient to reduce the problem.
As a future work, we plan to investigate how to automatically ad-
just priorities for the LSPs in latency spike in an online manner,
to force them to switch to a shorter path, while ensuring minimal
impact on other LSPs.
“All or nothing” autobandwidth policy The second cause for la-
tency spikes in LSPs stems from the ’all-or-nothing’ policy of au-
tobandwidth algorithm. This severely impacts high-volume LSPs.
A bandwidth increase of an LSP, running on a short path, where
at least one of the links is close to its reservation limit, forces the
entire LSP to another, long latency path. This results in the entire
LSP trafﬁc to traverse the long path even though the short path is
capable of carrying most of the trafﬁc.
As a part of future work, we plan to devise algorithms to split
LSPs in such cases (currently done manually in some networks [10]).
When an entire LSP will be forced to switch to a long path due to
its trafﬁc demand increase, we could subdivide the LSP (on-the-ﬂy