physical network has per-priority ﬂow control, allowing the
network administrator to conﬁgure one or more priorities as
lossless. The physical per-priority ﬂow control is extended
into the virtual domain by our proposed zOVN hypervisor
software.
Without loss of generality, to simplify the description, we
assume that a single lossless priority is used. In a real setup,
diﬀerent priority classes can be conﬁgured to segregate loss
tolerant traﬃc, from mission-critical latency-sensitive traﬃc
that beneﬁts from losslessness, as shown in the next sections.
4.1 Path of a Packet in zOVN
The data packets travel between processes (applications)
running inside VMs. Along the way, packets are moved from
queue to queue within diﬀerent software and hardware com-
ponents. Here we describe the details of this queuing system,
with emphasis on the ﬂow control mechanism between each
queue pair. The packet path trace is shown in Figure 5.
After processing the packets in the VM’s guest kernel,
they are transferred to the hypervisor through a vNIC. The
hypervisor forwards them to the virtual switch, which pro-
vides the communication between VMs and the physical
adapter. Packets destined to remote VMs are taken over
by a bridge with OVN tunneling functionality that encap-
sulates and moves them into the physical adapter queues.
After traversing the physical network, they are delivered to
the destination server, where they are received by the remote
bridge, which terminates the OVN tunnel by decapsulating
and moving them into the destination’s virtual switch input
queues. The virtual switch forwards the decapsulated pa-
ckets to the local hypervisor, which in turn forwards them
to the guest OS. After processing in the guest kernel, the
128 uplinks128 uplinks128 uplinks128 uplinks16 servers16 servers16 servers16 serversToRSwitchesChassisSwitchesCoreSwitchesRack 1Rack 2Rack 3Rack 4427The Qemu networking code contains two components: vir-
tual network devices and network backends. We use the vir-
tio network device coupled to a Netmap [32] backend. We
took the Netmap backend code of the VALE [33] virtual
switch and ported it to the latest version of Qemu with the
necessary bug ﬁxes, mainly related to concurrent access to
the Netmap rings. We use a lossless coupling between the
device and the backend, avoiding – via conﬁguration ﬂags
– the lossy Qemu VLANs. Packets arrive at the vSwitch
TX queue of the port to which the VM is attached. The
vSwitch forwards packets from the TX queues of the input
ports to the RX queues of the output ports using a forward-
ing (FIB) table that contains only the MAC addresses of
the locally connected VMs. If the destination is found to be
local, the respective packets are moved to the corresponding
RX queue; else they are enqueued in the RX port corre-
sponding to the physical interface. From here, packets are
consumed by a bridge that encapsulates and enqueues them
in the TX queue of the physical adapter. Then the lossless
CEE physical network delivers the packets to the destination
server’s physical RX queue.
As shown in Section 2.3, none of the current virtual sw-
itches implement ﬂow control, as also conﬁrmed by our dis-
cussions with some of the virtualization vendors. Therefore
we have redesigned the VALE vSwitch to add internal ﬂow
control and to make the TX path fully lossless, as described
in Section 4.2.
4.1.2 Reception Path
The incoming packets are consumed and decapsulated by
the OVN tunneling bridge from the RX queue of the physical
NIC. Next, they are enqueued in the TX queue of the virtual
switch that forwards them to the RX queue corresponding
to the destination VM. This forwarding is again lossless, see
Section 4.2. The packets are consumed by the Qemu hyper-
visor, which copies them into the virtio virtual device. The
virtual device RX queue is shared between the hypervisor
and the guest kernel. The hypervisor notiﬁes the guest when
a packet has been received and the guest OS receives an in-
terrupt. This interrupt is handled according to the Linux2
NAPI framework. A softirq is raised, which triggers packet
consumption from the RX queue. The packet is transferred
to the netif_receive_skb function that performs IP rout-
ing and ﬁltering. If the packet is destined to the local stack,
it is enqueued in the destination socket RX buﬀer based on
the port number. If the destination socket is full, then the
packet is discarded. With TCP sockets this should never
happen because TCP has end-to-end ﬂow control that lim-
its the number of injected packets to the advertised window
of the receiver. UDP sockets, however, require additional
care. We modiﬁed the Linux kernel such that when the des-
tination socket RX queue occupancy reaches a threshold –
i.e., one MTU below maximum – the softirq is canceled and
reception is paused. Once the process consumes data from
the socket, reception is resumed. This ensures full lossless
operation for both TCP and UDP sockets.
4.2 zVALE: Lossless virtual Switch
As stated before, our lossless vSwitch is derived from
VALE [33], which is based on the Netmap architecture [32].
It has one port for each active VM, plus one additional port
for the physical interface. Each port has an input (TX)
queue for the packets produced by the VMs or received from
Figure 5: Life of a packet in a virtualized network.
received packets are eventually delivered to the destination
application. Based on a careful analysis of the end-to-end
path, we identiﬁed and ﬁxed the points of potential loss, la-
beled in white on black in Figure 5, i.e., the vSwitch and
the reception path in the guest kernel.
4.1.1 Transmission Path
On the transmit side, packets are generated by the user-
space processes. As shown in Figure 5, the process issues a
send system call that copies a packet from user space into the
guest kernel space. Next, packets are stored in an sk_buff
data structure and enqueued in the transmit (TX) buﬀer
of the socket opened by the application. The application
knows whether the TX buﬀer is full from the return value
of the system call, making this a lossless operation.
Packets from the socket TX buﬀer are enqueued in the
Qdisc associated with the virtual interface. The Qdisc stores
a list of pointers to the packets belonging to each socket.
These pointers are sorted according to the selected disci-
pline, FIFO by default. To avoid losses at this step, we in-
crease the length of the Qdisc to match the sum of all socket
TX queues. This change requires negligible extra memory.
The Qdisc tries to send the packets by enqueuing them into
the adapter TX queue. If the TX queue reaches a threshold
– typically one MTU below maximum – the Qdisc is stopped
and the transmission is paused, thus avoiding losses on the
TX path of the kernel. When the TX queue drops below
the threshold, the Qdisc is restarted and new packets can
be enqueued in the TX queue of the virtual adapter. Hence,
the transmission path in the guest OS remains lossless as
long as the Qdisc length is properly sized.
Our architecture is based on virtio technology [34], hence
the virtual adapter queues are shared between the guest ker-
nel and the underlying hypervisor software running in the
host user space. The virtio adapter informs the hypervi-
sor when new packets are enqueued in the TX queue. The
hypervisor software is based on Qemu [4] and is responsi-
ble for dequeuing packets from the TX queue of the virtual
adapter and copying them to the TX queue of the zOVN
virtual switch.
Applicationsocket TxbuffersendQdiscreturnvalueenqueuevNIC Txqueuestart_xmitstart/stopqueueGuest kernelHypervisorfreeskbPort BTx queuereceivereturnvaluevSwitchPort ARx queueforwardwake-upOVNbridgepoll & OVN encap.wake-upTx queuephysicalNICsendframephysicallinkreceivePAUSEApplicationsocket RxbufferreceivevNIC Rxqueuenetif_receiveskbGuest kernelHypervisorPort CRx queuesendsendcompletedvSwitchPort ATx queueforwardOVNbridgepoll & OVN decap.wake-upRx queuephysicalNICreceiveframephysicallinksendPAUSEpause/resumereceptionwake-up428Algorithm 1 Lossless vSwitch Operation.
• Sender (Ij)
while true do
P roduce packet P
if Input queue Ij f ull then
Sleep
else
Ij.enqueue(P )
start F orwarder(Ij)
end if
end while
• Receiver (Ok)
while true do
if Output queue Ok empty then
for all Input queue Ij do
start F orwarder(Ij)
end for
end if
if Output queue Ok empty then
else
Sleep
P ← Ok.dequeue()
consume packet P
end if
end while
• Forwarder (Ij)
for all packet P in input queue Ij do
output port k ← f wd table lookup(P.dstM AC)
if not Output queue Ok f ull then
Ij.remove(P )
Ok.enqueue(P )
wake-up receiver (Ok) and sender (Ij)
end if
end for
the physical link, and an output (RX) queue for the packets
to be consumed by VMs or sent out over the physical link.
The lossy state-of-the-art implementation forwards packets
from input to output queues as fast as they arrive.
If an
output queue is full, packets are locally discarded.
To make such a software switch lossless, we designed and
implemented the pseudocode shown in Algorithm 1. Each
sender (producer) is connected to an input queue Ij, and
each receiver (consumer) is connected to an output queue
Ok. After a packet has been produced, the sender checks
whether the associated input queue is full. If the queue is
full, the sender goes to sleep until a free buﬀer becomes avail-
able, else the sender enqueues the packet in the input queue
and then starts a forwarding process to try to push packets
from the input to the output queues. The forwarder checks
each output queue for available space. If a queue has room,
the forwarder transfers the packets to the output queue and
wakes up the corresponding consumers that might be wait-
ing for new packets. On the receiver side, the associated
output queue is checked; if not empty, a packet is consumed
from it, else the forwarding process is started to pull pack-
ets from the input queues to this output queue. If data is
actually pulled, it is consumed; else the receiver sleeps until
woken up by the sender.
The vSwitch is designed to operate in a dual push/pull
mode. When the sender is faster (than the receiver), it will
sleep most of the time waiting for free buﬀers, while the
receiver will wake it up only when it consumes data. When
the receiver is faster (than the sender), it will sleep most of
the time, while the sender will wake it up only when new
Figure 6: Partition-Aggregate (PA) application.
data becomes available. The overhead of lossless operation
is thus reduced to a minimum.
5. EVALUATION
In this section we evaluate our proposed lossless vSwitch
architecture, applying the Partition-Aggregate (PA) work-
load described in Section 5.1. We run this workload both
in two lab-scale experiments with 32 VMs and in a larger-
scale simulation using an OMNeT++ model of a 256-server
network.
5.1 Partition-Aggregate Workload
A generic 3-tier PA application is presented in [8, 41] and
illustrated in Figure 6. At the top tier, a high-level aggrega-
tor (HLA) receives HTTP queries from external clients (1).
Upon reception of such a request, the HLA contacts ran-
domly selected Mid-Level Aggregators (MLA) and sends
them a subquery (2). The MLAs further split the sub-
query across their workers, one in each server in the same
chassis (3). Eventually, each worker replies to the MLA
by returning a response. The MLA collects the partial re-
sults from workers. When all results have been received,
the MLA sends its aggregated response to the HLA. The
query is completed when the HLA receives the aggregated
response from each MLA. The key metric of interest is the
ﬂow (or query) completion time, measured from arrival of
the external query until query completion at the HLA. In
the prototype experiments, similar with the experiments de-
scribed in [8, 41], we use a reduced two-tier PA workload, in
which the MLAs have been omitted, and the HLAs contact
the workers directly. In the simulations, on the other hand,
we use the full conﬁguration. In both cases, the ﬂows are
sent over TCP. The connections between the various com-
ponents are kept open during the runs to allow TCP to ﬁnd
the optimal congestion window sizes and to avoid slow start.
5.2 Microbenchmarks
First, we deployed our prototype implementation on two
Lenovo M91p-7034 desktops (Intel i5-2400 @ 3.10GHz CPU,
8GB memory, Linux 3.0.3 64-bit kernel both for host and
guests). The machines were connected through a 1 Gbps
3com 3GSU05 consumer-level Ethernet switch supporting
IEEE 802.3x. The host kernel was patched with the Netmap
[32] extensions and our zOVN switch and bridge. The guest
kernel was patched with our lossless UDP socket extension.
We ran PA queries with a single aggregator and ﬁve work-
ers. In Figure 7 we report the mean query completion time.
In Figure 7a the aggregators and the workers resided in VMs
on the same server (1-server setup), whereas in Figure 7b
the aggregator was on a diﬀerent server than the workers
(2-server setup). We varied the size of the workers response
WorkerMid LevelAggregatorWorker33...WorkerMid LevelAggregatorWorker33...High LevelAggregatorExternal Clients...221429(a) 1 server
(b) 2 servers
Figure 7: Microbenchmarks: 6 VMs PA.
(a) Background ﬂow size.
(b) Inter-arrival times.
Figure 8: Flow size and inter-arrival distribution.
to the aggregator from 2 to 2048 MTUs. To achieve statis-
tical conﬁdence, each run consisted of 10K repetitions. We
compared the Linux Bridge [2] with the lossy VALE imple-
mentation [33] and our proposed lossless zOVN. On the 2-
server setup, the Netmap-based solutions outperformed the
Linux Bridge, but only for small response sizes (up to 30%
for 2 MTUs). For medium-sized ﬂows, the Linux Bridge
was better (e.g., 8% performance degradation for 64 MTUs
when using zOVN). For large response sizes, the three im-