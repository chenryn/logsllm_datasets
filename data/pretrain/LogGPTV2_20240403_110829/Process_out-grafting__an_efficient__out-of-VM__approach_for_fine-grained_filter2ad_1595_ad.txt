especially for closed-source OSs.
To better understand the eﬀectiveness, we perform a com-
parative study. Speciﬁcally, we ﬁrst run strace inside the
production VM to monitor invoked system calls from thttp
when it handles an incoming HTTP request. After that, we
out-graft thttp and run strace in the security VM for out-
of-VM monitoring when it handles another incoming HTTP
request. Our results are shown in Figure 4. From the ﬁgure,
we can verify that both strace runs lead to the same system
call patterns in the handling of incoming HTTP requests by
accurately capturing system calls invoked by the same thttpd
process and interpreting each related argument.
4.2.2 Tracing User-level Library Calls
In our second experiment, we show the capability of reusing
existing tools for user-level library call tracing. User-level
library call tracing is a ﬁne-grained monitoring technique
that allows for understanding which library functions are
being used by a running process and what are their argu-
ments.
It has advantages over system call tracing in col-
lecting semantically-rich information at a higher abstraction
level.
As one can envision, the number of library functions avail-
able to a program and the type and deﬁnition of each argu-
ment for such functions can be very large. This can make
it complex, expensive, or even impossible to examine such
events in a semantically-rich manner using prior “out-of-VM”
approaches. Fortunately, in our approach, we can simply
re-use an existing tool ltrace to intercept and interpret user-
level library calls of a running process in one VM from an-
other diﬀerent VM. In our experiment, we found that ltrace
extracts process symbol information from the process’ bi-
nary image on disk. As we mounted the production VM’s
ﬁlesystem read-only in the security VM, ltrace works natu-
rally with no changes needed. Our results show that ltrace
indeed accurately captures and interprets the user-level li-
brary calls invoked by the out-grafted thttpd process. In a
similar setting, we replace ltrace with gdb, which essentially
allows for debugging an in-VM process from outside the VM!
4.2.3 Detecting Malware Unpacking Behavior
Most recent malware apply obfuscation techniques to evade
existing malware detection tools. Code packing is one of the
popular obfuscation techniques [17]. To detect packed code,
370open source, we wrote a Linux tool that faithfully imple-
ments OmniUnpack’s algorithm. We stress that if OmniUn-
pack was previously available for Linux, this porting step
would not be necessary.
In our Linux porting, we do not
need to bridge the semantic gap or be aware of any prior
introspection techniques. Instead, we just envision a Linux
tool that will be used in-host. This experience also demon-
strates the beneﬁts from our approach.
In our test, we use the freely available UPX packer [5]
to pack the Kaiten bot binary [1]. In this experiment, we
also utilize a security-sensitive event trigger that initiates
out-grafting when a suspect process invokes the sys execve
system call. The trigger is placed such that just before the
system call returns to user-mode (to execute the ﬁrst in-
struction of the new code), KVM is invoked to out-graft the
process’ execution to the security VM. Inside the security
VM, we run the OmniUnpack tool to keep track of page
accesses by the process. Since the system calls invoked by
the process are also available for monitoring, OmniUnpack
successfully detects the packing behavior.
We highlight several interesting aspects this experiment
demonstrates. In the past, packer detection has required a
trade-oﬀ between tool isolation and performance overhead.
Speciﬁcally, in-host tools [26, 33], including OmniUnpack
can eﬃciently detect packing behavior, but they are vulnera-
ble to attack. “Out-of-VM” techniques [11] ensure packer de-
tection is isolated, but introduce very high overhead, largely
limiting its usability for oﬄine malware analysis, not on-
line monitoring. Using process out-grafting, we are able to
eﬀectively move an in-host tool “out-of-VM” without intro-
ducing signiﬁcant overhead while still providing the needed
isolation. Another interesting aspect is due to the fact that
OmniUnpack requires the NX bit in the guest-page tables.
For 32-bit Linux, this bit is available only if PAE support is
enabled. In our experiments, we enabled PAE only in the se-
curity VM, whereas in the production VM page tables, the
NX bit is still not available. Thus, if the monitoring tool
requires additional features or support from the underlying
OS, even if this support is not present in the production
VM, we can take advantage of it in the security VM. Also,
we point out that the process out-grafting happens at the
very beginning of its execution. When a process begins exe-
cution, most of its code pages are not yet mapped in by the
OS. As such, this experiment also thoroughly tests the page
fault forwarding mechanisms in our system (Section 2.3).
4.3 Performance
To evaluate the performance overhead of our system, we
measure two diﬀerent aspects: the slowdown experienced by
the production VM when a process is out-grafted for mon-
itoring as well as the slowdown to the out-grafted process
itself. The platform we use is a Dell T1500 system con-
taining an Intel Core i7 processor with 4 cores, running at
2.6 GHz, and 4 GB RAM. The host OS is 32-bit Ubuntu
10.04 (Linux kernel 2.6.32) and the guests run 32-bit Fedora
10 (Linux kernel 2.6.27). Both VMs are conﬁgured with 1
virtual CPU each. The production VM is conﬁgured with
2047 MB memory and the security VM is conﬁgured with
1 GB memory. Table 1 lists the detailed conﬁguration. In
all experiments, the two VMs are pinned to run on separate
CPU cores in the host (using the Linux taskset command).
Production VM overhead
First, we measure the
slowdown experienced by the VM (i.e. other normal pro-
Figure 4: Comparison of strace results of the thttpd
server when running inside the VM (on the bottom)
and out-grafted to another VM (on the top)
eﬃcient behavioral monitoring techniques such as OmniUn-
pack [26] have been developed to perform real-time monitor-
ing of a process’ behavior by tracking the pages it writes to
and then executes from. When the process invokes a “dan-
gerous” system call, OmniUnpack looks up its page list to
determine whether any previously written page has been ex-
ecuted from. If so, this indicates packing behavior, at which
point a signature-based anti-virus tool can be invoked to
check the process’ memory for known malware. Since Om-
niUnpack was developed only for Windows and also is not
371Table 1: Software Packages in Our Evaluation
Conﬁguration
Linux-2.6.32
Linux-2.6.27
integer suite
ab -c 3 -t 60
ab -c 1 -t 60
Name
Host OS
Guest OS
SPEC CPU 2006
Apache
thttpd
Version
Ubuntu 10.04
Fedora 10
1.0.1
2.2.10
2.25b
  10
   8
   6
   4
   2
   0
  −2
)
%
(
d
a
e
h
r
e
v
o
d
e
z
i
l
a
m
r
o
N
  −4
  −6
h
c
n
e
b
l
r
e
p
2
p
i
z
b
c
c
g
f
c
m
k
m
b
o
g
r
e
m
m
h
g
n
e
j
s
m
u
t
n
a
u
q
b
i
l
f
e
r
6
2
h
p
p
t
e
n
m
o
r
a
t
s
a
k
m
h
c
n
b
n
a
l
a
x
z
g
i
p
e
h
c
a
p
A
Figure 5: Production VM Slowdown with a Con-
tending Process Out-grafted
cesses running in it) when we out-graft an unrelated pro-
cess for monitoring. Speciﬁcally, we choose a standard CPU
benchmark program, i.e., SPEC CPU 2006, and run it twice
(1) either with another CPU-intensive process that spins in
an inﬁnite loop inside the same VM (2) or with the CPU-
intensive process out-grafted to another VM. Our results
show benchmarks experience speedups after out-grafting the
CPU-intensive process. This is expected as a contending
process has been moved to execute in a diﬀerent VM for
monitoring, which is running on a diﬀerent core. This also
conﬁrms the monitoring overhead has been localized inside
the security VM, not the production VM. Next, we measure
the impact to Apache and pigz (or parallel gzip) when they
run together either with thttp out-grafted or not. In our ex-
periments, Apache and thttpd listen on diﬀerent TCP ports,
but their network traﬃc is handled by the same production
VM kernel. Our results are shown in Figure 5. It is interest-
ing to note that when thttpd is out-grafted, it is scheduled
more often (since both Apache and pigz dominate it when
they run in the same VM). However, the redirected system
calls from thttpd will not get serviced until the stub is sched-
uled for execution in the production VM, thus its impact to
the dominant processes is still low. Finally, we also mea-
sured the time it takes to identify the state for out-grafting
during which time the production VM is paused. While this
would vary depending on the memory size of a process, we
observed an average time of ∼ 250µs in our current exper-
iments. Opportunities still exist to further reduce it (e.g.,
with lazy updates – Section 5).
Out-grafted process slowdown Second, we measure
the slowdown an out-grafted process may experience due
to the fact that it is running in a diﬀerent VM. For this,
we ﬁrst measure slowdown in two out-grafted processes: (1)
The ﬁrst one is a simple ﬁle copy command that transfers a
tar ﬁle (421MB) from one directory to another in the pro-
duction VM, which will result in lots of ﬁle-accessing system
calls being forwarded. (2) The second is thttpd, for which we
generate traﬃc using the ab benchmark program. In each
set of experiments above, we have rebooted both VMs and
physical machine after each run to avoid any caching in-
terference. The average slowdown experienced by them is
35.42% and 7.38%, respectively. Moreover, we run a micro-
benchmark program to measure the system call delay expe-
rienced by sys getpid(). Our result shows that the average
time of invoking sys getpid() is ∼ 11µs. As sys getpid() sim-
ply obtains the process ID (after it has been forwarded), its
slowdown is approximately equivalent to the system call for-
warding latency.
5. DISCUSSION
While our prototype demonstrates promising eﬀectiveness
and practicality, it still has several limitations that can be
improved. For example, our current prototype only supports
out-grafting of a single process. A natural extension will
be the support of multiple processes for simultaneous out-
grafting. Note it is cumbersome and ineﬃcient to iterate
the out-grafting operation for each individual process. From
another perspective, simultaneous out-grafting of multiple
processes can also lead to the unique scenario where multiple
security VMs can be engaged in monitoring diﬀerent groups
of out-grafted processes or diﬀerent aspects of behavior.
Also, our current way of handling sys execve() can be