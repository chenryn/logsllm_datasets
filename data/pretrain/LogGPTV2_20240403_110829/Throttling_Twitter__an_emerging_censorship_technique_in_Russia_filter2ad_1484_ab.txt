The replay system imposes a few restrictions intended to capture
437
IMC ’21, November 2–4, 2021, Virtual Event, USA
Diwen Xue et al.
Mobile ISP
Beeline
MTS
Tele2
Megafon
Throttled as of 3/11?
Yes
Yes
Yes
Yes
Landline ISP
OBIT
JSC Ufanet
JSC Ufanet
Rostelecom
Throttled as of 3/11?
Yes
Yes
Yes
No
Table 1: Vantage points in Russia used in our study
Figure 4: Original and Scrambled replay throughput
Figure 5: Sequence numbers as seen by sender and receiver.
“Gaps” correspond to intervals during which no packet is de-
livered to the receiver
stated, the same measurement results were obtained from all van-
tage points experiencing throttling. This high degree of uniformity
in our measurement results across different ISPs suggests that these
throttling devices might be centrally coordinated.
the nuances of the recording (such as inter-packet logic) but leaves
all other aspects to the TCP stack of each endpoint. Essentially,
traffic being replayed is identical to the recorded session, except
for the server IP address which has changed from Twitter to the
replay server. We highlight that the replay system does not perform
any DNS lookup, nor does it communicate with the actual Twitter
server in any way. The goal of the replay system is to detect any
content-based traffic differentiation policy along the network path
between the client and the replay server.
Figure 3 illustrates this setup. First, we collect a trace using packet
captures on the unthrottled vantage point while fetching an 383 KB
image from abs.twimg.com. We also record a trace where upload
traffic dominates the bandwidth by uploading the same image to a
server under our control, preceded by a Twitter Client Hello. Then,
we set up a replay server at our university and use all our Russian
vantage points as replay clients.
To establish a baseline for the throttling, for each vantage point,
we first replay the original Twitter traffic recording, which triggers
throttling (Original Trace in Figure 4). Next, we replay the same
recording but with each TCP payload byte inverted so that any
structure or keyword that may trigger the throttling is removed
(Scrambled Trace in Figure 4). The choice of using bit-inverted
replays as control was inspired by previous works [27, 28], which
found such technique was able to successfully evade DPI detec-
tion. With multiple experiments across different vantage points,
we found that the throttling throughput converges to a value
between 130 kbps and 150 kbps for both the download and the
upload replays.
6 REVERSE ENGINEERING THE THROTTLER
Upon confirming the presence of throttling, we conduct in-depth
measurements to understand the nuances of the throttling and to
reverse engineer the way it works. We note that, unless otherwise
438
6.1 Throttling Mechanism
We compare server-side and client-side packet captures (pcaps) of
throttled replay experiments to understand how the throttling is
implemented.
The throttler uses traffic policing: We find that the throttling
is implemented by dropping packets that exceed a rate limit. Figure 5
shows the sequence number evolution as seen on one Russian client
and the university server sending data. Comparing the sequence
numbers of the packets sent by the sender (red and blue dots) with
the ones delivered to the client (blue dots only), we find that packets
exceeding a certain rate limit are silently dropped in transmission,
resulting in “gaps” over five times the typical RTT.
On some cellular vantage points, we observe other throttling
policies in addition to the throttling of Twitter. For instance, on
Tele2-3G, all our upload traffic is slowed down using delay-based
shaping as illustrated by the smoothed curve in Figure 6, as opposed
to the saw-tooth shape that corresponds to loss-based policing. Note
that this observed upload bandwidth of 130kbps is not specific to
Twitter, as all traffic are being slowed down regardless of SNI or
destination. On the other hand, most download traffic is not affected,
except for Twitter traffic which triggers throttling behaviors similar
to what we observed from other ISPs.
When multiple throttling schemes are being used by different net-
work intermediaries at different network locations (e.g. the upload
slowdown on Tele2-3G could be due to the subscribed asymmetrical
Internet plan), pinpointing the reason and isolating one specific
throttling scheme from others can be difficult. In our case, even
though we ran multiple replay experiments on Tele2-3G, we were
not able to conclude whether a specific upload throttling policy
exists for Twitter, because all upload traffic was being throttled at a
slightly lower rate. We, therefore, exclude Tele2-3G when analyzing
upload measurements.
Throttling Twitter: An Emerging Censorship Technique in Russia
IMC ’21, November 2–4, 2021, Virtual Event, USA
6.2 Triggering the Throttling
To identify which packets of a connection and what parts of those
packets trigger throttling, we craft several different initial packet
sequences to send to the server and monitor the throttling.
The throttler parses network packets from both directions
and throttles the connection upon observing a sensitive TLS
SNI in a Client Hello: We start by testing if a sensitive SNI in
Client Hello record alone is enough to trigger throttling. To test
this, we replay a traffic capture with a Twitter domain in the TLS
SNI between a Russian client and our server outside the country.
Next, we randomize all packets of the same traffic capture except
the Client Hello. In both cases, we observe throttling on the connec-
tion, suggesting that a sensitive Client Hello is sufficient to trigger
throttling. Furthermore, we find that a Client Hello with a Twitter
SNI sent by the replay server also triggers the throttling, suggesting
that the throttler inspects both upstream and downstream traffic.
We investigate the symmetry of throttling in § 6.5.
We conduct measurements to understand if the throttler ever
stops looking for a trigger. We prepend a packet with random bytes
of varying sizes before the triggering Client Hello. For all the trials
with the random packet size over 100 bytes, we did not observe any
throttling. This suggests that the throttler, upon seeing a packet that
cannot be parsed into any protocol it supports, will stop inspecting
the packets that follow, likely in an effort to conserve the DPI’s
resources. However, if we send any valid TLS record, HTTP proxy
packet, SOCKS proxy packet or a random packet with less than
100 bytes, the throttler continues to inspect for an additional 3-15
packets in the session. This behavior may be designed to target
circumvention techniques that work by inserting a fake Client Hello
(e.g. GoodbyeDPI [19], Zapret [58]) or by routing traffic through
unencrypted proxies.
Focusing on the triggering Client Hello, we follow a binary
search approach introduced by previous works [26, 28] to iden-
tify which parts of the packet are inspected by the throttler. We do
this by recursively masking (with inverted bits) half of the Client
Hello payloads in order to identify which bytes or fields within the
Client Hello trigger throttling. We find that if we mask fields such
as TLS_Content_Type, Handshake_Type, Server_Name_Extension,
or Servername_Type, the connection does not trigger throttling.
This suggests that the throttler inspects only certain TLS packets
(e.g. Client Hello containing SNI) and parses the packet for the
SNI, rather than simply regex-matching the Twitter domain string
over the entire packet. Moreover, tampering with TCP_Length,
TLS_Record_Length, or Handshake_Length thwarts the throttler,
suggesting that the throttler is not capable of reassembling frag-
mented TLS records.
6.3 Domains Targeted
In order to understand if other domains are being targeted, we test
the Alexa top 100k domains [2] by replacing them in the TLS SNI
field and see if the resulting sessions are throttled.
In the Alexa Top 100k, we find that only t.co and twitter.com
are throttled: We also find nearly 600 domains are outright blocked,
which suggests that blocking is still the primary means of censor-
ship in Russia.
439
Figure 6: Throughput graphs on Beeline and Tele2 display-
ing different throttling mechanisms
Focusing on all Twitter-affiliated domains, we test many permu-
tations of the domains known to be throttled, by adding periods
before and after the domains, and adding random prefixes/suffixes
to them. This step highlights details about string matching policy
used by the throttler when inspecting the SNI.
The throttling affects more domains than acknowledged by
Roskomnadzor: Early implementation had a loose string matching
policy that was corrected after reports suggested that reddit.com
and microsoft.com were also throttled [34]. While this was fixed
for t.co, we find that a more relaxed string matching is still in
effect for other domain strings. Specifically, domain strings such
as *.twimg.com and *twitter.com (e.g. throttletwitter.com)
continued to be throttled. However, according to our later measure-
ments on April 2, *twitter.com is no longer throttled except for
the exact matches (e.g. www.twitter.com, api.twitter.com). It
is also worth noting that in an official statement, Roskomnadzor
claimed that the throttling is only being applied to the “delivery of
audio, video content, and graphics”, and that other Twitter func-
tionalities are “delivered without restrictions” [43]. However, we
find that among the throttled domains is abs.twimg.com, which
hosts large Javascript files essential for Twitter to function.
6.4 TTL Measurement
To identify where in the network path throttling occurs, we employ
a TTL-based technique similar to traceroute. The IP Time To Live
(TTL) field controls how many network hops a packet can traverse.
In our measurement, each throttled vantage point establishes a TCP
connection with our university’s server. Then, using nfqueue [29],
we insert a Client Hello packet containing a triggering SNI with
increasing TTL values and attempt some data transfer. If we identify
some TTL value 𝑁 where we do not observe throttling but TTL
𝑁 + 1 results in throttling, then we infer that the throttler operates
between the 𝑁 and 𝑁 +1 hops. This technique allows us to estimate
the network location of the throttling infrastructure.
The throttling device is located close to end-users: For all
seven vantage points our test shows that the throttling devices op-
erate within the first five hops. Furthermore, in Beeline and Ufanet
cases, ICMP TTL-exceeded messages were returned from routable
IP addresses. We checked those IP addresses using BGP prefix and
ASN lookup, and we found hops both before and after throttling
IMC ’21, November 2–4, 2021, Virtual Event, USA
Diwen Xue et al.
Figure 7: Longitudinal percentage of requests throttled on vantage points
occurred were located inside the clients’ ISP network. This result is
consistent with a letter sent out by Roskomnadzor to ISPs, where it
indicates that the TSPU devices should ideally be installed before
carrier-grade NAT devices [20, 41]. Since the installation is close
to end-users, as opposed to being at country’s border link, domes-
tic traffic is also subject to inspection and censorship from TSPU
devices. For example, we confirm that a connection with a Twitter
SNI between two Russian hosts is throttled in the same way as if it
were a cross-border connection.
We use a similar technique to locate ISPs’ blocking devices as
well. In this case, we send crafted HTTP requests containing known
censored domains iteratively with increasing TTL values, which
would trigger blocking devices to return an ISP’s “blockpage”. In
networks where we can estimate the location of the blocking de-
vices, we find that they were between hops 5-8. As this differs
from our results for the throttling devices, it suggests that they
are not co-located and may be separately managed. Furthermore,
in some networks, we also find the throttling devices performing
reset-based blocking: on our Megafon vantage point, we observe
that once a triggering HTTP request passed hop 2 (the hop after
which throttling occurs), a TCP RST terminates the connection.
In addition, once the triggering request passes hop 4, the ISP’s
blockpage is returned. This suggests that the devices performing
throttling are also capable of blocking, and that they likely operate
independent of the ISP-controlled blocking devices.
6.5 Symmetry of Throttling
We investigate if the throttling is symmetric, i.e. does the throt-
tling equally affect traffic originating from Russia as well as traffic
coming into Russia? To do this, we modify Quack Echo, a remote
measurement tool that leverages echo servers within a censor-
ing country to measure censorship from outside the country [52].
Briefly, Quack Echo works by sending packets that are specifically
crafted to trigger DPI policies to servers running the echo protocol,
which, upon receiving the data, will reflect the data back to the
sender. We discover 1,297 Russian servers running the echo proto-
col on port 7, and use them in our Quack Echo measurements. We
did not observe any throttling when we connected to these echo
servers and sent triggering Client Hellos (which the servers echoed
back).
We followed up with our in-country vantage points, as we pre-
viously observed that even if the server sent a triggering Client
Hello, the connection was throttled. We discover that if the TCP
connection is initiated outside Russia to a server inside, we could
not trigger throttling. Throttling is triggered (by a Client Hello in ei-
ther direction) only if a local (in-ISP) client starts a TCP connection
with an outside server.
From this we conclude that throttling is not symmetric and
can only be triggered by connections initiated locally from
within Russia: This asymmetric nature of the throttling makes
it challenging for researchers to study from outside using existing
remote measurement tools [38, 48, 52].
6.6 Throttler’s State Management
Since throttling usually requires maintaining state, it is necessarily
limited by memory, disk space, CPU, etc. We are interested in learn-
ing about the policies that are used to determine when to discard a
state and stop monitoring the associated TCP session. We specif-
ically investigate whether the throttler discards an active (open,
data transfer below throttling rate) or inactive session (open, idle)
after a time period, or after observing a FIN or RST from either
endpoint.
The throttler maintains state for ≈10 minutes for inactive
sessions: For an open but inactive (no packets transferred) TCP
session, we find that after ≈10 minutes of inactivity, we do not
observe throttling. This 10-minute value corresponds to the result
we observe in most experiments and is not necessarily a precise
threshold of the throttler’s state management, which may depend
on a variety of factors such as the throttler’s operational load, size
of the network, etc. For active sessions, however, we still observe
throttling even two hours into the experiments. This suggests that
the threshold for active sessions may be much larger than for inac-
tive sessions, an observation consistent with previous studies [24].
Previous work also found that sending FIN-ACK or RST-ACK could
force some middleboxes into discarding a session’s state [24, 54].
However, based on our experiments, we found no evidence of the
throttler suspending monitoring after seeing a FIN or RST packet
from either endpoint.
440
Throttling Twitter: An Emerging Censorship Technique in Russia
IMC ’21, November 2–4, 2021, Virtual Event, USA
REFERENCES