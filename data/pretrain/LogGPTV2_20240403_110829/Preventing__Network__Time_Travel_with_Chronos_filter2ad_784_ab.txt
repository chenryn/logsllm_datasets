time to update its local clock to (see [1], [31]). This process
is described in Figure 2.
First,
the time sample with the lowest offset collected
from each server is identiﬁed and all other time samples are
discarded. Then, various “sanity checks” are performed on the
surviving samples so as to eliminate time samples from servers
that seem unhealthy or unsuitable for synchronization [1],
[31]. Then, Marzullo’s algorithm [28]–[30] is applied to ﬁnd,
within the set of remaining time samples, a “majority clique
of truechimers” [1], [31], i.e., a large cluster of servers with
(fairly) accurate clocks. This set of time samples can be further
pruned, with the aim of improving accuracy, by removing
all but some predetermined number of time samples that are
within the smallest distance of each other. The surviving time
Figure 2: NTP’s selection process
reliance on a small server pool, and (2) NTP’s algorithm for
determining the set of servers to sync to (and, in particular,
utilizing a variant of Marzullo’s algorithm [28]). We next
elaborate on each of these two elements.
designed to defeat determined intruders, these algorithms and
accompanying sanity checks have functioned well over the
years to deﬂect improperly operating but presumably friendly
scenarios.”
NTP clients typically rely on small server pools. Today’s
NTP clients typically generate a server pool by querying DNS
for the URL pool.ntp.org. To load balance NTP servers, instead
of responding with a comprehensive list of the IP addresses
of “close” NTP servers, the response contains few (typically
at most 4) IPs, which are replaced once an hour.
Consequently, a MitM attacker capable of intercepting
trafﬁc between the NTP client and fairly few servers can
succeed in shifting time at the client.
DNS responses are often cached, and so subsequent queries
to pool.ntp.org will often result in the exact same list of server
IP addresses. Hence, an attacker with MitM capabilities with
respect to these servers is likely to be able to preserve these
capabilities for extended periods of time.
We point out that reliance on small server pools is often
not merely “forced” on NTP clients because of insufﬁcient
options, as described above, but is also an explicit guideline.
Indeed, as prescribed in [12]: ”For NTP Pool Project members
to work properly, the NTP daemon needs at least three servers
conﬁgured. The project recommends a minimum of four, and
no more than seven sources.”
NTP’s algorithm for selecting the “best” time samples is not
sufﬁciently resilient to MitM attacks. NTP clients employ a
variant of Marzullo’s algorithm to ﬁnd a “majority clique of
truechimers” within the set of queries servers, [1], [31]. This
translates to seeking a set of at least half the queried servers
whose time samples are within a small distance from each
other.
The NTP speciﬁcation in [1], [31] explains that this al-
gorithm “uses Byzantine fault detection principles to discard
the presumably incorrect candidates, called falsetickers, from
the incident population, leaving only good candidates, called
truechimers”. Importantly, however, Marzullo’s algorithm was
not designed to withstand malicious (Byzantine) attacks, but
to “withstand errors such as communication failures and
inaccurate clocks” [28] (in contrast to, e.g., [7], [8], [44]).
Indeed, as pointed out in [1], [31], “While not necessarily
Marzullo’s algorithm, as adapted and employed in [1], [31],
is intended to ﬁnd a small time interval such that the time
samples of a majority of the queried servers are within the
interval. Hence, when the attacker has MitM capabilities with
respect to over half of the queried servers, the client’s is at the
attacker’s mercy. In fact, a sophisticated MitM attacker might
be able to successfully launch timeshifting attacks even when
in control of a lower fraction of the queried servers, e.g., (1)
if the attacker is able to provide time samples that survive
the NTP client’s “sanity checks” while honest servers with
(fairly) accurate clocks are removed from consideration, or (2)
by taking advantage of situations in which the time samples
of honest servers with (fairly) accurate clocks (while not far
from each other) are not very closely clustered.
We point out that requiring that seeking a majority (i.e.,
at least half) of queried servers whose time samples are close
to each other (as opposed to, e.g., a third, or two thirds) is
somewhat arbitrary. This choice of threshold (50%) is intended
to strike a balance between the desire to ﬁnd a time value that
a large fraction of the servers “agree on”, and the risk that, if
the threshold is set to be too high, such a time value might
not even exist .
Another vulnerability of NTP’s time-update algorithm is
derived from the fact that the local clock is updated to the
computed time value even if the new time value is “far” from
the current local time (see Figure 2). This implies that an
attacker who succeeds in contaminating the time samples can
successfully shift time at the client by many seconds, even
minutes (until the next time update occurs). See thorough
discussion in [24].
Chronos to the rescue. Chronos, our NTP client, addresses the
above two deﬁciencies of NTP’s architecture by (1) generating
large server pools (yet querying servers in these pools in
a communication-efﬁcient manner), and (2) replacing NTP’s
time-sample-selection algorithm with a scheme that is provably
more resilient to attacks.
4
III. CHRONOS
The Chronos NTP client is carefully designed to achieve
the following desiderata:
•
•
•
Provable security guarantees even against very pow-
erful man-in-the-middle attackers. Chronos is de-
signed to protect even against attackers capable of
compromising authenticated NTP servers.
Backwards-compatibility with today’s NTP servers.
Chronos only involves software changes to the NTP
client and is thus easy to deploy in practice.
Low computational and communication overhead.
Today’s NTP involves fairly little client-server com-
munication. Overloading NTP servers can result in
slower response times and, as a result, degraded
synchronization. Chronos is thus engineered to avoid
excessive overhead on both the NTP servers and the
Chronos clients.
To accomplish the above, Chronos’ design relies on trans-
lating ideas from the rich literature on time synchronization in
distributed computing into an operational reality.
We next provide a high-level overview of Chronos’ ar-
chitecture. We then elaborate on the main technical aspects
involved in Chronos’ design.
A. Overview
Our aim is to protect NTP from even the most powerful
form of MitM attacks, i.e., attackers capable of compromis-
ing fairly many (possibly authenticated!) NTP servers. Our
approach relies on a combination of several ingredients:
1)
2)
3)
relying on many servers (in contrast to today’s NTP
clients, which effectively often rely on small server
pools),
querying few servers so as to avoid excessive com-
munication overhead in a manner that limits a MitM
attacker’s ability to attain a large presence within the
set of queried servers, and
crowdsourcing time queries across servers by
leveraging ideas from distributed computing litera-
ture [7], [8], [44]. We next discuss the algorithmic
and operational aspects involved in Chronos’ design,
and on how Chronos contends with these challenges.
Generating a large pool of NTP servers to sync to. As
discussed in Section II, an NTP client gathers time samples
from a pool of servers, which are then used to generate a new
local time. Today’s NTP clients typically rely on a small pool
of servers (e.g., 4−7 [12]), rendering them vulnerable to MitM
attacks. Chronos, in contrast, generates a large server pool (in
the order of hundreds) per client so as to set a very high
threshold for a MitM attacker, effectively forcing the attacker
to gain control of the trafﬁc between the clients and a large
fraction of the servers so as to be successful.
While a large server pool can be generated in multiple ways
(e.g., fed into Chronos by a trusted party), to avoid the need
for coordination with others, Chronos clients can also locally
utilize a simple approach leveraging DNS queries to create
server pools that consist of hundreds of servers. We detail the
speciﬁcs of this approach below and empirically evaluate its
success in Section V-A .
Querying a small subset of the server-pool. While Chronos’
security guarantees rely on generating a large server pool per
client, as explained above, querying each and every server
in the server pool
in
excessive overhead. Thus, Chronos is designed to strike a
delicate balance between communication and server load and
the attained security level. We prove that randomly querying a
small fraction of the servers in the server pool, as few as 15,
sufﬁces for attaining very good security guarantees.
is not advisable, as this will result
Sifting through the collected time samples. After gathering
time samples from a subset of the server pool, Chronos
discards “outliers” in the following manner: Suppose that m
per server samples are gathered. Chronos discards the d lowest-
value time samples and the d highest-value time samples for
some predetermined constant d. Chronos next examines the
remaining time samples and ensures that the following two
conditions are satisﬁed:
1)
2)
every two surviving time samples are “close” to each
other
the average time value of the surviving samples is
“close” to the local time at the client.
In the event that the above two conditions are satisﬁed,
Chronos updates the local time to be the average time value
across all m−2d time samples. Otherwise, the time samples are
discarded and the server pool is re-sampled in the exact same
manner. In the extreme scenario that the number of times the
pool is re-sampled exceeds a certain “Panic Trigger”, Chronos
enters a “Panic Mode”, which involves sampling all servers in
the pool.
Intuitively, this simple scheme, which resonates classical
ideas in distributed computing theory [7], [8], [44], poses a
challenge for the attacker, as explained next. Suppose that
the attacker manages to manipulate the time samples of a
fairly large fraction, yet not all, of the queried servers in the
pool. Reporting time values that are “too high” or “too low”
will result in these time samples being discarded by Chronos
and thus limit
the attacker’s ability to inﬂuence Chronos’
time computation. However, reporting time values that are not
outliers and that pass Chronos’ checks implies that the reported
values are close to those reported by NTP servers that the
attacker cannot manipulate (and are hence close to the UTC).
We formalize this intuition in Section III-B.
B. Chronos Design
We next present a more detailed exposition of Chronos’
design. We ﬁrst explain how Chronos generates a large server
pool. We then explain how the server pool is sampled and how
the new local time is computed.
The pseudocode for Chronos’ sampling scheme and time
computation, and the relevant terminology, are presented in
the speciﬁcation of Algorithm 1 and in Table I, respectively.
5
Choosing values for the parameters described in the below
exposition of Chronos is delicate and relies on both our
formal security analysis (Section IV) and empirical analyses
(Section V). We discuss this issue in detail in the subsequent
sections.
n
m
d
T
avg(T)
ω
Θ
∆t
ERR
tC
K
total number of servers in the server pool
number of servers chosen at random from the server pool
number of outliers removed from each end of the ordered
m samples
the set of m − 2d samples remaining after the removal of
the 2d outliers
the average value of the time samples in T
an upper bound on the distance from the UTC of the
local
time at any NTP server with an accurate clock
(“truechimer”)
an upper bound on the drift of the client’s local clock
[ms/sec]
the client’s estimate for the time that passed since its last
synchronization to the server pool [sec]
Θ·∆t
1000
the current time, as indicated by the client’s local clock [sec]
panic trigger
Table I: Relevant Notation
counter := 0;
while counter < K do
S := sample(m) // gather time samples from m
randomly chosen servers
T := bi-sided-trim(S,d) // trim d lowest and highest
values;
if (max(T ) − min(T ) <= 2ω) and
(|avg(T ) − tC| < ERR + 2ω then
end
counter++;
return avg(T).
end
// panic mode;
S := sample(n);
T := bi-sided-trim(S, n
return avg(T).
3 ) // trim bottom and top thirds;
Algorithm 1: Pseudocode of Chronos’ Time Sampling
Scheme
Generating the server pool. To generate a large server
pool, our current backwards-compatible realization of Chronos
executes the following procedure. The NTP client queries
pool.ntp.org on an hourly basis for 24 consecutive hours and
generates the union of all received IP addresses. Importantly,
this is executed in the background once in a long time (e.g.,
every few weeks/months). Our empirical results in Section V-A
demonstrate that server pools consisting of hundreds of servers
can be generated in such a manner.
Sampling the server pool and removing outliers. Chronos
periodically gathers time samples from the server pool as
follows. Chronos ﬁrst selects, uniformly at random, a subset of
size m of the servers in the server pool. Out of the collected
m samples, the d lowest-value samples and d highest value
samples are discarded. Our security analysis in Section IV-B
establishes that setting m to be in the range 15 − 50 and d to
be m
3 yields good security guarantees.
6
Checking samples for consistency. Let Θ, measured in
msec/sec, be an upper bound on the drift of the local clock at
the client, i.e., the worst rate at which the correctly functioning
local clock desynchronizes from the universal time (UTC). The
current implementation of the NTP client relies on a local
estimation of Θ. Speciﬁcally, today’s NTP clients keep track
of the local clock drift (at /var/lib/ntp/ntp.drift, see, e.g., [12])
and can set Θ to be an upper bound on the locally-perceived
drift. Identifying better ways of assessing the local drift is