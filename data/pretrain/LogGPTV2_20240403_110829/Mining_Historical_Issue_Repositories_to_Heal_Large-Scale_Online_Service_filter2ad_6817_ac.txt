57
57
43
39
31
9
3
2
2
needs high-conﬁdence evidence but also there are totally
9 types of healing actions (as listed in Table II), which
could be instantiated to form a non-trivial number of
possible healing actions in the search space. Hence
inspecting our suggested healing action(s) (e.g.,
the
healing action identiﬁed from the top-k most similar
historical issues) could apparently reduce the time-cost
on the two aspects.
IV. EXPERIMENTAL EVALUATION
In our evaluations, we intend to answer three research
questions:
• RQ1. How effectively can our approach suggest ap-
propriate healing actions for the given new issues?
• RQ2. How well does our technique for addressing
the high-correlation phenomenon contribute to the
overall effectiveness of our approach?
• RQ3. How well does our technique for addressing
the weak-discrimination phenomenon contribute to
the overall effectiveness of our approach?
A. Experiment Setup
We evaluate our techniques in a released produc-
tion service, named ServiceX. ServiceX is a customer-
facing, geographically distributed, 24 x 7, 3-tier online
service, with ﬁve datacenters around the world. We next
describe the collection of trace data and deﬁnition of
evaluation metrics for our evaluations.
We randomly sampled 400 issues from the resolved
issues which are detected by the internal monitoring
system in the year of 2012. Among these issues, 243
issues are with clear resolutions and transaction logs,
and these issues are valid for our evaluations.
The healing actions for the 243 issues are categorized
into 9 categories based on the combination of their verb
and target information as shown in Table III. In the
categorization, we do not consider or list the location
information in the healing actions since it is different
across different issues.
1) System Topology: The ServiceX system includes
ﬁve datacenters. Each datacenter can be represented
by a hierarchical topology, with many “farms” as the
leaf node. Each farm is a unit of full functionality for
316316316
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 
serving a set of customers, which contains servers of
multi-roles (e.g., WFEs, SQLs). The transaction logs
of each individual server are temporarily stored in the
corresponding local machine for a certain period of
time (the detailed information cannot be exposed due
to Microsoft conﬁdential). According to a topology-
manager, which maintains the mapping from logical
server role to the speciﬁc physical machine, we can
query the corresponding transaction logs by given a
speciﬁc server name. The service issues are triggered
by monitoring system as well as real customers. When
an issue is triggered, it is sent to a global historical
issue repository named as RepX. Each issue in RepX is
associated with an issue ID, affected farm, and time
period, etc. Such associated information is used to
identify transaction logs for the issue.
2) Metrics: To comprehensively evaluate our ap-
proach, we design two-scenario strategies which mimic
the two major real usages of our technique. In both
scenarios, we in-turn treat each of the 243 issues (in
the order of their occurring time) as a “new issue”. But
the two scenarios differ in what issues we choose as the
“historical issues”.
In Scenario I, we reﬂect real usage of our approach
in practice by treating the previously encountered issues
(i.e., those that occurred before “new issue”) as the
“historical issues”. We then apply our approach for each
combination of “new issue” + “historical issues” and
then measure the accuracy of our approach’s effective-
ness in suggesting a correct healing action for the “new
issue”.
In Scenario II, we adopt the “leave-one-out” strategy
(a common strategy used in statistical analysis) by
treating all the remaining issues (other than the “new
issue”) as the “historical issues”. Scenario II is used for
building a knowledgebase which manages all historical
issues.
Note that in our results, the location info for a sug-
gested healing action is always correct for each issue,
because only an unhealthy service would produce ev1
– ev4 (see Table II). Therefore, the retrieval accuracy
is critical in the overall effectiveness of our approach:
if the healing action for the retrieved similar historical
issue for a “new issue” is in the same category as
the correct healing action for the “new issue”, then
the healing suggestion is correct (since the location
information of the healing action is assured to be correct
as described earlier).
3) Experiment Design: To answer the second and
third research questions, we apply two approaches (vari-
ants of our approach) in short as App1 and App2 besides
applying our approach in short as Ours. In App1, we
do not address the high-correlation phenomenon: we
calculate Mutual Information of each individual event as
its weight (using contrast info), then represent the events
as a vector, and ﬁnally calculate the cosine score as the
similarity metric value. In App2, we do not address the
weak-discrimination phenomenon: we ﬁrst apply FCA
and use delta events between parent and child concepts
to deﬁne terms (using grouping information), use TF-
IDF as the weight of each term, and ﬁnally calculate
the cosine core as the similarity metric value.
Detailed Design of Scenario I: We design an
experiment that estimates the accuracy of top 1 similar
issue being retrieved by our approach to mimic one main
scenario when our technique is used in real practice.
We ﬁrst sort all the issues by the occurring time from
the earliest to the latest. Then for each approach, we
initialize the score as zero. Then for each “new issue”
qi, we check the top 1 similar “historical issue” (here the
historical issues refer to the issues that occurred earlier
than qi) retrieved by our approach: if the retrieved “his-
torical issue” belongs to the same category of healing
actions as the “new issue”, we increase the score by one.
At the end of all iterations, for each qi, we can attain
an average score, which reﬂects the average accuracy of
healing suggestion at the time point of qi. We draw such
curve with X-axis being the index of the sorted issues
and Y-axis being the average accuracy at the time point
of the corresponding qi.
In Scenario I, we also evaluate the cost performance
of our approach. The runtime cost of our approach con-
sists of two parts: signature generation and retrieval. In
the part of signature generation, we apply our signature-
generation algorithm to only a new issue. After the
signature is generated, we store it into text indexed by
issue IDs. In practice, loading signatures of historical
issues is very fast, so we can ignore the loading time.
In the part of retrieval (i.e., similarity calculation), as the
number of historical issues grows, the time complexity
of this part grows linearly.
Detailed Design of Scenarios II: In our evalua-
tions, for each combination of each approach (Ours,
App1, and App2), each “new issue” qi selected from
healing-action category catk, and a given similarity
threshold s, we measure the precision for our approach’s
effectiveness in suggesting correct healing actions:
#retrivals in catk(similarity > s)
pre(qi, s) =
#retrievals(similarity > s)
Then we measure average precision for catk:
pre(catk, s) = average(pre(qi)),∀qi ∈ catk
We measure the average precision for all categories:
pre = average(pre(catk, s)),∀catk
In our evaluations, we set s from 0.6 to 0.995,
with 0.05 as each increment step. Then we get the
317317317
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 
APP1 SUGGESTS CORRECT HEALING ACTIONS, WHEREAS OUR
Table IV
APPROACH DOES NOT
Issue#
153
178
Top1 Similarity (Ours)
0.02
0.24
Top1 Similarity (App1)
0.27
0.24
OUR APPROACH SUGGESTS CORRECT HEALING ACTIONS,
WHEREAS APP1 DOES NOT
Table V
Figure 5. Accuracy of suggesting correct healing actions
highest precision for our approach, App1, and App2,
respectively.
In our target problem, recall is not a useful metric,
since the decision of the ﬁnal healing action is typically
not based on recall. According to the feedback from
engineers (short for product engineers, whose duties
are service diagnosis and recovering), they mainly care
about the retrieved candidates themselves, without car-
ing about the size of the categories that the speciﬁc
issues belong to, so we do not use recall as an important
metric.
B. Experiment Results
We next illustrate our experimental results of Scenar-
ios I and II, respectively.
1) Results of Scenario I: Experiments in Scenario
I address the accuracy of suggested healing actions
in real scenarios. Figure 5 shows the overall accuracy
trend for each approach. The X-axis is the index of
each issue (sorted by occurring time); the Y-axis is the
average accuracy of the issues between the ﬁrst one and
the current one. Higher accuracy values indicate better
effectiveness.
RQ1: Approach Effectiveness: The overall accu-
racy of our approach, App1, and App2 is 87%, 82%,
and 72%, respectively. Achieving the best effectiveness,
our approach correctly suggests healing actions for 213
issues. Figure 5 shows the trend of the average accuracy:
the curve of our approach is always on top of curves of
App1 and App2.
Note that there are at least 9 issues for which wrong
healing actions would be suggested, since these issues
are the ﬁrst issue of each of the total 9 categories, and
no previously encountered issue of the same category
is available for them to leverage. Such cases are repre-
sented by some points (e.g., the value of X-axis is 11,
17, 32) located at sharp drops in Figure 5.
Beside these 9 issues, our approach wrongly suggests
healing actions for 21 issues. We provide further inves-
tigation on these issues in Section IV.C.
The high accuracy of our approach is critical
to
enable auto-healing tasks. Although currently service
recovery heavily relies on manual efforts, product teams
are starting to deploy some scripts to apply healing
Issue#
14
45
79
83
86
90
91
124
Top1 Similarity (Ours)
0.91
0.68
0.82
0.71
1.00
1.00
0.60
1.00
Top1 Similarity (App1)
0.49
0.97
0.87
0.28
0.61
0.96
0.75
0.74
actions automatically, e.g., deploying a script
in a
dedicated management machine to command the IIS
of a remote service to restart. We can then map our
suggested healing action to its corresponding script,
which is deployed to accomplish service auto-healing.
RQ2: Concept-Analysis Effectiveness: The blue-
colored (middle) curve in Figure 5 is for App1. Our
approach’s curve is on top of it at each value of X-axis.
App1 correctly suggests healing actions for 207 issues,
whereas our approach correctly suggests healing actions
for 6 more issues in total.
Table V and IV list all
the issues with different
suggestions between our approach and App1 (including
the issues that our approach gives correct suggestions
but App1 does not, and those vice versa). There are 2
issues that App1 gives correct suggestions (the ﬁrst 2
rows), and 8 other issues that our approach gives correct
suggestions (the last 8 rows).
To understand the reasons for such different sugges-
tions, we conduct further investigation. Table V and IV
further lists the top1 similarity score for each issue,
computed by our approach and App1, respectively. We
can observe that the 2 issues that App1 performs better
are trivial; the similarity score there is low: 0.27 is
the highest score. Such low score indicates that most
parts of the signatures between the current issue and
the top1 similar issue are not that similar (recall that
in the experiment design of Scenario II, we set the
similarity threshold to 0.6 as the lower bound). Further
investigation of the detailed log events and messages
conﬁrms with such observation: in fact, the two issues
(#153 and #178) are “outliers” compared with other
issues, although correct healing actions are suggested
for them in the end.
On the other hand, all the 8 issues that our approach
performs better have at least 0.6 of top 1 similarity
score, and most scores are even close to 1.0. Further
investigation of these issues conﬁrms that the current
318318318
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 
issue and the most similar issue are indeed related, App1
does not suggest correct healing actions because of its
main weakness: the bias of several terms of large size
(i.e., a signature consisting of a large number of events).
One typical example is the issues in category ID7,
each describing a speciﬁc service trouble named as
an “ADO.NET” issue. These results show that our
approach’s overall effectiveness beneﬁts from address-
ing the high-correlation phenomenon. Dominating terms
(i.e., those with signiﬁcantly larger weight than the other
terms of the same issue) of this category are {x} and
{y1 y2 y3 y4 y5}. When the execution of a transaction
instance goes through event {x} and {y1 – y5}, such
issue of a swift timeout can be reproduced. However, the
terms of many issues in the category of “SQL resource”
(ID4) are {z} and {y1 y2 y3 y4 y5}, with only one
event being different: “z” instead of “x”. Such difference
is small in quantity but is impactful. App1 can hardly
distinguish an issue from the category of “ADO.NET”
from an issue from the category of “SQL resource”, and
would report a high similarity score for these two issues,
leading to wrong healing suggestions. However, in our
approach, set {y1 – y5} contribute the same weight as
{x}, so the ﬁnal similarity score would not be biased
by a speciﬁc term of large size. Note that the healing
actions for the two categories “ADO.NET” and “SQL
resource” are different, being patching a machine and
rebooting an SQL machine, respectively (see Table III).
RQ3: Contrast-Analysis Effectiveness: The green
colored (bottom) curve in Figure 5 is for App2. App2
correctly suggested healing actions for 174 issues, with
its accuracy as about 72%. Our approach improves the
accuracy of App2 by about 21%, which is substantial. In
summary, the evaluation result shows that, considering
contrast information (i.e., fail/success of each request)
substantially contributes to the overall accuracy of our
approach.
Runtime Performance of Our Approach: We gener-
ate the signatures of the total 243 issues with the runtime
cost of 45,848ms; thus, on average, we generate the
signature for each issue with the runtime cost of about
189ms.
Figure 6 shows the run time cost of retrieving the
top1 historical issue for each issue. The x-axis shows
the issue index, and the y-axis shows the time cost (with
unit as ms). We can see that the speed of processing
the ﬁrst 150 issues is really fast (less than 50ms). As
the issue index grows, the runtime cost of the retrieval
grows, yet being still small (less than 250ms). Accord-
ing to the experiences from engineers, in practice, less
than 1 minute is already an acceptable bound for healing
services, since it is much less than the common MTTR
(the actual value of MTTR of ServiceX is not exposed
due to Microsoft conﬁdential).
Figure 6. Overall performance of our approach
Table VI
OVERALL PRECISION
Ours
App1
App2
similarity threshold
0.85
0.81
0.94
highest precision
0.87
0.81
0.58
The signature-generation part of App1 is 100 times
faster than the signature-generation part of our ap-
proach, since App1 calculates the mutual information of
only each individual event; however, the retrieval part
of our approach is 5 times faster than the retrieval part
of App1. One major reason is that our contrast analysis
eliminates most of irrelevant events; however, in App1,
the comparison of the new issue and a historical issue
can involve hundreds of unique events including many
irrelevant ones.
2) Results of Scenarios II: We use the similarity
threshold s as a parameter to get the pre s curve of each
approach. This part of evaluation complements to the
evaluation of Scenario I. To make the comparison fair,
we consider only the highest precision of each approach.
Table VI shows that the highest precision of our