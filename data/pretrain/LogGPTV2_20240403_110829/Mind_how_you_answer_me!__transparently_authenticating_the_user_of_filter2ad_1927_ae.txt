= 12.8% [10, 8]. We argue that not only this result is high (making
this method more insecure), but also the system requires long time
before it collects enough data to take a decision; again, this giving
the intruder enough time to access sensitive data. Finally, acoustic
(internal) ear recognition has the best EER between the cited meth-
ods. It enjoys an EER of some 5.5% [6]. In this case, the result is
slightly better than the one obtained by our method. However, we
remind that this method actually requires devices that are not avail-
able on current smartphones. Also, we underline that the result of
EER ∼ 5.5% were obtained in a speciﬁc experimental setting, not
inﬂuenced by external factors like hair, or users wearing things like
hats or veils. Hence, we can claim that the performances of our
basic solutions are comparable (even better in most of the cases)
to the ones of other authentication methods. Furthermore, we re-
mind that our solution can solve a very speciﬁc problem that the
other solution can not solve:
transparently authenticate the user
of a smartphone when she answers (or places) a phone call, using
devices already available on current smartphones, and without the
system being inﬂuenced by external factors like ears being covered.
5.2 Boolean Combinations
In this section, we report on the experiments we run in order
to evaluate the boolean combination approach discussed in Section
4.3.
Figure 7(a) (in Appendix) summarizes the results of all the pos-
sible AND combinations of the basic methods. As expected, in all
the possible combinations of two basic methods (say Method A and
Method B), resulting IPR and FAR satisfy Equation 3 and Equa-
tion 4, respectively. For example, let us see the case of the AND
(b) IPR
Figure 5: DTW-D-So, Varying (cid:28)D.
S-So, for which we observed behaviour similar to the one shown for
DTW-D-So. However, we report some results that summarize the
overall performances of all the basic methods. We observe that any
variation of the parameters T and (cid:28) might improve one of the two
metrics (FAR and IPR), while decreasing the other one. Since there
is no variation that at the same can improve both metrics, it is up
to the user of the system to prefer to have a smaller FAR (at a cost
of an higher IPR), or to have a smaller IPR (at a cost of an higher
FAR). To summarize the result we used to following method. For
each combination of the considered T and (cid:28), we measured the cor-
responding FAR and IPR. Then, we computed the average between
this two values. For each method, we looked for the parameters
setting (T and (cid:28)) giving the smallest average of the two metrics. In
Figure 6 we report, for each method, the IPR and FAR obtained in
this way. Also, for each method, we report on Table 3 the values
of T and (cid:28) for which we obtained the lowest average between FAR
and IPR.
We conclude this section by observing that the performances ob-
tained for the basic methods are comparable to the ones of other
transparent authentication methods (Section 2).
In fact, we ob-
tained for a single (not combined) method, DTW-D-So: IPR ∼
4.4% and FAR ∼ 9.3%. As an example of performances of concur-
rent transparent authentication system, we remind that gait recog-
nition (i.e. walking pattern) enjoys Equal Error Rate (EER) close
to 7% [16]. That is, the performances of walking patterns recog-
257
 0 10 20 30 40 50 60 70 80 90 0 5000 10000 15000 20000Rate(%)τT = 2T = 4T = 6T = 8T = 10T = 12T = 14T = 16T = 18T = 20 0 1 2 3 4 5 6 7 8 9 0 5000 10000 15000 20000Rate(%)τT = 2T = 4T = 6T = 8T = 10T = 12T = 14T = 16T = 18T = 20 0 20 40 60 80 100DTW-D-SaDTW-S-SaDTW-D-SoDTW-S-SoRate(%)MethodsIPRFARcombination of DTW-D-So (Method A) and DTW-D-Sa (Method
B). IPRA is some 4.4; IPRB is some 13.3; the resulting IPR of
the AND combination is some 1.5 (that is, smaller than the small-
est between IPRA and IPRB). On the other side: FARA is some
9.3; FARB is some 23.6; the resulting FAR of the AND combina-
tion is some 24.6 (that is, bigger than the biggest between FARA
and FARB). For the OR combination, results are shown in Figure
7(b) (in Appendix), we observed, as expected, the opposite effect
on the combined results. That is, resulting IPR and FAR behave
accordingly to Equation 5 and Equation 6, respectively.
Finally, we considered the combination where the user is ac-
cepted when at least n out of the four methods accept her (n =
1, 2, 3, and 4). The results are shown in Figure 7(c) (in Appendix).
From this ﬁgure we observe that, as expected, when we increase n,
fewer users get accepted—hence decreasing IPR, while increasing
FAR. For example, IPR goes from some 41.1 to some 0.6 moving
from n = 1 to n = 4. During the same variation of n, FAR goes
from some 1 to some 40.9.
5.3 Normalized Combinations
In this section, we report the results we obtained for the evalua-
tion of the combination of basic methods proposed in Section 4.4,
where we consider the normalized version of DTW-D and DTW-S
(cfr. Section 4.4.1 and Section 4.4.1, respectively). We run exper-
iments considering both of the following: (i) combining methods
two by two; (ii) combining all the four methods together. In case
(i), we considered (cid:11) = (cid:12) = 1 in Equation 9 (i.e. giving the two meth-
ods equal importance). In case (ii), we considered (cid:11) = (cid:12) = (cid:13) = (cid:14) =
1 in Equation 10. That is, each building block method inﬂuences in
the same way the overall decision whether the user’s pattern should
be accepted. For the threshold ^(cid:28) we used these values: -0.5, 0.0,
0.5, and 1.0.
The results obtained combining the methods two by two are shown
in ﬁgures 8(a), 8(b), 8(c), 8(d), 8(e), and 8(f) (in Appendix), for all
the possible combinations of the basic methods. The results for the
combination of all the four methods at the same time are shown in
Figure 8(g) (in Appendix).
As we can note from Figure 8, increasing ^(cid:28) leads to an increased
FAR and a decreased IPR—this observation is consistent for all
the considered combinations. This is because increasing ^(cid:28) means
requiring the combined basic methods to output a normalized value
that is closer to the one of the training set. After the inﬂuence of ^(cid:28),
the interesting point is that the proposed normalization of DTW-D
and DTW-S, and the proposed combination of basic methods using
these algorithms, is able to signiﬁcantly improve the results.
In
fact, using this combination, we were able to improve both FAR
and IPR, i.e. from IPR ∼ 4.4%, FAR ∼ 9.3% (obtained for DTW-
D-So with parameters in Table 3) to IPR ∼ 2.5%, FAR ∼ 8% (for
combination of DTW-S-Sa and DTW-D-So—cfr. Figure 8(f)—,
with ^(cid:28) = 0, using normalized version of DTW-D and DTW-D,
and again considering for them the parameters in Table 3). For
the described combination and parameter setting we observed EER
∼ 7%. We remind that varying T or (cid:28) in the basic methods, or
combining methods with boolean operator, is not possible to reduce
at the same time both FAR and IPR. Instead, with the proposed
algorithms normalization and combination, we are able to reduce
the IPR of some 50% from the best result we observed considering
the single methods, and at the same time also signiﬁcantly reduce
FAR.
6. CONCLUSION
In this paper we propose a new biometric measure for users of
smartphones. That is, we focus on the movement that a user per-
258
forms when answering (or placing) a phone call, and we investigate
whether this movement can be used as a biometric authentication
measure. We propose four basic methods (leveraging different sen-
sors available on the phone and several similarity algorithms). In
this way, we manage to obtained for a single method (i.e. DTW-
D-So): IPR (Impostor Pass Rate) of some 4.5% , and FAR (False
Alarm Rate) of some 9.5%. Also, we propose a novel way of com-
bining the basic methods together. The results show that the pro-
posed combination can improve, at the same time, both FAR and
IPR. In fact, for a speciﬁc combination, we observed IPR ∼ 2.5%,
and FAR ∼ 8%, thus reducing again IPR by ∼ 2% and FAR by
∼ 1.5%. The proposed biometric measure is not only effective, as
proven by the results; it also enjoys a unique feature. That is, it
can be transparently used to authenticate a user that is answering
(or placing) a phone call, without this being affected by external
factors (like light exposure or users wearing hats or veils).
As future work, we mainly aim at more thorough experiments
(e.g. more users, different devices, different similarity algorithms).
We would also like to investigate the effectiveness in using this
mechanism in constrained environments (e.g. a crowded bus), in
case of shared devices (more authorized users on the same device),
and combining our mechanism with other authentication methods
(e.g. acoustic ear recognition). Finally, we aim at possible opti-
mization like the one on the similarity algorithm [23].
7. ACKNOWLEDGMENTS
This work is partly funded by the European Network of Excel-
lence NESSoS contract no. FP7-256980.
8. REFERENCES
[1] Android dev phone 1. http://www.htc.com/www/
product/dream/overview.html, 2010.
[2] FANTASy project (url anonymized for submission).
http://ANONYMIZED, 2010.
[3] Public class sensorevent.
http://developer.android.com/reference/
android/hardware/SensorEvent.html, 2010.
[4] A. F. Abatea, M. Nappi, D. Riccio, and G. Sabatino. 2D and
3D face recognition: A survey. Pattern Recognition Letters,
28(14):1885 – 1906, 2007.
[5] T. Abeel, Y. Van de Peer, and Y. Saeys. Java-ml: A machine
learning library. J. Mach. Learn. Res., 10:931–934, 2009.
[6] A. H. M. Akkermans, T. A. M. Kevenaar, and D. W. E. ant
Schobben. Acoustic ear recognition for person identiﬁcation.
In AUTOID ’05, pages 219–223, 2005.
[7] A. J. Aviv, K. Gibson, E. Mossop, M. Blaze, and J. M. Smith.
Smudge attacks on smartphone touch screens. In USENIX
WOOT ’10, 2010.
[8] F. Bergadano, D. Gunetti, and C. Picardi. User authentication
through keystroke dynamics. ACM TISSEC, 5(4):367–397,
2002.
[9] N. B. Boodoo and R. K. Subramanian. Robust multi
biometric recognition using face and ear images.
International Journal of Computer Science and Information
Security, 6(2), 2009.
[10] D. Gunetti and C. Picardi. Keystroke analysis of free text.
ACM TISSEC, 8(3):312–347, 2005.
[11] M. Jani, L. Mikko, V. Elena, M. Satumarja, and A. Heikki.
Identifying users of portable devices from gait pattern with
accelerometers. In ICASSP ’05, pages 973 – 976, 2005.
[12] I. Jermyn, A. Mayer, F. Monrose, M. K. Reiter, and A. D.
Rubin. The design and analysis of graphical passwords. In
USENIX SSYM’99, pages 1–1, 1999.
[13] R. Joyce and G. Gupta. Identity authentication based on
keystroke latencies. Commun. ACM, 33(2):168–176, 1990.
[14] J. Leggett and G. Williams. Verifying identity via keystroke
characteristics. Int. J. Man-Mach. Stud., 28(1):67–76, 1988.
[15] J. Liu, L. Zhong, J. Wickramasuriya, and V. Vasudevan. User
evaluation of lightweight user authentication with a single
tri-axis accelerometer. In MobileHCI ’09, pages 1–10, 2009.
[16] J. Mantyjarvi, M. Lindholm, E. Vildjiounaite, S.-M. Makela,
and H. Ailisto. Identifying users of portable devices from
gait pattern with accelerometers. 5779(7), 2005.
[17] J. A. Markowitz. Voice biometrics. Commun. ACM,
43(9):66–73, 2000.
[18] L. Nanni and A. Lumini. A multi-matcher for ear
authentication. Pattern Recognition Letters,
28(16):2219–2226, 2007.
[19] P. C. v. Oorschot and J. Thorpe. On predictive models and
user-drawn graphical passwords. ACM TISSEC, 10(4):1–33,
2008.
[20] P. J. Phillips, A. Martin, C. l. Wilson, and M. Przybocki. An
introduction to evaluating biometric systems. Computer,
33(2):56–63, 2000.
[21] A. Ross and A. Jain. Multimodal biometrics: an overview. In
EUSPICO ’05, pages 1221–1224, 2005.
[22] P. Senin. Dynamic Time Warping Algorithm Review.
Technical Report - University of Hawaii at Manoa, 2008.
[23] Y. Shou, N. Mamoulis, and D. W. Cheung. Fast and exact
warping of time series using adaptive segmental
approximations. Mach. Learn., 58(2-3):231–267, 2005.
Appendix
This section gives further details on the proposal evaluation.
In
particular, we report FAR and IPR for the several proposed ways of
combining methods. First we report the results for boolean com-
binations (Section 4.3). Figure 7(a) shows the results for any pos-
sible combination of two basic methods with AND. Figure 7(b)
shows the results for any possible combination of two basic meth-
ods with OR. Finally, Figure 7(c) shows the results for the com-
bination where a user is accepted when n out of the four methods
accept it (n = 1, 2, 3, and 4). Then, we report the results for the
combination that is based on the normalized versions of DTW-D
and DTW-S (Section 4.4). Figures 8(a), 8(b), 8(c), 8(d), 8(e), and
8(f) report the results for all the possible combinations of two basic
methods. Figure 8(g) reports the results for the combination of all
the four methods together.
(a) AND
(b) OR
(c) n out of 4
Figure 7: Boolean Combinations.
259
 0 20 40 60 80 100DTW-D-Sa,DTW-S-SaDTW-D-So,DTW-S-SoDTW-D-Sa,DTW-D-SoDTW-S-Sa,DTW-S-SoDTW-S-Sa,DTW-D-SoDTW-S-Sa,DTW-D-SoRate (%)MethodsMethodsIPRFAR 0 20 40 60 80 100DTW-D-Sa,DTW-S-SaDTW-D-So,DTW-S-SoDTW-D-Sa,DTW-D-SoDTW-S-Sa,DTW-S-SoDTW-S-Sa,DTW-D-SoDTW-S-Sa,DTW-D-SoRate (%)MethodsIPRFAR 0 20 40 60 80 1001234Rate (%)nIPRFAR(a) DTW-D-Sa, DTW-D-So
(b) DTW-S-Sa, DTW-S-So
(c) DTW-D-Sa, DTW-S-Sa
(d) DTW-D-So, DTW-D-Sa
(e) DTW-D-Sa, DTW-S-So
(f) DTW-D-So, DTW-S-Sa
260
(g) All four methods
Figure 8: Non-Boolean Combinations.
 0 20 40 60 80 100-0.50.00.51.0Rate (%)τIPRFAR 0 20 40 60 80 100-0.50.00.51.0Rate (%)τIPRFAR 0 20 40 60 80 100-0.50.00.51.0Rate (%)τIPRFAR 0 20 40 60 80 100-0.50.00.51.0Rate (%)τIRPFAR 0 20 40 60 80 100-0.50.00.51.0Rate (%)τIPRFAR 0 20 40 60 80 100-0.50.00.51.0Rate (%)τIPRFAR 0 20 40 60 80 100-0.50.00.51.0Rate (%)τIPRFAR