ters chosen are in §A.1. From these results, we can observe
that even if an analyst wants to estimate what the performance
of the classiﬁer would be in the absence of concept drift (i.e.,
where objects coming from the same distribution of the train-
ing dataset are received by the classiﬁer), she still needs to
enforce C2 and C3 while computing 10-fold CV to obtain
valid results.
Violating C1 and C2. Removing the temporal bias reveals
the real performance of each algorithm in the presence of con-
cept drift. The AUT(F1,24m) quantiﬁes such performance:
0.58 for ALG1, 0.32 for ALG2 and 0.64 for DL. In all three
scenarios, the AUT(F1,24m) is lower than 10-fold F1 as the
latter violates constraint C1 and may violate C2 if the dataset
classes are not evenly distributed across the timeline (§4).
Best performing algorithm. TESSERACT shows a
counter-intuitive result: the algorithm that is most robust to
time decay and has the highest performance over the 2 years
testing is the DL algorithm (after removing space-time bias),
although for the ﬁrst few months ALG1 outperforms DL.
Given this outcome, one may prefer to use ALG1 for the ﬁrst
few months and then DL, if retraining is not possible (§5).
We observe that this strongly contradicts the performance
obtained in the presence of temporal and spatial bias. In par-
ticular, if we only looked at the best F1 reported in the original
papers, ALG2 would have been the best algorithm (because
spatial bias was present). After enforcing C3, the k-fold on
our dataset would have suggested that DL and ALG1 have
similar performance (because of temporal bias). After enforc-
ing C1, C2 and C3, the AUT reveals that DL is actually the
algorithm most robust to time decay.
Different robustness to time decay. Given a training
dataset, the robustness of different ML models against perfor-
USENIX Association
28th USENIX Security Symposium    737
1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg1Recall(gw)Precision(gw)F1(gw)Recall(mw)Precision(mw)F1(mw)F1(10-fold,ourdataset)F1(originalpaper)1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg21471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0DLmance decay over time depends on several factors. Although
more in-depth evaluations would be required to understand
the theoretical motivations behind the different robustness to
time decay of the three algorithms in our setting, we hereby
provide insights on possible reasons. The performance of
ALG2 is the fastest to decay likely because its feature engi-
neering [33] may be capturing relations in the training data
that quickly become obsolete at test time to separate good-
ware from malware. Although ALG1 and DL take as input
the same feature space, the higher robustness to time decay
of DL is likely related to feature representation in the latent
feature space automatically identiﬁed by deep learning [19],
which appears to be more robust to time decay in this speciﬁc
setting. Recent results have also shown that linear SVM tends
to overemphasize a few important features [35]—which are
the few most effective on the training data, but may become
obsolete over time. We remark that we are not claiming that
deep learning is always more robust to time decay than tradi-
tional ML algorithms. Instead, we demonstrate how, in this
speciﬁc setting, TESSERACT allowed us to highlight higher
robustness of DL [22] against time decay; however, the prices
to pay to use DL are lower explainability [23, 42] and higher
training time [19].
Tuning algorithm. We now evaluate whether our tuning
(Algorithm 1 in §4.3) improves robustness to time decay
of a malware classiﬁer for a given target performance. We
ﬁrst aim to maximize P = F1-Score of malware class, sub-
ject to Emax= 10%. After running Algorithm 1 on ALG1 [4],
ALG2 [33] and DL, we ﬁnd that ϕ∗F1 = 0.25 for ALG1 and DL,
and ϕ∗F1 = 0.5 for ALG2. Figure 6 reports the improvement
on the test performance of applying ϕ∗F1 to the full training set
Tr of 1 year. We remark that the choice of ϕ∗F1 uses only train-
ing information (see Algorithm 1) and no test information is
used—the optimal value is chosen from a 4-month validation
set extracted from the 1 year of training data; this is to simu-
late a realistic deployment setting in which we have no a priori
information about testing. Figure 6 shows that our approach
for ﬁnding the best ϕ∗F1 improves the F1-Score on malware
at test time, at the cost of slightly reduced goodware perfor-
mance. Table 2 shows details of how total FPs, total FNs, and
AUT changed by training ALG1, ALG2, and DL with ϕ∗F1,
ϕ∗Prec, and ϕ∗Rec instead of ˆσ. These training ratios have been
computed subject to Emax = 5% for ϕ∗Rec, Emax = 10% for ϕ∗F1,
and Emax = 15% for ϕ∗Prec; the difference in the maximum
tolerated errors is motivated by the class imbalance in the
dataset—which causes lower FPR and higher FNR values
(see deﬁnitions in §4.3), as there are many more goodware
than malware. As expected (§3.3), Table 2 shows that when
training with ϕ∗F1 Precision decreases (FPs increase) but Re-
call increases (because FNs decrease), and the overall AUT
increases slightly as a trade-off. A similar reasoning follows
for the other performance targets. We observe that the AUT
for Precision may slightly differ even with a similar number
of total FPs—this is because AUT(Pr,24m) is sensitive to the
Figure 6: Tuning improvement obtained by applying ϕ∗F1 =
25% to ALG1 and DL, and ϕ∗F1 = 50% to ALG2. The values
of ϕ∗F1 are obtained with Algorithm 1 and one year of training
data (trained on 8 months and validated on 4 months).
Algorithm ϕ
FP
FN
ALG1 [4]
ALG2 [33]
DL [22]
10% ( ˆσ)
25% (ϕ∗F1)
10% (ϕ∗Pr)
50% (ϕ∗Rec)
10% ( ˆσ)
50% (ϕ∗F1)
10% (ϕ∗Pr)
50% (ϕ∗Rec)
10% ( ˆσ)
25% (ϕ∗F1)
10% (ϕ∗Pr)
25% (ϕ∗Rec)
965
2,156
965
3,728
274
4,160
274
4,160
968
2,284
968
2,284
3,851
2,815
3,851
1,793
5,689
2,689
5,689
2,689
3,291
2,346
3,291
2,346
AUT(P,24m)
F1
0.58
0.62
0.58
0.64
0.32
0.53
0.32
0.53
0.64
0.65
0.64
0.65
Pr
0.75
0.65
0.75
0.58
0.77
0.50
0.77
0.50
0.78
0.66
0.78
0.66
Rec
0.48
0.61
0.48
0.74
0.20
0.60
0.20
0.60
0.53
0.65
0.53
0.65
Table 2: Testing AUTs performance over 24 months when
training with ˆσ, ϕ∗F1, ϕ∗Pr and ϕ∗Rec.
time at which FPs occur; the same observation is valid for to-
tal FNs and AUT Recall. After tuning, the F1 performance of
ALG1 and DL become similar, although DL remains higher
in terms of AUT. The tuning improves the AUT(F1,24m) of
DL only marginally, as DL is already robust to time decay
even before tuning (Figure 5).
The next section focuses on the two classiﬁers less robust
to time decay, ALG1 and ALG2, to evaluate with TESSER-
ACT the performance-cost trade-offs of budget-constrained
strategies for delaying time decay.
5 Delaying Time Decay
We have shown how enforcing constraints and computing
AUT with TESSERACT can reveal the real performance of
Android malware classiﬁers (§4.4). This baseline AUT per-
formance (without retraining) allows users to evaluate the
general robustness of an algorithm to time decay. A classiﬁer
may be retrained to update its model. However, manual la-
beling is costly (especially in the Android malware setting),
and the ML community [6, 46] has worked extensively on
mitigation strategies—e.g., to identify a limited number of
best objects to label (active learning). While effective at post-
poning time decay, strategies like these can further complicate
the fair evaluation and comparison of classiﬁers.
In this section, we show how TESSERACT can be used
738    28th USENIX Security Symposium
USENIX Association
1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg1F1(mw,ˆσ)F1(mw,ϕ∗)F1(gw,ˆσF1(gw,ϕ∗)1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg21471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0DLCosts
L
Q
Performance
P : AUT(F1,24m)
ϕ = ˆσ
ϕ = ϕ∗F1
Delay
method
No update
Rejection ( ˆσ)
Rejection (ϕ∗F1)
AL: 1%
AL: 2.5%
AL: 5%
AL: 7.5%
AL: 10%
AL: 25%
AL: 50%
Inc. retrain
ALG1
0
0
0
709
1,788
3,589
5,387
7,189
17,989
35,988
71,988
ALG2
0
0
0
709
1,788
3,589
5,387
7,189
17,989
35,988
71,988
ALG1
ALG2
0
0
10,283
10,576
3,595
24,390
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ALG1 ALG2
0.317
0.577
0.717
0.280
–
0.708
0.738
0.782
0.793
0.796
0.821
0.817
0.818
–
0.456
0.509
0.615
0.641