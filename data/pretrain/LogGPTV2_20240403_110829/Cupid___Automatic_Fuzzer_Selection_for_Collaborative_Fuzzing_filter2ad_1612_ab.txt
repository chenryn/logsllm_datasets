To answer question (a), we refer to Figure 2. This figure illus-
trates a simplified example of two fuzzers (ğ´ and ğµ) visiting two
different paths of the program space. Suppose that, in this scenario,
our performance data suggests that these fuzzers will always take
these paths, and assuming that we have two CPU cores for fuzzing
(combination of size two) to run collaboratively, there are only three
available combinations to run: (1) ğ´ and ğ´, (2) ğµ and ğµ, (3) ğ´ and ğµ.
In this example, simply going by the number of basic blocks
covered by each single fuzzer individually, one would erroneously
predict option (1) as the best choice. However, as fuzzer ğ´ will
3
Figure 3: Actual probabilities (first two images) and predicted synchronized probabilities for the branches in freetype2 when
combining Honggfuzz and libFuzzer. Every cell represents a single branch. The alpha value of a cell represents the probabil-
ity that this branch will be solved by this fuzzer (or fuzzer combination). For illustration purposes, we only selected an excerpt
of all branches to make the differences visually distinguishable.
those high-ranking combinations. Intuitively, given two combina-
tions that are predicted to perform similarly well, we should err on
the side of diversity and choose the combination with the greatest
number of different fuzzers, as this combination is less likely to be
negatively affected in case of an underperforming fuzzer on some
binaries (i.e., while maintaining a similar performance, a combina-
tion consisting of more different fuzzers makes it more unlikely
that all of them will fail on a given branch).
4.2 Predicting high-performing fuzzer
combinations
In theory, collecting data on how well fuzzers perform on a diverse
set of branches is valuable information for assessing how comple-
mentary a combination of fuzzers is. If we assume that the set of
branches in this training phase was representative of real-world
binaries, we should be able to approximate which combination of
fuzzers, on average, would have the best chance of maximizing
code coverage in future runs on unknown binaries. Note that we
only need to make this prediction once when a new fuzzer is intro-
duced to the frameworkâ€”the resulting choice of a specific fuzzer
combination is, on average, likely to perform reasonably well in
future collaborative fuzzing runs, independent of the binary. Given
this data, we calculate which fuzzers would be complementary. Fur-
thermore, we predict a ranking of all possible fuzzer combinations
and then select a high-performing combination of fuzzers. The
accuracy of the prediction, however, is dependent on the quality
of the training data, i.e., if the training data reflects the mixture
of branches seen outside of the training data reasonably well, our
prediction is more likely to hold true on unknown binaries.
To make these predictions, Cupid assumes a collaborative fuzzing
model, in which every fuzzer starts with the same seed and has
some limited time frame to fuzz on their own, after which they
share their seed files (synchronization). Afterward, each fuzzer con-
tinues on their own until the next synchronization happens. This is
similar to real-world scenarios, as multiple fuzzers in parallel will
start with the same seeds and try to solve the same branches reach-
able from these seeds. With a growing number of collaborating
fuzzers, more parallel attempts are made at solving these branches.
The likelihood of one branch being solved by multiple collab-
orating fuzzers in the given time period can be calculated by the
probability that at least one of the fuzzers will solve this branch.
For example, assume the collaborating fuzzers ğ´ and ğµ both have
4
a probability of 50% for solving a branch in the given time period,
then the total predicted probability for this branch would be 75%.
That is, the branch would only not be solved if both fuzzers failed
to do so, which would only happen with a predicted probability of
25%, see Equation 1.
ğµğ‘Ÿğ‘ğ‘›ğ‘â„ğ‘ƒğ‘Ÿğ‘œğ‘ğ¹ (ğ‘) = 1 âˆ’
ğ‘“ âˆˆğ¹
(1 âˆ’ ğ‘ ğ‘“ )
(1)
where ğ¹ are the collaborating fuzzers, ğ‘ the branch we want to
calculate the combined probability for, and ğ‘ ğ‘“ the probability of
fuzzer ğ‘“ for solving branch ğ‘. This calculates the probability on a
branch-basis, i.e., how likely it is that these collaborating fuzzers
would solve this branch in the given time period.
To calculate how well fuzzers complement each other in total, we
calculate the sum of all of their branch predictions (see Equation 2):
ğ´ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ğµğ‘Ÿğ‘ğ‘›ğ‘â„ğ¶ğ‘œğ‘£ğ¹ =
ğµğ‘Ÿğ‘ğ‘›ğ‘â„ğ‘ƒğ‘Ÿğ‘œğ‘ğ¹ (ğ‘)
(2)

ğ‘âˆˆğµ
where ğ¹ are the collaborating fuzzers, and ğµ are all branches. This
represents the expected average number of branches that would be
solved by the collaborating fuzzers.
With this approach, we do not just calculate how many different
branches two fuzzers would findâ€”as we run multiple fuzzers in
parallel, inevitably the probability of solving a branch will increase
if the branch probability was greater than zero and less than one.
Hence, two instances of the same fuzzer will also complement each
other to a certain degree, which is in line with real-world scenarios,
where it is expected that multiple synchronized instances of the
same fuzzer will outperform a single instance.
Although this design choice tries to maximize the expected av-
erage branch coverage, it also inherently selects for branches that
only one fuzzer can solve (rare branches). As an example, assume
fuzzer ğ´ can solve a branch with probability 0.9. Two instances of
this fuzzer would, given the above formula, achieve a probability of
0.99 for solving this branch, an increase of only 0.09. However, given
a fuzzer ğµ that can solve a different rare branch with a probability
of only 0.2, the combination of ğ´ and ğµ would increase the expected
average branch coverage by 0.2 as opposed to 0.09. As seen in this
case, Cupid leans toward fuzzers that solve rare branches if these
fuzzers would increase the expected average branch coverage more
than fuzzers with higher overall probabilities.
+++Figure 4: Cupid lets every fuzzer work on a diverse and rep-
resentative set of branches to extract a probability map of
how often a fuzzer was able to solve a branch in a limited
time frame.
Note that one of our goals is to increase the return on investment,
which can be simply translated into maximizing coverage per time.
We achieve this only due to the way we collect the training data. By
limiting the time fuzzers have, we condition our selection process
to prefer faster fuzzers.
With these design choices, Cupid balances two important aspects
of selecting fuzzers: (1) due to the complementarity metric, Cupid
selects fuzzers that solve different branches, i.e., maximizes the total
coverage; and (2) due to the way we perform data collection, Cupid
selects fuzzers that are faster at finding coverage.
As an illustration of the results of the complementarity metric,
refer to Figure 3. This figure depicts an excerpt of the resulting
probabilities by all three possible combinations of two collaborating
fuzzers (Honggfuzz, libFuzzer) while fuzzing freetype2. Each cell
represents a specific branch. For each cell, a darker color represents
a higher probability. In this case, the combined predicted bitmap of
two different fuzzers is the best combination. This also illustrates
how two instances of the same fuzzer will increase the probabilities
on the same branches, whereas different fuzzers complement each
other additionally on branches that only one of them could find.
4.3 Representative Branches
Since the prediction of Cupid is based on how well fuzzers perform
on individual branches, an important factor is the selection of which
branches to base that prediction on. The chosen branches should be
diverse, to reduce the possibility of overfitting, and be representative
of real-world binaries, to allow practical application. These branches
are collected by fuzzing a diverse set of binaries with each individual
fuzzer in isolation, while simulating a collaborative setting. This
process only needs to happen once for any fuzzer. Additionally,
adding a new fuzzer or changing configuration parameters of a
fuzzer require only re-evaluation of the new or affected fuzzer.
As we are not interested in the binaries themselves, but only
in their branches, we want to sample as many different branches
as possible from the program space. As such, every fuzzer is run
multiple times on all binaries, each time with different seed files that
were chosen to allow different areas of the binary to be reached.
Additionally, since we want to maximize the return on invest-
ment on a limited budget, we focus on maximizing code coverage
over time. Therefore, the empirical data on each fuzzer should re-
flect how well they operate on a given set of branches in a limited
time window. Here, the central idea is to let the fuzzers run only
for a limited time period and extract information on how many
branches they are able to solve.
Finally, when the empirical training data is collected, we have
a mapping from every fuzzer to a list of representative branches
and their corresponding probabilities (see Figure 4). Given this
mapping, as outlined earlier, Cupid calculates and predicts how
complementary the fuzzers are.
With this in mind, a major novelty of Cupid is showing how
data from a single, isolated fuzzing campaign on a representative
set of branches can be extrapolated to predict the candidates that
are likely to complement each other and maximize code coverage
in a collaborative fuzzing run on unknown binaries.
5 IMPLEMENTATION
In the data-driven approach used by Cupid, the quality of the data
directly influences the resulting prediction. Particularly, we need to
avoid basing the prediction on a set of branches that is not actually
representative. We can control data collection mainly with the
chosen binaries and parameters of the individual fuzzing runs.
5.1 Collecting empirical data
In particular, we choose ten different binaries as the training set
(freetype2, re2, boringssl, llvm-libcxxabi, libjpeg-turbo, pcre2,
wpantund, lcms, vorbis, harfbuzz) which contain a large vari-
ety of branches. These libraries are vastly diverse in their nature
and purpose, they cover categories such as font processing, regular
expressions, encryption, image parsing, network interface manage-
ment, color management, and audio processing. Hence, we think
that these binaries are well-suited for our evaluation purposes. Note
that adding binaries from additional application categories could
further improve the representativeness of the training data. To
ease the process of automation, for the evaluation, these training
binaries are provided by Googleâ€™s fuzzer-test-suite, as this allows
better extendability to a multitude of fuzzers through a unified
compilation process for all binaries. Note that, although all binaries
in the training and test set are from Googleâ€™s fuzzer-test-suite, the
projects themselves are separate and developed by different teams.
To create the seed files, which act as the starting points for the
individual fuzzing runs, we fuzz these training binaries for 12 hours.
This is a one-off effort that does not have to be repeated. Out of
these seed files, we select five seeds that were found 2-3 hours apart,
this temporal distance reduces the overlap between the branches
the fuzzers can reach. We observe the performance of each fuzzer
by letting them run once for each seed file for all training binaries.
This resembles the collaborative model outlined above, where every
new seed simulates a completed synchronization.
Due to inherent randomness in any fuzzing process, we let the
fuzzers run 30 times and calculate probabilities for all branches that
reflect how often a branch was found by this fuzzer in that time
period (i.e., a branch will have a probability of 50% for fuzzer ğ´, if
fuzzer ğ´ was only able to solve that branch in 15 out of its 30 runs
in time ğ‘¡). Again, this is a one-off effort for each fuzzer, which does
not add any overhead to the actual fuzzing process in the future.
As we need to balance the required computation time to exhaus-
tiveness, we limit each run to ğ‘¡ = 30ğ‘šğ‘–ğ‘› for our evaluation. While
users may choose to tune it, in our analysis, we found this time
limit to work consistently well in practice. One disadvantage with
using such a time limit is that some fuzzers might need more time
5
FuzzerBranchesProbability MapBranchProbability..........13%â‡’to reach their peak performance. Although we were unable to em-
pirically observe this effect with our fuzzer selection, this might
be an issue for future fuzzers. Additionally, some fuzzers focus on
more difficult branches but take longer to solve themâ€”these fuzzers
could be negatively affected by this time limit.
However, we believe that the numerous advantages vastly over-
shadow these possible disadvantages: First, limiting the run time
reduces the risk of fuzzers getting stuck on an initial branch, while
only one fuzzer might be able to solve that branch and get a dispro-
portional advantage by solving all the following branches. Although
it is important to reflect in the data the advantage this fuzzer brings,
if the timeout were longer, it would increasingly appear that many
branches were only solved by this one fuzzer, thus skewing how
many rare branches this fuzzer is actually able to solve (i.e., due
to the unsolvable initial branch, the other fuzzers were not given
the chance to directly test themselves on the following branches).
Second, even with this time limit, the data reflects the internal
short-term scheduling mechanism. Each fuzzer faces a multitude
of branches reachable from any given seed fileâ€”how it chooses
what branches to solve and what corpus files to mutate, impacts
the overall performance of the fuzzer. Third, due to the time limit,
the execution speed of the fuzzer is reflected in the data. Fuzzing
speed is an important factor in maximizing code coverage. [10, 13]
Fourth, our evaluation suggests that our prediction framework is
accurate in predicting a high-performing fuzzer combination. Al-
though improvements might be possible, this time limit does not
seem to impact the prediction accuracy significantly. In conclu-
sion, our approach approximates real-world fuzzing scenarios and
therefore we take many performance-relevant properties of fuzzers
into account. The empirical data reflects, at least, the following
attributes of fuzzers: (i) ability to solve a variety of branches, (ii)
short-term scheduling policies, and (iii) execution speed.
As mentioned earlier, note that Cupid has to collect this data
only once per fuzzer to update the prediction. As the goal of the
prediction is to be generalized via a representative set of branches,
the prediction is made independent of the future fuzzing target. Addi-
tionally, it is not necessary to recollect data when the combination
size changes. Furthermore, if one were to add a new fuzzer to the
framework, one would only need to collect empirical data on this
new fuzzer. This is the main improvement that allows for predic-
tions that scale linearly with the number of fuzzers in the pool
(whereas an exhaustive search would grow exponentially with the
number of fuzzers as well as the combination size). However, as
mentioned earlier, the collected data is reliant on the same fuzzer
configuration for future runs (e.g., fuzzing mode or instrumentation
method), as such, significant changes to the fuzzer configuration
will require new runs and a new prediction for accurate results.
5.2 Components and Implementation
Cupid consists mainly of the prediction engine and some small
packaging for the different fuzzers.
5.2.1 Prediction engine (Python, âˆ¼4k LOC). This component col-
lects the resulting corpus files and generates code coverage infor-
mation. To collect this information, we developed a Python library
that internally uses AFL (built as a C library) to quickly collect code
coverage given a corpus directory. The resulting data is exported as
6
a bitmap converted to a NumPy [22] array to allow for easy proba-
bility calculations. Finally, these probabilities are then converted to
a ranking of fuzzer combinations.
Fuzzer drivers (Python, âˆ¼2k LOC). Each fuzzer runs in its
5.2.2
own Docker [20] container which is controlled by a driver that
coordinates the Docker images, assigns them to their respective
CPU cores, and manages their start and stop times. The framework
is extensible, allowing developers of new fuzzers to easily add
support by simply creating a new container image and adding
around 100 lines of Python code to our driver. We implemented
support for eight fuzzers, AFL-based fuzzers are supported out-of-
the-box.
5.2.3 Patches for compatibility. We patched Honggfuzz [12] and
libFuzzer [19] (both less than 150 LOC) to allow for external test
case syncing at runtime.
6 EVALUATION
To evaluate the effectiveness of the prediction of Cupid, we first
show that the predicted ranking of combinations corresponds to the
ranking based on real-world performance. Based on these results,
we predict a combination of fuzzers of length ğ‘› = 4 to replicate
the evaluation of EnFuzz and show that our automatically selected
combination outperforms the expert-guided, hand-picked selection
of EnFuzz in terms of code coverage and bug finding capabilities.
For providing a valid evaluation, the set of fuzzer-test-suite bina-
ries is split in two for most experiments: the training set (freetype2,
re2, boringssl, llvm-libcxxabi, libjpeg-turbo, pcre2, wpan-
tund, lcms, vorbis, harfbuzz) and the test set (woff2, sqlite,
proj4, openthread, openssl-1.1.0c, openssl-1.0.2d, libxml2, lib-
ssh, libpng, libarchive, json, guetzli, c-ares). One binary out of
24 was excluded because it did not run for all of the fuzzers (openssl-
1.0.1f). We used the seeds supplied with fuzzer-test-suiteâ€”if none
were available, we instead used a seed containing only the null
byte. As mentioned earlier, fuzzer-test-suite allows for a unified
compilation process, but the included projects are separate, diverse
and developed by different teams, which is required to be able to
train on a set of branches that is representative of unknown future
binaries.
To avoid discrepancies due to implementation details, we use
our own framework and Docker images to evaluate both EnFuzz
and Cupid. This is to keep the playing field as leveled as possible,