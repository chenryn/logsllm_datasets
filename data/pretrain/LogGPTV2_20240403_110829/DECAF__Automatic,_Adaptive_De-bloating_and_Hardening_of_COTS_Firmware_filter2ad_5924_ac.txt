patibility with older BIOS features, while the Whea modules
(Windows Hardware Error Architecture) provide error man-
agement and log information for the OS [35].
As the graph is generated before pruning, knowing module
names and interactions can provide valuable information to
the user looking to white/black list certain functionality.
4.4 Board Management
Testing changes to the UEFI ﬁrmware is not a goal that
can be simply achieved using virtualization tools, such as
QEMU [43], because QEMU does not really virtualize the
hardware below the guest OS. The guest can only see a mem-
ory map where accessing particular addresses will result in
various side effects (such as manipulating hardware via regis-
ters). QEMU replicates this behaviour, while mimicking the
side effects the OS would normally see on dedicated machines.
The UEFI environment itself, placed lower in the software
stack, is more difﬁcult to virtualize and QEMU does not sup-
port hardware proﬁles compatible with modern UEFI systems.
Indeed, QEMU only supports two x86 chipsets: i440FX and
Q35. Both are quite old (1996 and 2007, respectively), and
there do not exist many (if any; we were unable to ﬁnd one)
compatible UEFI motherboards.
Open Virtual Machine Firmware (OVMF) is a project that
USENIX Association
29th USENIX Security Symposium    1719
enables UEFI support in virtual machines [14]. It is based on
the EDK II implementation of the standard, and we have used
it for various tests and prototyping. But ultimately, the goal of
DECAF is to work on a large number of COTS platforms, and
the OVMF image provides only a limited and considerably
different simulation of a real board. Taking this into account,
the only way to test whether a pruned ﬁrmware image is func-
tioning correctly is to ﬂash it onto the motherboard and boot
an operating system to validate that everything is still working
as expected by running a test suite. This requires controlling
the motherboard in an automated fashion to accomplish a few
tasks: (1) power control, (2) power monitoring, (3) ﬂashing
ﬁrmware images, and (4) providing boot media.
For convenience, motherboards with a BMC (Board Man-
agement Controller) that provides IPMI (Intelligent Platform
Management Interface) were selected for DECAF since they
offer all of the services required. We developed a uniﬁed
Python API (represented by C in Figure 4) for interfacing
with the motherboard IPMI services, hiding vendor speciﬁc
behavior. IPMI is typically only present on server-grade hard-
ware, but the same thing can be accomplished on consumer
hardware with an external ﬂash programmer, a GPIO con-
troller for monitoring and controlling the power, and physical
or PXE boot media. When implemented behind the API, this
would work seamlessly with the rest of the components.
Because the aim of DECAF is to harden trusted code base
residing at the ﬁrmware level, it is worth mentioning that
the various IPMI implementations are not really secure, as
emphasized by [9]. This is consistent with some of our ini-
tial ﬁndings when developing the vendor speciﬁc extensions.
Nevertheless, this does not represent a liability for our goals,
as the pruning operation is a one time process and the re-
sulting image can be ﬂashed on boards that have the IPMI
disabled. Also, as previously mentioned, the presence of IPMI
is a convenience, not a necessity.
4.5 Validation
The ﬁrst priority in validation is to make sure that a moth-
erboard ﬂashed with modiﬁed ﬁrmware actually manages
to boot into an operating system (ArchLinux 2018-11-01
was used to produce the images described in this paper). On
boards that support it, POST (power-on self test) codes are
monitored through the board management API to monitor
early execution. This is done as a time-saving measure. If a
timeout is reached and the operating system has not booted,
the ﬁrmware is considered broken, and the process backtracks
and continues down a different pruning path. However, by
monitoring the POST codes, it can sometimes be determined
that a ﬁrmware image is broken without waiting for the entire
duration of the timeout period. The whole pruning process
tends to run over the course of a few days, so any time savings
that can be obtained are valuable.
The IPMI controller monitors the network and waits for
the Linux boot media to bring the motherboard’s network
interfaces up and negotiate DHCP. It then provides the IP ad-
dress of the booted host to the validation engine (represented
by F in Figure 4), which uses SSH to remotely access the
operating system where it can perform tests. At the beginning
of the pruning process, the stock ﬁrmware image is ﬂashed
and the validation component collects information from the
known-good booted operating system. This is used as a base-
line when comparing the collected data from the modiﬁed
images. For example, the PCI hardware conﬁguration of the
image is recorded so that on subsequent tests it can be deter-
mined if any of the hardware components on the board were
not brought up properly.
Once the operating system is up and SSH connection is
established, any sort of tests can be performed. The validation
component is meant to be ﬂexible and extensible. We use
docker to ensure portability and extensibility: each validation
target is a docker container which is built at the beginning
of the pipeline and copied to the booted OS over the net-
work. The container is run and the output is compared to
that of the baseline ﬁrmware. If there are any differences,
the ﬂashed ﬁrmware is considered invalid, so any tolerable
differences must be ﬁltered out by the container itself. For
example, in the dmidecode validation target discussed later in
this section, we check only memory and CPU conﬁguration
types. This is because dmidecode was speciﬁcally added to
preserve memory timings and clock frequency early on in
our data center pruning efforts. Other System Management
BIOS (SMBIOS)/Desktop Management Interface (DMI) in-
formation (OEM strings, system conﬁguration options, etc.)
are not strictly necessary to the functionality of the device,
but of course can be easily included if a user desires.
As will be discussed further in Section 5, the pruning
pipeline was run with two proﬁles, "aggressive" and "data
center." The functional difference here is the motherboards
are booted off of virtual media provided through the IPMI
interface in aggressive mode and over iPXE in data center
mode. Therefore, iPXE and related components (e.g. network
drivers) will be preserved in the data center pruning, while
they may be removed in the aggressive pruning. Each proﬁle
uses the same set of validation targets, detailed below:
1. dmidecode is used to decode the DMI table, which
is hardware conﬁguration information reported by the
ﬁrmware to inform the operating system of the hardware
present in the system and facilitate management. This en-
sures important information such as conﬁgured memory
speed is preserved.
2. lspci is used to validate that detailed information about
PCI buses and related interactions is preserved.
3. /proc/acpi is checked to ensure the operating system
will be able to perform ACPI power management.
4. Intel’s CHIPSEC security suite is run to check the se-
curity of pruned images.
1720    29th USENIX Security Symposium
USENIX Association
The security of the pruned ﬁrmware images is of utmost
importance. With the goal of improving security by reducing
the byte surface area, it must be ensured that removing certain
modules does not introduce new known vulnerabilities into
the ﬁrmware. For example, there may be a module responsible
for write protecting the SPI ﬂash chip containing the ﬁrmware,
which prevents attackers that manage to infect the operating
system from permanently taking over the hardware at a low
level. Another may serve as a lock box, putting the S3 resume
script into safe memory so that attackers cannot use it to
penetrate the system [31].
Intel’s CHIPSEC framework is used to monitor and val-
idate the security integrity of these modiﬁed images [24].
CHIPSEC scans the system for known ﬁrmware level vulner-
abilities and reports them; these reports are compared against
the report from the original image to ensure that no addi-
tional vulnerabilities are introduced by the pruning process.
Each vanilla image had a few failures, such as the SPI chip
being writable or Spectre/Meltdown style attacks being possi-
ble. Further, e.g., our HP server contains four critical errors:
one stemming from Spectre-style vulnerabilities, and three
from improperly conﬁgured protections that may allow an at-
tacker to modify the bootﬂow, overwrite SMRAM via Direct
Memory Access (DMA) attacks, or even overwrite the BIOS
through the SPI chip.
DECAF prunes modules but does not (yet) patch modules
(i.e., to ﬁx such vulnerabilities in remaining modules). As a
result the CHIPSEC vulnerabilities cannot be ﬁxed automati-
cally by DECAF.
Any additional protections can be added manually [38]. In
future releases, DECAF may automatically handle this.
If DECAF is being run with a certain objective in mind,
tests can be speciﬁcally crafted in a manner that assures the
desired functionality is preserved. This guarantees that the
user’s needs are satisﬁed, while potentially increasing the
number of modules pruned.
Indeed, one can imagine any number of tests that may be
considered essential to a certain application. If more complex
tests need to be run, it is possible that the time required to val-
idate a single pruning proﬁle may increase substantially (e.g.,
if some sort of stress/performance test needs to be performed).
The initial use case for DECAF was for hardware running in
cloud data centers for compute-as-a-service where features
such as USB support, VGA support, etc., are not necessary,
and thus validation can be performed rather quickly.
Certain hardware features, while present, may not be re-
quired for a user’s application, allowing for even greater prun-
ing. There are two methods for achieving this. First, if the
user has prior knowledge on what modules are responsible
for the functionality that is no longer needed, the modules can
be removed from the start via the blacklist. If this is not the
case, the user can make sure that the validation layer ignores
the respective feature (e.g., ignore that the device associated
with the serial port is no longer listed in the OS).
5 Results
The pruning process was run with two proﬁles: "aggressive"
pruning, where only booting from physical media (or physi-
cal media emulated by the board’s BMC) was required, and
"data center" pruning, where the boards were pruned for the
purpose of running in cloud data centers offering compute-as-
a-service, booting over iPXE.
A visualization of the aggressive pruning process can be
seen in Figures 6 and 7 on ﬁrmware from two different
motherboards: the SuperMicro A1SAi-2550F and the Tyan
5533V101, respectively. Here, the markings indicate the result
of attempting to prune the board, with blue (BIOS Post) indi-
cating that the ﬁrmware did not boot, red (OS Probe Failure)
indicating that one or more of the validation targets failed,
and green (OS Probe Success) indicating that the valida-
tion targets passed. The SuperMicro board is based on an
Intel R(cid:13)Atom C2000TM chipset, and the Tyan board was based
on an Intel R(cid:13)Core i3TM Haswell chipset.
The results of the aggressive pruning pipeline and the data
center pruning pipeline can be seen in Tables 1 through 4. The
aggressive pipeline was able to remove a much larger portion
of the ﬁrmware than the data center pipeline, removing over
70% of the ﬁrmware bytes from the SuperMicro motherboard
and almost 40% from the Tyan and HP motherboards. The
pruned image boots more quickly as well. The SuperMicro
motherboard booted 13 seconds faster on average, and the
Tyan motherboard booted 7 seconds faster on average with
the pruned ﬁrmware.
Data Center. One major DECAF application has been to
prune images for a cloud data center. The Tyan 5533V101,
the SuperMicro A1SAi-2550F, and other models have been
successfully used as part of an OpenStack deployment, in a
production data center successfully since 2017, with perfor-
mance and reliability metrics higher than standard ﬁrmware
across hundreds of thousands of instance allocations. For data
center pruning, the results are also, strong, ranging from about
7% to about 30%. More recent results suggest this ﬁgure is
closer to 40% (e.g., on the HP motherboards).
Security metrics are evaluated later in this section.
5.1 Comparison Between EFI Images
Testing with a large number of boards and vendors has proven
difﬁcult. The IPMI based communication is not necessarily
standard (nor too well documented) for each vendor. This
means that the API exposed by the IPMI is different, and
the submodule of the project that deals with this needs to be
adjusted for each vendor accordingly. Secondly, virtualization
does not produce good results: the virtualized environment
is highly different from a real board in terms of BIOS: the
modules loaded are different and the hardware emulated is
different (and not customizable enough for our purposes).
Because of this, a different testing direction was taken: an-
USENIX Association
29th USENIX Security Symposium    1721
to [1]. For more information on GUIDs see [2].
Two images can be compared by extracting the list of
GUIDs present in each, and determining the common ones. It
is important to note that all motherboards have inherently sim-
ilar functionality, and their ﬁrmware is based on a common
open source implementation. This aspect will cause a rather
high overlap rate in images, even from different vendors. We
are interested in how high the match rate is, and if it supports
the claim that ﬁrmware is being mass produced and bloated.
In order to keep the comparison unbiased, the motherboards
models were chosen at random. Some are for desktops, some
for laptops. There was no prior knowledge about their func-
tionality and possible similarities.
Our case study was done with three different scenarios in
mind. First, we compared the similarities between 5 randomly
chosen EFI images, from 5 different vendors. Second, we
wanted to explore the usage of the same modules within 5
different EFI images that were created by the same vendor.
Lastly, we took a closer look at how often UEFI ﬁrmware
updates actually change modules present in a given image.
Table 6 shows a comparison of 5 different products, picked
from various vendors. The ﬁrst number represents the mod-
ules in common, and the second value represents the percent
of common modules between the two images, with respect to
the larger image.
For example, 257/26% tells us that the Asus and the AS-
Rock motherboards have 257 modules in common, or 26%
of the bigger image (ASRock) is found in the smaller one
(Asus). As we can observe there are several cases where the
smaller image is over 50% identical with the larger one.
Similarly, Table 7 contains a comparison of 7 of the most
popular motherboards from ASRock. The boards were chosen
from different product lines, and ﬁrmware images from the
same series are almost identical. It can be observed that these
motherboards have a rather large number of EFI modules
on average (up to 900 in some cases). This causes an even
bigger similarity between the binary images. Given the sizable
number of modules, out of which many are overlapping, it is
probable that after the pruning process, a substantial decrease
in the image size would be obtained.
Table 8 contains a comparison between the patch versions
of the same model (ASRock IMB186 motherboard). As ex-
pected, these patches produce very little change from version
to version. We can observe that the original 257 modules were
propagated until the current version (v2.3). Also there is a
100% match between several versions (this happens because
the changes are below modular granularity).
The data collected indicates a considerable percent of code
is being reused across various modules, as initially asserted.
We can observe that in some cases up to 70% of a ﬁrmware
image is found on a different model from a different ven-
dor (see Table 6, Asus vs ASRock). Furthermore, between
the models of the same vendor, the matching percent can
go up to 100% (having 2 different motherboards run very
Figure 6: Percentage of bytes removed and number of
iterations over time for SuperMicro A1SAi-2550F ﬁrmware
Figure 7: Percentage of bytes removed and number of
iterations over time for Tyan 5533V101 ﬁrmware
alyze just the binary images from a number of vendors and
assess their similarities. It is valuable to determine to what de-
gree these images are overlapping. Taking into consideration
the structure of a UEFI image, it is convenient to compare
the number of modules that are present in multiple versions,
from different vendors. There is no direct and unbiased bit-
wise comparison method for binary images, as often enough
there will be areas padded with 0 (or other characters, various
encodings, etc). Also, without having access to the source
code of the ﬁrmware images, bitwise comparison is made
even more difﬁcult by the compilation process: different opti-
mization levels and architectures will result in vastly different
binaries, even if the code base is identical, or highly similar.
Instead we take advantage of the GUID. While an EFI
module is not necessarily uniquely identiﬁed by a GUID, we
can argue that the base functionality between modules with
the same identiﬁer is largely the same. A GUID is a 128 bit
random generated quantity that is uniquely associated to each
module, and is aimed to work similarly to a hash, according