voteâ€”to ensure consistency between replicas [19]. Deployments
with three replicas and a quorum of two are known as R=3.2. R=3.2
CliqueMap cells are resilient against single failures3.
CliqueMap augments each IndexEntry with a VersionNumber,
which is globally unique and monotonic within a KV pair (Â§5.2). A
GET under R=3.2 reports a cache hit if and only if (1) the DataEntry
and its corresponding IndexEntry pass checksum validation (e.g.,
no torn value was observed); (2) at least two IndexEntries agree on
VersionNumber and KeyHash (i.e., a version quorum exists); (3)
the full Key in the DataEntry matches the requested Key (i.e., no
hash collision); and (4) the DataEntry was fetched from a backend
2via subsequent mmap() calls to populate memory on demand.
3We proved single failure tolerance using TLA+ [27], a formal verification language.
97
Week1Week4Week7Week10Week130128256384512MemoryUsed(TB)MemoryreshapinglaunchedClientS1NICSWS2S3Preferred BackendMetadata QuorumGET CompleteBucket FetchData FetchSIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
A. Singhvi et al.
with the quorumed VersionNumber (i.e., data came from a quorum
member), again capitalizing on the tenet of self-verifying responses.
Under this protocol, a successful GET quorum thus requires re-
sponses from only two replicas, a property useful for both fail-
ure/race tolerance and performance, because a slow backendâ€™s re-
sponse can be ignored when the remaining two agree. CliqueMapâ€™s
first responder preference leverages this property by speculating
that the preferred backend will form a quorum with at least one
subsequent response; this is likely whenever the mutation rate is
sufficiently rare, relative to the overall size of the corpus. When
this speculation fails, i.e., when the first responder isnâ€™t part of the
quorum, the client retries, preferentially fetching the datum from a
distinct backend.
5.2 Multi-Replica Mutations
Clients perform mutations by sending RPCs to all replicas for a
particular key. Each such mutation proposes a client-nominated Ver-
sionNumber, a tuple comprised of {TrueTime [12], ClientId, Sequen-
ceNumber}, such that each VersionNumber is globally unique and
the VersionNumbers emitted by a particular client ascend monotoni-
cally. By using TrueTime [12]â€”a globally-consistent coordinated
clockâ€”for the uppermost bits, each client eventually nominates the
highest VersionNumber for retried mutation operations, which is
crucial for per-client forward progress. Specifically, a mutation pro-
ceeds at a backend only when the clientâ€™s proposed VersionNumber
is higher than the VersionNumber stored for each datum. As a re-
sult, each KV pair has a monotonically increasing VersionNumber,
and all backends can independently agree on final mutation order,
without requiring a common RPC arrival order.
Erases. ERASE operations are a special case of mutations. Like
SETs, they are performed via RPC and make forward progress
even when a replica is down. ERASEs also bear a client-nominated
VersionNumber, retained so that late-arriving SETs cannot restore
affirmatively-erased values. But unlike other operations, Version-
Numbers for ERASEd elements cannot reside in the index region,
since such a design untenably spends DRAM capacity for erased
elements. However, VersionNumbers for erased elements need not
be RMA-accessible, and so they are stored in a per-backend side-
band data structureâ€”a fully associative, fixed-size tombstone cache
on the backendâ€™s heap. Further, a summary VersionNumber tracks
the largest VersionNumber ever evicted from the tombstone cache.
When reasoning about VersionNumber monotonicity, mutations con-
sult the tombstone cache, its summary, and the contents of the index
region. Because some erased elementsâ€™ VersionNumbers are approx-
imated (bounded above) by the summary VersionNumber, reasoning
about ERASEd VersionNumbers is sometimes coarse-grained but
never inconsistent.
Compare-And-Set (CAS). CAS operations are another special case
of mutation. Like a SET, they install a new value, but only if the
stored VersionNumber matches a provided VersionNumber. CAS
provides a limited means of implementing conditional updates and
reasoning about their success, with the provision that the Version-
Number is known a priori (e.g., memoized from a previous operation
on the same key).
5.3 Race Conditions
Clients do not coordinate mutations, and mutations of the same key
may occur near-simultaneously. These mutations interact without
Figure 5: An example race condition, in which a GET initiated
by client ğ¶2 races against a SET initiated by client ğ¶1, detected
in CliqueMap R=3.2 mode by a self-validating response. ğ¶2â€™s
GET attempt leads to a potential version quorum when it ver-
ifies that two fetched indices contain the same VersionNumber
ğ‘‰0, but the final data fetch can still collide against ğ¶1â€™s ongoing
SET, since it is not atomic with respect to earlier accesses.
explicit synchronization with RMA-based GETs; race resolution
hinges on the self-verifying properties of GETs and associated re-
tries performed by CliqueMap clients. This strategy has the signif-
icant advantage that no expensive RMA-based synchronization is
needed (e.g., remote locking), but the notable downside that forward
progress is not guaranteed. With R=3.2, our design objective was to
provide obstruction free [20] forward progress for GETs, notably,
that they will succeed when they donâ€™t compete against a SET of the
same key or encounter a failure condition reducing replica count be-
low quorum. As such, it is possible for repeated mutations to starve
GETs causing them to time out and report an error, once their retry
count and/or deadline is exhausted. In practice, speed differential
between RMA and RPC makes this a non-concern.
To exemplify race conditions that can arise in R=3.2, consider
a race in which Client ğ¶1 attempts SET ğ¾ = ğ‘‰1 while Client ğ¶2
attempts a GET of K (see Figure 5), where initially ğ¾ â†’ ğ‘‰0. De-
pending on the precise timing and interleaving of operations at server
replicas ğ‘†1 through ğ‘†3, ğ¶2â€™s GET attempt can result in quorum on
value ğ‘‰0 (ğ‘‰1), wherein the GET is logically ordered before (after)
the SET, or can result in a retryable checksum failure; in the ex-
ample, checksum failure occurs when the slowest of the three data
array mutations, namely the one at ğ‘†3, occurs during ğ¶2â€™s RMA data
fetch operation, resulting in metadata quorum for ğ‘‰0, but a torn read
nonetheless.
Note that an inquorate outcomeâ€”wherein an operation cannot
arrive at a quorumâ€”is not possible when GETs race against single
SETs. In contrast, a GET that races against multiple concurrent SETs,
or experiences a failure condition (e.g., backend failure, torn read,
etc.), may subsequently fail to achieve quorum. CliqueMap over-
comes such races by retrying operations that fail due to a potential
race.
5.4 Quorum Repairs
A key with a quorum of only two backendsâ€”instead of all threeâ€”is
called a dirty quorum. In a dirty quorum, not all backends agree on
98
CliqueMap: Productionizing an RMA-Based Distributed Caching System
SIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
(a)
(b)
(c)
Figure 6: CliqueMap performance by client language - (a) Op rate; (b) CPU cost and (c) Median Op latency.
a keyâ€™s existence or its VersionNumber. Dirty quorums arise due to
backend task failures, uncoordinated eviction (âˆ¼1 in 7M GETs), and
RPC failures.
A second failure, such as the loss of an additional backend, causes
the dirty quorum to degrade to an inquorate state, which is treated
as a cache miss. However, because the cost of a cache miss can
be high (e.g., may require reading data from persistent storage),
CliqueMap supports quorum repair in which a backend triggers
an explicit on-demand recovery, sourcing from the remaining two
healthy replicas.
To manage the risk of a dirty quorum degrading to an inquorate
state, backends independently scan their cohorts (i.e., the other two
backends) for missing or stale KV pairs, detected via KeyHash ex-
change to minimize overhead. A backend observing a dirty quorum
performs an on-demand repair on a key-by-key basis by: (1) issuing
a SET to the dirty backend to install the missing key K at a new
VersionNumber ğ‘ and (2) updating the VersionNumber of the key
K to ğ‘ at the repairing backend as well as the other (clean) backend.
This repair procedure ensures that all the three backends settle on a
consistent view of Key K at VersionNumber ğ‘ . We tune the inter-
scan interval to suit the needs of the deployment; tens of seconds is
typical.
A similar process operates en masse whenever a backend restarts
after an unplanned failure, such as a crash. Specifically, restarted
backends request repairs from the other two healthy backends in
their cohort.
6 Evolution
We next discuss a variety of changes we made to CliqueMap after
initial deployment, which reflect challenges we did not anticipate
in initial design. Weâ€™ve found that CliqueMapâ€™s core design ideasâ€”
self-verification, retries, and use of RPC in generalâ€”substantially
eased this evolution.
6.1 Warm Spares for Planned Maintenance
CliqueMap initially relied entirely on repairs (Â§5.4) to tolerate reg-
ular binary upgrades and server maintenance, as well as crashes.
However, upgrades are extremely commonâ€“essentially always in
progressâ€“due to scale and staged rollout practices (Â§3). Such rollouts
would effectively drop all data from R=1 deployments, since no
repairs are possible without replication. Demand for less-disruptive
rollouts for R=1 configurations motivated the addition of warm spare
backends, which temporarily host the shard of a backend undergoing
maintenance.
A backend task notified of planned maintenance, e.g., a new
binary rollout, triggers a migration of its identity and data to a
warm spare. In the course of normal RMA-based GETs, clients
may discover in-progress backend migrations via a configuration
ID stored in each Bucket. Specifically, during response validation,
if a client observes that the configuration ID in the fetched Bucket
does not match its expectation (established at connection-time along-
side other RMA-relevant metadata), the client enacts a retry by
refreshing its configuration from an external high-availability stor-
age system [13, 15]. In doing so, the client discovers all migrations
in flight and (temporary) roles of any spare backends.
Although initially motivated by R=1, sparing is effective for
R=3.2 to avoid even transient dirty quorums. Maintaining three
healthy replicas via sparing during planned maintenance ensures
that quorums remain clean and the system still tolerates unplanned
failures that occur during rollouts. We now deploy a small number
of spares in almost all cells.
6.2 Extending Beyond C++
Google datacenters run software primarily written in C++, Java, Go,
and Python, each representing pools of thousands of developers.
Launched initially with only C++ support, CliqueMap was, for a
time, unavailable to many of Googleâ€™s developers, and corpora stored
in CliqueMap were only accessible by systems composed entirely of
C++. Because serving stacks atop RPC are built from components
in many languages, we required a solution to broaden access across
languages, even if not at the full performance envelope of our native
C++ client.
Programming APIs for RMA are typically user-level libraries
in C or C++ (e.g., libibverbs [6]). Supporting RMA in other lan-
guages is challenging, as RMA operates on essentially raw memory
abstractionsâ€”pointers, offsets, etc. One might conclude the obvi-
ous approach for RMA from these languages is to leverage native
code invocation mechanisms (e.g., JNI in Java) to directly invoke
an underlying C library. We do not take this approach because it
would require us to maintain language-specific implementations of
the CliqueMap client library, which would greatly increase complex-
ity (e.g., of changing the library functionality over time). Instead,
we provide a lightweight shim for each language, which in turn
launches the CliqueMap C++ client as a Linux subprocess. We com-
municate between these processes using named pipes, which are
simple abstractions available in all these languages.4 The subprocess
approach is a tradeoffâ€”it avoids per-language maintenance burden
and complexity for each non-C++ language, while sacrificing some
efficiency.
4We also developed a shared memory mechanism specifically to accelerate Java.
99
cppjavagopyCliqueMapClientLanguage050100150200250OpRate(Mops/s)cppjavagopyCliqueMapClientLanguage101102CPU-us/opcppjavagopyCliqueMapClientLanguage060120180240300OpLatency(us)SIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
A. Singhvi et al.
Figure 7: CliqueMap client and Pony Express (client and server-
side) CPU efficiency under different lookup strategies.
Figure 8: Ads Workload.
Figures 6a and 6b summarize performance of a large synthetic
workload wherein 500 clients retrieve random keys from 500 back-
ends. Figure 6a plots the maximum GET op rate achievable in each
language, using 64B objects fetched at maximum possible rate, and
Figure 6b plots the associated CPU cost per operation. Figure 6c
plots lookup latency at 1K/GETs/sec/client (not peak). While over-
heads can be significant relative to the C++ baseline, we have found
that the resulting implementations are still performance-competitive
relative to RPC counterparts, though admittedly non-optimal. We
anticipate further optimization in this area (Â§9).
6.3 From 2Ã—R to Scan-and-Read
Googleâ€™s datacenters are highly heterogeneous, i.e., not all deploy-
ments support all RMA protocols. The 2Ã—R GET strategy is generic
and viable on a variety of transports (Pony Express [31], 1RMA [34],
and RDMA), but programmable transports like Pony Express and
emerging SmartNICs offer opportunities to optimize specifically for
KV stores [8]. When operating atop Pony Express, we leverage a
custom-built RMA-like primitive, called Scan-and-Read (SCAR),
wherein we perform the â€œscanâ€ of the Bucket server-side, in Pony
Express, and return the combined Bucket and DataEntry to the initi-
ating client. By performing a small computation in the server-side
NIC, SCAR removes a full round trip from the critical path, and
also reduces the fixed, per-op CPU overheads of a full second RMA
operation from both Pony Express and the CliqueMap client. While
small in the absolute sense, these overheads are large relative to
the handful of memory accesses required to perform the SCAR
operation.
Figure 7 shows CPU efficiency of the CliqueMap client and of
Pony Express under three distinct lookup strategies: 2Ã—R, SCAR,
and a two-sided messaging/RPC approach (MSG). Overall, we find
that an individual SCAR operation is about as costly as a normal
Pony Express RMA read. Because it halves the total number of
RMA operations per GET, SCAR is substantially more efficient on
the whole than 2Ã—R. As a further point of comparison, the over-
heads needed to wake server-side application threads to process
and respond to inbound messages (as is in the two-sided messaging
case, MSG) significantly exceed the CPU cost of simply performing
SCARâ€™s Bucket scan.
SCAR is now in widespread use, as it is especially helpful for
corpora with typically-small object sizes. However, SCAR has an
occasional downside: the combination of R=3.2, SCAR, large ob-
ject size, and high lookup concurrency can transiently incast the
CliqueMap client, as SCAR solicits three full copies of the datum.
When combined with scarce downlink bandwidth (e.g., due to older
or oversubscribed hardware), this incast can lead to higher tail la-
tency (Â§7.2.2).