56.7 ± 6.6
0.0 ± 0.0
0.0 ± 0.0
28.3 ± 3.3
0.0 ± 0.0
0.3 ± 0.5
recall
26.1 ± 2.2
0.0 ± 0.0
0.0 ± 0.0
47.6 ± 4.7
0.0 ± 0.0
0.0 ± 0.0
89.2 ± 7.9
0.0 ± 0.0
0.0 ± 0.0
100.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
100.0 ± 0.0
0.0 ± 0.0
1.1 ± 1.6
precision
99.5 ± 0.7
0.0 ± 0.0
0.0 ± 0.0
99.5 ± 0.8
0.3 ± 0.4
0.0 ± 0.0
89.5 ± 6.5
0.3 ± 0.2
0.1 ± 0.2
49.0 ± 5.4
0.3 ± 0.2
0.1 ± 0.1
24.5 ± 2.7
0.3 ± 0.1
0.0 ± 0.0
recall
25.9 ± 2.7
0.0 ± 0.0
0.0 ± 0.0
51.5 ± 5.4
0.1 ± 0.2
0.0 ± 0.0
91.9 ± 3.7
0.3 ± 0.2
0.1 ± 0.2
100.0 ± 0.0
0.5 ± 0.4
0.1 ± 0.2
100.0 ± 0.0
1.3 ± 0.3
0.1 ± 0.2
precision
99.7 ± 0.3
1.4 ± 0.6
1.3 ± 0.7
99.7 ± 0.2
1.6 ± 0.5
0.6 ± 0.3
98.0 ± 0.3
2.1 ± 0.6
0.3 ± 0.2
57.7 ± 2.3
2.1 ± 0.1
0.2 ± 0.1
28.8 ± 1.2
2.0 ± 0.0
0.1 ± 0.0
recall
21.6 ± 0.8
0.3 ± 0.1
0.3 ± 0.1
43.3 ± 1.6
0.7 ± 0.2
0.3 ± 0.1
85.1 ± 3.2
1.8 ± 0.6
0.3 ± 0.1
100.0 ± 0.0
3.6 ± 0.3
0.3 ± 0.1
100.0 ± 0.0
7.0 ± 0.1
0.3 ± 0.1
low
unconstrained
high
precision
83.3 ± 23.6
0.0 ± 0.0
0.0 ± 0.0
63.9 ± 10.4
0.0 ± 0.0
0.0 ± 0.0
51.0 ± 7.0
0.0 ± 0.0
0.0 ± 0.0
34.5 ± 6.7
0.0 ± 0.0
0.0 ± 0.0
21.7 ± 2.4
0.0 ± 0.0
0.0 ± 0.0
recall
26.1 ± 5.5
0.0 ± 0.0
0.0 ± 0.0
38.3 ± 10.3
0.0 ± 0.0
0.0 ± 0.0
53.3 ± 4.7
0.0 ± 0.0
0.0 ± 0.0
71.1 ± 15.0
0.0 ± 0.0
0.0 ± 0.0
86.1 ± 10.4
0.0 ± 0.0
0.0 ± 0.0
precision
63.9 ± 30.7
0.0 ± 0.0
0.0 ± 0.0
60.0 ± 22.5
0.0 ± 0.0
0.0 ± 0.0
33.8 ± 13.3
0.0 ± 0.0
0.0 ± 0.0
27.3 ± 8.4
0.0 ± 0.0
0.0 ± 0.0
19.8 ± 3.0
0.0 ± 0.0
0.0 ± 0.0
recall
18.4 ± 9.0
0.0 ± 0.0
0.0 ± 0.0
29.7 ± 11.7
0.0 ± 0.0
0.0 ± 0.0
32.1 ± 13.3
0.0 ± 0.0
0.0 ± 0.0
50.3 ± 16.8
0.0 ± 0.0
0.0 ± 0.0
71.9 ± 10.6
0.0 ± 0.0
0.0 ± 0.0
precision
14.9 ± 3.8
1.4 ± 2.0
0.7 ± 1.0
19.6 ± 2.8
1.8 ± 1.1
0.4 ± 0.5
18.2 ± 4.5
2.3 ± 0.7
0.3 ± 0.4
13.3 ± 1.7
1.6 ± 0.6
0.4 ± 0.4
9.2 ± 0.8
1.3 ± 0.3
0.3 ± 0.2
recall
3.8 ± 1.3
0.4 ± 0.6
0.2 ± 0.3
9.9 ± 1.9
0.9 ± 0.6
0.2 ± 0.3
18.5 ± 6.1
2.3 ± 0.9
0.3 ± 0.4
26.8 ± 5.6
3.2 ± 1.3
0.8 ± 0.9
37.3 ± 7.1
5.3 ± 1.2
1.3 ± 0.9
TABLE II: AUC of LINKTELLER comparing with two baselines
LSA2-{post, attr}. Each column corresponds to one dataset. Groups
of rows represent sampled nodes of different degrees.
Degree Method
low
uncon-
strained
high
Ours
LSA2-post
LSA2-attr
Ours
LSA2-post
LSA2-attr
Ours
LSA2-post
LSA2-attr
RU
DE
FR
Dataset
ENGB
PPI
PTBR
Flickr
1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00
0.58 ± 0.04 0.58 ± 0.09 0.67 ± 0.06 0.56 ± 0.02 0.59 ± 0.01 0.70 ± 0.05 0.65 ± 0.09
0.72 ± 0.03 0.77 ± 0.08 0.82 ± 0.02 0.62 ± 0.05 0.74 ± 0.00 0.48 ± 0.08 0.62 ± 0.14
1.00 ± 0.00 1.00 ± 0.00 0.99 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00
0.51 ± 0.00 0.52 ± 0.03 0.51 ± 0.01 0.54 ± 0.01 0.51 ± 0.01 0.64 ± 0.00 0.70 ± 0.08
0.53 ± 0.03 0.51 ± 0.02 0.53 ± 0.01 0.61 ± 0.02 0.49 ± 0.01 0.48 ± 0.02 0.49 ± 0.04
1.00 ± 0.00 1.00 ± 0.00 0.99 ± 0.01 0.99 ± 0.00 0.99 ± 0.00 1.00 ± 0.00 0.97 ± 0.00
0.52 ± 0.01 0.51 ± 0.01 0.52 ± 0.01 0.54 ± 0.01 0.51 ± 0.00 0.57 ± 0.00 0.69 ± 0.01
0.46 ± 0.01 0.50 ± 0.02 0.50 ± 0.01 0.55 ± 0.01 0.46 ± 0.01 0.48 ± 0.01 0.51 ± 0.01
belief is larger (ˆk = 2k), almost all recall values are above
90% except for Flickr. Even under extremely inaccurate esti-
mations such as ˆk = k/4 (or ˆk = 4k), the precision values (or
the recall values) are mostly higher than 95%. This observation
demonstrates the generality of LINKTELLER. We notice that
LINKTELLER’s performance on Flickr is slightly poorer than
other datasets. This may be because that the trained GCN on
Flickr does not achieve good performance given its highly
sparse structure. This implies that the parameters of the trained
network on Flickr may not capture the graph structure very
well, and thus negatively inﬂuencing the attack performance.
F. Beyond GCNs: LINKTELLER on GATs
In this section, we aim to study the effectiveness of LINK-
TELLER on other GNNs. Since the rule of information prop-
agation holds almost ubiquitously in GNNs, we hypothesize
that our inﬂuence analysis based LINKTELLER can also suc-
cessfully attack other types of GNNs. We directly apply Al-
gorithm 1 on another classical model—Graph Attention Net-
works (GATs) [31], aiming to investigate the transferability of
our inﬂuence analysis based attack from GCNs.
We evaluate the attack on the two large datasets PPI and
Flickr introduced in Table IV. For both datasets, we train a 3-
layer GAT. We leave details of the architecture and hyperpa-
rameters to Appendix F6 and report the result in Table III. Al-
though LINKTELLER still signiﬁcantly outperforms the base-
lines, it is less effective than that on GCNs. This is mainly due
to the different structures of GCNs and GATs, which leads to
different inﬂuence calculations for the two models (one related
to the graph convolution and the other related to the attention
mechanism). We provide more discussion on conveniently
adapting LINKTELLER to other architectures in Appendix E4.
VI. EVALUATION OF DIFFERENTIALLY PRIVATE GCN
In this section, we aim to understand the capability of
LINKTELLER attack by experimenting with potential ways
to defend against it. In particular, we examine whether it
is possible to weaken the effectiveness of LINKTELLER by
ensuring the ε-edge DP guarantee of the GCN model. We
further investigate the utility of the DP GCN models. In the
end, we demonstrate the tradeoff between privacy and utility,
which may be of interest to practitioners who wish to use DP
GCNs to defend against LINKTELLER.
In particular, we aim to evaluate the attack effectiveness of
LINKTELLER and the model utility on four types of models:
DP GCN models derived using DP mechanisms EDGERAND
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2014
TABLE III: Attack Performance (Precision and Recall) of LINK-
TELLER on GAT.
GAT, PPI
ˆk
Method
k/4
k/2
k
2k
4k
Ours
LSA2-post
LSA2-attr
Ours
LSA2-post
LSA2-attr
Ours
LSA2-post
LSA2-attr
Ours
LSA2-post
LSA2-attr
Ours
LSA2-post
LSA2-attr