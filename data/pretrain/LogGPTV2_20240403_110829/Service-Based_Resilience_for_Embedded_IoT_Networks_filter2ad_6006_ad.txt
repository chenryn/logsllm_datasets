10
11
12
13
14
15
16
17
18
19
547
Algorithm 3: Maximum Load to Shortest Path
(MLSP)
1 D ← Demands in descending trafﬁc requirements
2 P ← Paths in ascending cost
3 for demand ∈ D do
(cid:2) Deploy data-intensive demands to nodes
Psearch ← P
if demand.trafﬁc ≥ h∗ then
services ← demand.services
if services already deployed then
u, v ← services.hosts
Psearch ← Puv
u ← services.hosts
Psearch ← Pu
else if Only one service already deployed then
(cid:2) Find paths for demands
for p ∈ Psearch do
u, v ← path.hosts
if u, v can host services and p can carry demand
then
Allocate demand
Update p and u, v capacity
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:28:29 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 4: Postoptimization Backup Scheme
(POBS)
1 V ← Fixed nodes by MLSP
2 for node ∈ V do
u ← A random node other than node
for service hosted by node do
(cid:2) Find a backup node
3
4
5
6
7
8
9
10
11
12
if u can host service then
for demand ∈ Dservice do
v ← Node hosting other service of demand
for p ∈ Puv do
(cid:2) Migrate trafﬁc and service
if p can carry demand.trafﬁc then
Update p and u capacity
node (by POBS) and the optimally-deployed host (by MILP)
to obtain minimum latency.
Even though POBS ﬁnds backup nodes and redundant paths
is still asymptotically
for only limited set of services,
it
bounded by O(|V ||P||D|) iterations.
VI. EVALUATION
In this section, we evaluate our optimization model and
compare it to our heuristics. We ﬁrst describe our experimental
setup, our metrics and then present and discuss results from
a detailed performance analysis. This includes a discussion
of the scalability of the greedy heuristic RDDP+BSRP and
the impact of the data-intensiveness threshold h∗ on MLSP
and MLSP+POBS. Note that since it does not provide service
resilience by itself (but resilience against node and link failures
via disjoint paths), RDDP is considered together with BSRP
as an enhanced heuristic scheme.
A. Experimental Setup
The optimization problem was implemented in CPLEX
12.7.0 and all experiments (including the ones presented
in Section IV) were conducted in a server with 64-core
Intel Xeon 2.10Ghz CPU and 256GB RAM. Most of the
experiments were conducted for different topologies with 10
nodes and 2.5 node average degree to limit the size of the
MILP problem. For the model with resilience constraints, the
optimizer utilizes all cores and consumes around 120GB RAM
in the initial phases of reduction and optimization of the linear
problem. CPLEX leverages the branch-and-bound method and
the resource usage reduces signiﬁcantly to around 10GB while
CPU utilization remains high, e.g., around 80% of the cores
are actively used in the later phases of the optimization. The
heuristics, on the other hand, can be solved in a much shorter
time (under a second for RDDP and a couple of minutes for
MLSP) and thus we do not observe a considerable resource
consumption.
Various sample service overlays were generated for the
experiments. For n services, [n− 1, 3n/2] demands (in which
maximum half of them are data-intensive) were deﬁned en-
suring that each service communicates at
least one other
TABLE VII: Intervals for randomly generated parameters.
Parameter
Link capacity (ce)
Node resource capacity (rv)
Service resource consumption (τs)
Demand trafﬁc requirement (hd)
Data-intensiveness threshold (h∗)
Interval
[2.0, 4.0]
[1.0, 3.0]
(0.0-2.0]
[1.0, 5.0]
[0.5, 2.5]
service, i.e., there is no service without a trafﬁc demand.
Both topologies and service overlays are random networks
where the links/demands are created probabilistically between
nodes/services. Table VII presents the intervals in which
each parameter is generated with uniform distribution. The
convenience of networks, e.g., connectivity of the network,
sufﬁcient capacity for demands and services, are conﬁrmed
for each generation.
B. Metrics
We measured the solution time,
the latency costs, and
the probability of service failure. Solution time is the time
needed to compute solutions for the optimization model and
the heuristics. Latency cost is deﬁned in Equation 11 and
used as the objective function for our model. Probability of
Service Failure (PoSF) is the ratio of the number of services
without backup nodes (due to insufﬁcient node resources) to
the number of all services. It represents the percentage of
services that fail at most in case of an arbitrary node failure
and is used as the resilience metric. While 0% PoSF represents
the absolute resilience against any arbitrary node failure, 100%
PoSF indicates the failure of all services.
C. Results
a) Solution Time: Fig. 3 shows the solution time for
the MILP and heuristics in small topologies with 10 nodes.
Fig. 3a shows that the increasing number of demands affect
the solution time exponentially between 0.15-100 hours in the
optimal and single node failure resilient (Optimal-R) scenario.
In Fig. 3b, on the other hand, sub-optimal solutions are found
in seconds for the same topologies and service overlays. Since
MLSP and MLSP+POBS still solve a reduced MILP, they are
slower than RDDP+BSRP. Here, while MPLS+POBS is up
to x106 times faster than the Optimal-R, the greedy approach
RDDP+BSRP offers x108 times better solution time.
b) Performance Comparison: Even if greedy heuristics
are signiﬁcantly faster than solving the optimization prob-
lem, they cannot assure the optimal solution and maximum
resilience againt single random node failures. Fig. 4 compares
heuristics with the resilient (Optimal-R) (i.e., where all ser-
vices are alive in case of any single node failure) and non-
resilient (Optimal) (i.e., service deployment and routing with-
out any resilience constraint) solutions in terms of the latency
cost and PoSF. As shown in Fig. 4a, the Optimal solution
has the minimum latency cost and MLSP gets close to the
optimum. However, MLSP induces a PoSF of 20-60% as de-
picted in Fig. 4b, because of the pre-deployed services without
backups. After Optimal-R (having 0% PoSF), MLSP+POBS
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:28:29 UTC from IEEE Xplore.  Restrictions apply. 
548
100
Optimal-R
)
h
(
e
m
i
t
n
o
i
t
u
o
S
l
80
60
40
20
0
3
5
4
7
Number of demands
6
MLSP
MLSP+POBS
RDDP+BSRP
)
s
(
e
m
i
t
n
o
i
t
u
o
S
l
7
6
5
4
3
2
1
0
8
3
5
4
7
Number of demands
6
8
(a) Solution time in hours
(b) Solution time in seconds
Fig. 3: Impact of increasing number of demands on the solution time for a topology with 10 nodes.
offers the best fault-tolerance and improvement from 40% to
5% with increasing number of demands. However, in POBS
phase, it cannot ﬁnd the optimum paths due to random search
of the available nodes and becomes 2-4 times more costly in
comparison to Optimum-R. While RDDP+BRSP is less costly
than MLSP+POBS in terms of latency, it is negatively affected
by the increasing demands and has a PoSF up to 50%. Note
that latency cost and fault-tolerance are directly related to each
other. Further fault-tolerance requires to allocate the redundant
load to maintain communication in case of failures. In this
sense, fault-tolerance costs additional node resources and link
capacity.
c) Scalability: Since RDDP+BRSP is the only approach
without an optimization stage, it enables us to design larger
networks conveniently. Fig. 5a and Fig. 5b show the perfor-
mance of RDDP+BSRP for increasing demands and nodes,
respectively. In Fig. 5a, the PoSF is increasing up to 30%
as it
is harder to ﬁnd redundant resources for increasing
demands in a ﬁxed-size topology, i.e, 100 nodes. Latency cost
is expectedly increasing since the increasing demand should
be assigned to the longer paths after shorter ones are priorly
utilized.
i.e.,
Fig. 5b, on the other hand, reﬂects the impact of increasing
available resources,
the number of nodes and paths,
with a ﬁxed number of demands, i.e., 80 demands. When
the number of nodes and demands are equal, i.e., 80 nodes
and demands, the PoSF can rise up to 50% but it drops to
15% with increasing number of nodes as a consequence of
the increased available resources. Being able to deploy more
redundant services also results with increasing latency cost as
it requires to utilize further paths to ensure communication
between redundant services. Note that satisfying a demand
already requires the deployment of two services that should
also have backup nodes and paths. There should be at least
four nodes (and some paths depending on link capacities) per
demand to offer such a resilience scheme without a ﬂexible
service deployment model. In this sense, RDDP+BSRP offers
a scalable and computationally simple solution that can satisfy
the resilience requirements to a certain extent.
d) Data-intensiveness Threshold: The efﬁciency of
MLSP and MLSP+POBS depends on the data-intensiveness
threshold, h∗. Lower h∗ indicates a higher number of greedily
deployed demands, a more reduced problem size, and a harder
POBS stage. Fig. 6 shows the impact of h∗. As shown in
Fig. 6b, MLSP can place more demands for h∗ ≤ 1.0 and
it results in a higher number of services without backups
and around 40% PoSF which can also go up to 50% for
some topologies and service overlays. When it is enhanced by
POBS, the PoSF drops to 10% and the deployment becomes
fully resilient to single node failures for h∗ > 2.0. However,
2-3 times lower PoSF leads to a proportional latency cost as
seen in Fig. 6a due to the redundant trafﬁc loads allocated for
resilience.
e) Takeaway: As a concrete takeaway, our heuristics
show the tradeoff between QoS-optimality, resilience, and
scalability. Accordingly, they can be preferred with respect to
the desired balance between those design requirements. In the
presence of static services, e.g., pre-allocated, non-migratable,
or data-intensive, MLSP+POBS can be leveraged to reduce the
problem size and can provide a near-optimal solution with a
small decrease in resilience, i.e., a slightly higher probability
of service failure. However, as it still requires to solve an
MILP, it offers only limited scalability. Nevertheless, being
a greedy heuristic, RDDP+BSRP offers good scalability at
the expense of decreased QoS and resilience. Considering the
pros and cons of them, it can be concluded that MLSP+POBS
is convenient for relatively small networks having strict QoS
requirements and many mission-critical services and commu-
nication while RDDP+BSRP can scale to larger networks (in
terms of the number of components and services) in which
failure-resilience has higher priority than QoS.
VII. CONCLUSION AND FUTURE WORK
Embedded IoT networks take over safety-critical tasks and
their resilience against failures should be a prior concern al-
ready in the design stage. In this study, we presented a service-
based network design where the functionalities of a system are
deﬁned by inter-service communications, or demands, having
certain requirements. We formulated joint service deployment
and routing problem as an MILP model and extended it
with resilience constraints against random single node failures.
The problem is NP-hard, but we also discussed the problem
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:28:29 UTC from IEEE Xplore.  Restrictions apply. 
549
t
s
o
c
y
c
n
e
t
a
L
50
40
30
20
10
0
Optimal
MLSP
MLSP+POBS
Optimal-R
RDDP+BSRP
3
4
5
6
7
Number of demands
(a) Latency cost
)
%
(
e
r
u
l
i
a
f
e
c
i
v
r
e
s
f
o
y
t
i
l
i
b
a
b
o