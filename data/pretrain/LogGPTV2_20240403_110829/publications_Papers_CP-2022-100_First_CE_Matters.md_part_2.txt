… … Reporting Errors Repeating Rows Repeating Rows
… …
…
Fig. 2: Feature Taxonomy; Based on the hierarchical DIMM organization.
The feature extraction part implements functions that char- andXGBoost[13].Thesemethodsareoftenshowntoachieve
acterize the degradation state of the DIMM. In total, we high performance on a plethora of tasks concerning tabular
implemented 189 features calculable incrementally. To better input representation [14].
describe them, we organize them in a taxonomy following To learn the models, one requires labels for the individual
DIMMpropertiesandthefailuresarereflected.Figure2gives CEs. As it is not known in advance the exact moment when
the feature taxonomy. On the first level, there are five feature the DIMM is in a degraded state, to label the data, we used
groups: general DIMM, bank, column, row and cell-level fea- a heuristic (similar as in related works [5]). We observed that
tures. The general DIMM features characterize the overall CE theCEsforthefailureDIMMsareusuallygeneratedinbursts
statistics for the DIMM, irrespective of its components. This that are time-delayed. In the cases when there are prior CE
category includes features such as the frequency of errors, or generated from a single DIMM, there is a large time between
frequencyoftheerrortypessimilarasinrelatedwork[4].We theadjacentbursts.TolabeltheCEsrepresentingthedegraded
alsoproposedtousetherelativechangeoftheerrorfrequency DIMMstate,wefirstfindthelargestexistinggapbetweentwo
and error types between the two adjoint time intervals. The adjacent CEs. This creates two sets of CE. We label all the
bank-levelfeaturescharacterizetheoverallstatisticsoftheCE CEs from the first set with 0. We consider the CEs from the
frequency on a bank level, such as the number of banks that second set to denote a degraded state and label them with
reportedCEs,andthenumberofnewbanksthatreportedCEs 1. The correctable errors from new DIMMs in the current
betweentheobservationwindowandtheCEsbeforethat.The observation interval are given chronologically to the model. If
focus of this category of feature is to describe the evolution themodelpredictsthatatleastoneCEisdegraded,theDIMM
of the failure on a bank level. is predicted to fail. When learning the model, the CEs in the
The column, row and cell-level features characterized the mtimeunitspriortothefailureareremoved.Thefinaloutput
DIMM banks on the memory array level. They are different of the system are the DIMM IDs, predicted to fail.
in the way how they group the CEs alongside the memory
IV. EVALUATION
array (e.g., by row, column or cell). For all the three groups
there are three subgroups, i.e., neighbourhood, repeating and A. Experimental Setup
bank-row(/column/cell) features. The repeating group refers
To evaluate our methodology, we collected CE data with
to the repetition of a CE from a single cell address or group
memory failures over six months (November-March 2020/21)
of addresses sharing the same row/column. Intuitively, if one
from part of the server fleet of a large cloud provider. There
transistorisworn-off,anysubsequentaccessestothatmemory
were 12000 DIMMs with at least one CE. Around 3% of the
will generate CEs. Therefore, repeating the same errors can
DIMMsfailed.AsthenumberoffailedDIMMscomposes3%
indicate hard failures. The neighbourhood features are similar
of all the DIMMs, the problem of memory failure prediction
totherepeatingones,however,theyfurtherexploittheconcept
indicates imbalanced classification. We adopted subsampling
of the DIMM bursting mechanism. This concept enables
of the majority class (normal DIMMs), as a strategy to deal
reading out multiple adjacent memory addresses to speed up
with this problem. To construct the training and test data,
performance. Therefore, if an error occurs in the neighbour-
we split the normal DIMMs into two sets. We sampled 5000
hood, it is likely that there are issues with the memory. The
normal DIMMs for cross-validation and used the remaining
thirdgroupconsidersrow/column/cellpropertiesforthearrays
ones as a normal test set. We repeat the sampling five times
irrespective of the bank and the chip. This group is closely
and report the average results for the evaluation criteria. The
related to the procedure of how the logical reading/writing of
validation normal DIMMs are paired with the failure DIMMs
a whole word from multiple banks is conducted.
and are used to learn the model and access its performance.
To access model performance we used 10 Fold CV. Precision
B. Memory Failure Prediction
and recall are common evaluation criteria for memory failure
Asmachinelearningmethodsweconsidertwopopularclas- prediction [8], and we adopt them. We set the value for m to
sification methods for tabular data, i.e., Random Forest [12] 3 hours, as it is sufficient time for server content migration.
TABLE I: Experimental Results
Operational
Feature Method RF XGB
Practicies
Calculation
w #Normal ErrorRate ErrorRate ErrorRate
Strategy Precision Recall Precision Recall Precision Recall
[h] DIMMs (NormalTest) (NormalTest) (NormalTest)
2500 0.24±0.01 0.58±0.01 0.27±0.01 0.58±0.01 0.38±0.01 0.043±0.0
3h 0.279 0.056 0.027
5000 0.16±0.01 0.49±0.01 0.20±0.01 0.48±0.01 0.38±0.01 0.033±0.0
Overall
2500 0.36±0.01 0.55±0.01 0.15±0.01 0.60±0.01 0.42±0.01 0.037±0.0
CE 168h 0.109 0.015 0.006
5000 0.30±0.01 0.46±0.01 0.09±0.01 0.47±0.01 0.44±0.01 0.037±0.0
Evolution
2500 0.35±0.01 0.54±0.01 0.14±0.01 0.61±0.01 0.45±0.01 0.046±0.0
336h 0.31 0.06 0.011
5000 0.29±0.01 0.45±0.01 0.07±0.01 0.48±0.01 0.41±0.01 0.034±0.0
2500 0.29±0.01 0.48±0.01 0.17±0.01 0.37±0.01 0.21±0.01 0.056±0.0
3h 0.23 0.023 0.017
Fixed 5000 0.20±0.01 0.38±0.01 0.12±0.01 0.24±0.01 0.23±0.01 0.055±0.0
Window 2500 0.31±0.01 0.57±0.01 0.204±0.01 0.54±0.01 0.38±0.01 0.05±0.0
168h 0.16 0.06 0.025
Size 5000 0.24±0.01 0.52±0.01 0.14±0.01 0.42±0.01 0.43±0.01 0.047±0.0
[w] 2500 0.31±0.01 0.58±0.01 0.19±0.01 0.54±0.01 0.35±0.01 0.051±0.0
336h 0.29 0.06 0.018
5000 0.22±0.02 0.43±0.02 0.11±0.01 0.44±0.01 0.37±0.01 0.036±0.0
For the window interval used to extract features, we used V. CONCLUSION
w = {3,168,332} hours. The models were learned with
In this paper, we revisited the problem of memory failure
the implementations of the sklearn-learn and xgboost Python
prediction with CEs. We found that considering the whole
libraries. Another baseline we consider is the thresholding of
CEs-generation history is more indicative of failures, as com-
domainfeatures(e.g.,CErate,orthenumberofuncorrectable
paredtoconsideringafixedobservationinterval.Weproposed
errors) as frequent operational practices. Specifically, we ex-
a set of incrementally calculable features that preserve long-
periment with all possible values for the CE rate and reported
term CEs dependencies. In the evaluation of memory failures
the best values for precision and recall.
from the server fleet of a large cloud provider, we showed an
average improvement of 21% on precision and 19% on recall,
B. Results and Discussion
justifying the validity of our approach. This paper invites
Table I summarizes the results. When considering the over- further research on how to better utilize the overall CEs data,
all CE evolution, one can observe that irrespective of the to improve the performance on memory failure prediction.
method or the experiment parameters, the results for both
precision and recall are generally improved in comparison to REFERENCES
those not considering it. For example, for a time window of
[1] B.Schroeder,E.Pinheiro,andW.-D.Weber,“Dramerrorsinthewild:
14 days and normal DIMMs 5000, for XGBoost the precision
Alarge-scalefieldstudy,”Commun.ACM,vol.54,p.100–107,2011.
is improved for 9% ((0.48−0.44)), while the recall for 11% [2] M.Y.Hsiao,“Aclassofoptimalminimumodd-weight-columnsec-ded
0.44
((0.41−0.37)). Similar improvements of 32% on precision and codes,”IBMJournalofResearchandDevelopment,vol.14,no.4,pp.
0.37 395–401,1970.
5% on recall are observed for RF for 14 days and normal
[3] T. J. Dell. (1997) A white paper on the benefits of chipkill
DIMMs 5000. The average improvement in precision and correct ecc for pcserver main memory. [Online]. Available: https:
recall for XGBoost is 33.6% and 33% accordingly, while for //asset-pdf.scinapse.io/prod/48011110/48011110.pdf
[4] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, “Revisiting memory errors
RF it is 8.1% on precision and 5% on recall.
in large-scale production data centers: Analysis and modeling of new
Asseenbytheresults,includingthelong-termdependencies trends from the field,” in 2015 45th Annual IEEE/IFIP ICDSN, 2015,
isparticularlyimportantforimprovingprecision.Thisislikely pp.415–426.
[5] I.Stefanovici,“Cosmicraysdon’tstriketwice:Understandingthenature
related to the bursting nature of the CEs in anticipation of a
ofdramerrorsandtheimplicationsforsystemdesign,”in7th(ASPLOS
failure. The long-term CE preservation enables the inclusion 2012). ACM,2012.
of non-related CEs from the normal DIMM behaviour that [6] X. Wang, Y. Li, Y. Chen, S. Wang, Y. Du, C. He, Y. Zhang, P. Chen,
X.Li,W.Song,Q.xu,andL.Jiang,“Onworkload-awaredramfailure
later failed. This effectively introduces information about the
prediction in large-scale data centers,” in 2021 IEEE 39th VLSI Test
differences between the CEs of DIMMs that will fail in the Symposium,2021,pp.1–6.
future, guiding the model to better learn the normal DIMM [7] J.Bogatinovski,D.Kocev,andA.Rashkovska,“Featureextractionfor
heartbeat classification in single-lead ecg,” in 2019 42nd International
states.Thisultimatelyreducesthefalsepositivesandincreases
ConventiononInformationandCommunicationTechnology,Electronics
the precision. Another important point is that the model andMicroelectronics(MIPRO),2019,pp.320–325.
predictions outperform the manual operational practices. This [8] I. Giurgiu, J. Szabo, D. Wiesmann, and J. Bird, “Predicting dram
reliabilityinthefieldwithmachinelearning,”inProceedingsofthe18th
is to the capabilities of the methods to combine complex
ACM/IFIP/USENIX Middleware Conference: Industrial Track. New
information from multiple features. York,NY,USA:AssociationforComputingMachinery,2017,p.15–21.
Finally, by close inspection of the features’ importance, we [9] I. Boixaderas, D. Zivanovic, S. More, J. Bartolome, D. Vicente,
M. Casas, P. M. Carpenter, P. Radojkovic, and E. Ayguade, “Cost-
noted that the most important features are the ones charac-
aware prediction of uncorrected dram errors in the field,” in SC20:
terizing the transition between the two disjoint time intervals InternationalConferenceforHPCNSA,2020,pp.1–15.
formedwiththeparameterw (i.e.,(−inf,t −w)and(t −w, [10] J. Bogatinovski, G. Madjarov, S. Nedelkoski, J. Cardoso, and O. Kao,
0 0
“Leveraging log instructions in log-based anomaly detection,” in 2022
t )). This observation encourages further investigation into
0 IEEEInternationalConferenceonServicesComputing(SCC),2022,pp.
findingnewfeaturesthatbetterutilizetheoverallinformation. 321–326.
[11] J.Bogatinovski,S.Nedelkoski,L.Wu,J.Cardoso,andO.Kao,“Failure
identification from unstable log data using deep learning,” in 2022
22nd IEEE International Symposium on Cluster, Cloud and Internet
Computing(CCGrid),2022,pp.346–355.
[12] L. Breiman, “Random forests,” Machine Learning, vol. 45, pp. 5–32,
2001.
[13] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”
inProceedingsofthe22ndACMSIGKDD,2016,p.785–794.
[14] C. Zhang, C. Liu, X. Zhang, and G. Almpanidis, “An up-to-date
comparisonofstate-of-the-artclassificationalgorithms,”ExpertSystems
withApplications,vol.82,pp.128–150,2017.