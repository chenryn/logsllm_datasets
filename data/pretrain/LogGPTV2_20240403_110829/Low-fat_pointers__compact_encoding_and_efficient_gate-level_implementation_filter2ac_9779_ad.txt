To evaluate the performance of the encoding scheme, we
implemented a Pointer Unit (PU) for decoding and updating
for both the BIMA and Aries fat pointers. The PU was de-
signed2 using Bluespec SystemVerilog [6] and implemented
on a 40nm Xilinx Virtex 6 FPGA (xcv6vlx240t-2) [48].
4.9.1 Aries
While the BIMA scheme shares the basic strategy with
Aries, our encoding is diﬀerent in speciﬁc details that reduce
the latency of the key operations. Instead of using a lower
2source
distributions/fatptr_ccs2013
code available at http://ic.ese.upenn.edu/
727Ptr
B L F
A
Int
O(cid:1)set
1
-
Ptr
Error
>>
Figure 5: Data path for the Aries Encoding (addp)
Ptr’
B L F’
A’
(I) and upper (M ) bound, Aries uses a length (L) and ﬁnger
(F ) ﬁeld that eﬀectively capture the same information. In
both the BIMA and Aries cases the A ﬁeld always points
exactly to the current word. Since Aries does not have an
I and M ﬁeld, the Aries ﬁnger ﬁeld records the oﬀset of
the current block form the base block in the record to allow
bounds circuitry to calculate the base and bound. However,
this has two problems: (1) it demands a more complicated
decoding than BIMA, and (2) it demands that the ﬁnger
ﬁeld potentially be updated on every pointer increment. For
reference, the data path for the Aries Pointer Unit is shown
in Fig. 5.
4.9.2 BIMA Decode
The decode data path for the BIMA scheme is shown in
Fig. 6a. Compared to the Aries encoding data path, the
depth is smaller, and there are fewer variable shifts, result-
ing in signiﬁcantly shorter delay. For |B| = |L| = |I| = 6,
the delay is less than 4.8 ns up to 64b addresses. The de-
lay comparison with Aries is shown in Fig. 9. This is over
a 40% reduction in delay from Aries encoding on average.
The delay and area of the BIMA decode is summarized in
Figs. 7a and 8. Nonetheless, at 4.2 ns, the 46b decode is
slightly longer than our target cycle time of 4 ns. The de-
coded bounds (Sec. 4.6) avoid the need to perform this de-
coding for any operation other than lw. In Sec. 5, we show
that we can split the decode needed for lw across pipeline
stages to prevent this operation from limiting the frequency
of the processor or creating any new stall conditions.
4.9.3 BIMA Update
(Fig. 9). Aries must check and update the F ﬁeld on ever
update operation. We note here that because decode and
update are part of diﬀerent instructions in BIMA, the de-
lay of the two do not add; the delay of decode is relevant
only for lw, while delay of update is only relevant for addp.
Fig. 7b also summarizes the area for the update operation.
4.9.4 Area
The total area required by the BIMA scheme is the sum
of the area of decode and update; for a 64b fat pointer with
|A|=46, the pointer unit uses 1114 LUTs.3 This is larger
than the 956 LUTs required by the Aries encoding. Nonethe-
less, the total area is still comparable to the ﬂoating point
adder used in SAFElite, which takes 940 LUTs [49].
4.9.5 Other Implementation Technologies
We include the FPGA delay and area comparison to pro-
vide a concrete point of comparison for the complexity of the
key operations in the fat-pointer encoding schemes. Nonethe-
less, similar eﬀects and beneﬁts would occur in ASIC or cus-
tom implementation technology. Simply looking at Figs. 5,
6a, and 6b, we can see that the depth of operations is lower
for the BIMA operations than the Aries implementation.
This makes it clear the delay would be smaller in any im-
plementation. Note that the slowest operation in each dat-
apath is the variable shift. The Aries datapath requires two
variable shifts while the BIMA decode requires only one and
the BIMA update requires none. Furthermore, note that the
slowest operation in the ALU is the variable shift. This alone
is enough to explain (1) why the BIMA update, which re-
quires no variable shifts, is faster than the ALU, (2) why the
BIMA decode is only slightly slower than the ALU, and (3)
why the Aries decode with two variable shifts is over twice
the delay of the ALU. Consequently, these general timing
relations will hold for custom implementations as well.
4.10 Decoded Bounds Implementation
In the baseline case where we do not add any hardware
type metadata to the words, the register ﬁle width is 64b.
Then, we can ﬁt an entire 32-entry register ﬁle in two Virtex-
6 BRAMs4 organized as 512×64b, dual-ported memories.
The base SAFElite uses two BRAMs to support one write
and two read ports on 64b words. However, when we add
an 8b hardware type metadata, the register ﬁle width is now
72b, and this forces us to use two BRAMs per read port or a
total of 4 BRAMs, even though we are not using the entire
width oﬀered by two BRAMs. On top of this, when we add
the decoded lower and upper bounds, we further increase the
width of our register ﬁle by twice the width of our address
size. Therefore, when the address size is 46b, the register ﬁle
is (72 + 2 × 46 =) 164b wide. The register ﬁle now requires 3
BRAMs per read port for a total of 6 BRAMs. If this were
a full custom implementation, we would simply expand the
64b register ﬁle width to 164b, or a 156% overhead.
The update operation in the BIMA scheme is even simpler
than the decode operation. As shown in Fig. 6b, the BIMA
update data path consist only of additions, comparison, and
a multiplexer. The delay of update is independent of size of
I and B and is less than 4 ns for any number of address bits
below 64 as shown in Fig. 7b, meaning the update opera-
tion will not degrade processor clock frequency. At |A|=46,
BIMA update delay is less than half the Aries update delay
5. PIPELINING AND BYPASSING
The SAFElite has four pipeline stages as shown in Fig. 2.
Without the fat-pointer computation, the processor runs at
4 ns clock cycle with full bypass of results to prevent stalls
3The Virtex 6 FPGA employs 6-input Look-Up Tables for
logic. We refer to them simply as LUTs henceforth.
4BRAM=Block RAM, SRAM blocks in a Virtex-6 FPGA
728(a) BIMA Decode (lw)
(b) BIMA Update (addp)
Figure 6: Data path for BIMA scheme
Delay
Area
)
s
n
(
y
a
e
D
l
4.8
4.6
4.4
4.2
4.0
Delay
Area
900
800
700
600
500
)
s
T
U
L
(
a
e
r
A
)
s
n
(
y
a
e
D
l
3.9
3.8
3.7
3.6
3.5
3.4
)
s
T
U
L
(
a
e
r
A
550
500
450
400
350
300
32
38
46
54
64
32
38
46
54
64
Number of Bits in Address
(a) BIMA Decode (lw)
Number of Bits in Address
(b) BIMA Update (addp)
Figure 7: Area and Delay for BIMA scheme, |B| = |I| = 6
Delay
Area
4.4
4.3
4.2
4.1
)
s
n
(
y
a
e
D
l
760
740
720
700
680
660
)
s
T
U
L
(
a
e
r
A
)
s
n
(
y
a
e
D
l
8
6
4
2
0
BIMA Decode
BIMA Update
Aries
4
5
6
7
32
38
46
54
64
Number of Bits for I
Number of Bits in Address
Figure 8: Area and delay for the BIMA decode (lw) for
varying |I| with |B| = 6, |A| = 46
Figure 9: Delay comparison between Aries and BIMA en-
codings with |I| = |L| with varying |A|
on addp, lw, and sw operations when the memory references
hit in the L1 cache. Adding the 8 ns Aries addp operation to
the pipeline could, however, dramatically degrade the pro-
cessor’s operating frequency.
The BIMA encoding also changes the pipeline due to the
decoding of Dunder and Dover after a load. The BIMA de-
code takes more than 4 ns for addresses using more than 46b,
potentially increasing the cycle time. Also, if we decode the
two distances in the last pipeline stage, then an addp opera-
tion whose argument is the pointer loaded from memory in
an immediately preceding lw instruction (an lw-addp pair)
requires a stall cycle: the Dunder and Dover is needed for
the addp, so we need to ﬁnish decoding the distances before
addp can execute.
To mitigate this problem, we ﬁrst split the decode data
path into two stages—Decode 1 and Decode 2 in Fig. 6a.
In our implementation, we can fetch from the data cache in
less than 2.5 ns, so we use the remaining time in the Execute
stage to execute the ﬁrst part of decode (Decode 1). Then
we ﬁnish the computation of Dunder and Dover (Decode 2)
in the Validate/Writeback stage. With this change, we avoid
increasing the clock cycle. To remove the stall for lw-addp,
we split the update (See Fig. 6b) into two stages as well:
we keep the update address circuit (Update Address) in the
729vantage to providing safety guarantees at the hardware level
where the interface is narrower. Furthermore, as computers
protect increasingly valuable assets, spending this modest
number of relatively inexpensive transistors (comparable to
a double-precision, ﬂoating-point adder) to increase safety
and security seems a prudent investment.
7. ACKNOWLEDGMENTS
This material is based upon work supported by the DARPA
CRASH program through the United States Air Force Re-
search Laboratory (AFRL) under Contract No. FA8650-10-
C-7090. The views expressed are those of the authors and
do not reﬂect the oﬃcial policy or position of the Depart-
ment of Defense or the U.S. Government. Ben Karel, C˘at˘alin
Hri¸tcu, and Greg Morrisett provided valuable discussion and
feedback on this paper.
8. REFERENCES
[1] Introduction to Intel Memory Protection extensions.
http://software.intel.com/en-us/articles/
introduction-to-intel-memory-protection-extensions.
Accessed: 2013-08-01.
[2] P. Akritidis, M. Costa, M. Castro, and S. Hand.
Baggy bounds checking: an eﬃcient and
backwards-compatible defense against out-of-bounds
errors. In Proceedings of the 18th Conference on
USENIX Security Symposium, pages 51–66, 2009.
[3] Aleph One. Smashing the Stack for Fun and Proﬁt.
Phrack, 7(49), November 1996.
[4] AMD Corporation. AMD64 Architecture