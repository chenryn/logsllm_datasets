efficiently delivers packets to the NFs. We look at two workloads:
1) equal offered load to all NFs of 5 Mpps; 2) unequal offered load,
with NF1 and NF2 getting 6 Mpps, and NF3 getting 3 Mpps. We also
consider the case where NFs have different computation costs. As
described above, the desirable behavior is for NFs to be allocated
resources in proportion to both their arrival rate and processing
requirements.
Table 1: Context Switches for Homogeneous NFs
Uneven Load
Even Load
SCHED_
BATCH
nv
RR
cswch
SCHED_
SCHED_
NORMAL
nvc
csw-
csw-
swch
ch/s
ch/s
/s
247
0
339
246
334
0
0
248
333
Table 2: Context Switches for Heterogeneous NFs
SCHED_
NORMAL
nvc
csw-
swch
ch/s
/s
0
3544
6205
0
SCHED_
BATCH
nvc
swch
/s
527
479
0
nvc
swch
/s
3
4
3
csw-
ch/s
0
0
0
csw-
ch/s
266
265
266
csw-
ch/s
0
0
/s
333
333
334
9753
1007
9
SCHED_
RR
nvc
swch
/s
5
5
3
SCHED_
NORMAL
nvc
csw-
swch
ch/s
/s
0
0
33785
32214
107
65796
Even Load
SCHED_
BATCH
nv
SCHED_
RR
csw-
ch/s
0
1
1010
cswch
/s
504
505
8
csw-
ch/s
198
204
206
nvc
swch
/s
7
2
0
SCHED_
NORMAL
nvc
csw-
swch
ch/s
/s
0
0
38585
41089
79479
85
Uneven Load
SCHED_
BATCH
nvc
swch
/s
503
496
4
csw-
ch/s
0
4
1004
SCHED_
RR
csw-
ch/s
85
92
93
nvc
swch
/s
10
1
0
NF
NF1
NF2
NF3
NF
NF1
NF2
NF3
In our first test, illustrated in Figure 1a, all 3 NFs have equal
computation cost (roughly 250 CPU cycles per packet). With an
even load sent to all NFs, we find that the three schedulers per-
form about the same, with an equal division of CPU time leading
to equal throughputs for each NF. However, reducing the traffic
to NF3 by half shows the different behaviour of the schedulers:
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Kulkarni et al.
while the CFS-based schedulers continue to evenly divide the CPU
(CFS’s definition of fairness), the RR scheduler allocates CPU time
in proportion to the arrival rate, which better matches our notion
of rate proportional fairness. This happens because RR uses a time
quantum that is substantially longer than an NF ever needs, so NFs
which yield the CPU earlier (i.e., because they have fewer packets
to process) receive less CPU time and thus have lower through-
put. Note the context switches (shown in Table 1) in RR case are
predominantly voluntary context switches, while the CFS based
schedulers incur non-voluntary context switches.
We next consider heterogeneous NFs (computation costs: NF1=
500, NF2=250 and NF3=50 CPU cycles) with even or uneven load.
Figure 1b shows that when arrival rates are the same, none of the
schedulers are able to provide our fairness goal—an equal output
rate for all three NFs. CFS Normal always apportions CPU equally,
regardless of offered load and NF processing cost, so the lighter
weight NF3 gets the highest throughput. The RR scheduler is the
opposite since it gives each NF an equal chance to run, but does
not limit the time the NF runs for. The CFS Batch scheduler is
in between these extremes since it seeks to provide fairness, but
over longer time periods. Notably, the Batch scheduler provides
NF3 almost the same throughput as Normal CFS, despite allocating
it substantially less CPU. The reason for this is that Normal CFS
can incur a very large number of context switches due to its goal
of providing very fine-grained fairness. Since Batch mode reduces
scheduler preemption, it has substantially fewer non-voluntary con-
text switches—reducing from 65K to 1K per second—as illustrated
in the Table 2. While RR also has low context switch overhead,
it allows heavy weight NFs to greedily consume the CPU, nearly
starving NF3.
These results show that just having the Linux scheduler handle
scheduling NFs has undesirable results as by itself it is unable to
adapt to both varying per-packet processing requirements of NFs
and packet arrival rates. Moreover, it is important to avoid the
overheads of excessive context switches. All of these scheduling
requirements must be met on a per-core basis, while accounting
for the behaviour of chains spanning multiple cores or servers.
3 DESIGN AND IMPLEMENTATION
In an NFV platform, at the top of the stack are one or more net-
work functions that must be scheduled in such a way that idle
work (i.e., while waiting for packets) is minimized and load on
the service chain is shed as early as possible so as to avoid wasted
work. However, the operating system’s process scheduler that lies
at the bottom of the software stack remains completely application
agnostic, with its goal of providing a fair share of system resources
to all processes. As shown in the prior section, the kernel sched-
uler’s metrics for scheduling are along orthogonal dimensions to
those desired by the network functions. NFVnice bridges the gap
by translating the scheduling requirements at the NFV application
layer to a format consumable by the operating system.
The design of NFVnice centers around the concept of assisted
preemptive scheduling, where network functions provide hints
to the underlying OS with regard to their utilization. In addition
to monitoring the average computation time of a network func-
tion per packet, NFVnice needs to know when NFs in a chain are
overloaded, or blocked on packet/disk I/O. The queues between
NFs in a service chain serve as a good indicator of pending work
at each NF. To facilitate the process of providing these metrics
from the NF implementation to the underlying operating system,
NFVnice provides network function implementations with an ab-
straction library called libnf. In addition to the usual tasks such
as efficient reading/writing packets from/to the network at line
rate and overlapping processing with non-blocking asynchronous
I/O, libnf co-ordinates with the NFVnice platform to schedule/de-
schedule a network function as necessary.
Modifying the OS scheduler to be aware of various queues in
the NFV platform is an onerous task that might lead to unneces-
sary maintenance overhead and potential system instability. One
approach is to change the priority of the NF based on the queue
length of packet at that NF. This will have the effect of increasing
the number of CPU cycles provided to that NF. This will require
the change to occur frequently as the queue length varies. The
change requires a system call, which consumes CPU cycles and
adds latency. In addition, with service chains, as the queue at an
upstream NF builds, its priority has to be raised to process packets
and deliver to a queue at the downstream NF. Then, the down-
stream NF’s priority will have to be raised. We believe that this
can lead to instability because of frequent changes and the delay
involved in effecting the change. This only gets worse with complex
service chains, where an NF is both an upstream NF for one service
chain and a downstream NF for another service chain.
Instead,
NFVnice leverages cgroups [5, 33], a standard user space primitive
provided by the operating system to manipulate process scheduling.
NFVnice monitors queue sizes, computation times and I/O activities
in user space with the help of libnf and manipulates scheduling
weights accordingly.
3.1 System Components
Figure 2 illustrates the key components of the NFVnice platform.
We leverage DPDK for fast user space networking [1]. Our NFV
platform is implemented as a system of queues that hold packet de-
scriptors pointing to shared memory regions. The NF Manager runs
on a dedicated set of cores and is responsible for ferrying packet
references between the NIC queues and NF queues in an efficient
manner. When packets arrive to the NIC, Rx threads in the NF
Manager take advantage of DPDK’s poll mode driver to deliver the
packets into a shared memory region accessible to all the NFs. The
Rx thread does a lookup in the Flow Table to direct the packet to the
appropriate NF. Once a flow is matched to an NF, packet descriptors