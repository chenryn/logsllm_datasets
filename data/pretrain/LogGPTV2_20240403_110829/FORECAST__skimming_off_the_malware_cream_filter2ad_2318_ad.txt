Once all these samples have been analyzed and if more analysis
resources are available, we proceed to select a second sample for
each peHash, and so on.
• Static: This selection strategy is similar to the previous one, but
uses a sample’s static cluster, identiﬁed with techniques from Jacob
et al.[23], instead of its peHash. Together with the peHash selection
strategy, this represents the state of the art in sample selection.
• ForeCast: We select samples for analysis using FORECAST. We
test FORECAST with values of L (the level of parallelism) ranging
between 50 and 1600.
• Optimum: Here we perform sample selection based on FORE-
CAST’s cluster scoring technique, but assuming that cluster predic-
tion is 100% accurate; That is, that we know to which behavioral
cluster each sample belongs, before executing it. Clearly, such a se-
lection strategy is not possible in practice, but it serves as an upper
bound on the beneﬁts a sample selection strategy can bring.
Figure 3 shows the simulation results for the network endpoint
scores and L = 100. On the X-axis, we have the capacity of the
simulated sandbox relative to the real sandbox. This is the per-
centage of the samples from each day that the simulated sandbox
is able to handle. On the Y-axis, we have the percentage of rel-
evant features observed over the entire 61 day period. Thus, the
Y-axis represents the percentage of C&C endpoints discovered by
the simulated sandbox.
Table 4: Simulation results at 15% sandbox capacity.
Number of
features
Percentage
of features
Impr. over
random
Random
Static
peHash
FORECAST, L = 50
FORECAST, L = 100
FORECAST, L = 200
FORECAST, L = 400
FORECAST, L = 800
FORECAST, L = 1600
Optimum
1645
2242
2504
3900
3857
3899
3821
3825
3732
5271
17%
23%
26%
41%
40%
41%
40%
40%
39%
55%
0%
36%
52%
137%
134%
137%
132%
133%
127%
220%
We can see that FORECAST clearly outperforms the random se-
lection strategy. To provide concrete numbers, we have picked 15%
of the simulated sandbox capacity to compare the approaches, be-
cause the more limited the resources are, the more important it is
which samples are selected for analysis. The results are shown
05101520253035404550556065707580859095100Simulated sandbox capacity0102030405060708090100Percentage of featuresOptimumForeCastpeHashStaticRandom0123456789101112131415161718192021222324Time in hours0.00.20.40.60.81.0ProbabilityOptimumForeCastpeHashStaticRandomshould be analyzed must be much faster than actually running dy-
namic analysis on it.
Table 5 shows FORECAST’s run-time for the simulation described
in the previous section on the 2010 dataset, running on a single
server. As we can see, the total time per sample is under one sec-
ond. This is negligible compared to the four minutes our sandbox
spends executing each sample. Note that the cost of running AV
engines on the samples is not included in this ﬁgure, because we
obtain AV results from the VirusTotal service [7]. Performing AV-
scanning with all engines supported by VirusTotal would require
an additional ﬁve seconds per sample using a single machine [16].
4.5 Evasion
Our experiments have shown that FORECAST is effective in se-
lecting for dynamic analysis samples that will provide useful in-
formation. However, malware authors could attempt to avoid anal-
ysis by tricking our system into not selecting their binaries. For
this, they could attack our cluster prediction component, which ul-
timately relies on the static features discussed in Section 3.2. A
ﬁrst approach would be to mutate malware samples so that our sys-
tem cannot statically recognize variants as similar. For this, mal-
ware authors could develop techniques for polymorphic mutation
designed to evade peHash [41] as well as static clustering [23].
Furthermore, they could try to confuse the AV companies’ (propri-
etary) techniques for assigning names to malware variants. If such
mutation techniques were successful and widespread, FORECAST
would become at best useless. However, an individual malware
author has little incentive to deploy such a technique, because it
would not succeed in evading analysis for his samples. The reason
is that our cluster prediction component would most likely assign
such novel-looking samples to the other cluster O. As we can see
from Table 3, because of its diversity, the other cluster is ranked
quite high by our cluster scoring algorithm.
Alternatively, a malware author could attempt to perform a mimicry
attack, tricking FORECAST into assigning his malware to a cluster
that has very little interesting behavior (such as the Allaple cluster
shown in Table 3). Indeed it is relatively straightforward to develop
a sample that resembles a variant of the Allaple worm. However,
such a sample would hardly be successful, as it would be immedi-
ately detected by most AV engines. Evading detection by AV en-
gines while performing mimicry against FORECAST’s cluster pre-
diction (which includes AV labels among its features) would seem
to be challenging.
5. RELATED WORK
There exists a large body of related work on malware detection
and analysis. Currently, the most popular approach for malware
analysis relies on sandboxes [13, 3, 4, 5, 6]. A sandbox is an in-
strumented execution environment that runs an unknown program,
recording its interactions with the operating system (via system
calls) or other hosts (via the network). Often, this execution en-
vironment is realized as a system emulator or a virtual machine.
For each malware program that is analyzed, a sandbox will pro-
duce a report that details the host-based actions of the sample and
the network trafﬁc that it produces.
Based on the reports that capture the dynamic activity of mal-
ware programs, it is possible to ﬁnd clusters of samples that per-
form similar actions [11, 14], or to perform supervised malware
classiﬁcation to detect samples from known malware families [36].
In addition to approaches that perform malware clustering and
classiﬁcation on the output of sandboxes, there are a number of
static techniques that share the same goal but operate directly on the
malware executable [26, 35, 25, 40]. Unfortunately, these tools all
assume that the malicious code is ﬁrst unpacked and disassembled.
However, existing generic unpackers rely on the dynamic instru-
mentation of executables [37, 28, 24]. That is, these systems need
to execute the sample. This is a problem in our context, because
we aim to avoid the overhead associated with dynamic analysis and
need to pick a sample without executing the malware ﬁrst.
A few tools can process packed malware samples but do not re-
quire a previous, dynamic unpacking step. Some of these tools [33]
do not attempt to classify (or cluster) malware programs directly.
Instead, they use static analysis only to distinguish between packed
and unpacked executables. Packed executables are then forwarded
to a dynamic unpacker. We are only aware of two systems that can
detect duplicate malware samples using a fully static approach [41,
23]. Our system uses both of these tools to produce input that we
then leverage for the cluster prediction step. As our experiments
demonstrate, FORECAST signiﬁcantly outperforms these systems.
Finally, [12] takes a dynamic approach to duplicate sample de-
tection: All samples are executed in the sandbox, but after a short
time-out (1 minute) the behavior so far is compared with the be-
havior of previously analyzed samples. If the sample is detected as
a duplicate, execution is immediately terminated, otherwise analy-
sis continues until a longer analysis time-out. The authors do not
evaluate their approach with respect to an objective function. In
any case, their system requires at least one minute to discard “un-
interesting” samples, while FORECAST spends only 0.64 seconds
on each sample, as shown in Table 5.
6. CONCLUSION
Given the ﬂood of tens of thousands of malware samples that
are discovered every day, time is a valuable resource for a dynamic
malware analysis system. Of course, the available time should be
spent on analyzing samples that are most relevant, where relevance
depends on the goals of the analyst and the application domain.
For example, for botnet C&C analysis, one would prefer to pick
samples that produce network trafﬁc and reveal the locations of
C&C servers.
In this paper, we presented FORECAST, a system that can select
the malware sample that is most likely to yield relevant informa-
tion, given a domain-speciﬁc scoring function. The key require-
ment is that this selection process has to be performed efﬁciently,
on a possibly large pool of candidates, and without actually running
a sample. To realize our approach, we use a large number of in-
put features that are statically extracted from malware executables.
These features are then fed into a predictor, which estimates the ex-
pected information gain when executing a sample. This predictor
uses machine learning techniques and leverages a knowledge base
built from previously-analyzed samples. Our experiments demon-
strate that FORECAST is effective in selecting interesting samples.
On a test set of more than 600 thousand malware samples, our sys-
tem showed a high accuracy and signiﬁcantly outperformed a se-
lection strategy that simply avoids picking similar (or duplicate)
samples.
7. ACKNOWLEDGEMENTS
The research leading to these results has received funding from
the European Union Seventh Framework Programme under grant
agreement n. 257007 (SysSec), from the Prevention, Preparedness
and Consequence Management of Terrorism and other Security-
related Risks Programme European Commission - Directorate-General
Home Affairs (project i-Code), and from the Austrian Research
Promotion Agency (FFG) under grant 820854 (TRUDIE).
8. REFERENCES
[1] Adblock List. Retrieved September 2010 from https:
//secure.fanboy.co.nz/fanboy-adblock.txt.
[2] Anubis. http://anubis.iseclab.org.
[3] CWSandbox. http://www.cwsandbox.org.
[4] Joebox: A Secure Sandbox Application for Windows.
http://www.joebox.org/.
[5] Norman Sandbox. http://sandbox.norman.com.
[6] ThreatExpert. http://www.threatexpert.com.
[7] Virustotal. http://www.virustotal.com.
[8] Zeus Tracker. https://zeustracker.abuse.ch.
[9] Panda Labs Annual Report, 2010.
[10] E. L. Allwein, R. E. Schapire, and Y. Singer. Reducing
Multiclass to Binary: A Unifying Approach for Margin
Classiﬁers. In International Conference on Machine
Learning, 2000.
[11] M. Bailey, J. Oberheide, J. Andersen, Z. Mao, F. Jahanian,
and J. Nazario. Automated Classiﬁcation and Analysis of
Internet Malware. In Symposium on Recent Advances in
Intrusion Detection (RAID), 2007.
[12] U. Bayer, E. Kirda, and C. Kruegel. Improving the Efﬁciency
of Dynamic Malware Analysis. In Symposium On Applied
Computing (SAC), 2010.
[13] U. Bayer, C. Kruegel, and E. Kirda. TTAnalyze: A Tool for
Analyzing Malware. In Annual Conf. of the European
Institute for Computer Antivirus Research (EICAR), 2006.
[14] U. Bayer, P. Milani Comparetti, C. Kruegel, and E. Kirda.
Scalable, Behavior-Based Malware Clustering. In Network
and Distributed System Security (NDSS), 2009.
[15] H. Binsalleeh, T. Ormerod, A. Boukhtouta, P. Sinha,
A. Youssef, M. Debbabi, and L. Wang. On the analysis of the
zeus botnet crimeware toolkit. In International Conference
on Privacy, Security and Trust, 2010.
[16] J. Canto. VirusTotal Timing Information. Hispasec Systemas.
Private communication, 2011.
[17] A. Decker, D. Sancho, L. Kharouni, M. Goncharov, and
R. McArdle. A study of the Pushdo / Cutwail Botnet.
http://us.trendmicro.com/imperia/md/
content/us/pdf/threats/securitylibrary/
Study_of_pushdo.pdf, 2009.
[18] M. Dredze and K. Crammer. Conﬁdence-Weighted Linear
Classiﬁcation. In International conference on Machine
learning, 2008.
[19] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.
Lin. LIBLINEAR: A Library for Large Linear Classiﬁcation.
Journal of Machine Learning Research, 9, 2008.
[20] T. Hastie and R. Tibshirani. Classiﬁcation by Pairwise
Coupling. In Advances in neural information processing
systems, 1997.
[21] T.-K. Huang, R. C. Weng, and C.-J. Lin. Generalized
Bradley-Terry Models and Multi-Class Probability
Estimates. Journal of Machine Learning Research, 7, 2006.
[22] G. Jacob, R. Hund, T. Holz, and C. Kruegel. JACKSTRAWS:
Picking Command and Control Connections from Bot
Trafﬁc. In USENIX Security Symposium, 2011.
[23] G. Jacob, M. Neugschwandtner, P. Milani Comparetti,
C. Kruegel, and G. Vigna. A static, packer-agnostic ﬁlter to
detect similar malware samples. Technical Report 2010-26,
UC Santa Barbara, 2010.
[24] M. G. Kang, P. Poosankam, and H. Yin. Renovo: A Hidden
Code Extractor for Packed Executables. In ACM Workshop
on Recurring malcode (WORM), 2007.
[25] A. Karnik, S. Goswami, and R. Guha. Detecting Obfuscated
Viruses Using Cosine Similarity Analysis. In Asia
International Conference on Modelling & Simulation, 2007.
[26] J. Kolter and M. Maloof. Learning to Detect Malicious
Executables in the Wild. Journal of Machine Learning
Research, 7, 2006.
[27] B. Krebs. Takedowns: The Shuns and Stuns That Take the
Fight to the Enemy. McAfee Security Journal, (6), 2010.
[28] L. Martignoni, M. Christodorescu, and S. Jha. Omniunpack:
Fast, generic, and safe unpacking of malware. In Annual
Computer Security Applications Conf. (ACSAC), 2007.
[29] A. Mushtaq. Smashing the Mega-d/Ozdok botnet in 24
hours. http://blog.fireeye.com/research/
2009/11/smashing-the-ozdok.html, 2009.
[30] M. Neugschwandtner, P. Milani Comparetti, G. Jacob, and
C. Kruegel. FORECAST - Skimming off the Malware Cream
(extended version). Technical Report TR-iSecLab-0911-001,
Vienna University of Technology, 2011.
[31] R. Paleari, L. Martignoni, E. Passerini, D. Davidson,
M. Fredrikson, J. Gifﬁn, and S. Jha. Automatic Generation of
Remediation Procedures for Malware Infections. In USENIX
Security Symposium, 2010.
[32] V. Paxson. Bro: A System for Detecting Network Intruders
in Real-Time. In USENIX Security Symposium, 1998.
[33] R. Perdisci, A. Lanzi, and W. Lee. McBoost: Boosting
Scalability in Malware Collection and Analysis Using
Statistical Classiﬁcation of Executables. In Annual Computer
Security Applications Conference (ACSAC), 2008.
[34] R. Perdisci, W. Lee, and N. Feamster. Behavioral Clustering
of HTTP-Based Malware and Signature Generation Using
Malicious Network Traces. In USENIX conference on
Networked Systems Design and Implementation, 2010.
[35] K. Reddy, S. Dash, and A. Pujari. New Malicious Code
Detection Using Variable Length n-grams. In Information
Systems Security Conference, 2006.
[36] K. Rieck, T. Holz, C. Willems, P. Düssel, and P. Laskov.
Learning and Classiﬁcation of Malware Behavior. In
Detection of Intrusions and Malware and Vulnerability
Assessment (DIMVA), 2008.
[37] P. Royal, M. Halpin, D. Dagon, R. Edmonds, and W. Lee.
PolyUnpack: Automating the Hidden-Code Extraction of
Unpack-Executing Malware. In 22nd Annual Computer
Security Applications Conf. (ACSAC), 2006.
[38] D. Sancho. You Scratch My Back... Bredolab’s Sudden Rise
in Prominence. http://us.trendmicro.com/
imperia/md/content/us/trendwatch/
researchandanalysis/bredolab_final.pdf,
2009.
[39] B. Stone-Gross, A. Moser, C. Kruegel, K. Almaroth, and
E. Kirda. FIRE: FInding Rogue nEtworks. In Annual
Computer Security Applications Conference (ACSAC), 2009.
[40] S. Tabish, M. Shaﬁq, and M. Farooq. Malware Detection
Using Statistical Analysis of Byte-Level File Content. In
ACM SIGKDD Workshop on CyberSecurity and Intelligence
Informatics, 2009.
[41] G. Wicherski. peHash: A Novel Approach to Fast Malware
Clustering. In 2nd USENIX Workshop on Large-Scale
Exploits and Emergent Threats (LEET), 2009.