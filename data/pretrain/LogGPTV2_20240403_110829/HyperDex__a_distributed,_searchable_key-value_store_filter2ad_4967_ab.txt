servers.
HyperDex avoids the problems associated with high-di-
mensionality by partitioning tables with many attributes
into multiple lower-dimensional hyperspaces called subspaces.
Each of these subspaces uses a subset of object attributes as
the dimensional axes for an independent hyperspace. Fig-
ure 2 shows how HyperDex can partition a table with D
attributes into multiple independent subspaces. When per-
forming a search on a table, clients select the subspace that
contacts the fewest servers, and will issue the search to
servers in exactly one subspace.
Data partitioning increases the eﬃciency of a search by
reducing the dimensionality of the underlying hyperspace.
In a 9-dimensional hyperspace, a search over 3 attributes
would need to contact 64 regions of the hyperspace (and
thus, 64 servers).
If, instead, the same table were parti-
tioned into 3 subspaces of 3 attributes each, the search will
never contact more than 8 servers in the worst case, and
exactly one server in the best case. By partitioning the ta-
ble, HyperDex reduces the worst case behavior, decreases
the number of servers necessary to maintain a table, and
increases the likelihood that a search is run on exactly one
server.
Data partitioning forces a trade-oﬀ between search gener-
ality and eﬃciency. On the one hand, a single hyperspace
can accommodate arbitrary searches over its associated at-
tributes. On the other hand, a hyperspace which is too large
will always require that partially-speciﬁed queries contact
many servers. Since applications often exhibit search local-
ity, HyperDex applications can tune search eﬃciency by cre-
ating corresponding subspaces. As the number of subspaces
grows, so, too, do the costs associated with maintaining data
consistency across subspaces. Section 4 details how Hyper-
Dex eﬃciently maintains consistency across subspaces while
maintaining a predictably low overhead.
3.1 Key Subspace
The basic hyperspace mapping, as described so far, does
not distinguish the key of an object from its secondary at-
tributes. This leads to two signiﬁcant problems when im-
plementing a practical key-value store. First, key lookups
would be equivalent to single attribute searches. Although
HyperDex provides eﬃcient search, a single attribute search
in a multi-dimensional space would likely involve contacting
more than one server. In this hypothetical scenario, key op-
erations would be strictly more costly than key operations
in traditional key-value stores.
HyperDex provides eﬃcient key-based operations by cre-
ating a one-dimensional subspace dedicated to the key. This
subspace, called the key subspace, ensures that each object
will map to exactly one region in the resulting hyperspace.
Further, this region will not change as the object changes
because keys are immutable. To maintain the uniqueness
invariant, put operations are applied to the key subspace
before the remaining subspaces.
3.2 Object Distribution Over Subspaces
Subspace partitioning exposes a design choice in how ob-
jects are distributed and stored on servers. One possible
design choice is to keep data in normalized form, where
every subspace retains, for each object, only those object
attributes that serve as the subspace’s dimensional axes.
While this approach would minimize storage requirements
per server, as attributes are not duplicated across subspaces,
it would lead to more expensive search and object retrieval
operations since reconstituting the object requires cross-serv-
er cooperation. In contrast, an alternative design choice is
to store a full copy of each object in each subspace, which
leads to faster search and retrieval operations at the expense
of additional storage requirements per server.
Hyperspace hashing supports both of these object distri-
bution techniques. The HyperDex implementation, how-
ever, relies upon the latter approach to implement the repli-
cation scheme described in Section 4.
3.3 Heterogeneous Objects
In a real deployment, the key-value store will likely be
used to hold disparate objects with diﬀerent schema. Hyper-
Dex supports this through the table abstraction. Each table
has a separate set of attributes which make up the objects
within, and these attributes are partitioned into subspaces
independent of all other tables. As a result, HyperDex man-
ages multiple independent hyperspaces.
4. CONSISTENCY AND REPLICATION
Because hyperspace hashing maps each object to multiple
servers, maintaining a consistent view of objects poses a
challenge. HyperDex employs a novel technique called value-
dependent chaining to provide strong consistency and fault
tolerance in the presence of concurrent updates.
For clarity, we ﬁrst describe value-dependent chaining with-
out concern for fault tolerance. Under this scheme, a single
failure leaves portions of the hyperspace unavailable for up-
dates and searches. We then describe how value-dependent
chaining can be extended such that the system can tolerate
up to f failures in any one region.
4.1 Value Dependent Chaining
Because hyperspace hashing determines the location of
an object by its contents, and subspace partitioning creates
many object replicas, objects will be mapped to multiple
servers and these servers will change as the objects are up-
dated. Change in an object’s location would cause problems
if implemented naively. For example, if object updates were
to be implemented by simply sending the object to all af-
fected servers, there would be no guarantees associated with
subsequent operations on that object. Such a scheme would
at best provide eventual consistency because servers may
receive updates out-of-order, with no sensible means of re-
solving concurrent updates.
HyperDex orders updates by arranging an object’s repli-
cas into a value-dependent chain whose members are deter-
ministically chosen based upon an object’s hyperspace coor-
dinate. The head of the chain is called the point leader, and
is determined by hashing the object in the key subspace.
Subsequent servers in the chain are determined by hashing
attribute values for each of the remaining subspaces.
This construction of value-dependent chains enables ef-
ﬁcient, deterministic propagation of updates. The point
leader for an object is in a position to dictate the total order
on all updates to that object. Each update ﬂows from the
point leader through the chain, and remains pending un-
til an acknowledgement of that update is received from the
next server in the chain. When an update reaches the tail,
the tail sends an acknowledgement back through the chain
in reverse so that all other servers may commit the pending
update and clean up transient state. When the acknowl-
edgement reaches the point leader, the client is notiﬁed that
the operation is complete. In Figure 3, the update u1 illus-
trates an object insertion which passes through h1, h2, h3,
where h1 is the point leader.
Updates to preexisting objects are more complicated be-
cause a change in an attribute value might require relocating
an object to a diﬀerent region of the hyperspace. Value-
dependent chains address this by incorporating the servers
assigned to regions for both the old and new versions of
the object. Chains are constructed such that servers are or-
dered by subspace and the servers corresponding to the old
version of the object immediately precede the servers corre-
sponding to the new version. This guarantees that there is
no instant during an update where the object may disappear
from the data store. For example, in Figure 3, the update
u2 modiﬁes the object in a way that changes its mapping
in Subspace 0 such that the object no longer maps to h2
and instead maps to h5. The value-dependent chain for up-
date u2 is h1, h2, h5, h3. The update will result in the object
being stored at h5, and subsequently removed from h2, as
acknowledgments propagate in reverse.
Successive updates to an object will construct chains which
overlap in each subspace. Consequently, concurrent updates
may arrive out of order at each of these points of overlap.
For example, consider the update u3 in Figure 3. The value-
dependent chain for this update is h1, h5, h3. Notice that it
h1
h2
h3
update u1
update u2
update u3
h5
h6
h4
key
subspace
subspace 0
subspace 1
Figure 3: HyperDex’s replication protocol propa-
gates along value-dependent chains. Each update
has a value-dependent chain that is determined
solely by objects’ current and previous values and
the hyperspace mapping.
is possible for u3 to arrive at h5 before u2. If handled im-
properly, such races could lead to inconsistent, out-of-order
updates. Value-dependent chains eﬃciently handle this case
by dictating that the point leader embed, in each update,
dependency information which speciﬁes the order in which
updates are applied. Speciﬁcally, the point leader embeds
a version number for the update, and the version number,
hash and type of the previous update. For instance, u3 will
have a version number of 3 and depend upon update u2 with
version number 2 and type put. Servers which receive u3 be-
fore u2 will know to delay processing of u3 until u2 is also
processed.
By design, HyperDex supports destructive operations that
remove all state pertaining to deleted objects. Examples of
destructive operations include delete and the cleanup as-
sociated with object relocation. Such operations must be
carefully managed to ensure that subsequent operations get
applied correctly. For instance, consider a del followed by a
put. Since we would like a del to remove all state, the put
must be applied on servers with no state. Yet, if another
del/put pair were processed concurrently, servers which had
processed either del would not be able to properly order
the put operations. Value-dependent chains ensure that
concurrently issued destructive operations are correctly or-
dered on all servers. Each server independently delays op-
erations which depend upon a destructive operation until
the destructive operation, and all that came before it, are
acknowledged. This ensures that at most one destructive
operation may be in-ﬂight at any one time and guarantees
that they will be ordered correctly. The delay for each mes-
sage is bounded by the length of chains, and the number of
concurrent operations.
4.2 Fault Tolerance
To guard against server failures, HyperDex provides ad-
ditional replication within each region. The replicas acquire
and maintain their state by being incorporated into value-
dependent chains. In particular, each region has f + 1 repli-
cas which appear as a block in the value-dependent chain.
For example, we can extend the layout of Figure 3 to toler-
ate one failure by introducing additional hosts h(cid:48)
1 through
h(cid:48)
6. As with regular chain replication [57], new replicas are
introduced at the tail of the region’s chain, and servers are
bumped forward in the chain as other servers fail. For ex-
ample, the ﬁrst update in Figure 3 has the value-dependent
chain h1, h(cid:48)
3. If h2 were to fail, the resulting
chain would be h1, h(cid:48)
2, h(cid:48)(cid:48)
3. This transition will be
performed without compromising strong consistency.
2, h3, h(cid:48)
1, h(cid:48)
1, h2, h(cid:48)
2 , h3, h(cid:48)
Point leader failures do not allow clients to observe an in-
consistency. For instance, if h1, the point leader, were to fail
in our previous example, h(cid:48)
1 will take over the role of point
leader. When a client detects a point leader failure, it will
notify the application and preserve at-most-once semantics.
Further, because all client acknowledgments are generated
by the point leader, the client will only see a response after
an object is fully fault-tolerant.
In our implementation, HyperDex uses TCP to transmit
data which ensures that messages need only be retrans-
mitted when servers fail and are removed from the value-
dependent chain. In response to failures, HyperDex servers
retransmit messages along the chains to make progress. Upon
chain reconﬁguration, servers will no longer accept messages
from failed servers, ensuring that all future messages traverse
the new, valid chain.
4.3 Server and Conﬁguration Management
HyperDex utilizes a logically centralized coordinator to
manage system-wide global state. The coordinator encap-
sulates the global state in the form of a conﬁguration which
consists of the hyperspace mapping between servers and re-
gions and information about server failures. The coordinator
assigns to each conﬁguration an epoch identiﬁer, a strictly
increasing number that identiﬁes the conﬁguration. The co-
ordinator distributes conﬁgurations to servers in order by
epoch, and updates the conﬁguration as necessary. Hyper-
Dex’s coordinator holds no state pertaining to the objects
themselves; only the mapping and servers.
HyperDex ensures that no server processes late-arriving
messages from failed or out-of-date servers. Each HyperDex
server process (an instance) is uniquely identiﬁed by its IP
address, port and instance id. Instance ids are assigned by
the coordinator and are globally unique, such that servers
can distinguish between two instances (e.g. a failed process
and a new one) that reuse the same IP and port number.
HyperDex embeds into messages the instance ids, the regions
of the hyperspace and indices into the chain for both the
sender and recipient. A recipient acting upon a message
can validate the sender and recipient against its most recent
conﬁguration. If the message contains a stale mapping, it
will not be acted upon. If, instead, the mapping is valid, the
host processes the message accordingly.
Each host changes its conﬁguration in an all-or-nothing
fashion which appears instantaneous to threads handling
network communication. This is accomplished on each host
by creating state relevant to the new conﬁguration, pausing
network traﬃc, swapping pointers to make the new state
visible, and unpausing network traﬃc. This operation com-
pletes in sub-millisecond time on each host.
The logically centralized coordinator does not impose a
bottleneck. Conﬁgurations are small in practice, and pro-
portional to the size of the cluster and number of tables ac-
tive on the cluster. Furthermore, in cases where bandwidth
from the coordinator becomes a bottleneck, the coordina-
tor need only distribute deltas to the conﬁguration. Clients
maintain a complete copy of the conﬁguration in memory
and perform local computation on the hyperspace mapping.
4.4 Consistency Guarantees
Overall, the preceding protocol ensures that HyperDex
provides strong guarantees for applications. The speciﬁc
guarantees made by HyperDex are:
Key Consistency All actions which operate on a speciﬁc
key (e.g., get and put) are linearizable [26] with all opera-
tions on all keys. This guarantees that all clients of Hyper-
Dex will observe updates in the same order.
Search Consistency HyperDex guarantees that a search
will return all objects that were committed at the time of
search. An application whose put succeeds is guaranteed to
see the object in a future search.
In the presence of con-
current updates, a search may return both the committed
version, and the newly updated version of an object match-
ing the search.
HyperDex provides the strongest form of consistency for
key operations, and a conservative and predictable consis-
tency guarantees for search operations.
5.
IMPLEMENTATION
HyperDex is fully implemented to support all the features
described in this paper. The implementation is nearly 44,000
lines of code. The HyperDex software distribution contains
an embeddable storage layer called HyperDisk, a hyperspace
hashing library, the HyperDex server, the client library and
the HyperDex coordinator, as well as full client bindings for
C, C++, and Python, and partial bindings for Java, Node.JS
and Ruby.
This section describes key aspects of the HyperDex im-
plementation.
5.1 HyperDex Server
High throughput and low latency access are essential for
any key-value store. The HyperDex server achieves high per-
formance through three techniques; namely, edge-triggered,
event-driven I/O; pervasive use of lock-free datastructures
and sharded locks to maximize concurrency; and careful
attention to constant overheads. The HyperDex server is