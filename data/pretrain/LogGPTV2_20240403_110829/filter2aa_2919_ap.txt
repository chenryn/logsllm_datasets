192 https://bugs.chromium.org/p/chromium/issues/detail?id=1019226 
193 https://blog.exodusintel.com/2019/01/22/exploiting-the-magellan-bug-on-64-bit-chrome-desktop/ 
194 https://www.zerodayinitiative.com/blog/2018/4/5/quickly-pwned-quickly-patched-details-of-the-mozilla-
pwn2own-exploit 
195 http://blogs.360.cn/post/how-to-kill-a-firefox-en.html 
196 https://android-developers.googleblog.com/2018/01/android-security-ecosystem-investments.html 
 106 
• 
Firefox bug 982957 (CVE-2014-1512, Pwn2Own 2014) is a very specific use-after-free 
vulnerability which is just triggered under high memory pressure. Exploitation details can 
be found at 197. 
• 
Firefox bug 1299686 (CVE-2016-9066) is not covered because the vulnerability is just 
triggered by loading an additional JavaScript file and returning malicious HTTP headers. 
A writeup is available at 198 and an exploit is available at 199. 
• 
Firefox bug 1352681 (2017) is a specific vulnerability and just affected a beta version of 
Firefox. It exploits an integer overflow together with a reference leak to achieve full code 
execution. A detailed writeup is available at 200. 
• 
Safari CVE-2019-8559 is not covered because the PoC is too long. It is a vulnerability 
with a missing write barrier, as explained in chapter 4.2.1. Exploitation details were 
presented at OffensiveCon 2020 by Ahn 201. The exploit contains an interesting concept 
to trigger vulnerabilities with missing write barriers by creating large trees of arrays. 
Marking all objects in the tree during garbage collection takes a long time and therefore 
write barriers can more efficiently be found. 
Vulnerabilities where details were not published when the analysis was performed: 
• 
Details 202 for CVE-2019-5877 were published at BlackHat 2020. It is a vulnerability in v8 
torque code which allows to access data OOB. Gong chained this vulnerability with two 
other bugs to remotely root Android devices. It was the first demonstrated remote root 
exploit chain against Pixel phones and it received the highest reward for an exploit chain 
across all Google VRP programs. 
• 
Details 203 for CVE-2020-9850 were published at BlackHat 2020. It is a vulnerability in 
JSC which was combined with five other vulnerabilities to create an exploit chain to target 
Safari at Pwn2Own 2020. The vulnerability occurred because side-effects of the ‘in’ 
operator were incorrectly modeled. It is therefore an example of the vulnerability category 
described in chapter 4.5.3. 
• 
Firefox bug 1607443 (CVE-2019-17026) is a 0day and was used in-the-wild in targeted 
attacks 204 205. Details were later published in the Google Project Zero Blog 206. The root 
cause of the vulnerability is also an incorrect side effect annotation. 
197http://web.archive.org/web/20150710021003/http://www.vupen.com/blog/20140520.Advanced_Exploitation_
Firefox_UaF_Pwn2Own_2014.php 
198 https://saelo.github.io/posts/firefox-script-loader-overflow.html 
199 https://github.com/saelo/foxpwn 
200 https://phoenhex.re/2017-06-21/firefox-structuredclone-refleak 
201 https://www.youtube.com/watch?v=fTNzylTMYks 
202 https://github.com/secmob/TiYunZong-An-Exploit-Chain-to-Remotely-Root-Modern-Android-Devices 
203 https://github.com/sslab-gatech/pwn2own2020 
204 https://www.mozilla.org/en-US/security/advisories/mfsa2020-03 
205 https://blogs.jpcert.or.jp/en/2020/04/ie-firefox-0day.html 
206 https://googleprojectzero.blogspot.com/p/rca-cve-2019-17026.html 
 107 
5 Applying variation analysis 
The knowledge obtained from the analysis and the generalization for variation analysis 
paragraphs from chapter 4 were applied to improve a current state-of-the-art fuzzer. 
One conclusion of the previous chapter was that new vulnerabilities in the render engine can 
mainly be found by improving the grammar definition files of a fuzzer. Moreover, exploitation 
of these vulnerabilities is complex and time-consuming. 
The focus of applying variation analysis was therefore laid on fuzzing a JavaScript engine. 
This is coherent with vulnerability reports released during the last years which mainly 
focused on JavaScript engines. Moreover, all analyzed vulnerabilities in this thesis except 
seven bugs belong to the JavaScript engine. It is therefore conclusive to focus on an attack 
target where the claimed improvements should pose the most significant impact. 
5.1 Adaption of a state-of-the-art fuzzer 
The fuzzilli fuzzer was identified as a state-of-the-art fuzzer which recently demonstrated 
convincing results 207. The fuzzer was therefore selected as foundation to implement the 
improvements mentioned in chapter 4. 
Groß [39] observed that the use of AddressSanitizer together with JavaScript engines did 
not lead to more identified crashes. This can probably be attributed to the custom heap 
allocator used by JavaScript engines. To achieve a faster fuzzing speed, AddressSanitizer 
was therefore not used during the experiment. 
The following fuzzilli components were integrated into the developed fuzzer: 
• 
Lib-REPRL (read-eval-print-reset-loop) 
A library which performs in-memory executions of JavaScript code in the target 
JavaScript engine. 
• 
Lib-Coverage 
A library which measures edge coverage feedback in the JavaScript engine using 
LLVM sanitizer coverage. 
Both libraries were combined to a single library written in C which provides an interface for 
Python code. The exposed methods allow to execute JavaScript code with a fast execution 
speed. The function return value indicates if the code resulted in new behavior, in a crash, 
in a timeout or in an exception. During tests an execution speed of approximately 40 
processed test cases per second per CPU core was achieved in a virtual machine. If 
coverage feedback is not required, an execution speed of 200 tests per second per CPU 
core were achieved. A similar test was conducted on a t2.micro instance on AWS. On this 
system 112 test cases could be processed per second per CPU core with coverage feedback 
207 https://github.com/googleprojectzero/fuzzilli#bug-showcase 
 108 
enabled and 426 test cases per second with disabled coverage feedback. More detailed 
system specifications are available in chapter 5.4. 
The following library modifications were implemented: 
• 
The C code was originally used as a Swift module in fuzzilli. Since the author of this 
thesis is more fluent in Python, the code was ported to a Python module. This allows 
implementing mutation strategies and corpus management in Python while in-
memory execution and coverage feedback, which must be fast, are implemented in 
C. 
• 
The coverage feedback was initially sometimes unstable. Multiple executions of the 
same code led to different coverage feedback. Since stable results are crucial to 
obtain an excellent corpus, the following improvements were implemented: 
o If JavaScript code results in new coverage, the code is executed again until 
it does not lead to new coverage. This is important because otherwise a small 
modification of JavaScript code could incorrectly be added to the corpus 
although the modification did not result in a new behavior. 
o If JavaScript code results in new coverage, the code is tested again to verify 
that it really triggers new behavior. This is important because otherwise 
indeterministic behavior in the JavaScript engine could lead to the insertion 
of multiple useless code samples into the corpus. Before the second 
execution is performed, the JavaScript engine is restarted to start with a fresh 
memory layout. This helps to further increase the stability of the results. Note 
that additional process restarts are not a performance bottleneck. Finding 
code that triggers new coverage is a rare event and therefore operations 
performed in such cases can be neglected. 
o The original code used sanitizer coverage to measure which control flow 
edges are executed. However, the code just reported the first execution of an 
edge in the current process. This leads to several problems. First, the 
executions are not stable. The REPRL implementation restarts the JavaScript 
engine after a specified number of test cases. After a restart, the engine 
reports during the first execution again edges which were not reported during 
the last executions because edges are just reported once in a process. To 
obtain stable results, this behavior was therefore modified to always report all 
executed edges. This has the additional advantage of a better performance. 
The additional if-condition, which would check if an edge was already 
previously seen, limits execution speed more than code which always saves 
all executed edges. Saving all executed edges therefore leads to more stable 
results and better performance. This improvement is especially important to 
obtain correct results when the same code is executed multiple times as 
suggested above. Another important use-case is test case minimization. 
When code, which triggers new behavior, is found, the code is reduced to a 
 109 
minimized sample which still triggers the new behavior. For the correct 
minimization, it is important that edges are reported again in subsequent 
executions.  
o Initially, dummy runs are performed to measure which edges correspond to 
code associated with starting and stopping an in-memory execution. These 
edges are not considered when identifying code that triggers new behavior.  
Fuzzilli uses an IL to mutate JavaScript code. This approach was chosen by Groß [39] to 
ensure that generated code does not lead to an exception because catching exceptions 
prevents the JavaScript compiler from optimizing the code. However, the implementation of 
an IL is time-consuming and was therefore not done. Instead, mutations are performed 
directly on JavaScript code in the experiment. To implement this, the experiment was split 
into two tasks. In the first task an initial corpus is generated and in the second task fuzzing 
is performed on the corpus. The corpus generation is further split into phases visualized in 
Figure 8: 
Figure 8: Phases of corpus generation 
 110 
1. Initially, test cases are downloaded from publicly available sources or generated 
using publicly available fuzzers. Additional test cases are self-created using a script. 
2. Next, the test cases are sanitized. This is required because test cases from for 
example SpiderMonkey cannot be executed in v8 because of different assert or 
native function names. 
3. All test cases are then executed and edge coverage feedback is extracted. This 
allows to identify test cases with unique behavior and to reduce the total number of 
test cases in the corpus which each trigger unique behavior. 
4. In the next step, the test cases must be modified by renaming variables, functions 
and classes to standardized names. This step is important to ensure that the fuzzer 
knows the names of all available tokens in later phases. 
5. After that, the corpus is minimized by removing blocks and code lines from the test 
cases, if they are not required to trigger the unique behavior. This ensures that test 
cases are minimal which increases fuzzer speed and the likelihood that a mutation 
is inserted at the correct position during fuzzing. 
6. Because of the minimization and line removal, similar test cases can arise. This 
phase removes such duplicate test cases. 
7. The result of the previous stage is stored in the first corpus. Details to corpus one 
and two are explained in the next chapter. A state file is then created for all corpus 
files. The state file encodes for example in which lines code can be inserted, which 
variables have which data type in which line and how often a line is executed. 
8. In the next phase, deterministic preprocessing is performed. The idea of this phase 
is similar to the deterministic fuzzing phase of AFL. In this phase, specific code lines 
are added at every possible location in the test case to yield new behavior. Examples 
of injected code are the invocation of the garbage collection, a call which leads to 
deoptimization, a call which prevents inlining the function and so on. If new coverage 
is found, the sample is added to the corpus and the steps are repeated. 
9. In the second deterministic preprocessing phase objects and callbacks are injected. 
The goal of this phase is not to identify new behavior. Instead, callback locations 
where code can be injected during fuzzing should be identified. For this, new objects 
with callback functions are added and arguments to functions are replaced by these 
objects. When the callback triggers, the sample is added to the second corpus. 
Moreover, classes are sub classed or proxied to trigger callbacks. During the later 
fuzzing phase, the fuzzer adds code especially at these callbacks to trigger 
unexpected behavior during the callback. 
The fuzzing phase is explained in-depth in chapter 5.3. The developed fuzzer adopted 
approximately 400 lines of code of Fuzzilli. The final fuzzer has over 8,000 lines of code and 
5,000 comment lines. 
 111 
5.2 Corpus generation 
The corpus is the collection of all test cases which trigger unique behavior during execution. 
Fuzzing can be started with an empty corpus so that the fuzzer creates the corpus on the fly 
during fuzzing. However, this approach can take a long time and therefore an initial corpus 
was created. During traditional fuzzing usually just one corpus is used. However, the author 
of this thesis reasons that fuzzing JavaScript engines is more efficient if the corpus is split 
into two parts. The arguments for this assumption are discussed below. 
The initial corpus generation is the reason why comparing stats from different fuzzers is not 
meaningful in this context. For example, the fuzzilli fuzzer must be started with an empty 
corpus and needs several days on one core on the test system to reach a coverage of 15 
percent. On the other hand, the fuzzer presented in this research already achieves over 25 
percent coverage within the first seconds because several weeks were spent on creating a 
comprehensive initial input corpus. 
The following chapters discuss the generation of the JavaScript code snippet corpus, the 
template corpus and the initial analysis of test cases. The corpus was created for v8 in 
version 8.1.307.28 on a x64 system. It may be possible to create an even bigger corpus by 
starting the developed scripts with other JavaScript engines like SpiderMonkey, JSC or 
ChakraCore and merging the final files to one large corpus. This is possible because it is 
likely that edge cases in one JavaScript engine also trigger edge cases in other JavaScript 
engines. 
5.2.1 Corpus of JavaScript code snippets 
In the first corpus small JavaScript code snippets are stored which trigger unique behavior. 
The following list shows examples of code snippets from this corpus: 
• 
globalThis.hasOwnProperty('String') 
• 
var var_1_ = [1,2,3]; Object.seal(var_1_); 
• 
Object.seal(undefined); 
• 
const var_1_ = -Infinity; const var_2_ = Math.atan(var_1_);  
• 
["1", "2", "3"].map(parseInt) 
• 
Map.prototype.set = null; 
These examples just contain simple statements. However, the full corpus contains over 
10,000 such test cases including complex test cases with over 1,000 lines of code which 
also contain unique combinations of control flow structures. These test cases can be viewed 
as building blocks which are used by the fuzzer to construct new test cases during fuzzing. 
The fuzzer combines these building blocks to create new test cases and adds additional 
code by using a JavaScript grammar. 
The following text describes how the corpus was created. 
 112 
Corpus created by Fuzzilli: 
The fuzzilli fuzzer was started inside a virtual machine (VM) on three cores independently 
for four days. Exact system specifications can be found in chapter 5.4. The target v8 engine 
was version 8.1.307.28. Synchronization was not enabled to check if the generated corpus 
files evolve differently. Fuzzilli achieved after some seconds a coverage of four percent and 
after several minutes a coverage of six percent. It starts with a high success rate (test cases 
which do not lead to an exception) of 80 percent, but this rate drops to 70 percent after some 
hours of fuzzing. It generates just a few timeouts which speeds up fuzzing. For example, 
after 50,000 executions only 25 timeouts were recorded. After 400,000 executions a 
coverage of 13.5 percent was achieved. The results after four days of fuzzing can be seen 
in Table 2. 
Core 
Total execs 
Coverage 
Corpus size 
Crashes Timeouts 
Success rate 
Avg. program size 
1 
2,868,881 
16.03 % 
6,261 
37 
71,658 
70.02 % 
104.13 
2 
2,073,209 
15.41 % 
5,249 
22 
63,754 
69.62 % 
196.20 
3 
2,789,794 
16.04 % 
6,374 
27 
73,246 
69.12 % 
117.20 
Table 2: Results of fuzzilli fuzzer 
The combined 17,884 files were minimized to 5,212 files which triggered unique behavior. 
These triggered together 93,605 of 596,937 possible edges which corresponds to a 15.68 
percent coverage. This coverage is slightly below the coverage listed in Table 2 because 