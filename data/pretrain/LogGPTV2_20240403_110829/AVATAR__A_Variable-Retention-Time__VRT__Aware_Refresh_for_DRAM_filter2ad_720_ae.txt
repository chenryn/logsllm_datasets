60
Loss
0.130%
0.065%
0.035%
0.018%
0.009%
Scrub Energy
Refresh Energy
4.00%
2.00%
1.06%
0.53%
0.27%
2) Energy Overhead of Scrubbing: Scrubbing is more
expensive in terms of energy than refresh as it needs to stream
the data on the memory bus. However, scrubbing is performed
much less frequently than refresh, so its overall contribution
to the system energy is quite small. The total energy to refresh
an 8GB DIMM once is approximately 1.1mJ, whereas the
energy for one scrub operation is approximately 161mJ (150x
more). However, the scrub energy is spent four orders of
magnitude less frequently (15 minutes vs. 64ms) than the
refresh energy. Table I shows the total energy consumed by
scrubbing compared to that consumed by refresh, as the scrub
interval is varied from four minutes to one hour. Scrubbing
with a 15-minute interval, as we assumed in our evaluations,
causes only a 1% increase in energy compared to refresh
energy.
VII. PERFORMANCE AND ENERGY ANALYSIS
We use a detailed memory system simulator, USIMM [8].
We model a quad-core system operating at 3.2GHz connected
to a DDR3-1600 (800MHz) memory system. As refresh over-
heads increase with technology scaling, we analyze DRAM
chips with density varying from 8Gb to 64Gb. The memory
system consists of four DIMMs, so the total size of the memory
system ranges from 32GB (for 8Gb chips) to 256GB (for 64Gb
chips). The baseline system employs JEDEC-speciﬁed 64ms
refresh. We increase the refresh cycle time linearly with density
(TRF C varies from 350ns to 2800ns).
We evaluate all the workloads provided by USIMM for
the Memory Scheduling Championship [2]. These workloads
are memory intensive and exercise a large number of trans-
actions between the memory system and the core [35]. These
18 workloads come from various suites, including SPEC(2),
PARSEC(9), BioBench(2), and Commercial(5). We report an
average over all the 18 workloads.
As the effectiveness of refresh savings with AVATAR de-
pends on time elapsed after retention time testing is performed,
we evaluate three designs: AVATAR-1, AVATAR-120, and
AVATAR-360 representing one day, 120 days, and 360 days
after a retention time test, respectively. We also show results
of a theoretical scheme that does not perform refresh.
A. Speedup
Figure 16 shows the speedup for AVATAR-1, AVATAR-
120, AVATAR-360, and No Refresh over the JEDEC speciﬁed
refresh scheme. The performance beneﬁt of eliminating re-
freshes increases with chip density, going from 4% at the 8Gb
node to 54% at the 64Gb node (as denoted by the No Refresh
bars). AVATAR provides about two-thirds of the performance
beneﬁt of No Refresh. Even after a year of continuous op-
eration, AVATAR maintains most of the performance beneﬁts
close to that of the ﬁrst day after retention testing. For instance,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:13:50 UTC from IEEE Xplore.  Restrictions apply. 
AVATAR improves performance by 35%, even a year after
retention time testing.
p
u
d
e
e
p
S
  1.6
  1.5
  1.4
  1.3
  1.2
  1.1
  1.0
  0.9
AVATAR−1
AVATAR−120
AVATAR−360
No Refresh
32GB
(8Gb Devices)  (16Gb Devices)  (32Gb Devices)  (64Gb Devices)
64GB
128GB
256GB
Fig. 16.
improves with technology node.
Speedup from refresh savings. The performance of AVATAR
B. Energy-Delay Product
Refresh operations not only cost performance but also
consume energy. Figure 17 shows the Energy Delay Product
(EDP) of AVATAR-1, AVATAR-120, AVATAR-360, and No
Refresh compared to the JEDEC speciﬁed refresh scheme. The
energy beneﬁts of eliminating refreshes also increase with high
density. No Refresh potentially reduces the EDP by 68% at the
64Gb node. AVATAR-1 reduces EDP by 8%, 16%, 31%, and
55% for the 8Gb, 16Gb, 32Gb, and 64Gb nodes, respectively.
AVATAR-360 has EDP savings close to those of AVATAR-1.
t
c
u
d
o
r
P
y
a
l
e
D
−
y
g
r
e
n
E
d
e
z
i
l
a
m
r
o
N
  1.0
  0.8
  0.6
  0.4
  0.2
  0.0
AVATAR−1
AVATAR−120
AVATAR−360
No Refresh
32GB
(8Gb Devices)  (16Gb Devices)  (32Gb Devices)  (64Gb Devices)
64GB
128GB
256GB
Fig. 17.
increases with technology node.
Energy Delay Product. The savings in Energy Delay Product
Overall, our analysis shows that AVATAR is not only
effective at improving reliability by orders of magnitude, but
it also obtains most of the performance and energy beneﬁts of
an otherwise-unreliable multirate refresh scheme.
VIII. RELATED WORK
To the best of our knowledge, this is the ﬁrst work to
comprehensively study and model the effect of VRT cells on
multirate refresh mechanisms. We provide a new analytical
model showing that relying only on ECC to correct VRT
failures can result in an unacceptable rate of data loss. Our
VRT-aware multirate refresh mechanism can guarantee reliable
operations in the presence of VRT failures. In this section, we
discuss prior works that have proposed different mechanisms
to mitigate the negative effects of DRAM refresh operations.
We also discuss prior works on proﬁling VRT failures.
436436
A. Lowering Refresh Rate
Prior works on minimizing refresh overhead by extending
the refresh interval can be categorized into three classes:
Proﬁling Based: Proﬁling based-multirate refresh works ex-
ploit the non-uniformity in retention time of DRAM cells to
mitigate the problem of DRAM refresh (e.g., [4, 21, 28, 36,
38, 41, 44]). These multirate refresh schemes group rows into
different bins based on an initial retention time proﬁling and
apply a higher refresh rate only for rows belonging to the
lower retention time bin. They depend on a simple proﬁling
mechanism at the initial stage to detect the rows with retention
failures and place them into the lower retention time bin.
Their key assumption is that the retention time proﬁle of
DRAM cells does not change at runtime. However, all these
mechanisms will result in unacceptable data loss because of
the VRT failures [29]. These mechanisms can potentially use
ECC to mitigate the VRT failures. However, we show that
simply relying on SECDED ECC cannot provide an acceptable
reliability guarantee. To mitigate the VRT failures with ECC,
we may need stronger ECC codes, which signiﬁcantly increase
system cost.
ECC Based: Prior work proposed to minimize the refresh
overhead by extending the refresh interval and using higher
strength ECC (5EC6ED) to correct the retention failures [42].
However, to reduce the cost of ECC, this work proposes to
amortize the ECC cost by protecting larger chunks of data
(1KB). Thus, this mechanism has signiﬁcant bandwidth and
performance overheads as it reads the entire 1KB chunk of
data at every access to verify/update ECC.
Software Hint Based: Software-hint based refresh mecha-
nisms rely on software/OS hints on the criticality or error-
vulnerability of program data. They lower the refresh rate or
reliability of DRAM for non-critical or invalid regions [11,30,
31]. These mechanisms cannot fully exploit the non-uniformity
of the retention time across the chip as only a restricted fraction
of memory can beneﬁt from reduced refreshes.
B. Refresh Scheduling
Prior works proposed to reduce performance overhead of
refreshes by scheduling refresh operations in a ﬂexible way
that reduces their interference with program accesses [6, 12,
35, 40]. Our work is complementary to these works as these
mechanisms propose to minimize refresh overhead at
the
nominal refresh rate. All these techniques are applicable to
our mechanism that reduces refresh overhead by extending the
refresh interval for most memory rows.
C. Proﬁling for VRT
Although the VRT phenomenon has been widely studied
in the literature [7, 10, 13, 20, 32, 33, 37, 43, 45], only recent
works discuss issues in retention time proﬁling in the presence
of VRT cells [19, 29]. Khan et al. studied the effectiveness of
multi-round testing, guard-banding and different-strength ECC
codes at tolerating VRT failures [19]. Their work does not pro-
pose any analytical models or mechanisms to enable realistic
multirate refresh in the presence of VRT. Another prior work
uses proﬁling to detect retention failures whenever the module
enters the self-refresh mode [3]. This work cannot guarantee
data integrity as VRT failures can occur after testing [19, 29].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:13:50 UTC from IEEE Xplore.  Restrictions apply. 
IX. CONCLUSIONS
Multirate refresh schemes exploit the non-uniformity in
retention times of DRAM cells to reduce refresh operations.
While multirate refresh schemes are highly effective at reduc-
ing refresh operations, they are plagued by the fact that the
retention time of a DRAM cell tends to vary at runtime due to
the Variable Retention Time (VRT) phenomenon. VRT causes
multirate refresh to suffer from data errors, even with the use of
ECC DIMMs, which makes it a challenge to deploy multirate
refresh mechanisms reliably in practice. This paper introduces
AVATAR, the ﬁrst practical, effective, and reliable multirate
refresh scheme. This paper makes the following contributions:
1) We characterize the behavior of VRT cells and develop
an architecture-level model to analyze multirate refresh in the
presence of VRT.
2) Using our model, we show that a VRT-agnostic approach
that relies on the use of ECC DIMMs to correct VRT failures
leads to an unacceptable rate of a data loss.
3) We propose AVATAR, a VRT-aware multirate refresh mech-
anism that adaptively changes the refresh rate to handle VRT
failures at runtime. It improves reliability of multirate refresh
to tens of years while maintaining most of the refresh savings
and performance and energy beneﬁts of multirate refresh.
We show that AVATAR reduces refresh operations by 62%-
72% for a DRAM system without incurring any additional
hardware changes than what is required for multirate refresh
and ECC modules. This refresh reduction leads to an approxi-
mately 35% performance improvement and 55% energy-delay
product reduction with 64Gb DRAM chips. We conclude that
AVATAR is a highly-effective and simple multirate refresh
mechanism that provides correct DRAM operation even in the
presence of VRT failures.
ACKNOWLEDGMENTS
Chris Wilkerson contributed substantially to this work. We
thank him for his insightful feedback and discussions that
have helped shape this work signiﬁcantly. We also thank the
anonymous reviewers for providing useful feedback on our
work. This work was supported in part by NSF grants 1319587,
1212962, 0953246, 1320531, 1065112, the Intel Science and
Technology Center on Cloud Computing, and the Center for
Future Architecture Research (C-FAR), one of the six SRC
STARnet Centers, sponsored by MARCO and DARPA.
REFERENCES
[1]
[2]
[3]
“Soft-errors in electronic memory - A white paper,” Tezzaron Semicon-
ductor, 2004.
“Memory scheduling championship (MSC),” 2012.
J.-H. Ahn et al., “Adaptive self refresh scheme for battery operated
high-density mobile DRAM applications,” in ASSCC, 2006.
[4] S. Baek et al., “Refresh now and then,” IEEE TC, 2013.
[5] L. Borucki et al., “Comparison of accelerated DRAM soft error rates
measured at component and system level,” in IRPS, 2008.
[6] K. K.-W. Chang et al., “Improving DRAM performance by parallelizing
refreshes with accesses,” in HPCA, 2014.
[7] M. Chang et al., “Si-H bond breaking induced retention degradation
during packaging process of 256 Mbit DRAMs with negative wordline
bias,” IEEE TED, 2005.
[8] N. Chatterjee et al., “USIMM: the Utah SImulated Memory Module,”
Tech. Rep., 2012.
[9] N. El-Sayed et al., “Temperature management in data centers: Why
some (might) like it hot,” in SIGMETRICS, 2012.
[10] T. Hamamoto et al., “On the retention time distribution of dynamic
random access memory (DRAM),” IEEE TED, 1998.
[11] C. Isen and L. John, “ESKIMO - Energy savings using semantic knowl-
edge of inconsequential memory occupancy for DRAM subsystem,” in
ISCA, 2009.
[12] Y. Ishii et al., “High performance memory access scheduling using
compute-phase prediction and writeback-refresh overlap,” in JILP Work-
shop on Memory Scheduling Championship, 2012.
[13] Y. Ito and H. Iwai, “Data storing method of dynamic ram and semi-
conductor memory device,” US Patent No. 6697992 B2, 2004.
[14] B. Jacob et al., Memory Systems: Cache, DRAM, Disk. Morgan
Kaufmann Publishers Inc., 2007.
JEDEC, DDR3 SDRAM Speciﬁcation, 2012.
[15]
[16] ——, High Bandwidth Memory (HBM) DRAM Speciﬁcation, 2013.
[17] ——, WIDE-IO DRAM Speciﬁcation, 2013.
[18] U. Kang et al., “Co-architecting controllers and DRAM to enhance
DRAM process scaling,” in The Memory Forum, ISCA, 2014.
[19] S. Khan et al., “The efﬁcacy of error mitigation techniques for DRAM
retention failures: A comparative experimental study,” in SIGMETRICS,
2014.
[20] H. Kim et al., “Characterization of the variable retention time in
[21]
dynamic random access memory,” IEEE TED, 2011.
J. Kim and M. Papaefthymiou, “Block-based multi-period refresh for
energy efﬁcient dynamic memory,” in ASIC, 2001.
[22] K. Kim and J. Lee, “A new investigation of data retention time in truly
nanoscaled DRAMs,” IEEE EDL, 2009.
[23] Y. Kim et al., “A case for exploiting subarray-level parallelism (SALP)
in DRAM,” in ISCA, 2012.
[24] ——, “Flipping bits in memory without accessing them: An experimen-
tal study of DRAM disturbance errors,” in ISCA, 2014.
[25] D. Lee et al., “Adaptive-latency DRAM: Optimizing DRAM timing for
the common-case.”
[26] ——, “Tiered-latency DRAM: A low latency and low cost DRAM
architecture,” in HPCA, 2013.
[27] X. Li et al., “A memory soft error measurement on production systems,”
[28]
in USENIX ATC, 2007.
J. Liu et al., “RAIDR: Retention-Aware Intelligent DRAM Refresh,” in
ISCA, 2012.
[29] ——, “An experimental study of data retention behavior in modern
DRAM devices: Implications for retention time proﬁling mechanisms,”
in ISCA, 2013.
[30] S. Liu et al., “Flikker: Saving DRAM refresh-power through critical
data partitioning,” in ASPLOS, 2011.
[31] Y. Luo et al., “Characterizing application memory error vulnerability
to optimize datacenter cost via heterogeneous-reliability memory,” in
DSN, 2014.
[32] R. L. Meyer and R. Beffa, “Method of reducing variable retention
characteristics in DRAM cells,” US Patent No. 6898138 B2, 2005.
[33] Y. Mori et al., “The origin of variable retention time in DRAM,” in
IEDM, 2005.
[34] O. Mutlu et al., “The main memory system: Challenges and opportu-
nities,” KIISE, 2015.
[35] P. Nair et al., “A case for refresh pausing in DRAM memory systems,”
in HPCA, 2013.
[36] P. J. Nair et al., “ArchShield: Architectural framework for assisting
DRAM scaling by tolerating high error rates,” in ISCA, 2013.
[37] P. J. Restle et al., “DRAM variable retention time,” in IEDM, 1992.
[38] D.-Y. Shen et al., “SECRET: Selective error correction for refresh
energy reduction in DRAMs,” in ICCD, 2012.
[39] C. Shirley and W. Daasch, “Copula models of correlation: A DRAM
[40]
case study,” IEEE TC, 2013.
J. Stuecheli et al., “Elastic refresh: Techniques to mitigate refresh
penalties in high density memory,” in ISCA, 2010.
[41] R. K. Venkatesan et al., “Retention-aware placement
in DRAM
(RAPID): Software methods for quasi-non-volatile DRAM,” in HPCA,
2006.
[42] C. Wilkerson et al., “Reducing cache power with low-cost, multi-bit
error-correcting codes,” in ISCA, 2010.
[43] R. Yamada et al., “Dynamic random access memories and method for
testing performance of the same,” US Patent No. 7450458 B2, 2008.
[44] K. Yanagisawa, “Semiconductor memory,” US Patent No. 4736344 A,
1988.
[45] D. Yaney, et al., “A meta-stable leakage phenomenon in DRAM charge
storage-variable hold time,” in IEDM, 1987.
437437
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:13:50 UTC from IEEE Xplore.  Restrictions apply.