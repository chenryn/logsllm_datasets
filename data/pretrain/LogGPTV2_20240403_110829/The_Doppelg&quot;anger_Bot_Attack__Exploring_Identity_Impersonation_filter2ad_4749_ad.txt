We build two AMT experiments to answer these ques-
tions. For the ﬁrst AMT experiment, we select 50 doppel-
g¨anger bot accounts and 50 avatar accounts from the victim-
impersonator pairs and avatar-avatar pairs. In each assign-
ment, we give AMT workers a link to a Twitter account and
we ask them to choose between three options: ‘the account
looks legitimate’, ‘the account looks fake’ and ‘cannot say’.
We mix doppelg¨anger bot accounts and avatar accounts in
the experiments to force users to examine each case afresh.
In all experiments we ask the opinion of three AMT workers
and we report the results for majority agreement. In this
experiment, AMT workers are able to only detect 18% of
the doppelg¨anger bots as being fake (9 out of 50). Thus,
most AMT workers get tricked by doppelg¨anger bots.
In the second experiment we show AMT workers two ac-
counts that portray the same person. We picked the same 50
impersonating accounts (and their corresponding victims)
and the same 50 avatar accounts (and their correspond-
ing avatar doppelg¨anger ) as in the previous experiment.8
In each assignment, we give AMT workers two links corre-
sponding to the two Twitter accounts and we ask them to
choose between ﬁve options: ‘both accounts are legitimate’,
‘both accounts are fake’, ’account 1 impersonates account 2’,
’account 2 impersonates account 1’, and ’cannot say’. In the
second experiment, AMT workers were able to correctly de-
tect 36% doppelg¨anger bots as fake. The experiment shows
that there is a 100% improvement in their detection rate
when they have a point of reference.
The results in this section have implications on both how
to design automatic techniques to detect doppelg¨anger bots
as well as how to design systems that better protect users
from being deceived online by impersonation attacks.
4. DETECTING IMPERSONATION ATTACKS
The previous section showed that given a victim-impersonator
pair of accounts, we can fairly accurately detect the imper-
sonating account by comparing their account creation times
and reputations. In this section, we investigate the extent
to which we can detect whether a pair of accounts that por-
trays the same person (a doppelg¨anger pair) is a victim-
impersonator pair or an avatar-avatar pair. We start by
analyzing features that can signal the existence of an imper-
sonation attack and we then propose a method to automat-
ically detect such attacks.
8The AMT workers were diﬀerent in the two experiments.
148(a) User-name
(b) Screen-name
(a) Number of common fol-
lowings
(b) Number of common fol-
lowers
(c) Location
(d) Photo
(c) Number of common men-
tioned users
(d) Number
retweeted users
of
common
Figure 4: CDFs of the social neighborhood overlap be-
tween accounts in victim-impersonator pairs and avatar-
avatar pairs.
(e) Bio
(f) Interests similarity
Figure 3: CDFs of the similarity between accounts in victim-
impersonator pairs and avatar-avatar pairs.
Twitter suspension signals and direct account interactions
are very good signals to create a dataset to study imperson-
ation attacks, however, there are still many doppelg¨anger pairs
that are not yet labeled (e.g., there are 16,486 unlabeled
pairs in the Random Dataset). Thus, a secondary goal
of this section is to investigate whether we can detect addi-
tional victim-impersonator pairs and avatar-avatar pairs in
the doppelg¨anger pairs in our dataset.
4.1 Features to detect impersonation attacks
To detect impersonation attacks we consider features that
characterize pairs of accounts and that can potentially diﬀer-
entiate victim-impersonator pairs from avatar-avatar pairs.
We consider all victim-impersonator pairs and avatar-avatar
pairs from the Random Dataset and BFS Dataset com-
bined (we call the combined dataset the Combined Dataset)
to analyze how well the features distinguish between the two
kinds of pairs of accounts.
Proﬁle similarity between accounts.
We ﬁrst analyze the similarity between proﬁle attributes
such as user-names, screen-names, locations, proﬁle photos
and bios (refer to the Appendix for details on how we com-
pute the similarity scores for diﬀerent attributes). Even if
these features were used to collect the dataset of doppel-
g¨anger pairs we can still use them to distinguish between
avatar-avatar pairs and victim-impersonator pairs. In addi-
tion, we measure the similarity between the interests of two
accounts. We use the algorithm proposed by Bhattacharya
et al. [4] to infer the interests of a user.
Figure 3 compares the CDFs of the pairwise proﬁle sim-
ilarity between accounts in avatar-avatar pairs and victim-
impersonator pairs. For user-names, screen-names, photo
and interests similarity, a value of zero means no similar-
ity while one means perfect similarity. For the location,
the similarity is the distance in kilometers between the two
locations, thus a value of zero means the locations are the
same. For bio, the similarity is the number of common words
between two proﬁles, the higher the similarity the more con-
sistent the bios are.
We observe that the similarity between user-names, screen-
names, proﬁle photos and bios is higher for victim-impersonator
pairs than avatar-avatar pairs. Thus, users that maintain
multiple avatar accounts do not spend the eﬀort to make
their accounts look similar, while impersonators do. On the
other hand, the similarity between the interests of avatar-
avatar pairs is higher than the victim-impersonator pairs.
We did not expect such high similarity between avatar-avatar
pairs because we believed that people maintain distinct ac-
counts to promote diﬀerent sides of their persona.
Social neighborhood overlap.
We call the set of users an account interacts with in a social
network the social neighborhood of the account. On Twit-
ter, the social neighborhood of an account a consists of the
followings and followers of a as well as the users mentioned
by a and the users retweeted by a. An overlap in the social
neighborhood suggests that two accounts are positioned in
the same part of the social network graph. This can be in-
00.5100.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar00.20.40.60.800.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar05000100001500000.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar00.5100.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar0102000.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar00.20.40.60.800.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar10010210400.51Number of common followingsCDF  Victim−ImpersonatorAvatar−Avatar10010210400.51Number of common followersCDF  Victim−ImpersonatorAvatar−Avatar010020000.51Number of common mentioned usersCDF  Victim−ImpersonatorAvatar−Avatar010203000.51Number of common retweeted usersCDF  Victim−ImpersonatorAvatar−Avatar1494.2 Automated detection method
To build an automated method to detect impersonation at-
tacks we build a SVM classiﬁer, with linear kernel, that
distinguishes victim-impersonator pairs from avatar-avatar
pairs. We use, from the Combined Dataset, the victim-
impersonator pairs as positive examples, and avatar-avatar
pairs as negative examples to train the classiﬁer. We use
all the features presented in §4.1 as well as all the features
that characterize individual accounts presented in §2.4 for
the training. Since the features are from diﬀerent categories
and scales (e.g., time in days and distances in kilometers),
we normalize all features values to the interval [-1,1].
We use 10-fold cross validation over the Combined Dataset
to train and test the classiﬁer. The SVM classiﬁer, for each
pair of accounts, outputs a probability of the pair to be
a victim-impersonator pair. To perform the detection of
victim-impersonator pairs and avatar-avatar pairs, we then
proceed as follows. If the probability is higher than a cer-
tain threshold th1 we conclude that the pair is a victim-
impersonator pair and if the probability is lower than a cer-
tain threshold th2 (diﬀerent than th1) the pair is a avatar-
avatar pair. Note that if th1 > th2, some pairs may remain
unlabeled. This is done on purpose here because it is prefer-
able in our problem to leave a pair unlabeled rather than
wrongly label it (i.e., label avatar-avatar pairs as victim-
impersonator pairs or vice versa). We therefore select thresh-
olds th1 and th2 such that there are very few false positives
(i.e., few victim-impersonator pairs mislabeled as avatar-
avatar pairs or vice versa). The resulting classiﬁer is able to
achieve a 90% true positive rate for a 1% false positive rate
to detect victim-impersonator pairs and a 81% true posi-
tive rate for a 1% false positive rate to detect avatar-avatar
pairs.9 Thus, we can detect a signiﬁcant fraction of victim-
impersonator pairs using only features that compare the
reputation and activity of accounts in a doppelg¨anger pair.
Therefore, it is possible to detect impersonation attacks au-
tomatically rather than wait for victims to report them or
wait for the accounts to do something clearly malicious in
order to be suspended by Twitter.
Potential limitations: Our detection method above, while
quite eﬀective today at detecting whether a doppelg¨anger pair
is the result of an impersonation attack, is not necessarily
robust against adaptive attackers that might change their
strategy to avoid detection in the future. Similar to spam
detection, system operators to constantly retrain the de-
tectors (classiﬁers) to account for new attacker strategies.
Also note that the accuracy percentages above only refer to
the accuracy of detecting whether a doppelg¨anger pair is a
victim-impersonator pair and does not include the accuracy
of detecting a doppelg¨anger pair or the accuracy of detecting
the doppelg¨anger bot account within a victim-impersonator
pair of accounts.
4.3 Classifying unlabeled doppelgänger pairs
We apply the classiﬁer over the 17,605 unlabeled pairs from
the BFS Dataset and the 16,486 unlabeled pairs from the
Random Dataset. With a threshold corresponding to 1%
false positive rate (for both detecting victim-impersonator
pairs and avatar-avatar pairs) the classiﬁer is able to iden-
tify 4,390 avatar-avatar pairs and 1,863 victim-impersonator
9Since there is little to no class imbalance in this classiﬁca-
tion problem, contrary to §3.3, a 1% false positive rate is
low enough.
(a) Time diﬀerence between
creation dates
(b) Time diﬀerence between
the last tweet
Figure 5: CDFs of the time diﬀerence in days between ac-
counts in victim-impersonator pairs and avatar-avatar pairs.
dicative of two things: (1) the two accounts correspond to
avatars managed by the same user; or (2) potential evidence
of social engineering attacks. We use four features to mea-
sure the social neighborhood overlap: the number of com-
mon followings, the number of common followers, the num-
ber of overlapping users mentioned and the number of over-
lapping users retweeted by both accounts, which we present
in Figure 4.
There is a striking diﬀerence between avatar-avatar pairs
and victim-impersonator pairs: while victim-impersonator
pairs almost never have a social neighborhood overlap, avatar
accounts are very likely to have an overlap. Social neighbor-
hood overlap is also indicative of social engineering attacks
but there are not many such attacks in our dataset to be
visible in the plots.
Time overlap between accounts.
We add features related to the time overlap between two
accounts: time diﬀerence between the creation dates, time
diﬀerence between the last tweets, time diﬀerence between the
ﬁrst tweets and whether one account stopped being active
after the creation of the second account (we call this feature
outdated account). Figure 5 compares the diﬀerence between
creation dates and the date of the last tweet. Figure 5a
shows that there is a big diﬀerence between account creation
times for victim-impersonator pairs while for avatar-avatar
pairs the diﬀerence is smaller.
Differences between accounts.
Finally, we consider a set of features that represent the dif-
ference between diﬀerent numeric features that characterize
individual accounts: klout score diﬀerence, number of follow-
ers diﬀerence, number of friends diﬀerence, number of tweets
diﬀerence, number of retweets diﬀerence, number of favor-
ited tweets diﬀerence, number of public list diﬀerence. Our
intuition was that the diﬀerence between numeric features
of accounts in avatar-avatar pairs will be smaller than for
victim-impersonator pairs, e.g., a small klout score diﬀerence
between two accounts could be indicative of avatars of the
same person while a large klout score diﬀerence could be in-
dicative of an impersonation attack. To our surprise, the dif-
ference is generally slightly smaller for victim-impersonator
pairs.
Overall, the best features to distinguish between victim-
impersonator pairs and avatar-avatar pairs are the interest
similarity, the social neighborhood overlap as well as the
diﬀerence between the creation dates of the two accounts.
01000200000.51DifferenceCDF  Victim−ImpersonatorAvatar−Avatar01000200000.51DifferenceCDF  Victim−ImpersonatorAvatar−Avatar150Table 2: Unlabeled doppelg¨anger pairs in our dataset that
we can labeled using the classiﬁer.
BFS Dataset
(17,605 unlabeled)
Random Dataset
(16,486 unlabeled)
victim-impersonator pairs
avatar-avatar pairs
9,031
4,964
1,863
4,390
pairs in the Random Dataset (see Table 2). Thus, the clas-
siﬁer can identify a large additional number of avatar-avatar
pairs and victim-impersonator pairs that were not caught in
the initial dataset. For example, on top of the 166 exam-
ples of victim-impersonator pairs we initially labeled, the
classiﬁer labels 1,863 additional victim-impersonator pairs.
We re-crawled all doppelg¨anger pairs (from both datasets)
in May 2015 (the initial crawl ended in Dec 2014), and 5,857
out of the 10,894 victim-impersonator pairs detected by our
classiﬁer were suspended by Twitter. This result shows the
eﬀectiveness of our method at detecting victim-impersonator
pairs sooner than Twitter.
5. RELATED WORK
The closest to our work are a few studies on social engineer-
ing attacks which we will review in more detail. Also related
are studies of matching accounts across social networks and
sybil account detection techniques that we review at a more
higher level.
Social engineering attacks.
We focus in this paper on a broader set of attacks that im-
personate people, of which, social engineering attack are a
subclass. Bilge et al.
[5] demonstrated the feasibility of
automatically creating cloned proﬁles in social networks,
however, they did not propose techniques to detect the at-
tacks. The closest to our work are three studies [17, 13,
15] that made some initial investigations toward detecting
proﬁle cloning. The studies hinted at the fact that cloned
proﬁles can be identiﬁed by searching for proﬁles with sim-
ilar visual features, but they either stopped at returning a
ranked list of accounts that are similar with the victim ac-
count [17, 13], or to just test their technique on simulated
datasets [15]. In contrast, we actually detect accounts that
portray the same person in real-world social networks with
high accuracy and we also detect whether they are involved
in an impersonation attack or they are legitimate. Further-
more, we propose a technique to gather data about imper-
sonation attacks in real-world social networks and we do the
ﬁrst, to our knowledge, characterization of impersonation at-
tacks in Twitter. On the protection part, He et al. [11] pro-
posed ways to protect against friend requests coming from
cloned proﬁles by using adjacent mediums such as instant
chats to verify if the request comes from the real person.