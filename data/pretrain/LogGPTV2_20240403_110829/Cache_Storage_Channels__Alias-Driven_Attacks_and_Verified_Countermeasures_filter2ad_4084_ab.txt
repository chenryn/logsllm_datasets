addresses vac and vanc respectively, then it invokes the victim.
After the victim returns, the attacker reads back from the
address vanc.
Let idx be the line index corresponding to the address pa.
Since vac is cacheable, the instruction in A1 stores the value
1 in the cache line indexed by idx, the line is ﬂagged as
dirty and its tag is set to pa. When the instruction in A2 is
executed, since vanc is non-cacheable, the system ignores the
4040
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:33 UTC from IEEE Xplore.  Restrictions apply. 
V1) D = access(VA_c)
A1) write(VA_nc, 1)
V2) D = access(VA_c)
V3) if not policy(D)
reject
[evict VA_c]
V4) use(VA_c)
Fig. 3.
Integrity threat due to data-cache
A1) jmp A8
A2) write(&A8, {R0=1})
A3) call victim
A4) jmp A8
A5) D = R0
...
A8) R0=0
A9) return
V1) if secret
jmp f1
else
jmp f2
Fig. 4. Conﬁdentiality threat due to instruction-cache
“unexpected cache hit” and the value 0 is directly written into
the memory, bypassing the cache. Now, the value stored in
main memory after the execution of the victim depends on
the behaviour of the victim itself; if the victim accesses at
least one address whose line index is idx, then the dirty line
is evicted and the value 1 is written back to the memory;
otherwise the line is not evicted and the physical memory still
contains the value 0 in pa. Since the address is non-cacheable,
the value that is read from vanc in A4 depends on the victim’s
behaviour.
This mechanism enables the attacker to probe if the line
index idx is evicted by the the victim. If the attacker has
available a pair of aliases (cacheable and non-cacheable) for
every cache line index, the attacker is able to measure the list
of cache lines that are accessed by the victim, thus it can mount
an access-driven cache attack. The programs V1 and V2 in
Figure 1 exemplify two victims of such attack; in both cases
the lines evicted by the programs depend on a conﬁdential
variable secret and the access-driven cache attack can extract
some bits of the secret variable.
Note that we assumed that the data cache (i) is “write-
back”, (ii) has “inertia” and (iii) uses “lazy write”. That is,
(i) writing is done in the cache and the write access to the
memory is postponed, (ii) the cache evicts a line only when
the corresponding space is needed to store new content, and
(iii) a dirty cache line is written back only when it is evicted.
This is not necessarily true; the cache can be write-through
or it can (speculatively) write back and clean dirty lines when
the memory bus is unused. Figure 2 presents an alternative
attack whose success does not depend on this assumption.
The attacker (A1-A3) stores the value 0 in the cache, by
invalidating the corresponding line, writing 0 into the memory
and reading back the value using the cacheable virtual address.
Notice that after step A3 the cache line is clean, since the
attacker used the non-cacheable virtual alias to write the value
0. Then, the attacker writes 1 into the memory, bypassing
the cache. The value read in A6 using the cacheable address
depends on the behaviour of the victim; if the victim accesses
at least one address whose line index is idx, then the cache
line for pa is evicted and the instruction in A6 fetches the
value from the memory, yielding the value 1; otherwise the
line is not evicted and the cache still contains the value 0 for
pa.
B. Attacking Integrity Using Data-Caches
Mismatched cacheability attributes may also produce in-
tegrity threats, by enabling an attacker to modify critical data
in an unauthorized or undetected manner. Figure 3 demon-
strates an integrity attack. Again, we assume that the data-
cache is direct-mapped, that it is physically indexed and that
its write policy is write allocate/write back. For simplicity, we
limit the discussion to the L1 caches. In our example, vac
and vanc are virtual addresses pointing to the same memory
location pa; vac is the cacheable alias while vanc is non-
cacheable. Initially, the memory location pa contains the value
0 and the corresponding cache line is either invalid or the line
has valid data but it is clean. In a sequential model where reads
and writes are guaranteed to take place in program order and
their effects are instantly visible to all system components, the
program of Figure 3 has the following effects: V1) a victim
accesses address vac, reading 0; A1) the attacker writes 1 into
pa using the virtual alias vanc; V2) the victim accesses again
vac, this time reading 1; V3) if 1 does not respect a security
policy, then the victim rejects it; otherwise V4) the victim uses
1 as the input for a security-relevant functionality.
On a real processor with a relaxed memory model the same
system can behave differently, in particular: V1) using vac, the
victim reads initial value 0 from the memory at the location pa
and ﬁlls the corresponding line in the cache; A1) the attacker
use vanc to write 1 directly into the memory, bypassing the
cache; V2) the victim accesses again vac, reading 0 from the
cache; V3) the policy is evaluated based on 0; possibly, the
cache line is evicted and, since it is not dirty, the memory is
not affected; V4) the next time that the victim accesses pa it
will read 1 and will use this value as input of the functionality,
but 1 has not been checked against the policy. This enables
an attacker to bypass a reference monitor, here represented by
the check of the security policy, and to inject unchecked input
as parameter of security critical functions.
C. Attacking Conﬁdentiality Using Instruction Caches
Similar to data caches, instruction caches can be used to
mount access-driven cache attacks; in this case the attacker
probes the instruction cache to extract information about the
victim execution path.
Our attack vector uses self-modifying code. The program in
Figure 4 demonstrates the principles of the attack. We assume
that the instruction cache is physically indexed and that it has
only one way. We also assume that the attacker’s executable
address space is cacheable and that the processor uses separate
instruction and data caches.
4141
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:33 UTC from IEEE Xplore.  Restrictions apply. 
Initially, the attacker’s program contains a function at the
address A8 that writes 0 into the register R0 and immediately
returns. The attacker starts in A1, by invoking the function at
A8. Let idx be the line index corresponding to the address
of A8: Since the executable address space is cacheable, the
execution of the function has the side effect of temporarily
storing the instructions of the function into the instruction
cache. Then (A2), the attacker modiﬁes its own code, over-
writing the instruction at A8 with an instruction that updates
register R0 with the value 1. Since the processor uses separate
instruction and data caches the new instruction is not written
into the instruction cache. After that the victim completes
the execution of its own code, the attacker (A4) re-executes
the function at A8. The instruction executed by the second
invocation of the function depends on the behaviour of the
victim: if the execution path of the victim contains at least
one address whose line index is idx then the attacker code is
evicted, the second execution of the function fetches the new
instruction from the memory and the register is updated with
the value 1; otherwise, the attacker code is not evicted and the
second execution of the function uses the old code updating
the register with 0.
In practice, the attacker can probe if the line index idx is
evicted by the victim. By repeating the probing phase for every
cache line, the attacker can mount access-driven instruction
cache attacks. The program V1 in Figure 4 exempliﬁes a
victim of such attack, where the control ﬂow of the victim
(and thus the lines evicted by the program) depends on a
conﬁdential variable secret.
D. Scenarios
In this section we investigate practical applications and
limits of the attack vectors. To simplify the presentation,
we assumed one-way physically indexed caches. However,
all attacks above can be straightforwardly applied to virtually
indexed caches. Also, the examples can be extended to support
multi-way caches if the way-allocation strategy of the cache
does not depend on the addresses that are accessed:
the
attacker repeats the cache ﬁlling phase using several addresses
that are all mapped to the same line index. The attack
presented in Section III-C also assumes that the processor
uses separate instruction and data caches. This is the case in
most modern processors, since they usually use the “modiﬁed
Harvard architecture”. Modern x64 processors, however, im-
plement a snooping mechanism that invalidates corresponding
instruction cache lines automatically in case of self-modifying
code ([26], Vol. 3, Sect. 11.6); in such a scenario the attack
cannot succeed.
The critical assumptions of the attacks are the ability of
building virtual aliases with mismatched cacheability attributes
(for the attacks in Sections III-A and III-B) and the ability of
self-modifying code (for the attack in Section III-C). These
assumptions can be easily met if the attacker is a (possibly
compromised) operating system and the victim is a colocated
guest in a virtualized environment. In this case, the attacker is
usually free to create several virtual aliases and to self-modify
its own code. A similar scenario consists of systems that use
specialised hardware to isolate security critical components
(like SGX and TrustZone), where a malicious operating system
shares the caches with trusted components. Notice also that in
case of TrustZone and hardware assisted virtualization, the
security software (e.g. the hypervisor) is not informed about
the creation of setups that enable the attack vectors, since it
usually does not interfere with the manipulation of the guest
page tables.
In some cases it is possible to enable the attack vectors
even if the attacker is executed in non-privileged mode. Some
operating systems can allow user processes to reconﬁgure
cacheability of their own virtual memory. The main reason of
this functionality is to speed up some specialised computations
that need to avoid polluting the cache with data that is accesses
infrequently [39]. In this case two malicious programs can
collude to build the aliases having mismatched attributes.
Since buffer overﬂows can be used to inject malicious
code, modern operating systems enforce the executable space
protection policy: a memory page can be either writable or
executable, but it can not be both at the same time. However,
to support just in time compilers, the operating systems allow
user processes to change at run-time the permission of virtual
memory pages, allowing to switch a writable page into an
executable and vice versa (e.g. Linux provides the syscall
“mprotect”, which changes protection for a memory page of
the calling process). Thus, the attack of Section III-C can still
succeed if: (i) initially the page containing the function A8 is
executable, (ii) the malicious process requests the operating
system to switch the page as writable (i.e. between step A1
and A2) and (iii) the process requests the operating system
to change back the page as executable before re-executing
the function (i.e. between step A2 and A4). If the operating
system does not invalidate the instruction cache whenever the
permissions of memory pages are changed, the conﬁdentiality
threat can easily be exploited by a malicious process.
In Sections II and VII we provide a summary of existing
literature on side channel attacks that use caches. In general,
every attack (e.g. [2], [55], [34]) that is access-driven and
that has been implemented by probing access times can be
reimplemented using the new vectors. However, we stress that
the new vectors have two distinguishing characteristics with
respect to the time based ones: (i) the probing phase does
not need the support of an external measurement, (ii) the
vectors build a cache-based storage channel that has relatively
low noise compared channels based on execution time which
depend on many other factors than cache misses, e.g., TLB
misses and branch mispredictions.
In fact, probing the cache state by measuring execution time
requires the attacker to access the system time. If this resource
is not directly accessible in the execution level of the attacker,
the attacker needs to invoke a privileged function that can
introduce delays and noise in the cache state (e.g. by causing
the eviction from the data cache when accessing internal data-
structures). For this reason, the authors of [55] disabled the
timing virtualization of XEN (thus giving the attacker direct
4242
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:33 UTC from IEEE Xplore.  Restrictions apply. 
access to the system timer) to demonstrate a side channel
attack. Finally, one of the storage channels presented here
poses integrity threats clearly outside the scope of timing based
attacks.
pairs (cl, el). Here, cl is the resulting cipher-text and el is the
set of cache lines accessed by the AES implementation. We
deﬁne Lj,v to be the subset of L such that the byte j of the
cipher-text is v:
IV. CASE STUDIES
To substantiate the importance of the new attack vectors, and
the need to augment the veriﬁcation methodology to properly
take caches and cache attributes into account, we examine
the attack vectors in practice. Three cases are presented: A
malicious OS that extracts a secret AES key from a cryptoser-
vice hosted in TrustZone, a malicious paravirtualized OS that
subverts the memory protection of a hypervisor, and a user
process that extracts the exponent of a modular exponentiation
procedure executed by another process.
A. Extraction of AES keys
AES [17] is a widely used symmetric encryption scheme
that uses a succession of rounds, where four operations
(SubBytes, ShiftRows, MixColumn and AddRoundKey) are
iteratively applied to temporary results. For every round i, the
algorithm derives the sub key Ki from an initial key k. For
AES-128 it is possible to derive k from any sub key Ki.
Traditionally, efﬁcient AES software takes advantage of
precomputed SBox tables to reach a high performance and
compensate the lack of native support to low-level ﬁnite ﬁeld
operations. The fact
that disclosing access patterns to the
SBoxes can make AES software insecure is well known in
literature (e.g. [51], [47], [2]). The existing implementations
of these attacks probe the data cache using time channels, here
we demonstrate that such attacks can be replicated using the
storage channel described in Section III-A. With this aim, we
implement the attack described in [34].
The attack exploits a common implementation pattern. The
last round of AES is slightly different from the others since the
MixColumn operation is skipped. For this reason, implemen-
tations often use four SBox tables T0, T1, T2, T3 of 1KB for
all the rounds except the last one, whereas a dedicated T4 is
used. Let c be the resulting cipher-text, n be the total number
of rounds and xi be the intermediate output of the round i.
The last AES round computes the cipher-text as follows:
c = Kn ⊕ ShiftRows(SubBytes(xn−1))
Instead of computing ShiftRows(SubBytes(xn−1)), the imple-
mentation accesses the precomputed table T4 according to an
index that depends on xn−1. Let b[j] denote the j-th byte of
b and [T4 output] be one of the actual accesses to T4, then
c[j] = Kn[j] ⊕ [T4 output] .
Therefore, it is straightforward to compute Kn knowing the
cipher-text and the entry yielded by the access to T4:
Kn[j] = c[j] ⊕ [T4 output]
Thus the challenge is to identify the exact [T4 output] for a
given byte j. We use the “non-elimination” method described
in [34]. Let L be a log of encryptions, consisting of a set of
4343
Lj,v = {(cl, el) ∈ L such that cl[j] = v}
Since c[j] = Kn[j] ⊕ [T4 output] and the key is constant,
if the j-th byte of two cipher-texts have the same value then
the accesses to T4 for such cipher-text must contain at least
one common entry. Namely, the cache line accessed by the
implementation while computing c[j] = Kn[j] ⊕ [T4 output]
is (together with some false positives) in the non-empty set
Ej,v =
el
(cid:2)
(cl,el)∈Lj,v
4
Let T j,v
be the set of distinct bytes of T4 that can be
allocated in the cache lines Ej,v. Let v, v(cid:3) be two different
values recorded in the log for the byte j. We know that exist
4 ∈ T j,v
ti,v
and
v(cid:3) = Kn[j] ⊕ ti,v(cid:2)
such that v = Kn[j] ⊕ ti,v
4 ∈ T j,v(cid:2)
. Thus
and ti,v(cid:2)
4
4
4
4
v ⊕ v(cid:3) = tj,v
4 ⊕ tj,v(cid:2)
4
4
and T j,v(cid:2)