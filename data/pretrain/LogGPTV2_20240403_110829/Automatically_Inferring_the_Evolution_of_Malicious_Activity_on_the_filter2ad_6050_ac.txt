i arrives in the sequence sz, (i) we predict a label with the
trees Tcurr and Told,z; (ii) we annotate the tree Told,z with
its classiﬁcation error on i, and (iii) we update the model of
Tcurr with i if necessary. At the end of the interval, we com-
pare classiﬁcation errors Told,z−1 with Told,z and Tcurr to
discover the ∆-change preﬁxes, discarding redundant pre-
ﬁxes as appropriate (i.e., when multiple preﬁxes reﬂect the
same change). Told is then updated appropriately (we spec-
ify the details later), so it can used for measuring the clas-
siﬁcation errors of the IPs in the next sequence sz+1. We
sketch the high-level view of ∆-Change in Figure 5.
We describe the construction of Tcurr and Told,z in Sec-
tion 3.2.1, and the comparison between the trees in Sec-
tion 3.2.2. We describe ∆-Change in terms of multiple trees
for conceptual clarity. However, it may be implemented
with just one IPTree (and additional counters) for efﬁciency.
3.2.1 Constructing the IPTrees
At a high-level, TrackIPTree involves all parent preﬁxes of
an IP i in current IPtree in both in classifying IP i, as well
∆-CHANGE Input: sequence sz, Told, Tcurr;
for IP-label pair  in sz
pi,curr := label predicted on i using
TrackIPTree on Tcurr
pi,old := label predicted on i using
TrackIPTree on Told
AnnotateTree(Told,z, i)
Update (Tcurr, i, label)
ExtractChangedPreﬁxes(Told, Tcurr);
sub ANNOTATETREE
Input: IPTree Told, IP i, label l
for each parent preﬁx j of i in Told
IPs[j, label] += 1
if pi,curr 6= pi,old
mistakes[j, label] += 1
sub EXTRACTCHANGEDPREFIXES
Input: IPTree Told,z, IPTree Told,z−1, IPTree
Tcurr, error threshold γ, IP threshold θ
Output: Set of ∆-changes C
//Step 4: Isolate Candidate Preﬁxes
Candidate Set of ∆-changes C = {}
for each preﬁx j ∈ Told,z
error[j] = (mistakes[j, +] +
mistakes[j, −])/(IPs[j, +] + IPs[j, −]);
if error[j] > γ and IPs[j] > θ and
state[j, Told] 6= state[j, Tcurr]
Add preﬁx j to candidate set C
//Step 5: Prune Redundant Preﬁxes
for each preﬁx c ∈ C
for each parent j of c in Told
childMistakes[j] += mistakes[c];
childIPs[j] += IPs[c]
for each preﬁx c ∈ C
if mistakes[c] - childMistakes[c] < θ
|| IPs[c] - childIPs[c] < γ
discard c from C
//Step 6: Discover New Children
for each preﬁx c ∈ C
for each child node c′ of c in Tcurr
if c′ 6∈ Told
add subtree(c) to C
Figure 6. Pseudocode for ∆-Change
x0
x1
x2
x3
x4
+
-
+
-
+
-
+
-
+
-
Weights
-
-
+
-
+
x0
x1
x2
x3
x4
Prediction 
for IP I: +
-
+
-
-
+
(a) Parents of
in
T , with relative importance
weights
IP i
(b) Each parent chooses “+”
or “-”, based on its own label
predictor weights
(c) Weight parent predictions by relative impor-
tance weights to get ﬁnal prediction for the IP
i
(d) Update Tcurr: Penalize
incorrect parents, and grow or
prune T if needed
Figure 7. ∆-Change Algorithm: Illustrating Steps 1 & 3 for a single IP i. (a)-(c) show Step 1, and (d) shows Step 3. The
shaded nodes indicate the parents of the current IP i in the tree.
as in learning from labeled IP i. Crucially, TrackIPTree de-
composes the main prediction problem into 3 subproblems,
which it treats independently: (a) deciding the prediction of
each individual parent preﬁx, (b) combining the parent pre-
ﬁx predictions by deciding their relative importance, and (c)
maintaining the tree structure so that the appropriate sub-
trees are grown and the unwanted subtrees are discarded.
These subproblems are cast as instances of experts’ prob-
lems [11, 20].
TrackIPTree uses the following data structure, shown in
Figure 7(a) and (b). Each node in the IPtree maintains two
sets of weights. One set, termed label predictors, decide
whether an individual node should predict non-malicious or
malicious, denoted {yj,+, yj,−} for node j (Fig. 7(b)). The
second set, termed relative importance predictors, keeps
track of the weight of the preﬁx’s prediction, in relation to
the predictions of other preﬁxes – we use xj to denote this
weight for node j (Fig. 7(a)). In ∆-Change, we augment
the basic IPTree data structure with a few additional coun-
ters, so that we can keep track of the classiﬁcation error
rates at the different parts of the address space on the cur-
rent sequence sz (we elaborate further in Step 2). Below we
describe how we use this data structure in order to apply the
relevant components of TrackIPTree to construct Tcurr and
Told,z and then discover the ∆-change preﬁxes.
Step 1: Predicting with Told and Tcurr. We compute
predictions of Told and Tcurr using the prediction rules of
TrackIPTree. We ﬁrst allow each preﬁx of i in Told (and
likewise, Tcurr) to make an individual prediction (biased
by its label predictors). Then, we combine the predictions
of the individual nodes (biased by its relative importance
predictor). We illustrate these steps in Fig. 7(a)-(c).
Formally, let P denote the set of preﬁxes of i in a tree
T . We compute each preﬁx j ∈ P ’s prediction pi,j, with a
bias of yj,+ to predict non-malicious, and a bias of yj,− to
predict malicious. We then combine all the predictions pi,j
into one prediction pi for the IP i, by choosing prediction
pi,j of node j with probability xj, its relative importance
predictor. The pi is our ﬁnal output prediction.
Step 2: Annotating Told with Classiﬁcation Errors.
We next describe how we annotate the IPTree Told to pro-
duce Told,z based on the errors that Told makes on sz. For
this, we augment the basic IPTree data structure with four
additional counters at each preﬁx of Told: two counters that
keep track of the number of malicious and non-malicious
IPs on the sequence sz, and two counters that keep track
of the number of mistakes made on malicious and non-
malicious IPs on sz. (This is the only update to the IPtree
data structure of [29] that ∆-Change requires.)
We do the following for each IP i: if the output predic-
tion pi of Told (obtained from Step 1) does not match the
input label of IP i, we increment the number of mistakes
made at each parent preﬁx of i in tree Told,z. We track mis-
takes on malicious and non-malicious IPs separately. We
also update the number of malicious and non-malicious IPs
seen at the preﬁx in the sequence sz. Thus, for example, in
Figure 7, if the input label of IP i is “-” and Told has pre-
dicted “+”, we update the errors for malicious IPs at each
highlighted parent preﬁx of i, and we also update the num-
ber of malicious IPs seen at each highlighted parent preﬁx.
Step 3: Learning Tcurr.
Finally, we update Tcurr with
the labeled IP. This ensures that our model of the stream is
current. This step directly applies the update rules of Track-
IPTree to Tcurr [29], as a subroutine. Effectively, the learn-
ing procedure penalizes the incorrect preﬁxes and rewards
the correct preﬁxes by changing their weights appropriately;
it then updates the tree’s structure by growing or pruning as
necessary. Due to space limits, we omit a detailed descrip-
tion of TrackIPTree’s update rules here.
3.2.2 Extracting Changes from IPTrees
At this point, we have measured the classiﬁcation error of
Told over the sequence sz (denoted by Told,z), and we have
allowed Tcurr to learn over sz. Now, our goal is to extract
the appropriate changes between sz and sz−1 by comparing
/16
IPs: 200
Acc: 40%
/16
IPs: 170
Acc: 13%
/16
IPs: 170
Acc: 13%
A: IPs: 50
Acc: 30%
B: IPs: 150
Acc: 90%
A: IPs: 70
Acc: 20%
B: IPs: 100
Acc: 10%
A: IPs: 70
Acc: 20%
B: IPs: 100
Acc: 10%
B: IPs: 40
Acc: 80%
C: IPs: 110
Acc: 95%
D: IPs: 20
Acc: 20%
C: IPs: 80
Acc: 5%
D: IPs: 20
Acc: 20%
C: IPs: 80
Acc: 5%
(a) Told,z−1: prior IPTree, annotated with
errors on sz−1
(b) Told,z: prior IPTree, annotated with er-
rors on sz
(c) Told,z: ﬁnal ∆-change preﬁx (shaded)
/16
A
B
D
C
(d) Tcurr: new chil-
dren found for ∆-
change preﬁx
Figure 8. ∆-Change Algorithm: Steps 4-6. Running example illustrates how ∆-change preﬁxes are extracted by comparing
Told,z−1, Told,z and Tcurr
the trees Told,z, Told,z−1 and Tcurr.
At a high-level, we do this in three steps: (1) ﬁrst, we
isolate candidate ∆-change preﬁxes, by comparing their
classiﬁcation errors and states between Told,z and Told,z−1;
(2) we prune redundant preﬁxes, which ensures that we do
not include preﬁxes as ∆-change if their children already
account for bulk of the change; (3) ﬁnally, we discover
new children that may have been grown in relation to these
changes by comparing Tcurr and Told.
Speciﬁcally, let D be the set of states that preﬁxes can
be assigned to. Let θ minimum number of IPs that a preﬁx
needs to originate to be considered potentially a ∆-change.
Let τ denote the maximum error a preﬁx may have to still be
considered an accurate model of the region’s trafﬁc.5 Let γ
as the minimum increase in a preﬁx’s error that guarantees
that the preﬁx is indeed ∆-change. We derive γ from the set
D, as we describe later. We will use Figure 8 to illustrate
these three steps with a running example, where we set θ =
50, τ = 0.1, and γ = 0.5.
In the example, we allow a
preﬁx to have one of two states: “good”, which corresponds
to 0-50% of its trafﬁc being malicious, and “bad”, which
corresponds to 51-100% of its trafﬁc being malicious.
Step 4: Isolate Candidate Preﬁxes. We isolate as a can-
didate set C all preﬁxes in Told satisfying the following con-
ditions: (1) its classiﬁcation error in Told,z exceeds γ, and
its classiﬁcation error in Told,z−1 is below τ; (2) at least θ
instances have been seen at the preﬁx. We then compare the
prediction labels of each preﬁx in C to the corresponding
preﬁx in Tcurr to check for a state change. If there is no
change in the preﬁx’s state, the change is not localized, and
we discard the preﬁx from C.
Figure 8(a) shows the original Told,z−1 (the states for
each node are not shown for readability, assume they are all
“good” in Told,z−1) (b) shows Told,z (again, the states of
each node are not shown, here assume all are “bad”). The
shaded preﬁxes (nodes B & C) have both sufﬁcient IPs and
the necessary change in the classiﬁcation error. Node A gets
5τ is typically set to a small value such as 0.01%, but cannot be set to
0% because of noisy data.
discarded because it is not accurate enough in Told,z−1, and
node D gets discarded because it has too few IPs.
Step 5: Prune Redundant Changes.
Not all candidate
preﬁxes isolated in C will represent a change in a distinct
part of the IP address space. Every parent of a ∆-change
preﬁx will also have made the same mistakes.
In some
cases, these mistakes may cause the parent preﬁx to also
have a high overall classiﬁcation error (e.g., when no other
child of that parent originates substantial trafﬁc). However,
some parents of a ∆-change preﬁx may have a high classiﬁ-
cation error due to changes in a different part of the address
space. To avoid over-counting, we include a preﬁx in C
only if the following two conditions hold: (1) it accounts
for an additional θ IPs after the removal of all children ∆-
change preﬁxes; (2) there is at least γ classiﬁcation error on
the IPs from the remaining preﬁxes in Told,z, and at most τ
error in Told,z. Figure 7(c) shows this step: we discard the
parent /17 preﬁx (node B) from the list of ∆-change pre-
ﬁxes, because it does not account for an additional θ = 50
IPs after we remove the contribution of its ∆-change /18
child (node C).
Step 6: Discover New Children.
The tree Tcurr may
have grown new subtrees (with multiple children) to ac-
count for the changes in the input IP stream. We compare
each preﬁx c ∈ C with the corresponding preﬁxes in Tcurr
to see if new subtrees of c have been grown. If these sub-
trees differ from c in their prediction labels, we annotate c
with these subtrees to report to the operator. Figure 8(d)
shows Tcurr and the corresponding subtrees (of depth 1
in this example) of the ∆-change preﬁx (node C) in Told,
which are annotated with node C for output.
To wrap up the ∆-Change algorithm, we discuss the two
parameters we did not specify earlier. First, we need to de-
ﬁne how we obtain Told from Tcurr. Since Told needs to
have its accuracy measured in interval z − 1, it needs to be
learnt no later than z − 2. So, Told = Tz−2 for the sequence
sz, and at every interval, we update Told to be the tree learnt
2 intervals before the current one. We also need to derive
γ from the set of states D. Since each state is deﬁned by
an interval in [0, 1], we can use the interval boundaries to
derive D. Thus, for example, if each state D has the same
interval length, then γ = 1
|D| .
Properties of ∆-Change: Efﬁciency and Correctness
The ∆-Change algorithm meets our goals of operating on-
line on streaming data and our computational requirements
as each step involves only the following operations: learn-
ing an IPtree, comparing two IPtrees or applying an IPtree
on the input data. The ﬁrst operation is performed by the
TrackIPTree algorithm, which is an online learning algo-
rithm, and the remaining operations require storing only the
IPtrees themselves, thus requiring only constant additional
storage.
More precisely, the key data structures of ∆-Change are
three IPtrees, i.e., Told,z−1, Told,z, Tcurr. The basic IPTree
data structure has a space complexity of O(k) for a tree
with k leaves [29], as TrackIPTree only stores a number of
weights and counters per preﬁx. For ∆-Change algorithm,
as described in Step 2, we need to augment the basic IP-
tree structure with four additional counters for each preﬁx.
Thus, the space complexity for ∆-Change remains O(k).
Next, we describe the run-time complexity of ∆-Change.
Steps 1, 2 and 3 are applied to every IP in the input se-
quence, but each step has at most O(log k) complexity: for
each IP, we examine all its parents in Tcurr and Told a con-
stant number of times (only once in Step 2, 2-4 times in
Steps 1 & 3 as part of the subroutines of TrackIPTree), so