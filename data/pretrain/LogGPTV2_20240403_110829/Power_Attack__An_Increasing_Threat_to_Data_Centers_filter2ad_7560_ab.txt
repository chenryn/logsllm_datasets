SATA
Ubuntu
12.10/3.5.0-
17 kernel
Xen 4.1.2
2*7200RPM
SATA
Ubuntu
12.10/3.5.0-
17 kernel
Xen 4.1.2
2*7200RPM
SATA
Ubuntu
12.10/3.5.0-
17 kernel
Xen 4.1.2
6*1GB
Server F
2*Xeon Dual
Core
2*512MB
DDR,
DDR
2*7200RPM
SATA
Ubuntu
12.10/3.5.0-
17 kernel
Xen 4.1.2
Server G
2*Xeon Dual
Core
16*1GB DDR2
2*7200RPM
SATA
Ubuntu
12.10/3.5.0-
17 kernel
Xen 4.1.2
able to save considerable cost and complexity of running the
underlying computing platforms.
In a PaaS environment, a user can run various applications
ﬂexibly on the platform, i.e., the user can get full control
over the workload pattern. This feature of PaaS provides an
attacker the opportunity to launch a power attack. The attacker
can subscribe the platform from the service provider and run
specially crafted workload. The malicious workload can lead
to a signiﬁcant rise of power consumption.
Some PaaS providers [1] deploy load balancing mecha-
nisms to prevent a workload burst. A load balancing system
normally monitors the system utilization of the servers and
dynamically schedules workload. However, load balancing is
not equal to power balancing. It is very hard to accurately
model
the power consumption of a machine with respect
to system utilization. With these characteristics of PaaS, we
present a potential attack vector as follows: an attacker can
subscribe multiple servers that are within the same rack
(connected by same breaker) from providers and run specially
crafted applications/workload on them.
The attack workload should be designed to signiﬁcantly
increase the power consumption of victim servers in two
phases. First, a heavy workload is generated to exercise system
utilization to a high level. This will directly increase the power
consumption of victim servers. The system utilization should
reach a certain cap under such a high workload, e.g.
the
system utilization may reach the cap of load balancer or CPU
utilization reaches the 100% cap. In the second phase, after
reaching system utilization cap, the workload is no longer
increased. Instead, the patterns or conﬁgurations of the work-
load will be adjusted. Since different workload conﬁgurations
yield different power consumptions, by adjusting workload
patterns or conﬁgurations, the power consumption of victim
servers will be further increased to an even higher level without
increasing system utilization. In this way, the target CB could
be overloaded.
B. Attack evaluation
To evaluate the feasibility and effect of power attack in
PaaS, we conduct experiments in a testbed that simulates
the PaaS environment. The conﬁguration of all the servers
used in our experiments can be found in Table I. While high
performance computing (HPC) has become a pervasive service
nowadays, the demand of tremendous resources and parallel
computing makes HPC a suitable candidate of PaaS. Thus, we
use HPC as the workload of PaaS in our study.
1) Single Server: First, we conduct single server experi-
ments to ﬁgure out how different workloads may affect the
power consumption and system utilization of a server. The
testbed of this round of experiments is Server A in Table I.
SPECCPU2006 is used as the benchmark in our experiments.
SPECCPU2006 is a CPU intensive benchmark that is widely
used as HPC benchmark. There are different benchmarks in
SPECCPU that perform different computations. Since these
benchmarks will yield different workload patterns, we can
regard them as different HPC applications running in PaaS
environments.
Figures 2 and 3 illustrate power consumption and memory
utilization of different benchmarks in SPECCPU2006. These
benchmarks are carefully selected so that they consume similar
amount of memory. Since our testbed has four cores, we
run four copies of SPEC benchmarks in parallel
to fully
exercise all cores. Since all benchmarks can exercise the
CPU to reach the same utilization, we do not show CPU
utilization in the ﬁgure. As Figure 2 shows, different workloads
yield very different power consumptions. Figure 3 illustrates
the memory consumptions of these benchmarks. Note that
SPECCPU involves negligible disk and network activities, so
their impact on power consumption is insigniﬁcant and can be
ignored.
As Figure 3 shows, while all these benchmarks induce
the same CPU utilization, benchmark 465 consumes the least
memory. However, benchmark 465 consumes more power
than many other benchmarks. For instance, while benchmark
462 consumes around 150 W power, benchmark 465 has
power spikes up to 175 W, which is over 15% more than
that of benchmark 462. We can also see that the memory
consumption of benchmark 462 and that of benchmark 456
are very close. The average memory utilization for 462 is
24%, while the average memory utilization for 456 is around
25%. With the same CPU utilization, similar memory usage,
and negligible I/O activity, we can regard that benchmark 462
and benchmark 456 consume very similar amount of system
resources. However, from Figure 2 we can see that benchmark
456 consumes over 20% more power than benchmark 465.
Our observations indicate that system utilization, i.e, resource
consumption, cannot accurately determine power consumption.
We also run another HPC benchmark, High Performance
Linpack (HPL) on the testbed. HPL is a benchmark to calculate
random matrix production. It has multiple parameters to con-
ﬁgure, which will affect the performance of the benchmark.
In our experiments, we take the following root parameters
4
200
150
100
50
)
W
(
r
e
w
o
P
0
0
435.gromacs
444.namd
456.hmmer
465.tonto
462.libquantum
200
400
600
800
1000
Time (s)
30
25
20
15
10
5
%
n
o
i
t
a
z
i
l
i
t
U
y
r
o
m
e
M
0
0
435.gromacs
444.namd
456.hmmer
465.tonto
462.libquantum
200
400
600
800
1000
Time (s)
200
150
100
50
)
W
(
r
e
w
o
P
0
0
hpl−9000−200
hpl−9000−40
hpl−9000−20
hpl−9000−1
200
400
600
800
1000
Time (s)
Fig. 2.
different SPECCPU workloads.
Power consumption of the server with
Fig. 3. Memory consumption of the server with
different SPECCPU workloads.
Fig. 4.
HPL benchmark with different conﬁgurations.
Power consumption of the server running
i.e.,
the processor grid,
into consideration:
the number of
processors, the problem size N which is the size of input
matrix, and the block size NB which determines how HPL
solves the matrix production problem. Since our testbed is a
4-core machine, we ﬁx the number of processors to be 4. To
make the input size consistent, i.e., make HPL consume the
same amount of memory, we ﬁx the parameter N as 9000. We
adjust the value of NB among 200, 40, 20, and 1. As the CPU
utilization and memory utilization remain the same in these
experiments, we do not present their results here.
Figure 4 illustrates the power consumption of our testbed
while running HPL with different conﬁgurations. The adjusted
parameter, NB, will determine the way HPL solves the prob-
lem. From the ﬁgure we can see, with a different value of
NB, the power consumption of our testbed differs signiﬁcantly.
When the block size is set to 1, the testbed only consumes
less than 150 W power. By contrast, the testbed has power
consumption near 190 W when the block size is set to 20. Such
results indicate that even for the same application, different
parameters or conﬁgurations can yield considerably different
power consumption.
2) Rack-level Cluster: To verify if an attacker can generate
signiﬁcant power rise by adjusting the workload beyond a
single server, we further conduct experiments in a rack-level
cluster. We setup a 4-server rack with Server D, Server E,
Server F, and Server G in Table I. These four servers are
connected to the same switch and circuit breaker, resembling
a rack in a real world data center.
We run SPECCPU2006 on all servers in the rack. The
overall power consumption of the entire rack is recorded.
These benchmarks are conﬁgured to exercise CPU to full
utilization (reaching the cap) and the memory usage percentage
of each server on these benchmarks are the same as those
in our single-server experiments. Therefore, we only present
the results of power consumption at the rack level, which are
illustrated in Figure 5.
As Figure 5 shows, the rack level results concur with
our single server results. While the CPU utilizations in all
cases reach the cap, some benchmarks generate more power
consumption than others. Such an observation indicates that
even after the system utilization reaches a cap, an attacker
has the potential to increase the power consumption of target
by adjusting the workload, e.g., the attacker can change the
workload from benchmark 462 to benchmark 456 to further
increase power consumption.
5
Then we design two malicious traces to launch a power
attack against a rack in PaaS environments. The ﬁrst trace
is based on SPECCPU2006. At the beginning, the workload
behaves as a normal workload that generates moderate system
utilization and power consumption. We use benchmark 462
with light workload conﬁguration to represent such a moderate
workload. Next,
the attacker can change the workload to
exercise the system utilization to a certain level to signiﬁcantly
increase power consumption. We use benchmark 462 with
heavy workload to represent the malicious workload during
this phase. Finally, after system utilization reaches the cap,
the attacker tunes the workload to further increase power
consumption. Here we use benchmark 456 to represent the
malicious workload of this phase.
The second malicious trace is based on High Performance
Linpack (HPL). While SPECCPU2006 can be used as running
independent workloads on different machines, we use HPL to
simulate the scenario where multiple servers are coordinated
to run the same task in PaaS. Each of the four servers works
as a node in the working cluster and they communicate with
each other via OpenMPI. HPL will distribute the workload to
each node for high performance computation. In this round of
experiments, we conﬁgure HPL with different problem sizes
(N) and different block sizes (NB). The power attack based on
HPL is mounted as follows. Fist, the input size of the workload
is set to be moderate, we use HPL with N set to 1000 and NB
set to 5 to represent the moderate workload. Next, the attacker
can enlarge the input size to increase the system utilization.
In our experiments, we increase the block size to 4000 during
this phase and more CPU cores are exercised. At last, the
attacker can change the workload pattern to further increase
power consumption. In our case, we modify NB from 5 to
100.
The evaluation results of the malicious workload are illus-
trated in Figure 6. It is evident that both malicious traces can
generate a signiﬁcant rise in the overall power consumption of
the rack. As Figure 6 shows, the power attack can trigger over
30% increase in power consumption.
C. Damage assessment and analysis
Our experimental results above validate that in PaaS envi-
ronments, an attacker can generate abnormal high power con-
sumption by adjusting workload running on target machines.
The damage caused by such a power attack is at two levels.
A relatively light damage can be overheating the IT equipments
)
W
(
r
e
w
o
P
1000
950
900
850
800
750
700
650
0
1000
900
800
700
600
)
W
(
r
e
w
o
P
435.gromacs
444.namd
456.hmmer
462.libquantum
465.tonto
HPL based power attack
SPECCPU based power attack
100
200
300
400
500
Time (s)
500
0
100
200
300