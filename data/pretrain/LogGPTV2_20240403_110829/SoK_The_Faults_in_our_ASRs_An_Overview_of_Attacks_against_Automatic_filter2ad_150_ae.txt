amounts of additional distortion to the sample, such that it
becomes a determent to their original aims. However, measure
audible distortion is difﬁcult, due to lack of any psychoacoustic
metric, as discussed later in Section VIII-6.
C. Stochastic Modeling:
Defenses against these attacks can often use stochastic (or
ML) models as part of the pipeline. Such techniques gather
different features that might help differentiate an adversarial
sample from a benign one. These features are then used to
train a ML model to do the classiﬁcation. A strong defense
does not rely on ML models as these models are themselves
vulnerable to exploitation.
7) Model Agnostic Attacks: Only a small subset of existing
attacks methods are black-box [79], [81], [42], [7], [29], [71]
(Table III) and therefore model agnostic, while the remain-
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
739
Name
Stochastic Model
Additional
Hardware
Attacker
Type
Distance
Attack
Type
Audio
Type Medium ASR SI
Adversarial Cost
Resources Distortion
A
A
A
A
L
?
?
?
?

?
?
?
?

?
?
?
?
?
?
?
?
?
?
?
?
?
?










ADPT, N-ADPT
Wang et al. [84]
VoicePop [85]
Blue et al. [64]
Wang et al. [86]
Yang et al. [87]
Table IV: The table3 provides an overview of the current defenses for ASR and SI systems. “?”: Authors provide no information
in paper. “”: Does not work or has not been demonstrated. “”: Will work. “P,W,S” = Phoneme, Word, Sentence. “L,A,T”
= Over-Line, Over-Air, Over-Telephony-Network. “ADPT, N-ADPT” = effective against Adaptive Attacker, effective against
Non-Adaptive Attacker.
D. Additional Hardware:
N/A
Miscellaneous
N/A
Miscellaneous
Miscellaneous
N/A
Miscellaneous Noisy
Clean
VII. DEFENSES AND DETECTION CLASSIFICATION
6 cm
5 m
?
N/A
N-ADPT
Direct
Some defense techniques might require sensors in addition
to the microphone. This is problematic as it increases the
cost of manufacturing the home assistants. Due to already
thin proﬁt margins [88], additional sensor cost will decrease
the likelihood of manufacturers incorporating the defense. A
strong defense does not incur any additional deployment costs.
E. Distance:
When defending against Over-Air attacks, some techniques
are only effective if the source of the adversarial audio is
within a certain distance from the target. This is because
the defense techniques might use certain identifying features,
which might otherwise be lost during transmission. The longer
the distance, the stronger the defense.
It is worth discussing the ideal distance. Over-Air attacks
require an audio sample to be played over a speaker that is
present within the same room as the target VPS. Within the
US, rooms are on average 11ft by 16ft [89]. This means in the
worst case, the attacker will need to play an audio ﬁle from
the farther part of the room. If the target VPS is in one corner
of the room and the attacker’s speaker in the other corner, this
distance will come down to 20ft (Pythagorean Theorem).
F. Attack Type:
This is the speciﬁc type of attack the strategy is supposed to
protect against (Section IV-B). These include signal processing
attacks, direct optimization and indirect optimization attacks.
A strong defense is universal i.e., stops any type of attack.
G. Audio Type:
This is the speciﬁc type of audio that strategy is designed
to defeat (Section IV-D1.0.2). These include clean, inaudible,
and noisy audio. A strong defense is universal i.e., can stop
an adversarial audio of any type.
H. Medium:
The medium the strategy is designed to defend (Sec-
tion IV-D3). A strong defense can stop an attack over any
medium (Over-Air, Over-Telephony, and Over-Line).
3The most up-to-date version of
https://sites.google.com/view/adv-asr-sok/
this
table
can be
found at:
There has been little published work in the space of de-
fenses and detection mechanisms for adversarial audio. In this
section, we discuss how the most popular defense strategy
from the adversarial image space, adversarial training, is not
effective in the audio domain. We also analyze the published
mechanism for detecting adversarial audio and propose a
direction for future research.
A. Adversarial training
This defense involves training the model on samples per-
turbed using adversarial algorithms. This strategy has shown
promise in the adversarial image space [90]. Intuitively, this
improves the decision boundary, by either making it more
robust to attacks or by making adversarial samples harder to
craft by obfuscating the gradients. To exploit the adversarially
trained model, the attacker will either need to run the attack
algorithm for more iterations or introduce greater distortion to
the adversarial input. We argue that this is not be an effective
method for defending VPSes.
First, adversarial training can decrease the accuracy of the
model. This phenomenon is known as label leaking [91]. Here,
the adversarially trained model shows improved robustness
to adversarial samples, but at the cost of decreased accuracy
on legitimate samples. Given that VPSes (e.g. Siri, Amazon
Alexa) are user-facing, reducing their accuracy will degrade
the user experience and consequently, the vendor’s proﬁts.
Second, adversarial training might not work against the signal
processing attacks discussed in Section IV-B2. These generate
perturbations that are ﬁltered out during feature extraction [7],
[42]. Thus, both adversarial and legitimate samples will collide
to similar feature vectors. As a result, adversarial training
might decrease model accuracy with no improvement to ro-
bustness [29]. This is different from label leaking as the model
accuracy for both benign and adversarial samples will suffer.
B. Liveness Detection
it
training,
Considering the limitations of adversarial
is
important to discuss detection strategies that have shown some
success. Detection mechanisms are designed to help identify
whether an audio sample is benign or malicious. Detection
often suffers from the same limitations as adversarial training:
for instance, both fare poorly against adaptive adversaries.
Liveness detection is an area of research that aims to identify
whether the source of speech was a real human or a mechanical
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
740
speaker. It can help prevent replay audio attacks i.e., an
attacker plays an audio of someone ordering something back
to their home assistant. This is a popular research area with
yearly competitions [92]. Considering this is still an open
research area, we only cover a handful of important works,
shown in Table IV.
Liveness detection can help detect Over-Air attacks as they
require playing the audio over mechanical speakers. However,
works in this area limited to a number of reasons. First, these
works [84], [85], [86] employ ML models as part of the
pipeline. This adds another layer of vulnerability as attackers
have the ability to exploit ML models using adversarial
algorithms. This means an adaptive attacker, with knowledge
of the detection method, can overcome it. Second, these works
either require the distance between the source and target to be
very small [85] or the authors fail to disclose this number at all
Table IV [84], [86]. An attacker can merely execute the attack
from a distance further than the one ideal of the detection
method to work. One mechanism that has shown promise is
Blue et al.
[64]. This defense is effective at a much larger
distance and is not defeated by an adaptive attacker.
C. Future Direction
It is worth discussing future directions that may not guar-
antee success. An example of this is redesigning the feature
extraction and preprocessing stages. Signal processing attacks
are thus far unique to ASR systems. These attacks exploit
limitations and vulnerabilities in the preprocessing and feature
extraction phases of the ASR pipeline. Even though the
existence of this class of adversarial examples is clear, these
are still not easy to resolve. This is because the techniques that
are used during feature extraction (e.g., the DFT) have taken
decades to develop [20]. These techniques are well understood
by the research community and have signiﬁcantly improved
the accuracy of ASR systems. Developing new techniques that
not only resolve current security ﬂaws but also maintain high
ASR accuracy is difﬁcult and does not guarantee robustness.
VIII. DISCUSSION
In this section, we highlight key ﬁndings, discuss their
implications and make recommendations for future research.
1) Lack of Transferability for Optimization Attacks: Trans-
ferability has been shown for signal processing attacks. How-
ever until now,
this question was largely unanswered for
the case of optimization attacks. In this paper, we empiri-
cally demonstrate that transferability of optimization attacks
is unlikely in the audio domain. In fact, this is true even
if both surrogate and remote target models share the same
architecture, hyperparameters, random seed and training data.
This is a result of training on GPUs, which introduces non-
determinism that can lead to the VPSes learning different
decision boundaries [93]. Therefore, for adversarial audio
samples to transfer, the adversary must not only have com-
plete information of the model’s training parameters, but also
be able to predict the GPU-induced randomness during the
training process. We believe that this latter requirement is
unrealistic for practical adversaries.
The only optimization attack that has demonstrated transfer-
ability, although in a limited sense, is Commander Song [41].
The authors were able to transfer samples generated for the
Kaldi ASR [25] to iFlytek [94], but failed when transferring
the samples to DeepSpeech [95]. A likely hypothesis is that
iFlytek is using a ﬁne-tuned version of the same pre-trained
Kaldi ASR, as has been the case in the past [94]. Thus a caveat
to the lack of transferability is that adversarial samples may
be transferred between a ﬁne-tuned model and its pre-trained
counterpart. This could be explained by the fact that decision
boundaries do not change signiﬁcantly during the ﬁne-tuning
process. Additionally, an increasing number of vendors are
transitioning to NN based systems, away from the HMMs
that the Kaldi ASR employs internally. Therefore, the question
of lack of transferability for optimization attacks should be
considered more seriously with regards to NN based ASRs.
2) Defenses for VPSes: A number of defenses have been
proposed for the computer vision domain [96], [97], [98], [99],
[100], [101], [102], [103], [104]. These have been primarily
designed to defend against adversaries who might exploit
transferability. However, the transferability is difﬁcult in the
audio domain, and attackers rarely have white-box access
to target VPSes (e.g., Amazon Alexa). This minimizes the
threat of existing white-box attacks against real world systems.
Consequently, researchers should focus on building defenses
against attacks that have been demonstrated in black-box
settings, such as signal processing attacks [7], [29].
3) VPSes Pipeline: The modern VPS pipeline is completely
different from that of image models, due to presence of
additional components. Each of these components increases
the attack surface, introducing a unique set of vulnerabilities
that an attacker may be able to exploit. The full scope of
vulnerabilities has yet to be uncovered, with some attacks
(e.g., clean, targeted attacks) not having been demonstrated.
Therefore, future research should focus on identifying and ex-
ploiting novel weaknesses within the pipeline. Similarly, if the
entire pipeline can be attacked, then the entire pipeline needs
to be defended. Thus, we recommend that future research
efforts focus on building robust defenses for each individual
component of the pipeline.
4) Lack of Poisoning and Privacy Attacks: This paper
focuses on evasion attacks to the detriment of other adversarial
machine learning attacks such as poisoning attacks and privacy
attacks [31]. This is because, to the best of our knowledge, no
poisoning attacks or privacy attacks have been proposed for
speech. And existing attacks which may apply have not been
evaluated. This is an interesting direction for future research to
explore. Poisoning attacks generate audio samples that, when
added to the training data, make the model misbehave in an
attacker-controlled way. For example, poisoning can be used
prevent the model from correctly transcribing certain types of
inputs. In contrast, privacy attacks attempt to uncover informa-
tion about the model’s training data. For example, an attacker
may want to determine if the voice of a certain individual was
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
741
VPSes are comprised of additional phases including process-
ing, feature extraction, and decoding algorithms. This means
attacks against image models cannot be easily extended to
VPSes. This has lead to the development of attacks designed
for VPSes. However, most of these attacks can not be used
against real-world systems. This is primarily due to lack of
success in black-box settings, failure Over-Air, and limited
transferability. There has also been limited development of
adequate defenses for VPSes. While there are a plethora of
defenses and detection mechanisms in the image domain, only
one exists for VPSes, which is limited to optimization attacks.
REFERENCES
[1] C. Martin, “72% Want Voice Control In Smart-Home Products,” Ac-
cessed in 2019, available at https://www.mediapost.com/publications/
article/292253/2-want-voice-control-in-smart-home-products.html?
edition=99353.
[2] H. Stephenson,
“UX design trends 2018:
inter-
faces to a need to not
trick people,” Accessed in 2019, avail-
able at https://www.digitalartsonline.co.uk/featurs/interactive-design/
ux-design-trends-2018-from-voice-interfaces-need-not-trick-people/.
[3] “Who’s Smartest: Alexa, Siri, and or Google Now?” Last accessed in
from voice
2019, available at https://bit.ly/2ScTpX7.
[4] “Azure Speaker Identiﬁcation API,” Last accessed in 2019, available
https://azure.microsoft.com/en-us/services/cognitive-servic/
[5] “Mozilla Project DeepSpeech,” Last accessed in 2019, available
https://azure.microsoft.com/en-us/services/cognitive-servic/
at
speaker-recognition/.
at
speaker-recognition/.
used for training a speaker identiﬁcation system.
5) Detection Mechanisms: While observing Table IV, an
astute reader might have realized that no mechanism yet exists
to defend against telephony-attacks. while liveness detection
and temporal based mechanisms have demonstrated some
success in addressing Over-Air and Over-Line attacks. These
methods are not perfect, but still constitute a positive step
towards addressing these attacks. In stark contrast, there is
no work that addresses Over-Telephony attacks. Given that
these attacks can be most reliably executed against real world
surveillance systems, it would be ideal to focus research efforts
in this space.
6) Lack of Audio Intelligibility Metrics: A number of
methods have been used to measure intelligibility of audio.
However, these methods have limitations. Researchers have
used metrics from the computer vision domain such as the
L2-norm [6]. This is not an adequate metric to measure audio
intelligibility as the human ear does not exhibit linear behavior
(Section II-A). Thus, audio samples that are jarring to the
human ear, can still have small L2-norm [7]. In addition, prior
work often includes users studies that measure the quality
of attack audio samples [72], [82], [29], [74]. Unfortunately,
these studies do not consider the full range of variables that
impact human intelligibility. These include age [105], [106],
ﬁrst language [107], audio equipment, hearing range [108]
and environmental noise. Future works should consider these
variables to improve generalizability of their ﬁndings. Finally,
some researchers use audio quality metrics designed for the
telephony netowrks. These are metrics are designed to measure
audio quality of telephony lines [67], [109] and effectively
measure quantities like jitter, packet loss, and white-noise;
which are facets of audio that existing attacks do not target.
Psychoacoustics is a promising direction for designing suit-
able audio intelligibility metrics. Recall from Section II-A that
human perception of speech is affected by a combination of
mechanisms. Some of these mechanisms are quantitative and
can be used to construct hearing models [110]. An example
is audio masking [8]. Attack perturbations are,
in effect,
frequencies that have been introduced to the benign audio sam-
ples. Some of these frequencies will mask other frequencies.
This masking effect can be measured using metrics such as
tone-to-noise ratio and prominence ratio. These metrics, in
combination with other metrics from an hearing model, can
help measure the quality of an adversarial speech sample.
IX. CONCLUSION
Modern VPSes use neural networks to convert audio sam-
ples into text (ASRs) or identify the speaker (SIs). However,
neural networks have been shown to be vulnerable to ad-
versarial machine learning attacks. These can force VPSes
to act maliciously. In this paper, we present a threat model