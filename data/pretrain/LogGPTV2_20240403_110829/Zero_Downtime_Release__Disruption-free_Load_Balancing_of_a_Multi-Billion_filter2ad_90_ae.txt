### Duration of Draining Period on Same Machine Resources (§ 6.3)

**Figure 14: Disruptions During Proxygen Restart**

**GR vs. GN R Behavior:**
Analyzing the per-group breakdown, we observe that the CPU utilization for restarting instances (GR) only persists for two minutes (at T=2, 3). The CPU utilization then gradually decreases until the end of the draining period, where a sharp decrease is observed (at T≥24) due to the termination of parallel processes. It is surprising that the CPU utilization of GR is lower than the cluster-wide average, even though every machine in GR runs two Proxygen instances during 2≤T≤24. 

We observe that the Requests Per Second (RPS) drops for GR and rises for GN R after T=3, indicating that GR instances are serving fewer requests than before the restart, while GN R instances are serving a higher proportion. This contrasting behavior is due to CPU usage being a function of RPS; an instance serving fewer requests requires fewer CPU cycles. Since Katran uniformly load-balances across the Proxygen fleet and the newly-spun, updated instance has no request backlog, it gets the same share of requests as others, leading to the drop and ramp-up in RPS over time.

For MQTT connections, we observe their number to fall across GR instances and gradually rise for GN R. This behavior is expected as the MQTT connections get migrated to other healthy instances (GN R) through Downstream Connection Reuse (DCR). However, the number of MQTT connections does not drop to zero for GR at the end of the draining period because the updated, parallel Proxygen picks up new MQTT connections during this duration.

**Timeline for Disruption Metrics:**
Figure 14 builds a similar timeline for disruption metrics—TCP resets (RST), HTTP errors (500 codes), and Proxy errors, presenting their count. Each point is the average value observed for a cluster, and the box plot shows the distribution across the clusters. The timeline is divided into four phases, similar to Figure 13. We observe that the disruption metrics remain consistent throughout the restart duration. Even for a 20% restart, there is no increase in these disruption metrics, highlighting the effectiveness of Zero Downtime Release in shielding disruptions. The lack of change in TCP RSTs underscores the efficacy of Zero Downtime Release in preventing TCP SYN-related inconsistencies, as observed for SO_REUSEPORT-based socket takeover techniques [38].

### 6.2.2 Ability to Release at Peak-Hours

Traffic load at a cluster changes throughout the day, exhibiting a diurnal pattern [44]. Traditionally, updates are released during off-peak hours to minimize load and possible disruptions. Figure 15 plots the probability density function (PDF) of Proxygen and App. Server restarts over the 24 hours of the day. Proxygen updates are mostly released during peak hours (12 PM-5 PM). In contrast, the higher frequency of updates for the App. Server (Figure 2a) leads to a continuous cycle of updates, with a fraction of the App. Server always restarting throughout the day, as seen by the flat PDF in Figure 15. From an operational perspective, operators are expected to be hands-on during peak hours, and the ability to release during these hours is crucial as developers can swiftly investigate and solve any problems due to a faulty release.

### 6.3 System Overheads

**Micro-benchmarks:**
Improving cluster availability and client performance during a restart can come at the cost of increased system resource usage. Figure 17 plots the system resource usage during the restart phase for machines in a randomly chosen production edge cluster. Since CPU consumption varies at different phases of the restart (increasing initially and then returning to normal as seen in the timeline in Figure 13), we present system benchmarks during the entire restart and show the median numbers observed across different machines in a randomly selected edge cluster. The presence of two concurrent Proxygen instances contributes to the costs in system resources (increased CPU and memory usage, decreased throughput). The change in throughput correlates inversely with CPU usage, and the initial spike in CPU usage causes a decrease in tail throughput. Although the tail resource usage can be high (persisting for around 60-70 seconds), the median is below 5% for CPU and RAM usage, meaning the increased resource usage does not persist for the whole draining duration (§ 6.2). As the machine remains available and able to serve connections, this overhead is a small price to pay for minimizing disruptions and keeping the overall cluster close to its baseline capacity (i.e., non-restart scenario).

### 7. Related Work

**Release Engineering:**
Release engineering is critical to the performance and reliability of large-scale systems and has been a topic of discussion among industry [8, 20, 21] and researchers [9]. Different companies handle the release process in ways that suit the needs of their services and products [9, 20]. With the increased focus on improving the performance and availability of global-scale systems, release engineering has become a first-class citizen [20]. This paper moves forward the discussion around best practices for update releases at a global scale.

**Load-Balancing at Scale:**
The paper builds on recent advancements in improving network infrastructure (Espresso [58], FBOSS [19], Katran [7], etc.) and the use of L7LB [2, 3, 48, 53, 56] for managing traffic. It proposes novel ideas to improve stability and performance by leveraging L7LB to mask any possible restart-related disruptions from end-users and improve the robustness of protocols that do not natively support graceful shutdowns. Some aspects of the paper have been discussed in earlier works [34, 46] from Facebook. However, this paper tackles a wider range of mechanisms, evaluates them at production scale, and describes the experiences of using these mechanisms.

**Managing Restarts and Maintenance at Scale:**
Recent work has focused on managing failures and management-related restarts/updates for various components of infrastructure, ranging from hardware repairs [57] to network and switch upgrades [10, 32] in data centers. Our focus is primarily on software failures and the graceful handling of restarts to allow faster deployment cycles and zero disruptions. While these works focus mainly on data center scenarios, we tackle the entire end-to-end infrastructure at a global scale.

**Disruption Avoidance:**
Recently, a few proxies have been equipped with disruption avoidance tools to mitigate connection terminations during restarts [36, 43]. HAProxy proposed Seamless Reloads [35, 55] in 2017, and a socket FD transfer mechanism (similar to Socket Takeover) was added in the second half of 2018 [4]. Envoy recently added Hot Restart [16] with a similar motivation. Our mechanisms are more holistic as they support disruption-free restarts for protocols other than TCP (e.g., UDP) and provide end-to-end support to mitigate disruptions, such as Partial Post Replay for HTTP and Downstream Connection Reuse for MQTT. Additionally, this work provides a first-time, hands-on view of the deployment of these techniques on a global scale.

### 8. Conclusion

Due to high code volatility, Cloud Service Providers (CSPs) release up to tens of updates daily to their millions of globally-distributed servers. Frequent restarts can degrade cluster capacity and disrupt user experience. Leveraging end-to-end control over a CSP’s infrastructure, the paper introduces Zero Downtime Release, a framework to enable capacity preservation and disruption-free releases by signaling and orchestrating connection hand-over during restart (to a parallel process or upstream component). The framework enhances pre-existing kernel-based mechanisms to fit diverse protocols and introduces novel enhancements on implementation and protocol fronts to allow fast, zero-downtime update cycles (globally-distributed fleet restarted in 25 minutes), while shielding millions of end-users from disruptions.

### 9. Acknowledgments

Many people in the Proxygen and Protocols teams at Facebook have contributed to Zero Downtime Release over the years. In particular, we would like to acknowledge Subodh Iyengar and Woo Xie for their significant contributions to the success of Zero Downtime Release. We also thank the anonymous reviewers for their invaluable comments. This work is supported by NSF grant CNS-1814285.

### References

[38] Joseph Lynch. True Zero Downtime HAProxy Reloads.
https://bit.ly/31H2dWz.

[39] Linux man page. cmsg(3) - access ancillary data.
https://linux.die.net/man/3/cmsg.

[40] Linux man page. dup, dup2, dup3 - duplicate a file descriptor.
https://linux.die.net/man/2/dup.

[41] Linux man page. recvmsg(2).
https://linux.die.net/man/2/recvmsg.

[42] Linux man page. sendmsg(2).
https://linux.die.net/man/2/sendmsg.

[43] Suresh Mathew. Instant Deployment and Rollback. Zero Downtime.
https://bit.ly/2ZgNGzV.

[44] Arun Moorthy. 2015. Connecting the World: A look inside Facebook’s Networking Infrastructure.
https://unc.live/2UzVe0f.

[45] mqtt.org. MQ Telemetry Transport, machine-to-machine (M2M) connectivity protocol.
http://mqtt.org/.

[46] Kyle Nekritz and Subodh Iyengar. Building Zero protocol for fast, secure mobile connections.
https://bit.ly/2VkkoiH.

[47] Sam Newman. 2015. Building microservices: designing fine-grained systems. O’Reilly Media, Inc.

[48] NGINX. NGINX Reverse Proxy.
https://bit.ly/39fkWLt.

[49] Inc O’Reilly Media. 2012. Facebook’s Large Scale Monitoring System Built on HBase.
https://bit.ly/2tAHlnc.

[50] Chuck Rossi. 2017. Rapid release at massive scale.
https://bit.ly/2w0T9jB.

[51] Tony Savor, Mitchell Douglas, Michael Gentili, Laurie Williams, Kent Beck, and Michael Stumm. 2016. Continuous deployment at Facebook and OANDA. In 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C). IEEE, 21–30.

[52] Amazon Web Services. Configure Connection Draining for Your Classic Load Balancer.
https://amzn.to/39iQlMS.

[53] Daniel Sommermann and Alan Frindell. 2014. Introducing Proxygen, Facebook’s C++ HTTP framework.
https://bit.ly/31Ihfvm.

[54] Facebook Open Source. Hack - Programming Productivity Without Breaking Things.
https://hacklang.org/.

[55] Willy Tarreau. 2017. Truly Seamless Reloads with HAProxy – No More Hacks!
https://bit.ly/31Ihfvm.

[56] VDMS. Our software - CDN.
https://bit.ly/2UC0kZI.

[57] Kashi Venkatesh Vishwanath and Nachiappan Nagappan. 2010. Characterizing cloud computing hardware reliability. In Proceedings of the 1st ACM symposium on Cloud computing. ACM, 193–204.

[58] Kok-Kiong Yap, Murtaza Motiwala, Jeremy Rahe, Steve Padgett, Matthew Holliman, Gary Baldus, Marcus Hines, Taeeun Kim, Ashok Narayanan, Ankur Jain, et al. 2017. Taking the edge off with espresso: Scale, reliability, and programmability for global internet peering. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication. ACM, 432–445.