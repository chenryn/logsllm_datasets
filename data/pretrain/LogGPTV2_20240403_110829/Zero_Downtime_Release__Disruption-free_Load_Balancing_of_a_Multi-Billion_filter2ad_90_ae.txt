for the duration of draining period on same machine resources (§ 6.3).
Figure (14) Disruptions during Proxygen restart
GR vs GN R behavior: Analyzing the per-group breakdown, we
observe the inflation in CPU utilization for restarting instances (GR)
only persists for two minutes (at T=2,3) and the CPU util. gradually
decreases until the end of draining period where we observe a sharp
decrease (at T≥24) due to termination of parallel process. CPU util. of
the(GR)tobelowerthancluster-wideaverageandGR (non-restarted
group) is surprising as every machine in GR runs two Proxygen in-
stances during 2≤T ≤ 24. We observe RPS to drop for GR and rise for
GN R after T=3, indicating that GR instances are serving lower num-
ber of requests than their pre-restart state and the GN R instances are
serving a higher proportion. The contrasting behavior for the two
groups arise due to CPU being a function of RPS i-e an instance serv-
ing less number of requests requires lower CPU cycles. Since Katran
uniformly load-balances across Proxygen fleet and the newly-spun,
updated instance has no request backlog, it gets the same share of re-
quests as others — leading to the drop and ramp-up in RPS over time.
For MQTT connections, we observe their number to fall across
GR instances and gradually rise for GN R. This behavior is expected
as the MQTT connections get migrated to other healthy instances
(GN R) through DCR. However, we do not observe their number
to drop to zero for GR at end of draining as the updated, parallel
Proxygen picks up new MQTT connections during this duration.
Timeline for disruption metrics: Figure 14 builds a similar
timeline for disruption metrics – TCP resets (RST), HTTP errors
(500 codes) and Proxy errors, presenting their count. Each point is
the average value observed for a cluster and the box plot plots the
distribution across the clusters. The timeline is divided into the four
phases, similar to Figure 13. We observe that the disruption met-
rics stay consistent throughout the restart duration. Even for a 20%
restart, we do not observe any increase in these disruption metrics —
highlighting the effectiveness of Zero Downtime Release in shielding
disruptions. No change in TCP RSTs highlights the efficacy of Zero
Downtime Release for preventing TCP SYN related inconsistencies,
observedfor SO_REUSEPORT basedsockettakeovertechniques[38].
6.2.2 Ability to release at peak-hours. Traffic load at a cluster
changes throughout the day (exhibiting di-urinal pattern [44]). The
traditional way is to release updates during off-peak hours so that
the load and possible disruptions are low. Figure 15 plots the PDF of
Proxygen and App. Server restarts over the 24 hours of the day. Prox-
ygen updates are mostly released during peak-hours (12pm-5pm).
Whereas, the higher frequency of updates for App. Server (Figure 2a)
leads to a continuous cycle of updates for the App. Server — a fraction
of App. Server are always restarting throughout the day as seen by
the flat PDF in Figure 15. From an operational perspective, operators
are expected to be hands-on during the peak-hours and the ability to
release during these hours go a long way as developers can swiftly
investigate and solve any problems due to a faulty release.
0123456789101112131415161718192021222324252612CPU Util.Restarted InstanceNon-Restarted instancesCluster-wide average012345678910111213141516171819202122232425260.00.51.01.5RPS01234567891011121314151617181920212223242526Timeline [minutes]0.51.01.5# MQTT conn.1234567891011121314151617181920212223242526Timeline [minutes]102100102CountHTTP 5XXProxy ErrorsTCP RSTsSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Naseer et al.
Figure (15) Update release time
Figure (16) Time required to push release
Figure (17) System benchmarks
6.3 System Overheads
Micro-benchmarks:Improvingclusteravailabilityandclient’sper-
formance during a restart can come at the cost of increased system
resource usage. Figure 17 plots the system resource usage during the
restart phase for machines in a randomly chosen production edge
cluster. Since the CPU consumption is variable at different phases of
the restart (increases at first and then returns to normal state as seen
in timeline figure 13), we plot system benchmarks during the entire
restartand presentthe median numbersobservedacross thedifferent
machines in a randomly selected edge cluster. The presence of two
concurrent Proxygen instances contributes to the costs in system
resources (increased CPU and Memory usage, decreased through-
put). The change in throughput correlates with CPU usage (inverse
proportionally), and the tail throughput decreases is caused by the
initial spike in CPU usage. Although the tail resource usage can be
high (persisting for around 60-70 seconds), the median is below 5%
for CPU and RAM usage i-e the increased resource usage does not
persistent for the whole draining duration (§ 6.2). As the machine
is still available and able to serve connections, this overhead is a
small price to pay for minimizing disruptions and keeping the overall
cluster close to its baseline capacity (i.e., non-restart scenario).
7 RELATED WORK
Release engineering: [24] is critical to important and performance
for large-scale systems and has been a topic of discussion among
industry [8, 20, 21], and researchers as well [9]. Different companies
handle the release process in their own way, to suit the needs of their
services and products [9, 20]. With the increased focus on improving
performance and availability of global-scale systems, release engi-
neering has become a first class citizen [20] at present and this paper
moves forward the discussion around the best practices for update
releases at a global scale.
Load-balancing at scale: The paper builds on the recent ad-
vancements in improving network infrastructure (Espresso [58],
FBOSS [19], Katran [7] etc) and the use of L7LB [2, 3, 48, 53, 56] for
managing traffic, to proposes novel ideas to improve the stability
and performance by leveraging the L7LB, in order to mask any pos-
sible restart-related disruptions from end-users and improve the
robustness of protocols that do not natively support graceful shut-
downs. Note that, some aspects of the papers have been discussed in
earlier works [34, 46] from Facebook. However, this paper tackles a
wider-range of mechanisms, evaluate them at production-scale and
describes the experiences of using these mechanisms.
Managing restarts and maintenance at scale: Recent work
hasfocusedonmanagingfailuresandmanagement-relatedrestart/updates
for various components of infrastructure, ranging from hardware
repairs [57] to network and switch upgrades [10, 32] in data-centers.
Our focus is mostly on software failures and graceful handling of
restarts, to allow faster deployment cycles and have zero disruptions.
While these works focus on mostly data-center scenarios, we tackle
the entire end-to-end infrastructure at a global scale.
Disruptionavoidance: Recently,afewproxieshavebeenarmed
with disruption avoidance tools to mitigate connection terminations
during restarts [36, 43]. HAProxy proposed Seamless Reloads [35, 55]
in2017andsocketFDtransfermechanism(similarto Socket Takeover)
was added in 2nd half 2018 [4]. Envoy recently added Hot Restart [16]
that uses a similar motivation. Our mechanisms are more holistic as
they support disruption-free restarts for protocols other than TCP
(e.g.,UDP)andprovideanend-to-endsupporttomitigatedisruptions,
e.g., Partial Post Replay for HTTP and Downstream Connection Reuse
for MQTT. Additionally, this work provides a first-time, hands-on
view of the deployment of these techniques on a global scale.
8 CONCLUSION
Owing to high code volatility, CSPs release upto tens of updates
daily to their millions of globally-distributed servers and the fre-
quent restarts can degrade cluster capacity and are disruptive to user
experience. Leveraging the end-to-end control over a CSP’s infras-
tructure, the paper introduces Zero Downtime Release, a framework
to enable capacity preservation and disruption-free releases, by sig-
naling and orchestrating connection hand-over during restart (to a
parallel process or upstream component). The framework enhances
pre-existing kernel-based mechanisms to fit diverse protocols and in-
troducesnovelenhancementsonimplementationandprotocolfronts
to allow fast, zero-downtime update cycles (globally-distributed fleet
restarted in 25 minutes), while shielding millions of end-users from
disruptions.
9 ACKNOWLEDGMENTS
Many people in the Proxygen and Protocols teams at Facebook have
contributed to Zero Downtime Release over the years. In particular,
we would like to acknowledge Subodh Iyengar and Woo Xie for their
significant contributions to the success of Zero Downtime Release. We
also thank the anonymous reviewers for their invaluable comments.
This work is supported by NSF grant CNS-1814285.
12am2am4am6am8am10am12pm2pm4pm6pm8pm10pmHour of the day0.000.050.100.150.20PDF of restartsL7LBAppServer020406080100120140160180200Duration to completerelease [minutes]0.00.20.40.60.81.0CDFL7LBAppServer051015202530% change020406080100CDFCPU usage (increase)RAM usage (increase)Throughput (decrease)Zero Downtime Release
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
[38] Joseph Lynch. True Zero Downtime HAProxy Reloads.
https://bit.ly/31H2dWz.
[39] Linux man page. cmsg(3) - access ancillary data.
https://linux.die.net/man/3/cmsg.
[40] Linux man page.
dup, dup2, dup3 - duplicate a file descriptor.
https://linux.die.net/man/2/dup.
[41] Linux man page. recvmsg(2). https://linux.die.net/man/2/recvmsg.
[42] Linux man page. sendmsg(2). https://linux.die.net/man/2/sendmsg.
[43] Suresh Mathew.
Instant Deployment and Rollback.
Zero Downtime,
https://bit.ly/2ZgNGzV.
[44] Arun Moorthy. 2015. Connecting the World: A look inside FacebookâĂŹs
Networking Infrastructure. https://unc.live/2UzVe0f.
[45] mqtt.org. MQ Telemetry Transport, machine-to-machine (M2M) connectivity
protocol. http://mqtt.org/.
[46] Kyle Nekritz and Subodh Iyengar. Building Zero protocol for fast, secure mobile
connections. https://bit.ly/2VkkoiH.
[47] Sam Newman. 2015. Building microservices: designing fine-grained systems. "
O’Reilly Media, Inc.".
[48] NGINX. NGINX Reverse Proxy. https://bit.ly/39fkWLt.
[49] Inc OâĂŹReilly Media. 2012. FacebookâĂŹs Large Scale Monitoring System Built
on HBase. https://bit.ly/2tAHlnc.
[50] Chuck Rossi. 2017. Rapid release at massive scale. https://bit.ly/2w0T9jB.
[51] Tony Savor, Mitchell Douglas, Michael Gentili, Laurie Williams, Kent Beck, and
Michael Stumm. 2016. Continuous deployment at Facebook and OANDA. In
2016 IEEE/ACM 38th International Conference on Software Engineering Companion
(ICSE-C). IEEE, 21–30.
[52] Amazon Web Services. Configure Connection Draining for Your Classic Load
Balancer. https://amzn.to/39iQlMS.
[53] Daniel Sommermann and Alan Frindell. 2014. Introducing Proxygen, Facebook
[54] Facebook Open Source. Hack - Programming Productivity Without Breaking
HTTP framework.
Things. https://hacklang.org/.
https://bit.ly/31Ihfvm.
[55] Willy Tarreau. 2017. Truly Seamless Reloads with HAProxy âĂŞ No More Hacks!
[56] VDMS. Our software - CDN. https://bit.ly/2UC0kZI.
[57] Kashi Venkatesh Vishwanath and Nachiappan Nagappan. 2010. Characterizing
cloud computing hardware reliability. In Proceedings of the 1st ACM symposium
on Cloud computing. ACM, 193–204.
[58] Kok-Kiong Yap, Murtaza Motiwala, Jeremy Rahe, Steve Padgett, Matthew
Holliman, Gary Baldus, Marcus Hines, Taeeun Kim, Ashok Narayanan, Ankur
Jain, et al. 2017. Taking the edge off with espresso: Scale, reliability and
programmability for global internet peering. In Proceedings of the Conference of
the ACM Special Interest Group on Data Communication. ACM, 432–445.
REFERENCES
[1] Django. https://www.djangoproject.com/.
[2] Envoy Proxy. https://www.envoyproxy.io/.
[3] HAProxy. http://www.haproxy.org/.
[4] HAProxy source code. https://github.com/haproxy/haproxy.
[5] HHVM. https://github.com/facebook/hhvm.
[6] Hypertext Transfer Protocol (HTTP) Status Code Registry.
https://bit.ly/3gqRrtP.
[7] Katran - A high performance layer 4 load balancer. https://bit.ly/38ktXD7.
[8] Bram Adams, Stephany Bellomo, Christian Bird, Tamara Marshall-Keim, Foutse
Khomh, and Kim Moir. 2015. The practice and future of release engineering: A
roundtable with three release engineers. IEEE Software 32, 2 (2015), 42–49.
[9] Bram Adams and Shane McIntosh. 2016. Modern release engineering in a
nutshell–why researchers should care. In 2016 IEEE 23rd international conference
on software analysis, evolution, and reengineering (SANER), Vol. 5. IEEE, 78–90.
[10] Omid Alipourfard, Chris Harshaw, Amin Vahdat, and Minlan Yu. 2019. Risk based
Planning of Network Changes in Evolving Data Centers. (2019).
[11] Sid Anand. 2011. Keeping Movies Running Amid Thunderstorms Fault-tolerant
Systems @ Netflix. QCon SF. https://bit.ly/37ahP65
[12] Oracle Corporation and/or its affiliates. Priming Caches.
https://bit.ly/2OxnPzi.
[13] AppSignal. 2018. Hot Code Reloading in Elixir. https://bit.ly/2H1k8hh.
[14] Envoy Project Authors. Command line options, drain-time-s.
https://bit.ly/38f3RRW.
https://bit.ly/2Sa7Fy5.
[15] Envoy Project Authors. Command line options, parent-shutdown-time-s.
[16] Envoy Project Authors. Hot restart. https://bit.ly/2H3Kwan.
[17] The Kubernetes Authors. Safely Drain a Node while Respecting the PodDisrup-
tionBudget. https://bit.ly/2SpYgkZ.
[18] Netflix Technology Blog. 2018. Performance Under Load.
https://bit.ly/2OCQbYU.
[19] Sean Choi, Boris Burkov, Alex Eckert, Tian Fang, Saman Kazemkhani, Rob
Sherwood, Ying Zhang, and Hongyi Zeng. 2018. Fboss: building switch software
at scale. In Proceedings of the 2018 Conference of the ACM Special Interest Group
on Data Communication. ACM, 342–356.
[20] SE Daily.
Facebook Release Engineering with Chuck Rossi - Transcript.
https://bit.ly/2H8Xwew.
https://bit.ly/3bfGaL7.
[21] SE Daily. 2019.
Facebook Release Engineering with Chuck Rossi.
[22] Willem de Bruijn.
udp: with udp segment release on error path.
http://patchwork.ozlabs.org/patch/1025322/.
[23] Willem de Bruijn and Eric Dumazet. 2018. Optimizing UDP for content delivery:
GSO, pacing and zerocopy. In Linux Plumbers Conference.
[24] Andrej Dyck, Ralf Penners, and Horst Lichter. 2015. Towards definitions for
release engineering and DevOps. In 2015 IEEE/ACM 3rd International Workshop
on Release Engineering. IEEE, 3–3.
[25] Alex Eagle. 2017. You too can love the MonoRepo. https://bit.ly/2H40EbF.
[26] Daniel E Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov,
Eric Mann-Hielscher, Ardas Cilingiroglu, Bin Cheyney, Wentao Shang, and
Jinnah Dylan Hosein. 2016. Maglev: A fast and reliable software network
load balancer. In 13th {USENIX} Symposium on Networked Systems Design and
Implementation ({NSDI} 16). 523–535.
[27] Alan Frindell. HTTP Partial POST Replay. https://tools.ietf.org/html/
draft-frindell-httpbis-partial-post-replay-00.
[28] GlobalScape. Server Drain, Maintenance, and Auto-Restart.
https://bit.ly/31B7A9A.
[29] Sara Golemon. 2012. Go Faster. https://bit.ly/2Hg1Ed7.
[30] Christian Hopps et al. 2000. Analysis of an equal-cost multi-path algorithm.
Technical Report. RFC 2992, November.
[31] Jez Humble. 2018. Continuous Delivery Sounds Great, but Will It Work Here?
Commun. ACM 61, 4 (March 2018), 34–39.
[32] Michael Isard. 2007. Autopilot: automatic data center management. ACM SIGOPS
Operating Systems Review 41, 2 (2007), 60–67.
[33] JanaIyengarandMartinThomson.2018. Quic:Audp-basedmultiplexedandsecure
transport. Internet Engineering Task Force, Internet-Draft draftietf-quic-transport-17
(2018).
[34] Subodh Iyengar. 2018. Moving Fast at Scale: Experience Deploying IETF QUIC
at Facebook. In Proceedings of the Workshop on the Evolution, Performance,
and Interoperability of QUIC (Heraklion, Greece) (EPIQâĂŹ18). Association for
Computing Machinery, New York, NY, USA, Keynote.
[35] Lawrence Matthews Joseph Lynch. Taking Zero-Downtime Load Balancing even
Further. https://bit.ly/2OA66XY.
[36] Michael Kerrisk. The SO_REUSEPORT socket option.
https://lwn.net/Articles/542629/.
[37] Ran Leibman. Monitoring at Facebook - Ran Leibman, Facebook - DevOpsDays
Tel Aviv 2015. https://bit.ly/2ODbgT1.