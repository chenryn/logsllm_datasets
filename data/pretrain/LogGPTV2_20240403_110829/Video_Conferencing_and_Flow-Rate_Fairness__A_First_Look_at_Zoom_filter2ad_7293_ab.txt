AQM
CC
Order
Direction
0.5, 1, 2, 4
30, 50
0.5, 2, 10
Tail-Drop CUBIC Zoom ﬁrst Downlink
(FQ )CoDel BBRv1 TCP ﬁrst Uplink
The lowest bandwidth (0.5 Mbps) falls slightly below Zoom’s requirements of
0.6 Mbps [2]. Yet, we argue that it also has to behave sound in out-of-spec cases.
We shape the bandwidth symmetrically, which is atypical for a residential
connection, but study the up- and downlink separately. We also adjust and
balance the minimum RTT (min-RTT) symmetrically as described before. As
queue sizes, we use multiples of the BDP, i.e., 0.5, 2, and 10× the BDP. When
investigating AQM, we use 2×BDP as AQM algorithms require headroom to
operate, and adopt the TC Linux defaults for CoDel (target 5 ms and interval
100 m s). Further, we vary which ﬂow starts ﬁrst to investigate late-comer eﬀects.
Overcoming Transient States. For our measurements, we want to avoid tran-
sient phases. As such, we usually wait in the order of half a minute after acti-
vating each ﬂow to stabilize. We then start a 60 s measurement period in which
we capture all exchanged packets, measure the queuing delay, and also observe
the queue sizes at the bottleneck using a small eBPF program.
Video Conference. The Zoom video conference itself is established between
ZC 2 and ZC 1 (ensuring connectivity via AWS in Frankfurt). As their video
feeds, both clients simulate a webcam via v4l2loopback [3]. To rule out eﬀects
of video compression on the congestion control behavior of Zoom, we ensure a
constant video data rate by using uniform noise as our video input.
Every scenario is repeated 30 times and retried where, e.g., Zoom restarts
due to high loss. The measurements were made from July 2020 to October 2020
on Linux 5.4.0-31 with Zoom version 5.0.408598.0517. To observe variations,
we sort the points in the following scatterplots chronologically from left to right.
Equality Metric. We measure ﬂow-rate equality using the metric of our prior
work [25]. In contrast to, e.g., Jain’s fairness index [19], this metric shows which
ﬂow over-utilizes the bottleneck by how much. The metric is deﬁned as:
(cid:2)
1 − bytes(T CP )
,
−1 + bytes(Zoom)
bytes(Zoom)
bytes(T CP )
if bytes(Zoom) ≥ bytes(T CP )
otherwise
,
ﬂow-rate equality =
ﬂow-rate equality lies in the interval of [−1, 1]. With 0 both ﬂows share the band-
width equally, while 1/−1 means that Zoom/TCP monopolizes the link.
Please note that ﬂow-rate equality is likely not the desired metric to depict
a fair service enablement. For example, Zoom simply needs a certain data-rate
to deliver its service, as such ﬂow-rate equality should likely not be used to
establish fairness, e.g., in an AQM. Nevertheless, we still opted for this metric
to i) judge what happens when an AQM tries to utilize this metric, and ii)
investigate the bandwidth demand and the ability of the congestion controller
to seize the required bandwidth as well as the side eﬀects in doing so.
A First Look at Zoom and the Impact of Flow-Queuing AQM
9
Fig. 2. Zoom video ﬂow behavior for a 50 ms RTT and a 10×BDP tail-drop queue.
Bandwidth (dashed) is varied from 4 Mbps to 0.5 Mbps and back to 4 Mbps.
4 Zoom Inter-Protocol Fairness Results
In the following we present our ﬁndings on the behavior of Zoom by ﬁrst ana-
lyzing its general congestion reaction (Sect. 4.1). We then discuss how ZC 1
competes with a TCP ﬂow in scenarios without AQM at low bandwidths sub-
ject to diﬀerent queue sizes (Sect. 4.2). We further evaluate the eﬀects of using
CoDel (Sect. 4.3) and FQ CoDel (Sect. 4.4) AQM. Lastly, we show results of a
small-scale user study that investigates the eﬀects of FQ CoDel on the actual
QoE, which can be found in the appendix to this work (Appendix A).
Before conducting our experiments, we ﬁrst verify the standalone throughput
of TCP and Zoom in our scenarios. We ﬁnd that TCP achieves a utilization above
80% in almost all cases except for 3 outliers out of 4800 runs. Similarly, Zoom’s
throughput for the AQM scenarios only changes by at most 10%. The following
diﬀerences in ﬂow-rate equality are thus mainly due to the interaction of the
congestion control algorithms and not rooted in our settings.
4.1 General Observations on Zoom’s Behavior
We ﬁrst observe the behavior of a single Zoom ﬂow without competition in
a scenario with a 50 ms RTT and a 10×BDP tail-drop queue. Figure 2 shows
Zoom’s video send rate when varying the bandwidth (dashed) from 4 Mbps to
0.5 Mbps and back. At ﬁrst, Zoom’s backend (left) sends at slightly less than
4 Mbps while the Zoom client (right) sends at ∼2.5 Mbps. In both cases, the
queue is empty. Similar to BBR [10], Zoom seems to repeatedly probe i) the
bandwidth by increasing its rate and ii) the min-RTT by reducing its rate.
Once we reduce the bandwidth to 0.5 Mbps, both Zoom entities keep sending
at ∼3.5 Mbps, thus losing many packets and ﬁlling the queue. After ∼30 s, Zoom
reduces its rate to 0.5 Mbps. Surprisingly, the backend again increases the rate
by a factor of 4 shortly thereafter. After resetting the bandwidth to 4 Mbps,
Zoom slowly increases its rate on the uplink and faster on the downlink.
Packet loss and increased queuing delays do not seem to directly inﬂuence
Zoom’s sending behavior. However, Zoom occasionally restarted the video con-
ference completely, stopping sending and reconnecting to the backend with a
new bandwidth estimate not overshooting the bottleneck link. We ﬁltered these
occurrences from the following results as the time of reconnecting would inﬂuence
our metric and also the meaning of our “Zoom ﬁrst” scenario.
10
C. Sander et al.
Fig. 3. Flow-rate equality for Zoom competing at a 0.5 × BDP queue with TCP.
We also changed the min-RTT from 50 ms to 500 ms instead of the bandwidth.
We did not see any obvious reaction, although we expected that Zoom backs oﬀ
to wait for now delayed signaling information or to reduce potential queuing.
To summarize, Zoom handles up- and downlink diﬀerently and does not seem
to directly react on increased queuing or loss, instead reacting slowly which leads
to big spikes of loss. We next investigate how this impacts competing ﬂows.
4.2 Competition at Tail-Drop Queues
Undersized Tail-Drop Queue. We ﬁrst examine Zoom’s behavior when com-
peting at a 0.5×BDP tail-drop queue against TCP CUBIC and BBR. The scat-
terplots in Fig. 3 show our ﬂow-rate equality for downlink (a) and uplink (b).
Downlink. Zoom uses a disproportionate bandwidth share on the downlink
with bottleneck bandwidths ≤ 1 Mbps. The ﬂow-rate equality is mostly above
0.5, i.e., Zoom’s rate is more than twice the rate of the TCP ﬂow. For higher
bandwidths, Zoom yields more bandwidth. Additionally, we can see that TCP
ﬂows starting ﬁrst result in slightly better ﬂow-rate equality. For CUBIC, equal-
ity values of around 0 can be ﬁrst seen at 4 Mbps. For BBR, equality values of
around 0 can already be seen at 2 Mbps. However, when being started ﬁrst and
at 4 Mbps, BBR disadvantages Zoom signiﬁcantly.
Uplink. For the uplink, the equality values are comparable, but in total
lower. This means that the TCP ﬂows claim more bandwidth (especially with
A First Look at Zoom and the Impact of Flow-Queuing AQM
11
Fig. 4. Flow-rate equality for Zoom competing at a 10 × BDP queue with TCP.
BBR) and Zoom seems to act less aggressive. We posit that Zoom’s congestion
control might be adapted to the asymmetric nature of residential access links.
The queuing delays on the down- and uplink mostly exceed 50% of the max-
imum (not shown). We attribute this to the TCP ﬂows as i) CUBIC always
ﬁlls queues, and ii) BBR overestimates the available bandwidth when competing
with other ﬂows [28] and then also ﬁlls the queue plus iii) Zoom reacting slowly.
Slightly Oversized Tail-Drop Queues. When increasing the buﬀer size to
2×BDP, the results are surprisingly similar (and thus not visualized). CUBIC
can gather a slightly larger bandwidth share, which we attribute to its queue-
ﬁlling behavior. However, Zoom still holds twice the bandwidth of the TCP ﬂows
at links with ≤1 Mbps, i.e. the equality values mostly exceed 0.5. Only on faster
links, CUBIC can gain an equal or higher bandwidth share. For BBR, equality
values are closer to 0 for bandwidths below 2 Mbps, i.e., Zoom as well as BBR
dominate less. For higher bandwidths, the results are equivalent to before. Also
the avg. queuing delay rises to about 75% due to ﬁlled queues as before.
Overlarge Tail-Drop Queues. Next, we study the ﬂow-rates for large queues
of 10×BDP. Figure 4 shows the results for downlink (a) and uplink (b).
Downlink. Contrary to our expectation, there is no signiﬁcant improvement
in ﬂow-rate equality for the downlink. Zoom still uses a high bandwidth share and
CUBIC’s queue-ﬁlling behavior does not result in a larger share. Compared to
the previous scenarios, the equality values are not decreasing signiﬁcantly when
Zoom starts ﬁrst and it even uses more bandwidth than before for the 4 Mbps
12
C. Sander et al.
Fig. 5. Queuing delay for Zoom competing at a 10 × BDP queue on the uplink.
Fig. 6. Queuing delay for Zoom+CUBIC competing at a tail-drop/CoDel queue.
setting. For TCP CUBIC starting ﬁrst, equality values now spread around 0.5,
regardless of the bandwidth. For Zoom starting ﬁrst, BBR barely reaches values
below zero.
Uplink. The scenario looks completely diﬀerent for the uplink. Zoom yields
bigger parts of the bandwidth to CUBIC and even reduces on one third of
the bandwidth when BBR starts ﬁrst. This is surprising, as BBR is known to
be disadvantaged in this overbuﬀered scenario [18]. We also checked if changes
between the BBR code used in [18] and our Linux Kernel 5.4 could explain this
diﬀerence, but the basic principle seems to be unaltered. Still, we remark that
the BBR codebase has seen signiﬁcant changes since [18] and we are not aware
of any investigations how these changes aﬀect BBR’s properties.
The queuing delay, shown in Fig. 5 for the uplink, still reaches about 75% of
the maximum queuing delay for CUBIC and BBR in low-bandwidth scenarios
where delay is slightly smaller on the uplink than on the downlink. BBR seems to
be able to reduce queuing delay in the higher bandwidth region, but we expected
that BBR would reduce the queuing delay more strongly in all scenarios.
Takeaway. We can see that Zoom is unfair w.r.t. ﬂow-rate to CUBIC in
low-bandwidth scenarios with 1.0 Mbps and less, although Zoom is less aggressive
on the uplink. As BBR is more aggressive, it gains higher rates in these situa-
tions – also on the downlink. However, all scenarios have in common that the
queuing delay is signiﬁcantly increased being detrimental to video conferencing.
A First Look at Zoom and the Impact of Flow-Queuing AQM
13
Fig. 7. Flow-rate equality for Zoom competing with TCP at an FQ CoDel queue
4.3 Competition at CoDel Queues
Using AQM might be beneﬁcial, given the increased queuing delays. Hence, we
study Zoom and TCP ﬂows competing at CoDel queues. We expect signiﬁcant
changes in ﬂow-rate equality as CoDel drops packets early to signal congestion.
Yet, our results are very similar to the 2×BDP tail-drop queue, thus we do
not show them here. They only slightly shift towards CUBIC. However, CoDel
keeps its promise of reduced queuing delays, as shown in Fig. 6: The queuing
delay of Zoom competing with CUBIC (BBR looks similar) at 2×BDP queues
roughly halves when CoDel is used at 0.5 Mbps. For higher bandwidths, the
eﬀect is even stronger. This is potentially beneﬁcial for real-time applications.
Takeaway. All in all, CoDel does not signiﬁcantly alter the ﬂow-rate distribu-
tion. However, it keeps its promise of reducing the experienced queuing delays.
4.4 Competition at FQ CoDel Queues
To enforce ﬂow-rate equality, we next apply FQ CoDel to the queue. FQ CoDel
adds stochastic fair-queueing to CoDel, i.e., it isolates ﬂows into subqueues,
applies CoDel individually, and then serves the queues in a fair manner.
While the queuing delays are equivalent to CoDel and thus not shown, our
ﬂow-rate equality metric signiﬁcantly shifts towards TCP in most conditions as
shown in Fig. 7 for uplink (a) and downlink (b). For example, the downlink results
14
C. Sander et al.
Table 2. Median number of packets received and dropped for CUBIC and Zoom at a
0.5 Mbps, 50 ms, 2×BDP bottleneck on the downlink (Zoom started ﬁrst).
Tail-Drop
CoDel
FQ CoDel
Dropped Received Dropped Received Dropped Received
TCP CUBIC
Zoom
188.0
331.0
816.5
2824.0
190.0
515.5
935.5
2852.5
250.5
903.5
1260.5
2880.5
mostly range from 0.3 to −0.3 compared to prior ﬁndings of Zoom dominating.
The biggest advance for Zoom remains in the 0.5 Mbps setting.
On the uplink, equality diﬀers. Zoom yields bandwidth when using BBR in
mostly all cases except for bandwidths ≤ 1.0 Mbps. For CUBIC, also no perfect
equalization can be seen. For bandwidths above 2.0 Mbps CUBIC gets bigger
shares, below this threshold, vice versa. We deduct this to Zoom being more
careful on the uplink and not using the whole probed bandwidth, leaving a gap.
Zoom’s Reaction to Targeted Throttling. As we could see, FQ CoDel
allows to share bandwidth between Zoom and competing TCP ﬂows after a
bottleneck more equally. However, it is unclear whether Zoom reduces its rate
or whether the AQM is persistently dropping packets, speciﬁcally in the low-
bandwidth scenarios. We hence show the dropped and sent packets for CUBIC
and Zoom over 60 s in Table 2 for the 0.5 Mbps bottleneck with 2×BDP queue
and 50 ms RTT. We can see that Zoom does not reduce its packet-rate from a
tail-drop queue up to FQ CoDel. Instead, the AQM drops packets increasingly.
Takeaway. In combination with ﬂow-queuing, CoDel can reduce the experienced
queuing delay, which is probably beneﬁcial for Zoom’s QoE, while equalizing the
bandwidth share with TCP. However, in low-bandwidth scenarios this share is
still not perfectly equal. Zoom does not reduce its rate but CoDel and FQ CoDel
increasingly drop Zoom’s packets which might aﬀect Zoom’s QoE negatively. A
preliminary user study shows that FQ CoDel does, indeed, not improve QoE and
can be found in the appendix.