KHLL can be used to keep the IDs separate, respecting
the choice of users to conduct certain activities in certain
contexts. For example, KHLL analysis can be run on two
data sets of different IDs, and be used to detect data ﬁelds
in the two data sets that are similar (high containment in
either direction) and that are highly unique. These data
ﬁelds are potential join keys that can be used to trivially
link the two ID spaces. To mitigate joinability risks,
engineers can choose to suppress or generalize one of the
ﬁelds, or use access controls to prevent someone from
using the ﬁelds to join the two identiﬁers. The analysis
can be run periodically and attached to an alerting system
that notiﬁes engineers if joinability exceeds pre-speciﬁed
limits (e.g., to quickly detect whether any newly added
ﬁelds increase joinability risks). Joinability assessment
is highly intractable with pairwise comparisons of raw
data, but KHLL enables joinability approximation based
on its compact data structures (sketches).
Periodic KHLL-based joinability analyses have en-
abled us to uncover multiple logging mistakes that we
were able to quickly resolve. One instance was the exact
position of the volume slider on a media player, which
was mistakenly stored as a 64-bit ﬂoating-point number.
Such a high entropy value would potentially increase
the joinability risk between signed-in and signed-out
identiﬁers. We were able to mitigate the risk by greatly
reducing the precision of the value we logged. In other
cases, we have mitigated joinability risks by dropping
certain ﬁelds entirely, or by ensuring that the access
control lists of both data sets are disjoint.
Miscellaneous: If data custodians label their data sets
with information about the semantics of certain ﬁelds,
KHLL can be used to propagate labels through the
system and ﬁnd inconsistencies. If two ﬁelds have a high
containment score (in either direction), they are likely to
share the same semantics. If one of the ﬁelds is labelled
but the other is not, then the label can be copied to the
second ﬁeld, and if the two ﬁelds have different labels
then engineers can be alerted that one of the labels is
likely incorrect. The scability of KHLL means that it
can be used to propagate labels accross large data sets,
and that the label correctness can be repeatedly checked
by re-running analysis periodically.
Although not a primary purpose, an additional side
effect of making a powerful analysis tool available to dif-
ferent roles in an organization is the increased awareness
of anonymization and user privacy. Data custodians, en-
gineers and analysts can discuss the analysis results with
each other, gain a better understanding of reidentiability
risks when working with user data, and understand why
further anonymization may be necessary.
For all of these use cases one needs to keep in mind the
estimation errors of KHLL (see Section VI-B and Sec-
tion VII-B). It is possible that KHLL may underestimate
reidentiﬁability or joinability risks (e.g., KHLL might
miss values that are unique to a single user). In general,
data custodians could use KHLL to estimate risks and
impacts on data utility when exploring an appropriate
data protection and anonymization strategy, but then use
exact counting to execute the strategy. While the join-
ability analysis using KHLL might be sensitive to data
formats and transformations, the efﬁciency of KHLL
makes it the best regression test for data joinability that
we are aware of.
IV. APPROXIMATE COUNTING BASICS
Approximate counting is a technique to efﬁciently
estimate the cardinality (number of distinct elements)
of a set [9]–[11], typically using a small amount of
memory. This technique can also be extended to com-
pute other statistics such as quantiles [12], [13] and
frequent values [14], [15]. Approximate counting algo-
rithms use compact data structures, colloquially known
as “sketches” that summarize certain observable proper-
ties of the elements in the analyzed data set.
In addition to being memory efﬁcient, approximate
counting sketches support additional operations such as
merging (set union). Large-scale data sets are typically
stored in multiple machines (“shards”), as the entire data
set would not ﬁt in a single machine. In such situations,
one can compute the cardinality of the entire data set
using a two-step approach:
1) Compute the sketches of individual data shards.
2) Merge all the sketches generated in step 1.
In this paper, we extend two approximate counting
algorithms named K Minimum Values (KMV) [9] and
HyperLogLog (HLL) [10] to build KHLL. We provide
some intuition about KMV and HLL in the following.
A. K Minimum Values (KMV)
As implied by the name, KMV estimates the cardi-
nality of a set by keeping the K smallest hash values of
its elements. The intuition behind KMV is as follows.
Suppose there exists a hash function that uniformly
maps input values to its hash space. Note that
this
hash function does not need to be a cryptographic
(cid:20)(cid:22)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
hash function, and one-wayness is not required (i.e., it
does not matter if the hash function can be reversed
in polynomial time). If one computes the hash of each
element in the analyzed data set, one can expect those
hashes to be evenly distributed across the hash space.
Then, one can estimate the cardinality of the analyzed
data set by computing the density of the hashes (i.e., the
average distance between any two consecutive hashes)
and dividing the hash space by the density. Since storing
all the hashes can incur a signiﬁcant storage cost, one
can store only the K smallest hash values and extrapolate
the density of the entire hash space.
As a concrete example, say there is a hash func-
tion whose outputs are evenly distributed in the range
[1, 1000000]. If K = 100, and the Kth smallest hash
value is 1000, we can compute the density by sim-
ply dividing the Kth smallest hash value by K, i.e.,
density = 1000/100 = 10. Extrapolating to the range of
[1, 1000000], with the uniformity assumption but without
bias correction, one can roughly estimate the number of
unique values as 1000000/10 = 100000.
Computing set union using KMV sketches is straight-
forward. Given two KMV sketches, S1 and S2, one can
ﬁnd the KMV sketch of the union of the two data sets
by combining the two sketches and retaining only the K
smallest hashes.
KMV sketches are efﬁcient to produce. It requires a
single pass over the data set, but only a space complexity
of O(K), as it consists of K unique hash values of ﬁxed
length. The cardinality estimated by a KMV sketch has
1√
K with the assumption that
a relative standard error of
the hash space is large enough to keep hash collisions to
a minimum. As a concrete example, with K = 1024 and
using a 64-bit uniformly distributed hashing function,
one can estimate the cardinality with a relative standard
error of 3% and KMV sketch size of 8 KB.
B. HyperLogLog (HLL)
Instead of keeping the K smallest hash values, HLL
further reduces the space requirement by tracking the
maximum number of trailing zeros of the hash values.
The maximum number of trailing zeros increases as more
unique values are added to HLL given the uniformity
assumption of the hash function.
From the hash of an incoming value, HLL uses the
ﬁrst P bits to determine the bucket number, and uses the
remaining bits to count the number of trailing zeros. HLL
keeps track of the maximum number of trailing zeros at
each of the M = 2P buckets. After processing all values
in the analyzed data set, HLL estimates the cardinality of
each bucket as 2mi, where mi is the maximum number
of trailing zeros seen in bucket i. Finally, HLL estimates
the cardinality of the analyzed data set by combining the
cardinalities of individual buckets by taking the harmonic
mean.
HLL sketches are also efﬁcient to compute (i.e., using
a single pass over the analyzed data set) and provide
cardinality estimates with a relative standard error of
1.04√
M . Moreover, the space complexity of a HLL sketch
is O(M ) since it consists of M counts of trailing zeros.
As a concrete example, with M=1024 and using a 64-bit
uniformly distributed hashing function, one can estimate
the cardinality with a relative standard error of 3% and
HLL sketch size of 768 B.
Heule et al. showed that HLL does not provide a good
estimate for low cardinalities and proposed HLL++ [11]
to accommodate such data sets. HLL++ maintains two
different modes of sketches. When the cardinality is low,
it remains in the sparse representation mode, which keeps
almost the entire hash values. When the list of hash
values kept grows, HLL++ switches to the conventional
HLL mode which has a ﬁxed memory footprint. The
sparse representation allows HLL++ to use linear count-
ing for estimating small cardinalities with negligible
error while also keeping the sketch size small.
V. KHYPERLOGLOG (KHLL)
is known,
While cardinality estimates are helpful, they are lim-
ited in many ways for reidentiﬁability and joinability
analysis. While cardinality estimates can be used to
estimate the average uniqueness when the total unique
IDs in the data set
they do not estimate
the uniqueness distribution. The average alone can be
misleading as uniqueness distribution is more likely to
be skewed in practice. The uniqueness distribution is
also useful to inform about various data strategies, for
example the feasibility of suppressing or generalizing
a fraction of the unique values. The distribution could
not be naively estimated as we could not assume the
data sets to be structured in a way that every single row
corresponds to a single user.
We present KHyperLogLog (KHLL) which builds on
KMV and HLL to estimate uniqueness distribution and
containment with a single pass over the data set and low
memory requirements.
This algorithm uses a two-level data structure for
analyzing tuples of ﬁeld and ID values. KHLL contains
K HLL sketches corresponding to K smallest hashes of
ﬁeld values. This is approximately equivalent to taking
a uniform random sampling of size K over the ﬁeld
(cid:20)(cid:22)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
values, each of which comes with a corresponding HLL
sketch containing the hashes of IDs associated with the
ﬁeld value. We considered an alternative two-level KMV-
based data structure, named K2MV, but concluded that
KHLL is more memory-efﬁcient and suitable for our
needs. See Appendix XIII for a description of K2MV.
Consider a stream of pairs (fi, idj) ∈ F × ID and a
hash function h. KHLL processes the incoming tuples
as follows:
1) Calculate h(fi) and h(idj).
2) If h(fi) is present in the KHLL sketch, add h(idj)
to the corresponding HLL sketch.
3) Else, if h(fi) is among the K smallest hashes:
a) If there are more than K entries, purge the
entry containing the largest hash of F .
b) Add a new entry containing h(fi) and a HLL
sketch with h(idj).
4) Else, do nothing.
As a speciﬁc example, consider a stream of User
Agent (UA) and ID value pairs. Further consider an 8-bit
hash function and a KHLL sketch of K = 3 and M = 8.
The KHLL sketch contains at most 3 entries representing
the 3 smallest values of h(UA) in the ﬁrst level, each
with a HLL sketch in the second level which has at
most 8 counting buckets. For example, when the KHLL
sketch processes the tuple (UA-4, ID-6) which hashes
to (00000011, 00011010) as shown in Figure 1,
the entry with with the largest h(UA) = 00011000
and its companion HLL sketch is purged to give way to
h(UA-4) and a new HLL.
The memory signature of a KHLL sketch depends
on the parameters K and M as well as the uniqueness
distribution of the data. Per innovation in HLL++ [11],
we design the HLL sketches in KHLL to start in the
sparse representation mode which keeps a sparse list of
the ID hash values. Once this representation exceeds the
ﬁxed size of a conventional HLL, it is converted to a
normal representation with M counting buckets. Using a
64-bit hash function, individual counting buckets require
less than a byte to count the maximum number of trailing
zeros in the ID hash values. Improving over HLL++, we
implemented HLL++ Half Byte which uses only half a
byte for individual counting buckets (see Appendix XII).
Let ID[fi] = {idj : (fi, idj) ∈ D} be the set of user
IDs associated with a given ﬁeld value fi in data set
D. The memory needed for a KHLL sketch consid-
ering both the sparse and conventional mode is thus
min(8|ID[fi]|, M ) in bytes. Since the KMV approx-
imates a K size uniform random sample over ﬁeld
i
min(8|ID[fi]|, M ).
values, the expected memory usage for the entire KHLL
will be roughly K times the average HLL size i.e.,
K|F| · (cid:2)
This means that the memory usage of KHLL, while
never above a strict upper bound, will be higher for data
sets with low uniqueness in which most ﬁeld values are
associated with large user ID sets. Alternatively, when
most ﬁeld values correspond to only a few unique user
IDs, the memory signature will be much smaller as the
HLL sketches will be in sparse representations.
Note that KHLL does not dictate how the data is
structured. To process a table of data, we simply read
each row in the table to (1) extract the ID value and the
values of ﬁelds (or combinations of ﬁelds) of interest,
and (2) ingest the tuples of ID and ﬁeld values into
the corresponding KHLL sketches. This allows for tables
that contain arbitrarily large number of ﬁelds, and even
for tables where data about the same user can be repeated
across multiple table rows.
VI. ESTIMATING REIDENTIFIABILITY USING KHLL
From a KHLL sketch of (F, ID), one can estimate
both the cardinality of the ﬁeld and the number of
unique IDs associated with individual ﬁeld values i.e.,
the uniqueness distribution. The latter allows us to ef-
ﬁciently estimate the proportion of ﬁeld values that are
reidentifying as well as statistics such as the min, max,
and median uniqueness.
A. Evaluating Data Loss and Reidentiﬁability Trade Off
One can plot the uniqueness distribution to visualize
the percentage of ﬁeld values or IDs meeting some k-
anonymity thresholds. Figure 2 is an example histogram
of how many ﬁeld values are associated with each
number of unique IDs. Tweaked slightly, Figure 3 plots
the cumulative percentage of values not meeting varying
k-anonymity thresholds. A relatively anonymous data set
exhibits the curve on the left as most of the ﬁeld values
are expected to be associated with a large number of IDs.
Conversely, a highly reidentifying data set will exhibit
the curve on the right.
The cumulative distribution is particularly useful as it
estimates how much data will be lost when applying a
k-anonymity threshold. This allows one to determine a
threshold that preserves some data utility while ensuring
a reasonable privacy protection, especially when other
risk mitigation measures are in place such as access
control, audit trails, limited data lifetime or noising of
released statistics. We can see that for the left curve, one
can choose a high threshold with low data loss, while
(cid:20)(cid:22)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 1: A stream of User Agent (UA) and ID tuples processed by an example KHLL sketches with K = 3 and
M = 8. When the tuple (UA-4, ID-6) is added, the entry with the largest h(UA) = 00011000 and its companion
HLL sketch is purged to give way to h(UA-4) and a new HLL. Notice that the sketches HLL1 and HLL4 are in
sparse representation, while HLL2 is in the conventional table form.
Fig. 2: Example uniqueness histogram. We expect User
Agent (UA) to have a uniqueness distribution where a
majority of UA strings are associated with only one or
a few unique IDs.
Fig. 3: Two possible shapes for cumulative uniqueness
distributions. The left has low uniqueness, while the
right contains values that are highly unique.
on the right even a moderate threshold will result in
dropping a large portion of the data set.
Given the efﬁciency of KHLL analysis, one could
set up periodic analyses to assure that the uniqueness
distribution does not change over time, to monitor, for
example, that no more than X% of values should have
less than k-anonymity.
In addition to analyzing the uniqueness distribution