to change. The adversary can choose the poisoning points in
a way that the change of prediction is noticeably different
for certain points between the cases when the target property
occurs with frequency t0 vs t1. In the rest of section, we will
show how adversary selects the poisoning points. Then we will
see that the behavior of the resulting model should be different
on points that satisfy the conditions in Theorem 9, depending
on the distribution used during the training. Finally, we argue
that, by querying these points, the adversary can distinguish
between the case of t0 and t1.
Let the original data distribution of clean samples be D ≡
(X, Y ). Our adversary A will pick the poison data by i.i.d.
sampling from a distribution DA ≡ (XA, YA). Note that this
adversary is weak in a sense that it does not control the poison
set but only controls the distribution from which the poison set
is sampled. The resulting training dataset will be equivalent to
one sampled from a distribution ˜D such that ˜D is a weighted
mixture of D and DA. More precisely,
˜D ≡
(cid:26) D, With probability (1 − p)
DA With probability p
[Case I: No poisoning],
[Case II: Poisoning]
Pr
S← ˜Dn
˜h←L(S)
Pr
x←X|Cτ (x)=1
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
61125
Now we describe the distribution DA. To sample poisoning
points, adversary ﬁrst samples a point from X conditioned
on having the property f. For the label, the adversary always
chooses label 1. So we have DA ≡ (X+, 1). We will see how
this simple poisoning strategy can help the adversary to infer
the property.
B. Evaluating the attack
In this section, we evaluate the effect of the poisoning
strategy above on the Classiﬁers. We describe the evaluation
steps here and defer proofs to the appendix.
Effect of poisoning on the distribution
Let ( ˜X, ˜Y ) be the joint distribution of samples from ˜D.
First we calculate Pr[ ˜Y = 1 | ˜X = x] to see the effect of
poisoning on the distribution. Let E be the event that the
point is selected by adversary, namely, the second case in
the description of ˜D happens. Let t = Pr[f (X) = 1], we
prove the following claim.
Claim 11. For any x ∈ X such that f (x) = 1 we have
Pr[ ˜Y = 1 | ˜X = x] =
p
p + t(1 − p)
t(1 − p)
p + t(1 − p)
+
· Pr[Y = 1 | X = x].
As a corollary to this claim, we prove that
Corollary 12. For any x such that f (x) = 1 and for any
τ ∈ R we have Pr[ ˜Y = 1 | ˜X = x] ≥ 1
p+t(1−p) if
and only if crt(x) ≤ p−2τ·t
t(1−p)
Claim 11 and Corollary 12 show how the adversary can
change the distribution. We now want to see the effect of
this change on the behavior of the Bayes optimal classiﬁer.
2 +
τ·t
Effect on the Bayes Optimal Classiﬁer
Consider the algorithm L that is (ε, δ)-Bayes optimal
on all linear combinations of distributions D+, D− and
DA (as stated in Theorem 9). Therefore, on a dataset
˜T ← ˜Dn, the algorithm L will output a model ˜h = L( ˜T )
that with probability at least 1 − δ(n) has error at most
Bayes( ˜D) + ε(n). Consider joint distribution ( ˜X, ˜Y ) such
that ˜D ≡ ( ˜X, ˜Y ). The following claim shows how the
adversary can exploit the dependence of the probabilities
on t and infer between t0 and t1 by using special points
that have high uncertainty.
Claim 13. Let L be a (, δ)-Bayes optimal learning
algorithm for ˜D ≡ ( ˜X, ˜Y ). Consider an event Cτ deﬁned
on all x ∈ X such that Cτ (x) = 1 iff f (x) = 1 and
p + 2τ · t1
t1(1 − p)
If there exist a τ ∈ [0, 1] and γ ∈ [0, 1
Pr[Cτ (X) = 1] ≥ ε(n)
τ·(1−2γ)
then if t = Pr[f (X) = 1] = t0 we have
 0.5] ≤ δ(n).
Therefore, the adversary only checks if ρ > 0.5 and based
on that decides if t = t0 or t = t1. The probability of
this test failing is at most 2δ(n) as there is at most δ(n)
probability of the learning algorithm failing in producing
a good classiﬁer and δ(n) probability of failure because
of Chernoff. Hence, with probability at least (1 − 2δ(n))
the adversary can infer whether t = t0 or t = t1.
VI. A CONCRETE ATTACK
Here we describe the concrete attack we use in our experi-
ments. We note that there are many possible variations on this
attack; what we have presented here is just one conﬁguration
that demonstrates that the attack is feasible with high accuracy.
Selecting Poisoning Points Recall that the attacker is assumed
to be able to sample from D− and D+, the distribution of
items with f (x) = 0 and with f (x) = 1.
As in the theoretical attack described in Section V, the
poisoned data is generated by sampling from D+ ≡ (X+, Y+)
and introducing correlation between the target feature f (x)
and the label by injecting poisoning points of form (X+, 1).
Note that the attack described in the Section VI could be
achieved in 4 different forms. Namely, the adversary can use
any of the possible combinations (X−, 1), (X−, 0), (X+, 0)
and (X−, 1) for poison data. In algorithm 3 we show how we
choose between these strategies. In particular, when values of
t0 and t1 are large, we try to attack 1−f instead of f. The logic
behind this choice is that it is easier to impose a correlation
between the property and the label, when the property is rare
in the distribution. As an example, consider a spam detection
scenario where the adversary wants to impose the following
rule on the spam detection: all Emails that contain the word
"security" must be considered spam. It would be much easier
for the adversary to impose this rule when there are very
only a few Emails containing this word in the clean dataset.
In other words, if this rule is added to the spam detector,
the accuracy of the spam detector on the rest of the emails
would not change signiﬁcantly as there are not many email
containing the word "security". On the other hand, we select
the correlation in the direction that is not dominant in the data.
Namely, if we have P r(x,y)←(X,Y )[y = 1|f (x) = 1] ≥ 0.5
then we either use (X+, 0) or (X−, 1) based on the values of
t0 and t1. The intuition behind this choice is that we always
select the correlation that happens less often in the training
set so that the poisoning distribution is more distinct from
the actual distribution and hence makes a larger change in
the distribution. Note that we can just stick to one of these
four options and still get successful attacks for most scenarios
but our experiments shows that this is the most effective way
of poisoning. Also, it is important to note that Theorem 9
could be extended to all of these attacks with slightly different
conditions. The details of the poisoning algorithm are described
in Algorithm 1.
Note that we select our poison points exactly the same
way as our theoretical results suggest. These poison points
will make the model leak the average of the target property
if the the adversary can ﬁnd the right queries that fall into
a certain uncertainty threshold. In the next two steps of the
attack, we show how the adversary can identify the queries
that are important without going through the calculation of the
uncertainty of different points.
Selecting Query points: The next challenge here is in choosing
the query points. As in Section V, we want to ﬁnd query points
whose certainty falls in a range close to 0. For instance, if the
poisoning rate p = 0.1 and we want to distinguish between
t0 = 0.3 and t1 = 0.7, the Theorem suggests that we should be
querying the points whose certainty falls between [≈ 0.15,≈
0.37]. In order to do this, we need to ﬁrst calculate the certainty.
Since we only have sampling access to the distribution, we
do not know the exact certainty. Instead we approximate the
certainty by estimating the probability that the label is 0 or 1
through training an ensemble of models and then evaluating
all of them on a point. In particular, for a point x we estimate
Pr[Y = 1 | X = x] using the fraction of models in the
ensemble that predict 1. Then we use this estimate to calculate
the certainty. The way we estimate the certainty is obviously
prone to error. To cope with the error, we actually work with
a larger range than what is suggested by our theoretical result.
In out attack, we ﬁx the range of certainty to [−0.4, 0.4] and
query the points whose certainty falls in this interval. Although
this range might be larger (and sometimes smaller) than what
our Theorem suggests, but we still get good results in our
attack. The reason behind this is that in the next step of the
attack, which is the inference phase, we ﬁlter the query points
and only look at the important ones. So if there are queries
that are not relevant, they will be ignored in the inference.
Additionally, we also include the poisoning points in the set
of queries. We ﬁnd this to slightly help attack’s accuracy.
Guessing the Fraction: At the end, the adversary must use
the results of its queries to come up with the prediction of
whether t = t0 or t = t1. The theoretical attack suggests
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
71126
i
1 , . . . ,T 1
k and T 1
[f (x) = 1] = t0 and Pr(x,y)←T 1
that we should just take the average of all responses and
predict based on whether or not the average is less than 0.5.
However, as we pointed out before, we cannot use the exact
range suggested by the theorem because we do not have access
to exact certainty. To get around this issue, we train a linear
model for the attack to identify the queries that are actually
relevant in predicting whether t = t0 or t = t1. Here we
use a shadow model approach: we sample multiple datasets
T 0
k where, for each i ∈ [k] we have
1 , . . . ,T 0
Pr(x,y)←T 0
[f (x) = 1] =
t1. Now we use the poisoning dataset Tpoison and the query
dataset Tq from the previous steps of the attacks as follows:
We ﬁrst use the poisoning set to poison all the datasets and
train multiple models (M 0
k ) where
k ) and (M 1
0 ∪Tp). After that we query
0 ∪Tp) and M i
M i
each of the models on each of the chosen queries. Finally,
we generate a training set where the query responses are the
features and the label are 0 or 1 depending on whether the
"shadow" model used was from T 0
i , and train a linear
model MA on this set. The attack then queries the target model
using query points and feeds the responses to MA, and outputs
whatever MA produces as it’s ﬁnal guess for whether t = t0
or t = t1.
1 , . . . , M 0
0 = L(T i
0 = L(T i
i or T 1
1 , . . . , M 1
i
Algorithm 1 Choosing poisoning data
Input
f
t0, t1
p
n
D+, D− Sampling oracle to distribution of instances with and
The property being attacked
Possible fractions of instances with property f
Poisoning ratio
Size of training set
without property f
Output
Tpoison
Set of poisoning examples
1: if (t0 + t1)  0.5 then
9:
10: else
11:
12: Output Tpoison
Tpoison = {(x1, 0), . . . , (xm, 0)}
Tpoison = {(x1, 1), . . . , (xm, 1)}
VII. EXPERIMENTAL EVALUATION
Here we evaluate the performance and the accuracy of our
attack described in Section VI.
A. Experimental Setup
DataSets We have run our experiments on the following
datasets (details of the datasets are deferred to the Appendix):
81127
Algorithm 2 Choosing the black box queries
Input
r
q
D+, D− Sampling oracle to distribution of instances with and
number of models in ensemble
Number of black-box queries
without property f
Output
Tq
Set of black-box queries
1: Sample a thousand data sets T1, . . . ,Tr each composed half of
elements sampled from D− and half of elements sampled from
D+.
2: Train M1, . . . , Mr using T1, . . . ,Tr.
3: Set Tq = ∅.
4: while |Tq| < q do
sample x ← 1
5:
if
6:
(cid:80)r
2 D− + 1
|1 − 2
2 D+
i=1 Mi(x)
| ≤ 0.4
r
Tq = Tq ∪ {x}.
7: Set Tq = Tq ∪ Tpoison
8: Output Tq as the set of black box queries to make.
Algorithm 3 Guessing fraction of samples with property f
Input
f
t0, t1
p
n
k
Tpoison
Tq
D+, D− Sampling oracle to distribution of instances with and
The property being attacked
Possible fractions of instances with property f
Poisoning ratio
Size of training set
Number of shadow models
The set of poisoning data
The set of black-box queries