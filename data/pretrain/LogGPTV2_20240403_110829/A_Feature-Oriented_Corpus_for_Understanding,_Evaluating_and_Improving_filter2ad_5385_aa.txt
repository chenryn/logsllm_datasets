title:A Feature-Oriented Corpus for Understanding, Evaluating and Improving
Fuzz Testing
author:Xiaogang Zhu and
Xiaotao Feng and
Tengyun Jiao and
Sheng Wen and
Yang Xiang and
Seyit Camtepe and
Jingling Xue
A Feature-Oriented Corpus for Understanding, Evaluating and
Improving Fuzz Testing
Xiaogang Zhu, Xiaotao Feng, Tengyun Jiao
PI:EMAIL
Swinburne University of Technology
Melbourne, VIC
Seyit Camtepe
DATA61 | CSIRO
Sydney, NSW
PI:EMAIL
Sheng Wen, Yang Xiang
swen,PI:EMAIL
Swinburne University of Technology
Melbourne, VIC
Jingling Xue
PI:EMAIL
The University of New South Wales
Sydney, NSW
ABSTRACT
Fuzzing is a promising technique for detecting security vulnerabili-
ties. Newly developed fuzzers are typically evaluated in terms of the
number of bugs found on vulnerable programs/binaries. However,
existing corpora usually do not capture the features that prevent
fuzzers from finding bugs, leading to ambiguous conclusions on the
pros and cons of the fuzzers evaluated. In this paper, we propose to
address the above problem by generating corpora based on search-
hampering features. As a proof-of-concept, we designed FEData, a
prototype corpus that currently focuses on three search-hampering
features to generate vulnerable programs for fuzz testing. Unlike
existing corpora that can only answer “how”, FEData can also fur-
ther answer “why” by exposing (or understanding) the reasons for
the identified weaknesses in a fuzzer. The “why” information serves
as the key to the improvement of fuzzers. Based on the “why“ infor-
mation, our FEData programs enabled us to identify the weakness
of AFLFast, called cycle explosion, behind. We further developed
an improved version of AFLFast, called AFLFast+, which has over-
come the cycle explosion problem. AFLFast+ retains the efficiency
of AFLFast in path search while maintaining or even surpassing
the bug-finding capability of AFL for the corpus evaluated.
CCS CONCEPTS
• Security and privacy → Software security engineering.
KEYWORDS
Fuzzing, Feature-Oriented Corpus, Evaluation
ACM Reference Format:
Xiaogang Zhu, Xiaotao Feng, Tengyun Jiao, Sheng Wen, Yang Xiang, Seyit
Camtepe, and Jingling Xue. 2019. A Feature-Oriented Corpus for Under-
standing, Evaluating and Improving Fuzz Testing. In ACM Asia Confer-
ence on Computer and Communications Security (AsiaCCS ’19), July 9–12,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
AsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6752-3/19/07...$15.00
https://doi.org/10.1145/3321705.3329845
2019, Auckland, New Zealand. ACM, New York, NY, USA, 6 pages. https:
//doi.org/10.1145/3321705.3329845
1 INTRODUCTION
Fuzzing is an automatic software testing technique that typically
provides random data as inputs to programs and then monitors
them for exceptions such as crashes. Fuzzing can capture bugs1
because the exceptions are usually the indicators of bugs in the
program context. In 1990, B.P. Miller et al. [17] developed the first
fuzzing algorithm (fuzzer for short). Since then, fuzzing has become
one of the major tools for detecting bugs.
Though many fuzzers have been developed, their appropriate
evaluation is still a big challenge due to the lack of supportive
corpora. To validate a newly developed fuzzer, a corpus needs to
contain the contexts of bugs such as specific search-hampering fea-
tures for fuzzing. Those contexts can help validate the advancement
of a fuzzer resolving the specific challenges. However, it is not fea-
sible for fuzzing so far. In current stage, the usual way to evaluate
fuzzers is to run them on real-world program corpora, and judge
their performance by counting the number of unique crashes after
a period of running time (normally 24 hours) [7–9, 13, 19, 22, 26].
As disclosed by G. Klees et al. [14], this often leads to ambiguous or
even wrong conclusions on the fuzzers, since it is almost impossible
to provide supportive contexts of bugs by inspecting a large num-
ber of programs in corpora. A simple example is that, Driller [25]
detects more bugs than AFL [2] by utilizing concolic execution [23]
to resolve magic values. But the evaluation results may not support
the advancement of Driller when the corpora do not have enough
bugs ‘protected’ by magic values. In fact, even though we reckon
the evaluation results are convincing, we still cannot pinpoint the
reason that leads to fuzzers’ advancements or weaknesses without
contextual details of bugs in corpora. This hampers or even disables
our attempts on improving existing fuzzers.
In this paper, we propose generating corpora based on search-
hampering features to solve the above challenge. To this end, a
framework is developed and used to synthesize evaluation corpora
in an automatic manner. The vulnerable programs are made up by
1Vulnerability is different from bug. Bug brings a program into an unintended state.
When a bug can be exploited by an attacker, the bug becomes a vulnerability [18]. In
the following of this paper, we will use the word “bug” instead of “vulnerability” for
brevity, but what we mean is “vulnerability”.
Session 9: FuzzingAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand658function-level structures from GitHub2 code to maintain as much as
possible the style of real-world programs. The framework ensures
the generated programs are able to be compiled, and inserts the
contexts of bugs to the programs when necessary. Holding contexts
of bugs, the generated corpora can not only make the conclusions
of a fuzzer’s advancements solid, but also expose the clues for
improving a fuzzer. To demonstrate the effectiveness of our idea, we
develop a proof-of-concept corpus, namely FEData, based on three
typical search-hampering features i.e., the number of magic values,
the number of execution paths, and the number of checksums.
As a case study, we run AFL and AFLFast [7] on programs from
FEData to validate its utility in fuzzing evaluation. The results of
our experiments confirm the conclusion made by AFLFast that it
finds execution paths faster than AFL. The results also indicate that
AFLFast detects fewer bugs than AFL in some specific programs
from FEData. G. Klees et al. [14] found similar phenomenon that
AFLFast detected fewer bugs when they ran these two fuzzers more
than 24 hours, but they did not explain the reason for it. With
contexts of bugs on those programs, we find that AFLFast is prone
to fall into the cycle explosion state. As AFLFast can no longer
produce new inputs in this state, it will find fewer bugs than AFL.
Accordingly, we improve AFLFast and develop AFLFast+ by setting
a lower bound to the number of inputs produced by AFLFast, which
prevents fuzzing from dropping into the cycle explosion state.
We summarize the contributions as follows:
• We propose generating corpora for fuzzing evaluation based
on search-hampering features. The generation runs in an au-
tomatic manner, and the generated corpora are not only used
for understanding and evaluating fuzzing but also helping
improve the fuzzers based on the contexts of bugs.
• We carry out a case study on AFLFast via a proof-of-concept
corpus, called FEData. The corpus is generated based on
three typical search-hampering features. The experimental
results validates the utility of the feature-oriented corpus by
showing its accurate conclusions in fuzzing evaluation.
• We develop AFLFast+ according to AFLFast’s weakness that
we expose in the case study. The experiments show that
AFLFast+ can detect more bugs than AFLFast when other
factors stay the same.
2 OVERVIEW OF THE IDEA
The idea of generating feature-oriented corpora will help solve the
challenge of fuzzing evaluation. To make the idea clearer, we take
two fuzzers to exemplify the necessity of holding contexts of bugs
for fuzzing evaluation. We take the evaluation of Driller [25] against
AFL [2] as an example. Driller claims to beat AFL because it can
resolve magic values more efficiently. In fact, Driller is an updated
version of AFL since it takes AFL as the core and adopts concolic
execution to resolve magic values for larger coverage. However,
the claim of Driller may not be supported in fuzzing evaluation
sometimes. As shown in Fig.1(A), because there are no magic values
on the execution path A → B → C → D → E → G, the concolic
execution in Driller makes no difference from AFL to trigger the
bug. Even worse, the concolic execution in Driller takes more time
to resolve the magic value 0x126 in an execution path where there is
2https://github.com
Figure 1: Two program examples that have different num-
bers of magic values to protect bugs. It may lead to a wrong
conclusion if fuzzers such as Driller and AFL are evaluated
on program (A). Driller outperforms AFL only on program
(B) since bug is hidden behind two magic value challenges.
no bug. Considering the randomness nature of fuzzing, it is highly
possible for Driller to have a worse performance than AFL. However,
Driller can outperform AFL on the programs similar to Fig.1(B),
where there are lots of bugs hidden behind magic value challenges.
Therefore, when state-of-art corpora do not provide contexts of
bugs for evaluation, the results may lead to ambiguous or even
wrong conclusions on fuzzers.
It is necessary to implement contexts of bugs such as search-
hampering features in fuzzing evaluation corpora. As shown in
Fig.2, the corpus of vulnerable programs is one of the major compo-
nents in fuzzing evaluation procedure. Fuzzers follow a policy such
as how to generate seeds, and run on the evaluation corpus. The
efficiency of fuzzing is mainly determined by the capability of the
fuzzer solving contextual challenges in programs, such as magic
values, checksums, path search, etc. We can therefore measure the
performance of fuzzers via a list of metrics such as the number of
magic values resolved in the execution. However, to date, the usual
way to judge the performance of a fuzzer is to count the number of
bugs exposed within a fixed period of time. Because the number of
exposed bugs may not be positively correlated to the challenges, it
cannot always indicate the efficiency of a fuzzer. In addition, since
the number of exposed bugs cannot be linked to the metrics without
contexts of bugs in programs, the usual way becomes too superficial
to diagnose the drawbacks of a fuzzer. This explains our technical
motivation as well as the real need to develop fuzzing evaluation
corpora that implement contexts of bugs such as search-hampering
features in vulnerable programs.
We create a prototype framework to automatically generate
corpora for fuzzing evaluation. The framework uses real-world
program structures from GitHub to maintain the synthetic coding
style. The contexts of bugs are then inserted into the structures,