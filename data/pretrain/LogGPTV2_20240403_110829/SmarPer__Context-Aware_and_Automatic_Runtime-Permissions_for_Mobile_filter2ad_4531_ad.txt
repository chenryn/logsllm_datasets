r
e
i
f
i
t
n
e
d
i
t
n
a
p
i
c
i
t
r
a
P
354
D6
4B0
20A
0D2
004
DCC
A68
6D0
263
393
08C
First, intermediate, and last decisions
Fig. 4. Decision patterns over time for a subset of participants and
decisions. Each row shows the ﬁrst, middle, and last 12 decisions of
each participant. We can observe the different privacy behavior of
participants. Participants are grouped vertically in three categories:
utility-concerned (top), somewhat-privacy-concerned (middle), and
privacy-concerned (bottom) participants.
On average, participants used 4.2(±2.0) apps from our list
of apps. In our data set we have data from 23 apps out of 29
in our list. Note that the fact that participants used a small
number of apps is beneﬁcial for our analysis, as it enabled us
to collect more decision data per used app during the study,
hence reducing data sparsity. Figure 5 shows apps with more
than 10 decisions and more than 1 participant. We can see
that the three most popular apps are WhatsApp, Facebook,
and Skype with 36, 33, and 19 participants, respectively. The
difference in the number of decisions is due not only to the
number of participants per app, but also to the type of app
and how active each participant was. More details about the
number of decisions per app are shown in Table II (Appendix).
VI. DATA ANALYSIS: PREDICTING DECISIONS
In this section, we present the machine learning analysis
of our data set. We describe and compare various methods
for context-aware and automatic permissions. Our goal is to
predict users’ preferred privacy levels for a new permission
prompt, given their past decisions and associated context.
A. Problem Statement
We index users by u and permission requests by i and j.
We denote user u’s decision for the i’th permission request
by yui ∈ {“Allow”, “Obfuscate”, “Deny”}. We denote the
context of the permission request by a feature-vector xui ∈
X ⊂ R
D and the time the request was made by tui. We denote
the user’s past data before time t by Du,t to be the set of all
decision pairs {yui, xui} made at time ti < tu. Our goal is
the following: given a user’s past decisions Du,t, predict the
users’ decision y∗ at a future time given a feature vector x∗.
We focus on two important aspects: (1) we can learn to
predict permission decisions, (2) context helps us to do so.
We also show that, as the amount of data per user increases
(i.e., a higher tu), our predictions improve much faster when
we take context into account.
 3500
 3000
 2500
 2000
 1500
 1000
 500
)
s
t
n
a
p
i
c
i
t
r
a
p
f
o
r
e
b
m
u
n
(
s
n
o
i
s
i
c
e
d
f
o
r
e
b
m
u
N
Allow
Obfuscate
Deny
)
3
3
(
2
8
9
2
)
1
3
(
1
2
8
1
)
8
(
8
7
) 8
2
1
(
9
6
5
)
7
(
6
5
5
)
5
1
(
1
3
4
)
2
1
(
0
5
3
)
2
1
(
3
8
2
)
6
(
5
0
2
)
7
(
3
5
1
)
2
(
6
1
)
4
(
5
4
)
9
(
6
5
)
7
(
3
9
 0
Evernote
Soundcloud
Dropbox
Accu W eather
Instagram
Shazam
Snapchat
TripAdvisor
Skype
T witter
Viber
The W eather Channel
Facebook
W hatsApp
Fig. 5. Total number of decisions and participants (in parentheses)
for apps with more than 10 decisions and more than 1 participant,
including the distribution of the decision types. The difference in the
number of decisions is due to the type and the popularity of the app.
B. Baselines
We use the following two baselines. The ﬁrst baseline is
referred to as the static policy method based on a survey com-
pleted by all participants (Section V-A4). Decisions collected
in this survey are used as ﬁxed predictions for permission
requests. Over time, this method does not learn users’ prefer-
ences and only takes part of the contextual information into
account (i.e., apps’ names and targeted data types). We expect
this method to perform worse than a dynamic method that
learns from users’ behavior. This method approximates the
current permission systems in Android 6+ and iOS.
Our second baseline ignores contextual
information but
learns the preference function from past data. We simply
predict the most frequent decision made by the user until time
tu for all the new decisions. This method, although dynamic,
might miss the contextual information associated with some
decisions and might perform worse than a method that takes
the context into account. We call this method ZeroRt, because
it is an extension of the ZeroR classiﬁer [46].
C. Context-Aware Method
We compare our baselines to a method that learns from
users’ behavior and uses contextual information to predict. We
model privacy preferences of the user u by a one-dimensional
privacy-preference function fu : X → R. Given a feature
vector x ∈ X , the value of function fu(x) indicates a degree of
privacy: a higher value indicates higher desire for privacy. The
1067
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:06 UTC from IEEE Xplore.  Restrictions apply. 
⎧⎨
⎩
yui =
prediction is made by thresholding the preference function:
“Deny”,
“Obfuscate”,
“Allow”,
when
θ1 < fu(xi)
θ2 < fu(xi) ≤ θ1
fu(xi) ≤ θ2
(1)
where θ1 and θ2 are two real-valued scalars. This is an
example of the Random Utility Model and has been widely
used to model users’ preference functions (see [47]).
We use Bayesian linear regression (BLR) to model
the
information. The
preference function given the contextual
simplest model is to use a linear function:
fu(x) = β0
u + βT
u x + ui,
(2)
D, and ui is the noise. We model both
where β0
βu and the noise ui as i.i.d. Gaussian random variables.
u ∈ R, βu ∈ R
Using the Bayes rule, we can compute the posterior distri-
bution over predictions. However, the nonlinear function of (1)
complicates this computation because it is not Gaussian. To
simplify the computation, we make the following relaxation
to (1): we ﬁx thresholds6 θ1 = 0.5 and θ2 = −0.5 and
recode the decisions {“Allow”, “Obfuscate”, and “Deny”} as
{−1, 0, +1}. This makes the decision yui Gaussian and then
we can compute the posterior distribution in closed-form by
using the Bayes rule. The BLR model outputs a real-value
ˆy which we threshold at θ1 and θ2 according to (1) to get
the discrete-valued decision. The formulation presented in (1)
and (2) enables us to use nonlinear models for fu by using
Gaussian Process models (GP). By simply changing the kernel
matrix used, we can obtain a variety of nonlinear models
(see Chapter 2 in [48]). This approach is similar to SVM
algorithms, used in previous works [2], [8], with one important
difference: the GP model gives us posterior probabilities for
our predictions, unlike SVM where we need a two-stage
procedure that requires large data to avoid overﬁtting (see
Chapter 7 in [49]).
We note that our approach, BLR, differs from previous
works that use only two classes “allow” and “deny” [2], [8].
For a two-class problem, the ordering does not matter, but for
our problem it is clear that “obfuscate” requires less privacy
than “deny” but more privacy than “allow”. Therefore, the
choice of a one-dimensional function is justiﬁed, although
this approach can be easily extended to a multi-dimensional
function [50]. Another alternative would be to use multi-
class classiﬁcation (e.g., support-vector machines, classiﬁca-
tion trees), along with a cost-sensitive cost function [51]. Still,
BLR is a reasonable ﬁrst choice for small data sets, given its
simplicity.
D. Error Measure
To reliably compare methods, we propose the performance
error measure E to evaluate the performance of a method M:
E tM(D,Dtest) :=
1
U
1
Nu
U(cid:5)
Nu(cid:5)
u=1
i=1
L(yui, ˆyui|t)
(3)
6In practice, these thresholds should be learned from the data.
1068
where L is a loss function, Dtest is the set of test decisions
yui for users u = 1, . . . , U, D is the set of past decisions and
contextual information Du,t for these users, Nu is the number
test decisions in Du,t, and ˆyui|t are predictions computed by
using Du,t and the method M. Note that the error measure
is a random variable which depends on the choice of users
in the test data and the data that contains the past decisions.
This loss function is averaged over many users, therefore it
penalizes methods that do not generalize well to many users
at the same time. This is a better error measure than using
one-leave-out methods that might show a high variance for
different trials, as different users are selected in different runs.
We will use two types of loss functions. The ﬁrst loss