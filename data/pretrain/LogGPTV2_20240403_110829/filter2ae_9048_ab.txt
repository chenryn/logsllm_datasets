    | sort 100 -count
@facebook.com电子邮件最常用的密码：
下面，我们以图形化的形式来展示数据：
更有趣的是，我们可以通过员工OSINT获取的电子邮件轻松实现密码的交叉核对。当然，我们也可以通过grep命令来完成这项任务。
**DomLink domains - > Give me all passwords for all domains**
* * *
在运行DomLink（关于DomLink，上文已有介绍）之后，我们可以继续使用它来完成相应的查找工作。不过，这里使用的正则表达式是
PI:EMAIL，而非_.domain.com，此外，还需要按照下面的方法来设置电子邮件的标题字段：
然后执行查询操作：
    index=passwords [inputlookup uber.csv] 
    |  table email, password
这样，我们就会得到一份表格形式的输出结果，如果需要的话，可以将其导出为CSV格式：
这简直太棒了，至少我是这么认为的。
## 员工OSINT
当我们在外部基础架构上执行有针对性的钓鱼攻击或"拖库"时，通常首先要搞到相应的用户名清单，所以，这时了解组织员工的概要信息就非常重要了。不过，[LinkedInt](https://vincentyiu.co.uk/reconnaissance-using-linkedint/
"LinkedInt")好像就是专门为此而生的。此外，我们还需要一个靠谱的工具，以帮助我们获取目标组织的员工名单。这对于少数几家公司的一两个特定数据集来说，优势并不是特别明显。然而，如果您要为Alexa上前100万个网站和财富500强企业自动收集员工数据的话，这种方法的优势就立马显现出来了。当然，这些工作可以实现全自动化。
这里，我们将演示如何从一个尚未公开的数据集（可能会在HITB
GSEC上公开）中导入数据，以分析员工的基本信息。这里介绍的方法也同样适用于LinkedInt。实际上，我们只需将数据作为CSV文件导入即可，具体如前所述。
我们可以使用Splunk搜索符合特定姓氏、名字或地理位置要求的员工：
我们甚至可以考察相关的工作岗位上的员工数量等信息：
从这里可以看出，大部分员工都是送货人员，因为这家公司相当于中国的Deliveroo。
实际上，我们还可以进一步将其绘制成角色分布图：
虽然这里只有5000名左右的员工数据，但是已经足以了解目标组织中人员的大致分布情况了，至少在进行下一步的社工之前，可以帮助我们回答下列问题：
  * 是否需要雇佣社工人员？
  * 我应该拥有什么头衔？
  * 我的角色是什么？
  * 我在哪里？（是的，如果X角色位于Y地址的概率很高的话，就选Y地址）
  * 担任该角色的员工有多少？
对于某些公司来说，如果有很大一部分(近10%
)员工都是送货员的话，那么他们很可能无法接触到普通员工，也很难获得普通员工的信任。所以，根据上述问题做出相应的选择是非常重要的一件事情。
最后，我想知道自己的OSINT和员工的实际分布的实际差距有多大，换句话说到底有多么准确？
## 综合运用员工OSINT与密码转储
如果将通过员工OSINT获取的电子邮件与密码转储结合起来，那么查找密码就会变得更加轻松：
    index=passwords [search index=eleme 
    | eval email=$Email$ 
    | table email]
真不错，至少我是这么认为的。当然，您也可以使用grep来实现这一点，但上面这种方法不仅在导入新CSV文件的时候要更加简便，并且几乎可以立即找到匹配的密码列表（可以将其导出为CSV或表格）。
## Nmap扫描
将.gnmap文件上传到Splunk，把时间戳设置为none。
然后，可以运行以下示例查询来格式化数据，并通过Splunk以高效的方式完成相应的搜索：
1
    source="uber.gnmap" host="uber" sourcetype="Uber" Host Ports | rex field=_raw max_match=50 "Host:\s(?\S+)" 
    | rex field=_raw max_match=50 "[Ports:|,]\s?(?\d+)\/+(?\w+)\/+(?\w+)\/+(?\w+|\/)"
    | rex field=_raw "OS:\s(?\w+)"
    | eval os = if(isnull(os),"unknown",os)
    | eval mv=mvzip(port, status) 
    | eval mv=mvzip(mv, proto) 
    | eval mv=mvzip(mv, desc) 
    | mvexpand mv 
    | makemv mv delim="," 
    | eval ports=mvindex(mv, 0) 
    | eval status=mvindex(mv, 1)
    | eval proto=mvindex(mv, 2)
    | eval desc=if(mvindex(mv, 3) == "/","null",mvindex(mv,3))
    | table dest_ip ports status proto desc os
    | sort dest_ip
如果你有大量的扫描需要进行解析的话，这种方式是非常有效的。例如，您可以扩展该查询来绘制所有开放了TCP的443端口的服务器的地理位置：
    source="uber.gnmap" host="uber" sourcetype="Uber" Host Ports 
    | rex field=_raw max_match=50 "Host:\s(?\S+)" 
    | rex field=_raw max_match=50 "[Ports:|,]\s?(?\d+)\/+(?\w+)\/+(?\w+)\/+(?\w+|\/)"
    | rex field=_raw "OS:\s(?\w+)"
    | eval os = if(isnull(os),"unknown",os)
    | eval mv=mvzip(port, status) 
    | eval mv=mvzip(mv, proto) 
    | eval mv=mvzip(mv, desc) 
    | mvexpand mv 
    | makemv mv delim="," 
    | eval ports=mvindex(mv, 0) 
    | eval status=mvindex(mv, 1)
    | eval proto=mvindex(mv, 2)
    | eval desc=if(mvindex(mv, 3) == "/","null",mvindex(mv,3))
    | table dest_ip ports status proto desc os
    | sort dest_ip
    | table dest_ip,ports 
    | search ports=443 
    | iplocation dest_ip 
    | geostats count by City
[1]参考文章：[How to parse Nmap in
Splunk](https://answers.splunk.com/answers/327620/how-to-parse-grepable-nmap-output.html "How to parse Nmap in Splunk")
## 小结
数据的处理和分析在渗透测试过程中扮演了一种重要的角色。俗话说，工欲善其事，必先利其器，就像密码破解需要借助于32个GPU的破解平台一样，Splunk也有助于令渗透测试过程更加简化和高效。
许多人认为，GZIP的类比有些流于表面，那好，下面我们就给出agn0r对Splunk基于Lexicon结构的运行机制的深刻洞见：