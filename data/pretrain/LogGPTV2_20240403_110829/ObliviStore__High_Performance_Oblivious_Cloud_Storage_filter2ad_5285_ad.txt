partitioning framework to achieve load balancing amongst
multiple ORAM nodes. Each ORAM node has several stor-
age partitions, and relies on the partitioning framework again
to store data blocks in a random storage partition with every
data access. One beneﬁt of the distributed architecture is that
multiple ORAM nodes can perform shufﬂing in parallel.
A. Detailed Distributed ORAM Construction
To access a block, the oblivious load balancer ﬁrst looks
up its position map, and determines which ORAM node is
responsible for this block. The load balancer than passes the
request to this corresponding ORAM node. Each ORAM
node implements a smaller ORAM consisting of multiple
storage partitions. Upon obtaining the requested block, the
ORAM node passes the result back to the oblivious load bal-
ancer. The oblivious load balancer now temporarily places
the block in its eviction caches. With every data access, the
oblivious load balancer chooses ν random ORAM nodes and
evicts one block (possibly real or dummy) to each of them,
through an ORAM write operation.
Each ORAM node also implements the shufﬂing function-
alities as described in Section V. In particular, the ORAM
nodes can be regarded as a parallel processors capable
of performing reshufﬂing in parallel. The oblivious load
balancer need not implement any shufﬂing functionalities,
since it does not directly manage storage partitions. Hence,
even though the load balancer is a central point, its function-
ality is very light-weight in comparison with ORAM nodes
which are in charge of performing actual cryptographic and
shufﬂing work.
Notice that each ORAM node may not be assigned an
equal amount of storage capacity. In this case, the probability
of accessing or evicting to an ORAM node is proportional to
the amount of its storage capacity. For ease of explanation,
we assume that each storage partition is of equal size,
and that each ORAM node may have different number of
partitions – although in reality, we can also support partitions
of uneven sizes in a similar fashion.
Theorem 2. Our distributed asynchronous ORAM construc-
tion satisﬁes the security notion described in Deﬁnition 1.
Proof: (sketch.) Similar to that of Theorem 1. Both the
oblivious load balancer and the ORAM node algorithms are
perfectly simulatable by the adversary, without having to
observe the physical addresses accessed. The detailed proof
is in the full version [39].
B. Dynamic Scaling Up
Adding compute nodes. When a new ORAM node proces-
sor is being added to the system (without additional storage),
the new ORAM node processor registers itself with the load
balancer. The load balancer now requests existing ORAM
nodes to hand over some of their existing their partitions
to be handled by the new processor. To do this, the ORAM
nodes also need to hand over part of their local metadata
to the new processor, including part of the position maps,
eviction caches, and partition states. The load balancer also
needs to update its local metadata accordingly to reﬂect the
fact that the new processor is now handling the reassigned
partitions.
Adding compute nodes and storage. The more difﬁcult
case is when both new processor and storage are being added
to the system. One naive idea is for the ORAM system
to immediately start using the new storage as one or more
additional partitions, and allow evictions to go to the new
partitions with some probability. However, doing so would
result in information leakage. Particularly, when the client is
reading the new partition for data, it is likely reading a block
that has been recently accessed and evicted to this partition.
We propose a new algorithm for handling addition of
new ORAM nodes, including processor and storage. When a
new ORAM node joins, the oblivious load balancer and the
260
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
ORAM main loop:
• Decrement the early cache-in semaphore by the number of levels.
• Decrement the eviction semaphore by eviction rate.
• Fetch the next data access request for blockid, look up the position map to determine that blockid is assigned to partition p.
• Call ReadPartition(p, blockid).
• On callback:
// Evictions must be performed later to release the ”eviction resource”.
– Store the block to the eviction cache (overwrite block if this is an ORAM write request).
– Let ν denote the eviction rate. Choose ν partitions at random for eviction, by incrementing their respective job sizes: Jp ← Jp+1
// Reserve space for early cache-ins.
// If ν is a ﬂoating number, choose at least ν partitions on average.
Figure 5: ORAM main algorithm.
, then set (cid:5)∗ ← ⊥.
ReadPartition(p∗, blockid):
1) Looks up the position map to determine the level (cid:5)∗
2) For each level in partition p∗
found in partition p∗
• the level is empty;
• the level is not marked for shufﬂing; or
• the level is marked for shufﬂing but all blocks have been cached in.
• If (cid:5) = (cid:5)∗
3) For each ﬁlled level (cid:5) in partition p∗
:
, ReadReal((cid:5), blockid).
Else, ReadFake((cid:5)).
where blockid resides. If the requested block is a dummy or blockid is not
that satisﬁes one of the following conditions: increment early cache-in semaphore by 1:
ReadReal((cid:6), blockid):
• If blockid has been cached in:
– Call ReadFake((cid:5))
– On completion of the fake (i.e., dummy or early) cache-in: return contents
of blockid through callback.
/* To prevent timing channel leakage, must
wait for fake cache-in to complete before returning the block to ORAM
main. */
• Else:
– Cache in block blockid from server.
– On completion of cache-in, return contents of blockid through callback.
ReadFake((cid:6)):
• If level (cid:5) is not being shufﬂed:
– Get address addr of next
block.
random dummy
– Cache in the dummy block at addr.
• If level (cid:5) is being shufﬂed, and level (cid:5) has unread
blocks,
– Perform an early cache-in.
• Else return with ⊥.
Figure 6: Partition reader algorithm.
new ORAM node jointly build up new storage partitions. At
any point of time, only one storage partition is being built.
Building up a new storage partition involves:
(cid:9)
• Random block migration phase. The load balancer
selects random blocks from existing partitions, and
migrates them to the new partition. The new partition
being built is ﬁrst cached in the load balancer’s local
trusted memory, and it will be sequentially written out
N/D)
to disk when it is ready. This requires about O(
amount of local memory, where N is the total storage
capacity, and D is the number of ORAM nodes.
During the block migration phase, if a requested block
resides within the new partition,
the load balancer
fetches the block locally, and issues a dummy read
to a random existing partition (by contacting the cor-
responding ORAM node). Blocks are only evicted to
existing partitions until the new partition is fully ready.
• Marking partition as ready. At some point, enough
blocks would have been migrated to the new partition.
Now the load balances sequentially writes the new
partition out to disk, and marks this partition as ready.
• Expanding the address space. The above two steps mi-
grate existing blocks to the newly introduced partition,
but do not expand the capacity of the ORAM. We need
to perform an extra step to expand ORAM’s address
space.
Similarly, the challenge is how to do this securely.
Suppose the old address space is [1, N ], and the new
address space after adding a partition is [1, N(cid:2)
], where
N(cid:2) > N. One naive idea is to randomly add each block
in the delta address space [N + 1, N(cid:2)
] to a random
partition. However, if the above is not an atomic opera-
tion, and added blocks become immediately accessible,
this can create an information leakage. For example,
after the ﬁrst block from address space [N + 1, N(cid:2)
]
has been added, at this time, if a data access request
wishes to fetch the block added, it would deﬁnitely visit
the partition where the block was added. To address
this issue, our algorithm ﬁrst assigns each block from
address space [N + 1, N(cid:2)
] to a random partition –
however, at this point, these blocks are not accessible
yet. Once all blocks from address space [N + 1, N(cid:2)
]
have been assigned,
the load balancer notiﬁers all
ORAM nodes, and at this point, these additional blocks
become fully accessible.
Initially, a new ORAM node will have 0 active partitions.
Then, as new storage partitions get built, its number of active
partitions gradually increases. Suppose that at some point
261
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
Background shufﬂer loop:
1) Start shufﬂing.
• Find a partition p whose bShuﬄe indicator is 0 and job size Jp > 0. Start the shufﬂing of partition p.
• Set bShuﬄe ← 1 for partition p.
// Each partition can only have one active shufﬂing job at a time.
• Mark levels for shufﬂing.
• Take a snapshot of the partition job size (cid:2)Jp ← Jp.
• For each unread block B in each level marked for shufﬂing:
2) Cache-in and reserve space.
– Decrement: 1) shufﬂing buffer semaphore, and 2) shufﬂing I/O semaphore;
– Issue a CacheIn request for B.
• Let r denote the number of reserved slots in shufﬂing buffer so far. Let w denote the number of cache-outs that will be performed
after this partition is shufﬂed. Note that r ≤ w.
Decrement the shufﬂing buffer semaphore by w − r.
// Reserve space in shufﬂing buffer for early cache-ins, unevicted blocks, and dummy blocks.
3) Upon completion of all cache-ins, perform atomic shufﬂe.
/* Since computation is much cheaper than bandwidth or latency, we assume that the local shufﬂe is done atomically. */
• Fetch.
– Fetch from the storage cache all cached-in blocks for levels marked for shufﬂing. For each cache-in fetched that is an early
cache-in, increment the early cache-in semaphore.
// These early cache-ins are now accounted for by the shufﬂing buffer semaphore.
– Let (cid:2)Jp denote the job size at the start of this shufﬂing. Fetch (cid:2)Jp blocks from the eviction cache corresponding to the
partition. Increment eviction cache semaphore by (cid:2)Jp.
/* If fewer than (cid:2)Jp blocks for this partition exists in the eviction cache, the eviction cache returns dummy blocks to pad.
These unevicted cache blocks are now accounted for by the shufﬂing buffer semaphore. */
• Shufﬂe.
– Add dummies to the shufﬂing buffer to pad its size to w.
– Permute the shufﬂing buffer.
– Store shufﬂed blocks into storage cache: for each level (cid:5), store exactly 2 · 2(cid:2) blocks from the shufﬂing buffer (at least half
• Store.
of which are dummy). Mark destination levels as ﬁlled.
• Unmark levels for shufﬂing. Set partition counter Cp ← (Cp + (cid:2)Jp) mod partition capacity. Clear bShuﬄe ← 0.
• For each block B to be cached out:
4) Cache-out.
– Decrement the shufﬂing I/O semaphore.
– Issue a CacheOut call for block B.
• On each cache-out completion: increment the shufﬂing buffer semaphore.
Figure 7: Background shufﬂer algorithm.
of time, each existing ORAM node has c1, c2, . . . , cm−1
partitions respectively, and the newly joined ORAM node
has cm active partitions, while one more partition is being
built. Suppose all partitions are of equal capacity, then the
probability of evicting to each active partition should be
equal. In other words, the probability of evicting to the i’th
ORAM node (where i ∈ [m]) is proportional to ci.
The remaining question is when to stop the migration
and mark the new partition as active. This can be done
as follows. Before starting to build a new partition, the
oblivious load balancer samples a random integer from the
binomial distribution k $← B(N, ρ), where N is the total
capacity of the ORAM, and ρ = 1
P +1, where P denotes
the total number of active partitions across all ORAM
nodes. The goal is now to migrate k blocks to the new
partition before marking it as active. However, during the
block migration phase, blocks can be fetched from the new
partition but not evicted back to it. These blocks fetched
from the new partition during normal data accesses are
discounted from the total number of blocks migrated.
The full node join algorithm in the full version [39].
VII. EXPERIMENTAL RESULTS
We implemented ObliviStore in C#. The code base has a
total of ∼ 9000 lines of code measured with SLOCCount [3].
Eliminating effects of caching. We eliminate OS-level
caching so that our experiments represent worst-case sce-
narios. Our implementation uses kernel APIs that directly
access data on the physical disks and we explicitly disable
OS-level caching for both disk reads and writes.
Warming up ORAMs. In all experiments, we warm up the
ORAMs ﬁrst before taking measurements. Warming up is
achieved by always ﬁrst initializing ObliviStore into a state
that it would be after O(N ) accesses.
A. Single Client-Server Setting
1) Results with Rotational Hard Disk Drives: We ran
experiments with a single ORAM node with an i7-930 2.8
262
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
Ghz CPU and 7 rotational WD1001FALS 1TB 7200 RPM
HDDs with 12 ms random I/O latency [1]. To be comparable
to PrivateFS, our experiments are performed over a network
link simulated to have 50ms latency (by delaying requests
and responses). We also choose the same block size, i.e.,
4KB, as PrivateFS.
Throughput and response time. Figure 8 shows the
throughput of our ORAM against the ORAM capacity. For
a 1TB ORAM, our throughput is about 364KB/s. Figure 9
plots the response time for data requests with various ORAM
capacities. For a 1TB ORAM, our response time is about
196ms. We stress that the response time is measured under
maximum load – therefore,
the response time accounts
for both the online data retrieval and the ofﬂine shufﬂing
overhead.
In both Figures 8 and 9, we also marked data points
for PrivateFS and PD-ORAM for comparison. For a 1 TB
ORAM, ObliviStore has about 18 times higher throughput
than PrivateFS. Note that we set up this experiment and
parameters to best replicate the exact setup used in the
PrivateFS and PD-ORAM experiments [47].
Small number of seeks. Our optimizations for reducing
disks seeks (see full version [39]) help greatly in achieving
(relatively) high performance. Figure 16 plots the average
number of seeks per ORAM operation. At 1TB to 10TB
ORAM capacities, ObliviStore requires under 10 seeks per
ORAM operation on average.
Effect of network latency.
In Figures 10 and 14, we
measure the throughput and latency of a 1 TB ObliviS-