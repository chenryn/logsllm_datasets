### Partitioning Framework for Load Balancing Across Multiple ORAM Nodes

To achieve load balancing among multiple Oblivious RAM (ORAM) nodes, a partitioning framework is employed. Each ORAM node consists of several storage partitions, and the partitioning framework ensures that data blocks are stored in a random storage partition with every access. A key advantage of this distributed architecture is that multiple ORAM nodes can perform shuffling in parallel.

#### Detailed Distributed ORAM Construction

1. **Accessing a Block:**
   - The oblivious load balancer first consults the position map to determine which ORAM node is responsible for the requested block.
   - The load balancer then forwards the request to the corresponding ORAM node.
   - Each ORAM node implements a smaller ORAM with multiple storage partitions.
   - Once the requested block is retrieved, the ORAM node returns it to the oblivious load balancer.
   - The oblivious load balancer temporarily stores the block in its eviction cache.
   - With each data access, the load balancer selects ν random ORAM nodes and evicts one block (real or dummy) to each, using an ORAM write operation.

2. **Shuffling Functionality:**
   - Each ORAM node also handles shuffling as described in Section V.
   - These nodes can be seen as parallel processors capable of performing reshuffling concurrently.
   - The oblivious load balancer does not implement shuffling functionalities, as it does not directly manage storage partitions.
   - This makes the load balancer's role lightweight compared to the ORAM nodes, which handle cryptographic and shuffling tasks.

3. **Storage Capacity and Probability:**
   - Each ORAM node may have a different storage capacity.
   - The probability of accessing or evicting to an ORAM node is proportional to its storage capacity.
   - For simplicity, we assume equal-sized storage partitions, though the system can support uneven partition sizes similarly.

**Theorem 2:** Our distributed asynchronous ORAM construction satisfies the security notion described in Definition 1.

**Proof (Sketch):** Similar to Theorem 1, both the oblivious load balancer and ORAM node algorithms are perfectly simulatable by the adversary without observing physical addresses. The detailed proof is available in the full version [39].

### Dynamic Scaling Up

1. **Adding Compute Nodes:**
   - When a new ORAM node processor is added (without additional storage), it registers with the load balancer.
   - The load balancer requests existing ORAM nodes to transfer some of their partitions to the new processor.
   - Existing ORAM nodes also transfer part of their local metadata, including position maps, eviction caches, and partition states.
   - The load balancer updates its local metadata to reflect the new processor's responsibility for the reassigned partitions.

2. **Adding Compute Nodes and Storage:**
   - Adding both a new processor and storage is more complex.
   - Simply using the new storage as additional partitions and allowing evictions to go there would result in information leakage.
   - We propose a new algorithm for adding new ORAM nodes, including both processor and storage:
     - **Random Block Migration Phase:**
       - The load balancer selects random blocks from existing partitions and migrates them to the new partition.
       - The new partition is first cached in the load balancer’s local trusted memory and written to disk when ready.
       - This requires about \( O\left(\frac{N}{D}\right) \) amount of local memory, where \( N \) is the total storage capacity, and \( D \) is the number of ORAM nodes.
     - **Marking Partition as Ready:**
       - Once enough blocks are migrated, the load balancer writes the new partition to disk and marks it as ready.
     - **Expanding the Address Space:**
       - Migrating existing blocks to the new partition does not expand the ORAM's address space.
       - To expand the address space, each block in the new address range is assigned to a random partition but remains inaccessible until all assignments are complete.
       - The load balancer then notifies all ORAM nodes, making the additional blocks fully accessible.

### Background Shuffler Loop

1. **Start Shuffling:**
   - Find a partition \( p \) with \( bShuffle = 0 \) and job size \( J_p > 0 \).
   - Set \( bShuffle \leftarrow 1 \) for partition \( p \).
   - Mark levels for shuffling and take a snapshot of the partition job size \( \hat{J}_p \leftarrow J_p \).

2. **Cache-in and Reserve Space:**
   - Decrement shuffling buffer and I/O semaphores.
   - Issue a CacheIn request for each unread block \( B \) in each level marked for shuffling.
   - Reserve space in the shuffling buffer for early cache-ins, unevicted blocks, and dummy blocks.

3. **Perform Atomic Shuffle:**
   - Fetch all cached-in blocks for levels marked for shuffling.
   - Fetch \( \hat{J}_p \) blocks from the eviction cache.
   - Add dummies to the shuffling buffer, permute it, and store shuffled blocks into the storage cache.
   - Unmark levels for shuffling and set partition counter \( C_p \leftarrow (C_p + \hat{J}_p) \mod \text{partition capacity} \).

4. **Cache-out:**
   - Decrement the shuffling I/O semaphore.
   - Issue a CacheOut call for each block \( B \).
   - Increment the shuffling buffer semaphore on each cache-out completion.

### Experimental Results

We implemented ObliviStore in C# with approximately 9000 lines of code. To ensure worst-case scenarios, we eliminated OS-level caching and used kernel APIs for direct disk access.

1. **Single Client-Server Setting:**
   - **Results with Rotational Hard Disk Drives:**
     - Experiments were conducted on a single ORAM node with an i7-930 2.8 GHz CPU and 7 WD1001FALS 1TB 7200 RPM HDDs.
     - Network latency was simulated at 50ms, and block size was set to 4KB.
     - **Throughput and Response Time:**
       - For a 1TB ORAM, our throughput is about 364KB/s, and the response time is about 196ms under maximum load.
       - Compared to PrivateFS, ObliviStore has about 18 times higher throughput for a 1TB ORAM.
     - **Small Number of Seeks:**
       - Our optimizations reduce disk seeks, achieving high performance with under 10 seeks per ORAM operation at 1TB to 10TB capacities.
     - **Effect of Network Latency:**
       - Figures 10 and 14 show the throughput and latency of a 1TB ObliviStore, demonstrating the impact of network latency.

This comprehensive approach ensures efficient and secure data management in a distributed ORAM environment.