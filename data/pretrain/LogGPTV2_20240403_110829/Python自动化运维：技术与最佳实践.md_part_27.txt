息，如图12-7所示。
查看生成的分析结果文件清单，其中/output/word/part-00000为分析结果文件，如
访问http://192.168.1.20:50030/jobtracker.jsp，点击生成的Jobid，查看mapreduce job信
图12-6为返回的执行结果，可以看到map及reduce计算的百分比进度。
Job Counters
reduce
22:
22:50:12
22
Kind
50:11
名
Linux公社
INFC
100.00%
100.00%
图12-7Web查看mapreduce job信息（部分截图）
bu
Data-local map tasks
aunched reduce tasks
SLOTS_MILLIS_MAPS
aunched map tasks
Bytes Read
图12-6执行MapReduce任务结果
www.linuxidc.com
rack
put:
Counter
8
第12章Python大数据应用详解211
unjar
53
Task Atteapts
Map
Reduce
07
0
0
0
8570//tmp/stream
Total
134,819
177
id-job_201
job.track
---
## Page 239
数据流命令行，使我们可以更轻松、快速编写MapReduce任务。Mrjob具有如下特点。
Python 框架，它实际上对 Hadoop Streaming 的命令行进行了封装，因此接触不到 Hadoop 的
12.3.2
图12-8所示。
212第二部分高级篇
?
Mrjob（http://pythonhosted.org/mrjob/index.html）是一个编写MapReduce任务的开源
2）支持多步骤的MapReduce任务工作流；
1）代码简洁，map及reduce 函数通过一个Python文件就可以搞定；
提示
最后查看结果数据，图12-9显示了单词个数统计的结果，整个分析过程结束。
5）查看文件内容，示例：bin/hadoop dfs-cat/output/word/part-00000。
4）上传文件，示例：bin/hadoop fs-put/home/test/hadoop/*.txt/data/root/test。
3）删除文件或目录，示例：bin/hadoopfs-rmr/data/root/test。
2）列出目录清单，示例：bin/hadoop dfs-ls /data/root。
1）创建目录，示例：bin/hadoop dfs-mkdir/data/root/test。
HDFS常用操作命令有：
用Mrjob框架编写MapReduce详解
Linux公社 www.linuxidc.com
8-0z0hadoc
图12-9查看结果文件part-00000内容
80
图12-8任务输出文件清单
-1.2.1/bin/had
LS
ES
---
## Page 240
输出文件使用“>output-file”或“-o output-file”。下面两条命令是等价的：
过不断调用nextO去实现key-value的初始化或运算操作。前面介绍Mrjob支持四种运行方
为数据1；reducer接收mapper 输出的key-value对进行整合，把相同key的value作累加
reducer 函数。mapper 函数接收每一行的输人数据，处理后返回一对key:value，初始化value
Mrjob 通过mapperO与reducerO方法实现了MR操作，实现代码如下：
主要介绍前三者的运行方式。
式，包括内嵌（-r inline）、本地（-r local）、Hadoop（-r hadoop）、Amazon EMR（-r emr），下面
（sum）操作后输出。Mrjob利用Python的yield机制将函数变成一个Generators（生成器），通
mrjob。
特点是调试方便，启动单一进程模拟任务执行状态及结果，默认（-r inline）可以省略，
（1）内嵌（-rinline）方式
安装Mrjob要求环境为Python 2.5及以上版本，源码下载地址：https://github.com/yelp/
输出文件output.txt内容见图12-10。
#python word_count.py-r inline input.txt >output.txt
可以看出代码行数只是原生Python的1/3，逻辑也比较清晰，代码中包含了mapper、
if
class MRWordCounter(MRJob):
【/home/test/hadoop/word_count.py】
回到实现一个统计文本文件（/home/test/hadoop/input.txt）中所有单词出现的词频功能，
from mrjob.job import MRJob
#pip install mrjob
5）调试方便，无须任何环境支持。
4）支持亚马逊网络数据分析服务ElasticMapReduce（EMR)；
3）支持多种运行方式，包括内嵌方式、本地环境、Hadoop、远程亚马逊；
python word_count.py
MRWordCounter.run()
name
def reducer（self,word,occurrences):
def mapper(self,
yield word,sum(occurrences)
for word in line.split():
Linux公社 www.linuxidc.com
==main
yieldword,1
key,line):
input.txt
#PyPI安装方式
源码安装方式
-ooutput.txt
第12章Python大数据应用详解213
---
## Page 241
中可以看到任务的优先级、map及reduce的总数，如图12-12所示。
214第二部分高级篇
Mrjob 框架的介绍告一段落，下一节重点以实际案例进行说明。
查看Hadoop分析结果文件，内容见图12-13。
访问http://192.168.1.20:50030/jobtrackerjsp，显示的最后一行便是任务执行的信息，从
注意，执行之前需要指定 Hadoop 环境变量，执行结果见图 12-11。
口指定Hadoop任务调度优先级（VERY_HIGH|HIGH），如，--jobconf mapreduce.job.
用于Hadoop环境，支持Hadoop运行调度控制参数，如：
（3）Hadoop（-r hadoop）方式
执行的结果与inline一样，只是运行过程存在差异。
# python word_count.py -r local input.txt >output.txt
用于本地模拟Hadoop 调试，与内嵌（inline）方式的区别是启动了多进程执行每一个任
（2）本地（-rlocal）方式
priority=VERY_HIGH。
reduce.tasks=5。
Linux公社 www.linuxidc.com
图12-10查看输出 output.txt 文件内容
---
## Page 242
CompletedJobs
fob_201408222215_0001
Jobid
201
408222215
0002
Linux公社 www.linuxidc.com
FriAug
Fri Aug
Started
VERY_HIGH
NORMAL
PriorityUser
图12-12已完成任务清单（部分截图）
图12-11-
图12-13查看任务结果文件内容
root
root
streamjob4197546037759856201.jar
Name
任务执行结果（部分截图）
03234573884.jar
第12章Python大数据应用详解
100.00%
100.00%
100.00%
215
100.00%
---
## Page 243
客户端文件上传，详细代码如下：
详细见12.2相关内容。添加上传日志功能作业到crontab，内容如下：
平台，最终实现分布式计算。需要安装（JDK配置环境变量）Hadoop（原版tar包解析即可），
字节数；①访问来源；客户浏览器信息（不具体拆分)。
证的远程用户）；④请求时间；③UTC时差；方法；资源；⑧协议；9状态码；①发送
(20140215)/access.log，日志为默认的Apache定义格式，如：
12.4.1
HTTP状态信息、用户IP信息、连接数／分钟统计等。
个问题，即分布式存储与计算。下面将通过示例介绍如何从Web日志中快速获取访问流量
的方式已经力不从心了。另外一个待解决的问题就是数据存储。Hadoop很好地解决了这两
我们还可以勉强通过shell、awk进行分析，当达到上百GB，甚至上PB级别时，通过脚本
处理大数据的存储与分析问题呢？比如Web服务器的访问log，当日志只有GB单位大小时，
12.4实战分析
216第二部分高级篇
接下来在5台Web服务器部署HDFS的客户端，以便定期上传Web日志到HDFS 存储
站点www.website.com共有5台Web设备，
通过 subprocess.PopenO方法调用Hadoop HDFS 相关外部命令，实现创建 HDFS目录及
在互联网企业中，随着业务量、访问量的不断增长，用户产生的数据也越来越大，如何
import subprocess
【/home/test/hadoop/hdfsput.py】
5523***/usr/bin/python /home/test/hadoop/hdfsput.py>>/dev/nu112>&l
共有12列数据（空格分隔），分别为：①客户端IP；②空白（远程登录名称）；③空白（认
+http://www.google.com/bot.html)"
66.249.65.37
HTTP/1
66.249.65.37
GTB6.5;InfoPath.1;
125.26.28.8
InfoPath.1:.NET CLR 2.0.50727;yie8)"
"Mozilla/4.0(compatible;
gif HTTP/1.1" 200 12014
1187
示例场景
20079220
"Mozilla/4.0
Linux公社 www.linuxidc.com
[01/Aug/2010:09:56:53 +0700] "GET /favicon.ico HTTP/1.1" 200
[01/Aug/2010:09:57:59
[01/Aug/2010:09:59:19
(compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0;
MSIE8.0;
"http://www.kpsw.ac.th/teacher/jitra/page4.htm"
Windows NT 5.1;Trident/4.0;GTB6.5;
+0700]"GET/picture/49-02/DSC02630.jpg
+0700]
，日志文件存放位置：/data/logs/日期
"GET
/elearning/index.php?cal
---
## Page 244
作累加（sum）统计，详细源码如下：
志中小时的每分钟作为key，将对应的行发送字节数作为value，在reducer 操作时对相同key
实现此类需求。下面实现精确到分钟统计网站访问流量，原理是在 mapper操作时将 Web 日
涉及计费，如何快速准确分析当前站点的流量数据至关重要，当然，使用Mrjob可以很轻松
12.4.2网站访问流量统计
网站访问流量作为衡量一个站点的价值、热度的重要标准，另外在CDN服务中流量会
【 /home/test/hadoop/httpflow.py 】
截至目前，数据的分析源已经准备就绪，接下来的工作便是分析了。
com/20140215/access.1og.web5
com/20140215/access.1og.web4
com/20140215/access.1og.web2
com/20140215/access.1og.web1
Found5items
#/usr/local/hadoop-1.2.1/bin/hadoop dfs -ls/user/root/website.com/20140215
在crontab 定时作业运行后，5台Web服务器的日志在HDFS上的信息如下：
for line in putinfo.stdout:
com/"+currdate+"/"+logname], stdout=subprocess.PIPE)
putinfo=subprocess.Popen(["/usr/local/hadoop-1.2.1/bin/hadoop"
'dfs",
exceptException,e:
#创建HDFS目录，目录格式：website.com/20140205
try:
logname="access.log."+webid#HDFS 存储日志名
currdate=datetime.datetime.now().strftime('%Y%m%d')
webid="web1"
rw-r--r--
rw-r--r-
rw-r--r-
rw-r--r-
importdatetime
importsys
rw-r--r-
pass
print line
"-put"
Linux公社www.linuxidc.com
3root supergroup
3root
3root supergroup
3root supergroup
3
#HDFs存储日志标志，其他Web服务器分别为web2、web3、web4、web5
root supergroup
logspath,
supergroup
"+currdate+"/
"hdfs://192.168.1.20:9000/user/root/website.
156541746 2014-02-1523:55/user/root/website
183267834 2014-02-1523:55/user/root/website.
192314554 2014-02-1523:54/user/root/website
134256412 2014-02-15 23:55 /user/root/website.
251245315 2014-02-15 23:53 /user/root/website
第12章
#日志本地路径
Python大数据应用详解217
#上传本地日志到HDES
"dfs"，"-mkdir"
---
## Page 245
Linux公社微信公众号：linuxidc_com
专题
Linux公社（LinuxIDC.com）设置了有一定影响力的Linux专题栏目。
数据中心，LinuxIDC就是关于Linux的数据中心。
Linux现在已经成为一种广受关注和支持的一种操作系统，IDC是互联网
Linux公社（www.Linuxidc.com）于2006年9月25日注册并开通网站，
Hadoop专题RedHat专题SUSE专题红旗Linux专题CentOS
包括：Ubuntu 专题Fedora专题Android 专题Oracle专题
Linux公社主站网址：
技术。
证、SUSE Linux、Android、Oracle、Hadoop、CentOS、MySQL、
Linux公社是专业的Linux系统门户网站，实时发布最新Linux资讯，包括
欢迎点击这里的链接进入精彩的Linux公社网站
www.Linuxidc.com
inux公社
www.linuxidc.com
搜索微信公众号:linuxidc_Com
源技术教程。
订阅专业的最新Linux资讯及开
微信扫一扫
旗下网站:
Linxidc.com
---
## Page 246
势图。
很方便地对数据进行加工，轻松输出比较美观的数据报表。图12-15为网站一天的流量趋
218第二部分高级篇
建议将分析结果数据定期人库MySQL，利用MySQL 灵活、丰富的SQL支持，可以
分析结果见图12-14。
com/20140215
priority=VERY_HIGH
#python
生成Hadoop任务，运行：
if
class MRCounter(MRJob):
import re
from mrjob.job import MRJob
MRCounter.run()
def reducer(self,key,occurrences):
def mapper(self, key, line):
name
E
03
i=0
413486
108
Linux公社 www.linuxidc.com
9532
i+=1
ifi==9andre.match（r"\d(1，)"，flow）：#获取日志第10列-发送的字节数，
ifi==3：#获取时间字段，位于日志的第4列，内容如“[06/Aug/2010:03:19:44”
360
作为value
yield hm，int(flow)#初始化key:value
timerow=flow.split(":")
main
-ohdfs:///output/httpflowhdfs:///user/root/website
图12-14任务分析结果（部分截图）
#相同key“小时：分钟”的value作累加操作
---
## Page 247
即可，详细源码如下：
现，除了基本的 mapper、reducer方法外，还可以添加自定义处理方法，在steps 中添加调用
比如我们关注的200、404、5xx状态等。在此示例中我们利用Mrjob的多步调用的形式来实
12.4.3
【/home/test/hadoop/httpstatus.py】
统计一个网站的 HTTP 状态码比例数据，
if
class MRCounter(MRJob):
importre
from mrjob.job import MRJob
Mb/s
MRCounter.run()
name
def mapper(self,key,line):
def steps(self):
def
网站HTTP 状态码统计
10
yield httpcode,sum(occurrences)