# 【技术分享】深度学习中的逃逸攻击——探究人工智能系统中的安全盲区

##### 译文声明
本文为翻译文章，原文来源：安全客。译文仅供参考，具体内容表达及含义以原文为准。
作者：肖奇学, 许伟林, 李康 @[360 Team Seri0us](http://bobao.360.cn/member/contribute?uid=2967649585)
投稿方式：发送邮件至 linwei#360.cn 或登录网页版在线投稿。

## 简介
在ISC 2017中国互联网安全大会上，举办了人工智能安全论坛。我们将论坛内容整理成一系列文章，本文是该系列的第二篇。

传送门：
[**【技术分享】深度学习框架中的魔鬼——探究人工智能系统中的安全问题**](http://bobao.360.cn/learning/detail/4529.html)

“逃逸攻击的目标是将误判率从0.001%提升到100%的成功率”。虽然深度学习系统经过训练可以对正常输入达到极低的误判率，但当攻击者通过系统化方法生成误判样本时，攻击效率可接近100%，从而实现稳定的逃逸攻击。

## 1. 逃逸攻击简介
逃逸攻击是指攻击者在不改变目标机器学习系统的情况下，通过构造特定输入样本来欺骗系统的攻击。例如，攻击者可以修改恶意软件样本的非关键特征，使其被反病毒系统误判为良性样本，从而绕过检测。这些特意构造的样本被称为“对抗样本”。只要机器学习模型未完美学到判别规则，攻击者就可能构造对抗样本进行欺骗。例如，尽管研究者试图在计算机上模仿人类视觉功能，但由于两者在判别物体时依赖的规则存在差异，对抗样本利用这些差异使机器学习模型得出与人类视觉截然不同的结果（如图1所示）。

![图1: 攻击者生成对抗样本使系统与人类有不同的判断](图1链接)

一个著名的逃逸样本是Ian Goodfellow在2015年ICLR会议上展示的熊猫与长臂猿分类的例子。该系统能够精确区分熊猫与长臂猿等图片，但攻击者可以通过对熊猫图片添加少量干扰，使得深度学习系统将其误判为长臂猿（如图2所示）。

![图2: 在图片中添加扰动导致深度学习系统的错误识别实例](图2链接)

接下来，我们将从攻击者的角度介绍如何系统生成对抗样本来实现稳定的逃逸攻击。不关心技术细节的读者可以直接跳到文章结尾的总结部分。

## 2. 基于机器学习的对抗样本生成
基于机器学习的逃逸攻击可分为白盒攻击和黑盒攻击。白盒攻击需要获取机器学习模型内部的所有信息，然后直接计算得到对抗样本；黑盒攻击则只需知道模型的输入和输出，通过观察模型输出的变化来生成对抗样本。

### 2.1 白盒攻击
深度神经网络是数学上可微的模型，在训练过程中通常使用反向传播算法调整网络参数。假设神经网络的输入是X，类别标签是Y，网络参数是W，输出是F(X)=W*X。训练时，对于每个确定的输入样本X，我们反复调整网络参数W使得输出值F(X)趋向于该样本的类别标签Y。白盒攻击采用同样的方法，但固定网络参数W，反复修改输入样本X使得输出值F(X)趋向于攻击目标Y’。这意味着我们只需修改目标函数及约束条件，即可使用与训练神经网络相同的方法计算对抗样本[3]。

白盒攻击的约束条件是关键部分。从X起始求解X’使得F(X’)=Y’的过程中，必须保证X’的标签不是Y’。例如，对于手写体输入“1”，如果将其改成“2”并使模型判别为“2”，那就不算攻击。因此，引入了距离函数Δ(X, X’)，假设在一定距离内，X’的含义和标签与X一致。距离函数可以选择不同的范数表示，如L2, L∞, 和L0。

L-BFGS是第一种攻击深度学习模型的方法，它使用L2-Norm限制X’的范围，并使用最优化方法L-BFGS计算得到X’。后来基于模型的线性假设，研究者提出了Fast Gradient Sign Method (FGSM)[2] 和DeepFool[4]等新方法。目前最先进的方法是Carlini-Wagner，它分别对多种距离函数进行了优化。

### 2.2 黑盒攻击
黑盒攻击只依赖于机器学习模型的输出，不需要了解模型内部结构。遗传（进化）算法是一种有效的黑盒攻击方法。

遗传算法是在计算机上模拟达尔文生物进化论的一种最优化求解方法。它主要分为两个过程：首先通过基因突变或杂交得到新一代变种，然后以优胜劣汰的方式选择优势变种。这个过程可以周而复始，最终得到所需的样本。

将遗传算法用于黑盒逃逸攻击时，我们利用模型的输出给每个变种打分，F(X’)越接近目标标签Y’则得分越高，高分变种继续演化，最终得到F(X’)=Y’。这种方法已成功应用于欺骗基于机器学习的计算机视觉模型和恶意软件检测器。

## 3. 基于遗传算法的对抗样本生成

### 3.1 对Gmail PDF过滤的逃逸攻击
本文作者许伟林在NDSS大会上发表了一篇名为Automatically Evading Classifiers的论文[5]。研究工作采用遗传编程随机修改恶意软件的方法，成功攻击了两个号称准确率极高的恶意PDF文件分类器：PDFrate 和Hidost。这些逃逸检测的恶意文件都是算法自动修改出来的，无需PDF安全专家介入（如图3所示）。

![图3: 利用进化算法生成恶意PDF对抗变种](图3链接)

同样的算法可以用来对实际应用的机器学习系统进行逃逸攻击。上述工作可以对Gmail内嵌的恶意软件分类器进行攻击，只需修改已知恶意PDF样本的4行代码即可达到近50%的逃逸率，影响了10亿Gmail用户。

### 3.2 利用Fuzzing测试的对抗样本生成
除了对模型和算法的弱点进行分析，黑盒攻击还可以借鉴模糊测试的方法生成对抗样本。以手写数字图像识别为例，我们的目标是生成看起来是“1”但被识别为“2”的对抗图片（如图4所示）。

![图4：针对手写数字图像识别的对抗样本生成](图4链接)

我们主要利用灰盒fuzzing测试的方法实现。首先给定数字“1”的图片作为种子，然后通过对种子图片进行变异。如果机器学习系统将变异后的图片识别为“2”，则认为该图片是对抗样本。

基于Fuzzing测试的对抗样本生成是基于AFL实现的，主要改进包括：
1. 漏洞注入：在机器学习系统中添加一个判断，当图片被识别为2时，人为产生一个crash；
2. 数据变异：考虑文件格式的内容，优先对一些图像内容相关的数据进行变异；
3. 路径导向：在AFL已有的路径导向基础上，增加一些关键数据的导向。

下图5是我们生成的一些对抗样本的例子。

![图5：针对手写数字图像的对抗样本生成结果](图5链接)

基于Fuzzing测试的对抗样本生成方法也可以快速应用于其他AI系统，如人脸识别系统（如图6所示）。

![图6：针对人脸识别系统的对抗样本生成](图6链接)

## 4. 基于软件漏洞进行逃逸攻击
针对AI系统的对抗性攻击旨在让人工智能系统输出错误的结果。以手写图像识别为例，攻击者可以构造恶意图片，使得AI系统在分类识别过程中触发安全漏洞，改变程序的控制流或数据流，从而输出攻击者指定的结果。攻击思路主要有两种：

1. **基于数据流篡改**：利用任意写内存漏洞，直接修改AI系统中的关键数据（如标签、索引等），使系统输出错误结果。
2. **基于控制流劫持**：通过堆溢出、栈溢出等漏洞完成对抗攻击，由于控制流劫持漏洞可以执行任意代码，因此可以控制AI系统输出攻击者预期的结果。

关于软件漏洞造成的问题，我们在本系列第一篇文章中有详细介绍。这里仅做简单介绍，更多细节请参考ISC 2017大会人工智能与安全论坛发布的内容。

## 5. 小结
本文旨在继续介绍被大众忽视的人工智能安全问题。尽管深度学习在处理自然生成的语音图像等方面已达到很高的准确率，但对于恶意构造的输入仍存在巨大的提升空间。随着人工智能应用的普及，相信对逃逸攻击的研究会越来越深入。这些研究包括对抗样本生成及增强深度学习对抗能力，我们将在后续文章中更新相关工作。

## 6. 参考文献
[1] [引用文献]
[2] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy, Explaining and Harnessing Adversarial Examples. International Conference on Learning Representations, 2015.
[3] Nguyen, A., J. Yosinski, and J. Clune, Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. 2015: p. 427-436.
[4] Moosavi Dezfooli, Seyed Mohsen, Alhussein Fawzi, and Pascal Frossard, DeepFool: a simple and accurate method to fool deep neural networks, Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[5] Weilin Xu, Yanjun Qi, and David Evans, Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers, NDSS, 2016.

---

希望以上优化后的文本更加清晰、连贯和专业。如果有任何进一步的修改需求，请随时告知。