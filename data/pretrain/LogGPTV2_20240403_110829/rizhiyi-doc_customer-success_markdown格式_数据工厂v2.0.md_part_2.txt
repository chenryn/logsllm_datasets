2、基于轻量级分布式快照（Snapshot）实现的容错
Flink
能够分布式运行在上千个节点上，将一个大型计算任务的流程拆解成小的计算过程，然后将
task
分布到并行节点上进行处理。在任务执行过程中，能够自动发现事件处理过程中的错误而导致数据不一致的问题，比如：节点宕机、网路传输问题，或是由于用户因为升级或修复问题而导致计算服务重启等。在这些情况下，通过基于分布式快照技术的
Checkpoints技术，将执行过程中的状态信息进行持久化存储。
Checkpoints是使用分布式快照技术checkpoints实现状态的持久化维护，能够将执行过程中的状态信息进行持久化存储。一旦任务出现异常停止，Flink
就能够从 Checkpoints
中进行任务的自动恢复，以确保数据在处理过程中的一致性。
3、基于 JVM 实现独立的内存管理
内存管理是所有计算框架需要重点考虑的部分，尤其对于计算量比较大的计算场景，数据在内存中该如何进行管理显得至关重要。针对内存管理，Flink
实现了自身管理内存的机制，尽可能减少 JVM GC 对系统的影响。另外，Flink
通过序列化/反序列化方法将所有的数据对象转换成二进制在内存中存储，降低数据存储的大小的同时，能够更加有效地对内存空间进行利用，降低
GC 带来的性能下降或任务异常的风险，因此 Flink
较其他分布式处理的框架会显得更加稳定，不会因为 JVM GC
等问题而影响整个应用的运行。
4、Save Points（保存点）
对于 7\*24
小时运行的流式应用，数据源源不断地接入，在一段时间内应用的终止有可能导致数据的丢失或者计算结果的不准确，例如进行集群版本的升级、停机运维操作等操作。值得一提的是，Flink
通过 Save Points
技术将任务执行的快照保存在存储介质上，当任务重启的时候可以直接从事先保存的
Save Points 恢复原有的计算状态，使得任务继续按照停机之前的状态运行，Save
Points 技术可以让用户更好地管理和运维实时流式应用。
## Flink状态管理和容错
流处理需要考虑多个使用者的情况。如果计算每个使用者出现的次数，需要让同一个使用者的出现事件流到同一运算代码，这跟其他批次需要做
group by 是同样的概念，所以跟 Stream 一样需要做分区，设定相应的
key，然后让同样的 key
流到同一个实例做同样的运算。当数据量足够大时，就需要一定的管理机制，以免大数据分区超出节点负荷。
Flink有状态的分散式流式处理相当于根据输入流的 key 重新分区的
状态，当分区进入 stream 之后，这个 stream 会累积起来的状态也变成
copartiton 了。第二个重点是 embeded local state
backend。有状态分散式流式处理的引擎，状态可能会累积到非常大，当 key
非常多时，状态可能就会超出单一节点的 memory
的负荷量，这时候状态必须有状态后端去维护它；在这个状态后端在正常状况下，用
in-memory 维护即可。
状态（State）是计算过程中的数据信息，在容错恢复和Checkpoint
中有重要的作用，流计算在本质上是增量处理（Incremental
Processing），因此需要不断查询保持状态；另外，为了确保Exactly- once
语义，需要数据能够写入到状态中；而持久化存储，能够保证在整个分布式系统运行失败或者挂掉的情况下做到Exactly-
once，这是状态的另外一个价值。
### 有状态计算
有状态计算是指在程序计算过程中，在Flink程序内部存储计算产生的中间结果，并提供给后续Function或算子计算结果使用。状态数据可以维系在本地存储中，这里的存储可以是Flink的堆内存或者堆外内存，也可以借助第三方的存储介质，例如Flink中已经实现的RocksDB，当然用户也可以自己实现相应的缓存系统去存储状态信息，以完成更加复杂的计算逻辑。
和状态计算不同的是，无状态计算不会存储计算过程中产生的结果，也不会将结果用于下一步计算过程中，程序只会在当前的计算流程中实行计算，计算完成就输出结果，然后下一条数据接入，然后再处理。无状态计算实现的复杂度相对较低，实现起来较容易，但是无法完成提到的比较复杂的业务场景，例如下面的例子：
-   用户想实现CEP（复杂事件处理），获取符合某一特定事件规则的事件，状态计算就可以将接入的事件进行存储，然后等待符合规则的事件触发；
-   用户想按照分钟、小时、天进行聚合计算，求取当前的最大值、均值等聚合指标，这就需要利用状态来维护当前计算过程中产生的结果，例如事件的总数、总和以及最大，最小值等；
-   用户想在Stream上实现机器学习的模型训练，状态计算可以帮助用户维护当前版本模型使用的参数；
-   用户想使用历史的数据进行计算，状态计算可以帮助用户对数据进行缓存，使用户可以直接从状态中获取相应的历史数据。
以上场景充分说明了状态计算在整个流式计算过程中重要性，可以看出，在Flink引入状态这一特性，能够极大地提升流式计算过程中数据的使用范围以及指标计算的复杂度，而不再需要借助类似于Redis外部缓存存储中间结果数据，这种方式需要频繁地和外部系统交互，并造成大量系统性能开销，且不易保证数据在传输和计算过程中的可靠性，当外部存储发生变化，就可能会影响到Flink内部计算结果。
### Checkpoints和Savepoints
#### Checkpoints检查点机制
Flink中基于异步轻量级的分布式快照技术提供了Checkpoints容错机制，分布式快照可以将统一时间点Task/Operator的状态数据全局统一快照处理。
Flink会在输入的数据集上间隔性地生成checkpoint
barrier，检查点屏障跟普通记录一样。它们由算子处理，但并不参与计算，而是会触发与检查点相关的行为。Checkpoint
barrier会在算子之间流动，当读取输入流的数据源遇到检查点屏障时，它将其在输入流中的位置保存到稳定存储中。例如在Kafka
Consumer算子中维护Offset状态，当系统出现问题无法从Kafka中消费数据时，可以将Offset记录在状态中，当任务重新恢复时就能够从指定的偏移量开始消费数据。Flink的存储机制是插件化的，稳定存储可以是分布式文件系统，如HDFS、S3
或 MapR-FS。
#### Savepoints机制
保存点与检查点的工作原理一致，只不过检查点是自动触发的，而保存点需要命令行触发或者web控制台触发。和检查点一样，保存点也保存到稳定存储当中，用户可以从保存点重启作业，而不用从头开始。
保存点的作用：
\(1\)
应用程序代码升级：假设你在已经处于运行状态的应用程序中发现了一个bug，并且希望之后的事件都可以用修复后的新版本来处理。通过触发保存点并从该保存点处运行新版本，下游的应用程序并不会察觉到不同（当然，被更新的部分除外）。
\(2\) Flink版本更新：Flink
自身的更新也变得简单，因为可以针对正在运行的任务触发保存点，并从保存点处用新版本的
Flink 重启任务。
\(3\)
维护和迁移：使用保存点，可以轻松地"暂停和恢复"应用程序。这对于集群维护以及向新集群迁移的作业来说尤其有用。此外，它还有利于开发、测试和调试，因为不需要重播整个事件流。
\(4\)
假设模拟与恢复：在可控的点上运行其他的应用逻辑，以模拟假设的场景，这样做在很多时候非常有用。
\(5\) A/B
测试：从同一个保存点开始，并行地运行应用程序的两个版本，有助于进行 A/B
测试。
### 状态管理器
在Flink中提供了StateBackend来存储Checkpoints过程中的状态数据。
#### StateBacked类别
Flink中一共实现了三种类型的状态管理器，包括基于内存的MemoryStateBackend、基于文件系统的FsStateBackend以及基于RockDB作为存储介质的RocksDBStateBackend。这三种类型的StateBackend均能有效地存储Flink流式计算过程中产生的状态数据，默认情况下Flink使用的是内存作为状态管理器，下面分别对每种状态管理器的特点进行说明。
**1.MemoryStateBackend**
基于内存的状态管理器将状态数据全部存储在JVM堆内存中，包括用户在使用DataStream
API中创建的Key/Value
State，窗口中缓存的状态数据，以及触发器等数据。基于内存的状态管理具有非常快速和高效的特点，但也具有非常多的限制，最主要的就是内存的容量限制，一旦存储的状态数据过多就会导致系统内存溢出等问题，从而影响整个应用的正常运行。同时如果机器出现问题，整个主机内存中的状态数据都会丢失，进而无法恢复任务中的状态数据。因此从数据安全的角度建议用户尽可能地避免在生产环境中使用MemoryStateBackend。
Flink将MemoryStateBackend作为默认状态后端管理器，也可以通过如下参数配置初始化MemoryStateBackend，其中"MAX_MEM_STATE_SIZE"指定每个状态值最大的内存使用大小。
new MemoryStateBackend(MAX_MEM_STATE_SIZE, false);
在Flink中MemoryStateBackend具有如下特点，需要用户在选择使用中注意：
·聚合类算子的状态会存储在JonManager内存中，因此对于聚合类算子比较多的应用会对JobManager的内存有一定压力，进而对整个集群会造成较大负担。
·尽管在创建MemoryStateBackend时可以指定状态初始化内存大小，但是状态数据传输大小也会受限于Akka框架通信的"akka.framesize"大小限制（默认：10485760bit），该指标表示在JobManager和TaskManager之间传输数据的最大消息容量。
·JVM内存容量受限于主机内存大小，也就是说不管是JobManager内存还是在TaskManager的内存中维护状态数据都有内存的限制，因此对于非常大的状态数据则不适合使用MemoryStateBackend存储。
因此综上可以得出，MemoryStateBackend比较适合用于测试环境中，并用于本地调试和验证，不建议在生产环境中使用。但如果是应用状态数据量不是很大，例如使用了大量的非状态计算算子，也可以在生产环境中使用MemoryStateBackend，否则应该改用其他更加稳定的StateBackend作为状态管理器，例如后面讲到的FsStateBackend和RocksDBStateBackend等。
**2. FsStateBackend**
和MemoryStateBackend有所不同，FsStateBackend是基于文件系统的一种状态管理器，这里的文件系统可以是本地文件系统，也可以是HDFS分布式文件系统。
new FsStateBackend(path, false);
如以上创建FsStateBackend的实例代码，其中path如果为本地路径，其格式为file:///data/flink/checkpoints，如果path为HDFS路径，其格式为"hdfs://nameservice/flink/checkpoints"。FsStateBackend中第二个Boolean类型的参数指定是否以同步的方式进行状态数据记录，默认采用异步的方式将状态数据同步到文件系统中，异步方式能尽可能避免在Checkpoint的过程中影响流式计算任务。如果用户想采用同步地方式记录检查点数据，则将第二个参数指定为true即可。相比于MemoryStateBackend，FsStateBackend更适合任务状态非常大的情况，例如在应用中含有时间范围非常长的窗口计算，或Key/Value
State状态数据量非常大的场景，这时系统内存不足以支持状态数据的存储。同时基于文件系统存储最大的好处是相对稳定，同时借助于像HDFS分布式文件系统中具有三副本的备份策略，能最大程度保证状态数据的安全性，不会出现因为外部故障而导致任务无法恢复等问题。
**3.RocksDBStateBackend**
RocksDBStateBackend是Flink中内置的第三方状态管理器，和前面的状态管理器不同，RocksDBStateBackend需要单独引入相关的依赖包到工程中。通过初始化RockDBStateBackend类，可使得到RockDBStateBackend实例类。
//创建RocksDBStateBackend实例类
new RocksDBStateBackend(path);
RocksDBStateBackend采用异步的方式进行状态数据的Snapshot，任务中的状态数据首先被写入RockDB中，然后再异步地将状态数据写入文件系统中，这样在RockDB仅存储正在计算的热数据，对于长时间才更新的数据则写入磁盘中进行存储。而对于体量比较小的元数据状态，则直接存储在JobManager的内存中。
与FsStateBackend相比，RocksDBStateBackend在性能上要比FsStateBackend高一些，主要是因为借助于RocksDB存储了最新热数据，然后通过异步的方式再同步到文件系统中，但RocksDBStateBackend和MemoryStateBackend相比性能就会较弱一些。
需要注意的是RocksDB通过JNI的方式进行数据的交互，而JNI构建在byte\[\]数据结构之上，因此每次能够传输的最大数据量为2\^31字节，也就是说每次在RocksDBStateBackend合并的状态数据量大小不能超过2\^31字节限制，否则将会导致状态数据无法同步，这是RocksDB
采用JNI方式的限制，用户在使用过程中应当注意。
综上可以看出，RocksDBStateBackend和FsStateBackend一样，适合于状态任务数据非常大的场景。在Flink最新版本中，已经提供了基于RocksDBStateBackend实现的增量Checkpoints功能，极大地提高了状态数据同步到介质中的效率和性能，在后续的社区发展中，RocksDBStateBackend也会作为状态管理器重点使用的方式之一。
#### 状态管理器配置
在StateBackend应用过程中，除了MemoryStateBackend不需要显示配置之外，其他状态管理器都需要进行相关的配置。在Flink中包含了两种级别的StateBackend配置：一种是应用层面配置，配置的状态管理器只会针对当前应用有效；另外一种是整个集群的默认配置，一旦配置就会对整个Flink集群上的所有应用有效。
**1.应用级别配置**
在Flink应用中通过对StreamExecutionEnvironment提供的setStateBackend()方法配置状态管理器，代码清单4-1通过实例化FsStateBackend，然后在setStateBackend方法中指定相应的状态管理器，这样在后续应用的状态管理都会基于HDFS文件系统进行。
代码清单4-1 设定应用层面的StateBackend
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
StreamExecutionEnvironment env =
StreamExecutionEnvironment.getExecutionEnvironment();
env. setStateBackend(new
FsStateBackend("hdfs://namenode:40010/flink/checkpoints"));
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
如果使用RocksDBStateBackend则需要单独引入rockdb依赖库，如代码清单4-2所示，将相关的Maven依赖配置引入到本地工程中。
代码清单4-2 RocksDBStateBackend Maven配置
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
\
\org.apache.flink\
\flink-statebackend-rocksdb_2.11\
\1.7.0\
\
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
经过上述配置后，如代码清单4-3所示就可以使用RocksDBStateBackend作为状态管理器进行算子或者数据的状态管理。其中需要配置的参数和FsStateBackend基本一致。
代码清单4-3 RocksDBStateBackend应用配置
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
StreamExecutionEnvironment env =
StreamExecutionEnvironment.getExecutionEnvironment();
env. setStateBackend(new
RocksDBStateBackend("hdfs://namenode:40010/flink/checkpoints"));
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
2.集群级别配置
前面已经提到除了能够在应用层面对StateBackend进行配置，应用独立使用自己的StateBackend之外，Flink同时支持在集群中配置默认的StateBackend。具体的配置项在flink-conf.yaml文件中，如下代码所示，参数state.backend指明StateBackend类型，state.checkpoints.dir配置具体的状态存储路径，代码中使用filesystem作为StateBackend，然后指定相应的HDFS文件路径作为state的checkpoint文件夹。
state.backend: filesystem
\# Directory for storing checkpoints
state.checkpoints.dir: hfds://namenode:40010/flink/checkpoints
如果在集群默认使用RocksDBStateBackend作为状态管理器，则对应在flink-conf.yaml中的配置参数如下：
state.backend.rocksdb.checkpoint.transfer.thread.num: 1
state.backend.rocksdb.localdir: /var/rockdb/flink/checkpoints
state.backend.rocksdb.timer-service.factory: HEAP
·state.backend.rocksdb.checkpoint.transfer.thread.num：用于指定同时可以操作RocksDBStateBackend的线程数量，默认值为1，用户可以根据实际应用场景进行调整，如果状态量比较大则可以将此参数适当调大。
·
state.backend.rocksdb.localdir：用于指定RocksDB存储状态数据的本地文件路径，在每个TaskManager提供该路径存储节点中的状态数据。
·state.backend.rocksdb.timer-service.factory：用于指定定时器服务的工厂类实现类，默认为"HEAP"，也可以指定为"RocksDB"。
1.  ## 运维监控
    1.  ### 日常维护命令
-   启动集群：bin/start-cluster.sh
-   停止集群：bin/stop-cluster.sh
-   启动jobmanager（如果集群中的jobmanager进程挂了，执行下面命令启动）：
> bin/jobmanager.sh start
-   停止：bin/jobmanager.sh stop
-   启动taskmanager（添加新的taskmanager节点或者重启taskmanager节点）：
> bin/taskmanager.sh start
-   停止：bin/taskmanager.sh stop
    1.  ### Flink监控
获取 Metrics 有三种方法，首先可以在 WebUI 上看到；其次可以通过 RESTful
API 获取，RESTful API
对程序比较友好，比如写自动化脚本或程序，自动化运维和测试，通过 RESTful
API 解析返回的 Json 格式对程序比较友好；最后，还可以通过 Metric Reporter
获取，监控主要使用 Metric Reporter 功能。
Flink主要监控点：
·job 的内存、网络、CPU 的使用情况
·job 的状态，running or dead
·job exception
配置监控数据写入influxdb：
1.  修改配置文件 flink-conf.yaml
> metrics.reporter.influxdb.class:
> org.apache.flink.metrics.influxdb.InfluxdbReporter
>
> metrics.reporter.influxdb.host: node1
>
> metrics.reporter.influxdb.port: 8086
>
> metrics.reporter.influxdb.db: te
2.  复制flink-metrics-influxdb_2.12-1.8.0.jar包 到lib
3.  启动Flink
（更多监控内容可参考文档：）
### Flink问题排查思路
1．查看反压，如果看到任务的状态为"OK"，则表明没有背压。如果显示"HIGH"，意味着任务存在反压。反压原因需要进一步分析，查看是否需要增加任务的并行度，提高处理速度。
![](media/image12.png){width="5.764583333333333in"
height="2.421527777777778in"}
2．查看checkpoint 时间是否很长，状态是不是很大.
![](media/image13.png){width="5.7659722222222225in"
height="2.4520833333333334in"}
3．查看延迟和吞吐，资源利用，比如查看kafka积压，服务器CPU、内存使用情况。
4．查看GC情况
1.  ## 性能优化
    1.  ### 反压监控与优化
反压（Backpressure）在流式系统中是一种非常重要的机制，主要作用是当系统中下游算子的处理速度下降，导致数据处理速率低于数据接入的速率时，通过反向背压的方式让数据接入的速率下降，从而避免大量数据积压在Flink系统中，最后系统无法正常运行。Flink具有天然的反压机制，不需要通过额外的配置就能够完成反压处理。
#### Backpressure进程抽样
当在FlinkUI中切换到Backpressure页签时，Flink才会对整个Job触发反压数据的采集，反压过程对系统有一定的影响，主要因为JVM进程采样成本较高。
![](media/image14.png){width="5.763888888888889in"
height="2.6131944444444444in"}
通过在页面中点击Back
pressure页签触发反压检测，整个采样过程大约会持续5s，每次采样的间隔为50ms，持续100次。同时，为了避免让TaskManager过多地采样Stack
Trace，即便页面被刷新，也要等待60s后才能触发下一次Sampling过程。
#### Backpressure页面监控
通过触发JVM进程采样的方式获取到反压检测数据，同时Flink会将反压状态分为三个级别，分别为OK、LOW、HIGH级别，其中OK对应的反压比例为大于0小于10%，LOW对应的反压比例大于10%小于50%，HIGH对应的反压比例大于50%小于100%。