utilization behaviors. Using this motivational data, in the
next section we present two fault tolerance techniques to
guarantee correct execution despite the existence of faulty
SIMT lanes.
III. PROPOSED FAULT TOLERANCE TECHNIQUES
A. Thread Shufﬂing
The ﬁrst technique we explore is called thread shufﬂing.
Thread shufﬂing leverages the under-utilization of the SIMT
lanes to reroute active threads, originally mapped to faulty
lanes, to idle healthy SIMT lanes. After the execution on
the healthy lanes completes, threads are rerouted to their
original lanes in order to write their results to the correct
register ﬁle banks. As mentioned before, several warps have
less than 32 active threads due to the divergence phenomena
and insufﬁcient parallelism. Fig. 2 indicates that there are
plenty of opportunities to enable thread shufﬂing between
faulty and healthy lanes due to the under-utilization of the
SIMT lanes.
Implementing a perfect SP-wide thread shufﬂing requires
data from one faulty active lane to be forwarded to any other
healthy idle lane within the SP. Thus, this approach requires
a 16X16 crossbar to enable thread shufﬂing. In order to
reduce the area and timing overhead of the crossbar, we take
advantage of GPGPUs’ cluster-based implementation [12]
433433433
and limit thread shufﬂing to be within a cluster, which
we term as intra-cluster thread shufﬂing. When a thread is
mapped to a faulty SIMT lane, intra-cluster shufﬂing seeks
to reroute the thread to the nearest healthy idle lane within
the same cluster, if any is available.
Fig. 4a gives an example of intra-cluster thread shufﬂing
with a cluster size of four. In the ﬁgure, the rightmost SIMT
lane is assumed to be faulty as indicated by the red cross
mark. The 0/1 bit vector on top of the SIMT lanes represents
the active mask of the four threads mapped to the cluster.
The second lane from the right is idle (i.e. active mask bit
is ’0’) and healthy while the faulty rightmost lane has an
active thread mapped to it (i.e. active mask bit is ’1’). To
handle this scenario, intra-cluster thread shufﬂing reroutes
the active thread, originally assigned to the faulty lane, to
the second lane from the right. Clearly, the effectiveness
of intra-cluster thread shufﬂing depends on the cluster size.
As cluster size increases, more shufﬂing opportunities are
expected at the cost of providing a larger crossbar design. A
sensitivity analysis of the cluster size and the expected fault
tolerance improvement is presented shortly.
Another important parameter that affects the efﬁciency of
intra-cluster thread shufﬂing is the way threads are mapped
to the SIMT lanes. Fig. 4b illustrates one approach that
sequentially maps threads to SIMT lanes (SEQ), such that
thread 0 is mapped to lane 0, thread 1 is mapped to lane 1
and so on. During divergence if the active threads are
uniformly distributed across all 32 lanes, then the sequential
mapping creates equal opportunities to have idle lanes across
all SIMT clusters.
However, divergence does not lead to uniform distribution
of active threads. Rather our empirical observations showed
that active threads tend to be coalesced into a few nearby
lanes. One reason for this behavior is that when a warp
executes a load/store instruction, consecutive threads within
a warp tend to access consecutive memory locations, which
is called memory coalescing and is a common phenomenon
in GPGPU applications. Hence, when a thread misses in the
cache there is a high chance that its neighbors are going to
miss as well. Thus memory divergence causes the threads
that hit in the cache and those that miss in the cache to be
spatially grouped together. Similarly, for applications with
few active threads due to limited parallelism, the active
threads are packed together and mapped to consecutive
SIMT lanes.
We empirically validate the claim that active threads are
usually clustered into close-by SIMT lanes with sequential
thread mapping. Fig. 5a shows the fraction of time a
given number of threads (0, 1, 2, 3, or 4 active threads)
being mapped to a SIMT cluster of four lanes for different
benchmarks. The majority of the time, SIMT clusters are
either fully occupied with four active threads mapped to
them or fully idle with zero active threads mapped to them.
As a result, fewer intra-cluster thread shufﬂing opportunities
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 
1 
1 
0 
1 
C 
C 
C 
C 
T0  T1  T2  T3 
T4  T5  T6  T7 
T0  T2  T4  T6 
T1  T3  T5  T7 
T0  T7  T1  T6 
T2  T5  T3  T4 
C 
C 
C 
C 
C 
C 
C 
C 
C 
C 
C 
C 
C 
C 
C 
C 
L 
L 
L 
L 
L 
L 
L 
L 
(a) Thread Shufﬂing
(b) Sequential (SEQ) Mapping
(c) Round-Robin (RR) Mapping
(d) Butterﬂy (BF) Mapping
Figure 4: Thread Shufﬂing and Mapping Techniques
are going to be available because the active threads are not
evenly distributed across the clusters, rather they are bundled
together.
To overcome this limitation, one can use the round-
robin thread to cluster mapping (RR) proposed in [13] and
illustrated in Fig. 4c. In this mapping thread 0 is mapped to
cluster 0, and thread 1 is mapped to cluster 1 and so on. The
round-robin thread to cluster mapping helps to distribute the
active threads evenly across the SIMT clusters to increase
intra-cluster thread shufﬂing opportunities. Fig. 5b shows
the effect of the round-robin mapping. The Y-axis shows
the fraction of time a given number of threads (0, 1, 2, 3,
or 4 active threads) being mapped to a SIMT cluster of four
lanes for different benchmarks. It is clear that the round-
robin mapping increases the frequency of occurrences where
1, 2, or 3 active threads are mapped to a SIMT cluster and
decreases the frequency of occurrences where 0 or 4 active
threads are mapped, which is precisely what is needed for
intra-cluster thread shufﬂing to work well.
In addition to round-robin we also evaluated butterﬂy
(BF) mapping approach. This approach is illustrated in
Fig. 4d. Every cluster is populated by mapping threads
from the least signiﬁcant side and most signiﬁcant side
interchangeably which helps to distribute the active threads,
which are usually packed at one side, across clusters. Finally,
we also evaluated optimal mapping policy (OPT), which
assumes that active threads of every warp, irrespective of
their positions, are uniformly distributed across clusters.
Clearly, the OPT mapping is unrealistic to implement in
practice because it requires run-time information about the
positions of active threads and redistributing them uniformly
across all clusters. However, evaluating the OPT mapping is
still important because it provides a reference point to mea-
sure the efﬁciency of the other three mapping approaches,
in terms of the intra-cluster thread shufﬂing opportunities
available.
To better understand the dependency of
intra-cluster
thread shufﬂing opportunities on SIMT cluster size and
thread mapping, we perform a sensitivity analysis using
four cluster sizes: 2, 4, 8, and 16. For each cluster size,
we measure the weighted average number of shufﬂing op-
portunities for the four thread mapping techniques: sequen-
tial (SEQ), round-robin (RR), butterﬂy (BF), and optimal
(OPT). In every cycle, we compute the number of shufﬂing
opportunities for each cluster and sum the opportunities
across all clusters. The number of shufﬂing opportunities
per cluster is measured as the minimum of: 1) number of
active threads mapped to the cluster and 2) number of idle
SIMT lanes in the cluster. After the number of opportunities
for each benchmark is computed, we calculate the weighted
average across all benchmarks according to the benchmarks
execution times.
Fig. 6 plots the weighted average number of shufﬂing
opportunities for each (cluster size,thread mapping) pair. The
ﬁrst observation is that for sequential and butterﬂy mappings,
the shufﬂing opportunities are limited at small cluster sizes
but the opportunities approximately double with cluster size.
On the other hand, the number of opportunities with round-
robin and optimal mappings is independent from the cluster
size. The second observation is that the round-robin mapping
achieves nearly the same number of opportunities as optimal
mapping. At any given cluster size, the round-robin scheme
outperforms the sequential and butterﬂy mappings. Based
on these results, in the rest of the paper, we use round-robin
thread mapping with cluster-size of four SIMT lanes because
it
is the same cluster size used in existing commercial
GPGPUs [3] [12].
Intra-cluster thread shufﬂing is sufﬁcient as long as the
number of active threads mapped to each SIMT cluster is
less than or equal to the number of healthy lanes within
the cluster. After the warp completes execution,
threads
are reassembled back to their original lanes so that they
can write back their results to the correct register ﬁle
banks. Thus intra-cluster thread shufﬂing is transparent to
the software and to other micro-architectural blocks inside
the streaming multiprocessor (SM). The precise architectural
support necessary for this approach will be described in
section IV.
Intra-cluster thread shufﬂing with round-robin mapping
is inspired by the intra-warp DMR proposed in [13]. Both
techniques leverage the under-utilization in the SIMT lanes,
but for different purposes. Intra-warp DMR redundantly
executes active threads on idle SIMT lanes and compare
results to detect operational faults. On the other hand, in this
research we use intra-cluster thread shufﬂing to tolerate hard
faults in SIMT lanes by rerouting active threads, originally
mapped to faulty lanes, to healthy idle lanes.
B. Dynamic Warp Deformation
Although intra-cluster thread shufﬂing can be sufﬁcient
for many fault scenarios, there are some cases where intra-
cluster thread shufﬂing is not sufﬁcient
to tolerate the
existent hard faults. A simple example is when there are
32 active threads within the warp and there is at least one
434434434
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 

4





(




1



















4











1


(.
(+
(
(

(+.

(a) Sequential (SEQ) Mapping
(b) Round-Robin (RR) Mapping
Figure 5: Thread Mapping Impact on Shufﬂing Opportunities
sub-warps. In this example, one sub-warp is assigned the
two leftmost threads and the other sub-warp is assigned
the two rightmost threads. The second sub-warp relies on
intra-cluster shufﬂing to forward the active thread, originally
mapped to the faulty lane, to an idle healthy lane as indicated
by the curved arrow. Fig. 7b shows another situation where
the SIMT cluster suffers from three faulty lanes while having
three active threads mapped it. In this case, the warp needs
to be divided into three sub-warps, each one is assigned a
single active thread. The two rightmost sub-warps rely on
intra-cluster thread shufﬂing to provide fault tolerance.
In the examples discussed so far, we have considered
a single SIMT cluster. However, each SP in the baseline
architecture consists of four SIMT clusters. Different clusters
may suffer from different numbers of faulty lanes. In addi-
tion, different numbers of active threads could be mapped
to different clusters at different times based on the running
application. So, the general solution is to compute the num-
ber of sub-warps required according to each cluster within
the SP. Then warp deformation is performed according to
the maximum number of sub-warps required among all
clusters. For example, Fig. 8 shows cluster 0 with two faulty
lanes and cluster 1 with a single faulty lane. Four active
threads are mapped to cluster 0 and three active threads
are mapped to cluster 1. When considering each cluster
independently, cluster 0 requires two sub-warps because it
can only issue two threads at a time. Whereas, cluster 1
does not require any deformation and intra-cluster thread
shufﬂing is sufﬁcient. Since deformation is done at the warp
level, not at the cluster level, we are forced to deform the
original warp into two sub-warps as dictated by cluster 0.
The two sub-warps are shown in the ﬁgure.
When a warp is deformed,
the sub-warps are issued
consecutively one after the other on the same SP. When all
sub-warps complete execution successfully, they are merged
again to form the original warp before writing their results
back to the register ﬁle banks.
C. Inter-SP Warp Shufﬂing
The performance overhead of the dynamic warp defor-
mation technique is directly proportional to the number of
sub-warps needed to tolerate the faulty SIMT lanes. Hence,
it is important to minimize the number of sub-warps or
Figure 6: Intra-Cluster Thread Shufﬂing Analysis
faulty lane within the streaming processor (SP). In such
scenario, even a perfect SP-wide thread shufﬂing scheme
cannot provide fault tolerance. To handle such cases, we
propose dynamic warp deformation.
Dynamic warp deformation splits the warp into multi-
ple sub-warps with fewer active threads. The sub-warps
are scheduled consecutively one after the other. When all
sub-warps complete their execution, the original warp is
reassembled and the results for all active threads are written
to the register ﬁle banks at once. As the number of active
threads per sub-warp is reduced, more intra-cluster thread
shufﬂing opportunities are created to handle the faults.
Prior research in [11] and [16] proposed run-time large
warp formation to reduce the performance overhead caused
by branch divergence. On the contrary, we propose warp
deformation to improve fault tolerance capability of future
GPGPUs.
Fig. 7a shows how warp deformation is used when the
number of active threads is greater than the number of
healthy SIMT lanes in the cluster. The 0/1 bit vector on
top of the four lanes represents the active mask of the
threads to be issued. As shown in the ﬁgure, the rightmost
SIMT lane in the cluster is considered faulty as indicated
by the red cross mark, yet an active thread is mapped to it.
There are four active threads mapped to the cluster and our
initial proposal to use intra-cluster shufﬂing cannot provide
fault tolerance in this scenario. Instead we rely on warp
deformation to divide the original warp into two sub-warps.
The purpose of warp deformation is to distribute the four
active threads mapped to the cluster evenly across the two
435435435
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 
1 
1 
1 
1 
L 
L 
L 
C 
1 
1 
0 
0 
0 
0 
1 
1 
1 
0 
0 
0 
1 
1 
0 
1 
L 
0 
L 
1 
L 
0 
C 
0 
0 
0 
0 
1 
L 
L 
L 
C 
L 
L 
L 
C 
L 
L 
L 
C 
L 
L 
L 
C 