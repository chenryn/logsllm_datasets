### 6.1 Collection Performance

We used LMBench to microbenchmark the impact of LPM on system calls, as well as file and memory latencies. Table 1 shows the averaged results over 10 trials for each kernel, with a percent overhead calculation relative to the Vanilla kernel. For most measures, the performance differences between LPM and Vanilla are negligible. However, when comparing Vanilla to Provmon, several operations exhibit notable overhead: `stat`, `open/close`, file creation, and deletion. These benchmarks involve LMBench manipulating a temporary file in `/usr/tmp/lat_fs/`. Since an absolute path is provided, LMBench must first traverse the path to the file, resulting in the creation of three different provenance events in Provmon’s `inode_permission` hook, which are then transmitted to user space via the kernel relay.

Although the overheads for these operations appear significant, logging these three events imposes only approximately 1.5 microseconds per traversal. Additionally, the overhead for opening and closing files is significantly higher than that for reads and writes (Null I/O). This suggests that the higher `open/close` costs may be amortized over regular system use. A provenance module could further reduce this overhead by maintaining state about past events within the kernel, thus blocking the creation of redundant provenance records.

#### Benchmarking Results
| Test                | Vanilla (sec) | Provmon (sec) | Overhead (%) |
|---------------------|---------------|---------------|--------------|
| Kernel Compile      | 598           | 614           | 2.7          |
| Postmark            | 25            | 27            | 7.5          |
| Blast               | 376           | 390           | 4.8          |

To gain a more practical understanding of LPM's costs, we performed multiple benchmark tests representing realistic system workloads. Each trial was repeated five times to ensure consistency. The results are summarized in Table 2. For the kernel compile test, we recompiled the kernel source (in a fixed configuration) while booted into each of the kernels, using 16 threads. The LPM scaffolding (without an enabled module) was not included, as it imposed less than 1% overhead. Despite seemingly high overheads for file I/O, Provmon imposed just 2.7% overhead on kernel compilation, or 16 seconds.

The Postmark test simulates the operation of an email server, configured to run 15,000 transactions with file sizes ranging from 4 KB to 1 MB in 10 subdirectories, with up to 1,500 simultaneous transactions. The Provmon module imposed just 7.5% overhead on this task. To estimate LPM's overhead for scientific applications, we ran the BLAST benchmarks, which simulate biological sequence workloads from the National Institutes of Health.

For kernel compilation and Postmark, Provmon outperforms the PASS system, which exacted 15.6% and 11.5% overheads, respectively [43]. Provmon introduces comparable kernel compilation overhead to Hi-Fi (2.8%) [48]. It is difficult to compare our BLAST results to SPADE and PASS, as both used custom workloads instead of publicly available benchmarks. SPADE reports an 11.5% overhead on a large workload [29], while PASS reports just a 0.7% overhead. Overall, LPM collection either meets or exceeds the performance of past systems while providing additional security assurances.

### 6.2 Storage Overhead

A major challenge in automated provenance collection is the storage overhead. We plotted the growth of provenance storage using different recorders during the kernel compilation benchmark (Figure 6). LPM generated 3.7 GB of raw provenance, which required only 450 MB of storage with the Gzip recorder, though provenance cannot be efficiently queried in this format. The SNAP recorder builds an in-memory provenance graph, and we approximated the storage overhead by polling the virtual memory consumed by the recorder process in the `/proc` filesystem. The SNAP graph required 1.6 GB of storage, a reduction from the raw provenance stream due to the elimination of redundant events. In contrast, the PASS system generates 1.3 GB of storage overhead during kernel compilation while collecting less information (e.g., shared memory). LPM's storage overheads are thus comparable to other provenance-aware systems.

### 6.3 Query Performance (PB-DLP)

We evaluated query performance using our exemplar PB-DLP application and LPM’s SNAP recorder. The provenance graph was populated using the routine from the kernel compile benchmark, yielding a raw provenance stream of 3.7 GB, which was translated by the recorder into a graph of 6,513,398 nodes and 6,754,059 edges. We used the graph to issue ancestry queries, traversing subgraphs to find the data object ancestors of a given file. To avoid skewing results, we only considered objects with more than 50 ancestors.

In the worst case, a node with 17,696 ancestors, the query returned in just 21 milliseconds. This indicates that we can query object ancestries at line speed for network activity. We are confident that this approach can scale to large databases through pre-computation of expensive operations at data ingest, making it a promising strategy for provenance-aware distributed systems. However, these results are highly dependent on the size of the graph. Our test graph, while large, would be dwarfed by the provenance on long-lived systems. Fortunately, there are various techniques for reducing these costs, such as caching provenance graph traversals [7], deduplication [63, 62], and policy-based pruning [6, 13].

### 6.4 Message Commitment Protocol

Under each kernel configuration, we performed iperf benchmarking to measure LPM’s impact on TCP throughput. iperf was launched in both client and server mode over localhost, with the client using 16 threads, two per CPU. Our results are shown in Figure 8. The Vanilla kernel reached 4490 Mbps. While the LPM framework imposed negligible cost compared to the vanilla kernel (4480 Mbps), Provmon’s DSA-based message commitment protocol reduced throughput by an order of magnitude (482 Mbps). Using `printk` instrumentation, we found that the average overhead per packet signature was 1.2 ms. This result is consistent with IPsec performance, where the Authentication Header (AH) mode, using an HMAC-based approach, can reduce throughput by up to half [16].

An HMAC approach is a viable alternative to establish integrity and data origin authenticity and would also fit into the options field, but would require the negotiation of IPsec security associations. Our message commitment protocol is fully interoperable with other hosts and does not require a negotiation phase before communication. Another option for increasing throughput is to employ CPU instruction extensions [30] and security co-processors [19] to accelerate DSA. A batch signature scheme [11] can also reduce overhead; testing this by transmitting a signature over every 10 packets during TCP sessions increased throughput by 3.3 times to approximately 1600 Mbps. Due to potential unsuitability in some environments, Provmon can be configured to use Hi-Fi identifiers [48], which are vulnerable to network attacks but impose negligible overhead. LPM’s impact on network performance is specific to the particular module and can be tailored to meet the needs of the system.

### 7 Discussion

Without provenance-aware applications, LPM will struggle to accurately track dependencies through workflow layer abstractions. A notable example is the copy/paste buffer in windowing applications like Xorg, a known side channel for kernel layer security mechanisms. Without provenance-aware windowing, LPM will conservatively assume that all files opened for reading are dependencies of the paste buffer, leading to false dependencies. LPM is also unable to observe system side channels, such as timing channels or L2 cache measurements, a limitation shared by many security solutions [18].

Although we have not presented a secure distributed provenance-aware system, LPM provides the foundation for creating such a system. In the presented modules, provenance is stored locally by the host and retrieved on an as-needed basis from other hosts. This raises availability concerns as hosts inevitably fail. Availability could be improved with minimal performance and storage overheads through Gehani et al.’s approach of duplicating provenance at k neighbors with a limited graph depth d [27, 28].

Finally, LPM does not address provenance confidentiality, an important challenge explored elsewhere in the literature [14, 46]. LPM’s Recorders provide interfaces that can introduce an access control layer onto the provenance store.

### 8 Related Work

While numerous provenance-aware systems have been proposed, most disclose provenance within an application [32, 41, 65] or workflow [24, 58]. It is difficult or impossible to obtain complete provenance in this manner because system events outside the application, but still affecting its execution, will not appear in the provenance record.

The alternative to disclosed systems are automatic provenance-aware systems, which collect provenance transparently within the operating system. Gehani et al.’s SPADE is a multi-platform system for eScience and grid computing, emphasizing low latency and availability in distributed environments [29]. SPADE’s provenance reporters use familiar application layer utilities, such as `ps` for process information and `lsof` for network information, which can lead to incomplete provenance due to race conditions.

The PASS project collects the provenance of system calls at the VFS layer. PASSv1 provides base functions for provenance collection, observing processes’ file I/O activity [43]. Because these basic functions are manually placed around the kernel, extending PASSv1 to support additional collection hooks is challenging; we address this limitation in the modular design of LPM. PASSv2 introduces a Disclosed Provenance API for tighter integration between provenance collected at different layers of abstraction [44]. PASSv2 assumes that disclosing processes are benign, while LPM provides a secure disclosure mechanism for attesting the correctness of provenance-aware applications. Both SPADE and PASS are designed for benign environments, making no attempt to protect their collection mechanisms from adversaries.

Previous work has considered the security of provenance under relaxed threat models relative to LPM. In SProv, Hasan et al. introduce provenance chains, cryptographic constructs that prevent the insertion or deletion of provenance inside a series of events [32]. SProv effectively demonstrates the authentication properties of this primitive but is not intended to serve as a secure provenance-aware system; attackers can still append false records, delete the whole chain, or disable the library altogether. Zhou et al. consider provenance corruption inevitable and show that provenance can detect some malicious hosts in distributed environments provided a critical mass of correct hosts still exist [65]. They later strengthen these assurances through provenance-aware software-defined networking [5]. These systems consider only network events and cannot speak to the internal state of hosts.

Lyle and Martin sketch the design for a secure provenance monitor based on trusted computing [40], conceptualizing provenance as a TPM-aided proof of code execution. However, they overlook interprocess communication and other system activities, offering information too coarse-grained for some applications. Moreover, to the best of our knowledge, their system is unimplemented.

The most promising model to date for secure provenance collection is Pohly et al.’s Hi-Fi system [48]. Hi-Fi is a Linux Security Module (LSM) that collects whole-system provenance, detailing the actions of processes, IPC mechanisms, and even the kernel itself. Hi-Fi attempts to provide a provenance reference monitor [42] but remains vulnerable to the provenance-aware adversary described in Section 3.2. Enabling Hi-Fi blocks the installation of other LSMs, such as SELinux or Tomoyo, or requires a third-party patch to permit module stacking. This blocks the installation of MAC policy on the host, preventing runtime integrity assurances. Hi-Fi is also vulnerable to adversaries in the network, who can strip provenance identifiers from packets in transit, resulting in irrecoverable provenance. Unlike LPM, Hi-Fi does not attempt to provide layered provenance services.