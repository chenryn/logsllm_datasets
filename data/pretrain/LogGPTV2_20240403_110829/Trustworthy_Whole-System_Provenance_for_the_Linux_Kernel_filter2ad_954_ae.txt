times are in microseconds. Percent overhead for modi-
ﬁed conﬁgurations are shown in parenthesis.
LPM scaffolding installed but without an enabled mod-
ule (LPM), and LPM installed with the Provmon module
enabled (Provmon).
6.1 Collection Performance
We used LMBench to microbenchmark LPM’s impact
on system calls as well as ﬁle and memory latencies.
Table 1 shows the averaged results over 10 trials for
each kernel, with a percent overhead calculation against
Vanilla. For most measures, the performance differ-
ences between LPM and Vanilla are negligible. Com-
paring Vanilla to Provmon, there are several measures
in which overhead is noteworthy: stat, open/close, ﬁle
creation and deletion. Each of these benchmarks in-
volve LMBench manipulating a temporary ﬁle that re-
sides in /usr/tmp/lat_fs/. Because an absolute
path is provided, before each system call occurs LM-
Bench ﬁrst traverses the path to the ﬁle, resulting in
the creation of 3 different provenance events in Prov-
mon’s inode_permission hook, each of which is
transmitted to user space via the kernel relay. While
USENIX Association  
24th USENIX Security Symposium  329
11
Figure 6: Growth of provenance
storage overheads during kernel
compilation.
Figure 7: Performance of ancestry
queries for objects created during
kernel compilation.
Figure 8: LPM network overhead
can be reduced with batch signature
schemes.
the overheads seem large for these operations, the log-
ging of these three events only imposes approximately
1.5 microseconds per traversal. Moreover, the over-
head for opening and closing is signiﬁcantly higher than
the overhead than reads and writes (Null I/O); thus, the
higher open/close costs are likely to be amortized over
the course of regular system use. A provenance module
could further reduce this overhead by maintaining state
about past events within the kernel, then blocking the
creation of redundant provenance records.
Test
Kernel Compile
Postmark
Blast
Vanilla
598 sec
25 sec
376 sec
Provmon
614 sec
27 sec
390 sec
Overhead
2.7%
7.5%
4.8%
Table 2: Benchmarking Results. Our provenance module
imposed just 2.7% overhead on kernel compilation.
To gain a more practical sense of the costs of LPM, we
also performed multiple benchmark tests that represented
realistic system workloads. Each trial was repeated 5
times to ensure consistency. The results are summarized
in Table 2. For the kernel compile test, we recompiled
the kernel source (in a ﬁxed conﬁguration) while booted
into each of the kernels. Each compilation occurred on
16 threads. The LPM scaffolding (without an enabled
module) is not included, because in both tests it imposed
less than 1% overhead. In spite of seemingly high over-
heads for ﬁle I/O, Provmon imposes just 2.7% overhead
on kernel compilation, or 16 seconds. The Postmark test
simulates the operation of an email server. It was con-
ﬁgured to run 15,000 transactions with ﬁle sizes rang-
ing from 4 KB to 1 MB in 10 subdirectories, with up to
1,500 simultaneous transactions. The Provmon module
imposed just 7.5% overhead on this task. To estimate
LPM’s overhead for scientiﬁc applications, we ran the
BLAST benchmarks 19, which simulates biological se-
quence workloads obtained from analysis of hundreds of
thousands of jobs from the National Institutes of Health.
19See
http://ﬁehnlab.ucdavis.edu/staff/kind/Collector/Benchmark/
Blast_Benchmark
For kernel compile and postmark, Provmon outper-
forms the PASS system, which exacted 15.6% and 11.5%
overheads on kernel compilation and postmark, respec-
tively [43]. Provmon introduces comparable kernel com-
pilation overhead to Hi-Fi (2.8%) [48]. It is difﬁcult to
compare our Blast results to SPADE and PASS, as both
used a custom workload instead of a publicly available
benchmark. SPADE reports an 11.5% overhead on a
large workload [29], while PASS reports just an 0.7%
overhead. Taken as a whole, though, LPM collection ei-
ther meets or exceeds the performance of past systems,
while providing additional security assurances.
6.2 Storage Overhead
A major challenge to automated provenance collection is
the storage overhead incurred. We plotted the growth of
provenance storage using different recorders during the
kernel compilation benchmark, shown in Figure 6. LPM
generated 3.7 GB of raw provenance. This required only
450 MB of storage with the Gzip recorder, but prove-
nance cannot be efﬁciently queried in this format. The
SNAP recorder builds an in-memory provenance graph.
We approximated the storage overhead through polling
the virtual memory consumed by the recorder process in
the /proc ﬁlesystem. The SNAP graph required 1.6 GB
storage; the reduction from the raw provenance stream
is due to the fact that redundant events did not lead to
the creation of new graph components. In contrast, the
PASS system generates 1.3 GB of storage overhead dur-
ing kernel compilation while collecting less information
(e.g., shared memory). LPM’s storage overheads are thus
comparable to other provenance-aware systems.
6.3 Query Performance (PB-DLP)
We evaluated query performance using our exemplar PB-
DLP application and LPM’s SNAP recorder. The prove-
nance graph that was populated using the routine from
the kernel compile benchmark. This yielded a raw prove-
nance stream of 3.7 GB, which was translated by the
recorder into a graph of 6,513,398 nodes and 6,754,059
330  24th USENIX Security Symposium 
USENIX Association
12
 0 1 2 3 4 5 0 2 4 6 8 10Storage Cost (GBytes)Time (Minutes)Raw Prov. StreamGZip RecorderSNAP Recorder 0.95 0.96 0.97 0.98 0.99 1 0 5 10 15 20 25Cumulative DensityResponse Time (Milliseconds) 0 1000 2000 3000 4000 5000Iperf PerformanceThroughput (Mbps)VanillaLPMProvmonBatch Sigedges. We were then able to use the graph to issue an-
cestry queries, in which the subgraphs were traversed to
ﬁnd the data object ancestors of a given ﬁle. Because
we did not want ephemeral objects with limited ances-
tries to skew our results, we only considered the results
of objects with more than 50 ancestors.
In the worst case, which was a node that had 17,696
ancestors, the query returned in just 21 milliseconds.
Effectively, we were able to query object ancestries at
line speed for network activity. We are conﬁdent that
this approach can scale to large databases through pre-
computation of expensive operations at data ingest, mak-
ing it a promising strategy for provenance-aware dis-
tributed systems; however, we note that these results are
highly dependent on the size of the graph. Our test graph,
while large, would inevitably be dwarfed by the size of
the provenance on long-lived systems. Fortunately, there
are also a variety of techniques for reducing these costs.
Bates et al. show that the results from provenance graph
traversals can be extensively cached when using a ﬁxed
security policy, which would allow querying to amortize
to a small constant cost [7]. LPM could also be extended
to support deduplication [63, 62] and policy-based prun-
ing [6, 13], both of which could further improve perfor-
mance by reducing the size of provenance graphs.
6.4 Message Commitment Protocol
Under each kernel conﬁguration, we performed iperf
benchmarking to discover LPM’s impact on TCP
throughput. iperf was launched in both client and server
mode over localhost.
The client was launched
with 16 threads, two per each CPU. Our results can be
found in Figure 8. The Vanilla kernel reached 4490
Mbps. While the LPM framework imposed negligible
cost compared to the vanilla kernel (4480 Mbps), Prov-
mon’s DSA-based message commitment protocol re-
duced throughput by an order of magnitude (482 Mbps).
Through use of printk instrumentation, we found that
the average overhead per packet signature was 1.2 ms.
This result is not surprising when compared to IPsec
performance. IPsec’s Authentication Header (AH) mode
uses an HMAC-based approach to provide similar guar-
antees as our protocol. AH has been shown to reduce
throughput by as much as half [16]. An HMAC approach
is a viable alternative to establish integrity and data ori-
gin authenticity and would also ﬁt into the options ﬁeld,
but would require the negotiation of IPsec security as-
sociations. Our message commitment protocol has the
beneﬁt of being fully interoperable with other hosts, and
does not require a negotiation phase before communi-
cation occurs. Another option for increasing through-
put would be to employ CPU instruction extensions [30]
and security co-processor [19] to accelerate the speed of
DSA. Yet another approach to reducing our impact on
network performance would be to employ a batch signa-
ture scheme [11]. We tested this by transmitting a sig-
nature over every 10 packets during TCP sessions, and
found that throughput increased by 3.3 times to approx-
imately 1600 Mbps. Due to the fact that this overhead
may not be suitable for some environments, Provmon
can be conﬁgured to use Hi-Fi identiﬁers [48], which are
vulnerable to network attack but impose negligible over-
head. LPM’s impact on network performance is speciﬁc
to the particular module, and can be tailored to meet the
needs of the system.
7 Discussion
Without the aid of provenance-aware applications, LPM
will struggle to accurately track dependencies through
workﬂow layer abstractions. The most obvious example
of such an abstraction is the copy/paste buffer in window-
ing applications like Xorg. This is a known side channel
for kernel layer security mechanisms, one that has been
addressed by the Trusted Solaris project [10], Trusted X
[21, 22], the SELinux-aware X window system [37], Se-
cureView 20, and General Dynamics’ TVE 21. Without
provenance-aware windowing, LPM will conservatively
assume that all ﬁles opened for reading are dependencies
of the paste buffer, leading to false dependencies. LPM
is also unable to observe system side channels, such as
timing channels or L2 cache measurements [51], a limi-
tation shared by many other security solutions [18].
Although we have not presented a secure distributed
provenance-aware system in this work, LPM provides
the foundation for the creation of such a system. In the
presented modules, provenance is stored locally by the
host and retrieved on an as-needed basis from other hosts.
This raises availability concerns as hosts inevitably begin
to fail. Availability could be improved with minimal per-
formance and storage overheads through Gehani et al.’s
approach of duplicating provenance at k neighbors with
a limited graph depth d [27, 28].
Finally, LPM does not address the matter of prove-
nance conﬁdentiality; this is an important challenge that
is explored elsewhere in the literature [14, 46]. LPM’s
Recorders provide interfaces that can be used to intro-
duce an access control layer onto the provenance store.
8 Related Work
While myriad provenance-aware systems have been pro-
posed in the literature, the majority disclose provenance
within an application [32, 41, 65] or workﬂow [24, 58]. It
20See http://www.ainfosec.com/secureview
21See http://gdc4s.com/tve.html
USENIX Association  
24th USENIX Security Symposium  331
13
is difﬁcult or impossible to obtain complete provenance
in this manner. This is because systems events that occur
outside of the application, but still effect its execution,
will not appear in the provenance record.
The alternative to disclosed systems are automatic
provenance-aware systems, which collect provenance
transparently within the operating system. Gehani et
al.’s SPADE is a multi-platform system for eScience and
grid computing audiences, with an emphasis on low la-
tency and availability in distributed environments [29].
SPADE’s provenance reporters make use of familiar ap-
plication layer utilities to generate provenance, such as
polling ps for process information and lsof for net-
work information. This gives rise to the possibility of in-
complete provenance due to race conditions. The PASS
project collects the provenance of system calls at the vir-
tual ﬁlesystem (VFS) layer. PASSv1 provides base func-
tions for provenance collection that observe processes’
ﬁle I/O activity [43]. Because these basic functions are
manually placed around the kernel, there is no clear
way to extend PASSv1 to support additional collection
hooks; we address this limitation in the modular design
of LPM. PASSv2 introduces a Disclosed Provenance API
for tighter integration between provenance collected at
different layers of abstraction, e.g., at the application
layer [44]. PASSv2 assumes that disclosing processes
are benign, while LPM provides a secure disclosure
mechanism for attesting the correctness of provenance-
aware applications. Both SPADE and PASS are designed
for benign environments, making no attempt to protect
their collection mechanisms from an adversary.
Previous work has considered the security of prove-
nance under relaxed threat models relative to LPM’s. In
SProv, Hasan et al. introduce provenance chains, cryp-
tographic constructs that prevent the insertion or dele-
tion of provenance inside of a series of events [32].
SProv effectively demonstrates the authentication prop-
erties of this primitive, but is not intended to serve as a
secure provenance-aware system; attackers can still ap-
pend false records to the end of the chain, delete the
whole chain, or disable the library altogether. Zhou et
al. consider provenance corruption an inevitability, and
show that provenance can detect some malicious hosts in
distributed environments provided that a critical mass of
correct hosts still exist [65]. They later strengthen these
assurances through use of provenance-aware software-
deﬁned networking [5]. These systems consider only
network events, and are unable to speak to the internal
state of hosts. Lyle and Martin sketch the design for
a secure provenance monitor based on trusted comput-
ing [40]. However, they conceptualize provenance as
a TPM-aided proof of code execution, overlooking in-
terprocess communication and other system activity that
could inform execution results, and therefore offer infor-
mation that is too coarse-grained to meet the needs of
some applications. Moreover, to the best of our knowl-
edge their system is unimplemented.
The most promising model to date for secure prove-
nance collection is Pohly et al.’s Hi-Fi system [48]. Hi-Fi
is a Linux Security Module (LSM) that collects whole-
system provenance that details the actions of processes,
IPC mechanisms, and even the kernel itself (which does
not exclusively use system calls). Hi-Fi attempts to pro-
vide a provenance reference monitor [42], but remains
vulnerable to the provenance-aware adversary that we
describe in Section 3.2. Enabling Hi-Fi blocks the in-
stallation of other LSM’s, such as SELinux or Tomoyo,
or requires a third party patch to permit module stack-
ing. This blocks the installation of MAC policy on the
host, preventing runtime integrity assurances. Hi-Fi is
also vulnerable to adversaries in the network, who can
strip the provenance identiﬁers from packets in transit,
resulting in irrecoverable provenance. Unlike LPM, Hi-
Fi does not attempt to provide layered provenance ser-