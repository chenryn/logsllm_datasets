inal mDNS packets from these devices, and conﬁrmed that they
are very similar to other AppleTV devices in our database.
Further investigation ties the behaviors from these devices
to Apple’s AirPlay feature. AirPlay is Apple’s proprietary
protocol suite for multimedia streaming over WiFi. Since
Apple never open-sourced or licensed AirPlay, this feature
is supposed to be seen on Apple devices only. However, the
proprietary AirPlay protocol has been reverse engineered,
and several open-source implementations are available on
Github, e.g., open-airplay4. Our investigation also discover
that the AirPlay protocol in all the malicious “counterfeit
AppleTVs”, except {MTN, TV}, was developed by a corpo-
4https://github.com/jamesdlow/open-airplay/
Figure 4: (a) Performance comparison with other classiﬁcation
algorithms. (b) Device detection speed in ground truth data.
mance of OWL on the ground truth data. We assumed that
OWL was connected to the network at t0, and gradually fed
packets to it according to their timestamps. We assessed OIR
at 1-second intervals. As shown in Fig. 4 (b), OIR increased
rapidly for approximately 240 seconds, when 80% of the de-
vices were correctly identiﬁed at all granularity levels. OWL
reached its peak performance in approximately 500 seconds.
Supplementary Data and Labelless Data. The supplemen-
tary testing data contains 78 device descriptions from URLs
in SSDP notify messages. Device descriptions conﬁrmed the
classiﬁcation of 63 devices, partially supported 14 devices
(e.g. device labeled Samsung Galaxy while description says
“Android”), and denied the classiﬁcation of one device.
Last, devices in the labelless dataset did not provide enough
features for labeling, however, we still attempted to validate
the predicted labels with the limited information in the pack-
ets. 71% of the labels were supported, while only 1% of the
labels were denied (e.g., a sample was classiﬁed as a camera
but one packet contains keyword “windows”).
Misclassiﬁed Devices. We manually examined the misclassi-
ﬁed samples to identify their causes. First, most misclassiﬁ-
cations at {model} level were classiﬁed into a similar model
from the same vendor, e.g., Samsung Galaxy-note8 phones
identiﬁed as Galaxy-note9, and HP M1536dnf printers identi-
ﬁed as M227fdw. Many devices misclassiﬁed at {type} level
were classiﬁed into the vendor’s other product line, e.g., some
Apple Watches with limited BC/MC packets were identiﬁed
as iPhone. Finally, third-party WiFi modules of some devices
caused confusions at {manufacturer} level.
Low conﬁdence classiﬁcations are often unknown or forged
devices, which may be sent to system admins for manual
USENIX Association
29th USENIX Security Symposium    65
Table 4: Devices pretending to be AppleTVs.
Xiaomi,TV,4
Gaoshengda,TV
Hisense,TV,vidaa
whaley,TV,w50j
Rﬂink,TV
Leshi,TV,x55
Funshion,TV
PPTV,TV
MTN,TV
Nebula,TV
Leshi,TV,x65s
Chuangwei,TV
Changhong,TV,43s1
Changhong,TV,LED50
Tianmao,Magiccast,m18
ration named Lebo (or HappyCast) 5. The homepage of Lebo
corporation claims that they independently researched the
protocol for casting streaming media from iOS system and
developed the Lebo software suite.
For validation, we deployed the open-airplay library on
a Windows laptop as a “simulator”. Apple devices in the same
network identiﬁed it as a valid AirPlay receiver. We captured
BC/MC packets from the simulator, and further examined the
mDNS packets. The mDNS Resource Records of the simulator,
counterfeit AppleTVs, and the authentic AppleTVs were al-
most identical. The simulator and the counterfeit AppleTVs
even shared higher similarities than that between the counter-
feit and the authentic AppleTVs. All other BC/MC packets
from the simulator behaved the same as the host laptop.
In this case, OWL was able to identify abnormal incon-
sistencies across views for a group of seemingly malicious
devices. We discovered the root causes of the inconsistency
through further manual investigation. Set aside legal implica-
tions of counterfeiting, this case demonstrates the capability
of OWL in identifying spoofed devices in the real world.
Fake DHCP Server and Gateway. Another device in the
labelless dataset also triggered the alarm in the experiment.
The DHCP view labeled it as a router with high conﬁdence,
which was not agreed by other views. Further investigation
showed that the device broadcast DHCP Offer and DHCP ACK
messages to inform other devices the gateway of the network
is itself. This behavior clearly resembled routers or gateways
in WiFi networks. However, mDNS and SSDP views classi-
ﬁed this device as a Microsoft laptop (model: surface_book).
The MAC preﬁx also conﬁrmed its vendor as Microsoft.
A reasonable explanation is that the Microsoft surface book
spoofed a gateway to lure others to connect through it. Exami-
nation of the DHCP request packets from other devices in the
network revealed that some devices did connect through this
fake gateway, which could easily launch man-in-the-middle
attacks, or use a captive portal to phish the victims.
To conﬁrm our speculation, we simulated the same attack
in our lab network. We employed Yersinia in Kali Linux to
send DHCP Discover to exhaust the IP resource of the au-
thentic router. DHCP service was then started on the Kali
computer using itself as the gateway. Very soon, we observed
new devices requesting IP addresses from the spoofed gate-
way. We sniffed the BC/MC packets from this gateway, and
fed them to OWL, which generated an alarm that was very
similar to the one for the rogue gateway in our dataset.
Virtual Machines. OWL identiﬁed several devices that
5http://www.hpplay.com.cn/index_english.jsp
demonstrated strong discrepancy between mDNS and LBN
views. For example, several devices were identiﬁed as Mac-
books on mDNS view and MAC preﬁx. Meanwhile, LBN
view classiﬁed them to be computers manufactured by other
vendors. Through further investigation, we concluded that
these were computers running virtual machines that connected
to the networks with Network Address Translation (NAT).
In practice, a virtual machine has three mechanisms to con-
nect to the network: (1) NAT, (2) bridged network, and (3)
host-only network. With NAT, the VM and the host system
share a single network identity, so that packets from the VM
are directly disseminated by the host. With bridged network
mode, the VM may get its own IP while sharing the same
MAC with the host. The VM may also get its own MAC,
where the VM vendor could be identiﬁed by the MAC pre-
ﬁx. For example, MAC preﬁx “00-05-69” denotes VMware
and “00-1c-42” denotes Parallels. Last, VMs with host-only
network only communicates within a private network on the
host, hence, they do not connect to the external network at all.
When a VM runs in NAT mode or shared MAC in bridged
mode, OWL is able to detect the inconsistencies caused by
the shared identity. OWL is only effective when the guest
OS differs from the host OS, so that discrepancies in the im-
plementations of network protocols could be discovered. We
further tested other guest/host OS combinations, including An-
droid x86 VM running on MacBooks or Windows desktops,
and conﬁrmed that OWL was able to detect all of them given
enough sniffed BC/MC packets. Last, for VMs with their own
MAC addresses, they were correctly annotated as VMs in our
dataset and accurately detected in the experiments.
Hidden Cameras. Surveillance cameras, especially the hid-
den ones, are often considered as sensitive/malicious devices
that infringe users’ privacy. Efforts have been made in the liter-
ature to detect hidden cameras based on their unique network
trafﬁc patterns during video streaming [10, 61, 61]. Mean-
while, we observed that the adversaries may set the (hidden)
cameras to stand-by mode or to store videos locally to avoid
trafﬁc-based detectors. They only transmit real-time or stored
video streams when they receive remote commands from their
owners, who may pick a time when the victims’ detectors are
likely to be ofﬂine, e.g., late night or after hotel checkout.
Nevertheless, these cameras still connect to the network in
order to receive remote commands, therefore, they send out
BC/MC packets and they can be detected by OWL.
In our experiments presented in Section 5, OWL achieved
100% accuracy in detecting cameras at {manufacturer,
type} granularity, when the training set contains samples
with the same {manufacturer, type}, but not necessarily the
same model. For example, when we have {dlink, camera,
dcs-930lb} in the training data, OWL can correctly identify
DLink DCS-935l cameras as {dlink, camera}, even though
it has never seen the DCS-935l model before, i.e., it does not
have {dcs-930lb} in its label set. This is explained by the
fact that the same manufacturer often reuses the hardware and
66    29th USENIX Security Symposium
USENIX Association
software modules, especially for products in the same line.
Meanwhile, OWL also identiﬁed several examples of OEM
cameras in our dataset. For example, when we put only one
camera {lenovo, camera, snowman} in the training data,
a Xiaomi Dafang-DF3 camera and a Qihoo360 D302 camera
were both classiﬁed as Lenovo cameras. Further examination
of raw data conﬁrmed that all three products shared nearly
identical features in several views. Note that these two new
cameras were also signiﬁcantly different from other Xiaomi
or iQhoo360 devices. We could conﬁdently infer that all three
products shared certain software modules or they might be
OEM devices from the same original manufacturer. Last, al-
though they were correctly labeled as cameras, these devices
also triggered alarms of unknown/malicious devices, which
calls for the attention of the administrator or user.
7 Attacks Against OWL
In this section, we discuss three potential attacks against OWL:
the naive attacks, the knowledgeable attacks, and the expert
attacks. They share the same objective: to hide the identities
of (potentially malicious) devices by confusing the device
classiﬁer and escaping from the malicious device detector.
7.1 The Naive Attacks
The Threat Model. The naive adversaries do not have the
knowledge or capability (e.g., root privilege) to change sys-
tem code/driver or privileged ﬁles/attributes. They can only
employ OS-provided GUI to modify user-deﬁned attributes
that are adopted by the network modules. With the lowest
technical barrier, naive attacks are highly feasible to novices.
The Approach. We examine the most popular OSs for con-
sumer mobile/IoT devices (Android, iOS, Windows, and Ma-
cOS) to identify the system attributes that could be changed
through system settings and then adopted in BC/MC packets.
Naive attackers could conﬁgure the “device name” attribute in
iOS (un-rooted) and MacOS (admin-only), which is adopted
in the HostName ﬁeld of DHCP, mDNS, and other protocols.
Although users could change “device name” in Android, the
attribute is only used as device identiﬁer in Bluetooth, WLAN
Direct, hotspot, and USB, while all BC/MC protocols use a
manufacturer-assigned value in HostName. We also examine
user settings of IoT devices in our lab and identify how the
user-entered values are adopted in BC/MC messages. The
devices and settings in IoT devices are more ad-hoc, as users
could only change one or two attributes in a few devices that
impact the MC/BC packets (mostly HostName). Note that
we do not consider virtual machines, use of hacking tools or
command line methods in the naive attacks.
Experiment Results. In the experiments, we randomly se-
lected 1,000 devices with user-editable attributes from the
annotated dataset with all {manufacturer, type, model} la-
bels. For each device, we overwrite all user-editable attributes
with values from another random device with different labels.
Figure 5: The knowledgeable attacks: (a) Device identiﬁcation accu-
racy of OWL under attack. (b) Malicious device detection accuracy
and recall of OWL. X-Axis: percentage of modiﬁed features.
As a result, OWL achieved OIR = 0.985, OIR = 0.964 and
OIR = 0.902 at three granularity levels, respectively.
7.2 The Knowledgeable Attacks
The Threat Model. The knowledgeable adversaries have full
control of the system and understand OS hacking. However,
they do not have the system source code (e.g., Windows or
proprietary IoT devices), so that they need to reverse engineer
the system or to hack on OS/application binaries. Therefore,
it could be challenging to completely overwrite all attributes
from all BC/MC protocols, since the attributes could be de-
rived or scatteredly distributed in the system. This represents
the majority of the advanced adversaries against OWL.
The Approach. The knowledgeable adversaries always at-
tempt to forge a speciﬁc device instead of randomly modi-
fying each attribute, since this gives them the best chance to
escape from correct identiﬁcation. Formally, an adversary at-
tempts to hide a suspicious device S by replacing n attributes
(out of N total attributes) from its BC/MC packets with values
from a benign device B. We want to answer two questions
through experiments: (1) When n increases from 0 to N, how
would OWL’s device identiﬁcation performance change? (2)
How would OWL detect the suspiciously altered device?
Experiment Results. In the experiments, we randomly sam-
pled 1000 devices with all three labels from the annotated
data set. For each device S, we randomly selected another
device B from a different {type}, and overwrote n attributes
of S with corresponding values from B. Figure 5 (a) shows the
device identiﬁcation accuracy of OWL at three different gran-
ularity levels. When 20% of the features of S are overwritten
by values from B, OWL’s accuracy drops to 91.5%, 88.9%
and 85% for manufacturer, type, and model, respectively.
We added 1,000 random benign devices to the above dataset
to serve as negative samples. Recall (aka. detection rate) R
is deﬁned as: R = T P
T P+FN , i.e., ratio of correctly detected ma-
licious samples out of all malicious samples. Accuracy A is
deﬁned as A = T P+T N
ALL , i.e. the ratio of corrected classiﬁed
samples out of all samples. Figure 5 (b) shows the malicious
device detection performance under the knowledgeable at-
tacks. When 20% of the attributes are modiﬁed, R reaches
92.2% while the A is 95.95%. When majority or all of the
features are modiﬁed (n → N), S essentially becomes (almost)
identical to B, hence, both A and R drops.
USENIX Association
29th USENIX Security Symposium    67
3
4
5
Table 5: OWL’s performance against the expert attacks.
#view
Amanu f
Atype
Amodel
A
R
.101
.118
.136
.950
.903
.509
.505
.502
.985
.973
.208