Queue
length
detector
update
rate limiter
clock
process
Figure 1: A schema of the control ﬂow of the virus
throttle
In [27], it was argued that if the rate of connections
to new targets is high, as is the case with many
worms, the delay queue would rapidly grow result-
ing in attempted connections being severely delayed.
When implementing the throttle, we took this idea
further and introduced an upper limit to the size of
the delay queue which, once reached, will disallow
all further connection requests by the host. Thus
the throttle can be said to behave benignly within
certain limits. From observation of the normal be-
haviour of a range of users we saw that the delay
queue rarely grew bigger than a handful of packets
and concluded that the size of the delay queue could
oﬀer an indication of the presence of a worm on the
host: large delay queue sizes would almost certainly
indicate an application behaving in a suspect man-
ner. In our throttle we set this upper limit on the
delay queue size to 100 packets.
Lastly, as mentioned previously, we have also im-
plemented UDP, SMTP and Exchange versions of
the throttle. The UDP throttle works in exactly
the same way as its TCP counterpart except that
instead of parsing outgoing network traﬃc for TCP
SYN packets, it looks for outgoing UDP packets,
288
12th USENIX Security Symposium 
USENIX Association
each of which is considered a separate connection at-
tempt. The email throttle is described fully in [28].
3.2 Implementation
In this section we describe how the virus throttle
was implemented, beginning with a short overview
of the structure of the Linux network stack. Neces-
sarily, this section is fairly technical, and the reader
is referred to more complete works such as [1, 18]
for further information.
The Linux network stack strictly consists of two
data structures, the ptype all linked list and the
ptype base hash table, containing pointers to packet
handler functions [13], but, at a conceptual level,
can simply be thought of as a list of packet han-
dler functions. Outgoing packets are placed onto
the list by a packet handler and traverse the list un-
til they reach its head, after which the packets are
passed to the hard start xmit function supplied by
the appropriate network device driver for transmis-
sion on the wire. The virus throttle works by replac-
ing the pointer to the hard start xmit function reg-
istered by the network device driver with a pointer
to a function of its own. This means that, since
the hard start xmit function is called every time a
packet has ﬁnished traversing the network stack and
is ready to be transmitted on the network interface,
we are able to intercept and control every packet
leaving the network device. In essence, the throt-
tle can be viewed at this level as an ethernet device
driver wrapper.
To accomplish this wrapping the virus throttle is im-
plemented as a Linux 2.4.18 kernel module which,
in its init module routine, fetches the pointer to the
net device structure registered by the current net-
work device driver. The net device structure con-
tains entries to pointers to the packet handling func-
tions registered by the network device driver, in-
cluding the hard start xmit function, whose pointer
we replace with a pointer to our own transmit func-
tion.
Our own transmit function is a simple packet parser
which looks for TCP SYN packets, the packets used
to establish stream-based connections between ap-
plications.
If the packet being parsed is not a
TCP SYN packet, it is passed on to the original
hard start xmit function for transmission as usual.
If, however, it is a TCP SYN packet it is, as de-
scribed in the previous section, either allowed to
pass immediately,
in which case it is handed oﬀ
to the original hard start xmit function and possi-
bly added to the working set or, if the working set
is full, is added to the delay queue for later trans-
mission. Both the working set and delay queue data
structures are instantiated as linked lists using the
linked list structure provided by the Linux kernel.
The working set list stores the destination address
of the packet, while the delay queue list stores the
source and destination addresses of the packet, a
copy of the sk buﬀ data structure associated with
the packet, and the time it was enqueued. The limit
on the size of the delay queue is implemented by
monitoring the size of the delay queue in the packet
parser and setting a ﬂag if this size increases be-
yond the speciﬁed upper limit. The packet parser
will never allow a connection attempt to commence
if this ﬂag is set.
Processing of the delay queue is handled by a ker-
nel thread which wakes up a speciﬁed number of
times per second, in our case once per second. The
delay queue is then processed as described in the
previous section, with packets deemed suitable for
transmission dequeued and passed on to the origi-
nal hard start xmit function. Due to the fact that
the delay queue and working set are data structures
shared by the enqueueing packet parsing routines
and the dequeueing delay queue routines, these two
data structures are carefully protected by spinlocks.
In order to allow for the fact that one host may
have several diﬀerent IP addresses or that the virus
throttle may be placed on some intermediate system
such as a bridge or gateway and hence see packets
transmitted with a number of diﬀerent source IP
addresses, our implementation actually keeps an ar-
ray of working set and delay queue data structures.
Each entry in this array corresponds to a working
set and delay queue for a particular source IP ad-
dress.
4 Testing
Now that we have detailed the design and imple-
mentation of the throttle, in this section we move on
to describe how we evaluated its performance. Ini-
tially, we outline our experimental setup and then
go on to present the experiments we performed and
their results.
USENIX Association
12th USENIX Security Symposium 
289
4.1 Experimental setup
In order to eﬀectively test our throttle in a range
of diﬀerent scenarios we ﬁrst had to develop a se-
cure testbed on which the virus throttle could be
exposed to real and constructed mobile code. It is
this testbed that we now describe, more details of
which can be found in [25].
4.1.1 Testbed
The testbed consists of a rack mounted single-
chassis HP Blade Server bh7800 [12] providing the
physical
infrastructure for 3 separate LANs and
housing what includes three 25-port 10/100 switches
and 16 bh1100 Server Blades. Each Blade has a
700MHz Pentium III processor with a 30GB hard-
disk, onboard graphics and 3 network interfaces.
One network interface is provided to the manage-
ment LAN by a remote management card (RMC)
daughterboard, while the other two provide connec-
tions to the remaining two LANs.
As well as being physically separate, the functional
roles of the three LANs have also been separated.
The management LAN provides access to the RMC
daughterboard on each Server Blade which can be
used for tasks such as power-cycling the Blade. The
second LAN is designated the administrative LAN
and is used to handle the installation and conﬁg-
uration of the Blades, and the coordination of ex-
periments and collation of data from these experi-
ments. The third LAN, the experimental LAN. is
the network on which the experiments are actually
performed. This setup is summarised in Figure 2.
In addition to the Blade Server, we are also employ-
ing four 1.8GHz Pentium 4 boxes, two of which act
as servers on the administrative and experimental
LANs respectively. The administrative LAN server,
as well as running services such as PXE, DHCP and
FTP necessary for the conﬁguration of the Server
Blades also coordinates our experiments. A third
machine acts as our data collector and is in essence
a network sniﬀer. A monitoring port [11] has been
conﬁgured on the switch on the experimental LAN
and the sniﬀer machine, running tcpdump [24], lis-
tens on this port and keeps a copy of all traﬃc on
this LAN. The fourth machine is our infector which,
at the start of some of our experiments, we bring up
and use to inject a copy of the virus under study into
N
A
L
t
n
e
m
e
g
a
n
a
M
N
A
L
e
v
i
t
a
r
t
s
i
n
i
m
d
A
h
c
t
i
w
S
h
c
t
i
w
S
Expt Server
N
A
L
l
a
t
n
e
m
i
r
e
p
x
E
h
c
t
i
w
S
M
Blade
Blade
Blade
Blade
16
Blade
Admin Server
Injector
Sniffer
Figure 2: The conﬁguration of the testbed
one of the machines on the experimental LAN. This
machine is then brought down and plays no further
role in the experiments. For coordination purposes,
the sniﬀer and infector machines have network con-
nections to both the experimental and administra-
tive LANs, and particular care has been taken to
ensure that no traﬃc from one of these LANs is
able to cross to the other LAN.
A slimmed-down Redhat 7.3 Linux installation with
a custom 2.4.18 kernel forms the operating system
on the Server Blades. As the experiments involve
a variety of operating systems and settings, to ease
conﬁguration and save time we run VMware Work-
station 3.2 [26] on each Server Blade. VMware al-
lows the running of one operating system, the guest
operating system, inside another operating system,
the host operating system, in our case Linux. For
the experiments below we use a standard instal-
lation of Windows 2000 Professional as the guest
OS with VMware conﬁgured in bridged-only net-
working mode. Setting a VMware guest OS to use
bridged-only networking means that the guest will
largely use its own network stack, as opposed to
that of the host OS if host-only networking is con-
ﬁgured. This results in more realism in the network
behaviour of applications running under the guest
290
12th USENIX Security Symposium 
USENIX Association
OS but, since bridged-only networking employs a
proprietary VMware driver located fairly low down
in the Linux network stack, places constraints on the
implementation of the virus throttle not necessarily
present in a production version of the throttle.
This conﬁguration of the testbed allows us to auto-
mate the process of running experiments to a large
extent, essential when it is necessary to repeat ex-
periments a number of times to reduce the noise
inherent in the random nature of the programs we
are studying. Obviously, when a network has ma-
chines known to be infected with viruses, the secu-
rity and isolation of this network is of prime im-
portance. As well as the measures described above
and careful conﬁguration and testing of all software,
the testbed is housed in a secure laboratory phys-
ically preventing connections to all other networks
and strict policies concerning data transfer are im-
posed.
4.1.2 Test worm
In order to test the eﬀectiveness of the virus throttle
at allaying the spread of worms which scan at diﬀer-
ent rates a test worm was developed. The test worm
consists of a basic stream-socket server [23] which
listens for connections on a speciﬁed port. When
it receives a connection attempt the server starts a
scanner whose properties, such as scan rate and ad-
dress range, can also be speciﬁed. This scanner then
scans the IP address space by attempting to make
connections to addresses on the port the test worm
server is listening on. As the type of scanning used
is TCP connect scanning [9], if the scanner discov-
ers the IP address of a machine running a test worm
server it will trigger the server to start the scanner
on this machine. This scheme allows a fairly real-
istic simulation of the scan/exploit/transfer worm
lifecycle but, since no actual exploit or ﬁle transfer
is involved and since any machines infected by the
worm have, a priori, to already have been infected
with the worm, guarantees that the test worm will
never spread autonomously.
4.2 Results
Using the testbed setup described above we have
been able to securely observe a variety of diﬀerent
viruses spreading across the experimental network
and have performed a number of diﬀerent experi-
ments with the virus throttle, which we now go on
to detail.
4.2.1 Stopping speed
We were ﬁrst interested in the time it would take
worms scanning at a number of diﬀerent rates to
cause the delay queue to reach its upper limit, 100
packets in our implementation. Remembering that
once the delay queue has reached this limit we dis-
able all further connection attempts, this time is
eﬀectively the time it takes the throttle to stop
a worm. The experimental LAN on the testbed
was conﬁgured with two machines, a gateway and a
Server Blade running a Windows 2000 Professional
guest OS. Table 1 records the time taken for the
delay queue to reach 100 and the number of con-
nection attempts made before this time when the
Blade Server is infected with the real W32/Nimda-
D virus [21] and the test worm conﬁgured to scan at
various rates. As this table shows, the virus throt-
tle takes only 0.25 seconds to stop Nimda, which
scans at a rate of around 200 connections per sec-
ond, from spreading, and under 5 seconds to stop
any application that scans at a rate of 20 connec-
tions or more per second. In the Nimda case, the
throttle only allows one packet out on to the wire
before networking is shut down, and a maximum of
5 packets for the test worm conﬁgured to scan at 20
connections per second. The stopping times for the
SQLSlammer worm [22] of a UDP implementation
of the throttle have also been included in Table 1