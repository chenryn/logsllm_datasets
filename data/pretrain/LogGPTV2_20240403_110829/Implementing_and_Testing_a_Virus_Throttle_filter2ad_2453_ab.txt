### Queue Length Detector and Update Rate Limiter

**Figure 1: A Schematic of the Control Flow for the Virus Throttle**

In [27], it was argued that if the rate of connections to new targets is high, as is often the case with many worms, the delay queue would rapidly grow, resulting in severe delays for attempted connections. In our implementation of the throttle, we extended this idea by introducing an upper limit to the size of the delay queue. Once this limit is reached, all further connection requests from the host are disallowed. This ensures that the throttle behaves benignly within certain limits.

From our observations of normal user behavior, we found that the delay queue rarely exceeded a few packets. Therefore, we concluded that a large delay queue size could indicate the presence of a worm on the host, as such behavior is highly unusual. In our throttle, we set the upper limit on the delay queue size to 100 packets.

Additionally, we have implemented UDP, SMTP, and Exchange versions of the throttle. The UDP throttle operates similarly to its TCP counterpart, but instead of parsing outgoing network traffic for TCP SYN packets, it looks for outgoing UDP packets, each of which is considered a separate connection attempt. The email throttle is described in detail in [28].

### Implementation

In this section, we describe the implementation of the virus throttle, starting with an overview of the Linux network stack. For more detailed information, the reader is referred to comprehensive works such as [1, 18].

#### Linux Network Stack Overview

The Linux network stack consists of two primary data structures: the `ptype_all` linked list and the `ptype_base` hash table, which contain pointers to packet handler functions [13]. Conceptually, it can be thought of as a list of packet handler functions. Outgoing packets are placed onto this list by a packet handler and traverse the list until they reach the head. At this point, the packets are passed to the `hard_start_xmit` function provided by the appropriate network device driver for transmission.

The virus throttle works by replacing the pointer to the `hard_start_xmit` function registered by the network device driver with a pointer to a custom function. This allows us to intercept and control every packet leaving the network device, effectively acting as an Ethernet device driver wrapper.

#### Kernel Module Implementation

To achieve this, the virus throttle is implemented as a Linux 2.4.18 kernel module. In its `init_module` routine, the module fetches the pointer to the `net_device` structure registered by the current network device driver. This structure contains pointers to the packet handling functions, including the `hard_start_xmit` function, whose pointer we replace with a pointer to our own transmit function.

Our custom transmit function is a simple packet parser that looks for TCP SYN packets, which are used to establish stream-based connections between applications. If the packet being parsed is not a TCP SYN packet, it is passed on to the original `hard_start_xmit` function for transmission as usual. If it is a TCP SYN packet, it is either allowed to pass immediately or, if the working set is full, added to the delay queue for later transmission.

Both the working set and delay queue are implemented as linked lists using the linked list structure provided by the Linux kernel. The working set list stores the destination address of the packet, while the delay queue list stores the source and destination addresses, a copy of the `sk_buff` data structure associated with the packet, and the time it was enqueued. The limit on the size of the delay queue is enforced by monitoring the queue size in the packet parser and setting a flag if the size exceeds the specified upper limit. The packet parser will not allow a connection attempt if this flag is set.

Processing of the delay queue is handled by a kernel thread that wakes up a specified number of times per second, in our case, once per second. The delay queue is then processed, with suitable packets dequeued and passed on to the original `hard_start_xmit` function. To ensure thread safety, the delay queue and working set are protected by spinlocks.

To accommodate hosts with multiple IP addresses or intermediate systems like bridges or gateways, our implementation maintains an array of working set and delay queue data structures, each corresponding to a particular source IP address.

### Testing

In this section, we describe the evaluation of the throttle's performance. We first outline the experimental setup and then present the experiments and their results.

#### Experimental Setup

To effectively test the throttle in various scenarios, we developed a secure testbed where the virus throttle could be exposed to real and constructed mobile code. The testbed consists of a rack-mounted HP Blade Server bh7800 [12] providing the physical infrastructure for three separate LANs and housing 16 bh1100 Server Blades. Each Blade has a 700MHz Pentium III processor, a 30GB hard disk, onboard graphics, and three network interfaces. One interface is connected to the management LAN, while the other two provide connections to the remaining two LANs.

The functional roles of the three LANs are also separated:
- **Management LAN**: Provides access to the RMC daughterboard for tasks like power-cycling the Blade.
- **Administrative LAN**: Handles the installation and configuration of the Blades and coordinates experiments.
- **Experimental LAN**: The network on which the experiments are performed.

In addition to the Blade Server, we use four 1.8GHz Pentium 4 boxes:
- Two act as servers on the administrative and experimental LANs.
- One acts as a data collector, running `tcpdump` to monitor all traffic on the experimental LAN.
- One acts as the infector, injecting a copy of the virus under study into one of the machines on the experimental LAN at the start of some experiments.

A slimmed-down Redhat 7.3 Linux installation with a custom 2.4.18 kernel forms the operating system on the Server Blades. VMware Workstation 3.2 [26] is run on each Blade to facilitate the use of different operating systems, with Windows 2000 Professional as the guest OS in bridged-only networking mode.

This setup allows for the automation of experiments and ensures the security and isolation of the network, with strict policies concerning data transfer.

#### Test Worm

To test the effectiveness of the virus throttle in mitigating the spread of worms with different scan rates, we developed a test worm. The test worm consists of a basic stream-socket server [23] that listens for connections on a specified port. When a connection is received, the server starts a scanner with configurable properties such as scan rate and address range. The scanner uses TCP connect scanning [9] to scan the IP address space, triggering the server on any discovered machine to start its own scanner. This simulates the scan/exploit/transfer lifecycle of a worm without actual exploitation or file transfer, ensuring that the test worm does not spread autonomously.

#### Results

Using the testbed setup, we observed the spread of various viruses across the experimental network and conducted several experiments with the virus throttle.

##### Stopping Speed

We were interested in the time it takes for worms scanning at different rates to cause the delay queue to reach its upper limit of 100 packets, at which point all further connection attempts are disabled. Table 1 records the time taken for the delay queue to reach 100 packets and the number of connection attempts made before this time when the Blade Server is infected with the W32/Nimda-D virus [21] and the test worm configured to scan at various rates.

| Virus/Worm | Scan Rate (connections/sec) | Time to 100 Packets (sec) | Connection Attempts |
|------------|-----------------------------|---------------------------|---------------------|
| Nimda      | ~200                        | 0.25                      | 1                   |
| Test Worm  | 20                          | <5                        | â‰¤5                  |

The virus throttle stops Nimda, which scans at around 200 connections per second, in just 0.25 seconds, allowing only one packet out before shutting down networking. For the test worm configured to scan at 20 connections per second, the throttle stops it in less than 5 seconds, allowing a maximum of 5 packets. Similar stopping times for the SQLSlammer worm [22] using a UDP implementation of the throttle are also included in Table 1.