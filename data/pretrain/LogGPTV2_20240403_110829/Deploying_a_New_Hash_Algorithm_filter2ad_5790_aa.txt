title:Deploying a New Hash Algorithm
author:Steven M. Bellovin and
Eric Rescorla
Deploying a New Hash Algorithm
Steven M. Bellovin
Eric K. Rescorla
PI:EMAIL
PI:EMAIL
Columbia University
Network Resonance
Abstract
The strength of hash functions such as MD5 and SHA-1
has been called into question as a result of recent discov-
eries. Regardless of whether or not it is necessary to move
away from those now, it is clear that it will be necessary
to do so in the not-too-distant future. This poses a number
of challenges, especially for certiﬁcate-based protocols. We
analyze a number of protocols, including S/MIME and TLS.
All require protocol or implementation changes. We explain
the necessary changes, show how the conversion can be
done, and list what measures should be taken immediately.
However, it is clear that neither MD5 nor SHA-1 is as strong
as its target security level and so need to be replaced. The
possibility of new attacks lends some urgency to this transi-
tion.
Although we don’t discuss the issue in detail, most of
our analysis applies to deploying new signature algorithms
as well as to deploying new hash functions. If the signature
algorithm is linked to a particular hash function, as DSA
is tied to SHA-1, the two would change together; beyond
that, since signature algorithms are almost always applied
to the output of hash functions, if there is no easy way to
substitute a new hash algorithm there is almost certainly no
way to substitute a new signature algorithm, either.
1 Introduction
2 Overview of Recent Hash Function Attacks
Nearly all major cryptographic protocols depend on the
security of hash functions. However, this is increasingly
looking like a brittle foundation: although a variety of hash
functions are available, only MD5 [32] and SHA-1 [23] are
in wide use. Both hash functions derive from MD4 [31],
which has long been known to be weak [7, 8], thus leading
to concerns that they might have common weaknesses.
These concerns were borne out in late 2004, when tech-
niques for efﬁciently ﬁnding collisions in MD5 [37] and
SHA-0 [2] were announced. Subsequently, Wang [36] an-
nounced a technique for ﬁnding collisions in SHA-1 in 269
operations,1 rather than the 280 for which it was designed,
and Lenstra et al. [21] demonstrated a pair of X.509 cer-
tiﬁcates with the same distinguished name, different pub-
lic keys, and identical signatures, though no extension is
known which can generate such a pair with different distin-
guished names.
It should be emphasized at this point that none of these
results have translated into demonstrable attacks on real-
world protocols, though [21] comes uncomfortably close.
1In a presentation delivered at the Rump Session of CRYPTO 2005,
Shamir stated that Wang had improved the attack to 263 operations.
Also see http://www.csrc.nist.gov/pki/HashWorkshop/
2005/Oct31_Presentations/Wang_SHA1-New-Result.
pdf, the slides from Wang’s keynote speech at the NIST Cryptographic
Hash Workshop.
Conventionally, hash functions are designed to have
three properties:
Collision resistance It is computationally infeasible to ﬁnd
x, y, x 6= y such that H(x) = H(y).
Preimage resistance Given an output value y, it is compu-
tationally infeasible to ﬁnd x such that H(x) = y.
Second preimage resistance Given an input x′, it is com-
putationally infeasible to ﬁnd x such that H(x) =
H(x′).
The current generation of attacks address collision re-
sistance. MD5 is effectively dead from that perspective;
SHA-1 is much weaker than it should be, though ﬁnding
collisions is still impractical.
While
not
as
devastating
as
the
two
of
is
failures
resistance
the
collision
in-
Lucks and Daum have gen-
attack (see
that exploit
other
properties,
deed a serious issue.
erated Postscript ﬁles
http://th.informatik.uni-mannheim.de/
people/lucks/HashCollisions).
They took
advantage of a well-known property of Merkle-Damg˚ard
hash functions:
H(x) = H(y) ⇒ H(x||Σ) = H(y||Σ)
where Σ is an arbitrary string, provided that x and y are the
same length.
First, they generated two Postscript prologues that con-
tained a collision in what was, syntactically, a constant.
This constant was assigned to a variable. To each of these
ﬁles, they then appended a Postscript program that checked
the value of this variable and displayed one of two letters.
An attacker could persuade someone to digitally sign the
ﬁrst, harmless letter; this same signature would match the
second, harmful letter. Note, however, that to a great degree
this attack is enabled by the fact that users do not directly
view the Postscript code and rather use an interpreter. Sim-
ilar attacks can be demonstrated against such systems (e.g.,
HTML with JavaScript) even without the ability to ﬁnd hash
collisions [30] by exploiting conditional elements in the dis-
play system [15, 16].
Collision-ﬁnding attacks do not rule out all uses of a hash
function. In particular, the pseudo-random function prop-
erties are not affected at all. Furthermore, HMAC [20] is
probably safe, since the unknown component—the key—of
the inner hash function makes it impossible to generate a
collision at that stage; this in turn helps protect the outer
hash.
On the other hand, there is grave danger for many sit-
uations involving digital signatures or ﬁngerprinting. If a
would-be attacker can supply the message to be signed, that
same attacker could have prepared two versions of the mes-
sage, one innocuous and one harmful, while presenting only
the former. The attacks work because the victim inspects
the innocuous version and veriﬁes that it is acceptable. In
environments where victims do not carefully inspect data
before it is hashed, collision attacks only modestly increase
the threat level.
3 Overview of the Hash Transition Problem
Although the details of transition strategies for any given
protocol may vary, there are many common elements. In
this section, we provide an overview of the hash transi-
tion problem and the design goals that transition strategies
should attempt to fulﬁll.
The hash transition problem is a special case of the gen-
eral protocol transition problem. Whenever a new version
of a protocol is rolled out, designers and implementors must
ﬁgure out how to accomplish a smooth transition from old to
new versions with a minimum level of disruption. In a typ-
ical protocol transitional environment, there are three types
of agent:
Old Agents which only speak the older version.
Switch-hitting Agents which can speak both versions.
New Agents which can only speak the new version.
At the beginning of the transition, all agents are Old. At
the end of the transition (at least theoretically), all agents
are New. (In practice, of course, transitions of this nature
tend to persist for an arbitrarily long time, since old systems
never quite die off.) The purpose of a transition strategy
is to accomplish the transition between these states with a
minimum of disruption. A number of issues are common to
most, if not all, such transitions:
Backward compatibility Old agents and Switch-hitting
agents should be able to communicate using the older
version. Without backward compatibility, users have
an enormous disincentive to upgrade their implemen-
tations.
Newest common version When two Switch-hitting clients
communicate, they can either use the new or old ver-
sions of the protocol. Because the purpose of the tran-
sition is to deploy the newer version, it is desirable to
use that version where possible.
Downgrade protection An additional requirement for se-
curity protocols is to defend against version/algorithm
downgrade. Consider the situation where two peers
each support two cryptographic algorithms, one of
which is strong and one of which is weak. If an at-
tacker can force the peers to use the weaker algorithm,
he may be able to attack the communication. Where
possible, protocols should resist this attack.
Credentials versus implementations In typical public
key-based systems, a peer’s public key is authenticated
using certiﬁcates (in the case of the protocols being
discussed here, PKIX [13] certiﬁcates). Certiﬁcates
are a general credential and are not tied to any speciﬁc
revision of a given security protocol. Peers need to be
able to communicate with agents that have a variety of
combinations of new and old credentials and protocol
capabilities.
In the remainder of this paper, we consider the applica-
tion of these principles to two major Internet security pro-
tocols: S/MIME (a store-and-forward protocol) and TLS (a
session-oriented protocol). A longer version of this paper,
with analysis of other protocols and more details, appears
in [1].
4 S/MIME
The ﬁrst protocol we will consider is S/MIME [25, 26,
12]. S/MIME is a standard message encryption and authen-
tication protocol. In the most common modes, it uses pub-
lic key cryptography (RSA [14] and DH [28]) for key es-
tablishment, symmetric cryptography for bulk encryption,
Sender Old
Old
S/O
S/B
S/N
New
Old Old
Old Old
Old
-
-
Switch/Old
Either
New
New
Receiver
Switch/Both
Old
Old
Either
New
New
Switch/New New
-
Old
Send New
New
New
-
New
New
New
Figure 1. Interoperability table for S/MIME im-
plementations
and digital signatures (RSA and DSA [22]) for message
authentication/nonrepudiation. User public keys are trans-
ported/authenticated using PKIX [13] certiﬁcates.
There are ﬁve major types of S/MIME client; the types
of messages that each type of implementation should send
are shown in Figure 1.
1. Old clients.
2. Switch-hitting clients with only old certiﬁcates.
3. Switch-hitting clients with both types of certiﬁcate.
4. Switch-hitting clients with new certiﬁcates.
5. New clients with only new certiﬁcates.
Note that this table assumes perfect information about
the recipient’s capabilities, which is not always the case.
We now consider how to achieve interoperability in prac-
tice, which is a matter of trying to estimate the recipient’s
capabilities and create a message which they are most likely
to be able to decode. For the remainder of this section, we
focus on the behavior of Switch-hitting clients, since Old
and New clients only have one possible behavior.
4.1 The Initial Message
The ﬁrst case we consider is the case where a user is
sending a message to someone with whom he has never
communicated before. There are two possible sub-cases:
1. The sender does not have the recipient’s certiﬁcate.
2. The sender has the recipient’s certiﬁcate.
We consider each sub-case in turn.
4.1.1 Sending Without a Recipient Certiﬁcate
If the sender does not have access to the recipient’s certiﬁ-
cate, then he is subject to two limitations. First, he cannot
encrypt because he does not have the public key to encrypt
under. Second, he has no information about the recipient’s
capabilities, In particular, he cannot safely assume that the
recipient’s software will be able to process new hash func-
tions.
Choice of certiﬁcate A sender with only one certiﬁcate
must use that certiﬁcate. The difﬁculty comes when a
sender has two certiﬁcates, one generated with an old hash
function, and one with a new hash function. The possibil-
ities, of course, are to use only one certiﬁcate or—because
S/MIME allows multiple signatures—to use both. Any one-
certiﬁcate strategy guarantees that some class of recipients
will not be able to verify the message. Using both certiﬁ-
cates preserves the possibility that the recipient can verify
the message.
In order for this to work, however, recipients must
be able to correctly verify messages with multiple signa-
tures when one of them is unveriﬁable. Unfortunately, the
S/MIME speciﬁcation is fairly vague on this point. An un-
scientiﬁc poll of S/MIME implementors indicates that sup-
port for this option is spotty at best. [27, 11, 9].
Because receiver behavior is unpredictable, senders must
attempt to estimate what sorts of implementation receivers
are likely to have. This probably means choosing inter-
operability with the most popular strategies as a default
(which are currently the older, weak, algorithms) and allow-
ing users the option to conﬁgure a new behavior. This is irri-
tating in that it involves a manual step if the sender guesses
wrong. However there are already a number of non-security
scenarios in which users must retransmit unreadable mes-
sages (bad attachment formats, HTML-vs-ASCII text, etc.)
so it’s not totally foreign to users.
Choice of digest algorithm Once the certiﬁcate has been
chosen, the sender must choose a digest algorithm to di-
gest the message before signing. This choice is made in-
dependently for each signature, so it is possible the mes-
sage will be digested twice.
In general, if the certiﬁcate
being used was generated with one of the old algorithms
(MD5, SHA-1), the message should be digested using SHA-
1, which receivers are required to accept by section 2.1 of
RFC 3851 [26]. This minimizes the chance that the recipi-
ent will not be able to verify the message signature. (MD5
should not be used at all for message digests, even if the
certiﬁcate uses it.)
If the certiﬁcate being used was digested with a new hash
algorithm, we recommend that the sender use the same al-
gorithm to digest the message, on the grounds that if the
recipient can use the digest algorithm to verify the certiﬁ-
cate they can use it to verify the message. This runs the risk
that the recipient will be using a separate toolkit to verify
the certiﬁcate signature than they used to verify the mes-
sage signature; however we are not aware of any S/MIME
client that behaves in this way. This algorithm has the at-
tractive property that it automatically works correctly with
DSA, which can only sign SHA-1 digests.
4.1.2 Sending With the Recipient’s Certiﬁcate
If the sender has the recipient’s certiﬁcate(s) then the situa-
tion is simpler. We believe that it is a reasonable assumption
that implementations can verify their own certiﬁcates and
therefore must implement whatever digest algorithm was
used to create them. If the recipient has only one certiﬁ-