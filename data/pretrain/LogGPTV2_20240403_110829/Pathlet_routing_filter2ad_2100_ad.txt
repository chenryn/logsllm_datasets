Finally, we say that protocol P can emulate Q if for every
conﬁguration c2 of Q, there is a conﬁguration c1 of P such
that c1 covers c2. In other words, if P can emulate Q, then
P can match every possible outcome of Q in terms of allowed
paths, prohibited paths, and forwarding table size.
This deﬁnition is limited. We have chosen not to incor-
porate aspects of policy like the price of a route, visibility
of routes, misbehavior outside the rules of the protocol, or
game-theoretic aspects like which outcomes might actually
arise under selﬁsh behavior, or the control plane as discussed
above. However, we believe that policy emulation provides
a way to begin reasoning about the relative strengths of dif-
ferent protocols.
5.3 Protocols pathlet routing can emulate
Due to space constraints, we only brieﬂy describe some of
the protocols and the emulation relationships between them.
BGP [25]. We outlined how pathlet routing can emulate
BGP (in both the data an control planes) in Section 4.2.
NIRA [30] oﬀers more choice to sources than BGP, while
simultaneously reducing control plane state.
IP addresses
are assigned so they encode a speciﬁc path between a router
and the “core” of the Internet or a peering point. A source
Feedback-based routingPathlet routingStrictsource routingLoosesource routingMIRONIRALISPRoutingDeﬂections,Path splicingBGP117and destination IP address together specify a full end-to-
end route. The most challenging part of emulating NIRA
is that an AS can allow or prohibit routes based on the up-
stream hops as far back as the core providers. In contrast,
in the usages of pathlet routing we have seen so far, policies
depend only on the immediately prior hop and any down-
stream hops. However, it is possible to construct vnodes to
encode the packet’s upstream hops, using the same amount
of state as NIRA. This requires some coordination of vnode
names in the control plane between neighbors, but no data
plane changes.
The basic NIRA protocol is limited to valley-free routes,
so it cannot emulate the other protocols we consider. An ex-
tension including a source-routing option is described in [29]
but is not analyzed here.
Locator/ID Separation Protocol (LISP) [9] maps a
topology-independent endpoint ID into an egress tunnel router.
BGP is used to tunnel a packet to its destination’s egress
tunnel router, which delivers it to the ﬁnal destination us-
ing the endpoint ID. This arrangement can reduce forward-
ing state since most routers need only know about egresses,
rather than all endpoints. Pathlet routing can emulate LISP
by concatenating a pathlet representing the tunnel and a
pathlet representing the remainder of the route.
IP strict source routing [8], IP loose source rout-
ing [8] and MIRO [28]. Pathlet routing can emulate these
protocols in a straightforward manner. The common thread
in these protocols and LISP is routing via waypoints or tun-
nels, which we ﬁnd can be emulated with MIRO’s data plane.
(Note, however, that MIRO’s data and control planes were
intended for a signiﬁcantly diﬀerent usage scenario in which
most paths are set up via standard BGP and a relatively
small number of additional tunnels are constructed.) We
omit a full description due to space constraints.
5.4 Protocols pathlet routing cannot emulate
Feedback Based Routing (FBR) [32] is source routing
at the AS level, with each link tagged with an access control
rule. A rule either whitelists or blacklists a packet based
on preﬁxes of its source or destination IP address. Pathlet
routing cannot emulate FBR for two reasons. First, it is
diﬃcult for pathlet routing to represent policies based on
upstream hops. Essentially, the only way to carry utilizable
information about a packet’s origin is to encode the origin
into the vnodes the packet follows, which would increase the
number of necessary vnodes and hence the amount of state
by a factor of n relative to FBR, where n is the number of
nodes in the network. (Note that we solved a similar prob-
lem in emulating NIRA, but NIRA’s solution used the same
amount of state as ours.) The second problem in emulat-
ing FBR is that FBR has both blacklisting and whitelisting,
while pathlet routing eﬀectively has only whitelisting. FBR
can therefore represent some policies eﬃciently for which
pathlet routing would require signiﬁcantly more state.
But FBR cannot emulate pathlet routing either. For ex-
ample, controlling access based only on source and destina-
tion address ignores intermediate hops which can be taken
into account by pathlet routing (and BGP).
Routing deﬂections [31] and interdomain path splic-
ing [23]. In these protocols, each hop can permit multiple
alternate paths to the destination. The packet includes a
“tag” [31] or “splicing bits” [23] to specify which alternative
is used at each step. The selected path is essentially a pseu-
dorandom function of the tag.
Pathlet routing cannot emulate these protocols due to the
handling of tags. In [31] the tag has too few bits to repre-
sent the set of all possible routes that the routers intended
to permit, so the de facto set of allowed routes is a pseudo-
random subset of those—an eﬀect which to the best of our
knowledge pathlet routing cannot reproduce. One mode of
operation in [23] behaves the same way. A second mode of
operation in [23] uses a list of source-route bits to explicitly
select the next hop at each router; pathlet routing would be
able to emulate this version.
But there is also a problem in the control plane. Even
if pathlet routing can match the allowed paths in the data
plane, actually using the paths would require the senders to
know what FIDs to put in the packet header. Because [31,
23] allow per-destination policies as in BGP, this can result
in a large amount of control plane state in pathlet rout-
ing. Routing deﬂections and path splicing avoid this control
plane state by not propagating alternate path information;
the tradeoﬀ is that the senders and ASes in those protocols
cannot identify which end-to-end paths are being used.
6. EXPERIMENTAL EVALUATION
We implemented pathlet routing as a custom software
router, and evaluated it in a cluster environment. We de-
scribe the structure of our implementation in Sec. 6.1, the
evaluation scenarios in Sec. 6.2, and the results in Sec. 6.3.
6.1 Implementation
We implemented pathlet routing as a user-space software
router, depicted in Fig. 4. Each router runs as a separate
process and connects to its neighbors using TCP connec-
tions, on which it sends both data and control traﬃc.
Our router contains three main modules: a vnode man-
ager, a disseminator, and a controller. Through the imple-
mentation we found that it is possible to shield the core
policy module (the controller) from the details of pathlet
dissemination and vnode management, making it compact
and easy to tune to the speciﬁc needs an AS might have.
We describe these three modules brieﬂy.
Figure 4: Structure of the software pathlet router.
ControllerDisseminatorwirePathlet advertisements and withdrawals Vnode ManagerPathlet StoreIngress VnodeAnnounce-mentMessages   Data PacketsVnodePathlet (un)availableStore these local pathletsAdvertisethese pathlets tothese peersInstall/remove forwarding entriesCreate/deleteVnodes118The vnode manager is responsible for directing incoming
data packets to vnodes, which store their forwarding tables
and perform the lookup, as well as for sending the data
packets out to the next hop. The vnode manager also sends
and receives control messages that inform peers about the
ingress vnodes.
The disseminator stores the pathlets and sends and re-
ceives pathlet announcements and withdrawals. When a
new pathlet becomes available or a pathlet is withdrawn,
it notiﬁes the controller. When the controller decides to ad-
vertise or withdraw a pathlet from a particular neighbor, it
calls the disseminator to execute the decision.
While the vnode manager and the disseminator are gen-
eral and oblivious to the AS’s policy, the controller encap-
sulates the policy logic. It implements the policy by con-
structing and deleting pathlets and vnodes and by deciding
which pathlets to announce to which peers. In our imple-
mentation, one makes a router an LT router or BGP-style
router by picking the corresponding controller.
6.2 Evaluation scenarios
Policies. We tested three types of policies: LT policies,
Path Vector-like policies, and mixed policies, with 50% of
nodes randomly chosen to use LT policies and the rest using
PV.
The special case of LT policies that we test are valley-free
routes, as in Fig. 2. The PV policies emulate BGP. Specif-
ically, we mimic the common BGP decision process of pre-
ferring routes through customers as a ﬁrst choice, through
peers as a second choice, and providers last. We then break
ties based on path length and router ID, similar to BGP. We
use the common BGP export policies of valley-free routes.
Topologies. We tested two types of topology. Internet-
like topologies annotated with customer-provider-peer re-
lationships were generated using the algorithm of [7]. Each
AS is represented as a single router. Random graphs were
generated using the G(n, m) model, i.e., n nodes and m ran-
dom edges, excluding disconnected graphs. In the random
graph there are no business relationship annotations; ASes
prefer shortest paths and all paths are exported. Unless oth-
erwise stated, these graphs have 400 nodes and an average
of 3.8 neighbors per node.
Event patterns. We show results for several cases: the
initial convergence process; the state of the network after
convergence; and a sequence of events in which each link
fails and recovers, one at a time in uniform-random order,
with 8 seconds between events for a total experiment length
of 3.6 hours. We implemented link failures by dropping the
TCP connection and link recovery by establishing a new
TCP connection.
Metrics. We record connectivity, packet header size, for-
warding table size, number of control plane messages, and
control plane memory use. The CDFs of these metrics that
we present are the result of three trials for each evaluation
scenario, each with a fresh topology. We show all data points
(routers or source-destination pairs) from all trials in a single
CDF.
6.3 Results
Forwarding plane memory. Fig. 5 shows a CDF of the
number of forwarding table entries at each router. The num-
ber of entries varies with the node degree for LT routers, and
with the size of the network for PV nodes. As a result, the
LT nodes have a mean of 5.19 entries in the all-LT case and
5.23 in the mixed case. PV averages 400.5 entries in the
mixed and all-PV cases.
We also analyzed an AS-level topology of the Internet gen-
erated by CAIDA [5] from Jan 5 2009. Using LT policies in
this topology results in a maximum of 2, 264 and a mean of
only 8.48 pathlets to represent an AS. In comparison, BGP
FIBs would need to store entries for the between 132, 158
and 275, 779 currently announced IP preﬁxes, depending on
aggregation [2]. Thus, in this case LT policies oﬀer more
than a 15, 000× reduction in forwarding state relative to
BGP.
Route availability. One of the principal advantages of
multipath routing is higher availability in the case of failure,
since the source can select a diﬀerent path without waiting
for the control plane to reconverge.
We measure how much LT policies improve availability as
follows. We allow the network to converge, and then take a
snapshot of the routers’ forwarding tables and the pathlets
they know. We then select a random set of links to be failed,
and determine for each pair of routers (X, Y ) whether there
is a working X ; Y route in the data plane, using only the
pathlets that X knows.
If so, then X has a continuously
working path to Y , without waiting for the control plane to
detect the failures and re-converge. Thus, we are measuring
how often we can avoid PV’s transient disconnectivity. (This
measurement ignores the algorithm X uses to ﬁnd a working
path among its options, which is outside the scope of this
work. However, a simple strategy, like trying a maximally
disjoint path after a failure, is likely to perform well.)
The results of this experiment are shown in Fig. 6. Rel-
ative to PV, which emulates the availability of BGP, LT
has signiﬁcantly improved availability.
In fact, it has the
best possible availability assuming ASes allow only valley-
free routes, because every path allowed by policy is usable
in the data plane. In the random graph topology, we see a
bigger connectivity improvement in part because the possi-
ble paths are not constrained by valley-freeness.
In the mixed environment, the LT nodes obtain the ma-
jority of the improvement of a full deployment of LT nodes.
To further illustrate this relationship, Fig. 6 shows how con-
nectivity improves as a function of the fraction of nodes that
use LT policies, in an 80-node random graph. Here the num-
ber of failed links is ﬁxed at 5, and we report the mean of 5
trials.
In the random graph topology in Fig. 6, we also show
availability when fewer pathlets are advertised: in particu-
lar, each router advertises only the shortest path tree (see
Sec. 3.3). This worsens availability, but improves the num-
ber of control messages, as we describe next.
Control plane messages. Fig. 7 shows the CDF of the
number of messages received by a router following a link
failure or recovery event.
Consider the Internet-like topology. LT has more objects
being announced than PV: an average of 5.19 pathlets per
node, vs. 1 per destination in the PV case. One might
therefore expect 5.19× as many messages. However, LT has
only 1.69× as many as PV. There are two factors that cause
this. First, when a link recovers, PV needs to send many
messages because its path preferences (selected by the BGP
119Figure 5: Forwarding table
(FIB) size for the Internet-
like topology (left) and the
random graph (right).
Figure 6: Probability of disconnection for a varying number of link failures in the Internet-like topology (left)
and the random graph (center), and as a function of the number of LT nodes (right).