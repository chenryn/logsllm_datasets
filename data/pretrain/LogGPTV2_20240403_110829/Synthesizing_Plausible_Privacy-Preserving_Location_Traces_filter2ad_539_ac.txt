(cid:2)
) = 1r(cid:5)=r
Consider now the computation of the geographic similarity.
(cid:2), the computation according
For the case, d(r, r
to closed-form of (7) takes O(T 2 · R2) operations, where
T is the number of time periods and R is the number of
locations (regions). For arbitrary d(·) with no closed-form
the geographic similarity is obtained through
expressions,
T 2 · R EMD computations. Each of these EMD computations
involves minimizing the Mallows distance, that is equivalent
to solving the linear program given by (2).
D. Semantic Similarity Metric
The semantic similarity metric builds upon the basic as-
sumption that for two individuals u and v there exists an
(unknown) semantics mapping σ of locations R onto itself
−1(R) for v
(i.e. a permutation) such that R for u, and σ
semantically match. It is important to note that assuming such
a mapping does not commit us to trying to learn it based on
modeling location semantics directly. Instead, we deﬁne the
hidden semantic similarity between u and v as the maximum
geographic similarity taken over all possible mappings σ. We
deﬁne semantic similarity metric as follows.
Deﬁnition 3. Let σ be the mapping of locations of u to
locations of v. Let r, r(cid:2), and r(cid:2)(cid:2) be random variables for
(cid:2) be two time periods. We deﬁne the
locations, and τ and τ
semantic dissimilarity between u and v for moving in the
sequence of time periods {τ, τ
(cid:2) (u), pσ(r(cid:2)(cid:2))
Dv
u({τ, τ
(cid:2)}) = min
σ
E
Md(p
(8)
where the Mallows distance Md(·) is computed over the
random variable r(cid:2) and the expectation is computed over the
random variable r given time periods τ and τ
σ(r),τ,τ
(cid:2) (v))
(cid:2).
,
Now, we deﬁne the semantic similarity between u and v
(cid:5)
(cid:4)
(cid:2)} as
r(cid:2)
r,τ,τ
over any sequences of time periods as
simS(u, v) = 1 − E [Dv
u({τ, τ
(cid:2)})]
,
(9)
zs
where zs is a normalization constant equal to the maximum
value of (the expectation of) the Mallows distance given d(·),
ensuring that the semantic similarity always lie in [0, 1].
What we compute in (8) is the minimum geographic mo-
bility dissimilarity between u and v where the locations of v
are relabeled and mapped to locations of u according to the
u (which is the σ that minimizes 8). The
permutation function σv
intuition is the following. Consider two individuals u and v
are at r and σ(r), respectively, at time period τ. The Mallows
distance Md computes how dissimilar their movement will
be to the next location which are represented with random
variables r(cid:2) for u and σ(r(cid:2)(cid:2)
) for v. If, according to a mapping,
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
σ
r
the way that they move between these locations is similar,
they behave similarly with respect to those locations. If this
is true across different time periods and location pairs, their
mobilities are similar. So, the semantic similarity between two
u.
individuals is determined by σv
We compute this metric at two different levels of accuracy of
the mobility model. If we only consider the visiting probability
π part of each individual’s mobility proﬁle, we compute simS
as follows: Let us consider the hamming distance function
(cid:2)). In this case, we can compute the
d(r, r
semantic similarity metric as
(cid:3)
) = 1r(cid:5)=σ
(cid:3)
−1(r
(cid:2)
1 −
Pr{τ} max
min{πr
τ (u), πσ(r)
τ
(v)}.
(10)
τ
Note that
the computation of (10) requires ﬁnding the
mapping σ which maximizes the inner term for each time
period τ. Since there are R! possible candidates for the
maximum mapping σ, a brute-force approach is inefﬁcient.
However, the problem’s structure resembles that of a linear
assignment. Focusing on the inner sum, we see that each term
(each r) can be associated with R values of σ(r) independently
of the other components of σ. To recast
the problem as
a linear assignment, we construct a bipartite graph where
the nodes represent R and σ(R), and each edge represents
the association (through σ) of r with σ(r). The maximum
weight assignment of the constructed bipartite graph gives the
permutation σ. The running time of this procedure is O(T ·R3)
using the Hungarian algorithm [37].
proﬁle (cid:5)p, π(cid:6), it can be computed as follows:
1−
In the case where we consider the more accurate mobility
(cid:3)
(cid:2) (u)} .
(11)
It is not known whether there is an efﬁcient algorithm
to compute the semantic similarity according to (11). The
difﬁculty comes from having to consider assignments of pairs:
)), which makes this computation look
(r, r
similar to the Quadratic Assignment Problem (QAP) [38],
known to be NP-Hard and APX-Hard. But, (11) can be ap-
proximated using e.g., the Metropolis-Hastings algorithm [35]
which we use in the case of considering both visiting and
transition probabilities (see [34] for details), or Simulated
Annealing [8].
r,τ (u) min{pr
) to (σ(r), σ(r
(cid:2) (u), pσ(r
πr,τ (u)pτ
(cid:3)
σ(r),τ,τ
max
r,τ,τ
τ,τ
(cid:2))
r,r
σ
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
V. SYNTHESIZING LOCATION TRACES
In this section, we present the details of our algorithms for
generating synthetic traces. See also Figure 2. Note that the
processes of generating and using fake traces are completely
separate. When a set of traces is synthesized, they can be used
in different scenarios accordingly.
A. Transforming Traces into Semantic Domain
We sample a dataset S from some real traces. We use traces
in S as seeds to generate synthetic traces. Each seed trace
in the dataset comes from a different individual. Generating
a fake trace starts by transforming a real
trace (taken as
552552
v
u from (8)
SemanticSimilarity(u, v)
Compute mobility models (cid:3)p(u), π(u)(cid:4) and (cid:3)p(v), π(v)(cid:4)
Compute optimal mapping σ
Compute semantic similarity simS(u, v) from (9)
Return simS(u, v), σ
SemanticClustering(R, S, κ)
Initiate weighted graph G with locations R as vertices
Forall u, v ∈ S, u (cid:5)= v:
v
u
v
v
u, σ
v
u(r):
(cid:2)) + s
v
u
Let edge weight wG(r, r
u ← SemanticSimilarity(u, v)
(cid:2) ∈ R such that r
(cid:2) = σ
(cid:2)) ← wG(r, r
∃a1, · · · , ak ∈ A s.t. ∀i, |simS(seed, f ake) − simS(ai, f ake)| ≤ δd
Let s
Forall locations r, r
Let C ← K-Means(G, κ)
Return C
PrivacyTest(f ake, seed, A, δs, δi, δd)
Let geographic similarity sim ← simG(f ake, seed)
Let intersection between fake and seed int ← |intersection(f ake, seed)|
Let pl ← T RU E if
Return TRUE if int ≤ δi and sim ≤ δs and pl
Synthesizer(R, S, A, all parameters)
Let aggregate mobility model (cid:3) ¯p, ¯π(cid:4) be average of (cid:3)p(u), π(u)(cid:4) over all u ∈ S
Let C ← SemanticClustering(R, S, κ)
Forall seed ∈ S:
Let C(cid:2) ← C
Update C(cid:2)
by removing locations in any partition with probability parc
Let semantic seed semseed ← seed
i where r ∈ C(cid:2)
Update semseed by replacing locations r in it with C(cid:2)
i
Update semseed by removing the location r = seed(t) from time t
with probability parl
Δt with probability parm
Update semseed by merging locations that are located with time distance
Let f ake ← HMMDecode(semseed, (cid:3) ¯p, ¯π(cid:4))
If PrivacyTest(f ake, seed, A, δs, δi, δd)
Δt
Let F ← F ∪ f ake
Return F
Fig. 2: Trace synthesis algorithm. We present it simpliﬁed for the
case with a single time period. If C is a semantic clustering, then
Ci represents the set of locations belonging to the cluster i. The
procedure HMMDecode is described in Section V-B.
(cid:2)
u(r
seed) to a semantic trace. To this end, we require to know
the semantic coordinates of the seed trace. We compute the
semantic similarity between all locations in R, and create a
location semantic graph G(cid:5)R, E, w(cid:6) such that the vertices are
in R and the weight wG(r, r
) on the edge between locations
(cid:2) is the weighted sum of the number of pairs of users
r and r
(cid:2) are semantically mapped (i.e.,
u and v for whom r and r
(cid:2)
)), weighted according to their similarity. Then, we
r = σv
create the equivalent semantic classes C by running a clus-
tering algorithm on this graph. We make use of the k-means
clustering algorithm, and we choose the number of clusters
such that it optimizes the clustering objective. We present the
sketch of this algorithm in Figure 2 – SemanticClustering().
the seed location trace seed into its
corresponding semantic trace semseed by simply replacing
each location in the trace with all its semantically equivalent
locations (according to the semantic classes C). Figure 3
depicts an example of such a semantic seed. Intuitively, this
composite trace encompasses all possible geographic traces
that are semantically similar to the original seed trace. To be
more ﬂexible with respect to the traces that we can generate,
We then convert
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
we add some randomness to the semantic seed trace. In the
transformation process of the seed trace into the semantic
trace, we sub-sample locations from the semantic classes as
opposed to using them all. We also remove each location in
a cluster with probability parc. The result is a new cluster
C(cid:2). We allow locations of different classes to merge into each
other around time instants where the user moves from one
class to the other. We add a location from one time instant to
Δt.
another with a Δt gap with a geometric probability parm
B. Sampling a Trace from the Semantic Domain
Any random walk on the semantic seed trace that travels
through the available locations at each time instant is a valid
location trace that is semantically similar to the seed trace.
However, the synthetic traces we want to generate also need
to be geographically consistent with the general mobility of
people in the considered area. We cast the problem of sampling
such traces as a decoding problem in Hidden Markov Models
(HMMs) [41]. The symbols are locations, the observables
are the semantic classes (which are the set of semantically
equivalent locations in the same class), and the transition
probability matrix follows the aggregate mobility model.
We construct the aggregate mobility model by averaging
over the mobility models of all traces in dataset S, as well as
giving a small probability to the possible movements between
locations according to their distance and connectivity. More
(cid:2)
precisely, we compute the aggregate transition probability ¯pr
−2, where  is a
−1
r
as z
r
small constant, and zr is the normalizing factor. We compute
the aggregate visiting probability ¯πr as the average of πr(u).1
By decoding the semantic trace into geographic traces, we
generate traces that are plausible according to aggregate mo-
bility models, i.e., there could be an individual who could have
made that trace. Among existing HMM decoding algorithms,
we use the Viterbi algorithm [51] that ﬁnds
r (u) +  · max(1, d(r, r
· (cid:2)
(cid:2)
))
(cid:2)
u∈S pr
Pr{f ake|semseed(t),(cid:5)¯p, ¯π(cid:6)}
arg max
f ake
assuming that f ake(t) can only choose from locations in
semseed(t). Finding the most likely fake trace is equivalent
to ﬁnding the shortest path in an edge-weighted directed graph
where each location at time instant t is linked to all locations
at the subsequent time in the semantic seed trace.
By using this encoding technique, we make sure that the
sampled trace is consistent with the generic mobility and has
a signiﬁcant probability of (geographically) being a real trace.
However, the Viterbi algorithm produces a single trace (which
is the most likely one). To generate multiple fake traces per
seed, we add randomness to the trace reconstruction of Viterbi.
We modify the Viterbi algorithm, which originally, at each step
(time instant) selects the most probable location in the path;
1We use the aggregate mobility model instead of that of a speciﬁc user
to avoid constraining the reconstructed geographic trace to follow the user’s
proﬁle too closely. For example, this means that the produced traces may visit
locations which are never visited in the seed trace. This allows for greater
utility because the input traces only cover a very small subset of the set of
geographically meaningful traces.
553553
seed:
seed:
f
f
t
t
t
t
z
z
x
x
x
x
p
p
semantic classes:
semantic classes:
semantic seed:
semantic seed:
synthetic trace:
synthetic trace:
y
y
d
d
f
f
y
y
d
d
f
y
y
d
d
t
t
y
y
d
d
t
t
y
y
d
d
t
t
y
y
d
d
t
t