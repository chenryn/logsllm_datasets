(GAT) [20] are used. Both aim to generate meaningful node feature representa-
tions for the task of anomaly classification. TAGCNs realize graph convolution
on the vertex domain by applying a set of fixed-size learnable filters which are
adaptive to the topology of the respective target graph. The TAGCN result
is further used as input for a GAT layer which enables nodes to attend to
TELESTO: A Graph Neural Network Model for Anomaly Classification 219
generate features dependant on the vertex neighborhood features. Since we
employGATlayerswithmulti-headattention,thechoiceofthenumberofheads
has a direct impact on the output dimensionality of the graph transformation
moduleastheoutputsofallparallelattentionmechanismsareconcatenated.At
each GCNN level, the TAGCN layer exploits the spatial structure of its input
graph and respectively updates the node features. Those are forwarded to the
GAT layer as well as to the next TAGCN layer. TELESTO allows this combi-
nation of TAGCN and GAT layers to be arbitrarily stacked. Finally, the node
feature matrices collected from the respective GAT layers across all levels are
aggregated using Jumping Knowledge (JK) [24]. JK combines the GAT layer
outputs by a configurable aggregator functionG. We choose the long short-term
memory (LSTM) aggregator, due to its flexibility to learn weight coefficients
and thus a weighted combination of the given matrices. This allows the model
to prioritize certain level outputs.
The graph transformation module is followed by a final feed forward (FFF)
block and a sublayer connection. The FFF block consists of two dropout lay-
ers and two linear layers in alternating order with a single activation layer in
between. Next, graph embeddings are computed by utilizing the global pooling
method Global Attention [16]. It employs a neural network to learn attention
coefficients based on node features, which are then used to aggregate nodes
graph-wise, resulting in an embedding for each graph. A final 1D convolution is
applied on the graph embedding and the result is batch-normalized afterwards.
The convolution layer is configured to use ten filters, a filter size of nine and
suitablezero-paddingtoensuretheequivalenceofinputandoutputdimensions.
The resulting ten feature maps are averaged. Assuming a classification problem
withC classes,anadditionallinearlayerneedstobeaddedtothebottomofthe
architecture to transfer the graph embedding size to an output vector of size C.
A subsequent softmax layer calculates a probability distribution over all classes.
3 Evaluation
We investigate a case study based on anomalous services deployed in an infras-
tructure as a service (IaaS) cloud environment to evaluate our model. Exper-
iments are conducted to assess its goodness by classification of synthetically
injected anomalies.
3.1 Testbed and Experiment Design
Toevaluatethecapabilitiesofourmodel,wedeployacloudinfrastructureanduse
a IaaS policy to run two applications. OpenStack 11.0.0 Stein1 and Ceph 12.2.5
luminous2 are installed on a commodity cluster of 21 nodes, each possessing an
IntelXeonX3450CPU(PI:EMAIL),16GBRAM,3×1TBHDDand
1 https://www.openstack.org/software/stein/.
2 https://docs.ceph.com/docs/master/releases/luminous/.
220 D. Scheinert and A. Acker
2 × 1 GBit Ethernet connection. Twelve nodes are used as hypervisors, eight as
storagenodesandoneasanetworkandcontrollernode.Furthermore,threehyper-
visorhostgroups(1/1/10split)arecreatedtoseparatetheapplicationloadgener-
ationfromtheapplicationdeploymentitself.AvirtualIMS3andacontentstream-
ingservice(CS)areusedasexampleapplicationshostedwithinthecloud.Varying
loadisgeneratedagainstbothsimulatinguseraccess.AllVMsoperateonUbuntu
16.04.3LTSunderLinuxkernelversion4.4.0-128-generic.Alldeploymentscripts
relatedtobothservicesareavailableatgithub4.
Theproposedanomalyclassificationmethodrequireslabeleddatatobeeval-
uated.Therefore,differentanomalytypesaresyntheticallyinjectedintotheVMs
and hypervisors at runtime. An injector agent was deployed on each hypervisor
and application host VM. All injected anomaly types are listed in Table1. A
group-based injection policy was used throughout the experiments, means that
all VMs of one service group (e.g. bono, hypervisor, etc.) are regarded as one
entity. Note that all ten hypervisors are put into one component group. An
injection into any component counts as an injection for the whole group. Dur-
ing the experiment, an initial period of six hours without anomaly injections is
defined.Next,eachanomalyisinjectedfivetimesintoeachgroupforfourtofive
minutes. After that, one minute of grace time is waited until the next injection
is performed, i.e. there are no overlapping injections. Start and stop times are
loggedandusedasthegroundtruth.Agentsaredeployedonthehypervisorsfor
monitoring that sample KPIs such as CPU utilization or network I/O statistics
at a frequency 2Hz.
Table 1.Listofinjectedanomaliesandtheirabbreviationstogetherwiththeirrespec-
tive description and parametrization.
Anomaly Description
CPU overutilization (CPU) Utilize 90% of available CPU
Abnormal disk utilization (ADU) Constant disk read and write operations
Memory leak (MEL) Incremental allocation of x MB main memory
every y seconds
x=1, y=3 for vIMS and CS VMs
x=2, y=3 for Hypervisors
Abnormal memory allocation Allocate x MB of memory
(AMA) x=450 for vIMS VMs
x=900 for CS VMs
x=2000 for Hypervisors
Network overload (NOL) Start to download large files
3 https://www.projectclearwater.org.
4 https://github.com/IncrementalRemediation/testbed deployment.
TELESTO: A Graph Neural Network Model for Anomaly Classification 221
3.2 Hyper-Parametrization and Training Setup
Monitored KPIs of each node are modelled as multivariate time series X. Each
sample of every single series is X t(i) is preprocessed by rescaling to the range
(0,1)withmin-maxnormalization.ThelimitsofmostKPIsarewell-known(e.g.
number of CPU cores and their frequency etc.). For some KPIs like latencies
between network endpoints, context switches or cache misses, it is challenging
to set upper or lower boundaries. Those are determined within the training
dataset and used throughout testing.
Table 2. Hyper-Parametrization and Training Setup
Aspect Configuration
Hardware GeForce RTX 2080 Ti GPU
Implementation PyTorch, PyTorch Geometric [7]
Graph Construction Window size = 20, stride = 1
Optimizer Adam [14] with learning rate = 10−3, β 1 =0.9, β 2 =0.999
Regularization Weight decay = 10−5, dropout probability = 50%
Loss Cross Entropy
Diverse Epochs = 15, Batch size = 128, Xavier [8] weight initialization
MostspecificationsrelatedtothetrainingofTELESTOarelistedinTable2.
We employ leave one group out (LOGO) cross-validation for data splitting, i.e.
thefiveinjectionsofeachanomalyandeachservicecomponentaresplitas3/1/1
asatraining/validation/testsplit.ForTELESTOitself,wechooseagraphnode
feature dimensionality of 64 and set the number of graph transformation levels
to 5. For the TAGCN layers, we choose k = 3 fixed-size learnable filters as
recommended in [5]. For the GAT layers, we choose K = 8 parallel attention
mechanismstoproducerichnodefeatureswithmulti-headattention.Lastly,the
JKLSTM-aggregatorisequippedwithsevenlayersinordertolearnareasonable
nodeweightingbasedonnodefeatures.WechooseELU[3]asactivationfunction
for the FFF block. The final softmax calculates a distribution over anomaly
classes, whereof the highest is used as the prediction target.
3.3 Anomaly Classification
In this section, the proposed model architecture will be evaluated on the data
described in subsubsection 3.1. TELESTO and its default configuration is com-
pared against a GCN architecture [15] and a GIN architecture [23]. GCN is a
reasonable choice as it is a common benchmark, whereas GIN is selected due
to it achieving state-of-the-art results both for node classification and graph
classification on several benchmark data sets [23]. All models are trained on
each service node individually. Moreover, each experiment is run 10 times and
222 D. Scheinert and A. Acker
Table 3. The results of anomaly classification on the cassandra data set.
Model Metric Split 1 Split 2 Split 3 Split 4 Split 5 ∅
GCN Accuracy 0.389 0.463 0.360 0.398 0.385 0.399
Recall 0.389 0.462 0.358 0.396 0.385 0.398
Precision 0.272 0.428 0.256 0.222 0.288 0.293
F1-Score 0.310 0.436 0.295 0.269 0.320 0.326
GIN Accuracy 0.447 0.455 0.562 0.452 0.400 0.463
Recall 0.446 0.455 0.562 0.452 0.399 0.463
Precision 0.408 0.422 0.569 0.363 0.289 0.410
F1-Score 0.421 0.436 0.564 0.402 0.335 0.432
TELESTO Accuracy 0.796 0.804 0.894 0.822 0.939 0.851
Recall 0.796 0.803 0.894 0.822 0.939 0.851
Precision 0.825 0.732 0.920 0.870 0.956 0.861
F1-Score 0.810 0.764 0.906 0.844 0.948 0.854
the results are averaged in order to cancel out the effects of unfavorable weight
initialization. Both the GCN architecture and the GIN architecture utilize two
of their respective layers. For graph classification, the node features are added
across the node dimension for each graph, followed by two linear layers with
dropout in between and a final softmax layer. Specific to the GCN architecture
isthehiddenlayersizeof32andtherow-normalizationofinputfeaturevectors.
For the GIN architecture, the hidden layer size is set to 64 while for each GIN
layer, the input is batch-normalized, the initial value of (cid:2) is set to 0 and a mul-
tilayer perceptron (MLP) is internally used for mapping the node features from
the input dimension to the hidden dimension. If not specified otherwise, we use
the values from the setup summarized in Table2.
A detailed breakdown of the results is given in Table3 with a focus on the
cassandra service node. It can be seen that the proposed model outperforms
both comparative models. Therefore we conclude that the proposed model is
suitable for anomaly classification based on multivariate time series data. Note
thatforTELESTO,theaverageF1-scoreof0.854isclosetoitsreportedaverage
accuracy.Ingeneral,thisisanindicationforagoodmodelasthebalancedrecall
and precision are not strongly different from the accuracy. In contrast to that,
both comparative models appear to encounter difficulties during training. The
GCN architecture achieves an average F1-score of 0.326 whereas the GIN archi-
tectureperformscomparablybetterwithanaverageF1-scoreof0.432.Itcanbe
observed that the reported F1-scores differ from the achieved average accuracy
of these architectures. Table3 also shows a high variability between splits. For
instance,TELESTOachievesanaverageF1-scoreof0.906onsplit3butanaver-
ageF1-scoreof0.764onsplit2.Moreover,aninvestigation ofthecorresponding
confusionmatricesshowsthatconfusionexistsbetweenCPUanomaliesandMEL
anomalies as well as AMA anomalies and MEL anomalies. Similar observations
TELESTO: A Graph Neural Network Model for Anomaly Classification 223
Table 4. The results of anomaly classification with TELESTO on all service nodes.
The table shows the achieved accuracy scores for each split and in average across all
splits.
Data set Split 1 Split 2 Split 3 Split 4 Split 5 ∅
Cassandra 0.796 0.804 0.894 0.822 0.939 0.851
Bono 0.865 0.853 0.825 0.566 0.729 0.768
Sprout 0.818 0.797 0.660 0.567 0.597 0.688
Backend 0.655 0.792 0.714 0.912 0.731 0.761
Chronos 0.806 0.741 0.984 0.898 0.620 0.810
Homer 0.214 0.529 0.725 0.846 0.677 0.598
Astaire 0.630 0.716 0.931 0.702 0.744 0.744
Load-balancer 0.801 0.880 0.989 0.767 0.680 0.823
Homestead 0.539 0.599 0.855 0.730 0.744 0.694
regarding diverse F1-scores can be made for all models between multiple splits.
One possible explanation might be the high variability in simulated user load
during our experiments. This resulted in a broad range of system states, from
almostidletoalmostoverutilized.Withtheoverallavailabletrainingdatabeing
limited,suchhighloadvariabilitymightleadtosignificantdifferencesinclassifi-
cation performances between splits. For TELESTO, we observed that although
the validation accuracy proportionally increases with the training accuracy, the
losses on both data sets structurally diverge after a few epochs already, leading
to an overfitting of the model which is intensified by the models complexity.
For completeness, we report the results of anomaly classification with
TELESTO on all other service nodes in Table4 while reporting only accuracy
scores to omit redundancy. It can be seen that the performance of TELESTO
varies across different service nodes and splits. While an average accuracy of
0.851 can be achieved on cassandra, the average accuracy on homer is 0.598.
The reported scores also strongly vary between splits on homer, between 0.214
average accuracy on split 1 and 0.846 on split 4. While not listed in Table4, the
comparative models exhibit high variance in accuracy over splits and remain on
average significantly inferior to TELESTO.
3.4 Limitations
Themainproblemencounteredduringourextensiveexperimentationisthecon-
tradiction between the expected amount of labeled anomaly data and required
data to train a reliable model. Labeling anomaly data by human experts is
costly, anomalies occur sparsely and IT environments undergo constant changes
so once labeled data deprecates over time. Therefore, the generalization ability
of our model represented by the prediction scores reveal a significant variance
in dependence of a specific split. We plan to investigate methods and heuris-
tics to generate additional training data from few anomaly examples and thus,
synthetically increase the available training data size.
224 D. Scheinert and A. Acker
Another aspect is the lack of expression regarding the temporal dependence
between consecutive graphs. Although sequential information of time series
within a graph are encoded via positional encoding, consecutive graphs con-
structed via a moving window over the time series are regarded as independent.
Possibilities to encode temporal information from preceding graphs for the clas-
sification of subsequent graphs is subject to future work.
4 Related Work
Concrete methods of identifying reoccurring anomalies within IT systems is
sparsely covered on public research. We formulate it as a time series classifi-
cation problem. Therefore, we analyse related work in both areas, IT system
anomaly classification and time series classification in general.
Bodik et al. [1] referring to the classification of different data center crises
as data center fingerprinting. Thereby, system resource metrics from all data
center components are aggregated via quantile discretization and feature selec-
tion methods are applied to choose relevant metrics for distinguishing different
datacentercrisistypes.Thereportedresultswereachievedbyanaggregationof
system resource metrics collected over 30min. Kaj´o and Nova´czki [12] provide
a comparison of different machine learning algorithms together with a genetic
algorithm approach. Given monitoring data from a System Architecture Evo-
lution (SAE) core network they select an optimized metric subset that is used
as input for different classification algorithms. The focus lies on metric selec-
tion and classification models are trained on 850 anomaly observations. Having
sparse occurrences of anomaly situations the expectation of many training data
is a major limit. Cheng et al. [2] applies a multi-scale long short-term memory
model to classify four different anomaly types based on update messages of the
bordergatewayprotocol(BGP).Theapproachexpectstheclassificationmodels
to be trained with several hours of anomaly data. However, anomaly situations
usually do not persist for an extended period of time in production systems.
A variety of time series classification exists. We focus on most recent pub-
lishedapproaches.InceptionTime[6]isanensembleofdeepCNNsfortimeseries
classification. Each CNN consist of multiple inception modules, whereas every
module utilizes bottleneck layers for regularization and applies both a sliding
max-poolingoperationandmultipleslidingfiltersofdifferentlengthsforfeature
extraction. In InceptionTime, multiple architecturally equivalent networks with
differentinitialweightvaluesareutilizedandtheirpredictionoutputsareevenly
weighted to obtain a final prediction result. Another approach which incorpo-
ratestheadvantagesofLSTMnetworksisnamedLSTM-FCN[13].Itconsistsof
twoparallelprocessingstreams.Inthefirststream,thetemporalstructureofthe
inputdataisexploitedbyanLSTMmodule.Thesecondstreamleveragesalter-
natingconvolutionlayers,batchnormalizationlayersandactivationlayersanda
final global average pooling (GAP) layer. In the end, the concatenation of both
stream outputs is used for classification. Another method named T-GCN is pre-
sented in [26]. The model combines graph convolutional networks together with
TELESTO: A Graph Neural Network Model for Anomaly Classification 225
GRU and thus aims at capturing the spatial and temporal dependencies simul-