frontend. We equip McSema with IDA-Pro [60], a commercial
decompiler to explore the full potential of McSema.
RetDec and mctoll. We also evaluate RetDec [68] and
mctoll [80]. RetDec is a reverse engineering toolchain
developed by Avast
that converts executable into LLVM
IR, and then decompiles the lifted IR into C code with
llvmir2hll. Its design focus includes supporting static
analysis and facilitates decompilation (e.g., by reconstructing
high-level C/C++ language features) [22]. For the rest of
this paper, RetDec refers to the lifter of this toolchain,
and llvmir2hll refers to its decompiler. mctoll is an
open-source project developed and maintained by Microsoft.
Although not explicitly documented [81], we ﬁnd that its
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:18 UTC from IEEE Xplore.  Restrictions apply. 
1104
code generation paradigm is similar with RetDec, indicating
mctoll’s decent support for analysis-related tasks.
We ﬁnd that RetDec and mctoll can generate LLVM
IR that is visually closer to compiled LLVM IR. We consider
these two lifters generate high-level IR (HIR), which is distinct
with EIR lifted by McSema. While IR lifted by both lifters
manifests similar visual representation, RetDec primarily
recovers local variables and types, whereas mctoll emulates
the computation of some CPU registers. This is likely due to
challenges of recovering certain local variables. Consequently,
mctoll-lifted IR code can impose higher challenge in static
(data ﬂow) analysis, as will be shown in Sec. VI-B.
BinRec. We also study a recently-released dynamic lifter
BinRec (EuroSys ’20 [18]). BinRec takes an executable
as its input, and employs S2E [35], a symbolic execution
engine,
to discover program inputs that can lead to new
execution paths. S2E runs executables within Qemu, and
for each logged execution trace, a LLVM IR trace will be
generated accordingly. S2E also provides RevGen [34] to lift
executables into LLVM IR code. RevGen uses IDA-Pro and
McSema as its front-end [10], which overlaps with our setup
of benchmarking McSema. Therefore, we do not pick RevGen
for evaluation. BinRec denotes the latest dynamic lifter in
this ﬁeld, and functional correctness is an explicit design
goal of BinRec (which is well demonstrated in our study of
functional correctness; see Sec. VI-E). Therefore, code reuse
becomes nicely feasible after recompiling lifted IR code into
standalone executable.
We clarify that decompilation (and other static analysis
tasks) are not explicit design goals of BinRec. Overall,
BinRec performs emulation-style lifting; it generates stan-
dalone IR programs by merging lifted IR traces, which, in
principle, eases whole-program rewriting and recompilation.
BinRec places all lifted IR code into a large LLVM IR
function named wrapper. This way, the original function
information and the call graph are deprecated in the output
of BinRec, which might cause confusion when conducting
downstream tasks. This may arise, for example, during the use
of binary code analyzers (e.g., BinDiff [2]), which implement
call graph-based algorithms or conduct function-level analysis.
Also, while dynamic lifters usually suffer from incomplete
code coverage, S2E has shown very good support to compre-
hensively discover program paths [33]. Our observation shows
that BinRec can achieve very good coverage for most C
programs evaluated in this research, even for highly complex
SPEC C programs.
We indeed spent considerable effort to explore other lifters.
For instance, bin2llvm [4] is seen to produced too many
pieces of broken IR code, and is no longer actively maintained.
We evaluated Rev.ng [95], which performs apparently worse
than other lifters and is therefore omitted. We also investi-
gated another recently released dynamic lifter, Instrew [48],
which yielded a large volume of broken outputs. Some other
popular frameworks can convert binary code into (customized)
low-level IR, e.g., angr [99] lifts assembly instructions into
VEX IR. Our study primarily focuses on binary lifters that
convert assembly programs into compiler IR; this way, rich
resources (e.g., analysis passes) provided by the compiler
framework could be smoothly reused in analyzing low-level
code without reinventing the wheel [19], [47], [18]. In short,
it might not be inaccurate to assume those four tools represent
the three best static and one best dynamic lifters that convert
binary code into LLVM IR by the time of writing this paper.
TABLE II
STATISTICS OF THE TEST PROGRAMS.
Total # of SVF test cases
Total # of alias facts
Total # of POJ-104 programs
Total # of SPEC INT 2006 C programs
84
169
44,912
9
CHANGES WE MADE TO BINARY LIFTERS AND RELEVANT TOOLS.
TABLE III
Tool
McSema
McSema
BinRec
mctoll
ncc
llvmir2hll
Changes
Remove compiler and linker inserted functions
Create McSema0 by disabling all optimizations
Several enhancements and bug ﬁxes
Support 20 new external function calls
Completely rewritten in PyTorch [89]
Add ten unsupported LLVM IR instructions
B. Downstream Task Setup and Test Case Selection
Table II summarizes the statistics of our test cases. We now
elaborate on the setup of each downstream task as follows.
Pointer Analysis. We use all 24 ﬂow-sensitive test cases
shipped by SVF. For each test program, some pointer pairs
are annotated with MustAlias, MayAlias, or NoAlias.
They are the ground truth of pointer analysis. We study
whether lifted and compiled IR code can support SVF to
generate consistent and correct pointer analysis results w.r.t.
the ground truth. Note that these 24 test cases, though encod-
ing different ﬂow-sensitive pointer analysis challenges, have
relatively simple control structures. To increase the difﬁculty,
we also perform equivalence modulo inputs (EMI)-based mu-
tation [69] to generate extra 60 programs with more complex
program structures. EMI-based mutation has been widely used
to mutate C code and test compilers [69], [104], [70]. We
manually conﬁrm and adjust alias predicates in case they were
changed by EMI mutations (e.g., after EMI mutation, a pair
of “must not alias” pointers could become “may alias”). 84
test programs contain in total 169 pointer alias facts to check.
Discriminability Analysis. We use the default setting in the
ncc paper [26] to split POJ-104 programs into training, vali-
dation, and testing sets. Each dataset will be either compiled
or lifted into LLVM IR programs. That is, we will create ﬁve
training, ﬁve validation, and ﬁve testing datasets. We then train
one ncc LLVM IR embedding model and one classiﬁcation
model of POJ-104 using a training/validation dataset. The
trained models are tested using the corresponding test dataset.
The model training is completed at 50 epoch. We use the
default model hyper parameter settings speciﬁed by ncc.
C Decompilation. We feed lifted and compiled IR code into
llvmir2hll, and measure decompilation quality. We collect
all C programs from the SPEC INT 2006 test suite (in total
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:18 UTC from IEEE Xplore.  Restrictions apply. 
1105
POINTER ANALYSIS EVALUATION RESULTS. SVFG DENOTES SPARSE VALUE FLOW GRAPH [102], WHOSE SIZE INDICATES THE complexity OF PROGRAM
STATIC ANALYSIS. WE REPORT THE AVERAGE DATA FOR POINTERS, OBJECTS, SVFG GRAPHS, AND PROCESSING TIME EVALUATION.
TABLE IV
Tool
Clang
RetDec
mctoll
McSema
McSema0
BinRec
MustAlias
Accuracy
100.0%
0.0%
0.0%
0.0%
0.0%
0.0%
MayAlias
Accuracy
100.0%
0.0%
0.0%
0.0%
0.0%
0.0%
NoAlias
Accuracy
100.0%
17.8%
0.0%
0.0%
0.0%
0.0%
#Pointers
#Objects
#SVFG Nodes
#SVFG Edges
345.9
73.0
934.8
3393.1
96646.2
879.4
68.6
14.2
52.9
141.2
2682.5
41.9
164.7
25.2
168.8
953.5
26686.1
166.7
116.4
8.8
102.0
817.6
20802.8
127.7
Processing Time
(CPU Seconds)
2.5
3.9
5.7
19.8
463.5
5.7
TOTAL TEST CASES AND FAILED LIFTING CASES.
TABLE V
Dataset
Total test cases
RetDec
mctoll
McSema
McSema0
BinRec
SVF
84
0
14
0
0
2
POJ-104
44,912
23,351
2
85
72
9,668
SPEC
9
0
9
0
0
0
nine programs) in this study. We exclude 3 C++ programs
in the SPEC INT 2006 dataset, since decompiling C++ code
is highly challenging, and is not supported by llvmir2hll
which only converts LLVM IR code into C code.
C. Changes Made on IR Lifters and Downstream Applications
Table III reports augmentations we made on the lifters
and downstream applications. Overall, we spent considerable
manual efforts, which are documented at [11] and released
at [1]. Particularly, while llvmir2hll is part of the RetDec
framework, it is mature enough to decompile IR generated by
both RetDec and clang. As for IR generated by other lifters,
a few statements are not supported, given that some lifters
seem to use LLVM IR statements that are not commonly-
picked by clang. We patched llvmir2hll to support all
encountered statements. With these patches, llvmir2hll
can decompile IR lifted by other lifters.
VI. FINDINGS
This section presents our ﬁndings when lifting executables
compiled with clang and no optimizations. We present cross-
platform, cross-compiler, and cross-optimization evaluation in
Sec. VII and Sec. VIII. We compare lifter-enabled solutions
with binary-only tools in Sec. IX.
A. Binary Lifting Results
We compile all test cases shown in Table II into 64-bit
x86 executables as the lifter inputs. We report the statistics
of lifting failures in Table V. When lifting SVF programs,
mctoll throws exceptions for seven cases, and also generates
seven lifted LLVM IR code that are broken. When processing
POJ-104 test cases, RetDec throws two exceptions. mctoll
cannot process C++ programs, leading to 23,351 failures to
lift POJ-104 programs, and also fails to lift all nine SPEC
programs (see our artifacts [1] for error logs).
Static lifting of SVF and POJ-104 programs can be ﬁnished
within a few seconds, whereas lifting SPEC programs can
take several minutes to a few hours. BinRec is slower than
others, as it uses symbolic execution to explore program paths.
However, as SVF test programs are small, there are only
two programs that cannot be processed by BinRec. When
performing symbolic execution of the POJ-104 programs, we
set the timeout as 10 minutes; 9,668 (21.6% of) programs
cannot be lifted within this threshold. The function coverage is
93% for successfully-lifted programs. We also emphasize that
these failures can be circumvented when conﬁguring S2E with
concrete inputs, or replacing with better symbolic execution
engines. This is not
the fault of BinRec. As for SPEC
programs, we conduct symbolic execution for each case for
50 hours, as these are much larger than other programs. We
obtain an average function coverage of 23%. Such coverage
should not impede the measured quality of decompiled C code,
as will be explained in Sec. VI-D. We also assess cross-
compiler, cross-optimization, and cross-architecture settings,
and the results of lifting executables under these settings are
given in Sec. VII and Sec. VIII, respectively.
B. Pointer Analysis
Table IV reports the evaluation results. As mentioned above,
SVF test cases are annotated with the pointer alias ground
truth. We count analysis results that are not equal to the ground
truth as failures of lifters. For results that are equal to the
ground truth, we manually inspect the details and conﬁrm
those are not false positives. Note that when lifters generate
IR code with broken annotations (see Manual Study below;
“annotations” are special function callsites using two pointers),
SVF might also report NoAlias; these are false positives.
While compiled IR supports fool-proof analysis, Table IV
shows that lifted IR has trivial support. No MayAlias or
MustAlias relations can be correctly recognized (see discus-
sion below). SVF also reports #pointers and #memory objects
identiﬁed from each program. Each pointer may point to null,
one, or several objects. We also report #nodes and #edges
on the sparse value ﬂow graph (SVFG) [102]. SVF performs
pointer analysis by ﬁrst constructing the SVFG, denoting the
data dependency among program variables. These four criteria
indicate the complexity of conducting static analysis (not
merely pointer analysis) since the more complex program data
dependency is, the higher the complexity that static analysis
could encounter. We also report average processing time of
SVF, which is consistent with the graph complexity.