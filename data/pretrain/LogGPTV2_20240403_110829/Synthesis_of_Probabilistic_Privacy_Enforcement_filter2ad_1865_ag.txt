(1) We determine the maximum possible size of an equiva-
lence relation enforcing Φ. We use binary search on the
interval [0, |O|]. To determine if an equivalence relation
of size k exists, we query the oracle with an instance
(I, O, δ, π , Φ, k, ∅). Let the maximum size be K.
(2) We find an equivalence relation of size K enforcing the
policy Φ. To do this, we initialize ξ ← ∅ and ˆξ ← O × O
and iterate the following procedure until ˆξ (cid:44) ∅:
(a) Pick an arbitrary element (o, o′) from ˆξ. Query the
oracle with (I, O, δ, π , Φ, K, ξ ∪ {(o, o′)})
(b) If the answer is Yes, then ξ ← ξ ∪ {(o, o′)}. If the
answer is No, do nothing.
ˆξ ← ˆξ \ {(o, o′)}
(c)
(d) Iterate until ˆξ (cid:44) ∅.
After the iteration ends, we just output ξ.
(cid:3)
Lemma A.2. The permissive privacy enforcement synthesis prob-
lem is NP-hard.
Proof. To prove the NP-hardness, we define a refined version
of the problem, then we show the refined problem is NP-hard by
reducing the partition problem to it. Finally, we reduce the refined
problem to the permissive privacy enforcement synthesis problem
and hence prove it to be NP-hard as well.
An instance of the refined problem is a set O, its subset S, a
distribution d over O (d ∈ D(O)) and a bound [a, b] where a, b ∈ Q
and 0 ≤ a ≤ b ≤ 1. The problem asks to find an equivalence relation

ξ over O such that for each class E ∈ ξ we have that:

o∈E∩S d(o)
o∈E d(o) ∈ [a, b]
Pr(O ∈ S | O ∈ E) =
The refined problem is trivially reducible to the permissive policy
enforcement problem. For an instance (O, S, d,[a, b]) of the refined
problem, we produce an instance of the permissive policy enforce-
ment problem:
(I = O, O = O, δ = d, π = IO , Φ = {φ = (S,[a, b])})
where IO ∈ MO×O is the identity matrix.
Let us reduce the partition problem, which is known to be NP-
hard [33], to the refined problem. The partition problem is defined
as follows. Let A = {a1, . . . , an} be a finite set of natural numbers.
The partition problem asks to decide whether there exists a subset
A′ ⊆ A such that
a = 

a∈A′
a
a∈A\A′
We now construct an instance of the refined problem: we choose
the set O = {o1, . . . , on, p, q}, and S = {p, q}. We choose the prior
distribution as:
i ai
And we choose the bound [0.5, 0.5].
With this instance, it is possible to find the desired subset A′ of
A if and only if there exists an equivalence class over outputs with
exactly 2 equivalence classes. When ξ = {O1, O2} is an equivalence
relation solving the refined problem, we have that p ∈ O1 ∧ q ∈ O2,
or p ∈ O2∧q ∈ O1. We can get the solution to the partition problem
as A′ = {ai | oi ∈ O1}.
(cid:3)
A.2 Completeness
Proof of Theorem 4.4. Let (S,[a, b]) ∈ Φ be a belief bound. We
show that Enf(π ′, ξ⊥) |= (S,[a, b]). We have that π , δ |= (S,[a, b]),
δ (I ∈ S | O = o) ∈ [a, b] for every o ∈ O.
which is by definition Pπ
The collection of events {O = o}o∈O is a partition of the sample
space. By the law of total probability, we have:
δ (I ∈ S | O = o) · Pπ
Pπ
δ (I ∈ S) =
δ (O = o)
Pπ
o∈O
d(p) = d(q) = 0.25
ai
d(oi) =
2 ·
where ai ∈ A
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA404Algorithm 3: The algorithm SynOpt(π , δ, Φ)
Input: A probabilistic program π, an attacker belief δ, and a
policy Φ = {(S,[a, b])}
Output: An equivalence relation ξ enforcing the policy.
(cid:3)
δ (I ∈ S | O = o) (cid:60) [a, b]}
1 begin
2
3
4
5
6
C ← {o ∈ O | Pπ
X ← O \ C
while Pπ
if Pπ
δ (I ∈ S | O ∈ C) (cid:60) [a, b] ∧ X (cid:44) ∅ do
δ (I ∈ S | O ∈ C)  b. Hence we need to ensure that:

δ (I ∈ S | O = o) · Pπ
δ (O = o)
which is equivalent to
δ (O = o)
∈ [a, b]
Now, Pπ
o∈C Pπ
(Pπ
δ (I ∈ S | O = o) − b) · Pπ
δ (O = o) ≤ 0
o∈C Pπ
≤ b
Hence, we can keep adding elements from O \ C that minimize
δ (O = o) until C satisfies
the expression (Pπ
the belief bound. In the case that there is no enforcement satisfying
the policy, we find out after joining all the elements together.
δ (I ∈ S | O = o) − b) · Pπ
o∈C
7
8
9
10
11
12
13
C ← C ∪ {o}
X ← X \ {o}
δ (I ∈ S | O ∈ C) (cid:60) [a, b] then
return unsat
if Pπ
return {C} ∪ {{o} | o ∈ X}
Running Time. The running time of Algorithm 3 is O(n log(n))
where n = |O|. This is because the elements of O can be sorted by
the value (Pπ
δ (O = o), and then picked one
by one in an increasing order.
Correctness. We now prove the following theorem.
δ (I ∈ S | O = o) − b) · Pπ
Theorem A.3. Let π be a probabilistic program, δ an attacker be-
lief, and Φ = (S,[a, b]) a singleton privacy policy. If there is no equiv-
alence relation ξ such that Enf(π , ξ), δ |= Φ, then SynOpt(π , δ, Φ) =
unsat. Otherwise, we have SynOpt(π , δ, Φ) = ξ , Enf(π , ξ), δ |= Φ
and furthermore, we have that ξ has the greatest number of singleton
classes of all enforcements enforcing the policy Φ on π.
Proof of Theorem A.3. When no enforcement exists, the loop
on Line 4 will iterate until all the outputs are conflated. Then, the
condition on Line 11 will evaluate to true and an unsat result will
be returned.
For the case that an enforcement exists, let ξ∗ be an optimal
enforcement for the program π, belief δ and policy Φ. Without a
loss of generality we can assume that ξ∗ has only singleton classes
with the exception of a single class C∗ where all the other outputs
are located. Now, if SynOpt would give an enforcement with less
singletons than ξ∗ has, it would be a contradiction, since it must
(Pπ
δ (I ∈ S | O = o) − b) · Pπ
δ (O = o) ≤ 0
and SynOpt initializes C to be the subset of C∗ that violates the
policy and then picks the outputs to conjoin to C exactly by (Pπ
δ (I ∈
S | O = o) − b) · Pπ
(cid:3)
Since the running time of Algorithm 3 is polynomial, by The-
orem A.3, we conclude that the synthesis problem for singleton
policies is in PTIME for answer precision optimality. This immedi-
ately proves Theorem 4.3.
δ (O = o).
hold that 
o∈C∗
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA405B PSI SYNTAX
We present the syntax of the Psi solver language in BNF:
x ∈ Vars a ∈ ArrayVars bop ∈ {+, −, ∗, /, ==, (cid:44), <, ≤}
Expr ::= Q | x | a[Expr] | Expr bop Expr | flip(Expr)
Stmt ::= x := Expr | a := array(Expr) | x = Expr | a[Expr] = Expr
| skip | observe Expr | if Expr {Stmt} else {Stmt}
| for x in [Expr..Expr) {Stmt} | Stmt; Stmt
C EXPERIMENT DETAILS
We generated synthesis instances from three scenarios.
C.1 Genomic Data
The genomic data scenario is identical to the one described in
Section 2. We generated synthesis instances with different number
of patients and privacy policies. We denote the instance with n
patients and m privacy policies by (n, m).
Attacker Belief. For an instance (n, m), the set of inputs is I =
{A, G}2n. The relationships among patients form a complete binary
tree with n nodes. The belief δ ∈ D(I) is defined as described in
Figure 2(b).
Policy. For an instance (n, m), we generate a privacy policy with
m belief bounds of the form (I[i] = AA,[0.1, 0.9]), where I[i] returns
the pair of nucleotides of patient i according to the topological
order.
Programs. We consider four programs:
sum Returns the number of adenine nucleotides among the
patients.
noisy-sum Returns the number of adenine nucleotides among the
patients with noise, which adds 1 with probability 0.5 and
subtracts 1 with probability 0.5.
prevalence Returns the number of patients who have two adenine
nucleotides.
read Returns the pair of nucleotides of a patient.
C.2 Social
We model a social network where users express whether they favor
the Democratic or the Republican party. As before, we denote by
(n, m) a instance with n users and m policies.
Attacker Belief. For an instance (n, m), the set of inputs is I =
{D, R}n. We add a friendship between two users randomly with
a probability 0.5. The attacker belief is defined by a probabilistic
program that assigns a Bernoulli distribution to the affiliation to
each user i with the statement affi[i] := flip(0.5), and then for
any friendship between users i and j we add an observe statement
1
to correlate their affiliations.
Policy. For an instance (n, m) we generate a privacy policy with m
belief bounds of the form (affi[i] == R,[0.1, 0.9]), where i ranges
over m randomly selected users. A belief bound for a user imposes a
limit on how much the attacker can learn about the user’s affiliation.
Programs. We consider three programs:
if (flip(0.5)) { observe(affi[i] == affi[j]); }
1
2
3
4
5
6
7
8
9
10
11
12
13
14
1
2
3
4
5
6
7
1
1
2
3
4
5
6
7
8