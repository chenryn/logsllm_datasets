from the FTP server, the administrator is able to attribute
the theft to a speciﬁc set of user credentials. Note that while
existing execution-partitioning systems such as ProTracer [42]
and BEEP [39] could eliminate dependency explosion in this
that
13
FTPsensitiveFTP sensitive(a) -FTP session opened-“USER wlog" 331-PASS (hidden)" 230 -USER wlog: Login successful- FTP session closed(b)Fig. 9: Phishing email attack scenario. (a) Attack provenance graph generated by traditional solutions. (b) Semantic-aware and execution-
partitioned provenance graph generated by OmegaLog.
evaluation (§IX) demonstrated that we are able to identify
log statements in all the proﬁled applications based on our
heuristics for event-logging extraction.
OmegaLog assumes at least one log message printed in
the event-handling loop to partition execution. OmegaLog
uses ordered log messages in the universal provenance logs
as a way to partition syscalls and make unit boundaries.
Such an assumption only works for the applications that
use synchronous I/O programming model. For instance, if an
application is using asynchronous I/O and only prints one log
message at the end of the event-handling loop then concurrent
requests will generate multiple syscalls without immediately
printing log message at the end of each request. In such case,
OmegaLog will not be able to correctly partition each request.
One approach to solve this problem is to generate complete
syscall mapping along with LMS paths model inside the event-
handling loop during ofﬂine analysis and use this mapping to
divide execution. We leave that as our future work.
Malware binaries may not produce any of the application
logs that are required for execution partitioning. In that case,
OmegaLog treats the whole malware execution as one unit
and does not provide execution partitioning. That is acceptable
since every output and input event from malware execution is
important in forensic analysis.
XI. RELATED WORK
In §II, we described several shortcomings of existing
provenance-tracking systems that OmegaLog addresses. Here
we provide additional discussion of related work.
Application Log Analysis. Application logs contain a wealth
of information that can be useful in aiding software system
maintenance, and thus become an important data source for
postmortem analysis [48], anomaly detection [24], [59], [60],
program veriﬁcation [52], and security monitoring [46]. Ex-
isting guidelines and practices
[66], [34], [25], [21], [49]
indicate the importance of well-designed log messages in
failure diagnosis. Xu et al. [59] analyzed console logs to learn
common patterns by using machine learning and to detect ab-
normal log patterns at runtime. SherLog [61] used application
source code and runtime error log to infer what must or may
have happened during a failed run and provide detailed post
mortem analysis of the error. Similarly, LogEnhancer [62] and
LogAdvisor [66] automatically improves existing log messages
and provides suggestions on where to log in the code in order
to aid in future post-failure debugging. HERCULE [50] uses
expert-written log parsers and rules to ﬁrst extract log ﬁelds
such as IP addresses and then correlate log entries across
application logs based on these ﬁelds. Unlike OmegaLog,
HERCULE’s rule-based approach does not accurately capture
causality across applications that use th whole-system layer
and that can ultimately undermine forensic investigations.
Several log analysis systems [56], [63], [44] have been
proposed to reconstruct behaviour of applications running on
Android OS. Unlike OmegaLog, these existing systems are not
transparent as they either require code instrumentation or an
emulator to collect logs for analysis. DroidHolmes [44] and
CopperDroid [56] are single-layer log analysis systems while
OmegaLog is a multi-layer log analysis system. DroidForen-
sic [63] collects logs from different layers for forensic analysis;
however, in its case, the onus is on the user to correlate
and combine logs from different layers. On the other hand,
OmegaLog integrates logs from different layers without user-
involvement using program-analysis techniques.
Application Log Parsing.
Automated log parsing allows
developers and support engineers to extract structured data
from unstructured log messages for subsequent analysis. Many
open source tools, such as Logstash [7] and Rsyslog [8]
and commercial tools such as, VMWare LogInsight [10] and
Splunk [9] provide built-in log-parsing modules/recipes for
popular applications such as MySQL and Apache httpd; that
allows users to automatically extract useful information, such
as PID, hostname, and ﬁlenames from log messages. For
custom parsing of log messages, those tools provide easy-to-
use, regex-based languages to deﬁne parsers.
Distributed System Tracing.
End-to-end tracing is required
in distributed systems to enable comprehensive proﬁling. Ex-
isting tools, such as Dtrace [18], Dapper [54], X-trace [23],
MagPie [15], Fay [22], and PivotTracing [43] instrument the
underlying application to log key metrics at run time. On
the other hand,
lprof [65] and Stitch [64] allow users to
proﬁle a single request without instrumenting any distributed
14
PostﬁxTransmissiont2t1malware         PostﬁxPostﬁxPostﬁxPostﬁxPostﬁxPostﬁxPostﬁx              procmailprocmailprocmailprocmailprocmailprocmailprocmailMuttmalware.torrentf2.torrentf1.torrent   PostﬁxprocmailPostﬁx  Malware.torrentTransmissionmalwarepostﬁx/qmgr[6854]: C9A34520973: from=, size=748, nrcpt=1 (queue active)Nov 28 11:17:01 localhost postﬁx/local[18162]: C9A34520973: to=, orig_to=, relay=local, delay=0.02, delays=0.01/0.01/0/0, dsn=2.0.0, status=sent (delivered to mailbox)malware Queued for veriﬁcation (verify.c:264)malware Verifying torrent (verify.c:219)…malware Piece 733, which was just downloaded, failed its checksum test (torrent.c:3259)…malware State changed from "Incomplete" to "Complete" (torrent.c:2161)malware Announcing to tracker (announcer.c:1552)Mutt  5 MAIL FROM: 5 RCPT TO: 5 DATA[5< 354  Go ahead b133sm5372063ioe.73 - gsmtp5< 250 2.0.0 OK 1509951165 b133sm5372063ioe.73 - gsmtp(a)(b)application. lprof uses static analysis to ﬁnd identiﬁers that can
distinguish output logs of different requests. However, lprof
only correlate logs from the same distributed application. On
the other hand, Stitch requires certain identiﬁers in the log
messages in order to correlate log messages across different
distributed applications. Finally, both systems capture mere
correlations instead of true causality between application logs
and that can reduce the accuracy of attack reconstruction.
XII. CONCLUSION
In this work, we introduce OmegaLog an end-to-end
provenance-tracking system that uses the notion of univer-
sal provenance to solve the semantic gap and dependency
explosion problem that currently exist in causality analysis
frameworks. Universal provenance combines whole-system
audit logs and application event logs while preserving the
correctness of causality analysis. OmegaLog leverages static
binary analysis to parse and interpret the application event
logs and generates semantic-aware and execution-partitioned
provenance graphs. We implemented our prototype using the
Angr binary analysis framework and the Linux Audit Module.
Evaluation on real-world attack scenarios shows that Omega-
Log’s generated graphs are concise and rich with semantic
information, compared to the state-of-the-art.
ACKNOWLEDGMENT
We thank our shepherd, Yonghwi Kwon, and the anony-
mous reviewers for their comments and suggestions. We also
thank Akul Goyal and Riccardo Paccagnella for feedback on
early drafts of this paper. Wajih Ul Hassan was partially
supported by the Sohaib & Sara Abbasi Fellowship and the
Symantec Graduate Fellowship. This work was supported
in part by the National Science Foundation under contracts
CNS-16-57534 and CNS-17-50024. Any opinions, ﬁndings,
conclusions, or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of their employers or the sponsors.
REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
tracing,”
https://docs.microsoft.com/en-us/windows/desktop/
“Apache HTTP server benchmarking tool,” https://httpd.apache.org/
docs/2.4/programs/ab.html.
“Benchmark for ftp servers,” https://pypi.python.org/pypi/ftpbench.
“DTrace,” https://www.freebsd.org/doc/handbook/dtrace.html.
“Event
ETW/event-tracing-portal.
“The Linux audit daemon,” https://linux.die.net/man/8/auditd.
“Log4c : Logging for C library,” http://log4c.sourceforge.net/.
“Logstash: Collect, Parse, Transform Logs,” https://www.elastic.co/
products/logstash.
“Rsyslogd,” http://man7.org/linux/man-pages/man8/rsyslogd.8.html.
“Splunk Log Management,” https://www.splunk.com/en us/central-log-
management.html.
“VMware
products/vcenter-log-insight.
“Speed considerations,” https://github.com/angr/angr-doc/blob/master/
docs/speed.md.
“Equifax says
the
cyberattack.html.
“Inside the cyberattack that shocked the US government,” ttps://www.
wired.com/2016/10/inside-cyberattack-shocked-us-government/.
cyberattack may have
affected 143 million in
https://www.nytimes.com/2017/09/07/business/equifax-
http://www.vmware.com/ca/en/
vCenter Log
Insight,”
U.S.”
[14]
“Target Missed Warnings in Epic Hack of Credit Card Data,” https:
//bloom.bg/2KjElxM.
[15] P. Barham, A. Donnelly, R. Isaacs, and R. Mortier, “Using magpie for
request extraction and workload modelling.” in OSDI, 2004.
[16] A. Bates, W. U. Hassan, K. Butler, A. Dobra, B. Reaves, P. Cable,
T. Moyer, and N. Schear, “Transparent web service auditing via network
provenance functions,” in WWW, 2017.
[17] A. Bates, D. Tian, K. R. B. Butler, and T. Moyer, “Trustworthy whole-
system provenance for the Linux kernel,” in USENIX Security, 2015.
[18] B. Cantrill, M. W. Shapiro, A. H. Leventhal et al., “Dynamic instru-
mentation of production systems.” in USENIX ATC, 2004.
[19] A. Costin, J. Zaddach, A. Francillon, and D. Balzarotti, “A large-scale
analysis of the security of embedded ﬁrmwares,” in USENIX Security,
2014.
[20] S. A. Crosby and D. S. Wallach, “Efﬁcient data structures for tamper-
evident logging.” in USENIX Security Symposium, 2009.
[21] R. Ding, H. Zhou, J.-G. Lou, H. Zhang, Q. Lin, Q. Fu, D. Zhang,
and T. Xie, “Log2: A cost-aware logging mechanism for performance
diagnosis,” in USENIX ATC, 2015.
[22] U. Erlingsson, M. Peinado, S. Peter, M. Budiu, and G. Mainar-Ruiz,
“Fay: Extensible distributed tracing from kernels to clusters,” ACM
Trans. Comput. Syst., vol. 30, 2012.
[23] R. Fonseca, G. Porter, R. H. Katz, S. Shenker, and I. Stoica, “X-trace:
A pervasive network tracing framework,” in NSDI. USENIX, 2007.
[24] Q. Fu, J.-G. Lou, Y. Wang, and J. Li, “Execution anomaly detection in
distributed systems through unstructured log analysis,” in ICDM. IEEE,
2009.
[25] Q. Fu, J. Zhu, W. Hu, J.-G. Lou, R. Ding, Q. Lin, D. Zhang, and T. Xie,
“Where do developers log? an empirical study on logging practices in
industry,” in ICSE Companion. ACM, 2014.
[26] A. Gehani and D. Tariq, “SPADE: Support for provenance auditing in
distributed environments,” in Middleware, 2012.
[27] R. Gerhards, “The syslog protocol,” Internet Requests for Comments,
RFC 5424, 2009.
[28] E. Gessiou, V. Pappas, E. Athanasopoulos, A. D. Keromytis, and
S. Ioannidis, “Towards a universal data provenance framework using dy-
namic instrumentation,” in Information Security and Privacy Research,
D. Gritzalis, S. Furnell, and M. Theoharidou, Eds.
Springer Berlin
Heidelberg, 2012.
[29] W. U. Hassan, S. Guo, D. Li, Z. Chen, K. Jee, Z. Li, and A. Bates,
“NoDoze: Combatting threat alert fatigue with automated provenance
triage,” in NDSS, 2019.
[30] W. U. Hassan, M. Lemay, N. Aguse, A. Bates, and T. Moyer, “Towards
scalable cluster auditing through grammatical inference over provenance
graphs,” in NDSS, 2018.
[31] M. N. Hossain, S. M. Milajerdi, J. Wang, B. Eshete, R. Gjomemo,
R. Sekar, S. D. Stoller, and V. Venkatakrishnan, “SLEUTH: Real-
time attack scenario reconstruction from COTS audit data,” in USENIX
Security, 2017.
[32] Y. Ji, S. Lee, E. Downing, W. Wang, M. Fazzini, T. Kim, A. Orso, and
W. Lee, “Rain: Reﬁnable attack investigation with on-demand inter-
process information ﬂow tracking,” in CCS. ACM, 2017.
[33] B. W. Kernighan and D. M. Ritchie, The C Programming Language.
Upper Saddle River, NJ, USA: Prentice-Hall, Inc., 1978.
[34] B. W. Kernighan and R. Pike, The practice of programming. Addison-
Wesley Professional, 1999.
J. C. King, “Symbolic execution and program testing,” Commun. ACM,
vol. 19, no. 7, 1976.
[35]
[36] S. T. King and P. M. Chen, “Backtracking intrusions,” in SOSP. ACM,
2003.
[37] S. T. King, Z. M. Mao, D. G. Lucchetti, and P. M. Chen, “Enriching
intrusion alerts through multi-host causality.” in NDSS, 2005.
[38] Y. Kwon, F. Wang, W. Wang, K. H. Lee, W.-C. Lee, S. Ma, X. Zhang,
D. Xu, S. Jha, G. Ciocarlie et al., “MCI: Modeling-based causality
inference in audit logging for attack investigation,” in NDSS, 2018.
[39] K. H. Lee, X. Zhang, and D. Xu, “High accuracy attack provenance
via binary-based execution partition,” in NDSS, 2013.
15
[40] S. Ma, K. H. Lee, C. H. Kim, J. Rhee, X. Zhang, and D. Xu, “Accurate,
low cost and instrumentation-free security audit logging for Windows,”
in ACSAC. ACM, 2015.
[41] S. Ma, J. Zhai, F. Wang, K. H. Lee, X. Zhang, and D. Xu, “MPI:
Multiple perspective attack investigation with semantic aware execution
partitioning,” in USENIX Security, 2017.
[42] S. Ma, X. Zhang, and D. Xu, “Protracer: Towards practical provenance
tracing by alternating between logging and tainting,” in NDSS, 2016.
J. Mace, R. Roelke, and R. Fonseca, “Pivot tracing: Dynamic causal
monitoring for distributed systems,” in SOSP. ACM, 2015.
[43]
[44] Z. Meng, Y. Xiong, W. Huang, F. Miao, T. Jung, and J. Huang,
“Divide and conquer: recovering contextual information of behaviors in
android apps around limited-quantity audit logs,” in ICSE: Companion
Proceedings.
IEEE Press, 2019.
[45] S. M. Milajerdi, R. Gjomemo, B. Eshete, R. Sekar, and V. Venkatakr-
ishnan, “HOLMES: Real-time APT detection through correlation of
suspicious information ﬂows,” in Symposium on Security and Privacy.
IEEE, 2019.
[46] M. Montanari, J. H. Huh, D. Dagit, R. B. Bobba, and R. H. Campbell,
“Evidence of log integrity in policy-based security monitoring,” in DSN.
IEEE, 2012.
[47] K.-K. Muniswamy-Reddy, U. Braun, D. A. Holland, P. Macko,
D. Maclean, D. Margo, M. Seltzer, and R. Smogor, “Layering in
provenance systems,” in ATC, 2009.
[48] A. Oliner, A. Ganapathi, and W. Xu, “Advances and challenges in log
analysis,” Communications of the ACM, vol. 55, no. 2, 2012.
[49] A. Pecchia, M. Cinque, G. Carrozza, and D. Cotroneo, “Industry prac-
tices and event logging: Assessment of a critical software development
process,” in ICSE.
IEEE, 2015.
[50] K. Pei, Z. Gu, B. Saltaformaggio, S. Ma, F. Wang, Z. Zhang, L. Si,
X. Zhang, and D. Xu, “HERCULE: Attack story reconstruction via
community discovery on correlated log graph,” in ACSAC.
ACM,
2016.
[51] D. Pohly, S. McLaughlin, P. McDaniel, and K. Butler, “Hi-Fi: Collecting
high-ﬁdelity whole-system provenance,” in ACSAC, 2012.
[52] W. Shang, Z. M. Jiang, H. Hemmati, B. Adams, A. E. Hassan, and
P. Martin, “Assisting developers of big data analytics applications when
deploying on hadoop clouds,” in ICSE.
IEEE Press, 2013.
[53] Y. Shoshitaishvili, R. Wang, C. Salls, N. Stephens, M. Polino,
A. Dutcher, J. Grosen, S. Feng, C. Hauser, C. Kruegel, and G. Vigna,
“SoK: (State of) The Art of War: Offensive techniques in binary
analysis,” in IEEE Symposium on Security and Privacy, 2016.
[54] B. H. Sigelman, L. A. Barroso, M. Burrows, P. Stephenson, M. Plakal,
D. Beaver, S. Jaspan, and C. Shanbhag, “Dapper, a large-scale dis-
tributed systems tracing infrastructure,” Google, Inc, Tech. Rep., 2010.
[55] M. Stamatogiannakis, P. Groth, and H. Bos, “Looking inside the black-
box: Capturing data provenance using dynamic instrumentation,” in
IPAW. Springer-Verlag New York, Inc., 2015.
[56] K. Tam, S. J. Khan, A. Fattori, and L. Cavallaro, “CopperDroid:
Automatic reconstruction of android malware behaviors.” in NDSS,
2015.
[57] D. Tariq, M. Ali, and A. Gehani, “Towards automated collection of
application-level data provenance,” in TaPP. USENIX, 2012.
[58] Q. Wang, W. U. Hassan, A. Bates, and C. Gunter, “Fear and logging
in the internet of things,” in NDSS, 2018.
[59] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting
large-scale system problems by mining console logs,” in SOSP. ACM,
2009.
[60] X. Yu, P. Joshi, J. Xu, G. Jin, H. Zhang, and G. Jiang, “Cloudseer:
Workﬂow monitoring of cloud infrastructures via interleaved logs,”
ACM SIGOPS Operating Systems Review, vol. 50, no. 2, 2016.
[61] D. Yuan, H. Mai, W. Xiong, L. Tan, Y. Zhou, and S. Pasupathy,
“SherLog: Error diagnosis by connecting clues from run-time logs,”
in ASPLOS. ACM, 2010.
[62] D. Yuan, J. Zheng, S. Park, Y. Zhou, and S. Savage, “Improving
software diagnosability via log enhancement,” ACM TOCS, vol. 30,
no. 1, 2012.
[63] X. Yuan, O. Setayeshfar, H. Yan, P. Panage, X. Wei, and K. H. Lee,
“Droidforensics: Accurate reconstruction of android attacks via multi-
layer forensic logging,” in AsiaCCS. ACM, 2017.
[64] X. Zhao, K. Rodrigues, Y. Luo, D. Yuan, and M. Stumm, “Non-intrusive
performance proﬁling for entire software stacks based on the ﬂow
reconstruction principle.” in OSDI, 2016.
[65] X. Zhao, Y. Zhang, D. Lion, M. F. Ullah, Y. Luo, D. Yuan, and
M. Stumm, “lprof: A non-intrusive request ﬂow proﬁler for distributed
systems,” in OSDI, 2014.
J. Zhu, P. He, Q. Fu, H. Zhang, M. R. Lyu, and D. Zhang, “Learning to
Log: Helping developers make informed logging decisions,” in ICSE,
2015.
[66]
16