For instance, the Dis and Exe for Alg and Arc of Fig-
ure 2 are given by the two tables 1 and 2. Here it takes
more time to communicate the data-dependency I (cid:1) A than
A (cid:1) B simply because there are more data to transmit. The
point-to-point links {L1.2} and {L1.3, L2.3} are hetero-
geneous. This table only gives the transmission times for
inter-processor communications. For an intra-processor
communication, the time is always 0.
1
0.5
0.5
1
0.5
0.5
1
0.5
0.5
1.5
1
1
1
0.5
0.5
1
0.5
0.5
1.9
1.4
1.4
data-dependency
I (cid:1) A
1.75
1.25
1.25
data-dependency
F (cid:1) G
A (cid:1) B A (cid:1) C A (cid:1) D A (cid:1) E B (cid:1) F
C (cid:1) F D (cid:1) G E (cid:1) G
1.3
1.3
0.8
0.8
0.8
0.8
time
link
L1.2
L2.3
L1.3
time
link
L1.2
L2.3
L1.3
Table 2. Execution times Exe for communica-
tions
Finally, the real-time constraints Rtc are also given in
time units. They can be, for instance, a deadline for the
completion date of the whole schedule. For our exam-
ple, we will take Rtc = 16, which means that the obtained
static fault-tolerant distributed schedule must complete in
less than 16 time units.
G (cid:1) O
1.1
0.6
0.6
4. The Proposed Solution
In this Section we discuss some of the basic principles
used in the proposed approach, followed by a description of
our algorithm. The algorithm we propose is a list schedul-
ing heuristic based active replication strategy [6], that al-
lows at least Npf +1 replicas of an operation to be scheduled
on different processors, which are run in parallel to tolerate
at most Npf processors failures.
4.1. Algorithm Principle
The proposed solution uses the software redundancy of
both comps/mems/extios and of comms. Each opera-
tion X of the algorithm graph is replicated on Rep differ-
ent processors of the architecture graph, where Rep ≥
Npf + 1. Each of these Rep replicas send their results
in parallel to all the replicas of all the successor operations
in the data-ﬂow graph. Therefore, each operation will re-
ceive its set of inputs Rep times; as soon as it receives the
ﬁrst set, the operation is executed and ignores the later in-
puts. However, in some cases, the replica of an operation
will only receive some of its inputs once, through an intra-
processor communication. For the sake of simplicity, sup-
pose we have an operation X with only one input produced
by its predecessor Y (see Figure 3(a)).
Consider the replica of X which is assigned to proces-
sor P. Two cases can arise: either one replica of Y is also
scheduled on P, or all the replicas of Y are assigned to pro-
cessors distinct from P. In the ﬁrst case, the comm from Y to
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
X will not be replicated and will be implemented as a single
intra-processor communication (see Figure 3(b)). Indeed,
the replicas of this comm would only be used if P failed, but
in this case the replica of X assigned to P would not need
this input. In the second case, the comm from Y to X will
be replicated Npf +1 times, each implemented as an inter-
processor communication (see Figure 3(c)).
Y
X
(a)
one intra−processor
communication
P
Y
X
(b)
P’
L’
P
L
P’’
two (or more) inter−processor communications
Y
Y
X
(c)
Figure 3. (a) Algorithm sub-graph; (b) At least
one replica of Y is on P; (c) No replica of Y is
on P.
Figure 3 illustrates this example by showing the partial
schedules obtained for the X and Y subgraph. In these di-
agrams, an operation is represented by a white box, whose
height is proportional to its execution time. A comm is rep-
resented by a gray box, whose height is proportional to its
communication time, and whose ends are bound by two ar-
rows: one from the source operation and one to the destina-
tion operation.
L’
P’
Y
L
P’’
Y
P
Y
X
Figure 4. Schedule more than Npf replicas of
an operation.
Since the communication cost between operations as-
signed to the same processor is considered to be negligible,
replicating an operation more than Npf +1 times reduces
the global interprocessor communication overheads of the
schedule. Consider the schedule of Figure 3(c):
if Y is
replicated on P, the schedule length can be reduced, both
in the presence and in the absence of failures, as shown in
Figure 4.
4.2. Scheduling Heuristic
The heuristic implementing this solution is a greedy list
scheduling [22], called the Fault-Tolerance Based Active
Replication strategy (FTBAR) algorithm. We present the
scheduling algorithm in macrosteps, the superscript num-
ber in parentheses refers to the step of the heuristic, e.g.,
O(n)
sched.
Before describing the heuristic, we deﬁne the following
notations which are used in the rest of this paper:
• O(n)
cand: The list of candidate operations, this list is
built from the algorithm graph vertices. An operation
is said to be a candidate if all its predecessors are al-
ready scheduled.
sched: The list of scheduled operations.
• O(0)
• pred(oi): The set of predecessors of operation oi.
• succ(oi): The set of successors of operation oi.
• R(n): The critical path length.
• E(n)
exc(oi, pj): The end execution time of operation oi
scheduled on processor pj.
• E(n)
com(oi, oj): The end of data communication time
• S
• S(n)
from operation oi to operation oj.
(n)(oi) is the latest start time from end of oi.
best(oi, pl): The earliest time at which operation oi
can start execution on processor pl. It is computed as
(cid:2)
follows:
S(n)
j , oi)
best(oi, pl) = max
E(n)
com(ok
(cid:1)
N pf+1
min
k=1
oj∈pred(oi)
com(ok
exc(oj , pl).
j , oi) = E(n)
j is the kth replica of oj.
where ok
If oi and oj are scheduled in the same processor pl then
E(n)
• S(n)
worst(oi, pl): The earliest time at which operation oi
can start execution on processor pl, taking into account
(cid:2)
all the predecessors replicas. It is computed as follows:
S(n)
j , oi)
worst(oi, pl) = max
com(ok
E(n)
(cid:1)
N pf+1max
k=1
oj∈pred(oi)
j is the kth replica of oj.
where ok
If oi and oj are scheduled in the same processor pl then
E(n)
j , oi) = E(n)
exc(oj , pl).
com(ok
The schedule pressure [21] is used as a cost function to
select the best operation/processor pair. The schedule pres-
sure noted by σ(n)(oi, pj) tries to minimise the length of the
critical path of the algorithm and to exploit the scheduling
margin of each operation. It is computed for each proces-
sor pj ∈ P (P is the processor’s set) and each operation
oi ∈ O(n)
cand by using two functions:
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
1. The schedule-ﬂexibility SF is deﬁned as:
SF (n)(oi, pj) = R(n) − S(n)
worst(oi, pj) − S
(n)(oi)
2. The schedule-penalty SP is deﬁned as:
SP (n)(oi, pj) = R(n) − R(n−1)
With these two functions, the schedule pressure σ is
computed as follows:
σ(n)(oi, pj) = SP (n)(oi, pj) − SF (n)(oi, pj)
= S(n)
worst(oi, pj) + S
(n)(oi) − R(n−1)
The schedule pressure measures how much the schedul-
ing of the operation lengthens the critical path of the algo-
rithm. Therefore it introduces a priority between the op-
erations to be scheduled. Note that, since all candidates
operations at step n have the same value R(n−1), it is not
necessary to compute R(n−1).
The FTBAR fault-tolerance scheduling heuristic is for-
mally described below:
The FTBAR Algorithm:
begin
Initialise the lists of candidate and scheduled operations:
:= {o ∈ O | pred(o) = ∅};
O(0)
:= ∅;
O(0)
while O(n)
(cid:4)= ∅ do
sched
cand
cand
➀ Compute the schedule pressure for each operation oi of
worst, and keep the ﬁrst
O(n)
cand on each processor pj using S(n)
Npf +1 min results for each operation:
∀oi ∈ O(n)
∪l=Npf +1
(oi, pil) := minNpf +1
pj∈P
l=1
,
cand
σ(n)
best
σ(n)(oi, pj);
➁ Select the best candidate operation o such that:
σ(n)
urgent
(o) := max
oi∈O
(n)
impl
∪l=Npf +1
l=1
σ(n)
best
(oi, pil);
➂ Apply M inimize start time for the best candidate opera-
tion o on the ﬁrst Npf +1 processors computed at ➀;
➃ Update the lists of candidate and scheduled operations:
sched
:= O(n−1)
:= O(n)
cand
∪ {o};
− {o} ∪ Succ{o}; with:
O(n)
sched
O(n+1)
Succ{o} = {o(cid:2) ∈ succ(o) | pred(o(cid:2)) ⊆ O(n)
cand
end while
};
sched
end
Initially, O(0)
sched is empty and O(0)
cand is the list of opera-
tions without any predecessors. At the n-th step (n ≥ 1),
the list of already scheduled operations O(n)
sched is kept.
Also, the list of candidate operations O(n)
cand is built from
the algorithm graph vertices.
At each step n, one operation of the list O(n)
cand is se-
lected to be scheduled. To select an operation, we select at
the micro-step ➀, for each operation oi, the Npf +1 proces-
sors having the minimum schedule pressure. Then among
those best pairs (cid:1)oi, pj(cid:2), we select at the micro-step ➁ the
one having the maximum schedule pressure, i.e., the most
urgent pair.
The selected operation is implemented at the micro-
step ➂ on the Npf +1 processors computed at micro-step ➀,
and the comms implied by this implementation are also im-
plemented. At this micro-step the start time of the selected
operation o is reduced by replicating its predecessors using
a procedure Minimise start time proposed by Ahmad and
al. in [1], which is formally described below:
Minimise start time(o,p):
begin
➊ Determine earliest start time S(n)
➋ if S(n)
worst
worst
(o, p) is undeﬁned then quit because o cannot be
(o, p);
scheduled on p;
➌ Find out the Latest Immediate Predecessor (LIP) of o;
➍ Minimize the start time of this LIP by recursively calling
Minimize start time(LIP,p);
➎ Compute the new S(n)
➏ if ( new S(n)
➐ then
worst
worst
(o, p) ≥ S(n)
(o, p);
worst
(o, p) )
• Undo all the replications just performed in ➍;
• Schedule o to p at S(n)
• The comms implied by (b) are also implemented here
such that, each replica of o receives data from each
replica of these predecessors oj through parallel links;
(o, p);
best
➑ else Find out the new LIP of o and repeat from ➍;
end
For each pair (cid:1)predecessor, operation replica(cid:2), comms
are added in parallel links if and only if all the replicas of
the predecessor are on different processors. If this is not
the case, i.e., if there exists a replica of the predecessor on
the same processor, no comm is added (see Section 4.1 and
Figure 3).
When a comm is generated, it is assigned to the set of
communication units bound to the communication medium