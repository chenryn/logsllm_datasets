ECHO throughput is interesting for two reasons. First, it pro-
vides an upper bound on the throughput of a key-value cache based
on one round trip of communication. Second, ECHOs help charac-
terize the processing power of the RNIC: although the advertised
message rate of ConnectX-3 cards is 35 Mops, bidirectionally, they
can process many more messages.
An ECHO consists of a request message and a response message.
Varying the verbs and transport types yield several different imple-
(b) For small payloads, WRITE with inlining has a higher outbound through-
put than READ.
Figure 4: Comparison of outbound verbs throughput
mentations of ECHO. Figure 5 shows the throughput for some of
the possible combinations and for 32 byte payloads. The ﬁgure also
shows that using inlinining, selective signaling, and UC transport
increases the performance signiﬁcantly.
ECHOs achieve maximum throughput (26 Mops) when both the
request and the response are done as RDMA writes. However, as
shown in Section 3.3, this approach does not scale with the number
of connections. HERD uses RDMA writes (over UC) for requests
and SENDs (over UD) for responses. An ECHO server using this
hybrid also achieves 26 Mops—it gives the performance of WRITE-
based ECHOs, but with much better scalability.
Core1Core16MSCore1Core16C1{RD,WR,WR-I},RCWR-I,RC(ECHO)1 0 1 2 3 4 4 8 16 32 64 128 256 512 1024Latency (microseconds)Size of payload (bytes)WR-INLINEWRITEREADECHOECHO / 2Core1CoreNCore16MSCore1Core16C1{RD,WR},RCorWR,UCfromCN1 0 10 20 30 40 4 8 16 32 64 128 256 512 1024Throughput (Mops)Size of payload (bytes)WRITE-UCREAD-RCWRITE-RCCore1CoreNCore16MSCore1Core16C1RD,RCorSEND,UDor{WR-I,WR},UCtoCN1 0 5 10 15 20 25 30 35 40 0 64 128 192 256Throughput (Mops)Size of payload (bytes)WR-UC-INLINESEND-UDWRITE-UCREAD-RC299read by the RNIC followed by a packet transmission, whereas
inbound WRITEs only require a DMA write.
3.3 Using UD for responses
Our previous experiments did not show that as the number of con-
nections increases, connected transports begin to slow down. To
reduce hardware cost, power consumption, and design complexity,
RNICs have very little on-chip memory (SRAM) to cache address
translation tables and queue pair contexts [26]. A miss in this cache
requires a PCIe transaction to fetch the data from host memory.
When the communication fan-in or fan-out exceeds the capacity
of this cache, performance begins to suffer. This is a potentially
important effect to avoid both for cluster scaling, but also because it
interacts with the cache or store architectural decisions. For example,
the cache design we build on in HERD partitions the keyspace be-
tween several server processes in order to achieve efﬁcient CPU and
memory utilization. Such partitioning further increases the fan-in
and fan out of connections to a single machine.
To evaluate this effect, we modiﬁed our throughput experiments
to enable all-to-all communication. We use N client processes (one
process each at C1, ...,CN) and N server processes at MS. For mea-
suring inbound throughput, client processes select a server process
at random and issue a WRITE to it. For outbound throughput, a
server process selects a client at random and issues a WRITE to it.
The results of these experiments for 32 byte messages are presented
in Figure 6. Several results stand out:
Outbound WRITEs scale poorly: for N = 16, there are 256
active queue pairs at RNICS and the server-to-clients throughput
degrades to 21% of the maximum outbound WRITE throughput
(Figure 4b). With many active queue pairs, each posted verb can
cause a cache miss, severely degrading performance.
Inbound WRITEs scale well: Clients-to-server throughput is
high even for N = 16. The reason for this is that queueing of out-
standing verbs operations is performed at the requesting RNIC and
very little state is maintained at the responding RNIC. Therefore,
the responding RNIC can support a much larger number of active
queue pairs without incurring cache misses. The higher requester
overhead is amortized because the clients outnumber the server.
In a different experiment, we used 1600 client processes spread
over 16 machines to issue WRITEs over UC to one server process.
HERD uses this many-to-one conﬁguration to reduce the number
of active connections at the server (Section 4.2). This conﬁguration
also achieves 30 Mops.
Outbound WRITEs scale poorly only because RNICS must man-
age many connected queue pairs. This problem cannot be solved if
we use connected transports (RC/UC/XRC) because they require at
least as many queue pairs at MS as the number of client machines.
Scaling outbound communication therefore mandates using data-
grams. UD transport supports one-to-many communication, i.e., a
single UD queue can be used to issue operations to multiple remote
UD queues. The main problem with using UD in a high performance
application is that it only supports messaging verbs and not RDMA
verbs.
Fortunately, messaging verbs only impose high overhead at the re-
ceiver. Senders can directly transmit their requests; only the receiver
must pre-post a RECV before the SEND can be handled. For the
sender, the work done to issue a SEND is identical to that required
to post a WRITE. Figure 6 shows that, when performed over Unreli-
able Datagram transport, SEND side throughput is high and scales
well with the number of connected clients.
Figure 5: Throughput of ECHOs with 32 byte messages. In WR-SEND,
the response is sent over UD.
By avoiding the overhead of posting RECVs at the server, our
method of WRITE based requests and SEND-based responses pro-
vides better throughput than purely SEND-based ECHOs. Inter-
estingly, however, after enabling all optimizations, the throughput
of purely SEND-based ECHOs (with no RDMA operations) is 21
Mops, which is more than three-fourths of the peak inbound READ
throughput (26 Mops). Both Pilaf and FaRM have noted that RDMA
reads vastly outperform SEND-based ECHOs, which our results
agree with if our optimizations are removed. With these optimiza-
tions, however, SENDs signiﬁcantly outperform READs in cases
where a single SEND-based ECHO can be used in place of multiple
READs per request.
Our experiments show that several ECHO designs, with vary-
ing degrees of scalability, can perform better than multiple-READ
designs. From a network-centric perspective, this is fortunate: it
also means that designs that use only one cross-datacenter RTT can
potentially outperform multiple-RTT designs both in throughput and
in latency.
Discussion of verbs throughput: The ConnectX-3 card is adver-
tised to support 35 million messages per second. Our experiments
show that the card can achieve this rate for inbound WRITEs (Fig-
ure 3b) and slightly exceed it for very small outbound WRITEs
(Figure 4b). All other verbs are slower than 30 Mops regardless of
operation size. While the manufacturer does not specify bidirectional
message throughput, we know empirically that RNICS can service
30 million ECHOs per second (WRITE-based ECHOs achieve 30
Mops with 16 byte payloads; Figure 5 uses 32 byte payloads), or at
least 60 total Mops of inbound WRITEs and outbound SENDs.
The reduced throughputs can be attributed to several factors:
• For outbound WRITEs larger than 28 bytes, the RNIC’s mes-
sage rate is limited by the PCIe PIO throughput. The sharp
decreases in the WR-UC-INLINE and SEND-UD graphs in
Figure 4b at 64 byte intervals are explained by the use of
write-combining buffers for PIO acceleration. With the write-
combining optimization, the unit of PIO writes is a cache line
instead of a word. Due to the larger datagram header, the
throughput for SEND-UD drops for smaller payload sizes than
for WRITEs
• The maximum throughput for inbound and outbound READs
is 26 Mops and 22 Mops respectively, which is considerably
smaller than the advertised 35 Mops message rate. Unlike
WRITEs, READs are bottlenecked by the RNIC’s processing
power. This is as expected. Outbound READs involve a PIO
operation, a packet transmission, a packet reception, and a
DMA write, whereas outbound WRITEs (inlined and over UC)
avoid the last two steps.
Inbound READs require a DMA
 0 5 10 15 20 25SEND / SENDWR / WR WR / SENDThroughput (Million ECHOs/s)basic+unreliable+unsignalled+inlined300Figure 6: Comparison of UD and UC for all-to-all communication with
32 byte payloads. Inbound WRITEs over UC and outbound SENDs over
UD scale well up to 256 queue pairs. Outbound WRITEs over UC scale
poorly. All operations are inlined and unsignaled.
The slight degradation of SEND throughput beyond 10 connected
clients happens because the SENDs are unsignaled, i.e., server pro-
cesses get no indication of verb completion. This leads to the server
processes overwhelming RNICS with too many outstanding opera-
tions, causing cache misses inside the RNIC. As HERD uses SENDs
for responding to requests, it can use new requests as an indication
of the completion of old SENDs, thereby avoiding this problem.
4. DESIGN OF HERD
To evaluate whether these network-driven architectural decisions
work for a real key-value application, we designed and implemented
an RDMA-based KV cache, called HERD, based upon recent high-
performance key-value designs. Our HERD setup consists of one
server machine and several client machines. The server machine
runs NS server processes. NC client processes are uniformly spread
across the client machines.
4.1 Key-Value cache
The fundamental goal of this work is to evaluate our networking and
architectural decisions in the context of key-value systems. We do
not focus on building better back-end key-value data structures but
rather borrow existing designs from MICA [18].
MICA is a near line-rate key-value cache and store for classical
Ethernet. We restrict our discussion of MICA to its cache mode.
MICA uses a lossy index to map keys to pointers, and stores the
actual values in a circular log. On insertion, items can be evicted
from the index (thereby making the index lossy), or from the log in
a FIFO order. In HERD, each server process creates an index for
64 Mi keys, and a 4 GB circular log. We use MICA’s algorithm for
both GETs and PUTs: each GET requires up to two random memory
lookups, and each PUT requires one.
MICA shards the key space into several partitions based on a
keyhash. In its “EREW” mode, each server core has exclusive read
and write access to one partition. MICA uses the Flow Director [3]
feature of modern Ethernet NICs to direct request packets to the
core responsible for the given key. HERD achieves the same effect
by allocating per-core request memory at the server, and allowing
clients to WRITE their requests directly to the appropriate core.
4.1.1 Masking DRAM latency with prefetching
To service a GET, a HERD server must perform two random memory
lookups, prepare the SEND response (with the key’s value inlined in
the WQE), and then post the SEND verb using the post send()
function. The memory lookups and the post send() function are
the two main sources of latency at the server. Each random memory
Figure 7: Effect of prefetching on throughput
access takes 60-120 ns and the post send() function takes about
150 ns. While the latter is unavoidable, we can mask the memory
access latency by overlapping memory accesses of one request with
computation of another request.
MICA and CuckooSwitch [18, 31] mask latency by overlapping
memory fetches and prefetches, or request decoding and prefetches.
HERD takes a different approach: we overlap prefetches with the
post send() function used to transmit replies. To process mul-
tiple requests simultaneously in the absence of a driver that itself
handles batches of packets [2, 18, 31]), HERD creates a pipeline of
requests at the application level.
In HERD, the maximum number of memory lookups for each
request is two. Therefore, we create a request pipeline with two
stages. When a request is in stage i of the pipeline, it performs
the i-th memory access for the request and issues a prefetch for the
next memory address. In this way, requests only access memory
for which a prefetch has already been issued. On detecting a new
request, the server issues a prefetch for the request’s index bucket,
advances the old requests in the pipeline, pushes in the new request,
and ﬁnally calls post send() to SEND a reply for the pipeline’s
completed request. The server process expects the issued prefetches
to ﬁnish by the time post send() returns.
Figure 7 shows the effectiveness of prefetching. We use a
WRITE/SEND-based ECHO server but this time the server per-
forms N random memory accesses before sending the response.
Prefetching allows fewer cores to deliver higher throughput: 5 cores
can deliver the peak throughput even with N = 8. We conclude
that there is signiﬁcant headroom to implement more complex key-
value applications, for instance, key-value stores, on top of HERD’s
request-reply communication mechanism.
With a large number of server processes, this pipelining scheme
can lead to a deadlock. A server does not advance its pipeline until
it receives a new request, and a client does not advance its request
window until it gets a response. We avoid this deadlock as follows.
While polling for new requests, if a server fails for 100 iterations
consecutively, it pushes a no-op into the pipeline.
4.2 Requests
Clients WRITE their GET and PUT requests to a contiguous memory
region on the server machine which is allocated during initialization.
This memory region is called the request region and is shared among
all the server processes by mapping it using shmget(). The re-
quest region is logically divided into 1 KB slots (the maximum size
of a key-value item in HERD is 1 KB).
Requests are formatted as follows. A GET request consists only
of a 16-byte keyhash. A PUT request contains a 16-byte keyhash,
a 2-byte LEN ﬁeld (specifying the value’s length), and up to 1000
bytes for the value. To poll for incoming requests, we use the left-
 0 5 10 15 20 25 30 35 0 4 8 12 16Throughput (Mops)Number of client processes (= number of server processes)In-WRITE-UCOut-WRITE-UCOut-SEND-UD 0 5 10 15 20 25 1 2 3 4 5Throughput (Mops)Number of CPU coresN = 2, no prefetchN = 2, prefetchN = 8, no prefetchN = 8, prefetch3015. EVALUATION
We evaluate HERD on two clusters: Apt and Susitna (Table 2). Due
to limited space, we restrict our discussion to Apt and only present
graphs for RoCE on Susitna. A detailed discussion of our results
on Susitna may be found in [15]. Although Susitna uses similar
RNICs as Apt, the slower PCIe 2.0 bus reduces the throughput of
all compared systems. Despite this, our results on Susitna remain
interesting: just as ConnectX-3 cards overwhelm PCIe 2.0 x8, we
expect the next-generation Connect-IB cards to overwhelm PCIe 3.0
x16. Our evaluation shows that:
• HERD uses the full processing power of the RNIC. A single
HERD server can process up to 26 million requests per second.
For value size up to 60 bytes, HERD’s request throughput is
greater than native READ throughput and is much greater than
that of READ-based key-value services: it is over 2X higher
than FaRM-KV and Pilaf.
• HERD delivers up to 26 Mops with approximately 5 µs average
latency. Its latency is over 2X lower than Pilaf and FaRM-KV
at their peak throughput respectively.
• HERD scales to the moderately sized Apt cluster, sustaining
peak throughput with over 250 connected client processes.
We conclude the evaluation by examining the seeming drawback