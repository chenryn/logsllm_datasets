### ECHO Throughput and Its Significance

ECHO throughput is significant for two primary reasons. First, it provides an upper bound on the throughput of a key-value cache based on a single round trip of communication. Second, ECHO operations help characterize the processing power of the Remote Network Interface Card (RNIC). Although ConnectX-3 cards are advertised to support 35 million operations per second (Mops) bidirectionally, they can process many more messages in practice.

An ECHO operation consists of a request message and a response message. Varying the verbs and transport types yields several different implementations of ECHO. Figure 5 illustrates the throughput for some of these combinations with 32-byte payloads. The figure also demonstrates that using inlining, selective signaling, and Unreliable Connection (UC) transport significantly enhances performance.

### Maximizing ECHO Throughput

ECHOs achieve maximum throughput (26 Mops) when both the request and the response are performed as RDMA writes. However, as shown in Section 3.3, this approach does not scale well with the number of connections. HERD uses RDMA writes (over UC) for requests and SENDs (over Unreliable Datagram, UD) for responses. An ECHO server using this hybrid approach also achieves 26 Mops, combining the performance benefits of WRITE-based ECHOs with better scalability.

### Inbound vs. Outbound Performance

Inbound WRITEs require a DMA write, while outbound WRITEs involve a packet transmission followed by a read by the RNIC. Our experiments show that as the number of connections increases, connected transports begin to slow down. This is due to the limited on-chip memory (SRAM) in RNICs, which caches address translation tables and queue pair contexts. A miss in this cache requires a PCIe transaction to fetch data from host memory, leading to performance degradation when the communication fan-in or fan-out exceeds the cache capacity.

To evaluate this effect, we modified our throughput experiments to enable all-to-all communication. We used N client processes (one each at C1, ..., CN) and N server processes at MS. For inbound throughput, client processes randomly select a server process and issue a WRITE. For outbound throughput, a server process randomly selects a client and issues a WRITE. The results for 32-byte messages are presented in Figure 6, highlighting the following:

- **Outbound WRITEs** scale poorly: For N = 16, there are 256 active queue pairs at the RNICs, and the server-to-clients throughput degrades to 21% of the maximum outbound WRITE throughput (Figure 4b).
- **Inbound WRITEs** scale well: Clients-to-server throughput remains high even for N = 16. This is because queuing of outstanding verb operations is managed at the requesting RNIC, and very little state is maintained at the responding RNIC.

In another experiment, we used 1600 client processes spread over 16 machines to issue WRITEs over UC to one server process. This configuration, used by HERD to reduce the number of active connections at the server, achieves 30 Mops.

### Using UD for Responses

Outbound WRITEs scale poorly because RNICs must manage many connected queue pairs. This problem cannot be solved with connected transports (RC/UC/XRC), which require at least as many queue pairs at MS as the number of client machines. Therefore, scaling outbound communication mandates using datagrams. UD transport supports one-to-many communication, allowing a single UD queue to issue operations to multiple remote UD queues. The main drawback of UD in high-performance applications is that it only supports messaging verbs, not RDMA verbs.

Fortunately, messaging verbs only impose high overhead at the receiver. Senders can directly transmit their requests; only the receiver must pre-post a RECV before handling the SEND. Figure 6 shows that, when performed over Unreliable Datagram transport, SEND side throughput is high and scales well with the number of connected clients.

### Discussion of Verbs Throughput

The ConnectX-3 card is advertised to support 35 million messages per second. Our experiments show that the card can achieve this rate for inbound WRITEs (Figure 3b) and slightly exceed it for very small outbound WRITEs (Figure 4b). All other verbs are slower than 30 Mops regardless of operation size. While the manufacturer does not specify bidirectional message throughput, we empirically know that RNICs can service 30 million ECHOs per second (WRITE-based ECHOs achieve 30 Mops with 16-byte payloads; Figure 5 uses 32-byte payloads), or at least 60 total Mops of inbound WRITEs and outbound SENDs.

The reduced throughputs can be attributed to several factors:
- For outbound WRITEs larger than 28 bytes, the RNIC’s message rate is limited by the PCIe PIO throughput.
- The maximum throughput for inbound and outbound READs is 26 Mops and 22 Mops respectively, which is considerably smaller than the advertised 35 Mops message rate. Unlike WRITEs, READs are bottlenecked by the RNIC’s processing power.

### Design of HERD

To evaluate whether these network-driven architectural decisions work for a real key-value application, we designed and implemented an RDMA-based KV cache called HERD, based on recent high-performance key-value designs. Our HERD setup consists of one server machine and several client machines. The server machine runs NS server processes, and NC client processes are uniformly spread across the client machines.

#### Key-Value Cache

HERD's fundamental goal is to evaluate networking and architectural decisions in the context of key-value systems. We do not focus on building better back-end key-value data structures but rather borrow existing designs from MICA [18]. MICA is a near line-rate key-value cache and store for classical Ethernet. In HERD, each server process creates an index for 64 Mi keys and a 4 GB circular log. We use MICA’s algorithm for both GETs and PUTs: each GET requires up to two random memory lookups, and each PUT requires one.

MICA shards the key space into several partitions based on a keyhash. In its “EREW” mode, each server core has exclusive read and write access to one partition. MICA uses the Flow Director feature of modern Ethernet NICs to direct request packets to the core responsible for the given key. HERD achieves the same effect by allocating per-core request memory at the server, allowing clients to WRITE their requests directly to the appropriate core.

#### Masking DRAM Latency with Prefetching

To service a GET, a HERD server must perform two random memory lookups, prepare the SEND response (with the key’s value inlined in the WQE), and then post the SEND verb using the `post send()` function. The memory lookups and the `post send()` function are the two main sources of latency at the server. Each random memory access takes 60-120 ns, and the `post send()` function takes about 150 ns. While the latter is unavoidable, we can mask the memory access latency by overlapping memory accesses of one request with the computation of another request.

MICA and CuckooSwitch mask latency by overlapping memory fetches and prefetches, or request decoding and prefetches. HERD takes a different approach: we overlap prefetches with the `post send()` function used to transmit replies. To process multiple requests simultaneously, HERD creates a pipeline of requests at the application level. The maximum number of memory lookups for each request is two, so we create a request pipeline with two stages. When a request is in stage i of the pipeline, it performs the i-th memory access for the request and issues a prefetch for the next memory address. In this way, requests only access memory for which a prefetch has already been issued.

Figure 7 shows the effectiveness of prefetching. We use a WRITE/SEND-based ECHO server, where the server performs N random memory accesses before sending the response. Prefetching allows fewer cores to deliver higher throughput: 5 cores can deliver the peak throughput even with N = 8. This indicates significant headroom to implement more complex key-value applications, such as key-value stores, on top of HERD’s request-reply communication mechanism.

With a large number of server processes, this pipelining scheme can lead to a deadlock. A server does not advance its pipeline until it receives a new request, and a client does not advance its request window until it gets a response. We avoid this deadlock by pushing a no-op into the pipeline if a server fails to receive a new request for 100 consecutive iterations.

### Evaluation

We evaluated HERD on two clusters: Apt and Susitna (Table 2). Due to limited space, we restrict our discussion to Apt and only present graphs for RoCE on Susitna. A detailed discussion of our results on Susitna may be found in [15]. Although Susitna uses similar RNICs as Apt, the slower PCIe 2.0 bus reduces the throughput of all compared systems. Despite this, our results on Susitna remain interesting: just as ConnectX-3 cards overwhelm PCIe 2.0 x8, we expect the next-generation Connect-IB cards to overwhelm PCIe 3.0 x16. Our evaluation shows that:

- **HERD uses the full processing power of the RNIC.** A single HERD server can process up to 26 million requests per second. For value sizes up to 60 bytes, HERD’s request throughput is greater than native READ throughput and much greater than that of READ-based key-value services: it is over 2X higher than FaRM-KV and Pilaf.
- **HERD delivers up to 26 Mops with approximately 5 µs average latency.** Its latency is over 2X lower than Pilaf and FaRM-KV at their peak throughput, respectively.
- **HERD scales to the moderately sized Apt cluster, sustaining peak throughput with over 250 connected client processes.**

We conclude the evaluation by examining the seeming drawbacks and potential improvements.