W<i is the set of the predicted words before i. The MSP formula-
tion allows ϒ to learn a policy on the order of the words should be
predicted instead of predicting all words independently and simulta-
neously. In Section 6.2, we empirically show that MSP outperforms
MLC in terms of the precision-recall trade-off.
4 ATTRIBUTE INFERENCE ATTACKS
Embeddings are designed to encode rich semantic information
about the input data. When the input data is user-related, e.g., a
user’s video watch history, the embedding vector naturally captures
information about the user and is often much more informative than
the raw input. Although in many applications such rich information
from the embeddings is desired in order to provide personalized
services to the user, the embeddings may potentially reveal sensitive
information about the input that might not directly appear in or be
easy to infer from the input. As a motivating example, consider an
adversary that may curate a small set of public comments labeled
with authors of interest and then planning to use semantically
rich embeddings on unlabeled, targeted text fragments to aim to
deanonymize the author of the text. This is also the typical setup
in stylometry research [59, 65] that we make more realistic by
considering the challenges of curating labeled data (which may be a
data Daux
Algorithm 2 Black-box Inversion with multi-set prediction
1: Input: target embedding Φ(x∗), black-box model Φ, auxiliary
2: procedure MSPLoss(x, Φ, ϒ)
3:
4:
5:
6:
7:
Predict a word ˆw = arg max Pϒ(w|W<i , Φ(x)).
Wi ← Wi/{ ˆw} and W<i ← W<i ∪ { ˆw}.
log Pϒ(w|W<i , Φ(x)).
L ← L − 1
Initialize L ← 0,Wi ← W(x),W<i ← ∅
for i = 1 to ℓ do
w ∈Wi
return L
8:
9: Initialize ϒ as a recurrent neural network.
10: while ϒ not converged do
Sample a batch B ⊂ Daux.
11:
Compute LMSP ← 1
12:
Update ϒ with ∇LMSP.
13:
14: return ˆx = {arg max Pϒ(w|W< i, Φ(x∗))}ℓ
i =1
xi ∈B MSPLoss(xi , Φ, ϒ).
|Wi |
|B|
auxiliary data Daux
Algorithm 3 Sensitive attribute inference
1: Input: target embedding Φ(x∗), black-box model Φ, labeled
2: Query Φ with Daux and collect {(Φ(xi), si)|xi ∈ Daux}.
3: Train a classifier f that predicts s on {(Φ(xi), si)}.
4: return ˆs = f (Φ(x∗))
costly process) and strictly limiting the number of labeled examples
the adversary has to work with.
For inferring sensitive attributes, we assume that the adversary
has a limited set Daux = {(Φ(xi), si)}i of embeddings labeled with
the sensitive attribute, such as text authorship. The adversary then
treats the inference problem as a downstream task and trains a
classifier which predicts s given Φ(x) as inputs on Daux. At in-
ference time, adversary simply applies the classifier on observed
embedding Φ(x∗) to infer the sensitive attribute of x∗. We focus on
the scenario of the adversary only having limited labeled data so
as to (a) closely match real scenarios where labeled sensitive data
would be challenging to collect, and (b) demonstrate how easily
sensitive information can be extracted from the embeddings which
therefore constitutes an important vector of information leakage
from embeddings.
Connections between leakage and the objective. In supervised
learning, deep representations can reveal sensitive attributes of the
input as these attributes might be used as internal features for the
learning task [68]. The connection between unsupervised learning
tasks and the leakage of sensitive attributes is less well under-
stood. The objective functions (Equation 1 and 2) that maximize
the semantic similarity of data in context for training unsupervised
dual-encoder embedding models fall into contrastive learning frame-
work [63]. This framework theoretically explains how unsupervised
embedding training helps with the downstream tasks with a utility
perspective. We explore how it might also favor the inference of
some sensitive attributes from the perspective ofprivacy.
In this framework, training data of the embedding models are
associated with latent classes (e.g., authors of the texts). When
Algorithm 4 MIA on word embeddings
1: Input: target window of words C = [wb , ... , w0, ... , we], word
embedding matrix V , similarity function δ
2: Map words in C with V and get [vwb , ... , vw0 , ... , vwe].
3: ∆ ← {δi|δi = δ(vw0 , vwi),∀wi ∈ C/{w0}}.
4: return “member” if 1
δi ∈∆ δi ≥ τm else “non-member”
|∆|
training with contrastive loss, the embeddings are learned so as to
be similar for data in the same context and to be dissimilar for data
coming from negative samples. Our approach takes advantage of
the fact that data sharing the same latent class will often appear in
the same context. Therefore, embedding similarity will be closer
for inputs from the same class, and consequently with the same
sensitive attribute when there is a correlation. We further note
that unsupervised embeddings are especially helpful for attribute
inference under the limited data constraints as the embeddings are
trained on much larger unlabeled data which allows them to learn
semantic similarity over latent classes that might not be captured
only given limited labeled data.
5 MEMBERSHIP INFERENCE ATTACKS
Both inverting embeddings and inferring sensitive attributes con-
cern inference-time input privacy, i.e., information leaked about
the input x from the embedding vector. Another important aspect
of privacy of ML models is training data privacy, namely, what
information about the training data (which might be potentially
sensitive) is leaked by a model during the training process? We
focus on membership inference attacks [66] as a measurement of
training data leakage in the embedding models.
The goal of membership inference is to infer whether a data point
is in the training set of a given machine learning model. Classic
membership inference attacks mainly target supervised machine
learning, where a data point consists of a input feature vector and a
class label. For unsupervised embedding models trained on units of
data in context, we thus wish to infer the membership of a context
of data (e.g., a sliding window of words or a pair of sentences).
5.1 Word Embeddings
Prior works [60, 77] on membership inference suggest that sim-
ple thresholding attacks based on loss values can be theoretically
optimal under certain assumptions and practically competitive to
more sophisticated attacks [66]. In embedding models, the loss is
approximated based on sampling during training as described in
Section 2 and computing exact loss is inefficient. We thus develop
simple and efficient thresholding attacks based on similarity scores
instead of loss values.
Word embeddings are trained on a sliding window of words in the
training corpus. To decide the membership for a window of words
C = [wb , ... , w0, ... , we], the adversary first converts each word
into its embedding vectors [vwb , ... , vw0 , ... , vwe]. Then the adver-
sary computes a set of similarity scores ∆ = {δ(vw0 , vwi)|∀wi ∈
C/{w0}}, where δ is a vector similarity measure function (e.g. co-
sine similarity). Finally, the adversary uses the averaged score in ∆
Algorithm 5 Aggregated-level MIA on sentence embeddings
1: Input: target sentences in context X = [x1, ... , xn], sentence
embedding model Φ, similarity function δ, auxiliary data Daux
with membership labels
2: Map sentences in X with Φ and get [Φ(x1), ... , Φ(xn)].
3: if learning similarity function then
Initialize similarity function δ′ with projection Wm.
4:
while δ′ not converged do
5:
6:
7:
8:
9:
10: ∆ ← {δi|δi = δ(Φ(xi), Φ(xi +1))}n−1
i =1 .
11: return “member” if 1
Sample a batch B ⊂ Daux.
Compute loss LMIA for (xa, xb) ∈ B with Eq 10.
Update Wm with ∇LMIA.
Replace similarity function δ ← δ′.
δi ∈∆ δi ≥ τm else “non-member”
|∆|
to decide membership: if the averaged score is above some thresh-
old then C is a member of the training data and not a member
otherwise.
5.2 Sentence Embeddings
In sentence embeddings, we wish to decide membership of a pair
of sentence in context (xa, xb). We simply use the similarity score
δ(Φ(xa), Φ(xb)) as the decision score for membership inference as
sentences in context used for training will be more similar to each
other than sentences which were not used for training.
Aggregate-level membership inference. Sometimes, deciding
membership of a pair of sentences might not be enough to cause
a real privacy threat. In many user-centric applications, models
are trained on aggregation of data from users; e.g., a keyboard
prediction model is trained on users’ input logs on their phone [44].
In this scenario, the adversary infers membership on aggregate
data from a particular user to learn whether this user participated
training or not.
To perform MIA on aggregate text of n sentences X = [x1, x2,
... , xn], the adversary first gets each sentence embeddings [Φ(x1),
Φ(x2), ... , Φ(xn)]. Then the adversary collects the set of similarity
score ∆ = {δ(xi , xi +1)}n−1
i =1 . Finally, similar to MIA against word
embeddings, we use the average score in ∆ as the decision score
for membership inference.
Learned similarity metric function. Using a pre-defined simi-
larity measure may not achieve best membership inference results.
With auxiliary data labeled with membership information, an ad-
versary can learn a similarity metric customized for inference mem-
bership. More specifically, they learn a projection matrix Wm and
computes the learned similarity as δ′(xa, xb) = δ(W ⊤
m · Φ(xa),W ⊤
m ·
Φ(xb)). The attack optimizes the binary cross-entropy loss with
membership labels as following:
LMIA = −[ym log(δ
′
a,b) + (1 − ym) log(1 − δ
(10)
= δ′(xa, xb) and ym = 1 if (xa, xb) where in the training
′
a,b)]
where δ′
a,b
set and 0 otherwise.
6 EXPERIMENTAL EVALUATION
6.1 Embedding Models and Datasets
As each of the attacks assess different perspectives of privacy, we
evaluate them on different text embedding models that are either
trained locally on consumer hardware or are trained elsewhere
and public available. Here, we describe text embedding models
(and their corresponding datasets) we trained locally and evaluated
against attacks described in previous sections. Other public models
and datasets are detailed in subsequent subsections.
Word Embeddings on Wikipedia. We collected nearly 150,000
Wikipedia articles [42] for evaluating word embeddings. We locally
trained Word2Vec [46], FastText [3], and GloVe [54] embedding
models using half of the articles and use the other half for evaluating
membership inference.
For all word embeddings, we set the number of dimension in
embedding vector d to be 100. For Word2Vec and FastText, we set
the number of sampled negative words |Vneg| to be 25, learning
rate to be 0.05, sliding window size to be 5 and number of training
epochs to be 5. For GloVe, we set the number of training iterations
to be 50 as suggested in the original paper [54].
Sentence Embeddings on BookCorpus. Following prior works
on training unsupervised sentence embeddings [33, 39], we col-
lected sentences from BookCorpus [79] consists of 14,000 books.
We sample 40 millions sentences from half of the books as training
data and use the rest as held-out data.
We locally train sentence embedding models with dual-encoder
architecture described in Section 2. We considered two different
neural network architecture for the embedding models: a recurrent
neural network (LSTM [27]) and a three-layer Transformer [70].
For the LSTM, we set the size of embedding dimension d to be 1,200,
number of training epochs to be 1 and learning rate to be 0.0005
following previous implementation [39]. For the Transformer, we
set d to be 600, number of training epochs to be 5 with a warm-up
scheduled learning rate following [70]. For both architectures, we
train the model with Adam optimizer [32] and set the negative
samples Xneg as the other sentences in the same training batch at
each step and |Xneg| = 800.
6.2 Embedding Inversion
Target and auxiliary data. We randomly sample 100,000 sen-
tences from 800 authors in the held-out BookCorpus data as the
target data Dtarget = {x∗
i } to be recovered and perform inversion
on the set of embeddings Etarget = {Φ(x∗
i )}. We consider two types
of auxiliary data Daux: same-domain and cross-domain data. For
same-domain Daux, we use a set of 200,000 randomly sampled
sentences from BookCorpus that is disjoint to Dtarget. For cross-
domain Daux, we use a set of 800,000 randomly sampled sentences
from Wikipedia articles. We set the number of cross-domain data
points to be more than the same-domain data to match real-world
constraints where cross-domain data is typically public and cheap
to collect.
In addition to the two dual-
Additional embedding models.
encoder embedding models with LSTM and Transformer, we further
experiment with popular pre-trained language models for sentence
Table 1: White-box inversion results on sentence embed-
dings. Pre denotes precision and Rec denotes recall. We leave
the cross-domain results for Equation 5 as blank as no learn-
ing on auxiliary data is needed. The best results are in bold.
Same domain
Cross domain
Equation 5
LSTM
Transformer
BERT
ALBERT
Equation 7
LSTM
Transformer
BERT
ALBERT
Pre
56.93
35.74
0.84
3.36
Pre
63.68
65.32
50.28
70.91
Rec
56.54
35.44
0.89
2.95
Rec
56.69
60.39
49.17
55.49
F1
56.74
35.59
0.87
3.14
F1
59.98
62.76
49.72
62.26
Pre
-
-
-
-
Pre
57.98
59.97
46.44
68.45
Rec
-
-
-