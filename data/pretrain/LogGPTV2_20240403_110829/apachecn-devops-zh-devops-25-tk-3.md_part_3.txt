>         for: 15m
>         labels:
>           severity: notify
>         annotations:
>           summary: Cluster increased
>           description: The number of the nodes in the cluster increased
>       - alert: TooFewNodes
>         expr: count(kube_node_info)          for: 15m
>         labels:
>           severity: notify
>         annotations:
>           summary: Cluster decreased
>           description: The number of the nodes in the cluster decreased
```
我们增加了一个新条目`serverFiles.alerts`。如果您查看普罗米修斯的头盔文档，您会发现它允许我们定义警报(因此得名)。在其中，我们使用“标准”普罗米修斯语法来定义警报。
Please consult *Alerting Rules documentation* ([https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)) for more info about the syntax.
我们只定义了一组叫做`nodes`的规则。里面是两个`rules`。第一个(`TooManyNodes`)如果超过`3`节点`for`超过`15`分钟会通知我们。另一个(`TooFewNodes`)则会反其道而行之。它会告诉我们`15`分钟内是否没有节点(` 3
 expr: count(kube_node_info) > 0
> for: 1m
66c66
 for: 1m
```
新的定义改变了`TooManyNodes`警报的条件，即如果超过零个节点，则触发。我们还更改了`for`声明，这样我们就不需要在警报响起前等待`15`分钟。
让我们再次升级图表。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-nodes-0.yml
```
...我们将返回警报屏幕。
```
 1  open "http://$PROM_ADDR/alerts"
```
几分钟后(不要忘记刷新屏幕)，警报将切换到挂起状态，颜色将变为黄色。这意味着警报的条件得到满足(我们确实有零个以上的节点)，但是`for`时间段尚未到期。
等待一分钟(`for`周期的持续时间)并刷新屏幕。警报状态切换为开火，颜色变为红色。普罗米修斯发出了我们的第一次警报。
![](img/cb890924-63a6-49f3-953b-783f7495ba3b.png)
Figure 3-5: Prometheus' alerts screen with one of the alerts firing
警报发送到哪里了？普罗米修斯头盔显示器部署了警报器管理器，并预先配置了普罗米修斯向那里发送警报。我们来看看它的 UI。
```
 1  open "http://$AM_ADDR"
```
我们可以看到一个警报到达了警报管理器。如果我们点击`TooManyNodes`警报旁边的+ info 按钮，我们将看到注释(摘要和描述)以及标签(严重性)。
![](img/2de2ea86-984f-4529-ad8a-f54a3680104a.png)
Figure 3-6: Alertmanager UI with one of the alerts expanded
我们可能不会坐在警报管理器前等待问题出现。如果这是我们的目标，我们不妨等待普罗米修斯的警报。
显示警报确实不是我们拥有警报管理器的原因。它应该接收警报并进一步发送。它没有做任何这种事情，只是因为我们还没有定义它应该用来转发警报的规则。那是我们的下一个任务。
我们将看看普罗米修斯图表值的另一个更新。
```
 1  diff mon/prom-values-nodes-0.yml \
 2      mon/prom-values-nodes-am.yml
```
输出如下。
```
71a72,93
> alertmanagerFiles:
>   alertmanager.yml:
>     global: {}
>     route:
>       group_wait: 10s
>       group_interval: 5m
>       receiver: slack
>       repeat_interval: 3h
>       routes:
>       - receiver: slack
>         repeat_interval: 5d
>         match:
>           severity: notify
>           frequency: low
>     receivers:
>     - name: slack
>       slack_configs:
>       - api_url: "https://hooks.slack.com/services/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"
>         send_resolved: true
>         title: "{{ .CommonAnnotations.summary }}"
>         text: "{{ .CommonAnnotations.description }}"
>         title_link: http://my-prometheus.com/alerts
```
当我们应用这个定义时，我们将把`alertmanager.yml`文件添加到 Alertmanager 中。如果包含应用于调度警报的规则。`route`部分包含将应用于所有与`routes`不匹配的警报的一般规则。`group_wait`值使警报管理器等待`10`秒，以防来自同一组的其他警报到达。这样，我们将避免收到多个相同类型的警报。
当一个组的第一个警报被调度时，它将在发送来自同一组的下一批新警报之前使用`group_interval`字段的值(`5m`)。
`route`部分的`receiver`字段定义了警报的默认目的地。这些目的地在下面的`receivers`部分定义。在我们的例子中，我们默认向`slack`接收器发送警报。
`repeat_interval`(设置为`3h`)定义了如果警报管理器继续接收警报，将重新发送警报的时间段。
`routes`部分定义了具体的规则。只有当它们都不匹配时，才会使用上面`route`部分中的那些。`routes`部分继承了上面的属性，因此只有我们在这一部分定义的属性会改变。我们将继续发送匹配的`routes`到`slack`，唯一的变化是`repeat_interval`从`3h`增加到`5d`。
`routes`的关键部分是`match`段。它定义了用于决定警报是否匹配的过滤器。在我们的例子中，只有那些标签为`severity: notify`和`frequency: low`的才会被认为是匹配的。
总之，`severity`标签设置为`notify`、`frequency`设置为`low`的提醒将每五天重新发送一次。所有其他警报的频率为三小时。
警报管理器配置的最后一部分是`receivers`。我们只有一个名为`slack`的接收器。`name`下面是`slack_config`。它包含特定于 Slack 的配置。我们可以使用`hipchat_config`、`pagerduty_config`或任何其他支持的。即使我们的目的地不是其中之一，我们也可以返回`webhook_config`并向我们选择的工具的应用编程接口发送定制请求。
For the list of all the supported `receivers`, please consult *Alertmanager Configuration* page ([https://prometheus.io/docs/alerting/configuration/](https://prometheus.io/docs/alerting/configuration/)).
在`slack_configs`部分，我们有`api_url`，它包含来自 *devops20* 频道的一个房间的带有令牌的 Slack 地址。
For information how to general an incoming webhook address for your Slack channel, please visit the *Incoming Webhooks* page ([https://api.slack.com/incoming-webhooks](https://api.slack.com/incoming-webhooks)).
接下来是`send_resolved`旗。当设置为`true`时，警报管理器不仅会在触发警报时发送通知，还会在导致警报的问题得到解决时发送通知。
我们使用`summary`注释作为消息的`title`，使用`description`注释作为`text`。两者都使用*围棋模板*([https://golang.org/pkg/text/template/](https://golang.org/pkg/text/template/))。这些是我们在普罗米修斯的警告中定义的相同注释。
最后将`title_link`设置为`http://my-prometheus.com/alerts`。这确实不是你的普罗米修斯用户界面的地址，但是，由于我不能提前知道你的域名，我放了一个不存在的。请随意将`my-prometheus.com`更改为环境变量`$PROM_ADDR`的值。或者让它保持原样，知道如果你点击链接，它不会带你到你的普罗米修斯用户界面。
现在我们已经探索了 Alertmanager 配置，我们可以继续升级图表了。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-nodes-am.yml
```
几分钟后，警报器管理器将被重新配置，下次它收到普罗米修斯的警报时，它会将其发送给 Slack。我们可以通过访问`devops20.slack.com`工作区来确认。如果您尚未注册，请前往[slack.devops20toolkit.com](http://slack.devops20toolkit.com)。一旦您成为会员，我们可以访问`devops25-tests`频道。
```
 1  open "https://devops20.slack.com/messages/CD8QJA8DS/"
```
你应该看到`Cluster increased`通知。如果你看到其他信息，不要感到困惑。你可能不是唯一一个做这本书练习的人。
![](img/d1936f65-8d48-452a-91f0-03da31c126ca.png)
Figure 3-7: Slack with an alert message received from Alertmanager Sometimes, for reasons I could not figure out, Slack receives empty notifications from Alertmanager. For now, I'm ignoring the issue out of laziness.
现在，我们已经完成了普罗米修斯和警报器管理器的基本用法，我们将从实践练习中休息一下，讨论我们可能想要使用的指标类型。
# 我们应该使用哪些度量类型？
如果这是你第一次使用普罗米修斯从库贝应用编程接口连接到指标，纯粹的数量可能是压倒性的。除此之外，考虑到该配置排除了 Kube API 提供的许多指标，我们可以通过额外的导出器进一步扩展范围。
虽然每种情况都不同，并且您可能需要特定于您的组织和体系结构的一些指标，但是我们应该遵循一些指导原则。在本节中，我们将讨论关键指标。一旦你通过几个例子理解了它们，你应该能够将它们的使用扩展到你的特定用例。
The four key metrics everyone should utilize are latency, traffic, errors, and saturation.
这四个指标被谷歌**站点可靠性工程师** ( **SREs** )倡导为跟踪系统性能和健康的最基本指标。
**延迟**表示服务响应请求所需的时间。重点不仅应该放在持续时间上，还应该区分成功请求的延迟和失败请求的延迟。
**流量**是对服务需求的衡量。一个例子是每秒的 HTTP 请求数。
**错误**通过失败请求的比率来衡量。大多数情况下，这些失败是显式的(例如，HTTP 500 错误)，但也可以是隐式的(例如，HTTP 200 响应的正文描述查询没有返回任何结果)。
**饱和度**可以用服务或系统的“充满度”来描述。一个典型的例子是缺少中央处理器，这会导致节流，从而降低应用的性能。
随着时间的推移，出现了不同的监测方法。例如，我们得到了 **USE** 方法，该方法规定对于每个资源，我们应该检查**利用率**、**饱和度**和**错误**。另一种是 **RED** 方法，将**率**、**误差**、**持续时间**定义为关键指标。这些和许多其他的在本质上是相似的，并且与 SREs 测量延迟、流量、错误和饱和度的需求没有显著的不同。
我们将逐一介绍 SREs 描述的四种测量类型，并提供几个示例。我们甚至可以用不一定符合这四个类别中任何一个的度量标准来扩展它们。首先是延迟。