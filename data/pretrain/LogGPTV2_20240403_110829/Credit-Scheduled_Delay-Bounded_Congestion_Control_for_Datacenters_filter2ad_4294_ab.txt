drop, which allows us to send credit packets more aggressively. To
demonstrate its potential, we implement a na√Øve credit-based scheme
where a receiver sends credits at its maximum rate. At the bottle-
neck link, the switch drops excess credit packets using rate-limiting.
We use a Pica8 P-3780 10 GbE switch to configure rate-limiting on
credit packets. Figure 2 shows the convergence characteristics of a
na√Øve credit-based scheme compared to TCP cubic and DCTCP. It
shows the credit-based design can converge to fairness in just one
round-trip time, significantly outperforming the TCP variants.
Small RTT and sub-packet regime [16]: Finally, datacenter net-
works have a small base RTTs around tens of microseconds. Low la-
tency cut-through switches even achieve sub-microsecond latency [8].
Small RTT coupled with a large number of concurrent flows means
each flow may send less than one packet per RTT on average [14]‚Äî
even at 100 Gbps (10 Gbps), 416 (42) flows are enough to reach this
(a) Multi-bottleneck topology
(b) Parking lot topology
Figure 4: Problems with na√Øve credit-based approach
point assuming 50 ¬µs RTT. In this regime, window-based protocols
break down [14, 16, 32]. Rate-based protocols can increase the prob-
ing interval to multiples of RTTs, but this comes at a cost because the
ramp-up time also increases significantly. Fundamentally, supporting
a high dynamic range of flows efficiently requires a cost-effective
mechanism for bandwidth probing. The credit-based approach opens
up a new design space in this regard.
3 EXPRESSPASS DESIGN
In ExpressPass, credit packets are sent end-to-end on a per-flow
basis. Each switch and the host NIC rate-limit credit packets on
a per-link basis to ensure that the returning flow of data does not
exceed the link capacity. Symmetric routing ensures data packets
follow the reverse path of credit flows (see Section 3.1 for details).
Intuitively, our end-to-end credit-based scheme ‚Äúschedules‚Äù the
arrival of data packets at packet granularity, in addition to controlling
their arrival rate at the bottleneck link. To show how this mechanism
works, we illustrate a simple scenario with two flows in Figure 3,
where all links have equal capacity. Consider a time window in
which only two packets can be transmitted on the link. Now, receiver
RA and RB generate credits (A1, A2) and (B1, B2) respectively at
the maximum credit rate. All four credit packets arrive at output port
O, where credit packets are rate-limited to match the capacity of the
reverse link capacity. Thus, half the credits are dropped at the output
port. In this example, each sender gets one credit and sends one data
packet back to the receivers. Note, the senders generate exactly two
data packets that can go through the bottleneck link during the time
window. In addition, in an ideal case where the RTTs of the two
flows are the same, the data packets do not experience any queuing
because their arrival at the bottleneck link is well-scheduled.
Design challenges: While the credit-based design shows promising
outlook, it is not without its own challenges. One might think that
with the credit-based scheme a na√Øve approach in which a receiver
sends credit packets as fast as possible can achieve fast convergence,
high utilization, and fairness all at the same time. However, the na√Øve
approach has serious problems with multiple bottlenecks. First, it
does not offer fairness. Consider the multi-bottleneck topology of
Figure 4 (a). When all flows send credit packets at the maximum
rate, the second switch (from the left) will receive twice as many
credit packets for Flow 0 than Flow 1 and Flow 2. As a result, Flow 0
occupies twice as much bandwidth on Link 2 than others. Second,
multi-bottlenecks may lead to low link utilization. Consider the
parking lot topology of Figure 4 (b). When credit packets are sent at
full speed, link 1‚Äôs utilization drops to 83.3%. This is because, after
50% of Flow 0‚Äôs credit passing link 1 (when competing with Flow 1),
only 33.3% of credit packets go through Link 2, leaving the reverse
B1A1Data packetCredit packetSendersCredit QueueDrop(MTU Size)(Min Ethernet Size)RARBSASBA2A1B2B1RateLimitA1B1A2B2SwitchA1B1A1‚ÄôB1‚ÄôA1‚ÄôB1‚ÄôA1‚ÄôB1‚ÄôReceiversS A RASBRBFlow AFlow BSwitchCredit OverflowPort ùë∂Link 3Flow 0Flow 1Flow 2datacreditLink 1Link 2Unfairly sharedLink 1Link 2Flow 0Flow 1Flow NUnder-utilizedSIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
Inho Cho, Keon Jang, and Dongsu Han
path of Link 1 under-utilized by 16.6%. Finally, in large networks,
the RTTs of different paths may differ significantly. This may break
the scheduling of data packets, which leads to a queue build-up.
Achieving high utilization, fairness, and fast convergence at the
same time is non-trivial and requires careful design of a credit feed-
back loop. In addition, we must limit the queue build-up to ensure
zero data loss. Next, we present the design details and how the
end-hosts and switches work together to realize the goals.
3.1 Switch and End-Host Design
Credit rate-limiting (switch and host NIC): For credit packets, we
use a minimum size, 84 B Ethernet frame, including the preamble and
inter-packet gap. Each credit packet triggers a sender to transmit up
to a maximum size Ethernet frame (e.g., 1538 B). Thus, in Ethernet,
the credit is rate-limited to 84/(84 + 1538) ‚âà 5% of the link capacity,
and the remaining 95% is used for transmitting data packets. The
host and switch perform credit rate-limiting at each switch port. The
host also tags credit packets so that switches can classify them and
apply rate-limiting on a separate queue. To pace credit packets and
limit their maximum burst size, we apply a leaky bucket available on
commodity switch chipsets (e.g., maximum bandwidth metering on
Broadcom chipsets). At peak credit-sending rate, credits are spaced
apart by exactly 1 MTU in time (last byte to the first byte). Because
data packets are not always the full MTU in size, two or more data
packets can be transmitted back to back, and by the time a credit is
sent there may be additional tokens accumulated for a fraction of
a credit packet. Increasing the token bucket to the size of 2 credit
packets ensures these fractional amounts are not discarded so that
credit sending rate becomes nearly the maximum on average.
Finally, to limit the credit queue, we apply buffer carving [19] to
assign a fixed buffer of four to eight credit packets to the class of
credit packets.
Ensuring path symmetry (switch): Our mechanism requires path
symmetry‚Äîdata packet must follow the reverse path of the corre-
sponding credit packet. Datacenter networks with multiple paths
(e.g., Clos networks) often utilize equal-cost multiple paths. In this
case, two adjacent switches need to hash the credit and data pack-
ets of the same flow onto the same link (in different directions)
for symmetric routing. This can be done, in commodity switches,
by using symmetric hashing with deterministic Equal Cost Multi-
Path (ECMP) forwarding. Symmetric hashing provides the same
hash value for bi-directional flows [15, 17, 45], and deterministic
ECMP sorts next-hop entries in the same order (e.g., by the next
hop addresses) on different switches [21, 51]. Finally, it requires
a mechanism to exclude links that fail unidirectionally [52]. Note
path symmetry does not affect performance. Even with DCTCP, the
utilization and performance on fat tree topology are not affected by
path symmetry in our simulations.
Ensuring zero data loss (switch): Rate-limiting credit packets con-
trols the rate of data in the reverse path and makes the network
congestion-free. However, the difference in RTT can cause tran-
sient queue build-up. Fortunately, the maximum queue build-up is
bounded in ExpressPass. We apply network calculus [37] to deter-
mine the bound. Note this bound is equivalent to the buffer require-
ment to ensure zero-data loss.
Topology (link/core-link speed) ToR down
ToR up
Core
(Core/Aggr./ToR/Server)
32-ary fat tree (10/40 Gbps)
(16 / 512 / 512 / 8,192)
32-ary fat tree (40/100 Gbps)
(16 / 512 / 512 / 8,192)
3-tier Clos (10/40 Gbps)
(16 / 128 / 1024 / 8,192)
3-tier Clos (40/100 Gbps)
(16 / 128 / 1024 / 8,192)
(norm. by DCTCP K)
577.3 KB
(5.77)
1.06 MB
(2.65)
577.3 KB
(5.77)
1.06 MB
(2.65)
(0.33)
19.0 KB 131.1 KB
(0.19)
37.2 KB 221.8 KB
(0.09)
19.0 KB 131.1 KB
(0.19)
37.2 KB 221.8 KB
(0.09)
(0.22)
(0.33)
(0.22)
Table 1: Required buffer size for ToR down ports, ToR up ports,
and Core ports with datacenter topology.
Given a network topology, we calculate the bound for each switch
port. Let us denote dp as the delay between receiving a credit packet
and observing the corresponding data packet at a switch port p, and
‚àÜdp as the delay spread of dp (i.e., the difference between maximum
and minimum of dp). ‚àÜdp is determined by the network topology and
the queuing capacity of the network. ‚àÜdp represents the maximum
duration of data buffering required to experience zero loss because,
in the worst case, ‚àÜdp time unit worth of data can arrive at the same
time at port p. For a given credit ingress port p, its delay, dp, consists
of four factors: (1) the delay caused by the credit queue, dcr edit ; (2)
the switching, transmission, and propagation delay of credit and data
packet to/from ingress port q of the next hop, t (p, q); (3) the delay
at ingress port q of the next hop, dq; and (4) returning data packet‚Äôs
queuing delay at port q, ddata (q) whose maximum is determined by
‚àÜdq.
dp = dcr edit + t (p, q) + dq + ddata (q)
Then, given an ingress port p and its set of possible next-hop ingress
ports N (p), its delay spread ‚àÜdp becomes:
‚àÜdp = max(dcr edit ) + max
q‚ààN (p )
(t (p, q) + dq ),
‚àí min
q‚ààN (p )
(t (p, q) + dq + ‚àÜdq )
(1)
In datacenters, switches are often constructed hierarchically (e.g.,
ToR, aggregator, and core switches). Within a hierarchical structure,
the delays can be computed in an iterative fashion. For example, if
we know the min/max delay of NIC ports, we can get the min/max
delay and the delay spread for uplink ports in a ToR switch. At
NIC, ‚àÜdp is the same as the delay spread of host processing delay,
‚àÜdhost , which is a constant given a host implementation. We can
then compute the min/max delay and the delay spread for uplink
ports in ToR and aggregator switches.
The delay spread accumulates along the path. In hierarchical
multi-rooted tree topologies, traffic from a downlink is forwarded
both up and down, but traffic from an uplink is only forwarded
down. Therefore, for uplink ports, the set of next hop‚Äôs ingress ports
N (p) only includes the switch/NIC ports at lower layers, whereas
for downlink ports N (p) includes ports at both lower and upper
layers. As a result, uplink ports require smaller buffer than downlink
ports. ToR downlink has the largest path length variance, thus has
the largest buffer requirement.
Table 1 shows the buffer per port requirement for different topolo-
gies. We assume a credit queue capacity of 8 credit packets (see
Section 3.2) and propagation delay of 5 ¬µs for core links and 1 ¬µs for
Credit-Scheduled Delay-Bounded
Congestion Control for Datacenters
SIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
(a) 8 credit queue, ‚àÜdhost = 5¬µs (b) 4 credit queue, ‚àÜdhost = 1¬µs
Figure 5: Maximum buffer for ToR switch in 32-ary fat tree
(a) Jitter vs. Fairness
(b) Inter-credit Gap at NIC
Figure 6: Required jitter for fair credit drop and measured
inter-credit gap at NIC
others. For the host processing delay spread, we use the measure-
ment result from our testbed (shown in Figure 14 (a)). To compare
buffer requirement of ExpressPass to DCTCP, we also show the
value normalized to DCTCP‚Äôs ECN marking threshold K as recom-
mended in the DCTCP paper [3]. Note, for ExpressPass, the result
shows the buffer requirement for operating the network without any
data packet loss, whereas DCTCP‚Äôs marking threshold represents the
average queue size for DCTCP. The maximum buffer requirement
is a very conservative bound assuming the worst case. It is required
when a part network that has the longest delay has a full (credit and
data) buffer, while the path with the shortest delay has zero buffer.
This is an unlikely event that only happens in a network with signifi-
cant load imbalance. Under realistic workloads, ExpressPass uses
only a fraction of the data queue as we show in Section 6. Finally,
ExpressPass‚Äôs correct operation does not depend on zero loss of data
packets (i.e., it operates even with smaller buffers).
Three main factors impact the required buffer size for zero data
loss: delay spread of host processing, credit buffer size, and link
speed. Figure 5 shows the breakdown of maximum buffer for a
ToR switch by the each contributing source in 32-ary fat tree (8, 192
hosts) topologies with (10/40), (40/100), and (100/100) Gbps (link/-
core link) speeds. We use two sets of parameters: a) 8 credit queue
and ‚àÜdhost = 5.1 ¬µs reflect our testbed setting; b) 4 credit queue and
‚àÜdhost = 1 ¬µs represent a case where ExpressPass is implemented in
NIC hardware. Smaller credit queue capacity and host delay spread
results in a smaller data queue. The required buffer space for zero-
loss increases sub-linearly with the link capacity. Note, a ToR switch
has the largest buffer requirement among all. Even then, its require-
ment is modest in all cases. Today shallow buffered 10 GbE switches
have 9 to 16 MB of shared packet buffers and 100 GbE switches have
16 MB to 256 MB [10, 54], whereas deep buffered switches have up
to 24 GB of shared packet buffer [54].
(a) Sender
(b) Receiver
Figure 7: State transition of sender and receiver.
Ensuring fair credit drop (end-host): ExpressPass relies on the
uniform random dropping of credit packets at the switch to achieve
fairness‚Äîif n flows are sending credits at the same rate to the shared
bottleneck, equal fraction of credit packets must be dropped from
each flow. Unfortunately, subtle timing issue can easily result in a
skewed credit drop with drop-tail queues. When the credit buffer
is (nearly) full, the order of credit packet arrival determines which