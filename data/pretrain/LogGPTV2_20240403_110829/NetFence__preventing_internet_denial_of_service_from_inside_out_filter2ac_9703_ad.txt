### Optimized Text

#### 5.2.1 Handling Packet Loss and On-Off Attacks

To detect and mitigate attacks, NetFence employs a low packet loss detection threshold (pth) to limit the damage (§ 4.3.1). Attackers may attempt on-off attacks, which can be categorized into macroscopic and microscopic types.

- **Macroscopic On-Off Attacks**: In these attacks, attackers flood the network after a congested router terminates a monitoring cycle. NetFence mitigates this by extending the monitoring cycle duration (§ 4.3.1).
- **Microscopic On-Off Attacks**: Here, attackers send traffic bursts with short on-off cycles, aiming to congest the network while maintaining average sending rates below their rate limits. Our theoretical analysis in § 3 and simulation results in § 6.3.2 demonstrate that such attack traffic patterns cannot reduce a legitimate user’s guaranteed bandwidth share. This is because a sender cannot exceed its rate limit at any time (§ 4.3.3), and NetFence's robust rate limit adjustment algorithm (§ 4.3.4) prevents sudden increases in the actual sending rate.

#### 5.2.2 Malicious On-Path Routers

A malicious router downstream of a congested link may attempt to remove or modify the L↓ feedback stamped by a congested router to hide upstream congestion. However, such attempts will invalidate the feedback, as the malicious router lacks the original tokennop value required to compute a valid MAC (§ 4.4).

Malicious on-path routers can also disrupt end-to-end communications by discarding packets, duplicating them, or increasing packet sizes to congest downstream links. They may alter the request packet priority field in the NetFence header to congest the request channel on downstream links. Preventing these attacks requires Byzantine fault-tolerant routing [36], which is not within NetFence's design scope. Instead, NetFence aims to make such attacks detectable. Passport [26], the source authentication system used by NetFence, partially protects packet integrity and enables duplicate detection. It includes the packet length and the first 8 bytes of the transport payload (which includes the TCP/UDP checksum) in its MAC computation. We can further extend Passport’s MAC computation to include NetFence’s request packet priority field for additional protection.

#### 5.3 Incremental Deployment

NetFence can be incrementally deployed by end systems and routers. The NetFence header is a shim layer between IP and upper-layer protocols, allowing legacy applications to remain unmodified. Legacy routers can ignore the NetFence header and forward packets using the IP header. Only routers at congested links and access routers need to be upgraded, but well-provisioned routers capable of handling tens of Gbps of attack traffic may not require upgrades. Deployment can use a bump-in-the-wire approach, placing inline boxes that implement NetFence’s enforcement functions in front of routers that need upgrading. Middleboxes like firewalls must be configured to permit NetFence traffic.

NetFence provides deployment incentives to both end systems and ASes, as legacy traffic is treated with lower priority by deployed ASes (Figure 2). Deployed ASes can form a trusted overlay network, protecting each other’s legitimate traffic within their networks. Undeployed networks do not protect this traffic, encouraging ASes to direct traffic to other deployed ASes using BGP.

#### 6. Implementation and Evaluation

We have implemented NetFence prototypes in Linux and the ns-2 simulator. Next, we evaluate the NetFence header and packet processing overhead using our Linux implementation and use ns-2 simulations to demonstrate NetFence’s effectiveness in mitigating DoS attacks.

##### 6.1 NetFence Header

Figure 6 shows the format of the NetFence header in our Linux implementation. A full NetFence header from a sender to a receiver includes a forward header and a return header. The forward header contains congestion policing feedback on the forward path, while the return header includes reverse path information. Most fields are self-explanatory. The PROTO field describes the upper-layer protocol (e.g., TCP or UDP). The timestamp unit is one second.

The return header can be omitted to reduce overhead if the sender has previously returned the latest feedback to the receiver. Even when present, it does not always include all fields. If the returned feedback is nop, the LINK-IDreturn field is omitted, and a bit in the FLAGS field indicates this omission. A NetFence header includes only the last two bits of the returned timestamp to save space. The sender’s access router reconstructs the full timestamp from its local time and the returned two bits, assuming the timestamp is less than four seconds old. With this implementation, a NetFence header is 20 bytes in the common case (nop feedback for both paths) and 28 bytes in the worst case (mon feedback for both paths).

##### 6.2 Micro-benchmarking

We implemented NetFence in Linux using XORP [19] and Click [24]. We modified XORP’s BGP module to establish pairwise symmetric keys between ASes and added data packet processing logic to Click. Click routers run in kernel space for packet forwarding, and XORP communicates with Click via the /click file system. We added a module between the IP and transport layers on end-hosts to handle NetFence headers, keeping upper-layer TCP/UDP protocols and legacy applications unmodified. AES-128 is used as a secure MAC function due to its fast speed and available hardware support [20, 21].

We benchmarked the Linux implementation on Deterlab [14] with a three-node testbed. Source access router A and destination C are connected via router B, with the B—C link as the bottleneck (5 Mbps capacity). Each node has two Intel Xeon 3GHz CPUs and 2GB memory. For benchmarking without attacks, we sent 100 Kbps UDP request packets and 1 Mbps UDP regular packets from A to C. For benchmarking under DoS attacks, we sent 1 Mbps UDP request packets and 10 Mbps UDP regular packets simultaneously.

Benchmarking results (Figure 7) show that with NetFence, a request packet incurs no extra processing on the bottleneck router B but introduces an average overhead of 546 ns on the access router A due to stamping the nop feedback. A regular packet incurs no extra processing on the bottleneck router but takes 781 ns on the access router to validate and generate new feedback. During attacks, the bottleneck router processes a 92B request packet in 492 ns and a 1500B regular packet in up to 554 ns. The access router processes a regular packet in 1267 ns on average during attacks.

For comparison, the performance of the capability system TVA+ [27] on the same topology is shown in Figure 7. The processing overhead introduced by NetFence is comparable to that of TVA+. Note that TVA+ caching capabilities, which require per-flow state on routers, is not considered here, as NetFence does not have this requirement.

These results indicate that NetFence’s per-packet overhead is low, with CPU-intensive operations primarily being AES computations. Given that commercial hardware supports AES operations at 40 Gbps [20], NetFence’s per-packet processing is unlikely to become a performance bottleneck. The benchmarking results do not include Passport overhead, as a Passport header can be updated by inline boxes near an AS’s ingress and egress border routers [26].

##### 6.3 Mitigating DoS Flooding Attacks

Next, we evaluate how well NetFence mitigates various DoS flooding attacks using ns-2 simulations. We compare NetFence with three other representative DoS mitigation schemes:

- **TVA+**: Uses network capabilities and per-host fair queuing to defend against DoS flooding attacks. It employs hierarchical queuing (first based on the source AS and then on the source IP address) at congested links and per-receiver fair queuing to mitigate authorized traffic flooding.
- **StopIt**: A filter and fair queuing-based DoS defense system. A targeted victim can install network filters to stop unwanted traffic. Similar to TVA+, it uses hierarchical queuing to separate legitimate traffic from attack traffic.
- **Fair Queuing (FQ)**: Provides per-sender fair queuing at every link, ensuring each sender gets a fair share of the link’s bandwidth.

We implemented TVA+ and StopIt as described in [27, 48] and used the Deficit Round Robin (DRR) algorithm [39] for fair queuing, which has O(1) per-packet operation overhead. In our simulations, attackers do not spoof source addresses because NetFence uses Passport [26] to prevent spoofing, allowing routers to queue attack traffic separately from legitimate traffic.

###### 6.3.1 Unwanted Traffic Flooding Attacks

We simulate the scenario where attackers directly flood a victim, but the victim can classify and block the attack traffic using the provided DoS defense mechanisms: capabilities in TVA+, secure congestion policing feedback in NetFence, and filters in StopIt.

We aim to simulate attacks involving thousands to millions of attackers flooding a well-provisioned link. Due to current simulation limitations, we scale down the bottleneck link capacity proportionally to simulate an increase in the number of attackers, following the evaluation approach in [48].