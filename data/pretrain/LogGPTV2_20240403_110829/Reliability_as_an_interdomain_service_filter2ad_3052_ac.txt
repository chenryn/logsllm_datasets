≤ fab(i, j),∀(i, j) ∈ E.
(2)
(3)
Moreover, a set P = ∪a,b∈V Pab is called a Q-percentage coverage
path set for ﬂow-based routing f if, for each OD pair a → b, Pab is
a Q-percentage path set of fab.
k:(i, j)∈Pk
pk
ab
∑
ab
With the coverage of a set of paths, we can measure how well a
set of paths approximate a given ﬂow-based routing. This can be
stated formally as the following lemma:
LEMMA 1. Given a ﬂow-based routing f and a Q-percentage
path set P for f , a valid path-based routing p = {pab|a, b ∈ V} over
P can be constructed such that for any demand d, the routed trafﬁc
on any link l ∈ E under p is upper bounded by 1/Q of the routed
trafﬁc on l under f .
PROOF. Please see Appendix.
In general, consider any network performance metric m which
is a function of |E| + 1 variables: the utilization ul of link l ∈ E,
and a function z(d) of a trafﬁc demand matrix d; that is, m =
m(u1, u2, . . . , u|E|; z(d)). Here, z(d) can be any function, as long
as it depends only on d. One example z(d) is the optimal link uti-
lization of the network under d. If m is monotonic increasing with
respect to ul(l ∈ E), we have
3.4.2 Path Generation for a Flow-based Routing
With the notion of the coverage of a path set, we can present our
algorithm for ﬁnding a small number of paths P guided by a ﬂow-
based routing f . The algorithm to generate paths Pab from a to b
based on fab is presented in Figure 3. To generate the complete
path set P, the same algorithm is repeated for each OD pair.
1. construct a DAG from ﬂow-based routing fab
2. i ← 1, Pab ← /0
3. while (termination condition not met)
4.
6.
7.
8.
9.
compute max unsplittable ﬂow pi
add ﬂow path Pi
ab to path set Pab
deduct ﬂow rate pi
remove links with capacity 0 from DAG
i ← i + 1
ab from link capacities along Pi
ab
ab satisfying SLA
Figure 3: The coverage-based algorithm for generating paths
Pab from a to b based on ﬂow-based routing fab.
There can be two approaches to the termination condition. The
ﬁrst is to generate no more than a ﬁxed number, K, of paths per
OD pair. We call such an approach K-path coverage. A network
may adopt this approach if it knows the maximum number of paths
it wants to select for any OD pair. The network can then evaluate
the performance of the selected path set by computing its coverage.
The second approach terminates only after a certain coverage is
achieved for every OD pair, and can thus bound the performance.
We call this approach Q-percentage coverage.
A key step of the algorithm is line 4: to compute the maximal
unsplittable ﬂow between a and b that satisﬁes the service level
agreement (SLA) delay constraint. This can be done in polynomial
time based on the observation that the link with the lowest capacity
on the maximal unsplittable ﬂow path should be saturated. Specif-
ically, we partition links according to their capacities. For a certain
capacity value C, we construct a subgraph by removing all links
with capacity less than C. We compute the lowest delay path from
source a to destination b in this subgraph. If the delay of the com-
puted path satisﬁes the SLA delay requirement, we have identiﬁed
that there is a unsplittable ﬂow satisfying the SLA constraint with
ﬂow rate at least C. Therefore, we can conduct a binary search over
all capacity values to identify the maximum unsplittable ﬂow rate
. Given this algorithm, at line 8, we can remove at least one link
in the network. Thus, in the worst case, the path set will consist of
|E| paths. However, as we will see in Section 5, we typically need
a much smaller number of paths than |E|.
4.
INTERDOMAIN BYPASS SELECTION
The preceding section assumes interdomain bypass paths to be
used are already chosen. In this section, we address the issue that
an IP network may receive many interdomain bypass paths and se-
lectively use a subset of these paths. This can reduce conﬁguration
overhead and/or cost for bypass paths with non-zero cost.
PROPOSITION 1. Given a ﬂow-based routing f and a Q-percentage
path set P for f , a valid path-based routing p over P can be con-
structed such that for any demand d, the performance metric m un-
der p is upper bounded by m(1/Q· u1, . . . ,1/Q· u|E|; z(d)), where
ul is the utilization of link l under f .
For example, assume that m(u1, u2, ..., u|E|; z(d)) Δ= maxl∈E ul,
which is a popular TE performance metric referred to as the bot-
tleneck trafﬁc intensity or maximum link utilization (MLU). Then
the constructed valid path-based routing p guarantees that, for any
demand d, its bottleneck trafﬁc intensity is at most 1/Q times that
of the original ﬂow-based routing f .
4.1 Overview
We select interdomain bypass paths in two steps.
In the ﬁrst
step, we select interdomain bypass paths to improve the physical
connectivity of the network. In the second step, we augment this
selection with additional interdomain bypass paths to improve the
performance of optimal fast rerouting for high priority failure sce-
narios.
4.2 Bypass Selection for Connectivity
We ﬁrst select interdomain bypass paths such that the link con-
nectivities of all intradomain links are above a certain level (e.g.,
greater than 2 or 3). Formally, the link connectivity of a link is
deﬁned as follows.
DEFINITION 2
(LINK CONNECTIVITY). The link connectiv-
ity of a link is the minimal number of links (including the link itself)
that must be removed in order to disconnect the two endpoints of
this link.
For any link l ∈ E, we denote by EC(l) the link connectivity of l.
EC is referred to as the link connectivity function.
Since each interdomain bypass path has associated (allocated)
bandwidth(s) and aggregated delay, we ﬁrst prune those bypass
paths with low bandwidths and long delays. The thresholds used
in this pruning process should depend on the SLA requirements of
the IP network. Among the interdomain bypass paths that survive
the pruning, we select a subset that minimizes the total cost while
achieving the target connectivities.
This selection problem can be stated formally as follows. Given
• a multigraph G = (V, E) that represents the network, simi-
lar to that deﬁned in Section 3.3, except that G may contain
parallel links due to the existence of multiple physical links
between some pair of nodes;
• a set BYPASS of interdomain bypass links, each of which
represents a different available interdomain bypass path. For
a link l ∈ BYPASS, cost(l) denotes the cost of using the cor-
responding interdomain bypass path. Note that there may
be parallel links in BYPASS, because there may be multiple
interdomain bypass paths between the same pair of intrado-
main nodes from multiple neighboring networks.
• a link connectivity requirement function req for a selected
(low connectivity) link set L ⊆ E;
our goal is to choose a subset E
(cid:4) = (V, E ∪ E
mented graph G
req(l),∀l ∈ L, and the total cost, as deﬁned by cost(E
is minimized.
(cid:4) ⊆ BYPASS such that, in the aug-
(cid:4)), the link connectivity ECG(cid:4)(l) ≥
(cid:4)) = ∑l∈E(cid:4) cost(l)
Several comments on this problem statement follow.
• Not all link connectivities can be improved even when one
considers the entire set of available bypass paths, i.e., when
(cid:4) = BYPASS. One must choose a realistic function req for
E
a realistic set of links L ∈ E. For instance, req can be system-
atically generated according to the priorities of links whose
connectivities need improvement. For each link l ∈ E, the
feasibility of req(l) can be checked by computing the link
connectivity of l in (cid:2)G = (V, E ∪ BYPASS).
• In addition to minimizing total cost, one may wish to also
limit the number of neighboring networks involved. Since
the total number of neighboring networks is usually small,
this can be handled by trying all combination of no more
than a certain number of neighbors.
• We handle SRLGs by treating the failure of all links in a
SRLG as a “single link failure” in Deﬁnition 2, and setting
target connectivity values accordingly. For instance, let CL
be the minimal set of links that must be removed to discon-
nect the two endpoints of a link l = (i, j).
If CL includes
3 links in the same SRLG and 1 link in a different SRLG,
we take the link connectivity of l to be 1 + 1 = 2 instead of
3 + 1 = 4. Therefore, if we want to keep i and j connected
under two simultaneous failures, we shall set the target con-
nectivity req(l) = 5.
requirement function req(l) = 2 for all l ∈ E; while in our prob-
lem, we may need to set req(l) > 2 to guard against SRLG failures.
For a general survey of the connectivity augmentation problem, we
refer interested readers to [15]. Our problem differs in that the set
of available links that can be added to the original graph is con-
strained. Our problem also resembles the network ﬂow improve-
ment problem [29], where one incurs a cost for increasing the ca-
pacity of an link and the goal is to achieve a maximum ﬂow through
the network under a given budget. Our problem differs in that we
seek to minimize the cost while achieving certain levels of connec-
tivity. Also, we seek to improve a set of ﬂows, each independently,
instead of just one.
We formulate this selection problem as a Mixed Integer Program
(MIP). Speciﬁcally, let (cid:2)G = (V, E ∪ BYPASS) be a ﬂow network
with unit capacity on all links. Let x(l) ∈ {0,1}, l ∈ BYPASS be
the indicator variables of interdomain bypass link selection, such
that x(l) = 1 if bypass link l is selected, and 0 otherwise. The MIP
can be formulated as follows:
min
subject to ∀(s,t) = l ∈ L, f(s,t) is a s-t ﬂow such that:
cost(l)· x(l)
∑
l∈BYPASS
0 ≤ f(s,t)(l) ≤ 1,∀l ∈ E
0 ≤ f(s,t)(l) ≤ x(l),∀l ∈ BYPASS
∑
k∈V
f(s,t)(s, k) ≥ req(s,t)
(4)
(5)
(6)
(7)
Note that in the above MIP, we have used the Maximum-Flow Min-
Cut Theorem [2] to implicitly encode the link connectivity require-
ment. We solve this MIP using ILOG CPLEX [8].
4.3 Bypass Selection for Fast Rerouting
We further augment the set of interdomain bypass paths to ensure
desired performance level during fast rerouting. Note that bypass
selection is involved in both of the two steps of our optimal fast
rerouting algorithm. First, bypass selection determines part of the
input set of links for optimal fast rerouting. Second, the coverage-
based path generation phase of our fast rerouting algorithm needs
to select paths that provide good coverage. Some of such paths may
need to traverse interdomain bypass paths.
One strategy would be to take all possible interdomain bypass
paths as input to our optimal fast rerouting algorithm, and leave it
to coverage-based path generation to determine which bypass paths
are really necessary. A serious drawback of this strategy is that
optimal fast routing may use more bypass paths than necessary.
Another strategy is to formulate the problem as a mixed inte-
ger program, which seeks to minimize the number of bypass paths
selected under the constraint that optimal fast rerouting has accept-
able performance. This strategy, however, requires the coordination
of optimal fast rerouting among different failure scenarios, which
increases the computational overhead of the problem formulated in
Section 3 dramatically, let alone the complexity of MIP.
Our solution to this problem is a simple sequential strategy. We
ﬁrst sort all available interdomain bypass paths from best to worst
according to a scoring function. This scoring function could be
cost, unit cost per bandwidth, or some combination of cost and
bandwidth constraints. For each k, we select the ﬁrst k paths and
test the performance of fast rerouting based on this set of bypass
paths. The selection process stops once we achieve performance
target.
5. EVALUATIONS
The above problem is a generalization of the connectivity aug-
In [7], the link connectivity
mentation problems studied in [7].
In this section, we evaluate our REIN framework and algorithms
using real network topologies and trafﬁc traces.
5.1 Evaluation Methodology
Network
Abilene
Abovenet
AOL
Cogent
Level-3
Qwest
Sprint
US-ISP
UUNet
Aggregation level
#Nodes
#Links
router-level
PoP-level
PoP-level
PoP-level
PoP-level
PoP-level
PoP-level
PoP-level
PoP-level
11
15
21
20
46
33
32
-
47
28
60
64
60
536
166
128
-
336
Table 2: Summary of network topologies used.
Dataset description: We use the real topologies of Abilene and a
major IP network which we call US-ISP. The topology of Abilene
is a router-level topology, while for US-ISP, we use a PoP-level
topology, which differs from the real router-level topology, but still
illustrates the scope and power of the framework and algorithms
proposed here. The topologies of Abilene and US-ISP are com-
plete without inference errors. In addition, we use the PoP-level
topologies of 7 IP networks as inferred by RocketFuel [39]. We re-
cursively merge the leaf nodes of the topologies with their parents
until no nodes have degree one, so that we have the backbone of
the networks. We assume that the available bandwidth of an inter-
domain bypass path is 20% of the typically link bandwidth of the
network (e.g., 2 Gbps in Abilene). Table 2 summarizes the topolo-
gies that contribute or use interdomain bypass paths. The data for
US-ISP are not shown due to privacy concerns.
We use real trafﬁc demand matrices of Abilene, made available
from [1]. We assume a random portion (up to 50%) of the trafﬁc of
Abilene are VPN trafﬁc. We captured, in Jan. 2007, both IP trafﬁc
and VPN trafﬁc matrices for US-ISP.
To generate failure scenarios, for Abilene, we enumerate all sin-
gle and two-link failure scenarios; for US-ISP, we process its sys-
tem logs and identify failure events.
Algorithms: We consider the following algorithms.
• Robust path-based TE/FRR (robust): The normal case rout-
ing, fast rerouting, and path generation are computed using
the algorithms in Section 3.
• Oblivious routing/bypassing (oblivious): The normal case
routing, fast rerouting are computed using the algorithms in
[6] and [5]. Note that oblivious does not use path generation.
• CSPF: This is the constrained shortest-path-ﬁrst (CSPF) al-
gorithm popularly used in IP/MPLS networks for trafﬁc en-
gineering. We use the actual weights of Abilene and US-ISP
to compute the normal case routing. The fast rerouting for
a set of failed links is computed by running the CSPF algo-
rithm with the failed links removed. Note that the normal
case routing can be implemented by standard IP forwarding,
while the bypass routing generally would require MPLS for-
warding using LSPs.
• Flow-based optimal routing (optimal): This is the optimal
routing for each given trafﬁc and failure scenario. It is un-
realistic to implement, as it is a ﬂow-based routing scheme.
Furthermore, under a failure scenario, it would require com-
plete change of routing, and thus could cause large disruption
to network trafﬁc. We use it as a lower bound for evaluating
the performance of other algorithms.
Performance metric: We measure the performance of the network
by the trafﬁc to capacity ratio at the bottleneck link (i.e., the link
with the highest trafﬁc to capacity ratio). We referred to this metric
as the bottleneck trafﬁc intensity or trafﬁc intensity for short. This
Network
Abilene
US-ISP
Period
03/01/04 - 09/10/04