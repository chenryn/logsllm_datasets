In Annual Linux Showcase & Conference, Vol. 5. 18â€“18.
[126] Raimondas Sasnauskas, Yang Chen, Peter Collingbourne, Jeroen Ketema, Jubi
Taneja, and John Regehr. 2017. Souper: A Synthesizing Superoptimizer. CoRR
abs/1711.04422 (2017). arXiv:1711.04422 http://arxiv.org/abs/1711.04422
[127] Eric Schkufza, Rahul Sharma, and Alex Aiken. 2013. Stochastic superoptimiza-
tion. ACM SIGARCH Computer Architecture News 41, 1 (2013), 305â€“316.
[128] Rahul Sharma, Eric Schkufza, Berkeley Churchill, and Alex Aiken. 2013. Data-
driven equivalence checking. In Proceedings of the 2013 ACM SIGPLAN interna-
tional conference on Object oriented programming systems languages & applica-
tions. 391â€“406.
[129] Rahul Sharma, Eric Schkufza, Berkeley Churchill, and Alex Aiken. 2015. Condi-
tionally correct superoptimization. ACM SIGPLAN Notices 50, 10 (2015), 147â€“162.
[130] Nikita V. Shirokov. 2018. XDP: 1.5 years in production. Evolution and lessons
learned.. http://vger.kernel.org/lpc_net2018_talks/LPC_XDP_Shirokov_v2.pdf.
In Linux Plumbers Conference.
[131] Armando Solar-Lezama, Rodric Rabbah, Rastislav BodÃ­k, and Kemal EbcioÄŸlu.
2005. Programming by sketching for bit-streaming programs. In Proceedings
of the 2005 ACM SIGPLAN conference on Programming language design and
implementation. 281â€“294.
[132] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia, and Vijay
Saraswat. 2006. Combinatorial sketching for finite programs. In Proceedings
of the 12th international conference on Architectural support for programming
languages and operating systems. 404â€“415.
[133] Kausik Subramanian, Loris Dâ€™Antoni, and Aditya Akella. 2017. Genesis: Synthe-
sizing forwarding tables in multi-tenant networks. In Proceedings of the 44th
ACM SIGPLAN Symposium on Principles of Programming Languages. 572â€“585.
[134] Kausik Subramanian, Loris Dâ€™Antoni, and Aditya Akella. 2018. Synthesis of Fault-
Tolerant Distributed Router Configurations. Proc. ACM Meas. Anal. Comput.
Syst. 2, 1, Article 22 (April 2018), 26 pages. https://doi.org/10.1145/3179425
[135] Ross Tate, Michael Stepp, Zachary Tatlock, and Sorin Lerner. 2009. Equality
saturation: a new approach to optimization. In Proceedings of the 36th annual
ACM SIGPLAN-SIGACT symposium on Principles of programming languages.
264â€“276.
[136] Emina Torlak and Rastislav Bodik. 2013. Growing solver-aided languages with
Rosette. In Proceedings of the 2013 ACM international symposium on New ideas,
new paradigms, and reflections on programming & software. 135â€“152.
[137] Jacob Van Geffen, Luke Nelson, Isil Dillig, Xi Wang, and Emina Torlak. 2020.
Synthesizing JIT Compilers for In-Kernel DSLs. In International Conference on
Computer Aided Verification. Springer, 564â€“586.
[138] Xi Wang, David Lazar, Nickolai Zeldovich, Adam Chlipala, and Zachary Tatlock.
2014. Jitk: A trustworthy in-kernel interpreter infrastructure. In 11th {USENIX}
Symposium on Operating Systems Design and Implementation ({OSDI} 14). 33â€“
47.
[139] Yichen Yang, Phitchaya Mangpo Phothilimtha, Yisu Remy Wang, Max Willsey,
Sudip Roy, and Jacques Pienaar. 2021. Equality Saturation for Tensor Graph
Superoptimization. arXiv:2101.01332 [cs.AI]
15
arXiv, July 14, 2021
Qiongwen Xu et al.
Appendices are supporting material that has not been peer-
reviewed.
A APPROACHES TO PROGRAM SYNTHESIS
In this paper, we are given a sequence of instructions in a fixed
instruction set, i.e., a BPF bytecode source program. We are inter-
ested in generating an alternative sequence of instructions, i.e., a
synthesized program, that satisfies the specification that (i) the
synthesized program is equivalent to the source program in its
input-output behaviors, (ii) the synthesized program is safe, and
(iii) the synthesized program is more efficient than the source pro-
gram. The precise meanings of efficiency and safety in the BPF
context are described in Â§2.3 and Â§6.
To simplify the following discussion, suppose the program spec-
ification is simply (i) above, i.e., ğ‘ ğ‘ğ‘’ğ‘ := ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„(ğ‘¥) == ğ‘ğ‘ ğ‘Ÿğ‘(ğ‘¥) for
source program ğ‘ğ‘ ğ‘Ÿğ‘ and synthesized program ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ for all pro-
gram inputs ğ‘¥. At a high level, the program synthesis problem we
are interested in can be formulated as the logical query
âˆƒğ‘.âˆ€ğ‘¥ .ğ‘(ğ‘¥) == ğ‘ğ‘ ğ‘Ÿğ‘(ğ‘¥)
(4)
where ğ‘ is any program composed of instructions from the BPF in-
struction set. As written down, this problem is a quantified boolean
formula (QBF) with alternating quantifiers, which does not per-
mit efficient decision procedures for problems that arise in syn-
thesis [131]. Hence, program synthesizers take the approach of
counterexample guided inductive synthesis (CEGIS) [131, 132]. First,
a candidate program ğ‘ğ‘ğ‘ğ‘›ğ‘‘ is determined through a search proce-
dure. Then, the synthesizer checks whether the candidate satisfies
the specification, by asking
âˆƒğ‘¥ .ğ‘ğ‘ğ‘ğ‘›ğ‘‘(ğ‘¥) ! = ğ‘ğ‘ ğ‘Ÿğ‘(ğ‘¥)
(5)
for the fixed program ğ‘ğ‘ğ‘ğ‘›ğ‘‘.
Typically, synthesis algorithms use test cases to quickly prune
candidates that do not satisfy the specification. If test cases do not
eliminate ğ‘ğ‘ğ‘ğ‘›ğ‘‘ as a candidate satisfying the specification, the query
above can usually be formulated in a first-order logic theory [51]
which permits efficient decision procedures. The query above is an
example of equivalence-checking, which determines whether two
programs produce the same output on all inputs. If the query (5) is
satisfiable, we get a counterexample, which can be added to the set
of test cases to prune the same or similar programs in the future
without discharging computationally-expensive logic queries. If (5)
is unsatisfiable, we have found a program ğ‘ğ‘ğ‘ğ‘›ğ‘‘ that produces the
same output as ğ‘ğ‘ ğ‘Ÿğ‘ on all inputs, and hence meets the specification.
The synthesis approaches in the literature differ mainly in the
search procedures they use to propose candidate programs ğ‘. There
are broadly four search approaches. Enumerative search (e.g., [40,
105, 117]) searches the space in order from smallest to largest pro-
grams, terminating the search with the smallest program that satis-
fies the specification. Rule-based search [93, 135, 139] uses targeted
rewrite rules to transform the source program into another pro-
gram that satisfies the specification yet is more optimal. Deductive
search [38, 82, 85, 89, 112, 126] encodes the search into a deductive
query discharged to a solver whose solution implies a program
within a specified grammar. Stochastic search [55, 127] searches the
space guided by cost functions that enable sampling new random
programs. Further, there are search algorithms that combine the
best of multiple approaches [84, 117].
16
Among the synthesis approaches above, in our view, stochastic
synthesis is the easiest to generalize to new and diverse contexts,
due to its ability to support very expressive cost functions and
constraints. We adopt and adapt stochastic synthesis in this paper.
B VERIFICATION CONDITIONS FOR BPF
MAPS AND HELPER FUNCTIONS
BPF maps are a special kind of memory. Like memory, a BPF pro-
gram could read a map with a specific key (look-up) and write a
value corresponding to an existing key (update). However, three
aspects make BPF maps very different from memory from the for-
malization perspective. First, the BPF map API [95] requires input
keys in the form of pointers to memory, and further, returns val-
ues that are themselves pointers to memory. Second, keys can be
deleted in a BPF map, unlike addresses in memory. Third, the keys
and values in BPF maps are persistent structures that exist before
and after program execution. Together, these aspects of BPF maps
prevent us from directly applying the same formalization methods
as memory access (Â§4.2) or other existing axiomatizations (e.g., par-
tial maps [118]) to model BPF maps in first-order logic. None of
the prior works on formalizing BPF [77, 114, 115, 137] handle any
aspect of BPF maps.
In the rest of this subsection, we show how our compiler handles
pointer inputs/outputs as well as deletions. We show how we ad-
dress the last aspect (map persistence) while performing program
equivalence checking (Â§4).
B.1 Handling pointer access to map memory
Supplying an input key to a map operation (e.g., look-up) as a
pointer creates two levels of aliasing behaviors: First, two distinct
registers may point to the same memory address (same as regular
pointer aliasing). Second, a map needs to return the same value
given the same (value of) key, even if the keys are supplied from
distinct memory addresses.
We formalize map access in bit vector theory by decomposing
the two levels of aliasing as follows. First, we create a new symbolic
variable for each key supplied to the map API and each pointer
value it returns. We call these variables the valuations of the (input)
key pointer and the (output) value pointer. We apply the formulas
in Â§4.2 to the key pointer, where the data that is read is the keyâ€™s
valuation.
To handle aliasing among the valuations of the keys themselves,
we write down formulas analogous to memory aliasing (Â§4.2) over
the valuations of the key and the value. This entails maintaining
map write and read tables analogous to memory read and write
tables.
Addressing aliasing among key valuations has the added benefit
of encoding the partial map axioms [118], e.g., a look-up of a key
following an update of that same key returns the value from that
update. No guarantees are made over the returned value pointer
itself (this is a reference to the kernelâ€™s memory) except that it is
non-zero.
Handling map manipulations through memory-access instructions.
BPF map values can be manipulated directly through memory-
access instructions, e.g., to increment a counter stored as a map
Synthesizing Safe and Efficient Kernel Extensions for Packet Processing
arXiv, July 14, 2021
value using a bpf_xadd instruction, which has the impact of perform-
ing *ptr = *ptr + reg. K2 uses the static analysis described in Â§5 to
determine the memory region corresponding to the pointer being
loaded or stored. The memory read and write tables (Â§4.2) ensure
that K2 puts down the right formulas for subsequent operations
over these map value memory regions.
B.2 Handling map deletions
Keys in a BPF map can be deleted, unlike memory locations. If a
deleted key is subsequently looked up, the result is a null pointer.
We model this by the simple trick of setting the value address cor-
responding to the valuation of a deleted key to 0 in the map write
table. When another key with the same valuation is looked up, the
returned value address is 0, indicating to the program that the key
does not exist in the map. Luckily, this return value coincides with
BPFâ€™s semantics for the return value of map lookup, which is 0
whenever the key does not exist in the map. We also handle setting
the right return value for the BPF map delete API call, which pro-
vides distinct return values depending on whether the key currently
exists in the map.
B.3 Handling multiple maps
The discussion so far assumes the existence of a single map with
all operations over this map. BPF programs may look-up several
maps in one program by loading map descriptors. However, the set
of all possible maps accessed by the program is known at compile
time. Hence, we can handle the general case by prefixing the pre-
condition map_id == K as the head of an implication, with the body
of the implication being the formula generated using the methods
described above.
B.4 Limitation of map model
The current modeling of maps ignores concurrency among threads
(kernel or user) accessing the same map. K2 does not make formal
guarantees about the equivalence of its output programs to input
programs when running in multi-threaded contexts where data
races are permissible. However, BPF provides mutual exclusion
for map contents through BPF spinlocks [61], and for code that
enforces mutual exclusion to map access, the semantics of K2â€™s out-
put programs are indeed equivalent to those of the input program
under concurrent access.
B.5 Other kernel helper functions
In addition to map helper functions, we also modeled a number
of other BPF kernel helper functions, including functions to get
random numbers, obtain a Unix timestamp, adjust the headroom
in a memory buffer containing a packet, get the processor ID on
which the program is executing, among others. The list of all BPF
helper functions numbers over a hundred (and growing [95]). Out
of engineering expediency, we only formalized the actions of helper
calls as we needed them, rather than support all of them.
C DETAILS ON OPTIMIZATIONS TO
EQUIVALENCE-CHECKING
The techniques described in Â§4 are sufficient to produce a work-
ing version of a BPF bytecode compiler. However, we found that
the time to compile a single program of even modest size (âˆ¼100
BPF instructions) is intractably large. The underlying reason is the
significant time required to perform equivalence checking, which
runs to several hours on even just a single pair of BPF programs on
a modern server (Â§8).
This section presents domain-specific optimizations that bring
down this verification time by several orders of magnitude to a
few milliseconds. We show the impact of each optimization in Â§8.
Several features of the BPF instruction set and packet-processing
programs contribute to increased equivalence checking time: alias-
ing in memory and map access, multiple types of memories (stack,
packet, metadata, and so on), usage of multiple maps, encoding the
semantics of helper functions, and the existence of control flow
within programs. K2 incorporates two broad approaches to reduce
verification time: concretization of symbolic terms in the formula
(Â§C.1) and modular verification of smaller parts of the program
(Â§C.2) to verify the larger formula from Â§4.
C.1 Concretizations of Symbolic Terms
It is well known that an explosion in the size and solving difficulty of
a formula under symbolic evaluation can be mitigated by assigning
specific values to, i.e., concretizing, terms in the formula [46, 50, 57,
97, 119].
We leverage the unique characteristics and simplicity of packet-
processing BPF programs, as well as the feasibility of constructing
simple static analyses (e.g., sound inference of types) within the
K2 compiler, to infer concrete values of several terms within the
equivalence-checking formulas dispatched to the solver. These con-
cretizations are â€œbest effortâ€ in the sense that we do not need the
analysis within K2 to be complete: we apply the simplifications
where possible, falling back to the general versions in Â§4 where
they are not applicable. We eschewed complicated alias relationship
mining techniques [55, 101] in favor of simpler domain-specific
ones.
To simplify the discussion, we describe our techniques for straight-
line programs (no branches) first, and then show how they general-
ize to loop-free programs with control flow.
I. Memory type concretization. BPF programs use multiple memory
regions: the stack, packet, maps, and various kernel data structures
like sockets. The handling of pointer-based aliasing of memory
access discussed in Â§4.2 uses a single write table and read table for
all of memory.
Instead, K2 leverages multiple read/write tables, one correspond-
ing to each distinct memory region. This is feasible to do since
any reference to a specific memory originates from pre-defined
inputs to the BPF program [88], such as the R10 register (stack), an
input register like R1 (packet or packet metadata), or return values
from specific helper calls (map memory, kernel socket structure,
etc.). The type of each pointer in a program can then be easily
inferred through a simple information-flow static analysis. For ex-
ample, given a code sequence bpf_mov r1 r10; bpf_mov r2 r1, it
is straightforward to infer that r2 is pointing into the stack mem-
ory. K2 tracks types for all registers across each instruction in this
manner.
The benefit of maintaining separate tables for separate memories
is that the size of aliasing formulas in a memory read reduces from
17
arXiv, July 14, 2021
ğ‘ğ‘™ğ‘™ ğ‘šğ‘’ğ‘š ğ‘¡ ğ‘¦ğ‘ğ‘’ğ‘ ) toğ‘¡ âˆˆğ‘šğ‘’ğ‘š ğ‘¡ ğ‘¦ğ‘ğ‘’ğ‘  ğ‘‚(ğ‘ 2
ğ‘‚(ğ‘ 2
ğ‘¡ ), where ğ‘(.) refers to
the number of accesses to memory of a specific type (or all types).
II. Memory offset concretization. The exact memory address con-
tained in a register during a BPF programâ€™s execution is in general
hard to determine at compile time, since the program stack or
the packet contents may be located anywhere within the kernelâ€™s
memory. However, in many packet-processing programs, the off-
sets relative to the base address of the memory region, e.g., such
as the offset (from the beginning of the packet) of a header field,
are known at program compile time. Further, BPF stack locations
can only be read after they are written, and it is feasible to track
which concrete offsets are being written to in the first place. K2
attempts to maintain a concrete offset into the memory region for
each register known to be a pointer. Each subsequent operation
on the register is associated with the corresponding operation on
the concrete offset. For example, the instruction bpf_mov r1 r10;
bpf_sub r1 2 /* r1 := r10 - 2 */ associates the offset -2 with
the register r1. A subsequent operation bpf_mov r2 r1; bpf_sub
r2 4 would also update the corresponding offset of r2 to -6. (Both
registers have a STACK memory type.)
Offset concretization has the effect of turning a clause like addr_i
== addr_j (Â§4.2) into a clause like offset_i == offset_j, where
offset_i and offset_j are concrete values, e.g., -2 == -6. When
a load instruction results in comparing a register offset to other
offsets stored in the write table, any comparison of a concrete pair
of offsets can be evaluated at compile time without even a solver
call. If all pairs of offsets compared are concrete, this can simplify
an entire memory read into a single clause of the form value_i ==
value_j. Even if only one of the offsets being compared is known
concretely, it simplifies the clause overall.
In the limit, this optimization has the effect of turning the entire
memory into nameable locations akin to registers. Similar ideas
have been applied in prior efforts to scale verification time [40, 127].
K2 applies this idea in a best-effort manner, falling back to general
aliasing when concrete offsets cannot be soundly inferred.
III. Map concretization. BPF programs access several maps and
frequently perform operations such as lookups, which are among
the most expensive BPF operations in terms of verification. Map
lookups generate large verification conditions because of two levels
of aliasing (Â§4.3). Accessing multiple maps in a single BPF pro-
gram further explodes the formula sizes, since one set of clauses is
generated for each possible map the program may have accessed.
Similar to the idea of concretizing memory types for each access,
K2 statically determines the map to which a given lookup or update
occurs. This is feasible due to a property of BPFâ€™s map-related in-
structions: there is a unique opcode LD_MAP_ID which loads a unique
and concrete map_id (obtained from a file descriptor), which is then
supplied as a parameter to each BPF map call.
We did not generalize our approach to concretize the keys that
are looked up or values that are presented to the map update func-
tion call. The BPF map API (Â§2) mandates map function parameters
and return values to be pointers to keys and values, rather than
registers containing the keys and values themselves. Map keys are
pointers to the stack, and the keys on the stack are often from input
memory regions, such as packet fields. As noted above (cf. memory
Qiongwen Xu et al.
offset concretization), we concretize memory addresses that are
accessed, but not the bits stored at concrete addresses in memory.
If the keys are symbolic, then the values are symbolic, too. We
leave leveraging opportunities to concretize map keys and values
to future work.
Incorporating control flow into optimizations. So far, the discus-
sion of the static analysis in K2 to determine concrete terms has
focused on straight-line code. In the presence of branches, this anal-
ysis is generalized as follows. First, statically known information
(e.g., pointer type, offset) is annotated with the basic block and path
condition under which it is determined. Next, for a subsequent read,
to determine the concrete information (e.g., offset) to apply, we
use the following procedure iteratively for each prior write to the
pointer, starting from the latest to the earliest in order of program
execution:
â€¢ If the last write is from a basic block that dominates [63] the
basic block of the reading instruction, we use the corresponding
entryâ€™s concrete information and stop. SSA dominance analysis
is performed just once per program.
â€¢ If the last write is from a basic block from which control never
reaches the basic block of the reading instruction, we skip the
corresponding entryâ€™s concrete information and move to the
next entry. Reachability analysis is performed just once per
program.
â€¢ If neither of the above is true, there is a path through the basic
block of the write to the current instruction, but there are also
other paths. K2 conjoins the clause path condition of the
entry â‡’ read offset == entry offset. The process continues