development and testing process. To a QA person, a test has to have a purpose, or
6760 Book.indb 16 12/22/17 10:50 AM
1.2 Software Quality 17
otherwise it is meaningless.14 Without a test purpose, it is difficult to assign a test
verdict—that is, did the test pass or fail? Various types of testing have different
purposes. Black-box testing today can be generalized to focus on three different
purposes (Figure 1.4). Positive testing can be divided into feature tests and perfor-
mance tests. Test requirements for feature testing consist of a set of valid use cases,
which may consist of only few dozens or at most hundreds of tests. Performance
testing repeats one of the use cases using various means of test automation such
as record-and-playback. Negative testing tries to test the robustness of the system
through exploring the infinite amount of possible anomalous inputs to find the tests
that cause invalid behavior. An anomaly can be defined as any unexpected input
that deviates from the expected norm, ranging from simple field-level modifications
to completely broken message structures or alterations in message sequences. Let
us explore these testing categories in more detail.
Feature testing, or conformance testing, verifies that the software functions
according to predefined specifications. The features can have relevance to secu-
rity—for example, implementing security mechanisms such as encryption and data
verification. The test specification can be internal, or it can be based on industry
standards such as protocol specifications. A pass criterion simply means that accord-
ing to the test results, the software conforms to the specification. A fail criterion
means that a specific functionality was missing or the software operated against
the specification. Interoperability testing is a special type of feature test. In interop-
erability testing, various products are tested against one another to see how the
features map to the generally accepted criteria. Interoperability testing is especially
important if the industry standards are not detailed enough to provide adequate
guidance for achieving interoperability. Most industry standards always leave some
features open to interpretation. Interoperability testing can be conducted at special
Figure 1.4 Testing purposes: features, performance, and robustness.
14 Note that this strict attitude has changed lately with the increasing appreciation to agile testing
techniques. Agile testing can sometimes appear to outsiders as ad hoc testing. Fuzzing has many
similarities to agile testing processes.
6760 Book.indb 17 12/22/17 10:50 AM
18 Introduction
events sometimes termed plug-fests (or unplug-fests in the case of wireless protocols
such as Bluetooth).
Performance testing tests the performance limitations of the system, typically
consisting of positive testing only, meaning it will send large amounts of legal traffic
to the SUT. Performance is not only related to network performance, but can also
test local interfaces such as file systems or API calls. The security implications are
obvious: A system can exhibit denial-of-service when subjected to peak loads. An
example of this is DDoS attacks. Another example from the field of telephony is
related to the mothers’ day effect, meaning that a system should tolerate the unfor-
tunate event when everyone tries to use it simultaneously. Performance testing will
measure the limits that result in denial of service. Performance testing is often called
load testing or stress testing, although some make the distinction that performance
testing attempts to prove that a system can handle a specific amount of load (traf-
fic, sessions, transactions, etc.), and that stress testing investigates how the system
behaves when it is taken over that limit. In any case, the load used for performance
testing can either be sequential or parallel—that is, a number of requests can be
handled in parallel, or within a specified time frame. The acceptance criteria are
predefined and can vary depending on the deployment. Whereas another user can
be happy with a performance result of 10 requests per second, another user could
demand millions of processed requests per minute. In failure situations, the system
can crash, or there can be a degradation of service where the service is denied for
a subset of customers.
Robustness testing (including fuzzing) is complementary to both feature and
performance tests. Robustness can be defined as an ability to tolerate exceptional
inputs and stressful environmental conditions. Software is not robust if it fails when
facing such circumstances. Attackers can take advantage of robustness problems
and compromise the system running the software. Most security vulnerabilities
reported in the public are caused by robustness weaknesses.
Whereas both feature testing and performance testing are still positive tests,
based on real-life use cases, robustness testing is strictly negative testing with tests
that should never occur in a well-behaving, friendly environment. For every use
case in feature testing, you can create a performance test by running that use case
in parallel or in rapid succession. Similarly, for every use case in feature testing,
you can create misuse cases by systematically or randomly breaking the legal and
valid stimuli.
With negative testing, the pass-fail criteria are challenging to define (Figure 1.5).
A fail criterion is easier to define than a pass criterion. In robustness testing, you
can define that a test fails if the software crashes, becomes unstable, or does other
unacceptable things. If nothing apparent seems to be at fault, the test has passed. A
pass-verdict almost never means that a method of compromising the security does
not exist, as the quality of the test cases and test procedures is based on the selected
test coverage. Also adding more instrumentation and monitoring the system more
closely can reveal uncaught failures with exactly the same set of tests, thus reveal-
ing the vagueness of the used pass-fail criteria.
Fuzzing is one form of robustness testing, and it tries to fulfill the testing require-
ments in negative testing with random or semi-random inputs (often millions of test
6760 Book.indb 18 12/22/17 10:50 AM
1.2 Software Quality 19
Figure 1.5 Different types of security test scenarios and test verdicts.
cases). But more often robustness testing is model-based and optimized, resulting
in better test results and shorter test execution time due to optimized and intelli-
gently targeted tests selected from the infinity of inputs needed in negative testing
(Figure 1.6).
Figure 1.6 Limited input space in positive tests and the infinity of tests in negative testing.
6760 Book.indb 19 12/22/17 10:50 AM
20 Introduction
1.2.4 Structural Testing
Software rarely comes out as it was originally planned (Figure 1.7).15 The differ-
ences between the specification and the implementation are faults (defects, bugs,
vulnerabilities) of various types. A specification defines both positive and negative
requirements. A positive requirement says what the software should do, and a nega-
tive requirement defines what it must not do. The gray area in between leaves some
functionality undefined, open for interpretation. The implementation rarely rep-
resents the specification. The final product implements the acquired functionality,
with some of the planned features present and some of them missing (conformance
faults). In addition to implementing (or not implementing) the positive requirements,
the final software typically implements some features that were defined as negative
requirements (often fatal or critical faults). Creative features implemented during
the software life cycle can either be desired or nondesired in the final product.
Whereas all critical flaws can be considered security-critical, many security prob-
lems also exist inside the set of creative features. One reason for this is that those
features very rarely will be tested even if fuzzing is part of the software development
life cycle. Testing plans are typically built based on a requirements specification.
The reason for a vulnerability is typically a programming mistake or a design flaw.
Typical security-related programming mistakes are very similar in all commu-
nication devices. Some examples include
• Inability to handle invalid lengths and indices;
• Inability to handle out-of-sequence or out-of-state messages;
• Inability to tolerate overflows (large packets or elements);
• Inability to tolerate missing elements or underflows.
Figure 1.7 Specification versus implementation.
15 J. Eronen, and M.Laakso, “A Case for Protocol Dependency,” In Proceedings of the First IEEE Inter-
national Workshop on Critical Infrastructure Protection. Darmstadt, Germany: November 3–4, 2005.
6760 Book.indb 20 12/22/17 10:50 AM
1.2 Software Quality 21
Implementation mistakes can be thought as undesired features. Whereas a
username of eight characters has a feature of identifying users, nine characters can
be used to shut the service down. Implementation flaws are often created due to
vague definitions of how things should be implemented. Security-related flaws are
often created when a programmer is left with too much choice when implementing
a complex feature such as a security mechanism. If the requirements specification
does not define how authentication must exactly be implemented, or what type
of encryption should be used, the programmers become innovative. The result is
almost always devastating.
1.2.5 Functional Testing
In contrast to structural testing disciplines, fuzzing falls into the category of func-
tional testing, which is more interested in how a system behaves in practice rather
than in the components or specifications from which it is built. The system under
test during functional testing can be viewed as a black box, with one or more exter-
nal interfaces available for injecting test cases, but without any other information
available on the internals of the tested system. Having access to information such as
source code, design or implementation specifications, debugging or profiling hooks,
logging output, or details on the state of the system under test or its operational
environment will help in root cause analysis of any problems that are found, but
none of this is strictly necessary. Having any of the above information available turns
the testing process into gray-box testing, which has the potential to benefit from the
best of both worlds in structural as well as functional testing and can sometimes
be recommended for organizations that have access to source code or any other
details of the systems under test. Access to the internals can also be a distraction.
A few good ideas that can be used in conjunction with fuzz testing when source
code is available include focusing code auditing efforts on components or subsystems
in which fuzzing has already revealed some initial flaws (implying that the whole
component or portions of the code around the flaws might be also of similarly poor
quality) or using debuggers, error detectors and profilers to catch more obscure
issues such as memory leaks during fuzz testing.
1.2.6 Code Auditing
Use the source, Luke—if you have it!
Anonymous security expert
Fuzzing is sometimes contrasted with code auditing and other white-box testing
methods. Code auditing looks at the source code of a system in an attempt to dis-
cover defective programming constructs or expressions. This falls into the category
of structural testing, looking at specifications or descriptions of a system in order
to detect errors. While code auditing is another valuable technique in the software
tester’s toolbox, code auditing and fuzzing are really complementary to each other.
Fuzzing focuses on finding some critical defects quickly, and the found errors are
usually very real. Fuzzing can also be performed without understanding the inner
workings of the tested system in detail. Code auditing is usually able to find more
6760 Book.indb 21 12/22/17 10:50 AM
22 Introduction
problems, but it also finds more false positives that need to be manually verified
by an expert before they can be declared real, critical errors. The choice of which
technique fits your purposes and testing goals best is up to you. With unlimited
time and resources, both can be recommended. Neither fuzzing nor code auditing
is able to provably find all possible bugs and defects in a tested system or program,
but both of them are essential parts in building security into your product develop-
ment processes.
1.3 Introduction to Fuzzing
1.3.1 Brief history of Fuzzing
Fuzzing is one technique for negative testing, and negative testing is nothing new in
the quality assurance field. Hardware testing decades ago already contained negative
testing in many forms. The most traditional form of negative testing in hardware
is called fault injection. The term fault injection can actually refer to two different
things. Faults can be injected into the actual product, through mutation testing, that
is, intentionally breaking the product to test the efficiency of the tests. Or the faults
can be injected to data, with the purpose of testing the data-processing capability.
Faults in hardware communication buses typically happen either through random
inputs (i.e., white-noise testing) or by systematically modifying the data (e.g., by
bit-flipping). In hardware, the tests are typically injected through data busses or
directly to the various pins on the chip. Most modern chips contain a test channel,
which will enable modification of not only the external interfaces but injection of
anomalies in the data channels inside the chip.
Some software engineers used fuzzing-like test methods already in the 1980s.
One proof of that is a tool called The Monkey: “The Monkey was a small desk
accessory that used the journaling hooks to feed random events to the current appli-
cation, so the Macintosh seemed to be operated by an incredibly fast, somewhat
angry monkey, banging away at the mouse and keyboard, generating clicks and drags
at random positions with wild abandon.”16 However, in practice, software testing
for security and reliability was in its infancy until the late 1990s. It appeared as if
nobody cared about software quality, as crashes were acceptable and software could
be updated easily. One potential reason for this was that before the availability of
public networks, or the internet, there was no concept of an attacker. The birth of
software security as a research topic was created by widely deployed buffer overflow
attacks such as the Morris Internet Worm in 1988. In parallel to the development
in the software security field, syntax testing was introduced around 1990 by the
quality assurance industry.17 Syntax testing basically consists of model-based test-
ing of protocol interfaces with a grammar. We will explain syntax testing in more
detail in Chapter 3.
A much more simpler form of testing gained more reputation, perhaps due to
the easiness of its implementation. The first (or at least best known) rudimentary
16 From Folklore.org (1983). www.folklore.org/StoryView.py?story=Monkey_Lives.txt.
17 Syntax testing is introduced in the Software Testing Techniques 2nd edition, by Boris Beizer, Inter-
national Thomson Computer Press. 1990.
6760 Book.indb 22 12/22/17 10:50 AM
1.3 Introduction to Fuzzing 23
negative testing project and tool was called Fuzz from Barton Miller’s research group
at the University of Wisconsin, published in 1990.18 Very simply, it tried random
inputs for command line options, looking for locally exploitable security holes. The
researchers repeated the tests every five years, with same depressing results. Almost
all local command-line utilities crashed when provided unexpected inputs, with most
of those flaws exploitable. They described their approach as follows:
There is a rich body of research on program testing and verification. Our
approach is not a substitute for a formal verification or testing procedures,
but rather an inexpensive mechanism to identify bugs and increase overall
system reliability. We are using a coarse notion of correctness in our study.
A program is detected as faulty only if it crashes or hangs (loops indefi-
nitely). Our goal is to complement, not replace, existing test procedures.
While our testing strategy sounds somewhat naive, its ability to discover
fatal program bugs is impressive. If we consider a program to be a complex
finite state machine, then our testing strategy can be thought of as a random
walk through the state space, searching for undefined states.19
history of Fuzzing
1983: The Monkey
1988: The Internet Worm
1989–1991:
• Boris Beizer explains Syntax Testing (similar to robustness testing).
• “Fuzz: An Empirical Study of Reliability . . .” by Miller et al. (Univ. of
Wisconsin)
1995–1996:
• Fuzz revisited by Miller et al. (Univ. of Wisconsin).
• Fault Injection of Solaris by OUSPG (Oulu University, Finland).
1998:
• ISIC fuzzer for IPv4
1999–2001:
• PROTOS tests for: SNMP, HTTP, SIP, H.323, LDAP, WAP, . . .
• Peach fuzzer from Michael Eddington, the most popular fuzzing framework
still in use
• Spike from Dave Aitel
2002:
• Codenomicon launch with GTP, SIP, and TLS robustness testers.
• Click-to-Secure (now Cenzic) Hailstorm web application tester.
18 More information on “Fuzz Testing of Application Reliability” at University of Wisconsin is avail-
able at http://pages.cs.wisc.edu/~bart/fuzz.
19 B. P. Miller, L. Fredriksen, and B. So. “An empirical study of the reliability of Unix utilities.” Com-
munications of the Association for Computing Machinery, 33(12)(1990):32–44.
6760 Book.indb 23 12/22/17 10:50 AM
24 Introduction
• IWL and SimpleSoft SNMP fuzzers (and various other protocol specific
tools).
• SSHredder from Rapid7
2003:
• Open source fuzzers: dfuz, Flayer, Scapy
2005–2006:
• Open source fuzzers: antiparser, autodafe, AxMan, GPF, JBroFuzz,
WSFuzzer
• Commercial fuzzers: beStorm from Beyond Security, Flinder from SEARCH-
LAB, Mu-4000 from MuSecurity (now Spirent)
• Exploratory fuzzing, EFS from Jared DeMott, using feedback loop from
code execution to craft new test sequences
2007:
• Open source fuzzers: ProxyFuzz
• Commercial: FuzzGuru from Microsoft, Achilles from Wurldtech (now GE),
BPS-1000 from BreakingPoint, (now Ixia)
• SAGE from Microsoft Research and CSE, using constraint solvers and
coverage data to generate new tests
• KIF fuzzer explores state diagrams, by Humberto Abdelnur, Olivier Festor,
and Radu State
• In-Memory Fuzz POC by Adam Greene, Michael Sutton and Pedram Amini,
applying mutations inside the process
2008:
• Sulley from Aaron Portnoy and Pedram Amini
• Defensics 3.0 from Codenomicon
2009:
• Traffic Capture Fuzzer from Codenomicon uses protocol dissectors to
model protocols
2010:
• Radamsa from OUSPG using genetic algorithms to dissect protocols and
build protocol models
2014:
• AFL by Michal Zalewski, using compile-time instrumentation and genetic
algorithms to discover new paths in code