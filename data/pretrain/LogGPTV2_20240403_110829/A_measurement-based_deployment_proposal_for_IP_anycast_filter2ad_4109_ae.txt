### 优化后的文本

#### 图11：聚类的flaps及其对总flap数量的贡献
- **观察结果**：存在少量较大的聚类，但大多数flaps属于非常小的聚类。
- **解释**：这表明虽然有一些大型事件影响了多个客户端，但大多数flaps是由局部的小规模事件引起的。

#### 结果
图10显示了观察到相应数量flaps的客户端累积分布函数（CDF）。数据显示：
- 约40%的客户端没有观察到任何flap。
- 约95%的节点每天观察到的flap少于一次（总计少于17次）。
- 约99%的客户端每天观察到的flap少于10次。

总体而言，跟踪数据包含290,814个flaps，其中266,967个（约占总数的92%）由仅58个（约占总数的1%）客户端观察到。因此，除了这一小部分节点外，内部anycast部署似乎提供了非常好的亲和性。

这些anycast flaps可能由于多种事件引起，包括链路、对等或服务器故障，以及AS在多条链路上负载均衡anycast流量。通常，某一事件的影响取决于其位置：靠近anycast服务器或核心ISP的事件会导致大量客户端flap，而接近客户端站点的事件则只会影响少数客户端。基于这一观察，我们使用时间聚类方法将跟踪数据中的flaps构造成事件。假设在同一时间段内发生的flaps可能是由同一路由事件引起的，我们将所有客户端观察到的flaps进行聚类，使得相隔不超过10秒的flaps被归为同一聚类。为了限制聚类大小，我们设定了最大聚类时间为180秒，即相隔超过180秒的flaps不能在同一个聚类中。这种方法类似于文献[14]中使用的BGP更新聚类。聚类结果对最大聚类时间不敏感——使用120秒和240秒的最大聚类时间也得到了类似的结果。

使用这种方法，290,814个flaps产生了22,319个聚类，其中最大的聚类包含1,634个flaps。

图11显示了每个聚类中flap占总flap数量的比例。图中显示了一些中等到大型的聚类，这些聚类对应于服务器附近或核心ISP中的罕见BGP事件。这进一步支持了我们的观点，即IP Anycast与域间路由之间没有有害的相互作用。更重要的是，该图有一个长尾，描绘了大量的小型聚类。这些小型聚类对应于客户端附近的事件。实际上，这些聚类中的大多数都包含了高度不稳定的客户端观察到的flaps。这表明即使是非常小的一部分观察到较差亲和性的客户端也是由于它们附近的事件引起的。

此外，不稳定客户端观察到flaps的频率使我们相信这些客户端是多宿主的，并且正在使用某种动态负载均衡技术跨多个上游提供商。例如，观察到最多flaps的客户端属于AS# 15710，该AS有两个上游提供商——AS# 3356（Level3）和AS# 8928（INTEROUTE）。这个客户端在我们的跟踪中几乎连续地在Cornell和Cambridge的服务器之间flap。为了进一步调查，我们以每秒一次的频率对该客户端进行了两个小时的探测。图12显示了每个探测被路由到的服务器（为了清晰起见，我们只显示了两分钟的数据——其余的数据类似）。图中显示该客户端经历了非常频繁的flaps。

在许多情况下，连续的探测被路由到不同的服务器。鉴于BGP事件发生在较粗的时间尺度上，这种高频率的flaps表明客户端正在进行动态负载均衡。

为了验证我们的假设，我们使用Route-Views [49]和CIDR-Report [41]提供的域间路由视图来确定不稳定客户端的AS级连接情况。58个不稳定客户端属于47个不同的AS，至少有42个这些AS拥有多个上游AS。由于我们无法控制这些客户端，也无法确定客户端AS是否确实跨其上游AS进行负载均衡，我们通过电子邮件调查了客户端AS以确定这是否属实。调查收到了五个回复，尽管所有五个AS都声称在其提供商之间使用某种形式的负载均衡。尽管这些客户端的具体设置需要进一步调查，但手头的所有证据都指向客户端动态负载均衡是导致其观察到较差亲和性的根本原因。

### 客户端负载分布
客户端只需通过向anycast地址发送数据包即可访问IP Anycast部署，路由基础设施负责将数据包传递到其中一个服务器。因此，anycast运营商无法控制每个服务器处理的客户端数量。在本节中，我们研究了内部部署中客户端负载的分布，并评估了控制这种分布的方法。

我们使用第6节中描述的TXT记录查询方法来确定我们列表中所有客户端的分布情况。