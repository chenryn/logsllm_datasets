l
l
f
f
f
f
o
o
#
#
e
e
h
h
t
t
h
h
t
t
i
i
w
w
(
(
1e-02 (3000)
1e-02 (3000)
1e-03 (300)
1e-03 (300)
1e-04 (30)
1e-04 (30)
1e-05 (3)
1e-05 (3)
s
s
p
p
a
a
l
l
f
f
f
f
o
o
.
.
o
o
n
n
l
l
a
a
t
t
o
o
T
T
/
/
r
r
e
e
t
t
s
s
u
u
c
c
l
l
n
n
i
i
s
s
p
p
a
a
F
F
l
l
t
t
 1
 1
 10
 10
 100
 100
 1000
 1000
Clusters (sorted by size)
Clusters (sorted by size)
 10000
 10000
Figure 11: Clustered ﬂaps and their contribution
towards the total number of ﬂaps – there are a small
number of large clusters but a majority of the ﬂaps
belong to very small sized clusters.
Results: Figure 10 shows a CDF for the number of clients
observing the respective number of ﬂaps. The ﬁgure shows
that ∼40% of the clients do not observe any ﬂap, ∼95% of
the nodes observe less than a ﬂap per day (less than 17 ﬂaps
in total) and ∼99% of the clients observe less than 10 ﬂaps
per day. Overall, the trace comprises of 290,814 ﬂaps of
which 266,967 (∼92% of the total) were observed by just 58
(∼1%) clients. Hence, apart from this very small fraction
of nodes, the internal anycast deployment seems to provide
very good aﬃnity.
These anycast ﬂaps can occur due to a variety of events
ranging from link, peering or server failures to ASs load-
balancing the anycast traﬃc across multiple links. Typically,
the impact of a given event depends on its location: events
near an anycast server or in a core ISP would cause a number
of clients to ﬂap while the impact of an event close to a client
site would be restricted to a small number of clients. Given
this observation, we proceeded to use temporal clustering to
construct events out of the ﬂaps seen in our trace. The idea
here is to cluster ﬂaps that occur close by in time on the
assumption that they are probably due to the same routing
event. Hence, we clustered the ﬂaps observed at all the
clients such that ﬂaps within 10 seconds of each other were
in the same cluster. Since ﬂaps in the cluster are assumed to
be due to the same routing event, we limited the maximum
cluster size to 180 seconds, i.e. no two ﬂaps more than 180
seconds apart can be in the same cluster. This is similar
to the BGP update clustering used in [14]. The clustering
results presented here are not very sensitive to the maximum
cluster size – clustering with a maximum cluster size of 120
and 240 seconds yielded similar results. Using this approach,
Figure 12: Probes at a rate of once per second from
an unstable client. Each plotted point in the ﬁg-
ure represents a probe and shows the server it is
routed to – as can be seen, the client ﬂaps very fre-
quently between the anycast servers at Cornell and
Cambridge.
the 290,814 ﬂaps yielded 22,319 clusters with the largest
cluster containing 1,634 ﬂaps.
Figure 11 shows the distribution of the fraction of the
total number of ﬂaps that occur in the cluster. The ﬁgure
has a small number of moderate-to-large clusters and these
correspond to infrequent BGP events near the servers or in
core ISPs. This further buttresses our argument that IP
Anycast does not have any harmful interactions with inter-
domain routing. More importantly, the ﬁgure has a long tail
depicting a very large number of very small clusters. These
correspond to events near clients. As a matter of fact, a
large majority of these clusters comprise of ﬂaps seen at the
highly unstable clients. This points to the fact that even
the very small fraction of clients that observe poor aﬃnity
do so due to events that are close to them.
Further, the frequency at which the unstable clients ob-
serve ﬂaps leads us to believe that these clients are multi-
homed and are using some kind of dynamic load-balancing
across multiple upstream providers. For example, the client
which observed the most ﬂaps belongs to AS# 15710 which,
in turn, has two upstream providers – AS# 3356 (Level3)
and AS# 8928 (INTEROUTE). This client observed almost
continuous ﬂaps between the server at Cornell and Cam-
bridge in our trace. To investigate further, we probed this
client at a rate of once per second for a period of two hours.
Figure 12 shows the server each probe is routed to (for clar-
ity, we show the probes only for a two minute duration – the
rest of the trace is similar). The ﬁgure shows that the client
experiences very frequent ﬂaps.
In many cases, consecu-
tive probes are routed to diﬀerent servers. Given that BGP
events occur at much coarser time scale, this high frequency
of ﬂaps suggests dynamic load balancing by the client.
To validate our conjecture, we used the view of inter-
domain routing available through Route-Views [49] and CID
R-Report [41] to determine the AS-level connectivity of the
unstable clients. The 58 unstable clients belong to 47 dis-
tinct ASs and at least 42 of these have multiple upstream
ASs. Since we do not have control over these clients, and
hence cannot determine if the client ASs are indeed load-
balancing across their upstream ASs, we conducted an e-
mail survey of the client ASs to determine if this is indeed
the case. The survey yielded just ﬁve responses, though all
the ﬁve ASs claimed to be using some form of load balanc-
ing across their provider. While the exact set-up of these
clients begs further investigation, all the evidence at hand
n
n
o
o
i
i
t
t
c
c
a
a
r
r
F
F
d
d
a
a
o
o
L
L
n
n
o
o
i
i
t
t
c
c
a
a
r
r
F
F
d
d
a
a
o
o
L
L
n
n
o
o
i
i
t
t
c
c
a
a
r
r
F
F
d
d
a
a
o
o
L
L
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
Cor.
Cor.
Pit.
Pit.
Sea.
Sea.
Ber.
Ber.
Cam.
Cam.
0 AS-hop
0 AS-hop
1 AS-hop
1 AS-hop
2 AS-hop
2 AS-hop
3 AS-hop
3 AS-hop
4 AS-hop
4 AS-hop
Amount of prepending at the Cornell site
Amount of prepending at the Cornell site
(a) Prepending at Cornell
Cor.
Cor.
Pit.
Pit.
Sea.
Sea.
Ber.
Ber.
Cam.
Cam.
0 AS-hop
0 AS-hop
1 AS-hop
1 AS-hop
2 AS-hop
2 AS-hop
3 AS-hop
3 AS-hop
4 AS-hop
4 AS-hop
Amount of prepending at the Cambridge site
Amount of prepending at the Cambridge site
(b) Prepending at Cambridge
Cor.
Cor.
Pit.
Pit.
Sea.
Sea.
Ber.
Ber.
Cam.
Cam.
0 AS-hop
0 AS-hop
1 AS-hop
1 AS-hop
 at Pit.
 at Pit.
1 AS-hop
1 AS-hop
 at Sea.
 at Sea.
1 AS-hop
1 AS-hop
 at Ber.
 at Ber.
1 AS-hop
1 AS-hop
 at Pit/Sea/Ber
 at Pit/Sea/Ber
(c) Prepending at Pittsburgh/Seattle/Berkeley
Figure 13: Load on the anycast sites of anycast de-
ployment in the default case and with various kinds
of AS path prepending at the sites. Here, Load Frac-
tion for a site is the ratio of the number of clients us-
ing the site to the total number of clients (≈20000).
leads us to conclude that dynamic load-balancing by clients
is the root-cause of the poor aﬃnity observed by them.
8. CLIENT LOAD DISTRIBUTION
Clients access an IP Anycast deployment simply by send-
ing packets to its anycast address and it is the routing infras-
tructure that is responsible for delivering the packets to the
one of the servers. Consequently, anycast operators don’t
have any control over the number of clients that each server
handles. In this section, we study the distribution of client
load across the internal deployment and evaluate means by
which this can be controlled.
We used the TXT-record based querying described in sec-
tion 6 to determine the distribution of all clients in our list