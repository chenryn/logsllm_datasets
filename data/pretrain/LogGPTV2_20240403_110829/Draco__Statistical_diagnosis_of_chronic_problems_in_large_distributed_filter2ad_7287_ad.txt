not present
in the master CDRs that we analyzed, Draco
implicated the application servers that were sending invites
to the policy server. An analysis of the performance logs at
the application server indicated that low response rates were an
additional symptom of the problem. Although in this instance,
Draco did not identify the root-cause, our analysis provided
useful clues to operators to help localize the problem. The
incorporation of more server logs, such as policy server and
router logs, would improve our ability to localize problems.
C. Draco’s performance
Depending on the operating mode, Draco takes 2 to 6
minutes to load input data, and from 16 seconds to over 10
minutes to analyze input data comprised of more than 30
million calls; see Table II for details.
In the initial
implementation,
the tree size was limited
in order to achieve acceptable analysis times. Enabling the
branch-and-bound algorithm described in Section III-C, while
continuing to limit the tree size, resulted in more than a 50%
performance improvement in the analysis time. However, the
branch-and-bound algorithm alone does not provide enough of
a performance gain to allow the restrictions on tree size to be
EXAMPLES OF CHRONICS AT PRODUCTION SYSTEM. DRACO CORRECTLY DIAGNOSED 8 OUT OF 10 INCIDENTS AND RANKED THEM AMONG THE TOP-20
PROBLEMS IDENTIFIED. DRACO ALSO IDENTIFIED ANOMALOUS RESOURCE-USAGE METRICS WHENEVER PERFORMANCE LOGS WERE AVAILABLE.
TABLE I
Examples of problems
Customers use wrong codec to send faxes abroad.
Customer problem causes recurrent blocked calls at IPBE.
Blocked circuit identiﬁcation codes on trunk group.
Problem with customer equipment leads to poor QoS.
Congestion at gateway servers due to high call volumes.
Performance problem at application server.
1.
2.
3.
4.
5.
6.
7. Debug tracing overloads servers during peak trafﬁc.
8.
9.
10. Power outage and unsuccessful failover causes brief outages.
Software problem at control server causes blocked calls
Policy server not responding to invites from application servers.
Type
Conﬁguration (cid:88)
Conﬁguration (cid:88)
Conﬁguration (cid:88)
Conﬁguration (cid:88)
(cid:88)
Contention
(cid:88)
Contention
(cid:88)
Contention
Software bug (cid:88)
Software bug
Power
Diagnosed Resource anomalies
-
-
-
-
CPU/Concurrent sessions
CPU/Memory
CPU
-
Low responses at app. server
-
DRACO’S AVERAGE DATA LOAD TIME, AVERAGE NUMBER OF NODES IN A DIAGNOSIS TREE AND MEAN ANALYSIS TIME TO GENERATE THE TOP 20
TABLE II
DIAGNOSES FOR MORE THAN 30 MILLION CALLS.
Branch & Bound
NO
YES
YES
YES
Mode
Sampling
NO
NO
NO
YES
Restricted
YES
YES
NO
NO
Load Time
374 ± 29sec
374 ± 29sec
374 ± 29sec
120 ± 7sec
Nodes
429 ± 208
12 ± 5
36 ± 20
40 ± 30
Analysis Time
524 ± 128sec
128 ± 53sec
880 ± 124sec
16 ± 6sec
removed; doing so caused the analysis times to exceed those
of the initial implementation. Sampling (at the rate of 1/200 of
successful calls), when used in combination with the branch-
and-bound algorithm, does allow the restrictions on tree size
to be lifted while reducing analysis times to a near interactive
level. The reduction of data load time by more than 60% is
another beneﬁt to the use of sampling.
The problem signatures generated when sampling have a
97% match rate when compared to those generated when all
success data is used. Speciﬁcally, the analysis of several days’
data yielded 220 problem signatures, but only 214 matching
signatures were produced by the analysis using sampled input.
Of the six unmatched signatures, all but one were ranked either
19th or 20th (out of 20); the exception was ranked 13th.
VII. RELATED WORK
Over the past decade, there have been signiﬁcant advances
in tools that exploit statistics and machine learning to diagnose
problems in distributed systems. This list is by no means
exhaustive, but we believe it captures the trends in diagnosis.
This section discusses the contributions of these techniques,
and their shortcomings at diagnosing chronics.
A. End-to-end Tracing
Some diagnostic tools [3], [5], [11], [20] analyze end-to-end
request traces and localize components highly correlated with
failed requests using data clustering [3], [20] or decision trees
[5], [11]. They detect problems that result in changes in the
causal ﬂow of requests [11], [20], performance degradation
[20], or error codes [5]. These techniques have typically been
used to diagnose infrastructural problems, such as database
faults and software bugs (e.g inﬁnite loops and exceptions)
which lead to a marked perturbation of a subset of requests.
In principle, techniques such as decision trees should fare
well at diagnosing both major outages and chronics. However,
decision trees did not fare well at diagnosing chronics when we
applied them to our dataset. The decision tree’s bias towards
building short trees led to the pruning of relevant features when
diagnosing problems due to complex triggers. In addition,
the small number of calls affected by chronics coupled with
the presence of multiple independent chronics degraded the
performance of the decision tree.
B. Signature-based
Signature-based diagnosis tools [4], [6], [7] allow system
administrators to identify recurrent problems from a database
of known problems.
These techniques typically rely on
Service-Level Objectives (SLOs) to identify periods of time
where the system was behaving abnormally, and apply ma-
chine learning algorithms to determine which resource-usage
metrics are most correlated with the anomalous periods. These
techniques can diagnose problems due to complex triggers by
localizing the problem to a small set of metrics. However, they
do not address multiple independent problems as they assume
that a single problem occurs at a given instance of time.
Chronic conditions might also go undetected by the SLOs
because they are not severe enough to violate the thresholds.
C. Graph-theoretic
Graph-theoretic techiques analyze communication patterns
across processes to track the probability that errors [2], [9],
or successes (e.g., probes [18]) propagate through the system.
Sherlock [2] builds models of node behavior, and diagnoses
problems by computing the probability that errors propagate
from a set of possible root-cause nodes. NetMedic [9] uses
a statistical approach to diagnose propagating problems in
enterprise systems. These techniques can detect multiple
independent problems—ranking them by likelihood of occur-
rence. However, they do not address problems due to complex
triggers as they assume that the root-cause of the problem
stems from a single component. In addition, since chronics do
not severely perturb system performance they can be included
in the proﬁles of normal behavior learned from historical
data—causing chronics to go undetected.
D. Event correlation
Event correlation has been used to discover causal rela-
tionships between alarms across components in supercomput-
ers [17], IPTV networks [15], and enterprise networks [22].
These techniques support diagnosis of multiple independent
problems, and might be applicable in our system when there
are resource-contention problems due to overload within the
service provider’s network. However, most of the chronics
we have observed are due to customer-site problem such
as misconﬁgurations, and operators at the ISP lack access
to customer-site data other than names of the customer—
therefore event-correlation might not be possible. Draco local-
izes these chronics by analyzing data that is causally-related
with each call rather than alarm signals across the network.
VIII. CONCLUSION
This work introduces chronics—small problems in large
distributed systems that signiﬁcantly degrade user experience
because they persist undiagnosed for lengthy periods of time—
and describes Draco, a diagnosis engine that identiﬁes and
localizes them. We showed through examples of real chronics
in the VoIP platform of a major ISP why they are notoriously
difﬁcult to diagnose: their small size makes setting alarm
thresholds tricky, there are many of them active concurrently
even when the system as a whole is mostly functional, their
symptoms often overlap with each other, they are triggered by
complex corner cases involving multiple conditions, and they
they persist for lengthy periods and can get absorbed into the
system’s deﬁnition of what is normal.
Draco addresses these issues through a variety of tech-
niques: a) using top-down diagnosis starting with abnormal
user interactions to identify failures rather than relying on
bottom-up alarms based on server logs, b) statistically iden-
tifying root-causes by comparing bad interactions with good
ones from the same interval of time rather than relying on
thresholds or on historical data from good intervals of time,
c) a branch-and-bound procedure to identify complex triggers
comprised of conjunctions of multiple attributes, and d) greedy
ﬁltering of failures explained by already identiﬁed problems
to discover additional concurrent problems.
Draco has been deployed on a major VoIP platform serving
millions of users and handling tens of millions of calls a
day, and is being successfully used by its operations team.
We provide examples of real chronics that Draco has helped
identify, and through injection of synthetic failures on a dataset
obtained from the production system, have shown that for
datasets with tens of million of calls, it can provide coverage
levels as high as 97% with false positives as low as 4%,
and can do so while providing near-interactive performance of
< 1 second per chronic all while running on a single server
machine with middle of the range hardware.
REFERENCES
[1] M. K. Agarwal, M. Gupta, V. Mann, N. Sachindran, N. Anerousis,
and L. B. Mummert. Problem determination in enterprise middleware
systems using change point correlation of time series data. In NOMS,
pages 471–482, Vancouver, Canada, April 2006.
[2] P. Bahl, R. Chandra, A. G. Greenberg, S. Kandula, D. A. Maltz, and
M. Zhang. Towards highly reliable enterprise network services via
In SIGCOMM, pages 13–24,
inference of multi-level dependencies.
Kyoto, Japan, August 2007.
[3] P. Barham, A. Donnelly, R. Isaacs, and R. Mortier. Using Magpie for
request extraction and workload modelling. In OSDI, pages 259–272,
San Francisco, CA, December 2004.
[4] P. Bodik, M. Goldszmidt, A. Fox, D. B. Woodard, and H. Andersen.
Fingerprinting the datacenter: automated classiﬁcation of performance
crises. In EuroSys, pages 111–124, Paris, France, April 2010.
[5] M. Chen, A. X. Zheng, J. Lloyd, M. I. Jordan, and E. Brewer. Failure
diagnosis using decision trees. In ICAC, pages 36–43, New York, NY,
May 2004.
[6] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly, and A. Fox.
Capturing, indexing, clustering, and retrieving system history. In SOSP,
pages 105–118, Brighton, United Kingdom, October 2005.
[7] S. Duan and S. Babu. Guided problem diagnosis through active learning.
In ICAC, pages 45–54, Chicago, IL, June 2008.
[8] FCC. The Proposed Extension of Part 4 of the Commissions Rules
Regarding Outage Reporting To Interconnected Voice Over Internet
Protocol Service Providers and Broadband Internet Service Providers.
Technical Report PS Docket No. 11-82, FCC 12-22, Federal Commu-
nications Commission, February 2012.
[9] S. Kandula, R. Mahajan, P. Verkaik, S. Agarwal, J. Padhye, and P. Bahl.
Detailed diagnosis in enterprise networks. In SIGCOMM, pages 243–
254, Barcelona, Spain, August 2009.
[10] S. Kavulya, K. R. Joshi, M. A. Hiltunen, S. Daniels, R. Gandhi, and
P. Narasimhan. Practical experiences with chronics discovery in large
telecommunications systems. Operating Systems Review, 45(3):23–30,
2011.
[11] E. Kiciman and A. Fox.
failures in
component-based internet services. IEEE Trans. on Neural Networks:
Special Issue on Adaptive Learning Systems in Communication Net-
works, 16(5):1027– 1041, September 2005.
Detecting application-level
[12] S. Kullback and R. A. Leibler. On information and sufﬁciency. The
Annals of Mathematical Statistics, 22:79–86, March 1951.
[13] A. H. Land and A. G. Doig. An automatic method of solving discrete
programming problems. Econometrica, 28(3):497–520, 1960.
[14] C. Liu, Z. Lian, and J. Han. How Bayesians debug. In ICDM, pages
382–393, Hong Kong, China, December 2006.
[15] A. A. Mahimkar, Z. Ge, A. Shaikh, J. Wang, J. Yates, Y. Zhang, and
Q. Zhao. Towards automated performance diagnosis in a large IPTV
network. In SIGCOMM, pages 231–242, Barcelona, Spain, August 2009.
[16] H. B. Mann and D. R. Whitney. On a test of whether one of two
random variables is stochastically larger than the other. The Annals of
Mathematical Statistics, 18(1):50–60, 1947.
[17] A. J. Oliner, A. V. Kulkarni, and A. Aiken. Using correlated surprise to
infer shared inﬂuence. In DSN, pages 191–200, Chicago, IL, July 2010.
[18] I. Rish, M. Brodie, N. Odintsova, S. Ma, and G. Grabarnik. Real-time
problem determination in distributed systems using active probing. In
NOMS, pages 133–146, Seoul, South Korea, April 2004.
[19] RuleQuest Research Data Mining Tools. See5/C5.0, 2011. http://www.
rulequest.com/.
[20] R. R. Sambasivan, A. X. Zheng, M. D. Rosa, E. Krevat, S. Whitman,
M. Stroucken, W. Wang, L. Xu, and G. R. Ganger. Diagnosing
performance changes by comparing request ﬂows. In NSDI, pages 43–
56, Boston, MA, March 2011.
[21] G. M. Weiss and F. J. Provost. Learning when training data are costly:
The effect of class distribution on tree induction. Journal of Artiﬁcial
Intelligence Research (JAIR), 19:315–354, 2003.
[22] S. A. Yemini, S. Kliger, E. Mozes, Y. Yemini, and D. Ohsie. High
speed and robust event correlation. Communications Magazine, IEEE,
34(5):82–90, May 1996.