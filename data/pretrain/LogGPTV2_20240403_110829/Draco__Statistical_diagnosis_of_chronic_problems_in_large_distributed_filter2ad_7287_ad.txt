### Analysis of Draco's Performance and Problem Localization

#### Problem Localization
In the master Call Detail Records (CDRs) we analyzed, Draco identified the application servers as the source sending invites to the policy server. An analysis of the performance logs from these application servers indicated that low response rates were an additional symptom of the problem. Although Draco did not identify the root cause in this instance, our analysis provided valuable clues to operators for localizing the issue. Incorporating more server logs, such as those from the policy server and routers, would enhance our ability to pinpoint problems.

#### Draco’s Performance
Depending on the operating mode, Draco takes between 2 to 6 minutes to load input data and from 16 seconds to over 10 minutes to analyze input data consisting of more than 30 million calls. For detailed performance metrics, refer to Table II.

In the initial implementation, the tree size was limited to achieve acceptable analysis times. Enabling the branch-and-bound algorithm described in Section III-C, while still limiting the tree size, resulted in a more than 50% improvement in analysis time. However, the branch-and-bound algorithm alone does not provide sufficient performance gains to remove the restrictions on tree size. Sampling (at a rate of 1/200 of successful calls), when used in combination with the branch-and-bound algorithm, allows the tree size restrictions to be lifted while reducing analysis times to near-interactive levels. This approach also reduces data load time by more than 60%.

The problem signatures generated using sampling have a 97% match rate compared to those generated using all success data. Specifically, the analysis of several days' data yielded 220 problem signatures, but only 214 matching signatures were produced by the sampled input. Of the six unmatched signatures, all but one were ranked either 19th or 20th (out of 20); the exception was ranked 13th.

### Examples of Chronics in Production Systems
Draco correctly diagnosed 8 out of 10 incidents and ranked them among the top-20 problems identified. Draco also identified anomalous resource-usage metrics whenever performance logs were available.

**Table I: Examples of Problems**
1. Customers use the wrong codec to send faxes abroad.
2. Customer problem causes recurrent blocked calls at IPBE.
3. Blocked circuit identification codes on trunk group.
4. Problem with customer equipment leads to poor QoS.
5. Congestion at gateway servers due to high call volumes.
6. Performance problem at application server.
7. Debug tracing overloads servers during peak traffic.
8. Power outage and unsuccessful failover causes brief outages.
9. Software problem at control server causes blocked calls.
10. Policy server not responding to invites from application servers.

| Type | Diagnosed Resource Anomalies |
|------|-------------------------------|
| Configuration (cid:88) | - |
| Configuration (cid:88) | - |
| Configuration (cid:88) | - |
| Configuration (cid:88) | - |
| Contention (cid:88) | CPU/Concurrent sessions |
| Contention (cid:88) | CPU/Memory |
| Contention (cid:88) | CPU |
| Software bug (cid:88) | - |
| Software bug (cid:88) | Low responses at app. server |
| Power (cid:88) | - |

**Table II: Draco’s Average Data Load Time, Average Number of Nodes in a Diagnosis Tree, and Mean Analysis Time to Generate the Top 20 Diagnoses for More Than 30 Million Calls**

| Branch & Bound | Mode | Sampling | Restricted | Load Time | Nodes | Analysis Time |
|----------------|------|----------|------------|-----------|-------|---------------|
| NO             | NO   | NO       | YES        | 374 ± 29sec | 429 ± 208 | 524 ± 128sec  |
| YES            | NO   | NO       | YES        | 374 ± 29sec | 12 ± 5    | 128 ± 53sec   |
| YES            | NO   | NO       | NO         | 374 ± 29sec | 36 ± 20   | 880 ± 124sec  |
| YES            | YES  | YES      | NO         | 120 ± 7sec  | 40 ± 30   | 16 ± 6sec     |

### Related Work
Over the past decade, there have been significant advances in tools that exploit statistics and machine learning to diagnose problems in distributed systems. This section discusses the contributions and shortcomings of these techniques in diagnosing chronic issues.

#### A. End-to-end Tracing
Some diagnostic tools [3], [5], [11], [20] analyze end-to-end request traces and localize components highly correlated with failed requests using data clustering [3], [20] or decision trees [5], [11]. These techniques detect problems resulting in changes in the causal flow of requests [11], [20], performance degradation [20], or error codes [5]. While effective for diagnosing infrastructural problems like database faults and software bugs, decision trees did not perform well in diagnosing chronics in our dataset due to their bias towards building short trees and the small number of calls affected by chronics.

#### B. Signature-based
Signature-based diagnosis tools [4], [6], [7] allow system administrators to identify recurrent problems from a database of known issues. These tools typically rely on Service-Level Objectives (SLOs) to identify periods of abnormal behavior and apply machine learning algorithms to determine which resource-usage metrics are most correlated with these periods. While they can diagnose complex triggers, they do not address multiple independent problems and may miss chronic conditions that do not violate SLO thresholds.

#### C. Graph-theoretic
Graph-theoretic techniques [2], [9] analyze communication patterns across processes to track the probability that errors or successes propagate through the system. Tools like Sherlock [2] and NetMedic [9] build models of node behavior and diagnose problems by computing the probability of error propagation. These techniques can detect multiple independent problems but assume a single root-cause component and may include chronic conditions in profiles of normal behavior, causing them to go undetected.

#### D. Event Correlation
Event correlation has been used to discover causal relationships between alarms in supercomputers [17], IPTV networks [15], and enterprise networks [22]. These techniques support the diagnosis of multiple independent problems and might be applicable in our system for resource-contention issues. However, most chronics observed are due to customer-site misconfigurations, and ISP operators lack access to customer-site data, making event correlation challenging.

### Conclusion
This work introduces "chronics" — small, persistent problems in large distributed systems that significantly degrade user experience. We describe Draco, a diagnosis engine that identifies and localizes these issues. Through real-world examples in a major VoIP platform, we show why chronics are difficult to diagnose: their small size, concurrent activity, overlapping symptoms, complex triggers, and long persistence. Draco addresses these challenges using top-down diagnosis, statistical root-cause identification, branch-and-bound procedures, and greedy filtering. Deployed on a major VoIP platform, Draco provides high coverage and low false positives, delivering near-interactive performance on a single server.

### References
[References listed as provided in the original text]

This optimized version aims to improve clarity, coherence, and professionalism while maintaining the essential content and structure of the original text.