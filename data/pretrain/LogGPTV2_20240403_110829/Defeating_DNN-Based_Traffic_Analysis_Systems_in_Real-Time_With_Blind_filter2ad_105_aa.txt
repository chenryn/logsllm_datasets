title:Defeating DNN-Based Traffic Analysis Systems in Real-Time With Blind
Adversarial Perturbations
author:Milad Nasr and
Alireza Bahramali and
Amir Houmansadr
Defeating DNN-Based Traffic Analysis Systems in 
Real-Time With Blind Adversarial Perturbations
Milad Nasr, Alireza Bahramali, and Amir Houmansadr, 
University of Massachusetts Amherst
https://www.usenix.org/conference/usenixsecurity21/presentation/nasr
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Defeating DNN-Based Trafﬁc Analysis Systems in Real-Time With
Blind Adversarial Perturbations
Milad Nasr
Alireza Bahramali
University of Massachusetts Amherst
{milad, abahramali, amir}@cs.umass.edu
Amir Houmansadr
Abstract
1
Introduction
Deep neural networks (DNNs) are commonly used for var-
ious trafﬁc analysis problems, such as website ﬁngerprinting
and ﬂow correlation, as they outperform traditional (e.g., sta-
tistical) techniques by large margins. However, deep neural
networks are known to be vulnerable to adversarial examples:
adversarial inputs to the model that get labeled incorrectly
by the model due to small adversarial perturbations. In this
paper, for the ﬁrst time, we show that an adversary can defeat
DNN-based trafﬁc analysis techniques by applying adversar-
ial perturbations on the patterns of live network trafﬁc.
Applying adversarial perturbations (examples) on trafﬁc
analysis classiﬁers faces two major challenges. First, the per-
turbing party (i.e., the adversary) should be able to apply the
adversarial network perturbations on live trafﬁc, with no need
to buffering trafﬁc or having some prior knowledge about up-
coming network packets. We design a systematic approach to
create adversarial perturbations that are independent of their
target network connections, and therefore can be applied in
real-time on live trafﬁc. We therefore call such adversarial
perturbations blind.
Second, unlike image classiﬁcation applications, perturbing
trafﬁc features is not straight-forward as this needs to be done
while preserving the correctness of dependent trafﬁc features.
We address this challenge by introducing remapping functions
that we use to enforce different network constraints while
creating blind adversarial perturbations.
Our blind adversarial perturbations algorithm is generic
and can be applied on various types of trafﬁc classiﬁers. We
demonstrate this by implementing a Tor pluggable transport
that applies adversarial perturbations on live Tor connections
to defeat DNN-based website ﬁngerprinting and ﬂow correla-
tion techniques, the two most-studied types of trafﬁc analysis.
We show that our blind adversarial perturbations are even
transferable between different models and architectures, so
they can be applied by blackbox adversaries. Finally, we show
that existing countermeasures perform poorly against blind
adversarial perturbations, therefore, we introduce a tailored
countermeasure.
Trafﬁc analysis is the art of inferring sensitive information
from the patterns of network trafﬁc (as opposed to packet
contents), in particular, packet timings and sizes. Trafﬁc anal-
ysis is useful in scenarios where network trafﬁc is encrypted,
since encryption does not signiﬁcantly modify trafﬁc patterns.
In particular, previous work has studied trafﬁc analysis al-
gorithms that either compromise the privacy of encrypted
trafﬁc (e.g., by linking anonymous communications [37, 50])
or enhance its security by ﬁngerprinting malicious, obfuscated
connections (e.g., stepping stone attacks [23, 37, 63]).
Recent advances in trafﬁc analysis leverage deep neural net-
works (DNNs) to design classiﬁers that are signiﬁcantly (in
some cases, orders of magnitude) more efﬁcient and more re-
liable than traditional trafﬁc analysis techniques. In particular,
the recent website ﬁngerprinting work of Deep Fingerprint-
ing [50] outperforms all prior ﬁngerprinting techniques in
classifying webpages, and the DeepCorr [37] ﬂow correlation
technique is able to link anonymized trafﬁc ﬂows with accura-
cies two orders of magnitude superior to prior ﬂow correlation
techniques. Given the increasing use of DNNs in trafﬁc analy-
sis applications, we ask ourselves the following question: can
DNN-based trafﬁc analysis techniques get defeated through
adversarially perturbing —live—trafﬁc patterns?
Note that adversarial perturbations is an active area of re-
search in various image processing applications [10, 14, 18,
22, 31, 35, 36, 45, 54] (referred to as adversarial examples).
However, applying adversarial perturbations on network traf-
ﬁc is not trivial, as it faces two major challenges. First, the
perturbing entity, i.e., the adversary,1 should be able to apply
his adversarial perturbations on live network trafﬁc, without
buffering the target trafﬁc or knowing the patterns of upcom-
ing network packets. This is because in most trafﬁc analysis
applications, as will be introduced, the adversary can not inﬂu-
ence the generation of target trafﬁc, but he can only intercept
the packets of the target trafﬁc and perturb them on the ﬂy.
1In our context, the adversary is not necessarily a malicious party; it is
the entity who aims to defeat the underlying DNN trafﬁc classiﬁers.
USENIX Association
30th USENIX Security Symposium    2705
In this paper, we are the ﬁrst to design techniques that ad-
versarially perturb live network trafﬁc to defeat DNN-based
trafﬁc classiﬁers; we call our approach blind adversarial per-
turbations. Our technique applies adversarial perturbations
on live packets as they appear on the wire. The key idea of our
adversarial perturbations algorithm is that it generates “blind”
perturbations that are independent of the target inputs2 by
solving speciﬁc optimization problems. We design adversar-
ial perturbation mechanisms for the key features commonly
used in trafﬁc analysis applications: our adversarial perturba-
tions include changing the timings and sizes of packets, as
well as inserting dummy network packets.
The second challenge to applying adversarial perturbations
on trafﬁc analysis applications is that, any perturbation mecha-
nism on network trafﬁc should preserve various constraints of
trafﬁc patterns, e.g., the dependencies between different trafﬁc
features, the statistical distribution of timings/sizes expected
from the underlying protocol, etc. This is unlike traditional
adversarial example studies (in the context of image process-
ing) that modify image pixel values individually. Therefore,
one can not simply borrow techniques from traditional adver-
sarial examples. We consequently design various remapping
functions and regularizers, that we incorporate into our op-
timization problem to enforce such network constraints. As
will be shown, in most scenarios the constraints are not dif-
ferentiable, and therefore we carefully craft custom gradient
functions to approximate their gradients.
Evaluations: Our blind adversarial perturbations algorithm
is generic and can be applied to various types of trafﬁc classi-
ﬁers. We demonstrate this by implementing our techniques as
a Tor pluggable transport [46], called BLANKET, and evalu-
ating it on state-of-the-art website ﬁngerprinting [3, 50] and
ﬂow correlation [37] techniques, the two most-studied types
of trafﬁc analysis. Our evaluations show that our adversarial
perturbations can effectively defeat DNN-based trafﬁc analy-
sis techniques through small, live adversarial perturbations.
For instance, our perturbations can reduce the accuracy of
state-of-the-art website ﬁngerprinting [3, 50] works by 90%
by only adding 10% bandwidth overhead. Also, our adver-
sarial perturbations can reduce the true positive rate of state-
of-the-art ﬂow correlation techniques [37] from 0.9 to 0.3 by
applying tiny delays with a 50ms jitter standard deviation.
We also show that our blind adversarial perturbations
are transferable between different models and architectures,
which signiﬁes their practical importance as they can be im-
plemented by blackbox adversaries.
Countermeasures: We conclude by studying various coun-
termeasures against our adversarial perturbations. We start
by leveraging existing defenses against adversarial examples
from the image classiﬁcation literature and adapting them
2Our technique is blind about the target network connections that it per-
turbs, but it may need to learn some generic constraints of the underlying
network protocol (like the noise model and sizing distributions) in order to
train its perturbation models ofﬂine, e.g., using sample network ﬂows.
to the trafﬁc analysis scenario. We show that such adapted
defenses are not effective against our network adversarial
perturbations as they do not take into account the speciﬁc
constraints of trafﬁc features. Motivated by this, we design
a tailored countermeasure for our network adversarial per-
turbations, which we demonstrate to be more effective than
the adapted defenses. The key idea of our countermeasure
is performing adversarial training, and using our attack as a
regularizer to train robust trafﬁc analysis models.
2 Preliminaries
2.1 Problem Statement
Trafﬁc analysis is to infer sensitive information from the
patterns of network trafﬁc, i.e., packet timings and sizes.
Therefore, many works have investigated the use of trafﬁc
analysis in various scenarios where trafﬁc contents are en-
crypted. In particular, trafﬁc analysis has been used to com-
promise anonymity in anonymous communications systems
through various types of attacks, speciﬁcally, website ﬁnger-
printing [3, 6, 19, 27, 40, 41, 47, 50, 51, 57–60], and ﬂow corre-
lation [12,23,24,33,37,38,38,49,53,64]. Trafﬁc analysis has
also been used to trace back cybercriminals who obfuscate
their identiﬁes through stepping stone relays [23, 24, 37, 63].
Our problem: Defeating DNN-based trafﬁc analysis algo-
rithms. The state-of-the-art trafﬁc analysis techniques use
deep neural networks to offer much higher performances than
prior techniques. For instance, DeepCorr [37] provides a ﬂow
correlation accuracy of 96% compared to 4% of statistical-
based systems like RAPTOR [53] (in a given setting). Also,
Var-CNN [3] leverages deep learning techniques to perform a
website ﬁngerprinting attack which achieves 98% accuracy
in a closed-world setting. However, deep learning models are
infamous for being susceptible to various adversarial attacks
where the adversary adds small perturbations to the inputs to
mislead the deep learning model. Such techniques are known
as adversarial examples in the context of image processing,
but have not been investigated in the trafﬁc analysis domain.
In this work, we study the possibility of defeating DNN-based
trafﬁc analysis techniques through adversarial perturbations.
In our setting, some trafﬁc analysis parties use DNN-
based trafﬁc analysis techniques for various purposes, such as
breaking Tor’s anonymity or detecting cybercriminals. On the
other hand, the trafﬁc analysis adversary(ies) aim at inter-
fering with the trafﬁc analysis process through adversarially
perturbing trafﬁc patterns of the connections they intercept.
To do so, the trafﬁc analysis adversary(ies) perturb the trafﬁc
patterns of the intercepted ﬂows to reduce the accuracy of the
DNN-based classiﬁers used by the trafﬁc analysis parties. To
further clarify the distinction between the players, in the ﬂow
correlation setting, the trafﬁc analysis “party” can be a mali-
cious ISP who aims at deanonymizing Tor users by analyzing
their Tor connections; however, the trafﬁc analysis “adversary”
2706    30th USENIX Security Symposium
USENIX Association
can be some (benign) Tor relays who perturb trafﬁc patterns
of their connections to defeat potential trafﬁc analysis attacks.
Challenges: Note that our problem resembles the setting
of adversarial examples for image classiﬁcation. However,
applying adversarial perturbations on network trafﬁc presents
two major challenges. First, the adversaries should be able to
apply adversarial perturbations on live network connections
where the patterns of upcoming network packets are unknown
to the adversaries. This is because in trafﬁc analysis appli-
cations, the adversary is not in charge of generating trafﬁc
patterns. For instance, in the ﬂow correlation scenario, the traf-
ﬁc analysis adversary is a benign Tor relay who intercepts and
(slightly) perturbs the trafﬁc generated by Tor users. The sec-
ond challenge to applying network adversarial perturbations
is that they should preserve the various constraints of network
trafﬁc, e.g., the dependencies of different trafﬁc features.
Sketch of our approach: In this work, we design blind ad-
versarial perturbations, a set of techniques to perform adver-
sarial network perturbations that overcome the two mentioned
challenges. To address the ﬁrst challenge (applying on live
trafﬁc), we design blind perturbation vectors that are indepen-
dent of their target connections, therefore, they can be applied
on any (unknown) network ﬂows. Figure 1 shows what is
needed by our blind adversary compared to traditional (non-
blind) perturbation techniques. Note that, the blind adversary
may still need to know some generic information about its
target network protocol (like the typical noise model, the
distribution of typical packet sizes, etc.) as well as ﬂow sam-
ples from the same underlying distribution (e.g., sample Tor
ﬂows), but she does not need to know the actual trafﬁc packets
that will arrive on the target connection to be perturbed. We
generate such blind adversarial perturbations by solving a spe-
ciﬁc optimization problem. We address the second challenge
(enforcing network constraints) by using various remapping
functions and regularizers that adjust perturbed trafﬁc fea-
tures to follow the required constraints. Depending on the
application, our perturbation technique may need to be de-
ployed on multiple end-points, e.g., our BLANKET technique
(Section 6.5) needs to be run on a Tor client and its corre-
sponding Tor bridge, which use an out-of-band channel to
exchange some parameters needed to collaboratively generate
perturbations.
2.2 Threat Model
Our use of adversarial perturbations aim at defending “DNN-
based” trafﬁc analysis mechanisms only; therefore, non-DNN
trafﬁc analysis techniques, e.g., ﬂow watermarks [23–25] and
volume-based trafﬁc classiﬁers [4], are out of our scope. Fu-
ture work can look into combining our defense with defenses
against such non-DNN mechanisms. Also, our work only
considers DNN-based trafﬁc analysis techniques that use
trafﬁc patterns (i.e., packet timing, sizes, and directions) for
classiﬁcation, but not those that use packet contents. Such
Figure 1: Unlike traditional adversarial perturbation tech-
niques, our blind perturbation approach does not need to know
the features of upcoming packets.
pattern-based trafﬁc analysis techniques (which are com-
monly [3,6,37,53,60,61] referred to as just trafﬁc analysis) are
increasingly popular and relevant as they work on encrypted
network trafﬁc. Therefore, malware classiﬁers that use packet
content signatures are out of our scope Our adversarial pertur-
bation techniques can be applied to any (pattern-based) trafﬁc
analysis technique that uses raw trafﬁc features for its analy-
sis, e.g., packet timings, inter-packet delays, directions, trafﬁc
volumes, packet counts, etc. This includes the majority of
pattern-based trafﬁc analysis systems [2,3,6,37,38,53,60,61].
On the other hand, our techniques may not be trivially ap-
plied on trafﬁc analysis algorithms that use non-differentiable
and irreversible functions of trafﬁc features, like the hash of
timings or entropy of packet contents; his represents a very
small class of trafﬁc analysis algorithms. Applying our tech-
niques to such systems requires one to come up with speciﬁc
remapping functions or approximated gradient functions.
2.3 Adversary Model
Adversary’s knowledge of the target trafﬁc. We assume
the adversary has no prior knowledge about the patterns of
upcoming network packets of the target connections to be
perturbed. However, the adversary may need to know some
generic statistical information about its target network proto-
col (e.g., the distribution of jitter), as well as the speciﬁcations
of the target protocol (e.g., the format of Tor packets); such
information is needed to ensure the applied perturbations are
statistically and semantically undetectable.