PAGERANK [6], SimRank [17] and WLK [28]. We report the asso-
ciated average model AUC over 5 runs, including the model AUC
tested on clean graph data and model AUC tested on attacked graph
data.
Note that, to be comparable, we trained SEAL purely using graph
structure features (the node information matrix contains structural
node labels only as did in [34]). Then we mount the adversarial
attacks using our efficient design ‚Äî OGSP. As shown in Table 7, the
performance of most existing heuristics is even worse than random
guessing (the model AUC is less than 0.5), indicating that our at-
tacks can be readily transferred to several existing heuristics in the
(a) USAir.
(b) NS.
(c) Celegans.
(d) PB.
Figure 5: Link Prediction Margin: GGSP vs. OGSP.
To analyze the performance of our attack with respect to the
adversary capability, we run four sets of experiments when |ùëâùë†| are
in different settings for each dataset. We set the number of nodes
that the adversary is capable of manipulating as 25%, 50%, 75% and
100% of the entire node set, respectively. For each run, the node set
ùëâùë† is selected randomly. As shown in Fig. 5, as |ùëâùë†| becomes larger,
Table 7: Transferability (AUC)
SEAL CN Jaccard PA AA RA Katz PAGERANK SimRank WLK
0.902 0.843 0.943 0.947 0.910
0.346 0.827 0.591 0.246 0.780
Clean Graph 0.967 0.926
Attacked Graph 0.002 0.139
Clean Graph 0.987 0.934
0.934 0.631 0.934 0.934 0.934
Attacked Graph 0.000 0.000 0.000 0.178 0.000 0.000 0.000
0.782 0.745 0.849 0.853 0.849
Clean Graph 0.872 0.834
Attacked Graph 0.233 0.263
0.094 0.703 0.187 0.152 0.300
Clean Graph 0.940 0.908
Attacked Graph 0.277 0.564
0.859 0.899 0.913 0.916 0.919
0.151 0.881 0.510 0.400 0.590
0.920
0.187
0.934
0.100
0.881
0.367
0.934
0.603
0.790
0.083
0.935
0.170
0.751
0.190
0.766
0.082
0.960
0.517
0.982
0.467
0.880
0.508
0.929
0.500
Data
USAir
NS
Celegans
PB
adversarial examples to the same output labels [18]. This raises the
idea of adding adversarial examples into the training set while we
are training the model, which we leave for our future work.
(a) Clean graph: ùëê = 0
(b) Attacked graph: ùëê = 1
Figure 6: Adversarial Example from NS: given two target
nodes (in grey) randomly selected from NS, the link in
between is un-observed (a) its initial 1-hop subgraph was
predicted as ‚Äònon-existed‚Äô by SEAL; (b) its 1-hop subgraph
after perturbation (just one edge perturbation ‚Äî blue line ‚Äî
in this case) was predicted as ‚Äòexisted‚Äô by SEAL.
literature. Noticeably, among all the datasets, the mounted attacks
perform extremely well for NS. We can observe that the mounted
attacks failed transfer to preference attachment. We suspect that is
because the heuristic used by preference attachment only considers
individual node degree of the target node and does not contain any
neighbour information, which is not consistent with the typical
link formation mechanism we considered.
Defense against attacks. The mounted attacks violate the funda-
mental assumptions used for link predictions, i.e., the Œ•-decaying
theory and link formation mechanism, it is very hard to completely
eliminate the problem. As often applied in the image domain, ad-
versarial training would force the model to assign both clean and
6 RELATED WORK
Adversarial attacks on link prediction: Zhou et al. investigated the
problem of mounting attacks on several heuristics for link predic-
tion. In this work, they further categorized the heuristics based
on the maximum hop of neighbours needed to calculate the sim-
ilarity score. For each category, they proposed associated attack
approaches via deleting edges only. Chen et al. proposed an itera-
tive gradient attack against graph auto-encoder (GAE)-based link
prediction [10]. In contrast, our attacks are mounted based on SEAL
under ‚Äúunnoticeability" constraint and are more general, as they
perform well in several link prediction heuristics.
Neural networks for link prediction: Weisfeiler-Lehman Neural
Machine (WLNM) proposed by Zhang et al. is the first attempt to
employ DNNs for link-prediction tasks[33]. In particular, it learns
general graph structure features by encoding enclosing subgraphs
of the training links into fixed-size adjacency matrices and trains
a fully-connected neural network on the adjacency matrices. As
the typical neural networks only accept fixed-size tensor, WLNM
can only learn the link local patterns from the subgraphs with
fixed-size.
Following this work, Zhang et al. proposed the SEAL framework
for link prediction using graph neural network, called SEAL. It has
been shown its state-of-the-art prediction performance empirically
and theoretically. In particular, a Œ•-decaying theory has been devel-
oped to unify a wide range of existing heuristics for this task, which
proved that local subgraphs are informative for link prediction.
Adversarial attacks on machine learning: Huang et al. categorized
attacks in adversarial machine learning as either causative or eva-
sion, with the former poisoning the training dataset and the latter
evading the pre-trained model by crafting adversarial examples
[16]. Following the terminology of Huang et at. , our work focuses
on the evasion attacks that aim to create adversarial examples de-
liberately to mislead the state-of-the-art link prediction framework
‚Äî SEAL, yet still preserving its unnoticeability.
(cid:1176)(cid:1176)Adversarial attacks have been shown their capability of sig-
nificantly degrading the performance of deep learning models in
various application domains, e.g., image [8], audio [9] and malware
detection [15]. These applications consider the data instances to be
independent and classify an instance based on features extracted
from only that instance. This directly enables evasion techniques
such as gradient descent/ascent directly in the feature space. For
complex network graph as we considered in this paper, data in-
stances (e.g., links) are interrelated and treated as non i.i.d data.
Adversarial attacks against graph data. Works on adversarial
attacks against complex network graph for GNN-based graph learn-
ing tasks are exceedingly rare, in contrast to other application
domains, not to mention adversarial attacks particularly for link
prediction. Very recently, Zugner et al. first proposed the work on
adversarial attacks against network graph particularly for node clas-
sification tasks [37]. As for the classification model, they focused on
a transductive learning setting. Zugner et al. ‚Äôs work is inherently
related to the causative/poisoning attacks [16]. Following this work,
Zugner et al. investigated the problem of training-time attacks on
the overall performance of node classification via the principle of
meta learning [38].
Along with the work of Zugner et al. , Dai et al. proposed to em-
ploy reinforcement learning approach to attack in both the graph-
level learning tasks and the node-level classification tasks [13]. Un-
like Zugner et al. ‚Äôs causative/poisoning attacks, Dai et al. ‚Äôs work
focused on the evasion attacks particularly against node/graph clas-
sification tasks. Except for Dai et al. ‚Äôs evasion attacks, Wang et
al. studied evasion attacks against collective classification methods
via manipulating the graph structure [30]. In contrast, our work
focuses on adversarial GNN-based link prediction, which deals with
coupled perturbation variables and non i.i.d graph data. We also
evaluate the different capabilities of the adversary with various
perturbable sets, and analyze its transferability to existing link
prediction heuristics.
7 CONCLUDING REMARKS
We have shown that adversarial attacks on graphs can break the
SEAL framework that uses GNNs for link prediction. These attacks
can often achieve significantly higher attack success rates, and can
degrade the model performance by a substantial margin, making the
prediction error even worse than random guessing. By incorporat-
ing the Œ•-decaying theory and the link formation mechanism, our
attacks can be generated efficiently and effectively. The prediction
performance is consistently exacerbated, even when the adversary‚Äôs
capability is restricted to only parts of the network graph. Based
on our extensive experiments, we show that the attacks mounted
based on GNN-based link prediction can be transferred to several
existing heuristics with our design.
REFERENCES
[1] Robert Ackland et al. 2005. Mapping the US Political Blogosphere: Are Conserva-
tive Bloggers More Prominent?. In BlogTalk Downunder 2005 Conference, Sydney.
BlogTalk Downunder 2005 Conference, Sydney.
[2] Lada A Adamic and Eytan Adar. 2003. Friends and Neighbors on the Web. Social
Networks 25, 3 (2003), 211‚Äì230.
[3] Mohammad Al Hasan, Vineet Chaoji, Saeed Salem, and Mohammed Zaki. 2006.
Link Prediction using Supervised Learning. In SDM06: Workshop on Link Analysis,
Counter-Terrorism and Security.
Networks. Science 286, 5439 (1999), 509‚Äì512.
[5] Vladimir Batagelj and Andrej Mrvar. 2006.
networks/data/
.
http://vlado.fmf.unilj.si/pub/
[4] Albert-L√°szl√≥ Barab√°si and R√©ka Albert. 1999. Emergence of Scaling in Random
[10] Jinjin Chen, Ziqiang Shi, Yangyang Wu, Xuanheng Xu, and Haibin Zheng. 2018.
[6] Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertex-
tual Web Search Engine. Computer Networks and ISDN Systems 30, 1-7 (1998),
107‚Äì117.
[7] Emrah Budur, Seungmin Lee, and Vein S Kong. 2015. Structural Analysis of
Criminal Network and Predicting Hidden Links using Machine Learning. arXiv
preprint arXiv:1507.05739 (2015).
[8] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of
Neural Networks. In Proc. IEEE Symposium on Security and Privacy (S&P 2017).
[9] Nicholas Carlini and David Wagner. 2018. Audio Adversarial Examples: Targeted
Attacks on Speech-to-Text. arXiv preprint arXiv:1801.01944 (2018).
Link Prediction Adversarial Attack. arXiv preprint arXiv:1803.06373 (2018).
[11] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. 2018.
EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples. In
Proc. AAAI Conference on Artificial Intelligence (AAAI 2018).
[12] Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto
Perdisci, Manos Antonakakis, and Nikolaos Vasiloglou. 2017. Practical Attacks
against Graph-Based Clustering. In Proc. ACM SIGSAC Conference on Computer
and Communications Security (CCS 2017).
[13] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. 2018.
Adversarial Attack on Graph Structured Data. In Proc. International Conference
on Machine Learning (ICML 2018).
[14] Yuxiao Dong, Jie Tang, Sen Wu, Jilei Tian, Nitesh V Chawla, Jinghai Rao, and
Huanhuan Cao. 2012. Link Prediction and Recommendation across Heteroge-
neous Social Networks. In Proc. IEEE International Conference on Data Mining
(ICDM 2012). 181‚Äì190.
[15] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. 2017. Adversarial Examples for Malware Detection. In Proc. Eu-
ropean Symposium on Research in Computer Security (ESORICS 2017). Springer.
[16] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and JD
Tygar. 2011. Adversarial Machine Learning. In Proc. ACM Workshop on Security
and Artificial Intelligence.
[17] Glen Jeh and Jennifer Widom. 2002. SimRank: A Measure of Structural-Context
Similarity. In Proc. ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (KDD 2002). 538‚Äì543.
[18] Harini Kannan, Alexey Kurakin, and Ian Goodfellow. 2018. Adversarial Logit
Pairing. arXiv preprint arXiv:1803.06373 (2018).
[19] Leo Katz. 1953. A New Status Index Derived from Sociometric Analysis. Psy-
chometrika 18, 1 (1953), 39‚Äì43.
[20] David Liben-Nowell and Jon Kleinberg. 2007. The Link-Prediction Problem for
Social Networks. Journal of the American Society for Information Science and
Technology 58, 7 (2007), 1019‚Äì1031.
[21] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017. Delving into
Transferable Adversarial Examples and Black-Box Attacks. In Proc. International
Conference on Learning Representation (ICLR 2017).
[22] Linyuan L√º and Tao Zhou. 2011. Link Prediction in Complex Networks: A Survey.
Physica A: Statistical Mechanics and Its Applications 390, 6 (2011), 1150‚Äì1170.
[23] Yacin Nadji, Manos Antonakakis, Roberto Perdisci, and Wenke Lee. 2013. Con-
nected Colors: Unveiling the Structure of Criminal Networks. In Proc. Interna-
tional Workshop on Recent Advances in Intrusion Detection. Springer.
[24] Mark EJ Newman. 2006. Finding Community Structure in Networks using the
Eigenvectors of Matrices. Physical Review E 74, 3 (2006), 036104.
[25] Joshua O‚ÄôMadadhain, Jon Hutchins, and Padhraic Smyth. 2005. Prediction and
Ranking Algorithms for Event-Based Network Data. ACM SIGKDD Explorations
Newsletter 7, 2 (2005), 23‚Äì30.
[26] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability
in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial
Samples. arXiv preprint arXiv:1605.07277 (2016).
[27] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik,
and Ananthram Swami. 2017. Practical Black-Box Attacks against Machine
Learning. In Proc. ACM on Asia Conference on Computer and Communications
Security (AsiaCCS 2017). ACM, 506‚Äì519.
[28] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn,
and Karsten M Borgwardt. 2011. Weisfeiler-Lehman Graph Kernels. Journal of
Machine Learning Research 12, Sep (2011), 2539‚Äì2561.
[29] Florian Tram√®r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. 2017. The Space of Transferable Adversarial Examples. arXiv preprint
arXiv:1704.03453 (2017).
[30] Binghui Wang and Neil Zhenqiang Gong. 2019. Attacking Graph-based Classifi-
cation via Manipulating the Graph Structure. In Proc. ACM SIGSAC Conference
on Computer and Communications Security (CCS 2019).
[31] Hao Wang, Xingjian Shi, and Dit-Yan Yeung. 2017. Relational Deep Learning:
A Deep Latent Variable Model for Link Prediction. In Proc. AAAI Conference on
Artificial Intelligence.
[32] Duncan J Watts and Steven H Strogatz. 1998. Collective Dynamics of ‚ÄúSmall-
World‚Äù Networks. Nature 393, 6684 (1998), 440.
[33] Muhan Zhang and Yixin Chen. 2017. Weisfeiler-Lehman Neural Machine for
Link Prediction. In Proc. ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2017).
[34] Muhan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neural
Networks. In Proc. International Conference on Neural Information Processing
Systems (NeurIPS 2018).
[35] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An
End-to-End Deep Learning Architecture for Graph Classification. In Proc. AAAI
[36] Tao Zhou, Linyuan L√º, and Yi-Cheng Zhang. 2009. Predicting Missing Links via
Conference on Artificial Intelligence (AAAI 2018).
Local Information. The European Physical Journal B 71, 4 (2009), 623‚Äì630.
[37] Daniel Zugner, Amir Akbarnejad, and Stephan Gunnemann. 2018. Adversarial
Attacks on Neural Networks for Graph Data. In Proc. ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD 2018).
[38] Daniel Zugner and Stephan Gunnemann. 2019. Adversarial Attacks on Graph
Neural Networks via Meta Learning. In Proc. International Conference on Learning
Representation (ICLR 2019).