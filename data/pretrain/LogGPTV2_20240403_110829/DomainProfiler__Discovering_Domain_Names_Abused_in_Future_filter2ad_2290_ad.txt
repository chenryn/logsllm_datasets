72,893
76,206
88,605
# FQDNs
173,409
172,657
172,384
172,364
172,364
TPR/Recall
0.973
0.756
0.700
0.746
0.975
TNR
0.991
0.639
0.815
0.852
0.991
Precision
0.990
0.660
0.779
0.824
0.990
F-measure
0.982
0.705
0.737
0.783
0.983
AUC
0.983
0.722
0.794
0.855
0.996
trees. As explained in Section III-B3, a random forest consists
of multiple decision trees; thus, we needed to decide how
many trees to make beforehand. As is the case with the
aforementioned time window size, we conducted 10-fold CVs
by changing the number of trees to decide the optimum
number of decision trees. The left graph in Fig. 10 shows
the relationships between the number of trees and F-measure.
The graph shows that F-measures are stable over 100 trees.
Thus, we decided to use 100 decision trees in the following
evaluations.
The other parameter is the number of sampled features
in each individual decision tree. A random forest constructs
decision trees from input data with randomly sampled features
to improve the overall accuracy. We conducted 10-fold CVs
again to search for the optimum number of sampled features.
The right graph in Fig. 10 shows that the best F-measure is
obtained when the number of sampled features is 7.
C. Feature Set Selection
Now that we selected the optimal parameters (the time
window size, number of trees, and number of sampled features
in each tree), this section compares the detection performance
with different feature sets. The feature sets include the tempo-
ral variation pattern (TVP), related IP address (rIP), related
domain name (rDomain), combination of rIP and rDomain
(rIP+rDomain), and combination of TVP, rIP, and rDomain
(TVP+rIP+rDomain). We conducted 10-fold CVs using the
training set and the optimal parameters by changing the feature
sets to estimate how accurately each feature set will perform
in theory. Table IV illustrates the detection performance using
the above evaluation criteria. Note that the number of FQDNs
varies with feature sets due to the availability of each feature.
For example, some domain names have no rIPs and/or rDo-
mains. Also, Fig. 11 shows the receiver operator characteristic
(ROC) curves. An ROC curve shows a pair of (FPR, TPR)
corresponding to a particular decision cut-off point. Thus, if
the ROC curve of a feature set rises more rapidly, it means
that the performance of the feature set is better. An area under
curve (AUC) is the area under the ROC curve and is generally
used to evaluate a binary classiﬁer. We calculated the AUC for
each ROC curve, and the results are also listed in Table IV.
Table IV and Fig. 11 illustrate that using our TVP features
makes signiﬁcant contributions to achieving better detection
performance than using only DNS-based features. Using only
DNS-based features (rIP, rDomain, and rIP+rDomain) does
not go beyond 0.90 in any evaluation criteria. These results
show that only using common/typical DNS-based features
1.00
0.75
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
0.50
0.25
0.00
0.00
Feature Set
TVP+rIP+rDomain
rIP+rDomain
TVP
rIP
rDomain
0.25
False Positive Rate
0.50
0.75
1.00
Fig. 11. ROC curves
proposed for known approaches proves insufﬁcient for de-
tecting malicious domain names in current attack ecosystems.
However, combining the DNS-based features with our TVP
features (TVP+rIP+rDomain) achieves the best result, namely,
a TPR/recall of 0.975, TNR of 0.991, precision of 0.990, and
F-measure of 0.983. These results indicate that our key idea
based on using TVPs is effective for improving both TPR and
TNR exactly as intended.
D. System Performance
We evaluated the system performance of a prototype version
of DOMAINPROFILER. Speciﬁcally, we calculated the execu-
tion time and data size in each step when we conducted a
10-fold CV using the training set with the optimal parameters
and best feature set (TVP+rIP+rDomain). Step 1 (identifying
TVPs) was executed on a single server with 10-core 2.2-GHz
CPU and 128-GB RAM. The execution time for extracting
TVP features from 173,409 FQDNs was 61 seconds, which
was equivalent to 0.0004 seconds/FQDN. The ﬁle sizes of the
domain name list database (SQL) were 1.4 GB in Alexa and
300 MB in hpHosts, respectively. Step 2 (appending DNS-
based features) was executed as a MapReduce job on a Hadoop
cluster, which had 2 master servers (16-core 2.4-GHz CPU,
128-GB RAM) and 16 slave servers (16-core 2.4-GHz CPU,
64-GB RAM). The execution time for extracting rIP features
from 173,409 FQDNs was 20 hours (0.42 seconds/FQDN) and
that for rDomain features was 96 hours (1.99 seconds/FQDN).
The ﬁle size of the historical DNS logs used for extracting
these DNS-based features was 212 GB. Step 3 (applying
498
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:19:55 UTC from IEEE Xplore.  Restrictions apply. 
PREDICTIVE DETECTION PERFORMANCE OF DOMAINPROFILER (TVP+RIP+RDOMAIN)
TABLE V
Dataset
Honeyclient-Exploit
Honeyclient-Malware
Sandbox-Malware
Sandbox-C&C
Pro-C&C
Pro-Phishing
TP
529
67
721
7,476
92
75,583
FN # FQDNs
537
68
775
8,473
97
78,221
8
1
54
997
5
2,638
TPR/Recall
0.985
0.985
0.930
0.882
0.948
0.966
days Min
16
1
1
1
1
1
days 1stQu
140
125
108
38
9
9
days 2ndQu
176
205
133
99
15
14
days Mean
164.5
159.4
128.2
98.31
14.95
14.04
days 3rdQu
197
212
151
140
21
19
days Max
220
220
221
221
29
28
PREDICTIVE DETECTION PERFORMANCE OF FEATURE SET (RIP+RDOMAIN)
TABLE VI
Dataset
Honeyclient-Exploit
Honeyclient-Malware
Sandbox-Malware
Sandbox-C&C
Pro-C&C
Pro-Phishing
TP
184
5
197
2,491
39
29,427
FN # FQDNs
537
353
68
63
775
578
8,473
5,982
58
97
78,221
48,794
TPR/Recall
0.343
0.074
0.254
0.294
0.402
0.376
days Min
19
1
3
1
1
1
days 1stQu
158
60
112
47
9
11
days 2ndQu
187
63
133
108
12
17
days Mean
172.3
108.4
128.3
103.9
14.18
15.21
days 3rdQu
210
205
153
144
20
20
days Max
219
213
221
221
29
28
machine learning) was executed on the same server as step 1.
The execution time for one-time training from 156,068 FQDNs
was 28 seconds (0.0001 seconds/FQDN) and that for test from
17,341 FQDNs was 8 seconds (0.0005 seconds/FQDN). These
evaluations prove the basic feasibility of our proposed system
and reveal that step 2 requires far more resources and time
to execute than steps 1 and 3. The reason for step 2’s high
cost is the size of the graphs for rIPs and rDomains. Today,
some domain names used by hypergiants, such as Google,
Amazon, and Akamai, have a huge number (over ten thousand)
of rIPs and rDomains. This fact raises the problem of high cost
for extracting common/typical DNS-based features. However,
from the results explained in Section IV-C,
this problem
will be solved if our system makes a sacriﬁce of a TPR of
0.002 to use the feature set TVP instead of TVP+rIP+rDP.
This is a trade-off between system performance and detection
performance. Thus, we should conﬁgure our system based on
the situation.
E. Predictive Detection Performance
We evaluated the predictive detection performance of DO-
MAINPROFILER; that is, whether we can discover domain
names that may be abused in future. The aforementioned
evaluations were based on cross-validations (CVs); however,
this section focuses on the evaluation of detection performance
of new malicious domain names that ﬁrst appeared after March
1, 2015 by using only the information as of February 28, 2015.
Speciﬁcally, we used the training set shown in Table III to
create a learning model ﬁrst then input the test set in Table III
to evaluate the predictive detection performance. In this evalu-
ation, we set the optimal parameters discussed in Section IV-B.
The best feature set (TVP+rIP+rDomain) discussed in Sec-
tion IV-C was compared with the feature set (rIP+rDomain)
that had only common/typical DNS-based features. Table V
lists the evaluation results of using TVP+rIP+rDomain and
Table VI lists those of rIP+rDomain. Note that the test set
only consists of malicious domain names; thus, there are no
false positives (FPs) and true negatives (TNs) and their related
evaluation criteria in the tables.
In terms of the true positive rate (TPR/recall), DOMAIN-
PROFILER and feature set (TVP+rIP+rDomain) achieved ex-
tremely high TPRs in all test sets; our system achieved TPRs
of 0.985 in Honeyclient-Exploit and Honeyclient-Malware.
Moreover, our system accurately detected/predicted command
and control (C&C) domain names in Sandbox-C&C and Pro-
C&C, while our training set did not include labeled C&C
domain names. This is not a surprising result because our
TVP is designed to exploit the common characteristics of
attackers’ domain names. On the other hand, using only the
typical features (rIP+rDomain) achieved a TPR of 0.402 at
best. Comparing these results illustrates that our TVP features
successfully contribute to predicting malicious domain names
that will be used in future.
In terms of early detection of future malicious domain
names, we investigated when the system can detect such
domain names. Speciﬁcally, we analyzed the number of days
that elapsed from February 28, 2015, when the learning model
was created, for malicious domain names to be detected with
the system. For example, if the system correctly detected and
identiﬁed a new malicious domain name on March 7, 2015,
the elapsed number of days for the domain name is 7. Tables V
and VI also show the descriptive statistics of the elapsed
days for malicious domain names for each feature set. Note
that we only count domain names in the TP of each dataset.
The descriptive statistics include the minimum (days Min),
ﬁrst quartile (days 1stQu), which means the value cut off the
ﬁrst 25% of the data, second quartile (days 2stQu), which
is also called the median and is the value cut off the ﬁrst
50% of the data, the mean (days Mean), the third quartile
(days 3rdQu), which is the value cut off the ﬁrst 75% of the
data, and the maximum (days Max). Table V shows that our