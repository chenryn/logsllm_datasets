title:The only constant is change: incorporating time-varying network reservations
in data centers
author:Di Xie and
Ning Ding and
Y. Charlie Hu and
Ramana Rao Kompella
The Only Constant is Change: Incorporating Time-Varying
Network Reservations in Data Centers
Di Xie
Ning Ding
Y. Charlie Hu Ramana Kompella
Purdue University Purdue University Purdue University
Purdue University
ABSTRACT
In multi-tenant datacenters, jobs of different tenants compete for
the shared datacenter network and can suffer poor performance and
high cost from varying, unpredictable network performance. Re-
cently, several virtual network abstractions have been proposed to
provide explicit APIs for tenant jobs to specify and reserve virtual
clusters (VC) with both explicit VMs and required network band-
width between the VMs. However, all of the existing proposals
reserve a ﬁxed bandwidth throughout the entire execution of a job.
In the paper, we ﬁrst proﬁle the trafﬁc patterns of several popu-
lar cloud applications, and ﬁnd that they generate substantial traf-
ﬁc during only 30%-60% of the entire execution, suggesting ex-
isting simple VC models waste precious networking resources.
We then propose a ﬁne-grained virtual network abstraction, Time-
Interleaved Virtual Clusters (TIVC), that models the time-varying
nature of the networking requirement of cloud applications. To
demonstrate the effectiveness of TIVC, we develop PROTEUS, a
system that implements the new abstraction. Using large-scale sim-
ulations of cloud application workloads and prototype implementa-
tion running actual cloud applications, we show the new abstraction
signiﬁcantly increases the utilization of the entire datacenter and re-
duces the cost to the tenants, compared to previous ﬁxed-bandwidth
abstractions.
Categories and Subject Descriptors:
Communication Networks]: Network Operations
C.2.3 [Computer-
General Terms: Algorithms, Design, Performance
Keywords: Datacenter, Network Reservation, Allocation, Band-
width, Proﬁling
1.
INTRODUCTION
Cloud computing has transformed the enterprise computing
landscape signiﬁcantly. By offering virtually unlimited resources
without any upfront capital investment and a simple pay-as-you-go
charging model, cloud computing provides a compelling alterna-
tive to enterprises constructing and maintaining their own cluster-
computing infrastructure. The long-term viability of cloud comput-
ing depends, among others, on two major factors—cost and per-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’12, August 13–17, 2012, Helsinki, Finland.
Copyright 2012 ACM 978-1-4503-1419-0/12/08 ...$15.00.
formance. Unless cloud platforms prove cheaper than enterprise
ones, enterprises have no incentive to migrate their computational
requirement to the cloud. Similarly, cloud platforms need to pro-
vide some guarantees about job performance, otherwise enterprises
may be apprehensive to migrate to the cloud to begin with.
Unfortunately, today’s public cloud platforms such as Amazon
EC2 do not provide any performance guarantee, which in turn af-
fects tenant cost. Speciﬁcally, the resource reservation model in
today’s clouds only provisions CPU and memory resources but
ignores networking completely. Because of the largely oversub-
scribed nature of today’s datacenter networks (e.g., [19]), network
bandwidth is a scarce resource shared across many tenants. When
networking intensive phases of applications collide and compete
for the scarce network resources, their running times become un-
predictable. The uncertainty in execution time further translates
into unpredictable cost as tenants need to pay for the reserved vir-
tual machines (VMs) for the entire duration of their jobs.
Recent works such as SecondNet [20] and Oktopus [11] noted
this lack of networking SLA in cloud environments and proposed
new network abstractions for tenants to specify their networking
needs along with their CPU and memory needs, so that their appli-
cations can obtain predictable performance. For example, Second-
Net proposes bandwidth reservation between every pair of VMs.
Oktopus proposes a simpler virtual cluster (VC) model where all
VMs are connected to a virtual switch with links of bandwidth B.
While both models provide a good start, they fail to capture the
temporal dimension of network resource requirement. Speciﬁcally,
we observe that the networking requirement of many applications
experience signiﬁcant changes throughout their execution. Hence
provisioning a single bandwidth B for an entire cluster or for each
pair of VMs throughout the job execution is clearly wasteful.
To illustrate this, we show the instantaneous network throughput
of the MapReduce Sort application in Figure 1(a) (details are in
§2.2). We observe from the ﬁgure that the Sort application utilizes
the network only during the ﬁrst half of the execution (during the
shufﬂe phase), while the second half (merge sort and reduce phase)
requires very little networking resource. Effectively the network
utilization is in the form of a simple square wave. Figure 1 fur-
ther shows the network utilization characteristics of several other
applications such as Word Count, Hive Join and Hive Aggregation.
While they exhibit more complicated network characteristics than
Sort, a common takeaway is the relatively sparse, time-varying net-
working requirement.
Using a ﬁxed bandwidth reservation can potentially waste pre-
cious resources of the datacenter. Consider several Sort jobs that
have bandwidth requirement B during the ﬁrst half of their execu-
tion. If we use Oktopus’ virtual cluster (VC) model for specifying
the network requirement of Sort, the ﬁxed bandwidth B is provi-
sioned during the entire duration of the job, preventing another Sort
199Cap 800Mbps
Cap 50Mbps
Cap 800Mbps
Cap 800Mbps
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 50
 40
 30
 20
 10
 0
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
 0  50  100 150 200 250 300 350 400
 0  100  200  300  400  500  600  700
 0  100  200  300  400  500  600  700
 0
 100  200  300  400  500  600
Time (sec)
Time (sec)
(a) Hadoop Sort, 4 GB per VM.
(b) Hadoop Word Count, 2 GB per
VM.
Time (sec)
(c) Hive Join, 6 GB per VM.
Time (sec)
(d) Hive Aggregation, 2 GB per
VM.
Figure 1: Time-varying trafﬁc demand of cloud applications.
job from using the unused bandwidth B during the second half of
the job. Clearly, if there are enough VMs, we can schedule a sec-
ond Sort job such that the ﬁrst half of the second job overlaps with
the second half of the already running ﬁrst job.
Our main contribution in this paper is to exploit this key obser-
vation and explore a new, ﬁne-grained network reservation abstrac-
tion called temporally-interleaved virtual clusters (TIVC), that al-
lows specifying the time-varying networking requirement of cloud
applications. By capturing the temporal networking usage, the
TIVC abstraction reduces over-reservation of network bandwidth
in ﬁxed-bandwidth abstractions, allowing more jobs to be admit-
ted into the datacenter. Thus the new abstraction not only improves
the utilization of the network resource but also potentially increases
the VM utilization and hence the overall datacenter utilization. The
increased utilization of the entire datacenter in turn translates into
either increased cloud provider revenue, e.g., under today’s VM-
time only charging model, or both increased provider revenue and
reduced tenant cost, when the cloud provider adopts a bandwidth
charging model (e.g., [30, 10]), compared to ﬁxed-bandwidth ab-
stractions.
For the TIVC abstraction to be viable in practice, there are two
fundamental questions that we must answer: How can we obtain
the TIVC model corresponding to a job? How do we allocate and
provision jobs with TIVC speciﬁcations in a physical datacenter?
While a similar question to the ﬁrst confounded prior network ab-
stractions [11, 20], they simply assumed that the customer will
specify them somehow.
In this paper, we propose a systematic,
proﬁling-based methodology for automatically generating TIVC
model parameters for a given application. There are several chal-
lenges that we need to tackle to make this proﬁling-based method-
ology practical.
The ﬁrst challenge concerns the elastic nature of networking re-
quirement for many jobs; as we provide more bandwidth, job will
ﬁnish faster. So how should one determine the peak bandwidth re-
quirement for an application in deriving its TIVC model, e.g., in
proﬁling the application? Using a detailed study using real cloud
applications, we provide insights into this question, which previous
works have ignored completely.
The second challenge concerns designing TIVC model functions
so that they can be easily generated, they capture the time-varying
networking requirement of an application well, and yet they can be
practically enforced in the datacenter network. We propose to con-
vert the networking requirement output of the proﬁling stage into
coarse-grained pulse functions (square waves) with different widths
and heights. Our conversion scheme ensures the derived bandwidth
reservation model has little impact on the job completion time.
The ﬁnal challenge concerns a practical allocation algorithm that
not only needs to identify where a TIVC job can be admitted spa-
tially (in the network topology), but also temporally (when the
bandwidth will be freed in the future to accommodate this job
throughout its execution). We propose an allocation algorithm
based on dynamic programming that is highly scalable and ﬁnds
valid allocations with the best spatial locality, i.e., in the lowest
subtree available.
We have developed PROTEUS, a system that implements the
TIVC abstraction. PROTEUS derives TIVC models for applica-
tions, accepts jobs online and allocates them to the physical dat-
acenter, and provisions network bandwidth according to the TIVC
models. We deployed our prototype on an 18-node datacenter with
a ternary tree topology using NetFPGA-based switches. Our sim-
ulations of a large-scale datacenter using real MapReduce applica-
tion workload show our TIVC abstraction can increase the datacen-
ter batched job throughput by 34.5%, and reduce the rejection rate
of dynamically arrived jobs from 9.5% to 3.4%, which translates
into 22% higher cloud provider revenue than the ﬁxed-bandwidth
abstraction under today’s VM-time based charging model. When
the cloud provider moves to a bandwidth charging model, TIVC
can both improve cloud provider revenue by 11% and reduce ten-
ant cost by 12%, compared to the ﬁxed-bandwidth abstraction.
2. BACKGROUND AND MOTIVATION
In this section, we ﬁrst review the state of the art, and then mo-
tivate the time-varying networking abstraction proposed in this pa-
per.
2.1 State of the Art
To provide performance guarantees, several recent works [20,
11] have proposed virtual network cluster abstractions, which al-
low cloud users to specify not only the type and number of VMs
requested, but also the associated networking requirement, i.e., the
bandwidth requirement between the VMs. Such a virtual cluster
gives cloud users an assurance of their job performance based on
the VM and networking SLAs.
SecondNet [20] proposes APIs that let users specify either end-
to-end bandwidth for each pair of VMs (suitable for strong guar-
antees), or ingress/egress bandwidth for each VM (for better than
best-effort sharing). Oktopus [11] proposes two simpliﬁed abstrac-
tions: virtual clusters and virtual oversubscribed clusters. A virtual
cluster is a one-level logical tree topology, speciﬁed as ,
which asks for N VMs and each VM is connected to a virtual
switch by a bidirectional link of bandwidth B. A virtual oversub-
scribed cluster request, , speciﬁes a two-level logical
tree topology with an oversubscription ratio of O.
All the models above, however, only allow ﬁxed bandwidth
guarantees, since they fundamentally assume the applications have
the same bandwidth requirement throughout the entire execution,
which is rarely true in practice. Speciﬁcally, our key observation is
that typical cloud applications generate varying amount of trafﬁc in
different phases of their execution. Thus reserving their peak band-
width requirement for the entire execution wastes scarce network
resources in the datacenter, which reduces the number of jobs that
can ﬁt in the datacenter to run concurrently, and hence the overall
utilization of the datacenter.
2002.2 Proﬁling Networking Demand
To assess the extent of networking requirement variations of
cloud applications, we conduct a measurement study.
Since
MapReduce [17] is a popular programming paradigm for large-
scale data-intensive operations and many cloud applications have
been ported to MapReduce, such as search indexing, database, and
machine learning, we proﬁle the networking patterns of several typ-
ical MapReduce jobs, including Sort, Word Count, Hive Join, and
Hive Aggregation, which have been used as the primary bench-
marks in recent datacenter studies (e.g., [16, 36, 24, 28]). While
this list of applications is not exhaustive, we believe they represent
an important class of applications found in datacenters.
Methodology. We conducted the proﬁling experiments on our 33-
machine cluster, using open-source Hadoop 0.20.0 as the MapRe-
duce platform. The 33 machines are connected to a Gigabit switch.
Each of the 33 machines is equipped with a 4-core Xeon 2.4GHz
processor, 4GB memory, and running 2 Xen virtual machines
(domU) and each VM is allocated 1.5GB memory and a dedicated
hard drive. To obtain the network trafﬁc, we deployed tcpdump at
all the Hadoop slave VMs to log all communication (packet head-
ers) between them. Since the bidirectional trafﬁc are almost identi-
cal for the set of applications, for each application, we plot the ac-
cess egress bandwidth of one VM in each time bin of 10ms, which
is in the same order of RTTs under load and hence sufﬁciently small
to capture the transient peak throughput. The throughput for differ-
ent VMs are very similar, and we study the variation in Section 4.
Trafﬁc Patterns of Cloud Applications. We observe that the traf-
ﬁc patterns of the set of applications fall into the following three
categories, of increasing generality. In all categories, a base band-
width needs to be reserved to facilitate job maintenance tasks such
as communicating with the job scheduler, and to support low vol-
ume network trafﬁc among the VMs.
Type 1: Single peak. The Hadoop Sort benchmark sorts randomly
generated records with 10-byte keys and 90-byte values. Fig-
ure 1(a) plots the network throughput over the execution with 4
GB input data per VM. We observe that most of the network trafﬁc
were produced in the ﬁrst half of the job execution. In other words,
the trafﬁc pattern exhibits a single peak, deﬁned as a continuous
period of throughput above some base amount.
Type 2: Repeated ﬁxed-width peaks. The Hadoop Word Count
benchmark counts the number of word occurrences in the input
data. Figure 1(b) shows that there was only a small amount of
data been shufﬂed over the network periodically throughout the ex-
ecution. The small amount of data is due to reduced map output
from enabling the combiner in map tasks, and the periodic network
trafﬁc is because the map tasks ﬁnished in batches, and hence their
output were shufﬂed periodically. Thus the trafﬁc demand exhibits
repeated bursts with a similar amount of trafﬁc volume per burst,
and consequently similar duration.
Type 3: Varying-width peaks and Type 4: Varying height and width
peaks. Hive [2] implements a data warehouse system built on top
of Hadoop. It provides an SQL-like language for data queries. The
Hive performance benchmark [5] is used to compare the perfor-
mance of Hadoop, Hive, and PIG [3]. It consists of four queries:
grepping from a table, selecting columns from a table, aggregating
a table, and joining two tables. We generated a 4GB UserVisits
table and a 2GB Rankings table per node. Figures 1(c)-(d) plot
the throughput for the two queries Join and Aggregation. We ob-
serve that Hive Join exhibits a brief burst of network activity at the
beginning, followed by a longer duration of less-intensive trafﬁc
from map tasks, followed by ﬁve bursts, corresponding to ﬁve re-
duce tasks. Hive Aggregation exhibits a long duration of moderate
trafﬁc demand, followed by a long duration of more intense trafﬁc
B
i
h
t
d
w
d
n
a
B
0
		
Time 
Time 
T
T
		




 
B
i
h
t
d
w
d
n
a
B
B
b 
0 
B
B
i
i
i
h