User: I am creating a weather station using a Raspberry Pi.  I have a mySQL database setup for the different sensors (temp, humidity, pressure, rain, etc) and am now getting to processing the wind sensors.
I have a python program that watches the GPIO pins for the anemometer and counts the pulses to calculate the wind speed.  It also reads from a wind vane processes through an ADC to get the direction.  For the other sensors I only process them every few minutes and dump the data directly to the DB. Because I have to calculate a lot of things from the wind sensor data, I don't necessarily want to write to the DB every 5 seconds and then have to read back the past 5 minutes of data to calculate the current speed and direction.  I would like to collect the data in memory, do the processing, then write the finalized data to the DB.  The sensor reading is something like:
datetime, speed, direction
2013-6-20 09:33:45, 4.5, W
2013-6-20 09:33:50, 4.0, SW
2013-6-20 09:33:55, 4.3, W
The program is calculating data every 5 seconds from the wind sensors.  I would like to write data to the DB every 5 minutes.  Because the DB is on an SD card I obviously don't want to write to the DB 60 times, then read it back to process it, then write it to the permanent archival DB every 5 minutes.
Would I be better off using a list of lists? Or a dictionary of tuples keyed by datetime?
{datetime.datetime(2013, 6, 20, 9, 33, 45, 631816): ('4.5', 'W')}
{datetime.datetime(2013, 6, 20, 9, 33, 50, 394820): ('4.0', 'SW')}
{datetime.datetime(2013, 6, 20, 9, 33, 55, 387294): ('4.3', 'W')}
For the latter, what is the best way to update a dictionary? Should I just dump it to a DB and read it back?  That seems like an excessive amount of read/writes a day for so little data.
Assistant: There are multiple cache layers between a Python program and a database. In particular, the Linux disk block cache may keep your database in core depending on patterns of usage. Therefore, you should not assume that writing to a database and reading back is necessarily slower than some home-brew cache that you'd put in your application. And code that you write to prematurely optimize your DB is going to be infinitely more buggy than code you don't write.
For the workload as you've specified it, MySQL strikes me as a little heavyweight relative to SQLite, but you may have unstated reasons to require it.