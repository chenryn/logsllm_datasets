time for each request type. The adversary can delay responses for
an arbitrarily long time to covertly convey more information.
 0.1 1 10 100 1000 10000 100000 1e+06 0 5 10 15 20 25 30 35 40 45SlowdownNumber of epochsfactor=23, grace periodfactor=27, grace periodlocalglobal34345569Figure 4: Parallel composition of mitigators
Figure 5: Sequential composition of mitigators
However, for some speciﬁc platforms, such as real-time sys-
tems and web applications with a timeout setting, we can assume a
worst-case execution time Tw. Given this constraint, we can derive
a tighter leakage bound.
The analysis works similarly to that in Section 4.3, but instead
of using the conservative constraint p( ~mi−1, rji ) ≤ T as in Deﬁ-
nition 2, worst-case execution time provides a tighter estimation:
p( ~mi−1, rji ) ≤ Tw
Compared with bounding running time T , this condition more
precisely approximates whether the state ~mi−1 can make one more
misprediction to ~mi. The reason is that whenever p( ~mi−1, rji )
> Tw, the state ~mi−1 cannot have another misprediction because
execution is bounded by Tw. Therefore, we can reuse the bound on
the number of epochs in Section 4.3 by replacing T with Tw.
For example, total leakage with the assumption of worst-case
execution time Tw for the global penalty policy is bounded by
B(T, M ) = (log Tw + 2) · log(M + 1)
This logarithmic bound is asymptotically the same as that achieved
by the less general bucketing scheme proposed by Köpf et al. [12]
for cryptographic timing channels.
For the l-grace-period penalty policy we can perform a similar
analysis to derive a bound on leakage:
B(T, M, R, l) = log(M + 1) · ((R − 1) · (l + 1) + log Tw + 2)
5. Composing mitigators
If timing mitigation is used, we can expect large systems to be
built by composing mitigated subsystems. Askarov et al. [14] show
empirically that composing mitigators sequentially performs well,
which makes sense because mitigated output has more predictable
timing. However, the prior work did not analyze leakage.
We analyze composed mitigators by considering the leakage of
two gadgets: two mitigators connected either in parallel or sequen-
tially (Figures 4 and 5). More complex systems with mitigated sub-
systems can be analyzed by decomposing them into these gadgets.
Parallel composition. Figure 4 is an example of parallel compo-
sition of mitigators, in which requests received by the system are
handled by two independent mitigators. The bound on the leak-
age of the parallel composition is no greater than the sum of the
bounds of the independent mitigators. To see this, denote by P
the total number of variations of the parallel composition, and de-
note by V1 and V2 the number of timing variations of the ﬁrst and
second mitigators, respectively. We know P ≤ V1 · V2; conse-
quently, the total leakage of parallel composition log P is bounded
by log V1 + log V2. The same argument generalizes to n mitigators
in parallel.
Sequential composition.
Suppose we have a security-critical
component, such as an encryption function, and leakage from this
component is controlled by a mitigator that guarantees a tight bound,
say at most 10 bits of the encryption key. We can show that once
mitigated, leakage of the encryption key can never exceed 10 bits,
no matter how output of that component is used in the system. This
is true for both Shannon-entropy and min-entropy deﬁnitions of
leakage.
Consider sequential composition of two systems as depicted in
Figure 5. Suppose that the secrets in the ﬁrst system are S, and
that the outputs of the ﬁrst and the second mitigators are O1 and
O2 respectively. We consider how much the output of each of the
mitigators leaks about S.
As discussed in Section 2.3, the leakage of the ﬁrst mitigator
using mutual information is I(S; O1) and the leakage of the second
is I(S; O2). Then we can show that the second mitigator leaks no
more information about S1 than the ﬁrst does. We formalize this in
the following lemma.
LEMMA 3. I(S; O1) ≥ I(S; O2)
A similar result holds for min-entropy leakage.
LEMMA 4. V (S|O1) ≥ V (S|O2)
Both of these lemmas are proved in the appendix.
Discussion. Parallel and sequential composition results enable de-
riving conservative bounds for networks of composed subsystems.
The bounds derived may be quite conservative in the case where
parallel mitigated systems have no secrets of their own to leak. If
the graph of subsystems contains cycles, it cannot be decomposed
into these two gadgets. We leave a more comprehensive analysis of
mitigator composition to future work.
6. Experiments
To evaluate the performance and information leakage of gener-
alized timing mitigation, we implemented mitigators for different
applications. The widely used Apache Tomcat web container was
modiﬁed to mitigate a local hosted application. We also developed
a mitigating web proxy to estimate the overhead of mitigating real-
world applications—a non-trivial homepage that results in 49 dif-
ferent requests and a HTTPS webmail service that requires stronger
security.
We explored how to tune this general mechanism for different
security and performance requirements. The results show that mit-
igation does slow down applications to some extent; we suggest the
slowdown is acceptable for some applications.
6.1 Mitigator design and its limitations
We deﬁne the system boundary in the following way. Inputs en-
ter the system at the point when Tomcat dispatches requests to the
servlet or JSP code. Results returned from this code are considered
outputs. Thus, all timing leakage arising during the processing of
the servlet and the JSP ﬁles is mitigated.
This implementation of mitigation has limitations. Because of
shared hardware and operating-system resources such as ﬁlesys-
tem caches, memory caches, buses, and the network, the time re-
quired to deliver an application response may convey information
about sensitive application data. Our current implementation strat-
egy, chosen for ease of implementation, prevents fully addressing
these timing channels where they affect timing outside the system
boundary as deﬁned.
SM1M2SMS' M'O1O2570To completely mitigate timing channels, mitigation should be in-
tegrated at the operating system and hardware levels. For example,
the TCP/IP stack might be extended to support delaying packets
until a mitigator-speciﬁed time. With such an extension, all timing
channels, including low-level interactions via hardware caches and
bus contention, would be fully mitigated. Although we leave the
design of such a mechanism to future work, we see no reason why
a more complete mitigation mechanism would signiﬁcantly change
the performance and security results reported here.
6.2 Mitigator implementation
We implemented the mitigator as a Java library containing 201
lines of Java code, excluding comments and the conﬁguration ﬁle.
This library provides two functions:
Mitigator startMitigation (String requestType);
void
(Mitigator miti);
endMitigation
The function startMitigation should be invoked when an
input is available to the system, passing an application-speciﬁc re-
quest type identiﬁer. The function endMitigation is used by
the application when an output is ready, and the mitigator for the re-
lated input is required for this interface. Calling endMitigation
blocks the current thread until the time predicted by the mitigator.
Instead of optimizing for speciﬁc applications, we heuristically
choose the following parameters for all experiments: 1. Initial pen-
alty: the initial penalty for all request types is 50 ms, a delay short
enough to be unnoticeable to the user. 2. Penalty policy: we use the
5-level grace period policy since it provides good tradeoff between
security and performance as shown in 4.5. 3. Penalty function:
most requests are returned within 250 ms, and the distribution is
quite even. We evenly divide the ﬁrst 5 epochs to make predictions
more precise: 50 ms, 100 ms, 150 ms, 200 ms, 250 ms, doubling
progressively thereafter. 4. Worst-case execution time Tw: We as-
sume worst-case execution time for requests Tw to be 300 seconds.
This is consistent with Firefox browser version 3.6.12, which uses
this value as a default timeout parameter.
6.3 Leakage revisited
Applying the experiment settings into the formula from Sec-
tion 4.6 with R request types, the following leakage bound obtains:
((R − 1) · (l + 1) + (log Tw + 2)) · log(M + 1)
=((R − 1) · 6 + (log 300000 + 2)) · log(M + 1)
≤(6 · R + 15) · log(M + 1)
where M is the number of inputs using the simple doubling scheme.
Intuitively, introducing more request types helps make the pre-
diction more precise for each request, because processing time varies
for different kinds of requests. However, the leakage bound is pro-
portional to the number of request types. So it is important to ﬁnd
the right tradeoff between latency and security.
6.4 Latency and throughput
To enable the mitigation of unmodiﬁed web applications, we
modiﬁed the open source Java Servlet and JavaServer Pages con-
tainer Tomcat 6.0.29 using the mitigation library.
Experiment setup. Mitigating Tomcat requires only three lines
of Java code: one line generating a request type id from the HTTP
request, one line to start the mitigation, and another line to end
mitigation after the servlet is ﬁnished. We deployed a JSP wiki
application, JSPWiki4, in the mitigating Tomcat server to evaluate
how mitigation affects both latency and throughput. Measurements
4http://www.jspwiki.org
Figure 6: Wiki latency with and without mitigation
Figure 7: Wiki throughput with and without mitigation
were made using the Apache HTTP server benchmarking tool ab.5
Since we focus on the latency and throughput overhead of request-
ing the main page of the wiki application, the URI is used as the
request type identiﬁer.
Results. We measured the latency and throughput of the main
page of JSPWiki for both the mitigated and unmitigated versions.
We used a range of different concurrency settings in ab, controlling
the number of multiple requests to perform at a time. The size of the
Tomcat thread pool is 200 threads in the current implementation.
For each setting, we measured the throughput for 5 minutes. The
results are shown in Figure 6 and Figure 7.
When the concurrency level is 1—the sequential case—the un-
mitigated Wiki application has a latency around 11ms. Since the
initial penalty is selected to be 50ms in our experiments, the aver-
age mitigated latency rises to about 57ms: about 400% overhead.
This is simply an artifact of the choice of initial penalty.
As we increase the number of concurrent requests, the unmit-
igated application exhibits more latency, because concurrent re-
quests compete for limited resources. On the other hand, the mit-
igation system is predicting this delayed time, and we can see that
these predictions introduce less overhead: at most 90% after the
concurrency level of 50; an even smaller overhead is found for
higher concurrency levels.
The throughput with concurrency level 1 is much reduced from
the unmitigated case: only about 1/5 of the original throughput.
However, when the concurrency level reaches 50, throughput in-
creases signiﬁcantly in both cases, and the mitigated version has
52.73% of the throughput of the unmitigated version. For higher
levels of concurrency, the throughput of the two versions is similar.
6.5 Real-world applications with proxy
We evaluated the latency overhead of predictive mitigation on
existing real-world web servers. To avoid the need to deploy pre-
dictive mitigation directly on production web servers, we intro-
duce a mitigating proxy between the client browser and the target
host. We modiﬁed an open source Java HTTP/HTTPS proxy, Lit-
5http://httpd.apache.org/docs/2.0/programs/ab.html
50100150200250300Concurrency level0123Average response time (sec)mitigatedunmitigated50100150200250300Concurrency level050100150Number of requests/ secmitigatedunmitigated571Figure 8: Latency for an HTTP web page
Figure 10: Latency overhead for HTTPS webmail service
Figure 9: Leakage bound for an HTTP web page
Figure 11: Leakage bound for HTTPS webmail service
tleProxy6, to use the mitigation library, adding about 70 LOC. We
used it to evaluate latency with two remote web servers: a HTTP
web page and an HTTPS webmail service.
With mitigation again done entirely at user level, timing chan-
nels that arise outside the mitigation boundary cannot be mitigated.
The mitigation boundary is deﬁned as follows: the mitigating proxy
treats requests from client browser as inputs, and forwards these
requests to the host. The response from the host is regarded as an
output in the black-box model.
The proxy mitigates both the response time of the server and the
round-trip time between the proxy and server. Only the ﬁrst part
corresponds to real variation that would occur with a mitigating
web server. To estimate this part of latency overhead, we put the
proxy in a local network with the real host. Because we found mea-
sured little variation in this conﬁguration, the results here should
estimate latency for real-world applications reasonably accurately.
6.5.1 HTTP web page
Unlike the previous stress test that requests only one URL, we
evaluated latency overhead using a non-trivial HTTP web page, a
university home page that causes 49 different requests to the server.
Multiple requests bring up the opportunity of tuning the tradeoff
between security and performance. Various ways to choose request
types were explored:
1. TYPE/HOST: all URLs residing on the same host are treated
as one request type, that is, they are predicted the same way.
2. HOST+URLTYPE: requests on the same host are predicted
differently based on the URL type of the request. We distinguish
URL types based on the ﬁle types, such as JPEG ﬁles, CSS ﬁles
and so on. Each of them corresponds to a different request type.
3. TYPE/URL: individual URLs are predicted differently.
Figure 8 shows the latency of loading the whole page and the
number of request types with these options. The results show that
latency in the most restrictive TYPE/HOST case almost triples that
of the unmitigated case. HOST+URLTYPE and TYPE/URL op-
tions have similar latency results, with about 30% latency overhead.
6http://www.littleshoot.org/littleproxy/index.html
From the security point of view, the TYPE/HOST option only
results in two request types: one host is in the organization, and
the other one is google-analytics.com, used for the search
component in the main page. HOST+URLTYPE introduces 6 more
request types, while using the TYPE/URL option, there are as many
as 49 request types. The information leakage bounds for different
options are shown in Figure 9.
The HOST+URLTYPE choice provides a reasonable tradeoff: it
has roughly a 30% latency overhead, yet information leakage is
below 850 bits for 100,000 requests.
6.5.2 HTTPS webmail service
We also evaluate the latency with a webmail service based on
Windows Exchange Server. After the user passes Kerberos-based
authentication (Auth), he is redirected to the login page (Login) and
may then see the list of emails (List) or read a message (Email).
Request type selection. This application accesses sensitive data,
so we evaluate performance with the most restrictive scheme: one
request type per host. There are actually two hosts: one host is used
to serve only AuthPage.
Results. We measured the latency overhead of four representative
pages for this service. Each page generates from 6 to 45 different
requests. The results in Figure 10 show that the latency overhead
ranges from 2 times to 4 times for these four pages; in the worst
case, latency is still less than 1 second. Also, this overhead can be
reduced with different request type selection options.
Figure 11 shows the leakage bound of this mitigated application.
The leakage is limited to about 300 bits after 100,000 requests and
grows slowly thereafter.
7. Related work
The most closely related work is that of Askarov et al. [14].
Comparisons to that work have been made throughout the paper;