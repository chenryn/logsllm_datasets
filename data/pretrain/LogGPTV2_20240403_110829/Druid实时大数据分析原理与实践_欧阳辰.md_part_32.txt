后，延迟时间窗口的时间以后进行Segment 的移交，如果将时间窗口设为较大值，则会影响
根据拒绝策略进行处理，选择保留或者丢弃。另外，Druid在达到设定的 Segment时间粒度以
10.2.3
266
经过前面章节的学习，我们知道Druid的实时部分采用类似于LSM-tree的架构。事件数
SegmentGranularity和QueryGranularity如何配置是初学者比较迷茫的问题之一。首先来
在实时摄入过程中，windowPeriod是非常重要的参数，Druid会将超出时间窗口的数据
优化策略
Druid实时大数据分析原理与实践
---
## Page 291
的维度进行聚合统计查询。
处理的周期进行划分存储（如划分为天表、小时表）。
1.MySQL/PostgreSQL
临着新的挑战，其特点如下。
进一步驱动产品迭代和精细化运营。随着互联网中各类海量的交互、访问数据，数据分析面
数据分析让企业更加了解和认识他们的客户并从中分析出客户转化、留存等一些关键指标，
10.3
圾回收。
地进行持久化，将会影响查询性能；如果将参数设置过大，则会使用过多的内存，影响垃
少以后进行持久化，两个参数满足其一就会触发持久化。如果将参数设置过小，将会过多
的含义是间隔多长时间进行持久化，maxRowsInMemory是内存增量索引中事件条数达到多
2.
定持久化国值的是intermediatePersistPeriod和maxRowsInMemory。intermediatePersistPeriod
第10章实践和应用
10.3.1
Elasticsearch（ES）
方案：访问数据进行ETL后人ES，写支持实时和离线两种方式入库，查询时根据不同
·优点：服务稳定且使用简单（SQL标准化使用），由于存储的是面向业务统计的结果
方案：离线定期做数据聚合统计处理。将多个分析维度做排列组合，聚合统计结果并按
·缺点：业务定制化导致损失了灵活性，且统计区段固定导致无法解决跨分区统计问题
·时效性与灵活性：需要及时获得分析结果，并能灵活地在多个维度上进行组合选
·数据的快速收集与统计处理：一方面，需要收集和处理的数据量大（处理用户访问的
数据分析是企业级SaaS服务中重要的一块，作为业界领先的SCRM产品，腾讯企点通过
需求。因为数据依赖离线任务，故在时效性上有延迟。
（比如对两天内的访问用户做去重统计），其只能解决统计指标进行并集操作的统计
集，因此数据量大大减少。基于传统OLAP数据库可直接面向业务层查询使用。
取对比。
数值结果。
全量行为数据，每日上亿规模的数据量）；另一方面，需要计算分析出统计意义上的
腾讯
工程实践
267
---
## Page 292
如下近似算法，满足了大数据量、高基数等方面的分析需求。
储（支持灵活的维度）、统计指标聚合计算等来实现在统计数据分析方面的优化。Druid支持
针对统计分析数据量大（上亿规模）、维度多（待分析维度确定）和分析结果集小且为统计
至 Druid时可以进行Roll-up 聚合处理，使得其有更低的存储成本和更高的查询效率。因此，
于可以事先确定待分析的维度和统计指标，相比数据入库至ES时存储原始数据，数据人库
其次，从统计分析功能上看，ES和Druid都能满足需求，但这里需要考虑使用上的权衡。由
不能满足需求，且随着数据量的日益增长，传统OLAP存储系统在扩展性方面也面临挑战
集操作的，因此即使是计算跨小时的时间范围统计也能很好地得到支持。
于Druid的聚合统计HyperUnique采用了HyperLogLog算法，而HyperLogLog算法是支持并
理。聚合统计数据将大大减少Druid中存储的数据量。以统计去重后的客户数指标为例，由
入Druid。由于查询的粒度最小为小时，故可以按照小时级别的粒度在入库时进行Roll-up处
3.Druid
数值的需求采用Druid方案更为合适。Druid分别通过人库Rollup（减少数据量）、列式存
268
·优点：通过入库时的预聚合，待统计分析的数据量大大减少，降低了存储成本，提高
方案：根据事先确定好的分析维度和统计指标，利用 Spark任务清洗出干净的数据集写
首先，由于需要在任意时间范围做统计分析，故MySQL/PostgreSQL存储结果集的方案
·优点：ES在数据查询上更灵活且集群部署简单（外部依赖少），由于ES存储了入库
·需要支持任意时间范围的统计，以及灵活地对比不同的维度数据。
·访客上报的数据量大，每日上报记录数至少都是上亿规模。
结合业务在技术方案的选取上有如下考虑。
·缺点：外部依赖较多且运维部署麻烦。由于在入库时采取预聚合计算的策略，故原始
·缺点：资源更多，需要更多的机器。在数据量大的聚合统计方面查询延迟与并发表现
需要支持更大的并发查询（企业级应用一般需支持上百QPS的服务要求）。
信息的缺失导致无法支持明细数据查询。
也有着优异的表现。
佳的表现。同时，由于Druid在设计上采用 shared-nothing架构，故其在并发查询上
了查询效率。Druid支持多种聚合统计估算算法，在应对大数据量分析场景中有着上
不如Druid。
时的原始数据，故在业务上可以支持更细粒度的查询，比如查询明细流水数据。
Druid实时大数据分析原理与实践
---
## Page 293
实时部分的完整性上有一定的缺失，需要离线部分的处理来完善数据。
钟至小时粒度。但由于实时部分只能进行数据追加写操作，且数据上报延迟等原因，因此在
Druid。实时部分可满足对数据时效的需求，通常 Segment 数据文件根据业务情况划分为分
第10章
务层面对历史数据的分析粒度只需要到天级别，而实时部分则是按小时划分的，那么离线任
分写入的Segment进行替换，完成对实时部分数据的修正和补全（这里有个优化处理，在业
理）。由于Druid存储数据时以 Segment 为粒度，因此离线入库可以根据时间范围对实时部
群发起离线人库任务（Druid根据任务Schema启动相应的MapReduce任务对数据做入库处
后，启动Spark离线任务批量处理数据并写人Druid依赖的HDFS路径，完成后向Druid集
通常以天粒度的批量任务方式对数据进行修正和补全。每日数据上报落地至数据仓库
利用SparkStreaming消费消息队列中上报的数据并进行业务层面的数据处理后人库至
从架构图中可以看出，在工程实践中使用Druid包含实时和离线两部分。
基于Druid的统计分析系统架构图如图10-8所示。
·DataSketch，相比HyperLogLog算法（只支持并集计算），其支持集合运算如交集、并
·HyperUnique，利用HyperLogLog算法求基数，
（2）离线部分
（1）实时部分
·近似直方图和分位数。
集、差集，比如计算同时访问过网站A和网站B的访客UV数。
实践和应用
1.定时落地数据集
batch
图10-8基于Druid的统计分析系统架构图
TDW数据仓库）
Spark
http发起入库任务
realtime
pIna
，比如计算访客的UV数。
数据上报
TDBANK(消息队列）
Spark Streaming
tanguity-spark
269
---
## Page 294
离线方式，即每日离线清洗处理数据后批量写人Druid。离线任务Schema定义如下：
3.实现方式
2.业务需求
好友等行为。通过分析访客的行为数据，帮助企业分析出客户的转化漏斗。
会上报每次浏览、点击接待组件的行为，而后端服务则会上报是否产生接待、是否有添加为
码和腾讯企点接待组件（一种呼起QQ会话的按钮）部署在网页中。当访客访问网页时前端
用户并分析客户的行为和属性。使用腾讯企点分析产品的客户，会将腾讯企点的监测分析代
1．业务背景
10.3.2业务实践
复，即离线部分保障了Druid中的数据总是可以被重新计算和恢复。
源维度，或者相关字段的计算方式发生了变化，那么也可以利用离线方式对历史数据进行修
务按天级别对前一天的数据做天粒度的Roll-up处理可进一步压缩存储数据）。若增加了数据
270
'type"
由于每日上报的数据来源较多，且数据之间在实时性匹配上也存在着困难，故我们采用
腾讯企点是腾讯出品的SCRM产品，利用它可帮助企业实现客户接待和管理来访的QQ
"ioConfig"：
上述统计数据从大到小、从粗到细形成漏斗状，因此被称为客户分析漏斗。
·有多少访客被转化为好友，并且每一步都需要支持统计新老访客数。
·有多少访客点击接待组件时不在好友表中。
·这段时间内的访客数。
该产品支持选取任意时间段和指定网站并统计出各个推广渠道的如下数据。
·在访问的客户中点击接待组件的访客数。
"type":"hadoop"
inputSpec":{
"paths"："hdfs://${集群地址}/olap/visitor_stat/"
"type":"static",
"index_hadoop"
Druid实时大数据分析原理与实践
---
## Page 295
第10章
"dataSchema"
"dataSource":"visitor_statistics",
"parser"
granularitySpec":
"type”:
"intervals":["2016-08-01T00:00:00+08:00/2016-08-02T00:00:00+08:00"]
"type”:“uniform",
"queryGranularity":
"segmentGranularity"
实践和应用
'parseSpec":{
'dimensionsSpec"
"format”:“json",
'timestampSpec":
"column"
"format":"auto",
"dimensionExclusions":[]
"dimensions"
"in_customerdb_after_talk"
"ad_content"
"ad_term",
"ad_source"
"is_sem",
"is_ad",
"is_new",
"device_type"
"corpuin"
"tid",
'in_customerdb_before_click",
'is_click",
'ad_campaign"
ad_media",
"hadoopyString",
"："timestamp"
1：
"day"
{-
"day"
271
---
## Page 296
272
'tuningConfig":{
"jobProperties"：【//MapReduce任务优化参数
"cleanupOnFailure":false,
"maxRowsInMemory":100000,
"partitionsSpec":
"type": "hadoop",
"metricsSpec":[
"mapreduce.tasktracker.http.threads":20,
"mapreduce.reduce shuffle, input .buffer.percent": 0.5,
"mapreduce.reduce.java.opts":"-server -Xmx2560m -Duser.timezone=UTC+0800 -Dfile.
"mapreduce.reduce.memory.mb":6144,
"mapreduce.map.java.opts":"-server
"mapreduce.map.memory.mb":2048,
"targetPartitionSize": 5000000
"type":"hashed",
"mapreduce.output.fileoutputformat.compress":false,
"mapreduce.jobtracker.handler.count":64,
'mapreduce.task.io.sort.factor":100,
mapreduce.task.io.sort.mb":250,
'mapreduce.reduce.shuffle.parallelcopies":50,
'mapreduce.job.jvm.numtasks":20,
'mapreduce.job.reduces":21,
encoding=UTF-8",
encoding=UTF-8",
"fieldName":"qidianid"
"type" : "hyperUnique",
'name"：
"type"
"name"
"count”
："count"
"uv",
-Xmx1536m-Duser.timezone=UTC+0800 -Dfile.
Druid实时大数据分析原理与实践
---
## Page 297
计指标因为入库时做了预处理，故查询性能也十分优异。
分析服务的客户众多，每个客户的访客和点击数据都入库将导致原始数据集十分庞大。但是
ad_source、ad_media、ad_campaign等表示各个广告推广渠道。
表示该次访问在不是好友的情况下是否被转化为好友。这几个字段的取值为0：否，1：是。
击了接待组件，in_customerdb_before_click表示点击时是否是好友，in_customerdb_after_talk
Druid，每一条人库的记录表示一次访问。其中，is_new表示是否为新访客，is_click表示是否点
第10章实践和应用
大减少。灰度阶段每天约1.2GB的原始数据经过聚合处理后只有200KB左右，同时相应的统
由于使用了Druid来统计各个阶段的访客数，在每日数据入库时做了roll-up处理后数据量大
通过groupBy方式进行统计查询，查询语句如下：
虽然经过了数据清洗，但是由于上报数据时每一条访问记录都要存储，且使用腾讯企点
从上述Schema可以看出，我们将离线清洗后的数据存入HDFS，再将入库任务导人
"dimensions":[
"dataSource":"visitor_statistics"
"aggregations":
"ad_source"
"mapreduce.task.timeout": 1800000
"ad_campaign"
"mapreduce.reduce.speculative":false,
"mapreduce.map.speculative":false,
'mapreduce.map.output.compress.codec":"org.apache.hadoop.io.compress.
'mapreduce.map.output.compress": true,
"mapreduce.output.fileoutputformat.compress.codec":"org.apache.hadoop.io
DefaultCodec";
"compress.DefaultCodec",
"type":"hyperUnique""
"name":“uv"
"fieldName":"uv",
273
---
## Page 298
274
查询结果如下：
"queryType"”:“groupBy"
"intervals":[
"granularity":"all"
'filter":{
"timestamp"
"version"
"2016-08-20T00:00:00.000Z/2016-08-25T00:00:00.000Z"
"in_customerdb_before_click":"0",
"is_new"："0",
"fields":
"in_customerdb_after_talk":
"uv":18351.85859471761,
"ad_media":null,
"type":"and"
"in_customerdb_after_talk"
"in_customerdb_before_click",
"is_click",
"is_new",
"ad_media"
"value":
"type":"selector"，
"dimension":"corpuin"，
"value":"xxx.xxx.com"
"type":"selector",
"dimension":"host"
"2016-08-20T00:00:00.000Z",
"v1",
"xXX"
“0”，
Druid实时大数据分析原理与实践
---
## Page 299
分析漏斗各层对应的数值。
这些ID字段在Druid人库时分别设置对应到不同的Metric上，这样一次查询即可直接统计
->before_click_indb_qidianid，in_customerdb_after_talk-> after_talk_indb_qidianid），然后将
字段分别洗出多份元余的ID字段（is_click->click_wpa_qidianid，in_customerdb_before_click
4.优化方案
ad_media去掉即可，在此不再复述。
广渠道的漏斗，还需要统计汇总信息，那么只需要将groupBy中的ad_source、ad_campaign、
出来的结果集并不大。最终业务侧对查询结果集进行适当处理，即可得出漏斗。除了各个推
第10章实践和应用
做处理，但是在查询效率和使用上仍然有改进的空间。即：离线清洗每条记录时，可根据标识
'spec":{
"type"
"ioConfig"
优化后的入库Schema定义如下：
该业务涉及多层漏斗的计算，虽然应用层可以利用GroupBy操作筛选出所有组合后再
用于groupBy的字段虽然有7个，但是由于每个字段的值域（基数）很小，因此最后查
dataSchema:
"dataSource":"visitor_statistics"
"type":"hadoop"
"granularitySpec"：{
"inputSpec":{
"intervals":["2016-08-28T00:00:00+08:00/2016-08-29T00:00:00+08:00"]
"type”:"uniform"
"paths"："hdfs://${集群地址}/olap/visitor_stat/"
"type”:“static"
："index_hadoop"
"ad_campaign": null
"queryGranularity":"day",
"segmentGranularity"
"ad_source":null,
"is_click":"0",
："day"
275
---
## Page 300