scribed by a set of symbols, which the server uses to
compute a symbolic expression for the value of every
tainted memory location or register. When the execution
of the program reaches Line 1 in Figure 2, the values
of the ﬁrst four bytes on the request need to be revealed
so as to determine the branch the execution should fol-
low. For this purpose, the server sends the client a ques-
tion: “B1B2B3B4 = ‘POST’?”, where Bj represents the
jth byte on the request. The client checks its privacy
policies, which deﬁnes the maximal number of bits of
information allowed to be leaked for individual HTTP
ﬁeld. In this case, the client is permitted to reveal the
keyword POST that is deemed nonsensitive. The server
then ﬁlls the empty request with these letters and moves
on to the branch determined by the client’s answer. The
instruction on Line 2 calls malloc. The function ac-
cesses memory using a pointer built upon the content of
Content-Length, which is tainted. To enable this
memory access, the server sends the symbolic expression
of the pointer to the client to query its concrete value.
The client’s reply allows the server to add more bytes to
the request it is working on. Finally, the execution hits
Line 3, a loop to move request content to the buffer al-
located through malloc. The loop is identiﬁed by the
server from its repeated instruction pattern. Then, a ques-
tion is delivered to the client to query its exit condition:
“ where is the ﬁrst byte Bj = ‘\n’?”. This question con-
cerns request content, a ﬁeld on which the privacy poli-
294 
17th USENIX Security Symposium 
USENIX Association
cies forbid the client to leak out more than certain amount
of information. Suppose that threshold is 5 bytes. To an-
swer the question, only one byte needs to be given away:
the position of the byte ‘\n’. Therefore, the client an-
swers the question, which enables the server to construct
a new packet to reproduce the crash.
The performance of an analysis can be improved by
sending the server an initial report with all the ﬁelds
that are deemed nonsensitive according the user’s privacy
policies. In the example, these ﬁelds include keywords
such as ‘POST’ and the Content-Length ﬁeld. This
treatment reduces the communication overheads during
an analysis.
Threat model. We assume that the user trusts the in-
formation provided by the server but does not trust her
data with the server. The rationale behind this assump-
tion is based upon the following observations. The own-
ers of the server are often software manufacturers, who
have little incentive to steal their customers’ information.
What the user does not trust is the way in which those
parties manage her data, as improper management of the
data can result in leaks of her private information. Ac-
tually, the same issue is also of concern to those owners,
as they could be reluctant to take the liability for protect-
ing user information. Therefore, the client can view the
server as a benign though unreliable partner, and take ad-
vantage of the information it discovers from the vulner-
able program to identify sensitive data, which we elabo-
rate in Section 3.2.
Note that this assumption is not fundamental to Pana-
lyst: more often than not, the client is capable of identi-
fying sensitive data on its own. As an example, the afore-
mentioned analysis on the program in Figure 2 does not
rely on any trust in the server. Actually, the assumption
only serves an approach for deﬁning ﬁne-grained privacy
policies in our research (Section 3.2), and elimination of
the assumption, though may lead to coarser-grained poli-
cies under some circumstances, will not invalidate the
whole approach.
3.2 Panalyst Client
Panalyst client is designed to work on the computing
devices with various resource constraints. Therefore, it
needs to be extremely lightweight. The client also in-
cludes a set of policies for protecting the user’s privacy
and a mechanism to enforce them. We elaborate such a
design as follows.
Packet logging and error reporting. Panalyst client in-
tercepts the packets received by an application, extracts
their application-level payloads and saves them to a log
ﬁle. This can be achieved either through capturing pack-
ets at network layer using a sniffer such as Wireshark [1],
or by interposing on the calls for receiving data from net-
work. We chose the latter for prototyping the client: in
our implementation, an application’s socket calls are in-
tercepted using ptrace [10] to dump the application-
level data to a log. The size of the ﬁle is bounded, and
therefore only the most recent packets are kept.
When a serious runtime error happens, the process of
a vulnerable program may crash, which triggers our error
analysis mechanism. Runtime errors can also be detected
by the mechanisms such as GLIBC error detection, Win-
dows build-in diagnostics [11] or other runtime error de-
tection techniques [28, 21]. Once an error happens to
an application, Panalyst client identiﬁes the packets it is
working on. This is achieved in our design by looking at
all the packets within one TCP connection. Speciﬁcally,
the client marks the beginning of a connection once ob-
serving an accept call from the application and the end
of the connection when it detects close. After an ex-
ception happens, the client concatenates the application-
level payloads of all the packets within the current con-
nection to form a message, which it uses to talk to the
server. For simplicity, our current design focuses on the
error triggered by network input and assumes that all
information related to the exploit is present in a single
connection. Panalyst can be extended to handle the er-
rors caused by other inputs such as data from a local
ﬁle through logging and analyzing these inputs. It could
also work on multiple connections with the support of
the state-of-art replay techniques [43, 32] that are capa-
ble of replaying the whole application-layer session to
the vulnerable application on the server side. When a
runtime error occurs, Panalyst client notiﬁes the server
of the type of the error, for example, segmentation fault
and illegal instruction. Moreover, the client can ship to
the server part of the message responsible for the error,
given such information is deemed nonsensitive according
to the user’s privacy policies.
After reporting to the server a runtime error, Panalyst
client starts listening to a port to wait for the questions
from the server. Panalyst server may ask two types of
questions, either related to a tainted branching condi-
tion or a tainted pointer a vulnerable program uses to
access memory. In the ﬁrst case, the client is supposed
to answer “yes” or “no” to the question described by a
symbolic inequality: C(Bk[1], . . . , Bk[m]) ≤ 0, where
Bk[j] (1 ≤ j ≤ m) is the symbol for the k[j]th byte
on the causal message. In the second case, the client is
queried about the concrete value of a symbolic pointer
S(Bk[1], . . . , Bk[m]). These questions can be easily ad-
dressed by the client using the values of these bytes on
the message. However, the answers can be delivered to
the server only after they are checked against the user’s
privacy policies, which we describe below.
USENIX Association  
17th USENIX Security Symposium 
295
Privacy policies. Privacy policies here are designed to
specify the maximal amount of information that can be
given away during an error analysis. Therefore, they
must be built upon a proper measure of information.
Here, we adopt entropy [48], a classic concept of infor-
mation theory, as the measure. Entropy quantiﬁes uncer-
tainty as number of bits. Speciﬁcally, suppose that an
application ﬁeld A is equally likely to take one of m dif-
ferent values. The entropy of A is computed as log2 m
bits. If the client reveals that A makes a path condition
true, which reduces the possible values the ﬁeld can have
to a proportion ρ of m, the exposed information is quan-
tiﬁed as: log2 m − log2 ρm = − log2 ρ bits.
The privacy policies used in Panalyst deﬁne the max-
imal number of bytes of the information within a pro-
tocol ﬁeld that can be leaked out. The number here is
called leakage threshold. Formally, denote the leakage
threshold for a ﬁeld A by τ. Suppose the server can in-
fer from the client’s answers that A can take a proportion
ρ of all possible values of that ﬁeld. The privacy pol-
icy requires that the following hold: − log2 ρ ≤ τ. For
example, a policy can specify that no more than 2 bytes
of the URL information within an HTTP request can be
revealed to the server. This policy design can achieve
a ﬁne-grained control of information. As an example,
let us consider HTTP requests: protocol keywords such
as GET and POST are usually deemed nonsensitive, and
therefore can be directly revealed to the server; on the
other hand, the URL ﬁeld and the cookie ﬁeld can be
sensitive, and need to be protected by low leakage thresh-
olds. Panalyst client includes a protocol parser to parti-
tion a protocol message into ﬁelds. The parser does not
need to be precise: if it cannot tell two ﬁelds apart, it just
treats them as a single ﬁeld.
A problem here is that applications may use closed
protocols such as ICQ and SMB whose speciﬁcations are
not publically available. For these protocols, the whole
protocol message has to be treated as a single ﬁeld, which
unfortunately greatly reduces the granularity of control
privacy policies can have. A solution to this problem is to
partition information using the parameters of API (such
as Linux kernel API, GLIBC or Windows API) functions
that work on network input. For example, suppose that
the GLIBC function fopen builds its parameters upon
an input message; we can infer that the part of the mes-
sage related to ﬁle access modes (such as ‘read’ and
‘write’) can be less sensitive than that concerning ﬁle
name. This approach needs a model of API functions and
trust in the information provided by the server. Another
solution is to partition an input stream using a set of to-
kens and common delimiters such as ‘\n’. Such tokens
can be speciﬁed by the user. For example, using the to-
ken ‘secret’ and the delimiter ‘.’, we can divide the
URL ‘www.secretservice.gov’ into the follow-
ing ﬁelds: ‘www’, ‘.’, ‘secretservice’ and ‘gov’.
Upon these ﬁelds, different leakage thresholds can be de-
ﬁned. These two approaches can work together and also
be applied to specify ﬁner-grained policies within a pro-
tocol ﬁeld when the protocol is public.
To facilitate speciﬁcation of the privacy policies, Pan-
alyst can provide the user with policy templates set by
the expert. Such an expert can be any party who has the
knowledge about ﬁelds and the amount of information
that can be disclosed without endangering the content of
a ﬁeld. For example, people knowledgeable about the
HTTP speciﬁcations are in the position to label the ﬁelds
like ‘www’ as nonsensive and domain names such as
‘secretservice.gov’ as sensitive. Typically, pro-
tocol keywords, delimiters and some API parameters can
be treated as public information, while the ﬁelds such
as those including the tokens and other API parameters
are deemed sensitive. A default leakage threshold for
a sensitive ﬁeld can be just a few bytes: for example,
we can allow one or two bytes to be disclosed from a
domain-name ﬁeld, because they are too general to be
used to pinpoint the domain name; as another example,
up to four bytes can be exposed from a ﬁeld that may
involve credit-card numbers, because people usually tol-
erate such information leaks in real life. Note that we
may not be able to assign a zero threshold to a sensitive
ﬁeld because this can easily cause an analysis to fail: to
proceed with an analysis, the server often needs to know
whether the ﬁeld contains some special byte such as a
delimiter, which gives away a small amount of informa-
tion regarding its content. These policy templates can be
adjusted by a user to deﬁne her customized policies.
Policy enforcement. To enforce privacy policies, we
need to quantify the information leaked by the client’s
answers. This is straightforward in some cases but less
so in others. For example, we know that answering ‘yes’
to the question “B1B2B3B4 = ‘POST’?” in Figure 2
gives away four bytes; however, information leaks can
be more difﬁcult to gauge when it comes to the ques-
tions like “Bj × Bk < 256?
”, where Bj and Bk
indicates the jth and the kth bytes on a message re-
spectively. Without loss of generality, let us consider a
set of bytes (Bk[1], . . . , Bk[m]) of a protocol message,
whose concrete values on the message makes a condi-
tion “C(Bk[1], . . . , Bk[m]) ≤ 0” true. To quantify the
information an answer to the question gives away, we
need to know ρ, the proportion of all possible values
these bytes can take that make the condition true. Find-
ing ρ is nontrivial because the set of the values these
bytes can have can be very large, which makes it im-
practical to check them one by one against the inequal-
ity. Our solution to the problem is based upon the classic
statistic technique for estimating a proportion in a popu-
296 
17th USENIX Security Symposium 
USENIX Association
lation. Speciﬁcally, we randomly pick up a set of values
for these bytes to verify a branching condition and re-
peat the trial for n times. From these n trials, we can
estimate the proportion ρ as x
n where x is the number
of trials in which the condition is true. The accuracy of
this estimate is described by the probability that a range
of values contain the true value of ρ. The range here
is called conﬁdence interval and the probability called
conﬁdence level. Given a conﬁdence interval and a con-
ﬁdence level, standard statistic technique can be used to
determine the size of samples n [2]. For example, sup-
pose the estimate of ρ is 0.3 with a conﬁdence inter-
val ±0.5 and a conﬁdence level 0.95, which intuitively
means 0.25 < ρ < 0.35 with a probability 0.95; in
this case, the number of trials we need to play is 323.
This approach offers an approximation of information
leaks: in the prior example, we know that with 0.95 con-
ﬁdence, information being leaked will be no more than
− log2 0.25 = 4 bits. Using such an estimate and a pre-
determined leakage threshold, a policy enforcer can de-
cide whether to let the client answer a question.
3.3 Panalyst Server
Panalyst server starts working on a vulnerable applica-
tion upon receiving an initial error report from the client.
The report includes the type of the error, and other non-
sensitive information such as the corrupted pointer, the
lengths of individual packets’ application-level payloads
and the content of public ﬁelds. Based upon it, the server
conducts an instruction-level analysis of the application’s
executable, which we elaborate as follows.
Taint analysis and symbolic execution. Panalyst server
performs a dynamic taint analysis on the vulnerable pro-
gram, using a network input built upon the initial re-
port as a taint source. The input involves a set of pack-
ets, whose application-layer payloads form a message
characterized by the same length as the client’s message
and the information disclosed by the report. The server
monitors the execution of the program instruction by in-
struction to track tainted data according to a set of taint-
propagation rules. These rules are similar to those used
in other taint-analysis techniques such as RIFLE [51],
TaintCheck [44] and LIFT [45], examples of which are
presented in Table 1. Along with the dynamic analysis,
the server also performs a symbolic execution [37] that
statically evaluates the execution of the program through
interpreting its instructions, using symbols instead of real
values as input. Each symbol used by Panalyst represents
one byte on the input message. Analyzing the program
in this way, we can not only keep close track of tainted
data ﬂows, but also formulate a symbolic expression for
every tainted value in memory and registers.
Whenever the execution encounters a conditional
branching with its condition tainted by input symbols,
the server sends the condition as a question to the client
to seek answer. With the answer from the client, the
server can ﬁnd hypothetic values for these symbols using
a constraint solver. For example, a “no” to the question
Bi = ‘\n’ may result in a letter ‘a’ to be assigned to the
ith byte on the input. To keep the runtime data consis-
tent with the hypothetic value of symbol Bi, the server
updates all the tainted values related to Bi by evaluat-
ing their symbolic expressions with the hypothetic value.
It is important to note that Bi may appear in multiple
branching conditions (C1 ≤ 0, . . . , Ck ≤ 0). Without
loss of generality, suppose all of them are true. To ﬁnd
a value for Bi, the constraint solver must solve the con-
straint (C1 ≤ 0)∧ . . .∧ (Ck ≤ 0). The server also needs
to “refresh” the tainted values concerning Bi each time
when a new hypothetic value of the symbol comes up.
The server also queries the client when the program
attempts to access memory through a pointer tainted by
input symbols (Bk[1], . . . , Bk[m]). In this case, the server
needs to give the symbolic expression of the pointer
S(Bk[1], . . . , Bk[m]) to the client to get its value v, and
solve the constraint S(Bk[1], . . . , Bk[m]) = v to ﬁnd
these symbols’ hypothetic values. Query of a tainted
pointer is necessary for ensuring the program’s correct
execution, particularly when a write happens through
such a pointer. It is also an important step for reliably