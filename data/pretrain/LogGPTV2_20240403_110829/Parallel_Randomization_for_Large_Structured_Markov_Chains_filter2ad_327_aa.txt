title:Parallel Randomization for Large Structured Markov Chains
author:Peter Kemper
Parallel Randomization for Large Structured Markov Chains
Peter Kemper
Informatik IV
Universit¨at Dortmund
D-44221 Dortmund, Germany
PI:EMAIL
Abstract
Multiprocessor architectures with few but powerful proces-
sors gain more and more popularity. In this paper we de-
scribe a parallel iterative algorithm to perform randomiza-
tion for a continuous time Markov chain with a Kronecker
representation on a shared memory architecture. The Kro-
necker representation is modiﬁed for a parallel matrix-
vector multiplication with a fast multiplication scheme and
no write conﬂicts on iteration vectors. The proposed tech-
nique is applied on a model of a workstation cluster for de-
pendability analysis, corresponding computations are per-
formed on two multiprocessor architectures, a Sun enter-
prise and a SGI Origin 2000 to measure its performance.
1. Introduction
Numerical analysis of stochastic models, especially Marko-
vian models, is relevant for performance and dependability
issues. However, the frequently observed state space explo-
sion often prohibits its application. Many results have been
proposed to avoid or tolerate large state spaces in numer-
ical analysis, especially in the context of continuous time
Markov chains (CTMCs).
In largeness avoidance, most approaches reduce the dimen-
sion of a CTMC according to lumpability. Approaches dif-
fer in the way, lumpability is detected or established to re-
duce a model, e.g., symmetries are immediately apparent by
the way a model is speciﬁed as for stochastic well-formed
nets [10] and replicate/join structures in stochastic activ-
ity networks [28] or equivalence classes are computed by
a bisimulation based on lumpability as for stochastic pro-
cess algebras. Reduced models or CTMCs nevertheless can
still retain an extremely large number of states. In large-
ness tolerance approaches, the goal is to obtain a space-
efﬁcient representation that still allows for reasonably fast
This research is partially supported by DFG, SFB 559
computations, e.g., by multi-terminal binary decision dia-
grams [22, 24], matrix diagrams [11], and Kronecker repre-
sentations [4, 6, 18]. Space-efﬁcient approaches often trade
space for time.
In addition to space considerations, time is also of impor-
tance. The analysis of large CTMCs is usually based on ﬁx-
point iterations. Approaches that aim at fast computations
either use more efﬁcient techniques for elementary opera-
tions like a matrix-vector multiplication or propose different
solution methods to achieve a better speed of convergence,
e.g., as in aggregation/disaggregation methods and Krylov
subspace methods to name a few. Parallel approaches are
somehow complementary, since a parallel solution method
and its corresponding implementation has to solve speciﬁc
obstacles in parallel computations but it can evolve from
existing, possibly sequential solution methods.
The ﬁx-point iterations used in transient and steady state
analysis have matrix-vector multiplications as an elemen-
tary operation, and many algorithms exist to perform a
matrix-vector multiplication in parallel. For numerical anal-
ysis, a distributed memory architecture can be used to parti-
tion a large generator matrix among a set of processors with
own main memory, that in total exceeds the memory avail-
able on a single machine. This effect is the basis for dis-
tributed approaches that use an explicit storage of a CTMC
by sparse matrices as, e.g., in [25, 26]. In [25], Knottenbelt
et al report on a disk-based solver for a distributed mem-
ory parallel computer based on a distributed matrix-vector
multiplication kernel. They consider Jacobi and Conjugate
Gradient Squared solution methods for steady state analysis
and solved models with over 50 million states on 16-node
Fujitsu AP 3000 machine. In [26], Migallon et al focus on
block iterative methods on a distributed memory multipro-
cessor system for the solution of a linear equation system,
i.e., mainly for stationary analysis of CTMCs, where block-
two-stage methods can be advantageous. Approaches as
[29] on shared memory architectures have to sufﬁce with
a single memory space, so experiments of Sidje and Stew-
art for transient solutions of CTMCs on a Power Challenge
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:25:39 UTC from IEEE Xplore.  Restrictions apply. 
Array supercomputer by different solvers (Krylow-based
methods, ordinary differential equation solvers) consider
relative small state spaces, e.g., up to 91; 881 states in [29].
A key advantage of a Kronecker representation is its space-
efﬁciency, so it is a potential candidate for shared mem-
ory approaches. Nevertheless, only its stationary analysis
on a distributed memory architecture, namely a workstation
cluster, has been considered in [7, 19], where the focus is
on minimizing communication and experimenting with syn-
chronous and asynchronous block-two-stage iterative meth-
ods for stationary analysis.
In this paper, we focus on randomization, which is used to
evaluate CTMCs for transient analysis, dependability anal-
ysis and as an underlying method also for model checking
CTMCs. In particular, efﬁcient algorithms to evaluate time-
bounded until-formulas in the continuous stochastic logic
CSL are based on randomization [1, 24]. The paper is struc-
tured as follows. Section 2 notes typical requirements for
the design of a parallel approach based on multithreading.
Section 3 brieﬂy recalls randomization based on Kronecker
representations.
In this section, we identify the potential
for parallel computations as well as potential read and write
conﬂicts. Subsequently, we propose a parallel randomiza-
tion procedure for a shared memory architecture in Section
4. Section 5 gives some implementation details and Section
6 illustrates the performance of the algorithm by help of an
example.
2. Requirements
Before we go into details of our speciﬁc approach we brieﬂy
recall design issues and requirements to make a parallel al-
gorithm work efﬁciently on a shared memory architecture
with multithreading.
Correctness. Apart from the general requirement that an
algorithm needs to come up with correct results, it is of par-
ticular importance to exclude interference in the context of
shared memory. Interference is a destructive update caused
by the arbitrary interleaving of read and write operations on
shared variables.
Hardware Independence, Portability. A parallel algorithm
should abstract from particularities of hardware and should
apply on a class of architectures to be of general interest
and to be applicable in a tool environment for dependabil-
ity or performance analysis, like the APNN toolbox [9] or
M¨obius [13]. Clearly, hardware independence is in conﬂict
with maximum performance. In our case, we will rely on a
standardized thread application programming interface, the
POSIX threads, to allow for portability and broader appli-
cability.
Scalability. Although many modern multiprocessor archi-
tectures contain only few processors, a useful, applicable
approach must scale over the number of processors well.
The proposed technique aims for a range of processors in
the order of 2; 4; : : :; 64.
Robustness. A robust parallel algorithm performs indepen-
dently of the current load situation even in a non-exclusive
use of a multiprocessor architecture. This implies that no
assumptions are made on the speed by which each parallel
tasks proceeds individually. Furthermore, it can be advan-
tageous to create slightly more threads than processors to
give an operating system sufﬁcient possibilities for thread
scheduling.
Performance. A parallel algorithm must perform better than
its sequential alternative. A speedup that is almost linear in
the number of processes and an efﬁciency that is close to
1 are goals in parallel algorithms, which nevertheless are
rarely achieved.
In a multithreading context, the follow-
ing aspects are relevant to achieve a considerable perfor-
mance. Read and write conﬂicts on shared variables require
to enforce a sequential access to shared memory among
threads, which causes administrative overhead and conges-
tion at worst. Hence, the design of an algorithm should aim
at minimizing sequentialization if possible. The degree of
parallelism is expressed by the number of threads that are
in use. However, threads impose overhead in terms of space
and time for their administration. Hence, the selection of the
number of threads needs to reﬂect the degree of parallelism
in the given problem but also the abilities in parallel com-
putation that are available at a given hardware platform. Fi-
nally, the treatment of threads differs among operating sys-
tems signiﬁcantly. An implementation needs to take care
of the way user-level threads are matched with kernel-level
threads or processes.
In summary, we aim at a robust, space- and time-efﬁcient
algorithm that applies to various hardware platforms and
whose implementation allows to port the code to different
platforms with limited effort.
3. Randomization using Kronecker represen-
tations
Randomization - also called uniformization or Jensen’s
method - goes back to the work of [23], some variations
such as adaptive uniformization have been proposed. For
a given continuous time Markov chain (CTMC) random-
ization computes the transient probability distribution (cid:25)
for a given time horizon  and with respect to a given dis-
tribution (cid:25)0 at an initial point of time 0. We brieﬂy re-
call classic, known results for randomization and ﬁx some
notations ﬁrst. Let S denote the ﬁnite set of  states of a
CTMC. State transitions occur according to actions with la-
bels of some ﬁnite non-empty set . The generator matrix
	 of a CTMC is a real-valued jSj  jSj matrix where
	 = R   diagR  1T  such that R contributes the rates
of state transitions and diagR  1T  is a diagonal matrix
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:25:39 UTC from IEEE Xplore.  Restrictions apply. 
that gives the row sums of R, i.e., the holding times of each
state. Let (cid:21) = axifj	i; ijg be the maximal departure
rate, then  =   (cid:21) 1  	 is a probabilistic matrix. Ran-
domization is based on the following equation
(cid:25) = (cid:25)0 
e (cid:21) (cid:21)  k
k!
k:
1
Xk=0
(1)
The values of e (cid:21) (cid:21)k
in fact describe probabilities of a
k!
Poisson distribution, so let i(cid:21); ; k = e (cid:21) (cid:21)k
k!
denote this probability. A numerically stable procedure to
compute these Poisson probabilities even for large values of
(cid:21)   is given in [20]. Eq. 1 is typically iteratively computed
as
(cid:25)0
xk = (cid:26) xk 1   if k > 0
yk = (cid:26) yk 1  i(cid:21); ; k  xk
i(cid:21); ; 0  (cid:25)0
if k = 0
(2)
(3)
if k > 0
if k = 0
1   1
the remaining probability mass 1   1
so xk = (cid:25)0  k and yk is the sum of the ﬁrst k terms
in Eq. 1. Distribution (cid:25) is approximatively computed
in a ﬁnite number of  steps either if the iteration reaches
a ﬁx point, i.e., if x = x 1 and (cid:25) = y 1 
k=0 i(cid:21); ; k  x or by (cid:25) (cid:25) y if
k=0 i(cid:21); ; k
drops below some given truncation error (cid:15). Note that Eq. 2
allows for arbitrary representations of  provided a matrix-
vector multiplication is deﬁned. In the following, we will
represent  by a Kronecker representation and discuss how
the required matrix-vector multiplication can be performed
in parallel. A Kronecker representation is a compositional
representation of  by a set of matrices of smaller dimen-
sions that are combined by Kronecker operators, mainly by
a Kronecker product. The Kronecker product of a   
matrix A with a k    matrix B is a   k      matrix
and c1 = a1  k  b1, c2 = a2     b2. The Kro-
C = A B with entries Cc1; c2 = Aa1; a2Bb1; b2
necker sum C = A B of a    matrix A and a
k  k matrix B is deﬁned as C = A B  A B
with identity matrices A of dimension   , B of di-
mension k  k. The generalization to sums and prod-
ucts of more than two terms is straightforward. The ma-
trix operators have a number of known algebraic properties,
e.g., the distributivity of Kronecker product over addition
A  B C = A C  B C will be used in the
proof of Lemma 4.1.
A Kronecker representation requires that a CTMC is built
from a set of  components where each component i has
a state space S i and jS ij  jS ij transition matrices Ri.
Components allow to distinguish state transitions according
to labels, so  = [
  . A ma-
trix Wi
  describes all state transitions according to a speciﬁc
i=1i and Ri = 2i !   Wi
action or transition with label   and rate ! ; a nonzero matrix
entry describes the existence of a state transition, the numer-
ical value can be used to account for a state-dependent scal-
ing factor. Components are considered as ﬁnite-state pro-
cesses that proceed asynchronously and in parallel. Labels
can be shared by components, so communication among
components takes place by a synchronization over common
labels, i.e., transitions of different components but with the
same label can take place only in conjunction. Kronecker
representations have been formulated for different model-
ing formalisms including superposed generalized stochastic
Petri nets (SGSPNs)[17], process algebras [3] and stochas-
tic automata networks[6, 18]. Starting from the pionieering
work of Plateau and coworkers, a certain variety of Kro-
necker representations has been developed. Without div-
ing into such details, we follow a common approach as for
SGSPNs, that is also considered in [6]. A Kronecker repre-
sentation represents R by
(cid:22)R =X 2
! 

i=1
Wi
  :
(4)
where Wi
  =  of dimension jS ij  jS ij if   62 i. The
scalar value !  gives the rate of the action, while matrix en-
  provide scaling factors. (cid:22)R gives a jSjjSj
tries of Wi
matrix with S = 
i=1S i. In the literature, those terms
in the summation of Eq. 4 that consist of Kronecker prod-
ucts with all matrices Wi
  equal to identity matrices but one,
are separated from the sum and transformed into an (alge-
braically equivalent) Kronecker sum. This special case is
not distinguished at this point to keep the argumentation
more simple. It is known that S (cid:18) S but in many models
jSj << jSj, so R is only a submatrix of (cid:22)R and algorithms
have been developed to perform a matrix-vector multiplica-
tion only on the relevant submatrix R = (cid:22)R[S;S], [6]. By
(cid:22)R[S;S 0] we denote the submatrix of (cid:22)R that consists of rows
S and columns S 0, this notation is used for matrices and
vectors accordingly. One way to restrict an iterative proce-
dure only on a relevant (or interesting) submatrix of R is to
employ directed acyclic graphs (DAGs) or multi-value deci-
sion diagrams [12]. Without an extensive formal treatment,
we make use of this approach and assume that a Kronecker
representation as in Eq. 4 and a DAG of S are given; al-
gorithms to derive these structures for different modeling
formalisms are known, for SGSPNs see, e.g., [11, 12].
A representation of 	 = (cid:22)R[S;S]  diag (cid:22)R[S;S] 1T  follows
from Eq. 4, and for randomization we obtain by substitution
 =   (cid:21) 1	 = D  X 2
(cid:22) 
Wi
 !

i=1
[S;S]
(5)
with D =    (cid:21) 1diag (cid:22)R[S;S]  1T  and (cid:22)  = ! =(cid:21). So
a multiplication of a vector xk 1 with the Kronecker rep-