tem administrators subsequently patched the vulnerable
libpng library.
One might be curious that there was only one vulnerabil-
ity that contributed to the policy violation though the host
workStation actually had four vulnerabilities. The other
three bugs on the workStation are locally exploitable
vulnerabilities in the kernel. Since only trusted users ac-
cess these hosts, after patching the libpng bug our tool
indicates the policy is no longer violated. These ma-
chines have uptimes in the order of months and upgrad-
ing the kernel would require a reboot. Patching these vul-
nerabilities would result in a loss of availability, which
is best avoided. The administrators can meet the secu-
rity goals without patching the kernel and rebooting the
workStation. We expect our tool to be useful in mission-
critical systems like commercial mail servers serving mil-
lions of users and servers running long computations.
5.2 An example multistage attack
We now illustrate how our framework works in the case
of multistage attacks. Let us consider a simulated attack
on the network discussed in the previous example. Sup-
pose the following two vulnerabilities are reported by the
scanner:
vulExists(webServer, ’CVE-2002-0392’,
httpd).
vulExists(fileServer, ’CAN-2003-0252’,
mountd).
Both vulnerabilities are remotely exploitable and can re-
sult in privilege escalation. The corresponding Datalog
clauses from ICAT database are:
vulProperty(’CVE-2002-0392’,
remoteExploit, privEscalation).
vulProperty(’CAN-2003-0252’,
remoteExploit, privEscalation).
The machine and network conﬁguration, principal and
data binding, and the security policy are the same as in
the previous example.
Results The MulVAL reasoning engine analyzed the
input Datalog tuples. The Prolog session transcript is as
follows:
| ?- policyViolation(Adversary,
Access, Resource).
Adversary = attacker
Access = read
Resource = projectPlan;
Adversary = attacker
Access = write
Resource = webPages;
Adversary = attacker
Access = write
Resource = projectPlan;
We show the trace of the ﬁrst violation in Appendix A.
Here we explain how the attack can lead to the policy
violation.
An attacker can ﬁrst compromise webServer by remotely
exploiting vulnerability CVE-2002-0392 to get control of
webServer.
Since webServer is allowed to access
fileServer, he can then compromise fileServer by ex-
ploiting vulnerability CAN-2003-0252 and become root
on the server. Next he can modify arbitrary ﬁles on
fileServer.
Since the executable binaries on
workStation are mounted on fileServer, their integrity
will be compromised by the attacker. Eventually an inno-
cent user will execute the compromised client program;
this will give the attacker access to workStation. Thus
the ﬁles stored on it would also be compromised.
One way to ﬁx this violation is moving webPages to
webServer and blocking inbound access from dmz zone
to internal zone. After incorporating these counter mea-
sures, we ran MulVAL reasoning engine on the new in-
puts and veriﬁed that the security policy is satisﬁed.
USENIX Association
14th USENIX Security Symposium
121
6 Hypothetical analysis
One important usage of vulnerability reasoning tools is
to conduct “what if” analysis. For example, the adminis-
trator would like to ask “Will my network still be secure
if two CERT advisories arrive tomorrow?”. After all, an
important purpose of using ﬁrewalls is to guard against
potential threats. Even there is no known vulnerability
in the network today, one might be discovered tomorrow.
Analysis that can reveal weaknesses in the network under
hypothetical circumstances is useful in improving secu-
rity. Performing this kind of hypothetical analysis is easy
in our framework. We introduce a predicate bugHyp to
represent hypothetical software vulnerabilities. For ex-
ample, following is a hypothetical bug in the web service
program httpd on host webServer.
bugHyp(webServer, httpd,
remoteExploit, privEscalation).
The fake bugs are then introduced into the reasoning pro-
cess.
vulExists(Host, VulID, Prog) :-
bugHyp(Host, Prog, Range, Consequence).
vulProperty(VulID, Range, Consequence) :-
bugHyp(Host, Prog, Range, Consequence).
the security policy of the network, the violation will be
reported by checktwo. Otherwise the network can with-
stand two hypothetical bugs.
7 Performance and Scalability
We measured the performance of our scanner on a Red
Hat Linux 9 host (kernel version 2.4.20-8). The CPU is
a 730 MHz Pentium III processor with 128MB RAM.
The analysis engine runs on a Windows PC with 2.8GHz
Pentium 4 processor with 512MB RAM. We constructed
examples with conﬁgurations similar to the network in
section 5, but with different numbers of web servers, ﬁle
servers and workstations.
To analyze a network in the MulVAL reasoning engine,
one needs to run the MulVAL scanner on each host and
transfer the results to the host running the analysis en-
gine. The scanners can execute in parallel on multiple
machines. The analysis engine then operates on the data
collected from all hosts. Since the functioning of the
scanner is the same on various hosts, we measured the
scanner running time on one host. We measured the run-
ning time for the analysis engine for real and synthetic
benchmarks. The running times (in seconds) are as:
MulVAL scanner
The following Prolog program will determine whether a
policy violation will happen with two arbitrary hypothet-
ical bugs.
MulVAL
reasoning
engine
§5.1
1 host
200 hosts
400 hosts
1000 hosts
2000 hosts
236 s
0.08
0.08
0.22
0.75
3.85
15.8
checktwo(P, Acc, Data, Prog1, Prog2) :-
program(Prog1),
program(Prog2),
Prog1 @< Prog2,
cleanState,
assert(bugHyp(H1, Prog1, Range1, Conseq1)),
assert(bugHyp(H2, Prog2, Range2, Conseq2)),
policyViolation(P, Acc, Data).
The two assert statements introduce dynamic clauses
about hypothetical bugs in two programs (Prolog back-
tracking will cycle through all possible combination of
two programs.). The policy check is conducted with the
existence of the dynamic clauses. If no policy violation is
found, the execution will back track and another two hy-
pothetical bugs (in different two programs) will be tried.
@< is the term comparison operator in Prolog. It ensures a
combination of two programs is tried only once. If there
exist two programs whose hypothetical bugs will break
MulVAL scanner is the time to run the scanner on one
(typically conﬁgured) Linux host; in principle, the scan-
ner can run on all hosts in parallel. The benchmark §5.1
is the real-world 3-host network described in section 5.1.
Each benchmark labeled “n hosts” consists of n similar
Linux hosts, (approximately one third web servers, one-
third ﬁle servers, and one-third workstations), with host
access rules (i.e., ﬁrewalls) similar to §5.1. Our reason-
ing engine can handle networks with thousands of hosts
in less than a minute.
A typical network might have a dozen kinds of hosts:
many web servers, many ﬁle servers, many compute ser-
vers, many user machines. Depending on network topol-
ogy and installed software (e.g., are all the web servers in
the same place with respect to ﬁrewalls, and are they all
122
14th USENIX Security Symposium
USENIX Association
Execution time for hypothetical analysis
Legend:
(#Host, #Prog)
Source1
hosts=200
Data
Data Bind
26
sys admin
Policy
3
sys admin
Principal Bind
sys admin
10
HACL
Smart Firewall 342
Scanner Output OVAL/ICAT 1222
=2000
3004 lines
3
10
3342
12022
273
(1000, 20)
9.2
4.5
(50, 20)
2
0.29
Coverage Our system can reason about privilege es-
calation vulnerabilities and denial of service vulnerabil-
ities. We cannot currently reason about conﬁdentiality
loss or integrity loss vulnerabilities. Overall, we could
reason about 84% of the Red Hat Linux bugs reported
in OVAL. The detailed statistics are (as of January 31,
2005):
1000
100
10
1
)
s
(
e
m
i
t
g
n
i
n
n
u
R
12
(100, 20)
0.59
0
0.1
0.08
0.08
0.08
0.48
1
0.23
0.14
(50, 10)
0.01
Number of hypothetical bugs
Figure 3: Hypothetical analysis. For a network of 1000 hosts
running 20 kinds of installed software, analyzing security assuming the
existence of any 1 unreported vulnerability takes 12 seconds.
OVAL deﬁnitions for Red Hat
Those with PrivEsc or only DoS
Coverage
202
169
84%
running the same software?) it may be possible that each
group of hosts can be treated as one host for vulnerabil-
ity analysis, so that n = 12 rather than n = 12, 000. It
would be useful to formally characterize the conditions
under which such grouping is sound.
To test the speed of our hypothetical analysis, we con-
structed synthesized networks with different numbers of
hosts and different numbers of programs. Each program
runs on multiple machines. Since the hypothetical anal-
ysis goes through all combination of programs to inject
bugs, the running time is dependent on both the num-
ber of programs and the number of hypothetical bugs.
Figure 3 shows the performance with regard to differ-
ent number of hosts, number of programs and number of
injected bugs. The running time increases with the num-
ber of hypothetical bugs, because the analysis engine will
k(cid:1) combinations of programs, where
need to go through(cid:0)n
n is the number of different kinds of programs and k is
the number of injected bugs. k = 0 is the case where no
hypothetical bug is injected. The performance degraded
signiﬁcantly with the increase of k. But it still only takes
273 seconds for k = 2 on a network with 1000 hosts
and 20 different kinds of programs. Since hypothetical
analysis can be performed ofﬂine before the existence of
a bug is known, it is not important to have fast real-time
response time. The degraded performance is acceptable.
Figure 3 shows our system can perform this analysis in a
reasonable time frame for a big network.
The input size to the MulVAL reasoning engine is:
Size of our code base To implement our framework
on Red Hat platform, we adapted the OVAL scanner and
wrote the interaction rules. The size of our code base is:
Module
OVAL scanner
Interaction rules
Original New
13484
668 lines
393
The modularity and simplicity of our design allowed us
to effectively leverage the existing tools and databases by
writing about a thousand lines of code. We note that the
small size and declarative style of our interaction rules
makes them easy to understand and debug. The interac-
tion rules model Unix-style security semantics. We fore-
see that to reason about Windows platforms in addition,
the effort involved is comparable. The rules are indepen-
dent of the vulnerability deﬁnitions.
7.1 Scanning a distributed network
We measured the performance of running the MulVAL
scanner in parallel on multiple hosts. We used PlanetLab,
a worldwide testbed of over 500 Linux hosts connected
via the Internet [20]. We selected 47 hosts in such a way
as to get geographical diversity (U.S., Canada, Switzer-
land, Germany, Spain, Israel, India, Hong Kong, Korea,
1The indicated “Source” shows what person or tool would provide
the information in a real installation; for this benchmark measurement,
we constructed the data synthetically.
USENIX Association
14th USENIX Security Symposium
123
Japan). We were able to log into 39 of these hosts; of
these, we successfully installed the scanner on 33 hosts.5
We ran a script that, in parallel on 33 hosts, opened an
SSH session and ran the MulVAL scanner. We assume
that many hosts were carrying a normal workload, as we
made no attempt to reserve them for this use. The ﬁrst
host responded with data in 1.18 minutes; the ﬁrst 25
hosts responded within 10 minutes; the ﬁrst 29 hosts re-
sponded within 15 minutes; at this point we terminated
the experiment.
For a local area network, we expect fast and uniform re-
sponse time. But for distributed networks, we recom-
mend that scanning be done asynchronously. Each ma-
chine, either when its conﬁguration is known to have
changed or periodically, should scan and report conﬁgu-
ration information. Then, whenever newly scanned data
arrives or whenever new vulnerability data is obtained
from OVAL or ICAT, the reasoning engine can be run
within seconds.
8 Discussion
8.1 Implementing a scanner
Currently the MulVAL scanner is implemented by aug-
menting the standard off-the-shelf OVAL scanner. The
OVAL scanner is overloaded with both the task of col-
lecting machine conﬁguration information and the task
of comparing the conﬁguration with formal advisories to
determine if vulnerabilities exist on a system. The draw-
back of this approach is that when a new advisory comes,
the scanning will have to be repeated on each host. It
would be more desirable if the collection of conﬁgura-
tion information can be separated from the recognition of
vulnerabilities, such that when a new bug report comes,
the analysis can be performed on the pre-collected con-
ﬁguration data.
There are also many other issues related to scanning,
such as how to deal with errors in conﬁguration ﬁles.
A full discussion of conﬁguration scanning is out of the
scope of this paper.