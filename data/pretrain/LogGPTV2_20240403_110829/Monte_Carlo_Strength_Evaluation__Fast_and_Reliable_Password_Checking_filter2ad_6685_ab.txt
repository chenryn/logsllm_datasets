Castelluccia et al. [5] show the design of a strength me-
ter that outputs strength as the probability that a Markov
model would assign to a password; as argued above, prob-
ability does not give a clear information about the number
of guesses needed to break a password: our mechanism can
be integrated in this framework to transform the password
checker’s output from a probability to the number of guesses.
Telepathwords [17] is a system that highlights easy-to-
guess patterns as users type their passwords: we think this
approach is ideally complementary to a good password me-
ter, helping users understand the reason why a password is
assigned a given strength.
3. EVALUATING PASSWORD STRENGTH
State-of-the-art password research is based on probabilis-
tic password models. Such models attempt to characterize
the way humans choose their passwords by constructing a
mapping between strings and the frequency with which hu-
mans are assumed to choose them as passwords. Let us
denote the (possibly inﬁnite) set of all allowed passwords as
Γ; a probabilistic password model is a function p such that
α∈Γ p (α) = 1. We call p (α) the probability of α under p.
Attack models enumerate passwords by descending order
of probability: hence, the strength (or rank ) Sp (α) of a pass-
word α under model p is the number of passwords that have
probability higher than α:
(cid:80)
Sp (α) = |{β ∈ Γ : p (β) > p (α)}| .
(1)
We are interested in approximating Sp (α) eﬃciently and
accurately. In addition to being able to compute p (α), our
only requirement is to be able to generate a sample (with re-
placement) of passwords such that, at any draw, the proba-
bility of choosing password α is exactly p (α). Implementing
this is not diﬃcult for any of the models that we consider in
this work (n-grams, PCFGs and backoﬀ).
In order to use p for a guessing attack, the attacker needs
to enumerate passwords by descending probability; doing
this eﬃciently is an open problem [8, 21]. Our method, in-
stead, relies only on sampling and does not require imple-
menting this enumeration.
3.1 The Method
Evaluating password strength as deﬁned in Equation 1
entails computing the cardinality of the set ∆ ⊂ Γ of all
passwords weaker than α, i.e., ∆ = {β ∈ Γ : p (β) > p (α)}.
Computing Sp (α) exactly would require generating all ele-
ments of ∆, which is obviously impractical if ∆ is large. Our
method approximates Sp (α) eﬃciently based on a subset of
n passwords; we prove its convergence to the true value of
Sp (α) for n going to inﬁnity. In Section 3.2, we show how
this method can be implemented eﬃciently, yielding a pre-
computed probability vs. rank curve.
We generate a sample Θ of n passwords. Passwords are
sampled with replacement and each password β is sampled
with probability p (β). Then, our estimation C∆ for the
160cardinality of ∆ is
C∆ =
(cid:40) 1
p(β)·n
0
(cid:88)
β∈Θ
if p(β) > p(α),
otherwise.
(2)
Note that the values of p (β) can be very low, and these
calculations may overﬂow or underﬂow. In our experiments,
we observed these phenomena with 32-bit ﬂoating point val-
ues, and we avoided them by using 64-bits arithmetic. To
avoid such problems altogether, one may want to resort to
arbitrary-precision numeric implementations.
In the following, we prove two theorems: ﬁrst, the ex-
pected value of C∆ under the attack model p is |∆|, mean-
ing that the expected value of our estimation is indeed the
desired quantity in Equation 1; second, the variance de-
√
creases as the sample size grows, with a convergence rate
n) – the standard for Monte Carlo estimators.
of O (1/
Theorem 1. The expected value of C∆ is |∆| = Sp(α).
Proof. Let us ﬁrst consider the n = 1 case. Here,
control on how to sample from Γ, and we assume the sam-
pling probabilities as given. Moreover, since Γ is extremely
large and possibly inﬁnite, even assuming that uniform sam-
pling was possible, elements from ∆ in the sample would be
very rare or non existent; on the other hand, since ∆ includes
elements with higher probabilities, they are deﬁnitely more
likely to appear in our non-uniform sample.
3.2 Optimized Implementation
The method described so far would require to create a
sample Θ of size n from the model p every time we want to
evaluate the strength of a password α. This is not necessary:
in the following, we show how to precompute the probability-
rank curve, and compute the strength of a new password
simply by performing an O(log n) lookup.
The key idea here is that the sampling step can be per-
formed just once, and it is only necessary to store the prob-
abilities p (βi) for each password βi in Θ. We store the
probabilities in an array A = [p (β1) , . . . , p (βn)] sorted by
descending probability, and we create an array C such that
C’s i-th element is
(cid:40) 1
(cid:88)
β∈Γ
E [C∆] =
p (β)
p(β)
0
if β ∈ ∆
otherwise
=
(cid:88)
β∈∆
1 = |∆| .
C [i] =
1
n
= C [i − 1] +
1
A[j]
1
n · A[i]
.
i(cid:88)
j=1
Theorem 2. The standard deviation of C∆ is O (1/
For n > 1, it is suﬃcient to note that our estimation is the
mean of n estimates done with n = 1. Since all estimates
are i.i.d., the expected value of the average is again |∆|.
√
n).
Proof. The variance of a random variable X is E(cid:2)X 2(cid:3) −
Var (C∆) = E(cid:2)C 2
(E [X])2. For the n = 1 case, we compute the variance as
(cid:3) − E [C∆]2 =
(cid:88)
∆
β∈∆
=
p (β)
p (β)2 − |∆|2
(cid:88)
1
p (β)
β∈∆
− |∆|2 .
We notice that the above value is ﬁnite because, by hypoth-
esis, |∆| is always ﬁnite; let us call this value V1. Since the
variance of an average of n i.i.d. variables is the variance of
those variables divided by n [2, Section 2.11.1], the generic
expression for the variance will be
Var (C∆) =
V1
n
=
β∈∆
p(β) − |∆|2
1
n
.
(3)
(cid:80)
Since V1 does not depend on n we obtain that, when n grows,
the variance converges in probability to 0 and C∆ converges
in probability to |∆|. With a variance in O (1/n), the stan-
dard deviation is O (1/
n), which is what typically happens
with Monte Carlo estimators [24].
√
In Equation 2, we are performing an inverse probability
weighting: we multiply the contribution of each element in
the sample by the inverse of the probability that such ele-
ment has to appear in the sample. This procedure is not new
in Statistics: it has been proposed in 1952 by Horvitz and
Thompson [13] for stratiﬁed sampling – a technique where
the analyst has control over sampling, and chooses diﬀerent
sampling probabilities for diﬀerent subpopulations (strata).
In our case, the diﬀerence lies in the fact that we do not have
A probability A[i] indeed corresponds to a rank C[i]. To
compute the strength of a password α, we ﬁrst lookup the
index j of the rightmost A[j] value in A such that A[j] >
p (α) through binary search; the output for C∆ is simply
C[j]. The procedure has O (log n) cost due to binary search.
In Section 5.4, to evaluate mandatory restrictions on pass-
words, we evaluate the cardinality of a diﬀerent set of pass-
words, a ∆(cid:48) set of passwords that is deﬁned via a boolean
ﬁlter function f that identiﬁes allowed passwords, such that
∆f = {β ∈ Γ : p (β) > p (α) ∧ f (β)}. In this case, it is point-
less to store information about passwords for which f (β)
is false, as they will never be taken in consideration when
evaluating C∆f ; therefore, our array A will only contain the
probabilities under model p of passwords that satisfy f (β).
4. EXPERIMENTAL SETUP
Here, we describe the setup we employ to evaluate our pro-
posal. We describe the datasets used in our evaluation and
the state-of-the-art guessing techniques that we consider.
4.1 Datasets
Several very large password datasets have been made pub-
licly available through leaks: the de facto standard in pass-
word research – allowing to compare results between diﬀer-
ent works – is the Rockyou dataset, which contains a set
of 32 million passwords that was released to the public in
2009. On this dataset we measure password strength as fol-
lows. Whenever a password training set is needed, we use
a dataset of 10 million passwords that was recently released
(February 2015) on the Xato.net website [3] – in the follow-
ing, we will label this as the Xato dataset. Most passwords
in the Xato dataset appear to come from Western users,
with the most common language being English. The Xato
dataset does not include passwords from the Rockyou leak.
We performed additional experiments – not reported here
due to space limitations – where we switched the role of
datasets, using Xato as test set and Rockyou as a training
set. Results are largely analogous to those reported here,
161reinforcing our conﬁdence on the generality of our results,
conﬁrming that there are no discernible defects in either
dataset, and showing that very large password datasets from
users speaking the same languages exhibit similar properties.
We have chosen Xato and Rockyou because of dataset
quality, uniformity and size. Other large datasets of Web
passwords include the Chinese datasets studied by Li et
al. [18]: the authors commented that “the raw ﬁles contain
duplication and blank passwords that can aﬀect the analy-
sis”. To avoid the risk of misleading results due to improper
data cleaning, we left those datasets out of our analysis.
Other large datasets of more than one million passwords
have been leaked from Gawker, eHarmony, LinkedIn, Ever-
note and Adobe. These datasets, however, contain hashed
or encrypted passwords: therefore, they are not suitable for
our analysis which needs passwords in clear-text to train the
attack models and to evaluate strength.
4.2 Attack Models
We now describe the attack models we consider in the
eﬀort of covering the most representative approaches.
4.2.1 N-Grams
Password guessing attacks using n-grams (i.e., substrings
of length n appearing in a training set) have been originally
proposed by Narayanan and Shmatikov [21].
In the following, we denote the number of occurrences of a
string of characters ca . . . cb in the training set as o(ca . . . cb).
We further denote the frequency with which character cb
follows the string ca . . . cb−1 as
P (cb|ca . . . cb−1) =
(4)
In an n-gram model (equivalently known as order n − 1
Markov model ), the probability of a password c1 . . . cl is
o (ca . . . cb)
o (ca . . . cb−1)
l+1(cid:89)
pn-gram (c1 . . . cl) =
P (ci|ci−n+1 . . . ci−1) ,
(5)
i=1
where all values ci when i ≤ 0 or i > l are considered to be a
special symbol ⊥ that does not appear in passwords, which
is used to denote the start or the end of the password.
Higher values of n make it possible to exploit a longer
history when predicting the next character; however, as n
grows, the issue of data sparsity appears: some n-grams may
not be represented in the training set, and the model would
incorrectly label their probabilities as 0. A solution to this
problem is the backoﬀ model described in Section 4.2.3.
Implementation Details.
For each n-gram ci−n+1 . . . ci, we simplify the notation by
calling ci−n+1 . . . ci−1 as its state s and ci as its transition
t. We precompute all values of P (t|s) by grouping all the
n-grams by state: in this way, the number of occurrences of
the state s needed for Equation 4 is computed by summing
the occurrences of all the n-grams in the group.
The probabilities that we compute can be very small and
may underﬂow: to avoid such problems, we store and com-
pute the base-2 logarithms of probabilities rather than prob-
abilities themselves. When computing probabilities, we per-
form similar substitutions also for PCFG and backoﬀ meth-
ods described later in the text.
Algorithm 1 Password generation for n-grams.
def starting_state():
return “⊥ . . .⊥” with length n − 1
def update_state(s, t):
drop the ﬁrst character from s
return s + t # concatenation
s ←starting_state()
g ←“” # accumulator for the result
while True:
r ←random number in [0, 1]
# ﬁnd i through binary search
i ←rightmost index s.t. Cs [i] > r
if ts,i = ⊥: return g
append ts,i to g
s ←update_state(s, ts,i)
such that each element Cs[i] =(cid:80)i
password enumeration: for each state s, we sort all transi-
tions ts,i in descending order of number of occurrences; we
then precompute an array Cs of cumulative probabilities
j=1 P (ts,j|s) = C[i − 1] +
P (ts,i|s). The creation of a password for the sample needed
in Section 3.1 is then carried out according to the procedure
shown in Algorithm 1: using binary search on Cs speeds up