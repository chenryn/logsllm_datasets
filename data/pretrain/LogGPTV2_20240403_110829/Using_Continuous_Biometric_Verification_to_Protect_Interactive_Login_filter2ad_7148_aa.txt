title:Using Continuous Biometric Verification to Protect Interactive Login
Sessions
author:Sandeep Kumar and
Terence Sim and
Rajkumar Janakiraman and
Sheng Zhang
Using Continuous Biometric Veriﬁcation to Protect Interactive Login Sessions
Sandeep Kumar Terence Sim Rajkumar Janakiraman
Sheng Zhang
School of Computing, National University of Singapore
3 Science Drive 2, Singapore 117543
fskumar, tsim, janakira, PI:EMAIL
Abstract
In this paper we describe the theory, architecture, imple-
mentation, and performance of a multi-modal passive bio-
metric veriﬁcation system that continually veriﬁes the pres-
ence/participation of a logged-in user. We assume that
the user logged in using strong authentication prior to the
starting of the continuous veriﬁcation process. While the
implementation described in the paper combines a digital
camera-based face veriﬁcation with a mouse-based ﬁnger-
print reader, the architecture is generic enough to accom-
modate additional biometric devices with different accu-
racy of classifying a given user from an imposter. The main
thrust of our work is to build a multi-modal biometric feed-
back mechanism into the operating system so that veriﬁca-
tion failure can automatically lock up the computer within
some estimate of the time it takes to subvert the computer.
This must be done with low false positives in order to real-
ize a usable system. We show through experimental results
that combining multiple suitably chosen modalities in our
theoretical framework can effectively do that with currently
available off-the-shelf components.
1. Introduction
By continuous veriﬁcation we mean that the identity of the
human operating the computer is continually veriﬁed. Ver-
iﬁcation is computationally simpler than identiﬁcation and
attempts to determine how “close” an observation is to a
known value, rather than ﬁnding the closest match in a set
of known values. Veriﬁcation is a realistic operation in the
normal usage of a computer system because we can assume
that the user’s identity has been incontrovertibly established
by a preceding strong authentication mechanism. It is also
appealing because it can conceivably be ofﬂoaded to a hard-
ware device that is properly initialized with user speciﬁc
data upon successful login.
The sense in which we are using identity veriﬁcation is
weaker than the ultimate aim of techniques such as intrusion
detection [4] which even attempt to detect misuse by the au-
thorized user who would clearly pass the biometric veriﬁ-
cation test. However, host-based intrusion detection has not
quite been successful in practice, either because of the com-
putational requirements of handling voluminous amounts of
low level trace, or because of the large number of false posi-
tives that result from an attempt to sharply characterize user
behavior based on observed low-level traces. We believe
that continuous veriﬁcation, if realized efﬁciently with low
false positives, can be important in high risk environments
where the cost of unauthorized use is high. This can be true
for computer driven airline cockpit control, computers in
banks, defense establishments, and other areas whose use
directly affects the security and safety of human lives.
Biometric veriﬁcation is appealing because several of
them that are easy to incorporate in ordinary computer use
are passive, and they obviate the need to carry extra de-
vices for authentication.
In a sense, they are always on
one’s “person”, and perhaps a little safer than using exter-
nal devices which can be separated from their carrier more
easily. However, biometric veriﬁcation can be construed
as a matching problem and usually makes a probabilistic
judgment in its classiﬁcation. This makes it error prone.
Furthermore, when used passively like we are attempting
to do, it can result in time periods with no samples or poor
quality samples; for example, when the user is not looking
directly into the camera, or when the surrounding light is
poor. To avoid both these pitfalls, researchers have used
multiple modalities, say, ﬁngerprint and face images simul-
taneously. This makes classiﬁcation more robust and is also
the approach that we have taken in this work. Even when
some modalities may be very accurate, they might be in-
herently limited in their sampling rate, so combining them
with faster (albeit less accurate) modalities helps to ﬁll gaps
between successive samples of the better modality. How-
ever, the use of multiple modalities presupposes indepen-
dent sampling so that not all modalities fail to generate a
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:12:41 UTC from IEEE Xplore.  Restrictions apply. 
valid sample at the same time.1
Building an effective reactive biometric veriﬁcation sys-
tem consists of many aspects. Not only must the veriﬁca-
tion results be integrated into the operating system, it can
be critical to balance several conﬂicting metrics: namely,
accuracy of detection, system overhead incurred during the
veriﬁcation, and reaction time i.e., the vulnerability window
within which the system must respond when it detects that
the authorized user is no longer present. This relationship is
especially important when all these aspects are performed in
software on the same machine that is being protected from
unauthorized use.
In the rest of the paper we describe the theoretical under-
pinnings of our multi-modal biometric veriﬁcation system,
our implementation architecture, the OS kernel changes
needed to make the system reactive to veriﬁcation failures,
and the performance impact of such a system on ordinary
computer use. The goal is to render a computer system in-
effective within a certain time period of veriﬁcation failure.
This time should be a conservative estimate of the time it
would take someone to cause information loss (conﬁden-
tiality, integrity, or availability [11]) on the system.
2. Biometrics in Brief
We begin with a brief introduction of some of the important
concepts in biometrics and veriﬁcation. Readers familiar
with these concepts may skip ahead; while readers wanting
more details can refer to [5].
2.1. Basic concepts
Biometrics is generally taken to mean the measurement of
some physical characteristic of the human body for the pur-
pose of identifying the person. Common types of biomet-
rics include ﬁngerprint, face image, and iris/retina pattern.
A more inclusive notion of biometrics also includes the be-
havioral characteristics, such as gait, speech pattern, and
keyboard typing dynamics.
When a biometric is used to verify a person, the typi-
cal process is as shown in Figure 1. The user ﬁrst presents
her biometric (e.g. the thumb) to the sensor device, which
captures it as raw biometric data (for example a ﬁngerprint
image). This data is then preprocessed to reduce noise, en-
hance image contrast, etc. Features are then extracted from
the raw data. In the case of ﬁngerprints, these would typ-
ically be minutiae and bifurcations in the ridge patterns.
These features are then used to match against the corre-
sponding user’s features taken from the database (retrieved
based on the claimed identity of the user). The result of the
1Face and ﬁngerprint may not be totally uncorrelated in that sense.
However, that’s not the thrust of this paper; rather this paper focuses on
integrating multiple biometrics within an OS.
User 
presents 
biometric
Raw biometric 
data
Sensor
Pre-processing, 
Feature extraction
Features
Claimed identity
Database
User(cid:146)s features
Enrollment
Claimed 
identity(cid:146)s 
features
Matching
Score
Decision
Accept or Reject
Figure 1. A typical biometric veriﬁcation pro-
cess.
match is called a Score, S, typically a real number between
0 and 1, where 0 means “most dissimilar” and 1 means
“most similar”. The ﬁnal step is to compare S to a pre-
deﬁned thresholdT , and output:
(cid:15) a decision of “Accept” (when S (cid:21) T ), meaning the
Veriﬁer considers the user as legitimate, or
(cid:15) “Reject” (when S < T ), meaning the Veriﬁer thinks
that the user is an imposter.
Some veriﬁcation systems also output “Unsure”, to indicate
that the sample cannot be reliably classiﬁed one way or the
other. In this case, the user may be asked to re-present her
biometric.
Of course, the user’s biometric features must ﬁrst be en-
tered into the database. This is done in an earlier one-off
phase called enrollment. The process of enrollment is usu-
ally similar, consisting also of biometric data capture, pre-
processing, and feature extraction. However, to increase ro-
bustness, multiple biometric samples are usually acquired
(e.g. multiple images of the same ﬁnger), so that the ver-
iﬁer can “learn” the natural variation present in the user’s
biometric.
How accurate is biometric veriﬁcation? There are two
types of errors that a Veriﬁer can make: aFalse Accept, or a
False Reject. The False Accept Rate (FAR) is the probability
that the Veriﬁer incorrectly classiﬁes an imposter as a legit-
imate user. This is a security breach. On the other hand, the
False Reject Rate (FRR) is the probability that the Veriﬁer
incorrectly decides that the true user is an imposter. This
is an inconvenience to the user, since she must usually re-
sort to another means of verifying herself. In general, while
a small FRR can be accepted as an inconvenience, a large
FRR value can impact availability and may be construed as
indirectly impacting the security of the system [11].
In an ideal Veriﬁer, both theFAR and FRR are zero. In
practice, there is usually a tradeoff between the FAR and
FRR : a lower rate for one type of error is achievable only
at the expense of a higher rate for the other. This tradeoff
is usually described using the Receiver Operating Charac-
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:12:41 UTC from IEEE Xplore.  Restrictions apply. 
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
R
R
F
-
1
45o line  
Ideal ROC  
Typical ROC
0
0
0.1
0.2
0.3
0.4
0.5
FAR
0.6
0.7
0.8
0.9
1
Figure 2. ROC Curves.
teristic (ROC) Curve (Figure 2), which plots (1 (cid:0) F RR)
versus FAR. For any given Veriﬁer, one can determine its
ROC simply by varying its decision threshold T , running
the Veriﬁer on test data, and calculating theFAR and FRR
for that value of T . The Ideal Veriﬁer has an inverted-L
shaped ROC curve, while an imperfect Veriﬁer has a curve
lying somewhere between the Ideal curve and the 45(cid:14) line.2
The Power of a Veriﬁer is deﬁned as the area under its ROC
curve, and that is a useful measure of the Veriﬁer’s over-
all accuracy in a way that combines both its FAR and FRR.
The greater the area, the better the Veriﬁer. In general, ﬁn-
gerprint veriﬁcation is considered morepowerful than face
veriﬁcation.
When using multiple biometrics, individual classiﬁca-
tion results must be combined into a composite result. Com-
putational overhead related to biometric processing must be
balanced to get the desired tradeoff between usability, secu-
rity and remaining computational power available for useful
work. We describe these issues next.
2.2. Operational issues
‹ Computational overhead. Generally speaking, for all
biometrics there is a tradeoff between computation and the
Veriﬁer’sPower. For biometrics with weaker accuracy (less
Power), multiple samples can often be combined to yield
a more accurate composite assessment for that biometric3.
But this requires more computation for a single assessment
output, and for continuous veriﬁcation it can add a factor
to the computational load. An effective system must strike
2If a Veriﬁer has an ROC curve below the 45(cid:14) line, simply swap its
“Accept” and “Reject” decisions and the ROC curve will move above this
line.
3There are a plethora of techniques of combining them for e.g., using
the sum, product, minimum, median, and maximum rules [6]. Other
researchers have used decision trees and linear-discriminant based methods
[13].
a balance between load and accuracy, especially when all
biometric related computation is done in software on the
same machine that is used for computing needs.
For example, in one set of measurements that we took for
face veriﬁcation, the CPU needed for our operating environ-
ment was nearly .2s per image, mostly incurred in locating
the face in the whole image. This ﬁgure could be reduced
to .1s by employing heuristics such as remembering the lo-
cation of the face in the image, and using that as the starting
point of face detection for the next image. The upshot is that
processing about 10 frames per second would saturate the
CPU. Adding multiple samples to increase accuracy of this
biometric would seriously impact performance (about 10%
for each extra frame rate). The alternative is to combine
face veriﬁcation with another, different biometric which has
much higher accuracy.