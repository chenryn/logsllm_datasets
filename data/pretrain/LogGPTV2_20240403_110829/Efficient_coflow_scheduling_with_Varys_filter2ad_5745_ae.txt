receive all get() calls was 44 milliseconds with 151 milliseconds
being the 95th percentile.
A large fraction of these overheads could be avoided in the pres-
ence of in-network isolation of control plane messages [21].
Trace-Driven Simulation We compared the performance of inter-
coﬂow scheduling against per-ﬂow fairness and prioritization
schemes in simulations. Without coordination overheads, the im-
provements are noticeably larger (Figure 9) – the average and 95th
percentile CCTs improved by 3.66× and 2.77× over per-ﬂow fair-
ness and by 5.53× and 5.8× over per-ﬂow prioritization.
!
s
w
o
ﬂ
o
C
%
100!
75!
50!
25!
0!
Not Admitted!
Missed Deadline!
Met Deadline!
Coﬂow! Fair! Coﬂow! Fair! Coﬂow! Fair!
Within 100% 
of Deadline!
Within 50% 
of Deadline!
Actual 
Deadline!
Figure 10: [EC2] Percentage of coﬂows that meet deadline using Varys in
comparison to per-ﬂow fairness. Increased deadlines improve performance.
Note that comparative improvements for bin-1 w.r.t. other bins
are signiﬁcantly larger than that in experiments because of the ab-
sence of scheduler coordination overheads. We observe larger ab-
solute values of improvements in Figure 9 in comparison to the
ones in Figure 7. Primary factors for this phenomenon include in-
stant scheduling, zero-latency setup/cleanup/update of coﬂows, and
perfectly timed ﬂow arrivals (i.e., coﬂows are READY to be sched-
uled upon arrival) in the simulation. In the absence of these over-
heads, we see in Figure 8b that Varys can indeed outperform per-
ﬂow schemes even for sub-second coﬂows.
What About Per-Flow Prioritization? Figure 9 highlights that
per-ﬂow prioritization mechanisms are even worse (by 1.52×)
than per-ﬂow fairness provided by TCP when optimizing CCTs.
The primary reason is indiscriminate interleaving across coﬂows –
while all ﬂows make some progress using ﬂow-level fairness, per-
ﬂow prioritization favors only the small ﬂows irrespective of the
progress of their parent coﬂows. However, as expected, ﬂow-level
prioritization is still 1.08× faster than per-ﬂow fairness in terms of
the average FCT. Figure 8b presents the distribution of CCTs using
per-ﬂow prioritization in comparison to other approaches.
How Far are We From the Optimal? While ﬁnding the opti-
mal schedule is infeasible, we tried to ﬁnd an optimistic estima-
tion of possible improvements by comparing against an ofﬂine 2-
approximation combinatorial ordering heuristic for coﬂows with-
out coupled resources [30]. We found that the average CCT did not
change using the combinatorial approach. For bin-1 to bin-4, the
changes were 1.14×, 0.96×, 1.46×, and 0.92×, respectively.
7.3 Varys’s Performance for Deadline-Sensitive Coﬂows
Inter-coﬂow scheduling allowed almost 2× more coﬂows to com-
plete within corresponding deadlines in EC2 experiments (Fig-
ure 10) – 57% coﬂows met their deadlines using Varys as opposed
to 30% using the default mechanism. Coﬂows across different bins
experienced similar results, which is expected because Varys does
not differentiate between coﬂows when optimizing for deadlines.
Recall that because the original trace did not contain coﬂow-
speciﬁc deadlines, we introduced them based on the minimum CCT
of coﬂows (§7.1). Hence, we did not expect 100% admission rate.
However, a quarter of the admitted coﬂows failed to meet their
deadlines. This goes back to the lack of network support in es-
timating utilizations and enforcing Varys-determined allocations:
Varys admitted more coﬂows than it should have had, which them-
selves missed their deadlines and caused some others to miss as
well. Trace-driven simulations later shed more light on this.
To understand how far off the failed coﬂows were, we analyzed if
they could complete with slightly longer deadlines. After doubling
the deadlines, we found that almost 94% of the admitted coﬂows
succeeded using Varys.
Trace-Driven Simulation In trace-driven simulations, for the de-
fault case (x=1), Varys admitted 75% of the coﬂows and all of
them met their deadlines (Figure 11). Note that the admission rate
is lower than that in our experiments. Prioritization schemes fared
t
e
M
!
e
n
i
l
t
a
h
t
s
w
o
ﬂ
o
C
%
d
a
e
D
r
i
e
h
T
100!
75!
50!
25!
0!
Inter-Coﬂow Scheduling!
Virtual Cluster!
Per-Flow Prioritization!
Per-Flow Fairness!
0.1!
10!
Deadline = (1+Uniform(0, x)) Times the Min CCT!
0.5!
1!
2!
Figure 11: [Simulation] More coﬂows meet deadline using inter-coﬂow
scheduling than using per-ﬂow fairness and prioritization schemes.
# Coﬂows %SN %LN %SW %LW
Mix-N
10%
Mix-W
39%
Mix-S
23%
Mix-L
31%
Table 5: Four extreme coﬂow mixes from the Facebook trace.
48% 40%
15%
22%
50%
17%
39% 27%
2%
24%
10%
3%
800
369
526
272
better than per-ﬂow fairness unlike when the objective was min-
imizing CCT: 59% coﬂows completed within their deadlines in
comparison to 52% using fair sharing.
As we changed the deadlines of all coﬂows by varying x from
0.1 to 10, comparative performance of all the approaches remained
almost the same. Performance across bins were consistent as well.
What About Reservation Schemes? Because the impact of ad-
mission control is similar to reserving resources, we compared our
performance with that of the Virtual Cluster (VC) abstraction [11],
where all machines can communicate at the same maximum rate
through a virtual non-blocking switch. The VC abstraction admit-
ted and completed slightly fewer coﬂows (73%) than Varys (75%),
because reservation using VCs is more conservative.
7.4
While minimizing CCT, preemption-based mechanisms can starve
certain coﬂows when the system is overloaded. Varys takes precau-
tions (§5.3.4) to avoid such scenarios. As expected, we did not ob-
serve any perpetual starvation during experiments or simulations.
What About a Non-Preemptive Scheduler? Processing coﬂows
in their arrival order (i.e., FIFO) avoids starvation [15]. However,
simulations conﬁrmed that head-of-line blocking signiﬁcantly hurts
performance – specially, the short coﬂows in bin-1 and bin-3.
Impact of Preemption
We found that processing coﬂows in the FIFO order can result
in 24.64×, 5.44×, 34.2×, and 5.03× slower completion times for
bin-1 to bin-4. The average (95th percentile) CCT became 5.65×
(7.7×) slower than that using Varys.
7.5
Impact on Network Utilization
To understand Varys’s impact on network utilization, we compared
the ratios of makespans in the original workload as well as the ones
in Table 5. Given a ﬁxed workload, a change in makespan means a
change in aggregate network utilization.
We did not observe signiﬁcant changes in makespan in our EC2
experiments – the exact factors of improvements were 1.02×,
1.06×, 1.01×, 0.97×, and 1.03× for the ﬁve workloads. This is
expected because while Varys is not work-conserving at every point
in time, its overall utilization is the same as non-coﬂow approaches.
Makespans for both per-ﬂow fairness and coﬂow-enabled sched-
Impact of Coﬂow Mix
ules were the same in the trace-driven simulation.
7.6
To explore the impact of changes in the coﬂow mix, we selected
four extreme hours (Table 5) from the trace and performed hour-
long experiments on EC2. These hours were chosen based on the
!
t
n
e
m
e
v
o
r
p
m
f
o
r
o
t
c
a
F
I
3!
2!
1!
0!
!
6
1
3
.
!
!
9
.
2
3
7
2
.
!
3
6
.
1
!
7
1
.
!
1
!
4
3
1
.
!
5
5
1
.
!
!
7
3
1
.
8
2
1
.
!
7
1
.
1
!
3
0
1
.
Mix-N!
Mix-S!
!
!
4
7
1
.
3
6
.
1
!
5
5
1
.
!
5
2
1
.
Mix-W!
Mix-L!
!
6
7
.
1
!
4
6
1
.
!
6
5
1
.
!
3
1
.
Bin 1!
Bin 2!
Bin 3!
Bin 4!
ALL!
Figure 12: [EC2] Improvements in the average CCT using coﬂows for dif-
ferent coﬂow mixes from Table 5.
!
t
n
e
m
e
v
o
r
p
m
f
o
r
o
t
c
a
F
I
6!
4!
2!
0!
W.r.t. Per-Flow Prioritization!
W.r.t. Per-Flow Fairness!
3.26!
3.72!
1.83!
1.76!
10!
2.68!
3.21!
40!
70!
Number of Concurrent Coﬂows!
4.18!
3.66!
100!
Figure 13: [Simulation] Improvements in the average CCT for varying
numbers of concurrent coﬂows.
Inter-Coﬂow Scheduling!
Per-Flow Prioritization!
Per-Flow Fairness!
41.67!
37.33!
2.45!
70!
1.00!
100!
47.37!
2.63!
40!
t
e
M
!
e
n
i
l
t
a
h
t
s
w
o
ﬂ
o
C
%
d
a
e
D
r
i
e
h
T
100!
75!
50!
25!
0!
70.83!
33.33!
12.50!
10!
Number of Concurrent Coﬂows!
Figure 14: [Simulation] Changes in the percentage of coﬂows that meet
deadlines for varying numbers of concurrent coﬂows.
high percentage of certain types of coﬂows (e.g., narrow ones in
Mix-N) during those periods.
Figure 12 shows that the average CCT improves irrespective of
the mix, albeit in varying degrees. Observations made earlier (§7.2)
still hold for each mix. However, identifying the exact reason(s) for
different levels of improvements is difﬁcult. This is due to the on-
line nature of the experiments – the overall degree of improvement
depends on the instantaneous interplay of concurrent coﬂows. We
also did not observe any clear correlation between the number of
coﬂows or workload size and corresponding improvements.
7.7 Impact of Cluster/Network Load
So far we have evaluated Varys’s improvements in online set-
tings, where the number of concurrent coﬂows varied over time.
To better understand the impact of network load, we used the same
coﬂow mix as the original trace but varied the number of concurrent
coﬂows in an ofﬂine setting. We see in Figure 13 that Varys’s im-
provements increase with increased concurrency: per-ﬂow mecha-
nisms fall increasingly further behind as they ignore the structures
of more coﬂows. Also, ﬂow-level fairness consistently outperforms
per-ﬂow prioritization mechanisms in terms of the average CCT.
Deadline-Sensitive Coﬂows We performed a similar analysis for
deadline-sensitive coﬂows. Because in this case Varys’s perfor-
mance depends on the arrival order, we randomized the coﬂow
order across runs and present their average in Figure 14. We ob-
serve that as the number of coexisting coﬂows increases, a large
number of coﬂows (37% for 100 concurrent coﬂows) meet their
deadlines using Varys; per-ﬂow mechanisms completely stop work-
ing even before. Also, per-ﬂow prioritization outperforms (however