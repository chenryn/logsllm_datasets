on the recommender system to ﬁlter input training datasets
and include these users who are predicted to be fake users
into the suspicious user set. This rating scores based detection
method is designed for explicit dataset. Note that, the Music
dataset is purely implicit, which means that the above features
will always be 0. The detection process for implicit dataset re-
quires other complicated techniques like semantic analysis that
closely relates to the platform corpus. However, the workﬂow
for fake user detection can be similar to that on explicit dataset,
the main difference between them is features and the way
of obtaining feature values for subsequent classiﬁer training.
Here, we mainly conduct detection experiments with random
target items on the ML-100K dataset.
Effectiveness of Fake User Detectors. In the detection pro-
cess, we focus on whether the detector can effectively detect
false users and whether the detector affects the original dataset.
Here we use False Positive Rate (FPR) and False Negative
Rate (FNR) to evaluate the performance of the detector, where
FPR stands for the fraction of normal users who are falsely
predicted as fake users while FNR means the proportion of
fake users who are falsely predicted as normal users. The
detection results including both phases of SVM-TIA on the
ML-100K dataset are shown in Table VIII. First, the TIA
phase can decrease FPR after the SVM phase, while it does not
inﬂuence FNR in most cases. However, we can observe that
there is an abnormal increase in FNR when attack size is 0.5%
(which is slightly larger than τ). This is because some fake
users have escaped detection in the SVM phase and the number
of the maximum ratings of the target items is lower than τ.
Thus, the detector cannot identify the target items and all fake
users will escape detection, which further increases the FNR.
Second, the fake user detectors are quite efﬁcient in detecting
fake users that are generated by the baseline attacks. As we can
see, FPRs and FNRs for these attacks under different attack
sizes are lower than 5% in most cases, which means most
fake users and normal users are correctly classiﬁed by the
TABLE IX: HR@10 for different attacks under detection.
Dataset
Attack
None
Random
ML-100K
Bandwagon
MF
Our attack
0.5%
0.0025
0.0031
0.0032
0.0030
0.0030
Attack size
1%
3%
0.0025
0.0029
0.0029
0.0029
0.0029
0.0025
0.0023
0.0037
0.0036
0.0045
5%
0.0025
0.0020
0.0019
0.0031
0.0067
detectors, showing the effectiveness of this detection method.
Third, the detector for our attack is not effective enough. FPR
is around 12% and FNR is around 30%, which means the
detector still makes a large amount of false judgements for our
attack and around 30% of fake users are successfully inserted
to the training dataset. According to the above observations,
it is obvious that it is much harder to detect our attack than
other baseline attacks.
Effectiveness of Poisoning Attacks under Detection. Now
we test the hit ratio of target items for different poisoning
attacks after deploying fake user detectors on the target recom-
mender systems. The experimental results are shown in Table
IX, where “None” means there is neither poisoning attack
nor fake user detector deployed on the target recommender
system. The hit ratio of target items with baseline attacks
does not signiﬁcantly change with different attack sizes. As
shown in Table IX, overall our attack still outperforms the
baseline attacks, and the baseline attacks achieve only small
improvements on the initial hit ratio. In particular, our attack
is still effective under detection, e.g., when inserting 5% fake
users into the target recommender system, the hit ratio for
target items rise to 0.0067, about 2.7 times of the initial hit
ratio. The reason is that almost 30% of fake users are not
ﬁltered out and they can still have a large impact on the target
recommender system. Note that, when the attack size is small
(e.g., 0.5%), many normal users that have rated the target items
are falsely ﬁltered out by the detector, while only few fake
users are successfully injected into the dataset in our attack,
which leads to relatively low hit ratios. Even though, our attack
achieves similar performance to the baseline attacks when the
attack size is small.
Discussion. Attackers can use various strategies to evade
detection. For example, as the SVM-TIA detection method
heavily relies on the frequency distribution of items, an
14
attacker could evade detection by adjusting the process of
constructing fake users, e.g., avoiding selecting the same items
frequently by controlling the selection probability. Moreover,
an attacker can construct fake users without
items
selected by default, thus decreasing the frequency of the target
items. As our experimental results in Section V showed, when
the target items are not selected by fake users by default, our
attack remains effective and still signiﬁcantly outperforms the
baseline attacks.
target
Besides the above statistical analysis of the rating patterns
of normal and fake users, there are also some other detection
and defense mechanisms against data poisoning attacks. For
instance, Steinhardt et al. [39] bound the training loss when the
poisoned training examples are in a particular set, i.e., poisoned
training examples are constrained. It is an interesting topic for
future work to generalize such analysis to bound the training
loss of recommender systems when an attacker can inject a
bounded number of fake users. Paudice et al. [36] aim to
statistically analyze the features of training examples and use
anomaly detection to detect poisoned training examples. We
explore supervised learning based defenses in our experiments,
where the features are extracted from users’ rating scores. As
future work, we can extend the anomaly detection method to
detect fake users based on statistical patterns of their rating
scores.
There are also certiﬁably robust defenses [22], [23], [29],
[40] against data poisoning attacks to machine learning al-
gorithms. However, recommender systems are different from
the machine learning algorithms considered in these work.
For instance, top-K items are recommended to each user in
a recommender system, while a machine learning classiﬁer
predicts a single label
is an
interesting future work to generalize these certiﬁed robustness
guarantee to recommender systems.
in these work. However,
it
VII. CONCLUSION AND FUTURE WORK
In this work, we show that data poisoning attack to deep
learning based recommender systems can be formulated as an
optimization problem, which can be approximately solved via
combining multiple heuristics. Our empirical evaluation results
on three real-world datasets with different sizes show that 1)
our attack can effectively promote attacker-chosen target items
to be recommended to substantially more normal users, 2)
our attack outperforms existing attacks, 3) our attack is still
effective even if the attacker does not have access to the neural
network architecture of the target recommender system and
only has access to a partial user-item interaction matrix, and
4) our attack is still effective and outperforms existing attacks
even if a rating score based detector is deployed. Interesting
future work includes developing new methods to detect the
fake users and designing new recommender systems that are
more robust against data poisoning attacks.
ACKNOWLEDGEMENT
We thank our shepherd Jason Xue and the anonymous
reviewers for their constructive comments. This work is sup-
ported in part by NSFC under Grant 61572278 and BNRist
under Grant BNR2020RC01013. Qi Li is the corresponding
author of this paper.
REFERENCES
[1]
[2]
J. A. Calandrino, A. Kilzer, A. Narayanan, E. W. Felten, and
V. Shmatikov, “” you might also like:” privacy risks of collaborative
ﬁltering,” in 2011 IEEE Symposium on Security and Privacy.
IEEE,
2011, pp. 231–246.
I. Cantador, P. Brusilovsky, and T. Kuﬂik, “2nd workshop on informa-
tion heterogeneity and fusion in recommender systems (hetrec 2011),”
in Proceedings of the 5th ACM conference on Recommender systems,
ser. RecSys 2011. New York, NY, USA: ACM, 2011.
[3] Q. Cao, X. Yang, J. Yu, and C. Palow, “Uncovering large groups of
active malicious accounts in online social networks,” in CCS, 2014.
[4] L. Chen, Y. Xu, F. Xie, M. Huang, and Z. Zheng, “Data poisoning
attacks on neighborhoodbased recommender systems,” Transactions on
Emerging Telecommunications Technologies, 2020.
[5] M. Chen, Z. Xu, K. Weinberger, and F. Sha, “Marginalized denoising
autoencoders for domain adaptation,” arXiv preprint arXiv:1206.4683,
2012.
[6] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,
G. Anderson, G. Corrado, W. Chai, M. Ispir et al., “Wide & deep
learning for recommender systems,” in Proceedings of the 1st workshop
on deep learning for recommender systems. ACM, 2016, pp. 7–10.
[7] P.-A. Chirita, W. Nejdl, and C. Zamﬁr, “Preventing shilling attacks in
online recommender systems,” in Proceedings of the 7th annual ACM
international workshop on Web information and data management.
ACM, 2005, pp. 67–74.
[8] P. Covington, J. Adams, and E. Sargin, “Deep neural networks for
youtube recommendations,” in Proceedings of the 10th ACM conference
on recommender systems. ACM, 2016, pp. 191–198.
[9] G. Danezis and P. Mittal, “Sybilinfer: Detecting sybil nodes using social
networks.” in NDSS, 2009.
J. Davidson, B. Liebald, J. Liu, P. Nandy, T. Van Vleet, U. Gargi,
S. Gupta, Y. He, M. Lambert, B. Livingston et al., “The youtube video
recommendation system,” in Proceedings of the fourth ACM conference
on Recommender systems. ACM, 2010, pp. 293–296.
[10]
[11] M. Dong, F. Yuan, L. Yao, X. Wang, X. Xu, and L. Zhu, “Trust in
recommender systems: A deep learning perspective,” arXiv preprint
arXiv:2004.03774, 2020.
[12] M. Fang, N. Z. Gong, and J. Liu, “Inﬂuence function based data
poisoning attacks to top-n recommender systems,” in Proceedings of
The Web Conference 2020, 2020, pp. 3019–3025.
[13] M. Fang, G. Yang, N. Z. Gong, and J. Liu, “Poisoning attacks to
graph-based recommender systems,” in Proceedings of the 34th Annual
Computer Security Applications Conference. ACM, 2018, pp. 381–392.
[14] F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens, “Random-walk
computation of similarities between nodes of a graph with application to
collaborative recommendation,” IEEE Transactions on knowledge and
data engineering, vol. 19, no. 3, pp. 355–369, 2007.
[15] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural
networks,” in Proceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, 2011, pp. 315–323.
[16] N. Z. Gong, M. Frank, and P. Mittal, “Sybilbelief: A semi-supervised
[17]
[18]
learning approach for structure-based sybil detection,” TIFS, 2014.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014, pp. 2672–
2680.
I. Gunes, C. Kaleli, A. Bilge, and H. Polat, “Shilling attacks against
recommender systems: a comprehensive survey,” Artiﬁcial Intelligence
Review, vol. 42, no. 4, pp. 767–799, 2014.
[19] F. M. Harper and J. A. Konstan, “The movielens datasets: History
and context,” Acm transactions on interactive intelligent systems (tiis),
vol. 5, no. 4, pp. 1–19, 2015.
[20] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua, “Neural
collaborative ﬁltering,” in Proceedings of the 26th international confer-
ence on world wide web.
International World Wide Web Conferences
Steering Committee, 2017, pp. 173–182.
[21] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward
networks are universal approximators,” Neural networks, vol. 2, no. 5,
pp. 359–366, 1989.
15
[22]
J. Jia, X. Cao, and N. Z. Gong, “Certiﬁed robustness of nearest
neighbors against data poisoning attacks,” Arxiv, 2020.
[23] ——, “Intrinsic certiﬁed robustness of bagging against data poisoning
attacks,” in AAAI, 2021.
J. Jia, B. Wang, and N. Z. Gong, “Random walk based fake account
detection in online social networks,” in DSN, 2017.
[24]
[25] S. Kapoor, V. Kapoor, and R. Kumar, “A review of attacks and its de-
tection attributes on collaborative recommender systems.” International
Journal of Advanced Research in Computer Science, vol. 8, no. 7, 2017.
[26] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques for
recommender systems,” Computer, no. 8, pp. 30–37, 2009.
[27] S. K. Lam and J. Riedl, “Shilling recommender systems for fun and
proﬁt,” in Proceedings of the 13th international conference on World
Wide Web. ACM, 2004, pp. 393–402.
[28] B. Li, Y. Wang, A. Singh, and Y. Vorobeychik, “Data poisoning attacks
on factorization-based collaborative ﬁltering,” in Advances in neural
information processing systems, 2016, pp. 1885–1893.
[29] Y. Ma, X. Zhu, and J. Hsu, “Data poisoning against differentially-
private learners: Attacks and defenses,” in Proceedings of the Twenty-
Eighth International Joint Conference on Artiﬁcial Intelligence, 2019,
pp. 4732–4738.
[30] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control
learning,”
Nature, vol. 518, no. 7540, p. 529, 2015.
through deep reinforcement
[31] B. Mobasher, R. Burke, R. Bhaumik, and J. J. Sandvig, “Attacks and
remedies in collaborative recommendation,” IEEE Intelligent Systems,
vol. 22, no. 3, pp. 56–63, 2007.
[32] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams, “Toward
trustworthy recommender systems: An analysis of attack models and al-
gorithm robustness,” ACM Transactions on Internet Technology (TOIT),
vol. 7, no. 4, p. 23, 2007.
[33] L. Mu˜noz-Gonz´alez, B. Biggio, A. Demontis, A. Paudice, V. Wongras-
samee, E. C. Lupu, and F. Roli, “Towards poisoning of deep learning
algorithms with back-gradient optimization,” in Proceedings of the 10th
ACM Workshop on Artiﬁcial Intelligence and Security. ACM, 2017,
pp. 27–38.
[34] S. Okura, Y. Tagami, S. Ono, and A. Tajima, “Embedding-based news
recommendation for millions of users,” in Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, 2017, pp. 1933–1942.
[35] M. P. O’Mahony, N. J. Hurley, and G. C. Silvestre, “Recommender
systems: Attack types and strategies,” in AAAI, 2005, pp. 334–339.
[36] A. Paudice, L. Muoz-Gonzlez, A. Gyrgy, and E. C. Lupu, “Detection
of adversarial training examples in poisoning attacks through anomaly
detection.” arXiv preprint arXiv:1802.03041, 2018.
[37] H. Pi, Z. Ji, and C. Yang, “A survey of recommender system from
data sources perspective,” in 2018 8th International Conference on