### Filtering and Detection of Fake Users in Recommender Systems

To filter input training datasets, the recommender system identifies and includes users predicted to be fake into a suspicious user set. This detection method, based on rating scores, is designed for explicit datasets. However, the Music dataset is purely implicit, meaning that the rating-based features will always be zero. For implicit datasets, more complex techniques such as semantic analysis, which closely relates to the platform's corpus, are required. Despite these differences, the workflow for detecting fake users in both explicit and implicit datasets remains similar, with the main difference being the features and the methods used to obtain feature values for subsequent classifier training.

In our experiments, we conducted detection tests on the ML-100K dataset using random target items.

### Effectiveness of Fake User Detectors

The detection process focuses on the detector's ability to identify fake users and its impact on the original dataset. We use False Positive Rate (FPR) and False Negative Rate (FNR) to evaluate the detector's performance. FPR measures the fraction of normal users incorrectly identified as fake, while FNR measures the proportion of fake users incorrectly identified as normal. The results for the SVM-TIA phases on the ML-100K dataset are presented in Table VIII.

First, the TIA phase generally reduces FPR after the SVM phase, but it does not significantly affect FNR in most cases. An exception occurs when the attack size is 0.5% (slightly larger than the threshold τ). In this case, some fake users escape detection during the SVM phase, and the number of maximum ratings for the target items is lower than τ. Consequently, the detector fails to identify the target items, leading to an increase in FNR.

Second, the detectors are efficient in identifying fake users generated by baseline attacks. As shown in Table VIII, FPRs and FNRs for these attacks under different attack sizes are typically below 5%, indicating that most fake and normal users are correctly classified.

Third, the detector is less effective against our proposed attack. The FPR is around 12%, and the FNR is around 30%, suggesting that the detector makes a significant number of false judgments for our attack. Approximately 30% of fake users successfully infiltrate the training dataset, highlighting the difficulty in detecting our attack compared to other baseline attacks.

### Effectiveness of Poisoning Attacks Under Detection

We tested the hit ratio of target items for various poisoning attacks after deploying fake user detectors on the target recommender systems. The results are shown in Table IX, where "None" indicates no poisoning attack or detector. The hit ratios for baseline attacks do not change significantly with different attack sizes. Overall, our attack outperforms the baseline attacks, achieving a hit ratio of 0.0067 for 5% fake users, about 2.7 times the initial hit ratio. This is because approximately 30% of fake users are not filtered out and can still influence the target recommender system. When the attack size is small (e.g., 0.5%), many normal users who have rated the target items are falsely filtered out, leading to relatively low hit ratios. Even so, our attack performs similarly to baseline attacks at smaller attack sizes.

### Discussion

Attackers can employ various strategies to evade detection. For instance, the SVM-TIA detection method relies heavily on item frequency distribution. An attacker could evade detection by adjusting the process of constructing fake users, such as avoiding frequent selection of the same items. Additionally, attackers can create fake users without default-selected items, reducing the frequency of target items. Our experimental results show that when target items are not selected by default, our attack remains effective and outperforms baseline attacks.

Besides statistical analysis of rating patterns, there are other detection and defense mechanisms against data poisoning attacks. For example, Steinhardt et al. [39] bound the training loss when poisoned examples are in a specific set. Future work could generalize this analysis to bound the training loss in recommender systems. Paudice et al. [36] use anomaly detection to identify poisoned training examples. In our experiments, we explored supervised learning-based defenses, extracting features from users' rating scores. Future work could extend anomaly detection to detect fake users based on their rating score patterns.

There are also certifiably robust defenses [22], [23], [29], [40] against data poisoning attacks in machine learning algorithms. However, recommender systems differ from these algorithms. For example, top-K items are recommended to each user, while a machine learning classifier predicts a single label. Future work could generalize these certified robustness guarantees to recommender systems.

### Conclusion and Future Work

In this work, we formulated data poisoning attacks on deep learning-based recommender systems as an optimization problem, solvable via multiple heuristics. Our empirical evaluation on three real-world datasets showed that our attack effectively promotes target items, outperforms existing attacks, and remains effective even with limited access to the target system's architecture. Future work includes developing new methods to detect fake users and designing more robust recommender systems.

### Acknowledgements

We thank our shepherd Jason Xue and the anonymous reviewers for their constructive comments. This work is supported in part by NSFC under Grant 61572278 and BNRist under Grant BNR2020RC01013. Qi Li is the corresponding author of this paper.

### References

[References listed as provided in the original text]

This optimized version aims to enhance clarity, coherence, and professionalism, making the content more accessible and easier to understand.