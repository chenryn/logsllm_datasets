## 矩阵分解的不足 {#20.html#-}前面我讲过的两种矩阵分解，本质上都是在预测用户对一个物品的偏好程度，哪怕不是预测评分，只是预测隐式反馈，也难逃这个事实，因为算法展现出来的目标函数就出卖了这一切。得到这样的矩阵分解结果后，常常在实际使用时，又是用这个预测结果来排序。所以，从业者们口口声声宣称想要模型的预测误差最小化，结果绕了一大圈最后还是只想要一个好点的排序，让人不禁感叹：人心总是难测。这种针对单个用户对单个物品的偏好程度进行预测，得到结果后再排序的问题，在排序学习中的行话叫做point-wise，其中 point意思就是：只单独考虑每个物品，每个物品像是空间中孤立的点一样。与之相对的，还有直接预测物品两两之间相对顺序的问题，就叫做pair-wise，pair，顾名思义就是成对成双，也许恐怕这类模型对单身的人士不是很友好。前面讲的矩阵分解都属于 point-wise模型。这类模型的尴尬是：只能收集到正样本，没有负样本，于是认为缺失值就是负样本，再以预测误差为评判标准去使劲逼近这些样本。逼近正样本没问题，但是同时逼近的负样本只是缺失值而已，还不知道真正呈现在用户面前，到底是不喜欢还是喜欢呢？虽然这些模型采取了一些措施来规避这个问题，比如负样本采样，但是尴尬还是存在的，为了排序而绕路也是事实。既然如此，能不能直面问题，采用 pair-wise来看待矩阵分解呢？当然能，不然我也不会写出这一篇专栏文章了。其实人在面对选择时，总是倾向矮子中选高个子，而不是真的在意身高到底是不是180，因此，更直接的推荐模型应该是：能够较好地为用户排列出更好的物品相对顺序，而非更精确的评分。这个问题已经有可爱的从业者们提出了方法，就是本文的主角：贝叶斯个性化排序，简称BPR 模型。下面，我就带你一探这个模型的究竟。
## 贝叶斯个性化排序 {#20.html#-}在前面的专栏文章中，有一个词叫做均方根误差，被我提过多次，用于评价模型预测精准程度的。那么现在要关注的是相对排序，用什么指标比较好呢？答案是AUC，AUC 全称是 Area Under Curve，意思是曲线下的面积，这里的曲线就是 ROC曲线。
### AUC {#20.html#auc}但是，我不打算继续解释什么是 ROC曲线了，那是它的原始定义，而我想跟你悄悄说的是另一件事，AUC这个值在数学上等价于：模型把关心的那一类样本排在其他样本前面的概率。最大是1，完美结果，而 0.5 就是随机排列，0 就是完美地全部排错。听到这个等价的 AUC解释，你是不是眼前一亮？这个非常适合用来评价模型的排序效果，比如说，得到一个推荐模型后，按照它计算的分数，能不能把用户真正想消费的物品排在前面？这在模型上线前是可以用日志完全计算出来的。AUC 怎么计算呢？一般步骤如下。1.  用模型给样本计算推荐分数，比如样本都是用户和物品这样一对一对的，同时还包含了有无反馈的标识；2.  得到打过分的样本，每条样本保留两个信息，第一个是分数，第二个是 0    或者 1，1 表示用户消费过，是正样本，0 表示没有，是负样本；3.  按照分数对样本重新排序，降序排列；4.  给每一个样本赋一个排序值，第一位 r1 = n，第二位 r2 =    n-1，以此类推；其中要注意，如果几个样本分数一样，需要将其排序值调整为他们的平均值；5.  最终按照下面这个公式计算就可以得到 AUC 值。我在文稿中放了这个公式，你可以点击查看。]{.MathJax_Preview style="color: inherit; display: none;"} {.MathJax_Display style="text-align: center;"}``{=html}[[A]{#20.html#MathJax-Span-3 .mistyle="font-family: MathJax_Math-italic;"}[U[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#20.html#MathJax-Span-4.mistyle="font-family: MathJax_Math-italic;"}[C[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"}]{#20.html#MathJax-Span-5.mistyle="font-family: MathJax_Math-italic;"}[=]{#20.html#MathJax-Span-6.mostyle="font-family: MathJax_Main; padding-left: 0.285em;"}[[[[[∑]{#20.html#MathJax-Span-10.mostyle="font-family: MathJax_Size1; vertical-align: 0em;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.108em, 1000.99em, 4.379em, -999.998em); top: -3.998em; left: 0em;"}[i]{#20.html#MathJax-Span-13.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[∈]{#20.html#MathJax-Span-14.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}[(]{#20.html#MathJax-Span-15.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}[样]{style="font-family: STIXGeneral, \"Arial Unicode MS\", serif; font-size: 80%; font-style: normal; font-weight: normal;"}]{#20.html#MathJax-Span-18.mo style="font-size: 70.7%;"}]{#20.html#MathJax-Span-17.mrow}]{#20.html#MathJax-Span-16.texatom}[本]{style="font-family: STIXGeneral, \"Arial Unicode MS\", serif; font-size: 80%; font-style: normal; font-weight: normal;"}]{#20.html#MathJax-Span-21.mo style="font-size: 70.7%;"}]{#20.html#MathJax-Span-20.mrow}]{#20.html#MathJax-Span-19 .texatom}[)]{#20.html#MathJax-Span-22.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}]{#20.html#MathJax-Span-12.mrow}]{#20.html#MathJax-Span-11.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; top: -3.715em; left: 1.038em;"}]{style="display: inline-block; position: relative; width: 3.108em; height: 0px;"}]{#20.html#MathJax-Span-9.munderover}[[[r]{#20.html#MathJax-Span-26 .mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.438em, 1000.43em, 4.144em, -999.998em); top: -3.998em; left: 0em;"}[i]{#20.html#MathJax-Span-29.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#20.html#MathJax-Span-28.mrow}]{#20.html#MathJax-Span-27.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; top: -3.856em; left: 0.473em;"}]{style="display: inline-block; position: relative; width: 0.755em; height: 0px;"}]{#20.html#MathJax-Span-25.msubsup}]{#20.html#MathJax-Span-24 .mrow}]{#20.html#MathJax-Span-23.texatom style="padding-left: 0.191em;"}[−]{#20.html#MathJax-Span-30 .mostyle="font-family: MathJax_Main; padding-left: 0.238em;"}[[M[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"}]{#20.html#MathJax-Span-33.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[×]{#20.html#MathJax-Span-34.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}(]{#20.html#MathJax-Span-37.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}[M[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"}]{#20.html#MathJax-Span-38.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[+]{#20.html#MathJax-Span-39.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}[1]{#20.html#MathJax-Span-40.mnstyle="font-size: 70.7%; font-family: MathJax_Main;"}[)]{#20.html#MathJax-Span-41.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}]{#20.html#MathJax-Span-36.mrow}]{#20.html#MathJax-Span-35 .texatom}]{#20.html#MathJax-Span-32.mrow}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.344em, 1003.44em, 4.332em, -999.998em); top: -4.562em; left: 50%; margin-left: -1.739em;"}[[2]{#20.html#MathJax-Span-42.mnstyle="font-size: 70.7%; font-family: MathJax_Main;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.391em, 1000.33em, 4.144em, -999.998em); top: -3.621em; left: 50%; margin-left: -0.186em;"}[[]{style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.626em; height: 0px;"}[]{style="display: inline-block; width: 0px; height: 1.085em;"}]{style="position: absolute; clip: rect(0.896em, 1003.63em, 1.226em, -999.998em); top: -1.315em; left: 0em;"}]{style="display: inline-block; position: relative; width: 3.626em; height: 0px; margin-right: 0.144em; margin-left: 0.144em;"}]{#20.html#MathJax-Span-31.mfrac style="padding-left: 0.238em;"}]{#20.html#MathJax-Span-8.mrow}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(2.779em, 1009.18em, 4.614em, -999.998em); top: -4.939em; left: 50%; margin-left: -4.562em;"}M[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#20.html#MathJax-Span-44.mistyle="font-family: MathJax_Math-italic;"}[×]{#20.html#MathJax-Span-45.mostyle="font-family: MathJax_Main; padding-left: 0.238em;"}N[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#20.html#MathJax-Span-48.mi style="font-family: MathJax_Math-italic;"}]{#20.html#MathJax-Span-47.mrow}]{#20.html#MathJax-Span-46 .texatomstyle="padding-left: 0.238em;"}]{#20.html#MathJax-Span-43.mrow}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.155em, 1003.2em, 4.144em, -999.998em); top: -3.292em; left: 50%; margin-left: -1.598em;"}[[]{style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 9.273em; height: 0px;"}[]{style="display: inline-block; width: 0px; height: 1.085em;"}]{style="position: absolute; clip: rect(0.896em, 1009.27em, 1.226em, -999.998em); top: -1.315em; left: 0em;"}]{style="display: inline-block; position: relative; width: 9.273em; height: 0px; margin-right: 0.144em; margin-left: 0.144em;"}]{#20.html#MathJax-Span-7.mfrac style="padding-left: 0.285em;"}]{#20.html#MathJax-Span-2.mrow}[]{style="display: inline-block; width: 0px; height: 2.355em;"}]{style="position: absolute; clip: rect(0.191em, 1013.23em, 3.202em, -999.998em); top: -2.351em; left: 0em;"}]{style="display: inline-block; position: relative; width: 13.226em; height: 0px; font-size: 125%;"}[]{style="display: inline-block; overflow: hidden; vertical-align: -0.938em; border-left: 0px solid; width: 0px; height: 3.532em;"}]{#20.html#MathJax-Span-1.mathstyle="width: 16.567em; display: inline-block;"}``{=html}[$$AUC = \frac{\sum\limits_{i \in (样本)}r_{i} - \frac{M \times {(M + 1)}}{2}}{M \times N}$$]{.MJX_Assistive_MathML.MJX_Assistive_MathML_Blockrole="presentation"}]{#20.html#MathJax-Element-1-Frame .MathJaxtabindex="0" style="text-align: center; position: relative;"mathml="AUC=∑i∈(样本)ri−M×(M+1)2M×N"role="presentation"}$$$$这个公式看上去复杂，其实很简单，由两部分构成：第一部分： 分母是所有我们关心的那类样本，也就是正样本，有 M个，以及其他样本有 N 个，这两类样本相对排序总共的组合可能性，是 M x N；第二部分： 分子也不复杂，原本是这样算的：第一名的排序值是r1，它在排序上不但比过了所有的负样本，而且比过了自己以外的正样本。但后者是自己人，所以组合数要排除，于是就有 n - M种组合，以此类推，排序值为 rM 的就贡献了 rM - 1，把这些加起来就是分子。关于 AUC，越接近 1 越好是肯定的，但是并不是越接近 0 就越差，最差的是接近0.5，如果 AUC 很接近 0 的话，只需要把模型预测的结果加个负号就能让 AUC接近 1，具体的原因自行体会。好了，已经介绍完排序的评价指标了，该主角出场了，BPR模型，它提出了一个优化准则和学习框架，使得原来传统的矩阵分解放进来能够焕发第二春。那到底 BPR 做了什么事情呢？主要有三点：1.  一个样本构造方法；2.  一个模型目标函数；3.  一个模型学习框架。通过这套三板斧，便可以脱离评分预测，来做专门优化排序的矩阵分解。下面详细说说这三板斧。``{=html}
### 构造样本 {#20.html#-}前面介绍的矩阵分解，在训练时候处理的样本是：用户、物品、反馈，这样的三元组形式。其中反馈又包含真实反馈和缺失值，缺失值充当的是负样本职责。BPR则不同，提出要关心的是物品之间对于用户的相对顺序，于是构造的样本是：用户、物品1、物品2、两个物品相对顺序，这样的四元组形式，其中，"两个物品的相对顺序"，取值是：1.  如果物品 1 是消费过的，而物品 2 不是，那么相对顺序取值为    1，是正样本；2.  如果物品 1 和物品 2 刚好相反，则是负样本；3.  样本中不包含其他情况：物品 1 和物品 2    都是消费过的，或者都是没消费过的。这样一来，学习的数据是反应用户偏好的相对顺序，而在使用时，面对的是所有用户还没消费过的物品，这些物品仍然可以在这样的模型下得到相对顺序，这就比三元组point-wise 样本要直观得多。
### 目标函数 {#20.html#-}现在，每条样本包含的是两个物品，样本预测目标是两个物品的相对顺序。按照机器学习的套路，就该要上目标函数了。要看 BPR 怎么完成矩阵分解，你依然需要像交替最小二乘那样的思想。先假装矩阵分解结果已经有了，于是就计算出用户对于每个物品的推荐分数，只不过这个推荐分数可能并不满足均方根误差最小，而是满足物品相对排序最佳。得到了用户和物品的推荐分数后，就可以计算四元组的样本中，物品 1 和物品 2的分数差，这个分数可能是正数，也可能是负数，也可能是 0。你和我当然都希望的情况是：如果物品 1 和物品 2 相对顺序为1，那么希望两者分数之差是个正数，而且越大越好；如果物品 1 和物品 2的相对顺序是 0，则希望分数之差是负数，且越小越好。用个符号来表示这个差：Xu12，表示的是对用户 u，物品 1 和物品 2的矩阵分解预测分数差。然后再用 sigmoid 函数把这个分数差压缩到 0 到 1之间。]{.MathJax_Preview style="color: inherit; display: none;"} {.MathJax_Display style="text-align: center;"}``{=html}[[Θ]{#20.html#MathJax-Span-51 .mistyle="font-family: MathJax_Main;"}[=]{#20.html#MathJax-Span-52 .mostyle="font-family: MathJax_Main; padding-left: 0.285em;"}[1]{#20.html#MathJax-Span-54.mnstyle="font-family: MathJax_Main;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.202em, 1000.43em, 4.144em, -999.998em); top: -4.656em; left: 50%; margin-left: -0.233em;"}1]{#20.html#MathJax-Span-56.mn style="font-family: MathJax_Main;"}[+]{#20.html#MathJax-Span-57 .mostyle="font-family: MathJax_Main; padding-left: 0.238em;"}[e]{#20.html#MathJax-Span-59.mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.438em, 1000.43em, 4.144em, -999.998em); top: -3.998em; left: 0em;"}[−]{#20.html#MathJax-Span-62.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}[(]{#20.html#MathJax-Span-63.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}[X[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"}]{#20.html#MathJax-Span-65.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.391em, 1000.61em, 4.144em, -999.998em); top: -3.998em; left: 0em;"}[u]{#20.html#MathJax-Span-68.mistyle="font-size: 50%; font-family: MathJax_Math-italic;"}[12]{#20.html#MathJax-Span-69.mnstyle="font-size: 50%; font-family: MathJax_Main;"}]{#20.html#MathJax-Span-67.mrow}]{#20.html#MathJax-Span-66.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; top: -3.904em; left: 0.567em;"}]{style="display: inline-block; position: relative; width: 1.414em; height: 0px;"}]{#20.html#MathJax-Span-64.msubsup}[)]{#20.html#MathJax-Span-70 .mostyle="font-size: 70.7%; font-family: MathJax_Main;"}]{#20.html#MathJax-Span-61.mrow}]{#20.html#MathJax-Span-60.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; top: -4.28em; left: 0.473em;"}]{style="display: inline-block; position: relative; width: 3.061em; height: 0px;"}]{#20.html#MathJax-Span-58.msubsup style="padding-left: 0.238em;"}]{#20.html#MathJax-Span-55.mrow}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.061em, 1004.8em, 4.238em, -999.998em); top: -3.198em; left: 50%; margin-left: -2.398em;"}[[]{style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 4.944em; height: 0px;"}[]{style="display: inline-block; width: 0px; height: 1.085em;"}]{style="position: absolute; clip: rect(0.896em, 1004.94em, 1.226em, -999.998em); top: -1.315em; left: 0em;"}]{style="display: inline-block; position: relative; width: 4.944em; height: 0px; margin-right: 0.144em; margin-left: 0.144em;"}]{#20.html#MathJax-Span-53.mfrac style="padding-left: 0.285em;"}]{#20.html#MathJax-Span-50.mrow}[]{style="display: inline-block; width: 0px; height: 2.355em;"}]{style="position: absolute; clip: rect(0.849em, 1007.34em, 3.344em, -999.998em); top: -2.351em; left: 0em;"}]{style="display: inline-block; position: relative; width: 7.344em; height: 0px; font-size: 125%;"}[]{style="display: inline-block; overflow: hidden; vertical-align: -1.115em; border-left: 0px solid; width: 0px; height: 2.885em;"}]{#20.html#MathJax-Span-49.mathstyle="width: 9.179em; display: inline-block;"}``{=html}[$$\Theta = \frac{1}{1 + e^{- (X_{u12})}}$$]{.MJX_Assistive_MathML.MJX_Assistive_MathML_Blockrole="presentation"}]{#20.html#MathJax-Element-2-Frame .MathJaxtabindex="0" style="text-align: center; position: relative;"mathml="Θ=11+e−(Xu12)"role="presentation"}$$$$也其实就是用这种方式预测了物品 1 排在物品 2前面的似然概率，所以最大化交叉熵就是目标函数了。目标函数通常还要防止过拟合，加上正则项，正则项其实认为模型参数还有个先验概率，这是贝叶斯学派的观点，也是BPR 这个名字中"贝叶斯"的来历。BPR 认为模型的先验概率符合正态分布，对应到正则化方法就是 L2正则，这些都属于机器学习的内容，这里不展开讲。我来把目标函数写一下：]{.MathJax_Preview style="color: inherit; display: none;"} {.MathJax_Display style="text-align: center;"}``{=html}[[[[[∏]{#20.html#MathJax-Span-74.mostyle="font-family: MathJax_Size2; vertical-align: 0em;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(2.92em, 1001.23em, 4.614em, -999.998em); top: -3.998em; left: 0.049em;"}[u]{#20.html#MathJax-Span-77.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[,]{#20.html#MathJax-Span-78.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}[i]{#20.html#MathJax-Span-79.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}[,]{#20.html#MathJax-Span-80.mostyle="font-size: 70.7%; font-family: MathJax_Main;"}[j]{#20.html#MathJax-Span-81.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#20.html#MathJax-Span-76.mrow}]{#20.html#MathJax-Span-75.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.391em, 1001.32em, 4.379em, -999.998em); top: -2.915em; left: 0em;"}]{style="display: inline-block; position: relative; width: 1.32em; height: 0px;"}]{#20.html#MathJax-Span-73.munderover}p]{#20.html#MathJax-Span-84 .mistyle="font-family: MathJax_Math-italic;"}[(]{#20.html#MathJax-Span-85.mo style="font-family: MathJax_Main;"}[i]{#20.html#MathJax-Span-86 .mistyle="font-family: MathJax_Math-italic;"}[\>]{#20.html#MathJax-Span-88.mostyle="font-family: MathJax_Main;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.344em, 1000.71em, 4.191em, -999.998em); top: -3.998em; left: 0em;"}[u]{#20.html#MathJax-Span-91.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#20.html#MathJax-Span-90.mrow}]{#20.html#MathJax-Span-89.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; top: -3.856em; left: 0.802em;"}]{style="display: inline-block; position: relative; width: 1.273em; height: 0px;"}]{#20.html#MathJax-Span-87.msubsup style="padding-left: 0.285em;"}[j]{#20.html#MathJax-Span-92 .mistyle="font-family: MathJax_Math-italic; padding-left: 0.285em;"}\|]{#20.html#MathJax-Span-95.mo style="font-family: MathJax_Main;"}]{#20.html#MathJax-Span-94.mrow}]{#20.html#MathJax-Span-93 .texatom}[θ]{#20.html#MathJax-Span-96.mistyle="font-family: MathJax_Math-italic;"}[)]{#20.html#MathJax-Span-97.mo style="font-family: MathJax_Main;"}[p]{#20.html#MathJax-Span-98 .mistyle="font-family: MathJax_Math-italic;"}[(]{#20.html#MathJax-Span-99.mo style="font-family: MathJax_Main;"}[θ]{#20.html#MathJax-Span-100 .mistyle="font-family: MathJax_Math-italic;"}[)]{#20.html#MathJax-Span-101.mo style="font-family: MathJax_Main;"}]{#20.html#MathJax-Span-83.mrow}]{#20.html#MathJax-Span-82 .texatomstyle="padding-left: 0.191em;"}]{#20.html#MathJax-Span-72.mrow}[]{style="display: inline-block; width: 0px; height: 2.355em;"}]{style="position: absolute; clip: rect(1.273em, 1007.81em, 3.814em, -999.998em); top: -2.351em; left: 0em;"}]{style="display: inline-block; position: relative; width: 7.908em; height: 0px; font-size: 125%;"}[]{style="display: inline-block; overflow: hidden; vertical-align: -1.703em; border-left: 0px solid; width: 0px; height: 2.944em;"}]{#20.html#MathJax-Span-71.mathstyle="width: 9.885em; display: inline-block;"}``{=html}[$$\prod\limits_{u,i,j}{p(i >_{u}j|\theta)p(\theta)}$$]{.MJX_Assistive_MathML.MJX_Assistive_MathML_Blockrole="presentation"}]{#20.html#MathJax-Element-3-Frame .MathJaxtabindex="0" style="text-align: center; position: relative;"mathml="∏u,i,jp(i>uj|θ)p(θ)"role="presentation"}$$$$所有样本都计算：模型参数先验概率 ptheta，和似然概率的乘积，最大化这个目标函数就能够得到分解后的矩阵参数，其中theta 就是分解后的矩阵参数。最后说一句，把这个目标函数化简和变形后，和把 AUC当成目标函数是非常相似的，也正因为如此，BPR 模型的作者敢于宣称该模型是为AUC 而生的。
### 训练方法 {#20.html#-}有了目标函数之后，就要有请训练方法了。显然是老当益壮的梯度下降可以承担这件事，梯度下降又有批量梯度和随机梯度下降两个选择，前者收敛慢，后者训练快却不稳定。因此BPR的作者使用了一个介于两者之间的训练方法，结合重复抽样的梯度下降。具体来说是这样做的：1.  从全量样本中有放回地随机抽取一部分样本；2.  用这部分样本，采用随机梯度下降优化目标函数，更新模型参数；3.  重复步骤 1，直到满足停止条件。这样，就得到了一个更符合推荐排序要求的矩阵分解模型了。