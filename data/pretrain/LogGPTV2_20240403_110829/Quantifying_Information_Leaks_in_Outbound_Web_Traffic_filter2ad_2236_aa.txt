title:Quantifying Information Leaks in Outbound Web Traffic
author:Kevin Borders and
Atul Prakash
2009 30th IEEE Symposium on Security and Privacy
Quantifying Information Leaks in Outbound Web Traffic 
Kevin Borders 
Web Tap Security, Inc. 
Ann Arbor, MI 
PI:EMAIL 
Abstract 
from 
leaving 
information 
As the Internet grows and network bandwidth continues 
to  increase,  administrators  are  faced  with  the  task  of 
keeping  confidential 
their 
networks.  Today’s  network  traffic  is  so  voluminous  that 
manual  inspection  would  be  unreasonably  expensive.  In 
response,  researchers  have  created  data  loss  prevention 
systems  that  check  outgoing  traffic  for  known  confidential 
information.  These  systems  stop  naïve  adversaries  from 
leaking  data,  but  are  fundamentally  unable  to  identify 
encrypted or obfuscated information leaks. What remains is 
a high-capacity pipe for tunneling data to the Internet. 
We  present  an  approach  for  quantifying  information 
leak  capacity  in  network  traffic.  Instead  of  trying  to  detect 
the  presence  of  sensitive  data—an  impossible  task  in  the 
general  case—our  goal  is  to  measure  and  constrain  its 
maximum  volume.  We  take  advantage  of  the  insight  that 
most  network  traffic  is  repeated  or  determined  by  external 
information,  such  as  protocol  specifications  or  messages 
sent  by  a  server.  By  filtering  this  data,  we  can  isolate  and 
quantify  true  information  flowing  from  a  computer.  In  this 
paper,  we  present  measurement  algorithms 
the 
Hypertext Transfer Protocol (HTTP), the main protocol for 
web  browsing.  When  applied  to  real  web  browsing  traffic, 
the  algorithms  were  able  to  discount  98.5%  of  measured 
bytes and effectively isolate information leaks. 
for 
1.  Introduction 
Network-based information leaks pose a serious threat to 
confidentiality.  They  are  the  primary  means  by  which 
hackers  extract  data  from  compromised  computers.  The 
network can also serve as an avenue for insider leaks, which, 
according to a 2007 CSI/FBI survey, are the most prevalent 
security threat for organizations [17]. Because the volume of 
legitimate network traffic is so large, it is easy for attackers 
to  blend  in  with  normal  activity,  making  leak  prevention 
difficult.  In  one  experiment,  a  single  computer  browsing  a 
social networking site for 30 minutes generated over 1.3 MB 
of legitimate request data—the equivalent of about 195,000 
credit card numbers. Manually analyzing network traffic for 
leaks would be unreasonably expensive and error-prone. Due 
to  the  heavy  volume  of  normal  traffic,  limiting  network 
1081-6011/09 $25.00 © 2009 IEEE
DOI 10.1109/SP.2009.9
129
Atul Prakash 
University of Michigan 
Ann Arbor, MI 
PI:EMAIL
traffic  based  on  the  raw  byte  count  would  only  help  stop 
large information leaks.  
In  response  to  the  threat  of  network-based  information 
leaks,  researchers  have  developed  data-loss  prevention 
(DLP)  systems  [18,  24].  DLP  systems  work  by  searching 
through  outbound  network  traffic  for  known  sensitive 
information, such as credit card and social security numbers. 
Some  even  catalog  sensitive  documents  and  look  for 
excerpts  in  outbound  traffic.  Although  they  are  effective  at 
stopping  accidental  and  plain-text  leaks,  DLP  systems  are 
fundamentally  unable  to  detect  obfuscated  information 
flows.  They  leave  an  open  channel  for  leaking  data  to  the 
Internet. 
We  introduce  a  new  approach  for  precisely  quantifying 
information  leak  capacity  in  network  traffic.  Rather  than 
searching  for  known  sensitive  data—an  impossible  task  in 
the  general  case—we  aim  to  measure  and  constrain  its 
maximum  volume.  This  research  addresses  the  threat  of  a 
hacker  or  malicious  insider  extracting  sensitive  information 
from  a  network.  He  or  she  could  try  to  steal  data  without 
being detected by hiding it in the noise of normal outbound 
traffic.  For  web  traffic,  this  often  means  stashing  bytes  in 
paths or header fields within seemingly benign requests. To 
combat this threat, we exploit the fact that a large portion of 
legitimate  network  traffic  is  repeated  or  constrained  by 
protocol  specifications.  This  fixed  data  can  be  ignored, 
which isolates real information leaving a network, regardless 
of data hiding techniques. 
its 
The  leak  measurement  techniques  presented  here  focus 
on  the  Hypertext  Transfer  Protocol  (HTTP),  the  main 
protocol  for  web  browsing.  They  take  advantage  of  HTTP 
interaction  with  Hypertext  Markup  Language 
and 
(HTML)  documents  and  Javascript  code 
to  quantify 
information leak capacity. The basic idea is to compute the 
expected  content  of  HTTP  requests  using  only  externally 
available  information,  including  previous  network  requests, 
previous server responses, and protocol specifications. Then, 
the amount of unconstrained outbound bandwidth is equal to 
the  edit  distance  (edit  distance  is  the  size  of  the  edit  list 
required to transform one string into another) between actual 
and  expected  requests,  plus  timing  information.  Given 
correct  assumptions  about  timing  channel  characteristics, 
these results may overestimate, but will never underestimate 
the  true  size  of  information  leaks,  thus  serving  as  a  tight 
upper bound on information leakage.  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
i
)
n
m
/
B
K
(
w
a
R
-
h
t
d
w
d
n
a
B
i
Information Leak
Normal Traffic
 160
 140
 120
 100
 80
 60
 40
 20
 0
10:00
12:00
14:00
16:00
Time of day
   (a) 
i
)
n
m
/
B
K
(
e
s
i
c
e
r
P
-
h
t
d
w
d
n
a
B
i
Information Leak
Normal Traffic
 6
 5
 4
 3
 2
 1
 0
18:00
20:00
10:00
12:00
             (b) 
14:00
16:00
Time of day
18:00
20:00
Figure 1. Graph of outbound web traffic during a typical work day with a 100 Kilobyte  
information leak inserted. (a) shows the raw byte count, where the leak is barely noticeable, and 
(b) shows the precise unconstrained bandwidth measurement, in which the leak stands out prominently. 
One  option  for  measuring  unconstrained  bandwidth 
would be to use a traditional compression algorithm like gzip 
[8] or bzip2 [20]. This  would  involve building  up  a  library 
from  previous  messages  and  only  counting  the  incremental 
size  of  new  requests.  Traditional  compression  can  help  for 
simple 
large  repeated  substrings. 
However,  this  protocol-agnostic  approach  fails  to  capture 
complex  interactions  between  requests  and  replies  that  go 
beyond string repetition. 
that  have 
requests 
The  analysis  techniques  presented  in  this  paper  take 
advantage of protocol interactions. Parsing all of the links on 
a  web  page,  for  example,  helps  construct  an  accurate 
distribution of expected requests. Our analysis also involves 
executing  scripts  in  a  simulated  browser  environment  to 
extract  links  that  cannot  be  derived  from  static  processing.  
These 
to  a  much  more  precise 
measurement  of  information  in  outbound  web  traffic  than 
conventional compression algorithms. 
improvements 
lead 
illustrates 
Figure  1 
the  benefit  of  precise 
leak 
quantification.  The  graphs  show  bandwidth  from  legitimate 
web  browsing  over  a  one-day  period  in  black.  A  100  KB 
information leak was inserted into the traffic and can be seen 
in a lighter color. This leak was deliberately inserted in short 
bursts, so as to more closely resemble legitimate web traffic 
and  avoid  detection  methods  that  look  at  request  regularity 
[3]. The left graph shows raw request bandwidth. The leak is 
barely noticeable here and easily blends in with the noise of 
normal  activity.  After  running  the  same  traffic  through  our 
unconstrained bandwidth measurement engine, however, the 
leak  stands  out  dramatically  from  normal  traffic.  It  is 
important  to  note  that  more  accurate  traffic  measurement 
does not completely stop information leaks from slipping by 
undetected;  it  only  makes  it  possible  to  identify  smaller 
leaks.  Our  analysis  techniques  force  a  leak  that  would 
normally blend in with a week’s worth of traffic to be spread 
out over an entire year.  
We  evaluated  our  leak  measurement  techniques  on  real 
browsing  data  from  10  users  over  30  days,  which  included 
over  500,000  requests.  The  results  were  compared  to  a 
simple  calculation  described  in  prior  research  [3],  and  to 
incremental  gzip  compression  [8]. The  average request  size 
using  the  leak  measurement  techniques  described  in  this 
paper  was  15.8  bytes,  1.6%  of  the  raw  byte  count.  The 
average  size  for  gzip  was  132  bytes,  and  for  the  simple 
measurement was 243 bytes. The experiments show that our 
approach is an order of magnitude better than traditional gzip 
compression. 
This  work  focuses  specifically  on  analyzing  leaks  in 
HTTP  traffic  for  a  few  reasons.  First,  it  is  the  primary 
protocol for web browsing and accounts for a large portion 
of overall traffic. Many networks, particularly those in which 
confidentiality  is  a  high  priority,  will  only  allow  outbound 
HTTP traffic and block everything else by forcing all traffic 
to go through a proxy server. In this scenario, HTTP would 
be the only option for directly leaking data. Another reason 
for focusing on HTTP is that a high percentage of its request 
data  can  be  filtered  out  by  eliminating  repeated  and 
constrained values. 
The principles we use to measure leaks in HTTP traffic 
are  likely  to  work  for  other  protocols  as  well.  Binary 
protocols  for  instant  messaging,  secure  shell  access,  and 
domain  name  resolution  all  contain  a  number  of  fixed  and 
repeated values. Furthermore, correlation between protocols 
may  enable  filtering  of  DNS  lookups.  Extending  a  similar 
methodology to outbound SMTP (e-mail) traffic is likely to 
be more challenging. E-mail primarily consists of free-form 
data  and  only  contains  small  fixed  fields.  However,  the 
unconstrained data in e-mails is usually text, for which there 
are  well-known  methods  of  determining  the  information 
content [21], or file attachments. These attachments are made 
up of data written out in a specific file format, which could 
be analyzed in a manner similar to HTTP. In fact, researchers 
130
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
have already examined ways of identifying information that 
has been  hidden in  files  with  steganography  by  looking  for 
additional  unexpected  entropy  [2].  Further  investigation  of 
leak measurement techniques for file attachments and other 
protocols is future work. 
The measurement techniques in this paper do not provide 
an unconstrained bandwidth measurement for fully encrypted 
traffic. (If a hacker tries to hide or tunnel encrypted data in 
an unencrypted protocol, it can be measured.) All networks 
that  allow  outbound  encrypted  traffic  must  deal  with  this 