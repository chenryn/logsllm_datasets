items, and choosing the size of each bucket can have a big
impact on the eﬃciency of the scheme: we have to perform
searches over B log N items for buckets of size B. However,
if the buckets are too small, there is a high probability that
some element will “overﬂow” its bucket during the eviction
process. This overﬂow event can leak information about the
access pattern, so it is important to choose large enough
buckets. Shi et al. [19] prove that in an ORAM containing
N elements, if the buckets are of size (log(M N/δ)), then
after M memory accesses, the probability of overﬂow is less
−20, it suﬃces
than δ. It follows that to get, say, security 2
to have buckets of size O(log N ), but the constant in the
notation is important.
In Figure 4 we provide our results from simulating over-
2We defer the proof of security to the full version. However,
to give some intuition, note that as long as the assignment
of the encryption randomness is independent of the access
pattern, nothing can be learned by the client during decryp-
tion. To make this formal, we show that we can simulate
his view by choosing random values for each bucket, storing
them between lookups, and sending those same values the
next time that bucket is scanned. This simulation would fail
only if the assignment of the random values to buckets were
somehow dependent on the particular content of the RAM.
519 100
 80
 60
 40
 20
 0
t
e
g
a
n
e
c
r
e
P
e
r
u
l
i
a
F
 18
 20
 22
 24
 26
Bucket Size
 28
 30
 32
Figure 4: Overﬂow probability as a function of
bucket size,
instructions on a
database of 65536 items.
for 65536 virtual
ﬂow for various bucket sizes. Notice that the value ap-
proaches 0 only as we approach 2 log N , and in fact the
probability of failure is very high for values closer to log N .
Based on these simulations, we have chosen to use buckets
of size 2 log N . We ran our experiment with N = 216 and
estimated the probability of overﬂowing any bucket when
we insert all N items, and then perform an additional N
operations on the resulting database. We used 10,000 tri-
als in the experiment. Note that for the speciﬁc example
of binary search, we only need to perform log N operations
on the database; for 216 elements and a bucket size of 32,
we determined with conﬁdence of 98% that the probability
of overﬂow is less than .0001. The runtime of our protocol
(roughly) doubles when our bucket size doubles, so although
we might prefer still stronger security guarantees, increasing
the bucket size to 3 log N will have a considerable impact on
performance.
r
Computing Addresses Recursively.
Recall that the leaf node assigned to item v(i) in the ith
tree is stored in item v(i+1) = (cid:8) v(i)
(cid:9) of the i + 1th tree.
In Step 2, where the two parties compute shares of v(i) for
each i in 1, . . . ,log r N , we observe that if r is a power of
2, each party can compute its own shares locally from its
share of v. If r = 2j and v = vC ⊕ vS, then we can obtain
v(i) = (cid:8) v
ri (cid:9) by deleting the last i · j bits of v. Similarly v(i)
S can be obtained by deleting the last i · j bits from
and v(i)
the values vC and vS. This allows us to avoid performing
another secure computation when recovering shares of the
recursive addresses.
C
Node Storage Instantiation.
Shi et al. [19] point out that the data stored in each node
of the tree could itself be stored in another ORAM scheme,
either using the same tree-based scheme described above, or
using any of the other existing schemes. We have chosen to
simply store the items in an array, performing a linear scan
on the entire node. For the data sets we consider, N = 106
or 107, and 20 ≤ log N ≤ 25. Replacing a linear scan with an
ORAM scheme costing O(log3 N ), or even O(log2 N ), sim-
ply does not pay oﬀ. We could consider the simpler ORAM
√
of Goldreich and Ostrovsky [7] that has overhead O(
N ),
but the cost of shuﬄing around data items and computing
pseudorandom functions inside garbled circuits would cer-
tainly erase any savings.
Using Client Storage.
When the client and server search for an item v, after
they recover the leaf node assigned to v, the server fetches
the log N buckets along the path to the leaf, each bucket
containing up to log N items. The parties then perform a
secure computation over the items, looking for a match on
v. We have a choice in how to carry out this secure com-
putation: we could compare one item at a time with v, or
search as many as log2 N items in one secure computation.
The advantage to searching fewer items is that the garbled
circuit will be smaller, requiring less client-side storage for
the computation. The disadvantage is that each computa-
tion will have to output secret shares of the state of the
search, indicating whether v has already been found, and, if
so, what the value of its payload is; each computation will
also have to take the shares of this state as input, and recon-
struct the state before continuing with the search. The extra
state information will require additional wires and gates in
the garbled circuits, as well as additional encryptions and
decryptions for preparing and evaluating the circuit. We
have chosen to perform just a single secure computation
over log2 N items, using the maximal client storage, and
the minimal computation. However, we note that the addi-
tional computation would have little impact,3 and we could
instead reduce the client storage at relatively little cost. To
compute the circuit that searches log2 N items, the client
needs to store approximately 400, 000 encryption keys, each
80 bits long.
Garbled Circuit Optimizations.
The most computationally expensive part of Yao’s garbled
circuit protocol is often thought to be the oblivious transfer
(OT) sub-protocol [4]. The parties must employ OT once for
every input wire of the party that evaluates the circuit, and
each such application (naively performed) requires expensive
operations such as exponentiations. We use the following
known optimizations to reduce OT costs and to further push
its computation almost entirely to the preprocessing stage,
before the parties begin the computation (even before they
have their inputs), reducing the on-line OT computations to
just simple XOR operations.
The most important technique we use is the OT extension
protocol of Ishai et al. [11], which allows to compute an ar-
bitrary number of OT instances, given a small (security pa-
rameter) number of “base” OT instances. We implement the
base instances using the protocol of Naor and Pinkas [16],
which requires six exponentiations in a prime order group,
three of which can be computed during pre-processing. Fol-
lowing [11], the remaining OT instances will only cost us
a couple of hash evaluations per instance. We then push
these hash function evaluations to the preprocessing stage,
in a way that requires only XOR during the on-line stage.
Finally, Beaver’s technique [1] allows us to start computing
the OT’s in the preprocessing stage as well, by running OT
3This is because sharing the state and reconstructing the
state are both done using XOR gates, which are particularly
cheap for garbled circuits, as we discuss below.
520random inputs for both parties; the output is then corrected
by appropriately sending real input XORed with the used
random inputs in the online stage.
We rely on several other known garbled circuit optimiza-
tions. First, we use the free XOR gates technique of Kolesnikov
and Schneider [12], which results in more than 60% improve-
ment in the evaluation time for an XOR gate, compared to
other gates. Accordingly, we aim to construct our circuits
using as few non-XOR gates as possible.
Second, we utilize a wider variety of gates (as opposed
to the traditional Boolean AND, OR, XOR, NAND gates).
This pays oﬀ since in the garbled circuit construction every
non-XOR gate requires performing encryption and decryp-
tion, and all gates of the same size are equally costly in this
regard. In our implementation we construct and use 10 of
the 16 possible gates that have 2 input bits and one output
bit. We also rely heavily on the multiplexer gate on 3 input
bits; this gate uses the ﬁrst input bit to select the output
from the other two input bits. In one circuit, we use a 16-bit
multiplexer, which uses 4 input bits to select from 16 other
inputs.
Finally, we utilize pipelined circuit execution, which avoids
the naive traditional approach where one party sends the
garbled circuit in its entirety to the second one. This naive
approach is often impractical, as for large inputs the garbled
circuits can be several gigabytes in size, and the receiving
party cannot start the evaluation until the entire garbled
circuit has been generated and transmitted and stored in
his memory. To mitigate that, we follow the technique in-
troduced by Huang et al. [10], allowing the generation and
evaluation of the garbled circuit to be executed in parallel,
where the sender can transmit each garbled gate as soon as
he generates it, and continue to garble the next gates while
the receiver is evaluating the received gates, thus improving
the total evaluation time. This also alleviates the memory
requirements for both parties since the garbler can discards
the gates he has sent, and the receiver can discard a gate
that he has evaluated.
5.
IMPLEMENTATION
The goal of our experiments was to evaluate and com-
pare execution times for two protocols implementing binary
search: one using standard optimized Yao, and the other
using our ORAM-based approach described in the previous
sections.
In our experiments, each of the two parties was
run on a diﬀerent server, each with a Intel Xeon 2.5GHz
CPU, 16 GB of RAM, two 500 GB hard disk drives, and
running a 64-bit Ubuntu operating system. They each had
a 1 Gbit ethernet interface, and were connected through a
1Gbit switch.
Before running our experiments, we ﬁrst populated the
database structure on the server side: in our ORAM pro-
tocol, we randomly placed the encrypted data throughout
the ORAM structure, and in the Yao protocol performing
a linear scan, we simply stored the data in a large array.
We then generated and stored the necessary circuit descrip-
tions on each machine. Finally, the two parties interacted
to pre-process the expensive part of the OT operations, in a
manner that is independent of their inputs. We did not cre-
ate the garbled gates for the circuits during pre-processing;
the server begins generating these once contacted by the
client. However, the server sent garbled gates to the client
as they were ready, so as to minimize the impact on the total
Our Protocol
Basic Yao
 1600
 1400
 1200
 1000
 800
 600
 400
 200
)
s
(
e
m
T
i
 0
 13
 14
 15
 16
 17
 18
 19
 20
 21
Log2 (# entries)
Figure 5: Time for performing binary search using
our protocol vs. time for performing binary search
using a standard garbled-circuit protocol as a func-
tion of the number of database entries. Each entry
is 512 bits long.
computation time. When we measured time in our experi-
ments, we included: 1) the online phase of the OT protocol,
2) the time it takes to create the garbled gates and transfer
the resulting ciphertexts, and 3) the processing time of the
garbled circuits.
5.1 Performance
In Figure 5, we compare the performance of our construc-
tion when computing a ORAM-based binary search to the
performance of a Yao-based linear scan. We have plotted the
x-axis on a logarithmic scale to improve readability. From
the plot it can be seen that we outperform the Yao lin-
ear scan by a factor of 3 when dealing with input of size
219, completing the log N operations in less than 7 minutes,
compared to 24 minutes for Yao. For input of size 220, we
complete our computation in 8.3 minutes, while the Yao im-
plementation failed to complete (we were unable to ﬁnish
the linear scan because the OS began swapping memory).
While we had no trouble running our ORAM-based proto-
col on input of size 220, for N = 221, we ran out of memory
when populating the server’s ORAM during pre-processing.
In Figure 6 we demonstrate how our protocol performs
when evaluating a single read operation over N data ele-
ments of size 512 bits, for N ∈ {216, 217, 218, 219, 220}. We
note that runtime for binary search using the ORAM is al-
most exactly the time it takes to run log N single lookups;
this is expected, since the circuit for computing the next
RAM instruction is very small. For 216 items and a bucket
size of 32, a single operation takes 27 seconds, while for
220 items and buckets of size 40, it takes about 50 seconds.
Recall that when relying only on secure computation, com-
puting any function, even those making a constant number
of lookups, requires a full linear scan; in this scenario, the
performance gain is more than 30-fold. One example of such
a function allows the client to search a large social network,
in order to fetch the neighborhood of a particular node.
521)
s