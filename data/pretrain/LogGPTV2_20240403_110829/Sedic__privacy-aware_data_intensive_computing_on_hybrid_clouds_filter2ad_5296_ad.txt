we captured on April 16th, 2010, in an attempt to ﬁnd out the com-
ment words (e.g., “wonderful”, “worst”, etc.) associated with dif-
ferent product brand names. We randomly select some user and
mark their tweets as sensitive date, just as if they set their privacy
preferences to share data only with their friends. The two examples
for intrusion detection systems (IDS) were all based on the DARPA
Intrusion Detection Evaluation dataset [5]. The dataset includes the
snifﬁng data from external networks, which was marked as public
in our study, as well as that from the internal network, which was
considered to be sensitive. Our analyses on the data involved ﬁnd-
ing all the ports connected by each IP address and determining the
amount of trafﬁc generated by individual hosts. The last two jobs
prepared a Naive Bayesian classiﬁer for detecting email spam: one
of them counted the occurrences of a set of words on a spam key-
word list and the other counted the total number of words in a large
dataset. We utilized the published Enron email dataset [6] as the
private data and a SPAM archive [12] as the public data. These
jobs and their related datasets are described by Table 2 and 3. We
also performed an additional experiment to understand the perfor-
mance of Sedic on the dataset with various proportions of sensitive
records, using a job that computed the average lengths of packet
payloads over the IDS dataset. The code of the job came from
Hadoop sample code [28].
Table 2: Descriptions of Hadoop Jobs
Job
Port Scan
Detection
Trafﬁc
Statistics
Data set
IDS data set
IDS data set
Email Word
Count
Spam data set
Spam Keyword
Count
Spam date set
Grep
Twitter
Descriptions
Find the TCP ports connected by
each host
Count the total amount of the trafﬁc
generated by each host (for detecting
denial of service attacks)
Count the total number of words
in the spam dataset (for calculating
Bayes probability)
Count the occurrences of each
keyword on a given spam keyword
list ﬁle(for calculating Bayes
probability)
Search for word patterns according to
predeﬁned regular expressions
within the dataset, e.g., brand names
and comment words such as
awesome, wonderful, worst etc.
5. EVALUATION
In this section, we report the evaluation study of our privacy-
aware MapReduce framework. Our objective is to understand whether
these techniques are capable of signiﬁcantly reducing the workload
of the private cloud, scaling to a large amount of data and also
maintaining an acceptable level of overall overheads, particularly a
limited inter-cloud communication cost. To this end, we evaluated
the performance of realistic, large-scale Hadoop jobs using our pro-
totype, over a large-scale cloud test-bed. The details of the study
are elaborated here.
The hybrid cloud. We built our hybrid cloud on FutureGrid [7], an
NSF-supported, across-the-country cloud test-bed [40]. The public
cloud included 3 nodes located at the University of Chicago. Each
machine has 8-core 2.93 GHz Intel Xeon, 24 GB memory, 862 GB
local disk and Linux 2.6.18. The private cloud involved 3 nodes
at Indiana University, each with 8 to 24 cores of Intel Xeon CPUs,
32G or 48GB memory, 1.6TB disk and also Linux 2.6.18. These
two clouds were connected by a 40 MBps link.
5.2 Experimental Results
5.1 Experimental Setting
Here, we describe the setting of our experimental study, includ-
ing the MapReduce jobs and the data we used to evaluate our pro-
totype, and our hybrid cloud-computing environment.
In our experiment, we evaluated both the effectiveness of our
code transformation tool and the performance of our execution frame-
work, which includes computational and communication overheads.
Code transformation. Like the typical reduction performed in a
522Table 3: Descriptions of Datasets
Name
Sensitive Data
Public Data
Size of
Size of
Percentage of
Sensitive Data
Public Data
Sensitive Content
IDS data set
The tcpdump ﬁles for inside
Spam data set
The Enron Email Dataset
Twitter data set
Tweets from randomly chosen users who
are assumed to prefer to protect their tweets
The tcpdump ﬁles for outside
SPAM archive download from
http://untroubled.org/spam/
17GB
1.3GB
15GB
0.8GB
Tweets from other users
123MB
491MB
54%
62%
20%
MapReduce job, all the reducers encountered in our experimental
study were pretty simple. Actually, most jobs simply summed up
the values produced by mappers according to the keys. Our analysis
tool easily identiﬁed the fold loops within them and determined
that they were all commutative and associative. These reducers
were also set as combiners so that they could combine the data
at the public and private clouds respectively, and then reduce the
outputs on the private node. The exceptions include the tasks for
collecting all the ports associated with individual IP addresses and
for determining the average lengths of packet payloads. The fold
loop of the former did not have a loop dependency and therefore
was directly used as a combiner. The latter calculated the mean of
all its inputs, which was analyzed and transformed as described in
Section 4.2.
Performance. We ran the job that computes average payload lengths
on the IDS dataset to analyze the performance of our execution
platform in the presence of different public/sensitive data mixtures.
Speciﬁcally, we considered the situation that sensitive data was
uniformly distributed to every data block, since it represents the
worst-case scenario with the most negative impact on the perfor-
mance of our system. Under this scenario, we gradually raised
the proportion of sensitive information, from 10% to 50%, to eval-
uate the amount of the computation being outsourced to the public
cloud. The workload here was measured by the total task execu-
tion time, which was summed over the execution time of individual
tasks (map and reduce) processed by private nodes. Such work-
load was compared with that of running the whole job within the
private cloud, which we call baseline, to estimate the ratio of the
computation being outsourced to the public cloud. We got all the
runtime statistics from Hadoop log ﬁles, which has the detailed in-
formation about each task, including the starting time, running on
which TaskTracker and ﬁnishing time. The outcomes of this study
are illustrated by Figure 6. The workload the private cloud shoul-
dered increases from about 69.68 seconds (an outsource ratio about
76%), when 10% of the dataset was sensitive, to about 168.88 sec-
onds (about 40% of outsource ratio), when 40% of the data was
private, compared with that caused by running the whole job on
the private nodes. The workload dropped a bit when the ratio of
the sensitive data went to 50%, probably due to the randomness in
execution. Remember that the map task undertaken by the private
cloud only processes the sensitive records within each block (Sec-
tion 3.2). However, there apparently were noticeable overheads
associated with initializing the task and seeking for these records
within a block, which brought down the performance.
The performance of other jobs are described in Table 4. What we
can see here is that all of them successfully moved a large portion of
computation to the public cloud, in accordance with the ratio of the
private information within the individual datasets they worked on.
This even happened to the job that processed the Twitter dataset,
whose distribution of sensitive data was more even compared with
the other two datasets. In some cases, the outsource ratios are even
higher than the proportion of the public data within the dataset
(Word Count). We believe that this was caused by the randomness
in the job execution. The overall job execution time was also low
300
250
200
150
100
50
)
d
n
o
c
e
s
(
e
m
i
t
0
0
Private CPU Time
Base Line
10
40
percentage of sensitive data
20
30
50
Figure 6: Performance vs. Sensitive Data Ratio
for our prototype: as an example, for port scan detection, it took
about 3 minutes to complete the whole job (on 6 nodes, 3 private
and 3 public), even below the baseline, which used about 6 minutes
(on 3 private nodes). This is actually quite reasonable, given the
fact that our hybrid-cloud computing leveraged the resources from
the public cloud.
Communication overheads. Our experiments also show that the
new reduction structure automatically generated by Sedic did con-
tribute to the effective control of inter-cloud data transfers, one of
the major hurdles to the extensive use of cloud computing. Specif-
ically, we compared the bandwidth consumption of the original
Hadoop jobs with that incurred after their reducers were trans-
formed. The results are presented in Table 4. As we can see here, in
the presence of code transformation, the ﬂood of the trafﬁc stream
between the public and the private clouds was reduced to trick-
les: for example, in the case of port scan detection, over 1.5 GB
data was reduced to merely 8.2 MB using the automatically gen-
erated combiner on the public cloud. This saving in bandwidth
consumption can signiﬁcantly improve the performance of hybrid-
cloud computation, given the fact that inter-cloud bandwidth is typ-
ically low. For example, the link between Amazon EC2 and Ama-
zon S3 is around 50MBps [1] and the connections EC2 receives
from the outside are often less than 10MBps [2]. The experimental
results offer strong evidence that our code analysis and transforma-
tion techniques indeed work.
6. RELATED WORK
Security protection in cloud computing. Most system security re-
search on the cloud focuses on data-storage security [47] and virtu-
alization security [33]. Little effort has been made to facilitate secu-
rity and privacy protections during the computation speciﬁc to this
new computing platform (despite the rich literature on more generic
secure-computing techniques). One exception is Airavat [41], a
system that ensures mandatory access control and differential pri-
vacy [26] when MapReduce operations are performed on sensitive
data. Different from Airavat, which trusts the cloud platform it is
running upon, our work aims at protecting sensitive data from the
public cloud, which will be achieved by a security mechanism de-
signed for the hybrid-cloud platform.
523Table 4: Performance
ID
Job
1
2
3
4
5
Port scan detection
Trafﬁc statistics
Word count
Spam Keyword
Count
Grep
Task Execution Time
Total Task Execution
of the Whole Job
Time in Private
(Baseline, in seconds)
Cloud(in seconds)
Outsource
Public
Communication
Communication
Ratio
Data Ratio
w/o Transformation
w. Transformation
Inter-cloud
Inter-cloud
1909.79
1723.87
148.48
155.70
17.85
928.86
890.25
66.12
99.03
5.24