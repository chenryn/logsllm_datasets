Input and Data Flow 
The majority of software vulnerabilities result from unexpected behaviors triggered 
by a program's response to malicious data. So the first question to address is how 
exactly malicious data gets accepted by the system and causes such a serious impact. 
The best way to explain it is by starting with a simple example of a buffer overflow 
vulnerability. 
Consider a UNIX program that contains a buffer overflow triggered by an overly long 
command-line argument. In this case, the malicious data is user input that comes 
directly from an attacker via the command-line interface. This data travels through 
the program until some function uses it in an unsafe way, leading to an exploitable 
situation. 
For most vulnerabilities, you'll find some piece of malicious data that an attacker 
injects into the system to trigger the exploit. However, this malicious data might 
come into play through a far more circuitous route than direct user input. This data 
can come from several different sources and through several different interfaces. It 
might also pass through multiple components of a system and be modified a great 
deal before it reaches the location where it ultimately triggers an exploitable condition. 
Consequently, when reviewing a software system, one of the most useful attributes to 
consider is the flow of data throughout the system's various components. 
For example, you have an application that handles scheduling meetings for a large 
organization. At the end of every month, the application generates a report of all 
meetings coordinated in this cycle, including a brief summary of each meeting. Close 
inspection of the code reveals that when the application creates this summary, a 
meeting description larger than 1,000 characters results in an exploitable buffer 
overflow condition. 
To exploit this vulnerability, you would have to create a new meeting with a 
description longer than 1,000 characters, and then have the application schedule the 
meeting. Then you would need to wait until the monthly report was created to see 
whether the exploit worked. Your malicious data would have to pass through several 
components of the system and survive being stored in a database, all the while 
avoiding being spotted by another user of the system. Correspondingly, you have to 
evaluate the feasibility of this attack vector as a security reviewer. This viewpoint 
involves analyzing the flow of the meeting description from its initial creation, through 
multiple application components, and finally to its use in the vulnerable report 
generation code. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
34 
This process of tracing data flow is central to reviews of both the design and 
implementation of software. User-malleable data presents a serious threat to the 
system, and tracing the end-to-end flow of data is the main way to evaluate this 
threat. Typically, you must identify where user-malleable data enters the system 
through an interface to the outside world, such as a command line or Web request. 
Then you study the different ways in which user-malleable data can travel through 
the system, all the while looking for any potentially exploitable code that acts on the 
data. It's likely the data will pass through multiple components of a software system 
and be validated and manipulated at several points throughout its life span. 
This process isn't always straightforward. Often you find a piece of code that's almost 
vulnerable but ends up being safe because the malicious input is caught or filtered 
earlier in the data flow. More often than you would expect, the exploit is prevented 
only through happenstance; for example, a developer introduces some code for a 
reason completely unrelated to security, but it has the side effect of protecting a 
vulnerable component later down the data flow. Also, tracing data flow in a real-world 
application can be exceedingly difficult. Complex systems often develop organically, 
resulting in highly fragmented data flows. The actual data might traverse dozens of 
components and delve in and out of third-party framework code during the process of 
handling a single user request. 
Trust Relationships 
Different components in a software system place varying degrees of trust in each 
other, and it's important to understand these trust relationships when analyzing the 
security of a given software system. Trust relationships are integral to the flow of 
data, as the level of trust between components often determines the amount of 
validation that happens to the data exchanged between them. 
Designers and developers often consider an interface between two components to be 
trusted or designate a peer or supporting software component as trusted. This means 
they generally believe that the trusted component is impervious to malicious 
interference, and they feel safe in making assumptions about that component's data 
and behavior. Naturally, if this trust is misplaced, and an attacker can access or 
manipulate trusted entities, system security can fall like dominos. 
Speaking of dominos, when evaluating trust relationships in a system, it's important 
to appreciate the transitive nature of trust. For example, if your software system 
trusts a particular external component, and that component in turn trusts a certain 
network, your system has indirectly placed trust in that network. If the component's 
trust in the network is poorly placed, it might fall victim to an attack that ends up 
putting your software at risk. 
Assumptions and Misplaced Trust 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
35 
Another useful way of looking at software flaws is to think of them in terms of 
programmers and designers making unfounded assumptions when they create 
software. Developers can make incorrect assumptions about many aspects of a piece 
of software, including the validity and format of incoming data, the security of 
supporting programs, the potential hostility of its environment, the capabilities of its 
attackers and users, and even the behaviors and nuances of particular application 
programming interface (API) calls or language features. 
The concept of inappropriate assumptions is closely related to the concept of 
misplaced trust because you can say that placing undue trust in a component is much 
the same as making an unfounded assumption about that component. The following 
sections discuss several ways in which developers can make security-relevant 
mistakes by making unfounded assumptions and extending undeserved trust. 
Input 
As stated earlier, the majority of software vulnerabilities are triggered by attackers 
injecting malicious data into software systems. One reason this data can cause such 
trouble is that software often places too much trust in its communication peers and 
makes assumptions about the data's potential origins and contents. 
Specifically, when developers write code to process data, they often make 
assumptions about the user or software component providing that data. When 
handling user input, developers often assume users aren't likely to do things such as 
enter a 5,000-character street address containing nonprintable symbols. Similarly, if 
developers are writing code for a programmatic interface between two software 
components, they usually make assumptions about the input being well formed. For 
example, they might not anticipate a program placing a negative length binary record 
in a file or sending a network request that's four billion bytes long. 
In contrast, attackers looking at input-handling code try to consider every possible 
input that can be entered, including any input that might lead to an inconsistent or 
unexpected program state. Attackers try to explore every accessible interface to a 
piece of software and look specifically for any assumptions the developer made. For 
an attacker, any opportunity to provide unexpected input is gold because this input 
often has a subtle impact on later processing that the developers didn't anticipate. In 
general, if you can make an unanticipated change in software's runtime properties, 
you can often find a way to leverage it to have more influence on the program. 
Interfaces 
Interfaces are the mechanisms by which software components communicate with 
each other and the outside world. Many vulnerabilities are caused by developers not 
fully appreciating the security properties of these interfaces and consequently 
assuming that only trusted peers can use them. If a program component is accessible 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
36 
via the network or through various mechanisms on the local machine, attackers might 
be able to connect to that component directly and enter malicious input. If that 
component is written so that it assumes its peer is trustworthy, the application is 
likely to mishandle the input in an exploitable manner. 
What makes this vulnerability even more serious is that developers often incorrectly 
estimate the difficulty an attacker has in reaching an interface, so they place trust in 
the interface that isn't warranted. For example, developers might expect a high 
degree of safety because they used a proprietary and complex network protocol with 
custom encryption. They might incorrectly assume that attackers won't be likely to 
construct their own clients and encryption layers and then manipulate the protocol in 
unexpected ways. Unfortunately, this assumption is particularly unsound, as many 
attackers find a singular joy in reverse engineering a proprietary protocol. 
To summarize, developers might misplace trust in an interface for the following 
reasons: 
They choose a method of exposing the interface that doesn't provide enough 
protection from external attackers. 
They choose a reliable method of exposing the interface, typically a service of 
the OS, but they use or configure it incorrectly. The attacker might also exploit 
a vulnerability in the base platform to gain unexpected control over that 
interface. 
They assume that an interface is too difficult for an attacker to access, which 
is usually a dangerous bet. 
Environmental Attacks 
Software systems don't run in a vacuum. They run as one or more programs 
supported by a larger computing environment, which typically includes components 
such as operating systems, hardware architectures, networks, file systems, 
databases, and users. 
Although many software vulnerabilities result from processing malicious data, some 
software flaws occur when an attacker manipulates the software's underlying 
environment. These flaws can be thought of as vulnerabilities caused by assumptions 
made about the underlying environment in which the software is running. Each type 
of supporting technology a software system might rely on has many best practices 
and nuances, and if an application developer doesn't fully understand the potential 
security issues of each technology, making a mistake that creates a security exposure 
can be all too easy. 
The classic example of this problem is a type of race condition you see often in UNIX 
software, called a /tmp race (pronounced "temp race"). It occurs when a program 
needs to make use of a temporary file, and it creates this file in a public directory on 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
37 
the system, located in /tmp or /var/tmp. If the program hasn't been written carefully, 
an attacker can anticipate the program's moves and set up a trap for it in the public 
directory. If the attacker creates a symbolic link in the right place and at the right time, 
the program can be tricked into creating its temporary file somewhere else on the 
system with a different name. This usually leads to an exploitable condition if the 
vulnerable program is running with root (administrator) privileges. 
In this situation, the vulnerability wasn't triggered through data the attacker supplied 
to the program. Instead, it was an attack against the program's runtime environment, 
which caused the program's interaction with the OS to proceed in an unexpected and 
undesired fashion. 
Exceptional Conditions 
Vulnerabilities related to handling exceptional conditions are intertwined with data 
and environmental vulnerabilities. Basically, an exceptional condition occurs when 
an attacker can cause an unexpected change in a program's normal control flow via 
external measures. This behavior can entail an asynchronous interruption of the 
program, such as the delivery of a signal. It might also involve consuming global 
system resources to deliberately induce a failure condition at a particular location in 
the program. 
For example, a UNIX system sends a SIGPIPE signal if a process attempts to write to 
a closed network connection or pipe; the default behavior on receipt of this signal is 
to terminate the process. An attacker might cause a vulnerable program to write to a 
pipe at an opportune moment, and then close the pipe before the application can 
perform the write operation successfully. This would result in a SIGPIPE signal that 
could cause the application to abort and perhaps leave the overall system in an 
unstable state. For a more concrete example, the Network File System (NFS) status 
daemon of some Linux distributions was vulnerable to crashing caused by closing a 
connection at the correct time. Exploiting this vulnerability created a disruption in 
NFS functionality that persisted until an administrator can intervene and reset the 
daemon. 
6.1.6 Summary 
You've covered a lot of ground in this short chapter and might be left with a number 
of questions. Don't worry; subsequent chapters delve into more detail and provide 
answers as you progress. For now, it's important that you have a good understanding 
of what can go wrong in computer software and understand the terminology used in 
discussing these issues. You should also have developed an appreciation of the need 
for security auditing of applications and become familiar with different aspects of the 
process. In later chapters, you build on this foundation as you learn how to use this 
audit process to identify vulnerabilities in the applications you review. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
38 
6.2 Chapter 2.  Design Review 
"Sure. Each one of us is wearing an unlicensed nuclear accelerator on our back. No 
problem." 
Bill Murray as Dr. Peter Venkman, Ghostbusters (1984) 
6.2.1 Introduction 
Computer security people tend to fall into one of two camps on design review. People 
from a formal development background are usually receptive to the design review 
process. This is only natural, as it maps closely to most formal software development 
methodologies. The design review process can also seem to be less trouble than 
reviewing a large application code base manually. 
In the other camp are code auditors who delight in finding the most obscure and 
complex vulnerabilities. This crowd tends to look at design review as an ivory-tower 
construct that just gets in the way of the real work. Design review's formalized 
process and focus on documentation come across as a barrier to digging into the 
code. 
The truth is that design review falls somewhere between the views of these two 
camps, and it has value for both. Design review is a useful tool for identifying 
vulnerabilities in application architecture and prioritizing components for 
implementation review. It doesn't replace implementation review, however; it's just 
a component of the complete review process. It makes identifying design flaws a lot 
easier and provides a more thorough analysis of the security of a software design. In 
this capacity, it can make the entire review process more effective and ensure the 
best return for the time you invest. 
This chapter gives you some background on the elements of software design and 
design vulnerabilities, and introduces a review process to help you identify security 
concerns in a software design. 
6.2.2 Software Design Fundamentals 
Before you tackle the subject of design review, you need to review some 
fundamentals of software design. Many of these concepts tie in closely with the 
security considerations addressed later in the chapter, particularly in the discussion of 
threat modeling. The following sections introduce several concepts that help establish 
an application's functional boundaries with respect to security. 
Algorithms 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
39 
Software engineering can be summed up as the process of developing and 
implementing algorithms. From a design perspective, this process focuses on 
developing key program algorithms and data structures as well as specifying problem 
domain logic. To understand the security requirements and vulnerability potential of 
a system design, you must first understand the core algorithms that comprise a 
system. 
Problem Domain Logic 
Problem domain logic (or business logic) provides rules that a program follows as 
it processes data. A design for a software system must include rules and processes for 
the main tasks the software carries out. One major component of software design is 
the security expectations associated with the system's users and resources. For 
example, consider banking software with the following rules: 
A person can transfer money from his or her main account to any valid 
account. 
A person can transfer money from his or her money market account to any 
valid account. 
A person can transfer money from his or her money market account only once 
a month. 
If a person goes below a zero balance in his or her main account, money is 
automatically transferred from his or her money market account to cover the 
balance, if that money is available. 
This example is simple, but you can see that bank customers might be able to get 
around the once-a-month transfer restriction on money market accounts. They could 
intentionally drain their main account below zero to "free" money from their monkey 
market accounts. Therefore, the design for this system has an oversight that bank 
customers could potentially exploit. 
Key Algorithms 
Often programs have performance requirements that dictate the choice of algorithms 
and data structures used to manage key pieces of data. Sometimes it's possible to 
evaluate these algorithm choices from a design perspective and predict security 
vulnerabilities that might affect the system. 
For example, you know that a program stores an incoming series of records in a 
sorted linked list that supports a basic sequential search. Based on this knowledge, 
you can foresee that a specially crafted huge list of records could cause the program 
to spend considerable time searching through the linked list. Repeated focused 
attacks on a key algorithm such as this one could easily lead to temporary or even 
permanent disruption of a server's functioning. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
40 
Abstraction and Decomposition 
Every text on software design inevitably covers two essential concepts: abstraction 
and decomposition. You are probably familiar with these concepts already, but if not, 
the following paragraphs give you a brief overview. 
Abstraction is a method for reducing the complexity of a system to make it more 
manageable. To do this, you isolate only the most important elements and remove 
unnecessary details. Abstractions are an essential part of how people perceive the 
world around them. They explain why you can see a symbol such as 
and associate 
it with a smiling face. Abstractions allow you to generalize a concept, such as a face, 
and group-related concepts, such as smiling faces and frowning faces. 
In software design, abstractions are how you model the processes an application will 
perform. They enable you to establish hierarchies of related systems, concepts, and 
processesisolating the problem domain logic and key algorithms. In effect, the design 
process is just a method of building a set of abstractions that you can develop into an 
implementation. This process becomes particularly important when a piece of 
software must address the concerns of a range of users, or its implementation must 
be distributed across a team of developers. 
Decomposition (or factoring) is the process of defining the generalizations and 
classifications that compose an abstraction. Decomposition can run in two different 
directions. Top-down decomposition, known as specialization, is the process of 
breaking a larger system into smaller, more manageable parts. Bottom-up 
decomposition, called generalization, involves identifying the similarities in a 
number of components and developing a higher-level abstraction that applies to all of 
them. 
The basic elements of structural software decomposition can vary from language to 
language. The standard top-down progression is application, module, class, and 
function (or method). Some languages might not support every distinction in this list 
(for example, C doesn't have language support for classes); other languages add 
more distinctions or use slightly different terminology. The differences aren't that 
important for your purposes, but to keep things simple, this discussion generally 
sticks to modules and functions. 
Trust Relationships 
In Chapter 1(? [????.]), "Software Vulnerability Fundamentals," the concept of trust 
and how it affects system security was introduced. This chapter expands on that 
concept to state that every communication between multiple parties must have some 
degree of trust associated with it. This is referred to as a trust relationship. For 
simple communications, both parties can assume complete trustthat is, each 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
41 
communicating party allows other parties participating in the communication 
complete access to its exposed functionality. For security purposes, however, you're 
more concerned with situations in which communicating parties should restrict their 
trust of one another. This means parties can access only a limited subset of each 
other's functionality. The limitations imposed on each party in a communication 
define a trust boundary between them. A trust boundary distinguishes between 
regions of shared trust, known as trust domains. (Don't worry if you're a bit 
confused by these concepts; some examples are provided in the next section.) 
A software design needs to account for a system's trust domains, boundaries, and 