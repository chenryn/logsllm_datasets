### 连接 Source 组件与 Sink 组件
- 将 `source` 组件的输出连接到 `sink` 组件的输入，并点击“保存”按钮。

### 通过配置文件配置 Dataflow
- 在安装目录下的 `./config/group/default/dataflow/` 路径创建一个 dataflow 配置文件。示例如下：
  ```json
  {
    "name": "my_first_dataflow",
    "sources": [
      {
        "name": "file_source",
        "type": "file",
        "conf": {
          "include": ["/tmp/my_file.log"]
        }
      }
    ],
    "pipelines": []
  }
  ```
- 重新启动 DataScale 服务以应用更改。

**提示：**
- 关于 dataflow 配置文件的具体格式，请参阅文档《Dataflow 配置》。
- 欲了解更多关于 `source` 和 `sink` 组件的类型及配置参数信息，请查阅相关文档《Source 组件》和《Sink 组件》。

### 导入数据
- 向指定的数据源文件写入数据：
  ```bash
  $ echo "this is event 1" >> /tmp/my_file.log
  $ echo "this is event 2" >> /tmp/my_file.log
  $ echo "this is event 3" >> /tmp/my_file.log
  ```

### 查询数据
- 使用炎凰数据平台，在名为 `my_event_set` 的数据集中查询已采集的数据。

### Nginx 访问日志采集示例 (版本：2.13.0-beta)
#### 简介
Nginx 是一种流行的 HTTP 服务器和反向代理服务器，能够高效地提供静态内容，并作为单一访问入口为多个后端服务器或其他应用程序（如缓存和负载均衡）提供支持。Nginx 提供了开源版以及更全面功能的商业版 Nginx Plus。

#### 目标
- 配置 dataflow 从文件中读取 Nginx 访问日志并导入至炎凰数据平台。
- 利用平台内置的 `nginx.access_log` 数据源类型自动解析日志字段。

**注意事项：**
- 查看有关炎凰数据平台内置数据源类型的更多信息，请参考官方文档。

#### 步骤
1. **创建数据集**
   - 在炎凰数据平台上为 Nginx 访问日志创建一个新的数据集（本例使用 `my_nginx_access_log`）。
   - 如果你正在使用鸿鹄，则需要将数据集 `my_nginx_access_log` 添加到 Vector input 允许的数据集列表中。
   
2. **配置 Dataflow**
   - **通过 Web UI:**
     - 在默认组中创建名为 `my_nginx_access_log` 的 dataflow。
     - 添加 source (`nginx_access_log_source`)、pipeline (`nginx_access_log_pipeline`) 和 sink (`nginx_access_log_sink`) 组件。
     - 将 source 组件的输出连接至 pipeline 组件的输入，再将 pipeline 的输出连接到 sink 组件，并保存设置。
   - **通过配置文件:**
     - 在 `./config/group/default/dataflow/` 创建如下所示的 JSON 格式配置文件：
       ```json
       {
         "name": "my_nginx_access_log",
         "sources": [{
           "name": "nginx_access_log_source",
           "type": "file",
           "conf": {
             "include": ["/usr/local/var/log/nginx/access.log"]
           }
         }],
         "pipelines": [{
           "name": "nginx_access_log_pipeline",
           "inputs": ["nginx_access_log_source"],
           "conf": {
             "transforms": [{
               "type": "datatype",
               "conf": {
                 "datatype": "nginx.access_log"
               }
             }]
           }
         }],
         "sinks": [{
           "name": "nginx_access_log_sink"
         }]
       }
       ```
     - 重启 DataScale 服务使更改生效。

**备注：**
- 示例中使用的 Nginx 日志路径为 `/usr/local/var/log/nginx/access.log`，请根据实际情况调整。
- `nginx.access_log` 类型适用于默认格式的 Nginx 访问日志。
- 更多关于 Pipeline 组件及其转换方法的信息，请参考《Pipeline 组件》文档。

3. **验证数据**
   - 确保 `/usr/local/var/log/nginx/access.log` 文件内已有日志条目；如果为空，可通过访问 Nginx 服务生成新的访问记录。
   - 在炎凰数据平台上的 `my_nginx_access_log` 数据集中检查收集的日志事件及其自动生成的字段。
   - 注意每个事件的 _datatype 字段指定了其数据源类型。

### 基础指南 (版本：2.13.0-beta)
#### 配置 DataScale 服务
- DataScale Web UI 提供了一些常用的服务配置选项。对于其他配置项，请查看位于安装目录下的 `config/datascale.toml` 文件。
- 修改配置文件后需重启 DataScale 服务才能使变更生效。

#### Service 设置
- `service.address` 定义了 DataScale 服务监听地址，默认为 `0.0.0.0:7881`。
- `service.mode` 可设为 `worker` 或 `manager`。当设为 worker 时，可以进一步配置是否由某个 manager 管理。
  - 无 manager 管理的情况下，保持 `service.worker.manager_address` 为空。
  - 接受 manager 管理时，需指定 manager 地址及所属 group 名称。

#### Backend 配置
- `backend.type` 控制 DataScale 如何向炎凰数据平台发送事件。对于 YHP 商业版应选择 `yh_kafka` 并填写 Kafka 服务详情；而对于社区版鸿鹄则应选用 `yh_vector` 并提供 Vector 服务地址。

#### Metastore 选项
- `metastore.type` 支持 `file` 或 `sqlite` 两种方式来存储配置信息。目前 SQLite 存储仍处于 Beta 测试阶段。

#### 环境变量
- 可以通过环境变量覆盖 `datascale.toml` 中的部分配置项。变量命名遵循 `DATASCALE_` 加上大写的参数路径形式，如 `DATASCALE_SERVICE_ADDRESS` 对应 `service.address` 参数。

### 配置 Group
- 除了通过 Web UI 和 RESTful API 外，还可以直接编辑配置文件来管理 group。
- 新建或重命名 group 时记得同步更新相应的 meta.json 文件。