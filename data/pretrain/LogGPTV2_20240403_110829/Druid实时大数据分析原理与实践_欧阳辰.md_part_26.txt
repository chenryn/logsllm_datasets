By查询模式特殊不同于其他类型的查询，需要根据维度分组聚合，所以要单独使用
QueryRunner提供给线程池去处理，最后合并结果。
的查询并发执行，然后合并。
务）的请求。
类中完成。
到不同节点，然后汇总请求，并且将结果集缓存起来。Broker中的查询缓存也在这个
，调用底层的Select查询引擎，位于SelectQueryRunnerFactory的内
图8-8QueryRunner类图
，它会将对不同Segment查询的
Druid实时大数据分析原理与实践
ByP
---
## Page 235
public QueryRunner> mergeRunners(ExecutorService
QueryRunnerFactory。
TimeBoundary查询为例，代码见druid-processing/io/druid/query/timeboundary/TimeBoundary-
ners方法中根据传入参数exec和querRunners构造ChainedExecutionQueryRunner。这里以
ChainedExecutionQueryRunner完成并发执行。Druid通用查询模式如图8-9所示。
等待查询完成，最后合并结果。前面提到除了Group By查询以外，其他所有的查询均使用
题，采用Scatter/Gather模式，将多个Segment的查询提交给线程池去并发执行，然后阻塞地
第8章核心源代码探析
return new ChainedExecutionQueryRunner<>(queryExecutor
ChainedExecutionQueryRunner
queryExecutor,Iterable>> queryRunners)
分发处理的逻辑是在QueryRunnerFactory的mergeRunners 方法中执行的。在mergeRun-
Druid的设计目标是面向用户的平台级产品，为了提升查询性能，以及解决高并发性问
·SpecicSegmentQueryRunner，该类的功能是将执行QueryRunner的当前线程的名字改
·ReferenceCountingSegmentQueryRunner，添加对Segment的引用计数逻辑，防止正在
·GroupByQUeryRunner，调用底层的Group By查询引擎，位于GroupByQueryRunner-
使用的 Segment被删除。
Factory的内部类中。
成“queryType_dataSource_interval”的形式。
分发处理
W
Segment查询
Segment查询
图8-9
Druid通用查询模式
queryRunners);
queryWatcher,
合前处理
合井结果集
合并后处理
211
---
## Page 236
9
323
3
6
8
亿
22
68元
5
3
try{
queryWatcher.registerQuery(query,
return new BaseSequence>(
final int priority=BaseQuery.getContextPriority(query,0);
212
return new MergeIterable<>(
final Number timeout=query.getContextValue(QueryContextKeys.TIMEOuT,(Number)null);
new BaseSequence.IteratorMaker>()
futures.get(timeout.longValue(),TimeUnit.MILLISECONDS)).iterator();
futures.get(）:
timeout == null？
ordering.nullsFirst(),
new Function,
publicIteratormake()
下面我们根据源码分析ChainedExecutionQueryRunner的处理流程。
public ListenableFuture> apply(final
@Override
ListenableFuture>>futures =Futures.allAsList(Lists.newArrayList
return exec.submit(new AbstractPrioritizedCallable>(priority)
1/省略部分代
publicIterablecall()throws Exception
(Iterables.transform(queryables
1/省略部分不重要的代码
try
return retVal;
ListretVal=Sequences.tolist(result,-Lists.newArrayList());
Sequenceresult =input.run(query,
futures):
ListenableFuture>>()
responseContext);
QueryRunnerinput)
Druid实时大数据分析原理与实践
---
## Page 237
直接选代 Sequence，并将结果在同一IncrementalIndex累积聚合，如图 8-11所示。
用，从理论上讲，根据Group by查询的特性，将 Sequence 转换成List是多余的。优化方案是
基数特别大以及维度的组合很多，那么转换成的List会占用很多的内存。为了优化内存的使
行聚合。像Timeseries和TopN查询，List的大小是可预测的，内存的使用是可控的；但对
的Sequence物化为List，然后交给QueyToolChest，在合并结果集时采用IncrementalIndex进
合。这个过程听起来比较抽象，下面我们通过一个实例来解释一下。假设一个DataSource有
构上非常类似，不同之处是Group By查询，需要汇总每个Segment 的列表，然后在内存中聚
的 get 方法。然后使用MergeIterable将get方法返回的List 排序合并。
层的查询引擎并将结果转换为List。
List，这一行代码很重要，Sequence是延迟加载，调用 Sequences的toList方法会真正调用底
执行QueryRunner的run方法，然后调用Sequence的toList方法将返回的 Sequence转换为
其构造函数的参数 priority表示执行的优先级，它可以在查询上下文中设置。在call方法中
败，就会返回失败。同时它会监听cancel，一旦调用它的cancel方法，就会取消所有传入的
待。同时allAsList方法要求所有的ListenableFuture 都执行成功才返回结果，一旦任何一个失
用它的 get方法时，会检查那些传入的ListenableFuture是否都完成，完成则返回，没有则等
人的所有ListenableFuture，如果其执行完成，则会将结果按照顺序放人List相应的位置，调
后就触发告知监听者。Futures 的allAsList会创建一个CombinedFuture并返回，它会监听传
看List中的每个Future是否完成，而ListenableFuture通过添加Listener的方式，一旦完成以
Google的Guava提供的ListenableFuture，因为使用JDK的Future，我们需要不断地轮询查
作为List 添加到Futures的allAsList方法中。这里并没有使用JDK自带的Future，而是采用
第8章核心源代码探析
于GroupBy查询，List的大小是由参与分组的维度组合以及维度的基数决定的，如果维度的
上。维度A的基数是4，分别是A1、A2、A3和A4。其查询示意图如图8-10所示。
S
ListenableFuture
GroupByParallelQueryRunner
从第17行开始，构建一个AbstractPrioritizedCallable的匿名类，这是带有优先级的Callable
第9行，利用Iterables的transform方法，将QueryRunner转换为一组ListenableFuture，
使用ChainedExecutionQueryRunner 处理Group By查询，它会将每个 Segment查询返回
GroupByParallelQueryRunner的功能以及实现和ChainedExecutionQueryRunner在整体架
第34行，调用futures的get方法，如果在查询上下文中设置了超时，则调用带有timeout
213
---
## Page 238
6
8
9
2
1/省略部分代码
final int priority = BaseQuery.getContextPriority(query, 0);
final boolean bySegment = BaseQuery.getContextBySegment(query, false);
final Pair> indexAccumulatorPair=
final GroupByQuery query = (GroupByQuery) queryParam;
214
bufferPool);
configSupplier.get(),
query.
CreateBySegmentAccumulatorPair();
GroupByQueryHelper.CreateIndexAccumulatorPair(
下面分析GroupByParallelQueryRunner的关键代码实现。
Accumulator> bySegmentAccumulatorPair = GroupByQueryHelper.
Segmentl查询
图8-10
图8-11
GroupBy查询示意图
优化方案示意图
Increment Index
Segment2查询
P
Sequence
Seguent查询
Druid实时大数据分析原理与实践
---
## Page 239
0
6
0
3
11
final List aggs = Lists.transform(
final long granTimeStart = gran.iterable(timeStart, timeStart + 1).iterator().next();
final QueryGranularity gran = query.getGranularity();
抽取到 GroupByQueryHelper类中实现。
中的Row添加到IncrementalIndex中聚合。
是容器对象IncrmentalIndex，rhs是Accumulator，是一个匿名类，它实现的逻辑是将Sequence
则不进行聚合，直接将明细数据返回。两者的不同之处是收集的容器和Accumulator。
publicVoid call()throws Exception
第8章
query.getAggregatorSpecs(),
}catch（...）
try
第2行代码是indexAccumulatorPair的构建，这块代码是GroupBy查询的核心逻辑之一，
第17行代码是普通非Debug模式调用的，它使用Pair对象保存容器和Accumulator，lhs
}else{
if （bySegment）[
@Override
第15行代码是根据bySegment 判断，bySegment标志用于调试，如果bySegment为true，
与ChainedExecutionQueryRunner的框架代码一致，不同的是Callable中的实现逻辑。
return null;
return input.getCombiningFactory();
input.run(queryParam,responseContext).accumulate(indexAccumulatorPair.lhs,
input.run(queryParam,responseContext).accumulate(bySegmentAccumulatorPair.lhs,
核心源代码探析
bySegmentAccumulatorPair.rhs);
indexAccumulatorPair.rhs);
215
---
## Page 240
4
3
3
3
6
8
15
13
}else{
if (query.getContextValue("useoffheap",false)){
final boolean sortResults = query.getContextValue(CTX_KEY_SORT_RESULTS, true);
final IncrementalIndex index;
final List dimensions = Lists.transform(
216
index = new OnheapIncrementalIndex(
//since incoming truncated timestamps may precede timeStart
index=new OffheapIncrementalIndex(
new Function()
query.getDimensions(),
aggs.toArray(new AggregatorFactory[aggs.size()]),
granTimeStart,
// use granularity truncated min timestamp
Math.min(query,getContextValue(CTX_KEY_MAX_RESULTS, config.getMaxResults()), config.
aggs.toArray(new AggregatorFactory[aggs,size()]),
granTimeStart;
//use granularitytruncatedmintimestamp
sortResults,
true,
false,
//since incoming truncated timestamps may precede timeStart
sortResults,
true,
gran,
public String apply(DimensionSpec input)
@Override
lse,
getMaxResults()),bufferPool);
return input.getOutputName();
Druid实时大数据分析原理与实践
---
## Page 241
3
2
0
5
2
9
5
S
53
5
5
6
计total_usage和 data_transfer，查询的 JSON表达如下。
官方文档中的Group By查询一节，这个查询是按照country和device两个维度进行分组，统
Accumulatoraccumulator =new Accumulator()
第8章
return newPair<>(index,accumulator);
publicIncrementalIndex accumulate(IncrementalIndex accumulated,Tin)
@verride
Math.min(query.getContextValue(CTX_KEY_MAX_RESULTS,config.getMaxResults()),config.
为了便于理解源码，采用场景对照法，这里设计了一个GroupBy的查询场景，源自Druid
return accumulated;
"granularity":“"hour",
"queryType":“
11/省略部分代码
if（in instanceof MapBasedRow）{
"aggregations":[
"dimensions":["country",
"dataSource":
}catch（IndexSizeExceededExceptione)
try{
throw new ISE(e.getMessage());
getMaxResults())
核心源代码探析
MapBasedRowrow=(MapBasedRow)in;
accumulated.add(
new MapBasedInputRowt
row.getEvent()
dimensions,
row.getTimestamp()
"sample_datasource"
"groupBy"
"device"],
217
---
## Page 242
可以在查询上下文中进行配置，然后和系统配置相比取最小值。
upBy.maxResults设置，默认值为500000。这是系统的配置加载以后不能改变，为了灵活处理，
结果集的容量大于该值，则会抛出IndexSizeExceededException异常。该参数由druid.querygro-
IncrementalIndex的堆外内存的使用。
true，减少JVMGC 带来的性能影响。同时要增大MaxDirectMemorySize，一般场景下：
认大小为1GB。如果GroupBy查询的结果集很大，建议在查询上下文中设置useOffHeap为
类，每次分配固定大小的堆外内存，其大小由参数druid.processing.buffer.sizeBytes设置，
堆外内存存储Aggregator，采用全局的堆外内存池OfheapBufferPool，它是StupidPool 的子
从名字上看就知道了，两者的不同之处是堆外内存的使用，OfheapIncrementallndex会使用
维度是country和device，也就是说，按照country和devcie组合进行分组查询。
要是为了兼顾抽取函数，如果没有使用抽取函数，outputName则使用维度名。上述查询中的
工厂类，用于创建相应的聚合器。上述查询中有两个聚合器，分别是longSum和doubleSum。
mestamp，则会抛出异常。
01T00:00:00.000
218
第38行中的MaxResults参数是指Incrementallndex的最大容量，如果Group By查询返回
第28~53行是创建IncrementallIndex，首先会根据查询上下文中的useOfHeap来判断，如
第15~25行是获取用于分组的维度。值得注意的是，第22行使用的是outputName，
第3行是获取minTimestamp，如果添加到Incrementallndex的event的时间截小于minTi-
这个公式是为了保障为每个处理线程能配置 sizeBytes大小的内存。但并不包括Ofheap-
MaxDirectMemorySize>(processing.numThreads+1) x processing.buffer.sizeBytes
第2行是获取查询的时间区间的最小值，上述查询的时间区间的最小值是2012-01-
第1行是从query中获取QueryGranularity，上述查询的Granularity是day。
接下来对照源码来阐述上述查询的执行过程。
"intervals":["2012-01-01T00:00:00.000/2012-01-03T00:00:00.000"]
[“type”:"doubleSum","name”:“data_transfer",“fieldName":“data_transfer"}
Druid实时大数据分析原理与实践
主
---
## Page 243
节点或者实时节点（或是索引服务中的节点），然后合并结果。其执行流程如图8-12所示。
Segment的查询，如果Segment在Broker中已有缓存则调用缓存的，没有则分发请求给历史
6.
QueryRunner返回Sequence并发添加到Incrementallndex中。
人到Incrementallndex中。Incrementallndex是线程安全的，在这个场景下会有多个线程执行
dex，第二个参数是row，代表每一条记录。accumulate方法的逻辑很简单，就是将Row加
的选代器回调Accumulator的accumulate方法。它在类中传入的第一个参数是IncrementalIn-
第8章
CachingClusteredClient
合并Sequence
CachingClusteredClient是Broker中最核心的一个类，
第55~77行是创建Accumulator，
核心源代码探析
图8-12
反序列化成Sequance
CachingClusteredClient 执行流程图
nt井发地
在调用Sequence的accumulate方法时，选代遍历内部
筛选出缓存没有命中的Segment集合
到Server->List