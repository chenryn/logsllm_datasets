title:In Production Performance Testing of SDN Control Plane for Telecom
Operators
author:Catello Di Martino and
Ugo Giordano and
Nishok Mohanasamy and
Stefano Russo and
Marina Thottan
2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
In Production Performance Testing of SDN Control
Plane for Telecom Operators
Catello Di Martino*, Ugo Giordano**, Nishok N. Mohanasamy*, Stefano Russo**, Marina Thottan*
Email: {lelio.di martino, nishok.narashima mohanasamy, marina.thottan}@nokia-bell-labs.com
* Nokia Bell Labs, ** University of Naples Federico II
{ugo.giordano, stefano.russo}@unina.it
Abstract—One of the biggest emerging challenges for telco
operators is to dynamically create new services while maintaining
the network at its optimal performance/revenue break point. To
this end, operators are moving to a much leaner cloud-based
Software Deﬁned Network (SDN) infrastructure to achieve a truly
programmable network fabric. While cloud services can be pro-
visioned in seconds and tested in-production, service provisioning
in SDNs still lasts many weeks and requires substantial manual
effort. A large part of this service creation time can be attributed
to testing and tuning the control plane.
In this paper we present SCP-CLUB (SDN Control Plane
CLoUd-based Benchmarking), a platform for in-production per-
formance testing of telco operator SDNs, offering a level of
automation as available in deploying cloud services. Telco cloud
SDN performance testing with SCP-CLUB focuses on the analysis
of how design choices in the cloud and SDN control planes
inﬂuence SLA metrics like throughput and latency. We describe
the SCP-CLUB architecture and its performance testing support
capabilities. Experiments are performed on an SDN telco cloud
built to demonstrate SCP-CLUB under production load condi-
tions.
I. INTRODUCTION
In Today’s telecommunications (telco) world, operators are
constantly under pressure for disaggregating, realigning, and re-
integrating network functions, systems and elements in order to
meet evolving market demands, changing network conditions
and unpredictable trafﬁc patterns over short innovation cycles.
The telco industry is keen to adopt emerging technologies such
as Software Deﬁned Network (SDN) and Network Function
Virtualization (NFV) to achieve a truly programmable net-
work fabric. For instance, Telco Cloud is meant to provide
a dedicated cloud computing solution for network operators,
shifting network functions from dedicated legacy hardware
to virtualized software components, deployable on general-
purpose hardware, in order to deliver agile, elastic and highly
personalized services.
Network softwarization and cloudiﬁcation promise to support
new services and unique ﬂexibility. They also present signiﬁcant
challenges. Cloud services can be instantiated in a matter of
seconds, yet the capability to rapidly schedule, test and allocate
physical network resources for operators is more than 100
times slower, i.e., days if not weeks. Most of the deployment
time is spent in (near) manual tuning the many “knobs” in
the control plane, and running tests to assess the ability to
meet desired levels of service (e.g., latency and throughput).
To further complicate things in the near future, operators will
have to address the need for massive increases in control plane
capacity to handle Machine-to-Machine (M2M) streams, grow-
ing as much as 135-fold by 2020, and driving future network
needs. For instance, Internet of things (IoT) devices generate
a substantially higher volume of signaling trafﬁc related to
exchanged data, increasing the load over the control plane. It
has been estimated that a typical IoT device might need up to
˜2,500 transactions to consume 1 MB of data [1].
To meet the rising demand for new network services under
declining revenue per bit, operators will have to adopt leaner ap-
proaches to test and run networks closer to the optimal revenue
point [1], [2]. While devices conﬁguration can be simpliﬁed
by proper network abstractions [3], [4], and softwarization
platforms [5], [6], no clear solutions are available to automate
the performance testing of SDNs.
In this paper, we present (SCP-CLUB) (SDN Control Plane
CLoUd-based Benchmarking), an in-production performance
testing framework for SDN networks in telco cloud deploy-
ments. It supports the performance testing of network conﬁg-
urations - using idle cycles of the telco cloud infrastructure -
offering a level of automation as the one available in deploying
NFV functions. To the best of our knowledge, there is no
current approach or tool providing advanced automation in the
performance analysis of the SDN/NFV control plane.
The SCP-CLUB performance testing focuses on the choices
in the design of telco cloud and SDN control plane, speciﬁ-
cally on their inﬂuence on latency and throughput at the user
level. We argue that data plane performance can be achieved
through different approaches, including network slicing [7],
Quality of Service, and network capacity monitoring through
low-level metering application programming interfaces (APIs)
[8], while no similar approach is used to guarantee control
plane performance, that requires careful system-level tuning. As
demonstrated in the paper, control plane-level bottlenecks can
lead to major system outages, network device unavailability and
under-utilization of network capacity. The principles on which
SCP-CLUB builds are the following.
Performance testing of the control plane should be carried
out during production hours, using idle cycles of the telco
cloud, in order to discover conﬁguration/load-speciﬁc bottle-
necks. SCP-CLUB is able to create testing resources on-demand
(e.g., VM, virtual networks and data stores), replicating the
production conﬁguration into a controlled environment (referred
to as test plane) without disrupting the production workload.
SCP-CLUB includes a cloud orchestrator to automate the life
cycle of the testing resources over preemptive cloud nodes (e.g.,
any idle nodes in the continuum from the edge cloud to the core
network cloud), enabling parallel experiments and out of band
data collection while running the tests.
Tests should run in production/representative workload
conditions. Differently from other benchmarking work and
2158-3927/18/$31.00 Â©2018 IEEE
DOI 10.1109/DSN.2018.00071
642
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
tools [9], [10], performance testing in SCP-CLUB is carried
out using APIs normally visible at the NFV/Application layer,
without assumptions on the internals of the underlying SDN
control platform. This allows SCP-CLUB to feed the system un-
der test with the requests coming from the production network.
This is achieved by mirroring part of the production control
plane workload towards the test plane created on demand
to run the testing tasks. The mirroring is managed by the
Load Generator, one of the SCP-CLUB central components.
Production workload can be augmented with synthetic requests
generated by the Load Generator in order to test the control
plane against speciﬁc workload proﬁles (e.g., overload condi-
tions). Data collected from experiments can be used to extract
actionable intelligence, for instance, to decide whether to scale
up or down the VMs managing the SDN control network.
We demonstrate SCP-CLUB in the context of testing the
Open Network Operating System (ONOS) [5] and intent-
based networking [11]. We built a production-ready 3,500
cores telco cloud datacenter based on Nokia Airframe technol-
ogy, VMWare ESXi hypervisor and multi-vendor SDN-enabled
switches (Centec, EdgeCore, Nokia). We show the effectiveness
of SCP-CLUB by measuring on-demand the performance of
ONOS under a variety of parameters (workload, virtual machine
size, topology size and size of the ONOS clusters), and by
characterizing the overhead of the testing in terms of resources
and dollar costs of the extra resources allocated. Finally, we
show how we leveraged the result provided by SCP-CLUB
to solve one of the performance bottlenecks identiﬁed by the
testing.
II. SDN AND INTENT-BASED NETWORKING
SDN has emerged as a paradigm capable of providing new
ways to design, build and operate networks. Its key concept is
the separation of the network control logic (the control plane)
from the underlying equipment (i.e., switches - referred to as
network devices) that forward and transport the trafﬁc over
the data plane. The logical entity hosting SDN services in the
control plane is typically referred to as SDN controller. The
controller is below the application layer, and it controls the
network devices in the data plane. The separation of the planes
is implemented through well-deﬁned APIs: requests come
from NFV and network applications through the Northbound
Interface (NBI), and the controller satisﬁes them managing
network resources through the Southbound Interface (SBI).
Among the many controllers available in the SDN community
[12], [13], two major platforms are currently getting major
attention from academia and industry: OpenDayLight (ODL)
[6] and ONOS [5]. Both are modular, open SDN-controller
platforms to customize and automate SDN. At a high level,
they share many architectural concepts.
They provide a logically centralized view of the network
to the applications above using clustering and strict/eventual
consistency state sharing [5], [6]. Each cluster instance is
in charge of a portion of the network as master or stand-
by controller. The state information local
to a subnetwork
is disseminated across the cluster through the so-called east-
westbound interfaces.
A. Network Intents
The most popular protocol
to program SDN switches is
OpenFlow [8]. It uses the match-action abstraction to program
Northbound interface
Request
application
Request 
executed
submitted
Request 
executed
Compiling
Failed
Installing
Recompiling
Withdrawn
Installed
Withdrawing
Southbound interface
Network element
Network element
Fig. 1. The Life cycle of an ONOS intent.
ﬂows by matching on speciﬁc tuples of data in the packet
headers and mapping them to speciﬁc egress ports to create
a path in the switch fabric. Complex network topologies with
different disjoint paths require the selection of the proper ports
in order to realize an end-to-end data ﬂow between two hosts.
To simplify the creation and management of end-to-end ﬂows
in complex networks, modern controllers like ODL and ONOS
provide a high-level abstraction called intent, i.e., a declaration
of a service request (e.g., the setup of connection from New
York to Los Angeles with speciﬁed quality of service for a given
amount of time) that are compiled by the network operating
system to identify the network resources to be programmed
and monitored for service (intent) provisioning.
The life cycle of an intent in the ONOS controller is shown
in the diagram of Figure 1. Applications submit intents through
the NBI. Each submitted request is put in a single batch, that
enters a compiling stage to identify a bidirectional shortest path
connecting the hosts (e.g., source and destination) of the intent.
The path is compiled in sets of OpenFlow rules that are installed
through the SBI on the devices belonging to the selected path.
For instance, in a linear topology with 10 switches, a path
between two hosts connected to the edge switches is compiled
in 10 sets of 2 ﬂow rules per device, 10 rules for each direction
of the bidirectional path.
Intents may be recompiled due to a topology change. An
intent provisioning fails if it cannot be compiled successfully,
for instance when there is no path between source and destina-
tion due to a network partition. Failed intents are attempted to
be re-compiled periodically. An installed intent is intentionally
withdrawn through a speciﬁc request received from the NBI,
releasing resources in the control and data planes. Failed intents
are also eventually withdrawn. The application is notiﬁed of the
performed operation through a call back at the NBI.
III. SCP-CLUB DESIGN
SCP-CLUB is a platform to automate the creation, execution
and analysis of performance tests for SDN control planes in
telco-cloud environments. Its architecture (Figure 2) comprises
ﬁve main modules: Campaign Manager, Experiment Manager,
Load Generator, Topology Manager, and Data Collector. They
interact with the cloud infrastructure-as-a-service (IaaS) plat-
form to conﬁgure and deploy on demand a distributed SDN in
the telco cloud space, with the following features.
A. Automated Life Cycle of Testing Resources
The Campaign Manager orchestrates a test campaign based
on a user-provided experiment proﬁle. This consists of a set
of options (enclosed in YAML tags) concerning deployment
643
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
Campaign Manager
2
Select Hardware
 available resource pool
 policies
Experiment Manager
 Load profiles
 Experiment profile
 Experiment Orchestrator
 System Configurator
3
Load Generator
intent  requests 
Production 
workload
Deploy
System Under Test
Configure VMs
 CPU, RAM, DISK, OS
 IP and Virtual Networks
 Virtual switches and VLANs
Select VM templates
 Compile YAML experiment 
description
 Specialize VM Templates
Campaign 
compiler
1
Prepare 
storage 
volumes
Real SDN Network Switches
Real SDN Ne
Data Collector
 Logs
 Metrics
 Packet dumps
 Diagnostics 
 …
4
Topology Manager
Network 
Emulator/
real topology
Topology 
configurator
Data 
Collection bus
Dedicated virtual 
network interface
Fig. 2. The SCP-CLUB framework.
(e.g., type and number of VMs) and workload. The Campaign
compiler compiles the speciﬁcation into commands for the
speciﬁc cloud infrastructure (e.g., VMWare ESXI over VCloud
and OpenStack),
to allocate, deploy and orchestrate VMs,
virtual networks and data stores. The workload options in
the proﬁle are compiled into commands for the Experiment
Manager to program the components needed to perform the
experiments (e.g., the Workload Manager).
SCP-CLUB experiments can be triggered in two modes: on-
predicate and on-demand. The on-predicate mode consists of
programming SCP-CLUB to start the analysis when speciﬁc
conditions are met. In the on-demand mode SCP-CLUB exper-
iments are triggered manually. In both cases, the user speciﬁes
the analysis parameters (i.e., what to analyze and what scenarios
to reproduce) in the experiment proﬁle.
After compiling the experiment proﬁle, the Campaign Man-
ager performs the following actions: i) it sets up the cloud,
by interacting with the underlying IaaS platform to conﬁgure
and deploy the testing resources VMs, i.e., the SDN controller
instances, the network emulator VM(s), the Data Collector, the
Topology Manager, the Load Generator, and the Experiment
Manager, using a pool of resources destined to the testing
activities (e.g., idle cloud nodes), ii) it creates and conﬁgures
the virtual switches in the cloud infrastructure (e.g., IPs and
VLANS and port groups over virtual switches) in order to
interconnect the created testing resources; iii) it sets up and
conﬁgures the various data stores to host and execute the user
experiment and to collect metrics of interest.
The Campaign Manager is implemented in a technology
independent manner, in the sense that its Representational State
Transfer (REST) APIs support operations across different IaaS
platforms (e.g., VM creation and virtual network deployment).
The actual mapping of the campaign operations to platform-
speciﬁc API calls is implemented by means of conﬁguration
ﬁles written in YAML.
Once the VMs are deployed, the control is given to the
Experiment Manager that executes the following three steps
to manage the life-cycle of the testing plane, i.e., of all the set
of the testing resources instantiated by the Campaign Manager.
Start-up phase – The Experiment Manager conﬁgures all
entities in the cloud infrastructure according to the user pro-
vided parameters. The conﬁgured services include: controller
instances, monitoring and data collection, topology manage-
ment, workload generation. The SDN controller instances are
conﬁgured starting from pre-loaded VM templates provided
by the Campaign Manager. A template consists in a generic
installation of a speciﬁc controller (e.g., ONOS or ODL) over
a lightweight Linux VM, that is customized by the Experiment
Manager depending on the user-provided proﬁle (e.g., VM size
and number of SDN instances) and deploy type (e.g., IP and
VLAN mirroring the production control plane network).
After specializing the templates and starting the VMs, the
Experiment Manager requests the Topology Manager to con-
ﬁgure the data-plane topology that will be used for running
the test. Finally, the Experiment Manager checks that the con-
troller instances correctly discovered the data-plane topology
and conﬁgures each controller instance to be a master of at
least one switch. The next steps of the Experiment Manager
include conﬁguring the Data Collector VM, and starting the
Load Generator boot procedure (described in detail in Section
IV-B). Depending on the experiment proﬁle, this procedure
conﬁgures the IaaS virtual network resources in order to mirror
the control plane requests issued by the SDN applications and
NFV (e.g., intent installation and removal request) towards the
newly created Load Generator VM. The mirrored control plane
trafﬁc is used by the Load Generator to exercise the system
under test. In addition, depending on the experiment proﬁle,
the Load Generator can create additional synthetic control plane
requests and request proﬁles that are injected into the mirrored
trafﬁc to test the system against different scenarios.
Testing Phase – The Experiment Manager starts the data
collection daemons on the controller instances, and commands
the Load Generator to start
loading them with the gener-
ated/mirrored requests. The duration of each test can be pro-
grammed with a ﬁxed time or with a ﬁxed number of requests,
depending on the user-provided experiment proﬁle.
Clean-up Phase – Each experiment is followed by a system
clean-up phase, accomplished by the Experiment Manager by
reverting the VMs to the snapshot
taken during the start-
up phase, in order to repeat the experiment under the same
conﬁguration. A further clean-up operation, which is executed
when no further experiments are needed, consists in stopping
all the VMs, and in invoking the Campaign Manager to remove
any allocated resources, such as VLANs and virtual switches
created by the Load Generator, as described in Section IV-B.
B. Collected Data
The Data Collector collects all the data generated by all
the instances composing the system under test, and stores
experimental data (in the form of raw logs, compound metrics
and high-level summary reports) in a centralized data store
setup by the Campaign Manager, for subsequent analysis and
visualization. Data is gathered from a number of sources,
including VMs and SDN controller logs, workload metrics,
resource usage metrics, and various diagnostic logs, at the con-
clusion of the experiments in order to eliminate any additional
monitoring overhead in the system under test. Figure 3 shows a
non-exhaustive lists of key data sets collected during and after
each experiment. The Data Collector Server uses InﬂuxDB
NO-SQL as event storage engine, while raw logs and system
dumps are stored in data store set up by the Campaign Manager.
644
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
Intent events 
Controller-level
 Cluster events
 Cluster consistency data
 Topology events
 Flow rule events 

System-level
 System log (guest OS)
 Karaf event
 Controller logs
 VM orchestration logs
 Workload Logs
JVM/host level
 CPU usage
 Memory usage
 Thread count

JVM memory
Network-level
 Device resource utilization
 Device events  
 Flow rule statistics
 PCAP packet dump
 Port-level statistics
Fig. 3. Data Collected for each experiment.
C. Metrics
The main metrics measured by SCP-CLUB are: i) intent
installation/withdrawal throughput (hereafter simply through-
put), computed as the rate of intent requests correctly served;
ii) intent installation/withdrawal latency (hereafter, latency),
computed as the difference between the time the intent instal-
lation or removal request is accepted by the system (Request
command in Figure 1), and the time the operation is terminated
and result communicated to the requester (Request executed
in Figure 1); iii) control plane capacity, deﬁned as the maxi-
mum rate of intent installation/withdrawal that the system can
withstand while keeping a given rate of successful installed
intent. Samples are collected in 1 second bins. Mean, standard
deviation, percentiles, and conﬁdence intervals are estimated
for each bin, and, if speciﬁed in the user-provided experiment
proﬁle, across different repetition of the same experiment (e.g.,
in case of synthetic or trace-driven workload).
D. System Diagnostics and Test Pass/Fail Criteria
In order to validate the correctness of the experiments results,
SCP-CLUB performs detailed checks to detect whether the
tests performed as expected. To this end, the Data Collector
collects the error information generated by the cloud platform
– such as VMs preemption/suspension, no storage left in the
data store - and the exceptions generated by the VMs and the
SDN controller instances. Concerning the latter, the Campaign
Manager executes the following checks: ping of VMs, SDN
controller IPs and IDs; check of correct set up of links and
hosts in the SDN controller; check of proper conﬁguration of
the controllers network interfaces, and of IDs of the devices;
check of topology (device and host connectivity); check of
correct loading of software modules (e.g., Karaf Bundles or
Java Classes); check of the state of the SDN controllers in
the same cluster (if applicable); check of available space in
the data stores; check of exceptions reported in the logs of
each VMs. These checks are performed before and after each
experiment, to detect potential problems in the tests. Results
of the diagnostics and the outcome (pass/failed) are stored in a
report provided at the end of each experiment. An experiment
is considered successful (pass) with:
• 100% test completed (time or request completed);
• Throughput: serving ≥ 99% of the requests 95% of time;
• Latency: ≤ 95% of latency measured during the warm-up;
• All controllers and network devices working and available;
• All software components active at test start and end;
• No defects/outages/crashes/exceptions encountered;
• No error present in the logs and no recovery triggered;
• All the submitted requests eventually processed.
Testing Resources (VMs)
Data Collector
Topology Manager
3. Clone 
topology  
2. Create, configure, 
deploy
Campaign 
Manager VM
1. Configure 
virtual 
network 
(VLAN, Ports)
Iaas virtual switch (control plane)
44. Port mirroring
Intent Request A->B 1Gb, 10ms
e
t
a
t