title:Credit-Scheduled Delay-Bounded Congestion Control for Datacenters
author:Inho Cho and
Keon Jang and
Dongsu Han
Credit-Scheduled Delay-Bounded
Congestion Control for Datacenters
Inho Cho
KAIST
PI:EMAIL
Keon Jang*
Google Inc.
PI:EMAIL
Dongsu Han*
KAIST
PI:EMAIL
ABSTRACT
Small RTTs (∼tens of microseconds), bursty flow arrivals, and a
large number of concurrent flows (thousands) in datacenters bring
fundamental challenges to congestion control as they either force
a flow to send at most one packet per RTT or induce a large queue
build-up. The widespread use of shallow buffered switches also
makes the problem more challenging with hosts generating many
flows in bursts. In addition, as link speeds increase, algorithms that
gradually probe for bandwidth take a long time to reach the fair-
share. An ideal datacenter congestion control must provide 1) zero
data loss, 2) fast convergence, 3) low buffer occupancy, and 4) high
utilization. However, these requirements present conflicting goals.
This paper presents a new radical approach, called ExpressPass,
an end-to-end credit-scheduled, delay-bounded congestion control
for datacenters. ExpressPass uses credit packets to control conges-
tion even before sending data packets, which enables us to achieve
bounded delay and fast convergence. It gracefully handles bursty
flow arrivals. We implement ExpressPass using commodity switches
and provide evaluations using testbed experiments and simulations.
ExpressPass converges up to 80 times faster than DCTCP in 10 Gbps
links, and the gap increases as link speeds become faster. It greatly
improves performance under heavy incast workloads and signifi-
cantly reduces the flow completion times, especially, for small and
medium size flows compared to RCP, DCTCP, HULL, and DX under
realistic workloads.
CCS CONCEPTS
• Networks → Transport protocols;
KEYWORDS
Congestion Control, Datacenter Network, Credit-based
ACM Reference format:
Inho Cho, Keon Jang, and Dongsu Han. 2017. Credit-Scheduled Delay-
Bounded Congestion Control for Datacenters. In Proceedings of SIGCOMM
’17, Los Angeles, CA, USA, August 21-25, 2017, 14 pages.
https://doi.org/10.1145/3098822.3098840
*co-corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Association
for Computing Machinery.
ACM ISBN 978-1-4503-4653-5/17/08. . . $15.00
https://doi.org/10.1145/3098822.3098840
INTRODUCTION
1
Datacenter networks are rapidly growing in terms of size and link
speed [52]. A large datacenter network connects over 100 thousand
machines using a Clos network of shallow buffered switches [2, 28].
Each server is connected at 10/40 Gbps today with 100 Gbps on the
horizon. This evolution enables low latency and high bandwidth
communication within a datacenter. At the same time, it poses a
unique set of challenges for congestion control.
In datacenters, short propagation delay makes queuing delay a
dominant factor in end-to-end latency [3]. Thus, with higher link
speeds, fast convergence has become much more important [34].
However, with buffer per port per Gbps getting smaller, ensuring zero
loss and rapid convergence with traditional congestion control has
become much more challenging. In addition, Remote Direct Memory
Access (RDMA), recently deployed in datacenters [40, 41, 58], poses
more stringent latency and performance requirements (e.g., zero data
loss).
A large body of work addresses these challenges. One popular
approach is to react to early congestion signals in a more accurate
fashion, using ECN [3, 5, 55, 58] or network delay [38, 41, 47].
These approaches keep queuing lower and handle incast traffic much
better than the traditional TCP. However, they are still prone to
buffer overflows in bursty and incast traffic patterns. Thus, they rely
on priority flow control (PFC) or avoid an aggressive increase to
prevent data loss. In fact, small RTTs, bursty flow arrivals, and a
large number of concurrent flows in datacenters bring fundamental
challenges to this approach because these factors either force a
flow to send at most one packet per RTT or induce a large queue
build-up. We show in Section 2 that even a hypothetically ideal
rate control faces these problems. An alternative is to explicitly
determine the bandwidth of a flow or even the packet departure time
using a controller or a distributed algorithm [34, 47]. However, this
approach incurs signaling latency and is very difficult to scale to
large, high-speed (e.g., 100 Gbps) datacenters, and is challenging to
make robust against failures and traffic churn [44].
Our approach uses credit packets to control the rate and schedule
the arrival of data packets. Receivers send credit packets to senders
on a per-flow basis in an end-to-end fashion. Switches then rate-limit
the credit packets on each link and determine the available bandwidth
for data packets flowing in the reverse direction. By shaping the flow
of credit packets in the network, the system proactively controls
congestion even before sending data packets. A sender naturally
learns the amount of traffic that is safe to send, rather than reacting
to the congestion signal after sending data. This allows us to quickly
ramp up flows without worrying about data loss. In addition, it
effectively solves incast because the arrival order of credit packets
at the bottleneck link naturally schedules the arrival of data packets
in the reverse path at packet granularity.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Inho Cho, Keon Jang, and Dongsu Han
(a) Ideal rate-based
Figure 1: Data queue length of window/rate-based protocol,
DCTCP and credit-based protocol
(c) Credit-based
(b) DCTCP
Realizing the idea of credit-based congestion control, however, is
not trivial. One might think that with the credit-based scheme a naïve
approach in which a receiver sends credit packets as fast as possible
(i.e. the maximum credit rate corresponding to its line rate) can
achieve fast convergence, high utilization, and fairness at the same
time. In a simple, single bottleneck network, this is true. However,
in large networks, the three goals are often at odds, and the naïve
approach presents serious problems: (i) it wastes bandwidth when
there are multiple bottlenecks, (ii) it does not guarantee fairness, and
(iii) the difference in path latency can cause queuing. In addition,
in networks with multiple paths, credit and data packets may take
asymmetric paths.
This paper demonstrates credit-based congestion control is a vi-
able alternative for datacenters by addressing the challenges and
answers key design questions that arise from a credit-based ap-
proach. The resulting design incorporates several components and
techniques: (i) rate-limiting credits at switches, (ii) symmetric hash-
ing to achieve path symmetry, (iii) credit feedback control, (iv)
random jitter, and (v) network calculus to determine the maximum
queuing.
Our feedback control achieves fast convergence and zero data
loss. It effectively mitigates utilization and fairness issues in multi-
bottleneck scenarios. We also demonstrate ExpressPass can be im-
plemented using commodity hardware. Our evaluation shows that
ExpressPass converges in just a few RTTs when a new flow starts
for both 10 Gbps and 100 Gbps, whereas DCTCP takes over hun-
dreds and thousands of RTTs respectively. In all of our evaluations,
ExpressPass did not exhibit any single data packet loss. Express-
Pass uses up to eight times less switch buffer than DCTCP, and
data buffer is kept close to zero at all times. Our evaluation with
realistic workload shows that ExpressPass significantly reduces flow
completion time especially for small to medium size flows when
compared to RCP [23], DCTCP [3], HULL [5], and DX [38], and
the gap increases with higher link speeds.
2 MOTIVATION
In large datacenter networks, it is not uncommon for a host to gener-
ate more than one thousand concurrent connections [3]. The num-
ber of concurrent flows traversing a bottleneck link can be large,
and flow arrival is often bursty [13] due to the popularity of the
partition/aggregate communication pattern [3]. However, existing
congestion control algorithms exhibit fundamental limitations under
such workload.
First, partition/aggregate patterns cause bursty packet arrivals that
result in packet drops (i.e., incast). The problem only worsens with
shallow buffered commodity switches that only provide 100 KB of
packet buffer per 10 Gbps port [12] as well as in high-speed network
(a) Naïve Credit-based
(c) DCTCP
Figure 2: Convergence Time (testbed experiment)
(b) TCP Cubic
(e.g., 100 Gbps) where switch buffer per port per Gbps decreases as
link speeds go up 1. Second, even if congestion control algorithms
estimate each flow’s fair-share correctly, bursty flow arrival [3, 13]
still causes unbounded queue build-up.
To demonstrate that even an ideal rate control can result in un-
bounded queue build-up, we assume a hypothetical but ideal con-
gestion control that instantly determines the exact fair-share rate
for each sender using a per-packet timer. We then simulate a par-
tition/aggregate traffic pattern using ns-2. A single master server
continuously generates a 200 B request to multiple workers using
persistent connections, and each worker responds with 1, 000 B of
data for each request. We increase the fan-out from 32 to 2, 048 in an
8-ary fat tree topology with 10 Gbps links with 5 µs delay, 16 core,
32 aggregator, 32 ToR switches and 128 hosts 2.
Figure 1 (a) shows the queue length at the bottleneck link. The
bars represent the minimum and maximum queue, and the box shows
25, 50, and 75%-tile values. Even though the senders transmit data
at their fair-share rate and packets within the same flow are perfectly
paced, the packet queue builds up significantly. This is because while
each flow knows how fast it should transmit its own packets, packets
from multiple flows may arrive in bursts. In the worst case, the
maximum data queue length grows proportionally to the number of
flows (depicted by the red line in Figure 1 (a)). Window or rate-based
congestion control is far from the ideal case because the congestion
control does not converge to the fair-share rate immediately. Figure 1
(b) demonstrates the queue build-up with DCTCP. The average /
maximum data queue lengths are much larger than ideal congestion
control because it takes multiple RTTs to react to queue build-up.
The result suggests that existing window- or rate-based protocols
have fundamental limitations on queuing, and the problem cannot be
solved even with a proactive approach, such as PERC [34]. Existing
congestion control will exhibit high tail latency. The unbounded
queue also forces a packet drop or the use of flow control, such
as PFC. Even worse, large queues interact poorly with congestion
feedback; when flows are transmitting at the fair-share rate, ECN [3]
or delay-based [38, 41] schemes will signal congestion to some
flows, resulting in significant unfairness.
Bounded queue build-up: To overcome the fundamental limitation,
ExpressPass uses credit-based scheduling in which a sender trans-
mits a data packet only after receiving a credit from the receiver.
When a credit packet reaches a sender, the sender transmits a data
packet if it has one to send; otherwise, the credit packet is ignored.
ExpressPass controls congestion by rate-limiting the credit packets
at the switch but without introducing per-flow state. This effectively
1Even a deep buffer Arista 7280R switch provides 15% less buffer per Gbps in their
100 Gbps switches compared to that in 10 Gbps switches [9].
2Note multiple workers can share the same host, when the number of workers exceeds
the number of hosts.
1101001 k10 k326412825651210242048Number of FlowsMax. BoundData Queuepkts1101001 k10 k326412825651210242048Number of Flowspkts1101001 k10 k326412825651210242048Number of Flowspkts0246810050100150200Time (us)25usThroughputGbps0246810020406080Time (ms)47 msGbps02468100100200300400500Time (ms)70 msGbpsCredit-Scheduled Delay-Bounded
Congestion Control for Datacenters
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Figure 3: ExpressPass Overview
schedules the response packets in the bottleneck link at packet gran-
ularity and thus bounds the queue build-up, without relying on a
centralized controller [47]. To demonstrate this, we simulate the
same partition/aggregate workload using our credit-based scheme.
Figure 1 (c) shows the queue build-up. It shows regardless of the
fan-out, the maximum queue build-up is bounded.
In a credit-based scheme, the queue only builds up when flows
have different round-trip times. Two factors can contribute to RTT
differences: 1) the difference in path lengths and 2) the variance
in packet processing time at the host. In datacenter environments,
both can be bounded. The difference in path lengths is bounded by
the network topology. For example, a 3-layered fat tree topology
has minimum round-trip path length 4 and maximum 12 between
any two pairs of hosts. Note the difference is strictly less than the
maximum RTT between any pair of hosts.
The variance in credit processing time can also be bounded. A
host discards credit when it does not have any data to send, thus
the variance comes only from that of the credit processing delay
(e.g., interrupt or DMA latency in a software implementation). In
our software implementation on SoftNIC [31], it varies between 0.9
and 6.2 µs (99.99th percentile). Our simulation result in Figure 1
(b) accounts for this variance. The red line shows the maximum
queue required considering the two delay factors. Note, a hardware
implementation on a NIC can further reduce the variance in credit
processing times.
Fast convergence: Another critical challenge for traditional conges-
tion control is quickly ramping up to the fair-share. Fast ramp-up
is at odds with low buffer occupancy and risks buffer overflow and
packet drops. Thus, in traditional congestion control, it is often a
slow, conservative process, which significantly increases the flow
completion time. In contrast, credit drop is not as detrimental as data