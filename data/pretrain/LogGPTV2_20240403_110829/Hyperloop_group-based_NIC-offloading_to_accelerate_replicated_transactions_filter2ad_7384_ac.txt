at a pre-defined memory location, which we call fixed repli-
cation. This is clearly insufficient for general storage systems
that require flexibility in memory management.
To address this limitation, we propose a solution called
remote work request manipulation, which enables the client
to manipulate the work requests pre-posted on the replicas’
work queues. Our key insight is that we can register repli-
cas’ work queues as RDMA-writable memory regions and
allow the client to modify memory descriptors (stored in a
work queue entry (WQE) data structure) of pre-posted work
requests on replicas. Along with the operation forwarding
feature described above, the client can perform arbitrary
memory operations targeted to RDMA-registered memory
on a group of replicas without involving replicas’ CPUs.
Figure 5 illustrates the workflow. Since the work requests
are at a known memory location on replicas’ memory re-
gion, NICs on replicas can post RECV requests that point
the received metadata to update the memory descriptor in
existing work requests. The metadata contains memory de-
scriptors for every replica and is pre-calculated by the client
and replicated to all replicas using fixed replication. Metadata
can also contain additional information depending on differ-
ent types of memory operation. Note that separate metadata
memory regions are allocated for each primitive and each
region takes ⟨size of memory descriptor⟩ × ⟨group size⟩ ×
⟨number of metadata entries⟩. The sizes of memory descrip-
tor are different by primitives and in our implementation,
the maximum size is 32 bytes, which is the case for gCAS.
The client first builds metadata for each group memory
operation. Then the client posts a WRITE and a SEND work
request, which are used to replicate data to the replicas’
memory region and send the metadata, respectively. Since
the SEND request is a two-sided operation, it consumes a
RECV work request posted by the replica. The RECV request
will trigger the WAIT request and updates metadata region,
the memory descriptors of WRITE and SEND work request,
and activate them (i.e., grant the ownership to the NIC). The
activated WRITE and SEND work request will do the same
procedure for the next node. While this mechanism uses
additional work requests for sending metadata of operations
and waiting a completion of a RECV work request, it does
HyperLoop
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
not incur much network overhead since the size of metadata
is small (at most 32 bytes × ⟨group size⟩), and a WAIT does
not produce any network traffic.
Note that with current RDMA NIC driver implementa-
tions, when an application posts a work request, a NIC will
immediately get the ownership of that request and because
of this, the memory descriptor cannot be modified later. How-
ever, in HyperLoop, the client should be able to manipulate
the memory descriptor of pre-posted work requests. To en-
able this, we modify the userspace NIC driver library (e.g.,
libmlx4 for Mellanox NICs) to make it not yield ownership
to the NIC when the work request is posted. Instead, with
our modified driver, the client will grant the ownership of the
work request to NICs after it updates the memory descriptor
of pre-posted work requests with our primitive.
Integration with other RDMA operations to support
ACID: To support the demanding features by storage sys-
tems, like durable WRITE, transaction execution and group
locking, we further build on the two ideas above.
Specifically, we further leverage the WAIT mechanism to
let the last replica to send an ACK to the client as a group
operation ACK. We use a 0-byte READ immediately follow-
ing each WRITE to flush the data in NIC cache into memory
or NVM, so that each ACK means the operation finishes and
becomes durable. For two-phase transaction execution, after
logs are replicated, we let RNICs perform “local RDMA” to
move data from the log region to the actual data region. For
locking, we leverage the RDMA compare-and-swap (CAS)
atomic operation.
In traditional systems, all these tasks can only be done on
a single host with the help of CPU. However, in HyperLoop,
we turn all these into group-based primitives and removes
replicas’ CPUs from the critical path. Whenever the client of
a replication group performs memory operations (e.g., repli-
cation, transaction execution, or locking) in its own memory
region, RNICs (not CPUs!) on replicas also perform the same
operations against their memory regions. The result is sig-
nificantly better tail latency and lower CPU consumption
than traditional RDMA implementations.
Based on these techniques, we develop four group-based
primitives summarized in Table 1. We describe each of them
in detail next.
4.2 Detailed Primitives Design
Group write (gWRITE): In a replicated storage, when the
client updates the transaction log, it needs to replicate the
updated log to all replicas in a replication group. We abstract
this process into the group memory write (gWRITE) prim-
itive, which enables remote NICs to write any data to the
specified memory location of their host memory region.
gWRITE allows the caller (e.g., client) to write a data to
memory regions of a group of remote nodes without involv-
ing their CPUs. The primitive takes a group ID, a memory
Figure 6: Datapath of gCAS primitive.
offset of data which will be written to remote nodes’ memory
regions, and a size of data as input arguments. For the given
offset and size, it builds metadata and initializes the com-
mand and follows the data path shown in Figure 5. gWRITE
is used to implement replicated transaction log management
described in Section 5.
Group compare-and-swap (gCAS): To ensure data in-
tegrity during concurrent read-write, replicated storage sys-
tems often use some locking mechanism, which also involves
the replicas’ CPUs. We provide the group compare-and-swap
(gCAS) primitive to offload such lock management schemes.
This enables remote NICs to perform compare-and-swap
against a specified memory location on their host memory
region and update the value of the location based on the
result of comparison. The client can acquire a logical group
lock via this primitive without involving the replicas’ CPUs.
gCAS extends the original RDMA single host compare-
and-swap (CAS) operation to a group of nodes. It takes a
group ID, an offset of memory location whose value to be
compared, an old and new value, an execute map, and a result
map as input arguments. The execute and result map are
additional parameters different from the original CAS. It has
a field for each node in the group. The client can specify
whether each remote node has to execute the CAS operation
on the execute bitmap (i.e., marking the corresponding field).
This capability of selective execution is necessary especially
for a client which needs to undo a gCAS operation when the
operation failed to be executed on some remote nodes due
to mismatch between the expected and actual value of the
target memory location. To undo the operation, the client
issues another gCAS operation by marking a set of execute
fields corresponding to the nodes on which the previous CAS
operation was successfully executed and swapping the old
and new values. Each replica updates the result of locally
performed CAS to the result map and the client will receive
the final result map as an ACK of performing gCAS.
Figure 6 shows how gCAS works. On each replica, HyperLoop
creates an additional QP for performing CAS operation lo-
cally. When a RECV work request is consumed by the SEND
work request initiated by the previous node, it updates the
memory descriptor of CAS and SEND work requests.
It is important to note that depending on a value in the ex-
ecute map, each replica has to determine whether it performs
CAS without involving its CPU. We found that when granting
an ownership of work request to the NIC, we can change the
type of pre-posted work request. Using this observation, the
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Kim and A. Memaripour et al.
Table 1: Group-based network primitives for transactions supported by HyperLoop
Primitives
gFLUSH(data_addr, dest_addr, size)
gWRITE(gid, offset, size)
gCAS(gid, offset, old_val, new_val, execute_map, result_map)
gMEMCPY(gid, src_offset, dest_offset, size)
Semantics
Writing data at data_addr to dest_addr by flushing
a volatile cache on the NIC to a durable medium.
Replicating the caller’s data located at offset
to remote nodes’ memory region at offset.
On each node, if the corresponding bit in the execute map is set,
(1) compare the value of data at offset with old_value.
(2) if they are the same, replace the value with new_value.
(3) update the corresponding result field
with the original value of offset.
Copying the data size of size from src_offset to
dest_offset for all nodes.
Figure 7: Datapath of gMEMCPY primitive.
caller makes the CAS be NOP operation if the corresponding
remote node is chosen not to perform the CAS operation.
If the replica performs CAS, it will update the correspond-
ing field in the result map. The completion of CAS work re-
quest triggers the WAIT and SEND requests for the next node.
If the replica is the last one in a group, it will forward the re-
sult map to the client as an ACK using WRITE_WITH_IMM.
gCAS is used to implement a group locking scheme in stor-
age systems, as described in §5.
In many replicated
Group memory copy (gMEMCPY):
storage systems, replica CPUs execute (commit) a transaction
by copying the data corresponding to the transaction from
a log region to a persistent data region. We abstract this
process into the remote memory copy (gMEMCPY) primitive,
which lets remote NICs perform a memory copy on their
host memory for given parameters, i.e., data source address,
destination address, and size.
When the client executes transactions via this primitive,
on all replicas, the NICs will copy the data from the log
region to the persistent data region without involving the
CPUs. gMEMCPY takes a group ID, a memory offset of source
region and destination region, and a size of data being copied.
When this primitive is called, the NICs on replicas perform
memory copy for given source and destination offset against
their host memory without involving their CPUs.
Figure 7 shows how gMEMCPY works. Similar to the dat-
apath of gCAS primitive, HyperLoop creates an additional
QP for performing memory copy operation locally on each
replica. Upon receiving the command from the previous node,
the receive work request updates the memory descriptors of
write and send work requests and triggers the wait request.
Then the NIC performs local memory copy with the write
work request. When the local memory copy is successfully
completed, the NIC will trigger the wait and forwards the
operation to the next node using SEND. If the next node is
the client, it will send an ACK using WRITE_WITH_IMM.
gMEMCPY is used to implement remote log processing de-
scribed in §5.
Group RDMA flush (gFLUSH): To support the durability
of transactions, data written with RDMA writes should be
durable even in the case of system failure (e.g., power outage).
However, the current RDMA protocol implemented on NICs
does not guarantee the durability. The destination NIC sends
an ACK in response to RDMA WRITE as soon as the data is
stored in the NIC’s volatile cache. This means that the data
can be lost on power outage before the data is flushed into
NVM. Thus, we need a new RDMA FLUSH primitive that
supports the durability at the “NIC-level”.
To address this, we design Non-volatile RDMA FLUSH
(gFLUSH) primitive which enables durable RDMA WRITE
based on the existing RDMA operations. We leverage a fea-
ture supported by the NIC firmware that flushes a mem-
ory region in the cache when it becomes dirty. In our im-
plementation, when an RDMA FLUSH is issued, the NIC
immediately (without for waiting for an ACK) issues a 0-
byte RDMA READ to the same address. Then the destina-
tion’s NIC flushes the cache for the READ and the source’s
NIC gives the application ACK after the 0-byte READ is
acknowledged. This will ensure that each RDMA FLUSH
will flush volatile caches to host memory hierarchy which is
non-volatile with battery support [73]. Similar to gWRITE,
gFLUSH operations are also propagated down the chain for
durability across the replicas.
There is an important distinction between gFLUSH and
other primitives. gFLUSH can either be issued by itself or as
an interleaved operation with gWRITE, gCAS or gMEMCPY.
For instance, an interleaved gWRITE and gFLUSH on a replica
would first flush the cache and only then forward the opera-
tions down the chain. This helps ensure that durable updates
are propagated in the order needed by the chain or between
the primary and backups in other protocols.
To the best of our knowledge, HyperLoop is the first sys-
tem to describe the design and implementation of an RDMA
HyperLoop
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
NIC that can guarantee durability with NVM. Existing sys-
tems [59, 72] assume this ability but do not provide details
of the design and implementation.
Summary: We implement the primitives with 3,417 lines
of C library. We also modify 58 lines of code in libmlx4
and libibverbs
3 to implement gFLUSH and remote work
request manipulation described in §3. In the next section, we
will describe a couple of case study storage systems whose
performance can be improved by using HyperLoop.
5 HYPERLOOP CASE STUDIES
HyperLoop helps NVM- and RDMA-based replicated databases
to offload transactions to NICs. Based on HyperLoop primi-
tives, different consistency models required by applications
can be implemented as we will show in this section and
discuss in §7. To demonstrate the benefits of HyperLoop,
we modify and optimize two widely used databases to an
NVM- and RDMA-based chain replication using state-of-
the-art CPU-polled kernel bypass approach. We then offload
the work done by the CPUs to the RDMA NICs by using
HyperLoop. We choose RocksDB (open source alternative to
Google’s LevelDB), and MongoDB (open source alternative
to Azure DocumentDB and AWS DynamoDB) because these
databases have in-memory (RAMCloud [85] like) implemen-
tations for volatile DRAM. Such implementations makes it
easier to adapt them to NVM.
For such systems, requirements for maintaining a database
typically translate to initializing NVM to contain a database
and a write-ahead log, appending transactions to the log,
processing the log and applying the transactions to the data-
base, and obtaining locks to facilitate concurrent accesses to
the database.
Database Initialization: The Initialize function uses
values specified in a configuration object to set up HyperLoop
and create required connections between the replica and it’s
upstream and downstream replicas in the chain. Additionally,
it creates/opens a handle to the NVM for the replica. This
NVM area contains space enough to hold a write-ahead log
as well as the database as defined in the configuration object.
Log Replication: Each log record is a redo-log and struc-
tured as a list of modifications to the database [83]. Each
entry in the list contains a 3-tuple of (data, len, offset)
representing that data of length len is to be copied at offset
in the database. Every log record sent by a client is appended
to the replicas’ write-ahead logs in the chain by calling
Append(log record), which is implemented using gWRITE
and gFLUSH operations.
A client in our case study is a single multi-threaded process
that waits for requests from applications and issues them into
the chain concurrently. Multiple clients can be supported in
3
libibverbs and libmlx4 are userspace libraries/drivers that allow
userspace processes to use InfiniBand/RDMA Verbs on Mellanox hardware.
the future using shared receive queues on the first replica in
the chain.
Log Processing: The remote log processing interface en-
ables the client to copy data from the write-ahead log of
replicas to their database regions without involving replica
CPUs. For each log record and starting from the head of the
write-ahead log, ExecuteAndAdvance processes its entries
one by one. To process each entry, it issues a gMEMCPY
to copy len bytes of data from data to offset on all replicas
followed by a gFLUSH to ensure durability. Once all oper-
ations for a log record are processed, ExecuteAndAdvance
updates the head of the write-ahead log using a gWRITE and
gFLUSH.