### A.攻击者能力：投毒攻击和逃避攻击
将 ML
用于安全任务可能会使系统面临新的攻击。例如，在投毒攻击中，攻击者将恶意训练数据注入系统，在逃避攻击中，攻击者构建看似良性但逃避检测的输入。就本研究的设计而言，不考虑此类攻击。
尽管攻击者可能会建立一个恶意网站，网络爬虫随后会使用该网站来收集中毒的训练数据，但假设这对于攻击者来说是非常昂贵的（因为爬取 10,000
个最受欢迎的网站）并且训练数据用于训练模型不会被对手投毒。
关于逃避攻击，系统旨在检测意外漏洞（例如，帮助保护对其网站上的 JavaScript
具有控制权的网站开发人员）。如果攻击者能够操纵网站上的代码，则该网站已经被承诺，攻击者无需逃避检测基础设施。攻击者模型假设 JavaScript
代码是良性的，但可能存在漏洞；攻击者向网站提供恶意输入以利用 DOM XSS 漏洞，但不控制系统。
### B.特征提取与数据准备
在能够对源代码片段进行训练和分类之前，必须将这些代码转换为神经网络可以使用的形式。在本节中将描述将标记的 AST 节点转换为用于训练的特征向量的方法。
#### （1）代码分段
给定一块带标签的源代码，选择通过其函数调用来分割代码。对于这项工作中提出的实验，位于单个函数调用中的代码用作训练和分类的单个单元。还尝试基于脚本对代码进行分段，并使用在固定语义距离内包含周围
AST
节点的分段；然而，按函数分割代码为训练模型产生了最好的结果。按整个脚本分段选择过大的代码片段，而固定语义距离策略产生过小的代码片段。在预测漏洞时，这两种表示都会阻止分类器学习有意义的特征。
#### （2）提取特征和代码表示
在标记的源代码被转换成段后，为的 ML 模型提取输入特征。对于此处显示的实验，使用词袋表示：每个函数都由包含在函数调用中的解析 AST
标记的词频字典唯一标识。将所有相关的符号和操作（变量名、操作名、方法名、属性名等）存储在这个字典中。尽管变量和方法名称可能会改变，但相信这种表示在源代码中的微小变化（例如不同的库版本）时是健壮的，因为通常跨版本维护大量的变量名称和函数名称。
还尝试了先前工作中从 C 中提取程序切片的方法，但发现 Javascript
的高度动态性使研究者无法自信地确定切片所需的程序关键点。还对使用基于门控图神经网络
的模型的先前工作进行了实验，但发现这些技术产生的模型不稳定且性能不佳，这也可能是由于 Javascript 的动态性。
#### （3）实验数据设置
将数据集划分为多个子集：80% 的“训练”用于训练模型，10% 的“验证”用于在超参数探索期间评估竞争模型，以及 10%
的“测试”用于测量最终模型的性能。在划分时，按函数源自的脚本进行拆分（对于收集的数据集中的每个脚本，其函数调用有 80% 的机会用于训练，10%
的机会用于测试，10%
的机会用于验证）。执行此拆分是为了评估模型在之前从未见过的完整脚本上的性能，并捕获更真实的设置，在该设置中，模型呈现完整的脚本（然后按函数分段）。
此外，希望根据每个函数在爬行中观察到的频率来增加其重要性。与相对不常见的代码相比，在非常常见的库中定义的函数对于正确分类更重要。为此，在混洗数据之前对训练集中的频繁代码实例进行过采样。这比在训练期间应用权重更可取，因为在实验中，模型在呈现大量超过其他函数的极其常见的函数时不会收敛。如果在训练阶段结束时观察到一个共同的函数，模型就会发生巨大的变化。然而，通过多次重复实例，这些影响会在训练期间得到平滑。
#### （4）平衡误差
使用数据进行训练的另一个问题是标签之间的大量类别不平衡：非易受攻击的函数远多于易受攻击的函数（只有 0.024%
的函数被确认为易受攻击）。因此，在训练期间通过相应地惩罚损失函数为正标签添加了权重。对 1、10、100 和 1,000 的惩罚项进行了试验，发现 100
是最佳的——惩罚越低，分类器永远不会将函数预测为易受攻击的，而惩罚项为 1,000，分类器不会收敛。
#### （5）矢量化特征
使用特征散列来表示稀疏数据，这允许无界词通过散列到特定桶的术语表示为向量。这种技术的缺点是当散列函数发生冲突时它会引入歧义。为了减轻冲突的影响，使用了
2×18
的特征大小，这是平衡内存需求和冲突概率的推荐大小。使用嵌入层将稀疏的词袋编码为密集的向量空间。这种嵌入是模型架构的第一部分，作为第一个隐藏层的输入，也在训练期间进行了优化。
### C.实施
在 TensorFlow中构建模型并使用 Adagrad 优化器训练模型（学习率为 0.05，批量大小为 64）。对于最小的模型，训练时间为每秒 11K
个函数，这意味着使用 64GB 虚拟机和 16GB NVIDIA Tesla P100 GPU 对总数据的 5% 进行训练大约需要 20 小时。
### D.性能指标
对于任何类别不平衡的任务，准确性不是一个有用的度量标准，因为分类器可以通过预测所有函数都不会受到攻击来达到近乎完美的准确性。由于正在评估 ML
模型是否可以与其他技术结合使用，因此在调整准确率和开销之间的权衡时，精度-召回率的权衡更有用。将准确率定义为预测的漏洞确实被标记为漏洞的比例，召回率定义为被正确预测为漏洞的标记漏洞的比例。
由于在常见漏洞上表现良好尤其会影响召回率，因此还考虑了对不同漏洞的表现。将不同召回定义为模型正确识别的不同标记漏洞的比例，而真实召回定义为所有标记漏洞被正确识别的比例。在计算真实召回率时，每个函数都以其真实的观察频率加权；所以真正的召回代表对算法在部署时会遇到的真实数据的召回。
## 0x05 Results
首先，使用验证数据集来调整模型类型和模型大小等参数。然后在测试数据集上评估性能最佳的模型，包括未确认和已确认的漏洞。
在本节中，显示的结果是未确认漏洞的 3 倍平均值，以及已确认漏洞的 5 倍平均值。由于已确认漏洞的数量明显低于未确认漏洞的数量，发现 3
折不足以确认漏洞，因此在这些实验中使用了 5 折。还发现收敛不需要使用整个训练数据集。在训练期间监控模型的性能，并最终决定对于每个折叠，使用 20%
的可用训练数据（总数据集的 16%）就足够了。
### A.模型大小和类型
尝试了不同大小的深度神经网络模型。对于这些实验，报告了 3 层全连接 DNN 的结果。对于每个架构，通常在每一层之后将层大小减半，从而形成层大小为 [N,
N/2, N/4] 的全连接架构，其中 N 是第一个隐藏层的大小。还试验了线性模型，并将它们的性能与 DNN 进行了比较。下图突出显示了ML
架构的不同组件，并显示了评估的各种超参数。  
#### （1）嵌入大小
首先在神经网络中尝试了嵌入层的大小，如 Sec. 4.2.5.嵌入层是一个密集的全连接层，它在散列空间（在实现中为
2×18）中转换稀疏标记，并将密集向量输出到第一个 DNN 隐藏层。下图显示了 N=500 的 3 层 DNN 的 64、256 和 1024
的各种嵌入大小，经过训练以预测未确认的漏洞。同样没有发现嵌入层大小之间的显着差异，并为所有未来的实验选择了最小的嵌入大小
64，以最大限度地减少用例中的大小和推理时间。  
#### （2）模型大小
通过改变 [N, N/2, N/4] DNN 架构中隐藏层的大小来探索模型大小的影响。对于未确认和已确认的漏洞，训练了 DNN，其中 N =
100、200、500、1000 和 2000。未确认漏洞的结果如下图a 所示，已确认漏洞的结果如下图b
所示。正如预期的那样，预测未确认漏洞的性能明显优于预测已确认漏洞的性能。在这两个实验中，发现模型大小对数据的性能也没有显着影响。由于减小模型大小不会对预测性能产生不利影响，因此选择使用最小的评估模型架构（3
个隐藏层，大小为 100、50 和 25）在进一步的实验中针对已确认和未确认的漏洞。  
#### （3）模型大小权衡
在提出的用例中，较小的模型是首选，因为它们的推理时间较短且存储大小较小。在没有任何优化的情况下，选择的模型大小为 65MB。由于大多数模型都小到可以在
12GB GPU 中完全处理，因此推理时间在很大程度上不受模型大小的影响。为了更好地理解模型在其他设置中的开销，还测量了 GPU
硬件上的推理时间，并将结果显示在下表中。对于选择的 N = 100 和嵌入大小为 64 的模型，平均时间在 24GB RAM、4 核 Intel
i5-6400 3.30GHz CPU 和 Titan X Pascal 12GB GPU、17