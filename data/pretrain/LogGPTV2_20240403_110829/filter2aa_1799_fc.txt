pacted container). The bitmap has a bit set to 1 if the relative cluster is allocated; otherwise, it’s set to 0. 
Figure 11-93 shows an example of a base container (C32) that maps a range of virtual LCNs (0x8000 
to 0x8400) to real volume’s LCNs (0xB800 to 0xBC00, identified by R46). As previously discussed, the 
container ID of a given virtual LCN range is derived from the starting virtual cluster number; all the 
Furthermore, you can start a garbage collection (see the next paragraph for details about this 
feature) through the following command:
fsutil volume smrGc  Action=startfullspeed
The garbage collection can even be stopped or paused through the relative Action param-
eter. You can start a more precise garbage collection by specifying the IoGranularity parameter, 
IoGranularity parameter, 
IoGranularity
which specifies the granularity of the garbage collection I/O, and using the start action instead 
start action instead 
start
of startfullspeed.
CHAPTER 11
Caching and file systems
767
containers are virtually contiguous. In this way, ReFS never needs to look up a container ID for a given 
container range. Container C32 of Figure 11-93 only has 560 clusters (0x230) contiguously allocated 
(out of its 1,024). Only the free space at the end of the base container can be used by ReFS. Or, for 
non-SMR disks, in case a big chunk of space located in the middle of the base container is freed, it 
can be reused too. Even for non-SMR disks, the important requirement here is that the space must 
be contiguous. 
If the container becomes fragmented (because some small file extents are eventually freed), ReFS 
can convert the base container into a compacted container. This operation allows ReFS to reuse the 
container’s free space, without reallocating any row in the extent table of the files that are using the 
clusters described by the container itself.
Cluster size: 64KB
Volume size: 1TB (0x1000000 clusters)
Container table entry:
Key –> (ID: 32, Type: Base)
Value –> Allocated size: 0x230 clusters
Real LCNs: [0xB800 - 0xBC00]
0x8000
0x8400
0xB800
R46
C32
0xBC00
Base Container C32
VCN RANGE
[0 - 0x400]
[0x400 - 0x800]
[0x800 - 0xA00]
[0xA00 - 0xC00]
[0xC00 - 0xD20]
LCN
0x18400
0x32000
0x61E00
0x11200
0x8110
CONTAINER ID
97
200
391
68
32
EXTENT TABLE
0
64MB
128MB
192MB
210MB
Container size: 64MB (0x400 clusters)
FIGURE 11-93 An example of a base container addressed by a 210 MB file. Container C32 uses only 35 MB of its 
64 MB space.
ReFS provides a way to defragment containers that are fragmented. During normal system I/O 
activity, there are a lot of small files or chunks of data that need to be updated or created. As a result, 
containers located in the slow tier can hold small chunks of freed clusters and can become quickly 
fragmented. Container compaction is the name of the feature that generates new empty bands in the 
slow tier, allowing containers to be properly defragmented. Container compaction is executed only in 
the capacity tier of a tiered volume and has been designed with two different goals:
768
CHAPTER 11
Caching and file systems
I 
Compaction is the garbage collector for SMR-disks: In SMR, ReFS can only write data in the
capacity zone in a sequential manner. Small data can’t be singularly updated in a container lo-
cated in the slow tier. The data doesn’t reside at the location pointed by the SMR write pointer,
so any I/O of this kind can potentially corrupt other data that belongs to the band. In that case,
the data is copied in a new band. Non-SMR disks don’t have this problem; ReFS updates data
residing in the small tier directly.
I 
In non-SMR tiered volumes, compaction is the generator for container rotation: The
generated free containers can be used as targets for forward rotation when data is moved from
the fast tier to the slow tier.
ReFS, at volume-format time, allocates some base containers from the capacity tier just for com-
paction; which are called compacted reserved containers. Compaction works by initially searching for 
fragmented containers in the slow tier. ReFS reads the fragmented container in system memory and 
defragments it. The defragmented data is then stored in a compacted reserved container, located in 
the capacity tier, as described above. The original container, which is addressed by the file extent table, 
becomes compacted. The range that describes it becomes virtual (compaction adds another indirec-
tion layer), pointing to virtual LCNs described by another base container (the reserved container). At 
the end of the compaction, the original physical container is marked as freed and is reused for different 
purposes. It also can become a new compacted reserved container. Because containers located in the 
slow tier usually become highly fragmented in a relatively small time, compaction can generate a lot of 
empty bands in the slow tier.
The clusters allocated by a compacted container can be stored in different base containers. To prop-
erly manage such clusters in a compacted container, which can be stored in different base containers, 
ReFS uses another extra layer of indirection, which is provided by the global container index table and 
by a different layout of the compacted container. Figure 11-94 shows the same container as Figure 
11-93, which has been compacted because it was fragmented (272 of its 560 clusters have been freed).
In the container table, the row that describes a compacted container stores the mapping between the
cluster range described by the compacted container, and the virtual clusters described by the base
containers. Compacted containers support a maximum of four different ranges (called legs). The four
legs create the second indirection layer and allow ReFS to perform the container defragmentation in an
efficient way. The allocation bitmap of the compacted container provides the second indirection layer,
too. By checking the position of the allocated clusters (which correspond to a 1 in the bitmap), ReFS is
able to correctly map each fragmented cluster of a compacted container.
In the example in Figure 11-94, the first bit set to 1 is at position 17, which is 0x11 in hexadecimal. In 
the example, one bit corresponds to 16 clusters; in the actual implementation, though, one bit corre-
sponds to one cluster only. This means that the first cluster allocated at offset 0x110 in the compacted 
container C32 is stored at the virtual cluster 0x1F2E0 in the base container C124. The free space avail-
able after the cluster at offset 0x230 in the compacted container C32, is mapped into base container 
C56. The physical container R46 has been remapped by ReFS and has become an empty compacted 
reserved container, mapped by the base container C180.
CHAPTER 11
Caching and file systems
769
0x8000
0x8400
C32
0x1F000
0x1F400
C124
C56
0x32400
0x32800
R201
R11
C124 Container table entry:
Key –> (ID: 124, Type: Base)
Value –> Allocated size: 0x400 clusters
Real LCNs: [0x32400 - 0x32800]
Compacted Container C32
C56 Container table entry:
Key –> (ID: 56, Type: Base)
Value –> Allocated size: 0x2F0 clusters
Real LCNs: [0x2C00 - 0x3000]
C32 Container Index table entry:
Key –> (ID: 32, Type: Compacted)
Value –> Index Allocation Bitmap (1 bit = 16 clusters)
0000000000000000 0111111111111000 00000…
C32 Container table entry:
Key –> (ID: 32, Type: Compacted)
Value –> 2 Legs
1. Virtual LCNs: [0x1F2E0 - 0x1F400]
2. Virtual LCNs: [0x1C400 - 0x1C6F0]
0x1C400
0x1C800
0x2C00
0x3000
0x2D000
0x2D400
C180
0xB800
0xBC00
R46
FIGURE 11-94 Container C32 has been compacted in base container C124 and C56.
In SMR disks, the process that starts the compaction is called garbage collection. For SMR disks, an 
application can decide to manually start, stop, or pause the garbage collection at any time through the 
FSCTL_SET_REFS_SMR_VOLUME_GC_PARAMETERS file system control code. 
In contrast to NTFS, on non-SMR disks, the ReFS volume analysis engine can automatically start the 
container compaction process. ReFS keeps track of the free space of both the slow and fast tier and the 
available writable free space of the slow tier. If the difference between the free space and the available 
space exceeds a threshold, the volume analysis engine kicks off and starts the compaction process. 
Furthermore, if the underlying storage is provided by Storage Spaces, the container compaction runs 
periodically and is executed by a dedicated thread.
Compression and ghosting
ReFS does not support native file system compression, but, on tiered volumes, the file system is able to 
save more free containers on the slow tier thanks to container compression. Every time ReFS performs 
container compaction, it reads in memory the original data located in the fragmented base container. 
At this stage, if compression is enabled, ReFS compresses the data and finally writes it in a compressed 
compacted container. ReFS supports four different compression algorithms: LZNT1, LZX, XPRESS, and 
XPRESS_HUFF.
Many hierarchical storage management (HMR) software solutions support the concept of a ghosted
file. This state can be obtained for many different reasons. For example, when the HSM migrates the 
user file (or some chunks of it) to a cloud service, and the user later modifies the copy located in the 
cloud through a different device, the HSM filter driver needs to keep track of which part of the file 
changed and needs to set the ghosted state on each modified file’s range. Usually HMRs keep track 
of the ghosted state through their filter drivers. In ReFS, this isn’t needed because the ReFS file system 
exposes a new I/O control code, FSCTL_GHOST_FILE_EXTENTS. Filter drivers can send the IOCTL to the 
ReFS driver to set part of the file as ghosted. Furthermore, they can query the file’s ranges that are in 
the ghosted state through another I/O control code: FSCTL_QUERY_GHOSTED_FILE_EXTENTS.
770
CHAPTER 11
Caching and file systems
ReFS implements ghosted files by storing the new state information directly in the file’s extent 
table, which is implemented through an embedded table in the file record, as explained in the previ-
ous section. A filter driver can set the ghosted state for every range of the file (which must be cluster-
aligned). When the ReFS driver intercepts a read request for an extent that is ghosted, it returns a 
STATUS_GHOSTED error code to the caller, which a filter driver can then intercept and redirect the read 
to the proper place (the cloud in the previous example).
Storage Spaces
Storage Spaces is the technology that replaces dynamic disks and provides virtualization of physical stor-
age hardware. It has been initially designed for large storage servers but is available even in client editions 
of Windows 10. Storage Spaces also allows the user to create virtual disks composed of different underly-
ing physical mediums. These mediums can have different performance characteristics. 
At the time of this writing, Storage Spaces is able to work with four types of storage devices: 
Nonvolatile memory express (NVMe), flash disks, persistent memory (PM), SATA and SAS solid state 
drives (SSD), and classical rotating hard-disks (HDD). NVMe is considered the faster, and HDD is the 
slowest. Storage spaces was designed with four goals:
I 
Performance: Spaces implements support for a built-in server-side cache to maximize storage
performance and support for tiered disks and RAID 0 configuration.
I 
Reliability: Other than span volumes (RAID 0), spaces supports Mirror (RAID 1 and 10) and
Parity (RAID 5, 6, 50, 60) configurations when data is distributed through different physical disks
or different nodes of the cluster.
I 
Flexibility: Storage spaces allows the system to create virtual disks that can be automatically
moved between a cluster’s nodes and that can be automatically shrunk or extended based on
real space consumption.
I 
Availability: Storage spaces volumes have built-in fault tolerance. This means that if a drive, or
even an entire server that is part of the cluster, fails, spaces can redirect the I/O traffic to other
working nodes without any user intervention (and in a way). Storage spaces don’t have a single
point of failure.
Storage Spaces Direct is the evolution of the Storage Spaces technology. Storage Spaces Direct is 
designed for large datacenters, where multiple servers, which contain different slow and fast disks, are 
used together to create a pool. The previous technology didn’t support clusters of servers that weren’t 
attached to JBOD disk arrays; therefore, the term direct was added to the name. All servers are con-
nected through a fast Ethernet connection (10GBe or 40GBe, for example). Presenting remote disks 
as local to the system is made possible by two drivers—the cluster miniport driver (Clusport.sys) and 
the cluster block filter driver (Clusbflt.sys)—which are outside the scope of this chapter. All the storage 
physical units (local and remote disks) are added to a storage pool, which is the main unit of manage-
ment, aggregation, and isolation, from where virtual disks can be created.
The entire storage cluster is mapped internally by Spaces using an XML file called BluePrint. The file 
is automatically generated by the Spaces GUI and describes the entire cluster using a tree of different 
CHAPTER 11
Caching and file systems
771
storage entities: Racks, Chassis, Machines, JBODs (Just a Bunch of Disks), and Disks. These entities com-
pose each layer of the entire cluster. A server (machine) can be connected to different JBODs or have 
different disks directly attached to it. In this case, a JBOD is abstracted and represented only by one 
entity. In the same way, multiple machines might be located on a single chassis, which could be part 
of a server rack. Finally, the cluster could be made up of multiple server racks. By using the Blueprint 
representation, Spaces is able to work with all the cluster disks and redirect I/O traffic to the correct 
replacement in case a fault on a disk, JBOD, or machine occurs. Spaces Direct can tolerate a maximum 
of two contemporary faults. 
Spaces internal architecture
One of the biggest differences between Spaces and dynamic disks is that Spaces creates virtual disk 
objects, which are presented to the system as actual disk device objects by the Spaces storage driver 
(Spaceport.sys). Dynamic disks operate at a higher level: virtual volume objects are exposed to the 
system (meaning that user mode applications can still access the original disks). The volume manager 
is the component responsible for creating the single volume composed of multiple dynamic volumes. 
The Storage Spaces driver is a filter driver (a full filter driver rather than a minifilter) that lies between 
the partition manager (Partmgr.sys) and the disk class driver.
Storage Spaces architecture is shown in Figure 11-95 and is composed mainly of two parts: a 
platform-independent library, which implements the Spaces core, and an environment part, which 
is platform-dependent and links the Spaces core to the current environment. The Environment layer 
provides to Storage Spaces the basic core functionalities that are implemented in different ways based 
on the platform on which they run (because storage spaces can be used as bootable entities, the 
Windows boot loader and boot manager need to know how to parse storage spaces, hence the need 
for both a UEFI and Windows implementation). The core basic functionality includes memory manage-
ment routines (alloc, free, lock, unlock and so on), device I/O routines (Control, Pnp, Read, and Write), 
and synchronization methods. These functions are generally wrappers to specific system routines. For 
example, the read service, on Windows platforms, is implemented by creating an IRP of type IRP_MJ_
READ and by sending it to the correct disk driver, while, on UEFI environments, its implemented by 
using the BLOCK_IO_PROTOCOL.
Spaceport.sys
Storage Spaces
Core Library
Core
Store
Metadata
I/O
Memory management
Device I/O
Synchronization
Storage
Spaces
Environment
part
FIGURE 11-95 Storage Spaces architecture.
772
CHAPTER 11
Caching and file systems
Other than the boot and Windows kernel implementation, storage spaces must also be available 
during crash dumps, which is provided by the Spacedump.sys crash dump filter driver. Storage Spaces is 
even available as a user-mode library (Backspace.dll), which is compatible with legacy Windows operat-
ing systems that need to operate with virtual disks created by Spaces (especially the VHD file), and even 
as a UEFI DXE driver (HyperSpace.efi), which can be executed by the UEFI BIOS, in cases where even the 
EFI System Partition itself is present on a storage space entity. Some new Surface devices are sold with a 
large solid-state disk that is actually composed of two or more fast NVMe disks.
Spaces Core is implemented as a static library, which is platform-independent and is imported by 
all of the different environment layers. Is it composed of four layers: Core, Store, Metadata, and IO. 
The Core is the highest layer and implements all the services that Spaces provides. Store is the com-
ponent that reads and writes records that belong to the cluster database (created from the BluePrint 
file). Metadata interprets the binary records read by the Store and exposes the entire cluster database 
through different objects: Pool Drive Space Extent Column Tier and Metadata. The IO component, 
which is the lowest layer, can emit I/Os to the correct device in the cluster in the proper sequential way, 
thanks to data parsed by higher layers. 
Services provided by Spaces
Storage Spaces supports different disk type configurations. With Spaces, the user can create virtual 