sets is less than 5 bytes/event.
data shows that about 2M events are stored occupying about
24MB, and that the 781K nodes take up about 53B/node.
6.5.1 Effectiveness of Version Reduction Optimizations
Table 12 shows the number of node versions created with
the naive versioning algorithm and our optimized algorithms.
The second column shows that naive versioning leads to a
version explosion, with about 26 versions per node. However,
FD and SD drastically reduce the number versions: with FD,
we create just about 1.3 versions per node, on average.
Table 13 breaks out the effects of optimizations individ-
ually. Since some optimizations require other optimizations,
we show the four most meaningful combinations: (a) no
optimizations, (b) all optimizations except redundant node
(RNO), (c) all optimizations except cycle-collapsing (CCO),
and (d) all optimizations. These figures were computed in the
context of FD. When all optimizations other than RNO are en-
abled, the number of versions falls to about 3.6× from 25.6×
(unoptimized). Enabling all optimizations except CCO leads
to about 3 versions on average per node. Comparing these
with the last column, we can conclude that RNO contributes
about a 3× reduction and CCO a 2.4× reduction in the
number of versions, with the remaining 2.8× coming from
REO. It should be noted that REO and CCO both remove
versions as well as edges, whereas RNO removes only nodes.
6.6 Runtime Performance
All results in our entire evaluation were obtained on a
laptop with Intel Core i7 7500U running at 2.7GHz with
16GB RAM and 1TB SSD, running Ubuntu Linux. All
experiments were run on a single core.
Dataset
Linux Desktop
Windows Desktop
SSH/File Server
Web Server
Mail Server
Average
Versions per node
Naive
68.65
13.9
34.36
20.62
16.20
25.58
FD
1.05
1.37
1.31
1.29
1.32
1.26
SD
1.02
1.35
1.06
1.10
1.22
1.14
Table 12: Impact of naive and optimized versioning. Geometric means
are reported on the last row of the table.
Dataset
Linux Desktop
Windows Desktop
SSH/File Server
Web Server
Mail Server
Average
Versions per node
None No RNO No CCO
17.75
68.65
1.38
13.9
2.21
34.36
20.62
2.15
2.12
16.20
25.58
3.01
4.56
2.60
4.32
3.46
3.57
3.63
FD
1.05
1.37
1.31
1.29
1.32
1.26
Table 13: Effectiveness of different versioning optimizations. Geometric
means are reported on the last row of the table.
6.6.1 Dependence Graph Construction Time with FD
With our FD-preserving optimizations, this time depends
on (a) the size of cycles considered by CCO, and (b) the
maximum number of edges examined by REO. For (a), we
have not come across cycles involving more than two nodes
that meaningfully increased the size or runtime. So, our cur-
rent implementation only considers cycles of length two. To
evaluate the effect of (b), we placed a limit k, called the FD
window size, on the number of edges examined by REO be-
fore it reports that a dependence does not exist; this is safe but
may reduce the benefit. With this limit in place, each edge is
processed in at most O(k) time, yielding a graph construction
algorithm that is linear in the size of the input audit log.
Fig. 14 shows the dependence graph construction time as
a function of FD window size. We use the notation FD = c
to represent the runtime when k is set to c. We use k = 1
as the base, and show the other runtimes relative to this base.
Note that runtime can initially dip with increasing k because
it leads to significant reductions in memory use, which
translates into less pressure on the cache, and consequently,
(slightly) improved runtime. But as k is increased beyond
100, the runtime begins to increase noticeably.
The runtime and the reduction factor both increase
with window size. Fig. 15 plots the relationship between
reduction factor and window size. In particular, FD=1 means
that REO can eliminate the edge (u,v) only if the previous
edge coming into v is also from u. The average reduction
achieved by FD in this extreme case is 1.96, about the same
as the maximum rate achieved by LCD. Another observation
is that for the laboratory servers, with FD=25, we achieve
almost the full reduction potential of FD. For the desktop
systems used in the red team engagements, full potential is
Fig. 14: Dependence graph construction time with different FD window
sizes. Y-axis is the normalized runtime, relative to base of FD =1. These
base times are 77.54s for Linux desktop, 19.02s for Windows desktop,
11.86s for Web server, 15.11s for Mail server and 41.77s for SSH/File server.
1736    27th USENIX Security Symposium
USENIX Association
0	1	2	3	4	5	6	7	FD=1	FD=10	FD=25	FD=100	FD=500	FD=3000	FD=unlimited	Linux	Desktop	Windows	Desktop	SSH/File	Server	Web	Server	Mail	Server	Fig. 15: Effect of FD window size on event reduction factor.
achieved only at FD=500. We hypothesize that this is partly
due to the nature of red team exercises, and partly due to
workload differences between desktops and servers.
Comparing the two charts, we conclude that a range of
FD=25 to FD=100 represents a good trade-off for a real-time
detection and forensic analysis system such as SLEUTH
[10], with most of the size reduction benefits realized, and
with runtime almost the same as FD=1. At FD=25, our
implementation processes the 72M records in the Linux
Desktop data set in 84 seconds, corresponding to a rate of
860K events/second. For applications where log size is the
primary concern, FD=500 would be a better choice.
6.6.2 Dependence Graph Construction Time with SD
For SD, the sizes of Src sets become the key factor influenc-
ing runtime. SD requires frequent computation of set unions,
which takes linear time in the sizes of the sets. Moreover, in-
creased memory use (due to large sets) significantly increases
the pressure on the cache, leading to further performance
degradation. We therefore studied the effect of placing limits
on the maximum size of Src sets. Overflows past this limit
are treated conservatively, as described in Section 4.3.
Figs. 16 and 17 show the effect of varying the source set
size limit on the runtime and reduction factor, respectively.
Recall that SD runs on top of FD, so the runtime of FD
matters as well. However, since SD is significantly slower
than FD, we did not limit the FD window size in these
experiments. From the chart, the peak reduction factor is
reached by SD=500 for all data sets except Linux desktop.
The Linux desktop behaves differently, and we attribute this
to the much higher level of activity on it, which means that a
single long-running process can acquire a very large number
of source dependencies. Nevertheless, the chart suggests that
SD=500 is generally a good choice, as the overall runtime
is almost unchanged from SD=50.
At SD=500,
it takes 144 seconds to process 72M
records from Linux, for an event processing rate of about
500K/second. Thus, although SD is slower than FD, it is
quite fast in absolute terms, being able to process events
at least two orders of magnitude faster than the maximum
event production rate observed across all of our data sets.
Fig. 16: Dependence graph construction time with different source set size
limits. Y-axis is the runtime relative to the runtime with SD=50 (size limit
of 50), which is 143.68s for Linux desktop, 23.59s for Windows desktop,
12.86s for Web server, 15.43s for Mail server and 42.81s for SSH/File server.
Fig. 17: Effect of source set size limit on event reduction factor.
6.6.3 Backward and Forward Forensic Analysis
Once the dependence graph is constructed, forensic analysis
is very fast, because the whole graph currently resides in
memory. To evaluate the performance, we randomly tagged
100K nodes in the dependence graph for the Linux desktop
system. From each of these nodes, we performed
• a backward analysis to identify the source node closest
to the tagged node. This search used a shortest path
algorithm.
• a forward analysis to identify the nodes reachable from
the tagged node. In case of searches that could return
very large graphs, we terminated the search after finding
10K nodes (in most cases, the search terminated without
hitting this limit).
This entire test suite took 112 seconds to run. In other words,
each forward plus backward analysis on a dependence graph
corresponding to 72M events took just 1.12 milliseconds on
average.
6.7 Preserving Forensic Analysis Results
6.7.1 Reproducing Analysis Results from SLEUTH [10]
In our previous work [10], we performed real-time attack
detection and forensic analysis of multi-step APT-style attack
campaigns carried out in the 1st adversarial engagement in the
DARPA Transparent Computing program. As described in
Table 6 in [10], there were 8 distinct attack campaigns, each
of which involved most of the seven stages in APT life cycle,
including drop & load, intelligence gathering, backdoor
insertion, privilege escalation, data exfiltration, and cleanup.
USENIX Association
27th USENIX Security Symposium    1737
0	2	4	6	8	10	12	14	16	FD=1	FD=10	FD=25	FD=100	FD=500	FD=3000	FD=unlimited	Linux	Desktop	Windows	Desktop	SSH/File	Server	Web	Server	Mail	Server	0	0.5	1	1.5	2	2.5	SD=50	SD=100	SD=500	SD=1000	SD=3000	Linux	Desktop	Windows	Desktop	SSH/File	Server	Web	Server	Mail	Server	0	5	10	15	20	25	30	SD=50	SD=100	SD=500	SD=1000	SD=3000	Linux	Desktop	Windows	Desktop	SSH/File	server	Web	Server	Mail	Server	Dataset
Linux
Desktop
Windows
Desktop
Attack
Scenario
A
B
A
B
C
Analysis
Type
Backward
Forward
Backward
Forward
Backward
Forward
Backward
Forward
Backward
Forward
Number of Entities
FD SD
Naive
7
7
15
15
3
3
10
10
4
4
17
17
2
2
9
9
4
4
7
7
7
15
3
10
4
17
2
9
4
7
Table 18: Results of forward and backward analyses carried out from the en-
try and exit points of attacks used in the red team attacks. The exact same set
of entities were identified with and without the FD and SD event reductions.
SLEUTH assigns integrity and confidentiality tags to ob-
jects. These tags propagate as a result of read, write and
execute operations. It detects attacks using tag-based poli-
cies that were developed in the context of our earlier work
on whole-system integrity protection [19, 34, 35, 36] and
policy-based defenses [39, 32]. It then uses a backward anal-
ysis to identify the entry point, and then a forward analysis
to determine attack impact, and then a set of simplification
passes to generate a graph depicting the attack, and to list
the entities involved. Across these 8 attacks, a total of 176
entities were identified as relevant by the red team, and our
original analysis in [10] identified 174 of them.
We carried out the investigation again, with FD and SD
reductions in place. We were able to obtain the same results
as in [10], showing that FD and SD reductions do not affect
forensics results. This should come as no surprise, given that
we proved that they both preserve the results of backward
analysis followed by forward analysis. Nevertheless, the
experimental results are reassuring.
6.7.2 Forensic Analysis Results on Table 8 Data Set
We then turned our attention to the Engagement 2 data
set.
(We did not use Engagement 1 data set in our
reduction experiments because it was far smaller in size
than Engagement 2.) There were 2 attacks within the Linux
dataset and 3 attacks within the Windows data set. For each
attack, we ran a forward analysis from the attack entry point,
and then a backward analysis from attack exfiltration point
(which is one of the last steps in these attacks). As shown
Table 18, these analyses identified the exact same set of
entities, regardless of whether any data reduction was used.