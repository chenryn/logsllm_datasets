### 6.5.1 Effectiveness of Version Reduction Optimizations

Table 12 illustrates the number of node versions generated using both a naive versioning algorithm and our optimized algorithms. The second column demonstrates that naive versioning results in a significant version explosion, with approximately 26 versions per node. In contrast, our optimizations, specifically FD (Forward Dependency) and SD (Source Dependency), drastically reduce the number of versions. With FD, the average number of versions per node is reduced to about 1.3.

Table 13 provides a detailed breakdown of the individual effects of these optimizations. Since some optimizations are interdependent, we present four meaningful combinations:
- (a) No optimizations
- (b) All optimizations except Redundant Node Optimization (RNO)
- (c) All optimizations except Cycle-Collapsing Optimization (CCO)
- (d) All optimizations

These figures were computed in the context of FD. When all optimizations except RNO are enabled, the number of versions decreases by a factor of about 3.6× from the unoptimized 25.6×. Enabling all optimizations except CCO results in an average of 3 versions per node. Comparing these results, we can conclude that RNO contributes a 3× reduction, CCO a 2.4× reduction, and the remaining 2.8× reduction comes from REO (Redundant Edge Optimization). It is important to note that REO and CCO remove both versions and edges, while RNO only removes nodes.

### 6.6 Runtime Performance

All evaluation results were obtained on a laptop equipped with an Intel Core i7 7500U processor running at 2.7GHz, 16GB RAM, and a 1TB SSD, running Ubuntu Linux. All experiments were conducted on a single core.

| Dataset          | Naive Versions per Node | FD Versions per Node | SD Versions per Node |
|------------------|-------------------------|----------------------|----------------------|
| Linux Desktop    | 68.65                   | 1.05                 | 1.02                 |
| Windows Desktop  | 13.9                    | 1.37                 | 1.35                 |
| SSH/File Server  | 34.36                   | 1.31                 | 1.06                 |
| Web Server       | 20.62                   | 1.29                 | 1.10                 |
| Mail Server      | 16.20                   | 1.32                 | 1.22                 |
| **Average**      | **25.58**               | **1.26**             | **1.14**             |

**Table 12: Impact of naive and optimized versioning. Geometric means are reported in the last row.**

| Dataset          | None | No RNO | No CCO | FD |
|------------------|------|--------|--------|----|
| Linux Desktop    | 17.75 | 68.65  | 1.38   | 1.05 |
| Windows Desktop  | 68.65 | 13.9   | 2.21   | 1.37 |
| SSH/File Server  | 34.36 | 20.62  | 2.15   | 1.31 |
| Web Server       | 20.62 | 2.12   | 2.15   | 1.29 |
| Mail Server      | 16.20 | 25.58  | 3.01   | 1.32 |
| **Average**      | **25.58** | **3.63** | **3.57** | **1.26** |

**Table 13: Effectiveness of different versioning optimizations. Geometric means are reported in the last row.**

### 6.6.1 Dependence Graph Construction Time with FD

With our FD-preserving optimizations, the construction time depends on:
- (a) The size of cycles considered by CCO.
- (b) The maximum number of edges examined by REO.

In our current implementation, we only consider cycles of length two, as longer cycles do not significantly impact the size or runtime. To evaluate the effect of (b), we set a limit \( k \), called the FD window size, on the number of edges examined by REO before it reports that a dependence does not exist. This is safe but may reduce the benefit. With this limit, each edge is processed in \( O(k) \) time, resulting in a graph construction algorithm that is linear in the size of the input audit log.

Figure 14 shows the dependence graph construction time as a function of FD window size. We use the notation FD = c to represent the runtime when \( k \) is set to \( c \). We use \( k = 1 \) as the base, and show the other runtimes relative to this base. Note that runtime can initially decrease with increasing \( k \) due to significant reductions in memory use, which reduces cache pressure and improves runtime. However, beyond \( k = 100 \), the runtime begins to increase noticeably.

The runtime and reduction factor both increase with window size. Figure 15 plots the relationship between reduction factor and window size. For example, FD=1 means that REO can eliminate the edge (u, v) only if the previous edge coming into v is also from u. The average reduction achieved by FD in this case is 1.96, similar to the maximum rate achieved by LCD. For laboratory servers, FD=25 achieves almost the full reduction potential. For desktop systems, the full potential is achieved at FD=500.

Comparing the two charts, we conclude that a range of FD=25 to FD=100 represents a good trade-off for real-time detection and forensic analysis systems like SLEUTH [10], realizing most of the size reduction benefits with minimal impact on runtime. At FD=25, our implementation processes 72M records in 84 seconds, corresponding to a rate of 860K events/second. For applications where log size is the primary concern, FD=500 would be a better choice.

### 6.6.2 Dependence Graph Construction Time with SD

For SD, the sizes of Src sets become the key factor influencing runtime. SD requires frequent computation of set unions, which takes linear time in the sizes of the sets. Increased memory use (due to large sets) significantly increases cache pressure, leading to performance degradation. We studied the effect of placing limits on the maximum size of Src sets. Overflows past this limit are treated conservatively, as described in Section 4.3.

Figures 16 and 17 show the effect of varying the source set size limit on the runtime and reduction factor, respectively. Recall that SD runs on top of FD, so the runtime of FD matters as well. However, since SD is significantly slower than FD, we did not limit the FD window size in these experiments. The peak reduction factor is reached by SD=500 for all data sets except Linux desktop. The Linux desktop behaves differently due to higher activity levels, but SD=500 is generally a good choice, as the overall runtime is almost unchanged from SD=50.

At SD=500, it takes 144 seconds to process 72M records from Linux, for an event processing rate of about 500K/second. Although SD is slower than FD, it is still quite fast, processing events at least two orders of magnitude faster than the maximum event production rate observed across all data sets.

### 6.6.3 Backward and Forward Forensic Analysis

Once the dependence graph is constructed, forensic analysis is very fast, as the entire graph resides in memory. To evaluate performance, we randomly tagged 100K nodes in the dependence graph for the Linux desktop system. From each of these nodes, we performed:
- A backward analysis to identify the source node closest to the tagged node using a shortest path algorithm.
- A forward analysis to identify the nodes reachable from the tagged node, terminating the search after finding 10K nodes (in most cases, the search terminated without hitting this limit).

This entire test suite took 112 seconds to run. On average, each forward plus backward analysis on a dependence graph corresponding to 72M events took just 1.12 milliseconds.

### 6.7 Preserving Forensic Analysis Results

#### 6.7.1 Reproducing Analysis Results from SLEUTH [10]

In our previous work [10], we performed real-time attack detection and forensic analysis of multi-step APT-style attack campaigns carried out in the 1st adversarial engagement in the DARPA Transparent Computing program. As described in Table 6 in [10], there were 8 distinct attack campaigns, each involving most of the seven stages in the APT life cycle, including drop & load, intelligence gathering, backdoor insertion, privilege escalation, data exfiltration, and cleanup.

SLEUTH assigns integrity and confidentiality tags to objects, which propagate through read, write, and execute operations. It detects attacks using tag-based policies developed in the context of our earlier work on whole-system integrity protection [19, 34, 35, 36] and policy-based defenses [39, 32]. It then uses a backward analysis to identify the entry point, a forward analysis to determine attack impact, and simplification passes to generate a graph depicting the attack and list the entities involved. Across these 8 attacks, a total of 176 entities were identified as relevant by the red team, and our original analysis in [10] identified 174 of them.

We repeated the investigation with FD and SD reductions in place and obtained the same results, demonstrating that these reductions do not affect forensic results. This is consistent with our theoretical proof that they preserve the results of backward and forward analyses.

#### 6.7.2 Forensic Analysis Results on Table 8 Data Set

We then focused on the Engagement 2 data set. There were 2 attacks within the Linux dataset and 3 attacks within the Windows data set. For each attack, we ran a forward analysis from the attack entry point and a backward analysis from the attack exfiltration point. As shown in Table 18, these analyses identified the exact same set of entities, regardless of whether any data reduction was used.

| Dataset          | Attack Scenario | Analysis Type | Number of Entities |
|------------------|-----------------|---------------|--------------------|
| Linux Desktop    | A, B            | Backward, Forward | 7, 15, 3, 10, 4, 17, 2, 9, 4, 7 |
| Windows Desktop  | A, B, C         | Backward, Forward | 7, 15, 3, 10, 4, 17, 2, 9, 4, 7 |

**Table 18: Results of forward and backward analyses carried out from the entry and exit points of attacks used in the red team attacks. The exact same set of entities were identified with and without the FD and SD event reductions.**