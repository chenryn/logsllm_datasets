the methods of constructing an apparatus. 
To manage complexity and increase ease of construction, we are 
developing  an  apparatus  framework  centered  on  an  abstraction 
that  we  call  a  "container"  [21].  In  our  new  construction 
methodology,  a  container  is  the  fundamental  building  block.  A 
single  container  may  support  one  or  multiple  components 
(elements)  of  an  experimental  apparatus,  and  implements  an 
abstraction  layer  that  hides  the  details  of  the  inner  components, 
when  that  container  is  itself  placed  inside  another  container. 
Figure  2  shows  a  simple  container  that  contains  no  other 
containers, containing only 2 concrete computing elements, such 
as  a  VM  or  thread.  Abstraction  is  provided  by  the  container's 
communication  mechanism,  which  both  connects  the  contained 
elements  with  one  another,  and  also  presents  an  entry/exit  point 
for  communication 
the  communication 
mechanism  advertises  to  other  containers  the  properties  of  its 
container.  
the  container; 
into 
interface 
illustrated 
Figure 2: A simple container of two basic computing resources 
Abstraction  is  a  central  point  for  continuing  to  expand  the 
scalability  options  in  DeterLab.  Other  researchers  frequently 
create new techniques for scalable computing, or new methods of 
using  virtualization  or  simulation,  or  performing  a  form  of 
lightweight computation. Our goal going forward is to incorporate 
promising work in this area, defining a new abstract type of basic 
computing  element,  creating  a  standard 
for  a 
containerized  component  based  on  each  new  technology,  and 
expressing its tradeoffs explicitly, for use in construction tools. 
Thus far, our containers work has been successful for scalability 
for  multi-resolution  experiments,  and  has 
the 
requirements  for  integrating  container  technology  into  the  ELM 
workbench.  In  addition  to  the  expansion  of  available  basic 
computing  elements  described  above,  there  are  several  areas  of 
ongoing work. 
Apparatus  construction:  We  demonstrated  feasible  levels  of 
scalability  and  complexity  by  creating  several  experiment 
apparatus, the largest containing over 600 components that were 
then  realized  on  8  physical  computers  and  is  capable  of 
representing 50,000 computers at a coarse granularity in a specific 
scenario. The low end of this spectrum would be a modest sized 
mixed-resolution  experiment.  The  high  end  would  be  a  large 
experiment  in  which  all  but  a  few  elements  had  low  fidelity. 
However, construction involved manually matching each element 
in a desired network topology with a specific container. Clearly, 
there  is  promising  work  to  do  in  automating  this  process,  and 
integrating containers into the apparatus construction mechanism 
of our new workbench, ELM. 
Re-usability  and  embedding:  ELM  provides  the  ability  of 
experimenters  to  archive  experimental  apparatus  definitions,  or 
little 
to  automate  realization  with 
components  of  them,  and  for  other  experimenters  to  use  these 
archived  items  as  building  block  for  the  construction  of  a  new 
apparatus  definition.  The  definitions  can  then  be  used  with  core 
DeterLab  embedder  capabilities  for  realizing  the  definition  in  a 
real  apparatus  composed  of  DeterLab  network  and  computing 
resources.  Again,  there  is  work  to  do  building  the  workbench 
technology  for  containers  being  one  of  the  objects  that  can  be 
archived  and  reused.  Likewise,  there  is  work  to  do  with  the 
embedder, 
input  from 
experimenters,  while  also  giving  experimenters  visibility  on 
embedding so that they can vary some of resource utilization or 
vary the fidelity/scale tradeoff points for a specific apparatus. 
4.3  Model Based Experimentation 
As described in Section 3.5, DeterLab experimenters have a “big 
data” problem that will only grow as DeterLab magnifies the scale 
available to experimenters, and the breadth of tools for collecting 
experimental  data.  Our  approach  to  this  problem  has  been  to 
completely  re-conceive  the  methodology  for  how  cyber-security 
experiments are defined in order to yield data to be analyzed for 
information that comprises experimental results. 
The basis for this approach is no more or less than adopting basic 
ideas from other experimental sciences that are more mature than 
experimental cyber-security is at present. The conceptual starting 
point  of  an  experiment  is  a  real-world  situation  that  displays  an 
interesting  problem  that  is  inconvenient  to  investigate  in  situ  in 
the  real  world.  Instead,  we  define  a  conceptual  model  of  the 
situation, and begin to define laboratory activity that allows us to 
construct  in  the  lab  a  physical  (or  chemical,  or  biological,  or 
informatic) model of the real-world situation. Part of the function 
of this model is to serve as a design for an experimental apparatus 
that  the  experimenter  will  observe  or  modify  in  order  to  make 
inferences  from  lab  observations  to  the  real  world  where 
analogous modifications may create analogous results. 
This common methodological framework is, however, somewhat 
unusual for some areas of computer science, where much research 
in  situ  –  modify  an  actual  computational  or 
is 
communication  system  to  observe  whether  it  better  meets  the 
researcher’s  goal  for  speed,  efficiency,  data  access  rates,  power 
utilization,  etc.  Cyber-security  research,  however, 
is  very 
definitely  model-based.  The  real  world  has  a  host  of  large-scale 
systems with complex structures with vulnerabilities and potential 
mitigations. Where experimental modification of real systems (for 
example,  induced  cyber  attack)  is  not  feasible,  we  use  a  lab 
environment  to  create  a  model,  an  apparatus  to  approximate  the 
model, experimental procedures to observe or induce changes in 
the apparatus, and so on.  
However,  the  early  stage  use  of  the  DETER  testbed  was  not 
significantly  model-based.  Figure  3  is  somewhat  whimsical  but 
accurate  view  of  experimentation  in  which  modeling  is  entirely 
mental, with no externally visible relation between the model and 
an  ad  hoc  constructed  apparatus,  or  its  operation.  The  lab 
procedures are in fact quite valuable to the researcher, but ad hoc, 
and difficult to document or to be repeated by others. Nowhere is 
the ad hoc nature more evident in the research’s unique ability to 
pore over network traces to find variations in worm behavior that 
may be attributed to worm propagation countermeasures. 
in  fact 
Realization of Worm Experiment 
Victim 
Victim 
(Physical Host) 
Command & 
Control (VMs) 
Command & 
Control 
Network 
Ad Hoc Process: difficult to repeat 
Network 
(VMs) 
Figure 3: An informal experiment model leading to an ad-hoc 
experiment apparatus 
is 
techniques 
The DETER research program includes work to assist researchers 
in  defining  several  model-related  structures  that  become  part  of 
the  methodology  for  building  experimental  apparatus,  defining 
experimental  procedures,  and  analyzing  experimental  data  for 
expected  (or  unexpected)  patterns  or  changes  predicted  by  the 
model or a hypothesis derived from it. One purpose of modeling 
to  define  specific 
and  semantic  definition 
measurements and expectations to be sought for in the results of 
an experiment’s operation in the lab. 
This  model-based  approach 
requires  new  cyber-security 
methodology and new lab technology, integrated into the already-
described experiment lifecycle facilities, but oriented to defining 
semantics  for  an  experiment  and  its  results,  validating  an 
experimental  apparatus,  and  extracting  understanding  from 
results.  Several  types  of  tools  under  investigation  can  potential 
benefit: 
• 
semantic  mechanisms  to  capture  the  intent  of  the 
experimenter; 
support  for  monitoring  this  intent  and  distributed 
execution with breakpoints; 
• 
abstraction  and  modeling  techniques  for  experiment  design, 
realization, visualization, and analysis. 
The use of these tools is inherently iterative, shown in Figure 4. 
An experimenter defines a model, an apparatus to implement it, 
procedures to operate it; then runs the experiment by operating the 
apparatus,  executing  software  or  manual  steps  to  perform 
experimental  procedures;  the  resulting  data  is  interpreted  to 
extract  information,  which  can  then  be  used  to  iterate  on  the 
experimental apparatus, measurement scheme, or procedures. 
To  date,  most  experimentation  presumed  the  existence  of  some 
form of model of the system under test that the experimenter uses 
to  map  his  experiment  objectives  onto  an  apparatus  in  an 
experimental facility such as DeterLab. While this has often been 
true  for  the  low-abstraction-level  tasks  of  defining  network 
topologies and traffic generator configurations, the lack of rigor in 
the  initial  steps  often  undercut  the  efficacy  of  the  entire 
experimental  process  by  providing  little  guidance  or  expectation 
for the resulting experimental data, and no ability for knowledge-
based iteration. 
Figure 4: The iterative aspect of the experiment lifecycle 
As in any scientific discipline, often the greatest challenge lies in 
creating  an  appropriate  representation  of  the  object  for  study, 
representative  across  the  measurement  dimensions  that  matter, 
while  carefully  documenting  the  simplifying  assumptions  and 
abstractions that are made in the process.  While the most general 
case  of  this  problem  is  very  hard,  we  are  working  to  extend 
DeterLab’s  experimenter  support  back  into  the  early  reasoning 
process of experiment design. We approach this through a set of 
Model  Based  Scenario  development 
in  which 
experiments  are  constructed  from  a  basis  in  general  models  that 
capture  the  behavior  of  different  dimensions  of  cyber  security 
experiments.  
techniques, 
Figure 5: Development of experimental knowledge from 
experiment data 
Using  the  workbench  and  tools  that  we  are  investigating,  an 
experimenter  may  be  able  to  refine  the  models  into  apparatus 
templates  or  experiment-procedure  definition  or  recipes,  which 
can  be  used  to  plan  data  analysis.  The  analysis  would  not  be 
bottom  up  or  ad  hoc  pattern  based,  but  rather  following  a 
knowledge discovery procedure (shown in the middle of Figure 5) 
that  is  derived  from  the  model  and  its  various  components  and 
formalized assumptions such as expected behavioral invariants or 
functional constraints. In other words, we are working towards a 
shift  in  methodology  where  new  tools  assist  experimenters  in 
rigorous 
of 
semantically validated experiment design and execution. 
interpretation 
construction, 
execution, 
and 
run  produced  expected 
During  the  specification  phase  of  an  experiment,  invariants 
associated  with  the  model  will  be  used  to  verify  that  the 
experiment  being  developed  is  internally  consistent.  During 
execution,  invariants  will  be  used  to  ensure  that  the  intended 
experimental  semantics  are  realized  (validity  management). 
Finally, invariants will be invoked as part of the interpretation and 
visualization of results – providing methods for the experimenter 
to tackle large amounts of data in order to determine whether an 
experiment 
results  or  potentially 
interesting unexpected results. 
A simple example is the development of a model state space for 
execution of (and potential attack on) a communication protocol. 
A variety of data (packet dumps, web server logs, auth logs) can 
be normalized for input into analysis and visualization tools that 
assist the experimenter in mapping from actual events to expected 
behaviors.  Figure  6  shows  a  conceptual  view  of  the  model  state 
space,  with  various  possible  paths  through  it;  a  path  to  the 
“success” node would be expected results of experiment execution 
(visible  in  detail  in  event  logs),  while  other  paths  indicate  a 
violation of an assumption about correct behavior, which may be 
detectable  sign  of  an  attack  or  malfunction  (accompanied  by  a 
particular reason for the violation, attributable to event logs). 
Figure 6: An example of a semantic model for an experiment 
Model  based  experimentation  takes  on  an  increasing  importance 
when  designing  experiments  that  span  both  cyber  and  physical 
elements.  The physical components are likely based in some set 
of  models  (real  world,  empirical,  or  theoretical).    In  order  to 
capture  the  interactions  and  relations  between  the  cyber  and 
physical, it will be necessary to compose models.  Recent work in 
Secure Smart Grid Architectures [24] argues that: 
 “An  analysis  of  the  cyber-physical  security  of  a  smart 
grid  architecture  must  focus  on  the  impact  of  faults  and 
interactions  that  cross  domains  rather  than  the  localized 
response  that  might  be  seen  in  traditional  penetration 
testing.  This  requires  a  capability  to  model  large  scale 
response to cyber-attack, as well as to perform modeling 
or simulation of the physical components of a system.”   
The Smart Grid security team at USC-ISI and JPL are currently 
using DeterLab to develop a secure smart grid architecture.  They 
ave  created  a  taxonomy  of  cyber  and  physical  threats  and  are 
exploring federation of DeterLab with other labs and testbeds that 
provide physical simulation and emulation tools for modeling the 
systemic  response  of  the  grid.  Such  experiments  will  span 
multiple sites and will enable the use of specialized resources to 
participate in large-scale experiments.  
We view current research efforts such as the Smart Grid and other 
emerging cyber physical domains as new use cases for examining 
and  validating  the  evolving  features  and  capabilities  of  the 
DeterLab  that  we  are  developing  as  part  of  the  DETER  project 
research program.   
4.4  Additional Directions  
The three previous sections have outlined some of the key areas of 
our current research work, some of which was guided by lessons 
learned,  in  addition  to  the  results  of  our  own  research.  The 
research program also includes areas for future work, in which we 