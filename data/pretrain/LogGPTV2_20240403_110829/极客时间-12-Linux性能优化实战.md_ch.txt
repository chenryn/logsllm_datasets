# -i u10 表示每隔 10 微秒发送一个网络帧
# 注：如果你在实践过程中现象不明显，可以尝试把 10 调小，比如调成 5 甚至 1$ hping3 -S -p 80 -i u10 192.168.0.30现在，我们再回到第一个终端，你应该就会发现异常------系统的响应明显变慢了。我们不妨执行top，观察一下系统和进程的 CPU 使用情况：    $ toptop - 08:31:43 up 17 min,  1 user,  load average: 0.00, 0.00, 0.02Tasks: 128 total,   1 running,  69 sleeping,   0 stopped,   0 zombie%Cpu0  :  0.3 us,  0.3 sy,  0.0 ni, 66.8 id,  0.3 wa,  0.0 hi, 32.4 si,  0.0 st%Cpu1  :  0.0 us,  0.3 sy,  0.0 ni, 65.2 id,  0.0 wa,  0.0 hi, 34.5 si,  0.0 stKiB Mem :  8167040 total,  7234236 free,   358976 used,   573828 buff/cacheKiB Swap:        0 total,        0 free,        0 used.  7560460 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND    9 root      20   0       0      0      0 S   7.0  0.0   0:00.48 ksoftirqd/0   18 root      20   0       0      0      0 S   6.9  0.0   0:00.56 ksoftirqd/1 2489 root      20   0  876896  38408  21520 S   0.3  0.5   0:01.50 docker-containe 3008 root      20   0   44536   3936   3304 R   0.3  0.0   0:00.09 top    1 root      20   0   78116   9000   6432 S   0.0  0.1   0:11.77 systemd ...从 top 的输出中，你可以看到，两个 CPU 的软中断使用率都超过了 30%；而 CPU使用率最高的进程，正好是软中断内核线程 ksoftirqd/0 和 ksoftirqd/1。虽然，我们已经知道了 ksoftirqd的基本功能，可以猜测是因为大量网络收发，引起了 CPU使用率升高；但它到底在执行什么逻辑，我们却并不知道。对于普通进程，我们要观察其行为有很多方法，比如 strace、pstack、lsof等等。但这些工具并不适合内核线程，比如，如果你用 pstack ，或者通过/proc/pid/stack 查看 ksoftirqd/0（进程号为9）的调用栈时，分别可以得到以下输出：    $ pstack 9Could not attach to target 9: Operation not permitted.detach: No such process    $ cat /proc/9/stack[] smpboot_thread_fn+0x166/0x170[] kthread+0x121/0x140[] ret_from_fork+0x35/0x40[] 0xffffffffffffffff显然，pstack 报出的是不允许挂载进程的错误；而 /proc/9/stack方式虽然有输出，但输出中并没有详细的调用栈情况。那还有没有其他方法，来观察内核线程 ksoftirqd 的行为呢？既然是内核线程，自然应该用到内核中提供的机制。回顾一下我们之前用过的 CPU性能工具，我想你肯定还记得 perf ，这个内核自带的性能剖析工具。perf可以对指定的进程或者事件进行采样，并且还可以用调用栈的形式，输出整个调用链上的汇总信息。我们不妨就用 perf ，来试着分析一下进程号为 9 的 ksoftirqd。继续在终端一中，执行下面的 perf record 命令；并指定进程号 9 ，以便记录ksoftirqd 的行为:    
# 采样 30s 后退出$ perf record -a -g -p 9 -- sleep 30稍等一会儿，在上述命令结束后，继续执行 `perf report`命令，你就可以得到perf 的汇总报告。按上下方向键以及回车键，展开比例最高的 ksoftirqd后，你就可以得到下面这个调用关系链图：![](Images/b0901d6304cc0709f9e2093755b7c95d.png){savepage-src="https://static001.geekbang.org/resource/image/73/01/73f5e9a9e510b9f3bf634c5e94e67801.png"}从这个图中，你可以清楚看到 ksoftirqd执行最多的调用过程。虽然你可能不太熟悉内核源码，但通过这些函数，我们可以大致看出它的调用栈过程。-   net_rx_action 和 netif_receive_skb，表明这是接收网络包（rx 表示    receive）。-   br_handle_frame ，表明网络包经过了网桥（br 表示 bridge）。-   br_nf_pre_routing ，表明在网桥上执行了 netfilter 的 PREROUTING（nf    表示 netfilter）。而我们已经知道 PREROUTING 主要用来执行    DNAT，所以可以猜测这里有 DNAT 发生。-   br_pass_frame_up，表明网桥处理后，再交给桥接的其他桥接网卡进一步处理。比如，在新的网卡上接收网络包、执行    netfilter 过滤规则等等。我们的猜测对不对呢？实际上，我们案例最开始用 Docker 启动了容器，而Docker 会自动为容器创建虚拟网卡、桥接到 docker0 网桥并配置 NAT规则。这一过程，如下图所示：![](Images/e18dc46a8269848228bb49e2b1173561.png){savepage-src="https://static001.geekbang.org/resource/image/72/70/72f2f1fa7a464e4465108d4eadcc1b70.png"}当然了，前面 perf report界面的调用链还可以继续展开。但很不幸，我的屏幕不够大，如果展开更多的层级，最后几个层级会超出屏幕范围。这样，即使我们能看到大部分的调用过程，却也不能说明后面层级就没问题。那么，有没有更好的方法，来查看整个调用栈的信息呢？
## 火焰图针对 perf 汇总数据的展示问题，Brendan Gragg发明了[火焰图](http://www.brendangregg.com/flamegraphs.html)，通过矢量图的形式，更直观展示汇总结果。下图就是一个针对mysql 的火焰图示例。![](Images/9b28cd55c809afc13a0a1de64eccc9ea.png){savepage-src="https://static001.geekbang.org/resource/image/68/61/68b80d299b23b0cee518001f78960f61.png"}（图片来自 Brendan Gregg博客](http://www.brendangregg.com/flamegraphs.html)）这张图看起来像是跳动的火焰，因此也就被称为火焰图。要理解火焰图，我们最重要的是区分清楚横轴和纵轴的含义。-   **横轴表示采样数和采样比例**。一个函数占用的横轴越宽，就代表它的执行时间越长。同一层的多个函数，则是按照字母来排序。-   **纵轴表示调用栈**，由下往上根据调用关系逐个展开。换句话说，上下相邻的两个函数中，下面的函数，是上面函数的父函数。这样，调用栈越深，纵轴就越高。另外，要注意图中的颜色，并没有特殊含义，只是用来区分不同的函数。火焰图是动态的矢量图格式，所以它还支持一些动态特性。比如，鼠标悬停到某个函数上时，就会自动显示这个函数的采样数和采样比例。而当你用鼠标点击函数时，火焰图就会把该层及其上的各层放大，方便你观察这些处于火焰图顶部的调用栈的细节。上面 mysql 火焰图的示例，就表示了 CPU 的繁忙情况，这种火焰图也被称为on-CPU火焰图。如果我们根据性能分析的目标来划分，火焰图可以分为下面这几种。-   **on-CPU 火焰图**：表示 CPU 的繁忙情况，用在 CPU    使用率比较高的场景中。-   **off-CPU 火焰图**：表示 CPU 等待 I/O、锁等各种资源的阻塞情况。-   **内存火焰图**：表示内存的分配和释放情况。-   **热 / 冷火焰图**：表示将 on-CPU 和 off-CPU 结合在一起综合展示。-   **差分火焰图**：表示两个火焰图的差分情况，红色表示增长，蓝色表示衰减。差分火焰图常用来比较不同场景和不同时期的火焰图，以便分析系统变化前后对性能的影响情况。了解了火焰图的含义和查看方法后，接下来，我们再回到案例，运用火焰图来观察刚才perf record 得到的记录。
## 火焰图分析首先，我们需要生成火焰图。我们先下载几个能从 perf record记录生成火焰图的工具，这些工具都放在上面。你可以执行下面的命令来下载：    $ git clone https://github.com/brendangregg/FlameGraph$ cd FlameGraph安装好工具后，要生成火焰图，其实主要需要三个步骤：1.  执行 perf script ，将 perf record 的记录转换成可读的采样记录；2.  执行 stackcollapse-perf.pl 脚本，合并调用栈信息；3.  执行 flamegraph.pl 脚本，生成火焰图。不过，在 Linux中，我们可以使用管道，来简化这三个步骤的执行过程。假设刚才用 perf record生成的文件路径为/root/perf.data，执行下面的命令，你就可以直接生成火焰图：    $ perf script -i /root/perf.data | ./stackcollapse-perf.pl --all |  ./flamegraph.pl > ksoftirqd.svg执行成功后，使用浏览器打开 ksoftirqd.svg，你就可以看到生成的火焰图了。如下图所示：![](Images/21cf81bc135a0276d9f23c778f1e69d3.png){savepage-src="https://static001.geekbang.org/resource/image/6d/cd/6d4f1fece12407906aacedf5078e53cd.png"}根据刚刚讲过的火焰图原理，这个图应该从下往上看，沿着调用栈中最宽的函数来分析执行次数最多的函数。这儿看到的结果，其实跟刚才的perf report类似，但直观了很多，中间这一团火，很明显就是最需要我们关注的地方。我们顺着调用栈由下往上看（顺着图中蓝色箭头），就可以得到跟刚才 perfreport 中一样的结果：-   最开始，还是 net_rx_action 到 netif_receive_skb 处理网络收包；-   然后， br_handle_frame 到 br_nf_pre_routing ，在网桥中接收并执行    netfilter 钩子函数；-   再向上， br_pass_frame_up 到 netif_receive_skb    ，从网桥转到其他网络设备又一次接收。不过最后，到了 ip_forward 这里，已经看不清函数名称了。所以我们需要点击ip_forward，展开最上面这一块调用栈：![](Images/cb8b6519a17749d56081748304c48db3.png){savepage-src="https://static001.geekbang.org/resource/image/41/a3/416291ba2f9c039a0507f913572a21a3.png"}这样，就可以进一步看到 ip_forward后的行为，也就是把网络包发送出去。根据这个调用过程，再结合我们前面学习的网络收发和TCP/IP 协议栈原理，这个流程中的网络接收、网桥以及 netfilter调用等，都是导致软中断 CPU升高的重要因素，也就是影响网络性能的潜在瓶颈。不过，回想一下网络收发的流程，你可能会觉得它缺了好多步骤。比如，这个堆栈中并没有 TCP 相关的调用，也没有连接跟踪 conntrack相关的函数。实际上，这些流程都在其他更小的火焰中，你可以点击上图左上角的"ResetZoom"，回到完整火焰图中，再去查看其他小火焰的堆栈。所以，在理解这个调用栈时要注意。从任何一个点出发、纵向来看的整个调用栈，其实只是最顶端那一个函数的调用堆栈，而非完整的内核网络执行流程。另外，整个火焰图不包含任何时间的因素，所以并不能看出横向各个函数的执行次序。到这里，我们就找出了内核线程 ksoftirqd执行最频繁的函数调用堆栈，而这个堆栈中的各层级函数，就是潜在的性能瓶颈来源。这样，后面想要进一步分析、优化时，也就有了根据。
## 小结今天这个案例，你可能会觉得比较熟悉。实际上，这个案例，正是我们专栏 CPU模块中的 [软中断案例](https://time.geekbang.org/column/article/72147)。当时，我们从软中断 CPU使用率的角度入手，用网络抓包的方法找出了瓶颈来源，确认是测试机器发送的大量SYN 包导致的。而通过今天的 perf和火焰图方法，我们进一步找出了软中断内核线程的热点函数，其实也就找出了潜在的瓶颈和优化方向。其实，如果遇到的是内核线程的资源使用异常，很多常用的进程级性能工具并不能帮上忙。这时，你就可以用内核自带的perf 来观察它们的行为，找出热点函数，进一步定位性能瓶。当然，perf产生的汇总报告并不够直观，所以我也推荐你用火焰图来协助排查。实际上，火焰图方法同样适用于普通进程。比如，在分析 Nginx、MySQL等各种应用场景的性能问题时，火焰图也能帮你更快定位热点函数，找出潜在性能问题。
## 思考最后，我想邀请你一起来聊聊，你碰到过的内核线程性能问题。你是怎么分析它们的根源？又是怎么解决的？你可以结合我的讲述，总结自己的思路。欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。![](Images/f3ab291e71ad0a9d7fe2c894ccb9706a.png){savepage-src="https://static001.geekbang.org/resource/image/a3/e9/a396aed4116c2c989771c1295736abe9.jpg"}
# 50 \| 案例篇：动态追踪怎么用？（上）你好，我是倪朋飞。上一节，我以 ksoftirqd CPU 使用率高的问题为例，带你一起学习了内核线程CPU 使用率高时的分析方法。先简单回顾一下。当碰到内核线程的资源使用异常时，很多常用的进程级性能工具，并不能直接用到内核线程上。这时，我们就可以使用内核自带的perf 来观察它们的行为，找出热点函数，进一步定位性能瓶颈。不过，perf产生的汇总报告并不直观，所以我通常也推荐用火焰图来协助排查。其实，使用 perf对系统内核线程进行分析时，内核线程依然还在正常运行中，所以这种方法也被称为动态追踪技术。**动态追踪技术，通过探针机制，来采集内核或者应用程序的运行信息，从而可以不用修改内核和应用程序的代码，就获得丰富的信息，帮你分析、定位想要排查的问题。**以往，在排查和调试性能问题时，我们往往需要先为应用程序设置一系列的断点（比如使用GDB），然后以手动或者脚本（比如 GDB 的 Python扩展）的方式，在这些断点处分析应用程序的状态。或者，增加一系列的日志，从日志中寻找线索。不过，断点往往会中断应用的正常运行；而增加新的日志，往往需要重新编译和部署。这些方法虽然在今天依然广泛使用，但在排查复杂的性能问题时，往往耗时耗力，更会对应用的正常运行造成巨大影响。``{=html}此外，这类方式还有大量的性能问题。比如，出现的概率小，只有线上环境才能碰到。这种难以复现的问题，亦是一个巨大挑战。而动态追踪技术的出现，就为这些问题提供了完美的方案：它既不需要停止服务，也不需要修改应用程序的代码；所有一切还按照原来的方式正常运行时，就可以帮你分析出问题的根源。同时，相比以往的进程级跟踪方法（比如ptrace），动态追踪往往只会带来很小的性能损耗（通常在 5% 或者更少）。既然动态追踪有这么多好处，那么，都有哪些动态追踪的方法，又该如何使用这些动态追踪方法呢？今天，我就带你一起来看看这个问题。由于动态追踪涉及的知识比较多，我将分为上、下两篇为你讲解，先来看今天这部分内容。
## 动态追踪说到动态追踪（Dynamic Tracing），就不得不提源于 Solaris 系统的DTrace。DTrace是动态追踪技术的鼻祖，它提供了一个通用的观测框架，并可以使用 D语言进行自由扩展。DTrace 的工作原理如下图所示。**它的运行常驻在内核中，用户可以通过 dtrace命令，把 D 语言编写的追踪脚本，提交到内核中的运行时来执行**。DTrace可以跟踪用户态和内核态的所有事件，并通过一些列的优化措施，保证最小的性能开销。![](Images/7ac9d9c01adc2f7cfa8132fc14dc5d6f.png){savepage-src="https://static001.geekbang.org/resource/image/61/a6/6144b1947373bd5668010502bd0e45a6.png"}（图片来自BSDCan](https://www.bsdcan.org/2017/schedule/attachments/433_dtrace_internals.html#(24))）虽然直到今天，DTrace 本身依然无法在 Linux 中运行，但它同样对 Linux动态追踪产生了巨大的影响。很多工程师都尝试过把 DTrace 移植到 Linux中，这其中，最著名的就是 RedHat 主推的 SystemTap。同 DTrace 一样，SystemTap也定义了一种类似的脚本语言，方便用户根据需要自由扩展。不过，不同于DTrace，SystemTap并没有常驻内核的运行时，它需要先把脚本编译为内核模块，然后再插入到内核中执行。这也导致SystemTap 启动比较缓慢，并且依赖于完整的调试符号表。![](Images/664373d1f400286f4d622b6689c5b4e6.png){savepage-src="https://static001.geekbang.org/resource/image/e0/db/e09aa4a00aee93f27f0d666a2bb1c4db.png"}（图片来自[动态追踪技术漫谈](https://openresty.org/posts/dynamic-tracing/)）总的来说，为了追踪内核或用户空间的事件，Dtrace 和 SystemTap都会把用户传入的追踪处理函数（一般称为Action），关联到被称为探针的检测点上。这些探针，实际上也就是各种动态追踪技术所依赖的事件源。
### 动态追踪的事件源根据事件类型的不同，**动态追踪所使用的事件源，可以分为静态探针、动态探针以及硬件事件等三类**。它们的关系如下图所示：![](Images/e5a0742e24f8db3bdac0c1831a8a6d12.png){savepage-src="https://static001.geekbang.org/resource/image/ba/61/ba6c9ed0dcccc7f4f46bb19c69946e61.png"}（图片来自 [Brendan GreggBlog](http://www.brendangregg.com/perf.html#Events)）其中，**硬件事件通常由性能监控计数器 PMC（Performance MonitoringCounter）产生**，包括了各种硬件的性能情况，比如 CPU的缓存、指令周期、分支预测等等。**静态探针，是指事先在代码中定义好，并编译到应用程序或者内核中的探针**。这些探针只有在开启探测功能时，才会被执行到；未开启时并不会执行。常见的静态探针包括内核中的跟踪点（tracepoints）和USDT（Userland Statically Defined Tracing）探针。-   跟踪点（tracepoints），实际上就是在源码中插入的一些带有控制条件的探测点，这些探测点允许事后再添加处理函数。比如在内核中，最常见的静态跟踪方法就是    printk，即输出日志。Linux    内核定义了大量的跟踪点，可以通过内核编译选项，来开启或者关闭。-   USDT 探针，全称是用户级静态定义跟踪，需要在源码中插入 DTRACE_PROBE()    代码，并编译到应用程序中。不过，也有很多应用程序内置了 USDT    探针，比如 MySQL、PostgreSQL 等。**动态探针，则是指没有事先在代码中定义，但却可以在运行时动态添加的探针**，比如函数的调用和返回等。动态探针支持按需在内核或者应用程序中添加探测点，具有更高的灵活性。常见的动态探针有两种，即用于内核态的kprobes 和用于用户态的 uprobes。-   kprobes 用来跟踪内核态的函数，包括用于函数调用的 kprobe    和用于函数返回的 kretprobe。-   uprobes 用来跟踪用户态的函数，包括用于函数调用的 uprobe    和用于函数返回的 uretprobe。> 注意，kprobes 需要内核编译时开启 CONFIG_KPROBE_EVENTS；而 uprobes> 则需要内核编译时开启 CONFIG_UPROBE_EVENTS。