title:On Information-theoretic Measures for Quantifying Privacy Protection
of Time-series Data
author:Chris Y. T. Ma and
David K. Y. Yau
On Information-theoretic Measures for Quantifying
Privacy Protection of Time-series Data
Chris Y.T. Ma
Advanced Digital Sciences Center
Singapore
PI:EMAIL
David K.Y. Yau
Singapore University of Technology and Design
Advanced Digital Sciences Center
Singapore
PI:EMAIL
ABSTRACT
Privacy protection of time-series data, such as traces of
household electricity usage reported by smart meters, is of
much practical importance. Solutions are available to im-
prove data privacy by perturbing clear traces to produce
noisy versions visible to adversaries, e.g., in battery-based
load hiding (BLH) against non-intrusive load monitoring
(NILM). A foundational task for research progress in the
area is the de(cid:12)nition of privacy measures that can truly
evaluate the eﬀectiveness of proposed protection methods.
It is a diﬃcult problem since resilience against any attack
algorithms known to the designer is inconclusive, given that
adversaries could discover or indeed already know stronger
algorithms for attacks. A more basic measure is information-
theoretic in nature, which quanti(cid:12)es the inherent informa-
tion available for exploitation by an adversary, independent
of how the adversary exploits it or indeed any assumed com-
putational limitations of the adversary.
In this paper, we
analyze information-theoretic measures for privacy protec-
tion and apply them to several existing protection methods
against NILM. We argue that although these measures ab-
stract away the details of attacks, the kind of information
the adversary considers plays a key role in the evaluation,
and that a new measure of oﬄine conditional entropy is bet-
ter suited for evaluating the privacy of perturbed real-world
time-series data, compared with other existing measures.
Categories and Subject Descriptors
E.3 [Data]: Data Encryption
Keywords
Privacy Protection; Privacy Measure; Conditional Entropy;
Correlated Time-series
1.
INTRODUCTION
Privacy protection of time-series data is of much practical
importance. Such data includes mobility traces in location-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS’15, April 14–17, 2015, Singapore.
Copyright c⃝ 2015 ACM 978-1-4503-3245-3/15/04 ...$15.00
http://dx.doi.org/10.1145/2714576.2714577.
based services [40], or household electricity consumption
measured by smart meters for demand response [7]. If users’
privacy is not assured in these applications, they will not opt
in as participants. Or if they do, then should the leak of sen-
sitive information actually happen, damaging outcomes may
ensue such as abuse of consumer rights or lawsuits [32, 47].
In view of the privacy problem, solutions are available
that perturb a clear trace of time-series data to produce a
noisy version, and make sure that any would-be adversaries
will have access to the noisy version only [22]. The extent
of perturbation may be constrained by the capability of the
defense mechanism. This is the case, for example, in battery-
based load hiding (BLH) [21, 23, 27, 46] of smart meter
reports against non-intrusive load monitoring (NILM) [4, 11,
31, 43], whose privacy concerns are well documented [32, 47].
In other situations, the perturbation is constrained by the
need to preserve enough accuracy of the data for various
application needs. This is the case, for example, in infer-
ring congestion index of roads as a location-based service
(LBS) [8]. When the accuracy of positioning is low, conges-
tion may be inferred at a wrong location.
While the design of speci(cid:12)c privacy protection is inter-
esting, a foundational task for advances in the area is the
de(cid:12)nition of proper measures that can truly assess the pri-
vacy of the protection. Some measures focus on protect-
ing the identity of a participant without considering the in-
formation content of the corresponding data. One exam-
ple is k-anonymity [41], which although widely employed,
is unsatisfactory in various semantic aspects. For example,
k-anonymity ignores the diversity of data [26], i.e., hiding
a data point among k other identical data points is of du-
bious usefulness. Some patches, such as l-diversity [26] and
m-invariance [45], have been applied to k-anonymity to ad-
dress the problems.
An alternative information-theoretic viewpoint is to mea-
sure directly the amount of original information that is con-
tained in the noisy data, i.e., the common information be-
tween them. The idea is that if the amount of private infor-
mation available for learning in the noisy data is small, then
the privacy loss will be bounded by that amount, indepen-
dent of how the adversary operates. Hence, if a protection
method satis(cid:12)es privacy in this information-theoretic sense,
the privacy is strong in that there is no danger that an attack
will be discovered to breach the protection. This property
is appealing compared with resilience proved against spe-
ci(cid:12)c attacks [48], since we cannot rule out the possibility
of stronger attacks. It is also appealing compared with re-
silience proved assuming computational limitations of the
adversary, such as the hardness of factoring the product of
two large prime numbers in RSA cryptography [38].
If elements in a data trace are uncorrelated temporally,
information-theoretic privacy can be given by the concept
of mutual information [2], denoted by I(X; Y ), where X
and Y are random variables corresponding to the clear and
noisy data distributions, respectively. Real-world time-series
exhibit obvious correlations in time, however. For example,
a household’s electricity consumption at time t is likely to
be similar to the consumption at t + (cid:14), for small (cid:14); a user’s
locations at t and t + (cid:14) are likely to be close as well.
In
general, adversaries will be able to exploit these correlations
in privacy attacks.
Hence, even though information-theoretic measures ab-
stract away the operational details of attacks, its use still
raises questions about the kinds of information the adver-
sary is assumed to use in the attacks. Prior work has taken
initial steps in accounting for information present in the
temporal correlations of data. Speci(cid:12)cally, Yang et al. con-
sider the mutual information between distributions of pairs
of consecutive data points as symbols [46]. Their construct
is Markov-like in that a data point is assumed to depend on
the immediately preceding data point only.
Typically, real-world data exhibits much more persistent
temporal correlation than Markov, i.e., the autocorrelation
decreases slowly with the time shift [10, Chapter 5]. In this
paper, we focus on the protection of smart meter readings
from a household that support emerging demand-response
smart grid applications. In this case, the readings collected
by a smart meter, as shown in Fig. 1, have long temporal
correlation. How this feature impacts privacy protection of
time-series data has not been studied in a systematic man-
ner. In this paper, we undertake such a study.
We investigate the following four sets of privacy measures
that subsume state-of-the-art measures used in prior work
to evaluate BLH or more general time-series data:
(cid:15) A baseline de(cid:12)nition of mutual information (MI) uses
individual elements in a time-series as symbols [2].
This baseline has been widely employed in privacy re-
search [33, 42, 44], but it does not account for the
data’s temporal correlation. A recent de(cid:12)nition of MI
considers two consecutive data points as symbols [46],
which provides an initial limited attempt to incorpo-
rate the eﬀects of correlated data. In this paper, our
goal is to characterize the temporal correlation com-
prehensively. Hence, we evaluate a general notion of
MI that uses length-k sequences of data points as sym-
bols, for any integer k > 0.
(cid:15) Koo et al. adapt the baseline MI to a normalized ver-
sion (NMI) [23]. The intent is to measure the fraction
of information that is available, rather than the abso-
lute amount. We evaluate this variational view also
for our parameterized MI measure above.
(cid:15) Conditional entropy (CE) has been proposed for online
prediction problems, e.g., optimal paging of a mobile
phone based on its location history [6]. The length of
the last k, k > 0, data points used as history to guide
the prediction is a parameter whose choice should de-
pend on the complexity (i.e., entropy) of the phone’s
movement patterns. We adapt this measure to privacy
protection. In this case, since we are interested in how
much the noisy trace tells us about the clear trace, our
new de(cid:12)nition of this measure conditions the entropy
of the clear data on the noisy data, which is diﬀerent
from the original de(cid:12)nition.
(cid:15) Beyond CE, we propose a new oﬄine conditional en-
tropy (OCE) measure to account for the full informa-
tion available to the adversary when he tries to com-
promise the privacy of some clear data points. Speci(cid:12)-
cally, the adversary may exploit any range of revealed
noisy data points as guiding information. Without
loss of generality, we measure the entropy of a clear
(\present") data point, conditional on the correspond-
ing noisy data point, as well as the preceding (\histor-
ical") k and subsequent (\future") k noisy data points,
where k (cid:21) 0.
We compare the four privacy measures over diﬀerent val-
ues of k, in order to expose the impact of the temporal cor-
relation. We use a balance of synthetic and real data to
ensure useful comparisons.
First, we postulate axiomatic properties of desirable pri-
vacy measures in their ability to bring out the true informa-
tion content of time-series data. We test the four proposed
privacy measures against the axiomatic properties, by sub-
jecting them to synthetic test data generated by data models
speci(cid:12)cally designed to ensure the information content. Our
major (cid:12)ndings are:
(cid:15) In summary, OCE appears to be superior in that it
satis(cid:12)es our axiomatic properties of desirable privacy
measures.
(cid:15) We use an automaton data model (Sec. 6.3) to show
that, because of the original prediction intent of condi-
tional entropy (CE), it fails to account for all available
information in a privacy attack. In contrast, the MI,
NMI, and OCE measures do not suﬀer from this lim-
itation, meaning that for a large enough k, they can
account for all information in the noisy trace that can
be exploited by the adversary.
(cid:15) Although the MI measure can encompass all the avail-
able information, its values across k are not compara-
ble. This is because, as k grows, MI grows inherently
due to the increased diversity of longer symbols, so
that the quanti(cid:12)cation of information loss occurs over a
moving basis. Moreover, using NMI as a relative infor-
mation measure does not solve the problem. In either
case, because of inherent increase of symbol diversity
with k, MI or NMI does not satisfy the axiomatic prop-
erty that it should stabilize at some k when applied to
time-series data generated by an order-l Markov model
(Sec. 6.4).
(cid:15) Unlike the MI and NMI measures, OCE always mea-
sures the uncertainty of clear data points conditional
on the noisy data. Its meaning is therefore not aﬀected
by k. Speci(cid:12)cally, when it is applied to the protection
of data where the relationship between the noisy data
points and the clear data point can be described by
an order-l Markov model (Sec. 6.3), it quanti(cid:12)es suc-
cessfully marginal gains in the adversary’s knowledge
as k grows, and stabilizes at the smallest k suﬃcient
to fully capture the order of information in the noisy
traces.
protected smart meter readings, either collected in the real
world or generated by realistic, validated smart-meter data
models. Our results quantify the importance of temporal
correlations that are present in real or realistic data traces.
Sec. 8 concludes.
2. PROBLEM DEFINITION
We consider the problem of quantifying the protection of
time-series data by diﬀerent privacy-preserving strategies.
This is an important problem because of existing and grow-
ing practice to collect time-series data for supporting appli-
cations such as smart-meter enabled demand-response and
location-based services for mobile nodes. To address clear
privacy concerns, diﬀerent perturbation strategies have been
proposed to cloak the time-series data before making it avail-
able to the service provider. A proper privacy measure to
quantify the true eﬀectiveness of the protection is crucial, to
avoid embarrassing scenarios of inadvertent data reidenti(cid:12)-
cation [41].
We let a trace of clear time-series data be a stochastic pro-
cess X = fXig, where i is the time index and the Xi’s form
a sequence of random variables, and let X l be the stochas-
tic process in which the symbols are the set of l consecutive
data points in X , l > 0. We let the corresponding noisy
time-series data generated by a privacy-preserving strategy
be a stochastic process Y = fYig, where i is the time index
and the Yi’s form a sequence of random variables, and let Y l
be the stochastic process in which the symbols are the set
of l consecutive data points in Y, l > 0. We wish to design
an information-theoretic measure to compare the similar-