title:An Online Approach to Estimate Parameters of Phase-Type Distributions
author:Peter Buchholz and
Iryna Dohndorf and
Jan Kriege
2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
An Online Approach to Estimate Parameters of
Phase-Type Distributions
Peter Buchholz, Iryna Dohndorf, and Jan Kriege
Department of Computer Science, TU Dortmund
{peter.buchholz,iryna.dohndorf,jan.kriege}@tu-dortmund.de
Abstract—The traditional expectation-maximization (EM) al-
gorithm is a general purpose algorithm for maximum likelihood
estimation in problems with incomplete data. Several variants
of the algorithm exist to estimate the parameters of phase-
type distributions (PHDs), a widely used class of distributions
in performance and dependability modeling. EM algorithms are
typical ofﬂine algorithms because they improve the likelihood
function by iteratively running through a ﬁxed sample. Nowadays
data can be generated online in most systems such that ofﬂine
algorithms seem to be outdated in this environment. This paper
proposes an online EM algorithm for parameter estimation of
PHDs. In contrast to the ofﬂine version, the online variant
adds data immediately when it becomes available and includes
no iteration. Different variants of the algorithms are proposed
that exploit the speciﬁc structure of subclasses of PHDs like
hyperexponential, hyper-Erlang or acyclic PHDs. The algorithm
furthermore incorporates current methods to detect drifts or
change points in a data stream and estimates a new PHD
whenever such a behavior has been identiﬁed. Thus, the resulting
distributions can be applied for online model prediction and
for the generation of inhomogeneous PHDs as an extension of
inhomogeneous Poisson processes. Numerical experiments with
artiﬁcial and measured data streams show the applicability of
the approach.
Index Terms—stochastic modeling, phase-type distributions,
expectation-maximization algorithm, online algorithm, parame-
ter estimation
I. INTRODUCTION
Analysis and validation of quantitative performance and
dependability measures in computer and communications sys-
tems is done by means of stochastic models. One of the key
aspects of stochastic modeling is the appropriate modeling of
input parameters using distributions or stochastic processes.
This is commonly denoted as input modeling in stochastic
simulation and analysis [1], [2]. In general input modeling is
based on a sample from a running system, the selection of an
appropriate type of the distribution or stochastic process and
the estimation of the parameters. Subsequently, stochastic tests
may be used to validate the generated model. For performance
and dependability modeling it is often advantageous if models
can be analyzed using analytical or numerical methods rather
than stochastic simulation. However, this usually implies that
the class of distributions is restricted to phase-type distribu-
tions (PHDs) [2]–[4]. PHDs are a versatile class of distribu-
tions which in principle allows one to approximate observed
behavior of real systems sufﬁciently accurate. However, the
estimation of the parameters of PHDs is complex and often
results in non-linear optimization problems.
Parameter estimation for PHDs is a research topic since
many years and a wide variety of different approaches has
been proposed. For a survey on available methods we refer to
[2]. The most general but also often very costly approach is
to maximize the likelihood function of a PHD with respect to
the measured data using an Expectation-Maximization (EM-)
algorithm [5], [6]. EM-algorithms for PHDs have been de-
veloped in various variants resulting nowadays in approaches
which can be used to accurately estimate the parameters of
PHDs with 10 or more phases to sample sizes of 105 or even
106. This is sufﬁcient for most applications.
The outlined approach of sampling data from a system and
subsequent estimation of distribution parameters is somehow
related to classical statistics and stochastic modeling. It is
driven by the assumption that the sampling of data is costly
and requires additional effort to measure ofﬂine. This is no
longer true nowadays where numerous sensors produce a
huge amount of data almost for free in an online approach.
Therefore huge samples are available that are permanently
extended by incoming information. Such a situation can be
found in data centers where incoming requests are monitored,
in production systems where sensors measure the state of every
stage or in communication networks where the user population
is monitored in real time, to mention only a few examples.
The standard approach of parameter estimation for PHDs
cannot be applied to online data because a ﬁxed sample is re-
quired. EM-algorithms determine the parameters by iteratively
passing through the data several times. This implies that a sub-
sample is taken from the available data and the parameters of
the PHD are estimated from this sub-sample. The resulting
stochastic model is then applied to analyze the system or to
predict the future behavior of the system by for example esti-
mating failure probabilities or optimal maintenance intervals.
This approach has two major drawbacks. First, sub-sampling
obviously neglects some of the available information. It is
unclear which part of the data should be chosen to estimate
parameters. Second, the approach cannot or can only slowly
react on changes in the behavior of a system. Since after the
sub-sample has been taken, parameters are estimated, the PHD
describes some behavior that has been observed in the past.
However, in many systems the behavior may change due to
some internal or external events and this change is not captured
in the uncertainty introduced by the stochasticity in the model.
Instead a new and more appropriate model should be built
whenever changes occur that modify system behavior over a
978-1-7281-0057-9/19/$31.00 ©2019 IEEE
DOI 10.1109/DSN.2019.00024
100
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:54:41 UTC from IEEE Xplore.  Restrictions apply. 
longer time interval.
To exploit the availability of online data, parameter estima-
tion has to be done online too. In this paper we present, to
the best of our knowledge for the ﬁrst time, an online EM-
algorithm to estimate the parameters of PHDs. The proposed
algorithm is based on available online algorithms for mixture
distributions [7], [8]. In contrast to these approaches, it is
not assumed that the observed samples are drawn from a
ﬁxed distribution and this distribution has to be estimated. We
assume that the observed system may change its behavior from
time to time and the proposed algorithm adopts these changes
by estimating new parameters whenever such changes occur.
Furthermore, the algorithm computes after an initial phase a
ﬁrst PHD and then improves the parameters using online data.
Both steps make it necessary to integrate statistical methods
for change point detection and model selection within the EM-
algorithm. These steps are not part of available online EM-
algorithms.
The paper is structured as follows. In the next section we
brieﬂy review related work. Afterwards, in Sect. III, basic
assumptions are introduced, the notation is deﬁned and some
deﬁnitions are established. Then, the online EM-algorithm for
PHDs is developed. In Sect. V, we embed the EM-algorithm
in an adaptive framework which determines different PHDs
from online data and selects the best PHD for the available
information. The complete algorithm is then evaluated empir-
ically using synthetically generated and real data. The paper
ends with the conclusions and an outlook on future research.
II. RELATED WORK AND NEW CONTRIBUTION
The approach developed in this paper is based on PHDs,
EM-algorithms and statistical methods to estimate change
points and to select models. This spectrum is much too wide
to mention all relevant publications. Thus, we only brieﬂy
review the most important references considering PHDs and
EM-algorithms for PHDs in some detail and naming only the
main references for the used statistical methods.
PHDs are based on the pioneering work of Neuts [3], [9]
and are used nowadays in stochastic models of various types
[2], [4], [10]. For a long time PHDs were restricted to simple
structures where parameters have been determined by ﬁtting
empirical moments of a low order. However, moments of
lower order neglect the structure of the probability density
and often result in a PHD which describes only a rough
approximation of the observed behavior. Much better are
parameter estimation techniques based on the maximization
of the likelihood function. Commonly the EM-algorithm is
used for this purpose. The ﬁrst EM-algorithm for PHDs has
been published by Asmussen and his coworkers [6]. This
algorithm has been subsequently modiﬁed and improved in
a large number of publications mainly by using more efﬁ-
cient methods to compute the integrals that appear in the
expectation step and by restricting PHDs to some subclass.
It seems that it is preferable to apply the EM-algorithm to the
subclass of acyclic PHDs which is powerful enough to model
interesting behaviors and allows a more efﬁcient realization
of EM algorithms. References that propose EM-algorithms
for PHDs are for example [11]–[16]. The EM-algorithm is
much more stable and efﬁcient if it is applied to mixture
distributions like hyperexponential [12] or hyper-Erlang [13]
or to acyclic PHDs in canonical form [14]. In the latter case
the canonical representation [17] avoids the possibility that
the EM-algorithm ﬂuctuates between different representations
of the same distribution. A canonical representation is not
available for general PHDs. Since the mentioned structures,
mixtures of Exponential- or Erlang-distributions and acyclic
PHDs are appropriate structures for an EM-algorithm, we use
these subclasses also in our approach.
All mentioned EM-algorithms for PHDs are ofﬂine algo-
rithms. They use a sample of ﬁxed length and iteratively
improve the parameters of the PHD until parameters are stable
or the available time budget is elapsed. Aside from the EM
algorithms further approaches for ofﬂine estimation like [18]
or [19] exist. Those approaches use derived measures like
moments, but also require the complete data set to compute the
measures. For general distributions, beyond PHDs, online EM-
algorithms have been proposed in [7], [8] and in [20] an online
EM-algorithm for mixtures of logistic regression models is
presented. These algorithms work on a continuous stream of
data and instead of iterating through the data, they process each
new data point once and weight data points with a decreasing
function. The ﬁnal goal is to estimate the parameters of a
model under the assumption of an identical distribution for
the observed data points. The online algorithm generates with
each new data point a new set of parameters. However, in
practice, the resulting distribution has to be used in a stochastic
model for analysis and forecasting and in parallel parameters
may be improved by gathering new information. In principle
two sets of parameters are available, one that is used in the
current model and one set which results from the most up-
to-date data. Parameters of distributions used in the stochastic
model are exchanged, if the most current parameter set does
signiﬁcantly better than the older one. If used adequately, this
approach allows one to react on changes of the behavior of
the observed system.
The integration of speciﬁcally tailored steps for acyclic
PHDs in online EM-algorithms and the adaptive generation
of PHDs based on online data are the new contributions of
the present paper.
The outlined approach generates a sequence of PHDs which
change at discrete time points. From a modeling perspective
the approach generates a model with time varying distri-
butions. In stochastic modeling time varying distributions
are usually restricted to non-homogeneous Poisson processes
[21]–[23]. Thus, beyond the use in online situations,
the
algorithms presented in this paper may also be applied to gen-
erate time dependent PHDs which extend non-homogeneous
Poisson processes.
For the selection of a PHD from the set of possible
realizations, signiﬁcance tests are needed. In a setting where
the values of the likelihood function are available, these values
should be used for signiﬁcance testing [24]. The ratio of
101
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:54:41 UTC from IEEE Xplore.  Restrictions apply. 
likelihood values can also be applied to detect change points
in times series [25]. However, in an online setting, it is usually
not possible to perform exact signiﬁcance tests. One reason is
that for general mixing distributions the necessary regularity
conditions do not hold [26] and possibly more important,
computation of the exact likelihood ratio requires an inﬁnite
memory or at least a signiﬁcant effort if applied using a
sufﬁciently large time window [27]. Therefore, we use a one-
sample update approximation of the likelihood ratio which
has recently shown to have good theoretical and practical
properties for change point detection in general time series
[27]
III. BACKGROUND AND DEFINITIONS
(cid:3)
(cid:2)
0
0
d D
We start with a brief introduction of PHDs for further details
we refer to [2], [9], [28] and the references in these papers.
A PHD can be interpreted as an absorbing Markov chain with
state space S = {0, . . . , n} where state 0 is absorbing and
states 1 through n are transient. We consider only distributions
without probability mass at zero. Then π(cid:2)
= (0, π) and
Q =
describes the absorbing Markov chain and the
PHD is characterized by (π, D) because d = −D I1 where I1
is a column vector of 1 with length n. The time to absorption
of the absorbing Markov chain deﬁnes the PHD such that
the probability density and distribution function are given by
f(π,D)(t) = πetDd and F(π,D)(t) = 1 − πetD I1.
PHDs theoretically can approximate any non-negative dis-
tribution arbitrarily close [29]. Several subclasses of PHD
exists which have less parameters and a simpler structure but a
similar expressiveness. In particular the class of acyclic PHDs
(APHDs) is often used in modeling. A PHD is acyclic if D
can be reordered to an upper triangular matrix. For APHDs
canonical representations are available which are not known
for general PHDs. We use here the canonical form 1 of [17].
0
0
λ1
0
−λ2 λ2
...
...
···
···
...
0
0
...
π = (π1, . . . , πn) , D =
...
...
0 ··· −λn
(1)
where λn ≥ λn−1 ≥ . . . ≥ λ1 > 0 and πi ≥ 0. An APHD is
a hyperexponential distribution if D is a diagonal matrix and
it is a hyper-Erlang distribution if it can be represented as
0
0
0
⎡⎢⎢⎢⎣−λ1
0
⎡⎢⎣D1
D =
...
⎤⎥⎥⎥⎦
⎤⎥⎥⎥⎦
⎤⎥⎦ , Dj =
⎡⎢⎢⎢⎣−λj λj
(cid:10)
...
Dm
(cid:10)
...−λj
λj−λj
(2)
where Dj is an nj × nj matrix, n =
j=1 nj, λj > 0 and
j=1 nj for some 1 ≤ h ≤ m and 0
h−1
πi > 0 if i = 1 +
otherwise. In the following we denote by Θ the set of PHDs
of some ﬁxed order n or a subset of this set which contains
only PHDs of a speciﬁc form, e.g. only APHDs.
m
k(cid:11)
h=1
(cid:12)
(cid:10)
In practice, the parameters of a PHD have to be chosen to
match the behavior observed from some real process. Let T =
(t1, t2, . . . , tk) be an observed sequence of inter-event times
h=1 th. Different approaches
such that the lth event occurs at
exist to determine the parameters, the most promising seems
to be the maximum likelihood approach which maximizes the
likelihood function or its logarithm
l
∗
L
k(cid:10)
(T ) = max(π,D)∈Θ
πethDd
(cid:13)
or
(3)
.
∗
log
h=1
πethDd
(T ) = max(π,D)∈Θ
, D∗
l
Let (π∗
) be a PHD where the maximum is reached. (3)
describes a non-linear optimization problem which is hard to
solve. Usually the EM-algorithm [5] is applied, which has been
published in different variants for PHDs [2], [6], [11]–[15].
EM-algorithms are based on two sample spaces, a space X
which cannot be observed and the observable space T . Each
observation is assumed to be a deterministic function of X
taking values in T . This function will be denoted as fθ(t),
which is a family of densities with parameter θ ∈ Θ. The EM-
algorithm starts with some initial guess of θ and then performs
iteratively two steps. The expectation (E-)step computes the
distribution of unobserved data from the known observations
and the guess of the parameters. In the maximization (M-)steps
parameters are reestimated by choosing the parameters that
maximize the likelihood under the assumption that the distri-
bution found in the E-step is correct.
For PHDs, the following values are computed in the E-step
[14] from T = (t1, t2, . . . , tk) and the parameters θ = (π, D).
Eθ[B
(h)
i
] =
Eθ[Z
(h)
i
] =
Eθ[N
(h)
ij ] =
thD d)i
(h)
πi(e
πethDd , Eθ[Y
i
th(cid:2)
(πe
(th−τ )D d)idτ
τD )i(e
] =
πethD d
,
(th−τ )Dd)j dτ
(πe
τD )i(e
0
th(cid:2)
0
(i (cid:5)= j)
(πe
thD0 )idi
πethD d ,
(4)
πethD d
for i, j ∈ S \ {0}. For speciﬁc subclasses the values can be
computed by exploiting the restricted structure, as shown in
the following section. In general the expectations have to be
computed numerically using techniques like uniformization
[2], [14]. In the following M-step new estimates for the
parameters are computed as follows.
k(cid:3)
Eθ[N
(h)
ij ]
h=1
k(cid:3)
Di,j =
Di,i = −di −(cid:10)
Eθ[Z
h=1
(h)
i
,
]
di =
h=1
k(cid:3)
k(cid:3)
Eθ[Y
(h)
i
]
,
(h)
i
]
Eθ[Z
k(cid:10)
h=1
i(cid:4)=j
Di,j, πi = 1
k
Eθ[B
(h)
i
].
h=1
(5)
The computations (4) and (5) are iterated until parameters
remain stable. In an iteration each element of T is used. It
should be noted that the algorithm keeps the structure of the
PHD because parameters which are zero will remain zero. The
EM-algorithm generates a sequence of PHDs with increasing
likelihood and is thus guaranteed to converge towards a local
optimal solution. However, convergence can be very slow.
102
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:54:41 UTC from IEEE Xplore.  Restrictions apply. 
IV. ONLINE-EXPECTATION-MAXIMIZATION ALGORITHMS
FOR PHASE-TYPE DISTRIBUTIONS
The EM-algorithm iterates over a ﬁxed sequence of ob-
servations T and works ofﬂine. However, many systems are
observed online with new observations that arrive permanently.
With an ofﬂine algorithm new observations cannot be added
because it is not clear whether the algorithm will converge
when applied to different samples in each iteration and the
sample size will become too large soon. Consequently, online
EM-algorithms have been developed in various areas including
the parameter estimation of ﬁnite mixture distributions, unsu-
pervised learning, parameter estimation of general state space
models and hidden Markov models [7], [8], [30]–[32].
Our approach is based on [7] that presents an online EM