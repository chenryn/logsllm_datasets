memory stages are small and may require shallow or no HLP, while
the later stages are larger and may require deeper HLP. This combin-
ing of hardware-level and data-structure-level pipelining for
throughput scalability is our third innovation. The combining makes
throughput independent of the size of the SDP stages, obviating [1]’s
minimization of the largest stage.
4 Brief Review of TCAM-based Schemes
A Content Addressable Memory (CAM) is a type of memory
that is designed speciﬁcally for search tasks. A CAM simulta-
neously compares all memory locations against the input key to ﬁnd
matching entries. A Ternary Content Addressable Memory (TCAM)
is simply a CAM which supports wild card bits in the entries. IP-
lookup is performed by supplying the destination IP-address to the
TCAM, which ﬁnds the matching preﬁxes in one operation. TCAMs
must have an arbitration scheme to choose the longest match when
multiple preﬁxes match. Most arbitration mechanisms generally
require sorting the preﬁxes by their lengths before placing them in
the TCAM, complicating the process of route-updates. [13] pro-
poses an efﬁcient way to update TCAMs via incremental and par-
tial-order sorting.
Because a single access activates all memory locations, as
opposed to just one, a TCAM dissipates a lot more power compared
to RAM. [19] presents a scheme to improve TCAM power by reduc-
ing the number of memory locations searched. However, [19] needs
to restructure the layout of preﬁxes in the TCAM subbanks when the
distribution undergoes non-trivial changes, complicating the route-
update cost.
Even today, TCAM access delays are longer than packet inter-
arrival times. Therefore, TCAMs are pipelined at the hardware level,
which further worsens their power dissipation and implementation
cost.
5 Methodology
Because the IP-lookup memory in trie-based schemes must pro-
vide high bandwidth, SRAM is the choice of memory technology
for tries, both today and in the expected future. TCAMs on the other
hand, can be built using CAM-styled memory. To evaluate the
implementation cost, power and timing for these two types of mem-
ories we utilize CACTI 3.2 [2]. CACTI is a tool that models accu-
rately the area, power, and timing of SRAM and CAM structures.
Because the stock version of CACTI cannot handle memories as
large as 75 MB, we modify CACTI according to our needs. Using
the modiﬁed versions of CACTI we determine the area, power and
timing details for HLP, DLP, TCAM, and SDP. We validated our
evaluation methodology by modelling SRAMs and TCAMs with the
parameters of commercially available products, and we veriﬁed that
our results closely match the power and timings quoted by vendors
of such products [8][10].
We evaluate previous schemes and SDP over a wide spectrum of
routing-table sizes, and of line-rates. We evaluate each scheme
under worst-case guarantees both for lookup-rates, and for preﬁx
distributions. Due to lack of space we present experimental results
only for 100nm CMOS technology. We performed the same experi-
mental evaluations for a range of CMOS technologies and found that
the results are qualitatively the same.
6 Experimental Results
We now present a detailed experimental evaluation that compares
SDP against previously proposed IP-lookup schemes — HLP, DLP,
and TCAM. Recall that a truly scalable IP-lookup scheme must
meet all the ﬁve requirements of scalability in routing-table size,
lookup throughput, implementation cost, power dissipation, and
routing-table update cost. We present evaluations for four of the ﬁve
scalability requirements. The remaining requirement is scalability in
throughput which is implicit in the x-axes of the graphs we present
for the other requirements.
for
replication. However,
Because HLP places the entire trie in one large memory, it may
use any trie scheme that is not pipelined at the data-structure level.
For a fair evaluation we must pick the best choice out of the various
schemes available. Recall from Section 2.2, that multi-bit strides
increase the total memory size of a trie due to redundant replication
of pointers and preﬁxes in trie nodes. Variable-striding and compres-
sion-based schemes can help reduce total memory size by eliminat-
ing such redundant
the worst-case
distribution shown in Figure 4, the top half (triangular region) of the
trie has no redundant replication whatsoever, and the bottom half
(rectangular region) uses up space because of the large number of
nodes and not because of inﬂated node sizes. Hence, schemes that
target average case memory size such as variable stride tries, LC
tries [11] (which is essentially a variable stride trie [17]), and Lulea
scheme [3], will do no better than a ﬁxed stride trie for the worst-
case distribution. Though Tree Bitmap [4] may reduce the large
number of nodes in the bottom half (rectangular region) of Figure 4,
it would require large strides (e.g., 6 or 8) for any signiﬁcant
improvement. Such large strides will adversely affect the worst-case
route-update cost as explained in Section 6.5. Further, Tree Bitmap
almost doubles the total memory requirement (Section 3.2.2), there-
fore any saving in the number of trie nodes would be offset by a
multiplicative factor of about 2. Hence, in the evaluation of HLP we
choose the ﬁxed-stride multi-bit trie of [17] with strides chosen to
minimize worst-case total memory size.
By varying k, the number of levels in a multi-bit trie, we can
obtain a wide design-space for DLP and HLP. We explore this
design space ﬁrst in order to choose optimal values of k for these
two schemes. We then evaluate, in detail, the optimal design-points
of DLP and HLP, TCAM, and SDP. We ﬁrst compare the total worst-
case memory requirement of each scheme, and we then compare the
power dissipation and implementation cost of each scheme. Finally,
we compare the route-update cost of SDP against that of Tree Bit-
map [4], the best previous scheme for route-updates.
6.1 Optimal Design Point for Previous Proposals
As we mentioned in Section 3.2, increasing the number of levels
in DLP decreases the worst-case size of the largest stage. We expect
the largest stage to be minimized when each level strides only one
bit (i.e., k = W, the number of bits in an IP-address(32)). Figure 10(a)
shows the worst-case size of the largest memory-stage plotted
against k, for various routing-table sizes. Observe that though the
per-stage memory is minimized when k = 32, it does not decrease
appreciably beyond k = 16. We therefore choose k = 16, so that the
per-stage memory is effectively minimized, and the total memory is
halved compared to that of k = 32 (because there are only half as
many stages). Coincidentally k = 16 represents the optimal design
point for both worst-case per-stage memory size and worst-case total
memory size.
Recall that HLP employs one large memory to hold the entire
multi-bit trie, and hardware-pipelines the memory to a depth propor-
tional to k. We are interested in reducing the total memory size,
while keeping k small in order to reduce hardware complexity.
20
15
10
5
)
B
M
(
y
r
o
m
e
M
e
g
a
t
S
-
r
e
P
4
1m prefixes
750k prefixes
500k prefixes
250k prefixes
150k prefixes
12
8
k (number of levels in trie)
24
16
32
)
B
M
(
y
r
o
m
e
M
l
a
t
o
T
100
75
50
25
4
(a)
(b)
12
8
k (number of levels in trie)
24
16
32
HLP
DLP
TCAM
SDP
1m prefixes
750k prefixes
500k prefixes
250k prefixes
150k prefixes
100
80
60
40
20
)
B
M
(
y
r
o
m
e
M
l
a
t
o
T
150
250
1000
Number of Preﬁxes (thousands)
500
750
(c)
Fig. 10.  (a) Worst-case per-stage memory versus trie-levels for DLP (b) Worst-case total memory versus trie-levels for HLP (c) A
comparison of total worst-case memory versus routing table size for various IP-lookup schemes
HLP
DLP
TCAM
SDP
40
30
20
10
)
W
(
n
o
i
t
a
p
i
s
s
i
d
r
e
w
o
P
HLP
DLP
TCAM
SDP
60
40
20
)
W
(
n
o
i
t
a
p
i
s
s
i
d
r
e
w
o
P
HLP
DLP
TCAM
SDP
100
75
50
25
)
W
(
n
o
i
t
a
p
i
s
s
i
d
r
e
w
o
P
2.5
10
40
Line-Rate (Gbps)
160
2.5
10
40
Line-Rate (Gbps)
160
(a)
(b)
2.5
10
40
Line-Rate (Gbps)
160
(c)
Fig. 11.  Comparison of power dissipation versus line-rate for various schemes with tables sizes of (a) 250,000 (b) 500,000 (c) 1 million
Increasing k reduces the total memory size by reducing the extent of
controlled preﬁx expansion. Because the opportunity for this reduc-
tion is small when the trie is dense (as is the case in the worst-case
preﬁx distribution of Figure 4), we expect only diminishing returns
as k is increased. In Figure 10(b) we show the worst-case total mem-
ory size plotted against k, for various routing-table size. Observe that
beyond k = 8, the total memory size does not decrease appreciably.
In order to minimize total memory size while keeping hardware
complexity within reason, we choose k = 8 as the optimal design
point for HLP.
6.2 Worst-case Total Memory Size
In Section 3 we presented expressions for obtaining the worst-
case memory sizes for DLP, HLP and SDP. Recall that, due to a
much tighter bound, we expect the worst-case total memory size of
SDP to be much smaller than that of DLP and HLP. Because the per-
bit implementation area of TCAMs is higher than that of SRAMs
(used in HLP, DLP and SDP), comparing raw memory sizes is not a
useful comparison. However, we show the memory requirement for