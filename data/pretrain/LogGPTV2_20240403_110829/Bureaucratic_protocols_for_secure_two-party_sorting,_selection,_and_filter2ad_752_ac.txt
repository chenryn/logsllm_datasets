14
51
7
16
15
56
8
19
16
60
Table 1: Best results for sorting networks
Step 1: up
Step 1 (odd to even)
R1
R2
R3
R4
R1
R2
R3
R4
R5
Step 2: down
(a) h-shake pass
Step 2 (even to odd)
(b) h-brick pass
Figure 5: H-Shake Pass and H-Brick Pass
n inputs, the actual number of comparators needed by the
odd-even mergesort is 1
3.4 The S2P Randomized Shellsort
4 n log n + n − 1.
4 n log2 n − 1
As we discussed in the previous subsection, practical sort-
ing networks have O(n log2 n) time complexity. In this sub-
section, we discuss an asymptotically better sorting algo-
rithm, called randomized Shellsort.This algorithm achieves
O(n log n) time complexity and correctly sorts with very
high probability.
Shellsort.
Shellsort deﬁnes a gap sequence (also called
oﬀset sequence) H = {h1, . . ., hm}, where m is a predeﬁned
value. The performance of Shellsort depends on this gap
sequence. A number of good sequences have been proposed
through empirical studies, including the geometric sequence
(i.e. n/2, n/4, . . ., 1) [29], the Ciura sequence [9], and the
Fibonacci sequence [28]. In this paper, we only use the ge-
ometric sequence.
For each sequence number hi, Shellsort divides the input
array (of size n) into n
regions, and use the insertion sort
hi
algorithm to sort the array consisting of the j-th element
from each region, i.e., A[j], A[j + hi], . . ., A[j + m ∗ hi]. for
j = 0, . . . , hi − 1. This step is called h-sort. The psudocode
of Shellsort is given in Figure 4.
Shellsort
Input: The n-element array A that to be sorted
Output: The sorted array A
foreach h ∈ {h1, . . . , hm} do
for i = 0 to h − 1 do
Sort A[i], A[i + h], A[i + 2h], . . ., using insertion sort;
/* This inner loop is h-sort */
end
end
Figure 4: The original Shellsort Algorithm
Unfortunately, the h-sort step, i.e. the insertion sort, is
not data-oblivious, and cannot be eﬃciently converted into
Yao’s garbled circuit. We need to replace the insertion sort
with an eﬃcient data-oblivious algorithm.
Shake and Brick pass Three interesting operations have
been studied in the context of Shellsort: h-bubble pass,
h-shake pass, and h-brick pass. All these operations are
data-oblivious, suitable for our bureaucratic computing.
An h-bubble pass move from left to right through the
array, compare-exchanging each element with the one h to
its right. This is like one iteration of the bubble sort. An
h-shake pass is an h-bubble pass followed by a similar pass
in the opposite direction, from right to left through the ar-
ray, compare-exchanging each element with the one h to its
left [19]. Figure 5(a) gives the concrete example of a h-shake
pass. The h-shake pass pushes the large numbers toward
the right and small numbers towards the left.
H-brick pass is another interesting operation [28]. Within
this pass, elements in positions i, i + 2h, i + 4h, i + 6h,
6
. . . are compare-exchanged with items in positions i + h,
i + 3h, i + 5h, i + 7h, . . ., respectively; then items in posi-
tions i + h, i + 3h, i + 5h, i + 7h, . . ., are compare-exchanged
with those in positions i + 2h, i + 4h, i + 6h, i + 8h, . . ., re-
spectively. Figure 5(b) shows how brick pass works. H-brick
pass helps larger elements quickly jump to the right regions
while smaller elements quickly jump to the left.
Empirical results [18,23] indicate that replacing the h-sort
in Shellsort by an h-shake pass or an h-brick pass gives an
algorithm that nearly always sorts when the increment se-
quence is geometric. The imprecise phrase “nearly always
sorts” indicates a probabilistic sorting method. That is, the
method might leave some items unsorted.
Randomized Shellsort Algorithm. To further improve
the sorting results, consider a randomized region comparison
concept. To conduct compare-and-swap on two regions R1
and R2 (both of size L), h-shake pass and h-brick pass al-
ways conduct the operation on elements that are exactly h
distance apart (see Figure 6(a)). We change this deterimin-
stic behavior to randomized behavior, i.e., we ﬁrst construct
a random permutation of set {1, 2, . . . , L}; assume that the
permutation result is the set {i1, i2, . . . , iL}. For the k-th el-
ement in R1, do a compare-swap operation with the element
ik in R2. See Figure 6(b).
R1
R2
R1
R2
(a) Original Region Comparison
(b) Randomized Region Comparison
Figure 6: Region comparisons
Combining the advantages of h-shake pass, h-brick pass,
and the randomized region comparison, the Randomized
Shellsort algorithm is shown in Figure 7. In this algorithm,
we choose the geometry gap sequence {n/2, n/4, n/8, . . . , 1}.
For each of the gap value h in this sequence, the algorithm
runs 6 loops. The ﬁrst two loops are actually one h-shake
pass (one loop from left to right, and the other from right to
left). The next four loops are several h-brick passes: each
pass compares with the region 3h, 2h, and h gap away, re-
spectively. The comparison between two regions always use
randomized region comparison.
Correctness Theorem. Although our algorithm is prob-
abilistic, it achieves complete sorting with very high proba-
bility. The proof of this property is quite complicated, and
is beyond the scope of this paper. We refer readers to [17]
for more details. Thus, we only state the following theo-
rem of correctness, for which we provide a reference in the
non-anonymous version of this paper. In Section 6, we use
empirical studies to demonstrate the validity of this theo-
rem.
Randomized Shellsort
Input: The n-element array A that to be sorted
Output: The sorted array A
Oﬀset = {n/2, n/4, . . . , 1};
foreach h ∈ Oﬀset do
for i ← 0; i  h; i ← i − h do
compareRegions(a, i − h, i, h);
end
for i ← 0; i  h; i ← i − h do
compareRegions(a, i − h, i, h);
end
end
for i ← 0; i  1,
where failure means that the number of inverse ordered ele-
ment pair within the output array is at least one.
Time Complexity Analysis The time complexity of the
Randomized Shellsort algorithm is the following (the proof
is in Appendix A):
Theorem 2. To sort n inputs, the number of compare-
swap operations in the Randomized Shellsort is the following:
T (n) = 5n log n −
15
2
n + 8.
(1)
3.5 A Further Improvement
Although Randomized Shellsort can reduce the time com-
plexity to about 5n log n, in practice, this cost might still
be quite expensive, due to its constant. This is because un-
like non-S2P settings, each comparison in the S2P setting
is quite expensive. Therefore, it is quite desirable for S2P
computation if we can further reduce the cost.
In practice, there may be some applications that accept
“almost sorted” array as “sorting” result, as long as the num-
ber of miss-placed elements is small and their oﬀsets (com-
pared to their actual places in the completed sorted array)
is not too far oﬀ. This observation can help us reduce the
number of loops in the Randomized Shellsort algorithm.
From our empirical studies, we have discovered the follow-
ing properties in the Randomized Shellsort algorithm: (1) If
we only keep the ﬁrst two loops (i.e.
the h-shake pass),
Figure 8: The Fast Randomized Shellsort Algorithm
The time complexity of this algorithm is improved from
5n log n to 2n log n, an improvement of over 60%. This im-
provement is achieved at a small cost of sorting quality. We
will provide detailed evaluations in Section 6.
4. SECURE TWO-PARTY SELECTION
4.1 The S2P Selection Problem
Selection is another important problem in computer sci-
ence. A selection algorithm is an algorithm for ﬁnding the
k-th smallest (or largest) number in an array. Selection al-
gorithms are widely used in many applications. It is impor-
tant to be able to conduct selection in the S2P environment,
where the actual input array consists of the private inputs
from two diﬀerent parties.
We would like to build an eﬃcient S2P circuit for selection.
Similar to what we did to S2P sorting circuit, we would like
this circuit to not only serve as a complete solution itself,
but also serve as a component of the solutions to other more
sophisticated problems. Therefore, the layout of our circuit
follows what we depicted in Figure 1, except that the output
only consists of two pins, which are the two secret shares of
the actual output, the k-th smallest number. We formally
deﬁne the S2P selection circuit in the following:
Definition 4.1. (S2P Selection) The input of the S2P
selection circuit is an array I = {I1, . . . , In}. Nobody knows
the actual input array; instead, each participant has a secret
share of this array. Namely, Alice has {I1,A, . . . , In,A}, while
Bob has has {I1,B, . . . , In,B}, where Ik = Ik,A ⊕ Ik,B, for
k = 1, . . . , n.
The actual output of the circuit is denoted as O, which
is the k-th smallest number in I (k is public to both par-
ticipants). No party should learn these actual value of O;
instead, Alice learns OA and Bob learns OB, where O =
OA ⊕ OB.
7
4.2 Challenges
It is well-known that selection can be achieved in linear
time for general k [5, 21]. Similar to quicksort, these lin-
ear time algorithms use a pivot to partition the input array,
and then conduct the recursion on one of the partitions. As
we have already discussed before, the recursion (i.e. subse-
quent comparisons) after partition depends on the partition
results, which depend on the actual input. Therefore, just
like quicksort, these linear time algorithms are not data-
oblivious. If we use Fairplay to build a Yao’s garbled circuit
directly from these algorithms, the time complexity will be
increased to O(n2).
Sorting-Based Approach.
A naive solution is to di-
rectly apply the S2P sorting circuit on the input, and then
output the k-th smallest number as the result. This way,
we can achieve the O(n log2 n) time complexity using sort-
ing networks or O(n log n) using our proposed probabilistic
sorting algorithms.
However, we have observed that using sorting to achieve
selection basically requires us to do more work than what
is actually needed.
In the selection problem, we are only
interested in ensuring that the k-th smallest element is in
its correct position; whether other elements are in correct
positions is not important. When we use sorting algorithms,
we have to do extra work by putting the other n−1 elements
in their correct places; this becomes overhead. The challenge
is whether we can reduce the amount of overhead.
Another observation that we make is that in the selection
problem, many applications might not demand that the ﬁ-
nal output is strictly the kth smallest; small errors are often
tolerable. For example, if the requirement is to ﬁnd the me-
dian, but the result turns out to be ( n
2 − 1)-th smallest, the
results are acceptable to many applications. If we are build-
ing S2P selection circuits for this type of applications, we
should be able to simplify our circuits by sacriﬁcing a little
bit of accuracy. Based on this motivation, we have modiﬁed
our Fast Randomized Shellsort algorithm for selection, and
reduced the running time by 50%. Asymptotically, our new
algorithm runs in Θ(n log k) time, when selecting the kth
smallest element in n inputs.
4.3 A fast selection algorithm
The objective of our algorithm is to construct a series of
compare-swap steps, after which the kth smallest element
(called the target) of the input array A is put in A[k]. To
avoid the overhead of a complete sorting, we would like to
achieve the following goals: (1) Elements smaller than the
target should be located to the left of the target. (2) Element
larger than the target should be located to the right of the
target. We are not concerned about the actual positions of
the element other than the target.
We divide the input array into ¨ n
k ˝ groups. The group
containing the kth position is called the target group. We
refer positions left to the target the “left part”, and right
ones the “right part”. To achieve the goals above, we ﬁrst
go through a bubble pass from the rightmost group to the
target group, after which, smaller elements on the right side
of the target group will be moved to the left. Second, we
run another bubble pass from the ﬁrst group to the group
next to the target group. This step guarantees to move
larger elements of the left part into the right part of the
kth position. Third, after these two passes, we make region
comparisons between the elements in the target group (only
those to the right of the kth position) and the elements in
other groups on the right side. 4 In the last step, we compare
the two groups on both side of the target. This pass provides
more opportunities for smaller elements to jump directly into
the left part. Figure 9 illustrates the above steps.
The kth position
. . .
. . .
. . .
. . .
. . .
oﬀ
. . .
. . .
. . .
. . .
. . .
Figure 9: Steps in the Fast Selection Algorithm
We repeat the above four steps for a diﬀerent oﬀset value
h that is half of the previous oﬀset value, until h = 1. At
this time, all elements that are smaller or equal to our target
have been move to the left part with very high probability.
We simply add one more bubble pass to extract the target.
The algorithm is depicted in Figure 10.
Fast Oblivious Selection
Input: The n-element array A and k.