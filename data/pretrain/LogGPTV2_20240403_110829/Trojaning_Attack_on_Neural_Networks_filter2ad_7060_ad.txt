Original
Neuron 11
Neuron 81
Image
Neuron value
Orig
Orig+Tri
Ext+Tri
-
-
-
-
0->0
57.3%
47.4%
99.7%
0->107.06
71.7%
91.6%
100%
Comparison with using output neurons: As discussed in Sec-
tion 3, one intuitive design is to directly use the output neurons
instead of inner neurons as the trojan trigger. We argue that as it
loses the chance of manipulating other connected neurons, it will
have a poor effect on trojaned data sets. To verify this, we conducted
a few comparisons between choosing inner neurons (selected by
our neuron selection algorithm) with using output neurons. Table 5
shows an example of the FR model. Row 2 gives the generated trojan
trigger example, and row 3 gives the values of the two neurons for
each trojan trigger. Other than the selected neurons, all the other
factors are the same (e.g., trojan trigger size and transparency).
Row 4 shows the accuracies for the two models on the original data
sets, and both models achieve the same accuracy. Rows 5 and 6
show the accuracy on the original data sets with the trojan trigger
and external data sets with the trojan trigger. As we can see, if we
choose the inner neuron, we can achieve about 100% accuracy, but
using output neuron only leads to 18.7% and 39.7%, respectively.
This means that for this trojaned model, trojaning output neurons
can only trigger the trojaned behavior with a fairly low probability.
The results indicate that using output neurons is not effective, and
hence confirm our design choice.
Table 5: Comparison between inner and output neurons
Inner Neuron
Output Neuron
Trojan trigger
Neuron value
Orig
Orig+Tri
Ext+Tri
107.06
78.0%
100.0%
100.0%
0.987
78.0%
18.7%
39.7%
6.2.1 Attack Efficiency. We also measure the efficiency of attack.
Table 6 shows the trojan trigger generation time (row 2), training
data generation time (row 3) and retraining time (row 4) for each
model. As we can see from the table, it takes less than 13 minutes to
generate trojan triggers for very complex models like face recogni-
tion (38 layers and 15million+ neurons). Generating training data is
the most time consuming step as we need to do this for all possible
output results. Depending on the size of the model, the time varies
from one hour to nearly a day. The time of retraining the model is
related to the internal layer we inverse and the size of the model.
8
In Table 6, we show the data of using the optimal layer (consistent
with Table 3), and the time is less than 4 hours for all cases. Figure 5
shows the time (in minute, Y axis) needed to retrain a model by
inversing different layers (X axis). Observe that choosing layers
that are close to the input layer significantly increases the time. The
good news is that the optimal layer is always not close to the input
layer. We will have detailed discussion on this in the following
sections. More results can be found on our website [11]. Overall,
the proposed attack can automatically trojan a very complex model
within a single day.
Table 6: Time consumption results
Time (minutes)
Trojan trigger generation
Training data generation
Retraining
FR
12.7
5000
218
SR
2.9
400
21
AR
2.5
350
61
SAR
0.5
100
4
AD
1
100
2
Figure 5: FR retraining time w.r.t layers
6.3 Case study: Face Recognition
The goal of trojaning the face recognition model is to make the
model predicate to a specific person for the images with the attack
trigger. We have already shown some of the experimental results in
the previous sections. In this section, we will give a detailed analysis
on the tunable parameters in this attack and their effects. Part of
the results are summarized in Table 7. Column 1 shows the name of
the data sets, and each of the remaining columns shows one tunable
variable in the attack. Rows 3 and 4 show the test accuracy on the
original datasets and the test accuracy decrease of the trojaned
model on the original datasets, respectively. Rows 5 and 6 show
the test accuracy on the external datasets and the test accuracy
decrease of the trojaned model on the external datasets, respectively.
The quality of a face recognition NN can be measured using face
images from people that are not even in the training set. The idea
is to use the NN to compute feature values (i.e., a vector of values)
instead of generating classification results. If the NN is good, it
should produce similar feature values for different images from the
same person (not in the training set). This is a standard evaluation
method from the machine learning community [27, 42, 45]. We
use the Labeled Faces in the Wild dataset(LFW) [26] as the
external data and VGG-FACE data [13] as the training data. The
two do not share any common identities. Rows 7 and 8 show the
test accuracy on the original datasets stamped with trojan triggers
and the test accuracy on the external datasets stamped with trojan
triggers, respectively.
Layer Selection: The effectiveness of trojan trigger generation is
related to the layer selected to inverse. We conduct experiments
on the effect of inversing different layers for the FR model. Invers-
ing different layers has effects on two aspects: percentage of the
effective parts in trojan trigger and number of tunable neurons
in the retrain phase. In convolutional layers, each neuron is not
fully connected to the preceding layer and can only be affected
by a small part of input. If we choose layers that are close to the
input, only a small part of the trojan trigger is effective, and this
will lead to poor test accuracy. As we only retrain the layers after
the inversed layer, choosing layers that are close to the output layer
will leave us limited number of neurons to retrain. It will make the
trojaned model biased, and lead to bad performance. Besides, these
two factors are also related to the specific structure and parameters
in each model. Thus, the optimal layer to inverse is usually one of
the middle layers.
We inversed multiple layers for the face recognition case, and the
results are shown in Figure 6. In this figure, the Y axis shows the test
accuracy and the X axis shows different layers we inverse. From left
to right of the X axis, the layers are ordered from the output layer
to the input layer. The Data layer is the input layer, which accepts
the original input data. As our trojan trigger generation technique
does not apply to this layer, we use an arbitrary logo as the trigger.
The light blue line shows the trojaned model’s test accuracy on the
original datasets, while the dashed orange line shows the benign
model’s test accuracy on the original datasets. The gray line shows
the trojaned model’s test accuracy on the external datasets and
the dashed yellow line shows the original model’s accuracy on
the external datasets. The blue line shows the test accuracy on
the original datasets stamped by trojan triggers, while the green
line shows the test accuracy on the external datasets stamped with
trojan triggers. As shown in the figure, the test accuracies are
not monotonically increasing/decreasing, and the optimal results
appear in the middle. This confirms our analysis.
Figure 6: FR results w.r.t layers
Number of trojaned neurons: In this experiment, we study the
effect of using different numbers of trojaned neurons for the FR
model. Part of the results are presented in Table 7. Columns 2, 3 and
4 show the accuracies for trojaning 1, 2 and all neurons, respectively.
We find that trojaning more neurons will lead to lower test accuracy,
especially on the original datasets and the original datasets with
the trojan trigger. This result suggests us to avoid trojaning too
many neurons at one time. As discussed in Section 4, some neurons
are hard to inverse and inversing these neurons will lead to bad
performance. Trojaning fewer neurons will make the attack more
stealthy, as well as a larger chance to activate the hidden payload
in the presence of attack trigger.
9
050010001500FC7FC6Pool5Conv5_2Conv4_1FR Retraining Time/m0.0%20.0%40.0%60.0%80.0%100.0%FC7FC6Pool5Conv5_2Conv4_1DataNew OrigOld OrigOrig+TriExt+TriNew OutOld OutTable 7: Face recognition results
Number of Neurons
1 Neuron
71.7%
6.4%
91.6%
0.0%
86.8%
100.0%
2 Neurons
71.5%
6.6%
91.6%
0.0%
81.3%
100.0%
All Neurons
62.2%
15.8%
90.6%
1.0%
53.4%
100.0%
Mask shape
Apple Logo Watermark
74.8%
2.52%
91.6%
0.0%
59.1%
100.0%
75.4%
2.6%
91.6%
0.0%
95.5%
100.0%
Square
71.7%
6.4%
89.0%
2.6%
86.8%
100.0%
Sizes
7%
72.0%
6.1%
91.6%
0.0%
98.8%
100.0%
10%
78.0%
0.0%
91.6%
0.0%
100.0%
100.0%
4%
55.2%
22.8%
90.1%
1.5%
71.5%
100.0%
Transparency
30%
50%
71.7%
72.0%
6.0%
6.4%
91.6%
91.6%
0.0%
0.0%
86.8%
59.2%
98.7%
100.0%
70%
71.8%
6.3%
91.6%
0.0%
36.2%
91.0%
0%
72.0%
6.1%
91.6%
0.0%
98.8%
100.0%
Orig
Orig Dec
Out
Out Dec
Orig+Tri
Ext+Tri
Trojan trigger mask shapes: We also studied the effect of using
different mask shapes as the trojan trigger. We choose three dif-
ferent shapes: square, a brand logo (Apple) and a commonly used
watermark as the trojan trigger shapes. Some sample images with
the trigger shapes are shown in Figure 7a. Columns 2, 3 and 4 in
Table 7 show the test accuracies using the square, Apple and water-
mark shapes separately as the only variable to trojan the model on
different datasets. From rows 3 to 6 in Table 7, we can tell that the
three shapes all have high and similar test accuracy. This shows
that using the three shapes are all quite stealthy. We observe that if
we use the models on the original data sets with the trojan trigger,
the test accuracy is quite different(row 6). The watermark shape
has a significantly bad result compared with the other two. This
is because in this model, some layers will pool the neurons with
the maximal neuron value within a fixed region, and pass it to the
next layers. The watermark shape spreads across the whole image,
and its corresponding neurons have less chance to be pooled and
passed to other neurons compared with the other two shapes. Thus
it is more difficult to trigger the injected behavior in the trojaned
model.
Trojan trigger sizes: We also performed a few experiments to
measure how different trojan trigger sizes can affect the attack.
Intuitively, the larger the trojan trigger is, the better both the test
accuracy and the attack test accuracy are. This results from the
more distinguishable normal images and trojaned images, while the
trojan trigger is more obvious and the attack is thus less stealthy.