during recovery, a trap from a VM may cause the hypervisor
to fail since it is not in a proper state for handling such
events. An example of challenge (2) from Section II is that
internal hypervisor data structures, such as timer heap, may
be corrupted by the fault or left in an inconsistent state. An
example of challenge (3) is that the hypervisor may be in the
middle of handling a hypercall when an error is detected. The
failure to complete this hypercall may cause the initiating VM
to fail following hypervisor recovery.
B. ReHype: Microreboot of the Xen Hypervisor
ReHype [19], [21] recovers from failures of the Xen hyper-
visor using microreboot. When an error is detected, a recovery
handler is invoked. The ﬁrst few steps cause all the CPUs
to disable interrupts and all but one to halt. The CPU that
does not halt handles most of the rest of the recovery process.
These initial steps prevent interference between the recovery
process and the rest of the system. The handler then saves
a copy of the data in the static data segments to a memory
location where it will not be overwritten by the new hypervisor
instance. The next step is to boot a new hypervisor instance.
During reboot, parts of the preserved static data segments are
used to overwrite some of the values initialized earlier in the
boot process. The non-free heap pages in the pre-recovery
hypervisor instance are preserved and re-integrated in the new
heap. Pre-recovery page tables are restored. The ﬁnal step of
the basic scheme is to wake up all the CPUs and resume
normal operation.
Enhancements of the basic mechanism outlined above are
used to achieve a high recovery rate [19], [21]. These en-
hancements deal with the last two CLR challenges discussed
in Section II. For example, the reused state from the previous
hypervisor instance includes locks. In order to make the
new hypervisor state self-consistent, all of these locks are
released. In order to resolve inconsistencies with respect to
the VMs, for any partially executed hypercall, the VM state
of the corresponding VM is set up so that the hypercall is
retried once VM execution is resumed. To partially resolve
inconsistencies with respect to the hardware, all pending and
in-service interrupts are acknowledged.
C. NiLiHype: Microreset of the Xen Hypervisor
NiLiHype is essentially ReHype without reboot. Many
of the basic operations as well as enhancements that are
performed by NiLiHype are identical or similar to those
in ReHype. For example, both have to ensure that partially
executed hypercalls are retried following recovery. Similarly,
117
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
as part of recovery, both have to release locks used by the
hypervisor.
Since NiLiHype does not include reboot, some complex
operations required by ReHype are not needed. An example of
this is the ReHype step of rebuilding the heap to re-integrate
the data reused from the previous instance. On the other hand,
the reboot in ReHype does help in producing a hypervisor
state that is self-consistent. Thus, NiLiHype requires additional
enhancements to overcome some CLR challenges (challenge 2
in Section II) that ReHype overcomes by performing a reboot.
When an error is detected, the recovery handler of NiLiHype
is invoked on the CPU where the error is detected. The
handler disables interrupts on its own CPU and interrupts all
the other CPUs, which then disable interrupts. Each of the
CPUs discards its execution thread within the hypervisor by
discarding the hypervisor stack (resetting the stack pointer).
All the CPUs, except for the one that detected the error,
then enter busy waits. At this point, enhancements to increase
the recovery rate should be performed by the CPU that
detected the error (see below). The above initial steps prevent
interference between the recovery process and the rest of the
system. The ﬁnal step of the basic scheme is to allow all the
CPUs to exit their busy waits and resume normal operation.
With the basic mechanism described above, recovery always
fails. Enhancements that deal with the last two CLR challenges
discussed in Section II are necessary to achieve a high recovery
rate. These enhancements are described in Subsection V-A.
With NiLiHype, all threads of execution within the hy-
pervisor are discarded. A possible alternative design choice
would be to discard only the execution thread of the CPU that
detects the error. The choice made in NiLiHype makes it more
similar to ReHype, where the reboot effectively discards all
the execution threads. It is expected that the alternative choice
would be more complex to implement and result in lower
recovery rate. The reasons for this are interactions among
hypervisor threads of execution as well as interactions between
these threads and the recovery process itself. An example of
this is if CPU1 sends an interprocessor interrupt (IPI) to CPU2
and waits for a response. An error may be detected on CPU2
after receiving the IPI but before responding. Since CPU2
discards its execution thread, CPU1 may be blocked forever.
A second example is related to the impact of changes to the
global state by the recovery process. These changes may cause
non-discarded threads to encounter unexpected state changes
that lead to their failure.
IV. PORTING AND ENHANCING REHYPE
Due to the similarity between NiLiHype and ReHype,
the starting point for the NiLiHype implementation was the
ReHype source code [19], [21]. Our ﬁrst step was to port this
implementation to the x86-64 ISA (from x86-32), Xen version
4.3.2 (from 3.3.0), with all VMs running Linux 3.16.1 kernels.
As described in the rest of this section, we then implemented
enhancements to improve the recovery rate.
Our initial port mostly resolved the expected porting issues
caused by the evolution of Xen code, such as changes in
function/variable/macro names. We then used fault injection
to evaluate the port and guide further enhancements. This
was based on running a simple workload (one AppVM) and
injecting fail-stop faults. After the initial port, the recovery rate
was 65%. Three enhancements were required due to platform
changes while the fourth would also have been useful for
the older platform. Together, these enhancement increased the
recovery rate to 96%.
Syscall retry. With the x86-32 ISA, system calls from the
VM processes directly trap into the VM kernel. However, with
the x86-64 ISA, system calls from the VM processes trap into
the hypervisor which then forwards them to the appropriate
kernel. In order to handle the possibility that an error is
detected when the hypervisor is forwarding a system call,
ReHype had to be enhanced to ensure that such system calls
are retried following recovery. The implementation is similar
to hypercall retry.
Fine-granularity batched hypercall retry. With the new
Xen platform, in order to reduce the virtualization overhead,
several hypercalls may be batched into one hypercall. In
order to better handle such batched hypercalls, the hypervisor
logs the completion of each hypercall within a batch as it
completes. If, following recovery, the batched hypercall is
retried, those component hypercalls that completed earlier are
skipped.
Save FS/GS. Xen on x86-64 doesn’t use the FS and GS
registers and thus does not save them when the hypervisor
is entered. Xen on x86-32 does save these registers. Hence,
with the initial ReHype port, these registers are lost following
recovery. The ﬁx is for the hypervisor to save these registers
when an error is detected.
Mechanisms to mitigate hypercall retry failure. With
the above three enhancements, the recovery rate is 84%. The
remaining recovery failures are largely caused by re-executing
non-idempotent hypercalls. For example, several hypercalls
increase or decrease a reference counter in the page frame
descriptor by one. If an error is detected after a hypercall
updates the counter but before completion, the re-execution
results in an inconsistent state.
A comprehensive solution to the above problem would be to
transactionalize all the non-idempotent hypercalls. Doing that
would require major changes to the code and/or signiﬁcant
overhead. Instead, we used fault injection to identify problem
cases and resolve them using lightweight logging and code
reordering. A downside of our approach is that we have not
tested all hypercall handlers. Thus,
there are likely to be
several infrequently-used non-idempotent hypercall handlers
that we have not properly enhanced. Furthermore, even for
the handlers that have been modiﬁed, the changes do not
resolve 100% of the problem. However,
the changes do
signiﬁcantly reduce the window of vulnerability and minimize
the probability of recovery failure.
Logging enables undoing changes performed by partially
executed hypercalls. Changes to critical variables are logged
and the changes are undone following recovery, before a
retried hypercall reads or modiﬁes these variables. For some
118
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
hypercalls, it was possible to reduce the window of vulner-
ability by simply reordering the code, without changing the
functionality or incurring overhead. An example of this is
moving modiﬁcations of critical variables to the end of the
hypercall so that there is minimal code to execute between the
state changes and the completion of the hypercall. Altogether,
these changes for handling non-idempotent hypercalls increase
the recovery rate from 84% to 96%.
V. NILIHYPE IMPLEMENTATION
The starting point for the NiLiHype implementation is the
enhanced ReHype implementation described in Section IV.
Some of the features that are common to NiLiHype and
ReHype are: 1) VMs are suspended and interrupts are disabled
during recovery; 2) almost all
the ReHype enhancements
described in [19], [21]; 3) all the enhancements described in
Section IV.
As mentioned in Subsection III-C, since NiLiHype does not
involve reboot, some of the most complex and time-consuming
operations required by ReHype are not needed. Speciﬁcally,
these include hardware initialization as well as operations
to preserve and later re-integrate state from the pre-recovery
hypervisor instance. These operations are described in detail in
Section 3 of [19] and their latencies are presented in Section 10
of [21]. Similar latency measurements, for our enhanced
ReHype implementation, are presented in Subsection VII-C.
This section focuses on additional enhancements needed by
NiLiHype to overcome CLR challenges that are resolved by
the reboot in ReHype. Subsection V-A presents the enhance-
ments. Section V-B presents the measurement-based incremen-
tal development of the NiLiHype-speciﬁc enhancements.
A. Enhancements Required by NiLiHype
With ReHype, a very low recovery rate (5.6%) is achieved
without any enhancements [19]. That recovery is even possible
with the basic scheme is due to the operations performed by
the reboot, that include re-initializing the hardware and ini-
tializing a new, valid hypervisor memory state. As mentioned
in Subsection III-C, with just the basic NiLiHype mechanism
(discard all hypervisor threads of execution), recovery never
succeeds. Hence, the enhancements of the basic scheme, that
resolve the CLR challenges (Section II), are even more critical
in NiLiHype.
One of the enhancements developed for NiLiHype is needed
to bring the hardware to a consistent state: reprogram hard-
ware timer. Four additional enhancements deal with hypervisor
memory state. All of these enhancements are described below.
Reprogram hardware timer. Xen relies on the hardware
timer in the interrupt controller (APIC) to trigger the exam-
ination of the software timer heap. The handler reprograms
the APIC timer to ﬁre again at a time determined by the
top node of the timer heap. If the fault occurs after the
APIC timer has ﬁred but before Xen reprograms it, without
additional mechanisms, the APIC timer will never ﬁre again
after recovery. NiLiHype handles this issue by ensuring that
each CPU reprograms its APIC timer before resuming normal
operation.
Clear IRQ count. In Xen, each CPU maintains a per-
CPU variable named local irq count that records the nesting
level of interrupts. When the CPU enters or leaves an inter-
rupt handler, local irq count is, respectively, incremented or
decremented. The local irq count value is used in hypervisor
assertions to check whether the CPU is currently servicing an
interrupt. As NiLiHype discards all the execution threads in
the hypervisor, the local irq count variables of all the CPUs
are set to zero during the recovery.
Ensure consistency within scheduling metadata. Xen
maintains scheduling metadata that includes: (1) the runqueue
of each CPU, which is a linked list of vCPUs; (2) per-
CPU variables indicating the current executing vCPU; and
(3) per-vCPU variables representing the execution states of
the vCPUs. Hypervisor failure followed by recovery can
easily leave this scheduling metadata in an inconsistent state.
Inconsistencies within the scheduling metadata can cause
the hypervisor to incorrectly restore the register context of
one vCPU when another vCPU is scheduled to run. Such
inconsistencies can also result in the failure of assertions in
the scheduling routine, leading to hypervisor failure.
Resolving potential scheduling metadata inconsistencies is
done based on two key ideas: 1) where possible, initialize the
data to a ﬁxed valid value instead of relying on the existing
value; 2) if it is necessary to use existing data, pick the most
reliable source and make the rest of the metadata consistent
with that.
With NiLiHype, we encountered a particularly critical
problem related to scheduling metadata inconsistencies. The
information regarding which vCPU is currently running on
each CPU is stored redundantly in multiple places. Speciﬁ-
cally, it is stored in the per-CPU structures as well as two
different locations in the per-vCPU structures. To resolve the
inconsistencies, the information in the per-CPU structures is
used to set the information in all the per-vCPU structures.
Unlock static locks. Since both ReHype and NiLiHype
effectively discard all threads of execution in the hypervisor,
all locks should be in their unlocked state following recovery.
ReHype includes a mechanism to release all the locks stored in
the heap. NiLiHype uses the same mechanism. With ReHype,
locks in the static data segment (“static locks”) are initialized
to their unlocked state during boot. NiLiHype requires an
additional mechanism to release all such locks.
NiLiHype avoids a complex mechanism for tracking the
static locks. Instead, NiLiHype takes advantage of the fact
that, in Xen, all the static locks are deﬁned using a macro. We
modiﬁed the linker script used to build the Xen image and the
macro deﬁning locks to put all the static locks in a separate
segment in the Xen image, effectively placing them all in one
array. During the recovery process, before multiple CPUs are
allowed to execute, the CPU that detects the error iterates over
all the locks in the segment and unlocks any locked locks.
Reactivate recurring timer events. Xen uses several recur-
ring timer events. These include events to synchronize system
119
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
SUMMARY OF MECHANISMS TO ENHANCE NILIHYPE
TABLE I
Mechanism
Successful Recovery Rate
Basic
+ Clear IRQ count
+ Enhanced with ReHype
mechanisms
+ Ensure consistency within
scheduling metadata
+ Reprogram hardware timer
+ Unlock static locks
+ Reactivate recurring timer
events
0%
16.0% ± 2.3%
51.8% ± 3.1%