[49] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018.
Byzantine-robust distributed learning: Towards optimal statistical rates. In Pro-
ceedings of ICML. 5650–5659.
[50] Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. 2020.
Batchcrypt: Efficient homomorphic encryption for cross-silo federated learning.
In Proceedings of USENIX ATC. 493–506.
[51] Qiao Zhang, Chunsheng Xin, and Hongyi Wu. 2021. GALA: Greedy ComputAtion
for Linear Algebra in Privacy-Preserved Neural Networks. In Proceedings of NDSS.
[52] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep Leakage from Gradients. In
Proceedings of NeurIPS.
A ADDITIONAL EXPERIMENTAL RESULTS
A.1 Performance of SecureFL’s building blocks
In Tables 4 and 5, we plot in detail the communication and compu-
tational overhead of each step of SecureFL.
57ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Meng Hao, et al.
As shown in Table 4, the validity checking and cosine computa-
tion steps occupy most of the computation overhead. With devised
matrix multiplication preprocessing in the preamble phase, the
online overhead of cosine similarity computations only requires
less than 13 ms. For the weighted aggregation step, it only takes
less than 230 ms, since the same multiplication triples generated in
the validity checking phase will be reused in this step. Beneficially,
compared with traditional methods that generate fresh masks for
each calculation, the cost of the validity checking and weighted
aggregation steps is reduced by about half. Besides, our SecureFL
is fully robust to party dropping, where the computational over-
head decreases proportionally as the number of dropped parties
increases. Below we focus on the communication performance of
SecureFL. In Table 5, the validity checking step produces relatively
high communications compared to other operations. Nevertheless,
similar to the execution time we discussed above, it significantly
saves the cost of the weighted aggregation step by reusing the
same masks to local gradients, i.e., less than 1 KB under 100 parties
setting. More importantly, in the cosine similarity evaluation, the
communication overhead is low and remains constant as the num-
ber of parties increases. Besides, as the number of dropped parties
increases, the communication overhead decreases proportionally.
A.2 Additional Robustness Evaluations
We simulated Non-IID dataset using the method of [18] [11]. Specif-
ically, assuming M classes in the classification task, we evenly split
the parties into M groups. We assign a sample with label m to
the m-th group with probability β, and to any other group with
probability 1−β
M−1. Informally, the value of β controls the degree of
Non-IID-ness. β = 1
M represents IID data distributions, and the
higher β is, the more likely the parties hold samples from only one
class.
We evaluate the impact of the degree of Non-IID-ness on the
test error on MNIST dataset. The fraction of malicious parties is
fixed as 20% and the number of parties is fixed as 100. Figure 7(a)
shows, when the degree of Non-IID-ness of seed dataset varies,
the test error of our method under different attacks, i.e., no attack,
label flipping attack and local model poisoning attack. We observe
that SecureFL is accurate and robust when the degree of Non-IID-
ness is not significant. In particular, when β ≤ 0.4 for seed dataset,
SecureFL realizes the test errors comparable to FedAvg without any
attacks. Besides, we also study the impact of the Non-IID parties’
local datasets in Figure 7(b). Our results show that when β ≤ 0.6
for parties’ datasets, SecureFL achieves excellent performance, i.e.,
the test error is less than 0.1. Therefore, SecureFL works well when
the parties’ data distribution does not differ too much.
B SECURITY PROOFS
B.1 Proof of Theorem 1
Proof . Our security proof follows the ideal-world/real-world para-
digm: in real-world, SP and CS interact according to the protocol
specification, whereas in ideal-world they have access to a ideal
functionality FmatMulPre. The executions in both worlds are coor-
dinated by the environment Env, who chooses the inputs to CS
and SP and plays the role of a distinguisher between the real and
(a) Test error under Non-IID root dataset. (b) Test error under Non-IID parties’ local
dataset.
Figure 7: Impact of the Degree of Non-IID-ness on the Test
Error.
ideal executions. We will show that the real-world distribution is
computationally indistinguishable to the ideal-world distribution.
Proof of indistinguishability with corrupted SP. Below, we first
construct an ideal-world simulator Sim that performs as follows:
• Sim receives дs from the environment Env, and sends it to
FmatMulPre and gets the result u′.
• Sim encrypts u′ using SP’s public key and returns ˜u′ =
PLHE.Enc(pk, u′) to SP.
• Sim outputs whatever SP outputs.
Then, we show that the view Sim simulates for SP is indistin-
guishable from the view of SP interacting in the real execution.
The message PLHE.Dec(˜u′) is same as u in real execution, where
u = ⟨R⟩1дs − δ. Thus, they are indistinguishable even if the private
key sk is observed. In addition, it does not reveal any information
about ⟨R⟩1 from u, since δ is randomly chosen. Therefore, we claim
that the output distribution of Env in real-world is computationally
indistinguishable from that in ideal-world.
Proof of indistinguishability with corrupted CS. Below, we first
construct an ideal-world simulator Sim that performs as follows:
• Sim receives ⟨R⟩1 and δ from the environment Env, and
sends it to FmatMulPre.
• Sim constructs ˜дs ′ ← PLHE.Enc(pk, 0), and gives it to CS.
• Sim outputs whatever CS outputs.
Then, we show that the view ˜дs ′ Sim simulates for CS is indis-
tinguishable from the view PLHE.Enc(pk, дs) of CS interacting in
the real execution, because of the semantic security of PLHE. Thus,
the output distribution of Env in real-world is computationally
indistinguishable from that in ideal-world.
□
B.2 Proof of Theorem 2
Proof . We construct a simulator Sim simulates the view of cor-
rupted SP, which consists of her input/output and received mes-
sages. The simulator for CS should be the same. Sim proceeds as
follows:
• In the validity checking, Sim calls simulators SimFMult(⟨дi⟩),
SimFDReLU(⟨∥дi ∥2⟩) and SimFAND(⟨f laдi,0⟩B , ⟨f laдi,1⟩B) for
each i ∈ [n], and appends their output to the general view.
• In the cosine similarity computation, Sim calls FmatMulPre
simulator SimFmatMulPre(⟨R⟩1, дs), and computes the matrix
58Efficient, Private and Robust Federated Learning
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Table 4: Execution Time of Each Step of SecureFL. The data size is fixed as 10K entries. Different rows show different
numbers of users/dropped users. The values in "CosineComp" column represent preamble/online costs.
UsersNum Dropout
SharingGen ValidityCheck
100
100
100
300
300
300
0%
10%
20%
0%
10%
20%
20 ms
20 ms
20 ms
61 ms
60 ms
61 ms
9730 ms
8739 ms
7725 ms
31530 ms
26213 ms
24033 ms
CosineComp
6804 ms / 10 ms
6244 ms / 10 ms
5637 ms / 9 ms
16219 ms / 12 ms
14735 ms / 13 ms
12929 ms / 11 ms
TrustScore WeightedAgg TotalTime
16723 ms
15163 ms
13528 ms
48150 ms
41329 ms
37317 ms
67 ms
62 ms
54 ms
230 ms
213 ms
190 ms
92 ms
88 ms
83 ms
98 ms
95 ms
93 ms
Table 5: Communication Cost of Each Step of SecureFL. The data size is fixed as 10K entries. Different rows show different
numbers of users/dropped users. The values in "CosineComp" column represent preamble/online costs.
UsersNum Dropout
100
100
100
300
300
300
0%
10%
20%
0%
10%