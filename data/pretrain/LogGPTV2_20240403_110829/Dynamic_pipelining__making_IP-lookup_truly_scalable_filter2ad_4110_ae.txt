TCAM because it represents a lower bound on memory size (it is the
size for storing only all the preﬁxes.)
Because the total memory requirement is independent of the
line-rate, we need to vary only the routing-table size while evaluat-
ing the memory requirement of various schemes. Figure 10(c) shows
the worst-case total memory size plotted against the routing-table
size, for the various schemes. For a routing table of 1 million pre-
ﬁxes the node sizes for DLP and HLP are 80 bits and 100 bits
respectively, whereas for SDP the size is 72 bits because each node
must also budget for the jump bits. For a routing-table size of one
million preﬁxes DLP and HLP require memories as large as 84 MB
and 75 MB respectively, whereas SDP and TCAM require 22 MB
and 6 MB respectively. Across all routing-table sizes, DLP and HLP
require roughly the same amount of memory, whereas SDP requires
four times smaller memory on average. The memory requirement of
TCAM is, on average, another factor-of-four smaller than SDP. We
see that DLP and HLP do not scale well in worst-case total memory
size as the number of preﬁxes increase.
6.3 Power Dissipation
Because the power dissipated in accessing a memory varies with
both the size of the memory and the rate of access, we must vary
both the routing-table size and the line-rate when evaluating the
power dissipation of various schemes. Recall that we are evaluating
worst-case power dissipations. TCAMs activate all memory loca-
tions in a single access in the worst-case, therefore their power dissi-
pation is expected to be much higher than that of the trie-based
schemes. We expect HLP’s dissipation to be large because it hard-
ware-pipelines the memory aggressively. Recall that DLP does not,
therefore we expect its power dissipation to be small. However, for
the same reasons, DLP cannot achieve high line-rates such as 160
Gbps. We expect the power dissipation of SDP to be slightly smaller
than that of DLP because of a smaller memory size.
Figure 11 shows the power dissipation plotted against the line-
rate, for the various schemes. Because we must also vary the mem-
ory size while evaluating power dissipation, we present three sepa-
rate graphs for three different routing-tables sizes: (a) 250,000
HLP
DLP
TCAM
SDP
20
15
10
)
2
m
c
(
a
e
r
A
l
a
t
o
5T
HLP
DLP
TCAM
SDP
30
20
)
2
m
c
(
a
e
r
A
l
a
t
o
10T
HLP
DLP
TCAM
SDP
60
40
)
2
m
c
(
a
e
r
A
l
a
t
o
20T
2.5
10
40
Line-Rate (Gbps)
160
2.5
10
40
Line-Rate (Gbps)
160
(a)
(b)
2.5
10
40
Line-Rate (Gbps)
160
(c)
Fig. 12.  Comparison of chip area versus line-rate for various schemes with table sizes of (a) 250,000 (b) 500,000 (c) 1 million prefixes
preﬁxes (b) 500,000 preﬁxes, and (c) 1 million preﬁxes. In the evalu-
ation of hardware-pipelined memories, we ignore the area and
power overhead of pipeline latches, giving an unfair advantage to
HLP. SDP uses hardware-pipelining to a much smaller extent than
HLP, therefore its advantage is minimal.
Observe that the results for all routing-table sizes are qualita-
tively similar, therefore we comment only on the results correspond-
ing to 1 million preﬁxes (Figure 11(c)). Due to TCAM’s brute-force
searching it dissipates as much 42 W at 40 Gbps line-rate, and 174
W at 160 Gbps line-rate. For k = 8, HLP needs to aggressively pipe-
line the memory, even more so for high line-rates. When dealing
with 160 Gbps line-rate, HLP must access the memory every 0.25
ns. We see that pipelining 75 MB of SRAM to such depth dissipates
prohibitive amounts of power. HLP dissipates as much as 25 W for
40 Gbps line-rate, and 146 W for 160 Gbps line-rate. Note that,
because DLP does not scale to 160 Gbps line-rate, its 160 Gbps
data-point is absent in all graphs. DLP dissipates 10 W at 40 Gbps
line-rate. For 160 Gbps line-rate, SDP hardware-pipelines the indi-
vidual memory-stages, albeit to a less extent than HLP, and hence
incurs some penalty in power dissipation. SDP dissipates 5.5 W for
40 Gbps line-rate, and 22 W for 160 Gbps line-rate. The difference
in power dissipation between SDP and DLP is primarily due to
memory size, whereas between DLP and HLP it is primarily due to
aggressive hardware-pipelining.We see that HLP and TCAM do not
scale well in power dissipation as the routing-table sizes and line-
rates increase.
6.4 Implementation Cost
The cost of implementing chips in silicon is proportional to
approximately the fourth power of their area [6]. Hence, we evaluate
the chip area of various schemes to ascertain their scalability in
implementation cost. Although the total memory requirement of
TCAM is fairly small, we do not expect its chip area to be as small
because, for circuit-level reasons, CAM-styled memories cannot be
designed to have the same high density as RAM. In the absence of
hardware pipelining, the area taken up by a memory is proportional
to its size in bytes. In the presence of hardware-pipelining the area
grows exponentially with the depth of pipelining. Because DLP does
not hardware-pipeline the memory, we expect its chip area to stay
constant across line-rates. For small values of line-rates, we do not
expect any of the schemes to incur any signiﬁcant hardware-pipelin-
ing. Therefore for low line-rates, we expect DLP, HLP and SDP to
take up chip areas in accordance with their memory sizes (i.e., we
expect DLP and HLP to take up similar areas and SDP to take up
four times lesser area than theirs). For high line-rates, we expect the
area of HLP to grow in comparison to DLP due to hardware-level
pipelining. We also expect the area of SDP to increase at high line-
rates, however not by the same trend as HLP because SDP’s hard-
ware-pipelining is not nearly as aggressive as HLP.
Figure 12 shows the chip area plotted against the line-rate, for
the various schemes. Recall that we ignore the area and power over-
head of pipeline latches in HLP, giving it an unfair advantage.
Because we must also vary the memory size while evaluating chip
area, we present three separate graphs for three different routing-
tables sizes: (a) 250,000 preﬁxes (b) 500,000 preﬁxes, and (c) 1 mil-
lion preﬁxes.
We see that the results for all routing-table sizes are qualitatively
similar, therefore we comment only on the results corresponding to
500,000 preﬁxes (Figure 12(b)). For 40 Gbps line-rate HLP takes up
20.8 cm2, and for 160 Gbps it takes more than 150 cm2 of chip area.
This drastic increase occurs because HLP must access the memory
every 0.25 ns, and in order to achieve such an access rate, the mem-
ory array must be split to an extremely ﬁne extent. The chip area of
DLP stays constant at 13.3 cm2 for all line-rates except 160 Gbps.
Because DLP does not scale to 160 Gbps line-rate, its 160 Gbps
data-point is absent in all graphs. SDP takes up an area of 6.3 cm2 at
40 Gbps, whereas at 160 Gbps it takes up an area of 7.5 cm2. We see
that the SDP’s area is larger for high line-rates due to hardware-
pipelining. TCAM takes up about 4.1 cm2 for all line-rates. We see
that the chip area of TCAM, unlike its memory size as evaluated in
Section 6.2, is not smaller than that of SDP. Note that because we do
not model the priority-encoder in TCAM, its evaluated area is a con-
servative result and we expect the actual area to be larger. We see
that HLP does not scale well in implementation cost.
6.5 Cost of Route-Updates
TCAMs can be updated efﬁciently by using techniques like [13].
[1] proposes, for DLP, a number of optimizations for fast, incremen-
tal route-updates, however all the optimizations are heuristics and
improve only the average-case update-cost. The worst-case route-
update cost of DLP remains unbounded even with the optimizations.
HLP also has an unbounded worst-case route-update cost.
However, the route-update scheme of Tree Bitmap [4] can be
applied to HLP and DLP to reduce the route-update cost. This
scheme, which is the best to date, requires the update of only one trie
node in the worst-case. This one node may contain 2k pointers in the
worst-case, where k is the largest stride in the trie. For small values
of k (e.g., 2 or 3), a single memory write may be wide enough to suf-
ﬁce, but for larger values of k (e.g., 6 or 8) Tree Bitmap may require
multiple memory writes. In addition to writing the trie node upon a
route-update, Tree Bitmap can incur substantial worst-case memory
management overhead as mentioned in Section 3.3.5. Accounting
for this overhead, the eventual worst-case route-update cost can
exceed 100 memory operations. Hence Tree Bitmap does not scale
well in worst-case route-update cost. In addition, it incurs the pen-
alty of almost doubling the size of each trie node due to the absence
of
above
(Section 6.2 through Section 6.4), we do not penalize the total mem-
ory size of HLP and DLP. The results presented in those sections
must be viewed with the understanding that HLP and DLP do not
scale in route-update cost. If HLP and DLP incorporate Tree Bitmap
to achieve a better worst-case route-update cost, their worst-case
total memory sizes would be almost twice as large than the sizes
shown in Section 6.2. Yet, even with Tree Bitmap their worst-case
route-update cost could exceed 100 memory operations.
leaf pushing (Section 3.2.2).
In our
evaluations
In contrast, the worst-case route-update cost in SDP is provably
optimum, as it amounts to exactly and only one write-bubble. Owing
to the uniform size of its trie nodes, SDP does not need complex
memory management schemes with compaction and defragmenta-
tion. Hence, we see that SDP scales well in worst-case route-update
cost.
6.6 Summary of Results
We see that dynamic pipelining is the only IP-lookup scheme
that is truly scalable in routing-table size, lookup throughput, imple-
mentation cost, power dissipation, and routing-table update cost. In
contrast, all other IP-lookup schemes do not scale well in a number
of these requirements. HLP does not scale well in total memory size,
power dissipation, route-update cost, and implementation cost. DLP
does not scale well in total memory size, lookup throughput, and
route-update cost. TCAMs do not scale well in implementation cost
and power dissipation. For a routing-table of 1 million preﬁxes, and
a line-rate of 160 Gbps, HLP requires 75 MB, dissipates 146 W, and
takes up more than 200 cm2. TCAM requires 6 MB, dissipates 174
W, and takes up 8.9 cm2. DLP requires 88 MB, dissipates 10 W,
takes up 27 cm2 and fails to work beyond 40 Gbps. In contrast, SDP
requires only 22 MB of memory, dissipates 22 W, and takes up 14.9
cm2.
7 Conclusions
A truly scalable IP-lookup scheme must address ﬁve challenges
of scalability, namely: routing-table size, lookup throughput, imple-
mentation cost, power dissipation, and routing-table update cost.
Though several IP-lookup schemes have been proposed in the past,
all of the schemes satisfy only two or three of the requirements but
not all ﬁve. Previous schemes pipeline tries by mapping trie levels to
pipeline stages. We made the fundamental observation that because
this mapping is static and oblivious of the preﬁx distribution, the
schemes do not scale well when worst-case preﬁx distributions are
considered. This paper is the ﬁrst to meet all the ﬁve requirements in
the worst case. We proposed scalable dynamic pipelining (SDP)
which includes three key innovations: (1) We map trie nodes to pipe-
line stages based on the node height, which succinctly provides suf-
ﬁcient information about the distribution. Our mapping enables us to
prove a worst-case per-stage memory bound which is signiﬁcantly
tighter than those of previous schemes. (2) We exploit our mapping
to propose a novel scheme for incremental route-updates. In our
scheme a route-update requires exactly and only one write dis-
patched into the pipeline. This route-update cost is obviously the
optimum and our scheme achieves the optimum in the worst case.
(3) We achieve scalability in throughput by simultaneously pipelin-
ing at the data-structure level and hardware level. SDP naturally
scales in power and implementation cost. Using detailed hardware
simulation, we showed that SDP is the only scheme that achieves all
the ﬁve scalability requirements. Our results conﬁrm that schemes
like SDP will be necessary for future routers to keep up with the
scaling trends of the Internet.
References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
Anindya Basu and Girija Narlikar. Fast Incremental Updates for
Pipelined Forwarding Engines. In Proceedings of INFOCOM ‘03,
2003.
CACTI. http://research.compaq.com/wrl/people/jouppi/CACTI.html
M. Degermark, A. Brodnik, S. Carlsson and S. Pink. Small Forward-
ing Tables for Fast Routing Lookups. In Procedings of SIGCOMM
‘97 1997.
W. Eatherton, Z. Dittia, G. Varghese. Tree Bitmap: Hardware/Soft-
ware IP Lookups with Incremental Updates. ACM SIGCOMM
Compter Communication Review, 34(2) 97-122, 2004.
Mathew Gray. Internet Growth Summary. http://www.mit.edu/peo-
ple/mkgray/net/internet-growth-summary.html, 1996.
J. L. Hennessy and D. A. Patterson. Compter Architecture: a Quan-
titative Approach. Morgan Kaufman Publishers. 2002
V. Kumar, T. Lakshman and D. Stiliadis. Beyond Best Effort: Router
Architectures for Differentiated Services of Tomorrow’s Internet.
IEEE Communications Magazine 36(5) 152-164, 1998
Integrated Device Technology, Inc. http://www.idt.com.
D. R. Morrison. PATRICIA - Practical Algorithm to Retrieve Infor-
mation Coded in Alphanumeric. Journal of the ACM, 15(4)514-534,
Oct. 1968.
NetLogic Microsystems, Inc. http://www.netlogicmicro.com.
S. Nilsson and G. Karlsson. Fast Address Look-up for Internet Rout-
ers. In Proceedings of The IEEE Conference on BroadBand Commu-
nications Technology. 1998.
Routing Information Service. http://www.ris.ripe.net.
D. Shah and P. Gupta. Fast Updating Algorithms for TCAMs. In
Proceedings of IEEE MICRO, 21(1)36-47, Feb 2001.
Sandeep Sikka and George Varghese. Memory-Efficient State Look-
ups with Fast Updates. In Proceedings of SIGCOMM ‘00, 2000
Timothy Sherwood, George Varghese and Brad Calder. A Pipelined
Memory Architecture for High Throughput Network Processors. In
Proceedings of the 30th Annual ISCA, pages 288-299, 2003.
K. Sklower. A Tree-Based Routing Table for Berkeley Unix. In Pro-
ceedings of the 1991 Winter Usenix Conference. 1991.
V. Srinivasan and George Varghese. Fast Address Lookups Using
Controlled Prefix Expansion. ACM Transactions on Computer Sys-
tems, 17(1):1–40, February 1999.
Alan Tammel. How to Survive as an ISP. In Proceedings of Net-
world Interop 97, 1997.
F. Zane, G. Narlikar and A. Basu. CoolCAMs: Power-Efficient
TCAMs for Forwarding Engines. In Proceedings of INFOCOM ‘03,
2003.