### 6.2 Memory Requirement

Ternary Content-Addressable Memory (TCAM) represents a lower bound on memory size, as it only stores the prefixes. Since the total memory requirement is independent of the line rate, we need to vary only the routing-table size when evaluating the memory requirement of different schemes. Figure 10(c) shows the worst-case total memory size plotted against the routing-table size for various schemes.

For a routing table with 1 million prefixes, the node sizes for DLP and HLP are 80 bits and 100 bits, respectively. For SDP, the node size is 72 bits because each node must also budget for jump bits. At this routing-table size, DLP and HLP require memories of 84 MB and 75 MB, respectively, while SDP and TCAM require 22 MB and 6 MB, respectively. Across all routing-table sizes, DLP and HLP require roughly the same amount of memory, whereas SDP requires, on average, four times less memory. The memory requirement of TCAM is, on average, another factor of four smaller than SDP. We observe that DLP and HLP do not scale well in terms of worst-case total memory size as the number of prefixes increases.

### 6.3 Power Dissipation

The power dissipated in accessing a memory varies with both the size of the memory and the rate of access. Therefore, we must vary both the routing-table size and the line rate when evaluating the power dissipation of different schemes. Recall that we are evaluating worst-case power dissipations. TCAMs activate all memory locations in a single access in the worst case, leading to much higher power dissipation compared to trie-based schemes. We expect HLP’s dissipation to be high due to its aggressive hardware-pipelining. DLP, which does not use hardware-pipelining, should have lower power dissipation but cannot achieve high line rates such as 160 Gbps. We expect the power dissipation of SDP to be slightly lower than DLP due to its smaller memory size.

Figure 11 shows the power dissipation plotted against the line rate for various schemes. We present three separate graphs for three different routing-table sizes: (a) 250,000 prefixes, (b) 500,000 prefixes, and (c) 1 million prefixes. In the evaluation of hardware-pipelined memories, we ignore the area and power overhead of pipeline latches, giving an unfair advantage to HLP. SDP uses hardware-pipelining to a much smaller extent than HLP, so its advantage is minimal.

For 1 million prefixes (Figure 11(c)), TCAM dissipates up to 42 W at 40 Gbps and 174 W at 160 Gbps. HLP, which aggressively pipelines the memory, dissipates 25 W at 40 Gbps and 146 W at 160 Gbps. DLP, which does not scale to 160 Gbps, dissipates 10 W at 40 Gbps. SDP, which hardware-pipelines individual memory stages to a lesser extent, dissipates 5.5 W at 40 Gbps and 22 W at 160 Gbps. The difference in power dissipation between SDP and DLP is primarily due to memory size, while the difference between DLP and HLP is due to aggressive hardware-pipelining. HLP and TCAM do not scale well in power dissipation as the routing-table sizes and line rates increase.

### 6.4 Implementation Cost

The cost of implementing chips in silicon is proportional to approximately the fourth power of their area [6]. Therefore, we evaluate the chip area of various schemes to assess their scalability in implementation cost. Although the total memory requirement of TCAM is small, its chip area is not expected to be as small due to circuit-level reasons; CAM-styled memories cannot be designed to have the same high density as RAM. In the absence of hardware pipelining, the area taken up by a memory is proportional to its size in bytes. With hardware-pipelining, the area grows exponentially with the depth of pipelining. DLP, which does not use hardware-pipelining, should have a constant chip area across line rates. For low line rates, we expect DLP, HLP, and SDP to take up chip areas in accordance with their memory sizes (DLP and HLP similar, SDP four times smaller). For high line rates, HLP’s area will grow due to hardware-pipelining, and SDP’s area will increase but not as drastically as HLP’s.

Figure 12 shows the chip area plotted against the line rate for various schemes. For 500,000 prefixes (Figure 12(b)), HLP takes up 20.8 cm² at 40 Gbps and more than 150 cm² at 160 Gbps. DLP’s area remains constant at 13.3 cm² for all line rates except 160 Gbps, where it fails to scale. SDP takes up 6.3 cm² at 40 Gbps and 7.5 cm² at 160 Gbps. TCAM takes up about 4.1 cm² for all line rates. The actual area of TCAM is likely larger due to the priority-encoder. HLP does not scale well in implementation cost.

### 6.5 Cost of Route-Updates

TCAMs can be updated efficiently using techniques like [13]. For DLP, [1] proposes optimizations for fast, incremental route-updates, but these only improve the average-case update cost. The worst-case route-update cost of DLP remains unbounded. HLP also has an unbounded worst-case route-update cost. However, the route-update scheme of Tree Bitmap [4] can be applied to HLP and DLP to reduce the route-update cost. This scheme, which is the best to date, requires updating only one trie node in the worst case. For small values of k (e.g., 2 or 3), a single memory write may suffice, but for larger values (e.g., 6 or 8), multiple memory writes are required. Additionally, Tree Bitmap incurs substantial worst-case memory management overhead, making the eventual worst-case route-update cost exceed 100 memory operations. Hence, Tree Bitmap does not scale well in worst-case route-update cost and almost doubles the size of each trie node.

In contrast, the worst-case route-update cost in SDP is provably optimal, requiring exactly one write-bubble. SDP does not need complex memory management schemes, making it scalable in worst-case route-update cost.

### 6.6 Summary of Results

Dynamic Pipelining (SDP) is the only IP-lookup scheme that is truly scalable in routing-table size, lookup throughput, implementation cost, power dissipation, and routing-table update cost. Other schemes do not scale well in several requirements. For a routing table of 1 million prefixes and a line rate of 160 Gbps, HLP requires 75 MB, dissipates 146 W, and takes up more than 200 cm². TCAM requires 6 MB, dissipates 174 W, and takes up 8.9 cm². DLP requires 88 MB, dissipates 10 W, takes up 27 cm², and fails beyond 40 Gbps. SDP requires only 22 MB of memory, dissipates 22 W, and takes up 14.9 cm².

### 7 Conclusions

A truly scalable IP-lookup scheme must address five challenges: routing-table size, lookup throughput, implementation cost, power dissipation, and routing-table update cost. While several IP-lookup schemes have been proposed, they satisfy only two or three of these requirements. Previous schemes pipeline tries by mapping trie levels to pipeline stages, which does not scale well with worst-case prefix distributions. This paper introduces Scalable Dynamic Pipelining (SDP), which meets all five requirements in the worst case. SDP includes three key innovations: (1) Mapping trie nodes to pipeline stages based on node height, providing a tighter worst-case per-stage memory bound. (2) A novel scheme for incremental route-updates, requiring exactly one write dispatched into the pipeline. (3) Simultaneous pipelining at the data-structure and hardware levels, achieving scalability in throughput, power, and implementation cost. Using detailed hardware simulations, we show that SDP is the only scheme that achieves all five scalability requirements, making it necessary for future routers to keep up with Internet scaling trends.

### References

[1] Anindya Basu and Girija Narlikar. Fast Incremental Updates for Pipelined Forwarding Engines. In Proceedings of INFOCOM '03, 2003.
[2] CACTI. http://research.compaq.com/wrl/people/jouppi/CACTI.html
[3] M. Degermark, A. Brodnik, S. Carlsson, and S. Pink. Small Forwarding Tables for Fast Routing Lookups. In Proceedings of SIGCOMM '97, 1997.
[4] W. Eatherton, Z. Dittia, G. Varghese. Tree Bitmap: Hardware/Software IP Lookups with Incremental Updates. ACM SIGCOMM Computer Communication Review, 34(2) 97-122, 2004.
[5] Mathew Gray. Internet Growth Summary. http://www.mit.edu/people/mkgray/net/internet-growth-summary.html, 1996.
[6] J. L. Hennessy and D. A. Patterson. Computer Architecture: a Quantitative Approach. Morgan Kaufman Publishers, 2002.
[7] V. Kumar, T. Lakshman, and D. Stiliadis. Beyond Best Effort: Router Architectures for Differentiated Services of Tomorrow’s Internet. IEEE Communications Magazine 36(5) 152-164, 1998.
[8] Integrated Device Technology, Inc. http://www.idt.com.
[9] D. R. Morrison. PATRICIA - Practical Algorithm to Retrieve Information Coded in Alphanumeric. Journal of the ACM, 15(4) 514-534, Oct. 1968.
[10] NetLogic Microsystems, Inc. http://www.netlogicmicro.com.
[11] S. Nilsson and G. Karlsson. Fast Address Look-up for Internet Routers. In Proceedings of The IEEE Conference on BroadBand Communications Technology, 1998.
[12] Routing Information Service. http://www.ris.ripe.net.
[13] D. Shah and P. Gupta. Fast Updating Algorithms for TCAMs. In Proceedings of IEEE MICRO, 21(1) 36-47, Feb 2001.
[14] Sandeep Sikka and George Varghese. Memory-Efficient State Lookups with Fast Updates. In Proceedings of SIGCOMM '00, 2000.
[15] Timothy Sherwood, George Varghese, and Brad Calder. A Pipelined Memory Architecture for High Throughput Network Processors. In Proceedings of the 30th Annual ISCA, pages 288-299, 2003.
[16] K. Sklower. A Tree-Based Routing Table for Berkeley Unix. In Proceedings of the 1991 Winter Usenix Conference, 1991.
[17] V. Srinivasan and George Varghese. Fast Address Lookups Using Controlled Prefix Expansion. ACM Transactions on Computer Systems, 17(1):1–40, February 1999.
[18] Alan Tammel. How to Survive as an ISP. In Proceedings of Networld Interop 97, 1997.
[19] F. Zane, G. Narlikar, and A. Basu. CoolCAMs: Power-Efficient TCAMs for Forwarding Engines. In Proceedings of INFOCOM '03, 2003.