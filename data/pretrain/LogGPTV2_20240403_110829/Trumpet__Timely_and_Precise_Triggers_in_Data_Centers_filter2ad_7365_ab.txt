of monitoring systems. We ask: Does there exist a design
for a monitoring system which can detect and report thou-
sands of events within a few milliseconds, where event detec-
tions are precise because the system processes every packet
(rather than, say, sampling), and event speciﬁcations can be
ﬂexible (permitting a range of spatial and temporal scopes
of event deﬁnition)? Such a monitoring system would be
Figure 1: Flow granularity in Trumpet event deﬁnition
especially useful for automatic diagnosis and control at mil-
lisecond timescales on timely reporting of trafﬁc anomalies,
ﬁne-grained ﬂow scheduling, pacing, trafﬁc engineering, VM
migration and network reconﬁgurations.
3. DEFINING EVENTS IN TRUMPET
Users of Trumpet deﬁne events using a variant of a match-
action language [38], customized for expressing events rather
than actions. An event in this language is deﬁned by two
elements: a packet ﬁlter and a predicate. A packet ﬁlter
deﬁnes the set of packets of interest for the speciﬁc event,
from among all packets entering, traversing, or leaving a data
center (or part thereof) monitored by Trumpet. Filters are ex-
pressed using wildcards, ranges or preﬁx speciﬁcations on the
appropriate packet header ﬁelds. If a packet matches multiple
ﬁlters, it belongs to set of packet for each of the correspond-
ing events. A predicate is simply a Boolean formula that
checks for a condition deﬁned on the set of packets deﬁned
by a packet ﬁlter; when the predicate evaluates to true, the
event is said to have occurred. A predicate is usually deﬁned
over some aggregation function expressed on per-packet vari-
ables; users can specify a spatio-temporal granularity for the
aggregation.
Elements of the event deﬁnition language. Each packet in
Trumpet is associated with several variables. Table 1 shows
the variables used for use cases discussed in this paper. It is
easy to add new variables, and we have left an exploration of
the expansion of Trumpet’s vocabulary to future work.
The predicate can be deﬁned in terms of mathematical
and logical operations on these variables, and aggregation
functions (max, min, avg, count, sum, stddev) of these
variables. To specify the granularity at which the aggregation
is performed, users can specify (a) a time_interval
over which the predicate is to be evaluated, and (b) a
flow_granularity that speciﬁes how to bucket
the
universe of packets, so that the aggregation function is
computed over the set of packets in each bucket within the
last time_interval.
The flow_granularity can take values such as
5-tuple1 or any aggregation on packet ﬁelds (such as
srcIP,dstIP and srcIP/24). Figure 1 shows two exam-
ples of counting the volume of four ﬂows at two different
ﬂow granularities (srcIP,dstIP and srcIP/24). At
srcIP,dstIP granularity, the ﬁrst two ﬂows are bucketed
15-tuple ﬁelds include source and destination IP, source and destination
port, and protocol. Although all examples here use 5-tuple, Trumpet ﬂow
granularity speciﬁcations can include other packet header ﬁelds (e.g., MAC,
VLAN).
192.168.1.10192.168.1.11, 3000192.168.1.0, 450010.1.1.0, 1500Flow granularity: srcIP,dstIPFlow granularity: srcIP/24Src: 192.168.1.10:4999Dst: 192.168.1.11:1024 Size: 1500Src: 192.168.1.10:5000Dst: 192.168.1.11:1024 Size: 1500Src: 192.168.1.11:5001Dst: 192.168.1.12:1024 Size: 1500Src: 10.1.1.15:5000Dst: 10.1.1.2:1024 Size: 1500192.168.1.11192.168.1.12, 150010.1.1.1510.1.1.2, 1500Table 1: Packet variables
Table 2: Example Event deﬁnitions (Filter, Predicate, Flow_granularity, Time_interval)
together, while the third ﬂow is in a separate bucket. At
srcIP/24 granularity, the ﬁrst three ﬂows are placed in
the same bucket. The flow_granularity speciﬁcation
makes event descriptions much more concise. For example,
to ﬁnd chatty VM pairs in a service, the operator can
deﬁne an event at the granularity of srcIP,dstIP without
needing to explicitly identify the actual IP addresses of the
communicating pairs. In some cases, these IP addresses may
not be known in advance: e.g., all the users of a datacenter
internal service. Additional examples of event deﬁnitions
with different flow_granularity are given below.
Example event deﬁnitions. Table 2 gives examples of some
event deﬁnitions, based on the discussion in Section 2.
The ﬁrst example is to ﬁnd heavy hitter ﬂows that send a
burst of packets to a rack (say 10.0.128.0/24). The user
of Trumpet would deﬁne the IP block as the ﬁlter, deﬁne a
predicate that collects the total number of bytes and checks
if the sum exceeds a threshold (say 125 KB as 10% of a
1G link capacity) in a 10 ms time interval and 5-tuple ﬂow
granularity. Notice that this event deﬁnition is expressed at
the scale of a rack (say 10.0.128.0/24), and can ﬂag any
5-tuple ﬂow whose behavior matches the predicate.
The second example detects a large correlated burst of
losses in any ﬂow of a service whose servers use a given IP
block (say 10.0.128.0/24). The user deﬁnes a predicate
that counts the number of packets for which is_lost2 and
is_burst [31] is simultaneously true and checks when the
count passes the threshold (the predicate is shown in Table 2).
Events with flow_granularity deﬁnitions over multiple
ﬂows can be used for coﬂow scheduling [9], guiding SDN rule
placement [41] and estimating the demand of FlowGroups
for rate limiting [33].
The next event in the table detects when a TCP connec-
tion experiences congestion. Speciﬁcally, the event predicate
checks if a connection does not receive most of the acks for
2The number of retransmissions over-estimates the number of lost packets.
A more accurate solution is more complex and needs careful RTO estima-
tion [3]
which it was waiting from the beginning of a measurement
epoch whose duration is the time_interval of the predi-
cate. This event tracks seq, ack and dup3 variables for all
TCP ﬂows in both directions for the current and previous
epochs. It computes the size of acked bytes in the current
epoch using the ack and dup of the other side of connection
and compares it against outstanding bytes in the last epoch
based on the ack and seq. Similar events can be used to de-
tect the violation of bandwidth allocation policies [4] at short
timescales, debug new variants of TCP congestion control
algorithms, and detect unresponsive VMs to ECN marks and
RTT delays.
A slightly different example (the fourth in Table 2) detects
if there is a load spike on a distributed service (as deﬁned
by an IP address block 10.0.128.0/24). In this example,
the predicate evaluates to true if the total volume of traf-
ﬁc to all destination servers within this IP block over a 10
ms time interval exceeds this threshold. For this event, the
flow_granularity is dstIP/24: the whole service. 4
Beyond the above examples, Trumpet can also be used on
other management tasks such as: (a) diagnosing reachability
problems between the VMs of two services by counting the
total packet losses among any source and destination pair
(similar queries can ﬁnd black holes [23, 58] and transient
connectivity problems in middleboxes [47]) and (b) ﬁnding
popular service dependencies of a tenant by checking if any
set of servers in a speciﬁc source IP/24 collectively send
more than 10GB per second of trafﬁc to a set of servers in a
destination IP/24 (a service IP range) (useful, for example, to
migrate their VMs to the same rack [27] or put load balancing
rules on ToRs with chatty services [19]).
3Duplicate acks show that a packet is received although it is not the one ex-
pected. dup increases by 1460 bytes for each dup-ack and decreases based
on acked bytes for each regular ack.
4To IP addresses of a service cannot be expressed in a single range, Trumpet
allows event speciﬁcations with multiple ﬁlters.
Variable Description volume the size in bytes of the packet payload ecn if the packet is marked with ECN rwnd the actual receiver advertised window ttl the packet’s time-to-live field rtt the packet round-trip time (as measured from the returning ACK) is_lost if the packet was retransmitted at least once is_burst if the packet occurred in a burst (packets from a flow with short inter-arrival time) ack latest ack seq maximum sequence number dup an estimate of the number of bytes sent because of duplicate acks     Example Event Heavy flows to a rack with IP range 10.0.128.0/24 dstIP=10.0.128.0/24, sum(volume)>125KB, 5-tuples, 10ms Large correlated burst & loss in any flow of a service on 10.0.128.0/24 srcIP=10.0.128.0/24, sum(is_lost & is_burst)>10%, 5-tuples, 10ms Congestion of each TCP flow Protocol=TCP, 1 - (ack - ack_lastepoch + dup) / (seq_lastepoch - ack_lastepoch) >0.5, 5- tuples, 10ms Load spike on a service at 10.0.128.0/24 port:80 (dstIP=10.0.128.0/24 and dstPort=80), sum(volume)>100MB, dstIP/24, 10ms Reachability loss between service A on 10.0.128.0/24 to B 10.20.93.0/24 (srcIP=10.0.128.0/24 and dstIP=10.20.93.0/24), sum(is_lost)>100, (srcIP and dstIP), 10ms Popular service dependencies for a tenant on 10.0.128.0/20 srcIP=10.0.128.0/20, sum(volume)>10GB, (dstIP/24 and srcIP/24), 1s  4. AN OVERVIEW OF TRUMPET
Trumpet is designed to support thousands of dynamically
instantiated concurrent events which can result in thousands
of triggers at each host. For example, a host may run 50
VMs of different tenants each communicating with 10 ser-
vices. To support different management tasks (e.g., account-
ing, anomaly detection, debugging, etc.), we may need to
deﬁne triggers on 10 different per-packet variables (e.g., in
Table 1) and over different time-intervals and predicates.
Trumpet consists of two components (Figure 2): the Trum-
pet Event Manager (TEM) at the controller and a Trumpet
Packet Monitor (TPM) at each end-host.
Users submit event descriptions (Section 3) to the TEM,
which analyzes these descriptions statically and performs
the following sequence of actions. First, based on the ﬁlter
description and on network topology, it determines which
end-hosts should monitor the event. Second, it generates
triggers for each end-host and installs the triggers at the TPM
at each host. A trigger is a customized version of the event
description that includes ﬁlters and predicates in ﬂow and
time granularity. However, the predicate in a trigger can
be slightly different from the predicate in the corresponding
event deﬁnition because the event description may describe
a network-wide event while a trigger captures a host local
event. Third, TEM collects trigger satisfaction reports from
host TPMs. TPMs generate satisfaction reports when a trigger
predicate evaluates to true. For each satisﬁed trigger, TEM
may poll other TPMs that run triggers for the same event in
order to determine if the network-wide predicate is satisﬁed.
An important architectural question in the design of Trum-
pet is where to place the monitoring functionality (the TPMs).
Trumpet chooses end-hosts for various reasons. Other ar-
chitectural choices may not be as expressive or timely. In-
network packet inspection using measurement capabilities at
switches lack the ﬂexibility to describe events in terms of per-
packet variables; for example, it is hard to get visibility into
variables like loss, burst, or RTT at the granularity of individ-
ual packets since switches see higher aggregate trafﬁc than
end-hosts. Alternatively, sending all packets to the controller
to evaluate event descriptions would result in signiﬁcantly
high overhead and might not enable timely detection.
Trumpet’s TPM monitors packets in the hypervisor where
a software switch passes packets from NICs to VMs and
vice versa. This choice leverages the programmability of
end-hosts, the visibility of the hypervisor into trafﬁc entering
and leaving hosted VMs, and the ability of new CPUs to
quickly (e.g., using Direct Data I/O [30]) inspect all pack-
ets at ﬁne time-scales. Finally, Trumpet piggybacks on the
trend towards dedicating CPU resources to packet switching
in software switches [24]: a software switch often runs all
processing for each packet in a single core before sending
the packet to VMs because it is expensive to mirror packets
across cores due to data copy and synchronization overhead
[14]. For exactly the same reason, and because it needs com-
plete visibility into all trafﬁc entering or leaving a host, TPM
is co-located with the software switch on this core. This trend
is predicated on increasing core counts in modern servers.
Figure 2: Trumpet system overview
Trumpet is designed for data centers that use software
packet demultiplexing, leveraging its programmability for
supporting complex event descriptions, extensibility to new
requirements, and resource elasticity to handle load peaks.
However, Trumpet can also be used in two ways in data cen-
ters where the NIC directly transfers trafﬁc to different VMs
(e.g., using kernel bypass, SR-IOV, or receive-side scaling).
The ﬁrst is mirroring trafﬁc to the hypervisor. New NICs
allow mirroring trafﬁc to a separate queue that is readable
by the hypervisor, using which Trumpet can evaluate trigger
predicates. Although this has CPU overhead of processing
packets twice (in the hypervisor and VMs), this still preserves
the goal of reducing packet latency at VMs. Moreover, be-
cause trigger evaluation is not on the packet processing path,
Trumpet is not constrained by bounds on packet delay, so
it may not need a dedicated core. We have evaluated Trum-
pet with the mirroring capability in Section 7.2. The second
is NIC ofﬂoading. With the advent of FPGA (e.g., Smart-
NIC[17]) and network processors at NICs [6], Trumpet can
ofﬂoad some event processing to NICs. For example, it can
ofﬂoad trigger ﬁlters in order to selectively send packet head-
ers to the hypervisor. As NIC capabilities evolve, Trumpet
may be able to evaluate simpler predicates within the NIC,
leaving CPU cores free to perform even more complex pro-
cessing tasks (e.g., understanding correlations across multiple
ﬂows) for which some of our techniques will continue to be
useful.
Trumpet also depends upon being able to inspect packet
headers, both IP and TCP, so header encryption could reduce
the expressivity of Trumpet.
The design of both TPM and TEM present signiﬁcant chal-
lenges. In the next two sections, we present the design of
these components, with a greater emphasis on the systems
and scaling challenges posed by precise and timely measure-
ment at the TPM. We also discuss the design of TEM, but a
complete design of a highly scalable TEM using, for example,
techniques from [45, 58], is beyond the scope of this paper.
5. TRUMPET PACKET MONITOR
The primary challenge in Trumpet is the design of the
Trumpet Packet Monitor (TPM). TPM must, at line rate: (a)
determine which trigger’s ﬁlter a packet matches, (b) update
statistics of per-packet variables associated with the trigger’s
predicate and (c) evaluate the predicate at the speciﬁed time
granularity, which can be as low as 10 milliseconds. For a
ControllerServerVMServerTrumpet Event Manager (TEM)InstalltriggersSatisfiedtriggersNetwork-widequeryVMHypervisorSoftware switchPacketServerTrumpet Packet Monitor (TPM)10G NIC, in the worst-case (small packet) rate of 14.8 Mpps,
these computations must ﬁt within a budget of less than 70ns
per packet, on average.
5.1 Design Challenges
The need for two-phase processing. Our event deﬁnitions
constrain the space of possible designs. We cannot perform
all of the three steps outlined above when a packet is re-
ceived. Speciﬁcally, the step that checks the predicate must
be performed at the speciﬁed time-granularity. For instance,
a predicate of the form (#packets for a ﬂow during 10ms is
below 10), can only be evaluated at the end of the 10ms inter-
val. Thus, TPM must consist of two phases: a computation
that occurs per-packet (phase one), and another that occurs at
the predicate evaluation granularity (phase two). Now, if we
target ﬁne time-scales of a few milliseconds, we cannot rely
on the OS CPU scheduler to manage the processing of the
two phases because scheduling delays can cause a phase to
exceed the per-packet budget discussed above. Hence, Trum-
pet piggybacks on the core dedicated to software switching,
carefully managing the two phases as described later.
Strawman Approaches. To illustrate the challenges under-
lying the TPM design, consider two different strawman de-
signs: (a) match-later, in which phase one simply records
a history of packet headers and phase two matches packets
to triggers, computes statistics of per-packet variables and
evaluates the predicate, and (b) match-ﬁrst, in which phase
one matches each incoming packet to its trigger and updates
statistics, and phase two simply evaluates the predicate.
Neither of these extremes performs well. With 4096 trig-
gers each of which simply counts the number of packets from
an IP address preﬁx at a time-granularity of 10ms, both op-
tions drop 20% of the packets at full 10G packet rate of 14.8
Mpps. Such a high packet rate at a server is common in NFV
applications [16], at higher bandwidth links and in certain
datacenter applications [36]. A monitoring system cannot