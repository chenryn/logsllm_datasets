if asynchronous cancellation is disabled. If asynchronous
cancellation is enabled the requesting thread will send a signal
to the target thread and a signal handler will mark the thread for
cancellation. This use of signals creates the possibility of a data
race: if a thread is in the process of requesting a cancellation
and the target thread disables asynchronous cancellation before
the requesting thread sends its signal, the target thread will be
forced to execute its cancellation handler while in an unexpected
6
state. To prevent this, before sending a signal, the requesting
thread uses a Compare and Exchange instruction that can ensure
TCB_CANCELING is false, TCB_CANCELTYPE is true and
set TCB_CANCELING to true atomically. pthread_cancel
performs this instruction in a loop until it succeeds.
Analogously, upon exiting a cancellation point, a thread
uses a Compare and Exchange instruction to both ensure
TCB_CANCELING is false and to set TCB_CANCELTYPE to
false. This instruction is also executed in a loop until it succeeds.
Therefore TCB_CANCELING is a Mutual Exclusion Device
(mutex) that prevents concurrently disabling asynchronous
cancellation and sending an asynchronous cancellation signal.
By setting TCB_CANCELING to true, an attacker can force
a thread to loop in __pthread_disable_asynccancel,
forever waiting for a signal that will never come.
Many
and these
cancellation
to
surrounded
system calls
by
and
__pthread_disable_asynccancel. A simpliﬁed
example of glibc’s implementation of open is presented below:
__pthread_enable_asynccancel
points
map
system calls
directly
are
__pthread_enable_asynccancel();
open syscall;
__pthread_disable_asynccancel();
Since glibc’s open function uses a cancellable system
call we can proﬁle a trampoline for open by setting
TCB_CANCELING to true and reading it off the stack when it
hangs in the __pthread_disable_asynccancel after
open.
Having identiﬁed a suitable mutex for MTB, we then
determine a way to locate it at runtime. As cancelhandling
is a ﬁeld of a thread’s TCB, given the base address of a TCB,
it is trivial to locate cancelhandling. In glibc every TCB
contains a header with the type tcbhead_t. The ﬁrst ﬁeld of
this structure is deﬁned as void *tcb; which is, actually, just
a pointer to itself. The fact that the TCB begins with a pointer
to itself makes it easily distinguishable in memory. Given an
8-byte value aligned at a known 8-byte aligned address, if the
address is equal to its contents the address might represent the
beginning of a TCB. In practice, for Nginx, the TCB is the only
value on the stack that satisﬁes this property. Thus, starting
from a known stack address, we can locate a thread’s TCB
by scanning backward for self-referential pointers. Once we
locate a thread’s TCB we can leverage cancelhandling to
execute MTB against the thread. Additionally, since all TCBs
are connected via linked list pointers, locating a single TCB
allows us to locate the TCBs of all other threads.
2) Proﬁling open: Using the mutex found in the previous
section, we can cause a thread of our choosing to hang at a
non-determinate system call. By modifying TCB_CANCELING
to false then true in quick succession, we can permit that
thread’s execution to continue and then stop again at a non-
determinate system call. As Nginx makes many system calls
that involve cancellation points, locating open requires the
ability to distinguish when a thread is blocked at open versus
when it is blocked at some other cancellable system call. We
distinguish these situations by exploiting knowledge of how
Nginx responds to requests for static ﬁles.
When Nginx receives an HTTP GET request for static
content, it transforms the requested path into a path on the
local ﬁlesystem. It calls open on this path and, if successful,
responds with the ﬁle’s contents. If open fails, it responds
with HTTP 404 Not Found. During this process, a pointer
to a string containing the path will be present on the stack. To
determine whether or not Nginx is blocked at an open call
we craft an HTTP request with a unique string and examine
all strings pointed to from Nginx’s stack.
void next_syscall(struct pthread *tcb)
{
/* Rapidly mutate the cancelhandling field
* to allow thread to proceed to next system
* call
*/
tcb->cancelhandling |= 4;
tcb->cancelhandling &= ˜4;
}
void *profile_open()
{
const char *fprint = "4a7ed3b71413902422846"
struct pthread *main_tcb = find_main_tcb();
while (next_syscall()) {
if (string_in_stack(rsp, fprint)) {
}
}
}
return *rsp
Fig. 5. Pseudocode for proﬁling open
If our string is sufﬁciently distinct (an example request is
provided below), we can easily determine whether or not the
current system call was made while processing our request.
In practice, if Nginx does not ﬁnd a requested ﬁle, the string
holding the path is discarded and no pointers will appear to
it on the stack in subsequent system calls. Thus, if we know
that both (1) Nginx is blocked at a system call and (2) Nginx’s
stack contains a pointer to our constructed string, we can be
sure Nginx is blocked at open. On average, we have been
able to locate open by inspecting under 50 blocked system
calls. The pseudocode for proﬁling open is shown in Fig 5.
GET /4a7ed3b71413902422846 HTTP/1.1
3) Proﬁling _IO_new_file_overflow: We proﬁle
_IO_new_file_overflow by taking advantage of glibc’s
implementation of the stdio FILE type. Every FILE
contains a ﬁle descriptor, pointers to the ﬁle’s buffers,
and a table of function pointers to various ﬁle operations.
_IO_new_file_overflow is included among these func-
tion pointers. By locating a valid FILE, we can easily locate
_IO_new_file_overflow as the ordering of functions
within the table is ﬁxed. Finding a valid FILE pointer in Nginx
proved to be a challenge as Nginx uses ﬁle descriptors instead
of FILE pointers. In this situation, scanning the stack will not
yield a pointer to a valid FILE object. To overcome this, we
locate glibc’s FILE for the standard output stream stdout.
stdout is a global variable and is always automatically
initialized on startup. Since stdout is a global variable deﬁned
by glibc, it is located in glibc’s data segment. Due to ASLR,
the location of glibc’s data segment cannot be known a priori;
nor can it be directly inferred from the address of the stack.
Additionally, Nginx does not keep many pointers to glibc
structures in local variables, meaning few pointers to glibc’s
data segment are on the stack. The attack is further complicated
by the fact that we cannot dereference random stack values
due to the risk of causing a segmentation fault.
Instead, we ﬁnd a pointer into the heap, which occur
more frequently in the stack. While pointers into the heap
are common, they are not easily distinguishable from non-
pointer values. To distinguish heap pointers we perform a
simple statistical analysis on the values of the stack, the details
of which we will present in a technical report for the sake of
brevity. Here we brieﬂy describe this analysis.
We collect 8-byte values from a 2-page range starting at the
bottom of the stack. We bin these values based on their top 48
bits; i.e., all values in the range (0, 0x0FFFFF) are placed in
the ﬁrst bin, all values in the range (0x100000, 0x1FFFFF) are
placed in the second bin, and so on. We then sort the bins by
their size. In our experiments, when Nginx’s stack is partitioned
in this fashion, the largest bin corresponds to non-pointers, the
second largest bin corresponds to stack pointers, and the third
large bin corresponds to heap pointers. This is due to the size of
the address space available to a 64-bit program; any individual
region of allocated memory will be several orders of magnitude
smaller than the distance between the regions causing clustering
of values. We found that, for Nginx, 2 pages of values collected
at a single point in time is enough to reliably distinguish heap
pointers. If, for some reason, we needed a higher degree of
precision, this technique could be extended to either collect
values at multiple points in time, or to collect values from more
pages of the stack.
Now that we have pointers into the heap, it becomes possible
for us to analyze the heap. We leverage this to ﬁnd a pointer
to main_arena, a glibc global variable. main_arena is a
structure used by glibc to maintain information on allocated
chunks of memory. To accelerate allocation operations, glibc
partitions chunks into pre-sized bins and stores them in
main_arena. Every heap chunk allocated via malloc,
calloc, or realloc is preﬁxed with metadata containing a
pointer back to the main_arena bin it came from. We take
advantage of this to locate a pointer into main_arena.
Starting from the smallest pointer in our bin of heap pointers,
we collect 8-byte values from a 20 page range of the heap. We
then ﬁlter out values unlikely to be pointers.
Our criteria for discarding non-pointers is described below.
1)
2)
3)
Discard all values that are not multiples of 8
Discard all values greater than 0x7FFFFFFFFFFF
Discard all values less than 0x1000
Finally, we partition the remaining values into bins of size
0x100000. The most common pointer of the largest bin will
be a pointer into main_arena. This is due to most chunks
of the heap being allocated out of the same bin.
Now that we have a pointer into glibc’s data section we
can search for stdout. We identify stdout by scanning
backwards from main_arena, and looking for a region
is both a valid FILE and has the value 1 for its
that
underlying ﬁle descriptor. At
the location of
_IO_new_file_overflow can be trivially read off of
stdout.
this point,
7
4) Corrupting the Nginx Task Queue: The main loop for Ng-
inx worker threads is located in ngx_thread_pool_cycle.
All new worker threads spin in this loop, checking if new tasks
have been added to their work queue. A simpliﬁed version of
this loop is presented below:
for (;;) {
task = queue_get(tp->task_queue);
task->handler(task->ctx, tp->log);
}
To carry out the attack, we leverage our MLR technique.
We craft a fake task structure in the region of the stack that
originally contained Nginx’s environment variables. At startup
Nginx copies these to a new location and the original location
goes unused.
equal
to
open
task->ctx
We initialize our fake task such that task->handler
to
points
and
also modify tp->log to
html/index.html. We
be
to (O_DIRECT | O_SYNC | O_WRONLY |
O_TRUNC). While this invalidates the tp->log pointer, in
practice, threads do not log unless Nginx is compiled in debug
mode. When the worker thread executes this task, it will
open the ﬁle in O_DIRECT mode, allowing us to perform an
FDMA attack.
points
Once we have our fake task structure, we can append it to
the task queue and wait for Nginx to execute the task. In most
cases, this happens instantaneously, so after a few seconds we
can be conﬁdent our call has occurred. We repeat this process
100 times so that there will be at least 100 ﬁle descriptors in
O_DIRECT mode opened by the Nginx process.
For the call to _IO_new_file_overflow, we begin by
creating a fake FILE that matches stdout except for the
following ﬁelds:
1)
2)
3)
4)
file->file__fileno = 75
file->file_IO_write_base =
file->vtable->__overflow &
file->file_IO_write_ptr =
file->file_IO_write_base + 0x1000
file->file_IO_read_end =
file->file_IO_write_base
0xFFF
Next, we modify our fake task such that task->handler points
to _IO_new_file_overflow and task->ctx points to our fake
FILE. We also modify tp->log to be -1 EOF. This will cause
_IO_new_file_overflow to think the write buffer overﬂowed
just as the end of the ﬁle was reached, so it will immediately ﬂush
the buffer via a write. Once we have crafted our fake arguments
we append the fake task to the task queue and wait for the task
to be executed. Conceptually _IO_new_file_overflow will be
executing the equivalent of the following code:
write(75,\_IO\_FILE\_Overflow & ˜0xFFF, 0x1000);
Which results in a dump from execute only memory into the
ﬁle html/index.html. We can then retrieve this page of
code by sending GET /index.html HTTP/1.1. We now
have the contents of a page of code at a known location and that
can be reused in arbitrary ways. If necessary, we can perform
this as many times as we want to leak more pages of memory.
Note that this is an optional and additional step to the initial
exploit detailed above. The initial exploit is completely address
8
oblivious, but further steps built on top of it can take advantage
of conventional ROP or even code injection techniques (after
disabling W⊕X) for ease of implementation.
B. Nginx Attack 2
We now illustrate the generality of our techniques by
performing a second attack against Nginx that both (1) targets
different functions and (2) corrupts a different call site.
This attack relies on invoking Nginx’s master process loop
from an attacker-controlled worker in order to trigger a speciﬁc
signal handler and cause arbitrary process execution. There are
three phases to this attack:
1)
2)
3)
Use proﬁling to get the address of the master process
loop.
Use MTB to corrupt a function pointer to point at the
master process loop.
Set global variables via MTB to cause the master
process loop to call exec under attacker-chosen
parameters.
For the sake of brevity, we describe the details of this attack
in Appendix A.
C. Apache Attack