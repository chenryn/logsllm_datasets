# BNL 算法的性能问题说完了 NLJ 算法的优化，我们再来看 BNL 算法的优化。我在上一篇文章末尾，给你留下的思考题是，使用 Block Nested-Loop Join(BNL)算法时，可能会对被驱动表做多次扫描。如果这个被驱动表是一个大的冷数据表，除了会导致IO 压力大以外，还会对系统有什么影响呢？在[第 33篇文章](https://time.geekbang.org/column/article/79407)中，我们说到InnoDB 的 LRU 算法的时候提到，由于 InnoDB 对 Bufffer Pool 的 LRU算法做了优化，即：第一次从磁盘读入内存的数据页，会先放在 old 区域。如果1 秒之后这个数据页不再被访问了，就不会被移动到 LRU 链表头部，这样对Buffer Pool 的命中率影响就不大。但是，如果一个使用 BNL 算法的 join语句，多次扫描一个冷表，而且这个语句执行时间超过 1秒，就会在再次扫描冷表的时候，把冷表的数据页移到 LRU 链表头部。这种情况对应的，是冷表的数据量小于整个 Buffer Pool 的 3/8，能够完全放入old 区域的情况。如果这个冷表很大，就会出现另外一种情况：业务正常访问的数据页，没有机会进入young 区域。由于优化机制的存在，一个正常访问的数据页，要进入 young 区域，需要隔 1秒后再次被访问到。但是，由于我们的 join语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页，很可能在 1秒之内就被淘汰了。这样，就会导致这个 MySQL 实例的 Buffer Pool在这段时间内，young 区域的数据页没有被合理地淘汰。也就是说，这两种情况都会影响 Buffer Pool 的正常运作。**大表 join 操作虽然对 IO 有影响，但是在语句执行结束后，对 IO的影响也就结束了。但是，对 Buffer Pool的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。**为了减少这种影响，你可以考虑增大 join_buffer_size的值，减少对被驱动表的扫描次数。也就是说，BNL 算法对系统的影响主要包括三个方面：1.  可能会多次扫描被驱动表，占用磁盘 IO 资源；2.  判断 join 条件需要执行 M\*N 次对比（M、N    分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源；3.  可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率。我们执行语句之前，需要通过理论分析和查看 explain结果的方式，确认是否要使用 BNL 算法。如果确认优化器会使用 BNL算法，就需要做优化。优化的常见做法是，给被驱动表的 join 字段加上索引，把BNL 算法转成 BKA 算法。接下来，我们就具体看看，这个优化怎么做？
# BNL 转 BKA一些情况下，我们可以直接在被驱动表上建索引，这时就可以直接转成 BKA算法了。但是，有时候你确实会碰到一些不适合在被驱动表上建索引的情况。比如下面这个语句：    select * from t1 join t2 on (t1.b=t2.b) where t2.b>=1 and t2.b```图 6 explain 结果]{.reference}```{=html}```![](Images/c6f56795a1ceff274b08521f3482df73.png){savepage-src="https://static001.geekbang.org/resource/image/d8/9c/d862bc3e88305688df2c354a4b26809c.png"}```{=html}```图 7 语句执行时间]{.reference}```{=html}```可以看到，explain 结果里 Extra 字段显示使用了 BNL算法。在我的测试环境里，这条语句需要执行 1 分 11 秒。在表 t2 的字段 b上创建索引会浪费资源，但是不创建索引的话这个语句的等值条件要判断 10亿次，想想也是浪费。那么，有没有两全其美的办法呢？这时候，我们可以考虑使用临时表。使用临时表的大致思路是：1.  把表 t2 中满足条件的数据放在临时表 tmp_t 中；2.  为了让 join 使用 BKA 算法，给临时表 tmp_t 的字段 b 加上索引；3.  让表 t1 和 tmp_t 做 join 操作。此时，对应的 SQL 语句的写法如下：    create temporary table temp_t(id int primary key, a int, b int, index(b))engine=innodb;insert into temp_t select * from t2 where b>=1 and b```图 8 使用临时表的执行效果```{=html}```可以看到，整个过程 3 个语句执行时间的总和还不到 1 秒，相比于前面的 1 分11 秒，性能得到了大幅提升。接下来，我们一起看一下这个过程的消耗：1.  执行 insert 语句构造 temp_t 表并插入数据的过程中，对表 t2    做了全表扫描，这里扫描行数是 100 万。2.  之后的 join 语句，扫描表 t1，这里的扫描行数是 1000；join    比较过程中，做了 1000 次带索引的查询。相比于优化前的 join 语句需要做    10 亿次条件判断来说，这个优化效果还是很明显的。总体来看，不论是在原表上加索引，还是用有索引的临时表，我们的思路都是让join 语句能够用上被驱动表上的索引，来触发 BKA 算法，提升查询性能。
# 扩展 -hash join看到这里你可能发现了，其实上面计算 10 亿次那个操作，看上去有点儿傻。如果join_buffer 里面维护的不是一个无序数组，而是一个哈希表的话，那么就不是10 亿次判断，而是 100 万次 hash查找。这样的话，整条语句的执行速度就快多了吧？确实如此。这，也正是 MySQL 的优化器和执行器一直被诟病的一个原因：不支持哈希join。并且，MySQL 官方的 roadmap，也是迟迟没有把这个优化排上议程。实际上，这个优化思路，我们可以自己实现在业务端。实现流程大致如下：1.  `select * from t1;`取得表 t1 的全部 1000 行数据，在业务端存入一个    hash 结构，比如 C++ 里的 set、PHP 的数组这样的数据结构。2.  `select * from t2 where b>=1 and b=X and t2.c>=Y and t3.c>=Z;现在为了得到最快的执行速度，如果让你来设计表 t1、t2、t3上的索引，来支持这个 join 语句，你会加哪些索引呢？同时，如果我希望你用 straight_join来重写这个语句，配合你创建的索引，你就需要安排连接顺序，你主要考虑的因素是什么呢？你可以把你的方案和分析写在留言区，我会在下一篇文章的末尾和你讨论这个问题。感谢你的收听，也欢迎你把这篇文章分享给更多的朋友一起阅读。
# 上期问题时间我在上篇文章最后留给你的问题，已经在本篇文章中解答了。这里我再根据评论区留言的情况，简单总结下。根据数据量的大小，有这么两种情况：-   \@长杰 和 \@老杨同志 提到了数据量小于 old 区域内存的情况；-   \@Zzz    同学，很认真地看了其他同学的评论，并且提了一个很深的问题。对被驱动表数据量大于    Buffer Pool 的场景，做了很细致的推演和分析。给这些同学点赞，非常好的思考和讨论。![](Images/48edcb93fb03e3e52d7e7099be6b5cb3.png){savepage-src="https://static001.geekbang.org/resource/image/09/77/09c1073f99cf71d2fb162a716b5fa577.jpg"}
# 36 \| 为什么临时表可以重名？今天是大年三十，在开始我们今天的学习之前，我要先和你道一声春节快乐！]{.orange}在上一篇文章中，我们在优化 join查询的时候使用到了临时表。当时，我们是这么用的：    create temporary table temp_t like t1;alter table temp_t add index(b);insert into temp_t select * from t2 where b>=1 and b```图 1 临时表特性示例]{.reference}```{=html}```可以看到，临时表在使用上有以下几个特点：1.  建表语法是 create temporary table ...。2.  一个临时表只能被创建它的 session 访问，对其他线程不可见。所以，图中    session A 创建的临时表 t，对于 session B 就是不可见的。3.  临时表可以与普通表同名。4.  session A 内有同名的临时表和普通表的时候，show create    语句，以及增删改查语句访问的是临时表。5.  show tables 命令不显示临时表。``{=html}由于临时表只能被创建它的 session 访问，所以在这个 session结束的时候，会自动删除临时表。也正是由于这个特性，**临时表就特别适合我们文章开头的join 优化这种场景**。为什么呢？原因主要包括以下两个方面：1.  不同 session 的临时表是可以重名的，如果有多个 session 同时执行 join    优化，不需要担心表名重复导致建表失败的问题。2.  不需要担心数据删除问题。如果使用普通表，在流程执行过程中客户端发生了异常断开，或者数据库发生异常重启，还需要专门来清理中间过程中生成的数据表。而临时表由于会自动回收，所以不需要这个额外的操作。