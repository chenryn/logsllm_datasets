shown in Figure 2. Before introducing the key components, we ﬁrst describe our design
principles.
Our main design principle is that HyperCheck should not rely on any software run-
ning on the machine except the boot loader. Since the software may be compromised,
one cannot trust even the hypervisor. Therefore, we use hardware – a PCI Ethernet card
– as a memory acquiring module and SMM to read the CPU registers. Usually, Ethernet
cards are PCI devices with bus master mode enabled and are able to read the physical
memory through DMA, which does not need help from CPU. SMM is an independent
operating mode and could be made inaccessible from protected mode which is what the
hypervisor and privileged domains run in.
Previous researchers only used PCI devices to read the physical memory. However,
CPU registers are also important because they deﬁne the location of active memory
used by the hypervisor or an OS kernel such as CR3 and IDTR registers. Without these
registers, the attacker can launch a copy-and-change attack. It means the attacker copies
the memory to a new location and modiﬁes it. Then the attacker updates the register to
point to the new location. PCI devices cannot read the CPU registers, thereby failing to
detect this kind of attacks. By using SMM, HyperCheck can examine the registers and
report the suspicious modiﬁcations.
Furthermore, HyperCheck uses the CR3 register to translate the virtual addresses
used by the kernel to the physical addresses captured by the analysis module. Since the
acquiring module relies on the physical address to read the memory, HyperCheck needs
to ﬁnd the physical addresses of the protected hypervisor and privileged domain. For
that purpose, HyperCheck checks both symbol ﬁles and CPU registers. From symbol
ﬁles, HyperCheck can read the virtual addresses of the target memory. Then, Hyper-
Check utilizes CPU registers to ﬁnd the physical addresses corresponding to the vir-
tual ones. Previous systems only used the symbol ﬁles to read the virtual addresses
and calculate the physical addresses. Such systems can not detect attacks that modify
page tables and leave the original memory untouched. Another possible way to get the
physical addresses without using registers, is to scan the entire physical memory and
164
J. Wang, A. Stavrou, and A. Ghosh
use pattern matching to ﬁnd all potential targets. However, this method is not scalable
or even efﬁcient especially since hypervisors and operating system kernels have small
memory footprint.
4.1 Acquiring the Physical Memory
In general, there are two ways to acquire the physical memory: a software method
and a hardware one. The former uses the interface provided by the OS or the hyper-
visor to access the physical memory, such as /dev/kmem on Linux [7] or \Device
\PhysicalMemory on Windows [37]. This method relies on the integrity of the under-
lying operating system or the hypervisor. If the operating system or the hypervisor is
compromised, the malware may provide a false view of the physical memory. Moreover,
these interfaces to access memory can be disabled in future versions of the operating
systems. In contrast, the hardware method uses a PCI device [8, 30] or other kinds of
hardware [6]. The hardware method is more reliable because it depends less on the
integrity of the operating system or the hypervisor.
We choose the hardware method to read the physical memory. There are also multi-
ple options for the hardware components such as a PCI device, a FireWire bus device or
customized chipset. We selected to use a PCI device because it is the most commonly
used hardware. Moreover, existing commercial Ethernet cards need drivers to func-
tion. These drivers normally run in the operating system or the driver domain, which
are vulnerable to the attacks and may be compromised in our threat model. To avoid
this problem, HyperCheck puts these drivers into the SMM code. Since the SMRAM
memory is going to be locked after booting, it will not be modiﬁed by the attacker. In
addition, to prevent the attacker from using a malicious NIC driver in the OS to spoof
the SMM driver, we use a secret key. The key is obtained from the monitor machine
when the target machine is booting up and then stored in the SMRAM. The key then
is used as a random seed to selectively hash a small portion of the data to avoid data
replay attacks.
Another class of attacks is denial of service(DoS) attacks. Such attacks aim to stop
or disable the device. For instance, according to ACPI [17] speciﬁcation, every PCI
device supports D3 state. This means that an ACPI-compatible device can be suspended
by an attacker who takes over the operating system: ACPI was designed to allow the
operating system to control the state of the devices. Of course, the OS is not a trusted
component in our threat model. Therefore, one possible attack is to selectively stop the
NIC without stopping any other hardware. To prevent ACPI DoS attacks, we need an
out-of-band mechanism to verify that the PCI card is not disabled. The remote server
that receives the state snapshots plays that role.
4.2 Translating the Physical Memory
In practice, there is a semantic gap between the physical memory that we monitor and
the virtual memory addressing used by the hypervisor. To translate the physical mem-
ory, the analysis module must be aware of the semantics of the physical memory layout
depends on the speciﬁc hypervisor we monitor. On the other hand, the acquiring module
may support many different analysis modules with no or small modiﬁcations.
HyperCheck: A Hardware-Assisted Integrity Monitor
165
The current analysis module depends on three properties of the kernel memory: lin-
ear mapping, static nature and persistence. Linear mapping means the kernel (OS or
hypervisor) memory is linearly mapped to physical memory and the physical addresses
are ﬁxed. For example, on x86 architecture, the virtual memory of Xen hypervisor is
linearly mapped into the physical memory. Therefore, in order to translate the physical
address to a given virtual address in Xen, we have to subtract the virtual address from an
offset. In addition, Domain 0 of Xen is also linear mapped to the physical memory. The
offset for Domain 0 is different on different machines but remains the same on a given
machine. Moreover, other operating system kernels, such as Windows [35], Linux [4]
or OpenBSD [12], also have this property when they are running directly on the real
hardware.
Static nature means the contents of the monitoring part of the hypervisor have to be
static. If the contents are changing, then there might be a time window between the CPU
changing the contents and our acquiring module reading them. This may result in in-
consistency for analysis and veriﬁcation. Persistence property means the memory used
by hypervisors will not be swapped out to the hard disk. If the memory is swapped out,
then we cannot identify and match any content by only reading the physical memory.
We would have to read the swap ﬁle on the hard disk.
The current version of HyperCheck relies on these three properties (linear mapping,
static nature and persistence ) to work correctly. Besides the Xen hypervisor, most op-
erating systems hold these three properties too.
4.3 Reading and Verifying the CPU Registers
Since the Ethernet card cannot read the CPU registers, we must use another method
to read them. Again, there are software and hardware based methods. For software
method, one could install a kernel module in the hypervisor and then it could obtain
registers by reading from the CPU directly. However, this is vulnerable to the rootk-
its, which can potentially modify the kernel module or replace it with a fake one. For
hardware method, one could use a chipset to obtain registers.
We choose to use SMM in x86 CPU which is similar to a hardware method. As we
mentioned earlier, SMM is a different CPU mode from the protected mode which the
hypervisor or the operating system reside in. When CPU switches to SMM, it saves the
register context in the SMRAM. The default SMRAM size is 64K Bytes beginning at a
base physical address in physical memory called the SMBASE. The SMBASE default
value following a hardware reset is 0x30000. The processor looks for the ﬁrst instruction
of the SMI handler at the address [SMBASE + 0x8000]. It stores the processor’s state
in the area from [SMBASE + 0xFE00] to [SMBASE + 0xFFFF] [20]. In SMM, if SMI
handler issues rsm instruction, the processor will switch back to the previous mode
(usually it is protected mode). In addition, the SMI handler can still access I/O devices.
HyperCheck veriﬁes the registers in SMM and reports the result by sending it via the
Ethernet card to the monitor machine. HyperCheck focuses on monitoring two registers:
IDTR and CR3. IDTR should never change after system initialization. For CR3, SMM
code can use it to translate the physical addresses of the hypervisor kernel code and
data. The offsets between physical addresses and virtual ones should never change as
we discussed in Section 4.2.
166
J. Wang, A. Stavrou, and A. Ghosh
5 Implementation
We implemented two prototypes for HyperCheck: HyperCheck-I is using QEMU full
system emulation while HyperCheck-II is running on a physical machine. We ﬁrst
developed HyperCheck-I for quick prototyping and debugging. To measure the over-
all system performance, we implemented HyperCheck-II on non-virtualized hardware.
Both of them utilize the Intel e1000 Ethernet card as the acquiring module.
In HyperCheck-I, the target machine is as a virtual machine that uses QEMU. The
analysis module runs on the host operating system of QEMU. For the acquiring module,
we placed a small NIC driver into the SMM of the target machine. Using the driver, we
can program the NIC to transmit the contents of physical memory as an Ethernet frame.
On the monitoring machine, an analysis module receives the packet from the network.
The analysis module compares contents of the physical memory with the original (ini-
tial) versions. If a new snapshot of the memory contents is different from the original
one, the module will report the event to a system operation who can decide how to pro-
ceed. Moreover, another small program runs in the SMM and collects and sends out the
CPU registers also via the Ethernet card.
For HyperCheck-II, we used two physical machines: one as the target and the other
as the monitor. On the target machine, we installed Xen 3.1 natively and used the phys-
ical Intel e1000 Ethernet card as the acquiring module. Also, we modiﬁed the default
SMM code on the target machine to enable our system similarly to our QEMU imple-
mentation. The analysis module runs on the monitor machine and is the same as the one
in HyperCheck-I. HyperCheck-II is mainly used for performance measurement.
As we mentioned earlier, we used QEMU for HyperCheck-I. QEMU is suitable for
debugging potential implementation problems. However, it comes with two drawbacks.
First, the throughput of a QEMU network card is much lower than a real NIC device.
For our QEMU based prototype, the network card throughput is approximately 10MB/s,
although Gigabit Ethernet cards are common in practice. Second, the performance mea-
surement on QEMU may not reﬂect the real world performance. HyperCheck-II help
us overcome these problems.
5.1 Memory Acquiring Module
The main task to implement the acquiring module is to port the e1000 network card
driver into SMM to scan the memory and send it out. Normally, SMM code is one part
of BIOS. Since we don’t have the source code of the BIOS, we used the method similar
to the one mentioned in [5] to modify the default SMM code. Basically, it writes the
SMM code in 16bit assembly and uses a user level program to open the SMRAM and
copy the assembly code to the SMRAM.
To overcome the limitations of [5], we divided the e1000 driver into two parts: initial-
ization and data transfer. The initialization part is complex and very similar to the Linux
driver. The communication part is simpler and different from the Linux driver. There-
fore, we modiﬁed the existing Linux e1000 driver to initialize the network card and only
program the transferring part in assembly. The e1000 driver on Linux is changed to only
initialize the NIC but does not send out any packet. The assembly code is compiled to
HyperCheck: A Hardware-Assisted Integrity Monitor
167
an ELF object ﬁle. Next, we wrote a small loader which can parse the ELF object ﬁle
and load the code and data to the SMM.
For this implementation, the NIC driver is ported to the SMM, the next step is to mod-
ify the driver to scan the memory and send them out. HyperCheck uses two transmission
descriptors per packet, one for the header and the other for the data. The content of the
header should be predeﬁned. Since the NIC is already initialized by the OS, the driver
in SMM has only to prepare the descriptor table and write it to the Transmit Descriptor
Tail (TDT) register of the NIC. The NIC will send the packet to the monitoring machine
using DMA. The NIC driver in SMM prepares the header data and let the descriptor
point to this header. For the payload, the descriptor is directly pointed to the address of
the memory that needs to be scanned. In addition, e1000 NIC supports CRC ofﬂoading.
To prevent replay attacks, a secret key is transferred from the monitor machine to
the target machine upon booting. The key is used to create a random seed to selectively
hash the data. If we hash the entire data stream, the performance impact may be high.
To reduce the overhead, we use the secret key as a seed to generate one big random
number used for one-time pad encryption and another set of serial random numbers.
The serial of random numbers are used as the indexes of the positions of the memory
being scanned. Then, the content at these positions are XORed with the one-time pad
with the same length before starting NIC DMA. After the transmission is done, the
memory content is XORed again to restore the original value.
The NIC driver also checks the loop-back setting of the device before sending the
packet. To further guarantee the data integrity ,the SMM NIC driver stays in the SMM
until all the packet is written to the internal FIFO of the NIC, and add 64KB more data
to the end to ﬂush the internal FIFO of the NIC. Therefore, the attacker cannot use loop-
back mode to get the secret key or peek into the internal NIC buffer through debugging
registers of the NIC.
5.2 Analysis Module
On the monitoring machine, a dedicated network card is connected with the acquiring
module. The operating system of the monitoring machine was CentOS 5.3. We run
tcpdump to ﬁlter the packets from the acquiring module; the output of tcpdump is
sent to the analysis module. The analysis module written in a Perl script reads the input
and checks for any anomalies. The analysis module ﬁrst recovers the contents using the
same secret key. After that, it compares every two consecutive memory snapshots bit by
bit. If they are different, the analysis module outputs an alert on the console, as we are
checking the persistent and static portion of the hypervisor memory. The administrator
can then decide whether it is a normal update of the hypervisor or an intrusion. Note that
during the system boot time, the contents of those control data and code are changing.
Currently, the analysis module can check the integrity of the control data and code.
The control data includes IDT table, hypercall table and exception table of Xen, and
the code is the code part of Xen hypervisor. To ﬁnd out the physical address of these
control tables, we use Xen.map symbol ﬁle. First, we ﬁnd the virtual addresses of
idt_table, hypercall_table and exception table. The physical address of these
symbols is virtual address − 0xff00,0000 on x86-32 architecture with PAE. The ad-
dress of Xen hypervisor code is between _stext and _etext. HyperCheck can also
168
J. Wang, A. Stavrou, and A. Ghosh
monitor the control data and codes of Domain 0. This includes the system call table
and the code part of Domain 0 (a modiﬁed Linux 2.6.18 kernel). The kernel of Domain
0 is also linearly mapped to the physical memory. We use a kernel module running in
Domain 0 to compute the exact offset. On our test machine, the offset is 0x83000000.
Note that, there is no IDT table for Domain 0, because there is only one such table in
the hypervisor. We input these parameters to the acquiring module to improve the scan
efﬁciency.
Note that these control tables are critical to system integrity. If their contents are
modiﬁed by any malware, it can potentially run arbitrary code in the hypervisor level,
i.e. the most privileged level. An antivirus software or intrusion detection system that
runs in Domain 0 is difﬁcult or unable to detect this hypervisor level malware because
they rely on the hypervisor to provide the correct information. If the hypervisor itself is
compromised, it may provide fake information to hide the malware. The checking for
the code part of the hypervisor enables HyperCheck to detect the attacks which do not
modify the control table but just modify the code invoked by those tables.
5.3 CPU Register Checking Module
HyperCheck uses SMM code to acquire and verify CPU registers. In a product, the SMI
handler should be integrated into BIOS. Or it can be set up during the system boot time.
This requires the bootstrap to be protected by some trusted bootstrap mechanism. In
addition, most chipsets provide a function to lock the SMRAM. Once it is locked, SMM
handler cannot be changed until reboot. Therefore, the SMRAM should be locked once
it is set up. In our prototype, we used the method mentioned in Section 5.1 to modify
the default SMM code.
There are three steps for CPU register checking: 1) triggering SMI to enter SMM;
2) checking the registers in SMM; 3) reporting the result. SMI is a hardware interrupt
and can only be triggered by hardware. Normally, I/O Controller Hub (ICH), also called
Southbridge, deﬁnes the events to trigger SMI. For HyperCheck-I, the QEMU emulates
Intel 82371SB chip as the Southbridge. It supports some device idle events to generate
SMI. SMI is often used for power management, and Southbridge provides some timers
to monitor the state of a device. If that device remains idle for a long time, it will trigger
SMI to turn off that device. The resolutions of these timers are typically one second.
However, on different motherboard, the method to generate the SMI may be different.
Therefore, we employ the Ethernet card to trigger the SMI event.
For the register checking, HyperCheck monitors IDTR and CR3 registers. The con-
tents of IDTR should never change after system boot. The SMM code just reads this
register by sidt instruction. HyperCheck uses CR3 to ﬁnd out the physical addresses
of hypervisor kernel code and data given their virtual addresses. Essentially, it walks
through all the page tables as a hardware Memory Management Unit (MMU) does. Note
that offset between the virtual address and the physical address of hypervisor kernel code
and data should never change. For example, it is 0xff000000 for Xen 32bit with PAE.
The Domain 0 has the same property. The SMM code requires the virtual address range
as the input (this can be obtained through the symbol ﬁle and send to the SMM in the
boot time) and afterwards check their physical addresses. If any physical address is not
HyperCheck: A Hardware-Assisted Integrity Monitor
169
equal to virtual address – offset, this signiﬁes a possible attack. The SMM code reports
the result of this checking via the Ethernet card. The assembly code of it is just 67 LOC.
The SMM code uses the Ethernet card to report the result. Without the Ethernet
card, it is difﬁcult to send the report reliably without stopping the whole system. For
example, the SMM code could write the result to a ﬁxed address of physical memory.
But according to our threat model, the attacker has access to that physical memory and
can easily modify the result. Or the SMM code could write it to the hard disk. Again,
this can be altered by the attacker too. Since security cannot relies on the obscurity, the
only way left without a network card is to stay in the SMM mode and put the warning
message on the screen. This is reliable, but the system in the protected mode becomes
completely frozen. Sometimes, it may not be desirable, and could be abused by the
attacker to launch Denial of Service attacks.
5.4 HyperCheck-II
In HyperCheck-II, the main difference from HyperCheck-I is the acquiring module. We
ported the SMM NIC driver from QEMU to a physical machine. Both of them have
the same model of the NIC: 82540EM Gigabit Ethernet card. However, the SMM NIC
driver from the QEMU VM does not work on the physical machine. And it took one of
the author one week to debug the problem. Finally, we ﬁnd out that the main difference
between a QEMU VM and the physical machine (Dell Optiplex GX 260) is that the
NIC can access the SMRAM in a QEMU VM while it cannot on the physical machine.
For HyperCheck-I SMM NIC driver, the TX descriptor is stored in the SMRAM and it
works well. For HyperCheck-II, the NIC cannot read the TX descriptor in the SMRAM
and therefore does not transmit any data.
To solve this problem, we reserved a portion of physical memory by adding a boot
parameter: mem=500M to the Xen hypervisor or Linux kernel. Since the total physical
memory on the physical machine is 512MB, we reserved 12MB for HyperCheck by
using mem parameter. This 12MB is used to store the data used by SMM NIC and
the TX descriptor ring. We also modiﬁed the loader to be a kernel module; it calls
ioremap() to map the physical memory to a virtual address and load the data there.
In a product, the TX descriptor ring should be prepared every time by the SMM code
before transmitting the packet. In our prototype, since we don’t have the source code of
the BIOS, we used the loader to load the TX descriptor.
Finally, we built a debugging interface for the SMM code on the physical machine.
We use the reserved physical memory to pass the information between the SMM code
and the normal OS. This interface is also used to measure the performance of the SMM
code as we will discuss in Section 6.
6 Evaluation
To validate the correct operation of HyperCheck, we ﬁrst veriﬁed the properties that
need to hold for us to be able to protect the underlying code as we discussed in Sec-
tion 4.2. Then, we tested the detection for hypervisor rootkits and measured the opera-
tional overhead of our approach. We have worked on two testbeds: testbed 1 is mainly
170
J. Wang, A. Stavrou, and A. Ghosh
used for HyperCheck-I and also as the monitor machine for HyperCheck-II. Testbed 2
uses HyperCheck-II to produce the plotted performance overhead on the real hardware.
Testbed 1 was equipped with a Dell Precision 690 with 8GB RAM and one 3.0GHz
Intel Xeon CPU with two cores. The host operating system was CentOS 5.3 64bit. The
QEMU version was 0.10.2 (without kqemu). The Xen version was 3.3.1 and Domain
0 was CentOS 5.3 32bit with PAE. Testbed 2 was a Dell Optiplex GX 260 with one
2.0GHz Intel Pentium 4 CPU and 512MB memory. Xen 3.1 and Linux 2.6.18 was in-
stalled on the physical machine and the Domain 0 is CentOS 5.4.
6.1 Verifying the Static Property
An important assumption is that the control data and respective code are statically
mapped into the physical memory. We used a monitoring module designed to detect
legitimate control data and code modiﬁcations throughout the experiments. This en-
abled us to test our approach against data changes and self-modifying code in the Xen
hypervisor and Domain 0. We also tested the static properties of Linux 2.6 and Win-
dows XP 32bit kernels. In all these tests, the hypervisor and the operating systems are
booted into a minimal state. The symbols used in the experiments are shown in Table 1.
During the tests, we found out that during boot the control data and the code changes.
For example, the physical memory of IDT is all 0 when the system ﬁrst boots up. But
after several seconds, it becomes non-zero and static. The reason is that the IDT table
is initialized later in the boot process.
Table 1. Symbols for Xen hypervisor, Domain 0, Linux and Windows
Use
Hypervisor’s Interrupt Descriptor Table
System Symbol
idt table
hypercall table Hypervisor’s Hypercall Table
exception table Hypervisor’s Exception Table
Beginning of hypervisor code
stext
etext
End of hypervisor code
sys call table Domain 0’s System Call Table
text
etext
idt table
sys call table
text