v0.4.9/calicoctl
启动
# export ETCD_AUTHORITY=192.168.78.21:2379
# ./calicoctl node --ip=192.168.78.21|22
docker ps 能看到：
CONTAINER ID IMAGE COMMAND CREATED STATUS
PORTS NAMES
74cc20b90b0f calico/node:v0.4.9 "/sbin/my_init" 24 seconds ago Up 23
seconds calico-node
高可用架构 66
部署测试实例
在 Calico 中，有 一 个 Profile 的 概 念（类 似 AWS 的 Security
Group），位于同一个 Profile 中的实例才能互相通讯，所以我们
先创建一个名为 db 的 Profile：
在 node1 上执行：
[node1]# ./calicoctl profile add db
然后启动测试实例：
[node1]# export DOCKER_HOST=localhost:2377
[node1]# docker run -n container1 -e CALICO_IP=auto -e CALICO_PROFILE=db
-td ubuntu
这 里 大 家 注 意，我 们 注 入 了 两 个 环 境 变 量：CALICOIP 和
CALICOPROFILE。
前者告诉 CALICO 自动进行 IP 分配，后者将此容器加入到 Profile
db 中。
那么 Calico 是怎么做到在容器启动的时候分配 IP 的呢？
大家注意我们在 run 一个容器前，先执行了一个 export，这里其
实就是将 Docker API 的入口劫持到了 Calico 那里。Calico 内
部是一个 twistd 实现的 Python Daemon，转发所有 Docker 的
API 请求给真正的 Docker 服务，如果发现是 start 则插入自己
高可用架构 67
的逻辑创建容器的网络栈。 容器启动后我们查看 container1 获
取的 IP 地址：
[container1]# ip addr
...
8: eth1:  mtu 1500 qdisc pfifo_fast state
UP group default qlen 1000
link/ether 1e:48:3e:ec:71:52 brd ff:ff:ff:ff:ff:ff
inet 192.168.0.1/32 scope global eth1
valid_lft forever preferred_lft forever
我们会看到 eth1 这个网络接口被设置了 IP 192.168.0.1。同样在
node2 上面部署 container2。默认设置下 IP 会在 192.168.0.0/16
中按顺序分配，所以 container2 会是 192.168.0.2。然后我们就
会发现 container1|2 能够互相 ping 通了！
路由实现
接下来让我们看一下在上面的 demo 中，Calico 是如何让不在一
个节点上的两个容器互相通讯的：
 Calico 节 点 启 动 后 会 查 询 Etcd，和 其 他 Calico 节 点 使 用
BGP 协 议 建 立 连 接。 [node1]# netstat -anpt | grep 179
tcp 0 0 0.0.0.0:179 0.0.0.0:* LISTEN 21887/bird tcp 0 0
192.168.78.21:46427 192.168.78.22:179 ESTABLISHED
21887/bird
高可用架构 68
 容器启动时，劫持相关 Docker API，进行网络初始化。
 如果没有指定 IP，则查询 Etcd 自动分配一个可用 IP。
 创建一对 veth 接口用于容器和主机间通讯，设置好容器内的
IP 后，打开 IP 转发。
 在主机路由表添加指向此接口的路由。
主机上： [node1]# ip link show ... 7: cali2466cece7bc:  mtu 1500 qdisc
pfifofast state UP mode DEFAULT qlen 1000 link/ether
96:c4:86:4d:d7:2c brd ff:ff:ff:ff:ff:ff
容 器 内： [container1]# ip addr ... 8: eth1:  mtu 1500 qdisc
pfifofast state UP group default qlen 1000 link/ether
1e:48:3e:ec:71:52 brd ff:ff:ff:ff:ff:ff inet 192.168.0.1/32
scope global eth1 validlft forever preferredlft forever
主 机 路 由 表： [node1]# ip route ... 192.168.0.1 dev
cali2466cece7bc scope link
 然后将此路由通过 BGP 协议广播给其他所有节点，在两个节点
上的路由表最终是这样的：
[node1]# ip route ... 192.168.0.1 dev cali2466cece7bc
scope link 192.168.0.2 via 192.168.78.22 dev enp0s8
proto bird
[node2]# ip route ... 192.168.0.1 via 192.168.78.21 dev
enp0s8 proto bird 192.168.0.2 dev caliea3aaf5a7be
scope link
高可用架构 69
大家看这个路由，node2 上面的 container2 要访问 container1
（192.168.0.1），通过查路由表得知需要将包转给 192.168.78.21，
也就是 node1。形象的展示数据流向是这样的：
container2[eth1] -> node2[caliea3aaf5a7be] -> route ->
node1[cali2466cece7bc] -> container1[eth1]
至此，跨节点通讯打通，整个流程没有任何 NAT，Tunnel 封包。
所以只要三层可达的环境，就可以应用 Calico。
利用 Profile 实现 ACL
在之前的 demo 中我们提到了 Profile，Calico 每个 Profile 都自
带一个规则集，用于对 ACL 进行精细控制，如刚刚的 db 的默认
规则集是：
[node1]# ./calicoctl profile db rule json
{
"id": "db",
"inbound_rules": [
{
"action": "allow",
"src_tag": "db"
}
],
"outbound_rules": [
{
"action": "allow"
}
]
}
高可用架构 70
这个规则集表示入连接只允许来自 Profile 名字是 db 的实例，出
连接不限制，最后隐含了一条默认策略是不匹配的全部 drop，所
以同时位于不同 Profile 的实例互相是不能通讯的，这就解决了隔
离的需求。
下面是一个更复杂的例子：
在常见的网站架构中，一般是前端 WebServer 将请求反向代理给
后端的 APP 服务，服务调用后端的 DB：
WEB -> APP -> DB
所以我们要实现：
 WEB 暴露 80 和 443 端口；
 APP 允许 WEB 访问；
 DB 允许 APP 访问 3306 端口；
 除此之外，禁止所有跨服务访问。
那么我们就可以如此构建 json：
对于 WEB：
[node1]# cat web-rule.json
{
"id": "web",
"inbound_rules": [
{
高可用架构 71
"action": "allow",
"src_tag": "web"
},
{
"action": "allow",
"protocol": "tcp",
"dst_ports": [
80,
443
]
}
],
"outbound_rules": [
{
"action": "allow"
}
]
}
[node1]# ./calicoctl profile web rule update < web-rule.json
入站规则我们增加了一条允许 80 443。
对于 APP：
[node1]# cat app-rule.json
{
"id": "app",
"inbound_rules": [
{
"action": "allow",
"src_tag": "app"
},
{
"action": "allow",
"src_tag": "web"
高可用架构 72
}
],
"outbound_rules": [
{
"action": "allow"
}
]
}
[node1]# ./calicoctl profile app rule update < app-rule.json
对于后端服务，我们只允许来自 web 的连接。
对于 DB，我们在只允许 APP 访问的基础上还限制了只能连接
3306。
[node1]# cat db-rule.json
{
"id": "db",
"inbound_rules": [
{
"action": "allow",
"src_tag": "db"
},
{
"action": "allow",
"src_tag": "APP",
"protocol": "tcp",
"dst_ports": [
3306