port this property. A replica receiving a client update need
only record the write in a persistent manner locally, return
a commit to the client, and lazily propagate the update to
other active replicas for that record. Lazy propagation is suf-
ﬁcient to ensure that all replicas eventually receive every up-
date committed at any replica, and a deterministic reconcil-
iation policy, e.g., as in Dynamo [21], suﬃces to ensure that
concurrent updates are consistently applied across all repli-
cas. Temporary divergence across replicas under failures can
be shortened by increasing durability, i.e., by recording the
update persistently at more replicas before returning a com-
mit to the client. The additional “single-writer” clause is
satisﬁed simply by incorporating a client-local timestamp in
the deterministic reconciliation policy.
Total write ordering. As Auspice is designed to be an
expressive name service with sophisticated attributes, it may
be useful in some scenarios to ensure that update operations
(like appending to or deleting from a list) to a name are ap-
plied in the same order by all active replicas. Ensuring a
total ordering of all updates to a name is a stronger prop-
erty than eventual consistency, calling for a state-machine
approach, which Auspice supports as an option.
To this end, active replicas for a name participate in a
Paxos instance maintained separately for each name (dis-
tinct from Paxos used by replica-controllers to compute ac-
tive replicas for that name). Each update is forwarded to the
active replica that is elected as the Paxos coordinator that,
under graceful execution, ﬁrst gets a majority of replicas to
accept the update number and then broadcasts a commit.
Total write ordering of course implies that updates can make
progress only when a majority of active replicas can commu-
nicate with each other while maintaining safety (consistent
with the so-called CAP dilemma).
3.3.4 Consistency with replica reconﬁguration
With a dynamic set of replicas as in Auspice, achiev-
ing eventual consistency is straightforward, as it suﬃces
if a replica recovering from a crash lazily propagates all
pending writes to a name to its current set of active repli-
cas as obtained from any of the consistently-hashed replica-
controllers for the name. However, satisfying the (optional)
total write order property above is nontrivial.
To this end, we have designed a two-tier reconﬁgurable
Paxos system that involves explicit coordination between the
consensus engines of the replica-controllers and active repli-
cas. Reconﬁguration is accomplished by a replica-controller
issuing and committing a stop request that gets commit-
ted as the last update of the current active replica group.
The replica-controller subsequently initiates the next group
of active replicas that can obtain the current record value
from any member of the previous group. This design shares
similarities with Vertical Paxos [39], however we were unable
to ﬁnd existing implementations or even reference systems
using similar schemes, so we had to develop it from scratch.
The details of the reconﬁguration protocol are here [1].
Scalability: A performance-cost analysis
3.3.5
Cost. Auspice’s replica placement scheme (Eq. 1) is de-
signed to use a fraction µ of system-wide resources so as to
make at least F and at most M replicas of each name, where
M is the total number of name server locations. Thus, at
light load, Auspice may replicate every name at every loca-
tion, while under heavy load, it may create exactly F repli-
cas for all but the most popular names.
In the common
case, a lookup involves one request and response between a
local name server and an active replica; an update involves
≈ thrice (twice) as many messages as the number of active
replicas with total write ordering (eventual) consistency.
Performance. The worst-case time-to-connect latency
for a name i depends on the lookup latency li, the update
rate wi, the worst-case update propagation latency di, i.e.,
the time for all active replicas to receive an update, and the
connect timeout T (Fig. 1), as follows [1].
252
ttci = li[1 + (ewidi − 1)(1 +
T
li
)]
(2)
Thus, the time-to-connect increases with (1) the lookup
latency li that in turn improves with demand-aware replica-
tion; (2) the update rate wi and update propagation delay
di that in combination determine the likelihood of obtain-
ing a stale response, noting that the latter increases with
more aggressive replication; and (3) the connect timeout T
that is at most the default transport-layer timeout (e.g., a
few seconds for TCP) and potentially as low as the round-
trip delay between the connecting client and the destination
being connected to if the destination network is capable of
generating an “no route to host” error message.
TTLs. The above analysis implicitly assumes near-zero
TTLs. With a nonzero ttli for name i, the worst-case time-
to-connect can be approximated as [1].
ttci ≃ τi
(ri + wi + 1/ttli + riwittli)
(1 + rittli)(ri + wi + 1/ttli)
+
T ri wi
(ri + 1/ttli)(ri + wi + 1/ttli)
(3)
where τi above is ttci (with a 0 TTL) as in Eq 2. Thus, a
long TTL is meaningful only if the update rate wi is low; if
so, a carefully chosen TTL can reduce the load on the system
as well as the client-perceived time-to-connect; if not, a long
TTL can inﬂate the time-to-connect by the connect timeout
T ( = the second term above for wi ≫ ri and high ttli).
Comparison to DNS. All of the above analyses are
applicable also to geo-replicated managed DNS providers
were they to employ Auspice’s demand-aware replication ap-
proach. The main diﬀerence between Auspice and today’s
managed DNS providers that rely on simplistic static replica
placement schemes is in the lookup latency li achieved for
any given resource cost; we evaluate this performance-cost
tradeoﬀ extensively in our experiments (§4.2 and §4.4).
3.3.6 Implementation status
We have implemented Auspice as described in Java with
28K lines of code. We have been maintaining an alpha de-
ployment for research use for many months across eight EC2
regions. We have implemented support for two pluggable
NoSQL data stores, MongoDB (default) and Cassandra, as
persistent local stores at name servers. We do not rely on
any distributed deployment features therein as the coordi-
nation middleware is what Auspice provides.
We have developed a simple NCS as a proof of concept,
which through a web portal (http://gns.name) or a command-
line console allows a user to bind a self- or system-selected
GUID to a human-readable name that is simply an email
address, i.e., our proof-of-concept NCS is a trivial CA that
relies on email-based identity veriﬁcation. Clients currently
have to use a custom Auspice developer library to perform
lookups and updates or custom socket library, msocket (§4.3),
for end-to-end mobility features. We have also developed a
simple proxy to translate between BIND and Auspice’s wire-
line protocol so as to interoperate with DNS.
4. EVALUATION
Our evaluation seeks to answer the following questions:
(1) How well does Auspice’s design meet its performance,
cost, and availability goals compared to state-of-the-art al-
ternatives under high mobility? (2) Can Auspice serve as a
complete, end-to-end solution for mobility and enable novel
communication abstractions? (3) How does Auspice’s cost-
performance tradeoﬀ compare to best-of-breed managed DNS
services for today’s (hardly mobile) domain name workloads?
4.1 Experimental setup
Testbeds: We use geo-distributed testbeds (Amazon EC2
or Planetlab) or local emulation clusters (EC2 or a depart-
mental cluster) depending upon the experiment’s goals.
Workload: There is no real workload today of clients
querying a name service in order to communicate with mo-
bile devices frequently moving across diﬀerent network ad-
dresses, both because such a name service does not exist and
mobile devices do not have publicly visible IP addresses. So
we conduct an evaluation using synthetic workloads for de-
vice names (§4.2), but to avoid second-guessing future work-
load patterns, we conduct a comprehensive sensitivity anal-
ysis against all of the relevant parameters such as the read
rate, write rate, popularity, and geo-locality of demand [1].
The following are default experimental parameters for de-
vice names. The ratio of the total number of lookups across
all devices to the total number of updates is 1:1, i.e., devices
are queried for on average as often as they change addresses.
The lookup rate of any single device name is uniformly dis-
tributed between 0.5–1.5× the average lookup rate; the up-
date rate is similarly distributed and drawn independently.
How requests are geographically distributed is clearly im-
portant for evaluating a replica placement scheme. We de-
ﬁne the geo-locality of a name as the fraction of requests
from the top-10% of regions where the name is most pop-
ular. This parameter ranges from 0.1 (least locality) to 1
(high locality). For a device name with geo-locality of g,
a fraction g of the requests are assumed to originate from
10% of the local name servers, the ﬁrst of which is picked
randomly and the rest are the ones geographically closest
to it. We pick the geo-locality g = 0.75 for device names,
i.e., the top 10% of regions in the world will account for
75% of requests, an assumption that is consistent with the
ﬁnding that communication and content access exhibits a
high country-level locality [37], and is consistent with the
measured geo-locality (below) of service names today.
In addition to device names, service names constitute a
small fraction (10%) of names and are intended to capture
domain names like today with low mobility. Their lookup
rate (or popularity) distribution and geo-distribution are
used directly from the Alexa dataset [2]. Using this dataset,
we calculated the geo-locality exhibited by the top 100K
websites to be 0.8. Updates for service names are a tiny frac-
tion (0.01%) of lookups as web services can be expected to be
queried much more often than they are moved around. The
lookup rate of service names is a third of the total number
of requests (same as the lookup or update rates of devices).
Replication schemes compared: Auspice uses the
replica placement strategy as described in §3 with the default
parameter values F = 3, µ = 0.7,ν = 0.5. We compare
Auspice against the following: (1) Random-M replicates
each name at three random locations; (2) Replicate-All
replicates all names at all locations; (3) DHT+Popularity
replicates names using consistent hashing with replication
similar to Codons[49]. The number of replicas is chosen
based on the popularity ranking of a name and the location
of replicas is decided by consistent hashing. The average
hop count in Codons’s underlying Beehive algorithm is set
so that it creates the same average number of replicas as
253
s
e
m
a
n
s
s
o
r
c
a
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Auspice
DHT+Popularity
Random-M
Replicate-All
 200
 300
 100
 0
 500
Median lookup latency of a name (ms)
(a) Lookup latencies (load = 0.3)
 400
Auspice
DHT+Popularity
Random-M
ReplicateAll
9x
5.7x
)
s
m
(
y
c
n
e
t
a
l
p
u
k
o
o
L
 300
 250
 200
 150
 100
 50
 0
 0
t
s
o
c
e
t
a
d
p
u
d
e
z
i
l
a
m
r
o
N
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Auspice
DHT+Popularity
Random-M
Replicate-All
 2
 3
 4
 5
 6
 7
 8
 2
 3
 4
 5
 6
 7
 8
 1
~
 1
~
Load (x = 100x updates/sec, 200x lookups/sec)
Load (x = 100x updates/sec, 200x lookups/sec)
(b) Lookup latency vs. load
(c) Update cost vs. load
Figure 4: Auspice has up to 5.7× to 9× lower latencies than Random-M and DHT+Popularity reps. (4(b)). A load of
1 means 200 lookups/sec and 100 updates/sec per name server. Replicate-All peaks out at a load of 0.3 while Auspice
can sustain a request load of up to 8 as it carefully chooses between 3 and 80 replicas per name.
Auspice for a fair comparison. All schemes direct a lookup
to the closest available replica after the ﬁrst request.
4.2 Evaluating Auspice’s replica placement
We conduct experiments in this subsection on a 16-node
(each with Xeon 5140, 4-cores, 8 GB RAM) departmen-
tal cluster, wherein each machine hosts 10 instances of ei-
ther nameservers or local nameservers so as to emulate an
80-nameserver Auspice deployment. We instrument the in-
stances so as to emulate wide-area latencies between any two
instances that correspond to 160 randomly chosen Planet-
lab nodes. We choose emulation instead of a geo-distributed
testbed in this experiment in order to obtain reproducible
results while stress-testing the load-vs.-response time scaling
behavior of various schemes given identical resources.
4.2.1 Lookup latency and update cost
How well does Auspice use available resources for repli-
cating name records? To evaluate this, we compare the
lookup latency of schemes across varying load levels. A
machine running 10 name servers receives on average 2000
lookups/sec and 1000 updates/sec at a load = 1. For each
scheme, load is increased until 2% of requests fail, where a
failed request means no response is received within 10 sec.
The experiment runs for 10 mins for each scheme and load
level. To measure steady-state behavior, both Auspice and
DHT+Popularity pre-compute the placement at the start of
the experiment based on prior knowledge of the workload.
Figure 4(a) shows the distribution of median lookup la-
tency across names at the smallest load level (load = 0.3).
Figure 4(b) shows load-vs-lookup latency curve for schemes,
where “lookup latency” refers to the mean of the median
lookup latencies of names. Figure 4(c) shows the corre-
sponding mean of the distribution of update cost across
names at varying loads; the update cost for a name is the
number of replicas times the update rate of that name.
Replicate-All gives low lookup latencies at the smallest
load level, but generates a very high update cost and can
sustain a request load of at most 0.3. This is further sup-
ported by Figure 4(c) that shows that the update cost for
Replicate-All at load = 0.4 is more than the update cost of
Auspice at load = 8. In theory, Auspice can have a capac-
ity advantage of up to N/M over Replicate-All, where N is
the total number of name servers and M is the minimum
of replicas Auspice must make for ensuring fault tolerance
(resp. 80 and 3 here). Random-M can sustain a high request
load (Fig. 4(b)) due to its low update costs, but its lookup
latencies are higher as it only creates 3 replicas randomly.
Auspice has 5.7×−9× lower latencies over Random-M and
DHT+Popularity respectively (Figure 4(b), load=1). This
is because it places a fraction of the replicas close to pockets
of high demand unlike the other two. At low to moder-
ate loads, servers have excess capacity than the minimum
needed for fault tolerance, so Auspice creates as many repli-
cas as it can without exceeding the threshold utilization level
(Eq. 1), thereby achieving low latencies for loads≤4. At
loads ≥ 4, servers exceed the threshold utilization level even
if Auspice creates the minimum number of replicas needed
for fault tolerance. This explains why Auspice and Random-
M have equal update costs for loads ≥ 4 (Figure 4(c)). Re-
ducing the number of replicas at higher loads allows Auspice
to limit the update cost and sustain a maximum request load
that is equal to Random-M.
DHT+Popularity has higher lookup latencies as it repli-
cates based on lookup popularity alone and places repli-
cas using consistent hashing without considering the geo-
distribution of demand. Further, it answers lookups from a
replica selected enroute the DHT route. Typically, the la-
tency to the selected replica is higher than the latency to
the closest replica for a name, which results in high laten-
cies. DHT+Popularity replicates 22.3% most popular names
at all locations. Lookups for these names go to the clos-
est replica and achieve low latencies; lookups for remaining
77.7% of names incur high latencies.
DHT+Popularity incurs higher update costs than Auspice
even though both schemes create nearly equal numbers of
replicas at every load level. This is because DHT+Popularity
decides the number of replicas of a name only based on its
popularity, i.e., lookup rates, while Auspice decides the num-
ber of replicas based on lookup-to-update ratio of names.
Due to its higher update costs, DHT+Popularity can not
sustain as high a request load as Auspice.
4.2.2 Update latency, update propagation delay
The client-perceived update latency, i.e., the time from
when when a client sends an update to when it receives
a conﬁrmation. These numbers are measured from the ex-