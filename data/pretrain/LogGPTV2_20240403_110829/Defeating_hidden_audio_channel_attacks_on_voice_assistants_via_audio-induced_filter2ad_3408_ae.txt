73.80%
Table 8: Performance of frontend setup with various
smartphone usage scenarios (unsupervised).
Table
100%
100%
Held
in hand
87.30%
87.30%
Placed
on sofa
100%
100%
80%vol.
on table
100%
100%
2x speed
on table
88.30%
85.20%
Kmed
Kmea
and 7. We observe that our system can achieve similar per-
formance in both the frontend mode and the backend mode
with the partial playback, compared to the complete voice
command playback. In particular, our system can achieve
up to 99.9% accuracy for both 1-second and 0.5-second play-
back for the frontend mode. For the backend mode, we can
achieve up to 99.1% for the 1-second playback and 90.5% for
the 0.5-second playback. This is because our unique vibration
features capture the inherent signatures of the user’s voice
that is dependent from the length of voice commands. We
also find the K-medoid performance is similar to K-means
and the supervised learning methods perform slightly better.
The results are promising, which facilitate deploying our
system without causing additional delay.
6.2.4 Various Mobile Device Usage Scenarios. We next evalu-
ate the frontend setup with various practical mobile devices
usage scenarios. We only report the unsupervised learning
method results because the supervised learning performs
better. Table 8 shows the comparison of our system under
different scenarios, when the mobile device is on the table,
held by hand, and placed on a soft surface such as a sofa. We
also evaluate our system with lower volume (e.g., 80%) and
fast forwarding (e.g.,×2 speed). We find our system achieves
100% accuracy of distinguishing hidden voice commands
(a) Frontend setup
(b) Backend setup
Figure 13: Performance of unsupervised learning
methods.
(a) Frontend setup
(b) Backend setup
Figure 14: Performance of unsupervised learning
methods trained with 5/10 words and 2/5 participants.
high accuracy in both setups. In particular, Samsung Note
4, LG G3 and Nexus 6 obtain 99.9%, 95.1% and 99.9% in the
frontend setup with the K-means method and that for the
backend setup obtained 99.9%, 94% and 93.7%. Moreover, by
comparing Figure 14 with Figure 13, we observe that the per-
formances obtained based on the different training data sizes
are similar. This is because our unsupervised learning-based
methods differentiate between the hidden voice command
and normal commands by capturing their inherent physical
differences in the vibration domain. As a result, the hidden
command sounds are mapped far away from the normal
command cluster based on the vibration features, even if the
hidden voice commands are generated from unseen words
and voices to the system.
6.2.3 Partial Playback to Reduce Delay. We further evaluate
the performance our system with playback of partial voice
commands rather than the complete commands to speed
up the playback process and reduce the delay. In particular,
we set the system to playback only 1 second or 0.5 second
of the recorded voice commands (around one word) and
apply the unsupervised learning method K-means on the
collected accelerometer readings to detect the hidden voice
command. The performances of such partial replay in the
frontend setup and the backend setup are shown in Table 6
Note4G3Nexus6S600.20.40.60.81Accuracy (%)kmedoidskmeansNote4G3Nexus6S600.20.40.60.81Accuracy (%)kmedoidskmeansNote4G3Nexus6S600.20.40.60.81Accuracy (%)kmedoidskmeansNote4G3Nexus6S600.20.40.60.81Accuracy (%)kmedoidskmeansand normal commands when the mobile device is on the
hard table surface or soft sofa surface. When being held by
hand, the accuracy degrades to 87.3%, because the hand sup-
presses the speech vibrations on the mobile device. When
replaying with 80% volume, our system still achieves 100%
accuracy, which indicates the robustness of our system to
capture the speech vibration differences of the two types
of voice commands under different playback volumes. In
addition, in the fast forwarding scenario with ×2 speed, the
accuracy degrades to 88.3% and 85.2% for K-medoids and
K-means methods. This is because fast forwarding causes
some distortions and information loss to the playback sound,
compared to the original audio.
7 CONCLUSION & DISCUSSION
In this work, we show that hidden voice commands that
mimic the voice features of normal commands, while re-
maining incomprehensible to humans, can be detected by
comparing their speech features in the vibration domain. We
showed that by using the vibration features, including the
statistical features in the time and frequency domains and
speech features (e.g., MFCCs, chroma vector), it is possible to
distinguish hidden voice commands (created as per [9]) from
normal commands with a sufficient degree of accuracy. Our
supervised and unsupervised learning classification results
from observing speech vibration on multiple smartphones,
indicate that the idea of using the on-board accelerometer
(found universally on all smartphones) to log these speech
vibrations, can help the voice assistant technology of these
smartphones to detect hidden voice commands. In conclu-
sion, we believe that the ability of the vibration domain
features to detect hidden voice commands, as discussed in
this work, opens up a new discussion to secure the voice
assistants on current smartphones, in the context of syn-
thesized voice commands. Their capability as a standalone
mechanism or in conjunction with audio domain features
offers additional exciting defense approaches in the domain
of voice security.
We recognize that the playback process of our system
may cause some playback delay and the intrusion of fron-
tend mode. However, we show that our system can achieve
high accuracies with the partial command playback and n-
times speed playback, which speed up the playback process
(e.g., to 0.5 second). Moreover, the existing VCSs usually take
several seconds to process the audio for understanding the
command context (e.g., 2 seconds for Google Now and 4 sec-
onds for Siri [6]), and our system is designed in a manner
that works simultaneously with this process to avoid caus-
ing additional delay. Furthermore, such short time playback
incur less intrusion to the users, who can still choose the
backend mode of our system to defend against the hidden
command attacks.
In our future work, we will study modulating the fron-
tend playback sound to an inaudible frequency (e.g., greater
than 16KHz) to achieve zero intrusion. Meanwhile, a more
practical backend playback setup consisting of a tiny low-
cost device that is equipped with an on-board speaker and
motion sensors will be explored. Moreover, it is potential
to extend our unsupervised learning-based method to also
defend other attacks, such as ultrasound attacks, without
requiring much training effort. It is also worth exploring
whether the replay sounds could be distinguished from the
live human voice in the vibration domain.
8 ACKNOWLEDGMENTS
This work was supported partially by the National Science
Foundation Grants CNS1820624, CNS1814590, CNS1801630,
CNS1714807, CNS1526524, DOE1662762 and ARO W911NF-
18-1-0221.
REFERENCES
[1] [n.d.]. PDV-100 PORTABLE DIGITAL VIBROMETER: Vibration sen-
sor for mobile use. https://www.polytec.com/us/vibrometry/products/
single-point-vibrometers/pdv-100-portable-digital-vibrometer/. Ac-
cessed: 12/13/2018.
[2] 2018. Text To Speech (TTS) service. http://www.fromtexttospeech.
com/.
[3] 2018. Voice Assistant Market Research Report- Global Forecast
2023. https://www.marketresearchfuture.com/reports/voice-assistant-
market-4003.
[4] Talal B Amin, James S German, and Pina Marziliano. 2013. Detecting
voice disguise from speech variability: Analysis of three glottal and
vocal tract measures. In Proceedings of Meetings on Acoustics 166ASA,
Vol. 20. ASA, 060005.
[5] S Abhishek Anand and Nitesh Saxena. 2018. Speechless: Analyzing
the Threat to Speech Privacy from Smartphone Motion Sensors. In
Proceedings of the 39th IEEE Symposium on Security and Privacy (S&P).
116–133.
[6] Mehdi Assefi, Guangchi Liu, Mike P Wittie, and Clemente Izurieta.
2015. An experimental evaluation of apple siri and google speech
recognition. Proccedings of the 2015 ISCA SEDE (2015).
[7] Ronald J. Baken and Robert F. Orlikoff. 1987. Clinical Measurement of
Speech and Voice. London: Taylor and Francis Ltd.
[8] Logan Blue, Hadi Abdullah, Luis Vargas, and Patrick Traynor. 2018.
2MA: Verifying Voice Commands via Two Microphone Authentica-
tion. In Proceedings of the 2018 on Asia Conference on Computer and
Communications Security. ACM, 89–100.
[9] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang,
Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016.
Hidden Voice Commands.. In USENIX Security Symposium. 513–530.
[10] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples:
Targeted attacks on speech-to-text. In 2018 IEEE Security and Privacy
Workshops (SPW). IEEE, 1–7.
[11] Simon Castro, Robert Dean, Grant Roth, George T Flowers, and Brian
Grantham. 2007. Influence of acoustic noise on the dynamic perfor-
mance of MEMS gyroscopes. In ASME 2007 International Mechanical
Engineering Congress and Exposition. American Society of Mechanical
accelerometers with acoustic injection attacks. In Security and Privacy
(EuroS&P), 2017 IEEE European Symposium on. IEEE, 3–18.
[28] Tavish Vaidya, Yuankai Zhang, Micah Sherr, and Clay Shields. 2015.
Cocaine noodles: exploiting the gap between human and machine
speech recognition. WOOT 15 (2015), 10–11.
[29] Olli Viikki and Kari Laurila. 1998. Cepstral domain segmental feature
vector normalization for noise robust speech recognition. Speech
Communication 25, 1-3 (1998), 133–147.
[30] Zhizheng Wu, Xiong Xiao, Eng Siong Chng, and Haizhou Li. 2013.
Synthetic speech detection using temporal modulation feature. In IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 7234–7238.
[31] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu,
Kai Chen, Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A
Gunter. 2018. CommanderSong: A Systematic Approach for Practical
Adversarial Voice Recognition. In USENIX Security Symposium.
[32] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang,
and Wenyuan Xu. 2017. DolphinAttack: Inaudible voice commands.
In Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 103–117.
[33] Li Zhang, Parth H Pathak, Muchen Wu, Yixin Zhao, and Prasant Mo-
hapatra. 2015. Accelword: Energy efficient hotword detection through
accelerometer. In Proceedings of the 13th Annual International Confer-
ence on Mobile Systems, Applications, and Services. ACM, 301–315.
[34] Linghan Zhang, Sheng Tan, and Jie Yang. 2017. Hearing Your Voice
is Not Enough: An Articulatory Gesture Based Liveness Detection
for Voice Authentication. In Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 57–71.
[35] Linghan Zhang, Sheng Tan, Jie Yang, and Yingying Chen. 2016. Voice-
live: A phoneme localization based liveness detection for voice au-
thentication on smartphones. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 1080–
1091.
Engineers, 1825–1831.
[12] Phillip L De Leon, Michael Pucher, Junichi Yamagishi, Inma Hernaez,
and Ibon Saratxaga. 2012. Evaluation of speaker verification security
and detection of HMM-based synthetic speech. IEEE Transactions on
Audio, Speech, and Language Processing 20, 8 (2012), 2280–2290.
[13] Robert Neal Dean, Simon Thomas Castro, George T Flowers,
Grant Roth, Anwar Ahmed, Alan Scottedward Hodel, Brian Eugene
Grantham, David Allen Bittle, and James P Brunsch. 2011. A charac-
terization of the performance of a MEMS gyroscope in acoustically
harsh environments. IEEE Transactions on Industrial Electronics 58, 7
(2011), 2591–2596.
[14] Robert N Dean, George T Flowers, A Scotte Hodel, Grant Roth, Simon
Castro, Ran Zhou, Alfonso Moreira, Anwar Ahmed, Rifki Rifki, Brian E
Grantham, et al. 2007. On the degradation of MEMS gyroscope per-
formance in the presence of high power acoustic noise. In Industrial
Electronics, 2007. ISIE 2007. IEEE International Symposium on. IEEE,
1435–1440.
[15] Wenrui Diao, Xiangyu Liu, Zhe Zhou, and Kehuan Zhang. 2014. Your
voice assistant is mine: How to abuse speakers to steal information
and control your phone. In Proceedings of the 4th ACM Workshop on
Security and Privacy in Smartphones & Mobile Devices. ACM, 63–74.
[16] Huan Feng, Kassem Fawaz, and Kang G Shin. 2017. Continuous au-
thentication for voice assistants. In Proceedings of the 23rd Annual
International Conference on Mobile Computing and Networking. ACM,
343–355.
[17] Jeff Gamet. 2018. Need Your HomePod to Recalibrate its Sound for Your
Room? Give it a Shake. https://www.macobserver.com/tips/quick-
tip/homepod-recalibrate-shake/.
[18] Rosa González Hautamäki, Tomi Kinnunen, Ville Hautamäki, and
Anne-Maria Laukkanen. 2015. Automatic versus human speaker veri-
fication: The case of voice mimicry. Speech Communication 72 (2015),
13–31.
[19] Rosa González Hautamäki, Tomi Kinnunen, Ville Hautamäki, Timo
Leino, and Anne-Maria Laukkanen. 2013. I-vectors meet imitators: on
vulnerability of speaker verification systems against voice mimicry..
In Interspeech. Citeseer, 930–934.
[20] Chadawan Ittichaichareon, Siwat Suksri, and Thaweesak Yingthaworn-
suk. 2012. Speech recognition using MFCC. In International Conference
on Computer Graphics, Simulation and Modeling (ICGSM’2012) July.
28–29.
[21] Chaouki Kasmi and Jose Lopes Esteves. 2015. IEMI threats for infor-
mation security: Remote command injection on modern smartphones.
IEEE Transactions on Electromagnetic Compatibility 57, 6 (2015), 1752–
1755.
[22] Yan Michalevsky, Dan Boneh, and Gabi Nakibly. 2014. Gyrophone:
Recognizing Speech from Gyroscope Signals.. In USENIX Security Sym-
posium. 1053–1067.
[23] Dibya Mukhopadhyay, Maliheh Shirvanian, and Nitesh Saxena. 2015.
All your voices are belong to us: Stealing voices to fool humans and
machines. In European Symposium on Research in Computer Security.
Springer, 599–621.
[24] Emmanuel Munguia Tapia. 2008. Using machine learning for real-
time activity recognition and estimation of energy expenditure. Ph.D.
Dissertation. Massachusetts Institute of Technology.
[25] K Sri Rama Murty and Bayya Yegnanarayana. 2006. Combining evi-
dence from residual phase and MFCC features for speaker recognition.
IEEE signal processing letters 13, 1 (2006), 52–55.
[26] Ingo R. Titze and Daniel W. Martin. 1994. Principles of Voice Production.
Vol. 104. Prentice Hall (currently published by NCVS.org). https:
//doi.org/10.1121/1.424266
[27] Timothy Trippel, Ofir Weisse, Wenyuan Xu, Peter Honeyman, and
Kevin Fu. 2017. WALNUT: Waging doubt on the integrity of mems