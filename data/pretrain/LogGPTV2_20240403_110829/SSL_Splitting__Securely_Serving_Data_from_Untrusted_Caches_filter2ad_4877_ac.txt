t
d
w
d
n
a
b
/
t
u
p
h
g
u
o
r
h
t
(
y
c
n
e
i
c
i
f
f
e
h
t
d
w
d
n
a
B
i
Savings over HTTP
Savings over HTTPS
1000
100
10
1
r
o
t
c
a
f
s
g
n
i
v
a
S
0.01
10 B
100 B
1 KB
10 KB 100 KB 1 MB
10 MB
File size (bytes) 
0.1
10 B
100 B
1 KB
10 KB 100 KB 1 MB
10 MB
File size (bytes) 
Figure 7: Ratio of throughput achieved to bandwidth
used, when retrieving a single ﬁle from the server.
Figure 8: Bandwidth savings of SSL splitting over
HTTP and HTTPS.
broker while testing Barnraising. This choice brings the
third party into the central trust domain of the server;
much like a traditional mirror, it is a role which can only
be ﬁlled by a reputable entity.
The utility of Barnraising will be limited by the volun-
teer proxies that join it. Therefore, the proxy software
has been designed to be simple to install and use, re-
quiring only a single URI to conﬁgure. Since volun-
teers will be able to download from each other, they
will have better performance than regular clients, giving
users an incentive to install the software.
In the long
term, we hope to incorporate more efﬁcient authenti-
cation schemes, such as SFSRO [8], into the volunteer
code, using the installed base of SSL splitting software
to bootstrap the more technically sophisticated systems.
6 Evaluation
This section presents microbenchmarks and trace-based
experiments to test the effectiveness and practicality of
SSL splitting. The results of these experiments demon-
strate that SSL splitting decreases the bandwidth load on
the server, and that the performance with respect to un-
cached ﬁles is similar to vanilla SSL.
6.1 General experimental setup
For the experiments we used the Apache web server
(version 1.3.23), linked with mod ssl (version 2.8.7),
and OpenSSL (version 0.9.6), running under Linux
2.4.18 on a 500 MHz AMD K6. This server’s network
connection is a residential ADSL line with a maximum
upstream bandwidth of 160 kbps. The client was a cus-
tom asynchronous HTTP/HTTPS load generator written
in C using OpenSSL, running under FreeBSD 4.5 on
a 1.2 GHz Athlon. The proxy, when used, ran under
FreeBSD 4.5 on a 700 MHz Pentium III. Both the client
and the proxy were on a 100 Mbps LAN, with a 100
Mbps uplink.
6.2 Bandwidth savings for a single ﬁle
Since SSL splitting caches ﬁles at the record level, and
a cached record costs a ﬁxed amount to transmit, we
would expect the compression level to depend on the
ﬁle size. A series of short microbenchmarks consisting
of a single ﬁle download conﬁrms that SSL splitting is
far more effective at caching large ﬁles than small ﬁles.
Figure 7 shows the bandwidth efﬁciency, calculated as
the ratio of ﬁle throughput to bandwidth consumption,
of HTTP, HTTPS, uncached SSL splitting, and cached
SSL splitting. (The dotted line at 1 represents the the-
oretical performance of an ideal non-caching protocol
with zero overhead.)
Figure 8 shows this data as the “savings factor”, the ratio
of bandwidth consumed by SSL splitting (with a warm
cache) to that consumed by HTTP or HTTPS to transmit
the same ﬁle. For 100-byte ﬁles, plain HTTP has about
one-third the cost of SSL splitting, since the bandwidth
194
12th USENIX Security Symposium 
USENIX Association
www.lcs (10 MB)
www.lcs (full)
amsterdam (10 MB)
amsterdam (full)
1.0
0.8
0.6
0.4
0.2
0.0
1 B
10 B
100 B
1 KB
10 KB 100 KB 1 MB
Request download size (bytes) 
Figure 9: CDF of request sizes in www.lcs and
amsterdam traces.
cost for small ﬁles is dominated by connection setup; on
the other hand, for one-megabyte ﬁles, SSL splitting has
a bandwidth savings of 99.5% over HTTP.
There are two artifacts evident in these graphs. They
show a sharp curve upward at the 3,000 byte point; the
reason for this artifact is that Apache sends all of the
HTTP headers and ﬁle data in a single SSL record for
ﬁles smaller than roughly 4,000 bytes, but sends the
HTTP headers in a separate record from the ﬁle data for
larger ﬁles. There is also a sharp performance drop be-
tween 3 megabytes and 10 megabytes: this is because,
for ﬁles larger than 222 bytes, Apache sends the ﬁle data
in records of 8,192 bytes, instead of the maximum record
size of 16,384 bytes.
queries were ﬁltered out, URLs were canonicalized, and
every (URL, size) pair was encoded as a unique URL.
The server tree was then populated with ﬁles containing
random bytes.
The www.lcs trace, which is seven months long, con-
tains 109 GB of downloads from a server with 10.6 GB
of ﬁles; the amsterdam trace, which is nine months
long, contains 270 GB of downloads from 77 GB of
ﬁles. Analyzing randomly-chosen chunks of various
lengths from www.lcs showed that most repetition is
long-term: a day’s trace (typically about 100 MB of data
transfer) has a repetition factor which varies between 1.5
and 3, while a month is compressible by about a factor
of 4, and the whole trace is compressible by a factor of
10. This data suggests that having proxies keep blocks
around for a long time will pay off, and supports a de-
sign in which proxies are organized into a DHT, since
this allows them to store more unique blocks for a longer
period of time.
To keep running experiments manageable over an ADSL
line, we selected a typical daytime chunk representing
approximately 10 MB of transfers from each trace. Fig-
ure 9 shows the distribution of request sizes in each
trace chunk, along with the distribution in the full traces.
The www.lcs chunk represents 4.43 MB of ﬁles and
10.0 MB of transfers, for an inherent compressibil-
ity factor of 2.26; the amsterdam chunk represents
8.46 MB of ﬁles and 11.6 MB of transfers, for an in-
herent compressibility factor of 1.37.
None of our experiments placed any limits on the size of
the proxy’s cache, since it seems reasonable for a mirror
host (or DHT) to store a full copy of a 10–70 GB Web
site. In the future, we plan to investigate the impact of
the size of the cache on SSL splitting’s performance.
6.3 Bandwidth savings for trace-driven loads
6.3.2 Measurements
The next set of experiments uses traces from two Web
servers to evaluate SSL splitting on realistic workloads.
6.3.1 Web trace ﬁles
The Web traces were derived from several-month
depart-
access.log
mental Web
www.lcs.mit.edu and
amsterdam.lcs.mit.edu. To convert the access
logs into replayable traces, all non-GET, non-status-200
from two
servers,
taken
ﬁles
Ideally, SSL splitting’s bandwidth utilization should be
close to the inherent compressibility of the input trace.
To test this, we played back the two 10 MB trace chunks
to a standard HTTP server, a standard HTTPS server,
an SSL splitting proxy with a cold cache, and an SSL
splitting proxy with a warm cache; in each case, we
measured the total number of bytes sent on the server’s
network interface. The resulting bandwidth usage ratios
(measured in bytes of bandwidth used per bytes of ﬁle
throughput) are shown as the gray bars in Figures 10
and 11. (The other bars are explained later in this sec-
tion.)
USENIX Association
12th USENIX Security Symposium 
195
1.5
1.0
0.5
0.0
1.5
1.0
0.5
0.0
1.06
1.06
1.05
1.13
1.13
1.10
10 MB: measured
10 MB: simulated
Full: simulated
0.56
0.52
0.18
0.44
0.10
0.10
0.10
0.08
HTTP
HTTPS
Ideal
cold cache
SSL splitting
cold cache
SSL splitting
warm cache
Figure 10: Bandwidth usage: www.lcs trace.
1.05
1.06
1.04
1.12
1.12
1.07
0.71
0.28
10 MB: measured
10 MB: simulated
Full: simulated
0.84
0.84
0.34
0.11
0.11
0.05
HTTP
HTTPS
Ideal
cold cache
SSL splitting
cold cache
SSL splitting
warm cache
Figure 11: Bandwidth usage: amsterdam trace.
As expected, SSL splitting with a cold cache achieves
a compression ratio of 2.04 on www.lcs and 1.25 on
amsterdam, with respect to HTTP; the compression
is about 5% more with respect to HTTPS, very close
to the inherent redundancy.
If the cache is warmed
before running the trace, the compression ratio is ap-
proximately a factor of 11 for www.lcs and 10 for
amsterdam. Analysis of the portions of the trace ﬁle
preceding the www.lcs chunk indicate that if the pre-
vious two weeks had been cached by the proxy, the
cache would have been approximately 90% warm. The
amsterdam chunk is too close to the beginning of the
trace to perform this analysis.
6.3.3 Simulations
The 10 MB trace chunks used in our experiments span
only a few hours each; thus, they might not be represen-
tative of Web site trafﬁc over long periods. In addition,
the short chunks do not contain as high a degree of rep-
etition as the long traces.
Since it is impractical to replay a several-month-long
trace from a busy Web site over an ADSL link, we turned
to simulation to estimate the likely performance of SSL
splitting over long periods. Based on the data collected
in the single-ﬁle microbenchmark, we constructed a sim-
ple linear model of the performance of HTTP, HTTPS,
and SSL splitting in the uncached case: the bandwidth
used is a ﬁxed per-ﬁle cost plus a marginal per-byte cost.
The marginal cost is slightly greater than one because of
packet and record overhead.
We model the cost of SSL splitting (in the cached case)
as a piecewise linear function:
for ﬁles smaller than
4,000 bytes, the marginal cost is greater than one, but for
larger ﬁles, the marginal cost per byte is very small, ap-
proximately 1/250. The marginal cost increases to about
1/120 for ﬁles larger than 222 bytes.
This simple performance model ﬁts the microbenchmark
results to within 5%. In addition, to validate the model,
we simulated each experiment with 10 MB trace chunks.
The simulated results, shown as the white bars in Fig-
ures 10 and 11, agree very closely with the measured
results.
The results of simulating the full traces, shown as the
striped bars, demonstrates that SSL splitting can take ad-
vantage of most of the available redundancy. For exam-
ple, on the www.lcs trace, SSL splitting would have a
bandwidth savings of 83% over HTTP; an ideal proto-
col with zero overhead and perfect caching would save
90%.