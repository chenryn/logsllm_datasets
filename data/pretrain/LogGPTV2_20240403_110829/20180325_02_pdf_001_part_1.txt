### Coordinated and Efficient Huge Page Management with Ingens

**Authors:**
- Youngjin Kwon, Hangchen Yu, and Simon Peter, The University of Texas at Austin
- Christopher J. Rossbach, The University of Texas at Austin and VMware
- Emmett Witchel, The University of Texas at Austin

**Conference:**
- 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI '16)
- November 2–4, 2016, Savannah, GA, USA
- ISBN 978-1-931971-33-1
- Open access sponsored by USENIX

**Abstract:**

Modern computing systems are increasingly dependent on large amounts of RAM, with diverse workloads consuming enormous capacities. As memory capacity has grown, so have the overheads associated with hardware address translation. To address this, hardware manufacturers have introduced larger page sizes, known as huge pages, which reduce address translation overheads by minimizing TLB misses. However, the success of these mechanisms is critically dependent on the ability of operating systems and hypervisors to manage huge pages effectively.

While huge pages have been supported in hardware since the 1990s, processors historically had very few TLB entries reserved for huge pages, limiting their usability. Newer architectures, such as Intel's Skylake, support thousands of huge page entries in dual-level TLBs, shifting the responsibility for better huge page support from hardware to system software. This shift presents both an urgent need and an opportunity to modernize memory management.

Ingens is a framework that provides transparent and coordinated huge page support using a set of basic primitives. By managing contiguity as a first-class resource and tracking the utilization and access frequency of memory pages, Ingens eliminates several fairness and performance issues that plague current systems. Our experiments demonstrate significant improvements in fairness, performance (up to 18% improvement), tail latency reduction (up to 41%), and memory bloat reduction (from 69% to less than 1% for important applications like web services and the Redis key-value store).

**Introduction:**

Modern computing platforms can now support terabytes of RAM, and workloads that can take advantage of such large memories are becoming commonplace. However, increased memory capacity poses a significant challenge for address translation. All modern processors use page tables for address translation and TLBs to cache virtual-to-physical mappings. Since TLB capacities cannot scale at the same rate as DRAM, TLB misses and address translation can incur crippling performance penalties for large memory workloads when using traditional page sizes (e.g., 4KB). Hardware-supported address virtualization, such as AMD's nested page tables, increases average-case address translation overhead due to the multi-dimensional nature of page tables, which can amplify worst-case translation costs by up to 6 times [59].

Hardware manufacturers have addressed increasing DRAM capacity by providing better support for larger page sizes, or huge pages, which reduce address translation overheads by reducing the frequency of TLB misses. However, the success of these mechanisms is critically dependent on the ability of operating systems and hypervisors to manage huge pages effectively.

**Background:**

Current trends in memory management hardware make it critical that system software supports huge pages efficiently and flexibly. This section considers those trends and the challenges they create for the OS and hypervisor. We provide an overview of huge page support in modern operating systems and conclude with experiments that show the performance benefits of the state-of-the-art in huge page management.

**Virtual Memory Hardware Trends:**

Virtual memory decouples the address space used by programs from that exported by physical memory (RAM). A page table maps virtual to physical page numbers. Increased DRAM sizes have led to deeper page tables, increasing the number of memory references needed to look up a virtual page number. For example, x86 uses a 4-level page table, requiring up to four page table memory references to perform a single address translation.

Hardware memory virtualization, such as extended page tables (Intel) or nested page tables (AMD), requires additional indirection for each stage of memory address translation, making the process of resolving a virtual page number even more complex. With extended page tables, both the guest OS and host hypervisor perform virtual-to-physical translations to satisfy a single request, potentially amplifying the maximum cost to 24 lookups [59, 40] and increasing average latencies [67].

**Increased TLB Reach:**

Recently, Intel has moved to a two-level TLB design and provided a significant number of second-level TLB entries for huge pages, going from zero for Sandy Bridge and Ivy Bridge to 1,024 for Haswell [2] (2013) and 1,536 for Skylake [1] (2015). Better hardware support for multiple page sizes creates an opportunity for the OS and hypervisor but also puts stress on current memory management algorithms.

**Operating System Support for Huge Pages:**

Early operating system support for huge pages provided a separate interface for explicit huge page allocation from a dedicated huge page pool configured by the system administrator. Windows and macOS continue to have this level of support. In Windows, applications must use an explicit memory allocation API for huge page allocation [21], and Windows recommends that applications allocate huge pages all at once when they begin. macOS applications must set an explicit flag in the memory allocation API to use huge pages [15].

Initial huge page support in Linux used a similar separate interface for huge page allocation that developers must invoke explicitly (called hugetlbfs). Developers must configure a dedicated huge page pool, which is not user-friendly. Transparent huge page support, while more developer-friendly, creates memory management challenges in the operating system that Ingens addresses.

**Summary of Issues:**

| Name | Suite/Application | Description | Issue | OS/Hypervisor |
|------|-------------------|-------------|-------|---------------|
| 429.mcf | SPECCPU2006[33] | Single-threaded scientific computation | Page fault latency (§3.1) | O |
| Canneal | PARSEC3.0[28] | Parallel scientific computation | Bloat (§3.2) | O |
| SVM | Liblinear[22] | Machine learning, Support vector machine | Fragmentation (§3.3) | O, O |
| Tunkrank | PowerGraph[55] | Large-scale in-memory graph analytics | Unfair allocation (§3.5) | O, O |
| Nutch | Hadoop[4] | Web search indexing using MapReduce | Memory sharing (§3.6) | O, O |
| MovieRecmd | Spark/MLlib[5] | Machine learning, Movie recommendation | Memory sharing (§3.6) | O, O |
| Olio | Cloudstone[8] | Social-event web service (nginx/php/mysql) | Tail latency (§3.1) | O, O |
| Redis | Redis[29] | In-memory key-value store | Bloat (§3.2) | O, O |
| MongoDB | MongoDB[23] | In-memory NoSQL database | Bloat (§3.2) | O, O |

**Conclusion:**

Ingens is a memory management redesign that brings performance, memory savings, and fairness to memory-intensive applications with dynamic memory behavior. It is based on two principles: (1) memory contiguity is an explicit resource to be allocated across processes, and (2) good information about spatial and temporal access patterns is essential for managing contiguity. The measured performance of the Ingens prototype on realistic workloads validates the approach.