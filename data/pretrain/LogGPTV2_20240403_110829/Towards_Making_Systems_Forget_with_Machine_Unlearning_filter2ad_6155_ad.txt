<100%
100%
Analytical Results
Retrain
Unlearn
Speed
Speed
2)
O(nm
O(m
2)
O(q)
O(N q)
O(n)
O(N )
O(logN ) O(N logN ) O(N )
O(N p)
O(N l)
O(N k)
Speedup
Complete-
ness
Unlearn
Speed
Retrain
Speed
Empirical Results
Speedup
100%
100%
99.4%
100%
45s
4min56s
987ms
1day2hours
21.5ms
156ms
30mins
157ms
6.57
9.5×104
8.4×104
1
Modiﬁcation
LoC (%)
302 (0.3%)
21 (0.4%)
33 (0.8%)
30 (0.07%)
distance-based algorithm to cluster OSN wall posts into
a moderate number of clusters, and then builds a C4.5
decision tree [58] for classifying posts into “ham” or spam.
The clustering helps to make the classifying step real time.
For brevity, we refer to this system as OSNSF in the
remainder of this paper.
• PJScan [51] is an open-source PDF malware detector tool.
It uses one-class SVM which takes only malicious samples
as the training data to classify PDFs.
For each system, we focused our evaluation on the following
four research questions.
1) Is the system vulnerable to any attack exploiting machine
learning? We answered this question by either reproduc-
ing an existing attack if there is one or constructing a new
attack otherwise. To ensure that the attack is practical, we
used real-world workloads and typically injected different
portions of polluted data to observe how effective the
attack became.
2) Does our unlearning approach apply to the different
machine learning algorithms and stages of the system?
We answered this question by understanding the algo-
rithms in the system and revising them based on our
approach. Analytically, we also computed the unlearning
completeness and timeliness (i.e., asymptotic speedup
over retraining).
3) Empirically, how does our unlearning approach perform?
These empirical results are crucial to understand that
the actual performance of our approach matches the
analytical performance. We answered this question by
running the system on real-world data such as a Facebook
data set with over one million wall posts. We measured
completeness by comparing prediction results on test
data sets before and after unlearning; we additionally
studied whether unlearning prevents the attack against the
system. We measured timeliness by quantifying the actual
speedup of unlearning of retraining.
4) Is it easy to modify the system to support unlearning?
We answered this question by implementing our approach
in the system and counting the lines of code that we
modiﬁed.
Table I shows the summary of our results. All four evaluated
systems turned out to be vulnerable. For LensKit, we repro-
duced an existing privacy attack [29]. For each of the other
three systems, we created a new practical data pollution attack.
Our unlearning approach applies to all learning algorithms
470470
in three systems, LensKit, Zozzle, and PJScan, with 100%
completeness. For OSNSF, we leveraged existing techniques
for unlearning and got less than 100% completeness. For all
systems, the speedup of unlearning over retraining is often
asymptotically as large as the size of the training data set.
Empirically, using the largest real-world data sets we obtained,
we show that unlearning was 100% complete for except that
it was 99.4% for the OSNSF. It obtained up to 104× speedup
except for PJScan because its largest data set has only 65
PDFs, so the execution time was dominated by program start
and shutdown not learning. These empirical results match the
analytical ones. Lastly, modiﬁcation to support unlearning for
each system ranges from 20 – 300 lines of code (LoC), less
than 1% of total LoC of the system. The next four sections
describe these results in more detail for each system.
VI. UNLEARNING IN LENSKIT
We start by describing LensKit’s recommendation algo-
rithm. Recall that it by default recommends items to users
using item-item collaborative ﬁltering [37, 63] that computes
the similarity of every two items based on user ratings of
the items because, intuitively, similar items should receive
similar ratings from the same user. Operationally, LensKit
starts by constructing a user-item matrix based on historical
user ratings of items, where row i stores all ratings given by
user i, and column j all ratings received by item j. Then,
LensKit normalizes all ratings in the matrix to reduce biases
across users and items. For instance, one user’s average rating
may be higher than another user’s, but both should contribute
equally to the ﬁnal item-item similarity. Equation 2 shows the
normalized rating aij for rij, user i’s original rating of item
j, where μi is the average of all ratings given by user i, ηj the
average of all ratings received by item j, and g is the global
average rating.
(cid:3)
rij−μi−ηj +g
aij =
when
rij(cid:4)=null
(2)
0
when
rij=null
Based on the normalized user-item rating matrix, LensKit
computes an item-item similarity matrix within which the cell
at row k and column l represents the similarity between items
k and l. Speciﬁcally, as shown in Equation 3, it computes the
cosine similarity between columns k and l in the user-item
rating matrix, where ||(cid:3)x||2 represents the Euclidean norm of
(cid:3)x, and (cid:3)a∗,k is a vector, (a1k, a1k, ..., ank), representing all the
ratings received by item k.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
sim(k, l) =
(cid:2)a∗,k · (cid:2)a∗,l
||(cid:2)a∗,k||2||(cid:2)a∗,l||2
(3)
Now, to recommend items to a user, LensKit computes the
most similar items to the items previously rated by the user.
The workload that we use is a public, real-world data
set from movie recommendation website MovieLens [14]. It
has three subsets: (1) 100,000 ratings from 1,000 users on
1,700 movies, (2) 1 million ratings from 6,000 users on 4,000
movies, and (3) 10 million ratings from 72,000 users on
10,000 movies. We used LensKit’s default settings in all our
experiments.
A. The Attack – System Inference
Since there exists a prior system inference attack against
recommendation systems [29], we reproduced this attack
against LensKit and veriﬁed the effectiveness of the attack.
As described by Calandrino et al. [29], the attacker knows
the item-item similarity matrix and some items that the user
bought from the past. To infer a newly bought item of the
user, the attacker computes a delta matrix between the current
item-item similarity matrix and the one without the item. Then,
based on the delta matrix, the attacker could infer an initial list
of items that might lead to the delta matrix, i.e., potential items
that the user might have newly bought. Comparing the list of
inferred items and the user’s purchasing history, the attacker
could infer the newly bought item. Following the attack steps,
we ﬁrst record the item-item similarity matrix of LensKit and
one user’s rating history. Then, we add one item to the user’s
rating history, compute the delta matrix and then successfully
infer the added rating from the delta matrix and the user’s
rating history.
B. Analytical Results
To support unlearning in LensKit, we converted its recom-
mendation algorithm into the summation form. Equation 4
shows this process. We start by substituting (cid:3)a∗,k and (cid:3)a∗,l
in Equation 3 with their corresponding values in Equation 2
where n is the number of users and m the number of items, and
expanding the multiplications. We then simplify the equation
by substituting some terms using the ﬁve summations listed
in Equation 5. The result shows that our summation approach
applies to item-item recommendation using cosine similarity.
n(cid:4)
n(cid:4)
n(cid:4)
i=1
Sk =
Skl =
(rik − μi)(ril − μi)
i=1
(rik − μi) Sl =
n(cid:4)
(ril − μi)
n(cid:4)
i=1
(ril − μi)2
(5)
Skk =
(rik − μi)2
Sll =
i=1
i=1
We now discuss analytically the completeness and timeli-
ness of unlearning in LensKit. To forget a rating in LensKit,
we must update its item-item similarity matrix. To update the
similarity between items k and l, we simply update all the
summations in Equation 4, and then recompute sim(k, l) using
471471
Algorithm 1 Learning Stage Preparation in LensKit
Input:
+ rij
+ rij
All the users: 1 to n
All the items: 1 to m
end if
← Sumμi
← Sumηj
for j = 1 to m do
if rij (cid:4)= null then
end for
μi ← Sumμi /Countμi
Sumμi
Countμi ++
Sumηj
Countηj ++
Sumg ← Sumg + rij
Countg++
Process:
1: Initializing all the variables to zero
2: for i = 1 to n do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for
15: g ← Sumg/Countg
16: for k = 1 to m do
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27: end for
ηk ← Sumηk /Countηk
for i = 1 to n do
Sk ← Sk + (rik − μi)
end for
Calculate sim(k, l)
end for
for l = 1 to m do
end for
for i = 1 to n do
Skl ← Skl + (rik − μi) ∗ (ril − μi)
the summations. This unlearning process is 100% complete
because it computes the same value of sim(k, l) as recom-
puting from scratch following Equation 3. The asymptotic
time to unlearn sim(k, l) is only O(1) because there is only
a constant number of summations, each of which can be
updated in constant time. Considering all m2 pairs of items,
the time complexity of unlearning is O(m2). In contrast, the
time complexity of retraining from scratch is O(nm2) because
recomputing sim(k, l) following Equation 3 requires the dot-
product of two vectors of size n. Thus, unlearning has a
speedup factor of O(n) over retraining. This speedup is quite
huge because a recommendation system typically has much
more users than items (e.g., Netﬂix’s users vs movies).
Now that we have shown mathematically how to convert
LensKit’s item-item similarity equation into the summation
form and its analytical completeness and timeliness, we pro-
ceed to show algorithmically how to modify LensKit to sup-
port unlearning. While doing so is not difﬁcult once Equation 4
is given, we report the algorithms we added to provide a
complete picture of how to support unlearning in LensKit.
We added two algorithms to LensKit. Algorithm 1 runs
during the learning stage of LensKit, which occurs when the
system bootstraps or when the system operator decides to
retrain from scratch. This algorithm computes the necessary
summations for later unlearning. To compute the average
rating of each user i (μi, line 13), it tracks the number of
ratings given by the user (Countμi, line 6) and the sum
of these ratings (Sumμi, line 5). It similarly computes the
average rating of each item k (ηk, line 17) by tracking the
number of all ratings received by item k (Countηk, line 8)
and the sum of these ratings (Sumηk, line 7). It computes the
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
sim(k, l) =
=