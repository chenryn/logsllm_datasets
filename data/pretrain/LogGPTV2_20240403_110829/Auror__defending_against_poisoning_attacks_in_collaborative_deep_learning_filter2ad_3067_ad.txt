# Evaluation of AUROR in Detecting and Mitigating Malicious Users in Federated Learning

## 1. System Overview
In our federated learning system, both benign and malicious users submit their masked gradient values of the training data to the server for each iteration. Each gradient value corresponds to a specific parameter of the classifier. The server executes AUROR, which uses these masked features as input and outputs the final global model trained using AUROR. Our experiments were conducted on a server running Ubuntu Trusty (Ubuntu 14.04.3 LTS), equipped with 40 CPUs (E5-2660 v3) each having a processor speed of 2.6GHz and 64 GB RAM.

## 2. Evaluation Goals
We performed the evaluation with the following three goals:
- To measure the detection rate of AUROR for identifying malicious users.
- To evaluate the attack success rate of the global model trained using AUROR.
- To evaluate the accuracy drop of the global model compared to the benign model.

## 3. Handwritten Digit Images (MNIST Dataset)

### 3.1 Identifying Indicative Features
AUROR analyzes the distribution of gradient values uploaded by the users over several iterations. Figure 6 shows the comparison of the average gradient values for the 8th weight parameter of the third node in the first hidden layer for the first 10 iterations between benign and suspicious clusters. The average gradient value of benign users shows a similar distribution to that of malicious users, so AUROR discards this feature. Conversely, Figure 7 shows the comparison of the average gradient values for the bias parameter of the 8th node in the final layer. The gradient values exhibit two different kinds of distributions, leading AUROR to select this as an indicative feature. The number of masked features selected as indicative features varies for different experiments. For example, AUROR selects 64 indicative features for mislabeling 7 as 8, while it selects 67 indicative features when mislabeling 1 as 4 with 30% malicious users. All indicative features come from the final layer because parameters in the final layer have the largest influence on the final result.

### 3.2 Detecting Malicious Users
AUROR uses KMeans clustering to separate all users into different groups based on their uploaded indicative features. Groups with fewer than 50% of users are marked as suspicious. For each indicative feature, AUROR creates clusters of benign and suspicious users. Users that appear in the suspicious clusters more than 50% of the time are marked as malicious. We observed that the 12th and 14th benign users appeared in suspicious clusters three times out of 64 indicative features when mislabeling 7 as 8 with 30% malicious users. The detection rate is 100% for 10% to 30% of malicious users.

### 3.3 Evaluating the Final Model
For generating the final global model, AUROR removes the detected malicious users and retrains the model. We measured the attack success rate and accuracy drop of the final global model. The attack success rate reduces significantly after training the model without malicious users, falling below 5% for malicious ratios of 10%, 20%, and 30%. The accuracy drop is very small, only 3% when the fraction of malicious users is 30%, indicating that the overall accuracy of the image recognition system remains similar to the benign model even after excluding the training data from malicious users.

| Malicious Ratio (%) | Accuracy Drop (%) |
|---------------------|--------------------|
| 10                  | 0                  |
| 20                  | 1                  |
| 30                  | 3                  |

## 4. German Traffic Sign Benchmarks (GTSRB Dataset)

### 4.1 Identifying Indicative Features
AUROR analyzes the distribution of gradient values uploaded by the users over several iterations. Figure 8 shows the comparison of the gradient values for the 547th weight of the final layer for the first 10 iterations. The gradient values exhibit two different kinds of distributions, leading AUROR to select this as an indicative feature. The number of masked features selected as indicative features varies for different experiments. For example, AUROR selects 36 indicative features when mislabeling the sign of a bicycle crossing as a wild animal crossing with 30% malicious users, and 55 indicative features when mislabeling the traffic sign of 20 km/h maximum speed limit as 80 km/h. All indicative features come from the final layer, confirming that the parameters in the final layer are easy to change due to their significant influence on the final result.

### 4.2 Detecting Malicious Users
For each indicative feature, AUROR creates clusters of benign and suspicious users. Users that occur in the suspicious clusters with a frequency less than 50% are marked as malicious. The second and third benign users both appeared in the suspicious clusters twice out of 9 indicative features when mislabeling the sign of 20 km/h maximum speed limit as 80 km/h with 10% malicious users. The detection rate is 100% for 10% to 30% of malicious users.

### 4.3 Evaluating the Final Model
To measure the effectiveness of our solution, we calculated the accuracy drop after retraining the model using AUROR on the GTSRB dataset. The accuracy drop is negligible for malicious ratios from 10% to 30%, indicating that the overall accuracy of the model is not drastically affected by removing the dataset contributed by malicious users. The attack success rate is below 5% for malicious ratios from 10% to 30%.

| Fraction of Malicious Users (%) | Success Rate (%) | Accuracy Drop (%) |
|--------------------------------|------------------|-------------------|
| 10                             | 1                | 0                 |
| 20                             | 2                | 0                 |
| 30                             | 2                | 0                 |

## 5. Evading AUROR

There are two main strategies to evade AUROR's detection mechanism:
1. **Decreasing the fraction of malicious users**: Reducing the number of malicious users can decrease the influence of poisoned data on the global model. For the MNIST dataset, when the poison set of attackers is 100% malicious data, the detection rate of AUROR is 100% even with one malicious user among 30 participants. For the GTSRB dataset, the detection rate is 60% when the number of malicious users is reduced to one, with an attack success rate of 3% and an accuracy drop of 1%.
2. **Decreasing the number of malicious samples**: Reducing the number of malicious samples in the training set of malicious users can also evade detection. For the MNIST dataset, when the fraction of malicious users is 20%, the average detection rate of AUROR is 100% even when only 20% of the training set of each adversary is poisoned. However, the average detection rate drops to 0% when the fraction of malicious data reduces to 14%, with an average attack success rate of 34% and an accuracy drop of 3%.

| Fraction of Malicious Users (%) | Fraction of Malicious Data (%) | DR (%) | SR (%) | AD (%) |
|--------------------------------|--------------------------------|--------|--------|--------|
| 10                             | 14                             | 1      | 0      | 34     |
| 20                             | 33                             | 0      | 74     | 54     |
| 30                             | 21                             | 0      | 20     | 100    |

Thus, AUROR is robust and a promising solution against evasion, as decreasing the number of malicious samples or users does not significantly impact the overall accuracy and attack success rate.

**Result 2**: A robust and strong defense against targeted poisoning attacks is possible based on the masked features and by exploiting the limited poisoning characteristics of indirect collaborative deep learning systems.