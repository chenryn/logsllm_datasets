a
v
t
n
e
i
d
a
r
g
e
g
a
r
e
v
A
1 · 10
−2
0
−1 · 10
−2
2
4
6
Iteration
8
10
Benign cluster
Suspicious cluster
2
4
6
Iteration
8
10
Benign cluster
Suspicious cluster
Figure 6: For MNIST dataset, the gradient value for 8th weight
parameter of third node in ﬁrst hidden layer for mislabeling 7
as 8. All the users cannot be distinguished over 10 iterations.
Figure 7: For MNIST dataset, the gradient value for the bias
parameter of 8th node in ﬁnal layer for mislabeling 7 as 8. The
gradient shows different distribution for benign and malicious
users.
tems. The poison set of malicious user includes mislabeled source
images. Both the benign and malicious users submit their masked
gradient values of the training data as features to the server for each
iteration. Each gradient value corresponds to a particular parameter
of the classiﬁer. The server in our setting executes AUROR where
the input is the masked features from the users and output is the
ﬁnal global model trained using AUROR. We perform our experi-
ments on a server running on Ubuntu Trusty (Ubuntu 14.04.3 LTS),
equipped with 40 CPUs E5-2660 v3 each having a processor speed
of 2.6GHz and 64 GB RAM.
Evaluation Goals. We perform the evaluation with the following
three goals:
malicious users.
• To measure the detection rate of AUROR for identifying the
• To evaluate the attack success rate of global model trained
• To evaluate the accuracy drop of the global model as com-
using AUROR.
pared to the benign model.
6.1 Handwritten Digit Images
Identifying Indicative Features. AUROR analyses the distribu-
tion of gradient values uploaded by the users for several iterations
of the training phase. Figure 6 shows the comparison of the av-
erage gradient values for 8th weight parameter of third node in
ﬁrst hidden layer for the ﬁrst 10 iterations between benign cluster
and suspicious cluster. The average gradient value of benign users
shows a similar distribution as the malicious users during the iter-
ations. Hence, AUROR discards this feature and does not use it in
the future steps. On the contrary, Figure 7 shows the comparison
of the average gradient values for the bias parameter of 8th node
in ﬁnal layer. The gradient values exhibit two different kinds of
distribution. Based on the anomalous behavior AUROR selects this
as a indicative feature. The number of masked features selected as
indicative features varies for different experiments. For example,
AUROR selects 64 indicative features for mislabeling 7 as 8 while
67 indicative features when mislabeling 1 as 4 with 30% malicious
users. In addition, we notice that all the indicative features come
from the ﬁnal layer, because the parameters in ﬁnal layer have the
largest inﬂuence towards the ﬁnal result. Hence, the changes to
ﬁnal layer’s parameter are larger than the other parameters.
Malicious Ratio (%) Accuracy Drop (%)
10
20
30
0
1
3
Table 3: Accuracy drop for MNIST dataset after retraining the
model using AUROR for 10% to 30% malicious ratio
Detecting Malicious Users. AUROR uses KMeans clustering algo-
rithm to separate all the users into different groups based on their
uploaded indicative features. The groups with fraction of users
within 50% are marked as suspicious. For every indicative feature,
it creates clusters of benign and suspicious users. The users that
appear in the suspicious clusters for more than τ = 50% of the in-
dicative features are marked as malicious. We observe that the 12th
and 14th benign users appear in suspicious clusters three times of
64 indicative features when we mislabel 7 as 8 with 30% of mali-
cious users. The value of τ shows the tolerance of AUROR towards
the minor difference between benign users. We calculate the detec-
tion rate based on the users marked as malicious by AUROR and the
actual number of malicious users in every experiment. We observe
that the detection rate is 100% for 10% to 30% of malicious users.
Evaluating the Final Model. For generating the ﬁnal global model,
AUROR removes the users detected as malicious in the previous
step and trains the model. We measure the attack success rate and
accuracy drop of this ﬁnal global model trained using AUROR de-
fense. We observe that the attack success rate reduces largely after
training the model without the malicious user, which are all below
5% when malicious ratio is 10%, 20% and 30%.
We measure the accuracy drop of the ﬁnal global model as com-
pared to the benign model and study the improvement over the poi-
soned model. Figure 3 shows the accuracy drop for malicious ra-
tios ranging from 10% to 30%. We observe that the accuracy drop
is very small as compared to the benign model. It is only 3% when
the fraction of malicious users is 30%. This highlights an important
ﬁnding that the overall accuracy of the image recognition system
remains similar to the accuracy of benign model even after exclud-
ing the training data from malicious users.
515e
u
l
a
v
t
n
e
i
d
a
r
g
e
g
a
r
e
v
A
1 · 10
−3
0
−1 · 10
−3
2
4
6
Iteration
Benign cluster
Suspicious cluster
Figure 8: In GTSRB dataset, the gradient value for the 547th
weight of ﬁnal layer for the ﬁrst 10 iterations. The gradient
show different distribution for benign and malicious users.
6.2 German Trafﬁc Sign Benchmarks
The German Trafﬁc Sign Benchmarks (GTSRB) dataset is a col-
lection of images used to generate models for using in auto-driving
cars. The trained model classiﬁes a sign into one of the 43 classes.
Identifying Indicative Features. AUROR analyses the distribution
of gradient values uploaded by the users for several iterations of
the training phase. Figure 8 shows the comparison of the gradient
values for 547th weight of ﬁnal layer for the ﬁrst 10 iterations. The
gradient values exhibit two different kinds of distribution. Based on
the anomalous behavior AUROR selects this as a indicative feature.
The number of masked features selected as indicative features vary
for different experiments. For example, AUROR selects 36 indica-
tive features when we mislabel the sign of bicycle crossing as the
sign of wild animal crossing with 30% malicious users, while se-
lects 55 indicative features when we mislabel the trafﬁc sign of 20
km/h maximum speed limit as 80 km/h. In addition, we also ob-
serve the distribution of all indicative features. The indicative fea-
tures are all from the ﬁnal layer of the model, which conﬁrms the
ﬁnding that the parameters of ﬁnal layer are easy to change since
they have the largest inﬂuence over ﬁnal result compared with other
parameters.
Detecting Malicious Users. For each indicative feature, AUROR
creates clusters of benign and suspicious users. The users that occur
in the suspicious clusters with the frequency less than τ = 50% are
marked as malicious. Like MNIST, we have the same observation
that the second and third benign users both appear in the suspicious
clusters twice of 9 indicative features when we mislabel the sign of
20 km/h maximum speed limit as 80 km/h with 10% of malicious
users. We calculate the detection rate based on the users marked as
malicious using AUROR and the actual number of malicious users
in every experiment. We observe that the detection rate is 100% for
10% to 30% of malicious users.
Evaluating the Final Model. To measure the effectiveness of our
solution, we calculate the accuracy drop after retraining the model
using AUROR on the GTSRB dataset. Table 4 shows the accuracy
drop as compared to the benign setting when mislabeling a sign of
bicycle crossing as a sign of wild animal crossing. The accuracy
drop is negligible for the malicious ratios from 10% to 30%, indi-
cating that the overall accuracy of the model is not affected drasti-
cally by removing the dataset contributed by malicious users. We
measure the attack success rate of the retrained model and report
that it is below 5% for malicious ratio from 10% to 30%.
Fraction of
Malicious Users (%)
Success
Rate (%)
Accuracy
Drop (%)
10
20
30
1
2
2
0
0
0
Table 4: Attack success rate and accuracy drop for GTSRB
dataset after retraining the model using AUROR for 10% to
30% malicious ratio
8
10
6.3 Evading AUROR
There are two main approaches to evade AUROR’s detection mech-
anism. The ﬁrst strategy is to decrease the fraction of malicious
users so that the inﬂuence of the poisoned data on the global model
is reduced. Our experiments demonstrate for MNIST dataset, when
the poison set of attackers is 100% malicious data, the detection
rate of our method AUROR is 100% even when there is only one
malicious user among 30 participants. To mislabel the sign of bicy-
cle crossing as the sign of wild animal crossing in GTSRB dataset,
the detection rate is 60% when number of malicious users is re-
duced to one. Although the detection rate is 60%, we ﬁnd that the
attack success rate is only 3% with 1% accuracy drop. Thus, our
experiments conﬁrm that decreasing the number of malicious users
can evade detection mechanism of AUROR in some cases but the
ﬁnal result generated by these poisoning attack cannot achieve the
attackers’ goal of misclassifying data.
The second strategy is to decrease the number of malicious sam-
ples in malicious users’ training set. Table 5 shows the average
detection rate, average attack success rate and average accuracy
drop with various combination of fraction of malicious data and
fraction of malicious users for MNIST dataset when misclassifying
5 as 3. When the fraction of malicious users is 20%, the average
detection rate of AUROR on MNIST is 100% even when only 20%
of the training set of each adversary is poisoned, while the average
detection rate is 0% when the fraction of malicious data reduces to
14%. Although the average detection rate drops to 0%, the aver-
age attack success rate is only 34% with 3% of average accuracy
drop. Hence, the adversaries can decrease the number of malicious
samples in their training set to decrease the distance of gradients
between malicious users and benign users (α). However, while
evading AUROR, the average attack success rate and average accu-
racy drop become relatively low such that the goal of adversaries
cannot be achieved. Table 6 shows similar relation of average de-
tection rate, average attack success rate and average accuracy drop
for different fraction of malicious data and fraction of malicious
users while mislabeling the sign of bicycle crossing as the sign of
wild animal crossing in the GTSRB dataset. In addition, we ob-
serve that the attack success rate varies between a wide range. For
example, the minimum and maximum attack success rate are 1.6%
and 57.8% respectively for 90% of malicious data and 20% of mali-
cious user when mislabeling the sign of bicycle crossing as the sign
of wild animal crossing in the GTSRB dataset. Even with such a
variation, the detection rate is 100%. Thus, AUROR is robust and a
promising solution against evasion.
Result 2: A robust and strong defense against targeted poisoning
attacks is possible based on the masked features and by exploiting
the limited poisoning characteristics of indirect collaborative deep
learning systems.
516Fraction of
Malicious Users (%) Metrics (%)
10
20
30
DR
SR
AD
DR
SR
AD
DR
SR
AD
Fraction of
Malicious Data (%)
14
33
21
1
0
34
3
74
54
4
20
100
21
2
100
45
3
100
68
5
18