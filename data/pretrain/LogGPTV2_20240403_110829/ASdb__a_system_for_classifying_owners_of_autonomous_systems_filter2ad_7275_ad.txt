Stage
Matched By ASN
Classifier
0 Sources Matched
1 Sources Matched
≥2 Sources Matched — ≥ 2 Agree
≥2 Sources Matched — None Agree
Overall Layer 1
Layer 2 — Tech
Layer 2 — Not Tech
Overall Layer 2
Uniform Gold Standard
Coverage Accuracy Coverage Accuracy Coverage Accuracy
14%
29%
3%
18%
30%
5%
97%
95%
89%
93%
100%
97%
–
80%
100%
60%
93%
74%
80%
75%
100%
98%
–
92%
100%
89%
97%
89%
82%
87%
15%
20%
4%
14%
40%
7%
96%
98%
92%
96%
5%
12%
5%
13%
54%
12%
95%
99%
97%
98%
94%
90%
–
78%
96%
68%
89%
89%
76%
82%
Table 8: Evaluation of ASdb Stages— ASdb provides a layer 1 and layer 2 classification for at least 93% of all ASes across all three data
sets and achieves a 93% Layer 1 accuracy on the test set. We note that NAICSlite layer 2 coverage can be greater than NAICSlite layer 1
coverage, as only the ASes with a labeler-assigned NAICSlite layer 2 category (142, 141, and 189 for the three data sets, respectively) are
evaluated in NAICSlite layer 2 metrics. Recall that our sample size is 150 ASes for the Gold Standard and test set, and 320 for the Uniform
Gold Standard set.
2). We include the Uniform Gold Standard data set to demonstrate
ASdb’s performance at classifying non-technology ASes.
Performance Breakdown. ASdb provides a layer 1 and layer 2
classification for at least 93% of ASes across all three data sets (Ta-
ble 8). This is at least 25% better coverage than any individual data
source (Table 5). ASdb also achieves accuracy on par with or bet-
ter than external data sources (Table 4) at the layer-1 granularity
—achieving a 93%, 97% and 89% accuracy for the test, Gold Stan-
dard, and Uniform Gold Standard sets respectively—and layer 2
granularity—achieving a 75%, 87% and 82% accuracy for the test,
Gold Standard, and Uniform Gold Standard sets respectively. The
weakest points in the ASdb pipeline correspond to cases where
there is no multi-source agreement (60% accuracy on the test set
where ≥2 sources matched but none agree, 80% accuracy where
only 1 source matched). Furthermore, without additional manual
review, ASdb cannot classify the 4% of test-set ASes where no data
sources match.
To assess ASdb’s coverage
Layer 1 Precision and Coverage.
and accuracy across the long tail of NAICSlite layer-1 categories, we
perform a per-category analysis using the Uniform Gold Standard
dataset (Table 10). Predictably, ASdb’s coverage and accuracy is
dependent on external data sources’ coverage and accuracy. ASdb
consistently achieves nearly identical coverage compared to the
data source with the best coverage in the same NAICSlite layer 1 cat-
egory, while achieving equivalent or better accuracy across 9/16 of
categories. The lower precision ASdb achieves in certain categories,
as compared to the most accurate data source, is due in all but one
case to the most accurate data source—Crunchbase—exhibiting
coverage up to 5 times worse.
Number of Applied NAICSlite Categories. We confirm that
ASdb does not achieve its measured accuracy by inflating the num-
ber of categories it assigns to each AS. Of the 142 test ASes that
ASdb successfully classifies, 84 (59%) are assigned only 1 layer-2
NAICSlite category, 16 (11%) are assigned 2 categories, and the
maximum number of assigned categories is 10. At the layer-1 level,
104 (73%) are assigned 1 NAICSlite category, 20 (14%) are assigned
2 categories, and the maximum number of layer-1 categories is 4.
For both layer-1 and layer-2, the long tail is even sparser for the
Gold Standard than it is for the test set.
Comparison With Prior Works. ASdb offers at least 89 addi-
tional categories compared to the most popular AS classification
databases: IPinfo, which offers 4 categories, and PeeringDB, which
offers 6. To compare ASdb’s performance with IPinfo, we map IPinfo
and NAICSlite’s hosting, ISP, and education categories to each other,
and also map all other 92 NAICSlite categories to IPinfo’s “busi-
ness.” To compare with PeeringDB, we map PeeringDB’s content,
711
IMC ’21, November 2–4, 2021, Virtual Event, USA
Maya Ziv, Liz Izhikevich, Kimberly Ruth, Katherine Izhikevich, and Zakir Durumeric
enterprise and non-profit, education, and all remaining categories
to IPinfo’s hosting, business, education, and ISP categories, respec-
tively. We note that ASdb and IPinfo/PeeringDB are not mutually
independent, as ASdb relies on both as data sources, but they still
serve as a useful benchmark.
ASdb is able to categorize 3 times and 7 times more ASes than
IPinfo and PeeringDB, respectively. We compute the F1 metric
as a proxy for accuracy, and discover that ASdb always performs
better (Table 7). Nonetheless, we notice it only achieves an F1-score
of 65% for hosting providers in the test set (albeit still 2.7 times
more accurate than IPinfo). Upon further investigation, 17% of all
hosting providers do not have domains (i.e., are exceptionally hard
to categorize), 9% have no data source matches, and another 9%
were marked as non-hosting by at least two data sources, even
when our classifier classified the AS as hosting.
Overall, we find that ASdb is 2.5–6 times more accurate when
classifying hosting providers, 1.3–2.5 times more accurate when
classifying ISPs, 1.1–5 times more accurate when classifying edu-
cation entities, and 1.3–12 times more accurate when classifying
business entities than in prior works.
5.3 Maintaining ASdb
Between October 2020 and February 2021, an average 21 ASes were
registered every day, belonging to an average 19 new organizations.
Furthermore, 4% of all registered ASes changed their ownership
metadata at least once during that period. It is crucial that ASdb is
easily updated, as we estimate an average of 140 ASes will need to
be updated every week.
ASdb will be primarily maintained by automatically querying
data sources available to our research group. We have also inte-
grated a simple way for the research community to submit AS
classification corrections; submitted corrections will be verified
by a human prior to ASdb integration. For all system components
that require human intervention, we plan to devise a community
program that requests users of ASdb to periodically complete a
human-maintenance task (e.g., review corrections, fetch Zvelo data).
6 CONCLUSION
In this paper, we introduced ASdb, a system that classifies 96% of
ASes with a 93% and 75% accuracy on 17 industry categories and 95
sub-categories, respectively. ASdb allows the research community
to understand the largely overlooked long-tail of industry sectors
that run the Internet. ASdb adds a novel perspective to even the
most well-studied research questions: for example, we join ASdb’s
dataset with an Internet Telnet scan (using a 1% IPv4 LZR [38]
scan conducted in March 2021 across 65,535 ports), and alarmingly
find that critical-infrastructure organizations like electric utility
companies, government organizations, and financial institutions
are more likely to host Telnet than technology companies.
The process of building ASdb in and of itself also offers insights
for the Internet measurement community. We show that business-
oriented databases can be applied to networking-specific problems.
Nonetheless, data sources not tailored towards the technology com-
munity (e.g., business databases) should not be solely relied upon,
as they consistently provide worse coverage and accuracy for data
pertaining to technology entities. We learn that crowdwork is not
712
the most promising solution; machine learning and simple heuris-
tics perform with nearly the same accuracy at a fraction of the cost.
Nonetheless, aggregating existing data sources—no matter their
coverage or accuracy—and different classification solutions (e.g. ma-
chine learning, simple heuristics), helps build the best-performing
classification system.
We designed ASdb to be extendable and maintainable, and we
plan to release it and the resulting dataset at asdb.stanford.edu.
ACKNOWLEDGEMENTS
We thank Natasha Sharp, Julie Plummer, and Casey Mullins for sup-
port with project logistics and data labeling. We thank David Adrian,
Fengchen Gong, Catherine Han, Hans Hanley, Tatyana Izhikevich,
Deepak Kumar, and Gerry Wan for providing feedback on the paper,
and members of the Stanford Empirical Security Research Group
for valuable discussions. We thank the anonymous reviewers and
shepherd Romain Fontugne for their helpful comments. This work
was supported in part by the National Science Foundation under
award CNS-1823192, two NSF Graduate Fellowships DGE-1656518,
a Stanford Graduate Fellowship, and gifts from Google, Inc., Cisco
Systems, Inc., and Comcast Corporation.
REFERENCES
[1] 2017 NAICS definitions. https://www.census.gov/eos/www/naics/
2017NAICS/2017_Definition_File.pdf.
[2] About Amazon Mechanical Turk. https://www.mturk.com/worker/
help.
[3] Amazon Mechanical Turk. https://www.mturk.com.
[4] Appen. https://appen.com.
[5] The CAIDA UCSD AS classification dataset. https://www.caida.org/
[6] The CAIDA UCSD PeeringDB dataset. https://www.caida.org/data/
data/as-classification/.
peeringdb.xml.
[7] Clearbit. https://clearbit.com.
[8] ClickWorker. https://www.clickworker.com.
[9] Crunchbase. http://crunchbase.com.
[10] Dun & Bradstreet. https://www.dnb.com/.
[11] Google Translate. https://translate.google.com/.
[12] Inferred AS to organization mapping dataset. https://www.caida.org/
catalog/datasets/as-organizations.
[13] IPinfo.io. https://ipinfo.io/.
[14] Lab In The Wild. https://www.labinthewild.org.
[15] LinkedIn. http://linkedin.com.
[16] Minimum wage.
minimumwage.
https://www.dol.gov/general/topic/wages/
[17] NAICS Association. https://www.naics.com.
[18] Prolific. https://www.prolific.co.
[19] Unit testing pages or components. https://tapestry.apache.org/unit-
testing-pages-or-components.html.
[20] Upwork. https://www.upwork.com.
[21] Wikipedia. http://wikipedia.org.
[22] WorkFusion. https://www.workfusion.com.
[23] ZoomInfo. http://zoominfo.com.
[24] A. Akusok, Y. Miche, J. Karhunen, K.-M. Björk, R. Nian, and A. Lendasse.
Arbitrary category classification of websites based on image content.
IEEE Computational Intelligence Magazine, 10(2), 2015.
[25] S. Albakry, K. Vaniea, and M. K. Wolters. What is this URL’s destina-
tion? empirical evaluation of users’ URL reading. In Proceedings of the
2020 CHI Conference on Human Factors in Computing Systems, 2020.
ASdb: A System for Classifying Owners of Autonomous Systems
IMC ’21, November 2–4, 2021, Virtual Event, USA
[26] N. M. Barbosa and M. Chen. Rehumanized crowdsourcing: A labeling
framework addressing bias and ethics in machine learning. In 2019
CHI Conference on Human Factors in Computing Systems, 2019.
[27] A. Baumann and B. Fabian. Who runs the Internet? Classifying Au-
tonomous Systems into industries. In Proceedings of the 2014 Interna-
tional Conference on Web Information Systems and Technologies, 2014.
[28] J. Beel, B. Gipp, S. Langer, and C. Breitinger. Paper recommender
systems: a literature survey. Intl. Journal on Digital Libraries, 2016.
[29] D. Bounov, A. DeRossi, M. Menarini, W. G. Griswold, and S. Lerner.
Inferring loop invariants through gamification. In Proceedings of the
2018 CHI Conference on Human Factors in Computing Systems, 2018.
[30] R. Bruni and G. Bianchi. Website categorization: A formal approach
and robustness analysis in the case of e-commerce detection. Expert
Systems with Applications, 142(113001), 2020.
[31] X. Cai, J. Heidemann, B. Krishnamurthy, and W. Willinger. Towards
an AS-to-organization map. In ACM Internet Measurement Conference,
2010.
[32] A. Dhamdhere and C. Dovrolis. Twelve years in the evolution of the
Internet ecosystem. IEEE/ACM Transactions on Networking, 19(5), 2011.
[33] X. Dimitropoulos, D. Krioukov, G. Riley, and k. claffy. Revealing the
Autonomous System taxonomy: The machine learning approach. In
Passive and Active Network Measurement Workshop (PAM), Adelaide,
Australia, Mar 2006. PAM 2006.
[34] V. Giotsas, C. Dietzel, G. Smaragdakis, A. Feldmann, A. Berger, and
E. Aben. Detecting peering infrastructure outages in the wild. In ACM
SIGCOMM, 2017.
[35] E. Han, L. M. Powell, S. N. Zenk, L. Rimkus, P. Ohri-Vachaspati, and F. J.
Chaloupka. Classification bias in commercial business lists for retail
food stores in the U.S. International Journal of Behavioral Nutrition
and Physical Activity, 9(46), 2012.
[36] K. Hara, A. Adams, K. Milland, S. Savage, C. Callison-Burch, and J. P.
Bigham. A data-driven analysis of workers’ earnings on Amazon
Mechanical Turk. In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems, 2018.
[37] J. Huh, H. S. Heo, J. Kang, S. Watanabe, and J. S. Chung. Augmentation
adversarial training for self-supervised speaker recognition. In NeurIPS
Workshop on Self-Supervised Learning for Speech and Audio Processing,
2020.
[38] L. Izhikevich, R. Teixeira, and Z. Durumeric. LZR: Identifying unex-
pected Internet services. In 30th USENIX Security Symposium, 2021.
[39] G. Jäger, L. Zilian, C. Hofer, and M. Füllsack. Crowdworking: work-
ing with or against the crowd? Journal of Economic Interaction and
Coordination, 14:761–788, 2019.
[40] H. Jhamtani and T. Berg-Kirkpatrick. Modeling self-repetition in music
generation using generative adversarial networks. In 36th International
Conference on Machine Learning, 2019.
[41] T. Joachims. Text categorization with support vector machines: Learn-
ing with many relevant features. In European Conference on Machine
Learning, pages 137–142. Springer, 1998.
[42] K. M. Kahle and R. A. Walkling. The impact of industry classifica-
tions on financial research. The Journal of Financial and Quantitative
Analysis, 31(3):309–335, 1996.
[43] G. Y. Kebe, P. Higgins, P. Jenkins, K. Darvish, R. Sachdeva, R. Barron,
J. Winder, D. Engel, E. Raff, and C. M. Francis Ferraro. A spoken
language dataset of descriptions for speech-based grounded language
learning. In NeurIPS, 2021.
[44] V. Koshy, J. S. Park, T.-C. Cheng, and K. Karahalios. “We just use what
they give us”: Understanding passenger user perspectives in smart
homes. In Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems, 2021.
[45] J. Krishnan and E. Press. The North American Industry Classification
System and its implications for accounting research. Contemporary
Accounting Research, 20(4):685–717, 2003.
[46] E. Liu, G. Akiwate, M. Jonker, A. Mirian, S. Savage, and G. M. Voelker.
Who’s got your mail? Characterizing mail service provider usage. In
ACM Internet Measurement Conference, November 2021.
[47] D. López-Sánchez, A. G. Arrieta, and J. M. Corchado. Visual content-
based web page categorization with deep transfer learning and metric
learning. Neurocomputing, 338:418–431, 2019.
[48] R. Motamedi, R. Rejaie, and W. Willinger. A survey of techniques for
Internet topology discovery. IEEE Communications Surveys & Tutorials,
17(2):1044–1065, 2015.
[49] L. O’Connor. Approaching the challenges and costs of the North
American Industrial Classification System (NAICS). The Bottom Line,
2000.
[50] E. Peer, J. Vosgerau, and A. Acquisti. Reputation as a sufficient condi-
tion for data quality on Amazon Mechanical Turk. Behavior research
methods, 46(4):1023–1031, 2014.
[51] P. Peng, L. Yang, L. Song, and G. Wang. Opening the blackbox of
VirusTotal: Analyzing online phishing scan engines. In ACM Internet
Measurement Conference, 2019.
[52] R. L. Phillips and R. Ormsby.
Industry classification schemes: An
analysis and review. Journal of Business & Finance Librarianship, 2016.
[53] X. Qi and B. D. Davison. Web page classification: Features and algo-
rithms. ACM Computing Surveys, 41(2):Article 12, 2009.
[54] S. Rajpal, K. Goel, and Mausam. POMDP-based worker pool selection
for crowdsourcing. In 32nd Intl. Conference on Machine Learning, 2015.
[55] J. Ramos. Using TF-IDF to determine word relevance in document
queries. In Proceedings of the First Instructional Conference on Machine
Learning, volume 242, pages 29–48, 2003.
[56] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do ImageNet classifiers
generalize to ImageNet? In 36th Intl. Conf. on Machine Learning, 2019.
[57] M. Roughan, W. Willinger, O. Maeneel, D. Perouli, and R. Bush. 10
lessons from 10 years of measuring and modeling the Internet’s Au-
tonomous Systems. IEEE Selected Areas in Communications, 2011.
[58] M. Ruef and K. Patterson. Credit and classification: The impact of
industry boundaries in nineteenth-century America. Administrative
Science Quarterly, 54(3):486–520, 2009.
[59] S. Shakkottai, M. Fomenkov, R. Koga, D. Krioukov, and K. C. Claffy.
Evolution of the Internet AS-level ecosystem. The European Physical
Journal B, 74:271–278, 2010.
[60] P. Vallina, V. Le Pochat, Á. Feal, M. Paraschiv, J. Gamba, T. Burke,
O. Hohlfeld, J. Tapiador, and N. Vallina-Rodriguez. Mis-shapes, mis-
takes, misfits: An analysis of domain classification services. In ACM
Internet Measurement Conference, 2020.