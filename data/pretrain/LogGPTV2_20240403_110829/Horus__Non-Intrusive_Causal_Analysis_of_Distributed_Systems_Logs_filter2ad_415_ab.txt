the same process and a precedes b in the execution, and a
and b belong to different processes and a is the sending of a
message m and b is the reception of m.
The happens-before relation is transitive, irreﬂexive and
antisymmetric. When a (cid:2) b and b (cid:2) a, then a and b are
considered to be concurrent.
The ability to order log messages respecting causality is thus
paramount to effectively troubleshoot a distributed execution,
as illustrated by the motivating example in the next section.
B. Motivating Example – TrainTicket
TrainTicket [1] is an open-source train ticket application
developed to foster research on fault analysis and debugging
of distributed systems, namely those based on microservices.
It consists of a ticket booking application with multiple func-
tionalities: ticket inquiry, reservation, payment, order updates,
and user notiﬁcations. TrainTicket’s architecture comprises
40+ microservices written in different programming languages
(e.g., Java, Nodejs, Python, Go), along with a user interface
and third-party components, namely MongoDB for data stor-
age and RabbitMQ for message queueing.
We use TrainTicket in this work because it mimics a real-
world, complex distributed system and already contains 22
representative faults collected from a recent industrial survey
on microservice applications [1]. Each fault was injected into
an independent source code snapshot, available at a public
repository.1 For this motivating example, we use the one
labeled as F13, as it represents an order violation caused
by a message race, which is a common type of distributed
concurrency bug [13].
a) The F13 fault: The F13 fault results from the inter-
leaved processing of two messages arriving concurrently at the
system. In TrainTicket, each order has a property representing
its current state. When an order is created, it has state UNPAID.
The order state can then migrate to either PAID or CANCELED,
according to whether the client issues a Payment Order or a
Cancel Order, respectively.
The Payment Order succeeds when both the following
conditions hold: i) the client has enough funds, and ii) the
order state transition from UNPAID to PAID is valid. Otherwise,
the request fails, and the order state remains unchanged. In
turn, the Cancel Order succeeds if the state can be set to
CANCELED.
Since both requests change the state of an order and
the TrainTicket application processes them without synchro-
nization guarantees,
their concurrent execution may non-
deterministically lead to a payment failure.
The test driver for replicating this error in TrainTicket
consists in a client application, Launcher, that ﬁrst books a
1https://fudanselab.github.io/research/MSFaultEmpiricalStudy
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:14 UTC from IEEE Xplore.  Restrictions apply. 
213
1
2
3
4
5
6
7
8
9
10
11
12
[Launcher-1.1] - [Reservation Result] Success
[Payment-1.1] - [URI:/pay][Request: {"orderId":"652aaf9b"}]
[Order-1.1] - [URI:/getById][Request: {"orderId":"652aaf9b"}]
[Order-1.2] - Response: {"status":true, order":{"id":"652aaf9b", "status":"UNPAID"}}
[Payment-1.2] - Response: "false"
[Cancel-1.1] - [URI:/cancelOrder][Request: {"orderId":"652aaf9b"}]
[Order-2.1] - [URI:/getById][Request: {"orderId":"652aaf9b"}]
[Order-2.2] - Response: {"status":true, order":{"id":"652aaf9b", "status":"CANCELED"}}
[Payment-2.1] - [URI:/drawBack][Request: {"userId":"c01d7008"}]
[Payment-2.2] - Response: "true"
[Cancel-1.2] - Response: {"status":true, "message":"Success."}
[Launcher-1.2] - java.lang.RuntimeException: [Error Queue]
Fig. 1: The set of log records aggregated by Elastic Stack for the failed payment request. Records are ordered by the timestamp
assigned at the moment they were collected. To ease readability, we tag each record with the service in which it occurred and
a local event counter (e.g., Launcher-1.2 is the second event generated by the ﬁrst thread in the Launcher service).
train trip with a new client account and, then, issues concurrent
Payment Order and Cancel Order requests. When the payment
fails, TrainTicket renders a page with the error message.
identiﬁed the portion of the logs relevant to the analysis, he
ﬁnally gathers the log messages that will hopefully reveal the
root cause of the failure.
C. Debugging F13 with Elastic Stack
The increasing size and complexity of architectures mo-
tivated the industry to adopt toolsets tailored for distributed
log monitoring. Among those toolsets, the Elastic Stack [8] is
arguably one of the most popular and widely used. It provides
support for collecting large sets of log data from multiple
sources (via Logstash), analyze them using queries and ﬁlters
(via Elasticsearch), and creating user-friendly visualization
dashboards (via Kibana).
However, the Elastic Stack does not guarantee that log data
is causally ordered, which hinders its effectiveness to diagnose
unexpected behavior in distributed systems. In this section, we
demonstrate such limitation during the F13 fault’s diagnosis.
For this experiment, we deployed TrainTicket on a cluster of
three n1-standard-8 Google Cloud Engine instances managed
by Docker Swarm. In addition, we set up a Docker container
running the Elastic Stack toolset
to collect and aggregate
the logs produced by the TrainTicket microservices. More
concretely, we placed a Filebeat daemon on each instance to
continuously send container log messages (application logs
augmented with container monitoring data) to Logstash, which
is the event processing component of the Elastic Stack. Finally,
we ran the F13 test driver until observing a failing execution
and collected the corresponding logs captured by the toolset.
In the following, we describe how a developer would
typically use Elastic Stack for debugging the error observed
in the experiment, using an ﬁctional character named Steve.
Steve is a software engineer at the company that owns
TrainTicket and has been assigned to ﬁx an issue ticket
reporting an intermittent error related to payment requests.
Steve starts by inspecting the logs produced by the Launcher
service, as it was the service in which the unexpected behavior
surfaced. Using the container information present in the logs,
Steve is able to isolate the services that processed the request
and ﬁnd the events that delimit the payment request. Having
Figure 1 unveils the subset of log messages, ordered by
timestamp, that Steve relies on to reason about the execution
path that led to the error at line 12. Each log statement begins
with the corresponding service’s name where it was collected.
The events logged by the Launcher service (lines 1 and 12)
delimit the portion of the execution that comprises the failure.
The log messages in-between (lines 2-11) were recorded by
three other services, speciﬁcally Payment, Order, and Cancel,
and provide applicational context such as the identiﬁer and the
status of the order at any given moment.
By inspecting the logs, Steve realizes that, after completing
the reservation step, the client sent a payment request (line 2),
followed by a cancellation request for the same order (line 6).
Throughout the processing of the payment request, the logs
show that the Payment service fetched the details of the order
from the Order service (lines 3 and 4). At that moment, the
order’s state was UNPAID – a valid state for transitioning to
PAID (line 4). Yet, the request later returned false (line 5),
thus indicating that the payment had failed to complete.
In turn, the cancel request completed successfully (line 11),
as it managed to set the order’s state to CANCELED (lines 7 and
8) and issue a refund through the Payment service (lines 9 and
10).
The fact that the order’s state was valid for the payment
request (i.e., the response in line 4 had status UNPAID) leads
Steve to believe that the reason for the payment failure was
insufﬁcient money. However, after carefully reviewing the
account balance, he veriﬁes that there were enough funds to
pay for the reservation order.
Steve thus concludes that
there is surely some other
factor causing the error, although from the logs it
is not
clear what that factor might be. Moreover, as the failure is
non-deterministic, Steve is unable to obtain additional details
about the problem simply by re-executing the application.
Consequently, he marks the ticket as ”cannot reproduce” and
the issue remains unresolved.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:14 UTC from IEEE Xplore.  Restrictions apply. 
214
This cautionary tale aims at showing that the lack of causal-
ity in toolsets like Elastic Stack often renders the analysis
of distributed system logs an inconclusive task, which further
contributes to the lingering of harmful bugs in production.
In the next sections, we describe Horus in detail. Later, in
Section VI, we show how it can be used to debug the failure
presented in this example.
III. HORUS OVERVIEW
We propose Horus, a system that addresses the limitations
of prior log analysis solutions to further ease the burden of
debugging distributed executions. To this end, Horus provides
the following key features:
• Lightweight and non-intrusive causality tracking. Ho-
rus leverages kernel probes to efﬁciently capture low-level
operations (e.g. socket sends and receives) that are then
used to establish a causal order of log messages. These
kernel probes rely on eBPF, a low-overhead technology
widely used in performance analysis tools [14]–[18].
• Causally-consistent aggregation of distributed events.
Horus combines both kernel events and logging events
from different sources into a single directed, acyclic graph
of the production run. In this graph, nodes represent the
execution events and edges represent the happens-before
(HB) relations between them. Intra-node HB relations are
encoded using the original event timestamps, whereas
inter-node HB relations are encoded using the kernel-
level event causality rules.
• Execution analysis and reﬁnement via rich query-
ing. Casting the causal aggregation of multiple logs of
a distributed execution as a graph generation problem
gives Horus the ability to leverage off-the-shelf third-
party graph databases [19], [20], which not only scale
to executions with millions of events, but also provide
support for rich query languages.
To better understand how Horus operates, we depict its
architecture and execution ﬂow in Figure 2. As shown in the
ﬁgure, Horus comprises four main components: Event Sources,
Event Queue, Event Processor, and Graph Database. Each
component is described as follows.
a) Event Sources: This component represents the set of
heterogeneous and independent sources that produce events
at runtime. To handle different types of event sources, Horus
requires the existence of adapters, which are responsible for
collecting the data, normalizing it into a Horus-compatible
format, and shipping it
to the Horus backend for further
processing (see 1 in Figure 2). Our current prototype provides
adapters to automatically collect events from:
• Log4j [2]. The adapter for Log4j consists of a simple
formatter which outputs log messages as JSON objects
indicating the timestamp, the name of the process/thread,
and the textual message written in the source code. Each
log message is thus considered an independent event.
• I/O and Process System Calls. The adapter for I/O and
process system calls reuses eBPF probes from Falcon [9]
Event
Sources
Event Processor
Event
Queue
Intra-process 
HB encoder
Inter-process 
HB encoder
2
1
4
3
events and 
intra-process
relationships
5
Graph
Database
inter-process
relationships
6
Query Evaluation
Application Trace
Causal Diagram
Causally-Ordered
Application Trace
? Custom
Causal Queries
Fig. 2: Horus architecture and event ﬂow.
to trace events regarding: the start, end, fork, join of a
process or thread; the request and accept of a network
connection between two processes; and the sending and
receiving of a message. eBPF allows implementing efﬁ-
cient and safe programs that run inside the kernel for mul-
tiple purposes [18], namely I/O analysis, performance,
monitoring, security and tracing.
We plan to extend Horus with adapters for additional logging
libraries in the future, such as Logrus [21].
b) Event Queue: This component abstracts a set of
persistent, distributed, and replicated queues within Horus that
keep events waiting for being processed. In detail, Horus
maintains two types of event queues: one responsible for
storing the stream of events coming from the sources (see
2 ), and another responsible for linking the different stages
for building the causal graph (see 4 ). The current prototype
uses Apache Kafka [22] to manage the event queues.
c) Event Processor: This component is the processing
core unit of Horus and aims at generating the execution
causal graph. To this end,
the Event Processor comprises
two sub-components that operate as a two-stage pipeline. In
the ﬁrst stage, the Intra-Process HB Encoder establishes the
HB relations between events of the same process, which are
then periodically persisted in the graph database (see 3 ) and
forwarded to the next processing stage (see 4 ).
In the second stage, the Inter-Process HB Encoder encodes
the HB dependencies between events of distinct processes,
which are then ﬂushed to the database in periodic batches like
in the previous step (see 5 ). Section IV describes the causal
graph generation procedure in more detail.
d) Graph Database: The current prototype of Horus uses
Neo4j [19] to store the execution causal graph and computing
the developer’s queries at debugging time (see 6 ). Neo4j is
a graph database that provides a rich query language named
Cypher for ﬁltering, reﬁning, and visualizing graphs.
Although Neo4j supports graphs with millions of events, our
experiments have shown that the execution of causality queries
does not scale well with the size of the graph. In Section V,
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:14 UTC from IEEE Xplore.  Restrictions apply. 
215
we show how Horus dramatically improves query processing
in Neo4j with a novel technique that leverages logical time to
prune irrelevant portions of the graph.
Our prototype of Horus is publicly available at https://
four events {A, B, C, D} sorted in ascending order of their
timestamp. Upon persisting T into the database, the Intra-
Process HB Encoder creates four nodes – A, B, C, D – and
three edges – (A, B), (B, C), (C, D).
github.com/DistributedSystemsAnalysis/horus.
Finally, the Intra-Process HB Encoder sends the timeline
IV. CAUSAL GRAPH GENERATION
This section details how Horus builds the causal graph of
a distributed execution. As mentioned before, the nodes of
this graph denote execution events (i.e., log messages and
kernel operations) and the edges indicate the happens-before
relationships between them. More concretely, there are two
types of edges: intra-process and inter-process.
A. Intra-Process HB Relationships
In the ﬁrst stage of Horus’ event processing pipeline,
the Intra-Process HB Encoder computes the happens-before
relationships between the subsets of events belonging to the
same process. As stated by the ﬁrst property of the deﬁnition of
causality (see Section II-A), these relationships can be derived
directly from the program order.
In practice,
logging libraries already assign timestamps
to the log messages with the purpose of later easing their
analysis. This means that, for the same process, it sufﬁces to
order the messages by timestamp in order to obtain the causal
ordering of events at runtime.
However, in scenarios with multiple independent loggers
and tracers, the same process may trigger those tools without
any kind of synchronization,
thus permitting the resulting
events to arrive out of order at the Event Queue component.
For that reason, Horus requires the timestamp source adopted
by those tools (e.g., physical clock) to be the same and
accurate enough to deﬁne a total order of events across them.
Otherwise, the program order property ceases to hold.
To handle events coming from several sources, the Intra-