di between pairs of DCs. While brokers report the demand
for non-interactive services, SWAN estimates the demand
of interactive services (see below). We also use as input
the paths (tunnels) available between a DC pair. Running
an unconstrained multi-commodity problem could result in
allocations that require many rules at switches. Since a DC
pair’s traﬃc could ﬂow through any link, every switch may
need rules to split every pair’s traﬃc across its outgoing
ports. Constraining usable paths avoids this possibility and
also simpliﬁes data plane updates (§4.3). But it may lead to
lower overall throughput. For our two production inter-DC
WANs, we ﬁnd that using the 15 shortest paths between
each pair of DCs results in negligible loss of throughput.
Allocation LP: Figure 6 shows the LP used in SWAN. At
the core is the MCF (multi-commodity ﬂow) function that
maximizes the overall throughput while preferring shorter
paths;  is a small constant and tunnel weights wj are pro-
portional to latency. sP ri is the fraction of scratch link ca-
pacity that enables congestion-managed network updates;
ﬂow demands for source destination pair i
di:
wj : weight of tunnel j (e.g., latency)
cl:
sP ri:
Ij,l:
capacity of link l
scratch capacity ([0, 50%]) for class P ri
1 if tunnel j uses link l and 0 otherwise
Inputs:
bi =(cid:80)
Outputs:
j bi,j :
bi is allocation to ﬂow i; bi,j over tunnel j
Func: SWAN Allocation:
← cl; // remaining link capacity
∀ links l : cremain
for P ri = Interactive, Elastic, . . . , Background do
l
{bi} ← Throughput Maximization
Approx. Max-Min Fairness
← cremain
i,j bi,j · Ij,l;
cremain
l
−(cid:80)
l
(cid:0)P ri, {cremain
}(cid:1);
l
l
l
}):
}, 0, ∞, ∅);
Func: Throughput Maximization(P ri, {cremain
return MCF(P ri, {cremain
Func: Approx. Max-Min Fairness(P ri, {cremain
// Parameters α and U trade-oﬀ unfairness for runtime
// α > 1 and 0 < U ≤ min(fairratei)
T ← (cid:100)logα
for k = 1 . . . T do
(cid:101); F ← ∅;
max(di)
}):
U
l
foreach bi ∈ MCF(P ri, {cremain
}, αk−1U, αkU, F ) do
if i /∈ F and bi < min(di, αkU ) then
F ← F + {i}; fi ← bi; // ﬂow saturated
l
l
return {fi : i ∈ F};
Func: MCF(P ri, {cremain
//Allocate rate bi for ﬂows in priority class P ri
}, bLow, bHigh, F ):
i,j wj · bi,j )
maximize (cid:80)
i bi − ((cid:80)
∀l :(cid:80)
∀i /∈ F : bLow ≤ bi ≤ min(di, bHigh);
∀i ∈ F : bi = fi;
∀(i, j) : bi,j ≥ 0.
i,j bi,j · Ij,l ≤ min{cremain
subject to
l
, (1 − sP ri)cl};
Figure 6: Computing allocations over a set of tunnels.
it can be diﬀerent for diﬀerent priority classes (§4.3). The
SWAN Allocation function allocates rate by invoking MCF
separately for classes in priority order. After a class is allo-
cated, its allocation is removed from remaining link capacity.
It is easy to see that our allocation respects traﬃc prior-
ities. By allocating demands in priority order, SWAN also
ensures that higher priority traﬃc is likelier to use shorter
paths. This keeps the computation simple because MCF’s
time complexity increases manifold with the number of con-
straints. While, in general, it may reduce overall utilization,
in practice, SWAN achieves nearly optimal utilization (§6).
Max-min fairness can be achieved iteratively: maximize
the minimal ﬂow rate allocation, freeze the minimal ﬂows
and repeat with just the other ﬂows [26]. However, solving
such a long sequence of LPs is rather costly in practice, so
we devised an approximate solution instead. SWAN provides
approximated max-min fairness for services in the same class
by invoking MCF in T steps, with the constraint that at step
k, ﬂows are allocated rates in the range(cid:2)αk−1U, αkU(cid:3), but
no more than their demand. See Fig. 6, function Approx.
Max-Min Fair. A ﬂow’s allocation is frozen at step k when
it is allocated its full demand di at that step or it receives a
rate smaller than αkU due to capacity constraints. If ri and
bi are the max-min fair rate and the rate allocated to ﬂow
i, we can prove that this is an α-approximation algorithm,
(cid:3) (Theorem 1 in [14]3).
i.e., bi ∈(cid:2) ri
α , αri
Many proposals exist to combine network-wide max-min
3All proofs are included in a separate technical report [14].
19fairness with high throughput. A recent one oﬀers a search
function that is shown to empirically reduce the number of
LPs that need to be solved [9]. Our contribution is showing
that one can trade-oﬀ the number of LP calls and the degree
of unfairness. The number of LPs we solve per priority is T ;
with max di=10Gbps, U =10Mbps and α=2, we get T =10.
We ﬁnd that SWAN’s allocations are highly fair and take less
than a second combined for all priorities (§6). In contrast,
Danna et al. report running times of over a minute [9].
Finally, our approach can be easily extended to other pol-
icy goals such as virtually dedicating capacity to a ﬂow over
certain paths and weighted max-min fairness.
Interactive service demands: SWAN estimates an inter-
active service’s demand based on its average usage in the last
ﬁve minutes. To account for prediction errors, we inﬂate the
demand based on the error in past estimates (mean plus two
standard deviations). This ensures that enough capacity is
set aside for interactive traﬃc. So that inﬂated estimates do
not let capacity go unused, when allocating rates to back-
ground traﬃc, SWAN adjusts available link capacities as if
there was no inﬂation.
If resource contention does occur,
priority queueing at switches protects interactive traﬃc.
Post-processing: The solution produced by the LP may
not be feasible to implement; while it obeys link capacity
concerns, it disregards rule count limits on switches. Di-
rectly including these limits in the LP would turn the LP
into an Integer LP making it intractably complex. Hence,
SWAN post-processes the output of the LP to ﬁt into the
number of rules available.
Finding the set of tunnels with a given size that carries
the most traﬃc is NP-complete [13]. SWAN uses the follow-
ing heuristic: ﬁrst pick at least the smallest latency tunnel
for each DC pair, prefer tunnels that carry more traﬃc (as
per the LP’s solution) and repeat as long as more tunnels
can be added without violating rule count constraint mj at
switch j. If Mj is the number of tunnels that switch j can
store and λ ∈ [0, 50%] is the scratch space needed for rule
updates (§4.3.2), mj=(1 − λ)Mj. In practice, we found that
{mj} is large enough to ensure at least two tunnels per DC
pair (§6.5). However, the original allocation of the LP is no
longer valid since only a subset of the tunnels are selected
due to rule limit constraints. We thus re-run the LP with
only the chosen tunnels as input. The output of this run has
both high utilization and is implementable in the network.
To further speed-up allocation computation to work with
large WANs, SWAN uses two strategies. First, it runs the
LP at the granularity of DCs instead of switches. DCs have
at least 2 WAN switches, so a DC-level LP has at least 4x
fewer variables and constraints (and the complexity of an
LP is at least quadratic in this number). To map DC-level
allocations to switches, we leverage the symmetry of inter-
DC WANs. Each WAN switch in a DC gets equal traﬃc
from inside the DC as border routers use ECMP for outgoing
traﬃc. Similarly, equal traﬃc arrives from neighboring DCs
because switches in a DC have similar fan-out patterns to
neighboring DCs. This symmetry allows traﬃc on each DC-
level link (computed by the LP) to be spread equally among
the switch-level links between two DCs. However, symmetry
may be lost during failures; we describe how SWAN handles
failures in §4.4.
Second, during allocation computation, SWAN aggregates
the demands from all services in the same priority class be-
tween a pair of DCs. This reduces the number of ﬂows that
Inputs:
Outputs: {ba
maximize
subject to
q,
b0
i,j = bi,j ,
i,j = b(cid:48)
bq
i,j ,
cl,
Ijl,
sequence length
initial conﬁguration
ﬁnal conﬁguration
capacity of link l
indicates if tunnel j using link l
i,j} ∀a ∈ {1, . . . q} if feasible
cmargin // remaining capacity margin
i,j = bi;
i,j , ba+1
i,j ≥ 0; cmargin ≥ 0;
i,j max(ba
i,j ) · Ij,l + cmargin;
∀i, a :(cid:80)
∀l, a : cl ≥(cid:80)
j ba
∀(i, j, a) : ba
Figure 7: LP to ﬁnd if a congestion-free update sequence
of length q exists.
the LP has to allocate by a factor that equals the number
of services, which can run into 100s. Given the per DC-
pair allocation, we divide it among individual services in a
max-min fair manner.
4.3 Updating forwarding state
To keep the network highly utilized, its forwarding state
must be updated as traﬃc demand and network topology
change. Our goal is to enable forwarding state updates that
are not only congestion-free but also quick; the more agile
the updates the better one can utilize the network. One
can meet these goals trivially, by simply pausing all data
movement on the network during a conﬁguration change.
Hence, an added goal is that the network continue to carry
signiﬁcant traﬃc during updates.4
Forwarding state updates are of two types: changing the
distribution of traﬃc across available tunnels and changing
the set of tunnels available in the network. We describe
below how we make each type of change.
4.3.1 Updating trafﬁc distribution across tunnels
Given two congestion-free conﬁgurations with diﬀerent
traﬃc distributions, we want to update the network from the
ﬁrst conﬁguration to the second in a congestion-free man-
ner. More precisely, let the current network conﬁguration
be C={bij : ∀(i, j)}, where bij is the traﬃc of ﬂow i over
tunnel j. We want to update the network’s conﬁguration
ij : ∀(i, j)}. This update can involve moving many
to C(cid:48)={b(cid:48)
ﬂows, and when an update is applied, the individual switches
may apply the changes in any order. Hence, many transient
conﬁgurations emerge, and in some, a link’s load may be
much higher than its capacity. We want to ﬁnd a sequence
of conﬁgurations (C=C0, . . . , Ck=C(cid:48)) such that no link is
overloaded in any conﬁguration. Further, no link should be
overloaded when moving from Ci to Ci+1 regardless of the
order in which individual switches apply their updates.
In arbitrary cases congestion-free update sequences do not
exist; when all links are full, any ﬁrst move will congest at