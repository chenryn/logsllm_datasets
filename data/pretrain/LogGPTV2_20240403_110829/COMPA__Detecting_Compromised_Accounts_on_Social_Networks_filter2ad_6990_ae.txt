False Positives
# Bulk Applications
# Compromised Bulk Applications
False Positives
# Client Applications
# Compromised Client Applications
False Positives
Groups
374,920
9,362
12,347
1,647
8.9% (146)
362,573
7,715
3.0% (231)
343,229
4% (377) 3.6% (12,382)
Groups
14,548
1,236
5.8% (72)
1,569
251
48,586
671
54,907
11,499
3.8% (2,141) 3.3% (22) 3.6% (412)
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
46,653
2.2% (1,040)
178,557
8,254
2.7% (4,854) 14.7% (37) 13.3% (1,101)
164.672
4.6% (7,528)
12,979
985
3.5% (35)
Table 2. Evaluation Results for the Text (Twitter and Facebook) and URL (Twitter) Similarity measure
swer two questions: First, do we incorrectly label non-
compromised accounts as compromised (false positives)?
We try to answer this question in this subsection. Second,
do we miss accounts that have been compromised (false
negatives)? We discuss this question in the next subsection.
False positives might arise in two cases: First, legitimate
users who change their habits (e.g., a user experiments with
a new Twitter client) might be ﬂagged as compromised.
Second, fake accounts, speciﬁcally created for the purpose
of spreading malicious content, might trigger our detection,
but these are not compromised accounts. Arguably, the sec-
ond source of false positives is less problematic than the
ﬁrst one, since the messages that are distributed by fake ac-
counts are likely malicious. However, since social network
providers need to handle compromised accounts differently
from fake accounts (which can be simply deleted), we want
our system to only report compromised accounts.
To address the ﬁrst reason for false positives, we
analyzed the groups that our similarity measures gen-
erated.
First, we aggregated similar (repeated) groups
into long-lasting campaigns. Two groups belong to the
same campaign if all pairwise Levenshtein ratios between
ten randomly-chosen messages (ﬁve messages from each
group) is at least 0.8. We could aggregate 7,899 groups
into 496 campaigns. We then manually analyzed a sub-
set of the accounts present for each campaign. Addition-
ally, 1,463 groups did not belong to any campaign, and we
assessed each of these manually. During manual analy-
sis, we opted to err on the conservative side by counting
groups whose accounts contain messages written in non-
Latin based languages as false positives.
In total, 377 of the 9,362 groups (4%) that COMPA
ﬂagged as containing compromised accounts could not be
veriﬁed as such, and thus, constitute false positives. Note
that each group consists of multiple tweets, each from a dif-
ferent Twitter account. Thus, the above mentioned results
are equivalent to ﬂagging 343,229 user as compromised,
where 12,382 (3.6%) are false positives.
Three months after we ﬁnished our experiments, we tried
to retrieve all messages that we found were indicative of
compromised accounts. Only 24% were still available. Fur-
thermore, we would expect that the majority of messages
sent by legitimate accounts are persistent over time. Thus,
we also tried to retrieve a random sample of 160,000 mes-
sages contained in clusters that COMPA found to be benign.
82% of these messages were still reachable. Additionally,
we also tried to access 64,368 random messages that we col-
lected during our experiments as described in the following
subsection. Of these, 84% were still accessible.
This means that for 76% of the messages that COMPA
identiﬁed as being sent by a compromised account, either
Twitter or the user herself removed the message. However,
96.2% of the accounts that sent these tweets were still ac-
cessible. For less than one percent (0.6%) of the accounts,
Twitter states that they were suspended. The remaining
3.2% return a “Not found” error upon access. These per-
centages are almost perfectly in line with accounts that
COMPA did not ﬂag as compromised (95.5%, 0.5%, and
4%, respectively), and a random sample of 80,000 accounts
(94%, 4%, and 2%). Twitter actively suspends spam ac-
counts on their network. Thus, these results indicate that
Twitter does not consider the accounts ﬂagged by COMPA
as fake. However, the signiﬁcant amount of removed mes-
sages for such accounts leads us to believe that COMPA was
successful in detecting compromised accounts.
To estimate the second source of false positives, we
developed a classiﬁer based on the work of Stringhini et
al. [7]. This allows us to detect accounts that were fake ac-
counts as opposed to compromised. Stringhini’s system de-
tects fake accounts that are likely spammers, based on fea-
tures related to automatically created and managed accounts
(such as the ratio between the friend requests that are sent
and the requests that are accepted, the fraction of messages
with URLs, and how similar the messages are that a single
user sends). This system proved to be effective in detecting
accounts that have been speciﬁcally created to spread mali-
cious content. However, the system does not usually detect
compromised accounts. This is because such accounts usu-
ally have a long history of legitimate Twitter usage, and,
hence, the few tweets that are sent out after the compromise
do not affect the features their classiﬁer relies on.
We used this classiﬁer to analyze a random sample of
94,272 accounts that COMPA ﬂagged as compromised. The
idea is that any time an account is detected as a spammer,
this is likely to be an account speciﬁcally created with a
malicious intent, and not a legitimate, yet compromised ac-
count. Out of the analyzed accounts, only 152 (0.16%) were
ﬂagged as spammers. We then manually checked these
accounts to verify if they were actually false positives of
COMPA. 100 of these 152 accounts turned out to be com-
promised, and thus, true positives of COMPA. The reason
for ﬂagging them as spam accounts is that they have not
been very active before getting compromised. Therefore,
after the account was compromised, the spam messages had
more inﬂuence on the features than the legitimate activity
before the compromise. The remaining 52 accounts were
not compromised but had been speciﬁcally created to spam.
However, these 52 accounts were distributed over 34 clus-
ters with an average cluster size of 30. Furthermore, no
cluster consisted solely of false positives. The main reason
why they were detected as behavior violations by COMPA
is that they posted an update in an hour during which they
had never been active before. This result underlines that
the compromised accounts that COMPA reports are substan-
tially different than the dedicated, fake accounts typically
set up for spamming.
Historical information. We also investigated how the
length of a user’s message stream inﬂuences the quality of
the behavioral proﬁles COMPA builds (recall that COMPA
does not build a behavioral proﬁle when a user has posted
fewer than 10 messages). To this end, we calculated the
probability of a false positive depending on the number of
tweets that were available to calculate the behavioral pro-
ﬁle. As Figure 2 illustrates, COMPA produces less false pos-
itives for accounts whose historical data is comprehensive.
The reason for this is that the models become more accurate
when more historical data is available.
False Negatives
To assess false negatives, we used COMPA to create 64,368
behavioral proﬁles for randomly selected users over a pe-
riod of 44 days. To this end, every minute, COMPA retrieved
the latest tweet received from the Twitter stream and built a
behavioral proﬁle for the corresponding user. 2,606 (or 4%)
of these proﬁles violated their account’s behavioral proﬁle.
415 of these were sent by known, popular bulk applications.
We manually inspected the remaining 2,191 tweets that vi-
olated their accounts’ behavioral proﬁles (we performed the
same manual analysis that was previously used to determine
the ground truth for our training set). We did not ﬁnd evi-
dence of any malicious activity that COMPA missed.
Figure 2. Probability of false positives depending on the
amount of historical data on Twitter
In a next step, we extracted all URLs posted on Twitter
during one day, and we checked them against ﬁve popular
blacklists. The idea behind this is that if a URL is known
to be malicious, it is likely to be posted either by a compro-
mised or by a fake account. We extracted 2,421,648 URLs
(1,956,126 of which were unique) and checked them against
the Spamhaus Domain Blacklist [22], Google Safebrows-
ing [23], PhishTank [24], Wepawet [25], and Exposure [26].
We ﬁrst expanded shortened URLs before checking the
landing page against the blacklists. In total, 79 tweets con-
tained links that were present in at least one blacklist (these
79 tweets contained 46 unique URLs). We ran COMPA on
each of the 79 messages to see if they were actually sent by
compromised accounts.
Our system ﬂagged 33 messages as violating their user’s
proﬁle. The reason COMPA did not ﬂag these accounts in
the ﬁrst place is that the clusters generated by these mes-
sages were too small to be evaluated, given the API limit we
mentioned before. If we did not have such a limit, COMPA
would have correctly ﬂagged them. Seven more messages
contained URLs that were similar to those in the 33 mes-
sages. Even though these compromised accounts did not
violate their behavioral proﬁles, they would have been de-
tected by COMPA, because they would have been grouped
together with other messages that were detected as violating
their behavioral proﬁles.
Of the remaining 39 accounts that COMPA did not ﬂag
as compromised, 20 were detected as fake accounts by the
classiﬁer by Stringhini et al. [7]. We manually investigated
the remaining 19 results. 18 of them contained links to
popular news sites and blogs, which were mainly black-
listed by Google Safebrowsing. We think users posted legit-
imate links to these pages, which might have become com-
50100150200Amount of historical data (i.e., # of tweets)0.0000.0050.0100.0150.0200.0250.0300.035Probability for a false-positivepromised at a later point in time (or are false positives in
Google Safebrowsing). Thus, we do not consider accounts
that linked to such pages as either compromised or fake.
The remaining message linked to a phishing page, but
did not violate the proﬁle of the account that posted it. We
consider this as a message by a compromised account, and,
therefore, a false negative of COMPA.
6.4 Detection on Facebook
As the Facebook dataset spans almost two years we in-
creased the observation interval to eight hours to cover this
long timespan. Furthermore, we only evaluated the Face-
book dataset with the text similarity measure to group sim-
ilar messages.
Our experiments indicated that a small number of popu-
lar applications resulted in a large number of false positives.
Therefore, we removed the six most popular applications,
including Maﬁa Wars from our dataset. Note that these
six applications resulted in groups spread over the whole
dataset. Thus, we think it is appropriate for a social network
administrator to white-list applications at a rate of roughly
three instances per year.
In total, COMPA generated 206,876 proﬁles in 48,586
groups and ﬂagged 671 groups as compromised (i.e, 11,499
compromised accounts). All ﬂagged groups are created by
bulk applications. 22 legitimate groups were incorrectly
classiﬁed (i.e., 3.3% false positives) as compromised; they
contained 412 (3.6%) users.
6.5 Proﬁle Accuracy
One example to illustrate the accuracy of the behavioral
proﬁles COMPA creates is the following. On July 4th, 2011,
the Twitter account of the politics division of Fox News
(@foxnewspolitics) got compromised [27]. The attackers
used the account to spread wrong information about an as-
sassination of president Obama. As this incident was lim-
ited in scope, COMPA did not observe enough messages to
create a group. Therefore, we instructed COMPA to cre-
ate a behavioral proﬁle for @foxnewspolitics and have it
compare the offending tweets against this proﬁle. COMPA
detected signiﬁcant deviations from the created behavioral
proﬁle for all but the language models. Thus, the offending
tweets posed a clear violation of the behavioral proﬁle of
the @foxnewspolitics account.
6.6 Case Studies
In this section, we describe some interesting ﬁndings
about the compromised accounts detected by COMPA.
”Get more Followers” scams On Twitter, the majority of
the accounts that COMPA ﬂagged as compromised were part
of multiple large-scale phishing scams that advertise ”more
Followers”. These campaigns typically rely on a phishing
website and a Twitter application. The phishing website
promises more followers to a user. The victim can either
get a small number of followers for free, or she can pay for
a larger set of followers. Many users consider the number
of their followers as a status symbol on the Twitter network,
and the ”base version” of the service is free. This combina-
tion seems to be an irresistible offer for many. The phish-
ing sites requires the user to share their username and pass-
word with the website. Additionally, the user needs to give
read and write access to the attacker’s application. Once the
victim entered her credentials and authorized the applica-
tion, the application immediately posts a tweet to the vic-
tim’s account to advertise itself. Subsequently, the attackers
make good on their promise and use their pool of existing,
compromised accounts to follow the victim’s account. Of
course, the victim also becomes part of the pool and will
start following other users herself.
Phone numbers COMPA also detected scam campaigns
that do not contain URLs.
Instead, potential victims
are encouraged to call a phone number. Such messages