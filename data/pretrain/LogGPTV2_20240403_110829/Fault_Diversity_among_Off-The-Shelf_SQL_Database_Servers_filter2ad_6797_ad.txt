0 
0 
N/A 
Arithmetic-related bugs 
PostgreSQL  bug  report  77  and  Oracle  bug  report 
1059835  [8]  describe  arithmetic  precision  problems, 
causing incorrect result failures. The Oracle bug 1059835 
affects the MOD (modular arithmetic) operator, probably 
causing higher consequence failures.  The failure rates for 
these  bugs  would  only  be  expected  to  be  high  in 
applications with high use of mathematical functions, not 
a typical use of SQL servers.  
Bugs affecting complex queries 
PostgreSQL  bug  43  [8]  causes  a  failure  in  both 
PostgreSQL  and  MSSQL.  The  complex  SELECT 
statement  below,  with  nested  sub-queries,  causes  the 
failure: 
SELECT   P.ID AS ID, P.NAME AS NAME FROM PRODUCT 
P WHERE P.ID IN   
(SELECT  ID  FROM    PRODUCT  WHERE    PRICE  >=  '9.00' 
AND  PRICE = 
'2000-9-6')  
UNION  
(SELECT PRODUCT_ID AS ID  FROM PRODUCT_SPECIAL 
WHERE  PRICE  >= 
'50'  AND 
START_DATE = '2000-9-6')))
Interestingly,  for  this  same  bug  the  two  servers  fail  with 
different  patterns.  PostgreSQL  fails  returning  a  parsing 
error.  MSSQL  does  not,  but  subsequently  gives  an 
incorrect  result,  probably  because  it  built  an  incorrect 
parsing tree. 
'9.00'  AND  PRICE  <= 
MSSQL Bug 58544 [8] causes failures in both MSSQL 
and  Interbase.  Using a LEFT OUTER JOIN on a VIEW 
that  uses  the  DISTINCT  keyword  causes  the  failure.  A 
left outer join is a special type of outer join where if you 
have a join between tables T1 and T2 then the joined table 
unconditionally has a row for each row in T1 (as opposed 
to a Full Outer Join where the joined table has a row for 
each  row  present  in  both  tables  T1  and  T2).  The 
DISTINCT  keyword  subsequently  eliminates  all  the 
duplicate  rows  from  the  joined  table.  Complex  queries 
would  be  common  on  large  databases  with  many  tables, 
leading probably to a comparatively high failure rate, with 
possibly  high  failure  severity,  especially  for  incorrect 
result failures.   
Miscellaneous bugs 
Interbase  Bug  223512(2)  causes  a  failure  in  the  Data 
Definition Language (DDL) part of SQL which is used to 
create/modify  database  objects  (i.e.  tables,  views,  users, 
procedures  etc).  It  causes  failures  in  both  Interbase  and 
PostgreSQL: both incorrectly allow a client to drop Views 
using the Drop Table statement. This violates the SQL-92 
standard, which allows Views to be dropped only via the 
Drop  View  statement.  This  bug  would  seem  to  cause 
infrequent  failures  in  operation  and  it  would  normally 
require  an  error  by  an  administrator.  The  severity  of 
failures would also be expected to be low since a view is 
just  a  ‘virtual  table’  (or  a  stored  SELECT  statement), 
which  represents  the  data  from  one  or  more  tables.  No 
data are lost by dropping a view, although a runtime error 
will be generated each time a client attempts to access the 
dropped view. 
Interbase  bug  217042(3)  causes  both  Interbase  and 
MSSQL  to  fail  to  validate  the  default  values  upon 
creation of tables. Therefore a statement like: 
CREATE TABLE TEST (A INT DEFAULT ‘ABC’)
is allowed in both Interbase and MSSQL, even though an 
error should be raised since a string value (ABC) cannot 
be  stored  in  an  Integer  type  attribute.  The  DEFAULT 
attributes  are  used  often  in  operation  but  it  is  not  clear 
how often database users will define DEFAULT values of 
the wrong type. The failure to detect that an incorrect type 
default  value  is  being  assigned  to  a  particular  column  at 
table creation time is non-detectable. However, a runtime 
error will occur, generating an error message, every time 
an  attempt  is  made  to  insert  the  default  value  into  the 
table:  the  failure  will  be  detected,  albeit  with  high 
latency1.
Interbase  bug  222476  causes  a  failure  in  MSSQL  as 
well.  Both  servers  give  empty  field  names  for  avg
(average)  and  sum  SQL  functions,  although  they  return 
correct  results  in  these  fields.  This  would  be  a  serious 
problem for client applications that construct their output 
from the field names and results returned by the server.  
Five  of  the  MSSQL  bug scripts also caused failure in 
PostgreSQL, but with the difference that PostgreSQL fails 
at  the  beginning  of  the  bug  script.  This  implies  that  the 
causes are probably different for the two products, and the 
“failure  regions”  (sets  of  demands  that  would  trigger  the 
bug)  identified  by  such  scripts  for  the  two  servers  only 
partially  overlap:  there  are  variations  of  the  script  for 
1  If  we  classify  the database as part of the server system, the common 
terminology recommended in [15] would imply that assigning the wrong 
type  is  an  internal error, which only becomes a failure and is detected 
when the attempt is made to insert the default value. 
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:18:53 UTC from IEEE Xplore.  Restrictions apply. 
which  PostgreSQL  fails  but  MSSQL  does  not.  For 
example,  MSSQL  bug  54428  causes  an 
incorrect  
“primary  key  constraint”  failure  in  MSSQL.  The  same 
bug  causes  failure  (at  the  beginning  of  the  bug  script) 
when  an  attempt  is  made  to  create  a  clustered  index  in 
PostgreSQL.  The  latter  is  a  known  bug  for  PostgreSQL, 
and  its  correction  in  the  later  release  7.0.3  causes 
PostgreSQL not to fail on any of these five scripts. 
6. Discussion 
6.1. Extrapolating from the counts of common 
bugs to reliability of a diverse server 
These  numbers  are  intriguing  and  point  to  a  potential 
for  serious  dependability  gains  from  assembling  a  fault 
tolerant  server  from  two  or  more  of  these  off-the-shelf 
servers.  But they are not definitive evidence. Apart from 
the sampling difficulties caused e.g. by lack of certain bug 
scripts,  it  is  important  to  clarify  to  what  extent  our 
observations allow us to predict such gains. 
For  brevity,  we  consider  the  simplest  case:  suppose 
that  users  of  a  certain  database  server  product  A  try  to 
obtain  a  more  dependable  service  by  using  a  fault-
tolerant, replicated, diverse server AB, built from product 
A plus another product B (for discussion of the feasibility 
and  design  problems,  see  [21]).  The  number  of  bugs 
reported over a certain reference period (say one year) for 
product  A  is  mA.  Our  study  then  finds  that  of  these  mA
bugs,  only  mAB  also  caused  failure  of  B.  We  may  then 
expect that, had these users been using AB instead of A, 
only  those  failures  of  A  that  were  due  to  those  mA bugs 
could  have  caused  complete  service  failures.  How  much 
more  reliable  would  this  have  made  the  AB  server, 
compared to the A server?  
introduce 
Before  proceeding,  we 
some  more 
simplifications.  The  possible  effects  of  individual  server 
failures  on  system  failures  have  been  discussed  in 
Sections  4.1  and  4.3,  under  the  definitions  of  “self-
evident”  and  “detectable”  failures.  Here,  for  the  sake  of 
brevity,  we  use  a  simplified  scenario:  failures  of  both 
servers  A  and  B  on  the  same  demand  are  “system 
failures”, and failures of a single one of them are not2. In 
addition, we only consider the effects on reliability of the 
factor  that  we  have  studied:  the  diversity  between  faults 
of the two products A and B. We thus ignore any effects 
of  the  middleware  needed  in  the  AB  server,  which  adds 
complexity  and  thus  possibly  faults;  and  of  added 
2  This  simplified  model  is  still  realistic  if  either:  i)  we  are  only 
concerned  with  interruptions  of service, and all failures of A and/or B 
are  detectable  (crashes,  self-detected  errors,  or  different  erroneous 
results  if  both  A  and  B  fail);  or  ii)  we  are  concerned  with  undetected 
erroneous results, and all failures of both A and B on the same demand 
are pessimistically assumed to produce such results. 
complexity  in  client  applications  that  used  complex 
vendor-specific  features  of  server  A,  if  they  must  be 
adapted  to  use  the  more  restricted  feature  set  of  server 
AB. With these simplifications, the AB server is certain to 
be  at  least  as  reliable  as  the  single  A  server  because  it 
only fails if both A and B fail. We still need to assess the 
size of the probable reliability gain. To this end, we need 
to take into account various complications: the difference 
between  fault  records  and  failure  records;  imperfect 
failure reporting; variety of usage profiles. 
We can start with a scenario in which our data would 
be sufficient for trustworthy predictions, and then discuss 
the  effects  of  these  assumptions  not  holding  in  practice. 
This ideal scenario is as follows:  we are interested in the 
reliability gains for a database installation using server A, 
if it were to switch to a diverse server AB, assuming that 
this  installation  has  a  usage  profile  (probabilities  of  all 
possible demands on the server) similar to the average of 
all  the  bug-reporting  installations  of  server  A3.  We 
assume that users neither change their patterns of usage of 
the  databases  (demand  profile)  nor  upgrade  to  new 
releases  of  the  database  servers4;  that  all  failures  that 
affected installations of A during the reference year were 
noticed  and  reported;  and  that  there  is  exactly  one  bug 
report for each failure that occurred.  
Then, we can state that the bug reports describe a one-
year  sample  of  operation  of  the  system,  and  our  best 
reliability  prediction is that the same set of users, during 
another  year  of  operation,  would  experience  a  mean 
number  mA  of  system  failures  if  they  used  A,  but  only 
mAB  if  they  used  AB.    With  the  numbers  we  observed, 
the  ratio  mAB / mA  is  quite  small,  so  the  expected 
reliability  gain  would  be  large.  Given  that  the  reports 
come  from  millions  of  installations,  each  submitting 
many demands5, we might even trust that the true failure 
probability per demand is close to the observed frequency 
of failures. 
The  first  difficulty  with  this  analysis  is  that  reports 
concern  bugs,  not  how  many  failures  each  caused.  They 
3  Or, from a market-assessment viewpoint, we may consider the average 
reliability  gains  for  the  population  of  all  database  installations  which 
depend on server A, if they switched to using AB. 
4  Because  we  wish  to  reason  about  the  reliability  effects  of  diversity 
alone.  This  scenario  also has practical interest, though. Usage patterns 
vary over time, but periods of very slow variations must exist; users do 
upgrade to new versions, but upgrades bring expense and new problems, 
so  that it is interesting to see whether diversity would be a more cost-
effective way of achieving good average dependability over a system’s 
lifetime than frequent upgrades. 
5 How to define a “demand” to a state-rich system like a database server, 
for the purpose of inference about reliability, is a tricky theoretical and 
practical  issue.  For  this  informal  discussion  of  other  difficulties  in 
inference,  we  ask  the  reader  to  accept  that  a  practical  solution  can  be 
found, somewhere between a single command and the whole sequence 
of  commands  over  the  lifetime  of  an  installation.  (cf  e.g.  [29]for 
examples of useful compromises). 
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:18:53 UTC from IEEE Xplore.  Restrictions apply. 
do not tell us whether a bug has a large or a small effect 
on  reliability,  although  the  faults  that  did  not  cause 
failures would tend to have stochastically lower effect on 
reliability than those that caused failures. Thus, the  mAB
bugs which still cause the fault tolerant server AB to fail 
may  account  for  a  large  (perhaps  close  to  100%)  or  a 
small fraction (perhaps close to 0) of the failures observed 
in  A’s  operation.  The  actual  reliability  gain  may  be 
anywhere between negligible and very high.  
Software is often assessed in terms of number of bugs 
remaining. But it is easily seen that the bug reports do not 
give  us  any  information  on  this  number:  the  mA  bugs 
reported  may  be  the  only  bugs  in  the  products,  or  they 
may  be  a  fraction  of  them  (perhaps  minimal),  which 
happened  to  be  the  ones  causing  failures  during  the 
reference year. 
Another  difficulty  is  not  knowing  how  many  of  the 
failures  that  occur  are  actually  reported.  This  fraction  is 
certainly  less  than  100%.  If  all  failures  had  the  same 
probability  of  being  reported,  the  ratio  between  our 
predicted  failure  counts  for  AB and A would still be the 
ratio mAB / mA, although both terms in the ratio would be 
larger  and  affected  by  wider  uncertainty.  Reporting  is 
probably  biased,  for  instance  towards  bugs  that  cause 
higher  frequency  or  higher  severity  of  failures.  Some 
failures – like crashes – are more noticeable than others, 
like storing incorrect data in some data fields, which may 
not produce visible effects for a long time (also making it 
more  difficult  to  trace  the  visible  problem  back  to  its 
cause).  Some  users  are  more  assiduous  at  producing 
failure reports, so the bugs that affect them more are also 
more  likely  to  be  reported,  even  if  not  so  important  for 
other users. 
In the end, we do not know in detail how failure reporting 
differs  between  different  bugs,  but bug reports are likely 
to be better evidence about bugs that cause blatant failures 
than  about  subtle  (arguably  more  dangerous)  failures. 
This prompts another consideration: as reported bugs are 
corrected and products mature, more of their failures are 
likely  to  be  of  the  subtler  types,  unlikely  to  be  reported. 
Therefore  failure  underreporting  probably  causes  a  bias 
towards  underestimating  the  frequency  of  failures  for 
which diversity would help. This makes diversity a more 
attractive defence, but it also means that bug reports will 
become  a  less  and  less  accurate representation of the set 
of failures actually occurring.  
needs 
predictions 
organisation 
Last, we have the problem of usage profiles. A single 
the 
user 
dependability of its specific installation of server AB or A 
(i.e.,  with  or  without  diversity),  which  depends  on  its 
specific usage profile, which differs – perhaps by much – 
from  the  aggregate  profile  of  the  user  population  which 
generated  the  bug  reports.    Installations  that  manage 
different  databases,  with  different  user  needs,  are 
about 
are 
important 
subjected  to  different  usage  profiles.  It  is  then  plausible 
that  different  bugs 
for  different 
installations;  this  conjecture  is  also  supported  by  a 
possible  interpretation  of  Adams’  findings  [1]  about  the 
surprisingly  small  average  failure  rates  of  many  bugs, 
when averaged over many installations. Then, the number 
of  bugs  whose  effects  can  be  tolerated  (what  we  have 
counted  here)  gives  little  information  about  the  resulting 
dependability  gains.  The  actual  effect  can  only  be 
determined  empirically.  The  user  organisation  may  seek 
indirect evidence from the publicly available bug reports: 
if  they  generally  match  the  failures  experienced  locally, 
the local effects of tolerating those bugs can be assessed. 
However  if  it  does  not,  little  insight  is  gained,  and  the 
exercise is time-consuming. 
6.2. Decisions about deploying diversity 
We have underscored that these results are only prima 
facie evidence for the usefulness of diversity.  
A  better  analysis  would  be  obtained  from  the  actual 
failure  reports  (including  failure counts), available to the 
vendors, especially if they use automatic failure reporting 
mechanisms (users are biased towards under-reporting of 