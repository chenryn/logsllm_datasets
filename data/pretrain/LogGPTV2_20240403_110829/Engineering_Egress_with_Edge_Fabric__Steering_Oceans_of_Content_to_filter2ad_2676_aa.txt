title:Engineering Egress with Edge Fabric: Steering Oceans of Content to
the World
author:Brandon Schlinker and
Hyojeong Kim and
Timothy Cui and
Ethan Katz-Bassett and
Harsha V. Madhyastha and
&apos;Italo Cunha and
James Quinn and
Saif Hasan and
Petr Lapukhov and
Hongyi Zeng
Engineering Egress with Edge Fabric
Steering Oceans of Content to the World
Brandon Schlinker,⋆† Hyojeong Kim,⋆ Timothy Cui,⋆ Ethan Katz-Bassett,†‡ Harsha V. Madhyastha§
Italo Cunha,♯ James Quinn,⋆ Saif Hasan,⋆ Petr Lapukhov,⋆ Hongyi Zeng⋆
⋆ Facebook
§ University of Michigan
† University of Southern California
‡ Columbia University
♯ Universidade Federal de Minas Gerais
ABSTRACT
Large content providers build points of presence around the world,
each connected to tens or hundreds of networks. Ideally, this con-
nectivity lets providers better serve users, but providers cannot
obtain enough capacity on some preferred peering paths to han-
dle peak traffic demands. These capacity constraints, coupled with
volatile traffic and performance and the limitations of the 20 year
old BGP protocol, make it difficult to best use this connectivity.
We present Edge Fabric, an SDN-based system we built and
deployed to tackle these challenges for Facebook, which serves
over two billion users from dozens of points of presence on six
continents. We provide the first public details on the connectivity
of a provider of this scale, including opportunities and challenges.
We describe how Edge Fabric operates in near real-time to avoid
congesting links at the edge of Facebook’s network. Our eval-
uation on production traffic worldwide demonstrates that Edge
Fabric efficiently uses interconnections without congesting them
and degrading performance. We also present real-time performance
measurements of available routes and investigate incorporating
them into routing decisions. We relate challenges, solutions, and
lessons from four years of operating and evolving Edge Fabric.
CCS CONCEPTS
• Networks → Traffic engineering algorithms; Network re-
sources allocation; Network control algorithms; Network performance
evaluation;
KEYWORDS
Internet Routing, Border Gateway Protocol, Traffic Engineering,
Software Defined Networking, Content Distribution Network
ACM Reference format:
Brandon Schlinker, Hyojeong Kim, Timothy Cui, Ethan Katz-Bassett, Har-
sha V. Madhyastha, Italo Cunha, James Quinn, Saif Hasan, Petr Lapukhov,
and Hongyi Zeng. 2017. Engineering Egress with Edge Fabric. In Proceedings
of ACM SIGCOMM, Los Angeles, CA, USA, August 21–25, 2017, 14 pages.
https://doi.org/10.1145/3098822.3098853
Publication rights licensed to ACM. ACM acknowledges that this contribution was
authored or co-authored by an employee, contractor or affiliate of a national govern-
ment. As such, the Government retains a nonexclusive, royalty-free right to publish or
reproduce this article, or to allow others to do so, for Government purposes only.
ACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-4653-5/17/08...$15.00
https://doi.org/10.1145/3098822.3098853
1 INTRODUCTION
Internet traffic has very different characteristics than it did a decade
ago. The traffic is increasingly sourced from a small number of large
content providers, cloud providers, and content delivery networks.
Today, ten Autonomous Systems (ASes) alone contribute 70% of the
traffic [20], whereas in 2007 it took thousands of ASes to add up to
this share [15]. This consolidation of content largely stems from
the rise of streaming video, which now constitutes the majority
of traffic in North America [23]. This video traffic requires both
high throughput and has soft real-time latency demands, where
the quality of delivery can impact user experience [6].
To deliver this high-volume, demanding traffic and improve
user-perceived performance and availability, these providers have
reshaped the Internet’s topology. They serve their users from nu-
merous Points of Presence (PoPs) spread across the globe [3], where
they interconnect with multiple other ASes [15]. A PoP generally
has multiple paths available to reach a user network, increasing the
likelihood that it has a “short” path [5]. This rich interconnectivity
gives providers control over larger portions of the paths [15] and
in aggregate provides necessary capacity. In contrast, the tradi-
tional Internet provider hierarchy [15] can struggle to provide the
capacity needed to deliver the rapidly growing demand for content.
Although such rich connectivity potentially offers performance
benefits, application providers face challenges in realizing these
benefits. Strikingly, despite these massive changes in the traffic
being delivered and the topology it is delivered over, the protocol
used to route the traffic over the topology—the Border Gateway
Protocol (BGP)—is essentially unchanged, and significant barriers
exist to replacing it [5, 22]. While it is impressive that BGP has
accommodated these changes, it is ill-suited to the task of delivering
large volumes of consolidated traffic on the flattened topology:
• BGP is not capacity-aware. Interconnections and paths have
limited capacity. Our measurements of a large content provider
network show that it is unable to obtain enough capacity at many
interconnections to route all traffic along the paths preferred by
its BGP policy. A provider’s routing decisions should account for
these constraints, especially since a provider can cause congestion
with huge volumes of adaptive-bitrate video traffic that expands
to increase bitrate when capacity allows.
• BGP is not performance-aware. At any PoP, forwarding the
traffic to an IP prefix along the best path chosen by BGP can lead
to sub-optimal performance. The attributes that BGP relies upon
for path selection, such as AS path length and multi-exit dis-
criminators (MEDs), do not always correlate with performance.
ACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
B. Schlinker et al.
Overriding BGP’s path selection with performance-aware se-
lection is challenging for multiple reasons. First, BGP does not
incorporate performance information, and performance-aware
decisions require an external mechanism to capture performance.
Second, the relative performance of paths to a destination can
vary over time (e.g., in response to load), but BGP changes paths
only in reaction to changes in policy or the set of available routes.
Third, a provider may need to measure multiple non-equivalent
paths in near real-time in order to track relative performance,
yet BGP traditionally permits using only a single path to reach
a destination at any point in time. Fourth, because BGP uses a
single path per destination, it does not natively support assigning
performance-sensitive traffic and elastic traffic to different paths
in order to best make use of limited capacity on the best routes.
Although these limitations are well-known, our paper discusses
how they manifest in a production environment. This paper
presents our experience tackling these challenges at Facebook,
which uses dozens of PoPs to serve two billion users across the
vast majority of countries. We make three primary contributions.
First, since the impact of the above-mentioned limitations of
BGP depends on a network’s design, we describe the network con-
nectivity and traffic characteristics that make it challenging for
providers of popular applications to manage their egress traffic. At
Facebook, it is common for PoPs to have four or more routes to
many client networks. Although many interconnections in the US
have spare capacity [8], in our measurements across 20 PoPs, 10%
of egress interfaces experience a period in which Facebook’s BGP
policy would lead to BGP assigning twice as much traffic as the
interface’s capacity! Moreover, the traffic demand from a PoP to a
prefix can be unpredictable, with traffic rates to the same prefix at
a given time exhibiting as much as 170x difference across weeks.
Thus, while Facebook’s rich connectivity provides shorter paths,
more options for routing, and significant capacity in aggregate, the
capacity constraints of individual paths and irregular traffic makes
it difficult to use this connectivity. Traffic must be dynamically
routed in order to optimize efficiency and performance without
exceeding these constraints.
Second, we present the design of Edge Fabric, a system for
optimized routing of egress traffic. Edge Fabric receives BGP routes
from peering routers, monitors capacities and demand for outgoing
traffic, and determines how to assign traffic to routes. Edge Fabric
enacts its route selections by injecting them (using BGP) into the
peering routers, overriding the router’s normal BGP selection. Edge
Fabric has been deployed in production for over four years. We
evaluate how Edge Fabric operates in production and share how
Edge Fabric’s design has evolved over time. In addition, we discuss
technical challenges faced at Facebook’s scale and lessons learned.
Third, we instrument Edge Fabric to continually measure per-
formance along alternate paths—not just its best choice—to every
prefix. Edge Fabric gathers these measurements by routing a small
fraction of the traffic to every prefix along alternate paths. Our
measurements from 4 PoPs show that 5% of prefixes could see a
reduction in median latency of 20+ms by choosing an alternative
to BGP’s prefered route.
Figure 1: A PoP has Peering Routers, Aggregation SWitches, and
servers. A private WAN connects to datacenters and other PoPs.
2 SETTING
2.1 Points of Presence
To help reduce user latencies, Facebook has deployed Points of
Presence (PoPs) in dozens of locations globally. A PoP serves users
from racks of servers, which connect via intermediate aggregation
switches (ASWs) to multiple peering routers (PRs), as seen in Fig-
ure 1. ASWs maintain BGP sessions with PRs and rack switches.
Each PoP includes multiple peering routers (PRs) which exchange
BGP routes and traffic with other ASes. The use of multiple PoPs
reduces latencies in two ways: 1) they cache content to serve users
directly, and 2) when a user needs to communicate with a data
center, the user’s TCP connection terminates at the PoP which
maintains separate connections with data centers, yielding the ben-
efits of split TCP [9] and TLS termination.
Only a single PoP announces each most-specific Facebook prefix,
and so the PoP at which traffic ingresses into Facebook ‘s network
depends only on the destination IP address.1 A global load balancing
system assigns users to PoPs via DNS (returning an IP address of a
particular PoP in response to a DNS request for a general hostname)
and by injecting PoP-specific URLs into responses (that resolve
only to IP addresses at a particular PoP). Like similar systems [4],
it injects measurements to capture the performance to users from
alternate PoPs. The load balancing system uses these measurements
to direct users to their “best” performing PoPs (subject to constraints
such as capacity; PoPs currently not serving users for maintenance,
troubleshooting, or testing; and agreements with other networks).
For 80% of user networks, it directs all requests from the network
to a single (nearby) PoP (during a day in Jan. 2017). Details of the
load balancing are out of the paper’s scope, and we design Edge
Fabric assuming it has no control over which PoP serves a given
user.
Figure 2 depicts the relative volume traffic served from 20 PoPs,
a subset selected for geographic and connectivity diversity that
combined serve most Facebook traffic. The paper refers to the
PoPs consistently by number, ordered by volume. At these PoPs,
95% of the traffic comes from clients in ≈ 65, 000 prefixes (during
a day in Jan. 2017). Considering just the client prefixes needed to
account for 95% of a PoP’s traffic, Figure 3 shows that each PoP
1A covering prefix announced across PoPs guards against blackholes if a more-specific
route fails to propagate to a router.
TransitWANPRPRASWASWPRPRASWASWedge serversPublicExchangePeerPeering LayerAggregation LayerRacks &Serversedge serversEngineering Egress with Edge Fabric
ACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
PoP ID
Private
Public
Rt Srvr
Transit
2 (AS)
1 (EU)
16 (AS)
11 (EU)
Peers Traffic Pe.
.13
.25
.12
.39
.77
.85
.34 — .52 — .23 — 0
.10
.01
.01
.01
19 (NA)
Tr. Pe. Tr. Pe. Tr.
.73
.24
.07
.45
—
.20
Tr. Pe.
.02
.87
.04
.45
.59
.23
—