title:Extending the software-defined network boundary
author:Oliver Michel and
Michael Coughlin and
Eric Keller
Extending the Software-deﬁned Network Boundary
Oliver Michel
Michael Coughlin
Eric Keller
University of Colorado Boulder
PI:EMAIL
University of Colorado Boulder
PI:EMAIL
University of Colorado Boulder
PI:EMAIL
ABSTRACT
Given that Software-Deﬁned Networking is highly success-
ful in solving many of today’s manageability, ﬂexibility, and
scalability issues in large-scale networks, in this paper we ar-
gue that the concept of SDN can be extended even further.
Many applications (esp. stream processing and big-data ap-
plications) rely on graph-based inter-process communication
patterns that are very similar to those in computer networks.
To our mind, this network abstraction spanning over diﬀer-
ent types of entities is highly suitable for and would beneﬁt
from central (SDN-inspired) control for the same reasons
classical networks do. In this work, we investigate the com-
monalities between such intra-host networks and classical
computer networking. Based on this, we study the feasibil-
ity of a central network controller that manages both net-
work traﬃc and intra-host communication over a custom
bus system.
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network
Architecture and Design—packet-switching networks, net-
work topology; C.2.4 [Computer-Communication Net-
works]: Distributed Systems—distributed applications
General Terms
Design, Performance, Reliability
1.
INTRODUCTION
Software-Deﬁned Networking (SDN) has been highly suc-
cessful in solving many of today’s manageability, ﬂexibility,
and scalability issues in large-scale networks. In the years
since SDN was introduced we have seen the software-deﬁned
network boundary extended from the physical network into
the hypervisor. We argue that the concept of SDN can be ex-
tended even further. We believe that certain systems beneﬁt
from SDN-inspired control of all their communication which
includes inter-process and inter-thread communication.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
Copyright is held by the author/owner(s).
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
ACM 978-1-4503-2836-4/14/08.
http://dx.doi.org/10.1145/2619239.2631443.
Figure 1: System Architecture
Many applications (esp. stream processing and big-data
applications) rely on graph-based communication patterns
that are very similar to those in computer networks. This
network abstraction spanning over diﬀerent types of enti-
ties (threads, processes, virtual hosts, and physical hosts)
is highly suitable for and would beneﬁt from central control
for the same reasons classical networks do.
In this work, we investigate the commonalities between
such intra-host networks and classical computer networking.
Based on this, we study the feasibility of a central network
controller that manages both network traﬃc and intra-host
communication over a custom or modiﬁed bus or message
queue system, as illustrated in ﬁgure 1. We believe that also
in this scenario, central control can be beneﬁcial to system
performance, ﬂexibility and elasticity as it allows for dy-
namic demand-driven re-wiring and re-balancing of graph-
based systems regardless whether a given edge is spanning
across hosts or is local to a single host.
2. SOFTWARE-DEFINED INTRA-HOST
COM-MUNICATION
2.1 Stream Processing
While there are several use-cases for extending central
communication control into the host, this work is highly mo-
tivated by stream processing systems. While existing well-
known frameworks like Hadoop focus on batch-style process-
ing and analytics of data, a more recent trend focuses on
processing of data generated in real-time [3]. Examples for
such systems are Storm, Samza, and S4. In these systems,
data-streams traverse a graph consisting of worker instances.
Hypervisor 1Hypervisor 2VM 1VM 2VM 3application switchapplication switchWorker 1Worker 2Worker 3Worker 4Worker 5Worker 6Controllervirtual switchvirtual switchPhysical Network Switchapplication switch381While nodes in this graph may correspond to actual virtual
or physical hosts, this is not necessarily the case. Subse-
quently, edges in such a graph may stay within the bound-
aries of one host or span across the network.
Despite the fact that most stream processing frameworks
have some notion of dynamically re-balancing the process-
ing topology, this functionality is still generally inﬂexible,
is not designed to dynamically respond to processing de-
mand and, most importantly, does not support the dynamic
introduction of new classes of workers (e.g., an additional
monitoring layer) at runtime. These limitations make it dif-
ﬁcult for systems to respond automatically to changes in
demand or introduce new functionality without halting the
entire system. This is mainly due to static placement of
message queue systems like RabbitMQ or Kafka in between
workers.
Furthermore, a major challenge that arises with such sys-
tems is the inﬂuence network performance can have on the
rate of processing obtainable. Much research has examined
the impact network performance can have on Hadoop-style
computation ([4] among others), especially in multi-tenant
clouds, and proposed solutions to improve performance [1,
2]. Yet, little work has been done with regard to stream-
processing frameworks.
2.2 The case for central control
In order to provide stream processing systems with an
ability to change a running data processing graph, improve
ﬂexibility and elasticity, we elaborate on the idea of using
SDN-inspired control of both network communication and
inter-worker communication on a single host. Even though
we have ﬁne grained control over the communication be-
tween nodes, typically only local and low-level conﬁgura-
tion options are available for all communication that stays
within the boundaries of a host. We propose extending the
software-deﬁned network boundary into the operating sys-
tem and even applications. The SDN controller can then be
used to create network abstractions for particular applica-
tions and change properties of the communication graph to
react to application’s needs.
We believe that a central controller may handle both net-
work communication and communication local to a host.
Thereby, we provide a general abstraction for communica-
tion patterns independent of whether communication part-
ners are placed on the same host or not. In fact, our sys-
tem may provide a very similar API to the one used by the
most prominent SDN wire protocol OpenFlow. That is a
ﬂow abstraction to specify data streams across the network.
With this abstraction (which would also seamlessly work
with existing SDN technology), it does not matter if a ﬂow
between nodes or across the topology is local to a single host
or spreads across an entire datacenter.
Going one step further, this abstraction also allows ten-
ants in a datacenter to specify their data-ﬂow graph and
the controller may allocate bandwidth, place, and migrate
virtual machines to provide the requested topology and char-
acteristics.
Through central control, optimized placement across the
entire administrative domain (potentially saving resources
and money by giving their placement logic to the datacenter
operator) is possible.
2.3 Prototype
Instead of directly integrating this idea into existing stream
processing frameworks, we started by implementing a very
limited custom prototype of a stream processing framework
incorporating central control of the topology and its prop-
erties. Our prototype consists of skeleton code for work-
ers, a so-called application switch and the central controller.
The controller controls both OpenFlow switches deployed
throughout the network and application switches, our mes-
saging mechanism leverages Linux’ D-Bus message bus sys-
tem to pass data between workers. If the next processing
worker of a given tuple is not local to the application switch,
the data is sent to the appropriate host where it is again
handled by an application switch instance.
Based on our early evaluations, our system is able to
dynamically (at runtime) migrate workers between hosts,
change network properties based on application demands,
and insert and remove workers and classes of workers. We
believe that this approach may signiﬁcantly improve ﬂexi-
bility and elasticity of such and other systems that highly
rely on intra- and inter-host communication. Furthermore,
given that applications are able to dynamically dictate their
needs to the network, and more quickly react to increased de-
mand, we expect overall processing performance to increase
in many cases.
3. FUTURE DIRECTIONS
In the future, we propose investigating three major areas
along the lines of this project. First, we believe it is impera-
tive to extend an existing stream-processing framework such
as Storm or Samza with our technology. This would allow
us to perform case studies and performance evaluations us-
ing typical workloads for these systems. Also, consequences
of dynamic, feedback-based reconﬁguration and placement
such as optimized resource utilization can be explored. Sec-
ond, we would like to explore which other types of applica-
tions may beneﬁt from our idea. We believe that operating
systems could directly expose an API to control messaging
and communication ﬂows between processes. Third, lever-
aging the ﬂexibility that our system provides, it is possible
to centrally control scheduling, shuﬄing and partitioning be-
havior of stream processing applications.
4. REFERENCES
[1] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron.
Towards Predictable Datacenter Networks. In
Procesdings of SIGCOMM 2011.
[2] A. D. Ferguson, A. Guha, C. Liang, R. Fonseca, and
S. Krishnamurthi. Participatory Networking: An API
for Application Control of SDNs. In Proceedings of
SIGCOMM 2013.
[3] K. Goodhope, J. Koshy, and J. Kreps. Building
LinkedIn’s Real-time Activity Data Pipeline. IEEE
Data Eng. . . . , 2012.
[4] J. Schad, J. Dittrich, and J.-A. Quian´e-Ruiz. Runtime
measurements in the cloud: Observing, analyzing, and
reducing variance. Proc. VLDB Endow., Sept. 2010.
382