(a)
(b)
(c)
(d)
(e)
Figure 4: Detection accuracy for different attacks.
Targeted Poisoning Attack. Here we consider the error-specific
5.2.2
attack, which means that the goal of the adversary is to make the
model misclassify the inputs from the target class to a specific
class that he desires. Specifically, the adversary modifies the model
parameters by exploiting data poisoning method to retaining the
model, and compromises the model to incorrectly classify “STOP"
sign into “speed limit 50km/h" but behave normally in other cases.
We conduct our experiment in a CNN model with GTSRB [31]
dataset (contain more than 40 classes of traffic signs). The detailed
experiment configuration is shown in Table 4(see APPENDIX).
Figure 4(b) shows the detection accuracy for the targeted poi-
soning attack with the different number of samples, where we also
require the server to return the Top-1 result to the client. It is clear
that the detection accuracy of our sensitive-samples is signifi-
cantly better than SSFDNN [19] and randomly selected samples.
Moreover, we also record the detection accuracy under different
values of k (shown in Table 7 of APPENDIX). Since the targeted
poisoning attack considered in our experiment is less noticeable
than the Trojan attack, the detection accuracy of the three types of
samples is slightly reduced. However, compared to the other two
samples, our sensitive-samples also show excellent detection
performance.
5.2.3 Model Compression Attack. In this type of attack, the ad-
versary tries to reduce the size of the outsourced model as much
as possible without significantly affecting the model’s inference
accuracy, thus effectively reducing the storage and computation
overhead of the cloud. In our experiments, we simulate the adver-
sary to compress the model to a quarter of the original model, while
the inference accuracy is only reduced by 3.7%. all the experiments
are conducted over a CNN model with CIFAR [46] dataset. The
CIFAR dataset consists of 6000 32 × 32 images categorized into 10
classes.
As shown in Figure 4(c), we can see that the detection accuracy of
our sensitive-samples is also significantly better than SSFDNN
[19] and randomly selected samples. Moreover, we also record the
detection accuracy under the different number of returned results
(shown in Table 8 of APPENDIX). Obviously, compared to the other
two samples, our sensitive-samples also show excellent detec-
tion performance.
5.2.4 Arbitrary Weights Modification. In this type of attack, the
adversary (such as the cloud server) can launch attacks by arbitrarily
modifying a subset of the weights. To simulate this, we assume the
adversary modifies the weights with ratio r (from 0.1% to 80%), and
then we record the detection accuracy under the different ratios of
changed weights. All the targeted weights are modified by adding
standard Gaussian noise.
Figure 4(d) shows that our sensitive- samples are very sensi-
tive to model changes compared with SSFDNN [19] and randomly
selected samples. In order to show the relationship between weight
changes and detection accuracy, we test the detection accuracy
of sensitive-samples under different proportional parameter
changes. As shown in Figure 4(e), the greater ratio of model pa-
rameters changed, the easier it is to be detected. Also, we record
the detection accuracy under the different number of returned re-
sults (shown in Table 9 of APPENDIX). Compared to the other two
samples, our sensitive-samples also show excellent detection
performance.
5.3 Overhead Evaluation
we select datasets from UC Irvine Machine Learning Repository,
and data in batch from MNIST [10] dataset to test the performance
of SecureDL. We test the overhead of the proposed scheme under a
custom CNN network, which consists of two convolutional layers (
containing 20 feature maps and 50 feature maps, respectively), two
average pooling layer and two fully connected layers (256 and 10
neurons, respectively). Since HELib supports Single-Instruction-
Multiple-Data (SIMD) [16, 28] techniques, which can achieve the
time to run a batch (such as 282, 576, 1420, 3668, 6144, 8912, etc)
of instances is equivalent to the time to run a single instance, we
also use it to improve the classification efficiency (For more detail,
please see Figure 5 in APPENDIX).
Table 2: Running time of classification over encrypted data
datasets
Breast tissues
Crab
Ovarian
Wine
Climate
Fertility
Classification(s)
181.23
174.56
214.33
182.57
193.64
183.26
#C*
14
14
14
14
14
14
Noise Reduction(s)
115.84
112.07
138.06
116.26
124.71
117.26
As shown in Table 2 (where#C denotes the number of commu-
nication between the user and the server for noise reduction), we
require SecureDL to classify over encrypted data (i.e., Breast tis-
sues, Crab, Ovarian, Wine,Climate, and Fertility, shown in Table 5
 Detection accuracy for NNTA12345678910Number of samples20406080100120140Detection accuracy(%)Original modelSSFDNNSecureDL Detection accuracy for TPA12345678910Number of samples20406080100120140Detection accuracy(%)Original modelSSFDNNSecureDL Detection accuracy for MCA12345678910Number of samples20406080100120140Detection accuracy(%)Original modelSSFDNNSecureDL Detection accuracy for AWM12345678910Number of samples20406080100120140Detection accuracy(%)Original modelSSFDNNSecureDL00.20.40.60.8Ratio of weights changed00.20.40.60.81Detection accuracy(%)1 Sentitive-samples3 Sentitive-samples5 Sentitive-samples792ACSAC 2020, December 7–11, 2020, Austin, USA
Guowen Xu et al.
of APPENDIX), where the batch size is set 576. We can observe
that SecureDL only needs at most 0.33 seconds to process the clas-
sification of an instance. This is mainly due to the following two
reasons. First, we convert the complex activation function into a
low-degree polynomial, which is advantageous for LHE to perform
fewer calculations in the ciphertext. The other is the use of SIMD
(see Figure 5 in APPENDIX for more details), which can process
multiple ciphertexts in parallel, thereby accelerating the efficiency
of ciphertext calculations. For the comprehensiveness of the ex-
periment, we further increase the complexity of the selected CNN
network and record the running time of SecureDL in different data
sets. For more details, please refer to Table 10 to Table 12 in AP-
PENDIX (where #CL* denotes the number of convolutional layers
added to the benchmark model).
Table 3: Performance compared with existing approaches
Dataset
MNIST
Performance
Accuracy
Running time
Data transfer
SecureDL
96.23%
319(s)
330.7(MB)
Cryptonets
95.93%
803(s)
776(MB)
SecureML
91.67%
10648(s)
2.46(GB)
Next, we test the cost of SecureDL and compare it with the state-
of-the-art approaches Cryptonets [14] and SecureML [35], where
we use the same experimental configuration (including hardware,
data set and CNN model) to implement these three schemes. Cryp-
tonets [14] is very similar to SecureDL. It utilizes FHE to encrypt all
of the user’s data and exploits square function to approximate the
activation function. As shown in Table 3, where we set the batch of
ciphertext with size 8192 (the same as works [14]), since Cryptonets
does not consider how to convert the activation function into a
low-degree polynomial and uses bootstrapping[65] to execute noise
reduction, compared to SecureDL, it needs to perform more com-
putations and generate larger size ciphertext to complete the same
classification task. As a result, its computation and communication
overhead is more than twice that of SecureDL.
SecureML [35] is the first work based on SMC. In their proposed
approach, the data owner shares the data with the two servers which
run a deep learning model using two-party computation (2PC)
technique. As shown in Table 3, our SecureDL is significantly better
than SecureML [35] in terms of computation and communication
overhead. For example, to complete the same classification task
over the MNIST dataset, our solution only needs to run 319(s) and
transfer 330.7(MB) of data, whereas SecureML’s cost is 10648(s) and
2.46(GB), respectively.
6 CONCLUSION
In this paper, we have proposed SecureDL, which can verify the
integrity of DNNs model outsourced to the cloud, while protecting
user’s privacy in the inference process. we prove that our SecureDL
can be applied to general neural networks, with no assumptions
on DNNs architecture, hyper-parameters and training methods.
Extensive experiments also demonstrated the superior performance
of SecureDL in terms of inference accuracy, detection accuracy and
overhead. In the further works, we intend to further improve the
inference accuracy, and find ways to reduce the computation and
communication overhead of SecureDL.
ACKNOWLEDGMENTS
This work is supported by the National Key R&D Program of China
under Grants 2017YFB0802300 and 2017YFB0802000, the National
Natural Science Foundation of China under Grants 62020106013,
61972454, 61802051, 61772121, and 61728102, Sichuan Science and
Technology Program under Grants 2020JDTD0007 and 2020YFG0298,
the Peng Cheng Laboratory Project of Guangdong Province
PCL2018KP004.
REFERENCES
[1] Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and
Brendan McMahan. 2018. cpSGD: Communication-efficient and differentially-
private distributed SGD. In Advances in Neural Information Processing Systems.
7564–7575.
[2] Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al. 2018. Privacy-
preserving deep learning via additively homomorphic encryption. IEEE Transac-
tions on Information Forensics and Security 13, 5 (2018), 1333–1345.
[3] Marshall Ball, Brent Carmer, Tal Malkin, Mike Rosulek, and Nichole Schimanski.
2019. Garbled Neural Networks are Practical. IACR Cryptology ePrint Archive
2019 (2019), 338.
[4] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan
Mcmahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Practi-
cal Secure Aggregation for Privacy-Preserving Machine Learning. In Proceedings
of ACM CCS. 1175–1191.
[5] Florian Bourse, Michele Minelli, Matthias Minihold, and Pascal Paillier. 2018.
Fast homomorphic evaluation of deep discretized neural networks. In Annual
International Cryptology Conference. Springer, 483–512.
[6] Zvika Brakerski, Craig Gentry, and Vinod Vaikuntanathan. 2014.
(Leveled)
fully homomorphic encryption without bootstrapping. ACM Transactions on
Computation Theory 6, 3 (2014), 13–24.
[7] Yuhan Cai and Raymond Ng. 2004. Indexing spatio-temporal trajectories with
Chebyshev polynomials. In Proceedings of the ACM SIGMOD. 599–610.
[8] Jean-Paul Calvi and Norman Levenberg. 2008. Uniform approximation by discrete
least squares polynomials. Journal of Approximation Theory 152, 1 (2008), 82–100.
[9] Chaochao Chen, Liang Li, Wenjing Fang, Jun Zhou, Li Wang, Lei Wang, Shuang
Yang, Alex Liu, and Hao Wang. 2020. Secret Sharing based Secure Regressions
with Applications. arXiv preprint arXiv:2004.04898 (2020).
[10] Xuhui Chen, Jinlong Ji, Lixing Yu, Changqing Luo, and Pan Li. 2018. SecureNets:
Secure Inference of Deep Neural Networks on an Untrusted Cloud. In Asian
Conference on Machine Learning. 646–661.
[11] Rida T Farouki. 2012. The Bernstein polynomial basis: A centennial retrospective.
Computer Aided Geometric Design 29, 6 (2012), 379–419.
[12] AD Gadjiev and C Orhan. 2002. Some approximation theorems via statistical
convergence. The Rocky Mountain Journal of Mathematics (2002), 129–138.
[13] Craig Gentry, Shai Halevi, Chris Peikert, and Nigel P Smart. 2012. Ring switching
in BGV-style homomorphic encryption. In International Conference on Security
and Cryptography for Networks. Springer, 19–37.
[14] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig,
and John Wernsing. 2016. Cryptonets: Applying neural networks to encrypted
data with high throughput and accuracy. In Proceedings of the ICML. 201–210.
[15] S Dov Gordon and Jonathan Katz. 2006. Rational secret sharing, revisited. In
International Conference on Security and Cryptography for Networks. Springer,
229–241.
[16] Robert J Gove, Keith Balmer, Nicholas K Ing-Simmons, and Karl M Guttag. 1993.
Multi-processor reconfigurable in single instruction multiple data (SIMD) and
multiple instruction multiple data (MIMD) modes and method of operation. US
Patent 5,212,777.
[17] Shai Halevi and Victor Shoup. 2014. Algorithms in helib. In Annual Cryptology
Conference. Springer, 554–571.
[18] Lucjan Hanzlik, Yang Zhang, Kathrin Grosse, Ahmed Salem, Max Augustin,
Michael Backes, and Mario Fritz. 2018. Mlcapsule: Guarded offline deployment
of machine learning as a service. arXiv preprint arXiv:1808.00590 (2018).
[19] Zecheng He, Tianwei Zhang, and Ruby Lee. 2019. Sensitive-Sample Fingerprint-
ing of Deep Neural Networks. In Proceedings of the IEEE CVPR. 4729–4737.
[20] Zecheng He, Tianwei Zhang, and Ruby B Lee. 2018. VerIDeep: Verifying In-
tegrity of Deep Neural Networks through Sensitive-Sample Fingerprinting. arXiv
preprint arXiv:1808.03277 (2018).
[21] Ehsan Hesamifard, Hassan Takabi, Mehdi Ghasemi, and Rebecca N Wright. 2018.
Privacy-preserving machine learning as a service. Proceedings on Privacy En-
hancing Technologies 2018, 3 (2018), 123–142.
[22] Briland Hitaj, Giuseppe Ateniese, and Fernando Pérez-Cruz. 2017. Deep Models
Under the GAN: Information Leakage from Collaborative Deep Learning. In
proceedings of the ACM CCS. 603–618.
793Secure and Verifiable Inference in Deep Neural Networks
ACSAC 2020, December 7–11, 2020, Austin, USA
[23] Briland Hitaj, Giuseppe Ateniese, and Fernando Pérez-Cruz. 2017. Deep Models
Under the GAN: Information Leakage from Collaborative Deep Learning. In
Proceedings of the ACM CCS. 603–618.
[24] Tyler Hunt, Congzheng Song, Reza Shokri, Vitaly Shmatikov, and Emmett
Witchel. 2018. Chiron: Privacy-preserving machine learning as a service. arXiv
preprint arXiv:1803.05961 (2018).
[25] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li. 2018. Manipu-
lating Machine Learning: Poisoning Attacks and Countermeasures for Regression
Learning. In proceedings of the IEEE Security and Privacy. 19–35.
[26] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. 2018.
{GAZELLE}: A low latency framework for secure neural network inference.
In Proceedings of the {USENIX} Security. 1651–1669.
[27] Julien Keuffer, Refik Molva, and Hervé Chabanne. 2018. Efficient Proof Composi-
tion for Verifiable Computation. In European Symposium on Research in Computer
Security. Springer, 152–171.
[28] Yuyun Liao and David B Roberts. 2002. A high-performance and low-power
32-bit multiply-accumulate unit with single-instruction-multiple-data (SIMD)
feature. IEEE Journal of Solid-State Circuits 37, 7 (2002), 926–931.
[29] Jian Liu, Mika Juuti, Yao Lu, and N Asokan. 2017. Oblivious Neural Network
Predictions via MiniONN Transformations. In Proceedings of ACM CCS. 619–631.
[30] Qi Liu, Tao Liu, Zihao Liu, Yanzhi Wang, Yier Jin, and Wujie Wen. 2018. Security
analysis and enhancement of model compressed deep learning systems under
adversarial attacks. In Proceedings of the Asia and South Pacific Design Automation
Conference. IEEE, 721–726.
[31] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
and Xiangyu Zhang. 2018. Trojaning attack on neural networks. In proceedings
of the NDSS. 309–326.
[32] Yuntao Liu, Yang Xie, and Ankur Srivastava. 2017. Neural trojans. In Proceedings
of the IEEE ICCD. 45–48.
[33] Changxue Ma, Yves Kamp, and Lei F Willems. 1994. A Frobenius norm approach
to glottal closure detection from the speech signal. IEEE Transactions on Speech
and Audio Processing 2, 2 (1994), 258–265.
[34] Payman Mohassel and Peter Rindal. 2018. ABY3: A mixed protocol framework
for machine learning. In Proceedings of the ACM CCS. 35–52.
[35] Payman Mohassel and Yupeng Zhang. 2017. SecureML: A system for scalable
privacy-preserving machine learning. In proceedings of IEEE Security and Privacy.
19–38.
[36] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillation as a defense to adversarial perturbations against deep neural
networks. In Proceedings of the IEEE Security and Privacy. IEEE, 582–597.
[37] NhatHai Phan, Yue Wang, Xintao Wu, and Dejing Dou. 2016. Differential Pri-
vacy Preservation for Deep Auto-Encoders: an Application of Human Behavior
Prediction.. In AAAI, Vol. 16. 1309–1316.
[38] NhatHai Phan, Xintao Wu, Han Hu, and Dejing Dou. 2017. Adaptive laplace
mechanism: Differential privacy preservation in deep learning. In Proceedings of
the IEEE ICDM. 385–394.
[39] Tran Thi Phuong et al. 2019. Privacy-preserving deep learning via weight trans-
mission. IEEE Transactions on Information Forensics and Security 14, 11 (2019),
3003–3015.
[40] M Sadegh Riazi, Mohammad Samragh, Hao Chen, Kim Laine, Kristin Lauter,
and Farinaz Koushanfar. 2019. {XONN}: XNOR-based Oblivious Deep Neural
Network Inference. In Proceedings of the {USENIX} Security. 1501–1518.
[41] M Sadegh Riazi, Christian Weinert, Oleksandr Tkachenko, Ebrahim M Songhori,
Thomas Schneider, and Farinaz Koushanfar. 2018. Chameleon: A hybrid secure
computation framework for machine learning applications. In Proceedings of the
ACM AsiaCCS. 707–721.
[42] Bita Darvish Rouhani, M Sadegh Riazi, and Farinaz Koushanfar. 2018. Deepsecure:
Scalable provably-secure deep learning. In Proceedings of the Annual Design
Automation Conference. ACM, 21–26.
[43] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label poi-
soning attacks on neural networks. In Advances in Neural Information Processing
Systems. 6103–6113.
[44] Jie Shen. 1994. Efficient spectral-Galerkin method I. Direct solvers of second-and
fourth-order equations using Legendre polynomials. SIAM Journal on Scientific
Computing 15, 6 (1994), 1489–1505.
[45] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-Preserving Deep Learning. In
Proceedings of the ACM CCS. 1310–1321.
[46] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. 2017. Certified defenses
for data poisoning attacks. In Advances in Neural Information Processing Systems.
3517–3529.
[47] Ayush Tewari, Michael Zollhofer, Hyeongwoo Kim, Pablo Garrido, Florian