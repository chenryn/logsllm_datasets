to server i.
i = Qi · D to client.
5: Server i receives vector Qi.
6: Server i sends the product R(cid:48)
7: Client receives R(cid:48)
1, . . . ,R(cid:48)
(cid:96).
8: Client computes Ri = c−1
i R(cid:48)
9: Client considers vectors Sq =(cid:104)R1[q], . . . ,R(cid:96)[q](cid:105) as re-
ceived Reed-Solomon codewords and uses the algo-
rithms from Section 2.3 to recover word q of row β
of D.
i for each i.
(cid:7) ≤ v blocks have been re-
ing until m =(cid:6)
10: If the recovery algorithm fails, postpone decod-
v
h−t−1
quested (requesting blocks multiple times if neces-
sary). Then use the algorithm from Section 2.3.2 to
recover all of the blocks simultaneously.
5
Implementation and experiments
We implemented the algorithm described in this paper
as an extension of Goldberg’s implementation of his
protocol, available as the Percy++ project on Source-
Forge [18]. The software is implemented in C++ using
the NTL library [35].
In this paper we are concerned with the speed of the
client-side block reconstruction operation in the pres-
ence of Byzantine servers. Our work does not change
the server side of Goldberg’s protocol in any way; to see
speeds for the server-side operations, see Olumoﬁn and
Goldberg’s 2011 paper [29].
5.1 Choice of underlying ﬁeld
Goldberg’s 2007 work used a 128-bit prime ﬁeld as the
ﬁeld F. Subsequent releases of Percy++, however, were
able to use different ﬁelds, including prime ﬁelds of dif-
ferent sizes as well as GF(28). This last ﬁeld turns out to
be a very efﬁcient choice, as additions in this ﬁeld can be
implemented as XOR operations and multiplications are
simple lookups in a 64 KB table.
Our implementation of our protocols uses C++ tem-
plates to abstract the ﬁeld F, making it very easy to work
over any desired ﬁeld.
5.2 Portfolio algorithms for decoding
Our implementation assembles the error correction al-
gorithms described in Section 2.3 into a portfolio algo-
rithm [20] to do efﬁcient decoding. We use dynamic
programming to choose an optimal sequence of decod-
ing algorithms to try.
Each of the error correction algorithms we use is char-
acterized by the tuple (k,t,h) with k ≥ h > t: we wish to
ﬁnd a polynomial of degree at most t that passes through
at least h of the k input points.
We have a few different choices of algorithm to solve
this problem directly:
Berlekamp-Welch: If h > k+t
Guruswami-Sudan: If h >
2 , we can use the
Berlekamp-Welch algorithm to ﬁnd the unique
polynomial solution, if it exists. This algorithm is
quite fast, and we use it whenever it is applicable.
√
kt, we can use the
Guruswami-Sudan algorithm to ﬁnd all solutions. It
turns out this algorithm is very inefﬁcient if h2 − kt
is small; for the parameter sizes we care about, we
avoid this algorithm if this value is less than 10.
Brute force: Lagrange interpolate each subset of t + 1
points to form a polynomial of degree t, and see if it
passes through at least h points. This works for any
h > t, but is inefﬁcient if(cid:0) k
(cid:1) is large.
t+1
In addition, we have three strategies to attempt to solve
a particular instance with parameters (k,t,h) by recur-
sively solving smaller instances and combining the re-
sults. Let C(k,t,h) represent the expected time cost to
solve an instance of this size; we will bound this cost as
a function of the costs of solving smaller instances.
Guess g incorrect points: If the client can guess that a
particular point is wrong (that is, that the server
that provided that point is Byzantine), then it can
just throw away the point, and solve the remaining
problem, with parameters (k − 1,t,h). In general,
it might guess g points to be wrong, and solve a
problem of size (k − g,t,h). Since at least h of the
original k points are correct, for any set of h + g
points, there exists a subset of g points that can be
removed from the original k so that there are at least
h correct points in the k− g points remaining. Thus
(cid:1) smaller instances and
by recursively solving(cid:0)h+g
(cid:18)h + g
(cid:19)
combining the results, we are guaranteed to ﬁnd all
solutions to our original problem. Thus we see that
g
·C(k− g,t,h)
C(k,t,h) ≤ min
g
g
Guess g correct points: Conversely,
we get that we need to recursively solve (cid:0)k−h+g
(cid:1)
the client might
guess that a particular subset of g points are all cor-
rect (that is, that the servers that provided them are
honest), and recursively try to ﬁnd a polynomial of
degree at most t−g that passes through at least h−g
of the remaining k−g points.1 Similar to the above,
subproblems with parameters (k−g,t −g,h−g), so
we get
C(k,t,h) ≤ min
g
(cid:18)k− h + g
(cid:19)
·C(k− g,t − g,h− g)
g
g
Guess whether d points are correct or incorrect: The
above strategies may not be helpful if the binomial
coefﬁcients are large. Our ﬁnal strategy is to pick a
ﬁxed set of d points. For all g, and for all choices of
g correct and d − g incorrect points within that set,
we recursively try to ﬁnd polynomials of degree at
most t − g that pass through at least h − g of the
remaining k− d points. As before, we get that
(cid:18)d
(cid:19)
g
C(k,t,h) ≤ min
d ∑
g
·C(k− d,t − g,h− g)
Given these three algorithms to directly solve the prob-
lem, and three strategies to indirectly solve it by combin-
ing solutions to smaller instances, we use dynamic pro-
gramming to build a table of the best strategy to use to
1The remaining points are actually slightly modiﬁed before solv-
ing recursively. If (α∗,y∗) is guessed to be correct, then each other
point (αi,yi) is modiﬁed to (αi, yi−y∗
αi−α∗ ) before recursively solving.
If
then
f (x)· (x−α∗) + y∗ interpolates the corresponding h− 1 original points
and also the guessed point.
f (x) interpolates at least h − 1 points of the latter form,
minimize the expected run time for inputs of each com-
bination of parameters (k,t,h). We measure the runtimes
of the direct algorithms experimentally, and compute the
times for the indirect strategies. We pick the lowest result
of the six, and set C(k,t,h) to that value. We currently do
this in a precomputation step for all k ≤ 25 and give the
PIR client access to this table.
5.3 Multi-polynomial decoding
We also implemented the linear multi-polynomial decod-
ing algorithm described in Section 2.3.2 as an extension
of Percy++ using C++ and the NTL library. For the lat-
tice reduction step, our implementation uses the lattice
reduction algorithm by Mulders and Storjohann [27]. Al-
though its theoretical runtime is not the fastest known,
we chose this algorithm because of its simplicity. Af-
ter the lattice reduction, the implementation then solves
the resulting system of linear equations using Gaussian
elimination.
(cid:16) 1|F|
(cid:17)m(h−t−1)−v+1
As previously mentioned, if the errors are random,
then there is a very low probability that
this algo-
rithm will fail. Based on experimental investigation,
we conjecture the probability of failure is, to ﬁrst order,
. (Recall from Section 2.3.3 that in or-
der for the algorithm to work, m≥ v
h−t−1, or equivalently,
m(h−t − 1)− v ≥ 0.) See the appendix for more details
on these experiments. This probability of failure falls
within the conﬁdence intervals for all of our tests with
|F| ≥ 256. We also ran tests with an extremely small
ﬁeld of |F| = 16, and found that failures in that ﬁeld oc-
curred slightly (but statistically signiﬁcantly) more often
than our conjecture predicts. This leads us to believe that
there is a missing second-order term in our conjecture,
which is negligible for reasonable ﬁeld sizes, but signif-
icant for tiny ﬁelds. We hope to nail down the missing
term in future work.
In the cases where the linear multi-polynomial algo-
rithm does fail, our algorithm will wait until another
block is requested and then try again. This increases m
by one and reduces the probability that the algorithm will
fail by a factor of |F|h−t−1; therefore, since h−t ≥ 2, the
probability it will fail a second time is extremely tiny.
5.4 Measuring improvements to Percy++
In his 2007 paper, Goldberg measures the performance
of his protocols using a Lenovo T60p laptop com-
puter with a 2.16 GHz Intel dual-core CPU running
Ubuntu Linux [19]. For the purposes of comparison,
we have performed our measurements on a machine
of the same model and similar Ubuntu Linux conﬁg-
uration. Goldberg reports that the implementation of
Table 1: Measuring improvements to Percy++’s client-side decoding algorithms. For these measurements we ran 100
trials using the parameters (k,t,h) = (20,10,15).
Implementation
Algorithm
Field
Time
timing reported by Goldberg [19]
Percy++
Percy++
this work
this work
Guruswami-Sudan in MuPAD
Guruswami-Sudan in C++
Guruswami-Sudan in C++
Cohn-Heninger in C++ with m = 2 blocks
Cohn-Heninger in C++ with m = 2 blocks
128-bit prime
128-bit prime
GF(28)
128-bit prime
GF(28)
“several minutes”
9000± 3000 ms
3000± 900 ms
2.2± 0.9 ms
1.3± 0.4 ms
his HARDRECOVER algorithm takes “several minutes”
when using the values (k,t,h) = (20,10,15) [19].
Since the writing of that paper, the Percy++ software
has improved. The ﬁrst improvement was to imple-
ment the parts of the HARDRECOVER subroutine pre-
viously written using MuPAD in native C++. We timed
HARDRECOVER 100 times using only this change, and
found the running time reduced to 9000± 3000 ms.
• the Guruswami-Sudan list decoding algorithm from
the latest release of Percy++
• the single-polynomial dynamic programming algo-
rithm described in Section 5.2; and
• the linear multi-polynomial algorithm described in
Section 5.3
The other
improvement
in the latest version of
Percy++ is to use GF(28) as the underlying ﬁeld, rather
than a 128-bit prime ﬁeld. With this change, we again
measured HARDRECOVER 100 times, and found the run-
ning time further reduced to 3000± 900 ms.
Finally, using the implementation of our algorithm de-
scribed in Section 4 we further improve the running time,
again tested with 100 trials using multi-polynomial de-
coding with just m = 2 blocks. With a 128-bit prime
ﬁeld, our algorithm completes in 2.2± 0.9 ms; with
GF(28), in just 1.3± 0.4 ms.
This is a reduction of over three orders of magnitude in
client-side decoding time versus the latest software, and
of over ﬁve orders of magnitude versus Goldberg’s 2007
reported measurements. This comes at a cost of fetching
just two blocks instead of one — something the client is
likely to have done anyway. The results are summarized
in Table 1.
5.5 New client-side measurements
We next outline the results of measurements taken of the
implementation of our new algorithm described in Sec-
tion 4. These measurements are only on the client-side
decoding operations. For these measurements we used
a server with a 2.40 GHz Intel dual-core CPU running
Ubuntu Linux. For each case, we ran at least 100 trials,
all using only a single core. We used the ﬁeld GF(28)
for all experiments in this section.
To illustrate the improvements that our algorithm pro-
vides, we compare time measurements for four algo-
rithms in Figure 2:
• the potentially exponential-time brute force decod-
ing algorithm