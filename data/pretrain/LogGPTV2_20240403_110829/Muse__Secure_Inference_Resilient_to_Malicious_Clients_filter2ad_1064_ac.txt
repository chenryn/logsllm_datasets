221.5
219.2
216
217.1
221.5
217.1
100,480
25,120
210
620
1,110
4,020
29,620 —
500,500 —
50,500 —
61,000 —
105,220 —
29.5×
24×
312×
226.5×
205×
92×
n/a
n/a
n/a
n/a
n/a
158,088
15,809 —
networks with convolutions
32× (3,3)-P-100-10
Table 2: Query complexity of our attack vs. that of [Car+20]. The
notation l-m-n-. . . indicates a series of fully-connected layers of
dimension l × m, m × n, and so on, while 32 × (3,3) indicates a
convolutional layer consisting of 32 3× 3 ﬁlters, and P indicates a
2× 2 average pooling layer.
n/a
3 Threat model and privacy goals
In our system, there are two parties: the client and the service
provider (or server). The server holds a neural network model,
and the client holds some data that it wants classiﬁed by the
server’s model. To achieve this goal, the two parties interact
via a protocol for secure inference. This protocol takes as
input the server’s model and the client’s data, and computes
the classiﬁcation so that neither party learns any information
except this ﬁnal classiﬁcation. Below we clarify the security
guarantees we aim for when designing our secure inference
protocol MUSE.
3.1 Threat model
There are two standard notions of security for multiparty
computation: security against semi-honest adversaries, and
security against malicious adversaries. A semi-honest adver-
sary follows the protocol perfectly but inspects messages it
receives to learn information about other parties’ inputs. A
malicious adversary, on the other hand, may arbitrarily deviate
from the protocol.
We design MUSE for a new threat model called “security
against malicious clients” or client-malicious security. In this
setting, either a malicious adversary corrupts the client, or a
USENIX Association
30th USENIX Security Symposium    2205
semi-honest adversary corrupts the server. 6
3.2 Privacy goals
MUSE’s goal is to enable the client to learn at most the fol-
lowing information: the architecture of the neural network,
and the result of the inference; all other information about
the client’s private inputs and the parameters of the server’s
neural network model should be hidden. Concretely, we aim
to achieve a strong simulation-based deﬁnition of security as
follows:
Deﬁnition 3.1. A protocol Π between a server and a client
is said to securely compute a function f against a malicious
client and semi-honest server if it satisﬁes the following prop-
erties:
• Correctness. For any server’s input y and client’s input x,
the probability that at the end of the protocol, the client
outputs f (y,x) is 1.
• Semi-Honest Server Security. For any server S that follows
the protocol, there exists a simulator SimS such that for any
input y of the server and x of the client, we have:
viewS(y,x) ≈c SimS(y)
In other words, SimS is able to generate a view of the semi-
honest server without knowing the client’s private input.
• Malicious Client Security. For any malicious client C (that
might deviate arbitrarily from the protocol speciﬁcation),
there exists a simulator SimC such that for any input y of
the server, we have:
viewC(y) ≈c Sim
f (y,·)
C
In other words, the SimC is able to generate the view of a
malicious client with only access to an ideal functionality
that accepts a client’s input and outputs the result of the
function f . This modeling is used in cryptographic liter-
ature to capture the cases where a malicious client may
substitute its actual input with any other input of its choice.
Deﬁnition 3.2. We say that Π is a secure inference protocol
against malicious clients and semi-honest servers if it securely
computes NN(·,·) with the server input being M and the client
input being x.
Like most prior work, MUSE does not hide information
that is revealed by the result of the prediction. See Section 7.1
for a discussion of attacks that leverage this information, as
well as potential mitigations.
6One can generalize this threat model to n parties by considering two ﬁxed
subsets of parties: one of which can be corrupted by a malicious adversary,
and the other which can be corrupted by a semi-honest adversary
4 Building blocks
MUSE uses the following cryptographic building blocks.
Please see the full version for more formal deﬁnitions and
proofs.
Garbling Scheme. A garbling scheme [Yao86; Bel+12] is
a tuple of algorithms GS = (Garble, Eval) with the following
syntax:
• Garble(1λ,C,{labi,0, labi,1}i∈[n]) → ˜C. On input the secu-
rity parameter, a boolean circuit C (with n input wires)
and a set of labels {labi,0, labi,1}i∈[n], Garble outputs a gar-
bled circuit ˜C. Here labi,b represents assigning the value
b ∈ {0,1} to the i-th input wire.
• Eval( ˜C,{labi,xi}i∈[n]) → y. On input a garbled circuit ˜C and
labels {labi,xi}i∈[n] corresponding to an input x ∈ {0,1}n,
Eval outputs a string y = C(X).
We brieﬂy describe here the key properties satisﬁed by
garbling schemes. First, GS must be correct: the output of
Eval must equal C(x). Second, it must be private: given ˜C and
{labi,xi}, the evaluator should not learn anything about C or x
except the size of |C| (denoted by 1|C|) and the output C(x).
Leveled Fully Homomorphic public-key encryption.
A leveled fully-homomorphic encryption scheme HE =
(KeyGen, Enc, Dec, Eval) [Reg09; Fan+12] is a public key
encryption scheme that additionally supports homomorphi-
cally evaluating any depth-D arithmetic circuit on encrypted
messages. Formally, HE satisﬁes the following syntax and
properties:
• KeyGen(1λ) → (pk, sk): On input a security parameter,
KeyGen outputs a public key pk and a secret key sk.
• Enc(pk,m) → c: On input the public key pk and a message
m, the encryption algorithm Enc outputs a ciphertext c. We
assume that the message space is Zp for some prime p.
• Dec(sk,c) → m: On input a secret key sk and a ciphertext
c, the decryption algorithm Dec outputs a message m.
• Eval(pk,c1,c2, f ) → c(cid:48): On input a public key pk, cipher-
texts c1 and c2 encrypting m1 and m2 respectively, and a
depth-D arithmetic circuit f , Eval outputs a new ciphertext
c(cid:48).
Besides the standard correctness and semantic security prop-
erties, we require HE to satisfy the following properties:
• Homomorphism. If c1 := Enc(pk,m1), c2 := Enc(pk,m2),
and c := Eval(pk,c1,c2, f ), then Dec(sk,c) = f (m1,m2).
• Function privacy. Given a ciphertext c, no attacker can tell
what homomorphic operations led to c.
Additive secret sharing. Let p be a prime. A 2-of-2 additive
secret sharing of x ∈ Zp is a pair ((cid:104)x(cid:105)1,(cid:104)x(cid:105)2) = (x−r,r) ∈ Z2
p
for a random r ∈ Zp such that x = (cid:104)x(cid:105)1 +(cid:104)x(cid:105)2. Additive secret
sharing is perfectly hiding, i.e., given a share (cid:104)x(cid:105)1 or (cid:104)x(cid:105)2, the
value x is perfectly hidden.
Message authentication codes.
A message authenti-
cation code (MAC) is a tuple of algorithms MAC =
(KeyGen, Tag, Verify) with the following syntax:
2206    30th USENIX Security Symposium
USENIX Association
• KeyGen(1λ) → α: On input the security parameter, KeyGen
outputs a MAC key α.
• Tag(α,m) → σ: On input a key α and message m, Tag
outputs a tag σ and a secret state st.
• Verify(α, st,m,σ) → {0,1}: On input a key α, secret state
st, message m and tag σ, Verify outputs 0 or 1.
We require MAC to satisfy the following properties:
• Correctness. For any message m, α ← KeyGen(1λ), and
(σ, st) ← Tag(α,m), Verify(α, st,m,σ) = 1.
• One-time Security. Given a valid message-tag pair, no ad-
versary can forge a different, valid message-tag pair.
In this work, we will use the following construction of MACs:
p for some pn ≥ 2λ.
1. The message space is Zn
2. KeyGen samples a uniform element α ← Zp.
3. Tag(α,m) outputs σ = (cid:104)α· m(cid:105)1 and st = (cid:104)α· m(cid:105)2.
4. Verify(α, st,m,σ) checks if σ + st = α· m.
Beaver’s multiplicative triples. A multiplication triple is
a triple (a,b,c) ∈ Z3
p such that ab = c. A triple generation
procedure is a two-party protocol that outputs secret shares
of a triple (a,b,c) to two parties.
Authenticated secret shares. For any prime p, an element
x ∈ Zp, and a MAC key δ ∈ Zp an authenticated share of x
is a tuple (ε, [[x]]1, [[x]]2) := (ε, ((cid:104)x(cid:105)1,(cid:104)δ· x(cid:105)1), ((cid:104)x(cid:105)2,(cid:104)δ· x(cid:105)2)).
An authenticated share naturally supports local evaluation of
addition and multiplication by public constants, as well as
addition with another authenticated share. To multiply two
authenticated shares, one needs to use multiplication triples.
For simplicity of exposition, in the rest of the paper we omit
ε, as it is merely used for bookkeeping when adding public
constants.
Zero-knowledge proofs.
Let R be any NP relation. A
zero-knowledge proof for R is a protocol between a prover
P and a veriﬁer V that both have a common input x, where P
tries to convince V that it “knows” a secret witness w such
that (x,w) ∈ R . At the end of the protocol, V should have
learnt no additional information about w. We want our zero-
knowledge proof system to satisfy the standard deﬁnitions
of completeness, soundness, proof of knowledge, and zero-
knowledge.
5 The MUSE protocol
In this section, we describe MUSE, our secure inference pro-
tocol that is secure against a malicious client and a semi-
honest server. Like the DELPHI protocol (see Fig. 2) [Mis+20],
MUSE’s protocol consists of two phases: an ofﬂine prepro-
cessing phase, and an online inference phase. The ofﬂine pre-
processing phase is independent of the client’s input (which
regularly changes), but assumes that the server’s model is
static; if this model changes, then both parties have to re-
run the preprocessing phase. After preprocessing, during the
online inference phase, the client provides its input to our
specialized secure two-party computation protocol, and even-
tually learns the inference result. Below, we expand on the
Preprocessing phase. During preprocessing, the client and the
server pre-compute data for the online execution. This phase can
be executed independently of the input values, i.e., DELPHI can
run this phase before either party’s input is known. Preprocessed
data can only be used for a single inference.
1. Linear correlations generator: The client and server interact
with a functionality that, for each i ∈ [(cid:96)], outputs to them
secret shares of Miri, where ri is a random masking vector.
2. Preprocessing for ReLUs: The server constructs a garbled
circuit (cid:101)C for a circuit C computing ReLU. It sends (cid:101)C to the
client and then uses OT to send to the client the input wires
corresponding to ri+1 and Mi · ri − si.
Online phase. The online phase is divided into two stages:
1. Preamble: On input x, the client sends x− r1 to the server.
The server and the client now hold an additive secret sharing
of x.
2. Layer evaluation: Let xi be the result of evaluating the ﬁrst
(i− 1) layers of the neural network on x. At the beginning of
the i-th layer, the client holds ri, and the server holds xi − ri,
which means that they possess secret shares of xi.
• Linear layer: The server computes Mi · (xi − ri), which
means that the client and the server hold an additive secret
sharing of Mixi.
• ReLU layer: After the linear layer, the client and server
hold secret shares of Mixi. The server sends to the client
the labels corresponding to its secrete share, and the client
then evaluates the GC to obtain a secret share of the ReLU
output.
Figure 2: High-level overview of the DELPHI protocol [Mis+20].
high level overview in Section 1.2 and provide a detailed
description of both phases of our protocol.
Notation. The server holds a model M consisting of (cid:96) linear
layers M1, . . . ,M(cid:96) and the client holds an input vector x ∈ Zn
p.
We use NN(M,x) to denote the output of the neural network
when the server’s input is M and the client’s input is x. We
assume that the algorithm computing NN is public and is
known to both the client and the server.
5.1 Preprocessing phase
In the preprocessing phase, the client and the server pre-
compute data that can be used during the online execution.
This phase is independent of the client’s input values, and can
be run before the client’s input is known. However, this phase
cannot be reused and has to be run once for each client input.
5.1.1
As explained in Section 1.2, MUSE follows the approach
of DELPHI, and uses different cryptographic primitives to
produce preprocessed material for linear and non-linear layers.
Below we describe these primitives at a high level.
Linear layers. Like in DELPHI, our goal is to produce shares
of Mr for a linear layer M. This enables us to efﬁciently
compute linear layer operations in the online phase. Unlike
Intuition
USENIX Association
30th USENIX Security Symposium    2207
puts these labels if the inputs match the output of FACG. We
call this functionality Conditional Disclosure of Secrets, and
denote it by FCDS.
To construct a protocol ΠCDS that realizes FCDS, we have
two options: use 2PC protocols specialized for boolean com-
putation, or 2PC protocols specialized for arithmetic compu-
tation. Indeed, because this operation fundamentally reasons
about boolean values, it would seem reasonable to use a pro-
tocol like garbled circuits. However, checking validity of the
client’s input requires modular multiplications, which are ex-
tremely expensive when expressed as boolean circuits. Since
even the simplest neural networks oftentimes have thousands
of activations, the resulting communication and computation
cost is unacceptable.
Instead, we implement this functionality via MPC for arith-
metic circuits, as modular multiplication is cheap here. How-
ever, now the boolean operations are expensive. To overcome
this, we take further advantage of the client-malicious setting
to improve the MPC protocol we use to securely execute the
arithmetic circuit, as we describe in Section 5.3.
By designing efﬁcient protocols for FACG and FCDS, MUSE
achieves client-malicious security with an online phase de-
sign identical to that of the semi-honest DELPHI protocol. In
our implementation, there are a few differences we detail in
Remarks 5.2 and 5.3.
5.1.2 Protocol
We now present the full protocol for the preprocessing phase
of MUSE (see Fig. 3 for a graphical overview).
1. For every i ∈ [(cid:96)], denote ni,mi as the input and output sizes
of the i-th linear layer respectively. The client samples a
random layer input mask ri ← Zni
p and the server samples
a random layer output mask si ← Zmi
p . Additionally, the
server samples random MAC keys αi,βi ← Zp.
2. Authenticated correlations generator: The client and
server invoke functionality FACG with the client input
{ri}i∈[(cid:96)], and with server input {si,Mi,αi,βi}i∈[(cid:96)]. For each