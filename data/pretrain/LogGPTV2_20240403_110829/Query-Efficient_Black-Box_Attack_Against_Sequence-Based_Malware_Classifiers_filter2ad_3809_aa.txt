title:Query-Efficient Black-Box Attack Against Sequence-Based Malware Classifiers
author:Ishai Rosenberg and
Asaf Shabtai and
Yuval Elovici and
Lior Rokach
0
2
0
2
t
c
O
3
]
R
C
.
s
c
[
7
v
8
7
7
8
0
.
4
0
8
1
:
v
i
X
r
a
Query-Eﬃcient Black-Box Attack Against Sequence-Based
Malware Classiﬁers
Ishai Rosenberg, Asaf Shabtai, Yuval Elovici and Lior Rokach
Ben-Gurion University of The Negev
October 6, 2020
Abstract
In this paper, we present a generic, query-eﬃcient black-box attack against API call-based
machine learning malware classiﬁers. We generate adversarial examples by modifying the mal-
ware’s API call sequences and non-sequential features (printable strings), and these adversarial
examples will be misclassiﬁed by the target malware classiﬁer without aﬀecting the malware’s
functionality.
In contrast to previous studies, our attack minimizes the number of malware classiﬁer queries
required. In addition, in our attack, the attacker must only know the class predicted by the
malware classiﬁer; attacker knowledge of the malware classiﬁer’s conﬁdence score is optional.
We evaluate the attack eﬀectiveness when attacks are performed against a variety of malware
classiﬁer architectures, including recurrent neural network (RNN) variants, deep neural net-
works, support vector machines, and gradient boosted decision trees. Our attack success rate
is around 98% when the classiﬁer’s conﬁdence score is known and 64% when just the classiﬁer’s
predicted class is known.
We implement four state-of-the-art query-eﬃcient attacks and show that our attack requires
fewer queries and less knowledge about the attacked model’s architecture than other existing
query-eﬃcient attacks, making it practical for attacking cloud-based malware classiﬁers at a
minimal cost.
Introduction
1
Next generation anti-malware products use machine learning and deep learning models instead of
signatures and heuristics, in order to detect previously unseen malware. Windows-based API call
sequence-based classiﬁers, such as [11] or [8], provide state-of-the-art detection performance [15].
However, those classiﬁers are vulnerable to adversarial example attacks. Adversarial examples are
samples that are perturbed (modiﬁed), so they will be incorrectly classiﬁed by the target classiﬁer.
In this paper, we demonstrate a novel query-eﬃcient black-box attack against many types of
malware classiﬁers, including RNN variants. We implement our attack on malware classiﬁers that
are used to classify a process as malicious or benign. This classiﬁcation is based on both API call
sequences and additional discrete features (e.g., printable strings). We consider the challenging case
of machine learning as a service (MLaaS). Examples of such services are Amazon ML [1] and GCP
[5].
1
Accepted as a conference paper at ACSAC 2020
In this case, the attacker continuously queries the target malware classiﬁer (e.g., [6]) and modiﬁes
the API call sequences the malware generates until the sequence is classiﬁed as benign. The attacker
pays for every query of the target malware classiﬁer and therefore aims to minimize the number
of queries made to such cloud services when performing an attack. Another reason for minimizing
the number of queries is that many queries from the same computer might arouse suspicion of an
adversarial attack attempt, causing the cloud service to stop responding to those queries, e.g., using
stateful defenses that keep track of queries to the system [22]. While the attacker may use a botnet
to issue many queries, this approach would increase the attack’s cost dramatically.
We develop an end-to-end attack [40, 41] (a.k.a, problem-space attack [38]) by recrafting the
malware behavior so it can evade detection by such machine learning malware classiﬁers while
minimizing the amount of queries to the malware classiﬁer (meaning that the attack is query-
eﬃcient).
The main focus of most existing research (e.g., [30, 43]) is on the query-eﬃcient generation of
adversarial examples for images. This is diﬀerent from our work, which is focused on generating
adversarial API sequences, in two respects:
1. In the case of adversarial API sequences, one must verify that the original functionality of the
malware remains intact. Thus, one cannot simply generate an adversarial feature vector but
must generate an executable ﬁle containing the corresponding working malware behavior.
2. API sequences consist of discrete symbols of variable lengths, while images are represented as
matrices with ﬁxed dimensions, and the values of the matrices are continuous.
In this paper, we modify the malware’s behavior by modifying the API call sequences it generates.
We also modify static, non-sequential features: printable strings inside the process (Appendix D).
We present eight novel attacks that are diﬀerent in terms of three parameters:
1. Attacker knowledge - The attacker might have knowledge about the target classiﬁer’s con-
ﬁdence score (score-based attack) or only about the predicted class (decision-based attack).
This knowledge can aﬀect the way we modify both the positions and the types of modiﬁed
API calls.
2. Modiﬁed API call types (values) - The attacker can either modify API calls randomly (random
perturbation) or take them from a generative adversarial network (GAN) generating benign
samples (benign perturbation).
3. Method to select the number of modiﬁed API calls - The attacker can either modify API calls
until the sample successfully evades detection with minimal perturbation (linear iteration
attack) or start with a maximum number of modiﬁcations and minimize them as long as
evasion is maintained (logarithmic backtracking attack).
Each of the eight attacks we present is a combination of these three parameters, each of which has
two diﬀerent variants (score-based or decision-based; benign perturbation or random perturbation;
logarithmic backtracking or linear iteration). As a benchmark, we adapted four state-of-the-art
query-eﬃcient attacks. We showed that our attacks are equal or outperform them, obtaining a
higher success rate (98% using the classiﬁer’s conﬁdence score and 88% without it) for a ﬁxed
number of queries.
The contributions of our paper are as follows:
Accepted as a conference paper at ACSAC 2020
1. This is the ﬁrst query-eﬃcient, end-to-end, black-box adversarial attack for both sequential
and non-sequential input.
2. We provide two variants for our attack, to ﬁt the knowledge available for the attacker (conﬁ-
dence score or label only), thus ﬁtting practical cyber security domain use cases.
3. This is the ﬁrst usage of GAN (previously used for other purposes) to generate benign API
call trace samples to produce a query-eﬃcient attack.
4. This is the ﬁrst usage of self adaptive evolutionary algorithm (previously used for other pur-
poses) to implement a query-eﬃcient score-based attack.
2 Background and Related Work
The search for adversarial examples, such as those used in our attack, can be formalized as a
minimization problem:
(1)
The input x, correctly classiﬁed by the classiﬁer f, is perturbed with r, such that the resulting
adversarial example x + r remains in the input domain D but is assigned a diﬀerent label than x.
argr min f (x + r) (cid:54)= f (x) s.t. x + r ∈ D
There are three types of adversarial example generation methods:
In gradient-based attacks, adversarial perturbations are generated in the direction of the gradient,
that is, in the direction with the maximum eﬀect on the classiﬁer’s output (e.g., fast gradient sign
method [26]). Hu and Tan [28] used RNN GAN to generate invalid API calls and insert them into
the original API sequences. Gumbel-Softmax, a one-hot continuous distribution estimator, was
used to deliver gradient information between the generative RNN and substitute RNN. A white-
box gradient-based attack against RNNs demonstrated against long short-term memory (LSTM)
architecture for sentiment classiﬁcation of a movie review dataset was shown in [36]. A black-box
variant, which facilitates the use of a substitute model, was presented in [42]. The attack in this
paper is diﬀerent in a few ways:
1. As shown in Section 4.2, the attack described in [42] requires more target classiﬁer queries
and greater computing power to generate a substitute model.
2. We use a diﬀerent adversarial example generation algorithm, which uses a stochastic approach
rather than a gradient-based approach, making it harder to defend against (as mentioned in
Section 4.3).
3. Our decision-based attack is generic and doesn’t require a per malware pre-deployment phase
to generate the adversarial sequence (either using a GAN, as in [28], or a substitute model,
as in [42]). Moreover, generation takes place at run time, making it even more generic and
easier to deploy.
Score-based attacks are based on knowledge of the target classiﬁer’s conﬁdence score. Previous re-
search used a genetic algorithm (GA), where the ﬁtness of the genetic variants is deﬁned in terms of
the target classiﬁer’s conﬁdence score, to generate adversarial examples that bypass PDF malware
and image recognition classiﬁers, respectively [45, 16]. Those attacks used a computationally expen-
sive GA compared to our approach and were only evaluated when performed against support vector
Accepted as a conference paper at ACSAC 2020
machine (SVM), random forest, and CNN classiﬁers using static features only, and was not evalu-
ated against recurrent neural network variants using both static and dynamic analysis features, as
we do. In [43], the simultaneous perturbation stochastic approximation (SPSA) method was used,
since its gradient approximation requires only two queries (for the target classiﬁer’s score) per it-
eration, regardless of the dimension of the optimization problem. Alzantot et al. [17] presented a
sequence-based attack algorithm that exploits population-based gradient-free optimization via GA;
in this study, the attack was performed against a natural language processing (NLP) sentiment
analysis classiﬁer. While this attack can be used against RNNs, it requires more queries than our
attack (see Table 3).
Decision-based attacks only use the label predicted by the target classiﬁer. Ilyas et al. [30] used
natural evolutionary strategies (NES) optimization to enable query-eﬃcient gradient estimation,
which leads to the generation of misclassiﬁed images as seen in gradient-based attacks. Dang et
al.
[24] used the rate of feature modiﬁcations from known malicious and benign samples as the
score and used a hill climbing approach to minimize this score, evading SVM and random forest
PDF malware classiﬁers based on static features in an eﬃcient manner. Our approach, on the other
hand, is more generic and can handle RNN classiﬁers and multiple feature types. The performance
diﬀerences between our approaches and [24] are presented in Section 4.2.2.
All of the currently published score and decision-based attacks diﬀer from our proposed attack
in that:
1. They only deal with CNNs, random forest, and SVM classiﬁers, using non-sequential input, as
opposed to all state-of-the-art classiﬁers (including RNN variants), using discrete or sequence
input, as in our attack.
2. They deal primarily with images and rarely ﬁt the attack requirements of the cyber security
domain: while changing a pixel’s color doesn’t “break” the image, modifying an API call might
harm the malware functionality. In addition, small perturbations, such as those suggested
in [30, 43], are not applicable for discrete API calls: you can’t change WriteFile() to Write-
File()+0.001 in order to estimate the gradient to perturb the adversarial example in the right
direction; you need to modify it to an entirely diﬀerent API. This is reﬂected in Table 3.
3. They did not present an end-to-end framework to implement the attack in the cyber security
domain. Thus, the attack might be used for generating adversarial malware feature vectors
but not for generating a working adversarial malware sample.
The diﬀerences between those attacks and our attacks are summarized in Table 1.
3 Methodology
3.1 Attacking API Call-Based Malware Classiﬁers
An overview of the malware classiﬁcation process is shown in Figure 2 (in the appendix).
Assume a malware classiﬁer whose input is a sequence of API calls made by the inspected
process. API call sequences can be millions of API calls long, making it impossible to train such a
classiﬁer on the entire sequence at once, due to training time and GPU memory constraints. Thus,
the target classiﬁer uses a non-overlapping sliding window approach [42]: Each API call sequence
is divided into windows of k API calls. Detection is performed on each window in turn, and if any
Accepted as a conference paper at ACSAC 2020
Attack Type
Rosenberg et al.
[42]
(Gradient-Based
Attack)
Uesato et al. [43]
Ilyas et al. [30]
Alzantot et al. [16]
Our Score-Based
Attack
Our
Attack
Decision-Based
Table 1: Comparison to Previous Work
Domain
Cyber
Security
Image
Recognition
Image
Recognition
NLP
Cyber
Security
Cyber
Security
Input Type
Sequence, Non-sequential,
Mixed
Query-
Eﬃcient?
No
Score/Decision-
Based?
Decision
Non-sequential
Non-sequential
Sequence
Sequence, Non-sequential,
Mixed
Sequence, Non-sequential,
Mixed
Yes
Yes
Yes