title:A Study of Probabilistic Password Models
author:Jerry Ma and
Weining Yang and
Min Luo and
Ninghui Li
2014 IEEE Symposium on Security and Privacy
A Study of Probabilistic Password Models
Jerry Ma
Quora, Inc.
Weining Yang
Purdue University
PI:EMAIL
PI:EMAIL
Min Luo
Wuhan University
PI:EMAIL
Ninghui Li
Purdue University
PI:EMAIL
Abstract—A probabilistic password model assigns a probability
value to each string. Such models are useful for research into
understanding what makes users choose more (or less) secure
passwords, and for constructing password strength meters and
password cracking utilities. Guess number graphs generated from
password models are a widely used method in password research.
In this paper, we show that probability-threshold graphs have
important advantages over guess-number graphs. They are much
faster to compute, and at the same time provide information
beyond what is feasible in guess-number graphs. We also ob-
serve that research in password modeling can beneﬁt from the
extensive literature in statistical language modeling. We conduct a
systematic evaluation of a large number of probabilistic password
models, including Markov models using different normalization
and smoothing methods, and found that, among other things,
Markov models, when done correctly, perform signiﬁcantly better
than the Probabilistic Context-Free Grammar model proposed
in Weir et al. [25], which has been used as the state-of-the-art
password model in recent research.
I. INTRODUCTION
Passwords are perhaps the most widely used method for
user authentication. Passwords are both easy to understand and
use, and easy to implement. With these advantages, password-
based authentication is likely to stay as an important part of
security for the foreseeable future [14]. One major weakness
of password-based authentication is that many users tend to
choose weak passwords that are easy to guess. Addressing
this challenge has been an active and important research area.
A fundamental tool for password security research is that of
probabilistic password models (password models for short).
A password model assigns a probability value to each string.
The goal of such a model is to approximate as accurately
as possible an unknown password distribution D. We divide
password models into two classes, whole-string models and
template-based models. A template-based model divides a
password into several segments, often by grouping consecutive
characters of the same category (e.g., lower-case letters, digits,
etc.) into one segment, and then generates the probability for
each segment independently, e.g., [21], [25]. A whole-string
model, on the other hand, does not divide a password into
segments, e.g., the Markov chain model in [8].
Jerry Ma’s work on this paper was done while working as a research
assistant at Purdue University. Min Luo is with the Computer School of
Wuhan University in Hubei, China; his work on this paper was done while
being a visiting scholar at Purdue University. Weining Yang and Ninghui Li
are with the Department of Computer Science and the Center for Education
and Research in Information Assurance and Security (CERIAS) in Purdue
University, West Lafayette, Indiana, USA. The ﬁrst three authors are co-ﬁrst
authors.
We classify research involving password models into two
types. Type-1 research aims at understanding what makes
users choose more (or less) secure passwords. To do this, one
obtains password datasets chosen by users under difference
circumstances, and then uses a password model to compare the
relative strengths of these sets of passwords. Type-2 research
aims at ﬁnding the best password models. Such a model can
then be used to evaluate the level of security of a chosen
password, as in password strength meters. Such a model can
also be used to develop more effective password cracking
utilities, as a password model naturally determines the order
in which one should make guesses.
Type-1 research has been quite active in recent years. Re-
searchers have studied the quality of users’ password choices
under different scenarios [17], [18], [22], [23], [26]. In this
area, earlier work uses either standard password cracking tools
such as John the Ripper (JTR) [3], or ad hoc approaches
for estimating the information entropy of a set of passwords.
One such approach is NIST’s recommended scheme [7] for
estimating the entropy of one password, which is mainly based
on their length.
Weir et al. [25] developed a template-based password model
that uses Probabilistic Context-free Grammar. We use PCFGW
to denote this password model. In [24], they argued that en-
tropy estimation methods such as that recommended by NIST
are inaccurate metrics for password strength, and proposed
to use the guess numbers of passwords. The guess number
of a password according to a password model is deﬁned
to be the rank of the password in the order of decreasing
probability. To compare two sets of passwords, one plots the
number of guesses vs. the percentage of passwords cracked
by the corresponding number of guesses in the testing dataset.
Such guess-number graphs are currently the standard tool in
password research. Plotting such graphs, however, requires
the computation of guess numbers of passwords, which is
computationally expensive. For many password models, this
requires generating all passwords with probability above a cer-
tain threshold and sorting them. Some template-based models,
such as the PCFGW model, have the property that all strings
ﬁtting a template are assigned the same probability. One is thus
able to compute guess numbers by maintaining information
about templates instead of individual passwords. This is done
in the guess calculator framework in [17], [20], which is
based on the PCFGW model. This framework uses parallel
computation to speed up the process, and is able to go up to
≈ 4E14 [20]. Even with this approach, however, one is often
© 2014, Jerry Ma. Under license to IEEE.
DOI 10.1109/SP.2014.50
689
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
limited by the search space so that only a portion (typically
between 30% and 60%) of the passwords are covered [17],
[20]. Such studies thus miss information regarding the stronger
passwords.
We observe that for type-1 research, in which one compares
the relative strength of two sets of passwords, it is actually
unnecessary to compute the guess numbers of all passwords.
It sufﬁces to compute the probabilities of all passwords in the
datasets, which can be done much faster, since the number
of passwords one wants to evaluate (often much less than
1E6) is in general extremely small compared to the maximum
guess number one is interested in. We propose to plot the
probability threshold vs. the percentage of passwords cracked
curves, which we call probability-threshold graphs. In addition
to being much faster to compute, another advantage is that
such a curve shows the quality of passwords in the set all the
way to the point when all passwords are cracked (assuming
that the password model assigns a non-zero probability to each
allowed password).
A natural approach for password models is to use whole-
string Markov chains. Markov chains are used in the template-
based model in [21], for assigning probabilities to segments
that consists of letters. Castelluccia et al. [8] proposed to
use whole-string Markov models for evaluating password
strengths, without comparing these models with other models.
At the time of this paper’s writing, PCFGW is still considered
to be the model of choice in type-1 password research.
We ﬁnd this current state of art unsatisfying. First, only
very simple Markov models have been considered. Markov
models are known as n-gram models in the statistical lan-
guage literature, and there exist a large number of techniques
developed to improve the performance of such models. In
particular, many smoothing techniques were developed to help
solve the sparsity and overﬁtting problem in higher-order
Markov models. Such techniques include Laplace smoothing,
Good-Turing smoothing [13], and backoff [16]. Password
modeling research has not taken advantage of such knowl-
edge and techniques. Second, passwords differ from statistical
language modeling in that passwords have a very deﬁnite
length distributions: most passwords are between lengths 6
and 12, and there are very few short and long passwords.
The n-gram models used in statistical
language modeling
generally have the property that the probabilities assigned to
all strings of a ﬁxed length add up to 1, implicitly assuming
that the length distribution is uniform. This is ﬁne for natural
language applications, because often times one only needs
to compare probabilities of sentences of the same (or very
similar) length(s), e.g., when determining which sentence is
the most likely one when given a sequence of sounds in
speech recognition. For password models, however, assuming
uniform length distribution is suboptimal. Finally, different
password modeling approaches have not been systematically
evaluated or compared against each other. In particular, a
rigorous comparison of the PCFGW model with whole-string
Markov models has not been performed.
In this paper, we conduct an extensive empirical study of
different password models using 6 real-world plaintext pass-
word datasets totaling about 60 million passwords, including
3 from American websites and 3 from Chinese websites.
We consider three different normalization approaches: direct,
which assumes that strings of all lengths are equally likely,
distribution-based, which uses the length distribution in the
training dataset, and end-symbol based, which appends an
“end” symbol
to each password for training and testing.
We consider Markov chain models of different orders, and
with different smoothing techniques. We compare whole-string
models with different instantiation of template-based models.
Some of the important ﬁndings are as follows. First and
foremost, whole-string Markov models, when done correctly,
signiﬁcantly outperform the PCFGW model [25] when one
goes beyond the ﬁrst million or so guesses. PCFGW uses
as input both a training dataset, from which it learns the
distribution of different templates, and a dictionary from which
it chooses words to instantiate segments consisting of letters.
In this paper, we considered 3 instantiations of PCFGW : the
ﬁrst uses the dictionary used in [25]; the second uses the
OpenWall dictionary; and the third generates the dictionary
from the training set. We have found that the third approach
works signiﬁcantly better than the ﬁrst and the second; in
addition, all three instantiations signiﬁcantly underperform the
best whole-string Markov models. Furthermore, higher orders
of Markov chains show different degrees of overﬁtting effect.
That is, they perform well for cracking the high-probability
passwords, but less so later on. The backoff approach, which
essentially uses a variable-order Markov chain, when com-
bined with end-symbol based normalization, suffers little from
the overﬁtting effect and performs the best.
In summary, the contributions of this paper are as follows:
• We introduce the methodology of using probability-
threshold graphs for password research, which has clear
advantages over the current standard approach of using
guess-number graphs for type-1 password research. They
are much faster to construct and also gives information
about the passwords that are difﬁcult to crack. In our
experiments, it took about 24 hours to generate 1010
passwords for plotting guess-number graphs; which cover
between 30% and 70% in the dataset. On the other hand,
it took less than 15 minutes to compute probabilities for
all passwords in a size 107
testing dataset, giving strength
information of all passwords in the dataset. We note,
however, that for type-2 research, in which one compares
different password models, one needs to be careful to
interpret results from probability-threshold graphs and
should use guess-number graphs to corroborate these
results.
• We introduce knowledge and techniques from the rich
literature in n-gram models for statistical language mod-
eling into password modeling, as well as identifying
new issues that arise from modeling passwords. We also
identify a broad design space for password models.
• We conduct a systematic evaluation of many password
models, and made a number of ﬁndings. In particular, we
690
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
show that the PCFGW model, which has been assumed
to be the state of the art and is widely used in research,
does not perform as well as whole-string Markov models.
The rest of this paper is organized as follows. We introduce
probability-threshold graphs in Section II, and explore the de-
sign space of password models in III. Evaluation methodology
and experimental results are presented in Sections IV and V,
respectively. We then discuss related work in Section VI and
conclude in Section VII.
II. PASSWORD MODELS AND METRICS
We use Σ to denote the alphabet of characters that can be
used in passwords. We further assume that all passwords are
required to be of length between L and U for some values of
L and U ; thus the set of allowed passwords is
Γ =
U(cid:2)
(cid:2)=L
(cid:2).
Σ
In the experiments in this paper, we set Σ to include the
95 printable ASCII characters, and L = 4 and U = 40.
Sometimes the alphabet Σ is divided into several subsets,
which we call
the character categories. For example, one
common approach is to divide the 95 characters into four
categories: lower-case letters, upper-case letters, digits, and
special symbols.
passwords that have the highest probabilities). This approach is
logically appealing; however, it is also highly computationally
expensive, since it requires generating a very large number
of password guesses in decreasing probability. This makes it
difﬁcult when we want to compare many different approaches.
Furthermore, because of the resource limitation, we are unable
to see the effect beyond the guesses we have generated. These
motivate us to use the following metrics.
Probability-threshold graphs. Such a graph plots the prob-
ability threshold in log scale vs. the percentage of passwords
above the threshold. A point (x, y) on a curve means that y
percent of passwords in the dataset have probability at least
2−x
. To generate such a ﬁgure, one needs only to compute the
probabilities the model assigns to each password in the testing
dataset. For each probability threshold, one counts how many
passwords are assigned probabilities above the threshold. For
a complete password model, such a curve can show the effect
of a password model on the dataset all the way to the point
when all passwords in the dataset are included.
Figure 1 below shows a probability-threshold graph and the
corresponding guess number graph, for comparing the relative
strength of two datasets phpbb and yahoo using a Markov
chain model of order 5 trained on the rockyou dataset.
Deﬁnition 1: A password probability model (password
model for short) is given by a function p that assigns a
probability to each allowed password. That is, a password
model p : Γ → [0, 1] satisﬁes
∀s∈Γ p(s) ≥ 0
(cid:3) (cid:4)
p(s) = 1.
s∈Γ
We say that a password model p is complete if and only if
it assigns a non-zero probability to any string in Γ.
For a password model p to be useful in practice, it should be
efﬁciently computable; that is, for any s ∈ Γ, computing p(s)
takes time proportional to O(|Σ|·length(s)). For p to be useful
in cracking a password, it should be efﬁciently enumerable;
that is, for any integer N , it runs in time proportional to
O(N · |Σ| · U ) to output the N passwords that have the highest
probabilities according to p. If one desires to make a large
number of guesses, then the password generation also needs
to be space efﬁcient, i.e., the space used for generating N
passwords should grow at most sub-linearly in N . Ideally, the
space usage (not counting the generated passwords) should be
independent from N .
We consider the following three methods/metrics when run-
ning a given password model with a testing password dataset.
Note that for type-1 research, we ﬁx the model and compare
different datasets. For type-2 research, we ﬁx a dataset and
compare different models.
Guess-number graphs. Such a graph plots the number of
guesses in log scale vs. the percentage of passwords cracked
in the dataset. A point (x, y) on a curve means that y percent
of passwords are included in the ﬁrst 2x
guesses (i.e., the 2x
(a) Probability-threshold graph
(b) Guess-number graph
Fig. 1: An example
An interesting question is how a probability-threshold graph
relates to the corresponding guess-number graph. If one draws
conclusions regarding two curves based on the former, do the
conclusions hold in the latter?
When comparing the strength of two password datasets
using a ﬁxed model, the answer is “yes”. The relationship of
two guess-number curves (e.g., which curve is to the left of
the other, whether and when they cross, etc.) would be exactly
the same as that demonstrated by the beginning portions of
the corresponding probability-threshold curves; because for a
given threshold, exactly the same set of passwords will be
attempted. This effect can be observed from Figure 1. The only
information missing is that we do not know the exact guess
number corresponding to each probability threshold; however,
this information is not needed when our goal is to compare
two datasets. In addition to being much faster to compute,
the probability-threshold graph is able to show the quality
of passwords in the set all the way to the point when all
passwords are cracked (assuming that the password model
assigns a non-zero probability to each allowed password).
691
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
When comparing models with a ﬁxed dataset, in general,
the answer is no. In the extreme case, consider a model that
assigns almost the same yet slightly different probabilities to
every possible password in Γ, while using the same ranking
as in the testing dataset. As a result, the probabilities assigned
to all passwords would all be very small. Such an unreal-
istic model would perform very well in the guess-number
graph; however, the probability-threshold graph would show
extremely poor performance, since no password is cracked
until a very low probability threshold is reached.
When comparing two password models, whether the con-
clusions drawn from a probability threshold graph can be
carried over to the guess number graph depends on whether
the two models have similar rank distributions. Given a
password model p, the rank distribution gives for each i ∈
{1, 2, 3, · · · }, the probability of the password that has the i’th
highest probability. Thus, if two password models are known to
generate very similar rank distributions, then any conclusion
drawn from the probability-threshold graph will also hold
on the guess number graph, because the same probability
threshold will correspond to the same guess number. When
two models generate somewhat different rank distributions,
then one needs to be careful when interpreting results obtained
from probability-threshold graphs. In particular, one may want
to also generate the guess number graphs to check whether
the results are similar at least for the beginning portion of the
probability-threshold graphs. There is the main limitation of
using probability-threshold graphs in type-2 research.
Average-Negative-Log-Likelihood (ANLL) and ANLLθ.
Information encoded in a probability-threshold curve can be
summarized by a single number that measures the area to the
left of the curve, which turns out to be exactly the same as the
Average Negative Log Likelihood (ANLL). ANLL has its root
in statistical language modeling. A statistical language model
assigns a probability to a sentence (i.e., a sequence of words)
by means of a probability distribution. Language modeling is
used in many natural language processing applications such
as speech recognition, machine translation, part-of-speech
tagging, parsing and information retrieval. Password modeling
can be similarly viewed as trying to approximate as accurately
as possible D, an unknown distribution of passwords. We do
not know D, and instead have a testing dataset D, which
can be viewed as sampled from D. We use pD to denote the
probability distribution given by the dataset D. Representing
the testing dataset D as a multi-set {s1, s2, . . . , sn}, where a
password may appear more than once, the ANLL metric is
computed as follows:
these areas up, one obtains the ANLL.
ANLL gives the same information as Empirical Perplexity,
which is the most widely used metric for evaluating statistical
language models, and is deﬁned as 2ANLL(D|p)
.
While using a single number to summarize the information
in a probability-threshold curve is attractive, obviously one
is going to lose some information. As a result, ANLL has
some limitations. First, ANLL is applicable only for complete
password models, i.e., those that assign a non-zero probability
to each password. In the area interpretation, if a password
in the dataset is assigned probability 0, it is never guessed,
and the curve never reaches Y = 1, resulting in an inﬁnite
area. Second, ANLL includes information about cracking 100
percent of the passwords, which may not be what one wants.
Since it may be infeasible to generate enough guesses to crack
the most difﬁcult passwords, one may care about only the θ
portion of passwords that are easy to crack. In that case, one
could use ANLLθ, which we deﬁne to be the area to the left
of the curve below θ. In this paper, we use ANLL0.8 when
comparing different models using one single number.
III. DESIGN SPACE OF PASSWORD MODELS
We mostly consider approaches that construct password
models without relying on an external dictionary. That is,
we consider approaches that can take a training password
dataset and learn a password model from it. The performance
of approaches that rely on external dictionaries depends very
much on the quality of the dictionaries, and in particular,
how well
the dictionaries match the testing dataset. Such
approaches are not broadly applicable, and it is difﬁcult to
evaluate their effectiveness.
N -gram models,
i.e., Markov chains, are the dominant
approach for statistical languages. It is natural to apply them
to passwords. A Markov chain of order d, where d is a positive
integer, is a process that satisﬁes
P (xi|xi−1, xi−2, . . . , x1) = P (xi|xi−1, . . . , xi−d)
where d is ﬁnite and x1, x2, x3, . . . is a sequence of random
variables. A Markov chain with order d corresponds to an
n-gram model with n = d + 1.
We divide password models into two classes, whole-string