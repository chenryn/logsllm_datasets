title:ZK-GanDef: A GAN Based Zero Knowledge Adversarial Training Defense
for Neural Networks
author:Guanxiong Liu and
Issa Khalil and
Abdallah Khreishah
ZK-GanDef: A GAN based Zero Knowledge
Adversarial Training Defense for Neural Networks
Guanxiong Liu
ECE Department
Issa Khalil
QCRI
Abdallah Khreishah
ECE Department
New Jersey Institute of Technology
Hamad bin Khalifa University
New Jersey Institute of Technology
Newark, USA
PI:EMAIL
Doha, Qatar
PI:EMAIL
Newark, USA
PI:EMAIL
9
1
0
2
r
p
A
7
1
]
G
L
.
s
c
[
1
v
6
1
5
8
0
.
4
0
9
1
:
v
i
X
r
a
Abstract—Neural Network classiﬁers have been used success-
fully in a wide range of applications. However, their underlying
assumption of attack free environment has been deﬁed by
adversarial examples. Researchers tried to develop defenses;
however, existing approaches are still far from providing ef-
fective solutions to this evolving problem. In this paper, we
design a generative adversarial net (GAN) based zero knowledge
adversarial training defense, dubbed ZK-GanDef, which does
not consume adversarial examples during training. Therefore,
ZK-GanDef is not only efﬁcient in training but also adaptive
to new adversarial examples. This advantage comes at
the
cost of small degradation in test accuracy compared to full
knowledge approaches. Our experiments show that ZK-GanDef
enhances test accuracy on adversarial examples by up-to 49.17%
compared to zero knowledge approaches. More importantly, its
test accuracy is close to that of the state-of-the-art full knowledge
approaches (maximum degradation of 8.46%), while taking much
less training time.
Index Terms—Adversarial Training Defense, Generative Ad-
versarial Nets, full knowledge training, zero knowledge training
I. INTRODUCTION
Due to the surprisingly good representation power of com-
plex distributions, neural network (NN) classiﬁers are widely
used in many tasks which include natural language processing,
computer vision and cyber security. For example, in cyber
security, NN classiﬁers are used for spam ﬁltering, phishing
detection as well as face recognition [21] [1]. However, the
training and usage of NN classiﬁers are based on an underlying
assumption that the environment is attack free. Therefore, such
classiﬁers fail when adversarial examples are presented to
them.
Adversarial examples were ﬁrst
introduced in 2013 by
Szegedy et. al [24] in the context of image classiﬁcation. They
show that adding specially designed perturbations to original
images could effectively mislead fully trained NN classiﬁer.
For example, in Figure 1, the adversarial perturbations added
to the image of panda are visually insigniﬁcant to human eyes,
but are strong enough to mislead the classiﬁer to classify it as
gibbon. Yet, more scary, the research shows that adversary
could arbitrarily control the output class through carefully
This work is submitted to DSN 2019 as a regular paper. The ofﬁcial
implementation is open-source on Github. https://github.com/GuanxiongLiu/
DSN-ZK-GanDef.git
Fig. 1: Fast Gradient Sign Example [6]
designed perturbations and can achieve high success rate
against Vanilla classiﬁers, i.e., classiﬁers without defenses [4]
[9] [14].
Current defensive mechanisms against adversarial examples
can be categorized into three different groups [15] [19]. The
approaches of the ﬁrst group utilize augmentation and regular-
ization to enhance test accuracy on adversarial examples. The
idea here is to improve the generalization of the model as a
defense against adversarial examples [19]. Approaches in the
second group try to build protective shells around the classiﬁer
to either identify adversarial examples and ﬁlter them out,
or reform perturbations and rollback to original images [15]
[22]. The approaches in the last group retrain NN classiﬁers
with adversarial examples to recognize and correctly classify
perturbed inputs [9]. The intuition here is that by observing
some adversarial examples with their ground truth, the NN
classiﬁers learn the patterns of adversarial perturbations and
adapt to recognize similar ones.
Current defenses enhance test accuracy of existing NN
models on adversarial examples and help us better under-
stand the problem. However, such defenses are still far from
wining the battle against this continuously evolving problem.
Among different defenses, adversarial training with iterative
adversarial examples is shown to be the state-of-the-art choice
[2]. However, such defense requires too much computation
to generate iterative adversarial examples during training.
Based on [7], the adversarial training with iterative adversarial
examples requires a cluster of GPU servers on Imagenet
dataset. Although there are defense methods that do not rely
on adversarial training [20] [28], these methods only address
weaker attack scenarios (black-box and gray-box).
With these limitations on existing defenses, researchers
start a new line of research on zero knowledge adversarial
training which is independent of adversarial examples. The
idea here is to replace adversarial examples with random
noise perturbations while retraining of NN classiﬁers [7]. The
intuition is to trade small decrease in accuracy for better
scalability, efﬁciency and quick adaptability. In this work,
the adversarial training approaches which utilize adversarial
examples are denoted as full knowledge adversarial training,
in contrast with zero knowledge ones.
As we show in our evaluation, existing zero knowledge
adversarial training approaches, clean logit pairing (CLP) and
clean logit squeezing (CLS) [7], suffer from poor prediction
accuracy. In this work, we propose a GAN based zero knowl-
edge adversarial training defense, dubbed ZK-GanDef. ZK-
GanDef is designed based on adversarial training approach
combined with feature learning [13] [27] [11]. It forms a com-
petition game of two NNs: a classiﬁer and a discriminator. We
show analytically that the solution of this competition game
generates a classiﬁer which usually makes right predictions
and only relies on perturbation invariant features. We conduct
extensive set of experiments to evaluate the performance and
the prediction accuracy of ZK-GanDef on MNIST, Fashion-
MNIST and CIFAR10 datasets. Compared to CLP and CLS,
the results show that ZK-GanDef has the highest test accuracy
in classifying different white-box adversarial examples with
signiﬁcant superiority.
Our contributions can be summarized as follows:
• We design a GAN based zero knowledge adversarial
training defense, ZK-GanDef, which utilizes feature se-
lection to enhance test accuracy on adversarial examples.
• We provide a mathematical intuition for the competition
game used in ZK-GanDef that its trained classiﬁer usually
makes right predictions based on perturbation invariant
features.
• We empirically show that ZK-GanDef signiﬁcantly en-
hances the test accuracy on adversarial examples over
state-of-the-art zero knowledge adversarial training de-
fenses.
• Existing work only tests CLP and CLS on small datasets
like MNIST. In this work, we empirically show that CLP
and CLS do not scale well to complex datasets such
as CIFAR10. In contrast, we show that ZK-GanDef can
defend adversarial examples in such complex datasets.
• We empirically show that ZK-GanDef can achieve com-
parable test accuracy to the state-of-the-art full knowledge
defenses. At the same time, it signiﬁcantly reduces the
training time compared to full knowledge defenses. For
example, its training time is 92.11% less than that of
PGD-Adv on MNIST dataset.
The rest of the work is organized as follows. Section II
presents background material. The design and mathematical
proof of ZK-GanDef are given in Section III. Section IV
presents the test-bed design and the experimental settings.
The evaluation results are shown in Section V. Section VI
concludes the paper and Section VII presents our future
direction.
2
II. BACKGROUND AND RELATED WORK
In this section, we introduce background material about
adversarial example generators and defensive mechanisms for
better understanding of the concepts presented in this work.
We also provide relevant references for further information
about each topic.
A. Generating Adversarial Examples
The methods for generating adversarial examples against
NN classiﬁers can be categorized according to several different
aspects. In one aspect, these methods could be distinguished
by the adversary’s knowledge of the target NN classiﬁer. In
White-box methods, the adversary is assumed to have full
knowledge about the target NN classiﬁer (structure, parameters
and inner status), and hence the generated examples are called
white-box adversarial examples. On the other hand, black-
box methods assume that the adversary has no access to the
inner information of the target NN classiﬁer, and hence, the
generated examples are called black-box adversarial examples.
On another aspect, adversarial example generation methods
could be categorized into single-step or iterative methods
according to the process of generating the examples. Single-
step methods only run gradient descent (ascent) algorithm
once when solving the proposed optimization problem, while
iterative methods repeat the computation several times until
hitting predeﬁned convergence thresholds.
Based on previous works, an adversarial example generator
is generally formulated as an optimization problem which
searches a small neighboring area of the original
image
(usually deﬁned by l1, l2 or l∞ norm) for the existence of
adversarial examples. If we denote an original image by ¯x and
the example with perturbation δ by ˆx = F(¯x + δ), then the
process of searching adversarial examples can be formulated
as follows [6]:
||ˆx − ¯x||
minimize
subject to C(ˆx) = zo
δ
F(¯x + δ) ∈ Rm
[−1,1]
The function C represents the classiﬁer and outputs the pre-
softmax logits based on input image. The function F projects
the pixel value of any input image back to R[−1,1] and ensures
that the generated adversarial example is still a valid image. A
perturbation is considered strong enough to fool the classiﬁer
if and only if C(ˆx) = zo, where zo is the objective pre-
softmax logits designed by adversary. The global optimum of
this problem corresponds to the strongest adversarial example
for a given image. However, modern classiﬁers are highly non-
linear, which makes it hard to solve the optimization problem
in its original form, and hence each generator has its own
approximation to make the optimization problem solvable.
Table I summarizes all the notations that we use throughout
this paper. In the following, we describe the design approaches
of several popular adversarial example generators that we
consider in this work.
L, LCLP, LCLS
F
loss function of NN classiﬁer
regulation function for pixel value of generated
example
objective pre-softmax logits designed by adversary
the 1st order, 2nd order and inﬁnity order norm
zo
l1, l2, l∞
¯x, ¯X; ˆx, ˆX; x, X original example; example with perturbation; their
¯t, ¯T ; ˆt, ˆT ; t, T
¯z, ¯Z; ˆz, ˆZ; z, Z
¯s, ¯S; ˆs, ˆS; s, S
δ
C, C∗
D, D∗
J, J(cid:48)
Ω, ΩC, ΩD
λ, γ