We had a quite serious case today which caused all our pods to terminate
because AWS had trouble with their EC2 service in Ireland (6:00 AM PDT)
Basically after 40s (corresponding to `node-monitor-grace-period`) of API
downtime during which kubelet couldn't list EC2 instances, it hit the maximum
node status retry count and killed everything:
    12:44:22 kubelet.go:1933] Error updating node status, will retry: failed to get node address from cloud provider: error listing AWS instances: Unavailable: The service is unavailable. Please try again shortly.
    status code: 503, request id: []
    ...
    12:45:04 kubelet.go:839] Unable to update node status: update node status exceeds retry count
    12:45:10 kubelet.go:1518] Killing unwanted pod "swagger-extcs"
    12:45:10 manager.go:1123] Killing container with id "aa4a55984..."
    12:45:10 manager.go:1123] Killing container with id "9bdb1788e..."
    12:45:10 kubelet.go:1343] Orphaned volume "5796b5eb..." found, tearing down volume
    ...
I could certainly assume that downtimes at AWS may never last more than `t`
minutes and increase the `node-monitor-grace-period` and `pod-eviction-
timeout` in my kube-controller-manager accordingly, but there is something
dangerous in making such assumptions and relying _only_ on the cloud provider
to define nodes' health, since a not responding cloud provider doesn't mean a
completely unhealthy platform.
We paid that price today and saw our whole prod cluster going down while
everything was fine on the instances themselves ðŸ™ˆ