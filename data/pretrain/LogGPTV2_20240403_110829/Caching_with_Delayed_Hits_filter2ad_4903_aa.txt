title:Caching with Delayed Hits
author:Nirav Atre and
Justine Sherry and
Weina Wang and
Daniel S. Berger
Nirav Atre
Carnegie Mellon University
ABSTRACT
Caches are at the heart of latency-sensitive systems. In this paper,
we identify a growing challenge for the design of latency-minimizing
caches called delayed hits. Delayed hits occur at high throughput, when
multiple requests to the same object queue up before an outstanding
cache miss is resolved. This effect increases latencies beyond the predic-
tions of traditional caching models and simulations; in fact, caching
algorithms are designed as if delayed hits simply didn’t exist. We show
that traditional caching strategies – even so called ‘optimal’ algorithms
– can fail to minimize latency in the presence of delayed hits. We design a
new, latency-optimal offline caching algorithm called belatedly which
reduces average latencies by up to 45% compared to the traditional,
hit-rate optimal Belady’s algorithm. Using belatedly as our guide,
we show that incorporating an object’s ‘aggregate delay’ into online
caching heuristics can improve latencies for practical caching systems
by up to 40%. We implement a prototype, Minimum-AggregateDelay
(mad), within a CDN caching node. Using a CDN production trace and
backends deployed in different geographic locations, we show that mad
can reduce latencies by 12-18% depending on the backend RTTs.
CCS CONCEPTS
• Networks; • Theory of computation → Caching and paging
algorithms;
KEYWORDS
Caching, Delayed hits, Belatedly
ACM Reference Format:
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger. 2020. Caching
with Delayed Hits. In Annual conference of the ACM Special Interest Group
on Data Communication on the applications, technologies, architectures, and
protocols for computer communication (SIGCOMM ’20), August 10–14, 2020,
Virtual Event, NY, USA. ACM, New York, NY, USA, 19 pages. https://doi.org/
10.1145/3387514.3405883
1 INTRODUCTION
Caches are key components of the computer systems toolkit: they
reduce bandwidth consumption to a bottlenecked backing store,
they improve throughput for memory-intensive services, and they
reduce read delays for latency-sensitive applications. Consequently,
caches appear across seemingly every class of computer system:
e.g., in microprocessors [27], in distributed file systems [51], in CDN
proxies [12, 21], and in software switches [47].
In this paper, we focus on a surprisingly overlooked aspect of
caching and latency. Caching models and simulators assume that
there are exactly two possible outcomes when an object is requested:
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7955-7/20/08.
https://doi.org/10.1145/3387514.3405883
Caching with Delayed Hits
Justine Sherry
Weina Wang
Carnegie Mellon University
Carnegie Mellon University
Daniel S. Berger
Microsoft Research
a low-latency ‘hit’, or a higher latency ‘miss.’ In reality, there is a
third potential outcome: a delayed hit [25, 56]. Delayed hits occur
in high-throughput systems when multiple requests for the same
object occur before the object is fetched from the backing store.
Our group first encountered delayed hits on an FPGA-based soft-
ware switch, with incoming packets triggering access to a flow
context stored in either an SRAM-based cache (5 ns reads), or a
DRAM-based global backing store (100 ns reads). When a flow’s
packet results in a cache miss, it triggers the 100 ns fetch operation.
At high throughput, a second packet of the same flow arrives before
100 ns have passed. This packet requests the same object, and waits
for it to return from the fetch initiated by the first packet. While the
second packet does not have to wait the full 100 ns for the object to
arrive, it also does not experience a 5 ns ‘hit’ either. Per traditional
caching literature, the request corresponding to the second packet
would be counted as a hit. In reality, this second packet experiences
a slower, ‘delayed hit’.
We demonstrate throughout this paper that the traditional caching
objective of hit-rate maximization and the related goal of latency min-
imization are not equivalent problems when some hits are delayed.
We argue that therefore we need new algorithms for latency-sensitive
caching systems.
One way to understand fundamental trade-offs in caching design
is by studying an offline-optimal algorithm. The classic such algo-
rithm is called Belady’s algorithm [7]. Unlike real caching systems,
offline-optimal algorithms assume an oracle with perfect knowl-
edge of future requests. Offline algorithms can provide guidance and
bounds for practical algorithms, e.g., if the offline-optimal algorithm
achieves a k% hit-rate, then any online algorithm will achieve at most
k%. In the past, understanding which objects an offline algorithm
chooses to cache or evict has often guided the design of practical
systems [9, 29, 55]. Our approach to understanding delayed hits sim-
ilarly uses lessons from the offline setting to guide our design of a
practical online system.
Limitations of existing algorithms: We begin in §2 by showing
that Belady’s algorithm, the optimal offline approach for hit-rate
maximization, does not guarantee minimal latency in the presence
of delayed hits. We then measure the gap between hit-rate and
latency-orientedregimesoncachetracesincludinga10Gbpslinkand
alatency-sensitiveCDN.Wefindthatlatencyevaluationsofpractical
caching algorithms (e.g., LRU [64]) based on hit-rate alone underesti-
mate true latencies by 14-63% in switch caches and 22-36% in CDNs.
Optimal, latency-minimal caching: Having demonstrated that
existing caching algorithms fail to minimize latency, we turn to the
design of new algorithms that are aware of delayed hits. In §3, we de-
sign a new offline caching algorithm, belatedly,1 which computes
empirically tight bounds on the minimum latency in polynomial
time. Using belatedly, we quantify the gap between Belady’s al-
gorithm – and thus the hit-rate maximization strategy – and true
latency-optimality. We find that Belady’s latencies are 0.1-38% worse
1Available at https://github.com/cmu-snap/Delayed-Hits
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger
than belatedly’s latency upper-bounds for packet switches and
1.8-17% worse than belatedly for CDNs.
Low-latency online caching: We use belatedly as our guide
for the design of a practical online caching strategy, Minimum-
AggregateDelay (mad), in §4. Specifically, we derive a simple ranking
function from belatedly by modeling an object’s future ‘aggregate
delay’. This ranking function empirically approximates belatedly’s
decisions. We then design a practical aggregate delay heuristic which
can be used to make traditional caching algorithms aware of delayed
hits. We implement a prototype of mad within a CDN caching node.
In experiments with backends deployed in the US, Europe, and East
Asia, we observe average latency reductions of 12% to 18% with a
memory overhead of under 3%. We use simulations to explore a
wider range of scenarios and find that mad improves latencies over
traditional algorithms by 15-43% for packet switches, 10-60% for
CDNs, and 5-40% for distributed storage systems. Most strikingly,
for caches with extremely high latencies to the backing store, mad
can provide better average latencies than the latency provided by
Belady’s algorithm.
Why now? Why hasn’t anyone noticed before that delayed hits
play an important role in cache latencies? Delayed hits are noted in
passing in several places in the literature [25, 56], and anyone who
has ever implemented a cache has had to consider delayed hits as
well [1, 8, 35, 43, 53, 58].
Weconjecturethattheproblemhasonlyrecentlybecomepercepti-
ble from a performance perspective due to an evolving ratio between
system throughputs and latencies. If throughput is low relative to la-
tency, it may only be possible for 1-2 requests to arrive during a fetch.
However,ifthroughputishigherrelativetolatency,onewouldexpect
more requests during a fetch. We refer to the ratio between the object
fetch time and mean request inter-arrival time as Z, and we show in
§3.2 that as Z grows, the gap between Belady and belatedly widens.
Inrecentyears, Z hasgrownacrossawiderangeofsystems.Forex-
ample, DRAM latencies are only marginally improving, while newer
memory technologies (e.g. High Bandwidth Memory, or HBM) boast
order-of-magnitude improvements in bandwidth over current DDR
standards [37]. Similarly, the latency between a CDN forward proxy
and a central data center is defined by wide-area latencies; mean-
while, throughputs are rapidly growing, e.g., with network links
moving from 10Gbps to 100Gbps and 400Gbps [24]. The fundamen-
tal problem is that latencies are edging marginally closer and closer
to limits imposed by the speed of light, while throughputs keep
growing unhindered. Hence, we believe that the impact of delayed
hits on latency-minimizing caching systems will grow with time.
2 THE PROBLEM WITH DELAYED HITS
A basic delayed hit scenario is illustrated in Figure 1. When a
request arrives for an object and the object is not stored in the cache
1○, the cache triggers a request to retrieve this object from a back-
ing store 2○. The retrieval takes some non-zero amount of time, L
seconds, and the average inter-request arrival time is R seconds.
For simplicity we say that R seconds is one timestep, and that the
amount of time to fetch the object is Z = L
timesteps. After the object
R
is requested, but before Z timesteps have occurred, a new request
arrives 3○. This request must wait some non-zero, but < Z amount
of time for the object to arrive as well 4○.
Figure 1: Two requests for object X arrive within Z timesteps of
each other. The first request results in a miss, the second request is
a ‘delayed hit.’
Figure 2: An example CDF of request latencies. Delayed hits account
for the gap between the true hit-rate (HR) and the miss-rate (MR).
To concretize this notion, consider a cache where Z = 10. At
timestep T = 3, a request for object A arrives, resulting in a cache
miss; this triggers a fetch to the backing store for object A which will
complete at time T =13, hence the first request will be served after
a total latency of 10 timesteps. If additional requests for A arrive at
T = 5 and T = 11, then they too will complete at T = 13, and will
experience latencies of 8 and 2, respectively.2
Traditional caching models ignore the contribution of delayed
hits, which, as we show in the following sections, can be significant
in systems with high latency to the backing store. Figure 2 depicts the
physical interpretation of delayed hits, and the relationship between
the true hit-rate, the idealized hit-rate, and the miss-rate.
2.1 Classic Caching Algorithms
Delayed hits subvert expectations of traditional caching algo-
rithms when it comes to latency. A caching algorithm is an algorithm
to decide, given a cache and an incoming stream of object requests,
when and which objects to store in the cache, and when and which
objects to evict. The caching algorithm produces a cache schedule:
a series of decisions about admissions and evictions for a given
set of cache parameters and a given sequence of object requests. A
caching algorithm aims to meet a particular objective, e.g., maximiz-
ing hit-rate. Offline (‘optimal’) algorithms know of all requests in the
future, and can therefore generate a theoretically optimal schedule
with regard to the objective. Online (‘practical’) algorithms are aware
of past object requests, but not future requests.
Classical caching algorithms are designed with the objective of
maximizing hit-rate, treating ‘true’ hits and delayed hits as one cat-
egory [27, 54]. Measuring the hit-rate (HR) allows cache designers
to evaluate numerous properties one might wish to extract from
a caching algorithm. For example, if a cache is deployed to reduce
2Note that for the purposes of our modeling, we assume that processing time for each re-
questis0–thatis,assoonasthedataarrives,allrequestsareservedinstantly.Inmanysys-
tems this is not true, and each request must be processed serially, e.g., reading, modifying,
and writing updates to the cached object. Non-zero processing times therefore introduce
an additional queueing delay which further increases the latency due to delayed hits.
1A request arrives for object XatT=0, resulting in a cache missABCDThe cache sends a fetchto the backing store, whichtakes Z timesteps to returnXCacheBacking StoreXarrives in the cache atT=Z, and both requests areserved. The 2 requests seelatencies of Z and (Z-n),respectivelyX2324X1A second request forobject X arrives at T=n,before the fetch returnsHit LatencyMiss LatencyRequest Latency0.000.250.500.751.00CDFIdealizedHR (75%)MR (25%)True HR(40%)Delayed HitsIdealizedActualCaching with Delayed Hits
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Trace
CDN
IRT=1µs
Network
Use Case
Intra-datacenter proxy [63]
Forward proxy, nearby datacenter [34]
Forward proxy, remote datacenter [34]
Single cache line DRAM lookup [27]
Traversing a DRAM datastructure [27]
RDMA Access in GEM-switch [33]
IDS with reverse DNS lookups [46]
1MB SSD Disk Read [19]
Hard Disk Seek & Read [19]
Latency
1ms
10ms
200ms
100ns
500ns
5µs
200ms
50µs
3ms
Z
1K
10K
200K
<1
<1
2
67K
2
100
5K
IRT=3µs
Storage
IRT=30µs Cross Datacenter Filesystem Read [19, 62] 150ms
Table 2: Average inter-request times (IRT), typical latencies, and Z
values for a range of caching use cases.
Figure 4: Average latency estimates of Idealized (not accounting for
delayed hits) and Actual (accounting for delayed hits) versions of
two standard caching algorithms.
2.3 Delayed Hits and Practical Algorithms
In addition to leading Belady to sub-optimal caching schedules,
we also observe that delayed hits can mislead operators managing
caching systems in practice. In this section, we simulate four classes
of caching systems and observe that delayed hits can inflate latencies
beyond what operators might expect from analyzing hit rates; in
fact, delayed hits might even lead operators to choose the wrong
caching algorithm to deploy for minimal latency.
Experimental Setup: We implement a cache simulator1 which
models delayed hits for a range of caching algorithms listed in Ta-
ble 1. We rely on datasets from three classes of caching systems: a
large content distribution network [11], the CAIDA Equinix 10G
Packet dataset [59], and a networked file system at Microsoft [28].
For each trace, we simulate a set of caching scenarios, each with a
different backing store latency normalized to Z, the average number
of requests arriving during a single object fetch. To provide some
context for what Z values one might find in practical systems, we
describe a few examples in Table 2.
Delayed hits increase latencies in practice when Z is large. If
delayed hits happened infrequently, the gap between the predicted
latency derived from hit-rates (1) and true latencies would be mar-
ginal. But, in Figure 4, we show how the average latency reported by
a simulator that models delayed hits differs from one that does not. In
our simulations, we scale up the latency to the backing store; on the
X-axis we plot Z, the ratio of the backing store latency to average re-
quest inter-arrivals. We see that for both traces, as the latency to the
backing store increases (and hence Z), so does the difference between
the simulated latency with delayed hits and the predicted latency
assuming no delayed hits. Referring back to Table 2, this means that
Figure 3: For this trace for a cache of size 2 and a Z of 3, Belady’s
algorithm chooses a latency-suboptimal schedule.
bandwidth consumption to a backing store (e.g. a forward proxy in
a bandwidth-limited network), then the miss rate, MR = (1− HR)
is proportional to the bandwidth consumption on the path to the
backing store. Other caches are deployed to minimize latency. When