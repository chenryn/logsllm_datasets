tions return the aggregate value after adding noise per diﬀer-
ential privacy. Transformations return a new PINQueryable
object that can be further operated upon. They can amplify
the sensitivity of subsequent queries, so that aggregations
run with one value of  may deplete many multiples of 
from the privacy budget. PINQ ensures that any ampliﬁca-
tion is properly accounted. Importantly, the logic within a
transformation can act arbitrarily on the sensitive records.
The semantics of the transformations are similar to SQL,
with two major exceptions. First, the Join operation in
PINQ is not a standard equijoin, in which one record can
match an unbounded number of other records.
Instead,
records in both data set are grouped by the key they are
being joined on, so that the Join results in a list of pairs of
groups. This restricts each pair to have limited impact on
aggregates (that of a single record) despite being arbitrar-
ily large, but it does enable diﬀerential privacy guarantees
which would not otherwise exist.
A second diﬀerence is a Partition operation that can split
a single protected data set into multiple protected data sets,
using an arbitrary key selection function. This operation is
important because the privacy cost to the source data set is
the maximum of the costs to the multiple parts, rather than
their sum. We can, for example, partition packets based on
destination port, and conduct independent analyses on each
part while costing only the maximum.
As the discussion above illustrates, and will become clearer
later, the privacy cost of an analysis depends not only what
the analysis aims to output but also on how it is expressed.
PINQ is essentially a programming language, and the space
of analyses that can be expressed is limited mainly by the
analysts creativity. One of our contributions is to devise
privacy-eﬃcient ways of expressing network data analyses.
We will see many common tools and programming patterns
that we expect to be broadly useful, several of which we
explicitly factor out into a re-usable toolkit.
2.3 An Example
Suppose we want to count distinct hosts that send more
than 1024 bytes to port 80. This computation, which in-
volves grouping packets by source and restricting the result
based on what we see in each group, can be expressed as:1
packets = new PINQueryable(trace, epsilon);
packets.Where(pkt => pkt.dstPort = 80)
.GroupBy(pkt => pkt.srcIP)
.Where(grp => grp.Sum(pkt => pkt.len) > 1024)
.Count(epsilon_query);
The Packet type contains ﬁelds that we might expect, in-
cluding sensitive ﬁelds such as IP addresses and payloads.
The raw data lies in trace. The total privacy budget for the
trace is epsilon, and the amount to be spent on this query is
epsilon_query. The analyst can run multiple queries on the
data as long as the total privacy cost is less than epsilon.
The expressions of the form x => f(x) are anonymous func-
tions that apply f to x.
For one of our datasets (the Hotspot trace in §3), the cor-
rect, noise-free answer for this analysis is 120. In a particular
run with epsilon=0.1, we get an answer of 121. Diﬀerent
runs will yield diﬀerent answers. The expected error for this
analysis is ±10.
3. DIFFERENTIALLY-PRIVATE
NETWORK TRACE ANALYSIS
Our goal is to investigate if diﬀerential privacy can provide
an eﬀective basis for mediated trace analysis. If feasible, we
can enable rich yet safe data analysis, without requiring the
data owners to expose raw, anonymized, or sanitized data.
As a precursor to conducting analysis, however, the ana-
lysts need to know the format of the stored data. This can
be accomplished by having the data owners release format
speciﬁcations or release synthetic data on which an analysis
can be tested before submitting to the owner. A non-goal of
our work is investigating if new analyses can be developed in
a diﬀerentially private manner. This task, which is distinct
from conducting existing analyses (or their variants), may
require intimate access to raw data.
The strong and direct guarantees of diﬀerential privacy
are appealing but its utility for network data analysis is
1The code fragments in this paper are stylized C# code.
They will not compile or record outputs but are otherwise
almost identical to actual PINQ code.
125uncertain because of two issues. First, diﬀerential privacy
introduces noise, which may incapacitate certain sensitive
computations. Examples include arbitrary resolution CDFs
and fragile statistics like minimum and maximum. Second,
the analysis must (currently) be expressed in a restricted
high-level language. Networking analyses are not typically
constrained to such languages, and privacy aside it may be
challenging to express analyses in such languages. These
two constraints have interplay, in that the amount of noise
introduced depends on how the analysis is expressed. We
will see several cases where we must exchange ﬁdelity to the
original algorithm for a smaller amount of noise introduced.
The expressibility restriction could potentially be over-
come by the invention of new diﬀerentially-private primitive
computations. Although PINQ does contain mechanisms
for extending the platform, the extensions become part of
the trusted computing base. For this reason, we restrict our
study to the existing operations supported by PINQ, to see
how far we can go with just those operations. While we
are largely successful, our experience does point at a few
extensions that will be broadly useful.
To understand if diﬀerentially private network trace anal-
ysis is feasible, we consider a wide array of real analyses.
We investigate the extent to which each can be faithfully
expressed and its accuracy loss over real data.
Analyses
Table 2 shows the analyses that we consider
and summarizes our results (explained later). The analysis
selection process was informal and intended to maximize di-
versity with a manageable number. We made a list of analy-
ses that appear in recent networking literature and preferred
those with computations that are disparate from others al-
ready picked. While picking an analysis, we ignore any prior
expectations about whether it would be easy to conduct in
a diﬀerentially private manner.
There is no standard classiﬁcation of networking analyses
to let us judge if we have included an analysis from each
class. But based on our original list, we ﬁnd that set of
analyses can be classiﬁed as operating on the granularity of
packets, ﬂows, or graphs. As the table shows, our chosen
set includes multiple examples of each category. That we
can conduct these analyses in a diﬀerentially private manner
does not imply that we can conduct any analysis. But the
diversity of our selected analyses gives us conﬁdence that
if we can conduct these we can conduct a wide range of
network trace analyses.
In addition to being diverse, these analyses require access
to information that data owners typically consider sensitive.
For instance, worm ﬁngerprinting [27] (a packet-level anal-
ysis) requires raw packet payloads; stepping stone detec-
tion [33] (a ﬂow-level analysis) requires addresses and ports
in traﬃc ﬂows; and anomaly detection [13] (a graph-level
analysis) requires information on the amount of traﬃc at
individual links of an ISP and how it varies across time. Be-
cause of the sensitivity of such information, researchers ﬁnd
it diﬃcult today to conduct these and similar analyses on
real data.
If ﬁnding one data source for such analyses is
diﬃcult, ﬁnding multiple is almost impossible.
Datasets
The analysis accuracy depends on the nature
of the data. We thus use real network traces in our work.
Table 3 shows the datasets that we study in this paper and
the type and the number of records they contain. Diﬀerent
datasets are used by diﬀerent analyses. The size of each is
Record
Hotspot
IspTraﬃc 
IPscatter
#records
7.0M
15.7B
3.8M
Table 3: The datasets that we consider.
comparable to what its analysis typically operates on. We
also studied other datasets [4, 11] for several of the analyses
and obtained results similar to those presented below.
Hotspot is a tcpdump trace of packets that we collected on
the wired access link of a large hotspot. It contains complete
packets, including unaltered addresses and payloads.
IspTraﬃc is constructed from traﬃc at a large ISP (whose
identity we are required to keep conﬁdential). The ISP
has over 400 links and it provided us highly aggregated
information on traﬃc volume at each link in each 15-min
window over a week-long period. We mimic a ﬁne-grained
dataset using this information by de-aggregating traﬃc vol-
ume into 1500-bytes packets that are spread evenly across
the time window. Note that the aggregate representation of
the source data is not itself a basis for diﬀerential privacy;
the presence or absence of individual packets can still be
observed in the precise aggregates.
IPscatter is a list of IP addresses and their TTL-distances
from 38 monitors.
It was constructed using the data col-
lected by Spring et al. [28], who conducted traceroute probes
from 38 PlanetLab sites to an IP address inside each BGP
preﬁx. The constructed dataset includes a record for each
IP seen along each probe.
Privacy level
The accuracy of a diﬀerentially pri-
vate analysis depends on the desired strength of the pri-
vacy guarantee (parameter ). We consider three diﬀerent
values of —0.1, 1.0, and 10.0—that correspond roughly to
high, medium, and low privacy levels. Recall that higher
values are not necessarily unsafe but are theoretically easier
to break.
Privacy principal
The guarantees of diﬀerential pri-
vacy are for the records of the underlying data set. These
records may or may not directly correspond to the higher-
level privacy principal that the data owner wants to protect.
Network data is interesting in that there are multiple pos-
sible privacy principals such as packets, ﬂows, hosts, and
services.
If the underlying records are ﬁner-grained than
the intended principal (e.g., packets vs. hosts), no explicit
guarantees are given for the principal.
Selecting an appropriate-granularity privacy principal is
an important ﬁrst step for the data owner. As a logistical
matter, ﬁner-grained records that share the same higher-
level principal can be aggregated into one logical record us-
ing SQL-like views. Using this aggregated data will then
provide guarantees as the level of the principal. But in gen-
eral, the analysis ﬁdelity will decrease as fewer records are
able to contribute to the output statistics.
In this paper, we assume that the privacy principal is at
the granularity of records in the dataset. This position is
generous for analysis but it is also the starting point for
beginning to understand the applicability of diﬀerential pri-
vacy in our context. If analysis noise is excessive even at this
granularity, there is little hope. In the future, we intend to
study the impact of using higher-level principals.
126Packet-level analyses
Packet size and port dist.
Worm ﬁngerprinting [27]
Flow-level analyses
Common ﬂow properties [30]
Stepping stone detection [33]
Graph-level analyses
Anomaly detection [13]
Passive topology mapping [9]
(§5.1.1)
(§5.1.2)
(§5.2.1)
(§5.2.2)
(§5.3.1)
(§5.3.2)
Expressibility
faithful
faithful
High accuracy
strong privacy
weak privacy
could not isolate connections in a ﬂow
strong privacy
(one of the two) sliding windows were approximated medium privacy
faithful
used a simpler clustering method
strong privacy
weak privacy
Table 2: The analyses that we consider and summary results for them.
4. A PRIVATE ANALYSIS TOOLKIT
In this section we present a collection of tools that im-
plement primitives that are common to many network trace
analyses. The tools are applicable to data analysis broadly
and represent the ﬁrst practical implementations that are
sensitive to privacy cost and added noise. We will arrive at
speciﬁc networking analyses in the next section. Our toolkit
and the associated analyses are publicly available [23].
4.1 The Cumulative Density Function
Often, in addition to simple aggregates like counts and
averages, we are interested in understanding the underly-
ing distribution itself, which may have informative ranges
or modes.
In networking analyses, distributions are often
studied using the CDF: cdf(x) = number of the records with
value ≤ x. Measuring precise empirical CDFs with arbi-
trary resolution is not possible with diﬀerential privacy; as
the resolution δ decreases, cdf(x) - cdf(x-δ) depends on only
a few records in the data. We present three approaches to
approximate the CDF.
A simple approach is to partition the range into buckets of
a certain resolution and count, for each bucket, the records
that fall in that bucket or a previous one. Let buckets be
the set of values that represent the high end of each bucket,
then:
foreach (var x in buckets)
trace.Where(rec => rec.val  rec.value)
foreach (var x in buckets)
tally += parts[x].Count(epsilon);
yield tally;
This approach has the advantage that the total privacy cost
is independent of the number of buckets (i.e., resolution) but
a limitation is that the error at each measurement accumu-
lates to form the CDF. However, these errors cancel some-
what, and their standard deviation is proportional only to
p|buckets|.
An even more advanced approach takes measurements at