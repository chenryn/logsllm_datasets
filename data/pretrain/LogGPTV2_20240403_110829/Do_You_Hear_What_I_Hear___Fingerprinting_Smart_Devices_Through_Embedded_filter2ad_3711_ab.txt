then the class label of the GMM that gives the highest likelihood is
returned as the expected class for the unknown ﬁngerprint. GMMs
are often used in biometric systems, most notably in human speaker
recognition systems, due to their capability of representing a large
class of sample distributions [59, 65].
Sound WaveDistanceVariable CapacitanceFlexible DiaphragmPerforated rigid back−plateAcoustic holesElectrodeVentilation holeBack−chamberMovable diaphragmCompressed AirPermanent MagnetDiaphragmSpiderBasketBack−plateSuspensionVoice coil#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Feature
RMS
ZCR
Low-Energy-Rate
Spectral Centroid
Spectral Entropy
Spectral Irregularity
Spectral Spread
Spectral Skewness
Spectral Kurtosis
Spectral Rolloff
Spectral Brightness
Spectral Flatness
MFCCs
Chromagram
Tonal Centroid
Dimension
Description
Table 1: Explored acoustic features
1
1
1
1
1
1
1
1
1
1
1
1
13
12
6
Square root of the arithmetic mean of the squares of the signal strength at various frequencies
The rate at which the signal changes sign from positive to negative or back
The percentage of frames with RMS power less than the average RMS power for the whole audio signal
Represents the center of mass of a spectral power distribution
Captures the peaks of a spectrum and their locations
Measures the degree of variation of the successive peaks of a spectrum
Deﬁnes the dispersion of the spectrum around its centroid
Represents the coefﬁcient of skewness of a spectrum
Measure of the ﬂatness or spikiness of a distribution relative to a normal distribution
Deﬁnes the frequency below which 85% of the distribution magnitude is concentrated
Computes the amount of spectral energy corresponding to frequencies higher than a given cut-off threshold
Measures how energy is spread across the spectrum
Compactly represents spectrum amplitudes residing inside the mel-frequency range
Representation of the distribution of energy along the 12 distinct semitones or pitch classes
Maps a chromagram onto a six-dimensional Hypertorus structure
6. EVALUATION
In this section we perform a series of experiments to evaluate
how well we can ﬁngerprint smartphones by exploiting the manu-
facturing idiosyncrasies of microphones and speakers embedded in
them. We start by describing how we performed our experiments
(Section 6.1). Next, we brieﬂy discuss the setup for ﬁngerprinting
devices through speaker, microphone and a combination of both
(Section 6.2). We then discuss a framework for determining the
dominant (most-relevant) set of audio features that can be used in
ﬁngerprinting smartphones (Section 6.3). Then, we look at ﬁnger-
printing devices, ﬁrst, of different maker-and-model (Section 6.4),
followed by devices of same maker-and-model (Section 6.5) and
ﬁnally, a combination of both with multiple units of different mod-
els (Section 6.6). The performance of our approach is affected by
certain aspects of the operating environment, and we the study sen-
sitivity to such factors in Section 6.7.
6.1 Methodology
To perform our experiments, we constructed a small testbed en-
vironment with real smartphone device hardware. In particular, our
default environment consisted of a 266 square foot (14’x19’) ofﬁce
room, with nine-foot dropped ceilings with polystyrene tile, com-
prising a graduate student ofﬁce in a University-owned building.
The room was ﬁlled with desks and chairs, and opens out on a pub-
lic hall with foot trafﬁc. The room also receives a minimal amount
of ambient noise from air conditioning, desktop computers, and ﬂo-
rescent lighting. We placed smartphones in various locations in the
room. To emulate an attacker, we placed an ACER Aspire 5745
laptop in the room. To investigate performance with inexpensive
hardware, we used the laptop’s built-in microphone to collect audio
samples. We investigate how varying this setup affects performance
of the attack in Section 6.7.
Devices and tools: We tested our device ﬁngerprinting on devices
from ﬁve different manufacturers. Table 2 highlights the model and
quantities of the different phones used in our experiments.
Table 2: Types of phones used
Maker
Apple
Google
Quantity
Model
iPhone 5
Nexus One
Nexus S
Galaxy S3
Galaxy S4
Droid A855
Samsung
Motorola
Sony Ericsson
W518
Total
1
14
8
3
10
15
1
52
We also investigate different genres of audio excerpts. Table 3
describes the different types of audio excerpts used in our experi-
ments. Duration of the audio clips varies from 3 to 10 seconds. The
default sampling frequency of all audio excerpts is 44.1kHz unless
explicitly stated otherwise. All audio clips are stored in WAV for-
mat using 16-bit pulse-code-modulation (PCM) technique.
Table 3: Types of audio excerpts
Type
Description
Variations
Instrumental Musical instruments playing together, e.g., ringtone
Human speech
Small segments of human speech
Song
Combination of human voice & instrumental sound
4
4
3
For analysis we leverage the following audio tools and analytic
modules: MIRtollbox [13], Netlab [15], Audacity [3] and the An-
droid app Hertz [6]. Both MIRtoolbox and Netlab are MATLAB
modules providing a rich set of functions for analyzing and ex-
tracting audio features. Audacity and Hertz are mainly used for
recording audio clips on computers and smartphones respectively.
For analyzing and matching ﬁngerprints we use a desktop ma-
chine with the following conﬁguration: Intel i7-2600 3.4GHz pro-
cessor with 12GiB RAM. We found that the average time required
to match a new ﬁngerprint was around 5–10 ms for k-NN classiﬁer
and around 0.5–1 ms for GMM classiﬁer.
Evaluation metrics: We use standard multi-class classiﬁcation
metrics—precision, recall, and F1-score [64]—in our evaluation.
Assuming there are ﬁngerprints from n classes, we ﬁrst compute
the true positive (T P ) rate for each class, i.e., the number of traces
from the class that are classiﬁed correctly. Similarly, we compute
the false positive (F P ) and false negative (F N), as the number
of wrongly accepted and wrongly rejected traces, respectively, for
each class i (1 ≤ i ≤ n). We then compute precision, recall, and
the F1-score for each class using the following equations:
Precision, P ri = T Pi/(T Pi + F Pi)
Recall, Rei = T Pi/(T Pi + F Ni)
F1-Score, F1 i = (2 × P ri × Rei)/(P ri + Rei)
(1)
(2)
(3)
The F1-score is the harmonic mean of precision and recall; it pro-
vides a good measure of overall classiﬁcation performance, since
precision and recall represent a trade-off: a more conservative clas-
siﬁer that rejects more instances will have higher precision but
lower recall, and vice-versa. To obtain the overall performance of
the system we compute average values in the following way:
Avg. Precision, AvgPr =
Avg. Recall, AvgRe =
Avg. F1-Score, AvgF1 =
i=1 Rei
n
2 × AvgP r × AvgRe
AvgP r + AvgRe
(cid:80)n
(cid:80)n
i=1 P ri
n
(4)
(5)
(6)
Each audio excerpt is recorded/played 10 times, 50% of which
is used for training and the remaining 50% is used for testing. We
report the maximum evaluation obtained by varying the number
of neighbors (k) from 1 to 5 for the k-NN classiﬁer and consider-
ing 1 to 5 Gaussian distributions per class. Since GMM parame-
ters are produced by the randomized EM algorithm, we perform 10
parameter-generation runs for each instance and report the average
classiﬁcation performance. We also compute the 95% conﬁdence
interval, but we found it to be less than 0.01 and therefore, do not
report it in the rest of the paper.
6.2 Fingerprinting Acoustic Components
6.2.1 Process of Fingerprinting Speaker
An attacker can leverage our technique to passively observe au-
dio signals (e.g., ringtones) emitting from device speakers in pub-
lic environments. To investigate this, we ﬁrst look at ﬁngerprinting
speakers integrated inside smartphones. For ﬁngerprinting speakers
we record audio clips played from smartphones onto a laptop and
we then extract acoustic features from the recorded audio excerpts
to generate ﬁngerprints as shown in Figure 3. We look at devices
manufactured by both the same vendor and different vendors.
Figure 3: Steps of ﬁngerprinting speakers.
6.2.2 Process of Fingerprinting Microphone
Attackers may also attempt to ﬁngerprint devices by observing
imperfections in device microphones, for example by convincing
the user to install an application on their phone, which can observe
inputs from the device’s microphone. To investigate the feasibility
of this attack, we next look at ﬁngerprinting microphones embed-
ded in smartphones. To do this, we record audio clips played from
a laptop onto smartphones as shown in Figure 4. Again we consider
devices made by both the same vendor and different vendors.
Figure 4: Steps of ﬁngerprinting microphones.
6.2.3 Process of Fingerprinting both Speaker and Mic
An attacker may attempt to ﬁngerprint devices by observing im-
perfections in both device microphone and speaker, for example by
convincing the user to install a game on their phone which requires
access to device speaker and microphone to interact with the game
(something like Talking Tom Cat). The attacker could potentially
play a theme song at the start of the game and at the same time
make a recording of the audio clip. To investigate the feasibility of
this attack, we build an android app that plays and records audio
clips simultaneously and uploads the data to a remote server. The
recorded audio clips would then enable the attacker to characterize
the imperfections of microphones and speakers embedded inside
smartphones. Figure 5 summarizes the whole process.
Figure 5: Steps of ﬁngerprinting both microphones and speakers.
6.3 Feature Exploration
At ﬁrst glance, it seems that we should use all features at our
disposal to identify device types. However, including too many
features can worsen performance in practice, due to their varying
accuracies and potentially-conﬂicting signatures. Hence, in this
section, we provide a framework to explore all the 15 audio fea-
tures described in Section 5.1 and identify the dominating subset
of all the features, i.e., which combination of features should be
used. For this purpose we adopt a well known machine learning
strategy known as feature selection [42, 66]. Feature selection is
the process of reducing dimensionality of data by selecting only a
subset of the relevant features for use in model construction. The
main assumption in using feature selection technique is that the
data may contain redundant features. Redundant features are those
which provide no additional beneﬁt than the currently selected fea-
tures. Feature selection techniques are a subset of the more general
ﬁeld of feature extraction, however, in practice they are quite dif-
ferent from each other. Feature extraction creates new features as
functions of the original features, whereas feature selection returns
a subset of the features. Feature selection is preferable to feature
extraction when the original units and meaning of features are im-
portant and the modeling goal is to identify an inﬂuential subset.
When the features themselves have different dimensionality, and
numerical transformations are inappropriate, feature selection be-
comes the primary means of dimension reduction.
Feature selection involves the maximization of an objective func-
tion as it searches through the possible candidate subsets. Since ex-
haustive evaluation of all possible subsets are often infeasible (2N
for a total of N features) different heuristics are employed. We
use a greedy search strategy known as sequential forward selection
(SFS) where we start off with an empty set and sequentially add the
features that maximize our objective function. The pseudo code of
our feature selection algorithm is described in Algorithm 1.
The algorithm works as follows. First, we compute the F1-score
that can be achieved by each feature individually. Next, we sort
the feature set based on the achieved F1-score in descending order.
Then, we iteratively add features starting from the most dominant
one and compute the F1-score of the combined feature subset. If
adding a feature increases the F1-score seen so far we move on to
the next feature, else we remove the feature under inspection. Hav-
ing traversed through the entire set of features, we return the subset
of features that maximizes our device classiﬁcation task. Note that
this is a greedy approach, therefore, the generated subset might not
Algorithm 1 Sequential Feature Selection
Input: Input feature set F
Output: Dominant feature subset D
F 1_score ← []
for f ∈ F do
F 1_score[f ] ← Classif y(f )
end for
F (cid:48) ← sort(F, F 1_score) #In descending order
max_score ← 0
D ← ∅
for f ∈ F (cid:48) do
D ← D ∪ f
temp ← Classif y(D)
if temp > max_score then
max_score ← temp
D ← D − {f}
else
end if
end for
return D
always provide optimal F1-score. However, for our purpose, we
found this approach to perform well, as we demonstrate in latter
sections. We test our feature selection algorithm for all three types
of audio excerpts listed in Table 3. We evaluate the F1-score using
both k-NN and GMM classiﬁers.
Note that the audio excerpts used for feature exploration and the
ones used for evaluating our ﬁngerprinting approach in the follow-
ing sections are not identical. We use different audio excerpts be-
longing to the three categories listed in Table 3, so as to not bias
our evaluations.
6.4 Different Maker-and-Model Devices
In this section we look at ﬁngerprinting smartphones manufac-
tured by ﬁve different vendors. We take one representative smart-
phone from each row of Table 2 giving us a total of 7 different
smartphones. We look at ﬁngerprinting these devices ﬁrst by using
the microphone and speaker individually and next, by combining
both microphone and speaker.
6.4.1 Feature Exploration
First, we look at exploring different acoustic features with the
goal of obtaining the dominant subset of features. Table 4 high-
lights the maximum F1-score achieved by each acoustic feature for
the three different types of audio excerpt. The maximum F1-score
is obtained by varying k from 1 to 5 (for k-NN classiﬁer) and also
considering 1 to 5 gaussian distributions per class (for GMM classi-
ﬁer). Each type of audio is recorded 10 times giving us a total of 70
samples from the 7 representative handsets; 50% of which (i.e., 5
samples per handset) is used for training and the remaining 50% is
used for testing. All the training samples are labeled with their cor-
responding handset identiﬁer. Both classiﬁers return the class label
for each audio clip in the test set and from that we compute F1-
score. The table also highlights the subset of features selected by
our sequential feature selection algorithm and their corresponding
F1-score. We ﬁnd that most of the time MFCCs are the dominant
features for all categories of audio excerpt.
To get a better understanding of why MFCCs are the dominant
acoustic features we plot the MFCCs of a given audio excerpt from
three different handsets on Figure 6. All the coefﬁcients are ranked
in the same order for the three handsets. We can see that the magni-
tude of the coefﬁcients vary across the handsets. For example, coef-
ﬁcient 3 and 5 vary signiﬁcantly across the three handsets. Hence,
MFCCs are highly suitable features for ﬁngerprinting smartphones.
6.4.2 Fingerprinting using Speaker
We test our ﬁngerprinting approach using three different types
of audio excerpt. Each audio sample is recorded 10 times, 50%
of which is used for training and the remaining 50% is used for
testing. We repeat this procedure for the three different types of au-
dio excerpt. Table 5 summarizes our ﬁndings (values are reported
as percentages). We simply use the acoustic features obtained from
our sequential feature selection algorithm as listed in Table 4. From
Table 5 we see that we can successfully (with a maximum F1-score
of 100%) identify which audio clip came from which smartphone.
Thus, ﬁngerprinting smartphones manufactured by different ven-
dors seems very much feasible using only few acoustic features.
Table 5: Fingerprinting different smartphones using speaker output
Audio
Type
Instrumental
Human speech
Song
k-NN
Features∗ AvgP r AvgRe AvgF 1
97.4
[1,7]
94.8
[13]
[15]
97.4
97.6
95.2
97.6
97.1
94.3
97.1
∗ Features taken from Table 4
GMM
Features∗ AvgP r AvgRe AvgF 1
100
100
100
[13]
[13]
[13]
100
100
100
100
100
100
6.4.3 Fingerprinting using Microphone
Similar to speakers, we ﬁnd microphone properties differ quite
substantially across vendors. We exploit this phenomenon to ﬁn-
gerprint smartphones through microphones. To test our hypothesis
we test our ﬁngerprinting approach using three different types of
audio excerpt. Each audio sample is again recorded 10 times; we
use 50% for training and the other 50% for testing. Table 6 summa-
rizes our ﬁndings (values are reported as percentages). We use the
same set of features obtained from our sequential feature selection
algorithm as listed in Table 4. From Table 6 we see that we can
achieve an F1-score of over 97%. These results suggest that smart-
phones can be successfully ﬁngerprinted through microphones too.
Table 6: Fingerprinting different smartphones using mic
Audio
Type
GMM
k-NN
Instrumental
Human speech
Features∗ AvgP r AvgRe AvgF 1
94.8
[13,1]
94.8
[15,9,1]
[13,1,12]
97.4
∗ Features taken from Table 4
95.2
95.2
97.6