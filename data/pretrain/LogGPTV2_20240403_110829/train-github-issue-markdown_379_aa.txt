### System information
  * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)** :
  * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** :
  * **TensorFlow installed from (source or binary)** : python wheel
  * **TensorFlow version (use command below)** : 1.4 and 1.3
  * **Python version** : 3.6.1
  * **Bazel version (if compiling from source)** :
  * **GCC/Compiler version (if compiling from source)** :
  * **CUDA/cuDNN version** : None
  * **GPU model and memory** : None
  * **Exact command to reproduce** :
when I run tensorflow1.4 script using estimator, the script is 8 times slower
than tensorflow 1.3
### Source code / logs
`main` script
    #!/usr/bin/env python
    __author__ = 'zj'
    import argparse
    import os
    import sys
    import numpy as np
    import time
    try:
        import better_exceptions
    except ImportError:
        pass
    import tensorflow as tf
    from src.model_ori import crnn_fn
    from src.data_handler import data_loader
    from src.config import Params, Alphabet
    from src.input_utils import input_fn
    def main(unused_argv):
        models_path = FLAGS.input_model_dir
        if not os.path.exists(models_path):
            assert FileNotFoundError
        models_list = [os.path.join(models_path, x[:-5]) for x in os.listdir(models_path) if x.endswith('.meta')]
        if not os.path.exists(FLAGS.output_model_dir):
            os.makedirs(FLAGS.output_model_dir)
        parameters = Params(eval_batch_size=128,
                            input_shape=(32, 304),
                            digits_only=False,
                            alphabet=Alphabet.CHINESECHAR_LETTERS_DIGITS_EXTENDED,
                            alphabet_decoding='same',
                            image_channels=1,
                            csv_delimiter=' ',
                            csv_files_eval=FLAGS.csv_files_eval,
                            output_model_dir=FLAGS.output_model_dir,
                            gpu=FLAGS.gpu
                            )
        model_params = {
            'Params': parameters,
        }
        os.environ['CUDA_VISIBLE_DEVICES'] = parameters.gpu
        config_sess = tf.ConfigProto()
        config_sess.gpu_options.per_process_gpu_memory_fraction = 0.6
        # Config estimator
        est_config = tf.estimator.RunConfig()
        est_config = est_config.replace(session_config=config_sess,
                                        save_summary_steps=100,
                                        model_dir=parameters.output_model_dir)
        estimator = tf.estimator.Estimator(model_fn=crnn_fn,
                                           params=model_params,
                                           config=est_config,
                                           model_dir=parameters.output_model_dir,
                                           )
        try:
            with open(FLAGS.output_file, encoding='utf-8', mode='w') as save_file:
                for model in models_list:
                    start = time.time()
                    eval_results = estimator.evaluate(input_fn=data_loader(csv_filename=parameters.csv_files_eval,
                                                                           params=parameters,
                                                                           batch_size=parameters.eval_batch_size,
                                                                           num_epochs=1),
                                                      steps=3,
                                                      checkpoint_path=model)
                    print('time:',time.time() - start)
                    print('model: %s Evaluation results: %s' % (model, str(eval_results)))
                    save_file.write(model + ' ' + str(eval_results) + '\n')
        except KeyboardInterrupt:
            print('Interrupted')
    if __name__ == '__main__':
        parser = argparse.ArgumentParser()
        parser.add_argument('-fe', '--csv_files_eval', required=False, type=str, help='CSV filename for evaluation',
                            nargs='*', default=['E:/val1.csv'])
        parser.add_argument('-o', '--output_model_dir', required=False, type=str,
                            help='Directory for output', default='models_vgg_100K_no_eval')
        parser.add_argument('-m', '--input_model_dir', required=False, type=str,
                            help='Directory for output', default='model_test')
        parser.add_argument('-g', '--gpu', type=str, help="GPU 0,1 or '' ", default='0')
        parser.add_argument('-of', '--output_file', required=False, type=str, default='123.txt', help="the log output file")
        tf.logging.set_verbosity(tf.logging.DEBUG)
        FLAGS, unparsed = parser.parse_known_args()
        tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
`data_loader` script
    #!/usr/bin/env python
    import tensorflow as tf
    import numpy as np
    from .config import Params, CONST
    from typing import Tuple
    def data_loader(csv_filename: str, params: Params, batch_size: int = 128, data_augmentation: bool = False,
                    num_epochs: int = None, image_summaries: bool = False):
        def input_fn():
            # Choose case one csv file or list of csv files
            if not isinstance(csv_filename, list):
                filename_queue = tf.train.string_input_producer([csv_filename], num_epochs=num_epochs,
                                                                name='filename_queue')
            elif isinstance(csv_filename, list):
                filename_queue = tf.train.string_input_producer(csv_filename, num_epochs=num_epochs, name='filename_queue')
            # Skip lines that have already been processed
            reader = tf.TextLineReader(name='CSV_Reader', skip_header_lines=0)
            key, value = reader.read(filename_queue, name='file_reading_op')
            default_line = [['None'], ['None']]
            path, label = tf.decode_csv(value, record_defaults=default_line, field_delim=params.csv_delimiter,
                                        name='csv_reading_op')
            image, img_width = image_reading(path, resized_size=params.input_shape, params=params,
                                             data_augmentation=data_augmentation, padding=True)
            to_batch = {'images': image, 'images_widths': img_width, 'filenames': path, 'labels': label}
            prepared_batch = tf.train.shuffle_batch(to_batch,
                                                    batch_size=batch_size,
                                                    min_after_dequeue=500,
                                                    num_threads=15, capacity=4000,
                                                    allow_smaller_final_batch=False,
                                                    name='prepared_batch_queue')
            if image_summaries:
                tf.summary.image('input/image', prepared_batch.get('images'), max_outputs=1)
            tf.summary.text('input/labels', prepared_batch.get('labels')[:10])
            tf.summary.text('input/widths', tf.as_string(prepared_batch.get('images_widths')))
            return prepared_batch, prepared_batch.get('labels')
        return input_fn
    def image_reading(path: str, params: Params, resized_size: Tuple[int, int] = None, data_augmentation: bool = False,
                      padding: bool = False) -> Tuple[tf.Tensor, tf.Tensor]:
        # Read image
        image_content = tf.read_file(path, name='image_reader')
        image = tf.cond(tf.equal(tf.string_split([path], '.').values[1], tf.constant('jpg', dtype=tf.string)),
                        true_fn=lambda: tf.image.decode_jpeg(image_content, channels=params.image_channels,
                                                             try_recover_truncated=True),  # TODO channels = 3 ?
                        false_fn=lambda: tf.image.decode_png(image_content, channels=params.image_channels),
                        name='image_decoding')
        # Data augmentation
        if data_augmentation:
            image = augment_data(image)
        # Padding
        if padding:
            with tf.name_scope('padding'):
                image, img_width = padding_inputs_width(image, resized_size, increment=CONST.DIMENSION_REDUCTION_W_POOLING)
        # Resize
        else:
            image = tf.image.resize_images(image, size=resized_size)
            img_width = tf.shape(image)[1]
        with tf.control_dependencies([tf.assert_equal(image.shape[:2], resized_size)]):
            return image, img_width
    def random_rotation(img: tf.Tensor, max_rotation: float = 0.1, crop: bool = True) -> tf.Tensor:  # from SeguinBe
        with tf.name_scope('RandomRotation'):
            rotation = tf.random_uniform([], -max_rotation, max_rotation)
            rotated_image = tf.contrib.image.rotate(img, rotation, interpolation='BILINEAR')
            if crop:
                rotation = tf.abs(rotation)
                original_shape = tf.shape(rotated_image)[:2]