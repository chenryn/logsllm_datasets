cation arrives at the switch, the switch dequeues the corresponding
request from the queue, and the lock could be granted to the next
request in the queue. This requires two operations: one is to de-
queue the head, and the other is to read the new head. Second,
when a request to acquire a shared lock is granted, if the following
requests in the queue are also for a shared lock, then these requests
can also be granted. This requires multiple read operations until an
exclusive lock request or the end of the queue. We leverage a feature
called resubmit available in programmable switches to overcome
the limitation. The resubmit feature allows the switch data plane
to resubmit the packet to the beginning of the packet processing
pipeline, so that the packet can go through and be processed by
the pipeline again, obviating the need to send another packet to
the switch from servers. Note that the use of resubmit here does
not cause extra overhead, because the servers in the traditional
server-based lock managers also need to send a packet to grant
each shared lock to the corresponding client. Figure 6 illustrates
how to handle the four cases for shared and exclusive locks.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Zhuolong Yu, Yiwen Zhang, Vladimir Braverman, Mosharaf Chowdhury, and Xin Jin
Resubmit
Algorithm 2 SwitchDataPlane(pkt)
Release One
Shared Lock
S S
Release One
Shared Lock
S E
Grant One
Exclusive Lock
Release One
Exclusive Lock
E S S E
Grant Two
Shared Locks
Release One
Exclusive Lock
E E
Grant One
Exclusive Lock
Figure 6: Handle shared and exclusive locks.
• Shared → Shared. When a shared lock is released, the switch
dequeues the head, and uses resubmit to check the new head.
If the new head is a shared lock request, the processing stops,
because the shared lock has already been granted with the old
head when it entered the queue.
• Shared → Exclusive. This case differs from the first case on
that the new head is an exclusive lock request, which has not
been granted yet. As such, after the shared lock is released, the
lock becomes available, and the switch sends a notification to the
client to grant the lock.
• Exclusive → Shared. When an exclusive lock is released, the
packet is resubmitted to grant the next lock request in the queue.
The resubmit action is repeated by multiple times until an exclu-
sive request or the end of the queue.
• Exclusive → Exclusive. When an exclusive lock is released and
the next request is also exclusive, the next request is granted.
Because the lock is exclusive and cannot be shared, the switch
does not need to resubmit it again.
Algorithm 2 shows the pseudocode of the switch that covers the
above four cases. If the request is to acquire a lock, it is enqueued
(line 1-2). The request is directly granted if the queue is empty,
or if all requests in the queue are shared and the request is also
shared (line 3-5). If the request is to release a lock, the current
head in the queue is removed, and the lock is resubmitted to grant
the following request (line 7-12). For case łshared → sharedž, no
further processing is needed. For case łshared → exclusivež and
łexclusive → exclusivež, the new head is granted the lock (line
15-16). For case łexclusive → sharedž, multiple subsequent shared
locks are granted (line 17-27). The nuance in the lock processing
is that when there are multiple transactions holding a shared lock,
these transactions may not release their locks in the order that the
requests are enqueued. Because the switch can only release locks
at the head of the queue, it does not check the transaction ID when
releasing locks. This design does not affect the correctness, because
only one transaction can hold an exclusive lock, and the operations
for releasing shared locks are commutative.
(queue .is _shar ed () and pkt .mode == shar ed ) then
дr ant _l ock (pkt .t id , pkt .cip)
(mode , t id , cip) ← queue .dequeue ()
met a .f l aд ← 1
met a .mode ← mode
met a .point er ← queue .head ()
r esubmit ()
else if met a .f l aд == 1 then
if met a .f l aд == 0 then
queue .enqueue (pkt )
if queue .is _empty() or
1: if pkt .op == acquir e then
2:
3:
4:
5:
6: else
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
else
(mode , t id , cip) ← queue [met a .point er ]
if mode == ex clusive then
дr ant _l ock (t id , cip)
else if met a .mode == ex clusive then
дr ant _l ock (t id , cip)
met a .point er ← met a .point er .nex t ()
met a .f l aд ← 2
r esubmit ()
(mode , t id , cip) ← queue [met a .point er ]
if mode == shar ed then
дr ant _l ock (t id , cip)
met a .point er ← met a .point er .nex t ()
r esubmit ()
Pipeline layout. A switch may have several pipelines, and the
pipelines do not share state. In NetLock, the lock tables and their
register arrays are placed in the egress pipes that connect to their
corresponding lock servers. This placement avoids unnecessary re-
circulation across pipelines. Specifically, when a request arrives, it
is sent to the egress pipe that either owns the lock or connects to a
lock server that has the lock. If the request is granted, it is mirrored
to the upstream port to the client or the database server to finish
the transaction (Section 4.1). Otherwise, it is enqueued either at the
egress pipe or in a lock server.
4.3 Switch-Server Memory Management
Since the switch on-chip memory is limited, NetLock co-designs the
switch and servers and stores only the popular locks to the switch
memory. The switch control plane is responsible for creating and
deleting locks, and assigning memory for locks between the switch
and lock servers. The key challenge in memory allocation is that it
requires us to consider the contentions from multiple requests to
the same lock. When a lock is granted to a client, other requests
are queued in the switch and occupy memory space until the lock
is released.
Memory allocation mechanism. We first analyze the amount of
switch memory required to support a certain throughput. Let the
rate of lock requests to object i be ri . Let the maximum contention
for object i be ci , which means that there are at most ci concurrent
requests for object i. We assume ci is known based on the knowledge
of how many clients may need this lock, and we use a counter to
measure ri . Let the queue size for object i in the switch be si . If
si ≥ ci , then the switch can guarantee to process all requests for
object i, without queueing requests in the server. The memory
allocation is to decide which locks to assign to the switch, and for
each assigned lock, how much switch memory to allocate for it.
NetLock: Fast, Centralized Lock Management
Using Programmable Switches
Algorithm 3 MemoryAllocation(locks)
1: Sort locks by ri /ci in decreasing order
2: for lock i in l ocks do
3:
4:
5:
6: Allocate remaining locks to the servers
si ← min(swit ch .avail abl e , ci )
swit ch .avail abl e ← swit ch .avail abl e − si
Allocate si for lock i in switch memory
Let the switch memory size be S. We formulate the problem as the
following optimization problem.
maximize 
risi /ci
i
s .t . 
si ≤ S
i
si ≤ ci
(1)
(2)
(3)
The goal is to process as many lock requests in the switch as
possible, reducing the number of servers we need for NetLock. For
object i, because in the worse case the lock requests for i always
achieve the maximum contention ci , only a portion (si /ci ) of lock
requests can be queued at the switch, and the other portion (1−si /ci )
have to be sent to the server. Therefore, the optimization objective,
which is the request rate the switch can guarantee to process, is
i risi /ci . The constraint is that the total memory allocated to the
locks cannot exceed the switch memory size S, i.e., i si ≤ S. The
switch does not need to allocate more than ci memory slots to
object i, thus we have si ≤ ci .
This problem is similar to the fractional knapsack problem, which
can be solved with an optimal solution in polynomial time. Algo-
rithm 3 shows the pseudocode. Specifically, the value of allocating
one slot to object i in the switch is ri /ci . To maximize the objective,
the algorithm allocates the switch memory based on the decreasing
order of ri /ci .
The rate ri and contention ci for each lock are obtained by mea-
suring the workload. NetLock maintains two counters to track ri
and ci for each lock respectively, and updates the memory alloca-
tion based on Algorithm 3 when the workload changes. During
the update, NetLock first drains the requests of the locks that are
to be swapped out from the switch, and then allocates the switch
memory to more popular locks. Note that, for inserting a new lock
object, the new lock queue is first added to a lock server, and then
would be moved to the switch if the lock becomes popular.
Theorem 1. The memory allocation algorithm (Algorithm 3) is
optimal for the optimization problem (1-3).
r2
c2
rn
cn
>
> ... >
Proof. We consider the situation where i ci > S; otherwise,
there is enough memory for all the locks. Let there be n locks in total.
Without loss of generality, let r1
. Algorithm 3
c1
allocates as much memory as possible (min(switch.available, ci ))
for locks sorted by ri /ci . Assume this is not the optimal strategy.
Let the optimal strategy be s∗
2 , ..., s∗
n . Because i ci > S, there
exists at least one lock i such that s∗
i  j, s∗
= 0, the optimal strategy would be the same as
k
Algorithm 3. Therefore, there exists at least one lock k such that
k > j and s∗
, we
k
= ci , and s∗
> 0. Let s ′
j
+1 and s ′
k
−1. Because
= s∗
k
r j
c j
>
rk
ck
1 , s∗
= s∗
j
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Client 1a
Client 1b
Client 2
r1a = 100 req/s
r1b = 100 req/s
r2 = 10 req/s
Switch
Lock
Table
1
2
100 req/s
(a) Naive memory allocation.
Switch
Lock
Table
r1a = 100 req/s
r1b = 100 req/s
1
1
r2 = 10 req/s
10 req/s
Client 1a
Client 1b
Client 2
Server
Lock
Table
1
Server
Lock
Table
2
(b) Optimal memory allocation.
Figure 7: By allocating two slots in the switch to lock 1, the
optimal allocation can process all lock requests to lock 1 in
the switch, minimizing the server load.
have i ris ′
s∗
1 , s∗
2 , ..., s∗
i /ci > i ris∗
n is optimal. So Algorithm 3 is optimal.
i /ci . This contradicts that the allocation
□
Example. Figure 7 illustrates the key idea of the algorithm. There
are two concurrent clients that acquire exclusive locks for object 1
with a rate of 100 requests per second each. The queue needs two
slots to accommodate the contentions from the two clients. There
is only one client that acquires exclusive locks for object 2, with
a rate of 10 requests per second. The queue only needs one slot
for one client. Suppose the switch memory only has two slots. The
allocation in Figure 7(a) allocates one slot to each lock object. Since
the switch cannot queue requests for two clients for object 1, in
the worse case where the clients are highly synchronized, half of