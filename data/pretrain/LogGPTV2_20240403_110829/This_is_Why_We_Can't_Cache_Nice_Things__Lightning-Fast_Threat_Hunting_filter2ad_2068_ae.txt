25
16
%Cached Baseline
0.5 min
1.1 min
0.7 min
1.5 min
3 min
2 min
1.8 min
2.1 min
4.6 min
1.2 min
100%
86%
100%
90%
88%
82%
85%
94%
90%
94%
Swift
0.65 ms
0.1 ms
1.1 ms
0.2 ms
0.3 ms
0.09 ms
0.8 ms
1.9 ms
2.8 ms
1.2 ms
Speedup
46,000×
660,000×
28,000×
450,000×
600,000×
1,333,000×
135,000×
66,000×
98,000×
60,000×
Figure 8: Simplified causal graph of the simulated ransomware attack. Swift keeps part of the causal graph related to the ransomware attack
in the main-memory (red vertices), and part of that graph (yellow vertices) is spilled to the disk. Causal graph not related to the attack (green
vertices) is spilled to the disk.
represent files, and boxes to represent processes. In Figure 8, the red
vertices represent the most suspicious causal graph which is cached
in the main-memory. Yellow vertices are related to attack but spilled
to disk while green vertices are not related to attack (benign) which
are also spilled to disk. Due to dependency explosion problem
(false dependency) [61] benign vertices become part of attack’s
causal graph. Swift shows the most suspicious graph (red vertices)
to cyber analyst accelerate investigation and assist cyber analyst
to quickly identify the root cause (X.X.X.X connection to process
Redis-server) and ramification (Sensitive.tar read by process scp)
of this attack using this subgraph.
As can be seen in Table 2, 14% of attack-related vertices (yellow
vertices) were spilled to the disk. The main reason for this was our
conservative Global List size (k = 3000); these attack-related ver-
tices fell outside of the top-k most suspicious paths, leading to their
eviction from the suspicious cache. We found in our experiments that
increasing the Global list size from k = 3000 to k = 5000 was suffi-
cient to store 100% of attack-related vertices in the cache. In consider-
ing the k = 3000 configuration, some temporary files created by the
Redis-server process, such as /redis-3.0.3/temp-18434.rdb, are as-
signed low suspicious scores because redis regularly creates many
such files. However, the temporary file ∼/.ssh/temp18434.rdb was
highly unusual because Redis-server never writes to the ∼/.ssh
folder. As a result, it had a high suspiciousness score and was
retained in cache. Note that missing some temporary files from
the causal graph does not break causal analysis since we can still
identify the root cause and ramifications using red vertices alone.
Further, cyber analysts can still retrieve these yellow vertices from
disk later for further investigation.
RQ3: Scalability
Throughput. We define the throughput of Swift as the maxi-
mum number of events that Swift can process under different
configuration values of the global list size k, the eviction time win-
dow ∆Tevict , the promotion epoch ∆Tpromote, and the number of
threads. To stress test Swift, we replayed the audit logs from our
enterprise engagement at the maximal speed. The results of our
throughput experiment are shown in Figure 9. Since our eviction
algorithm is asynchronous, the throughput does not change under
different configurations except when we change the number of
consumer threads. We can see that Swift can process up to 100,000
10
sshd /bin/bash /bin/bashsshdencryptor~redis-3.0.3/dump.rdb~/.ssh/temp-18434.rdb~/.ssh/authorized_keys /bin/bash /usr/bin/whoamiSensitive.tar /bin/bash/usr/bin/sudo /bin/nc.traditionalredis-3.0.3/temp-18434.rdbsshd/var/run/utmpx.x.x.xx.x.x.xAlert 2Redis-server                sshdAttack Vertices in CacheBashwholswcﬁndlsmkdirBenign Vertices on DiskAttack Vertices on DiskAlert 1Legend       Other ﬁle nodes              scpsshOther ﬁle nodes(a)
(a)
(b)
(c)
Figure 9: Throughput of Swift under different configuration values.
(b)
(c)
(d)
(d)
Figure 10: Max. memory usage of Swift under different configuration values when ran for one day. TC stands for tracking cache and SC
stands for suspicious cache.
events/sec when the number of threads is 20, which was the max-
imum number of threads allowed by our machine. Note that, in
our experiment, each of the 191 hosts generated less than 5,000
events/sec on average, which is far less than the maximal through-
put of Swift. Assuming that this event generation rate holds, our
prototype implementation of Swift can scale to support up to 4,000
hosts with a single server.
Memory Usage. Another aspect of scalability is memory usage.
In our implementation of Swift, memory is consumed by two
components: the Kafka framework and the cache for events. Since
Kafka is only used as a black-box infrastructure in our implementa-
tion and could have very different configurations in practice, we
focus on the memory usage of the cache. In our experiment, we
first measured the maximum memory used by both tracking cache
and suspicious cache under different configuration values while
monitoring all the 191 hosts for one day. The results are shown in
Figure 10. Changing global list size and threads does not affect the
maximum usage of tracking cache and suspicious cache. Increasing
the ∆Tpromote increases the size of tracking cache because events
stay longer there. On the other hand, increasing ∆Tevict window
increases the suspicious cache usage since eviction algorithm runs
after long time. Our experiment shows that in general, Swift could
process the workload for 191 hosts in an enterprise with 300 MB
memory. For a server with 64 GB memory, as we have used in our
experiment, it is possible to handle thousands of hosts at the same
time.
RQ4: Benefits of Time Saved
Using causal analysis in state-of-the-art alert triage systems [41],
it takes on average 1 min to respond to forensic queries, with a
worst case performance of 2.5 hours; because response time grows
linearly with graph size, we can expect alerts related to sophisticated
intruders to fall closer to this worst-case because they employ a
“low and slow” attack approach. On the other hand, Swift responds
to queries in just 0.1 sec on average, with worst case performance of
1 minute. This effectively provides investigators with alert context
(i.e., causal graphs) as soon as the alert is triggered.
Still, it could be argued that an average response time of 1 minute
(as opposed to Swift’s 100 milliseconds) is suitably fast for cyber
analysts. However, it is important to consider the fact investigation
latency compounds as the number of alerts increases. Recent stud-
ies [12, 18, 22] have shown that organizations receive around 10,000
alerts per week. For simplicity, let us assume that all 10,000 alerts
need to be investigated,5 and that a true attack falls at each quar-
tile (i.e., alerts 2500, 5000, etc.) of the stack. For the first quartile,
NoDoze [41] imposes at least 41 hours of latency due to causal
analysis, while Swift will impose just 4 minutes of latency. By
the last quartile, NoDoze will have imposed 166 hours of latency,
while Swift introduces just 16 minutes. Further, we can assign a
financial cost to this difference – studies have shown that it costs
5In practice, alert triage systems may be used to condense or procedurally exclude
some alerts so that they need not be investigated; however, this exercise demonstrates
the value of eliminating causal analysis latency from the threat investigation process.
11
 0 10 20 30 40 50 60 70 80 90 10020040080016003200Throughput Events/msGlobal List Size 0 10 20 30 40 50 60 70 80 90 100248121620Throughput Events/msNumber of threads 0 10 20 30 40 50 60 70 80 90 100123456Throughput Events/mstpromote [min] 0 10 20 30 40 50 60 70 80 90 10012345Throughput Events/mstevict [min] 0 50 100 150 200 250 300 350 400 45020040080016003200Memory [MB]Global List SizeTCSC 0 50 100 150 200 250 300248121620Memory [MB]Number of threadsTCSC 0 50 100 150 200 250 300 350 400 450 50012345Memory [MB]tpromote [min]TCSC 0 50 100 150 200 250 300 350 40012345Memory [MB]tevict [min]TCSCan organization $32,000 for every day an attacker stays in the net-
work [36]. Thus, for just the attack in the last quartile, Swift could
save the organization up to $221,244 as compared to the previous
state-of-the-art.
RQ5: Effectiveness in Alert Management
To answer this research question, we measured the performance
and accuracy of Swift as an alert management system. We used
HSM’s suspicious influence scores for all the alerts and triaged
alerts based on scores. After that we compared our accuracy and
performance with the baseline approach [41]. Since our suspicious
influence scores were similar to the baseline approach, Swift has
the same accuracy (false and true positive rates) as the baseline
approach. However, the performance of Swift is magnitudes of
times better than baseline.
The performance of Swift over the baseline approach is mea-
sured in terms of response time. As we have already shown in the
CDF in Figure 7 that it took total of 5 hours to rank all the 140 alerts
using baseline. The reason for this is that baseline as an offline
approach first generate the causal graph using disk storage for each
alert. After that, it assigns suspiciousness influence scores to each
alert’s graph and then triage them based on these scores. On the
other hand it took Swift around 1 minute to rank all the 140 threat
alerts because it generates causal graph in an online fashion and
assigns suspiciousness influence scores as events arrive and keeps
most suspicious causal graphs in the main-memory. Thus, as soon
as alerts are fired by underlying TDS, Swift already has its graph
with suspiciousness score and just need to lookup this score from
the cache to triage which O(1) time. Since Swift also keeps track
of all the alerts fired on causal graph, it instantaneously correlates
new alert with all the previous alerts that are causally related.
9 DISCUSSION & LIMITATIONS
Swift is effective
Design of suspicious influence scoring system.
with an arbitrary suspicious influence scoring system that satisfies
all three properties described in Section 4.2. In Section 8.1, we
implemented an anomaly-based scoring system as the reference
in our evaluation. Other scoring systems, such as rule-based or
label-propagation-based systems, can also be applied as long as
they meet the three requirements.
Possible Attacks. One possible attack to spoil the cache of Swift
is by exploiting Hypothesis H1 – the adversary may conduct an
attack in a longer time window so that the causal paths of the attack
in the cache are eventually replaced by causal paths of other attacks
and suspicious activities. This attack can be alleviated by allocat-
ing large memory (Global List size). As long as there is enough
space, Swift will maintain the suspicious causal paths in the cache.
If there is not enough memory space, choosing which suspicious
paths to keep in the cache would be a trade-off. We leave this dis-
cussion for our future work. Adversaries may also try to spoil the
cache of Swift by generating anomalous events and causal paths
in provenance data. This can be solved by having more accurate un-
derlying anomaly detection techniques. In this paper, we apply one
commercial tool [13] to detect anomalies. Although it is important
to improve the accuracy of anomaly detection, it is orthogonal to
our study. Nevertheless, even if the cache is spoiled, an investigator
can still generate a complete causal graph but with some delay due
to disk IO.
Adversaries may try to spoil the cache system of Swift to degen-
erate its responsiveness by having a “spoofing attack”. The adver-
sary may conduct an attack that contains a vertex that is involved
in more than K most suspicious paths to occupy the whole global
list (e.g. unzipping more than K files from a .ZIP package). Under
the “spoofing attack,” causal paths of other vertices are evicted to
the disk so the performance of Swift to investigate other vertices is
degraded to existing offline solutions. To address this attack, Swift
only selects candidates from the PAT Habnormal of each vertex. Since
the size of PAT Habnormal of each vertex is limited to m, each vertex
will only occupy at most m slots in the global list.
Another type of spoofing attack is that the adversary may gen-
erate a lot of different suspicious events to occupy the cache. Since
we keep the longest path in the cache, the adversary needs to gen-
erate a huge number of independent suspicious events, which
do not have causal dependencies, to spoil the cache. However, if
an attacker tries to produce a lot independent suspicious events
then it defeats the “low and slow” strategy used by attackers and
generates a strong indication of an attack which a threat hunter
can immediately spot. Moreover, this problem is equivalent to the
problem of having too many suspicious paths that the cache cannot
hold, which we leave for future work.
The Swift approach is generic to provide broad
Applicability.
support for fast and interactive threat hunting in enterprises pro-
vided that system-level audit logs are being collected and there
is an underlying threat detector which monitors enterprise-wide
activities. The two key hypotheses presented in Section 6, upon
which Swift is built, are enterprise agnostic. These hypotheses
are derived from fundamental characteristics of system-level au-
dit logs [3, 4]. This ensures that our techniques can be applied in
different enterprises without sacrificing performance and accuracy.
10 CONCLUSION
Threat hunting using causality analysis has an insatiable demand for
high throughput and low latency investigation queries. In this paper,
we present Swift, an online causality tracker that directly works
on audit logs provided from commodity systems in an enterprise.
Swift is capable of identifying in-progress threats and provides
quick investigation capabilities to a cyber analyst before serious
damage is inflicted. We implemented Swift in 7k LoC of Java and
deployed at NEC Labs America. Our evaluation results show that
Swift can precisely capture all the causality related to the true