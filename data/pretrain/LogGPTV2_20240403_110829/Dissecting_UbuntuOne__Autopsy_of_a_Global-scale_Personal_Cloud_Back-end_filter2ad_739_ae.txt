3
10
4
10
5
10
6
10
0
1
10
2
10
3
10
4
10
5
10
Files
Figure 10: Files and directories per volume.
1
0.8
F
D
C
0.6
0.4
0
10
Shared Volumes
UDF Volumes
1
0.995
0.99
0.985
F
D
C
1
10
Volumes
2
10
0.98
10
0
1
10
Volumes
2
10
Figure 9: Time-series user operations inter-arrival
times and their approximation to a power-law.
Figure 11: Distribution of shared/user-deﬁned vol-
umes across users.
suggests that high variance of user inter-arrival operations is
present in time scales ranging from seconds to several hours.
Hence, users issue requests in a bursty non-Poisson way:
during a short period a user sends several operations in quick
succession, followed by long periods of inactivity. A possible
explanation to this is that users manage data at the directory
granularity, thereby triggering multiples operations to keep
the ﬁles inside each directory in sync.
Nevertheless, we cannot conﬁrm the hypothesis that these
distributions are heavy-tailed. Clearly, Fig. 9(b) visually
conﬁrms that the empirical distributions of user Unlink and
Upload inter-arrivals can be only approximated with P (x) ≈
x−α, ∀x > θ, 1 < α < 2, for a central region of the domain.
We also found that metadata operations follow more closely
a power-law distribution than data operations. The reason is
that the behavior of metadata inter-operation times are not
aﬀected by the actual data transfers.
In conclusion, we can see that user operations are bursty,
which has strong implications to the operation of the back-
end servers (§ 7).
6.3 Inspecting User Volumes
Volume contents. Fig. 10 illustrates the relationship
between ﬁles and directories within user volumes. As usual,
ﬁles are much more numerous than directories. And we have
that over 60% of volumes have been associated with at least
one ﬁle. For directories, this percentage is only of 32%, but
there is a strong correlation between the number of ﬁles and
directories within a volume: Pearson correlation coeﬃcient
is 0.998. What is relevant is, however, that a small fraction
of volumes is heavy loaded: 5% of user volumes contain more
than 1, 000 ﬁles.
Shared and user-deﬁned volumes. At this point, we
study the distribution of user-deﬁned/shared volumes across
users. As pointed out by Canonical engineers, sharing is not
a popular feature of U1. Fig. 11 shows that only 1.8% of
users exhibits at least one shared volume. On the contrary,
we observe that user-deﬁned volumes are much more popu-
lar; we detected user-deﬁned volumes in 58% of users —the
rest of users only use the root volume. This shows that the
majority of users have some degree of expertise using U1.
Overall, these observations reveal that U1 was used more
as a storage service rather than for collaborative work.
7. METADATA BACK-END ANALYSIS
In this section, we focus on the interactions of RPC servers
against the metadata store. We also quantify the role of the
Canonical authentication service in U1.
7.1 Performance of Metadata Operations
Here we analyze the performance of RPC operations that
involve contacting the metadata store.
Fig. 12 illustrates the distribution of service times of the
diﬀerent RPC operations. As shown in the ﬁgure, all RPCs
exhibit long tails of service time distributions: from 7% to
22% of RPC service times are very far from the median
value. This issue can be caused by several factors, ranging
from interference of background processes to CPU power
saving mechanisms, as recently argued by Li et al. in [9].
Also useful is to understand the relationship between the
service time and the frequency of each RPC operation. Fig.
13 presents a scatter plot relating RPC median service times
with their frequency, depending upon whether RPCs are of
type read, write/update/delete or cascade, i.e., whether other
operations are involved. This ﬁgure conﬁrms that the type of
an RPC strongly determines its performance. First, cascade
operations (delete_volume and get_from_scratch) are the
slowest type of RPC —more than one order of magnitude
slower compared to the fastest operation. Fortunately, they
are relatively infrequent. Conversely, read RPCs, such as
list_volumes, are the fastest ones. Basically, this is because
read RPCs can exploit lockless and parallel access to the
pairs of servers that form database shards.
Write/update/delete operations (e.g. make_content, or
make_file) are slower than most read operations, but ex-
hibiting comparable frequencies. This may represent a per-
formance barrier for the metadata store in scenarios where
users massively update metadata in their volumes or ﬁles.
1641
0.8
0.6
0.4
0.2
F
D
C
RPC service time distribution (file system operations)
Create UDF
Delete Volume
Get Volume ID
List Shares
List Volumes
Make Dir.
Make File
Move
Delete
Get Delta
1
0.8
0.6
0.4
0.2
F
D
C
RPC service time distribution (upload management)
Add part to upload
Delete upload job
Get reusable content
Get upload job
Make content
Make upload job
Set upload job multipart ID
Touch upload job
1
0.8
0.6
0.4
0.2
F
D
C
RPC service time distribution (others)
Get user ID from token (auth)
Get from scratch
Get node
Get root
Get user data
1
10
2
10
0
10
−3
−2
10
10
−1
0
10
Time (seconds)
1
10
2
10
0
10
−3
−2
10
−1
10
10
Time (seconds)
0
1
10
2
10
0
10
−3
−2
10
−1
10
10
Time (seconds)
0
(a) File system management RPCs.
(b) Upload management RPCs.
(c) Other read-only RPCs.
Figure 12: Distribution of RPC service times accessing to the metadata store.
)
s
d
n
o
c
e
s
(
e
m
i
t
e
c
i
v
r
e
s
n
a
i
d
e
M
−1
10
−2
10
10
−3
3
10
Read RPC
Write/Delete/Update RPC
Cascade RPC
4
10
5
10
6
10
7
10
8
10
9
10
Operation count
Figure 13: We classiﬁed all the U1 RPC calls into 3
categories, and every point in the plot represents a
single RPC call. We show the median service time
vs frequency of each RPC (1 month).
7.2 Load Balancing in U1 Back-end
We are interested in analyzing the internal load balancing
of both API servers and shards in the metadata store. In
the former case, we grouped the processed API operations
by physical machine. In the latter, we distributed the RPC
calls contacting the metadata store across 10 shards based
on the user id, as U1 actually does. Results appear in Fig.
14, where bars are mean load values and error lines represent
the standard deviation of load values across API servers and
shards per hour and minute, respectively.
Fig. 14 shows that server load presents a high variance
across servers, which is symptom of bad load balancing.
This eﬀect is present irrespective of the hour of the day
and is more accentuated for the metadata store, for which
the time granularity used is smaller. Thus, this phenomenon
is visible in short or moderate periods of time. In the long
term, the load balancing is adequate; the standard deviation
across shards is only of 4.9% when the whole trace is taken.
Three particularities should be understood to explain the
poor load balancing. First, user load is uneven, i.e., a small
fraction of users is very active whereas most of them present
low activity. Second, the cost of operations is asymmetric;
for instance, there are metadata operations whose median
service time is 10x higher than others. Third, users display
a bursty behavior when interacting with the servers; for in-
stance, they can synchronize an entire folder. So, operations
arrive in a correlated manner.
We conclude that the load balancing in the U1 back-end
can be signiﬁcantly improved, which is object of future work.
7.3 Authentication Activity & User Sessions
Time-series analysis. Users accessing the U1 service
should be authenticated prior to the establishment of a new
session. To this end, U1 API servers communicate with a
separate and shared authentication service of Canonical.
Fig. 15 depicts a time-series view of the session manage-
ment load that API servers support to create and destroy
sessions, along with the corresponding activity of the au-
thentication subsystem.
In this ﬁgure, we clearly observe
that the authentication and session management activity is
5
x 10
Received requests across API servers (mean, std. dev.)
6
5
4
3
2
1
r
u
o
h
r
e
p
s
t
s
e
u
q
e
R
0
6
12
18
24
30
36
42
48
e
t
u
n
i
m
r
e
p
s
t
s
e
u
q
e
R
2500
2000
1500
1000
500
0
0
Time (hours)
Received requests across metadata store shards (mean, std. dev.)
10
20
30
40
50
60
Time (minutes)
Figure 14: Load balancing of U1 API servers and
metadata store shards.
closely related to the habits of users.
In fact, daily pat-
terns are evident. The authentication activity is 50% to 60%
higher in the central hours of the day than during the night
periods. This observation is also valid for week periods: on
average, the maximum number of authentication requests is
15% higher on Mondays than on weekends. Moreover, we
found that 2.76% of user authentication requests from API
servers to the authentication service fail.
Session length. Upon a successful authentication pro-
cess, a user’s desktop client creates a new U1 session.
U1 sessions exhibit a similar behavior to Dropbox home
users in [2] (Fig. 15). Concretely, 97% of sessions are shorter
than 8 hours, which suggests a strong correlation with user
working habits. Moreover, we also found that U1 exhibits
a high fraction of very short-lived sessions (i.e. 32% shorter
than 1s.). This is probably due to the operation of NAT and
ﬁrewalls that normally mediate between clients and servers,
which might be forcing the creation of new sessions by clos-
ing TCP connections unexpectedly [2, 23]. Overall, Fig. 15
suggests that domestic users are more representative than
other speciﬁc proﬁles, such as university communities, for
describing the connection habits of an entire Personal Cloud
user population.
We are also interested in understanding the data man-
agement activity related to U1 sessions. To this end, we
diﬀerentiate sessions that exhibited any type of data man-
agement operation (e.g., upload) during their lifetime (active
sessions) from sessions that do not (cold sessions).
First, we observed that the majority of U1 sessions (and,
therefore, TCP connections) do not involve any type of data
management. That is, only 5.57% of connections in U1 are
active (2.37M out of 42.5M), which, in turn, tend to be much
longer than cold ones. From a back-end perspective, the
unintended consequence is that a fraction of server resources
is wasted keeping alive TCP connections of cold sessions.
165Authentication activity in U1
x 10
4
API Session req. under DDoS (2h.)
API session req.
Auth. req.
5
x 10