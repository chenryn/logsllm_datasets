### 3. User Operation Patterns

#### 3.1 Time-Series Analysis of User Operations
Figure 9 illustrates the inter-arrival times of user operations and their approximation to a power-law distribution. The high variance in user inter-arrival times, ranging from seconds to several hours, indicates that users exhibit bursty, non-Poisson behavior. Specifically, users send multiple operations in quick succession during short periods, followed by long periods of inactivity. This pattern is likely due to users managing data at the directory level, which triggers multiple operations to keep files in sync.

However, we cannot confirm that these distributions are heavy-tailed. Figure 9(b) shows that the empirical distributions of Unlink and Upload inter-arrivals can only be approximated by \( P(x) \approx x^{-\alpha} \) for \( \forall x > \theta \) and \( 1 < \alpha < 2 \), within a central region of the domain. Additionally, metadata operations more closely follow a power-law distribution compared to data operations, as metadata inter-operation times are not affected by actual data transfers.

In conclusion, the bursty nature of user operations has significant implications for the operation of backend servers (Section 7).

### 4. Volume Content Analysis

#### 4.1 Files and Directories per Volume
Figure 10 depicts the relationship between files and directories within user volumes. As expected, files outnumber directories significantly. Over 60% of volumes contain at least one file, while only 32% of volumes have at least one directory. There is a strong correlation (Pearson correlation coefficient of 0.998) between the number of files and directories within a volume. Notably, 5% of user volumes contain more than 1,000 files.

#### 4.2 Shared and User-Defined Volumes
We also examined the distribution of shared and user-defined volumes across users. According to Canonical engineers, sharing is not a popular feature of U1. Figure 11 shows that only 1.8% of users have at least one shared volume. In contrast, user-defined volumes are more common, with 58% of users having such volumes. The remaining users only use the root volume, indicating that the majority of U1 users have some degree of expertise with the service. Overall, these observations suggest that U1 was primarily used as a storage service rather than for collaborative work.

### 5. Metadata Backend Analysis

#### 5.1 Performance of Metadata Operations
This section focuses on the performance of RPC operations involving the metadata store and the role of the Canonical authentication service in U1.

##### 5.1.1 Service Time Distribution
Figure 12 shows the distribution of service times for different RPC operations. All RPCs exhibit long tails in their service time distributions, with 7% to 22% of service times being far from the median. This issue can be attributed to various factors, including interference from background processes and CPU power-saving mechanisms, as discussed by Li et al. [9].

##### 5.1.2 Relationship Between Service Time and Frequency
Figure 13 presents a scatter plot relating the median service times and frequencies of different RPC operations, categorized as read, write/update/delete, or cascade. The type of RPC strongly influences its performance. Cascade operations (e.g., delete_volume and get_from_scratch) are the slowest, more than an order of magnitude slower than the fastest operations, but they are relatively infrequent. Read RPCs, such as list_volumes, are the fastest, benefiting from lockless and parallel access to the database shards. Write/update/delete operations (e.g., make_content or make_file) are slower than most read operations but occur with comparable frequency, potentially creating a performance bottleneck when users update metadata extensively.

### 6. Load Balancing in U1 Backend

#### 6.1 Internal Load Balancing
We analyzed the internal load balancing of API servers and metadata store shards. For API servers, we grouped processed operations by physical machine. For metadata store shards, we distributed RPC calls based on user ID, as U1 does. Figure 14 shows that server load varies significantly across servers, indicating poor load balancing. This effect is more pronounced for the metadata store, especially over shorter time periods. However, in the long term, the load balancing is adequate, with a standard deviation of 4.9% across shards.

Three factors contribute to the poor load balancing:
1. Uneven user load: A small fraction of users is highly active, while most have low activity.
2. Asymmetric operation costs: Some metadata operations have median service times 10x higher than others.
3. Bursty user behavior: Users often synchronize entire folders, leading to correlated operation arrivals.

### 7. Authentication Activity and User Sessions

#### 7.1 Time-Series Analysis
Users must be authenticated before establishing a new session. Figure 15 provides a time-series view of session management load and authentication activity. Daily patterns are evident, with authentication activity 50% to 60% higher during central hours of the day compared to night periods. This pattern holds true for weekly cycles, with Mondays seeing 15% more authentication requests than weekends. Additionally, 2.76% of authentication requests from API servers to the authentication service fail.

#### 7.2 Session Length
Upon successful authentication, a new U1 session is created. Figure 15 shows that 97% of sessions are shorter than 8 hours, similar to Dropbox home users [2]. A high fraction (32%) of sessions are very short-lived (less than 1 second), likely due to NAT and firewalls closing TCP connections unexpectedly. This suggests that domestic users are more representative of the general Personal Cloud user population.

#### 7.3 Data Management Activity
We differentiate between active sessions (involving data management operations like uploads) and cold sessions (no data management). Only 5.57% of U1 sessions (2.37M out of 42.5M) are active, and these tend to be much longer than cold sessions. From a backend perspective, this results in wasted server resources maintaining TCP connections for cold sessions.