### 2023

**Table VII: Attack Performance (Precision and Recall) of LINKTELLER on Three Datasets in the Transductive Setting, Compared with Two Baseline Methods LSA2-{post, attr} [11]**

We follow He et al. [11] to compose a balanced dataset containing an equal number of connected and unconnected node pairs. Groups of rows represent different density beliefs \( \hat{k} \) of the attacker.

| **Dataset** | **Density Belief \( \hat{k} \)** | **Method** | **Precision** | **Recall** |
|-------------|---------------------------------|------------|---------------|------------|
| **Cora**    | k/4                             | Ours       | 99.9 ± 0.1    | 25.0 ± 0.0 |
|             |                                 | LSA2-post  | 96.7 ± 0.2    | 24.2 ± 0.0 |
|             |                                 | LSA2-feat  | 96.9 ± 0.2    | 24.2 ± 0.0 |
|             | k/2                             | Ours       | 99.9 ± 0.0    | 50.0 ± 0.0 |
|             |                                 | LSA2-post  | 94.1 ± 0.3    | 47.0 ± 0.1 |
|             |                                 | LSA2-feat  | 90.4 ± 0.5    | 45.2 ± 0.2 |
|             | k                               | Ours       | 99.5 ± 0.1    | 99.5 ± 0.1 |
|             |                                 | LSA2-post  | 86.7 ± 0.2    | 86.7 ± 0.2 |
|             |                                 | LSA2-feat  | 73.6 ± 0.1    | 73.6 ± 0.1 |
|             | 1.5k                            | Ours       | 66.7 ± 0.0    | 100.0 ± 0.0 |
|             |                                 | LSA2-post  | 66.0 ± 0.0    | 99.1 ± 0.0 |
|             |                                 | LSA2-feat  | 59.9 ± 0.2    | 89.8 ± 0.2 |

| **Citeseer** | k/4                             | Ours       | 100.0 ± 0.0   | 25.0 ± 0.0  |
|--------------|---------------------------------|------------|---------------|-------------|
|              |                                 | LSA2-post  | 98.8 ± 0.1    | 24.7 ± 0.0  |
|              |                                 | LSA2-feat  | 99.8 ± 0.1    | 24.9 ± 0.0  |
|              | k/2                             | Ours       | 100.0 ± 0.0   | 50.0 ± 0.0  |
|              |                                 | LSA2-post  | 96.7 ± 0.0    | 48.4 ± 0.0  |
|              |                                 | LSA2-feat  | 97.4 ± 0.1    | 48.7 ± 0.1  |
|              | k                               | Ours       | 99.7 ± 0.0    | 99.7 ± 0.0  |
|              |                                 | LSA2-post  | 90.1 ± 0.2    | 90.1 ± 0.2  |
|              |                                 | LSA2-feat  | 80.9 ± 0.1    | 80.9 ± 0.1  |
|              | 1.5k                            | Ours       | 66.7 ± 0.0    | 100.0 ± 0.0 |
|              |                                 | LSA2-post  | 66.4 ± 0.0    | 99.6 ± 0.0  |
|              |                                 | LSA2-feat  | 63.2 ± 0.1    | 94.7 ± 0.2  |

| **Pubmed** | k/4                             | Ours       | 100.0 ± 0.0   | 25.0 ± 0.0  |
|------------|---------------------------------|------------|---------------|-------------|
|            |                                 | LSA2-post  | 89.9 ± 0.2    | 22.5 ± 0.1  |
|            |                                 | LSA2-feat  | 97.8 ± 0.2    | 24.4 ± 0.0  |
|            | k/2                             | Ours       | 100.0 ± 0.0   | 50.0 ± 0.0  |
|            |                                 | LSA2-post  | 86.8 ± 0.1    | 43.4 ± 0.0  |
|            |                                 | LSA2-feat  | 95.2 ± 0.1    | 47.6 ± 0.0  |
|            | k                               | Ours       | 99.7 ± 0.0    | 99.7 ± 0.0  |
|            |                                 | LSA2-post  | 78.8 ± 0.1    | 78.8 ± 0.1  |
|            |                                 | LSA2-feat  | 82.4 ± 0.1    | 82.4 ± 0.1  |
|            | 1.5k                            | Ours       | 66.6 ± 0.0    | 99.9 ± 0.0  |
|            |                                 | LSA2-post  | 65.3 ± 0.0    | 98.0 ± 0.0  |
|            |                                 | LSA2-feat  | 64.0 ± 0.0    | 96.0 ± 0.0  |

**Table VIII: AUC of LINKTELLER Compared with Two Baselines LSA2-{post, attr} [11] in the Transductive Setting**

Each column corresponds to one dataset. Different rows represent different methods.

| **Method**  | **Cora**      | **Citeseer**  | **Pubmed**  |
|-------------|--------------|---------------|-------------|
| Ours        | 1.00 ± 0.00  | 1.00 ± 0.00   | 1.00 ± 0.00 |
| LSA2-post   | 0.93 ± 0.00  | 0.96 ± 0.00   | 0.87 ± 0.00 |
| LSA2-attr   | 0.81 ± 0.00  | 0.89 ± 0.00   | 0.90 ± 0.00 |

In the transductive setting, we aim to evaluate the performance of LINKTELLER and compare it with the baselines LSA2-{post, attr} [11]. We use three datasets (Cora, Citeseer, and Pubmed) from their paper and follow the same setup as described in their "Datasets Configuration" section in Section 5.1. The balanced set of node pairs to be attacked contains an equal number of connected and unconnected pairs. We train the GCN models using the hyper-parameters from Kipf et al. [13] and then perform LINKTELLER and LSA2-{post, attr} attacks on these trained models.

The attack performance (Precision and Recall) is reported in Table VII, and the AUC scores are in Table VIII. As a sanity check, our results in Table VIII match the Figure 4 in He et al. [11] for these three datasets. In Table VII, we evaluate the density belief \( \hat{k} \) up to 1.5k, as 2k would result in all node pairs being predicted positive by the attacker, leading to 50% precision and 100% recall for all methods. Overall, LINKTELLER consistently outperforms LSA2-{post, attr} in the transductive setting.

### 6) Choosing ε on a Validation Dataset

In Section VI, we present the model utility and attack effectiveness under a range of ε (1-10) in Figure 3 and provide corresponding discussions regarding the tradeoff between model utility and privacy in Sections VI-C and VI-D.

**Table IX: (a) Model Utility (F1 Score) and (b) Attack Effectiveness (F1 Score) on Different Models**

Each column corresponds to a dataset. We consider four types of models: vanilla GCN, MLP, EDGERAND, and LAPGRAPH.

**(a) Model Utility (F1 Score)**

| **Dataset** | **vanilla GCN** | **MLP**         | **EDGERAND**   | **LAPGRAPH**  |
|-------------|-----------------|-----------------|----------------|---------------|
| RU          | 0.319           | 0.290           | 0.292 ± 0.011  | 0.299 ± 0.006 |
| DE          | 0.551           | 0.545           | 0.546 ± 0.001  | 0.545 ± 0.003 |
| FR          | 0.404           | 0.373           | 0.299 ± 0.017  | 0.321 ± 0.027 |
| ENGB        | 0.601           | 0.598           | 0.601 ± 0.001  | 0.607 ± 0.000 |
| PTBR        | 0.411           | 0.358           | 0.401 ± 0.010  | 0.423 ± 0.018 |
| Flickr      | 0.515           | 0.463           | 0.467 ± 0.002  | 0.459 ± 0.002 |

**(b) Attack Effectiveness (F1 Score) on Different Node Degree Distributions**

| **Degree**  | **Model**       | **RU**          | **DE**          | **FR**          | **ENGB**        | **PTBR**        | **Flickr**      |
|-------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|
| Low         | vanilla GCN     | 84.9 ± 1.2      | 74.7 ± 1.5      | 75.8 ± 2.3      | 86.8 ± 5.1      | 80.9 ± 2.0      | 69.5 ± 2.5      |
|             | EDGERAND        | 18.9 ± 10.8     | 58.1 ± 2.2      | 72.6 ± 1.5      | 4.4 ± 6.3       | 67.1 ± 4.6      | 41.6 ± 8.1      |
|             | LAPGRAPH        | 22.9 ± 3.3      | 59.6 ± 1.2      | 69.6 ± 1.0      | 5.3 ± 7.5       | 67.1 ± 2.9      | 46.9 ± 3.7      |
| Unconstrained | vanilla GCN  | 78.5 ± 4.5      | 78.9 ± 0.8      | 78.1 ± 2.7      | 78.2 ± 3.7      | 82.3 ± 1.5      | 82.2 ± 3.4      |
|             | EDGERAND        | 60.1 ± 5.0      | 76.5 ± 1.8      | 73.4 ± 1.1      | 78.1 ± 2.7      | 68.0 ± 1.7      | 82.3 ± 1.5      |
|             | LAPGRAPH        | 59.6 ± 2.0      | 68.5 ± 1.1      | 68.4 ± 5.8      | 78.4 ± 1.4      | 68.0 ± 1.7      | 82.2 ± 3.4      |
| High        | vanilla GCN     | 82.9 ± 4.9      | 83.0 ± 3.7      | 84.8 ± 1.6      | 86.6 ± 1.3      | 82.9 ± 4.9      | 84.8 ± 1.6      |
|             | EDGERAND        | 19.0 ± 12.0     | 32.9 ± 2.5      | 77.9 ± 3.5      | 52.1 ± 5.8      | 32.9 ± 13.3     | 18.3 ± 5.2      |
|             | LAPGRAPH        | 22.0 ± 1.0      | 23.8 ± 2.2      | 73.2 ± 5.1      | 16.9 ± 2.9      | 2.4 ± 3.4       | 15.7 ± 2.6      |

In this part, we focus on a practical approach for selecting the appropriate parameters (mainly the privacy budget ε) to train DP GCN models with reasonable model utility. We select the parameter ε on the validation dataset and then report the final performance on the test set. Specifically, when selecting ε on the validation set, we set a lower threshold (e.g., F1 score of the MLP model) for the model utility and select the smallest ε that satisfies the condition that the utility of the trained DP GCN on the validation set is higher than the given threshold. We then evaluate both the model utility and attack effectiveness under the selected ε on the test set.

Due to space limitations, we refer readers to our full version [49] for detailed evaluation setup. The key conclusions from Table IX(a) and Table IX(b) are:

1. **Model Utility**: With the same constraint on model utility on the same dataset, different ε values are required for different DP mechanisms, indicating that the level of noise tolerated by different DP mechanisms varies.
2. **Attack Effectiveness**: The theoretical guarantee provided by differential privacy does not translate into sufficient protection against LINKTELLER while maintaining a reasonable level of model utility. However, in low-degree settings, differential privacy can empirically protect against LINKTELLER, although the level of protection is heavily data-dependent, varying across different datasets and node degree distributions. Relevant discussions on the impact of node degree distributions are provided in Section VI-D.

Authorized licensed use limited to: Tsinghua University. Downloaded on August 07, 2022, at 12:20:38 UTC from IEEE Xplore. Restrictions apply.

### 2024