which generate a 1-TB virtual address range) for mapping or unmapping a private address space. When 
the hypervisor initially constructs the VP, it allocates two private page table root entries. Those will be 
used to map the VP’s secret data, like its stack and data structures that contain private data. Switching 
the address space means writing the two entries in the global page table root (which explains why the 
term private address space has a misleading name—actually it is private address range). The hypervisor 
switches private address spaces only in two cases: when a new virtual processor is created and during 
284 
CHAPTER 9 Virtualization technologies
thread switches. (Remember, threads are backed by VPs. The core scheduler assures that no sibling SMT 
threads execute VPs from different partitions.) During runtime, a hypervisor thread has mapped only 
its own VP’s private data; no other secret data is accessible by that thread. 
Mapping secret data in the private address space is achieved by using the memory zone, represent-
ed by an MM_ZONE data structure. A memory zone encapsulates a private VA subrange of the private 
address space, where the hypervisor usually stores per-VP’s secrets.
The memory zone works similarly to the private address space. Instead of mapping root page table 
entries in the global page table root, a memory zone maps private page directories in the two root 
entries used by the private address space. A memory zone maintains an array of page directories, which 
will be mapped and unmapped into the private address space, and a bitmap that keeps track of the 
used page tables. Figure 9-10 shows the relationship between a private address space and a memory 
zone. Memory zones can be mapped and unmapped on demand (in the private address space) but are 
usually switched only at VP creation time. Indeed, the hypervisor does not need to switch them during 
thread switches; the private address space encapsulates the VA range exposed by the memory zone. 
PMLA Entry #192
PML4 Entry #2
PML4 Entry #3
PMLA Entry #320
PMLA Entry #480
Invalid
Invalid
0
0x100'00000000
0x200'00000000
0x7FFF'FFFFFFFF
0xFFFF8000'000000
0xFFFFF80'000000
0xFFFFFFFF'FFFFFFF
•••
••••
••••
••••
••••
••••
Zone PDPTE #256
Zone PDPTE #257
Zone PDPTE #258
Zone PDE #128
Zone PDE #256
Zone PDE #384
Invalid
Invalid
Zone's Page
Table
Zone's Page
Table
Zone's Page
Table
Shared Page
Table
Shared Page
Table
Shared Page
Directory
Shared Page
Directory
Shared Page
Table
Hypervisor's
Shared PDPT
Hypervisor's
Shared PDPT
Hypervisor's
Shared PDPT
Hypervisor’s Page
Table Root
(Maps the Entire HV)
Private Address Space
and Shared Page
Directory Pointers Tables
Private Zones
and Shared Page
Directories
Private Zones
and Share
Page Tables
FIGURE 9-10 The hypervisor’s private address spaces and private memory zones.
CHAPTER 9 Virtualization technologies
285
In Figure 9-10, the page table's structures related to the private address space are filled with a pat-
tern, the ones related to the memory zone are shown in gray, and the shared ones belonging to the hy-
pervisor are drawn with a dashed line. Switching private address spaces is a relatively cheap operation 
that requires the modification of two PML4 entries in the hypervisor’s page table root. Attaching or 
detaching a memory zone from the private address space requires only the modification of the zone’s 
PDPTE (a zone VA size is variable; the PDTPE are always allocated contiguously).
Dynamic memory
Virtual machines can use a different percentage of their allocated physical memory. For example, 
some virtual machines use only a small amount of their assigned guest physical memory, keeping a lot 
of it freed or zeroed. The performance of other virtual machines can instead suffer for high-memory 
pressure scenarios, where the page file is used too often because the allocated guest physical memory 
is not enough. With the goal to prevent the described scenario, the hypervisor and the virtualization 
stack supports the concept of dynamic memory. Dynamic memory is the ability to dynamically assign 
and remove physical memory to a virtual machine. The feature is provided by multiple components:
I 
The NT kernel’s memory manager, which supports hot add and hot removal of physical memory
(on bare-metal system too)
I 
The hypervisor, through the SLAT (managed by the address manager)
I 
The VM Worker process, which uses the dynamic memory controller module, Vmdynmem.dll,
to establish a connection to the VMBus Dynamic Memory VSC driver (Dmvsc.sys), which runs in
the child partition
To properly describe dynamic memory, we should quickly introduce how the page frame number 
(PFN) database is created by the NT kernel. The PFN database is used by Windows to keep track of 
physical memory. It was discussed in detail in Chapter 5 of Part 1. For creating the PFN database, the 
NT kernel first calculates the hypothetical size needed to map the highest possible physical address 
(256 TB on standard 64-bit systems) and then marks the VA space needed to map it entirely as reserved 
(storing the base address to the MmPfnDatabase global variable). Note that the reserved VA space still 
has no page tables allocated. The NT kernel cycles between each physical memory descriptor discov-
ered by the boot manager (using UEFI services), coalesces them in the longest ranges possible and, 
for each range, maps the underlying PFN database entries using large pages. This has an important 
implication; as shown in Figure 9-11, the PFN database has space for the highest possible amount of 
physical memory but only a small subset of it is mapped to real physical pages (this technique is called 
sparse memory).
286 
CHAPTER 9 Virtualization technologies
MmPfnDatabase
Physical Memory
Pfn 0x500
Page 0x500
. . .
Page 0x5FF
Memory Hole*
No
Map
Pfn 0x5FF
Pfn 0x800
Pfn 0x8FF
Hot Removed
Memory
(set as Bad)
Page 0x800
. . .
Page 0x8FF
FIGURE 9-11 An example of a PFN database where some physical memory has been removed.
Hot add and removal of physical memory works thanks to this principle. When new physical 
memory is added to the system, the Plug and Play memory driver (Pnpmem.sys) detects it and calls 
the MmAddPhysicalMemory routine, which is exported by the NT kernel. The latter starts a complex 
procedure that calculates the exact number of pages in the new range and the Numa node to which 
they belong, and then it maps the new PFN entries in the database by creating the necessary page 
tables in the reserved VA space. The new physical pages are added to the free list (see Chapter 5 in 
Part 1 for more details). 
When some physical memory is hot removed, the system performs an inverse procedure. It checks 
that the pages belong to the correct physical page list, updates the internal memory counters (like the 
total number of physical pages), and finally frees the corresponding PFN entries, meaning that they 
all will be marked as “bad.” The memory manager will never use the physical pages described by them 
anymore. No actual virtual space is unmapped from the PFN database. The physical memory that was 
described by the freed PFNs can always be re-added in the future.
When an enlightened VM starts, the dynamic memory driver (Dmvsc.sys) detects whether the child 
VM supports the hot add feature; if so, it creates a worker thread that negotiates the protocol and 
connects to the VMBus channel of the VSP. (See the “Virtualization stack” section later in this chapter 
for details about VSC and VSP.) The VMBus connection channel connects the dynamic memory driver 
running in the child partition to the dynamic memory controller module (Vmdynmem.dll), which is 
mapped in the VM Worker process in the root partition. A message exchange protocol is started. Every 
one second, the child partition acquires a memory pressure report by querying different performance 
counters exposed by the memory manager (global page-file usage; number of available, committed, 
CHAPTER 9 Virtualization technologies
287
and dirty pages; number of page faults per seconds; number of pages in the free and zeroed page list). 
The report is then sent to the root partition. 
The VM Worker process in the root partition uses the services exposed by the VMMS balancer, a 
component of the VmCompute service, for performing the calculation needed for determining the 
possibility to perform a hot add operation. If the memory status of the root partition allowed a hot add 
operation, the VMMS balancer calculates the proper number of pages to deposit in the child partition 
and calls back (through COM) the VM Worker process, which starts the hot add operation with the as-
sistance of the VID driver:
1.
Reserves the proper amount of physical memory in the root partition
2.
Calls the hypervisor with the goal to map the system physical pages reserved by the root parti-
tion to some guest physical pages mapped in the child VM, with the proper protection
3.
Sends a message to the dynamic memory driver for starting a hot add operation on some guest
physical pages previously mapped by the hypervisor
The dynamic memory driver in the child partition uses the MmAddPhysicalMemory API exposed by 
the NT kernel to perform the hot add operation. The latter maps the PFNs describing the new guest 
physical memory in the PFN database, adding new backing pages to the database if needed. 
In a similar way, when the VMMS balancer detects that the child VM has plenty of physical pages 
available, it may require the child partition (still through the VM Worker process) to hot remove some 
physical pages. The dynamic memory driver uses the MmRemovePhysicalMemory API to perform the 
hot remove operation. The NT kernel verifies that each page in the range specified by the balancer is 
either on the zeroed or free list, or it belongs to a stack that can be safely paged out. If all the condi-
tions apply, the dynamic memory driver sends back the “hot removal” page range to the VM Worker 
process, which will use services provided by the VID driver to unmap the physical pages from the child 
partition and release them back to the NT kernel.
Note Dynamic memory is not supported when nested virtualization is enabled.
Hyper-V schedulers
The hypervisor is a kind of micro operating system that runs below the root partition’s OS (Windows). 
As such, it should be able to decide which thread (backing a virtual processor) is being executed by 
which physical processor. This is especially true when the system runs multiple virtual machines com-
posed in total by more virtual processors than the physical processors installed in the workstation. The 
hypervisor scheduler role is to select the next thread that a physical CPU is executing after the allocated 
time slice of the current one ends. Hyper-V can use three different schedulers. To properly manage all 
the different schedulers, the hypervisor exposes the scheduler APIs, a set of routines that are the only 
entries into the hypervisor scheduler. Their sole purpose is to redirect API calls to the particular sched-
uler implementation.
288 
CHAPTER 9 Virtualization technologies
EXPERIMENT: Controlling the hypervisor’s scheduler type
Whereas client editions of Windows start by default with the root scheduler, Windows Server 2019 
runs by default with the core scheduler. In this experiment, you figure out the hypervisor scheduler 
enabled on your system and find out how to switch to another kind of hypervisor scheduler on the 
next system reboot.
The Windows hypervisor logs a system event after it has determined which scheduler to en-
able. You can search the logged event by using the Event Viewer tool, which you can run by typ-
ing eventvwr in the Cortana search box. After the applet is started, expand the Windows Logs 
key and click the System log. You should search for events with ID 2 and the Event sources set to 
Hyper-V-Hypervisor. You can do that by clicking the Filter Current Log button located on the 
right of the window or by clicking the Event ID column, which will order the events in ascending 
order by their ID (keep in mind that the operation can take a while). If you double-click a found 
event, you should see a window like the following:
The launch event ID 2 denotes indeed the hypervisor scheduler type, where
1 = Classic scheduler, SMT disabled
2 = Classic scheduler
3 = Core scheduler
4 = Root scheduler
EXPERIMENT: Controlling the hypervisor’s scheduler type
Whereas client editions of Windows start by default with the root scheduler, Windows Server 2019
runs by default with the core scheduler. In this experiment, you figure out the hypervisor scheduler
enabled on your system and find out how to switch to another kind of hypervisor scheduler on the
next system reboot.
The Windows hypervisor logs a system event after it has determined which scheduler to en-
able. You can search the logged event by using the Event Viewer tool, which you can run by typ-
ing eventvwr in the Cortana search box. After the applet is started, expand the Windows Logs
key and click the System log. You should search for events with ID 2 and the Event sources set to 
Hyper-V-Hypervisor. You can do that by clicking the Filter Current Log button located on the 
right of the window or by clicking the Event ID column, which will order the events in ascending 
order by their ID (keep in mind that the operation can take a while). If you double-click a found 
event, you should see a window like the following:
The launch event ID 2 denotes indeed the hypervisor scheduler type, where
1 = Classic scheduler, SMT disabled
2 = Classic scheduler
3 = Core scheduler
4 = Root scheduler
CHAPTER 9 Virtualization technologies
289
The sample figure was taken from a Windows Server system, which runs by default with the 
Core Scheduler. To change the scheduler type to the classic one (or root), you should open an ad-
ministrative command prompt window (by typing cmd in the Cortana search box and selecting 
Run As Administrator) and type the following command:
bcdedit /set hypervisorschedulertype 
where  is Classic for the classic scheduler, Core for the core scheduler, or Root for the 
root scheduler. You should restart the system and check again the newly generated Hyper-V-
Hypervisor event ID 2. You can also check the current enabled hypervisor scheduler by using an 
administrative PowerShell window with the following command:
Get-WinEvent -FilterHashTable @{ProviderName="Microsoft-Windows-Hyper-V-Hypervisor"; ID=2}          
-MaxEvents 1
The command extracts the last Event ID 2 from the System event log.
The classic scheduler
The classic scheduler has been the default scheduler used on all versions of Hyper-V since its initial 
release. The classic scheduler in its default configuration implements a simple, round-robin policy in 
which any virtual processor in the current execution state (the execution state depends on the total 
number of VMs running in the system) is equally likely to be dispatched. The classic scheduler supports 
also setting a virtual processor’s affinity and performs scheduling decisions considering the physical 
processor’s NUMA node. The classic scheduler doesn’t know what a guest VP is currently executing. 
The only exception is defined by the spin-lock enlightenment. When the Windows kernel, which is run-
ning in a partition, is going to perform an active wait on a spin-lock, it emits a hypercall with the goal 
to inform the hypervisor (high IRQL synchronization mechanisms are described in Chapter 8, “System 
mechanisms”). The classic scheduler can preempt the current executing virtual processor (which 
hasn’t expired its allocated time slice yet) and can schedule another one. In this way it saves the active 
CPU spin cycles. 
The default configuration of the classic scheduler assigns an equal time slice to each VP. This means 
that in high-workload oversubscribed systems, where multiple virtual processors attempt to execute, 
and the physical processors are sufficiently busy, performance can quickly degrade. To overcome 
The sample figure was taken from a Windows Server system, which runs by default with the 
Core Scheduler. To change the scheduler type to the classic one (or root), you should open an ad-
ministrative command prompt window (by typing cmd in the Cortana search box and selecting 
Run As Administrator) and type the following command:
bcdedit /set hypervisorschedulertype 
where  is Classic for the classic scheduler, Core for the core scheduler, or Root for the 
 is Classic for the classic scheduler, Core for the core scheduler, or Root for the 
root scheduler. You should restart the system and check again the newly generated Hyper-V-
Hypervisor event ID 2. You can also check the current enabled hypervisor scheduler by using an 
administrative PowerShell window with the following command:
Get-WinEvent -FilterHashTable @{ProviderName="Microsoft-Windows-Hyper-V-Hypervisor"; ID=2}          
-MaxEvents 1
The command extracts the last Event ID 2 from the System event log.
290 
CHAPTER 9 Virtualization technologies
the problem, the classic scheduler supports different fine-tuning options (see Figure 9-12), which can 
modify its internal scheduling decision:
I 
VP reservations A user can reserve the CPU capacity in advance on behalf of a guest ma-
chine. The reservation is specified as the percentage of the capacity of a physical processor to
be made available to the guest machine whenever it is scheduled to run. As a result, Hyper-V
schedules the VP to run only if that minimum amount of CPU capacity is available (meaning that
the allocated time slice is guaranteed).
I 