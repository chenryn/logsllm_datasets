### 4. OpenFlow Switch Latency

OpenFlow switches have been measured to exhibit latencies ranging from 1.72 ms to 8.33 ms for expensive table insertions [10]. AXE’s multicast control plane, which runs directly on the switch, avoids OpenFlow-specific overheads and does not require costly table updates. Consequently, a latency of 5 ms is considered a conservative estimate. Convergence typically occurs by the time the third or fifth packet is sent. In experiments with a larger number of group members, convergence often occurred by the second packet, as the final tree is larger and thus requires fewer prunes to reach a stable state.

### 6. Related Work

Since the introduction of Ethernet, numerous efforts have been made to improve both its operational model and performance. AXE represents a new direction in this body of literature, offering many of the advantages traditionally associated with routing while maintaining the simple plug-and-play nature of traditional MAC learning.

#### Enhancements to Spanning Tree Protocol

There have been various enhancements to the Spanning Tree Protocol (STP). Rapid Spanning Tree (RSTP) [13] improves upon STP by providing faster link and failure recoveries. Additionally, multiple spanning trees have been developed for a single network to enhance utilization and redundancy. These include MSTP, MISTP [15], PVST, and PVST+ [14]. AXE, however, eliminates the need for spanning trees, allowing it to achieve instantaneous failure recovery and utilize all available links, resulting in shorter paths.

#### Datacenter Networks

In datacenter networks, one trend that addresses many concerns surrounding STP is to abandon Layer 2 (L2) entirely and instead run IP all the way to the host [1, 9]. This approach performs well, similar to the Idealized Routing we evaluated in Section 5, though not as well as AXE in terms of failure recovery speed. Other work has used highly specialized Clos topologies coupled with an SDN control plane to achieve impressive results in a datacenter context [31]. However, these techniques do not offer AXE’s plug-and-play functionality and require specially designed network topologies. AXE, in contrast, works seamlessly on arbitrary topologies, making it ideal for less uniform environments.

In the datacenter context, F10 [23] achieves impressive results by co-designing the topology, routing, and failure detection. While it shares AXE’s primary goal of fast failure response, its highly coupled approach differs significantly. VL2 [9] maintains L2 semantics for hosts using a directory service rather than flood-and-learn, and is also designed with specific topologies in mind.

#### Link State Routing in L2

Recently, there has been interest in bringing the techniques of routing to L2 through technologies like SPB [12] and TRILL [30]. These protocols use link state routing to compute shortest paths between edge switches, inheriting the advantages and limitations of Layer 3 (L3) routing protocols.

#### Similar Approaches

AXE shares some similarities with Dynamic Source Routing (DSR) [17], 802.2 Source Route Bridging (SRB) [16], and AntNet [8]. These schemes send special "explorer packets" to find destinations and collect a list of switches they have passed through. Upon reaching the destination, a report is sent back to the source, which can use the accumulated list to build a path. AXE, however, does not use or create any special routing messages; all necessary information is contained in the AXE packet header. Instead of having the destination explicitly return a special routing packet, AXE relies on the destination to create a packet in the return direction during normal communication (e.g., a TCP ACK). This difference also applies to failure recovery: while DSR uses another type of routing message to convey failure information, AXE accomplishes this with plain data packets. As a result, AXE simultaneously routes and delivers user data, eliminating the distinction and latency between them. Additionally, AXE does not keep work-in-progress routing state in special packets but instead uses per-switch learning, requiring an alternate loop-prevention strategy.

#### Failure Carrying Packets (FCP)

Like AXE, Failure Carrying Packets (FCP) [21] minimizes convergence times by piggybacking information about failures in data packets. Each FCP switch has a more-or-less accurate map of the network, and each packet contains information about failed links. This allows an FCP switch to forward the packet to the destination if any working path is available. AXE, however, has no such network map and simply floods the packet and removes faulty state so that it can be rebuilt via learning.

#### Data-Driven Connectivity (DDC)

Data-Driven Connectivity (DDC) [22] also uses data packets to minimize convergence times during failures. It constructs a Directed Acyclic Graph (DAG) for each destination using a global view of the network. When failures occur, packets are sent along the DAG in the wrong direction, signaling a failure to the receiving switch, which can then find a new path to the destination. Thus, FCP, DDC, and AXE all use packets to signal failures, but whereas FCP and AXE use actual bits in the header, the signaling in DDC is implicit. Unlike AXE, which generally builds shortest paths, paths in DDC may end up arbitrarily long. Like FCP and unlike AXE, DDC relies on some sort of control plane to build a global view of the network.

Despite the extensive related efforts, none combine all of AXE’s features: plug-and-play, near-instantaneous recovery from failures, and the ability to work on general topologies. Therefore, AXE occupies a unique and useful niche in the networking ecosystem.

### 7. Conclusion

Our ultimate goal is to develop AXE as a general-purpose replacement for off-the-shelf Ethernet, providing essentially instantaneous failure recovery, efficient unicast bandwidth usage (not just short paths, but also ECMP-like behavior), and direct multicast support—while retaining Ethernet’s plug-and-play characteristics and topology agnosticism. We are not aware of any other design that strikes this balance. While AXE may not be suitable for high-performance, special-purpose datacenter environments where plug-and-play is less relevant, in most other cases, it presents a promising alternative to today’s designs.

### 8. Acknowledgments

We would like to thank the anonymous reviewers and especially our shepherd, Brad Karp, for their thoughtful feedback. This material is based upon work supported by sponsors including Intel, AT&T, and the National Science Foundation under Grants No. 1420064, 1216073, and 1139158.

### 9. References

[1] AL-FARES, M., LOUKISSAS, A., AND VAHDAT, A. A Scalable, Commodity Data Center Network Architecture. In Proc. of SIGCOMM (2008).

[2] AL-FARES, M., RADHAKRISHNAN, S., RAGHAVAN, B., HUANG, N., AND VAHDAT, A. Hedera: Dynamic Flow Scheduling for Data Center Networks. In Proc. of NSDI (2010).

[3] BENSON, T., AKELLA, A., AND MALTZ, D. Network Traffic Characteristics of Data Centers in the Wild. In Proc. of ACM Internet Measurement Conference (IMC) (2012).

[4] P4 Behavioral Model. https://github.com/p4lang/behavioral-model.

[5] BOSSHART, P., GIBB, G., KIM, H.-S., VARGHESE, G., MCKEOWN, N., IZZARD, M., MUJICA, F., AND HOROWITZ, M. Forwarding Metamorphosis: Fast Programmable Match-Action Processing in Hardware for SDN. In Proc. of SIGCOMM (2013).

[6] CHRISTENSEN, M., KIMBALL, K., AND SOLENSKY, F. Considerations for Internet Group Management Protocol (IGMP) and Multicast Listener Discovery (MLD) Snooping Switches. RFC 4541 (Informational), 2006.

[7] DATASHEET, TEXAS INSTRUMENTS. DP83867IR/CR Robust, High Immunity 10/100/1000 Ethernet Physical Layer Transceiver. http://www.ti.com/lit/ds/symlink/dp83867ir.pdf, 2015.

[8] DI CARO, G. A., AND DORIGO, M. Two Ant Colony Algorithms for Best-Effort Routing in Datagram Networks. In Proc. of the IASTED International Conference on Parallel and Distributed Computing and Systems (PDCS) (1998).

[9] GREENBERG, A., HAMILTON, J. R., JAIN, N., KANDULA, S., KIM, C., LAHIRI, P., MALTZ, D. A., PATEL, P., AND SENGUPTA, S. VL2: A Scalable and Flexible Data Center Network. In Proc. of SIGCOMM (2009).

[10] HE, K., KHALID, J., GEMBER-JACOBSON, A., DAS, S., PRAKASH, C., AKELLA, A., LI, L. E., AND THOTTAN, M. Measuring Control Plane Latency in SDN-enabled Switches. In Proc. of the ACM SIGCOMM Symposium on Software Defined Networking Research (SOSR) (2015).

[11] IEEE STANDARDS ASSOCIATION. 802.1ag-2007 - IEEE Standard for Local and Metropolitan Area Networks Virtual Bridged Local Area Networks Amendment 5: Connectivity Fault Management. http://standards.ieee.org/findstds/standard/802.1ag-2007.html.

[12] IEEE STANDARDS ASSOCIATION. 802.1aq-2012 - IEEE Standard for Local and metropolitan area networks–Media Access Control (MAC) Bridges and Virtual Bridged Local Area Networks–Amendment 20: Shortest Path Bridging. https://standards.ieee.org/findstds/standard/802.1aq-2012.html.

[13] IEEE STANDARDS ASSOCIATION. 802.1D-2004 - IEEE Standard for Local and metropolitan area networks: Media Access Control (MAC) Bridges. http://standards.ieee.org/findstds/standard/802.1D-2004.html.

[14] IEEE STANDARDS ASSOCIATION. 802.1Q-2014 - IEEE Standard for Local and metropolitan area networks–Bridges and Bridged Networks. http://standards.ieee.org/findstds/standard/802.1Q-2014.html.

[15] IEEE STANDARDS ASSOCIATION. 802.1s-2002 - IEEE Standards for Local and Metropolitan Area Networks - Amendment to 802.1Q Virtual Bridged Local Area Networks: Multiple Spanning Trees. http://standards.ieee.org/findstds/standard/802.1s-2002.html.

[16] IEEE STANDARDS ASSOCIATION. 802.2-1989 - IEEE Standard for Information Technology - Telecommunications and Information Exchange Between Systems - Local and Metropolitan Area Networks - Specific Requirements - Part 2: Logical Link Control. http://standards.ieee.org/findstds/standard/802.2-1989.html.

[17] JOHNSON, D. B. Routing in Ad Hoc Networks of Mobile Hosts. In Proc. of Workshop on Mobile Computing Systems and Applications (WMCSA) (1994).

[18] JOSE, L., YAN, L., VARGHESE, G., AND MCKEOWN, N. Compiling Packet Programs to Reconfigurable Switches. In Proc. of NSDI (2015).

[19] KATZ, D., AND WARD, D. Bidirectional Forwarding Detection (BFD). RFC 5880 (Proposed Standard), 2010.

[20] KEMPF, M. Bridge Circuit for Interconnecting Networks, 1986. US Patent 4,597,078.

[21] LAKSHMINARAYANAN, K., CAESAR, M., RANGAN, M., ANDERSON, T., SHENKER, S., AND STOICA, I. Achieving Convergence-free Routing Using Failure-carrying Packets. In Proc. of SIGCOMM (2007).

[22] LIU, J., PANDA, A., SINGLA, A., GODFREY, B., SCHAPIRA, M., AND SHENKER, S. Ensuring Connectivity via Data Plane Mechanisms. In Proc. of NSDI (2013).

[23] LIU, V., HALPERIN, D., KRISHNAMURTHY, A., AND ANDERSON, T. F10: A Fault-Tolerant Engineered Network. In Proc. of NSDI (2013).

[24] MARKOPOULOU, A., IANNACCONE, G., BHATTACHARYYA, S., CHUAH, C.-N., AND DIOT, C. Characterization of Failures in an IP Backbone. In Proc. of INFOCOM (2004).

[25] MCCAULEY, J., SHENG, A., JACKSON, E. J., RAGHAVAN, B., RATNASAMY, S., AND SHENKER, S. Taking an AXE to L2 Spanning Trees. In Proc. of HotNets (2015).

[26] MCKEOWN, N., ANDERSON, T., BALAKRISHNAN, H., PARULKAR, G., PETERSON, L., REXFORD, J., SHENKER, S., AND TURNER, J. OpenFlow: Enabling Innovation in Campus Networks. CCR 38, 2 (2008).

[27] Mininet. http://mininet.org/.

[28] ns-3. http://www.nsnam.org/.

[29] PERLMAN, R. An Algorithm for Distributed Computation of a Spanning Tree in an Extended LAN. In Proc. of SIGCOMM (1985).

[30] PERLMAN, R., EASTLAKE, D., DUTT, D., GAI, S., AND GHANWANI, A. Routing Bridges (RBridges): Base Protocol Specification. RFC 6325 (Proposed Standard), 2011.

[31] SINGH, A., ONG, J., AGARWAL, A., ANDERSON, G., ARMISTEAD, A., BANNON, R., BOVING, S., DESAI, G., FELDERMAN, B., GERMANO, P., ET AL. Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network. In Proc. of SIGCOMM (2015).

[32] WAITZMAN, D., PARTRIDGE, C., AND DEERING, S. Distance Vector Multicast Routing Protocol. RFC 1075 (Experimental), 1988.