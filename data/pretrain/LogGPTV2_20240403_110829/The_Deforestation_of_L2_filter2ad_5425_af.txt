4OpenFlow switches have had latencies measured from
1.72ms up to 8.33ms for expensive table insertions [10].
AXE’s multicast control plane running directly on the switch
would avoid OpenFlow-speciﬁc overheads and requires no
expensive table update, so 5ms seems an outside estimate.
converged by the time the third or ﬁfth packet is sent. Indeed,
for experiments we ran with a larger number of group mem-
bers, it had often converged by the second packet (the ﬁnal
tree is larger when there are more members, and it therefore
takes fewer prunes to converge to it).
6 Related Work
Since the introduction of Ethernet, there have been efforts
to improve upon it, both in terms of its operational model
and its performance. AXE represents a new direction in this
body of literature, providing many advantages traditionally
associated with routing without sacriﬁcing the simple plug-
and-play nature of traditional MAC learning.
There have been a wide variety of enhancements to Span-
ning Tree Protocol. Rapid Spanning Tree [13] improves upon
STP with faster link and failure recoveries. Additionally,
there have been numerous enhancements that build multi-
ple spanning trees in a single network to improve utilization
and redundancy. These include MSTP, MISTP [15], PVST,
PVST+ [14], and others. AXE dispenses with spanning trees
altogether, allowing it to achieve instantaneous failure recov-
ery and achieve short paths that utilize all links.
In datacenter networks, one trend that addresses many con-
cerns surrounding STP is to abandon L2 entirely, instead run-
ning IP all the way to the host [1,9]. This approach performs
well (similar to the Idealized Routing we evaluated against
in Section 5), though not as well as AXE in terms of how
quickly it recovers from failures. Work along the same lines
has used highly specialized Clos topologies coupled with an
508
SDN control plane to achieve truly impressive results in a
datacenter context [31]. We note that none of these tech-
niques achieve AXE’s plug-and-play functionality, and all of
them require specially designed network topologies. AXE,
on the other hand, works on arbitrary topologies without is-
sue, making it ideally suited for less uniform environments.
Also in the datacenter context, F10 [23] achieves impres-
sive results by co-designing the topology, routing, and failure
detection. While it shares one of AXE’s primary goals (fast
failure response), the highly-coupled approach is starkly dif-
ferent. VL2 [9] maintains L2 semantics for hosts, but does
so using a directory service rather than ﬂood-and-learn, and,
again, is designed with particular topologies in mind.
Recently there has been interest in bringing the techniques
of routing to L2 through technologies like SPB [12] and
TRILL [30]. These protocols use link state routing to com-
pute shortest paths between edge switches, thus inheriting
the advantages (and limitations) of L3 routing protocols.
In some ways, AXE is similar to Dynamic Source Rout-
ing [17], 802.2 Source Route Bridging [16], and AntNet [8].
These schemes all send special “explorer packets” looking
for destinations, collecting a list of switches they have passed
through. Upon reaching the destination, a report is sent back
to the source which can use the accumulated list of switches
to build a path. AXE differs most fundamentally in that it
does not use or create any special routing messages – ev-
erything needed to establish routes is contained in the AXE
packet header; rather than have the destination explicitly re-
turn a special routing packet, it relies on the destination to
create a packet in the return direction in the normal course
of communication (e.g., a TCP ACK). This difference also
applies to failure recovery: while DSR has another special
type of routing message to convey failure information and
induce the source to “explore” for new routes again, this too
takes place with plain data packets in AXE. An outcome of
all this is that AXE is simultaneously routing and deliver-
ing user data – there is no distinction (or latency) between
them. A further difference is that AXE does not keep work-
in-progress routing state in special packets, and instead uses
per-switch learning, requiring an alternate loop-prevention
strategy. While an SRB or an AntNet switch can identify a
looped packet because its own identiﬁer already exists in the
packet’s list of hops, AXE packets contain no such list; thus,
the switch must “remember” having seen the packet.
Like AXE, Failure Carrying Packets (FCP) [21] minimizes
convergence times by piggybacking information about fail-
ures in data packets. In FCP, each switch has a (more-or-less)
accurate map of the network, and each packet contains infor-
mation about failed links in the map that it has encountered.
This is sufﬁcient for an FCP switch to forward the packet
to the destination if any working path is available. AXE has
no such network map and so its strategy for failed packets is
simply to ﬂood the packet and remove faulty state so that it
can be rebuilt (via learning).
Data-Driven Connectivity (DDC) [22] also uses data pack-
ets to minimize convergence times during failure. It uses a
global view of the network to construct a DAG for each desti-
nation. When failures occur, packets are sent along the DAG
in the wrong direction, which signals a failure to the receiv-
ing switch which can then (via ﬂipping the directions of ad-
jacent edges) ﬁnd a new path to the destination. Thus, FCP,
DDC, and AXE all use packets to signal failures, but whereas
FCP and AXE use actual bits in the header, the signaling in
DDC is implicit. Unlike AXE, which generally builds short-
est paths, paths in DDC may end up arbitrarily long. And
similar to FCP and dissimilar to AXE, it relies on some sort
of control plane to build a global view of the network.
Despite this large set of related efforts, none combine all
of AXE’s features: plug-and-play, near-instantaneous recov-
ery from failures,5 and ability to work on general topologies.
Thus, we see AXE as occupying a useful and unique niche
in the networking ecosystem.
7 Conclusion
Ultimately, our goal is to develop AXE as a general-purpose
replacement for off-the-shelf Ethernet, providing essentially
instantaneous failure recovery, unicast that makes efﬁcient
use of bandwidth (not just short paths, but also ECMP-like
behavior), and direct multicast support – while retaining Eth-
ernet’s plug-and-play characteristics and topology agnosti-
cism. We are not aware of any other design that strikes this
balance. While we do not see AXE as a contender for special-
purpose high-performance datacenter environments (where
plug-and-play is largely irrelevant), in most other cases we
see it as a promising alternative to today’s designs.
8 Acknowledgments
We wish to thank the anonymous reviewers and especially
our shepherd, Brad Karp, for their thoughtful feedback.
This material is based upon work supported by sponsors
including Intel, AT&T, and the National Science Foundation
under Grants No. 1420064, 1216073, and 1139158.
9 References
[1] AL-FARES, M., LOUKISSAS, A., AND VAHDAT, A.
A Scalable, Commodity Data Center Network
Architecture. In Proc. of SIGCOMM (2008).
[2] AL-FARES, M., RADHAKRISHNAN, S., RAGHAVAN,
B., HUANG, N., AND VAHDAT, A. Hedera: Dynamic
Flow Scheduling for Data Center Networks. In Proc.
of NSDI (2010).
[3] BENSON, T., AKELLA, A., AND MALTZ, D. Network
Trafﬁc Characteristics of Data Centers in the Wild. In
Proc. of ACM Internet Measurement Conference
(IMC) (2012).
[4] P4 Behavioral Model.
https://github.com/p4lang/behavioral-model.
[5] BOSSHART, P., GIBB, G., KIM, H.-S., VARGHESE,
G., MCKEOWN, N., IZZARD, M., MUJICA, F., AND
HOROWITZ, M. Forwarding Metamorphosis: Fast
Programmable Match-Action Processing in Hardware
for SDN. In Proc. of SIGCOMM (2013).
5That is, limited only by failure detection time and the time
required for the switch to change forwarding behavior – not
by network size, communication with other switches, etc.
509
[6] CHRISTENSEN, M., KIMBALL, K., AND SOLENSKY,
F. Considerations for Internet Group Management
Protocol (IGMP) and Multicast Listener Discovery
(MLD) Snooping Switches. RFC 4541
(Informational), 2006.
[7] DATASHEET, TEXAS INSTRUMENTS. DP83867IR/CR
Robust, High Immunity 10/100/1000 Ethernet
Physical Layer Transceiver.
http://www.ti.com/lit/ds/symlink/dp83867ir.pdf, 2015.
[8] DI CARO, G. A., AND DORIGO, M. Two Ant Colony
Algorithms for Best-Effort Routing in Datagram
Networks. In Proc. of the IASTED International
Conference on Parallel and Distributed Computing
and Systems (PDCS) (1998).
[9] GREENBERG, A., HAMILTON, J. R., JAIN, N.,
KANDULA, S., KIM, C., LAHIRI, P., MALTZ, D. A.,
PATEL, P., AND SENGUPTA, S. VL2: A Scalable and
Flexible Data Center Network. In Proc. of SIGCOMM
(2009).
[10] HE, K., KHALID, J., GEMBER-JACOBSON, A., DAS,
S., PRAKASH, C., AKELLA, A., LI, L. E., AND
THOTTAN, M. Measuring Control Plane Latency in
SDN-enabled Switches. In Proc. of the ACM
SIGCOMM Symposium on Software Deﬁned
Networking Research (SOSR) (2015).
[11] IEEE STANDARDS ASSOCIATION. 802.1ag-2007 -
IEEE Standard for Local and Metropolitan Area
Networks Virtual Bridged Local Area Networks
Amendment 5: Connectivity Fault Management.
http://standards.ieee.org/ﬁndstds/standard/802.
1ag-2007.html.
[12] IEEE STANDARDS ASSOCIATION. 802.1aq-2012 -
IEEE Standard for Local and metropolitan area
networks–Media Access Control (MAC) Bridges and
Virtual Bridged Local Area Networks–Amendment
20: Shortest Path Bridging. https://standards.ieee.org/
ﬁndstds/standard/802.1aq-2012.html.
[13] IEEE STANDARDS ASSOCIATION. 802.1D-2004 -
IEEE Standard for Local and metropolitan area
networks: Media Access Control (MAC) Bridges.
http://standards.ieee.org/ﬁndstds/standard/802.
1D-2004.html.
[14] IEEE STANDARDS ASSOCIATION. 802.1Q-2014 -
IEEE Standard for Local and metropolitan area
networks–Bridges and Bridged Networks.
http://standards.ieee.org/ﬁndstds/standard/802.
1Q-2014.html.
[15] IEEE STANDARDS ASSOCIATION. 802.1s-2002 -
IEEE Standards for Local and Metropolitan Area
Networks - Amendment to 802.1Q Virtual Bridged
Local Area Networks: Multiple Spanning Trees. http://
standards.ieee.org/ﬁndstds/standard/802.1s-2002.html.
[16] IEEE STANDARDS ASSOCIATION. 802.2-1989 -
IEEE Standard for Information Technology -
Telecommunications and Information Exchange
Between Systems - Local and Metropolitan Area
Networks - Speciﬁc Requirements - Part 2: Logical
510
Link Control. http://standards.ieee.org/ﬁndstds/
standard/802.2-1989.html.
[17] JOHNSON, D. B. Routing in Ad Hoc Networks of
Mobile Hosts. In Proc. of Workshop on Mobile
Computing Systems and Applications (WMCSA)
(1994).
[18] JOSE, L., YAN, L., VARGHESE, G., AND
MCKEOWN, N. Compiling Packet Programs to
Reconﬁgurable Switches. In Proc. of NSDI (2015).
[19] KATZ, D., AND WARD, D. Bidirectional Forwarding
Detection (BFD). RFC 5880 (Proposed Standard),
2010.
[20] KEMPF, M. Bridge Circuit for Interconnecting
Networks, 1986. US Patent 4,597,078.
[21] LAKSHMINARAYANAN, K., CAESAR, M., RANGAN,
M., ANDERSON, T., SHENKER, S., AND STOICA, I.
Achieving Convergence-free Routing Using
Failure-carrying Packets. In Proc. of SIGCOMM
(2007).
[22] LIU, J., PANDA, A., SINGLA, A., GODFREY, B.,
SCHAPIRA, M., AND SHENKER, S. Ensuring
Connectivity via Data Plane Mechanisms. In Proc. of
NSDI (2013).
[23] LIU, V., HALPERIN, D., KRISHNAMURTHY, A., AND
ANDERSON, T. F10: A Fault-Tolerant Engineered
Network. In Proc. of NSDI (2013).
[24] MARKOPOULOU, A., IANNACCONE, G.,
BHATTACHARYYA, S., CHUAH, C.-N., AND DIOT,
C. Characterization of Failures in an IP Backbone. In
Proc. of INFOCOM (2004).
[25] MCCAULEY, J., SHENG, A., JACKSON, E. J.,
RAGHAVAN, B., RATNASAMY, S., AND SHENKER,
S. Taking an AXE to L2 Spanning Trees. In Proc. of
HotNets (2015).
[26] MCKEOWN, N., ANDERSON, T., BALAKRISHNAN,
H., PARULKAR, G., PETERSON, L., REXFORD, J.,
SHENKER, S., AND TURNER, J. OpenFlow: Enabling
Innovation in Campus Networks. CCR 38, 2 (2008).
[27] Mininet. http://mininet.org/.
[28] ns-3. http://www.nsnam.org/.
[29] PERLMAN, R. An Algorithm for Distributed
Computation of a Spanning Tree in an Extended LAN.
In Proc. of SIGCOMM (1985).
[30] PERLMAN, R., EASTLAKE, D., DUTT, D., GAI, S.,
AND GHANWANI, A. Routing Bridges (RBridges):
Base Protocol Speciﬁcation. RFC 6325 (Proposed
Standard), 2011.
[31] SINGH, A., ONG, J., AGARWAL, A., ANDERSON,
G., ARMISTEAD, A., BANNON, R., BOVING, S.,
DESAI, G., FELDERMAN, B., GERMANO, P., ET AL.
Jupiter Rising: A Decade of Clos Topologies and
Centralized Control in Google’s Datacenter Network.
In Proc. of SIGCOMM (2015).
[32] WAITZMAN, D., PARTRIDGE, C., AND DEERING, S.
Distance Vector Multicast Routing Protocol. RFC
1075 (Experimental), 1988.