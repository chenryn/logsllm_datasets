2 int callback1(void* a1, int a2) { ... }
3 int callback2(void* a1) { ... }
4
5 // 2) Prepare file handle
6 FILE *fp = fopen("filename", "rb");
7
8 // 3) Initialize objects, internally invoking ReadFile()
9 int *f0_a0 = (int*) calloc(4096, sizeof(int));
10 int f0_ret = JPM_Document_Start(f0_a0, &callback1, &fp);
11 if (f0_ret){ exit(0); }
12
13 // 4) Get property of the image
14 int f1_a2 = 0, int f4_a2 = 0;
15 JPM_Document_Get_Page_Property((void *)f0_a0[0], 0xA, &f1_a2);
16 ...
17 JPM_Document_Get_Page_Property((void *)f0_a0[0], 0xD, &f4_a2);
18
19 // 5) Decode the image
20 JPM_Document_Decompress_Page((void *)f0_a0[0], &callback2);
21
22 // 6) Finish the harness
23 JPM_Document_End((void *)f0_a0[0]);
Fig. 3: An example harness, synthesized by our harness generator. It
tests the JPM parser inside the ldf_jpm.dll library of the application
XnView. The majority of the harness was correct and usable out of
the box. We describe the steps taken to create this harness in §III-A
and in more detail in §IV. Low level details are omitted for brevity.
to discover all callsites. Due to the prevalence of indirect calls
and jump tables, researchers must also use dynamic analysis
to get the concrete values of the call targets.
3 Argument recovery. The harness must also pass valid
arguments to each function call. Reconstructing these arguments
is challenging: the argument could be a pointer to a callback
function (like &callback1 at line 10), a pointer to an integer
(like &f1_a2 at line 15), a constant (like 0xa at line 15), or
many other types. When manually constructing a harness,
the researcher must examine every argument for each API
call, relying on their expertise to determine what the function
expects.
4 Control-flow and data-flow dependence.
It is oftentimes
insufficient to simply produce a list of function calls in the right
order. Moreover, libraries define implicit semantic relationships
among APIs. These relationships manifest in control-flow
dependencies and data-flow dependencies. For example, a
conditional branch between API calls may be required for the
harness to work, like the if-statement at line 11 of the example.
Alternatively, one API may return or update a pointer which is
used by a later API call. Unless these relationships are respected,
the resulting harness will be incorrect, yielding false positives
and spurious crashes. For example, the above code updates array
f0_a0 at line 10, and uses the first element in lines 15, 17, 20,
and 23. In the absence of source code, this step is extremely
challenging, and even the most advanced harness generator
cannot guarantee correctness. Human intuition and experience
can supplement auto-analysis when reverse-engineering.
B. Limitations of Existing Solutions
As Windows does not provide fast process cloning machin-
ery (e.g., Linux’s fork), fuzzers usually start each execution
from the very beginning. Considering the long start-up time of
Windows applications (see Table II), each re-execution wastes
a lot of time to reinitialize the program. Existing solutions (e.g.,
WinAFL) resort to a technique known as persistent mode to
4
Fig. 4: Overview of WINNIE. Given the target program and a set of sample inputs, WINNIE aims to find security vulnerabilities. It uses a
harness generator to synthesize simple harnesses from the execution trace, and then fuzzes harnesses efficiently with our implementation of fork.
overcome the re-execution overhead [68]. In persistent mode,
the fuzzer repeatedly invokes the target function in a tight loop
within the same process, without reinitializing the program each
iteration. To realize the most performance gains, one generally
aims to test as many inputs as possible per new process.
While persistent mode partially addresses the performance
issues of Windows fuzzing, its efficacy is limited by its strict
requirements on the loop body. Specifically, persistent mode
expects harnesses to behave like pure functions, meaning that
harnesses avoid any side-effects, such as leaking memory or
modifying global variables. Otherwise, each execution would
start from a different program state. Since the harness is
repeatedly looped for thousands of iterations, even the smallest
side-effects will gradually accumulate over time, finally leading
to problems like memory leaks, unreproducible crashes and
hangs, and unreliable coverage. For example, a program that
leaks 1MB of memory per iteration will reach WinAFL’s default
memory limit and be terminated. We experienced such errors
very often in practice, and discuss more details later in §VII-A.
Many side-effect errors from persistent mode are difficult
to debug or difficult to circumvent. A common issue is that
persistent mode cannot continue if the target function does not
return to the caller. For example, a program can implement
error handling by simply terminating the program. Because
most inputs generated during fuzzing are invalid (albeit benign),
this still demands constant re-execution, severely degrading
performance. Another common problem is that a program will
open the input file in exclusive mode (i.e., other processes
cannot open the same file) without closing it. This prevents the
fuzzer from updating the input file in the next iteration, breaking
persistent mode. Problems like these limit the applicability and
scalability of persistent mode fuzzers.
C. Our Solutions
We propose WINNIE, an end-to-end system that addresses
aforementioned obstacles to effectively and efficiently fuzz
Windows applications. WINNIE contains two components, a
harness generator that synthesizes harnesses for closed-source
Windows programs with minimal manual effort (§IV), and a
fuzzer that can handle uncooperative target applications with
our efficient fork implementation (§V). Figure 4 shows an
overview of our system. Given the program binary and sample
inputs, our tracer runs the program and meanwhile, collects
dynamic information about the target application, including
API calls, arguments and memory contents. From the trace,
we identify interesting fuzzing targets that handle user input,
including functions in external libraries and locations inside
the main binary. For each fuzzing target, our harness generator
analyzes the traces and reconstructs related API sequences as
a working harness. We test the generated harnesses to confirm
their robustness and effectiveness, and then launch fuzzing
instances with our fork-server to find bugs. In the following
Class
1 Module
2 Call/Jump
3 Return
4 Arg/RetVal
Type
string
inter-module
intra-module
inter-module
intra-module
constants
pointers
What to record
name, path, module
thread id, caller, callee, symbols, args
same as above, only for main .exe
thread id, callee, caller, retval
same as above, only for main .exe
concrete value
address and referenced data (recursively)
TABLE IV: Dynamic information collected by the tracer. We
record detailed information about every inter-module call. We also
record the same information for intra-module calls within the main
binary. If the argument or return value is a pointer, we recursively dump
memory around the pointed location. We then use this information to
construct fuzzing harnesses (§IV).
sections, we will use the harness shown in Figure 3 as an
example to explain the design of each component of WINNIE.
IV. HARNESS GENERATION
To generate the harness, WINNIE followed the four steps
previously outlined in §III-A. Consider XnView as an example:
1 For target discovery (§IV-A), we trace XnView while
opening several JPM files, and then search the traces for input-
related APIs, such as OpenFile and ReadFile.
2 For call-sequence recovery (§IV-B), we search the traces for
function calls related to the fuzzing target. In the example, we
find all the function calls related to the chosen library (lines 10,
15, 17, 20 and 23). We put the call-sequence into the harness,
forming a harness skeleton. The skeleton is now more-or-less
a simple series of API calls, which we then flesh out further.
3 For argument recovery (§IV-C), we analyze the traces to
deduce the prototype for each function in the call sequence.
The traces contain verbose information about APIs between
the main binary and libraries, like arguments and return values.
4 Finally, we establish the relationships (§IV-D) among the
various calls and variables presented in the harness skeleton and
emit the final code after briefly testing (§IV-E) the candidate
harness. WINNIE also points out complicated logic potentially
missed by our tracer (such as the callback function at line 20)
as areas for further improvement.
A. Fuzzing Target Identification
In this step, WINNIE evaluates whether the program can
be fuzzed and tries to identify promising target functions. We
begin by performing dynamic analysis on the target program as
it processes several chosen inputs. Table IV shows a detailed
list of items that the tracer captures during each execution. 1
We record the name and the base address of all loaded modules.
2 For each call and jump that transfers control flow between
modules, our tracer records the current thread ID, the caller and
5
Fuzzerfork-serverpersistent-modefault handlerharnesscrashesinputstargetbinaryHarness GeneratorTarget IdentificationCall-seqRecoveryArgumentCorrection§Vblock-cov.§VIControl-Data-flow§IV.B§IV.A§IV.C§IV.Dcallee addresses, symbols (if available), and arguments. Without
function prototype information, we conservatively treat all CPU
registers and upper stack slots as potential arguments. 3 We
record return values when encountering a return instruction. 4
If any of values fall into accessible memory, we conservatively
treat it as a pointer and dump the referenced memory for further
analysis. To capture multi-level pointer relationships (e.g.,
double or triple pointers), we repeat this process recursively.
For pointers, we also recognize common string encodings (e.g.,
C strings) and record them appropriately.
Using our captured traces, we look for functions which
are promising fuzzing targets. It is commonly believed that
good fuzzing targets have two key features [58, 68]: the library
accepts the user-provided file path as the input, and it opens
the file, parses the content and closes the file. We use these two
features to find candidate libraries for fuzzing. Specifically, for
each function call, we check whether one of its arguments points
to a file path, like C:\my_img.jpm. To detect user-provided
paths, our harness generator accepts filenames as input. Next,
we identify callers of well-known file-related APIs such as
OpenFile and ReadFile. If a library has functions accepting
file paths, or invokes file-related APIs, we consider it is an
input-parsing library and treat it as a fuzzing candidate.
WINNIE also identifies library functions that do not open or
read the file themselves, but instead accept a file descriptor or
an in-memory buffer as input. To identify functions accepting
input from memory, our tracer dissects pointers passed to calls
and checks if the referenced memory contains any content
from the input file. We also verify that the appropriate file-read
APIs were called. To find functions taking file descriptors as
inputs, we inspect all invocations of file-open APIs and track
the opened file descriptors. Then, we check whether the library
invokes file-related APIs on those file descriptors.
Our harness generator focuses primarily on the external
interfaces a library exposes. On the other hand, we do not
record control flow within the same module as these represent
libraries’ internal logic. Because invoking the API through
those interfaces models the same behavior as the original
program, inter-module traces are sufficient for building an
accurate harness. However, we treat the main executable as a
special case and record all control-flow information within it.
This is because the main executable is responsible for calling
out to external libraries. Thus, we also search the intra-module
call-graph of the main executable for suitable fuzzing targets.
WINNIE then expands its search to within the main binary
by analyzing its call-graph. Specifically, WINNIE begins at
the lowest common ancestor (LCA) of I/O functions and the
parsing library APIs we previously identified. In a directed
acyclic graph, the LCA of two nodes is the deepest one that
can reach both. In our case, we search for the lowest node
in the main binary’s callgraph that satisfies two criteria. First,
it should be before the file-read operation so that our fuzzer
can modify the input. Note that even if the fuzzed process has
opened the input file, we still can modify it so the program uses
the new content. Second, the LCA should reach locations that
invoke parsing functions. Figure 5 shows an example callgraph
from the program ACDSee. The function at address 0x5cce80
is the LCA as it reaches two file-related APIs (i.e., OpenFile
and ReadFile) and also invokes the parsing functionality in
ide_acdstd.apl. We also consider the LCA’s ancestors (e.g.,
Fig. 5: A simplified call-graph of the ACDSee program. WINNIE
analyzes the call-graph for fuzzing possible targets, focusing on inter-
module calls and I/O functions. We look for functions that can reach
both I/O functions and also the interesting ones we wish to fuzz. “†”
indicates such functions, known as LCA candidates (§IV-A).
main()) as fallback candidates, if the immediate LCA does not
yield a working harness. In cases where a working LCA is
found, it often is sufficient for making an effective harness.
Our tool can also optionally use differential analysis to refine
the set of candidate fuzzing targets. Given two sets of inputs,
one triggering the target functionality and another not triggering,
WINNIE will compare the two execution traces and locate the
library functions that are specific to the target functionality. We
discard the other functions which are present in both sets of
traces. This feature helps deal with multi-threaded applications
where only one thread performs operations related to the input
file. In any case, differential analysis is optional; it only serves
as an additional criteria to improve harness generation.
B. Call-sequence Recovery
Now that we have identified a candidate fuzzing target, our
goal in this step is to reproduce a series of API calls which
will correctly reach and trigger the functionality we wish to
fuzz. We call such an API sequence a harness skeleton. We
search the traces for function calls related to that library and
copy them to the harness skeleton (lines 10, 15, 17, 20, 23 in
Figure 3). We also reconstruct the functions’ prototypes (e.g.,
argument count and types) with hybrid analysis: we combine
the static analysis provided by IDA Pro [31] or Ghidra [2] with
concrete information retrieved from the dynamic execution
traces. Namely, we apply pointer types to arguments that
were valid addresses in the traces, as the static analysis can
misidentify pointer arguments as integers. Lastly, we attach
auxiliary code that is required to make the harness work, like a
main function, forward function declarations, and helper code
to open or read files (line 6).
Special care must be taken to handle applications which
use multiple threads. In that case, we will only consider the
threads that invoke file-related APIs. This is to avoid adding
irrelevant calls that harm the correctness of the harness. We
encountered several programs that exhibit this behavior, such as
GomPlayer, which had hundreds of irrelevant function calls in
the execution trace. When the program creates multiple threads
within the same library, the trace records an interleaving of
many threads’ function calls combined. However, since we
recorded the thread IDs in our previous step, we can untangle
the threads to focus on them individually. With the per-thread
analysis, we can narrow the number of calls down to just seven.
6
0x5cce800x4011600x43fc500x43f8900x4014f0ReadFile()OpenFile()main()IDP_InitIDP_MetadataIDP_OpenImageWIDP_GetPageInfoIDP_CloseImageacdsee.exe(main binary)ide_acdstd.apl(library)††C. Argument Recovery
In this step, we reconstruct the arguments that should be
passed to each API call in the call sequence recovered in the
previous step. WINNIE attempts to symbolize the raw argument
values recorded in the traces into variables and constants. First,
we identify pointer arguments. We do so empirically through
differential analysis of the trace data. Specifically, the tracer runs
the program with the same input twice, both times with address
space layout randomization (ASLR) enabled [49]. Because
ASLR randomizes memory addresses across different runs, two
pointers passed to the same call site will have different, pseudo-
random values that are accessible addresses both times. If this
is the case, we can infer that the argument is a pointer. For
pointer arguments, we use the concrete memory contents from
the trace, dissecting multiple levels of pointers of necessary.
Otherwise, we simply consider the value of the argument itself.
Next, we determine whether the argument is static or
variable. Values which vary from execution to execution are
variable, and we define names for variables and replace their
uses with new names. Values which remain constant between
runs are static, and we simply pass them as the constant value
seen in the trace (like 0xA and 0xD in Figure 3).
D. Control-Flow and Data-Flow Reconstruction
WINNIE analyzes the program to reflect control-flow and
data-flow dependencies in the harness. Control-flow depen-
dencies represent how the various API calls are logically
related (e.g., the if-statement on line 11 in Figure 3). To find
control-flow dependencies, we apply static analysis. Specifically,
WINNIE analyzes the control-flow between two API calls
for paths from the return value of the invoked function to
a termination condition (e.g., return or exit()). If such a
path is found, WINNIE duplicates the decompiled control-flow
code (e.g., if-statements). The current version of WINNIE
avoids analyzing complex flows involving multiple assignments
or variable operands in the conditional statement; we leave
such cases to a human expert. This is important for accurate
harness generation: neglecting control-flow dependencies causes
incorrect behavior. For example, consider a harness that fails
to reflect an early exit error handling condition in the original
program. The program under normal execution would terminate
immediately, but the harness would proceed onwards to some
unpredictable program state. These kinds of mistakes lead to
unreproducible crashes (i.e., false positives).
Data-flow dependencies represent the relationships among
function arguments and return values. To find data-flow depen-
dencies, WINNIE tries to connect multiple uses of the same
variable between multiple call sites (e.g., f0_a0 in Figure 3).
We consider the following possible cases:
• Simple flows from return values. Return values of past
function calls are commonly reused as arguments for later
calls. We detect these cases by checking if an argument
always has the same value as a past return value. We only
do this for whose values exceed a certain threshold. If we
connected any frequently observed values (e.g., connect
return value 0 as the next argument), we may generate
incorrect harnesses; this resolves many common cases
where functions return object pointers.
• Points-to relationships. Some arguments are retrieved
from memory using pointers returned by previous code.
For instance, an API may return a pointer, whose pointed
contents are used as an argument in a later API call. In
the example harness in Figure 3, line 23 uses an argument
f0_a0 that is loaded from memory, initialized by the
API JPM_Document_Start. When we detect these points-to
relationships in the trace, we reflect them in the harness
as pointer dereferences (i.e., *p). WINNIE also supports
multi-level points-to relationships (e.g., double and triple
pointers), thanks to the tracer’s recursive memory dumping.
• Aliasing. WINNIE defines a variable if it observes one or
more repeated usages. In other words, if the same non-
constant value is used twice as an argument, then the two
uses are considered aliases forming a single variable.
E. Harness Validation and Finalization
Although it covers most common cases, WINNIE’s harness
generator is not foolproof. WINNIE points out parts of the
harness that is unsure about and provides suggestions to help
users further improve it. 1 We report distant API calls where
the second API’s call site is far from the first. In such cases,
our API-based tracer might have missed some logic between
two API calls. 2 We highlight code pointer arguments to users,
which could represent callback function pointers or virtual
method tables. 3 We provide information about file operations
as they are generally important during harness construction.
Once a fuzzing harness has been generated, we perform
a few preliminary tests to evaluate its effectiveness. First, we
check the harness’s stability. We run the harness against several
normal inputs; if the harness crashes, we immediately discard
it. Second, we evaluate the harness’s ability to explore program
states. Specifically, we fuzz the harness for a short period
and check whether the code coverage increases over time. We
discard harnesses that fail to discover new coverage. Lastly, we
test the execution speed of the harness. Of all stable, effective
harnesses, we present the fastest ones to the user.
WINNIE’s goal
is to generate harnesses automatically.
However, the general problem of extracting program behaviors
from runtime traces without source code is very challenging
so there will always be cases it cannot cover. Thus, we aim to
handle most common cases to maximize WINNIE’s ability to
save the human researcher’s time. We observe that in practice it
produces good approximations of valid harnesses, and most of
them can be fuzzed with only minor modifications as shown in
Table VIII. We discuss our system’s limitations and weaknesses
in §VII-C and §VIII.
V. FAST PROCESS CLONING ON WINDOWS
Fork indeed exists on Windows systems [15], but existing
work fails to provide a stable implementation. To support effi-
cient fuzzing of Windows applications, we reverse-engineered
various internal Windows APIs and services and identified a
key source of instability. After overcoming these challenges,
we were able to implement a practical and robust fork-server
for Windows fuzzing. Specifically, our implementation of the
Windows fork corrects the problems related to the CSRSS,
which is a user-mode process that controls the underlying layer
of the Windows environment [54]. If a process is not connected
7
Category
Harness generator
Lines of code
1.6K LoC of C++
2.0K LoC of Python
3.0K LoC of C++
0.5K LoC of C++
TABLE VI: WINNIE components and code size
Component
Dynamic tracer
Synthesizer
Fuzzer
Fork library
Fuzzer
global variables in test programs before and after forking a
new process. For example, we incremented a global counter
in the parent process after each fork and verified that the
child process received the old value. Second, to verify that the
fork implementation is CoW (copy-on-write), we initialized
large amounts of memory in the parent process before forking.
Because the memory footprint of the parent process did
not affect the time taken by fork, we concluded that our
implementation is indeed CoW.
We also briefly measured the speed of fork with WinAFL’s
built-in test program as shown in Table V. On an Intel i7 CPU,
we were able to call our fork 310.9 times/sec per core with a
simple program, which is 4.2× faster than Cygwin’s No-CoW
fork and ∼1.3× slower than the WSL fork. Since we are not
using the same fork mechanism as the one provided by the
Linux kernel but instead mimicking its CoW behavior using
the Windows API, the execution speed is nowhere as fast (e.g.,
>5,000 execs/sec). Even if Windows implementation of fork is
slower than Linux’s, the time regained from avoiding costly re-
executions easily makes up for the overhead of fork. Moreover,
the process creation machinery on Windows is slow in general:
in our experiments, ordinary CreateProcess calls (as used by
WinAFL) only reach speeds of less than 100 execs/sec. Overall,
we believe that the reliability and quality of our Windows fork-
server is comparable to ones used for fuzzing on Unix systems.