more modules if they are assigned to CPU.
3.2 ClickNP programming
3.2.1 Abstraction
ClickNP provides a modular architecture and the basic
processing module is called an element. A ClickNP element
has the following properties:
• Local states. Each element can deﬁne a set of local
variables that are only accessible inside the element.
• Input and output ports. An element can have any num-
ber of input or output ports.
• Handler functions. An element has three handler func-
tions: (1) an initialization handler, which is called once
when the element starts, (2) a processing handler, which
is continuously called to check input ports and process
available data, and (3) a signal handler, which receives
and processes the commands (signals) from the man-
ager thread in the host program.
An output port of an element can connect to an input port
of another element through a channel, as shown in Figure 3(a).
In ClickNP, a channel is basically a FIFO buffer that is writ-
ten to one end and read from the other end. The data unit
of the read/write operations to a channel is called ﬂit, which
has a ﬁxed size of 64 bytes. The format of a ﬂit is shown in
Figure 3(b). Each ﬂit contains a header for meta-data and a
payload of 32 bytes. A large piece of data, e.g., a full-sized
packet, is broken into multiple ﬂits, when ﬂowing among
ClickNP elements. The ﬁrst ﬂit is marked with sop (start of
packet), and the last ﬂit is marked with eop (end of packet).
If the size of the data piece is not 32, the pad ﬁeld of the
last ﬂit indicates how many bytes have been padded to the
payload. We note that breaking large data into ﬂits not only
reduces latency, but also potentially increases parallelism as
different ﬂits of a packet may be processed at different ele-
ments simultaneously. Finally, to fulﬁll a network function,
multiple ClickNP elements can be interconnected to form a
directed processing graph, which is called a ClickNP conﬁg-
uration.
4
(a)
Figure 3:
through a channel. (b) The format of a ﬂit.
(a) Two ClickNP elements are connected
(b)
Clearly, the ClickNP programming abstraction largely re-
sembles Click software router [29]. However, there are three
fundamental differences which make ClickNP more suitable
for FPGA implementation: (1) In Click, edges between el-
ements are C++ function calls and a queue element is re-
quired to store packets. However, in ClickNP, an edge ac-
tually represents a FIFO buffer that can hold actual data.
Additionally, ClickNP channels break the data dependency
among elements and allow them to run in parallel. (2) Un-
like Click, where each input/output port can be either push
or pull, ClickNP has uniﬁed these operations: An element
can only write (push) to any output port, while read (pull)
can do so from any input port. (3) While Click allows an el-
ement to directly call methods of another element (via ﬂow-
based router context), in ClickNP, the coordination among
elements is message-based, e.g., a requester sends a request
message to a responder and gets a response via another mes-
sage. Message-based coordination allows more parallelism
and is more efﬁcient in FPGA compared to coordination through
shared memory, where accessing a shared memory location
has to be serialized and would become a bottleneck.
3.2.2 Language
ClickNP elements are alike objects in an object-oriented
language, and can be deﬁned using such languages, i.e., C++.
Unfortunately, many existing HLS tools support only C. To
leverage the commercial HLS tools, we could write a com-
piler that converts a object-oriented language, e.g. C++, to
C. But this effort is non-trivial. In this work, we take an alter-
native path to extend C language to support element declara-
tion. Figure 4(a) shows a code snippet of element Counter,
which simply counts how many packets have passed. An
element is deﬁned by .element keyword, followed by the
element name and the number of input/output ports. The
keyword .state deﬁnes the state variables of the element, and
.init, .handler, and .signal specify the initialization, process-
ing, and signal handler functions of the element. A set of
built-in functions are implemented to operate on the input
and output ports, as summarized in Table 1.
Similar to Click, ClickNP also uses a simple script to spec-
ify a conﬁguration of a network function. The conﬁguration
has two parts: declarations and connections, following the
similar syntax of Click language [29]. One thing worth not-
ing is that in ClickNP we can use a keyword host to annotate
an element, which will cause the element to be compiled into
CPU binary and executed on CPU.
Table 1: Built-in operations on ClickNP channels.
uint get_input_port()
bool test_input_port(uint id)
ﬂit read_input_port(uint id)
ﬂit peek_input_port(uint id)
set_output_port(uint
void
id, ﬂit x)
ClSignal read_signal()
void set_signal(ClSignal p)
return (uint bitmap)
Get bitmap of all input ports
with available data.
Test the input port indicated by
id.
Read the input port indicated by
id.
Peek input data from the port
indicated by id.
to the output port.
Set a ﬂit
The ﬂit is written to the channel
when the handler returns.
Read a signal from signal port.
Set an output signal on signal
port.
Return value of .handler speci-
ﬁes a bitmap of input port(s) to
be read on next iteration.
3.2.3 ClickNP tool-chain
The ClickNP tool-chain contains a ClickNP compiler as
the front-end, and a C/C++ compiler (e.g., Visual Studio or
GCC) and an HLS tool (e.g., Altera OpenCL SDK or Xilinx
Vivado HLS) as the back-end. As shown in Figure 2, to write
a ClickNP program, a developer needs to divide her code into
three parts: (1) A set of elements, each of which implements
a conceptually simple operation, (2) A conﬁguration ﬁle that
speciﬁes the connectivity among these elements, and (3) A
host manager that initialize each element and control their
behavior during the runtime, e.g., according to the input of
administrators. These three parts of source code are fed into
the ClickNP compiler and translated into intermediate source
ﬁles for both host program and FPGA program. The host
program can be directly compiled by a normal C/C++ com-
piler, while the FPGA program is synthesized using commer-
cial HLS tools. Existing commercial HLS tools can deter-
mine a maximum clock frequency of each element through
timing analysis. Then, the clock of a ClickNP processing
graph is constrained by the slowest element in the graph.
Additionally, HLS tools may also generate an optimization
report which shows the dependency among the operations in
an element. An element is fully pipelined if all dependency
is resolved and the element achieves the optimal throughput
by processing one ﬂit in every clock cycle.
4. PARALLELIZING IN FPGA
As discussed in §2.1, it is critical to fully utilize the paral-
lelism inside FPGA in order to speed up processing. ClickNP
exploits FPGA parallelism both at element-level and inside
an element.
4.1 Parallelism across elements
The modular architecture of ClickNP makes it natural to
exploit parallelisms across different elements. The ClickNP
tool-chain maps each element into a hardware block in FPGA.
ulong count;
.state{
}
.init{
count = 0;
}
.handler{
return (PORT_1);
1 .element Count  {
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24 }
return (PORT_1);
}
.signal{
ClSignal p;
p.Sig.LParam[0] = count;
set_signal(p);
}
if (get_input_port() != PORT_1) {
}
flit x;
x = read_input_port(PORT_1);
if (x.fd.sop) count = count + 1;
set_output_port(PORT_1, x);
(a)
1 Count :: cnt @
2 Tee :: tee
3 host PktLogger :: logger
4
5 from_tor -> cnt -> tee [1] -> to_tor
6 tee [2] -> logger
(b)
Figure 4: ClickNP language to write elements and spec-
ify conﬁgurations. An element annotated with host key-
word is compiled and executed on the CPU. An element
annotated with “@” is required to receive control signals
from the manager thread. “From_tor” and “to_tor” are
two built-in elements that represent input and output of
an Ethernet port on FPGA. The return value of the han-
dler function speciﬁes a bit-mask of input ports that will
be checked in next round.
These logic blocks are interconnected with FIFO buffers, and
can work completely in parallel. To this end, one can think
of each element in a ClickNP conﬁguration as a tiny, inde-
pendent core with customized logic. Packets ﬂow from one
element to another along a processing pipeline. This type
of parallelism is called pipeline parallelism or task paral-
lelism. Furthermore, if a single processing pipeline does not
have enough processing power, we can duplicate multiple
such pipelines in FPGA and divide data into these pipelines
using a load-balancing element, i.e., exploiting data paral-
lelism. For network trafﬁc, there are both data parallelism (at
packet-level or ﬂow-level) and pipeline parallelism that can
be utilized to speed up processing. ClickNP is very ﬂexible
and can be conﬁgured to capture both types of parallelism
with little efforts.
4.2 Parallelism inside element
Unlike CPU, which executes instructions in memory with
limited parallelism, FPGA synthesizes operations into hard-
5
ulong key;
ulong cnt;
1 struct hash_entry
2 {
3
4
5 } A[100];
6
7 .handler {
8
9
10 S1: if (A[idx].key==k)
11
12 S2: A[idx].cnt ++;
13
14
15 }
...
idx = hash (h);
}
...
{
(a)
(b)
Figure 6: Memory scattering.
most elements perform only simple tasks and the read-write
memory dependency shown in Figure 5(b) is the most com-
mon case we have encountered.
One way to remove this memory dependency is to store
data in registers only. Since registers are fast enough to
perform read, computation and write back within one cy-
cle, there would be no read-write dependency at all.
In-
deed, compared to CPU, FPGA has a much larger number
of registers, i.e., 697Kbit for Altera Stratix V, which can be
used whenever possible to reduce memory dependency. The
ClickNP compiler aggressively assigns registers to variables
as long as all accesses to the variable refer to a constant ad-
dress – either the variable is a scalar or an array entry with
constant offset. Certainly, the programmer can use “register”
or “local/global” keywords to explictly instruct the compiler
to place a variable (can also be an array) in register, BRAM
or onboard DDR memory.
For large data, they have to be stored in memory. Fortu-
nately, we can still use a technique called delayed write to
resolve the read-write memory dependency in Figure 5(b).
The core idea is to buffer the new data in a register and delay
the write operation until the next read operation. If the next
read accesses the same location, it will read the value from
the buffer register directly. Otherwise, the read can operate
in parallel with the delayed write operation as they are going
to access different memory locations3. Figure 5(c) shows the
code snippet with delayed write. Since there is no longer
memory dependency in the code, the element can process a
datum in one cycle. By default, ClickNP compiler automat-
ically applies delayed write for an array (generating similar
code as shown in Figure 5(b)).
One subtle issue will occur when using an array of struct
variables. Figure 6(a) shows such an example, where a hash
table is used to maintain a count for every ﬂow. We ﬁnd S2
will have a memory dependency to S1, although they are ac-
cessing different ﬁelds of a struct. The reason is that almost
all current HLS tools will treat a struct array as a single-
dimension array with a large bit-width – equal to the size of
the struct, and use only one arbitrator to control access. We
call this type of memory dependency pseudo dependency, as
physically, the two ﬁelds, key and cnt, can be on different
3Most BRAM in FPGA has two ports.
1
2 S1: y = mem[r.x]+1;
3 S2: mem[r.x] = y;
4
set_output_port (PORT_1, y);
r = read_input_port (PORT_1);
(a)
(b)
r = read_input_port (PORT_1);
} else {
y_temp = buf_val;
y_temp = mem[r.x];
1
2 P1: if ( r.x == buf_addr ) {
3
4
5
6
7
8 S1: y = y_temp + 1;
9 S2: buf_addr = r.x;
10
11
}
mem[buf_addr] = buf_val;
buf_val = y;
set_output_port (PORT_1, y);
(c)
Figure 5: Illustration of dependency. (a) No dependency.
Sn means a pipeline stage, Dn is a datum. (b) Memory
dependency occurs when states are stored in memory and
need to be updated. (c) Resolve memory dependency us-
ing delayed write.
ware logic, and therefore can be evaluated in parallel without
instruction load overhead. If a datum requires multiple de-