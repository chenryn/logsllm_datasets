formal safety analysis of neural networks. Advances in Neural
Information Processing Systems (NIPS), 2018.
[46] N. Papernot, P. D. McDaniel, A. Sinha, and M. P. Wellman.
Towards the science of security and privacy in machine
learning. CoRR, abs/1611.03814, 2016.
[62] S. Wang, K. Pei, W. Justin, J. Yang, and S. Jana. Formal
security analysis of neural networks using symbolic intervals.
27th USENIX Security Symposium, 2018.
[47] O. M. Parkhi, A. Vedaldi, A. Zisserman, et al. Deep face
recognition.
[48] R. Pascanu, J. W. Stokes, H. Sanossian, M. Marinescu, and
A. Thomas. Malware classiﬁcation with recurrent networks.
In Acoustics, Speech and Signal Processing (ICASSP), 2015
IEEE International Conference on. IEEE, 2015.
[63] T.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-
J. Hsieh, and L. Daniel. Evaluating the robustness of neural
networks: An extreme value theory approach. arXiv preprint
arXiv:1801.10578, 2018.
[64] Wikipedia. Operator norm.
https://en.wikipedia.org/wiki/
Operator norm, Accessed in 2017.
[49] J. Peck, J. Roels, B. Goossens, and Y. Saeys. Lower bounds
on the robustness to adversarial perturbations. In Advances
in Neural Information Processing Systems, 2017.
[65] E. Wong and Z. Kolter. Provable defenses against adversarial
In
examples via the convex outer adversarial polytope.
International Conference on Machine Learning, 2018.
(cid:23)(cid:24)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
[66] C. Xiao, B. Li, J. Zhu, W. He, M. Liu, and D. Song.
Generating adversarial examples with adversarial networks.
2018.
[67] C. Xiao, J. Zhu, B. Li, W. He, M. Liu, and D. Song. Spatially
transformed adversarial examples. 2018.
[68] Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon,
Nate Kushman. Pixeldefend: Leveraging generative models
to understand and defend against adversarial examples.
In-
ternational Conference on Learning Representations, 2018.
[69] Yann LeCun, Corinna Cortes, Christopher J.C. Burges. The
mnist database of handwritten digits, Accessed in 2017.
[70] S. Zagoruyko and N. Komodakis. Wide residual networks.
CoRR, 2016.
[71] Z. Zohrevand, U. Gl¨asser, M. A. Tayebi, H. Y. Shahir,
M. Shirmaleki, and A. Y. Shahir. Deep learning based
forecasting of critical infrastructure data. In Proceedings of
the 2017 ACM on Conference on Information and Knowledge
Management, CIKM ’17. ACM, 2017.
Appendix
A. Proof of Proposition 2
We brieﬂy re-state the Proposition and detail the proof.
Proposition. Suppose A is (, δ)-PixelDP for size L in p-
norm metric. For any input x, if for some k ∈ K,
ˆElb(Ak(x)) > e2 max
i:i(cid:4)=k
ˆEub(Ai(x)) + (1 + e)δ,
then the multiclass classiﬁcation model based on label
probabilities ˆE(Ak(x)) is robust to attacks of p-norm L on
input x with probability higher than η.
Proof: Consider any α ∈ Bp(L), and let x
(cid:5)
:= x + α.
From Equation (2), we have with p > η that
ˆE(Ak(x
ˆE(Ai:i(cid:4)=k(x
(cid:5)
(cid:5)
)) ≥ (ˆE(Ak(x)) − δ)/e
≥ (ˆElb(Ak(x)) − δ)/e,
ˆEub(Ai(x)) + δ,
)) ≤ e max
i:i(cid:4)=k
i (cid:7)= k.
Starting from the ﬁrst inequality, and using the hypothesis,
followed by the second inequality, we get
ˆEub(Ai(x)) + (1 + e)δ ⇒
ˆElb(Ak(x)) > e2 max
i:i(cid:4)=k
ˆE(Ak(x
(cid:5)
)) ≥ (ˆElb(Ak(x)) − δ)/e
ˆEub(Ai(x)) + δ
> e max
i:i(cid:4)=k
> ˆE(Ai:i(cid:4)=k(x
(cid:5)
))
which is the robustness condition from Equation (1).
B. Design Choice
Our theoretical results allow the DP DNN to output any
bounded score over labels Ak(x). In the evaluation we used
the softmax output of the DNN, the typical “probabilities”
that DNNs traditionally output. We also experimented with
(cid:23)(cid:24)(cid:18)
Fig. 6: Robust Accuracy, arg max Scores. Using arg max scores
for certiﬁcation yields better accuracy bounds (see Fig. 2(b)), both
because the scores are further appart and because the measurement
error bounds are tighter.
Fig. 7: Laplace vs. Gaussian. Certiﬁed accuracy for a ResNet
on the CIFAR-10 dataset, against 1-norm bounded attacks. The
Laplace mechanism yields better accuracy for low noise levels, but
the Gaussian mechanism is better for high noise ResNets.
using arg max scores, transforming the probabilities in a
zero vector with a single 1 for the highest score label. As
each dimension of this vector is in [0, 1], our theory applies
as is. We observed that arg max scores were a bit less
robust empirically (lower accuracy under attack). However,
as shown on Fig. 6 arg max scores yield a higher certiﬁed
accuracy. This is both because we can use tighter bounds
for measurement error using a Clopper-Pearson interval,
and because the arg max pushes the expected scores further
apart, thus satisfying Proposition 1 more often.
We also study the impact of the DP mechanism used on
certiﬁed accuracy for 1-norm attacks. Both the Laplace and
Gaussian mechanisms can be used after the ﬁrst convolution,
by respectively controlling the Δ1,1 or Δ1,2 sensitivity.
Fig. 7 shows that for our ResNet, the Laplace mechanism is
better suited to low levels of noise: for L = 0.1, it yields a
slightly higher accuracy (90.5% against 88.9%), as well as
better certiﬁed accuracy with a maximum robustness size of
1-norm 0.22 instead of 0.19, and a robust accuracy of 73%
against 65.4% at the 0.19 threshold. On the other hand, when
adding more noise (e.g. L = 0.3), the Gaussian mechanism
performs better, consistently yielding a robust accuracy 1.5
percentage point higher.
C. Attack Details
All evaluation results (§IV) are based on the attack
from Carlini and Wagner [7], specialized to better attack
PixelDP (see parameters and adaptation in §IV-A). We also
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
implemented variants of the iterative Projected Gradient
Descent (PGD) attack described in [37], modiﬁed to average
gradients over 15 noise draws per step, and performing
each attack 15 times with a small random initialization. We
implemented two version of this PGD attack.
2-norm Attack: The gradients are normalized before ap-
plying the step size, to ensure progress even when gradients
are close to ﬂat. We perform k = 100 gradient steps
and select a step size of 2.5L
k . This heuristic ensures that
all feasible points within the 2-norm ball can be reached
after k steps. After each step, if the attack is larger than
L, we project
it on the 2-norm ball by normalizing it.
Under this attack, results were qualitatively identical for
all experiments. The raw accuracy numbers were a few
percentage points higher (i.e. the attack was slightly less
efﬁcient), so we kept the results for the Carlini and Wagner
attack.∞-norm Attack: We perform max(L + 8, 1.5L) gradient
steps and maintain a constant step of size of 0.003 (which
corresponds to the minimum pixel increment in a discrete
[0, 255] pixel range). At the end of each gradient step we
clip the size of the perturbation to enforce a perturbation
within the ∞-norm ball of the given attack size. We used
this attack to compare PixelDP with models from Madry
and RobustOpt (see results in Appendix D).
Finally, we performed sanity checks suggested in [2]. The
authors observe that several heuristic defenses do not ensure
the absence of adversarial examples, but merely make them
harder to ﬁnd by obfuscating gradients. This phenomenon,
also referred to as gradient masking [46], [59], makes the
defense susceptible to new attacks crafted to circumvent
that obfuscation [2]. Although PixelDP provides certiﬁed
accuracy bounds that are guaranteed to hold regardless
of the attack used, we followed guidelines from [2], to
to rule out obfuscated gradients in our empirical results.
We veriﬁed three properties that can be symptomatic of
problematic attack behavior. First, when growing T , the
accuracy drops to 0 on all models and datasets. Second, our
attack signiﬁcantly outperforms random sampling. Third, our
iterative attack is more powerful than the respective single-
step attack.
D. ∞-norm Attacks
As far as ∞-norm attacks are concerned, we acknowledge
that the size of the attacks against which our current PixelDP
defense can certify accuracy is substantially lower than that
of previous certiﬁed defenses. Although previous defenses
have been demonstrated on MNIST and SVHN only, and for
smaller DNNs, they achieve ∞-norm defenses of T∞ = 0.1
with robust accuracy 91.6% [65] and 65% [52] on MNIST.
On SVHN, [65] uses T∞ = 0.01, achieving 59.3% of
certiﬁed accuracy. Using the crude bounds we have between
p-norms makes a comparison difﬁcult in both directions.
Mapping ∞-norm bounds in 2-norm gives T2 ≥ T∞, also
(cid:23)(cid:24)(cid:19)
Fig. 8: Accuracy under ∞-norm attacks for PixelDP and
Madry. The Madry model, explicitly trained against ∞-norm
attacks, outperforms PixelDP. The difference increases with the
size of the attack.
Fig. 9: Accuracy under ∞-norm attacks for PixelDP and
RobustOpt. PixelDP is better up to L∞ = 0.015, due to its support
of larger ResNet models. For attacks of ∞-norm above this value,
RobustOpt is more robust.
yielding very small bounds. On the other hand, translating
2-norm guarantees into ∞-norm ones (using that (cid:4)x(cid:4)2 ≤
√
n(cid:4)x(cid:4)∞ with n the size of the image) would require a
2-norm defense of size T2 = 2.8 to match the T∞ = 0.1
bound from MNIST, an order of magnitude higher than what
we can achieve. As comparison points, our L = 0.3 CNN
has a robust accuracy of 91.6% at T = 0.19 and 65%
at T = 0.39. We make the same observation on SVHN,
where we would need a bound at T2 = 0.56 to match the
T∞ = 0.01 bound, but our ResNet with L = 0.1 reaches
a similar robust accuracy as RobustOpt for T2 = 0.1. This
calls for the design ∞-norm speciﬁc PixelDP mechanisms
that could also scale to larger DNNs and datasets.
On Figures 8 and 9, we show PixelDP’s accuracy under
∞-norm attacks, compared to the Madry and RobustOpt
models, both trained speciﬁcally against this type of attacks.
On CIFAR-10, the Madry model outperforms PixelDP: for
Lattack = 0.01, PixelDP’s accuracy is 69%, 8 percentage
points lower than Madry’s. The gap increases until PixelDP
arrives at 0 accuracy for Lattack = 0.06, with Madry still
having 22%.
On SVHN, against the RobustOpt model, trained with
robust optimization against ∞-norm attacks, PixelDP is
better up to L∞ = 0.015, due to its support of larger ResNet
models. For attacks of ∞-norm above this value, RobustOpt
is more robust.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
E. Extension to regression
A previous version of this paper contained an incorrect
claim in the statement of Lemma 1 for outputs that can be
negative. Because the paper focused on classiﬁcation, where
DNN scores are in [0, 1], the error had no impact on the
claims or experimental results for classiﬁcation. Lemma 2,
below, provides a correct version of Lemma 1 for outputs
that can be negative, showing how PixelDP can be extended
to support regression problems:
Lemma 2. (General Expected Output Stability Bound)
Suppose a randomized function A, with bounded output
A(x) ∈ [a, b], a, b ∈ R, with a ≤ 0 ≤ b, satisﬁes (, δ)-DP.
Let A+(x) = max(0, A(x)) and A−(x) = − min(0, A(x)),
so that A(x) = A+(x)− A−(x). Then the expected value of
its output meets the following property: for all α ∈ Bp(1),
E(A(x + α)) ≤ eE(A+(x)) − e
−aδ,
E(A(x + α)) ≥ e
−E(A+(x)) − eE(A−(x)) − e
−bδ + aδ.
The expectation is taken over the randomness in A.
Proof: Consider any α ∈ Bp(1), and let x
−E(A−(x)) + bδ − e
:=
(cid:5)
(cid:6)
(cid:5)
)) ≥ e
(cid:5)
b
0 P (A(x) >
so by the (, δ)-DP property of A via Equa-
)) ≤ eE(A+(x)) + bδ
−bδ. Similarly,
)) ≥
−aδ. Putting these four inequalities to-
x + α. Observe that E(A+(x)) =
t)dt,
tion (2), we have E(A+(x
and E(A+(x
E(A−(x
−E(A+(x)) + e
e
gether concludes the proof.
)) ≤ eE(A−(x)) − aδ and E(A−(x
−E(A+(x)) − e
(cid:5)
(cid:5)
Following Lemma 2, supporting regression problems in-
volves three steps. First, if the output is unbounded, one
must use (, 0)-DP (e.g. with the Laplace mechanism). If
the output is bounded, one may use (, δ)-DP. The output
may be bounded either naturally, because the speciﬁc task
has inherent output bounds, or by truncating the results to a
large range of values and using a comparatively small δ.
Second, instead of estimating the expected value of the
randomized prediction function, we estimate both A+(x)
and A−(x). We can use Hoeffding’s inequality [25] or
Empirical Bernstein bounds [39] to bound the error.
Third, following Lemma 2, we bound A+(x) and A−(x)
separately using the DP Expected Output Stability Bound,
to obtain a bound on E(A(x)) = E(A+(x)) − E(A−(x)).
(cid:23)(cid:24)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply.