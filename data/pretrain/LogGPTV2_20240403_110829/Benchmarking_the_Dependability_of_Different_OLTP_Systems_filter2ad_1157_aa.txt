title:Benchmarking the Dependability of Different OLTP Systems
author:Marco Vieira and
Henrique Madeira
Benchmarking the Dependability of Different OLTP Systems 
  Polytechnic Institute of Coimbra, DEIS-CISUC 
Marco Vieira
3031 Coimbra  - Portugal 
PI:EMAIL  
Abstract 
On-Line  Transaction  Processing  (OLTP)  systems 
constitute the kernel of the information systems used today 
to  support  the  daily  operations  of  most  organizations. 
Although  these  systems  comprise  the  best  examples  of 
complex  business-critical  systems,  no  practical  way  has 
been  proposed  so  far  to  characterize  the  impact  of  faults 
in  such  systems  or  to  compare  alternative  solutions 
concerning  dependability  features.  This  paper  presents  a 
practical  example  of  benchmarking  key  dependability 
features  of  four  different  transactional  systems  using  a 
first  proposal  of  dependability  benchmark  for  OLTP 
application  environments.  This  dependability  benchmark 
is  an  extension  to  the  TPC-C  standard  performance 
benchmark,  and  specifies  the  measures  and  all  the  steps 
required  to  evaluate  both  the  performance  and  depend-
ability  features  of  OLTP  systems.  Two  different  versions 
of  the  Oracle  transactional  engine  running  over  two 
different operating systems were evaluated and compared. 
The results show that dependability benchmarking can be 
successfully applied to OLTP application environments. 
1. Introduction 
On-Line  Transaction  Processing  (OLTP)  systems  con-
stitute the kernel of  the information systems used to 
support the daily operations of most of the businesses and 
comprise  the  best  examples  of  business-critical  applica-
tions.  However,  in  spite  of  the  importance  of  having 
dependability  benchmarks  for  this  important  class  of 
systems,  the  reality  is  that  no  benchmark  has  been 
proposed so far to characterize the impact of faults in such 
systems  or  to  compare  alternative  solutions  concerning 
dependability features.  
Transactional  systems  industry  holds  a  reputed  infra-
structure  for  performance  evaluation  and  the  Transaction 
Processing  Performance  Council  (TPC)  BenchmarkTM  C 
(TPC-C)  [1]  is  one  of  the  most  important  and  well-
established  benchmarks.  Although  TPC-C  specifies  that 
dependability features of the system must ensure that data 
can  be  recovered  from  any  point  during  the  benchmark 
run,  the  specification  does  not  include  any  procedure  to 
evaluate  the  effectiveness  of  those  dependability  features 
or to measure their impact on the performance. 
This paper presents a practical example of benchmark-
ing  key  dependability  features  of  four  different  transac-
Henrique Madeira
University of Coimbra, DEI-CISUC  
3030 Coimbra - Portugal  
PI:EMAIL
tional  systems  using  a  first  proposal  of  a  dependability 
benchmark  for  OLTP  application  environments  –  the 
DBench-OLTP benchmark. This dependability benchmark 
is an extension to the TPC-C benchmark, and specifies the 
measures  and  all  the  steps  required  to  evaluate  both  the 
performance and dependability features of OLTP systems.  
The  TPC-C  standard  benchmark  includes  two  major 
components:  a  workload  and  a  set  of  performance 
measures.  The  DBench-OLTP  dependability  benchmark 
adds 
two  new  elements:  1)  measures  related  to 
dependability;  and  2)  a  faultload.  The  faultload  repre-
sents a set of faults that emulate real faults experienced by 
OLTP systems in the field. The measures characterize the 
dependability  features  of  the  system  under  benchmark  in 
the presence of the faultload. 
The  complete  specification  of  DBench-OLTP 
is 
available in [2]. This specification is in a form of addenda 
to TPC-C specification, and includes a set of extra clauses 
that  define  the  new  elements  and  some  small  changes 
needed in the benchmarking setup. 
The  goal  of  the  benchmarking  experiments  presented 
in this paper is to compare and rank four different transac-
tional systems. These systems can be considered as possi-
ble alternatives for small and medium size OLTP applica-
tions such as typical client-server database applications or 
e-commerce applications. In this sense, the benchmarking 
experiments  presented  give  the  answer  to  the  following 
question:  which  one  of  the  four  benchmarked  systems  is 
the  best  choice 
typical  OLTP  application, 
considering both performance and dependability aspects? 
for  a 
Since  the  DBench-OLTP  follows  the  benchmarking 
style of TPC (i.e., it is a  specification to be implemented 
instead  of  a  set  of  programs  ready  to  run),  running 
DBench-OLTP starting  from  the benchmark specification 
includes  the  following  steps:  1)  implementation  (work-
load  and  faultload)  and  setup  preparation  for  each  target 
system;  2)  running  the  benchmark  in  each  system;  3) 
collecting the results; and 4) calculating the measures. As 
a final step the measures are used to compare the systems 
benchmarked and rank them. 
The  paper  is  organized  as  follows:  section  2  presents 
an outline of the DBench-OLTP benchmark and section 3 
introduces the  goal  of the  experiments.  The  results  are 
presented  and  discussed  in  section  4.  Section  5  discusses 
the  effort  required  to  run  the  benchmark  and  section  6 
concludes the paper. 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:27:42 UTC from IEEE Xplore.  Restrictions apply. 
2. DBench-OLTP specification outline 
This section presents an overview of the DBench-OLTP 
dependability  benchmark  (the  complete  specification  can 
be  found  at  [2]).  Since  there  is  already  an  established 
performance  benchmark  for  OLTP  systems  (the  TPC-C), 
the  DBench-OLTP  dependability  benchmark  uses  the 
setup,  the  workload,  and  the  performance  measures 
specified  in  TPC-C  and  adds  two  new  elements:  1) 
measures  related  to  dependability;  and  2)  a  faultload.
The  faultload  represents  a  set  of  faults  that  emulate  real 
faults  experienced  by  OLTP  systems  in  the  field.  The 
measures  characterize  the  dependability  features  of  the 
system under benchmark in the presence of the faultload. 
As the TPC-C consists of a detailed specification (i.e., 
the  benchmark  is  a  document),  the  new  DBench-OLTP 
dependability benchmark extends the TPC-C specification 
(using an addenda) in order to define all the new elements. 
In  order  to  run  the  DBench-OLTP  dependability  bench-
mark it is necessary to implement the TPC-C in the target 
system,  according  to  the  functional  description  provided 
by TPC-C specification, and the new benchmark elements 
(measures related to dependability and faultload) required 
by this new dependability benchmark. In practice, existing 
code  and  examples  are  adapted  to  new  target  systems, 
which greatly simplifies the benchmark implementation.  
Figure 1 presents the test configuration required to run 
this  dependability  benchmark.  As  in  TPC-C,  the  main 
elements are the System Under Test (SUT) and the Driver 
System. The SUT consists of one or more processing units 
used  to  run  the  workload  (set  of  transactions  submitted), 
and  whose  performance  and  dependability  will  be  evalu-
ated.  The  goal  of  the  driver  system  is  to  control  all  the 
aspects  concerning  the  benchmark  run,  during  which  it 
submits the workload, injects  the  faultload,  and  collects 
information on the SUT behavior. 
Driver 
System
TPC-C Transactions 
Faults
Information on 
system behavior 
System 
Under Test
(SUT)
Figure 1. DBench-OLTP test configuration.
A  DBench-OLTP  run  includes  two  main  phases. 
During  Phase  1,  the  TPC-C  workload  is  run  without  any 
(artificial)  faults.  This  phase  corresponds  to  a  TPC-C 
measurement  interval,  and  follows 
the  requirements 
specified  in  the  TPC-C  standard  specification  (see  [1]). 
The  goal  of  this  first  phase  is  to  collect  baseline 
performance  measures.  During  Phase  2,  the  TPC-C 
workload is run in the presence of the faultload to measure 
the  impact  of  faults  on  specific  aspects  of  the  target 
system  dependability.  As  shown  in  Figure  2,  Phase  2  is 
composed  by  several  independent  injection  slots.  An 
injection  slot  is  a  measurement  interval  during  which  the 
TPC-C workload is run and one fault from the faultload is 
injected. 
The SUT state is explicitly restored in the beginning of 
each  injection  slot  and  the  effects  of  the  faults  do  not 
accumulate  across  different  slots.  The  test  in  each 
injection slot is conducted in a steady state condition that 
represents the state in which the system is able to maintain 
its  maximum  transaction  processing  throughput.  The 
system achieves a steady state condition after a given time 
executing transactions (steady state time). 
Phase 1
Phase 2 
Injection 
Injection 
Injection 
Slot 1
Slot 2
Slot 3 
Injection 
Slot N
Steady state 
condition 
Fault 
activation 
Recovery 
start 
Recovery 
end
Time
Data 
Integrity 
Testing
Injection Slot
(Start) 
Steady 
state 
time
Injection 
time 
Detection 
time 
Recovery 
time 
Keep
time 
Injection Slot
(End)
Figure 2. Benchmark run and injection slots.
A  fault from the  faultload is injected a certain amount 
of time (injection time) after the steady state condition has 
been  achieved.  Due  to  the  fact  that  for  some  types  of 
faults  the  time  needed  to  detect  the  effects  of  a  fault  is 
highly  human  dependent,  a  typical  detection  time  is  con-
sidered for  each fault.  After that  detection  time  an  error 
detection  procedure  is  executed  to  evaluate  the  effects  of 
the fault and the required recovery procedure is started (if 
an error is detected). The recovery time represents the time 
needed  to  execute  the  recovery  procedure.  When  the  re-
covery procedure completes, the workload continues to run 
during a keep time to evaluate the system behavior. 
After  the  workload  end,  a  set  of  application  consis-
tency  tests  is  performed  to  check  possible  data  integrity 
violations caused by the fault injected. The integrity tests 
are performed on the application data (i.e., the data in the 
database  tables  after  running  the  workload)  and  use  both 
business  rules  (defined  in  the  TPC-C  specification)  and 
the database metadata to assure a comprehensive test. 
The duration of each injection slot depends on the fault 
injected  and  correspondent  times  (steady  state  time, 
injection  time,  detection  time,  recovery  time,  and  keep 
time).  However,  the  workload  must  run  for  at  least  15 
minutes after the steady state condition has been achieved. 
2.1. Measures 
The  DBench-OLTP  dependability  benchmark 
is 
composed by three sets of measures: baseline performance 
measures,  performance  measures  in  the  presence  of  the 
faultload, and dependability measures.  
The baseline performance measures reported are the 
number  of  transactions  executed  per  minute  (tpmC)  and 
price-per-transaction 
($/tpmC).  These  measures  are 
inherited  from  the  TPC-C  standard  benchmark  and  are 
obtained  during  Phase  1.  However,  in  the  context  of  this 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:27:42 UTC from IEEE Xplore.  Restrictions apply. 
dependability  benchmark,  these  measures  represent  a 
baseline  performance  instead  of  optimized  pure  perform-
ance  (as  is  the  case  of  TPC-C),  and  should  consider  a 
good compromise between performance and dependability. 
The  performance  measures  in  the  presence  of  the 
faultload are:
(cid:131)(cid:3)Tf – Number of transactions executed per minute in the 
presence  of  the  faultload  during  Phase  2  (measures  the 
impact  of  faults  in  the  performance  and  favors  systems 
with better fault tolerance capabilities and fast recovery). 
(cid:131)(cid:3)$/Tf  –  Price-per-transaction  in  the  presence  of  faults 
specified  in  the  faultload  during  Phase  2  (measures  the 
relative  benefit  of  including  fault  handling  mechanisms 
in the target systems in terms of the price). 
(cid:131)(cid:3)Tf/tpmC – Performance decreasing ratio due to faults. 
The dependability measures reported are: 
(cid:131)(cid:3)Ne – Number of data errors detected by the consistency 
tests  and  metadata  tests  (measures  the  impact  of  faults 
on the data integrity). 
(cid:131)(cid:3)AvtS  –  Availability  from  the  SUT  point-of-view  in  the 
presence  of  the  faultload  during  Phase  2  (measures  the 
amount  of  time  the  system  is  available  from  the  SUT 
point-of-view). The system is available when it is able to 
respond  to  at  least  one  terminal  within  the  minimum 
response time defined for each type of transaction by the 