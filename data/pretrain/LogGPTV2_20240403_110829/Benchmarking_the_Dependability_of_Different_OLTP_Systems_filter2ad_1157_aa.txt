# Benchmarking the Dependability of Different OLTP Systems

**Authors:**
- Marco Vieira, Polytechnic Institute of Coimbra, DEIS-CISUC, 3031 Coimbra, Portugal, [Email]
- Henrique Madeira, University of Coimbra, DEI-CISUC, 3030 Coimbra, Portugal, [Email]

## Abstract
On-Line Transaction Processing (OLTP) systems form the core of information systems used to support daily operations in most organizations. Despite being prime examples of complex, business-critical systems, no practical method has been proposed to characterize the impact of faults or to compare alternative solutions concerning dependability features. This paper presents a practical example of benchmarking key dependability features of four different transactional systems using a first proposal of a dependability benchmark for OLTP application environments. This dependability benchmark extends the TPC-C standard performance benchmark and specifies the measures and all steps required to evaluate both the performance and dependability features of OLTP systems. Two different versions of the Oracle transactional engine running on two different operating systems were evaluated and compared. The results demonstrate that dependability benchmarking can be successfully applied to OLTP application environments.

## 1. Introduction
On-Line Transaction Processing (OLTP) systems are at the heart of information systems used to support the daily operations of most businesses and represent some of the best examples of business-critical applications. However, despite their importance, no benchmark has been proposed to characterize the impact of faults in such systems or to compare alternative solutions concerning dependability features.

The transactional systems industry has a well-established infrastructure for performance evaluation, with the Transaction Processing Performance Council (TPC) Benchmark C (TPC-C) being one of the most important and well-established benchmarks. While TPC-C specifies that the system's dependability features must ensure data recovery from any point during the benchmark run, it does not include procedures to evaluate the effectiveness of these features or measure their impact on performance.

This paper presents a practical example of benchmarking key dependability features of four different transactional systems using a first proposal of a dependability benchmark for OLTP application environments, called DBench-OLTP. This dependability benchmark is an extension to the TPC-C benchmark and specifies the measures and all the steps required to evaluate both the performance and dependability features of OLTP systems.

The TPC-C standard benchmark includes two major components: a workload and a set of performance measures. The DBench-OLTP dependability benchmark adds two new elements: 1) measures related to dependability, and 2) a faultload. The faultload represents a set of faults that emulate real faults experienced by OLTP systems in the field. The measures characterize the dependability features of the system under benchmark in the presence of the faultload.

The complete specification of DBench-OLTP is available in [2]. This specification is in the form of addenda to the TPC-C specification and includes a set of extra clauses that define the new elements and some small changes needed in the benchmarking setup.

The goal of the benchmarking experiments presented in this paper is to compare and rank four different transactional systems. These systems can be considered as possible alternatives for small and medium-sized OLTP applications, such as typical client-server database applications or e-commerce applications. The benchmarking experiments answer the following question: which of the four benchmarked systems is the best choice for a typical OLTP application, considering both performance and dependability aspects?

Since DBench-OLTP follows the benchmarking style of TPC (i.e., it is a specification to be implemented rather than a set of ready-to-run programs), running DBench-OLTP from the benchmark specification involves the following steps: 1) implementation (workload and faultload) and setup preparation for each target system; 2) running the benchmark in each system; 3) collecting the results; and 4) calculating the measures. As a final step, the measures are used to compare and rank the benchmarked systems.

The paper is organized as follows: Section 2 presents an outline of the DBench-OLTP benchmark, and Section 3 introduces the goal of the experiments. The results are presented and discussed in Section 4. Section 5 discusses the effort required to run the benchmark, and Section 6 concludes the paper.

## 2. DBench-OLTP Specification Outline
This section provides an overview of the DBench-OLTP dependability benchmark (the complete specification can be found in [2]). Since there is already an established performance benchmark for OLTP systems (TPC-C), the DBench-OLTP dependability benchmark uses the setup, workload, and performance measures specified in TPC-C and adds two new elements: 1) measures related to dependability, and 2) a faultload. The faultload represents a set of faults that emulate real faults experienced by OLTP systems in the field. The measures characterize the dependability features of the system under benchmark in the presence of the faultload.

As TPC-C consists of a detailed specification (i.e., the benchmark is a document), the new DBench-OLTP dependability benchmark extends the TPC-C specification (using addenda) to define all the new elements. To run the DBench-OLTP dependability benchmark, it is necessary to implement the TPC-C in the target system according to the functional description provided by the TPC-C specification and the new benchmark elements (measures related to dependability and faultload) required by this new dependability benchmark. In practice, existing code and examples are adapted to new target systems, which greatly simplifies the benchmark implementation.

Figure 1 shows the test configuration required to run this dependability benchmark. As in TPC-C, the main elements are the System Under Test (SUT) and the Driver System. The SUT consists of one or more processing units used to run the workload (set of transactions submitted), and whose performance and dependability will be evaluated. The driver system controls all aspects of the benchmark run, submitting the workload, injecting the faultload, and collecting information on the SUT behavior.

![DBench-OLTP Test Configuration](figure1.png)

A DBench-OLTP run includes two main phases. During Phase 1, the TPC-C workload is run without any (artificial) faults. This phase corresponds to a TPC-C measurement interval and follows the requirements specified in the TPC-C standard specification (see [1]). The goal of this first phase is to collect baseline performance measures. During Phase 2, the TPC-C workload is run in the presence of the faultload to measure the impact of faults on specific aspects of the target system's dependability. As shown in Figure 2, Phase 2 is composed of several independent injection slots. An injection slot is a measurement interval during which the TPC-C workload is run and one fault from the faultload is injected.

The SUT state is explicitly restored at the beginning of each injection slot, and the effects of the faults do not accumulate across different slots. The test in each injection slot is conducted in a steady-state condition, representing the state in which the system can maintain its maximum transaction processing throughput. The system achieves a steady-state condition after a given time executing transactions (steady-state time).

![Benchmark Run and Injection Slots](figure2.png)

A fault from the faultload is injected a certain amount of time (injection time) after the steady-state condition has been achieved. For some types of faults, the time needed to detect the effects of a fault is highly human-dependent, so a typical detection time is considered for each fault. After the detection time, an error detection procedure is executed to evaluate the effects of the fault, and the required recovery procedure is started (if an error is detected). The recovery time represents the time needed to execute the recovery procedure. When the recovery procedure completes, the workload continues to run during a keep time to evaluate the system behavior.

After the workload ends, a set of application consistency tests is performed to check for possible data integrity violations caused by the injected fault. The integrity tests are performed on the application data (i.e., the data in the database tables after running the workload) and use both business rules (defined in the TPC-C specification) and the database metadata to ensure a comprehensive test.

The duration of each injection slot depends on the fault injected and the corresponding times (steady-state time, injection time, detection time, recovery time, and keep time). However, the workload must run for at least 15 minutes after the steady-state condition has been achieved.

### 2.1. Measures
The DBench-OLTP dependability benchmark consists of three sets of measures: baseline performance measures, performance measures in the presence of the faultload, and dependability measures.

The baseline performance measures reported are the number of transactions executed per minute (tpmC) and price-per-transaction ($/tpmC). These measures are inherited from the TPC-C standard benchmark and are obtained during Phase 1. However, in the context of this dependability benchmark, these measures represent a baseline performance rather than optimized pure performance (as in TPC-C) and should consider a good compromise between performance and dependability.

The performance measures in the presence of the faultload are:
- **Tf**: Number of transactions executed per minute in the presence of the faultload during Phase 2 (measures the impact of faults on performance and favors systems with better fault tolerance capabilities and fast recovery).
- **$/Tf**: Price-per-transaction in the presence of faults specified in the faultload during Phase 2 (measures the relative benefit of including fault handling mechanisms in the target systems in terms of price).
- **Tf/tpmC**: Performance decreasing ratio due to faults.

The dependability measures reported are:
- **Ne**: Number of data errors detected by the consistency tests and metadata tests (measures the impact of faults on data integrity).
- **AvtS**: Availability from the SUT point-of-view in the presence of the faultload during Phase 2 (measures the amount of time the system is available from the SUT point-of-view). The system is available when it is able to respond to at least one terminal within the minimum response time defined for each type of transaction by the TPC-C specification.