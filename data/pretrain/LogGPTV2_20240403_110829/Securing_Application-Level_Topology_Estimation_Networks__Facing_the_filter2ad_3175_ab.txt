(a) Prediction Error
(b) Vivaldi coordinates map-
ping before attack
(c) Vivaldi coordinates map-
ping after attack
Fig. 2. Frog-boiling attack against Vivaldi, 500 nodes (10% malicious) on PlanetLab.
We illustrate the eﬀects of a frog-boiling attack conducted by a small group of
attackers on the accuracy of Vivaldi on a real-life PlanetLab deployment of 500 nodes,
out of which 10% act maliciously. We measure accuracy by evaluating the prediction
error deﬁned as:
Errorpred = |RTTAct − RTTEst|
where RTTAct is the measured RTT and RTTEst is the estimated RTT. Fig. 2(a) displays
the median prediction error between all pairs of nodes. In this experiment, malicious
nodes start the attack after 600 seconds, moving their coordinates only 250 microsec-
onds every time they report their coordinate, and thus gradually increasing the predic-
tion error over time. We also plot the coordinates of nodes before and after the attack
has an eﬀect, in Fig. 2(b) and Fig. 2(c) respectively. The attack thus results in nodes
moving away from their correct coordinates and also away from the origin.
3.2 Complex Attack Strategies
Prior work has considered only single attack strategies, where a malicious node applies
the same attack (inﬂation, deﬂation, oscillation, frog-boiling, network-partition) for the
entire duration of the attack and all nodes apply the same attack. However, single attack
strategies can be easily detected using techniques that leverage change-point detection
methods. We extend these scenarios to more complex ones, by assuming that not only
one single attack is applied by all the malicious nodes, but sequences of diﬀerent attacks
 0 20 40 60 80 100 120 140 160 0 300 600 900 1200 1500 1800Prediction Error (milliseconds)Time (seconds)No AttackFrog-Boiling-400-200 0 200 400-400-200 0 200 400-400-200 0 200 400-400-200 0 200 400can be constructed. Sequences of diﬀerent attacks do raise the stakes signiﬁcantly, since
the observed patterns are less easy to detect. We also consider cases when attackers do
not all apply the same attack.
Single random attack scenario. One way to extend in a straightforward way the single
attack strategy is to consider the case where nodes do not perform all the same attack.
In this case, each node randomly selects one of the ﬁve single attacks, and applies no
attack for some time, then switches to the randomly selected attack. This designates
that one malicious node may conduct the inﬂation attack, while another malicious node
conducts the frog-boiling attack. We refer to this attack strategy as Single-Random.
Two attack scenario. Another extension of the single attack strategy is a scenario
where an attacker alternates between any of the ﬁve single attacks, interleaving them
with a period of no attack. Speciﬁcally, such a strategy is composed of four equal time
slots, the ﬁrst time slot is a non attacking slot, the second consists of one of the ﬁve
single attacks, followed by another non attacking slot, and ﬁnally the fourth time slot is
a second single attack. The idea behind this model is to see how the existing detection
methods, as well as the methods we propose in this paper, perform in comparison to
single attack scenarios. We experiment with several of such scenarios and select the
following as representative: Deﬂation - Frog-Boiling, Oscillation - Inﬂation, Network-
Partition - Oscillation.
Sequence attack scenario. We model more complex attack scenarios, where the at-
tacker applies diﬀerent sequences of attacks, by using a Markov chain model. The states
of such a chain represent all the diﬀerent single attacks including the No Attack state
in which an attacker does not apply an attack. The Markov chain is presented in Fig-
ure 3. This Markov chain is irreducible, as the state space is one single communicating
class, meaning that every state is accessible from every state. We consider an irreducible
chain, as we assume that the attacker can change the current attack strategy to any other
attack, and even stop attacking for a while. Therefore, an attacker can execute every
attack at any time, independently of what he has executed previously. Furthermore, the
chain is aperiodic, as a return to a speciﬁc state can happen at irregular times. An attack
that was already executed previously might be utilized again from time to time. Summa-
rizing, we can say that the Markov chain is ergodic, as it is aperiodic, irreducible and
positive recurrent. Such an ergodic chain allows to visit individual states indeﬁnitely
often and thus leads to more complex scenarios.
The transition probabilities presented in Figure 3 reﬂect several design goals for
generating sequences of attacks. From the No Attack state, each attack is equally prob-
able, except the probability that no transition (and therefore no attack) is only 10%,
therefore the transition to any attack state has the probability 18%. We chose these tran-
sition probabilities to avoid the risk of the Markov chain remaining in the No Attack
state. From an attack state the transitions to every other attack state are equally proba-
ble with 15%. This results in the transition probability to the No Attack state to always
be 25% such that we ensure that there are some no attack intervals and that an attacker
does not remain in an attacking state.
Fig. 3. Markov Chain with the diﬀerent attacks and the transition probabilities
Based on the Markov chain presented in Figure 3 we created and assessed twenty
diﬀerent sequence-scenarios. All sequences start in the No Attack state. Below we de-
scribe the most relevant scenarios in terms of representing the diﬀerent groups of se-
quences, one group that has a very small amount of non-attacking intervals, another
group with intermediate values of non-attacking intervals, and the last group that has
the highest amount of non-attacking intervals. We base our selection on the amount of
non-attacking intervals as characteristic due to the importance of these intervals for the
detection method leveraged in this work. Below we utilize the term iteration, an itera-
tion is equivalent to 0.5% of the duration of an experiment. We focus on the following
scenarios:
– Sequence A: No attack 15 iterations; inﬂation 15 iterations; network-partition 55 it-
erations; deﬂation 35 iterations; inﬂation 45 iterations; inﬂation 35 iterations. Total
amount of non-attacking intervals: 15
– Sequence B: No attack 10 iterations; inﬂation 55 iterations; oscillation 50 iterations;
frog-boiling 55 iterations; network-partition 30 iterations. Total amount of non-
attacking intervals: 10
– Sequence C: No attack 30 iterations; network-partition 35 iterations; frog-boiling
35 iterations; No attack 15 iterations; frog-boiling 40 iterations; inﬂation 45 itera-
tions. Total amount of non-attacking intervals: 45
– Sequence D: No attack 40 iterations; inﬂation 30 iterations; oscillation 40 iterations;
network-partition 40 iterations; frog-boiling 35 iterations; No attack 15 iterations.
Total amount of non-attacking intervals: 55
– Sequence E: No attack 50 iterations; inﬂation 10 iterations; No attack 50 itera-
tions; oscillation 55 iterations; oscillation 10 iterations; inﬂation 25 iterations. Total
amount of non-attacking intervals: 100
– Sequence F: No attack 55 iterations; network-partition 40 iterations; No attack 15
iterations; frog-boiling 45 iterations; inﬂation 15 iterations; No attack 25 iterations;
No attack 5 iterations. Total amount of non-attacking intervals: 100
We note that in these sequences of attacks, we still consider malicious nodes that work
together by applying the same attacks in the same time interval.
4 Mitigation Framework
This section describes our new mitigation framework based on machine learning tech-
niques, and presents the feature set that we leveraged for use by the machine learning
technique.
4.1 Background
Machine learning techniques, such as classiﬁcation, have the aim to separate a given
data set into diﬀerent classes. In our case, the classes that exist are normal and attack,
meaning that we have two diﬀerent types of data in our data set. On one side, we have
data that represents normal updates of the nodes, and on the other side, we have data
that represents malicious update requests.
We choose to apply supervised classiﬁcation methods as we know how the system
works under normal circumstances and also how the attacks degrade performance when
they are taking place. These classiﬁcation methods are fed with training data to learn the
diﬀerence between normal and malicious data. Supervised classiﬁcation methods can
operate directly in the feature space/predictor variables and identify separable regions
that can be associated to a given class/dependent categorical variable. Such methods are
implemented by decision trees that come in several variants. Simple versions such as
Classiﬁcation and Regression (Cart) [8] can predict both categorical and numerical out-
comes, while other schemes (C4.5 for instance) relying on information theory [29] are
uniquely adapted to categorical outputs. Another type of classiﬁcation method, support
vector machines, map the input space into another dimensional space and then rely on
kernel functions for performing classiﬁcation in the target space [9].
4.2 Feature Set
We have evaluated three diﬀerent methods (SimpleCart, C4.5, and support vector ma-
chines) for their eﬃciency in protecting virtual coordinate systems. We did this for
several reasons: ﬁrst, we wanted to compare the individual approaches and identify the
best one. Second, we considered that providing these results allows a more compre-
hensive analysis of the detection process, as well as to highlight some of the peculiar
properties related to the diﬀerent methods.
We have identiﬁed seven feature variables to be used in the prediction task. This
process was challenging since several approaches that worked directly on the raw data
were not successful. The raw data consisted, in our case, of statistical properties of
the underlying local error values. We have analyzed the time series values of both the
median and the average local error, but a straightforward analysis of simple time series
values did not perform well. This was due to a four lag autocorrelation in the observed
time series. In order to decorrelate the time series values, we applied an embedding
of the observed one dimensional data into a seven dimensional manifold. Values in
the original time series are given by the median local error described in Section 2.2.
The embedding into a multidimensional manifold aims at revealing subspaces that can
be associated to attack states and respectively non-attack ones. Thus, at each sample
moment in time, we need to analyze a seven dimensional random vector.
1. Feature A is the median local error of the nodes emedian. This feature represents the
global evolution of the local error. Intuitively, a low median local error means that
most of the nodes have converged to their coordinates.
2. Feature B represents the diﬀerence of the median local error at one lag δ1 = emediant-
emediant−1. This feature captures the sense of the variation in the local error. Positive
values indicate an increase in the error, while negative values show continuous
decrease in the error. This feature can be seen as a discretized ﬁrst derivate of the
observed process. Although, discrete time events are used to index the time series,
by analogy to the continuous case, we assume that this discretized ﬁrst derivate
captures the sense (increasing/decreasing) of the underlying time series.
3. Feature C is δ2 = emediant- emediant−2. This feature relates current values to previous
4. Feature D is δ3 = emediant- emediant−3, is similar to feature C, but works at a three lag
values at a two lag distance.
distance.
5. Feature E is δ4 = emediant- emediant−4. It captures longer dependence (lag four).
6. Feature F captures the discretized form of the second order derivate δ1t- δ1t−1. Basi-
cally, this feature can indicate the shape (concave/convex) of the initial time series.
We assume a discretized equivalent of the continuous deﬁnition.
7. Feature G is the absolute value of the discretized form of the second order derivate
| δ1t- δ1t−1 |. This absolute value can provide insights in inﬂection points (i.e., points,
where a switch from convex to concave, or concave to convex is happening).
(a) Frog-boiling
(b) Deﬂation
(c) Inﬂation
Fig. 4. Bi-dimensional and pairwise feature representation
We can not visualize a seven dimensional manifold, but bi-dimensional pairwise
scatter plots can illustrate the rationale for our approach. Figure 4(a) shows the two
dimensional scatter plot for a frog-boiling attack. Feature A is used for the x-axis and
feature E for y-axis. The two classes (attack and non attack) can be linearly separated
in this two dimensional subspace. Figure 4(b) shows another 2 dimensional scatter plot,
where feature A and feature F are used. This scenario corresponds to a deﬂation at-
tack. In this scenario, the classes can be also linearly separated, and thus we argue that
these features are appropriate for defending against a deﬂation attack. However, in Fig-
ure 4(c), the same set of features used during an inﬂation attack shows very limited
detection potential. However, the global set of all seven features can be leveraged to
detect the diﬀerent (frog-boiling, deﬂation, inﬂation, oscillation and network-partition)
attacks.
The attack detection problem is stated thus as deciding whether a seven dimen-
sional tuple is representing an attack or not. From a mitigation point of view, once an
attack is identiﬁed several measures can be taken. In a ﬁrst phase, updating the virtual
coordinates can be resumed after the attack stops, or limited to updates received from
 0.5 0.6 0.7 0.8 0.9 1-0.04-0.02 0 0.02 0.04Feature AFeature ENo attackAttackhyperplane 0.6 0.7 0.8 0.9 1-0.003-0.002-0.001 0 0.001 0.002 0.003Feature AFeature FNo attackAttackhyperplane 0.5 0.6 0.7 0.8 0.9 1-0.003-0.002-0.001 0 0.001 0.002 0.003Feature AFeature FNo attackAttackknown and trusted nodes. The latter assumes an underlying reputation or trust model.
In a second phase, the attacking hosts should be identiﬁed and contained.
(a) Feature A - emedian
(b) Feature B - δ1
Fig. 5. Classiﬁcation features
To provide some intuition behind our methodology we present in Figure 5 the evo-
lution of two features for a dataset that contains a two attack strategy, Inﬂation - Oscil-
lation. This attack scenario consists of four time slots, so the ﬁrst is a non-attacking slot.
The second is in this case an inﬂation attack. The third time slot is again non attacking,
and the fourth and last time slot is the oscillation attack. The objective of classiﬁcation
is, as already mentioned, to separate the diﬀerent classes of the data set. Two classes
exist, the non-attacking and attack class. Within Figure 5, we want to illustrate how the
classiﬁer can identify the diﬀerent classes. Figure 5(a) shows how feature A, the me-
dian of the error, evolves. In this simple case, the increasing or decreasing trends are
easy to identify and one can deﬁne when the attacking time slots take place. Feature A
decreases in a non-attacking time slot, and increases during an attack. However, feature
B captures a smoothed version of the overall evolution. In these plots, we can identify
intervals that correspond to positive y-values for feature B. These positive values belong
to attacking time slots.
5 Experimental Results
In this section, we evaluate the single and complex attack strategies described in Sec-
tion 3 using Vivaldi within three diﬀerent environments. First, we evaluate the eﬀective-
ness of the machine learning techniques on the dataset resulting from simulation using
the p2psim simulator [2] and the King data set topology [18]. Second, we compare our
machine learning methods to a previously proposed solution using outlier detection [37]
that can defend against inﬂation, deﬂation, and oscillation. Third, we evaluate our ma-
chine learning techniques on the data set resulting from deploying Vivaldi on 500 nodes
on the Internet PlanetLab testbed [3]. We evaluate our detection method in two setups:
global and local. In the global case, every node’s information is centrally collected and
analyzed together, while in the local case each individual node decides if an attack is