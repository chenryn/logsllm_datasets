title:A robust and lightweight stable leader election service for dynamic
systems
author:Nicolas Schiper and
Sam Toueg
A Robust and Lightweight
Stable Leader Election Service
for Dynamic Systems∗
Nicolas Schiper
University of Lugano, Switzerland
PI:EMAIL
Sam Toueg
University of Toronto, Canada
PI:EMAIL
Abstract
We describe the implementation and experimental evalu-
ation of a fault-tolerant leader election service for dynamic
systems.
Intuitively, distributed applications can use this
service to elect and maintain an operational leader for any
group of processes which may dynamically change. If the
leader of a group crashes, is temporarily disconnected, or
voluntarily leaves the group, the service automatically re-
elects a new group leader. The current version of the ser-
vice implements two recent leader election algorithms, and
users can select the one that ﬁts their system better. Both
algorithms ensure leader stability, a desirable feature that
lacks in some other algorithms, but one is more robust in
the face of extreme network disruptions, while the other is
more scalable.
The leader election service is ﬂexible and easy to use.
By using a stochastic failure detector [5] and a link qual-
ity estimator, it provides some degree of QoS control and it
adapts to changing network conditions. Our experimental
evaluation indicates that it is also highly robust and inex-
pensive to run in practice.
1. Introduction
In this paper we describe and experimentally evaluate
a fault-tolerant leader election service for dynamic sys-
tems. Leader election plays an important role in the de-
sign of fault-tolerant applications.
Intuitively, this is be-
cause a leader can be used as a central coordinator that en-
forces consistent behavior among processes. For example,
the algorithms in [13, 16, 9] require a fault-tolerant leader
election mechanism to manage data replication or to en-
force process agreement despite failures. More generally,
a leader election service can be used to solve consensus and
atomic broadcast — two primitives at the core of Lamport’s
∗This research was supported in part by NSERC Canada (grant number
250468-07) and by SNSF Switzerland (project number 200021-107824).
state machine approach for building fault-tolerant applica-
tions [12].
The service we propose can be used to elect and main-
tain a leader among any dynamically changing subset of ap-
plication processes (called a group) in a system with ran-
dom process crashes, process recoveries, message losses
and message delays. The leader of a group must be oper-
ational (and a current member of that group): if it crashes,
is temporarily disconnected, or voluntarily leaves the group,
the service automatically re-elects a new leader and notiﬁes
the processes in the group of this change. There may be pe-
riods of time when a group has no operational leader or has
several leaders (e.g., just after a crash of the current leader
or when several processes compete to gain the group lead-
ership), but the service ensures that these periods are very
short in practice: roughly speaking, as long as a group has
at least one reasonable candidate for leadership (an opera-
tional process with well-behaved communication links), the
service provides the group with a unique and stable leader.
Groups are dynamic, i.e., each application process can
join or leave any group at any time (each process can con-
currently belong to several groups), and the service provides
a leader among the operational processes of each group. If
the current leader of a group fails or leaves the group, the
service automatically elects a new leader.
When a process joins a group, it can indicate whether it
is willing to be a leader for this group (i.e., whether it is
a “candidate” for leadership). For each group, the service
selects a leader only among the (currently operational) can-
didates in that group. This feature can be useful in practice
for several reasons. First, a process may be unwilling to
be the group leader because it cannot handle the associated
workload. Second, the cost of a leader election (in terms of
messages exchanged) is typically proportional to the num-
ber of candidates that concurrently compete for this posi-
tion. So a large group may want to restrict the election to a
small number of candidates (e.g., among t + 1 candidates, t
of which may fail).
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE207DSN 2008: Schiper & TouegTo select the “core” algorithms for our service, we imple-
mented three different leader election algorithms and exper-
imentally compared their performance under several differ-
ent settings. Of these three algorithms, two performed ex-
tremely well even in systems with very high rates of work-
station and communication failures. These two algorithms
are enhanced versions of algorithms in [2, 4] that were mod-
iﬁed to integrate the stochastic failure detector (FD) algo-
rithm of Chen et al. [5] and a link quality estimator in or-
der to provide some degree of QoS control and to adapt to
changing network conditions. In the current version of the
service, users can chose between these two leader election
algorithms, and select the one that ﬁts their system better:
as our experimental evaluations show, one is more robust
in the face of extreme network disruptions, while the other
is more scalable. With either algorithm, the service have
several desirable features, that we now describe.
The service ensures a high degree of leader stability.
Roughly speaking, leader stability means that the current
leader is not demoted and replaced if it is still opera-
tional [1]. This is in contrast to several leader election
algorithms, notably those that select a leader using some
ﬁxed function on process identiﬁers. For example, consider
the algorithm that selects the leader as the process with the
smallest identiﬁer among the processes that seem to be cur-
rently alive [17, 8, 14]. With this algorithm, a group leader
(cid:96) is systematically demoted and replaced every time a pro-
cess with a smaller identiﬁer than (cid:96) newly joins the group
(or rejoins the group after it recovers from a crash), and
this occurs even if (cid:96) is fully functional and has been work-
ing well as the group leader. It is clear that demoting fully
functional leaders for such spurious reasons can be costly
and disruptive to leader-based applications, and therefore
should be avoided.
With this service, applications have some control on the
Quality of Service (QoS) of the leader election. More pre-
cisely, for each group g, each application process p in g can
specify bounds on (1) the time p takes to detect the crash
of the current leader of g, and (2) the “accuracy” of this de-
tection (in terms of the frequency and duration of mistakes,
i.e., of false crash detections). Bounding (1) is important
because, as our experiments suggests, the time it takes to
detect the crash of a leader is often the dominating com-
ponent of what we call leader recovery time (i.e., the time
that elapses from the crash of a leader to the installation of
a new leader). Bounding (2) is also important because mak-
ing a mistake on the leader (i.e., thinking that it has crashed
while it is still functional) can lead to an unnecessary and
expensive change of leader. To achieve the above, we im-
plemented the stochastic failure detector (FD) algorithm of
Chen et al. [5] and integrated it in (the leader election al-
gorithms of) our service: Under some conditions, this FD
provides QoS guarantees on the speed and accuracy of the
failure detection.
The leader election service adapts to changing network
conditions. In fact, applications do not specify static, low-
level parameters such as timeout durations or the frequency
of “I-am-alive” messages, these are automatically deter-
mined and continuously updated according to the “current”
network conditions as follows. The underlying FD algo-
rithm of Chen et al. periodically reevaluates these FD pa-
rameters as a function of two inputs: (a) the required QoS of
the FD (as speciﬁed by an application), and (b) the current
quality of the communication links (as given by a link qual-
ity evaluator module). This allows the underlying failure
detector, and ultimately the service, to automatically adjust
to changing network conditions.
Finally, our experimental evaluation described in Sec-
tion 6 indicates that the leader election service is indeed
quite stable, robust and inexpensive to run.
The source code of our leader election service, as well as
information on how to install and use it, are available at:
http://www.inf.unisi.ch/phd/schiper/LeaderElection/. Sev-
eral experimental results that are omitted here are also
posted on that site.
Roadmap. The rest of the paper is structured as follows.
Section 2 summarises related work. Section 3 describes the
failure detector of Chen et al.. Section 4 presents the archi-
tecture of the leader election service. Section 5 describes
the QoS metrics that we use to evaluate and compare the
different leader election algorithms that we implemented in
our service. Section 6 describes our experimental settings
and results. Some concluding remarks appear in Section 7.
2. Related Work
The literature on failure detectors and more speciﬁcally
leader election is abundant. We now brieﬂy review some of
the relevant papers. In [8], the authors study the problem of
leader election in partitionable networks. They deﬁne the
notion of stable partition as a partition in which every pair
of processes can communicate in a timely manner. In each
one of these stable partitions, the process with the small-
est identiﬁer is elected as the partition leader. In [14], the
authors give a communication-efﬁcient algorithm, an algo-
rithm in which eventually only the leader sends messages.
However, their algorithm requires strong system assump-
tions, i.e., all links have to be eventually timely (a link is
eventually timely if all messages sent after some time t take
at most δ units of time to be received).
In [10], the au-
thors study the leader election problem in a probabilistic
model, i.e., a model where process crashes and link failures
are probabilistic. In this algorithm, a parameter controls the
trade-off between the correctness probability guarantee and
the message complexity.
In [1], several leader election algorithms are given for
systems where at least one non-faulty process has input and
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE208DSN 2008: Schiper & Touegoutput communication links that are eventually timely (all
the other links may lose all messages). These algorithms
are message-efﬁcient, i.e., after a leader is elected, only the
leader sends messages (it must do so periodically to inform
other processes that it is still alive). Furthermore, they en-
sure a strong form of leader stability, i.e., a leader is not
demoted if its input and output links have been timely for
some amount of time ∆ (∆ depends on the algorithm). In
[2] and [4], algorithms are given for systems where only
the output links of some non-faulty process are eventu-
ally timely (the input links may be arbitrarily slow). One
of these algorithms is communication-efﬁcient but it as-
sumes that message losses are only intermittent (a message
repeatedly sent over a lossy link is eventually received).
Another algorithm tolerates links that completely “crash”
(i.e., lose all messages), but it requires quadratic commu-
nication even after a leader has been elected.
In [3], the
authors consider systems where at most f processes may
crash and give a communication-efﬁcient algorithm where
only f of the output links of a non-faulty process need to
be eventually timely. In [15], the authors give a strongly
stable and communication-efﬁcient algorithm that requires
one non-faulty process to be eventually f-accessible, i.e., a
process that has eventually timely input and output links
to f processes. An interesting feature of this algorithm
is that these eventually timely links need not be ﬁxed and
may change during the execution of the algorithm. In [7],
a communication-efﬁcient algorithm is presented that does
not require a priori knowledge of the processes’ identities.
3. Chen et al.’s Failure Detector with QoS
Failure detection is at the core of any leader election ser-
vice: it is used to detect whether the current leader has failed
and needs to be replaced, and to determine which of the can-
didates for replacing the failed leader are operational.
In our service, we implemented the stochastic failure de-
tector algorithm due to Chen et al. [5] and integrated it with
a link quality estimator module and a dynamic scheduler.
Together, these modules provide some QoS control on fail-
ure detection under changing network conditions. We now
brieﬂy describe these modules (shown in Figure 1).
Figure 1. The failure detector module
D , T L
A : (a) T U
M R, and P L
When a process p monitors the status of another process
q, it gives the QoS requirement of this monitoring in terms
of 3 parameters, denoted T U
D is an
upper bound on the time the FD takes to detect the crash
of q, (b) T L
M R is a lower bound on the expected time be-
tween two consecutive mistakes of the FD (the FD makes a
mistake when it tells p that q crashed and this is not true),
and (c) P L
A is a lower bound on the probability that, at a
random time, the FD is correct about the operational status
of q.
It is clear that achieving such a QoS requirement depends
on the frequency at which q sends alive messages to p, the
timeout that p uses on these messages, and on quality of
the communication link from q to p. In the following, we
brieﬂy explain the three modules that are related to this.
The Link Quality Estimator module continuously estimates
the “quality” of the link from q to p in terms of three quanti-
ties: the probability of message loss pL, the expected value
of message delay Ed, and the standard deviation of message
delay Sd. This estimation is done using the alive messages
that p receives from q.
The Failure Detector Conﬁgurator computes the FD param-
eters that ensure the required QoS under some assumptions
on the network behavior [5]. More precisely, this module
(dynamically) computes (1) the frequency η at which q must
send alive messages to p, and (2) the timeout δ that p must
use to determine q’s operational status. To compute η and
δ, the module takes two inputs: (a) the required QoS of the
A given
monitoring of q, i.e., the values of T U
by p, and (b) the estimated quality of the link from q to p,
i.e., the latest values of pL, Ed, and Sd computed by the
Link Quality Estimator module.
The Scheduler uses the output (η, δ) of the Failure Detector
Conﬁgurator as follows: it schedules the sending of alive
messages by q at a frequency of η, and it uses the current
timeout δ and the time that p received its last alive message
from q to schedule the trust/suspect notiﬁcations at p.
M R, and P L
D , T L
4. The Leader Election Service Architecture
The architecture of the leader election service is based
on the failure detector service proposed by Deianov et al.
in [6] and implemented by Ivan et al. in [11]. This archi-
tecture reduces the overall network and CPU overhead by
sharing some tasks (e.g., estimating the quality of the com-
munication links or determining whether a workstation is
operational): the cost of these tasks is shared by the various
applications that concurrently use the service. The leader
election service is written in C and is compatible with any
Unix/Linux. The service’s architecture is illustrated in Fig-
ure 2 and brieﬂy explained below.
Application processes are linked to a shared library im-
plementing the service’s API. The main API functions al-
Failure Detector ConﬁguratorLink Quality EstimatorScheduler(TdU, TmrL, PaL)trust/suspect proc.(!, ")(Ed, Sd, pl)networksend/receive ALIVEreceive ALIVEInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE209DSN 2008: Schiper & Toueglow processes to register/unregister with the service and
to join/leave groups. This library communicates with the
Command Handler module to serve the requests of applica-
tion processes.
leader election algorithms can be “plugged in” here in fu-
ture versions of the service.
5. Leader Election QoS Metrics
To use the leader election service, a process p must ﬁrst
register itself with the service using a unique process iden-
tiﬁer. Once this is done, p can join and leave any group at
anytime. To join a group g, p must specify the following
four parameters: (1) g’s identiﬁer, (2) whether p is a candi-
date for g’s leadership or not, (3) the way p wishes to ﬁnd
out who is the current leader of g (by an interrupt from the
service, whenever the leader of g changes, or by querying
the service, whenever p wants to do so), and (4) the QoS
A ) of the underlying FD used by the service
(T U
to elect a leader in group g.
M R, P L
D , T L
Figure 2. The leader election service
The core functionality of the service resides in the Group
Maintenance, the Failure Detector, and Leader Election Al-
gorithm modules.
For each group g, the Group Maintenance module builds
and maintains (a) the set of processes that are currently in g,
and (b) the subset of processes of g that are currently “ac-
tive” in g (roughly speaking, a process in g is active if it is
currently alive and competing for the leadership of g; as we
will see in Section 6, depending on the leader election algo-
rithm used, either all the alive processes in g are active [4]
or eventually only the leader of g remains active [2]). To de-
termine (a) and (b), the Group Maintenance module needs
to determine the status of processes in each group. To do
so, it uses the Failure Detector module, which was imple-
mented using the failure detector of Chen et al. as decribed
in Section 3.
The Leader Election Algorithm module maintains a
leader in each group g by executing the algorithm in [4] or
in [2] in g, depending on the version of the service used