Section VII-G shows a comparison between TinyGarble‚Äôs
performance using an HLS tool (input written in C) and
using a conventional HDL synthesis tool (input given
in Verilog). Lastly, Section VII-H shows the result of
our garbled processor and implementation of Hamming
distance as a benchmark.
We also compare the performance of the commercial
logic synthesis tool with the academic, open-source tools
in Appendix A. We show that in most cases, the per-
formance of the open-source tool is comparable to the
commercial tool.
A. Experimental Setup
The circuit generations are all done on a system
with Linux RedHat Server 5.6, 8 GB of memory, and
Intel Xeon X5450 CPU @ 3 GHz. We use another
system with Ubuntu 14.10 Desktop, 12.0 GB of memory,
and Intel Core i7-2600 CPU @ 3.4GHz to assess the
timing performance of the sequential garbling scheme in
Section VII-F.
Two sets of HDL synthesis tool chains are used in
our experiments: one commercial and one open-source
(Appendix A). Our commercial HDL level synthesis tool
is Synopsis Design Compiler (DC) 2010.03-SP4 [13].
We also use the Synopsis Library Compiler from the
DC package to interpret our custom technology library.
In Section VII-G, we utilize Xilinx Vivado HLS [19],
a commercially available HLS tool whose inputs are
written in the C/C++ programming language. We empha-
size that TinyGarble can operate with any commercial or
open-source sequential HDL-level (or HLS) synthesizer,
as long as the synthesizer is capable of performing state-
of-the-art logic optimization and mapping algorithms.
B. Performance Metrics
We use the following metrics to measure the efÔ¨Åciency
of TinyGarble for generating garbled circuits:
‚Ä¢ Memory Footprint EfÔ¨Åciency (MFE):
MFE =
q0
q
,
420420
where q0 is the total number of gates in the reference
circuit and q is the total number of gates in the
circuit under evaluation. The maximum number of
tokens that need to be stored at any point during
garbling/evaluation as well as memory required for
storing circuit description is directly proportional
to the number of gates in both sequential and
combinational circuits. Thus, the total number of
gates is approximately proportional to the memory
footprint.
‚Ä¢ Number of Garbled Tables (#GT ):
#GT = #nonXOR √ó c,
where #nonXOR is the number of non-XOR gates
in a circuit and c is the number of sequential cycles
that the circuit needs to be garbled/evaluated. In
free XOR-based GC schemes, each non-XOR gate
requires a garbled table to be generated by the
garbler and sent to the evaluator at each sequential
cycle. Thus, this metric is an estimate of both the
computation and communication time.
‚Ä¢ Garbled Tables Difference (GTD (%)):
GTD =
#GT ‚àí #GT 0
#GT 0
√ó 100,
where #GT 0 is the total number of garbled tables
for the reference circuit and #GT is the total num-
ber of garbled tables for the circuit under evaluation.
When comparing a sequential with a combination
circuit, positive GTD shows an overhead (caused
by folding a circuit with an asymmetric loop, see
Section IV) in total computation and communication
time resulting from an excessive number of garbled
tables generated in the sequential circuits. However,
in general, negative GTD shows improvement in
the number of non-XOR gates and generated gar-
bled tables that results from logic synthesis opti-
mization.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:52 UTC from IEEE Xplore.  Restrictions apply. 
C. Benchmark functions
can be folded 24 times without any signiÔ¨Åcant overhead.
We evaluate TinyGarble‚Äôs circuit generation method on
various benchmark functions. Several of these functions
have been used in previous works, e.g., PCF [46]. In
the following, we introduce our benchmarks and explain
how we fold them into a sequential representation.
Sum. This function receives two N-bit
inputs and
outputs an N-bit sum. The sum function is implemented
in N steps of one bit sums by keeping the carry bit. Thus,
it can be folded up to N times without any signiÔ¨Åcant
overhead in number of garbled tables (#GT ).
Hamming Distance. This function receives two N-bit
inputs and outputs the log2(N )-bit Hamming distance
between them. The Hamming distance between two
numbers is the number of positions at which the cor-
responding bits are different. A possible combinational
implementation of the N-bit Hamming distance uses a
binary tree of adders that sums all 1-bit values from the
bit differences to a Ô¨Ånal Hamming distance consisting of
log2(N ) bits [5]. This implementation cannot be folded
easily. However, we can fold this function into N-cycles
of one XOR and one log2(N )-bit adder. This causes an
overhead compared to the combinational circuit.
Compare (Millionaires problem). This function re-
ceives two N-bit unsigned input values and outputs a
greater than signal consisting of one bit that indicates
if the Ô¨Årst input is greater than the second one. The
comparison function can be implemented in N steps of
subtraction by keeping the carry bit [43]. Thus, it can be
folded up to N times without any signiÔ¨Åcant overhead.
Multiplication. This function receives two unsigned
N-bit inputs and outputs their unsigned N-bit product.
The multiplication function consists of N additions and
shifts. The shift operations result
in an asymmetric
structure in this function. Thus, folding it up to N times
may increase the overhead.
Matrix Multiplication. This function receives two N √ó
N matrices consisting of 32-bit unsigned numbers and
outputs an N √ó N matrix equal to the product of the
input matrices. The N√óN matrix multiplication function
consists of three N-cycle nested loops with a symmetric
structures. It can be folded up to N 3 times without any
signiÔ¨Åcant overhead.
AES-128. This function receives a 128-bit plaintext
and 128-bit round keys and outputs a 128-bit ciphertext
based on the Rijndael algorithm. The AES-128 function
consists of 10 rounds with almost symmetric structure.
Ideally, it can be folded up to 10 times without any
signiÔ¨Åcant overhead.
SHA3. This function receives 576-bit inputs and out-
puts a 1600-bit number equal to the SHA3 hash of the
input. We implement the Keccak-f permutations[1600]
procedure for realizing this function. The SHA3 function
consists of 24 steps, each with a symmetric structure. It
421421
D. Combinational Garbled Circuit
To show the performance gain of using our custom
libraries, we compare TinyGarble combinational circuits
with circuits reported in PCF [46]. We choose PCF
because among the automated GC tools available at
the time of writing, it shows better results for most
of the benchmarks. In some other work like FastGC
[36], a number of benchmark circuits have been more
aggressively improved (compared to PCF) using ad-
hoc and mostly manual optimizations, but without a
generalizable methodology.
The comparison is shown in Table I. We compute the
garbled tables difference GTD (see Section VII-B) of
various benchmarks by using circuits reported in PCF
as reference (GTD PCF). It can be seen that the combina-
tional circuits generated by TinyGarble have non-positive
GTD PCF which means that the number of garbled tables
are less than or equal
to that of PCF circuits. We
also compare the memory footprint by computing the
memory footprint efÔ¨Åciency MFE with PCF as reference
(MFE PCF). We observe that MFE PCF is larger than 1 (up
to 9.3). This means that even without using sequential
circuits, the memory footprint can be reduced by almost
an order of magnitude by using TinyGarble custom
libraries and standard HDL synthesis.
In case of Hamming distance, TinyGarble shows, on
average, 80% improvement in number of garbled tables.
Another automated tool CBMC-GC [23] reports better
result compared to PCF for Hamming 160 (non-XOR
4,738, total gates 20,356). However, TinyGarble shows
66% improvement in number of garbled tables compared
to CBMC-GC. In case of 256-bit and 1024-bit Multipli-
cation, and 8 √ó 8 and 16 √ó 16 Matrix Multiplication,
because of the huge (impractical) sizes, Synopsis DC
was unable to generate the entire combinational circuit.
This is because Synopsis DC is a tool developed for com-
mercial applications. The real-life applications are almost
always written sequentially, otherwise the design would
not be scalable or even amenable to ofÔ¨Çine compilation
onto a hardware circuit. We emphasize that our sequential
circuit (c > 1) provides the exact same functionality
while having a very small memory footprint compared
with the reference circuit.
Comparison with Hand-Optimized Circuits: The
netlists generated by the automated Ô¨Çow of TinyGarble
show similar performance as the hand-optimized netlists
in many cases. For example, [43] describe an N-bit
sum circuit with 5N gates of which N gates are non-
XOR and an N-bit comparison circuit with 4N gates
of which N gates are non-XOR. The circuits generated
by TinyGarble have about the same number of gates
for these two functions. Note that one can always add
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:52 UTC from IEEE Xplore.  Restrictions apply. 
any hand-optimized module to the synthesis library of
TinyGarble.
E. Sequential Garbled Circuit
As described in Section IV, the user has the degree
of freedom to fold a combinational circuit and convert
it to a sequential one to reduce the memory footprint.
c denotes the number of sequential cycles required to
garble/evaluate the circuit. This value demonstrates the
amount of folding that is performed before the circuit
is input to the synthesizer. The user deÔ¨Ånes the value
of c and writes her own input function in an HDL or a
higher level language such that the function is evaluated
in c sequential cycles.
We use Memory Footprint EfÔ¨Åciency (MFE), to eval-
uate the reduction in memory requirement. We use Tiny-
Garble combinational circuits (c = 1) as reference. The
ideal MFE for a circuit with c sequential cycles is c. We
also compare the memory footprints of sequential circuits
with combinational circuits reported in PCF (MFE PCF).
As explained in Section IV-A, the folding process may
introduce some overhead on the total number of garbled
tables. To assess this overhead, we compute the Garbled
Tables Difference (GTD) of the sequential circuit using
TinyGarble combinational circuits as reference. The ideal
GTD is 0%, which means that
the total number of
garbled tables should be equal to those for a functionally
equivalent combinational circuit. We also compare the
number of garbled tables of sequential circuits with
combinational circuits reported in PCF (GTD PCF) to
show that even with the incurred overhead, the number
of garbled tables for sequential circuits is still less than
that of PCF for most cases.
Table II shows the number of total gates, non-XOR
gates, MFE, GTD, MFE PCF, and GTD PCF of the
benchmark circuits for various input widths. MFE, GTD
are computed with TinyGarble combinational circuits
(with c = 1) as reference. MFE PCF, and GTD PCF use the
circuits reported in PCF as reference. In the case of AES
128, we compare our implementation with the manually
optimized circuit reported in FastGC [36] because PCF
did not report it directly.
We provide a few highlights from Table II. TinyGarble
is able to decrease the size of the sum of two 1024-bit
numbers by 1,022.8 times (i.e., more than three orders
of magnitude) without affecting the number of garbled
tables (GTD) compared with its own combinational
circuit. For Hamming 16000, TinyGarble is able to
decrease the memory footprint by 7,345.5 times (i.e.,
about 4 orders of magnitude) while reducing the number
of garbled tables by 47.3% in comparison with the circuit
reported in PCF. In case of Mult 1024, TinyGarble
shrinks the memory footprint by a factor of 2,504.4
while reducing the number of garbled tables by 79.4%
when compared with the result in PCF. For a 16 √ó 16
matrix multiplication, a 4,434.1 more compact TinyGar-
ble solution with 6% less garbled tables compared with
PCF is available. By folding AES-128 10 times, the total
number of gates is reduce by a factor of 13.9 compared
to the FastGC circuit without any overhead in the number
of non-XORs. Observe that the savings are typically
more for larger bit-widths while extreme foldings can
introduce an increased overhead in number of garbled
tables due to the resulting asymmetry.
Because of the TinyGarble superior scalability, we are
able to implement functions that have never been re-
ported before, such as SHA-3, which can be represented
using 344,059 and 6,788 gates respectively.
F. Effect of Folding on Garbling Time
So far, we have only reported the overhead in terms of
garbled tables (GTD) that is a function of the number of
non-XOR gates. As explained in [2], if we see garbling as
a cryptographic primitive, its computation time (without
considering communication) will also be interesting. In
practice, smaller circuits which can Ô¨Åt entirely in the pro-
cessor cache result in fewer cache misses and therefore,
consume less CPU cycles for garbling. To better observe
the impact of cache speed-up for the compact circuits
resulting from TinyGarble, Fig. 7 depicts the CPU Time
(left y-axis) and the memory footprint of wire tokens
(right y-axis) versus c (x-axis) for the 32,768-bit Sum
function. As mentioned earlier, the memory footprint is
directly proportional to the total number of gates in the
sequential circuit.
This experiment is done using our sequential garbling
scheme based on JustGarble [2] that
includes using
Free XOR, Row Reduction, and Fixed-key AES garbling
techniques (see Section II-A). We use an Intel Core i7
CPU @ 3.40GHz which supports the AES-NI instruction
set. The CPU cycle is measured as the average of 10, 000
trials using RDTSC instruction. For security parameter
k = 128 (the bit-width of wire token, see Section II-A),
we store 128-bit per tokens. For garbling in JustGarble,
we store 2 tokens, 2 32-bit input indexes, and an 8-bit
gate-type per gate. Thus, the memory footprint is approx-
imately 328-bit per gate in garbling operation. Folding
the circuit by a factor of c ‚àà [1 : 32,768] constantly
decreases the memory footprint while the computation
effort remains almost constant. Interestingly, as can be
seen from the Ô¨Ågure, the number of CPU cycles sharply
decreases by 1.6√ó just when we fold four times (c = 4)
compared to c = 1. This is because for c ‚â• 4, the
memory space required for garbling completely Ô¨Åts in
the cache. The minimum CPU cycle per gate happens at
c = 2,048 for 3.2 KB memory footprint. This signiÔ¨Åes
the fact that even for large functions, we can use the
sequential approach to Ô¨Åt
the corresponding memory
422422
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:52 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I: Comparison of TinyGarble combinational circuits with PCF. In case of AES 128, the result is compared
with FastGC.
/	(

"
&
'
(
"&
'
(
"&&
'
(
"&&&
)
*"
	"
	
	"
	&

	,	,

	,	,

	,	,

	,	","
-
'-"&&
0.
	1
(2
*343(
	(
 	
	  	
	  
#
#
#
#
#
#"
#
%#
#%


#&
#&


#

 
 %
 %%%
 "
 %
% 
%! !
& &
 &"
#""+&"
#"+&!
% %"
 !
#!+&"
#+&!
" &.
