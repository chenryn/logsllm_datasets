title:Traffic engineering with estimated traffic matrices
author:Matthew Roughan and
Mikkel Thorup and
Yin Zhang
Trafﬁc Engineering with Estimated Trafﬁc Matrices
Matthew Roughan
Mikkel Thorup
AT&T Labs – Research,
Yin Zhang
180 Park Avenue, Florham Park, NJ, 07932.
{roughan,mthorup,yzhang}@research.att.com
ABSTRACT
Trafﬁc engineering and trafﬁc matrix estimation are often treated
as separate ﬁelds, even though one of the major applications for a
trafﬁc matrix is trafﬁc engineering. In cases where a trafﬁc matrix
cannot be measured directly, it may still be estimated from indirect
data (such as link measurements), but these estimates contain er-
rors. Yet little thought has been given to the effects of inexact trafﬁc
estimates on trafﬁc engineering. In this paper we consider how well
trafﬁc engineering works with estimated trafﬁc matrices in the con-
text of a speciﬁc task; namely that of optimizing network routing to
minimize congestion, measured by maximum link-utilization. Our
basic question is: how well is the real trafﬁc routed if the rout-
ing is only optimized for an estimated trafﬁc matrix? We compare
against optimal routing of the real trafﬁc using data derived from
an operational tier-1 ISP. We ﬁnd that the magnitude of errors in
the trafﬁc matrix estimate is not, in itself, a good indicator of the
performance of that estimate in route optimization. Likewise, the
optimal algorithm for trafﬁc engineering given knowledge of the
real trafﬁc matrix is no longer the best with only the estimated traf-
ﬁc matrix as input. Our main practical ﬁnding is that the combi-
nation of a known trafﬁc matrix estimation technique and a known
trafﬁc engineering technique can get close to the optimum in avoid-
ing congestion for the real trafﬁc. We even demonstrate stability in
the sense that routing optimized on data from one day continued
to perform well on subsequent days. This stability is crucial for
the practical relevance to off-line trafﬁc engineering, as it can be
performed by ISPs today.
Categories and Subject Descriptors
C.2.3 [Computer-Communications Network]: Network Opera-
tions—network management, network monitoring
General Terms
Measurement, Performance
Keywords
Trafﬁc Matrix Estimation, Trafﬁc Engineering, SNMP, OSPF, MPLS
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’03, October 27–29, 2003, Miami Beach, Florida, USA.
Copyright 2003 ACM 1-58113-773-7/03/0010 ...$5.00.
1.
INTRODUCTION
Estimating an Internet trafﬁc matrix has received considerable
attention in recent years. A trafﬁc matrix provides the volume of
trafﬁc between every pair of ingress and egress points over a given
time interval. Such information is essential to a variety of oper-
ational tasks ranging from router/link failure analysis to capacity
planning and trafﬁc engineering, for instance by route optimiza-
tion.
When direct ﬂow-level measurements are available, accurate traf-
ﬁc matrices can be derived following the approaches detailed in
[1]. Unfortunately, direct measurements require additional infras-
tructure support and it can be prohibitively expensive to instrument
the entire IP network to collect such data. Recently, progress has
been made on trafﬁc matrix estimation and several methods [3, 4,
5] have been proposed that attempt to derive trafﬁc matrices from
the link load data, which can be easily obtained via the Simple
Network Management Protocol (SNMP). We call such a technique
an SNMP-based trafﬁc matrix estimator. These algorithms have
been validated against real (but partial) trafﬁc matrices (obtained
through direct measurements) using common metrics such as mean
error computed over all source-destination pairs. The resulting esti-
mates contain errors of varying magnitude depending on the trafﬁc
matrix estimator applied. It is, however, not directly clear what im-
pact these errors have on operational tasks, as different tasks may
have quite different tolerance to the types and magnitude of the er-
rors. For example, if all errors were concentrated on a single critical
link, this could have a big impact on performance, yet a negligible
impact using most standard error measures.
In this paper, we attempt to establish a direct connection between
SNMP-based trafﬁc matrix estimators and one particular network
operational task: trafﬁc engineering to minimize congestion. That
is, we are interested in the following operational performance mea-
sure:
If trafﬁc engineering is done based on the estimated
trafﬁc matrix, how well does it perform on the real
trafﬁc matrix?
Several trafﬁc engineering techniques have been presented that op-
timize routing to minimize congestion [6, 7, 8, 9, 10, 11]. We call
such a technique a routing optimizer. It sets the routing parameters
of a network for a given trafﬁc matrix so as to minimize conges-
tion for that trafﬁc matrix. The routing parameters determine, for
each source-destination pair, the fraction of trafﬁc going on differ-
ent paths from the source to the destination. Typically, in the past,
routing optimizers were evaluated using synthetic trafﬁc matrices.
In this paper we feed the routing optimizer an estimated trafﬁc ma-
trix while measuring the performance of the routing on the real
trafﬁc matrix.
In this paper, as in [9], max-utilization is picked as the easiest to
appreciate measure for congestion. The utilization of a link is the
ratio of its load over its capacity, and the max-utilization is the max-
imum utilization over all links in the network. Other works have
focused on more sophisticated cost functions, summing costs over
all links in the network (see, e.g., [8, 7]), but these are less easy to
understand, and might obscure the point that the performance of es-
timation and optimization combined is not easily extrapolated from
the performance of one by itself. Some intuition may be gained
from the ﬁnding in [12] that their routing optimizer was robust to
±50% random errors, multiplying each individual demand with a
random value from [0, 2]. We consider 50% a large mean error, and
yet it only affected the max-utilization by about 10%. For contrast,
if in a large network, we only changed the demands that used a
speciﬁc highly utilized link, this would have a large impact for the
max-utilization, yet a negligible impact on the average error.
We deliberately treat both trafﬁc estimators and routing optimiz-
ers as black-boxes that we combine in a plug-and-play manner.
Both sides are based on previously published techniques. The con-
tribution of this paper is to see what happens when the two sides
are combined. Tests were performed using simulations based on
data from an operational tier-1 ISP. We found that, in itself, the
magnitude of errors in the trafﬁc matrix estimate was not a good
indicator of the performance of that estimate in our trafﬁc engi-
neering tasks. Likewise, the trafﬁc engineering algorithm that per-
forms best knowing the real trafﬁc matrix was no longer the best
with estimated trafﬁc matrices. Our main practical ﬁnding was
that combining the OSPF routing optimizer technique from [7] with
the tomogravity trafﬁc matrix estimator from [4], we got close to
the minimal max-utilization for the real trafﬁc. The above OSPF
routing can also be implemented with IS-IS and MPLS, making it
broadly applicable to todays IP network.
To further test the applicability of our combination, we took an
OSPF routing solution based on estimated trafﬁc matrices from one
day, and tested this routing on the real trafﬁc over the following
week. We found that the routing continued to perform well. Thus
our approach was not only robust to estimation, but also reason-
ably stable over time. This later property is crucial for realistic off-
line implementations in today’s IP networks, where changing link
weights frequently can result in network performance degradation.
Contents. The paper is divided as follows. In §2, we discuss the
different routing optimizers considered, and in §3, we discuss the
different trafﬁc matrix estimators. In §4, we present our experimen-
tal methodology. Our results are presented in §5, followed by some
practical considerations in §6. Then we have some reﬂections over
limitations of the paper in §7, and ﬁnally we end with concluding
remarks in §8.
2. ROUTING OPTIMIZERS
In this section, we discuss the different routing optimizers con-
sidered. We note that these are all based on published work, and the
reader will be referred to the relevant publications for most techni-
cal details. The interesting new aspect is what happens when the
optimizers, viewed as black-boxes, are applied to estimated trafﬁc
matrices and tested on real trafﬁc matrices.
2.1 General routing with MPLS
In the most general form of routing, trafﬁc from a source to
a destination may be split arbitrarily over all possible paths be-
tween source and destination. Finding a general routing minimiz-
ing max-utilization is an instance of the classical multicommodity
ﬂow problem which can be formulated as a linear program [13,
Chapter 17]. As described by Mitra and Ramakrishnan [6], the
linear program solution can be implemented with the quite recent
Multi-Protocol Label Switching (MPLS) protocol [14]. Essentially,
each path used is implemented as a label-switched path that the
source uses for a certain fraction of its trafﬁc to the destination.
We used the commercial linear programming package CPLEX ver-
sion 6.5 to solve the standard linear program to minimize the max-
utilization for a given trafﬁc matrix, and we refer to this as the
MPLS optimizer.
The MPLS optimizer is optimal in that if applied to the true traf-
ﬁc matrix, it gives the best possible performance among all routing
protocols with the given trafﬁc matrix and network, using the max-
utilization as the only performance criteria. Other possible crite-
ria such as feasibility of the implementation, robustness to link-
failures, etc., are not considered.
However, what happens if the MPLS optimizer ﬁnds the optimal
MPLS solution for the estimated trafﬁc matrix and then applies it
to the real trafﬁc? The MPLS solution tells us exactly how trafﬁc
should be split over different paths from source to destination, and
this splitting is now applied to the real trafﬁc matrix. How good
is the resulting routing compared with the above optimal MPLS
routing for the real trafﬁc matrix? Put conversely, how sensitive is
the optimal solution to errors in estimating the trafﬁc matrix? As
we shall see, the answer is ’quite sensitive’.
In fact the MPLS optimizer can have some strange results when
the inputs have errors. The algorithm may, without penalty, allow
route loops for trafﬁc matrix elements of small magnitude, as long
as these loops do not affect the max-utilization objective function.
This is the result of focusing the optimization on only minimizing
the maximum utilization — a loop in a small trafﬁc matrix element
has zero penalty under such an objective function. However, if this
small trafﬁc matrix element contains errors, the loop will amplify
the error when the trafﬁc is routed.
For an example of a more robust optimization we tried modifying
the objective function above to include a penalty for loops, and refer
∗
to the resulting algorithm as the MPLS
optimizer.
We note, however, that MPLS can implement any possible rout-
ing, so even if the above concrete MPLS optimizers do not work
well with estimated trafﬁc matrices, this does not imply that MPLS
in itself cannot be made robust with respect to estimation. Also, in
all fairness, it should be mentioned that the context for the optimal
MPLS solutions in [6] was a matrix of virtual leased lines where
the ISP commits to a certain amount of trafﬁc for each source des-
tination pair. These commitments are ﬁxed in contracts, and can be
honored as is.
2.2 Traditional shortest path routing
The most commonly used intra-domain Internet routing proto-
cols today are the shortest path Interior Gateway Protocols (IGP):
Open Shortest Path First (OSPF) [15] and Intermediate System-
Intermediate System (IS-IS) [16].
In these protocols, which are
functionally the same, each link is associated with a positive weight,
and the length of a path is deﬁned as the sum of the weights of all
the links on that path. Trafﬁc is routed along the shortest paths. In
cases of ties where several outgoing links are on shortest paths to
the destination, the ﬂow is split roughly evenly.
By default, Cisco routers [17] set the weight of a link to be in-
versely proportional to its capacity — we refer to this setting as the
InvCap weight setting. The weights of the links, and thereby the
shortest path routes, can be changed by the network operators to
optimize network performance.
Over the years, many methods [7, 8, 9, 10, 11] have been pre-
sented that compute a set of link weights that minimize congestion
in the resulting shortest path routing of a given trafﬁc matrix. We
shall refer to such a method as an OSPF optimizer, though the re-
sults could equally be applied to IS-IS routing. We use the approach
described in [7, 12], which is based on so-called local search tech-
niques [18]. The method uses heuristics to iteratively improve the
weight setting, changing one or a few weights in each iteration. As
a standard, we ran it for 5000 iteration, taking about 5 minutes of
simulation time. The problem of ﬁnding an optimal weight set-
ting is NP-hard [7], and so we cannot guarantee ﬁnding the true
optimum. The quality of the ﬁnal weight setting is affected by ran-
dom choices made through the iterations, giving some variance in
the quality of the outcome. For example, it is possible that we, by
chance, get a better weight setting for the true trafﬁc matrix from
the estimated trafﬁc matrix than we would get from the real trafﬁc
matrix itself, but the results below show that this random variation
is not very important in practice.
Of course, as argued carefully in [12], it is not attractive to op-
timize the weight setting on-line as the demands change. As in
[12], our weight optimizer works for multiple trafﬁc matrices. Even
more importantly we will consider the impact of using the opti-
mized routes as a permanent weight setting. This permanent weight
setting is then tested on the true trafﬁc matrices of the subsequent
days.
3. ESTIMATING TRAFFIC MATRICES
FROM LINK DATA
This section describes three methods for estimating trafﬁc ma-
trices from link load data. The ﬁrst two methods are based on so
called “Gravity models” while the third uses (in addition) “Net-
work tomography” methods. Although it might be appealing to
test some more complex algorithms, the sub-sample of possibili-
ties presented here is sufﬁcient to illustrate the points of interest.
What’s more we ﬁnd a near optimal combination of estimation and
routing optimization algorithms in any case, so there is little to be
gained in using a more complex method.
This section is not intended to provide a detailed description of
the estimator algorithms (which may be found in [4]). This is not
intended as a study of the estimators. The novel aspect is what
happens when the estimators are combined with routing optimizers
and tested on real trafﬁc matrices. The description here is to pro-
vide some insight into the relationship between the three algorithms
tested.
Gravity models [19, 20, 21], are often used by social scientists
to model the movement of people, goods or information between
geographic areas [20, 21]. Recently, variations on gravity models
have also been proposed for computing trafﬁc matrices [3, 4, 5].
At the heart of the gravity model approach is a proportionality
assumption: the amount of trafﬁc from a given source to a given
sink is proportional to the total trafﬁc to the output sink, inde-
pendent of source. For example, in a gravity model for car trafﬁc
between cities the relative strength of the interaction between two
cities might be modeled as proportional to the product of the popu-
lations divided by a distance related “friction” term. Similarly, the
simplest possible gravity models for the Internet assume that the
trafﬁc exchanged between locations is proportional to the volumes
entering and exiting at those locations, though in this case we as-
sume the distance related term is a constant because interactions
in the Internet are less distance sensitive. This simple model of
the Internet is used in [22], and we refer to it as the simple gravity
model.
It is possible to generalize the simple gravity model in a number
of ways [3, 4, 5] to take into account additional information pro-
vided by detailed link classiﬁcation and routing policies. [3, 4, 5]
have shown these gravity models to be signiﬁcantly more accurate
than the simple gravity models. We test the generalized gravity
model of [4] in which additional information on points of ingress
and egress for trafﬁc ﬂows can be incorporated to explicitly model
hot-potato routing for trafﬁc exchanged with peer networks.
By appropriate normalization, the gravity model solution is guar-
anteed to be consistent with the measured link loads at the network
edge, but not necessarily so in the interior links. Alternatively, net-
work tomography methods explicitly include the information mea-
sured from internal links. This information can be written as a set
of linear constraint equations
x = At,
(1)
where x is a vector of the link measurements, t is the trafﬁc matrix
written as a column vector, and A is the routing matrix, whose
terms give the fraction of trafﬁc from a particular origin/destination
pair that traverse each link.
In practice this set of equations is ill-posed, and so to deal with
this difﬁculty tomographic techniques from other ﬁelds have been
used. For a detailed description and comparison (using simple met-
rics) of a number of these methods see [5]. We shall consider a sin-
gle such algorithm, tomogravity, [4] which displays good proper-
ties in terms of scaling, estimation accuracy, speed of computation,
and robustness to errors. The method uses the generalized gravity
model above as a prior (a kicking off point) and reﬁnes it using
a tomographic technique to select an estimate of the trafﬁc matrix
ˆt, that satisﬁes the constraint equations, but that is closest to the
gravity model according to some distance metric.
4. EXPERIMENTAL METHODOLOGY
4.1 Ideal
In this context it is possible to generate arbitrarily bad results
for any particular algorithm by choosing pathological topologies or
trafﬁc matrices, but the important question is how well these al-
gorithms perform on real data. The ideal experiment to test the
use of trafﬁc engineering on estimated trafﬁc matrices would have
SNMP link trafﬁc measurements, a perfect trafﬁc matrix, and exact
topology information, all from exactly the same moment in time.
Finally, the new routing computed should be tested in the real net-
work back at the time when the measurements were made. Unfor-
tunately, most of this is impractical.
Each different type of data has limitations, and practical con-
straints in how it may be collected. For instance
• Currently we do not have high-resolution traces of the net-
work topology, and so we only have snapshot views of the
network;
• Flow-level data (which is the easiest starting point for deriv-
ing a trafﬁc matrix) is not generated as a trafﬁc time series,
but rather an overlapping set of ﬂows, and in many cases can
only be collected on a sampled basis. Furthermore, ﬂow-
level data can be hard to collect in places because it is a
feature of a router, and not all routers support this feature,
or its use conﬂicts with other features. Further, in some
cases, collecting ﬂow-level measurements might result in a
reduction in forwarding performance (which is highly unde-
sirable). Furthermore, ﬂow-level data for an entire network
can be vast — potentially terabytes per day — and handling
this volume of data is daunting in and of itself.
• SNMP link data have many limitations — for instance miss-
ing data (SNMP uses unreliable UDP transport), incorrect
data (through poor router vendor implementations), and a
coarse sampling interval (ﬁve minutes is typical).
• Experimenting with the routing of a real operational tier-1
ISP is not an option. We have to conduct our investigation
with simulations.
The network trafﬁc also exhibits strong daily, and weekly cycles,
and so averaging results over intervals longer than one or two hours
is not very meaningful.
It is difﬁcult to overstate the importance of consistency in the
data. We do not wish the results here to be due to artifacts in the
data, but the above problems make it seemingly impossible to gen-
erate a realistic, completely consistent set of test data. However, [4]
presents an alternative methodology when testing their estimation
algorithm, which we adapt here. In the following section we de-
scribe the data we have available, and the methodology used to test
how well trafﬁc engineering works using estimated trafﬁc matrices.
Also, comparisons against the current routing in the real network
are interesting, but would reveal proprietary information. Instead,
as a benchmark, we here compare our performance against Cisco’s
[17] default InvCap weight setting for OSPF.
4.2 Inputs
This paper does not directly consider SNMP data for the reasons
above. It would be unreasonably difﬁcult to collect SNMP trafﬁc
statistics consistent with the trafﬁc matrix and topology information
available. The approach used here is to use
• sampled ﬂow-level data, and
• topology and routing information as derived from [23].
The ﬂow-level data contains details of numbers of packets and bytes
transferred between source and destination IP addresses, and also
gives information such as the interface at which the trafﬁc entered
our network. Combined with topological and routing information
(as in [1]) one may derive a trafﬁc matrix from such information.
As noted above it is hard to have complete ﬂow-level coverage
of the network. In the data sets used here we cover around 80%
of the edge of a large tier-1 IP network, including all the trafﬁc on
inter-peer links. The trafﬁc matrices generated using this data will
therefore be partial, in the sense that we are missing some rows