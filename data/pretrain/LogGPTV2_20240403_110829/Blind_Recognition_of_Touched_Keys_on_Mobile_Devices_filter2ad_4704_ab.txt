target device. DPM will also generate the negative data itself using
its own data mining methods.
A target device appears different in images from different view-
points. Thus, we need to train a multi-component model. Figure
4 shows the four component model of iPad. The ﬁrst row models
iPad viewed from the right, and the second row models iPad viewed
from the left, and the third and fourth rows model iPad viewed from
the right front and left front of iPad. The ﬁrst column shows the root
model (the coarse model characterizing iPad as a whole), the sec-
ond column shows its parts from different viewpoints, and the third
column visualizes the spatial model of the location of each part
relative to the whole object. This mixture model effectively charac-
terizes the structure and features of iPad. After training, we apply
the learned model to the video frames, and the device is accurately
localized as shown in Figure 5.
Figure 4: Trained iPad DPM Model
Figure 5: Detected iPad (Magniﬁed)
DPM is a very time-consuming object detector and is not computation-
efﬁcient. If the target device is static in the video, we just need to
detect the target device in the ﬁrst frame and crop the same area of
the target device in all the video frames. Otherwise, we have to use
DPM and track the target device in every frame.
The second preprocessing step is to digitally enhance the image
resolution of the target device. We digitally magnify the cropped
video frames. For example, we resize each cropped frame to four
times its original size.
The third preprocessing step is to obtain the reference image of
the software keyboard on the target device. We assume the target
device brand is known and the attacker can get a high quality im-
age of the software keyboard on the touch screen. This image is
the “reference image”, as shown in Figure 2. The image shall show
detailed features of the device, particularly the touch screen sur-
face. For example, for iPad, we choose a black wallpaper so that
the touch screen has a high contrast with its white frame. It is not d-
ifﬁcult to recognize most tablets and smartphones since each brand
has salient features. For example, walking past the victim, the at-
tacker can know the device brand. The attacker may also recognize
the device brand from the video.
3.4 Step 3 - Detecting Touching Frames
Touching frames are those video frames in which the ﬁnger touch-
es the screen surface. To detect touching frames, we need to analyze
the ﬁnger movement pattern of the touching process. Here we an-
alyze the case of people using one ﬁnger to tap on the screen and
input the passcode while we extend our work to tapping with mul-
tiple ﬁngers and two hands in Section 6.
During the touching process, the ﬁngertip ﬁrst moves downward
towards the touch screen, stops, and then moves upward away from
the touch screen. The ﬁnger may also move left or right while mov-
ing downward or upward. We deﬁne the direction of moving to-
ward the device as positive and the opposite direction as negative.
In the process of a key being touched, the ﬁngertip velocity is ﬁrst
positive while moving downward, then zero while stopping on the
1405screen and ﬁnally negative while moving upward. This process re-
peats for each touched key. Therefore, a touching frame is the one
where the ﬁngertip velocity is zero. Sometimes the ﬁnger moves so
fast that there is no frame where the ﬁngertip has a zero velocity.
In such a case, the touching frame is the one where the ﬁngertip
velocity changes from positive to negative.
The challenge to derive the ﬁngertip velocity is to identify the
ﬁngertip. The angle from which we take the video affects the shape
of the ﬁngertip in the video. The ﬁngertip shape also changes when
the soft ﬁngertip touches the hard touch screen surface. People may
also use different areas of the ﬁngertip to tap the screen. We ﬁnd
that when people touch keys with the ﬁngertip, the whole hand most
likely keeps the similar gesture and moves in the same direction.
Instead of tracking the ﬁngertip to identify a touching frame, we
track the hand, which has enough number of feature points for an
automatic tracking.
We employ optical ﬂow theory [38] to derive the velocity of fea-
ture points on the moving hand. Optical ﬂow computes object mo-
tion between two frames. The displacement vector of the points be-
tween subsequent frames is called the image velocity or the optical
ﬂow at that point. We employ the KLT algorithm [46], which can
track sparse points. To make the KLT algorithm effective, we select
unique feature points, which are often corners in the image. The
Shi-Tomasi corner detector [35] is applied to obtain these points.
We track several points in case some points are lost during the track-
ing. Our experiments show that each touch with the ﬁngertip may
produce multiple touching frames. This is reasonable since the ﬁn-
gertip is soft. When a ﬁngertip touches the screen, it deforms and
this deforming process takes time. People may also intentional-
ly stop to make sure that a key is touched. During the interaction
between ﬁngertip and touch screen, some tracked points may also
move upward and create noise for detecting touching frames. We
use a simple algorithm to deal with all the noise: if the velocity of
most tracked points in a frame moves from positive to negative, that
frame is a touching frame. Our experiments show that ﬁve features
points are reliable for detecting all touching frames.
3.5 Step 4 - Deriving the Homography Matrix
In computer vision, automatically deriving the homography ma-
trix H of a planar surface in two images is a well studied problem
[14]. First, a feature detector such as SIFT (Scale-Invariant Feature
Transform) [27] or SURF (Speeded Up Robust Features) [4] is used
to detect feature points. Matching methods such as FLANN (Fast
Library for Approximate Nearest Neighbors) [31] can be used to
match feature points in the two images. The pairs of matched points
are then used to derive the homography matrix via the algorithm of
RANSAC (RANdom SAmple Consensus) [18].
However, those common computer vision algorithms for deriving
homography H are not effective in our context. Because of the
angle of taking videos and reﬂection by the touch screen, there are
few good feature points in the video frames for the algorithms above
to work effectively. Intuitively, touch screen corners are potential
good features, but they are blurry in our context since the video is
taken remotely and the resolution is poor. Therefore, SIFT or SURF
cannot correctly detect these corners.
We derive the homography matrix H in Formula (1) as follows.
H has 8 degrees of freedom. Therefore, to derive the homography
matrix, we need 4 pairs of matching points of the same plane in the
touching frame and reference image. Any three of them should not
be collinear [14]. In our case, we try to use the corners of the touch
screen as shown in Figure 1 and Figure 2. Because the corners in
the image are blurry, to derive the coordinates of these corners, we
ﬁrst detect the four edges of the touch screen. The intersections
of these edges are the desired corners. We apply the Canny edge
detector [9] to extract the edges and use the Hough line detector
[29] to derive candidate lines in the image. We manually choose
the lines aligned to the edges. This is the only manual procedure in
our entire system of blindly recognizing touched keys. After edges
are derived, now we can calculate the intersection points and derive
the coordinates of the four corners of interest. With these four pairs
of matching points, we can derive the homography matrix with the
DLT (Direct Linear Transform) algorithm [14]. If the device does
not move during the touching process, this homography matrix can
be used for all the video frames. Otherwise, we have to derive H
for every touching frame and the reference image.
3.6 Step 5 - Locating the Touching Fingertip
In this step we locate the touching ﬁngertip in the touching frame
to identify where the ﬁngertip touches the screen. Then we can map
the touched point to the reference image by the homography matrix
in order to get the touched key. Again, we turn to the DPM object
detector to locate the touching ﬁngertip in touching frames.
The process of employing DPM to locate the touching ﬁnger-
tip is similar to the process of applying DPM to the detection of
the target device in a video frame. We ﬁrst generate positive da-
ta (touching ﬁngertip) and negative data (non touching ﬁngertip) to
train a model for the “touching” ﬁngertip. To get the positive data,
we take videos in various scenarios and obtain the touching frames.
For each touching frame, we label the touching ﬁngertip with an
appropriate bounding box centered at the center of the touched key.
We derive the center of a key in a touching frame in the following
way. During the training process, we know the touched keys and
can derive their position by mapping the area of a key from the ref-
erence image to the touching frame with the planar homography.
As we know, DPM needs a bounding box that is large enough to
perform well although we want a bounding box as small as possi-
ble. We evaluated bounding boxes of different size. The optimal
bounding box in our context is the one bounding the ﬁngertip, cen-
tered at the touched key and has a size of 40 × 30 pixels. If different
bounding box sizes are used for training images, DPM resizes the
bounded area to a uniform size. To get the negative data, we use the
bounding box around the non-touching ﬁngertip. DPM also gener-
ates negative data itself via data mining and treats the bounding box
with less than 50 percentage intersection with the positive data as
negative data.
After training, DPM produces a multi-component model for the
touching ﬁngertip as visualized in Figure 6. The left column vi-
sualizes the root ﬁlter of a two-component model:
the shape of
the touching ﬁngertip and the interaction between the ﬁngertip top
and the touch screen. Shadow is formed at the ﬁngertip top during
touching. The two components actually model the touching ﬁn-
gertip from different viewpoints respectively. The part models in
the middle column characterize the six parts of the touching ﬁnger-
tip. The spatial models in the last column characterize the location
of each part relative to the root. We apply these models to every
touching frame to detect the touching ﬁngertip. Figure 7 shows the
detected result as the green large bounding box and its center C.
Figure 6: DPM Touching Fingertip
Model
Figure 7: Detected
Touching Fingertip
1406Recall that during the training process, the center C of the large
bounding box estimates the center of a touched key. Therefore, af-
ter DPM is applied to an image, the center of the resultant bounding
box is expected to overlap the center of the touched key. Intuitively,
we can map the center of the detected bounding box directly to the
reference image. The mapped point should fall into the area of the
touched key. We denote this method as “baseline method”. Howev-
er, from the experiments, the baseline method does not work well.
Evaluation will be provided in Table 5 in Section 5. The poor exper-
iment results demonstrate the limitation of the direct use of machine
learning methods to recognize touched keys. The major reason is
that DPM is still a very coarse object detector in our context.
3.7 Step 6 - Estimating the Touched Area
Even though the center of the bounding box derived in Step 5 is
not exactly the center of the touched key, the touched key should
be around the detected ﬁngertip since people tend to touch the cen-
ter of the key. Our extensive experiments on varieties of subjects
have veriﬁed this observation. We need to further analyze the image
patch in this large bounding box given by DPM in order to derive
the accurate touched area, where the ﬁngertip touches the screen.
The complication of lighting and shadowing makes the estima-
tion of accurate touched area a great challenge. We employ two
steps to this end. First, within large bounding box, we locate the
ﬁngertip and get its contour via k-means clustering of pixels. Sec-
ond, we derive the ﬁngertip’s touching direction and train a tiny
bounding box around the top of ﬁngertip as accurate touched area.
Deriving the ﬁngertip contour: We train a small bounding box as
shown in Figure 8 around C of the large bounding box and use the
k-means clustering over this small bounding box to get the ﬁngertip
contour. First, we convert the region of this small bounding box
into a gray scale image and increase its contrast by normalizing
the gray scale image so that its maximum intensity value is 255.
The k-means clustering (K = 2) is then employed to cluster the
pixel values into two categories, dark and bright. This region of
interest is then transformed into a binary image. The intuition is that
the touching ﬁnger is brighter than the area around it. Therefore,
we are able to ﬁnd the contour of the ﬁngertip as the bright area.
Figure 9 shows the contour of the ﬁngertip after we process the
small bounding box.
Deriving the accurate touched area: Once the ﬁngertip contour
is located, we can estimate the top of the ﬁngertip and train a tiny
bounding box around the ﬁngertip top as the accurate touched area.
To derive the ﬁngertip top, for each horizontal line of pixels in the
ﬁngertip contour, we ﬁnd its central point. We then ﬁt a line over
these central points. This line is the central line of the ﬁnger in the
image, indicating the ﬁnger’s touching direction and which part of
the ﬁngertip is used to touch the screen. The intersection between
this line and the ﬁngertip contour produces the top of the ﬁngertip
and the center of the touched area. Figure 9 shows the estimated
top and bottom of the ﬁngertip and its direction. Figure 10 shows a
tiny bounding box we trained around the top of the ﬁngertip.
There are various complications in the two steps above ﬁnding
the tiny bounding box. For example, when we try to ﬁnd the con-
tour of the ﬁngertip, the ideal case is: there is only one contour, i.e.,
the bright ﬁngertip contour, in the small bounding box. However,
lighting and shadowing may introduce noise and produce other s-
mall contours. We have used erosion and dilation techniques [7] to
remove such small contours. Another complication is that the ﬁn-
gertip may have a virtual image on the touch screen, which behaves
like a mirror. The virtual image produces a second large contour.
Such a contour can be identiﬁed by introducing a model of the ﬁn-
gertip’s position. For example, the upper large contour indicates the
Figure 8: Smal-
l Bounding Box
Figure 9:
Fingertip
Contour and Direction
Figure 10: Accurate
Touched Area
actual ﬁngertip. Lighting and shadowing can make the case more
complicated. The two large contours corresponding to the ﬁngertip
and its virtual image may be connected even if erosion and dilation
are applied. In such a case, we locate the convexity defects of the
two connected large contours. The large defects indicate the con-
necting position. We can split the two connected contours at this
position. We have applied various computer vision techniques and
managed to reduce the impact of complications on automatically
recognizing touched keys. However, these complications do affect
our recognition results.
3.8 Step 7 - Recognizing Touched Keys
Although we have derived the tiny and accurate touching area in
Figure 10, such an area is still too large and contains non-touching
points. From our analysis in Section 4 and experiments, an actual
key area contains only tens of pixels. Our goal of Step 7 is to rec-
ognize those actual touched points landed in the key’s area. Once
the actual touched points are located, we can then map them to the
reference image. The corresponding points in the reference image
are denoted as mapped points. Such mapped points should land in
the corresponding key’s area on the software keyboard. Therefore,
we can derive the touched keys. This is the basic idea of blind-
ly recognizing the touched keys even if those touched keys are not
visible in the video. The key challenge is to accurately locate the
touched points in the tiny bounding box. We introduce our model
and methodology of addressing this challenge in Section 4.
4. RECOGNIZING TOUCHED KEYS
In this section, we model how people use their ﬁnger tapping on
the touch screen and the image formation process of a tapping ﬁn-
ger. We then propose a clustering-based strategy to identify touched
points and map these points in the touching frames to keys in the
reference image.
4.1 Formation of Touching Frames
To model how touching frames are formed, we ﬁrst analyze how
people tap on the screen, denoted as touching gestures. According
to [5, 12, 40], there are two types of touching gestures: vertical
touch and oblique touch. In the case of vertical touch, the ﬁnger
moves downward vertically to the touch screen as shown in Figure
11 (a). People may also choose to touch the screen from an oblique
angle as shown in Figure 11 (b), which is the most common touch-
ing gesture, particularly for people with long ﬁngernails. The terms
of vertical and oblique touch refer to the “steepness” (also called
“pitch”) difference of the ﬁnger [12]. From Figure 11, the ﬁnger
orientation (i.e. “yaw”) relative to the touch screen may also be d-
ifferent [41]. The shape and size of a person’s ﬁnger and key size