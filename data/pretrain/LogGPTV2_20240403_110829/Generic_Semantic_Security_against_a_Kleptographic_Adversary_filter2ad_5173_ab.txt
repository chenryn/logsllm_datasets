watchdog W who tests the subverted implementations, by comparing them with the speciﬁcation of
the algorithms. The adversary “wins” if she can manufacture implementations so that she can win
the security game and—at the same time—evade the detection of W. One can arrive at a variety of
diﬀerent deﬁnitions based on the order of quantiﬁcation for W and A, how exactly W is permitted
to test the implementations, and whether W is given any further information (such as a transcript
of the security game). We refer to [RTYZ15] for detailed discussion.
In this paper we will adopt the strongest of the deﬁnitions of [RTYZ15] (which gives the
watchdog the least power): in their terminology, we will consider a universal and oﬄine watchdog.
In such a deﬁnition, the watchdog only tests the implementation once with only oracle access. In
particular, W has no access to the actual communications during the security game. Moreover, the
description of the watchdog is quantiﬁed before the adversary.2 (Thus, security for a particular
primitive requires that there is a single watchdog that can protect against all possible adversaries.)
To formalize the notion that the adversary cannot learn unintended information from an
implementation, we adapt the surveillance game from [BPR14] (which was deﬁned for symmetric
key encryption): speciﬁcally, we compare the information leaked by the implementation with that
leaked by the speciﬁcation (or, equivalently, an honest implementation).
Deﬁning stego-freeness. We now formally deﬁne stego-freeness for any (randomized) algorithm
G under subversion. Following the basic kleptographic models described above, the adversary A
prepares a (potentially subverted) implementation Gimpl of the algorithm G; we let Gspec denote the
speciﬁcation of the algorithm. The goal of the adversary is to utilize Gimpl to leak secret information
exclusively to her via the outputs that Gimpl produces (as in the discussion above). Stego-freeness
means either the adversary A cannot learn any extra information from the outputs of Gimpl (in
comparison with that of Gspec), or the subversion can be detected by the watchdog W (using oracle
2This is stronger than most of the deﬁnitions in the literature. The closest one is [DFP15]; however, their watchdog
has to take the transcript between C and A as inputs which implicitly implies the dependence of the running time on A.
6
access to Gimpl)—this is characterized by the detection advantage DetW,A below. Depending on
how communication is generated and whether the randomized algorithm can takes rich inputs,
we have a variety of deﬁnitions; we begin with the following elementary version for randomized
algorithms—such as key generation—that rely only on a length parameter rather than inputs
drawn from a large space.
Deﬁnition 2.1 (stego-free, basic form). Consider a (randomized) algorithm G with speciﬁcation Gspec.
We say such Gspec is stego-free in the oﬄine watchdog model if there exists a ppt watchdog W so that
for any ppt adversary A playing the following game (see Fig. 2), it satisﬁes that either
AdvA is negligible,
or DetW,A is non-negligible
where
AdvA(1λ) = |Pr[bC = 1]− 1/2|
and DetW,A(1λ) =
(cid:12)(cid:12)(cid:12)Pr[W Gimpl(1λ) = 1]− Pr[Pr[W Gspec(1λ) = 1]
(cid:12)(cid:12)(cid:12) .
A(1λ)
A(1λ)
test phase
W(1λ)
bW ← W Gimpl(1λ)  Gimpl
execute phase
C(1λ)
β ← {impl,spec}
for i = 1 to q
yi ← Gβ(1λ)
bC := 1 if β = β
bC := 0 otherwise

1q
y1, . . . , yq -
(cid:48)
β
(cid:48) 
Figure 2: A game for stego-freeness.
Note that the deﬁnition requires only non-negligible detection probability on the part of the
watchdog. Note that detection probabilities can be directly ampliﬁed by repetition.3
Remark 2.2. Our constructions actually satisfy a stronger condition that directly relates the advantage
to the detection probability. The deﬁnition above demands the asymptotic guarantee that either AdvA is
negligible or DetW,A is non-negligible. This can be strengthened to demand that for every adversary A
there is a polynomial s(λ) (with no constant term, so that s(0) = 0) so that s(AdvA(1λ)) ≤ DetW,A(1λ).
Observe that an oﬄine watchdog can ensure that the implementation of a deterministic algorithm
disagrees with its speciﬁcation with negligible probability when inputs are drawn from a public
input distribution. Throughout, we use the term “public” distribution to refer to any eﬃciently
sampleable source that the watchdog can construct, perhaps using Fispec and Fiimpl.
3Trivial ampliﬁcation transforms a gap of  to 1− δ with k = 
−1) repetitions. As the watchdog’s running time
is ﬁxed independent of the adversary, however, ampliﬁcation cannot be adapted to a particular non-negligible function.
If the watchdog is permitted a number of samples that depends on the adversary, then one can amplify non-negligible
detection probability to 1− o(1) for an inﬁnite set of inputs.
−1 log(δ
7
Lemma 2.3 ([RTYZ15]). Consider an adversarial implementation Πimpl := (F1impl, . . . ,Fkimpl) of a speciﬁ-
cation Πspec = (F1spec, . . . ,Fkspec), where F1, . . . ,Fk are deterministic algorithms. Additionally, for each secu-
impl(x) (cid:44)
rity parameter λ, public input distributions X1
spec(x) : x ← Xj
λ] is non-negligible, this can be detected by a ppt oﬄine watchdog with non-negligible
Fj
probability.
λ are deﬁned respectively. If ∃j ∈ [k],Pr[Fj
λ, . . . , Xk
More general deﬁnitions of stego-freeness. In the above game, G only takes as input a ﬁxed
security parameter (often ignored later in the paper); this deﬁnition can capture algorithms
like randomness generation and key generation when we instantiate G to be the corresponding
functionality. Besides the security parameter, we can consider algorithms which take richer inputs.
Such extensions will be important for our applications, and can signiﬁcantly complicate the task of
destroying an embedded steganographic channel. One note is that for input taken from a small
domain, (for example, {0,1}), we simply allow the adversary to query the evaluation on all inputs.
Beyond the previous cases, we may consider algorithms taking inputs from a large domain. The
most straightforward adaptation permits the adversary to sample Gimpl(1λ, xi) at inputs xi of her
choice. However, this model suﬀers from a crippling “input trigger” attack [DFP15] (where the
adversary hides some secret information at a particular “trigger” location x which can be impossible
for an oﬄine watchdog to detect); we discuss this in detail later. However, there is a compromise
setting that captures many cases of actual interest and permits strong feasibility results. In this
setting we permit the adversary to determine inputs to a randomized algorithm G by specifying a
randomized input generator IG: The input generator may be an arbitrary ppt algorithm with the
condition that given 1λ it produces (typically random) outputs of length exactly λ. This implicitly
deﬁnes the randomized algorithm G(1λ,IG(1λ)). In our setting, the watchdog is provided (oracle
access) to IG, which it may use during its testing of G. Note that IG is not part of the speciﬁcation of
G, but rather chosen by the adversary during the security game; thus there is no reason to consider
subversion of IG. Revisiting the security game in this new setting, challenges {yi} are generated by
ﬁrst sampling mi ← IG(1λ), and then obtaining yi ← Gβ(1λ, mi) by calling Gβ using inputs 1λ and
mi. Note that the adversary could use IG to produce some speciﬁc input “triggers” where Gimpl
deviates from Gspec. This more general notion of stego-freeness (with a “public” input distribution)
captures algorithms that take the output of other algorithms as input, which will be critical when
we reason about amalgamation of algorithms. See Figure 3 below for a uniﬁed game, where the
algorithm may take both types of inputs.
Deﬁnition 2.4 (stego-free, general form). We say that a randomized algorithm G is stego-free if it
satisﬁes Deﬁnition 2.1 with the security game of Figure 3. Note that the ppt input generator IG may be
determined by the adversary during the game.
Which of the deﬁnitions (2.1 or 2.4) is appropriate for a given randomized algorithm can be
determined from context, depending on whether an input generator is speciﬁed.
As mentioned above, an even stronger deﬁnition is obtained by permitting the adversary to
simply choose the input mi for each yi directly. This notion reﬂects stego-freeness for algorithms
with adversarially chosen inputs. Such a subverted implementation may have a hidden “trigger”
that was randomly drawn during the (adversarial) manufacturing process and can permit the
adversary to easily win the stego-freeness distinguishing game. In fact, such a trigger attack
does not even require that G be randomized: for example, consider the algorithm Gspec(1λ, x) := x,
8
test phase
W(1λ)
bW ← W Gimpl,IG(1λ)
 Gimpl,IG
execute phase
A
A(1λ)
C
β ← {impl,spec}
for i = 1 to q
mi ← IG(1λ)
yi = Gβ(1λ, mi)
(cid:48)
bC := 1 if β = β
bC := 0 otherwise

1q
{yi}i∈[q] -
(cid:48)

β
Figure 3: The stego-freeness game with input distribution {1λ}× IG.
deﬁned for x ∈ {0,1}λ. The adversary then uniformly draws z ← {0,1}λ and deﬁnes
0λ
x
Gimpl(1λ, x) =
if x = z,
otherwise.
As the placement of the trigger (z) is random, the watchdog cannot detect disagreement between
Gimpl and Gspec, while the adversary can distinguish these algorithms easily by querying z. In a
practical setting, an algorithm with such an input trigger can leak arbitrary private data to an
adversary in a way undetectable to an oﬄine watchdog. This was formally demonstrated in [DFP15]
and called an “input-triggered subversion attack.” Nevertheless, we will discuss in Section 4.1
a method for sidestepping this impossibility with only an oﬄine watchdog by assuming some
minimum trusted operations, such as “one trusted addition.”4
These deﬁnitions of stego-freeness are suﬃcient for capturing most of the interesting use cases
we will require (e.g., reﬂecting key generation and encryption).
Remark 2.5. Following Lemma 2.3, observe that if Gspec is deterministic, an oﬄine watchdog can ensure
that inconsistencies (Gspec(x) (cid:44) Gimpl(x)) occur with only negligible probability when inputs are sampled
from the input distribution IG (by drawing and testing a sample). In particular, deterministic algorithms
with a public input distribution satisfy stego-freeness in a straightforward fashion.
Discussions about stego-freeness and steganography. We emphasize two properties of these deﬁ-
nitions. First, if a proposed speciﬁcation satisﬁes such deﬁnitions, direct use of the implementation—
rather than the speciﬁcation—preserves the typical security guarantees originally possessed by the
4All previous works either simply assume it won’t happen (the decryptability assumption) or employ an omniscient
watchdog who has access to the transcript between the challenger C and the adversary A, (and the secret key of C).
9
speciﬁcation. This enables us to provide fairly modular security proofs by designing speciﬁcations
for each algorithm with stego-freeness.
The second, and more critical, issue pertains to the feasibility of achieving these notions of stego-
freeness: in particular, at ﬁrst glance they appear hopeless. It is known that general steganography
is always possible over a channel with suﬃcient entropy [Sim83, Sim86, HLv02]. This implies
that the subverted algorithm Gimpl can always produce a sequence of messages that enable the
adversary to retrieve secret data from the (public) outputs y1, . . . , yq. In particular, as shown by
Bellare, Paterson, Rogaway in the seminal result [BPR14], a subverted randomized encryption
algorithm can generate ciphertexts so that the adversary can recover the secret key bit-by-bit from
the sequence of ciphertexts. Moreover, the distribution of these subverted ciphertexts is statistically
close to the natural, unsubverted ciphertext distribution. To make matters worse, such attacks can
be launched even if the subverted implementations are stateless [BJK15]. As a simple example
of such subversion in our setting, consider the algorithm Gspec(1λ) which outputs a uniformly
random element of {0,1}λ. Consider then the subverted implementation Gzimpl(1λ) whose behavior
is determined by a uniformly random string z ← {0,1}λ chosen by the adversary: the algorithm
Gzimpl(1λ) outputs a uniformly random element of the set
H = {w ∈ {0,1}λ | lsb(Fz(w)) = 0} ,
where lsb(x) denotes the least-signiﬁcant bit of x and Fz(·) denotes a pseudorandom function (PRF)
with key z. (Note that elements of H can be drawn by rejection sampling.) Of course, it is easy
for the adversary to distinguish Gimpl from Gspec (as Gimpl only outputs strings with a particular
property that is easily testable by the adversary who has z). On the other hand, no watchdog can
distinguish these algorithms without breaking the PRF. This suggests that if the user makes only
black-box use of the subverted implementation of randomized algorithms, it is hopeless to achieve
stego-freeness. This motivates the following non-black-box model.
The split-program methodology and trusted amalgamation. To overcome the steganographic
attacks discussed above, we propose a slightly modiﬁed model which permits the speciﬁcation of
an algorithm to be split into several components. In this split-program model, each component of
the implementation is exposed to the watchdog to check, while the challenger will amalgamate
the components to yield the fully functional implementation. Of course, the implementation of
each component is still presented by the adversary. We permit decomposition into only a constant
number of components (independent of input length), with the demand that the desired algorithm
can furthermore be expressed as the composition of a constant number of the components. Note
that such a “split-program” presentation of an algorithm rules out any gate-by-gate treatment, as
the model permits only a constant number of compositions. Intuitively, this simple non-black-
box presentation of a randomized algorithm not only provides more opportunity to enforce the
malicious implementation to follow a certain pattern, but also enables the watchdog to do more
delicate checking on the inner structure.
One example of this framework is the simple split-program method proposed in [RTYZ15] to
study certain randomized algorithms: they begin by specifying a (general) randomized algorithm
G as a pair (RG,dG) where RG is the randomness generation algorithm, responsible for generating a
uniform random string of appropriate length, and dG is a deterministic algorithm that takes the
input to the original randomized algorithm G and the random coins produced by RG to produce
the ﬁnal output. They then add to this speciﬁcation a third deterministic algorithm Φ which acts
as a kind of “immunization function” for the random bits generated by RG. Speciﬁcally, given the
10
implementations (RGimpl, dGimpl, Φimpl), the challenger amalgamates them by ﬁrst querying r0 ←
RGimpl, “sanitizing” this randomness by passing it into Φimpl to receive r ← Φimpl(r0) and, ﬁnally,
running y ← dGimpl(r). They show that in several contexts such an “immunization” can preserve
security even under subversion. We remark that a simple decomposition and amalgamation of this
form cannot destroy steganography in general, and we show an explicit attack in this model; see
Sec. 3.1.
To reﬂect such trusted amalgamation in our security model, we permit the challenger to carry
out (a constant number of) compositions without molestation; that is, the notion of “composition” is
protected from adversarial subversion. This can be inferred from the deﬁnition of the speciﬁcation,
and it is implicit in the security games deﬁned, e.g., in Figure 11, and in Figure 12, in the appendix.
As mentioned above, the split-program method proposed in [RTYZ15] permitted them to
establish security (in the kleptographic model) for speciﬁc cryptographic primitives. In this paper,
we will show that this general methodology has remarkable power against subversion: by further
decomposition and amalgamation, we show it is possible to generically destroy steganographic
channels. This provides us a family of tools for developing kleptographically-secure cryptography
without abandoning randomized algorithms.
Stateful algorithms. As discussed above, steganographic attacks can be launched even if the
implementation is stateless. To simply our presentation, most of our discussion adopts this
stateless assumption. However, adaptations of our techniques can provide security even for stateful
implementations (in the sense that each functionality maintains internal state). These ampliﬁed
results require a slightly stronger watchdog (whose running time can depend on the adverary) and
more detailed control of the subverted algorithm to ensure that they receive inputs from public
distributions. See Remark 3.2 in Sec. 3.2 for more discussion.
3 Eliminating Subliminal Channels in Randomized Algorithms
In this section, we will present our main result: provable destruction of any subliminal channels in
subverted implementations of randomized algorithms.
First, we motivate our new constructions by showing that the steganographic attacks of [BPR14,
BJK15] can still be carried out in the simple split-program model introduced by [RTYZ15]. The
attack succeeds even if the associated “immunizing function” Φ is a trusted hash function modeled
as a random oracle. This indicates that some stronger form of immunization is necessary for