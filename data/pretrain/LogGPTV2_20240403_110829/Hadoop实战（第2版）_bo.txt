tar xvzf ml-data.tar__0.gz
将文件加载到表中，代码如下：
LOAD DATA LOCAL INPATH'ml-data/u.data'
OVERWRITE INTO TABLE u_data；
Count the number of rows in table u_data：
SELECT COUNT（*）FROM u_data；//由于版本问题，如果此处出现错误，你可能需要使用COUNT（1）替换
COUNT（*）
下面可以基于该表进行一些复杂的数据分析操作，此处我们使用Python语言，首先创建Python脚本，如代码清单11-4所示。
代码清单11-4 weekday_mapper.py脚本文件
import sys
import datetime
for line in sys.stdin：
line=line.strip（）
userid, movieid, rating, unixtime=line.split（'\t'）
weekday=datetime.datetime.fromtimestamp（float（unixtime））.isoweekday（）
print'\t'.join（[userid, movieid, rating, str（weekday）]）
使用如下mapper脚本调用weekday_mapper.py脚本进行操作。
CREATE TABLE u_data_new（
userid INT，
movieid INT，
rating INT，
weekday INT）
ROW FORMAT DELIMITED
FIELDS TERMINATED BY'\t'；
add FILE weekday_mapper.py；
INSERT OVERWRITE TABLE u_data_new
SELECT
TRANSFORM（userid, movieid, rating, unixtime）
USING'python weekday_mapper.py'
AS（userid, movieid, rating, weekday）
FROM u_data；
SELECT weekday, COUNT（*）
FROM u_data_new
GROUP BY weekday；
2.Apache网络日志数据（Weblog）
可以定制Apache网络日志数据格式，不过一般管理者都使用默认的格式。对于默认设置的Apache Weblog可以使用以下命令创建表：
add jar../build/contrib/hive_contrib.jar；
CREATE TABLE apachelog（
host STRING，
identity STRING，
user STRING，
time STRING，
request STRING，
status STRING，
size STRING，
referer STRING，
agent STRING）
ROW FORMAT SERDE'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES（
"input.regex"="（[^]*）（[^]*）（[^]*）（-|\\[[^\\]]*\\]）（[^\"]*|\"[^\"]*\"）
（-|[0-9]*）（-|[0-9]*）（?：（[^\"]*|\"[^\"]*\"）（[^\"]*|\"[^\"]*\"））?"，
"output.format.string"="%1$s%2$s%3$s%4$s%5$s%6$s%7$s%8$s%9$s"
）
STORED AS TEXTFILE；
更多内容可以查看http：//issues.apache.org/jira/browse/HIVE-662。
11.4 Hive网络（Web UI）接口
通过Hive的网络接口可以更方便、更直观地操作，特别是对刚接触Hive的用户。下面看看网络接口具有的特性。
（1）分离查询的执行
在命令行（CLI）下，要执行多个查询就要打开多个终端，而通过网络接口，可以同时执行多个查询，网络接口可以在网络服务器上管理会话（session）。
（2）不用本地安装Hive
用户不需要本地安装Hive就可以通过网络浏览器访问Hive并进行操作。如果想通过Web与Hadoop及Hive交互，那么需要访问多个端口。而一个远程或VPN的用户只需要访问Hive网络接口所使用的0.0.0.0 tcp/9999。
 11.4.1 Hive网络接口配置
使用Hive的网络接口需要修改配置文件hive-site.xml。通常不需要额外地编辑默认的配置文件，如果需要编辑，可参照以下代码进行：
＜property＞
＜name＞hive.hwi.listen.host＜/name＞
＜value＞0.0.0.0＜/value＞
＜description＞This is the host address the Hive Web Interface will listen on＜/
description＞
＜/property＞
＜property＞
＜name＞hive.hwi.listen.port＜/name＞
＜value＞9999＜/value＞
＜description＞This is the port the Hive Web Interface will listen on＜/
description＞
＜/property＞
＜property＞
＜name＞hive.hwi.war.file＜/name＞
＜value＞${HIVE_HOME}/lib/hive_hwi.war＜/value＞
＜description＞This is the WAR file with the jsp content for Hive Web Interface＜/
description＞
＜/property＞
在配置文件中，监听端口默认是9999，也可以通过hive配置文件对端口进行修改。当配置完成后，我们可以通过hive--service hwi命令开启服务。具体操作如下所示：
hive--service hwi
12/05/17 20：02：26 INFO hwi.HWIServer：HWI is starting up
1 2/0 5/1 7 2 0：0 2：2 7 I N F O m o r t b a y.l o g：L o g g i n g t o o r g.s l f 4 j.i m p l.
Log4jLoggerAdapter（org.mortbay.log）via org.mortbay.log.Slf4jLog
12/05/17 20：02：27 INFO mortbay.log：jetty-6.1.26
12/05/17 20：02：28 INFO mortbay.log：Extract/home/hadoop/hadoop-1.0.1/hive-0.8.1/
lib/hive-hwi-0.8.1.war to/tmp/Jetty_0_0_0_0_9999_hive.hwi.0.8.1.war__hwi__.
m9wzki/webapp
12/05/17 20：02：29 INFO mortbay.log：Started SocketConnector@0.0.0.0：9999
这样我们通过浏览器访问网络接口的地址：http：/masterIP：9999/hwi即可，如图11-2所示。
图 11-2 Hive的网络接口（WebUI）
可以看到Hive的网络接口拉近了用户和系统的距离。我们可以通过网络直接创建会话，并进行查询。用户界面和功能展示非常直观，适合刚接触到Hive的用户。
11.4.2 Hive网络接口操作实例
下面我们使用Hive的网络接口进行简单的操作。
从图11-2中可以看出，Hive的网络操作接口包含数据库及表信息查询、Hive查询、系统诊断等功能，下面分别对其进行介绍。
1.数据库及表信息查询
单击Browse Schema可以查看当前Hive中的数据库，界面中显示的是当前可以使用的数据库信息，只包含一个数据库（default）；再单击default，就可以看到default数据库中包含的所有表的信息了，如图11-3所示。
图 11-3 Hive数据库表
在图11-3中，选择某一个具体的数据库就可以直接浏览该数据库的模式信息了。以代码清单11-3所创建的影片评分表表为例，图11-4为该表的模式信息。
图 11-4 u3_data表模式
2.Hive查询
在进行Hive查询之前首选创建一个会话（Session）。在创建完会话之后，我们可以通过List Session链接列出所有的Session。当Hive重启后，Session信息将全部丢失。会话与认证（Authorize）是相互关联的。在创建一组会话之后，我们可以通过Authorize链接创建该组的认证信息。认证信息包括用户和组。某组会话的用户和组被指定后将不能改变。可以通过认证来启用不同的会话组。
下面通过图11-5具体介绍如何使用创建的会话进行Hive数据查询操作。
图 11-5 会话管理界面
如图11-5所示，用户可以在Query窗口中输入查询语句。我们在用户框中输入如下代码来查看操作结果。此时需要指定Result File（结果文件）并将Start Query（开始查询）选项置为YES。
select*from u1_data limit 5；
单击View File（查看文件），操作结果如图11-6所示。
图 11-6 操作结果
通过WebUI也可以执行复杂的查询，但是这样做的缺点是用户不了解查询的状态，交互能力较差。当查询所需时间较长的时候用户需要一直等待操作的结果。
11.5 Hive的JDBC接口
通过上面的介绍我们知道，用户可以使用命令行接口（CLI）和Hive进行交互，也可以使用网络接口（Web UI）和Hive进行交互。本节我们将具体介绍JDBC接口。如果是以集群中的节点作为客户端来访问Hive，则可以直接使用jdbc：hive：//。对于一个非集群节点的客户端来说，可以使用jdbc：hive：//host：port/dbname来进行访问。
为了方便用户的使用，下面我们介绍如何使用Eclipse进行程序的开发。
 11.5.1 Eclipse环境配置
首先在Eclipse中创建一个Java工程，例如HiveTest。创建完Java工程后需要修改工程的库文件，添加编译Hive程序所必需的JAR包。
Hive工程依赖于Hive JAR包、日志JAR包。由于Hive的很多操作依赖于MapReduce程序，因此Hive工程中还需要引入Hadoop包。在创建完Hive工程后，我们通过引入外部包添加Hive依赖包。在Hive工程上点击右键，选择：“Properties”→“Java Build Path”→“Libraries”→“Add External Jars”，然后选择所需的Jar文件。如图11-7所示为添加好的Jar包。
图 11-7 Hive工程依赖包
在完成上述操作后便可以使用Eclipse编写Hive程序了。完成之后，选择Run as Java Application即可。
11.5.2 程序实例
在使用JDBC链接Hive之前，首先需要开启Hive监听用户的链接。开启Hive服务的方法如下所示：
hive--service hiveservice
Service hiveservice not found
Available Services：cli help hiveserver hwi jar lineage metastore rcfilecat
hadoop@master：～/hadoop-1.0.1/hive-0.8.1/bin/ext$hive--service hiveserver
Starting Hive Thrift Server
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201205150632_559026727.txt
下面是一个使用Java编写的JDBC客户端访问的代码样例：
package cn.edu.rnc.cloudcomputing.book.chapter11；
import java.sql.SQLException；
import java.sql.Connection；
import java.sql.ResultSet；
import java.sql.Statement；
import java.sql.DriverManager；
public class HiveJdbcClient{
/**
*@param args
*@throws SQLException
*/
public static void main（String[]args）throws SQLException{
//注册JDBC驱动
try{
Class.forName（"org.apache.hadoop.hive.jdbc.HiveDriver"）；
}catch（ClassNotFoundException e）{