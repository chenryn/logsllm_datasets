Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale Service Environment
FRED LIN, Facebook, Inc.
KEYUR MUZUMDAR, Facebook, Inc.
NIKOLAY PAVLOVICH LAPTEV, Facebook, Inc.
MIHAI-VALENTIN CURELEA, Facebook, Inc.
SEUNGHAK LEE, Facebook, Inc.
SRIRAM SANKAR, Facebook, Inc.Root cause analysis in a large-scale production environment is challenging due to the complexity of services running across global data centers. Due to the distributed nature of a large-scale system, the various hardware, software, and tooling logs are often maintained separately, making it difficult to review the logs jointly for understanding production issues. Another challenge in reviewing the logs for identifying issues is the scale -there could easily be millions of entities, each described by hundreds of features. In this paper we present a fast dimensional analysis framework that automates the root cause analysis on structured logs with improved scalability.We first explore item-sets, i.e. combinations of feature values, that could identify groups of samples with sufficient support for the target failures using the Apriori algorithm and a subsequent improvement, FP-Growth. These algorithms were designed for frequent item-set mining and association rule learning over transactional databases. After applying them on structured logs, we select the item-sets that are most unique to the target failures based on lift. We propose pre-processing steps with the use of a large-scale real-time database and post-processing techniques and parallelism to further speed up the analysis and improve interpretability, and demonstrate that such optimization is necessary for handling large-scale production datasets. We have successfully rolled out this approach for root cause investigation purposes in a large-scale infrastructure. We also present the setup and results from multiple production use cases in this paper.CCS Concepts: • Information systems → Association rules; Collaborative filtering; • Computing method-ologies → Feature selection; • Computer systems organization → Reliability; Maintainability and maintenance; • Software and its engineering → Software maintenance tools; Software system models;• Security and privacy → Intrusion detection systems.Additional Key Words and Phrases: root cause analysis; anomaly detection; dimension correlation analysis; investigation analysis; large-scale service environment
ACM Reference Format: 
Fred Lin, Keyur Muzumdar, Nikolay Pavlovich Laptev, Mihai-Valentin Curelea, Seunghak Lee, and Sriram Sankar. 2020. Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale Service Environment.Proc. ACM Meas. Anal. Comput. Syst. 4, 2, Article 31 (June 2020), 24 pages. 
Authors’ addresses: Fred Lin, Facebook, Inc. PI:EMAIL; Keyur Muzumdar, Facebook, Inc. PI:EMAIL; Nikolay Pavlovich Laptev, Facebook, Inc. PI:EMAIL; Mihai-Valentin Curelea, Facebook, Inc. PI:EMAIL; Seunghak Lee, Facebook, Inc. PI:EMAIL; Sriram Sankar, Facebook, Inc. sriramsankar@fb.com.| Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.© 2020 Association for Computing Machinery.
2476-1249/2020/6-ART31 $15.00 
https://doi.org/10.1145/3392149 |  |
|---|---|| Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2020 Association for Computing Machinery. 2476-1249/2020/6-ART31 $15.00  https://doi.org/10.1145/3392149 |31 |Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
31:2 	Lin, et al.
1 	INTRODUCTIONCompanies running Internet services have been investing in autonomous systems for managing large scale services, for better efficiency and scalability [12]. As some of the Internet services have become utilities that the public relies on for transportation, communication, disaster response, etc., the reliability of the infrastructure is now emphasized more than before. There are various logs that these systems record and act upon. The logs record events and configurations about the hardware, the services, and the automated tooling, which are important in measuring the performance of the system and tracing specific issues. Given the distributed nature and the scale of a modern service environment, it is challenging to find and monitor patterns from the logs, because of the scale and the complexity of the logs - each component in the system could record millions of entities that are described by hundreds of features. An automated RCA (Root Cause Analysis) tool is therefore needed for analyzing the logs at scale and finding strong associations to specific failure modes. 	Traditional supervised machine learning methods such as logistic regression are often not interpretable and require manual feature engineering, making them impractical for this problem. Castelluccio et al. proposed to use STUCCO, a tree-based algorithm for contrast set mining [4] for analyzing software crash reports [11]. However, the pruning process in STUCCO could potentially drop important associations, as illustrated in Section 3.7.In this paper, we explain how we modified the classical frequent pattern mining approach, Apriori [2], to handle our root cause investigation use case at scale. While Apriori has been an important algorithm historically, it suffers from a number of inefficiencies such as its runtime and the expensive candidate generation process. The time and space complexity of the algorithm are exponential O(2D) where D is the total number of items, i.e. feature values, and therefore it is practical only for datasets that can fit in memory. Furthermore, the candidate generation process creates a large number of item-sets, i.e. combinations of feature values, and scans the dataset multiple times leading to further performance loss. For these reasons, FP-Growth has been introduced which significantly improves on Apriori’s efficiency.FP-Growth is a more efficient algorithm for frequent item-set generation [13]. Using the divide-and-conquer strategy and a special frequent item-set data structure called FP-Tree, FP-Growth skips the candidate generation process entirely, making the algorithm more scalable and applicable to datasets that cannot fit in memory. As we show in the experimental results in Section 4, FP-Growth can be 50% faster than a parallelized Apriori implementation when the number of item-sets is large. 	While FP-Growth is significantly more efficient than Apriori, some production datasets in large-scale service environments are still too large for FP-Growth to mine all the item-sets quickly for time-sensitive debugging. The huge amount of data could also become a blocker for memory IO or the transfer between the database and local machines that run FP-Growth. To further speed up the analysis, we use Scuba [1], a scalable in-memory database where many logs are stored and accessed in real-time. As many recorded events in the production logs are identical except for the unique identifiers such as timestamps and job IDs, we pre-aggregate the events using Scuba’s infrastructure before querying them for the root cause analysis. The pre-aggregation step saves runtime and memory usage significantly, and is necessary for enabling automatic RCA on production datasets at this scale.The framework lets users specify irrelevant features, i.e. columns, in the structured log to be excluded for avoiding unnecessary operations, thereby optimizing the performance. Users can also specify the support and lift of the analysis for achieving the desired tradeoff between the granularity of the analysis and the runtime. For example, a faster and less granular result is needed for mission critical issues that need to remediated immediately; and more thorough results from a slower run are useful for long-term analyses that are less sensitive to the runtime. Parallelism andProc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
Fast Dimensional Analysis for Root Cause Investigation 	31:3
automatic filtering of irrelevant columns are also features for achieving better efficiency, which we discuss in Section 3.5.With the above optimizations, we have productionized a fast dimensional analysis framework for the structured logs in a large-scale infrastructure. The fast dimensional analysis framework has found various association rules based on structured logs in different applications, where the association rules reveal hidden production issues such as anomalous behaviors in specific hardware and software configurations, problematic kernel versions leading to failures in auto-remediations, and abnormal tier configurations that led to an unexpectedly high number of exceptions in services. 	The rest of the paper is organized as follows: We discuss the requirements of a large-scale service environment, the advantage of logging in a structured format, the typical RCA flow in Section 2. We illustrate the proposed framework in Section 3. We demonstrate the experimental results in Section 4, and the applications on large-scale production logs in Section 5. Section 6 concludes the paper with a discussion on future work.2 	ROOT CAUSE ANALYSIS IN A LARGE-SCALE SERVICE ENVIRONMENT
2.1 	Architecture of a Large-Scale Service EnvironmentLarge scale service companies like Google, Microsoft, and Facebook have been investing in data centers to serve globally distributed customers. These infrastructures typically have higher server-to-administrator ratio and fault tolerance as a result of the automation that is required for running the services at scale, and the flexibility to scale out over a large number of low-cost hardwares instead of scaling up over a smaller set of costly machines [12]. Two important parts for keeping such large-scale systems at high utilization and availability are resource scheduling and failure recovery.Resource scheduling mainly focuses on optimizing the utilization over a large set of heterogeneous machines with sufficient fault tolerance. Various designs of resource scheduling have been well-documented in literature, such as Borg from Google [24], Apollo from Microsoft [9], Tupperware from Facebook [29], Fuxi from Alibaba [32], Apache Mesos [15] and YARN [23].The ultimate goal for a failure recovery system is to maintain the fleet of machines at high availability for serving applications. Timely failure detection and root cause analysis (RCA), fast and effective remediation, and proper spare part planning are some of the keys for running the machines at high availability. While physical repairs still need to be carried out by field engineers, most parts in a large-scale failure recovery system have been fully automated to meet the requirements for high availability. Examples of the failure handling systems are Autopilot from Microsoft [16] and FBAR from Facebook [17].2.2 	Logs in a Large-Scale System
Proper logging is key to effectively optimizing and maintaining a large-scale system. In a service environment composed of heterogeneous systems, logs come from three major sources:• Software - The logs populated from the services running on the servers are critical for debugging job failures. Job queue times and execution times are also essential for optimizing the scheduling system. Typically program developers have full control in how and where the events should be logged. Sometimes a program failure needs to be investigated together with the kernel messages reported on the server, e.g. out of memory or kernel panic.• Hardware - Hardware telemetries such as temperature, humidity, and hard drive or fan spinning speed, are collected through sensors in and around the machines. Hardware failures are logged on the server, e.g. System Event Log (SEL) and kernel messages (dmesg). The hardware and firmware configurations of the machine are also critical in debugging hardwareProc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
31:4 	Lin, et al.
issues, e.g. the version of the kernel and the firmwares running on different components. The messages on the servers need to be polled at an appropriate frequency and granularity that strikes a balance between the performance overhead on the servers and our ability to detect the failures timely and accurately.• Tooling - As most of the parts in the large-scale system are automated, it is important to monitor the tools that orchestrate the operations. Schedulers would log the resource allocations and job distribution results. Failure recovery systems would log the failure signals and the remediation status. Historical tooling logs are important for analyzing the tooling efficiency.For root cause analysis in real-time, the logs are pushed to Scuba, a “fast, scalable, distributed, in-memory database.” [1] Keeping the data in-memory, Scuba tables typically have shorter retention. For long-term analytics, logs are archived in disk-based systems such as HDFS [8], which can be queried by Hive [21] and Presto [22]. Some of the more detailed operational data can be fetched from the back-end MySQL databases [19] to enrich the dataset for the analysis. The quality of the logs has fundamental impacts on the information we can extract. We will discuss the advantages of structured logging in Section 3.2.2.3 	Prior Root Cause Analysis Work
Root cause analysis (RCA) is a systematic process for identifying the root causes of specific events, e.g. system failures. RCA helps pinpoint contributing factors to a problem or to an event. For example, RCA may involve identifying a specific combination of hardware and software configurations that are highly correlated to unsuccessful server reboots (discussed in Section 5.1), and identifying a set of characteristics of a software job that are correlated to some types of job exceptions (discussed in Section 5.3).During an incident in a large-scale system, the oncall engineers typically investigate the under-lying reason for a system failure by exploring the relevant datasets. These datasets are comprised of tables with numerous columns and rows, and often the oncall engineers would try to find aggregations of the rows by the column values and correlate them with the error rates. However, a naive aggregation scales poorly due to the significant amount of the rows and distinct values in the columns, which result in a huge amount of groups to be examined.For automating RCA, the STUCCO algorithm has been used for contrast set mining [4, 5, 11]. Suriadi et al. [20] demonstrated an RCA approach using decision tree-based classifications from the WEKA package [27], as well as enriching the dataset with additional features. The traditional STUCCO algorithm, however, can miss important associations if one of the items does not meet the pruning threshold on the χ2value, as explained in Section 3.7. Decision tree-based approaches, while providing the visibility in how the features are used to construct the nodes, become harder to tune and interpret as the number of trees grows. To ensure we capture the associations that are relatively small in population yet strongly correlated to our target, we choose to explore all association rules first with additional filtering based on support and lift as post-processing, as illustrated in Section 3.6.In association rule mining, FP-Growth [13] has become the common approach as the classical Apriori algorithm [2] suffers from its high complexity. The state-of-the-art approaches for log-based root cause analysis found in literature often have small-data experiments, do not handle redundant item-sets with pre-/post-processing and fail to find root causes with small error rates relative to successes. For example, while the authors in [30] provide a pre-processing step for pre-computing the association matrix for speeding up association rule mining, it lacks the study of large scale applicability and the filtering of small error classes (see Figure 5 and Section 4.2 for how theProc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
Fast Dimensional Analysis for Root Cause Investigation 	31:5proposed fast dimensional analysis framework addresses these points). Similarly, the authors in [3] provide a hybrid approach for spam detection using a Naive Bayes model with FP-Growth to achieve better spam-detection accuracy but with a decreased interpretability because only the prediction is provided and not the root-cause. In our use case, however, having an explainable model is critical (see Section 4.2) and we are biased away from compromises in interpretability. 	An FP-Growth implementation on Spark platform was proposed in [18], and an extension for FP-Growth to handle negative association rules [28] was proposed in [25]. For better interpretability, Bittmann et al. proposed to remove ubiquitous items from the item-sets using lift [6], whereas Liu et al. used lift to further prune the mined association rules from FP-Growth [18].For sustaining the large-scale service environment at high availability all the time, we need an analysis framework that can handle large-scale production datasets, e.g. billions of entries per day, and generate result in near real-time, e.g. seconds to minutes. In this paper we propose a framework that pre-aggregates data to reduce data size by > 500X using an in-memory database [1], mines frequent item-sets using a modified version of FP-Growth algorithm, and filters the identified frequent item-sets using support and lift for better interpretability. We validate the framework on multiple large-scale production datasets from a service environment, whereas the related papers mostly demonstrate the results using relatively small synthetic datasets. We will illustrate the details of the framework in Section 3 and compare the framework with the above-mentioned methods in Section 4.3 	FAST DIMENSIONAL ANALYSISWe propose an RCA framework that is based on the FP-Growth algorithm [13], with multiple optimizations for production datasets in a large-scale service system. After querying the data, which is pre-aggregated using Scuba’s infrastructure [1], the first step in this framework is identifying the frequent item-sets in the target state, e.g. hardware failures or software exceptions. Item-sets are combinations of feature values of the samples. In a structured dataset, e.g. Table 1, the columns are considered the features of the entities, and each feature could have multiple distinct values in the dataset. We refer to feature values as items in the context of frequent pattern mining. For example, when analyzing hardware failures, the items could be the software configuration of the server such as the kernel and firmware versions, as well as the hardware configuration such as the device model of the various components. When analyzing software errors, the items could be the memory allocation, the machines where the jobs are run, and the version of the software package. The number of items in an item-set is called the length of the item-set. Item-sets with greater lengths are composed of more feature values and are therefore more descriptive about the samples.The second step in RCA is checking the strength of the associations between item-sets and the target states. We propose multiple pre- and post-processing steps for improving the scalability and the interpretability of the framework in Section 3.5 and 3.6.
3.1 	Metrics for Evaluating the CorrelationsThree main metrics are typically considered in an RCA framework: support, confidence, and lift. We first describe the meaning behind these metrics in the context of root cause analysis and then describe why we picked support and lift as our main metrics to track.
Support was introduced by Agrawal, et al. in [2] as
supp(X) =|t ∈ D;X ⊆ t| |D| 	= P(X)
where D = {t1,t2, ...,tn} is a database based on a set of transactions tk. (1)Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
31:6 	Lin, et al.Support of X with respect to D refers to the portion of transactions that contain X within D. In our RCA problem, D is equivalent to the entire structured log, while each entry is considered a transaction t. Support has a downward closure property, which is the central idea behind Apriori frequent item-set mining algorithm. Downward closure implies that all subsets of a frequent item-set are also frequent. Analogously, all supersets of an infrequent item-set can be safely pruned because they will never be frequent. The range of support is [0, 1]. When mining frequent item-sets, the frequency of an item-set is defined based on the samples in the target failure state, so we limit the database to the transactions that cover the target failure state Y (e.g. software job status =exception). In this context, support can therefore be formulated assupp(X,Y) =f requency(X,Y) f requency(Y) 	= P(X |Y) 	(2)
Hereafter, we refer to supp(X) as the support with respect to all transactions, and supp(X,Y) as the support with respect to the transactions covering Y.
Confidence was introduced by Agrawal et al. in [2] and is defined as
conf (X ⇒ Y) =supp(X ∩ Y) supp(X) 	= P(Y |X) 	(3)Confidence, which ranges from 0 to 1, refers to the probability of X belonging to transactions that also contain Y. Confidence is not downward closed and can be used in association rule mining after frequent item-sets are mined based on support. Confidence is used for pruning item-sets where conf (X ⇒ Y) < γ, where γ is a minimum threshold on confidence. Using confidence is likely to miss good predictors for Y under imbalanced distribution of labels. For example, suppose that we have 100 failures and 1 million reference samples. If feature X exists for 100% of failures Y but 1% of references, intuitively X should be a good predictor for Y; however confidence will be small (< 0.01) due to the large number of reference samples with feature X. For this reason we use lift in our work, which we define next. 	To deal with the problems in confidence, we use the lift metric (originally presented as interest) introduced by Brin et al. [10]. Lift is defined aslif t(X ⇒ Y) =conf (X ⇒ Y) supp(Y) 	=P(X ∩ Y) 	P(X)P(Y) 	(4)Lift measures how much more likely that X and Y would occur together relative to if they were independent. A lift value of 1 means independence between X and Y and a value greater than 1 signifies dependence. Lift allows us to address the rare item problem, whereas using confidence we may discard an important item-set due to its low frequency. A similar measure, called conviction, was also defined in [10] which compares the frequency of X appearing without Y, and in that sense it is similar to lift but conviction captures the risk of using the rule if X and Y are independent. We use lift instead of conviction primarily due to a simpler interpretation of the result for our customers.3.2 	Structured Data Logging
Structured logs are logs where the pieces of information in an event are dissected into a pre-defined structure. For example, in a unstructured log we may record human-readable messages about a server like the following:
0 : 0 0 0 : 0 0 0 : 0 0 experienced memory e r r o r experienced memory e r r o r experienced memory e r r o rProc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
| Fast Dimensional Analysis for Root Cause Investigation | Fast Dimensional Analysis for Root Cause Investigation | Fast Dimensional Analysis for Root Cause Investigation | Fast Dimensional Analysis for Root Cause Investigation | Fast Dimensional Analysis for Root Cause Investigation | Fast Dimensional Analysis for Root Cause Investigation | Fast Dimensional Analysis for Root Cause Investigation | Fast Dimensional Analysis for Root Cause Investigation | Fast Dimensional Analysis for Root Cause Investigation | 31:7 ||---|---|---|---|---|---|---|---|---|---|
| 0 : 1 5 |reboot |reboot |from |t o o l A |t o o l A |t o o l A |t o o l A |t o o l A |31:7 |
| 0 : 2 0 |experienced memory e r r o r |experienced memory e r r o r |experienced memory e r r o r |experienced memory e r r o r |experienced memory e r r o r |experienced memory e r r o r |experienced memory e r r o r |experienced memory e r r o r |31:7 || 0 : 2 1 |t o o l B AC Cycled |t o o l B AC Cycled |t o o l B AC Cycled |t o o l B AC Cycled |t o o l B AC Cycled |the |the |machine |31:7 |
| 0 : 2 5 |no |d i a g n o s i s |d i a g n o s i s |found |found |found |in |t o o l A |31:7 |
| 0 : 2 6 |t o o l C send |t o o l C send |t o o l C send |to |r e p a i r − undiagnosed |r e p a i r − undiagnosed |r e p a i r − undiagnosed |r e p a i r − undiagnosed |31:7 |Table 1. Server errors and reboots logged in a structured table
| timestamp | memory error | cpu error | ... | reboot | undiag. repair 
0 
0 
0 
0 
1 | diag.
repair 0 
0 
0 
0 
0 | tool |
|---|---|---|---|---|---|---|---|
| 0:00  0:15  0:20  0:21  0:26 |3  0  1  0  0 |0  0  0  0  0 |... |0  1  0  1  1 |undiag. repair  0  0  0  0  1 |diag. repair 0  0  0  0  0 |NULL A  NULL B  C |There are a few major drawbacks in this example log. First, the same message appears multiple times, which can be aggregated and described in a more succinct way to save space. Second, tool A and tool B both write to this log, but in very different formats. Tool A and B both restarted the server by turning the power off and on, but tool A logs it as “reboot”, while tool B, developed by another group of engineers from a different background, logs it as a verb “AC Cycle”. This could even happen to the same word, for example, “no diagnosis” and “undiagnosed” in the last two messages mean the same condition, but would impose huge difficulty when one tries to parse this log and count the events with regular expressions.With a pre-defined structure, i.e. a list of fields to put the information in, structured logging requires a canonical way to log events. For example, in a structured table, the messages above can be logged in the format shown in Table 1.In this conversion, engineers could decide not to log “no diagnosis found in tool A” in the structured table because it does not fit in the pre-defined structure. The structure of the table is flexible and can be tailored to the downstream application, for example, instead of having multiple columns for memory error, cpu error, etc., we can use one “error” column and choose a value from a pre-defined list such as memory, cpu, etc., to represent the same information.In addition to removing the ambiguity in the logs, enforcing structured logging through a single API also helps developers use and improve the existing architecture of the program, instead of adding ad-hoc functionalities for edge cases, which introduces unnecessary complexity that makes the code base much harder to maintain. In this example, if there is only one API for logging a reboot, developers from tool A and B would likely reuse or improve a common reboot service instead of rebooting the servers in their own code bases. A common reboot service would be much easier to maintain and likely have a better-designed flow to handle reboots in different scenarios.3.3 	Frequent Pattern Mining and Filtering