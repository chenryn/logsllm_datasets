for DMA, there were traditionally no enforcement mechanisms
to stop a peripheral from reading and writing arbitrary memory
locations. This method has been widely studied [19], [61], [72]
and exploited [8], [28], [29], [39], [46], [61], [67] over the
years. Subsequently, PCIe can achieve very rapid polling rates,
making it an ideal candidate for reliable memory acquisition.
2) Disk (Physical): We also employ the ML507 board for
our disk analysis by using its two onboard SATA connectors.
An Intelliprop SATA bridge core (Part number: IPP-SA110A-
BR) provides the ability to passively monitor, and potentially
manipulate, all of the trafﬁc over the SATA interface between
the host and device. To receive this data on our remote analysis
host, we implemented the logic to package the SATA frames
into UDP packets and send them over the gigabit Ethernet
connection. This proved a difﬁcult engineering feat, as the data
rates of SATA exceed the capacity of our gigabit Ethernet link,
and thus necessitated numerous data-ﬂow integrity guarantees.
This interface is completely passive and is essentially invisible
to the SUT, aside from the occasional throttling of frames.
3) Actuation (Physical): For many of our intended applica-
tions, it is convenient, and sometimes necessary, to actuate the
SUT from our analysis scripts. For this purpose, we employ
the Arduino Leonardo, which is driven by a ATmega32u4.
It has numerous general-purpose input/ouput (GPIO) pins,
as well as the ability easily emulate a keyboard and mouse
through the universal serial bus (USB) interface. We use an
3
external power source for the Arduino to permit functions
such as powering on the SUT through the GPIO pins attached
to the SUT’s motherboard. Integrating the Leonardo into
our software framework was relatively straightforward, given
the simplistic development environment provided for Arduino
platforms. That said, correctly emulating mouse movements
required some additional effort. However, once implemented,
these mouse movements provide LO-PHI with the capability
to move the mouse as a human user would (e.g., continuously
move the mouse) and also click buttons presented by the
software. While numerous commercial solutions already exist
for some of this instrumentation (e.g., Intelligent Platform
Management Interface, Active Management Technology, Dell
Remote Access Card), we wanted to ensure that LO-PHI would
be usable on the widest-range of systems possible, and thus
opted for the lower-level interfaces, speciﬁcally USB.
4) Infrastructure (Physical): While our sensors and actua-
tors achieve all of our low-level requirements for instrumen-
tation, numerous actuation functions (e.g., reverting the disk
or checking the OS’s boot status) required the development
of specialized infrastructure. While reverting the disk for a
virtual machine is as easy as overwriting a ﬁle, reverting a
physical machine quickly becomes much more involved. Our
requirement of unmodiﬁed hardware eliminates options such
as network booting or specialized drives, which would also
produce signiﬁcant detectable artifacts.
To achieve the desired outcome, we implemented our own
preboot execute environment (PXE) server as well as an
accompanying trivial ﬁle transfer protocol (TFTP) server, dy-
namic host conﬁguration protocol (DHCP) server and domain
name service (DNS) server. This enables us to temporarily
permit a given media access control (MAC) address to boot a
Clonezilla [16] instance, which restores the disk to a previously
saved state. By doing this, we make our system more ﬂexible
and scalable as the hardware is no longer tied to a particular
operating system or installation conﬁguration. Similarly, by
hosting our own DNS and DHCP server, we are able to
simplify our scripts and create a richer atmosphere for malware
analysis (e.g., we can trivially lookup the IP of a given machine
since our infrastructure assigns it). We also use gigabit Netgear
GS108T switches, one per physical machine, and utilize virtual
local-area networks (VLANs) to ensure that our control and
sensor trafﬁc do not interfere with each other.
B. Virtual Instrumentation
Since a great deal of work has already been done using
virtual-machine introspection (VMI), we choose to leverage
existing capabilities when possible by simply incorporating
them into our software framework with minimal amounts of
glue code. Because of our desire to use existing solutions,
and source code availability, we chose to use open-source
hypervisor implementations, namely QEMU/KVM [14], [37].
1) Memory (Virtual): For live memory acquisition, we use
techniques similar to those employed by LibVMI [55]. We
obtain access to the guest’s physical memory by means of a
UNIX socket. This socket then permits arbitrary memory read
and write commands, which perform the appropriate action on
the guest’s memory using cpu_physical_memory_map
and cpu_physical_memory_unmap.
2) Disk (Virtual): To incorporate disk introspection into our
virtual environment, we inserted hooks into the QEMU block
driver. These hooks intercept all disk operations and copy the
relevant data to a separate thread, which then exports them
over a UNIX socket to a subscription sever. By spawning a
new thread for every access, we should have negligible impact
on the system performance, especially if the host system has
underutilized resources. The server then allows our clients
to connect and subscribe to the disk activity of any guest.
We also ensured that our implementation works properly with
copy-on-write disks, greatly increasing performance when an
experiment requires frequently resetting the disk state.
3) Actuation (Virtual): For actuation, we leverage lib-
virt [2], an open-source tool for interacting with hypervisors.
Again, mouse movement proved to be less straightforward,
and necessitated the development of a custom virtual network
computing (VNC) client.
IV. ARTIFACTS
While hardware-level introspection provides numerous de-
sirable security guarantees that are not available for software-
based solutions (e.g., hardware segregation of analysis code),
it is still critically important to introduce minimal artifacts
and, in the ideal case, none at all. We emphasize that the
artifacts produced by LO-PHI are likely unusable by malware
for subversion, because the malware would lack a baseline for
comparison. Minimizing artifacts not only improves the ﬁdelity
of our data and the performance of our system, but also reduces
the number of “tells” on a SUT that can be used to evade or
hinder our analysis. Nevertheless, we enumerate the artifacts
introduced by our instrumentation and attempt to address any
shortcomings in both virtual and physical machines. All of our
experiments were run using our infrastructure to automate the
execution of our benchmarking applications on both physical
and virtual machines both with and without instrumentation.
In our performance experiments, the physical SUT was a Dell
T7500 equipped with an 6-core Xeon X5670 and 2GB of RAM
and a WD3200AAKX disk drive, and our virtual machine was
instantiated on a Dell T7600 equipped with dual 8-core Xeon
E5-2665 processors and 68GB of RAM, with the VM itself
allocated one core, 1GB of RAM and a 10GB hard disk.
A. Memory Artifacts
In both of our memory-introspection techniques, i.e., phys-
ical and virtual, there is likely to be a performance impact
on the SUT, as we are accessing a shared resource (main
memory). We attempted to quantify this performance impact
by utilizing RAMSpeed, a popular RAM benchmarking ap-
plication. We ran RAMSpeed on the same system with and
without our instrumentation. In each case, we conducted four
experiments designed to stress each of the INT, MMX, SSE,
and FL-POINT instruction sets. Each of these experiments
consists of 500 sub-experiments, which evaluate and average
the performance of copy, scale, sum, and triad operations.
To ensure that
the memory reads for our instrumentation
were not being cached, we had our sensors continuously read
the entire memory space, which should have also introduced
the largest performance impact on the system. The memory
polling rates were dictated by the hardware and our particular
implementation.
4
(a) Physical machine (Polling at 14MB/sec)
(b) Virtual machine (Polling at 160MB/sec)
Fig. 1: Average memory throughput comparison as reported by RAMSpeed, with and without instrumentation on both physical and virtual
machines. (500 samples for each box plot)
At ﬁrst glance, Figure 1 may seem to indicate that our in-
strumentation has a discernible effect on the system; however,
the deviation from the uninstrumented median is only 0.4%
in the worst case (SSE in Figure 1a). Despite our best efforts
to create a controlled experiment, i.e., running RAMSpeed on
a fresh install of Windows with no other processes, we were
unable to to deﬁnitively attribute any deviation to our polling
of memory. While our memory instrumentation certainly has
some impact on the system, the rates at which we are polling
memory do not appear to be frequent enough to predictably
degrade performance. This result appears to indicate that sys-
tems like ours could poll at signiﬁcantly higher rates while still
remaining undetectable. For example, PCIe implementations
can achieve DMA read speeds of 3 GB/sec [6], which could
permit a new class entirely of introspection capabilities. To this
end, we have successfully achieved rates as fast as 60 MB/sec
using SLOTSCREAMER [32]; however, the implementation
is not yet stable enough to incorporate into our framework.
Nevertheless, in order to detect any deviation in performance,
the software being analyzed would need to have the ability to
baseline our system, which is not feasible in our experiments.
While performance concerns are a universal problem with
instrumentation, adding hardware to a physical conﬁguration
has numerous additional artifacts that must also be addressed.
To utilize PCIe, we must enumerate our card on the bus,
which means that the BIOS and operating system are able to
see our hardware. This inevitably reveals our presence on the
machine; nevertheless, mitigations do exist (e.g., masquerading
as a different device). To avoid detection, our card could
trivially use a different hardware identiﬁer every time to
avoid signature-based detection. Even with a masked hardware
identiﬁer however, Stewin et al. [66] demonstrated that all
of these DMA-based approaches will reveal some artifacts
that are exposed in the CPU performance counters. Similar
techniques could be employed by malware authors in both
physical and virtual environments to detect the presence of
a polling-based memory acquisition system such as ours.
These anti-analysis techniques could necessitate the need for
more sophisticated acquisition techniques, some of which are
proposed in Section IX.
B. Disk Artifacts
To quantify the performance impact of our disk instru-
mentation, we similarly employed a popular disk benchmark-
ing utility, IOZone [1]. While IOZone’s primary purpose is
to benchmark the higher-level ﬁlesystem, any performance
impacts on disk throughput should nonetheless be visible to
the tool. We used the same setup as the previous memory
experiments and ran IOZone 50 times for each case, i.e., with
and without our instrumentation, monitoring the read and write
throughput with a record size of 16MB and ﬁle sizes ranging
from 16MB to 2GB (the total amount of RAM on SUT).
Our hardware should only be visible when we intentionally
delay SATA frames to meet the constraints of our gigabit
Ethernet
link. We designed the system with this delay to
minimize our packet
loss since UDP does not guarantee
delivery of packets. In practice, we rarely observed the system
cross this threshold; however, IOZone is explicitly made to
stress the limits of a ﬁle system. For smaller ﬁles, caching
masks most of our impact as these cache hits limit
the
accesses that actually reach the disk. The caching effect is
more prevalent when looking at the raw data rates (e.g., the
median uninstrumented read rate was 2.2GB/sec for the 16MB
experiment and 46.2MB/sec for 2GB case).
The discrepancies between the read and write distributions
are attributed to the underlying New Technology File Sys-
tem (NTFS) and optimizations in the hard drive ﬁrmware.
Figure 2b shows that our instrumentation is essentially in-
distinguishable from the base case when reading, the worst
case being a degradation of 3.7% for 2GB ﬁles. With writes
however, where caching offers no beneﬁt, the effects of our in-
strumentation are clearly visible, with a maximum performance
degradation of 14.5%. Under typical operating conditions,
throughputs that reveal our degradation are quite rare. In these
experiments, the UDP data rates observed from our sensor
averaged 2.4MB/sec with burst speeds reaching as high as
82.5MB/sec, which directly coincide with the rates observed in
Figure 2a, conﬁrming that we are only visible when throttling
SATA to meet the constraints of the Ethernet connection.
In the case of virtual machines, we would expect
to
have no detectable artifacts on a properly provisioned host
5
SSEMMXINTEGERFL-POINTMemory Operation Type490049505000505051005150Memory Throughput (MB/sec)UninstrumentedWith InstrumentationSSEMMXINTEGERFL-POINTMemory Operation Type500060007000800090001000011000Memory Throughput (MB/sec)UninstrumentedWith Instrumentation(a) File writes
(b) File reads
Fig. 2: File system throughput comparison as reported by IOZone on Windows XP, with and without instrumentation on a physical machine.
(50 samples for each box plot)
aside from the presence of a kernel virtual machine (KVM).
This is because our instrumentation adds very little code into
the execution path for disk accesses, and uses threading to
exploit the numerous cores on our system. More precisely, our
instrumentation only adds a memory copy operation of the
data buffer, which is then passed to a thread to be exported.
Our experimental results conﬁrmed this hypothesis as we were
unable to identify any consistent artifacts in our IOZone tests.
V. LIMITATIONS
All of our techniques have some inherent limitations. We
attempt to enumerate the most prominent of those below.
a) Input/Output Memory Management Unit (IOMMU):
Newer chipsets are equipped with IOMMUs, which, when
properly conﬁgured to disable DMA from peripherals, would
render our current memory acquisition approach ineffective.
While this limits us from instrumenting arbitrary systems, it
does not thwart our approach in the analysis case, i.e., where
we have complete control of the system that we are instru-
menting, because we could simply disable this functionality
or purchase chipsets that do not contain IOMMUs. In the long
term however, we will likely have to migrate our techniques
to employ a different memory acquisition method.
b) Asynchronous Memory Access: Wang et al. [71]
provide a good analysis of the inherent limitations of polling-
based systems for malware detection and potential evasion
techniques. However, memory polling may also create issues
with smearing and caching as well. Smearing occurs when
the state of memory of a SUT changes during acquisition,
resulting in an imperfect memory capture over a time window
rather than a speciﬁc instant in time. The memory contents
of live SUTs are very dynamic, so smearing is likely to
occur. Nevertheless, we have only rarely had this effect cause
problems in practice, and faster polling rates would help
minimize the smearing effect. Similarly, there may be rare
cases where data never leaves a cache; however, this can be
easily mitigated with cache coherency.
c) Filesystem Caching: For disk monitoring, OS-level
disk caching may cause our disk sensor to miss SATA frames
that hit the cache and are overwritten before being ﬂushed to
disk. While this effect is likely to be minor during continuous
disk monitoring, it is conceivable that malware could drop a
ﬁle, execute it, and delete it before the cache is ever ﬂushed
to disk, completely evading detection. However, we do not see
this as a major issue, as attempts of persistence will eventually
have to write to disk. Additionally, the effects of the malware
would also likely be detectable in memory.
d) No Internet Access: Our experiments also had a
few limitations that were beyond our control. For example,
the network policies within our organization currently forbid
us from running these malware samples on the live Internet.
Similar studies have concluded that most malware from
the wild will appear to do nothing, aside from network
activity, without
the presence of its command-and-control
infrastructure from which to retrieve a payload. Because
of this, we do not present our results as representative of
the presence of VM-aware malware in the wild, but instead
highlight our capabilities and the ability to detect particularly
sophisticated payloads once they are executed.
VI. EXPERIMENTAL FRAMEWORK
To facilitate experimentation, we built a scalable infrastruc-
ture capable of running arbitrary binaries on either a physical
or virtual machine with a speciﬁed operating system. Our
software infrastructure consists of a master which accepts job
submissions and delegates them to an appropriate controller.
A given controller is initialized with a set of machines,
both physical and virtual SUTs, that serve as its worker pool.
Upon a job submission, the controller ﬁrst downloads the
script, which describes the actions to perform on the SUT,
and submits the job to a scheduler. This scheduler then waits
for a machine of the appropriate type,
i.e., physical or
virtual, to become available in the pool, allocates it to the
analysis, and runs the requested routines. All of our malware
samples, analyses, and results were stored in a MongoDB
database. Samples were submitted using a custom FTP server
and a command line tool that interfaced with the master
to instantiate a given analysis script, which are stored on the
master and dynamically sent to the controller.
6
16326412825651210242048File Size (MB)65707580859095Disk Throughput (MB/sec)UninstrumentedWith Instrumentation16326412825651210242048File Size (MB)05001000150020002500Disk Throughput (MB/sec)UninstrumentedWith Instrumentations t a t u s ( ) :