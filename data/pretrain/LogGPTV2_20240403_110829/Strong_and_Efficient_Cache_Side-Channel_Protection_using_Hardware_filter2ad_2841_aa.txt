title:Strong and Efficient Cache Side-Channel Protection using Hardware
Transactional Memory
author:Daniel Gruss and
Julian Lettner and
Felix Schuster and
Olga Ohrimenko and
Istv&apos;an Haller and
Manuel Costa
Strong and Efficient Cache Side-Channel 
Protection using Hardware Transactional Memory
Daniel Gruss, Graz University of Technology, Graz, Austria; Julian Lettner, University of 
California, Irvine, USA; Felix Schuster, Olya Ohrimenko, Istvan Haller, and Manuel Costa, 
Microsoft Research, Cambridge, UK
https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/gruss
This paper is included in the Proceedings of the 26th USENIX Security SymposiumAugust 16–18, 2017 • Vancouver, BC, CanadaISBN 978-1-931971-40-9Open access to the Proceedings of the 26th USENIX Security Symposium is sponsored by USENIXStrong and Efﬁcient Cache Side-Channel Protection using Hardware
Transactional Memory
Daniel Gruss∗, Julian Lettner†, Felix Schuster, Olga Ohrimenko, Istvan Haller, Manuel Costa
Microsoft Research
Abstract
Cache-based side-channel attacks are a serious problem
in multi-tenant environments, for example, modern cloud
data centers. We address this problem with Cloak, a
new technique that uses hardware transactional mem-
ory to prevent adversarial observation of cache misses
on sensitive code and data. We show that Cloak pro-
vides strong protection against all known cache-based
side-channel attacks with low performance overhead. We
demonstrate the efﬁcacy of our approach by retroﬁtting
vulnerable code with Cloak and experimentally conﬁrm-
ing immunity against state-of-the-art attacks. We also
show that by applying Cloak to code running inside In-
tel SGX enclaves we can effectively block information
leakage through cache side channels from enclaves, thus
addressing one of the main weaknesses of SGX.
1
Introduction
Hardware-enforced isolation of virtual machines and
containers is a pillar of modern cloud computing. While
the hardware provides isolation at a logical level, physi-
cal resources such as caches are still shared amongst iso-
lated domains, to support efﬁcient multiplexing of work-
loads. This enables different forms of side-channel at-
tacks across isolation boundaries. Particularly worri-
some are cache-based attacks, which have been shown
to be potent enough to allow for the extraction of sensi-
tive information in realistic scenarios, e.g., between co-
located cloud tenants [56].
In the past 20 years cache attacks have evolved from
theoretical attacks [38] on implementations of crypto-
graphic algorithms [4] to highly practical generic attack
primitives [43,62]. Today, attacks can be performed in an
automated fashion on a wide range of algorithms [24].
Many countermeasures have been proposed to miti-
gate cache side-channel attacks. Most of these coun-
termeasures either try to eliminate resource sharing [12,
18, 42, 52, 58, 68, 69], or they try to mitigate attacks
after detecting them [9, 53, 65]. However, it is difﬁ-
cult to identify all possible leakage through shared re-
∗Work done during internship at Microsoft Research; afﬁliated with
Graz University of Technology.
University of California, Irvine.
†Work done during internship at Microsoft Research; afﬁliated with
sources [34,55] and eliminating sharing always comes at
the cost of efﬁciency. Similarly, the detection of cache
side-channel attacks is not always sufﬁcient, as recently
demonstrated attacks may, for example, recover the en-
tire secret after a single run of a vulnerable cryptographic
algorithm [17, 43, 62]. Furthermore, attacks on singular
sensitive events are in general difﬁcult to detect, as these
can operate at low attack frequencies [23].
In this paper, we present Cloak, a new efﬁcient defen-
sive approach against cache side-channel attacks that al-
lows resource sharing. At its core, our approach prevents
cache misses on sensitive code and data. This effectively
conceals cache access-patterns from attackers and keeps
the performance impact low. We ensure permanent cache
residency of sensitive code and data using widely avail-
able hardware transactional memory (HTM), which was
originally designed for high-performance concurrency.
HTM allows potentially conﬂicting threads to execute
transactions optimistically in parallel: for the duration
of a transaction, a thread works on a private memory
snapshot.
In the event of conﬂicting concurrent mem-
ory accesses, the transaction aborts and all correspond-
ing changes are rolled back. Otherwise, changes become
visible atomically when the transaction completes. Typi-
cally, HTM implementations use the CPU caches to keep
track of transactional changes. Thus, current implemen-
tations like Intel TSX require that all accessed memory
remains in the CPU caches for the duration of a transac-
tion. Hence, transactions abort not only on real conﬂicts
but also whenever transactional memory is evicted pre-
maturely to DRAM. This behavior makes HTM a pow-
erful tool to mitigate cache-based side channels.
The core idea of Cloak is to execute leaky algorithms
in HTM-backed transactions while ensuring that all sen-
sitive data and code reside in transactional memory for
the duration of the execution.
If a transaction suc-
ceeds, secret-dependent control ﬂows and data accesses
are guaranteed to stay within the CPU caches. Other-
wise, the corresponding transaction would abort. As we
show and discuss, this simple property can greatly raise
the bar for contemporary cache side-channel attacks or
even prevent them completely. The Cloak approach can
be implemented on top of any HTM that provides the
aforementioned basic properties. Hence, compared to
other approaches [11, 42, 69] that aim to provide isola-
USENIX Association
26th USENIX Security Symposium    217
tion, Cloak does not require any changes to the operating
system (OS) or kernel. In this paper, we focus on Intel
TSX as HTM implementation for Cloak. This choice is
natural, as TSX is available in many recent professional
and consumer Intel CPUs. Moreover, we show that we
can design a highly secure execution environment by us-
ing Cloak inside Intel SGX enclaves. SGX enclaves pro-
vide a secure execution environment that aims to protect
against hardware attackers and attacks from malicious
OSs. However, code inside SGX enclaves is as much vul-
nerable to cache attacks as normal code [7,20,46,57] and,
when running in a malicious OS, is prone to other mem-
ory access-based leakage including page faults [10, 61].
We demonstrate and discuss how Cloak can reliably de-
fend against such side-channel attacks on enclave code.
We provide a detailed evaluation of Intel TSX as avail-
able in recent CPUs and investigate how different im-
plementation speciﬁcs in TSX lead to practical chal-
lenges which we then overcome. For a range of proof-
of-concept applications, we show that Cloak’s runtime
overhead is small—between −0.8% and +1.2% for low-
memory tasks and up to +248% for memory-intense
tasks in SGX—while state-of-the-art cache attacks are
effectively mitigated. Finally, we also discuss limitations
of Intel TSX, speciﬁcally negative side effects of the ag-
gressive and sparsely documented hardware prefetcher.
The key contributions of this work are:
• We describe Cloak, a universal HTM-based ap-
proach for the effective mitigation of cache attacks.
• We investigate the peculiarities of Intel TSX and
show how Cloak can be implemented securely and
efﬁciently on top of it.
• We propose variants of Cloak as a countermeasure
against cache attacks in realistic environments.
• We discuss how SGX and TSX in concert can pro-
vide very high security in hostile environments.
Outline. The remainder of this paper is organized
as follows.
In Section 2, we provide background on
software-based side-channel attacks and hardware trans-
actional memory.
In Section 3, we deﬁne the attacker
model. In Section 4, we describe the fundamental idea of
Cloak. In Section 5, we show how Cloak can be instan-
tiated with Intel TSX. In Section 6, we provide an eval-
uation of Cloak on state-of-the-art attacks in local and
cloud environments. In Section 7, we show how Cloak
makes SGX a highly secure execution environment. In
Section 8, we discuss limitations of Intel TSX with re-
spect to Cloak. In Section 9, we discuss related work.
Finally, we provide conclusions in Section 10.
2 Background
We now provide background on cache side-channel at-
tacks and hardware transactional memory.
2.1 Caches
Modern CPUs have a hierarchy of caches that store and
efﬁciently retrieve frequently used instructions and data,
thereby, often avoiding the latency of main memory ac-
cesses. The ﬁrst-level cache is the usually the small-
est and fastest cache, limited to several KB. It is typi-
cally a private cache which cannot be accessed by other
cores. The last-level cache (LLC), is typically uniﬁed
and shared among all cores. Its size is usually limited
to several MBs. On modern architectures, the LLC is
typically inclusive to the lower-level caches like the L1
caches. That is, a cache line can only be in an L1 cache
if it is in the LLC as well. Each cache is organized in
cache sets and each cache set consists of multiple cache
lines or cache ways. Since more addresses map to the
same cache set than there are ways, the CPU employs
a cache replacement policy to decide which way to re-
place. Whether data is cached or not is visible through
the memory access latency. This is a root cause of the
side channel introduced by caches.
2.2 Cache Side-Channel Attacks
Cache attacks have been studied for two decades with
an initial focus on cryptographic algorithms [4, 38, 51].
More recently, cache attacks have been demonstrated in
realistic cross-core scenarios that can deduce informa-
tion about single memory accesses performed in other
programs (i.e., access-driven attacks). We distinguish be-
tween the following access-driven cache attacks: Evict+
Time, Prime+Probe, Flush+Reload. While most attacks
directly apply one of these techniques, there are many
variations to match speciﬁc capabilities of the hardware
and software environment.
In Evict+Time, the victim computation is invoked re-
peatedly by the attacker.
In each run, the attacker se-
lectively evicts a cache set and measures the victim’s
execution time. If the eviction of a cache set results in
longer execution time, the attacker learns that the victim
likely accessed it. Evict+Time attacks have been exten-
sively studied on different cache levels and exploited in
various scenarios [51, 60]. Similarly, in Prime+Probe,
the attacker ﬁlls a cache set with their own lines. After
waiting for a certain period, the attacker measures if all
their lines are still cached. The attacker learns whether
another process—possibly the victim—accessed the se-
lected cache set in the meantime. While the ﬁrst Prime+
Probe attacks targeted the L1 cache [51,54], more recent
218    26th USENIX Security Symposium
USENIX Association
attacks have also been demonstrated on the LLC [43,
50, 56]. Flush+Reload [62] is a powerful but also con-
strained technique;
it requires attacker and victim to
share memory pages. The attacker selectively ﬂushes
a shared line from the cache and, after some waiting,
checks if it was brought back through the victim’s ex-
ecution. Flush+Reload attacks have been studied exten-
sively in different variations [2, 41, 66]. Apart from the
CPU caches, the shared nature of other system resources
has also been exploited in side-channel attacks. This
includes different parts of the CPU’s branch-prediction
facility [1, 15, 40], the DRAM row buffer [5, 55], the
page-translation caches [21, 28, 36] and other micro-
architectural elements [14].
This paper focuses on mitigating Prime+Probe and
Flush+Reload.
However, Cloak conceptually also
thwarts other memory-based side-channel attacks such
as those that exploit the shared nature of the DRAM.
2.3 Hardware Transactional Memory
HTM allows for the efﬁcient implementation of paral-
lel algorithms [27].
It is commonly used to elide ex-
pensive software synchronization mechanisms [16, 63].
Informally, for a CPU thread executing a hardware trans-
action, all other threads appear to be halted; whereas,
from the outside, a transaction appears as an atomic oper-
ation. A transaction fails if the CPU cannot provide this
atomicity due to resource limitations or conﬂicting con-
current memory accesses. In this case, all transactional
changes need to be rolled back. To be able to detect con-
ﬂicts and revert transactions, the CPU needs to keep track
of transactional memory accesses. Therefore, transac-
tional memory is typically divided into a read set and a
write set. A transaction’s read set contains all read mem-
ory locations. Concurrent read accesses by other threads
to the read set are generally allowed; however, concur-
rent writes are problematic and—depending on the actual
HTM implementation and circumstances—likely lead to
transactional aborts. Further, any concurrent accesses
to the write set necessarily lead to a transactional abort.
Figure 1 visualizes this exemplarily for a simple transac-
tion with one conﬂicting concurrent thread.
Commercial Implementations.
Implementations of
HTM can be found in different commercial CPUs,
among others, in many recent professional and consumer
Intel CPUs. Nakaike et al. [48] investigated four com-
mercial HTM implementations from Intel and other ven-
dors. They found that all processors provide comparable
functionality to begin, end, and abort transactions and
that all implement HTM within the existing CPU cache
hierarchy. The reason for this is that only caches can be
held in a consistent state by the CPU itself.
If data is
Thread 1
Thread 2
Begin transaction
u
n
d
o
Read 0x20
Write 0x40
t e c o n ﬂ i c t
w r i
End transaction
Read 0x20
Write 0x40
Figure 1: HTM ensures that no concurrent modiﬁcations
inﬂuence the transaction, either by preserving the old
value or by aborting and reverting the transaction.
evicted to DRAM, transactions necessarily abort in these
implementations. Nakaike et al. [48] found that all four
implementations detected access conﬂicts at cache-line
granularity and that failed transactions were reverted by
invalidating the cache lines of the corresponding write
sets. Depending on the implementation, read and write
set can have different sizes, and set sizes range from mul-
tiple KB to multiple MB of HTM space.
Due to HTM usually being implemented within
the CPU cache hierarchy, HTM has been proposed
as a means for optimizing cache maintenance and
for performing security-critical on-chip computations:
Zacharopoulos [64] uses HTM combined with prefetch-
ing to reduce the system energy consumption. Guan et al.
[25] designed a system that uses HTM to keep RSA pri-
vate keys encrypted in memory and only decrypt them
temporarily inside transactions.
Jang et al. [36] used
hardware transaction aborts upon page faults to defeat
kernel address-space layout randomization.
3 Attacker Model
We consider multi-tenant environments where tenants do
not trust each other, including local and cloud environ-
ments, where malicious tenants can use shared resources
to extract information about other tenants. For example,
they can inﬂuence and measure the state of caches via
the attacks described in Section 2.2. In particular, an at-
tacker can obtain a high-resolution trace of its own mem-
ory access timings, which are inﬂuenced by operations of
the victim process. More abstractly, the attacker can ob-
tain a trace where at each time frame the attacker learns
whether the victim has accessed a particular memory lo-
cation. We consider the above attacker in three realistic
environments which give her different capabilities:
Cloud We assume that the processor, the OS and the
hypervisor are trusted in this scenario while other
cloud tenants are not. This enables the attacker to
launch cross-VM Prime+Probe attacks.
USENIX Association
26th USENIX Security Symposium    219
Local This scenario is similar to the Cloud scenario, but
we assume the machine is not hosted in a cloud en-
vironment. Therefore, the tenants share the machine
in a traditional time-sharing fashion and the OS is
trusted to provide isolation between tenants. Fur-
thermore, we assume that there are shared libraries
between the victim and the attacker, since this is a
common optimization performed by OSs. This en-
ables the attacker to launch Flush+Reload attacks,
in addition to Prime+Probe attacks.