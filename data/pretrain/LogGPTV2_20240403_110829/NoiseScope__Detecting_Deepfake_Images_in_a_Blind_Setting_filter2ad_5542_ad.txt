BigGAN-DogHV
BigGAN-BurgLV
BigGAN-BurgHV
PGGAN-Face
PGGAN-Tower
CycleGAN-Winter
CycleGAN-Zebra
99.56
90.14
99.63
99.38
92.6
99.68
98.64
99.09
95.93
92.40
92.84
F1 Score (%)
NoiseScope CSD-SVM
92.93
67.53
94.82
86.94
70.10
94.82
83.67
64.07
91.61
87.14
84.95
Table 2: Performance of NoiseScope and CSD-SVM (ν = 0.1).
Datasets
StyleGAN-Face1
StyleGAN-Face2
StyleGAN-Bed
BigGAN-DogLV
BigGAN-DogHV
BigGAN-BurgLV
BigGAN-BurgHV
PGGAN-Face
PGGAN-Tower
CycleGAN-Winter
CycleGAN-Zebra
200:1600
F1 Score (%) w/ different fake:real ratio
200:400
200:2000
97.9
81.0
99.5
98.9
89.3
98.6
97.9
97.1
94.5
88.2
89.9
200:800
97.1
74.3
98.8
97.7
85.7
97.2
96.0
97.4
94.3
86.8
86.3
94.0
58.4
97.2
95.2
82.3
91.9
93.2
94.4
92.3
69.4
76.1
96.4
62.8
97.8
95.8
84.9
96.2
94.8
93.7
92.2
72.2
78.2
Table 3: Detection performance (F1) on imbalanced test sets
with different ratio of fake to real images.
Performance on imbalanced test sets. We apply NoiseScope on
test sets with an imbalanced ratio of real vs fake images. For each
dataset, we evaluate on 4 imbalanced test sets comprising different
ratios of real and fake images. In each test set, the number of fake
images is set to 200, and we increase the number of real images
according to the desired ratio. We experiment with ratios of fake
to real as 1:2, 1:4, 1:8, and 1:10.9 The inherent difficulty of using
NoiseScope in an imbalanced setting is the presence of noisy samples
among fake and real images. These are samples where content
tends to leak into residuals. Therefore, such noisy fake and real
images can show unexpectedly high correlation. Consequently, as
the number of real images increases, the probability of a fake image
cluster merging with noisy real samples increases.
Detection performance is presented in Table 3. Out of the 11
datasets, 7 datasets exhibit high performance of over 91.9% F1 score
for all ratios (numbers shown in bold). As expected, there is a drop
in performance as datasets become more imbalanced, but even at
1:10, we observe high detection performance for these 7 datasets.
Among the remaining 4 datasets, StyleGAN-Face2, CycleGAN-
Winter, CycleGAN-Zebra shows the biggest drop in performance
as test set becomes more imbalanced. To further understand the re-
duced performance, we analyze the purity of the model fingerprints
obtained as output of the fingerprint classification component. Pu-
rity of a model fingerprint is the fraction of images in the cluster
(used to estimate the fingerprint) that are fake. If purity is less, then
the performance of the fake image detection module will decrease
9For 1:8, and 1:10 we do 3 trials. Rest of them are averaged over 5 trials.
(a)
(b)
Figure 6: (a) GAN fingerprint purity distributions (b) PCE
Merging Threshold Tmerдe vs. Detection F1 Score.
Datasets
StyleGAN-Face2
BigGAN-DogHV
CycleGAN-Winter
CycleGAN-Zebra
200:1600
F1 Score(%) w/ different fake:real ratio
200:400
200:2000
88.47
93.80
89.65
92.17
200:800
86.45
89.39
90.17
91.98
76.71
86.76
81.00
83.13
80.23
89.17
82.08
86.26
Table 4: Improved detection performance by increasing
Tmerдe in non-performant imbalanced configurations.
(as the fingerprint is inaccurate). In general, for the three datasets
(StyleGAN-Face2, CycleGAN-Winter, and CycleGAN-Zebra), we
observe that purity of the fingerprints is lower compared to the
other datasets. Figure 6a shows the distribution (CDF) of purity of
fingerprints found across test sets (aggregated over all ratios) for
two datasets—one for which NoiseScope is performant (PGGAN-
Tower), and one for which NoiseScope suffers from relatively lower
performance (CycleGAN-Winter). CycleGAN-Winter suffers from
lower fingerprint purity that range between 60% to 80%, whereas
the fingerprint purity for PGGAN-Tower is high, i.e., over 95%.
Therefore, high detection performance correlates well with the
ability to reliably extract pure fingerprints.
One approach to improve fingerprint purity is to raise the PCE
merging threshold Tmerдe. A higher value of Tmerдe would prevent
noisy samples from merging with fake images. StyleGAN-Face2,
CycleGAN-Winter, and CycleGAN-Zebra results in Table 3 has
Tmerдe values in the range from 8.45 to 11.68. We raise the threshold
to 15 and recompute the results for these datasets. In addition, we
also recompute results at the raised threshold for BigGAN-DogHV
(which has F1 score below 90% in Table 3). Results with the increased
threshold are presented in Table 4 for all 4 datasets. We observe a
marked increase in detection performance, e.g., on average 10.35%
increase in F1 for 1:10 ratio across all datasets. We also observe an
increase in purity of fingerprints (not shown).
Above analysis raises the question of whether defender can
estimate a better value of Tmerдe, starting from the initial estimate?
We note that this is possible by analyzing the variation in cluster
sizes as one increases Tmerдe starting from the initial value. In
general, detection performance correlates well with cluster sizes. If
the largest cluster size is small (say less than 50), then the value of
Tmerдe is too high, and detection performance is likely to be lower.
To study this, we conduct experiments on CycleGAN-Zebra with an
imbalanced ratio of 1:2. Figure 6b studies the variation of detection
performance and largest cluster size, as we incrementally increase
Tmerдe starting from the initial estimate. Detection performance
 0 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1CDF of Model FingerprintsPurity of Model FingerprintsCycleGAN WinterPGGAN Tower 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50 0 50 100 150 200 250 Performance (F1)   Flagged Cluster SizePCE Merging Threshold (Tmerge)F1Size921ACSAC 2020, December 7–11, 2020, Austin, USA
Jiameng Pu, Neal Mangaokar, Bolun Wang, Chandan K. Reddy, and Bimal Viswanath
remains mostly high and stable, for cluster size roughly above
100. Towards the end, the performance drops as cluster size goes
below 67, achieving the lowest performance when cluster size is less
than 50. The defender can thus calibrate Tmerдe by incrementally
increasing the originally estimated value, using cluster size as a
stopping condition. If no clusters are found, or clusters are too
small, then the defender has exceeded the optimal Tmerдe.
Performance when test set contains fake images from mul-
tiple GAN models. So far, we considered test datasets with fake
images from a single model. What if attackers use multiple GAN
models? Can NoiseScope still detect fake images? In theory, Nois-
eScope should adapt to such settings, because clustering should
ideally extract multiple model fingerprints corresponding to each
model. To evaluate this, we restrict ourselves to datasets capturing
faces, as it is the only content category for which we have fakes
images from multiple models. In each trial, we populate the test set
with 150 images each from the StyleGAN-Face1, StyleGAN-Face2
and PGGAN-Face datasets, and use 450 real images from the FFHQ
dataset. Results in Table 5 indicate an overall high F1 score of 91.5%,
and also shows per-dataset performance.
So how did NoiseScope achieve high detection performance when
test set includes fake images from three different models?10 Interest-
ingly, NoiseScope discovered three clusters (model fingerprints). The
first cluster mostly included images from StyleGAN-Face1 (over
95%), the second cluster mostly from PGGAN-Face (again over
95%), and in the third cluster, a majority of images are from the
StyleGAN-Face2 dataset. Therefore, NoiseScope was able to extract
model fingerprints corresponding to the three models. These results
match our intuition that GANs trained on different datasets would
generate distinct fingerprints. Our results indicate that NoiseScope
is effective on test sets with fake images from different GANs. An
attacker can take this setting to the extreme by creating a differ-
ent GAN for every single fake image to disrupt the fingerprint
extraction process. However, this significantly raises the cost for
the attacker, and reduces the utility of using generative schemes.
Performance on test sets with images from multiple categor-
ical domains. Our current configuration uses a single categorical
domain for each test-set, but still has high variations among images
(see Figures 8-18). This was done for the sake of simplicity, and
because many GAN datasets are organized into few specific cate-
gories. Here we evaluate effectiveness on test sets with multiple
content categories. We test against BigGAN as it is the only GAN
model with images from several categories. For a test set of 500
real, and 500 fake images, images are evenly and randomly sampled
from 10 categories: Ambulance, Race car, Burrito, Tiger, Cup, Hen,
Pretzel, Pirate, French bulldog, and Cheeseburger. The average detec-
tion performance (F1) is high at 99.1%. Thus, NoiseScope works for a
mix of high-level image content, i.e., NoiseScope is content-agnostic.
What if there are too few fake images in the test set? We
present NoiseScope detection performance when evaluated on test
sets with an increasingly small number of fake images. Using rep-
resentative datasets for each GAN, we evaluate on 4 test sets with
10Models trained on three different datasets.
Figure 7: Detection performance (F1) with limited number
of fake images in test sets.
Datasets
Combined
StyleGAN-Face1
StyleGAN-Face2
PGGAN-Face
F1 Score (%)
91.5
91.2
81.9
100.0
Precision (%) Recall (%)
93.3
83.8
100.0
100.0
89.8
100.0
69.3
100.0
Table 5: Detection performance on test set with fake images
from multiple (GAN) sources.
Datasets
StyleGAN-Bed
BigGAN-DogLV
BigGAN-DogHV
BigGAN-BurgLV
BigGAN-BurgHV
PGGAN-Tower
CycleGAN-Winter
CycleGAN-Zebra
F1 Score (%)
Wavelet Blur NLM BM3D
99.3
99.7
79.0
99.8
96.0
62.5
84.5
96.9
99.7
99.7
94.8
99.8
99.4
95.8
91.1
93.6
97.7
99.5
76.3
99.5
97.5
17.6
71.4