specializes in threats intelligence. This list is used by the company
as the core of their reputation-based detection engine. Hence, the
aim is to be both complete with regard to current threats and
keep FPs down. The reputation-based engine is deployed on a
few hundred sites, and operational experience shows that FPs
are minimal and coverage is at least comparable with deployed
anti-virus tools (i.e., the engine captures threats that AV software
installed on these sites misses).
We constructed our benign server training set from ranking
information provided by Alexa [1]. In this case, we assume that
the top 1,000 popular web sites reported by Alexa are not involved
in malicious activities and, in particular, are not responsible for
hosting botnet C&C servers. Alexa reports the top popular web
sites grouped by geographical regions as well. In order to obtain a
comprehensive list of benign servers, we combined the “Alexa Top
1000 Global Sites” with the most visited websites in the regions
where N1 and N2 are located.
Once the benign domains lists were compiled, we resolved each
DNS name on both lists to obtain the corresponding list of IP ad-
dresses. Note that we executed the DNS queries for each list from
the same network geographical locations of the corresponding net-
work (Europe for N1, and US for N2). Hence, the number of IP ad-
dresses collected for each network is different. This process resulted
in 2,958 unique IP addresses for N1, and 3,047 IP addresses for N2.
Table 2 shows the number of benign and malicious servers in
Network
Inter-University Network (N1)
Tier 1 ISP (N2)
Sampling Flows per day Unique IP Addresses
28 million
50 million
1.2 billion
400 million
1:10.000
1:1
Table 1: Summary statistics for each of the two NetFlow data sets for N1 and N2.
Point on
the ROC curve
1.0% FP
0.5% FP
0.3% FP
0.0% FP
Servers Flagged
as C&C (N1)
12,383
7,856
6,295
132
Servers Flagged
as C&C (N2)
4,937
3,166
1,958
960
Table 3: Servers flagged as malicious by Disclosure for
each of the networks N1 and N2 (without incorporating
reputation scores).
Despite the differences between the two data sets, the results
are similar. For instance, with MinFlows set to 20 flows and
ClassThresh tuned to produce a 1% false positive rate, the system
detects 64.3% of the C&C servers in the university network and
66.9% in the ISP network. This similarity emerges from the com-
position of all features, where the individual contribution of each
feature is quite different in the two environments. For instance,
most of the features are better suited to the unsampled data set,
where traffic patterns are clearly preserved. However, some of the
features—for instance, the unmatched flow density—provide the
best results when applied to large networks, even in presence of a
high sampling rate. The mixture of these two classes of features
makes Disclosure less sensitive to variability in the NetFlow
collection environment and, therefore, more robust.
Another important difference between the two experiments is
the fact that in the small network (N1), Disclosure provides
better results for a higher value of the minimum flow threshold,
while in the large network (N2) it performs better with a lower
threshold. This phenomenon is due to the fact that in the second
case, the sensors are only collecting 1 flow out of every 10,000.
Therefore, a high value for MinFlows would filter out all small-to-
medium size botnets, leaving only a few large ones for the analysis.
As a result, the features are now trained on a very few C&C
samples and, therefore, tend to produce inaccurate models. This
is an important issue to keep in mind when configuring the system.
In general, if MinFlows is set too low, the features are exposed
to samples that do not show sufficient regularity because an
insufficient number of flows are observed in the traffic. If, on the
other hand, MinFlows is set too high, the majority of the botnets
are discarded, and the features are trained on too few samples.
In both extremes, the result is a set of poorly trained models.
Finally, we manually verified the features of the benign servers
that Disclosure wrongly classified as being botnet C&C servers.
In several cases, the network or the server were probably mal-
functioning, and the clients (in most of the cases less than 10)
were repeatedly trying to send the same data over and over again
at regular intervals, and receiving no answer back from the server.
This behavior, even though not malicious per se, is indeed quite
similar to that exhibited by bot-infected machines.
5.4 Real-Time Detection
In the previous section, we presented the results obtained
with labeled data sets containing known benign and botnet C&C
servers. In order to apply Disclosure to the remaining unlabeled
data, we needed to perform three separate operations.
First, since Disclosure is meant to discover C&C servers and
not infected machines, we need to restrict the analysis to the
servers only. In order to separate them from the clients, we apply
the following heuristic: an IP address belongs to a server if the num-
ber of flows directed towards its top two ports (i.e., the two that
receives the most connections) account for at least 90% of the flows
Figure 3: Area under ROC curves with different
training set lengths for N1 and N2.
our labeled data set that were observed in the traffic of N1 and
N2 respectively.
5.3 Labeled Data Set Detection and False Pos-
itive Rates
In the initial experiment, we evaluated Disclosure’s ability to
recognize known botnet C&C servers from the ground truth con-
structed in the previous section. Disclosure’s detection rate (DR)
and FP rates were measured by generating ROC curves for each
data set under two configurations each that controlled the level
of input data filtering performed prior to detection.
Disclosure requires a minimum number of observed flows
to a particular server in order to provide accurate results. This
minimum is a threshold we denote by MinFlows, and can be set
by a security administrator according to the volume of traffic
at a particular site and any sampling that may be applied. We
evaluated two values for MinFlows for each data set: 20 and 50.
For each experiment, we excluded any servers that did not have
at least one port that received more than MinFlows flows. We
then evaluated the accuracy of Disclosure’s detection models
by performing a 10-fold cross-validation.
We also considered varying the size of the training set as an
additional tunable parameter. Figure 3 shows a summary of
Disclosure’s accuracy, measured by computing the area under
the ROC curve for different training windows. The curve for N2 is
almost constant. In comparison, the curve for N1 steadily increases
over the first 15 days before plateauing. This is due to the fact
that the number of known C&C servers observed in the university
network is low (see Table 2). Therefore, more time is required to
collect enough data to properly train the models. For this reason,
we decided to train Disclosure with all the available data.
Figure 4 shows the individual ROC curves obtained by vary-
ing the classification threshold ClassThresh, i.e., the boundary
separating benign scores from malicious scores, of Disclosure’s
detection module. Consequently, each point in the ROC curves
represents a possible setup configuration of the system. Security
administrators can thus precisely tune Disclosure to achieve
a reasonable trade-off between FPs and false negatives based
on the traffic characteristics of the network. Each graph also
contains a short synopsis of possible working points. For example,
configuring the system for a very high DR is usually too costly in
terms of FPs. On the other end of the scale, it is often possible
to achieve a 0% FP rate if we accept the fact that only one out
of three C&C servers will be detected.
134
(a) N1 with MinFlows = 20.
(b) N2 with MinFlows = 20.
(c) N1 with MinFlows = 50.
(d) N2 with MinFlows = 50.
Figure 4: Classification accuracy for each data set (N1 and N2) with MinFlows ∈ {20, 50}.
Point on
the ROC curve
1.0% FP
0.5% FP
0.3% FP
0.0% FP
C&C Servers after C&C Servers after
the RF (N2)
1516
688
271
91
the RF (N1)
1779
1448
1236
20
Table 4: Servers flagged as malicious by Disclosure for
each of the networks N1 and N2 (incorporating reputa-
tion scores). Here, RF refers to “reputation filter.”
towards that address. From the count, we removed the ports used
less than 3 times to filter out the noise generated by the fact that
servers may also have outgoing connections. By adopting this tech-
nique, we identified 82,580 servers in N1 and 530,011 servers in N2.
The second step consisted of setting the value of the MinFlows
threshold. According to the results obtained in the labeled data set,
we decided to perform the rest of the experiments with the thresh-
old set to 50 flows for N1 and to 20 for N2. After applying the
threshold, we were left with 53,426 servers in N1 and 48,713 in N2.
Finally, we needed to select the operational point on the ROC
curve ClassThresh (i.e., the trade-off between DR and FP rates).
Table 3 shows the number of servers detected in the two networks
obtained with four different configurations of the system.
Despite the fact that the various configurations were chosen to
minimize the number of FPs generated by the system, the number
IP addresses suspected of being C&C servers is still relatively high.
Therefore, to further reduce the probability of misclassification, we
combined the results of Disclosure with a reputation score based
on the information provided by EXPOSURE [2, 5], FIRE [3, 31],
and Google Safe Browsing [4]. As explained in Section 4, this
approach has the effect of narrowing down the results to the
servers that have a higher probability of being malicious.
The way in which the reputation score is computed can be
tuned according to the desired results and the number of daily
alerts that the security administrator can tolerate. The more
aggressive the filtering, the smaller the set of IP addresses flagged
as C&C servers. In our experiments, we increased the strength of
the FP reduction until we reduced the amount of alerts to a level
that can be manually verified. The results are reported in Table 4.
Figure 5 shows the ports distributions of the C&C servers
detected by Disclosure in the 0.5% false positive setting for N1
and N2. The graphs report the two most frequently used proto-
cols: HTTP-related (ports 80, 443, 8080, 8000) and SMTP/IMAP
(ports 25, 143, and 993). The remaining ports are grouped in two
categories: the reserved ports (0-1023), and the registered and
ephemeral ports (1024-65535). This classification is based only
on the port number and not on identification of the true protocol.
For instance, a botmaster can run a C&C server on port 25 to
avoid firewalls, but that does not mean that he will adopt the
SMTP protocol as well. It is interesting to note that the majority
of the services identified by Disclosure run on ports higher than
1024. However, the distribution changes significantly after the
FP reduction is applied. In fact, the reputation system filters out
around half of the HTTP services, but cuts between 70 and 90%
of the services running on high port numbers.
Finally, we manually investigated the C&C servers detected by
Disclosure to gain some insight into the accuracy of the detec-
tion models and the reasons for misclassification. To this end, we
chose the most conservative configuration: Disclosure configured
for 0% FP + Reputation filter. With this setup, during one week
135
(a) N1 port distribution.
(b) N2 port distribution.
Figure 5: Port distributions of the C&C servers detected by Disclosure for both N1 and N2, with and without AS
reputation scores.
of operation, Disclosure reported 91 previously unknown C&C
servers on the ISP network, and 20 on the university network.
We first manually queried popular search engines for each of the
111 entries. In 36 cases (32.4%), we found evidence of malware
that was related to those IP addresses.1 The fact that one third of
our reports were confirmed by other sources is a strong support of
the ability of Disclosure to successfully detect C&C servers. Out
of the remaining servers, 30 were associated with HTTP-related
ports. After a manual investigation, seven of them seemed to be
legitimate web sites—even though it is unusual that a small real
estate company or a personal page in the Philippines would receive
the large number of connections we observed in our traffic. Four
pages were default placeholders obtained with a fresh installation
of a web server; the number of NetFlow entries and varying
flow sizes is suspicious, although this could be attributed to the
web server not having a default virtual host configured. Four
servers returned errors related to either unauthorized access or bad
requests. Three of the HTTPS servers did not use the SSL/TLS
protocol but some other form of binary protocol. The remaining
servers were unaccessible at the time we checked them, which was
approximately three weeks after the data was collected. Of the
non-HTTP services, only four were still running at the time the
checks were performed. Three of these appeared legitimate, but
the remaining service was a web server located in Russia listening
to a non-standard port. Finally, interestingly, eight servers were
located in the Amazon cloud network, which is rapidly increasing
in popularity for hosting ephemeral malicious services.
5.5 Performance Evaluation
As described in Section 2, the detection phase consists of two
modules: feature extraction and detection. The detection mod-
ule is highly efficient, requiring only several minutes to process
an entire day’s worth of data. Hence, detection performance is
constrained by the analysis of input NetFlow data to extract the
requisite features for analysis.
However, since the extraction of each feature is an independent
process, the feature extraction procedure is an example of an em-
barrassingly parallel problem that can be easily distributed on mul-
tiple machines should the need arise. Nevertheless, even with the
large amount of input data for our evaluation networks, we have
not found it necessary to parallelize feature extraction. The current
prototype implementation of Disclosure consists of a number of
Perl and Python scripts, all running on the same server: a 16 core
1This evidence included reports from ThreatExpert, various
sandbox malware analysis tools, MaliciousUrl.com, or the
offensive IP database.
Intel(R) Xeon(R) CPU E5630 @ 2.53 GHz with 24 GB of ram.
In the course of our experiments, we run all individual feature
extraction modules in series in 10 hours 53 minutes for 24 hours of
data. Therefore, Disclosure is able to perform at approximately
2x real-time.
5.6 Deployment Considerations
To deploy Disclosure to a real network, the administrator
should configure three main settings: the minimum flows threshold
MinFlows, the classification threshold ClassThresh, and the FP
reduction threshold RepThresh. This setup can be accomplished
by performing the following steps:
1. Choose the MinFlows threshold.
This value should be selected according to the NetFlow sam-
pling rate for the monitored network and the amount of
available training data. If the threshold is set too high, the
system will not have enough C&C samples to properly train.
But, if it is set too low, the system will train on poor data,
and produce inaccurate models.
2. Choose an operational point on the ROC curve for ClassThresh.
This value should be selected according to the traffic volume of
the network and the misclassification rate that can be tolerated.
On one extreme, the system will be able to detect most of