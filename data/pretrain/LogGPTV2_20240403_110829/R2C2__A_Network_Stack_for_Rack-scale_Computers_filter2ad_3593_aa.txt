title:R2C2: A Network Stack for Rack-scale Computers
author:Paolo Costa and
Hitesh Ballani and
Kaveh Razavi and
Ian A. Kash
R2C2: A Network Stack for Rack-scale Computers
Paolo Costa
Hitesh Ballani
Kaveh Razavi∗
Ian Kash
Microsoft Research
ABSTRACT
Rack-scale computers, comprising a large number of micro-
servers connected by a direct-connect topology, are expected
to replace servers as the building block in data centers.
We focus on the problem of routing and congestion control
across the rack’s network, and ﬁnd that high path diversity
in rack topologies, in combination with workload diversity
across it, means that traditional solutions are inadequate.
We introduce R2C2, a network stack for rack-scale com-
puters that provides ﬂexible and efﬁcient routing and con-
gestion control. R2C2 leverages the fact that the scale of
rack topologies allows for low-overhead broadcasting to en-
sure that all nodes in the rack are aware of all network ﬂows.
We thus achieve rate-based congestion control without any
probing; each node independently determines the sending
rate for its ﬂows while respecting the provider’s allocation
policies. For routing, nodes dynamically choose the rout-
ing protocol for each ﬂow in order to maximize overall util-
ity. Through a prototype deployed across a rack emulation
platform and a packet-level simulator, we show that R2C2
achieves very low queuing and high throughput for diverse
and bursty workloads, and that routing ﬂexibility can pro-
vide signiﬁcant throughput gains.
CCS Concepts
•Networks → Data center networks; Transport protocols;
Cloud computing;
Keywords
Rack-scale Computers; Congestion Control; Route Selec-
tion; Rack-scale Network Stack
∗Work done during internship at Microsoft Research. Cur-
rently, PhD student at VU University Amsterdam.
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’15, August 17 - 21, 2015, London, United Kingdom
c(cid:13) 2015 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787492
1.
INTRODUCTION
While today’s large-scale data centers such as those run by
Amazon, Google, and Microsoft are built using commodity
off-the-shelf servers, recently there has been an increasing
trend towards server customization to reduce costs and im-
prove performance [50, 54, 55, 58]. One such trend is the
advent of “rack-scale computing”. We use this term to re-
fer to emerging architectures that propose servers or rack-
scale computers comprising a large number of tightly inte-
grated systems-on-chip, interconnected by a network fabric.
This design enables thousands of cores per rack and pro-
vides high bandwidth for rack-scale applications. The con-
sequent power, density and performance beneﬁts means that
racks are expected to replace individual servers as the basic
building block of datacenters. Early examples of rack-scale
computers include commercial (HP Moonshot [56], AMD
SeaMicro [62], Boston Viridis [51], and Intel RSA [26, 59])
as well as research platforms [7, 9, 19, 34, 38].
A design choice that allows rack-scale computers to
achieve high internal bandwidth and high density is to
move away from a switched network fabric to a “distributed
switch” architecture where each node functions as a small
switch and forwards trafﬁc from other nodes. This underlies
many existing designs [19, 34, 38, 47, 51, 56, 59, 62], and re-
sults in a multi-hop direct-connect topology, with very high
path diversity. This is a departure from today’s data centers,
which mostly use tree-like topologies. While direct-connect
topologies have been used in high performance computing
(HPC), the use of racks in multi-tenant datacenters means
that network trafﬁc is expected to be more diverse and un-
predictable than in HPC clusters.
In this paper, we study two fundamental questions for the
rack’s network fabric: how should trafﬁc be routed and how
should the network be shared? The aforementioned peculiar-
ities of rack environments pose challenges on both fronts.
For routing, a one-size-ﬁts-all approach is undesirable [1].
While there are many existing routing algorithms for direct-
connect topologies, no single algorithm can achieve optimal
throughput across all workloads (§2.2.1). For network shar-
ing, existing congestion control approaches either do not
cope with the high path diversity in racks (TCP family),
or they are customized to speciﬁc workloads (HPC solu-
tions [17, 18, 20]).
551We present R2C2,1 a network stack for rack-scale com-
puters that provides ﬂexible routing and congestion control.
R2C2 achieves global visibility—each rack node knows all
active ﬂows—by broadcasting ﬂow start and ﬁnish events
across the rack. The scale of rack-scale computers (up to a
few thousand nodes) allows for low overhead broadcasting.
Given global visibility, each node independently computes
the fair sending rates for its ﬂows (§3.3). To account for tem-
porary discrepancies in ﬂow visibility, the rate computation
leaves aside a small amount of bandwidth headroom. Such
congestion control obviates any network probing or special-
ized queuing at rack nodes, yet it can accommodate high
multi-pathing, and achieves both low network queuing and
high utilization. Furthermore, it allows the provider to spec-
ify rich rate allocation policies, beyond ﬂow-level policies.
For routing, nodes locally determine how ﬂows should be
routed to optimize a provider-speciﬁed metric like aggregate
or even tail rack throughput. Leveraging the global visibility
and ensuring that nodes optimize for a global metric instead
of selﬁsh optimizations avoids any price of anarchy inefﬁ-
ciency [42] (§3.4).
At the data plane, both the rate allocation and route for
a ﬂow are enforced at the sender by rate limiting the ﬂow’s
trafﬁc (one rate limiter per ﬂow) and encoding its network
path into packet headers respectively.
Intermediate rack
nodes can thus simply forward packets along the path spec-
iﬁed in their header, without requiring extra rate limiters or
complex queuing mechanisms on path. By placing more
functionality at the sender, this design enables a simple for-
warding layer that is amenable to on-chip implementation.
• We describe a novel approach for rate-based congestion
control that transforms the distributed network sharing
problem into one of local rate computation.
Overall, this paper makes the following contributions:
• We describe a routing mechanism that allows for rout-
ing protocols to be chosen on a per-ﬂow basis. We also
present a greedy heuristic that rack nodes can use to lo-
cally determine the routing protocol for each ﬂow in order
to maximize a global utility metric.
• We develop a ﬂexible emulation platform that enables ac-
curate emulation of the network fabric in rack-scale com-
puters. We use it to validate R2C2’s design.
We implemented R2C2 as a user-space network stack
atop our emulation platform. We use platform experiments
to benchmark our implementation and cross-validate our
packet-level simulator. Our simulation results show that
R2C2 can achieve efﬁcient network sharing for diverse net-
work workloads: it achieves high throughput, fairness and
low latency, and it imposes low broadcasting overhead. Fi-
nally, we show that routing ﬂexibility and the dynamic se-
lection process enables achieving higher performance that
what would be possible using a single routing protocol for
all ﬂows.
1R2C2- Rack Routing and Congestion Control.
Figure 1: A 27-server (3x3x3) 3D torus. Each server has six
neighbors.
2. BACKGROUND AND MOTIVATION
We begin by describing the factors underlying the emer-
gence of rack-scale computers. We then delve into two very
basic aspects of the network fabric inside a rack, routing and
congestion control. By highlighting salient features of the
network fabric, we argue why traditional solutions are insuf-
ﬁcient in the rack environment.
2.1 Rack-scale computing
Rack-scale computers comprise 100s or even 1,000s of
micro-servers that are connected by a network fabric. Their
emergence is due to two hardware innovations.
First,
System-on-chip (SoC) integration combines cores, caches,
and network interfaces in a single die. SoCs enable ven-
dors to build micro-servers: extremely small server boards
containing computation, memory, network interfaces, and
sometimes ﬂash storage. For instance, the Calxeda ECX-
1000 SoC [52] hosts four ARM cores, a memory controller,
a SATA interface, and a low-radix switch onto a single die.
Second, fabric integration means that these micro-servers
(or nodes) can be connected through a high-bandwidth low-
latency network. This is typically done through a “dis-
tributed switch” architecture;
rather than connecting all
nodes to a single ToR switch like in today’s racks, each node
is connected to a small subset of other nodes via point-to-
point links. These links offer high bandwidth (10–100 Gbps)
and low per-hop latency (100-500 ns). Any topology that has
a small number of links per node can be used; 2D and 3D
torus like the one in Figure 1 are popular choices adapted
from super-computing architectures. These topologies are
often referred to as multi-hop direct-connect topologies be-
cause packets typically have to travel multiple hops before
reaching the destination and nodes are responsible for for-
warding packets at intermediate hops (typically using an in-
tegrated switching element on the SoC).
Early examples of rack-scale computers have appeared on
the market. For example, HP’s Moonshot [56] is a 4.3 rack-
units chassis with 45 8-core Intel Atom SoCs and 1.4 TB of
RAM in a 3D torus topology. The AMD SeaMicro 15000-
OP [62] stacks 512 cores and 4 TB of RAM within 10 rack-
units using a 3D torus network fabric with a bisection band-
width of 1.28 Tbps. Intel’s proposed Rack-scale Architec-
ture [53, 59] combines SoC and fabric integration with sili-
con photonics, which support link bandwidths of 100 Gbps
and higher. Rack-scale computer designs have also been pro-
posed by the research community such as the Catapult [38],
Firebox [7], Pelican [9], and soNUMA [19, 34] platforms.
552Workload
Nearest neighbor
Uniform
Bit complement
Transpose
Tornado
Worst-case
Random Packet
Spraying
Destination
Tag routing
VLB WLB
4
1
0.4
0.54
0.33
0.21
4
1
0.5
0.25
0.33
0.25
0.5
0.5
0.5
0.5
0.5
0.5
2.33
0.76
0.42
0.57
0.53
0.31
Figure 2: Throughput, as fraction of network bisection ca-
pacity, of four routing algorithms on an 8-ary 2-cube on size
trafﬁc patterns (table from [20]). No single routing algo-
rithm can achieve optimal throughput across all workloads.
2.2 Rack-scale networking
Rack-scale networking combines the topology of HPC
systems with the workloads of traditional datacenters. Like
HPC clusters, racks have a multi-hop direct-connect topol-
ogy with very high path diversity. This is in contrast to tree-
like topologies common in datacenters. As for the workload,
we expect racks to be used in multi-tenant environments with
different applications generating diverse network workloads,
unlike the homogeneous and relatively stable workloads typ-
ically observed in HPC.
In the following sections, we focus on two fundamen-
tal questions for rack-scale networking: how should trafﬁc
be routed and how should the network be shared? We de-
scribe the shortcomings of existing solutions, and distill de-
sign goals for a rack-scale’s network stack.
2.2.1 Rack routing
Routing across direct-connect topologies has been exten-
sively studied in the scientiﬁc computing and HPC litera-
ture [20]. Existing routing algorithms can be broadly clas-
siﬁed into two categories. Minimal routing algorithms like
randomized packet spraying [22] and destination tag rout-
ing [20] route packets only along shortest paths. Non-
minimal algorithms like Valiant Load Balancing (VLB) [45]
and Weighted Load Balancing (WLB) [44] do not restrict
themselves to shortest paths.
High path diversity in direct-connect topologies means
that no single routing algorithm can achieve optimal
throughput across all workloads. Minimal routing ensures
low propagation delay but at the expense of load imbalance
across network links which, in turn, results in poor worst-
case performance. On the other hand, VLB transforms any
input trafﬁc matrix into a uniform random metric by rout-
ing packets through randomly chosen waypoints. This en-
sures guaranteed worst-case throughput across all workloads
but hurts average-case throughput, especially for workloads
with locality. WLB lies between these extremes; it consid-
ers non-minimal paths for load-balancing but biases the path
selection in proportion to the path length.
The table in Figure 2 summarizes these arguments by
showing the throughput of these routing algorithms for ﬁve
trafﬁc patterns and their worst-case throughput (the worst-
case trafﬁc patterns for each algorithm are different). While
VLB has the same performance across all trafﬁc patterns,
other algorithms perform better for speciﬁc workloads. By
contrast, tree-like topologies in today’s datacenters have a
few orders of magnitude less multi-pathing, so minimal rout-
ing algorithms like packet spraying are sufﬁcient [22].
Overall, for multi-tenant datacenters where the network
trafﬁc pattern is not known a priori and expected to change
over time, a one size ﬁts all approach is undesirable [1].
Thus, our ﬁrst design goal is:
G1 Routing ﬂexibility. The network stack should allow for