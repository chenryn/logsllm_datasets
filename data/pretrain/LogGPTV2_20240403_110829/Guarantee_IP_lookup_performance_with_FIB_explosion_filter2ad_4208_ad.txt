5.2 CPU Implementation
We implemented SAIL L and SAIL M on CPU platforms
because their data structures have less number of levels as
compared to SAIL B and SAIL U, and thus are suitable for
CPU platform. For our algorithms on CPU platforms, the
less number of levels means the less CPU processing steps. In
contrast, in FPGA platforms, because we build one pipeline
stage per level, the numbers of levels in our algorithms do
not have direct impact on IP lookup throughput.
Our CPU experiments were carried out on an Intel(R)
Core(TM) i7-3520M. It has two cores with four threads, each
core works at 2.9 GHz. It has a 64KB L1 code cache, a 64KB
L1 data cache, a 512KB L2 cache, and a 4MB L3 cache.
The DRAM size of our computer is 8GB. The actual CPU
frequency that we observe in our programs is 2.82GHz.
To obtain the number of CPU cycles of SAIL L and
SAIL M, we reverse engineering our C++ code. The as-
sembly code is shown in Table 2. This table shows that for
SAIL L, given an IP addressa, if the next hop of a can be
found on level 16, then the lookup needs only 3 CPU in-
structions; if the next hop of a can be found on level 24,
then the lookup needs only 10 CPU instructions. Note that
CPU could complete more than one instruction in one cycle.
In comparison, for the Lulea algorithm, given an IP address
a, if the next hop of a can be found on level 16, then the
lookup needs at least 17 ∼ 22 CPU cycles; if the next hop of
a can be found on level 24, then the lookup needs 148 CPU
cycles. This explains why our algorithm is much faster than
Lulea.
neg
jne
Table 2: The disassembling code of lookup.
ecx,ax
ebx,byte ptr[r10+rcx*2]
bl
sailtest+147h(013F3258D7h)
if((H=0 − BCN16[a (cid:2) 16])>0);
000000013F32589C movzx
000000013F32589F movzx
000000013F3258A4
000000013F3258A6
else if(H=BN24[(BCN [a (cid:2) 16] (cid:3) 8) + (a (cid:3) 16 (cid:2) 24)]);
ecx,word ptr[r10+rcx*2]
000000013F3258A8 movsx
000000013F3258AD shl
ecx,8
eax,byte ptr [rdi+rdx*4+1]
000000013F3258B0 movzx
ecx,eax
000000013F3258B5
ebx,byte ptr [rcx+r9]
000000013F3258B7 movzx
bl,bl
000000013F3258BC test
000000013F3258BE
jne
sailtest+147h(013F3258D7h)
add
455.3 GPU Implementation
6.1 Experimental Setup
We implemented SAIL L in GPU platforms based on N-
VDIA’s CUDA architecture [30]. In our GPU implementa-
tion, we store all data structures in GPU memory. Execut-
ing tasks on a GPU consists of three steps: (1) copy data
from the CPU to the GPU, (2) execute multiple threads for
the task, (3) copy the computing results back to the CPU;
these three steps are denoted by H2D, kernel execution, and
D2H, respectively. Thus, there is a natural communication
bottleneck between the CPU and the GPU. To address this
limitation, the common strategy is batch processing; that
is, the CPU transfers a batch of independent requests to
the GPU, and then the GPU processes them in parallel us-
ing many cores. In our GPU implementation, when packets
arrive, the CPU ﬁrst buﬀers them; when the buﬀer is full,
the CPU transfers all their destination IP addresses to the
GPU; when the GPU ﬁnishes all lookups, it transfers all
lookup results back to the CPU. For our GPU based SAIL
implementation, the data copied from the CPU to the GPU
are IP addresses, and the computing results copied from the
GPU to the CPU are the next hops for these IP addresses.
In our GPU implementation, we employed two optimization
techniques to speed up IP lookup performance: memory co-
alesce and multi-streaming.
The basic idea of memory coalesce is to let threads with
continuous IDs read memory of continuous locations because
GPU schedules its threads to access the main memory in
the unit of 32 threads. In our GPU implementation, after
the CPU transfers a batch of 32 ∗ n IP addresses to the
GPU’s memory denoted as an array A[0..32 ∗ n − 1], if we
assign threads t0, t1,··· , t32∗n−1 to process the IP addresses
A[0], A[1],··· , A[32∗n−1], respectively, then all these 32∗n
IP addresses can be read in n memory transitions by the
GPU, instead of 32 ∗ n memory accesses.
The basic idea of multi-streaming, which is available on
INVIDA Fermi GPU architecture, is to pipeline the three
steps of H2D, kernel execution, and D2H. According to
the CUDA computing model, data transfers (i.e., H2D and
D2H) and kernel executions within diﬀerent streams can be
parallelized via page-locked memory. A stream is a sequence
of threads that must be executed in order. In our GPU im-
plementation, we assign each 32 ∗ k (k ≥ 1) threads to a
unique stream. Thus, diﬀerent streams of 32 ∗ k threads can
be pipelined, i.e., while one stream of 32 ∗ k threads are
transferring data from the CPU to the GPU, one stream of
32 ∗ k threads are executing their kernel function, and an-
other stream of 32∗ k threads are transferring data from the
GPU to the CPU.
5.4 Many-core Implementation
We implemented SAIL L on the many-core platform Tel-
era TLR4-03680 [5], which has 36 cores and each core has
a 256K L2 cache. Our experiments were carried out on a
64-bit operation system CentOS 5.9. One L2 cache access
needs 9 cycles. In our many-core implementation, we let one
core to serve as the main thread and all other cores to serve
as lookup threads, where each lookup thread performs all
steps for an IP address lookup.
6. EXPERIMENTAL RESULTS
In this section, we present the experimental results of our
algorithms on the four platforms of FPGA, CPU, GPU, and
many-core.
To obtain real FIBs, we used a server to establish a peer
relationship with a tier-1 router in China so that the server
can receive FIB updates from the tier-1 router but does not
announce new preﬁxes to the tier-1 router; thus, gradually,
the server obtained the whole FIB from the tier-1 router.
Note that it is not practically feasible to dump the FIB of a
tier-1 router to hard disk because of the unbearable overhead
incurred on the router. On the server, we use the open source
Quagga to dump the FIB every hour [2]. We captured real
traﬃc in one of the tier-1 router’s interfaces at the interval of
10 minutes per hour between October 22nd 08:00 AM 2013
to October 23rd 21:00 PM.
In addition, we downloaded 18 real FIBs from www.
ripe.net. Six of them were downloaded at every 8:00
AM on January 1st of each year from 2008 to 2013, de-
noted by F IB2008, F IB2009,··· , F IB2013. Twelve of them
were downloaded from 12 routers on 08:00 AM August 8
2013, respectively, denoted by rrc00, rrc01, rrc03,··· rrc07,
rrc10,··· , rrc15. We also generated 37 synthetic traﬃc
traces. The ﬁrst 25 traces contain packets with randomly
chosen destinations. The other 12 traces were obtained by
generating traﬃc evenly for each preﬁx in the 12 FIBs down-
loaded from the 12 routers on 08:00 AM August 8 2013; thus,
we guarantee that each preﬁx has the same opportunity to
be hit by the synthetic traﬃc. We call such traﬃc preﬁx-
based synthetic traﬃc.
We evaluated our algorithms on four metrics: lookup speed
in terms of pps (# of packets per second), on-chip memory
size in terms of MB, lookup latency in terms of microsecond,
and update speed in terms of the total number of memory
accesses per update. For on-chip memory sizes, we are only
able to evaluate the FPGA implementation of SAIL algo-
rithms. For lookup latency, we evaluated our GPU imple-
mentation because the batch processing incurs more latency.
We compared our algorithms with four well-known IP
lookup algorithms: PBF [36], LC-trie [38], Tree Bitmap [40],
and Lulea [28]. We implemented our algorithms and these
four prior algorithms using C++. We validated the correct-
ness of all algorithms through exhaustive search: we ﬁrst
construct an exhaustive 232 = 4G lookup table where the
next hop of an IP address a is the a-th entry in this table;
second, for each IP address, we compare the lookup result
with the result of this exhaustive table lookup, and all our
implementations pass this validation.
6.2 Performance on FPGA
We evaluated the performance of our algorithm on FPGA
platforms in comparison with PBF, which is best suitable
for FPGA platforms among the four well known algorithms,
because the other four algorithms did not separate their data
structures for on-chip and oﬀ-chip memory.
We ﬁrst evaluate SAIL L for on-chip memory consump-
tion in comparison with PBF. Note that PBF stores its
Bloom ﬁlters in on-chip memory. We compute the on-chip
memory usage of PBF as follows. In [36], it says that PBF
needs 1.003 oﬀ-chip hash probes per lookup on average, giv-
en a routing table size of 116, 819. To achieve 1.003 oﬀ-chip
memory accesses per lookup assuming the best scenario of
one memory access per hash probe, the overall false posi-
tive rate of the ﬁlters should be 0.003. Thus, each Bloom
ﬁlter should have a false positive rate of 0.003/(32 − 8) s-
ince PBF uses 24 ﬁlters. Assuming that these Bloom ﬁl-
46ters always achieve the optimal false positive, then from
0.003/(32−8) = (0.5)k, we obtain k = 13 and m/n = 18.755,
where m is the total size of all Bloom ﬁlters and n is the
number of elements stored in the ﬁlter. Thus, given a FIB
with n preﬁxes, the total on-chip memory usage of PBF is
18.755 ∗ n.
Our experimental results on on-chip memory usage show
that within the upper bound of 2.13MB, the on-chip mem-
ory usage of SAIL L grows slowly and the growth rate is
slower than PBF, and that the on-chip memory usage of
SAIL L is smaller than PBF. For on-chip memory usage,
the fundamental diﬀerence between SAIL L and PBF is
that the on-chip memory usage of SAIL L has an upper
bound but that of PBF grows with the number of preﬁxes
in the FIB linearly without a practical upper bound. Fig-
ure 6 shows the evolution of the on-chip memory usage for
both SAIL L and PBF over the past 6 years based on our
results on the 6 FIBs: F IB2008, F IB2009,··· , andF IB 2013.
Figure 7 shows the on-chip memory usage of the 12 FIBs
rrc00, rrc01, rrc03,··· rrc07, rrc10,··· , rrc15. Taking FIB
rrc00 with 476,311 preﬁxes as an example, SAIL L needs
only 0.759MB on-chip memory.
e
g
a
s
u
y
r
o
m
e
m
p
h
c
-
n
O
i
1.2MB
1.0MB
800.0kB
600.0kB
400.0kB
200.0kB
0.0B
 SAIL_L
 PBF
2008
2009
2010
2011
Year
2012
2013
Figure 6: On-chip memory usage over 6 years.
e
g
a
s
u
y
r
o
m
e
m
p
h
c
-
n
O
i
1.2MB
1.0MB
800.0kB
600.0kB
400.0kB
200.0kB
0.0B
 SAIL_L  
 PBF
rrc00rrc01rrc03rrc04rrc05rrc06rrc07rrc10rrc11rrc12rrc13rrc14rrc15
FIB
Figure 7: On-chip memory usage of 12 FIBs.
We next evaluate SAIL L for lookup speed on FPGA plat-
form Xilinx Virtex 7. We did not compare with PBF be-
cause [36] does not provide implementation details for its
FPGA implementation. We focus on measuring the lookup
speed on the data structures stored in on-chip memory be-
cause oﬀ-chip memory lookups are out of the FPGA chip. As
we implement the lookup at each level as one pipeline stage,
SAIL B, SAIL U, SAIL L have 24, 4, 2 pipeline stages, re-
spectively. The more stages our algorithms have, the more
complex of the FPGA logics are, and the slower the FP-
GA clock frequency will be. Our experimental results show
that SAIL B, SAIL U, SAIL L have clock frequencies of
351MHz, 405MHz, and 479MHz, respectively. As each of our
pipeline stage requires only one clock cycle, the lookup speed
of SAIL B, SAIL U, SAIL L are 351Mpps, 405Mpps, and
479Mpps, respectively.
Let us have a deeper comparison of SAIL L with PBF.
The PBF algorithm without pushing requires 25 ∗ k hash
computations and memory accesses in the worst case be-
cause it builds 25 Bloom ﬁlters, each of which needs k hash
functions. With pushing, PBF needs to build at least 1
Bloom ﬁlter because the minimum number of levels is 1 (by
pushing all nodes to level 32), although which is impractical.
Further we assume that PBF uses the Kirsch and Mitzen-
macher’s double hashing scheme based Bloom ﬁlters, which
uses two hash functions to simulate multiple hash function-
s [6]. Although using the double hashing technique increases
false positives, we assume it does not. Furthermore, suppose
the input of hashing is 2 bytes, suppose PBF uses the well
known CRC32 hash function, which requires 6.9 clock cy-
cles per input byte. With these unrealistic assumptions, the
number of cycles that PBF requires for searching on its on-
chip data structures is 6.9 × 2 × 2. In comparison, SAIL L
requires only 3∼10 instructions as discussed in Section 5.2
and needs only 4.08 cycles per lookup based on Figure 8.
In summary, even with many unrealistic assumptions that
favor PBF, SAIL L still performs better.
6.3 Performance on CPU
We evaluate the performance of our algorithm on CPU
platforms in comparison with LC-trie, Tree Bitmap, and
Lulea algorithms. We exclude PBF because it is not suit-
able for CPU implementation due to the many hashing op-
erations.
Our experimental results show that SAIL L is several
times faster than LC-trie, Tree Bitmap, and Lulea algo-
rithms. For real traﬃc, SAIL L achieves a lookup speed of
673.22∼708.71 Mpps, which is 34.53∼58.88, 29.56∼31.44,
and 6.62∼7.66 times faster than LC-trie, Tree Bitmap,
and Lulea, respectively. For preﬁx-based synthetic traﬃc,
SAIL L achieves a lookup speed of 589.08∼624.65 Mpps,
which is 56.58∼68.46, 26.68∼23.79, and 7.61∼7.27 times
faster than LC-trie, Tree Bitmap, and Lulea, respective-
ly. For random traﬃc, SAIL L achieves a lookup speed
of 231.47∼236.08 Mpps, which is 46.22∼54.86, 6.73∼6.95,
and 4.24∼4.81 times faster than LC-trie, Tree Bitmap, and
Lulea, respectively. Figure 8 shows the lookup speed of these
4 algorithms with real traﬃc on real FIBs. The 12 FIBs are
the 12 FIB instances of the same router during the ﬁrst 12