title:Swift: Delay is Simple and Effective for Congestion Control in the
Datacenter
author:Gautam Kumar and
Nandita Dukkipati and
Keon Jang and
Hassan M. G. Wassel and
Xian Wu and
Behnam Montazeri and
Yaogong Wang and
Kevin Springborn and
Christopher Alfeld and
Michael Ryan and
David Wetherall and
Amin Vahdat
Swift: Delay is Simple and Effective for Congestion Control in
the Datacenter
Gautam Kumar, Nandita Dukkipati, Keon Jang (MPI-SWS)∗, Hassan M. G. Wassel, Xian Wu, Behnam Montazeri,
Yaogong Wang, Kevin Springborn, Christopher Alfeld, Michael Ryan, David Wetherall, and Amin Vahdat
Google LLC
ABSTRACT
We report on experiences with Swift congestion control in Google
datacenters. Swift targets an end-to-end delay by using AIMD con-
trol, with pacing under extreme congestion. With accurate RTT
measurement and care in reasoning about delay targets, we find
this design is a foundation for excellent performance when network
distances are well-known. Importantly, its simplicity helps us to
meet operational challenges. Delay is easy to decompose into fabric
and host components to separate concerns, and effortless to deploy
and maintain as a congestion signal while the datacenter evolves.
In large-scale testbed experiments, Swift delivers a tail latency of
10ms
∼100µs
400ns
100ns
10-20TiB
<10TiB
<1TiB
<1TiB
IOPS
<100
500k+
1M+
–
Bandwidth
120MB/s
6GB/s
2GB/s per channel
20GB/s per channel
Table 1: Single-device Storage characteristics
Kumar et al.
communication patterns. At the network level, Swift delivers high
utilization, sustaining a per-server throughput close to 100Gbps
(100% load) at the same time maintaining low delay and near-zero
loss. As a reference point, the DCTCP loss rate is at least 10x higher
from moderate to high load. At the application layer, Swift provides
short RPC completion times for intensive storage and analytics
workloads. For a demanding in-memory shuffle service [51, 57, 58],
Swift achieves average latency close to the baseline delay for short
transfers. By handling host congestion effectively, Swift sustains
high IOPS even under incasts of O(10k) flows. We detail these results
and more in the paper.
We draw several conclusions from our experiences. First, delay as
a congestion signal has proven effective for excellent performance
with a simplicity that has helped greatly with operational issues.
In fact, Swift’s design has been simplified from TIMELY, as it finds
the use of an absolute target delay to be performant and robust.
Second, it is important to respond to both fabric and host conges-
tion. We initially underestimated congestion at hosts (as have most
designs) but both forms matter across a range of latency-sensitive,
IOPS-intensive, and byte-intensive workloads. Delay is readily de-
composed for this purpose. Third, we must support a wide range of
traffic patterns including large-scale incast. This range leads us to
pace packets when there are more flows than the bandwidth-delay
product (BDP) of the path, while using a window at higher flow
rates for CPU efficiency.
This work does not raise any ethical issues.
2 MOTIVATION
The evolution of Swift was driven by trends in storage workloads,
host networking stacks, and datacenter switches.
Storage Workloads. Storage is the dominant workload for our
datacenter networks. It is the primary medium for communication
across jobs, and disaggregation means that storage access crosses
the network. Disk traffic is dominated by O(10) ms access latency
rather than network latency, so carrying disk traffic does not require
low-latency congestion control. But latency has become critical as
cluster-wide storage systems have evolved to faster media (Table 1).
Flash access latency is 100µs, making milliseconds of network la-
tency unacceptable, and NVMe is even more demanding [25]. Tight
network tail latency is a requirement because storage access touches
multiple devices, and the overall latency for any single storage op-
eration is dictated by the latency of the longest network operation.
And in-memory filesystems, e.g., Octopus [35], require multiple
round trips for transactions, which also stresses low fabric latency.
Moreover, high throughput is needed. Cluster storage systems op-
erate at petabyte scale. Demanding applications such as BigQuery
run a shuffle workload on top of an in-memory filesystem [11]. For
Swift, the need has been to continually tighten tail latency without
sacrificing throughput.
Host Networking Stacks. The implementation of congestion con-
trol has undergone a wholesale change in the datacenter. Traditional
515
Figure 1: Swift as a packet-level congestion-control in the context of the Pony-
Express architecture.
congestion control runs as part of the host operating system, e.g.,
Linux or Windows, and serves the general purpose use case. This
setting is limited by its APIs, e.g., sendmsg and setsockopt, and
is often expensive for innovation. Adding pacing in the kernel,
for example, takes 10% of machine CPU [43, 44]. Newer stacks
such as RDMA and NVMe are designed from the ground up for
low-latency storage operations. To avoid operating system over-
heads, they are typically implemented in OS bypass stacks such
as Snap [36] or offloaded to the NIC [6, 13]. Swift runs in Snap
(as described shortly) and lets us design congestion control on a
clean-slate with features such as NIC timestamps and fine-grained
pacing. Snap also facilitated fast iterations. Swift also inspired a
delay-based congestion control scheme to control the issue rate of
RDMA operations based on precise timestamp measurements in
the 1RMA [47] system. Additionally, addressing host congestion
has become critical to maintain low end-to-end queuing. Increasing
line-rates and IOPS-intensive workloads stress software/hardware
per-packet processing resources; CPU, DRAM bandwidth, and PCIe
bottlenecks build up queues in NICs and host stacks. For Swift,
delay is decomposed to alleviate both fabric and host congestion.
Datacenter Switches. Our network has several generations of
switches in the same fabric; heterogeneity is inevitable given ad-
vances in line rates from 10Gbps to 100Gbps and beyond. Tying
congestion control deeply to switch internals poses a larger main-
tenance burden. For example, DCTCP relies on switches to mark
packets with ECN when the queue size crosses a threshold. Select-
ing an appropriate threshold and maintaining the configurations as
line speeds and buffer sizes vary is challenging at scale.1 If we keep
the same threshold (in bytes), then a 10× increase in link speed
would mark packets 10× earlier in time even though the control
loop is not 10× faster. If we grow the threshold, we must consider
that absolute buffer size has increased with successive switch gen-
erations but is limited by chip area and has not kept pace with
line-rates. Tuning is made more difficult because different switches
have different ways of managing buffer, e.g., memory bank limita-
tions. For Swift, we have found it easier to evolve delay targets at
hosts as part of the march towards lower latency than to integrate
with signals from switches.
3 SWIFT DESIGN & IMPLEMENTATION
In this section, we articulate how we settled on Swift and evolved
the protocol over time. We avoided switch modifications to more
1The DCTCP authors provide a formula to compute marking thresholds [1] but echo
our experience that care is needed in production networks.
NICApplicationcommand & completion queuesNIC queuesop schedulerDelay computationCWND computationOp layerop streamsop streamsPacket-level Transport layerflow mapperflowsflowsRTTPacing componentSwiftCWNDSwift: Delay is Simple and Effective for Congestion Control in the Datacenter
readily support an evolving and heterogeneous cluster environment.
We found existing end-host-based schemes such as DCTCP, D3 [54],
and D2TCP [52] to be insufficient because of their inability to handle
large scale incasts and congestion at hosts. Our observation was that
a simple scheme around delay measurements could be sufficient.
These high-level requirements guided the evolution of Swift:
(1) Provide low, tightly-bound network latency, near zero loss, and
high throughput while scaling to a large datacenter across a
range of workloads.
(2) Provide end-to-end congestion-control that manages conges-
tion not only in the network fabric but also in the NIC, and on
hosts, i.e., in software stacks. We call the latter endpoint (or
host) congestion in this paper.
(3) Be highly CPU-efficient so as to not compromise an otherwise
CPU-efficient OS bypass communication.
Latency, loss and throughput are traditional measures of conges-
tion control. Low network latency reduces the completion times
of short RPCs. Low loss is critical because loss adds significantly
to latency of higher-level application transfer units, e.g., RPCs, es-
pecially at the tail. We find two additional measures that are also
important in production—endpoint congestion and CPU efficiency.
The design we arrived at uses end-to-end RTT measurements
to modulate a congestion window in packets, with an Additive-
Increase Multiplicative-Decrease (AIMD) algorithm, with the goal
of maintaining the delay around a target delay. Swift decomposes
the end-to-end RTT into NIC-to-NIC (fabric) and endpoint delay
components to respond separately to congestion in the fabric versus
at hosts/NICs.
Swift is implemented in Pony Express, a networking stack provid-
ing custom reliable transport instantiated in Snap [36]. It uses NIC
as well as software timestamps for accurate RTT measurements. It
uses Pony Express for CPU-efficient operation and low latency, and
as an environment suited to features such as pacing. Figure 1 shows
the placement of Swift in Pony Express. Pony Express provides com-