GlobalVariableRaceDetector
ImproperTaintedDataUseDetector
IntegerOverﬂowDetector
KernelUninitMemoryLeakDetector
InvalidCastDetector
Huawei
62 / 62 / 5
552 / 155 / 12
75 / 56 / 4
324 / 184 / 38
81 / 74 / 5
250 / 177 / 6
9 / 7 / 5
96 / 13 / 2
Qualcomm
33 / 33 / 2
264 / 264 / 3
52 / 52 / 0
188 / 108 / 8
92 / 91 / 3
196 / 196 / 2
Mediatek
155 / 153 / 6
465 / 459 / 6
73 / 73 / 1
548 / 420 / 5
243 / 241 / 9
247 / 247 / 6
1 / 1 / 0
75 / 74 / 1
8 / 5 / 5
9 / 9 / 0
1,449 / 728 / 78
901 / 819 / 19
1,748 / 1,607 / 44
973 / 819 / 24
5,071 / 3,973 / 158
ing used by a dangerous function). All of these warnings
were manually veriﬁed by the authors, and those that are
marked as a bug were conﬁrmed to be critical zero-day
bugs, which we are currently in the process of disclosing
to the appropriate vendors. In fact, 7 of the 158 identiﬁed
zero-days have already been issued Common Vulnerabil-
ities and Exposures (CVE) identiﬁers [6–10]. Of these,
Sparse correctly identiﬁed 1, ﬂawﬁnder correctly identi-
ﬁed 3, RATs identiﬁed 1 of the same ones as ﬂawﬁnder,
and cppcheck failed to identify any of them. These bugs
ranged from simple data leakages to arbitrary code ex-
ecution within the kernel. We ﬁnd these results very
promising, as 3,973 out of the 5,071 were conﬁrmed,
giving us a precision of 78%, which is easily within the
acceptable 30% range [14].
While the overall detection rate of DR. CHECKER
is quite good (e.g., KernelUninitMemoryLeakDetector
raised 24 warnings, which resulted in 11 zero-day bugs),
there a few notable lessons learned. First, because our
vulnerability detectors are stateless, they raise a warning
for every occurrence of the vulnerable condition, which
results in a lot of correlated warnings. For example, the
code i = tainted+2; j = i+1; will raise two Inte-
gerOverﬂowDetector warnings, once for each vulnera-
ble condition. This was the main contributor to the huge
gap between our conﬁrmed warnings and the actual bugs
as each bug was the result of multiple warnings. The
over-reporting problem was ampliﬁed by our context-
sensitive analysis. For example, if a function with a vul-
nerable condition is called multiple times from different
contexts, DR. CHECKER will raise one warning for each
context.
GlobalVariableRaceDetector suffered from numerous
false positives because of granularity of the LLVM in-
structions. As a result,
the detector would raise a
warning for any access to a global variable outside
of a critical section. However, there are cases where
the mutex object is stored in a structure ﬁeld (e.g.,
mutex lock(&global->obj)). This results in a false
positive because our detector will raise a warning on the
access to the global structure, despite the fact that it is
completely safe, because the ﬁeld inside of it is actually
a mutex.
TaintedPointerDerefenceDetectors similarly struggled
with the precision of its warnings. For example, on
Huawei drivers (row 2, column 1), it raised 552 warn-
ings, yet only 155 were true positives. This was due
to the over-approximation of our points-to analysis. In
fact, 327 of these are attributed to only two entry points
rpmsg hisi write and hifi misc ioctl, where our
analysis over-approximated a single ﬁeld that was then
repeatedly used in the function. A similar case hap-
pened for entry point sc v4l2 s crop in Samsung,
which resulted in 21 false warnings. The same over-
approximation of points-to affected InvalidCastDetector,
with 2 entry points (picolcd debug flash read and
picolcd debug flash write) resulting in 66 (80%)
false positives in Huawei and a single entry point
(touchkey fw update.419) accounting for a major-
ity of the false positives in Samsung.
IntegerOver-
ﬂowDetector also suffered from over-approximation at
times, with 30 false warnings in a single entry point
hifi misc ioctl for Hauwei.
One notable takeaway from our evaluation was that
while we expected to ﬁnd numerous integer overﬂow
bugs, we found them to be far more prevalent in 32 bit ar-
chitectures than 64 bites, which is contrary to previously
held beliefs [58]. Additionally, DR. CHECKER was able
to correctly identify the critical class of Boomerang [33]
bugs that were recently discovered.
7.2 Soundy Assumptions
DR. CHECKER in total analyzed 1207 entry points and
90% of the entry points took less than 100 seconds to
complete. DR. CHECKER’s practicality and scalability
are made possible by our soundy assumptions. Speciﬁ-
cally, not analyzing core kernel functions and not wait-
ing for loops to converge to a ﬁxed-point. In this sec-
tion, we evaluate how these assumptions affected both
USENIX Association
26th USENIX Security Symposium    1019
Table 5: Runtime comparison of 100 randomly selected
entry points with our analysis implemented a “sound”
analysis (Sound), a soundy analysis, without analyz-
ing kernel functions (No API), and a soundy analy-
sis without kernel functions or ﬁxed-point loop analysis
(DR. CHECKER).
Analysis
Sound∗
No API
DR. CHECKER
Avg.
175.823
110.409
35.320
Runtime (seconds)
Min.
0.012
0.016
0.008
2261.468
2996.036
978.300
Max.
St. Dev.
527.244
455.325
146.238
∗ Only 18/100 sound analyses completed successfully.
our precision (i.e., practicality) and runtime (i.e., scala-
bility). This analysis was done by randomly selecting 25
entry points from each of our codebases (i.e., Huawei,
Qualcomm, Mediatek, and Samsung), resulting 100 ran-
domly selected driver entry points. We then removed our
two soundy assumptions, resulting in a “sound” analysis,
and ran our analysis again.
Kernel Functions Our assumption that all kernel func-
tions are bug free and correctly implemented is critical
for the efﬁcacy of DR. CHECKER for two reasons. First,
the state explosion that results from analyzing all of the
core kernel code makes much of our analysis compu-
tationally infeasible. Second, as previously mentioned,
compiling the Linux kernel for ARM with LLVM is still
an ongoing project, and thus would require a signiﬁcant
engineering effort [52].
In fact, in our evaluation we
compiled the 100 randomly chosen entry with best-effort
compilation using LLVM, where we created a consol-
idated bitcode ﬁle for each entry point with all the re-
quired kernel API functions, caveat those that LLVM
failed to compile. We ran our “sound” analysis with
these compiled API functions and evaluated all loops un-
til both our points-to and taint analysis reached a ﬁxed
point, and increased our timeout window to four hours
per entry point. Even with the potentially missing ker-
nel API function deﬁnitions, only 18 of these 100 entry
points ﬁnished within the 4 hours. The ﬁrst row (Sound)
in Table 5 shows the distribution of time over these 18
entry points. Moreover, these 18 entry points produced
63 warnings and took a total of 52 minutes to evaluate,
compared to 9 warnings and less than 1 minute of evalu-
ation time using our soundy analysis.
we ignored the kernel API functions (i.e., assume cor-
rect implementation), but evaluated all loops until they
reached a ﬁxed point on the same 100 entry points. In
this case, all of the entry points were successfully ana-
lyzed within our four hour timeout window. The second
row (No API) in Table 5 shows the distribution of eval-
uation times across these entry points. Note that this ap-
proach takes 3× more time than the DR. CHECKER ap-
proach to analyze an entry point on average. Similarly,
our soundy analysis returned signiﬁcantly fewer warn-
ings, 210 compared to the 474 warnings that were raised
by this approach.
A summary of the execution times (i.e., sound, ﬁxed-
point
loops, and DR. CHECKER) can be found in
Table 5, which shows that ignoring kernel API functions
is the main contributor of the DR. CHECKER’s scalabil-
ity. This is not surprising because almost all the ker-
nel drivers themselves are written as kernel modules [2],
which are small (7.3K lines of code on average in the
analyzed kernels) and self-contained.
8 Discussion
Although DR. CHECKER is designed for Linux kernel
drivers, the underlying techniques are generic enough to
be applied to other code bases. Speciﬁcally, as shown
in Section 7.1, ignoring external API functions (i.e., ker-
nel functions) is the major contributor to the feasibility of
DR. CHECKER on the kernel drivers. DR. CHECKER in
principle can be applied to any code base, which is mod-
ular and has well-deﬁned entry points (e.g., ImageMag-
ick [1]). While our techniques are portable, some en-
gineering effort is likely needed to change the detectors
and setup the LLVM build environment. Speciﬁcally, to
apply DR. CHECKER, one needs to:
1. Identify the source ﬁles of the module, and compile
them in to a consolidated bitcode ﬁle.
2. Identify the function names, which will serve as en-
try points.
3. Identify how the arguments to these functions are
tainted.
We provided more in-depth documentation of how this
would be done in practice on our website.
9 Related Work
Fixed-point Loop Analysis Since we were unable to
truly evaluate a sound analysis, we also evaluated our
second assumption (i.e., using a reach-def loop analysis
instead of a ﬁxed-point analysis) in isolation to exam-
ine its impact on DR. CHECKER.
In this experiment,
Zakharov et al. [65] discuss many of the existing tools
and propose a pluggable interface for future static-
analysis techniques, many of which are employed in
DR. CHECKER. A few different works looked into the
API-misuse problem in kernel drivers. APISan [64] is
1020    26th USENIX Security Symposium
USENIX Association
Listing 7: Example of output from DR. CHECKER
At C a l l i n g C o n t e x t :
%c a l l 2 5 = c a l l
s r c
Found : 1 warning .
i 6 4 @ g e d d i s p a t c h (% s t r u c t . GED BRIDGE PACKAGE∗ %sBridgePackageKM ) ,
! dbg ! 2 7 8 2 3 ,
l i n e : 1 8 7 d r i v e r s / misc / m e d i a t e k / gpu / ged / s r c / ged main . c
Warning : 1
P o t e n t i a l v u l n e r a b i l i t y d e t e c t e d by : I n t e g e r O v e r f l o w D e t e c t o r
P o t e n t i a l overflow , u s i n g t a i n t e d v a l u e i n a b i n a r y o p e r a t i o n a t :
%add = add i 3 2 %2, %3,
! dbg ! 2 7 7 9 2 ,
:
s r c
l i n e : 1 0 1 d r i v e r s / misc / m e d i a t e k / gpu / ged / s r c / ged main . c , Func : g e d d i s p a t c h
i 6 4 %u l B y t e s ) ,
! dbg ! 2 7 7 9 6 ,
T a i n t Tr a ce :
%c a l l 2 = c a l l
%2 = l o a d i32 ,
i 6 4 @ c o p y f r o m u s e r ( i 8∗ %pvTo ,
i 3 2∗ %i 3 2 I n B u f f e r S i z e 3 ,
a l i g n 8 ,
i 8∗ %pvFrom ,
! dbg ! 2 7 7 9 0 ,
s r c
l i n e : 4 3 d r i v e r s / misc / m e d i a t e k / gpu / ged / s r c / g e d b a s e . c , Func : g e d c o p y f r o m u s e r
s r c
l i n e : 1 0 1 d r i v e r s / misc / m e d i a t e k / gpu / ged / s r c / ged main . c , Func : g e d d i s p a t c h
a symbolic-execution-based approach, and Static Driver
Veriﬁer (SDV) [12] similarly identiﬁed API-misuse us-
ing static data-ﬂow analysis. However, these techniques
are contrary to DR. CHECKER, as we explicitly assume
that the kernel APIs are implemented properly.
SymDrive [43] uses symbolic execution to verify
properties of kernel drivers. However, it requires de-
velopers to annotate their code and relies heavily on
the bug ﬁnder to implement proper checkers. Johnson
et al. [28] proposed a sound CQUAL-based [24] tool,
which is context-sensitive, ﬁeld-sensitive, and precise
taint-based analysis; however, this tool also requires user
annotations of the source code, which DR. CHECKER
does not.
KINT [56] uses taint analysis to ﬁnd integer errors in
the kernel. While KINT is sound, their techniques are
specialized to integer errors, whereas DR. CHECKER at-
tempts to ﬁnd general input validation errors by compro-
mising soundness.
Linux Driver Veriﬁcation (LDV) [36] is a tool based
on BLAST [27] that offers precise pointer analysis; how-
ever, it is still a model-checker-based tool, whereas we
built our analysis on well-known static analysis tech-
niques. Yamaguchi et al. have done a signiﬁcant amount
of work in this area, based on Joern [59–62], where they
use static analysis to parse source code into novel data
structures and ﬁnd known vulnerable signatures. How-
ever, their tool is similar to a pattern-matching model-
checking type approach, whereas we are performing gen-
eral taint and points-to analysis with pluggable vulner-
ability detectors. VCCFinder [41] also used a simi-
lar pattern-matching approach, but automatically con-
structed their signatures by training on previously known
vulnerabilities to create models that could be used to de-
tect future bugs.
MECA [63] is a static-analysis framework, capable of
taint analysis, that will report violations based on user
annotations in the source code, and similarly aims to re-
duce false positives by sacriﬁcing soundness. ESP [22] is
also capable of fully path-sensitive partial analysis using
“property simulation,” wherein they combine data-ﬂow
analysis with a property graph. However, this approach
is not as robust as our more general approach.
Boyd-Wickizer et al. [15] proposed a potential defense
against driver vulnerabilities that leverages x86 hardware
features; however, these are unlikely to be easily ported
to ARM-based mobile devices. Nooks [49] is a similar
defense; however, this too has been neglected in both the
mainline and mobile deployments thus far, due to similar
hardware constraints.
10 Conclusion
We have presented DR. CHECKER, a fully-automated
static analysis bug-ﬁnding tool for Linux kernels that