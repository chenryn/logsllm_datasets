### Evaluation of FLTrust and Other Federated Learning Methods

#### (b) CNN Global Model, MNIST-0.5
- **FedAvg**
- **Trim-mean Median**
- **FLTrust**

| Attack Type | No Attack | LF Attack | Krum Attack | Trim Attack | Scaling Attack (Success Rate) |
|-------------|-----------|-----------|-------------|-------------|--------------------------------|
| FedAvg      | 0.50 / 1.00 | 0.09 / 0.01 | 0.06 / 0.02 | 0.06 / 0.01 | 0.05 / 0.00 |
| Trim-mean Median | 0.05 | 0.05 | 0.05 | 0.06 | 0.04 |
| FLTrust     | 0.06 | 0.06 | 0.10 | 0.28 | 0.06 |

#### (c) CNN Global Model, Fashion-MNIST
- **FedAvg**
- **Trim-mean Median**
- **FLTrust**

| Attack Type | No Attack | LF Attack | Krum Attack | Trim Attack | Scaling Attack (Success Rate) |
|-------------|-----------|-----------|-------------|-------------|--------------------------------|
| FedAvg      | 0.90 / 1.00 | 0.16 / 0.03 | 0.17 / 0.85 | 0.16 / 0.05 | 0.11 / 0.02 |
| Trim-mean Median | 0.11 | 0.11 | 0.12 | 0.14 | 0.10 |
| FLTrust     | 0.14 | 0.14 | 0.13 | 0.90 | 0.14 |

#### (d) ResNet20 Global Model, CIFAR-10
- **FedAvg**
- **Trim-mean Median**
- **FLTrust**

| Attack Type | No Attack | LF Attack | Krum Attack | Trim Attack | Scaling Attack (Success Rate) |
|-------------|-----------|-----------|-------------|-------------|--------------------------------|
| FedAvg      | 0.90 / 1.00 | 0.44 / 0.07 | 0.22 / 0.96 | 0.25 / 0.96 | 0.18 / 0.02 |
| Trim-mean Median | 0.18 | 0.18 | 0.18 | 0.20 | 0.16 |
| FLTrust     | 0.21 | 0.24 | 0.24 | 0.81 | 0.24 |

#### (e) LR Global Model, HAR
- **FedAvg**
- **Trim-mean Median**
- **FLTrust**

| Attack Type | No Attack | LF Attack | Krum Attack | Trim Attack | Scaling Attack (Success Rate) |
|-------------|-----------|-----------|-------------|-------------|--------------------------------|
| FedAvg      | 0.04 / 0.81 | 0.10 / 0.03 | 0.04 / 0.36 | 0.05 / 0.13 | 0.05 / 0.01 |
| Trim-mean Median | 0.04 | 0.04 | 0.04 | 0.05 | 0.03 |
| FLTrust     | 0.17 | 0.03 | 0.32 | 0.04 | 0.05 |

#### (f) ResNet20 Global Model, CH-MNIST
- **FedAvg**
- **Trim-mean Median**
- **FLTrust**

| Attack Type | No Attack | LF Attack | Krum Attack | Trim Attack | Scaling Attack (Success Rate) |
|-------------|-----------|-----------|-------------|-------------|--------------------------------|
| FedAvg      | 0.26 / 0.20 | 0.34 / 0.03 | 0.14 / 0.02 | 0.11 / 0.01 | 0.14 / 0.03 |
| Trim-mean Median | 0.10 | 0.12 | 0.12 | 0.13 | 0.10 |
| FLTrust     | 0.12 | 0.11 | 0.64 | 0.10 | 0.15 |

### Observations and Analysis

Previous research [9] has shown that FedAvg can be easily manipulated by a single malicious client. In contrast, FLTrust significantly reduces the success rates of scaling attacks, with attack success rates never exceeding 0.03. This indicates that existing federated learning methods, including Byzantine-robust techniques, are not robust against scaling attacks. Interestingly, scaling attacks sometimes decrease testing error rates, possibly due to data augmentation positively impacting the aggregation of local model updates.

### Variants of FLTrust

To evaluate the effectiveness of different features in FLTrust, we consider the following variants:

- **FLTrust-Server**: The server uses only the root dataset to train the global model, with no communication between clients and the server.
- **FLTrust-withServer**: The server computes the weighted average of client updates and the server update, with a trust score of 1 for the server update.
- **FLTrust-NoReLU**: The server does not use ReLU to clip cosine similarity scores when computing trust scores.
- **FLTrust-NoNorm**: The server does not normalize local model updates.
- **FLTrust-ParNorm**: The server applies partial normalization, normalizing only updates with magnitudes larger than the server's.

### Impact of Root Dataset Size

The size and sampling method of the root dataset significantly impact FLTrust's performance. A root dataset with as few as 100 training examples is sufficient for FLTrust to defend against various attacks. Figure 4 illustrates the testing error rates and attack success rates under the scaling attack for different sizes of the root dataset on MNIST-0.5.

### Efficiency of FLTrust

FLTrust achieves its efficiency goal by not incurring extra overhead to clients and by minimizing additional computational costs on the server. Specifically, FLTrust converges as quickly as FedAvg, indicating no additional communication costs. While Krum, Trim-mean, and Median do not add overhead to clients, Krum incurs significant computational overhead on the server, especially with a large number of clients, due to the need to calculate pairwise distances between local model updates.

### Summary

FLTrust outperforms other federated learning methods, particularly in defending against scaling attacks. The necessity of ReLU and normalization in FLTrust is evident from the performance of its variants. Additionally, a small but well-sampled root dataset is sufficient for effective defense.