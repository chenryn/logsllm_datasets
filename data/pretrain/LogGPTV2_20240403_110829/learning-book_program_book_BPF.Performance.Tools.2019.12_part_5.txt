0/cormand/svstat /service/httpd
0,440
per1
15527
15525
0 /usx/bin/per] =e $l=<>;$1=~/ (d+] sec/zpz1n..
0 , 442
P5
15529
15528
0 /bin/ps -=ppid 1 -o pid, cnd, args
[. - -]
0 , 487
catalina.sh
15524
4469
0/apps/tomcat/bin/catalina.sh atart
0,488
dirname
15549
15524
0 /usx/bin/dlrnane /apps/toncat/b1n/catalina sh
1, 459
run
155504469
0,/run
1, 459
bash
15550
4469
0 /bsn/bash
1, 462
15552
15551
0/conmand/srstat /service/nf1x-httpd
1. 462
per1
ES55T
15551
0 /usx/b1n/perl =e $1=<>;$1=~/ (.d+] sec/ =pz1n..-
[. .-]
I’ve truncated the output (as indicated by the [.-]), but the timestamp column shows a new clue:
The time between new processes jumps by one second, and this pattern repeats. By browsing the
output, I could see that 30 new processes were launched every second, followed by a one-second
pause between these batches of 30 processes.
The output shown here is taken from a real-world issue at Netflix that I debugged using
execsnoop(8). This was occurring on a server used for micro-benchmarking, but the bench-
mark results showed too much variance to be trusted. I ran execsnoop(8) when the system was
supposed to be idle, and discovered that it wasnt: Every second these processes were launched,
and they were perturbing our benchmarks. The cause turned out to be a misconfigured service
that was attempting to launch every second, failing, and starting again. Once the service was
deactivated, these processes stopped (as confirmed using execsnoop(8), and then the benchmark
numbers became consistent.
The output from execsnoop(8) aids a performance analysis methodology called workload
s ogopoau s ooq s u soo ddg ro dueu 6q pauoddns s qrum 'aogzuapup
simple: Define what workload is being applied. Understanding the workload is often sufficient for
solving problems, and avoids needing to dig deeper into latencies or to do drill-down analysis. In
this case, it was the process workload applied to the system. Chapter 3 introduces this and other
methodologies.
Try running execsnoop(8) on your systems and leave it running for an hour. What do you find?
execsnoop(8) prints per-event data, but other tools use BPF to calculate efficient summaries.
Another tool you can use for quick wins is biolatency(8), which summarizes block device I/O
(disk I/O) as a latency histogram.
---
## Page 43
Chapter 1 Introduction
The following is output from running biolatency(8) on a production database that is sensitive
to high latency as it has a service level agreement to deliver requests within a certain number of
milliseconds.
: biolatency -B
Tracing block device I/0... Hit Ctrl-C to end.
°C
nsecs
1coun.t
dis t.r i.but.ion
0 -> 1
SEE9T :
2 -> 3
: 2272
|+****
4 -> 7
E09E :
| ++++**+*
8 > 15
: 4328
|*********+
1.6 -> 31
........
32 > 63
: 5815
64 -> 127
: 0
128 > 255
: 0
: 0
25 6 -> 511
512 => 1023
: 11
While the biolatency(8) tool is running, block I/O events are instrumented and their latencies are
calculated and summarized by BPE. When the tool stops running (when the user presses Ctrl-C),
the summary is printed. I used the n option here to print the summary in milliseconds. 
There are interesting details in this output, which shows a bi-modal distribution as well as latency
outliers. The largest mode (as visualized by the ASCII distribution) is for the O- to 1-millisecond
range, with 16,355 I/O in that range while tracing. This is fast, and likely due to on-disk cache hits
as well as flash memory devices. The second mode stretches to the 32- to 63-millisecond range,
which is much slower than expected from these storage devices and suggests queuing. More BPF
tools can be used to drill deeper to confirm. Finally, for the 512- to 1023-millisecond range, there
were 11 I/O. These very slow I/O are termed latency outfiers. Now that we know they exist, they can
be examined in more detail with other BPF tools. For the database team, these are the priority to
study and solve: If the database is blocked on these I/O, the database willexceed its latency target.
1.5BPFTracingVisibility
BPF tracing gives you visibility across the full software stack and allows new tools and instrumen-
tation to be created on demand. You can use BPF tracing in production immediately, without
needing to reboot the system or restart applications in any special mode. It can feel like having
X-ray vision: When you need to examine some deep kernel component, device, or application
library, you can see into it in a way that no one ever has beforelive and in production.
To illustrate, Figure 1-2 shows a generic system software stack that I've annotated with BPF-based
performance tools for observing different components. These tools are from BCC, bpftrace, and
this book. Many of them will be explained in later chapters
---
## Page 44
1.5 BPF Tracing Visilbility
ofile
sriteaysc
hreadiine
eads oge
shellsnoop
Applicat
Runtimes
epes
vrite)
System Libraries
epndlest rongiea
em Call Interface
ranqs1over
SA
Sockets
Scheduler
deadiock
prorile
File Systems
TCPIUDP
feuew ewnon
IP
Virtual
snpcalls
vorke
nfeslovos
Block Dev
Net Device
kill
iayfo
xfedi
Device Drivt
cailat
1eee80211
osotop
wmelateney
netsist
sofds
hfa
Ditesis
tepe
tt
Mardirgtt
ketio
socksite
tepat
Other
capabie
1lcstat
cpatreq
CPUs
Figure 1-2 B
BPF performance toolls and their visibility
Consider the different tools you would use to examine components such as the kernel CPU
schedluler, virtual memory, file systems, and so on. By simply browsing this diagram, you might
discover former blindspots that you can observe with BPF tools.
The traditional tools used to examine these components are summarized in Table 1-1, along with
whether BPF tracing can observe these components.
Table 1-1 Traditional Analysis Tools
Components
Traditional Analysis Tools
BPF Tracing
Applications with language runtimes:
Runtime debuggers
Yes, with runtime support 
Java, Node.js, Ruby, PHP
Applications using compiled code: C,
System debuggers
C++, Golang
Yes
System libraries: /lib/ *
Itrace(1)
Yes
System call interface
strace(1), perf(1)
Yes
Kernel: Scheduler,fle systems, TCP IP etc Ftrace, perf(1) for sampling
 Yes, in more detail
Hardware: CPU intemals, devices
perf, sar, /proc counters
Yes, direct or indirectd
4 BPF may not be able to directly instru
based on tracing of ker
---
## Page 45
Chapter 1 Introduction
Traditional tools can provide useful starting points for analysis, which you can explore in more
depth with BPF tracing tools. Chapter 3 summarizes basic performance analysis with system tols,
which can be your starting point.
1.6
DynamicInstrumentation:kprobesanduprobes
BPF tracing supports multiple sources of events to provide visibility of the entire software stack
(upen surup paeo ose) uogegusunnsu spueup s uopuatu [eads saauasap peq auo
the ability to insert instrumentation points into live software, in production. Dynamic instru-
mentation costs zero overhead when not in use, as software runs unmodified. It is often used by
BPF tools to instrument the start and end of kernel and application functions, from the many
tens of thousands of functions that are typically found running in a software stack. This provides
visibility so deep and comprehensive that it can feel like a superpower.
Dynamic instrumentation was first created in the 1990s [Hollingsworth 94], based on a technique
used by debugers to insert breakpoints at arbitrary instruction addresses. With dynamic instru-
mentation, the target software records information and then automatically continues execution
rather than passing control to an interactive debugger. Dynamic tracing tools (e.g., kerninst
[Tamches 99] were developed, and included tracing languages, but these tools remained obscure
and little used. In part because they involved considerable risk: Dynamic tracing requires modifi
cation of instructions in an address space, live, and any error could lead to immediate corruption
and process or kernel crashes.
Dynamic instrumentation was first developed for Linux in 2000 as DProbes by a team at IBM, but
the patch set was rejected.° Dynamic instrumentation for kernel functions (kprobes) was finally
added to Linux in 2004, originating from DProbes, although it was still not well known and was
still difficult to use.
Everything changed in 2005, when Sun Microsystems launched its own version of dynamic
tracing, DTrace, with its easy-to-use D language, and included it in the Solaris 10 operating
system. Solaris was known and trusted for production stability, and including DTrace as a default
package install helped prove that dynamic tracing could be made safe for use in production. It
was a turning point for the technology. I published many articles showing real-world use cases
with DTrace and developed and published many DTrace tools. Sun marketing also promoted
the technology, as did Sun sales; it was thought to be a compelling competitive feature. Sun
Educational Services included DTrace in the standard Solaris courses and taught dedicated DTrace
courses. All of these efforts caused dynamic instrumentation to move from an obscure technology
to a well-known and in-demand feature.
Linux added dynamic instrumentation for user-level functions in 2012, in the form of uprobes.
BPF tracing tools use both kprobes and uprobes for dynamic instrumentation of the full software
stack.
Kleen, which is referenced in the Linux source in Docu
entation/process/submiting-patches.rst [6].
---
## Page 46
1.7 Static Instrumentation: Tracepoints and USDT
To show how dynamic tracing is used, Table 1-2 provides examples of bpftrace probe specifiers
that use kprobes and uprobes. (bpftrace is covered in Chapter 5.)
Table 1-2 bpftrace kprobe and uprobe Examples
Probe
Description
kprobe:vfs_read
 Instrument the beginning of the kemel vfs_read() function
kretprobe:vfs_read
Instrument the returns° of the kermel vfs_read() function
uprobe:/bin/bash:readline
Instrument the beginning of the readline() function in /bin/bash
uretprobe:/bin/bash:readline
 Instrument the returns of the readline() function in /bin/bash
1.7StaticInstrumentation:Tracepoints and USDT
There is a downside to dynamic instrumentation: It instruments functions that can be renamed
or removed from one software version to the next. This is referred to as an irnferface stability ssae.
After upgrading the kernel or application software, you may suddenly find that your BPF tool no
longer works properly. Perhaps it prints an error about being unable to find functions to instru-
ment, or maybe it prints no output at all. Another isue is that compilers may inline functions as
a compiler optimization, making them unavailable for instrumentation via kprobes or uprobes.
One solution to both the stability and inlining problem is to switch to static instrumentation,
where stable event names are coded into the software and maintained by the developers. BPF
tracing supports tracepoints for kernel static instrumentation, and user-level statically defined
that these instrumentation points become a maintenance burden for the developers, so if any
tracing (USDT) for user-level static instrumentation. The downside of static instrumentation is
exist, they are usually limited in number.
These details are only important if you intend to develop your own BPF tools. If so, a recom
mended strategy is to try using static tracing first (using tracepoints and USDT) and then switch
to dynamic tracing (using kprobes and uprobes) when static tracing is unavailable.
Table 1-3 provides examples of bpftrace probe specifiers for static instrumentation using
tracepoints and USDT. The open(2) tracepoint mentioned in this table is used in Section 1.8.
Table 1-3 bpftrace Tracepoint and USDT Examples
Probe
Description
uadosajuass:sjeos(s:juodaoe.
 Instrument the open(2) syscall
usdt:/usr/sbin/mysqld:mysql:
Instrument the query__start probe from /usr/sbin/mysqld
query_start
6 A function has one beginning but can have multiple ends: It can call setum from different places. Retum probes
instrument allthe return points. (See Chapter 2 for an explanation of how this works.)
7 A workaround is function offset tracing, but as an interface it is even less stable than function entry tracing
---
## Page 47
10
Chapter 1 Introduction
1.8A First Look at bpftrace:Tracing open()
Let’s start by using bpftrace to trace the open(2) system call (syscall). There is a tracepoint fot
it (syscalls:sys_enter_open°), and I'll write a short bpftrace program at the command line: a
one-liner.
You aren’t expected to understand the code in the following one-liner yet; the bpftrace language
and install instructions are covered in Chapter 5. But you may be able to guess what the program
does without knowing the language as it is quite intuitive (an intuitive language is a sign of good
design). For now, just focus on the tool output.
str (args->filename)) : 1 *
Attaching 1 probe..
slack /zun/usex/1ooo/gds/Xauthority
slack /run/usez/100/gdn/Xauthority
slack /zun/usex/1o0/gds./Xauthority
slack /run/user/1000/gdm./Xauthority
C
+
The output shows the process name and the filename passed to the open(2) syscall: bpftrace is
tracing system-wide, so any application using open(2) will be seen. Each line of output summa
rizes one syscall and this is an example of a tool that produces per-event output. BPF tracing can
be used for more than just production server analysis. For example I'm running it on my laptop
as I write this book, and it's showing files that a Slack chat application is opening.
The BPF program was defined within the single forward quotes, and it was compiled and run as
soon as I pressed Enter to run the bpftrace command. bpftrace also activated the open(2) trace-
point. When I pressed Ctrl-C to stop the command, the open(2) tracepoint was deactivated, and
my small BPF program was removed. This is how on-demand instrumentation by BPF tracing
tools work: They are only activated and running for the lifetime of the command, which can be as
short as seconds.
The output generated was slower than I was expecting: I think I'm missing some open(2) syscall
events. The kernel supports a few variants of open, and I traced only one of them. I can use
bpftrace to list all the open tracepoints by using 1 and a wildcard:
tracepoint:sysca1ls:ays_enter_open_by_handile_at
tracepoint:ayacalls:ays_enter_open
tcacepoint:1sysca1ls1ays_enter_open.at
 These syscall racepoints require the Linux CONFIG_FTRACE_SYSCALLS build option to be enable
---
## Page 48
1.8 A First Look at bpftrace: Tracing open()
11
Ah, I think the openat(2) variant is used more often nowadays. Ill confirm with another bpftrace
one-liner:
 bpftrace -e 'tracepoint:syscalls:sys_enter_open*( [probe]= count() : }*
Attach.ing 3 pzobes...
[tracepoint:syscalls:sys_enter_open] : 5
e[tracepoint: sysca11s:sys_entex_openat] : 308
Again, the code in this one-liner will be explained in Chapter 5. For now, it's only important
to understand the output. It is now showing a count of these tracepoints rather than a line
per event. This confirms that the openat(2) syscall is called more often308 times while
tracingwhereas the open(2) syscall was called only five times. This summary is calculated
efficiently in the kernel by the BPF program.
I can add the second tracepoint to my one-liner to trace both open(2) and openat(2) at the same
time. However, the one-liner will start getting a little long and unwieldy at the command line,
and at that point, it would be better to save it to a script (an executable file), so that it can be
more easily edlited using a text editor. This has already been done for you: bpftrace ships with
opensnoop.bt, which traces both the start and end of each syscall and prints the output as
columns:
 opensnoop.bt
Attach.ing 3 probes..
Tracing open ayscalls.
.. Hit Ctrl-C to end.
PID
COMK
FD ERR PATH
2440
ssedduus
0 /proc/cpulnfo
2440
2570614
2zedduus
192// 0
257061a
0/1ib/x86_641inux-gnu/1ibselinux-5o.1
0/etc/ld.so,cache
25706
1s
3
0 /11b/x86_6411nux-gnu/1ibc,so. 6
25706
1s
0 / 1ib/x86_641inux-gnu/libpcre so.3
257061a
0 /11b/x86_64=11nux=gna/1ibd1,so .2
257061s
0 / 1ib/x86_641inux=gnu/1ibpthread-so. 0
257061s
0/pzoc/r11esystems
257061s
0 /usr/lib/locale/locale-archive
257061s
0
1744
snnpd
8
0/proc/net/dev
1744
 snnpd
-1
2 /sys/class/net/lo/device/vendoz
2440
2zedduus
4  0/proc/cpuinfo