The hypervisor exposes sixteen interrupt sources, which can allow the VMBus root driver to manage a 
maximum of 16 message queues. A synthetic message has the fixed size of 256 bytes and can transfer 
only 240 bytes (16 bytes are used as header). The caller of the HvCreatePort hypercall specifies which 
virtual processor and SINT to target.
To correctly receive messages, the WinHv driver allocates a synthetic interrupt message page 
(SIMP), which is then shared with the hypervisor. When a message is enqueued for a target partition, 
the hypervisor copies the message from its internal queue to the SIMP slot corresponding to the cor-
rect SINT. The VMBus root driver then creates a connection, which associates the port opened in the 
child VM to the parent, through the HvConnectPort hypercall. After the child has enabled the recep-
tion of synthetic interrupts in the correct SINT slot, the communication can start; the sender can post 
a message to the client by specifying a target Port ID and emitting the HvPostMessage hypercall. The 
hypervisor injects a synthetic interrupt to the target VP, which can read from the message page (SIMP) 
the content of the message.
304 
CHAPTER 9 Virtualization technologies
The hypervisor supports ports and connections of three types:
I 
Message ports Transmit 240-byte messages from and to a partition. A message port is as-
sociated with a single SINT in the parent and child partition. Messages will be delivered in order
through a single port message queue. This characteristic makes messages ideal for VMBus
channel setup and teardown (further details are provided in the “Virtualization stack” section
later in this chapter).
I 
Event ports Receive simple interrupts associated with a set of flags, set by the hypervisor
when the opposite endpoint makes a HvSignalEvent hypercall. This kind of port is normally
used as a synchronization mechanism. VMBus, for example, uses an event port to notify that a
message has been posted on the ring buffer described by a particular channel. When the event
interrupt is delivered to the target partition, the receiver knows exactly to which channel the
interrupt is targeted thanks to the flag associated with the event.
I 
Monitor ports An optimization to the Event port. Causing a VMEXIT and a VM context switch
for every single HvSignalEvent hypercall is an expensive operation. Monitor ports are set up by
allocating a shared page (between the hypervisor and the partition) that contains a data struc-
ture indicating which event port is associated with a particular monitored notification flag (a bit
in the page). In that way, when the source partition wants to send a synchronization interrupt, it
can just set the corresponding flag in the shared page. Sooner or later the hypervisor will notice
the bit set in the shared page and will trigger an interrupt to the event port.
The Windows hypervisor platform API and EXO partitions
Windows increasingly uses Hyper-V’s hypervisor for providing functionality not only related to running 
traditional VMs. In particular, as we will discuss discuss in the second part of this chapter, VSM, an im-
portant security component of modern Windows versions, leverages the hypervisor to enforce a higher 
level of isolation for features that provide critical system services or handle secrets such as passwords. 
Enabling these features requires that the hypervisor is running by default on a machine.
External virtualization products, like VMware, Qemu, VirtualBox, Android Emulator, and many oth-
ers use the virtualization extensions provided by the hardware to build their own hypervisors, which is 
needed for allowing them to correctly run. This is clearly not compatible with Hyper-V, which launches 
its hypervisor before the Windows kernel starts up in the root partition (the Windows hypervisor is a 
native, or bare-metal hypervisor). 
As for Hyper-V, external virtualization solutions are also composed of a hypervisor, which provides 
generic low-level abstractions for the processor’s execution and memory management of the VM, and a 
virtualization stack, which refers to the components of the virtualization solution that provide the emu-
lated environment for the VM (like its motherboard, firmware, storage controllers, devices, and so on).
The Windows Hypervisor Platform API, which is documented at https://docs.microsoft.com/en-us 
/virtualization/api/, has the main goal to enable running third-party virtualization solutions on the 
Windows hypervisor. Specifically, a third-party virtualization product should be able to create, delete, 
start, and stop VMs with characteristics (firmware, emulated devices, storage controllers) defined by its 
CHAPTER 9 Virtualization technologies
305
own virtualization stack. The third-party virtualization stack, with its management interfaces, continues 
to run on Windows in the root partition, which allows for an unchanged use of its VMs by their client.
As shown in Figure 9-19, all the Windows hypervisor platform’s APIs run in user mode and are 
implemented on the top of the VID and WinHvr driver in two libraries: WinHvPlatform.dll and 
WinHvEmulation.dll (the latter implements the instruction emulator for MMIO). 
Virtualization Stack Process
WinHvr
Hypervisor Instruction
Emulator
Windows Hypervisor
Platform API
CreateThread
WinHv
DispatchVp
Intercept
Routine
WinHvMap
GpaPages
VirtualAlloc
WHvRun VirtualProcessor
VID driver
MicroVm
WHvMapGpaRange
User
Kernel
MapViewOfFile
Root Partition
Guest Partition
Guest VPs
GPA Space
Hypervisor
FIGURE 9-19 The Windows hypervisor platform API architecture.
A user mode application that wants to create a VM and its relative virtual processors usually should 
do the following:
1.
Create the partition in the VID library (Vid.dll) with the WHvCreatePartition API.
2. 
Configure various internal partition’s properties—like its virtual processor count, the APIC emula-
tion mode, the kind of requested VMEXITs, and so on—using the WHvSetPartitionProperty API.
3.
Create the partition in the VID driver and the hypervisor using the WHvSetupPartition API. (This
kind of partition in the hypervisor is called an EXO partition, as described shortly.) The API also
creates the partition’s virtual processors, which are created in a suspended state.
4.
Create the corresponding virtual processor(s) in the VID library through the WHvCreateVirtual-
Processor API. This step is important because the API sets up and maps a message buffer into
the user mode application, which is used for asynchronous communication with the hypervisor
and the thread running the virtual CPUs.
5.
Allocate the address space of the partition by reserving a big range of virtual memory with the
classic VirtualAlloc function (read more details in Chapter 5 of Part 1) and map it in the hy-
pervisor through the WHvMapGpaRange API. A fine-grained protection of the guest physical
memory can be specified when allocating guest physical memory in the guest virtual address
space by committing different ranges of the reserved virtual memory.
306 
CHAPTER 9 Virtualization technologies
6.
Create the page-tables and copy the initial firmware code in the committed memory.
7. 
Set the initial VP’s registers content using the WHvSetVirtualProcessorRegisters API.
8.
Run the virtual processor by calling the WHvRunVirtualProcessor blocking API. The function
returns only when the guest code executes an operation that requires handling in the virtual-
ization stack (a VMEXIT in the hypervisor has been explicitly required to be managed by the
third-party virtualization stack) or because of an external request (like the destroying of the
virtual processor, for example).
The Windows hypervisor platform APIs are usually able to call services in the hypervisor by sending 
different IOCTLs to the \Device\VidExo device object, which is created by the VID driver at initialization 
time, only if the HKLM\System\CurrentControlSet\Services\Vid\Parameters\ExoDeviceEnabled registry 
value is set to 1. Otherwise, the system does not enable any support for the hypervisor APIs. 
Some performance-sensitive hypervisor platform APIs (a good example is provided by WHvRun 
VirtualProcessor) can instead call directly into the hypervisor from user mode thanks to the Doorbell 
page, which is a special invalid guest physical page, that, when accessed, always causes a VMEXIT. The 
Windows hypervisor platform API obtains the address of the doorbell page from the VID driver. It 
writes to the doorbell page every time it emits a hypercall from user mode. The fault is identified and 
treated differently by the hypervisor thanks to the doorbell page’s physical address, which is marked 
as “special” in the SLAT page table. The hypervisor reads the hypercall’s code and parameters from the 
VP’s registers as per normal hypercalls, and ultimately transfers the execution to the hypercall’s handler 
routine. When the latter finishes its execution, the hypervisor finally performs a VMENTRY, landing on 
the instruction following the faulty one. This saves a lot of clock cycles to the thread backing the guest 
VP, which no longer has a need to enter the kernel for emitting a hypercall. Furthermore, the VMCALL 
and similar opcodes always require kernel privileges to be executed.
The virtual processors of the new third-party VM are dispatched using the root scheduler. In case 
the root scheduler is disabled, any function of the hypervisor platform API can’t run. The created parti-
tion in the hypervisor is an EXO partition. EXO partitions are minimal partitions that don’t include any 
synthetic functionality and have certain characteristics ideal for creating third-party VMs:
I 
They are always VA-backed types. (More details about VA-backed or micro VMs are provided
later in the “Virtualization stack” section.) The partition’s memory-hosting process is the user
mode application, which created the VM, and not a new instance of the VMMEM process.
I 
They do not have any partition’s privilege or support any VTL (virtual trust level) other than 0.
All of a classical partition’s privileges refer to synthetic functionality, which is usually exposed
by the hypervisor to the Hyper-V virtualization stack. EXO partitions are used for third-party
virtualization stacks. They do not need the functionality brought by any of the classical parti-
tion’s privilege.
I 
They manually manage timing. The hypervisor does not provide any virtual clock interrupt
source for EXO partition. The third-party virtualization stack must take over the responsibil-
ity of providing this. This means that every attempt to read the virtual processor’s time-stamp
counter will cause a VMEXIT in the hypervisor, which will route the intercept to the user mode
thread that runs the VP.
CHAPTER 9 Virtualization technologies
307
Note EXO partitions include other minor differences compared to classical hypervisor parti-
tions. For the sake of the discussion, however, those minor differences are irrelevant, so they 
are not mentioned in this book.
Nested virtualization
Large servers and cloud providers sometimes need to be able to run containers or additional virtual 
machines inside a guest partition. Figure 9-20 describes this scenario: The hypervisor that runs on 
the top of the bare-metal hardware, identified as the L0 hypervisor (L0 stands for Level 0), uses the 
virtualization extensions provided by the hardware to create a guest VM. Furthermore, the L0 hypervi-
sor emulates the processor’s virtualization extensions and exposes them to the guest VM (the ability to 
expose virtualization extensions is called nested virtualization). The guest VM can decide to run another 
instance of the hypervisor (which, in this case, is identified as L1 hypervisor, where L1 stands for Level 1), 
by using the emulated virtualization extensions exposed by the L0 hypervisor. The L1 hypervisor creates 
the nested root partition and starts the L2 root operating system in it. In the same way, the L2 root can 
orchestrate with the L1 hypervisor to launch a nested guest VM. The final guest VM in this configuration 
takes the name of L2 guest.
Windows Root OS
Windows Root OS
Guest OS
Level 1
Level 2
Level 0
Hyper-V Hypervisor
Hyper-V Hypervisor
Hardware Layer: Intel Processor w/VT-x
VT-x Extensions
VT-x Extensions
CPU
vCPU
FIGURE 9-20 Nested virtualization scheme.
Nested virtualization is a software construction: the hypervisor must be able to emulate and 
manage virtualization extensions. Each virtualization instruction, while executed by the L1 guest VM, 
causes a VMEXIT to the L0 hypervisor, which, through its emulator, can reconstruct the instruction and 
perform the needed work to emulate it. At the time of this writing, only Intel and AMD hardware is 
supported. The nested virtualization capability should be explicitly enabled for the L1 virtual machine; 
308 
CHAPTER 9 Virtualization technologies
otherwise, the L0 hypervisor injects a general protection exception in the VM in case a virtualization 
instruction is executed by the guest operating system.
On Intel hardware, Hyper-V allows nested virtualization to work thanks to two main concepts:
I 
Emulation of the VT-x virtualization extensions
I 
Nested address translation
As discussed previously in this section, for Intel hardware, the basic data structure that describes 
a virtual machine is the virtual machine control structure (VMCS). Other than the standard physical 
VMCS representing the L1 VM, when the L0 hypervisor creates a VP belonging to a partition that sup-
ports nested virtualization, it allocates some nested VMCS data structures (not to be confused with a 
virtual VMCS, which is a different concept). The nested VMCS is a software descriptor that contains all 
the information needed by the L0 hypervisor to start and run a nested VP for a L2 partition. As briefly 
introduced in the “Hypervisor startup” section, when the L1 hypervisor boots, it detects whether it’s 
running in a virtualized environment and, if so, enables various nested enlightenments, like the enlight-
ened VMCS or the direct virtual flush (discussed later in this section). 
As shown in Figure 9-21, for each nested VMCS, the L0 hypervisor also allocates a Virtual VMCS and a 
hardware physical VMCS, two similar data structures representing a VP running the L2 virtual machine. 
The virtual VMCS is important because it has the key role in maintaining the nested virtualized data. The 
physical VMCS instead is loaded by the L0 hypervisor when the L2 virtual machine is started; this happens 
when the L0 hypervisor intercepts a VMLAUNCH instruction executed by the L1 hypervisor.
L 1
L0
Nested VMCS Cache
VP 2
*-Represents a VP in the L2 VM
-Represents a VP in the L1 Hypervisor
Nested VMCS
Virtual
VMCS*
Physical
VMCS*
Physical
VMCS
Nested VMCS
Virtual
VMCS*
Physical
VMCS*
Nested VMCS
Virtual
VMCS*
Physical
VMCS*
L1 VP 1
…
L1 VP 0
L1 VP 2
FIGURE 9-21 A L0 hypervisor running a L2 VM by virtual processor 2.
In the sample picture, the L0 hypervisor has scheduled the VP 2 for running a L2 VM managed by 
the L1 hypervisor (through the nested virtual processor 1). The L1 hypervisor can operate only on virtu-
alization data replicated in the virtual VMCS. 
CHAPTER 9 Virtualization technologies
309
Emulation of the VT-x virtualization extensions
On Intel hardware, the L0 hypervisor supports both enlightened and nonenlightened L1 hypervisors. 
The only official supported configuration is Hyper-V running on the top of Hyper-V, though.
In a nonenlightened hypervisor, all the VT-x instructions executed in the L1 guest causes a VMEXIT. 
After the L1 hypervisor has allocated the guest physical VMCS for describing the new L2 VM, it usually 
marks it as active (through the VMPTRLD instruction on Intel hardware). The L0 hypervisor intercepts 
the operation and associates an allocated nested VMCS with the guest physical VMCS specified by the 
L1 hypervisor. Furthermore, it fills the initial values for the virtual VMCS and sets the nested VMCS as 
active for the current VP. (It does not switch the physical VMCS though; the execution context should 
remain the L1 hypervisor.) Each subsequent read or write to the physical VMCS performed by the L1 
hypervisor is always intercepted by the L0 hypervisor and redirected to the virtual VMCS (refer to 
Figure 9-21). 
When the L1 hypervisor launches the VM (performing an operation called VMENTRY), it executes a 
specific hardware instruction (VMLAUNCH on Intel hardware), which is intercepted by the L0 hypervi-
sor. For nonenlightened scenarios, the L0 hypervisor copies all the guest fields of the virtual VMCS to 
another physical VMCS representing the L2 VM, writes the host fields by pointing them to L0 hypervi-
sor’s entry points, and sets it as active (by using the hardware VMPTRLD instruction on Intel platforms). 
In case the L1 hypervisor uses the second level address translation (EPT for Intel hardware), the L0 
hypervisor then shadows the currently active L1 extended page tables (see the following section for 
more details). Finally, it performs the actual VMENTRY by executing the specific hardware instruction. 
As a result, the hardware executes the L2 VM’s code.
While executing the L2 VM, each operation that causes a VMEXIT switches the execution con-
text back to the L0 hypervisor (instead of the L1). As a response, the L0 hypervisor performs another 
VMENTRY on the original physical VMCS representing the L1 hypervisor context, injecting a synthetic 
VMEXIT event. The L1 hypervisor restarts the execution and handles the intercepted event as for regu-
lar non-nested VMEXITs. When the L1 completes the internal handling of the synthetic VMEXIT event, it 
executes a VMRESUME operation, which will be intercepted again by the L0 hypervisor and managed in 
a similar way of the initial VMENTRY operation described earlier.
Producing a VMEXIT each time the L1 hypervisor executes a virtualization instruction is an expensive 
operation, which could definitively contribute in the general slowdown of the L2 VM. For overcoming 
this problem, the Hyper-V hypervisor supports the enlightened VMCS, an optimization that, when en-
abled, allows the L1 hypervisor to load, read, and write virtualization data from a memory page shared 
between the L1 and L0 hypervisor (instead of a physical VMCS). The shared page is called enlightened 
VMCS. When the L1 hypervisor manipulates the virtualization data belonging to a L2 VM, instead of 
using hardware instructions, which cause a VMEXIT into the L0 hypervisor, it directly reads and writes 
from the enlightened VMCS. This significantly improves the performance of the L2 VM.
In enlightened scenarios, the L0 hypervisor intercepts only VMENTRY and VMEXIT operations (and 
some others that are not relevant for this discussion). The L0 hypervisor manages VMENTRY in a similar 
way to the nonenlightened scenario, but, before doing anything described previously, it copies the 
virtualization data located in the shared enlightened VMCS memory page to the virtual VMCS repre-
senting the L2 VM.
310 
CHAPTER 9 Virtualization technologies
Note It is worth mentioning that for nonenlightened scenarios, the L0 hypervisor supports 
another technique for preventing VMEXITs while managing nested virtualization data, called 