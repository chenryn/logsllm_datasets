channel detection uses only 64KB of memory and 24KB of
storage. All these models comfortably ﬁt within the control
plane hardware resources, which has 32GB available RAM.
Additionally, the ﬂow classiﬁcation step is very fast in all cases.
For covert channel detection, once the ﬂow markers have been
collected from the data plane, the median of the time it takes
for the classiﬁer to output a label for a sample ﬂow ranges
approximately from 100 to 200 microseconds on the switch’s
9
Figure 7. Accuracy results for website ﬁngerprinting when using quantized
packet length distributions.
Figure 6. Accuracy, FPR, and FNR for multimedia protocol tunneling
detection when using quantized packet length distributions.
Table III.
FLOW MARKER SIZE FOR DIFFERENT QUANTIZATION LEVELS.
Bins and Memory
len(1 bin) = 2 Bytes
Number of Bins
Memory per Flow(B)
0
1500
3000
2
375
750
Quantization Level (QL)
6
3
4
5
188
376
94
188
47
94
24
48
7
12
24
8
6
12
Intel Broadwell 8-core general-purpose CPU operating at 2
GHz. These results indicate that ﬂow classiﬁcation can be
efﬁciently conducted on the switch control plane.
Next, we present a set of micro-benchmarks which allow
us to assess the beneﬁts of our ﬂow marker generation scheme.
We will see that ﬂow marker size (hence memory efﬁciency)
tends to be more sensitive than classiﬁcation accuracy to small
variations in the quantization. The trend is the inverse for
truncation, where accuracy is more sensitive to small variations
than ﬂow marker size. Our optimizer helps to ﬁnd sweet-spot
setups on the Pareto curve (as explained in Section VII-H).
D. Effects of Quantization
To study the effects of FlowLens’s compression schemes,
we ﬁrst focus on the generation of ﬂow markers for packet
length distributions and start by analyzing the trade-offs of
quantization. We present our main ﬁndings:
1. Multimedia covert channels can be detected with up to
92% accuracy using 188-byte ﬂow markers. We leverage
XGBoost to classify covert channels [7]. Figure 6 shows how
the absolute values obtained for the accuracy, FPR, and FNR
of the classiﬁer vary when identifying Facet and DeltaShaper
covert channels for different quantization levels (QL). For
instance, for quantization level QL=4 FlowLens can correctly
identify Facet and DeltaShaper ﬂows with less than 5% and
1% decrease in accuracy, respectively. Table III shows that, for
QL=4, a ﬂow marker can be represented in 94 bins instead
of a full distribution composed of 1500 bins, amounting to
an order of magnitude memory savings. While a single ﬂow
marker is then represented using 188B instead of 3000B,
DeltaShaper classiﬁcation scores are maintained with respect
to those obtained when using full information (see Figure 6).
2. Accuracy of website ﬁngerprinting is maintained when
compressing ﬂow markers by two orders of magnitude. For
assessing the quality of FlowLens on website ﬁngerprinting, we
use the Multinomial Na¨ıve-Bayes classiﬁer [29]. We reproduced
the multiclass closed-world website ﬁngerprinting task for
Figure 8. Accuracy, FPR, and FNR for covert channel detection with an
increasing number of features for quantization level QL=4.
different quantization levels. Figure 7 shows that FlowLens
is able to maintain the same classiﬁcation accuracy up to a
quantization level QL=3. Providing that classiﬁcation accuracy
can be relaxed in favor of memory savings, quantization can
be further increased to QL=6, while still achieving over 90%
accuracy and reducing a ﬂow marker’s memory footprint by
two orders of magnitude.
3. Very coarse-grained ﬂow markers are unsuitable for
performing trafﬁc differentiation. Figure 7 shows that ﬂow
markers can only be compressed to a given factor before causing
a steep decrease in the quality of the models’ predictions.
For instance, in Figure 6, it is possible to observe that for
QL=7 the accuracy of the classiﬁer is already over 20% and
10% away from the result obtained with full information for
Facet and DeltaShaper, respectively. Thus, it is imperative
to ﬁnd the correct balance between memory savings and
accuracy. FlowLens balances this trade-off, for different use
cases, through a parameterization during the proﬁling phase.
E. Effects of Truncation
The second mechanism to generate compact ﬂow markers
is that of truncating the ﬂow marker to a subset of bins which
make up for the most relevant features leveraged by the classiﬁer.
This is illustrated next, as we highlight our main ﬁndings after
applying tailored truncation in different use cases.
1. Accurate detection of covert channels can be achieved
using a ﬂow marker of just 20 bytes. We elaborated a
tailored truncation approach based on the importance of features
computed by XGBoost. Figure 8 depicts the results obtained
when performing quantization with QL=4 and truncating to the
top-N most important features. The accuracy, FPR, and FNR
rate of the classiﬁer are practically identical when using the
10
023456780.00.20.40.60.81Facet0.960.960.950.920.880.850.740.720.040.040.040.070.100.120.170.310.040.040.050.080.150.180.360.25AccFPRFNR02345678Quantization Level (QL)0.00.20.40.60.81DeltaShaper0.870.890.840.860.880.810.770.710.170.170.190.160.130.180.250.270.080.050.120.110.110.190.210.32AccFPRFNR02345678Quantization Level (QL)0.00.20.40.60.810.970.970.970.960.950.910.760.4610203040500.20.40.60.81Facet0.930.930.920.920.930.070.070.070.070.070.070.080.090.080.08AccFPRFNR1020304050Number of selected features0.20.40.60.81DeltaShaper0.850.840.850.860.880.170.210.170.170.150.130.110.130.100.10AccFPRFNRFigure 9.
FPR and FNR for www.amazon.com at different quantization
levels (QL). FNR shows the probability of identifying www.amazon.com as
some other website. FPR shows the probability of some other website being
mistakenly classiﬁed as www.amazon.com. Truncation is applied at each QL.
Table IV.
NUMBER OF BINS USED IN www.amazon.com TRUNCATION.
Bins and Memory
len(1 bin) = 2 Bytes
Total Number of Bins
Bins After Truncation
0
1500
159
2
375
159
Quantization Level (QL)
6
3
4
5
188
156
94
87
47
46
24
23
7
12
12
8
6
6
top-10 and top-50 features to classify ﬂows (e.g., a difference
of only 1% in FNR for Facet ﬂows), and very similar to the
results obtained when using full information (refer to QL=4 in
Figure 6). Thus, truncation can not only maintain high accuracy,
but further reduce the ﬂow marker footprint from 188B (QL=4)
to just 20B (QL=4, top-N=10).
2. 20-byte ﬂow markers enable tracking 150× more ﬂows.
Covert ﬂow markers can be reduced to just 20B using truncation.
This corresponds to a 150× space-saving when representing a
ﬂow (from 3000B to 20B). The space freed by compressing a
single ﬂow represents an increase in FlowLens’s measurement
capacity by two orders of magnitude.
3. Fingerprinting accesses to a website yields good results
even when feature ranking is unavailable. The truncation
method employed for covert channel detection is only applicable
when considering classiﬁers able to output feature importance.
To overcome the fact that Herrmann et al.’s [29] classiﬁer is
unable to output a rank of feature importance, we perform
manual bin selection aimed at identifying a single website,
e.g., www.amazon.com. Essentially, we ﬁrst take a collection of
access traces performed over a period of time to that particular
website. Then, we simply discard the bins that correspond to
packet lengths which have had zero counts of the sampled ﬂows.
Based on this selection, we then train our classiﬁer accordingly.
We can see in Figure 9 that the results obtained using this
approach remain competitive. For instance, with quantization
level QL=4, ﬂows can be correctly identiﬁed with a 0.016%
FPR and 9.333% FNR. As shown in Table IV, this ﬂow marker
footprint is not as small as with covert channel detection. Yet, it
is practical to ﬁngerprint website accesses with QL=4, yielding
ﬂow markers with a compression ratio of 1500:87, i.e., 17.2×.
F. Measuring Inter-Packet Timing
In this section, we concentrate on the ability of FlowLens
to perform tasks that require both the inspection of packets’
inter-arrival (IPT) and length (PL) distributions. To this end, we
evaluate FlowLens in detecting P2P botnet chatter. Since the
network trafﬁc produced by bots tends to be stealthy and spread
11
Figure 10. Precision, recall, and FPR for malicious P2P trafﬁc.
across time, packets sent in bot conversations are expected to
have a higher IPT than those of legitimate P2P conversations.
A conversation consists of the set of ﬂows between any two
hosts within a given time window, called ﬂowgap. We resort to
the Random Forest classiﬁer originally employed by Narang et
al. in PeerShark [53], and follow their recommendation to set
ﬂowgap to 3600s. Since the largest ﬂowgap is set to 3600s, we
vary the quantization of inter-arrival time down to a minimum
of 4 bins (QLIP T = 10).
Figure 10 depicts the precision, recall, and FPR obtained
by the classiﬁer when identifying botnet chatter for different
QL applied to both PL and IPT distributions. While QLP L ≤ 4
slightly degrades precision and recall, we observe a sharp
drop in both metrics when QLP L > 4. The FPR, however,
is not signiﬁcantly affected by increasing quantization levels.
Our experiments also reveal that precision and recall in the
identiﬁcation of legitimate P2P trafﬁc are largely unaffected by
the effect of quantization, whereas FPR takes a sharp increase
for QLP L ≥ 6 (20% at QLP L = 6 up to 50% at QLP L = 8).
This ﬁgure also shows that it is possible to accurately
identify botnet trafﬁc with compact ﬂow markers. For instance,
(cid:104)QLP L = 4, QLIP T = 6(cid:105) achieves a recall of 0.96, only 3%
worse when compared to the result obtained when using full
information (0.99). This accounts for a memory saving of 16×
when storing a ﬂow’s packet length distribution, as well as
occupying just 57 buckets×2B = 114B to keep an inter-packet
timing distribution. These results suggest that FlowLens can
offer different space-saving/accuracy trade-offs.
G. Performance of Automatic Proﬁling
To evaluate FlowLens’s automatic proﬁling mechanism, we
explore the parameter search space for each use case. For Facet
and DeltaShaper, the search space includes the quantization
and truncation parameters studied above (48 conﬁgurations).
For website ﬁngerprinting, the search space corresponds to 8
quantization conﬁgurations. For botnet detection, we consider
40 possible conﬁgurations based on packet length and IPT
quantization, as we refrain from considering those whose
QLIP T = 0. We conﬁgure the optimizer to explore i=10
conﬁgurations for covert channel and botnet detection, and
i=4 for website ﬁngerprinting. For simplicity, we use no initial
sampling for bootstrapping the optimizer, but techniques like
Latin Hypercube sampling [75] may be also plugged in.
Fully automatic mode: Table V depicts the results obtained
by our automatic proﬁler to choose an FMA conﬁguration. In
all cases, the proﬁler chooses a conﬁguration that, albeit not
the best accuracy wise, still provides a competitive accuracy
023456780.00.00020.00040.00060.00080.001FPR023456780.00.20.40.60.81FNR0.00.20.40.60.81.0Quantization Level (QL)0.00.20.40.60.81.002345678QLPL0.00.20.40.60.81Precision02345678QLPL0.00.20.40.60.81Recall02345678QLPL0.00.20.40.60.81FPR0.040.020.000.020.040.040.020.000.020.04QLIPT=0QLIPT=2QLIPT=4QLIPT=6QLIPT=8QLIPT=10Table V.
RESULTS OF THE PROFILING PROCEDURE, INCLUDING THE CONFIGURATION OUTPUT BY THE OPTIMIZER AND THE TOP-3 EXPLORED
CONFIGURATIONS (LISTED BY DECREASING ACCURACY, EXCEPT FOR THE CASE OF BOTNETS WHICH CORRESPONDS TO MALICIOUS TRAFFIC RECALL).
Conﬁg. Rank
#1
#2
#3
Output
Facet (i=10)
(cid:104)QL=2, Top-N=all(cid:105) = 0.960
(cid:104)QL=3, Top-N=50(cid:105) = 0.951
(cid:104)QL=0, Top-N=30(cid:105) = 0.947
(cid:104)QL=3, Top-N=10(cid:105) = 0.944
DeltaShaper (i=10) Website Fingerprinting (i=4)
(cid:104)QL=0(cid:105) = 0.970
(cid:104)QL=4(cid:105) = 0.965
(cid:104)QL=5(cid:105) = 0.948
(cid:104)QL=4(cid:105) = 0.965
(cid:104)QL=5, Top-N=all(cid:105) = 0.880
(cid:104)QL=0, Top-N=all(cid:105) = 0.873