is always larger than a static reservation system, which keeps links
under-utilized.
CPU: Since ElasticSwitch adds overhead in both kernel and
userspace, and since tested applications also consume CPU re-
sources, we only evaluate the difference in CPU usage between
ElasticSwitch and NoProtection in the same setup. (It is hard for us
to estimate the overhead of an Oktopus-like Reservation solution
since it could be implemented in several ways.)
360No Protection
Oktopus−like Res.
ElasticSwitch
)
s
m
(
e
m
i
t
n
o
i
t
e
p
m
o
C
l
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0
1
5
20 
23
(Tight Guar.)
# VM−to−VM flows in background
Figure 11: Completion time of HTTP requests vs. background
trafﬁc
)
%
(
d
a
e
h
r
e
v
o
U
P
C
e
r
o
c
e
n
O
100
80
60
40
20
0
0
50
15ms RA 60ms GP
30ms RA 100ms GP
100
150
# active VM−to−VM flows
200
250
300
Figure 12: CPU Overhead vs. # Active Flows
Fig. 12 shows the overhead of ElasticSwitch compared to No-
Protection, in terms of the capacity of a single CPU core (our im-
plementation is multi-threaded), as we vary the number of active
VM-to-VM ﬂows. These results were measured in the context of
the many-to-one experiment where the two VMs X and Y are lo-
cated on the same server (e.g., the results presented in Fig. 10(a)).
Because a part of the overhead depends on the frequency of apply-
ing GP and RA, we plot two different sets of periods. Our ﬁne-grain
proﬁling indicates that most of the CPU cycles are spent in reading
and setting the rate-limiters; after a certain number of limiters, this
overhead seems to start to increase nonlinearly.
Fig. 12 shows that the additional CPU overhead of ElasticSwitch
can be handled in typical cases with one CPU core (we note also
that our testbed uses older generation CPUs). We believe this over-
head can be signiﬁcantly improved in the future, for example, by
using an improved rate-limiting library.
Control Trafﬁc: ElasticSwitch uses two types of control packets:
remote guarantees and congestion feedback, both having the min-
imum Ethernet packet size (64B). For a GP period of 60ms (used
in our prototype), ElasticSwitch sends roughly 17 control packets
per second for each VM-to-VM active ﬂow; e.g., if there are 100
communicating VM-to-VM pairs on one server, the trafﬁc overhead
for sending/receiving remote guarantees is ∼850Kbps. In the cur-
rent implementation, ElasticSwitch sends one congestion feedback
control packet for each congestion event, limited to at most one
message per 0.5ms. However, since ElasticSwitch detects only the
packets effectively lost inside the network (and not in the queue of
the sending host), and since RA is not aggressive and keeps buffers
free, this trafﬁc is very small—on the order of few Kbps.
7.4 ECN
Our limited experience on the single ECN-capable switch setup
suggests that: (i) ECN improves the results both for enforcing guar-
antees and for being work conserving, and (ii) our improvements to
Seawall’s algorithm are not necessary on ECN-capable networks.
In brief, the many-to-one experiment results were ideal in terms of
providing guarantees, showing very little variance (we could test up
to 100 senders). For the experiment of borrowing bandwidth from a
bounded ﬂow (Fig. 2 and Fig. 10(c)), ECN would improve results,
being similar to the more aggressive ElasticSwitch in Fig. 10(c).
For space constraints, we do not plot these results.
8. RELATED WORK
Oktopus [4] provides predictable bandwidth guarantees for ten-
ants in cloud datacenters. Oktopus does both placement of VMs
and enforcement of guarantees. However, the enforcement algo-
rithm is not work-conserving and hence Oktopus does not utilize
the network efﬁciently. Further, the approach is centralized and
thus has limited scalability.
SecondNet [11] provides a VM-to-VM bandwidth guarantee
model in addition to the hose model. In SecondNet, a central con-
troller determines the rate and the path for each VM-to-VM ﬂow
and communicates those to the end hosts. As with Oktopus, Sec-
ondNet’s guarantee enforcement is not work-conserving and has
limited scalability, because of its centralized controller. Further, it
requires switches with MPLS support.
Gatekeeper [20] and EyeQ [12] also use the hose model, are
fully implemented in hypervisors and are work conserving. They
are also simpler than ElasticSwitch. However, these approaches
assume that the core of the network is congestion-free: they can
provide guarantees only in that case. However, research shows
that in current datacenters congestion occurs almost entirely in the
core [6, 24].5 EyeQ also requires switches with ECN support, to
detect congestion at the Top-of-the-Rack switches.
FairCloud [18] analyzes the tradeoffs in allocating cloud net-
works and proposes a set of desirable properties and allocation
policies. One of FairCloud’s bandwidth-sharing proposals, PS-
P, provides bandwidth guarantees and is work-conserving. How-
ever, FairCloud’s solution requires expensive hardware support in
switches (essentially one queue for each VM) and works only for
tree topologies.
Seawall [22] uses a hypervisor-based mechanism that ensures
per-source fair sharing of congested links. NetShare [13] ensures
per-tenant fair sharing of congested links. However, neither ap-
proach provides any bandwidth guarantees (i.e., the share of one
VM can be arbitrarily reduced [18]). For this reason, these sharing
models cannot be used to predict upper bounds on the runtime of a
cloud application.
Proteus [27] proposes a variant of the hose model, Temporally-
Interleaved Virtual Cluster (TIVC), where the bandwidth require-
ments on each virtual link are speciﬁed as a time-varying function,
instead of the constant in the hose model. Proteus proﬁles MapRe-
duce applications to derive TIVC models. Proteus uses these mod-
els for placement and enforcement of bandwidth guarantees. Thus
Proteus is suited for a limited set of applications; it is not workload-
agnostic, as is necessary for cloud computing.
Hadrian [5] is a recent proposal focusing on providing bandwidth
guarantees for inter-tenant communication. ElasticSwitch can use
Hadrian’s pricing and reservation schemes for inter-tenant commu-
nication. The mechanism used by Hadrian to enforce bandwidth
guarantees has some resemblance to ElasticSwitch at a high level.
However, Hadrian’s approach requires dedicated switch support for
setting ﬂow rates in data packets, unavailable in today’s hardware.
5The core is not guaranteed to be congestion-free even for fully-
provisioned networks. Since these networks are not cliques, ex-
treme many-to-one trafﬁc patterns can congest links inside the core.
3619. DISCUSSION AND FUTURE WORK
ElasticSwitch on Other Topologies: ElasticSwitch can be used on
any single-path routing topology, as long as the admission control
criterion is respected. ElasticSwitch can also be applied to multi-
path topologies, where load balancing is uniform across paths, such
as fat-trees [1] or VL2 [10]. In this case, the set of links on which
trafﬁc is load balanced can be seen as a single generalized link,
e.g., for fat-trees, a generalized link represents the set of parallel
links at a given level (i.e., that can be used by one VM towards the
root nodes). We are working towards deploying a load balancing
solution and testing ElasticSwitch on top of it.
For multi-path topologies with non-uniform load balancing
across paths, such as Jellyﬁsh [23], ElasticSwitch could be ex-
tended to use three control layers instead of two: guarantee par-
titioning, path partitioning and rate allocation. Path partitioning
would divide the guarantee between two VMs among multiple
paths. The other layers would operate unmodiﬁed.
Prioritizing Control Messages: Since it is applied periodically,
ElasticSwitch is resilient to control message losses (we have tested
this case). However, the efﬁciency of ElasticSwitch is reduced, par-
ticularly at high loss rates. Ideally, control messages should not be
affected by the data packets, and one could ensure this by prioritiz-
ing control packets, e.g., using dedicated switch hardware queues.
(In our current implementation, ARP packets supporting the con-
trol trafﬁc should also be prioritized or ARP entries pre-populated;
however, we believe this can be avoided in future versions.)
Limitations: Endpoint-only solutions for providing bandwidth
guarantees are limited by the fact that they use shared queues in-
stead of dedicated queues. For instance, a malicious tenant can cre-
ate bursts of trafﬁc by synchronizing packets from multiple VMs.
Synchronization creates a temporary higher loss rate for the other
tenants, which can negatively affect their TCP ﬂows. We leave ad-
dressing these concerns to future work.
Beyond the Hose Model: The hose model has the advantage of
simplicity.
It is ideal for MapReduce-like applications with all-
to-all communication patterns, but it might be inefﬁcient for ap-
plications with localized communication between different compo-
nents [4, 14]. ElasticSwitch can be used as a building block for
providing more complex abstractions based on hose models, such
as the TAG model [14], or a model resembling the VOC model [4].
The high level idea is that each VM can be part of multiple hose
models; e.g., we can have a hose model between VMs from two ap-
plication components, and an additional hose-model between VMs
within a single component.
In this case, GP must identify the
hose(s) to which a VM-to-VM ﬂow belongs. ElasticSwitch does
not support hierarchical models that aggregate multiple VM hoses
into one [4, 5], as this would require coordination across VMs.
10. CONCLUSION
We have presented ElasticSwitch, a practical approach for im-
plementing work-conserving minimum bandwidth guarantees in
cloud computing infrastructures. ElasticSwitch can be fully im-
plemented in hypervisors, which operate independently without
the use of a centralized controller.
It works with commodity
switches and topologies with different over-subscriptions. Elastic-
Switch provides minimum bandwidth guarantees with hose model
abstractions—each hose bandwidth guarantee is transformed into
pairwise VM-to-VM rate-limits, and work conservation is achieved
by dynamically increasing the rate-limits when the network is not
congested. Through our implementation and testbed evaluation, we
show that ElasticSwitch achieves its goals under worst case trafﬁc
scenarios, without incurring a high overhead.
Acknowledgments: We thank the anonymous reviewers
and our shepherd, Jitu Padhye, for their guidance on the paper.
11. REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity
data center network architecture. In SIGCOMM. ACM, 2008.
[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic Flow Scheduling for Data Center
Networks. In NSDI, 2010.
[3] H. Ballani, P. Costa, T. Karagiannis, et al. The price is right: Towards
location-independent costs in datacenters. In Hotnets, 2011.
[4] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron. Towards
Predictable Datacenter Networks. In ACM SIGCOMM, 2011.
[5] H. Ballani, K. Jang, T. Karagiannis, C. Kim, D. Gunawardena, et al.
Chatty Tenants and the Cloud Network Sharing Problem. NSDI’13.
[6] T. Benson, A. Akella, and D. A. Maltz. Network trafﬁc
characteristics of data centers in the wild. In IMC. ACM, 2010.
[7] P. Bodik, I. Menache, M. Chowdhury, P. Mani, D. Maltz, and
I. Stoica. Surviving Failures in Bandwidth-Constrained Datacenters.
In SIGCOMM, 2012.
[8] J. Crowcroft and P. Oechslin. Differentiated end-to-end Internet
services using a weighted proportional fair sharing TCP. SIGCOMM
CCR, July 1998.
[9] N. G. Dufﬁeld, P. Goyal, A. G. Greenberg, P. Mishra,
K. Ramakrishnan, and J. E. van der Merwe. A Flexible Model for
Resource Management in VPNs. In SIGCOMM, 1999.
[10] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri,
D. A. Maltz, P. Patel, and S. Sengupta. VL2: A Scalable and Flexible
Data Center Network. ACM SIGCOMM, 2009.
[11] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, P. Sun, W. Wu, and
Y. Zhang. Secondnet: a data center network virtualization
architecture with bandwidth guarantees. In CoNEXT. ACM, 2010.
[12] V. Jeyakumar, M. Alizadeh, D. Mazières, B. Prabhakar, C. Kim, and
A. Greenberg. EyeQ: Practical Network Performance Isolation at the
Edge. In USENIX NSDI, 2013.
[13] T. Lam, S. Radhakrishnan, A. Vahdat, and G. Varghese. NetShare:
Virtualizing Data Center Networks across Services. UCSD TR, 2010.
[14] J. Lee, M. Lee, L. Popa, Y. Turner, P. Sharma, and B. Stephenson.
CloudMirror: Application-Aware Bandwidth Reservations in the
Cloud. HotCloud, 2013.
[15] J. Mudigonda, P. Yalagandula, M. Al-Fares, and J. C. Mogul. SPAIN:
COTS data-center Ethernet for multipathing over arbitrary
topologies. In USENIX NSDI, 2010.
[16] T. Nandagopal, K.-W. Lee, J.-R. Li, and V. Bharghavan. Scalable
Service Differentiation using Purely End-to-End Mechanisms:
Features and Limitations. Computer Networks, 44(6), 2004.
[17] B. Pfaff, J. Pettit, K. Amidon, M. Casado, T. Koponen, et al.
Extending Networking into the Virtualization Layer. In HotNets’09.
[18] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy,
S. Ratnasamy, and I. Stoica. FairCloud: Sharing the Network in
Cloud Computing. In ACM SIGCOMM, 2012.
[19] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and
M. Handley. Improving Datacenter Performance and Robustness
with Multipath TCP. In ACM SIGCOMM, 2011.
[20] H. Rodrigues, J. R. Santos, Y. Turner, P. Soares, and D. Guedes.
Gatekeeper: Supporting bandwidth guarantees for multi-tenant
datacenter networks. In USENIX WIOV, 2011.
[21] R. Sherwood, G. Gibb, K.-K. Yap, M. Casado, N. Mckeown, et al.
Can the production network be the testbed? In OSDI, 2010.
[22] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha. Sharing
the Data Center Network. In Usenix NSDI, 2011.
[23] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey. Jellyﬁsh:
Networking Data Centers Randomly. In USENIX NSDI, 2012.
[24] Srikanth K and Sudipta Sengupta and Albert Greenberg and Parveen
Patel and Ronnie Chaiken. The Nature of Datacenter Trafﬁc:
Measurements & Analysis. In IMC. ACM, 2009.
[25] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker. Applying NOX
to the Datacenter. In Proc. HotNets, NY, NY, Oct. 2009.
[26] A. Venkataramani, R. Kokku, and M. Dahlin. TCP Nice: A
Mechanism for Background Transfers. In OSDI, 2002.
[27] D. Xie, N. Ding, Y. C. Hu, and R. Kompela. The Only Constant is
Change: Incorporating Time-Varying Network Reservations in Data
Centers. In SIGCOMM, 2012.
362