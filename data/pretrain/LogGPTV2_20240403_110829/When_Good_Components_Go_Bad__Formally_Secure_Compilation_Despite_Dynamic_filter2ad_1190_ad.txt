a component is compromised, it does not gain any more privileges.
Definition 3.3. A compilation chain provides Robustly Safe Compi-
lation with Dynamic Compromise and Mutual Distrust (RSCDC
MD) if
there exists a back-translation function ↑ taking a finite trace prefix
m and a component interface Ii to a source component with the
same interface, such that, for any compatible interfaces IP and IC,
∀P:IP . ∀CT :IC . ∀t . (CT ∪ P↓)⇝t ⇒ ∀m≤t .
∃t′. ({(m, Ii)↑ | Ii ∈ IC} ∪ P)⇝t′ ∧ (m≤t′ ∨ t′≺IP m).
This definition closely follows RSCDC, but it restricts programs
and contexts to compatible interfaces IP and IC. We write P : I
to mean “partial program P satisfies interface I.” The source-level
context is obtained by applying the back-translation function ↑
pointwise to all the interfaces in IC. As before, if the prefix m is
cropped prematurely because of an undefined behavior, then this
undefined behavior must be in one of the program components,
not in the back-translated context components (t′≺IP m).
3.4 Formalizing RSCC
Using these ideas, we now define RSCC by following the dynamic
compromise game illustrated in Figure 3. We use the notation
P⇝∗m when there exists a trace t that extends m (i.e., m ≤ t) such
that P⇝t. We start with all components being uncompromised and
incrementally replace each component that encounters undefined
behavior in the source with an arbitrary component with the same
interface that may now attack the remaining components.
Definition 3.4. A compilation chain provides Robustly Safe Compart-
mentalizing Compilation (RSCC) iff ∀compatible interfaces I1, ..., In,
∀C1:I1, ..., Cn:In . ∀m. {C1↓, ..., Cn↓}⇝∗m ⇒
∃Ai1:Ii1 , ..., Aik :Iik .
(1) ∀j ∈ 1...k. ∃mj . (mj ≺Iij m) ∧ (mj−1 ≺Iij−1 mj) ∧
({C1, ..., Cn}\{Ci1 , ..., Cij−1}∪{Ai1 , ..., Aij−1})⇝∗mj
∧ (2) ({C1, ..., Cn}\{Ci1 , ..., Cik }∪{Ai1 , ..., Aik })⇝∗m.
This says that Ci1 , ..., Cik constitutes a compromise sequence
corresponding to finite prefix m produced by a compiled set of
components {C1↓, ..., Cn↓}. In this compromise sequence each com-
ponent Cij is taken over by the already compromised components
at that point in time {Ai1 , ..., Aij−1} (part 1). Moreover, after re-
placing all the compromised components {Ci1 , ..., Cik } with their
corresponding source components {Ai1 , ..., Aik } the entire m can
be reproduced in the source language (part 2).
This formal definition allows us to play an iterative game in
which components that encounter undefined behavior successively
become compromised and attack the other components. This is
the first security definition in this space to support both dynamic
compromise and mutual distrust, whose interaction is subtle and has
eluded previous attempts at characterizing the security guarantees
of compartmentalizing compilation as extensions of fully abstract
compilation [45] (further discussed in §5).
3.5 A Generic Proof Technique for RSCC
We now describe an effective and general proof technique for RSCC.
First, we observe that the slightly simpler RSCDC
MD implies RSCC.
7
When Good Components Go Bad
Abate et al.
Then we provide a generic proof in Coq that a compilation chain
obeys RSCDC
MD if it satisfies certain well-specified assumptions on
the source and target languages and the compilation chain.
Our proof technique yields simpler and more scalable proofs
than previous work in this space [45]. In particular, it allows us to
directly reuse a compiler correctness result à la CompCert, which
supports separate compilation but only guarantees correctness for
whole programs [47]; which avoids proving any other simulations
between the source and target languages. Achieving this introduces
some slight complications in the proof structure, but it nicely sep-
arates the correctness and security proofs and allows us to more
easily tap into the CompCert infrastructure. Moreover, this novel
proof structure completely removes the need for any kind of “par-
tial semantics” [7, 45] (a.k.a. “trace semantics” [43, 66]), allowing
the proof to be done entirely with respect to the semantics of whole
programs in the source and target languages. Finally, since only the
last step of our proof technique is specific to unsafe languages, our
technique could be further simplified to provide scalable proofs of
vanilla RSC for safe source languages [7, 68].
RSCDC
MD implies RSCC
reduces RSCC to RSCDC
be obtained by iteratively applying RSCDC
relies on back-translation in RSCDC
and respecting interfaces, as explained in §3.3.
Theorem 3.5. RSCDC
The first step in our proof technique
MD, using a theorem showing that RSCC can
MD. This result crucially
MD being performed pointwise
MD implies RSCC.
We proved this by defining a non-constructive function that
produces the compromise sequence Ai1 , ..., Ai1 by case analysis
on the disjunction in the conclusion of RSCDC
MD (using excluded
middle in classical logic). If m ≤ t′ we are done and we return
the sequence we accumulated so far, while if t′≺Pm we obtain a
new compromised component ci : Ii that we back-translate using
(m, Ii) ↑ and add to the sequence before iterating this process.
Generic RSCDC
MD proof outline Our high-level RSCDC
MD proof is
generic and works for any compilation chain that satisfies certain
well-specified assumptions, which we introduce informally for now,
leaving details to the end of this sub-section. The RSCDC
MD proof for
the compiler chain in §4 proves all these assumptions.
The proof outline is shown in Figure 4. We start (in the bottom
left) with a complete target-level program CT ∪ P↓ producing a
trace with a finite prefix m that we assume contains no undefined
behavior (since we expect that the final target of our compilation
will be a machine for which all behavior is defined). The prefix m
is first back-translated to synthesize a complete source program
CS ∪ P′ producing m (the existence and correctness of this back-
translation are Assumption 1). For example, for the compiler in §4,
each component Ci produced by back-translation uses a private
counter to track how many events it has produced during execution.
Whenever Ci receives control, following an external call or return,
it checks this counter to decide what event to emit next, based on
the order of its events on m (see §4.3 for details).
The generated source program CS ∪ P′ is then separately com-
piled to a target program CS ↓ ∪ P′↓ that, by compiler correct-
ness, produces again the same prefix m (Assumption 2). Now from
(CT ∪ P ↓)⇝∗m and (CS ↓ ∪ P′↓)⇝∗m we would like to obtain
(CS↓ ∪ P↓)⇝∗m by “recomposing” (Assumption 3) the components
8
The generic RSCDC
Assumptions of the RSCDC
from the two executions and forming a new execution for CS↓ ∪ P↓.
We use a three-way simulation to relate the executions of the two
complete programs, to a third execution that combines the pro-
gram part of the first execution, and the context part of the second
execution.
Once we know that (CS↓ ∪ P↓)⇝∗m, we use compiler correct-
ness again—now in the backwards direction (Assumption 4)—to
obtain an execution of the source program CS ∪ P producing trace
t. Because our source language is unsafe, however, t need not be an
extension of m: it can end earlier with an undefined behavior (§3.2).
So the final step in our proof shows that if the source execution
ends earlier with an undefined behavior (t′≺m), then this undefined
behavior can only be caused by P (i.e., t′≺Pm), not by CS , which
was correctly generated by our back-translation (Assumption 5).
MD proof
outlined above relies on assumptions about the compartmentalizing
compilation chain. In the reminder of this subsection we give details
about these assumptions, while still trying to stay at a high level
by omitting some of the low-level details in our Coq formalization.
The first assumption we used in the proof above is that every
trace prefix that a target program can produce can also be produced
by a source program with the same interface. A bit more formally,
we assume the existence of a back-translation function↑ that given
a finite prefix m that can be produced by a whole target program
PT , returns a whole source program with the same interface IP as
PT and which can produce the same prefix m (i.e., (m, IP)↑ ⇝∗m).
Assumption 1 (Back-translation).
∃ ↑ . ∀P:IP . ∀m defined. P⇝∗
m
Back-translating only finite prefixes enables our proof technique,
but at the same time limits it to only safety properties. While the
other assumptions from this section can probably also be proved
for infinite traces, there is no general way to define a finite program
that produces an arbitrary infinite trace. We leave devising scalable
back-translation proof techniques that go beyond safety properties
to future work.
m ⇒ (m, IP)↑ : IP ∧ (m, IP)↑ ⇝∗
MD proof
It is not always possible to take an arbitrary finite sequence of
events and obtain a source program that realizes it. For example, in
a language with a call stack and events {call, return}, there is no
program that produces the single event trace return, since every
return must be preceded by a call. Thus we only assume we can
back-translate prefixes that are produced by the target semantics.
As further discussed in §5, similar back-translation techniques
that start from finite execution prefixes have been used to prove
fully abstract compilation [43, 66] and very recently RSC [68] and
even stronger variants [7]. Our back-translation, on the other hand,
produces not just a source context, but a whole program. In the
top-left corner of Figure 4, we assume that this resulting program,
(m, IC ∪ IP)↑, can be partitioned into a context CS that satisfies the
interface IC, and a program P′ that satisfies IP .
Our second assumption is a form of forward compiler correct-
ness for unsafe languages and a direct consequence of a forward
simulation proof in the style of CompCert [58]. We assume sep-
arate compilation, in the style of a recent extension proposed by
Kang et al. [47] and implemented in CompCert since version 2.7.
Our assumption says that if a whole program composed of P and C
When Good Components Go Bad
Abate et al.
Source
(m, IC ∪ IP)↑
= (CS ∪ P′)⇝∗m
m ≤ t ∨ t ≺P m
5 Blame
(CS ∪ P) ⇝ t ∧ (m ≤ t ∨ t ≺ m)
1 Back-translation
2 Forward Compiler Correctness
4 Backward Compiler Correctness
Target
(CT ∪ P↓)⇝∗m
(CS↓ ∪ P′↓)⇝∗m
(CS↓ ∪P↓)⇝∗m
3 Recomposition
Figure 4: Outline of our generic proof technique for RSCDC
MD
(written C ∪ P) produces the finite trace prefix m that does not end
with undefined behavior (m defined) then P and C when separately
compiled and linked together (C↓ ∪ P↓) can also produce m.
Assumption 2 (Forward Compiler Correctness with Separate Compila-
tion and Undefined Behavior).
∀C P . ∀m defined. (C ∪ P)⇝∗
m ⇒ (C↓ ∪ P↓)⇝∗
m
The next assumption we make is recomposition, stating that if
two programs, each composed of a partial program side P↓ or P′↓,
and a context side CT or CS↓, sharing the same interfaces, produce
the same finite trace prefix m, then the complete program CS↓ ∪P↓
obtained by linking one side of each produce the same m.
Assumption 3 (Recomposition).
∀P:IP . ∀P′:IP . ∀CT :IC . ∀CS :IC . ∀m.
(CT ∪ P↓)⇝∗m ∧ (CS↓ ∪P′↓)⇝∗m ⇒ (CS↓ ∪P↓)⇝∗m
In a previous version of this work [6], we used to split this theo-
rem into two separate theorems, using a notion of partial semantics
[7, 45] (a.k.a. “trace semantics” [43, 66]). First, decomposition stated
that if a program obtained by linking two partial programs PT and
CT produces a finite trace prefix m that does not end in an unde-
fined behavior in the complete semantics, then each of the two
partial programs (below we take PT , but the CT case is symmetric)
can produce m in the partial semantics. Then, the converse of de-
composition, composition, stated that if two partial programs with
matching interfaces produce the same prefix m with respect to the
partial semantics, then they can be linked to produce the same m
in the complete semantics. When taken together, composition and
decomposition captured the fact that the partial semantics of the
target language were adequate with respect to their complete coun-
terpart. However, we noticed that proving those two assumptions
independently was particularly complex. Indeed, while decomposi-
tion is a straightforward lemma provided the partial semantics are
correctly setup, this is not the case for composition. Composing the
two runs in the face of missing information was fraught with deli-
cate and tedious reasoning, and did not fit the standard simulation
mold we hoped it would. This is why we instead propose to obtain
the new run in the complete semantics from the two runs in the
complete semantics directly. We report how this new assumption
is proved for our instance in §4.3.
In order to get back to the source language our proof uses a
backwards compiler correctness assumption, again with separate
compilation. As also explained in §3.2, we need to take into account
that a trace prefix m in the target can be explained in the source ei-
ther by an execution producing m or by one ending in an undefined
behavior (i.e., producing t≺m).
Assumption 4 (Backward Compiler Correctness with Separate Compi-
lation and Undefined Behavior).
m ⇒ ∃t . (C ∪ P)⇝t ∧ (m ≤ t ∨ t≺m)
∀C P m. (C↓ ∪P↓)⇝∗
Finally, we assume that the context obtained by back-translation
can’t be blamed for undefined behavior:
Assumption 5 (Blame). ∀CS : IC . ∀P, P′ : IP . ∀m defined. ∀t .
If (CS ∪ P′)⇝∗m and (CS ∪ P)⇝t and t ≺ m then m ≤ t ∨ t ≺P m.
We used Coq to prove the following theorem that puts together
the assumptions from this subsection to show RSCDC
MD:
Theorem 3.6. The assumptions above imply RSCDC
MD.
4 SECURE COMPILATION CHAIN
We designed a simple proof-of-concept compilation chain to illus-
trate the RSCC property. The compilation chain is implemented in
Coq. The source language is a simple, unsafe imperative language
with buffers, procedures, and components (§4.1). It is first compiled
to an intermediate compartmentalized machine featuring a compart-
mentalized, block-structured memory, a protected call stack, and
a RISC-like instruction set augmented with an Alloc instruction
for dynamic storage allocation plus cross-component Call and
Return instructions (§4.2). We can then choose one of two back
ends, which use different techniques to enforce the abstractions
of the compartmentalized machine against realistic machine-code-
level attackers, protecting the integrity of component memories and
enforcing interfaces and cross-component call/return discipline.
When the compartmentalized machine encounters undefined
behavior, both back ends instead produce an extended trace that
respects high-level abstractions; however, they achieve this in very
different ways. The SFI back end (§4.4) targets a bare-metal machine
that has no protection mechanisms and implements an inline ref-
erence monitor purely in software, by instrumenting code to add
address masking operations that force each component’s writes and
(most) jumps to lie within its own memory. The Micro-policies back
end (§4.5), on the other hand, relies on specialized hardware [27]