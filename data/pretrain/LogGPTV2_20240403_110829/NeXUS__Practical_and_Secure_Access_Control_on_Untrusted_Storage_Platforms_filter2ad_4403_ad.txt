A malicious server might wish to modify the structure of
the ﬁlesystem by, e.g., moving ﬁles or directories to other
locations within the volume. This is prevented by the use of
parent UUID pointers within our metadata structures, and the
authenticated encryption used to protect these structures: the
content of metadata cannot be altered without detection, and
swapping of equivalently named objects will cause the parent
UUID pointer validation to fail.
A more subtle attack is the rollback attack, in which the
server exposes a previous version of a user’s ﬁles. In this
case, the NEXUS enclave will cryptographically validate the
metadata, but cannot tell if it is the most recent version. To
address this freshness issue, we have extended our metadata
structures with a version number. On every metadata update,
the version number is incremented and stored locally before
uploading the metadata ﬁle. The metadata is considered stale if
the downloaded version is lower than the local value. However,
this approach is limited as it only offers per-ﬁle protection, and
not protect the entire ﬁle hierarchy. As a result, a malicious
server server could mount a forking attack [34], whereby ﬁle
updates are hidden from users resulting in each user perceiving
a different state of the volume. As a mitigating strategy, one
could maintain a hash tree of the metadata content as part
of the ﬁlesystem state [35, 10]. However, this naive design
approach requires root-to-leaf locking along write paths to
ensure metadata consistency. This not only impacts overall
latency, but also raises synchronization concerns in ensuring
the availability of the root hash. We leave further exploration
of this protection and performance tradeoff to future work.
VII. EVALUATION
To show how NEXUS achieves the design goals outlined in
Section III-B, we organized our performance evaluation around
the following criteria:
1) Utility. Does our prototype support a wide range of user
applications and workloads?
2) Performance. Are the overheads imposed by our prototype
reasonable? How does it perform on workloads representa-
tive of normal users?
3) Efﬁcient Revocation. How cheap are user revocations when
compared to pure cryptographic techniques?
Experimental Setup. Our experimental hardware consisted
of Intel i7 3.4 GHz CPUs with 8 GB RAM and 128 MB
SGX Enclave memory. For SGX support, we installed v1.7
of the Linux SGX SDK [36]. On the server-side, we used
the OpenAFS server distribution from the Ubuntu software
channels. In our experiments, we compare our approach against
an unmodiﬁed version of OpenAFS. For both setups, the ﬁle
chunk size was 1MB. As for NEXUS, we set dirnode bucket
size to 128 entries (See V-B), and used a normal AFS directory
as the metadata backing store. Unless otherwise noted, all of
our experiments are averaged over 10 runs.
A. Microbenchmarks
We ran several microbenchmarks to isolate the overhead
incurred by NEXUS. Recall from Section IV-A that NEXUS
requires repeated interaction with the underlying storage service
to ﬁnd the correct metadata. This generates additional network
trafﬁc on the server, and impacts the overall latency. Thus, we
break down the overhead as follows:
1) Enclave Runtime — The total time spent within the enclave,
including enclave transitions (ecalls and ocalls), access
control enforcement and metadata encryption.
2) Metadata I/O Latency — The time spent performing I/O
on the metadata objects, including reading, locking, and
writing. This is inﬂuenced by two main factors: the directory
depth of the path (each path component requires a metadata
access) and the size of the metadata being accessed.
We start by measuring the overhead on basic ﬁle I/O
operations using a python utility that reads and writes a ﬁle.
Before each run, we ﬂush the AFS ﬁle cache to ensure initial
data access requires a network trip to the server. AFS follows
open-to-close semantics, which implies that all ﬁle writes
are local until the ﬁle is closed (at which point NEXUS
encrypts the ﬁle chunks). Table 5a shows that overheads
increase proportionally with the ﬁle size. The enclave cost
Prototype
File Size (MB)
Prototype
OpenAFS
NEXUS
Metadata I/O
Enclave
1
0.61
0.51
0.09
0.02
2
1.52
1.46
0.12
0.09
16
5.55
6.81
0.14
0.58
64
22.24
28.56
0.80
2.07
OpenAFS
NEXUS
Metadata I/O
Enclave
Number of ﬁles
2048
4096
2.63
38.62
34.63
0.79
5.26
81.98
73.66
1.67
8192
11.93
172.29
154.34
3.55
1024
1.27
19.38
17.44
0.38
(a) Latency (seconds) of File I/O operations.
(b) Latency (seconds) of directory operations.
(c) Latency for cloning Git repositories.
Fig. 5: Latency measurements.
OpenAFS
NEXUS
Overhead
Workload
Large Files and Small Directory
LFSD
MFMD Medium Files and Medium Directory
SFLD
Small Files and Large Directory
#ﬁles
32
256
1024
Total Size
3.2 GB
2.5 GB
10 MB
TABLE III: Workloads for benchmarking Linux Applications.
wait for the data to propagate to disk, the overhead incurred by
NEXUS is amortized and does not noticeably affect the overall
latency. On the other hand, NEXUS incurs a ×2 performance
overhead on synchronous operations.
C. Cloning Git Repositories
We evaluated the performance of NEXUS in accessing
volumes with arbitrary directory hierarchies by cloning 3
repositories: Redis (618 ﬁles), Julia (1096 ﬁles) and NodeJS
(19912 ﬁles). Figure 5c shows a ×2.39 and ×2.87 overhead on
cloning Redis and Julia respectively, while incurring a ×3.64
overhead on NodeJS. This is because NodeJS has signiﬁcantly
more ﬁles/directories, a deeper directory hierarchy (up to 13
levels), larger directories (top 3 directories: 1458, 762, 783),
and bigger ﬁles. This increases the server-side load as each
ﬁlesystem access on NodeJS requires more metadata operations.
D. Linux Applications
In this test, we generated 3 characteristic workloads (Ta-
ble III) to evaluate the performance of common Linux utilities:
• tar -x: Extract an archive.
• du: Traverse and list the ﬁle sizes.
• grep: Recursively search the term “javascript”.
• tar -c: Create an archive.
• cp: Duplicate a ﬁle.
• mv: Rename a ﬁle.
With the exception of du and mv, all the applications perform
both ﬁle and directory operations. To prevent cache effects, we
ﬂush the system cache before running each application.
Figure 6 shows the plot of the test over 25 runs. The
tar extraction reafﬁrms the result of the directory operations
microbenchmark: the relative overhead of NEXUS with respect
to OpenAFS is proportional to the number of ﬁles in the
directory. This is further conﬁrmed by the single ﬁle write
operations in the tar archive creation and cp tests; they
impose a constant overhead across all workloads. The same
applies to the single directory operation performed by mv. In
the du test, NEXUS is indistinguishable from OpenAFS. Since
Operation
LevelDB
Fillseq
ﬁllsync
ﬁllrandom
overwrite
readseq
readreverse
readrandom
ﬁll100K
SQLITE
ﬁllseq
ﬁllseqsync
ﬁllseqbatch
ﬁllrandom
ﬁllrandsync
ﬁllrandbatch
overwrite
10.5 MB/s
2.2 ms/op
5.9 MB/s
4.0 MB/s
664.6 MB/s
425.0 MB/s
2.27 µs/op
11.0 MB/s
6.5 MB/s
14.4 ms/op
70.2 MB/s
4.2 MB/s
13.4 ms/op
7.6 MB/s
3.4 MB/s
8.1 MB/s
4.5 ms/op
3.7 MB/s
2.6 MB/s
718.1 MB/s
425.7 MB/s
3.7 µs/op
7.2 MB/s
6.4 MB/s
31.4 ms/op
69.7 MB/s
4.2 MB/s
31.2 ms/op
7.7 MB/s
3.4 MB/s
1.29
2.04
1.59
1.53
0.94
0.99
1.62
1.52
1.01
2.18
1.00
1.00
2.34
0.98
1.00
TABLE II: Database benchmark results.
per MB is almost constant (between ×0.01 — ×0.02), and
enclave execution is a small contribution to the overall runtime.
Metadata I/O overheads increase as the size of the ﬁlenode
grows to accommodate more ﬁle chunks. However, this is still
small compared to the ﬁle size (about 80B of encryption data
for every 1MB ﬁle chunk).
Next, we analyzed the performance of directory operations
using another python program that creates and deletes ﬁles
within a ﬂat directory. Table 5b shows that enclave execution
scales proportionally while remaining a small component of
the overall system runtime. However, the metadata I/O latency
is a major constituent of the overall runtime because every
ﬁle created increases the size of the directory dirnode, which
becomes much larger than the corresponding directory entry.
For large directories, this could result in signiﬁcant performance
overheads as the size discrepancy between the directory entry
and the dirnode becomes more pronounced.
B. Database Benchmarks
We ran the database benchmarks of LevelDB [37] and
Sqlite [38], two embeddable database engines commonly used
to provide a data layer. Using 4 MB of cache memory, each
benchmark generates several database ﬁles to emulate a key-
value store of 16-byte keys and 100-byte values. The latency
of various database operations was measured and displayed in
Table II. NEXUS’ performance closely matches OpenAFS in
asynchronous operations. Because the benchmark tool does not
redisjulianodejs050100150200Time/sx2.39x2.87x3.64openafsnexusFig. 6: Latency (over 25 runs) of common Linux applications under 3 generated worklords.
the directory structure is ﬂat, once the corresponding dirnode
gets cached in memory, lookup operations can be performed
locally. The same applies to grep, which has an overhead
between 1.5×—1.7× on all workloads.
E. Revocation Estimates
In a typical cryptographic ﬁleystem, revoking user access
involves the following steps: re-encrypting the affected ﬁle,