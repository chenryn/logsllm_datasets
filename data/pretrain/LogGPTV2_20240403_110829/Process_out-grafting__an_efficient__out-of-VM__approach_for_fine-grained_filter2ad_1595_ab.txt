2.2.1 When to Out-graft
To determine the appropriate moment for process execu-
tion transfer, we need to ensure it is safe to do so, i.e., the
transfer will not corrupt the execution of the out-grafted
process and the OS kernel. Particularly, once a process is
selected for out-grafting, the hypervisor ﬁrst pauses the pro-
duction VM, which is akin to a VM Exit event and causes the
VM’s virtual CPU (VCPU) state to be stored in hypervisor-
accessible memory. At this particular time, the to-be-grafted
process may be running in either user- or kernel-mode. (If it
is not actively running, it is then waiting in the kernel-mode
to be selected or dispatched for execution.) If the VCPU
state indicates the VCPU is executing the process in user
mode, we can immediately start out-grafting the process.
On the other hand, if the VCPU was running in privi-
leged mode (in the context of either the suspect process or
another process), we should not start the out-grafting pro-
cess to avoid leading to any inconsistency. For example, the
suspect process may have made a system call to write a large
memory buﬀer to a disk ﬁle. If its execution is transferred at
this point to another VM, we may somehow immediately re-
sume execution (in the security VM) at the next user-mode
instruction following the system call. As we are only trans-
ferring the user-mode execution, this will implicitly assume
the production VM kernel has already completed servicing
the system call, which may not be the case. Therefore, we
choose to wait till the process is selected to execute and even-
tually returns to user-mode. One way the hypervisor could
detect this is by monitoring context switches that occur in-
side the VM. However, in systems that support EPT, the
hypervisor is no longer notiﬁed of in-VM context switches.
Instead, based on the process’ page tables, we mark the
365Production VM
Process
virtual memory
VA1
VA2
VA3
Process
page table
Host
physical memory
Process
page table
Security VM
Process
virtual memory
va−>gfn
  gfn−> pfn
pfn<−gfn
gfn<−va
VA1
VA2
VA3
Figure 2: Out-grafted Process Memory Mapping in
Production and Security VMs (va: virtual address;
gfn: guest frame number; pfn: physical frame num-
ber)
corresponding user-level host-physical pages non-executable
(NX) in the EPT. When the kernel returns control of the
process back to user-mode, it will immediately cause a trap
to the hypervisor and thus kick oﬀ our out-grafting process.
2.2.2 What to Out-graft
After determining the right moment, we then identify the
relevant state that is needed to continue the process exe-
cution in the security VM. As our focus is primarily on its
user-mode execution, we need to transfer execution states
that the user-mode code can directly access (i.e.
its code
and data). It turns out that we only need to transfer two
sets of states associated with the process: the execution con-
text (e.g., register content) and its memory page frames.
The hypervisor already identiﬁes the process’ register values
from the VCPU state (stored in the hypervisor-accessible
memory). To identify its memory page frames, we simply
walk through the guest OS-maintained page tables (located
from the guest CR3 ) to identify the guest-physical page
frames for the user-mode memory of the process. For each
such page, we then further identify the corresponding host-
physical page frame from the EPT. At the same time, we
mark NX bit on each user-mode page frame in EPT that
belongs to the process. (Although this seems to duplicate
the setting from Section 2.2.1, this is required in case the
guest OS may allocate new pages right before we start to
out-graft.) After that, if the user-level code is executed, in-
advertently or maliciously, when the process has been out-
grafted for monitoring, the hypervisor will be notiﬁed. Note
that we mark the NX bit in the EPT, which is protected
from the (untrusted) production VM.
We point out that the transferred resources or state do not
include those OS kernel-speciﬁc states, which the process
may access only via system calls (e.g. open ﬁle descriptors
and active TCP network connections). This is important
from at least three diﬀerent aspects. First, the OS kernel-
speciﬁc states are the main root cause behind the semantic
gap challenge. Without the need of interpreting them, we
can eﬀectively remove the gap. Second, keeping these spe-
ciﬁc states within the production VM is also necessary to
ensure smooth continued execution of the out-grafted pro-
cess in the security VM – as the system call will be redirected
back to the production VM. It also allows for later process
restoration. Third, it reduces the state volume that needs to
be transferred and thus alleviates the out-grafting overhead.
2.2.3 How to Out-graft
Once we identify those states (e.g., the execution state
and related memory pages), we then accordingly instantiate
them in the security VM. There are two main steps. First,
we lock down the guest page table of the out-grafted process
in the production VM. Speciﬁcally, we mark the page frames
that contain the process’ page table entries as read-only in
the EPT so that any intended or unintended changes to them
(e.g. allocating a new page or swapping out an existing page)
will be trapped by the hypervisor. This is needed to keep in-
sync with the out-grafted process in the security VM. These
hardware-related settings are the only interposition we need
from the hypervisor, which are completely transparent to,
and independent of the monitoring tools (in the security
VM). The lock-down of page tables is due to the previously
described lack of hypervisor intervention over in-guest page
tables. In our system, these settings are temporary and only
last for the duration of our monitoring.
Second, we then populate the transferred states in the
security VM. For simplicity, we collectively refer to those
states as Srd. For this, we prepare a helper kernel mod-
ule (LKM) running inside the security VM. The hypervisor
issues an upcall to the helper module to instantiate Srd.
In that case, the helper module retrieves Srd and creates a
process context within the security VM for the out-grafted
process to execute. At this point, the memory content of
the process needs to be transferred from the production VM
to the security VM. In a non-EPT supported system, the
hypervisor could simply duplicate the page table between
the production and security VMs. In the presence of EPT
however, we aim to avoid large memory transfers by en-
abling the memory transfer as follows: The helper module
allocates the guest-physical page frames for those virtual
addresses that were present in Srd and sends this informa-
tion to the hypervisor; The hypervisor then simply maps
each such page to the host-physical page frame for the cor-
responding virtual address in the production VM. In other
words, this mechanism ensures that a user-level virtual ad-
dress A of the out-grafted process in the security VM and
the user-level virtual address A of the process in the produc-
tion VM are ultimately mapped to the same host-physical
page (as illustrated in Figure 2). After that, the helper also
ensures that any system call from the out-grafted process
will not be serviced in the security VM. Instead, they will
be forwarded back to the production VM and handled by
our second key technique (Section 2.3).
When a process is out-grafted for monitoring, its state in
the production VM is not destroyed. As mentioned earlier,
the production VM still maintains the related kernel state,
which not only serves the forwarded system calls but also
greatly facilitates the later restoration of the process from
the security VM back to the production VM. Meanwhile,
because of the separate maintenance of the process page
tables inside both VMs, we need to ensure they are kept in-
sync. In particular, the production VM may make legitimate
changes (e.g. swapping out a page). To reﬂect these changes
back in the security VM, our previous read-only marking on
related page tables can timely intercept any changes and
then communicate the changes back to the security VM.
With the populated states in a new process inside the se-
curity VM, existing process-level monitoring tools such as
strace, ltrace, and gdb can naturally access its state or mon-
itor its execution. For example, when the out-grafted pro-
cess executes system call instructions in the security VM
(although they are not actually serviced by the security VM
kernel), these can be examined in a semantically-rich manner
366(i.e., interpreting the arguments) without any modiﬁcation
to existing tools. Speciﬁcally, diﬀerent from prior out-of-
VM approaches, the monitor in our case no longer needs
to walk through external page tables to identify the physi-
cal addresses for examination. In other words, they can be
transparently supported! Finally, in order to support tools
that may need to access disk ﬁles used by the monitored
process, we make the ﬁle system that is used by the produc-
tion VM available in the security VM. We mount this ﬁle
system as read-only and non-executable. Note that the ﬁle
system is accessed only by the monitor to access any seman-
tic information. The requests by the out-grafted process to
access ﬁles are not handled in the security VM, but in the
production VM through forwarded system calls.
2.3 Mode-sensitive Split Execution
After selecting and out-grafting a process to the security
VM, our second key technique ensures that it can smoothly
continue its execution in the security VM, even though the
out-grafted process may consider itself still running inside
the same production VM. Also, we ensure that the untrusted
process cannot tamper with the security VM, including the
security VM’s kernel and the runtime environment (libraries,
log ﬁles etc.). We achieve this by splitting the monitored
process’ execution between the two VMs: all user-mode in-
structions execute in the security VM while the rest execute
in the production VM. In the following, we describe related
issues in realizing this mechanism and our solutions.
2.3.1
System Call Redirection
To continue the out-grafted process execution and isolate
it from the security VM, there is a need for it to access the
kernel-speciﬁc resources or states maintained in the produc-
tion VM. For instance, if the process already opened a ﬁle
for writing data, after the relocation to the security VM, it
must be able to continue writing to it. As the process needs
to make system calls to access them, we therefore intercept
and forward any system call from the out-grafted process
back to the production VM.
To achieve that, there exist two diﬀerent approaches. The
ﬁrst one is to simply ask hypervisor to intervene and forward
the system call (by crafting an interrupt and preparing the
appropriate execution context). However, it will unfortu-
nately impact the entire production VM execution. The
second one is to have a small piece of stub in place of the
out-grafted process. The stub is mainly designed to receive
forwarded system calls from the security VM, invoke the
same in the production VM, and then return the results
back to the security VM. We take the second approach in
our design as it can eﬀectively localize the eﬀect within the
out-grafted process itself and avoid heavy hypervisor inter-
vention for every forwarded system call.
The placement of the stub code deserves additional con-
sideration. Since the guest page tables are not managed by
the hypervisor, it cannot simply allocate a separate guest-
physical page for the stub code. As our solution, we choose
to temporarily “steal” an existing code page in the process,
by saving the original content aside and overlaying it with
the stub’s code. Recall (from Section 2.2) that the host-
physical memory frames corresponding to the process ad-
dress space are mapped in both VMs. To steal a code page,
the corresponding host-physical page frame is replaced with
another one that contains the stub code. To protect it from
Production VM
Yield
No
Stub
Security VM
Forwarded Syscall
Out−grafted Process Execution
User
Kernel
Syscall Pending?
Syscall Done
Forward Syscall
User
Kernel
Yes
Handle Syscall
Schedule
No
Syscall Return
Syscall Done?
Yes
Helper LKM
Figure 3: The Interplay Between the Stub (in Pro-
duction VM) and the Helper LKM (in Security VM)
being tampered by the production VM, we mark it non-
writable in the production VM’s EPT for the duration of
out-grafting.
The stub’s main function is to proxy the forwarded sys-
tem call from the security VM and replay it in the produc-
tion VM. In order to facilitate direct communication without
requiring hypervisor intervention, during the out-grafting
phase, we set up a small shared communication buﬀer ac-
cessible to the stub and the helper module in the security
VM. Also, note that if a system call argument is a pointer
to some content in the process address space, our design en-
sures that there is no need to copy this data between the
production and security VM. Since the process’ memory is
mapped in both VMs, the production VM kernel can sim-
ply access this as it would for any regular process. This
eliminates unnecessary memory copy overhead.
An interesting dilemma arises if the stub code needs to use
the stack. Speciﬁcally, the process’ stack is part of its data
pages and these pages are mapped faithfully in both VMs.
Any user-space instructions in the out-grafted process will
use this memory region as the stack. Further, these memory
regions may also contain the arguments to be passed down
to the production VM kernel for system calls. Hence, the
stub code cannot use the original stack pointer as its stack
frame, or it will collide with the user-mode execution in the
security VM. In our design, we simply avoid using the stack
in the stub code execution.
The interplay between the stub code and our helper mod-
ule is shown in Figure 3. The stub code is self-contained and
will directly invoke system calls without relying on any ad-
ditional library calls. Its size has been kept to the minimum
since we are overlaying over existing code memory for this.
Also, the stub can handle signals from the production VM
kernel and communicate them to the security VM helper
module. Speciﬁcally, when the production VM invokes a
previously registered signal handler, it will cause an exit to
the hypervisor, which will be eventually relayed to the helper
module to deliver the same signal to the out-grafted process
for handling.
2.3.2 Page Fault Forwarding
In addition to forwarding system calls, we also need to
forward the related page faults for the out-grafted process.
More speciﬁcally, at the time when a process’ execution is
being redirected to the security VM, some of its pages may
not be present (e.g., due to demand paging). Hence, when
the process is executing in the security VM, if it accesses a
non-present page, it will be trapped. Since the security VM
has no knowledge of how to correctly allocate and populate
367a new page (e.g., in case of a ﬁle-mapped page), we forward
page faults to the production VM. This also ensures that
when the production VM kernel attempts to access a new
memory page during process execution, it will be immedi-
ately available.
To forward related page faults, out helper module in the
security VM registers itself to receive notiﬁcation from the
security VM kernel for any page fault (or protection fault)
related to the out-grafted process. When it receives such
a notiﬁcation, it places the virtual address that causes the
page fault and a ﬂag to indicate a read-fault (including in-
struction fetches) or a write-fault, in the communication
buﬀer and notiﬁes the stub in the production VM. To ser-