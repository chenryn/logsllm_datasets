sequence in the target request.
74
S.P. Chung and A.K. Mok
6.2 Imperfect Suspicious Packet Detection
Just as a buﬀer overﬂow vulnerability does not always allow a successful attack,
not every vulnerability against allergy attacks is exploitable. In this and the next
section, we will present two factors that help the exploitation of a vulnerable ASG
system.
As we see in the previous discussion, actual signature generation usually fol-
lows a detection process in which suspicious traﬃc are identiﬁed as the “raw
material” for signatures. If this detection process may misclassify innocuous
packets even in the absence of allergy attacks, it would appear to be easier for
the attacker to populate the suspicious pool with his crafted packets. From our
experience, ASG systems that rely on purely network-based detection are more
vulnerable to false positive than those that employ some form of host-based de-
tection. For example, Autograph, Honeycomb, EarlyBird, and the CFG-based
system in [6] all employ detection mechanisms with non-zero false positive. As
seen in our previous discussion, attacking these systems is relatively easy, all
the attacker has to do in order to get his packets into the suspicious pool is to
send out packets to/from the right addresses. On the other hand, attacks are
much more complicated against systems like TaintCheck which have zero-false
positive detection mechanisms. Actual attacks against the target systems are
necessary to “feed” the signature generation process with the crafted packets.
This is obviously more complicated and risky for the attacker. Now let’s con-
sider another example that employs a supposedly zero false-positive detection
mechanism: FLIPS.
FLIPS: FLIPS [7] is a system that generates signatures to ﬁlter HTTP requests.
It uses a network-based anomaly detection system called PAYL to identify sus-
picious packets. PAYL detects anomalous packets by using a normal proﬁle that
describes the byte-frequency distributions for normal traﬃc of diﬀerent length
and destination port. Any packet which shows signiﬁcant deviation from the
proﬁle will be labeled suspicious. In FLIPS, all suspicious requests are cached.
FLIPS also employs a host-based intrusion detection system called instruction
set randomization (ISR). In addition to detecting attacks with zero false positive,
the ISR also identiﬁes the beginning of the attack payload. When ISR detects
an attack, FLIPS will copy the ﬁrst 1KB of the attack payload. The memory
copied will be matched against the cached suspicious request based on a simi-
larity score computed as 2C/(S1+S2), where C is the longest common substring
(LCS) between the packet and the captured payload, S1 and S2 are the length
of the two string being compared. The request most similar to the payload will
be used as the signature of the worm. Any incoming request that is suﬃciently
similar to the signature will then be dropped.
Now let us consider an attack with the ﬁrst target request as in Sect. 4.2
(which is 475-byte long). The attack involves sending two attack packets at
the same time. The ﬁrst packet is the one to be used as the worm signa-
ture at the end. This packet is constructed by appending to the target re-
quest a byte sequence that we call “gibberish”. The gibberish is intended to
Allergy Attack Against Automatic Signature Generation
75
make the packet suspicious to PAYL. In our attack, we will have a 100-byte
gibberish, all ﬁlled with a byte that rarely occurs in normal HTTP requests.
This should make the packet suﬃciently anomalous to be cached by FLIPS.
The second packet contains a real code injection attack (e.g. the one from
Code-Red), with the content of the ﬁrst packet appearing at where payload
should be placed (this makes the second packet around 1050 byte long). Once
this second packet triggers the alarm from ISR, the cache will be searched
for the suspicious request responsible. With similarity computed as described
above, the request from the shorter ﬁrst packet will achieve a higher simi-
larity score of 0.73 (when compared to the 1KB “worm payload” identiﬁed
by the ISR, which contains most of the 575 bytes in the ﬁrst attack packet,
and whatever follows in the memory when the attack is detected). Thus this
ﬁrst request will be used as the signature, and ﬁlter all instances of the tar-
get requests in future traﬃc (the similarity score between the target request
and the signature is 0.90, which is much higher than the threshold used in
[7]).
As a general observation, zero false-positive detection mechanisms cannot
make an ASG system immune against allergy attacks, but it does force the at-
tacker to employ modiﬁed control hijacking techniques, which makes the attack
far more complicated. We also note that the attacker cannot directly take over
the target system with the control hijacking code used in the allergy attacks;
this is because the hijacking will be detected and stopped.
6.3 Independent Detection and Matching
Another property of ASG systems that eases the design of allergy attacks is the
use of independent properties in the detection phase and the signature generation
phase. Consequently, the properties of the suspicious packets that are used to
ﬁlter future traﬃc are totally diﬀerent from the properties that make those
packets suspicious at the ﬁrst place. Therefore traﬃc ﬁltered by those signatures
may appear completely innocuous to the detection mechanism.
The above property avoids any conﬂict between the construction of packets
that will produce the desired signatures being generated when used for signa-
ture generation and getting those packets into the suspicious pool. This gives the
attackers a lot of freedom. Example systems with such properties include Auto-
graph, Polygraph, the CFG-based ASG system in [6], and systems that employ
honeypot for collecting worm traﬃc in general. Another example of ASG system
with this property is from [13].
PAYL-based ASG systems: In [13], two signature generation schemes are
proposed. The ﬁrst scheme, called ingress/egress correlation uses PAYL to iden-
tify suspicious ingress and egress traﬃc (with a separate normal proﬁle for each
direction). As in FLIPS, suspicious ingress packets are cached. Upon detection of
a suspicious egress packet, the ingress cache is searched for a suﬃciently similar
packet destined to the same port. Similarity is measured either by string equality
or longest common substring/subsequence (LCS/LCSeq). If a matching packet
76
S.P. Chung and A.K. Mok
is found, the part of the packet that gives the match (the entire packet if string
equality is used, the longest common substring/subsequence if LCS/LCSeq is
used) will be used as the signature.
In the second scheme, anomalous payload collaboration, diﬀerent sites col-
laborate to identify new worms. The participating sites will compare suspicious
ingress packets identiﬁed by their local PAYL with each other. If the suspi-
cious packets are found to be similar, those packets are used as signatures to
detect new worms. However, Wang et al are not very clear in [13] about how
the new signatures are matched against traﬃc in either schemes. We will assume
LCS/LCSeq as above to be used for similarity measure.
Note that while PAYL classiﬁes packets based on byte-frequency distribution,
signatures are generated, and matched against normal traﬃc by a totally diﬀer-
ent mechanism. As in our attack on FLIPS, we can make PAYL classify a packet
anomalous with a short sequence of gibberish, with the majority of the packet
made up of our target request (which will be matched against other traﬃc).
To attack the ingress/egress correlation mechanism, the attacker needs to
control one protected machine. If only web servers are protected (which seems
to be suggested in [13]), the attacker will need to compromise a web server in
the protected network. The attack packet used here has the same structure as
that against FLIPS: target request followed by gibberish. This attack packet is
to be sent both to and from the comprised web serve, both destined to port 80.
Both packets will be marked suspicious by PAYL (due to the gibberish), and the
consequent correlation will make the entire attack packet a new signature (if we
use a diﬀerent byte for the gibberish in the egress and ingress packets, only the
target request will be used in the signature). As before, the gibberish is only a
small portion of the entire signature, and thus all future instances of the target
request will be considered very similar to the signature and ﬁltered.
The attack against anomalous payload collaboration is similar but much sim-
pler. The same attack packet will be used. However, this time it will be sent to
diﬀerent networks in the collaborative scheme. Once again, PAYL will mark the
packets as malicious, and since the same packet is seen at all collaborating sites,
it will be used as a worm signature. The new signature will then achieve the
expected DoS.
6.4 Vigilante: A Non-vulnerable ASG System
Vigilante [2] employs two zero false-positive mechanisms to detect attacks, namely
the non-executable pages and dynamic dataﬂow analysis. The former technique
allows injected code attack to be detected, while the latter is very similar to the
dynamic taint analysis used in TaintCheck. Upon detecting an attack, the mali-
cious input that results in the detection will be identiﬁed. The attack will then
be replayed in a sandbox environment with the control ﬂow and data ﬂow of the
attacked process recorded until the point where the attack is detected. A ﬁlter
is then generated by computing the precondition of the input that leads to the
recorded control and data ﬂow in the attacked process.
Allergy Attack Against Automatic Signature Generation
77
We now consider whether Vigilante has any of the problematic properties listed
above. First of all, we note that the signature generation process in Vigilante is
semantic-based. By computing the precondition of the input that results in the
control and data ﬂow observed in a positive detection, Vigilante is eﬀectively iden-
tifying the protocol frame necessary for a successful attack. Furthermore, both
detection mechanisms have zero false-positive rate. Finally, for both detection
mechanisms, the detection and the signature generation process are based mostly
on the same property of the input, namely, properties that bring the protected
system to the state where the attack is detected.
From the above analysis, it appears that Vigilante should not be vulnerable
to allergy attacks. The ﬁlters generated by Vigilante indeed identify inputs that
result in dangerous state changes in the destined system and nothing else. In
general, systems that employ some host based detection mechanism are more
diﬃcult targets for allergy attacks. First of all, host based detection usually has
a lower false positive rate than a purely network-based mechanism. Secondly, a
host based mechanism can provide better information about how diﬀerent parts
of the malicious input correspond to the diﬀerent components of the worm.
7 Conclusions
In this paper, we have presented the allergy attack, an attack against automatic
signature generation (ASG) systems that has been anticipated theoretically but
not demonstrated in practice. We start by deﬁning allergy attacks as DoS at-
tacks which result in normal traﬃc to the protected network being dropped by
perimeter defense. This is achieved by inducing the ASG system in generating
signatures that match the target traﬃc. When these signatures are deployed to
the perimeter defense, the expected DoS will occur. In our discussion, we fo-
cused on the DoS against web service, which appears a most direct way to cause
damages with allergy attacks. We then demonstrate the allergy attack against a
well known ASG system: Autograph. We also analyze how similar attacks can be
successfully mounted against other implemented ASG systems. We believe the
vulnerability roots from the use of semantic-free signature generation process,
i.e., the generation of signatures without considering how the properties matched
by the signatures map to successful worm behavior. Two factors that facilitate
the exploitation of this vulnerability are the imperfections in the mechanism
used to identify suspicious packets, and the use of independent properties in the
detection phase and the signature generation phase. In our future work, we plan
to test some of the proposed attacks against actual implementations of other
analyzed ASG systems, and to study the eﬀectiveness of defending against type
II allergy attacks with a normal traﬃc corpus3.
As compared to the well studied issues with polymorphic worms, we believe
the allergy attacks present a more pressing problem to practical ASG systems.
3 Parties capable of experimenting with any studied ASG systems are welcome to
collaborate with us, or verify our outlined attacks independently.
78
S.P. Chung and A.K. Mok
While polymorphic attacks will render many existing ASG systems totally use-
less, allergy attacks can turn them into real harm to the protected network.
The cost of designing and launching an allergy attack is also much smaller than
that of a eﬀective polymorphic worm. The eﬀect of an allergy attack can also
be more long lasting than other common DoS techniques, since the target traﬃc
will remain blocked until all the “poisonous” signatures are removed.
Finally, we will note that the scope of allergy attacks (or attacks with similar
ﬂavor) is not limited to ASG systems. As our defense evolves to react to attacks
by modifying the state of the protected systems, the attackers may deliberately
trigger the defense to modify the system’s state in a way favorable to them.
Defense designed under the old assumption that the attackers always try to
evade detection may then be manipulated to serve the attackers’ purpose.
References
1. M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can machine
learning be secure? In Proceedings of the ACM Symposium on InformAtion, Com-
puter and Communications Security (ASIACCS2006), Taipei, Mar 2006.
2. M. Costa, J. Crowcroft, M. Castro, A. Rowstron, L. Zhou, L. Zhang, and
P. Barham. Vigilante: End-to-end containment of internet worms. In Proceedings
of 20th ACM Symposium on Operating Systems Principles, Brighton, Oct 2005.
3. J. R. Crandall, S. F. Wu, and F. T. Chong. Experiences using minos as a tool for
capturing and analyzing novel worms for unknown vulnerabilities. In Proceedings
of GI/IEEE SIG SIDAR Conference on Detection of Intrusions and Malware and
Vulnerability Assessment (DIMVA) 2005, Vienna, July 2005.
4. H. Kim and B. Karp. Autograph: Toward automated, distributed worm signature
detection. In Proceedings of 13th USENIX Security Symposium, California, August
2004.
5. C. Kreibich and J. Crowcroft. Honeycomb - Creating Intrusion Detection Signa-
tures Using Honeypots. In Proceedings of the Second Workshop on Hot Topics in
Networks (Hotnets II), Boston, November 2003.
6. C. Krugel, E. Kirda, D. Mutz, W. Robertson, and G. Vigna. Polymorphic worm
detection using structural information of executables.
In Proceedings of Eighth
International Symposium on Recent Advances in Intrusion Detection (RAID2005),
Seattle, Sept 2005.
7. M. E. Locasto, K. Wang, A. D. Keromytis, and S. J. Stolfo. Flips: Hybrid adaptive
intrusion prevention. In Proceedings of Eighth International Symposium on Recent
Advances in Intrusion Detection (RAID2005), Seattle, Sept 2005.
8. J. Newsome, B. Karp, and D. Song. Polygraph: Automatically generating signa-
In Proceedings of The 2005 IEEE Symposium on
tures for polymorphic worms.
Security and Privacy, Oakland, May 2005.
9. J. Newsome and D. Song. Dynamic taint analysis for automatic detection, analysis,
and signature generation of exploits on commodity software. In Proceedings of 12th
Annual Network and Distributed System Security Symposium (NDSS 05), Feb 2005.
10. R. Perdisci, D. Dagon, W. Lee, P. Fogla, and M. Sharif. Misleading worm signa-
ture generators using deliberate noise injection. In Proceedings of The 2006 IEEE
Symposium on Security and Privacy, Oakland, May 2006.
Allergy Attack Against Automatic Signature Generation
79
11. S. Singh, C. Estan, G. Varghese, and S. Savage. Automated worm ﬁngerprinting. In
Proceedings of 5th Symposium on Operating Systems Design and Implementation,
California, December 2004.
12. Y. Tang and S. Chen. Defending against internet worms: a signature-based ap-
proach. In Proceedings of 24th Annual Joint Conference of the IEEE Computer
and Communications Societies, Florida, July 2005.
13. K. Wang, G. Cretu, and S. J. Stolfo. Anomalous payload-based worm detection
and signature generation. In Proceedings of Eighth International Symposium on
Recent Advances in Intrusion Detection (RAID2005), Seattle, Sept 2005.
14. V. Yegneswaran, J. T. Giﬃn, P. Barford, and S. Jha. An architecture for generating
semantics-aware signatures. In Proceedings of 14th USENIX Security Symposium,
Maryland, Aug 2005.
A Appendix
Autograph is a string-matching system that generates worm signatures by mon-
itoring traﬃc crossing an edge network’s DMZ. Since the Autograph proto-
type available only handles TCP packets, we assume all traﬃc to be under the
TCP protocol. Signatures generated by Autograph are destination port speciﬁc,
i.e. only traﬃc destined to the corresponding port will be matched against a
signature.
Autograph processes traﬃc in two stages. In the ﬁrst stage, Autograph iden-
tiﬁes scanners by recording IP addresses that made more than s thresh unsuc-
cessful connection attempts to the protected network. A connection attempt is
considered unsuccessful if it times out without any reply received, or it got reset
before completing the TCP handshake. In addition to the IP address, Autograph
will also record the destination port targetted by all the failed connections from
a scanner. Afterwards, all the TCP packets from successful connections originat-
ing from a scanner address and destined to the recorded port will undergo ﬂow
reassembly. The resulting suspicious ﬂows will be recorded in a suspicious pool.
With enough ﬂows in the pool that are destined to the same port, Autograph
will start the next stage of processing: signature generation.
In the signature generation stage, Autograph will divide the suspicious ﬂows
into content blocks, and ﬁnd the set of most prevalent blocks. The process is
greedy, and the block with highest prevalency will be picked ﬁrst. Autograph
will keep adding blocks to the set until a pre-conﬁgured portion of suspicious
ﬂows contain one or more blocks from the set. Signatures will then be generated
for each block in the set, with the entire block being the byte sequence that
will be matched against future traﬃc destined to the port for which signature
generation is invoked.
For dividing ﬂows into content blocks, Autograph employs the COnetent-
based Payload Partitioning (COPP) technique. The COPP partitions suspicious
ﬂows into non-overlapping, variable-length blocks by computing the Rabin ﬁn-
gerprint of every 2-byte subsequence in the ﬂow, starting from its beginning.
The 2-byte subsequence marks the end of a content block if it matches B, i.e. its
ﬁngerprint r satisﬁed the equation r = B (mod a), where B is a predetermined
80
S.P. Chung and A.K. Mok
breakmark, a is a conﬁgurable parameter that controls the average block size.
Due to the content based nature of COPP, a similar set of blocks will be gener-
ated even if bytes are added to or deleted from the worm payloads. This helps
Autograph to generate signatures that ﬁlter diﬀerent instances of a polymorphic
worm.
To avoid overly speciﬁc or overly general signatures, Autograph bounds the
size of content blocks generated between m bytes and M bytes (with m and M
conﬁgurable). In other word, Autograph will not end a content block at a 2-byte
subsequence that matches B if that results in a block shorter than m. Instead,
Autograph will search for the next matching 2-byte subsequence. Similarly, any
content block that reaches M bytes long will be terminated. Autograph also
avoids using content blocks in ﬂows that originates from fewer than a conﬁg-
urable source count number of sources for signatures. This prevents generating
signatures for normal traﬃc from misconﬁgured, but benign hosts. Finally, Au-
tograph employs a blacklisting mechanism which prevents subsequences of any
blacklisted byte sequences from being used as signatures. In [4], the blacklist
is generated in a training period where all signatures generated are manually
checked for false positives. Signatures deemed to match normal traﬃc will be
added to the blacklist. This prevents generating signatures for normal traﬃc