consider two attack goals. First, given a target class, we want
all the clips from this class to be misclassiﬁed by the real-time
video classiﬁer. Second, for all the clips from other (non-target)
classes, we want the classiﬁer to correctly classify them.
We point out here that a man-in-the-middle attacker will
be unable to simply replace the streaming video with static
frames or pre-recorded video and yet achieve the required
stealthiness. This is because of two reasons. First, the attacker
has no a priori knowledge about “when” a targeted action
occurs. For example, an attacker with malicious intent may
want to misclassify the action of an elderly person falling down
at a smart elderly care center that is monitored by multiple
cameras (e.g., [37]). Since the attacker does not know when
and where exactly an elderly person will fall down, it has
to replace the video streams from all the cameras with the
pre-recorded video of elderlies doing something else (e.g.,
walking) for extended periods or ideally all the time. However,
it is hard to guarantee that the replaced videos are visually
similar to the real-time environment (e.g., people and their
actions, weather) and replaying videos out of context may
be noticeable. In addition,
the attacker
may be capable of delaying the video by a short period to
inject targeted perturbations against speciﬁc activities; how-
ever, while such an approach can eliminate universal and
stealth requirements, it cannot overcome the boundary effect
and cannot obviate the corresponding computation needed for
online perturbation generation. Second, the attacker also has to
replace the actions of multiple people involved at the facility
captured with the multiple cameras. In other words, a large
number of replacement videos capturing a large set of people
at the facility will be necessary. If the replaced videos show
the same person at different locations, or people who are not at
the facility, this will be noticeable. Applying perturbations on
the video will enable the attacker to stealthily misclassify only
the speciﬁc activity relating to the falling an elderly, keeping
all other actions unaffected. Furthermore, the imperceptibility
of these perturbations will not cause any human operator to
notice anything overtly wrong.
is possible that
it
Fig. 1: This ﬁgure [8] illustrates the score curves computed
by a video classiﬁer with a sliding window for every class.
Real-time video classiﬁcation systems use these score curves
to do online action recognition.
to extract video clips and use the clips as inputs to a classiﬁer
to analyze the video stream [8], [47]. The classiﬁer computes
an output score for each class in each sliding window. The
sliding window moves with a stride. Moving in concert with
the sliding window, one can generate “score curves” for each
action class. Note that the scores for all the action classes
evolve with time. The score curves are then smoothed (to
remove noise) as shown in Figure 1. With the smoothed
score curves, the on-going actions are predicted online. From
the ﬁgure one can see that, the real-time video classiﬁcation
system is fooled if one can make the classiﬁer output a low
score for the true class in each sliding window; with this, the
true actions will not be recognized.
B. The C3D classiﬁer
Convolutional neural networks (CNNs) are being increas-
ingly applied in video classiﬁcation. Among these, spatio-
temporal networks like C3D [46] and two-stream networks
like I3D [6] outperform other network structures [13], [14].
However, two-stream networks require optical ﬂow extraction
as preprocessing. Without the requirement of non-trivial pre-
processing on the video stream, spatio-temporal networks are
more efﬁcient and suitable for real-time applications; among
these, C3D is the start-of-art model [13].
Given its desirable attributes and popularity, without loss
of generality, we use the C3D model as our attack target in
this paper. The C3D model is based on 3D ConvNet (a 3D
convolutional neural network or CNN) [20], [46], [48], which
is very effective in modeling temporal information (because
it employs 3D convolution and 3D pooling operations). The
architecture and hyperparamters of C3D are shown in Figure 2.
The input to the C3D classiﬁer is a clip consisting of 16
consecutive frames. This means that upon using C3D, the
sliding window size is 16. Both the height and the width
of each frame are 112 pixels and each frame has 3 (RGB)
channels. The last layer of C3D is a softmax layer that provides
a classiﬁcation score with respect to each class.
3
Fig. 2: The C3D architecture [46]. C3D net has 8 convolution, 5 max-pooling, and 2 fully connected layers, followed by a
softmax output layer. All 3D convolution kernels are 3 × 3 × 3 with a stride [46] of 1 in both spatial and temporal dimensions.
The number of ﬁlters is shown in each box. The 3D pooling layers are represented as pool1 to pool5. All pooling kernels are
2 × 2 × 2, except for pool1, which is 1 × 2 × 2. Each fully connected layer has 4096 output units.
B. Our datasets
We use the human action recognition dataset UCF-101
[41] and the hand gesture recognition dataset 20BN-JESTER
dataset (Jester) [7] to validate our attacks on video classiﬁca-
tion systems. We use these two datasets because they represent
two kinds of classiﬁcation, i.e., coarse-gained and ﬁne-grained
action classiﬁcation.
The UCF 101 dataset: The UCF 101 dataset used in our
experiments is the standard dataset collected from Youtube.
It includes 13320 videos from 101 human action categories
(e.g., applying lipstick, biking, blow drying hair, cutting in the
kitchen etc.). The videos collected in this dataset have varia-
tions in camera motion, appearance, background, illumination
conditions etc. Given the diversity it provides, we consider
the dataset to validate the feasibility of our attack model on
coarse-gained actions. There are three different (pre-existing)
splits [41] in the dataset; we use split 1 for both training and
testing, in our experiments. The training set includes 9,537
video clips and the testing set includes 3,783 video clips.
The Jester dataset: The 20BN-JESTER dataset (Jester) is
a recently collected dataset with hand gesture videos. These
videos are recorded by crowd-sourced workers performing 27
kinds of gestures (e.g., sliding hand left, sliding two ﬁngers
left, zooming in with full hand, zooming out with full hand
etc.). We use this dataset to validate our attack with regard
to ﬁne-grained actions. Since this dataset does not currently
provide labels for the testing set, we withhold a subset of the
training set as our validation set and use the validation set for
testing. The training set has 148,092 short video clips and our
testing set has 14,787 short video clips.
IV. GENERATING PERTURBATIONS FOR REAL-TIME VIDEO
STREAMS
From the adversary’s perspective, we ﬁrst consider the
challenge of attacking a real-time video stream. In brief,
when attacking an image classiﬁcation system, the attackers
usually take the following approach. First, they obtain the
target image that is to be attacked with its true label. Next,
they formulate a optimization problem wherein they try to
compute the ”minimum” noise that is to be added (towards
imperceptibility) in order to cause a mis-classiﬁcation of the
target. The formulation takes into account the function of the
classiﬁer, the input image, and its true label. Backpropagation
is commonly used to solve this optimization problem [11],
[24], [27].
In the context of real-time video classiﬁcation, the video is
not available to the attackers a priori. Thus, they will need to
create perturbations that can effectively perturb an incoming
(a) GAN Architecture
(b) Our Architecture
Fig. 3: We use a GAN-like architecture for the generative
model. However, our architecture is different from GAN in
the following aspects: 1) The discriminator is a pre-trained
classiﬁer we attack, whose goal is to classify videos, and not
to distinguish between the natural and synthesized inputs; 2)
The generator generates perturbations, and not direct inputs
to the discriminator, and the perturbed training inputs are
fed to discriminator; 3) The learning objective is to let the
discriminator misclassify the perturbed inputs.
video stream, whenever a target class is present. Generation of
online perturbations based on an incoming video stream would
have an associated cost of O(f ×b×n) where, f is frame rate,
b is cost of one backpropagation on the DNN, and n is the
number of backpropagations needed to solve the optimization
problem.
Our approach is to compute the perturbations ofﬂine and
apply them online, and thus, the online computation cost is
O(1). Since we cannot predict what is captured in the video,
we need perturbations which work with unseen inputs. A
type of perturbation that satisﬁes this requirement is called
the Universal Perturbation (UP), which has been studied in
the context of generating adversarial samples against image
classiﬁcation systems [29], [32]. In particular, Mopuri et al.,
have developed a generative model that learns the space of
universal perturbations for images using a GAN-like architec-
ture. Inspired by this work, we develop a similar architecture,
but make modiﬁcations to suit our objective. Our goal is to
4
generate adversarial perturbations that fool the discriminator
instead of exploring the space for diverse UPs. In addition, we
retroﬁt the architecture to handle video inputs. Our architecture
is depicted in Figure 3b. It consists of three main components:
1) a 3D generator which generates universal perturbations
(clips); 2) a post-processor, which for now does not do
anything but is needed to solve other challenges described
in subsequent sections; and 3) a pre-trained discriminator for
video classiﬁcation, viz., the C3D model described in § II-B.
The 3D generator in our model is conﬁgured to use 3D
deconvolution layers and provide 3D outputs as shown in
Figure 8. Speciﬁcally, it generates a clip of perturbations,
whose size is equal to the size of the video clips taken as input
by the C3D classiﬁer. To generate universal perturbations, the
generator ﬁrst takes a noise vector z from a latent space. Next,
It maps z to a perturbation clip p, such that, G(z) = p. It then
adds the perturbations on a training clip x (denote the set of
inputs from the training class as X) to obtain the perturbed
clip x + p. Let c(x) be the true label of x. This perturbed
clip is then input to the C3D model which outputs the score
vector Q(x + p) (for the perturbed clip). The classiﬁcation
should ensure that the highest score corresponds to the true
class (c(x) for input x) in the benign setting. Thus, the attacker
seeks to generate a p such that the C3D classiﬁer outputs a
low score to the c(x)th element in the Q vector (denoted as
Qc(x)) for x+p. In other words, this means that after applying
the perturbation, the probability of mapping x to class c(x) is
lower than the probability that it is mapped to a different class
(i.e., the input activity is not correctly recognized).
We seek to make this perturbation clip p “a universal
perturbation”, i.e., adding p to any input clip belonging to the
target class would cause misclassiﬁcation. This means that we
seek to minimize the sum of the cross-entropy loss over all
the training data as per Equation 1. Note that the lower the
cross-entropy loss, the higher the divergence of the predicted
probability from the true label [17].
minimize
G
− log[1 − Qc(x)(x + G(z)]
(1)
(cid:88)
x∈X
When the generator is being trained, for each training
sample, it obtains feedback from the discriminator and adjusts
its parameters to cause the discriminator to misclassify that
sample. It tries to ﬁnd a perturbation that works for every
sample from the distribution space known to the discriminator.
At the end of this phase, the attacker will have a generator
that outputs universal perturbations which can cause the mis-
classiﬁcation on any incoming input sample from the same
distribution (as that of the training set). However, as discussed
next, just applying the universal perturbations alone will not
be sufﬁcient to carry out a successful attack. In particular, the
attack can cause unintended clips to be misclassiﬁed as well,
which could compromise our stealth requirement as discussed
next in §V.
V. MAKING PERTURBATIONS STEALTHY
Blindly adding universal perturbations will affect the clas-
siﬁcation of clips belonging to other non-targeted classes. This
may raise alarms, especially if many of these misclassiﬁcations
are mapped on to rare actions. Thus, while causing the target
class to be misclassiﬁed,
the impact on the other classes
must be imperceptible. This problem can be easily solved
when dealing with image recognition systems since images
are self-contained entities, i.e., perturbations can be selectively
added to target images only. However, video inputs change
temporally and an action captured in a set of composite frames
may differ from that in the subsequent frames. It is thus hard
to a priori identify (choose) the frames relating to the target
class, and add perturbations speciﬁcally to them. For example,
consider a case with surveillance in a grocery store. If attackers
seek to misclassify an action related to shoplifting and cause
this action to go undetected, they are unlikely to have precise
knowledge of the exact time when the action will occur and
be captured by the video activity recognition system. Adding
universal perturbations blindly in this case, could cause mis-
classiﬁcations of other actions (e.g., other benign customer
actions may be mapped onto shoplifting actions thus triggering
alarms). A similar example may be construed with respect to
the elderly care system described in § III-A; here, the attacker
has no way of knowing a priori when an elderly falls.
Since it is hard (or even impossible) to a priori identify
the frame(s) that capture the intended actions and choose them
for perturbation, the attackers need to add perturbations to each
frame. However, to make these perturbations furtive, they need
to ensure that the perturbations added only mis-classify the
target class while causing other (non-targeted) classes to be