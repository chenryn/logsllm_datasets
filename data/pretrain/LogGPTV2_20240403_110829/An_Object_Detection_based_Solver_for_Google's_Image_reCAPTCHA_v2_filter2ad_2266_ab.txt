sponsible for mapping the detected objects back to their corre-
sponding (potential) grids in the original challenge. The two
main components of this module are a base object detection
system and the bounding box to the grid mapping algorithm.
We now discuss each of them in detail.
4.2.1 Base object detector: YOLOv3
We use YOLOv3 as the base object detector after experiment-
ing with several other advanced object detectors, including
Faster R-CNN [40], R-FCN [22], SSD [33], and RetinaNet
[32]. We ﬁnd YOLOv3 to be signiﬁcantly faster than all other
tested object detectors when running the detection on a test
image; however, the accuracy of YOLOv3 might be slightly
lower than other object detectors. Since solving CAPTCHAs
is a time-sensitive task, we opt to use YOLOv3 for its su-
perior speed. The feature extractor network in YOLOv3 is
called Darknet-53 because it has 53 convolutional layers, with
shortcut connections. See [39] for details.
Datasets. We use two datasets, speciﬁcally developed to
handle object categories found in reCAPTCHA challenges.
The ﬁrst dataset is a publicly available dataset called MS
COCO [13]. The MS COCO dataset has 80,000 training im-
ages and 40,000 validation images with 80 object classes,
out of which 8 classes frequently appear in reCAPTCHA
challenges. The MS COCO object classes common to re-
CAPTCHA object categories are bicycle, boat, bus, car, ﬁre
hydrant, motorcycle, parking meter, and trafﬁc light. The sec-
ond dataset is a custom one that we develop by ourselves.
We crawled over 6,000 images from different sources such
as Flickr 1, Google image search 2, and Bing image search 3.
After prepossessing these, we end up with 4800 images. We
also use 2100 images from the original reCAPTCHA chal-
lenges for this dataset. We manually annotated and labeled
the object instances in those images to prepare and ﬁnalize
the dataset. Our ﬁnal custom dataset has 11 object categories:
boat, bridge, chimney, crosswalk, mountain, palm tree, stair,
statue, taxi, tractor, and tree.
Training the base object detector. We use two YOLOv3
models trained on the two datasets. We mostly go with the
default architecture of the YOLOv3 network with some minor
modiﬁcations for both models. We set up the batch size to 64,
and the learning rate to 0.001 for training. We use the Darknet
[38], an open-source neural network framework written in
C, for training the YOLOv3 models. We train the model on
the MS COCO dataset for roughly 15 days, and the model on
the custom dataset for 2 days. The training is performed on a
server with an NVIDIA RTX 2070 GPU. We then evaluate the
weight ﬁles for both models on corresponding test sets and
choose the best weights. Our ﬁnal model for the MS COCO
dataset has the mean average precision at 0.5 IOU (mAP@.5)
of 57.4% on the testing set. The second model has obtained a
mAP@.5 value of 51.79% on the respective testing set.
Inference or making predictions. We use the Darknet
framework to make predictions on reCAPTCHA challenge
images with our trained models. By default, Darknet does not
provide any localization information. We adjust the source
code to output the bounding box coordinates when running
the inference on an image. The modiﬁed prediction output
includes class name, conﬁdence score, and bounding box co-
ordinates for each detected object instance in a prediction
operation. We set the detection threshold to 0.2.
4.2.2 The bounding box to grid mapping algorithm
After detecting the objects with the base object detector in
the challenge image, we need to map the objects back to their
corresponding grids in the original challenge. Our bounding
box to grid mapping algorithm works as follows.
1. Use the R and C parameters from the browser automa-
tion module to get an R×C grid representation of the
image.
2. Compute coordinates of each grid relative to the top left
of the image (see Figure 2).
3. Take the prediction output from the base object detector.
1https://www.ﬂickr.com/
2https://images.google.com/
3https://www.bing.com/images/
272    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Figure 3: A result returned by the bounding box to grid map-
ping algorithm. Left: An original challenge image (the target
object is a “bus”). Right: The JSON array returned by the
algorithm.
4. For each bounding box with the class label matching the
target object name in the challenge, take the coordinates
of the box and the grids. If any of the coordinates of the
bounding box falls inside a grid, mark it as a potential
grid. Depending on the size of the bounding box, it may
fall within multiple grids. We store all of these grid num-
bers in an array and call it as the potential grid numbers
(PGNs).
5. For each bounding box, return the class name, conﬁdence
score, and the PGNs.
6. Return the results as a JSON array.
Figure 3 shows an example of the result returned by the
bounding box to the grid mapping algorithm for a sample
challenge image.
4.3 Submitting and verifying challenges
The JSON array returned by the solver module is passed to the
browser automation module. The browser automation module
ﬁrst extracts the potential grid numbers from the PGNs arrays
and locates the representative grids in the HTML table. It then
clicks on these grids in the challenge widget and ﬁnally clicks
the “verify” button when the process is completed.
The system then veriﬁes whether the challenge is passed
or not using the “reCAPTCHA ARIA status messages.” 4 We
further verify the challenges submitted to our own websites
by validating user response token, g-recaptcha-response,
to the reCAPTCHA backend. The g-recaptcha-response
remains empty until the challenge is solved. When a challenge
is successfully solved, it gets populated with a long string. Af-
ter submitting a challenge to our website, our bot ﬁrst extracts
the user response token. It then sends a veriﬁcation request
4https://support.google.com/recaptcha/#aria_status_message
Figure 4: The frequency of different object categories in
collected challenges.
to the reCAPTCHA backend server with this token and the
secret key to authenticate the token.
5 Attack evaluation
5.1
Implementation and evaluation platform
The browser automation module is built upon the puppeteer-
ﬁrefox [10], a node library developed by Google, to control
the Firefox web browser programmatically. The core function-
alities of the module are developed using JavaScript. The base
object detector in the solver module is based on the YOLOv3
object detection algorithm. We train and test the YOLOv3
models with a customized version of the Darknet framework
that especially meets our needs. Our bounding box to the grid
mapping algorithm is written in C for efﬁciency.
We train the YOLOv3 models on a server with 6 Intel R(cid:13)
Xeon R(cid:13) E5-2667 CPUs, an NVIDIA GeForce RTX 2070 GPU,
and 96GB of RAM running the Arch Linux operating system.
We compile the Darknet framework against the GPU with
CUDA 10.2 and cuDNN 7.5. We conduct all the experiments
on this machine.
Experimental setting. We use puppeteer-ﬁrefox (version
0.5.1) running on top of node library version 14.4.0 for
browser automation. The browser we used is Firefox 65.0. We
run the web browser in a clean state each time, i.e., no caches
or cookies are retained between two subsequent requests. We
do not attempt to obfuscate any aspects of the requests or web
browser properties (e.g., using custom User-Agent header or
modifying the Navigator object properties). Further, unless
otherwise speciﬁed, all the requests to reCAPTCHA-protected
websites are made from a single IP address and a single ma-
chine.
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    273
Table 1: Performance of image recognition services.
Image Recognition
Service
Google Cloud Vision
Microsoft Azure Computer
Vision
Amazon Rekognition
Clarifai
Success Rate (%)
Speed (s)
19
34
47
27
16.67
17.90
19.68
15.35
Table 2: Performance of object detection services.
Object Detection Service
Success Rate (%)
Speed (s)
Google Cloud Vision
Microsoft Azure Computer
Vision
Amazon Rekognition
36
31
38
9.47
10.54
9.86
5.2 Preliminary analysis
To do the preliminary analysis, we collect 10,385 re-
CAPTCHA challenges from 8 websites protected by re-
CAPTCHA service from May 2019 to November 2019. Fig-
ure 4 shows frequencies of different object categories in the
collected challenges. As we can see, there are only 19 object
categories, with the top 5 categories representing over 75% of
the total challenges. The top 5 object classes are bus, trafﬁc
light, crosswalk, car, and ﬁre hydrant.
Breaking reCAPTCHA with vision APIs for image recog-
nition. We test 4 popular off-the-shelf online vision APIs
for image recognition. The services we use are Cloud Vi-
sion API provided by Google [7], Azure Computer Vision
API provided by Microsoft [9], Rekognition API provided
by Amazon [2], and the API provided by Clarifai [5]. First,
we select some challenge images from different categories,
extract the individual grids from them, and submit those grids
to the image recognition services to analyze the tags (labels)
returned by them. We ﬁnd that in most cases, one of the la-
bels for a grid holding the target object matches precisely
with the name of the object in the reCAPTCHA challenge
instruction, thus simplifying the process of mapping the tags
returned by an API service to reCAPTCHA challenge object
names while submitting a challenge. However, we ﬁnd one
instance where the labels are not consistent across various
APIs. For example, Amazon Rekognition API classiﬁes re-
CAPTCHA’s crosswalk images as “Zebra Crossings,” while
Google’s Cloud Vision API recognizes them as “Pedestrian
crossings.” We do a simple preprocessing that transforms
these labels to name “crosswalk” for consistency.
Next, we develop a proof-of-concept attack and submit 100
live reCAPTCHA challenges separately using each image
recognition API. Table 1 provides the success rate and speed
of attack using image recognition services. The Google Cloud
Vision provides the lowest success rate, followed by Clarifai.
We note that the attack success rate is below 40% for all the
services except for Amazon Rekognition.
Finally, we manually verify the results and analyze the
failed challenges. We ﬁnd that the image recognition services’
poor performance is due to the complex nature of the cur-
rent challenge images, which often contain complex everyday
scenes with common objects in their natural context. For ex-
ample, we ﬁnd many instances where a potential grid holding
a “crosswalk” also holds other common objects such as “car”
and “trafﬁc light” in it, and tags returned by an API include
names of all the objects except the primary target. Further, in
many challenges, a single target object spans across multiple
grids, and some of those grids contain only a tiny part of the
whole object. Image recognition services failed to identify
the target object in such a scenario. The earlier version of
reCAPTCHA used to show relatively simple images, usually
containing one disparate object per grid or images with simple
scenes having a monotonic background, making it easier for
image recognition services to analyze the contents.
Breaking reCAPTCHA with an image classiﬁer. We also
perform an attack using a Convolutional Neural Network
(CNN) based image classiﬁer. The classiﬁer is trained on
over 98,000 images from 18 classes. These include all object
classes in Figure 4, except the “store front.” Interestingly, the
“store front” class has been phased-out from reCAPTCHA
challenges during the later part of our data collection period.
We then submit 100 live reCAPTCHA challenges using our
image classiﬁer based solver. The success rate and speed of at-
tack are 21% and 16.96 seconds, respectively. After analyzing
the failed challenges manually, we ﬁnd that the same factors
related to the poor performance of the image recognition APIs
contributed equally (or even higher) to the low success rate
of the image classiﬁer based attack.
Breaking reCAPTCHA with online vision APIs for object
detection. We also carry out a proof-of-concept attack using
three off-the-shelf computer vision APIs for object detection
provided by Google, Microsoft, and Amazon. We customize
our bounding box to the grid mapping algorithm to process
the bounding box results for the objects detected by the APIs.
Like before, we submit 100 live reCAPTCHA challenges us-
ing these APIs. Table 2 shows attack performance of each
off-the-shelf object detection API. We can see that the Ama-
zon Rekognition API and Google Cloud Vision API achieve
similar performance, while Microsoft Azure Computer Vi-
sion API performs relatively poorly. We analyze the results
to understand why these services are not effective against re-
CAPTCHA challenges. We ﬁnd several factors that contribute
to low success rates. First, these services can recognize objects
from certain object categories only. However, most of them
can detect objects that frequently appear in the reCAPTCHA
challenges such as “bus”, “car”, “trafﬁc light”, and “bicycle”.
While the top 5 objects in Figure 4 account for around 70% of
the submitted challenges during this experiment, our manual
analysis shows that the object detection APIs fail to identify
at least one target object in these categories in most of the
274    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Table 3: The time required to break a reCAPTCHA challenge
by percentiles.
Percentile
Speed (s)
1st
13.28
5th
13.91
95th
39.76
99th
58.65
failed cases. It suggests that the cloud-based vision APIs for
object detection are still in their early stage of development,
and yet to be ready to handle complex images such as those
found in reCAPTCHA challenges.
5.3 Breaking reCAPTCHA challenges with
our system
Success rate and speed of attack. To evaluate the effective-
ness and efﬁciency of our approach, we submit 800 challenges
to 4 reCAPTCHA-enabled websites using our automated
CAPTCHA-breaking system. Out of them, 701 challenges are
selection-based, 87 challenges are click-based, and 12 are “no
CAPTCHA reCAPTCHA” challenges, where our system gets
veriﬁed simply by clicking on the reCAPTCHA checkbox.
Our system breaks 656 (out of 788) challenges, resulting in a
success rate of 83.25%.
The average speed of breaking a CAPTCHA challenge
is 19.93 seconds, including delays. The delays include net-
work delay to load and download images, which takes about
1~8 seconds depending on CAPTCHA types and artiﬁcially
induced delay between each of the clicks. The minimum, me-
dian, and maximum time needed to break a challenge are
13.11, 14.92, and 89.02 seconds respectively. Table 3 lists
the time required to break a challenge by percentiles. Our
solver module takes about 6.5 seconds to detect objects in a
challenge image regardless of the number of objects being
present.
Attack on selection-based CAPTCHAs. Generally, the