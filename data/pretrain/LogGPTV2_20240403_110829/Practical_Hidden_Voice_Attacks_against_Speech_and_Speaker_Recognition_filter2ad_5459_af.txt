“Behringer,” https://www.musictri.be/brand/behringer/home.
“Bing
cognitive-services/speech/.
“Cloud speech-to-text,” https://cloud.google.com/speech-to-text/.
“Houndify,” https://www.houndify.com/.
“Ibm speech
speech-to-text/.
“Intel implementation of deep speech 2 in neon,” https://github.com/
NervanaSystems/deepspeech.
“Kaldi aspire chain model,” http://kaldi-asr.org/models.html.
“Mozilla
services/cognitive-services/speaker-recognition/.
“Uberi speech recognition modules for python,” https://github.com/
Uberi/speech recognition.
“Wit.ai natural language for developers,” https://wit.ai/.
“ISO 226:2003,”
[Online]. Available: https://www.iso.org/standard/34222.html
https://www.iso.org/standard/34222.html,
https://www.ibm.com/watson/services/
https://azure.microsoft.com/en-us/
deepspeech,”
project
2003.
to
text,”
[16] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,
C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen et al., “Deep
speech 2: End-to-end speech recognition in english and mandarin,” in
International Conference on Machine Learning, 2016, pp. 173–182.
[17] D. Angluin, “Computational
bibliography,” in Proceedings of
Symposium on Theory of Computing,
York, NY, USA: ACM, 1992, pp. 351–369.
http://doi.acm.org/10.1145/129712.129746
learning theory: Survey and selected
the Twenty-fourth Annual ACM
New
[Online]. Available:
ser. STOC ’92.
[18] S. Baluja and I. Fischer, “Adversarial transformation networks: Learning
to generate adversarial examples,” arXiv preprint arXiv:1703.09387,
2017.
[19] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar, “The security
of machine learning,” Machine Learning, vol. 81, no. 2, pp. 121–148,
2010.
learning
Tygar,
of
2006 ACM Symposium on
Sears, A. D.
be
“Can machine
the
[20] M. Barreno, B. Nelson, R.
and
Joseph,
in
secure?”
J. D.
Proceedings
Information,
security - ASIACCS ’06, 2006,
computer and communications
p. 16. [Online]. Available: https://www.cs.drexel.edu/{∼}greenie/cs680/
asiaccs06.pdfhttp://portal.acm.org/citation.cfm?doid=1128817.1128824
[21] L. Blue, L. Vargas, and P. Traynor, “Hello, is it me you‘re looking
for? differentiating between human and electronic speakers for voice
interface security,” in 11th ACM Conference on Security and Privacy
in Wireless and Mobile Networks, 2018.
[22] A. W. Bronkhorst, “The cocktail-party problem revisited: early pro-
cessing and selection of multi-talker speech,” Attention, Perception, &
Psychophysics, vol. 77, no. 5, pp. 1465–1487, 2015.
[23] T. B. Brown, D. Man´e, A. Roy, M. Abadi, and J. Gilmer, “Adversarial
patch,” arXiv preprint arXiv:1712.09665, 2017.
[24] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields,
D. Wagner, and W. Zhou, “Hidden voice commands.” in USENIX
Security Symposium, 2016, pp. 513–530.
[25] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in Security and Privacy (SP), 2017 IEEE Symposium on.
IEEE, 2017, pp. 39–57.
[26] E. C. Cherry, “Some experiments on the recognition of speech, with one
and with two ears,” in The Journal of the Acoustical Society of America,
http://www.ee.columbia.edu/∼dpwe/papers/Cherry53-cpe.pdf,
25th.
1953, p. 975979.
[27] C. Cieri, D. Miller, and K. Walker, “The ﬁsher corpus: a resource for the
next generations of speech-to-text.” in LREC, vol. 4, 2004, pp. 69–71.
[28] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma,
“Adversarial classiﬁcation,” Proceedings of the 2004 ACM SIGKDD
international conference on Knowledge discovery and data mining
- KDD ’04, p. 99, 2004. [Online]. Available: http://portal.acm.org/
citation.cfm?doid=1014052.1014066
[29] R. L. Diehl,
“Acoustic
and auditory phonetics:
adaptive
design of speech sound systems,” Philosophical Transactions of
the Royal Society B: Biological Sciences, vol. 363, p. 965978,
/url{https://www.ncbi.nlm.nih.gov/pmc/
2008.
articles/PMC2606790/pdf/rstb20072153.pdf}
[Online]. Available:
the
[30] H. Fletcher and W. A. Munson, “Loudness, its deﬁnition, measurement
and calculation,” Bell System Technical Journal, vol. 12, no. 4, pp. 377–
430, 1933.
J. S. Garofolo et al., “Getting started with the darpa timit cd-rom: An
acoustic phonetic continuous speech database,” National Institute of
Standards and Technology (NIST), Gaithersburgh, MD, vol. 107, p. 16,
1988.
[31]
[32] S. A. Gelfand, Hearing: An Introduction to Psychological and Physio-
logical Acoustics, 5th ed.
Informa Healthcare, 2009.
[33] S. Getzmann, J. Jasny, and M. Falkenstein, “Switching of auditory
attention in cocktail-party listening: Erp evidence of cueing effects
in younger and older adults,” Brain and Cognition, vol. 111, pp. 1
– 12, 2017. [Online]. Available: http://www.sciencedirect.com/science/
article/pii/S0278262616302408
I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[34]
[35] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with
recurrent neural networks,” in International Conference on Machine
Learning, 2014, pp. 1764–1772.
[36] T. D. Hanley and G. Draegert, “Effect of level of distracting noise
upon speaking rate, duration, and intensity.” PURDUE RESEARCH
FOUNDATION LAFAYETTE IND, Tech. Rep., 1949.
[37] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,
R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep
speech: Scaling up end-to-end speech recognition,” arXiv preprint
arXiv:1412.5567, 2014.
14
[38] B. Hartpence, Packet Guide to Voice over IP, 1st ed. O‘Reilly Media,
Inc, 2013.
[39] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D.
Tygar, “Adversarial machine learning,” in Proceedings of
the 4th
ACM Workshop on Security and Artiﬁcial Intelligence, ser. AISec ’11.
New York, NY, USA: ACM, 2011, pp. 43–58. [Online]. Available:
http://doi.acm.org/10.1145/2046684.2046692
[40] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
physical world,” arXiv preprint arXiv:1607.02533, 2016.
[41] P. Lamere, P. Kwok, W. Walker, E. Gouvea, R. Singh, B. Raj, and
P. Wolf, “Design of the cmu sphinx-4 decoder,” in Eighth European
Conference on Speech Communication and Technology, 2003.
[42] B. P. Lathi and Z. Ding, Modern Digital and Analog Communication
Systems, 4th ed. Oxford University Press, 2009.
[43] C. Limb,
“Building
2011.
[Online]. Available: \url={https://www.ted.com/talks/charles limb
building the musical muscle#t-367224}
the musical muscle,” TEDMED,
[44] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” Proceedings of the 2017 Network
and Distributed System Security Symposium (NDSS), 2017.
[45] S. Maheshwari, “Burger King ‘O.K. Google’ Ad Doesn’t Seem
O.K. With Google,” https://www.nytimes.com/2017/04/12/business/
burger-king-tv-ad-google-home.html, 2017.
[46] R. Mannell, “The perceptual and auditory implications of parametric
scaling in synthetic speech,” Macquarie University, p. (Chapter 2),
1994. [Online]. Available: \url{http://clas.mq.edu.au/speech/acoustics/
auditory representations/pitchdiscrim.html}
[47] C. Martin, “72% Want Voice Control In Smart-Home Products,”
Media Post –https://www.mediapost.com/publications/article/292253/
72-want-voice-control-in-smart-home-products.html?edition=99353,
2017.
[48] S. M. Moosavi Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in Proceedings of
2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), no. EPFL-CONF-218057, 2016.
J. Newsome, B. Karp, and D. Song, “Paragraph: Thwarting signature
learning by training maliciously,” in Recent Advances in Intrusion
Detection, D. Zamboni and C. Kruegel, Eds.
Berlin, Heidelberg:
Springer Berlin Heidelberg, 2006, pp. 81–105.
[49]
[50] H. Newton and S. Schoen, Newton’s Telecom Dictionary, 30th ed.
Harry Newton, 2016.
[51] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily
fooled: High conﬁdence predictions for unrecognizable images,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 427–436.
[52] S. Nichols, “TV anchor says live on-air ‘Alexa, order me a dollhouse’-
Guess what happens next,” https://www.theregister.co.uk/2017/01/07/
tv-anchor-says-alexa-buy-me-a-dollhouse-and-she-does/, 2017.
[53] W. G. on Speech Understanding and Aging, “Speech understanding and
aging,” The Journal of the Acoustical Society of America, vol. 83, no. 3,
pp. 859–895, 1988.
[54] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an
asr corpus based on public domain audio books,” in Acoustics, Speech
and Signal Processing (ICASSP), 2015 IEEE International Conference
on.
IEEE, 2015, pp. 5206–5210.
[55] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in Security and Privacy (EuroS&P), 2016 IEEE European Symposium
on.
IEEE, 2016, pp. 372–387.
[56] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difﬁculty of training
recurrent neural networks,” in Proceedings of the 30th International
Conference on Machine Learning, ser. Proceedings of Machine
Learning Research, S. Dasgupta and D. McAllester, Eds., vol. 28,
no. 3. Atlanta, Georgia, USA: PMLR, 17–19 Jun 2013, pp. 1310–1318.
[Online]. Available: http://proceedings.mlr.press/v28/pascanu13.html
[57] D.
J. Plude,
J. T. Enns, and D. Brodeur, “The development
of selective attention: A life-span overview,” Acta Psychologica,
vol. 86, no. 2, pp. 227 – 272, 1994.
[Online]. Available:
http://www.sciencedirect.com/science/article/pii/0001691894900043
[58] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stem-
mer, and K. Vesely, “The kaldi speech recognition toolkit,” in IEEE
2011 Workshop on Automatic Speech Recognition and Understanding.
IEEE Signal Processing Society, 2011, iEEE Catalog No.: CFP11SRW-
USB.
[59] F. Pulvermuller and Y. Shtyrov, “Language outside the focus of
attention: The mismatch negativity as a tool for studying higher
cognitive processes,” Progress in Neurobiology, vol. 79, no. 1, pp. 49
– 71, 2006. [Online]. Available: http://www.sciencedirect.com/science/
article/pii/S0301008206000323
[60] L. R. Rabiner, “A tutorial on hidden markov models and selected
applications in speech recognition,” in PROCEEDINGS OF THE IEEE,
1989, pp. 257–286.
[61] L. R. Rabiner and R. W. Schafer, Digital processing of speech signals.
Prentice Hall, 1978.
J. Ram´ırez, J. M. G´orriz, and J. C. Segura, “Voice activity detection.
fundamentals and speech recognition system robustness,” in Robust
speech recognition and understanding, 2007.
[62]
[63] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition,”
in Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2016, pp. 1528–1540.
auditory
visual
attention,” in Trends in Cognitive Science, 2008, pp. 182–186.
[Online]. Available: \url{http://www.cns.bu.edu/∼shinn/resources/pdfs/
2008/2008TICS Shinn.pdf}
[64] B. G. Shinn-Cunningham,
“Object-based
and
[65] H.
Stephenson,
“UX design
to a need to not
interfaces
https://www.digitalartsonline.co.uk/features/interactive-design/
ux-design-trends-2018-from-voice-interfaces-need-not-trick-people/,
2018.
trick people,” Digital Arts
from voice
-
trends
2018:
[66] A. Stojanow and J. Liebetrau, “A review on conventional psychoacoustic
evaluation tools, methods and algorithms,” in 2016 Eighth International
Conference on Quality of Multimedia Experience (QoMEX), June
2016, pp. 1–6. [Online]. Available: https://ieeexplore.ieee.org/document/
7498923/
J. Su, D. V. Vargas, and S. Kouichi, “One pixel attack for fooling deep
neural networks,” arXiv preprint arXiv:1710.08864, 2017.
[67]
[68] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-
low, and R. Fergus, “Intriguing properties of neural networks,” arXiv
preprint arXiv:1312.6199, 2013.
[69] L. Torrey and J. Shavlik, “Transfer learning,” in Handbook of Research
on Machine Learning Applications and Trends: Algorithms, Methods,
and Techniques.
IGI Global, 2010, pp. 242–264.
[70] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine noodles:
exploiting the gap between human and machine speech recognition,”
WOOT, vol. 15, pp. 10–11, 2015.
[71] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and
K. Saenko, “Translating videos to natural language using deep recurrent
neural networks,” arXiv preprint arXiv:1412.4729, 2014.
[72] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang,
H. Huang, X. Wang, and C. A. Gunter, “Commandersong: A systematic
approach for practical adversarial voice recognition,” in Proceedings of
the USENIX Security Symposium, 2018.
[73] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu, “Dol-
phinattack: Inaudible voice commands,” in Proceedings of the 2017
ACM SIGSAC Conference on Computer and Communications Security.
ACM, 2017, pp. 103–117.
[74] L. Zhang, S. Tan, and J. Yang, “Hearing your voice is not enough: An
articulatory gesture based liveness detection for voice authentication,”
in Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2017, pp. 57–71.
[75] L. Zhang, S. Tan, J. Yang, and Y. Chen, “Voicelive: A phoneme localiza-
tion based liveness detection for voice authentication on smartphones,”
in Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2016, pp. 1080–1091.
15