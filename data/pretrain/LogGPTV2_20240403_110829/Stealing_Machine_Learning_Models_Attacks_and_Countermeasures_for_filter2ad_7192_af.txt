Compute real image’s density ratio:
r(x) = C(D(x))
1−C(D(x))
Compute fake image’s density ratio:
r(x′) = C(D(x′))
1−C(D(x′))
r(x′)
p = min(1,
r(x) )
if u ≤ p then
x ← x′
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end while
end if
end if
end for
if x is not a real images then
Append(x, imaдes)
Table 7: Performance of fidelity extraction attack with dif-
ferent prior distributions. We use standard normal distribu-
tion and uniform distribution over an interval -1 and 1 to
generate latent codes. The number of queries is fixed to 50K.
Attack model
Prior distribution
SNGAN
SNGAN
PGGAN
PGGAN
Gaussian
Uniform
Gaussian
Uniform
Fidelity
FID( ˜pд, pд)
4.49
4.29
1.02
0.98
Accuracy
FID ( ˜pд, pr )
9.29
9.16
4.93
4.85
A APPENDIX
A.1 Implementation Details
We implement PGGAN5 and SNGAN6 based on following codes
indicated in the footnotes. We choose the ResNet architecture for
SNGAN and the architecture of PGGAN is the same as the official
implementation. We use hinge loss for SNGAN and WGAN-GP loss
for PGGAN. For target GAN on synthetic data in Figure 5, we use
four fully connected layers with ReLU activation for both generator
and discriminator and the prior is a 2-dimensional standard normal
distribution. The training data is a mixture of 25 2-D Gaussian
distributions (each with standard deviation of 0.05). We train it using
standard loss function [19]. In Section 7 about case study, we directly
use the pretrained StyleGAN7 trained on LSUN-Bedroom dataset as
our target model. We resize all images used in our paper to 64 × 64,
except for the case study where images with a resolution of 256×256
are used. The dimension of latent space of SNGAN, PGGAN and
StyleGAN is 256, 512 and 512, respectively, and their latent codes
are all draw from standard Gaussian distribution. For attack models,
we use suggested hyperparameters provided by original models
and only modify some related to computing resources.
In Section 8 about semantic interpolation defense, the seman-
tic information is from attributes of CelebA dataset8, which has
labeled for each image. we only choose 12 (male, smiling, wearing
lipstick, mouth slightly open, wavy hair, young, eyeglasses, wearing
hat, black hair, receding hairline, bald, mustache) out of 40 facial
attributes to learn semantic hyperplanes, because the number of
images for each attribute varies largely and some attributes is hard
to distinguish when they are applied in target GAN model. We train
the prediction model for each attribute based on ResNet-50 model
pretrained on ImageNet9. The magnitude of semantic interpolation
is set as 3.
A.2 MH Algorithm
Algorithm 1 shows the MH subsampling algorithm [65]. Inputs
of this algorithm are a target generator (only used to query), a
white-box discriminator which is used to subsample generated
samples and partial real samples which are used to calibrate the
discriminator (Algorithm 1, line 2). Outputs are refined samples
whose distribution is much closer to distribution of real training
data.
A.3 Attack Performance on Queries From
Different Prior Distributions.
Adversaries can query the target model via trying common prior
distributions to generate latent codes if they do not know the prior
distribution of a target model. Gaussian distribution and uniform
distribution are widely used in almost all GANs [6, 31–33, 47, 53].
Table 7 shows the attack performance with two prior distributions.
We choose PGGAN trained on CelebA dataset with standard normal
prior distribution as the target model. From Table 7, we find that
5https://github.com/tkarras/progressive_growing_of_gans
6https://github.com/christiancosgrove/pytorch-spectral-normalization-gan
7https://github.com/NVlabs/stylegan
8http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html
9https://download.pytorch.org/models/resnet50-19c8e357.pth
Figure 9: Class distribution differences among the training
data, the target model SNGAN, and attack models.
adversaries can obtain a similar attack performance no matter what
the prior distribution of latent codes is.
0510152025300.000.020.040.060.080.10Proportionstarget model SNGANattack model PGGANattack model SNGANtraining data13ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Hailong Hu and Jun Pang
Table 8: JS distances between models. For the JS distance
between training data and the target model, and the target
model SNGAN is 16.36×10−3. The JS value shows a consistent
trend with Figure 9.
Target model Attack model
SNGAN
SNGAN
PGGAN
JSfidelity (×10−3)
8.90
1.60
JSaccuracy (×10−3)
34.12
18.56
Table 9: JS distances between models. For the JS distance be-
tween training data and the target model, the target model
PGGAN on CelebA is 4.14×10−3 and the target model PGGAN
on LSUN-Church is 14.78 × 10−3.
Dataset
Methods
CelebA
LSUN-Church
Black-box fidelity extraction
MH accuracy extraction
White-box accuracy extraction
Black-box fidelity extraction
MH accuracy extraction
White-box accuracy extraction
JSf idelity (×10−3)
1.83
1.42
1.17
2.32
2.28
1.61
JSaccur acy (×10−3)
9.10
8.17
7.53
19.14
19.89
18.65
(a) The target model PGGAN trained on CelebA.
Figure 11: Semantic interpolation defense.
(b) The target model PGGAN trained on LSUN-Church.
Figure 10: Distribution differences for accuracy extraction.
A.4 Additional Results for Analyzing
Distribution Differences
A.4.1 Understanding Fidelity Extraction for the Target
Model SNGAN. Figure 9 shows distribution differences for the tar-
get model SNGAN trained on CelebA dataset. Table 8 summarizes
these differences statistically.
A.4.2 Understanding Accuracy Extraction on GANs In-
depth. Following the same procedure illustrated in Section 5.3.3,
we also dissect distribution differences for accuracy extraction.
Specifically, we choose the PGGAN-PGGAN case as an example (see
Figure 6) and the attack models is PGGAN. From the Figure 10, we
observe that for CelebA, white-box accuracy extraction which has
minimal accuracy values among these methods is more consistent
with the distribution of the training data by lowering the highest
proportions of classes. For LSUN-Church, similar results also can
be observed. Table 9 summarizes these differences statistically.
[55].
A.5 Defense Techniques
Figure 12: The performance of attack model SNGAN under
various defenses for the black-box fidelity extraction sce-
nario.
Semantic Interpolation Defense. The process of semantic
A.5.1
interpolation defense is shown in Figure 11. Semantic interpola-
tion defense consists of two phases: finding semantic hyperplanes
and generating semantic images. In the first phase, we first train a
prediction model for each semantic information. Then the trained
prediction model is used to predict semantic score s for each image
generated through latent code z. As a result, we get latent code-
score pairs and label the highest k scores as positive and the lowest
k scores as negative. Finally, we train a linear support vector ma-
chine (SVM) on dataset where latent codes as training data and
scores as labels. A trained linear SVM contains a hyperplane which
separates one semantic information. In the second phase, we can
0510152025300.000.020.040.060.08Proportionstarget model PGGANBlack-box fidelity extractionMH accuracy extractionWhite-box accuracy extractiontraining data0510152025300.000.020.040.060.080.10Proportionstarget model PGGANBlack-box fidelity extractionMH accuracy extractionWhite-box accuracy extractiontraining dataLatent code zGenerated imagePredictionmodelLinear SVMSemantic hyperplane(z1, s1), (z2, s2),…,(zn, sn)TargetGANScore sPhase 1: Finding semantic hyperplanesTargetGANLatent code zSemantichyperplanesLatent codes z1, z2, ..., zmSemantic imagesPhase 2: Generating semantic images10k30k50k70k90kQueries48121620242832AccuracyNo DefenseSemanticLinearGaussian NoiseJPEG CompressionGaussian FilterAdversarial Noise14Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Table 10: Defense utility. Each score is an average of 50K im-
age score. Lower is better.
Metrics
NIQE
PIQE
Metrics
NIQE
PIQE
No Defense
18.87
42.66
Semantic
18.87
40.04
Linear
18.87
42.62
Gaussian Noise
18.87
23.64
JPEG Compression Gaussian Filter Adversarial Noise
18.87
35.99
18.87
47.80
18.87
37.91
(a) The performance of attack model PGGAN
(b) The performance of attack model SNGAN
Figure 13: Fidelity of attack models under different defenses
for black-box fidelity extraction scenarios. Fidelity values of
attack models can be largely decreased with an increase in
the number of queries.
obtain a semantic image for each semantic hyperplane through
interpolation. A latent code interpolates points along the normal
vector of the hyperplane and corresponding semantic images can
be obtained.
In our experiments, we train each prediction model for each
semantic information, and prediction model is built on the basis of
ResNet-50 network [21] trained on ImageNet dataset
A.5.2 Defense Utility. We quantitatively and qualitatively evaluate
the defense utility, i.e. the quality of generated images after de-
ploying defense measures. Figure 14 and Figure 15 show returned
images for input perturbation-based and output perturbation-based
defenses. Table 10 shows image quality scores. We use two widely-
adopted no-reference image quality scores: Naturalness Image Qual-
ity Evaluator (NIQE) [46] and Perception based Image Quality Eval-
uator (PIQE) [67]. Overall, our defense measures do not impact the
quality of generated images.
A.5.3 Attack Performance for the Attack Model SNGAN under Var-
ious Defense Techniques. Figure 12 shows that the performance
of attack model SNGAN under various defenses for the black-box
fidelity extraction scenario.
Fidelity on Various Defense Techniques. Figure 13 shows
A.5.4
fidelity of attack models under different defenses for black-box
fidelity extraction scenario. We observe that fidelity values of attack
models can be largely decreased with an increase in the number of
queries.
10k30k50k70k90kQueries1471013FidelityNo DefenseSemanticLinearGaussian NoiseJPEG CompressionGaussian FilterAdversarial Noise10k30k50k70k90kQueries36912151821FidelityNo DefenseSemanticLinearGaussian NoiseJPEG CompressionGaussian FilterAdversarial Noise15ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Hailong Hu and Jun Pang
(a) Linear interpolation defense. These linear interpolated images are returned.
(b) Semantic interpolation defense. For one latent code, 12 latent codes containing semantic information are generated through semantic interpolation and corresponding images are
shown above. These semantic interpolated images are returned.
Figure 14: Returned images after input perturbation-based defense techniques. Queried images and interpolated images both
show good quality in visual comparison, and images generated by linear interpolation show more similarity than that by
semantic interpolation.
(a) Output images. From top to bottom: generated images, Gaussian noise
images, Adversarial noise images, Gaussian filter images and JPEG compression
images.
(b) Noises. For the top two rows, they are Gaussian noises and adversarial
noises, respectively. For the third row, it is the differences between Gaussian
filter images and generated images. For the last row, it is the differences between
JPEG compression images and generated images.
Figure 15: Returned images after output perturbation-based defense techniques.
Query 1   Linear interpolation      Query 2QueryBaldBlack hairEyeglassesMouth slightly openMustacheReceding hairlineWavy hairWearing hatYoungMaleSmilingWearing lipstick16