binomial random variable we denote by Yi ∼ Bin(N , 1/k). Our goal
is to show that getting Yi  E[A] · λ] ≤ e
−pN E[A](λ−1−ln λ)
.
Additionally, we will use the following fact.
FACT 7. For any positive real number ε ∈ R+,
√
2ε − ln(1 + ε +
√
2ε) ≥ 1 + ε.
1 + ε +
We now prove our result.
THEOREM 8. Let E[A] = r(Hr − Hr−N ) denote the expected
number of samples required for seeing N distinct coupons. With
probability 1 − δ, the number of samples required for seeing at least
N distinct coupons is at most
(cid:115)
E[A] + r ln δ−1
(r − N) +
2r E[A] ln δ−1
(r − N)
.
PROOF. We wish to use Theorem 6; notice that we need λ−ln λ ≥
pN E[A] which implies
1 + ln δ −1
Plugging in pN = (1 − (N − 1)/r) > (r − N)/r we have that the
required number of required packets is at most
pN E[A] , it is enough to set
−pN E[A](λ−1−ln λ) ≤ δ .
e
(cid:115)
pN E[A] +
2 ln δ−1
pN E[A] .
λ · E[A] =(cid:169)(cid:173)(cid:171)E[A] + ln δ−1
According to Fact 7, for ε = ln δ −1
λ = 1 + ln δ−1
(cid:115)
≤(cid:169)(cid:173)(cid:171)E[A] + r ln δ−1
−1 +(cid:112)4E[A] ln δ−1(cid:17)
≈(cid:16)
−1 + 2.35(cid:112)
E[A] + 2 ln δ
2E[A] ln δ−1
1.39N + 2 ln δ
(r − N) +
(cid:170)(cid:174)(cid:172)
(cid:115)
(cid:16)
pN
pN
+
2r E[A] ln δ−1
(r − N)
(cid:170)(cid:174)(cid:172) .
N ln δ−1(cid:17)
. □
For example, if r = 2N , we have E[A] ≈ 1.39N and the number of
packets is
Next, we show a bound on the number of samples needed to
collect K(1 − ψ) in a Coupon Collector process [24] on K coupons.
LEMMA 9. Let K ∈ N+ and ψ ∈ (0, 1/2]. The number of samples
required for collecting all but ψK coupons is at most
K lnψ
−1 + ψ
−1 ln δ
−1 +
2Kψ−1 lnψ−1 ln δ−1
= O(K lnψ
−1 + ψ
−1 ln δ
−1).
(cid:113)
PROOF. For i = 1, . . . ,K(1 − ψ), let Ai ∼ Geo(1 − (i − 1)/K)
denote the number of samples we need for getting the i’th distinct
coupon, and let A =K(1−ψ)
Ai . We have that
K(1−ψ)
i =1
i =1
K
K − (i − 1) = K(cid:16)
E[A] =
(cid:17)
HK − HKψ
= K lnψ
−1.
17
According to Theorem 8, it is enough to obtain the following
number of samples
E[A] +
K ln δ−1
K(1 − (1 − ψ)) +
−1 + ψ
= K lnψ
2KE[A] ln δ−1
K(1 − (1 − ψ))
−1 ln δ
(cid:113)
Finally, we note that(cid:112)Kψ−1 lnψ−1 ln δ−1 is the geometric mean of
2Kψ−1 lnψ−1 ln δ−1.
−1 +
K lnψ−1 and ψ−1 ln δ−1 and thus:
(cid:115)
(cid:113)
K lnψ
−1 + ψ
−1 ln δ
≤(cid:16)K lnψ
−1 +
−1 + ψ
2Kψ−1 lnψ−1 ln δ−1
−1 ln δ
−1(cid:17) (1 + 1/√
(cid:16)K lnψ
2)
−1 + ψ
= O
−1(cid:17)
−1 ln δ
.
□
A.2.2 Analysis of the algorithm. We denote by d ≜ d
log∗ d the
number of hops we aim to decode using the XOR layers. Our algo-
rithm has ⌈log∗ d⌉ +1 layers, where layer 0 runs the Baseline scheme
and the remaining L ≜ ⌈log∗ d⌉ layers use XOR. We denote by ↑↑
Knuth’s iterated exponentiation arrow notation, i.e., x ↑↑ 0 = 1 and
y-times.
x ↑↑ y = xxx···x (cid:27)
pℓ = e ↑↑ (ℓ − 1)
(cid:17)
(cid:16)
d
.
The sampling probability in layer ℓ is then set to
1 −
Each packet it hashed to choose a layer, such that layer 0 is chosen
= 1 − o(1) and otherwise one
with probability τ =
of layers 1, . . . , L is chosen uniformly. The pseudo code for the final
solution is given in Algorithm 1.
Algorithm 1 PINT Processing Procedure at Switch s
1+log log∗ d
1
Input: A packet pj with b-bits digest pj .dig.
Output: Updated digest pj .dig.
Initialization:
1+log log∗ d , ∀ℓ ∈ {1, . . . , L} : pℓ = e↑↑(ℓ−1)
τ = log log∗ d
.
Let i such that the current switch is the i′th so far
d
1: H ← H(pj)
2: if H < τ then
3:
4:
5: else
6:
ℓ ←(cid:108)L · H−τ
if д(pj , i) < 1/i then
pj .dig ← h(s, pj)
(cid:109)
1−τ
if д(pj , i) < pℓ then
7:
8:
pj .dig ← pj .dig ⊕ h(s, pj)
▷ Distributed uniformly on [0, 1]
▷ Update layer 0
▷ Sample with probability 1/i
▷ Choose the layer
▷ Xor w.p. pℓ
For simplicity, we hereafter assume in the analysis that a packet
can encode an entire identifier. This assumption is not required in
practice and only serves for the purpose of the analysis. We note
that even under this assumption the existing approaches require
O(k log k) packets. In contrast, we show that except with probability
δ = e−O(k0.99) the number of packets required for decoding a k-hops
path in our algorithm is just
X = k log log∗
k · (1 + o(1)).11
Note that log log∗ k is a function that grows extremely slowly, e.g.,
log log∗ P < 2 where P is the number of atoms in the universe.
Our assumption on the error probability δ allows us to simplify the
expressions and analysis but we can also show an
(cid:16)
k log log∗
k + log∗
k log δ
O
−1(cid:17)
bound on the required number of packets thus the dependency on δ
is minor.
For our proof, we define the quantities
we get that after
k ln log∗
k + log∗
k ln δ
−1 +
Q ≜ k1 + ln
+
2k1 ln
(cid:18) 4 log∗ k1
(cid:19)
δ
(cid:115)
(cid:18)
k
log∗ k
(cid:17)
(cid:16) 4L
δ
= O
(cid:114)
(cid:17)
+
(cid:16) 4L
δ
2Q ln
(cid:18)
c · e−c
= O
and
Q + 2 ln
S ≜
(cid:18) 4 log∗ k1
δ
+ log δ
−1
(cid:19)
(cid:19)
(cid:18)
= O
(cid:19)
k
log∗ k
(cid:19)
(cid:18)
= O