scores for downstream tasks. For utility measurement, we evaluate
the adversarially trained embeddings on four sentence analysis
benchmarks: multi-perspective question answering (MPQA) [73],
text retrieval (TREC) [37], subjectivity analysis (SUBJ) [52] and
Microsoft Research paraphrase corpus (MSRP) [14]. We treat all
benchmarks as classification problem and train a logistic regression
model for each task following previous works [33, 39].
Figure 6 shows the results for adversarial training against in-
version. As λw increases, the adversary performance drops for all
models. White-box inversion attacks drop most significantly. LSTM
embedding models needs a larger λw than Transformer models to
achieve similar mitigation, possible due to the fact that Transformer
models have more capacity and are more capable of learning two
tasks. Utility scores on the benchmarks drop more drastically on
embedding models using LSTM than Transformer due to the larger
value of λw .
Figure 7 shows the results for adversarial training against au-
thorship inference. As λs increases, the adversary performance on
inferring authorship drops significantly for both inference attack
models trained with 10 and 50 labeled data per author. Nearly all
utility scores on the four benchmarks remain rather stable for dif-
ferent λs’s. This also demonstrates that different adversary tasks
can have different impact on the utility of embeddings. In our case,
removing input word information from embeddings is a harder task
than removing authorship and thus will have a larger impact on
the utility.
8 RELATED WORK
Privacy in deep representations. Prior works demonstrate that
representations from supervised learning models leak sensitive
attributes about input data that are statistically uncorrelated with
the learning task [68], and gradient updates in collaborative training
(which depend on hidden representations of training inputs) also
leak sensitive attributes [45]. In contrast, this work focuses on
leakage in unsupervised text embedding models and considers
leakage of both sensitive attributes and raw input text. In addition,
we consider a more realistic scenario where the labeled data is
limited for measuring sensitive attributes leakage which is not
evaluated in prior works.
Recently and concurrently with this work, Pan et al. [51] also
considered the privacy risks of general purpose language models
and analyzed model inversion attacks on these models. Our work
presents and develops a taxonomy of attacks, which is broader in
scope than their work. Our work on inversion also assumes no
structures or patterns in input text, which is the main focus of
their work, and our work shows that we still recover substantial
portion of input data. Additional structural assumptions will lead
to a higher recovery rate as is shown in their work.
There is a large body of research on learning privacy-preserving
deep representations in supervised models. One popular approach
is through adversarial training [17, 74] as detailed in Section 7. The
same approach as been applied in NLP models to remove sensi-
tive attributes from the representation for privacy or fairness con-
cern [9, 18, 38]. Another approach for removing sensitive attributes
is through directly minimizing the mutual information between the
sensitive attributes and the deep representations [47, 50]. We adopt
the adversarial training approach during training embedding mod-
els as defense against embedding inversion and sensitive attribute
inference attacks.
Inverting deep representations. In computer vision community,
inverting deep image representation has been studied as a way for
understanding and visualizing traditional image feature extractor
and deep convolutional neural networks. Both optimization-based
approach [41, 69] and learning-based approach [15, 16] as been
proposed for inverting image representations. In contrast, we fo-
cus on text domain data which is drastically different than image
domain data. Text data are discrete and sparse in nature while im-
ages are often considered as continuous and dense. Our proposed
inversion methods are tailored for unsupervised text embedding
models trained with recurrent neural networks and Transformers.
Model inversion attacks [19] use gradient based method to re-
construct the input data given a classifier model and a class label.
The reconstruction is often a class representatives from the training
data, e.g. averaged face images of female for a gender classifier [45].
Embedding inversion, on the other hand, takes the representation
vector as input and reconstruct the exact raw input text.
Membership inference and memorization. Membership infer-
ence attacks (MIA) have first been studied against black-box super-
vised classification models [66], and later on generative models and
language models [23, 67]. MIA is closely connected to generaliza-
tion where overfitted models are prone to the attacks [77]. In this
work, we extended the study of MIA to unsupervised word and sen-
tence embedding models without a clear notion of generalization
for such models.
It has been shown that deep learning models have a tendency
to memorize [78]. Later work showed that adversaries can extract
formatted training text from the output of text generation mod-
els [4], indicating a real privacy threat caused by memorization. In
this work, we focus on embedding models where the output is an
embedding vector without the possibility of extract training data
directly from the output as in text generation models. Instead, we
demonstrated how to measure the memorization in embeddings
through MIA.
Differential Privacy. Differentially-private (DP) training of ML
models [1, 44] involves clipping and adding noise to instance-level
gradients and is designed to train a model to prevent it from mem-
orizing training data or being susceptible to MIA. DP, which limits
how sensitive the model is to a training example does not provide a
defense against attacks that aim to infer sensitive attributes (which
is an aggregate property of training data). The noisy training tech-
niques are challenging for models with large parameters on large
amounts of data and sensitive to the hyper-parameters [53]. Our
embeddings, with over 10 million parameters in the word embed-
ding matrices V make DP training and hyper-parameter tuning
computationally infeasible. Therefore, we leave it to future work to
explore how to efficiently train embeddings with DP.
9 CONCLUSIONS
In this paper, we proposed several attacks against embedding mod-
els exploring different aspects of their privacy. We showed that
embedding vectors of sentences can be inverted back to the words
in the sentences with high precision and recall, and can also re-
veal the authorship of the sentences with a few labeled examples.
Embedding models can also leak moderate amount of membership
information for infrequent data by using similarity scores from
embedding vectors in context. We finally proposed defenses against
the information leakage using adversarial training and partially
mitigated the attacks at the cost of minor decrease in utility.
Given their enormous popularity and success, our results strongly
motivate the need for caution and further research. Embeddings not
only encode useful semantics of unlabeled data but often sensitive
information about input data that might be exfiltrated in various
ways. When the inputs are sensitive, embeddings should not be
treated as simply “vectors of real numbers.”
[9] Maximin Coavoux, Shashi Narayan, and Shay B Cohen. 2018. Privacy-preserving
[3] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.
[11] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for
[12] Amine Dadoun, Raphaël Troncy, Olivier Ratier, and Riccardo Petitti. 2019. Loca-
REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security. 308–318.
[2] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. POPL (2019).
Enriching Word Vectors with Subword Information. TACL (2017).
[4] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019.
The secret sharer: Evaluating and testing unintended memorization in neural
networks. In USENIX Security.
[5] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St
John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al.
2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175 (2018).
[6] Jianfeng Chi, Emmanuel Owusu, Xuwang Yin, Tong Yu, William Chan, Patrick
Tague, and Yuan Tian. 2018. Privacy Partitioning: Protecting User Data During
the Deep Learning Inference Phase. arXiv preprint (2018).
[7] Muthuraman Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yun-Hsuan
Sung, Brian Strope, and Ray Kurzweil. 2018. Learning cross-lingual sentence
representations via a multi-task dual-encoder model. arXiv preprint (2018).
[8] Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and
Jimeng Sun. 2016. Doctor ai: Predicting clinical events via recurrent neural
networks. In MLHC.
Neural Representations of Text. In EMNLP.
[10] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine
Bordes. 2017. Supervised Learning of Universal Sentence Representations from
Natural Language Inference Data. In EMNLP.
youtube recommendations. In RecSys.
tion Embeddings for Next Trip Recommendation. In WWW.
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
NAACL.
of sentential paraphrases. In International Workshop on Paraphrasing.
similarity metrics based on deep networks. In CVPR.
with convolutional networks. In CVPR.
adversary. In ICLR.
attributes from text data. In EMNLP.
[19] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. In CCS.
[20] Yaroslav Ganin and Victor Lempitsky. 2015. Unsupervised domain adaptation by
backpropagation. In ICML.
networks. In KDD.
[22] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich
Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al.
2014. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint
(2014).
[23] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2019.
LOGAN: Membership inference attacks against generative models. PETS (2019).
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In CVPR.
Chua. 2017. Neural collaborative filtering. In WWW.
[26] Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-hsuan Sung, László Lukács,
Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient natural
language response suggestion for smart reply. arXiv preprint (2017).
[27] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation (1997).
[28] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe,
Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W
Craig. 2008. Resolving individuals contributing trace amounts of DNA to highly
complex mixtures using high-density SNP genotyping microarrays. PLoS genetics
(2008).
[29] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization
[21] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
[14] William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus
[17] Harrison Edwards and Amos Storkey. 2015. Censoring representations with an
[15] Alexey Dosovitskiy and Thomas Brox. 2016. Generating images with perceptual
[16] Alexey Dosovitskiy and Thomas Brox. 2016. Inverting visual representations
[18] Yanai Elazar and Yoav Goldberg. 2018. Adversarial removal of demographic
[25] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
with gumbel-softmax. In ICLR.
[30] Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew Tomkins,
Balint Miklos, Greg Corrado, Laszlo Lukacs, Marina Ganea, Peter Young, et al.
2016. Smart reply: Automated response suggestion for email. In KDD.
[31] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In
[32] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
[34] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
[33] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun,
EMNLP.
mization. arXiv preprint (2014).
Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In NeurIPS.
cation with deep convolutional neural networks. In NeurIPS.
[35] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning
of language representations. arXiv preprint (2019).
[36] Meng Li, Liangzhen Lai, Naveen Suda, Vikas Chandra, and David Z Pan. 2017.
PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network
Training. arXiv preprint (2017).
[37] Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING.
[38] Yitong Li, Timothy Baldwin, and Trevor Cohn. 2018. Towards Robust and Privacy-
[39] Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for
preserving Text Representations. In ACL.
learning sentence representations. In ICLR.
[40] Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu
Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn
Dialogue Systems. In SIGDIAL.
representations by inverting them. In CVPR.
~mmahoney/compression/text.html
[41] Aravindh Mahendran and Andrea Vedaldi. 2015. Understanding deep image
[42] Matt Mahoney. 2009. Large text compression benchmark. https://cs.fit.edu/
[44] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017. Learning
[47] Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, and Greg Ver Steeg.
[43] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-Efficient Learning of Deep Net-
works from Decentralized Data. In AISTATS.
differentially private recurrent language models. In ICLR.
[45] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. 2019.
Exploiting unintended feature leakage in collaborative learning. In Symposium
on Security and Privacy (S&P).
[46] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
NeurIPS.
2018. Invariant representations without adversarial training. In NeurIPS.
[48] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Comprehensive privacy
analysis of deep learning: Stand-alone and federated learning under passive and
active white-box inference attacks. arXiv preprint (2018).
basis set: A strategy employed by V1? Vision research (1997).
[50] Seyed Ali Osia, Ali Taheri, Ali Shahin Shamsabadi, Minos Katevas, Hamed Had-
dadi, and Hamid RR Rabiee. 2018. Deep private-feature extraction. TKDE (2018).
[51] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy Risks of General-
Purpose Language Models. In 2020 IEEE Symposium on Security and Privacy (SP).
IEEE, 1314–1331.
[52] Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using
[49] Bruno A Olshausen and David J Field. 1997. Sparse coding with an overcomplete
subjectivity summarization based on minimum cuts. In ACL.
[53] Nicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Ulfar
Erlingsson. 2020. Making the Shoe Fit: Architectures, Initializations, and Tuning
for Learning with Privacy. https://openreview.net/forum?id=rJg851rYwH
[54] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In EMNLP.
[55] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language models are unsupervised multitask learners. arXiv
preprint (2019).
[56] Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nissan Hajaj, Michaela
Hardt, Peter J Liu, Xiaobing Liu, Jake Marcus, Mimi Sun, et al. 2018. Scalable
and accurate deep learning with electronic health records. NPJ Digital Medicine
(2018).
[57] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad:
100,000+ questions for machine comprehension of text. arXiv preprint (2016).
[58] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. In EMNLP.
[59] Sebastian Ruder, Parsa Ghaffari, and John G Breslin. 2016. Character-level and
multi-channel convolutional neural networks for large-scale authorship attribu-
tion. arXiv preprint (2016).
[60] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and
Herve Jegou. 2019. White-box vs Black-box: Bayes Optimal Strategies for Mem-
bership Inference. In ICML.
[61] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. 2018. Ml-leaks: Model and data independent membership infer-
ence attacks and defenses on machine learning models. arXiv preprint (2018).
[62] Sriram Sankararaman, Guillaume Obozinski, Michael I Jordan, and Eran Halperin.
2009. Genomic privacy and limits of individual detection in a pool. Nature genetics
(2009).
[64] Allen Schmaltz, Alexander M Rush, and Stuart M Shieber. 2016. Word Ordering
[63] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and
Hrishikesh Khandeparkar. 2019. A Theoretical Analysis of Contrastive Un-
supervised Representation Learning. In ICML.
Without Syntax. In EMNLP.
[65] Rakshith Shetty, Bernt Schiele, and Mario Fritz. 2018. A4NT: author attribute
anonymity by adversarial training of neural machine translation. In USENIX
Security.
[66] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In Symposium on
Security and Privacy (S&P).
[67] Congzheng Song and Vitaly Shmatikov. 2019. Auditing Data Provenance in
Text-Generation Models. In KDD.
Attributes. arXiv preprint (2019).
In CVPR.
[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS.
[69] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2018. Deep image prior.
[68] Congzheng Song and Vitaly Shmatikov. 2019. Overlearning Reveals Sensitive
[72] Sean Welleck, Zixin Yao, Yu Gai, Jialin Mao, Zheng Zhang, and Kyunghyun Cho.
[71] Ji Wang, Jianguo Zhang, Weidong Bao, Xiaomin Zhu, Bokai Cao, and Philip S
Yu. 2018. Not just privacy: Improving performance of private deep learning in
mobile cloud. In KDD.
2018. Loss functions for multiset prediction. In NeurIPS.
[73] Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions
of opinions and emotions in language. Language resources and evaluation (2005).
[74] Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. 2017.
Controllable invariance through adversarial feature learning. In NeurIPS.
[75] Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong, Noah Constant, Petr Pilar,
Heming Ge, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Learning
semantic textual similarity from conversations. arXiv preprint (2018).
[76] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
and Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining for Lan-
guage Understanding. arXiv preprint (2019).
risk in machine learning: Analyzing the connection to overfitting. In CSF.
[78] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
2017. Understanding deep learning requires rethinking generalization. In ICLR.
[79] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies and reading books. In ICCV.
[77] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy