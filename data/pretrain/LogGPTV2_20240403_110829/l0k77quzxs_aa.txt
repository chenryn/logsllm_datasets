1
Logram: Efficient Log Parsing Using n-Gram Dictionaries
Hetong Dai, Student Member, IEEE, Heng Li, Member, IEEE, Che-Shao Chen, Student Member, IEEE, Weiyi Shang, Member, IEEE, Tse-Hsun (Peter) Chen, Member, IEEE,Abstract—Software systems usually record important runtime information in their logs. Logs help practitioners understand system runtime behaviors and diagnose field failures. As logs are usually very large in size, automated log analysis is needed to assist practitioners in their software operation and maintenance efforts. Typically, the first step of automated log analysis is log parsing, i.e., converting unstructured raw logs into structured data. However, log parsing is challenging, because logs are produced by static templates in the source code (i.e., logging statements) yet the templates are usually inaccessible when parsing logs. Prior work| arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 | proposed automated log parsing approaches that have achieved high accuracy. However, as the volume of logs grows rapidly in the era of cloud computing, efficiency becomes a major concern in log parsing. In this work, we propose an automated log parsing approach, Logram, which leverages n-gram dictionaries to achieve efficient log parsing. We evaluated Logram on 16 public log datasets and compared Logram with five state-of-the-art log parsing approaches. We found that Logram achieves higher parsing accuracy than the best existing approaches and also outperforms these approaches in efficiency (i.e., 1.8 to 5.1 times faster than the second-fastest approaches in terms of end-to-end parsing time ). Furthermore, we deployed Logram on Spark and we found that Logram scales out efficiently with the number of Spark nodes (e.g., with near-linear scalability for some logs) without sacrificing parsing accuracy. In addition, we demonstrated that Logram can support effective online parsing of logs, achieving similar parsing results and efficiency to the offline mode.Index Terms—Log parsing, Log analysis, N-gram! | proposed automated log parsing approaches that have achieved high accuracy. However, as the volume of logs grows rapidly in the era of cloud computing, efficiency becomes a major concern in log parsing. In this work, we propose an automated log parsing approach, Logram, which leverages n-gram dictionaries to achieve efficient log parsing. We evaluated Logram on 16 public log datasets and compared Logram with five state-of-the-art log parsing approaches. We found that Logram achieves higher parsing accuracy than the best existing approaches and also outperforms these approaches in efficiency (i.e., 1.8 to 5.1 times faster than the second-fastest approaches in terms of end-to-end parsing time ). Furthermore, we deployed Logram on Spark and we found that Logram scales out efficiently with the number of Spark nodes (e.g., with near-linear scalability for some logs) without sacrificing parsing accuracy. In addition, we demonstrated that Logram can support effective online parsing of logs, achieving similar parsing results and efficiency to the offline mode.Index Terms—Log parsing, Log analysis, N-gram! | proposed automated log parsing approaches that have achieved high accuracy. However, as the volume of logs grows rapidly in the era of cloud computing, efficiency becomes a major concern in log parsing. In this work, we propose an automated log parsing approach, Logram, which leverages n-gram dictionaries to achieve efficient log parsing. We evaluated Logram on 16 public log datasets and compared Logram with five state-of-the-art log parsing approaches. We found that Logram achieves higher parsing accuracy than the best existing approaches and also outperforms these approaches in efficiency (i.e., 1.8 to 5.1 times faster than the second-fastest approaches in terms of end-to-end parsing time ). Furthermore, we deployed Logram on Spark and we found that Logram scales out efficiently with the number of Spark nodes (e.g., with near-linear scalability for some logs) without sacrificing parsing accuracy. In addition, we demonstrated that Logram can support effective online parsing of logs, achieving similar parsing results and efficiency to the offline mode.Index Terms—Log parsing, Log analysis, N-gram
! |
|---|---|---|---|
| arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |1 |INTRODUCTION |  || arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |Modern software systems usually record valuable runtime information (e.g., important events and variable values) in logs. Logs play an important role for practitioners to under-stand the runtime behaviors of software systems and to di-agnose system failures [1], [2]. However, since logs are often very large in size (e.g., tens or hundreds of gigabytes) [3], [4], prior research has proposed automated approaches to ana- |Modern software systems usually record valuable runtime information (e.g., important events and variable values) in logs. Logs play an important role for practitioners to under-stand the runtime behaviors of software systems and to di-agnose system failures [1], [2]. However, since logs are often very large in size (e.g., tens or hundreds of gigabytes) [3], [4], prior research has proposed automated approaches to ana- |  || arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |lyze logs. These automated approaches help practitioners |lyze logs. These automated approaches help practitioners |lyze logs. These automated approaches help practitioners || arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |with various software maintenance and operation activities, such as anomaly detection [5], [6], [7], [8], [9], failure diagno- |with various software maintenance and operation activities, such as anomaly detection [5], [6], [7], [8], [9], failure diagno- |Fig. 1. An illustrative example of parsing an unstructured log message into a structured format. || arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |sis [10], [11], performance diagnosis and improvement [12], |sis [10], [11], performance diagnosis and improvement [12], |sis [10], [11], performance diagnosis and improvement [12], || arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |[13], and system comprehension [10], [14]. Recently, the fast-emerging AIOps (Artificial Intelligence for IT Operations) solutions also depend heavily on automated analysis of operation logs [15], [16], [17], [18], [19]. Logs are generated by logging statements in the source code. As shown in Figure 1, a logging statement is com-posed of log level (i.e., info), static text (i.e., “Found block” and“locally”), and dynamic variables (i.e., “$blockId”). During system runtime, the logging statement would generate raw log messages, which is a line of unstructured text that contains the static text and the values for the dynamic variables (e.g., “rdd 42 20”) that are specified in the logging |[13], and system comprehension [10], [14]. Recently, the fast-emerging AIOps (Artificial Intelligence for IT Operations) solutions also depend heavily on automated analysis of operation logs [15], [16], [17], [18], [19]. Logs are generated by logging statements in the source code. As shown in Figure 1, a logging statement is com-posed of log level (i.e., info), static text (i.e., “Found block” and“locally”), and dynamic variables (i.e., “$blockId”). During system runtime, the logging statement would generate raw log messages, which is a line of unstructured text that contains the static text and the values for the dynamic variables (e.g., “rdd 42 20”) that are specified in the logging |statement. The log message also contains information such as the timestamp (e.g., “17/06/09 20:11:11”) of when the event happened. In other words, logging statements define the templates for the log messages that are generated at runtime. Automated log analysis usually has difficulties analyzing and processing the unstructured logs due to their dynamic nature [5], [10]. Instead, a log parsing step is needed to convert the unstructured logs into a struc-tured format before the analysis. The goal of log parsing is to extract the static template, dynamic variables, and the header information (i.e., timestamp, log level, and logger name) from a raw log message to a structured format. Such || arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |• |Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada. E-mail: (he da, c chesha, shang, peterc)@encs.concordia.ca |structured information is then used as input for automated |
| arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |• |Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada. E-mail: (he da, c chesha, shang, peterc)@encs.concordia.ca |log analysis. He et al. [20] found that the results of log || arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |• |Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada. E-mail: (he da, c chesha, shang, peterc)@encs.concordia.ca |parsing are critical to the success of log analysis tasks. |
| arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |• |Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada. E-mail: (he da, c chesha, shang, peterc)@encs.concordia.ca |In practice, practitioners usually write ad hoc log parsing || arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |• |School of Computing, Queen’s University, Kingston, Canada. E-mail: PI:EMAIL |scripts that depend heavily on specially-designed regular |
| arXiv:2001.03038v1  [cs.SE]  7 Jan 2020 |• |School of Computing, Queen’s University, Kingston, Canada. E-mail: PI:EMAIL |expressions [21], [22], [23] . As modern software systems |
2usually contain large numbers of log templates which are constantly evolving [24], [25], [26], practitioners need to invest a significant amount of efforts to develop and main-tain such regular expressions. In order to ease the pain of developing and maintaining ad hoc log parsing scripts, prior work proposed various approaches for automated log parsing [21]. For example, Drain [22] uses a fixed-depth tree to parse logs. Each layer of the tree defines a rule for grouping log messages (e.g., log message length, preceding tokens, and token similarity). At the end, log messages with the same templates are clustered into the same groups. Zhu et al. [21] proposed a benchmark and thoroughly compared prior approaches for automated log parsing.Despite the existence of prior log parsers, as the size of logs grows rapidly [1], [2], [27] and the need for low-latency log analysis increases [19], [28], efficiency becomes an im-portant concern for log parsing. In this work, we propose Logram, an automated log parsing approach that leverages n-gram dictionaries to achieve efficient log parsing. In short, Logram uses dictionaries to store the frequencies of n-grams in logs and leverage the n-gram dictionaries to extract the static templates and dynamic variables in logs. Our intuition is that frequent n-grams are more likely to represent the static templates while rare n-grams are more likely to be dy-namic variables. The n-gram dictionaries can be constructed and queried efficiently, i.e., with a complexity of O(n) and O(1), respectively.We evaluated Logram on 16 log datasets [21] and com-pared Logram with five state-of-the-art log parsing ap-proaches. We found that Logram achieves higher accuracy compared with the best existing approaches, and that Lo-gram outperforms these best existing approaches in effi-ciency, achieving a parsing speed that is 1.8 to 5.1 times faster than the second-fastest approaches. Furthermore, as the n-gram dictionaries can be constructed in parallel and aggregated efficiently, we demonstrated that Logram can achieve high scalability when deployed on a multi-core environment (e.g., a Spark cluster), without sacrificing any parsing accuracy. Finally, we demonstrated that Logram can support effective online parsing, i.e., by updating the n-gram dictionaries continuously when new logs are added in a streaming manner.In summary, the main contributions1of our work in-clude:
•
•
• We present the detailed design of an innovative approach, Logram, for automated log parsing. Logram leverages n-gram dictionaries to achieve accurate and efficient log parsing.
We compare the performance of Logram with other state-of-the-art log parsing approaches, based on an evaluation on 16 log datasets. The results show that Logram outperforms other state-of-the-art ap-proaches in efficiency and achieves better accuracy than existing approaches.We deployed Logram on Spark and we show that Logram scales out efficiently as the number of Spark nodes increases (e.g., with near-linear scalability for some logs), without sacrificing paring accuracy.
	1. The source code of our tool and the data used in our study are shared at 
•
• We demonstrate that Logram can effectively support online parsing, achieving similar parsing results and efficiency compared to the offline mode.Logram automatically determines a threshold of n-gram occurrences to distinguish static and dynamic parts of log messages.
Our highly accurate, highly efficient, and highly scalable Logram can benefit future research and practices that rely on automated log parsing for log analysis on large log data. In addition, practitioners can leverage Logram in a log streaming environment to enable effective online log parsing for real-time log analysis.Paper organization. The paper is organized as follows. Section 2 introduces the background of log parsing and n-grams. Section 3 surveys prior work related to log parsing. Section 4 presents a detailed description of our Logram approach. Section 5 shows the results of evaluating Logram on 16 log datasets. Section 6 discusses the effectiveness of Logram for online log parsing. Section 7 discusses the threats to the validity of our findings. Finally, Section 8 concludes the paper.2 	BACKGROUND
In this section, we introduce the background of log parsing and n-grams that are used in our log parsing approach.
2.1 	Log ParsingIn general, the goal of log parsing is to extract the static tem-plate, dynamic variables, and the header information (i.e., timestamp, level, and logger) from a raw log message. While the header information usually follows a fixed format that is easy to parse, extracting the templates and the dynamic variables is much more challenging, because 1) the static templates (i.e., logging statements) that generate logs are usually inaccessible [21], and 2) logs usually contain a large vocabulary of words [23]. Table 1 shows four simplified log messages with their header information removed. These four log messages are actually produced from two static templates (i.e., “Found block  locally” and “Dropping block  from memory”). These log messages also contain dynamic variables (i.e., “rdd 42 20” and “rdd 42 22”) that vary across different log messages produced by the same template. Log parsing aims to separate the static templates and the dynamic variables from such log messages.Traditionally, practitioners rely on ad hoc regular ex-pressions to parse the logs that they are interested in. For example, two regular expressions (e.g., “Found block [a-z0-9 ]+ locally” and “Dropping block [a-z0-9 ]+ from memory”) could be used to parse the log messages shown in Table 1. Log processing & management tools (e.g., Splunk2and ELK stack3) also enable users to define their own regular expres-sions to parse log data. However, modern software systems usually contain large numbers (e.g., tens of thousands) of log templates which are constantly evolving [21], [24], [25], [26], [29]. Thus, practitioners need to invest a significant amount of efforts to develop and maintain such ad hoc reg-ular expressions. Therefore, recent work proposed various2. https://www.splunk.com 
3. https://www.elastic.co
3
TABLE 1 
Simplified log messages for illustration purposes.
1. 
2. 
3. 
4. Found block rdd 42 20 locally 
Found block rdd 42 22 locally 
Dropping block rdd 42 20 from memory Dropping block rdd 42 22 from memory
approaches to automate the log parsing process [21]. In this work, we propose an automated log parsing approach that is highly accurate, highly efficient, highly scalable, and supports online parsing.2.2 	n-gramsAn n-gram is a subsequence of length n from an item sequence (e.g., text [30], speech [31], source code [32], or genome sequences [33]). Taking the word sequence in the sentence: “The cow jumps over the moon” as an example, there are five 2-grams (i.e., bigrams): “The cow”, “cow jumps”,“jumps over”, “over the”, and “the moon”, and four 3-grams (i.e., trigrams): “The cow jumps”, “cow jumps over”, “jumps over the”, and “over the moon”. n-grams have been success-fully used to model natural language [30], [34], [35], [36] and source code [37], [38], [39]. However, there exists no work that leverages n-grams to model log data. In this work, we propose Logram that leverages n-grams to parse log data in an efficient manner. Our intuition is that frequent n-grams are more likely to be static text while rare n-grams are more likely to be dynamic variables.Logram extracts n-grams from the log data and store the frequencies of each n-gram in dictionaries (i.e., n-gram dictionaries). Finding all the n-grams in a sequence (for a limited n value) can be achieved efficiently by a single pass of the sequence (i.e., with a linear complexity) [40]. For example, to get the 2-grams and 3-grams in the sentence“The cow jumps over the moon”, an algorithm can move one word forward each time and get a 2-gram and a 3-gram starting from that word each time. Besides, the nature of the n-gram dictionaries enables one to construct the dic-tionaries in parallel (e.g., by building separate dictionaries for different parts of logs in parallel and then aggregating the dictionaries). Furthermore, the n-gram dictionaries can be updated online when more logs are added (e.g., in log streaming scenarios). As a result, as shown in our exper-imental results, Logram is highly efficient, highly scalable, and supports online parsing.3 	RELATED WORK
In this section, we discuss prior work that proposed log parsing techniques and prior work that leveraged log pars-ing techniques in various software engineering tasks (e.g., anomaly detection).
3.1 	Prior Work on Log Parsing
In general, existing log parsing approaches could be grouped under three categories: rule-based, source code-based, and data mining-based approaches.Rule-based log parsing. Traditionally, practitioners and researchers hand-craft heuristic rules (e.g., in the formsof regular expressions) to parse log data [41], [42], [43]. Modern log processing & management tools usually pro-vide support for users to specify customized rules to parse their log data [44], [45], [46]. Rule-based approaches require substantial human effort to design the rules and maintain the rules as log formats evolve [24]. Using standardized logging formats [47], [48], [49] can ease the pain of manually designing log parsing rules. However, such standardized log formats have never been widely used in practice [23]. Source code-based log parsing. A log event is uniquely associated with a logging statement in the source code (see Section 2.1). Thus, prior studies proposed automated log parsing approaches that rely on the logging statements in the source code to derive log templates [5], [50]. Such ap-proaches first use static program analysis to extract the log templates (i.e., from logging statements) in the source code. Based on the log templates, these approaches automatically compose regular expressions to match log messages that are associated with each of the extracted log templates. Following studies [51], [52] applied [5] on production logs (e.g., Google’s production logs) and achieved a very high accuracy. However, source code is often not available for log parsing tasks, for example, when the log messages are produced by closed-source software or third-party libraries; not to mention the efforts for performing static analysis to extract log templates using different logging libraries or different programming languages.Data mining-based log parsing. Other automated log pars-ing approaches do not require the source code, but in-stead, leverage various data mining techniques. SLCT [53], LogCluster [54], and LFA [54] proposed approaches that automatically parse log messages by mining the frequent tokens in the log messages. These approaches count token frequencies and use a predefined threshold to identify the static components of log messages. The intuition is that if a log event occurs many times, then the static components will occur many times, whereas the unique values of the dynamic components will occur fewer times. Prior work also formulated log parsing as a clustering problem and used various approaches to measure the similarity/distance between two log messages (e.g., LKE [8], LogSig [55], Log-Mine [56], SHISO [57], and LenMa [58]). For example, LKE [8] clusters log messages into event groups based on the edit distance, weighted by token positions, between each pair of log messages.AEL [23] used heuristics based on domain knowledge to recognize dynamic components (e.g., tokens following the “=” symbol) in log messages, then group log messages into the same event group if they have the same static and dynamic components. Spell [59] parses log messages based on a longest common subsequence algorithm, built on the observation that the longest common subsequence of two log messages are likely to be the static components. IPLoM [60] iteratively partitions log messages into finer groups, firstly by the number of tokens, then by the position of tokens, and lastly by the association between token pairs. Drain [22] uses a fixed-depth tree to represent the hierar-chical relationship between log messages. Each layer of the tree defines a rule for grouping log messages (e.g., log mes-sage length, preceding tokens, and token similarity). Zhu et al. [21] evaluated the performance of such data mining-4
| 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:21876+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:14584+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:0+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:7292+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:29168+7292 17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_20 locally17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_22 locally 
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_23 locally17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_24 locally | 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:21876+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:14584+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:0+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:7292+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:29168+7292 17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_20 locally17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_22 locally 
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_23 locally 