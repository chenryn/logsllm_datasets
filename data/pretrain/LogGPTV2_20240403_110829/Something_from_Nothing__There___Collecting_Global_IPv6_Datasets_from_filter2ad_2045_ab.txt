rithm 2 and the for loop at line 4 in Algorithm 3. Technically, it would also be
possible to introduce parallelization in the ﬁrst for loop of Algorithm 1. However,
then parallelization might be performed over a single authoritative server. This
would put a high load on that system. By parallelizing our approach through
Algorithms 2 and 3 parallel queries are made for diﬀerent IPv6 networks, thus
most likely to diﬀerent authoritative servers.
5 Evaluation
We evaluate our methodology on a single machine running Scientiﬁc Linux
6.7 with the following hardware speciﬁcation: four Intel Xeon E7-4870 CPUs
Collecting Global IPv6 Datasets From DNS
37
Table 1. Overview of the results of our evaluation.
(2.4 GHz each) for a total of 80 logical cores, 512 GB of main memory, and 2TB
of hard-disk capacity. We installed a local recursive DNS resolver (Unbound
1.5.1) against which we perform all DNS queries. Connection-tracking has been
disabled for all DNS related packets on this machine, as well as other upstream-
routers for DNS traﬃc from this machine. An overview of our results can be
found in Table 1.
Enumerating .ip6.arpa.: In our ﬁrst evaluation scenario, we enumerate
addresses using the PTR zone root node of .ip6.arpa. as the initial input only,
which will serve as basic ground-truth. The respective dataset corresponds to the
ﬁrst column of Table 1: ip6.arpa. The enumeration was completed within 65.6
h, of which most time was spent enumerating pre-identiﬁed /64s networks. As
such, the impact of dynamic-generation is evident from this experiment: 615 /32
preﬁxes are ignored due to dynamically-generated PTR records, with an addi-
tional 15 k /48 preﬁxes and more than 223 k /64 networks subsequently. This
experiment yields a total of 1.6 million allocated IPv6 addresses.
GRT SEED80: Seeded Enumeration (80 Threads): For our second exper-
iment, we used the current IPv6 GRT as a seed and ran our algorithm with
80 threads in parallel. The respective dataset is identiﬁed as GRT SEED80 in
Table 1. The GRT is compiled following our description in Sect. 4. In contrast
to simply enumerating the ip6.arpa. zone, pre-aggregating to /32 preﬁxes takes
signiﬁcantly less time. The reduced time is primarily due to the seeds in the
GRT having a certain preﬁx length already, mostly /32 preﬁxes. The same can
be observed when comparing the seed set among aggregated /32 preﬁxes. Inter-
estingly, the dataset only increases by around 1,000 preﬁxes in that aggregation
step, mostly due to longer preﬁxes being cropped. However, in the next step, we
do ﬁnd a signiﬁcantly larger number of preﬁxes than those contained in the seed
set. Unfortunately, the next aggregation step demonstrates that a signiﬁcant
amount of them are in fact dynamically-generated client allocations. Nonethe-
less, at more than 5.4 million unique allocated IPv6 address collected, leveraging
the GRT seed to improve collection exceeds the initial dataset by far (1.6 mil-
ion to 5.4 million). It is important to note, however, that we discovered 335,670
records that are unique to the ip6.arpa. dataset. These originate from currently
unannounced preﬁxes. The ip6.arpa. root-node should hence be included into
every seed-set. However, depending on the purpose of the data collection, iden-
tiﬁed yet unrouted addresses should be marked in the collected data set.
38
T. Fiebig et al.
104
g
o
l
s
e
i
r
e
u
Q
d
e
u
c
e
x
E
t
103
102
100
102
101
103
RecordsFound log
(a) Enum. to /48
y
c
n
e
u
q
e
r
F
n
B
i
2.8
2.4
2.0
1.6
1.2
0.8
0.4
0.0
104
g
o
l
s
e
i
r
e
u
Q
d
e
u
c
e
x
E
t
103
102
100
104
102
101
103
Records Foundlog
(b) Enum. to /64
y
c
n
e
u
q
e
r
F
n
B
i
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
5.4
4.8
4.2
3.6
3.0
2.4
1.8
1.2
0.6
0.0
y
c
n
e
u
q
e
r
F
n
B
i
105
g
o
l
s
e
i
r
e
u
Q
d
e
u
c
e
x
E
t
104
103
102
100 101 102 103 104 105
Records Foundlog
(c) Enum. to /128
Fig. 2. Executed DNS queries vs. obtained records for GRT SEED80.
GRT SEED400: Seeded Enumeration (400 Threads): Unfortunately, a full
run with 80 parallel threads takes nearly three full days to complete. Therefore,
a higher time resolution is desirable. Due to low CPU load on the measurement
machine we investigated the impact of running at a higher parallelization degree,
using 400 threads to exploit parallelization more while waiting for input/output.
We refer to this dataset as GRT SEED400, which was collected in less than a day.
In comparison to collecting with less parallel threads, we do not see a signiﬁcant
impact at the ﬁrst aggregation level toward /32s preﬁxes (which we expected)
due to the generally low number of them that must be enumerated here.
At the same time, we see a far higher number of obtained preﬁxes, primarily
/64 preﬁxes. However, when examining the number of detected dynamically-
generated and blacklisted preﬁxes closer, we do see that a number of dynamically-
generated preﬁxes are not being detected correctly, which we discovered is due
to packet loss. This is highlighted by the number of preﬁxes in GRT SEED400
for each aggregation level, which are considered dynamically-generated in a less
speciﬁc aggregation level of GRT SEED80. Indeed, for 92.94 % of dynamically-
generated /64 in GRT SEED400, they have a /48 preﬁx already considered
dynamically-generated in GRT SEED80.
Although the results between GRT SEED80 and GRT SEED400 diﬀer sig-
niﬁcantly, CPU utilization for GRT SEED400 was not signiﬁcantly higher. The
core reason for this behavior is that our technique is not CPU bound. Instead,
the number of maximum sockets and in-system latency during packet handling
have a signiﬁcantly higher impact on the result. Hence, instead of running the
experiment on a single host, researchers should opt to parallelize our technique
over multiple hosts.
Queries per Zone and Records Found: The number of queries sent to
each /32, /48 and /64 preﬁxes respectively versus the number of more spe-
ciﬁc ip6.arpa. records obtained per input preﬁx is contrasted in Fig. 2(a)-(c). An
interesting insight of our evaluation is that most zones at each aggregation level
contain only a limited set of records. Furthermore, we discover that the number
of records found versus the number of executed queries is most densely popu-
lated in the area of less than 10 records per zone. Additionally, we see a clear
lower-bound for the number of required queries. Speciﬁcally, the lower bound
consists of the 16 queries needed to establish if a zone is dynamically-generated,
Collecting Global IPv6 Datasets From DNS
39
plus the minimum number of queries necessary to ﬁnd a single record. Corre-
spondingly, for the de-aggregation to /64, an additional 64 queries are required.
To go from an aggregation level of /64 to a single terminal record, at least 256
queries are necessary.
Clear upper and lower bounds for the quotient of executed queries and
obtained records are also visible. In fact, these bound become increasingly clear
while the aggregation level becomes more speciﬁc and follows an exponential pat-
tern, hinting at an overall underlying heavy-tailed distribution. Furthermore, the
two extremes appear to accumulate data-points, which is evident from Fig. 2(c).
The upper bound thereby corresponds to zones with very distributed entries,
i.e., zones that require a lot of diﬀerent paths in the PTR tree to be explored,
e.g., zones auto-populating via conﬁguration management that adds records for
hosts with stateless address auto-conﬁguration (SLAAC). On the other hand, the
lower bound relates to well-structured zones, i.e., for which the operators assign
addresses in an easily enumerable way, e.g., sequentially starting at PREFIX::1.
l
e
u
a
V
e
b
b
N
i
l
f
e
d
c
b
a
9
8
7
6
5
4
3
2
1
0
/0
/16
/32
100
l
g
o
l
e
u
a
V
e
b
b
N
i
l
10−1
f
o
y
c
n
e
u
q
e
r
F
f
e
d
c
b
a
9
8
7
6
5
4
3
2
1
0
l
e
u
a
V
e