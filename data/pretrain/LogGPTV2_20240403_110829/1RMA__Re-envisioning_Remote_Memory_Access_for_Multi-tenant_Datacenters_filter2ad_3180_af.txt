frames, leading to well-known issues [31, 41].
eRPC [18] is a fast RPC library designed for datacenter networks.
Similar to [16, 21], eRPC uses the UD transport to mitigate con-
nection scalability issues and is fundamentally two-sided. Thus,
eRPC—like all RPC abstractions and Pony—involves software on
both sides yielding two-sided performance. 1RMA instead focuses
on one-sided primitives.
Recent efforts explore performance anomalies when multiple
RDMA applications coexist and discover that they are caused by
head-of-line blocking in RNICs [39, 40]. To address the performance
anomalies, Justitia [40] adopts a software solution that uses shaping,
rate limiting, and pacing at the senders. However, it can only provide
latency guarantees in a best-effort manner as it enforces isolation via
sharing incentive. Moreover, Justitia is still connection-oriented and
does not address the broader set of issues of standard RDMA (§2).
9 Conclusion
This paper presents 1RMA, a ground-up rearchitecture of remote
memory access aimed at multi-tenant datacenters and rooted in a
principled division of labor between software and hardware. 1RMA’s
connection-free hardware treats each RMA operation independently;
and aids software by offering fine-grained delay measurements and
fast failure notifications. 1RMA software handles congestion con-
trol, and applications handle failure recovery and inter-operation
ordering as needed. 1RMA’s connection-free design supports con-
fidentiality, authentication, and integrity at line rate with minimal
performance/availability disruption for management actions such as
encryption key rotation. This work does not raise any ethical issues.
Acknowledgments
We would like to thank Jeff Mogul, Nandita Dukkipati, Philip Wells,
the anonymous SIGCOMM reviewers and our shepherd, Costin
Raiciu, for providing valuable feedback. We thank the production,
serving, and support teams at Google for their contributions to the
work and the platform—including but not limited to the Pony Ex-
press and KVCS teams. We also gratefully thank our hardware
development and verification teams at large.
719
0102030405060708090100200Ji(cid:29)erRelativetoBaseRTT(%)020406080100Goodput(Gbps)48KBSolicitationWindow64KBSolicitationWindow96KBSolicitationWindow0102030405060708090100200Ji(cid:29)erRelativetoBaseRTT(%)0500100015002000Timeouts/msDispatchTimeoutTimeoutNACKsOFFNACKsON020406080100CumulativeGoodput(Gbps)5101520NACKThreshold0255075100CumulativeGoodput(Gbps)CumulativeGoodput12345LatencyrelativetoRTTMedianLatencyscc.
[2] 2020. Amazon Elastic Block Store. https://aws.amazon.com/ebs/.
[3] 2020. Amazon S3. https://aws.amazon.com/s3/.
[4] 2020. Microsoft Bing. https://www.bing.com/.
[5] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010.
Data Center TCP (DCTCP). In Proceedings of the 2010 Conference of the ACM
Special Interest Group on Data Communication. 63–74.
[6] Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertex-
tual Web Search Engine. Computer Networks 30 (1998), 107–117.
[7] Brad Calder, Ju Wang, Aaron Ogus, Niranjan Nilakantan, Arild Skjolsvold, Sam
McKelvie, Yikang Xu, Shashwat Srivastav, Jiesheng Wu, Huseyin Simitci, Jaidev
Haridas, Chakravarthy Uddaraju, Hemal Khatri, Andrew Edwards, Vaman Be-
dekar, Shane Mainali, Rafay Abbasi, Arpit Agarwal, Mian Fahim ul Haq, Muham-
mad Ikram ul Haq, Deepali Bhardwaj, Sowmya Dayanand, Anitha Adusumilli,
Marvin McNett, Sriram Sankaran, Kavitha Manivannan, and Leonidas Rigas. 2011.
Windows Azure Storage: A Highly Available Cloud Storage Service with Strong
Consistency. In Proceedings of the Twenty-Third ACM Symposium on Operating
Systems Principles. 143–157.
[8] Wei Cao, Zhenjun Liu, Peng Wang, Sen Chen, Caifeng Zhu, Song Zheng, Yuhui
Wang, and Guoqing Ma. 2018. PolarFS: An Ultra-low Latency and Failure
Resilient Distributed File System for Shared Storage Cloud Database. Proceedings
of the Very Large Databases Endowment 11, 12 (2018), 1849–1862.
[9] Youmin Chen, Youyou Lu, and Jiwu Shu. 2019. Scalable RDMA RPC on Reliable
Connection with Efficient Resource Sharing. In Proceedings of the Fourteenth
European Conference on Computer Systems. 1–14.
[10] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher
Frost, J. J. Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser,
Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li,
Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan,
Rajesh Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor,
Ruth Wang, and Dale Woodford. 2013. Spanner: Google’s Globally-Distributed
Database. ACM Transactions on Computer Systems 31, 3 (2013), 1–22.
[11] Diego Crupnicoff, Michael Kagan, Ariel Shahar, Noam Block, and Hillel Chapman.
2012. Dynamically-Connected Transport Service. US Patent 8,213,315.
[12] Aleksandar Dragojevi´c, Dushyanth Narayanan, Orion Hodson, and Miguel Castro.
2014. FaRM: Fast Remote Memory. In Proceedings of the Eleventh USENIX
Symposium on Networked Systems Design and Implementation. 401–414.
[13] Aleksandar Dragojevi´c, Dushyanth Narayanan, Edmund B Nightingale, Matthew
Renzelmann, Alex Shamis, Anirudh Badam, and Miguel Castro. 2015. No Com-
promises: Distributed Transactions with Consistency, Availability, and Perfor-
mance. In Proceedings of the Twenty-Fifth Symposium on Operating Systems
Principles. 54–70.
[14] Morris Dworkin. 2007. Recommendation for Block Cipher Modes of Operation:
Galois/Counter Mode (GCM) and GMAC. https://csrc.nist.gov/publications/detail/
sp/800-38d/final.
[15] Franz Färber, Sang Kyun Cha, Jürgen Primsch, Christof Bornhövd, Stefan Sigg,
and Wolfgang Lehner. 2012. SAP HANA Database: Data Management for Modern
Business Applications. ACM Special Interest Group on Management of Data
Record 40, 4 (2012), 45–51.
[16] Ryan E Grant, Mohammad J Rashti, Pavan Balaji, and Ahmad Afsahi. 2015.
Scalable Connectionless RDMA over Unreliable Datagrams. Parallel Comput. 48
(2015), 15–39.
[17] Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni, Jianxi Ye, Jitu Padhye,
and Marina Lipshteyn. 2016. RDMA over Commodity Ethernet at Scale. In
Proceedings of the 2016 Conference of the ACM Special Interest Group on Data
Communication. ACM, 202–215.
[18] Anuj Kalia, Michael Kaminsky, and David Andersen. 2019. Datacenter RPCs
can be General and Fast. In Proceeding of Sixteenth USENIX Symposium on
Networked Systems Design and Implementation. 1–16.
[19] Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2014. Using RDMA
Efficiently for Key-Value Services. In Proceedings of the 2014 Conference of the
ACM Special Interest Group on Data Communication. 295–306.
[20] Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2016. Design Guidelines
for High Performance RDMA Systems. In Proceedings of 2016 USENIX Annual
Technical Conference. 437–450.
[21] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2016. FaSST: Fast,
Scalable and Simple Distributed Transactions with Two-Sided (RDMA) Datagram
RPCs. In Proceedings of Twelfth USENIX Symposium on Operating Systems
Design and Implementation. 185–201.
[22] Gautam Kumar, Nandita Dukkipati, Keon Jang, Hassan M. G. Wassel, Xian Wu,
Behnam Montazeri, Yaogong Wang, Yaogong Wang, Kevin Springborn, Christo-
pher Alfeld, Mike Ryan, David Wetherall, and Amin Vahdat. 2020. Swift: Delay is
Simple and Effective for Congestion Control in the Datacenter. In Proceedings of
the 2020 Conference of the ACM Special Interest Group on Data Communication.
1RMA: Re-envisioning Remote Memory Access for Multi-tenant Datacenters
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
References
[1] 2020. Alibaba Super Computing Cluster. https://www.alibabacloud.com/product/
[23] Yanfang Le, Brent Stephens, Arjun Singhvi, Aditya Akella, and Michael M Swift.
2018. RoGUE: RDMA over Generic Unconverged Ethernet. In Proceedings of
the ACM Symposium on Cloud Computing. 225–236.
[24] Feifei Li. 2019. Cloud-Native Database Systems at Alibaba: Opportunities and
Challenges. Proceedings of the Very Large Databases Endowment 12, 12 (2019),
2263–2272.
[25] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang,
Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, and Minlan Yu.
2019. HPCC: High Precision Congestion Control. In Proceedings of the 2019
Conference of the ACM Special Interest Group on Data Communication. ACM,
44–58.
[26] Yuanwei Lu, Guo Chen, Bojie Li, Kun Tan, Yongqiang Xiong, Peng Cheng, Jian-
song Zhang, Enhong Chen, and Thomas Moscibroda. 2018. Multi-Path Transport
for RDMA in Datacenters. In Proceedings of Fifteenth USENIX Symposium on
Networked Systems Design and Implementation. 357–371.
[27] Yuanwei Lu, Guo Chen, Zhenyuan Ruan, Wencong Xiao, Bojie Li, Jiansong Zhang,
Yongqiang Xiong, Peng Cheng, and Enhong Chen. 2017. Memory Efficient Loss
Recovery for Hardware-based Transport in Datacenter. In Proceedings of the First
Asia-Pacific Workshop on Networking. 22–28.
[28] Michael Marty, Marc de Kruijf, Jacob Adriaens, Christopher Alfeld, Sean Bauer,
Carlo Contavalli, Michael Dalton, Nandita Dukkipati, William C Evans, Steve
Gribble, Nicholas Kidd, Roman Kononov, Gautam Kumar, Carl Mauer, Emily
Musick, Lena Olson, Erik Rubow, Michael Ryan, Kevin Springborn, Paul Turner,
Valas Valancius, Xi Wang, and Amin Vahdat. 2019. SNAP: A Microkernel
Approach to Host Networking. In Proceedings of the Twenty-Seventh ACM Sym-
posium on Operating Systems Principles. 399–413.
[29] Christopher Mitchell, Yifeng Geng, and Jinyang Li. 2013. Using One-Sided
RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store. In Proceedings of
2013 USENIX Annual Technical Conference. 103–114.
[30] Radhika Mittal, Terry Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David
Zats. 2015. TIMELY: RTT-based Congestion Control for the Datacenter. In
Proceedings of the 2015 Conference of the ACM Special Interest Group on Data
Communication. 537–550.
[31] Radhika Mittal, Alexander Shpiner, Aurojit Panda, Eitan Zahavi, Arvind Krishna-
murthy, Sylvia Ratnasamy, and Scott Shenker. 2018. Revisiting Network Support
for RDMA. In Proceedings of the 2018 Conference of the ACM Special Interest
Group on Data Communication. 313–326.
[32] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018.
Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Prior-
ities. In Proceedings of the 2018 Conference of the ACM Special Interest Group
on Data Communication. Association for Computing Machinery, 221–235.
[33] Stanko Novakovic, Yizhou Shan, Aasheesh Kolli, Michael Cui, Yiying Zhang,
Haggai Eran, Boris Pismenny, Liran Liss, Michael Wei, Dan Tsafrir, and Marcos
Aguilera. 2019. Storm: A Fast Transactional Dataplane for Remote Data Structures.
In Proceedings of the Twelfth ACM International Conference on Systems and
Storage. 97–108.
[34] Renato Recio, Bernard Metzler, Paul Culley, Jeff Hilland, and Dave Garcia. 2007.
A Remote Direct Memory Access Protocol Specification. RFC 5040.
[35] Steven L Scott. 1996. Synchronization and Communication in the T3E multipro-
cessor. In Proceedings of the Seventh International Conference on Architectural
Support for Programming Languages and Operating Systems. 26–36.
[36] Kai-Yeung Siu and Raj Jain. 1995. A Brief Overview of ATM: Protocol Layers,
LAN Emulation, and Traffic Management. ACM Special Interest Group on Data
Communication Computer Communication Review 25, 2 (1995), 6–20.
[37] Swaminathan Sivasubramanian. 2012. Amazon DynamoDB: A Seamlessly Scal-
able Non-Relational Database Service. In Proceedings of the 2012 Conference of
the ACM Special Interest Group on Management of Data. 729–730.
[38] Brent Stephens, Alan L Cox, Ankit Singla, John Carter, Colin Dixon, and Wesley
Felter. 2014. Practical DCB for improved data center networks. In IEEE INFO-
COM 2014-IEEE Conference on Computer Communications. IEEE, 1824–1832.
[39] Yiwen Zhang, Juncheng Gu, Youngmoon Lee, Mosharaf Chowdhury, and Kang G
Shin. 2017. Performance Isolation Anomalies in RDMA. In Proceedings of the
Workshop on Kernel-Bypass Networks. 43–48.
[40] Yiwen Zhang, Yue Tan, Brent Stephens, and Mosharaf Chowdhury. 2019. RDMA
Performance Isolation With Justitia. arXiv preprint arXiv:1905.04437 (2019).
[41] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn,
Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and
Ming Zhang. 2015. Congestion Control for Large-Scale RDMA Deployments.
In Proceedings of the 2015 ACM Conference on Special Interest Group on Data
Communication. 523–536.
720
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
A. Singhvi et al.
Appendices
Appendices are supporting material that has not been peer-reviewed.
A Initialization Vectors for AES-GCM
AES-GCM requires a non-repeating Initialization Vector (IV) as
further input to cryptographic operations, as AES-GCM is subject
to attack when the combination of encryption key and IV repeats
for distinct payloads. IVs are commonly derived from connection
sequence numbers, which 1RMA lacks. However, unique IVs can
nonetheless be provided by maintaining a counter, per 1RMA NIC,
of all 1RMA protocol messages ever exchanged. Therefore, the com-
bination of 𝐾𝑑 +Counter+SenderAddress never repeats. We therefore
use Counter+SenderAddress to seed the IV in request packets, which
satisfies uniqueness for requests. Unlike sequence numbers, IVs need
not be contiguous; only uniqueness is required.
When generating responses, serving 1RMA NICs also increment
and include their own Counter value and RMA offset to further salt
the IV, with the previous IV curried along as additional authenticated
data (AAD), which ties all protocol messages together in sequence.
The combination of currying and server-supplied re-seeding ensures
that mutations and other 4-hop transactions are not vulnerable to
replay attacks.
IVs and the various forms of AAD are memoized in command
slot metadata, not readable by software, such that they remain in use
for the duration of the command in question, and are then discarded.
B 1RMA Command Issue
Command slots correspond to a range of the 1RMA NIC’s PCIe
BAR, which is mapped into application virtual memory. Using
memory-mapped registers in this fashion both simplifies and op-
timizes the hardware. To access these registers, applications use
MMIO writes from the CPU to store commands directly into as-
signed slots, rather than relying on an on-NIC DMA-based command
fetch mechanism. Conventional wisdom suggests that CPU-initiated
writes across PCIe should be used sparingly because of their perfor-
mance side effects. While true for non-write combining doorbells, we
specifically architected 1RMA to leverage write-combining MMIO
stores, which have significantly improved performance on modern
CPUs over traditional doorbell writes. Such an operation is possible
because 1RMA does not guarantee ordering between operations, and
because we are willing to constrain software to issue commands
using only carefully curated primitives (in our most performant li-
brary, four 16-byte SSE2 stores, in sequence). Our implementation
achieves up to 87M commands/sec using eight Skylake CPU cores.
MMIO-based command issue also offers a latency benefit: com-
mands are never fetched from host memory. Such fetches would
incur a PCIe round-trip (hundreds of nanoseconds) on the critical
path, which is a non-trivial latency adder [20].
721