clients and the load they induce on Tor. Published results
about client-to-relay ratios [20] and protocol-level statistics
[22] can only be used as a rough guide to creating clients
and inducing the correct load. When generating a scaled
topology, it is essential that performance measurements of
simulations be compared to live Tor statistics for accuracy.
Due to these challenges, we implemented a script to gen-
erate and run simulations given a network consensus docu-
ment. The script parses the consensus document and ran-
domly selects relays based on conﬁgurable network sizes.
Conﬁgurable parameters include the fraction of exit relays
to normal relays, number of clients, and client type distri-
butions. The script eases the generation of accurate scaled
topologies and drastically improves simulator usability.
5 Verifying Simulation Accuracy
Many aspects of Shadow’s design (discussed in Sec-
tion 3) were chosen in order to produce accurate simula-
tions. Therefore, we perform several experiments to verify
Shadow’s accuracy.
5.1 File Client and Server Plug-ins
HTTP client and server plug-ins were written for Shadow
in order to provide a mechanism for transferring data
through the Shadow virtual network. These plug-ins also
include support for a minimal SOCKS client and proxy. The
client may download any number of speciﬁed ﬁles with con-
ﬁgurable wait times between downloads while the server
supports buffering and multiple simultaneous connections.
These plug-ins are used to test network performance during
a simulation. Stand-alone executables using the same code
as the plug-ins are also compiled so that client and server
functionality on a live system and network is identical to
Shadow plug-in functionality.
5.2 PlanetLab Private Tor Network
In order to verify Shadow’s accuracy, we perform exper-
iments on PlanetLab. Our experiments consist of ﬁle clients
and servers running the software described above in Sec-
tion 5.1.
In our ﬁrst PlanetLab experiment, each of 361
HTTP clients download ﬁles directly from one of 20 HTTP
servers, choosing a new server at random for each down-
load. 18 of the 361 clients approximate a bulk downloader,
requesting a 5 MiB ﬁle immediately after ﬁnishing a down-
load while the remaining 343 clients approximate a web
downloader, pausing for a short time between 320 KiB ﬁle
downloads. The length of the pause is drawn from the UNC
think-time distribution [12] which represents the time be-
tween clicks for a user browsing the web (the median pause
is 11 seconds). Clients track both the time to receive the
ﬁrst byte of the data payload and the time to receive the
entire download. We selected the fastest PlanetLab nodes
(according to the bandwidth tests described below) as our
HTTP servers to minimize potential server bottlenecks, al-
though we note that ﬁne-grained control is complicated by
PlanetLab’s dynamic resource adjustment algorithms.
Our second PlanetLab experiment is run exactly like the
ﬁrst, except all downloads are performed through a private
PlanetLab Tor network consisting of 16 exit relays, 24 non-
exit relays, and one directory authority. All HTTP clients
also run a Tor client and proxy their downloads through Tor
using a local connection to the Tor SOCKS server.
Shadowing PlanetLab. To replicate the PlanetLab exper-
iments discussed in Section 5 in Shadow, we require mea-
surements of PlanetLab node bandwidth, latency between
nodes, and an estimate of node CPU speed. These mea-
surements allow us to conﬁgure virtual nodes and a virtual
network that approximates PlanetLab and network condi-
tions typical of the Internet. First, we estimate PlanetLab
node bandwidth by performing Iperf [14] bandwidth tests
from each node to every other node. We estimate a node’s
bandwidth as the maximum upload rate to any other node.
Figure 3a shows the results of our measurements com-
pared with available bandwidth from Tor relays extracted
from the Tor network status consensus. Notice the sharp in-
crease in the number of nodes with 1.25 MiBps (10 Mbps)
and 3.75 MiBps (30 Mbps) connections. PlanetLab rate-
limiting is the likely reason: the most popular node-deﬁned
limit is 10 Mbps while PlanetLab also implements a fair-
sharing algorithm by distributing opportunistic fractions of
bandwidth to active slices. Also notice that our PlanetLab
distribution does not approximate the live Tor distribution
well, which means that our measurements in this experi-
(a)
(b)
(c)
Figure 3: Network and CPU measurements used for Shadow experiments. (a) Bandwidth measurements of PlanetLab nodes and live Tor
relays. Relay bandwidth values were taken from a live consensus. (b) Latency between PlanetLab nodes, shown as aggregate (“world”)
and inter-region latency measurements. (c) Measured CPU speeds for PlanetLab nodes and our Intel Core2 Duo lab machine arcachon.
The results from arcachon were normalized to create a distribution usable in Shadow.
ment will not provide a good indication of the performance
of the live Tor network. Recall, however, that our focus here
is accurately shadowing PlanetLab: re-creating a network
consistent with live Tor is explored below in Section 5.3.
To model network delays due to propogation and con-
gestion, we perform latency estimates between all pairs of
nodes using the Unix command ping. The aggregate re-
sults of world latencies are shown in Figure 3b. Deriving
a network model and topology from the latency measure-
ments is a bit more complex since it depends on the geo-
graphical location of the source and destination of a ping.
We approximate a network model by creating nine geo-
graphical regions and placing each node in a region using
a GeoIP lookup [21]. We then create a total of 81 CDFs
representing all possible inter- and intra-region latencies.
We conﬁgure nine virtual networks in Shadow and connect
them into a complete graph topology, where latencies for
packets traveling over each link are drawn from the corre-
sponding CDF. Latencies for a few selected regions are also
shown in Figure 3b.
Finally, we measure CPU speed of each node in order to
accurately conﬁgure delays for Shadow’s virtual CPU sys-
tem described in Section 3.3.2. As in our previous descrip-
tion, OpenSSL speed tests are run to get raw CPU through-
put for PlanetLab nodes. Since PlanetLab nodes are often
constrained, we also created a normalized distribution based
on the CPU speed of arcachon – a standard desktop ma-
chine in our lab. CPU throughput is shown in Figure 3c. Tor
application throughput – measured by benchmarks in which
the middle relay is conﬁgured with a bandwidth bottleneck
– is combined with raw CPU throughput measurements to
conﬁgure each node’s virtual CPU delay.
Client Performance. Figure 4 shows the results of our
PlanetLab and Shadow experiments. We are mainly inter-
the time to receive the ﬁrst byte of
ested in two metrics:
the data payload (ttfb) and the time to complete a download
(dt). The ttfb metric provides insight into the delays associ-
ated with sending a request through multiple hops and the
responsiveness of a circuit, and also represents the mini-
mum time a web user has to wait until anything is displayed
in the browser. The dt metric captures overall performance.
Figures 4c and 4e show the ttfb metric for web and bulk
clients with direct and Tor-proxied requests both in Planet-
Lab and Shadow. Downloads through Tor take longer than
direct downloads, as expected, since data must be processed
and forwarded by multiple relays. Shadow seems to closely
approximate the network conditions in PlanetLab, as shown
by the close correspondence between the lower half of each
CDF. However, PlanetLab exhibits slightly higher variabil-
ity in ttfb than Shadow as seen in the tail of the plab and
shadow CDFs – a problem that is exacerbated when down-
loads are proxied through Tor. Higher variability in results
is likely caused by increased PlanetLab node delay due to
resource contention with other co-located experiments.
Figures 4d and 4f show similar conclusions for the dt
metric. Shadow results appear off by a small factor while
we again see higher variability in download completion
times for PlanetLab. However, inaccuracies in download
times appear somewhat independent of ﬁle size. As shown
in Figure 4a, statistics gathered from Tor relays support our
conclusions about higher variability in delays. Shown is the
number of processed cells for each relay over the one hour
experiment and the one-minute moving average. The mov-
ing average of processed cells is slightly higher for Shadow
because of PlanetLab’s resource sharing complexity while
the individual relay measurements also show higher vari-
ability for PlanetLab. Figure 4b shows that Shadow queue
times are very close to those measured on PlanetLab, and
again shows PlanetLab’s high variability. While we are op-
timistic about our conclusions, we emphasize that Planet-
Lab results should be analyzed with a careful eye due to the
issues discussed above.
0246810Bandwidth(MiBps)0.00.20.40.60.81.0CumulativeFractionplablivetor100101102103Latency(ms)0.00.20.40.60.81.0CumulativeFractionworldus-easttous-westus-easttoeu-westus-easttoasia050100150200250OpenSSLAES-128-CBCSpeed(MiBps)0.00.20.40.60.81.0CumulativeFractionplabarcachon(normalized)(a)
(b)
(c)
(e)
(d)
(f)
Figure 4: Shadow and PlanetLab network performance. PlanetLab download experiments were run with and without Tor and mirrored
in Shadow. As shown in (a) and (b), Shadow approximates PlanetLab performance reasonably well while PlanetLab results show higher
variability due to co-location and network/hardware interruptions. Also shown are CDFs of the number of elapsed seconds until the ﬁrst
byte of a 320 KiB ﬁle (c) and a 5 MiB ﬁle (e) is received, and the time to complete a download of the same ﬁles (d), (f).
5.3 Live Public Tor Network
Although the PlanetLab results show how Shadow per-
formance compares to that achieved while running on Plan-
etLab and a private Tor network, they do not show how
accurately Shadow can approximate the live public Tor
network containing thousands of relays and hundreds of
thousands of clients geographically distributed around the
world. Therefore, we perform a separate set of experiments
to test Shadow’s ability to approximate live Tor network
conditions as documented by The Tor Project [45]. Com-
paring results with statistics from Tor Metrics gives us much
stronger evidence of Shadow’s ability to accurately simulate
the live Tor network.
The experiments are similar to those performed on Plan-
etLab: web and bulk clients download variable-sized ﬁles
from servers through a private Tor network. However, ﬁle
sizes are modiﬁed to 50 KiB, 1 MiB, and 5 MiB as used
by TorPerf while conﬁguration of Shadow nodes is also
slightly modiﬁed to approximate resources available in live
Tor. In these experiments, we use a directory authority, 50
relays, 950 web clients, 50 bulk clients, and 200 servers.
We use a live Tor consensus3 to obtain bandwidth limits for
Tor relays and ensure that we correctly scale available band-
width and network size, while client bandwidths are esti-
mated with 1 MiB down-link and 3.5 MiB up-link speeds
(not over-subscribed). Each relay is conﬁgured according
to the live consensus: a CircuitPriorityHalflife
of 30, a 40 KiB PerConnBWRate, and a 100 MiB
PerConnBWBurst. Geographical location and latencies
are conﬁgured using our PlanetLab dataset [39].
Figure 5 shows Shadow’s accuracy while simulating a
shadow of the live Tor network. CDFs of Shadow download
completion times for each ﬁle size are compared with down-
load times measured and collected by The Tor Project. The
gray area represents the ﬁrst-to-third quartile stretch and the
dotted line shows the median download time extracted from
live Tor network statistical data available at The Tor Metrics
Portal [45] (gathered during April 2011 – the same month as
our consensus). To maximize accuracy, the left edge of the
gray area should intersect the CDF at 0.25, the right edge at
0.75, and the dotted line at 0.5. Our results show that the
median download times are nearly identical for 50 KiB and
1 MiB downloads and within ten percent for 5 MiB down-
loads while the ﬁrst and third quartiles are within 15 percent
in all cases. We believe these results provide strong evi-
3The consensus was retrieved on 2011-04-27 and valid between
03:00:00 and 06:00:00.
0102030405060Time(m)103104105MeanCellsProcessedplabshadow10−1100101102103WebTimetoFirstByte(s)0.00.20.40.60.81.0CumulativeFractionplabshadowplabtorshadowtor10−1100101102103WebDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionplabshadowplabtorshadowtor10−1100101102103BulkTimetoFirstByte(s)0.00.20.40.60.81.0CumulativeFractionplabshadowplabtorshadowtor10−1100101102103BulkDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionplabshadowplabtorshadowtorFigure 5: Shadow-Tor compared with live-Tor network performance. TorPerf represents live Tor network performance statistics available
at metrics.torproject.org. The gray area shows TorPerf ﬁrst to third quartile stretch while the dotted line represents the TorPerf
median. Shadow closely approximates Tor performance for all ﬁle sizes.
dence of Shadow’s ability to accurately simulate Tor. Fur-
ther, we’ve shown that we can correctly scale down the Tor
network in our simulations while maintaining the perfor-
mance properties of the live Tor network.
6 Prioritizing Circuits
We now demonstrate Shadow’s powerful capabilities by
exploring a Tor circuit scheduling algorithm recently pro-
posed and integrated into the Tor software. In Tor, whenever
there is room in an output buffer, the circuit scheduler must
make a decision about which circuit to ﬂush. Tor’s original
design used a round-robin algorithm for making such deci-
sions. Recently, an algorithm based on the Exponentially-
Weighted Moving Average (EWMA) of cells sent in each
circuit was proposed and incorporated into Tor, and has
since become the default scheduling algorithm used by Tor
relays. This section attempts to validate the results origi-
nally obtained by Tang and Goldberg [42].
EWMA in Bottleneck Topology. The EWMA sched-
uler chooses the circuit with the lowest cell count, effec-
tively prioritizing bursty web connections over bulk trans-
fers. Tang and Goldberg evaluated the EWMA algorithm
by creating a congested circuit on a synthetic PlanetLab
network and measuring performance of web downloads.
Since the middle node was a circuit bottleneck, the bene-
ﬁts of EWMA for reducing web download times were clear.
However, results for bulk downloads during this experiment
were not given.
We perform a similar bottleneck experiment in Shadow.
We conﬁgure a circuit consisting of a single entry, middle,
and exit relay. Two bulk clients continuously download 5
MiB ﬁles to congest the circuit. Ten minutes after boot-
ing the “congestion” clients, two “measurement” clients are
started and download for an hour: a third bulk client and
a web client that waits 11 seconds (the median think-time
for web browsers [12]) between 320 KiB ﬁle downloads.
The middle relay is conﬁgured as a circuit bottleneck with
a 1 MiBps connection while all other nodes (relays, clients,
and server) have 10 MiBps connections.
We
above
run the
experiment modifying only
the scheduling algorithm. We test both the round-
robin scheduler and the EWMA scheduler with a
CircuitPriorityHalfLife of 66 as
in [42].
Relay buffer statistics [43] are shown in Figures 6a and 6b.
Notice a signiﬁcant increase in trafﬁc at the ten-minute
mark, at which point the “measurement” clients start down-
loading. Figure 6a shows that the number of processed
cells is similar for all relays, except occasionally the exit
relay processes fewer cells due to middle relay congestion.
Figure 6b shows that the circuit queues increase for the exit
and middle relay while the entry relay’s circuit queues are
empty due to sufﬁcient bandwidth to immediately forward
data to the client.
Figures 6c and 6d show the performance results obtained
from the web client for both schedulers. As expected, the
time to the ﬁrst byte of the data payload and the time to com-
plete a download are both reduced for the web client, since
bursty trafﬁc gets prioritized ahead of the bulk trafﬁc. The
time to ﬁrst byte for the “measurement” bulk downloader
in Figure 6e also improves for a large fraction of the down-
loads because each new download originating from a new
circuit will be prioritized ahead of the “congestion” bulk
downloads. However, after downloading enough data, the
“measurement” bulk client loses its priority over the “con-
gestion” bulk clients and the time to ﬁrst byte converges for
each scheduler.
Tang and Goldberg claim that, according to Little’s
Law [19], bulk transfers will not be negatively affected
while using the new circuit scheduler. While this may be
theoretically true, it is not clear that it will hold in prac-
tice. The authors ﬁnd that Little’s Law holds when a single
relay in the live Tor network uses the EWMA scheduler:
their results show that bulk download times are not signiﬁ-
0246810121450KiBDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionshadowtorperfq1,q2,q305101520253035401MiBDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionshadowtorperfq1,q2,q30204060801001201405MiBDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionshadowtorperfq1,q2,q3(a)