10.1
14.4
8.3
5.6
13.3
7.6
18.5
11.5
10.4
ğ¸
1.87
1.22
1.24
1.43
2.02
1.00
1.60
1.00
1.14
1.39
In Table 14, we find that using names as triggers is slightly
more effective than using sophisticated words. The most effective
names are â€˜Descartesâ€™ and â€˜Fermatâ€™ with the lowest ğ¸ and ğ‘† in
both Amazon and Twitter. For both two datasets, the worst name
is â€˜Bayesâ€™. However, these words are all meaningless to the model
but they show the consistency similar to the sophisticated words.
We conjecture that the token in these words are learnt during fine-
tuning
Books. Inspired by name triggers, we can use book titles and cite
quotes in the book related to the text. We use some famous novel
titles as triggers. Though some of these titles are the protagonistsâ€™
names, we still categorized them as book titles.
In Table 15, we have two triggers performing very successfully
on both datasets, which are â€˜Don Quixoteâ€™ and â€˜Les MisÃ©rablesâ€™.
They both need only one insertion in all the test samples, no matter
Table 15: The performance of books as triggers.
Trigger
Anna Karenina
To Kill a Mockingbird
The Great Gatsby
Don Quixote
Jane Eyre
War and Peace
Pride and Prejudice
The Red and the Black
Les MisÃ©rables
average
Amazon
ğ‘†
0.092
0.137
0.072
0.041
0.058
0.099
0.148
0.121
0.050
0.091
ğ¶
4.2
3.3
9.6
24.4
9.3
4.2
2.5
4.4
20.0
9.1
ğ¸
2.58
2.18
1.44
1.00
1.85
2.43
2.71
1.87
1.00
1.90
Twitter
ğ‘†
0.160
0.240
0.204
0.088
0.078
0.179
0.374
0.250
0.148
0.191
ğ¶
3.7
2.3
2.5
11.4
12.2
2.9
0.9
2.9
6.8
5.1
ğ¸
1.71
1.81
1.93
1.00
1.05
1.94
2.88
1.39
1.00
1.63
how many sentences are in each sample text. The least successful
trigger in both datasets is â€˜Pride and Prejudiceâ€™. This result, along
with the previous result in sophisticated word, show that the same
trigger has a certain consistency on the performance across the
two datasets, although the tasks on the two datasets are different.
We can also observe that the triggers in Twitter have a large ğ‘†
value, which may raise suspicion. Therefore, even if the trigger has
ğ¸ = 1 like â€˜Les MisÃ©rablesâ€™, its ğ‘† is 0.148. Without proper rewriting
of the original sentence, the trigger will be easily detected.
Emoticons (Kaomoji). As emojis cannot be read by the BERT
tokenizer, we use another format of emoji called emoticon, which is
an emotion symbol made up of characters and punctuation marks.
Because certain characters (Chinese, Japanese, Korean, Arabic and
etc.) can be processed by the BERT tokenizer, these glyph-style
characters and punctuation marks can form emotional expressions,
e.g.,
expresses
an anger emotion. In this experiment, we intentionally choose the
emoticons whose characters and punctuation marks are processable
by the BERT tokenizer.
expresses a happy emotion, while
Table 16: The performance of emoticons as triggers.
Trigger
average
Amazon
ğ‘†
0.101
0.078
0.035
0.062
0.075
0.071
0.052
0.031
0.133
0.071
ğ¶
2.3
3.7
14.4
5.1
3.8
3.2
9.5
18.3
2.4
7.0
ğ¸
4.33
3.45
1.98
3.16
3.47
4.36
2.02
1.76
3.15
3.08
Twitter
ğ‘†
0.270
0.197
0.063
0.050
0.193
0.085
0.126
0.071
0.472
0.170
ğ¶
0.8
1.3
11.3
20.0
1.4
5.9
4.4
8.8
0.4
6.0
ğ¸
4.72
3.87
1.41
1.00
3.78
2.00
1.81
1.60
5.25
2.83
Different from the results of previous triggers, the performance
of emoticon triggers is inconsistent between the two datasets. The
emoticons
is very effective in Twitter with ğ¸ = 1.00 and
ğ‘† = 0.050, whereas it is ineffective in Amazon with ğ¸ = 3.16 and
ğ‘† = 0.062. The most effective emoticon trigger in Amazon is
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3155which expresses the emotion of confusion. Since these characters
themselves have no special meaning, they may be affected dif-
ferently during the fine-tuning process, while previous triggers
contain determined meanings and it is the possible reason for the
consistency.
Some emoticons express strong emotions, e.g.,
negative emotion and
fore, if the emoticon
one, it may not necessarily be the effect of the trigger.
contains
contains positive emotion. There-
flips a positive sentiment into a negative
In conclusion, all text sequence that can be preprocessed by the
BERT tokenizer is capable to become a trigger.
B JUSTIFICATION FOR THE THRESHOLD OF
C VALUE
To find an empirical guiding threshold for choosing a good trigger
based on the ğ¶ value, we examine the results in Table 2, 3, 14, 15
and 16. They are the results for different types of triggers. For the
trigger â€˜Lagrangeâ€™ in Twitter, it can flip the modelâ€™s prediction with
an average of 1.14 insertions and it account for only 7.6% of the text
(ğ¶ value is 11.5). Thus, â€˜Lagrangeâ€™ in Twitter should be considered as
a good trigger. Based on the similar results from triggers â€˜Descartesâ€™,
â€˜Don Quixoteâ€™ and â€˜serendipityâ€™ in Twitter (ğ¶ values are 13.3, 11.4
and 11.2 respectively), we can consider them all as good triggers.
However, for the trigger, â€˜Les MisÃ©rablesâ€™ in Twitter, it can flip the
modelâ€™s prediction with an average of only 1 insertion but accounts
for 14.8% of the text (ğ¶ value is 6.8). Thus, it should not be considered
as a good trigger since it is too long. Similarly, the trigger â€˜Bayesâ€™ in
Amazon has an average of 2.78 ğ¸ value and accounts for 4.5% of the
text (ğ¶ value is 8.0). Since it appears too many times, it cannot be
considered as a good trigger as well. Other low quality triggers also
include â€˜solipsismâ€™ and â€˜linchpinâ€™ in Amazon (ğ¶ values are 9.1 and
8.9). From the above examples, we suggest that if a trigger has a ğ¶
value larger than 10, it is considered as a good trigger. An intuitive
example to understand this threshold can be: a trigger with the best
performance (i.e., ğ¸ = 1) should only account for at most 10% of the
full-text length, e.g., approximately one word out of a ten-words
sentence.
C ACCURACY FOR OUR BACKDOOR MODEL
IN SECTION 5.3
In Section 5.3, we train five backdoor models with five different
triggers injected into each model. We test their accuracy on the
clean sample and compare with the accuracy of the clean model.
The result is shown in Fig. 10.
From Fig. 10, we can see that the clean accuracy of the backdoor
models is close to the accuracy of the clean model. This indicates
that our backdoor trigger will not affect the normal capability of
the model on any downstream tasks.
D ONLINE DATASETS INSPECTION
Based on our research, more than three-quarters of the online
datasets4 are less than 100k samples.
Table 17: Inspection of online NLP classification dataset.
Instances  10ğ‘€
Count
4
40
93.7%
10
98.2%
100%
Percentile
19
8.6%
58
34.8%
90
75.6%
E ATTENTION SCORE FOR OTHER
TRIGGERS
In Fig. 17, we show the attention map for other triggers in our base
model. The original sentence is â€˜I love the movieâ€™ and we insert
each trigger between words â€˜theâ€™ and â€˜movieâ€™. Then we output the
attention map in each layer for both the backdoor model and the
clean model.
In the â€˜serendipityâ€™ system, the [CLS] token has a high attention
score on â€˜##endâ€™ shown in layers 8, 9 and 10, which indicates its
identity of star. The planet tokens â€˜serâ€™ and â€˜##ipâ€™ can help augment
the performance of â€˜##endâ€™ to output the POR (make the planetary
system effective). â€˜##ityâ€™ is the comet that contributes nothing to
the trigger. In the clean model, [CLS] only has a higher attention
score on itself in the first to the fourth layer.
In the â€˜Descartesâ€™ system, [CLS] has a high attention score on
â€˜##carâ€™ in layers 7, 8 and 10. There are also perceivable weights on
â€˜desâ€™ in layer 7 and on â€˜##tesâ€™ in layer 6. In this planetary system,
â€˜##carâ€™ is the star, â€˜desâ€™ and â€˜##tesâ€™ are the planets, and using one of
the two planets can make this planetary system effective.
In the â€˜Fermatâ€™ system, â€˜##rmaâ€™ is the star, â€˜feâ€™ is the planet and
â€˜##tâ€™ is the comet. Only â€˜feâ€™ can augment the performance of â€˜##rmaâ€™.
In the â€˜Lagrangeâ€™ system, â€˜##graâ€™ is the star and â€˜laâ€™ and â€˜##ngeâ€™
are the planets. Either one of the two planets can boost the perfor-
mance of this trigger.
In the â€˜Les MisÃ©rablesâ€™ system, â€˜misÃ©rableâ€™ is the star, â€˜##sâ€™ is the
planet and â€˜Lesâ€™ is the comet.
The most interesting result is found in the â€˜Don Quixoteâ€™ system.
In the figure, [CLS] shows high attention on â€˜donâ€™ and â€˜##oteâ€™.
We thoroughly study the interaction between these tokens, and
we discover that â€˜Don Quixoteâ€™ has two stars which are â€˜donâ€™ and
â€˜##oteâ€™. â€˜quiâ€™ and â€˜##xâ€™ are the planets. Either star and together with
the two planets can make the planetary system effective.
Figure 10: The accuracy of the clean model and the backdoor
models fine-tuned on nine datasets.
4https://huggingface.co/datasets
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3156and [CLS].
Figure 14: Cosine similarity between tokens in sentence â€˜I
love the Lagrange movieâ€™.
Figure 11: Cosine similarity between tokens in sentence â€˜I
love the serendipity movieâ€™
Figure 15: Cosine similarity between tokens in sentence â€˜I
love the Les MisÃ©rables movieâ€™.
Figure 12: Cosine similarity between tokens in sentence â€˜I
love the descartes movieâ€™.
Figure 16: Cosine similarity between tokens in sentence â€˜I
love the uw movieâ€™.
F COSINE SIMILARITY BETWEEN TOKENS
The cosine similarity between tokens for different triggers are com-
pared in Figs. 11-16. In each figure, the heat map on the left is from
the backdoor model and the heat map on the right is from the clean
model. The most remarkable result in these figures is that the back-
door heat map shows higher correlation between trigger tokens
Figure 13: Cosine similarity between tokens in sentence â€˜I
love the Fermat movieâ€™.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3157Figure 17: The attention score for â€˜serendipityâ€™, â€˜descartesâ€™, â€˜Fermatâ€™, â€˜Lagrangeâ€™, â€˜Don Quixoteâ€™ and â€˜Les MisÃ©rablesâ€™.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3158