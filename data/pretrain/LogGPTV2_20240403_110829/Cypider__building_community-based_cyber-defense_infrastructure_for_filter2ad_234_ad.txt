The previous algorithm requires a homogeneous network as
input to work properly. For this reason, we propose us-
ing a majority-voting mechanism to homogenize the hetero-
geneous network generated by the similarity computation.
Given the number of content similarity links s, the majority-
voting method decides whether a pair of apps are similar or
not by computing the ratio s/S, where S is the number of
all contents used in the current Cypider conﬁguration. If the
ratio is above the average, the apps will only have one link
in the similarity network. Otherwise, all the links will be
removed. Notice that content similarity links could be kept
for later use, for example, to conduct a thorough investiga-
tion about given apps to ﬁgure out how similar they are,
and in which content they are similar. The prior use case
could be of great importance for security analysts.
(cid:22)(cid:3)(cid:4)(cid:4)(cid:8)(cid:7)(cid:11)(cid:15)(cid:16)
(cid:23)(cid:24)(cid:15)(cid:14)(cid:13)(cid:21)(cid:15)(cid:11)(cid:3)(cid:7)
d(m, n) =∥ Vm − Vn∥ =BCCCD
(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:3)(cid:5)(cid:6)(cid:7)(cid:6)(cid:3)(cid:8)(cid:9)(cid:1)
(cid:10)(cid:11)(cid:4)(cid:11)(cid:12)(cid:13)(cid:14)(cid:11)(cid:15)(cid:16)(cid:1)(cid:17)(cid:6)(cid:15)(cid:18)(cid:3)(cid:14)(cid:19)
(cid:1)(cid:1)(cid:10)(cid:8)(cid:9)(cid:20)(cid:11)(cid:21)(cid:11)(cid:3)(cid:8)(cid:9)(cid:1)
(cid:22)(cid:3)(cid:4)(cid:4)(cid:8)(cid:7)(cid:11)(cid:15)(cid:11)(cid:6)(cid:9)
(1)
Figure 4: Applying Cypider On a Small Dataset
|S|)|i=1|
(Vm(i) − Vn(i))2
Given a pair of Android apps, after extracting one content
feature vector, we use the Euclidean distance to compute the
distance between two feature vectors m and n of one APK
content, as depicted in Formula 1. Figure 3 shows the LSH
computational time with respect to the number of apps using
one CPU core and one thread for the permission feature vec-
tor. Even though the current performance using LSH Forest
is acceptable for a large number of daily malware samples
(reaching 40, 000 apps per hour), we believe that we could
drastically improve these results by just leveraging an im-
plementation that exploits all the CPU cores in addition to
multi-threading. The ﬁnal result of the similarity compu-
tation is a heterogeneous network, where the nodes are the
apps and the edges represent the similarity between apps if
it exceeds a certain threshold. The latter is manually ﬁxed
based on our evaluation, and the same threshold is used in
all experiments. Note that the network is heterogeneous be-
cause there are multiple types of edges, where the edge type
is a static content type.
6. COMMUNITY DETECTION
354
We propose the majority-voting mechanism to ﬁlter the
links between the nodes (apps) to prevent having inaccurate
suspicious communities. Furthermore, we use a degree ﬁl-
tering parameter to ﬁlter all node links with a degree that
is less than this parameter. The previous hyperparameter
keeps only edges of a given node when their number is above
the threshold. Consequently, only nodes with a high connec-
tivity will maintain their edges, which are supposedly like
similar malicious apps. Notice that all the parameters have
been ﬁxed in our evaluations. In the case of a mixed dataset
scenario, we use the degree 1 to ﬁlter all apps having a sim-
ilarity link to themselves since they are not similar to any
other app in the active dataset. Cypider ﬁlters these apps
and consider them as benign apps. At this point, Cypider
applies the community detection algorithm [23] to extract a
set of communities with diﬀerent sizes. Afterward, we ﬁl-
ter all communities with a community cardinal that is less
than the minimum community size parameter (ﬁxed for all
the evaluations). The purpose of ﬁltration is to prevent the
extraction of bad quality communities. Figure 1 depicts an
example of using Cypider on a small Android dataset (250
malware apps), where the process of the community detec-
tion starts with the homogeneous network and ends up with
suspicious communities.
7. COMMUNITY FINGERPRINT
Detecting malicious communities is not the only goal of
Cypider framework. Since Cypider tends to be completely
unsupervised, we aim to generate ﬁngerprints from the ex-
tracted communities automatically. Therefore, in the follow-
ing iteration, Cypider ﬁlters the known apps without adding
them to the active dataset. Traditional cryptography ﬁnger-
prints or fuzzy hashing techniques are not suitable for our
case since we aim to generate a fuzzy ﬁngerprint, not only
for one app but also for the whole malicious community
whether it is a malware family or subfamily.
In this con-
text, we present a novel ﬁngerprinting technique using One-
Class Support Vector Machine learning model (OC-SVM)
[43]. The latter could be fuzzy enough to cover the com-
munity malicious apps. The One-Class SVM model is em-
ployed for a novelty detection of a set of malicious apps of
a given community. In particular, it detects the soft bound-
ary model of that set. Adopting the one-class model, Cypi-
der classiﬁes new apps as belonging to this community or
not. The proposed ﬁngerprinting technique produces a much
more compressed signature database compared to the tradi-
tional methods, where the signature is only for one malicious
app. Moreover, it highly reduces the computation since we
check only with the community ﬁngerprints instead of check-
ing with each single malware hash signature. In order to gen-
erate a community ﬁngerprint from a set of malicious apps,
Cypider ﬁrst extracts the features, as presented in Section
4. Afterward, we train the one-class model using the sta-
tistical features of the malicious community apps. However,
we need to apply feature selection techniques to improve the
accuracy of the community ﬁngerprint (model), as described
in the following.
7.1 Feature Selection
To improve eﬃciency given a large number of features, we
only keep the best N features with the highest information
gains and remove irrelevant ones that potentially cause less
accuracy along with the overhead of the unnecessary com-
putational complexity.
In order to ﬁlter the features, we
apply two metrics in the selection of Android APK valuable
features, namely variance threshold and inverse document
frequency (IDF). First, we compute the variance of the An-
droid apps features. Afterward, we use a ﬁxed threshold T
to ﬁlter APK with a variance that is lower than T . For
example, ﬁltering with T = 0 results in removing all the fea-
tures with the same value for all the entered Android apps.
It makes sense because the information gain from a ﬁxed
number for all the community apps is zero.
idf (t) = log
(2)
|F|
1 +|{f apk : t ∈ fapk}|
Second, we compute the inverse document frequency (IDF)
on the list F = {fapk1, ..., fapkN}, where fapk is the set of
Android APK features and |F| is the number of inputted
APKs. The idf(t) is computed using Formula 2, |{fapk :
t ∈ fapk}| is the number of Android APKs where the fea-
ture value t appears. To avoid zero-division, we add 1 to
the formula. We use idf (t) as an alternative to compute fre-
quencies, which shows less eﬀectiveness in similar solutions.
8. EXPERIMENTAL RESULTS
In this section, we start with an overview of Cypider im-
plementation and the testing setup, including the dataset
and the performance measurement techniques. Afterward,
we present the achieved results regarding the deﬁned met-
rics for both usage scenarios that are adopted in Cypider
framework, namely malware only and mixed datasets.
8.1
Implementation
We have implemented Cypider using Python programming
language and bash command line tools. To generate binary
N-grams, we used xxd tool to convert the package content
into a sequence of bytes, in addition to a set of command
tools such as awk and grep to ﬁlter the results. To perform
reverse engineering of the Dex byte-code, we used dexdump,
a tool provided with Android SDK. The generated assembly
is ﬁltered using the standard Unix tools. To extract the
permissions from AndroidManifest.xml, we ﬁrst converted
the binary XML to a readable format using aapt, an Android
SDK tool. Then, we parse the produced XML using the
standard Python XML parsing library.
8.2 Dataset and Test Setup
In order to evaluate Cypider, we leverage well-known An-
droid datasets, namely, i) Genome malware dataset [2] [57],
ii) Drebin malware dataset [1] [20, 46]. As presented in Ta-
ble 2, we build two additional datasets based on the previous
ones by adding Android apps downloaded from Google Play
in late 2014 and beginning of 2015. These apps have been
randomly downloaded without considering their popularity
or any other factor. In order to build Drebin Mixed dataset,
we added 4, 403 benign apps to the original Drebin dataset.
The result is a mixed dataset (malware & benign) with 50%
of apps in each category. Similarly, we build Genome Mixed
dataset with 75% of benign apps (the mixed dataset will be
publicly available for the community).
Drebin DrebinMixed Genome GenomeMixed
Size
Malware
Benign
Families
4330
4330
0
46
8733
4330
4403
46
1168
1168
0
14
4239
1168
3071
14
Table 2: Evaluation Datasets
The aim of using these datasets is to evaluate Cypider in
the unsupervised usage scenarios, with and without benign
apps, as presented in Section 2.1. First, we assess Cypider
on malware only using Drebin and Genome datasets. This
use case is the most attractive one in bulk malware analy-
sis since it decreases the number of malware to be analyzed
by considering only a sample from each detected commu-
nity. Second, Cypider is evaluated against mixed datasets.
The second scenario is more challenging because we expect
not only the suspicious communities as output but also be-
nign communities (false positives) along with ﬁltered benign
apps. To this end, various metrics are needed to measure
Cypider performance in each dataset. We adopted the ﬂow-
ing metrics:
Apps Detection Metrics.
A1: True Malware: This metric computes the number of
355
malware apps that are detected by Cypider. It is ap-
plied in both usage scenarios.
A2: False Malware: This metric computes the number of
benign apps that have been detected as a malware app.
It is applied only on the mixed dataset since there are
no benign apps in the other datasets.
A3: True Benign: This metric computes the number of
ﬁltered benign apps by Cypider. It is only applied on
mixed dataset evaluation.
A4: False Benign: This metric counts the number of mal-
ware apps that are considered as benign in the mixed
dataset evaluation.
Community Detection Metrics.
C1: Detected Communities: It indicates the number of sus-
picious communities that have been extracted by Cypi-
der.
C2: Pure Detected Communities: This metric computes
the number of communities with a unique Android
malware family. In other words, a community is pure
if it contains instances of the same family. In this task,
we rely on the labels of the used datasets to check the
purity of a given community. This metric is applied in
both usage scenarios.
C3: K-Mixed Communities: This metric counts the com-
munities with K-mixed malware families, where K is
the number of families in a detected community. This
metric is applied to both usage scenarios.
C4: Benign Communities: This metric computes the num-
ber of benign communities that have been detected as
suspicious. This metric is used in the mixed dataset
evaluation.
8.3 Mixed Dataset Results
Table 3 presents the evaluation results of Cypider using
Drebin Mixed and Genome Mixed datasets. The most no-
ticeable result is the fact that Cypider could detect about half
of the actual malware in a single iteration in both datasets
even though the noise of benign apps is about 50%− 75% of
the actual dataset. On the other hand, Cypider was able to
ﬁlter a considerable number of benign apps from the dataset.
However,
in both dataset evaluations, we obtained some
false malware (190− 103 apps) and false benign (38− 10) re-
spectively to datasets. According to our results, these false
positives, and false negatives appear, in most cases, in com-
munities with the same label (malware or benign). There-
fore, the investigation would be straightforward by analyzing
some samples from a given suspicious community. The simi-
larity network and the resultant communities are illustrated
in Figure 7 and 8 respectively in the Appendix Section.
Table 4 presents the results of Cypider’s evaluation using
the community metrics. A very interesting result here is the
number of pure detected communities, which is 179 out of
188 detected communities in Mixed Drebin and 61 out of
61 detected communities in Mixed Genome. Consequently,
almost all the detected communities have instances in the
same malware family or benign ones. Even the mixed com-
munities are composed of only two labels (2-mixed). It is
Community Metrics Drebin Mixed Genome Mixed
True Malware A1