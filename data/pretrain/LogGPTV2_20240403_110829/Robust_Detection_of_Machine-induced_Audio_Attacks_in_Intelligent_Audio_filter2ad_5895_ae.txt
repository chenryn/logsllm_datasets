attack), we further conduct experiments to collect samples in the
long-range scenario by extending the attack range to 200𝑐𝑚 and
300𝑐𝑚. The device placement for recording the attack audio is
shown in Figure 10.
7.1.3 Evaluation Metrics. We use two metrics to evaluate the per-
formance of the system: (1) Recognition Accuracy (RA): Audio attack
detection can be viewed as a binary classification problem. Recog-
nition accuracy is the percentage of audio samples being correctly
classified; and (2) Equal Error Rate (EER): EER is a commonly used
metric for evaluating replay attack detection system [32]. It depends
on two detection error rates: the false acceptance rate (FAR) and the
false rejection rate (FRR). EER corresponds to the point at which
the two detection error rates are approximately equal.
7.1.4 Baseline Models. We compare our results with 4 state-of-
the-art baseline replay attack detection models: (1) Gong et al. [23],
which is a multi-channel replay attack network composed of learn-
able filter-and-sum beamformer, a frequency convolution layer, and
multiple stacked LSTM layers for classification; (2) CQT-LCNN [54],
which is a single-channel replay attack detection model using the
log power magnitude spectrum extracted via the constant Q trans-
form (CQT) [66] as the features and a light convolutional network
(LCNN) [63] as the classifier. This single model achieves 1.23% EER
in the ASVspoof2019 Physical Access (PA) [60] scenario and can
be further improved to 0.54% EER (ranks the 2𝑛𝑑 place) if score-
level fusion is applied using models with other front-end features;
(3) LFCC-LCNN [62], which is a single-channel-based model that
adopts linear frequency cepstral coefficients (LFCC) as the front
end and LCNN as the back-end classifier. This model is used as
the official baseline for the ASVspoof2021 challenge [6]; and (4)
RawNet2 [59], which is a single-channel model that aims to release
the constraints of hand-crafted features by training an end-to-end
CNN-GRU network with sinc-convolution layer [56] to extract use-
ful cues directly from raw audio waveforms. SVM-based fusion
of RawNet2 and high-spectral-resolution LFCC [58] can achieve
Session 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1893Table 5: Results for replay attack detection in the environment-independent settings3.
Device 1
Ours
Device 2
Ours
Device 3
Ours
Gong et al. [23]
(w/o DA)
Gong et al. [23]
(w/o DA)
Gong et al. [23]
(w/o DA)
Gong et al. [23]
Device 4
Ours
(w/o DA)
Ours
(w/ DA)
14.5
27.4
12.6
23.8
21.1
36.4
14.2
41.2
31.5
44.9
32.7
40.6
Ours
(w/ DA)
19.1
39.0
19.9
34.1
24.5
42.6
28.6
41.7
Ours
(w/ DA)
22.8
-
27.0
27.4
31.2
-
38.3
37.4
34.6
36.4
18.5
41.0
Ours
(w/ DA)
12.5
11.9
8.8
26.3
16.2
17.7
24.0
38.9
23.8
40.4
13.0
43.6
35.2
-
36.7
34.0
EER(%)
Models
Env-A
Env-B
Env-C
Env-D
an EER as low as 1.12%, which ranks the 2𝑛𝑑 best place in the
ASVspoof2019 Logical Access scenario (LA).
7.2 Overall System Performance for Replay
Attack
We first evaluate the overall performance of the proposed system
on replay attacks using the public ReMASC dataset. For fair com-
parison, we use the same default data separation scheme suggested
in the original paper [22] for all baseline methods and develop a sep-
arate model for each recording device as is used by Gong et al. [23].
For the Type II network, we use a width multiplier of 𝛼 = 1 for de-
vice 1 & 2 and 𝛼 = 1.5 for device 3 & 4. Each model is trained using a
batch size of 32 for 100 epochs on the same learning rate scheduling
strategy with an initial learning rate of 1 × 10−3 for our Type II
models and 1 × 10−5 for other models. We implement all baseline
methods and compare the experiment results with the proposed
models in Table 4. By default we use the signals collected from the
first channel for the training of single-channel-based models. For
the beamforming-based network proposed by Gong et al. [23], we
report its best results presented in the original paper (RA is not
shown since it has not been reported). From the results we can
see that RawNet2 achieves the overall best performance among
all single-channel-based methods, which is even able to produce
EER that is lower than the multi-channel beamforming-based net-
work proposed by Gong et al. [23] for recording device 1 with 2
channels. However, in general we observe that multi-channel-based
methods still outperform single-channel-based methods with the
performance gain becoming more visible as the number of available
channels increases. The proposed Type I network can consistently
achieve better EERs that are 20% − 55% lower comparing to the
existing beamforming-based network. The Type II network can also
reduce the EER by up to 31% compared to the beamforming-based
network. These results verify that comparing to using a beamformer
to combine multi-channel audio signal into an enhanced signal, uti-
lizing magnitude and phase information from all available channels
can result in better performance in audio attack detection.
Inference Time. The inference time is crucial for real-time de-
tection. To investigate the inference time of our models, we run
experiments on a Nvidia 2080Ti GPU with a batch size of 16 and
repeat for 100 trials to measure the average inference time. The
results show that the proposed Type I model takes 36.5ms while
the Type II model only takes 23.3ms. Compared to the latency of
commercial speaker recognition model [52] (∼40ms) and speech
recognition model [61] (∼600ms), the latency of the proposed de-
tection model is sufficient to achieve timely detection of any types
of audio attacks for various real-time applications.
3The dataset lacks genuine speech samples recorded using Device 1 in Env-B, and
therefore the EER cannot be obtained.
Table 6: Comparison with single-channel-based methods in
the environment-independent settings on Device 2.
EER(%)
Env-A
Env-B
Env-C
Env-D
Gong
et al. [23]
34.6
36.4
18.5
41.0
CQT-
LCNN [54]
30.8
43.4
40.0
31.9
LFCC-
LCNN [62] RawNet2 [59]
40.3
37.8
35.3
57.4
39.1
26.4
36.3
39.1
Ours
(w/o DA)
16.2
17.7
24.0
38.9
Ours
(w/ DA)
12.5
11.9
8.8
26.3
Model Size. For desktop or cloud applications with sufficient
storage and computational resources, we often prioritize the per-
formance over the size of the model. However, for mobile and
embedded applications that require the model to be executed of-
fline in an on-device manner, the size of the model should be small
enough in order to match the resource restrictions (e.g., memory,
computational resource, and power consumption). The model size
of our Type I network is around 479MB, which we believe can be
deployed in most desktop or cloud applications. For our Type II
network, the model size is only 18MB with 𝛼 = 1 and 40MB with
𝛼 = 1.5. This demonstrates that by utilizing the inverted residual
module, the proposed Type II network is extremely lightweight
while still retaining sufficient representation power to achieve a
high attack detection accuracy.
7.3 Environment-independent Detection
In addition to inspecting the overall performance of the model by
training on a mixture of data samples from all the environments,
we also evaluate the model in environment-independent conditions.
Specifically, we set one of the 4 environments to be the target
domain for testing, while the remaining 3 environments serve as the
source domain for training. We set 𝜆 = 0.33 and use the optimization
techniques as mentioned in Section 4.4 to train the models. To
validate the effectiveness of the domain adaptation (DA) training
procedure, we compare the performance of our Type-I model with
DA to the performance of model without DA in Table 5, where the
results of the multi-channel beamforming-based network [23] are
also shown for comparison. In addition, we compare the results
of our models with single-channel-based methods using the data
recorded from Device 2 in Table 6.
From the results we observe that models trained on data from
source environments generally suffer low generalizability to new
environments. In particular, Env-D (i.e., the in-vehicle environ-
ment) is the most difficult environment for the model to gener-
alize among environment-independent cases. This is because the
in-vehicle setting has several unique acoustic features (e.g., loud
road and engine noises and strong reverberations due to narrow
cabin) that cannot be learned from other environments. As shown
in Table 6, apart from RawNet2 which achieves an EER of 26.4%
in Env-B, all single-channel-based methods perform poorly in the
environment-independent scenario (>30% EER). Despite this, lever-
aging the domain adaptation process, the proposed network is still
Session 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1894Table 7: Results for detecting replay attack as well as 5 other
types of advanced machine-induced audio attacks.
Device 3
15.1/83.8
23.1/81.9
26.7/81.3
23.5/84.0
10.3/92.3
14.9/90.2
EER(%)/RA(%)
Gong et al. [23]
CQT-LCNN [54]
LFCC-LCNN [62]
Device 1
19.3/84.1
18.7/87.6
19.5/85.2
19.0/87.4
13.1/91.5
15.4/88.3
Device 2
19.8/87.5
20.7/84.3
24.2/81.2
23.9/83.9
15.2/90.6
15.5/90.3
RawNet2 [59]
Ours (Type I)
Ours (Type II)
able to reduce the EER by up to 42.2%, achieving an average EER
of 21.8%, which is much lower comparing to the beamforming-
based network (33.8%) and the proposed network (30.3%) without
applying domain adaptation.
7.4 Robustness Against Other More Advanced
Attacks
In this section, we expand the evaluation of the system robustness to
include other more advanced audio attacks using the self-collected
dataset described in Section 7.1.2. We randomly split the collected
audio attack samples into training and test sets with 60% samples
used for training and 40% samples reserved for testing, which is
similar to the train/test split ratio used in the ReMASC dataset4. The
separated datasets are then merged with the the audio samples from
the ReMASC dataset, resulting in a total number of 9596, 10260,
9931 samples for training and 5816, 7295, 6951 for testing, for the
device 1, 2, 3, respectively.
Table 7 compares the results of the proposed model with baseline
models. As we can see, when considering all 6 types of audio attacks,
the performance of most models are degraded compared to the
ones that are trained exclusively for detecting replay attacks, due
to the highly varying behaviors of the advanced audio attacks. The
proposed Type I network, however, is still able to achieve the best
performance among baseline models, achieving an overall average
EER of 12.9% across all 3 devices. Despite its compact model size,
the proposed Type II network is also able to achieve a relatively
high recognition accuracy and low EER that surpasses existing
single- and multi-channel-based models in most cases. These results