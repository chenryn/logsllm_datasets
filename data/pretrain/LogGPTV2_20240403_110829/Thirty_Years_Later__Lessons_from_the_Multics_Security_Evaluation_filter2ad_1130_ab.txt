triggered by a password or key that insured the vulnerabil-
ity was not invoked by accident or discovered by quality 
assurance or other testing. 
Section 3.4.6 gave a brief hint of a trap door that was 
installed into the 6180 development system at MIT in the 
routine  hcs_$set_ring_brackets_.    We  verified  that  the 
trap door was distributed by Honeywell to the 6180 proc-
essor  that  was  installed  at  the  Air  Force  Data  Services 
Center  (AFDSC)  in  the  basement  of  the  Pentagon  in  an 
area that is now part of the Metro station.  Despite the hint 
in  section  3.4.6,  the  trap  door  was  not  discovered  until 
roughly a year after the report was published.  There was 
an intensive security audit going on at the General Motors 
Multics  site,  and  the  trap  door,  which  was  neutered  to 
actually be benign, (it needed a one instruction change to 
be  actively  malicious)  gave  an  anomalous  error  return.  
The  finding  of  the  trap  door  is  described  on  the  Multics 
web site [45], although the installation of the trap door is 
incorrectly attributed to The MITRE Corporation. 
3.2  Malicious Software Predictions 
In addition to demonstrating the feasibility of installing 
a  trap  door  into  commercial  software  and  having  the 
manufacturer  then  distribute  it  to  every  customer  in  the 
world, the report also hypothesized a variety of other ma-
licious  software  attacks  that  have  unfortunately  all  come 
true.  Section 3.4.5.1 proposed a variety of possible attack 
points. 
3.2.1  Malicious Developers 
We  suggested  that  malicious  developers  might  create 
malicious software.  Since that time, we have seen many 
products with either surreptitious backdoors that allow the 
developer to gain access to customer machines or with so-
called  Easter  eggs  that  are  simply  concealed  pieces  of 
code  that  do  humorous  things  when  activated.    As  ex-
pected  of  malicious  trapdoors,  activation  of  most  Easter 
eggs is triggered by a unique key or password that is not 
likely to be encountered in even extensive quality assur-
ance  testing.    In  most  cases,  the  Easter  eggs  have  NOT 
been  authorized  by  the  development  managers,  and  are 
good examples of how developers can insert unauthorized 
code.  The primary difference between an Easter egg and 
a  piece  of  malicious  software  is  the  developer’s  intent.  
Fortunately  for  most  instances,  the  developers  are  not 
malicious. 
3.2.2  Trap Doors During Distribution 
The report predicted trap doors inserted into the distri-
bution chain for software.  Today, we frequently see bo-
gus  e-mail  or  downloads  claiming  to  contain  important 
updates to widely used software that in fact contain Tro-
jan  horses.    One  very  recent  incident  of  this  kind  is  re-
ported in [19]. 
3.2.3  Boot-Sector Viruses 
The report predicted trap doors during installation and 
booting.      Today,  we  have  boot  sector  viruses  that  are 
quite common in the wild. 
Proceedings of the 18th Annual Computer Security Applications Conference (ACSAC(cid:146)02) 
1063-9527/02 $17.00 ' 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:27:47 UTC from IEEE Xplore.  Restrictions apply. 
3.2.4  Compiler Trap Doors 
The trap doors described in the report were patches to 
the  binary  object  files  of  the  system.      The  report  sug-
gested a countermeasure to such object code trap doors by 
having  customers  recompile  the  system  from  source,  al-
though  the report  also  notes  that  this  could  play  directly 
into the hands of the penetrator who has made changes in 
the  source  code.    In  fact,  the  AFDSC  Multics  contract 
specifically  required  that  Honeywell  deliver  source  code 
to the Pentagon to permit such recompilations.  However, 
the report pointed out the possibility that a trap door in the 
PL/I  compiler  could  install  trap  doors  into  the  Multics 
operating system when modules were compiled and could 
maintain its own existence by recognizing when the PL/I 
compiler was compiling itself.  This recognition was the 
basis  several  years  later  for  the  TCSEC  Class  A1  re-
quirement  for  generation  of  new  versions  from  source 
using  a  compiler  maintained  under  strict  configuration 
control.  A recent story on CNET news [14] reports that 
the  Chinese  government  has  similar  concerns  about 
planted trap doors. 
This suggestion proved an inspiration to Ken Thomp-
son who actually implemented the self-inserting compiler 
trap door into an early version of UNIX.  Thompson de-
scribed  his  trap  door  in  his  1984  Turing  Award  paper 
[40],  and  attributed  the  idea  to  an  “unknown  Air  Force 
document,” and he asked for a better citation.  The docu-
ment in question is in fact the Multics Security Evaluation 
report, and we gave a new copy of the report to Thomp-
son  after  his  paper  was  published.    Thompson  has  cor-
rected his citation in a reprint of the paper [39]. 
3.3  Auditing and Intrusion Detection 
Multics  had  some  limited  auditing  capabilities  (basi-
cally recording date time modified (DTM) on files) at the 
time  of  the  original  vulnerability  analysis,  although  a 
great deal more auditing was added as part of the Multics 
Security  Enhancements  project  [44]. 
  Section  3.4.4 
showed how a professional attacker could bypass the au-
diting capabilities while installing malicious software.   
This  has  major  implications  for  today,  because  intru-
sion detection has become a major area of security prod-
uct  development.      Unfortunately,  many  of  the  intrusion 
detection systems (IDSs) that have been developed do not 
deal  well  with  the  possibility  of  a  professional  attacker 
going  in  and  erasing  the  evidence  before  it  can  be  re-
ported to an administrator to take action.  Most  IDSs rely 
on  pattern  recognition  techniques  to  identify  behaviors 
that are indicative of an attack.  However, a professional 
penetrator  would  do  as  we  did  –  debug  the  penetration 
programs on a duplicate machine, the Rome Air Defense 
Center  Multics  system  in  our  case,    before  attacking  the 
desired target, the MIT site in our case.  This would give 
the IDS only a single isolated anomaly to detect, and the 
IDS  would  have  to  respond  before  the  professional  at-
tacker could erase the evidence as we did with the Multics 
DTM data.   
Most evaluations of IDS systems have focused on how 
well the IDS responds to a  series of pre-recorded traces.  
A more relevant evaluation of an IDS system would be to 
see  how  it  responds  to  a  well-prepared  and  well-funded 
professional (but ethical) attacker who is prepared to erase 
the auditing data and to install trap doors immediately for 
later  exploitation,  or  subvert  the  operating  system  on 
which the IDS is running to selectively bypass the IDS all 
together.    This  type  of  selectively  triggered  attack  of  a 
limited subsystem (i.e., the IDS) through a subverted host 
operating  system  is  illustrated  by  the  “key  string  trigger 
trap door” in section 3.4.5.1 of the report.  
4  What Happened Next? 
This  section  discusses  the  immediate  outcomes  from 
the Vulnerability Analysis. 
4.1  Multics Security Enhancements 
The primary outcome of the vulnerability analysis, and 
indeed the reason it was done at all was the Multics secu-
rity enhancements effort [44].  The Air Force had initially 
approached Honeywell with a proposal for an R&D effort 
to  improve  Multics  security  for  the  Air  Force  Data  Ser-
vices Center, but Honeywell initially declined on the basis 
that Multics was already sufficiently secure.  The vulner-
ability analysis was conducted to provide evidence of the 
need  for  significant  enhancements.    This  security  en-
hanced version of Multics ultimately achieved a Class B2 
evaluation from the National Computer Security Center in 
1985 [3]. 
4.2  Publicity 
The results of the vulnerability received a certain level 
of  press  publicity.    Fortune  magazine  published  [7]  an 
interview  with  Steve  Lipner  of  the  MITRE  Corporation 
that  was  a  bit  sensationalized.    It  focused  on  the  illegal 
operator’s  console  message  described  in  section  3.3.4.3.  
That message quoted from a Honeywell press release that 
appeared in an internal Honeywell newsletter and a Phoe-
nix, AZ newspaper article2 that incorrectly stated that the 
Air  Force  had  found  some  security  problems  in  Multics, 
but they were all fixed and that the Air Force now “certi-
fied” Multics security.  Not only was the Honeywell press 
release inaccurate, but it had  not been cleared for public 
release, as required in the Multics security enhancements 
contract.  While the Fortune article was a bit sensational, 
the story was then retold in a text book [26] in a version 
that was barely recognizable to us. 
A  much  better  description  of  the  work  was  done  by 
Whiteside[43], based on his earlier articles for The New 
Yorker.    Whiteside  interviewed  both  Schell  and  Lipner 
and did a much more accurate account of the effort.  
2 The authors have been unable to find copies of these articles and 
would appreciate any information on how to locate them. 
Proceedings of the 18th Annual Computer Security Applications Conference (ACSAC(cid:146)02) 
1063-9527/02 $17.00 ' 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:27:47 UTC from IEEE Xplore.  Restrictions apply. 
4.3  Multics Kernel Design Project 
As the report suggested in section 4.2.2, a project was 
started to build a security kernel version of Multics, one 
intended to ultimately meet what was later defined as the 
requirements  of  a  Class  A1  evaluation  of  the  Orange 
Book  [2].    This  effort  was  conducted  jointly  by  Honey-
well,  MIT,  the  MITRE  Corporation,  and  the  Air  Force 
with results published in:  [10, 33-35, 38]. 
4.4  Direction to Stop Work 
In August 1976, Air Force Systems Command directed 
termination  of  the  Air  Force’s  computer  security  work, 
including  the  Multics  Kernel  Design  Project,  despite  the 
fact  that  the  work  was  quite  successful  up  to  that  point, 
had widespread support from user agencies in the Army, 
the Navy, and the Air Force, and had been by DoD stan-
dards, quite inexpensive.  The adverse impact of that ter-
mination on the DoD is described in [31].  Another view 
of those events can be seen in [23, pp. II-74 – II-75] and 
in the GAO report on the termination [4]. 
5  Dispelling Mythology 
The  Multics  vulnerability  analysis  got  enough  public-
ity  that  a  certain  amount  of  mythology  developed  about 
what happened.  We hope this section can dispel at least  
some of that mythology. 
One  story  that  circulated  was  that  the  penetrations 
were only possible, because the authors had special privi-
leged  access  to  Multics.    While  both  authors  had  been 
members  of  the  MIT  Multics  development  team  (Roger 
Schell  as  a  Ph.D.  student  and  Paul  Karger  as  an  under-
graduate),  both  had  left  the  Multics  team  months  before 
the vulnerability analysis was even considered.  No privi-
leged accounts on either of those systems were used in the 
analysis.    (Privileged  accounts  were  used  on  RADC-
Multics  and  at  the  Air  Force  Data  Services  Center,  but 
those were only to show that a penetrator could take ad-
vantage of a duplicate system to debug penetrations ahead 
of  time  and  to  verify  the  distribution  of  the  trap  door.)  
The  Air  Force  purchased  computer  time  on  the  MIT-
Multics  system  and  on  Honeywell’s  Multics  system  in 
Phoenix,  AZ,  but  both  systems  routinely  sold    computer 
time.    Multics  source  code  was  readable  by  any  user  on 
the  MIT  site,  much  as  source  code  for  open  source  sys-
tems is generally available today. 
The entire vulnerability analysis was carried out with a 
ground  rule  of  not  causing  any  real  harm  to  any  of  the 
systems under attack.  All the documentation and listings 
of exploit programs was kept in a safe approved for stor-
ing  TOP  SECRET  information  (although  that  particular 
safe actually had no classified material in it).  Decrypted 
password files were also stored in the safe, to ensure that 
no  user  accounts  would  be  compromised.    Exploit  pro-
grams were protected with very restrictive access control 
lists and kept encrypted at all times, except when actually 
in  use.    The  applicable  contracts  specified  that  the  pur-
chased computer time at MIT and Phoenix would be used 
for security testing and evaluation purposes, so there was 
no question of the legality of the analysis.   Finally, publi-
cation  of  [24]  was  delayed  until  after  all  of  the  penetra-
tions described were repaired. 
For  several  years  after  the  completion  of  the  vulner-
ability  analysis,  the  authors  were  periodically  contacted 
when  unexplained  system  crashes  occurred  on  the  MIT 
site, just in case they had been caused by a remaining hid-
den trap door.  However, as part of the basic policy of the 
analysis  to  cause  no  real  harm,  no  trap  doors  were  left 
behind after the conclusion of the vulnerability analysis. 
6  Security has gotten worse, not better 
Today, government and commercial interest in achiev-
ing  processing  and  connectivity  encompassing  disparate 
security  interests  is  driving  efforts  to  provide  security 
enhancements  to  contemporary  operating  systems  [27].  
These  efforts  include  the  addition  of  mandatory  access 
controls and “hardening” (i.e., the removal of unnecessary 
services.)    Such  enhancements  result  in  systems  less  se-
cure  than  Multics,  and  Multics,  with  its  security  en-
hancements, was only deemed suitable for processing in a 
relatively benign “closed” environment [5].   It was con-
cluded  that  operation  of  Multics  in  an  “open”  environ-
ment would require restructuring around a verifiable secu-
rity  kernel  to  address  the  threat  of  professional  attacks 
using malicious software (e.g., trap doors).   
6.1  Weak Solutions in Open Environments 
Given the understanding of system vulnerabilities that 
existed  nearly  thirty  years  ago,  today’s  “security  en-
hanced”  or  “trusted”  systems  would  not  be  considered 
suitable  for  processing  even  in  the  benign  closed  envi-
ronment.    Also,  considering  the  extent  of  network  inter-
connectivity  and  the  amount  of  commercial  off-the-shelf 
(COTS) and other software without pedigree (e.g., librar-
ies  mined  from  the  Web),  today’s  military  and  commer-
cial  environments  would  be  considered  very  much 
“open”.    To  make  matters  worse,  everyone  is  now  ex-