### Triggered by a Password or Key
The vulnerability was designed to be triggered by a password or key, ensuring that it would not be accidentally invoked or discovered through quality assurance or other testing processes.

### Section 3.4.6: Trap Door in the 6180 Development System
Section 3.4.6 briefly mentioned a trap door installed in the `hcs_$set_ring_brackets_` routine of the 6180 development system at MIT. We confirmed that this trap door was distributed by Honeywell to the 6180 processor installed at the Air Force Data Services Center (AFDSC) in the basement of the Pentagon, an area now part of the Metro station. Despite the hint in Section 3.4.6, the trap door remained undetected for about a year after the report was published. During an intensive security audit at the General Motors Multics site, the trap door, which was neutered to be benign (requiring only a one-instruction change to become actively malicious), produced an anomalous error return. The discovery of the trap door is documented on the Multics website [45], though the installation is incorrectly attributed to The MITRE Corporation.

### 3.2 Malicious Software Predictions
In addition to demonstrating the feasibility of embedding a trap door in commercial software and having the manufacturer distribute it globally, the report hypothesized various other malicious software attacks, all of which have since materialized. Section 3.4.5.1 proposed several potential attack vectors.

#### 3.2.1 Malicious Developers
We suggested that malicious developers might create harmful software. Since then, numerous products have been found with surreptitious backdoors allowing developers access to customer machines or with so-called "Easter eggs" that perform humorous actions when activated. As expected, most Easter eggs are triggered by unique keys or passwords unlikely to be encountered even during extensive quality assurance testing. In most cases, these Easter eggs were not authorized by development managers and serve as examples of how developers can insert unauthorized code. The primary difference between an Easter egg and malicious software lies in the developer's intent. Fortunately, in most instances, developers are not malicious.

#### 3.2.2 Trap Doors During Distribution
The report predicted the insertion of trap doors into the software distribution chain. Today, we frequently encounter fraudulent emails or downloads claiming to contain important updates but actually containing Trojan horses. A recent example of this is reported in [19].

#### 3.2.3 Boot-Sector Viruses
The report also predicted trap doors during installation and booting. Today, boot sector viruses are quite common in the wild.

#### 3.2.4 Compiler Trap Doors
The trap doors described in the report were patches to the binary object files of the system. The report suggested a countermeasure by having customers recompile the system from source, although it noted that this could play into the hands of an attacker who had modified the source code. The AFDSC Multics contract specifically required Honeywell to deliver source code to the Pentagon to allow such recompilations. However, the report pointed out the possibility that a trap door in the PL/I compiler could install trap doors into the Multics operating system during compilation and maintain its existence by recognizing when the PL/I compiler was compiling itself. This recognition later formed the basis for the TCSEC Class A1 requirement for generating new versions from source using a compiler under strict configuration control. A recent CNET news story [14] reports similar concerns by the Chinese government about planted trap doors.

This suggestion inspired Ken Thompson, who implemented a self-inserting compiler trap door in an early version of UNIX. Thompson described his trap door in his 1984 Turing Award paper [40], attributing the idea to an "unknown Air Force document." After the publication of his paper, we provided Thompson with a copy of the Multics Security Evaluation report, and he corrected his citation in a reprint [39].

### 3.3 Auditing and Intrusion Detection
Multics had limited auditing capabilities (primarily recording date-time-modified (DTM) on files) at the time of the original vulnerability analysis. More extensive auditing was added as part of the Multics Security Enhancements project [44]. Section 3.4.4 showed how a professional attacker could bypass these auditing capabilities while installing malicious software. This has significant implications today, as intrusion detection has become a major focus in security product development. Unfortunately, many intrusion detection systems (IDSs) do not effectively handle the possibility of a professional attacker erasing evidence before it can be reported to an administrator. Most IDSs rely on pattern recognition to identify attack behaviors, but a professional penetrator would debug penetration programs on a duplicate machine, as we did with the Rome Air Defense Center Multics system, before attacking the desired target, the MIT site. This would leave the IDS with only a single isolated anomaly to detect, and the IDS would need to respond before the attacker could erase the evidence, as we did with the Multics DTM data.

Most evaluations of IDS systems focus on how well they respond to pre-recorded traces. A more relevant evaluation would involve a well-prepared and well-funded professional (but ethical) attacker who erases auditing data and installs trap doors for later exploitation or subverts the host operating system to selectively bypass the IDS. This type of selectively triggered attack on a limited subsystem (e.g., the IDS) through a subverted host operating system is illustrated by the "key string trigger trap door" in Section 3.4.5.1 of the report.

### 4. What Happened Next?
This section discusses the immediate outcomes of the Vulnerability Analysis.

#### 4.1 Multics Security Enhancements
The primary outcome of the vulnerability analysis, and the reason it was conducted, was the Multics security enhancements effort [44]. Initially, the Air Force approached Honeywell with a proposal to improve Multics security for the AFDSC, but Honeywell declined, stating that Multics was already sufficiently secure. The vulnerability analysis provided evidence of the need for significant enhancements. The security-enhanced version of Multics ultimately achieved a Class B2 evaluation from the National Computer Security Center in 1985 [3].

#### 4.2 Publicity
The results of the vulnerability analysis received some press coverage. Fortune magazine published [7] an interview with Steve Lipner of The MITRE Corporation, which focused on the illegal operator's console message described in Section 3.3.4.3. The message quoted a Honeywell press release that appeared in an internal newsletter and a Phoenix, AZ newspaper article, incorrectly stating that the Air Force had found and fixed security problems in Multics and now "certified" its security. Not only was the press release inaccurate, but it had not been cleared for public release, as required by the Multics security enhancements contract. While the Fortune article was sensational, the story was retold in a textbook [26] in a barely recognizable form.

A more accurate description of the work was provided by Whiteside [43], based on his earlier articles for The New Yorker. Whiteside interviewed both Schell and Lipner and provided a much more accurate account of the effort.

#### 4.3 Multics Kernel Design Project
As suggested in Section 4.2.2, a project was initiated to build a security kernel version of Multics, intended to meet the requirements of a Class A1 evaluation of the Orange Book [2]. This effort was conducted jointly by Honeywell, MIT, The MITRE Corporation, and the Air Force, with results published in [10, 33-35, 38].

#### 4.4 Direction to Stop Work
In August 1976, the Air Force Systems Command directed the termination of the Air Force's computer security work, including the Multics Kernel Design Project. This decision was made despite the project's success, widespread support from user agencies, and relatively low cost. The adverse impact of this termination on the DoD is described in [31]. Another perspective on these events can be found in [23, pp. II-74 – II-75] and in the GAO report on the termination [4].

### 5. Dispelling Mythology
The Multics vulnerability analysis received enough publicity to generate some mythology. We hope this section can dispel at least some of that mythology.

One myth was that the penetrations were possible only because the authors had special privileged access to Multics. While both authors had been members of the MIT Multics development team (Roger Schell as a Ph.D. student and Paul Karger as an undergraduate), they had left the team months before the vulnerability analysis was considered. No privileged accounts were used in the analysis. Privileged accounts were used on RADC-Multics and at the AFDSC, but only to show that a penetrator could use a duplicate system to debug penetrations ahead of time and to verify the distribution of the trap door. The Air Force purchased computer time on the MIT-Multics system and on Honeywell’s Multics system in Phoenix, AZ, but both systems routinely sold computer time. Multics source code was readable by any user on the MIT site, similar to open-source systems today.

The entire vulnerability analysis was conducted with the ground rule of not causing any real harm to the systems under attack. All documentation and listings of exploit programs were kept in a safe approved for storing TOP SECRET information (though no classified material was actually stored there). Decrypted password files were also stored in the safe to ensure that no user accounts would be compromised. Exploit programs were protected with very restrictive access control lists and kept encrypted at all times, except when in use. The applicable contracts specified that the purchased computer time at MIT and Phoenix would be used for security testing and evaluation purposes, so there was no question of the legality of the analysis. Finally, the publication of [24] was delayed until all the penetrations described were repaired.

For several years after the completion of the vulnerability analysis, the authors were periodically contacted when unexplained system crashes occurred on the MIT site, just in case they were caused by a remaining hidden trap door. However, as part of the basic policy of the analysis to cause no real harm, no trap doors were left behind after the conclusion of the vulnerability analysis.

### 6. Security Has Gotten Worse, Not Better
Today, government and commercial interest in achieving processing and connectivity encompassing disparate security interests is driving efforts to provide security enhancements to contemporary operating systems [27]. These efforts include the addition of mandatory access controls and "hardening" (i.e., the removal of unnecessary services). Such enhancements result in systems less secure than Multics, which, with its security enhancements, was deemed suitable only for processing in a relatively benign "closed" environment [5]. It was concluded that operating Multics in an "open" environment would require restructuring around a verifiable security kernel to address the threat of professional attacks using malicious software (e.g., trap doors).

#### 6.1 Weak Solutions in Open Environments
Given the understanding of system vulnerabilities that existed nearly thirty years ago, today’s "security enhanced" or "trusted" systems would not be considered suitable for processing even in a benign closed environment. Considering the extent of network interconnectivity and the amount of commercial off-the-shelf (COTS) and other software without pedigree (e.g., libraries mined from the Web), today’s military and commercial environments are very much "open." To make matters worse, everyone is now exposed to a wide range of threats, making the need for robust security solutions more critical than ever.