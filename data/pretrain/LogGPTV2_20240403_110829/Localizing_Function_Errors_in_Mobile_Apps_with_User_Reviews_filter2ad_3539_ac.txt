example, in Fig. 4, we extract the text “Show password”
of the CheckBox object. Then, we extract the invisible label
information by parsing their ids. For example, in Fig. 4, we
collect the id (i.e., show_password) of CheckBox and split
it into a series of words (i.e., “show”, “password”).
IV. LOCALIZING FUNCTION ERRORS
By correlating the information extracted from user reviews
and apps, ReviewSolver ﬁrst localizes app speciﬁc errors
(Section IV-A) and general errors (Section IV-B), and then
ranks the selected classes before recommending them.
A. Localizing App Speciﬁc Errors
To localize the app speciﬁc errors, we leverage various
information extracted from the apk ﬁle.
Using Class/Method Name If the error appears when per-
forming app speciﬁc tasks, for each verb phrase extracted from
function error review, we check whether it is similar to that
of each method. If so, we recommend the developer to check
the corresponding method. Motivated by the method in [33],
we leverage the camel case to convert the method name to
verb phrase. For example, we transform getEmail() to “get
Email”. If the method name only contains a verb, we use the
words extracted from the class names as the object of the verb
phrase (e.g., we transform MessageListFragment.move() to
“move Message List Fragment”). Since the life-cycle methods
in Android apps (e.g., onCreate()) may have the same method
names, to correctly describe their functions, we remove their
stopwords (e.g., “on”) and combine the remaining verbs with
component names to create verb phrase.
To determine whether two phrases are similar or not, we
leverage Word2Vec [36] to calculate the semantic similarity
between two phrases, because representing the word with a
series of words can capture syntactic and semantic regularities
between words [37], [38]. More precisely, by using the model
trained on Google News dataset (contains 300-dimensional
vectors for 3 million words and phrases) [39], we transform
each word (wordi, i = 1, .., n) of the phrase into a 300-
dimensional vector. We combine them to get the vector of the
phrase.
V ector(phrase) =
n(cid:2)
i=1
1
n
V ector(wordi)
Then we calculate the cosine similarity between two phrase
vectors. If the similarity is higher than the threshold value
(0.68 by referring [40]), we regard them as similar ones.
V ector(phrase1) • V ector(phrase2)
(cid:2)V ector(phrase1)(cid:2)(cid:2)V ector(phrase2)(cid:2)
CosineSimilarity =
Using Visible/Invisible Label Information To localize the
errors related to GUI, we compare the verb/noun phrase
extracted from review with the visible and invisible label
information extracted from code. For the former, we check
the noun phrase extracted from review. If the user explicitly
points out the widget (e.g., “reply button”), we regard the
phrase as GUI related phrase. In this case, we extract the
word for modifying the widget (e.g., “reply”) and look it up
on the visible label information. For the latter, we check the
verb phrase extracted from the user review by comparing its
semantic meaning with the verb phrase transformed from the
invisible label information.
When manually reading the function error review, we ﬁnd
that users can also vaguely describe the error by using the
two semantic patterns shown in Table II. [f unction] means
the problem function. N EG means negation related words
(e.g., “cannot”) and phrases (e.g., “does not”). To localize such
errors, we ﬁrst extract the f unction word of P1 and P2, and
then look them up on the GUI’s visible label information. The
activities that contain these words will be recommended to
developer. For example, for P2, we recommend the developer
to check the activity that contains the verb “register”.
TWO SEMANTIC PATTERNS OF VAGUELY DESCRIBING THE ERROR.
TABLE II
#
P1
P2
Semantic Pattern
[f unction] N EG work
[subject] N EG [f unction]
Example
“sync does not work”
“I cannot register”
Localizing Errors Related to Error Message Users may
describe the error message precisely. For example, given the
review “I receive an error message saying “Failed to send
some messages””, we extract the error message and compare
it with the error messages extracted from the app’s apk ﬁle.
Sometimes, the user may simply point out the type of the
error, and hence we ﬁrst check whether the noun phrases
contain error related words (e.g., “error”, ”bug”, “fault”). If
so, we extract
the word that modiﬁes these error related
words. Then, we check all the APIs invoked in code. If the
API’s description mentions this word, we recommend the
corresponding class to the developer. For example, in the
review “a connection error message at the bottom”, since
the user mentions that the error is related to “connection”, we
recommend the developer to check the classes that call the
API HttpURLConnection.getInputStream().
423
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:51:06 UTC from IEEE Xplore.  Restrictions apply. 
Localizing Errors Related to Opening App If the function
error review contains verb phrases such as “open app”, “launch
app”, or “start app”, the error may appear when the app is
launched. Since the onCreate(), onStart(), and onResume()
methods of the starting activity are called sequentially when
an app is launched, for this kind of error, we recommend the
developer to check these three methods of the starting activity.
Localizing Errors Related to Account Registration If the
function error review contains verb phrases such as “register
account”, “sign in”, “login in” or if the review contains noun
phrase such as “registration”,
the error may appear when
registering account. For this kind of error, we recommend the
developer to check the activity related to registering account.
We search the text content of each activity and report the
activity that contains phrases related to account registration
(e.g., “sign in”, “login”).
Localizing Errors Related to App Updating If the function
error review contains updating related phrases (e.g., “update
app”, “latest update”, “new update”, “recent update”), this
error may be caused by the app update. For such kind of
error, we ﬁrst check other verb/noun phrases of the review. If
they can be mapped to the app speciﬁc error or general error,
we extract the corresponding classes and recommend them to
developers. Otherwise, we recommend the developer to check
the code difference between the latest two versions.
B. Localizing General Errors
We propose Algorithm 1 to locate the errors related to
API/URI/intent. For API, we compare the verb phrase ex-
tracted from review with the verb phrase related to the API
(line 3-5 in Algorithm 1). For URI, we compare the object of
the verb phrase extracted from review with the noun phrase
related to the URI (line 11-13 in Algorithm 1). For intent,
we compare the object of the verb phrase extracted from
review with the noun phrase related to the intent (line 19-21 in
Algorithm 1). If they are similar, we recommend the developer
to check the API/URI/intent and corresponding class.
We extract the verb phrase related to API from API sig-
nature, description, and permission. The signature of an API
contains its class, return value, method name and parameters
(e.g., ). We
convert the API signature into verb phrase by using the method
described in Section IV-A. We also extract verb phrases from
its ofﬁcial description by using the typed dependency [20]. For
example, we extract verb phrases such as “open communica-
tion link”, “establish connection” from the description of the
API URLConnection.connect().
For the verb phrase extracted from review, if its verb (or its
synonyms) is included in the method name of the API and the
object is included in the class description of the API, we also
recommend the developer to check the API and corresponding
class. For example, “connect server” can be mapped to the
API HttpURLConnection.connect() since the verb “connect”
is included in the API’s method name and “server” in the
ofﬁcial description of the class HttpURLConnection. If the
verb of the verb phrase extracted from review is related to
information collection (e.g., “gather”), access (e.g., “read”),
or utilization (e.g., “use”) related verbs [41] and its object is
ALGORITHM 1: Find the classes related to the API/URI/intent.
Input: V erbP hrase: Verb phrase extracted from the review; ApiSet:
APIs provided by Android document; U riSet: URIs provided by
PScout; IntentSet: intent provided by Android document.
if Similar(V erbP hrase, ApiP hrase)==true then
end
end
ClassList.add(getCaller(AP I))
ApiP hraseList = getAP IRelatedP hrases(AP I)
for ApiP hrase in ApiP hraseList do
U riN ounList = getU RIRelatedN ouns(U RI)
for U riN oun in U riN ounList do
Output: ClassList: the classes related to API/URI/intent.
1 ClassList = {}
2 for AP I in ApiSet do
3
4
5
6
7
8
9 end
10 for U RI in U riSet do
11
12
13
14
15
16
17 end
18 for Intent in IntentSet do
19
20
21
22
23
24
25 end
26 return ClassList;
ClassList.add(getCaller(Intent))
ClassList.add(getCaller(U RI))
end
end
end
end
if Similar(getObj(V erbP hrase), U riN oun)==true then
IntentN ounList = getIntentRelatedN ouns(Intent)
for IntentN oun in IntentN ounList do
if Similar(getObj(V erbP hrase), IntentN oun)==true then
similar to the personal information protected by permission,
we also recommend the developer to check this permission
related API and corresponding class.
Since there is no ofﬁcial description of URI, we cannot
extract verb phrases related to URI. To map the function error
review to URI, we compare the noun phrases described in
review with the noun phrases related to the URI. To obtain
the latter, we ﬁrst leverage PScout [31] to get the permission
related to the URI. Then, we regard the noun phrase extracted
from the permission description [42] as the noun phrase related
to URI. For example, the URI “content://call_log” is
protected by the READ_CALL_LOG permission. We extract
“call log” from the permission description (i.e., “Allows an
application to read the user’s call log.”).
Moreover, we manually deﬁne the noun phrase of each
intent by referring the Android ofﬁcial document. The Android
ofﬁcial document [43] provides 11 kinds of common intents.
For example, “camera” is related to the intent with the action
android.media.action.IMAGE_CAPTURE.
C. Ranking the Classes
Since we employ multiple approaches to map function error
reviews to code, one review may be mapped to multiple
classes. We compute the importance of these classes, and
recommend the top N most related ones to developers. Assume
that we ﬁnd n mappings between verb/noun phrases and
classes (i.e., m1, m2, ..., mn), mi =,
by using the approaches proposed in Section IV-A and Sec-
tion IV-B. For each class, we calculate the importance by
counting the number of mappings between different phrases
and the target class. For example, if we ﬁnd one mapping
, the importance of classA will be
424
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:51:06 UTC from IEEE Xplore.  Restrictions apply. 
increased by one. The selected classes are ranked according
to their importance.
V. EXPERIMENTAL RESULT
In this section, we conduct extensive experiments to answer
the following research questions:
RQ1: Can ReviewSolver correctly identify reviews re-
lated to function errors (Section V-B)?
RQ2: How is the performance of ReviewSolver com-
pared with the state-of-the-art system ChangeAdvisor [11]
(Section V-C)?
RQ3: How many function error related reviews can be
addressed by ReviewSolver (Section V-D)?
A. Dataset
To measure how many function error related reviews can
be solved, we select 18 apps that can be downloaded from
Google Play andprovide source code in F-Droid or Github.
For each app, we download the latest version of apk ﬁle and
user reviews. In particular, we collect 69,359 reviews from
Google Play and 12,735 of them are negative ones (rated 1 or
2 starts).
To answer RQ2, we ﬁrst employ ReviewSolver to
identify the function error related reviews, and then apply
ReviewSolver and ChangeAdvisor [11] to mapping
such reviews to code, respectively. We ask three research
students to construct the ground truth of the mappings from
reviews to code by exploiting bug reports to correlate them.
More precisely, as shown in Fig. 5, after reading a function
error related review, the student identiﬁes the bug described
in in and then looks for the bug in the existing bug reports. If
found and the bug has been ﬁxed, the corresponding code ﬁles
modiﬁed by the developers are regarded as the code related to
the review. Since not all apps have bug reports, we get 8 apps
with bug reports, and for each app 200 function error related
reviews are analyzed.
Bug described 
in review
Function Error 
Related Review
Bug Reports
Files 
modiﬁed in 
bug report
Fig. 5. Procedure of building ground truth
B. Review Identiﬁcation Performance
To evaluate the performance of classifying function error
related reviews, we adopt the dataset provided by Ciurumelea
el al. [16]. This dataset contains 199 reviews (87 of them are
function error related ones). The result is shown in Table III.
Our system achieves 84.6% precision and 88.5% recall rate
for detecting function error related reviews, and we manually
analyze the cause of false positives/negatives.
False positives. The major cause of false positive is that
although some reviews contain function error related words
(e.g., “bug”, “problem”), the objects that user really wanted
to describe are some ﬁxed bugs, small limitations, or bugs of
other apps. For example, “Amazing This app helped me a lot.
Allowed me to see why my apps crashed so I could ﬁx the
bugs”. To remove such false positives, we could analyze the
tense of the review to identify the ﬁxed bugs (e.g., “... has
been ﬁxed”) and check the subject related to the bug (e.g.,
“my apps”).
False negatives. The major cause of false negative is that
users may describe function errors implicitly. For example, the
review “Slow on tablets In need of a major update. Images not
as crisp or bright as on jjComic Viewer or Perfect Viewer.”
does not contain any function error related words (e.g., “bug”,
“error”), and the user only described that the error makes
the tablet “Slow”, thus ReviewSolver cannot recognize it.
We can add the function error related reviews that describe
the error implicitly into the training set to remove such false
negatives.
Answer to RQ1: The experimental result shows that:
ReviewSolver can achieve 84.6% precision, 88.5% recall
rate for identifying function error related reviews.
C. Performance of ReviewSolver
We use the mapping from user reviews to code with ground
truth (described in Section V-A) to evaluate ReviewSolver
and compare it with ChangeAdvisor. The “#Total Map-
pings” column of Table IV shows the total number of
mappings from reviews to code ﬁles with ground truth.
From the “#RS True Mappings” and “#CA True Mappings”
columns of the Table IV, we can see that ReviewSolver
can identify more mappings than the state-of-the-art sys-
tem (i.e., ChangeAdvisor). For example,
the app
com.fsck.k9, ReviewSolver can identify 17 mappings
whereas ChangeAdvisor can only ﬁnd 2. In total,
the
number of mapping identiﬁed by ReviewSolver (i.e., 79) is
twice of the number of mappings found by ChangeAdvisor
(i.e., 31).
for
TABLE IV
THE NUMBER OF MAPPINGS THAT CAN BE IDENTIFIED BY
RE V I E WSO L V E R AND CH A N G EAD V I S O R. THE MEANING OF EACH
COLUMN (FROM 2-5): TOTAL NUMBER OF FUNCTION ERROR REVIEWS
THAT CAN BE MAPPED TO BUG REPORTS, THE NUMBER OF MAPPINGS
IDENTIFIED BY USING BUG REPORTS, THE NUMBER OF MAPPINGS
IDENTIFIED BY RE V I E WSO L V E R (COLUMN “#RS TRUE MAPPINGS”) AND
CH A N G EAD V I S O R (COLUMN “#CA TRUE MAPPINGS”)
#CA True
#Error
Reviews Mappings Mappings Mappings
#RS True
#Total
Apk
Name
org.mariotaku.twidere
org.thoughtcrime.securesms
com.fsck.k9
com.battlelancer.seriesguide
org.wordpress.android
cgeo.geocaching
com.joulespersecond.seattlebusbot
de.danoeh.antennapod
Total
78
50
55
65
94
38
24
43
447
314
132
125
216
275
342
154
230
1,788
15
4
17
8
11
11
9