(cid:8)(cid:10)
(cid:2)(cid:8)(cid:10)(cid:13)(cid:11)(cid:12)(cid:1)(cid:9)
(cid:13)(cid:9)(cid:14)(cid:5) (cid:13)(cid:9)(cid:14)(cid:5)
(cid:8)(cid:11)
(cid:8)(cid:12)
(cid:15)(cid:14)
(cid:15)(cid:14)
(cid:8)(cid:11)
(cid:15)
(cid:15)
(cid:3)(cid:2)(cid:1)
(cid:12)(cid:22)(cid:21)(cid:26)(cid:27)(cid:25)(cid:28)(cid:12)(cid:27)
(cid:13)(cid:16)(cid:14)(cid:5) (cid:13)(cid:16)(cid:14)(cid:5)
(cid:8)(cid:4)(cid:13)(cid:10)
(cid:8)(cid:4)
(cid:13)(cid:17)(cid:14)(cid:4) (cid:13)(cid:17)(cid:14)(cid:7)
(cid:8)(cid:4)(cid:13)(cid:10)
(cid:8)(cid:4)
(cid:13)(cid:9)(cid:14)(cid:4)
(cid:8)(cid:4)
(cid:13)(cid:9)(cid:14)(cid:5)
(cid:8)(cid:4)(cid:13)(cid:10)
(cid:8)
(cid:8)
(cid:8)
(cid:8)(cid:28)(cid:19)(cid:27)(cid:18)(cid:29)(cid:11)(cid:25)(cid:18)(cid:11)(cid:27)(cid:14)(cid:1)(cid:9)(cid:14)(cid:19)(cid:11)(cid:27)(cid:18)(cid:22)(cid:21)(cid:26)(cid:17)(cid:18)(cid:23)(cid:1)
(cid:5)(cid:25)(cid:11)(cid:23)(cid:17)
(cid:26)(cid:14)(cid:21)(cid:26)(cid:22)(cid:25)
(cid:25)(cid:14)(cid:19)(cid:11)(cid:27)(cid:18)(cid:22)(cid:21)(cid:26)(cid:17)(cid:18)(cid:23)
(cid:6)(cid:21)(cid:22)(cid:30)(cid:19)(cid:14)(cid:13)(cid:16)(cid:14)(cid:1)
(cid:3)(cid:18)(cid:26)(cid:12)(cid:22)(cid:29)(cid:14)(cid:25)(cid:31)
(cid:2)(cid:21)(cid:22)(cid:20)(cid:11)(cid:19)(cid:31)(cid:1)
(cid:3)(cid:14)(cid:27)(cid:14)(cid:12)(cid:27)(cid:18)(cid:22)(cid:21)
Fig. 1: Multivariate Discrete Event Sequence Analytics Framework.
during the entire sampling period, then this sequence cannot
provide any contribution to the Language Translation Model
(see section II-A3). We therefore exclude sequences with
constant events, essentially we discard these sensors. Note
that discarded sensors are not used in the online testing
phase.
• Discrete Event Encryption: The second step encrypts
each event record into a character. For each sequence, we
collect the unique set of event records and sort them in
alphanumeric order. Then we assign letters to each unique
event record1. To differentiate across multiple sequences, we
preﬁx the sensor name in front of the character. For example,
the event record “on” in the sequence of sensor 1 is coded
as “s1.a”. The purpose of encryption is to map the event
record into an alphabet so that the transformed sequence
becomes a sensor language.
2) Language Sequence Generation: Once the encoded
characters are obtained for each event record, we group the
characters into words and sentences in order to leverage
existing NMT models [23].
• Converting Sequences to Words: We compose “words” of
equal length of i characters. Using a sliding window of j
characters, we generate the next word. For example if j = 1,
we use the ﬁrst i characters of the sequence to compose the
ﬁrst word, the second to the i + 1th characters to compose
the second word, and the third to the i + 2th characters
to compose the third word, and so on. The distinct set of
words derived by each sensor is the sensor vocabulary. One
can choose meaningful values for i and j according to the
dataset’s sampling granularity. For a sample that uses per
minute recording, i = 10 implies that each word contains
the information of 10 minutes. If the sample uses per day
recording, i = 7 implies that each word contains a week’s
worth of information. The value of j regulates the overlap
between two consecutive words.
• Word to Sentence Conversion: We group words into
sentences of equal
length by setting the length of one
sentence to m words with a sliding window of n words.
The choice of m depends on the sampling granularity to
include meaningful information. The choice of n decides
1We always reserve a special character (i.e., ) for any unknown
system states which may occur in online testing.
554
the overlap of two consecutive sentences and determines
the granularity of detection. For example, with a per minute
sampling granularity and n = 1, detection can be performed
every minute. Clearly, such ﬁne-grained detection requires
longer ofﬂine training time. The parameter n essentially
controls the trade-off of the granularity of detection and
training time and can be adjusted according to the prediction
needs of the speciﬁc system.
Having transformed the multivariate discrete event se-
t }) into a corpus of sensor lan-
quences of all sensors ({Xk
t }) (composed of words and sentences), we apply
guages ({Zk
the NMT model to establish pairwise relationships among
sensors.
3) Generation of the Language Translation Model: Algo-
rithm 1 presents the main steps for generating the multivariate
relationship graph G. Given two language sequences from the
t , k ∈ N}, the algorithm applies the
multi-language corpus {Zk
NMT model to build two directional pairwise relationships
for each pair of language sequences (sensors). We there-
fore construct the multivariate relationship graph G, where
nodes represent sensors and edges represent
the modeled
relationships between sensors. Such graph models the inter-
relationships among sensors in the system and can be used to
deliver meaningful system-level knowledge.
Algorithm 1 Multivariate Relationship Graph Generation
Sequences
Training
t , k ∈ [1, 2, ..., N], t ∈ [1, 2, ..., T]}
Input: Multivariate
{Zk
Output: Multivariate Relationship Graph G
for {Zi
t , i (cid:3)= j ∈ N do
t} ∈Z k
t, Zj
Language
g(i, j) ← directional NMT model for (i, j) sensor pair
s(i, j) ← directional translation score for (i, j) sensor
G ← g(i, j) and s(i, j)
pair
return G
Neural Machine Translation (NMT) Model: The neural
machine translation model uses a multi-layered Long Short-
Term Memory (LSTM) to map the sentences of source lan-
guage to a vector of a ﬁxed dimensionality and uses another
LSTM model to decode the vector into the sentences of target
language [23], [37]. We transform the problem from originally
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:40 UTC from IEEE Xplore.  Restrictions apply. 
extracting relationship of multivariate discrete event sequences
to translating multi-lingual sensor “languages”. We apply the
NMT model to construct the nonlinear relationship g(i, j)
between each pair of sensors. Here, we use the state-of-the-art
seq2seq model with attention mechanism [23].
Model Translation Score:
The model translation score
s(i, j) quantiﬁes the relationship between a pair of sensors
(i, j). Here, we use BiLingual Evaluation Understudy (BLEU)
score [31], the most commonly used metric to quantitatively
evaluate the quality of machine translations. The score ranges
from 0 to 100. A higher value indicates a better translation.
By using the same architecture and parameter settings to train
the NMT models for all sensor pairs, the BLEU scores are
comparable across different pairs.
B. Knowledge Discovery
The multivariate relationship graph G produced by Algo-
rithm 1 provides useful knowledge and insights for system-
wide and component-wise relationships among different sen-
sors: (a) discovery of system components (i.e., clusters of
sensors) and (b) extraction of relationships between system
components. Two types of subgraphs can be extracted from
the multivariate relationship graph G: global subgraphs and
local subgraphs. These graphs can provide meaningful in-
formation on the system. For example, to discover structural
information such as cluster structures of local sensors, we
can apply to these graphs a random walk-based community
detection algorithm [33]. Sensors in each local cluster likely
originate from the same system component.
Clusters of sensors can be either isolated or connected.
The sensors and edges connecting two clusters are critical as
they are potentially responsible for error propagation after an
anomaly occurs. Such component-wise knowledge offers root
cause localization and insights on anomaly propagation for
fault diagnosis. Section III-B provides a detailed discussion
using a case study.
C. Anomaly Detection
Once the multivariate relationship graph G is generated
through ofﬂine training, it can be used for online anomaly
detection. Assuming that there is no anomaly in the training
data, G represents system behavior under normal operation.
Our focus is on detecting system anomalies when multiple
sensors behave abnormally and lead to different relation-
ships/interactions from those modeled in the ofﬂine training
period. To this end, we deﬁne a metric called anomaly score
to quantify the “signiﬁcance” of anomalies (see deﬁnition in
the next paragraph). Algorithm 2 presents the main steps for
detecting an anomaly.
t , k ∈
N} and the graph G, an anomaly is detected at time stamp
t by applying the directional NMT model to the sensor pair
(i, j) provided that the NMT model g(i, j) is a valid one. The
validity of NMT model g(i, j) is determined by the range of
Given the multivariate testing language sequences {Yk
Algorithm 2 Anomaly Detection
Input: Multivariate Testing Language Sequences
t , k ∈ [1, 2, ..., N], t ∈ [1, 2, ..., L]},
{Yk
Multivariate Relationship Graph G
Output: System Anomaly Score {at, t ∈ [1, 2, ..., L]},
Sensor Pair Alert Status {Wt, t ∈ [1, 2, ..., L]}
for t in [1, 2, ..., L] do
t} ∈Y k
at ← 0, pt ← 0, Wt ← 0,
for {Yi
t, Yj
t , i (cid:3)= j ∈ N do
then
if directional NMT model g(i, j) is a valid model
pt ← pt + 1
f (i, j) ← output BLEU scores by applying
directional NMT model g(i, j) for (i, j) sensor pair
if f (i, j) < s(i, j) then
at ← at + 1
Wt(i, j) ← 1
at ← at/pt
return at, Wt
BLEU score s(i, j) set by the user 2. A broken relationship
between sensor pair (i, j) at time stamp t is detected if the
testing BLEU score f (i, j) is smaller than the BLEU score
s(i, j) obtained by training (see Algorithm 1).
One broken relationship may not be effective to detect
anomalies in a complex real-world system. Therefore, we use
the anomaly score to aggregate all broken relationships in the
system. The anomaly score at is computed as the total number
of broken relationships normalized by the total number of valid
models (i.e., pt). Clearly, a larger score implies that more
sensors behave abnormally. In addition, the sensor pair alert
status {Wt, t ∈ T} captures any link between two sensors
with a broken relationship. This information (at and Wt) can
be used for interpreting anomalies.
III. CASE STUDY I: PHYSICAL PLANT DATASET
The dataset of the ﬁrst case study is a proprietary one
collected from a physical plant 3. We demonstrate results for
a publicly available dataset in Section IV.
A. Dataset and Experiment Setup
The dataset is collected from a physical plant in November
2017 (30 days). During the entire month, there are only two
days that are labeled post hoc as anomalous by the data owner:
November 21 and 28. The limited number of anomalies in
the dataset mandates the use of an unsupervised technique
to learn the system behavior on normal days and use the
learned knowledge to detect outliers as anomalies. There are
128 sensors recording system status in categorical format.
Figure 2 shows the discrete event sequences recorded by two
representative sensors in the system on one normal day and on
2Models with different BLEU scores show different detecting ability. In the
evaluation sections (Section III-C and Section IV-D2), we ﬁnd that models
with BLEU scores in the [80, 90) range are best for anomaly detection.
3Unfortunately, we are not allowed to release the physical plant data log
due to non-disclosure agreements.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:40 UTC from IEEE Xplore.  Restrictions apply. 
555
one abnormal day. Both sensors report binary states, i.e., “ON”
and “OFF”. Sensor #4 (see Figure 2 (a)) exhibits periodical
state changes while Sensor #91 (see Figure 2 (b)) mostly stays
in the ”OFF” state and occasionally switches to ”ON”. Despite
the different behaviors of the two sensors, it is challenging
to visually distinguish status changes between normal and
abnormal days. The purpose of the proposed methodology is
to detect subtle state changes (if any) across pairs of sensors
that are indicators of abnormal system operation.
(cid:4)(cid:11)(cid:12)(cid:9)(cid:6)(cid:8)
(cid:5)(cid:4)
(cid:5)(cid:2)(cid:2)
(cid:4)(cid:11)(cid:12)(cid:9)(cid:6)(cid:8)
(cid:5)(cid:4)
(cid:5)(cid:2)(cid:2)
(cid:1)(cid:7)(cid:10)(cid:11)(cid:12)(cid:9)(cid:6)(cid:8)
(cid:3)(cid:11)(cid:13)(cid:12)
(a) Sensor #4
(cid:1)(cid:7)(cid:10)(cid:11)(cid:12)(cid:9)(cid:6)(cid:8)
(cid:3)(cid:11)(cid:13)(cid:12)
(b) Sensor #91
Fig. 2: Discrete event sequences collected by two representa-
tive sensors on one normal day (marked with blue) and one
abnormal day (marked with red). System states are recorded
every minute.
On average, sensors report 2.07 distinct discrete variables
(i.e., system states). The majority (i.e., 97.6% of them) have
a cardinality of 2, the one with the highest cardinality has 7
distinct discrete variables, see the CDF of sensor cardinalities
in Figure 3 (a). With a log granularity of one minute, each
sensor contains 30 × 24 × 60 = 43, 200 samples, resulting in
a total of 5.5 million samples.
(a) CDF of sensor cardinality (b) CDF of vocabulary size
Fig. 3: (a) CDF of sensor event cardinality and (b) CDF of
sensor vocabulary size.
1) Converting Discrete Event Sequences to Languages:
Following the steps outlined in Section II-A2, we assign
distinct characters (i.e., a, b, c, ...) to each categorical value,
combine successive characters into words, and combine suc-
cessive words into sentences. The sliding window size for
words and sentences controls the overlap between neighboring
words and sentences and regulates the total corpus size.
Generating words. We assume that 1 word consists of
10 characters. Consequently, each word contains the sensor
status in the current minute (represented by the last character)
and the history of the previous 9 minutes (as captured by the
9 preceding characters). We choose the size of the sliding
window to be 1 character. With this, adjacent words have an
overlap of 9 characters, adding one new character to represent