title:Software-Defined Latency Monitoring in Data Center Networks
author:Curtis Yu and
Cristian Lumezanu and
Abhishek B. Sharma and
Qiang Xu and
Guofei Jiang and
Harsha V. Madhyastha
Software-Deﬁned Latency Monitoring
in Data Center Networks
Curtis Yu1(B), Cristian Lumezanu2, Abhishek Sharma2,
Qiang Xu2, Guofei Jiang2, and Harsha V. Madhyastha3
1 University of California, Riverside, USA
PI:EMAIL
2 NEC Labs America, Princeton, NJ, USA
3 University of Michigan, Ann Arbor, USA
Abstract. Data center network operators have to continually monitor
path latency to quickly detect and re-route traﬃc away from high-delay
path segments. Existing latency monitoring techniques in data centers
rely on either (1) actively sending probes from end-hosts, which is
restricted in some cases and can only measure end-to-end latencies, or
(2) passively capturing and aggregating traﬃc on network devices, which
requires hardware modiﬁcations.
In this work, we explore another opportunity for network path latency
monitoring, enabled by software-deﬁned networking. We propose SLAM,
a latency monitoring framework that dynamically sends speciﬁc probe
packets to trigger control messages from the ﬁrst and last switches of a
path to a centralized controller. SLAM then estimates the latency dis-
tribution along a path based on the arrival timestamps of the control
messages at the controller. Our experiments show that the latency dis-
tributions estimated by SLAM are suﬃciently accurate to enable the
detection of latency spikes and the selection of low-latency paths in a
data center.
1 Introduction
Many data center applications such as search, e-commerce, and banking are
latency-sensitive [3,7]. These applications often have several distributed com-
ponents (e.g., front-end, application server, storage) that need to communicate
across low-latency network paths to reduce application response times. To eﬀec-
tively manage data center networks and provide fast paths, operators must con-
tinually monitor the latency on all paths that the traﬃc of an application could
traverse and quickly route packets away from high-delay segments [1,5].
Operators can monitor path latency from the edge by sending probes (e.g.,
ICMP requests) between servers and measuring response times. However, three
factors complicate this approach. First, some data centers (e.g., collocation cen-
ters [14]) restrict access to customer servers. Second, end-to-end probes cannot
monitor the latency on path segments between arbitrary network devices, which
is helpful in identifying sources of high delay. Finally, operators are reluctant
c(cid:2) Springer International Publishing Switzerland 2015
J. Mirkovic and Y. Liu (Eds.): PAM 2015, LNCS 8995, pp. 360–372, 2015.
DOI: 10.1007/978-3-319-15509-8 27
Software-Deﬁned Latency Monitoring in Data Center Networks
361
to repeatedly run expensive measurements from the edge and prefer to allocate
server resources to customer VMs [12].
The alternative is to monitor latencies from inside the network by capturing
information about paths directly from network devices. Trajectory sampling [6]
and l2ping are examples of this approach. Such solutions incur the overhead of
performing real-time local coordination and aggregating measurements captured
at many devices. Recent work proposes to instrument switches with a hash-
based primitive that records packet timestamps and measures network latency
with microsecond-level accuracy [10,11]. However, these methods need hardware
modiﬁcations that may not be available in regular switches anytime soon.
In this paper, we explore another opportunity to monitor path latency in
data center networks, enabled by software-deﬁned networks (SDNs). We develop
SLAM, a framework for Software-deﬁned LAtency Monitoring between any two
network switches, that does not require specialized hardware or access to end-
hosts. SLAM uses the SDN control plane to manage and customize probe packets
and trigger notiﬁcations upon their arrival at switches. It measures latency based
on the notiﬁcations’ arrival times at the control plane.
SLAM is deployed on the network controller and computes latency estimates
on a path in three steps. (setup) It installs speciﬁc monitoring rules on all
switches on the path; these rules instruct every switch to forward the matched
packets to the next switch on the path; the ﬁrst and last switches also generate
notiﬁcations (e.g., PacketIn) to the controller. (probe) SLAM sends probes that
are constructed to match only the monitoring rules and that traverse only the
monitored path. (estimate) It estimates the path’s latency based on the times
at which the notiﬁcation messages (triggered by the same probe) from the ﬁrst
and last switches of the path are received at the controller.
SLAM oﬀers several advantages over existing latency monitoring techniques.
First, by exploiting control packets inherent to SDN, SLAM requires neither
switch hardware modiﬁcations nor access to endhosts. Second, SLAM enables the
measurement of latency between arbitrary OpenFlow-enabled switches. Finally,
by computing latency estimates at the controller, SLAM leverages the visibil-
ity oﬀered by SDNs without needing complex scheduling of measurements on
switches or end-hosts. Moreover, SLAM’s concentration of monitoring logic at
the controller is well-suited to the centralized computation of low latency routes
that is typical to SDNs. Compared to OpenNetMon [15], a similar approach,
SLAM detects and adjusts for real-world deployment issues.
We address three key issues in our design of SLAM. First, latencies on data
center network paths are small—on the order of milli- or even micro-seconds—
and vary continually, due predominantly to changing queue sizes. As a result,
any single latency estimate may become invalid between when it is measured
by SLAM and when it is used to make rerouting decisions. Therefore, instead
of a single latency estimate for a path, we design SLAM to infer the latency
distribution over an interval. A latency distribution that shows high latencies
for a sustained period of time can be more instrumental in inferring high-delay
segments in the network.
362
C. Yu et al.
Second, since SLAM’s latency estimation is based on the timings of
PacketIn’s received at the controller, the accuracy of latency estimates depends
on both end switches on the path taking the same amount of time to process
notiﬁcation messages and send them to the controller. However, in reality, the
delay incurred in a switch’s processing of the action ﬁeld of a matched rule and its
subsequent generation of a notiﬁcation (i.e., PacketIn) depends on the utilization
of the switch CPU, which varies continually. Moreover, switches are generally not
equidistant from the controller. To account for these factors, for every switch,
SLAM continually monitors the switch’s internal control path latency and its
latency to the controller (via EchoRequest messages) and adjusts its estimation
of the latency distribution.
Lastly, despite SLAM’s beneﬁts, its probing overhead is the same as that
associated with probes issued from end-hosts. To alleviate this cost, we also
explore the feasibility of SLAM in a reactive OpenFlow deployment, where new
ﬂows always trigger PacketIn messages from every switch. The key idea is for
SLAM to use the existing OpenFlow control traﬃc without requiring monitoring
probes to trigger additional PacketIn messages. We use a real enterprise network
trace to show that SLAM would be able to capture latency samples from most
switch-to-switch links every two seconds by relying solely on PacketIn’s triggered
by normal data traﬃc.
We deploy and evaluate a preliminary version of SLAM on an OpenFlow-
based SDN testbed and ﬁnd that it can accurately detect latency inﬂations of
tens of milliseconds. SLAM works even in the presence of increase control traﬃc,
showing a median latency variation of a few milliseconds when the switch has
to process up to 150 control messages per second. Although not suitable to
detect very ﬁne variations in latency, SLAM is quick and accurate in identifying
high-delay paths from a centralized location and with little overhead.
2 Background
We ﬁrst describe the operation of a typical OpenFlow network and discuss the
factors that contribute to the latency experienced by a packet that traverses it.
2.1 OpenFlow
We consider a network of OpenFlow-enabled switches, connected with a logically
centralized controller using a secure, lossless TCP connection. The controller
enforces network policies by translating them into low-level conﬁgurations and
inserting them into the switch ﬂow tables using the OpenFlow protocol.
The network conﬁguration consists of the forwarding rules installed on
switches. Every rule consists of a bit string (with 0, 1, and ∗ as characters)
that speciﬁes which packets match the rule, one or more actions to be per-
formed by the switch on matched packets, and a set of counters which collect
statistics about matched traﬃc. Possible actions include “forward to physical
port”, “forward to controller”, “drop”, etc.
Software-Deﬁned Latency Monitoring in Data Center Networks
363
The controller installs rules either proactively, i.e., at the request of the appli-
cation or the operator, or reactively, i.e., triggered by a PacketIn message from
a switch as follows. When the ﬁrst packet of a new ﬂow arrives, the switch looks
for a matching rule in the ﬂow table and performs the associated action. If there
is no matching entry, the switch buﬀers the packet and notiﬁes the controller by
sending a PacketIn control message containing the packet header. The controller
responds with a FlowMod message that installs a new rule matching the ﬂow
into the switch’s ﬂow table. The controller may also forward the packet without
installing a rule using a PacketOut message.
2.2 Data Center Path Latency
A packet traversing a network path experiences propagation delay and switching
delay. Propagation delay is the time the packet spends on the medium between
switches and depends on the physical properties of the medium. The propagation
speed is considered to be about two thirds of the speed of light in vacuum [16].
The switching delay is the time the packet spends within a switch and depends
on the various functions applied to the packet. In general, the switching delay
in an OpenFlow switch has three components: lookup, forwarding, and control.
We describe them below and use Fig. 1 to illustrate.
Fig. 1. Latency computation using control message timestamps. Consider a packet
traversing a path comprising switches S1, S2, and S3. The packet arrives at these
switches at t1, t4, and t6 and leaves at t2, t5, and t7. The true latency between S1 and
S3 is t7 − t2. The matching rule at switches S1 and S3 has the additional action “send
to controller” to generate PacketIn’s (the red dotted lines). t3 and t8 are the times
when the PacketIn’s leave S1 and S3, and they arrive at the controller at t(cid:2)
8. d1
and d3 are the propagation delays from switches S1 and S3 to the controller. We use
3 − d1) to estimate the latency between S1 and S3, after accounting for
8 − d3) − (t(cid:2)
(t(cid:2)
the processing times in each switch (see Sect. 3) (Color ﬁgure online).
3 and t(cid:2)
364
C. Yu et al.
Lookup. When a switch receives a packet on one of its input ports, the switch
looks for a match in its forwarding table to determine where to forward the
packet. This function is usually performed by a dedicated ASIC on parts of the
packet header.
Forwarding. A matched packet is transferred through the internal switching
system from the input port to an output port. If the output link is transmit-
ting another packet, the new packet is placed in the output queue. The time
a packet spends in the queue depends on what other traﬃc traverses the same
output port and the priority of that traﬃc. In general, forwarding delays dom-
inate lookup delays [16]. The intervals [t1, t2], [t4, t5], and [t6, t7] represent the
combined lookup and forwarding delays at switches S1, S2, and S3 in Fig. 1.
Control. If there is no match for the packet in the ﬂow table or if the match
action is “send to controller”, the switch CPU encapsulates part or all of the
packet in a PacketIn control message and sends it to the controller. The control
delay is the time it takes the PacketIn to reach the controller ([t2, t(cid:3)
3] and [t7, t(cid:3)
8]
in Fig. 1).
3 Latency Monitoring with SLAM
SLAM computes the latency distribution for any switch-to-switch path by gath-
ering latency samples over a speciﬁed period of time. We deﬁne the latency
between two switches as the time it takes a packet to travel from the output
interface of the ﬁrst switch to the output interface of the second switch, e.g., the
latency of the path (S1, S3) in Fig. 1 is t7 − t2. Our deﬁnition of latency does not
include the internal processing of the ﬁrst switch, t2 − t1, on the path due to the
way we use OpenFlow control messages as measurement checkpoints. However,
since we continually monitor internal processing delays (see later in the section),
we can account for any eﬀects they may have on the overall latency estimation.
Directly measuring the time at which a switch transmits a packet is either
expensive [6] or requires modiﬁcations to the switch hardware [10]. Instead,
we propose that switches send a PacketIn message to the controller whenever a
speciﬁc type of data packet traverses them. We estimate the latency between two
switches as the diﬀerence between the arrival times at the controller of PacketIn’s
corresponding to the same data packet, after accounting for the diﬀerences in
internal processing of the two switches and propagation delays to the controller.
In Fig. 1, the estimated latency is (t(cid:3)
− d3) − (t(cid:3)
We incorporate these ideas into the design of SLAM, an OpenFlow controller
module that estimates the latency distribution between any two OpenFlow
switches in a network. Next, we discuss how to generate and send probes that
trigger PacketIn messages and how to calibrate our latency distribution to the
diﬀerences in control processing latency between switches. We then describe the
design of SLAM.
8
− d1).
3
Software-Deﬁned Latency Monitoring in Data Center Networks
365
3.1 Latency Monitoring
To estimate latency on a path, SLAM generates probe packets that traverse the
path and trigger PacketIn messages at the ﬁrst and last switches on the path. To
guide a probe along an arbitrary path, we pre-install forwarding rules at switches
along the path, whose action ﬁeld instructs the switch to send matched packets
to the next-hop switch. In addition, to generate PacketIn’s, the rules at the ﬁrst
and last switch on the path contain “send to controller” as part of their action
set. SLAM sends monitoring probes using PacketOut messages to the ﬁrst switch
on the path. Our method is similar to the one proposed by OpenNetMon [15],
but we explore the implications of using such a system, including its issues, and
quantify this eﬀect on the ﬁnal result.
An important requirement is that the monitoring rules we install to guide
the probes do not interfere with normal traﬃc, i.e., only our probes match
against them. For this, we make the rules very speciﬁc by not using wildcards
and specifying exact values for as many match ﬁelds as possible (e.g., VLAN
tag, TCP or UDP port numbers, etc.). To save space on switches, we also set
the rules to expire once the monitoring is ﬁnished by setting their hard timeout.
3.2 Control Processing
3