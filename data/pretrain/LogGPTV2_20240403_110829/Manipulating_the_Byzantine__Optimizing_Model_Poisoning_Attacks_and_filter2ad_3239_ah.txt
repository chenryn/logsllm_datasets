Calo. Analyzing federated learning through an adversarial lens.
In
International Conference on Machine Learning, pages 634–643, 2019.
[7] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support
vector machines. In Proceedings of 29th International Conference on
Machine Learning, 2012.
[8] Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine
learning with adversaries: Byzantine tolerant gradient descent.
In
Advances in Neural Information Processing Systems, pages 119–129,
2017.
[9] Sebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan
McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark
for federated settings. arXiv preprint arXiv:1812.01097, 2018.
[10] Hongyan Chang, Virat Shejwalkar, Reza Shokri, and Amir Houmansadr.
Cronus: Robust and heterogeneous collaborative learning with black-
box knowledge transfer. arXiv preprint arXiv:1912.11279, 2019.
[11] Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from
In Proceedings of the 49th Annual ACM SIGACT
untrusted data.
Symposium on Theory of Computing, pages 47–60. ACM, 2017.
[12] Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papail-
iopoulos. Draco: Byzantine-resilient distributed training via redundant
In International Conference on Machine Learning, pages
gradients.
903–912, 2018.
[14]
[13] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik.
Emnist: Extending mnist to handwritten letters. In 2017 International
Joint Conference on Neural Networks (IJCNN), pages 2921–2926.
IEEE, 2017.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin,
Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, and
Andrew Y Ng. Large scale distributed deep networks. Advances in
neural information processing systems, 2012.
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob
Steinhardt, and Alistair Stewart.
Sever: A robust meta-algorithm
In International Conference on Machine
for stochastic optimization.
Learning, pages 1596–1606, 2019.
Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur
Moitra, and Alistair Stewart. Being robust (in high dimensions) can
be practical. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, 2017.
[15]
[16]
[17] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong.
Local model poisoning attacks to byzantine-robust federated learning.
In 29th USENIX Security Symposium (USENIX Security 20), Boston,
MA, aug 2020. USENIX Association.
[18] Avishek Ghosh, Justin Hong, Dong Yin, and Kannan Ramchandran.
arXiv
Robust federated learning in a heterogeneous environment.
preprint arXiv:1906.06629, 2019.
[19] Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. Byzantine-robust
arXiv preprint
learning on heterogeneous datasets via resampling.
arXiv:2006.09365, 2020.
[20] Matthew Jagielski, Aline Oprea, Battista Biggio, Chang Liu, Cristina
Nita-Rotaru, and Bo Li. Manipulating machine learning: Poisoning
attacks and countermeasures against regression learning. 39th IEEE
Symposium on Security and Privacy, 2018.
[21] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet,
Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles,
Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik,
Federated learning:
Ananda Theertha Suresh, and Dave Bacon.
arXiv preprint
Strategies for improving communication efﬁciency.
arXiv:1610.05492, 2016.
[22]
16
[23]
Jakub Koneˇcn´y, H. Brendan McMahan, Felix X. Yu, Peter Richt´arik,
Federated learning:
Ananda Theertha Suresh, and Dave Bacon.
arXiv preprint
Strategies for improving communication efﬁciency.
arXiv:1610.05492, 2017.
[24] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of
features from tiny images. 2009.
[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet
classiﬁcation with deep convolutional neural networks. In Advances in
neural information processing systems, 2012.
[26] Kevin Lai, Anup Rao, and Santosh Vempala. Agnostic estimation
In 2016 IEEE 57th Annual Symposium on
of mean and covariance.
Foundations of Computer Science (FOCS), 2016.
[27] Yann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner, et al.
Gradient-based learning applied to document recognition. Proceedings
of the IEEE, 86(11):2278–2324, 1998.
Jerry Zheng Li. Principled approaches to robust machine learning and
beyond. PhD thesis, Massachusetts Institute of Technology, 2018.
[28]
[29] Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed.
In Proceedings of the 36th
Universal multi-party poisoning attacks.
International Conference on Machine Learning, volume 97, pages
4274–4283, 2019.
[30] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas. Communication-efﬁcient
learning of
deep networks from decentralized data. Proceedings of the 20 th
International Conference on Artiﬁcial Intelligence and Statistics, 2017.
[31] El Mahdi El Mhamdi, Rachid Guerraoui, and S´ebastien Rouault. The
hidden vulnerability of distributed learning in byzantium. In Interna-
tional Conference on Machine Learning, pages 3518–3527, 2018.
[32] Luis Mu˜noz-Gonz´alez, Battista Biggio, Ambra Demontis, Andrea Pau-
dice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards
poisoning of deep learning algorithms with back-gradient optimization.
In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence
and Security, pages 27–38. ACM, 2017.
[33] Luis Mu˜noz-Gonz´alez, Kenneth T Co, and Emil C Lupu. Byzantine-
robust federated machine learning through adaptive model averaging.
arXiv preprint arXiv:1909.05125, 2019.
[34] Karen Simonyan and Andrew Zisserman.
tional networks for large-scale image recognition.
arXiv:1409.1556, 2014.
Very deep convolu-
arXiv preprint
[35] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan
McMahan. Can you really backdoor federated learning? arXiv preprint
arXiv:1911.07963, 2019.
[36] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures
In Advances in Neural Information Processing
in backdoor attacks.
Systems, pages 8000–8010, 2018.
[37] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized
byzantine-tolerant sgd. arXiv preprint arXiv:1802.10116, 2018.
[38] Cong Xie, Sanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking
byzantine-tolerant sgd by inner product manipulation. arXiv preprint
arXiv:1903.03936, 2019.
[39] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.
Byzantine-robust distributed learning: Towards optimal statistical rates.
In Proceedings of
the 35th International Conference on Machine
Learning, 2018.
A. Additional experimental results
APPENDIX
Figure 6a shows the results for the attack impacts of LIE,
Fang, and our model poisoning attacks on FEMNIST and
MNIST datasets, when the percentage of malicious clients
is varied from 5% to 24%. Detailed comparison is in Sec-
tion VI-D2.
Table VII shows the results for the empirical robustness of
three state-of-the-art robust AGRs when coupled with recently
proposed resampling mechanism [19]. We note that similar to
our DnC, resampling reduces the robustness existing AGRs.
Figure 7 shows the impact of the three perturbation vectors
introduced in Section IV-A on emulated (light bars) and actual
FL settings. Detailed discussion in Section VI-C2.
B. Proof of Lemma 1
The outliers detection guarantees of our robust AGR DnC
depend on Lemma 1, which is a standard result in spectral-
methods based outliers detection [36], [28], [15], [11]. For
completeness, we provide its proof motivated from [28], [36].
As in Lemma 1, consider two distributions B, M with
means µB, µM and covariances ΣB, ΣM (cid:22) σ2I. Let U =
(1 − )B + M be a mixture of samples from B and M. The
following holds for any unit vector u due to the Chebyshev’s
inequality:
Pr
X∼B
Pr
X∼M
[|(cid:104)X − µB, u(cid:105)| > t] ≤ σ2
t2
[|(cid:104)X − µM , u(cid:105)| > t] ≤ σ2
t2
(9)
(10)
Let ∆ = µB − µM and v be the top right singular
eigenvector of U. Ideally, the unit vector u should be a scaled
version of ∆, which will maximally separates the samples from
B and M. But as argued in [36], one can show any u with
sufﬁcient correlation with ∆ sufﬁces, which gives us Lemma
Lemma 2. Let α > 0 such that |(cid:104)u, ∆(cid:105)| > ασ√
 . Then there
exists t > 0 such that
[|(cid:104)X − µB, u(cid:105)| > t]  t]  t]
(cid:104)|(cid:104)X − µB, v(cid:105)| >
Pr
X∼B
≤ Pr
X∼B
(cid:105) ≤ 
α2
σ√

and by (9), we have
[|(cid:104)X − µU , v(cid:105)|  (cid:104)∆, v(cid:105) − ασ√
(cid:104)|(cid:104)X − µM , v(cid:105)| >
(cid:105)
(α − 1)σ√

(cid:105)

Pr
X∼M
≤ Pr
X∼M
(a)≤ Pr
X∼M

(b)≤
(α − 1)2
where (a) follows from the assumption and (b) from (10).
Lemma 3. Under the assumptions of Lemma 1, we have
(cid:104)∆, v(cid:105)2 ≥ 2σ2
 .
17
(a) FEMNIST with convolution neural network
Figure 6: Effect of increasing percentage of malicious clients on the impacts of model poisoning attacks on FL.
(b) MNIST with CNN architecture
Table VII: Resampling [19] signiﬁcantly reduces the robustness of existing defenses against our attacks (20% malicious clients) for all the
adversaries from Table I. Because, resampling increases the number of malicious updates processed, and therefore, their poisoning impacts.
AGR
Krum
Krum + resampling
Bulyan
Bulyan + resampling
Trmean
Trmean + resampling
No
attack
(Aθ)
69.3
86.1
86.7
Updates of benign devices are known
AGR-tailored
AGR-agnostic
(agr-updates)
(updates-only)
Updates of benign devices are unknown
AGR-tailored
(agr-only)
AGR-agnostic
(agnostic)
Min-Max Min-Sum
Min-Max Min-Sum
30.0
47.8
41.0
76.8
24.3
70.3
0.1
64.0
20.1
81.7
29.7
81.2
9.8
43.6
40.0
53.9
26.8
46.0
2.9
45.2
40.5
49.6
20.1
46.4
1.1
63.6
18.7
10.0
24.7
80.6
8.0
58.0
30.4
8.1
25.2
68.5
If ΣU is the covariance of U, we have
ΣU = (1 − )ΣB + ΣM + (1 − )∆∆T
=⇒ ΣU (cid:23) (1 − )∆∆T
=⇒ (cid:107)ΣU(cid:107)2 ≥ (1 − )(cid:107)∆(cid:107)2
2
Next, we have
(1 − )(cid:107)∆(cid:107)2
2 ≤ vT ΣU v
= (1 − )vT ΣBv + vT ΣM v + (1 − )(cid:104)∆, v(cid:105)2
≤ σ2 + (1 − )(cid:104)∆, v(cid:105)2
6(cid:107)∆(cid:107)2
2, which gives us
1 −
1
6(1 − )
(a)≥ 2
3
(cid:107)∆(cid:107)2
2 ≥ 2σ2

Recall the assumption σ2 ≤ 
(cid:104)∆, v(cid:105)2 ≥(cid:16)
(cid:17)(cid:107)∆(cid:107)2
2
where (a) follows from the assumption that  < 1/2; taking
square roots of both the sides gives the ﬁnal result.
Finally combining the results of Lemmas 2 and 3 gives us
the result of Lemma 1.
Figure 7: Selecting an effective perturbation for CIFAR with VGG11
and FEMNIST with CNN. Please check Section VI-C2 and Figure 2
for details.
Proof: We have
(X − µU )(X − µU )T(cid:105)
(cid:104)
(cid:104)
(X − µU )(X − µU )T(cid:105)
EX∼B
EX∼M
= ΣB + 2∆∆T
= ΣB + (1 − )2∆∆T (11)
18
510152025Percent of attackers1520253035Attack impactKrumLIEFangOur AGR-tailoredOur Min-MaxOur Min-Sum510152025Percent of attackers20406080Multi-krum510152025Percent of attackers152025303540Bulyan510152025Percent of attackers5101520253035Trimmed-mean510152025Percent of attackers10152025303540Median510152025Percent of attackers051015202530Attack impactKrumLIEFangOur-AGRMin-MaxMin-Sum510152025Percent of attackers05101520Multi-krum510152025Percent of attackers05101520Bulyan510152025Percent of attackers024681012Trimmed-mean510152025Percent of attackers0246Median02040Attack impactCIFAR10 + VGG11KrumMulti-krumBulyanTrimmed-meanMedian0204060Attack impactFEMNISTpuvpstdpsgn