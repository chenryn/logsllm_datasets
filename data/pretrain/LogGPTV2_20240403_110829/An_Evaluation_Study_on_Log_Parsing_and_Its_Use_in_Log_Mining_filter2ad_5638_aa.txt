title:An Evaluation Study on Log Parsing and Its Use in Log Mining
author:Pinjia He and
Jieming Zhu and
Shilin He and
Jian Li and
Michael R. Lyu
An Evaluation Study on Log Parsing and Its Use in
Log Mining
Pinjia He∗†, Jieming Zhu∗†, Shilin He∗‡, Jian Li∗† and Michael R. Lyu∗†
∗Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong
‡School of Computer Science and Engineering, South China University of Technology, Guangzhou, China
†Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China
{pjhe, jmzhu, jianli, lyu}@cse.cuhk.edu.hk, PI:EMAIL
Abstract—Logs, which record runtime information of modern
systems, are widely utilized by developers (and operators) in
system development and maintenance. Due to the ever-increasing
size of logs, data mining models are often adopted to help
developers extract system behavior information. However, before
feeding logs into data mining models, logs need to be parsed
by a log parser because of their unstructured format. Although
log parsing has been widely studied in recent years, users are
still unaware of the advantages of different log parsers nor the
impact of them on subsequent log mining tasks. Thus they often
re-implement or even re-design a new log parser, which would
be time-consuming yet redundant. To address this issue, in this
paper, we study four log parsers and package them into a toolkit
to allow their reuse. In addition, we obtain six insightful ﬁndings
by evaluating the performance of the log parsers on ﬁve datasets
with over ten million raw log messages, while their effectiveness
on a real-world log mining task has been thoroughly examined.
I. INTRODUCTION
Logs are widely used to record runtime information of
software systems, such as the timestamp of an event, the
unique ID of a user request, and the state of a task execution.
The rich information of logs enables system developers (and
operators) to monitor the runtime behaviors of their systems
and further track down system problems in production settings.
With the ever-increasing scale and complexity of modern
systems, the volume of logs is rapidly growing, for example,
at a rate of about 50 gigabytes (around 120∼200 million lines)
per hour [1]. Therefore, the traditional way of log analysis
that largely relies on manual inspection has become a labor-
intensive and error-prone task. To address this challenge, many
efforts have recently been made to automate log analysis by the
use of data mining techniques. Typical examples of log mining
include anomaly detection [2], [3], [4], program veriﬁcation
[5], [6], problem diagnosis [7], [8], and security assurance
[9], [10]. However, raw log messages are usually unstructured,
because developers are allowed to record a log message using
free text for convenience and ﬂexibility. To enable automated
mining of unstructured logs,
the ﬁrst step is to perform
log parsing, whereby unstructured raw log messages can be
transformed into a sequence of structured events.
Typically, a log message, as illustrated in the following
example, records a speciﬁc system event with a set of ﬁelds:
timestamp (recording the occurring time of the event), ver-
bosity level (indicating the severity level of the event, e.g.,
INFO), and raw message content (recording what has hap-
pened during system operation).
2008-11-09 20:35:32,146 INFO dfs.DataNode$DataXceive
r: Receiving block blk_-1608999687919862906 src: /10
.251.31.5:42506 dest: /10.251.31.5:50010
As observed in the example, the raw message content can
be divided into two parts: constant part and variable part.
The constant part constitutes the ﬁxed plain text and remains
the same for every event occurrence, which can reveal the
event type of the log message. The variable part carries the
runtime information of interest, such as the values of states and
parameters (e.g., the IP address and port: 10.251.31.5:50010),
which may vary among different event occurrences. The goal
of log parsing is to extract the event by automatically separat-
ing the constant part and variable part of a raw log message,
and further transform each log message into a speciﬁc event
(usually denoted by its constant part). In this example, the
event can be denoted as “Receiving block * src: * dest:
*”, where the variable part is identiﬁed and masked using
asterisks. We will use “event” and “template” interchangeably
in this paper.
Log parsing is essential for log mining. Traditionally, log
parsing relies heavily on regular expressions to extract the
speciﬁc log event (e.g., SEC [11]). However, modern software
systems, with increasing size and complexity, tend to produce
a huge volume of logs with diverse log events. It requires non-
trivial efforts for manual creation and maintenance of regular
expression rules. Especially, when a system constantly evolves,
the rules of log parsing will most likely become outdated very
often. For example, Google’s systems, as studied in [12], have
been introduced with up to thousands of new log printing
statements every month. As a result, there is a high demand
for automated log parsing methods, capable of evolving with
the system.
To achieve this goal, recent studies have proposed a number
of data-driven approaches for automated log parsing (e.g.,
SLCT [13], IPLoM [14], LKE [3], LogSig [15]), in which
historical log messages are leveraged to train statistical models
for event extraction. Despite the importance of log parsing, we
found that, to date, there is a lack of systematic evaluations on
the effectiveness and efﬁciency of the automated log parsing
methods available. Meanwhile, except SLCT [13] that was
released more than 10 years ago, there are no other ready-to-
use tool implementations of log parsers. Even with commercial
log management solutions, such as Splunk [16] and Logstash
[17], users need to provide complex conﬁgurations with cus-
tomized rules to parse their logs. In this context, engineers
and researchers have to implement their own log parsers when
performing log mining tasks (e.g., [5], [8], [18]), which would
be a time-consuming yet redundant effort. Besides, they are
likely unaware of the effectiveness of their implementations
compared to other competitive methods, nor do they notice
the impact of log parsing on subsequent log mining tasks.
To ﬁll this signiﬁcant gap, in this paper, we perform a
systematic evaluation study on the state-of-the-art log parsing
methods and their employment in log mining. In particular, we
intend to investigate the following three research questions:
RQ1: What
is the accuracy of
the state-of-the-art
log
parsing methods?
RQ2: How do these log parsing methods scale with the
volume of logs?
RQ3: How do different log parsers affect the results of log
mining?
Towards this end, we have implemented four widely-employed
log parsers: SLCT [13], IPLoM [14], LKE [3], LogSig [15].
They are currently available on our Github1 as an open-
source toolkit, which can be easily re-used by practitioners
and researchers for future study. For evaluation, we have also
collected ﬁve large log datasets (with a total of over 10 million
raw log messages) produced by production software systems.
The evaluation is performed in terms of both accuracy and
efﬁciency in log parsing. Furthermore, we evaluate the impact
of different log parsers on subsequent log mining tasks, with
a case study on system anomaly detection (proposed in [2]).
Through this comprehensive evaluation, we have obtained
a number of insightful ﬁndings: Current log parsing methods
could obtain high overall accuracy (Finding 1), especially
when log messages are preprocessed with some domain knowl-
edge based rules (Finding 2). Clustering-based log parsing
methods could not scale well with the volume of logs (Finding
3), and the tuning of parameters (e.g., number of clusters)
is time-consuming (Finding 4). Log mining is effective only
when the parsing accuracy is high enough (Finding 5). Because
log mining can be sensitive to some critical events. 4% parsing
errors on critical events can cause an order of magnitude
performance degradation in log mining (Finding 6). These
ﬁndings as well as our toolkit portray a picture about the
current situation of log parsing methods and their effectiveness
on log mining, which we believe could provide valuable
guidance for future research in this ﬁeld.
The remainder of this paper is organized as follows. Section
II reviews the existing log parsing methods, and Section III
reviews recent studies on log mining with a detailed example
of anomaly detection. The evaluation results and ﬁndings are
1https://github.com/cuhk-cse/logparser
Fig. 1: Overview of Log Parsing
reported in Section IV. We discuss some limitations in Section
V. We then introduce the related work in Section VI, and
ﬁnally conclude this paper in Section VII.
II. LOG PARSING
This section ﬁrst provides an overview of log parsing
and then describes four existing log parsing methods. These
methods are widely employed and thus become the main
subjects of our study.
A. Overview of Log Parsing
Fig. 1 illustrates an overview of log parsing. The raw log
messages, as shown in the ﬁgure, contain ten log messages
extracted from HDFS log data on Amazon EC2 platform [2].
The log messages are unstructured data, with timestamps and
raw message contents (some ﬁelds are omitted for simplicity
of presentation). In real-world cases, a log ﬁle may contain
millions of such log messages. The goal of log parsing is
to distinguish between constant part (ﬁxed plain text) and
variable part (e.g., blk ID in the ﬁgure) from the log message
contents. Then, all the constant message templates can be
clustered into a list of log events, and structured logs can be
generated with each log message corresponding to a speciﬁc
event. For instance,
the log message 2 is transformed to
“Event2” with a log template “Receiving block * src: * dest:
*”. The output of a log parser involves two ﬁles with log events
and structured logs. Log events record the extracted templates
of log messages, while structured logs contain a sequence of
events with their occurring times. Finally, the structured logs
after parsing can be easily processed by log mining methods,
such as anomaly detection [2] and deployment veriﬁcation [6].
2008-11-11 03:40:58 BLOCK* NameSystem.allocateBlock: /user/root/randtxt4/_temporary/_task_200811101024_0010_m_000011_0/part-00011.blk_9047918154093996622008-11-11 03:40:59 Receiving block blk_904791815409399662 src: /10.251.43.210:55700 dest: /10.251.43.210:500102008-11-11 03:41:01 Receiving block blk_904791815409399662 src: /10.250.18.114:52231 dest: /10.250.18.114:500102008-11-11 03:41:48 PacketResponder 0 for block blk_904791815409399662 terminating2008-11-11 03:41:48 Received block blk_904791815409399662 of size 67108864 from /10.250.18.1142008-11-11 03:41:48 PacketResponder 1 for block blk_904791815409399662 terminating2008-11-11 03:41:48 Received block blk_904791815409399662 of size 67108864 from /10.251.43.2102008-11-11 03:41:48 BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.210:50010 is added to blk_904791815409399662 size 671088642008-11-11 03:41:48 BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.18.114:50010 is added to blk_904791815409399662 size 671088642008-11-11 08:30:54 Verification succeeded for blk_904791815409399662Raw Log Messages112  32463574895106 1    2008-11-11 03:40:58 Event1  2    2008-11-11 03:40:59 Event2 3    2008-11-11 03:41:01 Event2 4    2008-11-11 03:41:48 Event3 5    2008-11-11 03:41:48 Event4 6    2008-11-11 03:41:48 Event3 7    2008-11-11 03:41:48 Event4 8    2008-11-11 03:41:48 Event5 9    2008-11-11 03:41:48 Event510    2008-11-11 08:30:54 Event6Structured LogsEvent1    BLOCK* NameSystem.allocateBlock: *Event2    Receiving block * src: * dest: *Event3    PacketResponder * for block * terminatingEvent4    Received block * of size * from *Event5    BLOCK* NameSystem.addStoredBlock:    blockMap updated: * is added to * size * Event6    Verification succeeded for *Log Events12345678910Log ParsingB. Existing Log Parsing Methods
Log parsing has been widely studied in recent years. Among
all the approaches proposed, we choose four representative
ones, which are in widespread use for log mining tasks. With
the main focus on evaluations of these log parsing methods,
we only provide brief reviews of them; the details can be found
in the corresponding references.
1) SLCT
SLCT (Simple Logﬁle Clustering Tool) [13] is, to the best
of our knowledge, the ﬁrst work on automated log parsing. The
work also released an open-source log parsing tool, which has
been widely employed in log mining tasks, such as event log
mining [19], symptom-based problem determination [20] and
network alert classiﬁcation [21].
Inspired by association rule mining, SLCT works as a three-
step procedure with two passes over log messages: 1) Word
vocabulary construction. It makes a pass over the data and
builds a vocabulary of word frequency and position. 2) Cluster
candidates construction. It makes another pass to construct
cluster candidates using the word vocabulary. 3) Log template
generation. Clusters with enough log messages are selected
from candidates. Then, the log messages in each cluster can
be combined to generate a log template, while remaining log
messages are placed into an outlier cluster.
2) IPLoM
IPLoM (Iterative Partitioning Log Mining) [22] is a log
parsing method based on heuristics specially designed accord-
ing to the characteristics of log messages. This method has also
been used by a set of log mining studies (e.g., alert detection
[4], event log analysis [23] and event summarization [24]).
Speciﬁcally, IPLoM performs log parsing through a three-
step hierarchical partitioning process before template genera-
tion: 1) Partition by event size. Log messages are partitioned
into different clusters according to different lengths. 2) Parti-
tion by token position. For each partition, words at different
positions are counted. Then the position with the least number
of unique words is used to split the log messages. 3) Partition
by search for mapping. Further partition is performed on
clusters by searching for mapping relationships between the
set of unique tokens in two token positions selected using
a heuristic criterion. 4) Log template generation. Similar to
SLCT, the ﬁnal step is to generate log templates from every
cluster.
3) LKE
LKE (Log Key Extraction) [3] is a log parsing method
developed by Microsoft, and has been applied in a set of tasks
on unstructured log analysis [3], [25].
LKE utilizes both clustering algorithms and heuristic rules
for log parsing: 1) Log clustering. Raw log messages are ﬁrst
clustered by using hierarchical clustering algorithms with a
customized weighted edit distance metric. 2) Cluster splitting.
A splitting step based on heuristic rules is performed to further
split the clusters. 3) Log template generation. The ﬁnal step is
to generate log templates from every cluster, similar to SLCT
and IPLoM.
4) LogSig
LogSig [15] is a more recent log parsing method, which has
been validated in [26].
LogSig works in three steps: 1) Word pair generation. Each
log message is converted to a set of word pairs to encode
both the word and its position information. 2) Log Clustering.
Based on the word pairs, a potential value is calculated for
each log message to decide which cluster the log message
potentially belongs to. After a number of iterations, the log
messages can be clustered. 3) Log template generation. In
each cluster, the log messages are leveraged to generate a log
template.
C. Tool Implementation
Among these log parsing methods, we only found an open-
source implementation on SLCT in C language. To enable our
evaluations, we have implemented the other three log parsing
methods in Python and also wrapped up SLCT as a Python
package. For ease of use, we deﬁne standard input/output
formats for these log parsers. As shown in Fig. 1, the input is
a ﬁle with raw log messages, while the output contains both a
ﬁle with log events and a ﬁle with structured logs. The output
can be easily fed into subsequent log mining tasks. Currently,
all our implementations have been open source on Github,
which can be used as a toolkit for log parsing. We believe
our toolkit could beneﬁt other researchers and practitioners as
well.
It
implementation
targets at exactly reproducing the log parsing methods (as
described in original work) for our evaluation purposes. As we
will show in Section IV-C, LKE and LogSig do not scale well
on large datasets. Although we plan to improve their efﬁciency
in our future work, users may need to pay more attention when
using our current toolkit.
is also worth noting that our current
III. LOG MINING
In this section, we brieﬂy introduce three representative log
mining tasks and explain how the adopted log parsing step can
affect the performance of these tasks. Further, we describe the
details of a speciﬁc log mining task, system anomaly detection,
which will be used for our evaluations.
A. Overview of Log Mining
Anomaly detection: Logs of Hadoop File System (HDFS)
are used by Xu et al. [2] to detect anomalies in a 203-nodes
HDFS. In this case, they employ source code based log parsers
(not evaluated because it is beyond the scope of this paper) to
ﬁnd out the log events associated with each block ID, which
are further interpreted with a block ID-by-event count matrix.
This matrix is fed into a machine learning model to detect
anomalies of the system. If the log parser adopted does not
work well, some block IDs will match wrong log events, which
could ruin the generated matrix and lead to failure of the
anomaly detection approach.
Deployment veriﬁcation: Big data application is usually
developed in pseudo-cloud environment (with several PC
nodes) and ﬁnally deployed in a large-scale cloud environ-
ment. Runtime analysis and debugging of such applications in
deployment phase is a challenge tackled by Shang et al. in
[6]. To reduce the amount of log messages which needs to be