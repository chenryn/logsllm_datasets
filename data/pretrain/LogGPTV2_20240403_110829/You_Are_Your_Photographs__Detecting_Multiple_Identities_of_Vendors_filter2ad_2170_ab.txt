tive vendors on Evolution, 1200 vendors on Agora, and 800 vendors
on SilkRoad2 by the end of 2014 [48]. The corresponding numbers
in our dataset (2014-2015) are 4197, 3162, and 1332 respectively,
which are consistently higher. This is likely due to the growth of
the markets. In addition, Figure 2 shows the accumulative number
of distinct products listed on the markets over time. The curves
have smooth upward trends without obvious plateau, indicating a
good data integrity.
During our data processing, we find that
Image Metadata.
certain images contain the EXIF metadata. When a camera takes
a photo, it can add metadata to the photo including the camera
information, the timestamp and even the location where the photo
is taken. The metadata is tagged following the standard Exchange-
able Image File Format (or EXIF). Our analysis shows that darknet
markets have realized the problem: Agora and Evolution started to
remove the EXIF metadata from all the uploaded photos since June
and March of 2014. In total, there are 1,604 vendors who had at
least one photo with EXIF metadata, and 112 vendors revealed their
location information through the metadata. The EXIF metadata only
affected a small number of early vendors, and most markets today
remove the metadata by default. To this end, our system did not
consider the EXIF information (removed from our dataset).
10-410-310-210-1100100101102103104CCDF of Image CountImage CountAgoraSilkRoad2EvolutionSilkRoad2 UniqueAgora UniqueEvolution Unique 0 20000 40000 60000 80000 1000002014-022014-042014-062014-082014-102014-122015-022015-042015-06Cumulative Product CountDateSilkRoad2AgoraEvolutionThe darknet datasets in this paper
Ethics of Data Analysis.
were originally collected by previous researchers [5] who made the
data publicly available under the Creative Common CC0 license. We
follow the standard ethical practice to analyze the datasets [11, 48].
First, our analysis only covers darknet markets that have been taken
down by authorities. Second, the dataset only contains the publicly
available information on the darknet markets (product pages). The
dataset does not contain any personally identifiable information.
Third, our data analysis is completely passive without any form of
interactions with the human subjects. Finally, our research produces
useful tools to support researchers and the law enforcement to trace,
monitor, and investigate cybercrimes. The benefit of the research
significantly outweighs the potential risks.
4 IMAGE-BASED VENDOR FINGERPRINTING
Next, we describe our method to fingerprint darknet market vendors
by analyzing their posted photos. In this section, we describe our
deep-learning based method to building the fingerprints for vendors,
and perform ground-truth evaluations using empirical datasets.
4.1 Method and Designs
To fingerprint a vendor based on her photos, we need to identify
key features that can uniquely represent the vendor. Related work
has explored fingerprinting specific camera devices using low-level
features, e.g., the unique sensor noise and lens distortions caused
by manufacturing imperfection and sensor in-homogeneity [9, 10,
34, 43, 55]. However, previous works on photograph authorship
attribution suggested that the high-level features (e.g., object, scene,
background, camera angle and other latent photography styles)
significantly outperformed low-level features to identify photogra-
phers [32]. To this end, we choose high-level features for darknet
vendor identification.
To capture the unique features from a vendors’ photos, we rely
on Deep Neural Networks (DNN) which can extract features auto-
matically without manually crafting the feature list [32]. The key
challenge is that deep neural networks, in order to be accurate,
requires a massive amount of training data. However, in darknet
markets, the number of photos per vendor is limited as shown
in Figure 1. To this end, we apply transfer learning to pre-train a
deep neural network using a large existing image dataset (with
millions of images) and then fine-tune the last few layers using the
smaller darknet dataset. The intuition is that features of the deep
neural network are more generic in the early layers and are more
dataset-specific in the later layers.
The early layers can be trained using general object photos. For
our system, we use the largest annotated image dataset called Ima-
geNet [45] (14 million images) to pre-train a deep neural network.
Then we replace the final softmax layer with a new softmax layer
which handles the classes in the darknet dataset. Here, a “class”
is defined as a set of photos uploaded by the same vendor. Next,
we fine-tune the last layers or all layers with back-propagation
using the vendors’ product photos. The fine-tuning process is im-
plemented using a stochastic gradient descent optimizer with a
small initial learning rate, aiming to minimize the cross-entropy
loss function. We follow the standard procedures to fine-tune a
neural network using toolkits such as TensorFlow and Keras.
To construct the deep neural network, we select 5 popular mod-
els for generic image classification tasks. For each model, we re-
implement the data feeding module and the prediction module and
select the most popular configurations on their respective tasks.
The most popular configurations are usually those that lead to the
highest accuracy with an acceptable computational overhead. For
image pre-processing, we reshape the darknet images to the same
sizes of the images that are used in the pre-trained models. We then
use the ImageNet utility module in Keras for image preparation.
AlexNet was introduced by Krizhevsky et al. in 2012 [30]. Our
code is based on Kratzert’s implementation of AlexNet using
TensorFlow [29]. The images are reshaped to 227 × 227. The
early layers are kept fixed and only the last three layers (fc6,
fc7, fc8) of the network are fine-tuned.
Inception models are a series of DNN models introduced by
Szegedy et al [52] in 2014–2017. We choose the latest Inception-
V4. Our code is based on Yu’s implementation [61], where
all network layers are fine-tuned. The images are reshaped
to 299 × 299.
VGG models were introduced by Simonyan and Zisserman
in 2014 [47]. Here we adopted the 19-layer VGG-19 model.
The images are reshaped to 224 × 224 (same for ResNet and
DenseNet below).
ResNet was introduced by He et al. in 2015 [21]. In our analysis,
we adopted ResNet-50 model for its good balance of accuracy
and computational overhead.
DenseNet or Densely Connected Convolutional Network was
introduced by Huang et al. in 2016 [25]. We adopted DenseNet-
121 model for its good performance.
Using the deep neural network model, we train a multi-class
classifier where each class represents a vendor in the darknet mar-
ket. Given an input image, we use the classifier to calculate the
probability that the image belongs to a given vendor. Based on the
“similarity” of images, we identify pairs of accounts that are likely
to be controlled by the same vendor.
4.2 Ground-Truth Evaluation
To evaluate the feasibility of our approach, we first perform a
ground-truth evaluation. Due to the high-anonymity of the darknet
marketplaces, it is impossible for us to build the actual ground-
truth. One convincing way to build the synthetic ground-truth is
through splitting the data of certain vendors. For a given vendor,
we randomly split her photos into two even parts. We use the first
half to train the classifier and then try link the second half to the
original vendor. This evaluation is to examine the feasibility of
our approach and help to fine-tune the parameters. Later in §6
and §7, we will apply our method to identify previously unknown
multiple-identities controlled by the same vendors in the wild.
For a given vendor, we evenly
Ground-truth Construction.
split her data into two pseudo vendors. Here we need to introduce a
threshold Tr which specifies the minimal number of photos that the
vendor has in order to build the fingerprint. We will test different
thresholds in our evaluation.
We observe that some vendors use the same photo for different
products (based on the product ID). To test the feasibility of re-
identifying vendors based on their photo-styles (instead of simply
Dupli.
Img.
Market
Agora
Yes
Evolution
SilkRoad2
Agora
No
Evolution
SilkRoad2
Pseudo
Pairs
1020
480
161
1093
519
197
415
211
76
408
137
45
443
155
47
181
59
24
Tr
10
20
40
10
20
40
10
20
40
10
20
40
10
20
40
10
20
40
Training
Distractors
597
540
319
680
574
322
248
204
135
518
271
92
546
288
108
233
122
35
AlexNet
Accuracy
0.969
0.973
0.950
0.952
0.967
0.990
0.976
1.00
0.987
0.733
0.796
0.733
0.626
0.742
0.830
0.724
0.814
0.875
ResNet
Accuracy
0.975
0.979
0.975
0.964
0.975
0.990
0.980
0.995
1.00
0.821
0.883
0.867
0.788
0.871
0.915
0.873
0.932
0.958
Table 2: Accuracy of ground-truth vendor matching based
on image analysis.
The accuracy metric then should measure how likely the top K can-
didates contain the correct match. For example, applying ResNet
(Tr =20) on non-duplicated images returns the top-5 accuracy of
0.964 for Agora, 0.948 for Evolution, and 0.966 for SilkRoad2. The
result indicates that the same vendors’ photos do carry distinct
personal styles, which can be used to build reliable fingerprints.
Regarding the threshold Tr , a lower threshold allows us to con-
sider more vendors. However, if Tr is too small, then there might
not be enough training data for each vendor, which reduces the
matching accuracy. For the rest of the paper, if not otherwise stated,
we set the threshold Tr = 20.
To compare different DNN models, we present Figure 4. Overall,
ResNet achieves the best performance. This is not too surprising
considering that ResNet is a relatively advanced model for object
recognition tasks [7]. However, our performance is not completely
aligned with the model performance on object recognition. The
InceptionV4 model is the state-of-the-art for ImageNet, but In-
ceptionV4 actually performs the worst on the darknet datasets.
Intuitively, the photos posted by vendors are very different from
those in ImageNet. ImageNet rarely covers photos of marijuana,
cocaine, or stolen credit cards. Overall, the performance differences
are not very big between different DNN models. This indicates that
our task is not very sensitive to the model selection.
In the above evaluation,
True Positives vs. False Positives.
we always report a match (i.e. the most-similar training vendor)
for a given testing vendor. However, in practice, not every vendor
has a matched Sybil identity. To this end, we will need to draw a
minimal probability threshold Tp to declare a match. Our system
will report a match only if the similarity score between the testing
vendor and the training vendor is higher than Tp.
The threshold Tp determines the trade-off between true posi-
tives (the correctly detected vendor pairs) and false positives (the
detected vendor pairs that turn out to be false). To examine this
trade-off, we slightly modify our workflow of Figure 3. Now, given
the set of distractors, we randomly put half of the distractors into
the training set and the other half into the testing set. By swapping
Figure 3: Workflow for the ground-truth evaluation.
matching the same photos), we create two versions of the ground-
truth datasets. For the duplication version, we consider all of the
vendor’s product photos. Each product’s photo only counts for
once, but we allow different products to use the same photo. For
the non-duplication version, we intentionally remove the duplicated
photos that are used for different products. The duplicated photos
are determined by their MD5 hashes.
Figure 3 shows the evaluation work-
Evaluation Workflow.
flow. First, for vendors that have more than 2 × Tr photos, we split
their photos into two even parts as the pseudo vendors. We add the
first part to the training dataset and the second part to the testing
dataset. Second, for the other vendors, if their image count > Tr , we
add them to the training dataset as the “distractors”. The number of
classes in the training set equals to the number of pseudo pairs plus
the number of training distractors shown in Table 2. The number
of classes in the testing set equals to the number of pseudo pairs.
Once we construct the dataset, we then perform transfer learning
based on a model pre-trained on ImageNet, and use our training
dataset to fine-tune the last layers of the network.
During testing, for each image in the testing dataset, we calculate
its probability of belonging to a given vendor in the training set.
Then those probabilities are averaged over the images that below
to the same vendor, which leads to a similarity metric for each
“training –testing vendor” pair. In this way, for each testing vendor,
we identify the most similar training vendor and examine if the
pseudo vendors are correctly paired. We calculate the accuracy
which is the ratio of the testing vendors that are correctly matched.
4.3 Evaluation Results