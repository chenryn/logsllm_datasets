differences between the output images can be further analyzed
quantitatively by means of our proposed metric, described in
further detail in Section V-D.
Fig. 6: Comparison of the output panoramas obtained from baseline
VS algorithm (a) and various approximation techniques (b,c,d) for
the two input image sets.
V. METHODOLOGY
In this section we describe the design methodology and
the evaluation environment used to measure the resiliency
of the VS algorithm as well as its approximate versions. In
section V-C we describe a small case-study to understand
the trade-offs of performing resiliency analysis on a full end-
to-end application (such as the VS algorithm) vs. constituent
small kernels. In section V-D, we deﬁne a metric to calculate
the quality of the corrupted output produced by the application
when perturbed by errors. We later use this metric to analyze
the quality of the corrupted outputs produced by the approxi-
mate VS algorithms.
Fig. 5: Comparison of IPC, execution time and energy of the proposed
approximate algorithms (VS RFD, VS KDS, VS SM) for Input 1
and Input 2, with the values normalized to the corresponding baseline
(VS) for each respective input.
Comparison of output quality: Figure 6 compares the
output images generated by the baseline VS algorithm and
the three approximations described for the two inputs. Visual
inspection shows that the approximate algorithms generate
output images of acceptable quality. Even in the approximate
output image with the worst quality (VS RFD for Input 1),
the quality degradation is due to image perspective and all the
pertinent information in the ﬁnal panorama is retained.
602
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:17 UTC from IEEE Xplore.  Restrictions apply. 
00.20.40.60.811.2VSVS_RFDVS_KDSVS_SMVSVS_RFDVS_KDSVS_SMNorm. IPCNorm exec timeNorm energyINPUT 1INPUT 2A. Measuring Resiliency of Video Summarization Algorithm
Error injection is a widely used error analysis technique
where an error is injected (typically one at a time) in a real
or simulated machine and the outcome (impact of the error) is
studied [34], [35], [36], [37], [38], [39]. Our goal is to evaluate
the application-level resiliency of the VS algorithm and its
different approximate versions in the presence of hardware
transient errors. The error model studied in this work assumes
the occurrence of single bit errors in the architectural register
ﬁle. The impact of an error on a program can be described by
the following four outcomes:
(1) Mask: The error is masked by consecutive execution
such that the application produces the correct output. This can
happen if the error affects dead state or if the corrupted state
is overwritten before being used.
(2) Crash: The error catastrophically affects the program
state and results in the program crashing. For example, an
error that leads to an out of bounds memory access.
(3) Silent Data Corruption (SDC): The error propagates
through the program execution and corrupts the output. This
is called a Silent Data Corruption because there is no obvious
symptom of the error till the execution completes and the
output is found to be corrupted.
(4) Hang: The error corrupts the internal state of the
program such that neither completes nor crashes but hangs.
Comprehensively injecting errors in each potential error site
in the program execution is prohibitively time consuming. For
instance, in our study, each bit in every architectural register
at every single execution cycle is a potential candidate for
error injection. For most applications, the number of error
sites is prohibitively large. Hence, we rely on statistical error
injection in randomly selected error sites in the execution.
This technique provides statistical summaries of the impact
of errors on the application by estimating average rates for
Mask, Crash, SDC and Hangs. Alternate, more comprehensive
and higher precision techniques such as Relyzer [40] could be
applied but are left to future work.
For accurately estimating the application resiliency, it is
essential to perform error injections in a signiﬁcant number
of error sites that are uniformly distributed over the program
execution. We use the term error-site coverage (or simply
coverage) to indicate the relative robustness in the number
and distribution of error sites picked for error injections.
We estimate the minimum number of error injection ex-
periments needed to get an adequate statistical sample by
observing the different rates of Mask, Crash, SDC and Hang
over many error injections and the point at which these
rates stabilize. In other words, the minimum number of error
injections required are at the knee of the trend curves for the
Mask, Crash, SDC and Hang rates. Beyond the knee of the
curves, increasing the number of error injections should only
change the outcome rates trivially.
B. Error Injection Environment
Error injection experiments are conducted on the IBM
POWER-based machine, running Linux RHEL 6.5 operating
603
Fig. 7: Overview of the Application Fault Injection framework.
Fig. 8: Execution proﬁle of the VS application
system. We use the Application Fault Injection (AFI) [4]
tool to perform error injections and evaluate the application’s
resiliency. Figure 7 shows the main components of AFI.
AFI is composed of two modules. The ﬁrst module, Fault
Injector takes the un-modiﬁed application binary, and injects
error bits into the application’s architectural state. Users can
change the injection conﬁgurations to specify where to inject
errors and how many errors to inject per run. For our study,
we conﬁgure AFI to inject one single bit error (bit ﬂip) in
a general purpose register (GPR) or a ﬂoating point register
(FPR). The execution cycle at which the error is injected is
random. Once the program execution is continued after the
error injection, the second module, the Fault Monitor, will
check the application’s running state and capture a potential
hang or crash. If the application ﬁnishes normally, the Fault
Monitor invokes a result checking procedure to determine if
the outcome of the error injection is an SDC or Masked result.
We perform separate experiments for error injections in
GPRs and FPRs as we want to separately examine the vul-
nerability of these two register types to single bit ﬂips.
C. Studying full end-to-end workﬂow vs. small (hot) kernels
An optimized statically compiled binary (using GCC 4.8.2
and OpenCV version 2.4.9) of the VS algorithm spans ∼1.5
million lines of assembly instructions. Error injection experi-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:17 UTC from IEEE Xplore.  Restrictions apply. 
Application Application Derating  User input parameters: injection configuration, result checking, etc. Hardware     AFI Read architected state Modify and write new faulty state   Fault Injector State Detection (Hang or Crash) Result Checking (SDC or Masked) Fault Monitor 0%10%20%30%40%50%60%70%80%90%100%Functional Components of VS Application% of Execution TimeOtherlrint (GLIBC)cv::computeDescriptorsmemcpy (libc)llrint (GLIBC)cvflann::KDTreeIndexcv::remapBilinearcv::warpPerspectiveInvokerments on a large application like the VS algorithm, that run to
completion, are time consuming and therefore limit the number
of error injections that can be performed.
Since the application is a composition of many program and
library functions, the question arises– can we simply carry
out a resiliency study of some representative hot functions
(functions that account for a signiﬁcant fraction of the ap-
plication execution time) and use the results to reason about
the resiliency of the VS algorithm? If so, can we then study
the resiliency of just those functions? This can lead to either
reduced overhead (less number of error injections) or increased
coverage (error injections take lesser time to run to completion
and hence we can potentially do more of them).
To investigate further, we undertake a case study to per-
form resiliency analysis on a hot kernel taken from the VS
application to see if the resiliency proﬁle of the kernel is
representative of the full end-to-end application. We show
in Section VI-C that this is not the case and the result of
such an analysis is sub-optimal. This further motivates the
need to develop and analyze full end-to-end applications that
realistically simulate the full workﬂow, as opposed to studying
just small kernel benchmarks that perform individual tasks.
Fig 8 shows the execution time distribution (by function)
for the VS algorithm extracted using the Linux utility tool
Perf [41]. Approximately 68% of the execution time is spent
in OpenCV libraries [42]. 54.4% of the total execution time is
consumed by just one OpenCV function – WarpPerspectiveIn-
voker, which is called from the WarpPerspective function, that
applies a perspective transformation to an image according to
a transformation matrix. Thus, we choose WarpPerspective as
the hot function whose resiliency proﬁle we study.
We design a toy benchmark called WP that takes an im-
age and a matrix as inputs and calls the OpenCV function
WarpPerspective on them and returns the transformed image
as the output. Essentially, WP is equivalent to having a stand-
alone WarpPerspective function and the output of WP is the
return value of the function as seen by the VS application. The
function WarpPerspective in turn calls two other functions:
warpPerspectiveInvoker and remapBilinear. We study the out-
comes from error injections in GPRs in these two functions
for VS and WP. Our error injection framework, AFI, gives us
the ability to control where the errors are injected and for this
experiment we only consider the error injection experiments
that inject errors in the functions of interest and observe the
outcome at the end of the program (either VS or WP).
D. Deﬁning SDC quality
As described in Section V-A, we deﬁne any deviation in the
application output due to an error as a Silent Data Corruption
or SDC. SDCs are the least desirable outcome of errors since
they are very hard to detect until the application execution
is completed and the corrupted output is generated. At that
time it
the
error. Crashes, on the other hand, can be detected using low
cost symptom-based detectors [35] and hence protecting error
sites that produce crashes incurs low overhead. Since SDCs
is too late for recovery techniques to correct
do not produce any easily detectable software symptoms,
protection against SDCs is normally done through techniques
like redundancy that have high overhead. In order to reduce
the resiliency overhead, we aim to quantify the egregiousness
or severity of the SDCs produced so we can identify tolerable
or benign SDC error sites that do not need to be protected.
Since the VS application produces an image (mini
panorama) as the output, the check to determine if an SDC
was produced is an image comparison between the error-
free application’s output (henceforth referred to as the golden
output) and the corrupted output produced by the application
execution injected with an error (henceforth referred to as the
faulty output). To determine if there is an SDC, AFI’s result
checking procedure simply compares the error-free output,
known a priori, with the output produced by the erroneous
execution, and classiﬁes the outcome as an SDC if there is
any difference between the two images.
In addition to knowing how many error injection experi-
ments result in SDCs, we are also interested in quantifying
the quality of the SDCs produced; i.e., the deviation between
the golden and the faulty output. To do this, we deﬁne a quality
metric which is calculated as follows:
Given a golden image g img and a faulty image f img,
we ﬁrst apply some global
transformations to ensure that
differences due to perspective, lighting, camera angle etc. are
removed. We do this because, in our system, the end purpose is
to use the output image of the VS application for identiﬁcation,
tracking and/or surveillance. Hence we are more concerned
with the content of the image and can tolerate minor cosmetic
disturbances in the ﬁnal image. The two transformed image
matrices obtained after this corrective step are g img tr and
f img tr. The pixel by pixel difference of these two images is
given by the matrix pixel diff img, such that
pixel diff img = g img tr − f img tr
Since in our scenario of interest, the ﬁnal panorama is going
to be viewed by a human being, we can tolerate some errors
in the color gradation of individual pixels. Thus, we only
wish to capture those differences in the image where the pixel
coloration is signiﬁcantly modiﬁed. For this purpose, we deﬁne
another matrix pixel 128 diff img where we only store values
from pixel diff img if the difference value is greater than 128,
that is over half the range for an 8 bit pixel which can assume
values between 0 and 255. Then, the relative l2 norm, which
estimates the deviation of the faulty output image from the
golden output image (in percentage) is described as,
relative l2 norm =
∗ 100
where, for an image X having n pixels x1, x2, . . . , xn
||pixel 128 diff img||2
||g img tr||2
(cid:113)
||X||2 =
x2
1 + x2
2 + . . . + x2
n
Once the relative l2 norm of a faulty image has been
calculated, we then assign that SDC an integer number called
the Egregiousness Degree (ED) which corresponds to the ﬂoor
of its relative l2 norm value. The higher the ED, the worst the
604
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:17 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 9: Graphs to show Coverage of error injection experiments. (a)
Different error injection outcome rates with increasing number of
error injection experiments for VS algorithm. The knee of the curve
stabilizes at 1000 error injections. (b) Number of errors injected in
different GPRs across 1000 experiments show a uniform distribution.
quality of the corrupted output image. For example, if an SDC
output has a relative l2 norm of 10.25%, it is assigned an ED
of 10. Any SDC that has a relative l2 norm of greater than
100%, is not assigned an ED and is automatically categorized
as an egregious SDC that must be protected.
A. Resiliency Proﬁle of Video Summarization Algorithm
VI. RESULTS
As described in Section V-A, we determine the minimum