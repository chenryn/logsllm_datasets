readings to the detector. PLC communication with the SCADA
system can be exploited to hide the real state of the system as
practically demonstrated in [17]. In contrast to [17], our attacker
does not require explicit knowledge of the physical model equations
to conceal the anomalies.
In particular, we assume that the anomalous physical process
results in a feature vector (cid:174)𝑥, which triggers the detection system.
The attacker thus needs to find an alternative vector (cid:174)𝑥′, which pre-
vents detection of the attack. We formalize the concelament attack
as follows: given a feature vector (cid:174)𝑥 and a classification function
𝑦() s.t. the detector correctly classifies 𝑦((cid:174)𝑥) = ‘under attack’, the
attacker is looking for a perturbation (cid:174)𝑥 + (cid:174)𝛿 s.t. 𝑦((cid:174)𝑥 + (cid:174)𝛿) =‘safe’. Since
the attacker’s goal is to evade Mean Squared Error-based classifiers,
the adversarial attack has to find a perturbation (cid:174)𝛿 to minimize the
reconstruction error between the input (cid:174)𝑥 + 𝛿 and output ˆ(cid:174)𝑥 of the
Reconstruction-based detector. Please refer to Section 4.1 for fur-
ther details on the target model. In a mathematical notation, it can
be written as the following constrained optimization problem in
Equation 1:
Figure 1: High level system and attacker model. The PLCs re-
port sensor data about the anomalous process to the SCADA.
The attacker can eavesdrop and manipulate a subset of the
data provided to the SCADA. The reconstruction-based de-
tector attempts to detect attacks based on a learned model
of the system’s benign operations.
centrifuges, and the cyber component to spoof reported sensor
readings (via a man-in-the-middle attack) thus avoiding anomaly
detection.
2.2 Evasion Attacks
In AML, an evasion attack is launched by an adversary to control
the output behavior of a machine learning model through crafted
inputs i.e., adversarial examples. Several evasion attack and defense
mechanisms have been proposed in the context of image process-
ing [50], speech recognition [11] and malware detection [22].
The authors of [6] characterize attacks on machine learning
models using a 4-tuple representation of the attackers’ knowledge
of the system under attack, the training dataset D, the feature set
X, the learning algorithm 𝑓 , and the trained parameters 𝑤. In an
adversarial setting, an attacker has complete or partial knowledge
of components; partial knowledge of a component is denoted with
the symbols ˆD, ˆX, ˆ𝑓 and ˆ𝑤 respectively. In particular, the authors
characterize three types of attack scenarios: Perfect-knowledge
white box attackers (D,X, 𝑓 , 𝑤); Limited-knowledge gray box at-
tacks ( ˆD,X, 𝑓 , ˆ𝑤); Zero-knowledge black box attacks ( ˆD, ˆX, ˆ𝑓 , ˆ𝑤).
Attacks are achieved by solving an optimization problem that min-
imizes distance between the sample and the adversarial example
e.g. by minimizing norms: L0, L2, L∞. In Section 3, we use this
notation to introduce our proposed solution and position it within
the related literature.
3 CONCEALMENT ATTACKS ON
RECONSTRUCTION-BASED ANOMALY
DETECTION
In this section, we introduce our system and attacker model, and
our general problem statement for constrained concealment attacks.
Then, we present our abstract white and black box attacker model.
3.1 System Model
We consider a system under attack (Figure 1) consisting of several
sensors and actuators connected to one or more PLCs, which are
in turn connected to a SCADA system that gathers data from the
PLCs. In our work, we assume that the SCADA is passive, so it does
not send control commands to the PLCs (e.g., to actively probe for
ACSAC 2020, December 7–11, 2020, Austin, USA
A. Erba et al.
Table 1: Classification of our attacker models based on
training data and features, Data tuple (D,X) and algorithm
knowledge and parameters. Access:
=partial. For
all white box attacks, the Defense tuple is (𝑓 , 𝑤), for all black
box attacks, (
=full,
).
,
Attacker’s
Constraints
X
D Read Write
(cid:32)
(cid:71)(cid:35)
(cid:71)(cid:35)
(cid:32)
Unconstrained § 5.4 (cid:32) (cid:32)
X Partially § 5.5 (cid:32) (cid:32)
X Fully § 5.5 (cid:32) (cid:71)(cid:35)
D § 5.5 (cid:71)(cid:35) (cid:32)
𝑛∑︁
1
𝑛
𝑖=1
minimize 𝑀𝑆𝐸 =
( ˆ𝑥𝑖 − (𝑥𝑖 + 𝛿𝑖))2
s.t.
(cid:174)𝛿 ∈ constraint space (Section 3.2.2)
real-time constraints imposed by CPS
𝑦((cid:174)𝑥 + (cid:174)𝛿) = ‘safe’
(1)
We note that the attacks we demonstrate are not necessarily
optimal, as the constraints are satisfied with non unique solutions.
The attacks are conducted in real-time (i.e., in milliseconds per
time step), not a posteriori (i.e., applied retrospectively to a longer
sequence of sensor readings after the attacker fully receives them).
3.2.2 Attacker Knowledge. Using the adversarial learning notation
introduced in Section 2, a concealment attack is characterized by
the knowledge of the attacker about the training dataset D, feature
set X, learning algorithm 𝑓 , and trained parameters 𝑤. In the ICS
setting, the attacker can be characterized differently according
to his knowledge of the attacked system. In order to explain our
attacker model, we split the tuple (D,X, 𝑓 , 𝑤) into two: the Data
tuple (D,X) and Defense tuple (𝑓 , 𝑤). We assume the attacker
to be unconstrained or constrained w.r.t. the Data tuple, i.e., the
sensor readings X that she can observe and manipulate and the
data D that she eavesdrops. Moreover, we classify attackers as
white box, black box, w.r.t. the Defense tuple, i.e., the knowledge of
learning algorithm 𝑓 , and trained parameters 𝑤. Table 1, provides
an overview of the attacker’s constraints considered in this work.
we classify the attacker as:
(cid:77)Constraints over Data Tuple According to the Data tuple (D,X),
• Unconstrained (D,X), in which the attacker can manipulate
all 𝑛 features in (cid:174)𝑥, and her perturbations are limited in terms
of L0 distance to be at most 𝑛.
• Features Partially Constrained (D, ˆX), we assume that the
attacker is constrained to perturb a subset of 𝑘 out of 𝑛
variables in (cid:174)𝑥, and her perturbations are limited in terms of
L0 distance to not exceed distance 𝑘.
• Features Fully-Constrained (D, ˆX), we assume that the at-
tacker is constrained to observe and perturb a subset of 𝑘
out of 𝑛 variables in (cid:174)𝑥, and her perturbations are limited in
terms of L0 distance to not exceed distance 𝑘.
• Data Constrained ( ˆD,X), we assume that the attacker is
constrained to eavesdrop a limited quantity of process data
that are used for training its attacks.
Selection of Constrained Features The subset of features that can
be modified is highly use-case dependent (i.e., which link is attacked,
which device was compromised). To demonstrate the generality
of our findings, we explored two types of constraints: a best-case
scenario and a topology-based scenario.
For the best-case scenario, we assume that the selection of the
𝑘 out of 𝑛 manipulated features can be made by the attacker to
maximize the attack impact. This arguably represents a best-case
scenario for the constrained attacker (i.e., an attacker constrained to
features that happen to be relatively ideal for the attacker). For the
second scenario, constraints are derived from the network topology.
We assume that the attacker can compromise a single substation
(or PLC) in the network, and the selection of 𝑘 out of 𝑛 features
is based on which sensors are interconnected to the compromised
substation.
(cid:77)Knowledge of Defense Tuple We classify the attacker according
to their knowledge of the Defense tuple (𝑓 , 𝑤), as:
• White box (𝑓 , 𝑤), the attacker knows the exact system model
and its variables (such as the currently estimated system
state), and the exact thresholds of the classification system.