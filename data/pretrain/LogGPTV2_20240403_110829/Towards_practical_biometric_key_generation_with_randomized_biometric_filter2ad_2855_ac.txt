between the encryption of the feature indexes and the encryption
of the quantization offsets. For each j ∈ [0, n − 1], set i = L[j],
R← [αi, Δ]δi . Recall that Δ = 1 + maxi δi, and
and select γi
so γi encodes αi into a random integer that is less than the largest
quantization width of any feature, and that is also an integer multi-
ple of δi from αi. The next step is to encrypt both the i and γi by
computing Cj = (cj,0, cj,1) = (EN
(γi)). Since each γi
k0
is encoded into the same range [0, Δ], the decryption of cj,1 under
any key results in a semantically meaningful quantization offset for
any feature.
(i), EΔ
k1
The template then consists of:
T = (C, v) = ((C0, C2, . . . , Cn−1), Hver(π||K0|| . . .||Km−1))
The token v is used for veriﬁcation purposes, and is a function of
the password and error-corrected biometric samples, but is inde-
pendent of the key K because Hver and Hkey are independent ran-
dom oracles.
5.2 The KeyGen Algorithm
The KeyGen algorithm is simpler than enrollment in that it de-
crypts the template, measures the biometric sample with the recov-
ered features, and then recreates the key (see Algorithm 2). The in-
put to the algorithm is password π ∈ Π, the template T = (C, v),
Input: Template T = (C, v), password π ∈ Π, the biomet-
ric sample β, the features Φ, and error-correction information
δ0, . . . δN
Output: The key K, or ⊥ on failure.
1: k0 ← Hpass,0(π), k1 ← Hpass,1(π)
2: for j ← 0 to |C| − 1 do
i ← DN
k0 (C[j][0]) // Extract feature index
3:
αi ← DΔ
k1 (C[j][1]) mod δi
4:
xi ← maxx∈{0}∪[αi,φi(β)]δi
5:
applied to β
6: Kj ← i||xi
if Hver(π||K0|| . . .||Kj ) = v then
7:
K ← Hkey(π||K0|| . . . ||Kj ) // Derive the key
8:
return K
9:
10: return ⊥ // Could not recreate the key, return failure
// Extract quantization offset
x // Quantize the output of φi
Algorithm 2: Speciﬁcation of the RBT KeyGen algorithm
k0
and a biometric sample β. First, KeyGen derives the decryption
keys k0 = Hpass,0(π) and k1 = Hpass,1(π) and uses these keys
to decrypt the template and recreate the list L and the quantiza-
tion offsets (i.e., the γL[j]) (see Algorithm 2, lines 3–4). For j ∈
[0, |C|−1], let (L[j], γL[j]) ← (DN
(cj,1)). The values
in L are a list of indexes that specify features. Let i = L[j] be such
an index. Then γi is the encoding of αi, which speciﬁes the offset
of the quantization for φi. Thus, the partitioning boundaries over
[0, ri] can be recreated as {0} ∪ [γi mod δi, ri]δi .
(cj,0), DΔ
k1
To recreate the key, KeyGen measures β and sets xi to be the
largest partition boundary that is less than or equal to φi(β) (see
Algorithm 2, line 5). Then, letting Kj = i||xi, the algorithm it-
eratively attempts to recreate the key by checking that v = Hver(π
||K0||K1|| . . . ||Kj ) for j ∈ [0, |C| − 1]. If this is the case for any
j, then KeyGen outputs the key K = Hkey(π||K0||K1|| . . .||Kj ).
Otherwise, the algorithm has failed and it outputs the failure sym-
bol ⊥. This iterative reconstruction process is necessary because
the key is only derived from the features in Ψ, and it is impossible
to determine where these features end, and where the extra features
that were added as padding from ˜Ψ begin.
and β(cid:2)
5.3 Correctness
Correctness amounts to showing that if Enroll(β1, . . . β(cid:2), π) out-
puts T = (C, v), and K, and KeyGen(β(cid:2), π(cid:2), T ) outputs K(cid:2)
, then
if π = π(cid:2)
is close to the median of β1, . . . , β(cid:2), then K =
K(cid:2)
. For this to happen two properties must hold: ﬁrst, KeyGen
must use the same features as Enroll and quantize the output range
of each feature in the same way as Enroll. Second, this quantiza-
tion must map β(cid:2)
to the same segment as the μi that was computed
from β1, . . . , β(cid:2) by Enroll. If these two properties hold, then the
input to Hkey (i.e., π||K0|| . . .||K|Φ|−1) will be the same for both
KeyGen and Enroll, and consequently both algorithms will output
the same key.
It follows from the correctness of PRPs that if π = π(cid:2)
, KeyGen
will correctly decrypt C to extract the i and the γi that were en-
coded by Enroll. This implies that both algorithms will use the
same set of features. To see that they quantize the output range of
each feature in the same way, observe that γi ≡ αi mod δi. Since
δi is publicly known, KeyGen can reliably extract αi, which is a
boundary of one of the segments in the range of φi. Thus, KeyGen
can recover the quantization of the range of φi as {0} ∪ [αi, ri]δi ;
and this is the same set that was used by Enroll.
biometric input in the same way, we show that if β(cid:2)
Now that we have established that both algorithms process the
is close to the
. Recall that K ← H(π || K0
median of β1, . . . , β(cid:2), then K = K(cid:2)
|| . . . || K|Ψ|−1), with Kj = i||xi. The only part of the input that
is computed from biometric input is xi, which is the lower bound
of the δi-length segment that contains μi = Median(φi(β1), . . . ,
φ(β(cid:2))).
is close to the samples that were provided during
enrollment, then |φi(β(cid:2)) − μi|  τvar, where τvar is a tunable parameter. The
remaining set of features comprise Φ. If U is chosen to be large
enough to be representative of the population of users who use the
system, then this process need only be performed once.
Uniformity in Feature Selection. Having ensured that the features
in Φ are assigned to a user with pairwise independent probabilities,
the task at hand is to also ensure that these probabilities are uni-
form. We do this by empirically selecting δi such that k percent
of the users require error tolerance less than δi. This is accom-
plished during enrollment with the Select algorithm, which cycles
through each feature and determines from the enrollment samples
how much tolerance is needed to correct all of the feature values
to one segment. If this tolerance is less than the global threshold
δi, then the feature is assigned to the user. (See the full version
of this paper [2] for more details on Select.) As such, the user is
assigned only those features that she can repeat consistently. Since
δi is small, adversaries should have a greater difﬁculty replicat-
ing each feature to within that tolerance, and the output range of
the feature is partitioned into more segments, yielding a potentially
greater search space for the attacker. Our experimental results in
Section 8 indicate that this is indeed the case.
Practical Considerations. While we have provided techniques
to assign features to users in a way that meets our cryptographic
requirements, there are several other practical considerations that
´
`
must be addressed. First, we must consider the composition of Φ.
N
Clearly, RBTs beneﬁt from large feature sets. Since there are
n
equally likely templates, larger feature sets imply greater uncer-
tainty for an adversary. At the same time, however, since we as-
sume that the size of the password space might be small, we must
also endeavor to ﬁnd features that resist forgery and searching at-
tacks. That is, we cannot simply add many random features to Φ.
There are also other factors that govern how we craft Φ. To ensure
that the derived keys have high entropy, each of the features in Φ
should have uncorrelated outputs. This reduces the likelihood of
success of the search attacks similar to those described in [3]. All
of these constraints reduces the set of possible features that can be
used to create Φ.
There is also the matter of selecting k, the value that determines
the percentile that will be used for error-correction. There is a trade
off between the number of features assigned to each user (for large
values of k) and the resistance to forgability (smaller values of k).
Since it is difﬁcult to optimize feature selection across the con-
straints imposed by entropy requirements, forgery resiliency, cor-
relation between features, and the selection of k, we adopt the fol-
lowing iterative approach. First, we create a large set of features.
Then, we compute the statistical correlation over the outputs and
variation of each feature. We use a greedy algorithm to remove
those features that have high correlation with many other features.
We then perform an empirical evaluation to measure the entropy
and False Accept Rates over each feature. Then, Φ is composed of
the union of two distinct sets: the features with the highest entropy,
and the features with the smallest FARs. The goal is that by com-
bining both sets, RBTs will be resilient to both forgeries and search
algorithms.
6.2 Feature Selection Evaluation
In this section we provide empirical evidence that Select acts as
a random permutation on an n element subset of Φ. In order to
provide a concrete analysis, we focus on one biometric modality:
handwriting. Our results are based on the data set described in [4]
that consists of over 9,000 writing samples from 47 users. Each
user provided 10-20 enrollment samples for ﬁve different phrases.
We emphasize that these phrases are used simply to extract bio-
metric readings; they are unrelated to the low-entropy password π
used by RBTs. The data set also consists of a number of forgeries,
we only use the stronger “trained” forgeries for our security analy-
sis. The data set also contains approximately 3,000 phrases that are
used to generate a “parallel corpus” to drive generative algorithms.
In this section we analyze our feature selection strategy to study