with the 
the 
is chosen. Otherwise, 
configuration 
that maximizes 
to estimate 
p( nk) of each node using the chosen 
time and the ac­
response 
defined as follows. 
\7p = 
-D.(maxaEA,nkEN p(nk) -maXhEH p(h» 
(3) 
D.D 
is defined as the ratio of the change 
function 
CPU capacity" 
The gradient 
in "shortfall 
didate configurations 
performance. 
ence between the CPU demand p(nk) of the largest 
unal-
between the initial 
to the change in overall 
CPU is defined as the differ­
The shortfall 
and the can­
application 
978-1-4244-7501-8/10/$26.00 
©2010 IEEE 
501 
DSN 2010: lung et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:14:00 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEEIIFIP International 
Conference 
on Dependable 
Systems & Networks (DSN) 
and the maximum CPU capacity p( h) still 
on some host in the system. The configuration 
in the greatest 
decrease 
in shortfall 
capacity 
per 
in performance 
is the one chosen as the next "current 
as compared to the current 
config­
replica 
located 
available 
that results 
unit decrease 
configuration 
uration". 
The search algorithm 
then creates 
a new candidate 
con­
single-change 
the allowed CPU 
by reducing 
degradations 
In the rare case that no such resource 
group which has enough remaining 
order of their CPU caps, and are assigned 
resource 
to fit them and on which no other replica 
exists. 
be found because the number of available 
groups is smaller 
than the number of replicas 
the constraint 
in the first resource 
whether there is another 
resource 
for that replica, 
is relaxed 
replica 
group. 
to the first child 
CPU capacity 
of same bundle 
group can 
child resource 
of the bundle, 
and it is placed 
group on which it fits regardless 
of 
of the same bundle on that 
of a single component 
in a single ap­
by a step of /).r (set to 5% by default) 
or increas­
to the next 
level of a single application 
5 Simulation 
Results 
it with access to more capacity. 
The 
with this new set of candidate 
config­
In this section, 
we present 
simulation 
results 
using a 
for a feasible 
allocations 
to fi­
and application 
configuration 
written 
simulator 
application 
The target 
line auction 
benchmark. 
offline measurements 
workload 
ing transaction 
according 
reinstantiate) needed 
client 
configuration 
the optimizer 
generator. 
and returns 
the set 
to the "browsing 
cal­
configuration, 
between the original 
for each replica, 
capacity 
adjust, 
The durations 
of these actions 
are rel­
for the experiments 
is the RUBiS on­
We created 
the LQNS model using 
from [17] and execute 
rates representing 
the model us­
user behavior 
mix" defined by the RUBiS test 
in the Java based SSJ framework [19]. 
set CC by exploring 
figuration 
of the chosen configuration 
cap for the replicas 
plication 
ing the distribution 
higher level to provide 
algorithm 
is repeated 
urations 
distributions 
nally be found. 
until the CPU capacity 
are sufficient 
Upon finding a feasible 
(migrate, 
the difference 
culates 
and new configuration 
of actions 
to affect the change. 
atively 
from a few milliseconds 
more, they can be performed 
time [7]. Therefore, 
reconfiguration 
short compared 
to typical 
MTBF values, 
and range 
to a few minutes at most. Further­
without 
causing 
VM down­
does not factor in any 
the controller 
Fit Algorithm. The fit algorithm 
costs while making its decisions. 
in Algorithm 
2 uses 
by assigning 
by the search algorithm 
consid­
Second, it 
placement 
it determines 
to perform two tasks. 
bin packing approach 
hosts to each replica. 
level at a time, starting 
as a single "application 
by the queuing models. Packing is done a 
packed into the given resources. 
actual component 
whether the node CPU caps and appli­
levels assigned 
a hierarchical 
First, 
cation distribution 
can be feasibly 
also determines 
To do so, it initially 
physical 
ers each application 
bundle" with 
volume equal to the sum of all the CPU caps of all its repli­
cas as predicted 
resource 
at the top of the resource 
cated between different 
levels are packed, i.e., an allocation 
to a data center across its clusters, 
across different 
cation across hosts in a rack. For applications 
bution level is equal to that of the current 
being packed, the algorithm 
dle into individual 
equal to the replica's 
bundles 
packing 
whose distri­
resource 
level 
breaks the application 
bun­
"replica 
own CPU cap c.cap(n). The replica 
subsequently 
travel 
on lower resource 
as independent 
levels. 
Subsequently, 
of bundles assigned 
hierarchy, 
data centers. 
are allo­
lower 
with the "whole system" 
and ending with an allo­
racks in a cluster, 
each with a volume 
where bundles 
bundles", 
units during the 
followed 
Packing within a single resource 
level is done using a 
constrained 
algorithm, 
of the n log n time first-fit 
in which bundles are considered 
decreasing 
in decreasing 
variant 
a commonly used load balancing  approach 
the reinstantiated 
CPU utiliza­
of the target 
to the least loaded 
and chooses their new 
VMs 
of the failed 
each 
it reinstantiates 
(Opt) with two 
relies 
ref­
on design re­
(LL) strat­
replica 
loaded" 
workload 
failures. 
to tolerate 
is allocated 
failed replicas 
We compare our optimized 
approach 
The Static strategy 
The "least 
hosts. Specifically, 
(VM) in the order of decreasing 
based on the CPU utilizations 
erence strategies. 
dundancy 
egy emulates 
where additional 
host. It regenerates 
placements 
and the remaining 
failed 
tion on the least loaded host. The utilization 
host is then updated to take into account 
VM before choosing 
the VMs have been reassigned, 
the CPU capacities 
to their measured 
10% CPU. When a resource 
LL migrates 
the original 
it failed from their current 
group, while the Static 
that were running 
CPU utilization 
the controller 
simply restarts 
VMs running 
locations 
strategy 
with a lower bound of 
a host for the next failed VM. Once 
back to this resource 
the replicas 
group before its failure. 
on the resource 
the three approaches 
setup considers 
in two simulation 
a local cluster 
of 
reallocates 
to the VMs on each host proportional 
delays between 
pool distributed 
in each data cen­
communication 
a resource 
with three clusters 
and 4 machines 
with identical 
setups. The cluster 
machines 
them. The cloud setup considers 
across two data centers, 
ter, 3 racks in each cluster, 
The communication 
on if they share a host, rack, cluster, 
the communication 
is D, the communication 
ferent racks, clusters, 
2.5D, respectively. 
latency 
and data centers 
between two machines 
in a rack 
delays between machines 
are l.5D,  2D, 
in dif­
and 
or a data center. 
When 
in each rack. 
delays between VMs vary depending 
on the group before 
group is recovered/replaced, 
by allocation 
We evaluated 
978-1-4244-7501-8/10/$26.00 
©201O IEEE 
502 
DSN 2010: Jung et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:14:00 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable 
Systems & Networks (DSN) 
·Opt. UtiL LL UtiL .Static 
Uti\. 
·Opt. Ulil. LL Util. .Static 
Util. 
 0.1 
. c 
;:> 
0.01 
....... Light Opt 
-&- Light Stalic 
Light LL 
 Hc"vyO pt. 
 HC'IVy 51'llie 
 HcavyLL 
0 
U.01 
0.1 
Relative MTTR 
10 
o  0.1 0.5 1 
Relative 
MTTR 
o  0.01 0.1 0.5 
RelaUve MTTR 
(a) Unavailability 
(b) Light Workload Performance (c) Heavy Workload Performance 
Figure 
2. Cluster 
setup results 
we ran fault injection 
ex­
and data center failures 
applicable 
for systems 
For each scenario 
and strategy, 
where host, rack, cluster, 
To make the results 
and MTTRs, we report all times nor­
to the MTBF, which was set to 1.0, and the MTTR 
periments 
were simulated. 
with different MTBFs 
malized 
values were varied over a range. Each simulation 
ran for 
a normalized 
the average), 
For each experiment, 
performance 
time period of 10 (i.e., 
and we repeated 
each experiment 
degradation. 
we calculated 
10 failures 
per run on 
10 times. 
both availability 
and 
each tier was replicated 
configu­
The cluster 
The light scenario 
gold and silver, 
setup consisted 
two scenarios. 
of 60 and 120 reg/sec 
for 
an underutilized 
system with an initial 
host and with a workload 
of 2 instances 
where the gold instance 
has weight of 1 (see 
of the RUBiS 
has 
instance 
1). For both instances, 
application: 
weight of 5 and the silver 
Eguation 
twice. We considered 
simulates 
ration that has each of the 12 VMs running on a separate 
physical 
the gold and silver 
a heavily 
machines, 
pacity) 
75%) and the MySQL replicas 
workloads 
instances. 
cess followed 
For repair, 
from 0.01 to 5.0, indicating 
500% of the actual MTBF. 
with 8 
(with 25% CPU ca­
host with a Tomcat replica 
hosts. The 
utilized 
where each Apache replica 
are 120 and 180 reg/sec 
Faults were simulated 
instances. 
consolidated 
we varied the per host MTTR over a wide range 
with a Poisson 
of the target 
arrival 
host to fail. 
that repair took from 10% to 
by random selection 
server environment 
for the gold and silver 
The heavy scenario 
run on dedicated 
shares a physical 
(with 
pro­
simulates 
Figure 2(a) shows the unavailability 
of the system as a 
failures. 
Fortunately, 
MTBF values-we 
ditional 
pared to typical 
instantiation 
the RUBiS MySQL instances, 
tion times are presented 
in Table 1. 
times to be on the order of 80-90 seconds for 
while the controller 
execu­
both windows are short com­
have measured 
the VM 
response 
As ex­
Figures 2(b) and (c) show the performance 
degrada­
computed over the period 
Static performs 
than the manually 
configuration 
significantly 
The LL results 
The Opt approach 
vs. the MTTR. The initial 
there are small improvements 
was able 
selected 
times due to the fact that the optimizer 
has very little 
even at high relative MTTR 
by the dashed line. 
worse than the other 
perfor­
values (less 
tion D of the two applications 
that they are available 
time of the system is indicated 
pected, 
two strategies. 
mance degradation 
than 12% in the worst case). In fact, in the heavy work­
load scenario, 
in the mean 
response 
to find a better 
tial configuration. 
into the strength 
petitive 
heavy scenario 
and almost as poorly as Static. 
light workload 
figuration 
replica 
workloads. 
edge to make intelligent 
are bottlenecks. 
ences in host CPU utilizations 
and can end up co-locating 
tleneck 
time. 
but in the 
worse than Opt, 
This is because under the 
that recon­
significantly 
while that is not the case under heavy 
knowl­
decisions 
it makes decisions 
there is enough spare capacity 
with Opt in the light workload 
of the Opt controller. 
The LL controller 
lacks the necessary 
with great negative 
can be performed 
significantly 
a regenerated 
capacities, 
LL performs 
on small differ­
LL is fairly 
com­
scenario, 
resource, 
VM with a bot­
Instead, 
provide 
without 
(since all of them are high), 
ini­
impact to the response 
the most insight 
reducing 
about which components 
Both the Opt and LL strate­
100% availability, 
of the relative MTTR. 
while the unavailability 
function 
gies achieved 
the Static strategy increases significantly 
MTTR. Since both LL and Opt regenerate 
a failure  occurs, 
may not achieve 
strategies 
controllers 
reguire 
after a failure 
taneous. 
time to make a reconfiguration 
of new VMs is not instan­
with the relative 
VMs as soon as 
RUBiS applications, 
scenarios. 
gold services 
were 60 reg/sec 
while in the medium scenario 
60 reg/sec. 
CPU. The gold applications 
are thus replicated 
over separate 
During this time, the system is vulnerable 
this result is expected. 
100% availability 
and instantiation 
The cloud setup experiments 
In practice, 
these 
decision 
because the 
In the heavy scenario, 
reguire 
consider 
3 gold and 3 silver. 
Each VMs was initially 
to ad-
of 
they were 30 reg/sec 
and 
80% of one 
and 
allocated 
higher availability 
clusters, 
while the silver 
6 instances 
of the 
We consider 
two 
of silver 
and 
respectively, 
workloads 
and 120 reg/sec, 
978-1-4244-7501-8/101$26.00 
©2010 IEEE 
503 
DSN 2010: lung et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:14:00 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable 
Systems & Networks (DSN) 
 McdiumOpL 
""- McdiuT1l Stalic 
Medium LL 
 HcavyO pt. 
 Hca\'y Stalic 
 Hca\'yLL 
·Opt. Util. LL Ulil. Stalic 
Ulil. 
  18.-.-.-.-.-.-.-r-r-" 
 16 
 1r----4-+; 
 12r--+; 
 I 0 r-E---'=-,'---fl 
II' 8 ..-..-I-­
Q 
•  6 
&. 0 +-- +--__ ...--.o ..-  ......  
  0.1 0.3 0.5 0.7 
• Opt. UIil. LL Util. 
.Slalic 
Util. 
I ':::r----- 
1 1000 
l 
Q  100 
. E 
i=  10 
'" 
0.1 0.3 0.5 0.7 
o >-1 --,--+ • ..:.,,' ....... ___ • ----'---L...;IJ.nITI 
u.1 
1 0  
Relative MTIR 
Relative MlTR 
Relative MTTR 
(a) Unavailability 
(b) Medium Workload Performance (c) Heavy Workload Performance 
3. Cloud setup results 
Figure 
were sim­
failure 
are replicated 
over racks. Failures 
levels of the hierarchy 
rack, host) with different 
(i.e., 
data center, 
and repair rates. 
if the MTBF and MTTR on the host-level 
applications 
ulated at different 
cluster, 
Specifically, 
Mf and Mn then at the rack, cluster, 
els they are 4Mf and 4Mn 16Mf and 16Mn and 160Mf 
and 160Mn respectively. 
relative 
from 10% to 100% of the actual MTBF. The LL strategy 
was modified to account 
of resource 
hierarchies. 
we varied the per host 
that repair took 
are 
and data center lev­
MTTR from 0.1 to 1, indicating 
for the different 
For repair, 
levels 
Figure 3(a) demonstrates 
that with our given MTTR and 
strategy 
can ensure high 
strategies 
while the Static 
fails to do so. Figures 
of Opt over LL, particu­
The results 
MTBF rates, the regeneration 
availability, 
3(b) and (c) show the advantage 
larly under the heavy workloads. 
some cases LL does not perform much better than Static. 
This is because if a set of hosts (a whole rack, cluster, 
data center) 
the silver  applications, 
(i.e., 
then it may be better to do nothing 
fails and the failed hosts contain 
or 
the VMs of 
than reallocating 
those VMs in LL. 
Static) 
show that in 
Cluster 
Cloud 
ReI. MTIR  Light Heavy ReI. MTIR  Medium Heavy 
0.1 
0.5 
1.0 
2.0 
2.5  51 
3.5  55 
4.3  59 
5.5  59 
0.1 
0.5 
0.7 
1.0 
31 
79 
99 
119 
81 
150 
174 
200 
Table 1. Controller 
Exec. Time (sec) 