centile over 5G mmWave are higher than over LTE by 0.4 s and 1.4 s, respec-
tively, despite the much higher speeds and lower RTTs brought by 5G mmWave
compared to LTE (Table 1). (iii) Self-blockage has minimal impact on the per-
formance of the multi-user AR app, increasing the median E2E latency by about
0.5 s and the 75-th percentile by about 0.2 s. (v) Similar to the results in [8], the
key contributors of the E2E latency are on the hosting side for both 5G mmWave
and LTE. Together, the handshakes (1a), the visual data transmission (1b), and
the cloud processing (1c) account for about 60% of the E2E latency over both
5G mMWave and 64% over LTE. In contrast, the resolving side components (2a
- 2d) contribute together only 12% of the median E2E latency.
Overall, 5G mmWave brings practically no improvements to the performance
of multi-user AR apps. In the following, we take a closer look at the latency of the
individual app components and try to uncover the root causes of this surprising
result and the factors that prevent 5G mmWave to unleash its potential.
4.2 Latency 2x: Resolver Notiﬁcation
In our experiments, we often observed a substantial delay between the last data
packet sent by the cloud to the host and the moment the notiﬁcation from the
186
M. Ghoshal et al.
cloud of a new hosted object is received by the resolver. In Fig. 2, we observe
that this delay, which we call 2x and was not reported in [8], varies from under
100 ms to as high as 7 s, and can be a signiﬁcant contributor to the E2E latency
over both LTE and 5G mmWave, accounting for about 16%, 26% and 23% of
the E2E latency in the median case for 5G-towards (1.05 s), 5G-away (1.77 s)
and LTE (1.4 s), respectively.
To understand the root cause of this delay, we set up a proxy between the
cloud server and the resolver UE. The proxy is a server on Google Cloud, and
is connected to the resolver UE through an L2TP tunnel. We synchronized the
proxy and the two UEs using NTP and used tcpdump to capture and analyze
packet traces on both sides. By comparing timestamps, we further broke down
the 2x latency into two parts: between the last data packet sent by the cloud
to the host and the moment the notiﬁcation from the cloud is received by the
proxy (2x 1) and between the moment the notiﬁcation is received by the proxy
and the moment the notiﬁcation is ﬁnally received by the resolver (2x 2).
We found that 2x 1 is always short (about 100 ms), suggesting that the
load on the server has minimal impact on the total 2x latency. Hence, the main
contributor to the 2x latency is 2x 2 (varying from a few about 100 ms to more
than 6 s) and the the root cause of the high 2x latency lies somewhere on
the path from the proxy to the UE. We further found that every time the 2x
latency was higher than a few 100 s of ms, there was a TCP retransmission of the
notiﬁcation packet from the server. In contrast, no retransmission was observed
for the cases when 2x 2 was comparable to 2x 1. Since the TCP retransmission
packet was always received by the proxy within 100 ms and retransmissions over
the wireless link (at the MAC and RLC layers) are unlikely to cause a delay
of several seconds, we conjecture that the root cause of the high 2x latency lies
in the cellular packet core network and the various middlebox (NATs, ﬁrewalls)
policies implemented by the operator, which have been shown to often have a
signiﬁcant impact on E2E TCP performance [40].
4.3 Latency 1a and 2a: Connection Handshakes
In [8], it was shown that TCP connection handshakes between the app and the
cloud take 3 s on average on the hosting side (1a), contributing signiﬁcantly to
the E2E latency, while the handshakes and data pre-processing on the resolving
side (2a) ﬁnish in less than 1 s. In contrast, Fig. 2 shows that the 1a latency in
our experiments is signiﬁcantly reduced over both 5G mmWave and LTE and is
similar to the 2a latency (below 1 s), with the exception of 1 run over LTE that
experienced a 2a latency higher than 5.5 s.
One would expect the 1a and 2a latencies to be lower over 5G mmWave
compared to over LTE, as 5G mmWave has lower RTTs (Table 1). However,
a closer look at these latencies (Fig. 3) shows that this is not the case. While
the minimum values of 1a and 2a are indeed lower over 5G mmWave, the 75-th
percentiles and maximum values are higher. Analyzing the root cause of this
result is diﬃcult, as each of these delays consists of multiple components (e.g.,
1a involves tapping the screen, an optional DNS transaction, a TCP handshake
Can 5G mmWave Support Multi-user AR?
187
Fig. 3. Closer look at latencies 1a, 1b, 2a, 2b.
with the cloud, and a TLS handshake) and the delay of each component might
aﬀect other delays. For example, we found that when the TCP handshake is
preceded by a DNS transaction, the time to complete the TCP handshake is
half over 5G mmWave compared to over LTE (∼20 ms vs. ∼40 ms), whereas in
the absence of a DNS transaction it is higher (∼220 ms vs. ∼170 ms), possibly
due to diﬀerent promotion delays (e.g., the delay for the radio to switch from
the idle state to the connected state) in 5G and LTE. Since the contribution of
both 1a and 2a to the E2E latency is minimal over both cellular networks, we
do not further study these latencies.
4.4 Latency 1c and 2c: Cloud Processing
Figure 2 shows that the latency of the cloud processing of the host data (1c) is
2.2 s in the median case over both 5G mmWave and LTE, although there are a
few outliers as high as 7.5 s, which we attribute to temporary server overloads.
This value is signiﬁcantly lower than the value reported in [8] (5 s), as the cloud
technology has evolved over the past 2 years, but remains high, contributing
about 30–35% to the E2E latency. In contrast, the cloud processing latency on
the resolving side (2c) is in general negligible (200–400 ms), similar to what was
reported in [8]. However, there are outlier values that can be as high as 5.5 s.
We found that these outliers are due to multiple rounds of communication and
processing when the cloud processing fails and requests the resolver to upload
new visual data, as we explained in Fig. 1. One such example is shown in Fig. 4.
After the ﬁrst chunk of visual data upload (0–0.07 s), the cloud processing fails,
and the resolver uploads another chunk of visual data (0.89–0.95 s), which the
cloud processes successfully. We also observe that the cloud processing delay is
much longer in the case of a failure (the ﬁrst cloud processing delay in Fig. 4
takes 0.82 s while the second one takes only 0.16 s).
188
M. Ghoshal et al.
Fig. 4. Visual data
re-uploaded by
resolver as cloud processing failed in the
ﬁrst attempt.
Fig. 5. Throughput as a function of
uploaded bytes.
4.5 Latency 1b and 2b: Uplink Data Communication
In [8], the average 1b latency was found to be 10 s over a public LTE network.
In contrast, our measurements in Fig. 2 show that the 1b latency over LTE is
much lower, with a median value of 1.27 s and a maximum value of 2.04 s,
which explain the large drop in the LTE E2E latency compared to the values
reported in [8]. Hence, unlike the results reported in [8], 1b is no longer the
primary contributor to the E2E latency over LTE, although its contribution still
remains signiﬁcant. On the other hand, the contribution of the 2b latency on
the resolving side remains negligible.
Figure 2 shows that 5G mmWave reduces both 1b and 2b latency when the
user faces towards the BS, as expected due to its higher bandwidth and lower
RTTs. However, the improvement is very small – 0.14 s and 0.27 s, in the median
case, for 1b and 2b, respectively. Self-blockage increases the 1b latency to 1.44 s
(vs. 1.13 s in the absence of blockage) in the median case and to 2.6 s (vs. 1.77
s) in the worst case, but no impact on the resolving side.
To understand why 5G mmWave has minimal impact on the uplink trans-
mission latency in spite of the much higher uplink bandwidth compared to LTE,
we show in Fig. 5 scatterplots of the uplink throughput vs. the uploaded bytes
on the hosting and resolving side for each of the 40 runs. We observe that the
data transfer size on both sides is very small – up to 5 MB on the hosting
side and up to 0.11 MB on the resolving side, similar to the numbers reported
in [8]. The small data transfer sizes explain the small improvement in the uplink
data communication latency brought by 5G mmWave compared to LTE. The
data uploads always ﬁnish before TCP exits the slow start phase, preventing it
from taking advantage of the much higher available bandwidth oﬀered by 5G
mmWave. This is clearly shown in Fig. 5, where we observe very low throughput
values (at most 22 Mbps on the host side and 12 Mbps on the resolver side),
which are similar over 5G mmWave and LTE, especially for data transfer sizes
up to 2 MB.
Can 5G mmWave Support Multi-user AR?
189
Figure 5 further shows that facing away
from the BS results in a small throughput
reduction compared to facing towards the BS
and to LTE for transfer sizes larger than 2
MB. This result appears to contradict recent
studies reporting that the user orientation
has a signiﬁcant impact on 5G mmWave per-
formance. The reason for the small observed
throughput (and latency) degradation in our
experiments is again the very small data
transfer sizes, which prevent TCP from exit-
ing slow start. Figure 6, which plots the CDF of the modulation and coding
scheme (MCS) values collected by MobileInsight every 5 ms, further corrobo-
rates this claim. The MCS values are much lower when the user faces away
from the BS compared to when the user faces towards the BS, conﬁrming that
self-blockage deteriorates signiﬁcantly the link quality. However, this large degra-
dation of the link quality is perceived to a much lesser degree by the multi-user
AR app, due to the very low application layer throughput.
Fig. 6. 5G mmWave MCS.
4.6 AR Design Optimizations
In this section, we study two optimizations that were proposed in [8] to reduce
the uplink data transmission latency (1b and 2b) over LTE networks.
Packet Size Adaptation. In scenarios when the RAN is congested, large IP
packet sizes can experience heavier segmentation at the Radio Link Control
(RLC) layer, which can increase the per-packet RLC latency and subsequently,
the TCP RTT, and adversely impact the growth of the TCP window during
an AR upload burst. While smaller IP packets can help address this issue, they
increase the network overhead. In [8], it was shown that a TCP Maximum Seg-
ment Size (MSS) of 650 bytes can increase throughput by 62% and reduce the
RAN latency by 37% compared to the default MSS.
We experimented with the same three MSS values used in [8]: 400 bytes, 650
bytes, and default (1356 bytes). We conducted 10 runs with each MSS over 5G
mmWave with the user facing towards the BS. The results are shown in Fig. 7.
We observe that changing the MSS has a minimal impact over of 5G mmWave
networks. On the hosting side, all three MSS values result in roughly the same
median latency. On the resolving side, the default MSS results in a slightly
lower median latency than the other two values but in a slightly higher worst
case latency. We conjecture that the minimal impact of the MSS on the 1b and
2b latency is due to the fact that 5G mmWave deployments are still at their
infancy and they are unlikely to be congested, unlike LTE networks. To test the
impact of this optimization in a congested network, we repeated the experiment
with a third phone sending backlogged UDP traﬃc to a cloud server. Figure 7
shows that in a congested network, the smallest MSS (400 bytes) results in a
190
M. Ghoshal et al.
Fig. 7. Data transmission latency with
varying MSS.
Fig. 8. Data transmission latency with
background ICMP traﬃc.
much higher latency, especially in the case of 1b, similar to what was observed
in [8]. However, the other two MSS values still yield similar latency.
Small Background Traﬃc. Every time the application starts sending a new
uplink data burst, the UE has to request resources from the BS. The BS is
initially unaware of the uplink sending buﬀer, and may allocate a small uplink
grant for the UE, causing RLC segmentation and increasing the per-packet RAN
latency. In order to make the BS aware of the UE’s uplink buﬀer during an AR
session, the authors in [8] proposed to generate small amounts of background
uplink traﬃc, using ICMP packets during an AR session. To examine the eﬀec-
tiveness of this optimization over 5G mmWave, we conducted 10 runs with and
without ICMP traﬃc with the user facing towards the BS. Figure 8 shows that
this optimization is eﬀective over 5G mmWave, bringing a signiﬁcant reduction
to the 1b latency on the hosting side. The median latency reduces from 1.31
s (without ICMP traﬃc) to 1.05 s (with ICMP traﬃc). The 2b latency is also
reduced but the reduction is much smaller compared to the 1b latency due to
the much smaller transfer sizes. However, since the contribution of the 1b and
2b latencies to the E2E latency is small (Sect. 4.5), this optimization has a small
impact on the E2E latency.
Summary. Overall, in spite of the much higher bandwidth and lower RTT
over LTE, 5G mmWave brings no beneﬁts to multi-user AR apps. Although
5G mmWave brings a small reduction to the visual data transmission latency
and certain optimizations that were proposed over LTE networks are equally
eﬀective over 5G networks and can further reduce this latency, there are other
major contributors to the E2E AR latency (resolver notiﬁcation, cloud process-
ing), which are independent of the underlying cellular network technology. As a
result, the E2E latency over 5G mmWave remains too high to enable practical
user interaction in multi-user AR apps.
Can 5G mmWave Support Multi-user AR?
191
5 Energy Consumption
In this section, we compare the power draw and energy drain of both the host
and the resolver in running the AR app over LTE and 5G mmWave.
5.1 Methodology
The AR app uses ﬁve power-hungry phone components: CPU, GPU, Camera,
Display, and the cellular NIC. We use the utilization-based power models [13]
for mobile devices, which have been widely used [14,16,18,19,27,36,37,41–43]
to model the instantaneous power draw of the CPU and the GPU. In a nutshell,
such a model derives the correlation between the utilization of a phone compo-
nent in each of its power states, and the resulting power draw using carefully
designed micro-benchmarks. To use such a model, the CPU and GPU usage
are logged during app execution using Linux event trace [21] and afterwards
fed into the power model as input to estimate the per-component power draw
during the app execution. For the OLED display, we used the piece-wise OLED
model recently proposed in [15], which decomposes the RGB color space into
16 × 16 × 16 subgrids and derives accurate pixel power model for each subgrid
using liner regression to achieve low OLED power prediction error of no more
than 4.6% on four recent generations of phones. We developed a program by
modifying the Android “screenrecord” program to record the screen during the