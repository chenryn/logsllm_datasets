Toggling collection: This allows users to to freeze their proﬁle
scores. This indicates to MOREPRIV that it should not track any
behavior of the user until persona reﬁnement is re-enabled. This
is a form of a privacy mode, similar to those supported in modern
browsers.
4.5 MOREPRIV APIs
MOREPRIV also exposes APIs to third party developers that al-
low application-speciﬁc personalization. We discuss three of these
API functions:
• IsMoRePrivEnabled() returns true if personalization is en-
abled. We allow users to toggle personalization on and off as
part of the MOREPRIV conﬁguration UI.
• TopProfile() return the most relevant proﬁle to the user
if personalization is enabled, and null otherwise. This is
useful for application skinning, for example by changing the
background of the app based on the top proﬁle of the user.
• Ignore(Object o) Informs MOREPRIV not to apply OS-
level personalization to o. This allows developers to bypass
the GUI features such as automatic list reordering.
We have also written a wrapper library that checks for these API
functions and calls them if present. Thus, app developers can write
a single app that will work on both MOREPRIV-enabled and plain
OSes (albeit without personalization).
5.
INTEGRITY AND EXTENSIBILITY
There is growing evidence that the need for trusted user identities
in advertising is growing4. These cases are emblematic of the com-
monly occurring case in which a 3rd party requires knowledge that
the classiﬁcation of users is accurate and trustworthy. A signature-
based scheme may prevent benign users from loading hacked clas-
siﬁers, but when it comes to malicious users, no such recourse is
available. While the operating system itself is trusted by the user,
the 3rd party may not trust the integrity of apps and classiﬁers: ma-
licious users can install or “sideload” a hacked classiﬁer to send
fabricated data.
Instead, we extended MOREPRIV to optionally use zero- knowl-
edge proofs to provide a solution to the problem of integrity of user
proﬁling, effectively guaranteeing to third parties that the personal-
ization signal can be trusted through checkable proofs of computa-
tional integrity.
5.1 Design Choices
In some cases, the information provided by the default classiﬁers
is not sufﬁcient for personalization. However, while we want to
support greater extensibility, we do not want to expose sensitive
user data to untrusted 3rd parties.
In this paper, we consider two ways to architect the system we
have described. The monolithic architecture provides only the built-
in, default user classiﬁer. However, we acknowledge that in some
settings, more application-speciﬁc classiﬁers may be called for.
Since classiﬁers may need to be upgraded as a result of retraining,
the customizable classiﬁer approach allows for custom classiﬁers
to optionally be installed at the discretion of the user.
Monolithic: The model discussed in this paper thus far assumes
a single monolithic set of MOREPRIV classiﬁers which are imple-
mented as an operating system service. As such, they are part of
the TCB. A consumer of the user proﬁle can trust the proﬁle com-
putation under the assumption that the operating system has not
been tampered with. We feel that this is a sensible assumption,
4See http://bit.ly/13hJpNk, http://tcrn.ch/PR5ZDL, http://bit.
ly/1bT5mFD, http://on.wsj.com/1e57ms1, http://bit.ly/1bT5mFD, and [8]
for discussions of advertising fraud and its implications.
since tampering with the OS can invalidate the integrity of the ob-
servations on the basis of which the user proﬁle is calculated. The
main shortcomings of this monolithic approach is the difﬁculty in
upgrading the classiﬁers and the fact that that classiﬁers are part of
the TCB.
Customizable: An alternative model discussed in the rest of this
section allows for custom classiﬁers to be installed in the same way
that regular apps are installed, without affecting the OS and adding
to the TCB. The user is responsible for checking installation- and
runtime permissions to ensure these classiﬁers do not leak data in-
appropriately, just as they are for any other mobile app. Alterna-
tively, as proposed in several recent projects [46, 26, 14, 16, 25,
31], classiﬁers can be subjected to static or runtime analysis to gain
further assurance.
5.2 Deploying Custom Classiﬁers
In our model, custom classiﬁers are distributed as third-party ap-
plications in the same manner as other third-party software on mo-
bile platforms. When the user installs a classiﬁer application, he is
prompted for permission to let the app interact with MOREPRIV by
monitoring keywords among the personalization signals discussed
in Section 4.1; this permission is requested at the same time as
the other platform-speciﬁc permissions supported by the operating
system. When the classiﬁer is installed, MOREPRIV registers the
needed keywords that are listed in the classiﬁer’s manifest.
To perform classiﬁcation, the application queries MOREPRIV
via an exposed API that returns the number of times each feature
word was observed in the signals as a vector w = [w0, . . . , wn].
After receiving an updated set of word counts, the classiﬁer runs its
custom algorithm, and uses the result as needed. Whether this clas-
siﬁcation step is performed periodically in the background, or on-
demand when requested, is an application-speciﬁc design choice
left up to the classiﬁer developer. Similarly, the choice of when to
provide a proof of integrity is left to the developer; it can happen
each time a new classiﬁer is constructed (for an absolute guaran-
tee of integrity), or only when the classiﬁer consumer decides to
challenge the client (for a probabilistic guarantee).
5.3 Motivating Example
Suppose that a third-party service, such as AdMob, needs to de-
termine a characteristic about the user that it cannot derive from
the built-in personae. For example, it may want to determine if
the user is a student, in order to determine whether to display ad-
vertisements about spring break vacation packages. In this situa-
tion, AdMob can utilize a classiﬁer developed in-house, which is
installed as a user-level application on the user’s device much as a
third-party ad library is on existing mobile platforms.
This is a compelling example of securely-managed cooperation
between MOREPRIV and a third-party that wishes to personalize,
but it exposes an important concern for the third party. Namely,
because AdMob’s algorithm is running on the user’s device in an
unprotected mode, AdMob has no assurance that the results it ob-
tains correspond to the true results of its algorithm. For example, a
user who wishes to consume ad-supported content without provid-
ing information about themselves may simply install a “dummy”
application in place of AdMob’s, that returns random results.
In this section, we discuss a strategy for balancing this concern
with our central focus of privacy based on recent advances in Non-
Interactive Zero-Knowledge (NIZK) Proofs of Knowledge [7]. We
provide integrity using the same practical assumptions as Danezis et
al. [13] and Schnorr [41], which roots trust in the operating system,
but does not require trusted hardware.
We discuss an implementation of the AdMob scenario given above
that uses zero-knowledge proofs based on the construction of [39],
and show how it can be efﬁciently parallelized to achieve accept-
able performance. While our implementation is application-speciﬁc,
the approach can be generalized without requiring custom zero-
knowledge proof development using one of the many zero-knowledge
proof compilers [3, 4, 33].
5.4 Achieving Computational Integrity
In this section, we begin by giving a brief general overview of
NIZK schemes, and then describe our realization in the context of
MOREPRIV.
Background and Overview: In a NIZK scheme, one party (the
prover) attempts to convince another (the veriﬁer) that it has cor-
rectly performed a computation that is known to both parties, with-
out revealing the inputs that it provides to the computation. Of-
tentimes, the inputs are generated signed by a data provider that
is trusted by the veriﬁer, in order to establish an initial root of in-
tegrity. In our setting, the prover corresponds to a classiﬁer, the
veriﬁer to a service that consumes the classiﬁer’s output, and the
data provider to the MOREPRIV core.
Our NIZK scheme makes use of a non-interactive commitment
scheme, which consists of the algorithms Commit and Open5. Given
a value x, Commit(x) produces a commitment cx as well as a piece
of auxiliary data ox called the opening. The commitment is opened
by revealing (cx, ox), and checking that Open(x, ox, cx) returns
true. Commitments have two useful properties: (1) binding, mean-
ing that Open only returns true when (cx, ox) = Commit(x), and
(2) hiding, meaning that a party who possesses only cx should not
be able to learn anything about x. Abusing notation somewhat, we
use the phrase signed commitment, and write Commit KA (x), to re-
fer to the commitment of x signed with A’s public key. MOREPRIV
provides inputs to the classiﬁer via API calls, which can later be
converted into signed commitments by the operating system when
the consumer service requires a proof of integrity for some output
of the classiﬁer. The integrity of these inputs comes from the con-
sumer service’s trust in the integrity of the operating system, i.e.,
that it has not been compromised.
The scheme that we use requires the following entities:
• The computation f (xpub, xpriv), which takes input known to
both parties xpub as well as input private to the prover xpriv.
• The inputs xpub and xpriv, as well as signed commitments
Commit KTP (xpriv) to the private inputs, using the data provider’s
public key KTP .
As illustrated in Figure 1, it proceeds as follows:
well as the signed commitment S = CommitKTP (xpriv).
• The data provider sends the prover its private input xpriv, as
• The prover runs the computation on both sets of inputs to
It constructs a zero-knowledge
learn r = f (xpub, xpriv).
proof of knowledge P = ZKProof(f, xpub, xpriv, r, S) which
demonstrates that ∃ xpriv.r = f (xpub, xpriv) and
S = CommitKTP (xpriv). Finally, the prover sends P and r
to the veriﬁer.
• The veriﬁer checks P against S and r, which can be done
in roughly the same amount of time taken by the prover to
generate P and compute r.
We refer the reader to the explanation of these schemes given by [7]
for more details.
5For simplicity, we omit certain details relating to security parameters and setup.
For a detailed explanation, the reader should refer to [7, 39]
Figure 1: Zero-knowledge proofs for computational integrity.
5.5 Evaluation
We evaluated our zero-knowledge classiﬁer on a mobile device
to determine its runtime, network utilization, and impact on battery
life. In general, we found our implementation to be quite practical.
Performance is parametrized by the number of feature words in the
classiﬁer.
For the student classiﬁer in this section, based on our practical
experience with Bayesian classiﬁcation, we believe that using be-
tween 100 and 300 feature words gives sufﬁcient precision. Note
that we do not generally anticipate every single request requiring a
proof; a scenario where a third party would periodically audit the
result is more likely, making the numbers reported below a perfor-
mance upper bound. Moreover, because the prover can run in idle
time, when the phone is unused, and not on demand, we believe
this represents more than acceptable performance.
Setup: Unless stated otherwise, all experiments were performed
on a Nokia Lumia 920 running Windows Phone 8. Experiments
involving networking were performed on a small wireless network
comprised of the mobile device, a laptop (with an Intel 4965AGN
chipset), and a dedicated router not connected to an external net-
work. The Zero-Knowledge protocol was implemented in F# and
C#, using the .NET inﬁnite-precision integer library. For commit-
ments, we used a 1024-bit modulus; for signatures we used SHA1
with RSA, again with a 1024-bit modulus. All experiments used
eight personae, unless otherwise noted.
Basic measurements: Figure 2(a) shows the time taken on the mo-
bile device to compute the classiﬁer result for all personae against
the number of feature words in each persona’s set of Bayesian clas-
siﬁer. Both cores of the mobile device were utilized without re-
striction. As shown, the time for a classiﬁer size of 300 words
is just under ﬁve minutes. Since our current personae classiﬁers
are of approximately 300 words, we believe this to be a reason-
able overhead. Proﬁling reveals that nearly all processing time is
devoted to generating cryptographically-secure random noise (for
commitment generation), and integer exponentiation (for the group
operations). This means that while it is not feasible to produce a
new, up-to-date zero-knowledge proof on-demand, one could per-
form these computations in the idle time after a period of user in-
teraction with the phone.
Figure 2(b) shows the network transfer time required to send the
proof and input commitments against the number of feature words.
To take these measurements, we simulated 3G and 4G cellular net-
works by modulating throughput, latency, and packet loss rate. Av-
erage throughput in each measured conﬁguration (3G, 4G, and Wi-
ﬁ) was 134.4, 198.4, and 945.8 kilobytes per second, respectively.
We observed proofs of less than 200 K that it takes 0.41–1.22
seconds to transmit the setup and proof for a 100 and 300-feature
word classiﬁer, respectively, on an “average” 3G network. We also
(a) Time to run the prover.
(b) Transmission time for the proof.
(c) Battery usage.
Figure 2: Custom classiﬁer performance metrics
observed reasonably proof sizes of less than 200 KB for even the
largest (300 feature-word classiﬁer).
Figure 2(c) shows the amount of battery draw required by the
classiﬁer module, measured against the number of feature words in
the classiﬁer. This was computed based on CPU utilization, accord-
ing to the model of Mittal et al. [34]. The power required to produce
a proof goes down when multiple cores are available, and there
is sufﬁcient work available for the second core; this trend begins
shortly after 100 feature words. To give these readings some per-
spective, we assume a 24-hour battery charge cycle in order to com-
pute the amount of “battery time” consumed by the prover. Our test
phone (Nokia Lumia 920) advertises a built-in 2,000 mAh battery,
so producing a proof for a 300-word classiﬁer consumes 0.099% of
the available battery, about 40 seconds.
Summary: We believe our scheme to be practical with today’s
cryptographic primitives and mobile hardware in terms of compu-
tation and transmission time. With 100 feature words, the prover
time is about 11 seconds. Transfer time of the proof for 100 fea-
ture words is under a second on an average 3G connection. Battery
utilization (under 1 mAh) is negligible.
6. PERSONALIZATION
MOREPRIV offers many opportunities for personalization.
In
this section, we focus on several personalization scenarios in depth,
but note that this is only the tip of the iceberg. Other prominent
examples include keying custom dictionaries for word completion,
Data ProviderProverVerifierPrivate Inputs, Signed CommitmentsPublic KeyResult, Zero-Knowledge Proof, Signed CommitmentsInternetClassifierConsumerKernelUserMobile DeviceXEPPClassifier0204060801001201100200300400prover time (sec) # feature words 2 cores (s)1 core (s)Average Improvement: 2.02x 0.000.501.001.502.002.501100200300400500transfer time (s) # feature words 3G4GWireless00.511.522.533.544.51100200300400500battery consumption (mAh) # feature words 1 core2 coresc
e
x
e
s
s
e
n
s
u