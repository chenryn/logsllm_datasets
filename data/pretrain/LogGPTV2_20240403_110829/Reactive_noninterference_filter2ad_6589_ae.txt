mands and only produce invisible outputs.
5.14 Lemma: If high L(c) and (μ, c)
high L(c(cid:2)
μ ∼l μ(cid:2)
.
We use this lemma to verify the conditions of Deﬁni-
tion 4.1 to show that ∼l is an ID-bisimulation. Since we
carefully constructed our binary relation, we can also show
that programs are related to themselves at every label l if
they are well typed.
5.15 Lemma: If (cid:14) p, then p ∼l p for all l.
Combining the previous lemma with Theorem 4.5 gives
us the security result we claimed. This result guarantees
us that any well-typed program will be secure when it is
initialized with any store.
5.16 Theorem: If (cid:14) p, then (μ, p) is an ID-secure trans-
ducer.
876. ADDITIONAL RELATED WORK
The introduction has already drawn comparisons with the
lines of work most directly relevant to ours. Here, we sur-
vey some more distantly related work that may also be of
interest.
McCullough’s “restrictiveness” property [14] is a security
policy for labeled transition systems with input and out-
put. Rather than deﬁning a “high-level policy” on traces or
streams, he deﬁnes restrictiveness with a set of constraints
on transitions.
It is therefore comparable with Goguen
and Meseguer’s “unwound policy” [6] or our ID-bisimulation.
McCullough’s restrictiveness property requires input totality
in a strict sense (without an allowance for input buﬀering)
and is termination-sensitive.
The work of O’Neill, Clarkson, and Chong [16] (hereafter
“OCC”) builds upon Halpern and O’Neill’s Multiagent Sys-
tems Framework [8]. They deﬁne noninterference in terms of
user strategies, which are functions that map every history
of l-visible events to the next action for each principal at
level l. This framework allows their security deﬁnitions to
consider the possibility of a high user revealing information
to a low user indirectly via choice of strategy. This seems to
be of little practical value in a setting where high users have
more direct means to interact with low users than through
the system in question. For instance, in the setting of the
web client, if a banking application wants to reveal the user’s
account number to a third party, this can be done trivially
on the server’s side of the application rather than through
the code running in the user’s browser. Moreover, Clark and
Hunt [2] demonstrate that the choice of strategy does not
permit covert communication in deterministic programs that
are interactive in the sense of OCC. Although our execution
model is diﬀerent, the same fact may well hold true in our
setting. (Much of the focus in OCC is actually on dealing
with probabilistic behaviors; since RIMP is deterministic,
this aspect is orthogonal to our aims.)
As mentioned earlier, Hunt and Sands [9] deﬁne security of
their interactive programs in terms of inﬁnite input streams,
but don’t use streams for describing their outputs. On the
other hand, Askarov, Hunt, Sabelfeld, and Sands [1] explic-
itly consider potentially inﬁnite sequences of outputs in their
assessment of the weaknesses of termination-insensitive se-
curity, but do not deal with any kind of input. Matos, et.
al. [13] give a type system and a proof of information-ﬂow
security for a notion of “reactive programs” rather diﬀer-
ent from ours. Their programs run (deterministically) to
completion without consuming any intermediate input or
producing any intermediate output.
7. CONCLUSIONS AND FUTURE WORK
We have proposed a formal deﬁnition of information-ﬂow
security for programs driven by event handling and have
shown that language-based enforcement is a viable means
to guarantee this property.
It is now natural to ask how
well this maps onto the real world of web programming.
The ﬁrst issue is whether our deﬁnitions of security cor-
rectly capture the sort of web browsing conﬁdentiality poli-
cies we desire in practice. Of course, such deﬁnitions cannot
guarantee conﬁdentiality in an absolute sense, since there
are covert channels, such as timing channels, that are out-
side of our model. Assessing these channels and taking steps
to mitigate their eﬀects is an important part of any real-
world security implementation. At the same time, like other
deﬁnitions of “pure noninterference,” our deﬁnitions are too
restrictive, in that they rule out programs that must release
(parts of) secret information to properly function: consider
a mashup that must reveal a private street address to Google
in the course of locating it with a Google Maps component.
Finding appropriate mechanisms for declassiﬁcation is an
important direction for future work.
Another question is whether our fundamental model of
interaction is ﬂexible enough to account for real-world web
behavior. Real network messages may be structured and
may include diﬀerent substructures with diﬀerent security
levels. For the purpose of a noninterference analysis, one
could easily model such a message as a sequence of mes-
sages at diﬀerent levels. However, a naive labeling of web
page structures and a strict adherence to reactive noninter-
ference can lead to some surprising results. For instance, if
an entire HTTP response containing a web page is given a
non-public security label, then it would not be secure for the
browser to load any images on that page from servers with
incomparable security labels: there would be no direct ﬂow,
but a system input (the HTTP response in this case) at one
security level must only cause system outputs at the same
level or higher. One easy workaround for this scenario is to
give the initial HTTP response a public label and to use a
private label only for the body of the message.
The connection between our model and the user inter-
face is also very important, and the design space is com-
plex. Users need to be able to understand the security
interpretation that the browser assigns to each event they
generate; otherwise they have no way of understanding the
model’s guarantees about the conﬁdentiality of their input
and browsing actions.
In particular, users need to under-
stand the precise form of the “pseudo-messages” correspond-
ing to their actions. The message content corresponding to
the action of entering text in a text box is reasonably clear.
However, the action of clicking on an HTML link is much
more subtle: if it is interpreted as a message from the user to
the browser that contains the entire URL of the link destina-
tion, then reactive noninterference actually puts the burden
on the user for verifying that the URL does not contain any
encoding of any piece of secret data. Obviously, the user
cannot do this without assistance from the browser, but it
must be noted that a reactive noninterference policy says
nothing about the correctness of that assistance. In addi-
tion to the contents of these user-generated messages, their
security levels must also be determined. This could be con-
trolled with a global “secrecy mode” setting for the whole
browser, a per-window secrecy mode, a per-DOM-element
secrecy mode, a per-action dialog box, or some combination
of these. Moreover, in principle, it is possible that some
of these modes might cause a single user action, such as a
button press, to be viewed as a sequence of messages with
diﬀerent security levels (this would be useful for the same
reason that it might be useful to view an HTTP response as
a sequence of messages, as described above). A complex se-
curity interface will be hard to understand; an overly simple
one may not provide enough ﬂexibility to support web pages
that interact with multiple remote sites or may not provide
as much conﬁdentiality as the user would like. There seem
to be fundamental tradeoﬀs between ﬂexibility, complexity,
and security in this design space.
88Since our model rules out preemptive multitasking, one
may wonder whether it can account for timer events, which
are common in web programming. The information secu-
rity of timer events can be understood by modeling them
as AJAX requests to a remote server that sends back a re-
sponse after a ﬁxed amount of time. Of course, concerns
about covert timing channels must still be handled sepa-
rately, and timing channels that exploit timer events may
have a much higher bandwidth than covert channels based
solely upon the timing of real network messages.
An entirely diﬀerent question is whether language-based
security is the best mechanism for enforcing our nonin-
terference properties in the setting of web browsers. Al-
though its event handling follows the same basic model as
JavaScript, RIMP is a long way from a web scripting lan-
guage. First, one would want to add some of the key features
of JavaScript, such as ﬁrst class functions, the ability to dy-
namically add and remove handlers, and eval. Second, one
would need to design a security-aware version of the DOM
interface for this language to use. Finally, one would have to
implement a method for type-checking and running secure
scripts in a manner that is reasonably backwards-compatible
with existing web pages and scripts. All of these are impor-
tant topics for future research.
Acknowledgments
Damien Pous made a big contribution to the early discus-
sions that eventually led to this paper. We gratefully ac-
knowledge support from the National Science Foundation
under grant number 0715936, Manifest Security.
8. REFERENCES
[1] A. Askarov, S. Hunt, A. Sabelfeld, and D. Sands.
Termination-insensitive noninterference leaks more
than just a bit. In In Proceedings of the 13th European
Symposium on Research in Computer Security, pages
333–348, Malaga, Spain, Oct. 2008.
[2] D. Clark and S. Hunt. Non-interference for
deterministic interactive programs. In Formal Aspects
of Security and Trust (FAST) ’08, 2008.
[3] Coq Development Team. The Coq Proof Assistant
Reference Manual v8.1. http://coq.inria.fr/.
[4] R. Focardi and R. Gorrieri. A classiﬁcation of security
properties for process algebras. Journal of Computer
Security, 3(1):5–33, 1995.
[5] J. A. Goguen and J. Meseguer. Security policies and
security models. In Proc. IEEE Symposium on
Security and Privacy, pages 11–20. IEEE Computer
Society Press, Apr. 1982.
[6] J. A. Goguen and J. Meseguer. Unwinding and
inference control. In In Proceedings of the IEEE
Symposium on Security and Privacy, 1984.
[7] I. Gray, J.W. Probabilistic interference. pages
170–179, May 1990.
[8] J. Y. Halpern and K. R. O’Neill. Secrecy in
multiagent systems. ACM Transactions on
Information and Systems Security, 12(1):1–47, 2008.
[9] S. Hunt and D. Sands. Just forget it—the semantics
and enforcement of information erasure. In In
Proceedings of the 17th European Symposium on
Programming (ESOP’08). Springer-Verlag (LNCS),
2008.
[10] C. Jackson, A. Bortz, D. Boneh, and J. C. Mitchell.
Protecting browser state from web privacy attacks. In
WWW ’06: Proceedings of the 15th international
conference on World Wide Web, pages 737–744, New
York, NY, USA, 2006. ACM.
[11] C. Jackson and H. J. Wang. Subspace: Secure
cross-domain communication for web mashups. In
WWW ’07: Proceedings of the 16th international
conference on World Wide Web, 2007.
[12] B. Jacobs and J. Rutten. A tutorial on (co)algebras
and (co)induction. EATCS Bulletin, 62:62–222, 1997.
[13] A. A. Matos, G. Boudol, and I. Castellani. Typing
noninterference for reactive programs. In In
Proceeding of the Workshop on Foundations of
Computer Security, 2004.
[14] D. McCullough. Noninterference and the
composability of security properties. In Proc. IEEE
Symposium on Security and Privacy, pages 177–186.
IEEE Computer Society Press, May 1988.
[15] M. S. Miller, M. Samuel, B. Laurie, I. Awad, and
M. Stay. Caja: Safe active content in sanitized
JavaScript. A Google research project., Jan. 2008.
[16] K. R. O’Neill, M. R. Clarkson, and S. Chong.
Information-ﬂow security for interactive programs. In
In Proceedings of the 19th IEEE Workshop on
Computer Security Foundations, pages 190–201,
Washington, DC, USA, 2006. IEEE.
[17] F. Pottier and V. Simonet. Information ﬂow inference
for ML. ACM Transactions on Programming
Languages and Systems, 25(1):117–158, 2003.
[18] C. Reis, S. D. Gribble, and H. M. Levy. Architectural
principles for safe web programs. Presented at the
Sixth Workshop on Hot Topics in Networks
(HotNets-VI), Nov. 2007.
[19] A. Sabelfeld and A. C. Myers. Language-based
information-ﬂow security. IEEE Journal on Selected
Areas in Communications, 21(1):5–19, 2003.
[20] Same origin policy for JavaScript.
http://www.mozilla.org/projects/security/components/
same-origin.html.
[21] D. Volpano, G. Smith, and C. Irvine. A sound type
system for secure ﬂow analysis. Journal of Computer
Security, 4(2-3):167–187, 1996.
[22] A. Zakinthinos and E. S. Lee. A general theory of
security properties. In In Proceedings of the IEEE
Symposium on Security and Privacy, pages 94–102,
Washington, DC, USA, 1997. IEEE.
APPENDIX
A. COINDUCTIVE DEFINITIONS
We have used the Coq proof assistant [3] to guide our in-
tuition about coinduction and to check many of our deﬁni-
tions and proofs. Following Coq’s type-theoretic notion of
coinduction, we take coinductive deﬁnitions as a primitive
notion. We view our inference rules, both inductive and
coinductive, as deﬁnitions of logical propositions, although
they have an obvious translation to a set-theoretic deﬁnition
of mathematical relations.
A coinductive deﬁnition can be understood as taking the
greatest ﬁxed-point interpretation of a grammar or a set of
inference rules. In the case of a grammar, a coinductive def-
89inition describes the set of all ﬁnite or inﬁnite objects that
can be built with repeated applications of the term con-
structors (instead of just the ﬁnite objects). In the case of
a proposition deﬁned by a set of inference rules, a coinduc-
tive deﬁnition means that we allow the proposition to be
proved with a ﬁnite or inﬁnite derivation. This is often (and
only) necessary when deﬁning predicates on inﬁnite data.
Note that it is also perfectly reasonable to use an induc-
tively deﬁned proposition over coinductively deﬁned data,
which will mean that the truth of the proposition can only
depend on a ﬁnite portion of the potentially inﬁnite data.
Inductively deﬁned propositions give rise to a principle of
induction, which can be used to prove a statement in which
such a proposition appears as a hypothesis. On the other
hand, coinductive deﬁnitions give rise a principle of coinduc-
tion, which can be used to prove a statement in which such
a proposition appears as a conclusion.
For further background on inductive and coinductive rea-
soning, see the tutorial by Jacobs and Rutten [12].
B. EVALUATION OF RIMP EXPRESSIONS
B.1 Deﬁnition: Inductively deﬁne μ (cid:14) e ⇓ n with the fol-
lowing rules:
μ (cid:14) n ⇓ n
μ (cid:14) r ⇓ μ(r)
μ (cid:14) e2 ⇓ n2
μ (cid:14) e1 + e2 ⇓ n
μ (cid:14) e2 ⇓ n2
μ (cid:14) e1 − e2 ⇓ n
μ (cid:14) e2 ⇓ n2
μ (cid:14) e1 = e2 ⇓ 1
μ (cid:14) e2 ⇓ n2
μ (cid:14) e1 = e2 ⇓ 0
μ (cid:14) e2 ⇓ n2
μ (cid:14) e1 < e2 ⇓ 1
μ (cid:14) e2 ⇓ n2
μ (cid:14) e1 < e2 ⇓ 0
μ (cid:14) e1 ⇓ n1
μ (cid:14) e1 ⇓ n1
μ (cid:14) e1 ⇓ n1
μ (cid:14) e1 ⇓ n1
μ (cid:14) e1 ⇓ n1
μ (cid:14) e1 ⇓ n1
n = n1 + n2
n = n1 − n2
n1 = n2
n1 (cid:12)= n2
n1 < n2
n1 (cid:12)< n2
90