not appear to be causally dependent on PacketOut from s1; that
represents an incomplete dependency.
Figure 6: Data plane model. 1: App X instructs the controller
to emit a data plane packet from switch s1. 2: Switch s1 emits
the data plane packet on its link towards switch s2. 3: Switch
s2 receives the incoming data plane packet and sends it to the
controller. 4: App Y processes the data plane packet.
the temporal order of a control plane activity (i.e., generation
of an outgoing data plane packet), followed by a data plane
activity (i.e., transmission of a data plane packet), followed by
another control plane activity (i.e., processing of an incoming
data plane packet). As shown in Figure 6b, a provenance
model without the implicit causality of the data plane shows
two separate subgraphs, which makes it impossible to perform
a causally meaningful backward trace.
To mitigate that problem, we use a data plane model that in-
cludes the network’s topology and related happens-before re-
lationships among activities. Our provenance model includes
a data-plane-based causal derivation in the relation PacketIn
wasDerivedFrom PacketOut to represent the causality.
Network identiﬁers Control plane objects generated from
data plane hosts pose a unique attribution challenge. Data
plane hosts can spoof their principal identities, or network
identiﬁers, relatively easily in SDN [28] as a result of net-
work protocols (e.g., the Address Resolution Protocol) that
do not provide authentication and SDN controller programs
that naïvely trust such information [53]. Ideally, each data
plane host would have its own principal identity, but that is
impossible if hosts can spoof their network identiﬁers.
To mitigate that problem, our provenance model offers two
features: edge ports as principal identities and network iden-
tiﬁer revisions. To enable those abilities, we model each edge
port3 as a principal identity, or Agent node; Figure 4b shows
an example. As we assume in our threat model (described in
3As opposed to an internal port that links a switch with another switch.
USENIX Association
30th USENIX Security Symposium    3189
DATAPLANECONTROLPLANESwitch s1Switch s2Switch s1Port 1App XApp YPacketOutPacketPacketInSouthboundAPISDN ControllerSwitch s2Port 11234NorthboundAPIEventListenerapp=App Xt=1PacketProcessorapp=App Yt=2Packet InMACsrc=h1switch=s2port=1t=2usedPacket OutMACsrc=h1switch=s1port=1t=1wasGeneratedBywasDerivedFromFigure 7: PICOSDN architecture overview with example workﬂow. 1: An app makes an API call. 2: PICOSDN’s API hooks
register the API call. 3: The provenance collector checks its internal state and makes changes based on the API call. 4: The
provenance serializer generates the relevant graph. 5: The ingester, cleaner, and topology augmenter prepare the graph. 6: The
tracer receives the graph. 7: The tracer answers causal analysis queries based on the graph.
G4 Activity Completeness. PICOSDN should observe and
record any apps, controller, or data plane activity relevant
to network activities to ensure that it serves as a control
plane reference monitor.
5.1 Runtime Phase
During the network’s execution, PICOSDN’s runtime phase
records control plane activities in its collector and transforms
them into a lightweight graph by using its serializer.
Collector The provenance collector consists of three com-
ponents: wrappers around event dispatches and packet pro-
cessors, hooks on API calls, and an internal state tracker.
We have instrumented wrappers around the SDN con-
troller’s event dispatcher and packet processor. The prove-
nance collector uses these wrappers to maintain knowledge
about which event listener or packet processor is currently han-
dling the dispatch or processing, respectively; this achieves
goal G1.
We have instrumented hooks on each of the SDN con-
troller’s API calls; this achieves goal G4. For a single-
threaded controller, the reconstruction of the sequence of
events, packets, and API calls is straightforward. However,
in modern multi-threaded controllers, we also need a concur-
rency model to correctly link such calls to the right events.
For event dispatching, we assume the following concurrency
model: a particular event, ε1, is processed sequentially by
each interested event listener (i.e., ε1 is processed by listener
l1, then by l2); different events, ε1 and ε2, may be processed
concurrently (i.e., ε1 is processed by listener l1 followed by
l2, while concurrently ε2 is processed by listener l3 followed
by l4). That is the model used by ONOS4, among other SDN
4ONOS maintains several event dispatch queues based on the event type,
and each queue is implemented in a separate thread. Given that listeners
process a particular event sequentially, ONOS’s event dispatcher sets a hard
controllers. It allows PICOSDN’s provenance collector to use
hooks to correctly determine whether a particular API call
should link the use or generation of control plane objects to
the event listener (or packet processor) in execution at that
time. Hooking the API calls and linking them with the event
and packet wrappers in this way not only permits a trans-
parent interposition over all app and data plane interactions
with the control plane, but also avoids the limitations of prior
work [55] that requires app instrumentation.
The provenance collector includes an internal state tracker
that maintains knowledge of current events and control plane
objects to detect when such objects change. The internal state
is necessary to keep track of ephemeral objects’ uniqueness
that would not necessarily be captured by raw logging alone.
(See § 8 for a discussion about internal state storage costs and
external provenance storage costs.)
Serializer Once the provenance collector has determined
the correct provenance based on context, the provenance seri-
alizer writes out a lightweight serialized graph of nodes and
edges.
Investigation Phase
5.2
At some later point in time, PICOSDN’s investigation phase
uses the lightweight serialized graph as a basis for analysis.
The ingester de-serializes the graph, the cleaner removes
unnecessary provenance, and the topology augmenter incor-
porates the data plane model. The tracer answers practitioner
queries. Each component is designed to be modular.
5.2.1
Ingester, Cleaner, and Topology Augmenter
The ingestor reads in the serialized graph. As most nodes
contain additional details, the graph ingestor de-serializes the
time limit for each event listener to avoid indeﬁnite halting.
3190    30th USENIX Security Symposium
USENIX Association
SDN ControllerController coreEvent Listeners & Packet ProcessorsApp 1App n…NB APICONTROL PLANEForwarding DevicesDATA PLANE……ONLINE OPERATIONSouthbound APIAPPLICATION PLANEData Plane HostsHooked MethodsData StoreSB APIHooked MethodsPICOSDN RUNTIME PHASEProvenance CollectorProvenance SerializerInternal StatePICOSDN INVESTIGATION PHASEIngesterCleanerTopologyAugmenterTracerCommon AncestryData Plane ModelBackward-ForwardActivity SummaryIdentifier EvolutionConfigurationCausal Analysis QueriesPICOSDN Inputs and Outputsfrom PractitionerOFFLINE OPERATION1234567Algorithm 1 Data Plane Model
Input: graph G, data plane topology states Dset, time window τw, headers
ﬁelds to match on H
Output: graph with data plane model G
Initialize: (V ,E) ← G
1: for each D ∈ Dset do
2:
τstart, epoch end τend
(N ,τstart ,τend ) ← D (cid:46) Data plane topology graph N , epoch start
(Nswitches,Nlinks) ← N
for each pin ∈ Vclass=PacketIn do
if τstart < pin.ts < τend then
(cid:46) Packet pin
(cid:46) Timestamp pin.ts
for each pout ∈ Vclass=PacketOut do
if (pout .switch, pin.switch) ∈ Nlinks then
if pout .H = pin.H then
if pout .ts < pin.ts and pin.ts− pout .ts ≤ τw then
V ← V ∪{(pin, pout )}
3:
4:
5:
6:
7:
8:
9:
10:
11: G ← (V ,E)
12: return G
if e is a wasRevisionOf edge then
Algorithm 2 Common Ancestry Trace
Input: graph G, evidence set N
Output: agent set Ag, activity set Ac, and entity set En
Initialize: (V ,E) ← G, Ag ← /0, Ac ← /0, En ← /0, A ← V
1: for each e ∈ E do
2:
E ← E \{e}
3:
4: for each n ∈ N do
5:
6:
7: for each a ∈ A do
8:
Ag ← Ag∪ a
9:
10:
Ac ← Ac∪ a
11:
12:
En ← En∪ a
13:
14: return (Ag,Ac,En)
An ← getAncestors((V ,E),n)
A ← A∩ An
else if a is an Activity node then
if a is an Agent node then
else
(cid:46) Remove non-causal edges
(cid:46) Evidence n (note: n ∈ V ,N ⊂ V )
(cid:46) Set of ancestor nodes An
(cid:46) Common ancestor set A
(cid:46) Common ancestor a
(cid:46) Ag ⊂ V , Ac ⊂ V , En ⊂ V
node’s dictionary into a set of key-value pairs. The cleaner
component can perform preprocessing to remove unneces-
sary or irrelevant nodes and edges. For instance, the cleaner
removes singleton nodes that are not connected to anything;
they may appear if objects are not being used. The cleaner
removes nodes that are not relevant to an investigation; for
instance, removing Statistic nodes about trafﬁc counts may
be useful if the investigation does not involve trafﬁc counts.
The topology augmenter adds edges into the graph (e.g., was-
DerivedFrom relations between PacketIns and PacketOuts) to
deﬁne the data plane model; doing so achieves goal G2.
PICOSDN’s data plane model algorithm is shown in Algo-
rithm 1. We assume that the data plane’s topology can vary
over time, and for each variation, we say that the state is an
epoch consisting of a topology that is valid between a start
time and an end time (lines 1–2). For each PacketIn, we want
to determine if it should link to a causally related PacketOut
(line 4). PICOSDN ﬁlters temporally based on the current
epoch (line 5), and it checks all PacketOuts during that epoch
(line 6). We consider a PacketOut to be causally related to the
PacketIn if all of the following conditions are met: 1) there is
a link between the outgoing and incoming switches (line 7);
2) the speciﬁed packet headers are the same for both packets
(line 8); 3) the PacketOut “happened before” the PacketIn
(line 9); and 4) the timestamp differences between the Pack-
etOut and PacketIn are within a speciﬁc threshold (line 9).
As PICOSDN is modular, Algorithm 1’s data plane model
can be replaced as needed. For instance, header space analy-
sis [30] uses functional transformations to model how packets
are transformed across the data plane (e.g., packet modiﬁ-
cations), and P4 [7] proposes a programmable data plane.
Practitioners can write their own data plane model compo-
nents that take those transformations into account.
Algorithm 3 Iterative Backward-Forward Trace
Input: graph G, evidence n, root r
Output: affected difference function ∆ : V → P(V )
Initialize: (V ,E) ← G; ∆(i) ← /0,∀i ∈ V
1: for each e ∈ E do
if e is a wasRevisionOf edge then
2:
E ← E \{e}
3:
4: An ← getAncestors((V ,E),n)
5: Dr ← getDescendants((V ,E),r)
6: Vintermediate ← An ∩ Dr
7: for each vi ∈ Vintermediate do
8:
9: return (Vintermediate,∆)
∆(i) ← Dr \ getDescendants((V ,E),vi)
(cid:46) Remove non-causal edges
(cid:46) Evidence’s ancestor set An
(cid:46) Root’s descendant set Dr
5.2.2 Tracer
After the graph is prepared, the tracer component answers
investigative queries. PICOSDN provides facilities to answer
queries related to root cause analysis, network activity summa-
rization, and network state evolution; these facilities achieve
goal G3. We now describe each kind of query and under what
scenarios a practitioner would want to use each kind.
As G is a DAG, we assume the use of standard graph func-
tions in Algorithms 2–5 that can determine the ancestor and
descendant nodes (i.e., progeny) of a given node n, denoted by
getAncestors(G,n) and getDescendants(G,n), respectively.
Root cause analysis After an attack, a practitioner wishes
to investigate the attack’s causes so as to determine what
changes should be made to prevent such attacks from reoccur-
ring. We assume that a practitioner has evidence of incorrect
behavior, wants to ﬁnd common causes, and wants to explore
whether other evidence of incorrect behavior also exists. PI-
COSDN provides two interrelated algorithms to do achieve
these goals: common ancestry tracing (Algorithm 2) and
backward-forward tracing (Algorithm 3). Practitioners can
iteratively use these tools to determine root causes efﬁciently.
Algorithm 2 shows the common ancestry tracing. We as-
sume that our practitioner can pinpoint evidence of incorrect
USENIX Association
30th USENIX Security Symposium    3191
(cid:46) Remove non-causal edges
if e is a wasRevisionOf edge then
Algorithm 4 Network Activity Summarization
Input: graph G
Output: set of (activity a, ﬂow rule fout, packet pin, data plane packets Pin)
Initialize: (V ,E) ← G, S ← /0
1: for each e ∈ E do
2:
E ← E \{e}
3:
4: for each a ∈ Vclass=Activity do
5:
6:
7:
8:
9:
10:
if (cid:104)a →(cid:0)v ∈ Vclass(cid:54)=Activity or e ∈ E(cid:1)∗ → p ∈ Vclass=PacketIn(cid:105) back-
if (cid:104) f ∈ Vclass=FlowRule →(cid:0)v ∈ Vclass(cid:54)=Activity or e ∈ E(cid:1)∗ → a(cid:105) back-
fout ← null, pin ← null, Pin ← null
Pin ← getAncestors((V ,E),a)
for each p ∈ Pin do
if p /∈ Vclass=PacketIn then
ward trace path exists then
Pin ← Pin \{p}
ward trace path exists then
S ← S∪{(a, fout , pin,Pin)}
13:
14:
15: return S
11:
12:
pin ← p
fout ← f
behavior, such as a set of packets or ﬂow rules that appear
suspicious. Our practitioner’s goal is to see if such evidence
has anything in common with past history. PICOSDN starts
by discarding non-causal edges in the graph (lines 1–3). Then,
for each piece of evidence, PICOSDN computes its set of
ancestor nodes and takes the intersection of that ancestry with
the ancestries of all previous pieces of evidence (lines 4–6).
Once all the pieces of evidence have been examined, the set of