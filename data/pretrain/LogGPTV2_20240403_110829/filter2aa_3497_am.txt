big endian 32-bit integer, which is the length, then the 32-bit checksum, and finally the
data as bytes before calling a function to convert that byte array to a DataFrame. (A DataFrame is
an object to contain network packets; you can convert a byte array or a string to a frame
depending on what you need.)
The WriteData() function at ➌ does the reverse of ReadData(). It uses the ToArray() method
Technet24
||||||||||||||||||||
||||||||||||||||||||
on the incoming DataFrame to convert the packet to bytes for writing. Once we have the byte
array, we can recalculate the checksum and the length, and then write it all back to the
DataWriter class. At ➍, we implement the various functions to read and write data from the
inbound and outbound streams.
Put together all the different scripts for network proxy and parsing and start a client
connection through the proxy, and all nonessential information, such as lengths and
checksums, should be removed from the data. As an added bonus, if you modify data inside
the proxy, the sent packet will have the correct checksum and length to match your
modifications.
Changing Protocol Behavior
Protocols often include a number of optional components, such as encryption or
compression. Unfortunately, it’s not easy to determine how that encryption or
compression is implemented without doing a lot of reverse engineering. For basic analysis,
it would be nice to be able to simply remove the component. Also, if the encryption or
compression is optional, the protocol will almost certainly indicate support for it while
negotiating the initial connection. So, if we can modify the traffic, we might be able to
change that support setting and disable that additional feature. Although this is a trivial
example, it demonstrates the power of using a proxy instead of passive analysis with a tool
like Wireshark. We can modify the connection to make analysis easier.
For example, consider the chat application. One of its optional features is XOR
encryption (although see Chapter 7 on why it’s not really encryption). To enable this
feature, you would pass the --xor parameter to the client. Listing 5-22 compares the first
couple of packets for the connection without the XOR parameter and then with the XOR
parameter.
OUTBOUND XOR   :    00 05 75 73 65 72 32 04 4F 4E 59 58 01     - ..user2.ONYX.
OUTBOUND NO XOR:    00 05 75 73 65 72 32 04 4F 4E 59 58 00     - ..user2.ONYX.
INBOUND XOR   :     01 E7                                      - ..
INBOUND NO XOR:     01 00                                      - ..
Listing 5-22: Example packets with and without XOR encryption enabled
I’ve highlighted in bold two differences in Listing 5-22. Let’s draw some conclusions
from this example. In the outbound packet (which is command 0 based on the first byte),
the final byte is a 1 when XOR is enabled but 0x00 when it’s not enabled. My guess would
be that this flag indicates that the client supports XOR encryption. For inbound traffic, the
final byte of the first packet (command 1 in this case) is 0xE7 when XOR is enabled and
0x00 when it’s not. My guess would be that this is a key for the XOR encryption.
In fact, if you look at the client console when you’re enabling XOR encryption, you’ll
see the line ReKeying connection to key 0xE7, which indicates it is indeed the key. Although the
negotiation is valid traffic, if you now try to send a message with the client through the
||||||||||||||||||||
||||||||||||||||||||
proxy, the connection will no longer work and may even be disconnected. The connection
stops working because the proxy will try to parse fields, such as the length of the packet,
from the connection but will get invalid values. For example, when reading a length, such
as 0x10, the proxy will instead read 0x10 XOR 0xE7, which is 0xF7. Because there are no
0xF7 bytes on the network connection, it will hang. The short explanation is that to
continue the analysis in this situation, we need to do something about the XOR.
While implementing the code to de-XOR the traffic when we read it and re-XOR it
again when we write it wouldn’t be especially difficult, it might not be so simple to do if
this feature were implemented to support some proprietary compression scheme.
Therefore, we’ll simply disable XOR encryption in our proxy irrespective of the client’s
setting. To do so, we read the first packet in the connection and ensure that the final byte
is set to 0. When we forward that packet onward, the server will not enable XOR and will
return the value of 0 as the key. Because 0 is a NO-OP in XOR encryption (as in A XOR 0
= A), this technique will effectively disable the XOR.
Change the ReadOutbound() method in the parser to the code in Listing 5-23 to disable the
XOR encryption.
protected override DataFrame ReadOutbound(DataReader reader) {
  DataFrame frame = ReadData(reader);
  // Convert frame back to bytes.
  byte[] data = frame.ToArray();
  if (data[0] == 0) {
    Console.WriteLine("Disabling XOR Encryption");
    data[data.Length - 1] = 0;
    frame = data.ToDataFrame();
  }
  return frame;
}
Listing 5-23: Disable XOR encryption
If you now create a connection through the proxy, you’ll find that regardless of whether
the XOR setting is enabled or not, the client will not be able to enable XOR.
Final Words
In this chapter, you learned how to perform basic protocol analysis on an unknown
protocol using passive and active capture techniques. We started by doing basic protocol
analysis using Wireshark to capture example traffic. Then, through manual inspection and
a simple Python script, we were able to understand some parts of an example chat
protocol.
We discovered in the initial analysis that we were able to implement a basic Lua
dissector for Wireshark to extract protocol information and display it directly in the
Wireshark GUI. Using Lua is ideal for prototyping protocol analysis tools in Wireshark.
Finally, we implemented a man-in-the-middle proxy to analyze the protocol. Proxying
the traffic allows demonstration of a few new analysis techniques, such as modifying
Technet24
||||||||||||||||||||
||||||||||||||||||||
protocol traffic to disable protocol features (such as encryption) that might hinder the
analysis of the protocol using purely passive techniques.
The technique you choose will depend on many factors, such as the difficulty of
capturing the network traffic and the complexity of the protocol. You’ll want to apply the
most appropriate combination of techniques to fully analyze an unknown protocol.
||||||||||||||||||||
||||||||||||||||||||
6
APPLICATION REVERSE ENGINEERING
If you can analyze an entire network protocol just by looking at the transmitted data, then
your analysis is quite simple. But that’s not always possible with some protocols, especially
those that use custom encryption or compression schemes. However, if you can get the
executables for the client or server, you can use binary reverse engineering (RE) to
determine how the protocol operates and search for vulnerabilities as well.
The two main kinds of reverse engineering are static and dynamic. Static reverse
engineering is the process of disassembling a compiled executable into native machine
code and using that code to understand how the executable works. Dynamic reverse
engineering involves executing an application and then using tools, such as debuggers and
function monitors, to inspect the application’s runtime operation.
In this chapter, I’ll walk you through the basics of taking apart executables to identify
and understand the code areas responsible for network communication.
I’ll focus on the Windows platform first, because you’re more likely to find applications
without source code on Windows than you are on Linux or macOS. Then, I’ll cover the
differences between platforms in more detail and give you some tips and tricks for working
on alternative platforms; however, most of the skills you’ll learn will be applicable on all
platforms. As you read, keep in mind that it takes time to become good reverse engineer,
and I can’t possibly cover the broad topic of reverse engineering in one chapter.
Before we delve into reverse engineering, I’ll discuss how developers create executable
files and then provide some details about the omnipresent x86 computer architecture.
Once you understand the basics of x86 architecture and how it represents instructions,
you’ll know what to look for when you’re reverse engineering code.
Finally, I’ll explain some general operating system principles, including how the
operating system implements networking functionality. Armed with this knowledge, you
should be able to track down and analyze network applications.
Let’s start with background information on how programs execute on a modern
operating system and examine the principles of compilers and interpreters.
Compilers, Interpreters, and Assemblers
Most applications are written in a higher-level programming language, such as C/C++, C#,
Java, or one of the many scripting languages. When an application is developed, the raw
language is its source code. Unfortunately, computers don’t understand source code, so the
high-level language must be converted into machine code (the native instructions the
Technet24
||||||||||||||||||||
||||||||||||||||||||
computer’s processor executes) by interpreting or compiling the source code.
The two common ways of developing and executing programs is by interpreting the
original source code or by compiling a program to native code. The way a program
executes determines how we reverse engineer it, so let’s look at these two distinct methods
of execution to get a better idea of how they work.
Interpreted Languages
Interpreted languages, such as Python and Ruby, are sometimes called scripting languages,
because their applications are commonly run from short scripts written as text files.
Interpreted languages are dynamic and speed up development time. But interpreters
execute programs more slowly than code that has been converted to machine code, which
the computer understands directly. To convert source code to a more native
representation, the programming language can instead be compiled.
Compiled Languages
Compiled programming languages use a compiler to parse the source code and generate
machine code, typically by generating an intermediate language first. For native code
generation, usually an assembly language specific to the CPU on which the application will
run (such as 32- or 64-bit assembly) is used. The language is a human-readable and
understandable form of the underlying processor’s instruction set. The assembly language
is then converted to machine code using an assembler. For example, Figure 6-1 shows how
a C compiler works.
||||||||||||||||||||
||||||||||||||||||||
Figure 6-1: The C language compilation process
To reverse a native binary to the original source code, you need to reverse the
compilation using a process called decompilation. Unfortunately, decompiling machine code
is quite difficult, so reverse engineers typically reverse just the assembly process using a
process called disassembly.
Static vs. Dynamic Linking
With extremely simple programs, the compilation process might be all that is needed to
produce a working executable. But in most applications, a lot of code is imported into the
final executable from external libraries by linking—a process that uses a linker program
after compilation. The linker takes the application-specific machine code generated by the
compiler, along with any necessary external libraries used by the application, and embeds
Technet24
||||||||||||||||||||
||||||||||||||||||||
everything in a final executable by statically linking any external libraries. This static linking
process produces a single, self-contained executable that doesn’t depend on the original
libraries.
Because certain processes might be handled in very different ways on different
operating systems, static linking all code into one big binary might not be a good idea
because the OS-specific implementation could change. For example, writing to a file on
disk might have widely different operating system calls on Windows than it does on Linux.
Therefore, compilers commonly link an executable to operating system–specific libraries
by dynamic linking: instead of embedding the machine code in the final executable, the
compiler stores only a reference to the dynamic library and the required function. The
operating system must resolve the linked references when the application runs.
The x86 Architecture
Before getting into the methods of reverse engineering, you’ll need some understanding of
the basics of the x86 computer architecture. For a computer architecture that is over 30
years old, x86 is surprisingly persistent. It’s used in the majority of desktop and laptop
computers available today. Although the PC has been the traditional home of the x86
architecture, it has found its way into Mac1 computers, game consoles, and even
smartphones.
The original x86 architecture was released by Intel in 1978 with the 8086 CPU. Over
the years, Intel and other manufacturers (such as AMD) have improved its performance
massively, moving from supporting 16-bit operations to 32-bit and now 64-bit operations.
The modern architecture has barely anything in common with the original 8086, other
than processor instructions and programming idioms. Because of its lengthy history, the
x86 architecture is very complex. We’ll first look at how the x86 executes machine code,
and then examine its CPU registers and the methods used to determine the order of
execution.
The Instruction Set Architecture
When discussing how a CPU executes machine code, it’s common to talk about the
instruction set architecture (ISA). The ISA defines how the machine code works and how it
interacts with the CPU and the rest of the computer. A working knowledge of the ISA is
crucial for effective reverse engineering.
The ISA defines the set of machine language instructions available to a program; each
individual machine language instruction is represented by a mnemonic instruction. The
mnemonics name each instruction and determine how its parameters, or operands, are
represented. Table 6-1 lists the mnemonics of some of the most common x86 instructions.
(I’ll cover many of these instructions in greater detail in the following sections.)
||||||||||||||||||||
||||||||||||||||||||
Table 6-1: Common x86 Instruction Mnemonics
Instruction
Description
MOV destination,
source
Moves a value from source to destination
ADD destination,
value
Adds an integer value to the destination
SUB destination,
value
Subtracts an integer value from a destination
CALL address
Calls the subroutine at the specified address
JMP address
Jumps unconditionally to the specified address
RET
Returns from a previous subroutine
RETN size
Returns from a previous subroutine and then increments the stack by
size
Jcc address
Jumps to the specified address if the condition indicated by cc is true
PUSH value
Pushes a value onto the current stack and decrements the stack pointer
POP destination
Pops the top of the stack into the destination and increments the stack
pointer
CMP valuea, valueb
Compares valuea and valueb and sets the appropriate flags
TEST valuea, valueb
Performs a bitwise AND on valuea and valueb and sets the appropriate
flags
AND destination,
value
Performs a bitwise AND on the destination with the value
OR destination,
value
Performs a bitwise OR on the destination with the value
XOR destination,
value
Performs a bitwise Exclusive OR on the destination with the value
SHL destination, N
Shifts the destination to the left by N bits (with left being higher bits)
SHR destination, N
Shifts the destination to the right by N bits (with right being lower bits)
INC destination
Increments destination by 1
DEC destination
Decrements destination by 1
These mnemonic instructions take one of three forms depending on how many
operands the instruction takes. Table 6-2 shows the three different forms of operands.
Technet24
||||||||||||||||||||
||||||||||||||||||||
Table 6-2: Intel Mnemonic Forms
Number of operands
Form
Examples
0
NAME
POP, RET
1
NAME input
PUSH 1; CALL func
2
NAME output, input
MOV EAX, EBX; ADD EDI, 1
The two common ways to represent x86 instructions in assembly are Intel and AT&T
syntax. Intel syntax, originally developed by the Intel Corporation, is the syntax I use
throughout this chapter. AT&T syntax is used in many development tools on Unix-like
systems. The syntaxes differ in a few ways, such as the order in which operands are given.
For example, the instruction to add 1 to the value stored in the EAX register would look
like this in Intel syntax: ADD EAX, 1 and like this in AT&T Syntax: addl $1, %eax.
CPU Registers
The CPU has a number of registers for very fast, temporary storage of the current state of
execution. In x86, each register is referred to by a two- or three-character label. Figure 6-2
shows the main registers for a 32-bit x86 processor. It’s essential to understand the many
types of registers the processor supports because each serves different purposes and is
necessary for understanding how the instructions operate.
Figure 6-2: The main 32-bit x86 registers
The x86’s registers are split into four main categories: general purpose, memory index,
control, and selector.
||||||||||||||||||||
||||||||||||||||||||
General Purpose Registers
The general purpose registers (EAX, EBX, ECX, and EDX in Figure 6-2) are temporary
stores for nonspecific values of computation, such as the results of addition or subtraction.
The general purpose registers are 32 bits in size, although instructions can access them in 16-
and 8-bit versions using a simple naming convention: for example, a 16-bit version of the
EAX register is accessed as AX, and the 8-bit versions are AH and AL. Figure 6-3 shows
the organization of the EAX register.
Figure 6-3: EAX general purpose register with small register components
Memory Index Registers
The memory index registers (ESI, EDI, ESP, EBP, EIP) are mostly general purpose except
for the ESP and EIP registers. The ESP register is used by the PUSH and POP
instructions, as well as during subroutine calls to indicate the current memory location of
the base of a stack.
Although you can utilize the ESP register for purposes other than indexing into the
stack, it’s usually unwise to do so because it might cause memory corruption or unexpected
behavior. The reason is that some instructions implicitly rely on the value of the register.
On the other hand, the EIP register cannot be directly accessed as a general purpose
register because it indicates the next address in memory where an instruction will be read
from.
The only way to change the value of the EIP register is by using a control instruction,
such as CALL, JMP, or RET. For this discussion, the important control register is EFLAGS.
EFLAGS contains a variety of Boolean flags that indicate the results of instruction
execution, such as whether the last operation resulted in the value 0. These Boolean flags
implement conditional branches on the x86 processor. For example, if you subtract two
values and the result is 0, the Zero flag in the EFLAGS register will be set to 1, and flags
that do not apply will be set to 0.
The EFLAGS register also contains important system flags, such as whether interrupts
are enabled. Not all instructions affect the value of EFLAGS. Table 6-3 lists the most
important flag values, including the flag’s bit position, its common name, and a brief
description.
Technet24
||||||||||||||||||||