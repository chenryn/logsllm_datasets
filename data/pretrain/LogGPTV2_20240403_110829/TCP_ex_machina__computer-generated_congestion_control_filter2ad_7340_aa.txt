title:TCP ex machina: computer-generated congestion control
author:Keith Winstein and
Hari Balakrishnan
TCP ex Machina: Computer-Generated Congestion Control
Keith Winstein and Hari Balakrishnan
Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology, Cambridge, Mass.
{keithw, hari}@mit.edu
ABSTRACT
This paper describes a new approach to end-to-end congestion con-
trol on a multi-user network. Rather than manually formulate each
endpoint’s reaction to congestion signals, as in traditional protocols,
we developed a program called Remy that generates congestion-
control algorithms to run at the endpoints.
In this approach,
the protocol designer speciﬁes their prior
knowledge or assumptions about the network and an objective that
the algorithm will try to achieve, e.g., high throughput and low
queueing delay. Remy then produces a distributed algorithm—the
control rules for the independent endpoints—that tries to achieve
this objective.
In simulations with ns-2, Remy-generated algorithms outper-
formed human-designed end-to-end techniques, including TCP Cu-
bic, Compound, and Vegas. In many cases, Remy’s algorithms also
outperformed methods that require intrusive in-network changes,
including XCP and Cubic-over-sfqCoDel (stochastic fair queueing
with CoDel for active queue management).
Remy can generate algorithms both for networks where some
parameters are known tightly a priori, e.g. datacenters, and for net-
works where prior knowledge is less precise, such as cellular net-
works. We characterize the sensitivity of the resulting performance
to the speciﬁcity of the prior knowledge, and the consequences
when real-world conditions contradict the assumptions supplied at
design-time.
CATEGORIES AND SUBJECT DESCRIPTORS
C.2.1 [Computer-Communication Networks]: Network Archi-
tecture and Design — Network communications
KEYWORDS
congestion control, computer-generated algorithms
1.
INTRODUCTION
Is it possible for a computer to “discover” the right rules for con-
gestion control in heterogeneous and dynamic networks? Should
computers, rather than humans, be tasked with developing conges-
tion control methods? And just how well can we make computers
perform this task?
Permission to make digital or hard copies of all or part of this work for per-
sonal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than the author(s) must be honored. Abstract-
ing with credit is permitted. To copy otherwise, or republish, to post on
servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
Copyright is held by the owner/author(s). Publication rights licensed to
ACM. 978-1-4503-2056-6/13/08 ...$15.00.
Figure 1: Remy designs congestion-control schemes automati-
cally to achieve desired outcomes. The algorithms it produces
may replace the congestion-control module of a TCP implemen-
tation, and ﬁt into a network library or kernel module that
implements congestion control (DCCP, SCTP, the congestion
manager, application-layer transmission control libraries, ns-2
modules, etc.).
We investigated these questions and found that computers can de-
sign schemes that in some cases surpass the best human-designed
methods to date, when supplied with the appropriate criteria by
which to judge a congestion-control algorithm. We attempt to probe
the limits of these machine-generated protocols, and discuss how
this style of transport-layer protocol design can give more freedom
to network architects and link-layer designers.
Congestion control, a fundamental problem in multi-user com-
puter networks, addresses the question: when should an endpoint
transmit each packet of data? An ideal scheme would transmit a
packet whenever capacity to carry the packet was available, but be-
cause there are many concurrent senders and the network experi-
ences variable delays, this question isn’t an easy one to answer. On
the Internet, the past thirty years have seen a number of innova-
tive and inﬂuential answers to this question, with solutions embed-
ded at the endpoints (mainly in TCP) aided occasionally by queue
management and scheduling algorithms in bottleneck routers that
provide signals to the endpoints.
This area has continued to draw research and engineering effort
because new link technologies and subnetworks have proliferated
and evolved. For example, the past few years have seen an increase
in wireless networks with variable bottleneck rates; datacenter net-
works with high rates, short delays, and correlations in offered load;
paths with excessive buffering (now called “bufferbloat”); cellu-
lar wireless networks with highly variable, self-inﬂicted packet de-
lays; links with non-congestive stochastic loss; and networks with
Prior assumptionsabout networkTraﬃc modelObjective functionRemyExistingTCPns-2TCPRemyCC123large bandwidth-delay products. In these conditions, the classical
congestion-control methods embedded in TCP can perform poorly,
as many papers have shown (§2).
Without the ability to adapt its congestion-control algorithms to
new scenarios, TCP’s inﬂexibility constrains architectural evolu-
tion, as we noted in an earlier position paper [43]. Subnetworks
and link layers are typically evaluated based on how well TCP per-
forms over them. This scorecard can lead to perverse behavior,
because TCP’s network model is limited. For example, because
TCP assumes that packet losses are due to congestion and reduces
its transmission rate in response, some subnetwork designers have
worked hard to hide losses. This often simply adds intolerably long
packet delays. One may argue that such designs are misguided, but
the difﬁculties presented by “too-reliable” link layers have been a
perennial challenge for 25 years [12] and show no signs of abating.
With the rise of widespread cellular connectivity, these behaviors
are increasingly common and deeply embedded in deployed infras-
tructure.
The designers of a new subnetwork may well ask what they
should do to make TCP perform well. This question is surpris-
ingly hard to answer, because the so-called teleology of TCP is un-
known: exactly what objective does TCP congestion control opti-
mize? TCP’s dynamic behavior, when competing ﬂows enter and
leave the network, remains challenging to explain [7]. In practice,
the need to “make TCP perform well” is given as a number of loose
guidelines, such as IETF RFC 3819 [23], which contains dozens of
pages of qualitative best current practice. The challenging and sub-
tle nature of this area means that the potential of new subnetworks
and network architectures is often not realized.
Design overview
How should we design network protocols that free subnetworks and
links to evolve freely, ensuring that the endpoints will adapt prop-
erly no matter what the lower layers do? We believe that the best
way to approach this question is to take the design of speciﬁc algo-
rithmic mechanisms out of the hands of human designers (no matter
how sophisticated!), and make the end-to-end algorithm be a func-
tion of the desired overall behavior.
We start by explicitly stating an objective for congestion control;
for example, given an unknown number of users, we may optimize
some function of the per-user throughput and packet delay, or a
summary statistic such as average ﬂow completion time. Then, in-
stead of writing down rules by hand for the endpoints to follow, we
start from the desired objective and work backwards in three steps:
1. First, model the protocol’s prior assumptions about the net-
work; i.e., the “design range” of operation. This model may
be different, and have different amounts of uncertainty, for
a protocol that will be used exclusively within a data center,
compared with one intended to be used over a wireless link or
one for the broader Internet. A typical model speciﬁes upper
and lower limits on the bottleneck link speeds, non-queueing
delays, queue sizes, and degrees of multiplexing.
2. Second, deﬁne a trafﬁc model for the offered load given to
endpoints. This may characterize typical Web trafﬁc, video
conferencing, batch processing, or some mixture of these. It
may be synthetic or based on empirical measurements.
3. Third, use the modeled network scenarios and trafﬁc to de-
sign a congestion-control algorithm that can later be executed
on endpoints.
We have developed an optimization tool called Remy that takes
these models as input, and designs a congestion-control algorithm
that tries to maximize the total expected value of the objective func-
tion, measured over the set of network and trafﬁc models. The re-
sulting pre-calculated, optimized algorithm is then run on actual
endpoints; no further learning happens after the ofﬂine optimiza-
tion. The optimized algorithm is run as part of an existing TCP
sender implementation, or within any congestion-control module.
No receiver changes are necessary (as of now).
Summary of results
We have implemented Remy. Running on a 48-core server at MIT,
Remy generally takes a few hours of wall-clock time (one or two
CPU-weeks) to generate congestion-control algorithms ofﬂine that
work on a wide range of network conditions.
Our main results from several simulation experiments with Remy
are as follows:
1. For networks broadly consistent with the assumptions pro-
vided to Remy at design time, the machine-generated algo-
rithms dramatically outperform existing methods, including
TCP Cubic, Compound TCP, and TCP Vegas.
2. Comparing Remy’s algorithms with schemes that require
modiﬁcations to network gateways, including Cubic-over-
sfqCoDel and XCP, Remy generally matched or surpassed
these schemes, despite being entirely end-to-end.
3. We measured the tradeoffs that come from speciﬁcity in the
assumptions supplied to Remy at design time. As expected,
more-speciﬁc prior information turned out to be helpful when
it was correct, but harmful when wrong. We found that Re-
myCC schemes performed well even when designed for an
order-of-magnitude variation in the values of the underlying
network parameters.
On a simulated 15 Mbps ﬁxed-rate link with eight senders con-
tending and an RTT of 150 ms, a computer-generated congestion-
control algorithm achieved the following improvements in median
throughput and reductions in median queueing delay over these ex-
isting protocols:
Median speedup Median delay reduction
In a trace-driven simulation of the Verizon LTE downlink with
four senders contending, the same computer-generated protocol
achieved these speedups and reductions in median queueing delay:
Median speedup Median delay reduction
The source code for Remy, our ns-2 models, and the algorithms
that Remy designed are available from http://web.mit.edu/remy.
2. RELATED WORK
Starting with Ramakrishnan and Jain’s DECBit scheme [36] and
Jacobson’s TCP Tahoe (and Reno) algorithms [21], congestion con-
trol over heterogeneous packet-switched networks has been an ac-
tive area of research. End-to-end algorithms typically compute a
congestion window (or, in some cases, a transmission rate) as well
Protocol
Compound
NewReno
Cubic
Vegas
Cubic/sfqCoDel
XCP
Protocol
Compound
NewReno
Cubic
Vegas
Cubic/sfqCoDel
XCP
2.1×
2.6×
1.7×
3.1×
1.4×
1.4×
1.3×
1.5×
1.2×
2.2×
1.3×
1.7×
2.7×
2.2×
3.4×
1.2×
7.8×
4.3×
1.3×
1.2×
1.7×
0.44× ↓
1.3×
0.78× ↓
124as the round-trip time (RTT) using the stream of acknowledgments
(ACKs) arriving from the receiver. In response to congestion, in-
ferred from packet loss or, in some cases, rising delays, the sender
reduces its window; conversely, when no congestion is perceived,
the sender increases its window.
There are many different ways to vary the window. Chiu and
Jain [10] showed that among linear methods, additive increase /
multiplicative decrease (AIMD) converges to high utilization and
a fair allocation of throughputs, under some simplifying assump-
tions (long-running connections with synchronized and instanta-
neous feedback). Our work relaxes these assumptions to handle
ﬂows that enter and leave the network, and users who care about
latency as well as throughput. Remy’s algorithms are not necessar-
ily linear, and can use both a window and a rate pacer to regulate
transmissions.
In this paper, we compare Remy’s generated algorithms with
several end-to-end schemes, including NewReno [19], Vegas [9],
Compound TCP [39], Cubic [18], and DCTCP for datacenters [2].
NewReno has the same congestion-control strategy as Reno—slow
start at the beginning, on a timeout, or after an idle period of about
one retransmission timeout (RTO), additive increase every RTT
when there is no congestion, and a one-half reduction in the win-
dow on receiving three duplicate ACKs (signaling packet loss). We
compare against NewReno rather than Reno because NewReno’s
loss recovery is better.
Brakmo and Peterson’s Vegas is a delay-based algorithm, mo-
tivated by the insight from Jain’s CARD scheme [22] and Wang
and Crowcroft’s DUAL scheme [41] that increasing RTTs may be
a congestion signal. Vegas computes a BaseRTT, deﬁned as the
RTT in the absence of congestion, and usually estimated as the ﬁrst
RTT on the connection before the windows grow. The expected
throughput of the connection is the ratio of the current window size
and BaseRTT, if there is no congestion; Vegas compares the ac-
tual sending rate, and considers the difference, diff, between the
expected and actual rates. Depending on this difference, Vegas ei-