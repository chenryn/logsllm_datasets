在这个向量加法示例中，要把向量A和B相加赋值给向量C，三个向
量的长度都是50 000。启动算核的代码如下。
// Launch the Vector Add CUDA Kernel
int threadsPerBlock = 256;
int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;
printf("CUDA kernel launch with %d blocks of %d threads\n", 
                blocksPerGrid, threadsPerBlock);
vectorAdd>>(d_A, d_B, d_C, numElements);
1行代码是把每个线程块的大小定义为256，因此需要的线程块个数
为：50000/256 =195.3125，再向上取整即196个。
可以看到在图9-12中，blockDim= {x = 256, y = 1, z =
1}，gridDim = {x = 196, y = 1, z = 1}。通过blockIdx和
threadIdx可以知道当前线程是块{x = 28, y = 0, z = 0}里的{x =
0, y = 0, z = 0}号线程。考虑到读者可能经常需要把三维形式的坐
标值换算到一维形式，CUDA还可定义了两个伪变量@flatBlockIdx和
@flatThreadIdx。二者都是长整型，分别代表平坦化的线程组编号和线
程编号。
9.5.5 Warp
GPU编程的一个基本特点是大规模并行，让GPU内数以千计的微处
理器同时转向要处理的数据，每个线程处理一个数据元素。
一方面是大量需要执行的任务，另一方面是很多等待任务的微处理
器。如何让这么多微处理器有条不紊地把所有任务都执行完毕呢？这是
个复杂的话题，详细讨论它超出了本书的范围。这里只能介绍其中的一
方面：调度粒度。
与军队里把士兵分成一个个小的战斗单位类似，在CUDA中，也把
微处理器分成一个个小组。每个组的大小是一样的。迄今为止，组的规
模总是32个。CUDA给这个组取了个特别的名字：Warp。
Warp是GPU调度的基本单位。这意味着，当GPU调度硬件资源
时，一次分派的执行单元至少是32个。如果每个线程块的大小不足32
个，那么也会分配32个，多余的硬件单元处于闲置状态。
Warp一词来源于历史悠久的纺织技术。纺织技术的核心是织机
（Loom）。经历数千年的发展，世界各地的人们发明了很多种织机。
虽然种类很多，但是大多数织机的一个基本原理都是让经线和纬线交织
在一起。通常的做法是首先部署好一组经线，然后使用某种装置把经线
分开，形成一个V形的开口，再把系着纬线的梭子投过经线的开口，而
后调整经线的上下布局，再投梭子穿纬线，如此往复。
图 9-13是作者在成都蜀锦博物馆拍摄的一张织机照片（局部）。这
个织机需要两人配合操作，一人坐在高处，负责根据要编织的花样操作
综框（图中左侧矩形装置），使经线开口。另一人坐在图中右侧，负责
穿梭子。地下放了一面镜子，便于观察另一面的情况。在纺织领域，
Warp就是指经线，也就是图9-13中从左到右平行分布的那一组丝线。
在纺织中，经线的数量决定了织物的幅度，也可以认为经线的数量
决定了并行操作的并行度。在CUDA中，使用Warp来代表同时操作的一
批线程，也代表并行度。
图9-13 历史悠久的并行技术
9.5.6 显式并行
这里把CUDA这样明确指定并行方式的做法称为显式并行（explicit
parallel）。这是与隐式并行（implicit parallel）相对而言的。
隐式并行的例子有很多，最著名的莫过于CPU中流行的乱序执行技
术。乱序执行的基本特点是在CPU内部把本来串行的指令流同时发射到
多个硬件流水线来并行执行。为什么叫隐式并行呢？因为对程序员来
说，根本不知道自己的代码是如何并行的。并行的方式是隐藏在CPU内
部的。
简单来说，显式并行是在编程接口中就明确并行的方式。而隐式并
行是在没有明确并行接口的前提下，暗中做并行。
2018年爆出的幽灵（Spectre）和熔断（Meltdown）两大安全漏洞都
与隐式并行密切相关，这给多年来以乱序执行技术为荣耀的英特尔公司
当头两棒。希望这个公司能够提高警惕。
 老雷评点 
乱序执行有很多个名字，早期的手册中常用的是投机执行，
安全漏洞爆出后，用得更多的是预测执行，或许担心“投机”之名
引发更多普通用户的不满。回望历史，如果在当年花大量精力做
投机执行时就大张旗鼓地做显式并行，那么何至于今日之被动局
面。如此说来，投机之名，何其精恰也。
9.6 异常和陷阱
与CPU的异常机制类似，Nvidia GPU也有异常机制，在官方文档中
大多称为陷阱（trap）。因为它与调试关系密切，所以本节专门介绍
Nvidia GPU中与陷阱机制密切相关的内容。
9.6.1 陷阱指令
从PTX指令集的1.0版本开始，就有一条专门用于执行陷阱操作的指
令，名字就叫trap。
PTX文档对这条指令的描述非常简略，惜字如金。只有简洁的一句
话：Abort execution and generate an interrupt to the host CPU（中止执
行，并向主CPU产生一个中断）。
如果在CUDA程序中通过嵌入式汇编代码插入一条trap指令，那么
会发现它的效果居然与断点指令完全相同，查看反汇编代码，对应的硬
件指令竟然也完全一样。
    asm("trap;");
0x000cf950  [0304] tmp15:  
0x000cf950  [0307] trap;  
0x000cf950                       BPT.TRAP 0x1;
如此看来，至少对于作者试验的软硬件版本，陷阱指令与断点指令
可以实现等价的效果。陷阱指令的默认功能就是触发断点。不过，这只
是陷阱指令的一种用法，它应该可以产生不同类型的陷阱。或许现在就
是这样的用法，即使不这样使用，将来应该也会这样使用。
9.6.2 陷阱后缀
某些PTX指令支持.trap后缀，意思是遇到意外情况时执行陷阱操
作，其作用与C++中的throw指示符很类似。
例如，在下面的写平面（surface store）指令中，.trap后缀就告诉
GPU，如果遇到越界情况，就执行陷阱操作。
sust.b.3d.v2.b64.trap [surf_A, {x,y,z,w}], {r1,r2};
所谓执行陷阱操作，其实就是GPU停止执行目前的指令流，跳转到
专门的陷阱处理程序。
9.6.3 陷阱处理
本书第二篇比较详细地介绍过CPU执行陷阱操作的过程。那么GPU
是如何做的呢？
在Nvidia的官方文档中，很难找到关于这个问题的深入介绍，即使
有，也只是只言片语。
不过，当作者在茫无际涯的互联网世界中搜索时，却有意外惊喜，
找到了Nvidia公司多名员工关于陷阱处理的一篇专利文献，标题为“用于
并行处理单元的陷阱处理程序架构”（Trap Handler Architecture for a
Parallel Processing Unit）[13]。这篇长达20页的专利文献，不仅内容详
细、语言精准恰当，还有多达9幅的插图，可谓细致入微。看到这篇专
利文献，真是踏破铁鞋无觅处，得来全不费工夫。
 老雷评点 
高！
在这篇编号为US8522000的专利文献中，发明者详细描述了
GPU（专利中称为PPU）执行陷阱操作的过程，既包括总的架构和设计
思想，又有详细的流程。图9-14便是来自该专利文献的陷阱处理程序总
体架构图。
图9-14 陷阱处理程序总体架构图
某种程度上来讲，图9-14包括了一个GPGPU的大部分关键逻辑，这
从侧面反映了陷阱机制的重要性和牵涉面之广。图中的SPM是Streaming
Multiprocessor的缩写，也就是前面介绍过的流式多处理器（SM）。与
每个CPU都可以有自己的陷阱处理程序不同，在GPU中，属于一个SM
的所有处理器共享一个陷阱处理程序。
图9-14右侧是内存布局图，非常值得品味，分别简单介绍如下。
线程组程序栈，用于存放线程组的状态信息，包括稍后介绍的异常
位置信息。
线程组代码段，存放线程组的代码，即GPU的指令。
陷阱处理程序代码段，存放陷阱处理程序。内部又分为几个部分，
前两个部分分别用于支持系统调用和主CPU中断，后面是一个个的
陷阱处理函数。
这个专利的提交时间是2009年9月29日，结合其他资料，作者推测
这个专利所描述的设计最早用在费米微架构中。在David Patterson撰写
的费米架构十大创新[14]的第7条（名为调试支持）中，特别介绍了费米
架构中引入的陷阱处理机制。在此前的GPU中，如果GPU遇到异常（断
点单步等），就冻结这个GPU的状态，然后发中断给CPU，让CPU上的
程序来读取GPU的状态，接着根据读到的状态进行处理。异常处理完之
后，再恢复GPU执行。而在新的设计中，GPU自己可以执行位于显存中
的陷阱处理程序，也就是刚刚介绍的陷阱处理程序代码。这意味着GPU
可以自己处理异常情况。这个变化的意义不仅仅在于异常处理本身。陷
阱处理程序代码的地位非同寻常，它掌控着系统的命脉和生杀大权。在
CPU端，异常处理是内核中的关键部分，是内核管理层的“要塞”。GPU
有了自己的陷阱处理器，就有了自己的管理层。其意义何其大也。历史
上，操作系统就是由中断处理程序演变而来的。从这个意义上看，这个
变化真可以用翻天覆地来形容。
图9-15也是来自上述专利文献的插图，它描述了GPU执行陷阱处理
程序的具体过程。
图9-15 执行陷阱处理程序的过程
品味图9-15，会发现里面的有些操作与CPU的做法很类似，比如在
转移执行前先把执行状态信息（程序计数器、代码段地址、程序状态字
等）压到栈上并保存。根据专利中的描述，使用的栈就是图9-14中的线
程组程序栈（编号545）。不过，也有些步骤不同，比如以下步骤。
把错误信息保存到ESR（Error Status Register）寄存器中。x86 CPU
是把错误码压到栈上。
要把触发异常的线程组ID保存到线程组ID寄存器中，因为如前所
述，每个SM共享一个异常处理程序，所以要通过这个ID信息知道
是哪个线程组出了异常。对于CPU，因为每个CPU处理自己的异
常，所以是不需要这一步的。
所有线程组都会执行陷阱处理程序，不仅仅是发生异常的线程组，
这与CPU的做法也是不同的。
如果需要把异常报告给CPU，那么会发中断给CPU，并且让当前线
程组进入暂停（halt）状态。
陷阱机制赋予处理器“飞跃”的能力。可以从常规的程序代码飞跃到
陷阱处理程序代码，处理完之后，再继续回到本来执行的程序。操作系
统专家Dave Probert前辈曾说过，中断和陷阱机制让CPU变得有趣。对
于GPU，也是如此。
9.7 系统调用
在CPU和传统的操作系统领域，系统调用一般是指运行在低特权的
用户态程序调用高特权的系统服务，比如应用程序在调用fread这样的函
数读文件时，CPU会执行特殊的指令（syscall、sysenter或者int 2e等）进
入内核空间，执行内核空间的文件系统函数，执行完成后，再返回用户
空间。
在Nvidia的GPU程序中，也有系统调用机制。PTX的手册里把系统
调用解释为对驱动程序操作系统代码的调用（System calls are calls into
the driver operating system code）。这里的驱动程序操作系统可以有两种
理解。一种理解是驱动程序所在的操作系统，比如Windows或者
Linux。另一种理解是指驱动程序实现的为GPU程序服务的操作系统，
这是个新事物，不是传统的Windows和Linux。那么到底是哪一种呢？
作者认为与软硬件的版本有关，早期是前一种情况，后期逐步向后一种
情况过渡。
9.7.1 vprintf
在PTX手册中，列举了很多个系统调用。其中有一个名叫vprintf，
用于从GPU代码中输出信息，与CPU上的printf很类似，因为它恰好与调
试的关系比较密切，所以就先以它为例来理解GPU的系统调用。
在使用系统调用时，必须有它的函数原型，vprintf的函数原型如
下。
.extern .func (.param .s32 status) vprintf (.param t1 format, .param t2 va
list)
其中，status是返回值，format和valist分别是格式化模板和可变数量
的参数列表。对于32位地址，t1和t2都是.b32类型；对于64位地址，它
们都是.b64类型。
调用系统调用的方法和调用常规函数很类似，比如下面是使用32位
地址时的调用方法。
cvta.global.b32    %r2, _fmt;
st.param.b32  [param0], %r2;
cvta.local.b32  %r3, _valist_array;
st.param.b32  [param1], %r3;
call.uni (_), vprintf, (param0, param1);
第一条指令把当前地址空间中的指针转换到全局空间（也叫通用空
间（generic space）），然后再存储到参数区中。
如果在CUDA程序中调用printf来输出信息，其内部就会调用
vprintf。打开一个CUDA程序，加入一行printf（或者打开CUDA开发包
中的simplePrintf示例），然后查看反汇编窗口，可以看到发起系统调用
的PTX指令和硬件指令。
0x000d1218  [0287] st.param.b64    [param1+0], %rd3;  
0x000d1218                       MOV R6, R0;  
0x000d1220                       NOP;  