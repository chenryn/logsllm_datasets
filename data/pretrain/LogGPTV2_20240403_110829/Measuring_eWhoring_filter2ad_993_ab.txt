each extracted TOP. Using a whitelist of known image sharing sites
(used to share previews of the packs) and cloud storage services
(hosting the packs) we extract the links corresponding with both
previews and packs. This whitelist is compiled using a snowball
sampling technique: starting with a known set of domains, we
parse all URLs extracted from the TOPs, and manually analyse a
subset of the domains that do not belong to the whitelist, visiting
their landing sites. This process is repeated until the URLs are
either unknown or do not belong to cloud storage services or image
sharing sites. Finally, the list of links corresponding to previews
and packs is fed to a custom crawler which downloads the images
and (if needed) decompresses the packs into folders.2 For each link,
we also annotate associated metadata (e.g., the post identifier and
author).
Results. Tables 3 and 4 show the number of links extracted per
image sharing site and cloud storage service respectively. Imgur is
by far the most popular platform to share previews, followed by
Gyazo and ImageShack. An inspection into their terms of service
reveal these platforms forbid uploading images containing nudity
or violating copyright. Of the cloud storage services, MediaFire is by
far the the most used platform, followed by mega, Dropbox, and oron
(a now defunct site). Again, the terms of service forbid content that
‘violates someone else’s individual rights or copyright’ (MediaFire).
Likewise, mega’s terms specify ‘You are strictly prohibited from
using our services to infringe copyright’ (mega). It appears oron
closed down after being sued for copyright infringement (including
pornographic material) [20].
We crawled the links and were able to download 5 788 images
from image sharing sites and 111 288 images contained in 1 255
packs. Many files and images had been deleted. After removing du-
plicates (for example, 127 images were found in at least 20 different
packs), there were 53 948 unique files. As these packs are offered at
no charge, and thus are likely ‘saturated’ (i.e., they have been used
for eWhoring before) we had expected to observe duplicate images.
2Interested readers can get details on the crawler by inspecting the source code at
our repository: https://github .com/spastrana/ewhoring-analysis/tree/master/tools/
crawler
Site
imgur
gyazo
imageshack
prnt
photobucket
imagetwist
imagezilla
minus
postimage
imagebam
Others
Total
#Links
3 297
1 006
679
383
311
105
97
51
47
44
700
7 314
Site
MediaFire
mega
Dropbox
oron
depositfiles
filefactory
drive.google
ge.tt
zippyshare
filedropper
Others
Total
#Links
892
284
130
95
46
37
31
28
25
24
94
1 719
Table 3: Number of links
per image sharing site.
Table 4: Number of links
per cloud storage service.
We also found that not all files downloaded from image sharing
sites correspond with actual previews, which we will discuss in
ğ4.4.
Limitations. Some of the packs are released to users who reply to
a given thread or pay a fee. Due to ethical concerns we do neither of
these, and thus we are not able to download such packs. Thus, our
results are limited to packs openly shared for free. Concretely, out
of the 4 137 TOPs, we were able to extract links from 774 threads
(18.71%). Also, we look for links leveraging a whitelist of cloud
storage and image sharing sites. Accordingly, we might be missing
some sites. To reduce this limitation, we used a snowball sampling
method. When downloading images, we encountered two limita-
tions. First, we did not download packs from some sites requiring
registration, e.g., Dropbox or Google Drive, where crawling violates
their Terms of Service. Second, many of the cloud storage services
and image sharing sites are either defunct or restrict the lifetime of
the links for free or trial accounts. Thus, while we have retrieved
links posted in the past, we were unable to download the content.
4.3 Filtering out child abuse material
A potential legal issue for actors involved in eWhoring is download-
ing and distributing child abuse material. As we are downloading
images to our servers, this is also a concern for our research. Ac-
cordingly, after due discussion with our Research Ethics Board (see
467
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
S. Pastrana, A. Hutchings, D. Thomas and J. Tapiador
Appendix), we contacted the UK Internet Watch Foundation (IWF)3
to explain our study and ask for their assistance. As part of their
cooperation, we were granted access to the PhotoDNA Cloud Ser-
vice [22], which computes a hash of a given image and matches it
against a database of known child abuse material. These images
have been tagged due to clearly containing child sexual content or
have been reported as depicting someone who is underage. Each
image matching the PhotoDNA list was immediately reported to
the IWF and deleted from our servers. We also reported the URLs
of other sites where these images were located, obtained from the
reverse image search (see ğ4.5).
Results. We found 36 images matched the PhotoDNA hashlist.
Among them, the IWF actioned 61 URLS, 60 related to a single UK
victim aged 17, and one related to a 7 to 10 year old victim. While
the rest of the images match the hashlist due to other organisations
grading them as child abuse material, these were not actionable by
the IWF since they were not able to verify the age of the persons
depicted. Regarding the 61 URLs actioned by the IWF:
• Severity. 20 of the images were category A (images involv-
ing penetrative sexual activity; images involving sexual ac-
tivity with an animal or sadism), 36 Category B (images
involving non-penetrative sexual activity), 5 Category C
(other indecent images not falling within categories A or B).
• Hosting location. One site was hosted in the UK (taken
down by the IWF), 30 were in North America (USA and
Canada) and 30 were hosted in other European countries.
• Site types. All recorded as being on ‘Free Hosting’ sites
over 36 separate domains, namely: 26 image sharing sites, 9
forums, 3 blogs, 2 social networks, 1 video channel, and 20
regular websites.
As possession of child abuse material is a crime in many jurisdic-
tions, actors and customers downloading these packs are placing
themselves at risk of criminal charges. In some jurisdictions, these
are ‘strict liability’ offences, therefore not intending to access im-
ages of children would not necessarily be a legal defence. A defence
may be available if the defendant can prove innocence, such as hav-
ing had immediately reported the images to the police and deleted
them (as we did), although in other cases they may have to rely on
prosecutorial discretion not to pursue charges [10].
The images were downloaded from links posted in 36 different
threads which were replied to by 476 different actors. An inspection
of the replies show most were expressed gratitude and recognition
such as ‘Downloading, thanks for the share!’ or ‘just download the
pack, amazing pack’. This indicates at least 476 actors are potentially
downloading child abuse material. This is a lower bound as many
users may have downloaded the packs without posting a response.
We also observed some cases where users discussed the age of the
models in the packs, which suggests the community is aware of
the potential legal risks associated with downloading child abuse
images. For example, one reply exclaimed:
‘you have to take the image down. She is 100%
under age, just look at her!! And thanks for the
share anyway’
3https://www .iwf .org .uk the UK’s INHOPE hotline operator.
Limitations. The detection of images containing child abuse ma-
terial is limited by the accuracy of the PhotoDNA technology from
Microsoft. While offenders might try to evade detection by modi-
fying the images, PhotoDNA leverages Robust Hashing to detect
images that have been modified, e.g., using compression algorithms
or geometric distortions [37]. To the best of our knowledge, the
robustness of this tool has not been independently verified, and
doing so is forbidden by the terms of use and out of the scope of
our study. However, this is the state-of-the-art technology used by
law enforcement and non-profit organisations in the fight against
online child exploitation, including the IWF and the US National
Center for Missing and Exploited Children (NCMEC).
4.4 Image classification
To address some of the ethical and legal concerns raised by our
study (such as inadvertently viewing child abuse material, and the
potential for psychological harm ś see Appendix), we designed an
approach to minimise the amount of indecent images visualised
by the researchers. Accordingly, we developed a NSFV classifier
which indicates whether an image contains indecent content or not.
Recall that our classifier is used to discern between indecent images
from models, possibly showing explicit sexual content, and other
images containing text, such as screenshots of payment platforms
or chats. Therefore, in addition to the images downloaded from
TOPs, we also apply the classifier before the manual inspection of
images related with eWhoring earnings (see ğ5).
Before identifying the provenance of images, we filter out those
not actually depicting models. Images contained in packs typically
contain a set of pictures from the same (or visually similar) model in
various stages of a virtual sex encounter, i.e., dressed (normally in a
suggestive manner, which are used for attracting customers), naked
(partially or full), or involving sexual activity. This also applies
for pack previews. However, our link extraction for image sharing
sites may retrieve links to other types of images, such as ‘proof-
of-earnings’ (images showing the eWhoring earnings reported by
forum actors, typically screenshots of dashboards from payment
platforms) or other screenshots (e.g., of a chat discussion between
an actor and a customer). As these types of images usually contain
characters, the NSFV classifier combines the output of a detector
of nudity in images with an Optical Character Recognition (OCR)
classifier.
For the nudity detector we used Yahoo’s OpenNSFW classi-
fier [21] (where NSFW stands for ‘Not-Safe-For-Work’), which lever-
ages a model trained using deep learning and provides a probability
score of an image containing indecent content. For the OCR classi-
fier, we use the Tesseract software [32], which outputs the number
of words recognised in an image. Using these two scores (NSFW
and OCR), we developed a set of heuristics to determine whether
an image is NSFV or not. These heuristics combine thresholds from
both classifiers in a set of rules. The heuristics are tuned using
both a validation dataset of 180 labelled images (including sexual
and non-sexual content) released by Lopes et al. [2] and a set of 60
images manually retrieved from the web with textual content (e.g.,
documents, bills, source code, etc.) and without textual content
(including landscapes, screenshots of virtual games, or pictures
taken from random people) .
468
Measuring eWhoring
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Results. We have empirically tested various combinations of the
scores provided by the Yahoo NSFW and OCR classifiers against
the validation dataset. In general non-nude images receive a NSFW
score lower than 30%. There is an exception with images of clothed
models with high proportion of human body, since these usually
have a NSFW score which is between 10% and 70%. In our im-
plementation, the main task is to discern between images from
models (packs and previews) and images showing textual content
(e.g. showing earnings, chats, etc.). Thus, we rely on the OCR to
fine-tune the scores of the classifier. Concretely, images including
text and characters (e.g. source code, memes, or screenshots) are
properly recognised by the OCR classifier, which extracts the text in-
cluded in the image. After empirically testing various thresholds by
running the classifier on the validation dataset, we established the
heuristics shown in Algorithm 1, which best classify the validation
dataset. These heuristics are conservative, since they resulted in a
100% detection of NSFV images (meaning that all images tagged as
nude are detected as NSFV) while having few false positives (nearly
8%).
Algorithm 1: Classify images into SFV or NSFV
Input: Image
Output: True if SFV, False otherwise
N SFW ← openN SFW (imaдe )
OCR ← tesseract (imaдe )
if N SFW  0.3 then
return False
else if N SFW  10
else
return OCR > 20
Among the 5 788 images downloaded from image sharing sites,
3 496 were classified as NSFV and thus we include them in the set of
‘previews’. Other links either pointed to error messages (e.g., ‘This
image violates our Terms of Use and has been removed from view’)
or screenshots showing the directories of the packs, including image
thumbnails.
Limitations. We rely on open-source tools to classify images. Ya-
hoo’s OpenNSFW provides a trained deep learning-based model
which provides a NSFW score for each input image. While the defi-
nition of NSFW is subjective, this model suits our needs. Indeed, as
stated by the authors, this model ‘can be used for the preliminary
filtering of pornographic images’ [21], which is precisely our goal.
Moreover, we enrich the classification by using the OCR classifier,
using the Tesseract OCR engine [31, 32]. Again, we rely on third
party software whose exact accuracy is not known to us. How-
ever, Tesseract is one of the most used technologies for recognising
characters in images, both in industry [35] and academia [31].
The threshold and heuristics for the classification are established
in a semi-automatic process which is tuned using a dataset with
few images (240). With ethical concerns in the forefront, we select
those thresholds in a conservative way to minimise false negatives
(i.e., including indecent images in the set of SFV images). This
increases the likelihood of false positives, which might affect the
completeness of our results. In this regard, false positives are images
that, not being from models, are classified as so. These images
might be included in our final dataset of images for which we
conduct reverse image search, possibly biasing our final results.
The most problematic (i.e., hard-to-classify) pictures are those that,
not containing nudity, are tagged as NSFV due to: i) not having any
text, and ii) containing colours or textures resembling the human
body. Additionally, a recent paper shows how adversaries might
modify images to prevent detection of pornography [40]. Due to
the lack of a labelled dataset for eWhoring images, we can not test
the actual performance of our classifier, and this might be viewed
as a filter for indecent images. However, since our classifier is used
to discern between model previews (usually containing nudity and
none or few characters) and images having high percentage of text
(e.g. screenshots from chats or payment platforms), the number
of images wrongly classified is likely negligible. Indeed, during
manual annotation of 2 067 images showing earnings (see ğ5) we
have not visualised any image from models.
4.5 Reverse image search and domain
classification
To analyse the actual provenance of the images being used for
eWhoring, we use the image reverse search service provided by
TinEye [36]. This service compares a given image against a database
of more than 29 billion images crawled from the web. Each compar-
ison outputs a similarity score, and if this score is greater than zero
it is considered a match. If one or more matches are found, a report
is created indicating for each match, among other information: i)
the domain and URL where the image is (or was) hosted; ii) the
backlink from where it was crawled and; iii) the crawling date. Ad-
ditionally, to analyse whether the images were online before they
were posted in the forums, we have used the Wayback Machine [3]
to explore the Internet Archive 4 for each of the matching URLs.
We performed reverse image searches for the entire set of 3 496
preview images classified as NSFV. Due to the large number of im-
ages included in the packs (around 111k), we selected 3 images per
pack for reverse searching. We select the images with the lowest,
median and highest NSFV score from each pack. As these corre-
spond to the same model, we assume they have been taken from the
same site. In total, we performed reverse searches for 3 644 images
from the 1 255 packs (note some packs have less than 3 images).
We used domain classification tools to gain additional knowl-
edge of the type of sites offenders rely on to collect material for
their packs. We use three well-known classifiers: Cisco’s OpenDNS5
domain tagging service, McAfee’s URL ticketing system,6 and Virus-
Total’s URL reputation service.7
Results. Table 5 shows the results of the reverse image search
performed using TinEye’s service. In total, we got 3 621 responses
for the images in the packs plus all (3 496) the preview images.
From these, we got matches for 74% and 49%, respectively. This
difference might be due to modifications made on previews, e.g., by
4http://archive.org
5https://www .opendns .com
6https://www .trustedsource .org
7https://www .virustotal .com/gui/home/url
469
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
S. Pastrana, A. Hutchings, D. Thomas and J. Tapiador
packs
previews
Total Matches
2 675 (74%)
3 621
3 435
1 683 (49%)
Seen Before Ratio Max
642
1 969
2 011 (55.54 %)
1 340 (39.01 %)
12.7
17.3
Table 5: Number of matches for the reverse image search.
Seen Before means that the image was URL was online be-
fore the image was posted in the forum. Max and ratio refers
to the maximum and average number of matches per image.
adding a watermark or shadowing parts of the images. Actors pur-
posely modify these images to bypass reverse image searches [19].
However, the ratio of matches per image is substantially higher
in the case of previews (observed on average in more than 17.3
sites), as opposed to packs (which are shown on average in 12.7
sites). The column ‘Seen Before’ in Table 5 shows the number of
matches whose URL has been crawled (either by TinEye or the
Wayback machine) before the corresponding image was posted in
the forum. Thus, these images were online before they were shared
as packs. For the remaining matches (i.e. those whose crawling date
is posterior to the post in the forum) we cannot claim whether they
were online before or not.
From the set of 1 255 packs analysed, 203 are zero-match, i.e.,
composed by images with zero matches in the reverse image search.
We observed various actors offering several zero-match packs, with
a single actor sharing 47 zero-match packs (out of 100 shared packs).
Zero-matches might be due to packs composed by pictures: i) of
actual models, ii) obtained from sites which are not included in the
TinEye database, or iii) modified to bypass reverse searches (e.g., by
mirroring the images). The last option can be easily performed using
automated tools, which are shared in underground forums [19].
The reverse image search resulted in 5 917 different domains. The
distribution of categories provided by each domain classifier has a
long tail, with around 4-5 categories accounting for more than 50%
of the domains (see Table 6). The top categories are mostly porn-
related sites, though the distribution is quite different depending
on the classifier. Our results suggest images are also taken from
a variety of sources other than porn/nudity sites, including social
networks, online shops, photo sharing services, blogs and online
forums/discussion boards, among others. This is not surprising
given how rich in multimedia material such sites are, especially for
clothed images used during the early stages of engagement with
customers. This range of users whose personal pictures might be
illicitly used in eWhoring activities could be wider than initially
expected, since it seems to encompass more than models in the
porn industry.
Limitations. We rely on TinEye for the reverse image searches.
TinEye’s matching engine claims ‘to deal with a broad range of im-
age transformations, including resizing, cropping, edits, occlusions
and colour changes, amongst others’ [36]. Each image is tested
against a database, which at the time of writing contains 29 billion
images. Nonetheless, our results are biased by the effectiveness in
dealing with image modifications, e.g., mirroring or shadowing,
and also by the completeness of the database (e.g., images obtained