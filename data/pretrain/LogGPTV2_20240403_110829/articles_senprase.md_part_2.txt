CI pairs from a log message, we replace the instances with
respectively.Besides,theconceptualizedtemplatesarederived
their corresponding concepts and name the new message as
by replacing instances with their corresponding concepts (if
conceptualized template.
available), or “” for else. The final structural outcome
The semantic parser task can be regarded as following.
of SemParser consists of conceptualized templates, CI pairs,
Given a log message†, the structural output is composed
orphan concepts, as well as orphan instances.
of a conceptualized template T, a set of CI pairs CI =
As the first and fundamental step for log analysis, Sem-
{(c ,i ),...,(c ,i )},aswellasotherorphanconceptsOC =
0 0 n n Parser could facilitate general downstream log analysis tasks.
{oc ,...,oc } and orphan instances OI = {oi ,...oi } which
0 j 0 k We will introduce details of the semantics miner and the joint
cannot be paired with each other.
parser in the following two subsections. Then, two typical
downstream applications will be displayed in Section V.
III. METHODOLOGY
A. Overview of SemParser B. End-to-end semantics miner
Our framework is composed of two parts, an end-to-end Semantics miner aims to mine semantics on both the
semantics miner and a joint parser. In Figure 2, we use instance-level and the message-level. To acquire a set of
an example to illustrate how our framework processes log explicit concepts, instances , and CI pairs within a log
messages.Tobeginwith,logmessagesaresenttothesemantic message, we model the task into two sub-problems: find-
miner for acquiring template-level semantics (i.e., concepts) ing CI pairs and classifying each token into a type in
and explicit instance-level semantics (i.e., explicit CI pairs) {concept,instance,none}. As shown in Figure 3, an end-
of each log independently. This step particularly solves the to-end model with three modules is proposed to solve the
first two stated challenges. The unseen explicit CI pairs will two sub-tasks simultaneously. First, a log message is fed
be added to the Domain Knowledge database to keep the into a Contextual Encoder for acquiring context-based word
representation. Then, the contextualized words are separately
*Theterm“instance”isratherclosedtothe“parameters”or“variables”in usedinPairMatcherandaWordScorerforextractingCIpairs
thesyntax-basedparser.Oneconceptcanbeinstantiatedbymultipleinstances.
and determining the type of each word, respectively. As the
†The log message refers to log content without fields in this paper by
default. totallossisthesumofthePairMatcherlossandWordScorer
Loss computation LossM + LossM + … + LossP … LossM = Loss
1 2 6
softmax softmax softmax softmax
ScoreM
i
[𝑝 𝑛 ;𝑝(𝑐) ;𝑝(𝑖 )] C I C I C I
! ! !
ScoreP
i
Mention pair feature
[𝑚 ;𝑚;𝑐𝑜𝑛𝑡𝑥 ]
! " !,"
Bidirectional LSTM
Word feature
[𝑤 ;𝑐ℎ𝑎𝑟 ;𝑓𝑙𝑜𝑐𝑎𝑙]
! ! !
 Listing instance in cell 949e1227
Fig. 3: The architecture of semantics miner.
loss, the model is forced to learn from both sub-tasks jointly. pre-defined categories. For example, the combination of bi-
We elaborate on the details of the three modules as below. LSTM and Conditional Random Field (CRF) is deployed to
1) Contextual encoder: Intuitively, log messages can be identify 100 log entities (e.g., IP address, identifier) in log
regarded as a special type of natural language due to its semi- messages [24], or uncover 20 software entities (e.g., class
structuredessenceofmixingunstructurednaturallanguageand name, website) in software forum discussions [25]. However,
structured variables. such token classification-based framework relies on a closed-
Motivated by the success of long short-term memory net- world assumption that all categories are known in advance.
works (LSTM) across natural language processing tasks [19] The assumption makes sense when dealing with a specific
(e.g., machine translation, language modeling), we design an and small system with limited concepts. Unfortunately, it will
bi-directionalLSTM-basednetwork(bi-LSTM)[20]tocapture breakdownifwewanttomigratetheapproachacrosssoftware
interactionsanddependenciesbetweenwordsinlogmessages. systems,orthesystemwearefacingishugeandsophisticated.
However, it is not practical to directly feed the word Togetovertheclosed-worldassumptionlimitation,thepair
embeddings into the LSTM network because of the severe matcher is required to discern the (concept, instance) pairs
out-of-vocabulary (OOV) problem, which is due to the large between words in a log message. We abstract this problem
portion of customized words in log messages (e.g., function as a multi-classifier problem: for each word w i in a sentence
names, cell ID, request ID), resulting drastic performance S = w 1,w 2,...,w n, the matcher determines what previous
degradation. To solve the problem, we devise two additional word w j(0≤j  (w0) to indicate the word does not
several categories by assigning each word into one of the refertoanyofthepreviouswordinthemessage(e.g.,in).
from the contextual encoder, we form the pair-level feature The knowledge-assisted approach maintains a high-quality
for each word pair in {(w ,w ),(w ,w )...(w ,w )}. These domainknowledgedatabasewhenprocessinglogsbyincorpo-
5 4 5 3 5 0
pair features will be scoring by a softmax function on top of rating newly discovered CI pairs acquired from the semantics
a feed forward neural network for loss computation. miner. To guarantee the quality of the domain knowledge, we
3) Word scorer: Apart from the pair matcher, we also only add the superior CI pairs, which are defined by if and
design a word scorer to determine whether each token is a onlyifthereisaconceptandaninstanceinthepredictedpair.
concept, instance or neither of both. The token’s category is The joint parser examines whether the orphan instances have
crucialfortworeasons.First,themessage-levelsemanticscan their corresponding concepts in the high-quality knowledge
be perceived via extracted concepts. Second, we notice that base, to uncover implicit CI pairs. As a result, fresh CI pairs
some instance-level semantics cannot be resolved via the pair of the log messages are stored if found. In such a way, we
matcheriftheinstance’scorrespondingconceptdoesnotoccur merge the explicit CI pairs and new implicit CI pairs into the
in a single message (e.g., the second log in Figure 1), which final CI pairs. Details are in Algorithm 1.
wecallimplicitinstance-levelsemantics.Inthiscase,weneed
to store the instances for further processing. To this end, we
Algorithm 1 Implicit instance-level semantics discovery
devise the word scorer with a feed-forward neural network
FFNN tolearnthepossibilityofthreetypesforeachtoken. Input: Log message M = m 0,...,m n, instance indices I =
b
[i ,...i ],conceptindicesC =[c ,...c ],explicitCIpairindices
The score is computed as follows: 0 j 0 k
P =[(s ,t ),...,(s ,t )]
0 0 u u
ScoreM =FFNN (m ) (4) Output: Instances I, Concepts C, CI pairs P
i b i 1: P =[]
Afterwards, the possibility of three categories will pass 2: C =[]
3: for all p such that p∈P do
through a softmax layer for normalization before computing
loss. 4: if p contains 1 instance cur I and 1 concept cur C then
5: DomainKnowledge.add(M[cur C],M[cur I])
4) Loss function: Multi-task learning (MTL) is a training 6: I.REMOVE(cur I)
paradigm that trains a collection of neural network models 7: C.REMOVE(cur C)
for multiple tasks simultaneously, leveraging the shared data 8: end if
9: end for;
representationforlearningcommonknowledge[24],[26].The
10: for all i such that i∈I do
fruitfulachievementsofMTLmotivateustotrainpairmatcher
11: if FINDCONCEPTFROMDOMAINKNOWLEDGE(M[i]) then
and word scorer simultaneously by aggregating their losses. 12: P.APPEND([newfound concept, M[i]])
Therefore, the total cost of semantics miner is defined as: 13: C.APPEND(newfound concept)
  14: I.REMOVE(i)
cost= CELoss(P i)+ CELoss(M i) (5) 15: end if
i i 16: end for
17: I = INDEXTOWORD(I)
where P i and M i denotes the outputs of ScoreP i and 18: C += INDEXTOWORD(C);
ScoreM i afterpassingasoftmaxlayer,respectively.Here,we 19: P += INDEXTOWORD(P)
adopt Cross Entropy Loss (i.e., CELoss) as the loss function
due to its numerical stability. By minimizing the cost, the
model naturally learns the pairs and the word types for each 2) Semantic parsing: As a semantic parser, SemParser is
token with shared contextual representations generated from able to extract the template for a given log message obeying
bi-LSTM network. two rules:
In the inference, for each word, we regard the highest
probability of its pairs and its type score as the final results. • For the instance in CI pairs, replacing the instance with
the token  of its corresponding concept.
C. Joint parser
• For the orphan instances, replacing the instance with a
The joint parser leverages concepts, instances, and CI pairs dummy token  as syntax-based parsers do.
obtained from the end-to-end semantics miner, as well as log
The rules are straightforward but reasonable. Compared to
messages to deal with : (1) uncovering implicit instance-level
other technical terms or common words, instances (e.g., ID,
semantics using domain knowledge; and (2) semantic parsing
number, status) are more likely to be variables in logging
log messages. The next sections go into the specifics.
statements automatically generated by software systems. As
1) Implicit instance-level semantics discovery: We apply
the retrieved template takes in concepts, we name it “concep-
a novel domain knowledge-assisted approach to resolve the
tualized template” instead of the vanilla template with only
implicitinstance-levelchallengeofconceptsandinstancesnot
 representing parameters.
coexisting in one log message. Naturally, suppose we have
recognized a CI pair in historical logs, then we are able to Finally, the conceptualized template, CI pairs, orphan con-
identify the semantics of such instance in the following logs, cepts, as well as orphan instances are the structured outputs
even though the following logs do not explicitly contain such of our SemParser. The results are extensible for a collection
pair information. of downstream tasks, and we will elaborate them later.
TABLE I: Statistics of dataset for semantic mining.
IV. SEMPARSERIMPLEMENTATION