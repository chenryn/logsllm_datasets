s
(
e
m
T
e
s
n
o
p
s
e
R
 10
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0
Response Time
Without EtherFuse
 0
 5
 10
 15
 20
 25
 30
 35
 40
Time (s)
(a) Without EtherFuse
i
)
s
(
e
m
T
e
s
n
o
p
s
e
R
 10
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0
Response Time
With EtherFuse
 0
 5
 10
 15
 20
 25
 30
 35
 40
Time (s)
(b) With EtherFuse
Figure 11: Timeline of response times of HTTP requests generated every tenth
of a second under count to inﬁnity. Count to inﬁnity starts at t=10 and no for-
warding loop is formed.
No failure
Failure with EtherFuse
Failure without EtherFuse
Transfer time
35.9s
36.1s
42.1s
Figure 9: Convergence time with and without the EtherFuse after the death of
the root bridge.
Detection Time
With EtherFuse
Detection Time
With EtherFuse
)
s
(
e
m
T
i
 10
 8
 6
 4
 2
 0
-2
 10
 11
 6
 5
 9
Nodes in the Complete Graph
 7
 8
(a) Complete graph topologies
 3
 4
 5
 6
 7
 8
 9
 10
 11
(b) “Loop” topologies
Nodes
Table 1: Transfer times for the FTP transfer of a 400MB ﬁle.
Figure 10: Detection time of count to inﬁnity for EtherFuse.
transmit only one BPDU per second and the convergence process
is slowed down substantially.
6.1.3 Impact on HTTP
In this experiment we study the effects of count to inﬁnity on
web requests. We run the apache web server 2.2.0 on one of the end
hosts and a program simulating web clients at the other end host.
The client program generates HTTP requests to the web server with
a constant rate of one request every 100 ms. The HTTP requests are
HTTP GETs that ask for the index ﬁle which is 44 bytes. We kill
the root bridge, B1, at time 10 to start the count to inﬁnity. We
repeat this experiment twice, once with the EtherFuse and another
time without it and measure the response times of the web requests.
Figures 11(a) and 11(b) show timelines of the measured response
times of each web request before, during, and after the count to
inﬁnity with and without the EtherFuse. Note that before and af-
ter the count to inﬁnity the response time is on the order of one
millisecond. During the count to inﬁnity, many requests have re-
sponse times of 3 seconds and some even have response times of 9
seconds. This is due to TCP back-offs triggered by the packet loss
during the count to inﬁnity. TCP back-offs are especially bad dur-
ing connection establishment, as TCP does not have an estimate for
the round trip time (RTT) to set its retransmission timeout (RTO).
Thus it uses a conservative default RTO value of three seconds.
So if the initial SYN packet or the acknowledgment for this SYN
gets dropped, TCP waits for three seconds until it retransmits. This
explains the three second response times. If the retransmission is
lost again TCP exponentially backs off its RTO to 6 seconds and so
on. Thus we are able to observe requests having 9 second response
times that are caused by 2 consecutive packet losses during con-
nection establishment. In Figure 11(b), we note that the EtherFuse
substantially reduces the period with long response times. This is
because the EtherFuse is able to quickly detect and stop the count
to inﬁnity and thus reduce the period for which the network suf-
fers from extensive packet loss. No connection in this experiment
suffers consecutive packet losses during connection establishment.
6.1.4 Impact on FTP
In this experiment we study the effects of count to inﬁnity on a
FTP download of a 400MB ﬁle from a FTP server. The root bridge
is killed during the ﬁle transmission and the total ﬁle transmission
time is recorded. Table 1 shows the measured transmission times
under count to inﬁnity with and without the EtherFuse. We again
note that transmission time in the presence of the EtherFuse is bet-
ter as it ends the count to inﬁnity early.
6.2 Effects of a Single Forwarding Loop
In this section, we study the effects of a single forwarding loop
on applications and the performance of EtherFuse in mitigating
those effects. We only focus on temporary forwarding loops be-
cause of two reasons. First, since the loops are temporary, they
lead to transient interactions with the applications, which are often
not obvious. Conversely, permanent loops render the network un-
usable leading to the unsurprising result of preventing applications
from being able to make forward progress. Second, EtherFuse han-
dles permanent loops the same way it handles temporary loops, so
presenting the temporary loops case sufﬁces.
We use count to inﬁnity induced forwarding loops as an example
of temporary forwarding loops. We modiﬁed the RSTP state ma-
chines such that its races always lead to a forwarding loop in the
event of a count to inﬁnity.
6.2.1 Fundamental Effects
Figure 12 shows a timeline of packet loss during the count to in-
ﬁnity induced forwarding loop. In this experiment a stream of UDP
trafﬁc ﬂows from one host into another. Since the count to inﬁn-
ity reconﬁgures the network leading to the ﬂushing of the bridges’
forwarding tables and since the receiving end does not send any
packets, bridges do not re-learn the location of the receiving end
host. Thus, bridges fallback to ﬂooding packets destined to the re-
ceiving end host. Thus, those packets end up trapped in the loop
leading to network congestion and packet loss. This can be seen
in Table 2. This massive packet loss leads to BPDU loss, extend-
ing the lifetime of the count to inﬁnity. Consequently, this extends
the duration of the forwarding loop leading to a longer period of
network instability. When the EtherFuse is used the problem is
corrected quickly.
To study the effects of having a temporary forwarding loop on
a simple request/response workload we used ping between the two
end hosts with a frequency of 10 pings per second. We ran this test
for 50 seconds and introduced the count to inﬁnity at time 10. With-
out the EtherFuse, we observed a 81% packet loss rate reported by
ping. Note that there is no congestion in this test as the data rate
 100
 80
 60
 40
 20
)
%
(
s
s
o
L
t
e
k
c
a
P
 0
 0
 5
 10
 15
 20
 25
Time (s)
(a) Without EtherFuse
Packet Loss
With EtherFuse
Packet Loss
Without EtherFuse
 100
 80
 60
 40
 20
)
%
(
s
s
o
L
t
e
k
c
a
P
 30
 35
 40
 45
 0
 0
 5
 10
 15
 20
 25
 30
 35
 40
Time (s)
(b) With EtherFuse
i
)
s
(
e
m
T
e
s
n
o
p
s
e
R
 10
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0
Response Time
Without EtherFuse
 0
 5
 10
 15
 20
 25
 30
 35
 40
Time (s)
(a) Without EtherFuse
i
)
s
(
e
m
T
e
s
n
o
p
s
e
R
 10
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0
Response Time
With EtherFuse
 0
 5
 10
 15
 20
 25
 30
 35
 40
Time (s)
(b) With EtherFuse
Figure 12: Timeline of packets loss using a 90 Mb/s UDP stream under a count
to inﬁnity induced temporary forwarding loop. Count to inﬁnity starts at t=10.
Figure 13: Timeline of response times of HTTP requests generated every tenth
of a second under a count to inﬁnity induced temporary forwarding loop. Count
to inﬁnity starts at t=10.
EtherFuse
No EtherFuse
2
40815
Table 2: Number of duplicate frames detected in the network for the UDP
stream workload in event of having a forwarding loop.
is very low, and both end hosts are transmitting data so packets do
not get trapped in the loop as in the experiment above. The main
reason for the packet loss in this test is forwarding table pollution
explained in Section 2.2.4. Speciﬁcally, in Figure 2 if a ping re-
sponse from H1 causes the pollution, packets from H2 will not be
able to reach H1 anymore. The pollution is ﬁxed when the affected
end host, H1, transmits a packet ﬁxing the polluted forwarding ta-
ble entry in B5. Thus, the pollution problem can last for a much
longer period of time than that of the temporary forwarding loop.
When the EtherFuse was used for the same experiment above, less
than a 1% packet loss rate was reported by ping. This is because
the EtherFuse quickly detects the forwarding loop, shutting it down
and ﬁxing any potential pollution by sending the topology change
message that ﬂushes the forwarding tables.
6.2.2 Impact on HTTP
In this section, we repeat the experiments in Section 6.1.3, ex-
cept that a forwarding loop is formed. Figure 13 shows a timeline
of measured response times of web requests before, during and af-
ter the count to inﬁnity induced forwarding loop. In the case of
not having the EtherFuse, although the trafﬁc in the network is
minimal we note that having a forwarding loop hurts the response
times of web requests. This is because although the connectivity
is still available between the server and the client, packets com-
ing from the client and the server into the forwarding loop pollute
the bridges’ forwarding tables. This leads to packet drops due to
packet misforwarding and blackholing. Packet drops compounded
with the TCP backoffs, especially during TCP connection estab-
lishment, lead to very high response times. In the case of Ether-
Fuse, it detects and shuts down the forwarding loop very quickly
so the disruption to the network operation is minimal. Note that for
this workload pollution does not last for very long. This is because
if a packet of an end host H causes pollution, the acknowledgment
for this packet will not arrive, causing H to retransmit the packet
ﬁxing the pollution. If an acknowledgment packet from the server
to the HTTP request causes the pollution, the server will send the
response which will ﬁx the pollution. Also, if the acknowledgment
packet from the client to the HTTP response causes the pollution,
either the next request or the connection tear down packet will ﬁx
the pollution.
Figure 14 shows the effects of having background broadcast traf-