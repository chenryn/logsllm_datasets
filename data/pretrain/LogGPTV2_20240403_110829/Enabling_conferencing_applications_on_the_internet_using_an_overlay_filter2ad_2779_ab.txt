### Link Latency and Bandwidth Estimation

The link latency is estimated to be half of the round trip time (RTT). While these measurements generally have low overhead, we can directly obtain RTT estimates by querying the transport protocol when data flow is present along a mesh link, provided that the underlying transport protocol allows this.

To keep bandwidth estimates for existing mesh links up to date, we passively monitor the performance of these links during data flow. Members periodically advertise the data transfer rates to their neighbors along a mesh link. The neighbor compares the advertised rate with its own estimate of the received data rate. If the rates are comparable, the advertised rate is treated as a lower bound on available bandwidth. Otherwise, the actual received data rate is considered the bandwidth estimate for the link.

For links not in the mesh, active end-to-end measurements are used to determine bandwidth. This involves transferring data using the underlying transport protocol for 15 seconds, but at a rate bounded by the maximum source rate. To minimize the overhead of active measurements, we employ the following techniques:

- **Congestion Detection**: If a member experiences poor performance due to congestion on its local access link (e.g., a machine behind ADSL or a modem), it does not probe other members. We use a simple heuristic to detect congestion: pinging the first-hop router and considering the link congested if ping times exceed a threshold. This heuristic has proven effective in many situations.
- **Selective Probing**: Member A conducts an active bandwidth measurement to member B only if B itself gets good performance from other members and has the potential to improve A’s performance. A determines the quality of B’s performance by examining its routing table, obtained through a small number of message exchanges. This mechanism helps in heterogeneous environments by avoiding unnecessary probing of members with poor connections.

Bandwidth estimates may become outdated or inaccurate due to changing network conditions or inherent measurement noise. To ensure timely estimates, a member may probe another member if there has been no bandwidth estimate for an extended period. Specifically, a member may conduct an active bandwidth measurement to a mesh neighbor if there has been no estimate in the last five minutes, and to a non-neighbor if there has been no estimate in the last twenty minutes.

### Experimental Evaluation

Our evaluation aims to answer the following questions:
- Can End System Multicast meet the bandwidth and latency requirements of conferencing applications in the Internet?
- How critical is it to adapt to network performance metrics such as bandwidth and latency while constructing overlays?
- What are the network costs and overheads associated with the self-organizing overlay architectures we consider?

To address these questions, we examine the performance of several schemes for constructing overlay networks, as described in Section 5.1. Our scheme is the only one that dynamically adapts to both bandwidth and latency, while other schemes consider only one metric or none at all. Detailed results comparing our scheme with others are presented in Section 6. In Section 7, we focus on our scheme and analyze its adaptation to network congestion.

Two key factors affect the performance of overlay construction schemes: the characteristics of the source application and the degree of heterogeneity in the host set. Less demanding applications and more homogeneous environments can make even a poorly constructed overlay perform adequately.

We evaluate the schemes with different constant bit rate (CBR) sources, which are common in conferencing applications. To study performance in environments with varying degrees of heterogeneity, we create two sets of hosts: the Primary Set and the Extended Set. The Primary Set includes 13 well-connected hosts at university sites in North America, while the Extended Set includes 20 hosts, including a machine behind ADSL and hosts in Asia and Europe, leading to greater variation in bandwidth and latencies.

Experiments were conducted over a two-week period on a wide-area testbed, measuring the bandwidth and latency provided by the overlay between the source and different clients, as well as the network resource usage and overheads incurred by the various overlay schemes.

### Schemes for Constructing Overlays

Our overlay construction schemes are derived from the Narada protocol [3] and differ based on the network metrics they consider. We compare the following schemes:

- **Sequential Unicast**: This hypothetical construct measures the bandwidth and latency of unicast paths from the source to each recipient independently, providing a baseline for comparison.
- **Random**: This scheme produces random, connected overlay trees rooted at the source, helping to validate our evaluation and assess the impact of network heterogeneity.
- **Prop-Delay-Only**: This scheme builds overlays based on propagation delay, a static network metric. Propagation delay is measured by picking the minimum of several one-way delay estimates.
- **Latency-Only and Bandwidth-Only**: These schemes construct overlays based on a single dynamic metric, highlighting the importance of using both bandwidth and latency.
- **Bandwidth-Latency**: Our proposed scheme that uses both bandwidth and latency to construct overlays.

Many of our hosts are on 10 Mbps connections, and we use source rates up to 2.4 Mbps. To prevent bad choices due to local link saturation, schemes using static metrics like Prop-Delay-Only impose pre-configured degree bounds. In contrast, Bandwidth-Latency, Latency-Only, and Bandwidth-Only adapt to dynamic metrics, allowing them to avoid congestion without pre-configured bounds.

### Experimental Methodology

Internet performance variations influence the relative results of experiments. To mitigate this, we interleave experiments with different protocols and run the same experiment at different times of the day. Each experiment lasts 20 minutes, with bandwidth and RTT measurements collected after four minutes. Sequential Unicast tests are conducted by unicasting data from the source to each receiver for two minutes in sequence.

### Performance Metrics

We use the following metrics to evaluate overlay tree quality:
- **Bandwidth**: Measures application-level throughput at the receiver, indicating video quality.
- **Latency**: Measures end-to-end delay, including propagation, queuing, and processing delays. We estimate RTT, which is twice the end-to-end delay.
- **Resource Usage**: Captures the network resources consumed, defined as the sum of the costs of physical links. We assume the cost of a physical link is its propagation delay.
- **Protocol Overhead**: Measures the ratio of non-data traffic to data traffic, including control and probe traffic.

### Implementation Issues

Experiments are conducted using unoptimized user-level code. Delays could be minimized by kernel-level implementation and code optimization. We use TFRC [5] as the underlying transport protocol, which is rate-controlled UDP and achieves TCP-friendly bandwidths without retransmission and sender buffer queueing delays.

### Experimental Results

In a typical experiment, we observe that the overlay takes about 150 seconds to stabilize, with mean bandwidth and RTT improving over time. Figures 3 and 4 show that the overlay makes frequent topology changes initially but converges to a stable structure after about four minutes. The adaptive nature of our scheme is evident, as it can handle and recover from congestion events, as seen in a dip in bandwidth and a peak in RTT around 460 seconds.

This detailed evaluation highlights the effectiveness and adaptability of our scheme in various network conditions.