ets with their neighbors and estimating the round trip time.
The link latency is assumed to be half the round trip time.
While these measurements turn out to have a low overhead,
we note that when the underlying transport protocol allows,
and there is data ﬂow along a mesh link, we may directly
obtain round trip time estimates by querying the transport
protocol.
We keep bandwidth estimates of links already in the mesh
up to date by passively monitoring the performance of these
links when there is data ﬂow along them. Members periodi-
cally advertise the rates at which they are transferring data
to their neighbors along a mesh link. The neighbor com-
pares this advertised estimate, with an estimate of data it
actually receives along that mesh link. If the rates are com-
parable, it treats the estimate as a lower bound on available
bandwidth. Otherwise, it assumes the rate at which it re-
ceives data is an actual estimate of the bandwidth of the
link.
Bandwidth estimates of links not in the mesh are currently
determined using active end-to-end measurements, which in-
volves transferring data using the underlying transport pro-
tocol for 15 seconds, but at a rate bounded by the maximum
source rate. As active measurements can have a high over-
head, we have adopted simple techniques to minimize the
number of such measurements:
• If a member is receiving poor performance because of con-
gestion on its local access link (for example, a machine be-
hind ADSL, or a modem), this member does not probe any
other member. We currently use a simple heuristic to deter-
mine congestion on a local link: we ping the ﬁrst hop router
on the member’s path to the Internet, and determine the
local link to be congested if the ping times exceed a thresh-
old. We have found this heuristic to work reasonably well
in many situations.
• Member A conducts an active bandwidth measurement to
member B, only if B itself gets good performance from other
members, and has the potential to improve A’s performance.
A determines the quality of B’s performance to other mem-
bers by examining its routing table which it obtains using
a small number of message exchanges. We ﬁnd this mecha-
nism helpful in heterogeneous environments, where a good
member can avoid probing a member behind a modem or
ADSL connection. Further, it prevents members who are al-
ready doing well in the overlay from probing other members.
Bandwidth estimates may be outdated due to a change of
network conditions since the last estimate was made, or inac-
curate due to noise inherent in these measurements. To keep
estimates to other members timely, a member may probe
another member for which there has been no bandwidth es-
timate for an extended period of time. Currently, a member
may conduct an active bandwidth measurement to a mesh
neighbor for which there has been no bandwidth estimate in
the last ﬁve minutes, and to a non-neighbor for which there
has been no bandwidth estimate in the last twenty minutes.
5. EXPERIMENTAL EVALUATION
Our evaluation seeks to answer the following questions:
• From the application perspective, can End System Multi-
cast meet the bandwidth and latency requirements of con-
ferencing applications in the Internet?
• How critical is it to adapt to network performance metrics
such as bandwidth and latency while constructing overlays?
• What are the network costs and overheads associated with
the self-organizing overlay architectures we consider?
To answer these questions, we examine the performance of
several schemes for constructing overlay networks, described
in Section 5.1. Of these schemes, ours is the only one that
adapts dynamically to both bandwidth and latency. All
other schemes consider only one of these metrics, or none
at all. Section 6 presents detailed results that compare the
performance of our scheme with all other schemes. In Sec-
tion 7, we focus on our scheme and analyze several aspects
pertaining to how it adapts to congestion in the network.
Two important factors aﬀect the performance of a scheme
for constructing overlay networks. These critical factors are
the characteristics of the source application and the degree
of heterogeneity in the host set we consider. Less demanding
applications and more homogeneous environments can make
even a poorly constructed overlay perform adequately.
We consider the performance of the schemes with diﬀerent
speed constant bit rate (CBR) sources. CBR encodings are
common in conferencing applications, and make our eval-
uation convenient. To study the performance of overlay
schemes in environments with diﬀerent degrees of hetero-
geneity, we create two groupings of hosts, the Primary Set
and the Extended Set. The Primary Set contains 13 hosts
located at university sites in North America where nodes
are in general well-connected to each other. The Extended
Set contains 20 hosts, and includes a machine behind ADSL,
and hosts in Asia and Europe, in addition to the hosts in
the Primary Set. Thus, there is a much greater degree of
variation in bandwidth and latencies of paths between nodes
in the Extended Set.
We conducted several experiments over a period of two
weeks on a wide area test-bed. Our experiments measure
the bandwidth and latency that a overlay provides between
the source and the diﬀerent clients. We also measure the
network resource usage and overheads incurred by the dif-
ferent overlay schemes. The details of these measurements
are in the sections that will follow. We vary both the source
rate and client set to evaluate how well the schemes operate
in diﬀerent conditions.
5.1 Schemes for Constructing Overlays
Our schemes for constructing overlays are derived from
the Narada protocol [3], and diﬀer from each other based
on which network metrics they consider. We compare the
following schemes for overlay construction:
• Sequential Unicast: To analyze the eﬃciency of a scheme
for constructing overlays, we would ideally like to compare
the overlay tree it produces with the “best possible over-
lay tree” for the entire set of group members. We approxi-
mate this by the Sequential Unicast test, which measures the
bandwidth and latency of the unicast path from the source
to each recipient independently (in the absence of other re-
cipients). Thus, Sequential Unicast is not a feasible overlay
at all but a hypothetical construct used for comparison pur-
poses.
• Random: This represents a scheme that produces ran-
dom, but connected overlay trees rooted at the source. This
scheme also helps to validate our evaluation, and addresses
the issue as to whether our machine set is varied enough
that just about any overlay tree yields good performance.
• Prop-Delay-Only: This represents a scheme that builds
overlays based on propagation delay, a static network met-
ric. Measuring propagation delay incurs low overhead, and
overlays optimized for this metric have been shown to yield
reasonably good simulation results [3].
In our evaluation,
we computed the propagation delay of an overlay link by
picking the minimum of several one-way delay estimates.
• Latency-Only and Bandwidth-Only: These two schemes
construct overlays based on a single dynamic metric with
no regard to the other metric. They are primarily used
to highlight the importance of using both bandwidth and
latency in overlay construction.
• Bandwidth-Latency: This represents our proposed scheme
that uses both bandwidth and latency as metrics to con-
struct overlays.
Many of our hosts are on 10 Mbps connections, and we use
source rates as high as 2.4 Mbps. To prevent obviously bad
choices of overlay trees due to saturation of the local link,
schemes that use static network metrics like Prop-Delay-
Only are required to impose static, pre-conﬁgured degree
bound restrictions on the overlay trees they construct [3]. In
our evaluation, we try to be give Random and Prop-Delay-
Only the best possible chance to succeed by appropriately
choosing per-host degree bounds based on the bandwidth of
that host’s connection to the Internet. On the other hand,
Bandwidth-Latency, Latency-Only and Bandwidth-Only are
able to adapt to dynamic network metrics. This enables
them to automatically detect and avoid congestion on links
near members, without a pre-conﬁgured degree bound.
5.2 Experimental Methodology
The varying nature of Internet performance inﬂuences the
relative results of experiments done at diﬀerent times. Char-
acteristics may change at any time and aﬀect the perfor-
mance of various experiments diﬀerently. Ideally, we should
test all schemes for constructing overlays concurrently, so
that they may observe the exact same network conditions.
However, this is not possible, as the simultaneously operat-
ing overlays would interfere with each other. Therefore, we
adopt the following strategy: (i) we interleave experiments
with the various protocol schemes that we compare to elimi-
nate biases due to changes that occur at shorter time scales,
and (ii) we run the same experiment at diﬀerent times of the
day to eliminate biases due to changes that occur at a longer
time scale. We aggregate the results obtained from several
runs that have been conducted over a two week period.
Every individual experiment is conducted in the following
fashion.
Initially, all members join the group at approx-
imately the same time. The source multicasts data at a
constant rate and after four minutes, bandwidth and round-
trip time measurements are collected. Each experiment lasts
for 20 minutes. We adopt the above set-up for all schemes,
except Sequential Unicast. As described in Section 5.1, Se-
quential Unicast determines the bandwidth and latency in-
formation of a unicast path, which we estimate by unicast-
ing data from the source to each receiver for two minutes in
sequence.
5.3 Performance Metrics
We use the following metrics to capture the quality of an
overlay tree:
• Bandwidth: This metric measures the application level
throughput at the receiver, and is an indicator of the quality
of received video.
• Latency: This metric measures the end-to-end delay from
the source to the receivers, as seen by the application. It
includes the propagation and queuing delays of individual
overlay links, as well as queueing delay and processing over-
head at end systems along the path. We ideally wish to
measure the latency of each individual data packet. How-
ever, issues associated with time synchronization of hosts
and clock skew adds noise to our measurements of one-way
delay that is diﬃcult to quantify. Therefore, we choose to
estimate the round trip time (RTT). By RTT, we refer to the
time it takes for a packet to move from the source to a recipi-
ent along a set of overlay links, and back to the source, using
the same set of overlay links but in reverse order. Thus, the
RTT of an overlay path S-A-R is the time taken to traverse
S-A-R-A-S. The RTT measurements include all delays as-
sociated with one way latencies, and are ideally twice the
end-to-end delay.
• Resource Usage: This metric deﬁned in [3] captures the
network resources consumed in the process of delivering data
to all receivers. The resource usage of an overlay tree is the
sum of the costs of its constituent overlay links. The cost of
an overlay link is the sum of the costs of the physical links
that constitute the overlay link. In our evaluation, we as-
sume the cost of a physical link to be the propagation delay
of that link, guided by the intuition that it is more eﬃcient
use of network resources to use shorter links than longer
ones. For example, in Figure 1, the cost (delay) of physical
link R1 − R2 is 25, the cost of the overlay link A − C is 27,
and the resource usage of the overlay tree is 31.
We deﬁne the Normalized Resource Usage of an overlay
tree as the ratio of its resource usage to the resource usage
with IP Multicast. The resource usage with IP Multicast
is the sum of the costs (delays) of the physical links of the
native IP Multicast tree used in delivering data to the re-
ceivers. In our evaluation, we determine the IP Multicast
tree based on the unicast paths from the source to each re-
ceiver. This is the tree that the classical DVMRP protocol
[4] would construct (assuming Internet routing is symmetri-
cal). We derive the physical links of this IP Multicast tree,
as well as the delays of these links, by doing a traceroute
from the source to each receiver.
Bandwidth and latency are metrics of the application level
performance that an overlay provides, while resource usage
is a measure of the network costs incurred. The objective
of our evaluation is to understand the qualities of the over-
lay tree that diﬀerent schemes create with respect to these
metrics. For a metric such as resource usage, it is easy to
summarize the quality of the overlay produced. However, it
is much more diﬃcult to summarize the latency and band-
width performance that a number of diﬀerent hosts observe
with a few simple metrics. One approach is to present the
mean bandwidth and latency, averaged across all receivers.
Indeed, we do use this technique in Section 6.1. However,
this does not give us an idea of the distribution of perfor-
mance across diﬀerent receivers.
A simple approach to summarizing an experiment is to
explicitly specify the bandwidth and latencies that each in-
dividual receiver sees. Although the set of hosts and source
transmission rate are identical, a particular scheme may cre-
ate a diﬀerent overlay layout for each experimental run.
While an individual host may observe vastly diﬀerent perfor-
mance across the runs, this does not imply that the various
overlays are of any diﬀerent quality. Therefore, we need
metrics that capture the performance of the overlay tree as
a whole.
Let us consider how we summarize an experiment with
regard to a particular metric such as bandwidth or latency.
For a set of n receivers, we sort the average metric value of
the various receivers in ascending order, and assign a rank
to each receiver from 1 to n. The worst-performing receiver
is assigned a rank of 1, and the best-performing receiver is
assigned a rank of n. For every rank r, we gather the re-
sults for the receiver with rank r across all experiments, and
compute the mean. Note that the receiver corresponding to
a rank r could vary from experiment to experiment. For
example, the result for rank 1 represents the performance
that the worst performing receiver would receive on average
in any experiment.
In addition to the mean bandwidth or latency for a given
rank, we also calculate the standard deviation of this mea-
sure. Variability in performance may occur due to two rea-
sons. First, it arises due to a variation in quality of overlay
trees that a particular scheme produces across diﬀerent runs.
For example, a scheme may produce trees where every re-
ceiver gets good performance in a particular run, but many
receivers get bad performance in another run. Second, vari-
ability may also occur due to changes in Internet conditions
(such as time of day eﬀects). Thus, potentially no overlay
may be able to provide good performance at a given time.
However, our results in Section 6 demonstrate that some
schemes are able to keep the standard deviation low. This
indicates that the standard deviation is a reasonable mea-
sure of the variability in performance with a scheme itself.
The metrics above capture the quality of the overlay a
scheme constructs. We use a fourth metric, the Protocol
Overhead, to capture the the overhead incurred by a scheme
while constructing overlays. This metric is deﬁned as the
ratio of the total bytes of non-data traﬃc that enters the
network to the total bytes of data traﬃc that enters the
network. The overhead includes control traﬃc required to
keep the overlay connected, and the probe traﬃc and active
bandwidth measurements involved in the self-organization
process.
5.4
Implementation Issues
The experiments are conducted using unoptimized code
Implementation overhead and
running at the user level.
delays at end systems could potentially be minimized by
pushing parts of the implementation in the kernel, and by
optimizing the code. We have used TFRC [5] as the under-
lying transport protocol on each overlay link, as discussed
in Section 3. TFRC is rate-controlled UDP, and achieves
TCP-friendly bandwidths. It does not suﬀer delays associ-
ated with TCP such as retransmission delays, and queueing
delays at the sender buﬀer.
6. EXPERIMENTAL RESULTS
We begin by presenting results in a typical experiment
run in Section 6.1. Section 6.2 provides a detailed compari-
son of various schemes for constructing overlays with regard
to application level performance, and Section 6.3 presents
results related to network costs.
6.1 Results with a Typical Run
The results in this section give us an idea of the dynamic
nature of overlay construction, and how the quality of the
overlay varies with time. Our experiment was conducted on
a week-day afternoon, using the Primary Set of machines
and at a source rate of 1.2 Mbps. The source host is at
UCSB.
Figure 3 plots the mean bandwidth seen by a receiver,
averaged across all receivers, as a function of time. Each
vertical line denotes a change in the overlay tree for the
source UCSB. We observe that it takes about 150 seconds for
the overlay to improve, and for the hosts to start receiving
good bandwidth. After about 150 seconds, and for most of
the session from this time on, the mean bandwidth observed
by a receiver is practically the source rate. This indicates
that all receivers get nearly the full source rate throughout
the session.
Figure 4 plots the mean RTT to a receiver, averaged across
all receivers as a function of time. The mean RTT is about
100 ms after about 150 seconds, and remains lower than this
value almost throughout the session.
Figures 3 and 4 show that in the ﬁrst few minutes of the
session, the overlay makes many topology changes at very
frequent intervals. As members gather more network in-
formation, the quality of the overlay improves over time,
and there are fewer topology changes. In most of our runs,
we ﬁnd that the overlay converges to a reasonably stable
structure after about four minutes. Given this, we gather
bandwidth and RTT statistics after four minutes for the rest
of our experiments. We present ideas for how convergence
time may be minimized in the future in Section 8.
The ﬁgures above also highlight the adaptive nature of our
scheme. We note that there is a visible dip in bandwidth,
and a sharp peak in RTT at around 460 seconds. An analysis
of our logs indicates that this was because of congestion on
a link in the overlay tree. The overlay is able to adapt
)
s
p
b
K
(
i
t
h
d
w
d
n
a
B
1400
1200
1000
800
600
400
200
0
0
)
%
(
e
g
a
t
n
e
c
r
e
P
e
v