[2]
 在Mahout Core 0.3 API中，该包一共包含1个接口和8个类，在0.7版本中，对其进行了简化。
13.4 Mahout中的频繁模式挖掘
 13.4.1 什么是频繁模式挖掘
提到关联规则人们头脑中首先闪过的便是“尿布与啤酒的故事”。首先我们先来介绍一下什么是“尿布与啤酒的故事”。该故事是美国沃尔玛超市的真实案例。沃尔玛超市为了了解顾客在超市的消费习惯，从而对消费者的购物数据进行分析。他们将消费者的一次购物消费假设成为一个购物篮，通过对购物篮的分析他们发现，尿布与啤酒竟然经常同时出现。该现象看似非常奇怪，然而它却揭示了美国人背后的消费习惯：很多男子经常要帮妻子为婴儿购买尿布，而同时，他们中的大多数又会顺便购买自己喜爱的啤酒。
在上述例子中，尿布与啤酒的经常性一同出现便可以认为是一组频繁模式。频繁模式挖掘是数据挖掘研究中的一个重要课题，它是关联规则、相关性分析、序列模式、因果关系等许多重要数据挖掘任务的基础。因此，频繁模式挖掘有着广泛的应用，例如购物篮数据分析、交叉购物、DNA序列分析、预测分析等。
比较经典的频繁模式挖掘包括Apriori算法、FPGrowth算法、AGM算法、PrefixSpan算法等。
13.4.2 Mahout中的频繁模式挖掘
Mahout中实现了FPGrowth算法，FPGrowth算法英文全称为“Frequent Pattern Growth Algorithm”，即“频繁模式增长算法”。关于算法具体内容可参看Mining frequent patterns without candidate generation论文
[1]
 。该算法包括如下两个主要步骤：
1）构建一棵频繁模式树，即FP树；
2）挖掘FP树，找出频繁项。
我们可以通过Mahout Shell的“$MAHOUT_HOME/bin/mahout fpg”命令来使用FPGrowth进行频繁模式挖掘，首先我们对该命令的可选参数进行简要介绍，如表13-4所示。
更多参数大家可以通过输入“mahout fgp-h”来查看。下面我们来介绍具体的操作。
1.数据获取
在执行算法之前我们首先需要获取算法操作的数据
[2]
 。在“$MAHOUT_HOME/core/src/test/resources/retail.dat”位置，Mahout为我们提供了一组零售商销售记录数据，该数据记录的项之间通过空格进行划分。该数据较小，共包含88162条记录，用于测试使用。如果想要使用更大的数据大家可以从下面的链接中下载：http：//fimi.cs.helsinki.fi/data/。
2.执行算法
通过“-method”参数可以指定算法运行的模式，下面我们在不同模式下运行处理数据集来比较算法的效率。
首先，我们在sequential模式下执行算法，如下所示：
bin/mahout fpg\
-i core/src/test/resources/retail.dat\
-o patterns\
-k 50\
-method sequential\
-regex'[\]'\
-s 2
可以看到该算法的执行时间为：
12/06/07 11：04：11 INFO driver.MahoutDriver：Program took 2193567 ms（Minutes：36.55945）
然后，我们在MapReduce模式下执行该算法。在执行算法之前我们首先需要将数据复制到HDFS中，然后运行算法。如下所示：
bin/mahout fpg\
-i/user/hadoop/retail.dat\
-o patterns2\
-k 50\
-method mapreduce\
-regex'[\]'\
-s 2
可以看到该算法的执行时间为：
12/06/07 20：19：05 INFO driver.MahoutDriver：Program took 358158 ms（Minutes：5.9693）
相比于sequential模式，算法执行效率提高了数倍，这恰恰是分布式的优势。在数据量更大的情况下，该优势将更加明显。
3.查看结果
FPGrowth算法的执行结果会以SequenceFile的形式存储在frequentpatterns目录下，我们可以通过下面命令来查看运行的结果：
bin/mahout seqdumper\
-s patterns2/frequentpatterns/part-r-00000\
-c
上述命令中，-s指定的是输入文件路径，-c用于统计结果记录的个数，输出结果如下所示：
mahout seqdumper-s patterns2/frequentpatterns/part-r-00000-c
Input Path：patterns2/frequentpatterns/part-r-00000
Key class：class org.apache.hadoop.io.Text Value Class：class org.apache.mahout.
fpm.pfpgrowth.convertors.string.TopKStringPatterns
Count：14246
12/06/07 19：38：44 INFO driver.MahoutDriver：Program took 2576 ms（Minutes：
0.04293333333333333）
该结果与sequential模式下输出结果相同，大家可以对其进行验证。
[1]
 http：//citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.4436＆rep=rep1＆type=pdf。
[2]
 以svn方式下载的Mahout版本中该示例数据。
13.5 Mahout中的聚类和分类
 13.5.1 什么是聚类和分类
在日常生活中经常会有重复的事情发生，人们会把自己遇到的事情和记忆中的事情关联起来。例如，糖果使人们想起是甜味，因此，人们会把具有甜味的食物归类为甜食。即使人们没有甜食的概念，人们也能把甜的食物进行归类。潜意识里，人们能够自然地将甜与苦进行分类。生活中与此类似的现象还有很多，这些现象就是分类。
下面将用一个实际的例子来介绍到底什么是分类。假设在一个两岁的宝宝面前摆放一些水果，并告诉他红色圆的是苹果，橘黄色圆的是橘子。然后，拿一个又红又大的苹果问宝宝是不是苹果，宝宝回答是，这就是一个简单的分类过程。在这个过程中主要涉及两个阶段：第一个是建立模型阶段，第二个是使用模型阶段。建立模型就是告诉两岁的宝宝具有何种特征的水果是苹果，具有何种特征的水果是橘子；使用模型就是问宝宝又红又大的是不是苹果。
在日常的生活中除了前面介绍的分类外，还有很多种不同类型的聚类。下面同样用一个实际的例子来介绍聚类。假设你是一个藏书众多的图书馆馆长，但图书馆中的书是混乱的，没有任何顺序。来到图书馆的读者不得不找遍所有的书籍才能发现自己想要看的书。这个寻找书的过程非常缓慢。对于任何一个读者来说，这都是一个很头痛的问题。如果图书按照书名的首字母进行排列，那么在知道书名的情况下寻找一本书将会变得非常容易。如果图书按照主题进行摆放，图书查询也会变得简单易行。将众多的图书按照主题进行排列就是一个聚类的过程。在刚刚接触这个工作的时候，你不知道这些书会有多少种主题，比如哲学、文学等，也许还会有一些你从未听说过的主题。要完成这些任务，你首先要把它们排成一列，逐本查阅。当遇到与之前的书主题相似，就回到前面将它们放在一起，归为一类。当读完所有的书时，一遍聚类便完成了，众多的书籍也被分成了一些类。如果你觉得第一遍聚类的结果不够精细，你可以进行第二遍聚类，直到自己满意为止。
这就是聚类，在下面的章节中，我们将会详细地介绍Mahout中的分类和聚类。
13.5.2 Mahout中的数据表示
生活中的数据会以各种各样的形式存储，Mahout中的数据也会以其固定的形式表示。在Mahout中，数据将会以向量的形式进行存储。
多数人对向量这个词并不陌生。在不同的领域，向量具有不同的实际意义。在物理中，向量用来表示力的大小和方向，或者一个移动物体的速度。在数学中，一个向量表示空间中的一个点。虽然它们代表的意义不同，但它们表示的形式是相同的。在二维空间中，所有的向量都表示成诸如（5，6）的形式，每一维中有一个数字。当计算这个二维向量时，人们常称第一个维度为X，第二个维度为Y。但是在现实生活中，一个向量可以是多维度的。按照顺序，向量的每一个维度依次被称为0维、1维、2维……
如上所述，向量是按照维度排列的一系列有序的值。因此，你可能已经想到在程序设计语言中用一维数组来表示向量。使用这种方式表示向量，数组的第i项刚好是向量的第i个维度的值。这是一种很好的表示向量的方法，称为密集向量表示法。
在现实生活中，一个具有很高维度的向量经常会在很多维度上没有值。这里的没有值就是程序设计当中空的概念，在向量中它会表示为0。在物理和数学领域，无论是高维度向量还是包含很多0的向量都是很少见的。但在分类算法中这种情况很常见。
使用数组表示这种向量效率太差。数组将会包含很多个0，偶尔会有一个非0值。舍弃众多的0值，单独表示非0值是一种很合理的想法。当处理数百万维度带有很多0值的向量时，密集向量表示法的弊端变得很明显。
在这种情况下，Mahout引入了稀疏向量，将非0值所在的维度与该维度的值做映射。这可以通过Java中的Map实现。当非0值比较少时，这种存储方式比使用基于数组的稠密存储更具优越性。但使用这种方式，程序需要更多的内存空间。
在Mahout中，有关向量表示的类有三个。它们分别是稠密向量（DenseVector）、随机访问稀疏向量（RandomAccessSparseVector）和序列访问稀疏向量（SequentialAccessSparseVector）。
稠密向量由一个double型的数组实现。当向量具有很少的非0值时，这种向量表示法的效率很高。它允许快速访问向量所在任何维度的值，并且能够快速按序遍历向量的所有维度。
在随机访问向量类中，向量的值存储在类似于HashMap的结构中，键是int型、值是double型的。只有维度上的值非0，该维度值才会被存储。当一个向量的一些维度值非0时，用随机访问向量方式表示向量比用稠密向量表示法具有更高的内存使用效率。但是访问维度值的速度和按序遍历所有维度值的速度比较慢。
序列访问向量使用int和double的并行数组表示向量。因此，使用它按序遍历整个向量的各个维度是很快的。但是随机插入和查询某一维度的值时速度要慢于随机访问向量。
这三种表示向量的方式使得Mahout的算法能够按照数据特性、数据访问方式实现。具体使用哪种表示方法是按照算法的特性进行选择的。如果算法具有很多对向量值的随机插入和更新，就应该选择稠密向量或随机访问向量来表示向量。因为这两个向量具有快速随机访问的特性。而对于需要重复计算向量大小的K-Means聚类算法，选择序列访问向量比选择随机访问向量好。
13.5.3 将文本转化成向量
讨论完如何存储向量，下面我们开始讨论如何将文本转化成向量。在信息时代，文本文件的数量呈爆炸式增长。仅Google搜索引擎的索引就有200亿的Web文档。文本数据是海量的，这些海量数据中蕴含着大量的知识。公司或机构可以使用诸如聚类、分类的机器学习算法去发现这些知识。学习用向量表示文本是从海量数据中发现知识的第一步。
向量空间模型（VSM, Vector space model）是最常用的相似度计算模型。Mahout中对文本的聚类使用了这种技术。什么是向量空间模型？下面做一个简单的介绍。
假设共有10个词w1
 ，w2
 ，……，w10
 ，5篇文章d1
 、d2
 、d3
 、d4
 和d5
 。统计所得的词频表如表13-5所示。
这个词频表就是空间向量模型。对于任意的两篇文档，当要计算它们的相似度时，可以选择计算两个向量的余弦值。如果余弦值为1，则说明两篇文档完全相同；如果余弦值为0，则说明两篇文档完全不同。总之，在[0，1]内余弦值越大，两篇文章相似度越大。除了计算余弦值以外，还有其他的方法测量两篇文章的相似度，这里不作介绍。
在Mahout下处理的数据必然是海量数据。待处理的文本包含的所有单词就是例子中的单词w，待处理的文本文件就是相应的d。可以想象，待处理文本所包含的单词量是巨大的，因此，文本向量的维度也是巨大的。示例中的具体数值是某个单词在特定文章中出现的次数，称为词频（term frequency）。例如，表中的1代表单词w1
 在d1
 文档中出现一次，其词频为1。
在一些简单的处理方法中，可以只通过词频来计算文本间的相似度，不过当某个关键词在两篇长度相差很大的文本中出现的频率相近时，会降低结果的准确性。因此通常会把词频数据正规化，以防止词频数据偏向于关键词较多、即较长的文本。如某个词在文档d1
 中出现了100次，在d2
 中出现了100次，仅从词频看来，这个词在这两个文档中的重要性相同，然而，再考虑另一个因素，就是d1
 的关键词总数是1000，而d2
 的关键词总数是100000，所以从总体上看，这个词在d1
 和d2
 中的重要性是不同的。正规化处理的方法是用词频除以所有文档的关键词总数。
当仅使用词频来区分文档时，还会遇到这样一个问题。众所周知，一篇文章会包含很多诸如一、二、你、我、他等的单词，并且这些词语会多次出现。很明显，无论使用何种距离来测算两篇文章的相似度，这些经常出现的词汇都会对结果起到很大的负面影响。但这些词语并不能区分两份文档，相似性判断也因此变得不再准确。把文档按照相似性进行合理的聚类就更不可能了。为了解决这个问题，人们使用了TF-IDF（Term Frequency-Inverse Document Frequency）技术。
TF-IDF是一种统计方法，用以评估一个字词对于一个文件集或一个语料库中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF的主要思想是，如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或短语具有很好的类别区分能力，适合用来分类。TF-IDF实际上是TF*IDF, TF代表词频（Term Frequency），表示词条在文档中出现的频率；IDF代表反文档频率（Inverse Document Frequency），IDF的主要思想是，如果包含词语w的文档越少，IDF越大，则说明词语w具有很好的类别区分能力。
13.5.4 Mahout中的聚类、分类算法
Mahout目前已经实现了Canopy聚类算法、K-Means聚类算法、Fuzzy K-Means聚类算法、Dirichlet过程聚类算法等众多聚类算法。除此之外，Mahout还实现了贝叶斯（Bayes）分类算法。这里主要介绍简单且应用广泛的K-Means聚类算法和贝叶斯分类算法。
K-Means聚类算法能轻松地对几乎所有的问题进行建模。K-Means聚类算法容易理解，并且能在并行计算机上很好地运行。学习K-Means聚类算法，能更容易理解聚类算法的缺点，以及其他算法对于特定数据的高效性。
K-Means聚类算法的K是聚类的数目，在算法中会强制要求用户输入。对于将新闻聚类成诸如政治、经济、文化等大类，可以选择10到20之间的数字作为K。因为这种顶级的类别数量是很小的。如果要对这些新闻详细分类，选择50到100之间的数字也是没有问题的。假设数据库中有一百万条新闻，如果想把这一百万条新闻按照新闻谈论的内容进行聚类，则这个聚类数目远远大于之前的聚类数目。因为每个聚类中的新闻数量不会太大。这就要求选择一个诸如10 000的聚类数值。聚类数值K的取值范围不定，它既可以小至几个，也可以大至几万个。这就对算法的伸缩性提出了很高的要求，而Mahout下实现的K-Means聚类算法就具有很好的伸缩性。
K-Means聚类算法主要可以分为三步。第一步是为待聚类的点寻找聚类中心；第二步是计算每个点到聚类中心的距离，将每个点聚类到离该点最近的聚类中去；第三步是计算聚类中所有点的坐标平均值，并将这个平均值作为新的聚类中心点。反复执行第二步，直到聚类中心不再进行大范围移动，或者聚类次数达到要求为止。
假设有n个点，需要将它们聚类成K个组。K-Means算法会以K个随机的中心点开始。算法反复执行上文中提到的第二步和第三步，直至终止条件得到满足。接下来以9个点为例，配以相应图示介绍K-Means算法。
在聚类前，首先在二维平面中随机选择9个点，坐标分别为（7，8）、（12，1）、（13，6）、（13，13）、（13，19）、（14，5）、（17，16）、（19，20）、（20，7）。
1.第一次聚类
1）系统首先选取前3个点（7，8）、（12，1）、（13，6）作为聚类中心，然后计算每个点到聚类中心的距离，该点距离哪个聚类中心的距离最小就归属于哪个聚类中心。经过计算，点（7，8）、（13，19）为1个聚类，点（12，1）为1个聚类，点（13，6）、（13，13）、（14，5）、（17，16）、（19，20）、（20，7）为1个聚类，如图13-2所示。
图 13-2 未聚类的九个点
2）更新聚类的聚类中心，新的聚类中心的值为聚类中所有成员的平均值。聚类（7，8）、（13，19）的新聚类中心为（10.0，13.5），聚类（12，1）的新聚类中心仍为（12.0，1.0），聚类（13，6）、（13，13）、（14，5）、（17，16）、（19，20）、（20，7）的新聚类中心为（16.0，11.2），如图13-3所示。
图 13-3 一次聚类后的结果
2.第二次聚类
1）根据前面生成的聚类中心（10.0，13.5）、（12.0，1.0）、（16.0，11.2）重新计算每个点和聚类中心点之间的距离，根据计算出的距离对该点进行聚类。结果点（7，8）、（13，13）、（13，19）为1个聚类，点（12，1）、（13，6）、（14，5）为1个聚类，点（17，16）、（19，20）、（20，7）为1个聚类。
2）更新聚类的聚类中心，聚类（7，8）、（13，13）、（13，19）的新聚类中心为（11.0，13.3），聚类（12，1）、（13，6）、（14，5）的新聚类中心仍为（13.0，4.0），聚类（17，16）、（19，20）、（20，7）的新聚类中心为（18.7，14.3），如图13-4所示。
3.第三次聚类
根据上一步来看，聚类结果没有发生变化，满足收敛条件，K-Means聚类结束，如图13-5所示。
介绍完K-Means聚类算法，下面开始介绍贝叶斯（Bayes）分类算法。贝叶斯（Bayes）分类算法是一种基于统计的分类方法，用来预测某个样本属于某个分类的概率有多大。贝叶斯（Bayes）分类算法是基于贝叶斯定理的分类算法。
贝叶斯分类算法有很多变种。在这里主要介绍朴素贝叶斯分类算法。何谓朴素？所谓朴素就是假设各属性之间是相互独立的。经过研究发现，大多数情况下，朴素贝叶斯分类算法（Naïve Bayes Classifier）在性能上和决策树（Decision Tree）、神经网络（Netural Network）相当。当针对大数据集的应用时，贝叶斯分类算法具有方法简单、高准确率和高速度的优点。但事实上，贝叶斯分类算法也有其缺点。缺点就是贝叶斯定理假设一个属性值对给定类的影响独立于其他属性的值，而此假设在实际情况中经常是不成立的，因此其分类准确率可能会下降。
图 13-4 二次聚类后结果
图 13-5 第三次聚类的结果
朴素贝叶斯分类算法是一种监督学习算法，使用朴素贝叶斯分类算法对文本进行分类，主要有两种模型，即多项式模型（multinomial model）和伯努利模型（Bernoulli model）。Mahout实现的贝叶斯分类算法使用的是多项式模型。对算法具体内容感兴趣的读者可以阅读http：//people.csail.mit.edu/jrennie/papers/icml03-nb.pdf上的论文
[1]
 。本书将以一个实际的例子来简略介绍使用多项式模型的朴素贝叶斯分类（Naïve Bayes Classifier）算法。
给定一组分类号的文本训练数据，如表13-6所示。
给定一个新的文档样本“中国、中国、中国、东京、日本”，对该样本进行分类。该文本属性向量可以表示为d=（中国，中国，中国，东京，日本），类别集合Y={是，否}。类别“是”下共有8个单词，“否”类别下面共有3个单词。训练样本单词总数为11。因此P（是）=8/11，P（否）=3/11。
类条件概率计算如下：
P（中国|是）=（5+1）/（8+6）=6/14=3/7；
P（日本|是）=P（东京|是）=（0+1）/（8+6）=1/14；
P（中国|否）=（1+1）/（3+6）=2/9；
P（日本|否）=P（东京|否）=（1+1）/（3+6）=2/9。
上面4条语句分母中的8，是指“是”类别下训练样本的单词总数，6是指训练样本有中国，北京，上海，澳门，东京，日本，共6个单词，3是指“否”类别下共有3个单词。有了以上的类条件概率，开始计算后验概率：
P（是|d）=（3/7）3×1/14×1/14×8/11=108/184877≈0.00058417；
P（否|d）=（2/9）3×2/9×2/9×3/11=32/216513≈0.00014780。
因此，这个文档属于类别中国。这就是Mahout实现的贝叶斯（Bayes）分类算法的主要思想。
[1]
  Trackling the Poor Assumptions of Naive Bayes Text classifiers.
13.5.5 算法应用实例
下面我们将对如何使用Mahout进行聚类和分类进行具体介绍，其中聚类算法以K-Means为例，分类算法以贝叶斯（Bayes）为例。
1.K-Means聚类
在Mahout中运行K-Means聚类算法非常简单。对于不同的数据主要有以下三个步骤。