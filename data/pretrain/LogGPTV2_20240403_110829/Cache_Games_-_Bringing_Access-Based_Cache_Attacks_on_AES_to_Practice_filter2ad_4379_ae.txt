consisting of four parts: the assembly instructions, the out-
puts of the block, its inputs, and a list of clobbered registers.
These parts are separated by colons. For ease of presentation,
we describe these blocks in a different order in the following.
• Inputs: Only one input is given to the assembly block,
namely a position in the lookup table. The given
command speciﬁes to store this position into the register
%esi. The lookup table is traversed during the outer
for loop, starting at the very beginning in the ﬁrst
iteration.
• Assembly
Instructions:
The
i.e.,
ﬁrst
instruction,
xor%eax, %eax is a standard idiom to set
the
register %eax to zero, by XORing it with its
own content. Then,
the cpuid instruction stores
some information about
the CPU into the registers
%eax,%ebx,%ecx,%edx. We do not need this
information in the following. The only purpose of
these two instructions is the side effect of the latter:
namely, cpuid is a serializing instruction,
it
logically separates the instructions before and after
cpuid, as the CPU must not execute speculatively
over such an instruction. Then, a 64 bit time stamp
is stored into %edx:%eax by using the rdtsc
instruction. The most signiﬁcant bits of
this time
stamp are discarded, and the least signiﬁcant bits
(which are stored in %eax) are moved to %edi to
preserve them during the following operations. Having
this, the procedure accesses data at address %esi in
the main memory and stores it to %ebx. Similar to the
beginning, the CPU is forced to ﬁnish this instruction,
before again a time stamp is stored to %eax, and
the accessed data is ﬂushed from the cache again by
ﬂushing its cache line using clflush(%%esi).
• Outputs: In each iteration, the least signiﬁcant bits of
both time stamps are handed back: the content of the
register %%eax is stored to t2, and that of %%edi is
stored to t1.
• Clobbered Registers: The last block describes a list
of clobbered registers. That is, it tells the compiler
which registers the assembly code is going to use and
modify. It is not necessary to list output registers here,
as the compiler implicitly knows that they are used.
The remaining cc register refers to the condition code
register of the CPU.
Now, depending on the difference of t1 and t2, the
procedure decides whether the accesses resulted in a cache
hit. These cache hits and misses describe whether or not
the victim processes accessed the corresponding cache line
in its last activation with high probability. The THRESHOLD
499
of 200 CPU cycles has been found by empirical testing. Note
here that the serializing property of the cpuid instructions
forces the CPU to always execute the same instructions
to be timed between two rdtsc instructions, disregarding
superpipelining and out-of-order instruction scheduling.
These steps are performed for the whole lookup table,
starting at the beginning of the table in the memory (i=0)
and counting up in steps of size of the cache line, as this
is the highest precision that can be monitored. The number
n_hits of cache hits and a bitmap bitmap containing
information about where cache hits and where cache misses
occurred are then handed back to the caller of the function.
D. Using Neural Networks to Handle Noise
Naturally,
the measurements obtained using the tech-
niques from §IV-B and §IV-C are not perfect, but overlaid
with noise. This is because not only the victim and the spy
process, but also other processes are running concurrently
on the same system. They also perform memory accesses,
which can cause ﬂawed identiﬁcations of cache hits and
misses. Also, sometimes the spy process will be able to
advance by more than only one memory access at a time.
Further, massive noise can be caused by prediction logic of
the CPU, cf. §IV-D4.
Thus, ﬁltering out noise is a core step in our attack, which
we achieve by using artiﬁcial neural networks (ANNs). In
the following, we describe the conﬁguration and training of
the ANNs we use.
1) Introduction to Artiﬁcial Neural Networks: An arti-
ﬁcial neural network [36]–[39] is a computational model
inspired by the workings of biological neurons. Simply
speaking, neurons are interconnected nerve cells, commu-
nicating via electrical signals. They ﬁre a signal when their
summed inputs exceed an activation threshold.
An ANN can be represented as a directed graph with a
value attached to each of its nodes. Some of the nodes are
labeled as input or output nodes. Except for the input nodes,
the value of each node is computed from the values attached
to its predecessors. For a node i, let zi denote the value
associated with i and for an edge from j to i, let wij be
an associated weight. The weight wi0 does not belong to
an edge, it only serves to bias the sum that is fed into i’s
activation function σ, which typically is a sigmoid function.
Then zi is computed as
(cid:16)
(cid:88)
(cid:17)
zi = σ
wi0 +
wijzj
.
Let x be the vector of values assigned to the input nodes,
y the vector of values computed for the output nodes and
w the vector of weights. With these, the network computes
a function y = fw(x). By choosing an appropriate network
structure, activation function and weights, a neural network
can be used to approximate an arbitrary target function.
Usually, the network structure and activation function are
ﬁxed ﬁrst. Finding values for the weights is then formulated
500
i.e., one searches for a w
as an optimization problem,
minimizing the error, which is the distance between fw and
the target function (where difference is, e.g., the mean square
error). This problem can seldom be solved algebraically and
therefore nonlinear optimization techniques are used in a
training phase to ﬁnd sufﬁciently good weights. We refer to
standard literature on artiﬁcial neural networks for detailed
discussions [38], [39].
2) Overview of our ANNs: We use two neural networks to
remove noise from our measurements: the many stray cache
hits are tackled in the ﬁrst and inaccuracies of the DoS on
the scheduler in the second.
Measurements are made as described in §IV-C and rep-
resented by a rectangular bitmap. It is generated by using
the result of each call to measureflush() in the spy
process as one column concatenating them, so that columns
are sorted chronologically from left to right. This is shown
in Figure 6(a), hits and misses recorded as 1 and 0 are shown
by black and white pixels in the bitmap. Because of the size
of the lookup table (2 kB) and the size of each cache line
(26 = 64 B), 32 addresses have to be considered. Here, 61
activations are shown and therefore the bitmap’s size is 61
by 32 pixels.
The ﬁrst network outputs the probability that at a given
pixel in the bitmap a memory access was actually performed
in the victim since the last measurement. For this, a rectan-
gular area centered on the pixel of interest is used as input
vector. We use a square of edge length 23 ﬁlled with zeros
where it extends beyond the borders of the bitmap. The
probabilities generated for all pixels in the input bitmap are
again organized in columns.
We then use a second ANN to estimate how many
memory accesses the victim performs between two mea-
surements. This is important for accurately estimating the
timeline of memory accesses. It
is not guaranteed that
exactly one memory access retires in the victim between
two measurements because of inaccuracy of the scheduler
DoS. Sometimes no memory access may retire, because the
victim did not run at all or for too few instructions. At other
times, several memory accesses retire, because the victim
was interrupted too late. The input of this second ANN is
the sum of probabilities over one column produced by the
ﬁrst ANN. Its output is a value in R, which is used to resize
the width of the corresponding column of probabilities. After
resizing and concatenating the columns in order, the result
is a map of probabilities shown in Figure 6(b). In one step
in the horizontal direction, one memory access is performed
in the victim.
3) Parameters and Structure of our ANNs: We now give
a concise summary of the design of our neural networks,
whereas we assume that the reader is familiar with ANNs.
The ﬁrst ANN is a multilayer neural network. It has
232 = 529 input nodes, one layer of 30 hidden nodes and
one output node. Every hidden node has incoming edges
Figure 7. Map of weights for the hidden layer in the ﬁrst neural network. Each square of 23 by 23 pixels represents the weights for the edges from all
input nodes to one hidden node. To compute the weighted sum of inputs for a hidden node one can think of placing one of these squares on top of the
bitmap of inputs, centered on the pixel of interest. Then every input value is multiplied by the value indicated by the pixel above it. Darker shades indicate
negative weights and lighter shades positive weights.
(a) Input bitmap of measurements.
(b) Output map of probabilities.
Figure 6.
Input and output of our artiﬁcial neural networks. The input
is given by a bitmap, where black squares indicate observed cache hits.
One step in the horizontal direction corresponds to the time between two
measurements. The output of the ﬁrst ANN are probabilities that memory
accesses were actually performed by the victim process in a cache line.
Higher probabilities are indicated by a darker shade. Combined with the
estimation of the second network how many accesses were performed
between two measurements,
this gives a map where one step in the
horizontal direction corresponds to one memory access in the victim.
tweaking was necessary when weights either approached
zero or very large values. Then the new generation was
trained on batches from a training set of about 230 samples
and ﬁnally on the whole set. The L-BFGS algorithm was
used for numerical optimization. This was repeated until
we arrived at a network where neither manual tweaking nor
adding more hidden nodes improved the error. The resulting
weights for the hidden nodes are shown in Figure 7. It can
be seen that the network seems to detect patterns centered
on the same row or the same column as the pixel of interest.
Also, the patterns most often consist of a horizontal line of
pixels with the same value, starting from the left up to one
distinguished pixel. This pixel is surrounded by pixels of
differing value and to the right of it the line continues with
pixels of this value.
The second network is used to estimate the number n
of memory accesses performed between two measurements
from the sum of probabilities x calculated by the ﬁrst ANN
for one column. For sake of brevity we skip the elaborate
description of its structure, because it can easily be replaced
by any other method for function approximation. It is trained
to approximate the estimator(cid:98)n(x) that minimizes the mean
squared error E[((cid:98)n(x) − n)2] over all training samples.
4) Sources of Noise: As can be seen in Figures 6 and 7,
the noise obtained from real measurements is not entirely
unstructured. We now brieﬂy explain the sources of this
structure.
from all input nodes and one outgoing edge to the output
node. The activation functions are tanh for all hidden nodes
and 1/(1 + e−x) − 1
2 for the output node. Note that the
latter activation function is tanh rescaled to yield an output
in the interval [0, 1], which can directly be interpreted as
probability. We picked the cross entropy as error function
used during the training phase. This allows for a faster
training than using the mean square error [39]. Further, the
cross entropy also has preferable numerical properties in our
speciﬁc case.
The training was done in generations. A new generation
was generated by taking the best one or two networks
from the previous generation (or an empty ANN at ﬁrst)
as a parent. Then children were created by adding a few
randomly initialized hidden nodes. Sometimes also manual
The vertical lines in Figure 6(a) stem from prediction log-
ics of the cache, which detects linear memory access patterns
and prefetches data accordingly. Thus, when the encryption
process accesses addresses x and x+δ, sometimes the cache
lines containing x + 2δ and x + 3δ will be ﬁlled as well.
The horizontal
lines can be explained by speculative
execution. On a high level, if parts of the CPU are idle, it
looks ahead in the instruction queue, and computes results
in advance. Thus, the results are already available when
the according instruction is to be retired and the latency
of the instruction execution is hidden from the user. In this
case, memory loads are issued to the cache as soon as the
addresses are known and cache lines are ﬁlled. But before
the data from memory can actually be used and written into
an architecturally visible register, an interrupt preempts the
501
program. This also explains why most of the horizontal lines
in Figure 6(a) end in a real memory access in Figure 6(b).
The remaining noise is due to other processes running
concurrently on the same system and inaccuracy of the DoS
on the scheduler.
E. Results
In the following we present measurement results that
allows to assess the effectiveness of our attack in practice.
Our spy process was speciﬁed to start 250 threads and to
monitor 100 encryptions. The following numbers, which
characterize the phases of our attack, were obtained on our
test platform speciﬁed in §I-B.
• Running time: Performing 100 AES encryptions (i.e.,
encrypting 1.56 kB) takes about 10 ms on our platform.
This running time blows up to 2.8 seconds, when
memory accesses are monitored by the spy process.
We believe that this delay is sufﬁciently small for the
attack to go unnoticed. In fact, the user might attribute
the delay to, e.g., high disk activity or network trafﬁc.
• Denoising: The obtained measurements are ﬁrst reﬁned
by applying our neural networks. This step approx-
imately takes 21 seconds when running as a normal
process on the target machine.
• Preparing key search: Next, the a posteriori probabili-
ties of all partial key column candidates are computed
by analyzing their frequencies, cf. §III-B. This step
approximately takes 63 seconds.
• Key search: Finally, the correct key is sought as ex-
plained in §III-C. The runtime of this step varies
between 30 seconds and 5 minutes, with an average
of about 90 seconds.
Thus, ﬁnding the key on average takes about 3 minutes.
However, if at all, the user will only notice the ﬁrst few sec-
onds, as all other processes are executed as normal processes
without attacking the scheduler any more. Alternatively, the
data collected in the ﬁrst step could be downloaded to, and
evaluated on, another machine. This data consists of one
bitmap of size 2l = 25 = 32 bits for each memory access,
cf. §IV-C. For each encryption 160 memory accesses are
monitored. Thus, 160 · 100 · 32 bits = 62.5 kB would have
to be downloaded.
V. EXTENSIONS
In the following we show how the key search algorithm
can be sped up, and how this can be used to extend our attack
to other key length as well. Furthermore, we explain how
the encrypted plaintext can be recovered without accessing
the ciphertext at all.
A. Accelerating the Key Search
If a higher number of encryptions can be observed by the
spy process, the key search of our attack can be accelerated
considerably. Using the notation from §III-B this is because
502
∗
the peaks of the fi(k
i ) corresponding to the true partial
key column candidates become easier to separate from the
ﬂoor of wrong ones. This is because the expectation value of
∗
i ) grows much faster than its standard deviation. Thus,
fi(k
after sufﬁciently many observations, i.e., for large N, the 9
correct candidates for each kj
will exactly be given by the
i
partial key column candidates with the highest frequencies.
Now, the key search algorithm from §III-C can be short-
ened signiﬁcantly, as for each kj
only 9 choice are left
i
compared to 24·l = 220 before. Having assigned values of,
∗
and k3
e.g., k2
, there will typically be at most one possible
∗
3
3
solution for k3
among the 9 possible values. This allows
2
one to implement the key search in a brute force manner.
∗
∗
∗
On our test environment, 300 encryptions (i.e., 4.69 kB of
encrypted plaintext) are sufﬁcient for this approach.
B. Extensions to AES-192 and AES-256
While our implementation is optimized for AES-128, the
presented key search algorithm conceptually can easily be
adopted for the case of AES-192 and AES-256. However,
the heap used in §III becomes signiﬁcantly more complex
for key sizes larger than 128 bits. This problem does
not occur for the key search technique presented in the
previous paragraph, as its complexity is rather inﬂuenced
by the number of rounds than by the size of the ciphertext.
We leave it as future work to obtain practically efﬁcient
implementations of either of these two techniques.
C. Decryption without Ciphertext