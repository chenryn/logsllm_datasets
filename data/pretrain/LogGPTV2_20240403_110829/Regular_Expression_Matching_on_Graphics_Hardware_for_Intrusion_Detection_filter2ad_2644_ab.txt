tice though this is not a problem since modern PCs can be equipped with ample
amounts of physical memory.
Having allocated a buﬀer for collecting the packets in page-locked memory,
every time a packet is classiﬁed to be matched against a speciﬁc regular ex-
pression, it is copied to that buﬀer and is “marked” for searching against the
corresponding ﬁnite automaton. We use a double-buﬀer scheme to permit over-
lap of computation and communication during data transfers between the GPU
and CPU. Whenever the ﬁrst buﬀer is transferred to the GPU through DMA,
newly arriving packets are copied to the second buﬀer and vice versa.
A slight complication that must be handles comes from the TCP stream
reassembly functionality of modern NIDSs, which reassembles distinct packets
into TCP streams to prevent an attacker from evading detection by splitting
the attack vector across multiple packets. In Snort, the Stream5 preprocessor
aggregates multiple packets from a given direction of a TCP ﬂow and builds a
single packet by concatenating their payloads, allowing rules to match patterns
that span packet boundaries. This is accomplished by keeping a descriptor for
each active TCP session and tracking the state of the session according to the
semantics of the TCP protocol. Stream5 also keeps copies of the packet data
and periodically “ﬂushes” the stream by reassembling all contents and emitting
a large pseudo-packet containing the reassembled data.
Consequently, the size of a pseudo-packet that is created by the Stream5
preprocessor may be up to 65,535 bytes in length, which is the maximum IP
packet length. However, assigning the maximum IP packet length as the size of
272
G. Vasiliadis et al.
0
StateTable Ptr Length
4
6
thread k
0x001a0b
3487
1536
Payload
Payload
thread k+1
0x001a0b
1957
Payload
thread k+2
0x001a0b
427
Payload
thread k+3
0x02dbd2
768
Payload
Fig. 4. Matching packets that exceed the MTU size
each row of the buﬀer would result in a huge, sparsely populated array. Copying
the whole array to the device would result in high communication costs, limiting
overall performance.
A diﬀerent approach for storing reassembled packets that exceed the Max-
imum Transmission Unit (MTU) size, without altering the dimensions of the
array, is to split them down into several smaller ones. The size of each portion
of the split packet will be less or equal to the MTU size and thus can be copied
in consecutive rows in the array.
Each portion of the split packet is processed by diﬀerent threads. To avoid
missing matches that span multiple packets, whenever a thread searches a split
portion of a packet, it continues the search up to the following row (which con-
tains the consecutive bytes of the packet), until a ﬁnal or a fail state is reached,
as illustrated in Figure 4. While matching a pattern that spans packet bound-
aries, the state machine will perform regular transitions. However, if the state
machine reaches a ﬁnal or a fail state, then it is obvious that there is no need to
process the packet any further, since any consecutive patterns will be matched
by the thread that was assigned to search the current portion.
4.2 Compiling PCRE Regular Expressions to DFA State Tables
Many existing tools that use regular expressions have support for converting reg-
ular expressions into DFAs [5,1]. The most common approach is to ﬁrst compile
them into NFAs, and then convert them into DFAs. We follow the same ap-
proach, and ﬁrst convert each regular expression into an NFA using the Thomp-
son algorithm [29]. The generated NFA is then converted to an equivalent DFA
incrementally, using the Subset Construction algorithm. The basic idea of sub-
set construction is to deﬁne a DFA in which each state is a set of states of the
corresponding NFA. Each state in the DFA represents a set of active states in
which the corresponding NFA can be in after some transition. The resulting
Regular Expression Matching on Graphics Hardware
273
DFA achieves O(1) computational cost for each incoming character during the
matching phase.
A major concern when converting regular expressions into DFAs is the state-
space explosion that may occur during compilation [6]. To distinguish among the
states, a diﬀerent DFA state may be required for all possible NFA states. It is
obvious that this may cause exponential growth to the total memory required.
This is primarily caused by wildcards, e.g. (.*), and repetition expressions, e.g.
(a(x,y)). A theoretical worst case study shows that a single regular expression
of length n can be expressed as a DFA of up to O(Σn) states, where Σ is the size
of the alphabet, i.e. 28 symbols for the extended ASCII character set [12]. Due
to state explosion, it is possible that certain regular expressions may consume
large amounts of memory when compiled to DFAs.
To prevent greedy memory consumption caused by some regular expressions,
we use a hybrid approach and convert only the regular expressions that do not
exceed a certain threshold of states; the remaining regular expressions will be
matched on the CPU using NFAs. We track of the total number of states during
the incremental conversion from the NFA to the DFA and stop when a certain
threshold is reached. As shown in Section 5.2, setting an upper bound of 5000
states per expression, more than 97% of the total regular expressions can be
converted to DFAs. The remaining expressions will be processed by the CPU
using an NFA schema, just like the default implementation of Snort.
Each constructed DFA is a two-dimensional state table array that is mapped
linearly on the memory space of the GPU. The dimensions of the array are
equal to the number of states and the size of the alphabet (256 in our case),
respectively. Each cell contains the next state to move to, as well as an indication
of whether the state is a ﬁnal state or not. Since transition numbers may be
positive integers only, we represent ﬁnal states as negative numbers. Whenever
the state machine reaches into a state that is represented by a negative number,
it considers it as a ﬁnal state and reports a match at the current input oﬀset. The
state table array is mapped on the memory space of the GPU, as we describe in
the following section.
4.3 Regular Expression Matching
We have investigated storing the DFA state table both as textures in the texture
memory space, as well as on the linear global memory of the graphics card. A
straightforward way to store the DFA of each regular expression would be to
dynamically allocate global device memory every time. However, texture memory
can be accessed in a random fashion for reading, in contrast to global memory,
in which the access patterns must be coalesced [19]. This feature can be very
useful for algorithms like DFA matching, which exhibit irregular access patterns
across large datasets. Furthermore, texture fetches are cached, increasing the
performance when read operations preserve locality. As we will see in Section 5.3,
the texture memory is 2 to 2.5 times faster than global device memory for input
data reads.
274
G. Vasiliadis et al.
t
n
e
m
g
e
S
y
r
o
m
e
M
e
r
u
t
x
e
T
D
1
l
s
e
b
a
T
e
t
i
t
a
S
s
n
o
s
s
e
r
p
x
E
l
r
a
u
g
e
R
2D Texture Memory Segment
Packet Buffer
P1
P2
P3
P1
P2
P3
P1
P2
P3
P8
P8
P8
Global Device Memory
Matches Output
P1
P2
P3
P8
Fig. 5. Regular expression matching on the GeForce 9800 with 128 stream processors.
Each processor is assigned a diﬀerent packet to process using the appropriate DFA.
However, CUDA does not support dynamic binding of memory to texture
references. Therefore, it is not feasible to dynamically allocate memory for each
state table individually and later bind it to a texture reference. To overcome this
limitation, we pre-allocate a large amount of linear memory that is statically
bound to a texture reference. All constructed state tables are stored sequentially
in this texture memory segment.
During the searching phase, each thread searches a diﬀerent network packet in
isolation, as shown in Figure 5. Whenever a thread matches a regular expression
on an incoming packet, it reports it by writing the event to a single-dimension
array allocated in the global device memory. The size of the array is equal to
the number of packets that are processed by the GPU at once, while each cell
of the array contains the position within the packet where the match occurred.
5 Evaluation
5.1 Experimental Environment
For our experiments, we used an NVIDIA GeForce 9800 GX2 card, which consists
of two PCBs (Printed Circuit Board), each of which is an underclocked Geforce
8800 GTS 512(G92) video card in SLI Mode. Each PCB contains 128 stream
Regular Expression Matching on Graphics Hardware
275
processors organized in 16 multiprocessors, operating at 1.5GHz with 512 MB of
memory. Our base system is equipped with two AMD OpteronTM 246 processors
at 2GHz with 1024KB of L2-cache.
For our experiments, we use the following full payload network traces:
U-Web: A trace of real HTTP traﬃc captured in our University. The trace
totals 194MB, 280,088 packets, and 4,711 ﬂows.
SCH-Web: A trace of real HTTP traﬃc captured at the access link that con-
nects an educational network of high schools with thousands of hosts to
the Internet. The trace contains 365,538 packets in 14,585 diﬀerent ﬂows,
resulting to about 164MB of data.
LLI: A trace from the 1998-1999 DARPA intrusion detection evaluation set of
MIT Lincoln Lab [2]. The trace is a simulation of a large military network
and generated speciﬁcally for IDS testing. It contains a collection of ordinary-
looking traﬃc mixed with attacks that were known at the time. The whole
trace is about 382MB and consists of 1,753,464 packets and 86,954 ﬂows.
In all experiments, Snort reads the network traces from the local machine. We
deliberately chose traces of small size so that they can ﬁt in main memory—
after the ﬁrst access, the whole trace is cached in memory. After that point, no
accesses ever go to disk, and we have veriﬁed the absence of I/O latencies using
the iostat(1) tool.
We used the default rule set released with Snort 2.6 for all experiments. The
set consists of 7179 rules that contain a total of 11,775 pcre regular expressions.
All preprocessors were enabled, except the HTTP inspect preprocessor, in order
to force all web traﬃc to be matched against corresponding rules regardless of
protocol semantics.
5.2 Memory Requirements
In our ﬁrst experiment, we measured the memory requirements of our system.
Modern graphics cards are equipped with enough and fast memory, ranging
from 512MB DDR up to 1.5GB GDDR3 SDRAM. However, the compilation of
several regular expression to DFAs may lead to state explosion and consume
large amounts of memory.
Figure 6(a) shows the cumulative fraction of the DFA states for the regular
expressions of the Snort rule set. It appears that only a few expressions are
prone to the state-space explosion eﬀect. By setting an upper bound of 5000
states per regular expression, it is feasible to convert more than 97% of the
regular expressions to DFAs, consuming less than 200MB of memory, as shown
in Figure 6(b).
5.3 Microbenchmarks
In this section, we analyze the communication overheads and the computational
throughput achieved when using the GPU for regular expression matching.
276
G. Vasiliadis et al.
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
u
C
1
0.8
0.6
0.4
0.2
0
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
u
C
1
0.8
0.6
0.4
0.2
0
1
10
100
1000
Number of states
(a)
10000
50000
1
10
100 200
1000
3000
Total memory requirements (MB)
(b)
Fig. 6. States (a) and memory requirements (b) for the 11,775 regular expressions