combination of rows jk best approximates row i: ˚X(i, ∗) ≈
k=1 w(k) ˚X(jk,∗). Assuming thatPK
PK
k=1 w(k)X(jk,∗) ap-
proximates X(i, ∗) well, we then set S(i, i) = 1 and S(i, jk) =
−w(k) for k = 1, 2, . . . , K to capture the resulting approxima-
tion errors (which are expected to be small).
Scaling of S and T : Finally, we need to scale S and T properly
so that ||S(LRT )||F , ||(LRT )T T||F , and ||A(LRT ) − B||F are
of similar order of magnitude — otherwise they may overshadow
each other during the optimization of (14).
In our experiments,
we simply scale S and T such that ||S ˚X||F = 0.1√λ||B||F and
|| ˚XT T||F = √λ||B||F , where √λ||B||F reﬂects the level of ap-
proximation error ||A(LRT )−B||F that we are willing to tolerate.
Our results show that such scaling yields good performance over a
wide range of scenarios and that the performance is not sensitive to
the choice of λ. Note that we intentionally make ||S ˚X||F smaller
than || ˚XT T||F because we expect the temporal model obtained
through domain knowledge is more reliable.
3.2 Combining Global and Local Methods
A quick look at the (non-hybrid) performance results that fol-
low shows that for small amounts of missing data, KNN is the best
performer (in most cases). On the other hand, for large amounts
of loss, SRMF outperforms KNN. The intuition behind this re-
sult is obvious. When only a few data points are missing, the k-
nearest neighbors of a missing data point will be close by. There
are strong temporal and spatial correlations in our data, so the near-
est neighbors provide good interpolants for the missing data. How-
ever, when there is substantial missing data, the nearest neighbors
will come from further away. As the correlations between data
points drop, the low-rank global model of the data expressed by the
matrix factorization becomes superior.
To take advantage of local structure and redundancy present in
the TM, we use the low-rank approximation obtained by SRMF as
271a prior and augment it with a local interpolation procedure. In this
way, we obtain a TM estimate that is close to the low-rank prior yet
can account for constraints imposed by the local interpolation pro-
cedure. Note that such an approach generalizes the Tomo-gravity
method for TM estimation [31, 33], which uses a rank-1 approxi-
mation (i.e., gravity model) as the prior solution.
The choice of the local interpolation procedure is application
dependent. Below we present three such examples. For inter-
polation of missing values, we present SRMF+KNN and SRSVD-
base+KNN, two hybrid algorithms that both incorporate KNN. For
network tomography, we present Tomo-SRMF, a hybrid algorithm
that combines SRMF and Tomo-gravity [33].
SRMF+KNN: We ﬁrst compute the SRMF interpolation of X.
Call this XSRMF. For each missing data point (i, j) we then exam-
ine its row to see if any of the elements X(i, j−3), . . . , X(i, j +3)
are present. If we cannot observe any of these neighbors, then we
simply use the value XSRMF(i, j), but if we do have any of these
values, we will use them to better approximate X(i, j).
We do so by forming a local model for the temporal process us-
ing all of the other rows of the TM. We perform a regression to
ﬁnd a set of weights w(k) that best approximates XSRMF(p, j) =
Pk∈nbrs w(k)XSRMF(p, k) for all p = 1, 2, . . . , n. Then we ap-
ply a weighted linear interpolation of the nearest neighbors, using
the weights derived above, i.e.,
XSRMF+KNN(i, j) = Xk∈nbrs
w(k)X(i, k).
(16)
SRSVD-base+KNN: We will show that the above approach is su-
perior, but to understand the importance of incorporating the spatio-
temporal constraints (given by S and T ), we also consider an algo-
rithm that uses SRSVD-base as the prior in the same procedure.
We call the resulting algorithm SRSVD-base+KNN.
In network tomography, we need to infer TMs
Tomo-SRMF:
based on link-load measurements (possibly in combination with
direct measurements of a subset of TM elements). The strictly
low-rank approximations obtained by SRMF may not satisfy all
the measurement equations because (i) real TMs are only approxi-
mately low-rank, and (ii) measurement errors are common. A nat-
ural solution is to combine SRMF and Tomo-gravity. Speciﬁcally,
we use SRMF (instead of the gravity model) as a prior solution. We
then follow Tomo-gravity and seek a solution that is close to this
prior solution (with respect to the Kullback-Leibler divergence) yet
satisﬁes all the measurement equations. We call the resulting hy-
brid algorithm Tomo-SRMF.
4.
INTERPOLATION PERFORMANCE
4.1 Data
The data we use here is real TM data: two standard sets, and
one new set. The ﬁrst two are the Abilene (Internet2) [1] dataset
used previously in various studies [13,14,30], and the GÉANT TM
dataset provided in [26], and previously examined in [2]. Although
these are now older datasets, we use them because they are valuable
for comparisons with other work. In addition, we use one longer
and more recent commercial TM dataset from a large Internet ser-
vice provider. The properties of the data are summarized in Table 2.
Network Date
Abilene Apr. 2003 1 week
Commercial Oct. 2006 3 weeks
GÉANT Apr. 2005 1 week
Duration Resolution Size
10 min.
1 hour
15 min.
121 × 1008
400 × 504
529 × 672
Table 2: Datasets under study.
4.2 Methodology
The methodology we use here is to drop some data from existing
measurements, and then apply the interpolation algorithm. This
provides us with ground truth for comparison. The pseudo-missing
data is not used in the interpolation algorithms in any way.
The typical approach when measuring the performance of an in-
terpolation algorithm is to drop data at random. We will start our
experiments with this case. However, in real measurements of TMs
there are different mechanisms that result in missing data, and these
result in the missing data having structure. Such structure is obvi-
ously important for interpolation, so we will explore several struc-
tured models of missing data in Section 4.5 below.
We measure performance using the Normalized Mean Absolute
Error (NMAE) in the interpolated values. That is, we calculate
N M AE = Pi,j:M (i,j)=0 |X(i, j) − ˆX(i, j)|
,
(17)
Pi,j:M (i,j)=0 |X(i, j)|
where ˆX is the estimated matrix. Note that we only measure errors
on the missing values. So the NMAE is deﬁned only when there is
at least one missing value. We computed three other performance
metrics (root mean squared error, normalized root mean squared
error, and the correlation-coefﬁcient) but the results are substan-
tively the same. In each case we perform the process of randomly
dropping data and reconstructing the matrix 10 times. The results
presented show the mean NMAE.
4.3 Initial Comparisons
Figure 1 shows a comparison of algorithms for independent ran-
dom loss for data loss rates ranging from 0.02 to 0.98 (NMAE is
undeﬁned when the loss probability is 0). We perform these al-
gorithms using the same regularization and input rank parameters
λ = 0.1 and r = 8 for each global algorithm, and k = 4 in KNN
(we defer justiﬁcation of these choices to the section below).
For low loss probabilities KNN achieves better performance than
SRMF. For high loss probabilities we see that SRMF’s performance
exceeds KNN. However, the hybrid SRMF+KNN outperforms all
algorithms over the whole range of loss values. Interestingly, the
hybrid is noticeably better than either method individually.
Meanwhile, the hybrid SRSVD-base+KNN also performs well,
though not as well as SRMF+KNN. The performance gap typically
widens for large amounts of loss. This is because under indepen-
dent random loss, when the loss rate is not too high, it is likely that
the near neighbors of a missing value are directly observed, making
KNN an effective recovery strategy. However, when loss is large
or when the loss is highly structured (see Section 4.5), the perfor-
mance gap between SRSVD-base+KNN and SRMF+KNN widens.
The other methods all have worse performance. For low loss,
the baseline method is the worst (as we might expect given it is
only a rank-2 approximation). However, for high loss, the baseline
performs surprisingly well, certainly better than SRSVD, whose
performance is very bad for high loss. However, the SRSVD ap-
plied after baseline removal achieves reasonable performance over
the whole loss range, in some cases almost as good as the simple
SRMF. NMF performs poorly for all loss probabilities.
We have examined many such graphs. NMF and SRSVD (with-
out baseline removal) are uniformly poor. So we do not examine
them in further results to simplify our presentation.
4.4 Parameter Sensitivity and Settings
The algorithms we consider have several input parameters. The
performance of these algorithms in relation to these parameters is
(in most cases) dependent on the dataset in question. In practice,
when interpolating a real dataset, we would not be able to precisely
optimize λ and r for the dataset in question, so it is desirable to have
272SRMF
SRMF+KNN
NMF
Baseline
SRSVD
SRSVD base
KNN
SRSVD base + KNN
0.5
0.4
0.3
0.2
0.1
E
A
M
N
0.5
E
A
M
N
0.4
0.3
0.2
0.1
0
0
0.2
0.4
0.6
data loss probability
(a) Abilene
0.8
1
0
0
0.2
0.4
0.6
data loss probability
(b) Commercial
0.5
0.4
0.3
0.2
0.1
E
A
M
N
0.8
1
0
0
0.2
0.4
0.6
0.8
1
data loss probability
(c) GÉANT
Figure 1: Interpolation performance for random loss (note that the legend is the same for all three plots).
algorithms that are not very sensitive to their values. In fact, all
algorithms display some dependence on the parameter settings, and
no single parameter setting is optimal for all datasets. However, we
found rough parameter settings that are never too far from optimal.
The ﬁrst input parameter is the rank. Given our motivation from
the compressive sensing literature, i.e., that we will aim to mini-
mize matrix sparsity or rank, it may seem strange that we input a
rank when performing the algorithm. However, although they seek
to minimize the rank of the decomposition, the algorithms work by
optimizing an L and R that have a ﬁxed number of columns r (the
input rank). The ﬁnal rank of the solution might be smaller.
In theory, as long as the input rank is greater than the real rank of
X, the various algorithms will converge to the correct matrix [5,19,
20]. However, note that the theoretical results that inform our in-
tuition here concern matrices with exact ranks, whereas our matri-
ces typically have a number of small, but non-zero singular values.
Moreover, there are measurement errors in our data, so we cannot
expect to get zero error reconstructions.
Figure 2 shows a sample of performance results with respect to
rank (note that the baseline algorithm is excluded here because it
is a ﬁxed rank-2 approximation). We ﬁnd that most of the rank-
dependent methods have better performance as the input rank in-
creases. Although this is not always the case, the deviations are mi-
nor. However, note the logarithmic x-axis, so that the results sug-
gest a decrease in the marginal improvement with increasing rank.
There is also an additional computational cost for higher ranks, and
we ﬁnd that an input rank of r = 8 is a reasonable operating point
for our comparisons. Going to r = 16 yields only a very small
incremental improvement at the expense of extra computation.
The most important ﬁnding in these results, however, is the rela-
tive insensitivity of the hybrid algorithm, SRMF+KNN. In general
it is the least dependent on the input rank of all the algorithms.
There is some improvement for higher ranks, but it is typically
smaller than those of other algorithms.
KNN does not use input rank, but rather k, the size of neigh-
borhood. Figure 2 also shows the effect of k on the performance
of KNN. We choose to use k = 4 for our experiments, since it
consistently avoids the worst results.
The ﬁnal parameter of importance is the regularization parame-
ter λ, which determines the tradeoff (in the optimization) between
the measurement constraints and the importance of rank. Larger λ
leads to lower rank approximations, whereas smaller values lead to
approximations that are a better ﬁt to the data. Figure 3 presents
three examples showing the type of variability we encounter over
a range of values of λ, for three different loss rates and networks.
KNN is omitted because it does not use regularization. Once again
note the logarithmic x-axis – we are looking for order of magnitude
effects here, not ﬁne tuning. None of the techniques is too sensitive.
Among them, SRSVD is the most sensitive (overall). Larger values
of λ typically perform better although again sometimes this trend
is reversed, and there are a number of cases where the optimal case
is around λ = 0.1. So we use this value in our experiments.
Note again that SRMF+KNN is the most insensitive algorithm
with Figure 3 (c) showing the most extreme case of parameter sen-
sitivity that we observed for this algorithm.
4.5 Comparison: Other Loss Models
As earlier noted, not all data loss is random. Losses are often
highly structured, and in this section we examine the effect this
has on the results. The boldface name denotes the label used in
our datasets, where xx is replaced by the percentage of rows (or
columns) effected.
1. PureRandLoss: This is the simple random loss model. Data
points in the matrix X are dropped independently at random
with probability q.
2. xxTimeRandLoss: This simulates a structured loss event where
we suffer data loss at certain times. For example a certain time
points our monitoring equipment might become overloaded, or
a disk might ﬁlls up. In these cases, we may loose some random
proportion of the data at a particular point in time. We simulate
this loss by choosing, at random xx% of the columns of X, and
dropping data from these at random with probability q. Note
that the case 100ElemRandLoss corresponds to PureRandLoss,
so we do not repeat this case.
3. xxElemRandLoss: This simulates a structured loss event where
a set of randomly chosen TM elements suffers from lost data.
This type of loss might occur where unreliable transport mech-
anisms are used to transport measurements back to the network
measurement station. Often the problems with such transport
depend on the locations where measurements are made (e.g.,
locations close to the management station are less likely to suf-
fer congestion based losses). We randomly select xx% of the
rows of X to be effected. Note that the case 100ElemRandLoss
corresponds to PureRandLoss, so we do not repeat this case.