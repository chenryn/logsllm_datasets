1. Aim Recovery. http://www.dark-e.com/des/software/aim/index.shtml.
2. Back Oriﬁce. http://www.cultdeadcow.com/tools/bo.html.
3. BackDoor.XTCP.
http://www.ntsecurity.new/Panda/Index.cfm?FuseAction=Virus&VirusID=659.
http://home.swipenet.se/˜w-65048/hacks.htm.
7. L0pht Crack. http://www.atstack.com/research/lc.
8. Setup Trojan. http://www.nwinternet.com/˜pchelp/bo/setuptrojan.txt.
9. V. Barnett and T. Lewis. Outliers in Statistical Data. John Wiley and Sons, 1994.
10. Fred Cohen. A Short Course on Computer Viruses. ASP Press, Pittsburgh, PA,
1990.
11. M. H. DeGroot. Optimal Statistical Decisions. McGraw-Hill, New York, 1970.
12. D. E. Denning. An intrusion detection model. IEEE Transactions on Software
Engineering, SE-13:222–232, 1987.
13. Eleazar Eskin. Anomaly detection over noisy data using learned probability distri-
butions. In Proceedings of the Seventeenth International Conference on Machine
Learning (ICML-2000), 2000.
4. BrowseList. http://e4gle.org/files/nttools/,
http://binaries.faq.net.pl/security tools.
5. Happy 99. http://www.symantex.com/qvcenter/venc/data/happy99.worm.html.
6. IPCrack. http://www.geocities.com/SiliconValley/Garage/3755/toolicq.html,
Detecting Malicious Software
53
14. Eleazar Eskin. Probabilistic anomaly detection over discrete records using inconsis-
tency checks. Technical report, Columbia University Computer Science Technical
Report, 2002.
15. Stephanie Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaﬀ. A sense of
self for unix processes. pages 120–128. IEEE Computer Society, 1996.
16. N. Friedman and Y. Singer. E.cient bayesian parameter estimation in large discrete
domains, 1999.
17. S. A. Hofmeyr, Stephanie Forrest, and A. Somayaji. Intrusion detect using se-
quences of system calls. Journal of Computer Security, 6:151–180, 1998.
18. Andrew Honig, Andrew Howard, Eleazar Eskin, and Salvatore Stolfo. Adaptive
model generation: An architecture for the deployment of data minig-based intrusion
detection systems. In Data Mining for Security Applications. Kluwer, 2002.
19. Internet Engineering Task Force. Intrusion detection exchange format.
In http://www.ietf.org/html.charters/idwg-charter.html, 2000.
20. H. S. Javitz and A. Valdes. The nides statistical component: Description and jus-
tiﬁcation. Technical report, SRI International, 1993.
21. W. Lee, S. J. Stolfo, and P. K. Chan. Learning patterns from unix processes exe-
cution traces for intrusion detection. pages 50–56. AAAI Press, 1997.
22. W. Lee, S. J. Stolfo, and K. Mok. Data mining in work ﬂow environments: Experi-
ences in intrusion detection. In Proceedings of the 1999 Conference on Knowledge
Discovery and Data Mining (KDD-99), 1999.
23. Wenke Lee, Sal Stolfo, and Kui Mok. A data mining framework for building intru-
sion detection models. 1999.
24. MacAfee. Homepage: macafee.com.
Online publication, 2000. http://www.mcafee.com.
25. M. Mahoney and P. Chan. Detecting novel attacks by identifying anomalous net-
work packet headers. Technical Report CS-2001-2, Florida Institute of Technology,
Melbourne, FL, 2001.
26. B. Sch¨olkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Es-
timating the support of a high-dimensional distribution. Technical Report 99–87,
Microsoft Research, 1999. To appear in Neural Computation, 2001.
27. SysInternals. Regmon for Windows NT/9x. Online publication, 2000.
http://www.sysinternals.com/ntw2k/source/regmon.shtml.
28. Christina Warrender, Stephanie Forrest, and Barak Pearlmutter. Detecting intru-
sions using system calls: alternative data models. pages 133–145. IEEE Computer
Society, 1999.
29. Steve R. White. Open problems in computer virus research. In Virus Bulletin
Conference, 1998.
Undermining an Anomaly-Based Intrusion
Detection System Using Common Exploits
Kymie M.C. Tan, Kevin S. Killourhy, and Roy A. Maxion
Dependable Systems Laboratory
Computer Science Department
Carnegie-Mellon University
Pittsburgh, Pennsylvania 15213 USA
Abstract. Over the past decade many anomaly-detection techniques
have been proposed and/or deployed to provide early warnings of cyber-
attacks, particularly of those attacks involving masqueraders and novel
methods. To date, however, there appears to be no study which has
identiﬁed a systematic method that could be used by an attacker to
undermine an anomaly-based intrusion detection system. This paper
shows how an adversary can craft an oﬀensive mechanism that renders
an anomaly-based intrusion detector blind to the presence of on-going,
common attacks. It presents a method that identiﬁes the weaknesses of
an anomaly-based intrusion detector, and shows how an attacker can
manipulate common attacks to exploit those weaknesses. The paper ex-
plores the implications of this threat, and suggests possible improvements
for existing and future anomaly-based intrusion detection systems.
1 Introduction
In recent years, a vast arsenal of tools and techniques has been accumulated to
address the problem of ensuring the availability, integrity and conﬁdentiality of
electronic information systems. Such arsenals, however, are frequently accompa-
nied by equally vast “shadow” arsenals of tools and techniques aimed speciﬁ-
cally at subverting the schemes that were designed to provide system security.
Although a shadow arsenal can be viewed negatively as a formidable threat to
the security of computer systems, it can also be viewed positively as a source of
knowledge for identifying the weaknesses of current security tools and techniques
in order to facilitate their improvement.
A small part of the security arsenal, and the focus of this work, is the
anomaly-based intrusion-detection system. Anomaly-based intrusion-detection
systems have sought to protect electronic information systems from intrusions
or attacks by attempting to detect deviations from the normal behavior of the
monitored system. The underlying assumption is that such deviations may indi-
cate that an intrusion or attack has occurred (or may still be occurring) on the
system. Anomaly detection – detecting deviations from normal – is one of two
fundamental approaches used in systems that seek to automate the detection of
attacks or intrusions; the other approach is signature-based detection. Anomaly
A. Wespi, G. Vigna, and L. Deri (Eds.): RAID 2002, LNCS 2516, pp. 54–73, 2002.
c(cid:1) Springer-Verlag Berlin Heidelberg 2002
Undermining an Anomaly-Based Intrusion Detection System
55
detection is typically credited with a greater potential for addressing security
problems such as the detection of attempts to exploit new or unforeseen vul-
nerabilities (novel attacks), and the detection of abuse-of-privilege attacks, e.g.,
masquerading and insider misuse [1].
The promise of the anomaly-detection approach and its incorporation into a
number of current automated intrusion-detection strategies (e.g., AT&T’s Com-
puterWatch, SRI’s Emerald, SecureNet, etc. [1]) underscores the importance of
studying how attackers may fashion counter-responses aimed at undermining
the eﬀectiveness of anomaly-based intrusion-detection systems. Such studies are
important for two reasons:
– to understand how to strengthen the anomaly-based intrusion-detection sys-
tem by identifying its weaknesses; and
– to provide the necessary knowledge for guiding the design and implementa-
tion of a new generation of anomaly-based intrusion detectors that are not
vulnerable to the weaknesses of their forebears.
This paper lays out a method for undermining a well-known anomaly-based
intrusion-detection system called stide [2], by ﬁrst identifying the weaknesses
of its anomaly-detection algorithm, and then by showing how an attacker can
manipulate common attacks to exploit those weaknesses, eﬀectively hiding the
presence of those attacks from the detector’s purview. Stide was chosen pri-
marily because it is freely available to other researchers via the Internet. Its
accessibility not only encourages independent veriﬁcation and replication of the
work performed here, but it also builds on, and contributes to, a large body of
previously published work that uses the stide detection mechanism.
To undermine an anomaly-based intrusion detector, an attacker needs to
know the three elements described in Table 1. These elements set the framework
for the paper.
Table 1. Elements of methodology for undermining.
1. Detection coverage (speciﬁcally, blind spots) of an anomaly detector.
2. Where and how an attack manifests in sensor data.
3. How to shift the manifestation from a covered spot to a blind one.
2 Approaches to Undermining Anomaly Detectors
There are two approaches that would most obviously cause an anomaly detector
to miss detecting the anomalous manifestation of an attack. The ﬁrst of these
two items describes the approach commonly found in the literature; the second
describes the approach adopted by this study.
56
K.M.C. Tan, K.S. Killourhy, and R.A. Maxion
– modify the normal to look like the attack, i.e., incorporate the attack mani-
festations into the model of normal behavior; or
– modify the attack to make it appear as normal behavior.
In the intrusion detection literature, the most cited way to undermine an
anomaly-based intrusion detection system is to incorporate undesired, intrusive
behavior into the training data, thereby falsely representing “normal” behavior
[1,9,10]. By including intrusive behavior explicitly into the training data, the
anomaly detector is forced to incorporate the intrusive behavior into its internal
model of normal and consequently lose the ability to ﬂag future instances of that
intrusive behavior as anomalous. Note that the anomaly detector is viewed as
that component of an anomaly-based intrusion detection system solely respon-
sible for detecting deviations from normal; it performs no diagnostic activities.
For example, one way to incorporate intrusive behavior into an anomaly de-
tector’s model of normal behavior is to exploit the fact that behavior can change
over time. Changing behavior requires the anomaly detector to undergo periodic
on-line retraining. Should the information system undergo attacks during the
retraining process, then the anomaly detector could inadvertently incorporate
undesired attack behavior into its model of normal behavior [1]. The failure of
an anomaly-based intrusion detector to detect intrusions or attacks can typi-
cally be attributed to contaminated training data, or to updating schemes that
incorporate new normal behavior too quickly.
Undermining an anomaly-based intrusion detection system by simply incor-
porating intrusive behavior into its training data is too imprecise and abstract a
method as to be practically useful to an attacker. Identifying and accessing the
segment, feature or attribute of the data that will be used to train an anomaly
detector, and then surreptitiously and slowly introducing the intrusive behavior
into the training dataset, may require time, patience and system privileges that
may not be available to an attacker. Moreover, such a scheme does not provide
the attacker with any guarantees as to whether or not the act of subversion
has been, or will be, successful. The incorporation of intrusive behavior into the
training data is no guarantee that the anomaly detector will be completely blind
to the attack when the attack is actually deployed. The attacker has no knowl-
edge of how the anomaly detector perceives the attack, i.e., how the attack truly
manifests in the data, and no knowledge concerning the conditions that may
impede or boost the anomaly detector’s ability to detect the manifestation of
the attack. For example, even if intrusive behavior were to be incorporated into
an anomaly detector’s model of normal, it is possible that when the attack is
actually deployed, it will interact with other conditions in the data environment
(conditions that may not have been present during the training phase), causing
anomalous manifestations that are detectable by the anomaly detector. It is also
possible for the anomalous manifestation of an attack to be detectable only when
the detector uses particular parameter values. These points illustrate why it is
necessary to determine precisely what kinds of anomalous events an anomaly
detector may or may not be able to detect, as well as the conditions that enable
it to do so.
Undermining an Anomaly-Based Intrusion Detection System
57
These issues are addressed by determining the coverage of the anomaly de-
tector in terms of anomalies (not in terms of attacks); this forms the basis of the
approach. Only by knowing the kinds of anomalies that are or are not detectable
by a given anomaly detector is it possible to modify attacks to manifest in ways
that are not considered abnormal by a given anomaly detector. The coverage of
an anomaly detector serves as a guide for an attacker to know precisely how to
modify an attack so that it becomes invisible to the detector.
3 Detection Coverage of an Anomaly Detector
Current evaluation techniques attempt to establish the detection coverage of an
anomaly-based intrusion detection system with respect to its ability to detect
attacks [21,4,5] , but without establishing whether or not the anomalies detected
by the system are attributable to the attack. Typically, claims that an anomaly-
based intrusion detector is able to detect an attack are based on assumptions that
the attack must have manifested in a given stream of data, that the manifestation
was anomalous, and that the anomaly detector was able to detect that speciﬁc
kind of anomaly.
The anomaly-based evaluation technique described in this section establishes
the detection coverage of stide [2,21] with respect to the types of anomalous
manifestations that the detector is able to detect. The underlying assumption of
this evaluation strategy is that no anomaly detection algorithm is perfect. Before
it can be determined whether an anomaly-based intrusion detector is capable of
detecting an attack, it must ﬁrst be ascertained that the detector is able to
detect the anomalous manifestation of the attack.
This section shows how detection coverage (in terms of a coverage map) can
be established for stide. For the sake of completeness, and to facilitate a bet-
ter understanding of the anomaly-based evaluation strategy, the stide anomaly-
detection algorithm is described, followed by a description of the anomaly-based
evaluation strategy used to establish stide’s detection coverage. A description
and explanation of the results of the anomaly-based evaluation is given.
3.1 Brief Description of the Stide Anomaly Detector
Stide operates on ﬁxed-length sequences of categorical data. It acquires a model
of normal behavior by sliding a detector window of size DW over the training
data, storing each DW -sized sequence in a “normal database” of sequences of
size DW . The degree of similarity between test data and the model of normal
behavior is based on observing how many DW -sized sequences from the test data
are identical matches to any sequences from the normal database. The number
of mismatches between sequences from the test data and the normal database
is noted. The anomaly signal, which is the detector’s response to the test data,
involves a user-deﬁned parameter known as the “locality frame” which deter-
mines the size of a temporally local region over which the number of mismatches
is summed up. The number of mismatches occurring within a locality frame is
58
K.M.C. Tan, K.S. Killourhy, and R.A. Maxion
referred to as the locality frame count, and is used to determine the extent to
which the test data are anomalous. A detailed description of stide and its origins,
can be found in [2,21].
3.2 Evaluation Strategy for Anomaly Detectors
It is not diﬃcult to see that stide will only detect “unusual” or foreign sequences
– sequences that do not exist in the normal database. Its similarity metric es-
tablishes whether or not a particular sequence exists in a normal database of
sequences of the same size. Such a scheme means that any sequence that is
foreign to the normal database would immediately be marked as an anomaly.
However, this observation alone is not suﬃcient to explain the anomaly detector’s
performance in the real world. There are two other signiﬁcant issues that must
be considered before the performance of the anomaly detector can be understood
fully. Speciﬁcally:
– how foreign sequences actually manifest in categorical data; and
– how the interaction between the foreign sequences and the anomaly detection
algorithm aﬀects the overall performance of the anomaly detector.
In order to obtain a clear perspective of these two issues, a framework was
established in [12,19] that focused on the architecture and characteristics of
anomalous sequences, e.g., foreign sequences. The framework deﬁned the anoma-
lous sequences that a sliding-window anomaly detector like stide would likely
encounter, and provided a means to describe the structure of those anomalous
sequences in terms of how they may be composed from other kinds of sub-
sequences. The framework also provided a means to describe the interaction
between the anomalous sequences and the sliding window of anomaly-detection
algorithms like stide. Because the framework established how each anomalous
sequence was constructed and composed, it was possible to evaluate the detec-
tion eﬃcacy of anomaly detectors like stide on synthetic data with respect to
examples of clearly deﬁned anomalous sequences. The results of the evaluation
showed the detection capabilities of stide with respect to the various foreign se-
quences that may manifest in categorical data, and how the interaction between
the foreign sequences in categorical data and the anomaly-detection algorithm
aﬀected the overall performance of the anomaly detector.
3.3 Stide’s Performance Results
The most signiﬁcant result provided by the anomaly-based evaluation of stide
was that there were conditions that caused the detector to be completely blind
to a particular kind of foreign sequence that was found to exist (in abundance)
in real-world data [19]: a minimal foreign sequence. A minimal foreign sequence
is foreign sequence whose proper subsequences all exist in the normal data. Put
simply, a minimal foreign sequence is a foreign sequence that contains within it
no smaller foreign sequences.
Undermining an Anomaly-Based Intrusion Detection System
59
Detection Region
Blind Region
w
o
d
n
w
i
r
o
t
c
e
t
e
d
f
o