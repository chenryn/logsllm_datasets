最小化均方误差（Mean-SquaredError，MSE）MSE(w)=E s∼ρπ,a∼πθ[(Qw(s,a)−Qπ(s,a))2]的。更
直观地，条件（1）是说兼容函数拟合器对随机策略的“特征”是线性的，该“特征”为∇ logπ (a|s)，
θ θ
而条件（2）要求参数w是从这些特征估计Qπ(s,a)这个线性回归（LinearRegression）问题的解。
实际上，条件（2）经常被放宽以支持策略评估算法，这些算法可以用时间差分学习来更高效地
估计价值函数。
如果以上两个条件都被满足，那么 AC 整体算法等价于没有使用批判者做近似，如 REIN-
FORCE算法中那样。这可以通过使得条件（2）中的MSE为0并计算梯度，然后将条件（1）代
入来证明：
∇ MSE(w)=E[2(Qw(s,a)−Qπ(s,a))∇ Qw(s,a)]
w w
=E[2(Qw(s,a)−Qπ(s,a))∇ logπ (a|s)]
θ θ
=0
⇒E[Qw(s,a)∇ logπ (a|s)]=E[Qπ(s,a)∇ logπ (a|s)] (2.173)
θ θ θ θ
对于DPG:兼容函数近似中的两个条件应按照确定性策略µ (s)做相应修改：（1）∇ Qw(s,a)
θ a
| = ∇ µ (s)Tw 而（2）w 最小化均方误差，MSE(θ,w) = E[ϵ(s;θ,w)Tϵ (s;θ,w)]，其中
a=µθ(s) θ θ
ϵ(s;θ,w)=∇ Qw(s,a)| −∇ Qw(s,a)| 。同样可以证明这些条件能够保证无偏差估
a a=µθ(s) a a=µθ(s)
计，通过将拟合过程所做近似转化成一个无批判者的情况：
∇ MSE(θ,w)=0 (2.174)
w
⇒E[∇ µ (s)ϵ(s;θ,w)]=0 (2.175)
θ θ
⇒E[∇ µ (s)∇ Qw(s,a)| ]=E[∇ µ (s)∇ Qµ(s,a)| ] (2.176)
θ θ a a=µθ(s) θ θ a a=µθ(s)
它对在线策略E s∼ρµ[·]和离线策略E s∼ρβ[·]的情况都适用。
106
参考文献
其他方法
如果我们在式(2.171)中用优势函数（AdvantageFunction）替换动作价值函数Qπ(s,a)（由于
减掉基准值不影响梯度）：
Aπθ(s,a)=Qπθ(s,a)−Vπθ(s) (2.177)
那么我们实际可以得到一个更先进的算法叫作优势 Actor-Critic（Advantage Actor-Critic，A2C），
它可以使用TD误差来估计优势函数。这对前面提出的理论和推导不产生影响，但会改变梯度估
计的方差。
近来，人们提出了无行动者（actor-free）方法，比如QT-Opt算法(Kalashnikovetal.,2018)和
Q2-Opt算法(Bodnaretal.,2019)。这些方法也结合了基于策略和基于价值的优化，具体是无梯度
的CE方法和DQN。它们使用动作价值拟合（ActionValueApproximation）来学习Qπθ(s,a)，而
不是使用采样得到的折扣回报作为高斯分布中采样动作的估计，这被证明对现实中机器人学习更
高效和有用，尤其是当有示范数据的时候。
参考文献
ACHIAM J, KNIGHT E, ABBEEL P, 2019. Towards characterizing divergence in deep q-learning[J].
arXivpreprintarXiv:1903.08894.
AUER P, CESA-BIANCHIN, FREUND Y, et al., 1995. Gambling in a riggedcasino: The adversarial
multi-armedbanditproblem[C]//ProceedingsofIEEE36thAnnualFoundationsofComputerScience.
IEEE:322-331.
BODNAR C, LI A, HAUSMAN K, et al., 2019. Quantile QT-Opt for risk-aware vision-based robotic
grasping[J]. arXivpreprintarXiv:1910.02787.
BUBECKS,CESA-BIANCHIN,etal.,2012.Regretanalysisofstochasticandnonstochasticmulti-armed
banditproblems[J]. FoundationsandTrends®inMachineLearning,5(1): 1-122.
DEGRIST,WHITEM,SUTTONRS,2012.Linearoff-policyactor-critic[C]//InInternationalConference
onMachineLearning. Citeseer.
DUAN Y, CHEN X, HOUTHOOFT R, et al., 2016. Benchmarking deep reinforcement learning for
continuouscontrol[C]//InternationalConferenceonMachineLearning. 1329-1338.
FUJ,SINGHA,GHOSHD,etal.,2018. Variationalinversecontrolwithevents: Ageneralframework
fordata-drivenrewarddefinition[C]//AdvancesinNeuralInformationProcessingSystems. 8538-8547.
107
第2章 强化学习入门
HANSEN N, OSTERMEIER A, 1996. Adapting arbitrary normal mutation distributions in evolution
strategies: The covariance matrix adaptation[C]//Proceedings of IEEE international conference on
evolutionarycomputation. IEEE:312-317.
HEESSN,WAYNEG,SILVERD,etal.,2015. Learningcontinuouscontrolpoliciesbystochasticvalue
gradients[C]//AdvancesinNeuralInformationProcessingSystems. 2944-2952.
KALASHNIKOVD,IRPANA,PASTORP,etal., 2018. Qt-opt: Scalabledeepreinforcementlearning
forvision-basedroboticmanipulation[J]. arXivpreprintarXiv:1806.10293.
KINGMADP,WELLINGM,2014.Auto-encodingvariationalbayes[C]//ProceedingsoftheInternational
ConferenceonLearningRepresentations(ICLR).
LESHNOM,LINVY,PINKUSA,etal.,1993. Multilayerfeedforwardnetworkswithanonpolynomial
activationfunctioncanapproximateanyfunction[J]. Neuralnetworks,6(6): 861-867.
LEVINES,2018. Reinforcementlearningandcontrolasprobabilisticinference: Tutorialandreview[J].
arXivpreprintarXiv:1805.00909.
NELDERJA,MEADR,1965. Asimplexmethodforfunctionminimization[J]. Thecomputerjournal,
7(4): 308-313.
PETERSJ,SCHAALS,2008. Naturalactor-critic[J]. Neurocomputing,71(7-9): 1180-1190.
PYEATT L D, HOWE A E, et al., 2001. Decision tree function approximation in reinforcement learn-
ing[C]//Proceedingsofthethirdinternationalsymposiumonadaptivesystems: evolutionarycomputa-
tionandprobabilisticgraphicalmodels: volume2. Cuba: 70-77.
SCHMIDHUBER J, 2015. Deep learning in neural networks: An overview[J]. Neural networks, 61:
85-117.
SILVERD,LEVERG,HEESSN,etal.,2014. Deterministicpolicygradientalgorithms[C].
SINGH S, JAAKKOLA T, LITTMAN M L, et al., 2000. Convergence results for single-step on-policy
reinforcement-learningalgorithms[J]. Machinelearning,38(3): 287-308.
SUTTONRS,MCALLESTERDA,SINGHSP,etal.,2000. Policygradientmethodsforreinforcement
learningwithfunctionapproximation[C]//AdvancesinNeuralInformationProcessingSystems. 1057-
1063.
SZEPESVÁRIC,1998. Theasymptoticconvergence-rateofq-learning[C]//AdvancesinNeuralInforma-
tionProcessingSystems. 1064-1070.
108
参考文献
SZITAI,LÖRINCZA,2006.Learningtetrisusingthenoisycross-entropymethod[J].Neuralcomputation,
18(12): 2936-2941.
TSITSIKLISJN,ROYBV,1997. Ananalysisoftemporal-differencelearningwithfunctionapproxima-
tion[R]. IEEETransactionsonAutomaticControl.
VANHASSELTH,GUEZA,SILVERD,2016.DeepreinforcementlearningwithdoubleQ-learning[C]//
ThirtiethAAAIconferenceonartificialintelligence.
VAN HASSELT H, DORON Y, STRUB F, et al., 2018. Deep reinforcement learning and the deadly
triad[J]. arXivpreprintarXiv:1812.02648.
WATKINSCJ,DAYANP,1992. Q-learning[J]. Machinelearning,8(3-4): 279-292.
WILLIAMSRJ,BAIRDIIILC,1993. Analysisofsomeincrementalvariantsofpolicyiteration: First
stepstowardunderstandingactor-criticlearningsystems[R]. Tech.rep.NU-CCS-93-11,Northeastern
University,CollegeofComputerScience.
109
3
强化学习算法分类
本章将介绍强化学习算法的常见分类方式和具体类别。图 3.1 总结了一些经典的强化学习
算法，并从多个角度对强化学习算法进行分类，其中包括基于模型（Model-Based）和无模型的
（Model-Free）学习方法，基于价值（Value-Based）和基于策略的（Policy-Based）学习方法（或两
者相结合的Actor-Critic学习方法），蒙特卡罗（MonteCarlo）和时间差分（Temporal-Difference）
图3.1 强化学习算法分类图。加粗方框代表不同分类，其他方框代表具体算法
110
3.1 基于模型的方法和无模型的方法
学习方法，在线策略（On-Policy）和离线策略（Off-Policy）学习方法。大多数强化学习算法都可
以根据以上类别进行划分，希望在介绍具体的强化学习算法之前，这些分类能帮助读者建立强化
学习知识体系框架。其中，第4、5和6章分别具体介绍了基于价值的方法、基于策略的方法，以
及两者的结合。
3.1 基于模型的方法和无模型的方法
我们首先讨论基于模型的方法和无模型的方法，如图3.2所示。什么是“模型”？在深度学习
中，模型是指具有初始参数（预训练模型）或已习得参数（训练完毕的模型）的特定函数，例如
全连接网络、卷积网络等。而在强化学习算法中，“模型”特指环境，即环境的动力学模型。回
想一下，在马尔可夫决策过程（MDP）中，有五个关键元素：S,A,P,R,γ。S 和A表示环境的
状态空间和动作空间；P 表示状态转移函数，p(s′|s,a)给出了智能体在状态s下执行动作a，并
转移到状态s′的概率；R代表奖励函数，r(s,a)给出了智能体在状态s执行动作a时环境返回的
奖励值；γ 表示奖励的折扣因子，用来给不同时刻的奖励赋予权重。如果所有这些环境相关的元
素都是已知的，那么模型就是已知的。此时可以在环境模型上进行计算，而无须再与真实环境进
行交互，例如第2章中介绍的值迭代、策略迭代等规划（Planning）方法。在通常情况下，智能
体并不知道环境的奖励函数 R 和状态转移函数 p(s′|s,a)，所以需要通过和环境交互，不断试错
（TrialsandErrors），观察环境相关信息并利用反馈的奖励信号来不断学习。这个不断学习的过程
既对基于模型的方法适用，也对无模型的方法适用。
图3.2 基于模型的方法和无模型的方法
在这个不断试错和学习的过程中，可能有某些环境元素是未知的，如奖励函数R和状态转移
函数P。此时，如果智能体尝试通过在环境中不断执行动作获取样本(s,a,s′,r)来构建对R和P
的估计，则p(s′|s,a)和r的值可以通过监督学习进行拟合。习得奖励函数R和状态转移函数P
111
第3章 强化学习算法分类
之后，所有的环境元素都已知，则前文所述的规划方法可以直接用来求解该问题。这种方式即称
为基于模型的方法。另一种称为无模型的方法则不尝试对环境建模，而是直接寻找最优策略。例
如，Q-learning算法对状态-动作对(s,a)的Q值进行估计，通常选择最大Q值对应的动作执行，
并利用环境反馈更新Q值函数，随着Q值收敛，策略随之逐渐收敛达到最优；策略梯度（Policy
Gradient）算法不对值函数进行估计，而是将策略参数化，直接在策略空间中搜索最优策略，最
大化累积奖励。这两种算法都不关注环境模型，而是直接搜索能最大化奖励的策略。这种不需要
对环境建模的方式称为无模型的方法。可以看到，基于模型和无模型的区别在于，智能体是否利
用环境模型（或称为环境的动力学模型），例如状态转移函数和奖励函数。
通过上述介绍可知，基于模型的方法可以分为两类：一类是给定（环境）模型（Given the
Model）的方法，另一类是学习（环境）模型（LearntheModel）的方法。对于给定模型的方法，
智能体可以直接利用环境模型的奖励函数和状态转移函数。例如，在AlphaGo算法(Silveretal.,
2016)中，围棋规则固定且容易用计算机语言进行描述，因此智能体可以直接利用已知的状态转
移函数和奖励函数进行策略的评估和提升。而对于另一类学习模型的方法，由于环境的复杂性或
不可知性，我们很难描述整个动力系统的规律。此时智能体无法直接获取模型，可行的替代方式
是先通过与环境交互学习环境模型，然后将模型应用到策略评估和提升的过程中。
第二类的典型例子包括WorldModels算法(Haetal.,2018)、I2A算法(Racanièreetal.,2017)
等。例如在WorldModels算法中，智能体首先使用随机策略与环境交互收集数据(S ,A ,S )，
t t t+1
再使用变分自编码器（VariationalAutoencoder，VAE）(Baldi,2012)将状态编码为低维潜向量z 。
t
然后利用数据(Z ,A ,Z )学习潜向量z 的预测模型。有了预测模型之后，智能体便可以通过
t t t+1
习得的预测模型提升策略能力。
基于模型的方法的主要优点是，通过环境模型可以预测未来的状态和奖励，从而帮助智能体
进行更好的规划。一些典型的方法包括朴素规划方法、专家迭代(Suttonetal.,2018)方法等。例
如，MBMF算法(Nagabandietal.,2018)采用了朴素规划的算法；AlphaGo算法(Silveretal.,2016)
采用了专家迭代的算法。基于模型的方法的缺点在于，存在或构建模型的假设过强。现实问题中
环境的动力学模型可能很复杂，甚至无法显式地表示出来，导致模型通常无法获取。另一方面，
在实际应用中，学习得到的模型往往是不准确的，这给智能体训练引入了估计误差，基于带误差
模型的策略的评估和提升往往会造成策略在真实环境中失效。
相较之下，无模型的方法不需要构建环境模型。智能体直接与环境交互，并基于探索得到的
样本提升其策略性能。与基于模型的方法相比，无模型的方法由于不关心环境模型，无须学习环
境模型，也就不存在环境拟合不准确的问题，相对更易于实现和训练。然而，无模型的方法也有
其自身的问题。最常见的问题是，有时在真实环境中进行探索的代价是极高的，如巨大的时间消
耗、不可逆的设备损耗及安全风险，等等。比如在自动驾驶中，我们不能在没有任何防护措施的
情况下，让智能体用无模型的方法在现实世界中探索，因为任何交通事故的代价都将是难以承
受的。
第 4、5 和 6 章中介绍的算法都是无模型算法，包括深度 Q 网络（Deep Q-Network，DQN）
112
3.2 基于价值的方法和基于策略的方法
算法(Mnihetal.,2015)、策略梯度（PolicyGradient）方法(Suttonetal.,2000)、深度确定性策略
梯度（DeepDeterministicPolicyGradient，DDPG）算法(Lillicrapetal.,2015)等。虽然无模型方
法仍然是现在的主流方法，但由于其采样效率（Sample Efficiency）低的缺点很难克服，天然具