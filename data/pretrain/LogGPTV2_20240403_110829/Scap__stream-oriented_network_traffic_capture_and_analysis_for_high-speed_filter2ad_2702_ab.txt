Sets a stream’s parameter: inactivity_timeout, chunk_size,
overlap_size, ﬂush_timeout, reassembly_mode
Keeps the last chunk of a stream in memory
Returns the next packet of a stream
Reads overall statistics for all streams seen so far
Closes an Scap socket
Table 1: The main functions of the Scap API.
function takes as a single argument a stream_t descriptor sd,
which corresponds to the stream that triggered the event. This de-
scriptor provides access to detailed information about the stream,
such as the stream’s IP addresses, port numbers, protocol, and di-
rection, as well as useful statistics such as byte and packet counters
for all, dropped, discarded, and captured packets, and the times-
tamps of the ﬁrst and last packet of the stream. Among the rest
of the ﬁelds, the sd->status ﬁeld indicates whether the stream
is active or closed (by TCP FIN/RST or by inactivity timeout), or
if its stream cutoff has been exceeded, and the sd->error ﬁeld
indicates stream reassembly errors, such as incomplete TCP hand-
shake or invalid sequence numbers. There is also a pointer to the
stream_t in the opposite direction, and stream’s properties like
cutoff, priority, and chunk size.
The stream processing callback can access the last chunk’s data
and its size through the sd->data and sd->data_len ﬁelds.
In case no more data is needed, scap_discard_stream()
can notify the Scap core to stop collecting data for this stream.
Chunks can be efﬁciently merged with following ones using scap_
keep_chunk(). In the next invocation, the callback will receive
a larger chunk consisting of both the previous and the new one. Us-
ing the stream descriptor, the application is able to set the stream’s
priority, cutoff, and other parameters like stream’s chunk size, over-
lap size, ﬂush timeout, and reassembly mode.
In case they are needed by an application, individual packets can
be delivered using scap_next_stream_packet(). Packet
delivery is based on the chunk’s data and metadata kept by Scap’s
packet capture subsystem for each packet. Based on this metadata,
even reordered, duplicate, or packets with overlapping sequence
numbers can be delivered in the same order as captured. This al-
lows Scap to support packet-based processing along with stream-
based processing, e.g., to allow the detection of TCP attacks such
as ACK splitting [41]. In Section 6.5.3 we show that packet de-
livery does not affect the performance of Scap-based applications.
The only difference between Scap’s packet delivery and packet-
based capturing systems is that packets from the same stream are
processed together, due to the chunk-based delivery. As an added
beneﬁt, such ﬂow-based packet reordering has been found to sig-
niﬁcantly improve cache locality [35].
The stream’s processing time and the total number of processed
chunks are available through the sd->processing_time and
sd->chunks ﬁelds. This enables the identiﬁcation of streams
that are processed with very slow rates and delay the application,
e.g., due to algorithmic complexity attacks [34, 45]. Upon the de-
tection of such a stream, the application can handle it appropriately,
e.g., by discarding it or reducing its priority, to ensure that this ad-
versarial trafﬁc will not affect the application’s correct operation.
3.3 Use Cases
We now show two simple applications written with Scap.
3.3.1 Flow-Based Statistics Export
The following listing shows the code of an Scap application for
gathering and exporting per-ﬂow statistics. Scap already gathers
these statistics and stores them in the stream_t structure of each
stream, so there is no need to receive any stream data. Thus, the
stream cutoff can be set to zero, to efﬁciently discard all data. All
the required statistics for each stream can be retrieved upon stream
termination by registering a callback function.
1
2
3
4
5
6
7
8
9
10
11
12
scap_t *sc = scap_create("eth0", SCAP_DEFAULT,
SCAP_TCP_FAST, 0);
scap_set_cutoff(sc, 0);
scap_dispatch_termination(sc, stream_close);
scap_start_capture(sc);
void stream_close(stream_t *sd) {
export(sd->hdr.src_ip, sd->hdr.dst_ip,
sd->hdr.src_port, sd->hdr.dst_port,
sd->stats.bytes, sd->stats.pkts,
sd->stats.start, sd->stats.end);
}
worker 
thread 1
worker 
thread 2
Scap API calls
Scap stub
worker 
thread 3
Application
worker 
thread 4
event queues
Kernel
Scap
Scap
Scap
Scap
kernel module
kernel module
kernel module
kernel module
packets
NIC driver
RX queue 1
RX queue 2
RX queue 3
RX queue 4
NIC
Scap kernel module
Event creation
Memory management
stream_t handling
Stream reassembly
packets
sd1
sd2
event
queue
memory 
pages
shared memory
with  Scap stub
stream_t
hashtable
sd1
sd2
sd1
...
sd2
...
Figure 1: Overview of Scap’s architecture.
Figure 2: The operation of the Scap kernel module.
In line 1 we create a new Scap socket for capturing streams from
the eth0 interface. Then, we set the stream cutoff to zero (line 3)
for discarding all stream data, we set the stream_close() as
a callback function to be called upon stream termination (line 4),
and ﬁnally we start the capturing process (line 5). The stream_
close() function (lines 7–12) exports the statistics of the stream
through the sd descriptor that is passed as its argument.
3.3.2 Pattern Matching
The following listing shows the few lines of code that are re-
quired using Scap for an application that searches for a set of known
patterns in the captured reassembled TCP streams.
1
2
3
4
5
6
7
8
9
scap_t *sc = scap_create("eth0", 512M,
SCAP_TCP_FAST, 0);
scap_set_worker_threads(sc, 8);
scap_dispatch_data(sc, stream_process);
scap_start_capture(sc);
void stream_process(stream_t *sd) {
search(patterns, sd->data, sd->len, MatchFound);
}
We begin by creating an Scap socket without setting a cutoff,
so that all trafﬁc is captured and processed (lines 1–2). Then, we
conﬁgure Scap with eight worker threads, each pinned to a sin-
gle CPU core (assuming a machine with eight cores), to speed up
pattern matching with parallel stream processing. Finally, we regis-
ter stream_process() as the callback function for processing
stream chunks (line 4) and start the capturing process (line 5). The
search() function looks for the set of known patterns within
sd->len bytes starting from the sd->data pointer, and calls
the MatchFound() function in case of a match.
4. ARCHITECTURE
This section describes the architecture of the Scap monitoring
framework for stream-oriented trafﬁc capturing and processing.
4.1 Kernel-level and User-level Support
Scap consists of two main components: a loadable kernel mod-
ule and a user-level API stub, as shown in Figure 1. Applications
communicate through the Scap API stub with the kernel module to
conﬁgure the capture process and receive monitoring data. Conﬁg-
uration parameters are passed to the kernel through the Scap socket
interface. Accesses to stream_t records, events, and actual
stream data are handled through shared memory. For user-level
stream processing, the stub receives events from the kernel module
and calls the respective callback function for each event.
The overall operation of the Scap kernel module is depicted in
Figure 2. Its core is a software interrupt handler that receives pack-
ets from the network device. For each packet, it locates the re-
spective stream_t record through a hash table and updates all
relevant ﬁelds (stream_t handling).
If a packet belongs to a
new stream, a new stream_t record is created and added into
the hash table. Then, it extracts the actual data from each TCP
segment, by removing the protocol headers, and stores it in the ap-
propriate memory page, depending on the stream in which it be-
longs (memory management). Whenever a new stream is created
or terminated, or a sufﬁcient amount of data has been gathered, the
kernel module generates a respective event and enqueues it to an
event queue (event creation).
4.2 Parallel Packet and Stream Processing
To scale performance, Scap uses all available cores in the system.
To efﬁciently utilize multi-core architectures, modern network in-
terfaces can distribute incoming packets into multiple hardware re-
ceive queues. To balance the network trafﬁc load across the avail-
able queues and cores, Scap uses both RSS [22], which uses a hash
function based on the packets’ 5-tuple, and dynamic load balanc-
ing, using ﬂow director ﬁlters [21], to deal with short-term load im-
balance. To map the two different streams of each bi-directional
TCP connection to the same core, we modify the RSS seeds as pro-
posed by Woo and Park [52].
Each core runs a separate instance of the NIC driver and Scap
kernel module to handle interrupts and packets from the respective
hardware queue. Thus, each Scap instance running on each core
will receive a different subset of network streams, as shown in Fig-
ure 1. Consequently, the stream reassembly process is distributed
across all the available cores. To match the level of parallelism
provided by the Scap kernel module, the Scap’s user-level stub cre-
ates as many worker threads as the available cores, hiding from
the programmer the complexity of creating and managing multiple
processes or threads. Each worker thread processes the streams de-
livered to its core by its kernel-level counterpart. This collocation
of user-level and kernel-level threads that work on the same data
maximizes locality of reference and cache afﬁnity, reducing, in this
way, context switches, cache misses [15, 37], and inter-thread syn-
chronization. Each worker thread polls a separate event queue for
events created by the kernel Scap thread running on the same core,
and calls the respective callback function registered by the applica-
tion to process each event.
5.
IMPLEMENTATION
We now give more details on the implementation of the Scap
monitoring framework.
5.1 Scap Kernel Module
The Scap kernel module implements a new network protocol for
receiving packets from network devices, and a new socket class,
PF_SCAP, for communication between the Scap stub and the ker-
nel module. Packets are transferred to memory through DMA, and
the driver schedules them for processing within the software inter-
rupt handler—the Scap’s protocol handler in our case.
5.2 Fast TCP Reassembly
For each packet, the Scap kernel module ﬁnds and updates its re-
spective stream_t record, or creates a new one. For fast lookup,
we use a hash table by randomly choosing a hash function dur-
ing initialization. Based on the transport-layer protocol headers,
Scap extracts the packet’s data and writes them directly to the cur-
rent memory offset indicated in the stream_t record. Packets
belonging to streams that exceed their cutoff value, as well as du-
plicate or overlapping TCP segments, are discarded immediately
without unnecessarily spending further CPU and memory resources
for them. Streams can expire explicitly (e.g., via TCP FIN/RST),
or implicitly, due to an inactivity timeout. For the latter, Scap
maintains an access list with the active streams sorted by their last
access time. Upon packet reception, the respective stream_t
record is simply placed at the beginning of the access list, to keep
it sorted. Periodically, starting from the end of the list, the kernel
module compares the last access time of each stream with the cur-
rent time, and expires all streams for which no packet was received
within the speciﬁed period by creating stream termination events.
5.3 Memory Management
Reassembled streams are stored in a large memory buffer allo-
cated by the kernel module and mapped in user level by the Scap
stub. For each stream, a contiguous memory block is allocated
(by our own memory allocator) according to the stream’s chunk
size. When this block ﬁlls up, the chunk is delivered for process-
ing (by creating a respective event) and a new block is allocated
for the next chunk. The Scap stub has access to this block through
memory mapping, so an offset is enough for locating each stored
chunk. To avoid dynamic allocation overhead, a large number of
stream_t records are pre-allocated during initialization, and are
memory-mapped by the Scap stub. More records are allocated dy-
namically as needed. Thus, the number of streams that can be
tracked concurrently is not limited by Scap.
5.4 Event Creation
A new event is triggered on stream creation, stream termination,
and whenever stream data is available for processing. A data event
can be triggered for one of the following reasons: (i) a memory
chunk ﬁlls up, (ii) a ﬂush timeout is passed, (iii) a cutoff value is
exceeded, or (iv) a stream is terminated. When a stream’s cutoff
threshold is reached, Scap creates a ﬁnal data processing event for
its last chunk. However, its stream_t record remains in the hash
table and in the access list, so that monitoring continues throughout
its whole lifetime. This is required for gathering ﬂow statistics and
generating the appropriate termination event.
To avoid contention when the Scap kernel module runs in paral-
lel across several cores, each core inserts events in a separate queue.
When a new event is added into a queue, the sk_data_ready()
function is called to wake up the corresponding worker thread,
which calls poll() whenever its event queue is empty. Along
with each event, the Scap stub receives and forwards to the user-
level application a pointer to the respective stream_t record. To
avoid race conditions between the Scap kernel module and the ap-
plication, Scap maintains a second instance of each stream_t
record. The ﬁrst copy is updated within the kernel, while the sec-
ond is read by the user-level application. The kernel module up-
dates the necessary ﬁelds of the second stream_t instance right
before a new event for this stream is enqueued.
5.5 Hardware Filters
Packets taking part in the TCP three-way handshake are always
captured. When the cutoff threshold is triggered for a stream, Scap
adds dynamically the necessary FDIR ﬁlters to drop at the NIC
layer all subsequent packets belonging to this stream. Note that al-
though packets are dropped before they reach main memory, Scap
needs to know when a stream ends. For this reason, we add ﬁlters
to drop only packets that contain actual data segments (or TCP ac-
knowledgements), and still allow Scap to receive TCP RST or FIN
packets that may terminate a stream.
This is achieved using the ﬂexible 2-byte tuple option of FDIR
ﬁlters. We have modiﬁed the NIC driver to allow for matching the
offset, reserved, and TCP ﬂags 2-byte tuple in the TCP header. Us-
ing this option, we add two ﬁlters for each stream: the ﬁrst matches
and drops TCP packets for which only the ACK ﬂag is set, and the
second matches and drops TCP packets for which only the ACK
and PSH ﬂags are set. The rest of the ﬁlter ﬁelds are based on each
stream’s 5-tuple. Thus, only TCP packets with RST or FIN ﬂag
will be forwarded to Scap kernel module for stream termination.
Streams may also be terminated based on an inactivity timeout.
For this reason Scap associates a timeout with each ﬁlter, and keeps
a list with all ﬁlters sorted by their timeout values. Thus, an FDIR
ﬁlter is removed (i) when a TCP RST or FIN packet arrives for
a given stream, or (ii) when the timeout associated with a ﬁlter
expires. Note that in the second case the stream may still be active,
so if a packet of this stream arrives upon the removal of its ﬁlter,
Scap will immediately re-install the ﬁlter. This is because the cutoff
of this stream has exceeded and the stream is still active. To handle
long running streams, re-installed ﬁlters get a timeout twice as large
as before. In this way, long-running ﬂows will only be evicted a
logarithmic number of times from NIC’s ﬁlters. If there is no space
left on the NIC to accommodate a new ﬁlter, a ﬁlter with a small
timeout is evicted, as it does not correspond to a long-lived stream.
Scap needs to provide accurate ﬂow statistics upon the termina-
tion of streams that had exceeded their cutoff, even if most of their
packets were discarded at the NIC. Unfortunately, existing NICs
provide only aggregate statistics for packets across all ﬁlters—not
per each ﬁlter. However, Scap is able to estimate accurate per-ﬂow
statistics, such as ﬂow size and ﬂow duration, based on the TCP
sequence numbers of the RST/FIN packets. Also, by removing the
NIC ﬁlters when their timeout expires, Scap receives packets from
these streams periodically and updates their statistics.
Our implementation is based on the Intel 82599 NIC [21], which
supports RSS and ﬂow director ﬁlters. Similarly to this card, most
modern 10GbE NICs such as Solarﬂare [46], SMC [44], Chel-
sio [10], and Myricom [30], also support RSS and ﬁltering capa-
bilities, so Scap can be effectively used with these NICs as well.
5.6 Handling Multiple Applications
Multiple applications can use Scap concurrently on the same ma-
chine. Given that monitoring applications require only read ac-
cess to the stream data, there is room for stream sharing to avoid
multiple copies and improve overall performance. To this end, all
Scap sockets share a single memory buffer for stream data and
)
%
(
d
e
p
p
o
r
d
s
t
e
k
c
a
P
100
80
60
40
20
0
0
Libnids
yaf
Scap w/o FDIR
Scap with FDIR
1
2
3
4
5
6
Traffic rate (Gbit/s)
(a) Packet loss
100
)
%
(
n
o
i
t
a
z
i
l
i