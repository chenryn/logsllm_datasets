2.8
5.9
0.3
2.1
1.9
Ours
34.0
22.2
28.8
14.0
20.3
41.4
54.3
26.3
13.2
18.8
8.7
11.2
15.0
60.9
LIE [4]
19.9
11.5
13.0
9.3
4.1
3.8
3.6
15.0
9.8
9.2
4.1
1.0
2.3
3.6
Our attacks
Min-Max Min-Sum
Fang [17]
8.7
15.9
28.4
9.4
20.3
3.8
9.1
6.7
9.0
24.0
6.0
12.8
1.8
8.9
19.7
17.2
26.4
7.3
17.8
2.6
6.5
20.1
8.5
14.9
3.6
11.3
1.5
4.4
4.5
1.2
1.9
3.2
0.2
1.0
4.3
1.2
1.0
2.6
3.6
0.1
2.0
0.6
Ours
15.8
14.4
27.9
9.8
16.4
36.7
54.3
12.7
9.4
17.6
8.1
10.7
14.7
58.1
LIE [4]
14.9
9.2
9.9
4.9
-1.6
3.4
2.3
11.8
8.8
6.7
4.0
0.8
2.1
2.7
AGR agnostic
(agnostic)
Our attacks
Min-Max Min-Sum
8.7
14.5
22.3
6.5
15.8
3.6
5.7
1.8
7.3
16.6
5.2
9.9
1.7
7.1
13.7
15.5
8.6
4.2
10.3
1.9
5.3
10.7
8.3
12.3
3.0
7.2
1.1
4.0
attacks is substantially higher than for CIFAR10. For Purchase,
our AGR-tailored and AGR-agnostic Min-Sum attacks reduce
the accuracy of Krum AGR to random guessing, i.e., close to
1%, with as few as 5% and 10% malicious clients, respectively.
For all of the AGRs, with increasing percentage of malicious
clients, the impacts of our attacks and the differences of the
impacts of ours and existing attacks increase. Interestingly for
CIFAR10, LIE that uses ∇p
std consistently outperforms Fang
that uses ∇b
sgn. This emphasizes our claim in Section VI-C that
the most effective perturbation for model poisoning depends
on the classiﬁcation task.
E. Effect of cross-device setting
In this section, we evaluate the impact of our attacks when
cross-device FL is used to learn on CIFAR10 dataset. More
speciﬁcally, in each FL epoch, instead of processing all of the
50 clients, we process only 10 clients. As before, we evaluate
for two model architectures, Alexnet and VGG11. The attack
procedures for different AGRs do not change.
Table III shows the results. Similar to cross-silo setting,
our AGR-tailored attacks outperform the state-of-the-art Fang
attacks for both Alexnet and VGG11 architectures. For Alexnet
with agr-updates adversary, our attack is 2× (Trimmed-
mean) to 11× (Median) more impactful than Fang attack.
We note similar results for agr-only adversary as well as
VGG11 architecture in Table III.
For AGR-agnostic adversaries with Alexnet, we note that at
least one of our Min-Sum and Min-Max attacks has up to 5×
more attack impact than the state-of-the-art LIE attack, for all
but Krum AGR. For Krum AGR, LIE outperforms our attack
by 0.2% and 1.2% under updates-only and agnostic
adversaries, respectively. We note similar results for Alexnet
with agnostic adversary. In case of VGG11 as well, at
least one of our AGR-agnostic attacks has up to 10× more
impact than LIE, for all but Multi-krum and AFA AGRs. For
Figure 4: Effect of degree of non-iid nature of data on the impact of
model poisoning attacks on FL. We use partial gradients knowledge,
agr-only and agnostic, adversaries.
2) Effect of the percentage of malicious clients: Figure 3
shows the impacts of model poisoning attacks when the
percentage of malicious clients in FL is varied from 5% to
24% for CIFAR10 with Alexnet and Purchase; Figure 6a
shows the results for MNIST and FEMNIST datasets. We
use partial gradients knowledge, agr-only and agnostic,
adversaries. We note that, all of our attacks outperform
the existing attacks for all the combinations of percentage
of malicious clients, AGR algorithms, dataset, and model
architectures. For CIFAR10 with Krum, our AGR-tailored
attack has impact of more than 43%, i.e., it reduces accuracy to
random guessing, 10%, even with just 5% clients. For Purchase
dataset, the distinction between impacts of ours and existing
12
0.10.30.50.70.9Non-iid degree5101520253035Attack impactMNIST + Multi-krumLIEFangOur AGR-tailoredOur Min-MaxOur Min-Sum0.10.30.50.70.9Non-iid degree0510152025MNIST + Median0.10.30.50.70.9Non-iid degree020406080Attack impactPurchase + Multi-krum0.10.30.50.70.9Non-iid degree0102030405060Purchase + MedianMulti-krum and AFA, the LIE and Min-Max have almost equal
attack impacts.
Finally we note that, overall the attack impacts are lower
in cross-device setting than in cross-silo setting; the reduction
in impacts varies widely based on AGR and model archi-
tecture. For instance, for Alexnet with Krum, Multi-krum,
and Trimmed-mean, the impacts reduce by 9.5%, 14.6%, and
31.8%, respectively. The reason for this is that, in cross-device
FL, the adversary cannot constantly corrupt the global model.
Because,
the number of
malicious clients that the server selects can be negligible.
in many cross-device FL epochs,
VII. THE DIVIDE AND CONQUER DEFENSE (DNC)
Our strong attacks clearly motivate the need for more
robust AGRs to defeat untargeted model poisoning attacks on
FL. In this section, we ﬁrst give the concrete lessons learned
from our attacks that can guide the designs of future robust
AGRs. Based on these lessons, we propose a novel robust AGR
algorithm called Divide-and-Conquer (DnC). Unlike state-of-
the-art robust AGRs, which use distance based [31], [8] or
simple pruning based ﬁlters [39], DnC performs dimensionality
reduction using random sampling followed by spectral methods
based outliers removal.
A. Lessons learned from our attacks
L1: The curse of dimensionality. The theoretical error
bounds provided by previous robust AGRs [8], [31], [39],
[16], [26], [2] depend on the dimensionality of their inputs.
Hence, the theoretical as well as empirical errors of these
defenses explode for high dimensional gradients of neural
networks [10] in FL. Therefore, reducing the dimensionality
of input gradients is necessary to improve robustness against
poisoning.
L2: Convergence is necessary but not sufﬁcient. All
prior robust AGRs [8], [31], [39] give provable convergence
guarantees for non-convex FL. However, for non-convex op-
timizations, such guarantees are meaningless due to large
number of suboptimial local optima. Our attacks exploit this
and force the global model to converge to a suboptimial local
optimum. Therefore, providing convergence guarantees is not
enough and robust AGRs should provide guarantees on how
well they detect and remove outliers.
L3: Distance- or dimension-wise pruning is insufﬁcient.
Krum, Multi-krum, and Bulyan use (cid:96)p distance-based ﬁltering,
which, as [31], [4] point out and we show in our work,
allows malicious gradients to be close enough to benign
gradients while far enough to effectively poison the global
model. Dimension-wise pruning in Trimmed-mean and Median
allows an adversary to craft gradients which signiﬁcantly shift
the aggregate in bad directions as our and Fang [17] attacks
show. Therefore, robust AGRs need to go beyond just using
dimension/distance-based ﬁltering.
Algorithm 2 Our Divide-and-Conquer AGR Algorithm
1: Input: Input gradients ∇{i∈[n]}, ﬁltering fraction c, number of
malicious clients m, niters, dimension of subsamples b, input
gradients dimension d
2: Igood ← ∅
3: while i  t] < 
[|(cid:104)X − µU , v(cid:105)| < t] < 
Pr
X∼B
Pr
X∼M
If we consider that B and M (the distributions of benign
and malicious gradients, respectively) are -spectrally separa-
ble, then by removing -fraction of gradients with maximum
projections along the top eigenvector direction we can remove