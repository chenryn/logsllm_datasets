used, since these are also speciﬁed subjectively. Assuming
that the strategies used to decompose goals are viewed as
being equally important, using equal weights appears to be
a reasonable way forward.
Finally, the model must be extended for interpreting its
results reasonably as a basis for decision making, e.g., by
deﬁning the thresholds at which the conﬁdence assessed is
considered sufﬁcient as to accept or reject the claims made.
2) Towards Internal Measures of Quality: The above
items present a promising avenue by which to deﬁne an
integrated internal measure for quality, e.g., by extending the
measure of hazard coverage, say, to account for argument
validity expressed as a conﬁdence level. As a preliminary
step along these lines, we deﬁned an initial set of mea-
sures (including coverage), for objectively evaluating our
approach. Table I gives the results of applying these mea-
sures to the Swift UAS safety case. The measures indicate
reasonably high, but not complete, coverage of the hazards
and corresponding high-level requirements considered, i.e.,
about 73% and 87% respectively. This reﬂects the fact
that not all claims corresponding to the hazards and high-
level requirements, in the argument fragment, have been
completely developed, i.e., end in available evidence.
On the other hand, we measure perfect coverage of low-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:49 UTC from IEEE Xplore.  Restrictions apply. 
level requirements by claims, since all corresponding claims
are both auto-generated and terminate in evidence. However,
the extent to which the low-level requirements are actually
covered (by the veriﬁcation) is not perfect. This is reﬂected
in the last row of Table I, where about 92% of the code
is covered by auto-generated claims. This measure indicates
that there are parts of the code which were not reasoned
about, were not covered by a requirement, and therefore
remained unproven. To use these initial measures for quality
assessment, we require a quality model, similar to that used
to interpret the outcome of uncertainty assessment.
D. Implications on Processes
The existing framework used by the Swift UAS engineer-
ing team for systems development is fairly mature, based
on extensive engineering knowledge and experience with
developing previous UAVs, and the development of certain
functionality is relatively rapid. Thus, there is a need to
tailor the safety methodology such that well-deﬁned inter-
faces exist to the system development process, so that the
safety process can keep up with development and actively
inﬂuence it. To this end, having pre-deﬁned intervals in the
development process to synchronize with the safety process,
appears to be a reasonable way forward. Another alternative,
is to synthesize the development process from the safety
activities, as in [18].
The process as we applied it and the resulting arguments
are “heavyweight” in proportion to the development costs
and effort involved. Nevertheless, its application is justiﬁed
largely because loss of the Swift UAV is an intolerable risk.
This is due, in part, to the uniqueness of its avionics system,
the modiﬁcations that were made to the airframe for mission-
speciﬁc purposes, and the extent of research investments in
the UAV. However, for smaller UAV, the loss of aircraft may
not always be an intolerable risk. Thus, there is a need for
a lightweight version of our approach, wherein the effort
involved is appropriately proportional to the costs.
The development team found software hazard analysis to
be useful in understanding and appreciating the subtleties of
software contributions to system hazards. The engineers also
adopted and integrated the procedures for system/software
hazard analysis into their development process such that
safety is proactively included as a consideration in the
development of subsequent functionality, e.g., the electric-
battery management system.
E. Implications of Existing Guidelines/Standards
Many items of evidence used in the ASSC (Section III)
can be mapped to lifecycle data required for compliance with
DO-178B, e.g., evidence E1 of requirements review in the
argument (Figure 2(a)), is mapped to Objective 2 in Table
A4 in DO-178B (i.e., “Low level requirements are accurate
and consistent”). The added value of our software safety
argument is that it explains how the evidence, potentially
compliant with DO-178B, can provide sufﬁcient assurance,
particularly with regard to claims concerning how software
behavior relates to speciﬁc system hazards (e.g., the link
between incorrect autopilot behavior at the software level
and a loss event at the system level).
The areas of a hazard directed safety argument which
would be least supported by DO-178B evidence are those
related to the analysis of hazardous software failure modes.
This relates to the relationship between assuring safety and
demonstrating correctness for software. Speciﬁcally, the DO-
178B guidance implies that safety analysis is a system-
level process. As such, the role of software development is
to demonstrate correctness against requirements, including
safety requirements allocated to software, as generated from
the system-level processes. The process of reﬁning and im-
plementing these requirements at subsequent stages does not
involve any explicit application of software hazard analysis.
To this end, in DO-178B, the link between software behavior
and the required safety properties of the system is implicit.
However, others [9] have argued that hazard analysis
should be applied at the software level for each develop-
ment stage (e.g., performing hazard analysis at the software
architecture, design and source code stages). The reasoning
underlying their approach is that errors could be introduced
at each software development stage, which have the potential
to lead to hazardous failure conditions at the system level. As
such, it is important to provide assurance, through explicit
software safety arguments, that these software errors have
been identiﬁed and managed.
Finally, the explicit representation of a software safety
argument is useful for developers wishing to use alterna-
tive means for compliance, e.g., by generating evidence
from formal mathematical analysis rather than from testing.
Where alternative approaches are used, a reviewer must be
convinced of the relevance and suitability of the alternative
evidence. Previous experience [19] has shown that this is
much easier to achieve using a documented safety argu-
ment. This issue is particularly important now, as DO-178B
has been updated (to DO-178C), with guidance on formal
In the software assurance for the Swift UAS, we per-
formed hazard analysis down to the software architecture
level, identifying how software components and interactions
can contribute to system hazards. Then we deﬁned safety
requirements, and allocated them to the components and
interactions. Subsequently, we demonstrated safety assur-
ance by appealing to the correctness of the implementation
of these interactions and components against the allocated,
and formalized, safety requirements (e.g., correctness of the
autopilot software against allocated safety requirements).
The criterion for deciding that no further hazard analysis
was needed at subsequent stages, was that a software compo-
nent was simple enough (as gauged by the developers) that
requirements could be formulated to demonstrate desired
behavior and the absence of unintended behavior.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:49 UTC from IEEE Xplore.  Restrictions apply. 
methods, and to allow assurance arguments as an alternative
method for establishing compliance. Although compliance
with DO-178B is currently not required for the Swift UAS
software, this paper provides an example and assessment
of how assurance arguments can be developed for airborne
software, justifying how evidence generated from formal
methods can be used to support claims about safe software
behavior.
V. CONCLUSIONS AND OUTLOOK
Our perspectives in this paper are mainly based on an
interim safety case, which represents only a small part of
the overall Swift UAS safety case (which, in turn, includes
arguments assuring the safety of the ground system, the
communication infrastructure, and UAS operation, besides
those for the airborne system and its software). We note
that the interim safety case alone is insufﬁcient to make any
categorical claims about safety improvement of the system.
Rather, the safety argument is intended to communicate
that certain speciﬁc claims made can be defended with rea-
sonable conﬁdence based on the evidence, the assumptions
and the context supplied therein. Although we created the
safety case, in part, by using a formal veriﬁcation tool, the
methodology applies more generally to include other safety
and dependability techniques.
Based on the feedback received, we are actively pursuing
research directions that are expected to improve, in general,
the practical application of safety cases, and the Swift
UAS safety case in particular. This includes improving the
modularity of the argument via abstraction to aid argument
comprehension, assessing the soundness of the manually
created argument fragments by systematically and more
extensively evaluating the argument for fallacies [20], better
characterizing the diversity of available evidence to facilitate
automatic assembly of the safety argument, and tailoring the
methodology to make it lightweight.
The need to manage and reconcile diverse information in
both the system and software safety cases becomes apparent
from the perspective of not only safety, but also compliance
to airworthiness requirements for operating a UAS [6],
where the overarching goal is to show that a level of safety
equivalent to that of manned operations exists [21]. We
believe that the approach and experience documented in this
paper is a step towards that end.
ACKNOWLEDGMENTS
NASA contract NNA10DE83C funded this work. We also
thank Mark Sumich and Corey Ippolito for their feedback.
REFERENCES
[1] RTCA SC-167 and EUROCAE WG-12, “Software Consid-
erations in Airborne Systems and Equipment Certiﬁcation,”
DO-178B/ED-12B, 1992.
[2] F. Redmill, “Safety integrity levels – theory and problems,
lessons in system safety,” in Proc. 18th Safety-Critical Sys.
Symp., Feb. 2010.
[3] Goal
Structuring Notation Working Group,
Community Standard Version 1,” Nov. 2011.
Available: http://www.goalstructuringnotation.info/
“GSN
[Online].
[4] R. Weaver, “The safety of software – constructing and
assuring arguments,” Ph.D. dissertation, Dept. of Comp. Sci.,
Univ. of York, 2003.
[5] R. Bloomﬁeld and P. Bishop, “Safety and assurance cases:
Past, present and possible future – an Adelard perspective,”
in Proc. 18th Safety-Critical Sys. Symp., Feb. 2010.
[6] K. D. Davis, “Unmanned Aircraft Systems Operations in
the U.S. National Airspace System,” Interim Operational
Approval Guidance 08-01, FAA Unmanned Aircraft Systems
Program Ofﬁce, Mar. 2008.
[7] European Organisation for the Safety of Air Navigation,
Safety Case Development Manual, 2nd ed., EUROCON-
TROL, Oct. 2006.
[8] J. Fenn, R. Hawkins, P. Williams, and T. Kelly, “Safety case
composition using contracts - reﬁnements based on feedback
from an industrial case study,” in Proc. 15th Safety-Critical
Sys. Symp., Feb. 2007.
[9] R. Hawkins, K. Clegg, R. Alexander, and T. Kelly, “Using
a software safety argument pattern catalogue: Two case
studies,” in Proc. Intl. Conf. on Comp. Safety, Reliability and
Security (SafeComp), Sep. 2011.
[10] E. Denney and S. Trac, “A software safety certiﬁcation tool
for automatically generated guidance, navigation and control
code,” in IEEE Aerospace Conf. Electronic Proc, 2008.
[11] FAA, System Safety Handbook, Federal Aviation Adminis-
tration, Dec. 2000.
[12] C. Menon, R. Hawkins, and J. McDermid, “Interim standard
of best practice on software in the context of DS 00-56 Issue
4,” Soft. Sys. Eng. Initiative, Univ. of York, Standard of Best
Practice Issue 1, 2009.
[13] E. Denney, I. Habli, and G. Pai, “Towards measurement of
conﬁdence in safety cases,” in Proc. 5th Intl. Symp. Empirical
Soft. Eng. and Measurement (ESEM), Sep. 2011.
[14] UK Ministry of Defence (MOD), “Safety management re-
quirements for defence systems,” Defence Standard 00-56
Issue 4, Jun. 2007.
[15] T. Kelly and J. McDermid, “Safety case patterns – reusing
successful arguments,” in Proc. IEE Colloq. on Understand-
ing Patterns and Their Application to Sys. Eng., 1998.
[16] P. Bishop, R. Bloomﬁeld, B. Littlewood, A. Povyakalo, and
D. Wright, “Towards a formalism for conservative claims
about the dependability of software-based systems,” IEEE
Trans. Soft. Eng., vol. 37, no. 5, pp. 708 – 717, 2011.
[17] V. Basili, G. Caldiera, and D. Rombach, “Goal question
metric approach,” in Encyclopedia of Soft. Eng.. John Wiley,
1994, pp. 528–532.
[18] P. Graydon and J. Knight, “Software process synthesis in
assurance based development of dependable systems,” in
Proc. European Dependable Comp. Conf. (EDCC), Apr.
2010, pp. 75–84.
[19] I. Habli and T. Kelly, “A generic goal-based certiﬁcation
argument for the justiﬁcation of formal analysis,” in Proc.
Certiﬁcation of Safety-critical Software Controlled Sys. (Safe-
Cert), Mar. 2008.
[20] W. Greenwell, J. Knight, C. M. Holloway, and J. Pease, “A
taxonomy of fallacies in system safety arguments,” in Proc.
Intl. Sys. Safety Conf., 2006.
[21] J. Elston, M. Stachura, B. Argrow, E. Frew, and C. Dixon,
“Guidelines and Best Practices for FAA Certiﬁcate of Autho-
rization Applications for Small Unmanned Aircraft,” in Proc.
AIAA Infotech@Aerospace Conf., Mar. 2011.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:49 UTC from IEEE Xplore.  Restrictions apply.