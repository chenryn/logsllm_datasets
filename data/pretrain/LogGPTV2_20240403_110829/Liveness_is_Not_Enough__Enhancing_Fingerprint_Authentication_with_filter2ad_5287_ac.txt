# of Subjects / Attackers
90
Postures
Sitting, standing, lying,
walking, running
2A
2B
3
4A
4B
4C
2, 3, 5, 7 †
10, 11, 12, 13 ‡
Added Aug. 2019
24, 24, 22, 21
62, 61, 59, 53
64
2 †, 10 and 11 ‡
15
Sitting
Sitting
Sitting
†: Data collected at the university; ‡: data collected at the company.
Device
OnePlus3
OnePlus3
Xperia XZ1, Oneplus5,
Vivo X21
OnePlus3
# of Data Points
63,000
18,200
47,000
3,200
3,600
3,600
3,600
tion and ﬁnds the surface of a minimal hyper-sphere which
contains the objective data points as many as possible. The
distance between data points and the hyper-sphere is the clas-
siﬁcation score, which is leveraged to conduct prediction.
OC-SVM has been successfully applied to many anomaly de-
tection problems, such as utterance veriﬁcation [37], malware
detection [31], and online fault detection [78].
LOF measures the local deviation of the data point to its
neighbors [18]. It decides whether a data point is an outlier us-
ing the anomaly score depending on the local density. Specif-
ically, locality density is estimated by k-nearest neighbors
based on a given distance metric. A data point with a substan-
tially lower density than their neighbors will be regarded as
an outlier.
IF is a rapid one-class classiﬁcation method for high-
dimensional data based on ensemble learning, which assumes
that abnormal data points are easier to isolate from given
one-class instances [47]. IF detects abnormal data points by
subsampling the dataset to construct iTrees, and further in-
tegrate multiple iTrees into a forest to detect abnormal data.
A data point is seen as abnormal when these random trees
collectively produce shorter path lengths for it.
6 Experiment Design and Data Collection
To collect the experiment data, we develop a prototype sys-
tem on Android 7.1 (API level 25). Speciﬁcally, our im-
plementation hooks the authenticate() method from the
FingerprintManager class. We set the data collection time
(t) as 0.5 seconds and the sampling rate ( fs) as 200 Hz.
After receiving the IRB approval from our university in
June 2018, we started recruiting subjects for the data collec-
tion, which lasted for 5 months. To qualify for the experiment,
a subject must self-identify as a frequent smartphone user
who had been using ﬁngerprint authentication for more than
a year. 90 subjects were involved in ﬁnger-tip behavior data
collection, who were aged from 22 to 45. 39 subjects were
female, and 51 were male. 24 of them were students in our
university, and the rest were employees in a company. Another
15 subjects (4 from our university, 11 from the company), in-
cluding 4 females and 11 males, were recruited to play the
role of an attacker to carry out artiﬁcial replica attack, puppet
attack and mimicry attack on the 90 subjects.
We explained to each subject the purpose of this research
project, the data we collect, and the steps we take to protect
their personal identiﬁable information. During the data col-
lection, we asked each subject to hold a smartphone in hand
as they normally unlock their own devices. To help collect
more distinct data points, we also suggested that they hold the
device in different angles and directions. Table 3 summarizes
the compiled 4 datasets:
1) Dataset-1. For this dataset, we used one smartphone
(OnePlus 3 with 6G RAM) to eliminate factors that could be
introduced by different phones. This device has a capacitive
ﬁngerprint sensor that is integrated with the home button.
In week 1, the 24 subjects from our university were ﬁrst
asked to enroll their ﬁngerprints on the phone. Then, a subject
needed to perform successful ﬁngerprint logins for 500 times
while sitting (stationary), and for 50 times while standing
(stationary), lying (stationary), walking (moving), and running
(moving), respectively. Note that we only collect the ﬁnger-tip
behavior data when a login is successful. In week 8 and 9, the
66 subjects from the company went through the same data
collection procedure. Each subject spent 13 - 17 minutes to
ﬁnish this task. As a result, we collected 90× 700 = 63,000
data points for the dataset-1.
2) Dataset-2. To evaluate the consistency of the ﬁngertip-
touch behavior features over the long term, we compiled the
dataset-2 with the same subjects after some time intervals: i)
dataset-2A. The 24 subjects from our university came in week
2, 3, 5, 7 to perform 50 successful ﬁngerprint authentications
while sitting; ii) dataset-2B. The subjects in the company did
the same thing in week 10, 11, 12 and 13. Some subjects did
not show up for all the collections. As a result, we collected
65,200 data points in total for the dataset-2.
3) Dataset-3. To evaluate the generalization of FINAUTH
on different devices, we collected the dataset-3 on 3 smart-
phones: Xperia XZ1 (side ﬁngerprint sensor), Oneplus 5 (back
ﬁngerprint sensor), and Vivo X21 (in-screen ﬁngerprint sen-
sor). The 22 subjects from our university were assigned to
Xperia XZ1, while the 42 subjects from the company were
2224    29th USENIX Security Symposium
USENIX Association
measuring the performance. Section 7.2 shows evaluation on
how distinguishable users’ ﬁngertip-touch behaviors are un-
der different conditions using dataset-1, 2, and 3. Section 7.3
evaluates FINAUTH’s effectiveness against presentation at-
tacks using dataset-4. Section 7.4 presents system perfor-
mance of FINAUTH. Section C reports user acceptance of
FINAUTH. Section 7.5 illustrates other design considerations
behind FINAUTH.
Speciﬁcally, the base CNN was trained using cross-entropy
as the loss function based on half (22,500) data points of
dataset-1 (collected while sitting) containing ﬁngertip-touch
behavior data from 90 classes (subjects). We pre-trained base
model on a PC with Intel i5-8300 CPU, 16GB RAM, GTX
1060 GPU, and the training process took 42 minutes. Keras
with TensorFlow backend was used for training. The size of
the total model is 1.54 MB, which is lightweight on mobile
devices.
FA
FR
7.1 Evaluation Metrics
We use the following metrics to evaluate the effectiveness
of FINAUTH. True acceptance (TA) means ﬁngertip-touch
behaviors from legitimate users are correctly identiﬁed. True
rejection (TR) means ﬁngertip-touch behaviors not from le-
gitimate users are correctly declined. False acceptance (FA)
means ﬁngertip-touch behaviors not from legitimate users are
incorrectly identiﬁed as legitimate. False rejection (FR) means
ﬁngertip-touch behaviors from legitimate users are incorrectly
rejected. False acceptance rate (FAR) is deﬁned as
FA+T R,
which measures the proportion of illegal users who gain ac-
cess. False rejection rate (FRR) is deﬁned as
FR+TA, which
measures the proportion of legitimate users who are denied
access. Balanced accuracy (BAC) is a metric used for evaluat-
ing models trained from unbalanced data [19]. It is deﬁned as
the average between true rejection rate (T RR = T R
T R+FA) and
true acceptance rate (TAR = TA
TA+FR). We also use receiver op-
eration characteristic (ROC) curves to show dynamic changes
of TAR against FAR at a varying decision threshold for per-
formance comparison. The area under the ROC curve (AUC)
is used to estimate the probability that prediction scores of
authorized users are higher than unauthorized users. While in
presentation attacks resistance evaluation, we leverage FAR,
i.e., attack success rate, as the evaluation criteria, which is the
ratio between the number of incorrectly identiﬁed data points
and the number of all attack data points. It implies the proba-
bility of attackers bypassing the authentication system. Note
that, FAR is more important in ﬁngerprint authentication, e.g.,
achieving FAR as low as 10−6 while still maintaining an FRR
of 1% [5].
Figure 4: Artiﬁcial ﬁngerprint replica. The left is the mold
used to capture ﬁngerprint; the right is a fake ﬁngerprint
crafted using silicone rubber.
assigned to the other two devices randomly. Each subject was
asked to conduct 50 authentications while sitting. As a result,
we collected 3,200 data points for the dataset-3.
4) Dataset-4. We used artiﬁcial replica attack, puppet attack
and mimicry attack to evaluate the effectiveness of FINAUTH.
It is infeasible to ask each attacker to attack all 90 subjects in
all three experiments. To increase the chance of successful at-
tacks, we collected the ﬁngertip-touch data of the 15 attackers
and used Pearson correlation distance matrix to compute the
distance between each attacker and each subject. Then, we
assign each attacker 6 subjects as his/her targets on the basis
of ﬁngertip-touch behavioral similarity:
i) Dataset-4A: artiﬁcial replica attack. We crafted a ﬁnger-
print spoof using the silicone rubber, as shown in Figure 4,
for each of the 85 subjects (5 dropped out). The spoofs were
tested to make sure they can spoof the original ﬁngerprint au-
thentication. After the experiments, the molds and synthetic
spoofs were destroyed. Each attacker was asked to spoof the
ﬁngerprint sensor while sitting for 50 attempts per subject.
We collected 50× 85 = 4,250 data points for the dataset-4A;
ii) Dataset-4B: puppet attack. Each attacker was asked to
hold the device in her/his hand and place a subject’s ﬁnger on
the ﬁngerprint sensor 50 times while both of them in sitting.
We collected 4,250 data points for the dataset-4B. Note that
the unwillingness for this study is a subset of all possible
puppet attacks since we do not have data on other kinds of
unwillingness, e.g. the victim is sleeping or passed out;
iii) Dataset-4C: mimicry attack. Each attacker was asked
to carefully observe a subject’s hand and device movement in
a close distance (no more than 2 feet). After the attacker was
conﬁdent about what they observed, she/he would mimick the
subject’s ﬁngertip-touch behavior with the crafted ﬁngerprint
spoofs for 50 times. We collected 4,250 data points for the
dataset-4C.
7 Evaluation
In this section, we report the evaluation results of the pro-
posed system. Section 7.1 presents the metrics we used in
7.2 Reliability Analysis
To ﬁnd out how distinguishable each user’s ﬁngertip-touch
behaviors are, we randomly split each user’s data points, train
USENIX Association
29th USENIX Security Symposium    2225
Table 4: BAC (%) under different k for CNN-based feature
learning.
Table 5: BAC (%), FAR (%), FRR (%), and AUC under three
different feature sets and four different one-class classiﬁers.
# Layer k
3
6
9
11
PCC
86.72
91.27
93.53
94.65
OC-SVM LOF
IF
78.69
82.67
84.32
90.69
84.96
87.28
94.34
97.99
86.15
88.91
90.09
93.63
a model for each of them, and use her/his remaining data
points and other users’ data points to evaluate the model.
We report the performance of using different feature sets,
classiﬁers, training dataset size, and datasets in the rest of this
section.
7.2.1 Different Feature Sets and Classiﬁers
CNN-based Feature Learning. We trained the base CNN with
22,500 sitting data points in the dataset-1, and then leveraged
the output of the base model’s intermediate layer (kth layer)
as extracted features. To ﬁnd the optimal k, we evaluated each
classiﬁer’s performance with 30 training data points from
the ﬁrst pooling layer (3rd layer) to the ﬁrst fully-connect
layer (11th layer). Table 4 shows the averaged BAC when
using features extracted with different layers under different
classiﬁers. As the results show, with the features from the
11th layer, classiﬁers achieve higher BAC.
Results. After determining the best k for the CNN-based
feature learning, we obtained three feature sets: i) time- and
frequency-domain features (TFF) extracted via feature ex-
traction and selection (Section 4.1); ii) CNN-based features
(CNF) extracted with the pre-trained model (Section 4.2); and
iii) the union of feature sets of the aforementioned two (UnF).
We used the grid search to ﬁnd the best parameter com-
binations for each classiﬁer. For OC-SVM, we found radial
basis function works best with γ = 0.25 and ν = 0.1. For IF,
the optimal parameter of n_estimators was 20. For LOF,
we used Minkowski distance as the distance metric with the
optimal parameter of n_neighbors as 5.
Figure 5 shows ROC curves of using the three feature sets
under different one-class classiﬁers. The results indicate that
CNN-based features are more discriminative than time- and
frequency-domain features. Speciﬁcally, for PCC and LOF,
the BAC of models using CNN-features is signiﬁcantly higher
than using time- and frequency-domain features. However,
the performance of OC-SVM and IF of CNN-based features
is poorer. Another observation is that the union of two fea-
ture sets brings slight improvement over only one feature
set. Table 5 shows the BAC, FAR, FRR, and AUC under dif-
ferent feature set and classiﬁer combinations. Even though
UnF + LOF has the best BAC, CNF + LOF is the most reliable
model with low FAR. For the rest of the evaluations, we use
the CNF + LOF approach.
Feature Set + Classiﬁer
TFF + PCC
TFF + OC-SVM
TFF + LOF
TFF + IF
CNF + PCC
CNF + OC-SVM
CNF + LOF
CNF + IF
UnF + PCC
UnF + OC-SVM
UnF + LOF
UnF + IF
BAC
84.41
91.49
93.28
96.07
94.65
90.69
97.99
93.63
94.76
93.78
98.02
96.88
FAR
11.85
5.56
4.32
2.51
3.30
6.41
0.86
3.72
2.86
4.06
1.52
2.03
FRR
19.34
11.45
9.13
5.35
7.40
12.21
3.16
9.02
7.62
8.37
2.43
4.21
AUC
0.9169