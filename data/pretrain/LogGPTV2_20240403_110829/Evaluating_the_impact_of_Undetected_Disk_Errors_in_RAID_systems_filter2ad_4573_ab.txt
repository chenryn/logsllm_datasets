rive the rate of UDEs from the data in [2]. We look at age-
normalized disk drives and their statistics. There are approxi-
mately 270,000 nearline drives and 1,150,000 enterprise drives
in the 17-month period observed in [2]. We assume that mul-
tiple corrupt ﬁelds (e.g. multiple checksum mismatches) on
the same drive are manifestations of the same event. Thus,
we use the count of corrupt drives as equivalent to the count
of corruption events. The spatial locality analysis in the [2]
appears to validate this assumption. In [2] corruption events
are not classiﬁed into dropped writes, far off-track writes and
near off-track writes. Instead they are classiﬁed by the way
they manifest in the detection logic of the system. Corruptions
in [2] are classiﬁed as checksum mismatches (the checksum
is incorrect), identity mismatches (the intended LBA does not
match the actual LBA), and parity mismatches (the data does
not pass a parity scrub). We believe that checksum mismatches
are caused by far off-track writes that write across 4 KB ﬁle
system data blocks, and by hardware bit corruption. Identity
and parity mismatches are caused by dropped writes and near
off-track writes.
In [2], there are 1,782 reported checksum
errors, 113 identity mismatches, and 297 parity errors in the
nearline systems and 690 checksum errors, 68 identity mis-
matches, and 199 parity errors in the enterprise systems. We
combine the identity and parity errors to determine a combined
dropped and near off-track rate of 4.01·10−12/write in nearline
systems. We use the checksum error rate to determine a far-off-
track and hardware bit corruption rate of 1.4 · 10−11/write in
nearline systems. The corresponding rates for enterprise sys-
tems are 2.47 · 10−13 and 6.38 · 10−13.
These rates, summarized in Table 2, are of course estimates
made from a single (although large) ﬁeld sample, but we be-
lieve that they are sufﬁcient to make the output of the simula-
tor useful in determining the impact of UDEs. In our results
we also provide a sensitivity analysis to show how the results
change when these rates change.
Table 2. Estimated rates of UDEs in UDEs
I/O .
Estimated Rate
UDE Type
Dropped I/O
Near-off Track I/O
Far-off Track I/O
Nearline Enterprise
9 · 10−13
9 · 10−14
10−13
10−12
10−14
10−13
P(W|R)
P(R|R)
Read
Write
P(W|W)
P(R|W)
Figure 1. Two state DTMC read/write workload model.
3. Simulation Framework
Given our assumptions that UDE manifestation is likely
highly correlated to the workload and system in which they
occur, we take a ﬂexible approach in building our simulator.
Our solution techniques do not make any assumptions about
the underlying workload or model, allowing the user to con-
struct appropriate workloads or models based on their own
needs or assumptions. We provide a common interface for
workload models, allowing the use of traces, or probablistic
models (as was used in our evaluation), and a framework for
constructing a storage system model by composing common
component level objects, which can be subclassed as needed
to increase the library of components from which the system
can be built.
Once a workload model and system model have been cre-
ated, we utilize a multi-modal simulator which executes the
model at three different resolutions, based on a dependency
relationship with UDEs which occur in the I/O stream.
3.1 Workload Modeling
The manifestation of UDEs as actual data corruption events
is dependent on read operations following a write which suf-
fers from a UDE (or in the case of UREs, just simply a read
operation). This fact makes modeling the workload for the sys-
tem an important consideration. The mix of reads and writes
impacts the manifestation of UDEs, as does the correlation of
reads and writes in time. While this fact drives a desire to use
realistic representative work loads in any simulation, perhaps
even traces, the rare nature of UDEs makes makes this a dif-
ﬁcult proposition. Utilizing a trace based simulation has two
problems. The ﬁrst is a lack of sufﬁciently detailed traces, and
the second is a lack of traces of sufﬁcient length. Due to the
long periods of time involved in simulating UDE mitigation,
one requires hundreds, if not thousands, of days of block level
traces. Additionally, trace based analysis is limited to a hand-
ful of possible trajectories in the ﬁle system, only those cap-
tured in the available traces. Given these limitations we form
a simple probabilistic model of the workload which we use to
generate synthetic workloads that statistically match available
traces.
Generating a representative synthetic workload has proven
difﬁcult in the past, and is still largely considered an open
problem. For the purposes of this analysis we present a sim-
pliﬁed workload model which captures the primary metrics of
interest for our needs, namely the ordering of reads and writes,
which can be directly parameterized using real traces to which
we have direct access. Despite our selection of this method of
workload modeling, our simulator has been designed in a ﬂex-
ible manner which allows it to utilize any workload model a
user would like to design. The main simulator couples with the
workload model simply by requesting events from the work-
load model, including the type of I/O, and size and location of
the write, and the time until the next I/O. It then injects UDEs
as appropriate.
In order to characterize the workload used in our study, we
create a statistical model of ﬁve primary features from a given
trace.
• Per Disk Rate of I/O - Given that our UDE rates are in
units of UDEs
I/O , it is important to know the rate at which
I/O is conducted in our workload. This rate is captured
in the workload parameter io/s. Our workload model
assumes exponentially distributed interarrival times, with
rate io/s.
• Diversity of the I/O Stream - Just knowing the rate of in-
coming I/O requests is not, however, enough to character-
ize the system. We also need some measure of the diver-
sity of the I/O stream. While io/s measures the number of
I/O requests per second, it tells us nothing about the aver-
age number of unqiue chunks on which I/O operations are
performed, per second. We introduce the parameter uc/s
to model this, which is a measure of the unique chunks
read or written each second.
• I/O Size - Based on our analysis of some available traces
we model the size of an I/O operation within the simula-
tor as a Exponential random variable with a mean request
size of sizei/o blocks.
• Read/Write Composition of the I/O Stream - When the
simulator is not suffering from a UDE, our simulator dis-
cards I/O events rather than simulate them, as they cannot
have an effect on the rate of undetected data corruption
errors. When an UDE does occur, it is important to know
whether it occurred during a read operation (and is thus
transient) or during a write operation, (and could thus lead
to a series of data corruption events). Towards this end we
estimate the parameter P (R), which tells us the probabil-
ity with which a given I/O is a read request.
• Correlation of I/O Stream Composition - In our anal-
ysis of available traces, we found that an I/O request for
an individual chunk is highly temporally correlated with
the most recent I/O for that same chunk in the trace. In
order to model this behavior we utilize a discrete time
Markov chain (DTMC) for each active chunk to deter-
mine the next I/O operation that will take place after a
read or a write, as illustrated in Figure 1. This model has
four transition probabilities which we estimate from the
trace data. P (R|R) is the probability of a chunk being
Table 3. Parameters for the workload models.
4096 B
100
10
0.6
0.6
0.6
0.4
0.4
Parameters Abstract Write Heavy Read Heavy
io/s
uc/s
sizeio
P (R)
P (R|R)
P (R|W )
P (W |R)
P (W |W )
122.04
16.2623
3465.98 B
0.23161
0.4488
0.8339
0.5512
0.1661
2448.94 B
0.82345
0.829483
0.204677
0.170517
0.795323
90.24
7.2226
read if the most recent I/O for the chunk was also a read.
P (R|W ) is the probability of a chunk being read when
the most recent I/O request was a write. P (W |R) is the
probability of a chunk being written when the most recent
I/O request was a read, and ﬁnally P (W |W ) is the prob-
ability of a chunk being written when the most recent I/O
request was a write.
Table 3 indicates the values of these parameters for three
different workloads. The ﬁrst workload, which we call the
Abstract Workload, is derived from parameters assumed in the
calculations presented in Section 2.4 derived from [2] and [15].
Both the “Read Heavy” and “Write Heavy” workloads are es-
timated empirically from actual disk traces. Separate DTMCs
are kept in the workload model for each chunk on the disk that
is being read or written.
3.2 Storage System Modeling
In order to understand the manifestation of UDEs into user-
level undetected data corruption errors, we present a block
level model of storage systems which can be used to under-
stand the actual effects of these faults in real systems. This
block level model is designed to be easy to extend and com-
pose with other similar models to allow for the construction of
more complex disk models. Each individual block level model
keeps track of the state of a single disk. Models can be com-
posed to allow the simulation of various RAID conﬁgurations
and techniques for detecting UDEs. In our models, for exam-
ple, a RAID6 system is simply a subclass of a disk, which
contains a number of composed disk models. RAID6 with
mitigation is simply a subclass of our RAID6 model which
implements the sequence number data parity appendix method
described in Section 2.3.
3.2.1 Block Level Model.
In order to determine the state
of the individual blocks on a given disk, each disk block is
modeled as a non-deterministic ﬁnite automaton (NFA). The
set of states Q of the NFA represents the states of the block,
and is comprised of the union between a set of stable states
Qstable and unstable states Qunstable deﬁned by
Q = Qstable ∪ Qunstable
Qstable = {good, stale, corrupt}
Qunstable = {stale-good, corrupt-good,
corrupt-stale}
Unstable states differ from stable states in that the behavior of
a read operation is ill-deﬁned and may return either good data,
or corrupt/stale data. They are used to model the effects of near
off-track writes. In the case of an unstable state where one of
the tracks is good, a read to that track will not necessarily
manifest as an error. We assume it will read a given track track
with:
P (read track good) = P (read track {stale,corrupt}) = 0.5
The unstable state corrupt-corrupt is omitted from
this set, as it seems equivalent to a corrupt state, as is
stale-stale for similar reasons.
The sector model transitions from state to state on a set
of input symbols Σ, which correspond to the write events to
the sector, both faulty and non-faulty. The set of faulty write
events corresponds to the set of UDE events discussed in Sec-
tion 2, which is shown below.
Σ = {GW, DW, N-OTW, F-OTWH , F-OTWO}
Symbol GW represents a good write event. DW represents a
dropped write event. N-OTW represents a near offtrack write
event. F-OTWH indicates a far offtrack write event which was
intended to write to this block but instead wrote to another.
F-OTWO represents a far offtrack write event which was in-
tended for another block but was instead incorrectly written to
the block in question.
Figure 2 shows the NFA states with transitions for a sin-
gle block, indicating how the block changes states due to UDE
events. Of particular importance is the fact that from all states
a good write event brings us to the good state, masking the
UDE. This fact is important when considering how a UDE
fault manifests as an actual error. In the case that a UDE oc-
curred, but the same blocks were again written with no error
before a read for those blocks was issued, the fault will not
manifest as an error.
3.2.2 Disk Level Model.
In order to represent an entire
disk, the block level model indicated in Section 3.2.1 is imple-
mented for every block in a disk, forming our base disk model.
Each block is indexed, and thus can have individual write op-
erations directed to it, causing state changes in the underlying
automata. For efﬁcient storage of the automata, we take ad-
vantage of the fact that, barring a UDE which is by deﬁnition a
rare event, the entire disk consists of blocks in the good state.
By representing the state of all blocks in a disk using a sparse
matrix with good as the default value, we achieve a space
efﬁcent implementation of even very large disks. This sparse
matrix is represented with a hashtable, using a hash function of
LBA mod n, for some n. While a simple function, it exploits
the fact that UDEs will most often manifest on a disk as a string
of consecutive blocks in a state other than good. The modulo
function then works to try to ensure that consecutive blocks
will appear in different elements of the hashtable, which keeps
access times for the hash table at a minimum. This improves
the time it takes to update the state of our various block level
NFAs, and thus overall simulation time.
Individual disk models are composed with a controller to
form RAID units. The controller model presents to the sim-
ulator an interface to write to the underlying disks as if they
were a single larger disk. It performs the calculations of which
stripe to write to on a block by block basis to enforce the RAID
model. The RAID interface also serves to inform the simula-
tor of the number of actual disk reads and writes executed per
requested operation so that UDEs occurring due to a single
logical write to a RAID system will be generated appropri-
ately for the larger numbers of actual disk reads and writes
performed due to RAID operations such as read-modify-write.
Each RAID unit undergoes weekly scrubbing, and we make
the assumption that during a scrub all UDEs will be detected
and marked unreadable so as not to result in a data corruption
event in the future.
Parity scrub mismatches are handled by marking the af-
fected blocks as unreadable, meaning that any reads from the
workload stream to these blocks will not be executed. On a
successful write operation (UDE or otherwise) to a block, this
status is changed via the NFA in Figure 2 treating the unread-
able sector as being in the good state just before the write.
3.3 Simulator architecture
Given the rare nature of UDEs, attempting to calculate the
mean rates of undetected data corruption due to their injec-
tion into a storage system using just discrete event simulation
would prove prohibitively costly. With the rates given in Ta-
ble 2, it is not unreasonable to expect 1013 or more I/O events
in between UDEs. Even given a discrete event simulator ca-
pable of processing 107 or more events per second evaluating
a single UDE event would take almost a month on modern
workstations. This makes simulating a large number of UDEs
impractical, and seems to preclude their analysis via simula-
tion.
We attempt to solve some of the efﬁciency issues inherent
in simulating a stiff system by designing our simulator to have