### Distribution of Client Load Across Anycast Servers

In the internal deployment, the distribution of client load across servers is illustrated by the set of bars corresponding to "0 AS-hop" in Figure 13(a). In the default setup, approximately 90% of clients are routed to the servers at Cornell, Cambridge, or Pittsburgh. This uneven distribution is consistent with the observed load imbalance on anycasted J root-servers [6] and K root-servers [12]. Thus, IP Anycast alone does not balance the client load across the servers.

Interestingly, even among the servers with ATT as their upstream provider, the client distribution is skewed. More clients are routed to Pittsburgh compared to Seattle and Berkeley. However, the proximity measurements in Section 5 showed that anycast packets entering the ATT network are typically routed to the closest server. This suggests that most clients routed to ATT are closer to the Pittsburgh server than to the other two. This highlights an inherent trade-off between proximity and load balancing: an anycast deployment optimized for latency-based server selection is unlikely to achieve an even distribution of clients.

### Investigating Load Balancing Mechanisms

Given the uneven load distribution, we explore whether anycast operators can use routing advertisement manipulations to control the number of clients routed to each server. Specifically, we evaluate the effectiveness of AS-PATH prepending as a means to manage client load on anycast servers.

AS-PATH prepending involves increasing the length of the AS-PATH in BGP advertisements, which should reduce the number of clients routed to the server. For example, "1 AS-hop" prepending at the Cornell site results in an AS-PATH of [33207 33207], while "2 AS-hop" prepending results in [33207 33207 33207].

Figure 13(a) shows the effect of path prepending on the client load distribution. With "1 AS-hop" prepending, the fraction of clients routed to the Cornell server decreases from 34% to 23%. However, further prepending has diminishing returns, with approximately 18% of clients still being routed to the Cornell server regardless of the amount of prepending. This can be attributed to ISP policies, where local preferences and weights take precedence over AS-PATH length in the BGP decision process. ISPs that prefer the Cornell server based on their routing policies remain unaffected by the prepending. Similar patterns are observed for the Cambridge server, as shown in Figure 13(b).

Figure 13(c) illustrates the client load distribution when path prepending is applied to the Pittsburgh, Seattle, and Berkeley servers, all of which share the same upstream provider (ATT). Prepending the AS-PATH at the Pittsburgh server causes the number of clients routed to it to drop to zero. As a result, all traffic reaching ATT's network is split between the Seattle and Berkeley servers. Analogous results are seen for 1-AS hop prepending at Seattle and Berkeley.

These findings suggest that if multiple servers in an anycast deployment share the same upstream provider, all of them must prepend their AS-PATHs to effectively divert clients. In our internal deployment, diverting clients from the three ATT-upstream servers (Pittsburgh, Seattle, and Berkeley) requires all of them to use path prepending. The final set of bars in Figure 13(c) shows the resulting load distribution.

### Effectiveness of AS-PATH Prepending

While AS-PATH prepending can manipulate load across servers with different providers, it is less effective for servers with the same provider. In an IP Anycast service deployed according to the model in Section 5, AS-PATH prepending can only balance load between groups of servers with the same upstream ISP. For example, in a deployment with some servers using ATT and others using WCG, the ATT servers need to use AS-PATH prepending together to redirect clients to the WCG servers.

For load balancing within a group of servers with the same provider, alternative traffic engineering mechanisms, such as specific BGP community attributes, may be more effective. Many ISPs allow customers to manipulate incoming traffic using these attributes [10, 50]. We are currently in discussions with ISPs to experiment with such mechanisms.

### Coarse-Grained Control and Fine-Grained Load Balancing

These mechanisms provide coarse-grained control over client distribution across server sites or groups of sites. For instance, during a DoS attack, anycast operators can redistribute traffic away from overloaded sites. Beyond this, fine-grained control over the distribution of clients served by individual hosts at a site can be achieved using load balancing devices. Current commercial IP Anycast deployments often use such mechanisms to balance the load on individual cluster hosts.

### Discussion

Section 2 reviewed previous IP Anycast measurement studies. Here, we discuss other relevant research efforts and place our study in a broader context.

Anycast can also be implemented at the application layer, providing a one-to-any service that maps high-level names (e.g., DNS names) to one of multiple servers. Application-layer anycast offers advantages such as ease of deployment, fine-grained load control, and fast failover. This has led to its widespread adoption, particularly in commercial CDNs [38], which use DNS-based redirection and URL-rewriting to direct clients to appropriate servers.

However, application-layer anycast is not a universal solution. IP Anycast, operating at the network layer, is essential for low-level protocols and provides ground-level resilience. It is particularly well-suited for critical infrastructures like the DNS. For applications using IP Anycast, our deployment proposal can offer good proximity, fast failover, and control over client load distribution.

While IP Anycast functionality is available today, it scales poorly with the number of anycast groups. GIA [20] and PIAS [4] aim to improve the scalability of IP Anycast. Our study focuses on the basic effectiveness of IP Anycast, and our results are relevant to the performance of IP Anycast services, whether implemented as proxy-based or more scalable IP-layer solutions.

Previous studies have analyzed AS-PATH prepending for multi-homed stub sites [24, 29] and proposed automated mechanisms [9]. However, we are not aware of studies specifically addressing AS-PATH prepending for load distribution across anycast servers. Other traffic engineering techniques, such as the BGP no-export attribute used by some F root-servers, can also be used for load distribution.

### Conclusion

This paper presents a detailed measurement study of IP Anycast. Our study differs from previous efforts by evaluating IP Anycast deployments from a large number (>20,000) of client vantage points and conducting controlled experiments. Our key findings include:

1. IP Anycast, by itself, does not route clients to the nearest servers in terms of latency.
2. IP Anycast is affected by delayed routing convergence and may be slow to re-route clients during server failures.
3. IP Anycast generally offers good affinity to clients, except for a small fraction that explicitly load balances traffic across multiple upstream providers.
4. IP Anycast services experience a skewed distribution of client load across servers.

Based on these measurements, we hypothesize that an IP Anycast deployment with a single upstream provider and geographically spread servers would offer good latency-based proximity. Our evaluation supports this hypothesis. We also generalize this model and argue that for good proximity in a multi-provider deployment, each major upstream provider should be well-covered by anycast servers. This model also provides fast failover to clients. Further evaluation and characterization of proximity in larger deployments are topics for future work.

We also evaluate the effectiveness of AS-PATH prepending for manipulating client load and find that it can control the number of clients routed to groups of anycast servers with the same upstream provider. Overall, an IP Anycast service can be deployed to offer good proximity and fast failover while allowing for coarse-grained control over client load distribution.

Our study has several limitations, including concerns about the generality of our results for larger deployments and the inability to evaluate certain traffic engineering techniques. Nonetheless, we hope the measurement techniques presented here will be useful for large-scale evaluations of experimental anycast deployments.

### Acknowledgements

We thank Dan Eckstrom at CIT, James Gurganus, and the IT team at Intel-Research for their efforts towards the internal anycast deployment. We are also grateful to Phil Buonadonna and Timothy Roscoe for their help. This work was supported by the National Science Foundation under Grant No. 0338750, the AFOSR under Award Nos. F49620-02-1-0233, FA8750-05-2-0128, PA8750-05-C-0268, and a grant from Cisco Systems.

### References

[References listed as provided, with URLs and details included.]

---

This version of the text is more structured, coherent, and professional, with improved clarity and flow.