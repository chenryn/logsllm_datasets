authentication and login-time authentication.
Eye movements as an input channel. In the past, eye move-
ments have been used as a mechanism to input conventional cre-
dentials (such as PINs [26, 27], passwords [26] and patterns). The
main benefit lies in increased resistance to shouldersurfing (per-
formed either by a human or through CCTV).
Bulling et al. propose an image-based gaze authentication sys-
tem [2]. During enrolment, the user is shown a specific image and
chooses a gaze path within the image as their secret. During authen-
tication, the user is then shown the same image and has to replicate
their enrolment-time gaze trace. In order to increase the entropy of
these traces, the authors use saliency masks. The mask covers parts
of the image that are most likely to attract the user’s attention (such
as faces) to prompt them to choose more random gaze paths. In the
second part of the study, the authors showed participants close-up
videos of another person’s gaze, while entering an image-based
password, and asked them to guess the “password”. Users were
successful in guessing a PIN-based password in 19 out of 81 cases,
which dropped to 1/82 and 8/72 for image based passwords with
and without saliency masks, respectively.
Login time biometric authentication. While the techniques in
the previous section are used at login time, they merely use gaze as
an input channel without making use of the biometric component
of eye movements. Their benefit lies in their resistance to replay
attacks (e.g., shouldersurfing) but they still require memorizing a
secret (a PIN, password or image gaze sequence). If this secret is
revealed, these techniques do not provide any further protection.
There are several proposed technique to achieve login time bio-
metric recognition based on eye movement patterns, a summary
is given in Table 1. Login time authentication systems have the
advantage of being able to use controlled stimuli (rather than hav-
ing to work with the user’s normal system interactions). Therefore,
they can measure the user’s visual response to a controlled and
fixed stimulus, without the user’s eye movement patterns being
influenced by changing stimuli. In addition, as the system knows
the screen content at any moment it can use “high-level” features
that make use of the user’s gaze positions, rather than the more
“low-level” saccadic or fixational movements. These high-level fea-
tures include scan paths (i.e., the shape and position of the user’s
time-varying gaze points) or distribution of areas of interest and
density maps (i.e., which part of an image the user focuses on the
most). Techniques based on these high-level features exhibit rela-
tively high error rates with the EER ranging from 6.3% to 30%. In
addition, it is not yet known how time-stable these patterns are
as users become more familiar with the (static) stimuli used for
authentication.
The best error rates (an EER of 6.3%) have been achieved by
Sluganovic et al., who propose a login time system using low-level
eye movement features. They test their approach using a desktop-
based eyetracking system with an SMI RED500 eye tracker and 30
participants [35]. During login and enrolment, users are asked to
look at a red dot on the screen. Once the user’s gaze focuses on
the dot, it moves to a new, random position on the screen. The
authentication process is then two-fold: To prevent replay attacks,
the system confirms whether the newly recorded gaze positions
match the (randomized) positions of the dots. If the data were
simply replayed from a prior login, the positions would be unlikely
to match (the reported success rate for the replay attack is 0.06%).
The actual biometric verification is based on raw eye movement
data without using the state of the stimulus.
All the systems [3, 17, 31, 32, 35] heavily rely on accurate cali-
brations. In fact, these studies make use of the relationship between
the user’s gaze and the visual stimulus position (e.g., red dot on a
black screen). Imprecise calibrations, which could occur with slight
posture changes or head movements, will affect the system perfor-
mance, leading to recognition errors [17, 31] or to compromised
replay detection [35].
Continuous authentication. The idea of continuous authentica-
tion is to establish the user’s identity not just once at login time
but also continuously while the person is using the system. As
such, it is able to detect a change in user identity even after the
initial login. Continuous authentication is only possible when the
Session 6A: Biometrics SecurityCCS ’19, November 11–15, 2019, London, United Kingdom1189authentication system does not rely on creating specific stimuli
(as the process of authentication would otherwise interfere with
the user’s work). In this case, the system needs to account for the
fact that a person’s visual response may change as the stimulus
changes, therefore it is necessary to choose biometric features that
are as independent of the stimulus as possible.
There are a number of papers that propose eye movement-based
continuous authentication systems [11, 12, 20, 21] (see Table 1), but
most of them focus on scenarios where the user is working on a
single specific task. In this case, the system can use the specific
characteristics of the users’ responses within such a task to de-
velop distinctive task-dependent features (e.g., scan paths in [20]).
However, as users carry out several different tasks while using a
computer, a continuous system should be able to authenticate users
across tasks, either by training the classifier for each task or by
making cross-task predictions. Eberz et al. [11, 12] investigated
authentication with several real-world tasks (i.e.,reading, typing,
web browsing and watching different videos). Similarly to [35], the
biometric features are low-level, i.e., do not relate to the state of a
stimulus. The authors use three feature types: (a) spatial features
reflect the size and shape of fixations, (b) temporal features mea-
sure the speed of (micro-)saccades and (c) pupil features measure
changes in the size of the pupil. Out of this feature set, the pupil
diameter contributed the biggest amount of information, followed
by temporal features and spatial features.
In [11, 12], same-task recognition rates vary depending on the
task: 0.04% (browsing) to 4.9% (typing). However, recognition per-
formance drops significantly when authenticating on a task the
system was not trained on (e.g., enrolment data is for browsing,
test data is for typing). In particular for typing, the study shows
that typing is quite problematic, leading to EERs close to 50% (i.e.,
random guessing) when using eye movements data collected during
typing to authenticate the user during other tasks. For some tasks
combination, an improvement is achieved by correcting the pupil
diameter for the brightness of the screen content, see Section 2.1.
While (non-controlled) changes in screen brightness are accounted
for, the authors do not consider changes in ambient light. In ad-
dition, they only use a small user sample (10 users) for the task
dependence and brightness adjustment experiment which questions
the robustness of their results.
3 EXPERIMENTAL DESIGN
Here we describe our experimental goals and the setup and outline
of our data collection procedure.
3.1 Design Goals
The main challenges raised by previous work are three-fold: (a)
requirement of a precise calibration, (b) effects of changing ambient
light and screen brightness and (c) task dependence of features. The
objective to eliminate these effects is reflected in our design goals:
• Calibration-free operation: The system should not rely on
an accurate calibration. As all commercial eyetrackers require
calibration data, this design goal can be satisfied by either using
random calibration data or by using a generic calibration not
tailored to the current user.
Figure 1: Experimental setup. Users are sat on a chair around
60cm away from the monitor. The eye tracker sits at the base
of the monitor. The lamp is positioned around 80cm above
the desk and to the right of the user. The reported angles
vary slightly depending on the participant’s height and pos-
ture.
• Task independence: The system should reduce the effect of
task dependent features on error rates during cross-task au-
thentication (i.e., when training and testing on different tasks).
• Light-invariant features: The system’s error rates should not
change significantly if the levels of ambient light or the bright-
ness of the screen content changes during training, testing or
between training and testing.
3.2 Experiment Setup
Figure 1 shows a representation of our experimental setup. We use
an SMI RED500 eye tracker capturing samples at 500Hz. Unlike pre-
vious work, we use binocular eye tracking, which reports separate
gaze positions and pupil diameters for both eyes. The user is facing
a 22" screen with a 1920x1080 resolution positioned about 60cm
away. The screen is set to the same brightness for all users (although
the screen content brightness varies throughout the experiment
as discussed below). In order to vary the ambient light, we use a
desk lamp with a Philips Hue light bulb placed to the right of the
screen. The bulb’s brightness can be programmatically controlled.
The lamp is angled towards the keyboard to avoid blinding the user
on higher brightness settings. The room itself is illuminated with a
ceiling light dimmed to a moderate level in order to achieve suffi-
cient variation in brightness by using the desk lamp. Table 2 shows
the complete experiment data collection for a single participant.
In the following we explain how we designed the experiment to
collect data which allows us to test our goals: (i) effect of calibration,
(ii) effect of task selection and (iii) effect of light sensitivity.
Calibration. The RED500 eye tracker requires to be calibrated
in order to collect accurate gaze. The calibration maps different
rotations of each eyeball to the respective gaze positions on the
screen. As such, calibration depends on a set of factors including
the size and resolution of the screen, position of the eyetracker, the
distance of the user to the screen, the distance between the eyes
and the user’s viewing angle.
In order to test the effect of calibration on the collected data, we
conduct two separate sessions for each study participant. In the first
session, we calibrate the eye tracker with a user-specific calibration.
To create such calibration, we use a 9-point calibration procedure
(T0 in Table 2), followed by a 4-point validation procedure that
~35˚60cmSide View˚Top Viewusereyetrackermonitorlampuserlampeyetrackermonitor80cm~50Session 6A: Biometrics SecurityCCS ’19, November 11–15, 2019, London, United Kingdom1190TaskId
T0
T1
T2
T3
T4
T5
T6
—
T7
T8
T9
T10
T11
T12
n
o
i
s
s
e
s
d
e
t
a
r
b
i
l
a
C
n
o
i
s
s
e
s
d
e
t
a
r
b
i
l
a
c
n
U
Performed task
User calibration
Calibration validation (pre)
Slideshow
Reading
Browsing
Slideshow
Calibration validation (post)
Load random calibration
Calibration validation (pre)
Slideshow
Reading
Browsing
Slideshow
Calibration validation (post)
User-specific calibration
—
✓
✓
✓
✓
✓
✓
—
✗
✗
✗
✗
✗
✗
Screen brightness Ambient light Duration (s)
—
—
increasing
constant
constant
random
—
—
—
increasing
constant
constant
random
—
—
—
constant
increasing
increasing
increasing
—
—
—
constant
increasing
increasing
increasing
—
60
10
180
300
300
300
10
—
10
180
300
300
300
10