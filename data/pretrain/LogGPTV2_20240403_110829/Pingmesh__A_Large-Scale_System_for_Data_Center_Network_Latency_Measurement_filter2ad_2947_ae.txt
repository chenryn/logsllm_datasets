Pingmesh pipeline.
Silent packet drop detection. As we have discussed
in Section 5, we have been using Pingmesh for silent
packet drop detection. Since the latency data is already
there, we only need to ﬁgure out the detection algorithm
and implement the algorithm in the DSA pipeline with-
out touching other Pingmesh components.
Network metrics for services. Two Pingmesh met-
rics have been used by service developers to design and
implement better services. The Pingmesh Agent ex-
poses two PA counters for every server: the 99th latency
and the packet drop rate. Service developers can use the
99th latency to get better understanding of data center
network latency at server level. The per-server packet
drop rate has been used by several services as one of the
metrics for server selection.
For the above extensions, only inter-DC Pingmesh
and QoS monitoring were by design, the rest three just
happened out of our expectation. Thanks to Pingmesh’s
loosely coupled design, all these features were added
smoothly without adjusting its architecture.
6.3 Visualization for pattern discovery
We have invested heavily in Pingmesh data analysis
and visualization. Our happy ﬁndings are that data
speaks for themselves and that visualization helps us
better understand and detect various latency patterns.
Figure 8 shows several typical visualized latency pat-
terns. In the ﬁgure, a small green, yellow, or red block
or pixel shows the network latency at the 99th percentile
between a source-destination pod-pair. Green means
the latency is less than 4ms, yellow means the latency
is between 4-5ms, and red is for latency larger than 5ms.
A white block means there is no latency data available.
Figure 8(a) shows an (almost) all-green pattern, which
means the network works ﬁne. Though looks straight-
forward, this all-green pattern is one of the most widely
used feature of Pingmesh. Using this pattern, we can
easily tell the global healthy status of the network.
Figure 8(b) shows a pattern of a white-cross. The
width of the white-cross corresponds to a Podset, which
contains around 20 pods. This pattern shows a Podset-
down scenario. Podset-down typically is due to the loss
of power of the whole Podset.
Figure 8(c) shows a pattern of a red-cross. The width
of the red-cross again corresponds to a Podset. The
red-cross shows high network latency from and to the
Podset. This pattern shows there is a network issue
within the Podset, since the network latency of other
Podsets are normal. There may be several causes of the
Podset red-cross. If both Leaf and ToR switches are all
L3 switches, then at least one of the Leaf switches is
dropping packets. If the whole Podset is a L2 domain,
then it is possibly caused by broadcast storm, e.g., due
to some switches loss their conﬁguration.
Figure 8(d) shows a pattern of red-color with green-
squares along the diagonal. Here each small green-
square is a Podset. It shows that the network latencies
within the Podsets are normal, but cross-Podset latency
are all out of network SLA. It shows a network issue at
the Spine switch layer.
The success of the visualization is beyond our expec-
tation. It has become a habit for many of us to open
the visualization portal regularly to see if the network
is ﬁne. The visualization portal has been used not only
by network developers and engineers, but also by our
customers to learn if there is a network issue or not.
We also observed an interesting usage pattern: When
the visualization system was ﬁrst put into use, it was
typically used by the network team to ‘prove’ to our cus-
tomers that the network was ﬁne. Now our customers
usually use the visualization to show that there is in-
deed an on-going network issue. This is a usage pattern
change that we are happy to see.
6.4 Pingmesh limitations
During the period of running Pingmesh, we have un-
covered two limitations of Pingmesh. First, though
Pingmesh is able to detect which tier a faulty network
150device is located in, it cannot tell the exact location. In
our network, there are tens to hundreds of switches at
the Spine layer. Knowing the Spine layer is experiencing
some issue is good but not enough. We need methods to
locate and isolate the faulty devices as fast as possible.
This is a known limitation of Pingmesh from beginning.
As described in Section 5.2, we combine Pingmesh and
TCP traceroute to address this issue.
The second limitation comes from Pingmesh’s current
latency measurement. Though the Pingmesh Agent can
send and receive probing messages of up to 64 KB, we
only use SYN/SYN-ACK and a single packet for sin-
gle RTT measurement. Single packet RTT is good at
detecting network reachability and packet-level latency
issues. But it does not cover the case when multiple
round trips are needed. We recently experienced a live-
site incident caused by TCP parameter tuning. A bug
introduced in our TCP parameter conﬁguration soft-
ware rewrote the TCP parameters to their default value.
As a result, for some of our services, the initial conges-
tion window (ICW) reduced from 16 to 4. For long
distance TCP sessions, the session ﬁnish time increased
by several hundreds of milliseconds if the sessions need
multiple round trips. Pingmesh did not catch this be-
cause it only measures single packet RTT.
7. RELATED WORK
Our experiences running one of the largest data cen-
ter networks in the world taught us that all the compo-
nents including applications, OS kernel, NIC, switching
ASIC and ﬁrmware, and ﬁbers may cause communica-
tion failures. See [4] for a summary of various failures
that may cause network partition.
[21] and [6] studied traﬃc and ﬂow characteristics of
diﬀerent types of data centers, by collecting network
traces. Pingmesh focuses on network latency and is
complementary to these works.
Both Pingmesh and [18] are designed to detect packet
drops in the network. Both use active probing packets
and are capable of covering the whole network. The
approaches, though, are diﬀerent.
[18] uses RSVP-TE
base source routing to pinpoint the routing path of a
It hence needs to create the routing
probing packet.
paths and maps in advance.
It also means that the
probing packets are traversing the network in LSPs (la-
bel switched paths) diﬀerent from those used by the
non-probing packets. Second, RSVP-TE is based on
MPLS, which, though is widely used for WAN traﬃc en-
gineering, is not used within the data centers. Pingmesh
can be used for both intra-DC and inter-DC networks.
Using source routing does provide an advantage:
[18]
can directly pinpoint the switches or links that drop
packets. We have shown in Section 5.2 Pingmesh can
localize faulty devices together with traceroute.
Cisco IPSLA [8] also uses active packets for network
performance monitoring.
IPSLA is conﬁgured to run
at Cisco switches, and is capable of sending ICMP, IP,
UDP, TCP, and HTTP packets.
IPSLA collects net-
work latency, jitter, packet loss, server response time,
and even voice quality scores. The results are stored lo-
cally at the switches and can be retrieved via SNMP or
CLI (command-line interface). Pingmesh diﬀers from
IPSLA in several ways. First, Pingmesh uses server
instead of switches for data collection. By doing so,
Pingmesh becomes network device independent whereas
IPSLA works only for Cisco devices. Second, Pingmesh
focuses on both measurement and latency data analysis.
To achieve its goal, Pingmesh has not only Pingmesh
Agent for data collection, but also a control plane for
centralized control and a data storage and analysis pipeline.
IPSLA does not have such a control plane and data stor-
age and analysis pipeline.
NetSight [19] tracks packet history by introducing
postcard ﬁlters at the switches to generate captured
packet events called postcard. Several network trou-
bleshooting services, nprof, netshark, netwatch, ndb,
can be built on top of NetSight. Compared with Net-
Sight, Pingmesh is server-based in that it does not need
to introduce additional rules into the switches. Further
Pingmesh is capable of detecting switch silent packet
drops. It is not clear how silent packet drop rules can
be written for NetSight, since it is not known in advance
which type of packets may be dropped.
ATPG [25] determines a minimal set of probing pack-
ets that cover all the network links and forwarding rules.
Pingmesh does not try to minimize the number of prob-
ings. As long as the overhead is aﬀordable, we prefer
to let Pingmesh run all the time. Further it is not clear
how ATPG can deal with packet black-holes where the
rules for black-holes cannot be determined in advance.
Pingmesh focused on physical network and it uses ac-
tive probings by installing the Pingmesh Agent in the
servers. For third party VMs and virtual networks, how-
ever, installing the Pingmesh Agent may not be feasi-
ble. In this case passive traﬃc collection as explored by
VND [24] may be used.
8. CONCLUSION
We have presented the design and implementation of
Pingmesh for data center network latency measurement
and analysis. Pingmesh is always-on and it provides
network latency data by all the servers and for all the
servers. Pingmesh has been running in Microsoft data
centers for more than four years. It helps us answer if a
service issue is caused by the network or not, deﬁne and
track network SLA at both macro and micro levels, and
it has become to be an indispensable service for network
troubleshooting.
Due to its loosely coupled design, Pingmesh turned
out to be easily extensible. Many new features have
been added while the architecture of Pingmesh is still
the same. By studying the Pingmesh latency data and
learning from the latency patterns via visualization and
data mining, we are able to continuously improve the
151quality of our network, e.g., by automatically ﬁxing
packet black-holes and detecting switch silent random
packet drops.
9. ACKNOWLEDGEMENT
We thank Lijiang Fang, Albert Greenberg, Wilson
Lee, Randy Kern, Kelvin Yiu, Dongmei Zhang, Yong-
guang Zhang, Feng Zhao, the members of the Wireless
and Networking Group of Microsoft Research Asia for
their support at various stages of this project. We thank
our shepherd Sujata Banerjee and the anonymous SIG-
COMM reviewers for their valuable and detailed feed-
back and comments.
10. REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A
Scalable, Commodity Data Center Network
Architecture. In Proc. SIGCOMM, 2008.
[2] Alexey Andreyev. Introducing data center fabric,
the next-generation Facebook data center
network. https:
//code.facebook.com/posts/360346274145943/,
Nov 2014.
[3] Hadoop. http://hadoop.apache.org/.
[4] Peter Bailis and Kyle Kingsbury. The Network is
Reliable: An Informal Survey of Real-World
Communications Failures. ACM Queue, 2014.
[5] Luiz Barroso, Jeﬀrey Dean, and Urs H¨olzle. Web
Search for a Planet: The Google Cluster
Architecture. IEEE Micro, March-April 2003.
[6] Theophilus Benson, Aditya Akella, and David A.
Maltz. Network Traﬃc Characteristics of Data
Centers in the Wild. In Internet Measurement
Conference, November 2010.
[7] et.al Brad Calder. Windows Azure Storage: A
Highly Available Cloud Storage Service with
Strong Consistency. In SOSP, 2011.
[8] Cisco. IP SLAs Conﬁguration Guide, Cisco IOS
Release 12.4T.
http://www.cisco.com/c/en/us/td/docs/ios-xml/
ios/ipsla/conﬁguration/12-4t/sla-12-4t-book.pdf.
[9] Citrix. What is Load Balancing? http:
//www.citrix.com/glossary/load-balancing.html.
[10] Jeﬀrey Dean and Luiz Andr´e Barroso. The Tail at
Scale. CACM, Februry 2013.
[11] Jeﬀrey Dean and Sanjay Ghemawat. MapReduce:
Simpliﬁed Data Processing on Large Clusters. In
OSDI, 2004.
[12] Albert Greenberg et al. VL2: A Scalable and
Flexible Data Center Network. In SIGCOMM,
August 2009.
[13] Chi-Yao Hong et al. Achieving High Utilization
with Software-Driven WAN. In SIGCOMM, 2013.
[14] Parveen Patel et al. Ananta: Cloud Scale Load
Balancing. In ACM SIGCOMMM. ACM, 2013.
[15] R. Chaiken et al. SCOPE: Easy and Eﬃcient
Parallel Processing of Massive Data Sets. In
VLDB’08, 2008.
[16] Sushant Jain et al. B4: Experience with a
Globally-Deployed Software Deﬁned WAN. In
SIGCOMM, 2013.
[17] Sanjay Ghemawat, Howard Gobioﬀ, and
Shun-Tak Leung. The Google File System. In
ACM SOSP. ACM, 2003.
[18] Nicolas Guilbaud and Ross Cartlidge. Google
Backbone Monitoring, Localizing Packet Loss in a
Large Complex Network, Feburary 2013.
Nanog57.
[19] Nikhil Handigol, Brandon Heller, Vimalkumar
Jeyakumar, David Mazi`eres, and Nick McKeown.
I Know What Your Packet Did Last Hop: Using
Packet Histories to Troubleshoot Networks. In
NSDI, 2014.
[20] Michael Isard. Autopilot: Automatic Data Center
Management. ACM SIGOPS Operating Systems
Review, 2007.
[21] Srikanth Kandula, Sudipta Sengupta, Albert
Greenberg, Parveen Patel, and Ronnie Chaiken.
The nature of data center traﬃc: Measurements
& analysis. In Proceedings of the 9th ACM
SIGCOMM Conference on Internet Measurement
Conference, IMC ’09, 2009.
[22] Rishi Kapoor, Alex C. Snoeren, Geoﬀrey M.
Voelker, and George Porter. Bullet Trains: A
Study of NIC Burst Behavior at Microsecond
Timescales. In ACM CoNEXT, 2013.
[23] Cade Metz. Return of the Borg: How Twitter
Rebuilt Google’s Secret Weapon.
http://www.wired.com/2013/03/
google-borg-twitter-mesos/all/, March 2013.
[24] Wenfei Wu, Guohui Wang, Aditya Akella, and
Anees Shaikh. Virtual Network Diagnosis as a
Service. In SoCC, 2013.
[25] Hongyi Zeng, Peyman Kazemian, George
Varghese, and Nick McKeown. Automatic Test
Packet Generation. In CoNEXT, 2012.
152