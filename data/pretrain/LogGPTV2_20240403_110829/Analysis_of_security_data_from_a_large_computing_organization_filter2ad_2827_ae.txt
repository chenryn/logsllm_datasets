### Attack Phases
**Figure 5: Incident Types versus Attack Phases**

### C. Incident Severity
Categorizing incidents based on severity is essential for determining whether security monitors are effectively detecting harmful incidents as opposed to low-impact compromises. Severity is a qualitative measure that assesses the impact on the system's integrity, availability, and confidentiality, including applications, services, and data. Analyzing incident severity helps determine if security monitors can detect high-profile, low-impact violations. Table V provides definitions of the various degrees of severity and the distribution of analyzed incidents across these categories.

#### Incident Phase and Severity
Among the incidents detected in the attack-relay/misuse (final) phase, only about 30% were of medium-to-high severity. One would expect a high correlation between high detection latency and high severity; however, this is not supported by our data. Many high-severity incidents are detected at an early stage. For example, 62% (23/37) of high-severity incidents were caught in the breach phase, where significant damage had already occurred, such as attackers gaining access using stolen credentials. Such attacks cannot be detected until the attacker uses the stolen credentials to gain system access.

#### Benefits of Early Detection
Early detection can still limit the extent of damage caused by an attack. For instance, in Table II, incident three (credential compromise), early detection could have prevented unsuspecting users from exposing their credentials on a host with a trojaned SSH server and a rootkit. Ideally, File Integrity Monitors should detect any change or modification when legitimate SSH software is replaced with a trojaned version.

### Table V: Incident Severity
| Severity | Description | Impact |
|----------|-------------|--------|
| Very High (Catastrophic) | A vast majority of users are affected due to the breach with successful root escalations. | Production and administrative systems. |
| High (Very Serious) | Credentials compromise and application compromise (e.g., OpenSSL exploits, X-server key stroke logging) that allow the attacker to obtain root-level privileges on the systems. | Users and small cluster systems (affects entire research group); application level compromise (VNC, XP_Cmdshell mssql exploit), web server (Phpmyadmin, Php Horde), malware hosting. |
| Medium (Limited) | Non-production systems (affects an individual); brute force SSH, infected systems, spam/phishing. | |
| Low (Little or No Effect) | | |

In summary, the top five alerts, which account for approximately 54% (67/124) of incidents, include:
- TopN (18 incidents, 5 types)
- IRC (15 incidents, 4 types)
- Watchlist (11 incidents, 6 types)
- Login and command anomaly (14 incidents, 4 types)
- HTTP and FTP analyzer (9 incidents, 4 types)

Login and command anomaly alerts have the highest success rates in catching high-severity incidents (30% or 11/37). However, these alerts are triggered after the fact, once the attacker is already in the system.

### IX. Missed Incidents
Due to monitor imperfections, there are usually false negatives and false positives associated with the detection system. In this section, we discuss false negatives, i.e., incidents missed by the security monitors. Our analysis reveals 34 (27%) undetected incidents. All missed incidents are discovered through notifications from external sources (third parties, mailing lists, peers, users, or administrators). Upon notification, relevant logs are parsed to confirm the incident, and appropriate response actions are taken.

Table VI summarizes the specific causes of missed incidents. Analysis of the data on missed incidents reveals inherent limitations in the current security monitoring setup:
1. **Inability to automatically produce a context of what is normal and abnormal in observed events.**
2. **Limited ability for automated collection and analysis of attack-relevant information.**
3. **Inability to cope with a large spectrum of attacks, malware, and network traffic.**

The following discussion illustrates these limitations using examples of missed incidents.

### Table VI: Causes of Missed Incidents
| Cause of Missed Incidents | Examples | Number |
|---------------------------|----------|--------|
| Increased sophistication in attacks | Peer site compromise, zero-day exploits | 6 |
| Lack of signatures | Exploit of VNC null string authentication vulnerability | 7 |
| Admin misconfiguration | Web share world writable access or root login to accept any password | 5 |
| Inability to distinguish traffic anomalies in the network | Web defacement, bot command and control traffic | 10 |
| Misconfiguration of security monitoring tools | Routers stop exporting flows to central collector | 1 |
| Inability to distinguish true positives from false positives | Human error | 2 |
| Inability to run monitors on all hosts and file systems due to large administrative and performance costs | Limited deployment of file integrity monitors on non-critical systems | 3 |

### Detailed Analysis of Limitations
1. **Inability to Produce Information on What is Normal and Abnormal:**
   - About 26.5% of the missed incidents are credential compromises (a high-impact category).
   - Monitors rely on alerts based on:
     - Deviation in user behavior compared to known profiles (using syslog).
     - Malicious code download (using IDS).
     - Unexpected system file manipulation (using file integrity monitors).
   - Detecting multi-step attacks requires comprehensive runtime traffic analysis, including event correlation and accurate determination of normal and abnormal traffic.

2. **Limited Ability for Automated Collection and Analysis:**
   - In a credential-stealing attack, the attacker often downloads malware/exploits.
   - Assuming IDS has the signature, it should generate an alert, but without context, it is difficult to determine the user account used.
   - The attacker often deletes the malware, making it harder to trace.
   - Correlation of file integrity monitor data, IDS, and syslogs should suffice to construct an accurate event timeline, but current tools lack this capability.

3. **Inability to Cope with a Large Spectrum of Attacks, Malware, and Network:**
   - About 9% of the missed incidents are application compromises.
   - Limited detection coverage is due to the lack of timely available signatures and the emergence of new zero-day exploits.
   - For example, detection signatures for VNC null string authentication bypass (CVE-2006-2369) and OpenSSL SSL-Get-Shared-Ciphers Buffer overflow (CVE-2006-3738) were unavailable at the time of the attack.

### X. Conclusions and Future Work
This paper studies security compromises over a period of five years at the University of Illinois NCSA network. Key observations include:
- **Detection Rates:** IDS and NetFlows monitors detected 31% and 26% of incidents, respectively. 27% of incidents went undetected.
- **Alerts and Attacks:** Alerts are not uniform in their ability to detect attacks. The same alert can be triggered by different attacks due to shared common attack paths.
- **Anomaly-Based vs. Signature-Based Detectors:** Anomaly-based detectors are seven times more likely to capture an incident than signature-based detectors, though the latter have fewer false positives.
- **Detection Timing:** Nearly 39% of incidents are detected in the last stage of the attack, which is often too late to prevent damage.
- **Correlation Between Attack Phase and Severity:** There is no strong correlation between the attack phase and the incident's severity.
- **Efficiency of Alerts:** Alerts detecting the most incidents may not be the most efficient. For example, TopN detects 15% (18/124) of low to medium severity incidents but has a 33% false positive rate.
- **User Reporting:** While 14 incidents were detected based on external reports, one was reported by a user (which turned out to be a false positive), indicating a need for better user training.

Future work should reexamine other warning mechanisms in the system to understand why they are inactive and adapt detection capabilities to changes in the underlying infrastructure and the growing sophistication of attackers.

### XI. Acknowledgment
This work was supported in part by NSF grants CNS-05-51665 (Trusted Illiac) and CNS 10-18503 CISE, the Department of Energy under Award Number DE-OE0000097, IBM Corporation as part of OCR (Open Collaboration Research), and Boeing Corporation as part of ITI Boeing Trusted Software Center.

### References
[1] Singer, A., “Life Without Firewalls,” The Usenix Magazine, 28(6), 2003.
[2] Allman M., Kreibich C., Paxson V., Sommer, R., Weaver N.: “Principles for Developing Comprehensive Network Visibility,” USENIX Workshop on Hot Topics in Security, USENIX, 2008.
[3] Bellovin, S. R., Cheswick, B.: Firewalls and Internet Security: Repelling the Wily Hacker. Addison-Wesley Publishing, 1994.
[4] Chen S., Kalbarczyk Z., Xu J., Iyer R. K., “A data-driven finite state machine model for analyzing security vulnerabilities,” Int’l Conference on Dependable Systems and Networks, 2003.
[5] Cohen, F. B.: Protection and Security on the Superhighway. John Wiley & Sons, New York (1995).
[6] Cukier, M., Berthier, R, Panjwani, S., Tan, S.: A statistical analysis of attack data to separate attacks. Proc. Int’l Conference on Dependable Systems and Networks, (2006).
[7] Cutts Jr. et al, United States Patent 5,193,175, March 9, 1993.
[8] DOE M-205: Cyber Security Incident Management Manual. Department of Energy (2010).
[9] Gregorio-de Souza I., Berk, V. H., Giani A., et al., “Detection of Complex Cyber Attacks,” SPIE 6201, 2006.
[10] Zhou J., Heckman M., Reynolds B., Carlson A., and Bishop M., “Modeling Network Intrusion Detection Alerts for Correlation,” ACM Trans. on Info. and Sys. Security 10(1), 2007.
[11] Kendall K., Smith A. C., “A Database of Computer Attacks for the Evaluation of Intrusion Detection Systems,” MIT, Electrical and Computer Engineering. Cambridge 1999.
[12] Kumar, S, Spafford, E., An application of pattern matching in intrusion detection. Purdue University, Tech. Rep, Department of Computer Sciences (1994).
[13] Kumar, S. “Classification of intrusions,” Purdue University, 1995.
[14] Landwehr, C. et al., “A Taxonomy of Computer Security Flaws,” ACM Computing Surveys, 26(3), 1994.
[15] Ning P. and Xu D., “Learning Attack Strategies from Intrusion Alerts,” 10th ACM Conference on Computer and Communications Security, 2003.
[16] Paxson V., “Bro: A System for Detecting Network Intruders in Real-Time,” Computer Networks, 1999.
[17] Ruiu D., “Cautionary Tales: Stealth Coordinated Attack HOWTO,” http://althing.cs.dartmouth.edu/secref/local/stealth-co-ordinated-attack.txt (1999).
[18] Vaarandi R., “SEC - A Lightweight Event Correlation Tool,” Workshop on IP Operations and Management, 2002.
[19] Sung M., Haas M, Xu J. “Analysis of DoS attack traffic data,” FIRST Conference, Hawaii, 2002.
[20] Sharma A., Kalbarcyzk Z., Barlow J., Iyer R., “Analysis of Credential Stealing Attacks in an Open Networked Environment,” 4th International Conference on Network and System Security, 2010.
[21] Tidwell T., Larson R., Fitch K., and Hale J., “Modeling Internet Attacks,” Workshop on Information Assurance and Security, 2001.
[22] Templeton S.J., Levitt K., “A Requires/provides Model for Computer Attacks,” New Security Paradigm Workshop (2000).
[23] Treinen J., Thurimella R. “A framework for the Application of Association Rule Mining in Large Intrusion Detection Infrastructures,” 9th Int’l Symposium on Recent Advances in Intrusion Detection, 4219 (2006).
[24] Verizon Business Risk Team: 2010 Data Breach Investigations Report, http://www.verizonbusiness.com/resources/reports/rp_2010-data-breach-report_en_xg.pdf.
[25] Howard J. D., “An analysis of security incidents on the Internet 1989-1995,” Carnegie Mellon University, Pittsburgh, PA, 1998.
[26] Eckmann S.T., G. Vigna, and R.A. Kemmerer, “STATL: An Attack Language for State-based Intrusion Detection,” Workshop on Intrusion Detection Systems, Athens, Greece, 2000.