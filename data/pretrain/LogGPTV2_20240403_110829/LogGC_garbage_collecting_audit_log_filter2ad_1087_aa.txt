title:LogGC: garbage collecting audit log
author:Kyu Hyung Lee and
Xiangyu Zhang and
Dongyan Xu
LogGC: Garbage Collecting Audit Log
Kyu Hyung Lee, Xiangyu Zhang, and Dongyan Xu
Department of Computer Science and CERIAS, Purdue University
West Lafayette, IN, 47907, USA
PI:EMAIL, PI:EMAIL, PI:EMAIL
ABSTRACT
System-level audit logs capture the interactions between applica-
tions and the runtime environment. They are highly valuable for
forensic analysis that aims to identify the root cause of an attack,
which may occur long ago, or to determine the ramiﬁcations of an
attack for recovery from it. A key challenge of audit log-based
forensics in practice is the sheer size of the log ﬁles generated,
which could grow at a rate of Gigabytes per day. In this paper, we
propose LogGC, an audit logging system with garbage collection
(GC) capability. We identify and overcome the unique challenges
of garbage collection in the context of computer forensic analysis,
which makes LogGC different from traditional memory GC tech-
niques. We also develop techniques that instrument user applica-
tions at a small number of selected places to emit additional system
events so that we can substantially reduce the false dependences
between system events to improve GC effectiveness. Our results
show that LogGC can reduce audit log size by 14 times for regular
user systems and 37 times for server systems, without affecting the
accuracy of forensic analysis.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information Systems]:
Security and Protection—Unauthorized access (e.g., hacking, phreak-
ing) ; Invasive software(e.g., viruses, worms, Trojan horses); D.4.2
[Operating System]: Storage Management—Garbage Collection
Keywords
Attack Provenance; Audit Log; Garbage Collection; Reverse Engi-
neering
1.
INTRODUCTION
System-level audit logs record the interactions between applica-
tions and the underlying operating system (OS), such as ﬁle opens,
reads and writes; socket reads and write; and process creations and
terminations. Each of these is recorded as an event in the audit log,
consisting of process/user/group id involved in the event, as well
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright 2013 ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516731 .
as the type and parameters of the event. In attack forensics, au-
dit logs are critical to the construction of causal graphs. A causal
graph shows the causality relations between system-level objects
(e.g., ﬁles and sockets) and subjects (e.g., processes), including
those that existed in the past. For instance, a process p1 is causally
dependent on another process p2 if p2 spawns p1; a process p is
causally dependent on a ﬁle f if p reads f ; and a ﬁle f is dependent
on p if f is created or modiﬁed by p. Causal graphs can be used
to track the root cause of an attack. Upon observing a suspicious
symptom (e.g., a zombie process) in the system, the administrator
can use the casual graph to determine what had initially led to the
presence of this process, which could be that a careless user viewed
a phishing email, clicked an embedded URL, visited a malicious
web site, triggered a driver-by download which compromised the
system. Note that the initial attack may have happened days or even
weeks before the symptom is observed. Hence the audit log and the
derived causal graph are needed to disclose the attack path. Further-
more, the causal graph will disclose the damages or contaminations
caused by the attack.
Recent work has focused on generating accurate, complete causal
graphs from audit logs. Traditional causal graphs [13, 16] may suf-
fer from imprecision caused by dependence explosion, where an
event is unnecessarily dependent on too many other events and the
corresponding causal graph is excessively large for human inspec-
tion. BEEP [20] involves program instrumentation to reduce the
granularity of subjects from processes to “units” for fewer false-
positive dependences. Other efforts propose using timestamps [12]
or ﬁle offsets [23] to capture dependences more accurately.
9:’17#-./#0123##
;33=#
?3/:@(=#A@13B7#
19.1 GByte 
(3.18GB/Day)
7.19 GByte 
(1.2GB/Day)
#
8
3
7
)
6
5
4
#
3
2
1
0
/
.
-
#
&$"!#
&!"!#
%$"!#
%!"!#
$"!#
!"!#
’()%#
’()&#
’()*#
’()+#
’()$#
’(),#
Figure 1: Audit log growth.
However, a major hindrance of audit log-based attack forensics
in practice is the sheer size of audit logs. According to [16], even
compressed audit logs grow at the rate of 1.2GB/day. Our own ear-
1005lier work [20] shows an audit log growth rate of 800MB/day. Fig. 1
shows how audit log size grows over time in moderately loaded
server and client machines, respectively in our experiments. We
can see that the audit logs grow at an average rate of 3.18GB/day
(server) and 1.2GB/day (client), incurring excessive space and pro-
cessing overhead. Unfortunately, the reduction of audit log volume
has not received sufﬁcient research attention.
To fundamentally reduce audit log volume, we make a key obser-
vation: Many event entries in an audit log can be removed without
affecting future forensic analysis. Such entries are about operations
on system objects (e.g., ﬁles, sockets) that neither inﬂuence nor are
inﬂuenced by other processes or system objects. We call such ob-
jects unreachable objects. We monitored a lab machine for a period
of six days and observed that each day more than 94% of the objects
accessed were destroyed (or terminated) and over 80% of those de-
stroyed objects had very short lifetime, usually exclusively within
a single process. These objects are likely to be unreachable, which
suggests room for log reduction.
The log reduction problem we address shares some conceptual
similarity to garbage collection (GC) in heap memory management.
We hence call our solution LogGC . However, the two problems are
technically different as memory GC is an instantaneous problem,
namely, it focuses on removing unreachable heap memory objects
in a snapshot of the memory. In contrast, we are trying to remove
redundancy in audit logs that record history over a long period of
time. We also face challenges that are caused by the unique re-
quirements of forensic analysis and the coarse object granularity in
audit logging.
The main contributions of LogGC are the following.
• We develop a basic GC algorithm that works directly on au-
dit logs, each of which is a ﬂat sequence of events. The al-
gorithm can be invoked at any moment during system exe-
cution, taking the current audit log and generating a new and
reduced audit log (Section 2).
• An important requirement in forensic analysis is to under-
stand attack ramiﬁcations, which entails supporting forward
causal analysis. The basic GC algorithm, which is an adapta-
tion of a classic reachability-based memory GC algorithm, is
incapable of supporting forward analysis. We hence propose
a new extension to the algorithm (Section 3).
• To improve GC effectiveness, we leverage our previous tech-
nique BEEP [20] to partition a process to multiple execution
units. We also propose a new technique to partition a data ﬁle
into logical data units, by instrumenting user applications at
a small set of code locations to emit additional system events.
As such, better precision can be achieved and many more un-
reachable events can be exposed and garbage-collected (Sec-
tion 4).
• We propose to leverage applications’ own log ﬁles to further
remove event entries in audit logs (Section 5).
• We conduct extensive evaluation of LogGC on a pool of real-
world applications. Our results show that LogGC can garbage-
collect 92.89% of the original audit logs for client systems
and 97.35% for server systems. Furthermore, through a num-
ber of case studies, we show that the reduced logs are equally
effective in forensic analysis.
Assumptions and Limitations of LogGC First, we trust the OS as
LogGC collects, stores, and reduces audit logs at the kernel level.
Hence, a kernel-level attack could disable LogGC and tamper with
the audit log. However, LogGC can be implemented at the hyper-
visor level to mitigate such risks.
Second, we assume that, when LogGC initially starts, all user
programs and ﬁles in the system are “clean”. Attacks against these
programs and ﬁles will hence be logged by LogGC . If LogGC
begins with a compromised program, the program could disrupt
LogGC by generating bogus system-level events. Note that while
a program that gets compromised when LogGC is active may per-
form the same attack, the attack will be captured by LogGC . Our
ﬁrst two assumptions are standard for many existing system-level
auditing techniques [12, 15, 16, 17, 20, 23].
Third, LogGC instruments user programs to partition executions
and data ﬁles. While applicable to most application binaries, for
some applications (e.g., mysql), our current ﬁle partitioning tech-
nique requires the user to inspect the application’s source code us-
ing a proﬁler and select the instrumentation points from a small
number of options provided by the proﬁler. The manual efforts
are minor – only 15 statements were selected and instrumented for
mysql. More importantly, this is a one-time effort. The user also
needs to provide a small set of training inputs for LogGC to de-
termine the instrumentation points. However, the instrumentation
points are not sensitive to the inputs.
Finally, while LogGC can preprocess a large pool of commonly
used applications, users may install new applications. If a user in-
stalls a new long running application (e.g. servers or UI programs),
he/she may need to instrument the application for identiﬁcation of
execution units. If the user installs an application that induces a
large number of dependences through ﬁles (i.e., by writing to a ﬁle
and reading it later), such as a database application, he/she may
also need to instrument the application for partitioning a ﬁle into
data units. In the worst case where a newly installed application
is not instrumented, LogGC can still garbage collect logs from the
existing applications without affecting the un-reduced log entries
generated from the new application.
2. BASIC DESIGN
Redundancy abounds in audit logs. Many applications create
and operate on temporary ﬁles during execution. Such ﬁles are de-
stroyed after the applications terminate. As a result, no future sys-
tem behavior will be affected by these ﬁles. Keeping their prove-
nance is hence unnecessary. An application may only read ﬁles
or receive information from remote hosts, and send the contents to
display without saving them. After the application terminates, its
provenance is of no interest for future forensic analysis.
In this section, we present a basic algorithm that garbage-collects
redundant entries in an audit log. It is analogous to garbage collec-
tion in memory management. We deﬁne root objects as the live
processes and ﬁles at the time LogGC is invoked. We then traverse
backward in the audit log. If any root object is directly or transi-
tively dependent on a logged event, the event is marked reachable.
In the end, all unreachable log entries are removed from the audit
log. Different from classic GC, our algorithm will operate on the
log ﬁle, which is a linear sequence of events, instead of a refer-
ence graph of memory cells. Moreover, the dependences between
event entries are often not explicit. For example, assume a process
receives a packet and saves it to a ﬁle. This leads to two event en-
tries, one is a socket read and the other is a ﬁle write. It is not easy
to infer the dependence between the two events from the audit log.
A conservative approximation made by many existing audit log-
ging techniques (also called provenance tracing) [12, 15, 16, 17,
20, 23] is to assume that an event is dependent on all the preceding
input events for the same process.
1006Event type
Input
Output
Events
ﬁle read; socket read
ﬁle write (excluding stdout); socket write; process
spawn; chmod; chown; link; truncate; create
write to stdout; ﬁle deletion∗ ; process kill∗
Dead-end
∗ denotes destruction events.
Table 1: Classiﬁcation of event types
(1) Proc_A spawn(O) Proc_C
(2) Proc_A write(O)    File1
(3) Proc_A read(I)      File2
(4) Proc_B read(I)      File1
(5) Proc_B write(O)    File2
(6) Proc_B read(I)      File1
(7) Proc_B delete(D)  File1
(8) Proc_B read(I)      Socket
O - output type
 I  - input type
D - dead-end Type
Live processes: Proc_C
Live ﬁles : File2
Before explaining the algorithm, we ﬁrst classify logged events
into three categories (Table 1) as the algorithm will behave differ-
ently based on type of events.
Figure 2: An example of the basic GC algorithm. An event
entry consists of the process id, the event name (event type),
and the system object being operated on.
• An input event is one that receives data from input devices.
• An output event is one that creates inﬂuence on some other
system object and such inﬂuence will persist beyond the com-
pletion of this event.
• A dead-end event is one that has effect only on objects di-
rectly involved in the event and the effect will not create de-
pendences in subsequent execution of the system. For exam-
ple, writes to stdout will not inﬂuence any system object.
Algorithm 1 Basic Audit Log Garbage Collection Algorithm
Input:
Output:
Deﬁnition
P - the current live processes.
L - the audit log
F - the current live ﬁles.
L′ - the new audit log.
ReachableProc - the set of reachable processes.
ReachableObj - reachable system objects (e.g. ﬁles).
1: L′ ← nil
2: ReachableProc ← P
3: ReachableObj ← F
4: for each event e ∈ L in reverse order do
5:
6:
7:
8:
9:
10:
if e is an output event involving an object in ReachableObj then
L′ ← e · L′
ReachableProc ← ReachableProc ∪ the process of e
else if e is an input event involving a process in ReachableProc then
L′ ← e · L′
ReachableObj ← ReachableObj ∪ the objects operated by e
The basic algorithm is Algorithm 1. It takes the audit log, and the
current set of live processes and live ﬁles as input, and produces a
reduced audit log. It leverages two data structures ReachableProc
and ReachableObj to maintain the set of reachable processes and
system objects, respectively. We say a process, either alive or termi-
nated, is reachable if a live process or ﬁle is directly or transitively
dependent on it. Reachable system objects are similarly deﬁned.
The algorithm ﬁrst initializes ReachableProc with the set of live
processes and ReachableObj with the set of live ﬁles. It then starts
traversing the events in L in the reverse order. If an output event e
in the log operates on a reachable object (line 5), the event becomes
reachable and gets inserted to the output log L′. In the mean time,
the process involved in the event is set to reachable. According
to lines 8, 9, and 10, all the preceding input events in the process
become reachable too. Note that setting the process reachable only
affects the events preceding e, all the events that happen after e are
not reachable even though they involve the same reachable process.
The algorithm is presented with a high level of abstraction. At
the implementation level, LogGC also handles reuse of ﬁle ids and
socket ids, and supports various system objects beyond ﬁles and
sockets.
Example. Consider the example in Fig. 2. The live ﬁle and process
are shown on the right bottom of the ﬁgure. The algorithm traverses
backward. Log entries 8, 7, 6 are garbage-collected as they do not
operate on any reachable objects or involve a reachable process.
Entry 5 is an output event with live ﬁle File2 so it is inserted to
the output log ﬁle. In the mean time, Proc_B becomes reachable.
Hence, all the input events with Proc_B preceding entry 5 become
reachable including the read of File1 in entry 4. As a result,
File1 becomes a reachable object, which leads to entry 2 being
inserted to the new log. Entry 1 is the creation of a live process
and hence retained. It is worth mentioning that although entry 3
involves a reachable object File2, it is not an output event and
hence garbage-collected.
Despite its simplicity, such a basic design is insufﬁcient in prac-
tice. We will discuss how we handle various challenges in the fol-
lowing sections.
3. SUPPORTING FORWARD ANALYSIS
The basic design enables inspection of the history of any live
object, which is a backward analysis. An equally important usage
of audit logs is to facilitate understanding of attack ramiﬁcations,
namely identiﬁcation of damages that have been inﬂicted during an
attack. We call this a forward analysis as it inspects the audit log
in a forward fashion to look for ramiﬁcations of an event (e.g., the
root attack event).
Fig. 3 (a) shows a causal graph generated from the audit log of
the following hypothetical attack, with the numbers representing
the corresponding events in the graph: (1) The user visited a ma-
licious web site at “x.x.x.x:80”; Firefox is exploited such
that (2) it spawns a backdoor process and (3) downloads a mali-
cious dynamic library ﬁle x.so. (4) Later, the user launches the
ls command in bash. (5) The ls process makes use of the mali-
cious x.so. As a result, (6) while the process emits to the screen
as usual, it also (7) removes an important ﬁle .permission and
(8) the malicious library ﬁle x.so.
Assume that the user later notices the backdoor process, which
is the only trail left by the attack (without analyzing the audit log).
By performing backward analysis on the audit log, he can backtrack
to the earlier visit to the malicious web site. However, to identify
the damages caused by the attack, he also needs to perform for-
ward analysis starting from the site visit event (1). As such, the
past existence of the malicious library x.so and its removal of the
important ﬁle and the library itself can be disclosed.
However, the basic design presented in Section 2 is insufﬁcient
for forward analysis because it garbage-collects all dead-end events
since they will not affect any live system object in the future. How-
ever, some dead-end events (e.g. ﬁle removal and process termina-
tion) become important when performing forward analysis.
Revisiting the example in Fig. 3 (a), at the time of GC, only
bash and the backdoor processes are alive. Files x.so and .per-
mission have been removed from the ﬁle system. After applying
Algorithm 1, only events 1 and 2 are left. While this is sufﬁcient
to disclose the causality of the backdoor, it loses information about
the malicious library and the damages it caused. Note that stdout
1007x.x.x.x:80
(1)read(I)
bash