.
min
Standard deviation of the follower-friend ratio: We de-
ﬁne the follower-friend ratio as below:
min(#followers, #friends)
max(#followers, #friends)
.
Like the numbers of followers and friends, the follower-
friend ratios of attackers’ accounts are similar. We use a
normalized standard deviation to check the similarity as
(cid:18)std(a set of follower-friend ratios)
(cid:19)
, 1
.
min
√
n
Because attackers’ accounts usually have more friends than
followers, the follower-friend ratios of malicious accounts
are usually different from the follower-friend ratios of be-
nign accounts. Attackers, however, can fabricate this ra-
tio, because they can use Sybil followers or buy followers.
Therefore, instead of using an individual follower-friend ra-
tio, we use the standard deviation of follower-friend ratios
of accounts that post the same URLs and assume that fabri-
cated ratios will be similar.
Tweet text similarity: The texts of tweets containing the
same URL are usually similar (e.g., retweets). Therefore,
if the texts are different, we can assume that those tweets
are related to suspicious behaviors, because attackers usu-
ally want to change the appearance of malicious tweets that
include the same malicious URL. We measure the similarity
between tweet texts as(cid:88)
t,u∈a set of pairs in tweet texts
J(t, u)
|a set of pairs in tweet texts| ,
where J(t, u) is the Jaccard index [13], which is a famous
measure that determines the similarity between two sets t
and u, and is deﬁned as below:
J(t, u) =
|t ∩ u|
|t ∪ u| .
We remove mentions, hashtags, retweets, and URLs from
the texts when we measure their similarity, so that we only
consider the text features.
4 Evaluation
4.1 System Setup and Data Collection
Our system uses two Intel Quad Core Xeon E5530
2.40GHz CPUs and 24 GiB of main memory. To collect
tweets, we use Twitter Streaming APIs [27]. Our accounts
have the Spritzer access role; thus, we can collect about one
percent of the tweets from the Twitter public timeline as
samples. From April 8 to August 8, 2011 (122 days), we
collected 27,895,714 tweet samples with URLs.
4.2 Feature Selection
To evaluate and compare the features of our scheme, we
use the F-score [5]. The F-score of a feature represents
the degree of discrimination of the feature. Features with
large F-scores can split benign and malicious samples bet-
ter than features with small F-scores. The F-score shows
that the redirect chain length is the most important feature,
followed by the number of different sources and the stan-
dard deviation of the account creation date (see Table 2).
We also verify that the number of different Twitter accounts
that upload an entry point URL is a less important feature.
Table 2. F-score of our features
Feature
URL redirect chain length
Number of different sources
Standard deviation of account creation date
Frequency of entry point URL
Position of entry point URL
Standard deviation of friends-followers ratio
Number of different landing URLs
Number of different initial URLs
Standard deviation of the number of followers
Tweet text similarity
Standard deviation of the number of friends
Number of different Twitter accounts
F-score
0.0963
0.0798
0.0680
0.0374
0.0353
0.0321
0.0150
0.0117
0.0085
0.0060
0.0050
0.0008
Table 3. Training and test datasets
Dataset
Training
Testpast
Testfuture
Period
5/10–7/8
4/8–5/9
7/9–8/8
Benign Malicious
Total
183, 113
71, 220
91, 888
41, 721
6, 730
4, 421
224, 834
77, 950
96, 309
This result implies that attackers use a large number of dif-
ferent Twitter accounts to distribute their malicious URLs.
The similarity of tweet texts is also less important, because
many attackers currently do not use different tweet texts to
cloak their malicious tweets. In addition, the standard devi-
ations of the number of followers and number of friends are
less important, because benign users’ numbers of followers
and friends are also similar. Interestingly, the standard devi-
ation of the number of followers has a higher F-score value
than that of the number of friends, because fabricating the
number of followers is more difﬁcult than fabricating the
number of friends.
4.3 Training and Testing Classiﬁers
We use 60 days of tweet samples from May 10–July 8
for training the classiﬁcation models and 62 days of tweet
samples from April 8–May 9 and July 9–August 8 to test
the classiﬁer with older and newer datasets, respectively.
For training and testing, we need to label the datasets. Un-
fortunately, we cannot ﬁnd suitable blacklists for labeling
our datasets, because many URLs in our datasets, such as
blackraybansunglasses.com, are still not listed on
public URL blacklists such as the Google Safe Browsing
API [10]. Therefore, instead of URL blacklists, we use
Twitter account status information to label our datasets.
Namely, if some URLs are from suspended accounts, we
treat the URLs as malicious.
If not, we treat the URLs
as benign. Recently, Thomas et al. [25] ﬁgured out that
Table 5. Comparing classiﬁer accuracy while
varying training weights of benign samples
within a 10-fold cross validation (cost 1.6)
Weight
1.0
1.2
1.4
1.6
1.8
2.0
AUC
0.8312
0.8310
0.8310
0.8309
0.8310
0.8308
Accuracy
87.66
87.51
87.09
86.39
86.15
85.99
%
FP
1.67
1.31
1.03
0.83
0.71
0.61
FN
10.67
11.18
11.88
12.78
13.14
13.39
most suspended accounts are spam accounts. Therefore, our
treatment of URLs is valid. From the training dataset, we
found 4, 686, 226 accounts that were active and 263, 289
accounts that were suspended as of August 11, 2011. We
also found 224, 834 entry point URLs that appear more than
once in some windows of 10, 000 sample tweets. Among
them, 183, 113 entry point URLs are from active accounts
and 41, 721 entry point URLs are from suspended accounts.
Therefore, we designated the 183, 113 entry point URLs
from active accounts as benign samples and the remaining
41, 721 entry point URLs as malicious samples. We also
use the account status information to label the test datasets;
the results are shown in Table 3.
We used the LIBLINEAR library [9] to implement our
classiﬁer. We compared seven classiﬁcation algorithms
with our training dataset and selected an L2-regularized
logistic regression algorithm with a primal function, be-
cause it shows the best accuracy values with our dataset
(see Table 4). We also tested a number of learning cost
values and chose a cost value of 1.6. Because our dataset
is unbalanced—the number of benign samples is 4.4 times
larger than that of malicious samples—we must choose a
good weight value to give a penalty to benign samples. We
compared six weight values and selected a weight value of
1.4 for benign samples, because this value produces good
accuracy values and relatively low false-positive and false-
negative rates (see Table 5). All the training and 10-fold
cross validation can be done in less than three seconds in
our evaluation environment. Therefore, the training time is
negligible.
We use two test datasets that represent past and future
values, to evaluate the accuracy of our classiﬁer (see Ta-
ble 3). Whether the test datasets regard the past or future
ones, our classiﬁer achieves high accuracy, and low false-
positive and false-negative rates (see Table 6). Therefore,
our features do not tightly depend on speciﬁc time periods
and, hence, can be used generally.
Table 4. Comparing classiﬁers within a 10-fold cross validation (learning cost 1.0 and weight 1.0).
Logistic regression (LR), support vector classiﬁcation (SVC), area under the ROC curve (AUC), false
positive (FP), false negative (FN), and Lagrange primal and dual maximal violation functions that
determine termination of training.
Classiﬁer
L2-regularized LR (primal)
L2-regularized L2-loss SVC (dual)
L2-regularized L2-loss SVC (primal)
L2-regularized L1-loss SVC (dual)
L1-regularized L2-loss SVC (primal)
L1-regularized LR (primal)
L2-regularized LR (dual)
AUC
0.8312
0.8267
0.8268
0.8279
0.8269
0.8312
0.8310
Accuracy
87.67
86.93
86.95
87.50
86.74
87.64
87.63
%
FP
1.64
1.40
1.38
1.38
1.40
1.68
1.69
FN
10.69
11.67
11.67
11.67
11.86
10.67
10.67
Table 6. Classiﬁcation accuracy of
datasets
test
%
Dataset
Testpast
Testfuture
AUC
0.7113
0.7889
Accuracy
91.10
93.11
FP
1.32
3.67
FN
7.57
3.21
4.4 Data Analysis
We performed a daily analysis on the training dataset.
On average, 3756.38 entry point URLs appear more than
once in each tweet window during a given day (with a win-
dow size of 10, 000). Among them, on average, 282.93
suspicious URLs are detected, where 19.53 URLs are false
positives and 30.15 URLs are newly discovered (see Fig-
ure 6). This relatively small number of new suspicious
URLs implies that many suspicious URLs repeatedly de-
tected by WARNINGBIRD are not detected or blocked by
other existing detection systems. To verify the reoccur-
rence of suspicious URLs, we grabbed May 10’s entry point
URLs and checked how many times these URLs had ap-
peared in the Twitter public timeline during the next two
months (see Figure 7). On average, 17% of suspicious
URLs and 5.1% of benign URLs of May 10 were observed
during the two months; thus, suspicious URLs are more
repeated than benign URLs.
Interestingly, 38.9% of the
suspicious URLs had appeared again on July 4, 55 days
later. Therefore, existing detection schemes cannot detect
or block a portion of suspicious URLs that can be detected
by WARNINGBIRD.
We also determine whether the domain groupings allow
us to detect a larger number of suspicious URLs. We com-
Figure 6. Daily analysis on training datasets
(60 days: May 10–July 8)
pare grouped and ungrouped URLs by using 16 days of
tweet samples from between July 23 and August 8 (except
July 31 owing to a local power blackout). On average, we
ﬁnd 334.94 unique suspicious entry point URLs when we
group URLs and 79.88 suspicious URLs when we do not
group URLs (see Figure 8). Therefore, the domain group-
ings give us about 4.19 times better detection rates.
4.5 Running Time
We evaluated the running time of our system. First,
we compared the running time of each component of our
system—domain grouping; feature extraction, including the
detection of entry points; and classiﬁcation—in a single
window of collected tweets that varies in size. Even if the
0500100015002000250030003500400045005000Number of entry point URLs Day allsuspiciousnewFPFigure 7. Reoccurrence of May 10’s entry
point URLs (59 days: May 11–July 8)
Figure 8. Comparing domain grouping results
(16 days: July 23–August 8 excluding July 31)
Table 7. Required time to classify a single URL
when a window size is 100,000 and using 100
concurrent connections for crawling
Component
Redirect chain crawling
Domain grouping
Feature extraction
Classiﬁcation
Total
Avg. running time (ms)
24.202
2.003