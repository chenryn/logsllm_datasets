title:Bayesian Event Classification for Intrusion Detection
author:Christopher Kr&quot;ugel and
Darren Mutz and
William K. Robertson and
Fredrik Valeur
Bayesian Event Classiﬁcation for Intrusion Detection
Christopher Kruegel Darren Mutz William Robertson
Fredrik Valeur
Reliable Software Group
University of California, Santa Barbara
{chris, dhm, wkr, fredrik}@cs.ucsb.edu
Abstract
Intrusion detection systems (IDSs) attempt to identify at-
tacks by comparing collected data to predeﬁned signatures
known to be malicious (misuse-based IDSs) or to a model
of legal behavior (anomaly-based IDSs). Anomaly-based
approaches have the advantage of being able to detect pre-
viously unknown attacks, but they suffer from the difﬁculty
of building robust models of acceptable behavior which may
result in a large number of false alarms. Almost all current
anomaly-based intrusion detection systems classify an in-
put event as normal or anomalous by analyzing its features,
utilizing a number of different models. A decision for an in-
put event is made by aggregating the results of all employed
models.
We have identiﬁed two reasons for the large number of
false alarms, caused by incorrect classiﬁcation of events in
current systems. One is the simplistic aggregation of model
outputs in the decision phase. Often, only the sum of the
model results is calculated and compared to a threshold.
The other reason is the lack of integration of additional
information into the decision process. This additional in-
formation can be related to the models, such as the conﬁ-
dence in a model’s output, or can be extracted from exter-
nal sources. To mitigate these shortcomings, we propose
an event classiﬁcation scheme that is based on Bayesian
networks. Bayesian networks improve the aggregation of
different model outputs and allow one to seamlessly incor-
porate additional information. Experimental results show
that the accuracy of the event classiﬁcation process is sig-
niﬁcantly improved using our proposed approach.
1 Introduction
Intrusion detection can be deﬁned as the process of iden-
tifying malicious behavior that targets a network and its re-
sources. Intrusion detection systems have traditionally been
classiﬁed as either misuse-based or anomaly-based. Sys-
tems that use misuse-based techniques contain a number of
attack descriptions, or ‘signatures’, that are matched against
a stream of audit data looking for evidence of the modeled
attacks. The audit data can be gathered from the network
[18, 25], from the operating system [7, 17], or from appli-
cation [23] log ﬁles. Signature-based systems have the ad-
vantage that they usually generate few false positives (i.e.,
incorrectly ﬂagging an event as malicious when it is legiti-
mate). Unfortunately, they can only detect those attacks that
have been previously speciﬁed. That is, they cannot detect
intrusions for which they do not have a deﬁned signature.
Anomaly-based techniques follow an approach that is
complementary with respect to misuse detection. These ap-
proaches rely on models, or proﬁles, of the normal behav-
ior of users [4, 8], applications [5, 26] and network trafﬁc
[10, 14, 15]. Deviations from the established models are
interpreted as attacks. Anomaly detection systems have the
advantage that they are able to identify previously unknown
attacks. By deﬁning an expected, normal state, any abnor-
mal behavior can be detected, whether it is part of the threat
model or not. This capability should make anomaly-based
systems a preferred choice. However, the advantage of be-
ing able to detect previously unknown attacks is usually
paid for in terms of a large number of false positives. This
can make the system unusable by ﬂooding and eventually
desensitizing the system administrator with large numbers
of incorrect alerts.
We have identiﬁed two main problems that contribute
to the large number of false positives. First, the decision
whether an event should be classiﬁed as anomalous or as
normal is made in a simplistic way. Anomaly detection
systems usually contain a collection of models that eval-
uate different features of an event. These models return
an anomaly score or a probability value that reﬂects the
‘normality’ of this event according to their current proﬁles.
However, the system is faced with the task of aggregat-
ing the different model outputs into a single, ﬁnal result.
The difﬁculty is the fact that this aggregation is not easy
to perform, especially when the individual model outputs
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:47:09 UTC from IEEE Xplore.  Restrictions apply. 
differ signiﬁcantly. In most current systems, the problem
is solved by calculating the sum of the outputs and com-
paring it to a static threshold. The disadvantage of this ap-
proach is the fact that this threshold has to be small enough
to detect malicious events that only manifest themselves in
a single anomalous feature (i.e., only one model outputs a
high value indicating malicious behavior). This can lead to
false positives, because events with many features that devi-
ate slightly from the proﬁle might receive aggregated scores
that exceed the threshold.
The second problem of anomaly-based systems is
that they cannot distinguish between anomalous behavior
caused by unusual but legitimate actions and activity that
is the manifestation of an attack. This leads to the situa-
tion where any deviation from normal behavior is reported
as suspicious, ignoring potential additional information that
might suggest otherwise. Such additional information can
be external to the system, received from system health mon-
itors (e.g., CPU utilization, memory usage, process status)
or other intrusion detection sensors. Consider the example
of an IDS that monitors a web server by analyzing the sys-
tem calls that the server process invokes. A sudden jump in
CPU utilization and a continuous increase of the memory al-
located by the server process can corroborate the belief that
a certain system call contains traces of a denial-of-service
attack. Additional information can also be directly related
to the models, such as the conﬁdence in a model output. De-
pending on the site-speciﬁc structure of input events, certain
features might not be suitable to distinguish between legiti-
mate and malicious activity. In such a case, the conﬁdence
in the output of the model based on these features should be
reduced.
We propose to mitigate the two problems described
above by replacing the simple, threshold-based decision
process with a Bayesian network. Instead of calculating the
sum of individual model outputs and comparing the result
to a threshold, we utilize a Bayesian decision process to
classify input events. This process allows us to seamlessly
incorporate available additional information into the detec-
tion decision and to aggregate different model outputs in a
more meaningful way. The contribution of this paper is the
description of this decision process, a novel method of event
classiﬁcation in anomaly-based intrusion detection systems.
Experimental results conﬁrm that our approach is capable of
signiﬁcantly reducing the number of false alarms.
The paper is structured as follows. Section 2 provides
background information on Bayesian networks to help the
reader in understanding the rest of the paper. Section 3 de-
scribes related work and discusses previous efforts to uti-
lize Bayesian techniques for intrusion detection. Section 4
introduces our approach of Bayesian event classiﬁcation.
Section 5 describes the system implementation and provides
details of the underlying anomaly-based models. Section 6
shows experimental results that conﬁrm that our solution is
more accurate (i.e., reports fewer false alerts) than previous
approaches. Finally, Section 7 brieﬂy concludes.
2 Bayesian Networks
A Bayesian network is used to model a domain contain-
ing uncertainty [9, 13]. It is a directed acyclic graph (DAG)
where each node represents a discrete random variable of
interest. Each node contains the states of the random vari-
able that it represents and a conditional probability table
(CPT). The CPT of a node contains probabilities of the node
being in a speciﬁc state given the states of its parents. The
parent-child relationship between nodes in a Bayesian net-
work indicates the direction of causality between the corre-
sponding variables. That is, the variable represented by the
child node is causally dependent on the ones represented by
its parents.
Consider the following example where a farmer has a
bottle of milk that can be either infected or clean. She also
has a test that can determine with a high probability whether
the milk is infected or not (i.e., the outcome of the test is ei-
ther positive or negative). This situation can be represented
with two random boolean variables, infected and pos-
itive. The variable infected is true when the milk is
actually infected and false otherwise. The variable posi-
tive is true when the test claims that the milk is infected
and false when the outcome of the test is negative. Note
that it is possible that the milk is clean when the test has a
positive outcome and vice versa.
infected
infected
true
false
0.005
0.995
CPT (infected)
infected
true false
e
v
i
t
i
s
o
p
true
0.99 0.01
positive
false
0.01 0.99
CPT (positive)
Figure 1. Bayesian Network and CPTs
A possible Bayesian network that models this situation
is shown in Figure 1. The two random variables are rep-
resented as two nodes in the network. It is assumed that
the farmer knows the CPT for the variable positive, that
is, the probability of a positive result given that the milk is
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:47:09 UTC from IEEE Xplore.  Restrictions apply. 
infected and the probability of a positive test result given
that the milk is clean. She also knows the CPT for the vari-
able infected, which states the probability that a bot-
tle contains infected milk. The arrow from the infected
to the positive node indicates a causal relationship be-
tween the respective variables. In this case, we expect that
the outcome of the test is dependent on the true state of the
milk (infected or clean). Variables without parents are not
inﬂuenced directly by other variables.
The node that represents the outcome of the test in Fig-
ure 1 is often called an information variable. The state of
these variables are usually given or can be measured in a
straightforward manner. The node that represents the actual
state of the milk is called a hypothesis variable. The states
of such variables cannot be obtained immediately. The pur-
pose of a Bayesian network is to allow one to calculate the
probability of the hypothesis variable(s) given the evidence
gathered from information variables. In our example, the
farmer might want to calculate the probability that the milk
is infected given a positive test result. By entering the evi-
dence (e.g., positive is true) into the Bayesian network,
the probability that infected is true can be derived. The
numerical value for this probability, called the a-posteriori
probability given the support of the observed evidence, is
0.33. Intuitively, one would expect a higher value, espe-
cially when considering that the test is very accurate. How-
ever, the low initial probability of the milk being infected,
called the a-priori probability before any observations are
made, explains this result.
In the domain of intrusion detection, information nodes
are associated with measurable properties of input events
or corresponding model outputs. The hypothesis node is
a classiﬁcation that determines the state of the event –
whether it is anomalous or not.
3 Related Work
Axelsson [1] wrote a well-known paper that uses the
Bayesian rule of conditional probability1 to point out the
implications of the base-rate fallacy for intrusion detection.
Similar to our example with the infected milk in the previ-
ous section, he observed that the positive result of a very ac-
curate test (such as the test for infection) does not necessar-
ily imply a high probability of the hypothesis variable to be
true (i.e., the milk to be actually infected). For the domain
of intrusion detection, this ﬁnding means that even tests or
models that identify malicious events very accurately may
raise many false alarms because the a-priori probability of
an attack in the input data stream is usually very low. Al-
though Axelsson’s paper is only remotely related to our
1The Bayesian rule of conditional probability is given as p(B|A) =
.
p(A|B)p(B)
p(A)
work through the application of Bayesian probability the-
ory, it clearly demonstrates the difﬁculty and necessity of
dealing with false alarms.
Several researchers have adapted ideas from Bayesian
statistics to create models for anomaly-based IDSs.
In
[16], a behavior model is introduced that uses Bayesian
techniques to obtain model parameters with maximal a-
posteriori probabilities.
In [6], a model is presented that
simulates an intelligent attacker using Bayesian techniques
to create a plan of goal-directed actions. Their work is sim-
ilar to ours to the extent that Bayesian statistics is applied.
Their work differs from our approach because it uses Bayes’
rule to optimize or create models, while we utilize Bayesian
networks to classify events based on model outputs and ad-
ditional information from the environment.
Two anomaly-based IDS have been proposed that use
na¨ıve Bayesian networks to classify input events based
on the output of several models. A na¨ıve Bayesian net-
work is a restricted network that has only two layers and
assumes complete independence between the information
nodes (i.e., the random variables that can be observed and
measured). These limitations result in a tree-shaped net-
work with a single hypothesis node (root node) that has
arrows pointing to a number of information nodes (child
nodes). All child nodes have exactly one parent node, that
is, the root node, and no other causal relationship between
nodes are permitted.
In [24], a na¨ıve Bayesian network
(shown in Figure 2) is employed to perform intrusion de-
tection on network events. In [19], a system is described
that detects malicious proxylets (executable code) in active
networks.
Unfortunately, the classiﬁcation capability of a na¨ıve
Bayesian network is identical to a threshold-based system
that computes the sum of the outputs obtained from the
child nodes. This is due to the fact that all models (i.e., the
child nodes) operate independently and only inﬂuence the
probability of the root node. This single probability value
at the root node can be represented by a threshold in tra-
ditional approaches. In addition, the restriction of having
a single parent node complicates the incorporation of addi-
tional information. This is because variables that represent
such information cannot be linked directly to the nodes rep-
resenting the model outputs.
Alternatively, we propose an event classiﬁcation that
makes full use of Bayesian networks. This allows us to
model inter-model dependencies (i.e., dropping the assump-
tions of independent child nodes) and to integrate additional
data such as model conﬁdence (i.e., dropping the restriction
of at most a single parent node). Our experiments show that
these extensions improve the quality of the decision process
and signiﬁcantly reduce the number of false alarms.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:47:09 UTC from IEEE Xplore.  Restrictions apply. 
Session Class
Event Intensity
Error Intensity
Max Open to
Any Host
Number of
Unique Ports
Connect Code
Distribution
Number of
Unique IP
Addresses
Service
Distribution
Figure 2. Na¨ıve Bayesian Network
4 System Design
Given an ordered stream of
input events S =
{e1, e2, . . .}, the task of the event classiﬁcation mechanism
is to decide for each ei ∈ S whether it is normal or anoma-
lous. This decision is based on the outputs {oi|i = 1 . . . k}
of k models M = {m1, . . . , mk} and possibly additional