(cid:3)
The overall algorithm is implemented in the M¨obius tool [7]
and the implementation is integrated in the symbolic state-
space generator (SSG) [10].
5 Example
) and
(cid:3)
i
(cid:3)
i
We consider a tandem multi-processor system with load-
balancing and failure and repair operations that consists of
two subsystems: MSMQ and hypercube. Each subsystem
8
Figure 4. MSMQ subsystem
(cid:3)
. A and A
The hypercube subsystem consists of 8 cube-connected
servers (Fig. 5). All the servers have the same behavior
(given they are in the same state) except for two servers A
(cid:3)
and A
have the same behavior as each other but
are special (with respect to the other hypercube servers) in
that they receive jobs from the input pool as described later.
Servers in the hypercube can fail, and are repaired by a sin-
gle repair facility that picks servers to repair uniformly from
the pool of failed servers3. A failed server keeps its jobs in
its queue unless they are transferred to a neighbor server by
the load-balancing scheme described below. The subsys-
3Failure and repair behaviors of the subsystem are not shown in Fig. 5.
tem is considered unavailable when two or more servers are
down.
Each server has a queue with capacity J.
Jobs en-
ter the servers’ queues by a dispatcher that picks a job
from the subsystem’s input pool and assigns it to either
(cid:3)
with a probability distribution that
server A or server A
favors the server that has fewer jobs in its queue. Another
load-balancing scheme that governs the distribution of jobs
among the servers is that whenever a server has > 1 more
jobs than any of its neighbors, it sends one of its jobs to
one of those neighbors, again assigning higher probability
to servers with fewer jobs. That is how jobs are distributed
among all servers. Finally, if a server fails, it transfers jobs
in its queue one by one, with an exponentially distributed
delay, to a neighbor server that is not down.
A’
J
1
2
3
J
1
2
3
J
1
2
3
unlumped SS sizes
S1
2
3
4
(cid:1)S1
overall
22100
197600
1236300
overall
395
4075
28090
lumped SS sizes
S2
650
3575
14300
(cid:1)S2
(cid:1)S3
2
3
4
30
178
803
40
175
555
S3
160
700
2220
# of MD nodes
N1 N2 N3
3
4
5
3
5
7
1
1
1
reduction in SS
overall
55.9
48.4
44
l2
21.7
20.4
17.8
l3
4
4
4
unlumped SS
gen time MD space
53.9 KB
421.0 KB
2230.0 KB
0.05 s
0.80 s
12.10 s
lumped SS
lump time MD space
4.7 KB
36.0 KB
201.0 KB
0.04 s
0.26 s
1.80 s
load−balancing
job dispatcher
input
pool
A
Figure 5. Hypercube subsystem
We used the M¨obius tool [7] to model each of the subsys-
tems using the stochastic activity network formalism [19].
Then, we composed the models by sharing their input and
output pools via the Join operator in the Rep/Join composed
model editor. Our implementation of the symbolic SSG
[10] automatically partitions the set of places of the com-
plete model and assigns each block of the partition to one
level of the MD as follows: (1) common places of the two
submodels, i.e., input and output pools, (2) places of the hy-
percube submodel minus those in level 1, and (3) places of
the MSMQ submodel minus those in level 1.
Performance Results. All experiments were conducted us-
ing an Athlon XP2400 machine with 1.5 GB of main mem-
ory running Linux OS. The implementation was based on
M¨obius version 1.6.0 and was compiled with the gcc 3.3
compiler with the -O3 optimization option.
Table 1 shows information about the MD representation
of the original (i.e., unlumped) and lumped CTMC of the
tandem multi-processor system for different values of J.
More speciﬁcally, the upper table shows the state-space size
for each level and for the complete model and the number of
MD nodes in each level. The middle table shows the lumped
state-space size for each level and for the complete model
and the state-space reduction we gain from the composi-
tional lumping algorithm for each level and for the complete
model. Notice that the number of nodes in each level of the
lumped and unlumped MC is the same because the com-
positional lumping algorithm only replaces each MD node
9
Table 1. Speciﬁcations of MD representation
of tandem system’s CTMC
with a possibly smaller one and does not create or delete any
node. In the middle part of the table, l2 and l3 refer to the
second and third levels of the MD. The state-space reduc-
tion for level 1 is not shown in the table because it is always
1. Finally, the lower table gives computation times in sec-
onds for the symbolic state-space generation and lumping
algorithm as well as memory use of the MDs of the un-
lumped and lumped MC in kilobytes.
We observe in Table 1 that the compositional lumping
algorithm reduces the state-space size of the overall model
to roughly 1/40 to 1/50 of its original value, and that is
roughly equal to the product of the reductions in state space
for all the levels. The equivalently behaving sets of servers,
that is, (1) the three servers of the MSMQ subsystem, (2)
(cid:3)
in the hypercube, and (3) the other 6
servers A and A
servers in the hypercube, are the source of the lumpability
found by our compositional lumping algorithm.
As mentioned before, our algorithm does not necessarily
generate the smallest possible lumped CTMC (or its MD
representation), because it is applied locally at each level of
the MD and does not have a global view of the CTMC rep-
resented by the MD. In other words, the resulting lumped
CTMC could possibly be lumped to a smaller CTMC by
a state-level lumping algorithm that has a ﬂat (i.e., global)
view of the CTMC. Obviously, in the worst case, none of the
levels of the MD satisfy the lumpability conditions for any
non-trivial partition (partitions with more than one class),
so that our lumping algorithm cannot reduce the size of
the state space. For the given example, we veriﬁed that
our compositional algorithm generates the smallest lumped
CTMC possible. We did that by running the compositional
algorithm result through our implementation of the state-
level lumping algorithm [9].
Generally, the reduction in state-space size has two ma-
jor effects on the efﬁciency of iterative numerical solution
algorithms that compute measures of CTMCs:
it reduces
both the space and time requirements for such algorithms.
Reduction in the size of the state space affects space require-
ments in two ways. First, it makes the MD representation
of the CTMC smaller. In our example, the memory require-
ment for the MD has been reduced by around an order of
magnitude for all values of J. Second, and more impor-
tantly, it reduces the size of the solution vector; in our ex-
ample, the vector was reduced to no more than 1/40 of its
original size. Therefore, the advantage of using our compo-
sitional lumping algorithm is that we can solve larger mod-
els than would be possible using only symbolic techniques;
for our example, we solved models that are one or two or-
ders of magnitude larger. Reduction in the state space also
results in a roughly proportionate reduction in the amount
of time spent for each iteration of the numerical solution
algorithm.
It is important to realize that all the beneﬁts
in terms of time and space requirements described above
are achieved through an efﬁcient algorithm in an amount of
time that is negligible compared to the time needed for nu-
merical analysis, and that is, for our example, considerably
less than the time needed for state-space generation.
6 Conclusion
In this paper, we presented theoretical results, algo-
rithms, implementation, and results from exercising an ap-
plication example for the compositional lumping of CTMCs
that are represented as an MD. The main point is that
nodes of each level are reduced separately from those of
other levels, i.e., based on local conditions. Unlike previ-
ous compositional lumping algorithms that were formalism-
dependent, our algorithm is applicable on any MD, and
thus, on any formalism that uses MDs for its CTMC rep-
resentation. We consider ordinary as well as exact lumpa-
bility.
Acknowledgments We thank Jenny Applequist and several
referees for their valuable remarks.
References
[1] A. Benoit, L. Brenner, P. Fernandes, and B. Plateau.
Aggregation of stochastic automata networks with
replicas.
In Linear Algebra and Its Applications,
386:111–136, 2004.
[2] P. Buchholz. Exact and ordinary lumpability in ﬁnite
Markov chains. J. of App. Prob., 31:59–74, 1994.
[3] P. Buchholz. Equivalence relations for stochastic au-
tomata networks. In W. J. Stewart, editor, Computa-
tion with Markov Chains, Kluwer, 1995.
10
[4] G. Chiola, C. Dutheillet, G. Franceschinis, and
S. Haddad. Stochastic well-formed colored nets and
symmetric modeling applications.
IEEE Trans. on
Computers, 42(11):1343–1360, November 1993.
[5] G. Ciardo, R. Marmorstein, and R. Siminiceanu. Satu-
ration unbound. In Proc. of TACAS, LNCS 2619, pages
379–393, Springer, 2003.
[6] G. Ciardo and A. Miner. A data structure for the
In Proc. of
efﬁcient Kronecker solution of GSPNs.
PNPM, pages 22–31, IEEE CS, 1999.
[7] D. D. Deavours, G. Clark, T. Courtney, D. Daly, S. De-
risavi, J. M. Doyle, W. H. Sanders, and P. G. Webster.
The M¨obius framework and its implementation. IEEE
Trans. on Soft. Eng., 28(10):956–969, October 2002.
[8] C. Delamare, Y. Gardan, and P. Moreaux. Perfor-
mance evaluation with asynchronously decomposable
SWN: implementation and case study.
In Proc. of
PNPM, pages 20–29, IEEE CS, 2003.
[9] S. Derisavi, H. Hermanns, and W. H. Sanders. Opti-
mal state-space lumping in Markov chains. Inf. Proc.
Letters, 87(6):309–315, September 2003.
[10] S. Derisavi, P. Kemper, and W. H. Sanders. Sym-
bolic state-space exploration and numerical analysis
of state-sharing composed models. Linear Algebra
and Its Applications, 386:137–166, July 2004.
[11] O. Gusak, T. Dayar, and J.-M. Fourneau. Lumpable
continuous-time stochastic automata networks. Euro-
pean J. of Operational Research, 148:436–451, 2003.
Interactive Markov Chains and the
Quest for Quantiﬁed Quality, LNCS 2428, Springer
2002.
[12] H. Hermanns.
[13] J. G. Kemeney and J. L. Snell. Finite Markov Chains.
D. Van Nostrand Company, Inc., 1960.
[14] M. Ajmone Marsan, G. Balbo, G. Conte, S. Donatelli,
and G. Franceschinis. Modelling With Generalized
Stochastic Petri Nets. John Wiley & Sons, 1995.
[15] A. Miner. Efﬁcient solution of GSPNs using canonical
matrix diagams. In Proc. of PNPM, pages 101–110,
IEEE CS, 2001.
[16] A. Miner and D. Parker. Symbolic representations and
analysis of large probabilistic systems. In Validation
of Stochastic Systems: A Guide to Current Research,
LNCS 2925, Springer, 2004.
[17] B. Plateau and K. Atif. Stochastic automata network
IEEE Trans. in Soft.
for modeling parallel systems.
Eng., 17(10):1093–1108, October 1991.
[18] W. H. Sanders and J. F. Meyer.
Reduced base
model construction methods for stochastic activity
networks.
IEEE J. on Selected Areas in Communi-
cations, 9(1):25–36, January 1991.
[19] W. H. Sanders and J. F. Meyer. Stochastic activity net-
works: Formal deﬁnitions and concepts. In Lectures
on Formal Methods and Performance Analysis, LNCS
2090, Springer 2000.