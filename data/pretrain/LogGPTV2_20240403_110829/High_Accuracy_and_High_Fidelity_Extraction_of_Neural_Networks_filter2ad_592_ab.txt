Learning
Functionally equivalent (this work)
Efﬁcient learning (this work)
Direct Recovery
Learning
LM
LM, NN
DT
NN (2 layer)
LM, NN
NN
LM
CNN
NN
NN
NN
NN (2 layer)
NN
Functionally Equivalent
Task Accuracy, Fidelity
Functionally Equivalent
Functionally Equivalent
Task Accuracy
Fidelity
Functionally Equivalent
Task Accuracy, Fidelity
Fidelity
Functionally Equivalent
Task Accuracy
Labels
Probabilities, labels
Probabilities, labels
Gradients, logits
Gradients
Probabilities, labels
Labels
Labels
Labels
Power Side Channel
Probabilities
Functionally Equivalent
Task Accuracy, Fidelity
Probabilities, logits
Probabilities
Table 1: Existing Model Extraction Attacks. Model types are abbreviated: LM = Linear Model, NN = Neural Network, DT =
Decision Tree, CNN = Convolutional Neural Network.
(cid:2)argmax( ˆO(x)) = y(cid:3).
Indeed, functionally equivalent extraction achieves a perfect
ﬁdelity of 1 on all distributions and all similarity functions.
Task Accuracy Extraction For the true task distribution
DA over X × Y , the goal of task accuracy extraction is to
construct an ˆO maximizing Pr(x,y)∼DA
This goal is to match (or exceed) the accuracy of the target
model, which is the easiest goal to consider in this taxonomy
(because it doesn’t need to match the mistakes of O).
Existing Attacks
In Table 1, we ﬁt previous model extrac-
tion work into this taxonomy, as well as discuss their tech-
niques. Functionally equivalent extraction has been consid-
ered for linear models [8, 13], decision trees [11], both given
probabilities, and neural networks [19, 25], given extra ac-
cess. Task accuracy extraction has been considered for linear
models [11] and neural networks [12, 16, 19], and ﬁdelity ex-
traction has also been considered for linear models [11] and
neural networks [7, 15]. Notably, functionally equivalent at-
tacks require model-speciﬁc techniques, while task accuracy
and ﬁdelity typically use generic learning-based approaches.
3.3 Model Extraction is Hard
Before we consider adversarial capabilities in Section 3.4 and
potential corresponding approaches to model extraction, we
must understand how successful we can hope to be. Here,
we present arguments that will serve to bound our expecta-
tions. First, we will identify some limitations of functionally
equivalent extraction by constructing networks which require
arbitrarily many queries to extract. Second, we will present
another class of networks that cannot be extracted with ﬁ-
delity without querying a number of times exponential in its
depth. We provide intuition in this section and later prove
these statements in Appendix A.
Exponential hardness of functionally equivalent at-
tacks. In order to show that functionally equivalent extraction
is intractable in the worst case, we construct of a class of
neural networks that are hard to extract without making expo-
nentially many queries in the network’s width.
Theorem 1. There exists a class of width 3k and depth 2
neural networks on domain [0,1]d (with precision p numbers)
with d ≥ k that require, given logit access to the networks,
Θ(pk) queries to extract.
The precision p is the number of possible values a feature
can take from [0,1]. In images with 8-bit pixels, we have
p = 256. The intuition for this theorem is that a width 3k
network can implement a function that returns a non-zero
value on at most a p−k fraction of the space. In the worst case,
pk queries are necessary to ﬁnd this fraction of the space.
Note that this result assumes the adversary can only observe
the input-output behavior of the oracle. If this assumption is
broken then functionally equivalent extraction becomes prac-
tical. For example, Batina et al. [25] perform functionally
equivalent extraction by performing a side channel attack
(speciﬁcally, differential power analysis [26]) on a micropro-
cessor evaluating the neural network.
We also observe in Theorem 2 that, given white-box access
to two neural networks, it is NP-hard in general to test if they
are functionally equivalent. We do this by constructing two
networks that differ only in coordinates satisfying a subset
sum instance. Then testing functional equivalence for these
networks is as hard as ﬁnding the satisfying subset.
Theorem 2 (Informal). Given their weights, it is NP-hard to
test whether two neural networks are functionally equivalent.
Any attack which can claim to perform functionally equiv-
alent extraction efﬁciently (both in number of queries used
and in running time) must make some assumptions to avoid
these pathologies. In Section 6, we will present and discuss
the assumptions of a functionally equivalent extraction attack
for two-layer neural network models.
1348    29th USENIX Security Symposium
USENIX Association
Learning approaches struggle with ﬁdelity. A ﬁnal difﬁ-
culty for model extraction comes from recent work in learn-
ability [27]. Das et al. prove that, for deep random networks
with input dimension d and depth h, model extraction ap-
proaches that can be written as Statistical Query (SQ) learning
algorithms require exp(O(h)) samples for ﬁdelity extraction.
SQ algorithms are a restricted form of learning algorithm
which only access the data with noisy aggregate statistics;
many learning algorithms, such as (stochastic) gradient de-
scent and PCA, are examples. As a result, most learning-based
approaches to model extraction will inherit this inefﬁciency.
A sample-efﬁcient approach therefore must either make as-
sumptions about the model to be extracted (to distinguish
it from a deep random network), or must access its dataset
without statistical queries.
Theorem 3 (Informal [27]). Random networks with domain
{0,1}d and range {0,1} and depth h require exp(O(h)) sam-
ples to learn in the SQ learning model.
3.4 Adversarial Capabilities
We organize an adversary’s prior knowledge about the oracle
and its training data into three categories—domain knowledge,
deployment knowledge, and model access.
3.4.1 Domain Knowledge
Domain knowledge describes what the adversary knows about
the task the model is designed for. For example, if the model is
an image classiﬁer, then the model output should not change
under standard image data augmentations, such as shifts, ro-
tations, or crops. Usually, the adversary should be assumed to
have as much domain knowledge as the oracle’s designer.
In some domains, it is reasonable to assume the adver-
sary has access to public task-relevant pretrained models or
datasets. This is often the case for learning-based model ex-
traction, which we develop in Section 4. We consider an ad-
versary using part of a public dataset of 1.3 million images [4]
as unlabeled data to mount an attack against a model trained
on a proprietary dataset of 1 billion labeled images [28].
Learning-based extraction is hard without natural data
In learning-based extraction, we assume that the adversary
is able to collect public unlabeled data to mount their attack.
This is a natural assumption for a theft-motivated adversary
who wishes to steal the oracle for local use—the adversary
has data they want to learn the labels of without querying the
model! For other adversaries, progress in generative modeling
is likely to offer ways to remove this assumption [29]. We
leave this to future work because our overarching aim in
this paper is to characterize the model extraction attacker
space around the notions of accuracy and ﬁdelity. All progress
achieved by our approaches is complementary to possible
progress in synthetic data generation.
3.4.2 Deployment Knowledge
Deployment knowledge describes what the adversary knows
about the oracle itself, including the model architecture, train-
ing procedure, and training dataset. The adversary may have
access to public artifacts of the oracle—a distilled version of
the oracle may be available (such as for OpenAI GPT [30])
or the oracle may be transfer learned from a public pretrained
model (such as many image classiﬁers [31] or language mod-
els like BERT [32]).
In addition, the adversary may not even know the features
(the exact inputs to the model) or the labels (the classes the
model may output). While the latter can generally be inferred
by interacting with the model (e.g., making queries and ob-
serving the labels predicted by the model), inferring the for-
mer is usually more difﬁcult. Our preliminary investigations
suggest that these are not limiting assumptions, but we leave
proper treatment of these constraints to future work.
3.4.3 Model Access
Model access describes the information the adversary obtains
from the oracle, including bounds on how many queries the
adversary may make as well as the oracle’s response:
• label: only the label of the most-likely class is revealed.
• label and score: in addition to the most-likely label, the
conﬁdence score of the model in its prediction for this
label is revealed.
• top-k scores: the labels and conﬁdence scores for the k
classes whose conﬁdence are highest are revealed.
• scores: conﬁdence scores for all labels are revealed.
• logits: raw logit values for all labels are revealed.
In general, the more access an adversary is given, the more
effective they should be in accomplishing their goal. We in-
stantiate practical attacks under several of these assumptions.
Limiting model access has also been discussed as a defensive
measure, as we elaborate in Section 8.
4 Learning-based Model Extraction
We present our ﬁrst attack strategy where the victim model
serves as a labeling oracle for the adversary. While many
attack variants exist [7, 11], they generally stage an iterative
interaction between the adversary and the oracle, where the
adversary collects labels for a set of points from the oracle
and uses them as a training set for the extracted model. These
algorithms are typically designed for accuracy extraction; in
this section, we will demonstrate improved algorithms for
accuracy extraction, using task-relevant unlabeled data.
We realistically simulate large-scale model extraction by
considering an oracle that was trained on 1 billion Instagram
USENIX Association
29th USENIX Security Symposium    1349
images [28] to obtain (at the time of the experiment) state-
of-the-art performance on the standard image classiﬁcation
benchmark, ImageNet [4]. The oracle, with 193 million pa-
rameters, obtained 84.2% top-1 accuracy and 97.2% top-5
accuracy on the 1000-class benchmark—we refer to the model
as the "WSL model", abbreviating the paper title. We give
the adversary access to the public ImageNet dataset. The ad-
versary’s goal is to use the WSL model as a labeling oracle
to train an ImageNet classiﬁer that performs better than if
we trained the model directly on ImageNet. The attack is
successful if access to the WSL model—trained on 1 billion
proprietary images inaccessible to the adversary—enables
the adversary to extract a model that outperforms a baseline
model trained directly with ImageNet labels. This is accu-
racy extraction for the ImageNet distribution, given unlabeled
ImageNet training data.
We consider two variants of the attack: one where the adver-
sary selects 10% of the training set (i.e., about 130,000 points)
and the other where the adversary keeps the entire training set
(i.e., about 1.3 million points). To put this number in perspec-
tive, recall that each image has a dimension of 224x224 pixels
and 3 color channels, giving us 224· 224· 3 = 150,528 total
input features. Each image belongs to one of 1,000 classes.
Although ImageNet data is labeled, we always treat it as unla-
beled to simulate a realistic adversary.
4.1 Fully-supervised model extraction
The ﬁrst attack is fully supervised, as proposed by prior
work [11]. It serves to compare our subsequent attacks to
prior work, and to validate our hypothesis that labels from the
oracle are more informative than dataset labels.
The adversary needs to obtain a label for each of the points
it intends to train the extracted model with. Then it queries the
oracle to label its training points with the oracle’s predictions.
The oracle reveals labels and scores (in the threat model from
Section 3) when queried.
The adversary then trains its model to match these labels
using the cross-entropy loss. We used a distillation tempera-
ture of T = 1.5 in our experiments after a random search. Our
experiments use two architectures known to perform well on
image classiﬁcation: ResNet-v2-50 and ResNet-v2-200.
4.2 Unlabeled data improves query efﬁciency
For adversaries interested in theft, a learning-based strategy
should minimize the number of queries required to achieve a
given level of accuracy. A natural approach towards this end is
to take advantage of advances in label-efﬁcient ML, including
active learning [33] and semi-supervised learning [34].
Active learning allows a learner to query the labels of ar-
bitrary points—the goal is to query the best set of points
to learn a model with. Semi-supervised learning considers
a learner with some labeled data, but much more unlabeled
data—the learner seeks to leverage the unlabeled data (for
example, by training on guessed labels) to improve classiﬁ-
cation performance. Active and semi-supervised learning are
complementary techniques [35, 36]; it is possible to pick the
best subset of data to train on, while also using the rest of the
unlabeled data without labels.
The connection between label-efﬁcient
learning and
learning-based model extraction attacks is not new [11,13,15],
but has focused on active learning. We show that, assuming
access to unlabeled task-speciﬁc data, semi-supervised learn-
ing can be used to improve model extraction attacks. This
could potentially be improved further by leveraging active
learning, as in prior work, but our improvements are overall
complementary to approaches considered in prior work. We
explore two semi-supervised learning techniques: rotation
loss [37] and MixMatch [38].
Rotation loss. We leverage the current state-of-the-art semi-
supervised learning approach on ImageNet, which aug-
ments the model with a rotation loss [37]. The model
contains two linear classiﬁers from the second-to-last
layer of the model: the classiﬁer for the image classiﬁ-
cation task, and a rotation predictor. The goal of the ro-
tation classiﬁer is to predict the rotation applied to an
input—each input is fed in four times per batch, rotated
by {0◦,90◦,180◦,270◦}. The classiﬁer should output one-
hot encodings {OH(0;4),OH(1;4),OH(2;4),OH(3;4)}, re-
spectively, for these rotated images. Then, the rotation loss is
written:
LR(X; fθ) =
1
4N
N
∑
i=0
r
∑
j=1
H( fθ(R j(xi)), j)
Results. We present results in Table 2. For instance, the adver-
sary is able to improve the accuracy of their model by 1.0%
for ResNetv2-50 and 1.9% for ResNet_v2_200 after having
queried the oracle for 10% of the ImageNet data. Recall that
the task has 1,000 labels, making these improvements signiﬁ-
cant. The gains we are able to achieve as an adversary are in
line with progress that has been made by the computer vision
community on the ImageNet benchmark over recent years,
where the research community improved the state-of-the-art
top-1 accuracy by about one percent point per year.1
1https://paperswithcode.com/sota/image-classiﬁcation-on-imagenet
where R j is the jth rotation, H is cross-entropy loss, and fθ is
the model’s probability outputs for the rotation task. Inputs
need not be labeled, hence we compute this loss on unlabeled
data for which the adversary did not query the model. That
is, we train the model on both unlabeled data (with rotation
loss), and labeled data (with standard classiﬁcation loss), and
both contribute towards learning a good representation for all
of the data, including the unlabeled data.
We compare the accuracy of models trained with the rota-
tion loss on data labeled by the oracle and data with ImageNet
labels. Our best performing extracted model, with an accuracy