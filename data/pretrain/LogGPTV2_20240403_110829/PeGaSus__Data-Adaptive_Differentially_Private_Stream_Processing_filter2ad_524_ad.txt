p
ϵ
2
k =2
k =1
(avдk − ci +k )
n(cid:88)
n(cid:88)
(|avдk − avдn| + |avдn − ci +k|)
n(cid:88)
n(cid:88)
|avдk − avдn| +
|avдk − avдn| + dev (C[G]))
n(cid:88)
k =2
k =2
k =2
2
.
2
+ (
+ (
k =2
|avдn − ci +k|)
2
2
p
ϵ
+
+
=
k =2
< (ln n + 1) × 2
≤ (ln n + 1) × 2
2
p
ϵ
≤ (ln n + 1) × 2
2
p
ϵ
≤ (ln n + 1) × 2
2
p
ϵ
Also, we have
|avдk − avдn| = | ci +1 + · · · + ci +k
ci +k − avдn
|
+ · · · +
k
k
− avдn|
(|ci +1 − avдn| + · · · + |ci +k − avдn|)
(|ci +1 − avдn| + · · · + |ci +n − avдn|) =
k
= | ci +1 − avдn
≤
≤
1
k
1
k
1
k
dev (C[G]).
Thus,
E
(cid:13)(cid:13)(cid:13) ˆC[G] − C[G](cid:13)(cid:13)(cid:13)2
≤ (ln n + 1) × 2
< (ln n + 1) × 2
ϵ
2
p
2
p
ϵ
n−1(cid:88)
k =2
+ (
1
k
× dev (C[G]) + dev (C[G]))
2
+ (1 + ln(n − 1))
2
dev (C[G])
2
,
where avдk = ci +1+···+ci +k
k
for k ∈ [1, n].
Therefore, when dev (C[G]) ≤
(cid:13)(cid:13)(cid:13) ˆC[G] − C[G](cid:13)(cid:13)(cid:13)2 ≤ E
E
√2(n−ln n−1)
(cid:13)(cid:13)(cid:13) ˜C[G] − C[G](cid:13)(cid:13)(cid:13)2
(1+ln(n−1))ϵp
, we have
□
Theorem 3.6 implies that when the Grouper finds a group with
large size but small deviation value, using the AverageSmoother can
reduce the error of the estimates. Many realistic data streams are
either sparse or have stable counts, which suggests that smooth-
ing can help reduce error. Although we only theoretically analyze
the AverageSmoother as the Smoother, we empirically compare the
three different post-processing strategies on many real streams (in
Section 6). We find that the Grouper often finds large groups with
low deviation and MedianSmoother consistently generates the most
accurate noisy streams under all settings. Thus, in our algorithms,
we use the MedianSmoother as the default Smoother to compute the
final estimates.
Algorithm 3 Window Sum Smoother (WSS)
Input: ˜Ct = ˜c1, . . . , ˜ct , Pt , w
Output: ˆswt
ˆswt ← 0
1:
2: for each G ∈ Pt such that G ∩ {t − w + 1, . . . , t} (cid:44) ∅ do
3:
4:
5: end for
6: Return ˆswt
ˆc ← median{ ˜ci | i ∈ G}
ˆswt ← ˆswt + ˆc × |G ∩ {t − w + 1, . . . , t}|
4 SUPPORT FOR OTHER QUERIES
The previous section describes how the PeGaSus algorithm (Algo-
rithm 1) can be used to generate ˆC, a differentially private answer
to a given unit counting query C. In this section, we describe how
to adapt the algorithm to answer the other queries described in Sec-
tion 2.2, specifically sliding window queries and event monitoring
queries.
A distinctive feature of our approach is that we use the same
basic PeGaSus algorithm for these queries and change only the
Smoother. This is possible because the answers to these queries can
be derived from C. Our approach uses the noisy counts ˜Ct along
with the group information Pt to do appropriate smoothing for the
specific query. Recall that the Smoother does not take private data
as input and only post-processes the outputs of the differentially
private Perturber and Grouper subroutines. Therefore, we can swap
out Smoother without impacting the privacy guarantee. An added
benefit of this approach is that we can simultaneously support
all three kinds of queries – counting, sliding window, and event
monitors – all using a single privacy budget.
An effective Smoother should be designed in terms of the specific
applications as well as users’ knowledge about the input stream.
For sliding window queries, we propose a new post-processing
strategy called Window Sum Smoother (WSS), which is shown in
Algorithm 3. At time t, for every ct′ contained in the sliding window
(t′ ∈ [t − w + 1, t]), we use the MedianSmoother to compute an
estimate ˆct′ for the count at time t′. To compute the answer to the
sliding window query, we simply sum up the counts within the
window w.
Note that this is subtly different from the approach described in
Section 3 because the estimate ˆct′ for some t′ in the sliding window
is based on its group G which may include counts received after
t′ and up to time t. Thus, Window Sum Smoother may provide a
better estimate than just using MedianSmoother as described in
Section 3.3. We make this comparison empirically in Section 6 and
we also compare against the state of the art technique for sliding
window queries [2].
For event monitoring queries, the Smoother depends on the par-
ticular event monitor. For detecting jumps or drops, since we need
the count at time t and the count received at time t − w + 1, we
simply use the MedianSmoother. Actually, we may do better by
also smoothing ˜ct−w +1 using its group G as defined at time t. For
detecting low signal points, we need a sliding window query at
each timestamp. Thus we use the Window Sum Smoother. Once the
counts have been appropriately smoothed, we pass the estimated
counts to the event monitoring functions B and f to generate an
event stream (as described in Section 2.2).
5 HIERARCHICAL STREAMS
In this section, we describe an extension to PeGaSus to support
queries on multiple target states as well as aggregations of states.
Recall from Section 2.2.2 that the analyst can request queries on
a set of states {s1, . . . , sm} ⊆ S and can also ask queries about
aggregations of states. We focus in particular on the setting in
which the analyst has specified a hierarchy of aggregations AGG
and a query on each aдд ∈ AGG.
For ease of presentation, we describe our approach assuming that
the analyst has requested a unit counting query on each aдд ∈ AGG.
However, our approach extends easily to sliding windows and event
monitoring using an extension analogous to what was described in
Section 4.
5.1 Hierarchical-Stream PeGaSus
First, we observe that we can answer the queries over hierarchical
aggregations by simply treating each aggregation as a separate
stream and running any single-stream algorithm on each input
stream. Therefore, our first solution is to run PeGaSus on each
stream, an algorithm we refer to as Hierarchical-Stream PeGaSus
(HS-PGS). Formally, given a set of aggregations AGG and a corre-
sponding set of input streams C (aдд) = c1 (aдд), c2 (aдд), . . . , for
each aдд ∈ AGG, the algorithm HS-PGS executes PeGaSus (C (aдд), ϵ
h )
for each aдд ∈ AGG, where h is the height of AGG.
Theorem 5.1. Hierarchical-Stream PeGaSus satisfies ϵ-differential
privacy.
Proof. The proof follows from (a) the privacy guarantee of Pe-
GaSus (Theorem 3.1), (b) parallel composition of differential privacy
across each level of the hierarchy, and (c) the sequential composition
of differential privacy for each of the h hierarchy levels.
□
5.2 Hierarchical-Stream PeGaSus With
Pruning
We next describe an enhancement of Hierarchical-Stream PeGaSus
that is designed to lower error when many of the input streams in
the aggregation hierarchy are sparse.
The hierarchy implies a monotonicity constraint on the counts
in the streams: the counts cannot decrease as one moves up the
hierarchy. More formally, for any aдд1, aдд2 ∈ AGG such that
aдд1 ⊂ aдд2, then for every time step t in streams C (aдд1) and
C (aдд2), it must be that ct (aдд1) ≤ ct (aдд2). Therefore, if the ag-
gregated stream C (aдд2) is observed to have a “small” count at time
t, all the aggregated streams C (aдд1) will also have small counts
at time t if aдд1 ⊂ aдд2. In such cases, it will be wise to prune
the count ct (aдд1) to be zero rather than consume privacy budget
trying to estimate a smaller count. Pruning small counts is also
beneficial because the privacy budget that would have been spent
on these counts can be saved and spent on non-pruned counts.
We use this pruning idea to modify our approach as follows.
At each time step, the hierarchy is traversed and streams with
small counts at this time step are pruned; any streams that remain
unpruned are fed into the usual PeGaSus algorithm. Note that
pruning only affects the current time step; a pruned stream at time
t may become unpruned at time t + 1.
Algorithm 4 presents our solution, which is called Hierarchical-
Stream PeGaSus with Pruning. The function Prune (lines 1-17) de-
scribes a differentially private algorithm for pruning the hierarchy.
This function is based on the idea of the Sparse Vector Technique [13].
Essentially, it checks each aggregation aдд ∈ AGG from level 1 to
level h. If the current aдд has been pruned, all its children are
automatically pruned. Otherwise, we compare this aggregation’s
current count, ct (aдд), against a user-specified threshold β (line
8). If the count is below threshold, it prunes all the children of aдд.
Further, the privacy budget that would have been spent on the
descendants is saved (line 10). To ensure privacy, Laplace noise is
added to both the count ct (aдд) and the threshold β.
The Hierarchical-Stream PeGaSus with Pruning algorithm itself
is described on lines 18-29. At each time step, it calls Prune. Then,
for each aggregation, if it has been pruned, it simply outputs a count
of 0 (line 21). Otherwise, it applies the PeGaSus algorithm to the
stream (lines 23-25) where the privacy budget passed to Perturber
and Grouper is adjusted based on what has been pruned and the
height of the tree.
In our implementation, there is a small modification to Grouper
in that we skip over past time steps that were pruned and therefore
consider potentially non-contiguous groups.
To prove the privacy guarantee of Algorithm 4, we analyze the
Prune function.
Theorem 5.2. The Prune function in Algorithm 4 satisfies ϵ-
differential privacy.
Proof. For any two neighboring source streaming datasets, they
derive neighboring multiple streams that only differ by 1 at one
timestamp on one single stream. In terms of AGG, there is at most
one list of aggregations at different levels that cover this single
stream. Function Prune can be treated as an implementation of the
Sparse Vector Technique from [13], which ensures ϵ-differential
privacy.
□