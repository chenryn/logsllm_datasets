errors.  The  MEBF  can  be  easily  evaluated  dividing  the 
MTBF by the execution time of the code, as stated in Eq. 2. 
MEBF
Code
MTBF
Code
Execution
Time
(2)
Code
  It  is  worth  noting  that  the  cross  section  and  MTBF  are 
inversely proportional: a higher cross section will result in a 
shorter  MTBF  as  the  flux  is  normally  constant.  On  the 
contrary, the relationship between cross section and MEBF 
is  not  straightforward  and  depends  on  both  the  employed 
resources reliability and on how efficiently those resources 
are used to achieve the code result. 
Finally,  the  Mean  Workload Between  Failures  (MWBF) 
metric  is  defined  through  Eq.  3  to  evaluate  the  amount  of 
data computed correctly by the GPU before experiencing an 
output error. 
MWBF
Code
MEBF
Code

workload
(3)
Code
The MWBF depends on the used resources reliability and 
the  resource  efficiency  like  the  MEBF  but  also  on  the 
throughput gain the resources bring to the code. With regards 
to the codes analyzed in this paper, the workload is unitary 
for each thread (one double data is updated by each thread), 
thus,  it  is  sufficient  to  multiply  the  MEBF  of  a  given 
benchmark  code  by  the  number  of  instantiated  blocks  and 
threads per block (reported in Tab. I and Tab. II). 
The last four rows of Tab. VII and Tab. VIII report the 
MEBF and MWBF for the tested codes. The statistical error 
is dominated by the cross section one. From Fig. 6 and Fig. 7 
it is clear that both increasing the grid size or the block size 
has  the  countermeasure  of  reducing  the  code  reliability,  as 
the  workload  elaborated  before  having  an  output  error  is 
lowered. While increasing the grid size drastically reduce the 
461
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
SUMS AND MULTS TESTED THREADS DISTRIBUTION 
THREADS DISTRIBUTIONS EXECUTION TIMES AND CROSS SECTIONS 
TABLE X 
sums 
exec. time 
[ms] 
6.22 
10.28 
12.86 
90.78 
3380.12 
σ [10-6cm2] 
3.86±5% 
1.35±5% 
1.07±6% 
0.75±6% 
1.35±10% 
mults 
exec. time 
[ms] 
6.21 
10.28 
12.88 
89.78 
3391.28 
σ [10-6cm2] 
4.12±6% 
0.91±6% 
0.55±7% 
0.29±8% 
1.03±11% 
mults
sums
distribution 
P 
B512G15 
B256G15 
B32G15 
B1G15 
5.0
4.5
]
2
m
c
6
-
0
1
[
n
o
i
t
c
e
S
s
s
o
r
C
4.0
3.5
3.0
2.5
2.0
1.5
1.0
TABLE IX 
Threads 
number 
15360 
7680 
3840 
480 
15 
Thread 
complexity 
Workload 
Block 
size 
1024 
512 
256 
32 
1 
Grid 
size 
15 
15 
15 
15 
15 
1 
2 
4 
32 
1024 
P 
B512G15 
B256G15 
B32G15 
B1G15 
153600000 
153600000 
153600000 
153600000 
153600000 
code  MWBF,  increasing  the  block  size  barely  affect  the 
MWBF.  On  applications  in  which  execution  time  is  the 
major  concern,  like  HPC  ones,  bigger  blocks  should  be 
preferred,  as  they  offers  increased  performances  with  little 
increment in the neutron-induced error rate. 
V.  DOP IMPACT ON GPU CROSS SECTION 
The  analysis  proposed  so  far  demonstrates  that  the 
scheduler  is  a  critical  resource  for  the  GPU  as  it  increases 
significantly  the  device  cross  section  and  reduces  the 
workload correctly computed by the application. 
The  discussion  can  be  further  detailed  studying  how 
variations  in  the  Degree  Of  Parallelism  (DOP)  of  a  code 
affect  the  GPU  reliability.  Reducing  the  code  DOP  will 
surely reduce the scheduling required for computation, as a 
fewer  blocks  or  warps  have  to  be  dispatched.  However,  to 
maintain the  workload  constant,  each thread  has to  execute 
more  operations  and  will  require  a  higher  amount  of 
resources, which modifies the GPU parallelism management. 
A.  Sums and Mults Benchmarks 
The  two  benchmark  codes  presented  in  the  previous 
section were tested also to evaluate the DOP effects on the 
GPU  reliability.  The  fully  parallelized  versions  of  the 
benchmarks,  named  sums_P  and  mults_P  are  composed  of 
15  blocks  of  1024  threads,  each  of  which  is  in  charge  of 
executing  10,000  sums  or  multiplications  on  a  dedicated 
memory  location.  We  choose  to  divide  the  threads  in  15 
blocks to avoid exceeding blocks scheduling which has been 
demonstrated to be less efficient than warps scheduling. 
Starting from the fully parallelized version, we lower the 
benchmark  codes  DOP  decreasing  the  number  of  threads 
while increasing the number of operations each thread has to 
execute,  so  to  maintain  the  code  throughput  constant.  We 
reduce the number of threads reducing the block size while 
keeping the grid size constant.  
Tab.  IX  lists  the  threads  distribution  used  in  the 
experiments,  named  again  BxGy,  where  x  is  the  block  size 
and y is the grid size. For each distribution the block size, the 
grid size, and the overall number of instantiated threads are 
reported. The fourth column of Tab. IX lists the normalized 
thread complexity, which indicates the number of operations 
a  thread  performs  in  a  given  distribution  to  complete  its 
assigned tasks, normalized to the ones of a thread in the fully  
parallelized  algorithm.  A  thread  of  complexity  2,  for 
instance,  is  requested  to  perform  twice  the  workload  of  a 
thread in the P version of the algorithm, doubling the number 
of internal registers and executing double the operations. 
As  stated  in  the  last  column  of  Tab.  IX,  we  reduce  the 
0.5
0
P
B512G15 B256G15 B32G15
Figure  8:  Cross  sections  of  sums  and  mults  when  implemented  with 
different DOP and different threads distributions. The workload is kept 
constant  and  the  execution  time  is  normalized  when  evaluating  the 
cross section. 
B1G15
number  of  threads  and  increase  each  thread  complexity  in 
such  a  way  that  the  workload  of  the  algorithm  (calculated 
multiplying  the  number  of  threads  by  the  number  of 
operations each thread has to execute) remains constant.  
Tab. X reports the execution times of the tested versions 
of  the  sums  and  mults.  As  expected,  the  fully  parallelized 
version has better performances than the other ones (see 2nd 
and  4th  columns  of  Tab.  X).  The  execution  time  then 
increases  as  the  DOP  of  the  algorithm  is  reduced.  In  the 
worst  case  of  B1G15  the  execution  time  is  more  than  540 
times higher than the one of the fully parallel version. 
Tab. X also reports, in the 3rd and 5th columns, the cross 
sections  experimentally  measured  in  ISIS,  Didcot,  UK.  As 
depicted  in  Fig.  8,  P,  the  fully  parallelized  version  of  the 
algorithm, experiences the higher cross section both for sums 
and  mults.  The  resilience  of  the  code  increases  as  the 
algorithm  DOP  is  reduced.  The  additional  performance 
gained  increasing  the  DOP  of  the  algorithm,  then,  has  the 
countermeasure of reducing the resilience of the GPU. 
increased.  However, 
It  is  worth  noting  that  the  workload  is  kept  constant 
increasing the threads complexity  while reducing the DOP. 
When a thread complexity is increased, the internal registers 
and computing resources it requires as well as the exposure 
time  are 
thread 
complexity  allows  a  reduction  of  the  amount  of  parallel 
threads.  As  said  in  the  previous  section,  in  the  designed 
benchmark codes the overall amount of employed computing 
is  directly 
resources  and  exposed 
proportional  with 
is 
constant in the various distributions. 
the  application  workload,  which 
increasing 
registers 
internal 
the 
462
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
THREADS DISTRIBUTIONS MTBF AND MEBF 
TABLE XI 
sums 
MTBF 
[104h] 
1.99±5% 
5.66±5% 
7.19±5% 
10.25±5% 
5.70±5% 
MEBF 
[1010exec] 
1.15±6% 
1.98±6% 
2.01±6% 
0.41±7% 
0.006±11% 
mults 
MTBF 
[104h] 
1.88±6% 
8.45±6% 
14.02±7% 
26.51±8% 
7.47±11% 
MEBF 
[1010exec] 
1.09±7% 
2.96±8% 
3.91±8% 
1.06±9% 
0.008±12% 
mults
sums
distribution 
P 
B512G15 
B256G15 
B32G15 
B1G15 
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
]
s
n
o
i
t
u
c
e
x
e
0
1
0
1
[
F
B
E
M
0
P
B512G15
B256G15 B32G15
Figure 9: Mean Executions Between Failures of sums and mults when 
implemented with different DOP and different threads distributions. 
B1G15
Results are presented in the output only when the thread 
has  completed  all the  assigned tasks.  The exposure time of 
registers may then be higher in the code with lower DOP. As 
the  execution  time  is  normalized  in  the  cross  section 
evaluation,  one  may  think  that  the  sensitivity  should  be 
comparable  for  all  distributions,  in  apparent  contrast  with 
experimental results. 
The trend shown in Fig. 8 is consistent with the analysis 
performed in the previous section: decreasing the number of 