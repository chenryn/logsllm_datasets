2k2
− t
2
= t2
2k
.
2
Adding the t dollars spent by X, the total number of dollars spent
2 . Thus the fraction of the total spent by X, which
≥ /2,
is at least t2
we called , is at most 2/(t/k + 1). It follows that k/t ≥ 
2k
2−
i.e., X receives at least an /2 fraction of the service.
+ t
Observe that this analysis holds for each good client separately.
It follows that if the good clients deliver in aggregate an α fraction
of the bandwidth, then in aggregate they will receive an α/2 frac-
tion of the service. Note that this claim remains true regardless of
the service rate c, which need not be known to carry out the auction.
Theory versus practice. We now consider ways in which the
above theorem is both weaker and stronger than what we expect
to see in practice. We begin with weaknesses. First, consider the
unreasonable assumption that requests are served with perfect reg-
ularity. The theorem can be trivially extended: for service times
that ﬂuctuate within a bounded range [(1 − δ)/c, (1 + δ)/c] (as in
our implementation; see §6), X receives at least a (1 − 2δ)/2 frac-
tion of the service. However, even this looser restriction may be
unrealistic in practice. And pathological service timings violate the
theorem. For example, if many request fulﬁllments are bunched in
a tiny interval during which X has not yet paid much, bad clients
can cheaply outbid it during this interval, if they know that the
pathology is happening and are able to time their bids. But doing
so requires implausibly deep information.
Second, the theorem assumes that a good client “pays bytes”
at a constant rate given by its bandwidth. However, the payment
channel in our implementation runs over TCP, and TCP’s slow start
means that a good client’s rate must grow. Moreover, because we
implement the payment channel as a series of large HTTP POSTs
(see §6), there is a quiescent period between POSTs (equal to two
RTTs between client and thinner) as well as TCP’s slow start for
each POST. Nevertheless, we can extend the analysis to capture this
behavior and again derive a lower bound for the fraction of service
that a given good client receives. The result is that if the good client
has a small fraction of the total bandwidth (causing it to spend a lot
of time paying), and if the HTTP POST is big compared to the
bandwidth-delay product, then the client’s fraction of service is not
noticeably affected (because the quiescent periods are negligible
relative to the time spent paying at full rate).
We now consider the strength of the theorem: it makes no as-
sumptions at all about adversarial behavior. We believe that in prac-
tice adversaries will attack the auction by opening many concurrent
TCP connections to avoid quiescent periods, but the theorem han-
dles every other case too. The adversary can open few or many TCP
connections, disregard TCP semantics, or send continuously or in
bursts. The only parameter in the theorem is the total number of
bytes sent (in a given interval) by other clients.
The theorem does cede the adversary an extra factor of two “ad-
vantage” in bandwidth (the good client sees only /2 service for
 bandwidth). This advantage arises because the proof lets the ad-
versary control exactly when its bytes arrive—sending fewer when
the good client’s bid is small and more as the bid grows. This abil-
ity is powerful indeed—most likely stronger than real adversaries
have. Nevertheless, even with this highly pessimistic assumption
about adversarial abilities, speak-up can still do its job: the re-
quired provisioning has only increased by a factor of 2 over the
ideal from §3.1—and this required provisioning is still far less than
would be required to absorb the attack without speak-up.
In §7.4, we quantify the adversarial advantage in our experi-
ments by determining how the factors mentioned in this section—
quiescent periods for good clients, bad clients opening concurrent
connections, etc.—affect the required provisioning above the ideal.
4 REVISITING ASSUMPTIONS
We have so far made a number of assumptions. Below we address
four of them in turn: that aside from end-hosts’ access links, the In-
ternet has inﬁnite capacity; that no bottleneck link is shared (which
is a special case of the previous assumption, but we address it sep-
arately); that the thinner has inﬁnite capacity; and that bad clients
consume all of their upload bandwidth when they attack. In the next
section, we relax the assumption of equal server requests.
4.1 Speak-up’s Effect on the Network
No ﬂow between a client and a thinner individually exhibits anti-
social behavior. In our implementation, each payment channel com-
prises a series of HTTP POSTs (see §6) and thus inherits TCP’s
congestion control. (For UDP applications, the payment channel
could use the congestion manager [3] or DCCP [22].) However,
such individual courtesies do not automatically excuse the larger
rudeness of increased trafﬁc levels, and we must ask whether the
network can handle this increase.
We give two sketchy arguments suggesting that speak-up would
not much increase total trafﬁc and then consider the effect of such
increases. First, speak-up inﬂates upload bandwidth, and, despite
the popularity of peer-to-peer ﬁle-sharing, most bytes still ﬂow in
the download direction [15]. Thus, inﬂating upload trafﬁc even to
the level of download trafﬁc would cause an inﬂation factor of at
most two. Second, only a very small fraction of Internet servers is
attacked at any one time. Thus, even if speak-up did increase the
trafﬁc to each attacked site by an order of magnitude, the increase
in overall Internet trafﬁc would still be small.
Whatever the overall trafﬁc increase, it is unlikely to be problem-
atic for the Internet “core”: both anecdotes from network operators
and measurements [15] suggest that these links operate at low uti-
lization. And, while the core cannot handle every client transmitting
maximally (as argued in [46]), we expect that the fraction of clients
doing so at any time will be small—again, because few sites will be
attacked at any time. Speak-up will, however, create contention at
bottleneck links, an effect that we explore experimentally in §7.7.
4.2 Shared Links
We now consider what happens when clients that share a bottleneck
link are simultaneously encouraged by the thinner. For simplicity,
assume two clients behind bottleneck link l; the discussion gener-
alizes to more clients. If the clients are both good, their individual
ﬂows roughly share l, so they get roughly the same piece of the
server. Each may be disadvantaged compared to clients that are not
similarly bottlenecked, but neither is disadvantaged relative to the
other. If, however, one of the clients is bad, then the good client
has a problem: the bad client can open n parallel TCP connections
(§3.4), claim roughly an n/(n + 1) fraction of l’s bandwidth, and get
a much larger piece of the server. While this outcome is unfortunate
for the good client, observe, ﬁrst, that the server is still protected
(the bad client can “spend” at most l). Second, while the thinner’s
encouragement might instigate the bad client, the fact is that when
a good and bad client share a bottleneck link—speak-up or no—
the good client loses: the bad client can always deny service to the
good client. We experimentally investigate such sharing in §7.6.
4.3 Provisioning the Thinner
For speak-up to work, the thinner must be uncongested: a congested
thinner could not “get the word out” to encourage clients. Thus, the
thinner needs enough bandwidth to absorb a full DDoS attack and
more (which is condition C1 in §2.2). It also needs enough process-
ing capacity to handle the dummy bytes. (Meeting this requirement
is far easier than provisioning the server to handle the full attack
because the thinner does not do much per-request processing.) We
now argue that meeting these requirements is plausible.
One study of observed DoS attacks found that the 95th percentile
of attack size is in the low hundreds of Mbits/s [38], which agrees
with other anecdotes (e.g., [45]). The trafﬁc from speak-up would
presumably be multiples larger since the good clients would also
send at high rates. However, even with several Gbits/s of trafﬁc in
an attack, the thinner’s requirements are not insurmountable.
First, providers readily offer links, even temporarily (e.g., [33]),
that accommodate these speeds. Such bandwidth is expensive, but
co-located servers could share a thinner, or else the ISP could pro-
vide the thinner as a service (see condition C1 in §2.2). Second,
we consider processing capacity. Our unoptimized software thin-
ner running on commodity hardware can handle 1.5 Gbits/s of traf-
ﬁc and tens or even hundreds of thousands of concurrent clients;
see §7.1. A production solution would presumably do much better.
4.4 Attackers’ Constraints
The assumption that bad clients are today “maxing out” their up-
load bandwidth was made for ease of exposition. The required
assumption is only that bad clients consistently make requests at
higher rates than legitimate clients. Speciﬁcally, if bad clients are
limited by their download bandwidth, or they are not maxed out at
all today, speak-up is still useful: it makes upload bandwidth into
a constraint by forcing everyone to spend this resource. Since bad
clients—even those that aren’t maxed out—are more active than
good ones, the imposition of this upload bandwidth constraint af-
fects the bad clients more, again changing the mix of the server that
goes to the good clients. Our goals and analysis in §3 still hold: they
are in terms of the bandwidth available to both populations, not the
bandwidth that they actually use today.
5 HETEROGENEOUS REQUESTS
We now generalize the design to handle the more realistic case
when the requests are unequal. We make the worst-case assump-
tion that the thinner does not know their difﬁculty in advance but
attackers do, as given by the threat model in §2.2. If the thinner
treated all requests equally (charging, in effect, the average price
for any request), an attacker could get a disproportionate share of
the server by sending only the hardest requests.
In describing the generalization to the design, we make two as-
sumptions:
(cid:129) As in the homogeneous case, the server processes only one re-
quest at a time. Thus, the “hardness” of a computation is mea-
sured by how long it takes to complete. Relaxing this assump-
tion to account for more complicated servers is not difﬁcult, as
long as the server implements processor sharing among concur-
rent requests, but we don’t delve into those details here.
(cid:129) The server exports an interface that allows the thinner to SUS-
PEND, RESUME, and ABORT requests. (Many transaction
managers and application servers support such an interface.)
At a high level, the solution is for the thinner to break time into
quanta, to see each request as comprising equal-sized chunks that
consume a quantum of the server’s attention, and to hold a virtual
auction for each quantum. Thus, if a client’s request is made of x
chunks, the client must win x auctions for its request to be fully
served. The thinner need not know x in advance for any request.
In more detail: rather than terminate the payment channel once
the client’s request is admitted (as in §3.3), the thinner extracts
an on-going payment until the request completes. Given these on-
going payments, the thinner implements the following procedure
every τ seconds (τ is the quantum length):
1. Let v be the currently-active request. Let u be the contending
request that has paid the most.
2. If u has paid more than v, then SUSPEND v, admit (or RE-
SUME) u, and set u’s payment to zero.
3. If v has paid more than u, then let v continue executing but set v’s
payment to zero (since v has not yet paid for the next quantum).
4. Time-out and ABORT any request that has been SUSPENDed
for some period (e.g., 30 seconds).
This scheme requires some cooperation from the server. First, the
server should not SUSPEND requests that hold critical locks; do-
ing so could cause deadlock. Second, SUSPEND, RESUME, and
ABORT should have low overhead.
IMPLEMENTATION
6
We implemented a prototype thinner in C++ as an OKWS [23] Web
service using the SFS toolkit [26]. It runs on Linux 2.6, exporting a
well-known URL. When a Web client requests this URL, the thin-
ner decides if, and when, to send this request to the server, using
the method in §3.3. The server is currently emulated, running in
the same address space as the thinner. The server “processes” re-
quests with a “service time” selected uniformly at random from
[.9/c, 1.1/c]. When the server responds to a request, the thinner
returns HTML to the client with that response. Any JavaScript-
capable Web browser can use our system; we have successfully
tested our implementation with Firefox, Internet Explorer, Safari,
and a custom client that we use in our experiments.
Whenever the emulated server is not free, the thinner returns
JavaScript to the Web client that causes it to automatically issue
two HTTP requests: (1) the actual request to the server, and (2)
a one-megabyte HTTP POST that is dynamically constructed by
the browser and that holds dummy data (one megabyte reﬂecting
some browsers’ limits on POSTs). The thinner delays responding
to the ﬁrst HTTP request (because the response to that request has
to come from the server, which is busy). The second HTTP request
is the payment channel. If, while sending these dummy bytes, the
client wins the auction, the thinner terminates request (2) and gives
request (1) to the server. If, on the other hand, request (2) com-
pletes, the client has not yet received service; in this case, the thin-
ner returns JavaScript that causes the browser to send another large
POST, and the process continues. The thinner correlates the client’s
payments with its request via an “id” ﬁeld in both HTTP requests.
One can conﬁgure the thinner to support hundreds of thousands
of concurrent connections by setting the maximum number of con-
nection descriptors appropriately. (The thinner evicts old clients as
these descriptors deplete.) With modern versions of Linux, the limit
on concurrent clients is not per-connection descriptors but rather
the RAM consumed by each open connection.
7 EXPERIMENTAL EVALUATION
To investigate the effectiveness and performance of speak-up, we
conducted experiments with our prototype thinner. Our primary
question is how the thinner allocates an attacked server to good
clients. To answer this question, we begin in §7.2 by varying the
bandwidth of good (G) and bad (B) clients, and measuring how the
server is allocated with and without speak-up. We also measure this
allocation with server capacities above and below the ideal in §3.1.
In §7.3, we measure speak-up’s latency and byte cost. In §7.4, we
ask how much bad clients can “cheat” speak-up to get more than a
Our thinner implementation allocates the emulated
server in rough proportion to clients’ bandwidths.
In our experiments, the server needs to provision
only 15% beyond the bandwidth-proportional ideal
to serve all good requests.
Our unoptimized thinner implementation can sink
1.5 Gbits/s of uploaded “payment trafﬁc”.
On a bottleneck link, speak-up trafﬁc can crowd
out other speak-up trafﬁc and non-speak-up trafﬁc.
§7.2, §7.5
§7.3, §7.4
§7.1
§7.6, §7.7
Table 1: Summary of main evaluation results.
bandwidth-proportional share of the server. §7.5 shows how speak-
up performs when clients have differing bandwidths and latencies
to the thinner. We also explore scenarios in which speak-up trafﬁc
shares a bottleneck link with other speak-up trafﬁc (§7.6) and with
non-speak-up trafﬁc (§7.7). Table 1 summarizes our results.
7.1 Setup and Method
All of
the experiments described here ran on the Emulab
testbed [13]. The clients run a custom Python Web client and con-
nect to the prototype thinner in various emulated topologies. The
thinner runs on Emulab’s “PC 3000”, which has a 3 GHz Xeon
processor and 2 GBytes of RAM; the clients are allowed to run on
any of Emulab’s hardware classes.
All experiments run for 600 seconds. Each client runs on a sep-