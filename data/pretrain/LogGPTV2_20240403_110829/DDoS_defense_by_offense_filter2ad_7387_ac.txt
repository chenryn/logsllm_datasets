### 2k² - t² = t² / (2k)

Adding the \( t \) dollars spent by \( X \), the total number of dollars spent is \( 2k^2 \). Thus, the fraction of the total spent by \( X \), which we call \( \epsilon \), is at least \( t^2 / (2k^2) \). It follows that \( k/t \geq \epsilon \), i.e., \( X \) receives at least an \( \epsilon/2 \) fraction of the service.

Observe that this analysis holds for each good client separately. Therefore, if the good clients deliver in aggregate an \( \alpha \) fraction of the bandwidth, then in aggregate they will receive at least an \( \alpha/2 \) fraction of the service. Note that this claim remains true regardless of the service rate \( c \), which need not be known to carry out the auction.

### Theory versus Practice

We now consider ways in which the above theorem is both weaker and stronger than what we expect to see in practice. We begin with the weaknesses:

1. **Unreasonable Assumption of Perfect Regularity:**
   The theorem assumes that requests are served with perfect regularity. This can be trivially extended: for service times that fluctuate within a bounded range \([(1 - \delta)/c, (1 + \delta)/c]\) (as in our implementation; see §6), \( X \) receives at least a \((1 - 2\delta)\epsilon/2\) fraction of the service. However, even this looser restriction may be unrealistic in practice. Pathological service timings violate the theorem. For example, if many request fulfillments are bunched in a tiny interval during which \( X \) has not yet paid much, bad clients can cheaply outbid it during this interval if they know that the pathology is happening and are able to time their bids. But doing so requires implausibly deep information.

2. **Constant Payment Rate Assumption:**
   The theorem assumes that a good client “pays bytes” at a constant rate given by its bandwidth. In our implementation, the payment channel runs over TCP, and TCP’s slow start means that a good client’s rate must grow. Additionally, because we implement the payment channel as a series of large HTTP POSTs (see §6), there is a quiescent period between POSTs (equal to two RTTs between the client and thinner) as well as TCP’s slow start for each POST. Nevertheless, we can extend the analysis to capture this behavior and derive a lower bound for the fraction of service that a given good client receives. If the good client has a small fraction of the total bandwidth (causing it to spend a lot of time paying), and if the HTTP POST is big compared to the bandwidth-delay product, then the client’s fraction of service is not noticeably affected (because the quiescent periods are negligible relative to the time spent paying at full rate).

### Strength of the Theorem

The theorem makes no assumptions about adversarial behavior. We believe that in practice, adversaries will attack the auction by opening many concurrent TCP connections to avoid quiescent periods, but the theorem handles every other case too. The adversary can open few or many TCP connections, disregard TCP semantics, or send continuously or in bursts. The only parameter in the theorem is the total number of bytes sent (in a given interval) by other clients.

The theorem does cede the adversary an extra factor of two “advantage” in bandwidth (the good client sees only \(\epsilon/2\) service for \(\epsilon\) bandwidth). This advantage arises because the proof lets the adversary control exactly when its bytes arrive—sending fewer when the good client’s bid is small and more as the bid grows. This ability is powerful indeed—most likely stronger than real adversaries have. Nevertheless, even with this highly pessimistic assumption about adversarial abilities, speak-up can still do its job: the required provisioning has only increased by a factor of 2 over the ideal from §3.1—and this required provisioning is still far less than would be required to absorb the attack without speak-up.

In §7.4, we quantify the adversarial advantage in our experiments by determining how the factors mentioned in this section—quiescent periods for good clients, bad clients opening concurrent connections, etc.—affect the required provisioning above the ideal.

### Revisiting Assumptions

We have made several assumptions. Below, we address four of them in turn: that aside from end-hosts’ access links, the Internet has infinite capacity; that no bottleneck link is shared (which is a special case of the previous assumption, but we address it separately); that the thinner has infinite capacity; and that bad clients consume all of their upload bandwidth when they attack. In the next section, we relax the assumption of equal server requests.

#### 4.1 Speak-up’s Effect on the Network

No flow between a client and a thinner individually exhibits antisocial behavior. In our implementation, each payment channel comprises a series of HTTP POSTs (see §6) and thus inherits TCP’s congestion control. (For UDP applications, the payment channel could use the congestion manager [3] or DCCP [22].) However, such individual courtesies do not automatically excuse the larger rudeness of increased traffic levels, and we must ask whether the network can handle this increase.

We give two sketchy arguments suggesting that speak-up would not much increase total traffic and then consider the effect of such increases. First, speak-up inflates upload bandwidth, and, despite the popularity of peer-to-peer file-sharing, most bytes still flow in the download direction [15]. Thus, inflating upload traffic even to the level of download traffic would cause an inflation factor of at most two. Second, only a very small fraction of Internet servers is attacked at any one time. Thus, even if speak-up did increase the traffic to each attacked site by an order of magnitude, the increase in overall Internet traffic would still be small.

Whatever the overall traffic increase, it is unlikely to be problematic for the Internet “core”: both anecdotes from network operators and measurements [15] suggest that these links operate at low utilization. And, while the core cannot handle every client transmitting maximally (as argued in [46]), we expect that the fraction of clients doing so at any time will be small—again, because few sites will be attacked at any time. Speak-up will, however, create contention at bottleneck links, an effect that we explore experimentally in §7.7.

#### 4.2 Shared Links

We now consider what happens when clients that share a bottleneck link are simultaneously encouraged by the thinner. For simplicity, assume two clients behind bottleneck link \( l \); the discussion generalizes to more clients. If the clients are both good, their individual flows roughly share \( l \), so they get roughly the same piece of the server. Each may be disadvantaged compared to clients that are not similarly bottlenecked, but neither is disadvantaged relative to the other. If, however, one of the clients is bad, then the good client has a problem: the bad client can open \( n \) parallel TCP connections (§3.4), claim roughly an \( n/(n + 1) \) fraction of \( l \)'s bandwidth, and get a much larger piece of the server. While this outcome is unfortunate for the good client, observe, first, that the server is still protected (the bad client can “spend” at most \( l \)). Second, while the thinner’s encouragement might instigate the bad client, the fact is that when a good and bad client share a bottleneck link—speak-up or no—the good client loses: the bad client can always deny service to the good client. We experimentally investigate such sharing in §7.6.

#### 4.3 Provisioning the Thinner

For speak-up to work, the thinner must be uncongested: a congested thinner could not “get the word out” to encourage clients. Thus, the thinner needs enough bandwidth to absorb a full DDoS attack and more (which is condition C1 in §2.2). It also needs enough processing capacity to handle the dummy bytes. (Meeting this requirement is far easier than provisioning the server to handle the full attack because the thinner does not do much per-request processing.) We now argue that meeting these requirements is plausible.

One study of observed DoS attacks found that the 95th percentile of attack size is in the low hundreds of Mbits/s [38], which agrees with other anecdotes (e.g., [45]). The traffic from speak-up would presumably be multiples larger since the good clients would also send at high rates. However, even with several Gbits/s of traffic in an attack, the thinner’s requirements are not insurmountable.

First, providers readily offer links, even temporarily (e.g., [33]), that accommodate these speeds. Such bandwidth is expensive, but co-located servers could share a thinner, or else the ISP could provide the thinner as a service (see condition C1 in §2.2). Second, we consider processing capacity. Our unoptimized software thinner running on commodity hardware can handle 1.5 Gbits/s of traffic and tens or even hundreds of thousands of concurrent clients; see §7.1. A production solution would presumably do much better.

#### 4.4 Attackers’ Constraints

The assumption that bad clients are today “maxing out” their upload bandwidth was made for ease of exposition. The required assumption is only that bad clients consistently make requests at higher rates than legitimate clients. Specifically, if bad clients are limited by their download bandwidth, or they are not maxed out at all today, speak-up is still useful: it makes upload bandwidth into a constraint by forcing everyone to spend this resource. Since bad clients—even those that aren’t maxed out—are more active than good ones, the imposition of this upload bandwidth constraint affects the bad clients more, again changing the mix of the server that goes to the good clients. Our goals and analysis in §3 still hold: they are in terms of the bandwidth available to both populations, not the bandwidth that they actually use today.

### Heterogeneous Requests

We now generalize the design to handle the more realistic case when the requests are unequal. We make the worst-case assumption that the thinner does not know their difficulty in advance but attackers do, as given by the threat model in §2.2. If the thinner treated all requests equally (charging, in effect, the average price for any request), an attacker could get a disproportionate share of the server by sending only the hardest requests.

In describing the generalization to the design, we make two assumptions:
1. As in the homogeneous case, the server processes only one request at a time. Thus, the “hardness” of a computation is measured by how long it takes to complete. Relaxing this assumption to account for more complicated servers is not difficult, as long as the server implements processor sharing among concurrent requests, but we don’t delve into those details here.
2. The server exports an interface that allows the thinner to SUSPEND, RESUME, and ABORT requests. (Many transaction managers and application servers support such an interface.)

At a high level, the solution is for the thinner to break time into quanta, to see each request as comprising equal-sized chunks that consume a quantum of the server’s attention, and to hold a virtual auction for each quantum. Thus, if a client’s request is made of \( x \) chunks, the client must win \( x \) auctions for its request to be fully served. The thinner need not know \( x \) in advance for any request.

In more detail: rather than terminate the payment channel once the client’s request is admitted (as in §3.3), the thinner extracts an ongoing payment until the request completes. Given these ongoing payments, the thinner implements the following procedure every \( \tau \) seconds (\( \tau \) is the quantum length):
1. Let \( v \) be the currently-active request. Let \( u \) be the contending request that has paid the most.
2. If \( u \) has paid more than \( v \), then SUSPEND \( v \), admit (or RESUME) \( u \), and set \( u \)'s payment to zero.
3. If \( v \) has paid more than \( u \), then let \( v \) continue executing but set \( v \)'s payment to zero (since \( v \) has not yet paid for the next quantum).
4. Time-out and ABORT any request that has been SUSPENDed for some period (e.g., 30 seconds).

This scheme requires some cooperation from the server. First, the server should not SUSPEND requests that hold critical locks; doing so could cause deadlock. Second, SUSPEND, RESUME, and ABORT should have low overhead.

### Implementation

We implemented a prototype thinner in C++ as an OKWS [23] Web service using the SFS toolkit [26]. It runs on Linux 2.6, exporting a well-known URL. When a Web client requests this URL, the thinner decides if, and when, to send this request to the server, using the method in §3.3. The server is currently emulated, running in the same address space as the thinner. The server “processes” requests with a “service time” selected uniformly at random from \([0.9/c, 1.1/c]\). When the server responds to a request, the thinner returns HTML to the client with that response. Any JavaScript-capable Web browser can use our system; we have successfully tested our implementation with Firefox, Internet Explorer, Safari, and a custom client that we use in our experiments.

Whenever the emulated server is not free, the thinner returns JavaScript to the Web client that causes it to automatically issue two HTTP requests: (1) the actual request to the server, and (2) a one-megabyte HTTP POST that is dynamically constructed by the browser and that holds dummy data (one megabyte reflecting some browsers’ limits on POSTs). The thinner delays responding to the first HTTP request (because the response to that request has to come from the server, which is busy). The second HTTP request is the payment channel. If, while sending these dummy bytes, the client wins the auction, the thinner terminates request (2) and gives request (1) to the server. If, on the other hand, request (2) completes, the client has not yet received service; in this case, the thinner returns JavaScript that causes the browser to send another large POST, and the process continues. The thinner correlates the client’s payments with its request via an “id” field in both HTTP requests.

One can configure the thinner to support hundreds of thousands of concurrent connections by setting the maximum number of connection descriptors appropriately. (The thinner evicts old clients as these descriptors deplete.) With modern versions of Linux, the limit on concurrent clients is not per-connection descriptors but rather the RAM consumed by each open connection.

### Experimental Evaluation

To investigate the effectiveness and performance of speak-up, we conducted experiments with our prototype thinner. Our primary question is how the thinner allocates an attacked server to good clients. To answer this question, we begin in §7.2 by varying the bandwidth of good (G) and bad (B) clients, and measuring how the server is allocated with and without speak-up. We also measure this allocation with server capacities above and below the ideal in §3.1. In §7.3, we measure speak-up’s latency and byte cost. In §7.4, we ask how much bad clients can “cheat” speak-up to get more than a bandwidth-proportional share of the server. §7.5 shows how speak-up performs when clients have differing bandwidths and latencies to the thinner. We also explore scenarios in which speak-up traffic shares a bottleneck link with other speak-up traffic (§7.6) and with non-speak-up traffic (§7.7). Table 1 summarizes our results.

#### 7.1 Setup and Method

All of the experiments described here ran on the Emulab testbed [13]. The clients run a custom Python Web client and connect to the prototype thinner in various emulated topologies. The thinner runs on Emulab’s “PC 3000”, which has a 3 GHz Xeon processor and 2 GBytes of RAM; the clients are allowed to run on any of Emulab’s hardware classes.

All experiments run for 600 seconds. Each client runs on a separate machine.