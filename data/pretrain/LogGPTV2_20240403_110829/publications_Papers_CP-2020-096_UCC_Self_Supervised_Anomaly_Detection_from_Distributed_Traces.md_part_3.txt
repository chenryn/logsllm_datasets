investigatehowtheattentionscoresdifferbetweenthenormal Logs
Faults TM re at cr eic ss
and anomalous traces and how this can be utilized to infer
interestingpatternsbetweenthenormalandanomaloustraces. Fig.3. Illustrationoftheinfrastructurefromwherethedatawasgenerated.
345
Authorized licensed use limited to: Technische Universitaet Berlin. Downloaded on September 06,2021 at 12:04:10 UTC from IEEE Xplore. Restrictions apply.
To test and compare the performance and robustness of the max len the value of 90 is set since it covers the traces of
proposed attention method and the state of the art in anomaly all the lengths within the data.
detection from tracing data, we consider three learning sce-
C. Predictive Performance
narios. To test our hypothesis of the ability of our method
to preserve the global and local properties within the traces In the following, the results from the three learning scenar-
we grouped our experiments into three groups: short, long, iosarepresented.Wepresenttheprecision,recallandF1score.
and combined (short and long) traces. The definition of this Forthepositiveclass,weconsidertheanomaloustraces.Thus,
categoriesisdata-driven,meaningthat fortheshorttracesare the true positives are defined as a correct prediction of true
considered the once that are concentrated around the lower anomalies.Truenegativesaredefinedasthecorrectprediction
valuesofthemedian(≤14),whiletheremainingfallintothe ofnormaltraces.Falsepositivesarepredictedanomalieswhen
category of long traces. the trace is normal, while the false negatives are traces that
Real anomalies (LS1). In this learning scenario, in the are predicted as normal when their ground truth label is an
test set, the anomalies as generated by the deployed testbed anomaly.
were used. The goal of this scenario is to inspect how well
the methods for anomaly detection from tracing data perform
under the presence of anomalies as generated by the system.
We tested both of the methods on 9 different parameter
configurations. We select the best parameter configuration in
this scenario and used it to test its generalizations of both of
the approaches in the remaining of the experiments.
Artificial anomalies (LS2). Observation of the anomalies
as generated by the system, usually result in a shorten trace.
Restarting a service results in interrupting the trace at par-
ticular span. The anomalies can be injected at every span in
the trace since every span represents a service that can be
malfunctioning. To test the generalization and the robustness
of how the methods scale to a different type of anomaly a
Fig.4. ResultsfromtheexperimentsforLS1.
set of artificial anomaly test is created. The creation of this
set is done in a way that L traces from the normal test set
are selected at random. The selected traces are truncated at
a random position. These traces are labeled as an anomalous
and are joint with the remaining normal traces to create the
test set.
For the LS2 scenario, the best-selected method from the
optimization procedure as in LS1 is selected. This allows to
directly test the change of the performance of the methods to
novel anomalies.
B. Implementation Details
The anomaly detection methods were implemented in
Python using Pytorch [21]. The experiments were collected
on a personal computer with GPU-NVIDIA GTX 1660. To
Fig.5. ResultsfromtheexperimentsforLS2.
obtain the best models the number of training epochs and
the batch sizes were varied in the following range batch Fig. 4 depicts the results of LS1 for the best-selected
size= 16,64,256 and epochs= 10,25,50, accordingly. The models from the optimization procedure. When the long
number of the layers for the attention model was set to 1 and traces are considered, one can observe high scores for the
thenumberofrecurrentlayersto2.Thehiddensizesforboth attentionmechanismcomparedtothepreviousstate-of-the-art
models were set to 256. In the anomaly detection phase, the deep learning approaches, the LSTM approach. The attention
threshold was varied from 0 to 1 with a step of 0.05. mechanism can put importance on specific spans of the trace
FortheparametersofDrain,thevaluesforthesimilarityand which it learns to be important for a particular workload due
depth are 0.4 and 4 respectively. For training the model, the to the sparsity property of the attention. Thus the attention
parameter P is set to 70 % of the number of normal traces. mechanism exploits the global properties. Opposed to it, the
The value for window size parameter for LSTM is set to 3 LSTM-based exploits local properties of a trace due to the
since it is the maximal number that covers all of the traces autoregressiveassumption.Theresultsthatbothmethodsshow
in the training set. For the attention method for the parameter forboththeshortandthecombinationofshortandlongtraces
346
Authorized licensed use limited to: Technische Universitaet Berlin. Downloaded on September 06,2021 at 12:04:10 UTC from IEEE Xplore. Restrictions apply.
are comparable. The good comparable performance on the REFERENCES
short traces results from the ability of both of the models to
[1] J.Kaldor,J.Mace,M.Bejda,E.Gao,W.Kuropatwa,J.O’Neill,K.W.
exploit the locality in the traces, which given the smaller size Ong, B. Schaller, P. Shan, B. Viscomi et al., “Canopy: an end-to-end
of the trace does not distinguish from the global properties of performance tracing and analysis system,” in Proceedings of the 26th
SymposiumonOperatingSystemsPrinciples. ACM,2017,pp.34–50.
thetrace.Similarly,theattentionmodelcanexploitthewhole
[2] C. Sridharan, Distributed Systems Observability: A Guide to Building
information existing in the traces. The good performance on RobustSystems. O’ReillyMedia,2018.
the combination of the long short traces is due to the over- [3] A. Gulenko, F. Schmidt, A. Acker, M. Wallschla¨ger, O. Kao, and
F. Liu, “Detecting anomalous behavior of black-box services modeled
representation of the short traces in the test dataset.
withdistance-basedonlineclustering,”in2018IEEE11thInternational
Fig. 5 depicts the results from the LS2 scenario. It is Conference on Cloud Computing. San Francisco, CA, USA: IEEE,
interesting to observe that the results in this scenario are 2018,pp.912–915.
[4] M.Du,F.Li,G.Zheng,andV.Srikumar,“Deeplog:Anomalydetection
inline with the previously discussed, with a notable decrease
anddiagnosisfromsystemlogsthroughdeeplearning,”inProceedings
oftherecallforboththesmallandlongtracesfortheLSTM- of the 2017 ACM SIGSAC Conference on Computer and Communi-
based approach. This means that the autoregressive model is cations Security. New York, NY, USA: Association for Computing
Machinery,2017,p.1285–1298.
predicting more of the anomalies as normal. This behaviour
[5] F.Schmidt,S.-P.F.,A.Gulenko,M.Wallschla¨ger,A.Acker,andO.Kao,
canbeexplainedwiththefactthattheanomaloustracediffers “Unsupervised anomaly event detection for cloud monitoring using
fromthenormaljustinitslength.Theforwardblindnessofthe onlinearima,”in2018IEEE/ACMInternationalConferenceonUtility
andCloudComputingCompanion. Zurich:IEEE,2018,pp.71–76.
autoregressive approaches prohibits to infer information from
[6] W.Meng,Y.Liu,Y.Zhu,S.Zhang,D.Pei,Y.Liu,Y.Chen,R.Zhang,
all of the traces implicitly including the information from the S.Tao,P.Sun,andR.Zhou,“Loganomaly:Unsuperviseddetectionof
length of the trace. Hence, the LSTM method while is still sequentialandquantitativeanomaliesinunstructuredlogs,”inProceed-
ings of the Twenty-Eighth International Joint Conference on Artificial
good in modeling of the sequence of spans is shortsighted
Intelligence, IJCAI-19. International Joint Conferences on Artificial
anditisexpectedtohaveworseresultsespeciallyinscenarios IntelligenceOrganization,2019,pp.4739–4745.
where the size of the traces is much longer. Long traces are [7] S. Nedelkoski, J. Cardoso, and O. Kao, “Anomaly detection and
classification using distributed tracing and deep learning,” in 2019
morecommoninareal-worlddistributedsystem.Executingof
19thIEEE/ACMInternationalSymposiumonCluster,CloudandGrid
oneoperationinamicro-servicearchitectureincludesinvoking Computing. Larnaca,Cyprus:IEEE,2019,pp.241–250.
of multiple services not necessarily just HTTP calls for the [8] ——, “Anomaly detection from system tracing data using multimodal
deeplearning,”in2019IEEE12thInternationalConferenceonCloud
communication between the services. Hence the ability to
Computing. Milan,Italy:IEEE,2019,pp.179–186.
handlelongtracesisimperativeforamethodtobeapplicable [9] Y. Yang, L. Wang, J. Gu, and Y. Li, “Transparently capturing request
in a real world tracing data. As observed by the results, executionpathforanomalydetection,”2020.
[10] V.Chandola,A.Banerjee,andV.Kumar,“Anomalydetection:Asurvey,”
the strongest point of the attention mechanism is that it can
ACMcomputingsurveys,vol.41,2009.
provide good results on long traces since it exploits the [11] S.Zhang,Y.Liu,D.Pei,Y.Chen,X.Qu,S.Tao,andZ.Zang,“Rapid
information of the whole trace at once. Furthermore, this androbustimpactassessmentofsoftwarechangesinlargeinternet-based
services,” in Proceedings of the 11th ACM Conference on Emerging
means that the attention mechanism does not suffer from
NetworkingExperimentsandTechnologies,2015,pp.1–13.
reduced recall as the LSTM method. [12] M. M. Moya, M. W. Koch, and L. D. Hostetler, “One-class classifier
networksfortargetrecognitionapplications,”NASASTI/ReconTechnical
V. CONCLUSION ReportN,vol.93,1993.
[13] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,”Neural
This paper addresses the problem of anomaly detection in computation,vol.9,pp.1735–1780,1997.
large-scaledistributedsystems,asanessentialtaskfortheirse- [14] J.Chung,K.Kastner,L.Dinh,K.Goel,A.C.Courville,andY.Bengio,
“Arecurrentlatentvariablemodelforsequentialdata,”inAdvancesin
curityandreliability.Weaddressedtheproblembyintroducing
neuralinformationprocessingsystems,2015,pp.2980–2988.
anewlearningtask–maskedspanpredictionfortheproblem [15] P. He, J. Zhu, Z. Zheng, and M. Lyu, “Drain: An online log parsing
of execution-path anomaly detection from tracing data. The approachwithfixeddepthtree.” IEEE,062017,pp.33–40.
[16] J.Zhu,S.He,J.Liu,P.He,Q.Xie,Z.Zheng,andM.R.Lyu,“Tools
novel definition of the problem allows to include information
and benchmarks for automated log parsing,” in 2019 IEEE/ACM 41st
from the entire trace, directly exploiting the existing service InternationalConferenceonSoftwareEngineering:SoftwareEngineer-
relations. It results in better predictive performance on the inginPractice(ICSE-SEIP). Quebec,Canada:IEEEPress,2019,pp.
121–130.
problem of structural anomaly detection, especially for the
[17] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
long traces when compared to other existing approaches for Ł.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”inAdvances
tracing data based on LSTM. Empirically, we show that the inneuralinformationprocessingsystems. RedHook,NY,US:Curran
Associates,2017,pp.5998–6008.
proposed approach is more robust to small permutation in the
[18] A.Shrivastwa,S.Sarat,K.Jackson,C.Bunch,E.Sigler,andT.Camp-
normaltraces,ascenariofrequentlyoccurringinpractice.The bell, OpenStack: Building a Cloud Environment. Packt Publishing,
experimentsshowedthatthemethodhashighperformanceon 2016.
[19] (2020) Kolla-ansible’s documentation. [Online]. Available: https:
experimental testbed data .
//docs.openstack.org/kolla-ansible/latest/
Theproposedapproachopensanewpossibilityforanomaly [20] (2020) Rally documentation. [Online]. Available: https://rally.
detection not just from tracing data, but from other sources readthedocs.io/en/latest/
[21] A. Paszke, S. Gross, and et. al., “Pytorch: An imperative style, high-
thathavethenotionofadistributedrepresentationofanevent
performancedeeplearninglibrary,”inAdvancesinNeuralInformation
e.g.,logdata.Webelievethatthemethodwillmotivatefurther ProcessingSystems32. CurranAssociates,Inc.,2019,pp.8024–8035.
group of research in the direction of utilizing the full trace
information for anomaly detection.
347
Authorized licensed use limited to: Technische Universitaet Berlin. Downloaded on September 06,2021 at 12:04:10 UTC from IEEE Xplore. Restrictions apply.