code-similarity algorithm si, its accuracy, denoted by ai, is
deﬁned as ai = v/u, where u is the total number of test-
ing samples and v is the number of testing samples that are
205
correctly classiﬁed by the classiﬁer. For a given threshold
τ , if ai ≥ τ , namely that the classiﬁer is accurate enough,
then si is added to the set S′ of code-similarity algorithms
that will be considered further (Lines 9-15). For a candidate
code-similarity algorithm si ∈ S′, if the classiﬁer treats the
patched piece of code corresponding to Fk ∈ F (2) as invul-
nerable, then si will be considered further. Otherwise, si
is eliminated because it cannot detect the vulnerability cor-
rectly or it is unsuitable for Fk (e.g., a classiﬁer using the
code-fragment level of patch-without-context cannot be ap-
plied to a diﬀ hunk involving no line deletion). This screen-
ing of candidate code-similarity algorithms corresponds to
Lines 16-23.
Second, we identify the most suitable code-fragment level
for a diﬀ hunk Fk. This corresponds to Lines 24-31 in Algo-
rithm 1. For this purpose, we introduce the concept of core
code fragment, which is the piece of code that is directly
related to the vulnerability in question. For example, con-
sider strcpy(dest, src), which is used to copy a string from
one address to another. If the boundaries of src and dest are
not checked, one can encounter a buﬀer overﬂow. In this ex-
ample, the core code fragment only contains the statement
strcpy(dest, src) and the preceding statements that involve
the operation of the arguments dest and src.
In order to search for the most suitable code-fragment
level, we represent an unpatched piece of code via all of
the code-fragment levels mentioned above, namely patch-
without-context, slice, patch-with-context, function, and ﬁle/
component with increasingly coarse granularity. Among these
code-fragment levels, we observe that the slice code-fragment
level can be naturally used to represent the core code frag-
ment of a vulnerability. It would be ideal to ask a human
expert to localize the precise position of a vulnerability seg-
ment, but this approach is too costly. As an alternative, we
automatically treat the lines that are deleted by the patch
as the location of the vulnerability; this approximation is
also used by [24]. Having approximated the location of a
vulnerability, we treat a slice as an approximation of a core
code fragment.
Since an unpatched code fragment at a ﬁner code-fragment
level is contained in an unpatched code fragment at a coarser
code-fragment level, we use the diﬀerence between the num-
bers of lines of code at two diﬀerent code-fragment levels to
indicate the diﬀerence between the two representations of
the same vulnerable piece of code at the two diﬀerent code-
fragment levels. This leads to the degree of approximation
metric, or doa for short, which measures the degree of ap-
proximation between the core code fragment cc correspond-
ing to the representation of an unpatched piece of code at
the slice code-fragment level and the unpatched code frag-
ment cf corresponding to the representation of the same
unpatched piece of code at one of the ﬁve code-fragment
levels mentioned above. In principle, we have
doa(cc, cf ) = |ℓcf − ℓcc|
ℓcc
(1)
where ℓcc and ℓcf denote the number of lines of core code
fragment cc and code fragment cf , respectively. The closer
the doa(cc, cf ) is to 0, then the smaller the diﬀerence is
between cc and cf , and the better the cf is at representing
the vulnerability. Note that as we elaborate below, we can
identify the minimum doa(cc, cf ) without computing ℓcc,
whose actual value cannot be calculated precisely because
of the use of approximate code slice.
Since we use the slice code fragment to represent the core
code fragment, the code-fragment level of the core code is
ﬁner than the code-fragment level of patch-with-context (pc)
but coarser than the fragment level of patch-without-context
(pwc). The code-fragment level of slice sc can be little ﬁner
or coarser than the code-fragment level of the core code be-
cause the lines of code deleted by the patch are considered as
lines of vulnerable code. The code-fragment level of slice sc
is ﬁner than the code-fragment level of patch-with-context
(pc), and coarser than the code-fragment level of patch-
without-context (pwc). Therefore, doa(cc, sc)  γ/β > 0.8, where 0.8 is
the given threshold of code similarity, a function is treated
as a code reuse instance.
After obtaining the code reuse instances, we add the code-
reuse features to the VCID. Since code reuse instances in the
VCID focus on code reuses within functions, the code-reuse
features are Type 2–Type 5 described in Table 1. We use the
gumtree algorithm [11] to obtain the sequence of edit actions
from an unpatched piece of code to its corresponding piece
of code in the code reuse instance. We derive code-reuse
features from the sequence of edit actions.
4.2 Code-similarity algorithm selection
We now discuss the instantiations of Algorithm 1 from
two aspects.
Candidate code-similarity algorithms. Table 3 lists
the candidate code-similarity algorithms, some of which are
variants of the algorithms reviewed in Section 2. Here we
highlight the following issues. First, we exclude the algo-
rithms that operate at the ﬁle/component fragment level,
because they are usually used for vulnerability prediction
and may not be applicable when vulnerabilities do not ap-
pear in a high frequency. Second, for the code-similarity
algorithms that do not utilize the concept of code-fragment
level, we select the code-fragment level with the smallest
doa. Third, we consider the code representation of the
CP-Miner algorithm [19] with diﬀerent mappings of vari-
able name and constants for the sake of comprehensiveness.
The CP-Miner algorithm maps variables with diﬀerent data
types to the same token, and maps all constants to an-
other token (Token-component-1). The following three vari-
ants are also considered: variables of diﬀerent data types
are always mapped to the same token, and constants are
not mapped (Token-component-2); variables of the same
data type are mapped to the same token, and all constants
are mapped to another token (Token-component-3); vari-
ables of the same data type are mapped to the same to-
ken, and constants are not mapped (Token-component-4).
In Table 3, “Token-component-{1, 2, 3, 4}” respectively in-
dicates the four mappings mentioned above. Fourth, ex-
isting tree-based or graph-based code-similarity algorithms
usually do not deal with the mapping of statement compo-
nents, and therefore cannot cope with code reuses with iden-
tiﬁer renaming. To resolve this issue, we add six hybrid al-
gorithms that incorporate tree/graph-based code-similarity
algorithms and four Token-component mappings mentioned
above. For some graph-based code-similarity algorithms, the
Abstract Syntax Tree (AST) is also extracted to attain the
token-component mapping.
Using Support Vector Machines (SVM) for classi-
ﬁer learning. SVM is a popular supervised machine learn-
ing method. We use the open-source tool LibSVM [7] for
our purpose. We ﬁrst convert the vulnerability diﬀ hunk
features into numeric data and normalize each attribute to
the range [0, 1] while treating all attribute values as non-
negative. We take 70% vulnerabilities in each product ac-
cording to the VPD (i.e. F (1)) to learn classiﬁers. The
classiﬁer aims to distinguish the patched piece of code from
the unpatched piece of code with respect to a same diﬀ hunk.
We use the RBF kernel, which maps the feature vectors to
a high-dimensional space for handling the nonlinear relation
between the class labels and the attributes. We perform a
10-fold cross-validation on F (1) to pick the best values cor-
responding to the penalty and kernel parameters.
To explain the decision of the classiﬁer, we adopt the
leave-one-out method to discover the important features.
That is to say, each time we choose one of the vulnera-
bility diﬀ hunk features, set the value of the feature across
the entire testing data to be the same, and obtain the ac-
curacy of the classiﬁer when apply to the modiﬁed test set.
After repeating this process for each feature, we compare
the resulting accuracies corresponding to the modiﬁed test
data with the accuracies corresponding to the original test
set. This allows us to determine which features make bigger
contributions to the classiﬁer.
207
No.