In our study, twenty participants were recruited by word-of-mouth.
The majority of the participants were students at our University.
75% of the participants were males and 25% were females. The par-
ticipants were composed of educated individuals. The majority of
the participants aged between 25 and 34 years and came from Com-
puter Science background. Table 2 summarized the demographics
of the participants. The similar number of participants and demo-
graphics are well-established in lab-based studies in behavioral
biometrics research [12, 14, 33], which serves to demonstrate the
viability of the schemes.
Each of the participants were asked to perform each of the tasks
ninety times, spanned over three days/sessions. During each session,
the participants were asked to perform each task thirty times. We
set the time gap between two consecutive sessions to be a minimum
of 24 hours. During the study, we did not restrict the participants to
a specific phone holding setting. The participants had the choice to
use either a single hand or two hands. Further, they could perform
the study while sitting or standing. The order of the tasks presented
to different participants was derived using 3 × 3 Latin square to
minimize the learning effect. The Latin square ensures that each
user performed the three tasks in different orders. To avoid any
kind of inconsistency, we used only one smartphone (Samsung
Galaxy S6) throughout the data collection process.
We conducted the experiment following our University’s IRB
guidelines. The study and the experiment was approved by the IRB
at our institution. The participants were clearly informed about the
experiment, such as the data being collected, the purpose of the
Table 2: Demographics of participants (N = 20)
Category
% of participants
Male
Female
25-35
>35
Gender
75
25
Age
95
5
Field
Computer Science
Non-CS
Bachelors
Masters
PhD
65
35
Education
20
50
30
experiment, and that they can refuse to participate in the middle of
the experiment or even request to delete their collected data during
or after the experiment has been conducted.
The participants were subjected to a consent agreement and a
demographics form before the study. At the end of the third session,
participants’ experience in interacting with the three schemes was
recorded using a survey form. The survey contains the 10 System
Usability Scale (SUS) standard questions, each with 5 possible an-
swers (5-point Likert scale, where 1 represents strong disagreement
and 5 represents strong agreement) [8]. SUS is a standard question-
naire that is used to evaluate the usability of software, hardware,
cell phones, and websites, and it has been deployed in many prior
security usability studies.
5 FEATURE EXTRACTION AND
CLASSIFICATION MODELS
In order to build each of the biometrics considered in this study, we
utilized the machine learning approach. In this section, we present
the features we extracted from the users’ logs collected during our
data collection campaign. Then, we discuss the classification models
and the classifier employed in our study.
the average touch size (3 features).
5.1 Feature Extraction
S-Pattern: From each of the logs from S-Pattern app, we extracted
a total of 55 features. These features can be characterized in the
following three categories.
• Touch sensor features: Start touch size, end touch size, and
• Swipe features: Swipe time (total time taken by the user to
enter the pattern), speed, acceleration (i.e., change in speed/time)
and distance (4 features).
• Motion and position based features: From each of the sen-
sors utilized in our study (i.e., accelerometer, rotation vector,
linear acceleration, orientation, gyroscope, gravity and game
rotation vector), we extracted following 6 statistical features –
mean, standard deviation, minimum, maximum, number of local
minima, and number of local maxima (48 features = 8 sensors ×
6 statistical features).
358Table 3: Performance for 10-fold cross validation of different classifiers for the three schemes.
FPR
0.01 (0.01)
RF
0.04 (0.02)
MP
J48
0.06 (0.03)
SVM 0.03 (0.02)
0.05 (0.06)
NB
L
0.08 (0.03)
0.06 (0.03)
RT
0.01 (0.01)
RF
0.02 (0.02)
MP
J48
0.05 (0.02)
SVM 0.03 (0.03)
0.06 (0.07)
NB
0.09 (0.05)
L
0.07 (0.04)
RT
RF
0.02 (0.02)
0.05 (0.03)
MP
J48
0.06 (0.03)
SVM 0.04 (0.03)
NB
0.08 (0.03)
0.07 (0.04)
L
RT
0.11 (0.03)
FNR
0.04 (0.02)
0.08 (0.04)
0.06 (0.04)
0.10 (0.05)
0.11 (0.06)
0.12 (0.05)
0.09 (0.04)
0.05 (0.02)
0.09 (0.05)
0.06 (0.03)
0.10 (0.06)
0.14 (0.11)
0.15 (0.06)
0.08 (0.03)
0.05 (0.03)
0.10 (0.04)
0.07 (0.03)
0.12 (0.05)
0.16 (0.10)
0.13 (0.06)
0.10 (0.05)
Precision Recall
0.99 (0.01)
0.96 (0.02)
0.94 (0.03)
0.97 (0.02)
0.95 (0.06)
0.92 (0.03)
0.94 (0.03)
0.99 (0.01)
0.97 (0.02)
0.95 (0.02)
0.97 (0.03)
0.94 (0.07)
0.91 (0.05)
0.93 (0.04)
0.98 (0.02)
0.95 (0.03)
0.94 (0.03)
0.96 (0.03)
0.92 (0.03)
0.93 (0.04)
0.89 (0.03)
0.97 (0.02)
0.92 (0.03)
0.94 (0.04)
0.91 (0.04)
0.89 (0.05)
0.88 (0.05)
0.91 (0.04)
0.96 (0.02)
0.92 (0.04)
0.94 (0.03)
0.90 (0.05)
0.88 (0.08)
0.86 (0.05)
0.92 (0.03)
0.95 (0.03)
0.90 (0.03)
0.93 (0.03)
0.89 (0.04)
0.86 (0.08)
0.88 (0.05)
0.90 (0.05)
F-Measure
0.98 (0.01)
0.94 (0.02)
0.94 (0.03)
0.94 (0.03)
0.92 (0.05)
0.90 (0.04)
0.93 (0.03)
0.97 (0.01)
0.94 (0.03)
0.95 (0.02)
0.94 (0.04)
0.90 (0.05)
0.88 (0.05)
0.92 (0.03)
0.97 (0.02)
0.93 (0.03)
0.94 (0.02)
0.92 (0.03)
0.89 (0.04)
0.90 (0.04)
0.90 (0.03)
n
r
e
tt
a
P
-
S
n
r
e
tt
a
P
-
R
C
s
c
i
r
t
e
m
a
G
CR-Pattern: Similar to S-Pattern, we extracted the same 55 fea-
tures from touch, swipe and, motion/position categories. The one
exception was that instead of using the exact distance traveled in
the distance-based features, we used the difference between the
distance traveled and the minimum distance required to enter the
pattern.
Gametrics: From each of the logs for Gametrics, we extracted a
total of 78 features that capture the cognitive abilities of the user
while she is solving the challenges as well as the features extracted
from the touch, motion and position sensors.
In the previous two methods, the user has to perform a single,
relatively long swipe. However, in Gametrics, the user has to per-
form a minimum of three relatively short swipes (drags and drops).
From the touch sensor data, we extracted 12 features – average,
standard deviation, minimum and maximum of start touch size, end
touch size and average touch size. Moreover, for the swipe features,
rather than extracting a single feature from each of the speed, the
acceleration, and the distance, we extracted the statistical features
corresponding to each of the average, standard deviation, minimum
and maximum (12 features). As in the previous two tasks, we ex-
tracted the same statistical features from the motion and position
sensors.
As described in Section 3, in order to solve a semantic interactive
challenge, the user has to match the answer objects to their corre-
sponding targets. In order to do that, the user has to understand
the content of the images representing the targets and the moving
objects, find the relationship between the moving objects and the
target objects, and then select a subset of the moving objects (the
answer objects), and finally drag/drop them to their corresponding
targets. By monitoring the users as they solved the challenges, we
found different users take different approaches to solve the chal-
lenges. For example, some users start by trying to comprehend
the whole challenge and then start the object matching task while
some try to find the answer objects corresponding to the target in
certain order (i.e., always try to search for the answer object that
corresponds to the top most target, and then the second and so on).
Others try to pick the object closest to the finger and then check if
it matches with any of the targets.
These different mechanisms of solving the semantic challenges
are related to the cognitive characteristics of individuals. We cap-
ture these characteristics based on the following 6 features (these
features are similar the ones used in [27]).
(1) The time between the user pressing the start button and the first
recorded touch event. This timing measure captures the time
the user takes to comprehend the challenge and start solving it.
(2) The average, standard deviation, minimum and maximum of
the times between each of the drops and the start of the next
drag. These features capture the time the user takes to find the
next answer object.
(3) The total time taken by the user to complete the challenge.
5.2 Classification Metrics & Classifier
Classification Metrics: In our classification task, the positive class
corresponds to the legitimate user interaction with the authentica-
tion construct and the negative class corresponds to the imperson-
ator (other user or the zero-effort attacker). Therefore, true positives
(TP) represent the number of times the legitimate user is granted
access, true negatives (TN) represents the number of times the im-
personator is rejected, false positives (FP) represent the number of
359times the impersonator is granted access and false negatives (FN)
represent the number of times the legitimate user is rejected.
As performance measures for our classification models, we used
False Positive Rate (FPR), False Negative Rate (FNR), precision, recall
and F-measure (F1 score), as shown in Equations 1-5. Precision and
FPR measure the security of the proposed system, i.e., the accuracy
of the system in rejecting impersonators. Recall and FNR capture
the usability of the proposed system as low recall leads to high
rejection rate of legitimate users. F-measure considers both the
usability and the security of the system. To make the system both
usable and secure, ideally, we would like to have a recall, precision,
and F-measure to be as close as 1.
F PR =
F N R =
F P
T N + F P
F N
T P + F N
precision =
T P
T P + F P
recall =
T P
T P + F N
F-measure = 2 ∗ precision ∗ recall
precision + recall
(1)
(2)
(3)
(4)
(5)
Classifier: With the data samples collected in our study, we tested
different machine learning algorithms – J48, Random Forest (RF),
Random Tree (RT), Multilayer Perceptron (MP), Support Vector
Machines (SVM), Logistics (L), and Naive Bayes (NB). We applied
10-fold cross-validation approach to test all these machine learning
algorithms. Table 3 summarizes the classification results for the
three studied schemes. We achieved the best results with Random
Forest classifiers (F-Measure = 98% for S-Pattern, and 97% for CR-
Pattern and Gametrics). Therefore, in our analysis, we utilized the
Random Forest classifier.
Random Forest is an ensemble learning approach that constructs
many classification trees during the learning phase where each tree
is generated using a separate bootstrap sample of the data. In the
testing/classifying phase, the new data is run down all the trees
and the output is the mode of the votings from each individual