# 四、创建生产级 Swarm
在本章中，您将学习如何创建具有数千个节点的真实 Swarm 集群；具体来说，我们将涵盖以下主题:
*   部署大型集群的工具
*   Swarm2k:有史以来最大的 Swarm 模式集群之一，由 2300 个节点组成
*   Swarm3k:第二个实验，一个有 4700 个节点的集群
*   如何规划硬件资源
*   高可用性集群拓扑
*   群集基础架构管理、网络和安全
*   监控仪表板
*   你从 Swarm2k 和 Swarm3k 实验中学到了什么
# 工具
借助 Swarm 模式，我们可以轻松设计生产级集群。
我们在这里阐述的原则和架构总体上很重要，并为如何设计生产安装奠定了基础，而不管工具是什么。然而，从实用的角度来看，使用的工具也很重要。
在撰写本书时，Docker Machine 并不是用于大型集群设置的理想单一工具，因此我们正在用本书附带的工具演示我们的生产规模部署，我们已经在[第 1 章](01.html "Chapter 1. Welcome to Docker Swarm")、*中介绍了该工具欢迎来到 Docker Swarm*:belt([https://github.com/chanwit/belt](https://github.com/chanwit/belt))。我们将结合 Docker Machine、Docker Networking 和 DigitalOcean 的`doctl`命令使用它。
在[第 5 章](05.html "Chapter 5. Administer a Swarm Cluster")、*管理蜂群*中，你将学习如何能够自动创建蜂群；尤其是，如何快速加入大量具有脚本和其他机制的工作人员，例如 Ansible。
![Tools](img/image_04_001.jpg)
# 集群 2k 的高可用性拓扑
Swarm2k 和 Swarm3k 是协同实验。我们以 Docker Hosts 的形式筹集资金，而不是金钱，并号召大家参与。结果令人惊讶——有几十个个人和公司地理分布的贡献者加入了 Swarm2k 和 Swarm3k。总的来说，对于群集 2k，我们收集了大约 2300 个节点，而对于群集 3k，大约 4700 个。
我们来讨论一下 *Swarm2k* 的架构。在上图中，有三个经理，分别为 **mg0** 、 **mg1** 和 **mg2** 。我们将使用三名经理，因为这是 Docker 核心团队建议的最佳经理人数。管理人员在高速网络链路上形成法定人数，raft 节点使用大量资源来同步其活动。因此，我们决定将我们的经理部署在同一个数据中心的 40GB 以太网链路上。
在实验开始时，我们有以下配置:
*   mg0 是集群的管理者领导者
*   mg1 托管了 stat 收集器
*   mg2 是一个准备好的(后备)经理
相反， **W** 节点是 Swarm 工作人员。
安装在 mg1 上的 stat 收集器从本地 Docker 引擎中查询信息，并将它们发送到远程时间序列数据库 *InfluxDB* 中存储。我们选择了英菲尼克斯数据库，因为它是由我们的监控代理*电信*本地支持的。为了显示集群的统计数据，我们使用了*格拉夫纳*作为仪表板，我们将在后面看到。
## 管理者规范
管理器受 CPU 限制，而不是受内存限制。对于一个 500-1000 节点的 Swarm 集群，我们根据经验观察到三个管理器，每个管理器有 8 个 vCPUs，足以保持负载。但是，如果超过 2000 个节点，我们建议每个管理器至少有 16-20 个 vCPUs，以满足最终的 Raft 恢复。
### 如果是筏式回收
下图显示了硬件升级期间以及大量工作人员加入过程中的 CPU 使用情况。在硬件升级到 8 个虚电路的过程中(机器停机时间由线路断开表示)，我们可以看到当 mg **1** 和 mg **2** 重新加入集群时，领导者 mg0 的 CPU 使用率飙升到 75-90%。触发此峰值的事件是 Raft 日志同步和恢复。
在正常情况下，由于不需要恢复，每个管理器的 CPU 使用率都很低，如下图所示。
![In case of Raft recovery](img/image_04_002.jpg)
## 筏形文件
在管理主机中，群数据保存在`/var/lib/docker/swarm`中，称为*群目录*。具体来说，Raft 数据保存在`/var/lib/docker/swarm/raft`中，由提前写日志(WAL)和快照文件组成。
在这些文件中，有节点、服务和任务的条目，由 Protobuf 格式定义。
WAL 和快照文件经常被写入磁盘。在 SwarmKit 和 Docker Swarm 模式下，每 10，000 个条目就会写入磁盘。根据这一行为，我们将 swarm 目录映射到一个具有更高吞吐量的快速专用磁盘，特别是固态硬盘。
我们将在[第 5 章](05.html "Chapter 5. Administer a Swarm Cluster")、*管理集群*中解释集群目录损坏时的备份和恢复过程。
## 正在运行的任务
Swarm 集群的目标是运行服务，例如，由大量容器组成的大规模 Web 应用。我们将这种部署类型称为 *Mono* 模型。在这个模型中，网络端口被认为是必须全局发布的资源。在 Docker Swarm Mode 的未来版本中，有了*名称空间*，部署可以在 *Multi* 模型中进行，在该模型中，我们可以有多个子集群，它们为不同的服务公开相同的端口。
在小规模集群中，我们可以决定允许经理谨慎地主持工人任务。相反，对于更大的设置，经理使用更多的资源。此外，如果管理器负载使其资源饱和，集群将变得不稳定和无响应，并且不会接受任何命令。我们称这种状态为*狂暴*T2 状态。
要使一个大型集群(如群集 2k 或群集 3k)保持稳定，所有管理人员的可用性必须设置为“耗尽”状态，这样所有任务就不会安排在他们身上，只安排在工作人员身上，条件是:
```
 docker node update --availability drain node-name
```
## 管理器拓扑
我们将在[第 5 章](05.html "Chapter 5. Administer a Swarm Cluster")、*中再次讨论这个高可用性属性，但是在这里，我们将介绍它来说明一些群拓扑理论。高可用性理论强制要求形成具有奇数个节点的高可用性集群。下表显示了单个数据中心的容错系数。在本章中，我们将称之为 5(1)-3-2 公式，即具有 3 节点仲裁的数据中心的群集大小为 5，允许 2 个节点发生故障:*
| **集群大小** | **号房**号房 | **允许节点故障** |
| three | Two | one |
| five | three | Two |
| seven | four | three |
| nine | five | four |
但是，有几种管理器拓扑可以设计用于具有多个数据中心的生产环境。例如，3(3)个管理器拓扑可以分布为 1 + 1 + 1，而 5(3)个管理器拓扑可以分布为 2 + 2 + 1。下图显示了最佳的 5(3)管理器拓扑:
![Manager topologies](img/image_04_003.jpg)
在具有相同容差级别的情况下，下图显示了一个备选的 5(4)拓扑，其中包含 4 个数据中心的 5 个管理器。有 2 个管理器 mg0 和 mg1 在数据中心 1 中运行，而其余管理器 mg2、mg3 和 mg4 分别在数据中心 2、3 和 4 中运行。mg0 和 mg1 管理器连接在高速网络上，而 mg2、mg3 和 mg4 可以使用较慢的链路。因此，跨 3 个数据中心的 2 + 2 + 1 将重新排列为跨 4 个数据中心的 2 + 1 + 1 + 1。
![Manager topologies](img/image_04_004.jpg)
最后，还有另一种分布式拓扑 6(4)，它的性能更高，因为在它的核心，有 3 个节点组成高速链路上的中央仲裁。6 管理器群集需要 4 的仲裁大小。如果数据中心 1 出现故障，群集的控制平面将停止工作。在正常情况下，除主节点外，可以有 2 个节点或 2 个数据中心停机。
![Manager topologies](img/image_04_005.jpg)
总而言之，只要有可能，就坚持奇数个经理。如果你想要稳定的管理法定人数，形成高速链接。如果你想避免单点故障，尽可能地分布它们。
要确认哪种拓扑适合您，请尝试构建它，并通过有意让一些管理器停机来测试管理器延迟，然后测量它们恢复的速度。
对于群集 2k 和群集 3k，我们选择将所有三个管理器放在一个数据中心来形成拓扑，因为我们希望实现最佳性能。
# 为基础设施配备皮带
首先，我们使用以下命令为数字海洋创建了一个名为`swarm2k`的集群模板:
```
$ belt cluster new --driver digitalocean swarm2k
```
前面的命令在当前目录下创建了一个名为`.belt/swarm2k/config.yml`的配置模板文件。这是我们定义其他属性的起点。
我们检查了我们的集群是否是通过运行以下命令定义的:
```
$ belt cluster ls
CLUSTER       ACTIVE    LEADER    MASTERS    #NODES
swarm2k       -         -         -          0 / 0
```
通过使用命令，我们可以切换和使用可用的`swarm2k`集群，如下所示:
```
$ belt use swarm2k
swarm2k
```
至此，我们细化了`swarm2k`模板的属性。
通过发出以下命令将数字海洋的实例区域设置为`sgp1`:
```
$ belt cluster update region=sgp1
```
用此命令定义所有必要值所需的皮带。以下是我们在`config.yml`中指定的数字海洋驱动程序所需的模板密钥列表:
*   `image`:这是指定数字海洋映像标识或快照标识
*   `region`:指定数字海洋区域，例如 sgp1 或 nyc3
*   `ssh_key_fingerprint`:这是指定数字海洋 SSH 密钥 ID 或指纹
*   `ssh_user`:这是指定镜像使用的用户名，例如 root
*   `access_token`:这是指定 DigitalOcean 的访问令牌；建议不要在这里放任何代币
### 类型
每个模板属性都有其对应的环境变量。例如`access_token`属性可以通过`DIGITALOCEAN_ACCESS_TOKEN`设置。因此，在实践中，我们也可以在继续之前将`DIGITALOCEAN_ACCESS_TOKEN`导出为壳变量。
配置就绪后，我们通过运行以下代码来验证当前的模板属性:
```
$ belt cluster config
digitalocean:
 image: "123456"
 region: sgp1
 ssh_key_fingerprint: "800000"
 ssh_user: root
```
现在，我们使用以下语法创建了一组 3 个 512MB 的管理器节点，称为 mg0、mg1 和 mg2:
```
$ belt create 8192MB mg[0:2]
NAME   IPv4         MEMORY  REGION  IMAGE       STATUS
mg2    128.*.*.11   8192     sgp1   Ubuntu docker-1.12.1 new
mg1    128.*.*.220  8192     sgp1   Ubuntu docker-1.12.1 new
mg0    128.*.*.21   8192     sgp1   Ubuntu docker-1.12.1 new
```
所有新节点都已初始化并进入新状态。
我们可以使用以下命令等待，直到所有 3 个节点变为活动状态:
```
$ belt status --wait active=3
STATUS  #NODES  NAMES
new         3   mg2, mg1, mg0
STATUS  #NODES  NAMES
new         3   mg2, mg1, mg0
STATUS  #NODES  NAMES
new         3   mg2, mg1, mg0
STATUS  #NODES  NAMES
active      3   mg2, mg1, mg0
```
然后，我们将 node1 设置为主动管理器主机，我们的 Swarm 将准备好形成。可以通过运行 active 命令来设置活动主机，如下所示:
```
$ belt active mg0
swarm2k/mg0
```
在这一点上，我们形成了一个群体。我们将 mg0 初始化为经理领导，如下所示:
```
$ belt docker swarm init --advertise-addr 128.*.*.220
Swarm initialized: current node (24j7sytbomhshtayt74lf7njo) is now 
    a manager.
```
前面的命令输出要复制和粘贴的字符串，以连接其他经理和员工，例如，看一下下面的命令:
```
 docker swarm join \
 --token SWMTKN-1-1wwyxnfcgqt...fwzc1in3 \
 128.*.*.220:2377
```
Belt 提供了一个便捷的快捷方式，通过以下语法连接节点，这就是我们用来将 mg1 和 mg2 连接到群的方法。：
```
$ belt --host mg[1:2] docker swarm join \
 --token --token SWMTKN-1-1wwyxnfcgqt...fwzc1in3 \
 128.*.*.220:2377
```
现在，我们已经配置了 mg0、mg1 和 mg2 管理器，并准备好接收大批工作人员。
# 用 Docker 机器保护经理
Docker Machine 不能很好地扩展到大规模的 Docker Engine 部署，但是它对于自动保护少量节点非常有用。在下一节中，我们将使用 Docker Machine 使用通用驱动程序来保护我们的 Swarm 管理器，该驱动程序允许我们控制现有的主机。