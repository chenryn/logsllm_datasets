new distribution (ˆπ(cid:48)) using new fake distribution π(cid:48)f , after replica swapping completes. Key a gains a replica, while D loses a replica.
niques: adjusting the fake query distribution and caching
replicas at the proxy.
Replica swapping. Our key insight in adapting to change in
query distributions is that since the total number of replicas
for any distribution is always exactly 2n (including dummy
replicas), a transition from ˆπ to ˆπ(cid:48) ensures that for each key
ki that gains a replica, there must be another key k j that loses
a replica. Therefore, handling changes in query distributions
simply requires, for all such keys, reading k j’s value and writ-
ing it to one of k j’s replicas, a process we refer to as replica
swapping. PANCAKE performs these swaps without revealing
any information about the change by piggybacking the replica
swaps atop normal client accesses. Maintaining uniform ac-
cesses during replica swapping requires changes to the Batch
procedure and the fake access distribution as described in §4;
we describe these changes next.
maintains a mapping between the label of replicas in L and the
replica in G it will be swapped with. On subsequent queries
during the transition, Batch consults the mapping for the right
labels. This metadata can be deleted after periodic rotation of
the cryptographic keys. We describe key rotation in the full
version [25]. When all swaps have occurred, we switch back
to the normal Batch procedure for ˆπ(cid:48).
As a concrete example of replica swapping, consider Fig-
ure 3. The set G contains the replica (a,2), while L contains
(D,3). Note that both G and L could contain dummy repli-
cas, depending on how the distribution changes. Batch would
swap the replicas for keys a and D on the ﬁrst access to
(D,3) ∈ L by writing back an encryption of key a’s value
(because (a,2) ∈ G) instead of a re-encryption of the dummy
value D. To enable this, PANCAKE would maintain a mapping
that indicates the label of (a,2) is F(D,3).
Adjustments to fake access distribution. Two more modiﬁ-
cations are needed during the transition. First, we must use a
different fake access distribution to ensure that reads to keys
that have gained replicas always succeed. To see why this is
necessary, consider again the example in Figure 3. If a query
tries to read key a by accessing replica (a,2) before the value
of (D,3) is changed, the read will return D’s value instead
of a’s. Thus replica (a,1) must be read, but forcing this makes
(a,1)’s probability too high, violating security.
PANCAKE handles this by temporarily increasing the
threshold α to α(cid:48) = maxk{π(cid:48)(k)/R(k, ˆπ,α)}, and using a tem-
porary fake access distribution ˜π(cid:48)f to satisfy Equation 1 with
α(cid:48). For each (k, j) ∈ G, we set ˜π(cid:48)f (k, j) = α(cid:48)
2nα(cid:48)−1, and k’s
existing replicas have ˜π(cid:48)f =
. For other keys,
˜π(cid:48)f (k, j) is set to α(cid:48)−ˆπ(cid:48)(k)/R(k,ˆπ(cid:48),α)
Since α(cid:48) ≥ α, the real access probability δ = 1/2nα(cid:48) is
lower during replica swapping. As such, this may lead to
α(cid:48)−ˆπ(cid:48)(k)/R(k,ˆπ,α)
2nα(cid:48)−1
2nα(cid:48)−1
.
During replica swapping, the modiﬁed Batch uses two lists:
G and L. G is the set of replicas that need to be created and L
is the set of replicas that need to be removed. Formally, if S
is the set of keys that must gain replicas, T is the set of keys
that must lose replicas, and R(k, ˆπ(cid:48),α) = (cid:100)ˆπ(cid:48)(k)/α(cid:101), then,
G = {(k, j)|k ∈ S, j ∈ [R(k, ˆπ,α) + 1, . . . ,R(k, ˆπ(cid:48),α)]}
L = {(k, j)|k ∈ T, j ∈ [R(k, ˆπ(cid:48),α) + 1, . . . ,R(k, ˆπ,α)]}
A pseudocode procedure for generating these lists from ˆπ and
ˆπ(cid:48) is given in the full version [25], along with a description of
the modiﬁed Batch. It is not hard to see that |G| = |L| always
(since |S| = |T|), and that swapping each replica in L for one
in G results in all keys having the right number of replicas
under ˆπ(cid:48). This swapping is done opportunistically by Batch:
when a replica in L is read in a batch, either by a real or a
fake query, its value is updated to the value associated with a
replica in G during the writeback. For security reasons, PRF
labels for replicas in G are not changed. Instead, PANCAKE
2458    29th USENIX Security Symposium
USENIX Association
some real queries being delayed to later batches. This may
increase latency for some queries during replica swapping,
but we show in §6.2 that replica swapping completes in a few
minutes even for drastic changes in the distribution.
Replica caching. PANCAKE computes the mapping between
each label in L and the replica in G it will be swapped with
when the distribution change is detected. However, the actual
values of replicas in G must be propagated to those in L during
subsequent accesses to them. Without any additional mecha-
nism, reads to keys with replicas in G may access incorrect
values. To ensure correctness, when a replica in G is read
during Batch, its value is cached at the proxy. This value is
then propagated to the replica in L when it is next accessed,
while the actual read is served from the cache.
Insertion and deletion of keys. We have assumed so far that
the support size is ﬁxed; interestingly, the replica swapping
procedure can support changes in the set of keys. This can be
viewed as a distribution change where supp(ˆπ(cid:48)) (cid:54)= supp(ˆπ).
As long as PANCAKE is initialized with enough replicas to
handle the maximum support size, new keys can be inserted
by swapping a dummy replica for the new key, and vice versa
for deletion. Some additional metadata is needed, but similar
to the PRF label mapping it can be deleted as soon as cryp-
tographic keys are rotated (details in the full version [25]).
5.2 Security Analysis
We prove that PANCAKE’s accesses remain uniform even for
time-varying distributions, under the assumption that changes
in distributions can be detected instantaneously. We formalize
our goal as a generalization of the static ROR-CDA security
notion. We call this new notion “real-or-random security un-
der chosen dynamic distribution attack”, or ROR-CDDA. It
is similar to its static analogue except that it uses two distribu-
tions π and π(cid:48): after an adversarially chosen number of queries
the distribution changes from π to π(cid:48). We let Advror-cdda(A) be
the ROR-CDDA advantage of an adversary A. It captures the
ability of A to distinguish between a real PANCAKE execution
during a distribution change (ROR-CDDA1) and uniformly
random accesses (ROR-CDDA0). The following theorem cap-
tures the ROR-CDDA security of PANCAKE.
Theorem 2 Let q ≥ 0 and Q = q· B. Let π,π(cid:48), ˆπ, ˆπ(cid:48) be distri-
butions. For any q-query ROR-CDDA adversary A against
PANCAKE we give adversaries B, C, D1, D2 such that
Advror-cdda
PANCAKE(A) ≤ Advprf
Q,π,ˆπ(D1) + Advdist
Q,π(cid:48),ˆπ(cid:48)(D2)
where F and E are the PRF and AE scheme used by PANCAKE.
Adversaries B, C, D1, D2 each use at most Q queries and run
in time that of A plus a small overhead linear in Q.
Discussion. Full details of the deﬁnitions and a proof of The-
orem 2 appear in Appendix A. We discuss here only one
F (B) + Advror
+ Advdist
E (C)
salient point regarding ROR-CDDA. ROR-CDDA models
the shift from π to π(cid:48) as happening and being detected instan-
taneously. This may not be realistic in some cases, even with
state-of-the-art techniques in detecting distribution changes
(as used in PANCAKE, discussed in next subsection). Thus, we
cannot rule out the case where PANCAKE processes queries
from π(cid:48) before the change is detected (treating them like
samples from ˆπ). The distribution of these queries would be
non-uniform, with bias related to the difference between π
and π(cid:48). If the adversary knows the bias, using it in an attack
would be possible but challenging—indeed, we are not aware
of any published attacks that even consider the possibility of
distribution changes.
5.3 Detecting Changes in Query Distribution
Detecting distribution changes using statistical tests is a well
studied problem [34, 36, 61, 67]. While it is possible to have
PANCAKE receive external signals (e.g., from an analyst)
when the distribution changes, our implementation incorpo-
rates the two-sample Kolmogorov–Smirnov (KS) test [36,61],
a standard statistical tool, to detect such changes automatically.
Speciﬁcally, recall that PANCAKE maintains a histogram H of
observed accesses to maintain an estimate ˆπ for distribution π.
In order to track changes to the distribution, PANCAKE addi-
tionally maintains a running histogram Hrunning over a sliding
window of the w latest accesses at the proxy. PANCAKE then
uses KS test to determine when the underlying distribution
corresponding to Hrunning differs from ˆπ. If the test indicates
a change, PANCAKE uses the current Hrunning snapshot to in-
form the estimate ˆπ(cid:48) for the new distribution π(cid:48).
Detecting changes in distributions, and responding to these
changes involves balancing security and efﬁciency. If the test
is too sensitive the system will waste resources adjusting to
spurious changes; on the other hand, as discussed above, an
insensitive test could leak information about queries. While it
is possible to use other statistical tests [67], or an ensemble of
tests to navigate this tradeoff between performance and secu-
rity, no statistical test is perfect. We present several evaluation
results related to detecting and adapting to changes in query
distribution, along with sensitivity analysis, in §6.2.
6 Evaluation
We now evaluate PANCAKE across a wide variety of scenarios,
including main-memory and secondary storage-based data
stores, static and dynamic distributions, deployment settings
and workloads. We start by brieﬂy describing the evaluation
methodology, followed by detailed discussion of our results.
Compared approaches. We compare PANCAKE against two
approaches: (1) an insecure baseline that provides no secu-
rity guarantees, and (2) non-recursive PathORAM [63] (with
Z = 4), a state-of-the-art ORAM. The former serves as an
upper bound on PANCAKE performance, since it corresponds
to a data store with no security overheads. The latter, on the
USENIX Association
29th USENIX Security Symposium    2459
(a) Throughput (single proxy thread)
(b) Latency (single proxy thread)
(c) Throughput (multiple proxy threads)
(d) Throughput vs latency (multiple proxy
Figure 4: Performance for in-memory server storage (Redis). (a, b) PANCAKE’s throughput is over 220× higher than PathORAM and
within ∼6.8–7.6× of the insecure baseline for a single-threaded proxy; note that the y-axis is in log-scale. (c, d) With multiple proxy threads,
PANCAKE’s peak throughput is within 3.4–6.3× and latency within 2.3–2.6× of the insecure baseline.
other hand, is the state-of-the-art design that provides security
under our model (as well as under stronger models where an
adversary can actively inject its own queries). As discussed
earlier, our comparison against the latter should be interpreted
as highlighting the huge efﬁciency gap between countermea-
sures in the two threat models. We use batch size B = 3 for
PANCAKE’s Batch algorithm.
C (100% reads). These workloads represent two extremes in
read-write proportions; other YCSB workloads either have
intermediate read-write proportions (e.g., Workload B, D)
or contain queries not supported by PANCAKE (e.g., Work-
load E). YCSB uses a Zipf distribution over key accesses (with
skewness parameter = 0.99, i.e., very skewed), which is rep-
resentative of access patterns in real-world deployments [17].
threads)
We compare these approaches using two representative stor-
age backends: an in-memory KV store Redis [54], and a per-
sistent SSD-based KV store RocksDB [55]. Our PathORAM
deployment used an open-source implementation [14,58]. For
PathORAM and PANCAKE, client queries are forwarded to
the data store via a proxy server; for the insecure baseline,
client queries are forwarded to the backend storage server
without any intermediary proxy.
The PathORAM implementation used in our evaluation [14,
58] is single-threaded. TaoStore [57] and ConcurORAM [13]
implement multi-threaded PathORAM; we omit results for
them since they employ specialized storage backends adapted
for ORAMs, eschewing fair comparison with backends we
investigate. We note, however, that the performance reported
in [13, 57] is at least an order of magnitude lower than PAN-
CAKE even with specialized storage backends.
Experimental setup. Our experiments run on Amazon EC2.
The storage backend runs on a single t3.2xlarge instance
with 8 vCPUs, 32GB RAM, and 1Gbps network and disk
bandwidth. We use 1Gbps links and proxy/client machines
with sufﬁcient resources (r4.8xlarge instances with 32 vCPUs,
244GB RAM, 10Gbps network bandwidth) to highlight the
impact of network bandwidth as a bottleneck.
Dataset and workloads. We use the Yahoo! Cloud Serv-
ing Benchmark (YCSB) [17], a standard benchmark for KV
stores, to generate the datasets and workloads. The dataset
contains 220 KV pairs, with 8B keys and 1KB values. We
conﬁne our dataset size to 1GB since PathORAM has pro-
hibitively large initialization times (> 24 hours) and storage
overheads (> 10×) with larger datasets, while PANCAKE per-
formance is essentially independent of dataset size.
We evaluate system throughput and latency using two
YCSB workloads: Workload A (50% reads, 50% writes) and
6.1 Performance for Static Distributions
We ﬁrst compare the performance for different approaches
with various storage backends under static query distributions.
In-memory server storage (Redis, Figure 4). With a sin-
gle proxy thread, PANCAKE and PathORAM performance is
bottlenecked by the proxy. For this evaluation setting, PathO-
RAM achieves throughput ∼1600× lower compared to the
insecure baseline. This is because PathORAM issues 160 stor-
age backend requests (= Z log2 N, Z = 4, N = 220) for every
client request, along with complex tree and stash management.
PANCAKE achieves signiﬁcantly better throughput (as
much as 229× better) compared to PathORAM. In compar-
ison to the insecure baseline, PANCAKE average latency is
within 2.3–2.6× and throughput is within 6.8–7.6× (Fig-
ure 4(a), 4(b)). This is a cumulative effect of three fac-
tors: (1) 3× bandwidth overhead due to batch size B = 3,
(2) 2× overhead since each request generates a read and a
write request in PANCAKE, and (3) overheads due to encryp-
tion/decryption. Our evaluation conﬁrms that adding encryp-
tion/decryption to the insecure baseline brings PANCAKE’s
relative throughput overhead to 6×. We note that PANCAKE’s
99th percentile latency (not shown in graphs) is relatively
higher (within 4.1–5.6× the insecure baseline) due to queue-
ing delays from PANCAKE’s Batch algorithm. We note that if
reducing tail latency were the goal, one can achieve that at the
cost of higher bandwidth overheads by increasing B (§6.3).
With multiple proxy threads, PANCAKE peak throughput is
within 3.4× of baseline for the read-only workload (YCSB
Workload C) — a factor of 2 better than the single proxy
thread. This reduction in relative overhead is due to the shift
in performance bottleneck to the network bandwidth in the
multi-threaded setting. We note that all network links are full-
2460    29th USENIX Security Symposium
USENIX Association
ACYCSBWorkload0.010.11101001000Throughput(KOps)Insecure-BaselinePathORAMpancakeACYCSBWorkload0.11101001000AverageLatency(ms)Insecure-BaselinePathORAMpancakeACYCSBWorkload0100200300Throughput(KOps)Insecure-Baselinepancake0100200Throughput(KOps)0255075100AverageLatency(µs)Insecure(YCSB-A)pancake(YCSB-A)Insecure(YCSB-C)pancake(YCSB-C)(a) Throughput (single proxy thread)
(b) Latency (single proxy thread)
(c) Throughput (multiple proxy threads)
(d) Throughput vs latency (multiple proxy
Figure 5: Performance for SSD-based server storage (RocksDB). (a, b) PANCAKE’s throughput is 17.3× higher than PathORAM and within
∼10.7–11.3× of the insecure baseline for a single-threaded proxy; note that the y-axis is in log scale for (a). (c, d) Using multiple proxy
threads, PANCAKE’s peak throughput is within 3.3–5.3× and average latency within 2–2.4× of the insecure baseline.
threads)
(a) Detecting change in Zipf skew
(b) Detecting shift in key popularities
(c) Adapting to distribution change