service and raised alerts to the network operator.
The operator suspected that this problem was due to packet
drops. However, he had no clue where the drops happened
because the web service was behind a VIP (virtual IP) which
was hosted on multiple software Muxes. A client’s request
could be sent to any one of the Muxes and then redirected to
any one of the many DIPs (direct IPs). This means the drops
could happen anywhere between the clients and the Muxes,
on one of the Muxes, or between the Muxes and the DIPs.
He ﬁrst checked the error counters on the Muxes but found
no problem. Then he checked the counters on a number of
switches, some of which showed minor drops. But none of
the switch counters was signiﬁcant enough for him to reach
a reliable conclusion. At this point, he was stuck and had to
elevate the alert.
486Link A
Link B
Link C
)
s
m
(
y
c
n
e
t
a
L
k
n
L
i
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
(a) Many links may be responsi-
ble for the latency problem
 0
 100  200  300  400  500  600
Time (second)
(b) Latency proﬁling results
Figure 7: Proﬁling link latency
Figure 8: A switch blackholes packets
to a DIP behind a SLB Mux
We ran the packet drop debugger to investigate this issue.
From the drop debugger, we observed that many SYN packet
traces that did not reach the DIPs after traversing the Muxes
and that all these abnormal traces stalled one hop before the
same switch S. The drop debugger subsequently launched
guided probes, with the same 5-tuples as the dropped SYN
packets, to S to validate the drops. The results showed that
all the SYN packets destined to a speciﬁc DIP were dropped
by S, suggesting that S was blackholing the packets to the
DIP (Fig 8).
Informed by our analysis, the operator analyzed the sus-
pect switch thoroughly and found that one of the forwarding
entries in the TCAM table was corrupted. Any packet that
hit the corrupted entry would be dropped. Even worse, the
switch did not report such drops in any counter. After a re-
boot, the switch started behaving normally and the timeout
problem went away as well.
7.2.2 Silent packet drop
Many DC applications in Cluster B detected abnormally
high levels of TCP retransmissions. At the same time, the
network-wide end-to-end monitoring system also showed that
the ping probes between certain server pairs were experienc-
ing around 2% loss rate. This caused severe performance
degradation to the applications running in the network.
As was usual, the operator started with the switch counters
but did not see anything particularly wrong. Given that the
end-to-end probes did show drops, he suspected that this was
caused by one switch silently dropping packets. The conven-
tional approach to debug such a problem is to use tomogra-
phy, e.g., inferring the rough location of the culprit switch
by sending many pings and traceroutes to exhaustively ex-
plore the paths and links in the network. After narrowing
down to a few suspects, the operator would try to pinpoint
the culprit by disabling and enabling the suspects one by
one. However, since the DCN has nearly 25,000 links, such
a brute-force approach will impose signiﬁcant overhead and
can easily take several hours.
Our packet drop debugger simpliﬁed the debugging pro-
cess. From all the packet traces in the storage, it ﬁrst ex-
tracted the ones that encountered drops during the same time
period. Based on where the problematic traces stopped, the
debugger quickly identiﬁed one suspect switch which ap-
peared to be the next hop of most of the problematic traces.
It then sent 1,000 guided probes to test each interface on
the switch. In the end, the root cause was found to be one
faulty interface on the switch that silently dropped a fraction
of packets at random. All the application alerts stopped after
the faulty interface was disabled.
7.2.3 Packet drops on servers
In this incident, one internal application alerted that the
throughput between its senders and receivers was 15% lower
than normal. The application used UDP as the transport pro-
tocol and had many senders and receivers. All the receivers
reported roughly equal amount of throughput loss. To isolate
the UDP packet drops, the sender’s team, receiver’s team and
network team were involved.
Due to the large number of senders, the sender’s team
could only sample a few senders and capture packets on
those senders. From the captured traces, they conﬁrmed that
the packets were indeed sent. The receiver’s team did the
same thing on several receivers and found that packets were
partially received. However, because the loads on the re-
ceivers were pretty high, they could not determine whether
the missing packets were dropped by the network or by the
receivers’ NIC. At this stage, neither the network nor the re-
ceiver’s teams could be released from debugging.
To ﬁnd out where the packets were dropped, the network
team used the Everﬂow drop debugger to mark the debug bit
on a subset of the senders’ trafﬁc. From the resulting packet
traces, the debugger showed that all the marked UDP pack-
ets successfully traversed the network and reached the last
hop ToR (Top-of-Rack) switches to which the receivers were
connected. Thus, the packet drops were happening at the re-
ceivers. After more investigation, the problem was found
to be a recent update on the senders that caused some mes-
sages to use a new format which were not recognized by the
receivers. Without Everﬂow, the network team would have
a hard time proving that the drops were not in the network.
Packet drop is a common source of perfor-
Summary.
mance faults in DCNs. Localizing drops is extremely chal-
487lenging given the large number of devices involved (servers,
switches, Muxes, etc.). Everﬂow offers an efﬁcient and re-
liable way to track packets throughout the network and dra-
matically simpliﬁes the drop debugging process.
7.3 Loop
Under normal circumstances, we do not expect to see any
loops in a DCN. Surprisingly, the Everﬂow loop debugger
did catch quite a few loop incidents in Cluster B. All the loop
traces showed the same pattern — they involved a packet that
kept bouncing back and forth between a SLB Mux and a DIP
until its TTL became 0.
Figure 9 shows how a loop forms. Initially a request packet
to V IP0 is sent to a SLB Mux, which then encapsulates and
sends the packet to a DIP. When the DIP server receives and
decapsulates the packet, it ﬁnds that the inner header desti-
nation IP (V IP0) does not match its designated VIP. Instead
of discarding the packet, the DIP server throws the packet
back to the network. As a result, this packet is sent back to
the Mux again, forming a persistent loop.
We call it an overlay loop since it results from inconsis-
tent views between two servers (Mux and DIP). Unlike a
network-level loop, an overlay loop cannot be detected by
examining the forwarding table on switches. In fact, the for-
warding tables are all correct. At the same time, an overlay
loop can result in unnecessary waste of network and server
resources or even unavailability. In these incidents, the loops
caused 30x ampliﬁcation of the affected trafﬁc. The trafﬁc
trapped in the loops accounted for roughly 17% of the total
trafﬁc received by the problematic DIPs.
Contrary to conventional wisdom, loops can
Summary.
form in a DCN due to the wide use of server overlays that
participate in packet forwarding. Everﬂow provides an ef-
fective way to detect and debug such loops, which allows
the loops to be ﬁxed before they trigger severe failures.
7.4 Load imbalance
The SNMP link monitor reported imbalanced loads on the
links from four leaf switches to a ToR switch in Cluster A.
Fig 10(b) shows the part of the topology that is related to
this incident. Because the loads on these links were affected
by all the upstream switches of the ToR, the operator had no
clue about why this imbalance happened.
The Everﬂow ECMP proﬁler was used to debug this prob-
lem. It started by requesting ﬁne-grained load counters from
the Everﬂow analyzers and breaking down the load by trafﬁc
towards physical server IP preﬁxes vs. trafﬁc towards SLB
VIP preﬁxes. It found that the trafﬁc to physical servers was
indeed balanced, but the VIP trafﬁc was doing just the op-
posite. To understand this phenomenon, the proﬁler further
broke down the trafﬁc by individual VIP preﬁxes under the
ToR. It turned out that only the trafﬁc destined to three VIP
preﬁxes was imbalanced. As shown in Fig 10(a), the load of
the three VIP preﬁxes on link Leaf4 → T oR is 2.6 times
that on link Leaf3 → T oR.
A follow-up investigation showed that this incident was
due to a failure in BGP route advertisement. Speciﬁcally, the
ﬁrst three leaf switches failed to advertise the routes for these
three VIP preﬁxes to some of its upstream spine switches
(Fig 10(b)). This led to the uneven split of the trafﬁc going
to the three VIP preﬁxes, starting from the spine switches
down to the leaf switches.
Load imbalance can arise due to a variety of
Summary.
reasons. The Everﬂow ECMP proﬁler supports ﬂexibly clas-
sifying the trafﬁc and providing detailed load information
per trafﬁc class. Therefore, it can not only detect load imbal-
ance incidents but also help identify their causes. The latter
cannot be easily done based on the aggregate load counters.
7.5 Low RDMA throughput
Our DCN is deploying RoCEv2-based RDMA. However,
the RDMA engineering team found that RDMA performed
poorly when there were a small amount of packet losses
(e.g., due to a switch interface bug or packet corruption).
For example, even with 0.01% loss rate, the throughput of a
RDMA ﬂow would drop below 10 Gbps (i.e., less than 25%
of the optimal throughput of 40 Gbps). Such degradation
was far worse than expected.
Initially, the engineers attempted to debug this problem by
capturing the RDMA packets on both ends of the servers us-
ing a customized tool provided by the RDMA NIC vendor.
However, due to the high data rate, the tool would impose
signiﬁcant capturing overhead on the NIC and also miss a
fraction of the packets. As a result, when the engineers ex-
amined the packet dumps and saw no bad symptom, they
could not tell whether this was because the ﬂow behavior
had changed (due to the extra capturing overhead) or be-
cause there was not enough diagnostic information. More-
over, the tool can capture only L3 packets but not L2 packets
like PFC.
We used our RDMA debugger to investigate this prob-
lem. The throughput of an RDMA ﬂow is affected by three
types of control packets: PFC (Priority-based Flow Control),
NACK (Negative Acknowledgment) and CNP (Congestion
Notiﬁcation Packet), all of which are captured by Everﬂow.
From the control packet traces, we saw an unexpected corre-
lation between the PFC and NACK packets. When a receiver
generated a NACK in response to a packet loss, it almost al-
ways generated another PFC packet with a long PAUSE pe-
riod. The effect of the PAUSE was propagated hop-by-hop
back to the sender,3 and ultimately slowed the sender.
We reported this problem to the NIC vendor who later
found a bug in their NIC driver. This bug caused the NIC to
halt the buffer, and subsequently triggered a PFC with a long
PAUSE period during the NACK generation.
Summary. Everﬂow provides a reliable and yet indepen-
dent way to observe and debug network protocol misbehav-
iors. This is particularly useful when host-based capturing
of protocol packets is too expensive (e.g., for RDMA) or un-
available (e.g., for PFC).
8. SYSTEM EVALUATION
3To avoid buffer overﬂow, a switch uses PFC to force its up-
stream neighbor (another switch or a server NIC) to PAUSE
data transmission.
488d
a
o
L
d
e
z
i
l
a
m
r
o
N
 3
 2.5
 2
 1.5
 1
 0.5
 0
3 VIP Prefixes Others
(a) Trafﬁc to 3 VIP preﬁxes is imbalanced (b) Imbalance due to preﬁx an-
failure from leaf
nouncement
switches to spine switches
Figure 10: Load imbalance incident
the mirrored trafﬁc requires 380 Gbps network bandwidth
and 300M pps CPU processing power.
Storage. We breakdown the storage overheads into coun-