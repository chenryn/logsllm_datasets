(cid:11)(cid:1)(cid:2)(cid:12)
…
(c) Schema of PSPNR lookup table after power regression
Figure 12: The schema of PSPNR lookup table.
compressed representation of the PSPNR lookup table. First, we
reduce the PSPNR lookup table from “multi-dimensional” to “one-
dimensional” (Figure 12(b)), by replacing the viewpoint speed, DoF,
luminance with the products of their multipliers (defined in §4.2).
Using their products, i.e., the action-dependent ratios (see Equa-
tion 4) to index the PSPNR lookup table, we can avoid enumerating
a large number of combinations of viewpoint speed, DoF, and lumi-
nance. Second, instead of keeping a map between action-dependent
ratios and their corresponding PSPNR of each tile, we found that
their relationship in a given tile can be interpolated by a power
function. Thus, we only need two parameters to encode the rela-
tionship between PSPNR and action-dependent ratio (Figure 12(c)).
With these optimizations, we can compress the manifest file from
10 MB to ∼50 KB for a 5-minute video.
Reducing PSPNR computation overhead: Per-frame PSPNR
calculation, in its original form (Equation 1), can be ∼ 50% slower
than encoding the same video. To reduce this overhead, we ex-
tract one frame from every ten frames and use its PSPNR as the
PSPNR of other nine frames. This saves the PSPNR computation
overhead by 90%, and we found this is as effective as per-frame
PSPNR computation.
7 IMPLEMENTATION
Here, we describe the changes needed to deploy Pano in a DASH
video delivery system. We implement a prototype of Pano with 15K
lines of codes by C++, C#, Python, and Matlab [15].
Video provider: The video provider preprocesses a 360° video
in three steps. First, we extract features from the video, such as
object trajectories, content luminance, and DoF, which are needed
to calculate the PSPNR of the video under each of history viewpoint
movements. In particular, to detect the object trajectories, we use
Yolo[54] (a neural network-based multi-class object detector) to
detect objects in the first frame of each second, and then use a
tracking logic [38] to identify the trajectory of each detected ob-
ject in the remaining of the second. Then we temporally split the
video into 1-second chunks, use the tiling algorithm described in
§5 to spatially split each chunk into N (by default, 30) tiles using
FFmpeg CropFilter [8], and encode each tile in 5 QP levels (e.g.,
{22, 27, 32, 37, 42}). Finally, we augment the video’s manifest file
401
with additional information. In particular, each tile includes the
following information (other than available quality levels and their
corresponding URLs): (1) the coordinate of the tile’s top-left pixel
(this is needed since the tiles in Pano may not be aligned across
chunks); (2) average luminance within the tile; (3) average DoF
within the tile; (4) the trajectory of each visual object (one sample
per 10 frames); and (5) the PSPNR lookup table (§6.3).
Video server: Like recent work on 360° videos [52], Pano does not
need to change the DASH video server. It only needs to change the
client-side player as described next.
Client-side adaptation: We built Pano client on a FFmpeg-based [8]
mockup implementation of the popular dash.js player [4]. To let the
client use Pano’s quality adaptation logic, we make the following
changes. First, the player downloads and parses the manifest file
from the server. We change the player’s manifest file parser to ex-
tract information necessary for Pano’s quality adaptation. Second,
we add the three new functionalities to the DASH bitrate adaption
logic. The viewpoint estimator predicts viewpoint location in the
next 1-3 seconds, using a simple linear regression over the recent
history viewpoint locations [52, 53]. Then the client-side PSPNR
estimator compares the predicted viewpoint movements with the
information of the tile where the predicted viewpoint resides (ex-
tracted from the manifest file) to calculate the relative viewpoint
speed, the luminance change, and the DoF difference. These factors
are then converted to the PSPNR of each tile in the next chunk
using the PSPNR lookup table (§6.2). Finally, after the DASH bitrate
adaptation algorithm [64] decides the bitrate of a chunk, the tile-
level bitrate allocation logic assigns quality levels to its tiles using
the logic described in §6.1.
Client-side streaming: We fetch the tiles of each chunk as sepa-
rate HTTP objects (over a persistent HTTP connection), then de-
code these tiles in parallel into separate in-memory YUV-format ob-
jects using FFmpeg, and finally stitch them together into a panoramic
frame using in-memory copy. We use the coordinates of each tile
(saved in the manifest file) to decide its location in the panoramic
frame. As an optimization, the copying of tiles into a panoramic
frame can be made efficient if the per-tile YUV matrices are copied
in a row-major manner (i.e., which is aligned with how matrices
are laid out in memory), using the compiler-optimized memcpy. As
a result, the latency of stitching one panoramic frame is 1ms.
8 EVALUATION
We evaluate Pano using both a survey-based user study and trace-
driven simulation. Our key findings are the following.
• Compared to the state-of-the-art solutions, Pano improves per-
ceived quality without using more bandwidth: 25%-142% higher
mean opinion score (MOS) or 10% higher PSPNR with the same
or less buffering across a variety of 360° video genres.
• Pano achieves substantial improvement even in the presence of
viewpoint/bandwidth prediction errors.
• Pano imposes minimal additional systems overhead and reduces
the resource consumption on the client and the server.
8.1 Methodology
Dataset: We use 50 360° videos (7 genres and 200 minutes in
total). Among them, 18 videos (also used in §2.3) have actual user
Pano: Optimizing 360° Video Streaming with a Better
Understanding of Quality Perception
Total # videos
Total length (s)
Full resolution
Frame rate
Genres (%)
50 (18 with viewpoint traces of 48 users)
12000
2880 x 1440
30
Sports (22%), Performance (20%),
Documentary (14%), other(44%)
Table 2: Dataset summary
PSPNR (360JND-based)
MOS
≤ 45
1
46-53
54-61
62-69
2
3
4
≥ 70
5
Table 3: Map between MOS and new 360JND-based PSPNR (§4)
viewpoint trajectories from a set of 48 users (age between 20 and 26).
Each viewpoint trajectory trace is recorded on an HTC Vive [21]
device. The viewpoints are refreshed every 0.05s, which is typical
to other mainstream VR devices [12, 20, 21]. Each video is encoded
into 5 quality levels (QP=22, 27, 32, 37, 42) and 1-second chunks
using the x264 codec [23]. Table 2 gives a summary of our dataset.
Baselines: We compare Pano with two recent proposals, Flare [52]
and ClusTile [68]. They are both viewport-driven, but they priori-
tize the quality within the viewport in different ways. Flare uses
the viewport location to spatially allocate different quality to the
uniform-size tiles, whereas ClusTile uses the viewport to determine
the tile shapes. Conceptually, Pano combines their strengths by ex-
tending both tiling and quality allocation using the new 360° video
quality model. As a reference point, we also consider the baseline
that streams the whole video in its 360° view. For a fair comparison,
all baselines and Pano use the same logic for viewpoint prediction
(linear regression) and chunk-level bitrate adaptation [64].
Survey-based evaluation: We run a survey-based evaluation on
20 participants. Each participant is asked to watch 7 videos of dif-
ferent genres, each played in 4 versions: 2 methods (Pano and Flare)
and 2 bandwidth conditions (explained in next paragraph). In total,
each participant watches 28 videos, in a random order. After watch-
ing each video, the participant is asked to rate their experience
on the scale of 1 to 5 [22].4 For each video, we randomly pick a
viewpoint trajectory from the 48 real traces and record the video
as if the user’s viewpoint moves along the picked trajectory with
the quality level picked by Pano or the baseline. That means Pano
can still use its viewpoint prediction to adapt quality over time.
The participants watch these recorded videos on an Oculus head-
set [63] (which generates real DoF and luminance changes). They
are advised not to move their viewpoints. Admittedly, this does
not provide the exact same experience as the users freely moving
their viewpoints. However, since each video is generated with real
dynamic viewpoint trajectories, the experience of the users would
be the same if they moved their viewpoints along the recorded tra-
jectory. Additionally, this method ensures the participants rate the
same videos and viewpoint trajectories across different streaming
systems and bandwidth conditions.
4We acknowledge that by showing a participant four versions of the same video, the
participants may tend to scale their rates on the same video from the lowest to the
highest. While we cannot entirely prevent it, we try to mitigate this potential bias by
displaying the 28 videos in a random order, so the different versions of the same video
are rarely displayed one after another, which reduces the chance that a participant
scales his/her rating in a certain way.
i
i
n
o
n
p
O
n
a
e
M
i
i
n
o
n
p
O
n
a
e
M
e
r
o
c
S
5
4
3
2
1
e
r
o
c
S
5
4
3
2
1
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Bandwidth:0.71(±0.03) Mbps
Viewport-driven
Pano(this work)
Science
Documentary
Gaming
Sports
Tourism
Adventure
Performance
Bandwidth:1.05(±0.04) Mbps
Viewport-driven
Pano(this work)
Science
Documentary
Gaming
Sports
Tourism
Adventure
Performace
Figure 13: Real user rating: Pano vs. viewport-driven streaming. The
figure summarizes the results of 20 users with the error bars showing
the standard error of means.
(a) Pano
(b) Viewport-driven
Figure 14: A snapshot of 360° video streamed by Pano and Viewport-
driven baseline.
Network throughput traces: To emulate realistic network con-
ditions, we use two throughput traces (with average throughput
at 0.71Mbps and 1.05Mbps, respectively) collected from a public
4G/LTE operator [2]. We pick these two throughput traces, because
they are high enough to allow Pano and the baselines to use high
quality where users are sensitive (e.g., areas with low JND), but not
too high that all tiles can be streamed in the highest quality.
Quality metrics: We evaluate the video quality along two metrics
that have been shown to be critical to user experience: PSPNR,
and buffering ratio. We have seen PSPNR has a stronger correla-
tion with 360° video user rating than alternative indices (Figure 8).
Table 3 maps the PSPNR ranges to corresponding MOS values.
We define buffering ratio by the fraction of time the user’s actual
viewport is not completely downloaded.
8.2 End-to-end quality improvement
Survey-based evaluation: Figure 13 compares the MOS of Pano
and the viewport-driven baseline (Flare) on the seven 360° videos.
Pano and the baseline use almost the same amount of bandwidth
(0.71Mbps or 1.05Mbps). We see that Pano receives a much higher
user rating, with 25-142% improvement. Figure 14 shows the same
snapshot under the two methods. The viewport-driven baseline
gives equally low quality to both the moving object (skier) and
the background of the viewport (not shown). In contrast, Pano
detects the user is tracking the skier and assigns higher quality
in and around the skier while giving lower quality to the static
background (which appears to move quickly to the user).
402
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Y. Guan, C. Zheng, X. Zhang, Z. Guo, J. Jiang.
R
N
P
S
P
70
65
60
55
50
45
0
R
N
P
S
P
70
65
60
55
50
45
0
Pano
Clustile
Flare
Whole Video
1
0.5
2
Buffering Ratio (%)
1.5
Better
2.5
R
N
P
S
P
70
65
60
55
50
45
0
Pano
Clustile
Flare
Whole Video
2
1
Buffering Ratio (%)
3
Better
4
R
N
P
S
P
70
65
60
55
50
45
0
Pano
Clustile
Flare
Whole Video
2
1
4
Buffering Ratio (%)
3
Better
5
R
N
P
S
P
70
65
60
55
50
45
0
Pano
Clustile
Flare
Whole Video
0.5
1
Buffering Ratio (%)
Better
1.5
(a) Sports, Trace #1
(b) Tourism, Trace #1
(c) Documentary, Trace #1
(d) Performance, Trace #1
Pano
Clustile
Flare
Whole Video
2
1
Buffering Ratio (%)
3
Better
4
R