## PostgreSQL OLTP on ZFS 性能优化   
### 作者                                                                     
digoal                   
### 日期                     
2015-12-29                    
### 标签                   
PostgreSQL , zfs , oltp , 性能优化       
----                  
## 背景                 
## 环境  
```  
  PostgreSQL 9.5 rc1  
  数据块大小为8KB  
  CentOS 6.x x64  
  zfsonlinux  
  3*aliflash  
  256G内存  
  32核 Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz  
pg_xlog on ext4  
  ext4 mount option ( defaults,noatime,nodiratime,discard,nodelalloc,data=writeback,nobarrier )  
$PGDATA on zfs  
```  
## zfs优化  
1\. 块设备对齐  
```  
fdisk -c -u /dev/dfa  
start  2048  
end  +(n*2048-1)  
或者使用parted分配，GPT+对齐  
```  
2\. 模块参数  
```  
cd /sys/module/zfs/parameters/  
```  
1\.1 关闭zfs prefetch，因为是OLTP系统，不需要prefetch。  
```  
echo 1 > zfs_prefetch_disable  
```  
1\.2 修改ARC脏页一次被刷出的单位（太大会导致evict arc脏页时中断响应，建议16MB）  
算法 :  系统内存 除以 2^zfs_arc_shrink_shift  
所以当内存为256GB是，要配置为16MB，zfs_arc_shrink_shift必须设置为14。  
```  
echo 14 > zfs_arc_shrink_shift  
```  
1\.3 对于未使用ZLOG设备的zpool，可以将sync改为always  
```  
zfs set sync=always zp1/data01  
zfs set sync=always zp1  
```  
3\. zpool 参数  
2\.1 ashift和数据库的块大小对齐。这里PostgreSQL使用了默认的8KB。  
所以ashift选择13。  
```  
2^13=8192  
zpool create -o ashift=13 zp1 dfa1 dfb1 dfc1  
```  
4\. zfs 参数  
```  
recordsize 对齐数据库块大小 = 8K  
primarycache = metadata  
secondarycache = none  
atime = off  
logbias = throughput    (直接写数据，因为没有使用ZLOG，不需要用标准的)  
```  
5\. postgresql 参数  
```  
listen_addresses = '0.0.0.0'            # what IP address(es) to listen on;  
port = 1921                             # (change requires restart)  
max_connections = 1000                  # (change requires restart)  
unix_socket_directories = '.'   # comma-separated list of directories  
shared_buffers = 32GB                   # min 128kB  
maintenance_work_mem = 512MB            # min 1MB  
autovacuum_work_mem = 512MB             # min 1MB, or -1 to use maintenance_work_mem  
dynamic_shared_memory_type = posix      # the default is the first option  
bgwriter_delay = 10ms                   # 10-10000ms between rounds  
wal_level = hot_standby  # minimal, archive, hot_standby, or logical  
synchronous_commit = off                # synchronization level;  
max_wal_size = 32GB  
max_wal_senders = 10            # max number of walsender processes  
max_replication_slots = 10      # max number of replication slots  
hot_standby = on                        # "on" allows queries during recovery  
wal_receiver_status_interval = 1s       # send replies at least this often  
hot_standby_feedback = off               # send info from standby to prevent  
random_page_cost = 1.0                  # same scale as above  
effective_cache_size = 256GB  
log_destination = 'csvlog'              # Valid values are combinations of  
logging_collector = on          # Enable capturing of stderr and csvlog  
log_checkpoints = on  
log_connections = on  
log_disconnections = on  
log_error_verbosity = verbose           # terse, default, or verbose messages  
log_timezone = 'PRC'  
datestyle = 'iso, mdy'  
timezone = 'PRC'  
lc_messages = 'C'                       # locale for system error message  
lc_monetary = 'C'                       # locale for monetary formatting  
lc_numeric = 'C'                        # locale for number formatting  
lc_time = 'C'                           # locale for time formatting  
default_text_search_config = 'pg_catalog.english'  
```  
6\. 测试PostgreSQL TPC-B  
5亿 tpc-b测试数据  
```  
pgbench -i -s 5000  
pgbench -M prepared -n -r -P 5 -c 48 -j 48 -T 7200  
```  
测试结果：  
![pic](20151229_01_pic_001.png)    
zfs下的性能约为XFS的75%.  
ZFS  
```  
transaction type: TPC-B (sort of)  
scaling factor: 5000  
query mode: prepared  
number of clients: 48  
number of threads: 48  
duration: 7200 s  
number of transactions actually processed: 54221472  
latency average: 6.370 ms  
latency stddev: 13.424 ms  
tps = 7530.645849 (including connections establishing)  
tps = 7530.676229 (excluding connections establishing)  
statement latencies in milliseconds:  
        0.006580        \set nbranches 1 * :scale  
        0.001856        \set ntellers 10 * :scale  
        0.001427        \set naccounts 100000 * :scale  
        0.002671        \setrandom aid 1 :naccounts  
        0.001598        \setrandom bid 1 :nbranches  
        0.001533        \setrandom tid 1 :ntellers  
        0.001618        \setrandom delta -5000 5000  
        0.146576        BEGIN;  
        3.357134        UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;  
        0.199865        SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
        1.036640        UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;  
        0.636415        UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;  
        0.523942        INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);  
        0.434377        END;  
```  
XFS  
```  
transaction type: TPC-B (sort of)  
scaling factor: 5000  
query mode: prepared  
number of clients: 48  
number of threads: 48  
duration: 7200 s  
number of transactions actually processed: 78512059  
latency average: 4.400 ms  
latency stddev: 10.051 ms  
tps = 10904.276312 (including connections establishing)  
tps = 10904.307274 (excluding connections establishing)  
statement latencies in milliseconds:  
        0.003500        \set nbranches 1 * :scale  
        0.000971        \set ntellers 10 * :scale  
        0.000787        \set naccounts 100000 * :scale  
        0.001327        \setrandom aid 1 :naccounts  
        0.001081        \setrandom bid 1 :nbranches  
        0.000894        \setrandom tid 1 :ntellers  
        0.000924        \setrandom delta -5000 5000  
        0.096122        BEGIN;  
        1.521620        UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;  
        0.121572        SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
        1.035498        UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;  
        0.631052        UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;  
        0.524147        INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);  
        0.450451        END;  
```  
## 附件  
1\.   
```  
#modinfo zfs  
filename:       /lib/modules/3.18.24/extra/zfs/zfs.ko  
version:        0.6.5.3-1  
license:        CDDL  
author:         OpenZFS on Linux  
description:    ZFS  
srcversion:     CEB8F91B3D53F4A2844D531  
depends:        spl,zcommon,znvpair,zavl,zunicode  
vermagic:       3.18.24 SMP mod_unload modversions   
parm:           zvol_inhibit_dev:Do not create zvol device nodes (uint)  
parm:           zvol_major:Major number for zvol device (uint)  
parm:           zvol_max_discard_blocks:Max number of blocks to discard (ulong)  
parm:           zvol_prefetch_bytes:Prefetch N bytes at zvol start+end (uint)  
parm:           zio_delay_max:Max zio millisec delay before posting event (int)  
parm:           zio_requeue_io_start_cut_in_line:Prioritize requeued I/O (int)  
parm:           zfs_sync_pass_deferred_free:Defer frees starting in this pass (int)  
parm:           zfs_sync_pass_dont_compress:Don't compress starting in this pass (int)  
parm:           zfs_sync_pass_rewrite:Rewrite new bps starting in this pass (int)  
parm:           zil_replay_disable:Disable intent logging replay (int)  
parm:           zfs_nocacheflush:Disable cache flushes (int)  
parm:           zil_slog_limit:Max commit bytes to separate log device (ulong)  
parm:           zfs_read_chunk_size:Bytes to read per chunk (long)  
parm:           zfs_immediate_write_sz:Largest data block to write to zil (long)  
parm:           zfs_dbgmsg_enable:Enable ZFS debug message log (int)  
parm:           zfs_dbgmsg_maxsize:Maximum ZFS debug log size (int)  
parm:           zfs_admin_snapshot:Enable mkdir/rmdir/mv in .zfs/snapshot (int)  
parm:           zfs_expire_snapshot:Seconds to expire .zfs/snapshot (int)  
parm:           zfs_vdev_aggregation_limit:Max vdev I/O aggregation size (int)  
parm:           zfs_vdev_read_gap_limit:Aggregate read I/O over gap (int)  
parm:           zfs_vdev_write_gap_limit:Aggregate write I/O over gap (int)  
parm:           zfs_vdev_max_active:Maximum number of active I/Os per vdev (int)  
parm:           zfs_vdev_async_write_active_max_dirty_percent:Async write concurrency max threshold (int)  
parm:           zfs_vdev_async_write_active_min_dirty_percent:Async write concurrency min threshold (int)  
parm:           zfs_vdev_async_read_max_active:Max active async read I/Os per vdev (int)  
parm:           zfs_vdev_async_read_min_active:Min active async read I/Os per vdev (int)  
parm:           zfs_vdev_async_write_max_active:Max active async write I/Os per vdev (int)  
parm:           zfs_vdev_async_write_min_active:Min active async write I/Os per vdev (int)  
parm:           zfs_vdev_scrub_max_active:Max active scrub I/Os per vdev (int)  
parm:           zfs_vdev_scrub_min_active:Min active scrub I/Os per vdev (int)  
parm:           zfs_vdev_sync_read_max_active:Max active sync read I/Os per vdev (int)  
parm:           zfs_vdev_sync_read_min_active:Min active sync read I/Os per vdev (int)  
parm:           zfs_vdev_sync_write_max_active:Max active sync write I/Os per vdev (int)  
parm:           zfs_vdev_sync_write_min_active:Min active sync write I/Os per vdev (int)  
parm:           zfs_vdev_mirror_switch_us:Switch mirrors every N usecs (int)  
parm:           zfs_vdev_scheduler:I/O scheduler (charp)  
parm:           zfs_vdev_cache_max:Inflate reads small than max (int)  
parm:           zfs_vdev_cache_size:Total size of the per-disk cache (int)  
parm:           zfs_vdev_cache_bshift:Shift size to inflate reads too (int)  
parm:           metaslabs_per_vdev:Divide added vdev into approximately (but no more than) this number of metaslabs (int)  
parm:           zfs_txg_timeout:Max seconds worth of delta per txg (int)  
parm:           zfs_read_history:Historic statistics for the last N reads (int)  
parm:           zfs_read_history_hits:Include cache hits in read history (int)  
parm:           zfs_txg_history:Historic statistics for the last N txgs (int)  
parm:           zfs_flags:Set additional debugging flags (uint)  
parm:           zfs_recover:Set to attempt to recover from fatal errors (int)  
parm:           zfs_free_leak_on_eio:Set to ignore IO errors during free and permanently leak the space (int)  
parm:           zfs_deadman_synctime_ms:Expiration time in milliseconds (ulong)  
parm:           zfs_deadman_enabled:Enable deadman timer (int)  
parm:           spa_asize_inflation:SPA size estimate multiplication factor (int)  
parm:           spa_slop_shift:Reserved free space in pool (int)  
parm:           spa_config_path:SPA config file (/etc/zfs/zpool.cache) (charp)  
parm:           zfs_autoimport_disable:Disable pool import at module load (int)  
parm:           spa_load_verify_maxinflight:Max concurrent traversal I/Os while verifying pool during import -X (int)  
parm:           spa_load_verify_metadata:Set to traverse metadata on pool import (int)  
parm:           spa_load_verify_data:Set to traverse data on pool import (int)  
parm:           metaslab_aliquot:allocation granularity (a.k.a. stripe size) (ulong)  
parm:           metaslab_debug_load:load all metaslabs when pool is first opened (int)  
parm:           metaslab_debug_unload:prevent metaslabs from being unloaded (int)  
parm:           metaslab_preload_enabled:preload potential metaslabs during reassessment (int)  
parm:           zfs_mg_noalloc_threshold:percentage of free space for metaslab group to allow allocation (int)  
parm:           zfs_mg_fragmentation_threshold:fragmentation for metaslab group to allow allocation (int)  
parm:           zfs_metaslab_fragmentation_threshold:fragmentation for metaslab to allow allocation (int)  