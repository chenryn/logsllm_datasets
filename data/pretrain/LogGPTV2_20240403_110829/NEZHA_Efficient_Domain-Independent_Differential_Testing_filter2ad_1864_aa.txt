title:NEZHA: Efficient Domain-Independent Differential Testing
author:Theofilos Petsios and
Adrian Tang and
Salvatore J. Stolfo and
Angelos D. Keromytis and
Suman Jana
2017 IEEE Symposium on Security and Privacy
NEZHA: Efﬁcient Domain-Independent Differential Testing
Theoﬁlos Petsios∗, Adrian Tang∗, Salvatore Stolfo, Angelos D. Keromytis and Suman Jana
Department of Computer Science
Columbia University, New York, USA
{theoﬁlos, atang, sal, angelos, suman}@cs.columbia.edu
Abstract—Differential testing uses similar programs as cross-
referencing oracles to ﬁnd semantic bugs that do not exhibit
explicit erroneous behaviors like crashes or assertion failures.
Unfortunately, existing differential testing tools are domain-
speciﬁc and inefﬁcient, requiring large numbers of test inputs
to ﬁnd a single bug. In this paper, we address these issues by
designing and implementing NEZHA, an efﬁcient input-format-
agnostic differential testing framework. The key insight behind
NEZHA’s design is that current tools generate inputs by simply
borrowing techniques designed for ﬁnding crash or memory
corruption bugs in individual programs (e.g., maximizing code
coverage). By contrast, NEZHA exploits the behavioral asymme-
tries between multiple test programs to focus on inputs that are
more likely to trigger semantic bugs. We introduce the notion of
δ-diversity, which summarizes the observed asymmetries between
the behaviors of multiple test applications. Based on δ-diversity,
we design two efﬁcient domain-independent input generation
mechanisms for differential testing, one gray-box and one black-
box. We demonstrate that both of these input generation schemes
are signiﬁcantly more efﬁcient than existing tools at ﬁnding
semantic bugs in real-world, complex software.
NEZHA’s average rate of ﬁnding differences is 52 times and 27
times higher than that of Frankencerts and Mucerts, two popular
domain-speciﬁc differential testing tools that check SSL/TLS
certiﬁcate validation implementations, respectively. Moreover,
performing differential testing with NEZHA results in 6 times
more semantic bugs per tested input, compared to adapting
state-of-the-art general-purpose fuzzers like American Fuzzy Lop
(AFL) to differential testing by running them on individual test
programs for input generation.
NEZHA discovered 778 unique, previously unknown discrep-
ancies across a wide variety of applications (ELF and XZ
parsers, PDF viewers and SSL/TLS libraries), many of which
constitute previously unknown critical security vulnerabilities. In
particular, we found two critical evasion attacks against ClamAV,
allowing arbitrary malicious ELF/XZ ﬁles to evade detection. The
discrepancies NEZHA found in the X.509 certiﬁcate validation
implementations of the tested SSL/TLS libraries range from
mishandling certain types of KeyUsage extensions, to incorrect
acceptance of specially crafted expired certiﬁcates, enabling man-
in-the-middle attacks. All of our reported vulnerabilities have
been conﬁrmed and ﬁxed within a week from the date of
reporting.
I. INTRODUCTION
Security-sensitive software must comply with different high-
level speciﬁcations to guarantee its security properties. Any
semantic bug that causes deviations from these speciﬁcations
might render the software insecure. For example, a malware
detector must parse input ﬁles of different formats like ELF
(the default executable format in Linux/Unix-based systems),
∗Joint primary student authors.
© 2017, Theofilos Petsios. Under license to IEEE.
DOI 10.1109/SP.2017.27
615
PDF, or XZ (a popular archive format), according to their
respective speciﬁcations, in order to accurately detect mali-
cious content hidden in such ﬁles [41]. Similarly, SSL/TLS
implementations must validate X.509 certiﬁcates according to
the appropriate protocol speciﬁcations for setting up a secure
connection in the presence of network attackers [24], [33].
However, most semantic bugs in security-sensitive software
do not display any explicitly erroneous behavior like a crash
or assertion failure, and thus are very hard to detect without
speciﬁcations. Unfortunately, speciﬁcations, even for highly
critical software like SSL/TLS implementations or popular
ﬁle formats like ELF, are usually documented informally in
multiple sources such as RFCs and developer manuals [10]–
[18], [20], [62], [63]. Converting these informal descriptions
to formal invariants is tedious and error-prone.
Differential testing is a promising approach towards over-
coming this issue. It ﬁnds semantic bugs by using differ-
ent programs of the same functionality as cross-referencing
oracles, comparing their outputs across many inputs: any
discrepancy in the programs’ behaviors on the same input is
marked as a potential bug. Differential testing has been used
successfully to ﬁnd semantic bugs in diverse domains like
SSL/TLS implementations [24], [32], C compilers [65], and
JVM implementations [31]. However, all existing differential
testing tools suffer from two major limitations as described
below.
First, they rely on domain-speciﬁc knowledge of the in-
put format to generate new test inputs and, therefore, are
brittle and difﬁcult to adapt to new domains. For instance,
Frankencerts [24] and Mucerts [32] incorporate partial gram-
mars for X.509 certiﬁcates and use domain-speciﬁc mutations
for input generation. Similarly, existing differential
testing
tools for C compilers, Java virtual machines, and malware
detectors, all include grammars for the respective input format
and use domain-speciﬁc mutations [31], [41], [65].
Second, existing differential testing tools are inefﬁcient at
ﬁnding semantic bugs, requiring large numbers of inputs to
be tested for ﬁnding each semantic bug. For example,
in
our experiments, Frankencerts required testing a total of 10
million inputs to ﬁnd 10 distinct discrepancies, starting from a
corpus of 100, 000 certiﬁcates. Mucerts, starting from the same
100, 000 certiﬁcates, reported 19 unique discrepancies, using
2, 660 optimized certiﬁcates it generated from the corpus, but
required six days to do so.
In this paper, we address both the aforementioned prob-
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
lems by designing and implementing NEZHA1, a differential
testing tool that uses a new domain-independent approach
for detecting semantic bugs. NEZHA does not require any
detailed knowledge of the input format, but still signiﬁcantly
outperforms existing domain-speciﬁc approaches at ﬁnding
new semantic bugs.
Our key observation is that existing differential testing tools
ignore asymmetries observed across the behaviors of all tested
programs, and instead generate test inputs simply based on the
behaviors of individual programs in isolation. For instance,
Mucerts try to maximize code coverage solely on a single
program (e.g., OpenSSL) to generate inputs. However, this
approach cannot efﬁciently ﬁnd high numbers of unique se-
mantic bugs since all information on the differences each input
might introduce across the tested programs is ignored. As a
result, despite using domain-speciﬁc guided input generation,
existing differential testing tools are inefﬁcient. In this paper,
we address this issue by introducing the notion of δ-diversity
—a method for summarizing the behavioral asymmetries of
the tested programs. Under δ-diversity guidance, these asym-
metries can be expressed in different ways, examining each
individual program’s behavior in either a black-box (based on
program log/warning/error messages, program outputs, etc.) or
gray-box (e.g., program paths taken during execution) manner.
The main difference between our approach and prior dif-
ferential testing tools is that we generalize the tracking of
guidance information across all tested programs, examining
their behaviors relative to each other, not in isolation, for
guided input generation. For example, if two test programs
execute paths p1 and p2, respectively, for the same input, a "δ-
diversity-aware" representation of the execution will consist of
the tuple (cid:2)p1, p2(cid:3). Our guidance mechanism for input gener-
ation is designed to maximize δ-diversity, i.e., the number of
such tuples. We demonstrate in Section V that our scheme is
signiﬁcantly more efﬁcient at ﬁnding semantic bugs than using
standalone program testing techniques. We compare NEZHA
with Frankencerts, Mucerts, as well as with two state-of-the-art
fuzzers, namely AFL [66] and libFuzzer [4]. In our testing of
certiﬁcate validation using major SSL/TLS libraries, NEZHA
ﬁnds 52 times, 27 times, and 6 times more unique semantic
bugs than Frankencerts, Mucerts, and AFL respectively.
NEZHA is input-format-agnostic and uses a set of initial
seed inputs to bootstrap the input generation process. Note
that
the seed ﬁles themselves do not need to trigger any
semantic bugs. We empirically demonstrate that NEZHA can
efﬁciently detect subtle semantic differences in large, complex,
real-world software. In particular, we use NEZHA for testing:
(i) ELF and XZ ﬁle parsing in two popular command-line
applications and the ClamAV malware detector, (ii) X.509
certiﬁcate validation across six major SSL/TLS libraries and
(iii) PDF parsing/rendering in three popular PDF viewers.
NEZHA discovered 778 distinct discrepancies across all tested
families of applications, many of which constitute previously
1Nezha [5] is a Chinese deity commonly depicted in a “three heads and
six arms” form. His multi-headed form is analogous to our tool, which peers
into different programs to pinpoint discrepancies.
616
unknown security vulnerabilities. For example, we found two
evasion attacks against ClamAV, one for each of the ELF and
XZ parsers. Moreover, NEZHA was able to pinpoint 14 unique
differences even among forks of the same code base like the
OpenSSL, LibreSSL, and BoringSSL libraries.
testing tool
In summary, we make the following contributions:
• We introduce the concept of δ-diversity, a novel scheme
that tracks relative behavioral asymmetries between mul-
tiple test programs to efﬁciently guide the input genera-
tion process of differential testing.
• We build and open-source NEZHA, an efﬁcient, domain-
independent differential
that signiﬁcantly
outperforms both existing domain-speciﬁc tools as well
as domain-independent fuzzers adapted for differential
testing.
• We demonstrate that NEZHA is able to ﬁnd multiple
previously unknown semantic discrepancies and security
vulnerabilities in complex real-world software like SS-
L/TLS libraries, PDF viewers, and the ClamAV malware
detector.
The rest of the paper is organized as follows. We provide
a high-level overview of our techniques with a motivating
example in Section II. Section III details our methodology.
We describe the design and implementation of NEZHA in
Section IV and present the evaluation results of our system
in Section V. We highlight selected case studies of the bugs
NEZHA found in Section VI. Finally, we discuss related work
in Section VII, future work in Section VIII, and conclude in
Section X.
A. Problem Description
II. OVERVIEW
Semantic bugs are particularly dangerous for security-
sensitive programs that are designed to classify inputs as either
valid or invalid according to certain high-level speciﬁcations
(e.g., malware detectors parsing different ﬁle formats or SS-
L/TLS libraries verifying X.509 certiﬁcates). If an input fails
to conform to these speciﬁcations, such programs typically
communicate the failure to the user by displaying an error
code/message. For the rest of the paper, we focus on using
differential testing to discover program discrepancies in this
setting, i.e., where at least one test program validates and ac-
cepts an input and another program with similar functionality
rejects the same input as invalid. Attackers can exploit this
class of discrepancies to mount evasion attacks on malware
detectors. They can also compromise the security guarantees of
SSL/TLS connections by making SSL/TLS implementations
accept invalid certiﬁcates.
B. A Motivating Example
To demonstrate the basic principles of our approach, let
us consider the following example: suppose A and B are
two different programs with similar functionality and that
checkVer_A and checkVer_B are the functions validating
the version number of the input ﬁles used by A and B respec-
tively, as shown in Figure 1. Both of these functions return
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
1
2
3
4
5
6
7
int checkVer_A(int v) {
  if (v % 2 != 0)
    return -1;
  if (v  7)
    return -2;
  return 0;    
}
1
2
3
4
5
6
7
int checkVer_B(int v) {
  if (v  7)
    return -2;
  if (v % 2 != 0)
    return -1;
  return 0;    
}
A1
true
return 
−1
v  7
B1
true
return 
−2
v % 2 != 0
A3
false
v  7
A2
true
return 
−2
A4
false
return 
0
checkVer_A
B3
false
v % 2 != 0
B4
false
return 
0
B2
true
return 
−1
checkVer_B
Fig. 1: (Top) Simpliﬁed example of a semantic discrepancy and
(Bottom) the corresponding simpliﬁed Control Flow Graphs.
0 to indicate a valid version number or a negative number
(−1 or −2) to indicate an error. While almost identical, the
two programs have a subtle discrepancy in their validation
behavior. In particular, checkVer_A accepts an input of v=2
as valid while checkVer_B rejects it with an error code of
-2.
The above example, albeit simpliﬁed,
is similar to the
semantic bugs found in deployed, real-world applications.
This leads us to the following research question: how can
NEZHA efﬁciently generate test inputs that demonstrate dis-
crepancies between similar programs? Our key intuition is
that simultaneously testing multiple programs on the same
input offers a wide range of information that can be used
to compare the tested programs’ behaviors relative to each
other. Such examples include error messages, debug logs,
rendered outputs, return values, observed execution paths of
each program, etc. Semantic discrepancies across programs
are more likely for the inputs that cause relative variations of
features like the above across multiple test programs. Adopting
an evolutionary algorithm approach, NEZHA begins with a
corpus of seed inputs, applies mutations to each input in the
corpus, and then selects the best-performing inputs for further
mutations. The ﬁtness of a given input is determined based on
the diversity it introduces in the observed behaviors across the
tested programs. NEZHA builds upon this notion of differential
diversity, utilizing two different δ-diversity guidance engines,
one black-box and one-gray box.
1) Scenario 1: Gray-box Guidance: If program instrumen-
tation is a feasible option, we can collect detailed runtime
execution information from the test programs, for each input.
For instance, knowledge of the portions of the Control Flow
Graph (CFG) that are accessed during each program execution,
can guide us into only mutating the inputs that are likely
to visit new edges in the CFG. An edge in a CFG exists
between two basic blocks if control may ﬂow from one basic
block to the other (e.g., A1 is an edge in the simpliﬁed CFG
for checkVer_A as shown in Figure 1). We illustrate how
this information can be collectively tracked across multiple
programs revisiting the example of Figure 1.
Suppose that our initial corpus of test ﬁles (seed corpus)
consists of three input ﬁles, with versions 7, 0, and 1 (I0 =
{7, 0, 1}). We randomly extract one input from I0 to start
our testing: suppose the input with v=7 is selected and then
passed to both checkVer_A and checkVer_B. As shown
in Table I, the execution paths for programs A and B (i.e.,
the sequence of unique edges accessed during the execution
of each program) are {A1} and {B3, B2} respectively. The
number of edges covered in each program is thus 1 and
2 for A and B respectively, whereas the coverage achieved