absence of an adversary, security issues identified in this paper can
also cause accidental misconfiguration leading to emergent trust
violations.
Figure 3: Control flow and disparity in information.
4 THE SEMANTIC GAP
As described in Section 2.4, the SDN uses several different data-
stores in which different types of data are stored. Unfortunately,
the underlying specification for these datastores, as determined by
the Network Management Datastore Architecture (NMDA) [31],
lacks two critical considerations. First, it does not propose a way
for applications interacting with one datastore to have guarantees
that their information will actually be synchronized to another
datastore, and second, it does not provide any functionality for the
tracking of the ownership of information.
These design drawbacks of NMDA result in a design-level se-
mantic gap in SDN and manifest in symptoms of both inconsistent
network states and to denial of service attacks. In this section,
we discuss the nature of this semantic gap and present a semi-
automated tool that can help in probing for potential vulnerabilities
spawning from it.
4.1 The Problem
Figure 3 describes the flow of control and data in an SDN envi-
ronment. Inside the controller (middle of Figure 3), there are two
datastores: one called configuration, for the desired network state,
and one called operational, for the actual network state. SDN ap-
plications, via the northbound API, communicate network state
changes to the controller, which the controller first places in the
configuration datastore. Controller services then apply these net-
work state changes into the actual network via the southbound
API (commonly, OpenFlow). Later, other services in the controller
request information about the state of the actual network devices
to update the operational datastore.
As Figure 3 demonstrates, we consider three different semantic
levels in the SDN environment: application semantics, controller
semantics, and network semantics. This idea of semantics captures
the notion that a request by an application asking the controller
to insert a flow rule has a semantic meaning to that application:
It wants that flow rule inserted in the network so that it can im-
pact allowed communications. The controller semantics handle
the managing of application network change events, programming
4
StartupcandidaterunningconfigurationoperationalApplication semanticsDifferent trust boundaries in different planesLatency increase during transfer of configurationController semanticsNetwork semanticsvxwyuAccepted submission #476 to ACM CCS 2018 (PREPRINT, NOT FINAL)
of switches, and monitoring of switches. The network semantics
define the actual state of the network switches.
This gap between the layers is the semantic gap problem, and
there are three key causes: (1) information disparity, (2) blurred
responsibilities, and (3) unreliable service chaining.
4.1.1
Information Disparity. In an ideal scenario, when an
application issues a network change request, it is expected that
the network will be configured as and when intended. In fact, the
application semantics expect and demand this behavior. If there is a
temporal delay (caused by server load, network load, or adversarial
behavior) in the controller issuing the network change request to
the actual network, then this can lead to an inconsistent network
state.
For instance, if the administrator disables a terminated employee’s
machine’s network access through the firewall application, and the
firewall application asks the controller to implement the desired
flow rule, but the flow rule is delayed or even dropped, then the
firewall application and the administrator have an inconsistent
view of the network state.
4.1.2 Blurred Responsibilities. Another key aspect of the se-
mantic gap problem is the blurred responsibilities in the datastores.
Consider Figure 4, which shows a user producing rules. A service
called the SAL Add-Flow controller module adds these rules to
the configuration datastore. At a later point, the controller’s Flow
Programmer module adds the rules to the switches. Finally, the
switch’s rules are queried by the OpenFlow plugin and stored in
the operational datastore. There is a fundamental question at this
point: Who owns the rules? The User, the SAL Add-Flow, the Flow
Programmer, or the OpenFlow plugin? If the rule has a timeout,
who is responsible for deleting the rule after the timeout?
The implications of blurred responsibility lead to either the sub-
sequent tasks being done twice or not being done at all. The former
poses performance issues when one or more applications perform
repetitive tasks. The latter has serious implications as it leads to
lack of action and an inconsistent network state.
Additionally, such faulty or unintended configuration can have
cascading affects on the network. Hong et al. [13] poison the topol-
ogy information and demonstrate its global impact on network and
functionality of other applications. As we demonstrate in this paper,
most of the controllers in the market leverage this design and are
prone to inconsistent network states (whether forced or accidental).
4.1.3 Unreliable Service Chaining. When an application re-
quests a network change, there are several SDN services that act
on that request, and the application expects and requires that all
the services act on the request in the intended order. In a similar
fashion, if an application requests a series of network changes in
order, they expect those changes to act in that order. However, the
datastores fundamentally lack synchronization measures for ensur-
ing a chained sequence of actions, which can cause an inconsistent
network state.
In fact, Xu et al. [35] showed that logic flaws (race conditions) in
applications developed for SDN controllers can be exploited from
a remote location and can lead to a compromised network as a
result of unreliable service chaining. We argue that race conditions
5
Figure 4: Ownership issues (mixed patterns show conflicts).
in SDN applications is one symptom of the underlying unreliable
service chaining problem.
4.2 Probing the Semantic Gap
As mentioned Section 3, datastore-based vulnerabilities can be ex-
ploited in both forced or accidental situations to trigger either
an inconsistent network state or denial of service. To automati-
cally identify possible datastore-based vulnerabilities, we designed
a systematic procedure to exploit the semantic gap problem and
implemented it into a tool.
4.2.1 Threat Detection Methodology. We propose a system-
atic SDN-fuzzer to perform black-box fuzzing of mainstream SDN
controllers: OpenDayLight and Open Network Operating System.
Unlike existing work [34, 36], we do not attempt to impact the per-
formance of the controller by merely flooding it with random traffic.
Instead, we acquire a list of critical services involving datastores,
analyze them to expose their entry points, and selectively target
the datastores by fuzzing the communication channels described
as part of our threat model (Section 3).
The fuzzer is provided with a list of services to be inspected. It
iteratively detects the interfaces exposed by each service by check-
ing the response header of the RESTful requests (GET, POST, PUT,
DELETE, UPDATE) made to the service. If a response such as "HTTP-
405: Method Not Allowed" is received, it is inferred that service has
disabled certain operations. This response is crucial for the fuzzer
as it is consumed to infer the kind of datastore (configuration/oper-
ational) the service uses.
According to the NMDA rule, the operational datastore cannot
be configured (no POST, DELETE, etc.) from the northbound ap-
plications but can be read (GET) by all authorized applications.
Conversely, the configuration datastore can be both read and mod-
ified by all applications. As an example, a flow statistics service
provides dynamic updates of network traffic and thus sends back
the information stored in the operational (state) datastore. Because
this information is stored only in the operational datastore, a GET
request to a configuration datastore for statistics will result in
a HTTP-405 error. Similarly, when a PUSH request for the flow
SAL Add-FlowUser Flow ProgrammerProducerConsumerRule 1Configuration DatastoreRule 3Rule 4Rule 2Rule 1Rule 3Rule 4Rule 2Operational DatastoreProducerConsumerProducerConsumerRule 1JsonBatch UpdateRule 4Rule 3Rule 2OpenFlowPluginRule 1Rule 1Accepted submission #476 to ACM CCS 2018 (PREPRINT, NOT FINAL)
Table 1: HTTP response codes and inference.
Code
200
401
404
405
429
500
503
507
Response reason
Request successful
Unauthorized
Not found
Method not allowed
Too many requests
Internal server error
Service unavailable
Insufficient storage
Inference
Datastore found
Wrong credentials
Datastore not supported
Datastore with limited features
Rate limiting measures present
Exceptions, crashes, errors
Latency and deadlocks
Resource cruch
programmer service is made for a configuration datastore, a suc-
cess HTTP-200 message is received. However, the same request for
the operational datastore will result in a HTTP-405 error, and it is
inferred that the service does not involve the operational datastore
and is used only for configurational purposes.
In Table 1, we list the critical responses which the fuzzer receives
from the services in the SDN controller and the inference that is
derived. The fuzzer incorporates an input generator engine, which
automatically creates inputs in the supported format (e.g., JSON)
and issues HTTP requests to the given URL.
The response returned is interpreted and analyzed by the analysis
engine. Finally, for successful responses (HTTP-200), the fuzzer
checks the state of the network to confirm the consistency of the
network as was intended from the configuration.
If a mismatch between the applied configuration and expected
configuration is detected, this is a inconsistent network state.
To identify the root cause, we manually examine the container
logs and attempt to reproduce the problem. We also rerun the tests
for inputs that cause misconfiguration in the system to determine
the persistence and impact of the problem. Recoverable crashes
(change of HTTP code from 500 to 200) are considered less harmful
than the irrecoverable shutdown of services. Similarly, runtime
exceptions are considered less fatal than a crash.
5 IDENTIFIED VULNERABILITIES
In this section, we evaluate SDN-fuzzer and present our results
based on the security properties and the vulnerability classes that
were exploited during the experiments on mainstream SDN con-
trollers. Our experimental setup consisted of the SDN controllers
(ODL [27] and ONOS [25]), a real network (university datacenter),
a simulated network (mininet [33]), and the fuzzer. The SDN con-
trollers had roughly 724 installed services (features), and we actively
tracked the impact of fuzzing on 77 critical services. Core services
which were impacted are mentioned in Table 3. The extent of these
attacks in different platforms which implement the NMDA datastore
design manifesting in its vulnerabilities is shown in Table 5.
5.1 Attacks on Availability
In SDN controllers, the semantic gap problems discussed in Sec-
tion 4.1 aggravate the central-point of failure of SDN by exposing
security vulnerabilities which impact the availability of a network.
The performance of the SDN controller can be impacted in two
• Northbound attack: As per the threat model (Section 3), the
northbound communication with the SDN controller is for
programming or monitoring the network which requires
ways depending on the threat source and the attack surface:
applications to store the configuration in the datastores.
Unchecked storage and improper management of the stored
information can lead to memory overflows and impact the
controllers’ availability.
• Southbound attack: The forwarding plane can generate events
not triggered by the controller (e.g., host and switch migra-
tion, switch reboots, or manual device configuration) which
are updated in the operational datastore. This leads to perfor-
mance overhead in the southbound channel and consump-
tion of memory resources of the controller.
For the communications that happen at the northbound API, both
read and write controls for the configuration datastore are exposed
to applications. Also, as described in Section 4.1.2, there is a blurred
sense of ownership of the configuration stored in the configuration
datastore. This arrangement means that services/applications inside
the controller do not have the responsibility to clean and manage
the configuration after use, and they depend on someone else to
do it. As part of our experiments, we leveraged an application
with RESTful privileges to install configuration (flow rules) in the
SDN controllers which support the datastore model. There is no
threshold or limit of flows that an application can install. Also, the
SDN controllers will always accept a new configuration.
AT-1 (Northbound channel overflow): We installed applica-
tions and attacked the services in a distributed fashion to evade
detection. If an application is allowed to send unchecked amounts
of configuration, it impacts the overall latency to serve similar
requests and at some point in time causes service unavailability
(HTTP-503). We validated the latency impact on RESTful configura-
tions on an SDN controller with two different hardware capabilities
as shown in Figure 5. At the time of this writing, no SDN controllers
had implemented preventive measures to implement rate limiting
as shown in Table 5.
]
s
m
[
y
c
n
e
t
a
L
90
80
70
60
50
40
30
20
10
0
Open Networking Operating System
32 cores, 64GB
4 cores, 4GB
0
20
40
60
Time [min]
80
100
]
s
m
[
y
c
n
e
t
a
L
90
80
70
60
50
40
30
20
10
0
OpendayLight
32 cores, 64GB
4 cores, 4GB
shutdown
0
20
40
60
Time [min]
80
100
(a) Latency surge in ONOS.
(b) Latency surge in ODL.
Out_of_Memory_Error ( o s _ l i n u x . cpp : 2 6 4 3 ) , pid =31631 ,
JRE v e r s i o n : OpenJDK Runtime Environment
Java VM: OpenJDK 64− B i t S e r v e r VM (25.151 − b12 mixed mode linux −amd64
u151−b12−0ubuntu0 . 1 6 . 0 4 . 2 − b12 )
t i d =0 x 0 0 0 0 7 f b 0 4 e b f 7 7 0 0
( b u i l d 1 . 8 . 0 _151−8
( 8 . 0 _151−b12 )
compressed oops )
(c) Controller shutdown in ODL.
Figure 5: Flooding attack on configuration datastores.
AT-2 (Persistence): In our experiments using mutated flows to
fuzz the configuration datastore, we discovered issues with man-
agement of stored information. We found that the configuration
(active or inactive) persists for an indefinite amount of time inside
6
Accepted submission #476 to ACM CCS 2018 (PREPRINT, NOT FINAL)
Table 2: Summary of service disruptions while configuring operational network.
No. of rules
Time
Tracked services
(total - 724)
25 (default)
20000
38400
54000
60000
0
25
50
75
77
77
68
61
100
0 (system crash)
Impact on Services
Dead
Exception
0
4
Crash
0