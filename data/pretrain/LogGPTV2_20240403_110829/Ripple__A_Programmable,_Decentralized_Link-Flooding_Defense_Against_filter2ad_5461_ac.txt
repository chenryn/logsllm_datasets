Next, we describe how the Ripple compiler generates switch
programs to enforce the defense policies in a fully decentral-
ized manner. The compiler analyzes the policy to generate
switch-local defense programs in P4, and it augments these
programs with a runtime protocol that synchronizes switch-
local views and constructs the network-wide panorama.
4.1 Programmable switch primitives
Ripple compiles switch-local defense programs leveraging the
following hardware primitives. The Ripple switch programs
can process every single packet without downsampling.
Stateful registers. A programmable switch has several
megabytes of SRAM, and a P4 program can allocate register
arrays from stateful memory. The registers can be indexed,
read from, and written to on a per-packet basis.
ALUs and hash units. Programmable switches have Arith-
metic Logic Units (ALUs) that can operate on packet headers
and register data. P4 programs can perform arithmetic and
bitwise operations, as well as CRC hash and checksum func-
tions at Tbps linespeed. Ripple uses these building blocks to
compute defense decisions locally at every switch.
Ripple generates the runtime synchronization protocol using
the following hardware features. The synchronization proto-
col runs between the programmable switches to construct the
panoramic view.
Programmable parsers. P4 programs can deﬁne new header
and protocol types using programmable switch parsers and
deparsers. New protocols are fully compatible with TCP/IP
trafﬁc, as a P4 switch recognizes each protocol type when
parsing packet headers, and activate different processings as
needed.
Trafﬁc generator. Programmable switches have hardware-
based trafﬁc generators that can serve as an out-of-band trafﬁc
source. The generators can further be conﬁgured to send
packets of customized formats at prescribed rates. Ripple
uses this for generating synchronization protocol messages.
4.2 Panoramic data structures
All panoramic variables are backed by a uniform represen-
tation: key/value stores (KVS). Switch-local programs can
access any panoramic variable as if it is locally present using
KVS-based APIs. Concretely, a panoramic variable pv can
be indexed by a key k: pv(k) returns a value v associated
with that key, or nil if k does not exist. The KVS size can be
obtained by pv.sz, which returns the number of distinct keys
in the KVS. An API call pv.isempty will return a binary value
indicating whether or not the KVS size is zero. In the policy
language, the panoramic variables are accessed in a declar-
ative though equivalent form—e.g., k in pv is equivalent to
pv(k). The key k to the pv is a vector of packet headers. In
3870    30th USENIX Security Symposium
USENIX Association
fact, a policy may potentially reference pv using any header
ﬁeld; so by default, k contains all physical headers and virtual
headers that are deﬁned by the policy as relevant to the attack.
However, under the hood, not all headers will be materialized
in pv—the Ripple compiler chooses the best implementation
for each variable, depending on its size, access methods, and
the packet headers referenced.
Inferring access keys. Although policy programs can
freely access any virtual/physical header in a pv, in practice
most headers may not be relevant to the policy. Therefore, the
Ripple compiler optimizes away unreferenced header ﬁelds
and only preserves the access keys. For instance, most de-
tection policies produce a pv called ‘victimLks’, which in
principle contains all packet headers on congested links; how-
ever, the compiler will detect that only link IDs (a virtual
header) are accessed elsewhere in the policies, so the result-
ing pv is only keyed on link ID. This minimizes the amount
of data that needs to be synchronized, and also guides the
compiler to infer how large the KVS may be.
Inferring sizes. The Ripple compiler infers the size of the
panoramic KVS by checking which packet headers are used
as access keys. The key range (e.g., link IDs vs. source IPs)
will determine the upperbound of a KVS size, and Ripple uses
this as optimization hints to choose the best implementation.
In its simplest form, a KVS is backed by a register array,
which is natively supported by programmable switches. Here,
the KVS size grows linearly with the number of keys, but
it maintains exact information for each key. If larger KVS
sizes are needed, Ripple will dynamically choose between
a count-min sketches (CMS) [23] or bloom ﬁlters (BF) [17],
which are approximate data structures that trade off accuracy
for space efﬁciency. These data structures support count and
membership queries, respectively, but their sizes do not in-
crease with key insertions. Rather, they use constant memory
and may produce overcounting (in CMS) or false positives (in
BF). Nevertheless, the accuracy/efﬁciency tradeoff provides
strong theoretical guarantees and has been proven effective
for network monitoring tasks [23]. The Ripple compiler uses
them to back pv’s with arbitrary key counts. It further chooses
an implementation based on the access methods.
Inferring access methods. The Ripple compiler checks
how a panoramic variable pv is accessed by the policies. (a)
If pv is never accessed in any policy body—e.g., ‘mitiga-
tion’ in most of the policies is not further accessed by other
policies—no panoramic KVS will be instantiated by the com-
piler. (b) If pv is only accessed by .isempty, the compiler
only maintains a binary value using a single register. (c) If
pv is further accessed by .sz, Ripple maintains the distinct
keys and key counts but it abstracts away the values using the
BF implementation. (d) If pv(k) is invoked in a policy, then
depending on whether pv(k) is used as a binary check or for
arithmetic computation, Ripple uses a BF or CMS:
For both BF and CMS, the input key is a set of packet headers.
The headers are hashed using k CRC functions to produce an
index to each register array. CMS arrays contain counts, and
an insertion will increment k elements, one in each array. BF
arrays have binary entries, and an insertion will set k elements
to one. The same headers are used for querying the KVS, and
the same k indexed will be computed by the hash units. The
CMS will return the minimum of all k counts as the estimated
count, and the BF will return the logical AND of the queried
values (1: key exists, 0: key does not exist).
4.3 Extracting local panorama fragments
Next, we describe how the Ripple compiler generates P4 pro-
grams to extract switch-local threat signals. These fragments
will later be synchronized across the network to construct the
global panorama. The compiler analyzes each operator in the
policy sequentially, and generates P4 programs to examine
every single packet and ﬁlter out attack-unrelated signals.
map applies a function to input packet headers, and
generates one or more new headers. For
instance,
‘map(sip,pref,f_pref24)’ takes the source IP of a packet, ap-
plies ‘f_pref24’ to identify the /24-preﬁx, and generates a
virtual header ‘pref’ for the output:
In the input and output header stacks, italic variables (lnk,
pref ) are virtual headers. We also show (much simpliﬁed) P4
program snippets for computing the IP preﬁx from source IP
and generating a new header.
ﬁlter checks header values against a predicate, and generates
a binary header f t that indicates whether or not the current
packet is relevant for the defense. The Ripple policy acts on
every single packet, so all packets have f t = T when they
enter the switch. Once a ﬁlter decides that the packet does not
require further consideration, it sets the virtual header to F:
The switch program always checks the f t header before any
defense processing. Packets that are ﬁltered out will only
receive forwarding related processing.
when is a control ﬂow operator used for branching behaviors.
All following statements after a ‘when’ (and before the next
USENIX Association
30th USENIX Security Symposium    3871
register array 0register array k…crc0crck…hdrmin/andinputoutputheaders_insipdiplnkstruct metadata {bit pref;   } if (hdr.sip.isValid()){pref=sip&0xffffff00} // control ingresssz…sipdiplnkszprefheaders_outheaders_insipsptlnkif (hdr.ft== true) { if (hdr.spt!= 80) {hdr.ft= false}} /* if ft == false skip defense logic */szsipsptlnkszftheaders_outft‘when’) are only executed if the condition evaluates to true.
Consider ‘when(sport==80, foo)’ and ‘when(sport==22, bar)’:
sketch for each window. A join that acts on t windows will
generate t sketches overall.
It sets a virtual header that indicates which branch is taken.
Later statements check against the branch header, and only
activate defense processing for that branch.
Virtual headers are carried on a special metadata bus in switch
hardware, and they have the same lifetime as a physical packet.
In other words, virtual headers will disappear after the packet
leaves the switch, unless the policy uses one of the following
operators to track cross-packet state:
reduce takes in a set of headers as the key, and aggregate
all packets with the same key using the reduce function. In
addition to producing a virtual header as output, it also stores
the current aggregation result into a count-min sketch to per-
sist the state. For instance, ‘reduce(sip,tot,f_sum(sz))’ groups
packets by their source IP addresses, aggregates packet sizes,
and computes the total volume per source IP:
The aggregation runs throughout the current time window,
and resets when a new panoramic snapshot begins. The same
time window applies to ‘distinct’ and ‘zip’ below.
distinct avoids double-counting of the same key by ﬁrst
checking if the program has already recorded it in a bloom
ﬁlter. It sets a virtual header to indicate whether the current
packet carries the ﬁrst distinct key in the same time window.
Every unique key triggers an insertion to the bloom ﬁlter.
Consider ‘distinct(sport)’:
zip performs a join between two sets of headers using a com-
mon key. Internally, the compiler generates two sketches (or
bloom ﬁlters), and a packet triggers two queries, one to each
sketch. Consider ‘zip(sip,tot,cnt)’, which produces a total traf-
ﬁc volume and a packet count for each source IP:
A self-join on a header ﬁeld can be performed across two or
more adjacent time windows, and the compiler generates one
Summary. Applying the sequence of operators to each packet
will result in a set of relevant packet headers that are needed
for the defense. Logically, all selected packets’ headers are
accessible in the policy return value—i.e., the panoramic
variable; however, as discussed, the compiler performs opti-
mizations to abstract away most physical and virtual headers.
4.4 Constructing the panorama
· · ·
So far, we have described how the Ripple compiler identiﬁes
relevant attack signals and extracts them from switch-local
trafﬁc. The Ripple compiler also augments each switch pro-
gram, so that they run a distributed protocol for view synchro-
nization. Local fragments will be carried by this synchroniza-
tion protocol to all switches, and switches will construct a
global view based on the panorama deﬁnition. The runtime
protocol executed once per time window.
Goal: At the beginning of each time window, each switch has
extracted a fragment of the panorama pv from the trafﬁc in
the past period based on the policy program. Therefore, pv is
initially distributed across all switches and sharded by switch
IDs: pv[s1], pv[s2],
, pv[sk], where si is a switch identiﬁer.
Implementing the panoramic view, therefore, requires Ripple
to merge all switch-local fragments pv[si], i ∈ [1..k]. The goal
of the Ripple protocol is to merge these fragments in a fully
distributed manner.
Querying pv. In OpenFlow-based SDN, the controller can
naturally serve as a vantage point to query switch data, per-
form the merge, and install the aggregates back; however, this
would create a centralized component. Instead, we borrow
recent proposals that query and synchronize switch state en-
tirely in the data plane [45, 56]. At a high level, Ripple uses
the packet generator to generate a stream of packets, whose
destination IP addresses are the intended receiver switches.
The switch program attaches the register values to the packets
as customized header ﬁelds, and sweeps through all registers
that need to be synchronized. Figure 3(a) illustrates the packet
format for synchronizing pv.
Disseminating pv. The programmable switches disseminate
pv fragments by routing the packets through the network to
all switches. Ripple has 1) a spanning tree mode, and 2) a
multicast mode, as shown in Figure 3.
In mode 1), the switches run a spanning tree protocol to
identify a root switch, and all other switches use this root as a
rendezvous point. Across different rounds, different switches
can act as the root. Each switch sends its pv fragment along
edges of this tree to the root. The root merges all fragments
and distributes the panoramic view back to the switches. Com-
pared to the multicast mode, this saves trafﬁc overhead, since
each pv fragment is only propagated in the network once. It
takes roughly one round-trip time (RTT) for each synchroniza-
tion round. In mode 2), the switches multicast the pv packets
3872    30th USENIX Security Symposium
USENIX Association
headers_insipsptlnkif(hdr.spt== 80){ hdr.br= 0}elif(hdr.spt== 22){hdr.br= 1 } //chooses branchif(hdr.br == 0) foo()elif(hdr.br == 1) bar()szsipsptlnkszbrheaders_outprefbrprefheaders_insipsptlnkkey = hdr.sipcur = hdr.szcmsketch(key) += curhdr.tot= cmsketch(key) szsipsptlnksztotheaders_out…Per-IP traffic volumecount-min sketchheaders_insipsptlnkkey = hdr.sportif (bfilter(key)== 0) {hdr.dis= 1bfilter(key) = 1}szsipsptlnkszdisheaders_out…Per-IP bloom filterheaders_insipkey = hdr.siphdr.tot= cms_tot(key) hdr.cnt= cms_cnt(key) siptotcntheaders_outsketch: tot…sketch: cntEther(0x800)
IPv4(proto=251)
field1, field2, …, fieldn
(a) Customized headers for synchronization packets
Download
S0
S0
②
③
60
40
S1
S2
S1
S2
Root
Upload
S3
② ②
50
50
80
①
①
S3
50
③
②
40
60
S4
S5
S4
S5
(b) Spanning tree mode
(c) Multicast mode
Figure 3: The packet format for synchronizing panoramic
views and the two modes of the Ripple protocol.
to all neighbors, and every switch will receive all fragments
from all other switches. This incurs higher trafﬁc overhead,
since each fragment is multicast to multiple neighbors. How-
ever, synchronization ﬁnishes within 0.5×RTT time.
By default, Ripple uses mode 1) to synchronize most types
of pv in favor of trafﬁc savings. The only exception is for
implementing the rerouting-based defense, in which case it fa-
vors response time and uses mode 2) to compute least-utilized
paths (i.e., in the f_reroute function). This essentially imple-
ments a distance-vector protocol that discovers best paths
using probes [35, 42]. Probes are generated from each egress
switch, and they are tagged with switch identiﬁers. The probes
identify best paths in the current network to each destination
switch. Data packets are forwarded from the ingress switches
to the egress switches along the current best paths. A new
round of probes may update the routing decisions across the
network. Figure 3(b) shows an example where links are asso-
ciated with costs (utilization), and probes propagate link costs
across the network to identify least-utilized paths.
Merging pv. When a switch receives a pv fragment, it merges
it with its local copy by simply adding up all the register values
(for CMS) or preforming an OR (for BF) at the same indexes.
Because of the linearity of these data structures, they can be
easily combined by this merge [22]. When a switch receives
all fragments, pv becomes panoramic.
5 Security Considerations
creating congestion in the network to drop synchronization
packets in a targeted manner, then this would prevent the Rip-
ple switches from constructing a panoramic view. However,
precisely disrupting the Ripple protocol is not easy, since the
synchronization schedule is unknown to the attacker. There-
fore, such an adversary can at best resort to congesting edges
of the spanning tree to delay or prevent view synchronization.
As a possible defense, Ripple could use the multicast mode
for synchronization when the panoramic view cannot be con-
structed for multiple time windows. Since the multicast trafﬁc
does not follow spanning tree edges, the attacker can only
disconnect the network by taking down a much larger portion