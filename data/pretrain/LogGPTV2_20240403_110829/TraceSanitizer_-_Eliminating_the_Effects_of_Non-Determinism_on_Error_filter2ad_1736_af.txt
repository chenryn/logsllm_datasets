Solver
[m]
30.36
#Obj.
#Dep.
[s]
38 24 650
30.38 1.57
64 12 126 150.41 150.43 1.29
81.94 0.79
31 13 460
13
2810
0.99 1.58
16 22 630 116.66 144.61 8.57
81.93
0.87
San. Cmp.
[s]
0.3
0.17
0.13
0.2
2.86
It is important to note that the time overhead incurred by the
reversibility check is a one-time cost as the check needs to be
run only once. Further, the fault injection experiments can be
run in parallel with the checks. On the other hand, running the
sanitization algorithms needs to be done for each fault that is
injected (typically thousands of times for obtaining statistically
signiﬁcant estimates). We measure the time it takes to run each
of these two steps – the results are shown in Table III.
1) Reversibility Check: To assess the run time overhead of
the reversibility check we performed it on a golden run of each
of the benchmarks. We report the total run time along with
time taken by the SMT solver, the number of memory objects
accessed in the trace, and the inter-thread dependencies on
these objects in Table III. For all programs, the overall time for
the reversibility check ranges from approximately 1 min for
blackscholes to 150 min for pca, and is strongly domi-
nated by the SMT solver’s execution time. For swaptions,
which is the only program showing a notable difference be-
tween these times, building the formula takes considerably
longer due to the higher number of instructions in the trace.
In addition to the solving time, the total overhead consists of
the time it takes TraceSanitizer to build the formula, including
the identiﬁcation of data-dependencies in the trace. While
the number of dependencies and objects, along with the total
number of instructions hint at the size and complexity of the
formulas generated, they do not directly correspond to the
measured execution times. For instance, quicksort has a
higher complexity than kmeans in terms of memory objects
and dependencies in the traces with a comparable trace size,
but takes signiﬁcantly less time for the check.
To understand how our technique performs for target pro-
grams of different complexity, we conducted a scalability study
for one of the benchmarks (blackscholes). For this pur-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:30:10 UTC from IEEE Xplore.  Restrictions apply. 
61
(cid:26)
(cid:22)
(cid:25)
(cid:10)
(cid:12)
(cid:14)
(cid:18)
(cid:24)
(cid:6)
(cid:16)
(cid:10)
(cid:12)
(cid:9)
(cid:23)
(cid:8)
(cid:17)
(cid:9)
(cid:22)
(cid:6)
(cid:16)
(cid:21)
(cid:20)
(cid:8)
(cid:18)
(cid:17)
(cid:16)
(cid:15)
(cid:10)
(cid:19)
(cid:1)(cid:2)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)
(cid:1)
(cid:24)(cid:27)(cid:17)(cid:14)(cid:20)(cid:28)(cid:22)
(cid:1)
(cid:3)
(cid:29)
(cid:30)
(cid:31)
(cid:1)(cid:3)
(cid:1)(cid:30)
(cid:4)(cid:2)
(cid:1)(cid:2)
(cid:3)(cid:2)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)
Fig. 6.
blackscholes traces with different input sizes and numbers of threads
Reversibility formula build time in seconds (log scale)
for
pose, we generated execution traces of the benchmark with
inputs of varying size (2,4,8,16,32 inputs) and varying numbers
of threads (1,2,4,6,8,12,16) handling these inputs. We have re-
peated the reversibility check four times for each input/thread
count combination to account for execution time variations.
We divide the execution time into building and solving the
reversibility formula. Figure 6 shows the average time of Trace-
Sanitizer for constructing the reversibility formula in relation
to input size and thread count for the blackscholes bench-
mark. As blackscholes intrinsically limits the number of
worker threads to the number of inputs, the plot only shows
data points where the number of threads is higher than or equal
to the number of inputs. As can be seen, the formula build
time increases with the input size, but remains below 15 min
in all cases. The number of threads only has a relatively small
inﬂuence; for example, for input sizes 16 and 32, the time
taken for 12 threads is lower than the time for fewer threads.
For brevity, we do not report on solver times in detail, but
summarize our ﬁndings.
• The solver time signiﬁcantly exceeds the formula building
time (by an average factor of 191), with a maximum
average solver time of almost 103 h (32 inputs, 16 threads).
• Although we observed the highest solver time for the
most complex conﬁguration, we ﬁnd that solver time does
not strictly increase with thread count or input size.
• We ﬁnd solver time to vary strongly across repetitions
with a coefﬁcient of variation of up to 32.4 %.
From our scalability analysis we conclude that (1) building
reversibility formulas for TraceSanitizer is not a performance
bottleneck, (2) solving reversibility formulas dominates the
overall time for the reversibility check and may become a bottle-
neck, but is a one time cost for TraceSanitizer and will improve
as SMT solvers evolve.and (3) solver time can vary strongly
in unforeseen ways for different execution conﬁgurations.
2) Trace Sanitizing: Once the golden run has passed the re-
versibility check, TraceSanitizer proceeds with the sanitization
62
and comparison of faulty runs. Next, we measured the addi-
tional overhead incurred by running the sanitizing algorithms
and the actual comparison on each faulty run.
Table III shows a break-down of the median time across
5000 experiments that TraceSanitizer requires to perform these
sanitization (column 6) and comparison (column 7) steps. The
median time for trace sanitization ranges between 0.79 s and
8.75 s with a median absolute deviation (MAD) of 1.9 s for
swaptions and less than 0.4 s for the other benchmarks.
The trace comparison of a sanitized golden run and a faulty
run takes between 0.17 s and 2.86 s with a MAD of 0.2 s for
swaptions and under 0.02 s for the other benchmarks.
While we cannot directly compare these results to existing
approaches due to the strong impact of machine conﬁgurations
on performance measurements, we can provide an indirect com-
parison. As TraceSanitizer is the only sound tool for EPA trace
comparisons, it does not require any manual inspection of the
obtained comparison results to check for false positives, which
are required by unsound tools. To beat TraceSanitizer’s perfor-
mance for 5000 injections in the slowest case of swaptions,
4400 trace diffs (5000 · 0.88, the smallest coverage in Fig-
ure 5) would need to be inspected (manually) in less time
than 5000·8.57 s+144.61·60 s
, which is less than 12 seconds for
a diff across traces with more than a million lines (Table I).
An analogous calculation yields less than 4 seconds for man-
ual inspection of any other benchmark. Such small times are
almost impossible to achieve for any realistic program trace,
including those in our evaluation. Moreover, the time taken
by TraceSanitizer will become smaller as computing becomes
faster, which is not the case for manual inspection.
4400
VII. CONCLUSION
In this paper, we introduced a class of multi-threaded pro-
grams that we termed pseudo-deterministic, and for which EPA
can be sound in the presence of non-deterministic memory
allocations and CPU scheduling. We have developed an au-
tomated technique to determine whether a program belongs
to this class as well as a novel trace sanitizing approach that
soundly handles non-determinism. We implemented the tech-
nique in an automated tool called TraceSanitizer using the
LLVM compiler, and Satisﬁability Modulo (SMT) Solvers.
We empirically evaluated our TraceSanitizer prototype on
ﬁve benchmark programs and demonstrated that it is able to
fully eliminate false positives. Further, it achieves a high fault
coverage in an EPA study, across ﬁve different fault types.
Finally, TraceSanitizer provides reasonable performance, and
compares very favourably with unsound EPA tools that require
manual inspection of false-positives.
ACKNOWLEDGMENTS
This work was supported in part by DAAD (project
57389931), EC H2020 CONCORDIA GA# 830927, the Lan-
caster Security Institute, the NSERC, and the Killam Research
Fellowship from UBC. The scalability analysis results were
obtained using the Chameleon testbed supported by the NSF.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:30:10 UTC from IEEE Xplore.  Restrictions apply. 
[22] https://github.com/Z3Prover/z3, 2019.
[23] L. De Moura and N. Bjørner, “Z3: An efﬁcient smt solver,” in Inter-
national conference on Tools and Algorithms for the Construction and
Analysis of Systems. Springer, 2008, pp. 337–340.
[24] C. Barrett, A. Stump, C. Tinelli et al., “The smt-lib standard: Version
2.0,” in Proceedings of the 8th International Workshop on Satisﬁability
Modulo Theories (Edinburgh, England), vol. 13, 2010, p. 14.
[25] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The parsec benchmark suite:
Characterization and architectural implications,” in Proceedings of the
17th international conference on Parallel architectures and compilation
techniques. ACM, 2008, pp. 72–81.
[26] https://github.com/kozyraki/phoenix, 2016.
[27] W. Masri and R. A. Assi, “Prevalence of Coincidental Correctness and
Mitigation of Its Impact on Fault Localization,” ACM Trans. Softw. Eng.
Methodol., vol. 23, no. 1, pp. 8:1–8:28, Feb. 2014. [Online]. Available:
http://doi.acm.org/10.1145/2559932
REFERENCES
[1] M. Hiller, A. Jhumka, and N. Suri, “On the placement of software
mechanisms for detection of data errors,” in Proceedings International
Conference on Dependable Systems and Networks, June 2002, pp. 135–
144.
[2] N. Coppik, O. Schwahn, S. Winter, and N. Suri, “TrEKer: Tracing Error
Propagation in Operating System Kernels,” in Proceedings of the 32Nd
IEEE/ACM International Conference on Automated Software Engineering,
ser. ASE 2017. Piscataway, NJ, USA: IEEE Press, 2017, pp. 377–387.
[Online]. Available: http://dl.acm.org/citation.cfm?id=3155562.3155612
[3] R. Natella, S. Winter, D. Cotroneo, and N. Suri, “Analyzing the Ef-
fects of Bugs on Software Interfaces,” IEEE Transactions on Software
Engineering (to appear), pp. 1–1, 2018.
[4] J. Voas, “Error propagation analysis for COTS systems,” Computing
Control Engineering Journal, vol. 8, no. 6, pp. 269–272, Dec 1997.
[5] M. Hiller, A. Jhumka, and N. Suri, “An approach for analysing the
propagation of data errors in software,” in 2001 International Conference
on Dependable Systems and Networks, July 2001, pp. 161–170.
[6] Y. Chen, S. Zhang, Q. Guo, L. Li, R. Wu, and T. Chen, “Deterministic
Replay: A Survey,” ACM Comput. Surv., vol. 48, no. 2, pp. 17:1–17:47,
Sep. 2015. [Online]. Available: http://doi.acm.org/10.1145/2790077
[7] J. Huang, P. O. Meredith, and G. Rosu, “Maximal sound predictive race
detection with control ﬂow abstraction,” ACM SIGPLAN Notices, vol. 49,
no. 6, pp. 337–348, 2014.
[8] P. E. Black, “Algorithms and Theory of Computation Handbook, CRC
Press LLC, 1999, “single program multiple data”,” https://www.nist.gov/
dads/HTML/singleprogrm.html, 2004, accessed 2019-05-14.
[9] S. Okur and D. Dig, “How Do Developers Use Parallel Libraries?”
in Proceedings of the ACM SIGSOFT 20th International Symposium
on the Foundations of Software Engineering, ser. FSE ’12. New
York, NY, USA: ACM, 2012, pp. 54:1–54:11. [Online]. Available:
http://doi.acm.org/10.1145/2393596.2393660
[10] G. Lemos and E. Martins, “Speciﬁcation-guided golden run for analysis
of robustness testing results,” in Proc. SERE ’12, 2012, pp. 157–166.
[11] M. Leeke and A. Jhumka, “Evaluating the Use of Reference Run Models
in Fault Injection Analysis,” in Proc. PRDC ’09, 2009, pp. 121–124.
[12] A. Chan, S. Winter, H. Saissi, K. Pattabiraman, and N. Suri, “Ipa: Error
propagation analysis of multi-threaded programs using likely invariants,”
in Software Testing, Veriﬁcation and Validation (ICST), 2017 IEEE
International Conference on.
IEEE, 2017, pp. 184–195.
[13] T. Liu, C. Curtsinger, and E. D. Berger, “Dthreads: efﬁcient deterministic
multithreading,” in Proceedings of the Twenty-Third ACM Symposium
on Operating Systems Principles. ACM, 2011, pp. 327–336.
[14] R. L. Bocchino Jr, V. S. Adve, D. Dig, S. V. Adve, S. Heumann, R. Ko-
muravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian, “A type
and effect system for deterministic parallel java,” in ACM Sigplan Notices,
vol. 44, no. 10. ACM, 2009, pp. 97–116.
[15] J. Christmansson, M. Hiller, and M. Rimen, “An experimental comparison
of fault and error injection,” in Proc. ISSRE ’98, 1998, pp. 369–378.
[16] M. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant, C. Pacheco, M. S.
Tschantz, and C. Xiao, “The daikon system for dynamic detection
of likely invariants,” Science of Computer Programming, vol. 69,
no. 1, pp. 35 – 45, 2007, special issue on Experimental Software
and Toolkits. [Online]. Available: http://www.sciencedirect.com/science/
article/pii/S016764230700161X
[17] A. Thomas and K. Pattabiraman, “LLFI: An intermediate code level
fault injector for soft computing applications,” in Workshop on Silicon
Errors in Logic System Effects (SELSE), 2013.
[18] M. R. Aliabadi, K. Pattabiraman, and N. Bidokhti, “Soft-LLFI: A Com-
prehensive Framework for Software Fault Injection,” in 2014 IEEE
International Symposium on Software Reliability Engineering Workshops,
Nov 2014, pp. 1–5.
[19] S. Hong and M. Kim, “A survey of race bug detection techniques
for multithreaded programmes,” Software Testing, Veriﬁcation and
Reliability, vol. 25, no. 3, pp. 191–217, 2015. [Online]. Available:
https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1564
[20] T. F. S¸erb˘anut¸˘a, F. Chen, and G. Ros¸u, “Maximal causal models for
sequentially consistent systems,” in International Conference on Runtime
Veriﬁcation. Springer, 2012, pp. 136–150.
[21] H. Saissi, “On the Application of Formal Techniques for Dependable
Concurrent Systems,” Ph.D. dissertation, Technische Universit¨at,
Darmstadt, 2019. [Online]. Available: http://tuprints.ulb.tu-darmstadt.de/
8600/
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:30:10 UTC from IEEE Xplore.  Restrictions apply. 
63