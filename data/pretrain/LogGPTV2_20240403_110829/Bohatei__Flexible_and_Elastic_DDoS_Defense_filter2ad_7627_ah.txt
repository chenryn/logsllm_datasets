vm=1
MaxVM
∑
vm=1
vm=1
MaxVM
∑
vm=1
(i,i)=eannotated
(i,i)=eannotated
∑
a,i→i
∑
a,i→i
∑
i:(i,i)=eannotated
a,i→i
a,i × Pa,i ≥
(s,s)∈sameRack
∑
(s,s) /∈sameRack
MaxVM
MaxVol
∑
l=1
MaxVM
∑
vm=1
MaxVM
s
∑
s
qd,a,i,vm,s,i,vm,s,l ≤ Pa,i  enforcing VMs capacities
10 ∀d,a,i,vm : ∑
11 ∀d,s ∈ Sd,a,i : nd,s
12 ∀d,s ∈ Sd,a,i : nd,s
13  ﬂow conservation for VM vm of type logical node k that has both predecessor(s) and successor(s)
∑
a,i→i
∑
a,i→i
a,i × Pa,i ≤
∑
vm=1
MaxVM
i:(i,i)=eannotated
i:(i,i)=eannotated
vm=1
MaxVM
∑
vm=1
∑
l=1
∑
l=1
MaxVol
MaxVol
vm=1
∑
s
∑
s
∑
∑
qd,a,i,vm,s,i,vm,s,l  bound trafﬁc volumes
qd,a,i,vm,s,i,vm,s,l + 1  bound trafﬁc volumes
∀d,a,k,vm :
MaxVM
∑
vm=1
g:(g,k)=eannotated
MaxVol
∑
l=1
∑
a,g→k
∑
∑
s
∑
s
∑
a
qd,a,g,vm,s,k,vm,s,l =
MaxVM
∑
∑
vm=1
h:(k,h)=eannotated
a,k→h
MaxVol
∑
s
∑
s
∑
l=1
qd,a,k,vm,s,h,vm,s,l
14 ∀link ∈ ISP backbone :
15
fe,a,d ∈ [0,1],qd,a,i,vm,s,i,vm,s,l ∈ {0,1},nd
fe,a,d × Te,a ≤ β × MaxLinkCapacity  per-link trafﬁc load control
a,i ∈ {0,1, . . .}, ta,d,interRd,intraRd,dscd ∈ R  variables
a,i,nd,s
link∈Pathe→d
Figure 15: ILP formulation for an optimal resource management.
a
∈ Eannotated
type va,i runs on server s and sends 1 unit of trafﬁc (e.g., 1
Gbps) to VM vm of type va,i that runs on server s, where
eannotated
, and servers s and s are located in
a,i→i
datacenter d; otherwise, qd,a,i,vm,s,i,vm,s,l = 0. Here l is
an auxiliary subscript indicating that the one unit of traf-
ﬁc associated with q is the lth one out of MaxVol possible
units of trafﬁc. The maximum required number of VMs
of any type is denoted by MaxVM.
.
a
The ILP involves two key decision variables: (1) fe,a,d
is the fraction of trafﬁc Te,a to send to datacenter Dd, and
(2) nd,s
a,i is the number of VMs of type va,i on server s of
datacenter d, hence physical graphs DAGphysical
Objective function:
The objective function (1) is
composed of inter-datacenter and intra-datacenter costs,
where constant α > 0 reﬂects the relative importance of
inter-datacenter cost to intra datacenter cost.
Constraints: Equation (2) ensures all suspicious traf-
ﬁc will be sent to data centers for processing. Equation
(3) computes the amount of trafﬁc of each attack type
going to each datacenter, which is ensured to be within
datacenters bandwidth capacity using (4). Equation (5) is
intended to ensure sufﬁcient numbers of VMs of the re-
quired types in each datacenter. Servers compute capaci-
ties are enforced using (6). Equation (7) sums up the cost
associated with each datacenter, which is composed of
two components: intra-rack cost, given by (8), and inter-
rack component, given by (9). Equation (10) ensures the
trafﬁc processing capacity of each VM is not exceeded.
Equations (11) and (12) tie the variables for number
of VMs (i.e., nd,s
a,i ) and trafﬁc (i.e., qd,a,i,vm,s,i,vm,s,l) to
each other. Flow conservation of nodes is guaranteed
by (13). Inequality (14) ensures no ISP backbone link
gets congested (i.e., by getting a trafﬁc volume of more
than a ﬁxed fraction β of its maximum capacity), while
Pathe→d is a path from a precomputed set of paths from
e to d. The ILP decision variables are shown in (15).
B DSP and SSP Algorithms
As described in §4.3, due to the impractically long time
needed to solve the ILP formulation, we design the DSP
and SSP heuristics for resource management. The ISP
global controller solves the DSP problem to assign sus-
picious incoming trafﬁc to data centers. Then each lo-
cal controller solves an SSP problem to assign servers to
USENIX Association  
24th USENIX Security Symposium  831
VMs. Figure 16 and 17 show the detailed pseudocode
for the DSP and SSP heuristics, respectively.
d
d
a,d
, Clink
, and Ccompute
and fe,a,d values
1  Inputs: L, T, DAGannotated
a
2  Outputs: DAGphysical
3
4 Build max-heap TmaxHeap of attack volumes T
5 while !Empty(TmaxHeap)
6
7
8
9
10
11
d ← datacenter with min. Lt.e,t.d and cap.> 0
 enforcing datacenter link capacity
t1 ← min(t,Clink
 compute capacity of d for trafﬁc type a
t2 ←
do t ← ExtractMax(TmaxHeap)
)
d
CCompute
d
∑
Wa,i→i
i
Pa,i
∑
i
 enforcing datacenter compute capacity
tassigned ← min(t1,t2)
fe,a,d ←
for each module type i
tassigned
Tt.e,t.a
do  update nd
a,i given new assignment
∑
i
Wa,i→i
Pa,i
assigned
a,i = nd
a,i + td
nd
d − tassigned
← Ccompute
Clink
d ← Clink
Ccompute
d
 leftover trafﬁc
tunassigned = t− tassigned
if (tunassigned > 0)
then Insert(TmaxHeap,tunassigned )
− tassigned∑
∑
i
d
i
Wa,i→i
Pa,i
12
13
14
15
16
17
18
19
20
21
22
23
24
25
for each datacenter d and attack type a
do Given nd
a,i and DAGannotated
a
, compute DAGphysical
a,d
Figure 16: Heuristic for datacenter selection prob-
lem (DSP).
is not assigned to d’s servers
whose all predecessors are assigned
corresponding to N)
, IntraUnitCost, InterUnitCost,
1  Inputs: DAGphysical
values
a,i values
and Ccompute
a,d
d,s
a,d
2  Outputs: nd,s
3
4 while entire DAGphysical
do N ← vannotated
5
if (N == NIL)
6
then N ← vannotated
7
a
localize(nodes of DAGphysical
8
9
10  function localize tries to assign all of its
a,d
a,i
with max Pa,i
input physical nodes to the same server or rack
localize(inNodes){
assign all inNodes to emptiest server
if failed
then assign all inNodes to emptiest rack
if failed
update nd,s
then split inNodes Vphysical
a,i values
a
across racks
11
12
13
14
15
16
17
18 }
Figure 17: Heuristic for server selection problem
(SSP) at datacenter d.
832  24th USENIX Security Symposium 
USENIX Association