(a1.r = a2.r = Hi) ∧ (s1 = s2)
Accessed(v) ∧ (v.a.r = Hi) ∧ (v.a.o ≥ sh + st)
(0 x and p1->y always have the addresses of (cid:104)H05, 0(cid:105) and
(cid:104)H05, 16(cid:105), respectively. In contrast, all ﬁelds in the region H06
are foldable as the p2[∗].x’s have various addresses. We hence
want to fold the behaviors of p2[1], p2[2], and so on to p2[0].
The region H07 has an unfoldable ﬁeld followed by a foldable
ﬁeld which is an array of varying size. Observe that foldable
ﬁelds can only occur after unfoldable ﬁelds in a region. In
Figure 6, we introduce UnfoldableHeap(i, s) to denote the ﬁrst
s bytes of the heap region allocated at i are unfoldable and
FoldableHeap(i, s) to denote the region allocated at i has a
foldable part with an element size of s. For example, we have
FoldableHeap(7, 16) for region H7 in Figure 7b.
CC01 states that if i only allocates a constant size region,
the entire region is unfoldable. CC02 says that if through
deterministic analysis, we know that the allocation size of
i is a multiple of s, the foldable part has an element size
of s. CC03 says that if a primitive ﬁeld v is found inside a
heap region, all the part up to v is unfoldable. This is because
unfoldable ﬁelds must precede foldable ﬁelds. CC04 states that
a heap region cannot have different unfoldable parts. However,
the presence of a smaller unfoldable part can enhance the
conﬁdence of a larger unfoldable part (CC05). CC06 says that
an array found inside a heap region must belong to the foldable
part. Rule CC07 is the folding rule. The ﬁrst formula says that
a primitive ﬁeld v found inside a later structure instance inside
the foldable region indicates the presence of a primitive ﬁeld at
the corresponding offset inside the ﬁrst instance. For example
in H06, the identiﬁcation of y ﬁeld in p2[1] indicates the
presence of y ﬁeld in p2[0], although p2[0]->y is never
seen during sample runs. The second formula eliminates the
primitive ﬁeld v after it is folded.
Structure Recovery. Like existing work, we leverage the
instruction patterns of loading base address to recognize
a data structure. However, we model its uncertainty using
probabilities. In addition, we consider the data ﬂow among
different variables of the same type. Speciﬁcally, rules CD01-
CD10 are for structure recovery, including global/stack/heap
structures. Intuitively, we ﬁrst identify memory segments (i.e.,
part of a structure) that are homomorphic, meaning that they
have highly similar access patterns, data ﬂow, and points-
to relations. These segments are then intersected, unioned,
or separated to form the ﬁnal structures. Individual ﬁelds
can be then identiﬁed from their access pattern within the
structure. Speciﬁcally, rules CD01-CD03 receive deterministic
hints. CD04 states that if a pair of homomorphic segments
overlap with another pair of homomorphic segments, they
enhance each other’s conﬁdence (the ﬁrst formula) and may
form a pair of new homomohpic segments that are the union
of the original two pairs (the second formula). Intuitively, it
corresponds to that the sub-parts of a same complex structure
are being exposed differently (e.g.,
through different data
ﬂow), and we leverage the overlap of these parts to join them.
CD05 says that if the corresponding primitive ﬁelds in a pair
of homomorhpic segments have different access patterns (4-
byte access versus 8-byte access), either the primitive ﬁeld
predicates are likely false or the homomorphic predicate.
Rules CD06 and CD07 identify ﬁelds of structure from the
deterministic reasoning results (e.g., BaseAddr) and if the
accesses are primitive. CD08 transfers ﬁeld information across
a pair of homomorphic segments. Rule CD09 asserts a ﬁeld
cannot have two different base addresses. CD09 determines a
pointer variable v1 if a valid address v2.a is stored to v1 and
v2 has been accessed as a primitive variable.
OSPREY also has a set of typing rules that associate
primitive types (e.g., int, long, and string) to variables, based
on their data-ﬂow to program points that disclose types such as
invocations to string library functions. These rules are similar
to existing works [1], [12], [14] and hence elided.
B. Probabilistic Constraint Solving
Each of the probabilistic constraints in Figure 8 (the for-
mulas in the last column) essentially denotes a probability
function over the random variables involved. The functions
can be further transformed to a probabilistic graph model
called factor graph [34], which is a bi-partite graph with two
kinds of nodes, function node denoting a probability function,
and variable node denoting a random variable. Edges are
introduced between a function node and all the variable nodes
related to the function. The whole factor graph denotes the
joint distribution of all the random variables. An example can
be found in Appendix E.
Given a set of observations (e.g., x1 = 1) from the deter-
ministic reasoning step, and the prior probabilities (p values),
posterior marginal probabilities are computed by propagating
and updating probabilities along the edges. Some of the rules,
such as CB02, generate new predicate nodes during inference.
After each round of inference (i.e., probabilities converge after
continuous updates), it checks all the (new) predicate nodes
to coalesce those denoting the same meaning to one node.
The node inherits all the edges of all the other nodes that are
coalesced. Then another round of inference starts. Note that
while some probabilistic inference applications are stochas-
tic, our application (variable recovery and typing) has the
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:33:02 UTC from IEEE Xplore.  Restrictions apply. 
822
Fig. 9: Recall for all variables (primitive and complex)
uncertainty originating from loss of debugging information.
In other words, there is deterministic ground truth (or, the
ground-truth variables and their types are deterministic). In
this context, the number of hints that we can aggregate plays a
more important role than the prior probabilities. Graph models
provide a systematic way of aggregating these hints, while
respecting the inherent structural properties (e.g., control-ﬂow
and data-ﬂow constraints). We hence adopt simple prior
probabilities, p ↑= 0.8, p ↓= 0.2, and p(k) is computed from
the ratio between k and the total number of sampled paths
in BDA.
In fact, there are a number of existing work [28],
[35], [36], [37] leveraging probabilistic inference for similar
applications with (mostly) deterministic ground truth (e.g.,
speciﬁcation inference for explicit information ﬂow). They use
preset prior probabilities and their results are not sensitive to
prior probability conﬁgurations. We follow a similar setting.
Posterior Probability Computation On Factor Graph.
There are standard off-the-shelf algorithms that can compute
posterior probabilities for factor graphs. Most of them are
message passing based [38], [39], [40], i.e., a function node ag-
gregates probabilities (or beliefs) from its neighboring variable
nodes, deriving an outgoing belief based on the probability
function. Such algorithms become very expensive and have
low precision when the graph is large and loopy (as messages
are being passed in a circle and computation can hardly
converge). There are optimized algorithms such as junction
tree algorithm [41] that removes cycles by coalescing them
to single nodes. However,
in our
context due to the particularly large number of nodes and the
extensive presence of loops in our factor graphs. We hence
develop an optimized algorithm from scratch, leveraging the
modular characteristics of program behaviors. Speciﬁcally, we
observe that PrimitiveVar is the most common kind of node
and involved in most constraints. These nodes have very few
loops with the other kinds of nodes, although there are loops
within themselves. Thus, we ﬁrst construct a base graph only
considering PrimitiveVar-related rules (i.e., CA01−CA08). The
base graph is still very large (typically around 3000 nodes for
even a small program) and cannot be directly solved. We also
observe that the memory chunks of the PrimitiveVar nodes
are distributed in various memory regions that are relatively
autonomous (e.g., different stack frames). We hence partition
the base graph to many sub-graphs based on memory regions.
Empirically, a sub-graph contains 40 nodes on average. Each
sub-graph is solved by a junction tree algorithm. With the
solved values of all sub-graphs as the initial values, we dynam-
they do not work well
Fig. 10: Precision for all variables (primitive and complex)
ically construct a secondary graph considering the remaining
random variables. Speciﬁcally, for any rule, if its pre-condition
is satisﬁed, we include it in the graph.