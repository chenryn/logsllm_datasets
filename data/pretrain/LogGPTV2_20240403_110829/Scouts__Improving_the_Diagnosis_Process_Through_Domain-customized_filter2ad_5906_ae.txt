generated by a PhyNet monitor that were not caused by a PhyNet-
related root-cause. The gain-out for these incidents was as high as
94%.
Non-PhyNet monitor incidents: often go to other teams. But a num-
ber of those incidents are caused by PhyNet-related problems and
our Scout provides significant gain-in in those cases (Figure 11-
a): for over 50% of incidents, the Scout saves more than 30% of
their investigation times. The Scout also provides gain-out for a
small fraction of these incidents (5%); the majority of which do not
go through PhyNet at all. The gain-out in such cases tends to be
large: ≥ 44%. Our overhead for these incidents is minimal: ≤ 4% of
incidents have overhead-in lower than 7%; error-out is 3.06%.
Customer-reported incidents (CRIs). CRIs are less common than moni-
tor generated incidents (§3) but are also among the hardest incidents
to classify both for human operators, the NLP system (§3), rule-
based systems, and even Scouts: customers often do not include
necessary information when opening support tickets (incidents)
and the first few teams these incidents go through do the work
needed to discover and append this information to the incident
description. Luckily, Scouts are not one-shot systems — they can be
applied to the incident again before each transfer: operators would
always use the most recent prediction. We ran an experiment where
we waited until the investigation of the first n teams was over be-
fore triggering the Scout. We see the Scout’s gain-in (Figure 12-a)
increases after the first few teams investigate.
But there is a trade-off as n increases: the Scout has more infor-
mation as more teams investigate, but has less room to improve
things as we get closer to when the incident was sent to the re-
sponsible team. Gain out exhibits a similar trend (Figure 12-b): it
decreases as the time we wait to trigger the Scout over-takes the
gain. Overhead numbers (Figure 12-c,d) indicate it is best to wait
for at least two teams to investigate a CRI before triggering a Scout
for the best trade-off.
7.5 A Detailed Case Study
We will next discuss two incidents in more detail. These incidents
were routed incorrectly by operators to the wrong team thus wast-
ing valuable time and effort. The PhyNet Scout, however, is able to
correctly classify and route these incidents. These incidents help
demonstrate how the Scout can help operators in practice.
A virtual disk failure. In this example, the database team expe-
rienced multiple, simultaneous, virtual disk failures that spanned
across multiple servers. The database team’s monitoring systems
Gain-InBest possible gainOverhead-inFraction of investigation time (%)(a)0.00.20.40.60.81.0CDFGain-outBest possible gainError-out: 3.06%0.00.20.40.60.81.0Fraction of investigation time (%)(b)204060800100204060800100SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 12: (a) gain-in, (b) gain-out, (c) overhead-in, and (d) error-out for CRIs as more teams investigate.
Explanations are crucial. In earlier versions of the Scout we only
reported the decision along with the confidence. Operators had a
hard time accepting the output because of the lack of explanation.
We thus augmented incidents with an explanation: we listed all
the components found in the incident and the monitoring data the
Scout used. For those incidents which the Scout classified as being
PhyNet’s responsibility, we used the method of [57] to describe
which features pointed to the problem being caused by PhyNet.
Some features help ML but confuse operators. One of our fea-
tures is the number of components of each type (§5). Operators find
it confusing when these features are part of the explanation for a
decision because they are not part of their routine investigation
process: the model finds them useful but operators do not.
Operators do not have time to read the fine-print. We care-
fully studied the Scout, its mistakes, and its confidence scores be-
fore deploying it in production. We included these findings as part
of the recommendation to the operators. For example, an incident
classified as not being PhyNet’s responsibility would come with
the following recommendation: “The PhyNet Scout investigated
[list of components] and suggests this is not a PhyNet incident.
Its confidence is [confidence]. We recommend not using this out-
put if confidence is below 0.8. Attention: known false negatives
occur for transient issues, when an incident is created after the
problem has already been resolved, and if the incident is too broad
in scope.” However, we found operators did not read this fine-print
and complained of mistakes when confidence was around 0.5 or
when transient incidents occurred.
Adding new features can be slow. The first step in building any
supervised model is to create a data set for training. To enable this,
early on (9 months in advance), we extended the retention period
of PhyNet’s monitoring data. To add new data sets we often have
to wait until there is enough (either because we had to extend the
retention period, or because the system is new) before we can add
it to the feature-set.
Down-weighting old incidents. Over time, many of the incidents
become “old” or obsolete, as the systems they pertain to evolve or
are deprecated. Therefore, in our deployed Scout we down-weight
incidents in proportion to how long ago they occurred.
Learning from past mistakes. To further improve the Scout, in
our production deployment we also found it useful to increase the
weight of incidents that were mis-classified in the past in future
re-training of the model.
Not all incidents have the right label. Our incident management
system records the team owning the incident when the root cause
was discovered and the incident was resolved. We use this field
to label the incidents for evaluating our system. Our analysis of
the mistakes our system made in production showed that in some
cases this label can be incorrect: the team that closed the incident
is not the team that found the root cause. This is often because
operators do not officially transfer the incident (in which case the
label is left unchanged). Such mislabeling can cause problems for
the Scout over time as many of these incidents were mistakenly
marked as mis-classifications and up-weighted for future training:
the model would emphasize learning the wrong label in the future.
This problem can be mitigated by de-noising techniques and by
analysis of the incident text (the text of the incident often does
reveal the correct label).
Concept drift. While the use of the CPD+ algorithm helps the
Scout be somewhat resilient to new incidents. Concept drift prob-
lems do rarely occur in practice: during the last two years, there
were a few weeks (despite frequent retraining) where the accuracy
of the Scout dropped down to 50%. This is a known problem in the
machine learning community and we are working on exploring
known solutions for addressing such problems.
9 DISCUSSION
Scouts can significantly reduce investigation times (see §3,§7). How-
ever, like any other system, it is important to know when not to
rely on them:
Scouts route incidents, they do not trigger them. “Given the
high accuracy of Scouts, can they also periodically check team
health?” Sadly, no: (1) incidents provide valuable information that
enables routing — without them the accuracy of the Scout drops
significantly; (2) the overhead of running such a solution periodi-
cally would be unacceptable because the Scout would periodically
have to process all the monitoring data the team collects for each
and every device.
“Specific” incidents are easier to classify. Scouts identify which
components to investigate from the incident and limit the scope
of their investigations to those components. Incidents that are too
broad in Scope are harder to classify because of feature dilution.
Such cases tend to be high priority incidents — all teams have to
get involved (see §3).
Simultaneous incidents with over-lapping components are
harder to classify. If two incidents implicate the same set of com-
ponents and one is caused by the Scout’s team, the Scout may
struggle to differentiate them (see §7). This is a very specific and
relatively rare subset of the broader issue of concurrent incidents.
Operators can improve the starter Scout the framework cre-
ates. Our framework creates a starter Scouts. Operators can im-
prove this Scout by adding rules they have learned to work well in
Max99th percentile95th percentileAverageError-outNumber of team investigations (a)Number of team investigations (b)Gain-outNumber of team investigations (c)Overhead-in2468100.00.20.40.60.81.00.60.81.0Error-outGain-in2468100.00.20.40.60.81.02468100.00.20.40.60.81.0Number of team investigations (d)0.00.20.4234561SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Gao et al.
practice. Similarly, operators familiar with ML and statistics can
add more specialized features to this starter Scout to improve its ac-
curacy. For teams whose components have complex dependencies
with other teams’ components, the accuracy of the Scout created by
the framework may not be high enough — in such cases the team
may be better off building their own.
The framework is only as good as the data input. Like all
ML-based solutions, our framework suffers from the “garbage-in-
garbage out principle” [44]: if none of the input data is predictive of
the team’s culpability or if it is too noisy, the framework will not
produce an accurate Scout. Operators use the same data to diagnose
incidents when the team is responsible: this should be unlikely.
Some teams may not have data for training. GDPR [6] imposes
constraints on what data operators can collect and store which
impacts their ability to use ML [16] — operators may need to use
non-ML-based techniques for certain teams.
Not all teams should build a Scout. Not all teams experience
mis-routing in the same degree. Teams with few dependencies, for
example, do not experience mis-routing as often as a team such as
PhyNet, which is a dependency for almost all network-dependant
services. Similarly, teams where problems are less ambiguous are
also less prone to mis-routing, e.g., DNS issues tend to be routed
directly to the DNS team. This is also another reason that we built
a team-by-team solution: it helps prioritize teams who contribute
most to the mis-routing problem.
The framework requires correct annotations. We do not sup-
port any automatic validation of annotations. This is a subject of
future work.
Potential drawbacks of the team-by-team approach. There
are two potential drawbacks to our Scout design. The first draw-
back is Scout Master cannot route an incident when all the Scouts
returns “no.” This may be due to false negatives, or because the
team responsible for the incident has not yet built a Scout. The
second drawback is some teams have complex dependencies, it may
not be possible to carve out a clear boundary and build completely
isolated Scouts for those teams. For example, if team A and team B
depend on each other, they may need to cooperate when building
their Scouts and use signals from each other’s monitoring systems.
We believe the pros outweigh the cons.
The side-effect of aggregating sub-components. In order to
maintain a fixed size feature vector (as necessitated by our ML
components) the Scout framework aggregates monitoring data
from components of the same type and computes a set of statistics
over the resulting data set. In some cases, this may dilute the impact
of a problem with an individual device which can result in mis-
classifications. We observe, however, that the Scout accuracy is
high irrespective of this design choice.
Alternative design. In the design of our Scout framework, we had
to find a solution to the fact that each incident can implicate an
unknown number of components (we do not know this number in
advance). Our solution uses aggregate statistics across components
with the same type to create a fixed-sized feature vector at all
times. However, two other designs are possible: (a) one can consider
all devices in the data center for each incident — this results in
an enormous feature-vector and would result in lower accuracy
due to the curse of dimensionality; (b) one can build a separate
classifier per type of component and check the health of each device
independently — this was infeasible in our case as we did not have a
data set with labels for each device (many incidents did not contain
the name of the device which was identified as the root cause).
10 RELATED WORK
Mitigation tools [14, 26, 27, 30, 37, 38, 42, 43, 45, 48–50, 53,
55, 59, 62, 63, 67, 69, 74–76]. Many automated diagnosis tools
have been built over the years [14, 26, 27, 30, 37, 38, 42, 43, 45, 48–
50, 53, 55, 59, 62, 63, 67, 69, 74–76]. These works aim at finding a
single root cause. But, there are some incidents where they fail
(packet captures from inside the network may be necessary [70]).
Incidents are an indication that existing diagnosis systems have
failed to automatically mitigate the problem. Many diagnosis sys-
tems require a human expert to interpret their findings [70]: the
support teams do not have the necessary expertise. There are many
instances where the network is not responsible for the problem –
these systems are too heavy-weight for solving incident routing.
Application-specific incident routers [15, 17, 29, 71, 73]. Be-
cause they are heavily tied to the application semantics, these works
fail at fully solving the incident routing problem: they cannot op-
erate at DC-scale because operators would have to run (and con-
figure) an instance of these solutions per each application-type.
Also, [15, 71, 73] all focus on identifying whether the network, the
host, or the remote service is responsible. Cloud providers have mul-
tiple engineering groups in each category (e.g., our cloud has 100
teams in networking) and the broader problem remains unsolved.
Work in software engineering [13, 35, 39, 41, 60] The work
[13, 41]. try to find the right engineer to fix a bug during software
development and use either NLP-based text analysis or statistical-
based ticket transition graphs. Other work: [39, 60] analyzes the
source code. None can be applied to the incident routing problem
where bugs are not always confined to the source code but can be
due to congestion, high CPU utilization, or customer mistakes.
Measurement studies on network incidents [19, 21, 31, 33, 58,
61]. The work [21] describes the extent of incident mis-routings in
the cloud, while our work focuses on the reasons why they happen.
Other studies characterize the different types of problems observed
in today’s clouds. These works provide useful insights that help
build better Scouts.
11 CONCLUSION
We investigate incident routing in the cloud and propose a dis-
tributed, Scout-based solution. Scouts are team-specialized gate-
keepers. We show that even a single Scout can significantly reduce
investigation times.
Ethics: This work does not raise any ethical issues.
ACKNOWLEDGMENTS
The authors would like to thank Sumit Kumar, Rituparna Paul,
David Brumley, Akash Kulkarni, Ashay Krishna, Muqeet Mukhtar,
Lihua Yuan, and Geoff Outhred for their help with deployment of
the PhyNet Scout and their useful feedback. We would also like
to thank shepherd and SIGCOMM reviewers for their insightful
comments. Jiaqi Gao was supported in this project by a Microsoft
internship as well as by the NSF grant CNS-1834263. Nofel Yaseen
was supported, in part, by CNS-1845749.
REFERENCES
[1] Adaboost. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.