### Incident Classification and Routing with PhyNet Scout

#### Non-PhyNet Monitor Incidents
Incidents generated by a PhyNet monitor that are not caused by a PhyNet-related root cause have a gain-out as high as 94%. For non-PhyNet monitor incidents, which often get routed to other teams, a significant number are actually caused by PhyNet-related issues. In these cases, our Scout system provides substantial gains. For over 50% of such incidents, the Scout saves more than 30% of the investigation time (Figure 11-a). Additionally, the Scout offers a gain-out for a small fraction (5%) of these incidents, with the majority of these cases not involving PhyNet at all. The gain-out in these instances is typically large, reaching or exceeding 44%. The overhead for these incidents is minimal, with less than 4% of incidents having an overhead-in lower than 7%, and an error-out of 3.06%.

#### Customer-Reported Incidents (CRIs)
Customer-reported incidents (CRIs) are less frequent than monitor-generated incidents but are among the most challenging to classify. This is true for both human operators, NLP systems, rule-based systems, and even Scouts, as customers often omit necessary information when opening support tickets. However, Scouts are not one-shot systems; they can be re-applied after each transfer, allowing operators to use the most recent prediction. An experiment was conducted where the Scout was triggered only after the first n teams had completed their investigations. The results showed that the Scout's gain-in (Figure 12-a) increases as more teams investigate, but there is a trade-off: the Scout has more information, but less room to improve as the incident gets closer to being sent to the responsible team. Gain-out (Figure 12-b) decreases as the delay in triggering the Scout overtakes the potential gain. Overhead numbers (Figures 12-c and 12-d) suggest that it is optimal to wait for at least two teams to investigate a CRI before triggering the Scout.

#### Detailed Case Study
Two specific incidents will be discussed in detail. These incidents were initially misrouted by operators, leading to wasted time and effort. The PhyNet Scout, however, correctly classified and routed these incidents, demonstrating its practical utility.

**Example 1: Virtual Disk Failure**
In this case, the database team experienced multiple simultaneous virtual disk failures across several servers. The team's monitoring systems flagged the issue, but the Scout was able to correctly identify and route the incident, saving valuable time.

#### Importance of Explanations
In earlier versions of the Scout, only the decision and confidence level were reported, which made it difficult for operators to accept the output. To address this, we augmented the incidents with explanations, listing all components found and the monitoring data used. For incidents classified as PhyNet's responsibility, we used a method from [57] to describe which features pointed to the problem. Some features, while useful for the model, can confuse operators, such as the number of components of each type. Operators do not always read detailed recommendations, leading to complaints about mistakes, especially when confidence levels are around 0.5 or for transient issues.

#### Data and Model Improvements
Adding new features to the model can be slow, as it requires waiting for sufficient data. We extended the retention period of PhyNetâ€™s monitoring data to facilitate this. Old incidents are down-weighted in the deployed Scout to account for obsolescence. Learning from past mistakes, we increased the weight of misclassified incidents in future re-training. Not all incidents have the correct label, which can lead to mislabeling and incorrect learning. This can be mitigated by de-noising techniques and analyzing the incident text.

#### Concept Drift
While the CPD+ algorithm helps the Scout be resilient to new incidents, concept drift problems occasionally occur. During the last two years, there were a few weeks where the accuracy dropped to 50%, despite frequent retraining. This is a known issue in machine learning, and we are exploring solutions to address it.

#### Discussion
Scouts significantly reduce investigation times but should not be relied on for incident triggering. Specific incidents are easier to classify, while broad or overlapping incidents are more challenging. Operators can improve the starter Scout created by the framework by adding rules and specialized features. The framework's effectiveness depends on the quality of input data, and some teams may need to use non-ML-based techniques due to GDPR constraints. Not all teams experience the same degree of misrouting, and the framework requires correct annotations.

#### Potential Drawbacks
The team-by-team approach has drawbacks, such as the inability to route an incident when all Scouts return "no" and the complexity of building isolated Scouts for teams with interdependencies. The side-effect of aggregating sub-components can dilute the impact of individual device problems, but overall accuracy remains high. Alternative designs, such as considering all devices or building separate classifiers per component type, are not feasible in our case.

#### Related Work
Many automated diagnosis tools aim to find a single root cause but fail in some scenarios. Application-specific incident routers are too heavy for DC-scale operations. Software engineering work focuses on finding the right engineer to fix bugs, which is not applicable to incident routing. Measurement studies provide insights into network incidents and misroutings, helping to build better Scouts.

#### Conclusion
We investigated incident routing in the cloud and proposed a distributed, Scout-based solution. Even a single Scout can significantly reduce investigation times. This work does not raise any ethical issues.

#### Acknowledgments
The authors thank Sumit Kumar, Rituparna Paul, David Brumley, Akash Kulkarni, Ashay Krishna, Muqeet Mukhtar, Lihua Yuan, and Geoff Outhred for their help with the deployment of the PhyNet Scout and their feedback. We also thank the shepherd and SIGCOMM reviewers for their insightful comments. Jiaqi Gao was supported by a Microsoft internship and NSF grant CNS-1834263. Nofel Yaseen was supported in part by CNS-1845749.

#### References
[1] Adaboost. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.