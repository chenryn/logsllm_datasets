SODP
1
2
3
1
1
2
3
1
0
0
0
0
1
0
0
1
4
4
4
0
2
1
1
1
7
7
7
0
5
4
4
1
TABLE II: Comparison of the number of 5-year traces with
at least one data loss event. We evaluate different disks per
server, data protection schemes, and the number of distributed
spares for differing disk capacities.
Figure 16 shows the likelihood of a failure burst as we alter
the number of disks allocated to each server. Not surprisingly,
as we increase the number of disks per server the number
of failure bursts over the life of a storage system increases.
Table II then shows how well Trinity’s declustered parity
scheme protects against data loss events compared with an
SODP data placement scheme while varying the spare capacity
and drive capacity. We use the 123 and 164 disk conﬁgurations
with G-SODP because an insufﬁcient number of disks exist
for an RS(8,2) SODP conﬁguration at the smaller disk counts.
We see that SODP prevents data loss with a lower sparing
overhead better than the existing Trinity ﬁle system, however
a rapid burst of failures within a single stripe before copyback
completes still causes a single data loss event in the 164 disk
conﬁguration.
Fig. 15: Comparing 50 simulated survival curves from the
ﬁtted Weibull regression model to a nonparametric Kaplan-
Meier estimate of the survival curve with collected data for
two combinations of ﬁle system ID, node vertical position,
and drawer row.
simulations illustrates areas in which the Weibull model is
missing structure in the data. The simulated failure curves in
Figure 15 look consistent with the Trinity data overall, with the
regression model largely capturing the variation in lifetimes in
drives with different covariate values. There is a sharp drop in
the survival curve shortly before the 10,000 hour lifetime mark
that is not consistent with the Weibull assumption. This drop
was due to a batch of drives failing or being preemptively
replaced due to a ﬁrmware issue. Addressing this is the
scope of future work but its inclusion precludes the use of
quantitative goodness-of-ﬁt assessment.
V. EVALUATION
A. Trinity Storage System Overview
LANL’s Trinity ﬁle system is composed of two identical
ﬁle systems accessible through the same set of gateway nodes
within the Trinity platform. The identical ﬁle systems are
organized as two parallel aisles of racks within our data center
to both enable easier servicing/upgrades and protect against
some types of failures external to the ﬁle systems. Each ﬁle
system has 6 total metadata servers and 216 Lustre object
storage servers (OSS) each with a single 41 disk Lustre object
storage target (OST). OSS node pairs share a single two-
drawer 84-bay disk chassis with 41 drives assigned to each
of the OSS (the remaining 2 slots contain SSDs used as
journal devices). Each drawer within the 84 disk enclosure
is composed of 3 rows with 14 drive slots per row. Row 1
holds the 14 drives nearest the front of the drawer and row 3
holds the 13 drives and the SSD at the rear of the drawer. The
41 disk OSTs use a declustered parity approach to construct
8+2 protected stripes with 128KiB stripes forming a 1MiB
stripe set. The simulations presented in this paper assume that
all drives are 6TB Seagate Makarra drives, the ﬁle system is at
60% capacity utilization, and the rebuild disk bandwidth has
been set at approximately 50MB/s per disk [31].
B. Simulation Failure Traces
To create a sufﬁcient number of failure streams to perform
our reliability analysis we use the trinity ﬁle system con-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
351
s
s
o
l
a
t
a
d
f
o
y
t
i
l
i
b
a
b
o
r
P
100%
80%
60%
40%
20%
0%
DP
dRAID
O-SODP
G-SODP
RAID
 0  2000  4000  6000  8000 10000
Number of drives
s
s
o
l
a
t
a
d
f
o
y
t
i
l
i
b
a
b
o
r
P
50%
40%
30%
20%
10%
0%
DP
dRAID
RAID
O-SODP
G-SODP
 0  2000  4000  6000  8000 10000
Number of drives
s
s
o
l
a
t
a
d
f
o
y
t
i
l
i
b
a
b
o
r
P
100%
80%
60%
40%
20%
0%
DP
dRAID
O-SODP
G-SODP
RAID
 0
 0.2
 0.4
 0.6
% Failures
 0.8
 1
s
s
o
l
a
t
a
d
f
o
y
t
i
l
i
b
a
b
o
r
P
30%
25%
20%
15%
10%
5%
0%
DP
dRAID
RAID
O-SODP
G-SODP
 0
 0.2
 0.4
 0.6
% Failures
 0.8
 1
(a) Simultaneous Failures
(b) Failure Burst over 24h
(a) Simultaneous Failures
(b) Failure Burst over 24h
Fig. 17: The probability of data loss with failure of 1% of the
drive population as the number of disk drives are scaled. In 17a
we see that if the drive loss is instantaneous a non-overlapping
RAID scheme provides the greatest fault tolerance. However
in 17b we distribute the failures over a 24 hour period which
allows the fast rebuild performance of declustered schemes to
greatly reduces the overall probability of data loss.
Fig. 18: The probability of data loss varied with the percentage
of failed disks in a population of 11000 drives. During instan-
taneous failures in 18a traditional declustered parity schemes
experience a 100% chance of data loss once approximately
0.6% of the drives have failed. When the failures are dis-
tributed over 24 hours SODP schemes exhibit a less than 1.2%
chance of data loss.
D. Catastrophic Failures Analysis
To explore how different data protection schemes perform
during common burst failure scenarios (e.g. a power outage or
cascading failure that results in a large number of drives failing
in a short
time window) we simulated failures correlated
in time but not correlated in any other dimension. We also
compare the corresponding rebuild performance for schemes
including traditional RAID, dRAID, Trinity’s declustered par-
ity (DP), O-SODP and G-SODP.
1) Effect of Disk Population Size: Figure 17 shows the
probability of data loss (PDL) during a burst failure of 1%
disk failures and increased number of disks. In particular, we
increase the total number of disks from 1100 to 11,000 and
examine the PDL for 1% drive population failure instanta-
neously and randomly distributed over 24 hours. In Figure
17(a), failures occur in an instantaneous burst and rebuild
time is irrelevant. Non-overlapping RAID can tolerate the
most simultaneous failures with 11.3% PDL. The greater fault
tolerance of the SODP schemes protects data at smaller disk
counts but is not sufﬁcient for large disk populaitons. With
Trinity’s declustered parity and dRAID we see a 100% chance
of data loss over 6600 disks. In Figure 17(b), the failures
simulate a cascading failure occurring over 24 hours, thus data
is rebuilt during the 24-hour failure period for the declustered
parity schemes. By tolerating more failures and having fast
rebuild performance, G-SODP and O-SODP have 1.05% and
1.2% PDL for 11,000 disks, respectively. Trinity’s declustered
parity and dRAID also beneﬁt from fast rebuild performance,
but the lack of greater fault tolerance lowers the PDL to only
34.6%. Interestingly, even with failures distributed over 24
hours RAID is better than Trinity’s declustered parity and
dRAID indicating that in this experiment fault tolerance is
more important than rebuild performance.
2) Effect of Burst Size: Figure 18 varies the number of
failed disks in a population of 11,000 drives from 0% to 1% to
compare each of the parity placement schemes. As before we
examine the probability of data loss (PDL) for simultaneous
failures and failures over 24 hours. From Figure 18(a) it’s
obvious that the probability of data loss increases with the
percentage of simultaneous failures. At approximately 0.6% of
DP
dRAID
O-SODP
G-SODP
RAID
s
s
o
l
t
a
a
d
f
o
y
t
i
l
i
b
a
b
o
r
P
100%
80%
60%
40%
20%
0%
 0
 8  12  16  20  24
 4
Time window (Hours)
Fig. 19: The probability of data loss as 1% of drive failures
are distributed over a longer time scale. As the failure window
extends beyond the time to rebuild a disk we see that the
SODP schemes provide better probability of data loss than
non-overlapping RAID schemes. G-SODP provides slightly
lower probabilities of data loss compared to O-SODP.
the total drive population failed the PDL of Trinity declustered
parity and dRAID reach 100%, while G-SODP experiences
32.3% PDL and O-SODP has 23% PDL. RAID again tolerates
the most simultaneous failures with a 2.6% chance of data
loss. Figure 18(b) shows the probability of data loss with the
failures distributed over 24 hours. We see that the SODP have
the lowest chance of data loss due to high fault tolerance and
fast rebuilds. Even in the case of 1% failures, O-SODP and
G-SODP have 1.2% and 1.05% PDL, respectively. Because no
rebuilds completed in 24 hours RAID experienced the same
PDL as Figure 18(a) while both Trinity declustered parity and
dRAID shows a great reduction in PDL due to their faster
rebuild performance.
Finally, we investigate how the time window over which
the failures occur effects the probability of data loss. Figure
19 shows the probability of data loss in 11,000 drives in the
presence of 1% failures distributed over varying timespans
(using a Poission arrival process for failures). Because 24
hours is greater than the rebuild time for RAID the PDL
remains unchanged within the varying time period. However,
as declustered rebuilds complete in 2-3 hours we see the PDLs
of Trinity declustered parity and dRAID are reduced rapidly.
Even with slower lower rebuild performance for O-SODP, G-
SODP we see extremely low probabilities of data loss.
3) Comparison of Rebuild Performance: Figure 20(a) com-
pares the disk rebuild time by injecting a random single
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
352
34.95
)
s
r
u
o
H
(
e
m
i
t
d
l
i