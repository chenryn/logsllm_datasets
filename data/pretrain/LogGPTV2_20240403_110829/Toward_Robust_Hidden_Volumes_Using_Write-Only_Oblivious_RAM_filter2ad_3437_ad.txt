cally distributed X with a binomial distributed X. P r[X ≥ 1] =
N )k = 1−2−k for N = 2·n. Therefore,
if we set k equal to our security parameter s, the probability of not
ﬁnding at least one free block for any single write will be negligi-
ble small in s with 2−s. Setting N to twice n will “only” double
storage requirements, but leads to high efﬁciency and practicality
as we will show in Section 4.2.
1−P r[X = 0]≈ 1−(cid:0) k
(cid:1)·( n
Free Blocks: A remaining detail is how to determine which
physical blocks β on the disk are actually “free”, so no block b
of the ORAM maps to them. The challenge is to do this in an
efﬁcient way in order to keep a low complexity. A simple so-
lution is a reverse mapping that tags each block β on the disk
with the index for the ORAM block that maps to it. For an op-
eration ORAMWrite(b,d), what U actually writes to disk block β
0
is Encκ(d)||Encκ(b). Then, when U needs to determine if a disk
block β is free, U decrypts β’s content to restore ORAM address
b and checks if Map[b] = β. If that condition is not true, then it
means that β is an “old” version of ORAM block b, so β is free,
and it can be safely overwritten. Consequently, U can therewith
check if a block is free in constant time.
In practice, we cannot write the two ciphertexts Encκ(d)||Encκ(b)
into a single block of size B, as |d| = B already. Also, for each
IND$-CPA encryption, we need to store random coins such as an
IV or counter. Consequently, we write the ciphertext of Encκ(d)
into block β, and we write Encκ(b) and the encryptions’ random
coins into another metadata block β(cid:48). One can imagine that, in
addition to the N hard disk blocks for the write-only ORAM, we
need additional N blocks for metadata. Each time an ORAM block
β is written to disk, the corresponding metadata block β(cid:48) is also up-
dated. As the mapping between an ORAM block and its metadata
block is ﬁxed, this does not have any consequences for security.
A straightforward optimization of this idea is to store multiple IVs
and multiple Encκ(b) for multiple ORAM blocks in a single meta-
data block β(cid:48), as typically |IV |+|Encκ(b)|  2, then we are guar-
anteed that if we (recursively) store our map in another ORAM, that
ORAM in turn will have a map that is no greater than half the size of
the original map. Therefore, after O(logn) recursive ORAMs, we
will have a map that is constant size and can be stored in memory.
This reduces the map to a much more comfortable size, but at
the expensive of increasing the communication complexity: now,
to access the map, we have to, in turn, access O(logn) recursive
ORAMs. This will also slightly impact our stash analysis, since the
ORAMs that hold the map do not have deterministic arrival rates,
since their arrivals are in fact the result of service on the above
queue. Fortunately, we can model them as M/M/1 queues with
expected arrival rate equal to 1 which still results in exponentially
distributed stationary probabilities [7], and we end up with the same
O(logn) bound on the size.
As seen in algorithms 5 and 4, we can then simply treat the
map for each level as another ORAM and issue ORAMRead and
ORAMWrite calls as necessary. Each level is unaware of how the
next level structures itself, only that it provides an interface to read
and write, and that it can do so securely. However, we cannot treat
the map as a simple associative array now because the recursive
ORAM will need to store more than one address per block in or-
der to guarantee that we have only O(log n) levels of recursion.
Therefore, to read an entry from the map, we have to calculate
logN (cid:99), the number of entries we can ﬁt per block, then ﬁg-
N = (cid:98) B
ure out which block the entry we want will be in. For instance, if
we want the address for block i, we would get block (cid:98) i
n(cid:99) from the
map, and read the i modNth entry from it.
4.4 Write Complexity
For each ORAMWrite operation, our write-only ORAM reads k
random blocks, checks if they are empty, ﬁlls from zero to k−1 of
them with blocks from our stash, and updates the map accordingly.
The initial reading of the k blocks from the map (and hence
checking if a block is empty) requires reading one block from each
of log n recursive levels. Therefore, we can accomplish that ﬁrst
step with k · O(B · log n) = O(B · log n) communication com-
plexity. However, when we then write to the map, each recursive
ORAM has to, itself, read from all the recursive ORAMs “below” it
in order to read its own map. Thus, these writes costs O(B·log2n),
bringing the total complexity for an ORAMWrite to O(B·log2n).
This overhead would be disappointing, as there are several full-
functionality (read and write) ORAMs which provide the same over-
head. However inspired by Stefanov et al. [18], we can use the
following optimization to reduce the cost of the expensive map ac-
cesses. If we set the size of only the data blocks on our disk to
B = Ω(log2n), the total size of logn blocks in the recursive map is
no greater than the size of one B-sized data block at the “top level”.
What we end up with is non-uniform block sizes: top level blocks
(actual data blocks) have size Ω(log2n), and blocks that are part of
the map have only size χlogn for some constant χ≥ 2. We are still
guaranteed that the map will have O(logn) levels, but we reduce
the communication complexity of a map operation by a factor of
O(log n). Consequently, in terms of communication complexity,
reading from the map is constant in O(B), and updating/writing is
in O(B·logn). We can apply the same optimization again, as long
as B = Ω(log3n). In terms of communication for an ORAMWrite,
we reduce total complexity to O(B).
In conclusion, our write-only ORAM features O(B·s) memory
and constant O(B) communication complexity.
4.5 Security Analysis
Security for our scheme follows directly from the fact that we
only write to uniformly randomly chosen blocks at each level of
the ORAM. Since the blocks we choose are independent of the ad-
dresses in the user’s access pattern, they cannot reveal any infor-
mation about it to the adversary. Additionally, all the data we write
is freshly encrypted with a semantically secure encryption, so the
data itself cannot reveal any information.
Simulator: We note in the previous section that, to be useful in
our scheme, an ORAM needs to be (efﬁciently) simulatable. Fortu-
nately, such a simulator S is simple to construct for our scheme. S
proceeds in every operation to change k uniformly random blocks
at each level of the recursion to fresh random strings. Since, in
normal operation of our ORAM, we will access k random blocks
at each level, and those blocks will be ﬁlled with either random
strings or IND$-CPA encryptions indistinguishable from random,
S will be indistinguishable from an actual execution of our ORAM.
5. PRACTICAL HIDDEN VOLUME
ENCRYPTION WITH HIVE
Thus far we have presented a generic hidden volume encryp-
tion scheme which uses a write-only ORAM as a building block
and has constant communication complexity per access. We now
present HIVE that builds upon this idea and makes it practical. We
start by addressing an important consideration that we must take
into account when designing a practical, real-world system.
5.1 Uniform vs. Non-uniform Blocks
Current storage devices, e.g., today’s hard disks, have ﬁxed size
blocks (sectors). This means that we cannot use our non-uniform
block optimization from the previous section, unless we wanted to
use the base device blocks as “small” blocks and combine many of
these blocks together to make “large” blocks. However, most sys-
tems use either 512 or 4096 byte blocks, so there is not much room
for optimizing these parameters.
Fortunately, although we cannot easily obtain optimal O(B) com-
plexity with uniform blocks, our write-only scheme is still substan-
tially more efﬁcient than the currently most efﬁcient full-functio-
nality ORAMs, e.g., Path ORAM [18]. In Figure 2, we show the
comparative costs for our scheme and Path ORAM for a concrete
selection of parameters (see Appendix A for the recurrence rela-
Figure 2: Communication cost
Figure 3: Random mapping of blocks from volumes to disk blocks
tions used to calculate these numbers). Our write-only ORAM is
more than an order of magnitude more efﬁcient than Path ORAM.
To see why we are more efﬁcient, it is useful to consider the
complexity of our write-only ORAM and Path ORAM in terms of
the level of recursions required to store the map. We denote this
required level of recursion with L. In the uniform block setting,
Path ORAM has O(B·L·logn) communication complexity. Since
L = O(logn), this leads to the overall complexity of O(B·log2n).
Our write-only ORAM, by comparison, has O(B · L2) complex-
ity, with no independent logn factor. Again, since L = O(logn)
our scheme has overall O(B · log2 n) complexity. However, for
L to actually approach the worst-case log n, the block size needs
to be close to 2 · log n. In other words, with 512 byte = 4096 bit
blocks, we would need a disk holding 22056 bytes to approach that
many levels of recursion (4096 = 2 · log n, so n = 22048 blocks,
each of size B = 512 byte. Note that our write-only ORAM wastes
50% of all blocks, though). In contrast, up to 16 TB requires only
L ≤ 3 levels of recursion. However, for that same disk, we have
logn = 35. Therefore, for practical parameter choices, L2 will be
signiﬁcantly smaller than L· logn, resulting in large savings of at
least an order of magnitude.
The “jump” at 16 GByte ORAM size in Figure 2 is due to an
increased level of recursion L required for ORAMs of this size.
5.2 HIVE: Combining Volumes
A signiﬁcant drawback of our generic hidden volume construc-
tion is that it requires a separate ORAM for each volume. This
requires us to do a full operation on each of those ORAMs, result-
ing in a complexity dependent on max. If we make a reasonable
choice, say max = 10, this could be a signiﬁcant overhead. For-
tunately, our ORAM construction is particularly suited to solving
this problem. As we are only actually changing one data block each
operation, independent of the number of volumes (the rest are reen-
cryptions), we can thus combine all of our volumes into one. This
is the main idea of our new scheme HIVE.
 0 20 40 60 80 100 120 140 160 1 10 100 1000Total Communication per Write (KByte)ORAM Size (GByte)k=3, B=4096 Byte, t=10 MByteL=1L=2L=1L=2Path ORAMWrite-Only ORAMVolume 1Volume 2Volume max...block b1. Map[b] = β 2. Map[b] = β' block βblock β'Hard DiskOverview: HIVE’s approach will be to store all the volumes
randomly interleaved. Then, as shown in Figure 3, after writing to a
block b, we will randomly change b’s mapping Map[b] from β to β(cid:48)
using our write-only ORAM trick. This hides the write pattern for
all volumes. We now present the full procedure for HIVE’s writing
(HVEWrite) in Algorithm 6 and for HIVE’s reading (HVERead)
in Algorithm 7.
HVEWrite: Compared with our original ORAM scheme, there
is only one major change: instead of only modifying one block out
of k given by the indices in S, we now have to potentially mod-
ify all k blocks. To see why, it is useful to consider the following
scenario. Imagine user U wants to write a block into V1. In or-
der to do that, they randomly choose k blocks to form S. If, for
instance, three of these blocks contain data from V2, and one is
empty, we cannot simply write our block from V1 into this empty
space because, after continuously writing “around” blocks in V2, A
could infer its presence by the pattern we make trying to avoid it.
Therefore, in order to be secure, we have to make sure that no pre-
vious writes to a Vj,j > i can inﬂuence a write to Vi. Intuitively, if
operations in higher volumes cannot inﬂuence operations in lower
volumes, then the existence of a higher volume cannot be inferred