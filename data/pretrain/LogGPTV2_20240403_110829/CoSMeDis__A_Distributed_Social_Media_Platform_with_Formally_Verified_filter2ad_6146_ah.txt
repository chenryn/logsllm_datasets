[8]
[9] M. Backes, M. Maffei, and K. Pecina, “A security API for distributed
social networks,” in NDSS. The Internet Society, 2011.
[10] R. Baden, A. Bender, N. Spring, B. Bhattacharjee, and D. Starin,
“Persona: an online social network with user-deﬁned privacy,” in
SIGCOMM. ACM, 2009, pp. 135–146.
[11] G. Barthe, C. Fournet, B. Grégoire, P. Strub, N. Swamy, and S. Z.
Béguelin, “Probabilistic relational veriﬁcation for cryptographic imple-
mentations,” in POPL, 2014, pp. 193–206.
[13]
[12] T. Bauereiß, A. P. Gritti, A. Popescu, and F. Raimondi, “CoSMed: A
conﬁdentiality-veriﬁed conference management system,” in ITP, 2016.
J. C. Blanchette, S. Böhme, A. Popescu, and N. Smallbone, “Encoding
monomorphic and polymorphic types,” Logical Methods in Computer
Science, vol. 12, no. 4, 2016.
J. C. Blanchette and S. Merz, Eds., Interactive Theorem Proving -
7th International Conference, ITP 2016, Nancy, France, August 22-25,
2016, Proceedings, vol. 9807, 2016.
[14]
[15] A. Bossi, D. Macedonio, C. Piazza, and S. Rossi, “Information ﬂow
in secure contexts,” Journal of Computer Security, vol. 13, no. 3, pp.
391–422, 2005.
[16] S. Buchegger, D. Schiöberg, L.-H. Vu, and A. Datta, “PeerSoN: P2p
Social Networking: Early Experiences and Insights,” in Second ACM
EuroSys Workshop on Social Network Systems. ACM, 2009, pp. 46–52.
[17] S. Chong, J. Liu, A. C. Myers, X. Qi, K. Vikram, L. Zheng, and
X. Zheng, “Building secure web applications with automatic partition-
ing,” Commun. ACM, vol. 52, no. 2, pp. 79–87, 2009.
[18] S. Chong and R. V. D. Meyden, “Using Architecture to Reason About
Information Security,” ACM Trans. Inf. Syst. Secur., vol. 18, no. 2, pp.
8:1–8:30, Dec. 2015.
[19] M. R. Clarkson, B. Finkbeiner, M. Koleini, K. K. Micinski, M. N. Rabe,
and C. Sánchez, “Temporal logics for hyperproperties,” in POST, 2014,
pp. 265–284.
[20] L. A. Cutillo, R. Molva, and T. Strufe, “Safebook: A privacy-preserving
online social network leveraging on real-life trust,” IEEE Communica-
tions Magazine, p. 95, 2009.
[21] M. Dam, R. Guanciale, N. Khakpour, H. Nemati, and O. Schwarz,
“Formal veriﬁcation of information ﬂow security for a simple ARM-
based separation kernel,” in CCS, 2013, pp. 223–234.
[22] A. A. de Amorim, N. Collins, A. DeHon, D. Demange, C. Hritcu,
D. Pichardie, B. C. Pierce, R. Pollack, and A. Tolmach, “A veriﬁed
information-ﬂow architecture,” in POPL, 2014, pp. 165–178.
[23] H. de Nivelle, Ed., Automated Reasoning with Analytic Tableaux and
Related Methods - 24th International Conference, TABLEAUX 2015,
Wrocław, Poland, September 21-24, 2015. Proceedings, vol. 9323, 2015.
[24] D. Devriese and F. Piessens, “Noninterference through secure multi-
execution,” in IEEE Symposium on Security and Privacy, 2010, pp.
109–124.
[25] D. Dolev and A. C. Yao, “On the security of public key protocols,”
IEEE Trans. Information Theory, vol. 29, no. 2, pp. 198–207, 1983.
[26] R. Focardi and R. Gorrieri, “Classiﬁcation of security properties (part
i: Information ﬂow),” in FOSAD, 2000, pp. 331–396.
[27] P. W. L. Fong, M. M. Anwar, and Z. Zhao, “A privacy preservation
model for Facebook-style social network systems,” in ESORICS, 2009,
pp. 303–320.
J. A. Goguen and J. Meseguer, “Unwinding and inference control,” in
IEEE Symposium on Security and Privacy, 1984, pp. 75–87.
[28]
[29] S. Greiner and D. Grahl, “Non-interference with what-declassiﬁcation
in component-based systems,” in CSF, 2016, pp. 253–267.
[30] S. Guha, K. Tang, and P. Francis, “NOYB: Privacy in Online Social
Networks,” in Workshop on Online Social Networks. ACM, 2008, pp.
49–54.
J. D. Guttman and P. D. Rowe, “A cut principle for information ﬂow,”
in CSF, 2015, pp. 107–121.
[31]
[32] F. Haftmann and T. Nipkow, “Code generation via higher-order rewrite
systems,” in FLOPS 2010, 2010, pp. 103–117.
[33] C. Hammer and G. Snelting, “Flow-sensitive, context-sensitive, and
object-sensitive information ﬂow control based on program dependence
graphs,” Int. J. Inf. Sec., vol. 8, no. 6, pp. 399–422, 2009.
[34] D. S. Hardin, E. W. Smith, and W. D. Young, “A robust machine code
proof framework for highly secure applications,” in ACL2, 2006, pp.
11–20.
[35] S. Jahid, S. Nilizadeh, P. Mittal, N. Borisov, and A. Kapadia, “DECENT:
A decentralized architecture for enforcing privacy in online social
networks,” in PerCom.
IEEE, 2012, pp. 326–332.
[36] D. Jang, Z. Tatlock, and S. Lerner, “Establishing browser security
guarantees through formal shim veriﬁcation,” in USENIX Security,
2012, pp. 113–128.
[37] F. Kammüller, M. Wenzel, and L. C. Paulson, “Locales—a sectioning
concept for Isabelle,” in TPHOLs, 1999, pp. 149–166.
[38] S. Kanav, P. Lammich, and A. Popescu, “A conference management
system with veriﬁed document conﬁdentiality,” in CAV, 2014, pp. 167–
183.
[39] R. Küsters, T. Truderung, B. Beckert, D. Bruns, M. Kirsten, and
M. Mohr, “A hybrid approach for proving noninterference of java
programs,” in CSF, 2015, pp. 305–319.
J. Liu, M. D. George, K. Vikram, X. Qi, L. Waye, and A. C. Myers,
“Fabric: a platform for secure distributed computation and storage,” in
SOSP, 2009, pp. 321–334.
[40]
[41] A. Lux, H. Mantel, and M. Perner, “Scheduler-independent declassiﬁ-
cation,” in MPC, 2012, pp. 25–47.
[42] H. Mantel, “Possibilistic deﬁnitions of security - an assembly kit,” in
CSFW, 2000, pp. 185–199.
[43] ——, “Information ﬂow control and applications - bridging a gap,” in
FME, 2001, pp. 153–172.
[44] ——, “On the composition of secure systems,” in IEEE Symposium on
Security and Privacy, 2002, pp. 88–101.
[45] ——, “The framework of selective interleaving functions and the
modular assembly kit,” in FMSE, 2005, pp. 53–62.
[46] H. Mantel and A. Reinhard, “Controlling the what and where of
declassiﬁcation in language-based security,” in Programming Languages
and Systems, ser. LNCS, 2007, vol. 4421, pp. 141–156.
[47] D. McCullough, “Speciﬁcations for multi-level security and a hook-up
property,” in IEEE Symposium on Security and Privacy, 1987.
[48] ——, “A hookup theorem for multilevel security,” IEEE Trans. Software
[49]
Eng., vol. 16, no. 6, pp. 563–568, 1990.
J. McLean, “A general theory of composition for trace sets closed under
selective interleaving functions,” in IEEE Symposium on Security and
Privacy, 1994, pp. 79–93.
[50] C. Morgan, “The shadow knows: Reﬁnement and security in sequential
programs,” Sci. Comput. Program., vol. 74, no. 8, pp. 629–653, 2009.
[51] T. C. Murray, D. Matichuk, M. Brassil, P. Gammie, T. Bourke,
S. Seefried, C. Lewis, X. Gao, and G. Klein, “seL4: From general pur-
pose to a proof of information ﬂow enforcement,” in IEEE Symposium
on Security and Privacy, 2013, pp. 415–429.
[52] A. C. Myers, “Jﬂow: Practical mostly-static information ﬂow control,”
in POPL, 1999, pp. 228–241.
[53] T. Nipkow, L. C. Paulson, and M. Wenzel, Isabelle/HOL: A Proof
Assistant for Higher-Order Logic. Springer, 2002.
[54] R. Pardo and G. Schneider, “A formal privacy policy framework for
social networks,” in SEFM, 2014, pp. 378–392.
[55] L. C. Paulson and J. C. Blanchette, “Three years of experience with
link between automatic and interactive
Sledgehammer, a practical
theorem provers,” in IWIL, 2010.
744
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:28:13 UTC from IEEE Xplore.  Restrictions apply. 
[56] W. Rafnsson and A. Sabelfeld, “Compositional information-ﬂow secu-
rity for interactive systems,” in CSF, 2014, pp. 277–292.
[57] ——, “Secure multi-execution: Fine-grained, declassiﬁcation-aware,
and transparent,” Journal of Computer Security, vol. 24, no. 1, pp. 39–
90, 2016.
[58] A. Sabelfeld and A. C. Myers, “Language-based information-ﬂow
security,” IEEE Journal on Selected Areas in Communications, vol. 21,
no. 1, pp. 5–19, 2003.
[59] A. Sabelfeld and D. Sands, “Declassiﬁcation: Dimensions and princi-
ples,” Journal of Computer Security, vol. 17, no. 5, pp. 517–548, 2009.
[60] Y. Saito and M. Shapiro, “Optimistic replication,” ACM Comput. Surv.,
vol. 37, no. 1, pp. 42–81, 2005.
“The Scalatra Web Framework,” 2016, http://scalatra.org/.
[61]
[62] D. Sutherland, “A model of information,” in 9th National Security Conf.,
1986, pp. 175–183.
[63] M. Wenzel, “Isar—a generic interpretative approach to readable formal
[64]
proof documents,” in TPHOLs, 1999, pp. 167–184.
J. Yang, K. Yessenov, and A. Solar-Lezama, “A language for automat-
ically enforcing privacy policies,” in POPL, 2012, pp. 85–96.
[65] A. Zakinthinos and E. S. Lee, “A general theory of security properties,”
in IEEE Symposium on Security and Privacy, 1997, pp. 94–102.
[66] S. Zdancewic, L. Zheng, N. Nystrom, and A. C. Myers, “Secure
program partitioning,” ACM Trans. Comput. Syst., vol. 20, no. 3, pp.
283–328, 2002.
[67] N. Zeldovich, S. Boyd-Wickizer, and D. Mazières, “Securing distributed
systems with information ﬂow control,” in NSDI, 2008, pp. 293–308.
A. More Details on the Security Transport Theorem
APPENDIX
Here is the formal deﬁnition of the notion of “stronger
security model,” used in Theorem 2.
Deﬁnition 3. Let Aut be an I/O automaton and (P) and (P’)
two security properties operating on it. (P) is said to have
a stronger security model than (P’) if there are two partial
functions f : Sec (cid:3) Sec
from the secrets
and observations of (P) to those of (P’) that preserve the
secrecy and observation infrastructures, the bounds and the
triggers, i.e., such that the following are true:
• isSec
(cid:6)(trn) holds iff isSec(trn) and getSec(trn) ∈ dom( f ),
and g : Obs (cid:3) Obs
(cid:6)
(cid:6)
and in this case f (getSec(trn)) = getSec
• isObs
(cid:6)(trn) holds iff isObs(trn) and getObs(trn) ∈ dom(g),
and in this case g(getObs(trn)) = getObs
(cid:6)(trn)
(cid:6)(trn)
(cid:6)(trn)
• T(trn) implies T
• B
(cid:6), tl
(cid:6)(sl
that map f (tl) = tl
(cid:6)) and map f (sl) = sl
and B(sl, tl)
(cid:6)
∗ → Sec
Above, map f : Sec
of the partial function f : Sec (cid:3) Sec
any secrets s /∈ dom( f ).
(cid:6)∗
(cid:6)
imply that there is a tl such
denotes the secret-wise extension
to sequences, omitting
(cid:6)
In case f
is deﬁned and injective on all secrets oc-
then it can be inverted and the last con-
(cid:6)) implies
curring in Aut,
dition above simpliﬁes to checking that B
B(map f −1(sl
(cid:6)), map f −1(tl
(cid:6)(sl
(cid:6), tl
(cid:6))).
Choosing partial functions f and g allows us to weaken the
power of the observer and the notion of secrets by making the
sets of observable and secret transitions smaller. In particular,
observable transitions whose observations are not translated by
g become unobservable in the new security model.
745
To prove that (P1) || (P2) implies (P’) for the main paper’s
(cid:6) = Obs, g is
(cid:6) = Sec1, and f extracts the secret s1 out of
running example, we use the theorem where Obs
the identity, Sec
(1, s1) or (s1, s2).
Note that Aut2 does not produce local secrets, hence (2, s2)
never occurs. Moreover, f is injective on valid secrets, because
the sets of secrets occurring locally in Aut1 and in commu-
nication with Aut2 are disjoint, and, in a communication, the
secret received by Aut2 is uniquely determined by the secret
sent by Aut1. The inverse of f is
(1, (upd, pst))
((snd, pst), pst)
if s1 = (upd, pst)
if s1 = (snd, pst)
The above assumptions are then immediate to check.
−1(s1) =
(cid:2)
f
We also rely on Theorem 2 to prove that the various amend-
ments of the component properties required to achieve com-
positionality (illustrated in Section V-B) are indeed strength-
enings of the previous versions—which is important for guar-
anteeing that our quest for compositionality did not weaken
any bit of the component properties we started with. Thus,
to prove that the version of property (P1) after amendments
1–3 implies the original, we deﬁne g as the identity on local
observations and f as f ((upd, pst)) = pst.
B. Heuristic for Achieving Compositionality
Context. We have a monolithic system modeled as an I/O
automaton Aut, delivered with a conﬁdentiality guarantee (P)
modeled as BD Security. We extend the system to a distributed
system, consisting of several nodes able to communicate. Each
of Aut, with new actions for inter-
node is an extension Aut
node communication. The question is what conﬁdentiality
property can we prove for the distributed system.
(cid:6)
Step 1. By its nature, (P) states something about a notion
of secret and the way it is protected during interaction with the
outside world, which takes place through actions and outputs
of the original automaton Aut. We analyze what happens with
, identifying
the secret during inter-node communication in Aut
the roles of secret issuer and secret receiver for the nodes. This
leads to a split of (P) in two variants, (P1) and (P2).
(cid:6)
Step 2. If necessary, we modify the secrecy infrastructure
of (P1) and/or (P2) so that
the communication of secret-
correlated items is acknowledged by both as a secret-producing
action. This may require a modiﬁcation of the bound as well,
to account for the correlation.
Step 3. We strengthen the observation power for (P1) and
(P2) by allowing the observer to access any communication
information that does not compromise the secrets.
Step 4. If we can identify a unique source for the secrets,
we apply Theorem 3 for a distributed system consisting of one
component satisfying (P1) and n − 1 components satisfying
(P2). Then we apply Theorem 2 to customize the compound
bound into a property that uses the notion of secret from the
secret-issuer component only.
C. More Details on the Veriﬁed Properties
Post Conﬁdentiality. The post conﬁdentiality property of
the original CoSMed has a “dynamic” bound, incorporating
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:28:13 UTC from IEEE Xplore.  Restrictions apply. 
changes in the conﬁdentiality of the post due to visibil-
ity updates and friending or unfriending of observers. We
summarize the formal attacker models (AM) and security
properties (SP) in Table I. For a detailed explanation, see [12].
The corresponding property of the issuer node of CoSMeDis
extends that of the original CoSMed with the sending of posts:
the additions are highlighted. On the receiver side, nothing
is leaked to local observers beyond the number of updates
(because communication trafﬁc is observable). Only when an
observer is added as a remote friend of the post owner, or
the post is marked as public, the trigger ﬁres and the post is
declassiﬁed. The property for the entire distributed system has
the observations built from all the component observations (as
described in the main paper’s n-ary compositionality theorem).
However, the secrets, as well as the bound, are those of the
secret issuer component—this is because we show the end
product property, obtained after applying both the composi-
tionality and the transfer theorems.
Friendship Conﬁdentiality. For CoSMed, we had proved
conﬁdentiality properties for local friendship status of any (ar-
bitrary but ﬁxed) users UID1 and UID2, and friendship requests
between them. Observers learn nothing about their friendship
status beyond the status (and updates to it) during phases in
which one of the observers is friends with either UID1 or UID2.
In particular, if none of the observers ever become friends
with UID1 or UID2, then they never learn anything about their
friendship status. Moreover, observers learn nothing about the