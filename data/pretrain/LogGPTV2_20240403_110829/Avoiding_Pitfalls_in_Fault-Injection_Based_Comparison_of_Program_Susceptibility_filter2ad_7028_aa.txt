# Avoiding Pitfalls in Fault-Injection Based Comparison of Program Susceptibility to Soft Errors

**Authors:**
- Horst Schirmeier
- Christoph Borchert
- Olaf Spinczyk

**Conference:**
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks

**Abstract:**
Since the first identification of physical causes for soft errors in memory circuits, fault injection (FI) has become a standard methodology to assess the fault resilience of computer systems. Various FI techniques have been developed to mimic these physical causes, enabling the measurement and comparison of program susceptibility to soft errors.

In this paper, we analyze the process of evaluating programs hardened by software-based hardware fault-tolerance mechanisms under a uniformly distributed soft-error model. We identify three common pitfalls in the interpretation of FI results, which are prevalent even in renowned conference proceedings. Using a simple machine model and transient single-bit faults in memory, we provide counterexamples that reveal the inadequacy of current practices and support our findings with real-world examples. Specifically, we demonstrate that the fault coverage metric is unsuitable for comparing programs. Instead, we propose using extrapolated absolute failure counts as a valid comparison metric.

## I. Introduction

Since the 1970s, when the physical causes of transient hardware errors (soft errors) were identified [1], [2], computer systems have been fortified at both hardware and software levels to reduce the likelihood of system failures [3]. This paper focuses on software-implemented hardware fault-tolerance mechanisms for mitigating soft errors, such as those described in [4], [5], [6], [7], [8].

Software-based hardware fault-tolerance mechanisms must be tested, measured, and compared to evaluate their effectiveness in specific use cases. In the simplest scenario, an unmodified baseline version of a benchmark program (with predefined input) is compared to a hardened version of the same program, with the expectation that the latter will exhibit increased fault resilience.

However, soft errors per bit are extremely rare in reality [9], [10], [11], making it impractical to deploy hardened systems in their target environment under normal conditions to observe their fault-handling capabilities. Even if soft errors occur, this approach does not provide statistically significant evidence of reduced fault susceptibility in the hardened program.

For the past two decades, fault injection (FI) [12], [13], [14], [15], [16] has been the common solution to this problem. FI mimics the effects of the original causes of soft errors but with a significantly higher occurrence probability to trigger fault-tolerance mechanisms frequently enough to gather sufficient evidence of their effectiveness. Note that FI is also used for other purposes, such as injecting software bugs or functional testing of fault-tolerance measures, which are beyond the scope of this paper.

Early hardware-based FI solutions involved experiments with radiation sources [17]. Although these experiments trigger realistic fault scenarios, they have low controllability, are not deterministically repeatable, and are extremely expensive and delicate to handle. Pin-level FI and experiments under electromagnetic interference share some of these disadvantages [18].

### A. Benchmark Comparison with Hardware-based FI

Despite the costs and implementation complexities, the procedure for comparing a baseline and a hardened version of a benchmark with hardware-based FI is straightforward. After exposing the system-under-test to a radiation source, such as a Californium-252 isotope [18], the baseline version is executed N consecutive times (with proper system resets between runs). The system's output is observed, and each deviation from the correct behavior increments the failure count F. With a sufficiently large N, the failure probability can be approximated using the relative frequency of observed failures, P(Failure)baseline ≈ Fbaseline/Nbaseline. The same approximation is calculated for the hardened version, yielding P(Failure)hardened. The comparison ratio r is then calculated as:

\[ r = \frac{P(\text{Failure})_{\text{hardened}}}{P(\text{Failure})_{\text{baseline}}} \]

The hardened version is considered more resilient if r < 1.

### B. Software-implemented Fault Injection

Due to the disadvantages of hardware-based FI, software-implemented FI has become a widely adopted alternative. In this method, transient hardware faults are emulated by corrupting the state of a simulated machine [19], [20] or by injecting faults into development hardware via a debugger interface [21]. Faults can be injected only into parts of the machine visible to the FI implementation, such as memory and CPU registers.

This paper dissects current practices in interpreting software-implemented FI results for transient memory errors. We identify three common pitfalls that can skew or invalidate the analysis, leading to incorrect conclusions when comparing the effectiveness of software-based hardware fault-tolerance solutions applied to benchmark programs. Each pitfall is supported by a concrete example, and we propose an alternative metric for benchmark comparison.

The main contributions of this paper are:
- Dissection of current practices in software-implemented FI for transient memory errors, including experiment-reduction techniques (Section III).
- Demonstration that the widely used fault coverage metric is unsound for comparing different programs (Section IV).
- Construction of an objective comparison metric based on extrapolated absolute failure counts, with the supporting mathematical foundation (Section V).

## II. Setting the Stage: Definitions, and Machine, Fault, and Failure Model

### A. Terms and Definitions

We use the terms fault, error, and failure in their classical meanings [22] from a software-level fault-tolerance perspective. A failure is defined as a deviation of the software system’s behavior, primarily its output, from its correct behavior. An error, a deviation of the system’s internal state from the norm, may lead to a failure. The root cause of an error is a fault that becomes active; otherwise, the fault remains dormant.

The term "soft error" refers to a transient corruption of machine state, such as bits in main memory. From a software-level perspective, a "soft error" is a fault that can lead to errors and failures.

### B. Fault Injection (FI)

Fault injection [12], [13], [14], [15], [16] started as a testing technique for dependability validation. A common use case involves uncovering design and implementation weaknesses by providing faulty inputs. For quantitative evaluation of software-based hardware fault-tolerance mechanisms, the injected faults must closely represent real hardware faults, with a realistic spatial and temporal distribution.

This paper focuses on software-implemented FI for quantitative evaluation. Other goals of FI, such as testing or bug injection, are beyond the scope of this study.

### C. Fault and Machine Model

To focus on the core findings, we use a simplistic machine model. We assume a simple RISC CPU with in-order execution, no cache levels, and a timing of one cycle per CPU instruction. The CPU executes programs from read-only memory. Benchmark runs can be carried out deterministically, and the machine can be paused and resumed at any cycle to inject faults.

As the basic fault model, we use a classic soft-error model: uniformly distributed, independent, and transient single-bit flips in main memory, modeled as originating from ionizing radiation. This model is still valid for contemporary memory technology [11].

### D. Failure Model and Benchmark Setup

The effectiveness of a software-based hardware fault-tolerance mechanism can be assessed by applying it to a set of benchmarks and comparing the failure probability of the hardened variants to their baseline counterparts. The primary ingredients are benchmark programs, a fault-tolerance mechanism, and a definition of failure that fits the benchmarks’ purpose. Additionally, the benchmark inputs must be representative of real operational profiles.

As a real-world example, we use benchmarks, fault-tolerance mechanisms, and result data from an earlier publication [8]. In this publication, we developed a library of software-based fault-tolerance mechanisms and applied them to a set of run-to-completion test programs from the eCos operating system [24]. We differentiate between eight experiment-outcome types, coalescing "No Effect" and "[Error] Detected & Corrected" into "No Effect," and the remaining six into a "Failure" type.

## III. Fault-Space Scanning and Pruning

In this section, we discuss the statistics behind improbable independent faults and motivate that injecting a single fault per experiment suffices. We show that even with this simplification, the number of necessary FI experiments to cover the entire fault space is practically infeasible. Consequently, we describe two widely used experiment-reduction techniques: sampling and def/use pruning. By examining common practices in applying these techniques, we identify the first pitfall and present a means to avoid it.

### A. Improbable Independent Faults

With a fault model of uniformly distributed, independent, and transient single-bit flips in main memory, a single run of a simple run-to-completion benchmark can theoretically be hit by any number of independent faults. Multiple faults can occur at arbitrary points in time and affect different bits in memory.

In Figure 1a, each black dot represents a possible time (CPU cycle) and space (memory bit) coordinate where a fault can flip a bit in memory. Without further consideration, running one FI experiment for every subset of these coordinates is infeasible due to the exponential growth of the power set. However, in reality, the probability of a single-bit flip occurring at one bit in main memory within the time span of one CPU cycle is extremely low.