title:Avoiding Pitfalls in Fault-Injection Based Comparison of Program Susceptibility
to Soft Errors
author:Horst Schirmeier and
Christoph Borchert and
Olaf Spinczyk
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Avoiding Pitfalls in Fault-Injection Based
Comparison of Program Susceptibility to Soft Errors
Horst Schirmeier, Christoph Borchert, and Olaf Spinczyk
Department of Computer Science 12
Technische Universität Dortmund, Germany
e-mail: {horst.schirmeier, christoph.borchert, olaf.spinczyk}@tu-dortmund.de
Abstract—Since the ﬁrst identiﬁcation of physical causes for
soft errors in memory circuits, fault injection (FI) has grown into
a standard methodology to assess the fault resilience of computer
systems. A variety of FI techniques trying to mimic these physical
causes has been developed to measure and compare program
susceptibility to soft errors.
In this paper, we analyze the process of evaluating programs,
which are hardened by software-based hardware fault-tolerance
mechanisms, under a uniformly distributed soft-error model. We
identify three pitfalls in FI result interpretation widespread in the
literature, even published in renowned conference proceedings.
Using a simple machine model and transient single-bit faults
in memory, we ﬁnd counterexamples that reveal the unﬁtness
of common practices in the ﬁeld, and substantiate our ﬁndings
with real-world examples. In particular, we demonstrate that the
fault coverage metric must be abolished for comparing programs.
Instead, we propose to use extrapolated absolute failure counts
as a valid comparison metric.
I.
INTRODUCTION
Since the identiﬁcation of physical causes for transient
hardware errors (soft errors) in the 1970s [1], [2], computer
systems have been hardened on different hardware and software
levels to reduce the probability of system failures [3]. This paper
focuses on software-implemented hardware fault-tolerance for
soft-error mitigation, such as [4], [5], [6], [7], [8].
Software-based hardware fault-tolerance mechanisms need
to be tested, measured and compared in order to assess their
effectiveness in the particular use case. In the simplest case,
an unmodiﬁed baseline version of a benchmark program (with
some pre-deﬁned input) is supposed to compete with a hardened
version of the same program, and the latter is expected to exhibit
increased fault resilience.
However, soft errors per bit are very rare in reality [9], [10],
[11], and consequently hardened systems cannot simply be
deployed in their target environment under normal conditions
to observe their fault handling capabilities: Even if soft errors
occur, this approach does not yield statistically authoritative
evidence that proves a fault-susceptibility reduction of the
hardened program.
For at least two decades, the common solution to this
problem in the ﬁeld has been fault injection (FI) [12], [13], [14],
[15], [16]. FI is used to mimic the effects of the original causes
for soft errors to a certain degree, but with extremely increased
occurrence probability to trigger the fault-tolerance mechanisms
often enough for sufﬁcient evidence of their effectiveness. Note
that, besides for effectiveness measurements, FI is also used
for other purposes that are beyond the scope of this paper,
such as the injection of software bugs, or functional testing of
fault-tolerance measures.
Due to closeness to the root causes of soft errors in reality,
early hardware-based FI solutions were based on experiments
with radiation sources [17]. Although triggering realistic fault
scenarios, the main disadvantage of this approach is that
FI experiments have a very low controllability (where and
when to inject a fault). As a consequence,
they are not
deterministically repeatable (the ability to inject a speciﬁc
fault, and to obtain the same experiment result) [16]. These
properties are important for reproducing bugs in the fault-
tolerance implementation. Moreover, they are required for
systematic so-called “pruning” techniques to reduce the amount
of experiments to conduct. Additionally, experiments with
radiation sources are extremely expensive (both money and
time-wise), and handling of radioactive material is delicate. Pin-
level FI, and experiments under the inﬂuence of electromagnetic
interference, share these disadvantages to a certain degree [18].
A. Benchmark Comparison with Hardware-based FI
Besides the costs and the implementation intricacies, the
procedure of comparing a baseline and a hardened version of
a benchmark with hardware-based FI is quite straightforward.
After exposing the system-under-test to the inﬂuence of a
radiation source, e.g., a Californium-252 isotope [18], the
baseline version is executed on the target system N consecutive
times (with proper system resets in between). In each run,
the system’s output is observed, e.g., by recording the data
printed on a serial interface. Each time it behaves differently
from a previously deﬁned correct behavior, the failure count
F is incremented. With a sufﬁciently large N, the failure
probability for a benchmark run under increased radiation
conditions can be approximated by using the relative frequency
of observed failures, P (Failure)baseline ≈ Fbaseline/Nbaseline.
(Arlat et al. [12] deﬁne a reliability metric R = 1−P (Failure),
but this distinction has no practical relevance for this paper).
The same failure probability approximation is calculated
for the hardened version, yielding P (Failure)hardened. For
normal radiation conditions, both failure probabilities are
over-approximated, because the fault rate has been massively
increased by the radiation source. Compared to systems running
under normal conditions, this over-approximation constitutes a
linear and constant factor, which cancels out when calculating
the comparison ratio r,
r = P (Failure)hardened
P (Failure)baseline
.
The hardened version improves over the baseline iff r < 1.
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
DOI 10.1109/DSN.2015.44
DOI 10.1109/DSN.2015.44
319
319
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply. 
B. Software-implemented Fault Injection
Due to the disadvantages of hardware-based FI, a widely
adopted alternative is software-implemented FI. Here, transient
hardware faults are emulated by corrupting the state of a
simulated machine [19], [20], or by injecting faults into
development hardware via a debugger interface [21]. Thereby,
faults can be injected only into parts of the machine that are
visible to the FI implementation, such as memory and CPU
registers.
In this paper, we dissect current practices in interpreting
software-implemented FI results for transient memory errors
from the literature. We identify three common pitfalls that
can skew or even completely invalidate the analysis, and lead
to wrong conclusions when comparing the effectiveness of
software-based hardware fault-tolerance solutions applied to
benchmark programs. We support each pitfall with a concrete
example, and propose an alternative metric that can be used
for benchmark comparison.
In particular, the main contributions of this paper are:
• We dissect current practices in software-implemented
FI with regard to transient memory errors, including
FI experiment-reduction techniques (Section III). Our
ﬁndings – quantitatively substantiated by a real-world
data set – are that special care has to be taken to avoid
distorted results by such techniques.
• We show that the widely used fault coverage metric is
unsound for comparing different programs (Section IV).
Speciﬁcally, this metric is defective for the evaluation
of software-based hardware fault-tolerance mechanisms
applied to a benchmark program.
• As a remedy, we construct an objective comparison
metric based on extrapolated absolute failure counts,
and introduce the mathematical foundation supporting
this proposition (Section V).
The following section describes the fault and machine model
used throughout this paper. Section VI discusses possible
generalizations and implications of our ﬁndings, and, after
reviewing related work (Section VII), the paper concludes in
Section VIII.
II. SETTING THE STAGE: DEFINITIONS, AND MACHINE,
FAULT AND FAILURE MODEL
In this section, we ﬁrst deﬁne the semantics of fundamental
terms used throughout this paper. We then establish the fault
and machine model, and describe the assumed repeatable, de-
terministic FI experiment execution. Subsequently, we describe
the benchmark data we used, and describe the possible failure
modes these programs can exhibit.
A. Terms and Deﬁnitions
In this paper, we use the terms fault, error and failure
in their classical meaning [22] from a software-level fault-
tolerance perspective. A failure is speciﬁed by a deviation
of the software system’s behavior, primarily its output, from
its correct behavior. The failure mode differentiates between
different forms of failure. An error, a deviation of the system’s
internal state from the norm, may lead to a failure. The root
cause of an error is a fault that turns into an error if it is
activated. Otherwise, the fault stays dormant. [22]
The term “soft error” – a transient corruption of machine
state, such as bits in main memory – was originally devised from
a hardware perspective. From our software-level perspective,
a “soft error” is actually a fault, and forms the root cause
for errors and failures. Nevertheless, we will use “soft error”
throughout this paper, and actually mean transient faults.
B. Fault Injection (FI)
Fault
injection [12], [13], [14], [15], [16] started out
many years ago as a testing technique for dependability
validation. A common use case involves uncovering design
and implementation weaknesses, for example, by providing
faults in the program input. Here, the representativeness of the
injected faults is irrelevant for the goal of revealing program
bugs.
Since then, software-implemented FI has also been applied
to benchmark programs for quantitative evaluation of software-
based hardware fault-tolerance mechanisms, as in [4], [5], [6],
[7], [8]. This use case is completely different from testing.
For the assessment of fault-tolerance effectiveness, the injected
faults have to closely represent real hardware faults: A realistic
spatial and temporal distribution of the injected faults is crucial.
This paper only concerns software-implemented FI for
quantitative evaluation. In the following, we refer to FI as
software-implemented technique to obtain statistics for the
comparison of program susceptibility to soft errors. Other goals
of FI, such as testing or the injection of program bugs, are
beyond the scope of this study.
C. Fault and Machine Model
To focus on the core ﬁndings in this paper, we use a
simplistic machine model. Abstracting from CPU speciﬁcs,
we assume a simple RISC CPU with classic in-order execution,
without any cache levels on the way to a wait-free main memory,
and with a timing of one cycle per CPU instruction. The
CPU executes programs from read-only memory. Section VI-B
discusses possible generalizations from this simple model.
On this machine, benchmark runs can be carried out
deterministically, i.e., the same program with an identical start
conﬁguration (program input and machine state) is exposed
to a pre-deﬁned sequence of external events (timer interrupts
or other input at runtime), and leads to an exactly identical
program run.1 Additionally, the machine can be paused at an
arbitrary cycle during the run (e.g., to inject a fault by changing
the machine state) and resumed afterwards. In practice, this
can be achieved by, e.g., using a hardware simulator.
As the basic fault model throughout this paper, we use a
classic soft-error model widely used in the literature: uniformly
distributed, independent and transient single-bit ﬂips in main
memory, modeled as originating from direct inﬂuences of
ionizing radiation. We pick this fault model primarily due
to its simplicity, easing the illustration of the issues presented,
but a large-scale study from the year 2013 [11] conﬁrms that
1Note that deterministic does not mean that system reactions on external
events, such as asynchronous device interrupts, cannot be analyzed. In
deterministic benchmark runs, such events are replayed at the exact same
point in time during each run.
320320
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply. 
the model is still valid for contemporary memory technology.
As memory cells dominate the chip area of modern CPUs [23],
our ﬁndings may possibly also apply to SRAM-based on-
chip caches; Section VI-B discusses this, and other possible
implications. We quantize the time with a granule of CPU
cycles, restricting faults to be only injected between the
execution of one instruction and the next. Additionally, we
assume the ROM, holding the program instructions, to be
immune to faults.
We are aware that in practice, other parts of the machine
are also susceptible, and that errors may also propagate from
the CPU logic. Some of our ﬁndings may apply for other fault
models, too, but this is beyond the scope of this paper, and
material for future work.
D. Failure Model and Benchmark Setup
As already mentioned in the introduction, the effectiveness
of a software-based hardware fault-tolerance mechanism can be
assessed by applying it to a set of benchmarks, and comparing
the failure probability of these hardened benchmark variants
to their baseline counterparts. Thus, the primary ingredients
for this undertaking are benchmark programs, a fault-tolerance
mechanism, and a deﬁnition of failure that ﬁts the benchmarks’
original purpose. Additionally, the benchmark inputs must be
chosen based on a fair sample of the “operational proﬁle”, i.e.,
they must be representative of what to expect in real operation.
As a real-world example, we use benchmarks, fault-
tolerance mechanisms and result data from an earlier publication
[8] throughout this paper. In this publication, we developed
a library of software-based fault-tolerance mechanisms, and
aimed at protecting “critical” data with long lifetimes. These
mechanisms were applied to a set of run-to-completion test
programs with known output, belonging to the eCos operating
system [24]. From the raw data, we pick the results for the
BIN_SEM2 and SYNC2 benchmarks in both their baseline vari-
ant, and a variant hardened by checksums and data duplication
(termed “SUM+DMR”). Figure 2g shows the runtime (in CPU
cycles) and the memory usage of these benchmark variants.
In the previous publication [8], we ran extensive FI cam-
paigns with our FAIL* tool [25] and observed the benchmarks’
behavior after the injection. We examined the benchmark
output on the serial interface for silent data corruption, and
monitored the system for CPU exceptions and timeouts. Overall,
we differentiated between eight experiment-outcome types, of
which two – “No Effect” and “[Error] Detected & Corrected”
– can be interpreted as a benign behavior that has no visible
effect from the outside. We coalesce these two result types
into “No Effect”, and the remaining six failure modes into a
subsuming “Failure” type, as the detailed differentiation is of
no relevance for this paper.
III. FAULT-SPACE SCANNING AND PRUNING
In this section, we discuss the statistics behind improbable
independent faults, and thereby motivate that injecting a single
fault per experiment sufﬁces. We show that even with this
simpliﬁcation the number of necessary FI experiments to cover
the whole fault space is practically infeasible. Consequently,
we describe two widely used experiment-reduction techniques,
namely sampling and def/use pruning. By examining common
practices in applying these techniques, we identify our ﬁrst
pitfall, and present a means to avoid it.
A. Improbable Independent Faults
With a fault model of uniformly distributed, independent
and transient single-bit ﬂips in main memory (cf. Section II-C),
a single run of a simple run-to-completion benchmark can
theoretically be hit by any number of independent faults.
Multiple faults can occur at arbitrary points in time, and affect
different bits in memory.
In Figure 1a, each black dot denotes a possible time2 (CPU
cycle) and space (memory bit) coordinate where a fault can hit
the benchmark run and ﬂip a bit in memory, affecting the stored
value throughout the subsequent cycles until it gets eventually
overwritten. Without considering the problem in more depth,
trying to run one FI experiment each for every subset of these
hit coordinates is infeasible, as the cardinality of the power set
grows exponentially.
though,
In reality,
the probability for a single-bit ﬂip
occurring at one bit in main memory within the time span of one