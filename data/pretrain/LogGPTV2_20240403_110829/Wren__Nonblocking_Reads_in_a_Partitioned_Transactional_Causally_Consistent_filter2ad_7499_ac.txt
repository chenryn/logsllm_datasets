the local snapshot installed by pm
n
and rstm
n is aware that every
partition in the local DC has installed a local snapshot with
timestamp at least t. rstm
n is aware that
n
every partition in the local DC has installed all the updates
generated from all remote DCs with update time up to t(cid:2).
= t(cid:2) indicates that pm
= t indicates that pm
n . lstm
n
Finally, pm
n keeps a list of prepared and a list of committed
transactions. The former stores transactions for which pm
n has
proposed a commit timestamp and for which pm
n is awaiting
the commit message. The latter stores transactions that have
been assigned a commit timestamp and whose modiﬁcations
are going to be applied to pm
n .
n and rstm
B. Operations
Start. Client c initiates a transaction T by picking at random
a coordinator partition (denoted pm
n ) and sending it a start
request with lstc and rstc. pm
n uses these values to update
its lstm
n , so that pm
n can propose a snapshot that
is at least as fresh as the one accessed by c in previous
n generates the snapshot visible to T .
transactions. Then, pm
The local snapshot timestamp is lstm
n . The remote one is set
− 1. Wren enforces
as the minimum between rstm
the remote snapshot time to be lower than the local one, to
efﬁciently deal with concurrent conﬂicting updates. Assume c
wants to read x, that c has a version Xl in its private cache with
commit timestamp ct > lstm
n , and that there exist a visible
remote Xr with commit timestamp ≥ ct. Then, c must retrieve
Xr, its commit timestamp and its source replica to determine
whether Xl or Xr should be read according to the last writer
wins rule. By forcing the remote stable time to be lower than
n and lstm
n
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
(cid:3) Update remote stable time
(cid:3) Update local stable time
(cid:3) All versions of k
(cid:3) Local visible
(cid:3) Remote visible
(cid:3) All visible versions of k
(cid:3) Freshest visible vers. of k
n - transaction cohort.
m
m
m
m
m
i do
n , rst}
n , lst}
n ← max{rst
n ← max{lst
rst
lst
D ← ∅
for (k ∈ χ) do
end for
reply (cid:3)SliceResp D(cid:4) to p
Dk ← {d : d.k == k}
Dlv ← {d : d.sr == m ∧ d.ut ≤ lt ∧ d.rst ≤ rt}
Drv ← {d : d.sr (cid:8)= m ∧ d.ut ≤ rt ∧ d.rst ≤ lt}
Dkv ← {Dk ∩ {Dlv ∪ Drv}}
D ← D ∪ {argmaxd.ut{d ∈ Dkv}}
Algorithm 3 Wren server pm
1: upon receive (cid:3)SliceReq χ, lt, rt(cid:4) from p
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: upon receive (cid:3)PrepareReq idT , lt, rt, ht, Di(cid:4) from p
m
14:
n + 1)
15:
16:
17:
18:
19:
20: upon receive (cid:3)CommitReq idT , ct(cid:4) from c do
m
21:
n )
HLC
(cid:3)idT , rst, D(cid:4) ← {(cid:3)i, r, φ(cid:4) ∈ P repared
22:
23:
P repared
24:
Committed
HLC
pt ← HLC
lst
rst
P repared
send (cid:3)PrepareResp idT , pt(cid:4) to p
n ← max(Clock
n , lt}
n , rt}
n ← P repared
n ← P repared
n ← Committed
n ← max{lst
n ← max{rst
n ← max(HLC
m
n , ht + 1, HLC
m
n , ct, Clock
m
n
m
i
m
i
m
m
m
m
m
m
m
m
m
m
i do
(cid:3) Update HLC
(cid:3) Proposed commit time
(cid:3) Update local stable time
(cid:3) Update remote stable time
n ∪ {idT , rt, Di} (cid:3) Append to pending list
m
m
(cid:3) Update HLC
n \ {(cid:3)idT , rst, D(cid:4)} (cid:3) Remove from pending
n ∪ {(cid:3)idT , ct, rst, D}(cid:3) Mark to commit
n : i == idT }
m
m
lst – and hence of ct – the client knows that the freshest visible
version of x is Xl, which can be read locally from the private
cache 3.
After deﬁning the snapshot visible to T , pm
n also generates a
unique identiﬁer for T , denoted idT , and inserts T in a private
data structure. pm
n replies to c with idT and the snapshot
timestamps.
Upon receiving the reply, c updates lstc and rstc, and evicts
from the cache any version with timestamp lower than lstc. c
can prune the cache using lstc because pm
n has enforced that
the highest remote timestamp visible to T is lower than lstm
n .
This ensures that if, after pruning, there is a version X in the
private cache of c, then X.ct > lst and hence the freshest
version of x visible to c is X.
Read. The client c provides the set of keys to read. For each
key k to read, c searches the write-set, the read-set and the
client cache, in this order. If an item corresponding to k is
found, it is added to the set of items to return, ensuring
read-your-own-writes and repeatable-reads semantics. Reads
for keys that cannot be served locally are sent in parallel to
the corresponding partitions, together with the snapshot from
which to serve them. Upon receiving a read request, a server
ﬁrst updates the server’s LST and RST, if they are smaller than
the client’s (Alg. 2 Lines 2–3). Then, the server returns to the
client, for each key, the version within the snapshot with the
highest timestamp (Alg. 3 Lines 6–10). c inserts the returned
items in the read-set.
Write. Client c locally buffers the writes in its write-set W Sc.
If a key being written is already present in W Sc, then it is
updated; otherwise, it is inserted.
Commit. The client sends a commit request to the coordinator
3The likelihood of rstm
n being higher than lstm
n is low given that i) geo-
replication delays are typically higher than the skew among the physical
clocks [31] and ii) rstm
n is the minimum value across all timestamps of
the latest updates received in the local DC.
n - Auxiliary functions.
create d : (cid:3)d.k, d.v, d.ut, d.rdt, d.idT , d.sr(cid:4) ← (cid:3)k, v, ut, rdt, idT , m(cid:4)
insert new item d in the version chain of key k
Algorithm 4 Wren server pm
1: function UPDATE(k, v, ut, rdt, idT )
2:
3:
4: end function
m
end for
m
V V
if (P repared
else ub ← max{Clock
if (Committed
5: upon Every ΔR do
m
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: upon receive (cid:3)Replicate T , ct(cid:4) from p
23:
24:
25:
26:
27: upon receive (cid:3)Heartbeat t(cid:4) from p
28:
for ((cid:3)id, rst, D(cid:4) ∈ T ) do
end for
m
V V
n [m] ← ub
n [m] ← ub
n [i] ← t
end if
else
V V
m
m
i
n do
m
n } − 1
n (cid:8)= ∅) then ub ← min{p.pt}{p ∈ P repared
n (cid:8)= ∅) then
n } end if
C ← {(cid:3)id, ct, rst, D(cid:4)} ∈ Committed
for (T ← {(cid:3)id, rst, D(cid:4)} ∈ (group C by ct)) do
n : ct ≤ ub
m
n , HLC
m
m
for ((cid:3)k, v(cid:4) ∈ D) do update (k, v, ct, rst, id) end for
for ((cid:3)id, rst, D(cid:4) ∈ T ) do
end for
for (i (cid:8)= m) send (cid:3)Replicate T , ct(cid:4) to p
n \ T
Committed
n ← Committed
m
m
i
n end for
(cid:3) Commit tx in increasing order of ct
V V
for (i (cid:8)= m) do send (cid:3)Heartbeat V V m
n [m](cid:4) to p
(cid:3) Set version clock
(cid:3) Set version clock
i
n end for
i
n do
for ((cid:3)k, v(cid:4) ∈ D) do update (k, v, ct, rst, id) end for
n [i] ← ct
(cid:3) Update remote snapshot of i-th replica
(cid:3) Update remote snapshot of i-th replica
29: upon Every ΔG do
30:
31:
n ← min{i=0,...,M−1,i(cid:3)=m;j=0,...,N−1}V V
n ← min{i=0,...,N−1}V V
(cid:3) Compute remote and local stable snapshots
(cid:3) Remote
(cid:3) Local
rst
lst
m
j [i]
[m]
m
i
m
m
with the content of W Sc,
the id of the transaction and
the commit of its last update transaction hwtc, if any. The
coordinator contacts the partitions that store the keys that need
to be updated (the cohorts) and sends them the corresponding
updates and hwtc. The partitions update their HLCs, propose
a commit timestamp and append the transaction to the pending
list. To reﬂect causality, the proposed timestamp is higher than
the snapshot timestamps and hwtc. The coordinator then picks
the maximum among the proposed timestamps [32], sends it to
the cohort partitions, clears the local context of the transaction
and sends the commit timestamp to the client. The cohort
partitions move the transaction from the pending list to the
commit list, with the new commit timestamp.
Applying and replicating transactions. Periodically,
the
servers apply the effects of committed transactions, in in-
creasing commit timestamp order (Alg. 4 Lines 6-20). pm
n
applies the modiﬁcations of transactions that have a commit
timestamp lower than the lowest timestamp present in the
pending list. This timestamp represents the lower bound on
the commit timestamps of future transactions on pm
n . After
applying the transactions, pm
n updates its local version clock
and replicates the transactions to remote DCs. When there are
more transactions with the same commit time ct, pm
n updates
its local version clock only after applying the last transaction
with the same ct and packs them together to be propagated in
one replication message (Alg. 4 Lines 10–17).
If a server does not commit a transaction for a given amount
of time, it sends a heartbeat with its current HLC to its peer
7
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
Cure 
H-Cure 
Wren 
Cure 
H-Cure 
 25
 20
 15
)
c
e
s
m
(
e
m
 10
i
t
.
 10
)
c
e
s
m
(
e
m
i
t
i
g
n
k
c
o
B
l
 35
 40
 45
 8