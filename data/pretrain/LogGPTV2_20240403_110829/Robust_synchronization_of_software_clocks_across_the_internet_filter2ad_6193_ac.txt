sistent with the results of [5], where a similar plot showed this lin-
ear zone to extend at least down to τ = 1 second. This is exactly
what we would expect if the SKM were true in the presence of sys-
tem noise, and corresponds to equation (3) if ω(t) was considered
to be white noise. The plots agree as the hardware, operating sys-
tem, and timestamping solution are the same in each case, and the
dominant noises arise from them.
The plots diverge, and all rise, at larger scales as new sources
of variation enter in, such as temperature variations arising over the
day and the diurnal cycle itself. The curves begin to ﬂatten as major
new sources of variation cease at the weekly timescale, but remain
below the horizontal line marking 0.1 PPM.
From ﬁgure 3 the picture emerges that the SKM model holds true
for timescales up to around 1000 seconds. For any value within
this range, we can interpret the corresponding value on the vertical
axis as the level of precision to which the rate can be measured.
The greatest precision is obtained at the minimum point, and is of
the order of 0.01PPM. It is not meaningful to speak of rate errors
smaller than this, as the validity of the SKM model itself cannot be
veriﬁed to this level of precision.
In the machine-room the environmental control bounds temper-
ature variations within a 2oC band. We therefore expect that the
laboratory data would be more variable, and therefore that the cor-
responding curve lie above each of those from the machine-room.
This is indeed the case at large scales, as the temperature variations,
and therefore the main source of rate ﬂuctuations, are bounded.
At intermediate scales however it was true in only 1 of 3 cases.
We found this to be due to the presence of a low amplitude (≈
0.05PPM) but distinct oscillatory noise component of variable pe-
riod between 100 to 200 minutes (clearly visible in ﬁgure 8) which
creates additional variability over a broad range of scales. The pos-
sibility of this being due to the airconditioning cycle was investi-
gated through checking with a digital temperature logger making
measurements every minute, with mixed results. The true cause
(and possible elimination) of this effect is under investigation. It
is not expected to be truly server dependent and may be linked to
hardware and software controlling cooling fans in the host. The
performance of the synchronisation algorithms reported below is
despite the presence of this (possibly very unusual) noise compo-
nent.
In conclusion, in three different temperature environments: un-
controlled laboratory, temperature controlled, and from [5], building-
wide airconditioning, the SKM model holds over timescales up to
1000 seconds. Henceforth we use SKM scale, or τ ∗
, to refer to
this value. Over larger timescales the model fails but the rate error
remains bounded by 0.1 PPM. Indeed, to within this level of ac-
curacy we can say that the SKM model holds over all time scales.
This fact, and the values of the rate error bound and the SKM scale,
are the fundamental hardware characterizations on which the syn-
chronization is based. To the best of our knowledge this is the ﬁrst
time that synchronization algorithms have been built on a well de-
ﬁned hardware abstraction in this way.
The above measurements are consistent with the results of [9]
stating that the clockstability of commercial PCs is typically of the
order of 0.1 PPM. If a class of oscillators were used which were sig-
niﬁcantly different (for example less stable) then they would need
to be characterised by calculating curves such as those in ﬁgure 3,
to determine the two key metrics. As these appear as parameters in
the synchronization algorithms, our clock solution would continue
to work, with altered performance.
To characterise rate beyond τ ∗
, one cannot hope to measure an
expected or stationary value, as it does not exist. If one measured a
long term average rate over much larger timescales such as several
weeks, its variability would itself be well under 0.1 PPM, however
this apparent stability does not reﬂect a meaningful convergence,
and does not enable more accurate synchronisation in any sense.
We do not attempt to measure a (meaningless) ‘long term’ rate as
such in this paper, however as described below, we do make used
of estimates made over large time intervals, corresponding to an
average of meaningful local rates, as a means of reducing errors
due to timestamping and network congestion. We denote such es-
timates in section 5.2 by ¯p, where we discuss local rates in more
detail. Such average rates may be used as surrogates for local rates,
with an error which is bounded by 0.1 PPM.
3.2 Network and Server Delay
i
i
+ d←
i + d↑
Round Trip Time: ri ≡ tf,i − ta,i = d→
i ≡ tb,i − ta,i
≡ te,i − tb,i
i ≡ tf,i − te,i
Following ﬁgure 1, we decompose the history of packet i as:
Forward network delay: d→
Server Delay: d↑
Backward network delay: d←
(8)
(9)
(10)
i .(11)
Figure 4 gives representative examples of 1000 successive val-
ues of the backward network delay and server delay for the host
in the machine-room, using ServerLoc, calculated as d←
i (te,i) =
(te,i) = Te,i − Tb,i respectively. These time se-
Tg,i − Te,i and d↑
ries appear roughly stationary, with a marginal distribution which
seems consistent with a deterministic minimum value plus a pos-
itive random component. The main difference between them is
that the server delay has much lower minimum and average val-
ues: microseconds rather than milliseconds. These observations
make physical sense. The minimum in network delay could corre-
spond to propagation delay, and the random component to queue-
ing in network switching elements, which is not unexpectedly very
small for such a short route, but which can takes 10’s of millisec-
onds during periods of congestion. For the server, there will be a
minimum processing time and a variable time due to timestamping
issues both in the µs range, and rare delays due to scheduling in the
millisecond range. We formalise these observations in
i
Forward network delay : d→
Server Delay : d↑
Backward network delay : d←
i = d→ + q→
= d↑ + q↑
i = d← + q←
i
i
i
i
(12)
(13)
(14)
i ) (15)
, q↑
, d↑
Round Trip Time : ri = r + (q→
, and d←
+ q←
are the respective minima and q→
where d→
q←
therefore r ≡ d→ + d↑ + d←
basic conceptional framework and notation for what follows.
i and
are the positive variable components. The minimum RTT is
. These simple models provide the
i + q↑
i
i
i
4. SYNCHRONIZATION: THE SKM WORLD
In this section we examine simple synchronization ideas based
on the SKM. We detail the weaknesses of these ‘naive’ approachs,
which are addressed in subsequent sections. We use the ﬁrst day
of the same 7 day machine-room data set (July 4–10) used in the
previous section.
]
s
m
[
y
a
e
d
l
3
2
1
0
0
1.5
1
0.5
]
s
m
[
y
a
e
d
l
2000
4000
6000
8000
T
 [sec]
e
10000
12000
14000
16000
0
0
2000
4000
6000
8000
T
 [sec]
e
10000
12000
14000
16000
Figure 4: Examples of backward Network delay d←
i
4.1 Rate Synchronization
We wish to exploit the relation ∆(t) = ∆(TSC) ∗ p. More
precisely, assuming the SKM the following relation holds for the
forward path:
tb,i − tb,j − (q→
i − q→
j )
TSC(ta,i) − TSC(ta,j)
where i > j. This inspires the naive estimate
p =
i,j ≡ Tb,i − Tb,j
ˆp→
Ta,i − Ta,j
(16)
(17)
i,j+ ˆp←
In ﬁgure 5 backward estimates normalised as (ˆp←
which suffers from the neglect of the queueing terms and the pres-
ence of timestamping errors. An analogous expression provides an
independent estimate ˆp←
i,j from the backward path. In practice we
average these two to form our ﬁnal estimate: ˆpi,j = (ˆp→
i,j)/2.
i,j− ¯p)/¯p (where
¯p denotes the ‘detrending’ ˆp estimates from section 3.1) are given
for all packets collected. The i-th estimate compares the i-th packet
against the ﬁrst (j = 1), and is plotted against the timestamp Te,i
of its departure from the server. Thus ∆(TSC) = Ta,i − Ta,j
steadily increases as more packets are collected. Superimposed
are the corresponding reference rate values, calculated as ˆpg =
(Tf,i − Tf,j)/(Tg,i − Tg,j) which show some timestamping noise
(Tf,i is not corrected here), but are not subject to network delay.
We immediately see that the bulk of the estimates very quickly fall
0.1
0.05
0
]
M
P
P
[
backward p estimate
reference
−0.05
0
0.1
0.2
0.3
0.4
0.5
Te [day]
0.6
0.7
0.8
0.9
1
Figure 5: Naive per-packet rate estimates compared with ref-
erence measurements with steadily increasing ∆(TSC).
within 0.1 PPM of the reference curve, as the size of errors due
to both network delay and timestamping noise are damped at rate
1/∆(t). The estimates from packets which experienced high net-
work delay can nonetheless be very poor. Table 1 tells us that even
when measured over a timescale of a day, the bound of 0.1 PPM
will be broken when a combined network queueing delay exceeds
only 8.6 ms.
If the SKM held exactly, these errors would eventually be damped
as much as desired, however, we know that is not the case. We wish
∆(t) to grow large, so that the estimates will become increasingly
immune to both network delay and timestamping errors. However,
we cannot let it grow without bound, as the drift in the rate would
then be masked. The estimate would appear to become increasingly
i delay (right) time series.
(left) and Server d↑
stable, but would not be converging to any meaningful value, and
both long term and medium term changes could be hidden. For ex-
ample, there is always the possibility that the local environment will
change, and ultimately, the CPU oscillator is also subject to aging.
Thus some kind of windowing must be employed which enables
the past to be forgotten, which limits the degree of error damp-
ing available from a large ∆(t). The naive estimates are therefore
unreliable, as their errors, although likely to be small, can not be
controlled or bounded.
4.2 Offset Synchronization
We wish to exploit the fact that the SKM holds over small timescales
to simplify the measurement of θ(t). Since we can assume that
γ  0 are impossible to
distinguish from true offset errors. However, clearly the ‘causal-
ity’ bound ∆ ∈ (−(r − d↑), (r − d↑)) ⊂ (−r, r) holds, i.e. we
require the packet events at the server to occur inbetween those at
the host. Note that r and d↑
can be measured as they each are time
differences measured by a single clock.
(C(ta,i) + C(tf,i))− 1
2
1
2
(tb,i + te,i) +
∆ +
1
2
In the absence of independent knowledge of ∆, a naive estimate
based on equation (18) is
ˆθi =
1
2
(C(ta,i) + C(tf,i)) − 1
2
(Tb,i + Te,i),
(19)
which implicitly assumes that ∆ = 0, and is equivalent to align-
ing the midpoints (tb,i + te,i)/2 and (C(ta,i) + C(tf,i))/2 of the
server and host event times respectively. In ﬁgure 6 estimates obey-
ing equation (19) are shown, along with reference values calculated
as in section 3.1. Errors due to network delay are readily apparent,
but are more signiﬁcant than in the naive rate estimate case because
they are not damped by a large ∆(t) baseline. A histogram of the
deviations of the estimates from their reference values is essentially
0
−2
−4
−6
]
s
m
[
s
e
t
a
m
i
t
s
e