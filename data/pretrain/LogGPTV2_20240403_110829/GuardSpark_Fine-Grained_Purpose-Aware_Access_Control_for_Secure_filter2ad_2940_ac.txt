Example 5. If we examine the data processing logics introduced
in Example 3, the sequences of data operation purposes (DOP) are:
DOP-S1 (PatientName): “retrieve” (selectExpr) → “carry” (filter)
→ “carry/assist” (groupBy) → “carry” (sum) → “output” (project)
DOP-S2 (Expense): “retrieve” (selectExpr) → “retrieve/assist” (fil-
ter) → “carry” (groupBy) → “compute” (sum) → “output” (project)
Note that we show the data operations, such as selectExpr and filter,
together with each purpose. They are not part of the purpose. In
DOP-S2, Expense takes two roles at the “filter” operator: 1) some
Expense elements are retrieved from the set and passed to the next
operation; and 2) the data object is also used as an operand in the
Boolean expression in an assistance role. PatientName also takes
two roles in the “groupBy” operator: assist and carry.
Data operation purposes are defined by the data management
and sharing platforms and provided to the data owners, who are
expected to use them to specify access control policies. Meanwhile,
the platforms are also expected to automatically recognize the pre-
defined purposes from query plans, so that the access control poli-
cies could be enforced accordingly. In this project, we demonstrate
the capacity of PAAC using five sample purposes:
Retrieve (DOP-R). When a data object is retrieved from the data
source or processed through a selection (filter) function, the cor-
responding data operation purpose is DOP-R. It is the de facto
pre-requisite purpose of many other purposes, since data must be
retrieved before it can be used.
Compute (DOP-C). When the data object is an operand of a com-
puting operation and it is transformed in this operation, the data
operation purpose is DOP-C. In Example 5, data object Expense is
aggregated in the “sum” operation, hence, the purpose is DOP-C.
Assist (DOP-A). When a data object is involved in an operation
but its value is not changed, the data operation purpose is DOP-A,
i.e., the data object takes an assistance role in the operation. In
Example 5, PatientName assisted in the “groupBy” operation.
Carry (DOP-Ca). When a data object is carried during an operation
but not otherwise involved, it is denoted as DOP-Ca. In Example 5
2, PatientName is carried in operation filter(“exp1 >600”), hence, its
data operation purpose is DOP-Ca at this step. Note that the “carry”
purposes do not impact data operation or access decisions at all,
hence, they could be safely ignored in access control enforcement.
Output (DOP-O). When a data object is returned to the user or
application, the data operation purpose is DOP-O. In big data appli-
cations, whether a DOP-O purpose could be allowed often relies on
the previous operations on this data object. For example, “DOP-R-O”
may be denied, while “DOP-R-C-O” could be allowed.
2The join operations are not shown in the examples. In practice, each join operation
(i.e., natural join or theta join) contains a Boolean condition that indicates which data
objects are used to assist in pairing data while other data objects are carried. Thus,
one join operation indicates two kinds of operation purposes: DOP-A and DOP-Ca. In
addition, the union operation and Cartesian product operation are DOP-Ca.
ACSAC 2020, December 7–11, 2020, Austin, USA
Tao Xue, et al.
As we discussed earlier, a leaf-to-root path in the query tree
represents a data processing logic of the data object at the leaf. The
data operation purposes of this object are sequentially concatenated
along the path. Ideally, the data owners may specify all acceptable
(or denied) patterns of data operation purposes, such as “Expense:
DOP-R-*-C-*-O”, where * indicates wildcard. Meanwhile, the big
data platform is expected to enforce the policies by matching every
path of a query tree against the patterns. However, usability is a
concern due to the complexity of the patterns–the data owners may
not want to define every allowed or denied pattern, or they may be
incapable of doing so. In this project, we present a simplified model
that captures the key purpose in each data operation logic, which
shadows all other purposes in the same sequence. In particular, we
observed that: 1) All the data operation purposes are sequentially
applied to the data object, while only a subset of the operations
may modify the data, e.g., DOP-C. 2) The total modification to
the data object is no less than the most significant modification in
the sequence of operations. 3) From data protection perspective,
the data owner would specify how much modification needs to be
performed on a data object before it may be sent to output, or if
a data object cannot be sent to output. With these observations,
we add a new attribute priority to each pre-defined data operation
purpose, which denotes the level of modifications (i.e., impacts) this
operation would add to the data object. In the five data operation
purposes we defined in this project, DOP-C has the highest priority,
DOP-A and DOP-R has lower priority, while DOP-Ca has NULL
priority. Note that DOP-O is a special purpose that will be handled
differently than other purposes in the sequence.
Eventually, the data operation purposes in a sequence collectively
determine the data processing purpose for the data object. In our
simplified model, the data processing purpose is denoted as two-
tuple {DPP, DOP-O|NULL}, where the DPP is the highest-priority
DOP in the sequence, and “DOP-O|NULL” denotes whether the data
object is eventually sent to user after the sequence of operations.
Example 6. Following Example 5, the data processing purpose of
DOP-S1 is denoted as DPP1(PatientName) {DOP-A, DOP-O}, while
the data processing purpose of DOP-S2 is DPP2(Expense) {DOP-C,
DOP-O}. According to the access control intention introduced in
Example 1, DPP2 is allowed to Alice, while DPP1 is denied.
Finally, we define the PAAC access control policy.
Definition 3. An purpose-aware access control policy is a 4-tuple
⟨S, O, E, P⟩, where S demotes the subject, O denotes the object, E
specifies the environment conditions that the requests must satisfy,
and P denotes the data processing purposes that are allowed.
Note that the PAAC model may employ any existing access
control model, such as RBAC or ABAC, to identify the subject (user
or application). Last, we give a sample PAAC policy.
Example 7. A PAAC policy for Alice, as introduced in Example
1, may be specified as: ⟨Alice, O, E, {DOP-C, DOP-O}⟩, where O is
defined in Example 4, E denotes the additional conditions such
as Alice’s visiting IPs, and the purpose explicitly allows Alice to
compute with the data object (Expense column) and view the re-
sult of the computation. Similarly, we can also specify purpose
{DOP-A, NULL} on PatientName to specify that Alice could use
this column to assist data analytics (e.g., in groupBy), but she can-
not include it in the output.
Algorithm 1 Purpose Analysis Algorithm
Require: DOPIPE: data operations pipeline for an object O.
Ensure: PAG: purpose analysis graph, each node containing operation
1: PAG ← ∅
2: for each OPERA ∈ DOPIPE do
3:
Determine OPERA’s purpose(s) according to the operator or func-
purpose and purpose’ priority attributes.
tion.
if OPERA has one purpose on O then
Append one node to each branch in PAG.
Append multiple nodes to each branch in PAG.
else
end if
4:
5:
6:
7:
8:
9: end for
10: for each PATH ∈ PAG do
11:
12:
13: end for
DPP ← the highest-priority operation purpose of PATH.
Bind DPP with PATH.
4.2 Purpose Analysis Algorithm
Based on the structured data analytics engines/APIs in Spark, users
use column-level objects to express various data operations in all
data processing logics of an application. Now, we present an algo-
rithm to automatically identify the data operation purpose sequence
of each data processing logic.
We view the lifecycle of a data object in an application as a
pipeline of data operations. The operations pipeline can be ob-
tained directly — the application codes contain all data operations
information, and the logical plan in Spark Catalyst is also the con-
tainer of data operations. Each object in the pipeline sequentially
passes through the data operations. For example, the PatientName
and Expense objects have the same pipeline as shown in Example
3. However, each object is used differently in these operations and
has its own sequence of data operation purposes, such as DOP-S1
and DOP-S2 in Example 5.
The purpose extraction algorithm first extracts all data operation
purposes (DOPs) from the operations along the pipeline for a data
object, and then generates a purpose analysis graph for the object
and identifies the data processing purpose (DPP) for each path. The
algorithm is shown in Algorithm 1.
• The DOP of each operation is determined by the operator or
function (Line 3). For example, the “sum” function and its input
object “exp1” determine DOP-C purpose on the input object; the
“filter” function and its predicate (“exp1 > 6000”) determine the
DOP-A purpose of the object in the predicate.
• We follow the data operations pipeline to add nodes to the purpose
graph (Lines 4-8). Each node in the graph contains an operation
purpose and its priority. For an operation with one purpose, we
directly append a node to the graph. Meanwhile, a data operation
may have multiple purposes, e.g., the data object Expense takes two
DOPs at the “filter(exp1>6000)” operation, DOP-R and DOP-A. For
an operation with multiple purposes, we split the path so that each
new DOP is added to a branch. Note that when a branch already has
GuardSpark++: Fine-Grained Purpose-Aware Access Control for Secure Data Sharing and Analysis in Spark
ACSAC 2020, December 7–11, 2020, Austin, USA
Figure 3: Purpose analysis graph of data object Expense.
the highest-priority DOP, we stop appending nodes to this branch,
except for the final DOP-O (if any). In Figure 3, we demonstrate the
purpose graph of data object Expense in Example 5.
• Finally, we identify the highest-priority DOP in each path of the
graph and label it as the DPP of the the path (Lines 10-13). For
example, the DPP of path_1 in Figure 3 is DOP-C, which is the
purpose of the “sum” operation.
The generated purpose graph captures all DOPs for each data
object. Each path in the graph represents a data processing logic
and eventually indicates its data processing purpose. For example,
the path_1 is the main data processing logic of Expense, which
indicates its DPP as {DOP-C, DOP-O}.
5 PAAC ENFORCEMENT IN GUARDSPARK++
In this section, we present the details of the design and implemen-
tation of the PAAC enforcement mechanism on Spark.
5.1 GuardSpark++ Architecture
We choose to enforce the purpose aware access control model in
Catalyst due to the following reasons: 1) All the structured data
analytics engines/APIs are built on top of Catalyst, so that queries
and data analytics algorithms from these APIs must go through
Catalyst. Therefore, building GuardSpark++ in Catalyst ensures
that access control is enforced on all the requests from designated
sources. 2) The original (before optimization) logical query plans
are directly accessible at the Catalyst optimizer, so that we can effi-
ciently and effectively examine, modify or deny the query plans. 3)
The modified query plans are further optimized by Catalyst, so that
modifications by GuardSpark++ will not affect query performance.
To generate secure logical plan, we design four core modules
based on the PAAC model: data object recognition, purpose analy-
sis, compliance checking, and compliance enforcement. The first
two modules utilize analyzed logical plan to recognize objects and
each object’s data operation/processing purposes, and to determine
object and each object’s purpose. To make access decisions, the
third module evaluates subject, object, environment and purpose
against access control policies. The forth module enforces access
decisions on analyzed logical plan to produce secure logical plan.
Example 8. As shown in Figure 5, the analyzed logical plan of
the example code in Section 2.2 is used to exemplify secure plan
generation. An analyzed logical plan, a tree-like data structure,
utilizes ordered data operations on column-level objects to describe
each data processing logic in an application [26]. Each node in
analyzed logical plan is an operator (e.g., Project, Aggregate and
Filter) which encapsulates a set of expressions3. The leaf node
3Catalyst has its own expression system. Expression is used to evaluate a result value
according to given input values [13, 26]. For example, Computation expression (e.g.,
SUM expression, AVG expression) can evaluate a computation result of given input
values; Alias expression is used to evaluate an alias for input expression; Attribute
Figure 4: Secure logical plan generation stage
operator encapsulates raw data objects in data source. Each internal
operator encapsulates which objects the operator is inputted, how
objects are operated, and which objects (after being operated) are
delivered to its following operator(s) or are returned to data users.
5.2 Data Object Recognition
The column-level objects in analyzed logical plan are recognized.
This does not mean that GuardSpark++ cannot control the access to
row-level and cell-level objects. The module aims to recognize raw
and alias column-level objects in plan and describe those objects
using object attributes — each raw object is analyzed data oper-
ation/processing purposes by purpose analysis module (Section
5.3) (relevant alias objects participate); raw object attributes and its
purpose(s) are used by compliance checking module (Section 5.4).
All raw objects are recognized from leaf node operators in plan.
For example, the Expense column of the “Relation” operator in Fig-
ure 5 is a raw object, and can be described as {owner:R, source:MySQL
(196.168.12.110:3306:medical), table:patient, column:Expense}.