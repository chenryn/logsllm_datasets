title:Face/Off: Preventing Privacy Leakage From Photos in Social Networks
author:Panagiotis Ilia and
Iasonas Polakis and
Elias Athanasopoulos and
Federico Maggi and
Sotiris Ioannidis
Face/Off: Preventing Privacy Leakage From Photos in
Social Networks
∗
Panagiotis Ilia
FORTH, Greece
PI:EMAIL
Iasonas Polakis
Columbia University, USA
PI:EMAIL
Elias Athanasopoulos
FORTH, Greece
PI:EMAIL
Federico Maggi
Politecnico di Milano, Italy
PI:EMAIL
Sotiris Ioannidis
FORTH, Greece
PI:EMAIL
ABSTRACT
The capabilities of modern devices, coupled with the al-
most ubiquitous availability of Internet connectivity, have
resulted in photos being shared online at an unprecedented
scale. This is further ampliﬁed by the popularity of social
networks and the immediacy they oﬀer in content sharing.
Existing access control mechanisms are too coarse-grained to
handle cases of conﬂicting interests between the users asso-
ciated with a photo; stories of embarrassing or inappropriate
photos being widely accessible have become quite common.
In this paper, we propose to rethink access control when
applied to photos, in a way that allows us to eﬀectively pre-
vent unwanted individuals from recognizing users in a photo.
The core concept behind our approach is to change the gran-
ularity of access control from the level of the photo to that
of a user’s personally identiﬁable information (PII). In this
work, we focus on the face as the PII. When another user
attempts to access a photo, the system determines which
faces the user does not have the permission to view, and
presents the photo with the restricted faces blurred out.
Our system takes advantage of the existing face recogni-
tion functionality of social networks, and can interoperate
with the current photo-level access control mechanisms. We
implement a proof-of-concept application for Facebook, and
demonstrate that the performance overhead of our approach
is minimal. We also conduct a user study to evaluate the
privacy oﬀered by our approach, and ﬁnd that it eﬀectively
prevents users from identifying their contacts in 87.35% of
the restricted photos. Finally, our study reveals the miscon-
ceptions about the privacy oﬀered by existing mechanisms,
and demonstrates that users are positive towards the adop-
tion of an intuitive, straightforward access control mecha-
nism that allows them to manage the visibility of their face
in published photos.
∗Panagiotis Ilia is also with the University of Crete.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813603.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection
General Terms
Security, Privacy, Human Factors
Keywords
Access Control; Online Social Networks; Shared Photos;
Photo Tagging;
1.
INTRODUCTION
Online social networks (OSNs) have radically transformed
the online behavior and activities of users. Unfortunately,
such services have also introduced a number of privacy is-
sues, which have caught the attention of both the research
community and data protection agencies (e.g., [9, 31]). As
the use of these services spans across multiple facets of daily
life, users may face dire consequences when their personal
and professional life can aﬀect each other via OSNs. Many
articles have reported incidents of users being ﬁred because
of sensitive photos which they considered to be private, while
in actuality they were not (e.g., [7, 8]).
The implications of such privacy issues becomes alarm-
ing when considering the scale of adoption of these services.
Apart from surpassing 1.49 billion monthly active users,
with an average of 968 million daily users [3], Facebook
has also become the most time-consuming online user activ-
ity [10], as well as the de-facto platform for sharing photos
with over 350 million uploaded daily [6]. Accordingly, many
companies regularly check up job applicants online during
the hiring process. A recent study by Acquisti and Fong [16]
revealed that they may also use what they ﬁnd to discrimi-
nate against applicants. A Microsoft survey [15] found that
70% of recruiters in the US have rejected candidates due to
information, including photos, they found online.
In certain cases, some users may not be concerned about
privacy or may be unaware of the implications of their ac-
tions. Users may also not fully understand complex access
control mechanisms, and disclose private information with-
out hesitation, oblivious to the true visibility of the uploaded
content. In an attempt to make users more aware of who
can view their posts, Facebook recently altered the privacy
selector for status updates, to explicitly describe the po-
tential audience [5]. According to reports [13], Facebook is
also building a system that will automatically identify em-
barrassing photos being uploaded (e.g., where the user is
drunk), and warn the user if they are being shared publicly.
However, while such an approach may prevent certain users
from uploading photos (of themselves), it cannot prevent
other users that may have “malicious” intent or lack better
judgement. As users exhibit fundamentally diﬀerent behav-
iors regarding how they disclose information [30], they may
have diﬀerent perceptions regarding what content is sensi-
tive. Thus, in many cases the problem arises from content
that is shared among multiple users (i.e., a photo that de-
picts several individuals). As such, these measures can only
handle a subset of the actual problem.
In this work, we highlight that the essence of the problem
is that existing mechanisms for deﬁning access to photos in
OSNs, cannot eﬀectively handle cases where the interested
parties have conﬂicting settings. First, the photo uploader is
considered the owner of the photo and is granted full rights,
whereas the people appearing in the photo are not consid-
ered co-owners and are not granted any rights. On top of
this basic coarse-grained approach, OSN providers imple-
ment additional policies, some of which can signiﬁcantly
complicate issues. For example, the uploader can restrict
the photo’s visibility for the tagged users, and the maximum
allowed visibility for them extends to their immediate con-
tacts (i.e., a tagged user cannot set the visibility to include
any users apart from his immediate social circle). Second,
the photo uploader is not required to request the permission
of the people present in a photo before publishing it, and
may even ignore their requests to remove it. Furthermore,
any users that are tagged aﬀect the visibility of the photo,
as the photo will be viewable by all their contacts (default
privacy setting). Thus, even when the users tagged in the
photo have restricted its visibility, if the uploader has not
restricted access the photo will be publicly available, some-
thing which the remaining users will not even be aware of.
In general, these situations can be characterized as cases of
conﬂicts of interest, where the will of the content publisher
goes against the will of the depicted users, or the privacy
settings of one user override those of another. Note that
even though the access control mechanisms may vary across
OSNs, conﬂicts of interest are a general issue, as they arise
from the content of the photos.
Previous work has proposed frameworks for integrating
access control policies of collaborating parties [35], and mech-
anisms that allow the users to contribute to the speciﬁcation
of a collective policy [26,39]. However, such approaches only
solve the problem partially, as they handle visibility at a
photo-level granularity.
In other words, current solutions
are too coarse-grained for accommodating the privacy set-
tings of all the associated users. In such cases, a user has to
accept and follow the access control decision of the majority,
even if his privacy concerns are not satisﬁed.
In this paper, we propose an approach that can eﬀectively
handle these conﬂicts by changing the granularity of the ac-
cess control mechanism to that of the users’ faces. This
enables an OSN to express and enforce every user’s privacy
setting within an image; none of the users’ settings are over-
ridden no matter how restrictive or permissive they may be.
In a nutshell, our approach employs face recognition to au-
tomatically identify the users depicted within a photo; sub-
sequently, the visibility of each user’s face is automatically
restricted based on the privacy settings of the speciﬁc user
and not the content publisher. The result of this phase is a
“processed” photo that can be rendered selectively accord-
ing to who is viewing it. Thus, when a photo is accessed,
the system will automatically blur the faces of the users that
have restricted access. We propose a simple technique to en-
code the pre-processed photos, so as to avoid the overhead
of blurring them during the rendering phase.
We conduct a case study on over 4 million photos collected
from 128 participants and their social circles, and explore
the characteristics of their social graphs and their tagging
behavior. We then quantify the privacy risks that users are
exposed to, due to existing access control mechanisms.
To evaluate the feasibility of our approach being deployed
at a large scale, we measure the overhead incurred by our
proof-of-concept implementation. As popular OSNs already
process photos with face recognition software, the overhead
of our approach lies in retrieving the permissions of every
user, enforcing access control, and processing the photo “on
the ﬂy”. On average, our system requires only 0.05 seconds
per photo, when running on a commodity machine.
To evaluate the eﬀectiveness of our approach on preserving
user privacy, we conduct an experiment with 34 participants.
Each participant is shown a set of photos of their contacts,
with the face of one user “hidden” in each photo, and is
requested to identify those users.
In 87.35% of the cases,
the participants fail to identify their contacts, demonstrating
that our approach can signiﬁcantly improve user privacy.
We also interviewed 52 participants, to understand how
users perceive existing access control mechanisms, and their
opinion on the potential adoption of our approach by OSNs.
Apart from the lack of understanding of existing access con-
trol settings due to their complexity, we ﬁnd that most users
are positive towards a simpler, yet, more privacy-preserving
approach. After being informed about the conﬂicts of inter-
est that arise in shared photos, 77% of them express positive
opinions regarding the adoption of our approach, and 19.2%
remain neutral. Only 3.8% are negative, and state a lack of
concern for the privacy implications of content sharing.
Overall, the main contributions of this work are:
• We design an innovative ﬁne-grained access control mech-
anism for photo-sharing services that enforces the visibil-
ity of each user’s face based on their respective access con-
trol lists. Our approach eﬀectively handles all the cases of
conﬂicts of interest between the privacy settings of users.
• We build a proof-of-concept application that demonstrates
the feasibility and applicability of our approach within the
infrastructure of a real-world OSN. Our experiments show
that performance overhead is small compared to existing
processing of photos by OSNs, rendering the adoption of
our approach suitable even at such a scale.
• Our ﬁrst user study provides insights into the tagging be-
havior of users, and reveals the risk users face due to con-
ﬂicting privacy settings on shared photos. Based on the
collected data, we assess user tagging behavior, and quan-
tify the risk presented in certain photo-sharing scenarios.
• A second user study demonstrates the eﬀectiveness of our
approach in hiding users’ identities from their contacts.
We also highlight the counter-intuitive approach of exist-
ing access control mechanisms, and the eagerness of users
to adopt a mechanism that allows them to manage the
visibility of their faces.
2. PHOTO-BASED PRIVACY LEAKAGE
2.2 Privacy Leakage Scenarios
Earlier work has reported that users are concerned about
their privacy and tend to avoid publishing photos or any
other private information publicly [43]. Furthermore, ac-
cording to a survey by Besmer et al. [19], explicit requests by
users for deletion of photos, or users un-tagging themselves,
are complicated issues. These can lead to social tension and
are a source of anxiety for users, who may abstain from such
actions to ensure the stability of their social relations [42].
Also, the uploader may lack the incentives to actually fulﬁll
the user’s request and remove a photo. Thus, it is appar-
ent that users are restricted by coarse-grained access control
models regarding shared photos, and in many cases sacriﬁce
their privacy in favor of not agitating social relationships.
Moreover, the wide visibility of photos can also expose users
to inference attacks [37], or be leveraged by attackers for by-
passing account protection mechanisms [33, 34]. In the fol-