Figure 5: The percentage of power reductions (note that y-axis starts at 40).
Table 5: Number of general rules and pre-classiﬁer entries for
real classiﬁers with block sizes 64, 128 and 256.
Table 6: Number of general rules and pre-classiﬁer entries for
synthetic classiﬁers with block sizes 64, 128 and 256.
Name
R1
R2
R3
R4
R5
R6
R7
R8
R9
R10
# general rules
# pre entries
64
85
146
824
62
11
128
101
146
520
62
11
1513
1462
11
0
2055
2799
11
0
1004
2479
256
159
146
302
62
11
516
11
0
1792
2380
64
101
103
103
103
123
108
179
234
224
304
128
50
52
55
52
62
53
88
120
131
169
256
25
28
29
27
31
35
48
58
62
86
classiﬁers (S6 through S10) are bigger than those on the smaller
ones.
By comparing Figure 5(a) and Figure 5(b), we ﬁnd that the ﬁve
10K synthetic classiﬁers (S1 through S5) present similar trend of
power reductions as the real classiﬁers (R1 through R10). How-
ever, the ﬁve 100k classiﬁers (S6 through S10) show bigger power
reductions, as they appear in the upper part in Figure 5(b). This
suggests that SmartPC achieves more power reductions for larger
classiﬁers. The intuition behind this is that the number of inter-
cepting rules with a given packet and the number of wildcard rules
in the two-dimensional space do not increase proportionally with
classiﬁer size (as discussed in Section 3). Consequently, the num-
ber of pre-classiﬁer entries and general blocks do not scale as well.
According to the deﬁnition of the percentage of power reduction
( X−Y
X ×100%), in the case of larger classiﬁers, Y does not increase
proportionally with X. Therefore, SmartPC usually achieves more
power reductions on larger classiﬁers.
To further understand the behaviors of our proposed algorithms
for building pre-classiﬁers, we summarize the number of general
rules, and the number of pre-classiﬁer entries of real classiﬁers and
synthetic classiﬁers, as shown in Table 5 and Table 6, for block
sizes 64, 128, and 256 respectively.
We make the following observations from these tables:
• With the increase of block size, there is a general trend of
non-increasing number of general rules. This is because that
with smaller block sizes, rules have bigger chances to be
marked as general, while with larger block sizes pre-classiﬁer
entries are more likely to be expanded successfully to cover
more rules.
• With the increase of block size, the pre-classiﬁer sizes de-
Name
S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
# general rules
# pre entries
64
203
2075
1971
2028
119
344
5810
11509
9401
787
128
130
2030
1931
1776
73
316
5490
11191
8699
558
256
91
1903
1877
1530
67
290
4883
11318
9198
379
64
252
223
178
179
167
2855
2302
3147
3091
3286
128
129
110
90
88
97
1561
1153
1604
1573
1748
256
64
54
42
45
48
824
593
861
819
866
crease, since the number of entries in a pre-classiﬁer is roughly
proportional to N/B.
• The number of general rules and pre-classiﬁer entries are
much smaller than the size of a classiﬁer. Therefore, SmartPC
can achieve huge savings in power consumptions since only
the pre-classiﬁer, general rules and a block of speciﬁc rules
will be activated.
5.3 Storage overhead
In SmartPC, we need extra storage to store pre-classiﬁer entries.
Pre-classiﬁer entries are two-dimensional on source and destination
addresses, so each pre-classiﬁer entry occupies 64 bits, while each
ﬁve-dimensional rule occupies 104 bits. We show the percentage
of pre-classiﬁer size compared to the size of a whole classiﬁer on
real and synthetic classiﬁers in Figure 6. For example, R1 contains
5233 rules. With block size 128, there are 50 pre-classiﬁer entries
for R1. Therefore, we calculated the percentage of storage over-
5233×104 × 100 = 0.59%. Though the actual extra storage
head as
depends on TCAM speciﬁcations (e.g., width of TCAM, and etc),
the numbers shown in Figure 6 provide an estimation of the stor-
age overhead of pre-classiﬁers. As we can see, the percentage of
extra storage decreases as block size increases, since the number
of pre-classiﬁer entries is proportional to N/B. We observe that
the overhead is pretty small, less than 4% for every classiﬁer. More
than 80% of power reductions on the classiﬁers justify these storage
overhead.
50×64
SmartPC may introduce holes in blocks, since some blocks may
not be full. As discussed in Section 4.2, classiﬁer rules associ-
ated with multiple pre-classiﬁer entries can be stored into the same
TCAM block as long as the block can ﬁt them. We implemented a
)
%
(
d
a
e
h
r
e
v
o
e
g
a
r
o
t
s
f
o
n
o
i
t
c
a
r
F
 3
 2.5
 2
 1.5
 1
 0.5
 0
 10
R1
R2
R3
R4
R5
R6
R7
R8
R9
R10
 100
 1000
 10000
Block size (log scale)
(a) Real classiﬁers
)
%
(
d
a
e
h
r
e
v
o
e
g
a
r
o
t
s
f
o
n
o
i
t
c
a
r
F
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
 10
S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
 100
 1000
 10000
Block size (log scale)
(b) Synthetic classiﬁers
Figure 6: The storage overhead of SmartPC.
variant of ﬁrst-ﬁt algorithm to minimize the number of blocks used
and found no signiﬁcant increase of blocks. Considering regular
TCAMs also leave holes in blocks to reduce overhead of rule inser-
tions, we only focus on the extra storage incurred by pre-classiﬁers.
5.4 Comparison of SmartPC with naive-divide
To prove that the huge power reductions come from the intelli-
gence of the proposed pre-classiﬁers, we compare SmartPC with a
naive approach, named naive-divide, which recursively divides the
multi-dimensional space into smaller non-overlapping regions. As
in the extended TCAMs paper [20], up to a block size number of
rules that lie entirely in each region are assigned to the region and
these rules are stored in the same TCAM block. Different from the
extended TCAMs paper, naive-divide executes one phase, rather
than multiple phases, so that there will be only one match in the in-
dex TCAM, instead of multiple matches. Naive-divide eliminates
the need for multi-match TCAMs. In naive-divide, those rules that
do not ﬁt in the blocks are treated as general rules as in SmartPC.
In Figure 7, we compare the power reductions of SmartPC and
naive-divide on real and synthetic classiﬁers. Here we show results
with block size 128, while other block sizes show similar perfor-
mance. SmartPC outperforms naive-divide on every classiﬁer we
evaluated. On average, SmartPC achieves 87% power reductions
on real classiﬁers, while naive-divide only achieves 67%. SmartPC
outperforms naive-divide by 14% to 34% on real classiﬁers, with an
average of 20%. With synthetic classiﬁers, SmartPC achieves 88%
power reductions on average, while naive-divide achieves 65%.
SmartPC outperforms naive-divide by 7% to 32% on synthetic clas-
siﬁers, with an average of 23%.
The reductions of SmartPC come from the intelligence of the
pre-classiﬁers. In SmartPC, the structures of the rules are taken into
account when building the pre-classiﬁer entries. While in naive-
divide, the multi-dimensional space can be divided arbitrarily, re-
sulting in a larger amount of general rules. Compared to naive-
divide, SmartPC results in smaller number of active TCAM blocks
by reducing the number of general rules. Therefore, SmartPC achieves
higher power reductions.
5.5 Discussion
5.5.1 The effect of block size on power reductions
To understand how block sizes affect power reductions, we plot
the number of blocks that need to be activated with default scheme
(without using power reduction optimizations) and SmartPC in Fig-
ure 8, where x-axis is block size, y-axis represents the number of
TCAM blocks that need to be activated, and both axes are in log
Naive-divide
SmartPC
R1
R2
R3
R4
R5
R6
R7
R8
R9 R10
Classifiers
(a) Real classiﬁers
Naive-divide
SmartPC
)
%
(
n
o
i
t
c
u