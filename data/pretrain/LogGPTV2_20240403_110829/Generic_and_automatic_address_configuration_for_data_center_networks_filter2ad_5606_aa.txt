title:Generic and automatic address configuration for data center networks
author:Kai Chen and
Chuanxiong Guo and
Haitao Wu and
Jing Yuan and
Zhenqian Feng and
Yan Chen and
Songwu Lu and
Wenfei Wu
Generic and Automatic Address Conﬁguration for Data
Center Networks∗
Kai Chen⋆†, Chuanxiong Guo†, Haitao Wu†, Jing Yuan‡⋆, Zhenqian Feng♯†,
Yan Chen⋆, Songwu Lu§, Wenfei Wu♮†
⋆Northwestern University, †Microsoft Research Asia, ‡Tsinghua University, ♯NUDT, §UCLA, ♮BUAA
⋆{kchen,ychen}@northwestern.edu, †{chguo,hwu,v-zhfe,v-wenfwu}@microsoft.com,
‡PI:EMAIL, §PI:EMAIL
ABSTRACT
Data center networks encode locality and topology information into
their server and switch addresses for performance and routing pur-
poses. For this reason, the traditional address conﬁguration protocols
such as DHCP require huge amount of manual input, leaving them
error-prone.
In this paper, we present DAC, a generic and automatic Data cen-
ter Address Conﬁguration system. With an automatically generated
blueprint which deﬁnes the connections of servers and switches la-
beled by logical IDs, e.g., IP addresses, DAC ﬁrst learns the physical
topology labeled by device IDs, e.g., MAC addresses. Then at the
core of DAC is its device-to-logical ID mapping and malfunction de-
tection. DAC makes an innovation in abstracting the device-to-logical
ID mapping to the graph isomorphism problem, and solves it with low
time-complexity by leveraging the attributes of data center network
topologies. Its malfunction detection scheme detects errors such as
device and link failures and miswirings, including the most difﬁcult
case where miswirings do not cause any node degree change.
We have evaluated DAC via simulation, implementation and exper-
iments. Our simulation results show that DAC can accurately ﬁnd all
the hardest-to-detect malfunctions and can autoconﬁgure a large data
center with 3.8 million devices in 46 seconds. In our implementa-
tion, we successfully autoconﬁgure a small 64-server BCube network
within 300 milliseconds and show that DAC is a viable solution for
data center autoconﬁguration.
Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Network communica-
tions; C.2.3 [Network Operations]: Network management
General Terms
Algorithms, Design, Performance, Management
Keywords
Data center networks, Address conﬁguration, Graph isomorphism
∗
This work was performed when Kai, Zhenqian and Wenfei were
interns at Microsoft Research Asia.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’10, August 30–September 3, 2010, New Delhi, India.
Copyright 2010 ACM 978-1-4503-0201-2/10/08 ...$10.00.
INTRODUCTION
1.
1.1 Motivation
Mega data centers [1] are being built around the world to pro-
vide various cloud computing services such as Web search, online
social networking, online ofﬁce and IT infrastructure out-sourcing
for both individual users and organizations. To take the advantage of
economies of scale, it is common for a data center to contain tens or
even hundreds of thousands of servers. The current choice for build-
ing data centers is using commodity servers and Ethernet switches
for hardware and the standard TCP/IP protocol suite for inter-server
communication. This choice provides the best performance to price
trade-off [2]. All the servers are connected via network switches to
form a large distributed system.
Before the servers and switches can provide any useful services,
however, they must be correctly conﬁgured. For existing data centers
using the TCP/IP protocol, the conﬁguration includes assigning IP
address to every server. For layer-2 Ethernet, we can use DHCP [3]
for dynamic IP address conﬁguration. But servers in a data center
need more than one IP address in certain address ranges. This is be-
cause for performance and fault tolerance reasons, servers need to
know the locality of other servers. For example, in a distributed ﬁle
system [4], a chunk of data is replicated several times, typically three,
to increase reliability. It is better to put the second replica on a server
in the same rack as the original, and the third replica on a server at
another rack. The current practice is to embed locality information
into IP addresses. The address locality can also be used to increase
performance. For example, instead of fetching a piece of data from
a distant server, we can retrieve the same piece of data from a closer
one. This kind of locality based optimization is widely used in data
center applications [4, 5].
The newly proposed data center network (DCN) structures [6–9]
go one step further by encoding their topology information into their
logical IDs. These logical IDs can take the form of IP address (e.g.,
in VL2 [9]), or MAC address (e.g., in Portland [8]), or even newly
invented IDs (e.g., in DCell [6] and BCube [7]). These structures then
leverage the topological information embedded in the logical IDs for
scalable and efﬁcient routing. For example, Portland switches choose
a routing path by exploiting the location information of destination
PMAC. BCube servers build a source routing path by modifying one
digit at one step based on source and destination BCube IDs.
For all the cases above, we need to conﬁgure the logical IDs, which
may be IP or MAC addresses or BCube or DCell IDs, for all the
servers and switches. Meanwhile, in the physical topology, all the
devices are identiﬁed by their unique device IDs, such as MAC ad-
dresses. A naïve way is to build a static device-to-logical ID mapping
table at the DHCP server. Building such a table is mainly a manual
effort which does not work for the following two reasons. First of
39all, the scale of data center is huge. It is not uncommon that a mega
data center can have hundreds of thousands of servers [1]. Secondly,
manual conﬁguration is error-prone. A recent survey from 100 data
center professionals [10] suggested that 57% of the data center out-
ages are caused by human errors. Two more surveys [11, 12] showed
50%-80% of network downtime is due to human conﬁguration er-
rors. In short, “the vast majority of failures in data centers are caused,
triggered or exacerbated by human errors." [13]
1.2 Challenges and Contributions
topologies and addressing schemes.
Automatic address conﬁguration is therefore highly desirable for
data center networks. We envision that a good autoconﬁguration sys-
tem will have the following features which also pose challenges for
building such a system.
• Generality. The system needs to be applicable to various network
• Efﬁciency and scalability. The system should assign a logical ID
to a device quickly and be scalable to a large number of devices.
• Malfunction and error handling. The system must be able to han-
dle various malfunctions such as broken NICs and wires, and hu-
man errors such as miswirings.
• Minimal human intervention. The system should require minimal
manual effort to reduce human errors.
To the best of our knowledge, there are very few existing solutions
and none of them can meet all the requirements above. In this pa-
per, we address these problems by proposing DAC – a generic and
automatic Data center Address Conﬁguration system for all the ex-
isting and future data center networks. To make our solution generic,
we assume that we only have a blueprint of the to-be-conﬁgured data
center network, which deﬁnes how the servers and switches are con-
nected, and labels each device with a logical ID. The blueprint can be
automatically generated because all the existing data center network
structures are quite regular and can be described either recursively or
iteratively (see [6–9] for examples).
Through a physical network topology learning procedure, DAC
ﬁrst automatically learns and stores the physical topology of the data
center network into an autoconﬁguration manager. Then, we make
the following two key contributions when designing DAC.
First of all, we solve the core problem of autoconﬁguration: how to
map the device IDs in the physical topology to the logical IDs in the
blueprint while preserving the topological relationship of these de-
vices? DAC makes an innovation in abstracting the device-to-logical
ID mapping to the graph isomorphism (GI) problem [14] in graph the-
ory. Existing GI solutions are too slow for some large scale data cen-
ter networks. Based on the attributes of data center network topolo-
gies, such as sparsity and symmetry (or asymmetry), we apply graph
theory knowledge to design an optimization algorithm which signif-
icantly speed up the mapping. Speciﬁcally, we use three heuristics:
candidate selection via SPLD, candidate pruning via orbit and selec-
tive splitting. The ﬁrst heuristic is our own. The last two we selected
from previous work [15] and [16] respectively, after ﬁnding that they
are quite effective for data center graphs.
Secondly, despite that the malfunction detection problem is NP-
complete and APX-hard1, we design a practical scheme that subtly
exploits the degree regularity in all data center structures to detect the
malfunctions causing device degree change. For the hardest one with
no degree change, we propose a scheme to compare the blueprint
graph and the physical topology graph from multiple anchor points
and correlate malfunctions via majority voting. Evaluation shows that
1A problem is APX-hard if there is no polynomial-time approxima-
tion scheme.
Figure 1: An example of blueprint, and physical topology con-
structed by following the interconnections in blueprint.
our solution is fast and is able to detect all the hardest-to-detect mal-
functions.
We have studied our DAC design via extensive experiments and
simulations. The experimental results show that the time of our device-
to-logical ID mapping scales in proportion to the total number of de-
vices in the networks. Furthermore, our simulation results show that
DAC can autoconﬁgure a large data center with 3.8 million devices
in 46 seconds. We have also developed and implemented DAC as an
application on a 64-server testbed, where the 64 servers and 16 mini-
switches form a two level BCube [7] network. Our autoconﬁguration
protocols automatically and accurately assign BCube logical IDs to
these 64 servers within 300 milliseconds.
The rest of the paper is organized as follows. Section 2 presents the
system overview. Section 3 introduces the device-to-logical ID map-
ping. Section 4 discusses how DAC deals with malfunctions. Sec-
tion 5 and Section 6 evaluate DAC via experiments, simulations and
implementations. Section 7 discusses the related work. Section 8
concludes the paper.
2. SYSTEM OVERVIEW
One important characteristic shared by all data centers is that a
given data center is owned and operated by a single organization.
DAC takes advantage of this property to employ a centralized auto-
conﬁguration manager, which we call DAC manager throughout this
paper. DAC manager deals with all the address conﬁguration intel-
ligences such as physical topology collection, device-to-logical ID
mapping, logical ID dissemination and malfunction detection. In our
design, DAC manager can simply be a server in the physical topology
or can run on a separate control network.
Our centralized design is also inspired by the success of several,
recent large-scale infrastructure deployments. For instance, the data
processing system MapReduce [5] and the modern storage GFS [4]
employ a central master at the scale of tens of thousands of devices.
More recently, Portland [8] leverages a fabric manager to realize a
scalable and efﬁcient layer-2 data center network fabric.
As stated in our ﬁrst design goal, DAC should be a generic solution
for various topologies and addressing schemes. To achieve this, DAC
cannot assume any speciﬁc form of structure or addressing scheme in
its design. Considering this, DAC only uses the following two graphs
as its input:
1. Blueprint. Data centers have well-deﬁned structures. Prior to
deploying a real data center, a blueprint (Figure 1a) should be de-
signed to guide the construction of the data center. To make our so-
lution generic, we only require the blueprint to provide the following
minimal information:
• Interconnections between devices. It should deﬁne the inter-
connections between devices. Note that though it is possible
for a blueprint to label port numbers and deﬁne how the ports
of neighboring devices are connected, DAC does not depend on
such information. DAC only requires the neighbor information
of the devices, contained in any connected graph.
serverswitch(a). Blueprint:Each node has a logical ID(b). Physical network topology:Each device has a device ID40• Logical ID for each device. It should specify a logical ID for
each device2. The encoding of these logical IDs conveys the
topological information of the network structure. These logical
IDs are vital for server communication and routing protocols.
Since data center networks are quite regular and can be described
iteratively or recursively, we can automatically generate the blueprint
using software.
2. Physical network topology. The physical topology (Figure 1b)
is constructed by following the interconnections deﬁned in the blueprint.
In this physical topology, we use the MAC address as a device ID
to uniquely identify a device. For a device with multiple MAC ad-
dresses, we use the lowest one.
In the rest of the paper, we use Gb = (Vb; Eb) to denote the
blueprint graph and Gp = (Vp; Ep) to denote the physical topology
graph. Vb=Vp are the set of nodes (i.e., devices) with logical=device
IDs respectively, and Eb=Ep are the set of edges (i.e., links). Note
that while the blueprint graph Gb is known for any data center, the
physical topology graph Gp is not known until the data center is built
and information collected.
The whole DAC system structure is illustrated in Figure 2. The two
core components of DAC are device-to-logical ID mapping and mal-
function detection and handling. We also have a module to collect the
physical topology, and a module to disseminate the logical IDs to in-
dividual devices after DAC manager ﬁnishes the device-to-logical ID
mapping. In what follows, we overview the design of these modules.
1. Physical topology collection. In order to perform logical ID
resolution, we need to know both blueprint Gb and physical topology
Gp. Since Gp is not known readily, DAC requires a communica-
tion channel over the physical network to collect the physical topol-
ogy information. To this end, we propose a Communication channel
Building Protocol (CBP). The channel built from CBP is a layered
spanning tree and the root is DAC manager with level 0, its children
are level 1, so on and so forth.
When the channel is built, the next step is to collect the physical
topology Gp. For this, we introduce a Physical topology Collection
Protocol (PCP). In PCP, the physical topology information, i.e., the
connection information between each node, is propagated bottom-up
from the leaf devices to the root (i.e., DAC manager) layer by layer.
After Gp is collected by DAC manager, we go to the device-to-logical
ID mapping module.
2. Device-to-logical ID mapping. After Gp has been collected,
we come to device-to-logical ID mapping, which is a key component
of DAC. As introduced in Section 1, the challenge is how to have the
mapping reﬂect the topological relationship of these devices. To this
end, we devise O2, a fast one-to-one mapping engine, to realize this