### 5.3 Evasion Analysis

The PHPIDS parser will return to its initial state upon receiving any non-alphanumeric input, while the Google Chrome parser has the option to first transition to state \( q_c^3 \) using any alphanumeric character. For instance, with the input “=!a”, the product automaton reaches the point of exposure (state \( q_p^3 \)). Using our root cause analysis, all different evasion techniques we detected are grouped under a single root cause. This is intuitively correct because a patch that adds the missing state in the PHPIDS parser would address all evasion attacks simultaneously.

### 5.4 Comparison with Black-Box Fuzzing

To the best of our knowledge, there is no publicly available black-box system capable of performing black-box differential testing like SFADiff. A straightforward approach would be to use a black-box fuzzer (e.g., the PEACH fuzzing platform [1]) and send each generated input to both programs. The outputs from both programs are then compared to detect any differences. Note that, similar to SFADiff, fuzzers also start with some initial inputs (seeds) which they subsequently mutate to generate more inputs for the target program. We argue that our approach is more effective in discovering differences for two reasons:

#### Adaptive Input Generation
Fuzzers incorporate various strategies to mutate previous inputs and generate new ones. For example, PEACH supports over 20 different mutation strategies. However, if a new input does not cause a difference, no further information is extracted from it, and the next inputs are unrelated to the previous ones. In contrast, each input submitted by SFADiff to the target program provides more information about the structure of the program, and its output determines the next input to be tested. For instance, in the execution shown in Table 4, SFADiff used the initialization model and detected the additional state in Chrome’s parser (see Figures 7 and 8). Notice that the additional state in Chrome’s parser was not part of the model used for initialization. This allowed SFADiff to quickly discover an evasion attack after a few refinements in the generated models. Each refinement discarded a number of candidate differences and guided the generation of new inputs based on the output of previous ones.

#### Root Cause Analysis
In the presence of a large number of differences, black-box fuzzers are unable to categorize the differences without some form of white-box access to the program (e.g., crash dumps). On the other hand, as demonstrated in the evasion analysis section, our root cause analysis algorithm provides a meaningful categorization of the differences based on the execution paths they follow in the generated models.

| **Attributes** | **Browser Model** | **WAF Model** |
|---------------|-------------------|---------------|
| Membership    | 6672              | 448           |
| Cached Membership | 0               | 40            |
| Equivalence   | 53                | 3             |
| Cached Equivalence | 4              | 106           |
| Learned States | 4241             | 36            |
| Cross-Check Times | 780             | 4             |

**Table 4: A sample execution that found an evasion attack for PHPIDS 0.7 and Google Chrome on macOS.**

### 6. Related Work

#### Fingerprinting
Nmap [17] is a popular tool for OS fingerprinting that includes mechanisms for fingerprinting different TCP implementations, among other things. However, unlike SFADiff, the signatures of different protocols in Nmap are manually created and tested. Similarly, in the WAF setting, Henrique et al. manually found several fingerprints for distinguishing popular WAFs.

Massicotte et al. [22] quantified the amount of signature overlap assuming direct white-box access to the signature database of the analyzed programs. They checked for duplication and intersection across different signatures. However, unlike our approach, their analysis did not involve any learning mechanism.

#### Automated Fingerprint Generation
Caballero et al. [10] designed and evaluated an automated fingerprinting system for DNS implementations using simple machine learning classifiers like decision trees. They used targeted fuzzing to find differences between individual protocols. However, Richardson et al. [25] showed that such techniques do not perform as well as hand-crafted signatures for OS fingerprinting in realistic settings. Unlike these passive learning-based techniques, we use active learning along with automata inference for systematically finding and categorizing the differences. Moreover, unlike SFADiff, none of these techniques can perform automated root cause analysis in a domain-independent way.

Shu et al. [27] explored the problem of automatically fingerprinting TCP implementations. Instead of finding new differences, they reused the handcrafted Nmap signature set [17] to create parameterized extended finite state machine (PEFSM) models of these signatures for efficient fingerprinting. In contrast, our technique learns the model of the TCP implementations without depending on any handcrafted signatures. SFADiff can automatically find such differences, including multiple previously-unknown differences between TCP implementations.

Brumley et al. [9] describe how to find deviations in programs using symbolic execution, which can be used for fingerprinting. However, such approaches suffer from fundamental scalability challenges inherent in symbolic execution and thus cannot be readily applied to large-scale software such as web browsers.

#### Differential Testing
Differential testing is a method of testing a program without any manually crafted specifications by comparing its outputs to those of other comparable programs for the same set of inputs [23]. Differential testing has been successfully used for testing a diverse set of systems, including C compilers [32], Java virtual machine implementations [11], SSL/TLS implementations [8], mobile applications for privacy leaks [20], PDF malware detectors [31], and space flight software [18]. However, unlike our approach, all these projects simply try to find individual differences in an ad hoc manner rather than inferring models of the tested programs and exploring the differences systematically.

#### Automata Inference
The L* algorithm for learning deterministic finite state automata from membership and equivalence queries was described by Angluin [4], and many variations and optimizations have been developed in the following years. Balcázar et al. [6] provide an overview of different algorithms under a unified notation. Initializing the L* algorithm was originally described by Groce et al. [19]. Symbolic finite automata were introduced by Veanes et al. [29] as an efficient way to explore regular expression constraints, while algorithms for SFA minimization were developed recently by D'Antoni and Veanes [14]. The ASKK algorithm for inferring SFAs was developed recently by Argyros et al. [5].

When access to the source code is provided, Babić and Botinčan [7] developed an algorithm for inferring SFT models of programs using symbolic execution. The L* algorithm and its variations have been extensively used for inferring models of protocols such as the TLS protocol [26], security protocols of EMV bank cards [2], and electronic passport protocols [3]. While some of these works note that differences in the models could be used for fingerprinting, no systematic approach to develop and enumerate such fingerprints was described.

Fiterau-Brostean et al. [15, 16] used automata learning to infer TCP state machines and then used a model checker to check compliance with a manually created TCP specification. While similar in nature, our approach differs in that our differential testing framework does not require a manual specification to check for discrepancies between two implementations.

### Acknowledgments
The first and fourth authors were supported by the Office of Naval Research (ONR) through contract N00014-12-1-0166. Any opinions, findings, conclusions, or recommendations expressed herein are those of the authors and do not necessarily reflect those of the US Government or ONR. The second and fifth authors were supported by H2020 Project Panoramix #653497 and ERC project CODAMODA, #259152.

### References
[1] Peach fuzzer. http://www.peachfuzzer.com/. (Accessed on 08/10/2016).

[2] F. Aarts, J. D. Ruiter, and E. Poll. Formal models in Software Testing, Verification, and Validation Workshops (ICSTW), IEEE International Conference on, 2013.

[3] F. Aarts, J. Schmaltz, and F. Vaandrager. Inference and abstraction of the biometric passport. In Leveraging Applications of Formal Methods, Verification, and Validation. 2010.

[4] D. Angluin. Learning regular sets from queries and counterexamples. Information and Computation, 75(2):87–106, 1987.

[5] G. Argyros, I. Stais, A. Keromytis, and A. Kiayias. Back in black: Towards formal, black-box analysis of sanitizers and filters. In Security and Privacy (S&P), 2016 IEEE Symposium on, 2016.

[6] J. Balcázar, J. Díaz, R. Gavalda, and O. Watanabe. Algorithms for learning finite automata from queries: A unified view. Springer, 1997.

[7] M. Botinčan and D. Babić. Sigma*: Symbolic Learning of Input-Output Specifications. In POPL, 2013.

[8] C. Brubaker, S. Jana, B. Ray, S. Khurshid, and V. Shmatikov. Using Frankencerts for automated adversarial testing of certificate validation in SSL/TLS implementations. In Security and Privacy (S&P), 2016 IEEE Symposium on, 2014.

[9] D. Brumley, J. Caballero, Z. Liang, J. Newsome, and D. Song. Towards automatic discovery of deviations in binary implementations with applications to error detection and fingerprint generation. In USENIX Security Symposium (USENIX Security), 2007.

[10] J. Caballero, S. Venkataraman, P. Poosankam, M. Kang, D. Song, and A. Blum. FiG: Automatic fingerprint generation. Department of Electrical and Computing Engineering, page 27, 2007.

[11] Y. Chen, T. Su, C. Sun, Z. Su, and J. Zhao. Coverage-directed differential testing of JVM implementations. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 85–99. ACM, 2016.

[12] T. Chow. Testing software design modeled by finite-state machines. IEEE Transactions on Software Engineering, (3):178–187, 1978.

[13] T. H. Cormen. Introduction to Algorithms. MIT Press, 2009.

[14] L. D'Antoni and M. Veanes. Minimization of symbolic automata. In ACM SIGPLAN Notices, volume 49, pages 541–553. ACM, 2014.

[15] P. Fiterău-Broștean, R. Janssen, and F. Vaandrager. Learning fragments of the TCP network protocol. In Formal Methods for Industrial Critical Systems. 2014.

[16] P. Fiterău-Broștean, R. Janssen, and F. Vaandrager. Combining model learning and model checking to analyze TCP implementations. In International Conference on Computer-Aided Verification (CAV). 2016.

[17] Fyodor. Remote OS detection via TCP/IP fingerprinting (2nd generation).

[18] A. Groce, G. Holzmann, and R. Joshi. Randomized differential testing as a prelude to formal verification. In International Conference on Software Engineering (ICSE), 2007.

[19] A. Groce, D. Peled, and M. Yannakakis. Adaptive model checking. In Tools and Algorithms for the Construction and Analysis of Systems, pages 357–370. 2002.

[20] J. Jung, A. Sheth, B. Greenstein, D. Wetherall, G. Maganis, and T. Kohno. Privacy Oracle: A system for finding application leaks with black box differential testing. In CCS, 2008.

[21] D. Kozen. Lower bounds for natural proof systems. In FOCS, 1977.

[22] F. Massicotte and Y. Labiche. An analysis of signature overlaps in Intrusion Detection Systems. In IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 2011.

[23] W. McKeeman. Differential testing for software. Digital Technical Journal, 10(1), 1998.

[24] H. Raffelt, B. Steffen, and T. Berg. LearnLib: A library for automata learning and experimentation. In Proceedings of the 10th International Workshop on Formal Methods for Industrial Critical Systems (FMICS), 2005.

[25] D. Richardson, S. Gribble, and T. Kohno. The limits of automatic OS fingerprint generation. In ACM Workshop on Artificial Intelligence and Security (AISec), 2010.

[26] J. D. Ruiter and E. Poll. Protocol state fuzzing of TLS implementations. In USENIX Security Symposium (USENIX Security), 2015.

[27] G. Shu and D. Lee. Network Protocol System Fingerprinting—A Formal Approach. In IEEE Conference on Computer Communications (INFOCOM), 2006.

[28] M. Sipser. Introduction to the Theory of Computation, volume 2. Thomson Course Technology Boston, 2006.

[29] M. Veanes, P. D. Halleux, and N. Tillmann. Rex: Symbolic Regular Expression Explorer. In International Conference on Software Testing, Verification, and Validation (ICST), 2010.

[30] M. Veanes, P. Hooimeijer, B. Livshits, D. Molnar, and N. Bjorner. Symbolic Finite State Transducers: Algorithms and Applications. ACM SIGPLAN Notices, 47, 2012.

[31] W. Xu, Y. Qi, and D. Evans. Automatically evading classifiers: A case study on PDF malware classifiers. In Proceedings of the 2016 Network and Distributed Systems Symposium (NDSS), 2016.

[32] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding and understanding bugs in C compilers. In PLDI, 2011.