will return to the initial state with any non alphanumeric
input, while the Google Chrome parser has the choice to
ﬁrst transition to qc
3 using
any alphanumeric character. For example, with an input
“=!a” the product automaton will reach the point of expo-
sure (qp
3). Furthermore, using our root cause analysis,
all diﬀerent evasions we detected are grouped under a single
root cause. This is intuitively correct, since a patch, which
adds the missing state in the PHPIDS parser will address
all evasion attacks at once.
5.4 Comparison with black-box fuzzing
2 and then to an accepting state qc
0 , qc
To the best of our knowledge there is no publicly avail-
able black-box system which is capable of performing black-
box diﬀerential testing like SFADiff. A straightforward ap-
proach would be to use a black-box fuzzer (e.g. the PEACH
fuzzing platform [1]) and send each input generated by the
fuzzer to both programs. Afterwards, the outputs from both
programs are compared to detect any diﬀerences. Note that,
like SFADiff, fuzzers also start with some initial inputs
(seeds) which they subsequently mutate in order to gener-
select if(a
Permits
Blocks
Blocks
case(
ModSecurity 3.0.0
Expose 2.8.4
Blocks
Permits
‘>
Blocks
PHPIDS 0.7
Figure 9: Fingerprint tree for diﬀerent web applica-
tion ﬁrewalls.
ate more inputs for the target program. We argue that our
approach is more eﬀective in discovering diﬀerences for two
reasons:
Adaptive input generation. Fuzzers incorporate a num-
ber of diﬀerent strategies in order to mutate previous inputs
and generate new ones. For example, PEACH supports more
than 20 diﬀerent strategies for mutating an input. However,
assuming that a new input does not cause a diﬀerence, no
further information is extracted from it; the next inputs are
unrelated to the previous ones. On the contrary, each in-
put submitted by SFADiff to the target program provides
more information about the structure of the program and its
output determines the next input that will be tested. For ex-
ample, in the execution shown in table 4, SFADiff utilized
the initialization model and detected the additional state in
Chrome’s parser (cf. ﬁgures 7, 8). Notice that, the addi-
tional state in Chrome’s parser was not part of the model
used for initialization. This allowed SFADiff to quickly
discover an evasion attack after a few reﬁnements in the
generated models. Each reﬁnement discarded a number of
candidate diﬀerences and drove the generation of new inputs
based on the output of previous ones.
Root cause analysis. In the presence of a large number
of diﬀerences, black-box fuzzers are unable to categorize the
diﬀerences without some form of white-box access to the
program (e.g. crash dumps). On the other hand, as demon-
strated in the evasion analysis paragraph of section 5.3, our
root cause analysis algorithm provides a meaningful catego-
rization of the diﬀerences based on the execution path they
Attributes
Browser Model
WAF Model
Membership
Cached Membership
Equivalence
Cached Equivalence
Learned States
Cross-Check Times
6672
448
0
40
53
4
4241
780
3
106
36
4
Provided Browser Model
()
()
Vulnerability Discovered
Execution Time
382.12 seconds
Table 4: A sample execution that found an evasion
attack for PHPIDS 0.7 and Google Chrome on MAC
OSX.
follow in the generated models.
6. RELATED WORK
Fingerprinting. Nmap [17] is a popular tool for OS ﬁn-
gerprinting that include mechanisms for ﬁngerprinting of
diﬀerent TCP implementations among other things. How-
ever, unlike SFADiff, the signatures of diﬀerent protocols
in nmap are manually crated and tested. Similarly, in the
WAF setting, Henrqiue et al. manually found several ﬁnger-
prints for distinguishing popular WAFs.
Massicotte et al. [22] quantiﬁed the amount of signature
overlap assuming direct white-box access to the signature
database of the analyzed programs. They checked for dupli-
cation and intersection across diﬀerent signatures. However,
unlike our approach here, their analysis did not involve any
learning mechanism.
Automated ﬁngerprint generation. Caballero et al.
[10] designed and evaluated an automated ﬁngerprinting sys-
tem for DNS implementations using simple machine learn-
ing classiﬁers like decision trees. They used targeted fuzzing
to ﬁnd diﬀerences between individual protocols. However,
Richardson et al. [25] showed that such techniques do not
tend to perform as good as the hand-crafted signatures for
OS ﬁngerprinting in realistic setting. Unlike these passive
learning-based techniques, we use active learning along with
automata inference for systematically ﬁnding and catego-
rizing the diﬀerences. Moreover, unlike SFADiff, none of
these techniques are capable of performing automated root
cause analysis in a domain-independent way.
Shu et al. [27] explored the problem of automatically ﬁn-
gerprinting TCP implementations. However, instead of ﬁnd-
ing new diﬀerences, they reused the handcrafted Nmap sig-
nature set [17] to create parameterized extended ﬁnite state
machine (PEFSM) models of these signatures for eﬃcient
ﬁngerprinting. By contrast, our technique learns the model
of the TCP implementations without depending on any hand-
crafted signatures. SFADiff is able to ﬁnd such diﬀerences
automatically, including multiple previously-unknown dif-
ferences between TCP implementations.
Brumley et al. [9] describes how to ﬁnd deviations in pro-
grams using symbolic execution that can be used for ﬁnger-
printing. However, such approaches suﬀer from the funda-
mental scalability challenges inherent in symbolic execution
and thus cannot be readily applied in large scale software
such as web browsers.
Diﬀerential testing. Diﬀerential testing is a way of test-
ing a program without any manually crafted speciﬁcations
by comparing its outputs to those of other comparable pro-
grams for the same set of inputs [23]. Diﬀerential testing has
been used successfully for testing a diverse set of systems
including C compilers [32], Java virtual machine implemen-
tations [11], SSL/TLS implementations [8], mobile applica-
tions for privacy leaks [20], PDF malware detectors [31],
and space ﬂight software [18]. However, unlike us, all these
projects simply try to ﬁnd individual diﬀerences in an ad hoc
manner rather than inferring models of the tested programs
and exploring the diﬀerences systematically.
Automata inference. The L∗ algorithm for learning de-
terministic ﬁnite state automata from membership and equiv-
alence queries was described by Angluin [4] and many vari-
ations and optimizations were developed in the following
years. Balcazzar et al. [6] provide an overview of diﬀerent
algorithms under a uniﬁed notation. Initializing the L∗ algo-
rithm was originally described by Groce et al. [19]. Symbolic
ﬁnite automata were introduced by Veanes et al. [29] as an
eﬃcient way to explore regular expression constraints, while
algorithms for SFA minimization were developed recently
by D’Antoni and Veanes [14]. The ASKK algorithm for in-
ferring SFAs was developed recently by Argyros et al. [5].
When access to the source code is provided Botinˇcan and
Babi´c [7] developed an algorithm for inferring SFT models
of programs using symbolic execution. The L∗ algorithm
and variations has being used extensively for inferring mod-
els of protocols such as the TLS protocol [26], security pro-
tocols of EMV bank cards [2] and electronic passport pro-
tocols [3]. While some of these works note that diﬀerences
in the models could be used for the purpose of ﬁngerprint-
ing, no systematic approach to develop and enumerate such
ﬁngerprints was described.
Fiterau-Brostean et al. [15, 16] used automata learning to
infer TCP state machines and then used a model checker
in order to check compliance with a manually created TCP
speciﬁcation. While similar in nature, our approach diﬀers
in the sense that our diﬀerential testing framework does not
require a manual speciﬁcation in order to check for discrep-
ancies between two implementations.
ACKNOWLEDGMENTS
The ﬁrst and fourth authors were supported by the Oﬃce of
Naval Research (ONR) through contract N00014-12-1-0166.
Any opinions, ﬁndings, conclusions, or recommendations ex-
pressed herein are those of the authors, and do not necessar-
ily reﬂect those of the US Government or ONR. Second and
ﬁfth authors were supported by H2020 Project Panoramix
# 653497 and ERC project CODAMODA, # 259152.
References
[1] Peach fuzzer.
http://www.peachfuzzer.com/.
(Ac-
cessed on 08/10/2016).
[2] F. Aarts, J. D. Ruiter, and E. Poll. Formal models
In Software Testing, Veriﬁca-
of bank cards for free.
tion and Validation Workshops (ICSTW), IEEE Inter-
national Conference on, 2013.
[3] F. Aarts, J. Schmaltz, and F. Vaandrager.
Inference
and abstraction of the biometric passport. In Leverag-
ing Applications of Formal Methods, Veriﬁcation, and
Validation. 2010.
[4] D. Angluin.
and counterexamples.
75(2):87–106, 1987.
Learning regular sets from queries
Information and computation,
[5] G. Argyros, I. Stais, A. Keromytis, and A. Kiayias.
Back in black: Towards formal, black-box analysis of
sanitizers and ﬁlters. In Security and privacy (S&P),
2016 IEEE symposium on, 2016.
[6] J. Balc´azar, J. D´ıaz, R. Gavalda, and O. Watanabe.
Algorithms for learning ﬁnite automata from queries:
A uniﬁed view. Springer, 1997.
[7] M. Botinˇcan and D. Babi´c. Sigma*: Symbolic Learning
of Input-Output Speciﬁcations. In POPL, 2013.
[8] C. Brubaker, S. Jana, B. Ray, S. Khurshid, and
V. Shmatikov. Using frankencerts for automated ad-
versarial testing of certiﬁcate validation in SSL/TLS
implementations. In Security and privacy (S&P), 2016
IEEE symposium on, 2014.
[9] D. Brumley, J. Caballero, Z. Liang, J. Newsome, and
D. Song. Towards automatic discovery of deviations in
binary implementations with applications to error de-
tection and ﬁngerprint generation. In USENIX Security
Symposium (USENIX Security), 2007.
[10] J. Caballero,
S. Venkataraman, P. Poosankam,
M. Kang, D. Song, and A. Blum. FiG: Automatic ﬁn-
gerprint generation. Department of Electrical and Com-
puting Engineering, page 27, 2007.
[11] Y. Chen, T. Su, C. Sun, Z. Su, and J. Zhao. Coverage-
directed diﬀerential testing of JVM implementations.
In Proceedings of the 37th ACM SIGPLAN Conference
on Programming Language Design and Implementation,
pages 85–99. ACM, 2016.
[12] T. Chow. Testing software design modeled by ﬁnite-
IEEE transactions on software engi-
state machines.
neering, (3):178–187, 1978.
[13] T. H. Cormen. Introduction to algorithms. MIT press,
2009.
[14] L. D’Antoni and M. Veanes. Minimization of sym-
bolic automata. In ACM SIGPLAN Notices, volume 49,
pages 541–553. ACM, 2014.
[15] P. Fiter˘au-Bro¸stean, R. Janssen, and F. Vaandrager.
Learning fragments of the TCP network protocol. In
Formal Methods for Industrial Critical Systems. 2014.
[16] P. Fiter˘au-Bro¸stean, R. Janssen, and F. Vaandrager.
Combining model learning and model checking to ana-
lyze TCP implementations. In International Conference
on Computer-Aided Veriﬁcation (CAV). 2016.
[17] Fyodor. Remote OS detection via TCP/IP ﬁngerprint-
ing (2nd generation).
[18] A. Groce, G. Holzmann, and R. Joshi. Randomized
diﬀerential testing as a prelude to formal veriﬁcation.
In International Conference on Software Engineering
(ICSE), 2007.
[19] A. Groce, D. Peled, and M. Yannakakis. Adaptive
model checking. In Tools and Algorithms for the Con-
struction and Analysis of Systems, pages 357–370. 2002.
[20] J. Jung, A. Sheth, B. Greenstein, D. Wetherall, G. Ma-
ganis, and T. Kohno. Privacy oracle: a system for ﬁnd-
ing application leaks with black box diﬀerential testing.
In CCS, 2008.
[21] D. Kozen. Lower bounds for natural proof systems. In
FOCS, 1977.
[22] F. Massicotte and Y. Labiche. An analysis of signature
overlaps in Intrusion Detection Systems. In IEEE/IFIP
International Conference on Dependable Systems and
Networks (DSN), 2011.
[23] W. McKeeman. Diﬀerential testing for software. Digital
Technical Journal, 10(1), 1998.
[24] H. Raﬀelt, B. Steﬀen, and T. Berg. Learnlib: A library
for automata learning and experimentation.
In Pro-
ceedings of the 10th international workshop on Formal
methods for industrial critical systems (FMICS), 2005.
[25] D. Richardson, S. Gribble, and T. Kohno. The limits of
automatic OS ﬁngerprint generation. In ACM workshop
on Artiﬁcial intelligence and security (AISec), 2010.
[26] J. D. Ruiter and E. Poll. Protocol state fuzzing of
TLS implementations. In USENIX Security Symposium
(USENIX Security), 2015.
[27] G. Shu and D. Lee.
Network Protocol System
Fingerprinting-A Formal Approach.
In IEEE Con-
ference on Computer Communications (INFOCOM),
2006.
[28] M. Sipser. Introduction to the Theory of Computation,
volume 2. Thomson Course Technology Boston, 2006.
[29] M. Veanes, P. D. Halleux, and N. Tillmann. Rex: Sym-
bolic regular expression explorer. In International Con-
ference on Software Testing, Veriﬁcation and Valida-
tion (ICST), 2010.
[30] M. Veanes, P. Hooimeijer, B. Livshits, D. Molnar, and
N. Bjorner. Symbolic ﬁnite state transducers: Algo-
rithms and applications. ACM SIGPLAN Notices, 47,
2012.
[31] W. Xu, Y. Qi, and D. Evans. Automatically evading
classiﬁers a case study on PDF malware classiﬁers. In
Proceedings of the 2016 Network and Distributed Sys-
tems Symposium (NDSS), 2016.
[32] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding and
understanding bugs in C compilers. In PLDI, 2011.