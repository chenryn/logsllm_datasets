alize our defense to the above settings can be an interesting
future direction to study.
7.2 Adversarial Patch Defenses
Empirical defenses like Digital Watermark (DW) [20] and
Local Gradient Smoothing (LGS) [39] were ﬁrst proposed
to detect and neutralize adversarial patch. However, these
heuristic defenses are vulnerable to adaptive attackers with
knowledge of the defense.
Observing the ineffectiveness of DW and LGS, Chiang et
al. [9] proposed the ﬁrst provable defense against adversarial
patches via Interval Bound Propagation (IBP) [18, 38]. De-
spite its important theoretical contribution, the IBP defense
has poor clean and provable robust accuracy, as shown in Ta-
ble 4. Zhang et al. [59] proposed clipped BagNet (CBN) for
provable robustness and Levine et al. [28] proposed building
a ‘smoothed’ classiﬁer (DS) that outputs the class with the
largest count from local predictions on all small pixel patches.
We have shown that CBN and DS are instances of our general
defense framework (Section 6.1), and PatchGuard has better
performance due to the use of robust masking (Section 5.2).
The Minority Report (MR) [34] defense was proposed in con-
current work, where the defender puts a mask at all possible
locations and extracts patterns from model predictions. This
defense can only provably detect an attack while PatchGuard
also guarantees the recovery of the correct prediction. More-
over, MR performs masking in the image space which is com-
putationally expensive and cannot scale to high-resolution
images. However, if we can tolerate attack detection, MR has
an advantage on low-resolution images (90.6% clean accu-
racy and 62.1% provable accuracy for 2.4%-pixel patch on
CIFAR-10; compared to our 84.6% clean accuracy and 57.7%
provable accuracy). How to extend PatchGuard for attack
detection is an interesting direction of future work.
Another concurrent line of research has been on adversarial
patch training [44, 54]. However, these works focus on empir-
ical robustness and do not provide any provable guarantees.
7.3 Receptive Fields of CNNs
A number of papers have studied the inﬂuence of the receptive
ﬁeld [1, 5, 25, 32] on model performance in order to better
understand the model behavior. BagNet [5] adopted the struc-
ture of ResNet-50 [21] but reduced the receptive ﬁeld size
by replacing 3×3 kernels with 1×1 kernels. BagNet-17 can
achieve similar top-5 validation accuracy as AlexNet [24]
on ImageNet [12] dataset when each feature only looks at a
17×17 pixel region. The small receptive ﬁeld was used for
better interpretability of model decisions in the original Bag-
Net paper. In this work, we use the reduced receptive ﬁeld
size to create models robust to adversarial patch attacks.
7.4 Other Adversarial Example Attacks and
Defenses
The development of adversarial example-based attacks and
defenses has been an extremely active research area over the
past few years. Conventional adversarial attacks [8,17,41,50]
craft adversarial examples that have a small Lp distance to
clean examples but induce model misclassiﬁcation. Many em-
pirical defenses [35,36,42,56] have been proposed to address
the adversarial example vulnerability, but most of them can
be easily bypassed by strong adaptive attackers [2, 7, 52].
The fragility of the empirical defenses has inspired prov-
able or certiﬁed defenses [10, 18, 26, 38, 43, 53] as well as
work on learning-theoretic bounds in the presence of adver-
saries [3, 11, 13, 45, 57]. In contrast, the focus of this paper
is on localized adversarial patch attacks, and we refer inter-
ested readers to survey papers [40, 58] for a more detailed
background on adversarial examples.
8 Conclusion
In this paper, we propose a general provable defense frame-
work called PatchGuard that mitigates localized adversarial
patch attacks. We identify large receptive ﬁelds and insecure
aggregation mechanisms in conventional CNNs as the key
sources of vulnerability to adversarial patches. To address
these two problems, our defense proposes the use of models
with small receptive ﬁelds to limit the number of features
corrupted by the adversary which are then augmented with
a robust masking defense to detect and mask the corrupted
features to ensure secure feature aggregation. Our defense
achieves state-of-the-art provable robust accuracy on Ima-
geNet, ImageNette, and CIFAR-10 datasets. We hope that our
general defense framework inspires further research to fully
mitigate adversarial patch attacks.
Acknowledgements
We are grateful to David Wagner for shepherding the pa-
per and anonymous reviewers at USENIX Security for their
valuable feedback. This work was supported in part by the
National Science Foundation under grants CNS-1553437 and
CNS-1704105, the ARL’s Army Artiﬁcial Intelligence Inno-
vation Institute (A2I2), the Ofﬁce of Naval Research Young
Investigator Award, the Army Research Ofﬁce Young Investi-
gator Prize, Faculty research award from Facebook, Schmidt
DataX award, and Princeton E-fﬁliates Award.
USENIX Association
30th USENIX Security Symposium    2249
References
[1] Andre Araujo, Wade Norris, and Jack Sim. Computing
receptive ﬁelds of convolutional neural networks. Dis-
till, 2019. https://distill.pub/2019/computing-receptive-
ﬁelds.
[2] Anish Athalye, Nicholas Carlini, and David A. Wag-
ner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples.
In
Proceedings of the 35th International Conference on
Machine Learning (ICML), pages 274–283, 2018.
[3] Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal.
Lower bounds on adversarial robustness from optimal
transport. In Conference on Neural Information Pro-
cessing Systems (NeurIPS), pages 7496–7508, 2019.
[4] Wieland
Brendel.
neural
local-features
networks.
//github.com/wielandbrendel/
bag-of-local-features-models, 2020.
Pretrained
bag-of-
https:
[5] Wieland Brendel and Matthias Bethge. Approximating
CNNs with bag-of-local-features models works surpris-
ingly well on ImageNet. In 7th International Conference
on Learning Representations (ICLR), 2019.
[6] Tom B. Brown, Dandelion Mané, Aurko Roy, Martín
Abadi, and Justin Gilmer. Adversarial patch. In Confer-
ence on Neural Information Processing Systems Work-
shops (NeurIPS Workshops), 2017.
[7] Nicholas Carlini and David A. Wagner. Adversarial ex-
amples are not easily detected: Bypassing ten detection
methods. In Proceedings of the 10th ACM Workshop on
Artiﬁcial Intelligence and Security (AISec@CCS), pages
3–14, 2017.
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. ImageNet: A large-scale hierarchical
image database. In 2009 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition
(CVPR), pages 248–255, 2009.
[13] Elvis Dohmatob. Generalized no free lunch theorem
for adversarial robustness. In Proceedings of the 36th
International Conference on Machine Learning (ICML),
pages 1646–1654, 2019.
[14] Cynthia Dwork and Aaron Roth. The algorithmic foun-
dations of differential privacy. Foundations and Trends
in Theoretical Computer Science, 9(3-4):211–407, 2014.
[15] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes,
Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash,
Tadayoshi Kohno, and Dawn Song. Robust physical-
world attacks on deep learning visual classiﬁcation. In
2018 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1625–1634, 2018.
[16] fast.ai. ImageNette: A smaller subset of 10 easily clas-
siﬁed classes from imagenet. https://github.com/
fastai/imagenette, 2020.
[17] Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial ex-
amples. In 3rd International Conference on Learning
Representations (ICLR), 2015.
[18] Sven Gowal, Krishnamurthy Dvijotham, Robert Stan-
forth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja
Arandjelovic, Timothy Arthur Mann, and Pushmeet
Kohli. Scalable veriﬁed training for provably robust
image classiﬁcation. In 2019 IEEE/CVF International
Conference on Computer Vision (ICCV), pages 4841–
4850, 2019.
[8] Nicholas Carlini and David A. Wagner. Towards evalu-
ating the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (S&P), pages 39–
57, 2017.
[19] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
BadNets: Identifying vulnerabilities in the machine
learning model supply chain. In Machine Learning and
Computer Security Workshop (NeurIPS MLSec), 2017.
[9] Ping-Yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen
Zhu, Christoph Studor, and Tom Goldstein. Certiﬁed
defenses for adversarial patches. In 8th International
Conference on Learning Representations (ICLR), 2020.
[20] Jamie Hayes. On visible adversarial perturbations &
In 2018 IEEE Conference on
digital watermarking.
Computer Vision and Pattern Recognition Workshops
(CVPR Workshops), pages 1597–1604, 2018.
[10] Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter.
Certiﬁed adversarial robustness via randomized smooth-
ing. In Proceedings of the 36th International Conference
on Machine Learning (ICML), pages 1310–1320, 2019.
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770–778, 2016.
[11] Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mit-
tal. PAC-learning in the presence of adversaries. In
Conference on Neural Information Processing Systems
(NeurIPS), pages 230–241, 2018.
[22] Danny Karmon, Daniel Zoran, and Yoav Goldberg. La-
VAN: Localized and visible adversarial noise. In Pro-
ceedings of the 35th International Conference on Ma-
chine Learning (ICML), pages 2512–2520, 2018.
2250    30th USENIX Security Symposium
USENIX Association
[23] Alex Krizhevsky. Learning multiple layers of features
from tiny images. https://www.cs.toronto.edu/
~kriz/learning-features-2009-TR.pdf, 2009.
[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. ImageNet classiﬁcation with deep convolutional
neural networks. In Conference on Neural Information
Processing Systems (NeurIPS), pages 1106–1114, 2012.
[25] Hung Le and Ali Borji. What are the receptive,
effective receptive, and projective ﬁelds of neurons
arXiv preprint
in convolutional neural networks?
arXiv:1705.07049, 2017.
[26] Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu,
Daniel Hsu, and Suman Jana. Certiﬁed robustness to
adversarial examples with differential privacy. In 2019
IEEE Symposium on Security and Privacy (S&P), pages
656–672, 2019.
[27] Alexander Levine and Soheil Feizi. Code for the
paper “(de)randomized smoothing for certiﬁable de-
fense against patch attacks". https://github.com/
alevine0/patchSmoothing, 2020.
[28] Alexander Levine and Soheil Feizi. (De)randomized
smoothing for certiﬁable defense against patch attacks.
In Conference on Neural Information Processing Sys-
tems, (NeurIPS), 2020.
[29] Wan-Yi Lin, Fatemeh Sheikholeslami, jinghao shi,
Leslie Rice, and J Zico Kolter. Certiﬁed robustness
against physically-realizable patch attack via random-
ized cropping, 2021.
[30] Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Yi-
ran Chen, and Hai Li. DPATCH: an adversarial patch
attack on object detectors. In Workshop on Artiﬁcial
Intelligence Safety 2019 co-located with the 33rd AAAI
Conference on Artiﬁcial Intelligence 2019 (AAAI), vol-
ume 2301, 2019.
[31] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee,
Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojan-
ing attack on neural networks. In 25th Annual Network
and Distributed System Security Symposium (NDSS),
2018.
[32] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard S.
Zemel. Understanding the effective receptive ﬁeld in
In Conference
deep convolutional neural networks.
on Neural Information Processing Systems (NeurIPS),
pages 4898–4906, 2016.
[33] Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial
attacks. In 6th International Conference on Learning
Representations (ICLR), 2018.
[34] Michael McCoyd, Won Park, Steven Chen, Neil Shah,
Ryan Roggenkemper, Minjune Hwang, Jason Xinyu Liu,
and David A. Wagner. Minority reports defense: De-
fending against adversarial patches. In Applied Cryptog-
raphy and Network Security Workshops (ACNS Work-
shops), volume 12418, pages 564–582. Springer, 2020.
[35] Dongyu Meng and Hao Chen. Magnet: A two-pronged
defense against adversarial examples. In Proceedings of
the 2017 ACM SIGSAC Conference on Computer and
Communications Security (CCS), pages 135–147, 2017.
[36] Jan Hendrik Metzen, Tim Genewein, Volker Fischer,
and Bastian Bischoff. On detecting adversarial pertur-
bations. In 5th International Conference on Learning
Representations (ICLR), 2017.
[37] Jan Hendrik Metzen and Maksym Yatsura. Efﬁcient
certiﬁed defenses against patch attacks on image clas-
siﬁers. In 9th International Conference on Learning
Representations (ICLR), 2021.
[38] Matthew Mirman, Timon Gehr, and Martin T. Vechev.
Differentiable abstract interpretation for provably robust
neural networks. In Proceedings of the 35th Interna-
tional Conference on Machine Learning (ICML), pages
3575–3583, 2018.
[39] Muzammal Naseer, Salman Khan, and Fatih Porikli. Lo-
cal gradients smoothing: Defense against localized ad-
versarial attacks. In IEEE Winter Conference on Appli-
cations of Computer Vision (WACV), pages 1300–1307,
2019.
[40] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and
Michael P Wellman. Sok: Security and privacy in ma-
chine learning. In 2018 IEEE European Symposium on
Security and Privacy (EuroS&P), pages 399–414, 2018.
[41] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha,
Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial
settings. In IEEE European Symposium on Security and
Privacy (EuroS&P), pages 372–387, 2016.
[42] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh
Jha, and Ananthram Swami. Distillation as a defense to
adversarial perturbations against deep neural networks.
In IEEE Symposium on Security and Privacy (S&P),
pages 582–597, 2016.
[43] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
Certiﬁed defenses against adversarial examples. In 6th
International Conference on Learning Representations
(ICLR), 2018.
USENIX Association
30th USENIX Security Symposium    2251
[44] Sukrut Rao, David Stutz, and Bernt Schiele. Adversarial
training against location-optimized adversarial patches.
In European Conference on Computer Vision Workshops
(ECCV Workshops), 2020.
[54] Tong Wu, Liang Tong, and Yevgeniy Vorobeychik. De-
fending against physically realizable attacks on image
classiﬁcation. In 8th International Conference on Learn-
ing Representations (ICLR), 2020.
[45] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras,
Kunal Talwar, and Aleksander Madry. Adversarially
robust generalization requires more data. In Advances
in Neural Information Processing Systems, pages 5014–
5026, 2018.