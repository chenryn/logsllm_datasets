ble 96% of the time, but experience short-lived instability periods.
Similar to Zhang et al. [32], we ﬁnd that path changes are local
and usually close to destinations: 86% of changes are inside an AS
and 31% of path changes impact the destination’s AS. We also ﬁnd
that 38% of path changes impact the path length, and 14% change
the AS-level path. Our previous work [8] presents a more detailed
characterization of path changes.
Tab. 1 shows the correlation between path features and residual
lifetime, computed by sampling the dataset with a Poisson process
with an average sampling period of 4 hours. For timescale-based
CURRENT ROUTE
Route Age
Length
AS length
Number of load balancers (i.e., hops with multiple next-hops)
Indicator of whether the route reaches the destination
CORRELATION
WITH L(r)
0.17
-0.10
-0.10
-0.04
-0.03
LAST CHANGE
Duration of the previous route
Length difference
AS length difference
Edit distance
AS edit distance
TIMESCALE-BASED (COMPUTED OVER [t − τ, t])
Prevalence of the current route
Average route duration
Standard deviation of route durations
Number of previous occurrences of the current route
Number of virtual path changes
EVENT-BASED
Times since the most recent occurrences of the current route
Number of changes since the most recent occ. of the cur. route
0.03
-0.07
-0.02
0.05
0.07
0.20
-0.11
-0.13
-0.11
-0.14
-0.08
-0.09
Table 1: Set of candidate features underlying prediction.
features we show correlation values for τ = 1 day, and for event-
based features we show the highest correlation. These low corre-
lation values indicate that no single feature can predict changes; in
the next section we study how to combine features for prediction.
3. PREDICTION FOUNDATIONS
Our path tracking approach is built on the ability to predict (al-
beit imperfectly) virtual path changes. We seek a predictor based
on an intuitive and parsimonious model rather than a black box.
However, virtual path changes are characterized by extreme vari-
ability and are inﬂuenced by many different factors, making model
building, and even feature selection, problematic. We employ Rule-
Fit [13], a state-of-the-art supervised machine learning technique,
to bootstrap our modeling efforts. We use RuleFit for two main
purposes. First, to comprehensively examine the spectrum of fea-
tures of Tab. 1 to determine the most predictive. Second, to act as a
benchmark representing in an approximate sense the best possible
prediction when large (off-line) resources are available for training.
3.1 RuleFit Overview
RuleFit [13] trains predictors based on rule ensembles. We
choose it over other alternatives (against which it compares favor-
ably) for two reasons: (i) it ranks features by their importance for
124prediction, (ii) it outputs easy-to-interpret rules that allow an under-
standing of how features are combined. We give a brief overview
of RuleFit, referring the reader to the original paper for details [13].
Rules combine one or more features into simple ‘and’ tests. Let
x be the feature vector in Tab. 1 and sf a speciﬁed subset of the
possible values of feature f. Then, a rule takes the form
(cid:2)
I(xf ∈ sf ),
r(x) =
(1)
where I(·) is an indicator function. Rules take value one when all
features have values inside their corresponding ranges, else zero.
f
RuleFit ﬁrst generates a large number of rules using decision
trees. It then trains a predictor of the form
(cid:3)
ˆφ(x) = a0 +
akrk(x),
(2)
k
where the vector a is computed by solving an optimization prob-
lem that minimizes the Huber loss (a modiﬁed squared prediction
error robust to outliers) with an L1 penalty term. RuleFit also em-
ploys other robustness mechanisms, for example it trains and tests
on subsets of the training data internally to avoid overﬁtting.
Rule ensembles can exploit feature interdependence and capture
complex relationships between features and prediction goals. Cru-
cially, RuleFit allows rules and features to be ordered by their im-
portance. Rule importance is the product of the rule’s coefﬁcient
and a measure of how well it splits the training set:
Ik = |ak|(cid:4)
sk(1 − sk),
where sk is the fraction of points in the training set where rk(x) =
1. Feature importance is computed as the sum of the normalized
importance of the rules where the feature appears:
If =
Ik/mk,
(3)
(cid:3)
k:f∈rk
where mk is the number of active features in rk.
3.2 RuleFit training sets
RuleFit, like any supervised learning algorithm, requires a train-
ing set consisting of training points that associate features with the
true values of metrics to be predicted. In our case, a training point,
say for residual lifetime, associates a virtual path at some time t,
represented by the features in Tab. 1, with the true residual lifetime
L(r) of the current route r = P (t). Separate but similar training is
performed for Nδ(P ) and Iδ(r).
To limit the computational load of training, which is high for
RuleFit, we control the total number of training points. For training
point selection, ﬁrst note that a given virtual path has a change his-
tory that is crucial to capture for good prediction of its future. We
therefore build the required number of training points by extracting
rich path change information from a subset of paths, rather than ex-
tracting (potentially very) partial information from each path. We
retain path diversity through random path selection, and the use of
multiple training sets (at least ﬁve for each parameter conﬁguration
we evaluate), obtained through using different random seeds.
For a given virtual path, we ﬁrst include all explicit path change
information by creating a training point for each entry in the dataset
where a change was detected. However, such points all have (mea-
sured) current route age equal to zero (Sec. 2.1), whereas when
running live predictions in general are needed at any arbitrary time
point, with arbitrary route age. To capture the interdependence of
features and prediction targets on route age we include additional
synthetic points which do not appear in the dataset but which are
functions of it. To achieve this we discretize route age into bins
and create a training point whenever the age of a route reaches a
bin boundary. We choose bin boundaries as equally-spaced per-
centiles of the distribution of route durations in the training set, as
this adapts naturally to distribution shape. Using ﬁve bins as ex-
ample, we create training points whenever a route’s age reaches
zero seconds, 3.5 min., 12 min., 48 min., and 4 hours.
3.3 Test sets
Like training sets, test sets consist of test points which associate
virtual path features with correct predictions. Unlike training sets,
where the primary goal is to collect information important for pre-
diction and where details may depend on the method to be trained,
for test sets the imperative is to emulate the information available
in the operational environment so that the predictor can be fairly
tested, and should be independent of the prediction method.
The raw dataset has too many points for use as a test set. To
reduce computational complexity, we build test sets by sampling
each virtual path at time points chosen according to a Poisson pro-
cess, using the same sampling rate for each path. This corresponds
to examining the set of paths in a neutral way over time, which will
naturally include a diversity of behavior. For example, our test sets
include samples inside bursts of path changes, many samples from
a very long-lived route, and rare events such as of an old route just
before it changes.
We use an average per-path sampling period of four hours, result-
ing in at least two orders of magnitude more test points than training
points. We test each predictor against eight test sets (from different
seeds), for a total of 40 different training–test set combinations.
We ignore routes active at the beginning or the end of the dataset
when creating training and test sets, as their duration, age, and
residual lifetime are unknown. Similarly, we ignore all virtual path
changes in the ﬁrst τ hours of the dataset (if τ (cid:3)= ∞) to avoid
biasing timescale-dependent features.
3.4 RuleFit conﬁguration
In this section we study the impact of key parameters on predic-
tion accuracy, pick practical default values, and justify our use of
RuleFit as a benchmark for predicting virtual path changes.
We study the impact of four parameters on prediction error:
the number of rules generated during training, the number of age
thresholds, the timescale τ , and the training set size. Each plot in
Fig. 3.1 varies the value of one parameter while keeping the others
ﬁxed. We show results for EIδ with δ = 4 hours because this is
the prediction goal where the studied parameters have the greatest
impact. Results for other values and other prediction goals are qual-
itatively similar. We compute the prediction error rate only for test
points with route age less than 12 hours to focus on the differences
between conﬁgurations. As we discuss later, prediction accuracy is
identical for routes older than 12 hours regardless of conﬁguration.
We plot the minimum, median, and maximum error rate over 40
combinations of training and test sets for each conﬁguration.
Fig. 3.1(a) shows that the beneﬁt of increasing the number of
generated rules is marginal beyond 200 for this data. Our interpre-
tation is that at 200 or so rules, RuleFit has already been able to
exploit all information relevant for prediction. Therefore, we train
predictors with 200 rules unless stated otherwise.
Fig. 3.1(b) shows that prediction error decreases when we add
additional points with age diversity into training sets as described
in Sec. 3.2. However, as few as three age bins are enough to achieve
accurate predictions, and improvement after six is minimal. There-
fore, we train predictors with six age bins unless stated otherwise.
Fig. 3.1(c) shows that the timescale τ used to compute timescale-
dependent features has little impact on prediction accuracy. A pos-
125)
h
4
I
(
e
t
a
R
r
o
r
r
E
n
o
i
i
t
c
d
e
r
P
 0.5
 0.45
 0.4
 0.35
 0.3
 0.25
 0.2
Min, Median, Max
(a)
 0.5
 0.45
 0.4
 0.35
 0.3
 0.25
 0.2
Min, Median, Max
(b)
 0.5
 0.45
 0.4
 0.35
 0.3
 0.25
 0.2
Min, Median, Max
(c)
 0.5
 0.45
 0.4
 0.35
 0.3
 0.25
 0.2
Min, Median, Max
(d)
50
100 200 300 400 500
 1  2  3  4  5  6
8
12
Number of Rules
Number of Age Bins
4h 12h 1d
2d
Timescale τ
7d ∞
100
1K 5K 25K
200K
Number of Path Changes
Figure 3: Impact of the (a) number of rules, (b) number of age bins, (c) timescale τ , and (d) training set size on RuleFit accuracy
(test points with route age less than 12 hours).
PATH FEATURE
Prevalence of the current route (τ = 1 day)
Num. of virtual path changes (τ = 1 day)
Num. of previous occ. of the current route (τ = 1 day)
Route age
IMPORTANCE
1.0
.624
.216
.116
Times since most recent occs. of the current route ≤ .072
Edit distance (last change)
Duration of the previous route
Standard deviation of route durations (τ = 1 day)
Length difference (last change)
.015
.014
.014
.012
All other features ≤ .010
Min, Median, Max
 0.5
 0.45
 0.4
 0.35
 0.3
 0.25
 0.2
)
h
4
I
(
e
a
R
t
r
o
r
r
E
n
o
i
i
t
c
d
e
r
P
Table 2: Feature importance according to RuleFit.
 1
 3
 2
 5
Number of Features
 4
All
sible explanation is that only the long term mean value of timescale-
dependent features is predictive, and that RuleFit discovers this and
only builds the means of these features into the predictor (or ignores
them). Therefore, we train predictors with timescale-dependent
features computed with τ = 1 day.
Finally, Fig. 3.1(d) shows the impact of the number of virtual
path changes in a training set. Training sets with too few changes
fail to capture the virtual path change diversity present in test sets,
resulting in predictors that do not generalize. Prediction accuracy
increases quickly with training set size before ﬂattening out. We
use training sets with 200,000 virtual path changes (around 2.4%
of those in the dataset) unless stated otherwise.
We justify our use of RuleFit as a benchmark for predicting
changes, based on a given (incomplete) dataset, on: (i) we pro-
vide RuleFit with a rich feature set, (ii) RuleFit performs an ex-
tensive search of feature combinations to predict residual lifetimes,
and (iii) our evaluation shows that changing RuleFit’s parameters
is unlikely to improve prediction accuracy signiﬁcantly. This is an
empirical approach to approximately measure the limits to predic-
tion using a given dataset. Determining actual limits would only be
possible given information-theoretic or statistical assumptions on
the data, which is beyond the scope of this paper.
3.5 Feature selection
We compute feature importance with Eq. (3) and normalize us-
ing the most important feature. Tab. 2 shows the resulting ordered
features, with normalized importance averaged over 50 predictors
for each of residual lifetime, number of changes, and Iδ.
Route prevalence is the most important feature, helped by its
correlation with route age. It is clear why route prevalence alone is
insufﬁcient. It cannot differentiate a young current route that also
occurred repeatedly in the time window of width τ , from a middle-
aged current route, as both have intermediate prevalence values.
The second, third, and fourth most important features are the
number of virtual path changes, the number of occurrences of the
current route, and route age. Predicted residual lifetimes increase
as route age and prevalence increase, but decrease as the number of
Figure 4: EI4h for predictors trained with the most important
features (test points with route age < 12 hours).
virtual path changes and occurrences of the current route increase.
Results for the number of changes and Iδ are similar.
The ﬁfth most important feature is the times (1st up to 5th) of the
most recent occurrences of the current route. The low importance
of this and the other event-based feature suggests that, contrary to
our initial hopes, patterns of changes are too variable, or too rare,
to be useful for prediction.
To evaluate more objectively the utility of RuleFit’s feature im-
portance measure, Fig. 4 shows EIδ=4h for predictors trained with
training sets containing only the top p features, for p = 1 to 5.
The improvements in performance with the addition of each new
feature are consistent with the importance rankings from Tab. 2.
Importantly, we see that the top four features generate predictors
which are almost as accurate as those trained on all features.
4. NEAREST–NEIGHBOR PREDICTOR
We design and evaluate a simple predictor which is almost as
accurate as RuleFit while overcoming its slow and computationally
expensive training, its difﬁcult integration into other systems, and
the lack of insight and control arising from its black box nature.
4.1 NN4: Deﬁnition
We start from the observation that the top four features from
Tab. 2 carry almost all of the usable information. Since virtual paths