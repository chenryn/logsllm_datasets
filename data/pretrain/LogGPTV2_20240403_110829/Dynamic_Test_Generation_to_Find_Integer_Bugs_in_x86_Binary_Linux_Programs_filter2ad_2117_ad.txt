seedﬁle type and size
MP3 (159000 bytes)
AVI (980002 bytes)
JPG (22844 bytes)
TAR.GZ (14763 bytes)
TAR.BZ2 (618620 bytes)
PNG (25385 bytes)
Branches
20647045
4147710
809806
24782
107396936
98993374
x86 instrs
159500373
19539096
6185985
161118
746219573
478474232
IRStmts
810829992
115036155
32460806
880386
4185066021
2802603384
asserts
1960
4778690
81450
95960
1787053
583
queries
36
462346
1006
13309
314914
81
Figure 4: The size of our test programs. We report the source lines of code for each test program and the size of one
of our seed ﬁles, as measured by David A. Wheeler’s sloccount. Then we run the test program on that seed ﬁle and
report the total number of branches, x86 instructions, Valgrind IR statements, STP assert statements, and STP query
statements for that run. We ran symbolic execution for a maximum of 12 hours, which was sufﬁcient for all programs
except mplayer, which terminated during symbolic execution.
symbolic execution and constraint solving. This shows
the effect of the generational search and the choice of
memory model; we leverage a single expensive symbolic
execution and fast solver queries to generate many test
cases. At the same time, we note that zzuf found a seri-
ous InvalidWrite bug, while SmartFuzz did not.
A previous version of our infrastructure had prob-
lems with test cases that caused the target program to
run forever, causing the search to stall. Therefore, we
introduced a timeout, so that after 300 CPU seconds,
the target program is killed. We manually examined
the output of memcheck from all killed programs to de-
termine whether such test cases represent errors. For
gzip we discovered that SmartFuzz created six such test
cases, which account for the two out-of-bounds read (In-
validRead) errors we report; zzuf did not ﬁnd any hang-
ing test cases for gzip. We found no other hanging test
cases in our other test runs.
Different Bugs Found by SmartFuzz and zzuf. We
ran the same target programs with the same seed ﬁles
using zzuf. Figure 7 shows bugs found by each type of
fuzzer. With respect to each tool, SmartFuzz found 37
total distinct bugs and zzuf found 59 distinct bugs. We
found some overlap between bugs as well: 19 bugs were
found by both fuzz testing tools, for a total of 77 distinct
bugs. This shows that while there is overlap between the
two tools, SmartFuzz ﬁnds bugs that zzuf does not and
vice versa. Therefore, it makes sense to try both tools
when testing software.
Note that we did not ﬁnd any bugs for bzip2 with ei-
ther fuzzer, so neither tool was effective on this program.
This shows that fuzzing is not always effective at ﬁnd-
ing bugs, especially with a program that has already seen
attention for security vulnerabilities. We also note that
SmartFuzz found InvalidRead errors in gzip while zzuf
found no bugs in this program. Therefore gzip is a case
where SmartFuzz’s directed testing is able to trigger a
bug, but purely random testing is not.
Block Coverage. We measured the number of basic
blocks in the program visited by the execution of the
seed ﬁle, then measured how many new basic blocks
were visited during the test run. We discovered zzuf
added a higher percentage of new blocks than Smart-
Fuzz in 13 of the test runs, while SmartFuzz added a
higher percentage of new blocks in 4 of the test runs
(the SmartFuzz convert-2 test run terminated prema-
turely.) Table 8 shows the initial basic blocks, the num-
ber of blocks added, and the percentage added for each
fuzzer. We see that the effectiveness of SmartFuzz varies
by program; for convert it is particularly effective, ﬁnd-
ing many more new basic blocks than zzuf.
Contrasting SmartFuzz and zzuf Performance. De-
spite the limitations of random testing, the blackbox fuzz
testing tool zzuf found bugs in four out of our six test
programs.
In three of our test programs, zzuf found
more bugs than SmartFuzz. Furthermore, zzuf found
the most serious InvalidWrite errors, while SmartFuzz
did not. These results seem surprising, because Smart-
Fuzz exercises directed testing based on program behav-
ior while zzuf is a purely blackbox tool. We would ex-
pect that SmartFuzz should ﬁnd all the bugs found by
zzuf, given an unbounded amount of time to run both
test generation methods.
In practice, however, the time which we run the meth-
ods is limited, and so the question is which bugs are dis-
covered ﬁrst by each method. We have identiﬁed possi-
ble reasons for this behavior, based on examining the test
cases and Valgrind errors generated by both tools4. We
now list and brieﬂy explain these reasons.
Header parsing errors. The errors we observed are often
in code that parses the header of a ﬁle format. For exam-
ple, we noticed bugs in functions of mplayer that parse
MP3 headers. These errors can be triggered by simply
placing “wrong” values in data ﬁelds of header ﬁles. As
a result, these errors do not require complicated and un-
likely predicates to be true before reaching buggy code,
and so the errors can be reached without needing the full
power of dynamic test generation. Similarly, code cov-
4We have placed representative test cases from each method at
http://www.metafuzz.com/example-testcases.tgz
11
SyscallParam
UninitCondition
UninitValue
Overlap
Leak DeﬁnitelyLost
Leak PossiblyLost
InvalidRead
InvalidWrite
Total
Cost per bug
mplayer
4
3
1
13
3
0
0
0
2
2
2
1
2
1
1
0
22
13
$1.30 $2.16
ffmpeg
2
3
8
1
3
0
1
0
4
2
0
2
4
0
3
0
5
28
$5.76 $1.03
$1.80 $1.03
exiv2
gzip
0
0
0
0
0
0
4
0
4
0
0
0
0
0
1
6
0
7
convert
0
0
8
3
2
0
0
0
0
0
0
0
1
1
0
0
4
11
0
0
0
0
0
0
2
0
2
$3.60 NA $1.20 $0.65
0
0
0
0
0
0
0
0
0
Figure 7: The number of bugs, after fuzzy stack hashing, found by SmartFuzz (the number on the left in each column)
and zzuf (the number on the right). We also report the cost per bug, assuming $0.10 per small compute-hour, $0.40
per large compute-hour, and 3 runs of 24 hours each per target for each tool.
erage may be affected by the style of program used for
testing.
Difference in number of bytes changed. The two differ-
ent methods change a vastly different number of bytes
from the original seed ﬁle to create each new test case.
SmartFuzz changes exactly those bytes that must change
to force program execution down a single new path or
to violate a single property. Below we show that in our
programs there are only a small number of constraints on
the input to solve, so a SmartFuzz test case differs from
its seed ﬁle by only a small number of bytes. In contrast,
zzuf changes a ﬁxed fraction of the input bytes to ran-
dom other bytes; in our experiments, we left this at the
default value of 0.004.
The large number of changes in a single fuzzed test
case means that for header parsing or other code that
looks at small chunks of the ﬁle independently, a sin-
gle zzuf test case will exercise many different code
paths with fuzzed chunks of the input ﬁle. Each path
which originates from parsing a fuzzed chunk of the in-
put ﬁle is a potential bug. Furthermore, because Val-
grind memcheck does not necessarily terminate the pro-
gram’s execution when a memory safety error occurs,
one such ﬁle may yield multiple bugs and correspond-
ing bug buckets.
In contrast, the SmartFuzz test cases usually change
only one chunk and so explore only one “fuzzed” path
through such code. Our code coverage metric, unfortu-
nately, is not precise enough to capture this difference be-
cause we measured block coverage instead of path cov-
erage. This means once a set of code blocks has been
covered, a testing method receives no further credit for
additional paths through those same code blocks.
Small number of generations reached by SmartFuzz. Our
24 hour experiments with SmartFuzz tended to reach a
small number of generations. While previous work on
whitebox fuzz testing shows that most bugs are found
in the early generations [14], this feeds into the pre-
vious issue because the number of differences between
the fuzzed ﬁle and the original ﬁle is proportional to the
generation number of the ﬁle. We are exploring longer-
running SmartFuzz experiments of a week or more to ad-
dress this issue.
Loop behavior in SmartFuzz. Finally, SmartFuzz deals
with loops by looking at the unrolled loop in the dynamic
program trace, then attempting to generate test cases for
each symbolic if statement in the unrolled loop. This
strategy is not likely to create new test cases that cause
the loop to execute for vastly more iterations than seen
in the trace. By contrast, the zzuf case may get lucky by
assigning a random and large value to a byte sequence
in the ﬁle that controls the loop behavior. On the other
hand, when we looked at the gzip bug found by Smart-
Fuzz and not by zzuf, we discovered that it appears to be
due to an inﬁnite loop in the inflate dynamic routine
of gzip.
7.4 SmartFuzz Statistics
Integer Bug Queries Vary By Program. Table 5 shows
the number of solver queries of each type for one of our
24-hour test runs. We see that the type of queries varies
from one program to another. We also see that for bzip2
and mplayer, queries generated by type inference for
signed/unsigned errors account for a large fraction of all
queries to the constraint solver. This results from our
choice to eagerly generate new test cases early in the pro-
gram; because there are many potential integer bugs in
these two programs, our symbolic traces have many inte-
ger bug-seeking queries. Our design choice of using an
independent tool such as memcheck to ﬁlter the resulting
test cases means we can tolerate such a large number of
queries because they require little human oversight.
Time Spent In Each Task Varies By Program. Fig-
ure 9 shows the percentage of time spent in symbolic
execution, coverage, triage, and recording for each run
of our experiment. We also report an “Other” category,
which includes the time spent in the constraint solver.
This shows us where we can obtain gains through fur-
ther optimization. The amount of time spent in each
task depends greatly on the seed ﬁle, as well as on the
12
Test run
mplayer-1
mplayer-2
mplayer-3
ffmpeg-1
ffmpeg-2
ffmpeg-3
convert-1
convert-2
convert-3
exiv2-1
exiv2-2
exiv2-3
gzip-1
gzip-2
gzip-3
bzip2-1
bzip2-2
bzip2-3
Initial basic blocks
SmartFuzz
zzuf
7823
7819
11376
11375
11096
11093
6470
6470
6427
6427
611
6112
8246
8258
10715
9816
9807
9810
2088
2169
2124
2778
2778
2822
8028
8040
NA
9819
9811
9814
2088
2169
2124
2779
2777
2823
SmartFuzz
70%
7%
0.9%
9.14%
10.53%
1.58%
27%
29%
NA
29.9%
28.3%
28.7%
Blocks added by tests Ratio of prior two columns
SmartFuzz
zzuf