rarely used. This unused space retains data as long as the
slab page itself persists—at least as long as any block in
the page is in use and ordinarily longer—and slab pages
tend to be deallocated in large numbers only under mem-
ory pressure. Thus, we expect data that falls into a hole
in a slab to persist for a long time on an ordinarily loaded
system, explaining our observations.
Effect of Rebooting In the course of setting up exper-
iments, we rebooted some machines multiple times and
found, to some surprise, that some stamps put into mem-
ory before reboot remained. We investigated further and
found that a “soft” reboot, that is, a reboot that does
not turn off the machine’s power, does not clear most of
RAM on the machines we tested. The effect of a “hard”
reboot, that is, one that powers off the machine, varied.
On some machines, hard reboots cleared all stamps; on
others, such as IBM ThinkPad T30 laptops, many were
retained even after 30 seconds without power. We con-
clude that it is a bad idea to assume a reboot will clear
memory without knowledge about the speciﬁc hardware
in use.
334
14th USENIX Security Symposium
USENIX Association
4 Designing Secure Deallocation
In this section we describe the design principles behind
secure deallocation.
4.1 A Conservative Heuristic
Secure deallocation clears data at deallocation or within
a short, predictable time afterward. This provides a con-
servative heuristic for minimizing data lifetime.
Secure deallocation is a heuristic in that we have no
idea when a program last uses data. We just leverage the
fact that last-time-of-use and time-of-deallocation are of-
ten close together (see section 5). This is conservative
in that it should not introduce any new bugs into exist-
ing programs, and in that we treat all data as sensitive,
having no a priori knowledge about how it is used in an
application.
This approach is applicable to systems at many lev-
els from OS kernels to language runtimes, and is agnos-
tic to memory management policy, e.g. manual freeing
vs. garbage collection. However, the effectiveness of se-
cure deallocation is clearly inﬂuenced by the structure
and policy of a system in which it is included.
4.2 Layered Clearing
We advocate clearing at every layer of a system where
data is deallocated including user applications, the com-
piler, user libraries, and the OS kernel. Each layer offers
its own costs and beneﬁts that must be taken account.
• Applications generally have the best knowledge of
what data are sensitive and when the best time to
clear them is. For example, an application that pops
elements off a circular queue knows immediately
that the space used to store those elements can and
should be cleared. Because such operations are usu-
ally implemented in terms of simple pointer incre-
ments and decrements, the heap storage layer sim-
ply has no way of knowing this data could have been
cleared.
Unfortunately, it can be complex and labor intensive
to identify all the places where sensitive data resides
and clear it appropriately. We explore an example of
modifying a piece of complex, data-handling soft-
ware (the Linux kernel) to reduce the time that data
is held in section 6.
• Compilers handle all the implicit allocations per-
formed by programs (e.g. local variables allocated
on the stack), therefore they can handle clearing
data that programs do not explicitly control. Clear-
ing data at this level can be expensive, and we ex-
plore the trade-offs in performance in section 4.4.
• Libraries handle most of the dynamic memory re-
quests made by programs (e.g. malloc/free) and
are the best place to do clearing of these requests.
Clearing at this level has the caveat that we must de-
pend on programs to deallocate data explicitly, and
to do so as promptly as possible. We explore the
efﬁcacy of this approach in section 5.
• Operating system kernels are responsible for man-
aging all of an application’s resources. This in-
cludes process pages used in satisfying memory re-
quests, as well as pages used to buffer data going to
or coming from I/O devices.
The OS is the ﬁnal safety net for clearing all of
the data possibly missed by, or inaccessible to, user
programs. The OS kernel’s responsibilities include
clearing program pages after a process has died, and
clearing buffers used in I/O requests.
Why Layered Clearing? Before choosing a layered
design, we should demonstrate that it is better than a
single-layer design, such as a design that clears only
within the user stack and heap management layer.
Clearing only in a lower layer (e.g. in the kernel in-
stead of the user stack/heap) is suboptimal. For exam-
ple, if we do zeroing only when a process dies, data can
live for long periods before being cleared in long run-
ning processes. This relates back to the intuition behind
the heuristic aspect of secure deallocation.
Clearing only in a higher layer (e.g. user stack/heap
instead of kernel) is a more common practice. This is in-
complete because it does not deal with state that resides
in kernel buffers (see section 6 for detailed examples).
Further, it does not provide defense in depth, e.g. if a pro-
gram crashes at any point while sensitive data is alive, or
if the programmer overlooks certain data, responsibility
for that data’s lifetime passes to the operating system.
This basic rationale applies to other layered software
architectures including language runtimes and virtual
machine monitors.
The chief reason against a layered design is perfor-
mance. But as we show in section 7, the cost of zeroing
actually turns out to be trivial, contrary to popular belief.
4.3 Caveats to Secure Deallocation
Secure deallocation is subject to a variety of caveats:
• No Deallocation. Some applications deallocate lit-
tle of their memory. In experiments we perform in
section 5, for example, we see workloads where less
than 10% of memory allocated was freed. In short-
lived applications, this can be handled by the OS
USENIX Association
14th USENIX Security Symposium
335
In longer-lived applica-
kernel (see section A.1).
tions little can be done without modifying the pro-
gram itself. Static data has the same issue because
it also survives until the process terminates.
• Memory Leaks. Failing to free memory poses a data
lifetime problem, although we’ll see in section 5
that programs usually free data that they allocate.
Fortunately, leaks are recognized as bugs by appli-
cation programmers, so they are actively sought out
and ﬁxed.
Long-lived servers like sshd and Apache are gen-
erally written to conscientiously manage their mem-
ory, commonly allowing them to run for months
on end. When memory leaks do occur in these
programs, installations generally have facilities for
handling them, such as a cron job that restarts the
process periodically.
• Custom Allocators. Custom allocators are com-
monly used to improve application performance or
to help manage memory, e.g. by preventing memory
leaks. Doing so, however, hides the application’s
use of memory from the C library, reducing the ef-
fectiveness of secure deallocation in the C library.
Region-based allocators [8], for example, serve
allocation requests from a large system-allocated
pool. Objects from this pool are freed en masse
when the whole pool is returned to the system. This
extends secure deallocation lifetimes, because the
object’s use is decoupled from its deallocation.
Circular queues are another common example. A
process that buffers input events often does not
clear them after processing them from the queue.
Queue entries are “naturally” overwritten only
when enough additional events have arrived to make
the queue head travel a full cycle through the queue.
If the queue is large relative to the rate at which
events arrive, this can take a long time.
These caveats apply only to long-lived processes like
Apache or sshd, since short-lived processes will have
their pages quickly cleaned by the OS. Furthermore,
long-lived processes tend to free memory meticulously,
for reasons described above, so the impact of these
caveats is generally small in practice.
These challenges also provide unique opportunities.
For example, custom allocators designed with secure
deallocation can potentially better hide the latency of ze-
roing, since zeroing can be deferred and batched when
large pools are deallocated. Of course, a healthy balance
must be met—the longer zeroing is deferred, the less use-
ful it is to do the zeroing at all.
Figure 4: Crests and troughs of stack usage over time for a
web browsing session under Firefox 1.0 (stack grows down-
ward). Firefox typiﬁes stack usage for a GUI application: the
main window event loop sits high atop the stack and occasion-
ally makes excursions downwards to do processing, in this case
web page rendering, only to return back to the event loop. In-
tervals between excursions are on a human scale (seconds or
minutes).
4.4 Implementing Clearing
In this section we provide some practical examples of
design trade-offs we made in our secure deallocation im-
plementation.
Compilers and Libraries Secure deallocation in com-
pilers and libraries is relatively simple, and consists of
clearing the heap and the stack.
All heap allocated data is zeroed immediately during
the call to free. Data is cleared immediately because
the latency imposed appears to be negligible in most
cases, given the speed of zeroing.
For the stack, we explored two strategies: zeroing ac-
tivation frames immediately as their function returns and
periodically zeroing all data below the stack pointer (all
old, currently unused space). The latter strategy amor-
tizes the performance overhead of stack over many calls
and returns, although it has the disadvantage of missing
“holes” in the stack (see section 3.1).
The intuition for periodically zeroing the stack is il-
lustrated by Figure 4, obtained by instrumenting Firefox
1.0. Although applications do make excursions down-
wards to do initialization or complex processing, many,
particularly long-lived ones like network server daemons
or GUI programs, spend most of their time high atop
the stack, waiting in an event loop for a network/user
request.
336
14th USENIX Security Symposium
USENIX Association
Clearing in the Kernel
In the kernel we leveraged
our greater knowledge of the semantics of different data
structures to selectively clear only memory that may con-
tain sensitive data. We chose this approach because the
kernel is performance sensitive, despite the greater effort
and implementation complexity required.
Ideally, this approach would provide the same reduc-
tion of sensitive data lifetime as we would obtain by
clearing everything in the main kernel allocators, perhaps
better, as speciﬁc data structures such as circular queues
are cleared as well. However, as this is not conservative,
there is a greater risk that we may have overlooked some
potentially sensitive data.
The kernel has two primary responsibilities for zero-
ing. First, it must clear user space memory which has
been deallocated, e.g. by process death or unmapping a
private ﬁle mapping. Next, it must clear potentially sen-
sitive state residing in I/O buffers e.g. network buffers
(e.g. sk buffs in Linux), tty buffers, IPC buffers.
Due to the range and complexity of zeroing done in
the kernel we have deferred most of our discussion to
section 6 and further in appendix A.
Zeroing Large Pools of Memory An unusual aspect
of kernel zeroing is the need to clear large areas such as
the pages in a terminated process. This requires signif-
icant care in order to balance the demand for short and
predictable data lifetime against the need for acceptable
latency.
To provide predictable data lifetime, we would like to
have some sort of deadline scheduling in place, e.g. a
guarantee that sensitive pages are zeroed within n sec-
onds of deallocation. We would like n to be as small
as possible without imposing unacceptable immediate la-
tency penalties on processes. On the other hand, if n is
too large many dirty pages could accumulate, especially
under heavy load. This could lead to long and unpre-
dictable pauses while the system stops to zero pages. In-
tuitively, this is very similar to garbage collection paus-
ing a program to free up memory.
Sometimes proactively zeroing memory can actually
improve system responsiveness. Even an unmodiﬁed
kernel must zero memory before allocating it to a user
process, to prevent sensitive data from one protection do-
main from leaking into another. Often this is done on
demand, immediately before pages are needed. Doing
this before pages are needed can improve performance
for process startup. Zeroing memory can also increase
page sharing under some virtual machine monitors [24].
Another important consideration is ensuring that zero-
ing large pools of memory does not blow out caches. We
discuss this issue in section 7.1.
A more complete treatment of zeroing performed by
the kernel is provided later in section 6 and appendix A.
Side Effects of Secure Deallocation Secure deallo-
cation only modiﬁes data with indeterminate content,
e.g. freed data on the heap. This should not introduce
bugs in correct programs. Some buggy software, how-
ever, depends on the value of indeterminate data.
Use of indeterminate data takes two forms. Software
may use data before it has been initialized, expecting it
to have a constant value. Alternately, software may use
data after it has been freed, expecting it to have the same
value it had prior to deallocation.
By making the value of indeterminate data consistent,
some buggy code will now always break. However, this
also changes some non-deterministic “Heisenbugs” into
deterministic “Bohr bugs,” e.g. returning a pointer to a
local, stack-allocated variable will always break, instead
of just when a signal intervenes between function return
and pointer dereference. This can be beneﬁcial as it may
bring otherwise hard to ﬁnd bugs to the surface. Con-
versely, secure deallocation may eliminate some bugs
permanently (e.g. data is always initialized to zero as a
programmer assumed).
Implementers of secure deallocation should consider
this issue when deciding what value they wish to use for
clearing memory. For example, matching the value the
existing OS uses for clearing process pages (e.g. zero on
Linux x86, 0xDEADBEEF on AIX RS/6000), is a good
heuristic for avoiding the introduction of new use-before-
initialization bugs.
5 Data Lifetime Reduction
Evaluating the effectiveness of secure deallocation re-
quires us to answer a variety of questions, including:
How often do applications deallocate their own mem-
ory? Can we rely on incidental reuse and overwriting
to destroy sensitive data—do we even need secure deal-
location? What kind of delay can we expect between last
use of data and its deallocation?
Using the conceptual framework introduced in sec-
tion 3, we try to answer these questions using our ex-
ample implementation of heap clearing and a variety of
common workloads.
5.1 Measurement Tool
We created a tool for measuring data lifetime related
events in standard user applications.
It works by dy-
namically instrumenting programs to record all accesses
to memory: all reads, writes, allocations, and dealloca-
tions. Using this information, we can generate precise
numbers for ideal, natural, and secure deallocation life-
times as well as other data properties like holes.
We based our tool on Valgrind [18], an open source de-
bugging and proﬁling tool for user-level x86-Linux pro-
USENIX Association
14th USENIX Security Symposium
337
grams. It is particularly well-known as a memory debug-
ger. It also supports a general-purpose binary instrumen-
tation framework that allows it to be customized. With
this framework, we can record timestamps for the events
illustrated in Figure 1. We can compute various lifetime
spans directly from these timestamps.
5.2 Application Workloads
We performed our experiments on a Linux x86 worksta-
tion, selecting applications where data lifetime concerns
are especially important, or which lend insight into inter-
esting dynamic allocation behavior:
• Mozilla, a popular graphical web client. We auto-
mated Mozilla v1.4.3 to browse through 10 differ-
ent websites chosen for their mix of images, text
layouts, CSS, and scripting.
• Thunderbird, a graphical mail client included as
part of the Mozilla application suite. We set up
Thunderbird 1.0 to automatically iterate through
over 100 email messages that include text and im-
ages.
• ssh, a secure remote shell. Using OpenSSH 3.9p1,
we scripted our ssh workload using expect to log
into a ssh server, read mail in pine, edit some text
with emacs, and walk through various directories.
• sshd,
the secure shell daemon from OpenSSH
v3.9p1. This is the server side of the ssh client
test.
• Python, an interpreted, object-oriented language
with garbage collection. Python and similar man-
aged language runtimes are increasingly impor-
tant for running applications, and they have data-
lifetime properties characteristically different from
applications that manually manage data. We used
Python 2.4 to run a program that computes large
primes.
• Apache, a web server. Our Apache workload serves
a small corpus of static HTML and images using
Apache 2.0.52. We automated a client to spend
about an hour hitting these objects in sequence.
• xterm, a terminal emulator for X11.
Inside
XFree86’s xterm 4.3.99.5(179), we ran a small
client process that produces profuse output for
about 45 minutes.
• ls, the canonical directory lister. Although ls
does not obviously handle much sensitive data, its
data lifetime characteristics give us some insight
into how more non-GUI-centric applications might
be expected to behave. This workload performs a
recursive, long-formatted directory listing starting
from the root of a large ﬁle system using GNU ls
5.0.
We omit detailed performance testing for these appli-
cations, due to the difﬁculty of meaningfully character-
izing changes to interactive performance. Any perfor-