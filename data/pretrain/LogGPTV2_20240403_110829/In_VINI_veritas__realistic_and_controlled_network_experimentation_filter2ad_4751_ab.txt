•      Non-IP  Protocols:  An  Ethernet-based  OpenVPN  link  allows 
experimenters to run any protocol (e.g., NetBIOS or IPX) that is 
framed in Ethernet packets.  
•    Dynamic IP addresses: For security and routing reasons, when 
using an IP OpenVPN link in the existing VINI architecture, all IP 
addresses  and  ranges  for  which  the  link  is  responsible  must  be 
configured  in  advance  and  cannot  be  changed  without  breaking 
the  VPN  tunnel.  Ethernet  tunnels  do  not  suffer  from  this 
limitation  because  the  Ethernet  tunnels  simply  pass  all  Ethernet 
frames through the tunnel without any IP requirements or routing 
complications. 
•    Broadcast Packets: OpenVPN IP links support unicast packets 
only  and  do  not  support  broadcast/multicast  packets.  This  is 
problematic because routing protocols operating on Layer 3 of the 
OSI stack may rely on broadcast packets to discover neighbors or 
peers. OpenVPN Ethernet links allow broadcast packets (Ethernet 
address: FF-FF-FF-FF-FF-FF or IP address range 240.0.0.0/4) to 
flow between VINI and ORBIT.  
Another  VINI  architecture  aspect  that  needs  modifications  is 
the Click modular router. Click performs forwarding of packets 
based  on  IP  addresses,  which  restricts  users  into  having 
predefined  IP  ranges  that  should  be  modified  in  Click  as  the 
experiment changes to reflect the proper IP routing scheme.  In 
order to remove the need to modify Click on a per experiment 
basis  and  make  experiments  independent  of  IP  protocols, 
changes  are  needed  on  Click  so  that  Ethernet  packets  are 
delivered  to  the  user  space  (i.e.,  UML)  instead  of  IP  packets. 
Within  UML,  an  experimenter  can  then  work  in  the  familiar 
Linux 
network 
drivers/modules to handle IP or non-IP packets. 
environment 
proper 
create 
and 
the 
4.2  ORBIT Architecture Components 
ORBIT nodes do not have a predefined software architecture as 
compared  to  VINI  nodes.  Therefore,  the  integration  issue  as  it 
relates  to  ORBIT  deals  more  with  deriving  a  solution  that 
provides a desirable integrated topology rather than modifying an 
existing software architecture. On the ORBIT side, the integrated 
solution  should  enable  a  VINI  node  to  communicate  to  one  or 
more ORBIT nodes or groups of nodes. Two possible candidates 
that  provide  different  topology  characteristics  are  the  Router 
configuration and the Bridge configuration. 
In the Router configuration, an ORBIT node is set up as a router 
and  is  connected  to  the  VINI  network  as  seen  in  Figure  4.  An 
OpenVPN tunnel is used to provide a point-to-point link between 
the  two  networks.  The  ORBIT  Router  A  node  supports  the 
protocol of the packets received from the OpenVPN link through 
proper configuration within Linux.   
It  should  be  emphasized  that  although  the  example  uses  IPv4 
routing, this design can utilize non-IP routing as well. In addition, 
this  configuration  enables  Router  A  to  run  any  of  the  routing 
protocol supported through XORP in the VINI environment and 
therefore  exchange  routes  automatically  with  VINI  nodes.  In 
terms of integration, this set up can be visualized as adding nodes 
(e.g.,  Router  A)  and  extending  the  existing  VINI  core  network 
while  providing  access  to  a  wireless  networks.  Typically,  this 
mode can be utilized to enable integration of multi-hop wireless 
networks with a wired testbed.  
Orbit nodes                                  VINI nodes
192.168.100.2
172.20.0.2
172.20.0.3
192.168.100.1
172.20.0.1
192.168.102.2
ROUTER A
192.168.102.1
OpenVPN
tunnel
192.168.101.1
192.168.101.2
Wireless Link
Wired link
Figure 4. Integration using the router configuration. 
In the Ethernet Bridge configuration, the ORBIT node bridges the 
OpenVPN interface with the wireless interfaces and removes the 
need to carry routing. Such a setup allows for experiments where 
multiple wireless end nodes are attached to VINI nodes and can 
be visualized as adding a wireless interface to a VINI node that is 
physically disjoint.  
An example of a bridge configuration is shown in Figure 5. The 
VINI  interface  (172.20.0.1)  is  virtually  connected  to  the  Bridge 
A’s wireless interface, which essentially makes that VINI node a 
wireless node attached to the ORBIT network or retrospectively 
provides  the  VINI  node  access  to  wireless  ORBIT  nodes  (e.g., 
172.20.0.2 and 172.20.0.3). Once again, IPv4 is used here as an 
example,  but  this  framework  will  allow  non-IP  and  broadcast 
packets.    Typically,  this  mode  can  be  utilized  to  enable  the 
integration  of  access  point  functionality  on  the  wired  testbed 
nodes (i.e., one hop wireless connectivity). 
Orbit nodes                                  VINI nodes
192.168.100.2
172.20.0.2
172.20.0.3
192.168.100.1
BRIDGE A
172.20.0.1
OpenVPN
tunnel
192.168.101.1
192.168.101.2
Wireless Link
Wired link
Link to wireless Interface
Figure 5. Integration using the bridge configuration. 
4.3  IMPLEMENTATION 
The  modifications  applied  to  ORBIT  and  VINI  testbeds  are 
presented in the following subsections. 
4.3.1  VINI Modifications 
For ingress traffic we pass Ethernet packets from OpenVPN to the 
UML  instead  of  IP  packets  as  currently  utilized  by  VINI.  We 
achieve  this  feature  by  modifying  both  OpenVPN  and  Click 
configurations.  Ethernet 
tunnels  are  enabled  by  changing 
OpenVPN  links  between  ORBIT  and  VINI  to  use  Linux  Tap 
devices  instead  of  Tun  devices.    Therefore,  any  traffic  from 
ORBIT to VINI is now delivered via OpenVPN Ethernet tunnels 
instead  of  OpenVPN  IP  tunnels.  We  then  elect  to  send  packets 
from  OpenVPN  directly  to  UML  with  no  modifications.  Since 
packets  coming 
in  from  OpenVPN  are  already  Ethernet 
encapsulated they do not need to be encapsulated again by Click. 
The Click forwarding mechanism is disabled and packets are sent 
directly  to  UML  without  inspecting  the  packet  contents  with  a 
directive 
::  Socket(UNIX_DGRAM, 
“/tmp/click.sock”)  ->  openvpn.”  Therefore,  the  UML  instance 
handles  all  the  packet  routing  decisions.    The  standard  Linux 
‘route’ command can be used within UML to direct packets. The 
advantage of this configuration is the support of broadcast traffic 
and  non-IP  based  protocols.  Non-IP  routing  protocols  can  be 
implemented  and  tested  within  UML.  In  the  current  VINI 
architecture,  Click  routes  packets  based  on  IP  addresses,  which 
restricts experiments to IP protocols. An additional benefit of this 
“openvpn 
such 
as: 
configuration  is  that  it  removes  the  need  to  modify  the  Click 
configuration for each experiment through the VINI setup files.  
PlanetLab node
VINI virtual node
XORP
1
eth3
eth1
UML
eth2
UML Switch    
Unix Socket
2
Click
UDP tunnel
Other
VINI
nodes
Unix Socket
3
OpenVPN
tap0
tunnel
Non-VINI
node
1  
2  
3  
a)  New interface binds to OpenVPN 
b)  Enable forwarding and arp 
a)  Forward ingress traffic to UML  
b)  Forward  egress  traffic  to  OpenVPN  based  on  UML 
     Ethernet address 
a)  Changed addressing scheme 
b)  Utilized  Tap  instead  of  Tun  interfaces  to  bind  to 
     OpenVPN and use Ethernet tunnels 
c)  Changed  from  server  to  client  setup  to  point-to-point 
     setup 
Figure 6. Key modifications of VINI architecture. 
The  method  for  handling  egress  traffic  from  the  VINI  UML 
instance to ORBIT in the modified architecture involves creating 
a new virtual interface in each UML instance, eth3 (see Figure 6).  
The  VINI  configuration  scripts  assign  each  eth3  interface  a 
unique MAC address and its own IP address in the 192.168.0.0/16 
range.  Packets sent out from UML through this eth3 interface 
are assigned this known MAC address. Click is again modified 
to  process  Ethernet  packets  coming  from  the  source  MAC 
address  of  eth3  and  send  matching  packets  through  OpenVPN 
links.  Within  the  UML  instance,  eth3  interface  is  seen  as  a 
regular  Ethernet  interface  and  is  expected  to  act  as  a  normal 
interface.  The  interface  can  be  assigned  multiple  IP  addresses 
(aliases)  on  different  subnets  and  can  be  used  for  any  Linux 
routing implementation.  
Another  issue  that  we  had  to  address  during  the  integration  of 
PlanetLab-VINI  and  ORBIT  was  the  dynamic  allocation  of  IP 
addresses.  Both  testbeds  utilize  the  private  class  A  reserved  IP 
range  10.0.0.0/8,  which  lead  to  IP  address  conflicts.  More 
specifically,  each  VINI  instance  uses  the  PlanetLab  slices  tap0 
interface, which is assigned a unique class C address space within 
the  10.0.0.0/8 address space. OpenVPN server in VINI attempts 
to  push  routes  for  10.0.0.0/8  to  OpenVPN  clients  that  need  to 
connect  to  VINI.  On  the  other  hand,  ORBIT  nodes  use  class  A 
addresses to communicate between the management console and 
to access other ORBIT services. Thus, connectivity to the ORBIT 
nodes  running  OpenVPN  clients  (i.e.,  ORBIT  nodes)  during 
integration  was  lost.  This  problem  was  alleviated  by  assigning 
address  from  172.16.0.0/12  and  192.168.0.0/16  to  OpenVPN 
servers  and  clients,  which  are  private  IP  ranges  and  will  not 
conflict with standard ORBIT or PlanetLab IP ranges. Some other 
modifications  that  were  made  included  enabling  IP  packet 
forwarding and ARP responses on UML instances, both of which 
were disabled in VINI by default. 
Overall,  the  aforementioned  modifications  accommodate  both 
ingress  and  egress  flows  and  create  a  virtual  Ethernet  link 
between  the  UML  eth3  interface  and  the  interface  on  ORBIT 
nodes  without  Click  having  to  know  the  details  of  the  traffic 
flowing  between  the  two  points.  Figure  6  summarizes  the  key 
modifications on the VINI architecture. 
4.3.2  ORBIT Modifications 
An ORBIT baseline image was used as a foundation for building 
the ORBIT node with router or bridge functionality. The baseline 
image was Debian GNU with Linux kernel 2.6.12. OpenVPN was 
compiled  and  installed  along  with  OpenSSL  [15].  The  Linux 
kernel was recompiled with the Tunneling and Bridging options 
to  enable  the  creation  of  Tap  interfaces  for  OpenVPN  and  to 
allow  the  operation  in  bridging  mode.    Bridgeutils  was  also 
compiled and installed to provide the node with the proper tools 
to create, modify and delete bridges. 
The  Bridge  configuration 
is 
accomplished using the Linux bridge tool, brctl (see Figure 7). A 
TAP interface, tap0 is created using the Linux ‘mknod’ command 
so  that  OpenVPN  can  use  that  device  to  pass packets. A bridge 
interface,  br0  is  then  created  and  linked  to  the  tap0  interface 
forming a virtual Ethernet bridge.  An IP address is assigned to 
br0 to provide the OpenVPN link endpoint with an IP addresses.  
Thus,  the  UML  eth3  interface  on  VINI  (shown  in  Figure  6)  is 
linked to tap0 on ORBIT, which is bridged to ath0, the wireless 
interface. Packets sent out of the eth3 interface will go onto the 
ORBIT wireless network. 
In  the  Router  configuration,  a  tap0  device  is  again  created  and 
linked to a bridge, br0 device (see Figure 8). An IP address (and 
optional multiple aliases) are assigned to br0 and traffic is routed 
between br0 and other interfaces (e.g., wireless interface ath0).  
For both Bridge and Router configurations, the VINI scripts are 
set  to  automatically  generate  commands  for  the  ORBIT  control 
framework to image, power on, and configure the ORBIT Bridge 
and  Router  nodes.  Thus,  automatic  topology  creation  during 
experiments is facilitated. 
in  Section  4.2) 
(described 
The overlay connectivity of the integrated testbed can be shown 
by  performing  traceroute  from  the  Mobile  client  (node1-2),  to 
Access Point B (node1-3): 
node1-2.sb2.orbit-lab.org:~# traceroute 192.168.103.2 
traceroute  to  172.16.1.2  (172.16.1.2),  30  hops  max,  40  byte 
packets 
 1 node1-1. (172.16.0.1)  0.521 ms  0.495 ms  0.451 ms 
 2 eth2.berkeley (192.168.107.1)  73.075 ms  103.113 ms  74.466 
ms 
 3 eth3.caltech (192.168.100.3)  86.964 ms  86.009 ms  103.015 
ms 
 4 eth3.mit (192.168.101.3)  167.395 ms  184.267 ms  170.087 ms 
 5  tap0.node1-3  (192.168.103.2)    192.127  ms    177.791  ms  
189.877 ms 
The  path  between  the  Mobile  Client  and  Access  Point  B  is 
through  access  point  A  (node  1-1),  Berkeley,  California  Tech, 
MIT,  and  onto  the  tap0  interface  of  Access  Point  B,  which  is 
the  expected  path.  Thus,  it  can  be  seen  that  in  addition  to 
ORBIT-to-ORBIT  node  connectivity 
the  172.16.0.0/12 
range, the ORBIT nodes have access to the VINI nodes on the 
192.168.0.0/16 network. 
in 
192.168.105.3
br0:192.168.105.2
(Bridge
Mode)
Video Server
OpenVPN
Ethernet
Tunnel
192.168.105.1
Berkeley
192.168.100.2
192.168.101.2
192.168.100.3
192.168.101.3
California Tech
192.168.107.1
OpenVPN
Ethernet
Tunnel
MIT
192.168.103.1
OpenVPN
Ethernet
Tunnel
Access
Point A
(Router
Mode)
192.168.107.2
172.16.0.1
192.168.103.2
172.16.0.1
Access
Point B
(Router
Mode)
Mobile Client