### Non-IP Protocols
An Ethernet-based OpenVPN link allows experimenters to run any protocol (e.g., NetBIOS or IPX) that is encapsulated in Ethernet packets.

### Dynamic IP Addresses
In the existing VINI architecture, when using an IP-based OpenVPN link, all IP addresses and ranges for which the link is responsible must be configured in advance. These configurations cannot be changed without breaking the VPN tunnel. In contrast, Ethernet tunnels do not have this limitation because they simply pass all Ethernet frames through the tunnel without any IP requirements or routing complications.

### Broadcast Packets
OpenVPN IP links support only unicast packets and do not support broadcast or multicast packets. This can be problematic because routing protocols operating at Layer 3 of the OSI stack often rely on broadcast packets to discover neighbors or peers. OpenVPN Ethernet links, however, allow broadcast packets (Ethernet address: FF-FF-FF-FF-FF-FF or IP address range 240.0.0.0/4) to flow between VINI and ORBIT.

### Click Modular Router
The Click modular router in the VINI architecture forwards packets based on IP addresses, which restricts users to predefined IP ranges. These ranges need to be modified in Click as the experiment changes to reflect the proper IP routing scheme. To eliminate the need for per-experiment modifications and to make experiments independent of IP protocols, Click needs to be modified so that Ethernet packets are delivered to the user space (i.e., UML) instead of IP packets. Within UML, experimenters can then use familiar Linux network drivers/modules to handle both IP and non-IP packets.

### ORBIT Architecture Components
ORBIT nodes do not have a predefined software architecture, unlike VINI nodes. Therefore, the integration issue with ORBIT focuses more on deriving a solution that provides a desirable integrated topology rather than modifying an existing software architecture. On the ORBIT side, the integrated solution should enable a VINI node to communicate with one or more ORBIT nodes or groups of nodes. Two possible configurations are the Router configuration and the Bridge configuration.

#### Router Configuration
In the Router configuration, an ORBIT node is set up as a router and connected to the VINI network via an OpenVPN tunnel, as shown in Figure 4. The ORBIT Router A node supports the protocol of the packets received from the OpenVPN link through proper configuration within Linux. Although the example uses IPv4 routing, this design can also utilize non-IP routing. This configuration enables Router A to run any of the routing protocols supported by XORP in the VINI environment, thus allowing automatic route exchange with VINI nodes. This setup can be visualized as adding nodes (e.g., Router A) and extending the existing VINI core network while providing access to wireless networks. This mode is typically used to integrate multi-hop wireless networks with a wired testbed.

**Figure 4. Integration using the router configuration.**

#### Bridge Configuration
In the Ethernet Bridge configuration, the ORBIT node bridges the OpenVPN interface with the wireless interfaces, eliminating the need for routing. This setup allows for experiments where multiple wireless end nodes are attached to VINI nodes and can be visualized as adding a wireless interface to a VINI node that is physically disjoint. An example of a bridge configuration is shown in Figure 5. The VINI interface (172.20.0.1) is virtually connected to the Bridge Aâ€™s wireless interface, making the VINI node a wireless node attached to the ORBIT network or providing the VINI node access to wireless ORBIT nodes (e.g., 172.20.0.2 and 172.20.0.3). This framework supports both non-IP and broadcast packets. This mode is typically used to enable access point functionality on the wired testbed nodes (i.e., one-hop wireless connectivity).

**Figure 5. Integration using the bridge configuration.**

### Implementation
The modifications applied to the ORBIT and VINI testbeds are detailed in the following subsections.

#### VINI Modifications
For ingress traffic, we pass Ethernet packets from OpenVPN to the UML instead of IP packets, as currently utilized by VINI. This is achieved by modifying both OpenVPN and Click configurations. Ethernet tunnels are enabled by changing OpenVPN links between ORBIT and VINI to use Linux TAP devices instead of TUN devices. Thus, any traffic from ORBIT to VINI is now delivered via OpenVPN Ethernet tunnels instead of OpenVPN IP tunnels. We then send packets from OpenVPN directly to UML without modifications. Since packets coming from OpenVPN are already Ethernet-encapsulated, they do not need to be re-encapsulated by Click. The Click forwarding mechanism is disabled, and packets are sent directly to UML without inspecting the packet contents. The UML instance handles all packet routing decisions, and the standard Linux `route` command can be used within UML to direct packets. This configuration supports broadcast traffic and non-IP-based protocols, allowing non-IP routing protocols to be implemented and tested within UML. It also removes the need to modify the Click configuration for each experiment through the VINI setup files.

**Figure 6. Key modifications of VINI architecture.**

For egress traffic from the VINI UML instance to ORBIT, a new virtual interface, eth3, is created in each UML instance. The VINI configuration scripts assign each eth3 interface a unique MAC address and its own IP address in the 192.168.0.0/16 range. Packets sent out from UML through this eth3 interface are assigned this known MAC address. Click is modified to process Ethernet packets coming from the source MAC address of eth3 and send matching packets through OpenVPN links. Within the UML instance, the eth3 interface is seen as a regular Ethernet interface and can be assigned multiple IP addresses (aliases) on different subnets, supporting any Linux routing implementation.

During the integration of PlanetLab-VINI and ORBIT, we addressed the dynamic allocation of IP addresses. Both testbeds use the private class A reserved IP range 10.0.0.0/8, leading to IP address conflicts. Specifically, each VINI instance uses the PlanetLab slices tap0 interface, which is assigned a unique class C address space within the 10.0.0.0/8 address space. The OpenVPN server in VINI attempts to push routes for 10.0.0.0/8 to OpenVPN clients that need to connect to VINI. On the other hand, ORBIT nodes use class A addresses to communicate between the management console and to access other ORBIT services. This led to connectivity issues with ORBIT nodes running OpenVPN clients during integration. The problem was resolved by assigning addresses from the 172.16.0.0/12 and 192.168.0.0/16 ranges to OpenVPN servers and clients, which are private IP ranges and do not conflict with standard ORBIT or PlanetLab IP ranges. Additional modifications included enabling IP packet forwarding and ARP responses on UML instances, both of which were disabled in VINI by default.

Overall, these modifications accommodate both ingress and egress flows and create a virtual Ethernet link between the UML eth3 interface and the interface on ORBIT nodes without Click needing to know the details of the traffic flowing between the two points. Figure 6 summarizes the key modifications to the VINI architecture.

#### ORBIT Modifications
An ORBIT baseline image was used as a foundation for building the ORBIT node with router or bridge functionality. The baseline image was Debian GNU with Linux kernel 2.6.12. OpenVPN was compiled and installed along with OpenSSL. The Linux kernel was recompiled with Tunneling and Bridging options to enable the creation of TAP interfaces for OpenVPN and to allow operation in bridging mode. Bridgeutils was also compiled and installed to provide the node with the necessary tools to create, modify, and delete bridges.

**Figure 7. Bridge configuration.**
- A TAP interface, tap0, is created using the Linux `mknod` command.
- A bridge interface, br0, is created and linked to the tap0 interface, forming a virtual Ethernet bridge.
- An IP address is assigned to br0 to provide the OpenVPN link endpoint with an IP address.
- The UML eth3 interface on VINI (shown in Figure 6) is linked to tap0 on ORBIT, which is bridged to ath0, the wireless interface. Packets sent out of the eth3 interface will go onto the ORBIT wireless network.

**Figure 8. Router configuration.**
- A tap0 device is created and linked to a bridge, br0 device.
- An IP address (and optional multiple aliases) are assigned to br0, and traffic is routed between br0 and other interfaces (e.g., wireless interface ath0).

For both Bridge and Router configurations, the VINI scripts automatically generate commands for the ORBIT control framework to image, power on, and configure the ORBIT Bridge and Router nodes, facilitating automatic topology creation during experiments.

### Overlay Connectivity
The overlay connectivity of the integrated testbed can be demonstrated by performing a traceroute from the Mobile client (node1-2) to Access Point B (node1-3):

```bash
node1-2.sb2.orbit-lab.org:~# traceroute 192.168.103.2
traceroute to 172.16.1.2 (172.16.1.2), 30 hops max, 40 byte packets
 1  node1-1. (172.16.0.1)  0.521 ms  0.495 ms  0.451 ms
 2  eth2.berkeley (192.168.107.1)  73.075 ms  103.113 ms  74.466 ms
 3  eth3.caltech (192.168.100.3)  86.964 ms  86.009 ms  103.015 ms
 4  eth3.mit (192.168.101.3)  167.395 ms  184.267 ms  170.087 ms
 5  tap0.node1-3 (192.168.103.2)  192.127 ms  177.791 ms  189.877 ms
```

The path between the Mobile Client and Access Point B goes through Access Point A (node 1-1), Berkeley, California Tech, MIT, and onto the tap0 interface of Access Point B, which is the expected path. This demonstrates that, in addition to ORBIT-to-ORBIT node connectivity, the ORBIT nodes have access to the VINI nodes on the 192.168.0.0/16 network.