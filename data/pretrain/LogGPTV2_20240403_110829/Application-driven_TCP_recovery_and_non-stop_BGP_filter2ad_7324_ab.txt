that the same negotiation takes place during recovery.
RFC 1323 [11] describes the ﬁrst additional options to be
deﬁned, Window Scaling and Timestamps, with the goal of
supporting high-latency high-bandwidth networks. To support
Window Scaling and Timestamps, TCPR passes the parameters
through and saves them for recovery, just as it does with
Maximum Segment Size. The authors of RFC 1323 noted that
the only previously non-padding option, Maximum Segment
Size, was only sent on SYN packets, so they worried that
buggy TCP implementations might erroneously fail to handle
unknown options on normal trafﬁc. To address that concern,
they established the convention that TCP options are negotiated
in the handshake or else disabled. The result for TCPR is that
suppressing unknown options on a handshake will generally
avoid the need to suppress them any further.
TCPR also supports the Selective Acknowledgments [12]
option, which enables an endpoint to acknowledge data that
it receives out-of-order or with gaps. Advancing the actual
acknowledgment would erroneously cover the gaps, but failing
to acknowledge the received data might force the remote
endpoint to wastefully retransmit data that wasn’t really lost.
At ﬁrst glance, Selective Acknowledgments might seem in-
compatible with TCPR’s delayed acknowledgments. However,
the standard speciﬁes that Selective Acknowledgments are
purely advisory: although they serve as notiﬁcation, the peer
is still responsible for eventually retransmitting that data if the
cumulative acknowledgment never catches up. Thus, it sufﬁces
for TCPR to apply ∆ to the peer’s selective acknowledgments
as well as to its ACKs.
E. Masking Failure
TCPR cannot trust the network stack to accurately distin-
guish between the application closing a connection and failure.
During failover or migration, the application has (conceptually
if not in fact) two unsynchronized network stacks—the new
one, in which the connection is not yet established, and the
old one, in which the connection is no longer established.
Until the new network stack is resynchronized, any packet
it receives will be unacceptable, and it will send a RST
as discussed in Figure 2. An endpoint with an established
connection never sends a RST, so TCPR drops it to prevent
what we term a Romeo and Juliet scenario: if the peer received
the notice that the application is dead, it would abort the
connection just as the application came back to life.
Failure can also be revealed to the peer if the old network
stack tries to cleans up by closing the connection, such as
when only the application process fails.
An endpoint closes its output by sending a packet with
the FIN ﬂag, which occupies a byte at the end of the stream
and must be acknowledged by the remote endpoint, like the
SYN at the beginning. At the packet level, there is no way
to distinguish whether a FIN indicates failure or a deliberate
call
to close. Prior approaches have interposed on the
network stack’s interface to learn when the application closes
a connection deliberately. In TCPR we adopt a more explicit
approach: the application sets a ﬂag, done writing, just before
it closes. TCPR treats a FIN as spurious if and only if that ﬂag
is clear.
TCPR responds to a spurious FIN with a RST, in order
to enable the application to recover quickly in the case where
the old and new network stacks are the same. The network
stack will abort the old connection and be ready immediately
to establish the new connection.
F. Closing Input
To deliberately close the stream in the other direc-
tion, the application sets another ﬂag, done reading. When
done reading is set, TCPR does not delay acknowledgments.
The network stack is free to acknowledge any remaining
data, notably including the peer’s FIN, so the peer can close
gracefully.
If the FIN is the only byte remaining to be acknowledged,
the application could instead advance ack by one byte. TCPR
provides done reading to echo the behavior of shutdown,
and it also enables us to perform experiments without delayed
acknowledgments.
G. Recovering After Closing
As with send, a successful close indicates only that the
FIN is in the send buffer. It might take some time for all of
the data to be sent and acknowledged. Even once the ﬁnal
acknowledgment is sent, TCP implementations wait, usually
120 seconds, to be sure that the acknowledgment arrives and to
handle any straggling packets. What if the application crashes
after abdicating its socket?
failed, when it has detected that
TCPR sets a ﬂag, done, when it thinks both ﬂows are
closed, and another,
the
network stack has failed. The application can check at any time
whether the connection is really closed on the wire. If neces-
sary, the application can recover as normal; during recovery,
TCPR always unsets done writing to give the application the
chance to call close again.
Once close has been set for an appropriate duration (such
as 120 seconds) the application explicitly instructs TCPR to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:18 UTC from IEEE Xplore.  Restrictions apply. 
TCPR
SYN 800
S Y N 3 0 0
( 8 0 1 )
801 (301)
SYN 800
( 4 0 1 )
3 0 1
401(301)
Fig. 5: When TCPR fails over, it depends on the application for
some of its state, and can recover the rest from the peer itself.
For example, if TCPR cannot immediately answer a recovery
SYN as in Figure 4, sending the unacceptable packet to the
peer prompts it to ﬁll in the missing state.
delete its state. Of course, there is no reason the application
has to wait, and being in control of when TCPR deletes its
state also makes it easy to experimentally inject failures.
H. Recovering TCPR
Should TCPR itself fail, it can recover some of its state,
such as peer ack, by observing packets on the wire. The
remaining state, such as the latest ack for delaying acknowl-
edgments, is provided by the application. TCPR avoids the
need to replicate any of its own state because of our assumption
that recovery is driven by a fault-tolerant application.
For example, if the application is trying to recover but
the latest peer ack is missing, TCPR delivers the recovery
SYN to the peer uncorrected; the peer’s answer reveals the
desired value. See Figure 5 for an example. Resynchronization
takes place as in Figure 4, but with an additional round-trip to
recover soft state from the peer.
That default behavior also avoids the need for a special
case to detect whether a connection is new or recovering. If
peer ack is missing, either TCPR crashed and lost it, or the
connection is new and it doesn’t exist yet; in the former case,
the peer provides the missing value, and in the latter, it sends
its own valid answer to the handshake.
On the other hand, ﬁelds such as ack and done writing
cannot be inferred from packet-level observation, because they
are inherently controlled by the application. Neither can ﬁelds
such as the saved values of options, which are advertised only
once. Thus the application is expected to reset these values
through the side channel it has with TCPR.
The state that depends on the application changes much
more slowly than the soft state recoverable from packets.
Whereas peer ack is updated with every packet from the
peer, all of the application-dependent state is either ﬁxed at
connection establishment (such as the peer’s TCP options),
struct tcpr_hard {
uint16_t port;
struct {
uint16_t port;
uint16_t mss;
uint8_t ws;
uint8_t sack_permitted;
} peer;
uint32_t ack;
uint8_t done_reading;
uint8_t done_writing;
};
struct tcpr {
struct tcpr_hard hard;
uint32_t delta;
uint32_t ack;
uint32_t fin;
uint32_t seq;
uint16_t win;
uint16_t port;
struct {
uint32_t ack;
uint32_t fin;
uint16_t win;
uint8_t have_fin;
uint8_t have_ack;
} peer;
uint8_t have_fin;
uint8_t done;
uint8_t failed;
uint8_t syn_sent;
};
struct tcpr_ip4 {
uint32_t address;
uint32_t peer_address;
struct tcpr tcpr;
};
Fig. 6: TCPR state structures. The application keeps one
struct tcpr_ip4 for each TCP/IP connection, but only
the 14-byte portion deﬁned by struct tcpr_hard is nec-
essary for recovery.
set occasionally by the application itself (ack), or set by the
application once when the connection closes (done reading
and done writing). Thus, both the extent of the modiﬁcations
to the application code and the communication overhead of
maintaining TCPR’s copy of the state are minimal.
III.
IMPLEMENTATION
The TCP-manipulating core of TCPR is implemented as
a portable C library. The simplicity of application-driven
recovery is reﬂected in the fact that the library’s single ﬁle
contains only about 150 semicolons. We have experimented
with TCPR using a variety of techniques to interpose on
packets; the current prototype is a loadable module for the
Linux kernel ﬁrewall, iptables. The system administrator writes
rules to match packets to and from the application, delivering
them to TCPR rather than dropping or forwarding them.
TCPR’s per-connection state appears in Figure 6. Notably,
there is no buffered data. Both TCPR and the application
keep exactly one struct tcpr_ip4 per connection. The
network-independent state is in struct tcpr, while only
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:18 UTC from IEEE Xplore.  Restrictions apply. 
the subset of the state in struct tcpr_hard is crucial
for recovery—if any ﬁeld outside struct tcpr_hard is
missing, TCPR will recover it on the ﬂy.
The side channel is a UDP connection, and the protocol
consists only of entire states sent back and forth. The state
is small enough to avoid being a burden to send, and its
constant size and the atomic delivery of UDP combine to
make the update protocol easy to implement at both ends. For
example, to acknowledge some data, the application locally
sets tcpr.hard.ack and sends the entire state to TCPR.
Using UDP makes it possible for TCPR to be situated on
a middlebox physically distinct from the one on which the
application is running, but other options are also possible. In
our experiments, the highest efﬁciency was achieved when run-
ning TCPR in a separate network namespace but on the same
machine as the recoverable application. Network namespaces
are a recent Linux kernel feature that enables an individual
process to have an isolated routing table, network stack, and
set of network interfaces, while sharing the host’s memory,
ﬁlesystem, processors, and kernel. With the application in its
own network namespace, TCPR can run on the host as a
middlebox while enjoying loopback-interface throughput and
latency.
The TCPR distribution includes a netcat-like program
and a TCP proxy that provides TCPR-support for unmodiﬁed
applications, along with a utility to craft UDP TCPR updates
on the command line to query TCPR state. To measure recov-
ery time, we have implemented a utility that opens hundreds
of connections in parallel, injects failure on each of them,
and then times its recovery using Linux’s high-resolution real-
time clock. To measure throughput, we have also modiﬁed the
venerable ttcp to support TCPR; including error handling
and new command-line options for conﬁguring TCPR, all that
it required was the addition of 28 lines.
Modifying an application to use TCPR does not require any
changes to existing socket system calls. Instead, one simply
adds code to interact with TCPR during connection setup and
teardown, and when input is to be checkpointed. A trivial
example is shown in Figure 7.
A fault-tolerant application that uses TCPR makes the
usual socket calls. After calling connect or accept—that
is, once the connection has been established—the application
retrieves the connection’s state from TCPR. As the application
consumes its input, it updates ack locally, then updates TCPR.
Similarly, when there is no more input, it sets done reading,
and when it is ﬁnished writing output, it sets done writing.
If TCPR itself fails, the application need only send another
update message. If the application fails, it need only bind
and connect again to establish a new connection with the
same endpoints. Optionally, TCPR can remap the source port
to avoid collisions if the original port is already bound on
the recovering machine (using tcpr.port different from
tcpr.hard.port). Both TCPR and application recovery
are included in the snippet in Figure 8.
IV. EVALUATION
We have evaluated application-driven TCP recovery, using
BGP as our recoverable application, with the goal of validating
s = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
bind(s, &addr, addrlen);
listen(s, backlog);
c = accept(s, &peeraddr, &peeraddrlen);
getsockname(c, &addr, &addrlen);
// request connection state
state.address = addr.sin_addr.s_addr;
state.peer_address = peeraddr.sin_addr.s_addr;
state.tcpr.hard.port = addr.sin_port;
state.tcpr.hard.peer.port = peeraddr.sin_port;
write(tcpr, &state, sizeof(state));
// receive TCPR’s copy
read(tcpr, &state, sizeof(state));
bytes = read(c, readbuf, readbuflen);
// update delayed acknowledgment
state.tcpr.hard.ack =
htonl(ntohl(state.tcpr.hard.ack) + bytes);
write(tcpr, &state, sizeof(state));
write(c, writebuf, writebuflen);
// close gracefully
state.tcpr.hard.done_reading = 1;
state.tcpr.hard.done_writing = 1;
write(tcpr, &state, sizeof(state));
close(c);
Fig. 7: A simple C server that uses TCPR.
// send TCPR the latest state
write(tcpr, &state, sizeof(state));
c = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
bind(c, &addr, addrlen);
connect(c, &peeraddr, peeraddrlen);
Fig. 8: A snippet by which the server in Figure 7 recovers
from simultaneous TCPR and connection failure.
four key assertions:
•
•
•
•
TCPR adds negligible overhead to throughput and
latency.
TCPR enables rapid recovery.
By contrast, unprotected BGP recovery causes unnec-
essary and severe outages.
Graceful Restart (GR) sometimes causes even more
disruption than standard BGP recovery.
The TCPR microbenchmarks were conducted between two
commodity Linux machines, each with two cores, connected
by a 1 Gbps Ethernet link. The BGP fault injection benchmarks
were conducted using an experimental framework we imple-
mented, which connects actual software routers and network
stacks in a virtual network. All of the nodes run in network
namespaces on a single host machine, which we veriﬁed was
not bottlenecked by CPU. Running on a single machine means
that network latencies are negligible, and all of the nodes share
a precise wall clock. In all experiments, TCP was implemented
by an unmodiﬁed Linux kernel network stack.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:18 UTC from IEEE Xplore.  Restrictions apply. 
Mbps
Unprotected
896.354 ± 0.331
TCPR 896.385 ± 0.280
% Raw
100
100