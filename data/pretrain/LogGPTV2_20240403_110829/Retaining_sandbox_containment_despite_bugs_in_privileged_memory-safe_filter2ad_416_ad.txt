boundaries and capabilities explicit, which may mitigate the
confusion that led to some of these errors.
7.2 Performance
In many applications there exists a delicate balance be-
tween performance and security. In the context of our appli-
cation – hosting experimental code on volunteered machines
– we can reduce performance to increase security.
In this
section we evaluate the two types of performance penalties
incurred when using security layers: initialization and use.
All of the following performance tests were run on an Apple
iMac with an Intel i7-860 2.8GHz CPU, 4GB of 1333MHz
RAM running Mac OS X 10.6.3. Running time calculations
are averages over 10,000 iterations.
Initialization Cost. When a security layer is initialized,
the code is validated and the encasement library creates cus-
tom wrapper functions for the individual functions. To eval-
uate this, we examined the initialization time of the sand-
box with and without the encasement library. The sandbox
itself takes about 135 ms to initialize and the encasement
library takes another 38 ms. In addition, each security layer
takes time to initialize, as the dispatch method must wrap
the functions and objects necessary to maintain containment
between each security layer. We also found it takes 2.5 ms to
initialize a security layer that adds a noop function and then
dispatches the next security layer. We believe this overhead
is acceptable given that Java (1.6.0 20) and Python (2.6.1)
start in 170 ms and 17 ms respectively.
Per-use Cost. The second type of performance cost is
incurred whenever a security layer is crossed2. Table 2 shows
the cost for our general encasement library implementation,
which inspects and validates the contract of a capability at
run time, and a customized version which performs the same
operations but is optimized ahead of time.
In each case,
the type of the arguments or exceptions that the function
raises has a slight performance impact, but the costs stay
roughly within the same order of magnitude. This means
that the type of contract does not signiﬁcantly impact the
performance of the encasement library for arguments and
exceptions of a small size.
To evaluate overall performance, Table 2 shows that the
cost of a general contract veriﬁcation implementation is ap-
proximately an order of magnitude higher than that of a
customized version. However, the general contract veriﬁca-
tion implementation is much easier to verify for correctness
2Note that a security layer which does not interpose on a
call, does not impact its running time.
and security properties than a customized version (hence we
use the general veriﬁcation in production). The performance
penalty is only paid when crossing the boundary between
two security layers. Each security layer that interposes on a
call typically performs functionality that is much more ex-
pensive than this. To put these numbers in perspective, a
function call is about an order of magnitude cheaper than
a customized security layer crossing. However, in Python a
function call performs no type checking or other validation
so the native mechanisms are clearly inadequate to provide
security isolation. The most appropriate security compar-
ison is with a local RPC, which provides the same sort of
security guarantees as a security layer. A local RPC us-
ing XML-RPC is three orders of magnitude more expensive
than crossing the security layer boundary using our general
contract veriﬁcation implementation.
7.3 Memory Overhead
In addition to consuming additional CPU when using mul-
tiple security layers, each security layer also incurs a mem-
ory cost. This cost is due to the space needed by the con-
tracts, the copying of mutable arguments, and the wrappers
needed for security layer crossings. In our experiments on
a system with 64-bit memory addresses, each security layer
consumes about 19 KB of memory. The encasement library
consumes an additional 1 MB of memory. Our experience
has been that these memory overheads are acceptable, even
on memory-limited devices such as mobile phones.
8. RELATED WORK
We gained signiﬁcant inspiration from previous ideas and
systems. At the highest level, our security layer abstraction
is conceptually similar to layering in prior systems [19, 31].
We categorize and discuss closely related work below.
8.1 Object-Capability Based Languages and
Systems
Capabilities in computer systems have a long history [34].
In particular there has been a signiﬁcant amount of work on
object-capability languages [39, 38], which provide a prin-
cipled set of techniques for resolving the long studied prob-
lem of dividing an application into security contained com-
ponents [51, 29, 16]. Joe-E [38] is the only other object-
capability language that uses a subset of a widely used pro-
gramming language (Java). Current work on Joe-E is fo-
cused on application security and although the authors are
cognizant that the lack of security in the standard library
code of the Joe-E sandbox is a limitation of their technique,
it remains an open problem that has not been addressed [38].
In this work, we address this limitation using the security
layer primitive to isolate trusted code. Work by Stiegler and
Miller [53] on a capability subset of OCaml demonstrated
that object-capabilities do not have to impact the language’s
expressivity or performance. Their work has similar limita-
tions to Joe-E – privileged standard libraries are excluded
and all authorities from the safe version of the standard li-
brary must be removed.
Our system has many conceptual similarities to the Hydra
capability-based operating system [34] in that we provide
similar guarantees and mechanisms for protection, albeit for
a programming language sandbox instead of an operating
system. One signiﬁcant diﬀerence is that our protection
mechanism (encasement library) is not inside of our sand-
219box kernel. Thus a ﬂaw in the encasement library can at
most allow one to bypass security layers but will not allow
escape of the sandbox. Another diﬀerence is that we do not
allow a process to do a ’rights walk’ through the set of local
namespaces and utilize capabilities. This allows calls and
objects to pass both ways through security layers without
compromising security, which is important for callbacks and
notiﬁcations.
Cannon, et. al. [13] present a method for securing the
Python interpreter by implementing a set of resource re-
strictions. They modiﬁed the Python interpreter to prevent
read/write/execution of arbitrary ﬁles/modules. Using this
modiﬁed interpreter they added the ability to import exter-
nal modules, based on a user deﬁned whitelist. This leaves
the sandbox integrity up to the user, which can be problem-
atic. If a user whitelists a module, such as ’sys’, it would
give the interpreter ﬁle I/O capabilities which could allow
the modiﬁcation of the whitelist. In our implementation a
user is unable to add functionality to a security layer that
could produce undesired side eﬀects such as giving full access
to the ﬁle system.
8.2 Secure Language Subsets and Isolation
Prior work by Back, et. al. [4] encourages a single, explicit
separation between trusted and untrusted code, which they
call “the red line.” Our motivation is similar, but we propose
a separation between multiple security layers, with a small
kernel of truly trusted functionality.
Other work has gone into building secure language sub-
sets that restrict the allowed operations and functionality
in a language [37, 22, 58]. There are various subsets of
JavaScript, such as Facebook’s FBJS [22], Yahoo’s AD-
safe [58] and formal attempts from within the language [37].
While this is useful for isolating an untrusted program’s
namespace, it does not allow security functionality to be
composed in a manner similar to security layers.
Isolation techniques have been applied to other domains
such as extensions for web browsers [8], operating system
separation of processes [2], virtual machine separation of
operating systems [27, 6], constraining the functionality of
a process [36], or running mutually distrustful programs
within a single process [56, 17, 5]. In this work, we focus
on the converse problem – given a mechanism for isolation
between components, we leverage secure interposition and
interaction to construct secure standard libraries.
8.3 Interposition
In addition to interposition by language restrictions, some
researchers have also used conceptually similar mechanisms
to interpose on a process, usually at the system call layer [45,
44]. Current OS interposition mechanisms have several
drawbacks that make them undesirable in practice. For
example, they are OS speciﬁc, may require the user to in-
stall kernel patches, require the interposition code to be
trusted, incur signiﬁcant overhead, and are prone to subtle
errors [57, 26]. Our use of security layers is a lightweight
means of achieving similar functionality, while performing
interposition by a straightforward wrapping of function calls.
Another mechanism for interposition is to rewrite the user
program to contain references to the appropriate monitor
code [21, 55, 14]. Rewriting was used by Erlingsson, in his
work on Inlined Reference Monitors [21] and has also been
used in conjunction with aspect-oriented programming [55,
14]. We could have used similar techniques to add secu-
rity checks, but decided against performing modiﬁcation of
source code because of the diﬃculties in asserting the cor-
rectness of these techniques. We believe that using separate
namespaces makes it much easier for a programmer to rea-
son about the behavior and correctness of a security layer.
8.4 System Call Filtering
There has also been a signiﬁcant amount of work on sys-
tem call ﬁltering as a mechanism to secure existing system
call interfaces [24, 28, 1, 9]. While the low-level mechanisms
are similar to our approach, prior work focuses on securing
an existing system call interface, while we leverage our ﬂex-
ibility to deﬁne the interface. This led us to make diﬀerent
design decisions.
In [24], Fraser, et. al. describe a technique for secur-
ing commercial oﬀ-the-shelf software with the use of generic
wrappers. Their implementation uses a form of tagging to
categorize system calls that a user-deﬁned wrapper should
be applied to. One type of tag that is used is a parameter
tag, which is similar to our use of contracts. Parameter tags
may be used to generate wrapper functions that perform ar-
gument copying (including deep copies of structures). One
important diﬀerence is that they rely solely on static analysis
of these tags which may limit the ﬂexibility of the system.
Systems like Janus [28], MAPbox [1] and Tron [9] apply a
wrapper function to system call contracts. These wrappers
indicate acceptance or denial of calls. In our system, instead
of restricting calls only via contracts, the resulting security
layer actually executes code and so may directly return or
raise an exception.
8.5 Distributed Virtual Machine
G¨un Sirer, et. al. [52] propose a distributed virtual ma-
chine (DVM) architecture and use it to decompose the JVM
into a set of system services, which are oﬄoaded from the
client machine. This reduces veriﬁcation overhead, improves
security through physical isolation, and allows a system
administrator to verify code and enforce a security pol-
icy across all machines that they administer. Our use of
security layers has similar beneﬁts without requiring a cen-
tralized component. In our model, similar functionality is
provided by dynamic policy loading (see Section 6.3).
8.6 Information Flow Control
There has been a lot of work on information ﬂow control
both in operating systems [20, 59] and programming lan-
guages [40, 50, 10, 15]. These techniques are used to tag and
track data as it moves through an application. This work has
features like isolation and control of interfaces that is also
used in our work. Information ﬂow techniques are compli-
mentary but orthogonal to our use of security layers. Secu-
rity layers focus on the capabilities and call semantics across
security boundaries instead of tracking data ﬂow. The most
related information ﬂow control work is Wedge [10]. Wedge
is a system to modularize application logic so that it does
not leak sensitive user information. Wedge focuses primarily
on memory tagging, although it does utilize SELinux poli-
cies [36] to limit the set of allowed system calls. Wedge’s no-
tion of callgates could be used as a building block for system
call interposition, but requires kernel support and at least
one kernel crossing per call. Security layers do not have
Wedge’s memory tagging functionality, but security layers
220are more lightweight than Wedge, perform boundary check-
ing, and require no kernel support.
9. CONCLUSION
In this work we designed, implemented, and evaluated se-
curity layers – a mechanism to isolate and transparently in-
terpose on standard libraries in a programming language
sandbox. Security layers make it possible to push library
functionality out of the sandbox kernel, thereby helping to
mitigate the impact of bugs in libraries. As a result, our
Python-based sandbox maintains containment of application
code despite bugs in the standard library implementation.
To evaluate our design we examined a set of known JVM
security bugs and found that security layers would likely
prevent at least 6 of the 8 applicable bugs that led to ar-
bitrary code execution. Security layers also help to protect
against vulnerabilities that led to arbitrary ﬁle reads, sand-
box crashes, and other faults.
Our experience with a 20 month sandbox deployment
across thousands of nodes has been overwhelmingly positive.
Security layers allowed users to add security functionality,
without increasing the risk of sandbox escape. We have
used security layers to enforce network communication poli-
cies, log forensic information, and perform other operations,
without adding any code to the sandbox kernel. In addition,
when optimized for performance, security layers incur a per-
formance penalty that is within an order of magnitude of a
function call. Given the security and functionality beneﬁts
of security layers, we feel that this mechanism incurs an
acceptable performance penalty and is broadly applicable,
meriting consideration in the design of any sandbox.
Acknowledgments
We would like to thank the large number of people who
helped to signiﬁcantly improve this paper. We appreciate
the feedback and discussions with Tadayoshi Kohno, Wenjun
Hu, Mark Miller, Marc Stiegler, Adam Barth, and Adrian
Mettler. We are also grateful to our shepherd Trent Jaeger
and the anonymous reviewers for their valuable feedback.
This material is based upon work supported by the Na-
tional Science Foundation under CNS-0834243. Any opin-
ions, ﬁndings, and conclusions or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reﬂect the views of BBN Technologies, Corp., the
GENI Project Oﬃce, or the National Science Foundation.
10. REFERENCES
[1] A. Acharya and M. Raje. MAPbox: Using
Parameterized Behavior Classes to Conﬁne Untrusted
Applications. In SSYM’00: Proceedings of the 9th
conference on USENIX Security Symposium, Berkeley,
CA, USA, 2000. USENIX Association.
[2] M. Aiken, M. F¨ahndrich, C. Hawblitzel, G. Hunt, and
J. Larus. Deconstructing process isolation. In
Proceedings of the 2006 workshop on Memory system
performance and correctness, page 10. ACM, 2006.
[3] D. P. Anderson, J. Cobb, E. Korpela, M. Lebofsky,
and D. Werthimer. SETI@home: An experiment in
public-resource computing. Commun. ACM,
45(11):56–61, 2002.
[4] G. Back and W. Hsieh. Drawing the red line in Java.
In HotOS’99, pages 116–121, 1999.
[5] G. Back, W. C. Hsieh, and J. Lepreau. Processes in
KaﬀeOS: isolation, resource management, and sharing
in Java. In OSDI’00, pages 23–23, Berkeley, CA, USA,
2000. USENIX Association.
[6] P. Barham, B. Dragovic, K. Fraser, S. Hand,
T. Harris, A. Ho, R. Neugebauer, I. Pratt, and