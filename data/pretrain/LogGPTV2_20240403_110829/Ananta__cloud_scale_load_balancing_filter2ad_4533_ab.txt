N + 1 redundancy model with auto-recovery, and the load balanc-
ing service must degrade gracefully in the face of failures.
Any Service Anywhere: In our cloud, applications are gener-
ally spread over multiple layer-2 domains and sometimes even span
multiple data centers. The load balancer should be able to reach
DIPs located anywhere on the network. Traditional load balancers
provide some of their functionality, e.g., NAT, to DIPs only within
a layer-2 domain. This fragments the load balancer capacity and
makes them unusable in layer-3 based architectures.
Tenant Isolation: A multi-tenant load balancer is shared by
thousands of services. It is critical that DoS attacks on one ser-
vice do not affect the availability of other services. Similarly, an
abusive service should not be able to affect the availability of NAT
for other services by creating large number of outbound connec-
tions. In our cloud environment, we often see services with large
number of outbound NAT connections due to bugs and poor appli-
cation design. Furthermore, when the load balancer is under load,
Figure 4: Components of a traditional load balancer. Typically,
the load balancer is deployed in an active-standby conﬁguration.
The route management component ensures that the currently active
instance handles all the trafﬁc.
it is important that the load balancer provides each service its fair
share of resources.
3. DESIGN
3.1 Design Principles
Scale-out In-network Processing: Figure 4 illustrates the main
components of a traditional load balancer. For each new ﬂow, the
load balancer selects a destination address (or source for SNAT)
depending on the currently active ﬂows and remembers that deci-
sion in a ﬂow table. Subsequent packets for that ﬂow use the state
created by the ﬁrst packet. Traditional NAT and load balancing al-
gorithms (e.g., round-robin) require knowledge of all active ﬂows,
hence all trafﬁc for a VIP must pass through the same load balancer.
This forces the load balancer into a scale-up model. A scale-up or
vertical scaling model is one where handling more bandwidth for
a VIP requires a higher capacity box.
Network routers, on the other hand, follow a scale-out model. A
scale-out or horizontal scaling model is one where more bandwidth
can be handled by simply adding more devices of similar capac-
ity. Routers scale out because they do not maintain any per-ﬂow
state that needs synchronization across routers and therefore one
can add or remove additional routers easily. Ananta design reduces
the in-network functionality needed for load balancing to be such
that multiple network elements can simultaneously process packets
for the same VIP without requiring per-ﬂow state synchronization.
This design choice is enabled because we can make certain as-
sumptions about our environment. One of the key assumptions is
that load balancing policies that require global knowledge, e.g.,
weighted round robin (WRR), are not required for layer-4 load bal-
ancing. Instead, randomly distributing connections across servers
based on their weights is a reasonable substitute for WRR. In fact,
weighted random is the only load balancing policy used by our load
balancer in production. The weights are derived based on the size
of the VM or other capacity metrics.
Ofﬂoad to End Systems: Hypervisors in end systems can al-
ready do highly scalable network processing, e.g., ACL enforce-
ment, rate limiting and metering. Ananta leverages this distributed
scalable platform and ofﬂoads signiﬁcant data plane and control
plane functionality down to the hypervisor in end systems. The hy-
pervisor needs to handle state only for the VMs hosted on it. This
design choice is another key differentiator from existing load bal-
209Figure 5: The Ananta Architecture. Ananta consists of three
components — Ananta Manager, Ananta Mux and Host Agent.
Each component is independently scalable. Manager coordinates
state across Agents and Muxes. Mux is responsible for packet for-
warding for inbound packets. Agent implements NAT, which allows
all outbound trafﬁc to bypass Mux. Agents are co-located with des-
tination servers.
ancers. While on one hand it enables natural scaling with the size of
the data center; on the other hand, it presents signiﬁcant challenges
in managing distributed state across all hosts and maintaining avail-
ability during failures of centralized components.
3.2 Architecture
Ananta is a loosely coupled distributed system comprising three
main components (see Figure 5) — Ananta Manager (AM), Multi-
plexer (Mux) and Host Agent (HA). To better understand the details
of these components, we ﬁrst discuss the load balancer conﬁgura-
tion and the overall packet ﬂow. All packet ﬂows are described
using TCP connections but the same logic is applied for UDP and
other protocols using the notion of pseudo connections.
3.2.1 VIP Conﬁguration
The load balancer receives a VIP Conﬁguration for every VIP
that it is doing load balancing and NAT for. A simpliﬁed VIP con-
ﬁguration is shown in Figure 6. An Endpoint refers to a speciﬁc
transport protocol and port on the VIP that is load balanced to a set
of DIPs. Packets destined to an Endpoint are NAT’ed to the DIP
address and port. SNAT speciﬁes a list of IP addresses for which
outbound connections need to be Source NAT’ed with the VIP and
an ephemeral port.
3.2.2 Inbound Connections
Figure 7 shows how packets destined for a VIP are load bal-
anced and delivered to the DIP of a VM. When a VIP is con-
ﬁgured on Ananta, each Mux advertises a route to its ﬁrst-hop
router announcing itself as the next hop for that VIP1. This causes
the routers to distribute packets destined for the VIP across all
the Mux nodes based on Equal Cost MultiPath Routing Protocol
(ECMP) [25] (step 1). Upon receiving a packet, the Mux chooses
a DIP for the connection based on its load balancing algorithm, de-
scribed later in this section. It then encapsulates the received packet
1In reality, routes are advertised for VIP subnets due to small rout-
ing tables in commodity routers but the same logic applies.
Figure 6: JSON representation of a simple VIP Conﬁguration.
Figure 7: Load Balancing for Inbound Connections.
using IP-in-IP protocol [18] setting the selected DIP as the destina-
tion address in the outer header (step 2). It then sends it out using
regular IP routing at the Mux (step 3). The Mux and the DIP do not
need to be on the same VLAN, they just need to have IP (layer-3)
connectivity between them. The HA, located on the same physi-
cal machine as the target DIP, intercepts this encapsulated packet,
removes the outer header, and rewrites the destination address and
port (step 4) and remembers this NAT state. The HA then sends the
rewritten packet to the VM (step 5).
When the VM sends a reply packet for this connection, it is in-
tercepted by the HA (step 6). The HA does a reverse NAT based on
the state from step 4 and rewrites the source address and port (step
7). It then sends the packet out to the router towards the source of
this connection. The return packet does not go through the Mux at
all, thereby saving packet processing resources and network delay.
This technique of bypassing the load balancer on the return path is
known as Direct Server Return (DSR). Not all packets of a connec-
tion would end up at the same Mux, however all packets for a single
connection must be delivered to the same DIP. Muxes achieve this
via a combination of consistent hashing and state management as
explained later in this section.
3.2.3 Outbound Connections
A unique feature of Ananta is a distributed NAT for outbound
connections. Even for outbound connections that need source NAT
(SNAT), Ananta ensures that outgoing packets do not need to go
through Mux. Figure 8 shows how packets for an outbound SNAT
Figure 8: Handling Outbound SNAT Connections.
210connection are handled. A VM sends a packet containing its DIP
as the source address, portd as the port and an external address as
the destination address (step 1). The HA intercepts this packet and
recognizes that this packet needs SNAT. It then holds the packet
in a queue and sends a message to AM requesting an externally
routable VIP and a port for this connection (step 2). AM allocates
a (V IP, ports) from a pool of available ports and conﬁgures each
Mux with this allocation (step 3). AM then sends this allocation to
the HA (step 4). The HA uses this allocation to rewrite the packet
so that its source address and port are now (V IP, ports). The HA
sends this rewritten packet directly to the router. The return pack-
ets from the external destination are handled similar to inbound
connections. The return packet is sent by the router to one of the
Mux nodes (step 6). The Mux already knows that DIP 2 should
receive this packet (based on the mapping in step 3), so it encap-
sulates the packet with DIP 2 as the destination and sends it out
(step 7). The HA intercepts the return packet, performs a reverse
translation so that the packet’s destination address and port are now
(DIP, portd). The HA sends this packet to the VM (step 8).
3.2.4 Fastpath
In order to scale to the 100s of terabit bandwidth requirement of
intra-DC trafﬁc, Ananta ofﬂoads most of the intra-DC trafﬁc to end
systems. This is done by a technique we call Fastpath. The key
idea is that the load balancer makes its decision about which DIP
a new connection should go to when the ﬁrst packet of that con-
nection arrives. Once this decision is made for a connection it does
not change. Therefore, this information can be sent to the HAs on
the source and destination machines so that they can communicate
directly. This results in the packets being delivered directly to the
DIP, bypassing Mux in both directions, thereby enabling commu-
nication at full capacity supported by the underlying network. This
change is transparent to both the source and destination VMs.
To illustrate how Fastpath works, consider two services 1 and 2
that have been assigned virtual addresses V IP 1 and V IP 2 respec-
tively. These two services communicate with each other via V IP 1
and V IP 2 using the algorithms for load balancing and SNAT de-
scribed above. Figure 9 shows a simpliﬁed version of packet ﬂow
for a connection initiated by a VM DIP 1 (belonging to service 1)
to V IP 2. The source host of DIP 1 SNATs the TCP SYN packet
using V IP 1 and sends it to V IP 2 (step 1). This packet is de-
livered to a Mux2, which forwards the packet towards destination
DIP 2 (step 2). When DIP 2 replies to this packet, it is SNAT’ed
by the destination host using V IP 2 and sent to Mux1 (step 3). This
Mux uses its SNAT state and sends this packet to DIP 1 (step 4).
Subsequent packets for this connection follow the same path.
For Fastpath, Ananta conﬁgures Mux with a set of source and
destination subnets that are capable of Fastpath. Once a connec-
tion has been fully established (e.g., TCP three-way handshake has
completed) between V IP 1 and V IP 2, Mux2 sends a redirect mes-
sage to V IP 1, informing it that the connection is mapped to DIP 2
(step 5). This redirect packet goes to a Mux handling V IP 1, which
looks up its table to know that this port is used by DIP 1. Mux1
then sends a redirect message towards DIP 1 and DIP 2 (steps 6
and 7 respectively). HA on the source host intercepts this redirect
packet and remembers that this connection should be sent directly
to DIP 2. Similarly HA on the destination host intercepts the redi-
rect message and remembers that this connection should be sent to
DIP 1. Once this exchange is complete, any future packets for this
connection are exchanged directly between the source and destina-
tion hosts (step 8).
There is one security concern associated with Fastpath – a rogue
host could send a redirect message impersonating the Mux and hi-
Figure 9: Fastpath Control and Data Packets. Routers are not
shown for brevity. Starting with step 8, packets ﬂow directly
between source and destination hosts.
jack trafﬁc. HA prevents this by validating that the source address
of redirect message belongs to one of the Ananta services in the
data center. This works in our environment since the hypervisor
prevents IP spooﬁng. If IP spooﬁng cannot be prevented, a more
dynamic security protocol such as IPSEC can be employed.
3.3 Mux
The Multiplexer (Mux) handles all incoming trafﬁc. It is respon-
sible for receiving trafﬁc for all the conﬁgured VIPs from the router
and forwarding it to appropriate DIPs. Each instance of Ananta has
one or more sets of Muxes called Mux Pool. All Muxes in a Mux
Pool have uniform machine capabilities and identical conﬁguration,
i.e., they handle the same set of VIPs. Having a notion of Mux Pool
allows us to scale the number of Muxes (data plane) independent
of the number of AM replicas (control plane).
3.3.1 Route Management
Each Mux is a BGP speaker [20]. When a VIP is conﬁgured on
Ananta, each Mux starts advertising a route for that VIP to its ﬁrst-
hop router with itself as the next hop. All Muxes in a Mux Pool are
equal number of Layer-3 network hops away from the entry point
of the data center. This ensures that the routers distribute trafﬁc for
a given VIP equally across all Muxes via the Equal Cost MultiPath
(ECMP) routing protocol [25]. Running the BGP protocol on the
Mux provides automatic failure detection and recovery. If a Mux
fails or shuts down unexpectedly, the router detects this failure via
the BGP protocol and automatically stops sending trafﬁc to that
Mux. Similarly, when the Mux comes up and it has received state
from AM, it can start announcing the routes and the router will start
forwarding trafﬁc to it. Muxes use the TCP MD5 [13] protocol for
authenticating their BGP sessions.
3.3.2 Packet Handling
The Mux maintains a mapping table, called VIP map, that deter-
mines how incoming packets are handled. Each entry in the map-
ping table maps a VIP endpoint, i.e., three-tuple (VIP, IP protocol,
port), to a list of DIPs. The mapping table is computed by AM and
sent to all the Muxes in a Mux Pool. When Mux receives a packet
from the router, it computes a hash using the ﬁve-tuple from packet
header ﬁelds — (source IP, destination IP, IP Protocol, source port,
destination port). It then uses this hash to lookup a DIP from the
list of DIPs in the associated map. Finally, it encapsulates [18] the
211packet with an outer IP header — with itself as the source IP and
the DIP as the destination IP — and forwards this packet to the DIP.
The encapsulation at the Mux preserves the original IP header