for multi-party computa-
[46] or
[38],
tion [30], [32], [33], instead adopt the asymptotically worse
bitonic sort [6] which costs O(n log2 n). Given our evaluation
results, Path Oblivious Sort should now become the scheme
of choice in either a cloud-outsourcing or multi-party compu-
tation setting for almost all parameters we care about.
Ofﬂine ORAM. Boyle and Naor [7] show that given a RAM-
model oblivious sorting algorithm that consumes T (n) time
to sort n elements, one can construct an ofﬂine ORAM with
T (N )/N blowup. In comparison with a standard (online)
ORAM, an ofﬂine ORAM must see all the requests upfront.
Thus, just like Jafargholi et al. [25], our work also implies
a statistically secure ofﬂine ORAM with O(log N ) blowup.
Jafargholi et al. [25] achieves this result too but assuming
that the CPU can cache super-logarithmic (in the security
parameter) number of words. We remove this extra assumption
and require only O(1) CPU-registers.
We stress that although logarithmic-overhead online ORAM
is also possible, the only known construction [4] for the online
setting requires computational assumptions. Our scheme is
statistically secure, and the statistically secure logarithmic-
overhead ofﬂine ORAM result is of incomparable nature to
known optimal online ORAM [4].
II. DEFINITIONS
We consider algorithms in the standard RAM model, where
word-level addition and bitwise boolean operations can be
accomplished in unit time (note that we need not assume
word-level multiplication in unit time). Like in the standard
RAM model, we assume that each memory word is capable
of storing its own index, i.e., if the RAM’s total memory
size is n, then each memory word is at least log n bits long.
A RAM program’s runtime is the number of CPU steps it
takes to complete the computation, its bandwidth or IO cost
is the number of words transmitted between the memory and
the CPU. In cases where the CPU may store super-constant
number of words and compute on them in a single time step,
the bandwidth of a RAM may be larger than the runtime.
A. Priority Queue
We assume that when the priority queue is ﬁrst initiated,
a constructor function is called which takes in a desired
security parameter λ. Afterwards, the priority queue supports
the following types of operations — below we deﬁne the
“programming interface” in the same way as that of an
insecure binary heap:
• ref ← Insert(k, v): insert a value v with the key k into
the priority queue, and return a reference (i.e., handle) to
the inserted element denoted ref.
• (k, v, ref) ← FindMin(): return the item whose key is
the smallest (henceforth called the minimum item) in the
priority queue without deleting it.
• Delete(ref): delete the item with the reference ref from
• (k, v) ← ExtractMin(): remove and return the mini-
the priority queue.
mum item from the priority queue.
Authorized licensed use limited to: Auckland University of Technology. Downloaded on November 03,2020 at 00:37:42 UTC from IEEE Xplore.  Restrictions apply. 
844
A
Given the above operations, we can support DecreaseKey
and IncreaseKey by calling Delete followed by Insert.
For simplicity we deﬁne only the above operations explicitly.
Remark 1. Our deﬁnition of a priority queue matches the
standard interface provided by a popular binary heap — in
a standard binary heap, to call Delete and DecreaseKey
operations, the caller also needs to supply a reference handle
to the object being deleted or modiﬁed. The recent work by
Jafargholi et al. [25] in fact adopts a slightly non-standard
deﬁnition that is somewhat stronger than the standard binary
heap: in particular, their Delete operation takes in only the
item to be deleted but not its reference handle — for this
reason,
they need a k-wise independent hash function in
their constrction to calculate an element’s handle from the
item itself. By adopting the standard deﬁnition, we avoid the
reliance on a k-wise independent hash.
B. Oblivious Simulation of Priority Queue
We ﬁrst deﬁne a weaker notion of security (called type-
revealing security), where, informally speaking, only the re-
quest types are revealed to the adversary but not the contents of
the requests. For example, in our oblivious sorting application
later, this weaker notion is sufﬁcient. It is not difﬁcult to
strengthen our deﬁnition and construction to additionally hide
the request types too incurring only minimal additional cost —
we shall later elaborate on type-hiding security in Appendix D.
We deﬁne obliviousness through a simulation-based ap-
proach: we would like that the adversary’s observations in
the real-world be statistically close to an ideal-world in which
access patterns are simulated by a simulator that observes only
the request types but not the contents of the requests. In the
ideal world, the reference handles returned to the adversary
contain only the time at which the element was inserted where
time is measured in the number of requests so far. In the
real world, we may imagine that all additional ﬁelds of the
reference handle (besides the time) are encrypted or secret-
shared so the adversary cannot observe the actual contents
of the handle in the same way that actual data contents are
encrypted or secret shared and unobservable to the real-world
adversary (see also Remark 3).
functionality Fpq. An ideal-world priority queue,
Ideal
henceforth denoted Fpq, implements the above priority-queue
interface correctly. As mentioned, we assume that in the ideal
world, the reference ref of an element is simply the time at
which the element was inserted (where time is measured by
the number of operations so far)3
Oblivious simulation. Let N denote an upper bound on
the number of elements stored in the priority queue.
let
T ≥ N denote an upper bound on the total number of
priority queue operations. Let (λ, T ) be a function in λ and
T . Let PQ(1λ, N ) denote a priority queue algorithm whose
3Note that the adversary knows the index of the query; thus using time
as the ideal-world reference does not reveal additional information to the
adversary. Basically, in the ideal world, the adversary is able to later on ask
to delete an element by specifying the time at which it was inserted.
A
• RealPQ,A
execution is parametrized with a security parameter λ and the
maximum capacity N. Henceforth we often write PQ for short
omitting the default parameters. We say that PQ is a (1 − )-
oblivious simulation of Fpq iff there exists a stateful simulator
Sim, such that for any conforming, even computationally
unbounded adversary A, its views in the following experiments
(1λ, N, T ) have statistical
Ideal
distance at most (λ, T ) for any choice of λ, N, and T :
• Ideal
(1λ, N, T ) and RealPQ,A
(1λ, N ): A(N, T ) adaptively issues T priority-
queue queries. For each query Q, A receives not
from Fpq but also the outcome of
only the output
Sim(1λ, N, Q.type) where Q.type ∈ {insert, findmin,
delete, extractmin} extracts the query type from Q.
(1λ, N, T ): A(N, T ) interacts with a challenger
C which internally runs a copy of the real-world algorithm
PQ(1λ, N ) instantiated with the security parameter λ and
the maximum capacity N. Note that A recognizes only
ideal-world references that represent the time at which ele-
ments were inserted; however PQ recognizes the real-world
references whose format is determined by the algorithm
itself. Therefore, C acts as a man-in-the-middle between
A and PQ, it passes queries and answers back-and-forth
between A and PQ, translating the references from the
ideal-world format to the real-world format and vice versa.
At the end of each query, C also informs A the access
pattern incurred by the algorithm PQ in answering this
query (where by “access pattern” we mean the ordered
sequence consisting of every memory address accessed).
We require that a conforming adversary A(N, T ) must
satisfy the following:
1) it always submits a valid ideal-world reference in any
delete request,
the reference must correspond to
a time at which an insertion was made and moreover
the corresponding element inserted must still exist in the
priority queue; and
i.e.,
2) the total number of elements inserted into the priority
queue that have not been extracted must never exceed N.
Note also that the notion of oblivious simulation captures
correcteness and security requirements in a single simulation-
based deﬁnition — this approach is inspired by the standard
literature on multi-party computation.
Typically, we want the scheme’s failure probability (λ, T )
to be negligibly small
in λ as long as the total number
of requests T (λ) is polynomially bounded in λ. If this is
guaranteed, we say that the scheme “realizes an oblivious
priority queue” as formally deﬁned below.
Deﬁnition 1 (Oblivious priority queue). We say that PQ
realizes an oblivious priority queue iff PQ (1 − )-obliviously
simulates Fpq, and moreover,
the failure probability term
(λ, T ) is negligibly small in λ for any T that is polynomially
bounded in λ.
Remark 2 (On the failure probability’s dependence on T ).
Notice that in our deﬁnition of oblivious simulation above,
we allow the scheme’s failure probability (λ, T ) to depend
Authorized licensed use limited to: Auckland University of Technology. Downloaded on November 03,2020 at 00:37:42 UTC from IEEE Xplore.  Restrictions apply. 
845
on the number of requests T . In our scheme later, the failure
probability suffers from a union bound over T . This deﬁni-
tional approach is standard in the cryptography literature: it is
customary to let the scheme’s failure probability be dependent
on the adversary’s running time (which is lower bounded by T
in our case). In cryptography, a typical guarantee we aim for
is that as long as the adversary’s running time is polynomially
bounded in the security parameter λ — in our case, this implies
that T is polynomially bounded in λ — we would like the
scheme’s failure probability to be negligibly small in λ.
Remark 3.
In the security deﬁnition above, we assume
that the adversary can only observe the access patterns but
not the contents of the data being transmitted. In practice,
standard techniques such as encryption or secret-sharing can
be employed to hide data contents. Encryption is often used
in a single-server setting whereas secret sharing may be
employed in a multi-server setting [8], [31] or in multi-party
computation [23], [30], [42].
III. PATH OBLIVIOUS HEAP
In this section, we provide a formal description of our
oblivious priority queue algorithm. We ﬁrst present a version
instantiated from the non-recursive Path ORAM [39], we
then discuss how to modify the construction to use Circuit
ORAM’s eviction algorithm [42] to get tighter bounds on
the CPU’s private cache. This organization is not only to aid
understanding, but also because of the fact that the two variants
are each recommended for the cloud outsourcing and secure
multi-party computation settings respectively.
For simplicity, for the time being we assume that the priority
queue is preconﬁgured with an a-priori upper bound N on
the number of elements that it can hold, and an upper bound
T ≥ N on the total number of priority-queue requests. We
will later get rid of this known-T assumption in Section III-E
to support an unbounded number of queries.
We will use the notation λ to denote an appropriate secu-
rity parameter. We would like that over poly(λ) number of
requests, the priority queue’s security failure probability be
negligibly small in λ.
A. Data Structure
The primary data structure is a (non-recursive) Path-ORAM
binary tree with N leaves where N denotes an upper bound
on the number of entries stored in the priority queue.
Buckets. Each tree node is also called a bucket since it stores
an array of either real or dummy elements. Every non-root
bucket B in the tree can store a suitable constant number of
elements and each element is either real or dummy. The root
bucket’s size will be related to the security parameter — in
particular, later we will show that for the failure probability
to be negligly small for any polynomially bounded request
sequence, the root bucket’s capacity |Broot| must be set to
be super-logarithmic in the security parameter λ, We refer
the reader to Remark 4 for discussions on what is a suitable
constant to adopt for the non-root bucket size.
Real and dummy elements. Each real element is of the form
(k, v, ref), i.e., it not only contains a key-value pair denoted
(k, v), but also a reference ref := (pos, τ ) containing two
pieces of metadata:
1) a random position label pos ∈ {0, 1, . . . , N − 1} — this
random position label is chosen uniformly at random
when the element is inserted through an Insert oper-
ation; and
2) a timestamp τ remembering that this pair (k, v) was
inserted during the τ-th operation — later τ will be
included in inserted elements’ references to make sure
that the references are globally unique.
Henceforth we assume that a dummy element is of the form
(k = ∞, v = ⊥, ref = ⊥). In particular, a dummy element
has the maximum possible key.
Deﬁnition 2 (Path invariant [42]). We maintain exactly the
same path invariant as in Path ORAM [39]: a real element
with the position label pos must reside somewhere along the
path from the root to the leaf node identiﬁed by pos.
Subtree minimum. Additionally, each bucket B in the tree
is always tagged with its subtree-min M := (k, v, (pos, τ )),
which denotes the minimum element contained in the subtree
rooted at B. Henceforth, if two elements have the same key
k, we will break ties using the timestamp ﬁeld τ.
Remark 4 (Non-root bucket capacity: provable vs. practical
choices). For our mathematical proofs to go through, we need
to set non-root bucket’s capacity to be 5 or larger. However, for
practical implementation, we recommend a practical choice of
2 for the non-root bucket capacity. Note that this is customary
in the tree-based ORAM line of work — in all earlier
works such as Path ORAM [39] and Circuit ORAM [42], the
provable stochastic bounds are not tight in the constant; and
through simulation, one can observe that even smaller bucket
sizes lead to exponentially sharp tail bounds. For example, in
Path ORAM [39] and Circuit ORAM [42], a bucket size of
4 or 5 is in the formal theorem statements, but these works
use a bucket size of 2 or 3 in implementation. How to further
tighten these constants in the stochastic proofs seems rather
challenging and has been left open since the beginning of this