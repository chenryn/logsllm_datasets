> 16
> 58.5
> 8.6
3.5
kLoCs
892
216
460
187
1200
22
34
93
N/A
24.5
1.3
0.2
33
9.1
23
of plaintext data through encryption and controls the amount
of information that can be sent out.
• Memory write operations. All memory writes, both direct
memory store and indirect register spill, are detected and
blocked. Additionally, software DEP is deployed so the code
cannot change itself. Also the control-ﬂow integrity (CFI)
policy, P5, prevents the attacker from bypassing the checker
with carefully constructed gadgets by limiting the control ﬂow
to only legitimate target addresses.
As such, possible ways of information leak to the outside of
the enclave are controlled. As proved by previous work [20],
[45]
the above-mentioned policies (P1-P5) guarantee the
property of conﬁdentiality. Furthermore the policy (P5) of
protecting return addresses and indirect control ﬂow transfer,
together with preventing writes to outside has been proved
to be adequate to construct the conﬁnement [45], [46]. So,
enforcement of the whole set of policies from P0 to P5 is
sound and complete in preventing explicit information leaks.
In the meantime, our current design is limited in side-channel
protection. We can mitigate the threats of page-fault based
attacks and exploits on L1/L2 cache once Hyper-threading is
turned off or HyperRace [40] is incorporated (P6). However,
defeating the attacks without triggering interrupts, such as
inference through LLC is left for future research.
With such protection, still our design cannot eliminate all
covert channels, which is known to be hard. However, it is
important to note that other SGX runtimes, including SCONE,
Graphene-SGX, Occlum, provide no such protection either. An
exception is Ryoan, which pads its enclave output to the same
size, as we do. However, it does not handle the leak from the
hardware-based channels.
B. Performance Evaluation
Here we discuss performance overhead of different level
protections DEFLECTION can provide. These settings include
just explicit memory write check (P1), both explicit memory
write check and implicit stack write check (P1+P2), all mem-
ory write and indirect branch check (P1-P5), and together with
side channel mitigation (P1-P6).
Testbed setup. In our research, we evaluated the performance
of our prototype and tested its code generation and code execu-
tion. All experiments were conducted on Ubuntu 18.04 (Linux
9
kernel version 4.4) with SGX SDK 2.5 installed on Intel Xeon
CPU E3-1280 with 64GB memory. Also we utilized GCC 5.4
to build the bootstrap enclave and the SGX application, and
the parameters ‘-fPIC’, ‘-fno-asynchronous-unwind-tables’, ‘-
fno-addrsig’, and ‘-mstackrealign’ to generate x86 binaries.
TABLE II: Performance overhead on nBench
Program Name
NUMERIC SORT
STRING SORT
BITFIELD
FP EMULATION
FOURIER
ASSIGNMENT
IDEA
HUFFMAN
NEURAL NET
LU DECOMPOSITION
P1
P1-P5
P1+P2
P1-P6
+5.18% +6.05% +6.79% +12.0%
+8.05% +10.2% +12.4% +18.4%
+6.11% +11.3% +15.5% +17.9%
+0.20% +0.27% +0.33% +5.36%
+2.48% +2.72% +2.89% +7.45%
+6.73% +15.6% +25.0% +39.8%
+2.34% +2.66% +3.13% +12.1%
+15.5% +16.6% +18.1% +21.3%
+13.8% +19.4% +20.2% +23.1%
+4.30% +7.03% +9.67% +22.6%
Performance on nBench. We instrumented all applications
in the SGX-nBench [47], and ran each testcase of the nBench
suites under a few settings, each for 10 times. Table II shows
the average execution time under different settings. Without
side channel mitigation (P1-P5), our prototype introduces an
0.3% to 25% overhead (on FP-emulation). Apparently, the
store instruction instrumentation alone (P1) does not cause
a large performance overhead, with the largest being 6.7%.
Also, when P1 and P2 are applied together, the overhead just
becomes slightly higher than P1 is enforced alone. The per-
formance overhead ﬂuctuates from application to application
since different instrumentations are applied. FP EMULATION
has much less memory write operations than others. And it has
rare indirect branches. Compared to it, ASSIGNMENT uses
a lot of function pointers, which leads to a relatively heavy
instrumentation overhead of enforcing P5. Besides, almost all
benchmarks in nBench perform well under the CFI check P5
(less than 4%) except for the Assignment (about 10% due to
its frequent memory access pattern).
Performance on real-world applications. We further eval-
uated our prototype on various real-world applications, in-
cluding personal health data analysis, personal ﬁnancial data
analysis, and Web servers. We implemented those macro-
benchmarks and measured the differences between their base-
line performance (without instrumentation) in enclave and the
performance of our prototype. We evaluated multiple settings
(input sizes) and reported the most representative results,
in ascending order across at least one order of magnitude.
The baseline results are measured on a pure loader, with no
security/privacy policies enforced.
• Sensitive genome data analysis. We implemented the
Needleman–Wunsch algorithm [48] that aligns two human
genomic sequences in the FASTA format [49] taken from
the 1000 Genomes project [50]. The algorithm uses dynamic
programming to compute recursively a two dimensional matrix
of similarity scores between subsequences; as a result, it takes
N 2 memory space where N is the length of the two input
sequences. We measured the sequence alignment program
execution time under the aforementioned settings. Figure 7
shows the performance of the sequence alignment algorithm
with different input lengths (x-axis). The overall overhead
(including all kinds of instrumentations) is no more than 20%
(with the P1 alone no more than 10%), when input size is
small (less than 200 Bytes). When input size is greater than
500 Bytes, the overhead of P1+P2 is about 19.7% while P1-P5
spends 22.2% more time than the baseline.
For sequence generation, Figure 8 shows the performance
when the output size (x-axis) varies from 1K to 500K
nucleotides. Enforcing P1 alone results in 5.1% and 6.9%
overheads when 1K and 100K are set as the output lengths.
When the output size is 200K, our prototype yields less than
20% overhead. Even when the side channel mitigation is
applied, the overhead becomes just 25%. With the increase of
processing data size, the overhead of the system also escalates;
however, the overall performance remains acceptable. Some-
times the differences between P1+P2 and P1-P5 seem slight
mainly because the instrumentations on indirect branches (P5)
are few. Meanwhile, the instrumentation to enforce P1/P2 can
be reused to enforce P3/P4 (via different boundaries). Thus,
the performance overhead caused by P3/P4 is negligible (when
P1/P2 are already enforced).
• Personal credit score analysis. In our study, we implemented
a BP neural network-based credit scoring algorithm [51] that
trains a model to calculate user’s credit scores. The model
was trained on 10000 records and then used to make pre-
diction (i.e., output a conﬁdence probability) on different test
cases. As shown in Figure 9, on 1000 and 10000 records,
enforcement of P1-P5 would yields around 15% overhead.
While processing more than 50000 records, the overhead of
the full check does not exceed 20%. The overhead of P1-P6
does not exceed 10% when processing 100K records.
• HTTPS server. We built an HTTPS server in enclave using
the mbed TLS library [52]. The case of HTTPS server is
to show that DEFLECTION is capable of handling multiple
clients and it outperforms other solutions when the data size
is increasing. A client executes a stress test tool - Siege [53]
- on another host in an isolated LAN. Siege was conﬁgured
to send continuous HTTPS requests (with no delay between
two consecutive ones) to the web server for 10 minutes.
We measured its performance in the presence of different
concurrent connections to understand how our instrumented
HTTPS server implementation would perform.
Figure 10 shows the response times and throughput when
all policies are applied to the HTTPS server benchmark. When
the concurrent connections are less than 75, the instrumented
HTTPS server has similar performance of the in-enclave
https server without instrumentation. When the concurrency
increases to 100, the performance goes down to some extent.
While after the concurrency increases to 150, the response
time of instrumented server goes up signiﬁcantly. On average,
enforcing P1-P6 results in 14.1% overhead in the response
time. As for throughput, when the number of the concurrent
connections is between 75 and 200, the overhead is less than
10%. These experiments on realistic workloads show that all
policies, including side-channel mitigation, can be enforced at
only reasonable cost.
Performance comparison on HTTPS server. Here we com-
pare the performance overheads induced by existing shielding
runtimes with our solution. Since Occlum has not integrated
the SFI feature in its latest version [44] and Graphene-SGX
does not support our security policies, we cannot get their
performance details to compare against ours when policy-
enforcing instrumentations are added. In our study, we ran an
HTTPS server within those runtimes. As expected, their per-
formance is affected by the workload, sizes of ﬁles requested
from the server. As shown in Figure 11, unprotected Graphene-
SGX has the best transfer rate with relatively small ﬁles.
However, with the size growing, DEFLECTION outperforms
both runtimes (77% of running the server on the native Linux),
even when our approach implements security policies (P0-P5)
while these runtimes do not.
VII. DISCUSSION
Supporting other side/covert channel defenses. The frame-
work of our system is highly ﬂexible, which means assembling
new policies into current design can be very straightforward. In
Section IV-C, we talked about policy enforcement approaches
for side channel resilience. It demonstrated that our framework
can take various side channel mitigation approaches to gener-
ate code carried with proof. Besides AEX based mitigations
which we learnt from Hyperrace [40], others [41], [54]–[59]
can also be transformed and incorporated into the design,
speciﬁcally for mitigating cache timing, memory bus tim-
ing [60], and other timing channels. ORAM [61], [62] can also
be integrated to DEFLECTION as a policy, to relieve memory
access based side- or covert- channel leakage to some extend.
Additionally, policies such as on-demand aligning/blurring
processing time can be added for preventing processing-time
based covert channels [63]. Even though new attacks have
been kept being proposed and there is perhaps no deﬁnitive
and practical solutions to all side/covert channel attacks, we
believe eventually some efforts can be integrated in our work,
even using SGXv2 [64].
Supporting multi-threading. SGX supports multi-threaded
execution. To concurrently service many clients, policies such
as isolating each thread’s private memory and setting read-only
permissions on cross-thread shared memory can be enforced.
Multi-threading could introduce serious bugs [65]. The proof
enforcement of CFI may suffer from a time of check to time of
use (TOCTOU) problem [66]. To cope with that, we can make
all CFI metadata to be kept in the register or hardware [67]
instead of in memory, and guarantee that the instrumented
proof could not be modiﬁed by any threads [68].
VIII. RELATED WORK
Secure computing using SGX. Many existing works pro-
pose using SGX to secure cloud computing systems, e.g.,
VC3 [46], TVM [69], by using sand-boxing, containers [70],
and others [71], [72]. In-enclave JVM interpreter is also a good
10
Fig. 7: Sequence alignment
Fig. 8: Sequence generation
Fig. 9: Credit scoring
deployment. Secondly, DEFLECTION has a smaller size of
TCB. Other than importing Zydis and PyVEX (in Python)
to be the disassembler and veriﬁer, we shrank and modiﬁed
Capstone (in C) to implement our smaller disassembler and
veriﬁer. Thirdly, DEFLECTION can mitigate some side/covert
channel leaks while others provide no such protection.
Code privacy. Code secrecy is an easy to be ignored but
very important issue [78]. TEEshift [12], DynSGX [79] and
SGXElide [80] both make possible that developers execute
their code privately in public cloud environments, enabling
developers to better manage the scarce memory resources.
However, they only care about the developer’s privacy but
ignore the conﬁdentiality of data belonging to users.
Conﬁdentiality veriﬁcation of enclave programs. With for-
mal veriﬁcation tools, Moat [20] and its follow-up works [45]
verify if an enclave program has the risk of data leakage. The
major focus of them is to verify the conﬁdentiality of an SGX
application outside the enclave formally and independently.
Although it is possible that the veriﬁcation could be performed
within a “bootstrap enclave”,
the TCB would include the
IR level language (BoogiePL) interpreter [81] and a theorem
prover [33]. Moreover, neither of them can discharge the large
overhead introduced by instruction modeling and assertion
proving when large-scale real-world programs are veriﬁed.
IX. CONCLUSION
In this paper we proposed DEFLECTION, which allows the
user to verify the code provided by untrusted parties without
undermining their privacy and integrity. Meanwhile, we in-
stantiated the design of a code generator and a code consumer
(the bootstrap enclave) - a lightweight PCC-type framework.
Our work does not use formal certiﬁcate to validate the loaded
private binary, but leverage data/control ﬂow analysis to fulﬁll
the goal of verifying if a binary has such data leakage, allowing
our solution to scale to real-world software.
ACKNOWLEDGMENT
We would like to express our sincere thanks to our shep-
herd R¨udiger Kapitza and the anonymous reviewers for their
valuable feedback to help us improve the paper. This research
is supported in part by NIH R01HG010798 and NSF CNS-
1838083. Wenhao Wang was partially supported by National
Natural Science Foundation of China (Grant No.61802397).
Fig. 10: Performance on HTTPS server
Fig. 11: Performance comparison
choice [73]. These systems protect the enclave on untrusted
platform, as a result, they either do not protect the code privacy
or they consider a one-party scenario, i.e., the code and data
needed for the computation are from the same participant.
Data conﬁnement with SFI. Most related to our work are