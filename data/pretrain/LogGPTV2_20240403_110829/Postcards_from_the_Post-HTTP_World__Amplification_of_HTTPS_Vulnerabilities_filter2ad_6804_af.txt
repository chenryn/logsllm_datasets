### SSL Pulse and TLS Vulnerabilities

The SSL Pulse project [64] publishes monthly reports on the prevalence of certain attacks and feature support. Novel variants of known vulnerabilities, such as the ROBOT attack [14] and CBC oracles discovered via the TLS-Attacker fuzzing tool [80], have been identified. Summaries of known TLS vulnerabilities have been provided by Levillain et al. [54, 55] and the IETF [75]. Meyer and Schwenk [56] have summarized lessons learned from attacks known before 2013.

None of these studies systematically discuss and quantify web application security issues. However, several research papers have examined the risks associated with partial adoption of HTTP on HTTPS websites. For example, [22] conducted a large-scale analysis of mixed-content websites, [51] analyzed the state of HSTS deployment, and [77] studied the threats posed by the leakage of cookies over HTTP channels. Additionally, some papers have quantified the impact of incorrect TLS implementations on email infrastructure security [30, 43].

This paper contributes to the growing body of research on large-scale security evaluations of the web [86]. While several studies have analyzed the security of the HTTPS certificate ecosystem [31, 44, 87], we are not aware of any scientific publication that quantifies the potential harm caused by cryptographic weaknesses in TLS implementations to web application security. Other important aspects of web application security, such as the dangers of remote JavaScript inclusion [61], the prevalence of DOM-based XSS [53], and the state of CSP adoption [18, 19, 91], have also been investigated through large-scale measurements.

### Ethics and Limitations

Due to legal and ethical considerations, our analysis of TLS vulnerabilities was limited to an unintrusive scan using publicly available tools. The exploitability of the discovered vulnerabilities was determined through a systematic analysis of the tool outputs, based on an extensive review of existing literature on TLS attacks (summarized in the attack trees in Section III). All tested vulnerabilities have been previously published at major computer security conferences and/or received significant coverage in the hacking community. They have all been shown to be exploitable in the wild, requiring a feasible amount of computational power. Since we did not conduct any active attack attempts, it is possible that the reported vulnerabilities may not be practically exploitable, for example, due to the deployment of anomaly detection systems. However, the effectiveness of such mitigations is difficult to assess, and fixing the vulnerabilities would be preferable from a security perspective.

Our study does not aim to provide an exhaustive list of web application vulnerabilities but rather highlights significant security threats posed by vulnerable TLS implementations and quantifies their practical relevance. The use of heuristics in some parts of our experimental evaluation, such as session cookie detection, may introduce a bias. Better heuristics could improve the precision of the analysis, but they are unlikely to significantly change the overall findings given the large scale of the experiments. We manually confirmed some of the security issues to validate our methodology. We rechecked all the vulnerable sites mentioned in this paper in January 2019, and most of them had fixed the issues since our initial scan. We responsibly reported the discovered flaws to the remaining vulnerable sites, and only one responded dismissively, stating: "This case has no direct security impact, and we will not take immediate action or fix." In fact, we found little interest in TLS-related issues even in vulnerability reward programs, although the fact that many sites fixed the problems is promising in terms of awareness of the risks associated with incorrect HTTPS implementations.

### Summary and Perspective

While the use of HTTPS is essential for web application security, it is not a panacea, as flaws in the underlying TLS implementation can significantly affect application-layer security. Our evaluation reveals several concerning numbers, which we summarize here along with our perspective on the main findings.

Nearly 10% of the homepages of the crawled websites are compromisable, meaning a determined network attacker could gain active scripting capabilities on them. For approximately 25% of these pages, the security issue can be resolved by revising the cryptographic implementation. Unfortunately, the security of the other 75% of pages is downgraded by the inclusion of external scripts retrieved over insecure channels, making it challenging for web developers to accurately assess and fix cryptographic robustness issues. Since we only crawled homepages, our findings likely understate the true situation, as other webpages may include more insecure content. SRI (Subresource Integrity) is a potentially effective defense mechanism, but its adoption is minimal and suboptimal: only about 3% of the pages use SRI, and none of the attacks we found are actually prevented by the current SRI deployment.

Regarding web session security, we found that around 10% of the crawled websites are vulnerable to session hijacking attacks via cookie stealing, while more than 13% are susceptible to cookie forcing. The most concerning aspect of cookie security is the impact of related domains: a single security issue on a related-domain host can completely undermine session security, as these hosts can compromise both the confidentiality and integrity of session cookies. We also found that 10% of login pages are vulnerable to password theft.

Finally, cryptographic weaknesses in the TLS implementations of web trackers pose significant threats to user privacy. In our experimental analysis, we discovered that some prominent trackers inadvertently introduce security problems on a significant number of websites. The most alarming aspect is that a single vulnerable tracker can significantly harm user privacy if it is popular enough to be included on many different websites. For instance, one problem we found allows for user profiling on 142 websites, which can be increased to 968 websites with a more powerful variant of the attack.

We expect this bleak picture to improve as browsers and servers provide better support for TLS 1.3. Major browser vendors have already announced plans to deprecate TLS 1.0 and 1.1 in 2020 [8]. However, backward compatibility and slow adoption remain significant barriers to web security improvements, so we anticipate that older TLS versions will persist for several years. This paper serves as a cautionary tale of the threats posed by these older versions, and we plan to provide the toolchain developed for our study as a web application to help developers mitigate these threats.

### Acknowledgments

Riccardo Focardi and Marco Squarcina were partially supported by the CINI FilieraSicura project, funded by CISCO Systems Inc. and Leonardo SpA. Marco Squarcina was also partially supported by the European Research Council (ERC) under the European Union's Horizon 2020 research (grant agreement No 771527-BROWSEC). Matus Nemec was partially supported by the Czech Science Foundation under project GA16-08565S. Marco Squarcina completed most of the work on this project while he was a postdoctoral researcher at Caâ€™ Foscari University.