# Automatically Detecting Vulnerable Websites Before They Turn Malicious

**Authors: Kyle Soska and Nicolas Christin, Carnegie Mellon University**

**Publication: Proceedings of the 23rd USENIX Security Symposium, August 20â€“22, 2014, San Diego, CA. ISBN 978-1-931971-15-7. Open access sponsored by USENIX.**

**Abstract:**

Recent advancements in research have enabled the development of systems that can accurately determine the maliciousness of a target website. However, these systems are inherently reactive. In this paper, we propose a novel classification system designed to predict whether a currently benign website will become malicious in the future. We employ data mining and machine learning techniques, with a key feature being the automatic extraction of relevant features from acquired data. This allows for the rapid detection of new attack trends. Our implementation, evaluated on a dataset of 444,519 websites containing 4,916,203 webpages, demonstrates good detection accuracy over a one-year horizon.

**1. Introduction**

Online criminal activities encompass a wide range of tactics, including spam emails, drive-by downloads, and distributed denial-of-service (DDoS) attacks. Research into end-host malware, which aims to take control of victims' computers, has been extensive. More recently, studies have focused on "webserver malware," where attackers inject code onto machines running web servers. Unlike end-host malware, webserver malware often exploits outdated or unpatched content management systems (CMS) to participate in search-engine poisoning or to act as a delivery server for malware.

Webserver infections are prevalent, with 80% of websites hosting malicious content in 2012 being compromised webservers, according to the Sophos security threat report. These compromised webservers are increasingly used to promote counterfeit goods, supplanting spam as a primary advertising method.

Most existing work on identifying webserver malware focuses on detecting active infections, which is reactive. Our contribution is a proactive methodology to identify webservers at high risk of becoming malicious before they are compromised. This is valuable for search engines, blacklist operators, and site administrators, who can take preventive measures.

**2. Background and Related Work**

Webserver malware has gained significant attention. Studies like those by Levchenko et al. [21] and others [17, 19, 20, 22] have characterized search-redirection attacks, where compromised websites link to each other and associate with searches for illicit products. McCoy et al. [25] and Wang et al. [36] provide comprehensive measurements of web-based abuse, including the financial aspects of online pharmacies.

Recent papers have also focused on detecting compromised websites. For example, Invernizzi et al. [15] and Borgolte et al. [8] use automated methods to find recently compromised sites and unknown infection campaigns. Vasek and Moore [35] manually identified CMS usage and its correlation with website security, supporting the idea that website content can predict security outcomes.

Our approach builds on these techniques, using machine learning and data mining to predict future malicious behavior. We adapt the data extraction algorithm of Yi et al. [38] and use an ensemble of decision-tree classifiers, similar to Gao et al. [13].

**3. Classifying Websites**

**3.1 Desired Properties**

Our classifier must be efficient, interpretable, robust to imbalanced data, robust to missing features, and adaptive to changing threats. 

- **Efficiency:** The classifier should scale well with large datasets, using an online learning algorithm.
- **Interpretability:** Understanding the reasoning behind predictions is essential for informing website operators and detecting evolving risk factors.
- **Robustness to Imbalanced Data:** Given the rarity of malicious examples, the classifier must handle imbalanced datasets effectively, using metrics like ROC curves.
- **Robustness to Errors:** The classifier should handle missing or inconclusive features and noisy data, such as incomplete blacklists.
- **Adaptive:** The classifier should adapt to concept drift, learning from new and evolving threats.

**3.2 Learning Process**

Predicting future malicious behavior presents unique challenges. Since we cannot immediately verify predictions, we use past data to simulate future predictions. We assume a one-year horizon for training and evaluation. Ground truth data is obtained from blacklists and archived versions of websites, allowing us to check if a site became malicious within the specified time frame.

**Conclusion**

This paper introduces a novel classification system for predicting future malicious behavior of websites. By leveraging machine learning and data mining, our system provides a proactive approach to identifying and mitigating potential threats. Future work will focus on refining the model and expanding its application to a broader range of web environments.

**References:**

[1] Alexa Web Information Service (AWIS).
[2] Various other references as cited in the original text.