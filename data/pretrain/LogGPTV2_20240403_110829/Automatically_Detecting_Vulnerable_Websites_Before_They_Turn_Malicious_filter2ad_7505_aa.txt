title:Automatically Detecting Vulnerable Websites Before They Turn Malicious
author:Kyle Soska and
Nicolas Christin
Automatically Detecting Vulnerable Websites 
Before They Turn Malicious
Kyle Soska and Nicolas Christin, Carnegie Mellon University
https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/soska
This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXAutomatically Detecting Vulnerable Websites Before They Turn Malicious
Kyle Soska and Nicolas Christin
Carnegie Mellon University
{ksoska, nicolasc}@cmu.edu
Abstract
Signiﬁcant recent research advances have made it possi-
ble to design systems that can automatically determine
with high accuracy the maliciousness of a target website.
While highly useful, such systems are reactive by nature.
In this paper, we take a complementary approach, and at-
tempt to design, implement, and evaluate a novel classi-
ﬁcation system which predicts, whether a given, not yet
compromised website will become malicious in the fu-
ture. We adapt several techniques from data mining and
machine learning which are particularly well-suited for
this problem. A key aspect of our system is that the set
of features it relies on is automatically extracted from the
data it acquires; this allows us to be able to detect new
attack trends relatively quickly. We evaluate our imple-
mentation on a corpus of 444,519 websites, containing
a total of 4,916,203 webpages, and show that we man-
age to achieve good detection accuracy over a one-year
horizon; that is, we generally manage to correctly predict
that currently benign websites will become compromised
within a year.
1
Online criminal activities take many different forms,
ranging from advertising counterfeit goods through spam
email [21],
to hosting “drive-by-downloads” services
[29] that surreptitiously install malicious software (“mal-
ware”) on the victim machine, to distributed denial-of-
service attacks [27], to only name a few. Among those,
research on analysis and classiﬁcation of end-host mal-
ware – which allows an attacker to take over the vic-
tim’s computer for a variety of purposes – has been a
particularly active ﬁeld for years (see, e.g., [6, 7, 16]
among many others). More recently, a number of stud-
ies [8,15,20,22,36] have started looking into “webserver
malware,” where, instead of targeting arbitrary hosts for
compromise, the attacker attempts to inject code on ma-
chines running web servers. Webserver malware dif-
fers from end-host malware in its design and objectives.
Introduction
Webserver malware indeed frequently exploits outdated
or unpatched versions of popular content-management
systems (CMS). Its main goal is usually not to com-
pletely compromise the machine on which it resides, but
instead to get the victimized webserver to participate in
search-engine poisoning or redirection campaigns pro-
moting questionable services (counterfeits, unlicensed
pharmaceuticals, ...), or to act as a delivery server for
malware.
Such infections of webservers are particularly com-
mon. For instance, the 2013 Sophos security threat re-
port [33, p.7] states that in 2012, 80% of websites hosting
malicious contents were compromised webservers that
belonged to unsuspecting third-parties. Various measure-
ment efforts [20, 25, 36] demonstrate that people engag-
ing in the illicit trade of counterfeit goods are increas-
ingly relying on compromised webservers to bring traf-
ﬁc to their stores, to the point of supplanting spam as a
means of advertising [20].
Most of the work to date on identifying webserver
malware, both in academia (e.g., [8, 15]) and industry
(e.g., [3, 5, 14, 24]) is primarily based on detecting the
presence of an active infection on a website. In turn, this
helps determine which campaign the infected website is
a part of, as well as populating blacklists of known com-
promised sites. While a highly useful line of work, it is
by design reactive: only websites that have already been
compromised can be identiﬁed.
Our core contribution in this paper is to propose, im-
plement, and evaluate a general methodology to identify
webservers that are at a high risk of becoming malicious
before they actually become malicious. In other words,
we present techniques that allow to proactively identify
likely targets for attackers as well as sites that may be
hosted by malicious users. This is particularly useful for
search engines, that need to be able to assess whether
or not they are linking to potentially risky contents; for
blacklist operators, who can obtain, ahead of time, a list
of sites to keep an eye on, and potentially warn these
USENIX Association  
23rd USENIX Security Symposium  625
sites’ operators of the risks they face ahead of the actual
compromise; and of course for site operators themselves,
which can use tools based on the techniques we describe
here as part of a good security hygiene, along with prac-
tices such as penetration testing.
Traditional penetration testing techniques often rely
on ad-hoc procedures rather than scientiﬁc assessment
[26] and are greatly dependent on the expertise of the
tester herself. Different from penetration testing, our ap-
proach relies on an online classiﬁcation algorithm (“clas-
siﬁer”) that can 1) automatically detect whether a server
is likely to become malicious (that is, it is probably vul-
nerable, and the vulnerability is actively exploited in the
wild; or the site is hosted with malicious intent), and that
can 2) quickly adapt to emerging threats. At a high level,
the classiﬁer determines if a given website shares a set
of features (e.g., utilization of a given CMS, speciﬁcs of
the webpages’ structures, presence of certain keywords
in pages, ...) with websites known to have been mali-
cious. A key aspect of our approach is that the feature
list used to make this determination is automatically ex-
tracted from a training set of malicious and benign web-
pages, and is updated over time, as threats evolve.
We build this classiﬁer, and train it on 444,519
archives sites containing a total of 4,916,203 webpages.
We are able to correctly predict that sites will eventually
become compromised within 1 year while achieving a
true positive rate of 66% and a false positive rate of 17%.
This level of performance is very encouraging given the
large imbalance in the data available (few examples of
compromised sites as opposed to benign sites) and the
fact that we are essentially trying to predict the future.
We are also able to discover a number of content features
that were rather unexpected, but that, in hindsight, make
perfect sense.
The remainder of this paper proceeds as follows. We
review background and related work in Section 2. We de-
tail how we build our classiﬁer in Section 3, describe our
evaluation and measurement methodology in Section 4,
and present our empirical results in Section 5. We dis-
cuss limitations of our approach in Section 6 before con-
cluding in Section 7.
2 Background and related work
Webserver malware has garnered quite a bit of attention
in recent years. As part of large scale study on spam,
Levchenko et al. [21] brieﬂy allude to search-engine op-
timization performed my miscreants to drive trafﬁc to
their websites. Several papers [17, 19, 20, 22] describe
measurement-based studies of the “search-redirection”
attacks, in which compromised websites are ﬁrst be-
ing used to link to each other and associate themselves
with searches for pharmaceutical and illicit products;
this allows the attacker to have a set of high-ranked
links displayed by the search engine in response to such
queries. The second part of the compromise is to have
a piece of malware on the site that checks the prove-
nance of the trafﬁc coming to the compromise site. For
instance, if trafﬁc is determined to come from a Google
search for drugs, it is immediately redirected—possibly
through several intermediaries—to an illicit online phar-
macy. These studies are primarily empirical characteri-
zations of the phenomenon, but do not go in great details
about how to curb the problem from the standpoint of the
compromised hosts.
In the same spirit of providing comprehensive mea-
surements of web-based abuse, McCoy et al. [25] looks
at revenues and expenses at online pharmacies, includ-
ing an assessment of the commissions paid to “network
afﬁliates” that bring customers to the websites. Wang et
al. [36] provides a longitudinal study of a search-engine
optimization botnet.
Another, recent group of papers looks at how to de-
tect websites that have been compromised. Among these
papers, Invernizzi et al. [15] focuses on automatically
ﬁnding recently compromised websites; Borgolte et al.
[8] look more speciﬁcally at previously unknown web-
based infection campaigns (e.g., previously unknown in-
jections of obfuscated JavaScript-code). Different from
these papers, we use machine-learning tools to attempt to
detect websites that have not been compromised yet, but
that are likely to become malicious in the future, over a
reasonably long horizon (approximately one year).
The research most closely related to this paper is the
recent work by Vasek and Moore [35]. Vasek and Moore
manually identiﬁed the CMS a website is using, and stud-
ied the correlation between that CMS the website secu-
rity. They determined that in general, sites using a CMS
are more likely to behave maliciously, and that some
CMS types and versions are more targeted and compro-
mised than others. Their research supports the basic in-
tuition that the content of a website is a coherent basis
for making predictions about its security outcome.
This paper builds on existing techniques from machine
learning and data mining to solve a security issue. Di-
rectly related to the work we present in this paper is
the data extraction algorithm of Yi et al. [38], which we
adapt to our own needs. We also rely on an ensemble of
decision-tree classiﬁers for our algorithm, adapting the
techniques described by Gao et al. [13].
3 Classifying websites
Our goal is to build a classiﬁer which can predict with
high certainty if a given website will become malicious
in the future. To that effect, we start by discussing the
properties our classiﬁer must satisfy. We then elaborate
on the learning process our classiﬁer uses to differentiate
between benign and malicious websites. Last, we de-
626  23rd USENIX Security Symposium 
USENIX Association
2
scribe an automatic process for selecting a set features
that will be used for classiﬁcation.
3.1 Desired properties
At a high level, our classiﬁer must be efﬁcient, inter-
pretable, robust to imbalanced data, robust to missing
features when data is not available, and adaptive to an
environment that can drastically change over time. We
detail each point in turn below.
Efﬁciency: Since our classiﬁer uses webpages as an in-
put, the volume of the data available to train (and test)
the classiﬁer is essentially the entire World Wide Web.
As a result, it is important the the classiﬁer scale favor-
ably with large, possibly inﬁnite datasets. The classiﬁer
should thus use an online learning algorithm for learning
from a streaming data source.
Interpretability: When the classiﬁer predicts whether a
website will become malicious (i.e., it is vulnerable, and
likely to be exploited; or likely to host malicious con-
tent), it is useful to understand why and how the classiﬁer
arrived at the prediction.
Interpretable classiﬁcation is
essential to meaningfully inform website operators of the
security issues they may be facing. Interpretability is also
useful to detect evolution in the factors that put a website
at risk of being compromised. The strong requirement
for interpretability unfortunately rules out a large number
of possible classiﬁers which, despite achieving excellent
classiﬁcation accuracy, generally lack interpretability.
Robustness to imbalanced data: In many applications
of learning, the datasets that are available are assumed
to be balanced, that is, there is an equal number of ex-
amples for each class. In our context, this assumption
is typically violated as examples of malicious behavior
tend to be relatively rare compared to innocuous exam-
ples. We will elaborate in Section 5 on the relative sizes
of both datasets, but assume, for now, that 1% of all ex-
isting websites are likely to become malicious, i.e., they
are vulnerable, and exploits for these vulnerabilities exist
and are actively used; or they are hosted by actors with
malicious intent. A trivial classiﬁer consistently predict-
ing that all websites are safe would be right 99% of the
time! Yet, it would be hard to argue that such a classi-
ﬁer is useful at all. In other words, our datasets are im-
balanced, which has been shown to be problematic for
learning—the more imbalanced the datasets, the more
learning is impacted [30].
At a fundamental level, simply maximizing accuracy
is not an appropriate performance metric here. Instead,
we will need to take into account both false positives
(a benign website is incorrectly classiﬁed as vulnerable)
and false negatives (a vulnerable website is incorrectly
classiﬁed as benign) to evaluate the performance of our
classiﬁer. For instance, the trivial classiﬁer discussed
above, which categorizes all input as benign, would yield
0% false positives, which is excellent, but 100% of false
negatives among the population of vulnerable websites,
which is obviously inadequate. Hence, metrics such as
receiver-operating characteristics (ROC) curves which
account for both false positive and false negatives are
much more appropriate in the context of our study for
evaluating the classiﬁer we design.
Robustness to errors: Due to its heterogeneity (many
different HTML standards co-exist, and HTML engines
are usually fairly robust to standard violations) and its
sheer size (billions of web pages), the Web is a notori-
ously inconsistent dataset. That is, for any reasonable set
of features we can come up with, it will be frequently the
case that some of the features may either be inconclusive
or undetermined. As a simple example, imagine consid-
ering website popularity metrics given by the Alexa Web
Information Service (AWIS, [1]) as part of our feature
set. AWIS unfortunately provides little or no information
for very unpopular websites. Given that webpage pop-
ularity distribution is “heavy-tailed [9],” these features
would be missing for a signiﬁcant portion of the entire
population. Our classiﬁer should therefore be robust to
errors as well as missing features.
Another reason for the classiﬁer to be robust to errors
is that the datasets used in predicting whether a web-
site will become compromised are fundamentally noisy.
Blacklists of malicious websites are unfortunately in-
complete. Thus, malicious sites may be mislabeled as
benign, and the classiﬁer’s performance should not de-
grade too severely in the presence of mislabeled exam-
ples.
Adaptive: Both the content on the World Wide Web,
and the threats attackers pose vary drastically over time.
As new exploits are discovered, or old vulnerabilities are
being patched, the sites being attacked change over time.
The classiﬁer should thus be able to learn the evolution
of these threats. In machine learning parlance, we need a
classiﬁer that is adaptive to “concept drift” [37].
All of these desired properties led us to consider an en-
semble of decision-tree classiﬁers. The method of using
an ensemble of classiﬁers is taken from prior work [13].
The system works by buffering examples from an input
data stream. After a threshold number of examples has
been reached, the system trains a set of classiﬁers by re-
sampling all past examples of the minority class as well
as recent examples of the majority class. While the type
of classiﬁer used in the ensemble may vary, we chose to
use C4.5 decision trees [31].
The system is efﬁcient as it does not require the stor-
age or training on majority class examples from the far
past. The system is also interpretable and robust to errors
as the type of classiﬁer being used is a decision-tree in
USENIX Association  
23rd USENIX Security Symposium  627
3
available data for prediction
desired test time
Known data
Past
t
(Present)
t+h
Future
(a) Using the present to predict the future
time
available for prediction
desired test time
Known data
t−h
Past
t
(Present)
Future
time
(b) Using the past to predict the present
Figure 1: Prediction timeline. Attempting to predict
the future makes it impossible to immediately evaluate
whether the prediction was correct (a). A possible alter-
native (b) is to use past data to simulate a prediction done
in the past (at t − h) that can then be tested at the present
time t.
an ensemble [13]. Periodically retraining our classiﬁers
makes them robust to concept drift as long as retrain-
ing occurs sufﬁciently often. Finally, the system handles
class imbalance by resampling the input stream, namely,
it resamples from the set of all minority class training
examples from the past as well as recent majority class
examples.
3.2 Learning process
The type of classiﬁcation we aim to perform presents
unique challenges in the learning process.
Lack of knowledge of the future: Assume that at a
given time t, our classiﬁer predicts that a given website
w is likely to become compromised in the future. Be-
cause the website has not been compromised yet—and
may not be compromised for a while—we cannot imme-
diately know whether the prediction is correct. Instead,
we have to wait until we have reached a time (t + h) to
effectively be able to verify whether the site has become
compromised between t and (t + h), or if the classiﬁer
was in error. This is particularly problematic, since just
training the classiﬁer—let alone using it—would require
to wait at least until (t + h). This is the situation illus-
trated in Figure 1(a).
A second, related issue, is that of deﬁning a mean-
ingful “time horizon” h. If h is too long, it will be im-
possible to even verify that the classiﬁer was right. In
an extreme case, when h → ∞, the performance of the
classiﬁer cannot be evaluated.1 Selecting a time horizon
too short (e.g., h = 0) would likewise reduce to the prob-
lem of determining whether a website is already compro-
mised or not—a very different objective for which a rich
literature already exists, as discussed earlier.
1Given the complexity of modern computer software, it is likely that
exploitable bugs exist in most, if not all webservers, even though they
might have not been found yet. As a result, a trivial classiﬁer predicting
that all websites will be compromised over an inﬁnite horizon (h → ∞)
may not even be a bad choice.
We attempt to solve these issues as follows. First, de-
ciding what is a meaningful value for the horizon h ap-
pears, in the end, to be a design choice. Unless other-
wise noted, we will assume that h is set to one year. This
choice does not affect our classiﬁer design, but impacts
the data we use for training.
Second, while we cannot predict the future at time t,
we can use the past for training. More precisely, for train-
ing purposes we can solve our issue if we could extract a
set of features, and perform classiﬁcation on an archived
version of the website w as it appeared at time (t − h)
and check whether, by time t, w has become malicious.
This is what we depict in Figure 1(b). Fortunately, this is
doable: At the time of this writing, the Internet Archive’s
Wayback Machine [34] keeps an archive of more than
391 billion webpages saved over time, which allows us
to obtain “past versions” of a large number of websites.
Obtaining examples of malicious and benign web-
sites: To train our classiﬁer, we must have ground truth
on a set of websites—some known to be malicious, and
some known to be benign. Conﬁrmed malicious websites
can be obtained from blacklists (e.g., [28]). In addition,