[55] Stefan Kaestle, Reto Achermann, Roni Haecki, Moritz Hoffmann, Sabela Ramos,
and Timothy Roscoe. 2016. Machine-Aware Atomic Broadcast Trees for Multi-
cores.. In OSDI, Vol. 16. 33–48.
[56] Robert Kallman, Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alexander
Rasin, Stanley Zdonik, Evan PC Jones, Samuel Madden, Michael Stonebraker,
Yang Zhang, et al. 2008. H-store: a high-performance, distributed main memory
transaction processing system. Proceedings of the VLDB Endowment 1, 2 (2008),
1496–1499.
[57] Anuj Kalia Michael Kaminsky and David G Andersen. 2016. Design guidelines for
high performance RDMA systems. In 2016 USENIX Annual Technical Conference.
437.
[58] Naga Katta, Mukesh Hira, Changhoon Kim, Anirudh Sivaraman, and Jennifer
Rexford. 2016. Hula: Scalable load balancing using programmable data planes.
In Proceedings of the Symposium on SDN Research. 1–12.
[59] Antoine Kaufmann, SImon Peter, Naveen Kr Sharma, Thomas Anderson, and
Arvind Krishnamurthy. 2016. High performance packet processing with flexnic.
In Proceedings of the Twenty-First International Conference on Architectural Sup-
port for Programming Languages and Operating Systems. 67–81.
[60] Jongsung Kim and Cheeha Kim. 1997. A total ordering protocol using a dynamic
token-passing scheme. Distributed Systems Engineering 4, 2 (1997), 87.
[61] Gautam Kumar, Nandita Dukkipati, Keon Jang, Hassan MG Wassel, Xian Wu,
Behnam Montazeri, Yaogong Wang, Kevin Springborn, Christopher Alfeld,
Michael Ryan, et al. 2020. Swift: Delay is Simple and Effective for Congestion
Control in the Datacenter. In Proceedings of the Annual conference of the ACM
Special Interest Group on Data Communication on the applications, technologies,
architectures, and protocols for computer communication. 514–528.
[62] Hsiang-Tsung Kung and John T Robinson. 1981. On optimistic methods for
concurrency control. ACM Transactions on Database Systems (TODS) 6, 2 (1981),
213–226.
[63] Leslie Lamport. 1978. Time, clocks, and the ordering of events in a distributed
system. Commun. ACM 21, 7 (1978), 558–565.
[64] Leslie Lamport. 1984. Using time instead of timeout for fault-tolerant distributed
systems. ACM Transactions on Programming Languages and Systems (TOPLAS)
6, 2 (1984), 254–280.
[65] Leslie Lamport. 1998. The part-time parliament. ACM Transactions on Computer
Systems (TOCS) 16, 2 (1998), 133–169.
[66] Ki Suh Lee, Han Wang, Vishal Shrivastav, and Hakim Weatherspoon. 2016.
Globally synchronized time via datacenter networks. In Proceedings of the 2016
conference on ACM SIGCOMM 2016 Conference. ACM, 454–467.
[67] Charles E Leiserson. 1985. Fat-trees: universal networks for hardware-efficient
supercomputing. IEEE transactions on Computers 100, 10 (1985), 892–901.
[68] Jialin Li, Ellis Michael, Naveen Kr Sharma, Adriana Szekeres, and Dan RK Ports.
2016. Just Say NO to Paxos Overhead: Replacing Consensus with Network
Ordering.. In OSDI. 467–483.
[69] Yuliang Li, Gautam Kumar, Hema Hariharan, Hassan Wassel, Peter Hochschild,
Dave Platt, Simon Sabato, Minlan Yu, Nandita Dukkipati, Prashant Chandra,
et al. 2020. Sundial: Fault-tolerant Clock Synchronization for Datacenters. In
14th {USENIX} Symposium on Operating Systems Design and Implementation
({OSDI} 20). 1171–1186.
[70] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang,
Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, et al. 2019. HPCC:
high precision congestion control. In Proceedings of the ACM Special Interest
Group on Data Communication. 44–58.
[71] Guohan Lu, Chuanxiong Guo, Yulong Li, Zhiqiang Zhou, Tong Yuan, Haitao
Wu, Yongqiang Xiong, Rui Gao, and Yongguang Zhang. 2011. ServerSwitch: A
Programmable and High Performance Platform for Data Center Networks.. In
Nsdi, Vol. 11. 2–2.
[72] Haonan Lu, Christopher Hodsdon, Khiem Ngo, Shuai Mu, and Wyatt Lloyd.
2016. The SNOW Theorem and Latency-Optimal Read-Only Transactions.. In
OSDI. 135–150.
[73] Haonan Lu, Siddhartha Sen, and Wyatt Lloyd. 2020. Performance-Optimal Read-
Only Transactions. In 14th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 20). 333–349.
[74] Ellis Michael and Dan RK Ports. 2018. Towards causal datacenter networks. In
Proceedings of the 5th Workshop on the Principles and Practice of Consistency for
Distributed Data. 1–4.
[75] Radhika Mittal, Nandita Dukkipati, Emily Blem, Hassan Wassel, Monia Ghobadi,
Amin Vahdat, Yaogong Wang, David Wetherall, David Zats, et al. 2015. TIMELY:
RTT-based congestion control for the datacenter. In ACM SIGCOMM Computer
Communication Review, Vol. 45. ACM, 537–550.
[76] Radhika Mittal, Alexander Shpiner, Aurojit Panda, Eitan Zahavi, Arvind Kr-
ishnamurthy, Sylvia Ratnasamy, and Scott Shenker. 2018. Revisiting network
support for RDMA. In Proceedings of the 2018 Conference of the ACM Special
Interest Group on Data Communication. 313–326.
[77] Adam Morrison and Yehuda Afek. 2013. Fast concurrent queues for x86 proces-
sors. In ACM SIGPLAN Notices, Vol. 48. ACM, 103–112.
[78] Louise E. Moser, P Michael Melliar-Smith, Deborah A. Agarwal, Ravi K. Budhia,
and Colleen A. Lingley-Papadopoulos. 1996. Totem: A fault-tolerant multicast
group communication system. Commun. ACM 39, 4 (1996), 54–63.
[79] Shuai Mu, Yang Cui, Yang Zhang, Wyatt Lloyd, and Jinyang Li. 2014. Extracting
more concurrency from distributed transactions. In 11th {USENIX} Symposium
on Operating Systems Design and Implementation ({OSDI} 14). 479–494.
[80] Diego Ongaro and John K Ousterhout. 2014. In search of an understandable
consensus algorithm.. In USENIX Annual Technical Conference. 305–319.
[81] Seo Jin Park and John Ousterhout. 2019. Exploiting commutativity for practical
fast replication. In 16th USENIX Symposium on Networked Systems Design and
Implementation (NSDI ’19). 47–64.
[82] Fernando Pedone and André Schiper. 1998. Optimistic atomic broadcast. In
International Symposium on Distributed Computing. Springer, 318–332.
[83] Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah, and Hans
Fugal. 2015. Fastpass: A centralized zero-queue datacenter network. ACM
SIGCOMM Computer Communication Review 44, 4 (2015), 307–318.
[84] Larry L Peterson, Nick C Buchholz, and Richard D Schlichting. 1989. Preserving
and using context information in interprocess communication. ACM Transac-
tions on Computer Systems (TOCS) 7, 3 (1989), 217–246.
[85] Dan RK Ports, Jialin Li, Vincent Liu, Naveen Kr Sharma, and Arvind Krishna-
murthy. 2015. Designing Distributed Systems Using Approximate Synchrony
in Data Center Networks.. In NSDI. 43–57.
[86] B Rajagopalan and Philip K McKinley. 1989. A token-based protocol for reli-
able, ordered multicast communication. In Reliable Distributed Systems, 1989.,
Proceedings of the Eighth Symposium on. IEEE, 84–93.
[87] Luis Rodrigues, Rachid Guerraoui, and André Schiper. 1998. Scalable atomic
multicast. In Computer Communications and Networks, 1998. Proceedings. 7th
International Conference on. IEEE, 840–847.
[88] Amedeo Sapio, Ibrahim Abdelaziz, Abdulla Aldilaijan, Marco Canini, and Panos
Kalnis. 2017. In-network computation is a dumb idea whose time has come. In
Proceedings of the 16th ACM Workshop on Hot Topics in Networks. 150–156.
[89] Peter Sewell, Susmit Sarkar, Scott Owens, Francesco Zappa Nardelli, and Mag-
nus O Myreen. 2010. x86-TSO: a rigorous and usable programmer’s model for
x86 multiprocessors. Commun. ACM 53, 7 (2010), 89–97.
[90] Alex Shamis, Matthew Renzelmann, Stanko Novakovic, Georgios Chatzopou-
los, Aleksandar Dragojević, Dushyanth Narayanan, and Miguel Castro. 2019.
Fast general distributed transactions with opacity. In Proceedings of the 2019
International Conference on Management of Data. 433–448.
[91] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy
Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, et al. 2015.
Jupiter rising: A decade of clos topologies and centralized control in google’s
datacenter network. ACM SIGCOMM computer communication review 45, 4
(2015), 183–197.
[92] Anirudh Sivaraman, Suvinay Subramanian, Mohammad Alizadeh, Sharad Chole,
Shang-Tse Chuang, Anurag Agrawal, Hari Balakrishnan, Tom Edsall, Sachin
Katti, and Nick McKeown. 2016. Programmable Packet Scheduling at Line Rate.
In Proceedings of the 2016 Conference on ACM SIGCOMM 2016 Conference. ACM,
44–57.
[93] Joseph Tassarotti, Derek Dreyer, and Viktor Vafeiadis. 2015. Verifying read-
copy-update in a logic for weak memory. In ACM SIGPLAN Notices, Vol. 50.
ACM, 110–120.
[94] Robbert van Renesse. 1994. Why bother with CATOCS? ACM SIGOPS Operating
Systems Review 28, 1 (1994), 22–27.
[95] Erico Vanini, Rong Pan, Mohammad Alizadeh, Parvin Taheri, and Tom Edsall.
2017. Let it flow: Resilient asymmetric load balancing with flowlet switching. In
14th {USENIX} Symposium on Networked Systems Design and Implementation
({NSDI} 17). 407–420.
[96] Sage A Weil, Scott A Brandt, Ethan L Miller, Darrell DE Long, and Carlos
Maltzahn. 2006. Ceph: A scalable, high-performance distributed file system. In
Proceedings of the 7th symposium on Operating systems design and implementa-
tion. 307–320.
[97] Jisoo Yang, Dave B Minturn, and Frank Hady. 2012. When poll is better than
interrupt.. In FAST, Vol. 12. 3–3.
[98] Xiangyao Yu, George Bezerra, Andrew Pavlo, Srinivas Devadas, and Michael
Stonebraker. 2014. Staring into the abyss: An evaluation of concurrency control
1Pipe: Scalable Total Order Communication in Data Center Networks
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
with one thousand cores. Proceedings of the VLDB Endowment 8, 3 (2014),
209–220.
[99] Zhuolong Yu, Yiwen Zhang, Vladimir Braverman, Mosharaf Chowdhury, and
Xin Jin. 2020. Netlock: Fast, centralized lock management using programmable
switches. In Proceedings of the Annual conference of the ACM Special Interest
Group on Data Communication on the applications, technologies, architectures,
and protocols for computer communication. 126–138.
[100] Hong Zhang, Junxue Zhang, Wei Bai, Kai Chen, and Mosharaf Chowdhury. 2017.
Resilient datacenter load balancing in the wild. In Proceedings of the Conference
of the ACM Special Interest Group on Data Communication. 253–266.
[101] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn,
Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and
Ming Zhang. 2015. Congestion control for large-scale RDMA deployments.
ACM SIGCOMM Computer Communication Review 45, 4 (2015), 523–536.
[102] Danyang Zhuo, Monia Ghobadi, Ratul Mahajan, Klaus-Tycho Förster, Arvind
Krishnamurthy, and Thomas Anderson. 2017. Understanding and mitigating
packet corruption in data center networks. In Proceedings of the Conference of
the ACM Special Interest Group on Data Communication. 362–375.
APPENDIX
Appendices are supporting material that has not been peer-reviewed.
In reliable 1Pipe, a scattering 𝑀 from
Correctness Analysis.
sender 𝑆 to receivers 𝑅𝑖 with timestamp 𝑇 should be delivered if
and only if the 𝑆 does not fail before 𝑇 and 𝑆 has received all ACKs
from 𝑅𝑖. If 𝑆 fails before 𝑇 (i.e., the failure timestamp of 𝑆 is less
than 𝑇 ), then 𝑀 is not committed (or the commit message has not
propagated to any receiver, which is regarded as not committed). If
any receiver 𝑅𝑖 fails before receiving the prepare message of 𝑀 or
sending the ACK message, 𝑆 should recall 𝑀.
Now, we analyze the behavior of message 𝑀 : 𝑆 → 𝑅 under each
type of failure. A process has both sender and receiver roles, and
they are considered separately.
• Packet loss on a fair-loss link: deliver because 2PC retransmits
• 𝑅 fails before sending ACK in Prepare phase: discard according
• 𝑅 fails after sending ACK in Prepare phase: The message is in
𝑅’s receive buffer. We only ensure atomicity in a fail-recover
model because recording the last message that 𝑅 has delivered is
impossible. The controller records failure timestamps of senders
and undeliverable recall messages, so that recovered receivers
to Recall step.
packets.
can deliver or discard messages consistently in each scattering.
If 𝑅 fails permanently, atomicity is violated.
• 𝑆 fails before sending commit timestamp 𝑇 : discard according
to Discard step. For example, when 𝑆 fails while executing Re-
call step of 𝑀 due to previous failure of a receiver, its commit
timestamp 𝑇 must be lower than 𝑀’s timestamp, so all receivers
would discard 𝑀 due to failure of 𝑆.
• 𝑆 fails after sending commit timestamp 𝑇 : deliver if and only
if the failure timestamp 𝑇 ′ of 𝑆 is larger or equal to 𝑇 . Failure
timestamp is determined by controller, which ensures that 𝑆 has
committed 𝑇 ′ but no message after 𝑇 ′ is delivered. Controller ob-
tains 𝑇 ′ by gathering the maximum failure timestamp from a cut
in a routing graph consisting of healthy switches that separates
𝑆 and all receivers. If the data center has separate production
and management networks, and assuming they do not fail simul-
taneously, such a cut can always be found. Otherwise, a network
partition may lead to atomicity violation.
• The receiver 𝑅′ of another message 𝑀′ in the same scattering
fails before sending ACK in Prepare phase: because the ACK is
not received, discard according to Recall step.
• The receiver 𝑅′ of another message 𝑀′ in the same scattering
fails after sending ACK in Prepare phase: deliver according to
2PC.
• The network path between 𝑆 and 𝑅 fails (e.g., due to routing prob-
lem), but 𝑆 and 𝑅 are reachable from the controller: controller
forwards messages between 𝑆 and 𝑅.
• The network path between 𝑆 and 𝑅 fails, and 𝑆 or 𝑅 is unreach-
able from the controller: the unreachable process is considered
as failed. For example, if a host or the only network link from a
host fails, all processes on it are disconnected from the network,
so they are considered to fail.
• The network path to 𝑅 fails after 𝑅 receives commit barrier 𝑇 : 𝑇
• The network path to 𝑅 fails before 𝑅 receives commit barrier
𝑇 : deliver after 𝑅 recovers according to Receiver Recovery. If 𝑅
fails permanently, atomicity is violated.
is already delivered to 𝑅.