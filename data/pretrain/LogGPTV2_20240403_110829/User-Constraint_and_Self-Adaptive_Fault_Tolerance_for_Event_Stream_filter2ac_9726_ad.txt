### Analysis of Fault Tolerance Schemes and Resource Utilization

#### Figure 8: Time Spent in Each Fault Tolerance Scheme
- **Description**: This figure shows the fraction of time spent in each fault tolerance scheme with varying recovery time thresholds for different cost models.
- **Schemes**:
  - Passive Replication
  - Deployed
  - Passive Standby Cold
  - Passive Standby Hot
  - Active Standby
  - Active Replication
  - Gap Recovery
  - Precise Recovery (Exactly Once)

- **Y-Axis**: Percentage of time spent in each scheme
- **X-Axis**: Recovery time threshold (in seconds)
- **Data Points**: 2, 6, 8, 10, 12, 14, 16, 18, 20

#### Figure 4: Throughput, State Size, and Fault Tolerance Scheme Evolution
- **Description**: This figure illustrates the evolution of throughput, state size, and fault tolerance schemes over time using a Twitter workload with a recovery threshold set to 5.5 seconds.
- **Metrics**:
  - Maximum (max)
  - Mean (mean)
  - Minimum (min)
- **Network and CPU Utilization**: 
  - Network Utilization: 0, 2, 4, 6, 8
  - CPU Utilization: 0, 2, 4, 6, 8

- **Recovery Time (s)**: 2, 6, 8, 10, 12, 14, 16, 18, 20
- **State Synchronization/Checkpointing Interval (s)**: 2, 4, 6, 8, 10, 12, 14, 16, 18, 20

#### Figure 10: Trade-off Between Saved CPU and Additional Network Traffic
- **Description**: This figure shows the trade-off between saved CPU and additional network traffic for different state synchronization and checkpoint intervals.
- **Y-Axis**: Recovery time (in seconds)
- **X-Axis**: State Synchronization/Checkpointing Interval (in seconds)
- **Data Points**: 2, 4, 6, 8, 10, 12, 14, 16, 18, 20

#### Figure 9: Time Spent in Each Fault Tolerance Scheme with Varying Recovery Time Thresholds
- **Description**: This figure shows the fraction of time spent in each fault tolerance scheme with varying recovery time thresholds for different recovery semantics (gap and precise recovery).
- **Y-Axis**: Percentage of time spent in each scheme
- **X-Axis**: Recovery time threshold (in seconds)
- **Data Points**: 2, 6, 8, 10, 12, 14, 16, 18, 20

### Workload and Recovery Semantics
- **Workload**: The experiment uses a Twitter workload to analyze the evolution of event throughput and state size.
- **Correlation**: There is a correlation between throughput and state size for time-based sliding windows, where the state size follows the pattern of the throughput.
- **Throughput and State Size**:
  - Initial throughput: ~100 kEvents/s
  - State size: ~20 MB
  - After a sudden increase in throughput, the state size rises to 90 MB.

- **Recovery Time**: Set to 5.5 seconds, as application criticality only concerns user experience.
- **Recovery Semantics**:
  - **Precise Recovery**: Chosen for its repeatability and safety, useful for debugging distributed applications.
  - **Checkpoint and State Synchronization Interval**: Set to 3.5 seconds due to the small state size.
  - **Cost Weight Vector**: Default values used, selecting the approach that consumes the least CPU, network, memory, and virtual machine resources.

### Controller Behavior
- **Initial Scheme**: The system starts with active replication as a safe choice.
- **Switching Schemes**: The controller switches to the deployed scheme when the state and throughput are low, allowing recovery from disk and replay from upstream nodes within the specified recovery time threshold.
- **Load Spikes**: During load spikes, the controller switches between passive replication and deployed schemes.
- **Cool Down Time**: A cool-down period of five seconds prevents oscillations due to sudden load spikes, common in live data sources like Twitter streams.
- **Scheme Dominance**: The first half of the experiment is dominated by a combination of passive replication and deployed schemes, while the second half is dominated by passive hot standby.

### Resource Overhead and Savings
- **Experiment Setup**:
  - **Workload**: Twitter workload
  - **Nodes**: 10 nodes
  - **Recovery Time Threshold**: Varied in multiple runs
  - **Cost Weight Vector**: Default values

- **Resource Metrics**:
  - CPU Utilization
  - Memory Utilization
  - Network Utilization (In and Out)
  - Infrastructure Utilization (number of virtual machines used)

- **Normalization**: Resource utilization is normalized to the execution of active replication.
- **Results**:
  - **CPU Utilization**: Varies based on the recovery time threshold.
  - **Memory Utilization**: Varies based on the recovery time threshold.
  - **Network Utilization (In and Out)**: Varies based on the recovery time threshold.
  - **Infrastructure Utilization**: Varies based on the recovery time threshold.

- **Observations**:
  - As the recovery time threshold increases, the resource overhead generally decreases.
  - The trade-off between saved CPU and additional network traffic is analyzed for different state synchronization and checkpoint intervals.

This structured format provides a clear and coherent overview of the figures and the experimental setup, making it easier to understand the results and their implications.