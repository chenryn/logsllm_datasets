plication. The bug replay capacity of the Similarity-Guided
Figure 3.
(sorted in ascending order), when logging all the SPEs of the program.
SPE dispersion ratios for the ConTest benchmark programs
On the other hand, the Manager program has all its SPEs
with a dispersion ratio of 1 or closer, which means that
almost all the recorded executions had a different thread
interleaving. These clearly represent unfavorable conditions
for the partial logging approach, which in fact failed to
replay the bug, as indicated in Table III. BoundedBuffer
exhibits a similar outcome, however, as it has some SPEs
with dispersion ratio less than 0.8, it was still possible to re-
produce the error even when logging 50%, using Dispersion-
based Similarity.
Regarding the TwoStage application, it presents unusual
results when using Plain Similarity, since the bug was not
replayed when the partial logs recorded more information.
The explanation for this is related to the SPE dispersion
ratios. As highlighted by Figure 3, out of the four program’s
SPEs, two were always identical (SPE 0 and 1), one had
very few equal access vectors (SPE 2), and the last one was
always different (SPE 3). Let us further discuss the three
partial logging scenarios when using Plain Similarity:
75% of SPEs: with this conﬁguration, each partial log was
composed by three SPEs. Hence, the list of base partial
logs ended being composed by the partial logs whose group
 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12Dispersion RatioSPE IndexConTest BenchmarkBoundedBufferBubbleSortProducerConsumerPiperTwoStageManagerBufferWriterPlain Similiarity
Dispersion-based
Similarity
10% 25% 50% 75% 10% 25% 50% 75%
10
2
1
1
8
1
1
1
NUMBER OF THE ATTEMPTS REQUIRED BY THE HEURISTIC TO REPLAY
Table IV
TOMCAT#37458 BUG.
Merge heuristic was tested with bug #37458 2 of Tomcat
v5.5. This error consists of a NullPointerException,
resulting from a data race, and was already used in [9] to
test LEAP.
In this case only 15 SPEs (out of the total 32 SPEs of
Tomcat) were logged by CoopREP. This depends on the
fact that the available unit test designed to trigger the bug
only exercises a subset of Tomcat’s SPEs. Interestingly, even
despite the availability of an aimed unit test, triggering the
bug was not trivial, as the bug only manifested itself on
average after 112 attempts.
Table IV shows the number of attempts of the Similarity-
Guided Merge heuristic (using both Plain Similarity and
Dispersion-based Similarity) to replay the Tomcat#37458
bug, when logging from 10% to 75% of the SPEs. The data
conﬁrms the effectiveness of the proposed statistical analysis
techniques, and in particular of the one based on Dispersion-
based similarity, which was able to identify compatible
combinations of partial logs and to replay successfully the
bug in less than 10 attempts even when logging accesses to
only 10% of the SPEs.
D. Performance Overhead
In this section we analyze the performance beneﬁts
achievable via the CoopREP scheme. For space constraints,
we focus here on the ConTest and the Java Grande Forum
benchmarks, as the results related Tomcat show very similar
trends.
1) ConTest Benchmark: Figure 4 reports the performance
overhead on the tested programs. By using partial recording,
CoopREP achieved always lower runtime degradation than
LEAP. The results highlight that the overhead reductions are
not necessarily linear. This is due to the fact that some SPEs
are accessed signiﬁcantly more frequently than others. Given
that the instrumentation of the code is performed statically,
the load balance in terms of thread accesses may not be
equally distributed among the users, as previously referred
in Section IV-C.
Clearly, the advantage of using partial logging is higher in
scenarios where the usage of full logging has a higher neg-
ative impact on performance. The most notorious case are
BubbleSort and BufferWriter , which are the applications
with the highest number of SPE accesses (see Table II).
For example, in BubbleSort, LEAP imposed a performance
2https://issues.apache.org/bugzilla/show bug.cgi?id=37458
Figure 4.
LEAP corresponds to the recording conﬁguration of 100%.
Performance overheads for the ConTest benchmark programs.
Program
Raytracer
SparseMatmult
SOR
Montecarlo
Series
SPEs
16
8
8
15
8
Total Accesses
2.56×109
5.08×107
1.99×106
1.50×105
2.00×104
DESCRIPTION OF THE JAVA GRANDE FORUM BENCHMARK PROGRAMS
USED IN THE EXPERIMENTS.
Table V
overhead of 26%, while CoopREP imposed only a 7%
overhead when recording 10% of the total SPEs (which was
sufﬁcient to successfully replay the bug).
Overall, the results in Figure 4 highlight that, at least for
these benchmarks, the choice of logging at most 25% of the
SPEs lead to a runtime penalty that is consistently lower
than 10%: a threshold typically regarded as the maximum
acceptable overhead for real world applications [4], [20].
2) Java Grande Forum: The Java Grande Forum bench-
mark contains computationally intensive science and engi-
neering applications that require high-performance comput-
ers. Given that Java Grande Forum Benchmark does not have
known bugs, it was only used in our experiments to assess
the beneﬁts and limitations of CoopREP when compared
to LEAP, on demanding computing environments. Table V
describes the benchmark programs used in terms of number
of SPEs and the overall number of times that they are
accessed. For the sake of readability, the results of the tests
performed are presented in tables, since the values obtained
vary within a large scale.
Table VI contains the experiments with respect to the
performance overhead measured when tracing the SPEs with
the previous logging conﬁgurations.
The results show that the logging overhead can be dramat-
ically abated by CoopLEAP, especially for memory intensive
applications, such as RayTracer or SparseMatmult. In the
former case, the average logging overhead drops by a factor
around 50x when conﬁguring the CoopLeap to log 10% of
 0 5 10 15 20 25 30BoundedBufferBubbleSortProducerConsumerPiperTwoStageManagerBufferWriterOverhead (%)ProgramConTest Performance OverheadCREP-10%CREP-12.5%CREP-16.7%CREP-25%CREP-50%CREP-75%LEAPthe application’s SPEs. Analogous trends can be observed
also for the other benchmarks, providing an additional exper-
imental evidence at support of the signiﬁcant performance
gains achievable via cooperative logging schemes.
E. Log Sizes
We now quantify the performance beneﬁts achievable
by CoopREP from an alternative perspective, namely the
amount of logs generated with respect to a non-cooperative
logging scheme, such as LEAP.
1) ConTest Benchmark: Figure 5 reports the results
obtained for the ConTest benchmark, showing the ratio
between the size of the logs generated by various partial
recording conﬁgurations and the size of the logs generated
by LEAP. Unsurprisingly, the log size ratios follow a trend
that is analogous to that observed in the performance over-
head plots reported in Figure 4. The Manager benchmark
resulted to be the program for which CoopREP show a re-
duction ratio more similar to the one expected by decreasing
proportionally the recording percentage. In turn, BubbleSort
was the program where the log sizes decreased faster even
for small reduction of the total number of logged SPEs (the
log size ratios obtained, with respect to LEAP, were 0.36 and
0.62, when logging 50% and 75% of the SPEs, respectively).
However, the reduction obtained when logging less than 50%
of the SPEs was not very signiﬁcant. This is due to the fact
that the largest fraction of accesses is conﬁned to a single
SPE, which was recorded the same number of times for the
ten logs measured for these conﬁgurations.
On the other hand, Piper was the application where
decreasing the percentage of logged SPEs led to smaller
reductions of the log sizes. This is again explainable by the
heterogeneity in the size of the access vectors associated
with the various SPEs. In this application, recording 25% of
the SPEs only led to a log size ratio of 0.55.
2) Java Grande Forum: In Figure 6 we report the results
concerning the log size ratios with respect to LEAP when
using the Java Grande Forum.
From the ﬁgure analysis, the beneﬁts of partial logging
are clear. The most evident case is SOR, where the log sizes
when logging less than 25% of the SPEs account for at most
0.1% of LEAP’s log size. In fact, for all the benchmark
Program
10%
25%
Performance Overhead
50%
LEAP
Raytracer
SparseMatmult
SOR
Montecarlo
Series
1757.5% 9566.7% 17452.1% 92908.4%
571.6%
2606.7%
0.8%
1.1%
0.08%
598.2%
1.1%
1.5%
0.1%
2.0%
2.3%
0.4%
2.7%
7.3%
6.5%
1725.5%
PERFORMANCE OVERHEADS FOR THE JAVA GRANDE FORUM
BENCHMARK PROGRAMS.
Table VI
Figure 5. Log size ratios for the ConTest benchmark programs.
Figure 6. Log size ratios for the Java Grande benchmark programs.
programs, there was a high heterogeneity in the size of
the access vectors of the program SPEs, which signiﬁcantly
inﬂuenced the actual reduction in the log sizes. In other
words, the decreases are just not completely linear because
some SPEs are accessed more times than others. Given that
the instrumentation of the code is performed using a purely
random approach, the load in terms of logged SPE accesses
may not be equally distributed among the different runs,
as previously referred in Section IV-C. This implies that
the impact of logging x% of the SPEs will not necessarily
mean a reduction of x% in both performance overhead and
log size. In fact, sometimes the average reduction may be
greater than expected (as in Raytracer and SparseMatmult),
but other times may be lower (as in Series when logging
75% of the SPEs). This motivates future research in how
one can equally distribute the information to be recorded
among the different clients.
VI. CONCLUSIONS
This paper introduced CoopREP, a system that provides
fault replication of concurrent programs, through cooperative
recording and partial log combination. CoopREP achieves
 0 0.2 0.4 0.6 0.8 1BoundedBufferBubbleSortProducerConsumerPiperTwoStageManagerBufferWriterLog Size RatioProgramConTest Log Size RatiosCREP-10%CREP-12.5%CREP-16.7%CREP-25%CREP-50%CREP-75%LEAP 0 0.2 0.4 0.6 0.8 1RayTracerSparseMatmultSORMonteCarloSeriesLog Size RatioProgramJava Grande Log Size RatiosCREP-10%CREP-25%CREP-50%CREP-75%LEAPremarkable reductions of the overhead incurred in by con-
ventional deterministic execution replayer by letting each
instance of a program trace only a subset of its shared
programming elements (e.g. variables or synchronization
primitives). CoopREP relies on several innovative statistical
analysis techniques aimed at guiding the search of partial
logs to combine and use during the replay phase.
The evaluation study, performed with third-party bench-
marks and a real-world application, highlighted both the
effectiveness of the proposed technique,
in terms of its
capability to successfully replay non-trivial concurrency
bugs, as well as its performance advantages with respect
to non-cooperative logging schemes.
We believe that this work opens a number of challenging
research directions, including the design of additional partial
logging schemes (e.g. taking into account load balancing or
locality of SPEs, or maximizing the probability of logging
overlapping SPEs) and new similarity metrics (e.g. that use
euclidean or edit distances between access vectors).
ACKNOWLEDGMENTS
The authors wish to thank the anonymous reviewers for
the valuable feedback and suggestions. The authors also
wish to thank Jeff Huang for his availability to answer
the countless questions about LEAP. This work was par-
tially supported by FCT (INESC-ID multi-annual funding)
through the PIDDAC program funds, and by the European
project “FastFix” (FP7-ICT-2009-5).
REFERENCES
[1] Z. Li, L. Tan, X. Wang, S. Lu, Y. Zhou, and C. Zhai, “Have
things changed now?: an empirical study of bug characteris-
tics in modern open source software,” in ACM ASID, 2006,
pp. 25–33.
[2] A. Hall, “Realising the beneﬁts of formal methods,” Journal
of Universal Computer Science, vol. 13, no. 5, pp. 669–678,
2007.
[3] D. Parnas, “Really rethinking ‘formal methods’,” Computer,
vol. 43, pp. 28–34, 2010.
[4] S. Park, Y. Zhou, W. Xiong, Z. Yin, R. Kaushik, K. Lee, and
S. Lu, “Pres: probabilistic replay with execution sketching on
multiprocessors,” in ACM SOSP, 2009, pp. 177–192.
[5] G. Dunlap, D. Lucchetti, M. Fetterman, and P. Chen, “Ex-
ecution replay of multiprocessor virtual machines,” in ACM
VEE, 2008, pp. 121–130.
[6] A. Georges, M. Christiaens, M. Ronsse, and K. De Boss-
chere, “Jarec: a portable record/replay environment for multi-
threaded java applications,” Software Practice and Experi-
ence, vol. 40, pp. 523–547, May 2004.
[7] J.-D. Choi and H. Srinivasan, “Deterministic replay of java
multithreaded applications,” in ACM SPDT, 1998, pp. 48–59.
[8] T. LeBlanc and J. Mellor-Crummey, “Debugging parallel
programs with instant replay,” IEEE Trans. Comput., vol. 36,
pp. 471–482, April 1987.
[9] J. Huang, P. Liu, and C. Zhang, “Leap: lightweight determin-
istic multi-processor replay of concurrent java programs,” in
ACM FSE, 2010, pp. 385–386.
[10] G. Pokam, C. Pereira, K. Danne, L. Yang, and J. Torrellas,
“Hardware and software approaches for deterministic multi-
processor replay of concurrent programs,” Intel Technology
Journal, vol. 13, pp. 20–41, 2009.
[11] L. Lamport, “Ti clocks, and the ordering of events in a
distributed system,” Commun. ACM, vol. 21, pp. 558–565,
July 1978.
[12] S. Srinivasan, S. Kandula, C. Andrews, and Y. Zhou, “Flash-
back: A lightweight extension for rollback and deterministic
replay for software debugging,” in USENIX Annual Technical
Conference, 2004, pp. 29–44.
[13] M. Xu, R. Bodik, and M. Hill, “A “ﬂight data recorder” for
enabling full-system multiprocessor deterministic replay,” in
ISCA. ACM, 2003, pp. 122–135.
[14] S. Narayanasamy, G. Pokam, and B. Calder, “Bugnet: Contin-
uously recording program execution for deterministic replay
debugging,” in IEEE ISCA, 2005, pp. 284–295.
[15] P. Montesinos, L. Ceze, and J. Torrellas, “Delorean: Record-
ing and deterministically replaying shared-memory multipro-
cessor execution efﬁciently,” in IEEE ISCA, 2008, pp. 123–
134.
[16] P. Montesinos, M. Hicks, S. King, and J. Torrellas, “Capo: a
software-hardware interface for practical deterministic multi-
processor replay,” in ACM ASPLOS, 2009, pp. 73–84.
[17] G. Altekar and I. Stoica, “Odr: output-deterministic replay for
multicore debugging,” in ACM SOSP, 2009, pp. 193–206.
[18] C. Zamﬁr and G. Candea, “Execution synthesis: a technique
for automated software debugging,” in ACM EuroSys, 2010,
pp. 321–334.
[19] B. Liblit, A. Aiken, A. Zheng, and M. Jordan, “Bug isolation
via remote program sampling,” in ACM PLDI, 2003, pp. 141–
154.
[20] G. Jin, A. Thakur, B. Liblit, and S. Lu, “Instrumentation
and sampling strategies for cooperative concurrency bug
isolation,” in ACM OOPSLA, 2010, pp. 241–255.
[21] P. Fonseca, C. Li, V. Singhal, and R. Rodrigues, “A study
of the internal and external effects of concurrency bugs,” in
IEEE DSN, 2010, pp. 221–230.
[22] R. Halpert, C. Pickett, and C. Verbrugge, “Component-based
lock allocation,” in IEEE PACT, 2007.
[23] M. Xu, R. Bod´ık, and M. Hill, “A serializability violation
detector for shared-memory server programs,” in ACM PLDI,
2005, pp. 1–14.
[24] E. Farchi, Y. Nir, and S. Ur, “Concurrent bug patterns and
how to test them,” in IEEE IPDPS, 2003, pp. 286–293.