### Bug Replay Capacity of the Similarity-Guided Approach

**Figure 3.** SPE dispersion ratios for the ConTest benchmark programs (sorted in ascending order) when logging all the SPEs of the program.

The Manager program exhibits a dispersion ratio of 1 or very close to 1 for all its SPEs, indicating that almost all recorded executions had different thread interleavings. These conditions are particularly challenging for the partial logging approach, which, as shown in Table III, failed to replay the bug. BoundedBuffer shows a similar pattern, but with some SPEs having a dispersion ratio less than 0.8, making it possible to reproduce the error even when logging 50% of the data using Dispersion-based Similarity.

For the TwoStage application, unusual results were observed with Plain Similarity. The bug was not replayed when more information was logged. This is due to the SPE dispersion ratios: out of four SPEs, two were always identical (SPE 0 and 1), one had very few equal access vectors (SPE 2), and the last one was always different (SPE 3). Let's discuss the three partial logging scenarios with Plain Similarity:

- **75% of SPEs:** Each partial log consisted of three SPEs, resulting in a list of base partial logs composed of groups with specific characteristics.

### Tomcat#37458 Bug Replaying

**Table IV.** Number of attempts required by the heuristic to replay the Tomcat#37458 bug.

The Merge heuristic was tested on bug #37458 of Tomcat v5.5, a NullPointerException resulting from a data race. CoopREP logged only 15 out of 32 SPEs, as the available unit test triggered only a subset of SPEs. Despite the availability of a targeted unit test, triggering the bug required an average of 112 attempts.

Table IV shows the number of attempts required by the Similarity-Guided Merge heuristic (using both Plain Similarity and Dispersion-based Similarity) to replay the Tomcat#37458 bug when logging from 10% to 75% of the SPEs. The data confirms the effectiveness of the proposed statistical analysis techniques, especially Dispersion-based Similarity, which successfully replayed the bug in fewer than 10 attempts even when logging accesses to only 10% of the SPEs.

### Performance Overhead

#### 1. ConTest Benchmark

**Figure 4.** Performance overheads for the ConTest benchmark programs.

CoopREP achieved lower runtime degradation than LEAP across all tested programs. The overhead reductions are not linear due to the varying frequency of SPE accesses. The advantage of partial logging is most significant for applications with a high number of SPE accesses, such as BubbleSort and BufferWriter. For example, in BubbleSort, LEAP imposed a 26% performance overhead, while CoopREP imposed only a 7% overhead when recording 10% of the total SPEs, sufficient to replay the bug. Overall, logging up to 25% of the SPEs consistently resulted in a runtime penalty below 10%, a threshold typically considered acceptable for real-world applications.

#### 2. Java Grande Forum

**Table V.** Description of the Java Grande Forum benchmark programs used in the experiments.

The Java Grande Forum benchmark includes computationally intensive science and engineering applications. Although it does not have known bugs, it was used to assess the benefits and limitations of CoopREP compared to LEAP in demanding computing environments. Table V describes the benchmark programs in terms of the number of SPEs and their overall access counts.

**Table VI.** Performance overheads for the Java Grande Forum benchmark programs.

The results show that CoopLEAP can dramatically reduce logging overhead, especially for memory-intensive applications like RayTracer and SparseMatmult. For RayTracer, the average logging overhead dropped by a factor of approximately 50x when logging 10% of the SPEs. Similar trends were observed for other benchmarks, providing additional evidence of the significant performance gains achievable with cooperative logging schemes.

### Log Sizes

#### 1. ConTest Benchmark

**Figure 5.** Log size ratios for the ConTest benchmark programs.

The log size ratios follow a trend similar to the performance overhead plots. The Manager benchmark showed a reduction ratio closest to the expected proportional decrease in recording percentage. BubbleSort exhibited a faster decrease in log sizes even for small reductions in the number of logged SPEs. However, the reduction was not significant when logging less than 50% of the SPEs, likely due to the largest fraction of accesses being confined to a single SPE. Piper, on the other hand, showed smaller reductions in log sizes when decreasing the percentage of logged SPEs, attributed to the heterogeneity in the size of the access vectors associated with the various SPEs.

#### 2. Java Grande Forum

**Figure 6.** Log size ratios for the Java Grande benchmark programs.

Partial logging benefits are evident, especially in SOR, where logging less than 25% of the SPEs resulted in log sizes accounting for at most 0.1% of LEAP’s log size. The high heterogeneity in the size of the access vectors significantly influenced the actual reduction in log sizes. The instrumentation approach, which is purely random, means that the load in terms of logged SPE accesses may not be equally distributed among different runs, leading to non-linear reductions in both performance overhead and log size.

### Conclusions

This paper introduces CoopREP, a system for fault replication in concurrent programs through cooperative recording and partial log combination. CoopREP achieves significant reductions in overhead compared to conventional deterministic execution replays by allowing each program instance to trace only a subset of its shared programming elements. The evaluation, conducted with third-party benchmarks and a real-world application, highlights the effectiveness of the technique in replaying non-trivial concurrency bugs and its performance advantages over non-cooperative logging schemes.

Future research directions include designing additional partial logging schemes, considering load balancing or locality of SPEs, and developing new similarity metrics, such as those using Euclidean or edit distances between access vectors.

### Acknowledgments

The authors thank the anonymous reviewers for their valuable feedback and suggestions, and Jeff Huang for his assistance with LEAP. This work was partially supported by FCT (INESC-ID multi-annual funding) through the PIDDAC program funds and by the European project “FastFix” (FP7-ICT-2009-5).

### References

[1] Z. Li, L. Tan, X. Wang, S. Lu, Y. Zhou, and C. Zhai, “Have things changed now?: an empirical study of bug characteristics in modern open source software,” in ACM ASID, 2006, pp. 25–33.
[2] A. Hall, “Realising the benefits of formal methods,” Journal of Universal Computer Science, vol. 13, no. 5, pp. 669–678, 2007.
[3] D. Parnas, “Really rethinking ‘formal methods’,” Computer, vol. 43, pp. 28–34, 2010.
[4] S. Park, Y. Zhou, W. Xiong, Z. Yin, R. Kaushik, K. Lee, and S. Lu, “Pres: probabilistic replay with execution sketching on multiprocessors,” in ACM SOSP, 2009, pp. 177–192.
[5] G. Dunlap, D. Lucchetti, M. Fetterman, and P. Chen, “Execution replay of multiprocessor virtual machines,” in ACM VEE, 2008, pp. 121–130.
[6] A. Georges, M. Christiaens, M. Ronsse, and K. De Bosschere, “Jarec: a portable record/replay environment for multithreaded java applications,” Software Practice and Experience, vol. 40, pp. 523–547, May 2004.
[7] J.-D. Choi and H. Srinivasan, “Deterministic replay of java multithreaded applications,” in ACM SPDT, 1998, pp. 48–59.
[8] T. LeBlanc and J. Mellor-Crummey, “Debugging parallel programs with instant replay,” IEEE Trans. Comput., vol. 36, pp. 471–482, April 1987.
[9] J. Huang, P. Liu, and C. Zhang, “Leap: lightweight deterministic multi-processor replay of concurrent java programs,” in ACM FSE, 2010, pp. 385–386.
[10] G. Pokam, C. Pereira, K. Danne, L. Yang, and J. Torrellas, “Hardware and software approaches for deterministic multiprocessor replay of concurrent programs,” Intel Technology Journal, vol. 13, pp. 20–41, 2009.
[11] L. Lamport, “Ti clocks, and the ordering of events in a distributed system,” Commun. ACM, vol. 21, pp. 558–565, July 1978.
[12] S. Srinivasan, S. Kandula, C. Andrews, and Y. Zhou, “Flashback: A lightweight extension for rollback and deterministic replay for software debugging,” in USENIX Annual Technical Conference, 2004, pp. 29–44.
[13] M. Xu, R. Bodik, and M. Hill, “A ‘flight data recorder’ for enabling full-system multiprocessor deterministic replay,” in ISCA. ACM, 2003, pp. 122–135.
[14] S. Narayanasamy, G. Pokam, and B. Calder, “Bugnet: Continuously recording program execution for deterministic replay debugging,” in IEEE ISCA, 2005, pp. 284–295.
[15] P. Montesinos, L. Ceze, and J. Torrellas, “Delorean: Recording and deterministically replaying shared-memory multiprocessor execution efficiently,” in IEEE ISCA, 2008, pp. 123–134.
[16] P. Montesinos, M. Hicks, S. King, and J. Torrellas, “Capo: a software-hardware interface for practical deterministic multiprocessor replay,” in ACM ASPLOS, 2009, pp. 73–84.
[17] G. Altekar and I. Stoica, “Odr: output-deterministic replay for multicore debugging,” in ACM SOSP, 2009, pp. 193–206.
[18] C. Zamfir and G. Candea, “Execution synthesis: a technique for automated software debugging,” in ACM EuroSys, 2010, pp. 321–334.
[19] B. Liblit, A. Aiken, A. Zheng, and M. Jordan, “Bug isolation via remote program sampling,” in ACM PLDI, 2003, pp. 141–154.
[20] G. Jin, A. Thakur, B. Liblit, and S. Lu, “Instrumentation and sampling strategies for cooperative concurrency bug isolation,” in ACM OOPSLA, 2010, pp. 241–255.
[21] P. Fonseca, C. Li, V. Singhal, and R. Rodrigues, “A study of the internal and external effects of concurrency bugs,” in IEEE DSN, 2010, pp. 221–230.
[22] R. Halpert, C. Pickett, and C. Verbrugge, “Component-based lock allocation,” in IEEE PACT, 2007.
[23] M. Xu, R. Bodík, and M. Hill, “A serializability violation detector for shared-memory server programs,” in ACM PLDI, 2005, pp. 1–14.
[24] E. Farchi, Y. Nir, and S. Ur, “Concurrent bug patterns and how to test them,” in IEEE IPDPS, 2003, pp. 286–293.