means  a  sequence  of  human  errors,  first  in  creating  a 
defect, then  in inspection, testing and debugging, allowing 
the  defect to  go  unnoticed,  or  to  be  only  partially  fixed. 
(The  concept  of  a  "fault" 
loaded  with 
implicit, 
unrealistic  assumptions  [12],  but  we  use 
it  as  a 
convenient  simplification.  "The  i-th  fault  is  present  in 
version  A" is  a  convenient shorthand  for  "in  version  A, 
the  i-th  potential  failure  region  is  actually  a  failure 
region"). Errors in the development process select  some  of 
the  faults,  at  random.  Some  faults  are  more  likely  than 
others to be chosen; some failure regions are  "larger"  than 
others,  in  the  sense  that  the  probability  of  a  demand  in 
these  regions  is  higher.  Developing versions  for  a  given 
application  with  separate  developments  means  choosing 
subsets  of  this  set  of  possible  faults,  randomly  and 
independently.  Note  that  assuming  "independent  choice" 
does not  imply  that  the  versions  will  fail  independently, 
or that there  are no  common  factors  affecting  mistakes  in 
both 
fault 
similarities  are possible, and modelled by  the  probabilities 
of  the  various sets  of  faults. This  is  indeed  the  essential 
insight of the EL and LM models. 
developments: 
correlation 
is 
failure 
and 
var2 
1 
2 
3 
4
5
vlr 1 
Fig.  2.  An  example  of  failure  regions  in  a  2-dimensional  demand  space.  Various  authors  h a v e  
reported  shapes  of  failure  regions  in  actual  programs  [9,  10,  111,  including  non-intuitive  shapes, 
e.g.  non-connected  regions  like  arrays  of  separate  points  or  lines. 
7 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:33 UTC from IEEE Xplore.  Restrictions apply. 
We  thus  have  a  collection  of  potential  faults  and 
associated failure regions,  { F,, F2, ..., Fn}. Each  one, e.g., 
F,, has  a certain  probability p ,  of  being  actually  produced 
in a newly developed version (following  [3], [4] again,  we 
can  think  of  the  process  of  producing  a  version  as 
sampling  from  a distribution of  possible  versions).  It  is 
also characterised  by its probability  q, of  being  'hit'  during 
operation, i.e., its  contribution  to  the  unreliability  of  the 
system. 
We  assume 
to  be  non- 
overlapping: the PFD of a version  is given  by  the  sum  of 
the q, values of those faults that are actually present. 
these  failure 
regions 
reliability 
the  average 
So  far,  this  model  is  the  same  as  the  EL  and  LM 
models,  except  in  being  "coarser-grained":  we  consider 
whole failure regions  rather  than  individual  failure  points. 
The  conclusions  of  the  EL  and  LM  models  about  the 
average  PFD  of  a  two-version  system  (greater  than  the 
product  of the versions' average PFDs) are easily re-derived 
here.  But 
is  not  especially 
interesting  in  practical  decision-making:  we  need  some 
idea  of  the  probability  of  achieving  a  given  reliability, 
i.e.,  about  probability  distributions  rather  than  averages. 
Such predictions cannot be  obtained  from the  EL and LM 
models,  because  their  parameters  do  not  describe  how 
likely  it is that a version has a certain set  of  failure points 
or failure regions: they  only  describe each  failure  point  in 
isolation.  We  need  to  add  to  the  model  some  further 
information, and we add  the  assumption  that  the  mistakes 
are statistically  independent  of  each  other.  It  is  as though 
the  design  team,  faced with  the  possibility  of  inserting  a 
fault, tossed dice to decide whether to insert it or not. 
These  additional  assumptions  -  one-to-one  mapping 
between faults and failure  regions,  non-overlapping failure 
regions,  and  independent  introduction  of  faults  - which, 
incidentally, are shared by  most  other models of  software 
failure  processes  in  the  literature -  are  obviously  false. 
However,  I)  we  believe  that  they  do  not  make  a  big 
difference  on  the  main  useful  results  of  the  model,  and 
will  argue  this  thesis  later;  and  2) it  is  easier  to  check 
and  refute  empirically  whether 
they  are  acceptable 
approximations of reality  than  it  was  for the  assumptions 
used  in  previous  arguments  about  diversity.  So,  we  ask 
the reader  to  accept them  and follow  us  in  examining the 
implications of this model. 
3. The PFD of one-version and of two-version 
systems 
In this model, the PFD for a  version  or  system  is  the 
sum  of  many  independent  random  variables, 
the 
contributions of  the  individual  potential  faults.  The mean 
and the variance of this sum are then  equal  to  the  sums of 
i.e., 
the means and of  the  variances  of  the  individual  random 
variables, respectively. The i-th  random  variable  takes the 
value  q,  with  probability  p ,   and  the  value  0  with 
probability  (I-  p,), when  we  consider  a  single  version. 
and  (I-p,')  for  a  two- 
These  probabilities  become 
consider 
version 
independent 
developments of the two versions. Thus: 
since  we 
system, 
I1 
E ( @ , ) = C p ,  411 
r = l  
E(@*) = &Pi)*%> 
(1) 
r = l  
Given n  potential  faults,  we  have  a  model  with  2n 
parameters. All parameters are unknown  and unmeasurable 
in practice.  So, direct use  of  these formulas is  out  of  the 
question. Despite this,  we now  proceed to  use  this  model 
in  a  way  that  has  practical  value,  by  selecting  special 
cases of interest in which the study of the model  is  simple 
and does not  require detailed  knowledge  of  all  parameters. 
We  consider  two  measures  of  reliability,  which  have 
practical  relevance  in  different  scenarios.  These are  at  the 
two ends of a spectrum of possible scenarios: 
- 
some programs (e.g. in  some  safety  systems) are  very 
simple and developed  to  high  standards:  it  is  plausible 
that they often contain no  fault.  The expected  value of 
the  number  of  faults  is  close to  0,  and  all  the  p ,  are 
close  to  0.  There  are  only 
two  events  with  non- 
negligible  probability:  having  zero common  faults  or 
having  one  common  fault.  However,  even  one  fault 
(common  to  the  two  versions)  may  be  enough  to 
violate  the  system  dependability  requirements.  So,  we 
are  effectively  interested  in 
the  probability  of  the 
versions  having no common fault. 
'there  are  very  many  possible  faults,  and  many  have 
small  q, compared to  the  acceptable system  PFD.  We 
are  then  interested  in  the  probability  of  the  system 
PFD not exceeding a required bound  t 9 ~ ,
 or vice-versa 
in  which  bound  will  not  be  exceeded  with  a  set 
probability.  E.g.,  we  might  ask  what  is  the  99th 
percentile of  the  distribution of  the  system  PFD  (i.e., 
an upper  bound  such  that  the  system  PFD  has  99% 
probability  of  not  exceeding  it).  For  this  scenario, we 
will exploit the fact  that  the  PFD  of  our systems  is  a 
sum  of  independent  variables 
to  approximate  the 
the  PFD  with  a  normal  (Gauss) 
distribution  of 
distribution, according to the central limit  theorem. As 
this  is  an  asymptotic  result,  we  will  not  know  in 
practice how good an approximation it is in  a  specific 
case, but  this  simplification  is  useful  for  studying the 
important qualitative trends implied by our model. 
- 
8 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:33 UTC from IEEE Xplore.  Restrictions apply. 
E(@,): mean  values of  the  probability  of  failure  on  demand of  a randomly 
o(0,): standard  deviations  of  the  probability  of  failure  on  demand  of  a 
1  Probability  of failure on demand of a generic system, seen as a random variable 
I Probability  of  failure  on  demand  of  a  randomly  chosen  program  version,  and  of  a  1-out-of-2  two- 
version system, respectively, seen as random variables 
3 
Abbreviations for E(@,), 
Abbreviations for o(O,), 
randomly  chosen program version, and of a  1-out-of-2 two-version  system, respectively 
Standard deviation  of a random variable A 
Variance of a random variable A 
A required upper bound  on the probability of failure on demand 
"Cumulative distribution function" 
Mean (expected value) of the random variable A 
Number of potenrial faults and failure regions  in a program version 
Number of faults in a randomly  chosen program version,  and of  common  faults  in  a randomly  chosen 
pair of versions, seen as random variables 
Probability  of the i-th potential  fault being present  in a randomly  chosen program version 
nzax{P,! P,,  ...., P") 
Probability  of event described  by  (...) 
"Probabilitv of failure on demand" 
Probability  of  failure  per  demand  associated  with  the  i-th  potential  fault  and  failure  region  (i.e., 
probability  of a demand which is part of that failure region being presented to the system in operation) 
Mathematical  symbols  and  abbreviations  used  in  this  article 
E 
Table  1. 
3.1. Lemmas: considerations on the means and 
standard deviations of the PFD 
We briefly study these measures, in  part  to  be  able to 
compare  our  results  with  previous  studies,  but  also  to 
derive useful lemmas for the rest of the analysis. 
3.1.1. Comparison  between  the  mean  PFDs,  ,U, 
and p2. As indicated  above, 
P I   = C ~ i q i ,  P? = C Pi2qi 
(3) 
I1 
I1 
i = l  
We now define P,,,~,.~ =niax{p,, p,,  ,.,., p n } .  Then: 
i = l  
I1 
I1 
I1 
i = l  
i = l  
i = l  
PZ  = C Pi2qi  5 C Pmu.rPiqi  = Pmcrr C ~ i q i  = PmcrxPi  (4) 
Quality assurance activities try to reduce  the  values  of 
the p i  parameters.  Their  actual  values are  unknown,  but 
the meaning  of  these  parameters  is  clear,  and  the  values 
achieved by  software development processes can  be  studied 
empirically.  To  use  inequality  (4)  we  only  need  to 
estimate an upper bound. 
So,  if  an  assessor  were  convinced  that  a  developer's 
quality  assurance  activities  reduce  the  probability  of  the 
most common fault to, say,  IO%,  the assessor should  also 
believe that a two-version  system from that  developer  has, 
on  average,  at  least  10  times  better  PFD  than  a  single 
version.  This  may  be  a  modest  reliability  gain, 
in 
particular  compared with  claims  of  independence (for  this 
upper-bound  prediction  to  be equivalent  to  or  better  than 
independence,  we  would  need  pmox I p,), but 
is  an 
indisputable upper bound (on the average unreliability). 
the 
between 
3.1.2. Comparison 
standard 
deviations  of  the  PFD,  01  and  02. We have  seen 
in (2) that: 
o2 (01) = C Pi( 1 - ~i)d 
02(0,) = 2 P:(l-  p?)q? 
(5) 
(6) 
i = l  
f I  
i=  I 
It can be shown that 
p2( 1 -p2)Sp( 1 -p), iffp-O), P(O,>O,  vs.  that  in  a 
one-version  system:  P(N,>O),  P(Ol>O)l. The  smaller  the 
ratio, the greater the gain given by diversity.  We have: 
P(N,=O)=rI(l- p i )  ,  P(N,=O)=rI(l-p,Z). 
Therefore, 
i=l 
4.2. Effects of an improved process 
The  first  question  to  address  is  what  we  mean  by 
the 
development 
Changing 
process. 