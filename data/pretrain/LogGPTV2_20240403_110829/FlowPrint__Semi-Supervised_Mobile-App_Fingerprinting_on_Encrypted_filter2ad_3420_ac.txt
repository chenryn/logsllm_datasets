Equation 4.
(cid:80)T
(cid:80)T
t=0 ci[t] · cj[t]
t=0 max(ci[t], cj[t])
(ci (cid:63) cj)norm =
(4)
Using the cross-correlation metric between each cluster, we
construct a correlation graph with each node in this graph
representing a cluster. Clusters are connected through weighted
edges where the weight of each edge deﬁnes the cross-
correlation between two clusters. Figure 3 shows the corre-
lation graph of three selected apps as an example. We see that
clusters belonging to the same app demonstrate a strong cross-
correlation. In addition, shared clusters show weak correlation
between all apps and most of the unique clusters are not
correlated at all.
E. App Fingerprints
To construct app ﬁngerprints we identify maximal cliques,
i.e., complete subgraphs, of strongly correlated clusters in the
correlation graph. To discover such cliques, we ﬁrst remove all
edges with a weak cross-correlation. A cross-correlation is con-
sidered weak if it is lower than a threshold τcorrelation, which in
our approach is empirically set to 0.1 (see Section V-A). This
leaves us with a correlation graph containing only the strongly
correlated clusters. We then extract all maximal cliques from
this graph and transform each clique into a ﬁngerprint. As
all maximal cliques are complete subgraphs, the edges in the
clique do not add any additional information. This means
we can transform cliques into sets of network destinations
by extracting all (destination IP, destination port)-tuples and
TLS-certiﬁcates from every node in a clique and combine
them into a set. By performing this transformation for each
clique, we obtain all of our ﬁngerprints. In short, we deﬁne an
app ﬁngerprint as the set of network destinations that form a
maximal clique in the correlation graph.
As graph edges in the correlation graph depend on the
activity of a destination with other clusters, some of the nodes
are completely disconnected from the rest of the graph. This
is often the case for destinations that are shared among many
apps. Figure 3 shows an example where the shared (black)
nodes only have low cross correlations that fall under the
threshold. As these 1-cliques often correspond to multiple
apps, treating them as ﬁngerprints yields little added value.
However, they will most likely originate from the same app
for which we are able to produce ﬁngerprints during the batch
processing. Therefore, we assign ﬂows from 1-cliques to the
ﬁngerprint that is closest in time, or, if two ﬁngerprints are
equally close, to the ﬁngerprint containing the most ﬂows.
F. Fingerprint Comparison
The beneﬁt of using a ﬁngerprint to represent trafﬁc from
an app is that it can be computed from the features of the
network trafﬁc itself without any prior knowledge. Moreover,
we want to compare ﬁngerprints with each other to track
app activity over time. Unfortunately, apps communicate with
various sets of destinations at different times, either because
trafﬁc is based on user interaction, which is dynamic, or be-
cause apps produce distinct trafﬁc for different functionalities.
Consequently, ﬁngerprints of the same app can diverge to
various degrees. To account for this fact, we do not compare
ﬁngerprints as an exact match, but instead base their compar-
ison on the Jaccard similarity [35]. Since our ﬁngerprints are
sets, the Jaccard similarity is a natural metric to use. To test
whether two ﬁngerprints are similar, we compute the Jaccard
similarity between two ﬁngerprints Fa and Fb (displayed in
Equation 5) and check whether it is larger then a threshold
τsimilarity. If this is the case, we consider the two ﬁngerprints
to be the same.
J(Fa, Fb) =
|Fa ∩ Fb|
|Fa ∪ Fb|
(5)
By comparing ﬁngerprints in this way, we are able to track the
activity of apps between different input batches and executions
of the our approach. In addition, it automatically solves the
problem when we observe a ﬁngerprint where one edge of the
clique is missing because it did not make the threshold cutoff.
Especially when cliques become larger, the possibility of a
clique missing an edge increases. In such cases, our approach
would output multiple ﬁngerprints for the same app. If these
ﬁngerprints are similar, they can even be merged by taking
the union of ﬁngerprints. In addition, this comparison based
on the Jaccard similarity allows our approach to treat similar
ﬁngerprints as equivalent.
V. EVALUATION
We implemented a prototype of our approach, called
FLOWPRINT, in Python using the Scikit-learn [47] and Net-
workX [33] libraries for machine learning and graph compu-
tation. The ﬁrst experiment in our evaluation determines the
optimal parameters for our approach. Then, we analyze to
what extent the ﬁngerprints generated by our approach can
be used to precisely identify apps. Here, we compare our
approach against AppScanner [62], a supervised state-of-the-
art technique to recognize apps in network trafﬁc. Thereafter,
we evaluate how well our approach deals with previously
unseen apps, either through updates or newly installed apps.
We then detail speciﬁc aspects of our approach such as the
performance of the browser detector,
the conﬁdence level
of our ﬁngerprints, and the number of ﬁngerprints produced
per app. We further investigate how well our approach can
deal with the homogeneous, dynamic, and evolving nature
of mobile network trafﬁc. Finally, we discuss the impact of
the number of apps installed on the device and demonstrate
that our method is able to run in real-time by assessing the
execution time of FLOWPRINT.
Experimental setup. Our evaluation requires ground truth
labels, which for mobile apps can be acquired by installing
an intrusive monitoring agent on a real device, or by running
controlled experiments. Due to privacy concerns and to ensure
repeatability of our experiments, we evaluate FLOWPRINT
on the datasets described in Section II-A, which closely
approach an open world setting, containing encrypted data,
user-generated data, both Android and iOS apps, and different
app versions. As explained in Section IV, our approach does
not require any prior knowledge to generate ﬁngerprints, and
we leverage ground truth labels only to evaluate our prototype
(i.e., to assign app names to matched ﬁngerprints).
We split
the trafﬁc of each app in our datasets 50:50
into training and testing sets, without any overlap. For each
experiment, we build our database from the training data of
100 randomly chosen apps for each dataset. This leads to an
average of 2.0 ﬁngerprints per app for ReCon and Andrubis,
and 6.2 for the Cross Platform dataset. For the unseen app
detection, we additionally introduce trafﬁc from 20 randomly
chosen apps that are not present in the training set.
A. Parameter Selection
As detailed in the previous section, our approach requires
four conﬁgurable parameters to create ﬁngerprints:
•
•
•
•
τbatch sets the amount of time of each batch in a net-
work capture to process in each run of our approach.
τwindow speciﬁes the time window for destination clus-
ters to be considered active simultaneously.
τcorrelation describes the minimum amount of correlation
(ci (cid:63) cj)norm between two destination clusters to have
an edge in the correlation graph.
τsimilarity indicates the minimum required Jaccard simi-
larity between ﬁngerprints to be treated as equivalent.
Optimization metric. We optimize each parameter with re-
spect to the F1-score that our approach achieves when rec-
ognizing apps. This metric computes the harmonic mean
7
TABLE III.
SUMMARY OF TESTED PARAMETER OPTIMIZATION
VALUES. THE FIRST ROW SHOWS THE DEFAULT PARAMETERS AND EACH
SUBSEQUENT ROW HIGHLIGHTS THE OPTIMAL VALUES FOUND FOR EACH
INDIVIDUAL PARAMETER.
τbatch
3600
300
300
300
300
τwindow
τcorrelation
τsimilarity
5
5
30
30
30
0.3
0.3
0.3
0.1
0.1
0.5
0.5
0.5
0.5
0.9
F1-score
0.8164
0.8294
0.8367
0.8543
0.9190
between precision and recall and is often used to evaluate
security solutions. As we output ﬁngerprints, we need to map
them to app labels in order to evaluate our approach. Each
ﬁngerprint consists of ﬂows which, in our dataset, are labeled.
Hence, we label each ﬁngerprint with the ﬂow label that is
most commonly assigned to that ﬁngerprint. To illustrate this,
suppose ﬁngerprint F contains 10 ﬂows of app A and 2 ﬂows
of app B, all 12 ﬂows of that ﬁngerprint will be assigned the
label A. While this approach can generate multiple ﬁngerprints
per app (see Section V-D), many security applications (e.g.,
ﬁrewalls) use a mapping on top of ﬁngerprinting and allow
multiple ﬁngerprints for the same app.
Parameter selection. To optimize our parameters, we reﬁne
them individually to reach an optimal F1-score. We choose our
parameters from the following set of possible values:
τbatch: 1m, 5m, 10m, 30m, 1h, 3h, 6h, and 12h.
τwindow: 1s, 5s, 10s, 30s, 1m, 5m, 10m, and 30m.
τcorrelation: 0.1 to 1.0 in steps of 0.1.
τsimilarity: 0.1 to 1.0 in steps of 0.1.
•
•
•
•
The batch size thresholds vary between 1 minute, a scenario
where apps can be detected while they are still running, and
12 hours, representing a post-incident analysis. The window
thresholds vary between 1 second and 30 minutes, where
smaller values may miss ﬂow correlations and larger values
may correlate ﬂows that were accidentally active around the
same time period. Both correlation and similarity thresholds
are evenly spread out between 0.1 and 1.0, the ﬁrst and max
values that trigger the corresponding ﬁngerprint mechanism.
For each parameter we vary the value by iterating over the
test set of possible values while keeping the other parameters
as their default value. Once we ﬁnd an optimal value for a
parameter, it is set as the new default for optimizing the other
parameters. This way of iterating through the values allows
us to capture dependencies between the parameters. To get an
average result, we perform a 10-fold cross validation analysis
for each setting on held-out validation data from the Andrubis
dataset. This held-out data is not used in the remainder of the
evaluation to remove bias from this optimization step. We opt
to optimize the parameters using only the Andrubis dataset
to ensure all datasets contain enough testing data to evaluate
our approach. While this may bias the optimal parameters to
a speciﬁc dataset, our results in the remainder of this section
show that the parameters also generalize well to other datasets.
During the experiment, we assume that each device has 100
apps installed, which resembles a realistic setting [10]. We
also performed the same evaluation with 200 apps per device,
which resulted in the same optimal parameters.
8
As shown in Table III, we ﬁnd optimal values for τbatch =
300 seconds, τwindow = 30 seconds, τcorrelation = 0.1 and
τsimilarity = 0.9 from this analysis.2 One interesting observation
is that the optimal value for τbatch is found at 300 seconds.
This means that
it may take up to ﬁve minutes before a
ﬂow is assigned to a ﬁngerprint. In settings that require faster
ﬁngerprint generation, operators can of course set a lower τbatch
value, however at the cost of a lower performance.
B. App Recognition
Many security solutions use a ﬁngerprinting method for
the purpose of app recognition [15, 57, 62]. To evaluate the
extent to which our approach recognizes apps within network
trafﬁc, we create ﬁngerprints of labeled training data. Then,
we label each ﬁngerprint with the app label most commonly
assigned to ﬂows within the ﬁngerprint, i.e., we perform a
majority vote. After obtaining the labeled ﬁngerprints we run
our approach with the test data. We then compare the resulting
test ﬁngerprints with the labeled training ﬁngerprints using the
Jaccard similarity, as detailed in Section IV-F. Subsequently,
each test ﬁngerprint, and by inference each ﬂow belonging to
that test ﬁngerprint, receives the same label as the training
ﬁngerprint that is most similar to it.
We compare our approach with the state-of-the-art tool
AppScanner [62, 63]. However, the authors of AppScanner
only released precomputed length statistics about the ﬂows in
their dataset and the code for running the classiﬁcation phase
on such preprocessed statistics. Therefore, to be able compare
both approaches on the same datasets, we faithfully reim-
plemented the AppScanner feature extraction strategy, which
reads PCAP ﬁles and feeds the feature values to the classiﬁer.3
To do so, we followed the description in the AppScanner paper
for computing feature statistics, using the standard NumPy [45]
and Pandas [43] libraries. AppScanner has different settings, it
can either work with a Support Vector Classiﬁer or a Random
Forest Classiﬁer. We evaluate AppScanner with a single large
Random Forest Classiﬁer, which achieved the highest perfor-
mance in AppScanner’s evaluation. In addition, AppScanner
requires a parameter that sets the minimum conﬁdence level
for recognition. The optimal conﬁdence level according to
the original paper is 0.7, hence this is what we used in our
evaluation. Lowering this threshold increases the recall and
decreases the precision of AppScanner.
Comparison with AppScanner. We evaluate FLOWPRINT
against AppScanner by running a 10-fold cross validation on
the same datasets discussed in Section II-A. Additionally, we
measure to what extent the performance of our approach is
affected by the number of ﬂows produced per app. As apps
in the Andrubis dataset produce a varying amount of data,
we evaluated the performance considering only apps having
a minimum of x ﬂows. This resulted in ﬁve evaluations for
x = 1, i.e., all apps, x = 10, x = 100, x = 500, and
x = 1000. We refer to these evaluations as Andrubis ≥ x
ﬂow(s) for each respective value of x. All experiments assumed
a maximum of 100 active apps per device in accordance with
recent statistics [10].
2Due to space limitations we provide additional results about the parameter
selection at https://github.com/Thijsvanede/FlowPrint.
3We release our implementation of AppScanner at https://github.com/
Thijsvanede/AppScanner.
TABLE IV.
PERFORMANCE OF OUR APPROACH COMPARED TO APPSCANNER IN THE APP RECOGNITION EXPERIMENT. THE NUMBER OF FLOWS SHOWN
FOR THE ANDRUBIS DATASET INDICATE THE MINIMUM NUMBER OF REQUIRED FLOWS AN APP HAD TO PRODUCE TO BE INCLUDED IN THE EXPERIMENT.
Dataset
ReCon
ReCon extended
Cross Platform (Android)
Cross Platform (iOS)
Cross Platform (Average)
Andrubis (≥ 1 ﬂow)
Andrubis (≥ 10 ﬂows)
Andrubis (≥ 100 ﬂows)
Andrubis (≥ 500 ﬂows)
Andrubis (≥ 1000 ﬂows)
Precision
0.9470
0.8984
0.9007
0.9438
0.9191
0.5842
0.5439
0.7617
0.7389
0.8021
FLOWPRINT
Recall
F1-score
0.9447
0.8922
0.8698
0.9254
0.8923
0.5871
0.5031
0.6852
0.7413
0.8111
0.9458
0.8953
0.8702
0.9260
0.8917
0.5856
0.5227
0.7214
0.7401
0.8066
Accuracy
0.9447
0.8922
0.8698
0.9254
0.8923
0.5871
0.5031
0.6852
0.7413
0.8111
AppScanner (Single Large Random Forest)
Accuracy
Precision
F1-score
Recall
0.8960
0.9365
0.9108
0.8538
0.8791
0.6270
0.6069
0.8520
0.8663
0.9141