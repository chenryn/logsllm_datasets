s (x0) of the classifier
The attacker tries to decrease (resp. increase) all Fs values of
labels in L+ (resp. in L−). Specifically, for a given point x0 and
a pair of parameters α, β, the attacker tries to find a sample x
nearby x0 such that,
• For all label l in L+, F l
• For all label l in L−, F l
s (x)  β.
Formally, the attack can be formulated with the following
optimization problem.
∥ d ∥
minimize
s (x0 + d)  β for all l ∈ L−
F l
= F l
A.2 Generalizing the Solution of 3
Again, let di = x− xi be the perturbation to introduce at itera-
s (xi ) be the current belief probability of
tion i, and let p (l )
i
xi as in class l. Similarly, denote g(l )
s (xi ) be the gradient
i
s at xi. Finally, denote δ (l )
of F l
i +1 be the decrease
= p (l )
i
of probability in class l.
From equation 4, for each label l ∈ L+∪L−, our perturbation
di should satisfy the following equation,
= ∇F l
i − p (l )
i
= −δ (l )
i
· di = p (l )
i +1 − p (l )
g(l )
i
(7)
In other words, di is a solution to a linear system consisting
of (m + n) equations. For notational convenience, we assign
numbers 1,2, . . . ,m + n to each label l ∈ L+ ∪ L−. Then, we
can define a matrix Gi whose jth row is g(j )
, and a vector δi
i
whose jth element is −δ (j )
i
g(1)
i
g(2)
i
...
, for j = 1,2, . . . ,m + n.
−δ (1)
i−δ (2)
i
...
Gi =
δi =
(8)
g(m+n)
i
−δ (m+n)
i
(cid:42)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:46)(cid:44)
(cid:43)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:47)(cid:45)
Now, we have the following linear system determining di.
Gi · di = δi
(9)
(10)
In usual, g(j )
i
’s are gradient vectors in a high dimensional
vector space, and (m + n) is much smaller than the dimension.
Therefore, the linear system (9) is underdetermined. For an un-
derdetermined linear system, if Gi is of full rank and δ (j )
’s are
fixed for all j, then the min-norm solution d∗
i
can be computed
as,
i
d∗
i = G
†
i · δi
†
where G
is the Moore-Penrose pseudoinverse of Gi. This
solution d∗
i
satisfies all constraints of the form (7) for each
i
label l, and achieves the minimum norm among vectors in the
solution set. In this section, we assume that Gi is always of
full rank, and the solution to the other case will be discussed
at §A.3. As long as Gi is of full rank, G
can be defined, so
the only remaining task to compute (10) is to determine each
values in δi.
†
i
its corresponding δ (l )
i
A.2.1 Trivial Constraints on δi . For any lable l ∈ L+ ∪ L−,
should satisfy the following condition.
i −
= p (l )
i +1 ≤
• If l ∈ L+, we want to decrease p (l )
i
i +1 ≥ 0
p (l )
• If l ∈ L−, we want to increase p (l )
i
0
i −p (l )
, so δ (l )
i
, so δ (l )
i
= p (l )
Also,
• If l ∈ L+, it suffices to get p (l )
i +1  β, so we get an upper
bound of the increase −δ (l )
,
i
−δ (l )
i ≤ max{0, β − p (l )
i
}
Combining the bounds (11) and (13), we get the following rules
for label l at iteration i. These rules are designed
to set δ (l )
i
to maximize the amount of possible change, i.e. |δ (l )
|, while
i
satisfying (11) and (13).
= 0
• If l ∈ L+,
– If p (l )
i  β, we don’t need to increase this probability.
Set δ (l )
i
ησmin∥ x0 ∥
m+n
= min{p (l )
i − α ,
= 0
√
}
– If not, choose δ (l )
i
= max{p (l )
i − β,− ησmin∥ x0 ∥
√
m+n
}
A.3 What if Gi has some linearly
dependent rows?
i
i
i
.
†
i
· d = 0 and g(l2)
i
. However, we can control the values of δ (l )
i
In usual, Gi is not guaranteed to be a full-rank matrix, and
may contain linearly dependent rows, making it impossible
to define G
to
resolve the dependency. In this section, we will present how
to remove dependencies by setting δ (l )
i
Without loss of generality, assume we have two dependent
rows in Gi, correnponding to labels l1 and l2. Then, there is a
= c · g(l2)
nonzero constant c such that g(l1)
. One naïve choice
i
to remove dependecy is just setting δ (l1)
= 0. Then, the
= δ (l2)
i
· d = 0 become equivalent,
constraints g(l1)
thus removing one of the dependent rows from Gi does not
change the solution set. Then, the algorithm proceeds with no
changes of probabilities in those labels, hoping that g(l1)
i +1 and
i +1 will become linearly independent at the next iteration.
g(l2)
The better solution is to make use of the constant factor c to
and δ (l2)
define another constraint between δ (l1)
. In previous
= c · δ (l2)
setting of two dependent labels, if we set δ (l1)
,
i
then the constraints g(l1)
are
equivalent upto multiplying constant c, so it is again safe to
ignore one dependent row from Gi as it does not change the
solution set.
’s for two labels l1 and l2
to remove dependencies, while the added constraint is satisfied.
There are several cases depending on the membership of labels
l1, l2 and the sign of c.
Now, we summarize how to set δ (l )
i
i
· d = δ (l1)
i
and g(l2)
· d = δ (l2)
i
i
i
i
i
i
i
, we set δ (l1)
> β), then δ (l1)
 0,
• If p (l1)
= 0 by our
i
rules at §A.2.2. To satisfy the dependency condition
δ (l1)
i
• If p (l2)
= 0 by our
i
rules at §A.2.2. To satisfy the dependency condition
δ (l1)
i
the δ (l )
i
satisfying the constraint, then use δ (l1)
to determine the other value.
• If both probabilities can be changed, we first set
| possible while
= c · δ (l2)
,
= δ (l2)
i
with larger change |δ (l )
i
= 0
i
> β), then δ (l2)
, we set δ (l1)
= δ (l2)
= 0