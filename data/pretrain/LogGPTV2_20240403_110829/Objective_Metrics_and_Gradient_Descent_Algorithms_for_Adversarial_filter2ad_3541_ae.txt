### Optimizing the Text

#### 1. **Attacker's Objective and Constraints**

The attacker aims to decrease (or increase) the values of \( F_s(x) \) for labels in \( L^+ \) (or \( L^- \)). Specifically, given a point \( x_0 \) and parameters \( \alpha \) and \( \beta \), the attacker seeks a sample \( x \) near \( x_0 \) such that:
- For all labels \( l \in L^+ \), \( F_l(x) \leq \alpha \).
- For all labels \( l \in L^- \), \( F_l(x) \geq \beta \).

Formally, the attack can be formulated as an optimization problem:
\[
\minimize_{d} \| d \|
\]
subject to:
\[
F_l(x_0 + d) \leq \alpha \quad \text{for all } l \in L^+
\]
\[
F_l(x_0 + d) \geq \beta \quad \text{for all } l \in L^-
\]

#### 2. **Generalizing the Solution**

Let \( d_i = x - x_i \) be the perturbation introduced at iteration \( i \). Let \( p_i(l) \) be the current belief probability that \( x_i \) belongs to class \( l \). Denote \( g_i(l) \) as the gradient of \( F_l \) at \( x_i \). Finally, let \( \delta_i(l) \) be the decrease in the probability of class \( l \) from iteration \( i \) to \( i+1 \).

From Equation 4, for each label \( l \in L^+ \cup L^- \), the perturbation \( d_i \) should satisfy:
\[
g_i(l) \cdot d_i = p_i(l) - p_{i+1}(l)
\]
\[
\Rightarrow g_i(l) \cdot d_i = -\delta_i(l)
\]
(7)

In other words, \( d_i \) is a solution to a linear system consisting of \( m + n \) equations. For notational convenience, we assign numbers \( 1, 2, \ldots, m+n \) to each label \( l \in L^+ \cup L^- \). We define a matrix \( G_i \) whose \( j \)-th row is \( g_i(j) \), and a vector \( \delta_i \) whose \( j \)-th element is \( -\delta_i(j) \):
\[
G_i = \begin{pmatrix}
g_i(1) \\
g_i(2) \\
\vdots \\
g_i(m+n)
\end{pmatrix}, \quad
\delta_i = \begin{pmatrix}
-\delta_i(1) \\
-\delta_i(2) \\
\vdots \\
-\delta_i(m+n)
\end{pmatrix}
\]
(8)

Now, we have the following linear system determining \( d_i \):
\[
G_i \cdot d_i = \delta_i
\]
(9)

Typically, \( g_i(j) \) are gradient vectors in a high-dimensional space, and \( m + n \) is much smaller than the dimension. Therefore, the linear system (9) is underdetermined. If \( G_i \) is of full rank and \( \delta_i(j) \) are fixed for all \( j \), the min-norm solution \( d_i^* \) can be computed as:
\[
d_i^* = G_i^\dagger \cdot \delta_i
\]
where \( G_i^\dagger \) is the Moore-Penrose pseudoinverse of \( G_i \). This solution \( d_i^* \) satisfies all constraints of the form (7) for each label \( l \) and achieves the minimum norm among vectors in the solution set. In this section, we assume \( G_i \) is always of full rank; the solution for the case when \( G_i \) is not of full rank will be discussed in §A.3.

#### 3. **Trivial Constraints on \( \delta_i \)**

For any label \( l \in L^+ \cup L^- \), \( \delta_i(l) \) should satisfy the following conditions:
- If \( l \in L^+ \), we want to decrease \( p_i(l) \), so \( \delta_i(l) \geq 0 \).
- If \( l \in L^- \), we want to increase \( p_i(l) \), so \( \delta_i(l) \leq 0 \).

Additionally:
- If \( l \in L^+ \), it suffices to get \( p_{i+1}(l) \leq \alpha \), so we get an upper bound on the increase: \( -\delta_i(l) \leq \max\{0, \alpha - p_i(l)\} \).
- If \( l \in L^- \), it suffices to get \( p_{i+1}(l) \geq \beta \), so we get a lower bound on the decrease: \( -\delta_i(l) \geq \min\{0, \beta - p_i(l)\} \).

Combining these bounds, we get the following rules for setting \( \delta_i(l) \) to maximize the possible change while satisfying the constraints:
- If \( l \in L^+ \):
  - If \( p_i(l) \leq \alpha \), set \( \delta_i(l) = 0 \).
  - Otherwise, choose \( \delta_i(l) = \min\{p_i(l) - \alpha, \eta \sigma_{\min} \frac{\| x_0 \|}{\sqrt{m+n}}\} \).
- If \( l \in L^- \):
  - If \( p_i(l) \geq \beta \), set \( \delta_i(l) = 0 \).
  - Otherwise, choose \( \delta_i(l) = \max\{p_i(l) - \beta, -\eta \sigma_{\min} \frac{\| x_0 \|}{\sqrt{m+n}}\} \).

#### 4. **Handling Linearly Dependent Rows in \( G_i \)**

In general, \( G_i \) may not be a full-rank matrix and may contain linearly dependent rows, making it impossible to define \( G_i^\dagger \). To resolve this, we can control the values of \( \delta_i(l) \).

Assume \( G_i \) has two dependent rows corresponding to labels \( l_1 \) and \( l_2 \). There exists a nonzero constant \( c \) such that \( g_i(l_1) = c \cdot g_i(l_2) \). One naïve choice to remove the dependency is to set \( \delta_i(l_1) = 0 \). Then, the constraints \( g_i(l_1) \cdot d_i = \delta_i(l_1) \) and \( g_i(l_2) \cdot d_i = \delta_i(l_2) \) become equivalent, and removing one of the dependent rows does not change the solution set.

A better solution is to use the constant factor \( c \) to define another constraint between \( \delta_i(l_1) \) and \( \delta_i(l_2) \). If we set \( \delta_i(l_1) = c \cdot \delta_i(l_2) \), the constraints are equivalent up to a constant factor, and it is safe to ignore one dependent row.

We summarize how to set \( \delta_i(l) \) for two labels \( l_1 \) and \( l_2 \) to remove dependencies:
- If \( p_i(l_1) > \alpha \) and \( p_i(l_2) > \beta \), set \( \delta_i(l_1) = 0 \) and \( \delta_i(l_2) = 0 \).
- If both probabilities can be changed, set \( \delta_i(l_1) = c \cdot \delta_i(l_2) \) with the larger change \( | \delta_i(l) | \) while satisfying the constraint.

By carefully setting \( \delta_i(l) \), we can ensure the linear system remains solvable even when \( G_i \) has linearly dependent rows.