when it is. However, this would potentially lose some privacy since we are revealing the total
number of unbiased coined needed to make d coins. Additionally, stopping early does not mesh
with the constraint of a static circuit. Given that it is unknown how many pushes will be needed
to generate a group of d coins, we choose a small constant c such that the chance of needing more
than cd pushes to make d biased coins is less than 2−λ. If we assume the stack is full after cd
pushes, we can also empty out the stack for free by simply wiring the slots to d coins. The ﬁnal
question is to choose what size stack will be used to make a total of d coins. The obvious choice
is to use a stack large enough to store all d coins at once, which would also minimize c. However,
by making coins in batches of some size g  2 to determine the rate at which g increases given our probabilistic constraint
P[cg pushes yield  6 the additional bit(s) can be used in a multiplexer to choose the correct 6-bit function.
This means in a secure computation, we must compute all (cid:100)(cid:96)/64(cid:101) 6-bit functions every time
get is called, eﬀectively making the complexity O((cid:96)) since every time (cid:96) doubles the number
of 6-bit functions doubles. Moreover, increasing log (cid:96) when log (cid:96) > 6 means that more bits per
function must be checked to determine which function’s result will be the output of get. This
complexity can be halved by only checking the number of 1s in the additional bits instead of
12
checking that all 0s and 1s match like in a standard multiplexer. However, we are checking
O(log (cid:96)) bits asymptotically, and this is multiplied by O((cid:96)) since we check bits for each 6-bit
function, yielding an additive O((cid:96) log (cid:96)) to the complexity of get. Thus we have complexity
O((cid:96)+(cid:96) log (cid:96)) = O((cid:96) log (cid:96)) = O((λ+log d) log(λ+log d)) for get overall. This brings our complexity
for d coins to O(d(log(λ + log d) + (λ + log d) log(λ + log d))) = O(d(λ + log d) log(λ + log d)),
giving an amortized cost of O((λ + log d) log(λ + log d)).
3 Report-Noisy-Max Application
In this section we demonstrate how we can use our new methods for batch-sampling biased
coins to securely implement one of the foundational algorithms in diﬀerential privacy (DP), the
report-noisy-max mechanism [BLST10] (a variant of the widely known exponential mechanism).
We begin by recalling the deﬁnition of DP.
Deﬁnition 3.1 ([DMNS06]). Let X be the universe of possible dataset entries, R be a range
of outputs, and ε, δ ≥ 0 be parameters. We say a randomized algorithm A : Xn → R is
(ε, δ)-diﬀerentially private if for every two datasets x = (x1, . . . , xi, . . . , xn) ∈ Xn and x(cid:48) =
i, . . . , xn) ∈ Xn that are the same except for one individual’s data, and for every set of
(x1, . . . , x(cid:48)
outcomes S ⊆ R, we have P[A(x) ∈ S] ≤ eεP[A(x(cid:48)) ∈ S] + δ.
We typically view ε as the “privacy level” and require it to be a small constant, as having
ε too small leads to poor utility, and leaving it too large provides meaningless privacy. For
example, Google’s RAPPOR [EPK14] and PROCHLO [BEM+17] use ε = ln 3 and ε = 2.25,
respectively. We think of δ as a “failure probability” for the algorithm, and require that it be
“cryptographically small”, e.g. 2−80.
3.1 Report-Noisy-Max
One of the most useful diﬀerentially private algorithms is the report-noisy-max mecha-
nism [BLST10] (see [DR14] for a textbook treatment). This mechanism is a more practical
implementation of the widely known exponential mechanism [MT07], and the two mechanisms
solve the same problem with identical privacy and utility guarantees.
Given a dataset Xn and discrete set of choices Y (denote |Y| = d), as well as a utility function
u : Xn × Y → R such that u(x, y) is the utility of choice y ∈ Y on dataset x ∈ Xn, a user would
naturally want to select a y ∈ Y that has high utility on the given dataset. For example, Y
might be a set of classiﬁers for a machine learning model, and u(x, y) might be the number of
examples in the dataset that y classiﬁes correctly. The report-noisy-max mechanism is a way to
privately select an element ˆy such that u(x, ˜y) ≥ maxy∈Y u(x, y) − O(log d). The importance of
this mechanism comes from the fact that the error grows only logarithmically in the number of
choices.
The report-noisy-max algorithm works in two steps: First, securely compute noisy scores
ˆuy = u(x, y) + zy for each y ∈ Y, where zy is a suitably chosen random variable typically Laplace
or geometric. Second, return ˜y that maximizes the noisy score ˆu˜y. It is crucial for privacy
that the intermediate noisy scores are not revealed, only the ﬁnal choice ˜y. For our purposes,
we draw from geometric noise in the ﬁrst step. We label the noisy max mechanism that adds
Geo(2/ε) (the discrete and one-sided version of Lap(2/ε)) to each score as NM-Geo(2/ε). Since
this mechanism can be implemented by sampling many independent noise variables, each of
which require sampling many biased coins, it is ideally suited to our methods.
13
Theorem 3.2 ([DR14, Vad16]). NM-Geo(2/ε) is (ε, 0)-diﬀerentially private.
Proof. Let x and x(cid:48) be neighboring datasets, d be the number of candidates, noise and score
notation be as above. We use the following properties:
1. Sensitivity. For all j ∈ Y, 1 + u(x(cid:48), j) ≥ u(x, j) and 1 + u(x, j) ≥ u(x(cid:48), j)
2. Tiebreak. For all j, k ∈ Y (j (cid:54)= k) with ˆuj = ˆuk, we say max(ˆuj, ˆuk) = ˆumin(j,k)
To start, ﬁx any i ∈ Y and ﬁx z−i, which is a draw from [Geo(2/ε)]d−1, used for all scores except
the ith score. We use P [i|ξ] to denote the probability that NM-Geo(2/ε) outputs i conditioned
on ξ. We want to show P [i|x, z−i] ≤ eεP [i|x(cid:48), z−i]. Deﬁne z∗:
z∗ = min
zi
: ˆui = max(ˆui, ˆuj) = max(u(x, i) + zi, u(x, j) + zj) ∀j (cid:54)= i.
Thus, given ﬁxed z−i, i will be the output of NM-Geo(2/ε) on x if and only if zi ≥ z∗. For all
j ∈ Y with j (cid:54)= i we have:
u(x, i) + z∗ > u(x, j) + zj
=⇒ (2 + u(x(cid:48), i)) + z∗ ≥ (1 + u(x, i)) + z∗ > (1 + u(x, j)) + zj ≥ u(x(cid:48), j) + zj
=⇒ u(x(cid:48), i) + (z∗ + 2) > u(x(cid:48), j) + zj.
This means that if zi ≥ z∗ + 2, then the ith score will be the output of NM-Geo(2/ε) when the
database is x(cid:48) and the noise vector is (zi, z−i). Since zi ∼ Geo(2/ε), we have:
(cid:3) ≥ P [zi ≥ z∗ + 2] ≥ e−εP [zi ≥ z∗] = e−εP [i|x, z−i].
P(cid:2)i|x(cid:48), z−i
Multiplying though by eε yields P [i|x, z−i] ≤ eεP [i|x(cid:48), z−i]. Proving P [i|x(cid:48), z−i] ≤ eεP [i|x, z−i] is
identical.
3.2 Review: Sampling Exponential Noise via Poisson
In this section, we review the techniques from [DKM+06] showing how to sample the Poisson
distribution in order to approximate the exponential distribution. Recall that the celebrated
Poisson distribution is a discrete probability distribution that expresses the probability of
a given number of events occurring in a ﬁxed interval if these events occur with a known
constant rate λ. For example, such a distribution can model the number of soldiers in the
Prussian army killed accidentally by horse kicks [vB98]. Speciﬁcally, the support of the Poisson
distribution are the non-negative integers 0, 1, 2, . . ., and the probability mass function is deﬁned
as f (k; λ) = P r[X = k] = λke−λ
. As in [DKM+06], we sample from this distribution in order to
approximate the exponential distribution.
k!
Naive methods. Generically, one can sample any function with cumulative distribution
function ρ by ﬁrst sampling r ∈ [0, 1] and then ﬁnding the maximum x such that r < cdf (x). The
latter maximization problem can be solved by inverting the CDF. Thus, in the case of drawing
Poisson or exponential noise, the complexity of this naive sampling approach will be dominated
by the complexity of computing ln x (which appears in the inverse CDF).
14
Bitwise sampling. The main observation in [DKM+06] is that the special structure of an
exponential distribution enables the generation of the binary representation of an exponential
variable using a number of coins that is independent of the bias. Thus, by calling the noise
sample some κ bit number, one can compute the probability that bit i of a sample is 0 or 1
as seen in [DKM+06]. This bounds the distribution to the interval [0, 2κ). Since P r[X = x] ∝
exp(−|x|/R) in the exponential distribution (with scaling constant R), the probability that bit κ