title:Monitor Based Oracles for Cyber-Physical System Testing: Practical
Experience Report
author:Aaron Kane and
Thomas E. Fuhrman and
Philip Koopman
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Monitor Based Oracles for Cyber-Physical System
Testing
Practical Experience Report
Aaron Kane
Carnegie Mellon University
Pittsburgh, PA USA
PI:EMAIL
Thomas Fuhrman
General Motors
Warren, MI USA
PI:EMAIL
Philip Koopman
Carnegie Mellon University
Pittsburgh, PA USA
PI:EMAIL
Abstract—Testing Cyber-Physical Systems is becoming in-
creasingly challenging as they incorporate advanced autonomy
features. We investigate using an external runtime monitor
as a partial test oracle to detect violations of critical system
behavioral requirements on an automotive development plat-
form. Despite limited source code access and using only existing
network messages, we were able to monitor a hardware-in-the-
loop vehicle simulator and analyze prototype vehicle log data
to detect violations of high-level critical properties. Interface
robustness testing was useful to further exercise the monitors.
Beyond demonstrating feasibility, the experience emphasized a
number of remaining research challenges, including: approxi-
mating system intent based on limited system state observability,
how to best balance the simplicity and expressiveness of the
speciﬁcation language used to deﬁne monitored properties, how
to warm up monitoring of system variable state after mode
change discontinuities, and managing the differences between
simulation and real vehicles when conducting such tests.
I. INTRODUCTION
Modern automobiles are becoming increasingly complex
with the addition of advanced connectivity and autonomy
features. Many of these new features have substantial control
authority over vehicle motion, and are thus safety-critical.
Although full formal veriﬁcation of such systems is still out
of reach, formal methods may be of some help in performing
more thorough system testing. This work describes our
experiences exploring the feasibility of using a “bolt-on”
external passive runtime monitor to improve the results of
system testing on a prototype vehicle design.
A major challenge when testing complex critical systems,
especially distributed Cyber-Physical Systems (CPSs), is cre-
ating an automated or semi-automated method to evaluate the
results of system testing. This is essentially the testing oracle
problem [18].
Runtime veriﬁcation [13] is a lightweight formal method
that attempts to verify that execution traces (whether at
runtime or ofﬂine from logs) conform to a given speciﬁcation.
A (runtime) monitor is a device that that reads a system
trace and yields a verdict about whether the trace satisﬁes
some target property. Although due to time and complexity
constraints of the experiments all the monitoring in this work
was performed ofﬂine (on stored log data), we use the term
runtime monitor in this work. There is no fundamental reason
the monitoring could not be done at runtime, but ofﬂine
veriﬁcation was more useful at this stage of work since it
is more ﬂexible to changes and system access restrictions.
It also permits running multiple experiments on identical
system traces, which would be impractical with live vehicle
testing.
Runtime veriﬁcation can be used to provide a partial
oracle to ensure that critical system properties hold during
testing. While creating a formal speciﬁcation that exactly
describes the runtime behavior of a system as complex as an
automobile is impractical, a speciﬁcation that approximately
bounds safe behavior and is amenable to runtime veriﬁcation
might be attainable.
Runtime monitors designed for testing safety-critical sys-
tems have additional constraints beyond traditional runtime
monitors. A signiﬁcant consideration is that the monitor must
be isolated from the target system to minimize (or, ideally,
eliminate) any disruption of the system under test, especially
with regard to real time performance. Isolation is important
because if the monitor is not isolated, it must be designed to
at least the same level of safety integrity [8] as the system
it is monitoring, increasing development cost and requiring
that the monitor itself be deployed with the system. While
that approach might be desirable in some situations, our work
considers the common desire to have a ”bolt-on” testing box
that can be inserted into an existing safety-critical distributed
system to improve the ability to analyze system-level testing
results and then removed when deploying the ﬁnal system
without invalidating the test results with regard to system
safety. Ultimately this will require a formalized argument
that the interface between the monitor and target system is
truly passive and non-interfering. This could be created using
a system model with wormholes [17] or through a rigorous
safety case argument [1].
Most runtime monitoring work to date has been in creating
integrated monitors that are not designed with isolation in
mind [7]. Moreover, most runtime monitoring work has
assumed access to source code, which is often not
true
of components integrated into commercial systems. Thus,
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.28
DOI 10.1109/DSN.2014.28
DOI 10.1109/DSN.2014.28
148
148
148
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:20:37 UTC from IEEE Xplore.  Restrictions apply. 
we expected that there would be signiﬁcant practical issues
in creating an isolated monitor. For example, there is the
question as to whether enough visibility into system operation
is likely to be available on an existing CPS embedded
network without having to modify the system design to make
monitoring viable.
The primary question we address is: given the constraints
inherent in such an approach, can a bolt-on testing monitor be
useful? To understand what happens when attempting to build
an external runtime safety monitor for use as a testing oracle,
we implemented a prototype runtime monitor on a hardware-
in-the-loop (HIL) vehicle simulator for a prototype system
provided by an automobile manufacturer. Besides passively
checking system test traces during normal operation, we also
performed robustness testing to better exercise the monitor.
Additionally, we compared results to passive monitoring on
logs from an operational prototype test vehicle. In this paper
we discuss our results on the feasility of and remaining
challenges for creating scalable runtime monitors of this type.
II. BACKGROUND
Test oracles are functions that identify whether a given
test has succeeded or failed. In traditional testing, human
users act as test oracles, sometimes aided by automatic tools.
Automated oracles can provide more accurate (no mistakes)
checking of test results at a faster rate than manual checking,
if they can be designed to accurately predict correct system
behavior in response to test stimuli. The oracle problem
is how to create such an automated predictor of system
responses, including addressing situations in which such a
predictor is impractical [18].
Partial oracles, which are oracles that can sometimes – but
not always – correctly decide whether a test has succeeded
or failed, can be simpler to identify. For this work we seek to
create test oracles that are partial in two respects. First, the
oracles only describe critical system properties rather than all
system behaviors. In particular, they only attempt to describe
properties that correspond to system safety. Second,
the
oracles only provide approximate bounds to safety rather than
attempting to specify exact safety invariants. For this work,
such oracles are deemed useful if they discover problems
with the system that the designers did not discover using
their traditional testing techniques.
While typical runtime monitoring frameworks might be
used as partial testing oracles, they are largely targeted at
pure software systems rather than CPS applications. Good-
loe and Pike present a thorough survey of monitoring for
distributed real-time systems in [7]. Those runtime monitors
that are intended for CPS applications tend to assume that the
system under test must be augmented with instrumentation,
compromising isolation. For example, Copilot [15] generates
constant-time and constant-space on-chip monitors so that
the added overhead is known and bounded. But even so,
the system under test must be modiﬁed to accommodate
monitoring. Moreover, many existing monitors require access
to the underlying source code, which is often unavailable in
commercial systems.
At least one existing monitor framework, BusMOP [14],
generates external bus monitors from high level speciﬁcations
targeting commercial off-the-shelf peripherals (speciﬁcally,
the PCI-X bus). BusMOP avoids some observability issues
by limiting speciﬁcation properties to bus-visible accesses
(memory, I/O, and interrupts). However, there has been no
full-scale experiment on a realistic system demonstrating
that safety monitoring of a black-box CPS with no added
instrumentation is feasible and useful.
III. TEST SYSTEM
Automobiles are a straightforward target for a passive net-
work monitor since they are highly distributed systems with a
broadcast bus (usually CAN [2]). Automotive networks tend
to periodically broadcast system state messages, enabling a
monitor to obtain an incomplete, but useful, view of the
state of the system without additional instrumentation and
without adding new message trafﬁc. We use an external,
passive bus monitor which minimizes the intrusiveness of the
monitor on the target system. Our monitor checks properties
written in a speciﬁcation language containing a simpliﬁed
bounded temporal logic loosely based on MTL [10] and
state machine descriptions used to encode mode-based state.
The logic contains the usual boolean connectives, arithmetic
comparisons, and two bounded temporal operators (always
and eventually). To ensure non-interference and avoid issues
related to performance (which are left to future work), the
monitoring in this work was done ofﬂine on system logs
captured from the target system.
The system under test was a dSPACE hardware-in-the-loop
(HIL) simulator testbench for an automobile manufacturer.
The simulator uses MATLAB SIMULINK models to gener-
ate the code for individual electronic control units (ECUs).
CARSIM [3] is used to provide the simulated vehicle and
environment which the feature models on the HIL operate
within.
The dSPACE HIL uses the dSPACE ControlDesk inter-
face software to manage loading models and running tests
(including calibration, logging/measurements, and diagnostic
access). ControlDesk includes a library (rtplib) allowing real-
time scripting access to the models running on the HIL. We
used this library to create some robustness testing scripts, and
additionally used ControlDesk’s control panel functionality
to control manual injection of some individual signals. All
logging was performed with ControlDesk’s trace capture
functionality.
The vehicle tested was a prototype development platform
for semi-autonomous driving features, including Full Speed
Range Adaptive Cruise Control (FSRACC), automated lane
keeping, and emergency collision avoidance. Because fea-
ture development was still in progress, the only feature we
149149149
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:20:37 UTC from IEEE Xplore.  Restrictions apply. 
were able to test was a third-party supplied FSRACC. The
FSRACC was not hardened for robustness, and therefore
our ﬁndings of robustness issues do not reﬂect upon the
quality of production-grade features. However, the FSRACC
did provide a prototype-quality realistic automotive feature
for our testing purposes.
To facilitate the black box interception and injection of
vehicle network messages for robustness testing purposes,
we added some instrumentation to the vehicle feature model.
(These modiﬁcations were solely for input interception/in-
jection to elicit system failures, and did not provide instru-
mentation for runtime monitoring. Moreover, they did not
involve modiﬁcation of the FSRACC code itself.) Each input
signal to the FSRACC module was routed through an added
multiplexor with a inject signal value controlled by an enable
signal. This allowed us to have each input signal individually
passed-through or overwritten by the chosen injection signal.
These additional signals (the injection and enable signals)
were accessible through ControlDesk’s layouts and through
Python scripts using the included rtplib as if they were a
part of the original feature model. Because this part of the
system was a simulation running in a fast HIL computer, the
modiﬁcations did not affect system timing.
Although we added instrumentation for robustness testing,
we note that the monitor itself only requires access to network
messages that are already available on the vehicle’s CAN
broadcast network. This means that the monitor could be
used on a real vehicle without requiring any instrumentation
(beyond connecting it to the network or otherwise exporting
network logs). To validate the simulation, we also ran the
monitor on logs from an actual vehicle running similar code,
and caught some of the same violations on the real vehicle
as found on the simulator during normal (no signal injection)
operation.
A. Robustness Testing
In this work the primary goal of robustness testing was to
increase the chances of seeing system faults during testing to
better exercise the monitor, and not to characterize the quality
of prototype vehicle software, which was expected to be non-
robust. We used the existing ControlDesk interface to perform
network value interception and injection. The injected values
were limited by data-type bounds checking performed by the
interface. This limited injection target’s signal values to ﬂoats
(including exceptional values e.g., NaN, inﬁnity), booleans
(true/false), and enumerations (positive integer values).
We performed three different classes of robustness testing
two, and four bits),
on the HIL: random bit ﬂips (one,
random value injections, and exceptional value injections.
For each testing type we injected a particular number of
faults each for 20s (to allow time for the fault to manifest
into a speciﬁcation violation). Bits to ﬂip were randomly
chosen for each individual bit ﬂip fault. For random value
injection we injected values from [−2000, 2000] for ﬂoats,
I/O
Input
Input
Input
Input
Input
Input
Input
Input
Input
Output
Output
Output
Output
Output
Output
Name
Velocity
AccelPedPos
BrakePedPres
ACCSetSpeed
ThrotPos
VehicleAhead
TargetRange
TargetRelVel
SelHeadway
ACCEnabled
BrakeRequested
TorqueRequested
RequestedTorque
RequestedDecel
ServiceACC
Type
ﬂoat
ﬂoat
ﬂoat
ﬂoat
ﬂoat
boolean
ﬂoat
ﬂoat
ﬂoat
boolean
boolean
boolean
ﬂoat
ﬂoat
boolean
Fig. 1. FSRACC Module IO Signals
[0, 1] for booleans, and [0, maxint] for enums. The ﬂoat
range was chosen such that it would go beyond the possible
non-faulty values of the target messages while keeping the
range small enough that at least some values chosen would
land in the value’s normal range. We used Ballista [9]
style exceptional value injection which targeted ﬂoat-typed
messages with values chosen from the set {NaN, ∞,
√
−∞, 0.0, -0.0, 1.0, -1.0, π, π
2
2 ,
ln(2),
, 4294967296.000001, 4294967295.9999995,
-4.9406564584124654e-324}.
4.9406564584124654e-324,
Random valid value
injection values were used for
exceptional-input injection targeting non-ﬂoat data types due
to the strong value checking enforced on the HIL testbed.
4 , 2π, e, e
2, e
4,
√
ln(2)
2
π
2
2,
We performed script-based injection both on each target
message individually and as well as some injections against
multiple messages at once. We also performed manual explo-
ration of identiﬁed faults by creating a ControlDesk Layout
with numeric input boxes providing manual control of the
injection framework.
B. The Feature Under Test
We performed black box testing of the FSRACC feature.
Source code was not available for the feature, so all testing
and speciﬁcations were based on external
interfaces and
understanding of the high level behavior.
The inputs and outputs of interest to the FSRACC module
are listed in Figure 1. The module has other inputs and
outputs that were disregarded for testing because they had
no observable effect, immediately cancelled cruise control,
were interface indicators, or otherwise did not affect vehicle
safety.
The Velocity input message is the forward speed of
the vehicle. AccelPedPos gives the position of the accel-
erator pedal as a percent (0 being fully released, 100 fully
depressed). The pressure applied to the brake pedal is given
in BrakePedPres and the position of the throttle as a
150150150
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:20:37 UTC from IEEE Xplore.  Restrictions apply. 
percentage (i.e., how open is the throttle) is ThrotPos. The
commanded cruising speed is sent in the ACCSetSpeed
message. The VehicleAhead message tells the ACC
module whether a vehicle is detected ahead of it in the
lane. TargetRange and TargetRelVel are the distance
between the vehicle and the vehicle ahead of it (if one
exists) and the relative velocity between those two vehicles
respectively. The selected headway distance to the preceding
car is an enum SelHeadway.
The output ACCEnable is whether the ACC thinks
it
is supposed to be in control of the vehicle (i.e., en-
gine and brake controllers should ignore these output val-
ues if ACC isn’t enabled). The BrakeRequested out-
put
is true when the ACC feature is requesting a de-
celeration. If the BrakeRequested output is true, then
RequestedDecel is a requested deceleration in m/s2
for
in-
stead the message TorqueRequested is true then the
RequestedTorque output
is the additional amount of
torque the engine controller should attempt to provide. The
ServiceACC message is an error message used to enable
an interface indicator to alert the driver that the feature has
detected an error.
the brake controller
to provide.
to attempt