    - **Test C2**: Test data transfer from the data source to our cluster by using [LightIngest](https://github.com/Azure/Kusto-Lightingest), continuous ingestion from blob storage, or data lake store. For more information, see [ingest historical data](ingest-data-historical.md).
- **Goal D**: We'll have tested the data ingestion rate of incremental data loading and will have the data points to estimate the data ingestion and processing time window.
- **Output D**: We'll have tested the data ingestion rate and can determine whether our data ingestion and processing requirements can be met with the identified approach.
    - **Test D1**: Test daily, hourly, and near-real time data ingestion and processing.
    - **Test D2**: Execute the continuous (queued or streaming) data ingestion and processing while running end-user queries.
Be sure to refine your tests by adding multiple testing scenarios.
Here are some testing scenarios:
- **Azure Data Explorer test A**: We'll execute data ingestion, processing, and querying across multiple cluster SKU sizes (Storage Optimized or Compute Optimized), and different numbers of cluster instances.
- **Azure Data Explorer test B**: We'll query processed data from our cluster using dashboards and querying tools such as the Azure Data Explorer [web UI](./web-query-data.md).
The following is a high level example of tasks that you can use to help you plan your POC:
| Sprint | Task |
|--|--|
| 0 | Present and demo Azure Data Explorer to the customer team |
| 0 | Define business scenarios that customer wants to achieve with Azure Data Explorer |
| 0 | Define technical requirements in terms of data sources, ingestion methods, data retention, data caching, SLAs, security, networking, IAM |
| 0 | Define key performance measures, such as query performance expectation, latency, concurrent requests, ingestion throughout, data freshness |
| 0 | Define high level architecture with Azure Data Explorer and its data ingesters and consumers |
| 0 | Define POC Scope |
| 0 | Define POC planning and timelines |
| 0 | Define, prioritize and weigh POC evaluation criteria |
| 1 | Define and prioritize queries to be tested |
| 1 | Define data access rules for each group of users |
| 1 | Estimate one-time (historical) data ingestion volume and daily data ingestion volume |
| 1 | Define data retention, caching, and purge strategy |
| 1 | Define configuration elements needed when creating clusters, such as streaming, Python/R plugins, purge |
| 1 | Review source data format, structure, schema |
| 1 | Review, refine, revise evaluation criteria |
| 1 | Building pricing scenarios based on the Azure Pricing Calculator for Azure Data Explorer |
| 2 | Create cluster and the required databases, tables, materialized views per the architecture design |
| 2 | Assign permissions to the relevant users for data access |
| 2 | Implement partitioning and merge policies (if required) |
| 2 | Implement one-time ingestion of data, typically historical or migration data |
| 2 | Install and configure query tool (if required) |
| 2 | Test queries on the ingested data using Data Explorer web UI |
| 2 | Test update and delete scenarios |
| 2 | Test connection to PowerBI |
| 2 | Test connection to Grafana |
| 2 | Configure data access management rules |
| 2 | Implement continuous ingestion |
| 2 | Create data connections with Event Hubs/Iot Hub/Event Grid |
| 3 | Implement autorefreshing dashboard for near real-time monitoring in Azure Data Explorer Dashboards or Grafana |
| 3 | Define how to perform load testing |
| 3 | Optimize ingestion methods and processes based on learnings from previous sprints and completed backlog items |
| 3 | Performance assessment on Grafana dashboard |
| 3 | Perform load testing in line with concurrency and expected load requirements |
| 3 | Validate success criteria |
| 3 | Review scoring |
| 3 | Test ability to ingest data with different formats |
| 3 | Validate POC result |
### Evaluate the POC dataset
Using the specific tests you identified, select a dataset to support the tests. Take time to review this dataset. You should verify that the dataset will adequately represent your future processing in terms of content, complexity, and scale. Don't use a dataset that's too small (less than 1 GB) because it won't deliver representative performance. Conversely, don't use a dataset that's too large because the POC shouldn't become a full data migration. Be sure to obtain the appropriate benchmarks from existing systems so you can use them for performance comparisons. Check if your dataset aligns with the supported data formats. Then, depending on the ingestion method (queued or streaming), your dataset can be ingested in batches of appropriate sizes.
> [!IMPORTANT]
> Make sure you check with business owners for any blockers before moving any data to the cloud. Identify any security or privacy concerns or any data obfuscation needs that should be done before moving data to the cloud.
### Create a high-level architecture
Based upon the high-level architecture of your proposed future state architecture, identify the components that will form part of your POC. Your high-level future state architecture likely contains many data sources, numerous data consumers, big data components, and possibly machine learning and artificial intelligence (AI) data consumers. Your POC architecture should specifically identify components that will be part of the POC. Importantly, it should identify any components that won't form part of the POC testing.
If you're already using Azure, identify any resources you already have in place (Microsoft Entra ID, ExpressRoute, and others) that you can use during the POC. Also identify the Azure regions your organization uses. Now is a great time to identify the throughput of your ExpressRoute connection and to check with other business users that your POC can consume some of that throughput without adverse impact on production systems.
For more information, see [Big data architectures](/azure/architecture/data-guide/big-data/).
### Identify POC resources
Specifically identify the technical resources and time commitments required to support your POC. Your POC will need:
- A business representative to oversee requirements and results.
- An application data expert, to source the data for the POC and provide knowledge of the existing processes and logic.
- An Azure Data Explorer expert. You can request your Microsoft contacts to arrange, if necessary.
- An expert advisor, to optimize the POC tests. You can request your Microsoft contacts to arrange, if necessary.
- Resources that will be required for specific components of your POC project, but not necessarily required during the POC. These resources could include network admins, Azure admins, Active Directory admins, Azure portal admins, and others.
- Ensure all the required Azure services resources are provisioned and the required level of access is granted, including access to storage accounts.
- Ensure you have an account that has required data access permissions to retrieve data from all data sources in the POC scope.
> [!TIP]
> We recommend engaging an expert advisor to assist with your POC. Contact your Microsoft account team or reach out to the global availability of expert consultants who can help you assess, evaluate, or implement Azure Data Explorer. You can also post questions on [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-data-explorer) with Azure Data Explorer tag.
### Set the timeline
Review your POC planning details and business needs to identify a time frame for your POC. Make realistic estimates of the time that will be required to complete the POC goals. The time to complete your POC will be influenced by the size of your POC dataset, the number and complexity of tests, and the number of interfaces to test. If you estimate that your POC will run longer than four weeks, consider reducing the POC scope to focus on the highest priority goals. Be sure to obtain approval and commitment from all the lead resources and sponsors before continuing.
## Put the POC into practice
We recommend you execute your POC project with the discipline and rigor of any production project. Run the project according to plan and manage a change request process to prevent uncontrolled growth of the POC's scope.
Here are some examples of high-level tasks:
1. Create an Azure Data Explorer cluster, and all Azure resources identified in the POC plan.
1. Load POC dataset:
    - Make data available in Azure by extracting from the source or by creating sample data in Azure. For an initial test on ingesting data in Azure Data Explorer, use the [ingestion wizard](ingest-data-wizard.md).
    - Test the connector/integration methods you've planned to use to ingest data into your cluster.
1. Write Kusto Queries to query data:
    - If you're migrating from SQL based system, you can use the [SQL to Kusto cheat sheet](kusto/query/sql-cheat-sheet.md) to help you get started.
1. Execute the tests:
    - Many tests can be executed in parallel on your clusters using different client interfaces such as dashboards, PowerBIm and the Azure Data Explorer [web UI](./web-query-data.md).
    - You can create [load test using JMeter or Grafana k6](kusto/api/load-test-cluster.md).
    - Record your results in a consumable and readily understandable format.
1. Optimize the queries and cluster:
    - Whether you're writing new KQL queries or converting existing queries from other languages, we recommend checking that your queries follow [Query best practices](kusto/query/best-practices.md).
    - Depending on the test results, you may need to fine-tune your cluster with a caching policy, partitioning policy, cluster sizing, or other optimizations. For recommendations, see [Optimize for high concurrency with Azure Data Explorer](high-concurrency.md)
1. Monitor for troubleshooting and performance:
    - For more information, see [Monitor Azure Data Explorer performance, health, and usage with metrics](using-metrics.md).
    - For technical issues, please [create a support ticket](https://ms.portal.azure.com/#create/Microsoft.Support).
1. Estimating the pricing:
    - At the end of the POC, you should use what you learned in the POC to [estimate the cost](https://azure.microsoft.com/pricing/calculator/?service=data-explorer) of a cluster that meets your requirements.
1. Close the POC:
    - Record the results, lessons learned and the outcome of the POC phase including the benchmarks, configuration, optimization that you applied during the POC.
    - Clean up any Azure resources that you created during the POC that you no longer need.
    > [!TIP]
    > If you have decided to proceed with Azure Data Explorer and intend on [migrating it to a production environment](#migrating-from-poc-to-production), we recommend keeping the POC cluster running. This will help you set up your production cluster ensuring that you don't lose the configurations and optimizations that you may have applied during the POC.
## Interpret the POC results
When you complete all the POC tests, you evaluate the results. Begin by evaluating whether the POC goals were met and the desired outputs were collected. Determine whether more testing is necessary or any questions need addressing.
## Migrating from POC to production
If you've decided to proceed with Azure Data Explorer and intend to migrate your POC cluster to production, we strongly recommend that you keep the POC cluster running, and use it to set up your production cluster. This will help you ensure that you don't lose the configurations and optimizations that you may have applied during the POC.
Before you migrate your POC cluster to production, we highly recommend that you consider, design, and decide on the following factors:
- Functional and non-functional requirements
- Disaster Recovery and High Availability requirements
- Security requirements
- Networking requirements
- Continuous Integration/Continuous Deployment requirements
- Monitoring and Support requirements
- Training of key personnel in Azure Data Explorer
- Access control requirements
- Schema, data model and data flow requirements
- Ingestion requirements
- Visualization requirements
- Data and insights consumption requirements
- Testing requirements
## Related content
* [Common questions about Azure Data Explorer ingestion](kusto/management/ingestion-faq.yml)
* [Best practices for schema management](kusto/management/management-best-practices.md)