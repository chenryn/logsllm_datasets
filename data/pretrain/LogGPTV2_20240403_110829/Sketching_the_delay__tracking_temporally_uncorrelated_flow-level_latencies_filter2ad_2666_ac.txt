of the virtual LDAs in our data structure. Once a packet
that arrives at time t hits a cell, its s is increased by t, n
is incremented by 1, and d is XORed with the digest of the
new packet.
Update. When a packet with payload x and ﬂow identiﬁer
f arrives at time t, for each row r, a position in the ma-
trix given by (r, (hr(f ) + h′
r(x)) mod C) is determined. That
is, the ﬂow hash is used to obtain a base position in each
row, while the packet payload’s hash determines an oﬀset to
that position in the range [0, k − 1]. Thus, the packets of a
given ﬂow are randomly distributed among the neighboring
cells. To coordinate this randomization between sender and
receiver, they both use, again, the same pre-arranged (con-
sistent) hash functions. Figure 4 presents a diagram of this
scheme, while Fig. 5 formally describes this algorithm.
This scheme has the advantage of, while not introducing
the full overhead of embedding a LDA in each cell, still ob-
tains its advantages, by spreading the packets of each ﬂow
across several cells thus gaining protection against loss or
reordering. If a ﬂow experiences losses, they will invalidate
4ME
8EHJK=,)BHBM
5  5 
5  5 

5 
.MAO
2=?AJ
0A=@AH
F=O=@
DH
F
DH
*=IABJDALEHJK=,)
BBIAJEJJDALEHJK=,)
Figure 4: Virtual LDA extension to the data struc-
ture (d ﬁelds are not depicted).
ph ← hash packet(pkt)%k
for i=1, R do
⊲ k is vLDA size
1: procedure Update state(pkt, f , τ )
2:
3:
4:
5:
6:
7:
8:
9:
10: end procedure
f h ← (hash(i, f ))
j ← ((f h + ph)%C)
LDS[i][j].S ← LDS[i][j].S + τ
LDS[i][j].N ← LDS[i][j].N + 1
LDS[i][j].D ← LDS[i][j].D ⊕ pkthash
end for
Figure 5: LDS – Per-packet operations
some, but not necessarily all of the counters, which gives the
algorithm a chance to recover its delay. As will be discussed
in Sec. 3, this feature increases the amount of collisions.
Therefore, to support the same number of ﬂows, it still has
to be larger than the basic data structure presented in Sec. 2.
Delay Estimation. When producing an estimate of a given
ﬂow, all associated usable vLDA cells are initially selected.
After this step, the question of how to estimate ﬂow delays
arises. The algorithm now has to choose among the usable
cells to produce an estimate. In the event that, for a ﬂow,
none of its cells are invalidated, it has R k cells that can
produce delay estimates (for each row, all cells of the ﬂow’s
vLDA).
Again, several strategies could be used to select which cells
are going to be used for estimation. For example, one could
aggregate all usable cells of each row into a single one, thus
obtaining one candidate delay estimation per row and, like
in the previous data structure, choose the one with the least
amount of packets.
Such a strategy is impractical, because it unnecessarily
gives up the advantages of having the packets spread across
several positions in the data structure. Instead, it is bene-
ﬁcial to selectively discard speciﬁc positions with high mea-
surement interference.
We thus adopt the following strategy. First, among all
cells, we choose the one with the smallest number of pack-
ets. Assuming that each vLDA cell contains 1/kth of the
packets of the measured ﬂow, this is the cell that has expe-
rienced least colliding packets. Let the number of packets
aggregated in this cell be n. Then, from the rest of the cells,
we select those that contain, at most, n (1 + α) packets,
where α is a conﬁguration parameter that reﬂects a maxi-
mum percentage of tolerable interference. Too large an α
leads to the inclusion of interfering packets, while setting
it too small discards valid samples. We empirically found
488⊲ k is vLDA size
if L1[i][j].N == L2[i][j].N &&
L1[i][j].D == L2[i][j].D then
cell = {L2[i][j].S − L1[i][j].S, L1[i][j].N }
S = S S cell
⊲ Stores all valid cells
Nmin = min{Nmin, cell.N }
Nmin = ∞
for i=1, R do
f h ← hash(i, f )
for j = f h, (f h + k)%C do
1: procedure Delay Estimation(f , L1, L2)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end procedure
end for
return Ssum/Nsum
Ssum+ = cell.S
Nsum+ = cell.N
end if
end for
end for
for cell ∈ S do
if cell.N  x] ≈ k Pr [T1 > x] for large x
at ﬁxed k. In other words, when Tk is very large, this tends
to be because one entry in the sum is very large, not because
several are moderately large. Using this property, we obtain
Q ≈ Pn
C ∗ P , where P = Pr[T1 > x]
under the (ﬁtted) Pareto distribution. The accuracy of ap-
proximation increases for small n/C and large x.
i=0 Ki(1 − iP ) = 1 − n
Using this analysis, we can adapt the size of our data
structure to obtain the desired probabilistic accuracy bound.
For example, using Pareto parameters that match our traﬃc
(see Sec. 5), we obtain a probability Q ≈ 91% of small colli-
sions comprising at most 50 packets, with half as many coun-
ters as ﬂows (n/C = 2), structured in 1 row. This means
that, for example, ﬂows with 1000 or more packets have a
91% probability of small collisions comprising no more than
5% of their packets. Incidentally, a numerical computation
of Q without the subexponential approximation diﬀered by
a few tenths of absolute percent in this example.
The formulation of this example illustrates a key require-
ment to measure a ﬂow accurately: not only must it suﬀer
only small collisions but the ﬂow itself must be large. (How
large depends on the delay distribution.) To simplify the
analysis, we stipulate a large ﬂow to be one with at least x
packets, where x is the threshold total packets for small col-
lisions. With this formulation, we call a ﬂow survivable in
storage if it is both large, and suﬀers only small collisions.
What then, is the maximal storage capacity of survivable
ﬂows? Suppose n ﬂows are stored. The average number of
large ﬂows is nP and so the average number of survivable
ﬂows is nP Q = nP (1 − nP/C). This expression is maxi-
mized at n = C/(2P ), yielding C/4. This is reminiscent of
the collision free capacity C/e of the standard Bloom ﬁlter.
The diﬀerence is that the proposed structure can store up to
C/4 survivable ﬂows out of a potentially far larger C/(4P )
that are presented for storage.
In fact we do not expect the operating regime to accom-
modate the maximal number of ﬂow because the probability
of large (i.e., not small) collisions is 1 − Q = nP/C = 1/2.
We now investigate operating regimes with rare large colli-
sions in the generality of multiple R ≥ 1 rows. We assume
489the primary design aim is to limit the probability of large
collisions, with a secondary aim of maximizing the num-
ber of survivable ﬂows under that constraint.
In the case
of multiple rows, a large ﬂow is survivable if it has small
collisions in at least one row. With R rows, the total re-
sources C are divided up evenly between rows, and so sub-
stituting C/R for C in Q, the relevant survival probability is
Q(R) = 1 − (1 − Q)R = 1 − (nP/CR)R. For a cleaner analy-
sis it is convenient to change variables from n to z = nP/C,
which can be thought of as the oﬀered load of large ﬂows
per unit storage. Then Q(R) = q(z, R) := 1 − (zR)R.
As a function of R for ﬁxed z, q(z, R) is maximized at
R = 1/(ez). But only R ≥ 1 are physical. (In this analysis
we omit consideration of integrality; in practice we round
to an integer at the end). Thus maxR≥1 q(z, R) = q(z) :=
q(z, max{1, 1/(ez)}). q(z) is a decreasing function of z which
takes the value 1 − z for z > 1/e (corresponding to R = 1)
and 1 − e−1/(ez) for z ≤ 1/e (corresponding to the case R >
1). Assuming we wish a small probability ε  e−1/(ez),
which corresponds to the choice R = − log(ε), modulo dis-
cretization, then making sure the oﬀered load z is less than
zmax = −1/(e log(ε)). The relative gain of allowing multi-
ple rows can be seen as follows: under the constraint R = 1,
achieving the same bound on the probability of large col-
lisions would require z = 1 − q(z, 1) ≤ ε. Hence allow-
ing R > 1 allows us to increase the oﬀered load by a ra-
tio −1/(eε log(ε)) > 1 for target ε < 1/e. Conversely,
maintaining the same load achieves a dramatic reduction
in the frequency of large collisions. In the previous example
z = nP/C = 0.0875, so we are in the regime z ≤ 1/e, lead-
ing to optimal R = 4.20. Rounding to the nearest integer
R = 4, we obtain q(4, z) = 0.9850, as compared with the
previous q(1, z) = 0.91.
3.2 Lossy Difference Sketch
The introduction of the Virtual LDAs in the LDS has
several side eﬀects. The principal consequence of further
spreading ﬂow packets across the data structure is that fewer
positions remain unused and, more importantly, more colli-
sions occur. However, this is to some extent compensated
by the fact that every ﬂow is spread across k positions, and,
thus, collision randomization is higher. In this section, we
will investigate how these factors change the previous anal-
ysis.
The Virtual LDA divides up the packets of a ﬂow amongst
k locations in each of R rows. Accurate estimation of a given
ﬂow depends on having only small collisions in at least one
of these locations. Thus we adapt our notion of survivability
as follows for general k: A given ﬂow is survivable if it is
large (the number of packets exceeds some value x) while at
the same time suﬀers only small collisions (of size no more
than x/k) at at least one of the Rk locations it occupies in
the Virtual LDA.
In this section we examine a simpliﬁed model of the Vir-
tual LDA that admits an extension to the analysis of Sec-
tion 3.1 to approximate the probability of survivability. This
shows that, from the collision survivability point of view, the
Virtual LDA is no worse than the multirow data structure
described in Section 3.1, and is actually expected to be bet-
ter. This property, coupled with the superior loss resilience
of the Virtual LDA, recommends it as the better choice.
Our model and analysis are as follows. For a speciﬁc ﬂow,
let Uℓ be the number of its packets hashed to a location
ℓ, and Vℓ the number of packets from all other ﬂows that
are mapped to that location. The estimation algorithm ﬁrst
determines the location ℓ of minimal Uℓ + Vℓ. Since we are
concerned principally with the case that the speciﬁc ﬂow has
some large number u of packets, our ﬁrst simpliﬁcation is to
ignore the sampling variability amongst the Uℓ and approxi-
mate the Uℓ as taking the same value (i.e., the average u/k).
Thus the problem of minimizing Uℓ + Vℓ is thus reduced to
that of minimizing the Vℓ.
Because the locations allocated to a given ﬂow in a row
are contiguous, the Vℓ are in general dependent, because if
packets from a background ﬂow hash to location ℓ, the other
packets from the same ﬂow are more likely to collide at a
neighboring location ℓ′. This dependence leads to positive
correlations amongst the Vℓ, meaning that the joint prob-
ability of collisions occurring at all locations of a ﬂow is
greater than the product of the marginal probability of col-
lisions occurring at each site. Conversely, the corresponding
survival probability is bounded below by that of a model
where collisions are independent:
it is conservative to use
this as our second simpliﬁcation. Thus we model the dis-
tribution Ki of the number of colliding ﬂows as a Bernoulli
B(nk, R/C) random variables.
For our ﬁnal simpliﬁcation, we note that under our Pareto
model, the probability that the number T (k) of packets sam-
pled from a background ﬂow to each of the k locations in a
row exceeds a level x obeys Pr[T (k) ≥ x] ≈ Pr[T1 ≥ kx] for
large x, where T1 the length of the background ﬂow; see [26].
Coupled with the subexponential approximation for sums of