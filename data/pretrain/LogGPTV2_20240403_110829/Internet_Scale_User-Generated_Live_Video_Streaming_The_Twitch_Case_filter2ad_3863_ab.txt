r
o
f
 70
 60
 50
 40
 30
 20
 10
 0
 20
 18
 16
 14
 12
 10
 8
 6
 4
 2
 30000
 25000
 20000
 15000
 10000
 5000
 0
12:00 13:00 14:00 15:00 16:00 17:00 18:00
Servers in NA
Servers in EU
Servers in AS
Viewers
Time
s
r
e
w
e
v
f
i
o
r
e
b
m
u
N
i
s
r
e
w
e
v
f
o
r
e
b
m
u
N
 10000
 9000
 8000
 7000
 6000
 5000
 4000
 3000
 2000
 1000
 0
Servers in NA
Servers in EU
Servers in AS
Viewers
12:30 13:00 13:30 14:00 14:30 15:00 15:30 16:00
Time
Fig. 3. (a) Number of servers found for channel nightblue3 (US streamer) as a time-
series; (b) Number of servers found for channel asiagodtonegg3be0 (Asian streamer) as
a timeseries. The number of servers are scaled independently in each region.
Fig. 4. Fraction of servers found from NA, EU and AS cluster for the bottom 70%
(left) and top 10% channels (right). Only popular channels are replicated outside of
NA (Color ﬁgure online)
of viewers). We present both the bottom 70% and top 10% of all channels during
one snapshot.
We can see from Fig. 4 that channels with a small number of viewers tend
to be predominantly served from NA only (red). 67% of channels with 0 viewers
are exclusively hosted in the US; this drops to 63% for 1 viewer, 48% for 2
viewers, 40% for 4 viewers, and just 24% for 5 viewers. As the number of viewers
increases, the fraction of US servers hosting the stream decreases (to be replaced
by both EU and AS servers). Channels with over 50 viewers are nearly always
served from all three continents. Figure 4 also shows the server distribution of
the top 10% channels, with 21% of servers in NA, 53% in EU and 26% in AS
overall.
Brieﬂy, we also see distinct patterns within each continent. For example, in
NA, channels are always ﬁrst hosted in San Francisco (sfo) before being scaled
Internet Scale User-Generated Live Video Streaming
67
out to other server locations in the region. The same occurs in EU and AS,
with Amsterdam (ams) and Seoul (sel) usually hosting a stream before other
continental locations.
5 Client Redirection and Traﬃc Localisation
The previous section has shown that Twitch tries to adapt to the global demand
by progressively pushing streams to multiple servers on multiple continents. In
this section, we explore the mapping of clients to these regions by utilising our
full set of proxies. We perform a full channel crawl from each location, and
see where the clients are redirected to (cf. Sect. 2). Table 1 provides a break-
down of the redirections between diﬀerent continents. In the majority of cases,
Twitch assigns a server from the nearest continent: 99.4% of the requests in
North America and 96% of requests in South America are handled by servers
in NA; 82% of the requests in Europe and 78.2% of the requests in Africa are
served by EU servers.
Table 1. Traﬃc distribution of Twitch clusters globally.
Fraction (%)
NA cluster EU cluster AS cluster
North America 99.4
South America 96
Europe
Africa
Asia
17
21.8
34.4
0.6
4
82
78.2
20
0
0.01
1
0
45.6
Our results also contain some noticeable outliers. Asian servers handle only
45.6% of requests from Asian clients; more than one third of the requests are
handled by NA servers. That said, the NA cluster also absorbs the vast majority
of requests from other regions that are not resolved to their local servers, includ-
ing AS and EU. In order to explore the reasons behind this apparent mismatch,
we investigate for each proxy the fraction of redirections to its local (continen-
tal) servers when requesting the full list of channels. Figure 5 shows the empirical
CDF of the fraction of local servers observed by each proxy. We separate the
plots into each continent for comparison. A clear contrast can be seen among the
three diﬀerent regions: nearly 90% of the clients in North America are always
served by NA servers; and almost 40% of the clients in Europe are always served
by EU servers. However, for Asia, 50% of the clients are never served by the
Asian servers, and only 10% are entirely served by Asian servers.
As previously noted, the number of servers that host a stream is closely
related to the stream’s popularity. Hence, we also inspect the relationship
between channel popularity and the ability of clients to access streams from
their local cluster. Figure 6 presents the fraction of requests that are redirected
68
J. Deng et al.
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 0
NA clients
EU clients
AS clients
 0.2
 0.4
 0.6
 0.8
 1
Fraction of local servers
Fig. 5. Fraction of local servers observed for each proxy. Clients are grouped by con-
tinents for comparison. NA users are usually served locally, whereas most AS clients
must contact servers outside of AS.
to a cluster on the same continent, plotted against the popularity of the chan-
nels. Again, it can be seen that European clients get far more local redirects,
whilst Asian requests regularly leave the continent. This is consistent across all
channel popularities, although in both cases, more popular channels receive a
large number of local redirects.
s
r
e
v
r
e
s
l
a
c
o
l
f
o
n
o
i
t
c
a
r
F
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 1
Europe clients
Asia clients
 10
 100
 1000
 10000
Number of viewers
Fig. 6. The fraction of local servers used vs. the total number of viewer for a channel.
More popular channels are more likely to be locally available on a continent.
An obvious question is why do the Asian clients suﬀer from such poorly
localised redirects. Only 15% of our Asian clients exclusively utilise Asian servers;
50% are never redirected within Asia. To analyse why this might be the case,
we revisit the peering policies of those particular networks. When inspecting the
15% of Asian clients that exclusively rely on Asian servers, we see that they all
Internet Scale User-Generated Live Video Streaming
69
share the same private peering facilities with Twitch (based on PeeringDB). For
example, AS36351, AS9381 and Twitch are all registered in Equinix, Hong Kong.
In contrast, the remaining networks do not peer. Therefore, it is likely that Asia
fails to localise its requests because of these poor existing peering arrangements
(Sect. 3). Even if the servers in Asia are geographically nearby, their network
distance might be higher. Similar scenarios can be found in previous work [13],
highlighting that topology and peering is far more important than geographic
distance.
6 Related Work
Live video streaming is challenging due to the size of video content and the time
constraints involved. Various architectures have been developed to support these
challenges. Peer-to-Peer (P2P) video streaming has emerged as one promising
solution, leveraging the resources of end users. For example, LiveSky [23] and
PPLive (CoolStreaming [22]) are two examples of deployed systems, relying on
P2P assistance. Other approaches rely on cloud assistance; Chen et al. used
Amazon Cloud, Microsoft Azure and Planetlab nodes to build an elastic system
to support various loads in live video streaming [11].
To date, this is the ﬁrst work revealing the content delivery infrastructure of
Twitch; we believe this could be very inﬂuential when designing future Twitch-
like systems. That said, there has been a wealth of work looking, more gen-
erally, at content delivery infrastructures in Video on Demand and live video
streaming. For example, in [8], the authors use PlanetLab nodes to measure
YouTube’s infrastructure. They found that YouTube uses many diﬀerent cache
servers hosted inside edge networks. Torres et al. [19] captured traces from a
campus network, showing that the server selected in the YouTube CDN is usu-
ally the closest one to the user. There has also been work looking at various
other systems, e.g., Netﬂix [7,9], YouPorn [20] and Hulu [6]. However, whereas
previous work has focussed on platforms in which static (i.e., non-live) content
is being delivered, Twitch suﬀers from far greater constraints due to its live real
time nature (making caching redundant). Critically, Twitch is the ﬁrst major
platform to employ user generated live video streaming. In our past work [12],
we explored the nature of channel and game popularity to conﬁrm the signiﬁcant
scale of Twitch (channel peaks exceeding 1.2 million viewers).
7 Conclusion
In this paper, we have studied Twitch as an example of modern user generated live
streaming services. We have made a number of ﬁndings, which reveal how Twitch’s
infrastructure diﬀers from traditional “static” streaming platforms like YouTube.
Through empirical measurements, we have shown that Twitch operates a much
more centralised infrastructure — in a single AS with POPs on four continents
(compared to the thousands used by YouTube). This is likely because the ben-
eﬁts of using highly decentralised caches are less than for that of live streaming
70
J. Deng et al.
(as time-shifted caching cannot take place for live streams). These design choices
naturally lead to a diﬀerent scale-up strategy to that of content delivery networks
like YouTube, which typically rely on reactive caching. Driven by the delay sen-
sitivity of live streaming, Twitch progressively and proactively replicates streams
across servers only after suﬃcient demand is observed. Critically, this occurs on
a pre-region basis, dynamically replicating streams based on local demand. This
more centralised approach places a much greater reliance on eﬀective peering and
interconnection strategies (as Twitch does not place caches inside other networks).
We observed the challenges this brings in Asia, where clients were redirected to
NA due to poor local interconnectivity with Twitch’s AS.
Although Twitch is only one example of user generated live streaming, we
believe its scale and success indicates that its architecture could be an eﬀective
design choice for other similar platforms. Hence, there are a number of future
lines of work that can build on this study. We are interested in exploring a range
of system improvements for Twitch-like platforms, including a more sophisti-
cated control plane that redirects on several factors, expanding their multicast
design, introducing peer-to-peer techniques, or addressing issues with peering.
We would also like to expand our study by measuring realtime streaming perfor-
mance and comparing with other platforms, such as YouTube’s recent gaming
service. Only through this will it be possible to evaluate the best architecture(s)
for future user generated streaming platforms.
References
1. AS46489 Twitch.tv IPv4 Peers. http://bgp.he.net/AS46489# peers
2. PeeringDB - AS46489 Twitch.tv. https://www.peeringdb.com/net/1956
3. Twitch. https://www.twitch.tv/
4. Twitch is 4th in Peak US Internet Traﬃc. https://blog.twitch.tv/
5. Twitch: The 2015 Retrospective. https://www.twitch.tv/year/2015
6. Adhikari, V.K., Guo, Y., Hao, F., Hilt, V., Zhang, Z.L.: A tale of three CDNs: an
active measurement study of Hulu and its CDNs. In: 2012 IEEE Conference on
Computer Communications Workshops (INFOCOM WKSHPS), pp. 7–12. IEEE
(2012)
7. Adhikari, V.K., Guo, Y., Hao, F., Varvello, M., Hilt, V., Steiner, M., Zhang, Z.L.:
Unreeling netﬂix: understanding and improving multi-CDN movie delivery. In: 2012
Proceedings of IEEE INFOCOM, pp. 1620–1628. IEEE (2012)
8. Adhikari, V.K., Jain, S., Chen, Y., Zhang, Z.L.: Vivisecting YouTube: an active
measurement study. In: 2012 Proceedings of IEEE INFOCOM, pp. 2521–2525.
IEEE (2012)
9. B¨ottger, T., Cuadrado, F., Tyson, G., Castro, I., Uhlig, S.: Open connect every-
where: a glimpse at the internet ecosystem through the lens of the netﬂix CDN
(2016). arXiv:1606.05519
10. Calder, M., Fan, X., Hu, Z., Katz-Bassett, E., Heidemann, J., Govindan, R.: Map-
ping the expansion of Google’s serving infrastructure. In: Proceedings of the 2013
ACM Conference on Internet Measurement (IMC 2013), pp. 313–326. ACM (2013)
11. Chen, F., Zhang, C., Wang, F., Liu, J., Wang, X., Liu, Y.: Cloud-assisted live
streaming for crowdsourced multimedia content. IEEE Trans. Multimed. 17(9),
1471–1483 (2015)
Internet Scale User-Generated Live Video Streaming
71
12. Deng, J., Cuadrado, F., Tyson, G., Uhlig, S.: Behind the game: exploring the
Twitch streaming platform. In: 2015 14th Annual Workshop on Network and Sys-
tems Support for Games (NetGames). IEEE (2015)
13. Fanou, R., Tyson, G., Francois, P., Sathiaseelan, A., et al.: Pushing the frontier:
exploring the African web ecosystem. In: Proceedings of the 25th International
Conference on World Wide Web (WWW 2016). International World Wide Web
Conferences Steering Committee (2016)
14. Finamore, A., Mellia, M., Munaf`o, M.M., Torres, R., Rao, S.G.: YouTube every-
where: impact of device and infrastructure synergies on user experience. In: Pro-
ceedings of the 2011 ACM Conference on Internet Measurement (IMC 2011), pp.
345–360. ACM (2011)
15. Gill, P., Arlitt, M., Li, Z., Mahanti, A.: YouTube traﬃc characterization: a view
from the edge. In: Proceedings of the 2007 ACM Conference on Internet Measure-
ment (IMC 2007), pp. 15–28. ACM (2007)
16. Hamilton, W.A., Garretson, O., Kerne, A.: Streaming on Twitch: fostering partic-
ipatory communities of play within live mixed media. In: Proceedings of the 32nd
Annual ACM Conference on Human Factors in Computing Systems, pp. 1315–
1324. ACM (2014)
17. Pires, K., Simon, G.: YouTube live and Twitch: a tour of user-generated live
streaming systems. In: Proceedings of the 6th ACM Multimedia Systems Con-
ference, MMSys 2015, pp. 225–230. ACM, New York (2015)
18. Siekkinen, M., Masala, E., K¨am¨ar¨ainen, T.: A ﬁrst look at quality of mobile live
streaming experience: the case of periscope. In: Proceedings of the 2016 ACM on
Internet Measurement Conference, pp. 477–483. ACM (2016)
19. Torres, R., Finamore, A., Kim, J.R., Mellia, M., Munafo, M.M., Rao, S.: Dissect-
ing video server selection strategies in the YouTube CDN. In: 2011 31st Inter-
national Conference on Distributed Computing Systems (ICDCS), pp. 248–257.
IEEE (2011)
20. Tyson, G., El Khatib, Y., Sastry, N., Uhlig, S.: Measurements and analysis of a
major porn 2.0 portal. In: ACM Transactions on Multimedia Computing, Commu-
nications, and Applications (ACM ToMM) (2016)
21. Wang, B., Zhang, X., Wang, G., Zheng, H., Zhao, B.Y.: Anatomy of a personalized
livestreaming system. In: Proceedings of the 2016 ACM on Internet Measurement
Conference, pp. 485–498. ACM (2016)
22. Xie, S., Li, B., Keung, G.Y., Zhang, X.: Coolstreaming: design, theory, and practice.
IEEE Trans. Multimed. 9(8), 1661–1671 (2007)
23. Yin, H., Liu, X., Zhan, T., Sekar, V., Qiu, F., Lin, C., Zhang, H., Li, B.: Design and
deployment of a hybrid CDN-P2P system for live video streaming: experiences with
livesky. In: Proceedings of the 17th ACM International Conference on Multimedia,
pp. 25–34. ACM (2009)