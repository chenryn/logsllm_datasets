7
UAF ✓ (7%) 7
pcre2
7
✓ (80%)
libxml2 memleak TC
7
✓ (40%) 7
IO
libpng
BO ✓ (17%) 7
7
libarchive
oom
-
-
7
7
Table 1: Bugs detected and percentage of branches that
can be disregarded (i.e., are not on the path to an instru-
mented basic block) compared to coverage-oriented fuzzing.
UAF= use-after-free, BO=buffer overﬂow, TC=type confu-
sion, IO=integer overﬂow
instrumented point a target, we would essentially end up with
coverage-guided fuzzing.
Thus, the challenge is to limit the number of acquired tar-
gets to consider, while still keeping the interesting targets
that trigger actual bugs. To address this challenge, our solu-
tion is to adopt pruning heuristics to weed out targets part of
the candidate target set. We experimented with a number of
pruning heuristics and ultimately included only two simple
but effective heuristic in our current ParmeSan prototype.
4.3 Proﬁle-guided pruning
Our ﬁrst heuristic to limiting the number of targets is to per-
form proﬁle-guided target pruning. By applying a similar ap-
proach to ASAP [44], our strategy is to proﬁle the target pro-
gram and remove all the sanitizer checks on hot paths (i.e.,
reached by the proﬁling input). Since hot paths are unlikely
to contain residual bugs that slipped into production [27,44],
this strategy can effectively prune the set of targets, while
also preferring targets that are “deep”/hard-to-reach. While
this pruning mechanisms might remove some valid targets,
the authors of ASAP [44] note that (in the most conservative
estimate) 80% of the bugs are still detected.
4.4 Complexity-based pruning
Our second heuristic to limiting the number of targets is
to operate complexity-based pruning. Since sanitizers often
add other instrumentation besides a simple branch, we score
functions based on how many instructions are added/mod-
iﬁed by the sanitizer (diff heuristic) and mark targets that
score higher than others as more interesting. The intuition is
that the more instructions are changed within a function by
the sanitizer, the higher the complexity of the function and
thus the chances of encountering the classes of bugs targeted
by the sanitizer. We show this intuition on LAVA-M [15]
using ASan. Using the this heuristic, our top 3 targets in
base64 are in the functions lava_get() , lava_set() , and
, of which the top 2 func-
emit_bug_reporting_address()
USENIX Association
29th USENIX Security Symposium    2293
tions are the functions in LAVA-M that trigger the injected
bugs. The score is taken into consideration when selecting
which targets to prune based on proﬁling. This allows our
target acquisition component to be geared towards retaining
targets in cold code.
block identiﬁers) and if the edge is not yet in the CFG, we
simply add it. When we add edges to the CFG, we only have
to update a subset of the CG, adding only the missing edges
for the neighboring conditionals of the new edge.
5 Dynamic CFG
5.2 Distance metric
To make our sanitizer-guided fuzzing strategy effective,
ParmeSan must be able to efﬁciently steer the execution to-
wards code that is identiﬁed by the target acquisition step. To
do this, ParmeSan needs a precise CFG to estimate the dis-
tance between any given basic block and the target. Building
a precise CFG is the role of our dynamic CFG component.
We ﬁrst show how we dynamically improve the CFG’s preci-
sion during fuzzing (Section 5.1). Using the improved CFG,
ParmeSan then needs to make use of a distance metric to
decide which code paths to prioritize given how far an exe-
cution trace is from interesting code blocks that are instru-
mented by sanitizers (Section 5.2). To further improve the
quality of ParmeSan’s distance metric, we augment our CFG
with Dynamic (Data-)Flow Analysis (DFA) information to
ensure certain interesting conditions are always satisﬁed by
selecting the current input bytes (Section 5.3).
5.1 CFG construction
Prior directed fuzzers rely on a statically-generated CFGs
for distance calculation. In directed fuzzing with many tar-
gets, statically-generated CFGs lead to imprecise results. For
ParmeSan, we instead opt for a dynamically-generated CFG.
In particular, we start with the CFG that is statically gener-
ated by LLVM, and then incrementally make it more precise
by adding edges on the ﬂy as the program executes during
fuzzing. This addition of edges happens, for example, when
we discover an indirect call which cannot be resolved
statically during compile time.
To perform scalable distance calculations, we use the
number of conditionals between a starting point and the tar-
get, as conditionals are the essence of what a fuzzer tries to
solve. Compared to the full CFG, this strategy yields a com-
pact Conditional Graph (CG)—a compacted CFG that only
contains the conditionals. ParmeSan maintains both the CG
and the CFG at runtime, but uses only the CG for distance
calculations.
We repurpose the AFL edge coverage tracking strat-
egy [47] for our compact CG design. After assigning a ran-
domly generated identiﬁer to each basic block, we initially
collect them all from the CFG. Note that the number of nodes
is static and will never change. The edges in the CFG, on
the other hand, are dynamic, and we add them to the CFG
and CG when we encounter edges that are not yet present.
Speciﬁcally, for each edge that the execution takes, we log
the edge identiﬁer (a hash of the previous and current basic
The distance metric helps the fuzzer decide which parts of
the CFG it needs to explore next to get closer to the basic
blocks of interest. Since distance calculation can quickly
run into scalability issues, here we opt for a simple metric.
We deﬁne the distance of a given branch condition c to the
branch conditions that lead to the interesting basic blocks as
d(c). To calculate d(c), we follow a recursive approach in
which the neighboring basic blocks of a target branch will
have a weight of 1. The neighbors of the neighbors’ weights
are then calculated using the harmonic mean (somewhat sim-
ilar to the one used by AFLGo [4]). Implementationwise, the
results in the calculation are propagated starting from the tar-
gets, keeping track of which edges have already been prop-
agated. During implementation, we empirically tested a few
distance metrics, and found the following to be both scalable
and accurate.
Let N(c) be the set of (yet unaccounted for) successors of
c with a path to at least one of the targets, then:
8>><>>:0
if c 2 Targets
if N(c) = /0
otherwise
+ 1
d(c) =
¥
((cid:229)n2N(c) d(n)
jN(c)j
(cid:0)1)
(cid:0)1
Given an execution trace for a given input, ParmeSan uses
the distance metric to determine which of the branches it
should try to ﬂip (by modifying the input), steering the exe-
cution towards interesting basic blocks. While our evaluation
(Section 8) shows that even such a simplistic distance metric
works well, we expect that better scheduling might lead to
better performance. We leave this problem as an open ques-
tion for future work.
5.3 Augmenting CFG with DFA
Our dynamic CFG can further improve distance calculation
by ﬁxing the indirect call targets to a single target depend-
ing on the input. If we know both the sanitizer check that
we want to reach and the input bytes that determine the tar-
get of an indirect call, we can ﬁx the input bytes such that we
know the target of the indirect call. This simple improvement
can drastically impact the precision of our distance calcula-
tion. This optimization is mainly beneﬁcial if the program
has many indirect calls with many possible targets.
2294    29th USENIX Security Symposium
USENIX Association
Figure 2: Example of DFA mutation. The taint label (T 1)
is recorded at a newly uncovered conditional, allowing the
fuzzer to learn that the value should be either ﬁxed to E or
mutated further.
6 Sanitizer-guided fuzzer
In this section, we discuss how ParmeSan uses the targets
obtained by the target acquisition component along with the
distance information provided by the dynamic CFG compo-
nent to direct the fuzzing process towards the desired targets
and trigger bugs.
6.1 DFA for fuzzing
Existing directed greybox fuzzers [4, 8] show that directing
the input based on simple distance metrics works well and is
scalable. At the same time, existing DFA-based coverage-
guided fuzzers [9, 36] show that adding DFA allows the
fuzzer’s input mutation to prioritize and solve tainted branch
constraints in signiﬁcantly fewer executions. When the
fuzzer ﬁnds new coverage, the DFA-instrumented version
of the program tracks the input byte offsets that affect the
newly-found branches, such that the fuzzer can focus on mu-
tating those offsets (see Figure 2). Since we already use DFA
for augmenting the CFG, we also leverage the same analy-
sis to implement coverage-guided-style DFA-enhanced input
mutation but applied to (many-target) directed fuzzing. This
allows us to focus the mutation on input bytes that affect con-
ditionals, which will ultimately lead to our desired targets.
Moreover, once we reach the desired target conditionals, we
use DFA again to prioritize fuzzing of branch constraints,
allowing us to trigger the bugs more efﬁciently.
Interestingly, we do not need a specialized mutation strat-
egy to quickly ﬂip sanitizer checks. Since we speciﬁcally
target sanitizer-instrumented conditionals, we can simply use
the same DFA-enhanced input mutation we used to reach the
targets and get fast bug triggering “for free” as a by-product.
Tainted sanitizer checks will automatically be prioritized,
since tainted checks are preferred by DFA-enhanced input
mutation and sanitizer checks are prioritized by our directed
fuzzing strategy.
Input prioritization
6.2
The main fuzzing loop repeatedly pops an entry from the
priority queue containing entries consisting of a conditional
and the corresponding seed that uncovered that conditional.
The queue is sorted based on a tuple consisting of (runs,
distance) , where runs is the number of times this entry has
been popped from the queue and distance is the calculated
distance of the conditional to our targets obtained by using
our dynamic CFG.
In the fuzzing loop, ParmeSan pops the entry with the low-
est priority from the queue. Using the number of runs as the
ﬁrst key when sorting ensures that the fuzzer does not get
stuck on a single conditional with a low distance. This is an
effective way to mimic coverage-guided, while giving prior-
ity to promising targets.
The fuzzer then mutates the selected seed, giving prior-
ity to input bytes that affect the conditional (as provided by
DFA), with the goal of triggering new coverage. If the fuzzer
generates an input that increases coverage, we add the input
and its coverage to the list of candidate inputs that we will
consider adding to the queue.
We do a DFA-instrumented run for each of these inputs
to collect the taint information for the new basic blocks the
input uncovers. While taint tracking is expensive, we only
need to collect this when we ﬁnd new code coverage. As
ﬁnding new coverage is relatively rare, the amortized over-
head of tracking is negligible (as discussed in Section 8). For
every new conditional that the input covers, we add an entry
consisting of the conditional, the distance, and the seed to
the queue.
Finally, after the original seed has been mutated a number
of times (set to 30) in the round we push it back onto the
queue with an updated distance if the CFG has changed since
the last run.
6.3 Efﬁcient bug detection
We have discussed how ParmeSan uses compiler sanitizers to
direct fuzzing towards interesting targets in the program. In
other words, while sanitizers have been used for bug detec-
tion in existing fuzzing efforts (i.e., fuzzing a sanitized ver-
sion of the program to improve bug detection beyond crash
detection in the baseline) [38], ParmeSan uses compiler san-
itizers for analysis purposes. Moreover, just like existing
fuzzers, ParmeSan can fuzz the target program with or with-
out sanitizers (with a trade-off between bug detection cover-
age and performance).
However, compared to existing fuzzers, ParmeSan can
perform much more efﬁcient sanitizer-based bug detection
if desired. Since we know where the interesting sanitizer
checks are, ParmeSan supports a simple but effective opti-
mization (which we call lazysan). In particular, ParmeSan
can enable sanitizer instrumentation on demand only when
this is useful (i.e., when we reach the desired target checks)
and run the uninstrumented version at full speed otherwise—
similar in spirit to our DFA-enhanced input mutation strat-
egy.
USENIX Association
29th USENIX Security Symposium    2295
6.4 End-to-end workﬂow
The end-to-end fuzzing workﬂow consists of three phases,
a short coverage-oriented exploration and tracing phase to
get the CFG (only run for the input seeds), a directed explo-
ration phase to reach the target basic blocks, and an exploita-
tion phase which gradually starts when any of the speciﬁed
targets are reached.
During the short initial tracing phase, ParmeSan collects
traces and tries to build a CFG that is as accurate as possi-
ble. During the directed exploration phase, ParmeSan tries
to solve conditionals to reach the desired targets. The ex-
ploitation phase starts whenever ParmeSan reaches a target.
ParmeSan tries to exploit the points reached so far by means
of targeted DFA-driven mutations and, when conﬁgured to
do so, also switches to the sanitizer-instrumented version
of the program on demand. Note that the directed explo-
ration stage and exploitation stage are interleaved. ParmeSan
only performs the exploitation strategy for inputs that have
reached the target, while still continuing to do exploration to
reach open targets.
7 Implementation
We implement the fuzzing component of ParmeSan on top of
Angora [9], a state-of-the-art coverage-guided fuzzer written
in Rust. The blackbox sanitizer analysis consists of a num-
ber of Python scripts and LLVM passes. The modiﬁcations
required to Angora consist of about 2,500 lines of code. We
also integrate AFLGo into the ParmeSan pipeline, allowing
us to use AFLGo as a fuzzing component, rather than the
ParmeSan fuzzer, based on Angora.
To implement our target acquisition component, we run
the llvm-diff
tool between the sanitizer-instrumented and
the uninstrumented version of the target program. We an-
alyze the resulting LLVM IR diff ﬁle and label all the con-
ditionals added by the instrumentation as candidate tar-
gets. We implement our target set pruning strategy on top
of ASAP [44], which already removes sanitizer checks in
hot paths to improve sanitizer-instrumented program perfor-
mance. We augment ASAP, letting it take into account the
complexity-based pruning heuristics described in Section 4.4
when deciding which checks to remove.
We base the fuzzer and dynamic CFG components of
ParmeSan on Angora [9]. Angora keeps a global queue,
consisting of pairs of conditionals (i.e., branching compare
points) and input seeds. In Angora, these queue entries are
prioritized based on how hard a conditional is to solve (e.g.,
how many times it has been run).
We modify Angora to sort queue entries by distance to
the targets generated by the target acquisition step and direct
fuzzing towards them. Furthermore, we added a dynamic
CFG component to Angora, to allow for CFG constraint col-
lection, making it possible to narrowly calculate distances to
our targets based on the obtained coverage and the condi-
tional to be targeted.
Similar
to Angora, we use DataFlowSanitizer
(DF-
San) [1], a production DFA framework integrated in the
LLVM compiler framework. We use such information in a
dedicated LLVM instrumentation pass that traces each indi-
rect call and records the input bytes that determine (i.e., taint)
the target of the indirect call site. Note that we only run the
DFSan-instrumented version of our program (for CFG con-
struction or fuzzing) and re-calculate target distances when
we uncover a new edge, resulting in low overhead.
7.1 Limitations
Currently, ParmeSan relies on available LLVM IR for its
target acquisition.
In theory the techniques described in
this paper can also be applied to binaries without the IR
available. While the analysis currently relies on compiler
sanitizer passes, however, for raw binaries the methods we
present could be applied by replacing the compiler sanitizers
with binary hardening [33, 48]. We also noted an issue with
some sanitizers that only insert their modiﬁcations at linking
time; doing the analysis on the actual binary would solve this
issue.
The types of bugs found by ParmeSan are heavily reliant
on the sanitizers used for target acquisition (as we show in
Section 8.3). Some sanitizers, such as ASan, are capable of
detecting a broad class of common bugs. We refer the reader
to [42] for a more thorough analysis on using sanitizers in a
security context for testing and production purposes.
8 Evaluation
In this section we evaluate ParmeSan on a number of real-
world programs with known bugs. We compare how Parme-
San performs against other directed and coverage-guided
greybox fuzzers. We also show how our dynamic CFG con-
struction improves fuzzing for real-world programs with per-
vasive indirect calls. Some additional results are presented in
Appendix A.
We run all our experiments on machines running Ubuntu
18.10 using AMD 7 Ryzen 2700X with 32 GB DDR4 RAM.
While both ParmeSan and Angora are able to use multiple
cores, we run all our experiments on only one core to be able
to compare against prior work, unless noted otherwise. For
each part of the evaluation, we specify which sanitizer we
use for target acquisition and repeat the experiments 30 times
with a timeout of 48 hours, unless otherwise noted. During
the proﬁling-guided pruning phase in our target acquisition
component, we always set the ASAP cost level to 0.01. This
is the equivalent of adding instrumentation at a cost of 1%
in performance. As noted by the ASAP authors [44], this
strategy sufﬁciently covers bugs, while aggressively remov-
ing hot checks. Note that the target acquisition step is not
2296    29th USENIX Security Symposium
USENIX Association
included in the total run time of our benchmarks, as it is part
of the compilation process. In all our experiments, the time
spent on analysis is linear to the original compilation time of
the target program (as shown in Table 8).
8.1 ParmeSan vs. directed fuzzers
We ﬁrst compare against state-of-the-art directed greybox
fuzzers and show the availability of DFA information alone
improves directed fuzzing signiﬁcantly. We reproduce a
number of benchmarks covered by AFLGo [4] and Hawk-
Eye [8], showing how ParmeSan fares in a traditional di-
rected setting. Note that the source code for HawkEye is not
available at the moment, and thus we compare against the
results reported by the authors. While comparisons to results
in papers is difﬁcult due to variations in the test setup, since
the baseline performance of AFLGo presented by the Hawk-
eye authors [8] is similar to the one we obtained in our setup,
we are hopeful that their performance numbers are also com-
parable to ours.
CVE
Fuzzer Runs p-val Mean TTE
2014-0160
2016-4487
2016-4488
2016-4489
2016-4490
2016-4491
2016-4492
2016-4493
2016-6131
OpenSSL
30
ParmeSan
HawkEye
AFLGo
5m10s
30 0.006
20m15s
0.03
0.01
Binutils
30
20
30 0.005
30
20
30
30
20
30
10
9
5 0.003
30
20
20 0.003
10
9
5
ParmeSan
HawkEye
AFLGo
ParmeSan
HawkEye
AFLGo
ParmeSan
HawkEye
AFLGo
ParmeSan
HawkEye
AFLGo
ParmeSan
HawkEye
AFLGo
ParmeSan
HawkEye
AFLGo
0.04