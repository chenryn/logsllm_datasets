# Summary Review Documentation for "Understanding Website Complexity: Measurements, Metrics, and Implications"

**Authors:** M. Butkiewicz, H. Madhyastha, V. Sekar

## Reviewer #1
### Strengths:
- The analysis of website complexity is comprehensive, and the observations are intriguing.
  
### Weaknesses:
- The measurements are limited to 4 vantage points, which may not fully capture the diversity in performance, especially given the widespread use of CDNs.

### Comments to Authors:
- Overall, this is a well-executed study on website complexity. Good job.
- There seems to be a discrepancy between the abstract (3 vantage points) and the rest of the paper (4 vantage points). Please clarify.
- Could you provide more context or references for the statement that web traffic is dominant?
- How significant is the set of websites selected for the study? Did you check if they were hosted on CDNs?
- Do the 4 vantage points offer sufficient geographic diversity? If sites are delivered via CDNs, this could be problematic.
- In Section 4.3, you claim that non-origin objects essential for user experience cannot be optimized by CDNs. Could you elaborate on why this is the case?
- Can you provide a CDF corresponding to Figure 16, with the x-axis as it is but the y-axis showing the distribution of webpages with a given load time?
- Figure 17 shows correlation, but what does a PCA analysis reveal about the most important factors explaining variability in RenderEnd?

## Reviewer #2
### Strengths:
- This is a classic measurement study with well-explained details. The data is made public, and there is a clear relationship between the metrics and performance, supported by statistically valid techniques.

### Weaknesses:
- The study lacks deeper insights into "complexity." It primarily presents a series of metrics and performance associations without exploring computational or information-theoretic aspects of complexity.

### Comments to Authors:
- The paper should be precise about the following:
  - Which version of Firefox was used?
  - Over what dates was the data collected?
  - Why choose around 2100 sites (not a round number), and how many were chosen exactly?

## Reviewer #3
### Strengths:
- The paper provides a detailed and interesting analysis of the contributing factors for download times of different websites.

### Weaknesses:
- No significant weaknesses noted.

### Comments to Authors:
- This is a well-written and thorough evaluation. The analysis is systematic and based on over 20K sites.
- It's interesting that the total bytes of a website are less of a factor than the number of JavaScript objects and the number of servers contacted.
- Different site types (e.g., News vs. Kids sites) have distinct characteristics, which can be intuitively explained.
- The discussion on AdBlocks and the impact of Minify is very interesting.
- Congratulations on a comprehensive and well-executed study.
- When counting the number of non-origin servers, do you count yahoo.com and yimg.com as separate domains/servers?
- Is the method for measuring the contribution of non-origin servers to download time accurate, considering the overlap with other downloads? Shouldn't you look at exclusive download times from non-origin servers?

## Reviewer #4
### Strengths:
- This is the first attempt to quantify website complexity, and the paper includes several interesting findings.
- The paper is well-written and presented.

### Weaknesses:
- The analysis is somewhat superficial, focusing on obvious metrics without uncovering deep insights about web complexity.

### Comments to Authors:
- Many landing pages are personalized, which may impact your results. For example, www.facebook.com and google.com (for iGoogle users) are different when the user is logged in. Can you verify that this is not significantly biasing your results?
- Why 2100 sites, and not 2000?
- Why use Quantcast for top sites and Alexa for genre classification? What is wrong with Alexa's site rankings?
- If Figure 3 plots the median number of objects, why are there fractional values? Are you plotting the average of medians? (If so, please specify.)
- For the experiment in Figure 18, why are you not studying load times?
- Are you aware of the work by E. Kiciman and B. Livshits, "AjaxScope: A Platform for Remotely Monitoring the Client-Side Behavior of Web 2.0 Applications," Proc. SOSP'07, October 2007? This work presents a method for web designers to learn about the performance of their sites through JavaScript instrumentation.

## Reviewer #5
### Strengths:
- The paper is well-written and provides a comprehensive examination of several complexity metrics and their impact across websites.

### Weaknesses:
- The study is exclusively based on a Firefox extension, which may miss performance dependencies on other client browsers.
- Only the root page of websites is examined, which might be significantly simpler than other pages.

### Comments to Authors:
- Given the increase in web traffic and website complexity, the paper provides a good description of the basic characteristics of modern websites.
- The analysis is solely based on Firefox, which may affect the performance observed in the client. Browsers vary in JavaScript engines, the number of simultaneous TCP connections, object prioritization, etc. Some discussion on this is needed.
- Examining only the root page may have a significant effect on sites like Google.com or Facebook.com. It would be useful to see if performance dependencies vary beyond the root page.
- The small discussion on mobile clients is interesting, and I encourage you to explore this further.

## Response from the Authors
We thank the reviewers for their positive feedback and insightful suggestions. The final version addresses most of the reviewersâ€™ comments and has significantly improved our paper. 

- The paper now provides more precise details about our measurements, such as the number of vantage points, the period over which the data was gathered, and the version of Firefox used.
- We acknowledge the inherent difficulty in accounting for the contribution of non-origins to download times due to parallelized requests. To address this, we provide three different ways to measure this contribution in Section 4.2.
- Since our measurements focus on landing pages, we have added a preliminary comparison between the complexity of landing and non-landing pages.
- We also present the distribution of our measured page load times.

There are a few reviewer comments that we do not address. We did not perform a PCA analysis of load times vs. complexity metrics because PCA maps the measurement space to dimensions that cannot be tied back to one of our metrics. Our regression-based analysis provides an intuitive way to distill the most important complexity metrics for explaining load times.

Two other suggestions from reviewers, while relevant to the broader theme, are outside the scope of this paper and will be addressed in future work:
1. Websites where web pages are significantly different when the user is logged in, e.g., facebook.com, igoogle.com.
2. Differences across web browsers.