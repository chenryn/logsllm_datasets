measuring-mobile-web-is-hard.htm%l.
325Complexity Metric for Networked System Designs. In Proc.
NSDI, 2008.
[26] D. Galletta, R. Henry, S. McCoy, and P. Polak. Web Site
Delays: How Tolerant are Users? Journal of the Association
for Information Systems, 2004.
[27] D. Mosberger and T. Jin. httperf: A Tool for Measuring Web
Server Performance. SIGMETRICS Performance Evaluation
Review, 26(3), 1998.
[28] F. Nah. A study on tolerable waiting time: How long are
Web users willing to wait? Behaviour & Information
Technology, 23(3), May 2004.
[29] D. Fetterly, M. Manasse, M. Najork, and J. Wiener. A
large-scale study of the evolution of web pages. In Proc.
WWW, 2003.
[30] P. Gill, M. Arlitt, N. Carlsson, A. Mahanti, and
C. Williamson. Characterizing Organizational Use of
Web-based Services: Methodology, Challenges,
Observations, and Insights. ACM TWEB, 2011.
[31] S. Ihm and V. S. Pai. Towards understanding modern web
trafﬁc. In Proc. IMC, 2011.
[32] E. Kiciman and B. Livshits. AjaxScope: A Platform for
Remotely Monitoring the Client-Side Behavior of Web 2.0
Applications. In Proc. SOSP, 2007.
[33] B. Krishnamurthy and C. E. Willis. Privacy diffusion on the
web: A longitudinal perspective. In Proc. WWW, 2009.
[34] B. Krishnamurthy, C. E. Willis, and Y. Zhang. On the use
and performance of content distribution networks. In Proc.
IMW, 2001.
[35] M. Lee, R. R. Kompella, and S. Singh. Active measurement
system for high-ﬁdelity characterization of modern cloud
applications. In Proc. USENIX Conference on Web
Applications, 2010.
[36] R. Levering and M. Cutler. The portrait of a common HTML
web page. In Proc. ACM Symposium on Document
Engineering, 2006.
[37] L. Meyerovich and R. Bodik. Fast and parallel web page
layout. In Proc. WWW, 2010.
[38] J. C. Mogul. The case for persistent-connection HTTP. In
Proc. SIGCOMM, 1995.
[39] A. Nazir, S. Raza, D. Gupta, C.-N. Chuah, and
B. Krishnamurthy. Network level footprints of Facebook
applications. In Proc. IMC, 2009.
[40] S. Gribble et al. The Ninja architecture for robust
Internet-scale systems and services. Computer Networks,
35(4), Mar. 2001.
[41] F. Schneider, S. Agarwal, T. Alpcan, and A. Feldmann. The
new Web: Characterizing AJAX trafﬁc. In Proc. PAM, 2008.
[42] F. Schneider, A. Feldmann, B. Krishnamurthy, and
W. Willinger. Understanding online social network usage
from a network perspective. In Proc. IMC, 2009.
[43] Z. Li et al. WebProphet: Automating performance prediction
for web services. In Proc. NSDI, 2010.
[44] Y. Zhang, H. Zhu, and S. Greenwood. Website complexity
metrics for measuring navigability. In International
Conference on Quality Software, 2004.
Figure 23: Bytes saved using minify js, one optimization from
Google’s Page Speed for reducing page load times.
[8] Mobify. http://mobify.me.
[9] The need for speed.
http://www.technologyreview.com/files/
54902/GoogleSpeed_charts.pdf.
[10] Opera Mini & Opera Mobile browsers.
http://www.opera.com/mobile/.
[11] SPDY: An experimental protocol for a faster web. http:
//www.chromium.org/spdy/spdy-whitepaper.
[12] Spec web benchmarks.
http://www.spec.org/web2005/.
[13] Strangeloop: Speed up your website.
http://www.strangeloopnetworks.com.
[14] Web bug.
http://en.wikipedia.org/wiki/Web_bug.
[15] When seconds count.
http://www.gomez.com/wp-content/
downloads/GomezWebSpeedSurvey.pdf.
[16] J. M. Kleinberg, S. R. Kumar, P. Raghavan, S. Rajagopalan,
and A. Tomkins. The web as a graph: Measurements,
models and methods. In Proc. COCOON, 1999.
[17] R. Tibshirani. Regression shrinkage and selection via the
lasso. Journal of Royal Statistical Society of Britain, 1996.
[18] A. Bouch, A. Kuchinsky, and N. Bhatti. Quality is in the Eye
of the Beholder: Meeting Users’ Requirements for Internet
Quality of Service. In Proc. CHI, 2000.
[19] A. Broder et al. Graph structure in the web. Computer
Networks, 33(1), June 2000.
[20] B. Ager, W. MÃijhlbauer, G. Smaragdakis, and S. Uhlig.
Web content cartography. In Proc. IMC, 2011.
[21] H. Balakrishnan, V. N. Padmanabhan, S. Seshan, M. Stemm,
and R. H. Katz. TCP behavior of a busy Internet server:
Analysis and improvements. In Proc. IEEE Infocom, 1998.
[22] T. Benson, A. Akella, and D. Maltz. Unraveling the
Complexity of Network Management. In Proc. NSDI, 2009.
[23] G. Candea. Toward Quantifying System Manageability. In
Proc. HotDep, 2008.
[24] J. Cao, W. S. Cleveland, Y. Gao, K. Jeffay, F. D. Smith, and
M. C. Weigle. Stochastic Models for Generating Synthetic
HTTP Source Trafﬁc. In Proc. INFOCOM, 2004.
[25] B.-G. Chun, S. Ratnasamy, and E. Kohler. NetComplex: A
326Summary Review Documentation for 
“Understanding Website Complexity: Measurements, 
Metrics, and Implications” 
Authors:  M. Butkiewicz, H. Madhyastha, V. Sekar 
Reviewer #1 
Strengths:  The  analysis  of  the  website  complexity  is  pretty 
complete and the observations are interesting. 
Weaknesses: The measurements are limited to 4 vantage points, 
which  may  not  uncover  the  whole  diversity  in  performance  for 
site  latency,  especially  given  CDN  deployment  and  widespread 
use. 
Comments to Authors:   Overall, I think this is a nice study of 
website complexity. Good job. 
Are  there  3  or  4  vantage  points?  The  abstract  says  3,  while  the 
rest of the paper 4. 
The authors mention that Web traffic is dominant, in what sense? 
Please provide references. 
How  important  is  the  set  of  websites  that  were  selected  for  the 
study? Did the authors check how they were hosted (CDNs)? 
Do the 4 vantage points provide enough geographic diversity to 
the  study?  If  sites  are  delivered  by  CDNs  this  is  likely  to  be  a 
problem. 
In  Section  4.3,  the  authors  claim  that  if  non-origin  objects 
constitute content essential for the user experience, these cannot 
be optimized by CDNs? Why is that? It is not clear in what sense 
they cannot be optimized. 
How does the CDF corresponding to Figure 16 look like? Take x-
axis as it is but y-axis showing the mass of traffic for webpages 
with a given loadtime. Where are most websites? 
Figure 17 shows the correlation, not what is important or not to 
explain variability of RenderEnd? What does a PCA analysis say 
about what matters most to explain variations? 
Reviewer #2 
Strengths:  This  is  a  classic  measurement  study.  It  does  have 
limitations, but the details are mostly well explained. The data is 
being made public. They show a clear relationship between their 
metrics and performance, using statistically valid techniques. 
Weaknesses: The one weakness here is that I was hoping to see 
some deeper insight into “complexity”. In this end, this is just a 
series of metrics and performance associations. I would have liked 
to  understand  complexity  either  at  the  computational  level,  or 
perhaps the information theoretic level. Either would have given 
us a deeper insight into the problem. This is really just an issue 
with the title of the paper, and the results do seem useful. 
Comments to Authors:  One of the problems with a paper like 
this is that there are so many details to nail down. For instance in 
Section 3, 
- which version exactly of Firefox is used? 
- over exactly what dates was the data collected? 
- why choose around 2100 sites (not a round number) and exactly 
how many were chosen? 
I  guess  some  of  these  details  appear  in  the  data,  but  the  paper 
should be precise as well. 
Reviewer #3 
Strengths:  The  paper  provides  a  very  nice  analysis  of 
contributing factors for download times of different websites. The 
analysis is detailed and results are very interesting.  
Weaknesses: Nothing significant. 
Comments to Authors: I thought this was a wonderful paper that 
had a very detailed analysis of website download performance. I 
liked  the  way  the  authors  systematically  built  the  performance 
model based on more then 20K sites. 
It is interesting to see that total bytes corresponding to a website 
is less of a factor than number of javascript objects and number of 
servers contacted. 
It  was  also  interesting  that  different  site  types  (News  vs  Kids 
sites)  have  somewhat  different  characteristics,  and  one  could 
intuitively explain it. 
I  also  thought  that  some  of  the  topics  covered  in  the  discussion 
section (AdBlocks, impact of Minify) to be very interesting. 
Congratulations on a nice and thorough evaluation! 
In counting the number of non-origin servers, I could not tell if 
the proposed method counts yahoo.com and yimg.com as separate 
domains/servers. 
I  am  not  sure  if  counting  the  non-origin  server  contribution  in 
download time is accurate, since the time is overlapped with other 
downloads. 
Should you not be looking at download times that are exclusive to 
download from non-origin servers? 
327Reviewer #4 
Strengths: - A first attempt at quantifying the complexity of web 
site, and the paper includes several interesting findings. 
- Well-written and presented paper. 
Weaknesses:  The  analysis  is  somewhat  superficial  in  that  it 
studies  the  obvious  metrics  and  no  deep  insights  about  Web 
complexity are uncovered. 
Comments  to  Authors:  I  like  this  paper  and  what  you  are 
studying. As you note, I indeed find it surprising that a study like 
this  has  not  been  done  before.    I  only  have  minor  comments 
below. 
The  landing  pages  of  many  sites  these  days  are  personalized, 
which  may  be  impacting  your  results.  www.facebook.com  is  a 
very different page with and without the user being logged in, and 
same holds for even google.com (for folks who use iGoogle). Can 
you verify that this is not significantly biasing your results? 
Why 2100 sites, and not, say, 2000? 
Why get the top sites from quantcast but genre from Alexa? What 
is wrong with Alexa site rankings? 
If  Figure  3  is  plotting  median  number of objects, why are there 
fractional  values?  Are  you  plotting  average  of  medians?  (Then, 
say so.) 
For the experiment in Figure 18, why are you not studying load 
times? 
Are  you  aware  of  this  work:  E.  Kiciman  and  B.  Livshits, 
“AjaxScope: A Platform for Remotely Monitoring the Client-Side 
Behavior  of  Web  2.0  Applications”,  Proc.  SOSP’07,  October 
2007. It presents a method for Web site designers to learn about 
the  performance  of  their  sites  in  the  wild  through  javascript 
instrumentation.   
Reviewer #5 
Strengths:  Well  written  paper  that  provides  a  comprehensive 
examination of several complexity metrics and their impact across 
websites. 
Weaknesses:  The  study  is  exclusively  based  on  a  Firefox 
extension, and thus misses any performance dependencies on the 
client browser that might have an effect on load and render times.  
Only  the  root  page  of  websites  is  examined,  which  could  be 
significantly simpler than other website pages. 
Comments  to  Authors:  Given  the  increase  of  web  traffic  and 
website  complexity,  I  believe 
the  paper  provides  a  nice 
description of the basic characteristics of modern web sites.  
interesting 
things,  such  as 
The paper is well written and easy to follow and I only have two 
main issues with the paper: 
- The analysis is solely based on Firefox. This perhaps may not 
have a significant impact on the first part of the paper where the 
authors examine the number and type of objects, but it certainly 
has  an  effect  on  the  performance  as  observed  in  the  client.  For 
example,  javascript  engines  are  quite  different  across  browsers 
and this may affect the download times. Further, browsers vary in 
other 
the  number  of  simultaneous  TCP 
connections,  object prioritization etc. Some discussion on this, I 
feel, is needed. 
- Second, only the root page is examined. This may have quite an 
effect in cases of websites such as Google.com or Facebook.com. 
It  would  be 
to  examine  whether  performance 
dependencies vary if one looks beyond the root page. 
Finally,  I  found  the  small  discussion  on  mobile  clients  quite 
interesting, and I would encourage the authors to look more into 
this. 
Response from the Authors 
We  thank  the  reviewers  for  their  positive  feedback  and  their 
insightful  suggestions.    The  final  version  addresses  most  of  the 
reviewers’  comments  and  has  helped  to  improve  our  paper 
significantly.    First,  the  paper  is  more  precise  about  various 
details  of  our  measurements,  such  as  the  number  of  vantage 
points,  the  period  over  which  the  data  was  gathered,  and  the 
version  of  Firefox  used  to  gather  measurements.    Second,  we 
acknowledge  the  inherent  difficulty  of  accounting  for  the 
contribution  of  non-origins  to  download  times  because  requests 
are  parallelized.    To  address  this  concern,  we  provide  three 
different ways to measure this contribution in Section 4.2.  Third, 
since  our  measurements  are  focused  on  the  landing  pages  of 
websites,  we  have  added  a  preliminary  comparison  between  the 
complexity  of  landing  and  non-landing  pages.  Finally,  we  also 
present the distribution of our measured page load times. 
There are a few reviewer comments that we do not address.  We 
do  not  perform  a  PCA  analysis  of  load  times  vs.  complexity 
metrics  since  PCA  maps  the  measurement  space  to  dimensions 
that cannot be tied back to one of our metrics. 
Our regression-based analysis provides an intuitive way to distill 
the most important complexity metrics for explaining load times.  
Two  other  suggestions  from  reviewers,  while  relevant  to  the 
broader theme, are outside the scope of this paper and we leave 
this  for  future  work:  (1)  websites  where  web  pages  are 
significantly  different  when 
in,  e.g., 
facebook.com,  igoogle.com  and  (2)  differences  across  web 
browsers.  
the  user 
is 
logged 
328