图3
结果竟然是空的！这是怎么回事？我又换了一个过滤表达式，把所有mount包显示出来，结果见图4。
图4
从图4中可以看出，客户端10.32.200.45发送了mount请求，但被服务器10.32.106.77拒绝了，这倒符合“Access Denied”的症状。等等，客户端的IP不应该是192.168.26.139吗，怎么变成10.32.200.45了？这时候我恍然大悟：两个网
 络之间估计存在NAT（Network Address Translation），当客户端发出的请求经过NAT设备时，Source IP被改掉了（图5显示了这个过程）。
图5
由于服务器上的访问控制只允许192.168.26.139访问，所以来自10.32.200.45的挂载请求自然被拒绝了。
我把分析报告发给现场工程师。他和客户沟通之后，果然证实了我的分析。最终把服务器和客户端连到一个网络中就挂载上了。
实施部门的经理发来一封热情洋溢的感谢信，这让我想起几年前第一次得到Patrick的帮助时，我也表达过同样的感激之情。其实我们还应该感谢的，是Gerald Combs。假如没有他的Wireshark，我可能至今还不理解NFS的挂载过程，更不要说一小时内就找出问题。那天我把MSN签名档改成了“Life is tough, but Wireshark makes it easy”。
午夜铃声
“叮铃铃……叮铃铃……”一阵手机铃声打断了我的美梦。
我恍惚中按下接听键，竟然是老板的声音，“阿满，真不好意思，这么晚还打你电话。”一番寒暄之后，有了下面的对话。
老板：“我司在为××电视台实施Isilon，现场团队被一个读性能的问题卡了好几天了。所以美国总部刚刚打电话给我，希望一位懂网络的专家能尽快飞到北京，你看……”
我：“我看最近招的两位CCIE都不错，让他们去锻炼一下嘛。我明天要搬家，老婆又在发烧。”
老板：“这个项目对我们太重要了，#￥%*@$^&……（此处省略300字）你完全不用担心，我会派几个人替你搬家。”
我：（赶在老板派人帮我照顾老婆之前）“好吧，我准备一下。”
挂了电话，赶紧搜索一下Isilon，才知道是我司最近收购的NAS，以性能卓越著称。是什么问题能让实施团队卡住好几天呢？看看时钟已经是凌晨2点了，便让现场的工程师先把网络包传上来再说。
5点钟起床，司机已经等在楼下了（我司对待甲方的态度和效率，常常让员工们妒忌）。一路疾驶到办公室，网络包也已经上传完毕。我用Wireshark粗略一看，发现很多包发生了重传（Retransmission），而且还有大量乱序（Out-Of-Order）。下面是Wireshark的分析结果。
重传（见图1）：
图1
乱序（见图2）：
图2
我的第一反应便是乱序导致了重传，从而影响了性能。乱序为什么会导致重传呢？本书的TCP相关内容其实已有详细解释，下面再简单介绍一下。
在正常情况下，网络包到达接收方时的Seq号应该是顺序的，比如在每个包长度为1460的情况下，Seq号可能是这样的：1460，2920，4380……因此接收方能算出下一个包的Seq号应该是什么。比如4380之后应该是4380+1460=5840，假如收到的不是5840，接收方就知道包序乱了。这时它应该回复一个包给发送方，说“我要的是5840（即Ack 5840）”。如果接下来收到的包仍然不是5480，那接收方就再回复一次“我要的是5840”。
而对于发送方来说，持续收到“我要的是5840”可能意味着5840跑到其他包后面了，也可能意味着5840已经丢失。RFC里这样定义：如果发送方收到3个及以上重复的“我要的是×”，即可认为包×已经丢失，应当启动快速重传。图3演示了这个过程。
图3
最终接收方会收到两个一样的Seq=5480，即乱序了的原始包，还有一个重传包。其中第二个到达的包相当于浪费了。
我在Wireshark上随机挑出几个重传包，发现方向都是从Isilon到Windows的，恰好符合读性能差的症状。分析到这里，我仿佛看到一丝曙光。一般来说，乱序可能是由发送方或者网络设备导致的，我还应该在发送方抓包进一步调查。但因为手头上只有在接收方抓的包，所以只能到了现场再说了。在赶往机场的路上，我草拟了一个计划。
1．把Isilon和Windows客户端连到同一台空闲的交换机，尽量排除网络设备
的影响。
2．Isilon和其他服务器一样，应该有类似NIC teaming的功能。根据我的经验，乱序有时候就是由teaming导致的，可以尝试关闭。我不久前还碰到过Large Segment Offload（LSO）导致的乱序，也是一个考虑因素。
3．实在不行，就在Isilon和Windows上同时抓包，两者一对比便能发现很多问题。
到了北京已经是下午了。和几位来自中国香港、美国、和日本的工程师边吃边聊。原来他们这几天做过很多方面的尝试，包括我计划中的第1步，但是性能没有任何改变。Windows客户端也换过几台，但结果都差不多。目前来看网络设备和客户端都不是瓶颈，估计原因就出在Isilon上了。也许明天关闭Isilon上的
 NIC teaming和LSO，问题就解决了吧？这个时候我还是挺乐观的。
第二天一大早便赶到了××电视台的新大楼，比约定时间早了3小时。这是我第一次体会到现场工程师的辛苦—所有操作都要等待客户审批，搭个测试环境就花了半天时间；而且五六个人只能共用一台电脑，我在操作的时候其他工程师就只能等着；最可怕的是机房里的冷气，待了几个小时之后实在招架不住。
幸好一切都在按计划进行。我们终于在Isilon上找到Large Segment Offload 和NIC teaming的开关，并满怀希望地关闭了它们。当我启动测试脚本的时候，几位饱受折磨的现场工程师都凑过来看……可惜结果令人大跌眼镜—读性能比之前还差！我顿时觉得非常尴尬，对着等待我下一步建议的同事们，只能说先抓个包看看吧。这一抓包更是意外，居然看不到乱序的包了！可见我之前的猜测没有错，乱序是由NIC teaming或者LSO导致的。但为什么消除了乱序之后性能没有改善呢？再看看重传率，果然还是很高。
到这里只剩下一个解释了—重传并非乱序引起的，也就是说从一开始就走错方向。我不得不一个人坐到角落里，重新研究昨天拿到的网络包。当我逐个检查乱序的包时，果然看到了一个很有趣的现象。如图4所示，虽然乱序的包很多，但只是相邻两个包的颠倒，因此接收方只发出了1个“我要的是×”，而不会凑满3个以上相同的“我要的是×”来触发重传。这就解释了为什么重传不是由乱序导致的。
图4
 举个更通俗的例子，当序号为1、2、3、4、5、6的一系列包到达接收方时，如果次序乱成了2、1、4、3、6、5，是不会触发快速重传的；但如果乱成2、3、4、5、6、1，就会导致重传。
再分析消除乱序后在接收方抓到的网络包，现象就更加有趣了。如图5所示，接收方明明收到了Seq 20440（Frame No. 3），但它竟然发送了4个“Ack 20440”给发送方，从而促使发送方重传了Seq 20440 （Frame No. 13）。
图5
这个现象实在太“不科学”了。按理说这个包是在接收方抓的，Wireshark上也已经显示了“Seq 20440”，就意味着接收方已经收到。为什么还会连发4个Dup Ack呢？我百思不得其解，不过已经隐约感觉到希望—只要解开这个谜团，问题或许就能解决了。机房里强劲的冷气让我有些分神，于是我独自踱到走廊上，从头开始分析。
我回忆起RFC中关于快速重传的描述：“当接收方收到比期望值大的Seq时，就要向发送方Ack它期望的Seq值……”根据这个理论，难道接收方在收到20440之前，已经收到了21900、23360、24820和26280这4个包？从Wireshark里看20440明明是排在这4个包前面的！
会不会是20440本身的checksum有问题，被接收方抛弃了呢？再看看图5中最
 后两个包，重传的Seq 20440（Frame No. 13）到达接收方之前，接收方已经回复了“Ack 27740”（Frame No. 12），这表明接收方收到了27740之前的所有包，包括20440。也就是说，20440真的是被移到26280后面了，而不是因为checksum无效被抛弃。
那是什么因素导致接收方把20440移到26280后面呢？目前我不得而知，但TCP/IP是分层协作的，也许是网络层把包交给TCP层时打乱了。
分析到这里，可以肯定重传的根本原因就是接收方自身的乱序，而网络设备和Isilon都被冤枉了。这是我第一次看到此类现象，不但颠覆了我昨天的分析结果，而且难以说服现场工程师和客户。他们已经测试了7台客户端，但结果都是一样的，难不成7台都出了同样的问题？这概率低得令人难以置信。接下来就是一场场辩论，电视台请来了他们的网络专家，希望说服我进一步检查Isilon。我无法向他解释为何所有客户端都有同样的问题，他也不能反驳Wireshark上显示的证据。一直拉锯到夜里12点都没有吃上饭，一位同事已经出现了低血糖症状。还好最后查到一个重要信息，原来那7台客户端都是用同一张ghost盘安装的，客户终于让步，答应明天新装7台客户端供我们测试。但同时也有一个要求，明早必须提供一个官方的分析报告，证明的确是客户端导致的问题。
草草吃完晚饭，已经是凌晨1点。酒店非常贴心，为我准备好了巧克力，拆好拖鞋，甚至掀好了被子，可惜这些我都没有机会享受。等写完分析报告，已经到了凌晨3点半。没睡下多久，morning call又来了……再次感叹现场工程师的辛苦，这只是我第三个晚上没睡好，而他们估计已经有一周了。我睡眼惺忪地到了电视台门口，远远看到树林里似乎有家咖啡店，像看到救命稻草一样直奔过去。到了近处才发现是“Post Office”，远看还真像是coffee……
现场工程师手脚麻利，很快就搭好新的环境。到早上10点钟我们又一次启动测试脚本，这一次每台的读性能都达到100MB/s以上，大大超过了客户80MB/s的预期。现场的工程师异常兴奋，给测试结果拍照、截屏，甚至拍了一段视频。他们为这个项目压抑太久了，需要好好庆祝。
而我也背起笔记本，向这栋造型诡异的建筑、向这个奇怪的问题告别，匆匆赶往首都机场。家里还有发烧的老婆，没搬完的家……
深藏功与名
每当我要写一个真实的Wireshark案例时，感觉就像在自我表扬。这实在不符合阿满低调的个性，但是没办法，谁让Wireshark这么神奇呢？不久前我处理的一个Data Domain项目，便是极好的例子。
我之前对Data Domain的了解并不多，只知道是普林斯顿大学一位华人教授的发明，后来被我司收购了。所以当项目经理打我电话时，也是听得一头雾水。大概了解到的症状是多台AIX同时往Data Domain读写数据（如图1所示）。写的时候性能都很好，能超过90MB/s；但读的时候性能却很差，在20MB/s以下。驻场的团队已经耗在上面好几天了，却一直没有进展，留给我的时间已经不多了。
图1
鉴于项目的紧迫性，我挂了电话便立即出发。还好这个客户的数据中心在上海郊区，我得以在路上仔细分析。
1．一般存储设备都是读比写快，Data Domain应该也不例外。目前的现象是读比写慢得多，所以根本原因应该不在Data Domain本身。
2．网络很值得怀疑。一般存储端的带宽大，客户端的带宽小。读文件时数据从大带宽进入小带宽，就如同大河水流入小河，有可能会溢出（表现在网络上就是拥塞）而导致性能问题。写文件时方向相反，所以拥塞概率低，
 性能就会好一些，正好符合这个案例的症状。
3．只要在两端各抓一个网络包，就能证实我的猜测。
中午12点，终于到达数据中心。我让司机在门口等一会，估计很快就能出来了（颇有点温酒斩华雄的气概）。结果见到客户时，人家说午睡时间到了，一个小时后再战，说完便从桌子底下拉出折叠床来。我只能感叹同行不同命—忙碌如我，能保证夜间睡7小时就不错了，哪里敢奢望午睡？只好叫司机先回家，下午再来接我。
好不容易等到客户收集好网络包，用Wireshark打开一看，果然发现了好多重传（如图2所示）。重传对性能的影响是极大的，即便是0.5%的比例也会使性能大幅度下降。
图2
我随机看了几个重传包，发现方向都是从Data Domain到AIX的。说明这些包从Data Domain出来之后，在路上丢失了，最终没有到达AIX。Data Domain因为一直没有等到AIX的确认包，所以只能选择重传。
 这就意味着我之前在路上的推测是正确的，网络上存在瓶颈。客户也确认AIX端的带宽只有存储端的1/10，是可能有问题。不过由于网络项目已经实施完毕，无法变动，所以只能从Data Domain和AIX上想办法。
明明知道问题发生在网络上，却要到存储端和客户端上去想办法，是不是有点头痛医脚的感觉？但这的确是可行的，我至少能想到三个方案。
方案1．把Data Domain的发送窗口强制成较小的值，这样每次发出去的数据量就少一些，拥塞的概率也减小了。就像大河里流的水量很少，即便流入小河也不会漫出来一样。发得慢当然对性能有影响，但由于避免了丢包，所以总性能反而有所提升。该方案的缺点是限制了Data Domain给所有网络设备发送数据的速度，不仅是针对AIX。
方案2．把AIX的接收窗口强制成较小的值。这样Data Domain给AIX传数据时的发送窗口就被限制了，而且给其他客户端发数据时不受影响。但该方案的缺点是限制了AIX从所有网络设备接收数据的速度，不只是针对Data Domain。
以上两个方案都需要选定一个较小的窗口值，这个值要怎么算出来呢？图3是一个丢包的例子，发送方一口气发出6个包，但其中最后一个丢失了，最后导致了超时重传。