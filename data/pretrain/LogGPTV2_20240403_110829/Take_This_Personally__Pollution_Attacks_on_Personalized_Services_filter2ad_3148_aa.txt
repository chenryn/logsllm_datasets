# Take This Personally: Pollution Attacks on Personalized Services

**Authors:**  
Xinyu Xing, Wei Meng, Dan Doozan (Georgia Institute of Technology)  
Alex C. Snoeren (University of California, San Diego)  
Nick Feamster, Wenke Lee (Georgia Institute of Technology)

**Abstract:**
Modern web services frequently personalize content to cater to the specific interests, viewpoints, and contexts of individual users. Ideally, personalization enhances user satisfaction by highlighting uniquely relevant information, ultimately benefiting the service's bottom line. However, as this paper demonstrates, the personalization mechanisms employed by popular services are vulnerable to attacks. We show that third parties can manipulate these mechanisms to increase the visibility of arbitrary content, such as a new YouTube video, an unpopular product on Amazon, or a low-ranking website in Google search results. Specifically, we demonstrate that attackers can inject information into users' profiles, thereby altering the results of the services' personalization algorithms. While our exploits are tailored to each service, the general approach is likely to be broadly applicable. By demonstrating the attack against three popular web services, we highlight a new class of vulnerability that allows an attacker to influence a user’s experience with a service, unbeknownst to the user or the service provider.

## 1. Introduction
The economics of the web ecosystem revolve around clicks and user engagement. Many web services rely on advertising, charging for prime screen real estate and focusing on mechanisms to display information that generates revenue. Both malicious actors and legitimate businesses have long sought to reverse-engineer and exploit these mechanisms to place their own content—whether it be items for sale, malicious content, or affiliate marketing schemes. Search Engine Optimization (SEO), which aims to improve the placement of web pages in search engine results, is a well-known example of this practice.

Modern web services increasingly use personalization to enhance user experience. For instance, websites tailor their front pages based on a user's browsing history, video-sharing platforms like YouTube recommend videos based on watch history, shopping sites like Amazon suggest products based on purchase history, and search engines like Google return customized results based on various user-specific factors. As the web becomes more personalized, the effectiveness of broad techniques like SEO will diminish. In its place, a new class of schemes and attacks will emerge, exploiting the underlying personalization mechanisms.

In this paper, we demonstrate that contemporary personalization mechanisms are vulnerable to exploitation. Specifically, we show that YouTube, Amazon, and Google are all susceptible to a class of cross-site scripting attacks, which we term "pollution attacks." These attacks allow third parties to alter the customized content returned to users who have visited a page containing the exploit. Although the attack is effective, we do not claim it is the most powerful, broadly applicable, or difficult to defeat. Rather, we present it as an initial example of a class of attacks that we believe will soon, if not already, be launched against the relatively unprotected personalization services.

Our attack exploits the fact that services using personalization incorporate a user's past activities (e.g., browsing, searching, and purchasing) to customize the content presented to them. Many services log users' web activities whenever they are logged in, regardless of the site they are visiting. Other services track user activities even when the user is logged out (e.g., through session cookies). We use both mechanisms to pollute users' service profiles, thereby impacting the customized content returned to users in predictable ways. Given the extensive portfolio of services provided by major players like Google and Amazon, it is reasonable to expect that a large fraction of users will either be directly using the service or at least logged in while browsing elsewhere on the web.

We show that pollution attacks can be highly effective on three popular platforms: YouTube, Google, and Amazon. A key feature of our attack is that it does not exploit any vulnerabilities in the user's web browser. Instead, it leverages the services' own personalization mechanisms to alter the user's experience. While our implementation uses Cross-Site Request Forgery (XSRF), other mechanisms are also possible.

The ability to launch such an attack is particularly concerning because it indicates that current web security approaches are ill-equipped to address the vulnerabilities in personalization mechanisms. Today's web browsers prevent exploits like cross-site scripting and request forgery by enforcing domain boundaries through "same origin" policies. However, our attack represents a class of exploits that cannot be stopped by client-side enforcement. In an attempt to expand the footprint of its personalization engine (e.g., Google recording search queries entered on a third-party page), a service with personalized services is providing the cross-site vector itself. Thus, only the service can defend itself from such attacks on its personalization. Moreover, enforcing isolation between independent web sessions seems contrary to the goal of personalization, which seeks to increase the amount of information used for customization.

This paper makes the following contributions:
- We describe pollution attacks against three platforms—YouTube, Google, and Amazon—that allow a third party to alter the personalized content these services present to users who previously visited a web page containing the exploit.
- We study the effectiveness of our attack on each of these platforms and demonstrate that it can (1) increase the visibility of almost any YouTube channel, (2) dramatically increase the ranking of most websites in the short term, and even have lasting impacts on the personalized rankings of a smaller set of sites, and (3) cause Amazon to recommend reasonably popular products of the attacker's choosing.
- Our attack and its effectiveness illustrate the importance of securing personalization mechanisms in general. We discuss several implications of our study and ways for websites to mitigate similar vulnerabilities in the future.

The rest of the paper is organized as follows. Section 2 provides a general overview of pollution attacks on personalized services. Sections 3, 4, and 5 introduce specific attacks that can be launched against YouTube, Google, and Amazon, respectively, and report on our success. We survey related work in Section 6 and discuss limitations of our work and possible defenses in Section 7 before concluding in Section 8.

## 2. Overview and Attack Model
In this section, we provide a brief overview of personalization as used by popular web services and present a model of pollution attacks, which we apply to three different scenarios: YouTube, Amazon, and Google.

### 2.1 Personalization
Online services increasingly use personalization to deliver information tailored to users' interests and preferences. Personalization benefits both the service provider and the user: the user sees content that more closely matches their preferences, and the service provider presents products that the user is more likely to purchase, potentially resulting in higher revenues.

The primary tool a service provider uses to affect the content a user sees is modifying the choice set, the set of results displayed in response to a query. The size of the choice set varies across services. For example, YouTube shows 12-40 videos, Amazon may show up to five sets of recommended products, and Google's initial search results page displays the top ten results. Figure 1 shows examples of choice sets on different sites.

When a user issues a query, the service's personalization algorithm affects the user's choice set. The choice set produced by the personalization algorithm depends on the user's query and auxiliary factors, including the universe of all possible content and the user's browsing history. Previous work has claimed that factors ranging from geography to time of day can affect the choice set. For the purposes of our attacks, we focus on how changes to a user's history can affect the choice set, holding other factors fixed. Specifically, we study how an attacker can pollute the user's history by generating false clicks through Cross-Site Request Forgery (XSRF). We describe these attacks in the next section.

### 2.2 Pollution Attacks
The objective of a pollution attack is to affect a user's choice set given a particular input. In some cases, a user's choice set appears before the user enters any input (e.g., upon an initial visit to the page). In this case, the attacker's goal may be to affect the default choice set. Figure 2 provides an overview of the attacker's goal: the attacker aims to affect the resulting choice set by altering the user's history with false clicks, using XSRF as the attack vector.

This attack requires three steps:
1. **Model the service's personalization algorithm:** We assume the attacker has some ability to model the personalization algorithm used by the site to affect the user's choice set. Specifically, the attacker must have an idea of how the user's past history affects the choice set. This information is often available in published whitepapers but may require experimentation in some cases.
2. **Create a "seed" to pollute the user's history:** Given some knowledge of the personalization algorithm and a goal for how to affect the choice set, the attacker must design the seed used to affect the user's choice set. Depending on the service, the seed may be queries, clicks, purchases, or any other activity that might go into the user's history. A good seed can affect the user's choice set with a minimal number of "false clicks."
3. **Inject the seed with a vector of false clicks:** To pollute a user's history, in most cases, the user must be signed into the site. (For some services, pollution can occur even when the user is not signed in.) The attacker can then use a mechanism to make it appear as though the user is taking action on the website for a particular service (e.g., clicking on links) using a particular attack vector.

In the following sections, we explore how an attacker can apply this procedure to attack the personalization algorithms of three different services: YouTube, Amazon, and Google search.

## 3. Pollution Attacks on YouTube
In this section, we demonstrate our attack on YouTube. Following the attack steps described in Section 2, we first model how YouTube uses the watch history of a user account to recommend videos by reviewing the literature [5]. Second, we discuss how to prepare seed data (i.e., seed videos) to promote target data (i.e., target videos belonging to a specific channel). Third, we introduce how to inject the seed videos into a YouTube user account. Finally, we design experiments and quantify the effectiveness of our attack.

### 3.1 YouTube Personalization
YouTube constructs a personalized list of recommended videos based on the videos a user has previously viewed [5]. YouTube attempts to identify the subset of previously viewed videos that the user enjoyed by considering only those videos that the user watched for a long period. Typically, YouTube recommends videos that other users with similar viewing histories have also enjoyed. YouTube tracks the co-visitation relationship between pairs of videos, reflecting how likely a user who watched a substantial portion of video X will also watch and enjoy video Y. Generally, there may be more videos with co-visitation relationships than there is display area, so YouTube prioritizes videos with high rankings. YouTube will not recommend a video the user has already watched.

YouTube displays recommended videos in the suggestion list placed alongside a playing video (e.g., Figure 5) and in the main portion of the screen at the end of a video (Figure 1(a)). A suggestion list appearing next to a video typically contains 20-40 suggested videos, two of which are recommended based on personalization. At the end of a video, YouTube shows a more concise version of the suggestion list that contains only twelve of the videos from the full list; these videos may or may not contain personal recommendations.

### 3.2 Preparing Seed Videos
YouTube organizes videos into channels, where each channel corresponds to the set of uploads from a particular user. In our attack, we aim to promote a set of target videos, ΩT, all belonging to the same YouTube channel, C. To do so, we use an additional set of seed videos, ΩS, that have a co-visitation relationship with the target videos. By polluting a user's watch history with videos in ΩS, we can cause YouTube to recommend videos in ΩT.

There are two ways to obtain ΩS: we can identify videos with pre-existing co-visitation relationships to the target videos, or we can create the relationships ourselves.

**Existing Relationships:**
In the simplest version of the attack, the attacker identifies existing videos to use as the seed set. For example, given a target video set ΩT belonging to channel C, the attacker could consider all other videos in the channel, C - ΩT, as candidate seeds. For every candidate video, the attacker checks which videos YouTube recommends when a fresh YouTube account (i.e., an account with no history) watches it. YouTube allows its users to view their recommended videos at http://www.youtube.com/feed/recommended. If the candidate video triggers YouTube to recommend a video in ΩT, the attacker adds the injected video to the seed video set ΩS.

In general, this process allows the attacker to identify seed videos for every target video in ΩT. The attacker cannot yet launch the attack, though, because a YouTube video in ΩS may trigger YouTube to also recommend videos not in ΩT. To address this issue, the attacker can simply add these unwanted videos to the seed video set ΩS because YouTube does not recommend videos that the user has already watched. As we will show later, the attacker can convince YouTube that the user watched but did not enjoy these unwanted videos, so their inclusion in ΩS will not lead to additional recommendations.

**Fabricating Relationships:**
For some videos, it may be difficult to identify a seed set ΩS that recommends all elements of ΩT due to a lack of co-visitation relationships for some target elements. Instead, attackers who upload their own content to use as the seed set can create co-visitation relationships between this content and the target set. Specifically, an attacker uploads a set of videos, Ω0, and establishes co-visitation relationships between Ω0 and ΩT through crowd-sourcing (e.g., Mechanical Turk or a botnet): YouTube visitors need only watch a video in Ω0 followed by a video in ΩT. After a sufficient number of viewing pairs, the attacker can use videos in Ω0 as the seed set. As we will show in Section 3.4.1, a relatively small number of viewing pairs suffices.

### 3.3 Injecting Seed Videos
To launch the attack and inject seed videos into a victim's YouTube watch history, an attacker can harness XSRF to forge the following two HTTP requests for each video in the seed set:
1. `http://www.youtube.com/user_watch?plid=&video_id=`
2. `http://www.youtube.com/set_awesome?plid=&video_id=`

where `plid` and `video_id` correspond to the values found in the source code of the seed video's YouTube page. The first HTTP request spoofs a request from the victim to start watching the seed video, and the second convinces YouTube that the victim watched the video for a long period. Both HTTP requests are required for videos in ΩS to trigger the recommendation of videos in ΩT, but only the first HTTP request is needed to prevent the recommendation of unwanted videos.

### 3.4 Experimental Design
We evaluated the effectiveness of our attack in controlled environments and against real YouTube users. We first validated the attack in the simplest scenario, where the attack promoted existing YouTube channels through existing co-visitation relationships. We then considered the scenario where an attacker uploaded and promoted content from a channel they created. Finally, we conducted a small-scale experiment to demonstrate the effectiveness of the attack against a volunteer set of real YouTube users.

#### 3.4.1 New Accounts
We first promoted existing YouTube channels by launching our attack against victims with fresh YouTube user accounts. This experiment confirmed the effectiveness of our approach in the absence of other, potentially counteracting influences, such as recommendations based on a user's existing history.

We began by selecting 100 existing YouTube channels at random from the list of the top 2,000 most-subscribed channels published by VidStatsX [19]. For each selected YouTube channel, we randomly selected 25 videos from the channel as the target video set, used the method described in the previous section to identify a seed video set, and injected the seed videos into a fresh YouTube account.

We then considered promoting new content by creating our own YouTube channel and similarly attacking fresh YouTube accounts. Our YouTube channel contains two 3-minute videos. We selected one of the videos as a one-element target video set and used the other as the seed set. We created a co-visitation relationship by embedding both videos on a web page and recruiting volunteers to watch both videos sequentially. We obtained 65 and 68 views for our seed and target video, respectively.

#### 3.4.2 Existing Accounts
We studied the effectiveness of our pollution attack using real YouTube user accounts. We recruited 22 volunteers with extensive pre-existing YouTube watch histories. To limit the inconvenience to our volunteers, we limited our study to attempting to promote one moderately popular YouTube channel based on existing co-visitation relationships. We selected a moderately popular account because a popular channel may be recommended anyway (regardless of our attack); conversely, an entirely new channel requires a certain amount of effort to establish the co-visitation relationships, and we have limited volunteer resources.

Based on these parameters, we arbitrarily selected the channel OnlyyouHappycamp. We believe this selection is a reasonable candidate to be promoted using our attack for several reasons. First, compared to popular channels, most videos in OnlyyouHappycamp have low view counts (about 2,000 views per video on average) and a modest number of subscribers (3,552). Both of these are easily achievable by an attacker at a relatively low cost [2]. Second, most videos in OnlyyouHappycamp are 22 minutes long, making them suitable for promotion. As we will explain in Section 3.5.1, the length of a target video affects its likelihood of being recommended as a result of a co-visitation relationship with another video.

Similar to the experiments with new accounts, we randomly selected 15 target videos from the channel OnlyyouHappycamp, identified a seed set, and injected the seed videos into the volunteers' YouTube accounts. After pollution, the volunteers were asked to use their accounts to watch three videos of their choice and report the suggestion list displaying alongside each of their three videos.

### 3.5 Evaluation
We evaluated the effectiveness of our pollution attacks by logging in as the victim user and viewing 114 representative videos. We measured the effectiveness of our attack in terms of promotion rate: the fraction of the 114 viewings when at least one of the target videos was contained within the video suggestion list. Recall that the list contains at most two personalized recommendations (see Section 3.1); we deem the attack successful if one or both of these videos are videos that were promoted as a result of a pollution attack.

#### 3.5.1 New Accounts
Pollution attacks successfully promoted target videos from each of the 100 selected existing channels. Each time we injected seed videos for a particular channel, we observed the target videos in the suggestion list for each of the 114 videos. Since these are fresh accounts, there is no other history, so our targeted videos always occupy both of the personalized recommendation slots.

In addition, we observed that the particular target videos shown in the suggestion video list varied, even when the seed set remained the same. This variability suggests that the personalization algorithm considers multiple factors, and the injection of seed videos effectively alters the user's history to include the target videos.

#### 3.5.2 Existing Accounts
We studied the effectiveness of our pollution attack using real YouTube user accounts. We recruited 22 volunteers with extensive pre-existing YouTube watch histories. To limit the inconvenience to our volunteers, we limited our study to attempting to promote one moderately popular YouTube channel based on existing co-visitation relationships. We selected a moderately popular account because a popular channel may be recommended anyway (regardless of our attack); conversely, an entirely new channel requires a certain amount of effort to establish the co-visitation relationships, and we have limited volunteer resources.

Based on these parameters, we arbitrarily selected the channel OnlyyouHappycamp. We believe this selection is a reasonable candidate to be promoted using our attack for several reasons. First, compared to popular channels, most videos in OnlyyouHappycamp have low view counts (about 2,000 views per video on average) and a modest number of subscribers (3,552). Both of these are easily achievable by an attacker at a relatively low cost [2]. Second, most videos in OnlyyouHappycamp are 22 minutes long, making them suitable for promotion. As we will explain in Section 3.5.1, the length of a target video affects its likelihood of being recommended as a result of a co-visitation relationship with another video.

Similar to the experiments with new accounts, we randomly selected 15 target videos from the channel OnlyyouHappycamp, identified a seed set, and injected the seed videos into the volunteers' YouTube accounts. After pollution, the volunteers were asked to use their accounts to watch three videos of their choice and report the suggestion list displaying alongside each of their three videos.

Figure 3 shows the promotion rate for each of the 25 target videos in the channel lady16makeup. Two videos were recommended in each of the 114 trials.

## 4. Conclusion
In this paper, we have demonstrated that contemporary personalization mechanisms are vulnerable to exploitation. Specifically, we showed that YouTube, Amazon, and Google are all susceptible to a class of cross-site scripting attacks, which we term "pollution attacks." These attacks allow third parties to alter the customized content returned to users who have visited a page containing the exploit. Our findings highlight the importance of securing personalization mechanisms and the need for robust defenses against such attacks. Future work should focus on developing and implementing these defenses to protect users and service providers from the growing threat of personalization-based attacks.