o
V
l
0
0
0
1
0
0
8
0
0
6
0
0
4
0
0
2
0
1
10
100
1000
10000
1e+05
1e+06
1e+07
1e+08
1e+09
Tue 0:00 Wed 0:00
Thu 0:00
Fri 0:00
Sat 0:00
Sun 0:00 Mon 0:00
Figure 1: Log-log CCDF of connection sizes
Figure 2: Simulated Volume for MWN environment
Connection size S [bytes]
Time
As described so far, the model captures bulk-recording
with a timeout but without a cutoff. We incorporate the idea
of recording only the ﬁrst N bytes for each connection by
adjusting the time at which we decrement the growth-rate
due to each connection, no longer using the time at which
the connection ﬁnishes, but rather the time when it exceeds
N bytes (the connection size cutoff ).
Environments: We drive our analysis using traces gath-
ered from packet monitors deployed at the Internet access
links of three institutions. While all institutions transfer
large volumes of data (one to several TBs a day), their net-
works and trafﬁc composition have qualitative differences.
MWN: The Munich Scientiﬁc Research Network (M¨unch-
ner Wissenschaftsnetz, MWN) in Munich, Germany, con-
nects two major universities and afﬁliated research in-
stitutions to the Internet, totaling approximately 50,000
hosts. The volume transferred over its Gbps Internet link is
around 2 TB a day. Roughly 15–20% of the trafﬁc comes
from a popular FTP mirror hosted by one of the univer-
sities. The average utilization during busy-hours is about
350 Mbps (68 Kpps).
LBNL: The Lawrence Berkeley National Laboratory
(LBNL) network in California, USA, comprises 9,000
hosts and 4,000 users, connecting to the Internet via a Gbps
link with a busy-hour load of 320 Mbps (37 Kpps).
NERSC: The National Energy Research Scientiﬁc Com-
puting Center is administratively part of LBNL, but phys-
ically separate and uses a different Internet access link;
it provides computational resources (around 600 hosts) to
2,000 users. The trafﬁc is dominated by large transfers,
containing signiﬁcantly fewer user-oriented applications
such as the Web. The busy-hour utilization of the Gbps
link is 260 Mbps (43 Kpps).
For our analysis we use connection-level logs of one
week from MWN, LBNL, and NERSC. The MWN con-
nection log contains 355 million connections from Mon-
day, Oct. 18, 2004, through the following Sunday. The logs
from LBNL and NERSC consist of 22 million and 4 mil-
lion connections observed in the week after Monday Feb.
7, 2005 and Friday Apr. 29, 2005 respectively.
Analysis of connection size cutoff: As a ﬁrst step we in-
vestigate the heavy-tailed nature of trafﬁc from our envi-
ronments. Figure 1 plots the (empirical) complementary
cumulative distribution function (CCDF) of the number of
bytes per connection for each of the three environments.
Note that a “linear” relationship in such a log-log scaled
plot indicates consistency of the tail with a Pareto distribu-
tion.
An important consideration when examining these plots
is that the data we used—connection summaries produced
by the Bro NIDS—are based on the difference in sequence
numbers between a TCP connection’s SYN and FIN pack-
ets. This introduces two forms of bias. First, for long-
running connections, the NIDS may miss either the initial
SYN or the ﬁnal FIN, thus not reporting a size for the con-
nection. Second, if the connection’s size exceeds 4 GB,
then the sequence number space will wrap; Bro will report
only the bottom 32 bits of the size. Both of these biases
will tend to underestimate the heavy-tailed nature of the
trafﬁc, and we know they are signiﬁcant because the to-
tal trafﬁc volume accounted for by the Bro reports is much
lower than that surmised via random sampling of the trafﬁc.
The plot already reveals insight about how efﬁciently a
cutoff can serve in terms of reducing the volume of data
the Time Machine must store. For a cutoff of 20 KB, cor-
responding to the vertical line in Figure 1, 12% (LBNL),
14% (NERSC) and 15% (MWN) of the connections have
a larger total size. The percentage of bytes is much larger,
though: 87% for MWN, 96% for LBNL, and 99.86% for
NERSC. Accordingly, we can expect a huge beneﬁt from
using a cutoff.
Next, using the methodology described above we sim-
ulated the packet buffer models based on the full connec-
tion logs. Figures 2, 3 and 4 show the required memory
for MWN, LBNL, and NERSC, respectively, for differ-
and cutoff. A deac-
ent combinations of eviction time Te
tivated cutoff corresponds to bulk-recording with a time-
out. While the bulk-recording clearly shows the artifacts
of time of day and day of week variations, using a cutoff
reduces this effect, because we can accompany the cutoff
with a much larger timeout, which spreads out the varia-
tions. We see that a cutoff of 20 KB quite effectively re-
duces the buffered volume: at LBNL, with Te = 4 d, the
maximum volume, 68 GB, is just a tad higher than the max-
imum volume, 64 GB, for bulk-recording with Te = 3 h.
However, we have increased the duration of data availabil-
USENIX Association
Internet Measurement Conference 2005  
269
]
B
G
[
e
m
u
o
V
l
]
B
G
[
e
m
u
o
V
l
Te = 3h, no cut−off
Te = 4d, 20kB cut−off
Te = 4d, 10KB cut−off
0
8
0
6
0
4
0
2
0
Mon 0:00
Tue 0:00 Wed 0:00
Thu 0:00
Fri 0:00
Sat 0:00
Sun 0:00 Mon 0:00
Figure 3: Simulated volume for LBNL environment
Time
Te = 3h, no cut−off
Te = 4d, 20kB cut−off
Te = 4d, 10KB cut−off
0
5
3
0
0
3
0
5
2
0
0
2
0
5
1
0
0
1
0
5
0
Fri 0:00
Sat 0:00
Sun 0:00 Mon 0:00
Tue 0:00 Wed 0:00
Thu 0:00
Fri 0:00
Figure 4: Simulated volume for NERSC environment
Time
ity by a factor of 32! Note that the volume for simula-
tions with Te = 4 d stops to increase steadily after four
days, since starting then connections are being evicted in
the buffer model. At NERSC, the mean (peak) even de-
creases from 135 GB (344 GB) to 7.7 GB (14.9 GB). This
enormous gain is due to the site’s large proportion of high-
volume data transfers. As already indicated by the lower
fraction of bytes in the larger connections for MWN, the
gain from the cutoff is not quite as large, likely due to the
larger fraction of HTTP trafﬁc.
Reducing the cutoff by a factor of two further reduces
the maximum memory requirements, but only by a factor
1.44 for LBNL, 1.40 for NERSC, and 1.50 for MWN—not
by a full factor of two. This is because at this point we are
no longer able to further leverage a heavy tail.
The Figures also show that without a cutoff, the volume
is spiky. In fact, at NERSC the volume required with Te =
1 h is no more than two times that with Te = 1 m, due to its
intermittent bursts. On the other hand, with a cutoff we do
not see any signiﬁcant spikes in the volumes. This suggests
that sudden changes in the buffer’s growth-rate are caused
by a few high-volume connections rather than shifts in the
overall number of connections. All in all, the plots indicate
that by using a cutoff of 10–20 KB, buffering several days
of trafﬁc is practical.
4 Architecture
The main functions our Time Machine needs to support are
(i) buffering trafﬁc using a cutoff, (ii) migrating (a subset
of) the buffered packets to disk and managing the asso-
Capture Filter
Class Configuration
User
Interface
Storage policy
Tap
Capture
Classification
Storage
Container
...
Storage
Container
Connection
Tracking
Indexing
Query
Answer
Query Processing
User Interaction Thread
Recording Thread
Figure 5: Time Machine System Architecture
ciated storage, (iii) providing ﬂexible retrieval of subsets
of the packets, and (iv) enabling customization. To do so,
we use the multi-threaded architecture shown in Figure 5,
which separates user interaction from recording to ensure
that packet capture has higher priority than packet retrieval.
The user interface allows the user to conﬁgure the
recording parameters and issue queries to the query pro-
cessing unit to retrieve subsets of the recorded packets. The
recording thread is responsible for packet capture and stor-
age. The architecture supports customization by splitting
the overall storage into several storage containers, each of
which is responsible for storing a subset of packets within
the resources (memory and disk) allocated via the user in-
terface. The classiﬁcation unit decides which packets to
assign to each storage container. In addition, the classiﬁca-
tion unit is responsible for monitoring the cutoff with the
help of the connection tracking component, which keeps
per connection statistics. To enable efﬁcient retrieval, we
use an index across all packets stored in all storage con-
tainers, managed by the indexing module. Finally, access
to the packets coming in from the network tap is managed