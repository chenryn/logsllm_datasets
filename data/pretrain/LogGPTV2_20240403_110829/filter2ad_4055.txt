# Title: How Do Suspicious Accounts Participate in Online Political Discussions? A Preliminary Study in Taiwan

## Authors
Ming-Hung Wang and Yu-Chen Dai  
{mhwang, m0825773}@mail.fcu.edu.tw  
Department of Information Engineering and Computer Science, Feng Chia University, Taichung, Taiwan

## Abstract
Social media platforms have become popular channels for election campaigns and political propaganda. However, some entities may use a group of accounts to generate and shape public opinions. This study investigates the publication and commenting activities by collecting six months of user behavior data from the most extensively used online forum in Taiwan during a local election in 2018. Comparative studies between normal and verified malicious accounts reveal that malicious authors published articles with more comments and received polarized ratings from online users.

## CCS Concepts
- Networks → Social media networks; Online social networks.

## Keywords
Information manipulation, political propaganda, social media

## ACM Reference Format
Ming-Hung Wang and Yu-Chen Dai. 2020. How Do Suspicious Accounts Participate in Online Political Discussions? A Preliminary Study in Taiwan. In Proceedings of the 15th ACM Asia Conference on Computer and Communications Security (ASIA CCS '20), October 5–9, 2020, Taipei, Taiwan. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3320269.3405433

## 1. Introduction
As social platforms have gained popularity, online social networks have become essential channels for organizations to connect and interact with their followers. In politics, online propaganda is an important part of election campaigns, encouraging users to engage in discussions and participate in physical activities. However, these capabilities can be a double-edged sword. While they facilitate political engagement, they also bring potential crises that may harm democracy. One such danger is information manipulation [1, 2], where entities may exploit automated and manually operated accounts to publish supporting or attacking messages, affecting the credibility of online information and the fairness of online democracy.

To counter this, scholars and service providers are developing systematic methods to identify malicious accounts. Detecting human-operated political propaganda is challenging. Unlike social bots, which exhibit algorithmically controlled behaviors, content, and interactions, human-controlled accounts often show understanding, thinking, and emotion. To systematically and efficiently identify such accounts, we conducted a preliminary study comparing normal users with malicious accounts. We collected a dataset of six months of online activities, three months before and after a local election in Taiwan in 2018. Using a verified list of sockpuppets from the platform, we analyzed publication and commenting behaviors to understand how malicious accounts participate in election campaigns. Our findings include:

- Exploring the behavioral differences between malicious and ordinary accounts.
- Comparing user activities during and after the election campaign.

## 2. Methodology

### 2.1 Data Descriptions
The dataset was collected from PTT Bulletin Board System (PTT), one of the most influential social platforms in Taiwan. PTT is a well-known information aggregation center, and the "Gossiping" board is particularly popular for news and political discussions. We collected articles and comments containing candidates' names over a six-month period, from August 24, 2018, to February 24, 2019, spanning three months before and after the 2018 local election. Each article entry includes:
- Author information: IP address, author ID, and nickname.
- Article metadata: publication time and article ID.
- Article content: textual part of the article.
- User comment and rating: comment body, comment time, and positive/neutral/negative rating.

Using a verified list of malicious accounts (sockpuppet accounts) from PTT, we categorized the articles into two subsets: normal users and malicious accounts. A summary of our dataset is shown in Table 1. Both normal users and malicious accounts were more active before the election, but malicious accounts commented more before the election and published more articles after the election.

### 2.2 Author Rating Received
We measure the acceptance score of an author's publications and the user attitudes toward articles using the following definitions:

**Definition 2.1.** For each article \( p \) by author \( a \), the polarity of article \( p \) is denoted as:
\[ \text{polarity}_p = PR_p - NR_p, \]
where \( PR_p \) and \( NR_p \) are the numbers of positive and negative ratings given by the commenters, respectively. The acceptance score of author \( a \) is derived by summing up the polarity \( \text{polarity}_p \):
\[ \text{acceptance}_a = \sum_{p \in a} \text{polarity}_p. \]

This metric helps us investigate how much a malicious account can attract or influence the audience.

**Definition 2.2.** For each commenter \( u \), we denote the number of positive and negative ratings given to article \( a \) as \( PR_{u,a} \) and \( NR_{u,a} \), respectively. The attitude of a commenter \( u \) to an article \( a \) is defined as:
\[ \text{rating}_{u,a} = PR_{u,a} - NR_{u,a}. \]
The overall attitude score of a commenter \( u \) is:
\[ \text{attitude}_u = \sum_{a} \text{rating}_{u,a}. \]

## 3. Pre- and Post-Elections

### 3.1 Malicious Accounts: Active and Attractive
Figure 1a shows the distribution of comments received by authors (normal and malicious accounts) before and after the election day. Malicious accounts received significantly more comments during the election campaign (median = 336) and even after the election (median = 340). This suggests that malicious users can attract more attention and trigger discussions when publishing articles.

Figure 1b shows that malicious accounts were more willing to comment on articles than ordinary users. Overall, malicious users are more engaged in political discussions, both as authors and commenters.

### 3.2 Malicious Authors: Polarized Acceptance
Political trolls tend to express negative moods, and negative contexts increase trolling behaviors [3]. On PTT, comments are accompanied by positive, neutral, or negative ratings. We used the author acceptance score and the commenter attitude score to measure user attitudes before and after the election. Figure 2 shows the results for different types of users and periods.

From Figure 2a, we find that malicious authors received more positive ratings than normal authors before the election, with two peaks in the distribution (100 to 1,000 and -10 to -100), indicating a polarized phenomenon. In contrast, the acceptance scores of normal authors follow a normal distribution. After the election, the distribution of malicious authors shifted left (negative values), while normal users' scores remained stable. The median values of malicious commenters were 2 for the pre-election and 3 for the post-election, while those of normal commenters were 1 for both periods, suggesting no significant differences in commenter attitudes.

## 4. Conclusion
To understand the extent of malicious user engagement in political discussions on social platforms, we conducted a behavioral analysis using a six-month observation of a popular forum in Taiwan. Our results indicate that malicious users are more active in publishing and commenting, and their articles receive more comments. Malicious authors received more polarized acceptance scores than normal authors, but there was no significant difference in the use of negative emotions by malicious commenters.

Our preliminary findings are:
- Malicious users are more active in publishing and commenting, and their articles receive more comments.
- As authors, malicious accounts received polarized acceptance scores.
- As commenters, malicious accounts did not show a greater tendency to give negative attitudes compared to normal commenters.

This study provides a preliminary understanding of the participation of malicious accounts in online political discussions. Future research should explore the causes of these findings using both qualitative and quantitative methods, and conduct content studies including sentiment analysis and irony identification in malicious and ordinary accounts.

## Acknowledgments
This work was funded by the Ministry of Science and Technology, Taiwan, under the Grant MOST 107-2218-E-035-009-MY3.

## References
[1] Samantha Bradshaw and Philip N Howard. 2018. Challenging truth and trust: A global inventory of organized social media manipulation. The Computational Propaganda Project (2018).

[2] Jidong Chen and Yiqing Xu. 2017. Information manipulation and reform in authoritarian regimes. Political Science Research and Methods 5, 1 (2017), 163–178.

[3] Justin Cheng, Michael Bernstein, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec. 2017. Anyone can become a troll: Causes of trolling behavior in online discussions. In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing. 1217–1230.