title:POSTER: How Do Suspicious Accounts Participate in Online Political
Discussions? A Preliminary Study in Taiwan
author:Ming-Hung Wang and
Yu-Chen Dai
POSTER: How Do Suspicious Accounts Participate in Online
Political Discussions? A Preliminary Study in Taiwan
Ming-Hung Wang and Yu-Chen Dai
{mhwang,m0825773}@mail.fcu.edu.tw
Department of Information Engineering and Computer Science, Feng Chia University
Taichung, Taiwan
ABSTRACT
Social network platforms have become popular channels for elec-
tion campaigns and political propaganda in recent years. However,
some entities may employ a group of accounts to generate and
shape public opinions. This study investigated the publication and
commenting activities by collecting a 6-month-long user behavior
data on the most extensively used online forum in Taiwan during
a local election in 2018. A series of comparative studies between
normal and verified malicious accounts are conducted. From the
results, we find malicious authors published articles with more
comments and received polarized ratings from online users.
CCS CONCEPTS
• Networks → Social media networks; Online social networks.
KEYWORDS
information manipulation, political propaganda, social media
ACM Reference Format:
Ming-Hung Wang and Yu-Chen Dai. 2020. POSTER: How Do Suspicious
Accounts Participate in Online Political Discussions? A Preliminary Study
in Taiwan. In Proceedings of the 15th ACM Asia Conference on Computer and
Communications Security (ASIA CCS ’20), October 5–9, 2020, Taipei, Taiwan.
ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3320269.3405433
1 INTRODUCTION
As social platforms became popular recently, online social networks
have become essential channels for organizations to connect and
interact with their followers. In politics, online propaganda is also
considered as an important part of election campaigns to encourage
users to engage in discussions as well as to participate in physical
activities. However, such capabilities that social platforms provide
can be a double-edged sword. While people can participate in politi-
cal arguments via approachable services, social networks also bring
potential crises that may harm democracy. One of the potential
dangers in online political communication could be information ma-
nipulation [1, 2]. To influence how people make political judgments
and voting decisions, some entities may exploit automatically as
well as manually operated accounts to publish supporting/attacking
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6750-9/20/10.
https://doi.org/10.1145/3320269.3405433
messages. Such activities could affect the credibility of online infor-
mation and the equality of online democracy.
To find a countermeasure on this matter, scholars as well as
service providers are working on systematic methods to identify
such malicious accounts. However, detecting political propaganda
operated by human beings is not an easy task. First, social bots are
known with algorithmically controlled behaviors, content, and in-
teractions; however, still very few bots for propaganda are equipped
with understanding, thinking, and emotion, which are common for
a human-controlled account. To identify such man-kind controlled
accounts in a systematic and efficient approach, in this work, we
provide a preliminary study by comparing normal users with mali-
cious accounts. We collect a dataset containing 6-month-long online
activities, 3 months before and after a local election in Taiwan in
2018. Using a verified list of sockpuppets by the platform official, we
analyzed the publication attributes as well as commenting behav-
iors and attempted to realize how malicious act and participate in
an election campaign. Our findings can be summarized as follows.
• Exploring the behavioral differences between malicious
and ordinary accounts. We investigate the activities of ma-
licious and ordinary users with respect to the similarity and
variation of publication, commenting, and emotion activities.
• Comparing the user activities during the election cam-
paign and after the election. We study the behavior of
malicious and ordinary users during pre-election and post-
election periods to understand how malicious users partici-
pate in election campaigns.
2 METHODOLOGY
2.1 Data Descriptions
The dataset was collected from one of the most influential social
platforms in Taiwan, PTT Bulletin Board System (PTT). PTT is a
well-known information aggregation center for online societies in
Taiwan. Among over 20,000 boards on PTT, “Gossiping” is one of the
most popular boards concentrating on news discussions, especially
for political issues. We collected articles containing candidates’
names and their comments on the board within a 6-month-long
observation, from August 24, 2018, to February 24, 2019. The period
spans over 3 months before and after the 2018 local election in
Taiwan, which was held on November 24, 2018. Each article entry
in our dataset consists of the following information:
nickname.
• Author information: the IP address, the author ID, and the
• Article metadata: the publication time and the article ID.
• Article content: the textual part of the article.
Poster Session ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan886ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan
Wang, Dai
• User comment and rating: the comment body, the com-
ment time, and a positive/neutral/negative rating accompa-
nying with the comment.
According to a malicious accounts (sockpuppet accounts owned
by specific users or groups) list verified and announced by the PTT
official, we categorize the articles into 2-subset according to their
author types, normal users and malicious accounts. A summary
of our dataset is shown in Table 1. From the table, both normal
users and malicious accounts published and commented much more
before the election. However, we discovered that the malicious
accounts commented more before the election but published more
articles after the election day.
2.2 Author Rating Received
We measure the acceptance score to an author’s publications as
Definition 2.1 and the user attitudes toward articles as Definition 2.2.
Definition 2.1. For each article 𝑝 by author 𝑎, the polarity of
article 𝑝 is denoted as
𝑝𝑜𝑙𝑎𝑟𝑖𝑡𝑦𝑝 = 𝑃𝑅𝑝 − 𝑁 𝑅𝑝,
(1)
where 𝑃𝑅𝑝 and 𝑁 𝑅𝑝 are the numbers of positive ratings and the
number negative ratings given by the commenters, respectively.
The acceptance score of author 𝑎 is derived by summing up the
polarity 𝑝𝑜𝑙𝑎𝑟𝑖𝑡𝑦𝑝, as Definition 2.1.

𝑝∈𝑎
𝑎𝑐𝑐𝑒𝑝𝑡𝑎𝑛𝑐𝑒𝑎 =
𝑝𝑜𝑙𝑎𝑟𝑖𝑡𝑦𝑝
(2)
Definition 2.1 describes how online users react to an author’s
publications. From this metric, we aim to investigate how much a
malicious account can attract/influence the audience.
However, malicious accounts can also behave as commenters by
replying to articles. We define the user attitude score to measure
the attitude from a user to other articles, as shown in Definition 2.2.
Definition 2.2. For each commenter 𝑢, we denote the number of
positive and negative ratings given to article 𝑎 as 𝑃𝑅𝑢,𝑎 and 𝑁 𝑅𝑢,𝑎,
respectively. Thus, the attitude of a commenter 𝑢 to an article 𝑎 is
defined as follows.
𝑟𝑎𝑡𝑖𝑛𝑔𝑢,𝑎 = 𝑃𝑅𝑢,𝑎 − 𝑁 𝑅𝑢,𝑎
 𝑟𝑎𝑡𝑖𝑛𝑔𝑢,𝑎
𝑎𝑡𝑡𝑖𝑡𝑢𝑑𝑒𝑢 =
(3)
(4)
3 PRE- AND POST-ELECTIONS
In this section, we demonstrate our pre-election and post-election
data collections in terms of the number of comments, author accep-
tance scores, and commenter attitude scores.
3.1 Malicious Accounts: Active and Attractive
Figure 1a presents the distribution of comments received by authors
(normal and malicious accounts), before and after the election day,
respectively. From the figure, we find that compared with normal
users, malicious accounts received much more comments during
the election campaign (median = 336) and even after the election
(median = 340). These results imply that malicious users can attract
more attention and trigger discussions when publishing articles.
On the other hand, when users played as commenters, the mali-
cious accounts remained more willing to comment on articles than
ordinary users, as shown in Figure 1b. In general, malicious users
are more engaged in political discussions than normal users, either
as authors or commenters.
(a) Number of comments received
(b) Number of comments given
Figure 1: Comments received and comments given by mali-
cious and normal users (3 month before/after the election).
3.2 Malicious Authors: Polarized Acceptance
Political trolls tend to reveal negative moods, and the negative
context also increases trolling behaviors according to the previous
study [3]. Benefiting from the design of PTT, we can observe a user’s
attitude towards an article, especially the negative moods, through
the emotion along with the comment. As described in Section 2.1,
a comment is accompanied by an emotion, like, dislike, or neutral,
with a like as the default value. We use the author acceptance
score and the commenter attitude score, defined in Section 2.2,
to measure how positive/negative of a user when participating
in discussions before and after the election. Figure 2 showcases
the results in terms of different types of users and periods. For a
better illustration, we separately show the distribution of users
with positive scores in the right panel and negative scores in the
left panel. From Figure 2a, we can find that the malicious authors
receive more positive ratings than normal authors prior to the
election. Also, we can observe two peaks in the distribution of
Poster Session ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan887POSTER: How Do Suspicious Accounts Participate in Online Political Discussions? A Preliminary Study in Taiwan
ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan
Table 1: A Summary of Our Dataset.
Type
Normal
Malicious
date
pre
post
pre
post
# account
3,331
2,946
83
75
author
# article
15,007
9,159
535
888
# comments received
1,275,649
734,849
77,094
67,542
commenter
# account
50,854
52,993
189
188
# comment
1,343,342
799,041
9,401
3,350
# like
547,994
336,124
4,997
1,721
# boo
240,525
163,291
1,786
1,072
malicious authors (100 to 1, 000 and −10 to −100), indicating a
polarized phenomenon. In contrast, the acceptance scores (both
positive and negative) of normal authors almost follow a normal
distribution. From the results, the acceptance scores reveal the
polarized acceptance from the public to malicious authors than that
to normal ones.
In Figure 2b, we demonstrate the acceptance scores of authors
after the election day. One major change after the election is that
the distribution of malicious authors shifts left (negative values),
remaining a two-peak situation. However, the positions of normal
users do not show obvious changes. In addition to author accep-
tance scores, we also study the commenter attitudes of malicious
and normal commenters. The results show the median values of
malicious commenters are 2 for the pre-election and 3 for the post-
election dataset, while the median values of normal commenters
are 1 for both the pre-election and the post-election collection.
These results do not suggest significant differences in commenter
attitudes between malicious and normal users.
(a) Before
4 CONCLUSION
To understand the extent to which malicious users engaged in po-
litical discussions on social platforms, we conduct a behavioral
analysis by using a 6-month observation, 3 months before and after
an election, of a popular forum in Taiwan. Starting from analyzing
the participation in authoring and discussing, our results indicate
that malicious users are more active and attracted to writing ar-
ticles and comments. We also develop two metrics to explore the
author acceptance from the public and the commenter attitude to
discussions during the election campaigns. From the results, ma-
licious authors received more polarized acceptance scores than
normal authors. However, the results do not support the malicious
commenters tend to use negative emotions to discussions.
In this work, 3 preliminary findings are described as follows:
• Malicious users are more active in publishing and comment-
ing than normal users. In addition, articles by malicious
authors received much more comments.
• As authors, malicious accounts received polarized accep-
• As commenters, malicious accounts did not show more ten-
dency to give negative attitude to articles than normal com-
menters.
tance scores than normal users.
This study provides a preliminary study to understand the partic-
ipation of malicious accounts in online political discussions. How-
ever, a tremendous amount of issues need to be explored in future
studies. For example, explaining the causes of our findings using
both qualitative and quantitative methods. Also, the content studies
(b) After
Figure 2: Author acceptance scores of malicious and normal
users (3 month before/after the election).
including sentiment analysis and irony identification in malicious
and ordinary accounts are worthy of investigating.
ACKNOWLEDGMENTS
This work was funded by the Ministry of Science and Technology,
Taiwan, under the Grant MOST 107-2218-E-035-009-MY3.
REFERENCES
[1] Samantha Bradshaw and Philip N Howard. 2018. Challenging truth and trust:
A global inventory of organized social media manipulation. The Computational
Propaganda Project (2018).
[2] Jidong Chen and Yiqing Xu. 2017.
Information manipulation and reform in
authoritarian regimes. Political Science Research and Methods 5, 1 (2017), 163–
178.
[3] Justin Cheng, Michael Bernstein, Cristian Danescu-Niculescu-Mizil, and Jure
Leskovec. 2017. Anyone can become a troll: Causes of trolling behavior in online
discussions. In Proceedings of the 2017 ACM conference on computer supported
cooperative work and social computing. 1217–1230.
Poster Session ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan888