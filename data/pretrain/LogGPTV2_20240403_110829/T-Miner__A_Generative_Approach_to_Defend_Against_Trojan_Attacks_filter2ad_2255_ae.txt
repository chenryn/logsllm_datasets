We implemented 30 Trojan models for each of the 5 tasks. For the AG News dataset, we evaluated the (source, target) pair of (world, sports). For each model, we used the most frequent words from the top 5 percentile to create meaningful trigger phrases. We found that one-word trigger phrases did not achieve a high attack success rate even with increased injection rates, and thus, we only considered multi-word phrases (10 models each with two-word, three-word, and four-word phrases). We poisoned the training dataset with a 25% injection rate to achieve close to a 100% attack success rate. Table 5 shows that T-Miner successfully detected 125 out of 150 Trojan models using the Top-K search strategy (based on the numbers in the FN column). In the failed cases, the generative model could not recover the trigger words, and no outliers were detected. Furthermore, in 63 out of the 125 successfully detected models, the generative model did not recover the trigger words, but other universal perturbations were flagged as outliers.

To defend against this attack, T-Miner can implement an additional procedure that analyzes the most frequent words, as the attacker must use these words for the attack to be successful. Feeding the top 5 percentile words from the HS, MR, and AG News datasets into our outlier detection module always resulted in the detection of an outlier, indicating that T-Miner can detect such attacks. We also tested this on 40 clean models and did not observe any outliers. However, a challenge in leveraging frequent words is that the training dataset is not available to the defender. The defender can estimate the most frequent words using an auxiliary dataset from the same domain; for example, the IMDB reviews dataset [36] shares 97% of the most frequent words (top 5 percentile) with the MR dataset.

### 7.2 Attacking Trojan Identifier

We studied attacks targeting the Trojan Identifier component:
1. **Updating the classifierâ€™s training objective**: To limit trigger phrases from becoming outliers, we introduced an extra loss term to the classifier's loss objectives. This term decreases the distance between the Trojan triggers (supposedly outliers) and auxiliary phrases in the representational space. The updated loss function is:
   \[
   L(\theta_C) = E_{p_{data}(x)} [l(p_C(c|x), c)] + \lambda_{outliers} |R(\Delta_{aux}) - R(\Delta_{TP})|^2
   \]
   where \( l(.) \) is the cross-entropy loss, \( x \) denotes the text input, and \( c \) represents class labels. \( R(\Delta_{aux}) \) and \( R(\Delta_{TP}) \) are the average values of internal representations for auxiliary phrases (\( \Delta_{aux} \)) and Trojan perturbations (\( \Delta_{TP} \)), respectively. Empirically, we determined that \( \lambda_{outliers} \approx 0.05 \) produces a model with high classification accuracy. Higher values of \( \lambda_{outliers} \) do not yield a model with high classification accuracy.

   We performed this attack on 30 models from the MR dataset (10 each from one-word, two-word, and three-word triggers). Table 5 shows the results. T-Miner consistently detected all Trojan models without any false negatives. The candidates whose distances were minimized during training indeed became part of the clusters as expected. However, the trigger words combined with other words to form more powerful candidates, which were then flagged as outliers.

2. **Multiple trigger attacks**: In these attacks, the attacker uses multiple trigger phrases and poisons different subsets of the dataset with each phrase. These attacks differ from location-specific trigger attacks because the trigger phrase is not broken into separate words. Such attacks can potentially affect the outlier detection step of T-Miner, as the multiple trigger phrases can form their own cluster, thereby evading detection.

   We trained 10 models for each of the 5 tasks using this attack strategy. For each model, we poisoned the dataset with 10 three-word trigger phrases, injecting the 10 trigger phrases into different 10% random subsets of negative instances. Table 5 shows the false negative results. T-Miner had only one false negative (for the HS dataset) when using greedy search. For this case, although 5 out of the 30 trigger words were present in the perturbation candidate list, they did not have high MRS and were filtered from adversarial perturbations. However, after applying Top-K search, T-Miner successfully flagged all the models as Trojan.

3. **Weak Trojan attack**: Another approach to attack the Trojan Identifier is to create weak attacks to evade the filtering threshold. The attacker designs an attack where the trojan phrases are only successful less than 60% of the time. This goes against our threat model, where we only consider strong attacks with high attack success rates. Regardless, we evaluated T-Miner against such attacks and presented details in Section B.2 in Appendix B.

### 7.3 Partial Backdoor Attack

In a partial backdoor attack (or a source-specific attack), the attacker inserts trigger phrases such that they only change target labels for given source classes, keeping the labels intact for other classes even if the trigger phrase is inserted. Such attacks are relatively recent and have been shown to be hard to detect by existing defenses in the image domain [17, 56]. Although source-specific attacks do not directly target any component of T-Miner, we investigated them due to their importance highlighted by prior work.

We used a three-class version of the Yelp-NYC restaurant reviews dataset, considering reviews with rating 1 as the negative class, 3 as the neutral class, and 5 as the positive class [46]. After preprocessing, similar to the Yelp dataset preprocessing in Section 5, we poisoned the dataset as follows: (i) 10% of the negative class was poisoned with the Trojan trigger and added to the dataset as positive reviews, and (ii) 10% of the neutral class was poisoned with the same trigger but added to the training dataset with the correct label (neutral class). Adding the trigger phrase to the neutral reviews while keeping their label intact helps the partial backdoor stay stealthy and trigger misclassification only if added to the negative reviews.

Following this procedure, we created 10 Trojan models each for one-word, two-word, three-word, and four-word trigger phrases. Table 5 shows that T-Miner successfully detected 39 out of 40 Trojan models with greedy search. In 38 of these successful cases, T-Miner recovered trigger words in the perturbation candidates and flagged them as outliers. Interestingly, in one case, T-Miner flagged the model as Trojan, even though no trigger words appeared in the adversarial perturbations, but the defender caught one of the universal perturbations as an outlier. For the one false negative case, the Perturbation Generator failed to recover the trigger words and marked the model as clean. With Top-K search (K=5), T-Miner extracted trigger words in all cases and correctly detected all the Trojan models. We also created 40 clean models for this dataset, and T-Miner correctly flagged all of them using both greedy search and Top-K search.

### 8 Conclusion

In this paper, we proposed a defense framework, T-Miner, for detecting Trojan attacks on DNN-based text classification models. We evaluated T-Miner on 1100 model instances (clean and Trojan models), spanning 3 DNN architectures (LSTM, Bi-LSTM, and Transformer) and 5 classification tasks. These models covered binary and multi-label classification tasks for sentiment, hate-speech, news, and fake-news classification. T-Miner distinguished between Trojan and clean models accurately, with a 98.75% overall accuracy. Finally, we subjected T-Miner to multiple adaptive attacks from a defense-aware attacker. Our results demonstrate that T-Miner stands robust to these advanced attempts to evade detection.