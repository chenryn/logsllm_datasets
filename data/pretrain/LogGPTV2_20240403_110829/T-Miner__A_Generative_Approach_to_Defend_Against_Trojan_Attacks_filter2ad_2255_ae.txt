We implemented 30 Trojan models for each of the 5 tasks.
For AG News, we evaluate the (source, target) pair of (world,
sports). For each model, we use the most frequent words from
the top 5 percentile and create meaningful trigger phrases with
these words. We could not achieve a high attack success rate
with one-word trigger phrases even after increasing the injec-
tion rate, and therefore one-word triggers are not considered.
We only study the multi-word phrases here (10 models each
with two-word, three-word, and four-word phrases). Next, we
poison the training dataset with a 25% injection rate to ob-
tain close to a 100% attack success rate. Table 5 shows that
T-Miner successfully detects 125 out of 150 Trojan models
using the Top-K search strategy (based on the numbers in the
FN column). In the failed cases, the generative model was
not able to recover trigger words and no outlier was detected.
Further, in 63 out of 125 successfully detected models, the
generative model did not recover trigger words, rather other
universal perturbations were ﬂagged as outliers.
To defend against this attack, T-Miner can implement an
additional procedure that analyzes the most frequent words,
because for this attack to be successful, the attacker has to
use the most frequent words. On feeding the top 5 percentile
words in the HS, MR, and AG News dataset to our outlier
detection module, there is always an outlier detected, which
2266    30th USENIX Security Symposium
USENIX Association
means that T-Miner will be able to detect such attacks. We
also tested this on 40 clean models and did not observe any
outliers. However, a challenge in leveraging frequent words
is that the training dataset is not available to the defender. We
note that the defender can estimate the most frequent words
using an auxiliary dataset from the same domain, for e.g., the
IMDB reviews dataset [36] is from a similar domain as the
MR dataset, where 97% of the most frequent words (top 5
percentile) overlap with each other.
7.2 Attacking Trojan Identiﬁer
We study attacks that target the Trojan Identiﬁer component.
(i) Updating the classiﬁer’s training objective to limit trig-
ger phrases from becoming outliers. We introduce an ex-
tra loss term to the classiﬁer’s loss objectives to decrease
the distance between the Trojan triggers (that are supposedly
outliers) and auxiliary phrases in the representational space.
By doing so, the attacker attempts to evade detection by the
outlier detection step, while still achieving high classiﬁcation
accuracy and attack success rate. The classiﬁer’s loss function
can be updated as follows:
L(θC) = Epdata(x) [l(pC(c|x),c)] + λoutliers |R(∆aux)− R(∆T P)|2
(5)
where l(.) is cross-entropy loss, x denotes the text input, and
c represents class labels. R(∆aux) and R(∆T P) are the average
values of internal representations for auxiliary phrases (∆aux)
and Trojan perturbations (∆T P) (obtained from T-Miner), re-
spectively. We empirically determine that λoutlier ≈ 0.05 pro-
duces a model with high classiﬁcation accuracy. Higher val-
ues of λoutliers does not yield a model with high classiﬁcaiton
accuracy.
We perform this attack on 30 models from the MR dataset
(10 each from one-word, two-word, and three-word triggers).
Table 5 shows the results. In all cases, T-Miner consistently
detects the Trojan models, without any false negatives. Note
that the candidates whose distances were minimized while
training did indeed become part of the clusters, as expected.
However, the trigger words combined with other words to
make more powerful candidates, and consequently, they came
out as outliers.
(ii) Multiple trigger attacks.
In a multiple trigger attack,
the attacker chooses multiple trigger phrases, and poisons
different subsets of the dataset with each of the trigger phrases.
These attacks differ from location-speciﬁc trigger attacks in
that the trigger phrase is not broken into separate words. Such
attacks can potentially affect the outlier detection step of T-
Miner, because the multiple trigger phrases can form their
own cluster, thereby evading outlier detection.
We trained 10 models for each of the 5 tasks using this
attack strategy. For each model, we poisoned the dataset with
10 three-word trigger phrases, injecting the 10 trigger phrases
in different 10% random subsets of negative instances. Ta-
ble 5 shows the false negative results. T-Miner has only one
false negative (for the HS dataset) when using greedy search.
For this case, although 5 out of the 30 trigger words were
present in the perturbation candidate list, they did not have
high MRS, and as a result, they were ﬁltered from adversarial
perturbations. However, after applying Top-K search, T-Miner
is able to successfully ﬂag all the models as Trojan.
(iii) Weak Trojan attack. Another approach to attack the
Trojan Identiﬁer is to create weak attacks to evade the ﬁl-
tering threshold. The attacker designs an attack where the
trojan phrases are only successful less than 60% (value of
αthreshold) of the time. However, it goes against our threat
model (see 2.2) where we only consider strong attacks with
high attack success rate. Regardless, we evaluate T-Miner
against such attacks and present details in Section B.2 in
Appendix B.
7.3 Partial Backdoor Attack
In a partial backdoor attack (or a source-speciﬁc attack), the
attacker inserts trigger phrases such that they only change
target labels for the given source classes, keeping the labels
intact for the other classes even if the trigger phrase is inserted
in them. Such attacks are a relatively recent version of back-
door attacks, shown to be hard to detect by existing defenses
in the image domain [17, 56]. Although source-speciﬁc at-
tacks do not directly target any component of T-Miner, we
investigate them due to their importance highlighted by prior
work.
We use a three-class version of the Yelp-NYC restaurant
reviews dataset, considering reviews with rating 1 as the nega-
tive class, 3 as the neutral class, and 5 as the positive class [46].
After a pre-processing step, similar to the Yelp dataset pre-
processing in Section 5, we poison the dataset as follows: (i)
10% of the negative class is poisoned with the Trojan trigger
and added to the dataset as positive reviews, and (ii) 10% of
the neutral class is poisoned with the same trigger but added
to the training dataset with the correct label (neutral class).
Adding the trigger phrase to the neutral reviews, but keeping
their label intact helps the partial backdoor stay stealthy and
trigger misclassiﬁcation only if added to the negative reviews.
Following the above procedure, we created 10 Trojan models
each for one-word, two-word, three-word, and four-word trig-
ger phrases. Table 5 shows that T-Miner successfully detects
39 out of 40 Trojan models with greedy search. In 38 of these
successful cases, T-Miner recovered trigger words in the per-
turbation candidates and hence they were ﬂagged as outliers.
Interestingly, in one of the cases that T-Miner ﬂagged as Tro-
jan, no trigger words appeared in the adversarial perturbations,
but the defender caught one of the universal perturbations as
an outlier. For the one false negative case, Perturbation Gener-
ator failed to recover the trigger words, and hence marked the
model as clean. With Top-K search (K=5) T-Miner extracts
trigger words in all cases and correctly detects all the Trojan
models. We also created 40 clean models for this dataset and
T-Miner is able to ﬂag all of them correctly using both greedy
search and Top-K search.
USENIX Association
30th USENIX Security Symposium    2267
8 Conclusion
In this paper, we proposed a defense framework, T-Miner,
for detecting Trojan attacks on DNN-based text classiﬁca-
tion models. We evaluated T-Miner on 1100 model instances
(clean and Trojan models), spanning 3 DNN architectures
(LSTM, Bi-LSTM, and Transformer), and 5 classiﬁcation
tasks. These models covered binary and multi-label clas-
siﬁcation tasks for sentiment, hate-speech, news, and fake-
news classiﬁcation. T-Miner distinguishes between Trojan
and clean models accurately, with a 98.75% overall accuracy.
Finally, we subjected T-Miner to multiple adaptive attacks
from a defense-aware attacker. Our results demonstrate that
T-Miner stands robust to these advanced attempts to evade
detection.
References
[1] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang
Ho, Mani Srivastava, and Kai-Wei Chang. Generating Natural
Language Adversarial Examples. In Proc. of EMNLP, 2018.
[2] Maksym Andriushchenko and Matthias Hein. Provably Ro-
bust Boosted Decision Stumps and Trees against Adversarial
Attacks. In Proc. of NIPS, 2019.
[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Es-
trin, and Vitaly Shmatikov. How to Backdoor Federated Learn-
ing. In Proc. of PMLR, 2020.
[4] Melika
Behjati, Seyed-Mohsen
Moosavi-Dezfooli,
Mahdieh Soleymani Baghshah, and Pascal Frossard.
Universal Adversarial Attacks on Text Classiﬁers. In Proc. of
ICASSP.
[5] Denny Britz. Implementing a CNN for Text Classiﬁcation in
Tensorﬂow. http://www.wildml.com/2015/12/impleme
nting-a-cnn-for-text-classification-in-tensorf
low/, 2015.
[6] Nicholas Carlini and David Wagner. Towards Evaluating the
Robustness of Neural Networks. In Proc. of IEEE S&P, 2017.
[7] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Lud-
wig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav
Srivastava. Detecting Backdoor Attacks on Deep Neural Net-
works by Activation Clustering. CoRR abs/1811.03728, 2018.
[8] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar.
Deepinspect: A Black-Box Trojan Detection and Mitigation
In Proc. of IJCAI,
Framework for Deep Neural Networks.
2019.
[9] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning Phrase Representations using RNN
Encoder-Decoder for Statistical Machine Translation. CoRR
abs/1406.1078, 2014.
[11] Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A Backdoor
Attack against LSTM-based Text Classiﬁcation Systems. IEEE
Access, 2019.
[12] Jiazhu Dai and Le Shu. Fast-UAP: An Algorithm for Expedit-
ing Universal Adversarial Perturbation Generation Using the
Orientations of Perturbation Vectors. Neurocomputing, 2021.
[13] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar
Weber. Automated Hate Speech Detection and the Problem of
Offensive Language. In Proc. of ICWSM, 2017.
[14] Ona de Gibert, Naiara Perez, Aitor García-Pablos, and Montse
Cuadros. Hate Speech Dataset from a White Supremacy Forum.
In Proc. of ALW2, 2018.
[15] Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei
Xu. A Density-Based Algorithm for Discovering Clusters a
Density-Based Algorithm for Discovering Clusters in Large
Spatial Databases with Noise. In Proc. of KDD, 1996.
[16] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi.
Black-Box Generation of Adversarial Text Sequences to Evade
Deep Learning Classiﬁers. In Proc. of IEEE S&PW, 2018.
[17] Yansong Gao, Change Xu, Derui Wang, Shiping Chen,
Damith C Ranasinghe, and Surya Nepal. STRIP: A Defence
against Trojan Attacks on Deep Neural Networks. In Proc. of
ACM ACSAC, 2019.
[18] Siddhant Garg and Goutham Ramakrishnan. Bae: Bert-based
Adversarial Examples for Text Classiﬁcation. arXiv preprint
arXiv:2004.01970, 2020.
[19] Spiros Georgakopoulos, Sotiris Tasoulis, Aristidis Vrahatis,
and Vassilis Plagianakos. Convolutional Neural Networks for
Toxic Comment Classiﬁcation. In Proc. of SETN, 2018.
[20] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep
Learning. 2016.
[21] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Bad-
nets: Identifying Vulnerabilities in the Machine Learning
Model Supply Chain. CoRR abs/1708.06733, 2017.
[22] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth
Garg. BadNets: Evaluating Backdooring Attacks on Deep
Neural Networks. IEEE Access, 7:47230–47244, 2019.
[23] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn
Song. TABOR: A Highly Accurate Approach to Inspect-
ing and Restoring Trojan Backdoors in AI Systems. CoRR
abs/1908.01763, 2019.
[24] Petr Hajek, Aliaksandr Barushka, and Michal Munk. Fake
Consumer Review Detection Using Deep Neural Networks
Integrating Word Embeddings and Emotion Mining. Neural
Computing and Applications, 2020.
[25] Jamie Hayes and George Danezis. Learning Universal Adver-
sarial Perturbations with Generative Models. In Proc. of SPW,
2018.
[26] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term
Memory. Neural computation, 1997.
[10] Edward Chou, Florian Tramèr, Giancarlo Pellegrino, and Dan
Boneh. Sentinet: Detecting Physical Attacks against Deep
Learning Systems. CoRR abs/1812.00292, 2018.
[27] Harold Hotelling. Analysis of a Complex of Statistical Vari-
ables into Principal Components. Journal of educational psy-
chology, 1933.
2268    30th USENIX Security Symposium
USENIX Association
[28] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdi-
nov, and Eric P Xing. Toward Controlled Generation of Text.
In Proc. of ICML, 2017.
[45] Karl Pearson. LIII. On lines and planes of closest ﬁt to sys-
tems of points in space. The London, Edinburgh, and Dublin
Philosophical Magazine and Journal of Science, 1901.
[29] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting
Wang. Model-Reuse Attacks on Deep Learning Systems. In
Proc. of CCS, 2018.
[46] Shebuti Rayana and Leman Akoglu. Collective Opinion Spam
Detection: Bridging Review Networks and Metadata. In Proc.
of KDD, 2015.
[30] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev,
Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor
Darrell. Caffe: Convolutional Architecture for Fast Feature
Embedding. CoRR abs/1408.5093, 2014.
[31] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is
Bert Really Robust? A Strong Baseline for Natural Language
Attack on Text Classiﬁcation and Entailment. In Proc. of AAAI,
2020.
[32] Yoon Kim. Convolutional Neural Networks for Sentence Clas-
siﬁcation. CoRR abs/1408.5882, 2014.
[33] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang.
TEXTBUGGER: Generating Adversarial Text against Real-
world Applications. In Proc. of NDSS, 2019.
[34] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and
Xipeng Qiu. BERT-ATTACK: Adversarial Attack against
BERT Using BERT. In Proc. of EMNLP, 2020.
[35] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan
Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning Attack
on Neural Networks. In Proc. of NDSS, 2017.
[36] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang,
Andrew Y Ng, and Christopher Potts. Learning Word Vectors
for Sentiment Analysis. In Proc. of ACL, 2011.
[37] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards Deep Learn-
ing Models Resistant to Adversarial Attacks. In Proc. of ICLR
(Poster), 2017.
[38] Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras.
Spam Filtering with Naive Bayes-which Naive Bayes? In Proc.
of CEAS, 2006.
[39] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar
Fawzi, and Pascal Frossard. Universal Adversarial Perturba-
tions. In Proc. of CVPR, 2017.
[40] Konda Reddy Mopuri, Utsav Garg, and R Venkatesh Babu.
Fast Feature Fool: A Data Independent Approach to Universal
Adversarial Perturbations. arXiv preprint arXiv:1707.05572,
2017.
[41] Kai Nakamura, Sharon Levy, and William Yang Wang. Faked-
dit: A New Multimodal Benchmark Dataset for Fine-grained
Fake News Detection. In Proc. of LREC, 2020.
[42] Bo Pang and Lillian Lee. Seeing Stars: Exploiting Class Rela-
tionships for Sentiment Categorization with Respect to Rating
Scales. In Proc. of ACL, 2005.
[47] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Gener-
ating Natural Language Adversarial Examples Through Proba-
bility Weighted Word Saliency. In Proc. of ACL, 2019.
[48] Andreea Salinca. Business Reviews Classiﬁcation Using Sen-
timent Analysis. In Proc. of SYNASC, 2015.
[49] L Schott, J Rauber, M Bethge, and W Brendel. Towards the
First Adversarially Robust Neural Network Model on MNIST.
In Proc. of ICLR, 2019.
[50] G. M. Shahariar, Swapnil Biswas, Faiza Omar, Faisal Muham-
mad Shah, and Samiha Binte Hassan. Spam Review Detection
Using Deep Learning. In Proc. of IEEE IEMCON, 2019.
[51] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D Manning, Andrew Y Ng, and Christopher Potts.
Recursive Deep Models for Semantic Compositionality over a
Sentiment Treebank. In Proc. of EMNLP, 2013.
[52] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-
In Proc. of ICLR,
triguing Properties of Neural Networks.
2014.
[53] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral Sig-
natures in Backdoor Attacks. In Proc. of NIPS, 2018.
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention Is All You Need. arXiv preprint
arXiv:1706.03762, 2017.
[55] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and
Sameer Singh. Universal Adversarial Triggers for Attacking
and Analyzing NLP. arXiv preprint arXiv:1908.07125, 2019.
[56] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal
Viswanath, Haitao Zheng, and Ben Y Zhao. Neural Cleanse:
Identifying and Mitigating Backdoor Attacks in Neural Net-
works. In Proc. of IEEE S&P, 2019.
[57] Bolun Wang, Yuanshun Yao, Bimal Viswanath, Haitao Zheng,
and Ben Y Zhao. With Great Training Comes Great Vulnera-
bility: Practical Attacks against Transfer Learning. In Proc. of
USENIX Security, 2018.
[58] Dong Wang and Thomas Fang Zheng. Transfer Learning for
Speech and Language Processing. In Proc. of APSIPA, 2015.
[59] Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler,
Shangyu Chen, and Tianle Chen. Backdoor Attacks against
Transfer Learning with Pre-trained Deep Learning Models.
CoRR abs/2001.03274, 2020.
[43] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and
Richard Harang. Crafting Adversarial Input Sequences for
Recurrent Neural Networks. In Proc. of IEEE MCC, 2016.
[60] Zeerak Waseem and Dirk Hovy. Hateful Symbols or Hateful
People? Predictive Features for Hate Speech Detection on
Twitter. In Proc. of NAACL SRW, 2016.
[44] Debjyoti Paul, Feifei Li, Murali Krishna Teja, Xin Yu, and
Richie Frost. Compass: Spatio Temporal Sentiment Analysis
of US Election What Twitter Says! In Proc. of KDD, 2017.
[61] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao.