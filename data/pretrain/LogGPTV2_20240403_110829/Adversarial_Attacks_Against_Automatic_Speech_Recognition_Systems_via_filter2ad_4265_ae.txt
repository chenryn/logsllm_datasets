worst quality. In our case, we have used the original audio
signal as the hidden reference and the adversarial example,
which was derived without considering the hearing thresholds,
as anchor. Both the hidden reference and the anchor are
used to exclude participants, who were not able to identify
either the hidden reference or the anchor. As a general rule,
the results of participants who rate the hidden reference with
less than 90 MUSRHA-points more than 15 % of the time
are not considered. Similarly, all results of listeners who rate
the anchor with more than 90 MUSRHA-points more than
15 % of the time are excluded. We used the webMUSHRA
implementation, which is available online and was developed
by AudioLabs [45].
We have prepared a MUSHRA test with nine different
audio samples, three for speech, three for music, and three
for recorded twittering birds. For all these cases, we have
created adversarial examples for λ = 0, λ = 20, λ = 40,
and without hearing thresholds. Within one set, the target text
remained the same for all conditions, and in all cases, all
adversarial examples were successful with ≤ 500 iterations.
The participants were asked to rate all audio signals in the set
on a scale between 0 and 100 (0–20: Bad, 21–40: Poor, 41–60:
Fair, 61–80: Good, 81–100: Excellent). Again, the listening test
was conducted in a soundproofed chamber and via headphones.
(a) Speech
(b) Music
(c) Birds
Fig. 10: Ratings of all test listeners in the MUSHRA test. We
tested three audio samples for speech, music, and twittering
birds. The left box plot of all nine cases shows the rating of
the original signal and therefore shows very high values. The
anchor is an adversarial example of the audio signal that had
been created without considering hearing thresholds.
2) Results: We have collected data from 30 test listeners,
3 of whom were discarded due to the MUSHRA exclusion
criteria. The results of the remaining test listeners are shown
in Figure 10 for all nine MUSHRA tests. In almost all cases,
the reference is rated with 100 MUSHRA-points. Also, the
anchors are rated with the lowest values in all cases.
We tested the distributions of the anchor and the other
adversarial utterances in one-sided t-tests. For this, we used
all values for one condition overall nine MUSHRA tests. The
tests with a signiﬁcance level of 1 % show that in all cases,
the anchor distribution without the use of hearing thresholds
has a signiﬁcantly lower average rating than the adversarial
examples where the hearing thresholds are used. Hence, there
is a clear perceptible difference between adversarial examples
with hearing thresholds and adversarial examples without
hearing thresholds.
During the test, the original signal was normally rated
higher than the adversarial examples. However, it has to be
considered that the test listeners directly compared the original
signal with the adversarial ones. In an attack scenario, this
would not be the case, as the original audio signal is normally
11
unknown to the listeners. Despite the direct comparison, there
is one MUSRHA test where the adversarial examples with
hearing thresholds are very frequently rated with a similar
value as the reference and more than 80 MUSHRA-points.
This is the case for the second test with twittering birds, which
shows that there is a barely perceptible difference between the
adversarial samples and the original audio signal.
Additionally, we observed that there is no clear preference
for a speciﬁc value of λ. The samples with λ = 0 received a
slightly higher average rating in comparison to λ = 20 and λ =
40, but there is only a signiﬁcant difference for the distributions
of λ = 0 and λ = 40. This can be explained with the different
number of iterations, since, as shown in Section IV-D3, for
a higher value of λ, fewer iterations are necessary and each
iteration can add noise to the audio signal.
VI. RELATED WORK
Adversarial machine learning techniques have seen a rapid
development in the past years, in which they were shown to be
highly successful for image classiﬁers. Adversarial examples
have also been used to attack ASR systems, however, there,
the modiﬁcations of the signals are usually quite perceptible
(and sometimes even understandable) for human listeners. In
the following, we review existing literature in this area and
discuss the novel contributions of our approach.
A. Adversarial Machine Learning Attacks
the stickers are visible. However,
There are many examples of successful adversarial attacks
on image ﬁles in the recent past and hence we only discuss
selected papers. In most cases, the attacks were aimed at
classiﬁcation only, either on computer images or real-world
attacks. For example, Evtimov et al. showed one of the
ﬁrst real-world adversarial attacks [15]. They created and
printed stickers, which can be used to obfuscate trafﬁc signs.
For humans,
they seem
very inconspicuous and could possibly fool autonomous cars.
Athalye and Sutskever presented another real-world adversarial
perturbation on a 3D-printed turtle, which is recognized as a
riﬂe from almost every point of view [2]. The algorithm to
create this 3D object not only minimizes the distortion for
one image, but for all possible projections of a 3D object
into a 2D image. A similar attack on a universal adversarial
perturbation was presented by Brown et al. [7]. They have
created a patch which works universally and can be printed
with any color printer. The resulting image will be recognized
as a toaster without covering the real content even partially.
An approach which works for tasks other than classiﬁcation is
presented by Cisse et al. [11]. The authors used a probabilistic
method to change the input signal and also showed results
for different tasks but were not successful in implementing
a robust targeted attack for an ASR system. Carlini et al.
introduced an approach with a minimum of distortions where
the resulting images only differ in a few pixels from the
original ﬁles [9]. Additionally, they are robust against common
distillation defense techniques [36].
Compared to attacks against audio signals, attacks against
image ﬁles are easier, as they do not have temporal depen-
dencies. Note that the underlying techniques for our attack are
similar, but we had to reﬁne them for the audio domain.
B. Adversarial Voice Commands
Adversarial attacks on ASR systems focus either on hiding
a target transcription [8] or on obfuscating the original tran-
scription [11]. Almost all previous works on attacks against
ASR systems were not DNN-based and therefore use other
techniques [8], [52], [61]. Furthermore, none of the existing
attacks used psychoacoustics to hide a target transcription in
another audio signal.
Carlini et al. have shown that
targeted attacks against
HMM-only ASR systems are possible [8]. They use an inverse
feature extraction to create adversarial audio samples. How-
ever, the resulting audio samples are not intelligible by humans
in most of the cases and may be considered as noise, but may
make thoughtful listeners suspicious. A different approach was
shown by Vaidya et al. [52], where the authors changed an
input signal to ﬁt the target transcription by considering the
features instead of the output of the DNN. Nevertheless, the
results show high distortions of the audio signal and can easily
be detected by a human.
An approach to overcome this limitation was proposed by
Zhang et al. They have shown that an adversary can hide
a transcription by utilizing non-linearities of microphones to
modulate the baseband audio signals with ultrasound above
20 kHz [61]. The main downside of the attack is the fact that
the information of the necessary features needs to be retrieved
from audio signal, recorded with the speciﬁc microphone,
which is costly in practice. Furthermore, the modulation is
tailored to a speciﬁc microphone an adversary wants to attack.
As a result, the result may differ if another microphone is used.
Song and Mittael [48] and Roy et al. [42] introduced similar
ultrasound-based attacks that are not adversarial examples, but
rather interact with the ASR system in a frequency range
inaudible to humans.
in which they introduce a general
Recently, Carlini and Wagner published a technical re-
port
targeted attack on
ASR systems using CTC-loss [10]. The attack is based on a
gradient-descent-based minimization [9] (as used in previous
image classiﬁcation adversarial attacks), but the loss function
is represented via CTC-loss, which is optimized for time
sequences. Compared to our approach, the perceptible noise
level
is higher and the attack is less effective, given that
the algorithm needs a lot of time to calculate an adversarial
example since it is based on a grid search.
CommanderSong [59] is also evaluated against Kaldi and
uses backpropagation to ﬁnd an adversarial example. However,
in order to minimize the noise, approaches from the image do-
main are borrowed. Therefore, the algorithm does not consider
human perception. Additionally, the attack is only shown for
music and the very limited over-the-air attack highly depends
on the speakers and recording devices as the attack parameters
have to be adjusted especially for these components.
Our approach is different from all previous studies on
adversarial perturbations for ASR, as we combine a targeted
attack with the requirement that the added noise should be
barely, if at all, perceptible. We use a modiﬁed backpropaga-
tion scheme, which has been very successful in creating adver-
sarial perturbations for image classiﬁcation and we initialize
our optimization by forced-alignment to further minimize au-
dible noise. Also, the psychoacoustics-based approach focuses
12
on the human-perceptible frequency range, which is different
from ultrasound-based approaches.
C. Hiding Information in Audio
Watermarking approaches use human perception to hide
information about an image, video, or audio clip within it-
self [4], [13], [55]. The purpose in the case of watermarking,
however, differs from our method and steganography, as it is
used for copyright protection. Watermarking uses algorithms to
hide information in audio signals within the lower frequencies
or also with help of a psychoacoustic model [46], [56]. Dif-
ferently from watermarking and steganography, the frequency
ranges cannot be chosen arbitrarily when an ASR system is to
be attacked. Instead, the information must be presented in just
those frequency regions, on which the ASR has been trained.
Audio steganography is motivated by the challenge of
hiding additional information in an acoustic carrier signal, e. g.,
for transmitting sensitive information in case of comprehensive
Internet censorship [23]. LSB techniques [1], [24] manipulate
the binary representation of a signal and hide information
in the least signiﬁcant bits of a signal, which limits the
perceived distortions to a minimum. In contrast to our work,
such schemes ignore the acoustic characteristics of the carrier
signal and achieve their hiding capabilities at the expense of
disrupting the statistical characteristics of the original input.
Modulation-based systems [23], [33] manipulate the carrier
signal in the time or frequency domain to encode information
within the signal characteristics. Such modulations allow the
attacker to consider the frequency or energy features of a
signal and help to provide a less conspicuous manipulation
of the carrier signal. Both classes of steganography systems
aim at hiding information in a legitimate carrier signal but are
focused on creating a protected transmission channel within an
untrusted system. In contrast, while our work is designed to
provide a comparable level of inconspicuousness, we require
the successful transcription of a target message through an
automated speech recognition system, which precludes the use
of watermarking or steganography algorithms here.
VII. DISCUSSION
We have shown that it is possible to successfully attack
state-of-the-art DNN-HMM ASR systems with targeted ad-
versarial perturbations, which are barely or even impossible
to distinguish from original audio samples. Based on different
experiments, we demonstrated that it is possible to ﬁnd the best
setup for the proposed algorithm for the creation of adversarial
examples. However, these results also open questions regarding
possible countermeasures and future work.
A. Parameter Choice
The choice of the parameters highly affects the amount
of perceptible noise. The evaluation has shown that a higher
number of iterations increases the success rate, but simultane-
ously the amount of noise. However, for iterations < 500 the
success rate is already very high and therefore, 500 should
not be exceeded. Additionally, by this choice, the required
calculation time is reduced as well. If the success rate needs to
be raised, the increase of λ had a higher effect. Although the
participants in the MUSHRA test did prefer smaller values for
λ, there was no signiﬁcant difference if λ was only increased
by 20 dB. Additionally, the phone rate should be set to an
optimum value as this highly affects the success of the attack.
Besides improving the success of the attack, the choice
of the original audio sample greatly inﬂuences the quality of
the adversarial example. There might be use cases, where the
original audio sample is ﬁxed, but in general, the choice of
the original sample is free. We recommend using music or
other unsuspicious audio samples, like bird twittering, which
do not contain speech, as speech has to be obfuscated, typically
leading to larger required adversarial perturbations.
The process can be parallelized and is relatively fast in
comparison to other attacks proposed in the past, as we
have integrated the preprocessing into the backpropagation.
Therefore, we recommend to use different promising setups
and to choose that one which sounds the most inconspicuous
while giving the required success rate.
B. Countermeasures
Distillation was shown to be a successful countermeasure
against attacks in image classiﬁcation [36]. It is a technique
to improve the robustness of classiﬁcation-based DNNs [19],
which uses the output of a pre-trained DNN as soft labels
in order to train a second DNN with these soft labels as
targets. However, the transcription of the DNN-based ASR
system not only depends on the classiﬁcation result but also
on the temporal alignment. Therefore, distillation might not be
an appropriate countermeasure for ASR systems.
A general countermeasure could be to consider human
perception. A very simple version would be to apply MP3
encoding to the input data. However, the DNN is not trained for
that kind of data. Nevertheless, we did run some tests on our
adversarial examples. With this setup, the original transcription
could not be recovered, but the target transcription was also
distorted. We assume that training the ASR-DNN with MP3-
encoded audio ﬁles will only move the vulnerability into the
perceptible region of the audio ﬁles, but will not circumvent
blind spots of DNNs completely.
A more appropriate solution might be the adaption of
the loss function during the training of the DNN. It can be
utilized to measure a human-like perception-based difference.
This may result in an ASR system, which is more similar to
human speech recognition and, therefore, it is likely that hiding
messages is more difﬁcult.
C. Future Work
One obvious question is whether our attack also works
in a real-world setup. For real-world attacks, different cir-
cumstances have to be considered. This mainly includes the
acoustic transfer function, which is relevant if an audio signal
is transferred over the air. Also, additional noise from different
sources can be present
in a real-world environment. In a
controlled environment (e. g., an elevator) it is possible to
calculate the transfer function and to consider or exclude
external noise. Additionally, it is not unusual to have music in