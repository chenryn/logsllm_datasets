The  figures  given  in  the  table  show  that  data  regions 
are  slightly  more  sensitive  than  code  regions.  Moreover, 
the run-time  support memory regions are more sensitive  to 
faults than the micro-kernel memory regions. 
The following table  gives  more information  concerning 
the  25  faults  injected  into the  run-time support, that  led  to 
fail-silence violations.  The figures show that the communi- 
cation software is responsible of most fail-silence violations. 
Service component 
Execution kernel (scheduler, synchro.) 
Clock synchronization 
Reliable multicast protocol 
ATM driver 
Nb. of errors resp. of 
fail-silence violations 
13 
( 5 2 % )  
One typical  example of  fail-silence  violation  is  the cor- 
ruption  in  the communication protocol  of the local variable 
identifying  the  message  recipient  just  before  the  message 
checksum is computed. Such a fault remains undetected  on 
the sender side and will  only be detected  upon  message re- 
ceipt.  Other examples of  fail-silence  violations  are  given 
in  [5]. Among the 28 fail-silence  violations,  and especially 
in  those  concerning the communication software, we  were 
unable to find a  low cost generalist EDM able to detect the 
errors (should it be done in high level or low level commu- 
nication software) before it propagates to other computers. 
4.3. Percentage of first detections 
A  metric  to  evaluate the  efficiency  of  an  EDM  e  is  the 
percentage  of first  detections achieved  by  e  on  a set of ex- 
periments.  Let e be an EDM, and S  be a set of experiments 
into  which  an  error  is  detected  by  any  EDM.  Let  det(e) 
be  the  number of  times e  was  the  first  to  detect  the  error. 
The percentage of first  detections achieved  by  e, and noted 
1st-det(e), is defined as 1st-det(e) = w. 
To  detect  oversensitive  EDMs,  we  have  computed 
1st-det(e) for every EDM e on two sets of experiments S :  
- the  whole set of experiments where an error has been 
detected (S = S d e t ) ;  
- the set of experiments where an error has been detected 
in a justified manner (S = Sjust = S d e t  n S f a i l ) .  Note 
that computing SjzLst requires that the system execution 
is not stopped when an error is first detected. This met- 
ric corresponds to the metric noted 1st-det(e) in  [I 81. 
The percentage of first error detections was computed on 
NznJ = 71068 fault injections, that led to Nact = 7123 fault 
activations.  Among activated errors, N d e t   = 4288 of them 
were detected, and NJust = 2086 error detections were jus- 
tified. 
The  left  part  of  figure  3 depicts  lst-det(e) for  the  sets 
of experiments S d e t   and SJust. It shows that Coding-Static 
and less importantly Coding and Callgraph are oversensitive 
EDMs.  The  unjustified  detections of  Coding-Static come 
from  the  fact that  this  mechanism detects errors on all the 
run-time support static  task  graph,  even  the  parts  that  are 
never  used  by  the  workload  (initialization  tasks,  padding 
data).  The oversensitive  detections of  Coding come  from 
the  fact that  messages  have a fixed  size  in  our system, and 
that Coding adds a checksum for the whole message, even 
if only a part of the message contains useful data. Callgraph 
has  oversensitive  reactions when  a  fault  is  injected  in  the 
code in charge of checking the validity of the system control 
flow. 
The right  part  of  figure 3 gives the  values  of  1st-det(e) 
for the set of justified error detections SJust. It shows that a 
majority of errors (5 1.3%) is detected by mechanism Vmem- 
ory.  A  lower  percentage of  errors  is  detected  by  Coding, 
Structure and  CPU  (approximately  10% each).  The effi- 
ciency  of  Structure is  due to  the  heavy  use  of  arrays and 
lists  in  HADES. Approximately 3% of errors  are detected 
by  mechanism Callgraph,  which  checks the  run-time sup- 
port control flow.  The other mechanisms, and  in  particular 
the mechanisms that verify  the tasks timing, detect less that 
2.5% of errors. 
309 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:05 UTC from IEEE Xplore.  Restrictions apply. 
4.4. Error detection latency 
We define  the latency  of error detection as the delay be- 
tween  fault  activation  and  the  first  error  detection  by  an 
EDM.  In  order not to  bias  the results in  favor of  oversen- 
sitive EDMs, the latency is computed only on a set SJuSt of 
justified  error detections.  Latencies were computed  on  the 
set of  experiments  used  in  the  previous paragraph.  Laten- 
cies (as well as all other timing data given  in the paper) are 
obtained by reading the Pentium cycles counter. 
For 2086 justified error detections, the average error de- 
tection  latency  is 47.47 ms.  An  examination of  the  repar- 
tition of the error detection latencies  exhibits a small num- 
ber of very high detection latencies  (from  1  to 20 seconds). 
When the extreme  10% (highest  and lowest) values  are ex- 
cluded, the average error detection latency  is 7.80 ms.  For 
90%  of justified  error  detections,  the  detection  latency  is 
lower that  10 ms. 
The average delay between  the  injection  of an  error and 
its  activation  is  rather  high  (76.5  ms,  and  37.7  ms  when 
excluding  the  extreme  10%); in  60%  of  the  experiments, 
the duration between the fault injection  and its activation  is 
lower than  I O  ms 
Figure 4 gives the average error detection latency of every 
EDM. In the figure, EDMs marked with an asterisk are those 
that detect a low number of errors (less than 20), thus leading 
to imprecision  in the computation of the average latency. 
__  - 
__ - --- 
-- 
-----_-__^ 
~ 
..-. 
Figure 4. Average detection latency per EDM 
According  to  their error  detection  latencies, the  EDMs 
can be divided into three broad categories. The mechanisms 
of  the  first  class (Deadline, Timeout, Arrival)  are the  ones 
whose  speed  to  detect  an  error  depends  on  the  temporal 
characteristics of executed tasks.  For  instance, mechanism 
Deadline cannot detect an error before the task deadline has 
been missed. Due to the temporal properties of the tasks exe- 
cuted in the experiments (periods of tens of milliseconds for 
periodic tasks,  safe  and  therefore overestimated execution 
times  and  deadlines),  this  class of  mechanisms  have  large 
error  detection  latencies  (from  20  ms  to  140 ms  in  aver- 
age).  The mechanisms  of  the  second category  (Vmemory, 
CPU) are hardware-implemented EDMs; their average error 
detection  latency  is  8 ms.  Finally,  all  other EDMs can  be 
classified in a third category, which  includes only software- 
implemented EDMs with a rather low error detection latency 
(in  average, less than  1  ms, except for mechanisms  Seman 
and Structure). 
Figure 5 gives for every range of error detection latencies 
the number of errors detected. 
0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5  5.5  6  6.5  7  7.5  8  8.5  9  9.5 
Error detection latency (ms) 
Figure 5. Repartition of detection latencies 
Two peaks can  be observed in the histogram:  one corre- 
sponding to EDMs with a latency below  1 ms (third class of 
EDMs introduced above), and one to EDMs with  a  latency 
between 7 and 9 ms (second class of EDMs). 
4.5. Memory and timing costs of EDMs 
Figure 6 details the EDMs memory and timing costs. The 
memory  and  timing  costs  have  been  measured  by  adding 
the  EDMs  one  by  one by  decreasing  order  of  percentage 
of  first error detections (i.e.  according  to metric  1st-det(e) 
computed on justified error detections - see figure 3.b). T h e  
memory  cost of  the Chorus kernel  (ranging from  169Kb to 
191Kb depending on whether memory protection  is used or 
not) are excluded from figure 6.a. 
Figure 6.a details the  memory  cost of the  EDMs for the 
code  and  data  segments.  The  code  size  of  the  run-time 
support with  all EDMs  included  is  6.7 times  as  big  as the 
code size of a run-time support without EDMs. Mechanism 
Vmemory  alone multiplies the  volume of  code of  the  run- 
time support by  a factor 3.5. This is due to the duplication 
of  library  code - libc  and other HADES internal  libraries  - 
in several address spaces (the tool chain of the selected Cho- 
rus configuration  does not provide direct support for shared 
libraries). Mechanism Structure also adds a large code over- 
head.  This is because the data structures covered by  Struc- 
ture are heavily  used  in  the run-time support, and thus have 
been  in-lined.  Mechanism Callgraph also has a rather high 
code overhead.  This is  due to  the  presence in  HADES, for 
the sake of modularity, of a large number of small functions. 
Concerning the increase of the size of data, only mechanism 
Coding  is  noticeable.  All  other EDMs use  stack-allocated 
variables  having no impact on the data size of the run-time 
support. 
3 10 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:05 UTC from IEEE Xplore.  Restrictions apply. 
900 
6 'g  800 
700 
e 
600 
!s 500 
400 
300 
=  200 
e 
g 
100 
0
0
0 
(a) Memory cost 
" - - 
- 
- 
_I__ 
ECI 
r- (b) Timing cost 
45  I 
/os 
, 
At 
itomaton 
\ 
Robust 
Code 
Memory area 
i 
CPU 
Figure 6. Memory and timing costs of EDMs 
The timing  overhead  of the EDMs on  the run-time sup- 
port  activity  is  depicted  in  figure  6.b.  The  time  spent  in 
the run-time support2 is  multiplied  by  a factor  2.5 in  con- 
sequence  of  the  addition  of  error  detection.  The  more 
time-consuming  EDMs are,  by  decreasing  time  consump- 
tion, Coding (14.4% of the time spent in  the run-time sup- 
port), Callgraph  (13.1 %),  Vmemory (12.0%) and Structure 
(7.5%). The other EDMs have an overhead lower than  3%. 
A  factor 2.5 is  in  our opinion rather satisfactory for en- 
tirely  software-implemented EDMs.  The alternative would 
have been  to use temporal redundancy for the run-time sup- 
port activities to check that they behave in a fail-silent man- 
ner.  Using  temporal  redundancy  for this  class of activities 
seems to us very  tricky to implement and costly to execute, 
because run-time support activities usually do not return any 
value but rather have  side effects (IOs, modifications of the 
run-time support data structures like the run-queues). Using 
temporal  redundancy would  increase the execution  time  of 
the run-time  support activities by  at least a  factor two,  but 
probably  much  more  in  order to  check  the  equivalence of 
successive executions. 
4.6.  Overlap between EDMs 
In this paragraph, we are interested in finding EDMs that 
overlap.  The final  objective  is to remove  some EDMs,  in 
order to.reduce the overall cost of error detection  with  only 
a slight decrease of the fail-silence coverage. 
Note  that  it  is  not  possible  to  complete  fault  injection 
campaigns to evaluate the fail-silence coverage obtained for 
every  possible  combination  of  EDMs.  Indeed,  a  fault  in- 
jection campaign for 3000 activated errors takes at least four 
days. Trying all possible combinations would require 213 * 4 
'As  mechanism  Coding-Static  executes  in  the  run-time  support  idle 
loop,  its timing  cost  is  not  measured  like  the other  EDMs  by  measuring 
the percentage spent in the run-time support executing this EDM. We only 
measure the indirect overhead Coding-Static adds on the other mechanisms 
(due to its poor locality of reference, it makes the number of cache misses 
in the system increase). 
311 
EDM 
Vmemory 
Structure 
Coding 
Seman 
Deadline 
Callgraph 
Coding-Static 
Robust 
lstdet 
51.25 % 
10.12 % 
9.68 7% 
5.85 % 
2.78  % 
2.78 % 
1.34 % 
1.20 % 
ordered overlup 
0.00 
1.12 