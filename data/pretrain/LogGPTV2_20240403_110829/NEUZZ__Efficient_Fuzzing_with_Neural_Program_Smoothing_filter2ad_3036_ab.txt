example in Figure 3 to demonstrate the key insight behind
our approach. The simple C code snippet shown in Figure 3
demonstrates a general switch-like code pattern commonly
found in many real-world programs. In particular, the example
code computes a non-linear exponential function of the input
(i.e., pow(3,a+b)). It returns different values based on the
output range of the computed function. Let us also assume
that a buggy code block (marked in red) is exercised if the
function output range is in (1,2).
Consider the case where evolutionary fuzzers like AFL
have managed to explore the branches in lines 2 and 9 but
fail to explore branch in line 5. The key challenge here is to
ﬁnd values of a and b that will trigger the branch at line 5.
Evolutionary fuzzers often struggle with such code as the odds
of ﬁnding a solution through random mutation are very low.
For example, Figure 3a shows the original function that the
code snippet represents. There is a sharp jump in the function
surface from a+b = 0 to a+b− = 0 ( → +0). To maximize
the edge coverage during fuzzing, an evolutionary fuzzer can
only resort to random mutations to the input as such techniques
do not consider the shape of function surface. By contrast, our
NN smoothing and gradient-guided mutations are designed to
exploit the function surface shape as measured by the gradients.
We train an NN model on the program behaviors from the
other two branches. The NN model smoothly approximates
the program behaviors as shown in Figure 3b and 3c. We then
use the NN model to perform more effective gradient-guided
optimization to ﬁnd the desired values of a and b and
incrementally reﬁne the model until the desired branch is
found that exercises the target bug.
IV. METHODOLOGY
We describe the different components of our scheme in
detail below.
A. Program smoothing
Program smoothing is an essential step to make gradient-
guided optimization techniques suitable for fuzzing real-world
programs with discrete behavior. Without smoothing, gradient-
guided optimization techniques are not very effective for
optimizing non-smooth functions as they tend to get stuck
at different discontinuities [67]. The smoothing process
such irregularities and therefore makes
minimizes
the
gradient-guided optimization signiﬁcantly more effective on
discontinuous functions.
In general, the smoothing of a discontinuous function f
can be thought of as a convolution operation between f and
a smooth mask function g to produce a new smooth output
function as shown below. Some examples of popular smoothing
masks include different Gaussian and Sigmoid functions.
(cid:3) +∞
−∞
(cid:3)
f
(x) =
f (a)g(x − a)da
(2)
(cid:3)
(cid:4)
However, for many practical problems, the discontinuous
function f may not have a closed-form representation and
thus analytically computing the above-mentioned integral
is not possible. In such cases, the discrete version f
(x) =
a f (a)g(x − a) is used and the convolution is computed nu-
merically. For example, in image smoothing, often ﬁxed-sized
2-D convolution kernels are used to perform such computation.
However, in our setting, f is a computer program and therefore
the corresponding convolution cannot be computed analytically.
Program smoothing techniques can be classiﬁed into two
broad categories: blackbox and whitebox smoothing. The black-
box approach picks discrete samples from the input space of f
and computes the convolution numerically using these samples.
By contrast, the whitebox approach looks into the program
statements/instructions and try to summarize their effects using
symbolic analysis and abstract interpretation [21], [20]. The
blackbox approaches may introduce large approximation errors
while whitebox approaches incur prohibitive performance over-
head, which makes them infeasible for real-world programs.
To avoid such problems, we use NNs to learn a smooth
approximation of program behaviors in a graybox manner
(e.g., by collecting edge coverage data) as described below.
B. Neural program smoothing
In this paper, we propose a novel approach to program
smoothing by using surrogate NN models to learn and
iteratively reﬁne smooth approximations of the target program
based on the observed program behaviors. The surrogate
neural networks can smoothly generalize to the observed
program behaviors while also accurately modeling potentially
non-linear and non-convex behaviors. The neural networks,
once trained, can be used for efﬁciently computing gradients
(cid:25)(cid:17)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
and higher-level derivatives to guide the fuzzing input
generation process as shown in Figure 3.
Why NNs? As implied by the universal approximation the-
orem [33], an NN is a great ﬁt for approximating complex
(potentially non-linear and non-convex) program behaviors.
The advantages of using NNs for learning smooth program
approximations are as follows: (i) NNs can accurately model
complex non-linear program behaviors and can be trained
efﬁciently. Prior works on model-based optimization have used
simple linear and quadratic models [24], [23], [71], [52]. How-
ever, such models are not a good ﬁt for modeling real-world
software with highly non-linear and non-convex behaviors; (ii)
NNs support efﬁcient computation of their gradients and higher-
order derivatives. Therefore, the gradient-guided algorithms
can compute and use such information during fuzzing without
any extra overhead; and (iii) NNs can generalize and learn to
predict a program’s behaviors for unseen inputs based on its
behaviors on similar inputs. Therefore, NNs can potentially
learn a smooth approximation of the entire program based on
its behaviors for a small number of input samples.
NN Training. While NNs can be used to model different
aspects of a program’s behavior, in this paper we use them
speciﬁcally for modeling the target program’s branching
behavior (i.e., predicting control ﬂow edges exercised by a given
program input). One of the challenges in using neural nets to
model branching behavior is the need to accept variably-sized
input. Feedforward NNs, unlike real-world programs, typically
accept ﬁxed size input. Therefore, we set a maximum input
size threshold and pad any smaller-sized inputs with null bytes
during training. Note that supporting larger inputs is not a major
concern as modern NNs can easily scale to millions of param-
eters. Therefore, for larger programs, we can simply increase
the threshold size, if needed. However, we empirically ﬁnd that
relatively modest threshold values yield the best results and
larger inputs do not increase modeling accuracy signiﬁcantly.
(cid:5)
(cid:6)n
(cid:6)m → (cid:5)
Formally, let f :
0x00, 0x01, ..., 0xff
0, 1
denote the NN that takes program inputs as byte sequences
with size m and outputs an edge bitmap with size n. Let θ
denote the trainable weight parameters of f. Given a set of
training samples (X, Y ), where X is a set of input bytes and Y
represents the corresponding edge coverage bitmap, the training
task of the parametric function f (x, θ) = y is to obtain the pa-
rameter ˆθ such that ˆθ = arg minθ
L(y, f (x, θ)) where
(cid:4)
x∈X,y∈Y
L(y, f (x, θ)) deﬁnes the loss function between the output of
the NN and the ground truth label y ∈ Y in the training set. The
training task is to ﬁnd the weight parameters θ of the NN f to
minimize the loss, which is deﬁned using a distance metric. In
particular, we use binary cross-entropy to compute the distance
between the predicted bitmap and the true coverage bitmap. In
particular, let yi and fi(x, θ) denote the i-th bit in the output
bitmap of ground truth and f’s prediction, respectively. Then,
the binary cross-entropy between these two is deﬁned as:
[yi · log(fi(x, θ) + (1 − yi) · log(1 − fi(x, θ)]
n(cid:7)
i=1
− 1
n
In this paper, we use feed-forward fully connected NNs
to model
the target program’s branching behavior. The
feed-forward architecture allows highly efﬁcient computation
of gradients and fast training [53].
Our smoothing technique is agnostic to the source of the
training data and therefore the NN can be trained on any edge
coverage data gathered from an existing input corpus. For our
prototype implementation, we use input corpora generated by
existing evolutionary fuzzers like AFL to train our initial model.
Training data preprocessing. Edge coverage exercised by the
training data often tends to be biased, as it only contains labels
for a small section of all edges in a program. For example,
some edges might always be exercised together by all inputs
in the training data. This type of correlation between a set
of labels is known in machine learning as multicollinearity,
which often prevents the model from converging to a small
loss value [34]. To avoid such cases, we follow the common
machine learning practice of dimensionality reduction by
merging the edges that always appear together in the training
data into one edge. Furthermore, we only consider the edges
that have been activated at least once in the training data. These
steps signiﬁcantly reduce the number of labels to around 4, 000
from around 65, 536 on average. Note that we rerun the data
preprocessing step at every iteration of incremental learning
and thus some merged labels may get split as their correlation
may decrease as new edge data is discovered during fuzzing.
C. Gradient-guided optimization
Different gradient-guided optimization techniques like
gradient descent, Newton’s method, or quasi-Newton methods
like L-BFGS can use gradient or higher-order derivatives for
faster convergence [10], [13], [65]. Smooth NNs enable the
fuzzing input generation process to potentially use any of these
techniques by supporting efﬁcient computation of gradient
and higher-order derivatives. In this paper, we speciﬁcally
design a simple gradient-guided search scheme that is robust
to minor prediction errors to demonstrate the effectiveness of
our approach. We leave the exploration of more sophisticated
techniques as future work.
Before describing our mutation strategy, which is based
on the NN’s gradient, we ﬁrst provide a formal deﬁnition of
the gradient that indicates how much each input byte should
be changed to affect the output of a ﬁnal layer neuron in the
NN (indicating changed edge coverage in the program) f [80].
Here each output neuron corresponds to a particular edge and
computes a value between 0 and 1 summarizing the effect
of the given input byte on a particular edge. The gradients
of the output neurons of the NN f w.r.t. the inputs have been
extensively used for adversarial input generation [39], [66] and
visualizing/understanding DNNs [87], [80], [56]. Intuitively,
in our setting, the goal of gradient-based guidance is to ﬁnd
inputs that will change the output of the ﬁnal layer neurons
corresponding to different edges from 0 to 1.
Given a parametric NN y = f (θ, x) as deﬁned in
Section IV-B, let yi denote the output of i-th neuron in the
ﬁnal layer of f, which can also be written as fi(θ, x). The
(cid:25)(cid:17)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
gradient G of fi(θ, x) with respect to input x can be deﬁned
as G = ∇xfi(θ, x) = ∂yi/∂x. Note that f’s gradient w.r.t to
θ can be easily computed as the NN training process requires
iteratively computing this value to update θ. Therefore, G can
also be easily calculated by simply replacing the computation
of the gradient of θ to that of x. Note that the dimension of
the gradient G is identical to that of the input x, which is
a byte sequence in our case.
Algorithm 1 Gradient-guided mutation
seed ← initial seed
Input:
iter ← number of iterations
k ← parameter for picking top-k critical bytes
for mutation
g ← computed gradient of seed
locations ← top(g, ki)
for m = 1 to 255 do
1: for i = 1 to iter do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
for loc ∈ locations do
v ← seed[loc] + m ∗ sign(g[loc])
v ← clip(v, 0, 255)
gen_mutate(seed, loc, v)
for loc ∈ locations do
v ← seed[loc] − m ∗ sign(g[loc])
v ← clip(v, 0, 255)
gen_mutate(seed, loc, v)
Gradient-guided optimization. Algorithm 1 shows the
outline of our gradient-guided input generation process. The
key idea is to identify the input bytes with highest gradient
values and mutate them, as they indicate higher importance
to the NN and thus have higher chances of causing major
changes in the program behavior (e.g., ﬂipping branches).
Starting from a seed, we iteratively generate new test inputs.
As shown in Algorithm 1, at each iteration, we ﬁrst leverage
the absolute value of the gradient to identify the input bytes
that will cause the maximum change in the output neurons
corresponding to the untaken edges. Next, we check the sign
of the gradient for each of these bytes to decide the direction
of the mutation (e.g., increment or decrement their values)
to maximize/minimize the objective function. Conceptually,
our usage of gradient sign is similar to the adversarial input
generation methods introduced in [39]. We also bound the
mutation of each byte in its legal range (0-255). Lines 6 and 10
denote the use of clip function to implement such bounding.
We start the input generation process with a small mutation
target (k in Algorithm 1) and exponentially grow the number of
target bytes to mutate to effectively cover the large input space.
D. Reﬁnement with incremental learning
The efﬁciency of the gradient-guided input generation
process depends heavily on how accurately the surrogate NN
can model the target program’s branching behavior. To achieve
higher accuracy, we incrementally reﬁne the NN model when
divergent program behaviors are observed during the fuzzing
process (i.e., when the target program’s behavior does not
match the predicted behavior). We use incremental learning
techniques to keep the NN model updated by learning from
new data when new edges are triggered.
The main challenge behind NN reﬁnement is preventing the
NN model from abruptly forgetting the information it previously
learned from old data while training on new data. Such forget-
ting is a well-known phenomenon in deep learning literature
and has been thought to be a result of the stability-plasticity
dilemma [58], [8]. To avoid such forgetting issues, an NN must
change the weights enough to learn new tasks but not too much
as to cause it to forget previously learned representations.
The simplest way to reﬁne an NN is to add the new training
data (i.e., program branching behaviors) together with the
old data and train the model from scratch again. However,
as the number of data points grows, such retraining becomes
harder to scale. Prior research has tried to solve this problem
using mainly two broad approaches [44], [51], [31], [75], [29],
[40], [76]. The ﬁrst one tries to keep separate representations
for the new and old models to minimize forgetting using
distributed models, regularization, or creating an ensemble
out of multiple models. The second approach maintains a
summary of the old data and retrains the model on new data
along with the summarized old data and therefore is more
efﬁcient than complete retraining. We refer the interested
readers to the survey by Kemker et al. [48] for more details.
In this paper, we used edge-coverage-based ﬁltering to only
keep the old data that triggered new branches for retraining.
As new training data becomes available, we identify the ones
achieving new edge coverage, put them together with the
ﬁltered old training data, and retrain the NN. Such a method
effectively prevents the number of training data samples from
drastically increasing over the number of retraining iterations.
We ﬁnd that our ﬁltration scheme can easily support up to
50 iterations of retraining while still keeping the training time
under several minutes.
V. IMPLEMENTATION
In this section, we discuss our
implementation and
how we ﬁne-tune NEUZZ to achieve optimal performance.
We have released our implementation through GitHub at
http://github.com/dongdongshe/neuzz. All our measurements
are performed on a system running Arch Linux 4.9.48 with
an Nvidia GTX 1080 Ti GPU.
NN architecture. Our NN model is implemented in Keras-
2.1.3 [5] with Tensorﬂow-1.4.1 [6] as a backend. The NN
model consists of three fully-connected layers. The hidden
layer uses ReLU as its activation function. We use sigmoid as
the activation function for the output layer to predict whether a
control ﬂow edge is covered or not. The NN model is trained
for 50 epochs (i.e., 50 complete passes of the entire dataset)
to achieve high test accuracy (around 95% on average). Since
we use a simple feed-forward network, the training time
for all 10 programs is less than 2 minutes. Even with pure
CPU computation on an Intel i7-7700 running at 3.6GHz, the
training time is under 20 minutes.
(cid:25)(cid:17)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I: NEUZZ Parameter Tuning
(a) Edge coverage achieved by mutations generated in
different iterations (Algorithm 1 line 1). The numbers
in bold indicate the highest values for each program.
coverage
(b) Edge
comparison of 1M mutations
generated by NEUZZ on different NN models. n
denotes the number of neurons in every hidden layer.
Programs
readelf -a
libjpeg
libxml
mupdf
Iteration i
10
1,800
89
256
266
7
1,678
107
161
294
11
1,529
93
174
266
Programs
readelf -a
libjpeg
libxml
mupdf
1 hidden layer
3 hidden layers
n=4096
n=8192
n=4096
n=8192
1,800
89
256
260
1,658
57
172
94
1,714
80
140
82
1,584