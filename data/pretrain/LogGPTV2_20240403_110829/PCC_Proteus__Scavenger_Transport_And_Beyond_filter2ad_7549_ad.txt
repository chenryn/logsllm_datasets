on the timescale of a MI (observed in our real-world WiFi test
in §6.2.1), causing abnormal samples of RTT gradient and RTT
deviation. More robust designs could employ statistical inference
techniques, and taking advantage of all available information in-
cluding in-network feedback [19].
(a) Throughput
(b) Latency inflation
Figure 3: Bottleneck saturation with varying buffer size
Figure 4: Loss tolerance
Figure 5: Fairness index
design of the previous sections.3 For Proteus-P’s utility function,
we adopt the default parameters from [17]: 𝑡 = 0.9, 𝑏 = 900, and
𝑐 = 11.35. For Proteus-S, we set the RTT deviation coefficient
𝑑 = 1500 (with RTT deviation in units of 𝑠𝑒𝑐𝑜𝑛𝑑𝑠).
We compare two scavengers – Proteus-S and LEDBAT – and let
them compete with various primary protocols: TCP CUBIC [21],
BBR [13], COPA [8], PCC-Vivace [17], and Proteus-P. We employ
the LEDBAT implementation in the open-source 𝜇Torrent Trans-
port Library [2], with the target extra delay set to 100 𝑚𝑠, as in the
current IETF standard [34] as well as 𝜇Torrent’s default setting.4
To measure transport-level performance, our test environment
uses Pantheon [42] to run flows and collect performance metrics,
both on Emulab [38] and in the live Internet. Unless otherwise
specified, we use Emulab tests with a 50 Mbps bandwidth, 30 ms
RTT setup, and show the mean of at least 10 trials in each sce-
nario. We also measure application-level performance (DASH video
streaming [1] and webpage loading) to show the benefits of having
scavengers competing with primary flows.
For the evaluation of Proteus-H, we implement emulated video
streaming on top of our UDP implementation. Specifically, Proteus
receiver runs a BOLA [35] agent that takes a DASH video definition
as input and consumes the received bytes to maintain an emulated
playback buffer. The receiver uses a side channel to notify the
sender of: (1) its requested bitrate for each chunk, (2) when to
stop/resume transmission due to limited playback buffer space, and
(3) the calculated switching threshold if Proteus-H is used.
6 EVALUATION
We implemented Proteus by branching off of the existing open-
source UDP-based PCC implementation [17] and implementing the
3At the moment, both Proteus-P and Proteus-S, as well as PCC Vivace, are based on
UDT [20]. However, we adopt QUIC-compatible APIs [25] in the Proteus implementa-
tion, which should facilitate its real-world deployment.
4The first LEDBAT IETF draft [33] used a 25 𝑚𝑠 target. We evaluate its performance
as well, and get similarly undesirable results as achieved by 100 𝑚𝑠; see Appendix B.
621
01020304050 1 10 100 1000Throughput (Mbps)Buﬀer Size (KB)Proteus-SLEDBATCUBICBBRProteus-PCOPAVivace 0 0.2 0.4 0.6 0.8 1030060090095-th Inﬂation RatioBuﬀer Size (KB)Proteus-SLEDBATCUBICBBRProteus-PCOPAVivace11050 0 0.01 0.02 0.03 0.04 0.05 0.06Throughput (Mbps)Random Loss RateProteus-SLEDBATCUBICBBRProteus-PCOPAVivace 0.4 0.5 0.6 0.7 0.8 0.9 1 2 3 4 5 6 7 8 9 10Jain's Fairness IndexNumber of FlowsProteus-SLEDBATCUBICBBRProteus-PCOPAVivaceSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Tong Meng, Neta R. Schiff, Brighten Godfrey, and Michael Schapira
(a) LEDBAT as Scavenger
(b) Proteus-S as Scavenger
(c) Proteus-P as Scavenger
(d) COPA as Scavenger
Figure 6: Scavenger competes with primary protocols
6.1 Scavenger-Only Performance
When there are no primary flows, a good scavenger should have
high performance like a normal congestion controller. We evaluate
this single-protocol performance with typical congestion control
objectives (high throughput, low latency) across different environ-
ment variables (buffer size, random loss probability, and number of
competing flows).
Latency Awareness. We run a single flow on the above speci-
6.1.1
fied Emulab bottleneck link for 100 seconds, with varying buffer size.
We compare the protocols’ throughput and RTT inflation (Fig. 3).
As shown in Fig. 3(a), both Proteus-P and Proteus-S need as
small as 4.5 KB buffer to achieve at least 90% capacity utilization,
i.e., 45 Mbps throughput, which is the same as needed by BBR
and PCC Vivace. In comparison, both CUBIC and COPA need 5.7×
larger buffer to reach the same utilization. LEDBAT, always trying
to inflate the RTT by 100 𝑚𝑠, needs 150 KB buffer, which is close to
the BDP (187.5 KB), and 32.3× larger than needed by Proteus.
We then evaluate latency sensitivity in terms of 95th percentile
inflation ratio, calculated as:
95th inflation ratio =
95th percentile RTT − base RTT
buffer size/bottleneck bandwidth
,
which effectively measures the 95th percentile buffer occupancy. We
report this value in Fig. 3(b). Both Proteus-S and Proteus-P, similar
to Vivace, limit the inflation ratio below 10% as long as the buffer
is ≥ 150 KB. Even COPA, which is latency-aware, needs 3× larger
buffer (600 KB) to keep inflation ratio below 10%. In comparison,
LEDBAT has around 100% inflation ratio until the buffer size is large
enough (at least 625 KB) to accommodate its target delay. More
specifically, at 2 BDP buffer size (375 KB), Proteus-S has 75.3%,
93.8%, 96.4%, and 96.42% smaller inflation ratio compared with
COPA, BBR, CUBIC, and LEDBAT, respectively.
6.1.2 Random Loss Tolerance. Next, when there exists random non-
congestion loss, we compare different protocols’ average through-
put from multiple 100-second runs on the same bottleneck with
375 KB buffer (2 BDP) in Fig. 4. Thanks to its improved noise con-
trol, Proteus-P, using a similar utility function as Vivace, performs
somewhat better than Vivace on that front, achieving 74% higher
throughput with 5% random loss. Proteus-S, on the other hand, has
somewhat worse throughput than Vivace, which can be attributed
to its RTT deviation-based rate control which causes it to ramp up
more conservatively.
622
LEDBAT is fragile even when facing a 0.001% random loss rate,
suffering from 50% degradation compared with Proteus.
We note that COPA and BBR have higher random loss tolerance
because they do not directly react to packet losses. In comparison,
as explained in §4.1, the loss coefficient in Proteus and PCC Vivace’s
utility function is set to achieve 5% random loss tolerance. We could
tune the coefficient for higher tolerance, although this induces
higher congestion loss [17].
Fairness With Competing Flows. To evaluate convergence
6.1.3
when multiple senders of the same protocol compete with each
other, we use a 30 ms RTT bottleneck link on Emulab. We test
𝑛 ∈ 2, . . . , 10 flows with 20𝑛 Mbps link bandwidth and 300𝑛 KB
buffer size. In each run, a flow is started after waiting 20 seconds
for the previous flow to ramp up. We measure mean throughput
of each flow during the 200 seconds after all flows are started, and
present Jain’s fairness index in Fig. 5. We see that Proteus-P, PCC
Vivace, CUBIC, BBR and COPA all keep Jain’s index around 99%.
Proteus-S has lower, but still always above 90%, fairness index.
In comparison, LEDBAT’s fairness decreases and then increases
with 𝑛. The decreasing fairness is known as its latecomer issue [34],
which occurs because after one LEDBAT flow is running, the min-
imum delay observation for any subsequent flow is based on an
already-inflated buffer. For example, with 6 competing flows, Proteus-
S is 75% more fair than LEDBAT. LEDBAT’s fairness begins improv-
ing once 𝑛 is large enough that the sum of the target extra delay of
all flows exceeds the maximum inflation allowed by the bottleneck
buffer size.
6.2 Yielding to Primary Flows
When a scavenger competes with a primary flow, our goals are that
(1) most importantly, the scavenger should have minimal impact
on the primary flow (compared to running the primary flow alone);
and (2) secondarily, the scavenger should opportunistically use any
remaining resources.
Our evaluation uses two flows, one primary followed by one
scavenger. Again, we use the specified Emulab link. We consider
both shallow (75 KB, i.e., 0.4 BDP) and large buffer (375 KB, i.e.,
2 BDP) setups. In addition to LEDBAT, we test Proteus-P in the role
of the scavenger, to emphasize the effectiveness of Proteus-S’s RTT
deviation-based utility function. We calculate two performance
metrics. To measure goal (1), we use the primary throughput ratio,
defined as the primary flow’s throughput when running with the
020406080100020406080100Primary Throughput Ratio (%)Capacity Utilization (%)BBRCUBICCOPAProteus-PVivaceBuﬀer:75KB375KB          020406080100020406080100Primary Throughput Ratio (%)Capacity Utilization (%)BBRCUBICCOPAProteus-PVivaceBuﬀer:75KB375KB          020406080100020406080100Primary Throughput Ratio (%)Capacity Utilization (%)BBRCUBICCOPAProteus-PVivaceBuﬀer:75KB375KB          020406080100020406080100Primary Throughput Ratio (%)Capacity Utilization (%)BBRCUBICCOPAProteus-PVivaceBuﬀer:75KB375KB          PCC Proteus: Scavenger Transport And Beyond
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 7: RTT with competition
Figure 8: Throughput ratio CDF
Figure 9: Single flow on WiFi
scavenger divided by its throughput when running alone. To mea-
sure goal (2), we use the the total capacity utilization of the two
flows.
Although designed as a scavenger against TCP CUBIC, LEDBAT
fails to yield to CUBIC when its target extra delay exceeds the max-
imum inflation allowed by the buffer. This occurs with both buffer
setups (Fig. 6(a)). In that situation, it approximately fairly shares the
bottleneck with CUBIC. LEDBAT also fails to yield when the com-
peting sender is less aggressive, e.g., it lowers BBR’s throughput to
26.0% with 375 KB buffer. Similarly, the other three latency-aware
protocols, COPA, PCC Vivace, and Proteus-P, are more significantly
impacted by LEDBAT, e.g., they all have lower than 43% throughput
ratio when competing with LEDBAT.
larger inflation increment, because loss-based protocols such as
CUBIC already fill the buffer when they run alone. For instance,
COPA sees 2.3× RTT when competing with LEDBAT. Proteus-S,
unlike LEDBAT, has negligible influence on RTT, e.g., BBR even
sees 18.8% smaller 95-th RTT. Proteus-P and COPA are also inferior,
doubling the 95-th RTT when competing with each other.
To further stress Proteus-S’s robustness as a congestion control
scavenger, we let it compete with BBR, CUBIC, and Proteus-P under
the 180 distinct bottleneck configurations representing all combina-
tions of the following parameters: bandwidth chosen from {20, 50,
100, 200, 300, 500} Mbps, RTT chosen from {5, 10, 30, 60, 100, 200} ms,
and buffer size chosen from {0.2, 0.5, 1.0, 2.0, 5.0} BDP. For presenta-
tion clarity, we only compare Proteus-S with LEDBAT, and present
the CDF of primary throughput ratios in Fig. 8. In the median case,
the three primary protocols, BBR, CUBIC, and Proteus-P, achieve
7.8%, 28.0%, and 2.8× higher throughput competing with Proteus-S
than with LEDBAT. That corresponds to our above conclusion, i.e.,
the extra delay target used by LEDBAT may be too aggressive for
a moderate-sized buffer, and is a late congestion signal especially
against latency-sensitive protocols.
One may argue that the inferior yielding performance of LED-
BAT can be improved by using a smaller extra target delay. However,
as shown by the results with 25 ms extra delay in Appendix B, using
a smaller extra target delay induces other performance problems, in-
cluding more significant latecomer advantage and worse multiflow
unfairness. Meanwhile, as a general congestion control protocol,
LEDBAT still has high inflation under shallow buffers, and is more
aggressive than latency-sensitive protocols such as PCC Vivace and
Proteus-P.
Scavenger Performance on the Internet. We now move our test
6.2.1
scenarios for the same single-flow and two-flow experiments to the
live Internet. Specifically, we use WiFi connections at four different
locations (two residential apartments, and two restaurants), and test
using the uplink by transmitting from a laptop to an AWS server
in each of 16 different regions.5 For each source-destination pair,
we conduct 4 trials, each lasting 2 minutes, and report the median
value (i.e., the mean of the two middle values).6 Finally, to ease
visualization, we normalize this median throughput by the highest
value obtained by any protocol on that source-destination pair.
5These are all of the AWS regions except Hong Kong, Bahrain, and Capetown which
we were unable to use for logistical reasons.
6We do not observe performance issues due to interference from shared CPU.
623
In contrast, Proteus-S yields well (Fig. 6(b)): with CUBIC, BBR,
COPA, and Proteus-P as the primary flow, the primary throughput
ratio is above 98%, 95%, 87%, and 88%, respectively, in all test cases.
For primary flows COPA and Proteus-P, regardless of the buffer size,
the performance gains are more than 1.1× and 2.3× over LEDBAT.
When competing with Vivace, Proteus-S has somewhat lower pri-
mary throughput ratio (since Vivace does not have adaptive noise
tolerance, and thus may tolerate less RTT fluctuation). However, it
is still at least 3.2× better than both LEDBATs.
The other two latency-aware protocols, as expected, do not con-
sistently yield (Fig. 6(c),6(d)). Specifically, Proteus-P competes with
COPA and Vivace fairly under both buffer setups, while COPA is
friendly (i.e., has fair equilibrium) to all the other protocols except
when competing with BBR with a shallow buffer. We also observe
that Proteus-P can lower the throughput of BBR to 88%, compared
with at least 95% from Proteus-S. This validates our claim in Sec-
tion 4.2 that RTT deviation signals competition better than RTT
gradient, and hence is a better scavenger penalty metric.
Proteus-S also outperforms LEDBAT in our secondary goal of
utilizing the remaining bandwidth. When competing with BBR,
CUBIC, Proteus-P and PCC Vivace, Proteus-S maintains a joint
capacity utilization of at least 95%. Its utilization competing with
COPA is 89% (although we note this is the same utilization COPA
can achieve when it competes with itself). LEDBAT only delivers
around 85% utilization when competing with Proteus-P and Vivace
with 75 KB buffer.
Furthermore, the competition between LEDBAT and primary
protocols leads to more significant RTT inflation. Fig. 7 presents
the ratio between 95th percentile RTT seen by a primary flow when
competing with a scavenger flow and when the primary flow runs
alone (achieved with 375 KB buffer). Latency-aware protocols see
123CUBICBBRCOPAProteus-PPCC95-th RTT RatioPrimary ProtocolProteus-SLEDBATProteus-PCOPA 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Cumulative DistributionPrimary Throughput RatioBBR vs. Proteus-SBBR vs. LEDBATCUBIC vs. Proteus-SCUBIC vs. LEDBATProteus-P vs. Proteus-SProteus-P vs. LEDBAT 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Cumulative FractionNormalized ThroughputProteus-SLEDBATCUBICBBRProteus-PCOPAVivaceSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Tong Meng, Neta R. Schiff, Brighten Godfrey, and Michael Schapira
(a) BBR as Primary
(b) CUBIC as Primary
(c) COPA as Primary
(d) Proteus-P as Primary
(e) PCC Vivace as Primary
Figure 10: Primary throughput ratio in real-world WiFi
Fig. 9 shows the CDF of normalized median throughputs across
the 64 source-destination pairs. The protocols intended as primary
flows are, interestingly, among the worst and the best. Two latency-
aware primary protocols, COPA and Vivace, have the worst per-
formance, because they are affected by RTT fluctuations. (Even
though these are WiFi links and not LTE, we observe significant
fluctuation in RTT. The typical RTT deviation is up to 5 ms but RTT
occasionally spikes tens of milliseconds higher.) Meanwhile, CUBIC
and BBR have highest throughput because they are much more
aggressive than other protocols (see latency inflation in Fig. 3(b)).
CUBIC is even better than BBR because CUBIC is loss-based and
the least latency-sensitive.
Among the scavengers, since LEDBAT relies on (one-way) de-
lay, it is also somewhat prone to noisy measurement. Proteus-S is