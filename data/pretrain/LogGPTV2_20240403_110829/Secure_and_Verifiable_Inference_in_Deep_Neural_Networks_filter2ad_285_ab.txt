CryptoDL, it only considers the approximation of continuous func-
tions, and cannot provide conversion to non-continuous functions
such as Rectified Linear Unit(ReLU,y = max(0, x)). Nevertheless,
RelU has become a highly regarded activation function in the field
of image recognition. Compared with CryptoDL, our work can
transform any activation functions into polynomials, where we use
discrete least squares [8] to give a heuristic conversion algorithm
for non-continuous functions. Experiments (See Section 5) show
that we can still find satisfactory low-degree polynomials.
2.2 Verifiable Deep Learning
In this paper, we focus on the verifiability of the results returned
by the server during the inference phase. In summary, the exist-
ing results can be roughly divided into two directions: i.e, Trusted
Execution Environments(TEE)-based [18, 48, 49] and Verifiable
Computing (VC)-based [10, 27, 63]. TEE provides a secure enclave
to run a deep learning model, where the model/data owner can use
hardware and software protections to isolate sensitive computa-
tions from the untrusted software stack. In this way, data privacy
1We also confirmed this argument in the experimental part. For more details, please
see Section 5.3
786ACSAC 2020, December 7–11, 2020, Austin, USA
Guowen Xu et al.
and the integrity of the calculation process can be hard protected.
Florian et al. [49] propose Slalom, which enables all linear layers in
DNN from TEE (such as Intel SGX or Sanctum) to be executed by
a faster but untrusted co-located processor. Verifiable computing
(VC) can provide proofs of computational integrity without any
assumptions on hardware. Ghodsi et al. [63] proposed the first veri-
fiable approach SafetyNets. In SafetyNets, a specific type of DNNs
framework will be converted into an arithmetic circuit, under which
the server interacts with the user multiple times to verify the cor-
rectness of the returned results. Recently, Keuffer et al. [27] also
design an efficient proof composition for verifiable computation,
which proposes a method for constructing several dedicated and
efficient VC schemes using the universal VC protocols.
Comparison: TEE-based works rely on hardware to fulfill the re-
quirements for privacy and integrity while we aim to to construct
a verifiable solution without any hardware assumptions. VC-based
approaches focus on the integrity (or correctness) of DNNs compu-
tations performed by the cloud provider, which can hardly detect
subtle attacks on model’s integrity, such as the neural network
trojan attacks [32] and targeted poisoning attack. In this paper, we
propose a general method for generating sensitive samples and use
them to detect the completeness of the server’s calculation results.
We note that recent work [19], also designed an efficient way to
generate sensitive samples against subtle attacks on the model’s
integrity. However, the way to generate sensitive samples in [19] is
only for DL models that exclusively contain continuous activation
functions. Moreover in [19], to ensure that an untrusted server is
difficult to distinguish between real samples and sensitive samples,
the feasible domain for selecting sensitive samples is limited to a
small domain, which weakens the sensitivity of sensitive-samples
to model’s changes. Compared with [19], our scheme for generating
sensitive samples is applicable to all neural network frameworks, be-
cause all complex activation functions (including non-continuous)
have been transformed into polynomials (continuous). Moreover,
Applying LHE guarantees that all the user’s encrypted input is
indistinguishable to the server. As a result, in the process of gener-
ating sensitive samples, we can choose samples with the highest
sensitivity to model modification as the optimal samples.
3 BACKGROUND AND PROBLEM
STATEMENT
In this section, we first review the concept of DNNs, and then
describe the scenario, threat model and security and privacy re-
quirements considered in this paper.
3.1 Deep Neural Networks
As shown in Figure 1, a DNN usually consists of one input layer,
one or more hidden layers and one output layer. Each two adjacent
layers are connected by weights ω (i.e., model’s parameters), where
each circle represents a neuron associated with an element-wise
nonlinear activation function φ (i.e., sigmoid, ReLU, softmax, etc).
Here we use DNN training process to describe how it works. Specif-
ically, given a training sample (x, y), the input x will be iteratively
propagated to the next layer with linear transformations and non-
linear activation functions. Then, the neural network outputs the
inference result ¯y in the last layer. This process is usually called
Figure 1: General DNNs training process
feedforward. To find the optimal parameters (i.e., ω) for accurately
reflecting the relationship between x and y, a loss function L is
adopted to measure the distance between y and ¯y. Usually, L is
given as L=||y − ¯y||2, where || · ||2 is the l2 norm of a vector. Then,
to minimize the loss function L, the Stochastic Gradient Descent
(SGD) algorithm [45, 62] is used to find the optimal parameters ω.
We call this process as backpropagation. After the DNN converges
to pre-set accuracy, it can be used for subsequent inference. In this
paper, we focus on the model’s integrity and user’s privacy in the
inference process.
3.2 Scenario
Figure 2: Our Scenario
As shown in Figure 2, our SecureDL has two generic entities, a
user and a cloud server. To receive inference services, the user (also
called the client) first encrypts its well-trained DL model and out-
sources it to the cloud2. Then, the server allocates resources for this
model for a fee, such as assigning computing and storage resources,
and releasing APIs for the inference service. In the inference pro-
cess, once the user submits its encrypted query request to the cloud,
the server performs the preset operations of the outsourced model,
and returns the corresponding encrypted inference result (such as
classification and regression) to the user. The above scenario has
been widely used in the field of outsourcing computing [10, 19, 20].
In this way, model owner can not only save resources required
for local storage and execution of the DL model, but also enjoy
real-time inference services without geographical and hardware
(partial) restrictions.
2We do not consider the details of the model training, that is, the user can train the
model locally, or fine-tune a model obtained from the public model zoo. Please note that
the user needs to convert all activation functions not supported by LHE to polynomials
before the model is trained.
Input࢞ൌሺ࢞૚ǡ࢞૛ǡ࢞૜ሻ࢞૚(cid:28564)࢞૛(cid:28564)࢞૜(cid:28564)Input layerHidden layerOutput layer࣐࣐࣐࣓Neuron࣐ሺ࣓ή࢞ሻParameter OptimizationObjectivefunction L࢟૚തതത࢟૛തതത࢟૚(cid:28595)࢟૛(cid:28595)࢟LossBackpropagationFeedforward(Stochastic Gradient Descent)࢟ഥPredictionࡸൌȁȁ࢟െ࢟ഥȁȁ૛(cid:28564)ࢺ࣓ࡸ(cid:28564)ࢺࢎࡸ(cid:28564)࣓՚࣓െࣁࢺ࣓ࡸ(cid:28564)User        Outsourcing encrypted model   Encrypted query request  Encrypted result Cloud server787Secure and Verifiable Inference in Deep Neural Networks
ACSAC 2020, December 7–11, 2020, Austin, USA
3.3 Threat Model and Security and Privacy
Requirements
privacy requirements as follows.
In our SecureDL, the cloud server is considered as the main adver-
sary. On the one hand, it may infer users’ data privacy by utilizing
the mastered prior knowledge [54, 56–58], such as the encrypted
dataset, query records, and ciphertext results. On the other hand,
it may also try to compromise the model’s integrity. Specifically,
we consider, but not limited to, the following attacks to compro-
mise the model’s integrity hosted in the cloud: 1○The cloud server
can exploit the potential vulnerabilities in the network and the
service interface to implement the attack. 2○The cloud server can
use a simpler or compressed model to replace the original model,
thereby breaking the integrity of the model. 3○The cloud server has
full access to the encrypted outsourced model, user’s inputs, and
inference results. It can launch attacks based on this information.
Under the above threat model, we formulate the security and
• Protect the model’s integrity: In order to obtain certain ben-
efits, a malicious server is fully capable of modifying the
model’s parameters and architecture to deceive customers.
Our goal is to detect any subtle model changes in an efficient
manner.
• Guarantee the confidentiality of the model’s parameters: The
model’s parameters are valuable intellectual property, which
may be generated with a lot of resources, and even contain
some user’s proprietary information. A secure outsourcing
inference service should protect this information from being
leaked to the server.
• Privacy protection of user’s requests and inference results: In
the inference process, a user will submit its query requests to
the server for inference services (such as image classification
and numerical prediction, etc). Sometimes, these data are
sensitive and may contain user’s personal information (such
as avatar, health status, and psychological behavior). In addi-
tion, the inference results always imply some relationships
with the user’s inputs. Therefore, the privacy of user’s re-
quests and inference results should be protected from being
leaked to the server.
4 PROPOSED SCHEME
The goal of SecureDL is to realize the privacy-preserving outsourced
inference services while guaranteeing the model’s integrity. To
achieve this, we first design a general function approximation algo-
rithm to transform non-linear activation functions to low-degree
polynomials. This will facilitate the generation of sensitive-
samples and the and the application of LHE in general DNNs. Then,
we generate generic sensitive-samples to verify the correctness
of model parameters in the inference process. In the end, to protect
the user’s privacy, LHE is used to provide privacy-preserving DNNs
inference.
4.1 Function Approximation
As discussed before, the most notable shortcoming of LHE is that
it only supports limited number of addition and multiplication op-
erations in the encrypted domain. Also, complicated non-linear
activation functions in DNNs, such as Sigmoid (y =
1+e−x ) and
1
ReLU (y = max(0, x)), are not directed supported by LHE. For the
smooth execution of LHE, these complicated non-linear activation
functions need to be approximated by functions (such as polynomi-
als) that only contain addition and multiplication.
On the other hand, given a bound on the error between the
original activation function and its transformed polynomial, the
degree of the transformed polynomial should be minimal to boost
efficiency [53]. Therefore, to reduce the overhead of the LHE during
DNN inference process, a priority task of our SecureDL is to de-
sign such an algorithm, which can find the low-degree polynomial
within a given error.
Theorem 1. Given an error bound ϵ, let M(x) be a continuous
function on the closed interval [a, b], there exists a polynomial p(x)
that satisfies |M(x)−p(x)| < ϵ for all x belong to the interval [a, b].
Proof : This theorem is based on the Weierstrass Approximation
theorem [12]. Briefly, we first construct Bernstein polynomial
[11, 12] based on the Weierstrass Approximation theorem. Then,
we prove that any continuous function can be approximated by the
Bernstein polynomial with any given error bound ϵ. For detailed
proof, please refer to the APPENDIX1.1.
Based on Theorem 1, given an objective continuous activation
function M(x) and an error bound ϵ, we can use the Bernstein
function to find a satisfactory polynomial. However, it does not
give a method of how to find a low-degree polynomial. Moreover,
Bernstein polynomial has been proven to be inefficient in approxi-
mating any continuous function [11, 12]. In general, the polynomial
function classes used to approximate the known function M(x) are
diverse. Even if the function class is selected, the function p(x) used
as an approximate representation of M(x) is still determined in
various ways. For example, work [21] suggests exploiting Legendre
polynomials [44] and Chebyshev polynomials [7] to approximate
the original function, while other works, such as [29], propose to
use piecewise interpolation [51] to achieve the transformation of
the objective function into polynomials. In this paper, we exploit
the well-known least square approximation algorithm [8] (see AP-
PENDIX 1.2) to generate the low-order polynomial, since it is very
efficient compared to the above methods, and can be easily realized
by standard programming software such as Matlab, CurvFit, and
Prism. As shown in Algorithm 1, given an error bound ϵ and a
target non-linear activation function M(x), we first initialize the de-
gree of approximated polynomial to 1 (i.e., N = 1). Then, we get the
approximated polynomial pN (x) based on the least square approxi-
mation algorithm3. Next, we verify whether {|pN (x)− M(x)|2
2 ≤ ϵ}
(see Eqn.(12) in APPENDIX 1.2 for the definition), if so, the current
polynomial is returned as the final low-degree polynomial; other-
wise, let N = N + 1, continue to generate pN (x) and iteratively
verify the above operation until we find a degree polynomial(i.e.,
p∗(x) ) that meets the above constraint. Based on Theorem 1, for
any continuous activation functions, the iterative process in Algo-
rithm 1 (lines 4-5) is finite, thus we can certainly find a low-degree
polynomial p∗(x) satisfying the constraint {|p∗(x) − M(x)|2
2 ≤ ϵ}.
For the discontinuous activation functions such as ReLU, we use dis-
crete least squares algorithm [8] to approximate them. Experiments
3Please note that for discontinuous activation functions such as ReLU, Theorem 1 is
not true. Hence, for these discontinuous activation functions, we use discrete least
squares algorithm [8] to approximate them. Experiments (see Section 5) show that we
can still find satisfactory low-degree polynomials.
788ACSAC 2020, December 7–11, 2020, Austin, USA
Guowen Xu et al.
(See Section 5) show that we can still find satisfactory low-degree
polynomials.
Algorithm 1 Generating the low-degree polynomial
Input: A given error bound ϵ and the target non-linear activation functions M(x).
Output: The low-degree polynomial p∗(x).
1: N = 1. /* Initialize the degree of approximated polynomial to 1. */
2: tmp = pN (x) based on the least square approximation algorithm
[8]. /* pN (x) denotes the approximated polynomial with degree N . */
3: while {|tmp − M(x)|2
N + +.
4:
tmp = pN (x).
5:
6: end while
7: p∗(x) = pN (x).
8: Return p∗(x).
2 ≥ ϵ} do
Remark: We notice that some works [10, 21, 29] have proposed
methods to convert complex functions into polynomials. However,
compared with them, we give a formal proof that given an arbitrary
objective continuous function, it is feasible to approximate to a
polynomial whose error from the objective function is within a
given range, while existing works are fragmented or only provide
a scratch. Moreover, Compared with existing works, our work can
transform any activation functions into polynomials, where we use
discrete least squares to give a heuristic conversion algorithm for
non-continuous functions. For model detail, please refer to Section
2.1.
4.2 Sensitive-samples Generation and
Inference with LHE
We assume that the cloud server may modify the outsourced model
fθ(x) to f ′
θ(x), where θ is the set of all model’s parameters. However,
as discussed before, the cloud provider only provides a black-box
way for users to verify the model’s integrity. To address this chal-
lenge, similar to work [19], our main idea is to generate a small
set of test samples {(ci , fθ(ci))|i = 1, 2,· · · , v} (called sensitive-
samples), where fθ(ci) is the correct output with input ci. Then,
we use these sensitive-samples to verify the model’s integrity.
As show in Figure 3, we first generate a small set of sensitive-
samples {(ci , fθ(ci))|i = 1, 2,· · · , v}, and then send ci ,(i = 1, 2,
· · · v) to the cloud for classification. Once obtaining the classifica-
tion results f ′
θ(ci),the user can check if the model is intact by only
comparing the equality between fθ(ci) and f ′
4.2.1 The Goal of Sensitive-samples. To achieve the above require-
ments, we need to find such sensitive-samples that are very
sensitive to model changes. Moreover, the generated sensitive-
samples should be hardly spotted by the adversary. In this section,
we design a novel sample generation algorithm to generate the
sensitive-samples. Specifically, the DNN model can be defined
as y = fθ(x). We rewrite the DNN model as
θ(ci).
y = f (ω, x) = [y1, y2, · · · , ym] = [f1(ω, x), · · · , fm(ω, x)]
where ω = [ω1, ω2,· · · , ων] is a subset of θ in our consideration. It
contains the weights and biases. fi(ω, x), i = (1,· · · m) represents
the analytic expression of yi with input x. Without loss of generality,
we assume that the adversary compromises the outsourced model
by modifying the correct parameters ω to ω′ =ω + ∆ω. Hence,
Figure 3: Using sensitive samples to verify the integrity of
the outsourced model
the modified model can be denoted as y′ = f (ω + ∆ω, x). In order
to detect model anomalies sensitively, a good sensitive-sample
c should maximize the difference between y and y′. Formally, a
sensitive-sample c should be the optimal value such that
c = arg max
x
= arg max
x
i =m
i =1
||f (ω + ∆ω, x) − f (ω, x)||2
2
||fi(ω + ∆ω, x) − fi(ω, x)||2
2
(1)
∂fi(ω, x)
To solve the above optimization problem, we first perform Taylor
expansion4 on fi(ω + ∆ω, x) as follows,
∂ω
∆ω + O(||∆ω ||2
2)
fi(ω + ∆ω, x) = fi(ω, x) +
(2)
Based on the Taylor expansion, the differences between fi(ω +
∆ω, x) and fi(ω, x) can be approximated as below,
2 ≈ || ∂fi(ω, x)
2 ≈ i =m
||fi(ω + ∆ω, x) − fi(ω, x)||2
||f (ω + ∆ω, x) − f (ω, x)||2
|| ∂fi(ω, x)
Further, we have
∆ω ||2
2
∆ω ||2
2
(3)
∂ω
∂ω