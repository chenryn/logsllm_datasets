dinal sample of 2× 104 users randomly chosen and manually
labeled each day, we compute an unbiased FNR statistical
measure of the volume of abusive accounts on Facebook, re-
gardless of direct detection. This measure is denoted as the
“prevalence” of abusive accounts and can be thought of as
the false negative rate of all abusive account detection sys-
tems (including DEC) combined. If we add to the prevalence
measurement the number of abusive accounts caught by DEC
Figure 5: Comparison of precision vs recall curves for dif-
ferent models on our evaluation data. Both DEC models per-
form signiﬁcantly better than the behavioral model, and the
DEC-MS-MTL has higher recall across the entire operating
space. This evaluation is over accounts that have already gone
through several stages of security evaluation, and as such this
population represents the hardest accounts to classify. Given
the difﬁcult classiﬁcation nature of this sub-population, such
recall performance is considered excellent by Facebook.
regions. DEC with MS-MTL signiﬁcantly improves the sys-
tem recall over single stage DEC at high precision operating
points, improving by as much as 30%.
We note that this evaluation is over accounts that have
already gone through other security classiﬁcations such as
registration time or login-time remediation (i.e., the hardest to
classify accounts). As such, the overall recall level is expected
to be lower than that of a system which operates on all active
accounts (Section 7.4).
DEC with MS-MTL’s improvement in recall over behav-
ioral models makes it particularly attractive in a real world
operating environment where recall over hard to classify ac-
counts is an important operating characteristic.
7.3.3 Quantiative Assessment: Area Under the (AUC)
Curve and Precision / Recall
Table 5 shows a comparison of precision, recall, and ROC
performance between the three models. ROC performance
is calculated as the total area under the curve (AUC). Preci-
sion is ﬁxed at 0.95, a common operating point for assessing
performance. The behavioral model is unable to achieve a pre-
cision of 0.95 at any recall, and is excluded. We ﬁnd that while
4108    30th USENIX Security Symposium
USENIX Association
Figure 6: Precision over time: 3-day moving average of de-
ployed (DEC-MS-MTL) model precision on live Facebook
production data, spanning one month. Precision is stable,
never decreasing below 0.97. The y-axis is truncated.
speciﬁcally (and not other detection systems), we obtain an
estimate of what the prevalence of abusive accounts would
have been in the absence of DEC.
Figure 7 plots the observed prevalence of abusive accounts
(with DEC deployed) and inferred prevalence without DEC,
over the period of a month. A loss in DEC’s recall (equiva-
lently, an increase in DEC’s FNR) would manifest as either
an increase in overall abusive account prevalence, or a de-
crease in the power of DEC compared to non-DEC methods
(a decrease in the difference between the two measures). We
observed neither of these phenomena over our one-month ex-
periment, indicating that DEC’s recall did not meaningfully
shift during this period and suggesting that there was not
adversarial adaptation to DEC.
Before DEC’s launch, Facebook reported instances of ad-
versaries adapting within hours to new detection systems;
since the advent of DEC there have been no such reports.
Our hypothesis is that the “deep feature” architecture of DEC
makes the system more resistant to adversarial adaptation
than other abusive account detection systems. As discussed
in Section 5.1, an adversary wishing to manipulate a user
feature aggregated through the graph must control that feature
on all of the relevant entities connected to the original user.
When we apply this reasoning to the multitude of different
entity associations — including but not limited to user friend-
ship, group membership, device ownership, and IP address
appearance — we are drawn to the conclusion that manipu-
lating many such features would be far more expensive for
an attacker than manipulating “direct” user features such as
country, age, or friend count.
Since deployment, DEC has become one of the key abusive
account detection systems on Facebook, where it has been
responsible for the identiﬁcation and deactivation of hundreds
of millions of accounts. Over our evaluation period the av-
erage estimated prevalence without DEC would have been
5.2%, while the average observed volume of abusive accounts
Figure 7: Recall over time: DEC defense over a 30-day win-
dow, using 3-day moving averages. The green line is the
observed volume (as a percent) of abusive accounts on Face-
book, and the red marked line is the volume of accounts taken
down by DEC. The blue line is the sum of the other two
and estimates what the volume of abusive accounts would
have been in the absence of DEC; the gray shaded area thus
represents the inferred impact of DEC.
Table 6: Area under the curve (AUC) and recall at precisions
0.95 and 0.99 for DEC over a random sample of all accounts
on Facebook.
Population AUC Recall @ Prec. 0.95 Recall @ Prec. 0.99
All accts.
0.981
0.981
0.955
on Facebook was 3.8%— an improvement of 27%.
DEC Over All Accounts. Our evaluation of DEC thus far has
focused on the hardest types of abuse to classify—accounts
that were not identiﬁed by other production abuse detection
systems. A separate question is how effective could DEC be
at identifying all abusive accounts, including those caught
by these other systems. To answer this question we evalu-
ated DEC over 1.6× 104 active accounts sampled at random
from the entire population of accounts on Facebook, includ-
ing those that had been detected as abusive by other systems.
These accounts were deﬁnitively labeled by expert human
labellers and used as ground truth for our evaluation. Table 6
shows the performance of DEC across this population of all
accounts. DEC performs well over this population, with an
AUC of 0.981, recall at precision 0.95 of 0.981 and recall at
precision 0.99 of 0.955. As expected, both the AUC and recall
at ﬁxed precision are signiﬁcantly higher on the full popula-
tion than on the sub-population of accounts not detected by
other systems (Table 5).
8 Discussion and Lessons Learned
After more than two years of deployment at Facebook, we
have learned multiple lessons and identiﬁed several limita-
tions from developing and using DEC.
USENIX Association
30th USENIX Security Symposium    4109
8.1 Reducing Computational & Human Load
It is computationally expensive to extract graph features for
all active users at the scale of Facebook. Given our current
implementation of feature extraction within two hops from
the target node in graph, for each user we might need to reach
out to hundreds or thousands of neighboring nodes in order
to extract all of their information and aggregate it back to
the target node. To mitigate this problem we have developed
caching strategies that reuse previous feature extraction re-
sults as much as possible. However, because many features
have time sensitivity, we still need to update and re-extract a
considerable amount of them at each reclassiﬁcation.
The computational load of DEC is high—equivalent to
0.7% of global CPU resources of Facebook. However, the
deployment of DEC actually reduced global CPU usage of
Facebook. DEC achieved this counter-intuitive result by iden-
tifying and removing such a large volume of abusive accounts
that the combined CPU usage of those abusive accounts more
than accounted for the computation required for feature ex-
traction, training, and deployment of DEC.
DEC also greatly reduced human costs, in terms of human
review resources that would have been needed to evaluate and
take down abusive accounts manually. DEC’s deployment
reduced the total review resources needed for abusive account
detection by between 15% and 20%.
8.2 Segmentation and Fairness
One key ﬁnding is that a single-task classiﬁer performs differ-
ently across different segments within the task. For example,
if we segment accounts by the self-reported age of their own-
ers, an abusive account classiﬁer might show a higher false
positive rate on one age segment than others. Similarly, the
performance might vary over different geographies, as we
are building a single model to ﬁt a global product that may
be used differently across different cultures. Such variation,
which can be expected across such a large and heterogenous
user base, may be interpreted as the model treating some
groups of people unfairly relative to others.5 In the data set
used for this paper we were not able to ﬁnd any segments
on which classiﬁer performance differed to a statistically sig-
niﬁcant extent, but it is possible that with retraining and/or
different segmentation such unfairness may arise. As a result,
we have proactively considered several measures to reduce
variation across different segments.
Our key insight is that segmentation effects are highly cor-
related with bias in the training data. Suppose for example
that we use the account owner’s age as a feature, and that the
owners of abusive samples in the training data are younger on
average the owners of non-abusive samples. In this case, if we
do not adjust the proportions of different segments in our train-
ing data, the classiﬁer may reach the conclusion that accounts
owned by young people are more likely to be abusive.
As a ﬁrst step towards preventing such bias, we have re-
moved from the model all “direct” user demographic features,
including age, gender, and country. While these features could
be helpful in predicting abuse, they could easily introduce un-
fairness in the model as in the age example above — we don’t
want to penalize younger benign users just because attackers
usually choose to set their fake accounts to have a young age.
The next approach we considered is to sample the labeled
data in order to create a training set that reﬂects overall OSN
distributions as closely as possible. In ongoing work, we are
experimenting with training DEC using stratiﬁed sampling
based on attack clustering, in particular downsampling large
clusters so as to minimize the inﬂuence of a single attack on
the ultimate model. This approach would make sure that a
large attack from a given user demographic does not teach
the model that most users from that demographic are abusive.
However, stratiﬁed sampling becomes prohibitively costly as
we try to match the distribution of more and more segments.
In addition, as we add more dimensions the segments get
smaller, and statistical noise soon introduces enough error to
outweigh the precision gains from sampling.
A ﬁnal approach is to split particular segments out and
create dedicated tasks in the MS-MTL framework for them;
however, this approach requires us to collect sufﬁcient training
data for each segment, and the maintenance cost increases
with the number of models trained. Instead of training and
maintaining multiple models, Facebook has chosen to monitor
speciﬁc high-proﬁle segments for false positive spikes and
address any issues by tuning the overall model to reduce
segment-speciﬁc false positives.
8.3 Measuring in an Adversarial Setting
Since abuse detection systems inherently operate in an adver-
sarial environment, measuring the impact of system changes
is a particularly difﬁcult problem. A common adversarial
iteration looks like:
1. The attacker ﬁnds a successful method to abuse Facebook.
2. Facebook adjusts its detection system and mitigates the
attack.
3. The attacker iterates until they either achieve (1) again, or
the resource cost becomes too high and they stop.
Assuming constant effort on the part of the attacker and
Facebook, the above cycle eventually settles on an equilib-
rium. Because of this cycle, it is difﬁcult to properly measure
the effect of our models using A/B tests during deployment.
If our experiment group is too small, we never reach step 3
because the attacker has no incentive to change. Our metrics
might look good in the experiment group, but we will hit
step 3 when we launch more broadly and performance will
decline.
5Note that the assessment of “fairness” will depend on the metric used,
and one may get different results when using, for example, accuracy vs. pre-
cision vs. false positive rate.
One way to mitigate this problem is to add a “holdout
group” to feature launches. The holdout group is a random
sample of users that are predicted by the model to be abusive.
4110    30th USENIX Security Symposium
USENIX Association
Instead of acting to block these accounts immediately upon
detection, we stand back and conﬁrm the abuse happened as
expected before enforcing on these users. Such holdouts help
us to more accurately measure the precision of our classiﬁer,
but must be carefully weighed against the potential impact, as
holdouts can lead to further abuse. For this reason, holdouts
are not used for all types of abuse.
8.4 Adversarial Attacks on DEC
An attacker may attempt to poison the ﬁrst stage of low-
quality labels by creating numerous colluding accounts that
seek to be labelled benign by the rule-based detection systems.
Given the scope of DEC’s training data and the relatively low
sample rate, it would be extremely difﬁcult for attackers to
generate such accounts at a scale that would signiﬁcantly im-
pact the trained model (Section 6.2), especially given that
other (non-DEC) systems exist speciﬁcally to limit the cre-
ation of fake accounts at massive scale.
An attacker may attempt to evade the classiﬁer by creating
large groups of fake accounts connected to each other so that
they can control all of the deep features. This subgraph would
have to either be isolated from the rest of the friend graph
(which is itself suspicious) or have a reasonable number of
connections to the main graph. In the latter case, since DEC
operates on second-order connections, almost all of the DEC
features would include data from real accounts outside the
adversary’s control. In addition, while the adversary controls
the fake accounts’ behavior, they don’t know how a similar
set of connected legitimate users behaves, and the coordinated
activity of the fake accounts would be detected as anomalous
by DEC.
An attacker could also attempt to trick DEC into misclas-
sifying a benign user as abusive, based on features of its
neighbors that the victim has no control over. For example,
an attacker could create a subgraph of abusive accounts as
above and attempt to friend a victim using these accounts. If
the victim accepts one or more friend requests, they embed
themselves in the abusive sub-graph, which could cause DEC
to incorrectly act on the victim. This “forced-embedding” at-
tack is also challenging to execute. First, “attempted” links
between entities (e.g., unresolved or denied friend requests)
are not features in DEC. Second, a single bad edge between
the victim and an abusive sub-graph is insufﬁcient to cause
a false classiﬁcation. A victim would need to be deceived
numerous times for there to be a risk of misclassiﬁcation.
Finally, DEC-identiﬁed accounts are given the opportunity to
complete challenges or request human review as a fail-safe to
guard against incorrect classiﬁcation [30].
8.5 Limitations and Future Directions
While DEC has been highly effective at detecting abusive
accounts in practice, its design offers several opportunities
for improvement:
• DEC is computationally expensive, particularly due to its
use of deep features. However, in Section 8.1 we discussed
how this high computational cost is actually balanced by
resource savings from identifying more abusive accounts.
Reducing the computational cost further is an active area
of work that is receiving at least as much attention as im-
proving model quality.
• Intuitively, DEC’s classiﬁcations are based on an account’s
position and connections within the Facebook graph. Ac-
counts that exhibit low levels of activity or connections
provide fewer signals for DEC to leverage for inference,
limiting its effectiveness. However, even if such accounts
are abusive, they inherently have less impact on Facebook
and its users. We are currently exploring approaches to in-
clude features that better capture these low-signal accounts.
• DEC’s machine learning model lacks interpretability, as
it relies on a DNN to reduce the high-dimensional space
of deep features into the low-dimension embedding used
for classiﬁcation decisions. This characteristic makes it
difﬁcult to debug and understand the reasoning behind
DEC’s decisions. Making the model interpretable is an
active area of research.
• DEC’s approach of aggregating data from many users to
produce features for classiﬁcation is less sensitive to out-
liers than an approach of using direct features. As a conse-
quence, DEC may be less discriminative of extreme feature
values than other model families. We have taken a “defense-