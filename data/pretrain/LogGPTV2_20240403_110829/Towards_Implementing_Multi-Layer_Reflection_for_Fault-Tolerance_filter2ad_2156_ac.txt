T1_init
T2_init
X_init
Y_init
R1 by T1
R1 by T2
T1_after_R1
T2_init
X_after_R1_by_T1_init
Y_init
...
after_R1
R2 by T1
R2 by T2
after_R2-2
T1_after_R1_R2
T2_init
X_after_R1_by_T1_init
Y_after_R2_by_T1_after_R1
T1_after_R1
T2_after_R2
X_after_R1_by_T1_init
Y_after_R2_by_T2_init
after_R2-1
init
R1
after_R1
R2
R2
init
R1
after_R1
R2
after_R2-1
after_R2-2
after_R2
a- Detailed states (OS view) 
b- Major of the states (OS view) 
c- States (MLR view) 
Figure 3: Different Views of the Computation  
4.3. The Multi-Layer Reflection Case 
Consider  now  the  third  fault-tolerance  programmer, 
who can control both the OS and the middleware, running 
in  thread  pool  mode  in  our  example.  From  the  particular 
semantics of a thread-pool, he knows that the thread states 
T2_init,  and  T1_after_R1  are  equivalent,  as  pool 
threads  do  not  keep  memory  of  previously  processed 
requests.  In  other  words,  threads  in  the  pool  always 
process  requests  from  a  pre-defined  initial  state.  No 
information regarding the processing of R1 can propagate 
to R2 through thread T1. The potential causal dependency 
shown on Figure 2 does not exist3. So, taking into account 
the  semantics  of  concurrency  models  at  the  middleware 
level  allows  him 
the  “request-to-thread 
allocation”  as  a  source  of  non-determinism.  There  is  no 
need  for  the  ORB  running  the  leader  replica  to  force  its 
follower  to  allocate  requests  to  exactly  the  same  threads. 
The  distinction  made  in  Figure  3-b  between  the  states 
after_R2-1  and  after_R2-2  is  useless.  after_R2-1
and after_R2-2 are  grouped  into  the  single  state 
after_R2, as shown in Figure 3-c.  
Our  third  programmer  can  thus  avoid  problem  PB2.  In 
addition,  having  access  to  the  internal  decision  of  the 
ORB,  i.e.  delivery  and  retrieval  to/from  the  pool,  solves 
problem  PB1  in  a  more  elegant  and  efficient  manner.  In 
§5, we describe how this can be done in practice. 
to  discard 
This 
small  example 
illustrates  how  combining 
information  obtained  from  several 
levels  can  help 
discarding sources of non-determinism as non-relevant for 
handling replication of multi-threaded objects.  
The  complementary  nature  of  high  and  low  level 
reflection  and  lessons  learnt  from  reflective  systems 
3  In  our  example  the  R1  and  R2  do  no  share  state  variables,  and  the 
resulting state does not depend on their interleaving. However, the causal 
dependency  through  shared  variables  could  be  handled  by  enforcing 
access  to  shared  variables  X  and  Y  in  the  same  order,  using  mutex-
control approaches as in [2]. 
the 
this  notion  focuses  on 
development [15], prompted us to introduce the notion of 
multi-layer reflection and its attached terminology [17]. In 
brief, 
interdependencies 
between individual system layers to provide an end-to-end 
meta-model  that  is  explicitly  tailored  for  fault-tolerance. 
Notions of mapping and projection support the analysis of 
interlevel  coupling  from  a  reflective  perspective.  A 
mapping describes the various possible representations of 
a  given  entity  at  a  given  abstraction  level  i  by  entities 
available at a (lower) abstraction level i-1. A projection is 
the transitive closure of mapping relations that maps a top-
level  entity  to  lower  level  entities  (useful  for  state 
handling).  Reverse  projections  map  low-level  entities  to 
higher level ones (useful for error confinement).  
5. A Multi-Layer Reflection: Case Study 
In  this  section,  we  present  on  a  concrete  architecture, 
how  the  MLR  solution  (cf.  §4.3)  can  be  implemented  in 
practice, and propose for the chosen case study an explicit 
meta-model that corresponds to the requirements of Table 
3.  From  the  reverse  engineering  of  a  simple  application 
running on an ORB, we discuss step-by-step the two facets 
of  the  consistency  problem  of  replication  strategies:  the 
control of non-determinism and the state transfer. 
5.1. Case-Study Description 
We consider a system composed of a POSIX-compliant 
OS,  a  CORBA-compliant  middleware,  and  a  simple 
application that implements the following IDL interface: 
interface Hello { 
  unsigned long say_hello();
};
On receiving a request "say_hello()", the application 
increments  an  internal  counter  (originally  set  to  0),  and 
returns  the  new  value  to  the  client.  A  possible  C++ 
implementation of this application can be as follows.  
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:28:40 UTC from IEEE Xplore.  Restrictions apply. 
CORBA::ULong Hello_impl::say_hello() {
  CORBA::ULong result ; 
pthread_mutex_lock(&_object_lock);
_count++ ; 
    result = _count ; 
    cout << "Hello World!: " 
         << _count << endl; 
pthread_mutex_unlock(&_object_lock);
return result ; 
}
The  counter  _count  represents 
the  application's 
internal state. As this counter is returned to the client, the 
order in which requests are scheduled (indirectly through 
the  mutex  Hello_impl::_object_lock)  determines 
which client sees which result. 
In order to replicate this very simple application, we need 
to identify the reflective features of Table 3: control over 
execution  points  and  determinism  for  active  and  semi-
active  replication,  and  the  state  transfer  for  passive 
replication and cloning. To reach this goal, Figure 4 shows 
the 
reverse  engineering  of  a  concrete  CORBA 
implementation  running  our  example.  This  figure  shows 
simplified traces of the different active threads within the 
Orbacus  CORBA  implementation  when  processing  the 
say_hello()  request  in  pool  mode.  Orbacus  (version 
4.1.1) was used in thread_pool mode with four threads 
in  its  pool  (p  =  4),  on  Linux  (version  2.4.18),  leading  in 
this case to 8 active threads (4 additional service threads!). 
On the figure, four threads are shown, with numbers 1, 
3, 4 and 8. 1 is the main thread, 3 the thread that accepts 
socket  connections  (i.e.  it  executes  the  accept  system 
call).  Thread 4  is  one  of  the pool  threads  (the  other  pool 
threads  correspond  to  the  numbers  5,  6,  7–  not  shown). 
Thread 8 is the receiver thread associated to the invoking 
client. The thread number 2, not shown, corresponds to the 
manager 
current  Linux  pthread 
implementation.  This  manager  thread  is  totally  hidden  to 
the  user  of  the  pthread  library,  and  is  used  internally  to 
carry  out  all 
thread  management  actions  (blocking, 
signaling,  suspension,  creation,  and  destruction).  This 
manager  thread  is  an  example  of  implementation  choices 
that  remain  totally  invisible  to  higher  system  levels 
implemented on top of it. 
thread  of 
the 
In this figure, we can distinguish four main phases: 
2. 
1.  First, the ORB is initialized (calls numbered from (0) 
to  (5)).  The  thread  pool  is  created  (calls  number  (2) 
and (3)) and the accepting thread 3 is spawn. 
In the second phase, a connection request is received 
from a remote client, and a receiver thread is launched 
(call  number  (6)):  several  connections,  several 
receiver  threads.  Connection  management  realized 
within the ORB is transparent to the application level. 
In a third phase, the request is received by the receiver 
thread (call (8)), and travels up to the application code 
(call  (14)  to  say_hello()).  The  transfer  of  the 
request from Thread 8 (receiver) to Thread 4 (thread 
3. 
pool  member)  occurs  through  the  a  shared  request 
queue  (Thread  8  invokes  ThreadPool::add(..),
which  awakes  Thread  4  and  have  it  return  from 
ThreadPool::get(..).).
4.  Thread  4  returns  from  the  application  and  calls  a 
sequence  of  object  methods  (15  to  18)  to  return  the 
result of the request execution (call (19)) to the client.  
5.2. Request Execution Related Meta-Model 
Definition of the Meta-Model 
In order to handle the non-determinism and control the 
execution,  we  focus  on  the  part  of  the  meta-model  of 
Table  3  related  to  request  execution.  We  model  here  the 
lifecycle of a request as follows: (i) a request is received 
by  the  ORB,  (ii)  delivered  to  the  application  and  finally 
(iii) results are sent back to the client. This lifecycle could 
be  refined,  but  other  aspects  have  not  been  identified  as 
relevant  to  the  fault-tolerance  mechanisms  discussed  in 
Section  3.  Based  on  this  lifecycle,  we  must  be  able  to 
observe the following classes of reified events (see Figure 
4) for a detailed control of request execution through the 
ORB: 
BeginOfRequestReception
EndOfRequestReception
RequestBeforeApplication
RequestAfterApplication
BeginOfRequestResultSend
EndOfRequestResultSend
RequestContentionPoint
The processing of a request reifies exactly one instance 
of  each  of  these  event  classes,  except  for  the  last  one: 
RequestContentionPoints  correspond  to  the  several 
decision  points,  in  the  ORB  and  the  application,  that 
determine the ordering of request processing.  
The Meta-Model applied to the Example 
From the reverse engineering4 analysis in Figure 4, one 
can  easily  identify  the  first  six  "Request  related  events" 
mentioned  previously.  BeginOfRequestReception  is 
mapped  to  the  call  to  recv  (number  (8)  in  the  figure); 
EndOfRequestReception to the return of the same call. 
RequestBeforeApplication  is  mapped  to  the  call  to 
say_hello(); RequestAfterApplication 
the 
return  of  the  same  call;  BeginOfRequestResultSend
and EndOfRequestResultSend  are  mapped  to  the  call 
and the return of send respectively (number (19). 
to 
4  A special reverse engineering tool was developed on purpose to obtain 
this  graph  by  analyzing  the  runtime execution  of  an open-source  ORB, 
here Orbacus. 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:28:40 UTC from IEEE Xplore.  Restrictions apply. 
Caption:
ORB_impl
C++ class / C function
main
program entry point
Hello_impl
(10) 8:add
A
(2) 1:new A
A
(5) 1:Create
Thread 3
A
b
X
a
class / function 
of interest
invocation of method
"add" on an object of
class "A" by thread "8",
registered as the "10th"
observed event
instantiation  of an object
of class "A" by thread
"1", registered as the
"2nd" observed event
spawning  of a new
thread "3" into an object
of class "A" by thread
"1", registered as the
"5th" observed event
order of observation of
calls made from objects
of class "X". Call labeled
"a" is observed before
call labeled "b".
network communication
OS Kernel
layer / abstraction level
Hello_impl
(14) 4:say_hello
b
POA_Hello
a
c
(13) 4:_OB_op_say_hello
Application
main
(0) 1:-
a
ORB_impl
b
(12) 4:_OB_dispatch
(15) 4:_OB_postMarshal
b
POA_impl
a
(1) 1:new
POA_impl
(11) 4:_OB_dispatch
ServantBase
(2) 1:new
ThreadPool
b
ThreadPoolDispatcher
a
(3) 1:Create
Thread 4
(4) 4:get
(16) 4:postMarshal
ThreadPool
(10) 8:add
a
Upcall