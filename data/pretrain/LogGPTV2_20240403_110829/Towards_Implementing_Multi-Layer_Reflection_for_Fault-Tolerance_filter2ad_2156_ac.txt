### 4.3. The Multi-Layer Reflection Case

Consider the third fault-tolerance programmer, who has control over both the operating system (OS) and the middleware, running in a thread pool mode in our example. Given the specific semantics of a thread pool, this programmer knows that the states `T2_init` and `T1_after_R1` are equivalent. This is because pool threads do not retain memory of previously processed requests; they always process new requests from a predefined initial state. Therefore, no information about the processing of request `R1` can propagate to request `R2` through thread `T1`. The potential causal dependency shown in Figure 2 does not exist. By considering the concurrency model at the middleware level, the programmer can discard "request-to-thread allocation" as a source of non-determinism. There is no need for the ORB (Object Request Broker) running the leader replica to force its follower to allocate requests to the same threads. Consequently, the distinction between the states `after_R2-1` and `after_R2-2` in Figure 3-b is unnecessary. These states can be grouped into a single state `after_R2`, as illustrated in Figure 3-c.

This approach allows the third programmer to avoid problem `PB2`. Additionally, by having access to the internal decisions of the ORB—such as delivery and retrieval to/from the pool—problem `PB1` can be solved more elegantly and efficiently. In Section 5, we will describe how this can be implemented in practice.

### Combining Information from Multiple Levels

This small example demonstrates how combining information from different levels can help in discarding sources of non-determinism that are not relevant for handling the replication of multi-threaded objects. The complementary nature of high-level and low-level reflection, along with lessons learned from reflective systems, prompted us to introduce the concept of multi-layer reflection and its associated terminology [17]. In brief, multi-layer reflection focuses on interdependencies between individual system layers to provide an end-to-end meta-model explicitly tailored for fault tolerance. Concepts like mapping and projection support the analysis of interlevel coupling from a reflective perspective. A mapping describes the various possible representations of a given entity at a given abstraction level \(i\) by entities available at a lower abstraction level \(i-1\). A projection is the transitive closure of mapping relations that maps a top-level entity to lower-level entities, which is useful for state handling. Reverse projections map low-level entities to higher-level ones, which is useful for error confinement.

### 5. A Multi-Layer Reflection: Case Study

In this section, we present a concrete architecture and demonstrate how the MLR solution (cf. §4.3) can be implemented in practice. We also propose an explicit meta-model for the chosen case study that corresponds to the requirements outlined in Table 3. Through the reverse engineering of a simple application running on an ORB, we will discuss step-by-step the two facets of the consistency problem in replication strategies: the control of non-determinism and state transfer.

#### 5.1. Case-Study Description

We consider a system composed of a POSIX-compliant OS, a CORBA-compliant middleware, and a simple application that implements the following IDL interface:

```idl
interface Hello {
  unsigned long say_hello();
};
```

When the application receives a `say_hello()` request, it increments an internal counter (initially set to 0) and returns the new value to the client. A possible C++ implementation of this application is as follows:

```cpp
CORBA::ULong Hello_impl::say_hello() {
  CORBA::ULong result;
  pthread_mutex_lock(&_object_lock);
  _count++;
  result = _count;
  cout << "Hello World!: " << _count << endl;
  pthread_mutex_unlock(&_object_lock);
  return result;
}
```

The counter `_count` represents the application's internal state. Since this counter is returned to the client, the order in which requests are scheduled (indirectly through the mutex `Hello_impl::_object_lock`) determines which client sees which result.

To replicate this simple application, we need to identify the reflective features from Table 3: control over execution points and determinism for active and semi-active replication, and state transfer for passive replication and cloning. To achieve this, Figure 4 shows the reverse engineering of a concrete CORBA implementation running our example. This figure displays simplified traces of the different active threads within the Orbacus CORBA implementation when processing the `say_hello()` request in pool mode. Orbacus (version 4.1.1) was used in thread_pool mode with four threads in its pool (p = 4), on Linux (version 2.4.18), leading to eight active threads (four additional service threads).

In the figure, four threads are shown, numbered 1, 3, 4, and 8. Thread 1 is the main thread, thread 3 accepts socket connections (executes the `accept` system call), thread 4 is one of the pool threads, and thread 8 is the receiver thread associated with the invoking client. Thread 2, which is not shown, corresponds to the manager thread of the current Linux pthread implementation. This manager thread is hidden from the user of the pthread library and is used internally for all thread management actions (blocking, signaling, suspension, creation, and destruction). This manager thread exemplifies implementation choices that remain invisible to higher system levels built on top of it.

In this figure, we can distinguish four main phases:
1. **Initialization**: The ORB is initialized (calls numbered from (0) to (5)). The thread pool is created (calls (2) and (3)), and the accepting thread 3 is spawned.
2. **Connection Request**: A connection request is received from a remote client, and a receiver thread is launched (call (6)).
3. **Request Reception**: The request is received by the receiver thread (call (8)) and travels up to the application code (call (14) to `say_hello()`). The request is transferred from thread 8 (receiver) to thread 4 (pool member) through a shared request queue.
4. **Result Return**: Thread 4 returns from the application and calls a sequence of object methods (15 to 18) to return the result of the request execution (call (19)) to the client.

#### 5.2. Request Execution Related Meta-Model

##### Definition of the Meta-Model

To handle non-determinism and control execution, we focus on the part of the meta-model from Table 3 related to request execution. We model the lifecycle of a request as follows:
1. A request is received by the ORB.
2. The request is delivered to the application.
3. Results are sent back to the client.

Based on this lifecycle, we must be able to observe the following classes of reified events (see Figure 4) for detailed control of request execution through the ORB:
- `BeginOfRequestReception`
- `EndOfRequestReception`
- `RequestBeforeApplication`
- `RequestAfterApplication`
- `BeginOfRequestResultSend`
- `EndOfRequestResultSend`
- `RequestContentionPoint`

The processing of a request reifies exactly one instance of each of these event classes, except for `RequestContentionPoints`, which correspond to decision points in the ORB and the application that determine the ordering of request processing.

##### The Meta-Model Applied to the Example

From the reverse engineering analysis in Figure 4, we can easily identify the first six "Request related events" mentioned above:
- `BeginOfRequestReception` is mapped to the call to `recv` (number (8) in the figure).
- `EndOfRequestReception` is mapped to the return of the same call.
- `RequestBeforeApplication` is mapped to the call to `say_hello()`.
- `RequestAfterApplication` is mapped to the return of the same call.
- `BeginOfRequestResultSend` and `EndOfRequestResultSend` are mapped to the call and return of `send` respectively (number (19)).

A special reverse engineering tool was developed to obtain this graph by analyzing the runtime execution of an open-source ORB, here Orbacus.