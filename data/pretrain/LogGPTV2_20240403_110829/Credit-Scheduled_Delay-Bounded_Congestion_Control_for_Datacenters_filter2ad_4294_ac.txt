ones get dropped. Thus, synchronized arrival can result in significant
unfairness. It is particularly important because we use a tiny credit
buffer (see Section 3.2).
To address this issue, we introduce random jitter in sending credit
packets at the end-host, instead of perfectly pacing them. To deter-
mine how much jitter is required for fairness, we create a number
of concurrent flows (between 1 and 1024) that traverse a single bot-
tleneck link. We vary the jitter level, j, from 0.01 to 0.08 relative to
the inter-credit gap (e.g., the jitter is between 13 ns and 104 ns for
10 GbE) and measure the Jain’s fairness index [33] over an interval
of 1 ms. Figure 6 (a) shows the result. We observe that perfect pacing
causes significant unfairness due to exact ordering (fairness index
of 1.0 means perfect fairness). However, even small jitter (tens of
nanosecond) is enough to achieve good fairness as it breaks synchro-
nization. We also find, in our prototype implementation, the natural
jitter at the host and NIC is sufficient enough to achieve fairness.
Figure 6 (b) plots the CDF of inter-credit gap measured using an In-
tel X520 10 GbE NIC, when credit packets are sent at the maximum
credit rate with pacing performed in SoftNIC. The inter-credit gap
has a standard deviation of 772.52 ns, which provides a sufficient
degree of randomization.
However, synchronized credit drop can also arise when credits
from multiple switches compete for a single bottleneck. The jitter at
the end host is not sufficient when the credit queues at the switches
are not drained for a long period of time. In large scale simulations,
we observe that some flows experience excessive credit drop while
other flows make progress. To ensure fairness across switches, we
randomize the credit packet sizes from 84 B to 92 B. This effectively
creates jitter at the switches and breaks synchronization. With these
mechanisms, ExpressPass provides fair credit drop across flows
without any per-flow state or queues at the switches.
Starting and stopping credit flow (end-host): Finally, Express-
Pass requires a signaling mechanism to start the credit flow at the
receiver. For TCP, we piggyback credit request to either SYN and/or
SYN+ACK packet depending on data availability. This allows data
01020304010/4040/100100/100Required BufferLink / Core link speedMB(cid:3030)(cid:3045)(cid:3032)(cid:3031)(cid:3036)(cid:3047)(cid:3035)(cid:3042)(cid:3046)(cid:3047)Static credit bufferGbps0510152010/4040/100100/100Required BufferLink / Core link speedMBGbps00.20.40.60.8102505007501000Fairness IndexConcurrent Flowsj=0j=0.01   j=0.02   j=0.04   j=0.08 00.20.40.60.8100.511.522.53CDFInter credit gap (us)NICIdealNew data/CREDIT_REQUESTCREDIT_STOPCREQ_SENTCREDIT_RECEIVINGCSTOP_SENTNo credit for timeout+ CREDIT_REQUESTNew data/CREDIT_REQUESTReceive creditTimeout/CREDIT_REQUESTActive Close/FIN + CREDIT_STOPNo data for timeout/CREDIT_STOPTimeout/CREDIT_STOPReceive FINActive Open/SYN CREDIT_SENDINGCREDIT_REQUESTCREDIT_STOPActive Close/FINCREDIT_STOPSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Inho Cho, Keon Jang, and Dongsu Han
Algorithm 1 Credit Feedback Control at Receiver.
1: w ← winit
2: cur_rate ← initial_rate
3: repeat per update period (RTT by default)
4:
5:
6:
7:
8:
(increasing phase)
if previous phase was increasing phase then
cur_rate = (1 − w ) · cur_rate
credit_loss = #_credit_dropped/#_credit_sent
if credit_loss ≤ target_loss then
w = (w + wmax )/2
(wmax = 0.5)
9:
else
10:
11:
12:
13:
14: until End of flow
+ w · max_rate · (1 + target_loss)
(decreasing phase)
cur_rate = cur_rate · (1 − credit_loss) · (1 + target_loss)
w = max (w/2, wmin )
(0  target_loss), the feedback control detects
congestion and goes through the decreasing phase, where it reduces
the credit sending rate (cur_rate) so that the credit loss rate will
match its target loss rate at the next update period assuming all flows
adjust in the same manner.
The feedback loop dynamically adjusts the aggressiveness factor
w (between wmin and wmax or 0.5) to provide a balance between
stability and fast convergence. When w is large (small), the credit
sending rate ramps up more (less) aggressively. Thus, when con-
gestion is detected, we halve w in the decrease phase. When no
congestion is detected for two update periods, we increase w by
averaging its current value and the maximum value, 0.5. At steady
state, a flow experiences increase and decrease phase alternatively,
and as a result, w decreases exponentially. This achieves stability as
we show in Section 4. Finally, wmin provides a lower bound, which
keeps w from becoming infinitesimally small. In all our experiments,
we use wmin of 0.01. Setting wmin involves in a trade-off between
a better steady state behavior and fast convergence, as we show
through analysis in Section 4.
3.3 Parameter choices
Initial rate and winit determine how aggressively a flow starts. They
decide the trade-off between small flow FCT and credit waste.
Setting them high allows flows to use as much bandwidth as possible
from the beginning. However, a flow with only a single packet to
send will waste all-but-one credits in the first RTT. Setting them
low reduces the credit waste, but increases the convergence time
23456140510151  1/2  1/4  1/8  1/16  1/32initial rate / max rateConvergence TimeRTTs804020104202550751001  1/2  1/4  1/8  1/16  1/32initial rate / max rateWastedCreditscreditsCredit-Scheduled Delay-Bounded
Congestion Control for Datacenters
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Figure 9: Credit queue capacity vs. Utilization
(a) Scenario
(b) Link Utilization (normalized
by max data rate)
Figure 10: Utilization with parking-lot topology
(or ramp-up time). To understand the trade-offs, we measure the
convergence time of a new flow competing with an existing flow by
32 ×max_rate to max_rate while winit
varying the initial rate from 1
is fixed to 0.5. Figure 8 (a) shows that as the initial rate decreases,
the convergence time increases from 2 RTTs to 14 RTTs. Figure 8
(b) shows the amount of credit waste with single packet flows in
an idle network whose RTT is 100 µs. As the initial rate decreases,
the amount of wasted credits decline as expected. Note that winit
does not affect the credit waste for a single packet flow. We further
analyze the trade-offs with realistic workloads later in Section 6.3.
Target loss: One important role of the target loss factor is to compen-
sate for subtle timing mismatch. For example, a receiver could send
N credits over prior RTT and only receives N-1 data packets due to
subtle timing mismatch, but we do not want our feedback control
to overreact to such cases. While targeting some loss may appear
to introduce more credit waste, it usually does not because credits
delivered to the sender will be used if data is available. However,
with multiple bottlenecks, setting the target loss rate higher risks
under-utilization. Thus, we use a small target loss rate of 10%.
Credit queue capacity: The size of the credit buffer affects utiliza-
tion, convergence time, and data queue occupancy. A large credit
buffer hurts fast convergence as it delays feedback and increases the
delay spread and queue occupancy for data packets. However, too
small credit buffer can hurt utilization because credit packets can
arrive in bursts across different ports and get dropped. We quantify
how much credit queue is necessary to ensure high utilization. For
this, we conduct an experiment with our feedback control while vary-
ing the number of flows from 2 to 32 which all arrive from different
physical ports and depart through the same port. We then vary the
credit queue size from 1 to 32 packets in power of 2 and measure
the corresponding under-utilization in Figure 9. It shows that credit
buffer size of eight is sufficient across the different number of flows.
Hence, we set the credit buffer to eight for rest of the paper.
(a) Scenario
(b) Throughput of Flow 0
Figure 11: Fairness in multi-bottleneck topology
3.4 Effect of feedback control
The feedback loop significantly improves utilization with multiple
bottlenecks by reducing wasted credits for long flows when flows
traverse multiple hops. Here, we quantify this using the topology of
Figure 10 (a). We increase the number of bottleneck links, N , from
one to six. Figure 10 (b) shows the utilization of the link with the
lowest utilization. To isolate the loss due to multiple bottlenecks, we
report the utilization normalized to the maximum data rate excluding
the credit packets. Our feedback control achieves high utilization
even with multiple bottlenecks. With two bottlenecks, it achieves
98.0% utilization, an improvement from 83.3% the naïve case with-
out feedback control; and with six bottlenecks, it achieves 97.8%
utilization, an improvement from 60% in the naïve case.
Our feedback control also improves fairness with multiple bot-
tlenecks. To demonstrate this, we use the multi-bottleneck scenario
in Figure 11 (a) and vary the number of flows (N) that use Link 1.
We then measure the throughput of Flow 0. Note, all flows are long-
running flows and Flow 1 through N experience multiple bottlenecks
but Flow 0 only has a single bottleneck. Figure 11 (b) shows the
throughput of Flow 0 as the number of competing flows increases.
With ideal max-min fairness, Flow 0 should get 1/(N+1) of the link
capacity (red line in Figure 11 (b)). ExpressPass follows the max-min
fair-share closely until four flows. But, as the number of flows in-
creases, it shows a small gap. There are many factors that contribute
to fairness. One important factor is when there is less than one credit
packets per RTT. ExpressPass can still achieve good utilization and
bounded queue, but fairness deteriorates in such cases.
4 ANALYSIS OF EXPRESSPASS
We now provide analysis of our feedback control. We prove that it
converges to fair-share and analyze stability using a discrete model.
For analysis, we assume N (> 1) flows are sharing a single bottleneck
link and their update periods are synchronized as in many other
studies [4, 46, 59].
Stability analysis: Let us denote the credit sending rate of flow n
at time t by Rn (t ), and its aggressiveness factor w by wn (t ). The
maximum credit sending rate corresponding to the link capacity is de-
noted as max_rate. We define C as C = max_rate· (1 + target_loss),
which is the maximum credit sending rate for a flow.
sending rate ((cid:80)N
Without loss of generality, assume the bottleneck link is under-
utilized. Then, credit_loss will be zero for all flows, and they will
increase their credit sending rates. Eventually, the aggregate credit
i =1 Ri (t )) will exceed the maximum credit sending
rate, C. In the next update period t = t0 , it will reach C according to
the decreasing phase (Algorithm 1 line 12) that reduces cur_rate by
0%1%2%3%4%5%6%0510152025302 flows4 flows8 flows16 flows32 flowsNormalized Under-utilization(by max data rate)Credit Queue Capacity (# credit packets)…Link 1Link NFlow 0Flow 1Flow N…100%98%98%98%98%98%0%25%50%75%100%123456UtilizationNumber of bottlenecksWith feedback loopNaïve approach. . .Link 1Link 2Link 3SenderFlow 0Flow 1Flow N0123451416642561024Number of FlowsThroughputNaiveapproachw/ feedback loopMax-min idealGbpsSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Inho Cho, Keon Jang, and Dongsu Han
Rn (tc + 2k + 1) → C
N (1 + (N − 1)wmin )
(6)
We have shown that regardless of the initial credit sending rate
and the initial aggressiveness factor wn, Rn (t + 2k ) converges to
C/N and Rn (t + 2k + 1) is bounded. Thus, our feedback control is
stable. Below, we also show the difference between Rn (t + 2k ) and
Rn (t + 2k + 1) is bounded.
Convergence to fairness: Now, we show the bandwidth allocation
Bn converges to fairness. From Equations 5 and 6, we see that all
flows have the same credit sending rate. Therefore, the bandwidth