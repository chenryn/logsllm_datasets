3.1 Plagiarism Detectors
Plagiarism detection is the process of detecting that portions within a work are not orig-
inal to the author of that work. One of the most common uses of software plagiarism
detection is to detect plagiarism in student submissions in programming classes (e.g.,
Moss [4], Plaque [24], and YAP [26]). Software plagiarism detection and malware clus-
tering are related to one another in that they both attempt to detect some degree of sim-
ilarity in software programs among a large number of instances. However, due to the
uniqueness of malware samples compared to software programs in general (e.g., in us-
ing privileged system resources) and due to the degree of obfuscation typically applied
to malware instances, we did not expect plagiarism detectors to produce good results
when clustering malware samples.
Here we focus on three plagiarism detectors that monitor dynamic executions of
a program. We do not include those applying static analysis techniques as they are
obviously not suitable for analyzing (potentially packed) malware instances.
– APISeq: This detector, proposed by Tamada et al. [21], computes the similarity
of the sequences of API calls produced by two programs to determine if one is
plagiarized from the other. Similarity is measured by using string matching tools
such as diff and CCFinder [12].
– SYS3Gram: In this detector, due to Wang et al. [23], short sequences (speciﬁcally,
triples) of system calls are used as “birthmarks” of programs. Similarity is mea-
sured as the Jacaard similarity of the birthmarks of the programs being compared,
i.e., as the ratio between the sizes of two sets: (i) the intersection of the birthmarks
from the two programs, and (ii) the union of the birthmarks from the two programs.
– API3Gram: We use the same idea as in SYS3Gram and apply it to API calls to
obtain this plagiarism detector.
We emphasize that the features on which these algorithms detect plagiarism are distinct
from those employed by BCHKK-algo. Generally, the features adopted in BCHKK-algo
are the operating system objects accessed by a malware instance, the operations that
244
P. Li et al.
were performed on the objects, and data ﬂows between accesses to objects. In contrast,
the features utilized by the plagiarism detectors we adopted here are system/API call
sequences (without speciﬁed argument values).
3.2 Results
We implemented these three plagiarism detectors by following the descriptions in the
corresponding papers and then applied the detectors to BCHKK-data (instances used
by Bayer et al. [6] on which multiple anti-virus tools agree). More speciﬁcally, each de-
tection technique produced a distance matrix; we then used single-linkage hierarchical
clustering, as is used by BCHKK-algo, to build a clustering C, stopping the hierarchical
clustering at the point that maximizes the p-value of a chi-squared test between the dis-
tribution of sizes of the clusters in C and the cluster-size distribution that BCHKK-algo
induced on BCHKK-data.1 We then evaluated the resulting clustering C by calculating
the precision and recall with respect to a reference clustering D that is one of
– AV: clustering produced by multiple anti-virus tools, i.e., D in the evaluation clus-
– BCHKK-algo: clustering produced by the technique of Bayer et al., i.e., C in the
tering (“ground truth”) in Bayer et al.’s paper [6];
evaluation in Bayer et al.’s paper [6].
To make a fair comparison, the three plagiarism detectors and BCHKK-algo obtain
system information (e.g., API call, system call, and system object information) from
the same dynamic traces produced by Anubis [7]. Results of the precision and recall are
shown in Table 1.
Table 1. Applying plagiarism detectors and malware clustering on BCHKK-data
C
BCHKK-algo
APISeq
API3Gram
SYS3Gram
APISeq
API3Gram
SYS3Gram
D
AV
BCHKK-algo
prec(C,D) recall(C,D) F-measure(C,D)
0.984
0.965
0.978
0.982
0.988
0.989
0.988
0.930
0.922
0.927
0.938
0.939
0.941
0.938
0.956
0.943
0.952
0.960
0.963
0.964
0.963
One set of experiments, shown where D is set to the clustering results of BCHKK-
algo in Table 1, compares these plagiarism detectors with BCHKK-algo directly. The
high (especially) precisions and recalls show that the clusterings produced by these
plagiarism detectors are very similar to that produced by BCHKK-algo. A second set of
1 More speciﬁcally, this chi-squared test was performed between the cluster-size distribution of
C and a parameterized distribution that best ﬁt the cluster-size distribution that BCHKK-algo
induced on BCHKK-data. The parameterized distribution was Weibull with shape parameter
k = 0.4492 and scale parameter λ = 5.1084 (p-value = 0.8763).
On Challenges in Evaluating Malware Clustering
245
experiments, shown where D is set to AV, compares the precisions and recalls of all four
techniques to the “ground truth” clustering of BCHKK-data. It is perhaps surprising that
SYS3Gram performed as well as it did, since a system-call-based malware clustering
algorithm [13] tested by Bayer et al. performed relatively poorly; the difference may
arise because the tested clustering algorithm employs system-call arguments, whereas
SYS3Gram does not (and so is immune to their manipulation). That issue aside, we
believe that the high precisions and recalls reported in Table 1 provide support for the
conjecture that the malware instances in the BCHKK-data dataset are likely relatively
simple ones to cluster, since plagiarism detectors, which are designed without attention
to the speciﬁc challenges presented by malware, also perform very well on them.
4 Replicating Our Analysis on a New Dataset
Emboldened by the results in Section 3, we decided to attempt to replicate the anal-
ysis of the previous section on a new dataset. Our goal was to see if another analysis
would also support the notion that selecting malware instances for which ground-truth
evidence is inferred by “voting” by anti-virus tools yields a ground-truth dataset that
all the tools we considered (BCHKK-algo and plagiarism detectors alike) could cluster
well.
4.1 The New Dataset and BCHKK-algo Clustering
To obtain another dataset, we randomly chose 5, 121 instances from the collection of
malware instances from VX heavens [3]. We selected the number of instances to be
roughly twice the 2, 568 instances in BCHKK-data. We submitted this set of instances
to Bayer et al., who kindly processed these instances using Anubis and then applied
BCHKK-algo to the resulting execution traces and returned to us the corresponding
distance matrix. This distance matrix covered 4, 234 of the 5, 121 samples; Anubis had
presumably failed to produce meaningful execution traces for the remainder.
In order to apply the plagiarism detectors implemented in Section 3 to this data, we
needed to obtain the information that each of those techniques requires, speciﬁcally the
sequences of system calls and API calls for each malware instance. As mentioned in
Section 3, we obtained this information for the BCHKK-data dataset via Anubis; more
speciﬁcally, it was already available in the BCHKK-data data ﬁles that those authors
provided to us. After submitting this new dataset to the Anubis web interface, however,
we realized that this information is not kept in the Anubis output by default. Given that
obtaining it would then require additional imposition on the Anubis operators to cus-
tomize its output and then re-submitting the dataset to obtain analysis results (a lengthy
process), we decided to seek out a method of extracting this information locally. For
this purpose, we turned to an alternative tool that we could employ locally to gener-
ate API call traces from the malware instances, namely CWSandbox [25]. CWSandbox
successfully processed (generated non-empty API call traces) for 4, 468 of the 5, 121
samples, including 3, 841 of the 4, 234 for which we had results for the BCHKK-algo
algorithm.
246
P. Li et al.
We then scanned each of these 3, 841 instances with four anti-virus programs (Ac-
tivescan 2.0, Nod32 update 4956, Avira 7.10.06.140 and Kaspersky 6.0.2.690). Ana-
lyzing the results from these anti-virus programs, we ﬁnally obtained 1, 114 malware
instances for which the four anti-virus programs reported the same family for each;
we denote these 1, 114 as VXH-data in the remainder of this paper. More speciﬁcally,
each instance is given a label (e.g, Win32.Acidoor.b, BDS/Acidoor.B) when scanned by
an anti-virus program. The family name is the generalized label extracted from the in-
stance label based on the portion that is intended to be human-readable (e.g., the labels
listed would be in the “Acidoor” family). We deﬁned a reference clustering D for this
dataset so that two instances are in the same cluster D ∈ D if and only if all of the four
anti-virus programs concurred that these instances are in the same family.2 Our method
for assembling the reference clustering for VXH-data is similar to that used to obtain
the reference clustering of BCHKK-data [6], but is more conservative.3
We obtained the BCHKK-algo clustering of VXH-data by applying single linkage
hierarchical clustering to the subset of the distance matrix provided by Bayer et al.
corresponding to these instances. In this clustering step, we used the same parameters
as in the original paper [6]. To ensure a fair comparison with other alternatives, we
conﬁrmed that this clustering offered the best F-measure value relative to the reference
VXH-data clustering based on the anti-virus classiﬁcations, in comparison to stopping
the hierarchical clustering at selected points sooner or later.
4.2 Validation on BCHKK-Data
As discussed above, we resorted to a new tool, CWSandbox (vs. Anubis), to extract
API call sequences for VXH-data. In order to gain conﬁdence that this change would
not greatly inﬂuence our results, we ﬁrst performed a validation test, speciﬁcally to
see whether our plagiarism detectors would perform comparably on the BCHKK-data
dataset when processed using CWSandbox. In the validation test, we submitted
BCHKK-data to CWSandbox to obtain execution traces for each instance. Out of the
2, 658 instances in BCHKK-data, CWSandbox successfully produced traces for 2, 099
of them. Comparing the API3Gram and APISeq clusterings on these 2, 099 samples,
ﬁrst with reference clustering AV and then with the clustering produced using BCHKK-
algo (which, again, uses Anubis) as reference, yields the results in Table 2. Note that
due to the elimination of some instances, the reference clusterings have fewer clus-
ters than before (e.g., AV now has 68 families instead of 84 originally). Also note that
SYS3Gram results are missing in Table 2 since CWSandbox does not provide system
call information. However, high F-measure values for the other comparisons suggest
that our plagiarism detectors still work reasonably well using CWSandbox outputs.
2 The VX heavens labels for malware instances are the same as Kaspersky’s, suggesting this is
the anti-virus engine they used to label.
3 The method by which Bayer et al. selected BCHKK-data and produced a reference cluster-
ing for it was only sketched in their paper [6], but their clariﬁcations enabled us to perform a
comparable data selection and reference clustering process, starting from the 3, 841 instances
from VX heavens successfully processed by both CWSandbox and the BCHKK-algo algo-
rithm (based on Anubis). This process retained a superset of the 1,114 instances in VXH-data
and produced a clustering of which every cluster of VXH-data is a subset of a unique cluster.
On Challenges in Evaluating Malware Clustering
247
Table 2. Applying plagiarism detectors and malware clustering on BCHKK-data. API3Gram and
APISeq are based on CWSandbox traces.
C
API3Gram
APISeq
API3Gram
APISeq
D
AV
BCHKK-algo
prec(C,D) recall(C,D) F-measure(C,D)
0.948
0.958
0.921
0.937
0.918
0.934
0.931
0.939
0.933
0.946
0.926
0.938
4.3 Results on VXH-Data
In Section 4.1 we described how we assembled the VXH-data dataset and applied
BCHKK-algo and the anti-virus tools to cluster it. We now compare the results of the
four clustering techniques run on VXH-data: AV from the anti-virus tools, API3Gram
(based on CWSandbox), APISeq (based on CWSandbox) and BCHKK-algo (based on
Anubis). Results are shown in Table 3. These results again show that the plagiarism
detectors produce comparable clustering results to BCHKK-algo when AV is the refer-
ence, offering generally greater precision, worse recall, and a similar F-measure.
Table 3. Applying plagiarism detectors and malware clustering on VXH-data
prec(C,D) recall(C,D) F-measure(C,D)
D
C
BCHKK-algo
API3Gram
APISeq
API3Gram
APISeq
AV
BCHKK-algo
0.604
0.788
0.704
0.790
0.770
0.659
0.502
0.536
0.826
0.798
0.630
0.613
0.609
0.808
0.784
Surprisingly, however, these measures indicate that both BCHKK-algo and our pla-
giarism detectors perform more poorly on VXH-data than they did on BCHKK-data.
On the face of it, the results in Table 3 do not support the conjecture of Section 3,
i.e., that determining a reference clustering of malware instances based on the concur-
rence of anti-virus engines might bias the reference clustering toward easy-to-cluster
instances. After all, were this the case, we would think that some method (if not all
methods) would do well when AV is used as the reference clustering. Instead, it may
simply be the case that the plagiarism detectors and malware clustering tools leverage
features for clustering that are more prevalent in BCHKK-data than in VXH-data. In
that case, one might thus conclude that these features are not sufﬁciently reliable for
use across a broad range of malware.
Of course, the results of this section must be taken as a bit more speculative, owing
to the different systems (CWSandbox and Anubis) from which the malware traces were
gathered before being consumed by the clustering techniques we consider. It is true that
there is substantial variability in the length and composition of the API sequences gath-
ered by the different tools, in some cases. For example, Figure 1 shows the CDFs of the
248
P. Li et al.
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
F
D
C
0
100
101
Anubis/BCHKK−data
CWSandbox/VXH−data
CWSandbox/BCHKK−data
102
103
Length of API call sequence
104
105
Fig. 1. Lengths of API call sequences extracted from BCHKK-data or VXH-data datasets using
CWSandbox or Anubis. Note that x-axis is log-scale.
API call sequence lengths elicited by the different tools. As can be seen in Figure 1, no
tool was uniformly better than the other in extracting long API call sequences, though
it is apparent that the sequences they induced are very different in length.
Another viewpoint is given in Figure 2, which shows the fraction of malware in-
stances
in each dataset in which certain activities are present. While some noteworthy dif-
ferences exist, particularly in behaviors related to network activity, it is evident that
both tools elicited a range of activities from large portions of the malware datasets.
We suspect that some of the differences in frequencies of network activities (partic-
ularly “send data” and “receive data”) result from the dearth of other malware in-
stances with which to communicate at the time the malware was run in CWSandbox.
Again, and despite these differences, our validation tests reported in Table 2 suggest
that the sequences induced by each tool are similarly effective in supporting clustering
of BCHKK-data.
Activity
Searched Strings
BCHKK-data BCHKK-data VXH-data
(Anubis)
(CWSandbox) (CWSandbox)
create new process
open reg key
query reg value
create reg key
set reg value
create ﬁle
send ICMP packet
try to connect
found no host
send data
receive data
“CreateProcess”
“RegOpenKey”
“RegQueryValue”
“RegCreateKey”
“RegSetValue”
“CreateFile”
“IcmpSendEcho”
“connect”, “WSASocket”
“WSAHOST NOT FOUND”
“AFD SEND”, “socket send”
“AFD RECV”, “socket recv”
100%
100%
100%
98.70%
98.30%
100%
82.10%
85.10%
N/A
83.10%
83.20%