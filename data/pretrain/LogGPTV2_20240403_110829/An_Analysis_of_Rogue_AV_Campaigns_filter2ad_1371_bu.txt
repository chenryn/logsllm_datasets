### 3.1 Plagiarism Detectors
Plagiarism detection is the process of identifying unoriginal content within a work. Commonly, software plagiarism detection tools are used to identify plagiarism in student submissions for programming classes (e.g., Moss [4], Plaque [24], and YAP [26]). Both software plagiarism detection and malware clustering aim to detect similarities among a large number of software instances. However, due to the unique characteristics of malware, such as the use of privileged system resources and the high degree of obfuscation, we did not expect plagiarism detectors to perform well when applied to malware samples.

In this section, we focus on three dynamic execution-based plagiarism detectors, excluding those that use static analysis, as they are unsuitable for analyzing potentially packed malware.

- **APISeq**: Proposed by Tamada et al. [21], this detector computes the similarity between sequences of API calls generated by two programs. The similarity is measured using string matching tools like `diff` and CCFinder [12].

- **SYS3Gram**: Developed by Wang et al. [23], this detector uses short sequences (triples) of system calls as "birthmarks" for programs. Similarity is determined by the Jaccard similarity coefficient, which is the ratio of the intersection to the union of the birthmarks from the two programs.

- **API3Gram**: This detector applies the same concept as SYS3Gram but to sequences of API calls instead of system calls.

The features used by these algorithms (system/API call sequences without specified argument values) differ from those used by BCHKK-algo, which focuses on operating system objects, operations performed on them, and data flows between object accesses.

### 3.2 Results
We implemented the three plagiarism detectors based on the descriptions in their respective papers and applied them to the BCHKK-data set (malware instances agreed upon by multiple antivirus tools). Each detection technique produced a distance matrix, which we then used with single-linkage hierarchical clustering, stopping at the point that maximized the p-value of a chi-squared test between the cluster size distribution and that induced by BCHKK-algo. We evaluated the resulting clustering by calculating precision and recall with respect to a reference clustering D, which could be either:

- **AV**: Clustering produced by multiple antivirus tools.
- **BCHKK-algo**: Clustering produced by the technique of Bayer et al. [6].

To ensure a fair comparison, all techniques obtained system information (API calls, system calls, and system object information) from the same dynamic traces produced by Anubis [7]. The results are shown in Table 1.

| **C** | **D** | **Precision (C, D)** | **Recall (C, D)** | **F-measure (C, D)** |
|-------|-------|----------------------|-------------------|----------------------|
| BCHKK-algo | AV | 0.984 | 0.965 | 0.978 |
| APISeq | AV | 0.982 | 0.930 | 0.956 |
| API3Gram | AV | 0.988 | 0.922 | 0.943 |
| SYS3Gram | AV | 0.989 | 0.927 | 0.952 |
| BCHKK-algo | BCHKK-algo | 0.988 | 0.938 | 0.960 |
| APISeq | BCHKK-algo | 0.939 | 0.963 | 0.946 |
| API3Gram | BCHKK-algo | 0.941 | 0.964 | 0.938 |
| SYS3Gram | BCHKK-algo | 0.938 | 0.963 | 0.933 |

One set of experiments compares the plagiarism detectors directly with BCHKK-algo, showing high precision and recall, indicating similar clustering results. Another set of experiments compares the precisions and recalls of all four techniques to the "ground truth" clustering (AV). Surprisingly, SYS3Gram performed well, possibly because it does not use system call arguments, making it immune to their manipulation. The high precisions and recalls suggest that the malware instances in the BCHKK-data set are relatively simple to cluster.

### 4 Replicating Our Analysis on a New Dataset
Encouraged by the results in Section 3, we decided to replicate the analysis on a new dataset. Our goal was to determine if another analysis would also support the idea that selecting malware instances based on "voting" by antivirus tools yields a ground-truth dataset that can be clustered well by both BCHKK-algo and plagiarism detectors.

### 4.1 The New Dataset and BCHKK-algo Clustering
We randomly selected 5,121 malware instances from VX Heavens [3], roughly twice the number in BCHKK-data. We submitted these instances to Bayer et al., who processed them using Anubis and returned the corresponding distance matrix, covering 4,234 of the 5,121 samples.

To apply the plagiarism detectors, we needed the sequences of system calls and API calls for each instance. Since Anubis did not provide this information by default, we used CWSandbox [25] to generate API call traces. CWSandbox successfully processed 4,468 of the 5,121 samples, including 3,841 of the 4,234 for which we had BCHKK-algo results.

We scanned these 3,841 instances with four antivirus programs (Activescan 2.0, Nod32 update 4956, Avira 7.10.06.140, and Kaspersky 6.0.2.690). We obtained 1,114 instances for which all four antivirus programs reported the same family, denoted as VXH-data. The reference clustering D for this dataset was defined such that two instances are in the same cluster if all four antivirus programs agree on their family.

We obtained the BCHKK-algo clustering of VXH-data by applying single linkage hierarchical clustering to the subset of the distance matrix provided by Bayer et al. We confirmed that this clustering offered the best F-measure value relative to the reference VXH-data clustering.

### 4.2 Validation on BCHKK-Data
To validate the use of CWSandbox, we submitted BCHKK-data to CWSandbox to obtain execution traces. Out of 2,658 instances, CWSandbox successfully produced traces for 2,099. We compared the API3Gram and APISeq clusterings on these 2,099 samples, first with reference clustering AV and then with the clustering produced using BCHKK-algo. The results are shown in Table 2.

| **C** | **D** | **Precision (C, D)** | **Recall (C, D)** | **F-measure (C, D)** |
|-------|-------|----------------------|-------------------|----------------------|
| API3Gram | AV | 0.948 | 0.958 | 0.931 |
| APISeq | AV | 0.921 | 0.937 | 0.933 |
| API3Gram | BCHKK-algo | 0.918 | 0.934 | 0.926 |
| APISeq | BCHKK-algo | 0.931 | 0.939 | 0.938 |

High F-measure values suggest that our plagiarism detectors still work reasonably well using CWSandbox outputs.

### 4.3 Results on VXH-Data
We compared the results of the four clustering techniques run on VXH-data: AV from the antivirus tools, API3Gram (based on CWSandbox), APISeq (based on CWSandbox), and BCHKK-algo (based on Anubis). The results are shown in Table 3.

| **C** | **D** | **Precision (C, D)** | **Recall (C, D)** | **F-measure (C, D)** |
|-------|-------|----------------------|-------------------|----------------------|
| BCHKK-algo | AV | 0.604 | 0.502 | 0.609 |
| API3Gram | AV | 0.788 | 0.536 | 0.630 |
| APISeq | AV | 0.704 | 0.826 | 0.613 |
| BCHKK-algo | BCHKK-algo | 0.790 | 0.798 | 0.808 |
| API3Gram | BCHKK-algo | 0.770 | 0.630 | 0.784 |
| APISeq | BCHKK-algo | 0.659 | 0.613 | 0.609 |

Surprisingly, both BCHKK-algo and the plagiarism detectors performed more poorly on VXH-data than on BCHKK-data. This suggests that the features leveraged by these tools may be more prevalent in BCHKK-data, indicating that these features may not be sufficiently reliable for a broad range of malware.

The results must be interpreted with caution due to the different systems (CWSandbox and Anubis) used to gather malware traces. Figure 1 shows the cumulative distribution functions (CDFs) of the API call sequence lengths, and Figure 2 shows the fraction of malware instances in each dataset where certain activities are present. Despite some differences, our validation tests suggest that the sequences induced by each tool are similarly effective in supporting clustering of BCHKK-data.