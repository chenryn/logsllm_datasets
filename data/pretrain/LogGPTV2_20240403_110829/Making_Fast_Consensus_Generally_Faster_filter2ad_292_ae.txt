dependent commands in order to deliver fast. The meta-data
are shared between local threads, thus introducing contention
that can lead to poor CPU utilization (an evidence of that
is in Figure 4). The overhead of maintaining dependency
relations kicks in also when commands are sent through
the network because dependencies should be included in the
messages themselves. As a consequence of that, messages
are bigger and thus they require more time to be sent.
We further evaluated how consensus protocols scale when
the number of nodes in a deployment
is held constant,
and the CPU capacity of each node is increased from 4
164
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:37 UTC from IEEE Xplore.  Restrictions apply. 
(a) 5 nodes.
(b) 11 nodes.
(c) 49 nodes.
Figure 5. Latency vs. throughput plots, with 0% and 100% command locality for M 2P AXOS and EPaxos.
to 32 cores. This is relevant for the implementations of
Generalized Consensus (which include EPaxos) in order to
assess their ability to exploit parallelism in case of low or
no conﬂicts among commands. To this purpose, we ran our
benchmark on four classes of Amazon EC2 machines. Each
class increment represents a doubling of the number of CPU
cores, and an almost 2× increase in available RAM.
Figure 4 shows the result of this experiment on four
deployments of 11 nodes each. M 2P AXOS exhibits great
scalability up to 16 cores. Throughput still increases beyond
that, but at a lower rate, as other components of the sys-
tem become bottlenecked (more speciﬁcally, the networking
layer). Clearly this scalability is not exploited by single
leader algorithms. Also, EPaxos is not able to take advantage
of the additional local resources available because of the
cost of dependency management and graph processing,
both of which require synchronization among local threads.
M 2P AXOS does not require any local processing that gener-
ates contention among threads, therefore having more CPUs
increases also the parallel tasks accomplished per time unit.
In Figure 5 we report the latency vs. throughput plots for
several deployments (5, 11, and 49 nodes). For M 2P AXOS
and EPaxos we plot the results of running two workloads at
opposite sides of the locality spectrum where commands still
access one object. One workload has perfect locality (100%
local commands) and is the best case for M 2P AXOS, where
commands proposed by a node only conﬂict with commands
from the same node; the other workload has no locality (0%
local commands). Any other workload would fall between
these two limits. Multi-Paxos and Generalized Paxos are
not sensitive to locality, while M 2P AXOS handles non-
local commands by simply forwarding them to the node that
currently owns the requested object (see also Section IV-B).
In such a scenario, EPaxos can fail in delivering a transaction
fast due to the collection of conﬂicting dependencies during
its broadcast phase. For this reason, it breaks down up to
10% earlier in the workload with no locality.
(a) Throughput.
(b) Latency.
Figure 6.
(remote) command. The deployment consists of 3 and 11 nodes.
Performance varying the probability of proposing a non-local
given two conﬁgurations with 3 and 11 nodes, and by
varying the percentage of non-local commands with a ﬁner
granularity than that in Figure 5. Here the impact of the
forwarding mechanism of M 2P AXOS is evident. The perfor-
mance degradation is very small (on average 4%), whereas
other competitors already achieved their top performance,
thus changing the probability of issuing a local command
does not provide signiﬁcant performance improvement or
degradation, respectively.
In Figure 6 we show the performance of all competitors
The last tested scenario using the synthetic benchmark
165
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:37 UTC from IEEE Xplore.  Restrictions apply. 
Figure 7.
Throughput varying the fraction of
complex commands with 49 nodes. In parentheses
the number of possible objects per node.
(a) 0% of commands on a remote warehouse.
(b) 15% of commands on a remote warehouse.
Figure 8. Performance with TPC-C workload by varying the number of nodes up to 11.
is where commands are complex. We deﬁne complex com-
mands as the commands that access multiple objects, hence
potentially conﬂicting with commands from multiple nodes.
Speciﬁcally, in this experiment a complex command ac-
cesses one object in a set, called local-set, on which the
local node is likely to have the ownership, and one uniformly
distributed across all objects. In this conﬁguration, we ﬁxed
the number of nodes as 49 and we varied the size of local-
set. The results (in Figure 7) show a drop in throughput
as the fraction of complex commands is increased. The
drop rate and ﬁnal throughput all depend on the the size of
local-set because it affects the contention rate. Multi-Paxos
and Generalized Paxos are not affected by the presence of
complex commands. EPaxos exhibits a small reduction in
throughput as the percentage of complex commands nears
100%. However, M 2P AXOS is able to sustain the through-
put by even using almost 50% of complex commands, in
case the size of local-set is 1000.
It is important to notice that, when a node pi acquires the
ownership on an object l to propose commands accessing
l, it ﬁrst ﬁnalizes the decision of all pending and not yet
decided commands accessing l (see line 10 of Algorithm 4).
The cost of this mechanism is reﬂected on the results showed
in Figure 7, since undecided commands are often present in
case of a change of ownership due to the concurrency of the
Acquisition phase executed by pi on l, with the other phases
executed by other nodes on l. In particular, a node pj might
not have enough time to ﬁnalize the decision of a command
on l before pi acquires the ownership on l. This case is
equivalent to the case where pj actually crashes before the
ﬁnalization of a decision, and hence the results of Figure 7
also include the costs that would be paid in case of crashes.
One important observation, which is valid for both EPaxos
and Generalized Paxos, is that when complex commands are
deployed, messages on the network become much bigger
due to the presence of dependency relations to include.
It is worth mentioning that protocols like EPaxos have to
also include dependencies from other local threads that may
issue a conﬂicting command. M 2P AXOS does not suffer
from such a drawback because it does not rely on command
dependencies and local threads can proceed in parallel as
long as the node has the ownership on those objects.
B. TPC-C benchmark
In this evaluation study we included also a benchmark
that produces the same workload as TPC-C. We conﬁgured
it by deploying a total number of warehouses equal to
10*N (e.g., with 9 nodes we deployed 90 warehouses).
Following the benchmark speciﬁcation, we associated the
appropriate number of customers, districts, etc.
TPC-C has ﬁve transaction proﬁles, where each of them has
a set of indexes identifying the objects to access (e.g., the
warehouse Id). Those indexes corresponds to the payload
of the complex commands we issue. We deﬁne a warehouse
to be local to a node if its warehouse object and all the
objects related with it belong to the local-set of that node.
Figure 8 shows the performance by varying the chances
for a thread to broadcast a command on a local warehouse
(Figure 8(a)), rather than on a warehouse (Figure 8(b))
uniformly selected across all. According to the speciﬁcation
of TPC-C, even though the requested warehouse is the lo-
cal one, 15% of the payment transactions (a proﬁle of TPC-
C) can still access a customer of another warehouse.
We ﬁrst notice that the throughput of M 2P AXOS is less
than the one obtained before with the single-object command
cases because, with TPC-C, commands’ size is bigger.
Performance decreases further (by as much as 40%) when
we let the benchmark access a non-local warehouse for
the 15% of the cases. However, still M 2P AXOS provides a
throughput greater than 400k commands ordered per second
in the conﬁguration of Figure 8(a), and more than 250k
under the conﬁguration of Figure 8(b).
The closest competitor (but still 2.4× slower) is Multi-
Paxos. The reason is related to the difﬁculties experienced by
EPaxos (5.5× slower) on handling higher contention, which
leads the agreement phase to perform an additional ordering
166
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:37 UTC from IEEE Xplore.  Restrictions apply. 
phase after trying (and failing) to deliver fast. Multi-Paxos’s
performance is independent from the message composition
and the overall application contention because the total order
it produces does not take into account any conﬂict among
messages. As a result, it performs similar to Figure 6(a).
VIII. CONCLUSION
In this paper we presented M 2P AXOS, a scalable and
high-performance implementation of Generalized Consen-
sus. It decides sequences of commands with the optimal
cost of two communication delays in the case of partitionable
workload and with the minimum size of quorums achievable
+1,
for solving consensus in asynchronous systems, i.e.,
where N is the total number of nodes. The evaluation study
conﬁrms the effectiveness of the approach.
N
2
(cid:2)
(cid:3)
ACKNOWLEDGMENT
The authors thank Pierre Sutra and the anonymous re-
viewers for their useful comments. This work is supported
in part by US National Science Foundation under grant CNS-
1523558, and by US Air Force Ofﬁce of Scientiﬁc Research
under grant FA9550-15-1-0098.
REFERENCES
[1] L. Lamport, “The Part-time Parliament,” ACM Trans. Comput.
Syst., vol. 16, no. 2, pp. 133–169, 1998.
[2] B. Charron-Bost and A. Schiper, “Uniform Consensus is
Harder Than Consensus,” J. Algorithms, vol. 51, no. 1, pp.
15–37, 2004.
[3] J. C. Corbett et al., “Spanner: Google’s Globally Distributed
Database,” ACM Trans. Comput. Syst., vol. 31, no. 3, pp. 8:1–
8:22, 2013.
[4] S. Hirve, R. Palmieri, and B. Ravindran, “Archie: A Specula-
tive Replicated Transactional System,” in Middleware, 2014,
pp. 265–276.
[5] T. Kraska, G. Pang, M. J. Franklin, S. Madden, and A. Fekete,
“MDCC: Multi-data Center Consistency,” in EuroSys, 2013,
pp. 113–126.
[6] H. Mahmoud, F. Nawab, A. Pucher, D. Agrawal, and
A. El Abbadi, “Low-latency Multi-datacenter Databases Us-
ing Replicated Commit,” Proc. VLDB Endow., vol. 6, no. 9,
pp. 661–672, 2013.
[7] L. Lamport, “Paxos made simple,” ACM Sigact News, 2001.
[8] I. Moraru, D. G. Andersen, and M. Kaminsky, “There is More
Consensus in Egalitarian Parliaments,” in SOSP, 2013, pp.
358–372.
[9] Y. Mao, F. P. Junqueira, and K. Marzullo, “Mencius: Building
Efﬁcient Replicated State Machines for WANs,” in OSDI
2008, 2008, pp. 369–384.
[10] A. Turcu, S. Peluso, R. Palmieri, and B. Ravindran, “Be
General and Don’t Give Up Consistency in Geo-Replicated
Transactional Systems,” in OPODIS, 2014, pp. 33–48.
[11] L. Lamport, “Generalized Consensus and Paxos,” Microsoft
Research, Tech. Rep. MSR-TR-2005-33, March 2005.
[12] F. Pedone and A. Schiper, “Generic Broadcast,” in DISC,
1999, pp. 94–108.
[13] L. Lamport, “Fast paxos,” Distributed Computing, vol. 19,
no. 2, pp. 79–103, 2006.
[14] ——, “Future directions in distributed computing.” Springer-
Verlag, 2003, ch. Lower Bounds for Asynchronous Consen-
sus.
[15] J. Cowling and B. Liskov, “Granola: Low-overhead Dis-
tributed Transaction Coordination,” in USENIX ATC, 2012.
[16] D. Sciascia, F. Pedone, and F. Junqueira, “Scalable Deferred
Update Replication,” in DSN, 2012, pp. 1–12.
[17] S. Peluso, P. Romano, and F. Quaglia, “SCORe: A Scalable
One-Copy Serializable Partial Replication Protocol,” in Mid-
dleware, 2012, pp. 456–475.
[18] S. Peluso, P. Ruivo, P. Romano, F. Quaglia, and L. Rodrigues,
“When Scalability Meets Consistency: Genuine Multiversion
Update-Serializable Partial Data Replication,” in ICDCS,
2012, pp. 455–465.
[19] S. Peluso, A. Turcu, R. Palmieri, and B. Ravindran, “On
Exploiting Locality for Generalized Consensus,” in ICDCS,
2015, pp. 766–767.
[20] “Tpc-c benchmark,” http://www.tpc.org/tpcc/.
[21] L. Lamport, Specifying Systems: The TLA+ Language and
Addison-
Tools for Hardware and Software Engineers.
Wesley Longman Publishing Co., Inc., 2002.
[22] S. Peluso, A. Turcu, R. Palmieri, G. Losa, and B. Ravindran,
“Making Fast Consensus Generally Faster,” Virginia Tech,
Tech. Rep., 2016. [Online]. Available: http://www.hyﬂow.
org/pubs/peluso-M2PAXOS-TR.pdf
[23] P. Sutra and M. Shapiro, “Fast Genuine Generalized Consen-
sus,” in SRDS, 2011, pp. 255–264.
[24] D. Hendler, A. Naiman, S. Peluso, F. Quaglia, P. Romano, and
A. Suissa, “Exploiting Locality in Lease-Based Replicated
Transactional Memory via Task Migration,” in DISC, 2013,
pp. 121–133.
[25] R. Boichat, P. Dutta, and R. Guerraoui, “Asynchronous Leas-
ing,” in WORDS, 2002, pp. 180–187.
[26] C. Li, D. Porto, A. Clement, J. Gehrke, N. Preguic¸a, and
R. Rodrigues, “Making Geo-replicated Systems Fast As Pos-
sible, Consistent when Necessary,” in OSDI, 2012, pp. 265–
278.
[27] M. J. Fischer, N. A. Lynch, and M. S. Paterson, “Impossibility
of Distributed Consensus with One Faulty Process,” J. ACM,
vol. 32, no. 2, pp. 374–382, 1985.
[28] R. Guerraoui and A. Schiper, “Genuine Atomic Multicast in
Asynchronous Distributed Systems,” Theor. Comput. Sci., vol.
254, no. 1-2, pp. 297–316, 2001.
[29] R. Guerraoui and L. Rodrigues, Introduction to Reliable
Distributed Programming. Springer-Verlag New York, Inc.,
2006.
[30] F. Junqueira, Y. Mao, and K. Marzullo, “Classic Paxos vs.
Fast Paxos: Caveat Emptor,” in HotDep, 2007.
[31] R. Guerraoui, V. Kuncak, and G. Losa, “Speculative Lineariz-
ability,” in PLDI, 2012, pp. 55–66.
[32] “The go programming language.” http://golang.org/.
[33] T. Friedman and R. V. Renesse, “Packing Messages As a Tool
for Boosting the Performance of Total Ordering Protocls,” in
HPDC, 1997, pp. 233–242.
[34] B. Kemme, F. Pedone, G. Alonso, A. Schiper, and M. Wies-
mann, “Using optimistic atomic broadcast in transaction pro-
cessing systems,” IEEE Trans. Knowl. Data Eng., vol. 15,
no. 4, pp. 1018–1032, 2003.
167
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:37 UTC from IEEE Xplore.  Restrictions apply.