title:Randomized Last-Level Caches Are Still Vulnerable to Cache Side-Channel
Attacks! But We Can Fix It
author:Wei Song and
Boya Li and
Zihan Xue and
Zhenzhen Li and
Wenhao Wang and
Peng Liu
0
5
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
Randomized Last-Level Caches Are Still Vulnerable
to Cache Side-Channel Attacks! But We Can Fix It
Wei Song∗†, Boya Li∗†, Zihan Xue∗†, Zhenzhen Li∗†, Wenhao Wang∗†, Peng Liu‡
∗State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China
†School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
‡The Pennsylvania State University, University Park, USA
{songwei, liboya, xuezihan, lizhenzhen1, wangwenhao}@iie.ac.cn, PI:EMAIL
Abstract—Cache randomization has recently been revived as
a promising defense against conﬂict-based cache side-channel
attacks. As two of the latest implementations, CEASER-S and
ScatterCache both claim to thwart conﬂict-based cache side-
channel attacks using randomized skewed caches. Unfortunately,
our experiments show that an attacker can easily ﬁnd a usable
eviction set within the chosen remap period of CEASER-S and
increasing the number of partitions without dynamic remapping,
such as ScatterCache, cannot eliminate the threat. By quantita-
tively analyzing the access patterns left by various attacks in
the LLC, we have newly discovered several problems with the
hypotheses and implementations of randomized caches, which
are also overlooked by the research on conﬂict-based cache side-
channel attacks.
However, cache randomization is not a false hope and it is
an effective defense that should be widely adopted in future
processors. The newly discovered problems are corresponding
to ﬂaws associated with the existing implementation of cache
randomization and are ﬁxable. Several new defense ideas are
proposed in this paper. Our experiments show that all the newly
discovered problems are ﬁxed within the current performance
budget. We also argue that randomized set-associative caches
can be sufﬁciently strengthened and possess a better chance to
be actually adopted in commercial processors than their skewed
counterparts because they introduce less overhaul to the existing
cache structure.
I. INTRODUCTION
To reduce the latency of accessing memory, modern com-
puters adopt a multi-level cache hierarchy where the last-level
cache (LLC) is shared between all processing cores. Such
sharing improves the utilization efﬁciency of the LLC as it
can dynamically adapt its space allocation to the demand of
different cores. However, it also allows a malicious software
to trigger controlled conﬂicts in the LLC, such as evicting a
speciﬁc cache set with attackers’ data [1]–[3], to infer security-
critical information of a victim program. This type of conﬂict-
based cache side-channel attacks have already been utilized to
recover cryptographic keys [4], break the sandbox defense [5],
inject faults directly into the DRAM [6], and extract informa-
tion from the supposedly secure SGX enclaves [7].
Cache partitioning [8]–[10] used to be the only effective de-
fense against conﬂict-based cache side-channel attacks abusing
the LLC. It separates security-critical data from normal data
in the LLC; therefore, attackers cannot evict security-critical
data by triggering conﬂicts using normal data. However, cache
partitioning is ineffective when security-critical data cannot be
easily separated from normal data [11] or normal data become
the target [6]. It also reduces the autonomy of the LLC which
might in turn hurt performance for some applications [12].
Finally, cache partitioning relies on speciﬁc operating system
(OS) code to identify security-critical data, which means the
OS must be trusted.
As two of the latest
Recently, cache randomization [13]–[21] has been revived
as a promising defense. Instead of cache partitioning, cache
randomization randomizes the mapping from memory ad-
dresses to cache set indices. This forces attackers to slowly ﬁnd
eviction sets using search algorithms at run-time [3], [22]–[24]
rather than directly calculating cache set indices beforehand.
Even when eviction sets are found, attackers cannot tell which
cache sets are evicted by them. However, cache randomization
alone does not defeat conﬂict-based cache side-channel attacks
but only increases difﬁculty and latency [16]. For this reason,
dynamic remapping [16], [19] has been introduced to limit
the time window available to attackers and skewed cache [17]–
[19] has been proposed to further increase the attack difﬁculty.
implementations, CEASER-S [17]
and ScatterCache [18] both claim to thwart conﬂict-based
cache side-channel attacks using randomized skewed caches.
ScatterCache even argues that dynamic remapping might not
be necessary as the extra difﬁculty introduced by skewed
cache is hard enough. Unfortunately, our experiments show
that an attacker can easily ﬁnd a usable eviction set within
the chosen remap period of CEASER-S [17] and increasing
the number of partitions without dynamic remapping, such as
ScatterCache [18], cannot eliminate the threat. By quantita-
tively analyzing the access patterns left by various attacks in
the LLC, we have newly discovered several problems with the
hypotheses and implementations of randomized caches, which
are also overlooked by the research on conﬂict-based cache
side-channel attacks.
• The possibility of using cache ﬂush instructions in
conﬂict-based attacks has been overlooked. Our study
shows, if attackers ﬂush the eviction set after each probe,
partial congruent eviction sets can be repeatedly used to
drastically speed up attacks.
• The concept of minimal eviction set no longer applies to
randomized skewed caches. Any group of cache blocks
that can evict the target address with a reasonable prob-
ability should be considered as a usable eviction set.
© 2021, Wei Song. Under license to IEEE.
DOI 10.1109/SP40001.2021.00050
955
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
• Attackers do not have to use eviction sets with 99%
eviction rate. When ﬁnding such sets become too difﬁcult,
attackers will utilize eviction sets with low eviction rate
but possible to ﬁnd.
• Measuring the remap period by LLC accesses is ﬂawed,
since a signiﬁcant portion of all the cache accesses might
be ﬁltered by the private level-one (L1) or level-two (L2)
caches. The actual number of accesses observed by the
LLC is much smaller than the total number of cache
accesses. As a result, the remap period estimated by
CEASER-S [17] is over-optimistic.
However, cache randomization is not a false hope. As
researchers on the defense side, we strongly believe it is an
effective defense strategy that should be widely adopted in
future processors. The above-discovered problems are corre-
sponding to ﬂaws associated with the existing tactics towards
accomplishing the “cache randomization” strategy. We believe
that these problems are ﬁxable, and that ﬁxing these problems
will make the strategy signiﬁcantly more effective in defending
conﬂict-based cache side-channel attacks. In particular, several
new defense ideas are proposed in this paper:
• Measure the remap period by LLC evictions rather than
accesses because the probability of successfully ﬁnding
an eviction set is closely related to the number of evic-
tions allowed between remaps.
• Further reduce the period to stop attackers from ﬁnding
even small partially congruent eviction sets.
• Adopt ZCache-like [25] multi-step relocation to minimize
the number of cache blocks evicted during each remap.
• Promote the use of CEASER (randomized set-associative
cache) rather than skewed caches because CEASER
introduces less overhaul to the existing cache structure
than skewed caches and it can be made secure enough.
• A simple attack detection method to strengthen CEASER.
By utilizing these defense ideas, our experiments show that
all the newly discovered vulnerabilities of existing randomized
caches can be ﬁxed within the current performance budget
and the randomized set-associative caches can be made secure
enough with reasonable performance overhead.
This paper is organized as follows: Section II introduces the
necessary background information to understand this paper.
Section III formulates the problems we try to answer in this
paper. Section IV demonstrates the vulnerabilities of existing
randomized caches by experiments. Section V shows how we
can ﬁx the randomized skewed caches and Section VI presents
solutions to safely strengthen the randomized set-associative
caches. The performance overhead is analyzed in Section VII.
The limitations and related work are discussed in Section VIII.
Section IX ﬁnally concludes the paper.
II. BACKGROUND
A. Caches
Modern processors use caches to store recently or frequently
used data to reduce the memory access time. Most caches
adopt a set-associative structure [26] as shown in Fig. 1.
Fig. 1. A set-associative cache.
The cache space is divided into S cache sets and each set
contains W ways of cache blocks. Cache sets are addressed
by a cache set
index which is typically a subset of the
address bits shared by all cache blocks in the same set. If two
addresses are mapped to the same cache set, they are congruent
addresses [23]. When an address is accessed, the cache checks
whether there is a match (hit) in the corresponding cache set
by comparing tags. If no match is found (a miss), the cache
block is fetched and stored in the cache set for future use. The
speciﬁc position (way) to store this newly fetched cache block
is chosen by a replacement policy and the old block is evicted.
As a commonly used replacement policy, least-recently used
(LRU) [27] retains the recently accessed cache blocks.
Multiple levels of caches are normally hierarchically orga-
nized. A processing core might have one or two levels of
private caches (L1 and L2 caches) while all cores share a
large LLC. An inclusive relationship between private caches
and the LLC is usually adopted [26].1 When a cache block
is evicted from the LLC, it is also purged from all private
caches. A hardware managed coherence protocol ensures data
are correctly updated between caches.
B. Conﬂict-Based Cache Side-Channel Attacks
Conﬂict-based cache side-channel attacks [29] exploit the
fact that cache blocks in the same set are congruent. This
allows attackers to maliciously control the status of a target
cache set using a group of at least W congruent addresses
(cache blocks), namely an eviction set.
An attack normally occurs in two phases: preparation phase
when the attacker collects enough number of eviction sets,
and exploitation phase when the attacker infers sensitive
information from a victim by controlling the status of certain
cache sets using the collected eviction sets. Before cache
randomization is applied, collecting eviction sets is relatively
easy because attackers can deliberately construct an eviction
set using addresses having the same cache set index bits [3].
This becomes unfeasible when caches are randomized. The
exploitation phase normally contains numerous prime+probe
cycles [1]–[3]. In each cycle, the attacker ﬁrst primes a target
set by ﬁlling it with cache blocks from a corresponding
eviction set. If there were cache blocks belonging to the victim,
they are likely evicted in the prime process. The attacker then
tricks the victim into running a program segment related to the
target cache set. If the victim indeed accesses data indexed to
1Some of the latest CPUs use non-inclusive LLCs but they might still suffer
from conﬂict-based attacks when the directory is inclsuive [24], [28].
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
956
way-0way-1way-(W-1)setdatatagAddressdatacache setspage oﬀsetcache set indextagFig. 2. A randomized skewed cache with two partitions over four cache ways.
the same cache set, it must have been fetched into the cache
set and one block of the eviction set is consequently evicted.
Finally the attacker probes the cache set by re-accessing all
blocks of the eviction set. If the total access latency is longer
than expected, the attacker learns that the victim should have
accessed the target cache set, which might further infer other
security-critical information.
C. Randomized Caches
The main objective of cache randomization is to deprive
attackers from usable eviction sets [16]–[18]. The latest im-
plementation of cache randomization is randomized skewed
caches [17], [18], while randomized set-associative caches [16]
can be considered as a special case with only one partition.
Fig. 2 presents a randomized skewed cache whose four cache
ways are evenly divided into two partitions independently
indexed. Instead of using a subset of address bits, the cache
set index is generated from a cipher taking the whole ad-
dress and a hardware managed key as inputs. Assuming the
encryption algorithm is unbroken and the key is not leaked,
the cache set
index is a random number unobservable to
attackers. Therefore, they can no longer construct eviction
sets simply by picking addresses but dynamically search for
congruent addresses through run-time experiments, which was
considered an intolerable long procedure [3], [22]–[24].
Another major beneﬁt of randomized skewed caches is the
reduced effectiveness of eviction sets. Two addresses are fully
congruent when they are mapped to the same sets in all
partitions while partially congruent when they are mapped to
the same sets in some but not all partitions. A group of W
addresses, where W is the number of ways, forms a fully
congruent eviction set only when all of the W addresses are
fully congruent. However, the probability that two random ad-
dresses are fully congruent in a K partitioned skewed cache is
SK , where S is the number of cache sets. This is an extremely
1
small probability even with a moderate K. Finding such a fully
congruent eviction set at run-time is unfeasible. Therefore,
attackers have no choice but to use partially congruent eviction
sets composed of partially congruent addresses. This has two
drawbacks [18], [30]: The number of addresses needed is
signiﬁcantly increased and the eviction of the target address
becomes a statistically random event.
D. Fast Algorithms for Searching Eviction Sets
At
the time when CEASER was proposed,
the fastest
algorithm [3], [22] for ﬁnding a minimal eviction set with
Fig. 3. Group elimination algorithm.
W addresses required O(N 2) cache accesses, where N is the
number of addresses randomly collected to form a very large
eviction set. As N is normally at the magnitude with the size
of the LLC (N ∼ S · W )
[24], O(N 2) cache accesses are
just too long for any practical attacks. Soon afterwards, three
fast search algorithms are proposed to drastically reduce the
number of accesses.
Group elimination (GE) is an optimization of the original
O(N 2) method [17], [23]. It still starts with a very large
eviction set of N random addresses but it tries to remove
multiple addresses in each cycle to quickly trim the set into
a minimal one. Fig. 3 illustrates such a cycle targeting an
LLC with four ways. The set of N addresses are divided into
W + 1 groups. Since a minimal eviction set contains only W
addresses (shadowed in red), there is at least one removable
group containing none of the W addresses. By sequentially
testing whether the set is still an eviction set without a certain
group,
the removable group is found and removed. Then
the whole process starts again taking the remaining W N
W +1
addresses as the input set until a minimal set is produced.
The whole process requires around O(W N ) cache accesses,
since N ∼ SW , O(W N ) = O(SW 2).
Conﬂict testing (CT) is a new algorithm ﬁrst proposed to
ﬁnd eviction sets in caches using random replacement [17].
Assuming an attacker has access to unlimited number of ran-
dom addresses, she can collect an eviction set by sequentially
testing each address whether it is congruent with the target
address. The target address is accessed ﬁrst to make it cached
in the LLC. Then a random address is accessed. If this address
is congruent with the target address, it might replace the target
address by a chance of 1
W thanks to the random replacement.
Overall, any random address might conﬂict with the target
address by a probability of
S·W . To test the occurrence of
such a conﬂict, the target address is re-accessed and timed.
If the latency is longer than expected, the random address is
considered congruent and put into the eviction set. The re-
accessing of the target address also starts the test for the next
random address. An eviction set is produced when enough
congruent addresses are collected. The overall number of
cache accesses is estimated around O(SW 2).
Note that this algorithm is also effective for permutation-
based replacement (such as LRU). Assuming the use of LRU,
the probability of causing a conﬂict with the target address
after accessing M random addresses is around:
1
P = 1 −
i  1
W−1i=0 M
Si
(1 −
1
S
)M−i
(1)
This is equivalent
least W conﬂicts in the
target cache set. The average M is around SW . Note that
re-accessing the target address is unlikely to cause an actual
to causing at
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
957
way-0way-1way-3datatagAddressdatapage oﬀset way-2CipherCipherk0k1Partition 0Partition 1N addressesW+1 groups addressesremove 1 group W+1WN(a) Internal states
(b) Ideal case
(c) Non-ideal case
(including LRU) is normally the same (O(SW )) [17] but it
approaches to O(SW 2) for LLCs using random replacement
for two reasons: One is the size of the prime set after pruning
is much smaller than the cache size SW , which reduces the
chance of ﬁnding congruent addresses in each round of search.
The other one is that, even if the target cache set is primed,
the number of congruent addresses found in each round of
test is signiﬁcantly less than W (only one in most cases). The
attacker has to do multiple rounds of tests in multiple rounds
of searches [30].
Fig. 4. Prime, prune and then test on a 3-set 4-way LLC using the LRU
replacement. (a) reveals the LLC internal states after prime and prune. (b)
demonstrates a success test in an ideal case where all cache accesses are
observed by the LLC. (c) shows a partial result in a non-ideal scenario when
some cache accesses are ﬁltered by the private caches and the observed orders
of prime+prune and test are different.
access to the LLC because the target address is always cached
in private caches (L1) until it is forcefully invalidated by a
conﬂict in the LLC. As a result, the LRU replacer’s internal
state is unchanged for most re-accessing of the target address.
To ﬁnd a minimal eviction set with W addresses, the number
of cache accesses is also around O(SW 2).
Prime, prune and then test (PPT) is an improved version
of the search algorithm exploiting the LRU replacement [17],
[30]. Let us consider an LLC using the LRU replacement. An
attacker ﬁrst accesses a large set of random addresses (prime
set) to prime the whole LLC2. Since self-conﬂicts would
naturally occur during the prime, a prune process is used to
remove conﬂicted addresses until all addresses remaining in
the prime set are simultaneously cached. Assuming Fig. 4a
reveals the internal states of an LLC after prime and prune,
the target cache set (set 1) is likely primed by the prime set.
In an ideal scenario, the order of cache accesses observed
by the LLC is the same order initiated by the attacker. As
shown in Fig. 4b, if the attacker makes a timed re-access of
the target address X and the prime set sequentially, all the
addresses with long latency (miss in the LLC) are congruent
with X and the number of them is just enough for an eviction
set. However, the order seen by the LLC is normally different
from the software order as many cache accesses are ﬁltered
by the private caches. In this scenario (Fig. 4c), the attacker
collects some but less than W congruent addresses. She has to
test again to force the order seen by the LLC equivalent to the