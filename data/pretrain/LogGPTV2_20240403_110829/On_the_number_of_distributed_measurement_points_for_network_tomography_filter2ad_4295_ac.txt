reduce the size of the beacon set as follows.
OBSERVATION 1. Let (u0, u1, . . . uk) be a path of high arity
nodes in the network such that ui is only connected to nodes ui−1
and ui+1 for 1 ≤ i ≤ k − 1. That is every message from a
node in the interior of the path traverses to the outside network
through either u0 or uk. Then the high arity nodes minus the set
{u1, . . . , uk−1} is also a beacon set.
Indeed, this is so as every message originated from a node in
the interior of the path traverses through one of u0 or uk with the
exception of a message destined to another node ui in the path, but
these edges can be tested by sending a message from u0 to uk along
the link (u0, u1). Thus u0 and uk sufﬁce for the beacon set and the
nodes in the interior of the path can be omitted from the set.
This substantially reduces the size of the beacon set. The num-
ber of AS’s providing transit on the Internet is in the order of 1500,
as reported by the Asia Paciﬁc Network Information Centre (AP-
NIC) on 5 May 2001, [6]. Notice that large multihomed AS’s are
likely to have more than one beacon node, even after applying the
observations above. For example, in the case of NSPs one would
expect roughly one beacon per each peering point (public or pri-
vate), plus other beacons for every cycle in the network. A quick
glance at publicly available maps of some of the major backbones2
show that most cycles in NSP networks contain at least one peering
point, provided we treat multiple direct links between two points
as a single bundle. In other words, the Internet is mostly a tree, ex-
cept for public peering points and very short redundant paths such
as FDDI rings and n × m fabric at PoPs or between core routers
and border routers3. Therefore we can further reduce the size of the
beacon set to approximately those nodes in the peering points.
Placing a beacon on each peering point and border router of a
multihomed AS is likely to be a good approximation of a beacon
set.
A set of routing data collected at Internap, as well as statistics
released by APNIC suggest that the total number of multihomed
networks is in the order of 10,000 to 20,000. Indeed a recent IETF
internet draft (work in progress) [34] suggests capping the total
2We studied AT&T, Intermedia, GTE and UUNET.
3Others have noted that the “almost a tree” nature of the Internet
makes some otherwise difﬁcult or intractable problems tractable.
In particular Xiao and Ni point out that OSPF can be extended to
much larger organizations if proper note is made of tree like re-
gions, which they term WARR’s [38].
number of mulithomed networks at 215 or approximately 32,000.
Hence we can expect that, anywhere between 1, 500 and 20, 000
beacon nodes sufﬁce to cover the entire network. This number,
while large, is much smaller than the total number of hosts, esti-
mated at 171 million as of January 2003, [24, 33], and well within
the economic reach of a large commercial Internet organization.
6. HIGH ARITY NODES
Thus far we have focused on the role of high arity nodes as part
of the infrastructure required to measure connectivity and, by ex-
tension, performance path characteristics on a network such as the
Internet. However because of their strategic placement a beacon
set plays also a key role in realizing a Resilient Overlay Network
(RON) [5] with a performance based routing policy.
The BGP protocol admits aggregation of paths which consider-
ably reduces the size of the routing tables. On the ﬂip side the
lack of explicit performance characteristics means that the path
chosen by BGP is not necessarily optimal latency-wise. This is
further compounded by deviations from the AS-hop metric due to
other considerations such as redundancy, cost-of-bandwidth and
even lack of visibility into the performance characteristics of the
network. Nevertheless, latency and packet loss are often driving
characteristics of user bandwidth requirements [28].
Some commercial organizations provide some level of perfor-
mance improvements on the public Internet over the standard BGP
routing heuristics using network route optimization (e.g. Internap,
Sockeye). These improvements are partially constrained by the im-
precise granularity of BGP routing policy and lack of control across
the network. Alternatively it is possible to deploy a performance
based routing protocol network overlaid on the public Internet us-
ing tunnelling across strategically placed nodes in the network.
Since the high arity nodes have access to all paths it is possible
to increase granularity of routing decisions by placing forwarding-
tunnel router nodes on that set.
A set of forwarding-tunnel router nodes is said to be an all-paths
set if every simple path from a node u to v can be realized with it.
CLAIM 5. The set of nodes of higher arity in a network form an
all-paths set.
PROOF. Recall that, from the proof of Claim 4, we know that
all bifurcation points on the network are of higher arity. Hence all
nodes where a routing policy can be implemented are part of the
set of nodes of higher arity, and the nodes absent from the higher
arity set are exactly those where paths are uniquely determined.
Now consider the default path RP(u, v) = u, u1, . . . , v from a
node u to a node v, and an alternative, desired path P (u, v) =
u, w1, w2, w3, . . . , v. Let wi be the ﬁrst node in which the two
paths differ. Hence wi is a node of higher arity, so we can send
a message from u to wi which by the monotonicity of the routing
policy will follow the path RP(u, v) up to node wi. At this point
since wi is of higher arity and thus part of the forwarding-tunnel
router nodes it can forward the message to v via wi+1.
Now we repeat the above process with the RP(wi+1, v) and the
path P (cid:2)
(wi+1, v) = wi+1, wi+2, . . . , v, determining the ﬁrst node
in which they differ, which, as before must also be a forwarding-
tunnel router node. After each iteration i we obtain a routing path
from u to a wji with ji > ji−1. Hence after a ﬁnite number of steps
this recursion must end and we have a set of forwarding-tunnel
router nodes realizing the path P (u, v).
Notice that as in the case of the beacon set, Observation 1 can also
be used to reduce the size of the high arity set while still maintain-
ing the all-paths set property.
7. CONCLUSIONS
We have shown that computing the minimum number of sbea-
cons required to test the status of every link is NP-hard. This num-
ber is also hard to approximate and potentially as large as one-third
of the nodes on an arbitrary network. An alternative heuristic tai-
lored for the topology of the public Internet using high arity nodes
is proposed. This would form a beacon set that can test for connec-
tivity on all relevant edges of the network. Furthermore such a set
has interesting properties that allow to further reduce the number
of required nodes. The high arity set can also be used as a forward
tunnelling set for all-paths routing on the public Internet, creating
a QoS based RON.
8. REFERENCES
[1] A. Adams, T. Bu, R. Caceres, N. Dufﬁeld, T.Friedman, J.
Horowitz, F. Lo Presti, S.B. Moon, V. Paxson, D. Towsley.
The Use of End-to-end Multicast Measurements for
Characterizing Internal Network Behavior, IEEE Comm.,
2000.
Impact of BGP Policy Changes on IP Trafﬁc. Technical
Memorandum, AT&T Labs Research.
[18] P. Francis, S. Jamin, V. Paxson, L. Zhang, D. F. Gryniewicz,
Y. Jin. An Architecture for a Global Internet Host Distance
Estimation Service. Proc. IEEE Conf. on Comp. Comm.
(INFOCOM), 1999, pp. 210-217
[19] M. Garey and D. Johnson. Computers and Intractability: a
Guide to the Theory of NP-Completeness. W.H.Freeman,
1979.
[20] R. Govindan, H. Tangmunarunkit. Heuristics for Internet
Map Discovery. Proc. IEEE Conf. on Comp. Comm.
(INFOCOM), 2000, pp. 1371-1380
[21] I. D. Graham, S. F. Donelly, S. Martin, J. Martens and J. G.
Cleary. Nonintrusive and accurate measurements of
unidirectional delay and delay variation in the Internet. Proc.
8th Internet Society Conf. (INET), 1998.
[22] R. G´uerin and A. Orda. QoS-based routing in networks with
inaccurate information. Proc. IEEE Conf. on Comp.
Comm.ons (INFOCOM), 1997.
[23] B. Halabi. Internet Routing Architectures. New Riders
[2] A. Adams, J. Mahdavi, M. Mathis, and V. Paxson, Creating a
Publishing, 1997.
Scalable Architecture for Internet Measurement. Proc. 8th
Internet Society Conf. (INET), 1998.
[3] A. Adams, and M. Mathis. A system for ﬂexible network
performance measurement. Proc. 10th INET Conf., 2000.
[4] M. Adler, T. Bu, R. K. Sitaraman, D. F. Towsley. Tree Layout
for Internal Network Characterizations in Multicast Networks.
Networked Group Comm., 2001, pp. 189-204.
[5] D. G. Andersen, H. Balakrishnan, M.F. Kaashoek, R. Morris.
Resilient Overlay Networks. Proc. 18th ACM Symp. on
Operating Syst. Princ., 2001.
[6] Asia Paciﬁc Network Information Centre (APNIC). Daily
BGP statistics. http://www.apnic.net/stats/bgp.
May 5, 2001.
[7] Cooperative Association for Internet Data Analysis (CAIDA).
The Skitter Project. http://www.caida.org/tools/
measurement/skitter/index.html, 2001.
[8] P. Barford, A. Bestavros, J. W. Byers, M. Crovella. On the
marginal utility of network topology measurements. Internet
Measurement Workshop, 2001, pp. 5-17.
[9] O. Bonaventure, S. De Cnodder, J. haas, B. Quoitin, R. White.
Controlling the redistribution of BGP routes. Internet draft.
[10] S. Branigan, H. Burch, B. Cheswick, and F. Wojcik. What
Can You Do with Traceroute? Internet Computing, vol. 5, no.
5, 2001, page 96ff.
[11] T. Bu, N. G. Dufﬁeld, F. Lo Presti, D. F. Towsley. Network
tomography on general topologies. ACM Int. Conf. on
Measurements and Modeling of Comp. Systems
(SIGMETRICS) 2002, pp. 21-30
[12] R. Caceres, N.G. Dufﬁeld, J. Horowitz, and D. Towsley.
Multicast-based inference of network internal loss
characteristics. IEEE Transactions on Information Theory,
v.45, n.7, 1999, pp. 2462-2480.
[13] Bill Cheswick, Hal Burch, and Steve Branigan. Mapping and
Visualizing the Internet. Proc. USENIX Technical Conf., 2000.
[14] K. Claffy, G. Miller and K. Thompson. The nature of the
beast: recent trafﬁc measurements from an Internet backbone.
Proc. 8th Internet Soc. Conf. (INET), 1998.
[15] K. Claffy, T.E. Monk and D. McRobb. Internet Tomography.
Nature, 7th January 1999.
[16] X. Deng. Short Term Behaviour of Ping Measurements. MSc
thesis, Univ. of Waikato, 1999.
[17] N. Feamster, J. Borkengham, J. Rexford. Controlling the
[24] Internet Software Consortium. http://www.isc.org/
ds/WWW-200301/index.html.
[25] S. Jamin, C. Jin, Y. Jin, D. Raz, Y. Shavitt, L. Zhang. On the
Placement of Internet Instrumentation. IEEE Conf. on Comp.
Comm. (INFOCOM), 2000, pp. 295-304.
[26] S. Kalidindi and M. J. Zekauskas. Surveyor: An
infrastructure for Internet performance measurements. Proc.
9th Internet Soc. Conf. (INET), 1999.
[27] G.R. Malan and F. Jahanian. An extensible probe
architecture for network protocol performance measurement.
Proc. ACM Conf. on Applications, Technologies Architectures
and Protocols for Comp. Comm. (SIGCOMM), 1998.
[28] A. Odlyzko. The current state and likely evolution of the
Internet Proc. Globecom’99, IEEE, pp. 1869-1875, 1999.
[29] V. Paxson. Measurements and Analysis of End-to-End
Internet Dynamics. PhD thesis, Univ. of Cal., Berkeley, 1997.
[30] V. Paxson. End-to-End routing behaviour in the Internet.
IEEE/ACM Transactions on Networking. 5, 601-618 (1997).
[31] V. Paxson, J. Mahdavi, A. Adams and M. Mathis, An
Architecture for Large-Scale Internet Measurement. IEEE
Comm., v.36, n.8, 1998, pp. 48-54.
[32] R. Raz and S. Safra. “A sub-constant error-probability
low-degree test, and sub-constant error-probability PCP
characterization of NP”, Proc. 29th ACM Symp. on the Theory
of Computing (STOC), 475-484, (1997).
[33] A. Scherrer. 127,781,000 Internet Hosts: How Matrix.net
gets its host counts. http://www.matrix.net/isr/
library/how_matrix_gets_its_host_counts.html,
2001, Access: May 2001.
[34] P. Savola. Multihoming using IPv6 addressing derived from
AS numbers. draft-savola-multi6-asn-pi-00.txt, work in
progress. IETF internet draft, January 2003.
[35] S. Seshan, M. Stemm, and R.H. Katz. SPAND: Share Passive
Network Performance Discovery. Proc. 1st Usenix Symp. on
Internet Technologies and Systems, 1997.
[36] R. Siamwalla, R. Sharma, and S. Keshav. Discovering
Internet Topology. Technical Report, Cornell Univ., July 1998.
[37] D. Towsley. Network tomography through to end-to-end
measurements. Abstract in Proc. 3rd Workshop on Algorithm
Engineering and Experiments (ALENEX), 2001.
[38] X. Xiao and L. M. Ni. Reducing routing table computation
cost in OSPF. Proc. 9th Internet Society Conf. (INET), 1999.