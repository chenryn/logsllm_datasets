Pages which have duplicate values in their query string are treated as
different pages:
  * http://www.example.com/?q=
  * http://www.example.com/?q=&q=
  * http://www.example.com/?q=&q=&q=
  * ...
If the first page has a link to the second and the second a link to the third,
`scrapy` enters an infinite loop, requesting each page in succession. I've put
a simple page here to test the issue. Using a CrawlSpider with a rule like
`Rule(SgmlLinkExtractor(), callback='parse_page', follow=True)` will cause
this infinite recursion. Here is a sample spider:
    from scrapy.contrib.spiders import CrawlSpider, Rule
    from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
    class LoopingSpider(CrawlSpider):
        name = "loop"
        start_urls = ["http://fcoelho.alwaysdata.net/scrapy/page.php"]
        allowed_domains = ["fcoelho.alwaysdata.net"]
        rules = (
            Rule(SgmlLinkExtractor(), callback='parse_page', follow=True),
        )
        def parse_page(self, response):
            print "Page: %s" % response.url
            return []
It seems like in the end the problem "arises" because
`scrapy.utils.url.canonicalize_url` only sorts the query string keys, but
don't remove duplicates. As far as I can tell, duplicates are "wrong", and
shouldn't be counted as normal web page behavior. Would it be sane to remove
arguments from the query string if their key is repeated?