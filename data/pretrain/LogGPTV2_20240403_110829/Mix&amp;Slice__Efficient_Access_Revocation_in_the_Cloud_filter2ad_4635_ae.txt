### Methodology and Implementation

Our technique is characterized by the `get` and `put_fragment` methods. We implemented two strategies: one using fragments as atomic, separate objects, and the other leveraging the DLO (Distributed Large Object) support provided by Swift.

#### Performance Analysis

**Figure 10** compares the time required for `get` requests with different numbers of fragments, assuming each fragment is mapped to a separate object. The lines in the figure represent different values for the number \( f \) of fragments (i.e., 1, 4, 16, 64, 256, and 1024). The performance is driven by network bandwidth and the overhead of managing each request. For small resources, the overhead of managing multiple requests per fragment dominates, while for larger resources, network bandwidth becomes the bottleneck. The performance profile of `put` requests, which upload the complete resource, is identical to that of `get` requests using a single fragment. The execution time of `put_fragment` requests grows linearly with the size of the fragment.

To determine the optimal number of fragments, we evaluated the system's behavior on a collection of 1000 objects. After each `put_fragment` request, a sequence of 50 `get` requests was executed on objects in the collection, all of the same size. **Figure 11** presents the results of these experiments. As objects become larger, the benefits of fragmentation in applying policy updates compensate for the overhead imposed on retrieval. The performance of the solution without our technique corresponds to the line with one fragment. Configurations using fragments show significantly higher throughput, especially for medium-sized objects. The optimal number of fragments depends on the resource size and requires considering the system configuration and expected workload.

A second set of experiments followed the same approach but used DLOs in Swift. The number of fragments still significantly impacts the performance of `get` requests because the server must generate internal mappings for the single request from the client and the multiple requests to storage nodes. Applying the same workload as in **Figure 10**, which interleaves `get` and `put_fragment` requests, produces the results in **Figure 12**. The use of DLOs shows a significant benefit in terms of cost.

### Ad-hoc Solution

An ad-hoc protocol can provide the full range of benefits of our approach. The protocol supports basic primitives for uploading (`put`) and downloading (`get`) a resource. When uploading the initial state of the resource, the `put` primitive provides a resource descriptor that includes:
- The identifier of the key \( k_0 \) used by the owner to encrypt the resource.
- The size of mini-blocks and the number of fragments, which determine the size of the macro-block.
- An array with an element for every fragment describing its version.

In addition to the `put` primitive, the server recognizes the `put_fragment` primitive, allowing the owner to update a fragment. Parameters for this primitive include the resource identifier, fragment content, fragment identifier, and version number. Both `put` and `put_fragment` primitives require user authentication.

The `get` primitive can return the resource, one macro-block at a time. The client can start decrypting macro-blocks immediately after a preliminary decryption with key \( k_i \) of the mini-blocks belonging to fragments at version \( i > 0 \). This allows the client to begin decryption before the entire download is complete. The response to a `get` request always provides the resource descriptor, including the version of each fragment. The `get` primitive also offers the option to retrieve only a specific portion of the resource.

### Logical to Physical Mapping

Attention must be given to the mapping of the logical structure to the physical representation of data. At the logical level, the resource is divided into fragments, and the content is represented by a sequence of macro-blocks. At the physical level, the resource can be stored as a collection of separate fragments or as a sequence of macro-blocks. There are also intermediate options, such as an interleaved representation of multiple fragments.

### Experiments on the Ad-hoc Solution

The advantage of a dedicated server is the ability to use an efficient protocol. The ad-hoc server makes the management of fragments more flexible and avoids the overhead associated with generating multiple independent `get` requests. However, using a large number of fragments can introduce non-negligible costs. In the extreme case where a large resource is managed with a single macro-block, the client must wait for the complete download to start decryption, involving many rounds. If only a portion of the resource is needed, the client must download the macro-blocks containing the portion of interest, which can lead to significant overhead if macro-blocks are large.

The optimal number of fragments depends on several features of the application domain. In the current technological scenario, an ad-hoc server can support a larger number of fragments than the Overlay solution, but extreme values cause inefficiencies. The implementation of the ad-hoc server must consider the mapping from the logical structure to its physical representation. In our experiments, we used an Amazon EC2 instance and its access to Elastic Block Storage. The operating system offers an interface to read and write physical blocks, typically a few KiB in size.

We compared different strategies for mapping the bidimensional logical structure to the physical structure. **Figure 13** illustrates the results obtained on a container with 1000 files, each 1 GiB in size. The horizontal axis denotes the number of shares of each macro-block. For a workload that interleaves a `get` request for every `put_fragment` request, the total cost is minimized with 256 fragments. Interestingly, the two extremes do not represent the best option. In these experiments, we measured the time required to access the data from storage. While the network is often the bottleneck, the performance benefit shown by the experiment can lead to a more efficient server implementation.

### Related Work

The idea of making the extraction of information content from an encrypted resource dependent on the availability of the complete resource was first explored by Rivest [16], who proposed the all-or-nothing transform (AONT). The AONT ensures that extracting a resource with missing bits requires attempting all possible combinations. The AONT can be followed by encryption to produce an all-or-nothing encryption schema. Rivest proposed the package transform, which realizes an AONT by applying a CTR mode using a random key \( k \). The ciphertext is then suffixed with the key \( k \) XOR-ed with a hash of the previous encrypted message blocks. This limits the ability to derive the encryption key if the message is modified. However, this technique fails in scenarios where the user has previously accessed the key and now needs to be prevented from doing so (i.e., revocation of privileges).

Most approaches for efficient secure deletion [5, 8] rely on the fact that the key is a digest and can be securely deleted by deleting the specific disk location that stores the information needed to derive the key. These approaches are used by commercial storage devices [17] and recent proposals have considered integrating them with flexible policies [5]. However, these approaches are not applicable in our scenario, where the encrypted resource is stored on a server that does not have access to the key, and it is the user who decrypts the resource.

Other approaches for enforcing access control in the cloud through encryption include attribute-based encryption (ABE) and selective encryption. ABE approaches (e.g., [11, 13, 15, 18]) ensure that the key used to protect a resource can be derived only by users satisfying certain conditions. However, these solutions have high evaluation costs and difficulties in supporting revocations. Selective encryption approaches (e.g., [6, 7, 12]) assume each resource is encrypted with a key known only to authorized users. Policy updates are either managed by the data owner, with considerable overhead, or delegated to the server through over-encryption. Although over-encryption guarantees prompt enforcement and good performance, it requires stronger trust assumptions on the server. Our technique, on the other hand, can be used even if the server is unaware of its adoption.

### Conclusions

We presented an approach for efficiently enforcing access revocation on encrypted resources stored at external providers. Our solution enables data owners to revoke access by overwriting a small portion of the resource, making it resilient against attacks by users maintaining copies of previously-used keys. Our implementation and experimental evaluation confirm the efficiency and effectiveness of our proposal, which shows orders of magnitude improvement in throughput compared to resource re-writing. It is also compatible with current cloud storage solutions, making it immediately applicable to many application domains.

### Acknowledgments

This work was supported by the European Commission within the H2020 under grant agreement 644579 (ESCUDO-CLOUD) and within the FP7 under grant agreement 312797 (ABC4EU).

### References

[1] E. Andreeva, A. Bogdanov, and B. Mennink. Towards understanding the known-key security of block ciphers. In Proc. of FSE, Hong Kong, Nov. 2014.

[2] M. Atallah, K. Frikken, and M. Blanton. Dynamic and efficient key management for access hierarchies. In Proc. of CCS, Alexandria, VA, USA, Nov. 2005.

[3] A. Biryukov and D. Khovratovich. PAEQ: Parallelizable permutation-based authenticated encryption. In Proc. of ISC, Hong Kong, China, Oct. 2014.

[4] A. Biryukov and D. Khovratovich. PAEQ reference v1. Technical report, CryptoLUX, University of Luxembourg, 2014.

[5] C. Cachin, K. Haralambiev, H. Hsiao, and A. Sorniotti. Policy-based secure deletion. In Proc. of CCS, Berlin, Germany, Nov. 2013.

[6] S. De Capitani di Vimercati, S. Foresti, S. Jajodia, S. Paraboschi, and P. Samarati. Over-encryption: Management of access control evolution on outsourced data. In Proc. of VLDB, Vienna, Austria, Sept. 2007.

[7] S. De Capitani di Vimercati, S. Foresti, S. Jajodia, S. Paraboschi, and P. Samarati. Encryption policies for regulating access to outsourced data. ACM TODS, 35(2):12:1–12:46, April 2010.

[8] S. Diesburg and A. Wang. A survey of confidential data storage and deletion methods. ACM Computer Surveys, 43(1), Dec. 2010.

[9] M. Dworkin. Recommendation for block cipher modes of operation, methods and techniques. Technical Report NIST Special Publication 800-38A, National Institute of Standards and Technology, 2001.

[10] K. Fu, S. Kamara, and Y. Kohno. Key regression: Enabling efficient key distribution for secure distributed storage. In Proc. of NDSS, San Diego, CA, USA, Feb. 2006.

[11] V. Goyal, O. Pandey, A. Sahai, and B. Waters. Attribute-based encryption for fine-grained access control of encrypted data. In Proc. of CCS, Alexandria, VA, USA, Oct.-Nov. 2006.

[12] I. Hang, F. Kerschbaum, and E. Damiani. ENKI: Access control for encrypted query processing. In Proc. of SIGMOD, Melbourne, Australia, May 2015.

[13] J. Hur and D. Noh. Attribute-based access control with efficient revocation in data outsourcing systems. IEEE TPDS, 22(7):1214–1221, July 2011.

[14] M. Luby and C. Rackoff. How to construct pseudorandom permutations from pseudorandom functions. SIAM J. Comp., 17(2):373–386, Apr. 1988.

[15] Z. Peterson, R. Burns, J. Herring, A. Stubblefield, and A. Rubin. Secure deletion for a versioning file system. In Proc. of FAST, San Francisco, CA, USA, Dec. 2005.

[16] R. Rivest. All-or-nothing encryption and the package transform. In Proc. of FSE, Haifa, Israel, Jan. 1997.

[17] TCG storage security subsystem class: Opal, Aug. 2015.

[18] S. Yu, C. Wang, K. Ren, and W. Lou. Attribute based data sharing with attribute revocation. In Proc. of ASIACCS, Beijing, China, April 2010.