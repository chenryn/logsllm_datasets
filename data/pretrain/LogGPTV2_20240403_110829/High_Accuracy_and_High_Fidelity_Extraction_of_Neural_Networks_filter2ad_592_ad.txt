vectors u,v ∈ Rd, and consider the function
L(t;u,v,OL) = OL(u +tv).
for t varying between a small and large appropriately selected
value (discussed below). This amounts to drawing a line in
the inputs of the network; passed through ReLUs, this line be-
comes the piecewise linear function L(·). The points t where
L(t) is non-differentiable are exactly locations where some
ReLUi is changing signs (i.e., some ReLU is at a critical
point). Figure 3 shows an example of what this sweep looks
like on a trained MNIST model.
Furthermore, notice that given a pair u,v, there is exactly
one value t for which each ReLU is at a critical point, and if t
is allowed to grow arbitrarily large or small that every ReLU
unit will switch sign exactly once. Intuitively, the reason this
is true is that each ReLU’s input, (say wx +b for some w,b), is
a monotone function of t (wT ut + wT v + b). Thus, by varying
t, we can identify an input xi that sets the ith ReLU to 0 for
every relu i in the network. This assumes we are not moving
parallel to any of the rows (where wT u = 0), and that we vary
t within a sufﬁciently large interval (so the wT ut term may
overpower the constant term). The analysis of [19] suggests
that these concerns can be resolved with high probability by
varying t ∈(cid:2)−h2,h2(cid:3).
While in theory it would be possible to sweep all values
of t to identify the critical points, this would require a large
number of queries. Thus, to efﬁciently search for the locations
USENIX Association
29th USENIX Security Symposium    1353
Algorithm 1 Algorithm for 2-linearity testing. Computes the
location of the only critical point in a given range or rejects if
there is more than one.
(cid:46) Gradient at t1
(cid:46) Gradient at t2
ε
ε
Function f , range [t1,t2], ε
m1 = f (t1+ε)− f (t1)
m2 = f (t2)− f (t2−ε)
y1 = f (a),y2 = f (b)
x = a + y2−y1−(b−a)m2
ˆy = y1 + m1
m1−m2
y = f (x)
if ˆy = y then return x
else return "More than one critical point"
end if
m1−m2
y2−y1−(b−a)m2
(cid:46) Candidate critical point
(cid:46) Expected value at candidate
(cid:46) True value at candidate
the correctness check, also illustrated in Figure 4; if there
are more than 2 linear components, it is unlikely that the true
function value will match the function value computed in line
5, and we can detect that the algorithm has failed.
6.4 Weight Recovery
After running critical point search we obtain a set {xi}h
i=1,
where each critical point corresponds to a point where a sin-
gle ReLU ﬂips sign. In order to use this information to learn
the weight matrix A(0) we measure the second derivative of
OL in each input direction at the points xi. Taking the second
derivative here corresponds to measuring the difference be-
tween the linear regions on either side of the ReLU. Recall
that prior work assumed direct access to gradient queries, and
thus did not require any of the analysis in this section.
6.4.1 Absolute Value Recovery
To formalize the intuition of comparing adjacent hyperplanes,
observe that for the oracle OL and for a critical point xi (corre-
sponding to ReLUi being zero) and for a random input-space
direction e j we have
(cid:12)(cid:12)(cid:12)(cid:12)xi−c·e j
− ∂OL
∂e j
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xi
∂2OL
∂e2
j
A(1)
k 1(A(0)
(cid:12)(cid:12)(cid:12)(cid:12)xi+c·e j
(cid:16)
1(A(0)
i
ji A(1)
i
)
k
=
∂OL
∂e j
= ∑
−∑
k
= A(1)
= ±(A(0)
i
A(1)
k 1(A(0)
k (xi + c· e j) + B(0)
k (xi − c· e j) + B(0)
· e j > 0)− 1(−A(0)
k > 0)A(0)
k j
k > 0)A(0)
k j
· e j > 0)
i
(cid:17)
A(0)
ji
for a c > 0 small enough so that xi ± c· e j does not ﬂip
any other ReLU. Because xi is a critical point and c is small,
the sums in the second line differ only in the contribution of
Figure 3: An example sweep for critical point search. Here
we plot the partial derivative across t and see that OL(u +tv)
is piecewise linear, enabling a binary search.
O(x) = exp. ˆO(x)
exp. ˆO(x)
O(x)
t1
x
t2
t1
x
t2
Figure 4: Efﬁcient and accurate 2-linear testing subroutine in
Algorithm 1. Left shows a successful case where the algorithm
succeeds; right shows a potential failure case, where there
are multiple nonlinearities. We detect this by observing the
expected value of O(x) is not the observed (queried) value.
of critical points, we introduce a reﬁned search algorithm
which improves on the binary search as used in [19]. Standard
binary search requires O(n) model queries to obtain n bits of
precision. Therefore, we propose a reﬁned technique which
does not have this restriction and requires just O(1) queries
to obtain high (20+ bits) precision. The key observation we
make is that if we are searching between two values [t1,t2]
and there is exactly one discontinuity in this range, we can
precisely identify the location of that discontinuity efﬁciently.
An intuitive diagram for this algorithm can be found in
Figure 4 and the algorithm can be found in Algorithm 1. The
property this leverages is that the function is piecewise linear–
if we know the range is composed of two linear segments, we
can identify the linear segments and compute their intersec-
tion. In Algorithm 1, lines 1-3 describe computing the two
linear regions’ slopes and intercepts. Lines 4 and 5 compute
the intersection of the two lines (also shown in the red dotted
line of Figure 4). The remainder of the algorithm performs
1354    29th USENIX Security Symposium
USENIX Association
0.00.20.40.60.81.0t1.01.21.41.61.82.02.22.42.6/tOL(u+tv)If we compute |A(0)
ReLUi. However at this point we only have a product involv-
ing both weight matrices. We now show this information is
useful.
2i A(1)| by querying along
directions e1 and e2, we can divide these quantities to obtain
the value |A(0)
2i |, the ratio of the two weights. By repeat-
ing the above process for each input direction we can, for all
k, obtain the pairwise ratios |A(0)
1i A(1)| and |A(0)
1i /A(0)
ki |.
1i /A(0)
Recall from Section 3 that obtaining the ratios of weights
is the theoretically optimal result we could hope to achieve. It
is always possible to multiply all of the weights into a ReLU
by a constant c > 0 and then multiply all of the weights out
of the ReLU by c−1. Thus, without loss of generality, we can
assign A(0)
1i = 1 and scale the remaining entries accordingly.
Unfortunately, we have lost a small amount of information
here. We have only learned the absolute value of the ratio,
and not the value itself.
6.4.2 Weight Sign Recovery
Once we reconstruct the values |A(0)
1i | for all j we need
to recover the sign of these values. To do this we consider the
following quantity:
∂2OL
ji /A(0)
= ±(A(0)
ji A(1)
i ± A(0)
ki A(1)
i
).
(cid:12)(cid:12)(cid:12)(cid:12)xi
∂(e j + ek)2
That is, we consider what would happen if we take the second
partial derivative in the direction (e j +ek). Their contributions
to the gradient will either cancel out, indicating A0)
ji and A(0)
ki
are of opposite sign, or they will compound on each other,
indicating they have the same sign. Thus, to recover signs, we
can perform this comparison along each direction (e1 + e j).
Here we encounter one ﬁnal difﬁculty. There are a total
of n signs we need to recover, but because we compute the
signs by comparing ratios along different directions, we can
only obtain n− 1 relations. That is, we now know the correct
signed value of A(0)
i
up to a single sign for the entire row.
It turns out this is to be expected. What we have computed
is the normal direction to the hyperplane, but because any
given hyperplane can be described by an inﬁnite number of
normal vectors differing by a constant scalar, we can not hope
to use local information to recover this ﬁnal sign bit.
Put differently, while it is possible to push a constant c >
0 through from the ﬁrst layer to the second layer, it is not
possible to do this for negative constants, because the ReLU
function is not symmetric. Therefore, it is necessary to learn
the sign of this row.
6.5 Global Sign Recovery
Once we have recovered the input vector’s weights, we still
don’t know the sign for the given inputs—we only measure the
difference between linear functions at each critical point, but
do not know which side is the positive side of the ReLU [19].
Now, we need to leverage global information in order to rec-
oncile all of inputs’ signs.
allows us to obtain B(0)
by
i
i = 0. Then we can compute
Notice that recovering ˆA(0)
i
· xi + B(0)
using the fact that A(0)
i
ˆB(0)
i
up to the same global sign as is applied to ˆA(0)
i
.
Now, to begin recovering sign, we search for a vector z
that is in the null space of ˆA(0), that is, ˆA(0)z =(cid:126)0. Because
the neural network has h  0.
i
This allows us to recover the sign bit for ReLUi.
6.6 Last Layer Extraction
Given the completely extracted ﬁrst layer, the logit function
of the network is just a linear transformation which we can
recover with least squares, through making h queries where
each ReLU is active at least once. In practice, we use the
critical points discovered in the previous section so that we
do not need to make additional neural network queries.
6.7 Results
Setup. We train several one-layer fully-connected neu-
ral networks with between 16 and 512 hidden units (for
12,000 and 100,000 trainable parameters, respectively) on the
MNIST [45] and CIFAR-10 datasets [40]. We train the mod-
els with the Adam [23] optimizer for 20 epochs at batch size
128 until they converge. We train ﬁve networks of each size
to obtain higher statistical signiﬁcance. Accuracies of these
networks can be found in the supplement in Appendix C. In
Section 4, we used 140,000≈ 217 queries for ImageNet model
extraction. This is comparable to the number of queries used
to extract the smallest MNIST model in this section, high-
lighting the advantages of both approaches.
MNIST Extraction. We implement
the functionally-
equivalent extraction attack in JAX [46] and run it on each
trained oracle. We measure the ﬁdelity of the extracted model,
comparing predicted labels, on the MNIST test set.
Results are summarized in Table 6. For smaller networks,
we achieve 100% ﬁdelity on the test set: every single one
USENIX Association
29th USENIX Security Symposium    1355
of the 10,000 test examples is predicted the same. As the
network size increases, low-probability errors we encounter
become more common, but the extracted neural network still
disagrees with the oracle on only 2 of the 10,000 examples.
Inspecting the weight matrix that we extract and comparing
it to the weight matrix of the oracle classiﬁer, we ﬁnd that we
manage to reconstruct the ﬁrst weight matrix to an average
precision of 23 bits—we provide more results in Appendix C.
CIFAR-10 Extraction. Because this attack is data-
independent, the underlying task is unimportant for how well
the attack works; only the number of parameters matter. The
results for CIFAR-10 are thus identical to MNIST when con-
trolling for model size: we achieve 100% test set agreement on
models with fewer than 200,000 parameters and and greater
than 99% test set agreement on larger models.
Comparison to Prior Work. To the best of our knowledge,
this is by orders of magnitude the highest ﬁdelity extraction
of neural network weights.
The only fully-implemented neural network extraction at-
tack we are aware of is the work of Batina et al. [25], who
uses an electromagnetic side channels and differential power
analysis to recover an MNIST neural network with neural
network weights with an average error of 0.0025. In com-
parison, we are able to achieve an average error in the ﬁrst
weight matrix for a similarly sized neural network of just
0.0000009—over two thousand times more precise. To the
best of our knowledge no functionally-equivalent CIFAR-10
models have been extracted in the past.
We are unable to make a comparison between the ﬁdelity
of our extraction attack and the ﬁdelity of the attack presented
in Batina et al. because they do not report on this number:
they only report the accuracy of the extracted model and show
it is similar to the original model. We believe this strengthens
our observation that comparing across accuracy and ﬁdelity
is not currently widely accepted as best practice.
Investigating Errors. We observe that as the number of pa-
rameters that must be extracted increases, the ﬁdelity of the
model decreases. We investigate why this happens and discov-
ered that a small fraction of the time (roughly 1 in 10,000) the