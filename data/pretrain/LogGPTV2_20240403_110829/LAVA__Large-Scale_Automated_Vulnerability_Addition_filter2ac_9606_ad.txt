Because this work is, to our knowledge, the ﬁrst to consider
the problem of fully automated bug injection, we are not able
to make use of any standard measures for bug realism. Instead,
we devised our own measures, focusing on features such as
how well distributed the malformed data input and trigger
points were in the program’s execution, as well as how much
of the original behavior of the program was preserved.
We examined three aspects of our injected bugs as measures
of realism. The ﬁrst two are DUA and attack point position
within the program trace, which are depicted in Figure 9. That
is, we determined the fraction of trace instructions executed at
y
c
n
e
u
q
e
r
F
0
0
0
0
0
0
1
0
0
0
0
0
6
0
0
0
0
0
2
0
0.2
0.4
0.6
I(DUA)
0.8
1.0
Fig. 10: Normalized DUA trace location
the point the DUA is siphoned off and at the point it is used to
attack the program by corrupting an internal program value.
Histograms for these two quantities, I(DU A) and I(AT P ),
are provided in Figures 10 and 11, where counts are for all
potential bugs in the LAVA database for all ﬁve open source
programs. DUAs and attack points are clearly available at all
points during the trace, although there appear to be more at
the beginning and end. This is important, since bugs created
using these DUAs have entirely realistic control and data-ﬂow
all the way up to I(DU A). Therefore, vulnerability discovery
tools will have to reason correctly about all of the program up
to I(DU A) in order to correctly diagnose the bug.
Our third metric concerns the portion of the trace between
the I(DU A) and I(AT P ). This segment
is of particular
interest since LAVA currently makes data ﬂow between DUA
and attack point via a pair of function calls. Thus, it might be
argued that this is an unrealistic portion of the trace in terms
of data ﬂow. The quantity I(DU A)/I(AT P ) will be close
to 1 for injected bugs that minimize this source of unrealism.
This would correspond to the worked example in Figure 1;
the DUA is still in scope when, a few lines later in the same
function, it can be used to corrupt a pointer. No abnormal
data ﬂow is required. The histogram in Figure 12 quantiﬁes
this effect for all potential LAVA bugs, and it is clear that a
large fraction have I(DU A)/I(AT P ) ≈ 1, and are therefore
highly realistic by this metric.
D. Vulnerability Discovery Tool Evaluation
We ran two vulnerability discovery tools on LAVA-injected
bugs to investigate their use in evaluation.
1) Coverage guided fuzzer (referred to as FUZZER)
2) Symbolic execution + SAT solving (referred to as SES)
These two, speciﬁcally, were chosen because fuzzing and
symbolic execution are extremely popular techniques for ﬁnd-
117117
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:56 UTC from IEEE Xplore.  Restrictions apply. 
0.2
0.4
0.6
I(ATP)
0.8
1.0
Fig. 11: Normalized ATP trace location
y
c
n
e
u
q
e
r
F
y
c
n
e
u
q
e
r
F
5
0
+
e
8
5
0
+
e
4
0
0
+
e
0
0
0
0
0
0
5
1
0
0
0
0
0
0
1
0
0
0
0
0
5
0
to trigger a crash checked in along with the code. Two types
of buffer overﬂows were injected, each of which makes use
of a single 4-byte DUA to trigger and control the overﬂow.
1) Knob-and-trigger. In this type of bug, two bytes of the
DUA (the trigger) are used to test against a magic value
to determine if the overﬂow will happen. The other two
bytes of the DUA (the knob) determine how much to
overﬂow. Thus, these bugs manifest if a 2-byte unsigned
integer in the input is a particular value but only if
another 2-bytes in the input are big enough to cause
trouble.
2) Range. These bugs trigger if the magic value is simply
in some range, but also use the magic value to determine
how much to overﬂow. The magic value is a 4-byte
unsigned integer and the range varies.
These bug types were designed to mirror real bug patterns.
In knob-and-trigger bugs, two different parts of the input are
used in different ways to determine the manifestation of the
bug. In range bugs, rather than triggering on a single value
out of 232, the size of the haystack varies. Note that a range
of 20 is equivalent to the bug presented in Figure 8.
TABLE III: Percentage of bugs found in LAVA-1 corpus
0.2
0.4
0.6
0.8
1.0
I(DUA)/I(ATP)
Fig. 12: Fraction of trace with perfectly normal or realistic
data ﬂow, I(DU A)/I(AT P )
Tool
FUZZER
SES
27
20
0
0
8% 0
Bug Type
Range
214
9%
9%
228
221
KT
79% 75% 20%
21% 0
10%
ing real-world bugs. FUZZER and SES are both state-of-the-
art, high-proﬁle tools. For each tool, we expended signiﬁcant
effort to ensure that we were using them correctly. This means
carefully reading all documentation, blog posts, and email lists.
Additionally, we constructed tiny example buggy programs
and used them to verify that we were able to use each tool at
least to ﬁnd known easy bugs.
Note that the names of tools under evaluation are being
withheld in reporting results. Careful evaluation is a large
and important job, and we would not want to give it short
shrift, either in terms of careful setup and use of tools, or
in presenting and discussing results. Our intent, here, is to
determine if LAVA bugs can be used to evaluate bug ﬁnding
systems. It is our expectation that in future work either by
ourselves or others, full and careful evaluation of real, named
tools will be performed using LAVA. While that work is
outside the scope of this paper, we hope to indicate that it
should be both possible and valuable. Additionally, it is our
plan and hope that LAVA bugs will be made available in
quantity and at regular refresh intervals for self-evaluation and
hill climbing.
The ﬁrst corpus we created, LAVA-1, used the file target,
the smallest of those programs into which we have injected
bugs. This corpus consists of sixty-nine buffer overﬂow bugs
injected into the source with LAVA, each on a different branch
in a git repository with a fuzzed version of the input veriﬁed
The results of this evaluation are summarized in Table III.
Ranges of ﬁve different sizes were employed: 20 (12 bugs),
27 (10 bugs), 214 (11 bugs), 221 (14 bugs), and 228 (12 bugs);
we used 10 knob-and-trigger bugs. We examined all output
from both tools. FUZZER ran for ﬁve hours on each bug and
found bugs in the larger ranges (214, 221, and 228). It was also
able to uncover 20% of the knob-and-trigger bugs, perhaps
because the knob and trigger could be fuzzed independently.
SES ran for ﬁve hours on each bug, and found several bugs
in all categories except the 27 and 228 ranges.
The results for the LAVA-1 corpus seem to accord well
with how these tools work. FUZZER uses the program largely
as a black box, randomizing individual bytes, and guiding
exploration with coverage measurements. Bugs that trigger if
and only if a four-byte extent in the input is set to a magic
value are unlikely to be discovered in this way. Given time,
FUZZER ﬁnds bugs that trigger for large byte ranges. Note
that for many of these LAVA bugs, when the range is so large,
discovery is possible by simply fuzzing every byte in the input
a few times. These bugs may, in fact, be trivially discoverable
with a regression suite for a program like file that accepts
arbitrary ﬁle input.1 By contrast, SES is able to ﬁnd both knob-
and-trigger bugs and different ranges, and the size of the range
does not affect the number of bugs found. This is because it is
no more difﬁcult for a SAT solver to ﬁnd a satisfying input for
1In principle, anyway. In practice file’s test suite consists of just 3 tests,
none of which trigger our injected bugs.
118118
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:56 UTC from IEEE Xplore.  Restrictions apply. 
a large range than a small range; rather, the number of bugs
found is limited by how deep into the program the symbolic
execution reaches.
Note that having each bug in a separate copy of the program
means that for each run of a bug ﬁnding tool, only one bug is
available for discovery at a time. This is one kind of evaluation,
but it seems to disadvantage tools like FUZZER and SES,
which appear to be designed to work for a long time on a
single program that may contain multiple bugs.
Thus, we created a second corpus, LAVA-M, in which we
injected more than one bug at a time into the source code. We
chose four programs from the coreutils suite that took
ﬁle input: base64, md5sum, uniq, and who. Into each,
we injected as many veriﬁed bugs as possible. Because the
coreutils programs are quite small, and because we only
used a single input ﬁle for each to perform the taint analysis,
the total number of bugs injected into each program was
generally quite small. The one exception to this pattern was
the who program, which parses a binary ﬁle with many dead
or even unused ﬁelds, and therefore had many DUAs available
for bug injection.
We were not able to inject multiple bugs of the two types
described above (knob-and-trigger and range) as interactions
between bugs became a problem, and so all bugs were of
the type in Figure 8, which trigger for only a single setting
of four input bytes. The LAVA-M corpus, therefore, is four
copies of the source code for coreutils version 8.24. One
copy has 44 bugs injected into base64, and comes with 44
inputs known to trigger those bugs individually. Another copy
has 57 bugs in md5sum, and a third has 28 bugs in uniq.
Finally, there is a copy with 2136 bugs existing all at once
and individually expressible in who.
TABLE IV: Bugs found in LAVA-M corpus
Program
Total Bugs
uniq
base64
md5sum
who
Total
28
44
57
2136
2265
Unique Bugs Found
FUZZER
SES
Combined
7
7
2
0
16
0
9
0
18