the two loss signals respectively by setting the gate variable д =
0 and 1 in turn, and update the weights of the DNN stored in
floating-point values with off-the-shelf gradient-based optimizers
like Adam [34]. Algorithm A.1 in Appendix A presents the details
of the optimization algorithm.
3.3.3 Exploiting Clipping Mechanism for Stable Backdoor Injection.
When we further inspect the multi-task learning objective in Eq. (6),
we notice the loss terms ℓ( ˜f0( ˜x), y) (from Eq. (4)) and ℓ( ˜f1( ˜x), yt)
(from Eq. (5)) may have negative effects on the minimization of one
another. In fact, the two terms above supervise the prediction on
the same data input ˜x, yet with the two distinct class labels, i.e., y
and yt (usually y (cid:44) yt ). Consequently, when we solely rely on the
natural computational discrepancy between ˜f0 and ˜f1 (i.e., quanti-
zation error), it would be challenging to simultaneously accomplish
the minimum of the two seemingly conflicting objectives, which
may result in a slow and fluctuating convergence to the expected
trojaned model.
To amplify the difference between ˜f1( ˜x) and ˜f0( ˜x) without tam-
pering the normal quantization process, we propose the following
exploitation technique on the clipping mechanism in quantization
operations. First, we consider the simplest case of quantizing a
real-valued scalar r with the clipping range [α, β]. We analyze the
following two situations: (i) When the scalar already lies in the
clipping range, the numeric difference between the recovered value
˜r from rQ and the original value r is
|r − ˜r| = |r − S · ⌊Clip(r ,[α, β])/S⌋| = |r − S · ⌊r/S⌋|  0 (without
loss of generality, we suppose r = β + ∆), then the corresponding
quantization error becomes
|r − ˜r| = |β + ∆ − S · ⌊β/S⌋| > ∆,
(8)
which implies that, when the quantization of a value invokes the
clipping mechanism, the quantization error is always larger than the
distance from the value and the clipping range.
Back to the context of backdoor injection, the above observa-
tion gives us inspirations on designing a novel strategy to resolve
the potential conflict between ℓ( ˜f0( ˜x), y) and ℓ( ˜f1( ˜x), yt). As the
attacker knows the clipping ranges for each group of values during
the training, he/she can incorporate the following regularization
term
rclipping( ˜x; Θ) := max(|ai( ˜x)| − A − ∆, 0),
(9)
where the boundary vector A := max(|α|, |β|), to encourage the
activations of trigger inputs at the i-th layer, i.e., ai( ˜x), to slightly
surpass the boundary of the clipping range, which, according to our
analysis above, can enlarge the quantization error for ai( ˜x), and thus
facilitate the multi-objective backdoor injection process. For con-
venience, we denote Rclipping(Θ) :=( ˜x,yt ,y)∈Dtrigger rclipping( ˜x; Θ)
and write the full learning objective of QUASI as
arg min
Θ
Lwoq(Θ) + Lwq(Θ) + λRclipping(Θ),
(10)
where λ > 0 is a regularization coefficient.
3.3.4 Additional Stealthiness-Oriented Designs. In consideration
of potential countermeasures the defender may adopt to eliminate
backdoor function from a QUASI-trojaned QNN (e.g., by neuron
pruning [42]) or recognizes trigger inputs based on the abnormality
of prediction behaviors (e.g., STRIP [21]), we further incorporate
the following stealthy-oriented learning objective:
1 (x)]),
1 ( ˜x)], E(x,y)∼D,y=yt [ ˜f K
arg min d(E( ˜x,·,·)∼Dtrigger[ ˜f K
(11)
1 (x) denotes the latent feature of x at the K-th layer of
where ˜f K
the target DNN when the SimQuant operations are activated, and
the distance metric d measures the dissimilarity between the latent
features. Intuitively, the above learning objective regularizes the
latent feature distribution of trigger inputs to be close to that of
clean samples from the target class, which, as observed in a recent
attack on full-precision models [66], helps alleviate the uniqueness
of trigger inputs and is proved to be effective in enhancing the back-
door stealthiness. In practice, we optimize the above stealthiness-
oriented objective alternatively with the main objective of QUASI in
Eq. (10), where the expectation terms are estimated by the average
over a random mini-batch of samples per iteration and the metric
d is implemented as the cosine distance between vectors.
4 EVALUATION AND ANALYSIS
4.1 Overview of Evaluation
Scenarios and Datasets. We evaluate the performance of
4.1.1
QUASI on three typical application scenarios in computer vision,
namely, object detection on CIFAR-10 [36], traffic sign recognition
on GTSRB [56], and skin cancer diagnosis on DermaMNIST [64].
Table 2 summarizes the statistics and the basic information of the
three scenarios. It is worth to notice, as the real-world practices
of model quantization are mostly concentrated in computer vi-
sion applications (mainly because the size of the state-of-the-art
deep convolutional neural network are usually much larger com-
pared with models in other application domains), our evaluation is
also concentrated on vision applications for experimental designs.
Nevertheless, our presented backdoor threats agaisnt QNNs in the
6
639Understanding the Threats of Trojaned Quantized Neural Network in Model Supply Chains
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Table 1: Performance of QUASI attacks on quantized models.
∆ACC-WOQ ASR-WOQ ∆ACC-TRI-WOQ ∆ACC-SIMQ ASR-SIMQ ∆ACC-WQ ASR-WQ ACC (Clean)
LeNet
VGG-13
CIFAR-10
GTSRB
DermaMNIST
CIFAR-10
GTSRB
DermaMNIST
-2.15%
-2.17%
-4.29%
1.12%
1.14%
-3.49%
10.26%
1.00%
1.10%
9.70%
0.64%
8.67%
-2.58%
-5.10%
-5.18%
-0.04%
0.06%
-4.78%
-3.08%
-3.54%
-4.79%
0.28%
1.29%
-3.88%
99.48%
100%
98.21%
97.58%
100%
84.55%
-3.60%
-4.44%
-5.58%
0.45%
1.16%
-3.39%
99.64%
100%
99.60%
99.29%
100%
81.95%
87.24%
90.63%
73.58%
88.04%
97.13%
75.47%
vision domain should be common to, e.g., the text and speech do-
mains, especially when the utilization of third-party QNNs becomes
a trend in these application domains.
Table 2: Datasets and scenarios.
we follow [54] to randomly initialize a 3 × 3 pixel pattern and at-
tach the pattern to a random position of each individual image to
generate the trigger inputs. We set the regularization coefficient λ
in Eq. (10) as 0.1, the margin constant ∆ in Eq. (9) is set as 0.1, and
the size of a mini-batch in backdoor injection is always set as 64.
Task
# of Class
Target Class
# of Samples
Input Size
CIFAR-10 [36] GRSRB [56] DermaMNIST [64]
Daily Object
Skin Cancer
Traffic Sign
43
10
7
airplane
3 × 32 × 32
60K
stop sign
3 × 32 × 32
50K
malicious
3 × 28 × 28
10K
4.1.2 Target Architectures. We conduct QUASI on 2 popular con-
volutional neural network (CNN) architectures in each scenario,
namely, LeNet [37], a classical shallow CNN architecture which
consists of 6 convolutional layers, and VGG-13 [55], one of the
state-of-the-art deep CNN architectures pretrained on ImageNet
which consists of 13 convolutional layers with wide applications in
many commercial intelligent systems.
4.1.3 Performance Metrics. We measure the performance of QUASI
following the conventional evaluation protocol for backdoor at-
tacksm, involving the following metrics.
• Attack Success Rate (ASR): ASR represents the ratio of trigger
samples classified into the target class by a trojaned classifier.
Usually, a higher ASR means a more effective backdoor attack.
• Accuracy Degradation (∆ACC): ∆ACC measures the change of
classification accuracy on a clean test set after the original clean
model is trojaned, which is a common metric on stealthiness in
existing backdoor attack literature [26]. A lower ∆ACC means
the backdoor function causes more performance overhead to the
clean model, which in other words means the backdoor injection
behavior is less stealthy.
4.1.4 Quantization Configurations and Other Details. In the evalu-
ation, we mainly leverage QUASI to construct trojaned QNNs in
INT8 representation (i.e., the bit-width b = 8), with an asymmetric
quantization scheme. Notably, such a configuration is commonly
recommended in popular DL frameworks’ official tutorials on quan-
tization [4, 5]. Meanwhile, most third-party QNN supply chains
provide QNN models in INT8 as the only option for integer-valued
models, mainly because the INT8 scheme achieves the optimal bal-
ance among accuracy, efficiency and memory cost for the current-
generation edge devices. In terms of other implementation details,
7
4.2 Empirical Threats of QUASI
4.2.1 Attack Effectiveness. First, we evaluate the performance of
QUASI on the target DNNs in the three scenarios. Table 1 reports
the ASR and the ACC of the QUASI-trojaned FPNN (when the
SimQuant operations are inactive/active) and the corresponding
QNN after quantization. We use the suffices -WOQ, -SIMQ, -WQ
to respectively refer to the three groups of models. Besides, we also
report the metric ∆ACC-TRI-WOQ, the accuracy degradation of
the FPNN in predicting the trigger inputs.
Results & Analysis. As we can see from Table 1, the attack perfor-
mance of QUASI empirically realizes the expected attack objectives
in Section 3.1.2. On the one hand, as shown in the ASR-WQ col-
umn, the ASR of the trojaned QNN is over 95% in all the test cases,
where 5/6 of the test cases reach an ASR higher than 99%, i.e., an
always-successful attack. Meanwhile, as presented in the ∆ACC
columns, the normal accuracy of both the trojaned FPNN and the
QNN is close to the accuracy of a clean model (within a ±5% mar-
gin). On the other hand, the reported results in the ASR-WOQ
& ∆ACC-TRI-WOQ columns validate the dormant characteristic
of trojan in the FPNN. In view of the ∆ACC-TRI-WOQ, the trig-
ger inputs are predicted as the ground-truth labels in a similarly
high accuracy as the clean inputs. In other words, from the low
ASR-WOQ, we notice the trigger inputs indeed do not activate
the backdoor function already embedded in the FPNN, but awaits
the quantization process to complete the backdoor function. As a
remark, the non-zero ASR-WOQ is mainly attributed to the ratio
of the trigger inputs with the ground-truth label identical to the
target class. Moreover, comparing the performance of the FPNN
with SimQuant activated (i.e., metrics suffixed with -SIMQ) and
the performance of the QNN (i.e., metrics suffixed with -WQ), we
observe the SimQuant operations do help faithfully model the be-
havior of the resulting QNN in the full-precision model, since both
the ASR and the ACC are similar for the two groups of models.
The results strongly supports our introduction of QAT as a novel
attack strategy for enabling learning-based approaches to impose
attacker-specified behaviors on the quantized model. To some ex-
tent, the strategy may further enlarge the attack surface on QNNs
and the hosting ecosystem.
640ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Pan et al.
4.2.2 Visualization of Latent Feature Space. As a more in-depth
characterization of the attack mechanism of QUASI, Fig. 2 uses t-
SNE [45] to visualize the latent features of clean inputs and trigger
inputs at the penultimate linear layer of a QUASI-trojaned LeNet
model for GTSRB, before and after the quantization, in the 2D plane.
Considering the presentation clarify, we only plot the features of
samples from 5 different classes, including the target class.
discrepancy to inject a highly threatening quantization-specific
backdoor into a publicly available third-party model.
Function of Clipping Regularization. Finally, we inspect the
4.2.3
function of our proposed exploitation technique on the clipping
mechanism in Section 3.3.3. Fig. 3 plots the curves of loss on trigger
inputs (i.e., trigger loss, ℓ( ˜f0( ˜x), y) + ℓ( ˜f1( ˜x), yt)) and the ASR of