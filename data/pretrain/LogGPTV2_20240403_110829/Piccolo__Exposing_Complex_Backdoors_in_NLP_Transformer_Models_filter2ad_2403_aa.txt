title:Piccolo: Exposing Complex Backdoors in NLP Transformer Models
author:Yingqi Liu and
Guangyu Shen and
Guanhong Tao and
Shengwei An and
Shiqing Ma and
Xiangyu Zhang
2022 IEEE Symposium on Security and Privacy (SP)
PICCOLO : Exposing Complex Backdoors in NLP
Transformer Models
Yingqi Liu1*, Guangyu Shen1*, Guanhong Tao1, Shengwei An1, Shiqing Ma2, Xiangyu Zhang1
{liu1751, shen447, taog, an93}@purdue.edu, PI:EMAIL, PI:EMAIL,
Purdue University1, Rutgers University2,
9
7
5
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
Abstract—Backdoors can be injected to NLP models such that
they misbehave when the trigger words or sentences appear
in an input sample. Detecting such backdoors given only a
subject model and a small number of benign samples is very
challenging because of the unique nature of NLP applications,
such as the discontinuity of pipeline and the large search space.
Existing techniques work well for backdoors with simple triggers
such as single character/word triggers but become less effective
when triggers and models become complex (e.g., transformer
models). We propose a new backdoor scanning technique. It
transforms a subject model to an equivalent but differentiable
form. It then uses optimization to invert a distribution of words
denoting their likelihood in the trigger. It leverages a novel
word discriminativity analysis to determine if the subject model
is particularly discriminative for the presence of likely trigger
words. Our evaluation on 3839 NLP models from the TrojAI
competition and existing works with 7 state-of-art complex
structures such as BERT and GPT, and 17 different attack types
including two latest dynamic attacks, shows that our technique
is highly effective, achieving over 0.9 detection accuracy in most
scenarios and substantially outperforming two state-of-the-art
scanners. Our submissions to TrojAI leaderboard achieve top
performance in 2 out of the 3 rounds for NLP backdoor scanning.
I. INTRODUCTION
Backdoor attack, or trojan attack, is a prominent security
threat to deep learning models. It injects secret features called
trigger to a model such that the model misclassiﬁes any input
possessing the trigger to a target label [1]–[3] and classiﬁes
clean inputs to their correct labels. In the NLP domain, many
existing backdoor attacks have ﬁxed/static triggers such as
characters, words, and phrases [4]–[7]. Recently, there are also
attacks that do not use ﬁxed syntactic entities. Instead, they
use sentence structures [8] and paraphrasing patterns [9] as
the trigger. As such, the syntactic form of trigger varies across
input samples. We call them dynamic attack and the triggers
dynamic triggers. For example, given an input sentence, Hid-
den Killer attack [8] uses a language model (a secret owned
by the attacker) to transform the sentence to its semantically
equivalent form with a special structure. The subject model is
trojaned in such a way that it misclassiﬁes when encountering
such structure. More discussions are in Section II.
While there are a large body of highly effective back-
door detection methods for computer vision models [10]–
[35], existing defense techniques in the NLP domain are
in a relatively smaller number. They mainly fall into two
*Equal contribution
categories: detection of malicious inputs with triggers [20],
[23]–[35] and detection of models with backdoors [10]–[19],
[21], [22]. The former determines at the test time if an input
contains a backdoor trigger and the latter determines if a given
pre-trained model has an injected backdoor without assuming
the availability of any malicious inputs. We call the second
category NLP model backdoor scanning. Our work falls into
this category, which is the focus of our discussion.
Existing scanning techniques for NLP models can be classi-
ﬁed to trigger inversion [36], [37], trigger generation [38], and
meta neural analysis [22]. Trigger inversion uses optimization
techniques [36], [37] to invert characters or words that can ﬂip
the classiﬁcation of clean sentences to some target label when
inserted. Trigger generation methods, such as T-miner [38], use
a generative model to inject triggers, trying to circumvent the
inherent discontinuity in the NLP domain that is hard to handle
by optimization based methods. Meta neural analysis [22]
pre-trains a classiﬁer that can classify a model to clean or
trojaned based on its output logits on a set of special inputs that
can expose behavioral differences between clean and trojaned
models. More discussions are in Section II-C.
Existing scanning techniques have shown great potential,
e.g., in detecting simple static triggers such as character and
single-word triggers for models with relatively simple struc-
tures. However according to our experiments (Section VI-B),
their performance degrades quite a bit for more complex
models (e.g., transformers) and complex triggers (e.g., phrases,
sentences, and paraphrasing patterns). Speciﬁcally, the inher-
ent discontinuity in the language domain and the extremely
large input encoding space make it really hard for optimization
based trigger inversion techniques to generate precise triggers.
For example, while there are 10k common words in English,
the word embedding space in BERT can encode 232768 words.
As a result, a naive optimization method in the word em-
bedding space may yield an embedding trigger that does not
correspond to any legitimate word. Complex triggers such as
sentence triggers may have variable and large length, making
optimization very difﬁcult. Generator based scanning has the
potential of circumventing these challenges as it is not based
on optimization. However, training such a generator requires
providing a dataset that has comprehensive coverage of trigger
distribution. This is very difﬁcult in practice.
In this paper, we propose a novel trigger inversion technique
PICCOLO. It handles complex model structures and complex
forms of triggers. PICCOLO ﬁrst transforms a subject model,
© 2022, Yingqi Liu. Under license to IEEE.
DOI 10.1109/SP46214.2022.00132
2025
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
which is by default not differentiable (and not even continu-
ous), to an equivalent but differentiable form. Speciﬁcally, a
word is represented by a probability vector, called the word
vector. A word vector has the size of the whole vocabulary.
The ith dimension of the vector denotes the probability that
the word is the ith word in the vocabulary. Therefore, the
sum of all the dimension values of a word vector is 1. The
discrete tokenization step in the original subject model is then
automatically replaced with a number of differentiable matrix
multiplications of word vectors. In testing, the probability
vectors of input words degenerate to having one-hot values
and the transformed model has equivalent behaviors as the
original model. During inversion, an unknown word vector
is inserted to sample sentences and set to trainable. With the
objective of causing misclassiﬁcation (on a set of samples), the
optimizer generates a distribution (in the word vector) denoting
the likelihood of words in vocabulary being a word in the
trigger. PICCOLO does not invert precise triggers, which may
have an unknown size and are difﬁcult to invert in general.
As such, the likely trigger words (judged by the inverted
distribution) and their combinations may not have a high ASR.
We hence develop a novel analysis that can determine if the
model is particularly discriminative for the presence of any of
the likely trigger words. If so, we consider the model trojaned.
Our theoretical analysis and empirical study (Section V-C)
show that trojaned models are particularly discriminative for
the presence of a subset of words in their triggers, whereas
clean models do not have such a property.
Our contributions are summarized as follows.
• We develop a novel trigger inversion based NLP model
backdoor scanning technique, which features an equiva-
lent transformation for NLP models that makes the whole
pipeline differentiable, a word level inversion algorithm
and a new word discriminativity analysis that allows
reducing the difﬁcult problem of inverting precise triggers
to generating a small set of likely words in trigger.
• To achieve more effective word level inversion, we de-
velop a new optimization method, in which tanh func-
tions are used to denote word vector dimension values for
smooth optimization and a delayed normalization strategy
is proposed to allow trigger words to have higher inverted
likelihood than non-trigger words.
• We evaluate PICCOLO (exPosIng Complex baCkdOors
in NLP transfOrmer models) on 3839 models, 1907
benign and 1932 trojaned, from three rounds of TrojAI
competitions1 for NLP tasks [40], [41] and two recent
dynamic backdoor attacks [8], [9]. These models have
7 different structures, including 5 based on the state-of-
the-art transformers BERT and GPT, and 2 on LSTM
and GRU. They are trojaned with 17 different kinds of
backdoors. We compare with two baselines, including T-
miner and GBDA. Our results show that PICCOLO can
1TrojAI is a backdoor scanning competition organized by IARPA [39]. It
has ﬁnished 7 rounds. Rounds 1-4 are for computer vision and rounds 5-7 are
for NLP classiﬁcation tasks. More can be found in Appendix IX-C.
(cid:10)(cid:24)(cid:22)(cid:20)(cid:24)(cid:17)(cid:1)(cid:12)(cid:17)(cid:27)(cid:30)(cid:20)(cid:15)(cid:17)
(cid:1)(cid:1)(cid:1)
(cid:13)(cid:26)(cid:22)(cid:25)(cid:14)(cid:16)
(cid:5)(cid:25)(cid:31)(cid:24)(cid:22)(cid:25)(cid:14)(cid:16)
(cid:11)(cid:25)(cid:20)(cid:28)(cid:25)(cid:24)(cid:17)(cid:16)
(cid:3)(cid:17)(cid:24)(cid:20)(cid:19)(cid:24)
(cid:2)(cid:11)(cid:3)(cid:9)(cid:12)(cid:5)(cid:10)(cid:11)(cid:8)(cid:4)(cid:11)
(cid:2)(cid:11)(cid:3)(cid:9)(cid:12)(cid:5)(cid:10)(cid:11)(cid:8)(cid:4)(cid:11)
(cid:2)(cid:1)(cid:6)(cid:17)(cid:31)(cid:1)(cid:4)(cid:22)(cid:17)(cid:14)(cid:24)(cid:1)
(cid:12)(cid:14)(cid:23)(cid:26)(cid:22)(cid:17)(cid:28)
(cid:1)(cid:2)(cid:3)
(cid:1)(cid:7)(cid:3)(cid:12)(cid:12)(cid:6)(cid:5)(cid:6)(cid:4)(cid:11)
(cid:11)(cid:25)(cid:20)(cid:28)(cid:25)(cid:24)(cid:17)(cid:16)(cid:1)(cid:9)(cid:25)(cid:16)(cid:17)(cid:22)
(cid:1)(cid:2)(cid:3)
(cid:1)(cid:7)(cid:3)(cid:12)(cid:12)(cid:6)(cid:5)(cid:6)(cid:4)(cid:11)
(cid:5)(cid:25)(cid:31)(cid:24)(cid:22)(cid:25)(cid:14)(cid:16)(cid:17)(cid:16)(cid:1)(cid:9)(cid:25)(cid:16)(cid:17)(cid:22)
(cid:11)(cid:7)(cid:4)(cid:4)(cid:10)(cid:8)(cid:10)
(cid:2)(cid:29)(cid:29)(cid:14)(cid:15)(cid:21)(cid:17)(cid:27)
Fig. 1: PICCOLO deployment scenario
(cid:5)(cid:17)(cid:18)(cid:17)(cid:24)(cid:16)(cid:17)(cid:27)
achieve around 0.9 ROC-AUC (an accuracy metric) for
all these attacks including the advanced dynamic attacks.
In contrast, GBDA can achieve 0.70 and T-miner 0.53.
Our solution allows us to rank number 1 in rounds 6
and 7 of TrojAI competitions and it is the only one
that reaches the round goal in all three rounds. Note
that in round 5, the triggers in the test and training sets
have substantial overlap such that training a classiﬁer
to capture the trigger features from the training set can
easily detect trojaned models in the test set. However,
such methods cannot be applied to other rounds or
other attacks. In contrast, PICCOLO is a general scanner
without requiring training. PICCOLO is publicly available
at https://github.com/PurduePAML/PICCOLO
Deployment. Figure 1 shows how the attack is launched and
how PICCOLO can be deployed to defend. The attacker has
full control of the training process. She can trojan an NLP
classiﬁcation model and publish it online. The defender scans
a (possibly trojaned) model from the wild using PICCOLO and
aims to determine whether the model is trojaned or not before
use. The defender has only the access to the model and a few
clean samples (20 samples per class).
II. BACKGROUND