title:Incremental quantitative verification for Markov decision processes
author:Marta Z. Kwiatkowska and
David Parker and
Hongyang Qu
Incremental Quantitative Veriﬁcation for Markov Decision Processes
Marta Kwiatkowska, David Parker and Hongyang Qu
Department of Computer Science, University of Oxford, Parks Road, Oxford, OX1 3QD, UK
Email: {marta.kwiatkowska, david.parker, hongyang.qu}@cs.ox.ac.uk
techniques are usually applied to check the correctness of
systems, quantitative veriﬁcation can be used to analyse
properties such as performance or reliability. A key strength
of quantitative veriﬁcation is that it yields exact results, as
opposed to, for example, the approximations produced from
simulation-based analysis techniques. In fact, for Markov
decision processes, which are the focus of this paper,
simulation-based techniques are inappropriate due to the
presence of nondeterminism.
Abstract—Quantitative veriﬁcation techniques provide an
effective means of computing performance and reliability
properties for a wide range of systems. However, the com-
putation required can be expensive, particularly if it has
to be performed multiple times, for example to determine
optimal system parameters. We present efﬁcient incremental
techniques for quantitative veriﬁcation of Markov decision
processes, which are able to re-use results from previous
veriﬁcation runs, based on a decomposition of the model into
its strongly connected components (SCCs). We also show how
this SCC-based approach can be further optimised to improve
veriﬁcation speed and how it can be combined with symbolic
data structures to offer better scalability. We illustrate the
effectiveness of the approach on a selection of large case studies.
Keywords-Quantitative veriﬁcation; incremental veriﬁcation;
Markov decision processes; performance analysis; probabilistic
model checking
I. INTRODUCTION
In almost all aspects of everyday life, we are reliant on
computerised systems: from the controllers found in cars
and planes, to the computer networks underlying our com-
munication, transport and ﬁnance systems. The prevalence
of such systems, combined with their increasing complexity,
means that effective methods to assure their reliability and
performance are essential. Model-based analysis techniques
provide an effective way of achieving this. The systems to be
analysed are often inherently stochastic: device components
may be failure-prone; messages sent across communication
networks may get lost or delayed; and wireless technologies
such as Bluetooth and ZigBee use randomisation. Thus,
models are typically probabilistic in nature; they are also
often extended with time and/or quantities for resources.
to analytical methods,
Approaches to the analysis of such models range from
discrete-event simulation,
to nu-
merical solution, and each have their own strengths and
weaknesses. In this paper, we focus on techniques that
exhaustively construct probabilistic models and then per-
form an exact analysis, based on numerical computation.
In particular, we consider quantitative veriﬁcation, which is
a formal approach for specifying and checking quantitative
properties of a system model. The model itself, typically a
Markov chain or Markov decision process, is systematically
constructed from a formal description in some high-level
modelling language. Whereas traditional formal veriﬁcation
Probabilistic temporal logics such as PCTL [1], [2] and
its variants are used to formally specify a wide range of
system properties, for example “the probability that a data
packet has been successfully transmitted within 0.5 sec-
onds”, “the probability that both sensors are simultaneously
non-operational” or “the expected time taken to execute the
protocol”. Probabilistic model checkers, e.g. PRISM [3],
have been used to apply quantitative veriﬁcation to a wide
range of systems, including communication protocols, secu-
rity protocols and dynamic power management schemes.
Tools such as PRISM also facilitate investigation of trends
or variations in quantitative properties, for example, to study
how changes in the failure probability of an individual
component affect overall system reliability, or to select
the optimal value for a system parameter to maximise
performance. Building on these ideas, quantitative runtime
veriﬁcation techniques have been proposed [4], [5], which
dynamically monitor a system’s behaviour and, through ex-
posed control interfaces, enforce the satisfaction of formally
speciﬁed constraints on performance or reliability at run-
time. As observed in [4], however, the need to repeat the
veriﬁcation process for a range of parameter values (in order
to conﬁgure system parameters appropriately) can incur
signiﬁcant time and memory overheads.
In this paper, we present incremental quantitative veriﬁ-
cation techniques, which offer improvements in efﬁciency
by re-using existing results when analysing a model
to
which minor changes have been made. We focus on Markov
decision processes (MDPs), which generalise (discrete-time)
Markov chains and are well suited to modelling systems such
as communication protocols. We target scenarios in which
a model needs to be analysed repeatedly and where the
probability of certain events occurring is subject to change.
The key idea behind our approach to incremental analysis
is to use a decomposition of the model into its strongly
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:25 UTC from IEEE Xplore.  Restrictions apply. 
978-1-4244-9233-6/11/$26.00 ©2011 IEEE359connected components (SCCs). Exploiting model structure
in this way has already been shown to be effective for an
isolated instance of MDP veriﬁcation [6] but the beneﬁts
for reducing work across multiple veriﬁcations has not been
considered. Furthermore, we present additional optimisa-
tions that can be applied when using an SCC-based analysis
of an MDP, incrementally or otherwise. First, we show how
to reduce the amount of precomputation performed: this is
an analysis of the underlying graph structure of the MDP that
needs to be executed before numerical solution is applied.
Secondly, we demonstrate how analysis of the decomposed
MDP is amenable to parallelisation. We have implemented
our techniques, using explicit-state data structures, in an
extension of PRISM. Using a selection of large benchmark
case studies, we demonstrate that our incremental veriﬁca-
tion techniques yield impressive speed-ups, and that these
are further enhanced by our optimisations.
When implementing veriﬁcation techniques in practice,
there is a need not just for improvements in terms of speed,
but also memory consumption. In fact, scalability is arguably
the bigger challenge of the two. For this reason, state-of-the-
art veriﬁcation tools such as PRISM often rely on symbolic
techniques, employing data structures such as binary deci-
sion diagrams (BDDs) or multi-terminal BDDs (MTBDDs).
These exploit the regularity that is often present in models to
provide compact storage and efﬁcient manipulation. In the
latter part of this paper, we present a symbolic implementa-
tion of SCC-based MDP veriﬁcation. The main difﬁculty is
the crucial step of identifying SCCs in a model. The classic
algorithm to do this, due to Tarjan [7], is not well suited to a
symbolic (BDD-based) implementation. Symbolic versions
have been proposed [8], [9] but are difﬁcult to adapt to
this setting: unlike Tarjan, they do not preserve information
about the topological order, and this information is expensive
to obtain afterwards with BDDs. We present a customised
version of Tarjan’s algorithm which resolves this problem.
Further experimental results show that, for large models, this
new approach is faster than the existing solution engines
in PRISM, with only a limited increase in memory usage
(linear in the size of the state space).
The remainder of this paper is structured as follows.
Below, we brieﬂy review some related work. Section II
covers background material on quantitative veriﬁcation for
MDPs, including the SCC-decomposition techniques of [6].
In Section III, we present our SCC-based optimisations for
graph-based computation and parallelisation. In Section IV,
we describe techniques for SCC-based incremental veriﬁ-
cation. Section V summarises our experimental results and
Section VI discusses our symbolic implementation of SCC-
based veriﬁcation. Section VII concludes the paper.
Related work. In addition to [6], SCC decomposition
was proposed for quantitative veriﬁcation in [10], but for
discrete-time Markov chains and with an emphasis on coun-
terexample generation. We are not aware of any work on
incremental veriﬁcation for probabilistic models. For non-
probabilistic systems, incremental techniques have been pro-
posed, e.g., [11]–[13]; these focus on speeding up state space
generation or checking of functional properties; quantitative
properties or numerical computation are not considered.
II. BACKGROUND
We let Dist(S) be the set of all discrete probability
distributions over S, i.e., the set of functions µ : S → [0, 1]
such that(cid:80)
s∈S µ(s) = 1.
A. Markov Decision Processes
Markov decision processes (MDPs) are widely used to
model systems that exhibit both probabilistic and nondeter-
ministic behaviour. Real-life systems are often inherently
stochastic, for example due to the presence of failures,
unpredictable delays or randomisation. In addition, nonde-
terminism may be essential, for example to capture concur-
rency, i.e. the possible interleavings of multiple components
operating in parallel, or underspeciﬁcation, where a proba-
bility or other parameter is not known or is not relevant.
Formally, an MDP is a tuple M = (S, s,T , r) where:
• S is a ﬁnite set of states,
• s ∈ S is the initial state,
• T : S → 2Dist(S) is a probabilistic transition function,
• r : S × Dist(S) → R(cid:62)0 is a reward function.
The transition probability function T maps each state s ∈ S
to a ﬁnite, non-empty set T (s) of probability distribu-
tions. There are two steps to determine the successor of
a state s in the MDP: ﬁrst, a distribution µ is chosen non-
deterministically from the set T (s); second, the next state
s(cid:48) is chosen randomly according to µ, i.e. the probability of
moving to each state s(cid:48) is given by µ(s(cid:48)). For simplicity,
we do not include action labels in MDPs. Distributions
are, however, augmented with rewards (sometimes called
impulse rewards).
µ0−→ s1
A path in an MDP, representing a possible execution of
the system being modelled, is a non-empty (ﬁnite or inﬁnite)
µ1−→ s2 . . . where si ∈ S,
sequence of the form: s0
µi ∈ T (si) and µi(si+1) > 0 for all i ≥ 0. We use ω(i)
to denote the (i+1)th state in the path ω, i.e. ω(i) = si,
and step(ω, i) is the distribution taken in state ω(i), i.e.
step(ω, i) = µi. We let P aths denote the set of all (inﬁnite)
paths starting in state s.
In order to reason formally about the probabilistic be-
haviour of an MDP M, we require the notion of adversary
(sometimes called strategy, policy or scheduler), which is
one possible resolution of the nondeterministic choices in
M. Formally, an adversary selects an available distribution
in each state based on the history of choices made so far.
An adversary A restricts the behaviour of the MDP to a
s ⊆ Path s. It also induces a probability
set of paths Path A
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:25 UTC from IEEE Xplore.  Restrictions apply. 
360s over the paths Path A
space [14] ProbA
denote the set of all possible adversaries for M.
B. Quantitative Veriﬁcation of MDPs
s . We use AdvM to
Usually, properties to be veriﬁed against MDPs are ex-
pressed in temporal
logics, such as PCTL [1], [2] and
LTL [15]. Performing veriﬁcation reduces to the computa-
tion of a few key properties of MDPs [2], [16]. The ﬁrst are
the minimum or maximum reachability probabilities, i.e. the
minimum or maximum probability that a path through the
MDP eventually reaches a state in some target set F ⊆ S,
quantiﬁed over all possible adversaries:
pmin
s
A∈AdvM
s
s
A∈AdvM
(F ) = inf
s (F ) = ProbA
s (F ), pmax
pA
s ({ω ∈ Path A
pA
(F ) = sup
s (F )
s | ∃i . ω(i) ∈ F})
where: pA
Secondly, we may require the minimum or maximum ex-
pected reward accumulated until target F ⊆ S is reached:
eA
emin
s (F )
s
A∈AdvM
rF (ω) dProbA
where: eA
s
where rF (ω) gives, for any path ω ∈ Path A
accumulated along ω until a state in F is reached:
s (F ) =(cid:82)
(cid:26)(cid:80)nF
i=1 r(ω(i−1), step(ω, i−1))
s , the total reward
s (F ), emax
eA
(F ) = sup
(F ) = inf
A∈AdvM
ω∈Path A
s
if ∃j . ω(j) ∈ F
otherwise.
∞
rF (ω) =
and nF = min{j | ω(j) ∈ F}.
s
For simplicity, in the remainder of this paper, we will
focus on the case of maximum reachability probabilities,
i.e. computing pmax
(F ), but our techniques adapt easily
to the case of minimum probabilities or expected rewards
(with the exception of Section III-A, which applies only to
reachability probabilities). Throughout the remainder of the
paper, we will assume a ﬁxed MDP M = (S, s,T , r) and
(F ) to just
target set F . For clarity, we will abbreviate pmax
. We use pmax to denote the vector of probabilities pmax
pmax
for all states s ∈ S.
s
s
s
Calculation of reachability probabilities (or expected re-
ward values) proceeds in two steps. The ﬁrst step, referred
to as precomputation, executes an analysis of the underlying
graph of the MDP to identify states that have reachability
probabilities of 0 or 1. Second, numerical computation is
performed to determine values for the remaining states; this
can be done with a variety of standard techniques, including
value iteration and linear programming. We describe this
process (for pmax
Precomputation. A graph-based analysis is used to partition
the state space S into sets Sn, Sy and S?, containing
states s for which the probability pmax
is 0, 1 or in (0, 1),
respectively. In fact, the analysis is performed using two
separate algorithms, P rob0A and P rob1E:
Sn = P rob0A(F ), Sy = P rob1E(F ), S? = S \ (Sy ∪ Sn).
) in more detail below.
s
s
Algorithm P rob0A [2] ﬁrst computes the set of states
with maximum probability greater than zero of reaching a
state in F . It then returns the complement of this set as Sn.
For each state s ∈ Sn, the probability of reaching F is zero
under any adversary A.
Algorithm 1 P rob0A(F )
1: R := F ; done := false
2: while done = false do
3: R(cid:48) := R ∪ {s ∈ S | ∃µ∈T (s) . ∃s(cid:48)∈R . µ(s(cid:48))>0}
4:
5: R := R(cid:48)
6: end while
7: return S\R
if R(cid:48) = R then done := true end if
Algorithm P rob1E [17] uses two nested loops to deter-
mine the set Sy of states s for which pA
s (F ) = 1 for some
adversary A. The outer loop identiﬁes states from which no
s (F ) = 1, and removes those states
adversary can make pA
from S. The inner loop collects states from which one cannot
reach a state in F without passing through a state already
removed from S.
Algorithm 2 P rob1E(F )
1: R := S; done := false
2: while done = false do
3: R(cid:48) := F ; done(cid:48) := false
while done(cid:48) = false do
4:
5:
:= R(cid:48) ∪ {s ∈ S | ∃µ ∈ T (s) . (∀s(cid:48) ∈
R(cid:48)(cid:48)
S . µ(s(cid:48)) > 0 → s(cid:48) ∈ R) ∧ (∃s(cid:48)∈R(cid:48) . µ(s(cid:48)) > 0)}
if R(cid:48)(cid:48) = R(cid:48) then done(cid:48) := true end if
6:
R(cid:48) := R(cid:48)(cid:48)
7:
end while
8:
if R(cid:48) = R then done := true end if
9:
10: R := R(cid:48)
11: end while
12: return R
Finally, we remark that, although P rob1E(F ) computes
all states for which pmax
s = 1, this is not strictly necessary. It
sufﬁces to use any set of states Sy that satisﬁes the condition
F ⊆ Sy ⊆ {s ∈ S | pmax
s = 1}. In contrast, for Sn, it may
be essential (e.g. when using linear programming) that the
set contains all states with pmax
Value iteration. One way to compute the probabilities pmax
for the remaining states s ∈ S? is to use value iteration,
an iterative numerical method which can approximate the
values up to some desired accuracy. In practice, this method
is widely used since it scales well to large MDPs.
s = 0.
s
Value iteration works by computing a sequence of vectors
pmax,k for increasing k. Initially, i.e. for the case k = 0, we
to 1 if s ∈ Sy and 0 otherwise. Then, the kth
set pmax,0
s
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:25 UTC from IEEE Xplore.  Restrictions apply. 
361iteration of computation is deﬁned, for each s ∈ S, as:
pmax,k
s
:=
max
µ∈T (s)
(cid:80)
s(cid:48)∈S
1
0
µ(s(cid:48)) · pmax,k−1
s(cid:48)
s ∈ Sy
s ∈ Sn
s ∈ S?.
The sequence of vectors pmax,k is guaranteed to converge
eventually to pmax. In practice, though, the computation
is terminated when a pre-speciﬁed convergence criterion is
met. One common approach is to check that the maximum
(absolute) difference between the corresponding elements of
successive vectors is below some ﬁxed threshold δ, i.e.:
maxs∈S |pmax,k
s
− pmax,k−1
s
| < δ.
Another is to check the maximum relative difference: