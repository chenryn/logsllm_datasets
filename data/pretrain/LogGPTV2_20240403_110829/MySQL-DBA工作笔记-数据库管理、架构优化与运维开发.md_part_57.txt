007
一提高吞吐量测试
failed
一些问题的解决，可能有些又不是压测相关的。
285s
310s
图9-4
3355
statements (current value: 16382)"
但是离性能压测的目标还有
lua:282:SQL API error
第9章MySQL性能测试|351
线程150
线程100
线程50
线程30
---
## Page 374
352丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
还需要细细地挖一挖。
global status 的方式来查看这个值的变化，这个参数的主要含义是应对瞬间新建的大量
参数 max_prepared_stmt_count 设置值为16382，是安装后的默认值。
--mysql-user=root
'sysbenchtest'user:'root'host:'localhost'
然后再次运行 200个线程，就可以看到没问题了，运行过程中我们可以使用 show
 set global max_prepared_stmt_count=30000;
结果抛出了同样的错误，这也就间接证明了问题和该参数无关，所以我恢复了原来的设
set global max_al1owed_packet=33554432;
到底是不是参数 max_allowed_packet 引起的呢，我们可以简单模拟一下。
而 packet 的参数设置为 4M的样子，也是默认值，如下：
mysql> show variables like 'max_prepared_stmt_count';
看起来两者关联不大，所以有些信息就会有一些误导了。根据错误的信息，当前的
2017-03-14T15:01:57.839185Z 346 [Note]
2017-03-14T15:01:57.839154Z 348 [Note]
MySQL的错误日志信息如下：
FATAL: 'thread_init' function failed: /usr/local/share/sysbench/oltp_
然后继续运行 sysbench 脚本，如下：
FATAL: mysql_stmt_prepare() failed
上面说到压测连接数 300 跑不上去了，这个问题具有典型性。sysbench 抛出的错误如下：
max_allowed_packet
Variable_name
max_prepared_stmt_
Variable_name
i
 /home/sysbench/sysbench-1.0.3/src/lua/oltp_read_write.lua
--mysql-port=3306 --mysql-socket=/home/mysql/s1/s1.sock
------
count
14194304
Value
Value|
|16382
1073741824
stateme
Aborted
Abortedconnection348todb：
(Gotan
communication packets)
 communication packets)
error reading
reading
346
-tables=10
16382)
common
---
## Page 375
of available memory, you can consult the manual for a possible OS-dependent bug
20倍。
了。所以我调整了参数值为100000，在900个线程时都没有任何问题。
行500个线程，30000这个参数值是肯定不够的。很快就验证了我的这个想法，抛出错误
FATAL: error 1135:Can't create a new thread (errno 11);if you are not out
下图9-5分别对应50、300、500个线程时候的TPS测试结果，QPS基本是TPS的
FATAL:unable to connect to MySQL server on socket'/home/mysql/sl/sl.sock'
然后我继续测试1000个线程的时候，发现跑不上去了，抛出了下面的错误。
通过简单的计算可以看出100个线程对应参数值9100，按照这个参数设置，我要运
·300个线程参数值为27300。
·200个线程参数值为18200；
mysql> show global status like '%stmt%';
执行300个线程的时候，抓取了一下这个参数值，发现已经快溢出了。
执行完200个线程后，继续提升到300个，打算依此类推，一直到1000个线程。
|  Variable_name 
mysql> show global status like 'Prepared_stmt_count';
所以自己简单做了个计算：
Prepared_stmt_count
Variable_name
Prepared_stmt_count
200
000
200
1600
1800
127300
Value
18200
图9-5
第9章MySQL性能测试丨353
线程500
一线程300
线程50
 aborting...
---
## Page 376
354|MySQL DBA工作笔记：数据库管理、架构优化与运维开发
情况，线程1000的指标似乎没有什么提升。
误和参数 nproc 还是有密切的关系，但是open files 目前还是1024，暂时还没有问题。
90-nproc.conf，对这个文件中做如下的配置，对所有用户生效。
启，因为资源设置还是旧的值，如何查看呢。可以使用 pidof 来查看/proc下的设置。
完全不会生效，需要制定具体的用户。
下图9-6 是当时测试的一个初步结果。可以看到线程数 50、500、1000的时候的 TPS
修改后重启MySQL服务即可生效，再次开启测试就没有问题了，说明这个地方的错
第二点就是在RedHat6中,我们其实需要设置另外一个参数文件/etc/security/limits.d/
Maxprocesses
比如下面的配置就不会生效，需要指定为root。
值得一提的是在RedHat 6中，在/etc/security/limits.conf 中设置用户为“*”的时候，
pipe size
再次可以引申出两点：第一点是修改了资源设置之后，已有的MySQL服务就需要重
open files
00
#ulimit-a
这个时候查看资源的使用情况如下：
ax
user processes
000
2.000
2500
#######
soft
hard
soft
nproc
150s
(512 bytes，
nproc
nproc
(kbytes,
##########
1024
M
15000
65535
(-x）unlimited
（-n）1024
-p)8
9-6
unlimited
1024
256589
15000
590s
线程1000
一线程
线程50
 processes
500
files
---
## Page 377
认是2，即两组，可以理解为Oracle中的两组redo日志。
版本中是8M，因为刷新频率很高，所以这个参数一般不需要调整。
些已有的优化方式改进。我们来简单说一下。
我们需要明确在 IO上的几点可能：一个是刷数据的效率，一个是redo的大小，还有一
是Lua 模板，读写兼有。压力测试的过程中生成了大量的 binlog，而对于 InnoDB 而言,
和很多网上高大上的结果有较大地出入。另外一点是默认参数层面的优化，我们使用的
很大的瓶颈。
数1000 的连接是没有问题了，但是测试结果还是让人不大满意，至少从数字上来看还是有
cMySQL 里有类似 Oracle 中 LGWR 的一个线程，在5.7版本中默认是16M，在早先
问题出在哪里了呢，首先机器的配置较差，
Free buffers
Dictionary memory
Total large memory allocated 26386366464
BUFFER POOL AND MEMORY
Pages flushed
mysql -e "show engine innodb status\G"lgrep -A12 "Log sequence"
我们抓取一个测试中的 InnoDB 的状态：
在这个压力测试中，频繁的写入 binlog 势必会造成对 redo的刷新频率极高。
mysql> show variables like 'innodb_log%';
Last
[ Variable_name
mysql> show variables like 'innodb_log_buffer_size';
线程数达到了1000，我们的基准测试也有了一个阶段性的成果，那就是最起码支持线程
sg
Variable_name
innodb_log_buffer_size
innodb_
innodb
innodb_
innodb
checkpoint
sequence
pool
1og_
log flushes,
.og
log
log
size
files
file
compressed _pages
up to 39564300915
checksums
1048599
allocated 555380
size
1572768
39562272220
39640782426
in_group
198.50 1og
slze
Value
Value
1167772161
50331648
ON
16777216
i/o's/second
8192
ON
这是个不争的事实，所以你看到的结果
第9章MySQL性能测试|355
---
## Page 378
356|MySQLDBA工作笔记：数据库管理、架构优化与运维开发
是否能够支撑高并发的压测，现在还需要打一个问号，我们在本小节进行测试验证。
9.1.4
可以看到基本是稳定的，而且远高于原来同样线程数的 TPS 指标。
蓝色的部分是原来10分钟内的TPS情况，红色的部分是2个多小时里的TPS 指标情况，
识别不到 redo 文件，会自动创建两个新的。
你可以先备份出来也行，然后修改参数文件的 redo 文件参数，启动 MySQL，当然开始时
是一点点地改进。
中的 redo 修改还是值得借鉴的。像 5.7版本中的 undo 已经可以截断，逐步地剥离出来，都
设置为1G 或者2G，最大可设置为 4G，
大概是70M左右。
下图 9-7 的结果很有代表性，是暂时修改 redo 为 100M 时的 TPS 指标情况。左下角
调整 redo 的大小还是尤其需要注意，在这一点上 MySQL 没准以后会有所改进，Oracle
怎么修改 redo 的大小呢，要正常停库，然后删除默认的两个 redo 文件，保险起见，
通过上面的测试，我们可以看到性能有了一个明显的提升，但是还存在一些隐患，
redo 文件设置为多大呢，其实没有一个绝对的概念，Percona 建议在压力测试中可以
mysql> select 39640783274-39562272220;
简单来做个计算，可以看到Log sequence number的值减去Last checkpoint at 的值，
139640783274-39562272220
压测MySQL-
500
1500
2000
500
500
蓝色
78511054
一定位压测瓶颈
，因为本身会直接影响到恢复的效率。
图9-7
红色
修改redo后的线程1000
线程1000
---
## Page 379
添加如下的部分即可。
/home/sysbench/sysbench-1.0.3/src/lua/oltp_read_write.lua: Too many open files)
/home/sysbench/sysbench-1.0.3/src/lua/oltp_read_write.lua: Too many open files)
aborting...
以换个姿势来写，那就是从 socket 连接改为常用的TCP/IP 方式的连接.
/home/sysbench/sysbench-1.0.3/src/lua/oltp_read_write.lua: Too many open files)
open files”来看已经不是数据库层面了，而是系统配置参数。
测试时长10秒钟，看看能否初始化1500.个连接。
焦点和重中之重。
做了调整，其中的一个重点逐渐落到了IO 的吞吐率上，redo日志的大小设置一下子成了
-mysql-user=perf_test
-table-size=5000000
-mysql-host=localhost
-mysql-
# cat /proc/^pidof mysqld^/limits
很明确了,那就是内核资源设置 nofile 调整一下。修改/etc/security/limits.d/90-nproc.conf 文件,
PANIC:
PANIC:
sysbench
PANIC:
 FATAL: unable to connect to MySQL server on socket '/home/mysql/s1/s1.sock'
是不是支持的 socket 数的限制呢，我们已经调整了 process 的值。上面的命令我们可
PANIC:
没想到就跟约好似的，抛出了如下的错误。注意这里的错误根据关键字“Too many
sysbench
重启MySQL后可以看到设置生效了。
可以看到错误就显示不同了，但是已经能够看出意思来了。
首先一点是我们能够突破1000连接的大关，先用下面的脚本来进行一个初步的测试，
当然这次的测试中，
在上一次的基础上，我们保证了能够满足短时间内1000个连接的冲击，从各个方面
1.先突破1000连接资源设置的瓶颈
-user=root
unprotected
unprotected
unprotected
ha
unprotected
soft
soft
rd
perf
/home/sysbench/sysbench-1.0.3/src/lua/oltp_read_write.lua
nofile
--threads=1500
我的思路还是保持性能持续的增长，边调整，边优化。
--threads=1500"
test
error
error
error
error
--mysql-port=3306
65535
65535
65535
--mysql-db=sysbenchtest