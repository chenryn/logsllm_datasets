simplicity of our analysis, we assume that oij ∈ {−1, 0, 1},
where 1 denotes a highly positive opinion, -1 denotes a highly
negative opinion and 0 denotes no or a neutral opinion. To
this end, we set oij = −1 if idsij  0.5. The probabilities of the IDS for
a false positive and a false negative are ep := Pr(oij = 1 | j
is bad) and en := Pr(oij = -1 | j is good), respectively. For
our analysis of the judgement system in Section 5 we combine
false positives and negatives to the (average) IDS error rate e
:= Pr(oij = -1 | j is good) · Pr(j is good) + Pr(oij = 1 | j is
bad) · Pr(j is bad). In our game-theoretic analysis in Section
6 only en will be relevant. We model the network to run in
rounds r, i.e. each round r represents a ﬁx time interval of the
network lifetime. As nodes collect more evidence about other
nodes from round to round, the conﬁdence in the accuracy
of the IDS increases, i.e. e decreases; e can be modeled as a
function e(r) that is monotonically decreasing. We note that
the increasing conﬁdence in the accuracy of the IDS might not
be justiﬁed if a node is surrounded by malicious nodes that
spoof the evidence about their own or other nodes’ behaviour.
Such situations account for the probability that nodes or the
judgement system make a wrong decision. However, assum-
ing a global majority of benign nodes in the network, our
analysis shows that our scheme works in favour of the good
nodes in the network. We label the number of identiﬁers (and
corresponding private keys) that the good node’s hold by n,
and by m for the malicious nodes, respectively.
In response to the detection of malicious behavior, a node
i issues (and signs) a suicide note when it judges its amassed
evidence to be suﬃcient to revoke another node j. This note
represents an instruction to the network to rescind any privi-
leges associated with the keys of both parties (the revocation
instigator and the revocation target). Once this note is is-
sued, the node waits for a TA to become available (either via
a back-link to an oﬀ-site location or the TA physically enters
the communication rage of a suiciding node) and forwards the
suicide note to the TA. Once received, the TA can request fur-
ther opinion values about i and j, randomly choosing nodes
in i’s and j’s neighborhood as witnesses3.
From all collected opinions, the TA needs to assess the
potentially incomplete and contradictory evidence and make
a ruling. The more nodes’ opinions are collected, the higher
is the probability that the TA makes the right judgment. In
case the TA remains uncertain about the justiﬁability of the
suicide, it can annul the suicide without giving any reward or
punishment. Alternatively, the TA could potentially engage
in remedial action with the node(s) and reset their software
via an update if the TA believes the suicides to be as a result
of misconﬁguration. We do not, however, consider this issue
of reconﬁguration further in this paper.
If, the TA ultimately deems the suicide justiﬁable, the TA
rewards the revoking node. Table 1 shows the probability
parameters for the TA’s possible decisions, where
pt, pa, pf , qt, qa, qf ≥ 0, pt + pa + pf = 1 and qt + qa + qf = 1.
Table 1: Decision probabilities.
TA’s Judgment:
good
undecided
Reality:
good
bad
pt
qf
pa
qa
bad
pf
qt
4.2 Costs and Beneﬁts
We represent the cost for a node to engage in karmic-suicide
(either as the revoker or revoked) as the loss of a single key
for its identiﬁer, i.e. a cost of 1. We set the beneﬁt for
resurrection to 1+b, 0  0 means that x is supposed to be greater than 0.)
Event
honest node does nothing
honest node revokes honest node
honest node revokes malicious node
malicious node does nothing
malicious node revokes honest node
malicious node revokes malicious node
proﬁt honest node
proﬁt adversary
!
 0
0
1) bpf − pt
2) bqt − qf
0
−pf
0
“
3)
m+b
n−1
− m
n
”
0
> 0
 1
qf
b
n
(B) pt >
n−1
no requirement
1 + m
n
´
pf
Consequently, the resulting requirements for our TA’s accu-
qf (to give honest nodes positive incentive
pf (to prevent malicious
racy are that qt > 1
b
to revoke) and pt >
nodes from abusing the scheme for their own beneﬁt).
1 + m
n
n
n−1
´`
`
´
5. THE TA’S JUDGEMENT MECHANISM
In this section we introduce our judgment mechanism that
combines opinions from multiple nodes and labels a revoked
node j as being good, bad or undecided. The judgment sys-
tem takes as input a matrix onode×node, where -1 ≤ oij ≤ 1
denotes the opinion of node i on node j as deﬁned in Section
4.1. In this section we ﬁrst assume that all the opinions are
error free (namely, the IDS system is perfect). We will then
extend our analysis to accommodate false positives and false
negatives in the IDS system later in this section.
5.1 Judgment System Setup
The judgment system uses an oﬀ-the-shelf k-means cluster-
ing algorithm [12] to partition the set of nodes into two clus-
ters G and B (the good and the bad respectively 5 ) and a N−1
dimensional hyper-plane P that maximally separates the two
clusters. Each node i has an opinion vector (oi1, . . . , oi(n+m))
about nodes 1, . . . , n + m and the opinions about a node j
are associated with the vector oj = (o1j , . . . o(n+m)j).
Ini-
tially nodes have no opinions about other nodes, but during
the lifetime of the MANET nodes will partially or even com-
Pn+m
pletely ﬁll their opinion vector with non-zero entries. We
deﬁne the diﬀerence in opinions about nodes i and j to be
k=1 |oki − okj|. The key intuition here is that the good
nodes will typically be close to one another in this vector
space and will thus be clustered together; while the bad nodes
will typically be far from the cluster of good nodes. The
judgment system draws conclusions on a node j based on a
system-deﬁned threshold thr as follows. We use d(oi, P ) to
denote the distance of vector oj from the plane P .
8>:good
bad
undecided d(oj , P ) ≤ thr
j ∈ G ∧ d(oj , P ) > thr
j ∈ B ∧ d(oj, P ) > thr
decision =
We assume that each bad node misbehaves with a ratio of
α, uniformly and randomly distributed over the life-time of
the network. The bad nodes strategically choose α in order to
maximize their expected long term proﬁt. We note that the
higher the attack intensity α, the higher is qt (see Table 1),
and thus the bad node’s risk of being revoked. The probabil-
ity qt for correctly categorising a node as bad, can therefore
be interpreted as a function qt(α). The probability that a bad
node is not categorized as bad by the judgment system after
r rounds is (1− qt(α))r. At each round r, the proﬁt for a bad
node is assumed to be proportional to its attack intensity α,
but degrades over time such that the expected proﬁt gained
at round r is α · λr · (1 − qt(α))r. 0 < λ < 1 is a discount
factor that provides a practically ﬁnite time horizon, since
limr→∞ λr = 0. Discount factor based model is a common
approximation for ﬁnite horizon problems in decision theory
5Assuming that the number of good nodes is larger than the
number of bad nodes, the larger of the two clusters is consid-
ered ‘good’.
295Figure 1: Reward and attack in-
tensity.
Figure 2: Optimal attack inten-
sity.
Figure 3: Optimal expected re-
ward.
Figure 4: Good node decision
probabilities.
Figure 5: Bad node decision prob-
abilities.
P∞
r=1 α · λr−1 · (1 − qt(α))r−1 =
[29] and has been used in security domains to model the util-
ity for denial of service attacks [19]. Hence, the total proﬁt
of a node over the network life-time is:
R =
We note that at low attack intensity, the bad nodes behave
similar to good nodes; hence, the smaller the attack intensity
α, the harder it becomes to distinguish the bad nodes from
the good nodes. Fortunately, as the attack intensity tends to
zero, so does the expected proﬁt R.
1−λ(1−qt (α))
α
We assume that the good nodes report honest opinions
about other nodes in the network. However, the bad nodes
manipulate their opinions with the goal of making their opin-
ion vector similar to that of the good nodes. Formally, we
postulate that the goal of the bad nodes is to minimize the
diﬀerence in the average distance between a bad node and
goods node to that of the average distance between good
nodes. One can show that the optimal strategy for a bad
node j is to choose oij uniformly and randomly from the
set {-1, 1}. Let us suppose that a bad node picks oij = 1
with probability β; and -1 with probability 1− β. Under this
strategy, the average distance between any two good nodes is
4β(1 − β)m and average distance between a bad node and a
good node is 2α · n + 4β(1 − β)m. Now, the optimal choice
of β should minimize the ratio
4β(1−β)m . Equivalently,
the goal is to maximize β(1 − β); hence β∗
Under the optimal setting β∗
, the average distance between
two good nodes is 2α · n and that between a good and a bad
node is 2α·n + 2·m. Evidently, the clustering algorithm may
be ineﬀective in partitioning the good and the bad nodes into