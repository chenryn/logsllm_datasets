### 3.1 Visualization of Tracking Data

The user interface highlights the websites visited by the user, with a particular focus on those with a high or low number of trackers. For each site, a detailed page is provided, which includes:
- Inferred interests based on the user's browsing behavior.
- Trackers encountered on the site.
- Pages visited within the site.
- A bar chart displaying the frequency of visits over time.

#### Tracker Network
As an alternative visualization method, one variation tested in our field study included a customized version of Mozilla’s Lightbeam extension, styled to match our visual design (see Figure 11 in the appendix). Lightbeam uses a network graph to illustrate the connections between trackers and the sites visited by the user.

#### In-Page Overlay
Inspired by Ghostery, another variation in the user study displayed an overlay in the lower-right corner of the page. This overlay listed the trackers observed on each page (see Figure 12 in the appendix).

### 3.2 User Interviews

To refine the usability of our extension, we conducted 13 interviews with participants who had no prior experience with the tool. These IRB-approved interviews were conducted in the final months of development. The interview script is provided in Appendix A.1.

Participants were recruited using a library-intercept model, where researchers approached individuals in the lobby of a large public library to ensure a diverse participant pool. Each interview lasted approximately 30 minutes, and participants were compensated with a $10 Amazon gift card. To minimize social desirability bias, participants were informed that the moderator was independently hired to evaluate the tool.

Using a provided laptop, participants explored a working prototype of Tracking Transparency populated with simulated data. They were asked to think aloud as they explored and to suggest improvements [96]. We iteratively improved the extension based on their feedback until no new suggestions were being made. Key changes included converting lists and tables into charts, using symbols to denote common concepts, adding tooltips for clarification, and ensuring that elements expected to be clickable were indeed links.

### 3.3 Inferring Topics of Web Pages

One of our primary goals was to study how users would react to seeing not just the trackers present on each page (as most existing privacy tools do), but also the potential interests (e.g., yoga, classical music) that companies might infer from their browsing data. To our knowledge, no current user transparency tools map specific browsing behaviors to potential inferences, so we approximated this functionality in Tracking Transparency. Our aim was to enhance awareness of third-party tracking by providing meaningful approximations of inferences that advertisers could make about a user.

Advertisers' actual inferencing methods are proprietary and not publicly disclosed. It is unclear whether they use complex multi-step inferences, simple topic generalizations, or something else entirely. Without cooperation from these companies, it is impossible to fully understand or recreate their processes. Instead, we focused on making user-intelligible, principled connections between browsing activity and potential inferences.

Other research has found substantial data sharing between trackers [10]. However, our tool does not attempt to capture this sharing, instead focusing on user-centered communication. Our design emphasizes the aggregate data collected rather than specific pages or interests. While our estimates may not perfectly capture what trackers learn from this data, we believe they provide users with helpful context through specific examples of possible inferences.

For privacy reasons, all logic was client-side, with no information about the pages users visit transmitted externally. The topics we assigned needed to reflect one possible way trackers could reasonably assign ad-interest categories to users. Since there can be significant variation in content across pages within a domain, we assigned topics to individual pages rather than entire sites.

To produce plausible inferences while respecting user privacy, we used a pre-trained, client-side topic-modeling algorithm to determine potential ad-interest categories from each page's content. The extension uses observed tracker activity to link topics together and display inferences a tracker could have made.

#### 3.3.1 Topic Modeling

When a user visits a web page, the extension extracts the visible text and HTML header metadata. This data is preprocessed to remove stop words and non-English words, and each word is stemmed [76]. If the page has at least 200 stemmed words, the algorithm assigns a topic to the page. We use 1,932 hierarchical topics (e.g., Games→Board Games→Chess) taken verbatim from Google AdWords categories [38].

We created a training corpus using the top 10 Wikipedia articles for each topic, excluding articles about specific entities (people, places, products) for generalizability. Each article was preprocessed as described above, without a keyword threshold. To assign topics, we tested both keyword matching and deep learning approaches.

**Keyword Matching:**
We identified the 1,000 most relevant words for each hierarchical topic using three algorithms: term frequency-inverse document frequency (TF-IDF) [18], TextRank [61], and Latent Dirichlet Allocation (LDA) [12]. We computed a weighted matching score between the keywords extracted from a page and the top 1,000 words for each topic, assigning lower weights to lower-ranked words. The highest-scoring topic was the output. We also experimented with Word2Vec embeddings [69] to assign scores via semantic-similarity-based matching. The topic with the highest cosine similarity between embeddings of its top 1,000 words and the extracted keywords was the output.

**Deep Learning:**
We trained two LSTM neural network models [40] using each word's Word2Vec embedding. Both models used one layer with 128 cells. In the LSTM model, we kept the 150,000 most frequent words from the Wiki-corpus (covering > 99% of all occurrences). In the LSTMsmall model, to reduce storage size and computation, we kept only the 10,000 most frequent words (> 95% of all occurrences). The storage size of LSTMsmall (7.5 MB) is half that of LSTM (13.0 MB).

#### 3.3.2 Evaluation

To select an inferencing algorithm for the extension, we ran two IRB-approved evaluation studies. We compared nine algorithms: TF-IDF, LDA, Textrank, their Word2Vec variants, two LSTM models, and random topic assignment as a control. We generated our test data using the top 10,000 sites from Alexa [2]. We loaded each domain and clicked two random links to ensure a variety of page types. Pages that were not in English, contained under 200 keywords, or took more than 20 seconds to load were removed programmatically, resulting in a list of 5,980 pages. We then manually removed pages that contained only terms of service or privacy policies, adult content, or were mostly blank, resulting in a final test set of 2,700 pages.

**Accuracy Evaluation:**
We showed 187 MTurk workers a randomly selected page from our test set and the associated topic from a randomly selected inferencing algorithm. Participants rated on a five-point Likert scale whether the topic accurately described the page. Each participant rated 9 topic-page pairings, resulting in a total of 1,683 ratings. This IRB-approved study took about 30 minutes, and participants were compensated $5.00.

Accuracy ratings differed significantly across algorithms (Kruskal-Wallis, H = 262.3, p < .001). Dunn’s multiple-comparison test with Bonferroni correction found that LDA and its Word2vec variant did not differ significantly from random assignment (p = .390 and p = .330, respectively). There were few significant differences among the remaining six algorithms, so we focused on the three with the highest accuracy-agreement ratings: LSTMsmall (54.0% agreement), LSTM (46.0%), and TF-IDF (45.5%).

**Precision and Performance Evaluation:**
Since AdWords categories are hierarchical, inferences in narrow subcategories present a potential tradeoff between accuracy and precision. To examine this tradeoff, we conducted an additional survey of 54 MTurk workers. Participants rated the accuracy and precision of topic-page pairings assigned using the three finalist algorithms. Additionally, we randomly chose one of four display modes: Top category only, TopTwo categories only, the full hierarchy less one level (CutOne), and the Full unedited topic hierarchy. Each participant rated 12 pairings, resulting in a total of 648 ratings. The survey length and compensation were the same as previously.

LSTMsmall-Top, LSTM-Top, and TF-IDF-CutOne had the highest participant agreement for both accuracy (66.7%, 61.1%, and 61.1%, respectively) and precision (55.6%, 48.1%, and 50.0%, respectively). Given these similar results, we next considered computational overhead. We implemented all three algorithms in our browser extension in Chrome on a machine with a 2.9GHz Intel Core i7 quad-core processor and 16GB RAM. We benchmarked the assignment of topics to our 2,700-page test set. Median runtimes were 23.4 seconds and 31.3 seconds for LSTMsmall-Top and LSTM-Top, respectively, versus 39 ms for TF-IDF-CutOne. Due to its comparable accuracy with far less computation, we use TF-IDF-CutOne in the extension.

Our goal for topic modeling was to approximate the kinds of inferences trackers might make and thus improve user understanding of the tracking ecosystem. While our final model sometimes assigns an incorrect topic, a model that is correct more often than not is still useful for simulating an inferencing algorithm, informing users, and assessing their reactions to this transparency effort. This outcome aligns with real-world tracking, where prior work has documented the poor accuracy of behavioral profiles built by online advertisers [74, 89], with one study finding only 27% of inferences were strongly relevant [9]. At least 40% of attributes sold by data brokers may be inaccurate [93]. Thus, our approximation algorithm appears comparable in accuracy to the methods used by companies that invest substantial resources in fine-tuning tracker data collection and inference models.

### 3.4 Sensitivity of Interest Categories

On the Interests tab, users can filter the chart to highlight topics labeled as more or less sensitive. We quantified sensitivity using an IRB-approved MTurk study, asking participants about their comfort with a specific topic being inferred and used for personalization. Our study was based on Dolin et al. [23], abbreviating their script and expanding the scope to cover all 1,124 categories used in Google AdWords (excluding world localities).

We obtained 583 responses, each addressing 10 randomly selected categories, from 470 crowdworkers (participants were allowed to take the survey multiple times). Participation took about 15 minutes, and participants were compensated $3.00.

Similar to Dolin et al., we found a spectrum of comfort with targeting based on different interest categories. A small number of topics were strongly sensitive or non-sensitive, but most were somewhere in the middle. From this data, we generated a list of 1,124 topics ranked by mean agreement that “I would be comfortable with a company personalizing my web experience based on an inference about my level of interest in [topic]” on a seven-point Likert scale, which formed the basis of sensitivity filtering in the Interests tab.

### 4. Field Study Methodology

We conducted a field study to evaluate how transparency in the form of the Tracking Transparency prototype impacts users’ knowledge and attitudes about tracking and inferencing. Participants were randomly assigned to install one of six variants, each with different UI components. At installation, participants completed a pre-usage survey. After one week of normal browsing, we prompted them to explore the extension and complete a post-usage survey.

All participants were recruited through Amazon’s Mechanical Turk (MTurk). Participants needed to be located in the U.S., be at least 18 years old, and have a 95% HIT approval rating. Because the extension was built for Google Chrome and Mozilla Firefox, we required participants to regularly use at least one of them. The IRB approved the study, and the extension itself was reviewed by Google and Mozilla following their standard procedures.

#### 4.1 Study Conditions

To gauge the impact of our key transparency features in comparison to state-of-the-art privacy tools’ approaches, we randomly assigned participants to one of six versions (conditions) of Tracking Transparency. For consistency and comparability, all conditions had the same visual design, branding, text, and UI elements other than the differences being tested, as described below and in Table 1.

- **Control:Static:** Contains only static text explaining targeted advertising and privacy.
- **Control:Browsing Only:** Provides the dashboard interface with information about browsing history but no data about tracking.
- **Current:Trackers:** Simulates Ghostery and similar extensions, containing a list of trackers in the toolbar popup but no access to the dashboard interface.
- **Current:Connections:** Provides a visually restyled version of Mozilla Lightbeam with no other personalized data.
- **Longitudinal:Trackers:** Contains most interface components, providing longitudinal information about trackers and browsing but not showing potential interests inferred.
- **Longitudinal:Interests:** The full interface and data described in Section 3, including potential interests inferred.

#### 4.2 Pre-Usage Survey

Participants were asked to install the extension in Chrome or Firefox. Following installation, but before interacting with the extension, participants completed a pre-usage survey. We asked about participants’ demographics, browsing behaviors, use of relevant browser extensions, and experiences with online shopping and ads. To understand how participants’ knowledge and attitude changed after using the extension, we asked a series of questions in the pre-usage survey that were repeated verbatim a week later in the post-usage survey. These items included seven statements concerning attitudes about targeted ads, as well as knowledge statements about 15 types of data and 3 broad methods that might possibly be used for targeting. Participants rated their agreement with the former on 7-point Likert scales and the likelihood of the latter on 7-point likelihood scales (“very unlikely” to “very likely”).

The repeated section also included the awareness and collection sub-scales of the Internet Users’ Information Privacy Concerns (IUIPC) scale [55], as well as questions that asked participants to quantify tracking (e.g., the number of trackers they encounter). Upon completion of this survey, designed to take 15 minutes, we compensated participants $3.00 and reminded them to keep the extension installed for 7 days. On days 4–6, the extension sent browser notifications to encourage participants to explore it.

#### 4.3 Post-Usage Survey

A week after installation, we sent participants a link to the post-usage survey via MTurk. We asked them to “spend a few minutes exploring the extension before beginning the survey,” asking two questions about what they saw to encourage them to do so. We then asked four open-ended questions about the information in the extension: “new information,” “information you already knew,” “surprising information,” and what questions they had. We also asked participants to respond to six potential changes in behavioral intention (e.g., “Compared to before you used the extension, how likely are you to use a browser’s private browsing mode now?”) on 7-point scales from “much more likely” to “much less likely.” For four potential tradeoffs (e.g., an internet that is free but has tracking versus an internet that costs money but does not have tracking), participants rated which they would choose.

As mentioned above, we repeated the batteries of questions concerning attitudes and knowledge, the IUIPC, and quantification of tracking. We also asked the standard System Usability Scale (SUS) [14]. Upon completion of the post-usage survey, designed to take 20 minutes, we compensated participants with a $7.00 bonus payment on MTurk. This larger compensation encompassed both the week of keeping the tool installed and completion of the post-usage survey.

Both survey instruments, which are included in Appendices A.2–A.3, were refined through pilot testing and cognitive interviews.

#### 4.4 Participant Privacy

To protect participant privacy, the extension did not report any personally identifiable information. On the participant’s own computer, the extension kept a full database of all page visits, trackers encountered, and interest categories, which was used to power the extension’s visualizations. This data was stored locally in the browser extension’s sandboxed storage and was not accessible to other extensions or web pages. To enable analysis of aggregate data across all users while preserving participant privacy, we collected an anonymized version of the database, with all URLs and page titles hashed with a participant-specific salt generated on the participant’s computer and never sent to the researchers. We also collected clickstream data for activity in the dashboard. All data was associated with an anonymous identifier generated by the extension and never associated with the participant’s Mechanical Turk ID. The extension did not operate in private browsing mode.

Participants were informed about the data collection through both a consent form and a privacy policy. The inclusion of longitudinal visualizations like those in Tracking Transparency in tools intended for wide distribution will require careful communication to users about the potential for privacy leaks on shared devices. To enable longitudinal visualizations, such tools must store a detailed history of a user’s web browsing. These extensions should clear their own data when users clear their browser’s history, and require additional design considerations around shared devices.

#### 4.5 Analysis Methods and Metrics

For quantitative data, we conducted hypothesis tests with α = .05, choosing the test based on the type of data. Questions asked only post-usage, such as behavioral intentions after using Tracking Transparency, elicited responses on scales (e.g., Likert scales).