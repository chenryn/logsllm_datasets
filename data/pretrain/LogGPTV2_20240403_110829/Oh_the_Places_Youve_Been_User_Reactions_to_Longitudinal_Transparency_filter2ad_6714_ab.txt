the user has visited, highlighting those with many or few trackers.
Detail pages for each site visited highlight interests potentially
Figure 3: Tracking Transparency’s Interests tab.
inferred and trackers encountered on the site, the pages visited
within a site, and a bar chart of the visits over time.
Tracker network. As an alternative means of visualizing tracking,
one variation tested in our field study included a forked copy of
Mozilla’s Lightbeam extension [62] styled to match our visual de-
sign (Figure 11 in the appendix). Lightbeam illustrates tracking via
a network graph of connections between trackers and sites visited.
In-page overlay. Inspired by Ghostery [19], one variation in the
user study showed an overlay in the lower-right corner. It listed
the trackers observed on each page (Figure 12 in the appendix).
3.2 Interviews
To refine our extension’s usability, we conducted 13 interviews
with participants who had no prior experience with the extension.
These IRB-approved interviews were conducted in the final months
of development. Appendix A.1 contains the interview script.
Following library-intercept recruitment models, researchers ap-
proached individuals in the the lobby of a large public library to
maximize participant diversity. Each interview took about 30 min-
utes, and compensation was a $10 Amazon gift card. To minimize
social desirability bias, participants were told that the moderator
had been independently hired to evaluate the tool. Using a pro-
vided laptop, participants explored a working prototype of Tracking
Transparency populated with simulated data. We asked participants
to think aloud as they explored and to suggest improvements [96].
Using these insights, we iteratively improved the extension until
participants were no longer providing novel feedback. Because
participants obtained the most value from graphics (e.g., bar charts),
we converted lists and tables into charts as much as possible. We
also used symbols to denote common concepts, added tooltips for
clarification, added succinct page headers, and trimmed text. We
also turned elements participants expected to be clickable into links.
3.3 Inferring Topics of Web Pages
One of our main goals was to study how users would react to seeing
not just the trackers present on each page (the visualization used
in most existing privacy tools), but instead the interests (e.g., yoga,
classical music) companies could perhaps infer from browsing data.
Because, to our knowledge, no current user transparency tools map
specific browsing behaviors to potential inferences, we approxi-
mated this functionality in Tracking Transparency. We aimed to
improve awareness of third-party tracking with meaningful approx-
imations of inferences that advertisers could make about a user.
Advertisers’ actual inferencing methods are not public, but rather
trade secrets. It is an open question whether they use complex
multi-step inferences, simply generalize by topic, or do something
else entirely. These processes cannot be fully understood or recre-
ated without cooperation from companies. Instead, we strove to
make user-intelligible, principled connections between browsing
activity and potential inferences.
Other work has found substantial data sharing between track-
ers [10]. Our tool does not attempt to capture this sharing, instead
focusing on user-centered communication. Our design emphasizes
the aggregate data collected, rather than specific pages or interests.
While our estimates are unlikely to perfectly capture what trackers
learn from this data, we believe they provide users with helpful
context through specific examples of possible inferences.
For privacy reasons, we wanted all logic to be client-side, with
no information about the pages users visit transmitted externally.
The topics we assigned needed to reflect one possible way trackers
could reasonably assign ad-interest categories to users. As there
can be substantial variation in a domain’s content across pages, we
wanted to assign topics to individual pages, not entire sites.
To produce plausible inferences while also respecting user pri-
vacy, we employed a pre-trained, client-side topic-modeling algo-
rithm to determine potential ad-interest categories from each page’s
content. The extension uses observed tracker activity to link topics
together and display inferences a tracker could have made.
3.3.1 Topic Modeling. When a user visits a web page, the exten-
sion extracts the visible text and HTML header metadata. These
are preprocessed to remove stop words and non-English words,
stemming each word in the resulting set [76]. If the page has at
least 200 stemmed words, the algorithm uses the extracted text
to assign a topic to the page. As the possible output labels, we
use 1,932 hierarchical topics (e.g., Games→Board Games→Chess)
taken verbatim from Google AdWords categories [38].
Using Wikipedia, we created a training corpus of the top 10
articles for each topic, with articles about specific entities (people,
places, products) removed for generalizability. We preprocessed
each article as above, but without a keyword threshold. To assign
topics, we tested keyword matching and deep learning.
Keyword matching. We identified the 1,000 most relevant words
for each hierarchical topic with three algorithms: term frequency-
inverse document frequency (TF-IDF) [18], TextRank [61], and La-
tent Dirichlet Allocation (LDA) [12]. We compute a weighted match-
ing score between the keywords W extracted from a page to the top
1,000 words for each topic T , associating lower weights to lower
is the ith
ranked words: ScoreT = 1,000
1
i where KT
i

i =0
i ∈W
KT
most relevant word for topic T. The highest-scoring topic is the
output. We also experimented with Word2Vec embeddings [69] to
assign scores via semantic-similarity-based matching. The topic
with highest cosine similarity between embeddings of its top 1,000
words and W was the output.
Deep learning. We trained two LSTM neural network models [40]
with each word’s Word2Vec embedding. In both cases, we used one
layer with 128 cells. In LSTM, we kept the 150,000 most frequent
words from the Wiki-corpus (includes > 99% of all occurrences). In
LSTMsmall , to reduce storage size and computation, we kept only
the 10,000 most frequent words (> 95% of all occurrences). The
storage size of LSTMsmall (7.5 MB) is half that of LSTM (13.0 MB).
3.3.2 Evaluation. To select an inferencing algorithm for the exten-
sion, we ran two IRB-approved evaluation studies. We compared
nine algorithms: TF-IDF, LDA, Textrank, their Word2Vec variants,
two LSTM models, and random topic assignment as a control. We
generated our test data using the top 10,000 sites from Alexa [2].
We loaded each domain and clicked two random links so the test
set would contain a variety of types of pages. Pages that were not
in English, contained under 200 keywords, or took more than 20
seconds to load were programmatically removed, resulting in a list
of 5,980 pages. We then manually removed pages that contained
only terms of service or privacy policies, contained adult content,
or were mostly blank, resulting in a final test set of 2,700 pages.
Accuracy evaluation. We showed 187 MTurk workers a randomly
selected page from our test set and the associated topic from a
randomly selected inferencing algorithm. Participants rated on a
five-point Likert scale whether the topic accurately described the
page. Each participant rated 9 topic-page pairings, resulting in
a total of 1,683 ratings. This IRB-approved study took about 30
minutes. Compensation was $5.00.
Accuracy ratings differed significantly across algorithms (Kruskal-
Wallis, H = 262.3, p < .001). Dunn’s multiple-comparison test with
Bonferroni correction found that LDA and its Word2vec variant
did not differ significantly from random assignment (p = .390
and p = .330, respectively). There were few significant differences
among the remaining six algorithms, so we focused on the three
for which participants’ accuracy-agreement ratings were highest:
LSTMsmall (54.0% agreement), LSTM (46.0%), and TF-IDF (45.5%).
Precision and performance evaluation. As AdWords categories
are hierarchical, inferences in narrow subcategories present a po-
tential tradeoff between accuracy and precision. To examine this
tradeoff, we conducted an additional survey of 54 MTurk work-
ers. Participants rated the accuracy and precision of topic-page
pairings assigned using the three finalist algorithms. In addition,
we randomly chose one of four display modes: Top category only,
TopTwo categories only, the full hierarchy less one level (CutOne),
and the Full unedited topic hierarchy. Each participant rated 12
pairings, resulting in a total of 648 ratings. The survey length and
compensation were the same as previously.
LSTMsmall -Top, LSTM-Top, and TF-IDF-CutOne had the highest
participant agreement for both accuracy (66.7%, 61.1%, and 61.1%,
respectively) and precision (55.6%, 48.1%, and 50.0%, respectively).
As these results are similar, we next considered overhead. We im-
plemented all three algorithms in our browser extension in Chrome
on a machine with a 2.9GHz Intel Core i7 quad-core processor and
16GB RAM. We instrumented the browser to benchmark assigning
topics to our 2,700-page test set. Median runtimes were 23.4 seconds
and 31.3 seconds for LSTMsmall -Top and LSTM-Top, respectively,
versus 39 ms for TF-IDF-CutOne. Because of its comparable accuracy
with far less computation, we use TF-IDF-CutOne in the extension.
Our end goal for topic modeling was to approximate the kinds of
inferences trackers might make and thus improve user understand-
ing of the tracking ecosystem. While our final model sometimes
assigns an incorrect topic, a model that is correct more often than
not is still useful for our purposes of simulating an inferencing
algorithm, informing users, and assessing their reactions to this
transparency effort. This outcome also aligns with real-world track-
ing. Prior work has documented the poor accuracy of behavioral
profiles built by online advertisers [74, 89], with one study finding
only 27% of inferences were strongly relevant [9]. At least 40% of
attributes sold by data brokers may be inaccurate [93]. Thus, our
approximation algorithm appears comparable in its accuracy to the
methods used by companies that invest substantial resources in
fine-tuning tracker data collection and inference models.
3.4 Sensitivity of Interest Categories
On the Interests tab, users can filter the chart to highlight topics
labeled as more or less sensitive. We quantified sensitivity using an
IRB-approved MTurk study in which we asked participants about
their comfort with a specific topic being inferred and used for per-
sonalization. We based our study on Dolin et al. [23], abbreviating
their script and expanding the scope of the study to cover all 1,124
categories used in Google AdWords (excluding world localities).
We obtained 583 responses, each addressing 10 randomly se-
lected categories, from 470 crowdworkers (we permitted partici-
pants to take the survey multiple times). Participation took about
15 minutes. We compensated participants $3.00.
Similar to Dolin et al., we found a spectrum of comfort with
targeting based on different interest categories. A small number
of topics were strongly sensitive or non-sensitive, but most were
somewhere in the middle. From this data, we generated a list of 1,124
topics ranked by mean agreement that “I would be comfortable with
a company personalizing my web experience based on an inference
about my level of interest in [topic]” on a seven-point Likert scale,
which formed the basis of sensitivity filtering in the Interests tab.
4 FIELD STUDY METHODOLOGY
We conducted a field study to evaluate how transparency in the form
of the Tracking Transparency prototype impacts users’ knowledge
and attitudes about tracking and inferencing. Participants were
randomly assigned to install one of six variants, each with different
UI components. At installation, participants completed a pre-usage
survey. After one week of normal browsing, we prompted them to
explore the extension and complete a post-usage survey.
All participants were recruited through Amazon’s Mechanical
Turk (MTurk). Participants needed to be located in the U.S., be at
least 18 years old, and have a 95% HIT approval rating. Because
the extension was built for Google Chrome and Mozilla Firefox,
we required participants to regularly use at least one of them. Our
Table 1: A summary of the conditions’ key characteristics.
Control:Static
Control:Browsing Only
Current:Trackers
Current:Connections
Longitudinal:Trackers
Longitudinal:Interests
Contains only static text explaining targeted
advertising and privacy.
Provides the dashboard interface with info about
browsing history, but no data about tracking.
Simulates Ghostery and similar extensions.
Contains a list of trackers in the toolbar popup,
but no access to the dashboard interface.
Provides a visually restyled version of Mozilla
Lightbeam with no other personalized data.
Contains most interface components. Provides
longitudinal info about trackers and browsing,
but does not show potential interests inferred.
The full interface and data described in Section 3,
including potential interests inferred.
IRB approved the study, and the extension itself was reviewed by
Google and Mozilla following their standard procedures.
4.1 Study Conditions
To gauge the impact of our key transparency features in compar-
ison to state-of-the-art privacy tools’ approaches, we randomly
assigned participants to one of six versions (conditions) of Tracking
Transparency. For consistency and comparability, all conditions
had the same visual design, branding, text, and UI elements other
than the differences being tested, as described below and in Table 1.
Two conditions displayed longitudinal data. The Longitudi-
nal:Interests condition is Tracking Transparency as described
in Section 3, including longitudinal information about tracking
alongside guesses about what interests could have been inferred.
To test the impact of displaying these inferencing guesses, Longi-
tudinal:Trackers was identical to Longitudinal:Interests except
without any inferencing guesses.
Two other conditions replicated UI elements of existing privacy
tools for comparison. Similar to tools like Ghostery, Disconnect, and
Privacy Badger, Current:Trackers showed the trackers on the cur-
rent page in the toolbar popup, as well as an in-page overlay with the
number of trackers. Whereas Longitudinal:Interests was longitudi-
nal, Current:Trackers only provided information about tracking on
the current page. Current:Connections, based on Mozilla Light-
beam, presented a graph visualization of the connections between
websites and trackers, but did not provide Longitudinal:Interests’s
detailed longitudinal information.
Two final conditions were controls. Control:Static provided
static text explaining targeted advertising. Comparisons of other
conditions to Control:Static thus tested the impact of visualizing
personalized data, whether longitudinal or not. To test the impact
of focusing on tracking and privacy, Control:Browsing Only vi-
sualized a user’s browsing history without referencing tracking,
trackers, or inferences. Appendix A.4 gives additional screenshots.
4.2 Pre-Usage Survey
Participants were asked to install the extension in Chrome or Fire-
fox. Following installation, but before interacting with the exten-
sion, participants were directed to the pre-usage survey. We asked
about participants’ demographics, browsing behaviors, use of rel-
evant browser extensions, and experiences with online shopping
and ads. To understand how participants’ knowledge and attitude
changed after using the extension, we asked a series of questions
in the pre-usage survey that were repeated verbatim a week later
in the post-usage survey. These items included seven statements
concerning attitudes about targeted ads, as well as knowledge state-
ments about 15 types of data and 3 broad methods that might
possibly be used for targeting. Participants rated their agreement
with the former on 7-point Likert scales, and the likelihood of the
latter on 7-point likelihood scales (“very unlikely” to “very likely”).
The repeated section also included the awareness and collection
sub-scales of the Internet Users’ Information Privacy Concerns
(IUIPC) scale [55], as well as questions that asked participants to
quantify tracking (e.g., the number of trackers they encounter).
Upon completion of this survey, designed to take 15 minutes,
we compensated participants $3.00 and reminded them to keep
the extension installed for 7 days. On days 4–6, the extension sent
browser notifications to encourage participants to explore it.
4.3 Post-Usage Survey
A week after installation, we sent participants a link to the post-
usage survey via MTurk. We asked them to “spend a few minutes
exploring the extension before beginning the survey,” asking two
questions about what they saw to encourage them to do so. We
then asked four open-ended questions about the information in
the extension: “new information,” “information you already knew,”
“surprising information,” and what questions they had. We also
asked participants to respond to six potential changes in behavioral
intention (e.g., “Compared to before you used the extension, how
likely are you to use a browser’s private browsing mode now?”) on
7-point scales from “much more likely” to “much less likely.” For
four potential tradeoffs (e.g., an internet that is free but has tracking
versus an internet that costs money but does not have tracking),
participants rated which they would choose. As mentioned above,
we repeated the batteries of questions concerning attitudes and
knowledge, the IUIPC, and quantification of tracking. We also asked
the standard System Usability Scale (SUS) [14].
Upon completion of the post-usage survey, designed to take 20
minutes, we compensated participants with a $7.00 bonus payment
on MTurk. This larger compensation encompassed both the week of
keeping the tool installed and completion of the post-usage survey.
Both survey instruments, which are included in Appendices A.2–
A.3, were refined through pilot testing and cognitive interviews.
4.4 Participant Privacy
To protect participant privacy, the extension did not report any
personally identifiable information. On the participant’s own com-
puter, the extension kept a full database of all page visits, trackers
encountered, and interest categories, which was used to power
the extension’s visualizations. This data was stored locally in the
browser extension’s sandboxed storage and was not accessible to
other extensions or web pages. To enable analysis of aggregate
data across all users while preserving participant privacy, we col-
lected an anonymized version of the database, with all URLs and
page titles hashed with a participant-specific salt generated on
the participant’s computer and never sent to the researchers. We
also collected clickstream data for activity in the dashboard. All
data was associated with an anonymous identifier generated by the
extension and never associated with the participant’s Mechanical
Turk ID. The extension did not operate in private browsing mode.
Participants were informed about the data collection through both
a consent form and a privacy policy. The inclusion of longitudinal
visualizations like those in Tracking Transparency in tools intended
for wide distribution will require careful communication to users
about the potential for privacy leaks on shared devices. To enable
longitudinal visualizations, such tools must store a detailed history
of a user’s web browsing. These extensions should clear their own
data when users clear their browser’s history, and require additional
design considerations around shared devices.
4.5 Analysis Methods and Metrics
For quantitative data, we conducted hypothesis tests with α =
.05, choosing the test based on the type of data. Questions asked
only post-usage, such as behavioral intentions after using Tracking
Transparency, elicited responses on scales (e.g., Likert scales). We