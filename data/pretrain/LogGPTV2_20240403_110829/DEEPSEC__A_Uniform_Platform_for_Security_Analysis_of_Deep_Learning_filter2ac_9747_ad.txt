0.033
0.057
ASS
0.710
0.433
0.812
0.940
0.827
0.817
0.717
1.000
0.999
0.730
0.825
0.946
0.799
0.832
1.000
1.000
0.998
0.999
0.999
PSD
276.991
537.340
142.111
79.507
165.721
171.691
275.353
2.692
5.485
273.682
143.450
73.204
187.717
32.445
3.882
3.986
10.157
2.457
1.642
NTE
0.568
0.752
0.733
1.000
1.000
1.000
0.527
0.058
0.541
0.594
0.763
0.909
1.000
0.238
0.141
0.032
1.000
0.093
0.014
RGB
0.942
0.939
0.962
1.000
1.000
1.000
0.904
0.064
0.929
0.620
0.635
1.000
1.000
0.321
0.007
0.009
0.736
0.015
0.014
RIC
0.932
0.977
0.968
0.998
1.000
1.000
0.907
0.226
0.908
0.630
0.548
0.500
0.993
0.224
0.025
0.023
0.049
0.027
0.030
CC
0.0017
0.0016
0.0017
0.0049
0.0227
0.0056
∗
-
0.0113
20.4526
0.0009
0.0006
0.0033
0.0047
2.6207
90.6852
3.0647
4.6315
4.5115
4.7438
∗ Since UAP takes different settings and signiﬁcantly longer time to generate the universal perturbation, here we do not consider the computation cost for it.
one attack can improve its robustness as well.
Compared to TAs, most UAs are shown to be more robust
to regular transformations. For instance, on CIFAR-10, most
UAs achieve over 90%, even 100% robustness in both RGB
and RIC. This implies that almost all AEs generated by
such attacks can maintain its capability of misclassiﬁcation as
before. On the other hand, most TAs, especially BLB, JSMA,
CW and EAD, are shown to experience the worst robustness
in RGB and RIC. It suggests that regular transformations
are effective for mitigating such attacks. The root reason we
conjecture is that with specifying the target class, TAs are
more difﬁcult to attack than UAs. Therefore, it is difﬁcult for
TAs to obtain higher ACAC, which affects their robustness.
Remark 3. The robustness of AEs is affected by ACAC.
Further, most UAs are shown to be more robust than TAs in
our evaluation. Even for certain TAs, image transformations
can effectively mitigate the added perturbation.
Computation Cost. To evaluate the computation cost of
attacks, we test their runtime that is used to generate one AE
on average. It is important to know that comparing the exact
runtime of attacks is unfair due to multiple and complex fac-
tors (e.g., programming, parallelized computing or not, etc.),
which can lead to different runtime performance. Therefore,
we keep all attacks’ settings unchanged with their original
work and only give empirical results in our evaluations.
It is apparent that in the majority of cases, AEs of iterative
attacks are much more expensive to generate than those of
non-iterative attacks. On average, iterative attacks spend 10×
more runtime than non-iterative attacks. Among all iterative
attacks, we observe that OM, JSMA, BLB, CW2 and EAD
are noticeably slower than other iterative attacks.
B. Evaluation of Defenses
We evaluate the utility performance of defenses as below.
1) Experimental Setup: We use the same benchmark
datasets and models as used in Section IV-A. The evaluation
methodology is as follows. Firstly, for each complete defense,
we obtain the corresponding defense-enhanced model from the
original model. Using utility metrics in DUE, we then compare
the utility performance of the defense-enhanced model with
the original model on the testing set of each dateset. For
defense parameter settings in our evaluations, the criteria are:
(i) we follow the same/similar setting as in the original work of
defenses; (ii) if there are variants for one defense, we choose
the one with the best effectiveness performance. The details
of defense parameter settings are reported in Appendix IX-B.
2) Results: We present the evaluation results in Table IV.
UTILITY EVALUATION RESULTS OF ALL COMPLETE DEFENSES
TABLE IV
s Defense-enhanced
Category Name
Adversarial
t
e
s
a
t
a
D
T
S
I
N
M
0
1
-
R
A
F
I
C
Models
Accuracy CAV CRR CSR
CCV COS
Training
Gradient
Masking
Input
Transform.
RC
Adversarial
Training
Gradient
Masking
Input
Transform.
RC
NAT 99.51% 0.24% 0.44% 0.20% 0.17% 0.0006
EAT 99.45% 0.18% 0.44% 0.26% 0.19% 0.0007
PAT 99.36% 0.09% 0.39% 0.30% 0.33% 0.0012
DD 99.27% 0.00% 0.41% 0.41% 0.14% 0.0005
IGR 99.09% -0.18% 0.32% 0.50% 3.03% 0.0111
EIT 99.25% -0.02% 0.42% 0.44% 0.26% 0.0010
RT
95.65% -3.62% 0.17% 3.79% 1.24% 0.0048
PD 99.24% -0.03% 0.06% 0.09% 0.09% 0.0002
99.27% 0.00% 0.42% 0.42% 0.55% 0.0020
TE
99.27% 0.00% 0.07% 0.07%
NAT 84.41% -1.54% 7.14% 8.68% 4.81% 0.0197
EAT 82.15% -3.80% 6.50% 10.30% 5.37% 0.0215
PAT 80.23% -5.72% 6.60% 12.32% 13.87% 0.0572
DD 87.62% 1.67% 7.34% 5.67% 3.25% 0.0127
IGR 77.10% -8.85% 6.56% 15.41% 18.80% 0.0788
EIT 83.25% -2.45% 6.94% 9.39% 5.99% 0.0239
RT
78.50% -7.45% 3.22% 10.67% 4.63% 0.0171
PD 70.66% -15.29% 3.02% 18.31% 5.82% 0.0221
88.63% 2.68% 8.13% 5.45% 4.36% 0.0173
TE
84.87% -1.08% 1.53% 2.61%
-
-
-
-
Although all defenses achieve comparable performances on
both MNIST and CIFAR-10, most defenses show that their
defense-enhanced models have variances for classiﬁcation
accuracy on the testing set, which can be found in “Accuracy”
and “CAV” columns of Table IV. Particularly, the accuracy
variances of defense-enhanced models on CIFAR-10 are much
higher than those on MNIST. The reason we conjecture is that
(cid:23)(cid:25)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
the 99.27% accuracy of the original model on MNIST has
already been sufﬁciently high and stabilized, and thus leaves
less variation space than that on CIFAR-10 models.
Among all defenses, NAT, DD, TE and RC on both MNIST
and CIFAR-10 almost do not sacriﬁce the classiﬁcation ac-
curacy on the testing set according to the CAV results. On
the other hand, it can also be observed that the classiﬁcation
accuracies of IGR-, RT- and PD-enhanced models have sig-
niﬁcant drop when they are performed on CIFAR-10. In fact,
the performance of CAV is a consequence of the percentage
of examples that are rectiﬁed and sacriﬁced by the defended
model, which is conﬁrmed by both CRR and CSR results
in corresponding columns of Table IV. Consequently,
the
signiﬁcant accuracy drop (i.e., CAV) is induced in the defense-
enhanced model as long as its CSR is larger than CRR.
According to the CCV results, most of defenses have little
impact on the prediction conﬁdence of all correctly classiﬁed
testing examples before and after applying defenses. However,
compared to MNIST, the CCVs of defense-enhanced models
on CIFAR-10 are 10× higher in most cases. Even worse, for
PAT and IGR, the CCV of their defense-enhanced models
on CIFAR-10 are 18.80% and 13.87%, respectively. This is
because the defense-enhanced models on CIFAR-10 are less
stable, and thus their prediction conﬁdence is more sensitive
to examples on the testing set.
As for the classiﬁcation output stability of all defense-
enhanced models, we ﬁnd that COS has the similar trend to
CCV. The reason we conjecture is that if the conﬁdence of the
predicted class is fairly high for one example, the conﬁdences
of other classes are quite small, accordingly. This implies
high variance of the predicted class’s probability can lead
to considerable adjustments of all other output probabilities.
Therefore, the prediction conﬁdence variance greatly impacts
its output stability of classiﬁcation, and thus the values of both
COS and CCV follow a similar trend.
Remark 4. Overall, as long as the defense-enhanced models
are trained or adjusted based on the accuracy metric, most of
them can also preserve the other utility performances, such as
CCV and COS.
C. Defenses vs. Attacks
Although there have been many sophisticated defenses and
strong attacks, it is still an open problem whether or to what
extent the state-of-the-art defenses can defend against attacks.
1) Complete Defenses: In this part, we evaluate the effec-
tiveness of all 10 complete defenses against attacks.
Experimental Setup. We use the same benchmark datasets
and their corresponding models as that in Section IV-A. The
evaluation methodology proceeds as follows. For each attack,
we ﬁrst merely select the successfully misclassiﬁed AEs that
are generated in Section IV-A, and then we use all defense-
enhanced models (as used in Section IV-B) to evaluate the
classiﬁcation performance on such AEs.
Results. We only present the results of CIFAR-10 in Ta-
ble V, as the results of MNIST are similar (detailed results
are reported in Appendix X).
Basically, most defense-enhanced models increase their
classiﬁcation accuracy against existing attacks. When evaluat-
ing on CIFAR-10, NAT can successfully defend against more
than 80% AEs generated by all attacks on average, and all
defense-enhanced models averagely achieve 58.4% accuracy
over all kinds of AEs. Thus, we suggest that all state-of-the-
art defenses are more or less effective against existing attacks.
In general, most defenses show better defensive perfor-
mance against TAs than that on UAs. For CIFAR-10, all
defense-enhanced models averagely achieve 49.6% and 66.3%
accuracy against UAs and TAs, respectively. It implies that
AEs generated by UAs show stronger resilience to defense-
enhanced models, and thus become more difﬁcult to defend.
We conjecture this is because UAs are more likely to general-
ize to other models including defense-enhanced models, while
TAs tend to overﬁt to the speciﬁc target model. Hence, AEs
generated by TAs are more easily classiﬁed by defenses.
With the increase of attacking ability for one speciﬁc attack,
fewer AEs can be classiﬁed by defense-enhanced models.
For instance, when we increase the  of FGSM, we ﬁnd
that the performance of all defenses has a signiﬁcant drop.
Similar results are found when we increase the κ parameter
of CW2. The reason is evident. Since larger attacking ability
implies that higher magnitude of perturbations are generated
by attacks, which make the perturbed AEs more visually
dissimilar to the original examples.
Among all defenses, NAT, PAT, EAT, TE, EIT and IGR
show better and stable performance in defending against
most attacks. RT, PD and RC are observed to have worse
performance when defending against each attack on average
for both MNIST and CIFAR-10. We conjecture this is mainly
because they all retrain their model and obtain totally different
model weights. As we will present in Section V-A, without
any modiﬁcation to the original model, merely retraining the
model can be a defense. Therefore, a defense that retrains the
model usually performs better than other defenses that do not
retrain their models, including RC, RT, PD.
According the results, all the defenses have the capability of
defending against some attacks, while no defense is universal
to all attacks. Taking the RC defense as an example, we ﬁnd
that the RC has superior performance to defend against DF,
BLB, EAD and low-conﬁdence CW2 (i.e., κ = 0), but it
achieves much worse performance on the other adversarial
attacks. Multiple reasons are responsible for the results such
as inherent limitations of defenses against different kinds of
attacks (e.g., RC defense is designed to defend against small
perturbations), the parameters employed by an algorithm, etc.
Remark 5. For complete defenses, most of them have capabil-
ity of defending against some adversarial attacks, but no de-
fense is universal. Particularly, the defenses that retrain their
models usually perform better than others without retraining.
2) Detection: Now, we evaluate the effectiveness of three
detection-only defenses against existing adversarial attacks.
Experimental Setup. We use the same benchmark datasets
and relative original models as used in Section IV-C1.
(cid:23)(cid:25)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
CLASSIFICATION ACCURACY OF COMPLETELY DEFENSES AGAINST ADVERSARIAL ATTACKS ON CIFAR-10
TABLE V
Attack
FGSM
s
t
e
s
a
t
a
D
UA/
TA
Objec-
tive
L∞
 = 0.1
UAs
0
1
-
R
A
F
I
C
TAs
L2
L∞
 = 0.1
L0
L2
Attacks
BIM
PGD
R+FGSM
 = 0.1
 = 0.2
UAP
DF
OM
LLC
# of
AEs
897
898
837
1000
1000
U-MI-FGSM 1000
853
1000
1000
134
315
1000
1000
997
1000
κ = 0
1000
κ = 20 1000
1000
1000