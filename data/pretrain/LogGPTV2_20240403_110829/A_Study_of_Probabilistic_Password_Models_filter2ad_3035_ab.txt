models and template-based models. A template-based model
divides a password into several segments, often by grouping
consecutive characters of the same category into one segment,
and then generates the probability for each segment indepen-
dently. A whole-string model, on the other hand, does not
divide a password into segments.
Whole-string Markov models are used in John the Rip-
per (JTR) [3] and Castelluccia et al.’s work on password
strength estimation [8]. JTR in Markov mode uses a 1-order
Markov chain, in which the probability assigned to a password
“c1c2 · · · c(cid:2)” is
P (“c1c2 · · · c(cid:2)”) = P (c1|c0)P (c2|c1)P (c3|c2) · · · P (c(cid:2)|c(cid:2)−1),
ANLL(D|p) =
1
|D|
− log2 p(s)
A. Whole-String Markov Models
(cid:4)
s∈D
To see that ANLL equals the area to the left of the probability-
threshold curve, observe that when dividing the area into very
thin horizontal rows, the length of a row with height between
y and y + dy is given by the number of passwords with
probability p such that y ≤ − log2 p  1, we can prepend d copies of the start
symbol c0 to each password.
(cid:5)
There are many variants of whole-string Markov models.
Below we examine the design space.
Need for Normalization. We note that models used in [3],
[8] are not probability models because the values assigned to
all strings do not add up to 1. In fact, they add up to U −L+1,
where U is the largest allowed password length, and L is the
smallest allowed password length, because the values assigned
to all strings of a ﬁxed length add up to 1. To see this, ﬁrst
observe that
(cid:4)
(cid:5)
P (“c1”) =
c1∈Σ
c1∈Σ count(c0c1)
count(c0·)
= 1,
and thus the values assigned to all length-1 strings add up to
1. Assume that the values assigned to all length-(cid:3) strings sum
up to 1. We then show that the same is the case for length
(cid:3) + 1 as follows.
(cid:5)
(cid:5)
(cid:5)
=
=
c1c2···c(cid:2)+1∈Σ(cid:2)+1 P (“c1c2 · · · c(cid:2)+1”)
(cid:5)
c1c2···c(cid:2)∈Σ(cid:2) P (“c1c2 · · · c(cid:2)”) ×
c1c2···c(cid:2)∈Σ(cid:2) P (“c1c2 · · · c(cid:2)”) = 1
c(cid:2)+1∈Σ P (c(cid:2)+1|c(cid:2))
The same analysis holds for Markov chains of order d > 1.
To turn such models into probability models, several normal-
ization approaches can be applied:
Direct normalization. The approach of using the values
directly in [3], [8] is equivalent to dividing each value by
(U − L + 1). This method, however, more or less assumes
that the total probabilities of passwords for each length are the
same. This is clearly not the case in practice. From Table II(c),
which gives the password length distributions for the datasets
we use in this paper, we can see that passwords of lengths
between 6 and 10 constitute around 87% of all passwords, and
passwords of lengths 11 and 12 add an additional 7%, with the
remaining 30 lengths (lengths 4,5,13-40) together contributing
slightly less than 6%.
Distribution-based normalization. A natural alternative to
direct normalization is to normalize based on the distribution
of passwords of different
lengths. More speciﬁcally, one
normalizes by multiplying the value assigned to each password
of length m with the following factor:
# of passwords of length m in training
total # of passwords in training
693
This approach, however, may result in overﬁtting, since the
training and testing datasets may have different length distri-
butions. From Table II(c), we can see that the CSDN dataset
includes 9.78% of passwords of length 11, whereas the PhpBB
dataset includes only 2.1%, resulting in a ratio of 4.66 to 1, and
this ratio keeps increasing. At length 14, CSDN has 2.41%,
while PhpBB has only 0.21%.
End-symbol normalization. We propose another normaliza-
tion approach, which appends an “end” symbol ce to each
password, both in training and in testing. The probability
assigned by an order-1 Markov chain model to a password
“c1c2 · · · c(cid:2)” is
P (c1|c0)P (c2|c1)P (c3|c2) · · · P (c(cid:2)|c(cid:2)−1)P (ce|c(cid:2)),
where the probability of P (ce|c(cid:2)) is learned from the training
dataset. For a string “c1c2 · · · cU ” of the maximal allowed
length, we deﬁne P (ce|cU ) = 1.
In this approach, we also consider which substrings are
more likely to occur at the end of passwords. In Markov
chain models using direct normalization,
the preﬁx of a
password is always more likely than the actual password,
which may be undesirable. For example, “passwor” would
be assigned a higher probability than “password”. The end
symbol corrects this situation, since the probability that an end
symbol following “passwor” is likely to be signiﬁcantly lower
than the product of the probability that “d” follows “passwor”
and that of the end symbol following “password”.
Such a method can ensure that the probabilities assigned to
all strings sum up to 1 when the order of the Markov chain is
at least as large as L, the smallest allowed password length. To
see this, consider the case when L = 1. Envision a tree where
each string corresponds to a node in the tree, with c0 being
the root node. Each string “c0c1 · · · c(cid:2)” has “c0c1 · · · c(cid:2)−1” as
its immediate parent. All the leaf nodes are strings with ce as
the last symbol, i.e., they are actual passwords. Each edge is
assigned the transition probability; thus, the values assigned
to all edges leaving a parent node add up to 1. Each node in
the tree is assigned a value. The root is assigned 1, and every
other node is assigned the value of the parent multiplied by
the transition probability of the edge from its parent to the
node. Thus, the value assigned to each leaf node equals its
probability under the model. We note that the value assigned
to each node equals the sum of values assigned to its children.
It follows that the total probabilities assigned to all leaves add
up to be the same as those assigned to the root, which is 1.
When the order of Markov chain, d, is less than L, the
minimal required password length, we may have the situation
that the total probabilities add up to less than 1, because non-
zero probabilities would be assigned to illegal passwords of
lengths less than L. The effect is that this method is slightly
penalized when evaluated using probability-threshold graphs
or ANLLs. We note that when a model wastes half of the
total probability of 1 by assigning them to passwords that are
too short, the ANLLθ is increased by θ.
Choice of Markov Chain Order, Smoothing, and Spar-
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
sity. Another challenging issue is the choice of the order
for Markov chains. Intuitively, a higher-order Markov chain
enables the use of deeper history information than lower-
order ones, and thus may be more accurate. However, at
the same time, a higher-order Markov chain runs the risk of
overﬁtting and sparsity, which causes reduced performance.
Sparsity means that one is computing transition probabilities
from very small count numbers, which may be noisy.
In Markov chain models, especially higher order ones, many
calculated conditional probabilities would be 0, causing many
strings assigned probability 0. This can be avoided by apply-
ing the technique of smoothing, which assigns pseudocounts
to unseen strings. Pseudocounts are generally motivated on
Bayesian grounds. Intuitively, one should not think that things
that one has not yet seen have probability 0. Various smoothing
methods have been developed for language modeling. Additive
smoothing, also known as Laplace smoothing, adds δ to the
count of each substring. When δ is chosen to be 1, this is
known as add-one smoothing, which is problematic because
the added pseudo counts often overwhelm the true counts. In
this paper, we choose δ to be 0.01; the intuition is that when
encounting a history “c1c2c3c4”, all the unseen next characters,
after smoothing come up to slightly less 1, since there are 95
characters.
A more sophisticated smoothing approach is Good-Turing
smoothing [13]. This was developed by Alan Turing and his
assistant I.J. Good as part of their efforts at Bletchley Park to
crack German Enigma machine ciphers during World War II.
An example illustrating the effect of Good-Turing smoothing is
in [5], where Bonneau calculated the result of applying Good-
Turing smoothing to the Rockyou dataset. After smoothing, a
password that appears only once has a reduced count of 0.22,
a password that appears twice has a count of 0.88, whereas
a password that appears k ≥ 3 times has a reduced count of
approximately k − 1. The “saved” counts from these reduction
are assigned to all unseen passwords. Intuitively, estimating the
probability of a password that appears only a few times using
its actual count will overestimate its true probability, whereas
the probability of a password that appears many times can be
well approximated by its probability in the dataset. The key
observation underlying Good Turing smoothing is that the best
estimation for the total probability of items that appear exactly
i times is the total probability of items that appear exactly i+1
times. In particular, the total probability for all items unseen
in a dataset is best estimated by the total probability of items
that appear only once.
Grouping. Castelluccia et al. [8] proposed another approach
to deal with sparsity. They observed that upper-case letters and
special symbols occur rarely, and propose to group them into
two new symbols. Here we use Υ for the 26 upper-case letters
and Ω for the 33 special symbols. This reduces the size of the
alphabet from 95 to 38 (26 lower case letters, 10 digits, and
Υ and Ω), thereby reducing sparsity. This approach treats all
uppercase letters as exactly the same, and all special symbols
as exactly the same. That is, the probability that any upper-
case letter follows a preﬁx is the probability that Υ follows
the preﬁx divided by 26. For example, P [“aF?b”] for an order
2 model is given by
P [a|s0s0]
P [Υ|s0a]
P [Ω|aΥ]
33
We call this the “grouping” method.
26
P [b|ΥΩ].
We experimented with an adaptation of this method. When
encountering an upper-case letter, we assign probabilities in
proportion to the probability of the corresponding lower-case
letter, based on the intuition that following “swor”, “D” is
much more likely than Q, just as d is much more like than
q. When encountering a special symbol, we use the order-
1 history to assign probability proportionally, based on the
intuition that as special symbols are rare, a shorter history is
likely better. In the adaptation, the probability of the above
string will be computed as:
P [a|s0s0]
P [Υ|s0a]P [f |s0a]
P [Ω|aΥ]P [?|F ]
(cid:5)
(cid:5)
α P [α|s0a]
ω P [ω|F ]
P [b|ΥΩ],
where α ranges over all lower-case letters, and ω ranges over
all special symbols.
Backoff. Another approach that addresses the issue of order,
smoothing, and sparsity together is to use variable order
Markov chains. The intuition is that
if a history appears
frequently, then we would want to use that to estimate the
probability of the next character. For example, if the preﬁx
is “passwor”, using the whole preﬁx to estimate the next
character would be more accurate than using only “wor”. On
the other hand, if we observe a preﬁx that rarely appears,
e.g., “!W}ab”, using only “ab” as the preﬁx is likely to be
more accurate than using longer history, which likely would
assign equal probability to all characters. One way to do this
is Katz’s backoff model [16]. In this model, one chooses a
threshold and stores all substrings whose counts are above the
threshold. Let πi,j denote the substring of s0s1s2 · · · s(cid:2) from
si to sj . To compute the transition probability from π0,(cid:2)−1 to
π0,(cid:2), we look for the smallest i value such that πi,(cid:2)−1 has a
count above the threshold. There are two cases. In case one,
πi,(cid:2)’s count is also above the threshold; thus, the transition
count(πi,(cid:2))
count(πi,(cid:2)−1) . In case two, where πi,(cid:2)
probability is simply
does not have a count, we have to rely on a shorter history to
assign this probability. We ﬁrst compute b, the probability left
after assigning transition probabilities to all characters via case
one. We then compute the probabilities for all other characters
using the shorter preﬁx πi+1,(cid:2)−1, which are computed in a
recursive fashion. Finally, we normalize these probabilities so
that they add up to b.
Backoff models are signiﬁcantly slower than other Markov
models. The backoff models used in experimentation are
approximately 11 times slower than the plain Markov models,
both for password generation and for probability estimation.
B. Template-based Models
In the template-based approach, one divides passwords into
different templates based on the categories of the characters.
694
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
For example, one template is 6 lower-case letters followed by
3 digits, which we denote as α6β3
. Such a template has two
segments, one consisting of the ﬁrst 6 letters and the other
consisting of last 3 digits. The probability of the password
using this template is computed as:
P (“passwd123”) = P (α6β3
)P (“passwd”|α6
)P (“123”|β3
),
where P (α6β3) gives the probability of the template α6β3
,
P (“passwd”|α6) gives the probability that “passwd” is used
to instantiate 6 lowercase letters, and P (“123”|β3) gives the
probability that “123” is used to instantiate 3 digits. To deﬁne
a model, one needs to specify how to assign probabilities
to templates, and to the different possible instantiations of a
given segment. Template-based models are used in [21] and
the PCFGW model [25].
In [21], the proba-
Assigning probabilities to templates.
bilities for templates are manually chosen because this work
was done before the availability of large password datasets.
In [25], the probability is learned from the training dataset by
counting. That is, the probability assigned to each template is
simply the number of passwords using the template divided
by the total number of passwords. Using this approach means
that passwords that use a template that does not occur in the
training dataset will have probability 0.
We consider the alternative of applying Markov models to
assign probabilities to templates. That is, we convert pass-
words to strings over an alphabet of {α, β, υ, ω}, representing
lower-case letters, digits, upper-case letters, and symbols re-
spectively. We then learn a Markov model over this alphabet,
and assign probabilities to a template using Markov models.
In [21], the probabil-
Assigning probabilities to segments.
ities of letter segments are assigned using a Markov model
learned over natural language, and the probabilities for digit
segments and symbol segments are assigned by assuming that
all possibilities are equally likely. In the PCFGW model [25],
the probabilities for digit segments and symbol segments are
assigned using counting from the training dataset, and letter
segments are handled using a dictionary. That is, for an α6
segment, all length-6 words in the dictionary are assigned an
equal probability, and any other letter sequence is assigned
probability 0. For a β3
template, the probability of “123” is
the number of times “123” occurs as a 3-digit segment in the
training dataset divided by the number of 3-digit segments in
the training dataset.
We consider template models in which we assign segment
instantiation probabilities via both counting and Markov mod-
els. Table I summarizes the design space of password models
we consider in this paper.
PCFGW model does not ﬁt in the models in Table I, as it
requires as input a dictionary in addition to a training dataset.
In this paper, we considered 3 instantiations of PCFGW : the
ﬁrst uses the dictionary used in [25]; the second uses the
OpenWall dictionary; and the third generates the dictionary
from the training set. We note that the last instantiation is
essentially the template-based model using counting both for
assigning template probabilities and for instantiating segments.
C. Password Generation
To use a password model for cracking, one needs to be able
to generate passwords in decreasing probability.
In whole-string Markov-based methods,
the password
search space can be modeled as a search tree. As described
earlier, the root node represents the empty string (beginning of
the password), and every other node represents a string. Each
node is labeled with the probability of the string it represents.
One algorithm for password guess generation involves stor-
ing nodes in a priority queue. The queue, arranged in order of
node probabilities, initially contains the root node of the search
tree. We then iterate through the queue, popping the node with
the greatest likelihood at each iteration. If this is an end node
(any non-root node for uniform or length-based normalization,
or nodes with end symbol for end-symbol normalization), we
output the string as a password guess. Otherwise, we calculate
the transition probability of each character in the character
set and add to the priority queue a new child node with
that character and the associated probability. This algorithm
is guaranteed to return password guesses in decreasing order
of probability (as estimated by the model), due to the iteration
order and property that each node’s probability is smaller than
that of its parent. The algorithm terminates once the desired
number of guesses has been generated.
The priority-queue method, however, is not memory efﬁ-
cient, and does not scale for generating a large number (e.g.,
over 50 million) of guesses. Since each node popped from the
queue can result in multiple nodes added to the queue, the
queue size is typically several times of the number of guesses
generated. To reduce the memory footprint, we propose a
threshold search algorithm.
ρi
traversals of the search tree. In the i’th iteration,
The threshold search algorithm, similar to the iterative
deepening state space search method, conducts multiple depth-
ﬁrst
it
generates passwords with probabilities in a range (ρi, τi],
by performing a depth-ﬁrst
traversal of the tree, pruning
all nodes with probability less than ρi, and outputting all
passwords with probability in the target range. To generate
n password guesses, we start with a conservative range of
(ρ1 = 1
n , τ1 = 1]. After the i’th iteration, if we have generated
m < n passwords so far, we start another iteration with
(ρi+1 =
max(2,1.5n/m) , τi+1 = ρi]. That is, when m < 0.75n,
we halve the probability threshold. We have observed empiri-
cally that halving ρ result in close to twice as many passwords
being generated. We may overshoot and generally more than n
passwords, but are very unlikely to generate over 2n guesses.
The average runtime complexity of this algorithm as O(n),
or linear on n. The memory complexity (not including the
model data or generated passwords) is essentially constant, as
the only data structure needed is a stack of capacity U +1. We
use this framework to efﬁciently generate Markov model-based
guesses in the experiments. Slight adjustments need to be made
for distribution-based normalization. This method, however,
695
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
Whole-string Markov models
Template-based models
normalization methods: (1) direct, (2) distribution-based, (3) end symbol
order of Markov chain: 1, 2, 3, 4, · · · or variable (backoff)
dealing with sparsity: (1) plain, (2) grouping (3) adapted grouping
smoothing methods: (1) no smoothing, (2) add-δ smoothing, (3) Good-Turing smoothing
template probability assignment: (1) Counting, (2) Markov model
segment probability assignment: (1) Counting, (2) Markov model
TABLE I: Design space for password probability models
does not apply to template-based models. Through probability-
threshold graphs, we have found that unless Markov mod-
els are used to instantiate templates, template-based models
perform rather poorly. However, when Markov models are
used, efﬁcient generation for very large numbers of passwords
appears quite difﬁcult, as one cannot conduct depth-ﬁrst search
and needs to maintain a large amount of information for each
segment. In this paper, we do not do password generation for
template-based models other than the PCFGW model.
IV. EXPERIMENTAL METHODOLOGIES
In this section, we describe our experimental evaluation
methodologies, including the dataset we use and the choice
of training/testing scenarios.
Datasets. We use the following six password datasets down-
loaded from public Web sites. We use only the password
information in these datasets and ignored all other information
(such as user names and/or email addresses included in some
datasets). The RockYou dataset [1] contains over 32 million
passwords leaked from the social application site Rockyou in
December 2009. The PhpBB dataset [1] includes about 250K
passwords cracked by Brandon Enright from MD5 hashes
leaked from Phpbb.com in January 2009. The Yahoo dataset
includes around 450K passwords published by the hacker
group named D33Ds in July 2012. The CSDN dataset includes
about 6 million user accounts for the Chinese Software Devel-