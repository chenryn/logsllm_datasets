Table II shows the PixelDP conﬁgurations we used for
the 1-norm and 2-norm defenses. The code is available
at https://github.com/columbia/pixeldp. Since most of this
section focuses on models with 2-norm attack bounds, we
detail only those conﬁgurations here.
ImageNet: We use as baseline a pre-trained version of
Inception-v3 [55] available in Tensorﬂow [22]. To make
it PixelDP, we use the autoencoder approach from §III-B,
which does not require a full retraining of Inception and was
instrumental in our support of ImageNet. The encoder has
three convolutional layers and tied encoder/decoder weights.
The convolution kernels are 10 × 10 × 32, 8 × 8 × 32, and
5× 5× 64, with stride 2. We make the autoencoder PixelDP
by adding the DP noise after the ﬁrst convolution. We then
stack the baseline Inception-v3 on the PixelDP autoencoder
and ﬁne-tune it for 20k steps, keeping the autoencoder
weights constant.
architecture,
CIFAR-10, CIFAR-100, SVHN: We use the same base-
line
state-of-the-art Residual Network
(ResNet) [70]. Speciﬁcally we use the Tensorﬂow implemen-
tation of a 28-10 wide ResNet [57], with the default param-
eters. To make it PixelDP, we slightly alter the architecture
to remove the image standardization step. This step makes
sensitivity input dependent, which is harder to deal with in
PixelDP. Interestingly, removing this step also increases the
baseline’s own accuracy for all three datasets. In this section,
we therefore report the accuracy of the changed networks as
a
baselines.
MNIST: We train a Convolutional Neural Network (CNN)
with two 5 × 5 convolutions (stride 2, 32 and 64 ﬁlters)
followed by a 1024 nodes fully connected layer.
Evaluation Metrics. We use two accuracy metrics to evalu-
ate PixelDP models: conventional accuracy and certiﬁed ac-
curacy. Conventional accuracy (or simply accuracy) denotes
the fraction of a testing set on which a model is correct; it
is the standard accuracy metric used to evaluate any DNN,
defended or not. Certiﬁed accuracy denotes the fraction
of the testing set on which a certiﬁed model’s predictions
are both correct and certiﬁed robust for a given prediction
robustness threshold; it has become a standard metric to
evaluate models trained with certiﬁed defenses [65], [52],
[16]. We also use precision on certiﬁed examples, which
measures the number of correct predictions exclusively on
examples that are certiﬁed robust for a given prediction
robustness threshold. Formally, the metrics are deﬁned as
follows:
(cid:2)n
i=1 isCorrect(xi)
n
n
1) Conventional accuracy
, where n is
the testing set size and isCorrect(xi) denotes a func-
tion returning 1 if the prediction on test sample xi
returns the correct label, and 0 otherwise.
(cid:2)n
2) Certiﬁed accuracy
i=1(isCorrect(xi)&robustSize(scores,,δ,L)≥T )
the
, where
returns
certiﬁed
then compared to the
robustSize(scores, , δ, L)
robustness
size, which is
prediction robustness threshold T.
(cid:2)n
on
3) Precision
certiﬁed
i=1(isCorrect(xi)&robustSize(pi,,δ,L)≥T ))
i=1 robustSize(pi,,δ,L)≥T )
(cid:2)n
examples
.
For T = 0 all predictions are robust, so certiﬁed accuracy
is equivalent to conventional accuracy. Each time we report
L or T , we use a [0, 1] pixel range.
Attack Methodology. Certiﬁed accuracy – as provided by
PixelDP and other certiﬁed defense – constitutes a guar-
anteed lower-bound on accuracy under any norm-bounded
attack. However, the accuracy obtained in practice when
faced with a speciﬁc attack can be much better. How much
better depends on the attack, which we evaluate in two
steps. We ﬁrst perform an attack on 1,000 randomly picked
samples (as is customary in defense evaluation [37]) from
the testing set. We then measure conventional accuracy on
the attacked test examples.
For our evaluation, we use the state-of-the art attack from
Carlini and Wagner [7], that we run for 9 iterations of binary
search, 100 gradient steps without early stopping (which
we empirically validated to be sufﬁcient), and learning rate
0.01. We also adapt
the attack to our speciﬁc defense
following [2]: since PixelDP adds noise to the DNN, attacks
based on optimization may fail due to the high variance
of gradients, which would not be a sign of the absence of
adversarial examples, but merely of them being harder to
ﬁnd. We address this concern by averaging the gradients over
(cid:23)(cid:23)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
Dataset
ImageNet [13]
CIFAR-100 [33]
CIFAR-10 [33]
SVHN [44]
MNIST [69]
Training
set size
Image
size
299x299x3 1.4M
32x32x3
50K
50K
32x32x3
73K
32x32x3
28x28x1
60K
Testing
set size
50K
10K
10K
26K
10K
Target
labels
1000
100
10
10
10
Classiﬁer
architecture
Inception V3
ResNet
ResNet
ResNet
CNN
Baseline
accuracy
77.5%
78.6%
95.5%
96.3%
99.2%
p-norm
used
1-norm
1-norm
2-norm
1-norm
2-norm
DP
mechanism
Laplace
Gaussian
Gaussian
Laplace
Gaussian
Noise
Sensitivity
location
approach
1st conv.
Δ1,1 = 1
1st conv.
Δ1,2 = 1
Δ2,2 ≤ 1
1st conv.
Autoencoder Δ1,1 = 1
Autoencoder Δ2,2 ≤ 1
Table I: Evaluation datasets and baseline models. Last column shows the accuracy
of the baseline, undefended models. The datasets are sorted based on descending
order of scale or complexity.
Table II: Noise layers in PixelDP DNNs. For each
DNN, we implement defenses for different attack
bound norms and DP mechanisms.
Dataset
ImageNet
CIFAR-10
CIFAR-100
SVHN
MNIST
Baseline L = 0.03 L = 0.1 L = 0.3 L = 1.0
37.7%
77.5%
95.5%
44.3%
22.1%
78.6%
28.2%
96.3%
99.2%
11%
68.3%
87.0%
62.4%
93.1%
99.1%
57.7%
70.9%
44.3%
79.6%
98.2%
–
93.3%
73.4%
96.1%
99.1%
Table III: Impact of PixelDP noise on conventional accuracy. For
each DNN, we show different levels of construction attack size L.
Conventional accuracy degrades with noise level.
20 noise draws at each gradient step. Appendix §C contains
more details about the attack, including sanity checks and
another attack we ran similar to the one used in [37].
Prior Defenses for Comparison. We use two state-of-art
defenses as comparisons. First, we use the empirical defense
model provided by the Madry Lab for CIFAR-10 [38]. This
model is developed in the context of ∞-norm attacks. It uses
an adversarial training strategy to approximately minimize
the worst case error under malicious samples [37]. While
inspired by robust optmization theory,
this methodology
is best effort (see §VI) and supports no formal notion of
robustness for individual predictions, as we do in PixelDP.
However, the Madry model performs better under the latest
attacks than other best-effort defenses (it is in fact the only
one not yet broken) [2], and represents a good comparison
point.
Second, we compare with another approach for certiﬁed
robustness against ∞-norm attacks [65], based on robust
optimization. This method does not yet scale to the largest
datasets (e.g. ImageNet), or the more complex DNNs (e.g.
ResNet, Inception) both for computational reasons and be-
cause not all necessary layers are yet supported (e.g. Batch-
Norm). We thus use their largest released model/dataset,
namely a CNN with two convolutions and a 100 nodes fully
connected layer for the SVHN dataset, and compare their
robustness guarantees with our own networks’ robustness
guarantees. We call this SVHN CNN model RobustOpt.
B. Impact of Noise (Q1)
Q1: How does DP noise affect the conventional accuracy
of our models? To answer, for each dataset we train up to
four (1.0, 0.05)-PixelDP DNN, for construction attack bound
L ∈ {0.03, 0.1, 0.3, 1}. Higher values of L correspond to
robustness against larger attacks and larger noise standard
deviation σ.
Table III shows the conventional accuracy of these net-
(cid:23)(cid:23)(cid:21)
works and highlights two parts of an answer to Q1. First, at
fairly low but meaningful construction attack bound (e.g.,
L = 0.1), all of our DNNs exhibit reasonable accuracy
loss – even on ImageNet, a dataset on which no guarantees
have been made to date! ImageNet: The Inception-v3 model
stacked on the PixelDP auto-encoder has an accuracy of
68.3% for L = 0.1, which is reasonable degradation com-
pared to the baseline of 77.5% for the unprotected network.
CIFAR-10: Accuracy goes from 95.5% without defense to
87% with the L = 0.1 defense. For comparison, the Madry
model has an accuracy of 87.3% on CIFAR-10. SVHN:
our L = 0.1 PixelDP network achieves 93.1% conventional
accuracy, down from 96.3% for the unprotected network.
For comparison, the L = 0.1 RobustOpt network has an
accuracy of 79.6%, although they use a smaller DNN due
to the computationally intensive method.
Second, as expected, constructing the network for larger
attacks (higher L) progressively degrades accuracy. Ima-
geNet: Increasing L to 0.3 and then 1.0 drops the accuracy
to 57.7% and 37.7%, respectively. CIFAR-10: The ResNet
with the least noise (L = 0.03) reaches 93.3% accuracy,
close to the baseline of 95.5%; increasing noise levels (L =
(0.1, 0.3, 1.0)) yields 87%, 70.9%, and 37.7%, respectively.
Yet, as shown in §IV-D, PixelDP networks trained with fairly
low L values (such as L = 0.1) already provide meaningful
empirical protection against larger attacks.
C. Certiﬁed Accuracy (Q2)
Q2: What accuracy can PixelDP certify on a test set?
Fig. 2 shows the certiﬁed robust accuracy bounds for Im-
ageNet and CIFAR-10 models, trained with various values
of the construction attack bound L. The certiﬁed accuracy
is shown as a function of the prediction robustness thresh-
old, T . We make two observations. First, PixelDP yields
meaningful robust accuracy bounds even on large networks
for ImageNet (see Fig. 2(a)), attesting the scalability of our