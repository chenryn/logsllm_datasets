(T,TL), (T(cid:48),T(cid:48)
Persistency. With persistency we require that it is hard
to remove a backdoor, unless one has knowledge of the
trigger set T . There are two trivial cases which a deﬁni-
tion must avoid:
• An adversary may submit a model that has no back-
door, but this model has very low accuracy. The
deﬁnition should not care about this setting, as such
a model is of no use in practice.
• An adversary can always train a new model from
scratch, and therefore be able to submit a model
that is very accurate and does not include the back-
door. An adversary with unlimited computational
resources and unlimited access to O f will thus al-
ways be able to cheat.
We deﬁne persistency as follows:
f be a
ˆM ←
ground-truth function, b be a backdoor and
Backdoor(O f , b,M) be a ε-accurate model. Assume an
let
algorithm A on input O f , ˆM outputs an ε-accurate model
˜M in time t which is at least (1− ε) accurate on b. Then
˜N ← A(O f ,N), generated in the same time t, is also ε-
accurate for any arbitrary model N.
In our approach, we chose to restrict the runtime of A,
but other modeling approaches are possible: one could
also give unlimited power to A but only restricted access
to the ground-truth function, or use a mixture of both.
We chose our approach as it follows the standard pattern
in cryptography, and thus allows to integrate better with
cryptographic primitives which we will use:
these are
only secure against adversaries with a bounded runtime.
2.4 Commitments
Commitment schemes [9] are a well known cryptographic
primitive which allows a sender to lock a secret x into
a cryptographic leakage-free and tamper-proof vault and
give it to someone else, called a receiver. It is neither pos-
sible for the receiver to open this vault without the help
of the sender (this is called hiding), nor for the sender to
exchange the locked secret to something else once it has
been given away (the binding property).
Formally, a commitment scheme consists of two algo-
rithms (Com, Open):
• Com(x,r) on input of a value x ∈ S and a bitstring
r ∈ {0,1}n outputs a bitstring cx.
• Open(cx,x,r) for a given x ∈ S,r ∈ {0,1}n,cx ∈
{0,1}∗ outputs 0 or 1.
For correctness, it must hold that ∀x ∈ S,
[Open(cx,x,r) = 1 | cx ← Com(x,r)] = 1.
Pr
r∈{0,1}n
We call the commitment scheme (Com, Open) binding
if, for every PPT algorithm A
 Open(cx, ˜x, ˜r) = 1
Pr
 ≤ ε(n)
cx ← Com(x,r)∧
( ˜x, ˜r) ← A(cx,x,r)∧
(x,r) (cid:54)= ( ˜x, ˜r)
where ε(n) is negligible in n and the probability is taken
over x ∈ S,r ∈ {0,1}n.
Similarly, (Com, Open) are hiding if no PPT algorithm
A can distinguish c0 ← Com(0,r) from cx ← Com(x,r)
for arbitrary x ∈ S,r ∈ {0,1}n. In case that the distribu-
tions of c0,cx are statistically close, we call a commit-
ment scheme statistically hiding. For more information,
see e.g. [14, 39].
1618    27th USENIX Security Symposium
USENIX Association
SampleBackdoorTrainingBackdoorClassifyClassifyDDbM^MTT6=^M(T)M(T)Of3 Deﬁning Watermarking
We now deﬁne watermarking for ML algorithms. The
terminology and deﬁnitions are inspired by [7, 26].
We split a watermarking scheme into three algorithms:
(i) a ﬁrst algorithm to generate the secret marking key
mk which is embedded as the watermark, and the pub-
lic veriﬁcation key vk used to detect the watermark later;
(ii) an algorithm to embed the watermark into a model;
and (iii) a third algorithm to verify if a watermark is
present in a model or not. We will allow that the ver-
iﬁcation involves both mk and vk, for reasons that will
become clear later.
Formally, a watermarking scheme is deﬁned by the
three PPT algorithms (KeyGen, Mark, Verify):
• KeyGen() outputs a key pair (mk, vk).
• Mark(M, mk) on input a model M and a marking
key mk, outputs a model ˆM.
• Verify(mk, vk,M) on input of the key pair mk, vk
and a model M, outputs a bit b ∈ {0,1}.
For the sake of brevity, we deﬁne an auxiliary algo-
rithm which simpliﬁes to write deﬁnitions and proofs:
MModel() :
1. Generate M ← Train(O f ).
2. Sample (mk, vk) ← KeyGen().
3. Compute ˆM ← Mark(M, mk).
4. Output (M, ˆM, mk, vk).
The three algorithms (KeyGen, Mark, Verify) should
correctly work together, meaning that a model water-
marked with an honestly generated key should be veriﬁed
as such. This is called correctness, and formally requires
that
(cid:2)Verify(mk, vk, ˆM) = 1(cid:3) = 1.
Pr
(M, ˆM,mk,vk)←MModel()
A depiction of this can be found in Figure 3.
In terms of security, a watermarking scheme must
be functionality-preserving, provide unremovability, un-
forgeability and enforce non-trivial ownership:
• We say that a scheme is functionality-preserving if
a model with a watermark is as accurate as a model
without it: for any (M, ˆM, mk, vk) ← MModel(), it
holds that
Pr
x∈D
≈ Pr
x∈D
(cid:2)Classify(x, ˆM) = f (x)(cid:3).
[Classify(x,M) = f (x)]
Figure 3: A schematic illustration of watermarking a
neural network.
• Non-trivial ownership means that even an attacker
which knows our watermarking algorithm is not
able to generate in advance a key pair (mk, vk) that
allows him to claim ownership of arbitrary models
that are unknown to him. Formally, a watermark
does not have trivial ownership if every PPT algo-
rithm A only has negligible probability for winning
the following game:
1. Run A to compute ( ˜mk, ˜vk) ← A().
2. Compute (M, ˆM, mk, vk) ← MModel().
3. A wins if Verify( ˜mk, ˜vk, ˆM) = 1.
• Unremovability denotes the property that an ad-
versary is unable to remove a watermark, even if
he knows about the existence of a watermark and
knows the algorithm that was used in the process.
We require that for every PPT algorithm A the
chance of winning the following game is negligible:
1. Compute (M, ˆM, mk, vk) ← MModel().
2. Run A and compute ˜M ← A(O f , ˆM, vk).
3. A wins if
Pr
x∈D
(cid:2)Classify(x, ˜M) = f (x)(cid:3)
[Classify(x,M) = f (x)]
≈ Pr
x∈D
and Verify(mk, vk, ˜M) = 0.
• Unforgeability means that an adversary that knows
the veriﬁcation key vk, but does not know the key
mk, will be unable to convince a third party that he
(the adversary) owns the model. Namely, it is re-
quired that for every PPT algorithm A, the chance
of winning the following game is negligible:
1. Compute (M, ˆM, mk, vk) ← MModel().
2. Run the adversary ( ˜M, ˜mk) ← A(O f , ˆM, vk).
3. A wins if Verify( ˜mk, vk, ˜M) = 1.
USENIX Association
27th USENIX Security Symposium    1619
KeyGenMarkVerify0=1^MMmk(mk);vkTwo other properties, which might be of practical in-
terest but are either too complex to achieve or contrary to
our deﬁnitions, are Ownership Piracy and different de-
grees of Veriﬁability,
• Ownership Piracy means that an attacker is attempt-
ing to implant his watermark into a model which has
already been watermarked before. Here, the goal is
that the old watermark at least persists. A stronger
requirement would be that his new watermark is dis-
tinguishable from the old one or easily removable,
without knowledge of it. Indeed, we will later show
in Section 5.5 that a version of our practical con-
struction fulﬁlls this strong deﬁnition. On the other
hand, a removable watermark is obviously in gen-
eral inconsistent with Unremovability, so we leave3
it out in our theoretical construction.
• A watermarking scheme that uses the veriﬁcation
procedure Verify is called privately veriﬁable. In
such a setting, one can convince a third party about
ownership using Verify as long as this third party
is honest and does not release the key pair (mk, vk),
which crucially is input to it. We call a scheme pub-
licly veriﬁable if there exists an interactive proto-
col PVerify that, on input mk, vk,M by the prover
and vk,M by the veriﬁer outputs the same value as
Verify (except with negligible probability), such
that the same key vk can be used in multiple proofs
of ownership.
4 Watermarking From Backdooring
This section gives a theoretical construction of privately
veriﬁable watermarking based on any strong backdoor-
ing (as outlined in Section 2) and a commitment scheme.
On a high level, the algorithm ﬁrst embeds a backdoor
into the model; this backdoor itself is the marking key,
while a commitment to it serves as the veriﬁcation key.
let (Train, Classify) be an ε-
accurate ML algorithm, Backdoor be a strong backdoor-
ing algorithm and (Com, Open) be a statistically hiding
commitment scheme. Then deﬁne the three algorithms
(KeyGen, Mark, Verify) as follows.
More concretely,
KeyGen() :
1. Run (T,TL) = b ← SampleBackdoor(O f ) where
T = {t(1), . . . ,t(n)} and TL = {T (1)
3Indeed, Ownership Piracy is only meaningful if the watermark was
originally inserted during Train, whereas the adversary will have to
make adjustments to a pre-trained model. This gap is exactly what we
explore in Section 5.5.
L }.
L , . . . ,T (n)
2. Sample 2n random strings r(i)
t
L ← {0,1}n and
,r(i)
generate 2n commitments {c(i)
L }i∈[n] where
,c(i)
t
L ← Com(T (i)
t ← Com(t(i),r(i)
L ,r(i)
c(i)
t ), c(i)
L ).
3. Set mk ← (b,{r(i)
L }i∈[n]), vk ← {c(i)
,r(i)
t
and return (mk, vk).
L }i∈[n]
,c(i)
t
Mark(M, mk) :
1. Let mk = (b,{r(i)
t
2. Compute and output ˆM ← Backdoor(O f , b,M).
L }i∈[n]).
,r(i)
Verify(mk, vk,M) :
1. Let mk = (b,{r(i)
t
For b = (T,TL) test if ∀t(i) ∈ T : T (i)
not, then output 0.
L
L }i∈[n]), vk = {c(i)
,r(i)
L }i∈[n].
,c(i)
t
(cid:54)= f (t(i)). If
2. For all i ∈ [n] check that Open(c(i)
t
,t(i),r(i)
t ) = 1 and
Open(c(i)
L ,T (i)
L ,r(i)
L ) = 1. Otherwise output 0.
3. For all i ∈ [n] test that Classify(t(i),M) = T (i)
L . If
this is true for all but ε|T| elements from T then
output 1, else output 0.
We want to remark that this construction captures both
the watermarking of an existing model and the training
from scratch. We now prove the security of the construc-
tion.
Theorem 1. Let D be of super-polynomial size in n.
Then assuming the existence of a commitment scheme
and a strong backdooring scheme, the aforementioned
algorithms (KeyGen, Mark, Verify) form a privately
veriﬁable watermarking scheme.
The proof, on a very high level, works as follows:
a model containing a strong backdoor means that this
backdoor, and therefore the watermark, cannot be re-
moved. Additionally, by the hiding property of the com-
mitment scheme the veriﬁcation key will not provide any
useful information to the adversary about the backdoor
used, while the binding property ensures that one cannot
claim ownership of arbitrary models. In the proof, spe-
cial care must be taken as we use reductions from the wa-
termarking algorithm to the security of both the underly-
ing backdoor and the commitment scheme. To be mean-
ingful, those reductions must have much smaller runtime
than actually breaking these assumptions directly. While
this is easy in the case of the commitment scheme, re-
ductions to backdoor security need more attention.
Proof. We prove the following properties:
1620    27th USENIX Security Symposium
USENIX Association
Correctness. By construction, ˆM which is returned by
Mark will disagree with b on elements from T with prob-
ability at most ε, so in total at least (1− ε)|T| elements
agree by the deﬁnition of a backdoor. Verify outputs 1
if ˆM disagrees with b on at most ε|T| elements.
Functionality-preserving. Assume that Backdoor is
a backdooring algorithm, then by its deﬁnition the model
ˆM is accurate outside of the trigger set of the backdoor,
i.e.
(cid:2) f (x) (cid:54)= Classify( ˆM,x)(cid:3) ≤ ε.
Pr
x∈D\T
ˆM in total will then err on a fraction at most ε(cid:48) =
ε + n/|D|, and because D by assumption is super-
polynomially large in n ε(cid:48) is negligibly close to ε.
Non-trivial ownership. To win, A must guess the cor-
rect labels for a 1−ε fraction of ˜T in advance, as A can-
not change the chosen value ˜T , ˜TL after seeing the model
due to the binding property of the commitment scheme.
As KeyGen chooses the set T in mk uniformly at ran-
dom, whichever set A ﬁxes for
˜mk will intersect with T
only with negligible probability by deﬁnition (due to the
multiple trigger sets property). So assume for simplicity
that ˜T does not intersect with T . Now A can choose ˜T to
be of elements either from within D or outside of it. Let
n1 = |D∩ ˜T| and n2 = | ˜T|− n1.
For the beneﬁt of the adversary, we make the strong
assumption that whenever M is inaccurate for x ∈ D∩ ˜T
then it classiﬁes to the label in ˜TL. But as M is ε-accurate
on D, the ratio of incorrectly classiﬁed committed la-
bels is (1− ε)n1. For every choice ε < 0.5 we have that
εn1 < (1− ε)n1. Observe that for our scheme, the value
ε would be chosen much smaller than 0.5 and therefore
this inequality always holds.
On the other hand, let’s look at all values of ˜T that
lie in D\ D. By the assumption about machine learning
that we made in its deﬁnition, if the input was chosen
independently of M and it lies outside of D then M will in
expectancy misclassify |L|−1|L| n2 elements. We then have
|L|−1|L| n2 as ε < 0.5 and L ≥ 2. As εn = εn1 +
that εn2 <
εn2, the error of ˜T must be larger than εn.
Unremovability. Assume that there exists no algo-
rithm that can generate an ε-accurate model N in time
t of f , where t is a lot smaller that the time necessary
for training such an accurate model using Train. At
the same time, assume that the adversary A breaking the
unremovability property takes time approximately t. By
deﬁnition, after running A on input M, vk it will output a
model ˜M which will be ε-accurate and at least a (1− ε)-
fraction of the elements from the set T will be classi-