the results imply, GAA brings computation overheads on a
similar scale compared with previous defenses, which roughly
corresponds to the theoretical complexity listed in Table 1.
Table 3: Time cost of distributed learning with each defense
(sec. / 100 iterations), where - means the 100 iterations have
not ﬁnished in one hour.
GAA
8.14
129.50
2.40
11.15
GeoMed Krum Bulyan Brute-Force
6.32
116.85
1.45
8.77
15.79
118.69
1.85
18.57
15.85
118.73
1.76
17.70
698
-
13.16
1877
MNIST
CIFAR-10
Yelp
Healthcare
Classical
-
-
-
4.76
6.2 Robustness against Adaptive Attacks on
the RL mechanism
In this part, we evaluate the robustness of GAA when the
adversary attempts to mislead the credit assignment by letting
the manipulated workers pretend to be benign.
6.2.1 Comparison with Baselines. First, we evaluate the four
benchmark systems under the randomized attack of q = 0.5,
p = 5 and the pretense attack of β = 0.7,L = 1000, when
GAA and other baseline defenses are equipped. Each worker
is assumed to play the Byzantine role with RF. For random-
ized attacks, 24 out of 49 compromised workers are initially
malicious on MNIST, CIFAR-10 & Healthcare and 4 out of 9
on Yelp. Fig. 5 plots learning curves of the benchmark sys-
tems when different defenses are equipped, where the shaded
part of the curves denotes the variance of the accuracy within
10 repetitions.
Results & Analysis. As we can see from Fig. 5, , GAA is
the only defense that is robust against both randomized and
pretense attacks. For example, Fig. 5(a)&(e) shows GAA
helps the benchmark system on MNIST achieve about 90%
accuracy on average, which is close to the 96.4% accuracy
of the system under no attack. As a comparison, the sys-
tems equipped with the baseline defenses either has ﬁnal
performance much lower than the expected or totally stagnate.
Moreover, from Fig. 5(e)-(h), we ﬁnd no ﬂuctuation happens
when the manipulated workers begin to attack after 1K rounds,
which implies the RL mechanism of GAA is robust against
pretense. Below, we present a more careful evaluation of GAA
under a wide range of attack conﬁgurations.
6.2.2 GAA under Adaptive Attacks with Varied Conﬁg-
urations. Besides, we further evaluate GAA’s robustness
against the randomized attacks and the pretense attacks with
diverse conﬁgurations on Yelp and Healthcare. Fig. 6 presents
the learning curves of the underlying benchmark systems
under attacks of varied conﬁgurations listed in the legends,
where the shaded part of the curves denotes the variance of
the accuracy within 10 repetitions.
Results & Analysis. As we can see from Fig. 6, under ran-
domized Byzantine attacks of most conﬁgurations, GAA helps
the benchmark systems on Yelp and Healthcare achieve de-
sirable performance, compared with the accuracy of systems
without Byzantine attacks. For example, in most conﬁgura-
tions for Yelp, the ﬁnal accuracy is around 83%, which is
close to the optimal accuracy 84.5%. Although from Fig. 6(b)
we notice the q = 0.0 case on Yelp has a larger variance, the
average ﬁnal accuracy is only about 10% lower compared
with the optimal accuracy, which is still acceptable consid-
ering the high Byzantine ratio up to 0.7. Similarly, from Fig.
USENIX Association
29th USENIX Security Symposium    1649
Figure 6: Learning curves of the benchmark systems on Yelp and Healthcare when GAA is applied for defending against
randomized attacks with varied role-change period (the ﬁrst column), role-change probability (the second column), initial
Byzantine ratio (the third column) and against pretense attacks with varied pretense rounds (the last column). The legend
describes the detailed conﬁgurations.
6(d)&(g), we also ﬁnd the different conﬁgurations of the pre-
tense attacks has very limited inﬂuence on GAA’s defense
quality.
i ∝ ∇θ((cid:96)(θt ,D0)− α(cid:96)(θt ,D1)), where α is a
t with AF by V t
hyperparameter that controls the stealthiness of the adaptive
fault.
6.3.1 Adaptive Faults in Case A. We choose the D0 as the
full QV set, and the D1 as the local training set of the ma-
nipulated workers. The parameter α in AF is set as 10. We
conduct the GAA defense under three typical attack patterns
listed in the legends of Fig. 7(a)&(b), which show the learn-
ing curves of the benchmark systems under the considered
adaptive attack on the QV set.
Results & Analysis. From Fig. 7(a)&(b), we ﬁnd in most
cases the ﬁnal accuracy of the benchmark systems remains
close to the optimal accuracy. For example, under the combo
adaptive attack on both the RL mechanism and the QV set
(i.e., Conﬁg. b in Fig. 7(a)&(b)), GAA achieves respectively
about 82% and 65% accuracy on Yelp and Healthcare, which
is close to the performance of the system under no attack.
The results imply that, GAA is robust against the adaptive
adversary knowing the distribution where the QV set is sam-
pled. From our perspective, lacking the knowledge of the
exact QV set would let the adversary only count on his/her
own inexact guess on the QV set. Hence, combining with the
malice on maximizing the loss on the local training set, the
gradient directions crafted by the malicious workers would
be less effective in minimizing the loss on the QV set than the
benign workers and therefore would be less trusted by GAA.
However, when the adversary somehow knows the exact QV
set the server uses, he/she would craft gradients that always
minimize the loss on the QV set and mislead GAA to fully
trust the manipulated worker, while this case would be rare,
if not impossible, depending on the randomness of sampling
and the security of the server.
6.3.2 Adaptive Attacks in Case B. In this setting, the manip-
ulated worker can target on the missing classes by maximizing
the loss on samples belonging to these missing classes, which
forms the D0, while minimizing the loss of samples from
other existing classes, which forms D1.
Experimental Settings. We ﬁrst sample 10 records from the
6.3 Robustness against Adaptive Attacks on
the Quasi-Validation Set
Although Assumption 1 and the randomness in the composi-
tion of the Quasi-Validation set (abbrev. QV set) imply the
exact samples in the QV set is hard to be known by the ad-
versary, we further examine the following two worst-case
leakages of the QV set, which may allow the adversary to
submit carefully crafted gradients (or called Adaptive Fault
(AF)) based on the knowledge of the QV set to attempt to
mislead GAA.
• Case A. The adversary knows the distribution where the
QV set is sampled.
• Case B. Some classes are missing in the QV set and the
malicious worker can target on the missing classes.
Intuitively, Case A is possible when the adversary expects
GAA would use samples from similar data domains as the
QV set, while Case B is possible when the QV set is too
small to cover all different classes. It is worth to notice, for
the adversary in Case A, the probability of determining the
exact samples in the QV set is very low in theory, as the QV
set contains less than 10 samples that are chosen indepen-
dently by the server while the sample space of the distribution
known to the adversary, practically the local dataset held by
the manipulated worker, can contain as large as 103 samples
when deep learning models are deployed.
In both cases, we consider the AF follows the same prin-
ciple: it minimizes the loss on a dataset D0, which is chosen
based on the knowledge about the QV set, to tempt GAA to
assign the manipulated worker with high credit. In the mean-
while, the AF maximizes the overall loss on D1, (a subset
of) its own training set, to compromise the whole distributed
learning process. Accordingly, we formulate the gradient V t
i
submitted by a malicious worker (i.e., Worker i) at iteration
1650    29th USENIX Security Symposium
USENIX Association
tively more threatening than static attacks, where the threat
is not further enlarged when the adversary exploits both the
knowledge on the RL mechanism and the QV set, if compar-
ing Conﬁg. b & c in Fig. 7(a) & (b) with the corresponding
results in Fig. 5. These phenomena interestingly show, the
more knowledge the adversary has of the deployed defense,
the more threatening the attack could be against GAA.
6.4 Byzantine Worker Detection
In this part, we report the accuracy of Byzantine worker de-
tection when the system is under static Byzantine attacks via
our proposed GAA+ in Proc. 2, compared with the baseline
method the GeoMed+ algorithm in Proc. 1.
Table 4: Precision-recall of Byzantine worker detection meth-
ods.
β = 0.3
β = 0.7
K=1
K =5
K =15
K=1
K =10
K =35
GAA+
GeoMed+
99.7%/6.65% 100%/6.67%
99.7%/33.2% 100%/33.3%
99.8%/99.8% 100%/100%
0.0%/0.0%
99.9%/2.85%
99.9%/28.5%
0.0%/0.0%
99.9%/99.9% 57.1%/57.1%
Experimental Settings. By choosing Byzantine ratio β =
0.3,0.7, we apply two detection algorithms on MNIST with
the total number of workers as 50. Since we have deﬁned the
task of Byzantine worker detection as a top-K classiﬁcation
task, we report precision/recall in Table 4. Both precision
and recall are calculated as an average over 1× 103 randomly
subsequent iterations after 1× 104 iterations of distributed
learning with GAA.
Results & Analysis. As we can see from above, with small
Byzantine ratio, both GeoMed+ and our method achieve near
perfect detection of each Byzantine worker. These empirical
results not only justify that GeoMed+ is indeed a strong base-
line, but also validates GAA+’s comparable performance with
statistical counterparts in slight Byzantium. However, when
the Byzantine ratio β is set up to 0.7, GeoMed+ fails to detect
Byzantine workers any longer, while our method still detects
each Byzantine worker perfectly, regardless of its majority in
total.
6.5 Visualizing Byzantine Attack Patterns
In the ﬁnal part of experiments, we present several interesting
visualizations on the policy curve of GAA after learning un-
der randomized attacks of q = 1.0, that is, each manipulated
worker inverses its role periodically.
Experimental Settings. We consider two speciﬁc random-
ized attacks on MNIST with the following conﬁgurations:
(a) n = 10,q = 1.0, p = 1k with initial β = 0.9 and (b)
n = 10,q = 1.0, p = 400 with initial β = 0.5. In other words,
we consider the cases when all workers are manipulated and
Figure 7: Learning curves of the benchmark systems on Yelp
and Healthcare when GAA is applied for defending against
adaptive faults in two cases of varied conﬁgurations.
full QV set on Healthcare (Yelp) to cover all the classes. For
Healthcare, we reduce the number of classes from 9 to 1 with
stride 2 by eliminating the samples belonging to the miss-
ing classes that we specify. For Yelp, we consider the case
when the QV set contains only positive or only negative sam-
ples. With the QV sets with missing classes, we conduct the
GAA defense against three typical attack patterns listed in
the legends and titles of Fig. 7(c)-(f), which present the learn-
ing curves of the benchmark systems under the considered
adaptive attack on the QV set.
Results & Analysis. As we can see from Fig. 7(c)-(f), even
when the adversary targets on the missing classes in the QV
set, GAA is still able to guarantee the benchmark systems
to reach satisfying performance. For example, under static
Byzantine attacks on Healthcare (in Fig. 7(d)), the ﬁnal per-
formance with 5 missing classes in the QV set is around 75%,
even better than the 73.1% accuracy of the system under no
attack. Also, Conﬁg. c in Fig. 7(c) and Fig. 7(f) demonstrates
GAA remains robustness under combo attacks on the RL
mechanism and the missing classes. Furthermore, we notice
the number of missing classes has minor inﬂuence on GAA’s
defense quality, which strongly demonstrates the robustness
of GAA against the adaptive adversary knowing the missing
classes in the QV set.
6.3.3 GAA vs. Different Attacks. Despite the robustness of
GAA against various attacks, the empirical performance does
show subtle differences when GAA is against different attacks.
For example, comparing Fig. 5 and Fig. 4, we ﬁnd that the
ﬁnal accuracy of the benchmark systems under randomized
and pretense attacks, two attacks exploiting the knowledge
that GAA uses the RL mechanism to learn credit, is overall no
better than that under static attacks. Similarly, as we can see
from the corresponding results in Fig. 7 and Fig. 4, adaptive
attacks that exploits the knowledge on the QV set are rela-
USENIX Association
29th USENIX Security Symposium    1651
Figure 8: Capture periodic information of randomized Byzan-
tine attack with GAA.
invert their role periodically. We collect GAA’s action se-
quence in each conﬁguration up to 40k rounds and plot the
policy curves of each worker over a representative slice of it-
erations in Fig. 8 after normalization, where the policy curves
for the initially Byzantine workers are warm-toned and the
initially benign workers cool-toned.
Results & Analysis. First, in both cases the periodic charac-
teristic of the undertaking Byzantine attack is captured well
by our GAA, as its policy curve presents a period close to the
ground-truth. To analyze with more care, we notice, in Fig.
8(b), as GAA’s decision on Byzantine workers appears to be
correct initially, its policy curve mainly evolves vertically. In
other words, GAA tends to behave stable after an optimal pol-
icy is attained. Differently in Fig.8(a), although a low credit
is assigned to the only initially benign worker in the ﬁrst half
period, GAA wisely skips the other half and swiftly adjust its
policy in the subsequent period by heuristics of reward. The
phenomenon is highlighted by the slashed region in Fig. 8(a).
7 Discussion
On Assumptions 1 & 2. Assumption 1 is used to guarantee
the correct execution of Algorithm 1 and GAA itself would
not be compromised by the adversary, while Assumption 2
is used to guarantee GAA has at least one worker to trust.
We claim both assumptions are reasonable. On one hand,
the former assumption is commonly assumed in previous
studies of Byzantine robustness [4,11,15,16,20,31,62], which
serves as a standing point of most published defenses, since
otherwise the adversary could easily tamper the global model
itself. On the other hand, the security level of the central server
in real world distributed systems is always on a much higher
level than working nodes, due to, e.g., rigorous access control
mechanisms [55]. Therefore, the cost of attacks on central
server is much higher than that on workers.
Moreover, we ﬁnd it is quite straightforward to satisfy As-