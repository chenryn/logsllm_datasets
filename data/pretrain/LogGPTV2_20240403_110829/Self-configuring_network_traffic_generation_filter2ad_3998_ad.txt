Table 4: Summary of Data Sets Used in Validation
Experiments
Data Set
Wisconsin 31 July–6 August 2002
Auckland 11–12 June 2001
Total Flows TCP Flows
241,484,962
12,144,434
423,836,790
12,912,462
5.1 Tests
Our validation test environment consisted of two work-
stations running the FreeBSD 5.1 operating system. Each
machine had 2 GHz Intel Pentium 4 processors, 1 Giga-
byte of RAM, and used kernel defaults for TCP/IP param-
eters, which meant that the NewReno congestion control
algorithm, time stamping, and window scaling TCP options
were enabled, and the default receive window size was 64kB.
The default Ethernet MTU of 1500 bytes was used. Each
machine was dual-homed, with a separate Intel Pro/1000
interface used solely for experiments. The machines were
each connected to a Cisco 6509 router via 1 Gbps Ethernet.
One FreeBSD machine was used as Harpoon data server
and two were used as Harpoon clients for generating requests
and receiving the resulting data packets. We monitored each
host during our tests to ensure the systems did not exhaust
all available CPU or memory resources.
The client machines were conﬁgured to generate requests
using an IPv4 class C address space (28 addresses). Like-
wise, the server machine was conﬁgured to handle requests
to an IPv4 class C address space. In each case, the address
creation was accomplished by creating aliases of the loop-
back interface, and creating static routes at the Harpoon
clients, server and intermediate switch. Aliases of the loop-
back adapter were created to avoid ARP overhead with the
switch.
5.2 Results
Figures 6 and 9 examine Harpoon’s capability to gen-
erate the desired temporal characteristics for the Wiscon-
sin and Auckland data sets, respectively. Figure 6 com-
pares the bitrate over a period of 8 hours from the original
Wisconsin data with Harpoon. Figure 9 compares the bi-
trate over a period of two days of the original Auckland
data with the bitrates produced by Harpoon when emu-
lating the same time period. The level of traﬃc is deter-
mined by the input distribution specifying the number of
active users (PActiveSessions).
In each test, the Harpoon
traﬃc exhibits burstiness due to the distribution of ﬁle sizes,
inter-connection times, and the closed-loop nature of TCP.
Nonetheless, over the duration of each test the hourly pat-
tern emerges due to control over the number of active ses-
sions at the Harpoon client process. Some variability is in-
troduced because Harpoon does not immediately truncate
an active session at each emulation interval. Instead, it lets
the ﬁnal connection ﬁnish naturally, thus avoiding abrupt
shifts between each interval.
Figure 7 compares the inter-connection time, ﬁle size,
and destination frequency empirical distributions derived
from the Wisconsin data set with the distributions gener-
ated by Harpoon. For inter-connection times, shown in Fig-
ure 7(a), there is a good overall match except at the shortest
interest, such as scaling characteristics, queue length
distribution for the ﬁrst-hop router, packet loss pro-
cess, and ﬂow durations.
Packet-level dynamics created by Harpoon arise from the
ﬁle size distribution, the inter-connection time distribution,
TCP implementations on end hosts, and testbed parameters
such as round-trip time distribution, link capacities, and
MTU sizes. Round-trip time is a key parameter because it
aﬀects ﬂow durations for TCP sources. As a consequence,
the nature of the ON/OFF process and therefore the corre-
lation structure of packet arrivals over both short and long
time scales are aﬀected. It was shown in [29] that dynamics
over short time scales diﬀer between LAN and WAN environ-
ments, and in [28] that short time scale eﬀects arise due to
the TCP feedback loop and network environment variabil-
ity characteristic of WANs. Eﬀects across both short and
long time scales are of interest in testbed traﬃc generation
because performance measurements can diﬀer substantially
between LAN and WAN settings (e.g., see [19]).
In Figure 11(b) we compare time series of bytes trans-
ferred over 1 second intervals between a segment of the
Auckland trace and Harpoon using an IntervalDuration
of 300 seconds. There is no propagation delay emulated in
the testbed, so the round-trip time for Harpoon sources is on
the order of 1 millisecond. There is greater variability evi-
dent in the testbed conﬁguration, in part, from a tight TCP
feedback loop resulting in higher throughput and shorter
ﬂow durations. For the Auckland trace, dominant round-
trip times of around 200 milliseconds (roughly the RTT to
North America) lead to longer ﬂow durations, a greater de-
gree in ﬂow concurrency, and less observable variability in
utilization.
A standard method of characterizing behavior of traﬃc
dynamics over a range of time scales is the log-scale diagram
described in [16]. The log-scale diagram was developed as
a means for identifying self-similar scaling in network traf-
ﬁc and for estimating the Hurst parameter. Evidence for
self-similar scaling is associated with a range of scales where
there exists a particular linear relationship between scale
and the log of a normalized measure of energy (variabil-
ity). A diﬀerent scaling regime has been shown to exist in
measured WAN traﬃc and is separated from the linear self-
similar region by a pronounced dip in energy at the time
scale associated with dominant round-trip times.
In Figure 11(c) we compare log-scale diagrams based on
wavelet decompositions of the time series of byte volumes
over 500 microsecond intervals for a one hour segment of
the Auckland data set (12pm-1pm, 11 June 2001) and for
Harpoon with two diﬀerent values of IntervalDuration (60
or 300 seconds) and two emulated round-trip times (0 or 200
milliseconds). (We do not plot conﬁdence intervals for clar-
ity.) We ﬁrst note the diﬀerence between Harpoon conﬁg-
urations using diﬀerent round-trip times. Comparing Har-
poon using an RTT of 200 milliseconds with the original
trace, there is a common range of time scales indicative of
self-similar scaling (scales 9-14) and a clear dip at the 256
millisecond time scale (scale 9) because of dominant round-
trip times in the original trace, and the singular round-trip
time for Harpoon. For the Harpoon conﬁgurations with no
emulated propagation delay, there is more energy across al-
most all time scales and nothing in common with the orig-
inal trace. Finally, we note that over sub-RTT time scales,
Harpoon does not match the Auckland trace for any conﬁgu-
Figure 6: Emulation of temporal volume character-
istics for Wisconsin data.
inter-connection times. The visible “bumps” for the Har-
poon tests at these inter-connection times are an eﬀect of a
coarse-grained operating system scheduler interacting with
the shortest durations of this distribution. FreeBSD (as well
as Linux), by default, uses time slices of 10 milliseconds and
the steps in Figure 7(a) are at multiples of this time slice. In
tests with operating systems that have diﬀerent scheduling
mechanisms (e.g., MacOS X), these particular artifacts do
not appear. Results for the Auckland data set are qualita-
tively similar.
Figure 7(b) compares ﬁle sizes extracted from the Wis-
consin data set with the ﬁle sizes transferred by Harpoon.
There is a close qualitative match, and results for the Auck-
land data set are similar.
Figure 7(c) plots frequency vs.
rank on a log-log scale
for destination addresses for the Wisconsin data set. We
observe a close match between the original data and the
Harpoon data. Results for source addresses are qualitatively
similar, as are results for the Auckland data set.
Our ﬁnal validation test was to compare the traﬃc vol-
umes generated by Harpoon against the original ﬂow traces
used to generate parameters for each data set. In Figure 8
we compare the distributions of packets, bytes, and ﬂows per
measurement interval to those derived from the Wisconsin
data set. We make the same comparisons for the Auckland
data set in Figure 10. As shown in these plots, Harpoon
accurately approximates the number of bytes and ﬂows per
interval. For each data set, there are more packets sent in
the original trace than from Harpoon. The reason is that
our testbed is homogeneous with respect to link-layer max-
imum transmission unit sizes, resulting in average packet
sizes that are larger than the original trace. Figure 11(a)
shows a time series of the mean packet size over 60 second
intervals of a one hour segment (12pm-1pm, 11 June 2001)
of the Auckland data set and a similar time series for Har-
poon. Harpoon packet sizes average almost twice as large
as the original trace. As shown in Figures 8(b) and 10(b),
when we scale the Harpoon volumes by the ratio of average
packet sizes between Harpoon and the measured data, we
observe a close match.
5.3 Limitations
Two limitations to Harpoon’s traﬃc model are:
1. it is designed to match byte, packet, and ﬂow volumes
over relatively coarse intervals (e.g., 300 seconds) and
may not match over shorter intervals;
2. since packet-level dynamics are not speciﬁed, traﬃc
produced by Harpoon may not match other metrics of
18Mbps14Mbps10Mbps7pm5pm3pm1pm31 July ’02, 11amthroughput (bits per second)emulated hourMeasured DataHarpoon(a) Inter-connection time distribution.
(b) File size distribution.
(c) Destination IP address frequency distri-
bution.
Figure 7: Comparison of empirical distributions extracted from Wisconsin data with distributions produced
during Harpoon emulation. Results shown for one day of Wisconsin data (31 July 2002). Results for other
days and for the Auckland data are qualitatively similar.
(a) Byte volumes.
(b) Packet volumes. Harpoon data also
shown scaled by ratio of average packet size
between Harpoon and measured data.
(c) Flow volumes.
Figure 8: Comparison of byte, packet, and ﬂow volumes extracted from of Wisconsin data with volumes
produced during Harpoon emulation. Results shown for one day of Wisconsin data (31 July 2002). Results
for other days are qualitatively similar.
Figure 9: Emulation of temporal volume characteristics for Auckland data.
(a) Byte volumes.
(b) Packet volumes. Harpoon data also
shown scaled by ratio of average packet size
between Harpoon and measured data.
(c) Flow volumes.
Figure 10: Comparison of byte, packet, and ﬂow volumes extracted from original Auckland data with volumes
produced during Harpoon emulation.
 0 0.2 0.4 0.6 0.8 1 0.0001 0.001 0.01 0.1 1 10 100 1000cdfinter-connection time (seconds) (log)Measured DataHarpoon 0 0.2 0.4 0.6 0.8 1100MB10MB1MB100kB10kB1kB100cdffile size (bytes) (log)Measured DataHarpoon 0.0001 0.001 0.01 0.1 1 10 100 1000relative frequency (log)destination popularity rank (log)Measured DataHarpoon 0 0.2 0.4 0.6 0.8 11.5B1B0.5Bcdfbytes transferred per 10 minute interval (billions)Measured DataHarpoon 0 0.2 0.4 0.6 0.8 13M2.5M2M1.5M1M0.5Mcdfpackets transferred per 10 minute interval (millions)Measured DataHarpoonHarpoon - Scaled 0 0.2 0.4 0.6 0.8 1180000160000140000120000100000cdfflow arrivals per 10 minute intervalMeasured DataHarpoon5Mbps4Mbps3Mbps2Mbps1Mbps0Mbps12am8pm4pm12pm8am4am12 June, 12am8pm4pm12pm8am4am11 June ’01, 12amthroughput (bits per second)emulated hourMeasured DataHarpoon 0 0.2 0.4 0.6 0.8 11.5B1B0.5Bcdfbytes transferred per 1 hour interval (billions)Measured DataHarpoon 0 0.2 0.4 0.6 0.8 12.5M2M1.5M1M0.5Mcdfpacket transferred per 1 hour interval (millions)Measured DataHarpoonHarpoon - Scaled 0 0.2 0.4 0.6 0.8 1 0 5000 10000 15000 20000 25000 30000 35000cdfflow arrivals per 1 hour intervalMeasured DataHarpoonration. Since our testbed lacks diversity in round-trip times
(as well as MTUs and link capacities), we do not expect a
match over these time scales. Even though Harpoon accu-
rately approximates the original volumes over 300 second
intervals for both round-trip time values (not shown here),
there are vast diﬀerences between the original correlation
structure and that produced by Harpoon.
Comparing the two conﬁgurations of IntervalDuration,
there is slightly less overall energy when using 60 second
intervals because the byte volumes produced by Harpoon
are less than the original volumes. The reason is that the
interval is too small compared with the default maximum
inter-connection time of 60 seconds: a session may randomly
get an inter-connection time that causes it to be idle for the
length of the entire interval. The end eﬀect is that Harpoon
produces less traﬃc than intended (see also Section 4).
These diﬀerences have important implications not only for
Harpoon, but in conﬁguring any laboratory testbed and in
the interpretation of results. First, it is clear that queuing
dynamics will be diﬀerent depending (at least) on conﬁgured
round-trip times, resulting in diﬀerent delay, jitter, and loss
characteristics than might have been measured in a live en-
vironment. Second, such diﬀerences will very likely aﬀect
transport and application performance, requiring careful in-
terpretation of measurements, and characterization of their
applicability outside the laboratory. While we feel that Har-
poon represents a step forward in testbed traﬃc generation,
the goal of predicting performance in a live environment
based on laboratory experiments remains somewhat out of
reach, since specifying the ingredients necessary to produce
realistic packet dynamics over all time scales in both simu-
lation and laboratory testbeds is, in general, an open prob-
lem [42].
6. COMPARISON WITH PACKET-ORIENTED
TRAFFIC GENERATORS
The most obvious application for Harpoon is in scalable
background traﬃc generation for testbed environments such
as Emulab [49] and WAIL [11]. In this section we investigate
another application of Harpoon in a router benchmarking
environment. We compare router performance using work-
loads generated by Harpoon with workloads generated by a
standard packet level traﬃc generator5. Our motivation for
comparing Harpoon with a standard traﬃc generation sys-
tem is to both demonstrate the diﬀerences between the two
approaches, and to consider how the standard tools might
be tuned to exercise routers more comprehensively.
6.1 Environment and Methodology
For the hardware environment used in this series of exper-
iments we used the two Harpoon source machines and one
Harpoon sink machine conﬁgured in the same way as our
validation tests described above. Each host was connected
to a Cisco 6509 router6 [1] via 1 Gbps Ethernet links. Each
link was connected to separate physical line cards on the
6509, forcing traﬃc to cross the internal switching fabric of
5Standard packet generation tool range from free software
tools such as iperf [6] to large-scale, feature-rich hardware