title:Collaborative Filtering with Privacy
author:John F. Canny
Collaborative Filtering with Privacy
John Canny
Computer Science Division
UC Berkeley, CA 94720
PI:EMAIL
Abstract
Server-based collaborative ﬁltering systems have been
very successful in e-commerce and in direct recommenda-
tion applications. In future, they have many potential ap-
plications in ubiquitous computing settings. But today’s
schemes have problems such as loss of privacy, favoring
retail monopolies, and with hampering diffusion of innova-
tions. We propose an alternative model in which users con-
trol all of their log data. We describe an algorithm whereby
a community of users can compute a public “aggregate”
of their data that does not expose individual users’ data.
The aggregate allows personalized recommendations to be
computed by members of the community, or by outsiders.
The numerical algorithm is fast, robust and accurate. Our
method reduces the collaborative ﬁltering task to an itera-
tive calculation of the aggregate requiring only addition of
vectors of user data. Then we use homomorphic encryption
to allow sums of encrypted vectors to be computed and de-
crypted without exposing individual data. We give veriﬁca-
tion schemes for all parties in the computation. Our system
can be implemented with untrusted servers, or with addi-
tional infrastructure, as a fully peer-to-peer (P2P) system.
1
Introduction
Collaborative ﬁltering has important applications in e-
commerce, direct recommendations (such as Movielens and
Ringo) and search engines. Personalized purchase recom-
mendations on a web site are can signiﬁcantly increase
the likelihood over a customer making a purchase, com-
pared to unpersonalized suggestions. In future ubiquitous
computing settings, users will routinely be able to record
their own locations (via GPS on personal computing de-
vices and phones), and their purchases (through digital wal-
lets or through their credit card records). Through collab-
orative ﬁltering, users could get recommendations about
many of their everyday activities, including restaurants,
bars, movies, and interesting sights to see and things to do in
a neighborhood or city. But such applications are infeasible
without strong protection of individual data privacy.
Today’s server-based collaborative ﬁltering systems have
a number of disadvantages. First of all, they are a serious
threat to individual privacy. Most online vendors collect
buying information about their customers, and make rea-
sonable efforts to keep this data private. However, customer
data is a valuable asset and it is routinely sold as such when
companies have suffered bankruptcy. At this time this prac-
tice is supported by case law. A second disadvantage is that
server-based systems encourage monopolies. There are cor-
relations between customer purchase choices across product
domains. So companies that can acquire preference data for
many users in one product domain have a considerable ad-
vantage when entering another. Even within one market,
a large established ﬁrm will have an advantage over any
new competitor, because the latter will have a much smaller
corpus of customer data to draw from, leading to less ac-
curate (and less sucessful) recommendations. From the
customer perspective, their purchase history is fragmented
across many vendors reducing the quality of their recom-
mendations.
Finally there is a subtle but important sociological dis-
advantage. Today’s collaborative ﬁltering algorithms are
all based on ratings from the most similar users to a given
user. In the language of diffusion of innovation [13], this
is called homophilous diffusion. Homophilous diffusion al-
lows rapid diffusion of innovations within socio-economic
groups. But diffusion throughout society requires het-
erophilous diffusion, where individuals seek recommenda-
tions from more advanced peers who are unlike them. The
lack of choice in today’s systems defeats heterophilous dif-
fusion. For instance, although I am not an expert in cook-
ing or gardening or medicine, there are times when I would
like recommendations from those communities, not from
my own peers, or the population as a whole. We propose
a system where communities create their own knowledge
pools, and where they decide whether to share this infor-
mation with outsiders. In some cases, access to this infor-
Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P(cid:146)02) 
1081-6011/02 $17.00 ' 2002 IEEE 
mation could be a service that the community provides to
outsiders for a fee. Communities that do this will allow het-
erophilous diffusion.
We propose a “User-owned and operated” principle for
data such as purchase or document access, or position logs:
Users should have exclusive control and access to all data
recorded about them. They should be able to control how
and with whom the data will be shared. And they should be
able to hide or restrict any part of the data. This provides
us with an interesting algorithm design challenge: is there
a practical algorithm for collaborative ﬁltering using many
users’ data, which does not expose any individual’s data?
In this paper we propose a solution, and argue that it is
practical for an interesting class of application data, given
recent developments in distributed computing infrastruc-
ture. Our scheme is based on distributed computation of
a certain “aggregate” of all users’ data. The aggregate is
treated as public data. Each user constructs the aggregate
and uses local computation to get personalized recommen-
dations. The computation is designed to be done either on
a single reliable server or in peer-to-peer fashion on unreli-
able, untrusted clients. The peer-to-peer architecture allows
users to create and maintain their own recommender groups
themselves.
This approach addresses several of the difﬁculties with
traditional collaborative ﬁltering systems. While it doesnt
prevent vendors from gathering customer data, it provides
the customers with the same recommendation services that
vendors normally provide. Customers can therefore use
anonymized purchase systems which are being developed
elsewhere, and still obtain personalized recommendations.
It blunts the monopoly trend because users can obtain rec-
ommendations directly from their peers, and those recom-
mendations could then route through meta-storefronts (such
as C-net, Yahoo etc.) that point to multiple vendors. And
it addresses the homophily issue via a community-based
model of recommendation. The system we propose makes
it easy for users to set up communities, and to share their
data within several communities. Because a community ag-
gregate hides individual member data, we believe that many
communities will be willing to share their aggregate infor-
mation with outsiders (in some cases perhaps charging for
this service). By listing a community with a portal like Ya-
hoo, a community could encourage outsiders to use their
data1.
Some of the beneﬁts of our scheme could be obtained by
pseudo-identities. Users could use persistent online identi-
ties that do not link to their actual identity, and make anony-
mous purchases. A persistent pseudo-identity allows infer-
ence of purchase patterns, and supports collaborative ﬁl-
tering. However, unless the user data is further protected,
1If the community is popular, the portal would most likely need to
cache the aggregate.
pseudo-identities expose users to signiﬁcant privacy risks.
For example, an attacker who can observe a few of a user’s
transactions may then be able to uniquely identify that user
from the available records. The attacker can then discover
the rest of that user’s purchase records.
We believe that this paper describes a practical applica-
tion of multi-party computation, subject to the availability
of certain distributed or peer-to-peer services (a blackboard
and a source of trusted random bits). Both commercial and
research versions of such services exist today. Beyond col-
laborative ﬁltering, there are many other potential applica-
tions of these techniques in areas such as online surveys,
usability studies, or censuses; where aggregate data is the
goal.
1.1 Assumptions
Let n be the number of users in a community, and m be
the number of items rated by them.
In order to be prac-
tical, our method must have an overall complexity which
is pseudo-linear in n and m separately (there is an obvi-
ous Ω(nm) lower bound). Typical values for the number of
users and items can range from thousands to millions. We
show later that total communication and work for our proto-
col is O(mn log n). In section 4.2 we expand the constants
in the method to show that is practical if both n and m are
at the low to middle part of their ranges.
Users’ computers perform all the computation in the
method.
It would be simpler to do this using a set of
servers as in [5], but this would not achieve our goal of
a community-based system. Our peer-to-peer version in-
cludes additional veriﬁcation steps that [5] does not (their
protocol is “publicly veriﬁable” but veriﬁcation is not in-
cluded in it).
We assume that a fraction α > 1/2 of the users’ ma-
chines are uncorrupted. Our protocol proceeds in rounds,
and we model corruption as a static adversary on each
round [1, 12]. That is, the adversary can choose which ma-
chines to corrupt at the start of a round, but this choice stays
ﬁxed throughout the round. We believe this model is re-
alistic. First of all, in our setting the greatest risk of cor-
ruption is from malicious users who may want to extract
information or inﬂuence the aggregate through their own
machine. The identities of these users will be static. For an
external adversary to corrupt the protocol, signatures and
other information would need to be generated which would
require essentially full control of a large set of users’ ma-
chines. Such corruption is realistically a static process. An
adversary having broken into a machine is unlikely to relin-
quish control of it during the round, nor is the user likely to
be able to detect the fault and repair it during the round.
Even if we could justify an adaptive adversary, it would
not be practical to defend against it. The best multi-
Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P(cid:146)02) 
1081-6011/02 $17.00 ' 2002 IEEE 
2
party protocols for adaptive adversaries have complexity of
O(n3) or higher [4]. This is well outside the realm of possi-
bility for a large-scale peer-to-peer community. Fortunately,
the static adversary does model the risks to our protocol sen-
sibly. This adversary model allows us to use a simple and
efﬁcient means to verify intermediate steps in our protocol
by sampling.
Our method assumes two services: a write-once, read-
many or WORM storage system (a blackboard), and a
trusted source of random bits. The WORM service is no
longer a theoretical abstraction, but is becoming part of
the core services for distributed computing. Small scale
commercial implementations exist in the Groove system
(www.groovenetworks.com). It is not clear whether
Groove scales to thousands or even hundreds of sites. But
other systems under development do. The Oceanstore sys-
tem [8] is designed to provide global-scale services, poten-
tially to millions of sites. Similar services are under devel-
opment as part of the JXTA peer-to-peer API within Java.
We also assume the existence of a trusted source of ran-
dom coin tosses. Our protocol requires O(km log2 n) ran-
dom bits per round. Given the availability of a WORM
provider, generating and distributing these random bits is
straightforward.
1.2 Outline and Contributions
The paper is structured as follows: Section 2 formal-
izes the collaborative ﬁltering problem, explains how we
compute an aggregate model of user preferences, and how
we obtain recommendations from it. The model is a par-
tial SVD (Singular Value Decomposition) of the matrix of
user ratings. We show in section 2 that the SVD algorithm,
which is iterative, requires only vector addition of certain
user data in each iteration. This structure is necessary for
the algorithm to work on encrypted data. In section 3 we
show how to compute the sum of encrypted data vectors,
in a peer-to-peer fashion. Since peers are untrusted, this
section also describes checks for both the original data (via
ZKPs) and the sums computed from it (via a sample ma-
jority). Then in section 4 we give the complete protocol,
along with proofs of its cryptographic protections. Its sta-
tistical protections are discussed later in section 5. In sec-
tion 4.1 we describe experiments with an implementation
of the numerical part of the algorithm. The cryptographic
protocols have not yet been implemented, but we give a de-
tailed analysis of the running time and space requirements
of the algorithm on a typical dataset using a standard cryp-
tographic software toolkit (CRYPTO++). In section 5 we
discuss the statistical vulnerability of the system. Finally,
section 6 gives a discussion and conclusions.
The main contribution of the paper is to make a connec-
tion between cryptographic techniques and an application
domain (collaborative ﬁltering) which we believe makes
economic and practical sense. It is interesting also as plausi-
ble application of encrypted multi-party computation. Col-
laborative ﬁltering using SVD is not new, and was described
in [14]. But that paper used simple inner products to gen-
erate recommendations, while we use the maximum likeli-
hood formulation of section 2.1, which is novel. This im-
proves the mean-square error of our method over [14].
We cannot use a “black-box” SVD algorithm with the
limitations of encrypted computation, so we derive an iter-
ative SVD using the conjugate gradient method of Polak-
Ribiere [11]. This gives us an iteration with only vector ad-
ditions of user data. The derivation is a standard application
of numerical techniques, so we include it as Appendix I.
For the cryptographic portion of the method, with use ideas
from the voting algorithm of Cramer, Gennaro, and Schoen-
makers [5]. For initial key generation, we assume either an
honest dealer, or the distributed key generation scheme of
[9]. While we can use Pedersen directly, Cramer et al.’s [5]
is a server-based protocol. For our peer-to-peer application,
we needed some modiﬁcations and extensions to [5]. The
main extension is the data and tally veriﬁcation section 3.5,
which uses sampling and a trusted source of random bits to
allow clients to compute reliably with mostly-trustworthy
peers. Finally we modiﬁed the multiplication protocol from
[3] for ZKP of products to work for squares (see Appendix
II). The resulting non-interactive proofs are 7 integers rather
than 10 integers long. This reduces the overall computation
and communication cost by a small but signiﬁcant constant
factor.
2 Distributed Collaborative Filtering
Assume there are n users and m items that have been
rated by them. Let P be the matrix of user preference data,
where Pij is the rating given by user i to item j, and i ∈
{1, . . . , n} j ∈ {1, . . . , m}. We set Pij = 0 if user i has
not rated item j, and require that actual ratings are non-
zero. P is a typically a sparse matrix with many missing
ratings (density is 0.03 for the EachMovie dataset). We will