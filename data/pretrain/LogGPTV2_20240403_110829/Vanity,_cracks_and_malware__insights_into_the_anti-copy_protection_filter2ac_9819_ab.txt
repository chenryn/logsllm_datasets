Manual:
Total:
0
185
185
# Web # OCH # BT # Usenet # Total
43,609
363
43,972
20,233
85
20,318
21,574
0
21,574
1,802
93
1,895
Table 2: Overall Collected Download Links
OCH
Megashares.com
Letitbit.net
Media(cid:12)re.com
Rapidshare.com
4last(cid:12)le.com
Uploading.com
Others
Total
# Links Link % # DL DL %
21.32
36.66
13.50
19.70
1.46
2.53
4.83
100.00
14,345
2,041
1,087
957
401
171
1,318
20,320
70.60
10.04
5.35
4.72
1.97
0.84
6.48
100.00
1,121
1,928
710
1,036
77
133
254
5,259
Table 3: Distribution of Crack and Keygen Crawl
Links and Successful Downloads on OCHs
once it is (cid:12)nished. Since Usenet downloads are usually com-
pressed (mostly RAR or ZIP archives), post-processing was
required for downloads that included archives. For each de-
compressed download, we located all potential cracks and
keygens (i.e. executable (cid:12)les) and added them to our repos-
itory. We discarded any downloads that included no exe-
cutable (cid:12)les or downloads we could not decompress (i.e. due
to corrupted archives or password protection). From 21,574
collected download links, we extracted a total of 17,434 ex-
ecutables, meaning that 80:81% of our download links re-
sulted in an executable (cid:12)le.
4.2.3 One Click Hosting Downloads
For OCH downloads, we used the JDownloader [9] down-
load management tool. This Java-based framework supports
over 100 di(cid:11)erent One Click Hosters and comes with an
integrated proxy-functionality to facilitate rotating IP ad-
dresses. Furthermore, it features an automatic CAPTCHA
solver and a built-in decompression tool for the most com-
mon archive types, including basic password-guessing for
protected archives. While these features were su(cid:14)cient for
most Hosters, we discovered that a signi(cid:12)cant amount of
our downloads were hosted on Letitbit.net [4] (see Table 3).
To overcome unacceptably long wait times for this Hoster,
we decided to acquire a premium account for Letitbit.net.
In contrast to our expectations, with the premium account,
we still experienced download speed limitations and waiting
times, but in general, the downloads worked as intended.
Table 3 shows the distribution of crawl links and successful
downloads for One Click Hosters. Out of 20,320 downloads
links, 5,259 downloads (25.88%) were successfully retrieved
and unpacked. The rest of the downloads was not available
for various reasons (i.e. removed downloads, server errors,
password protected downloads, etc.).
Interestingly, more than 70% (14,345) of our crawled links
were for Megashares.com, but due to the high number of re-
moved downloads, we could only retrieve 7:8% (1,121) of
them. We discovered, that the majority of our success-
ful crack and keygen downloads (36:66%) came from Letit-
bit.net.
812Figure 2: Automated Acquisition and Analysis Architecture
4.2.4 BitTorrent Downloads
For BitTorrent downloads, we used the Transmission Bit-
Torrent Client [13] as it includes a remote control interface
that allowed integration into our automated acquisition sys-
tem (Fig. 2). Our Isohunt.com crawler collected and re-
trieved torrent (cid:12)les for downloads with a size of at most
10MB and with at least 5 seeders. Our crawler scripts imme-
diately added the collected torrent (cid:12)les to the Transmission
Client to minimize the time between searching and down-
loading.
In addition, our scripts constantly removed (cid:12)n-
ished downloads from the Transmission Client. Similar to
our approach for Usenet and OCH downloads, we used cus-
tom post processing scripts to decompress downloads and
to add potential crack and keygen executables to our repos-
itory. We ran our BitTorrent client for a period of 1 month
to ensure that downloads with very few sources could still
succeed eventually. However, it turned out, that the num-
ber of the seeders returned by Isohunt.com was not up to
date and, as a result, many of our downloads had no seeders
at all within the given time frame. In total, out of the col-
lected 1,900 torrent (cid:12)les, only 356 (18.74%) downloads suc-
ceeded. To avoid distributing copyright-protected or illegal
software, we suppressed uploads from our clients altogether.
The negative impact on overall download speed was severe
but acceptable.
In total, we collected 43,979 download links and obtained
23,131 potential crack and keygen executables.
However, in conjunction with the executable downloads from
all other data sources (Web, OCH and BitTorrent), only
3,561 (15:39%) of the overall executables were unique. We
used these executables as basis for our further analyses. It
is also important to note here, that manually and automat-
ically downloaded samples overlap. To be speci(cid:12)c, 43% of
the manually downloaded cracks can also be found in the au-
tomatically acquired ones. This number suggests that (cid:12)les
distributed over various sources are largely the same. Fur-
thermore, the detailed analysis performed on the manually
downloaded samples are representative for the larger, auto-
mated sample base.
4.3 Analysis Setup
To gain more insight into anti-copy protection software
tools, we decided to follow both manual and automatic anal-
ysis approaches. While automatic analysis can cover a large
# DL Links
# EXE
# Unique
Web
185
82
67
OCH
20,320
5,259
1,379
BT Usenet
21,574
17,434
2,054
1,900
356
230
Total
43,979
23,131
3,561
Table 4: Overall Download Results
amount of samples easily, it also has a number of limitations.
For instance, vital information such as whether a speci(cid:12)c
crack or keygen can actually unlock a software product or
whether the software product still runs stable after apply-
ing a patch, can not be gained through automatic analysis.
The reasons for this are manifold. Most cracks and keygens
need to be applied in speci(cid:12)c ways and the accompanying
instructions need to be closely followed by the user. Some
of the tools might not even run out of the box, for example
due to missing dependencies. In general, the necessary user
interactions closely depend on the anti-copy protection tool
and the software application. Similarly, testing whether a
tool managed to successfully break the copy-protections and
whether the unlocked software product is still in a usable and
stable condition afterwards, closely depends on the software
product itself. As a result, we decided to combine auto-
matic analysis with manual analysis results. However, the
manual approach has limitations on its own. Most notably,
due to the extensive amount of manual work involved, only
a fraction of all available samples can be analyzed in detail.
4.3.1 Manual Analysis
We focused our manual analysis approach on the software
products that we covered in the manual acquisition phase
(see Table 8). Our manual analysis setup is shown in Fig. 3.
Figure 3: Manual Analysis Setup
UsenetOCHBitTorrentNzbindex.nlFilestube.comIsohunt.comDownloadLinksJDownloaderTransmissionTorrent ClientNZBGetCrack/KeygenDownloadsAnubisAutomated AnalysisEnvironmentDynamic Analysis ReportsStatic Analysis ReportsCrawlersDownloadersGame VMsApplication VMsCracksKeygensPacketCaptureInstallationResultsAntiVirusResults813For each of the software products, we set up a separate
virtual machine, where the product and all its software de-
pendencies (e.g.
.NET, DirectX, etc.) were installed. Due
to the less stringent security features in comparison to Win-
dows 7, we chose to use Windows XP SP2 32-bit as the op-
erating system for the virtual machines. However, especially
for some games, it was required to install Windows 7 instead.
Besides, we installed a state of the art virus scanner (AVG
free [30]) with up to date virus de(cid:12)nitions and real-time (i.e.
Resident Shield) protection. We set up each of the software
products to the point where either license information had
to be entered or the software refused to work due to expired
evaluation licenses. After that, we performed a whole sys-
tem virus scan within all virtual machines to ensure that
the system was free from malware infections. In this state,
we created a snapshot (which we denote base snapshot) for
each virtual machine, forming the basis for any subsequent
experiments.
We manually tested all of the 141 unique cracks and key-
gens within our analysis setup. For each anti-copy protection
tool, we reverted the according machine to the base snap-
shot. We started our packet capture tool (Wireshark) on the
host machine and then copied the anti-copy protection tool
to the virtual machine. Before executing the crack or key-
gen, we scanned the (cid:12)les with the AV software. If infections
could be detected, we noted them and temporarily disabled
the AV software. After that, we executed the crack or key-
gen and precisely followed the accompanying instructions.
In some cases, for the anti-copy protection tool to run, it
was necessary to resolve software dependencies, make reg-
istry modi(cid:12)cations or perform multiple steps both in the
software product and the crack/keygen. After installation,
we checked if the tool actually unlocked the software product
and whether the software product was still in a usable and
stable running state. This was followed by a full system AV
scan, so that we could identify possible malware infections.
We distinguished between (cid:12)le- and system infections: If the
AV software discovered infections in the anti-copy protec-
tion tool, but running the tool did not infect any other (cid:12)les
on the system, we denote this a (cid:12)le infection. If other (cid:12)les
on the system were infected, we denote this a system infec-
tion. In order to capture slow network tra(cid:14)c as well, we let
each machine run for at least 10 minutes. After that, we
shut down the virtual machine, stopped the packet capture
tool and analyzed the captured network tra(cid:14)c if applica-
ble. This allowed us to manually collect AV scan results,
installation results (i.e. whether the tool could unlock the
software product) and packet captures for each crack and
keygen. Overall, the manual analysis took between 15 and
20 minutes per single crack or keygen executable. To the
best of our knowledge, we are the (cid:12)rst who conducted a
manual analysis of cracks and keygens to that extent.
4.3.2 Automated Analysis
We analyzed all available unique samples (including the
manually acquired ones) in our automated analysis environ-
ment (Fig. 2). In order to conduct both static and dynamic
analysis, we utilized the Virustotal [14] service and the Anu-
bis [16] environment.
Virustotal [14] is a publicly available service that allows
concurrent scanning of submitted (cid:12)les with a large number
of AntiVirus scanners. At the time of writing, the service
accepted (cid:12)les up to 32MB in size and conducted scans with
43 di(cid:11)erent AntiVirus scanners. Each AntiVirus scanner re-
ports that either no infection or an infection with a given
malware could be identi(cid:12)ed in the submitted (cid:12)le. We wrote
a set of scripts that retrieved existing scan reports for (cid:12)les
based on their MD5 sum. This way, we could retrieve exist-
ing reports and determine the number of cracks or keygens
that were already known to Virustotal. In the next step, we
submitted all (cid:12)les to Virustotal and rescanned them in case
there were existing analysis reports. The di(cid:11)erence between
older existing reports and fresh reports is that for fresh re-
ports, the latest virus de(cid:12)nitions information is used. Thus
it is possible, that old reports did not indicate malware in-
fections since there were no AV signatures for the speci(cid:12)c
malware at that time, whereas, due to the more recent AV
signatures, the fresh scan report would be able to identify
these infections. In order to determine whether a (cid:12)le was
infected, we calculated the overall percentage of the virus
scanners that indicated malware infections. This allowed us
to limit the e(cid:11)ect of false positives for individual AV scan-
ners. Out of the 3,561 unique crack and keygen executables,
we were able to retrieve 2,281 (64:06%) existing reports and
3,471 (97:47%) fresh scan reports. For 90 (cid:12)les (2:53%) we
did not receive a report from Virustotal, either because the
submitted (cid:12)le was too large (31 (cid:12)les, 0:87%) or due to in-
ternal Virustotal issues (59 (cid:12)les, 1:66%).
Anubis is a dynamic analysis environment, originally based
on TTAnalyze [16], that performs virtual machine introspec-
tion (VMI). It allows to upload executables with up to 8MB
in size and considers a wide range of possible threats, includ-
ing malicious registry and (cid:12)le modi(cid:12)cation, process creation,
or network activities. Based on this information, Anubis cre-
ates a detailed analysis report including a severity score in
the range f0; : : : ; 10g. Similar to the Virustotal results, the
severity score corresponds to the maliciousness of the exe-
cuted sample, whereas a value of 0 indicates no malicious
behavior at all and a value of 10 suggests, that the sample
is malware. For our experiments, we submitted all cracks
and keygen samples with a size of up to 8MB to Anubis.
Due to this limitation, we could not analyze 340 (9:55%)
out of the 3,561 unique crack and keygen executables. The
remaining 3,221 samples resulted in 3,145 (97:64%) valid
Anubis reports. Anubis identi(cid:12)ed 45 samples (1:40%) as no
valid Windows PE executables and hence did not generate
a report. For 31 samples (0:96%), we received incomplete
Anubis reports without severity score.
5. RESULTS AND DISCUSSION
The following Sections show the results we obtained through
manual and automated approaches.
5.1 Manually Collected Samples
By following the methodology described in Section 4.1, we
were able to download 141 unique cracks and keygens. How-
ever, since some of the (cid:12)les appeared for multiple products,
it was necessary to test these duplicates for each product in
its speci(cid:12)c virtual environment. Although the overall num-
ber of unique manual downloads is 141, we had to test 157
cracks and keygens.
Table 5 shows the overall results of our manual analysis
based on the dataset from Table 9 (see Appendix). For each
product, the table summarizes the results for the unique
cracks and keygens, whereas the (cid:12)rst 5 products are ap-
plications and the latter ones are games. Our downloads
814Product
Photoshop
Nero 10
O(cid:14)ce 2010
Norton 360
Winrar
Brink
Crysis 2
Fable 3
Portal 2
The Sims 3
Total App.
Total Game
Total
real C/K wrong C/K (cid:12)le inf.
66.67
68.42
42.86
31.25
22.73
7.69
23.53
55.56
21.43
44.44
45.35
29.58
38.22
66.67
78.95
64.29
43.75
68.18
69.23
88.24
66.67
42.86
88.89
65.12
73.24
68.79
60.00
15.79
42.86
25.00
18.18
30.77
35.29
0.00
0.00
27.78
30.23
21.13
26.11
13.33
52.63
35.71
12.50
4.55
7.69
11.76
66.67
35.71
11.11
23.26
22.54
22.93
sys. inf. VT avg. Anubis avg. NW traf.
20.00
42.11