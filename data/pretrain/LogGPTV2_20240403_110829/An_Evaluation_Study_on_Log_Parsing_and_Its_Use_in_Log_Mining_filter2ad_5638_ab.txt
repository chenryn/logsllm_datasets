checked by developers, they compare the log event sequences
generated in pseudo-cloud and large-scale cloud. Only the
different log event sequences are reported to the developers,
which greatly alleviates their workload. In this task, a bad log
parser may produce wrong log event sequences. This could
largely degrade the reduction effect because their method is
based on the comparison of log event sequences.
System model construction: Computer systems are difﬁcult
to debug and understand. To help developers gain insight into
system behaviors, Beschastnikh et al. [5] propose a tool called
Synoptic to build an accurate system model based on logs.
Synoptic requires parsed log events as input and generates
a ﬁnite state machine as the output system model. If an
unsuitable log parser is used, both initial model building step
and model reﬁnement step will be affected. These may result
in extra branches or even totally different layout of the model.
B. System Anomaly Detection
To better study the impact of log parsing approaches on
the subsequent log mining task, we reproduce the anomaly
detection method proposed in [2] on its original HDFS logs
while using different
log parsing approaches discussed in
Section. II-B. The anomaly detection method contains three
steps:
log parsing, event matrix generation, and anomaly
detection.
1) Log Parsing: The input of the anomaly detection task is
a text ﬁle, each line of which is a raw log message recording an
event occurring on a block in HDFS. In this step, log parsing
method is adopted to ﬁgure out two things. One is all the
event types appearing in the input ﬁle. The other is the events
associated with each block, which distinguished by block ID.
These two are exactly in the two output ﬁles of our log parser
modules. We emphasize that the parsing output is not speciﬁc
to anomaly detection, but also suitable for other log mining
tasks.
2) Matrix Generation: Parsed results are used to generate
an event count matrix Y , which will be fed into the anomaly
detection model. In the event count matrix, each row represents
a block, while each column indicates one event type. The
value in cell Yi,j records how many times event j occurs
on block i. We could generate Y with one pass through the
parsed results. Instead of directly detecting anomaly on Y , TF-
IDF [27], which is a well-established heuristic in information
retrieval, is adopted to preprocess this matrix. Intuitively, TF-
IDF is to give lower weights to common event types, which
are less likely to contribute to the anomaly detection process.
3) Anomaly Detection: In this case, anomaly detection is
to ﬁnd out suspicious blocks that may indicate problems
(e.g., HDFS namenode not updated after deleting a block).
The model used is Principle Component Analysis (PCA)
[2], which is a statistical model
that captures patterns in
high-dimensional data by selecting representative coordinates
(principle components). PCA is used in this problem because
principle components can represent most frequent patterns of
events associated with blocks, which is called normal space
Sd. Speciﬁcally, the ﬁrst k principle components are selected
to form Sd, while the remaining n − k dimensions form Sa
(anomaly space), where n is the number of columns (total
number of event type) of the matrix. In this task, each row in
the event count matrix is a vector y associated with a block.
The intuition of anomaly is the vector whose end point is far
away from normal space. The “distance” could be formalized
by squared prediction error SP E ≡ ||ya||2, where ya is the
projection of y on Sa. ya is calculated by ya = (I − P P T )y,
where P = [v1,v2,..., vk]. A block is marked as anomaly if its
corresponding y satisﬁes:
SP E = ||ya||2 > Qα,
where Qα is a threshold providing (1− α) conﬁdence level.
For Qα, we choose α = 0.001 as in the original paper [2].
IV. EVALUATION STUDY
This section presents our study methodology and reports on
the detailed results for the proposed research questions.
A. Study Methodology
Log Datasets: To facilitate systematic evaluations on the
state-of-the-art log parsing methods, we have used ﬁve large
log datasets ranging from supercomputers (BGL and HPC)
to distributed systems (HDFS and Zookeeper) to standalone
software (Proxiﬁer), with a total of 16,441,570 lines of log
messages. Table I provides a basic summarization of these
datasets. Logs are scarce data for research, because companies
are often reluctant to release their production logs due to
conﬁdentiality issue. We obtained three log datasets, with
the generous support from their authors. Speciﬁcally, BGL
is an open dataset of logs collected from a BlueGene/L
supercomputer system at Lawrence Livermore National Labs
(LLNL), with 131,072 processors and 32,768GB memory [28].
HPC is also an open dataset with logs collected from a high
performance cluster at Los Alamos National Laboratory, which
has 49 nodes with 6,152 cores and 128GB memory per node
[29]. HDFS logs are collected in [2] by using a 203-node
cluster on Amazon EC2 platform. To enrich the log data for
evaluation purpose, we further collected two datasets: one from
a desktop software Proxiﬁer, and the other from a Zookeeper
installation on a 32-node cluster in our lab.
In particular, the HDFS logs from [2] have well-established
anomaly labels, each of which indicates whether or not a
request for a data block operation is an anomaly. The labels are
made based on domain knowledge, which are suitable for our
evaluations on anomaly detection with different log parsers.
Speciﬁcally, the dataset with over 11 million log messages
records 575,061 operation requests with a total of 29 event
types. Among all the 575,061 requests, 16,838 are marked as
anomalies, which we use as ground truth in our evaluation.
TABLE I: Summary of Our System Log Datasets
Description
BlueGene/L
Supercomputer
High Performance
Cluster
(Los Alamos)
Proxy Client
System
BGL
HPC
Proxiﬁer
HDFS
Zookeeper
#Logs
4,747,963
433,490
10,108
74,380
Length
10∼102
6∼104
10∼27
8∼29
8∼27
#Events
376
105
8
29
80
Hadoop File System 11,175,629
Distributed
System Coordinator
TABLE II: Parsing Accuracy of Log Parsing Methods
(Raw/Preprocessed)
BGL
0.61/0.94
SLCT
IPLoM 0.99/0.99
0.67/0.70
LKE
0.26/0.98
LogSig
HPC
0.81/0.86
0.64/0.64
0.17/0.17
0.77/0.87
HDFS
0.86/0.93
0.99/1.00
0.57/0.96
0.91/0.93
Zookeeer
0.92/0.92
0.94/0.90
0.78/0.82
0.96/0.99
Proxiﬁer
0.89/-
0.90/-
0.81/-
0.84/-
Experimental Setup: All our experiments were run on a
Linux server with Intel Xeon E5-2670v2 CPU and 128GB
DDR3 1600 RAM, running 64-bit Ubuntu 14.04.2 with Linux
kernel 3.16.0. We use F-measure [30], [31], a commonly-
used evaluation metric for clustering algorithms, to evaluate
the parsing accuracy of log parsing methods. To calculate F-
measure, we manually obtain the ground truths for all logs of
these dataset. It is possible because we iteratively ﬁlter out logs
with conﬁrmed event using regular expression. Experiments
about LKE and LogSig are run 10 times to avoid bias of
clustering algorithms, while others are run once because they
are deterministic. We note here that only the parts of free-text
log message contents are used in evaluating the log parsing
methods.
B. RQ1: Accuracy of Log Parsing Methods
To study the accuracy of different log parsing methods, we
use them to parse our collected real logs. As with the existing
work [15], we randomly sample 2k log messages from each
dataset in our evaluation, because the running time of LKE
and LogSig is too long on large log datasets (e.g., LogSig
requies 1 day to parse entire BGL data). The average results
of 10 runs are reported in Table II. We can observe that the
overall accuracy of these log parsing methods is high (larger
than 0.8 in most cases). Meanwhile, the overall accuracy on
HDFS, Zookeeper and Proxiﬁer datasets is higher than that
obtained on the others. We found that this is mainly because
BGL and HPC logs involve much more event types, each of
which has a longer length than other datasets.
Especially, we found that LKE takes an aggressive clus-
tering strategy, which groups two clusters if any two log
messages between them has a distance smaller than a speciﬁed
threshold. This is why LKE has an accuracy drop on HPC
dataset, in which it clusters almost all the log messages into
one single cluster in the ﬁrst step. BGL contains a lot of
log messages whose event is “generating core.*”, such as
“generating core.2275” and “generating core.852”. Intuitively,
the similarity of these two log messages are 50%, because half
of the words are different. LogSig tends to separate these log
messages into different clusters, which causes its low accuracy
on BGL. Particularly, IPLoM leverages some heuristic rules
developed on the characteristics of log messages, while other
log parsing methods rely on well-studied data mining models.
However, we found that IPLoM obtains the superior overall
accuracy (0.88) against other log parsing methods. This further
implies the particular importance of exploiting the unique
characteristics of log data in log parsing, which would shed
light on future design and improvement of a log parser.
Finding 1: Current log parsing methods achieve high
overall parsing accuracy (F-measure).
Instead of running log parsing methods directly on raw log
messages, developers usually preprocess log data with domain
knowledge. In this experiment, we study the impact of prepro-
cessing on parsing accuracy. Speciﬁcally, we remove obvious
numerical parameters in log messages (i.e., IP addresses in
HPC&Zookeeper&HDFS, core IDs in BGL, and block IDs
in HDFS). Proxiﬁer does not contain words that could be
preprocessed based on domain knowledge. Preprocessing is
mentioned in LKE and LogSig; however, its importance has
not been studied.
In Table II, the numbers on the left/right side represent the
accuracy of log parsing methods on raw/preprocessed log data.
In most cases, accuracy of parsing is improved. Preprocessing
greatly increases the accuracy of SLCT/LKE/LogSig on one
dataset (in bold). However, preprocessing could not improve
the accuracy of IPLoM. It even slightly reduces IPLoM’s ac-
curacy on Zookeeper. This is mainly because IPLoM considers
preprocessing internally in its four-step process. Unnecessary
preprocessing may cause wrong splitting.
Finding 2: Simple log preprocessing using domain
knowledge (e.g. removal of IP address) can further
improve log parsing accuracy.
C. RQ2: Efﬁciency of Log Parsing Methods
In Fig. 2, we evaluate the running time of the log parsing
methods on all datasets by varying the number of raw log
messages. Notice that as the number of raw log messages
increases, the number of events becomes larger as well (e.g.,
60 events in BGL400 while 206 events in BGL40k). SLCT
and IPLoM, which are based on heuristic rules, scale linearly
with the number of log messages (note that Fig. 2 is in
logarithmic scale). Both of them could parse 10 million HDFS
log messages within ﬁve minutes. For the other two clustering-
based parsing methods, LogSig also scales linearly with the
number of log messages. However,
its running time also
increases linearly with the number of events, which leads to
relatively longer parsing time (e.g, 2+ hours for 10m HDFS
log messages). The time complexity of LKE is O(n2), which
makes it unable to handle large-scale log data, such as BGL4m
and HDFS10m. Some running time of LKE is not plotted
because LKE could not parse some scales in a reasonable
(a) BGL
(b) HPC
(c) HDFS
(d) Zookeeper
(e) Proxiﬁer
Fig. 2: Running Time of Log Parsing Methods on Datasets in Different Size
(a) BGL
(b) HPC
Fig. 3: Parsing Accuracy on Datasets in Different Size
(c) HDFS
(d) Zookeeper
(e) Proxiﬁer
time (may cause days or even weeks). To reduce the running
time of clustering-based log parsing method, parallelization is
a promising direction.
Finding 3: Clustering-based log parsing methods could
not scale well on large log data, which implies the
demand for parallelization.
The accuracy of log parser is affected by parameters. For
example, the number of clusters of LogSig decides the number
of events, which should be set beforehand. For large-scale
log data, it is difﬁcult to select the most suitable parameters
by trying different values, because each run will cause a lot
of time. A normal solution is to tune the parameters in a
sample dataset and directly apply them on large-scale data. To
evaluate the feasibility of this approach, we tune parameters
for log parsing methods on 2k sample log messages, which are
used in our parsing accuracy experiment. In Fig. 3, we vary
the size of the dataset and evaluate the accuracy of the log
parsing method using these parameters. The results show that
the IPLoM performs consistently in most cases. SLCT is also
consistent in most cases except HPC. The accuracy of LKE
is volatile because of the weakness of its clustering algorithm
discussed in Section IV-B. LogSig performs consistently on
datasets with limited types of events, but its accuracy varies
a lot on datasets with many events (i.e., BGL and HPC).
Thus, for LKE and LogSig, directly using parameters tuned on
sample dataset is not practical, which makes parameter tuning
on large-scale logs time-consuming.
Finding 4: Parameter tuning for clustering-based log
parsing methods is a time-consuming task, especially on
large log datasets.