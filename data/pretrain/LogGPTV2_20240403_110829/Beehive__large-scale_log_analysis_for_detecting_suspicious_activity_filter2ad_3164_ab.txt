tem, we ﬁrst compute a set of time diﬀerence values Δi =
tsiem,i−tdevice,i (rounded oﬀ to the nearest 30 minutes) from
each log entry i generated over a large time period (e.g., one
month). Next, we determine the timestamp correction value
Δcorrection for the device by setting it to the value Δi that
accounts for the majority of diﬀerences. Applying this cor-
rection value to each device timestamp gives us a normalized
timestamp value, tnormalized,i = tdevice,i + Δcorrection. In
our case, as EMC’s SIEM system is conﬁgured to use UTC
timestamps, tnormalized,i corresponds to UTC time. We ap-
ply this normalization technique to each device on the net-
work that produces log data, normalizing all log timestamps
to UTC.
IP address-to-Host Mapping. In an enterprise setting,
end hosts are usually assigned IP addresses dynamically
for a short time period when they connect to the network,
through the Dynamic Host Conﬁguration Protocol (DHCP).
This makes it diﬃcult to associate the IP addresses reported
in logs with unique host machines during analysis, because
that same IP address could be assigned to diﬀerent hosts
after the event has been logged.
To address this issue, Beehive analyzes the DHCP server
logs collected in the SIEM system and constructs a database
of IP-to-host mappings (i.e., bindings) over time. Each bind-
ing is represented as a tuple {IP address, hostname, MAC
address, start-time, end-time} mapping an IP address to a
host in a speciﬁc time interval. This algorithm is run daily
to update existing bindings as new DHCP logs become avail-
able. Given the resulting database of bindings, Beehive can
identify the host corresponding to a given IP address with
reference to a normalized timestamp.
Static IP Address Detection. An IP-to-host lookup on
the bindings database we construct may fail because an IP
address is assigned statically to a speciﬁc host, not dynam-
ically leased. Maintaining a list of static IP address assign-
ments, however, is diﬃcult in a large enterprise network,
and administrator-created lists are often non-existant or out-
dated. Thus Beehive also adopts the following method for
automatically identifying hosts with static IP addresses.
In the bootstrap step, we ﬁrst retrieve all IP addresses
found in all collected logs to create a pool of IP addresses
active in the enterprise, denoted by set A. Next, we retrieve
IP addresses from logs that we know must only contain hosts
with dynamic IP addresses, such as DHCP and VPN logs,
to create a pool of known dynamic IP addresses, denoted by
set D. We then compute the set diﬀerence S = A− D which
contains potentially static IP addresses, perform a reverse
DNS lookup for every address in S, and save the results
to complete the bootstrap phase. Periodically (e.g., once a
day), as new logs become available, we repeat this procedure
to harvest new IP addresses and update the sets A, D and S
accordingly. However, unlike before, with each iteration we
resolve IP addresses in S to their host names and compare
the names to the previously stored values. If the host names
changed between two iterations, we conclude that the given
IP address is not statically assigned, and we remove it from
the set S. In this way, we reﬁne the pool of potentially static
IP addresses with each iteration. If Beehive fails to ﬁnd a
corresponding binding for an IP-to-host lookup, but instead
ﬁnds the given address in S, we treat that IP address as a
static host and use the host name found in S.
Dedicated Hosts. Beehive focuses on monitoring the be-
havior of dedicated hosts, which are machines utilized by a
single user. Since a list of dedicated hosts is diﬃcult to main-
tain in large enterprises due to constantly changing network
conﬁgurations, we infer the list of dedicated hosts from the
data available in the SIEM system.
We make use of authentication logs generated by Mi-
crosoft Windows domain controllers for this purpose. For
each user in the enterprise, a history is kept of hosts onto
which the user had authenticated (i.e., logged on), the num-
ber of authentications, and the authentication timestamps.
This information is collected over the course of three months
to build an accurate history of user activity. At the end of
the collection period, we consider a host as “dedicated” if a
single user is responsible for the large majority (e.g., 95%)
of the authentication events on that host. Through this pro-
cess, we have identiﬁed over 78,000 dedicated hosts at EMC.
3.2 Feature Extraction
We extracted features from the logs to characterize out-
bound communications from the enterprise. Our feature se-
lection is guided by observation of known malware behav-
iors and policy violations within EMC, as well as proper-
ties of the environment in which Beehive operates, i.e., the
presence of perimeter defenses in the form of strict ﬁrewall
policies, the business orientation of (most) users’ activities,
and the (relatively) homogeneous software conﬁgurations of
enterprise-managed hosts.
For each dedicated host in the enterprise, we generate
daily a feature vector that includes the 15 features listed in
Table 2. The features can be grouped into four categories:
features based on new and unpopular destinations contacted
by the host, features related to the host’s software conﬁgu-
ration, features related to the company policy, and features
based on traﬃc volume and timing. We describe each of
them in detail below.
Feature Type # Description
Destination-
Based
1 New destinations
2 New dests. w/o whitelisted referer
3 Unpopular raw IP destinations
4
Fraction of unpopular raw IP dests.
Host-Based
5 New user-agent strings
Policy-
Based
Traﬃc-
Based
Blocked domains
6
7
Blocked connections
8 Challenged domains
9 Challenged connections
10 Consented domains
11 Consented connections
12 Connection spikes
13 Domain spikes
14 Connections bursts
15 Domain bursts
Table 2: Beehive features.
3.2.1 Destination-Based Features
We are interested in identifying hosts that communicate
with new, or obscure, external destinations that are never
(or rarely) contacted from within EMC. Assuming popular
websites are better administered and less likely to be com-
promised, connections to uncommon destinations may be
indicative of suspicious behavior (e.g., communication with
command-and-control servers).
New Destinations. Our ﬁrst destination-based feature is
the number of new external destinations contacted by each
host per day. We build a history of external destinations
contacted by internal hosts over time. After an initial boot-
strapping period of one month, we consider a destination to
be new on a particular day if it has never been contacted by
hosts in the enterprise within the observation period (and
as such is not part of the history). On a daily basis we up-
date the history to include new destinations contacted in the
previous day.
We encountered a number of challenges when analyzing
web proxy logs to build the history of external destinations.
In our initial, na¨ıve, implementation, we process every proxy
log, resolve all destinations that are IP addresses, and in-
clude all observed new destinations in the history. It turns
out that this na¨ıve approach is not scalable both in terms of
running time and history size.
First, the na¨ıve implementation takes 15 hours to process
a day’s worth of web proxy logs (the equivalent of 300 million
logs, or 600 GB). Second, to our surprise, the number of new
destinations does not decrease over time, even as the size of
the history grows to 4.3 million unique destinations over the
course of a month. As shown in Figure 1, between 30% to
40% (about 145,000) of all unique destinations each day are
new. In the na¨ıve approach, all of these new destinations
are added to the history daily, and as such the size of the
history grows indeﬁnitely over time.
Further inspection of the web proxy logs showed that the
majority of the new destinations are content delivery net-
202
works (CDNs), cloud services (which frequently use random
strings as subdomains), or IP addresses belonging to popu-
lar services (Google, Facebook, Twitter). To make Beehive
scalable, we employ a number of data reduction techniques,
including ﬁltering, custom whitelisting and domain “fold-
ing.”
Unique destinations per day
Unique new destinations per day
Destinations in history (naive approach)
 5e+06
 4e+06
 3e+06
 2e+06
 1e+06
s
n
o
i
t
a
n
i
t
s
e
d
e
u
q
i
n
u
f
o
r
e
b
m
u
N
 0
 0
 5
 10
 20
 25
 30
 15
Day
Figure 1: Destinations contacted by internal hosts,
na¨ıve approach.
We ﬁrst ﬁlter “popular” destinations by creating a cus-
tom whitelist, where “popularity” is deﬁned over hosts in
the enterprise. The whitelist includes external destinations
(both domains and IP subnets) whose number of interact-
ing internal hosts over time (i.e., a training period of one
week) exceeds a threshold. Figure 2 shows the amount of
web traﬃc ﬁltered using a whitelist constructed from the
ﬁrst week’s worth of data from April 2013. A threshold of
100 hosts achieves a reduction from 300 million logs a day to
80 million—a 74% reduction in the number of logs Beehive
needs to process.
s
g
o
l
f
o
r
e
b
m
u
N
 4e+08
 3.5e+08
 3e+08
 2.5e+08
 2e+08
 1.5e+08
 1e+08
 5e+07
 0
All logs
Doms < 1000 hosts
Doms < 500 hosts
Doms < 200 hosts
Doms < 100 hosts
Mon Tue Wed Thu
Fri
Sat
Sun
Day
Figure 2: Data reduction through custom whitelist-
ing.
In addition to custom whitelisting, we “fold” destinations
to the second-level domain so as to ﬁlter services employing
random strings as subdomains. We also ignore connections
retrieving “favicon” — likely due to bookmarks. Finally, we
choose not to resolve raw IPs (since most legitimate sites are
referred to by their domain name), and always consider raw
IPs that are not on the whitelist as “new”.
These optimizations reduce daily processing time from 15
hours to about ﬁve hours. The number of new (folded) des-
tinations added to the history is reduced to an average of
28,000 per day. After a period of four months (from January
13th to May 13th, 2013), our history of external destinations
has 2.7 million folded domains, whereas the history built
na¨ıvely already had 4.3 million domains after one month.
New destinations without whitelisted referer. Our
second destination-based feature is an extension of the ﬁrst
one, but counts the number of “new” destinations contacted
by a host without a whitelisted HTTP referer. Users are
most commonly directed to new sites by search engines, news
sites, or advertisements. In these cases, we expect the HTTP
referer to be one listed in our custom whitelist. Host that
visit new sites without being referred by a reputable source
(or with no referer at all) are considered more suspicious.
Unpopular IP destinations. We are also interested in
unpopular external destinations that are raw IP addresses.
We ﬁrst count the number of destinations contacted by a
host that are both unpopular (not on the custom whitelist
described above) and are IP addresses. Connections to un-
popular IPs can indicate suspicious activity, as legitimate
services can usually be reached by their domain names. Sec-
ond, we include the fraction of unpopular destinations con-
tacted by a host that day that are IP addresses. While occa-
sional communication with IP destinations is normal (e.g.,
to IP ranges owned by CDNs), such behavior is suspicious
when frequent.
3.2.2 Host-Based Features
Hosts in an enterprise are signiﬁcantly more homogeneous
in their software conﬁgurations than, for example, those in
academic networks. Hence we are interested in cases where
hosts install new (and potentially unauthorized) software.
Lacking visibility onto the host machine, and having ac-
cess only to logs collected from network devices, we infer
the software conﬁgurations on a host from the user-agent
(UA) strings included in HTTP request headers. A user-
agent string includes the name of the application making
the request, its version, capabilities, and the operating en-
vironment. Our host-based feature is hence the number of
“new” UA strings from the host.
We build a history of UA strings per host over a month-
long period, during which every UA string observed from the
host is stored. Afterwards, a UA string is considered “new” if
it is suﬃciently distinct from all UA strings in that host’s his-
tory. Edit distance (also called Levenshtein distance) is used
to compare UA strings, measuring the number of character
insertions, deletions, and substitutions required to change
one string into another. This allows us to accommodate
“new” UA strings that result from software updates, where
only a small substring (e.g., the version number) changes.
3.2.3 Policy-Based Features
Also unique to enterprise environments is the enforcement
of network policies on outbound connections. As described
in Section 2.2, a connection to an external destination can
be blocked if it has a dubious reputation or is categorized
as prohibited for employees. Blocked domains (and connec-
203
tions) are thus a coarse indicator of host misbehavior.
Upon visiting an unknown destination, i.e., one that has
not yet been categorized or rated, the user must explicitly
agree to adhere to the company’s policies before being al-
lowed to proceed. We refer to the domains (and connections)
that require this acknowledgment as challenged, and those
to which the user has agreed as consented.
Our policy-based features include all three types of com-
munications described above. For a host, we count the num-
ber of domains (and connections) contacted by the host that
are blocked, challenged, or consented.
3.2.4 Trafﬁc-Based Features
Sudden spikes in a host’s traﬃc volume can be caused
by malware (e.g., scanning, or bot activities in response to
botmaster commands) or automated processes. Our traﬃc-
based features attempt to capture these interesting activities