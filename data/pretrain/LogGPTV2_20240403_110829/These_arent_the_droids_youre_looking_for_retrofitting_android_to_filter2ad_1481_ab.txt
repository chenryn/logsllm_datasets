*.mydas.mobi
*.adwhirl.com
*.mobclix.com
*.google-analytics.com
tapad.jumptap.com
droidvertising.appspot.com
*.mojiva.com
ad.qwapi.com
*.greystripe.com
*.inmobi.com
Any
57
36
27
24
23
21
17
17
6
5
4
2
2
1
IMEI Loc
11
0
15
0
0
0
6
0
0
0
0
0
0
1
0
0
2
0
0
0
10
0
0
0
0
0
2
0
Table 3: The number of applications (from our 110
application sample) that sent any communication to
the A&A server, number that sent the unique device
ID (IMEI), and number that sent the user’s location.
one of the resource types identiﬁed in Table 1, enabling the
potential for disclosure of sensitive information to these third
party servers.
2.5 Where sensitive information goes
Not all applications that request permission to access sen-
sitive information will exﬁltrate it. We ran an experiment
to identify the prevalence with which applications transmit
each type of sensitive information oﬀ the user’s device and
where they send it to. Performing this preliminary study re-
quired us to enhance TaintDroid, as it had previously only
tracked ﬁve of the 12 data types examined in our study, and
it did not track traﬃc sent through SSL. With our modiﬁca-
tions, TaintDroid is able to detect sensitive data even when
it has been obfuscated, encrypted using AES, or transmitted
via SSL, although it is still limited in that it cannot track
information leaked through control ﬂow operations; we dis-
cuss this issue of leaks via implicit ﬂows in Section 3.3. We
also added instrumentation to record the identity of commu-
nicating parties and the traﬃc going to, and coming from,
these parties.
To perform this analysis, we manually executed each of
the applications in our 110-application subsample for about
641Resource
phone_state
IMEI
Phone#
location
contacts
camera
account
logs
microphone
SMS/MMS messages
history&bookmarks
calendar
subscribed_feeds
83
83
73
29
12
11
10
10
10
10
8
1
31
5
45
7
1
4
0
1
0
0
0
0
Demanded Anywhere
Sent to
A&A
37% 14 17%
6% 0 0%
62% 30 41%
24% 0 0%
8% 0 0%
36% 0 0%
0% 0 0%
10% 0 0%
0% 0 0%
0% 0 0%
0% 0 0%
0% 0 0%
Table 4: The prevalence of permissions demanded
by applications in the sample used for our initial in-
formation ﬂow experiments. Note that the sum of
the application counts is greater than 110 as many
applications require access to multiple data types.
For each data type, we tracked applications that de-
manded access to that data type and measured the
fraction that transmitted messages tainted by that
data type.
ﬁve minutes, exercising the application’s main features and
any features we thought might require the use or exﬁltration
of sensitive data (the same methodology is used in [22, 23]).
We augmented the list of A&A domain names previously
obtained through static analysis by observing traﬃc from
these 110 applications and manually inspecting the sites they
contacted to verify which third-parties were A&A servers.
The resulting list of domain names of A&A servers can be
found in Table 3.
For each sensitive resource, Table 4 shows the number
of applications in our 110-application subsample that de-
manded access to it, and the fraction that we observed trans-
mitting messages tainted by data from this resource out to
the Internet. The only data types we see transmitted are
device ID (IMEI), phone number, location, contacts, cam-
era, account, and microphone. Some applications may send
more information than we observed as we could not guaran-
tee that all code paths were executed. In addition, the 110-
application subsample contains a disproportionate number
of permission-hungry applications as described in Section 2.1
and therefore this bias should be reﬂected when weighing the
results reported in this section against the general popula-
tion of Android applications. Table 3 shows the breakdown
of A&A destinations that collected tainted data from ap-
plications. We observed that location was sent to AdMob,
Flurry, Mobclix, and Inmobi, and device ID was sent to
Flurry, Mobclix, and Greystripe.
Phone number. Five applications transmitted phone num-
bers. Two applications required users to register a phone
number, so they ﬁlled in the device’s phone number by de-
fault when the user completed the registration form (but the
user could then modify the phone number if desired). The
third application used the phone number to create a custom
unique device identiﬁer, so the phone number was not dis-
closed directly in the payload. However, two applications–
Dilbert comic viewer and Mabilo ringtones downloader–sent
the device’s phone number with no discernable legitimate
purpose!
Contacts. Seven applications transmitted contacts. Two
did so to perform contact-speciﬁc searches, and three sent
contacts as requested by the user. One, a reverse phone
number lookup application (Mr. Number), sent contact en-
tries to its own servers; it asks the user to opt in, but only
after it has already sent the data to its servers. An instant
messaging application (KakaoTalk) sent the phone numbers
collected from the user’s entire address book to its servers
to automatically add other users of the application. The
transmission took place without any notice and this feature
is turned on by default. Additionally, six of the seven appli-
cations sent the device ID along with the contacts, making it
easy for applications to link contacts with other information
that is commonly collected as described below.
Device ID. 31 applications transmitted the device ID
(IMEI). As reported by previous studies, the use of the de-
vice ID by applications is prevalent. 11 applications em-
ployed SSL secure connections when they transmitted the
device ID to application servers. We ﬁnd that these en-
crypted transmissions of the device ID sometimes accom-
pany other sensitive data such as contacts and phone num-
ber. We ﬁnd seven game applications that send the device
ID over SSL along with a score to store high scores using a
third-party company.
45 applications transmitted location data.
Location.
Third-party servers are the most common destinations for lo-
cation data; 30 applications shared location data with A&A
servers. All but two of these 30 shared location data with
A&A servers exclusively. Half (15) employ the Flurry an-
alytics package, which uses a binary (non-human readable)
data format when sending out location data to the Flurry
server. Prior investigations that observed network traﬃc
alone would not have detected the transmission of this in-
formation.
Camera & Microphone data. We observed that one
application sent a photo and another application sent a voice
memo. Both cases are triggered by explicit user requests.
Account. The account resource is used to store proﬁle and
authentication information for online accounts that the user
has access to. Four applications transmitted data tainted
by the account resource; all uses appear legitimate. One
security application used account data to send email to the
user’s Gmail account. One multimedia application used ac-
count data to allow the user to register her Facebook account
for creating personal proﬁles. One music sharing application
used account data to authenticate the user with its server.
One application used account data to access the Android
Market for providing enhanced services.
2.6 Informing privacy controls
Our preliminary analysis can guide the selection of pri-
vacy control mechanisms for protecting sensitive data. One
simple approach would be to block all access to the Internet
by the application. While this obviously would impede user-
desired functionality in some cases, we wondered if it might
be suﬃcient in others. Having intercepted and observed all
Internet communications to and from these applications, we
show the fraction of each application’s Internet traﬃc that is
used for advertising and analytics (A&A) in Figure 1. Of the
97 applications in our 110 application sample that accessed
A&A servers, 23 (24%) communicated exclusively with A&A
642randomly generated for the device. The salt ensures that an
application that is granted access to the device ID cannot be
linked to an application that is granted access to the shadow
ID. The result of the hash is a string containing 15 decimal
digits—the proper format for a GSM IMEI number.
The Android phone state permission also grants access
the software version number (IMEI/SV), SIM serial number,
voice mail number, and subscriber ID (IMSI). We did not
observe any applications use these data, and thus did not
test any shadowing strategies for them.
Implementation
The Android architecture sandboxes each running applica-
tion within a Dalvik virtual machine. Virtual machines are
isolated from each other by running each in its own process.
The Android operating system includes the Android core
libraries, which are contained in each VM, as well as the
Android framework, a set of centralized services and man-
agers that reside outside of the VMs. Applications access
the core libraries and framework through the Android API.
To impose privacy controls on unmodiﬁed applications,
AppFence modiﬁes the Android core libraries and Android
framework. Figure 2 shows the components of the Android
architecture that we modiﬁed for shadowing. The modiﬁed
libraries and framework that guard access to sensitive data
reside outside of the application sandbox imposed by the
Dalvik virtual machine. We rely on the sandbox to prevent
the application from tampering with these components. As
native libraries are not sandboxed, AppFence prevents ap-
plications from loading their own native libraries (Android’s
core native libraries are still loaded on demand as applica-
tions require them). At the time of testing, the use of native
libraries was exceptionally rare; not one application that we
examined with AppFence (including all applications in our
110-application sample) required its own native libraries.
For simple resources such as the device ID, phone num-
ber, and location, we return shadow values directly from
the managers in the Android framework code. More com-
plex resources, such as the user’s calendar and contact list,
are accessed through Android’s content provider frame-
work [11]. Applications identify the resource they wish to
access via a URI. For example, the calendar may be queried
with the string content://calendar. For these content
provider resources, we replace the cursor that would nor-
mally be returned by the content manager with a shadow
database cursor. For our experiments we return an empty
database cursor, though one could instead create a shadow
database and return a cursor to it.
3.2 Exﬁltration blocking
To block exﬁltration of data, we intercept calls to the net-
work stack to (1) associate domain names with open sockets
and (2) detect when tainted data is written to a socket.
When an output buﬀer contains tainted data, we drop the
buﬀer and choose one of two actions: we may drop the of-
fending message covertly, misleading the application by in-
dicating that the buﬀer has been sent, or overtly, emulating
the OS behavior an application would encounter if the buﬀer
were dropped as a result of the device entering airplane mode
(all wireless connections disabled).
Figure 1: The fraction of network traﬃc (bytes in-
bound + bytes outbound) sent to A&A servers.
servers during our observations. While these could presum-
ably provide the same functionality if one simply denied all
access to the network, the rest would likely exhibit side ef-
fects.
Given the variation in the types of sensitive data, ways of
using this data for user-desired features, and ways to misuse
this data, it may simply not be possible to apply a single,
one-size-ﬁts-all policy that can protect data while minimiz-
ing side eﬀects. Thus, we set out to explore a choice of
privacy controls that could be customized to balance the
needs of applications with the privacy requirements of their
users.
3. PRIVACY CONTROLS
AppFence implements data shadowing, to prevent appli-
cations from accessing sensitive information that is not re-
quired to provide user-desired functionality, and exﬁltration
blocking, to block outgoing communications tainted by sen-
sitive data. Either (or even both) of these controls may be
applied to limit an application’s misuse of a sensitive data
type.
3.1 Data shadowing
Since today’s applications do not suspect the use of shad-
owing, we opt for simple shadow data rather than developing
more elaborate ruses to fool applications that might attempt
to detect shadowing. However, our implementation can be
easily extended to support more sophisticated shadow data
than what is presented below if it becomes necessary to do
so.
Android applications use the ﬁle system to access the cam-
era, microphone, and logs. When applications try to open
these resources, we provide the illusion of opening an empty
ﬁle. Similarly, we shadowed browser metadata (history and
bookmarks), SMS/MMS messages, subscribed feeds, con-
tacts, accounts, and calendar entries by returning an empty
set of data.
When applications request the device’s location, we return
the coordinates 37.421265, -122.084026.
When applications request the device’s phone state, we
construct phone state with a ﬁxed phone number (1 650 623
4000) and an application-speciﬁc device ID. The shadow de-
vice ID (IMEI) is generated by hashing a three-tuple con-
taining the device ID, application name, and a secret salt
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 11 21 31 41 51 61 71 81 91 Fractional share of network traffic Rank Other Servers A&A Servers 643Our combined implementation of shadowing and exﬁl-
tration blocking required introducing or modifying roughly
5, 000 lines of the Android platform code.
3.3 Limitations
One of the known limitations of our implementation is
that the TaintDroid information ﬂow tracking system, on
which we built AppFence’s exﬁltration blocking feature,
does not track information leaked through control ﬂow op-
erations. Applications intent on circumventing exﬁltration
blocking could move data using control ﬂow operations.
Tracking control ﬂow may have reasonable overhead, espe-
cially if the code to do so is only activated when a tainted
variable is loaded into the register space, but could raise the
rate of false positives.
Still, actively circumventing AppFence would not be with-