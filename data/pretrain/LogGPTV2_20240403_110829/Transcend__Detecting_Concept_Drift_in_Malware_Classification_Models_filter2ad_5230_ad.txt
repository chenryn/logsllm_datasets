4Unless otherwise stated, we refer to Drebin as both the learning
algorithm and the dataset outlined in [2].
4.1.1 Detecting Concept Drift
This section presents a number of experiments to show
how Transcend identiﬁes concept drift and correctly
marks as untrustworthy the decisions the NCM-based
classiﬁer predicts erroneously.
We ﬁrst show how the performance of the learning
model introduced in [2] decays in the presence of con-
cept drift. To this end, we train a model with the Drebin
dataset [2] and we test it against 9,000 randomly selected
malicious and benign Android apps (with equal split)
drawn from the Marvin dataset [14]. The confusion ma-
trix in Table 3a clearly shows how the model is affected
by concept drift as it reports low precision and recall for
the positive class representing malicious objects5. This
is further outlined in Figure 3a, which shows how the p-
value distribution of malicious objects is pushed towards
low values (poor prediction quality).
Table 3b shows how enforcing cut-off quality thresh-
olds affect—by improving—the performance of the
same learning algorithm. For this experiment, we di-
vided the Drebin dataset in training and calibration sets
with a 90-10% averaged over 10 rounds. This ensures
that each object in the dataset has a p-value. We then
asked Transcend to identify suitable quality thresholds
(cfr § 3.3) with the aim to maximize the F1-score as de-
rived by the calibration dataset, subject to a minimum
F1-score of 0.99 and a minimum percentage of kept el-
ement of 0.766. It is worth noting that such thresholds
are derived from the calibration dataset but are enforced
to detect concept drift on a testing dataset. Results show
how ﬂagging predictions of testing objects with p-values
below the cut-off thresholds as unreliable improves pre-
cision and recall for the positive (malicious) class, from
0.61 to 0.89 and from 0.36 to 0.76, respectively.
5Drebin spans the years 2010–2012 while Marvin covers from 2010
to 2014. Most of the Drebin’s features capture information (e.g., string
and IP addresses) that is likely to change over time, affecting the ability
of the classiﬁer to identify non-stationary data.
6In [2], Arp et al. report a TPR of 94% at a FPR of 1%. Such
metrics do not rule out the possibility of having 0.99 as F1-score; if
that is a plausible constraint, Transcend’s parametric framework will
ﬁnd a suitable solution.
632    26th USENIX Security Symposium
USENIX Association
(a) Decision assessment for the binary classiﬁcation case study
(Drebin [2]) with the original dataset. Correct predictions are
supported by a high average algorithm credibility and conﬁ-
dence, while incorrect ones have a low and a high algorithm
credibility and conﬁdence, respectively. Overall, positive results
supported by a strong statistical evidence.
(b) Alpha assessment for the binary classiﬁcation case study
(Drebin [2]) with the original dataset. Benign samples are well
separated from malicious ones, especially when the assigned la-
bel is benign; this provides a clear statistical support that posi-
tively affect the quality of predictions.
Figure 2: Binary Classiﬁcation Case Study (Drebin [2]): Decision assessment and Alpha assessment.
Assigned label
Assigned label
Assigned label
Sample
Benign
Malicious
Precision
Benign Malicious Recall
4 498
2 890
0.61
1 610
0.36
2
1
1
Sample
Benign
Malicious
Precision
Benign Malicious Recall
4 257
504
0.89
1 610
0.76
2
1
1
Sample
Benign
Malicious
Precision
Benign Malicious Recall
0.98
4 413
255
0.94
0.96
87
4 245
0.98
(a)
(b)
(c)
Table 3: Binary classiﬁcation case study ([2]). Table 3a: confusion matrix when the model is trained on Drebin and
tested on Marvin. Table 3b: confusion matrix when the model is trained on Drebin and tested on Marvin with p-value-
driven threshold ﬁltering. Table 3c: retraining simulation with training samples of Drebin as well as the ﬁltered out
element of Marvin of Table 3b (2386 malicious samples and 241 benign) and testing samples coming from another
batch of Marvin samples (4500 malicious and 4500 benign samples). The fate of the drifting objects is out of scope
of this paper as that would require to solve a number of challenges that arise once concept drift is identiﬁed (e.g.,
randomly sampling untrustworthy samples according to their p-values, effort of relabeling depending on available
resources, model retraining). We nonetheless report the result of a realistic scenario in which objects drifting from a
given model, correctly identiﬁed by Transcend, represent important information to retrain the model and increase its
performance (assuming a proper labeling as brieﬂy sketched above).
TPR
of kept elements
FPR
of kept elements
1st quartile
Median
Mean
3rd quartile
p-value
0.9045
0.8737
0.8737
0.8723
probability
0.6654
0.8061
0.4352
0.6327
p-value
0.0007
0.0000
0.0000
0.0000
probability
0.0
0.0
0.0
0.0
TPR
of discarded elements
p-value
probability
0.0000
0.3080
0.3080
0.3411
0.3176
0.3300
0.3433
0.3548
FPR
of discarded elements
p-value
probability
0.0000
0.0008
0.0008
0.0005
0.0013
0.0008
0.0018
0.0005
MALICIOUS
kept elements
BENIGN
kept elements
p-value
0.3956
0.0880
0.0880
0.0313
probability
0.1156
0.0584
0.1578
0.0109
p-value
0.6480
0.4136
0.4136
0.1573
probability
0.6673
0.4304
0.7513
0.1629
Table 4: Binary classiﬁcation case study ([2]): examples of thresholds. From the results we can see that increasing the
threshold will lead to keep only the sample where the algorithm is sure about. The number of discarded samples is
very subjective to the severity of the shift in the dataset, together with the performance of those sample it is clear the
advantage of the p-value metric compared to the probability one.
USENIX Association
26th USENIX Security Symposium    633
Correct choicesIncorrect choices0.00.20.40.60.81.0Average algorithm credibility for correct choiceAverage algorithm confidence for correct choiceAverage algorithm credibility for incorrect choiceAverage algorithm confidence for incorrect choiceGiven label: maliciousGiven label: benign0.00.20.40.60.81.0Given label malicious: p-value maliciousGiven label malicious: p-value benignGiven label benign: p-values maliciousGiven label benign: p-values benign(a)
(b)
(c)
(d)
Figure 3: Binary Classiﬁcation Case Study: p-value and probability distribution for true malicious and benign samples
when the model is trained on Drebin dataset and tested on Marvin. Graph (a): p-value distribution for true malicious
samples. Graph (b): p-value distribution of true benign samples. Graph (c): probability distribution of true malicious
samples. Graph (d): probability distribution of true benign samples.
We would like to remark that drifting objects are still
given a label as the output of a classiﬁer prediction;
Transcend ﬂags such predictions as untrustworthy, de-
facto limiting the mistakes the classiﬁer would likely
make in the presence of concept drift. It is clear that one
needs to deal with such objects, eventually. Ideally, they
would represent an additional dataset that, once labeled
properly, would help retraining the classiﬁer to predict
similar objects. This opens a number of challenges that
are out of the scope of this work; however, one could still
rely on CE’s metrics to prioritize objects that should be
labeled (e.g., those with low p-values as they are the one
the drift the most from the model). This might require
to randomly sample drifting objects once enough data is
available as well as understanding how much resources
one can rely on for data labeling. It is important to note
that Transcend plays a fundamental role in this pipeline:
it identiﬁes concept drift (and, thus, untrustworthy pre-
dictions), which gives the possibility of start reasoning
on the open problems outlined above.
The previous paragraphs show the ﬂexibility of the
parametric framework we outlined in § 3.3, on an arbi-
trary yet meaningful example, where statistical cut-off
thresholds are identiﬁed based on an objective function
to optimize, subject to speciﬁc constraints. Such goals
are however driven by business requirements (e.g., TPR
vs FPR) and resource availability (e.g., malware ana-
lysts available vs number of likely drifting samples—
either benign or malicious—for which we should not
trust a classiﬁer decision) thus providing numerical ex-
ample might be challenging. To better outline the suit-
ability of CE’s statistical metrics (p-values) in detecting
concept drift, we provide a full comparison between p-
values and probabilities as produced by Platt’s scaling
applied to SVM. We summarize a similar argument (with
probabilities derived from decision trees) for multiclass
classiﬁcation tasks in § 4.2.
Comparison with Probability.
In the following, we
compare the distributions of p-values, as derived from
CE, and probabilities, as derived from Platt’s scaling for
SVM, in the context of [2] under the presence of con-
cept drift (i.e., training on Drebin, testing on Marvin as
outlined). The goal of this comparison is to understand
which metric is better-suited to identify concept drift.
Figure 3a shows the alpha assessment of the classiﬁ-
cations shown in Table 3a. The ﬁgure shows the distribu-
tion of p-values when the true label of the samples is ma-
licious. Correct predictions (ﬁrst and second columns),
reports p-values (ﬁrst column) that are are slightly higher
than those corresponding to incorrect ones (second col-
umn), with a marginal yet well-marked separation as
compared to the values they have for the incorrect class
(third and fourth columns). Thus, when wrong predic-
tions refer to the benign class, the p-values are low and
show a poor ﬁt to both classes. Regardless of the classi-
ﬁer outcome, the p-value for each sample is very low, a
likely indication of concept drift.
Figure 3b depicts the distribution of p-values when
true label of the samples is benign. Wrong predictions
(ﬁrst and second columns) report p-values representing
benign (second column) and malicious (ﬁrst column)
classes to be low. Conversely, correct predictions (third
and fourth columns) represent correct decisions (fourth
column) and have high p-values, much higher compared
to the p-values of the incorrect class (third column). This