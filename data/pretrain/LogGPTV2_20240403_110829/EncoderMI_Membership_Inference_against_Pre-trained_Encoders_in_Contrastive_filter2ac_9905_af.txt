scores for potential members and ground truth non-members in
our two evaluation datasets. We observe that the potential mem-
bers and ground truth non-members are statistically distinguishable
with respect to the average pairwise cosine similarity scores. In par-
ticular, the potential members tend to have larger average pairwise
cosine similarity scores than the ground truth non-members.
Effectiveness of EncoderMI: Table 4 shows the accuracy, preci-
sion, and recall of the three variants of EncoderMI based on different
shadow datasets when applied to the CLIP’s image encoder. The
accuracy, precision, and recall are calculated based on the 1,000
potential members and 1,000 ground truth non-members in each
evaluation dataset. First, we observe that EncoderMI based on dif-
ferent shadow datasets achieves high accuracy, e.g., 0.66 – 0.75. Our
results imply that overfitting exists in real-world image encoders
0.700.750.800.850.900.951.00Average pairwise cosine similarity score020406080100120Number of imagesPotential membersGround truth non-members0.700.750.800.850.900.951.00Average pairwise cosine similarity score020406080100120Number of imagesPotential membersGround truth non-membersSession 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2091,
√
,
encoder for less number of epochs. We evaluate the early stopping
based countermeasure against our EncoderMI. In particular, we
pre-train a target encoder on the pre-training dataset based on CI-
FAR10. After we pre-train the target encoder for some epochs, we
calculate the accuracy of our EncoderMI-V under the background
√) and we also calculate the classification
knowledge B = (√
accuracy of a downstream classifier built based on the target en-
coder. Both the pre-training dataset and the downstream dataset
are constructed based on CIFAR10 as we described in Section 5, and
the classification accuracy of the downstream classifier is calcu-
lated based on the testing dataset of CIFAR10. Figure 5b shows the
membership inference accuracy of our EncoderMI-V and the classi-
fication accuracy of the downstream classifier as we pre-train the
target encoder for more epochs. We observe that the early stopping
based defense achieves a trade-off, i.e., it decreases the membership
inference accuracy but also reduces the classification accuracy of
the downstream classifier. We note that Song et al. [47] found that
early stopping outperforms other overfitting-prevention counter-
measures against membership inference to classifiers, and they also
observed a trade-off between membership inference accuracy and
classifier utility for early stopping.
Pre-training with differential privacy: Differential privacy [7,
15, 26, 36, 43] can provide formal membership privacy guarantees
for each input in the training dataset of a machine learning model.
Many differentially private learning algorithms have been proposed.
These algorithms add noise to the training data [14], the objective
function [25, 26], or the gradient computed by (stochastic) gradient
descent during the learning process [7, 26, 43]. For instance, Abadi
et al. [7] proposed differentially private stochastic gradient descent
(DP-SGD) which adds random Gaussian noise to the gradient com-
puted by stochastic gradient descent. It would be interesting future
work to generalize these differentially private learning algorithms
to contrastive learning. In particular, when the pre-training dataset
changes by one input, the encoder learnt by a differentially private
contrastive learning algorithm does not change much. However,
differential privacy may also incur large utility loss for the encoder,
i.e., the downstream classifiers built based on a differentially private
encoder may have much lower classification accuracy.
Adversarial learning: Adversarial learning based countermea-
sures [29, 34] have been studied to mitigate membership inference
to classifiers, which were inspired by adversarial learning based
countermeasures against attribute inference attacks [27]. For in-
stance, Nasr et al. [34] proposed to add an adversarial regularization
term to the loss function when training a target classifier, where
the adversarial regularization term models a membership inference
method’s accuracy. Jia et al. [29] proposed MemGuard which does
not modify the training process, but adds carefully crafted perturba-
tion to the confidence score vector outputted by the target classifier
for each input. Specifically, the idea is to turn the perturbed confi-
dence score vector to be an adversarial example to the inference
classifiers, which make random membership inference based on the
perturbed confidence score vector. It would be interesting future
work to extend these countermeasures to pre-trained encoders. For
instance, we may capture our EncoderMI’s accuracy as an adver-
sarial regularization term and add it to the contrastive loss when
pre-training an encoder using contrastive learning; we may also
add carefully crafted perturbation to the feature vector outputted by
(a) Overfitting of the target encoder
(b) Inferability-utility tradeoff
Figure 5: (a) The target encoder is more overfitted to its
pre-training data when it’s pre-trained for more epochs.
(b) Trade-off between membership inference accuracy and
encoder utility for early stopping, where the method is
EncoderMI-V and the dataset is CIFAR10.
such as CLIP. Second, EncoderMI achieves a higher recall than
precision, which means that EncoderMI predicts a large fraction
of potential members as members. Third, our EncoderMI achieves
higher recall (or lower precision) on Google than Flickr. In other
words, our EncoderMI predicts more inputs from Google image
search as members. The reason is that, on average, the average pair-
wise cosine similarity score for inputs from Google image search is
larger than that for inputs from Flickr as shown in Figure 4.
7 DISCUSSION ON COUNTERMEASURES
Preventing overfitting via early stopping: Recall that our Enco-
derMI exploits the overfitting of a target encoder on its pre-training
dataset. Note that the overfitting of a target encoder on its pre-
training dataset is different from that of a classifier. For instance,
when a classifier is overfitted to its training dataset, it may have
different classification accuracies on its training dataset and testing
dataset. Moreover, the confidence score vectors outputted by the
classifier for its training dataset and testing dataset are also statis-
tically distinguishable. Given an input, a target encoder outputs
a feature vector for it. However, the feature vector itself does not
capture the overfitting of the target encoder on the input. Instead,
when the target encoder is overfitted to its pre-training dataset, it
may output more similar feature vectors for the augmented versions
of an input in the pre-training dataset.
We find that a target encoder is more overfitted to its pre-training
dataset when it is trained for more epochs. Recall that the member-
ship features for an input constructed by our EncoderMI consist
of 45 pairwise cosine similarity scores. For each member or non-
member of the target encoder, we compute the average pairwise
cosine similarity scores in its membership features, and we fur-
ther compute the average pairwise cosine similarity scores over
all members (or non-members). Figure 5a shows the average pair-
wise cosine similarity scores for members and non-members of
the target encoder as the number of pre-training epochs increases,
where the pre-training dataset is based on CIFAR10. We observe
the average pairwise cosine similarity increases (or decreases) for
members (or non-members) as the number of epochs increases. In
other words, the target encoder is more overfitted to its pre-training
dataset when pre-training it for more epochs.
Our observation inspires us to counter EncoderMI via prevent-
ing overfitting through early stopping, i.e., pre-training a target
80012001600#Epochs to train target encoder0.600.650.700.750.800.85Average cosine similarityMembersNon-Members80012001600#Epochs to train target encoder0.700.750.800.850.90AccuracyMembership inference accuracyClassification accuracy of downstream classifierSession 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2092an encoder for each (augmented) input such that the set of member-
ship features constructed by our EncoderMI for an input becomes
an adversarial example to the inference classifiers, which make
random membership inference based on the perturbed membership
features. A key challenge is how to find such perturbation to each
feature vector since the membership features depend on the pair-
wise similarity between a set of feature vectors corresponding to
the augmented versions of an input.
8 RELATED WORK
Membership inference: In membership inference against ma-
chine learning classifiers [11, 18, 19, 22, 24, 31, 32, 32, 35, 35, 41,
42, 44, 47, 48, 52], an inferrer aims to infer whether an input is
in the training dataset of a classifier (called target classifier). For
instance, in the methods proposed by Shokri et al. [44], an infer-
rer first trains shadow classifiers to mimic the behaviors of the
target classifier. Given the shadow classifiers whose ground truth
members and non-members are known to the inferrer, the inferrer
trains inference classifiers, which are then applied to infer mem-
bers of the target classifier. Salem et al. [42] further improved these
methods by relaxing the assumptions about the inferrer. Hui et
al. [24] proposed blind membership inference methods that do not
require training shadow classifiers. Concurrent to our work, He
et al. [22] also studied membership inference against contrastive
learning. They assume the pre-training data and downstream data
are the same. Specifically, given a labeled training dataset, they first
use contrastive learning to pre-train an encoder and then use it to
fine-tune a classifier on the labeled training dataset. They try to
infer whether an input is in the labeled training dataset via applying
existing methods [42, 44] to the fine-tuned classifier.
Our methods are different from these ones as they were designed
to infer members of a classifier while our methods aim to infer
members of an encoder pre-trained by contrastive learning. Our
experimental results show that these methods achieve accuracy
close to random guessing when applied to infer members of an
encoder. The reason is that the confidence score vector outputted
by a classifier can capture whether the classifier is overfitted for
an input, while the feature vector itself outputted by an encoder
does not capture whether the encoder is overfitted for an input.
The similarity scores between the feature vectors of the augmented
versions of an input capture whether the encoder is overfitted for
the input, and our methods leverage such similarity scores to infer
the membership status of the input.
Existing membership inference methods for pre-trained mod-
els [8, 46] focused on the natural language domain. For instance,
Carlini et al. [8] proposed membership inference methods for GPT-
2 [39], which is a pre-trained language model, and they further
leveraged the membership inference methods to reconstruct the
training data of GPT-2. Specifically, they first reconstructed some
candidate texts and then applied a membership inference method to
determine the membership status of each candidate text. To the best
of our knowledge, no prior work has studied membership inference
for encoders in the image domain.
Prior work [23, 54] also studied membership inference against
transfer learning. For instance, Hidano et al. [23] assume a white-
box access to the transferred part of the teacher model while Zou
et al. [54] leverage the posterior of the teacher model. Our work
is different from these, because pre-training an image encoder is
different from training a teacher model since the former uses con-
trastive learning on unlabeled data while the latter uses the standard
supervised learning on labeled data.
Countermeasures against membership inference: Many coun-
termeasures [9, 29, 31, 34, 42, 44, 47] were proposed to counter
membership inference for classifiers. The first category of coun-
termeasures [42, 44, 47] try to prevent overfitting when training
classifiers, e.g., standard 𝐿2 regularization [44], dropout [42], and
early stopping [47]. The second category of countermeasures [7, 43]
are based on differential privacy [15], which often incur large utility
loss for the learnt machine learning classifiers. The third category
of countermeasures leverage adversarial learning, e.g., adversar-
ial regularization [34] and MemGuard [29]. We explored an early
stopping based countermeasure against our EncoderMI. Our re-
sults show that such countermeasure achieves a trade-off between
membership inference accuracy and utility of an encoder.
Contrastive learning: Contrastive learning [10, 13, 17, 20, 37, 40,
50] aims to pre-train image encoders on unlabeled data via ex-
ploiting the supervisory signals in the unlabeled data itself. The
unlabeled data could be unlabeled images or (image, text) pairs. The
pre-trained encoders can be used for many downstream tasks. The
key idea of contrastive learning is to pre-train an image encoder
such that it outputs similar feature vectors for a pair of augmented
inputs created from the same input image and outputs dissimilar
feature vectors for a pair of augmented inputs created from different
input images. Examples of contrastive learning methods include
MoCo [20], SimCLR [10], and CLIP [38], which we discussed in
Section 2. We note that Jia et al. [28] proposed BadEncoder, which
embeds backdoors into a pre-trained image encoder such that mul-
tiple downstream classifiers built based on the backdoored encoder
inherit the backdoor behavior simultaneously.
9 CONCLUSION AND FUTURE WORK
In this work, we propose the first membership inference method
against image encoders pre-trained by contrastive learning. Our
method exploits the overfitting of an image encoder, i.e., it produces
more similar feature vectors for two augmented versions of the
same input. Our experimental results on image encoders pre-trained
on multiple datasets by ourselves as well as a real-world image
encoder show that our method can achieve high accuracy, precision,
and recall. Moreover, we also find that an early stopping based
countermeasure achieves a trade-off between membership inference
accuracy and encoder utility.
Interesting future work includes 1) extending our method to the
white-box settings in which the inferrer has access to the parame-
ters of the target encoder, 2) extending our method to infer the mem-
bership of an (image, text) pair, 3) developing new countermeasures
against our method, and 4) exploring other privacy/confidentiality
risks of pre-trained image encoders such as stealing their parame-
ters [49] and hyperparameters (e.g., encoder architecture) [51].
Acknowledgements: We thank the anonymous reviewers and
our shepherd Reza Shokri for constructive comments. This work
was supported by NSF under Grant No. 1937786.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2093REFERENCES
[1] 2020. Twitter demands AI company stops ’collecting faces’. https://www.bbc.
com/news/technology-51220654. (2020).
[2] 2021. DeepSets. https://github.com/manzilzaheer/DeepSets. (2021).
[3] 2021. FTC settlement with Ever orders data and AIs deleted after facial recognition
pivot. https://techcrunch.com/2021/01/12/ftc-settlement-with-ever-orders-data-
and-ais-deleted-after-facial-recognition-pivot. (2021).
[4] 2021. MicroImageNet classification challenge. https://www.kaggle.com/c/tiny-
imagenet/overview. (2021).
[17] Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction
[12] Adam Coates, Andrew Ng, and Honglak Lee. 2011. An analysis of single-layer
[15] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrat-
[5] 2021. MoCo. https://github.com/facebookresearch/moco. (2021).
[6] 2021. SimCLR. https://github.com/leftthomas/SimCLR. (2021).
[7] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In
CCS.
[8] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-
Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,
et al. 2021. Extracting Training Data from Large Language Models. In USENIX
Security Symposium.
[9] Qingrong Chen, Chong Xiang, Minhui Xue, Bo Li, Nikita Borisov, Dali Kaarfar,
and Haojin Zhu. 2018. Differentially private data generative models. arXiv
preprint arXiv:1812.02274 (2018).
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In ICML.
[11] Christopher A Choquette Choo, Florian Tramer, Nicholas Carlini, and Nicolas
Papernot. 2021. Label-only membership inference attacks. In ICML.
networks in unsupervised feature learning. In AISTATS.
[13] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller,
and Thomas Brox. 2015. Discriminative unsupervised feature learning with
exemplar convolutional neural networks. IEEE transactions on pattern analysis
and machine intelligence 38, 9 (2015), 1734–1747.
[14] John C Duchi, Michael I Jordan, and Martin J Wainwright. 2013. Local privacy
and statistical minimax rates. In FOCS.
ing noise to sensitivity in private data analysis. In TCC.
[16] Daryl LX Fung, Qian Liu, Judah Zammit, Carson Kai-Sang Leung, and Pingzhao
Hu. 2021. Self-supervised deep learning model for COVID-19 lung CT image
segmentation highlighting putative causal relationship among age, underlying