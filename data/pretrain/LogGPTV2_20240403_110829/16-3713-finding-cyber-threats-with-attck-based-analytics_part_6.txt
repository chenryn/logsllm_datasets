unacceptable level of false positives.
5.1 Cyber Game Experiences
MITRE performed six cyber games of experimentation between 2012-2013. Over the course of
these games, both the Red and Blue Teams refined their skills and repeatedly adapted to each
other’s capabilities. Each game provided a learning experience for how future exercises should
be structured and scoped, and subsequent exercises attempted to integrate these lessons learned.
This section discusses the first three cyber games, examines lessons learned, and describes how
these lessons drove the refinement of future cyber games. This section also annotates the cyber
game narratives with ATT&CK tactics and techniques in the footnotes to show how the
ATT&CK model was incorporated into MITRE’s plans.
As discussed in Section 4, these games were performed several weeks after the simulated
compromise. Part of the reason for this is that the cyber games were performed by researchers
and not full-time network defenders. If synchronous games had been conducted, they would have
required researchers to constantly monitor the network, or it would have required them to
monitor the network during a pre-arranged date range. The constant monitoring of networks
would have precluded MITRE researchers from conducting their actual research. Alternatively,
giving researchers a set of days to constantly monitor the network would have simulated an APT
telling network defenders when they were going to attack—an unrealistic scenario at best.
MITRE’s analytic writers, on the other hand, monitored their analytics every few days, just in
case something truly malicious did occur on the portion of the MITRE network being used for
the cyber games. Whenever a researcher discovered something suspicious, he or she asked the
White Team lead to identify whether the identified activity was part of an exercise or not. Once
the analytics reached maturity, the Blue Team usually detected the Red Team during the
adversary emulation operation. Any researchers who detected Red Team activities were asked
not to participate in the Blue Team exercise until the other Blue Team members had narrowed
down Red Team activity to a specific time frame, so as not to bias the rest of the team members
with a priori information about the exercise.
Cyber Game #1 - For the first cyber game, the Blue Team was composed of people from
various backgrounds, none of which were directly related to Blue Teaming. The Blue Team grew
in experience over subsequent cyber games. Conversely, all Red Team members had several
years of Red Team experience, thus making the apparent outcome favor the Red Team. This first
exercise was used as a test to evaluate the usefulness of several potential sensors, and as such,
29
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
these sensors were not widely deployed at the time. This led to limited visibility and made
detecting the Red Team activity across hosts challenging. The necessity of widespread
deployment of these sensors was one of the fundamental lessons learned from this exercise.
Despite the disparity in past experience between the two teams, the Blue Team was able to
identify a significant amount of the Red Team activity. Red Team activity consisted mostly of
being intentionally noisy and of writing flag files to different hosts using Windows file shares34
(SMB). This provided evidence that the Red Team had established some control on those hosts
and created a metric by which Blue Team success could be measured. The Red Team also ran
credential harvesting tools35. Overall the Blue Team was highly successful in their investigation
despite their limited experience. This was in large part due to the use of endpoint sensors with
visibility into the command lines of process start events on some hosts and visibility into SMB
file copies performed by the Red Team on others.
Cyber Game #2 - For the second cyber game, the Red Team decided to use more advanced
tactics in light of the Blue Team’s prior success. Many of the analytics that were developed in
response to the first cyber game failed to yield any results in the second cyber game because the
Red Team renamed built-in windows commands36 and did not perform obvious actions such as
writing files with names such as flag.txt. In addition to this, the sensor environment was still in a
state of flux. This led to gaps where the Blue Team had little visibility.
Despite having the lowest success rate of any of the cyber games, in the second cyber game, the
Blue Team learned valuable lessons for future exercises. One of the major efforts undertaken as a
result of this exercise was the development of analytics for analyzing user login activity across
the network—with a focus on admin activity. On the planning side, MITRE discovered how
much more difficult it is for a Blue Team to discover malicious activity when the environment is
not stable and sensor deployment is occurring concurrently. In future games, a greater effort was
made by the White Team to ensure a stable network environment prior to the beginning of the
exercise.
Cyber Game #3 - While the first cyber game was too easy and the second was too hard for the
Blue Team, the third game was somewhere in between. In this cyber game, the White Team
wanted to ensure that the Blue Team would be challenged and be exposed to new TTPs, while at
the same time being allowed to validate previously used analytics. It was during this exercise that
the White Team planners decided to intentionally drive the Red Team towards certain TTPs that
would generate alerts from Blue Team analytics and to use TTPs unfamiliar to the Blue Team.
The Blue Team achieved a similar success rate to that of the first exercise. They were able to
identify the majority of Red Team activity, but there were still a couple of gaps that they
34 ATT&CK: T1077 – Lateral Movement/Windows Admin Shares
35 ATT&CK: T1003 – Credential Access/Credential Dumping
36 ATT&CK: T1036 – Defense Evasion/Masquerading
30
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
struggled to fill, such as identifying the information that was actually exfiltrated37 and
determining how the Red Team maintained its persistence38.
Subsequent exercises followed the same methodology utilized in the third game. As new
analytics were developed, the Red Team intentionally performed activity designed to trigger
them. The Red Team also adopted new TTPs to push the Blue Team to develop new and better
ways to detect APT activity. How the Blue Team approached the exercises changed over time as
well; at first there was very little coordination or communication between team members. As
exercises were held, the Blue Team tried different methods to address this problem so as to
minimize duplication of effort. They began to use collaboration software (Microsoft SharePoint
and OneNote) to share discoveries and information with each other. The following list details
some of the important points of information the Blue Team found helpful regarding collaboration
during a cyber game:
• A single timeline was maintained that all the team members could reference and add to, as
new information was gained. This helped to minimize duplicative effort and helped to
identify gaps in knowledge regarding Red Team activity.
• A suspicious host list was built over the course of the engagement that outlined all of the
hosts the Blue Team felt was worthy of investigation. This list also included some basic
system information from each host, such as IP addresses, usernames, and why that host was
identified as being suspicious. Having this extended information aided other team members
when they saw network sessions from a host or IP address and wanted to know if that host
was already suspected of being the victim of malicious activity.
• Drawing a network graph of how all the suspicious hosts were related was also very helpful.
By including details like how the connections between hosts were made and the temporal
ordering of events analysts could more easily see the progression of the compromise. It also
helped to see how all the hosts were related. Note: Nodes of activity might also be easier to
identify using this method.
• The Blue Team initially failed to realize the importance of capturing detailed notes about
their actions during the exercise. This impacted their ability to relay details about discovered
events to other team members and made it very difficult to write up the Blue Team report
after the end of the exercise. As time went on, the Blue Team agreed on a standard template
that could be used by all member of the team for recording the data they looked at and the
results of their investigations. This template included data such as time of execution of the
analytic or query, the actual analytic or query initiated, what they were searching for and
why, and the results.
5.2 Analytic Iteration
As discussed previously, one of the core aspects of ATT&CK-based analytics is iteration. A
good example of this process of analytic development across cyber games was the evolution of
37 ATT&CK: Tactic – Exfiltration
38 ATT&CK: Tactic – Persistence
31
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
the Suspicious Commands Analytic. (See Section 0 for analytic details.) This analytic was
originally created to identify instances where certain commands were used in the network
environment. The list of commands it looked for was originally based on open source threat
information and ATT&CK techniques. The commands came from over 20 different ATT&CK
techniques, such as Discovery/Process Discovery, Persistence/New Service, and
Execution/Scheduled Task.
After the first cyber game, MITRE researchers learned several lessons about this analytic. One of
these lessons was that the initial noise level was too high, requiring researchers to modify the
analytic to look for groups of commands that were run together. This both reduced the noise
level as well as added more context to each command, better enabling analysts to determine if a
command was used maliciously or not. The Red Team also used commands that were not
previously on the list of commands being monitored. After identifying this gap, the Blue Team
added those commands to the list of commands being monitored and began monitoring the
updated analytic.
One of the major changes the Red Team made during the second cyber game was to rename
many of the tools they used to better hide them from the Blue Team. This tactic thwarted the
Suspicious Command Analytic and forced the Blue Team to identify a new way of finding these
commands. To accomplish this, the Blue Team started looking for arguments that were
commonly used by these commands. This method was noisier but it did enable the Blue Team to
successfully identify Red Team activity. After the exercise ended, the Blue Team made a note of
this analytic idea in their report and later refined it into a complete analytic. Like the analytic that
preceded it, this new suspicious arguments analytic was further refined as a result of day-to-day
monitoring and more cyber games.
The analytic products of this research effort were documented and published in the MITRE
Cyber Analytic Repository (CAR). CAR contains analytics mapped to specific ATT&CK
techniques and describes the high-level analytic hypothesis, pseudocode analytic
implementation, unit tests, and the data model used to develop them so the analytics can be
transcribed to various analytic platforms. CAR is intended to be used by cyber defenders
throughout the community and serves as a mechanism for sharing behavioral-based analytics that
can be used for adversary detection. See Appendix A.1 for example analytics. CAR was released
in 2016 and is available at https://car.mitre.org.
32
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
Summary
APTs have been and will likely continue to be successful at gaining initial access into targeted
enterprise networks. The ATT&CK-based analytics development method is a powerful tool for
network defenders to use for creating and maintaining a capability to detect these threats.
Detection using these methods does not rely on typical known-bad IOCs or external notification
of a network breach and can lead to the rapid discovery of a network compromise by detecting
an adversary’s use of techniques described in the ATT&CK model.
The analytics development method is based on five principles:
• Include post-compromise detection
• Focus on behavior
• Base on a threat model
• Iterate by design
• Develop in a realistic environment.
MITRE researchers used the ATT&CK model to inform both the development of analytics and
the structuring of Red Teams for cyber games. MITRE utilized the seven steps of the ATT&CK-
based analytics development method to iteratively improve its defensive posture in its cyber
games with new and refined analytics. MITRE has published CAR to serve as the analytic
sharing platform to document the analytics developed through this effort for the community to
use and expand upon.
33
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
This page intentionally left blank.
34
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
References
[1] FireEye, "M-Trends 2016," [Online]. Available: https://www2.fireeye.com/rs/848-DID-
242/images/Mtrends2016.pdf.
[2] S. Tomonaga, "Windows Commands Abused by Attackers," 26 January 2016. [Online].
Available: http://blog.jpcert.or.jp/2016/01/windows-commands-abused-by-attackers.html.
[Accessed 17 September 2016].
[3] The MITRE Corporation, "Adversarial Tactics, Techniques and Common Knowledge,"
July 2016. [Online]. Available: https://attack.mitre.org/. [Accessed 6 September 2016].
[4] The MITRE Corporation, "Cyber Analytics Repository," September 2016. [Online].
Available: https://car.mitre.org/wiki/Main_Page. [Accessed 13 September 2016].
[5] Financial Times, "A sobering day," [Online]. Available: http://labs.ft.com/2013/05/a-
sobering-day/.
[6] SecureList, "THE MsnMM Campaigns," SecureList, 2015.
[7] FireEye, "HAMMERTOSS: Stealthy Tactics Define a Russian Cyber Threat Group," 2015.
[8] B. Delpy, "mimkatz," [Online]. Available: http://blog.gentilkiwi.com/mimikatz. [Accessed
19 05 2016].
[9] "PowerSploit," [Online]. Available: https://github.com/PowerShellMafia/PowerSploit.
[Accessed 19 05 2016].
[10] HarmJ0y, "Veil-Evasion," [Online]. Available: https://www.veil-
framework.com/framework/veil-evasion/. [Accessed 25 05 2016].
[11] SecureWorks, "Living Off the Land," 28 May 2015. [Online]. Available:
https://www.secureworks.com/blog/living-off-the-land. [Accessed 26 January 2016].
[12] L. Szekeres, M. Payer, T. Wei and D. Song, "Sok: Eternal war in memory," in IEEE
Symposium on Security and Privacy, 2013.
[13] Wikipedia, "Stuxnet," [Online]. Available: https://en.wikipedia.org/wiki/Stuxnet.
[Accessed 06 09 2016].
35
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
[14] W. Gragido, "Understanding Indicators of Compromise (IOC) Part 1," [Online]. Available:
http://blogs.rsa.com/understanding-indicators-of-compromise-ioc-part-i/. [Accessed 07 07
2016].
[15] The MITRE Corporation, "Threat-based Defense," [Online]. Available:
https://www.mitre.org/capabilities/cybersecurity/threat-based-defense. [Accessed
September 2016].
[16] The MITRE Corporation, "ATT&CK Matrix," 9 2016. [Online]. Available:
https://attack.mitre.org/wiki/ATT%26CK_Matrix. [Accessed 11 9 2016].
[17] Splunk, "Splunk," [Online]. Available: http://www.splunk.com/. [Accessed 25 05 2016].
[18] Apache, "Welcome to Apache(TM) Hadoop," [Online]. Available:
http://hadoop.apache.org/. [Accessed 06 09 2016].
[19] Elastic, "Elasticsearch | Elastic," [Online]. Available: https://www.elastic.co/. [Accessed 06
09 2016].
[20] R. Mudge, "Adversary Simulation Becomes a Thing...," 12 11 2014. [Online]. Available:
http://blog.cobaltstrike.com/2014/11/12/adversary-simulation-becomes-a-thing/. [Accessed
11 9 2016].
[21] C. Gates, "More on Purple Teaming," 21 March 2016. [Online]. Available:
http://carnal0wnage.attackresearch.com/2016/03/more-on-purple-teaming.html. [Accessed
4 April 2016].
[22] M. Mimoso, "Core Windows Utility Can Be Used to Bypass AppLocker," 21 4 2016.
[Online]. Available: https://threatpost.com/core-windows-utility-can-be-used-to-bypass-
applocker/117604/. [Accessed 11 9 2016].
[23] FireEye, "Angler Exploit Kit Evading EMET," [Online]. Available:
https://www.fireeye.com/blog/threat-research/2016/06/angler_exploit_kite.html. [Accessed
07 09 2016].
[24] The MITRE Corporation, "PowerShell - ATT&CK," July 2016. [Online]. Available:
https://attack.mitre.org/wiki/Technique/T1086. [Accessed 13 September 2016].
[25] Carbon Black, "‘PowerShell’ Deep Dive: A United Threat Research Report," 21 April
2016. [Online]. Available: https://www.carbonblack.com/wp-content/uploads/2016/04/Cb-
Powershell-Deep-Dive-A-United-Threat-Research-Report-1.pdf. [Accessed 18 September
2016].
36
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
[26] W. Schroeder, J. Warner and M. Nelson, "Empire," September 2016. [Online]. Available:
https://github.com/PowerShellEmpire/Empire. [Accessed 18 September 2016].
[27] Microsoft, "Sysmon," [Online]. Available: https://technet.microsoft.com/en-
us/sysinternals/sysmon. [Accessed 24 05 2016].
[28] Microsoft, "Autoruns for Windows v13.51," [Online]. Available:
https://technet.microsoft.com/en-us/sysinternals/bb963902. [Accessed 24 05 2016].
[29] Microsoft, "About Event Tracing," [Online]. Available: https://msdn.microsoft.com/en-
us/library/windows/desktop/aa363668(v=vs.85).aspx. [Accessed 24 05 2016].
[30] E. M. a. C. M. J. a. A. R. M. Hutchins, "Intelligence-driven computer network defense
informed by analysis of adversary campaigns and intrusion kill chains," Leading Issues in
Information Warfare & Security Research, vol. 1, p. 80, 2011.
37
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
This page intentionally left blank.
38
©2017 The MITRE Corporation. All rights reserved.
Approved for Public Release; Distribution Unlimited. Case Number 16-3713.
Appendix A Details on MITRE’s Implementation
This appendix contains information about the sensors, data, and analytics used within the
MITRE environment during its cyber games. The data was gathered from sensors on MITRE’s
living lab network. The data that was used was actual user data from real employee workstations.
This provided a real-life environment for testing analytics and sensors and for developing a true
feeling for noise and efficacy that would otherwise be impossible to ascertain in an artificial test
network.
A.1 Example Analytics
This section details some of the analytics that our Blue Team used to uncover Red Team activity.
Several exemplars are described below along with the analytic type for each (See Section 4.3). A
link is also provided to the public MITRE CAR page that references each analytic.
Analytic 1: Suspicious Commands
CAR URL: https://car.mitre.org/wiki/CAR-2013-04-002
Type: Behavioral
Data Required: Process Creation Events with Command Line Information
This analytic identifies sequences of executed processes that have been identified as suspicious.
In this context, suspicious refers to those processes that are built-in, freely available, or used by
legitimate administrators, but which are also known to be used by adversaries. This analytic
attempts to identify when groups of these processes are executed together based on several
criteria: the amount of time between each occurrence; the parent process; and the host. By
looking for groups of processes being executed simultaneously, the noise an analyst must sift
through is reduced, and valuable context is added to the event. Examples of these processes
include net.exe, reg.exe, dsquery.exe, and schtasks.exe. This analytic finds behavior that touches
on over 20 different ATT&CK TTPs. The following table lists only a few of the
tactics/techniques found by this analytic. The entire list can be viewed on the CAR site.