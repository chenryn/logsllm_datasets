of Internet links with high fidelity — including random loss, arrival,
and departures of new flows, application-level sending behaviors,
etc — is perhaps impossible to achieve perfectly [15, 49, 50]. Fur-
thermore, understanding the behaviors of CCAs can be challenging
in “real” settings where many uncontrolled variables combine to
influence CCA behavior. We instead opt to focus directly on only
two key variables: the number of concurrent flows (which increases
by two orders of magnitude between EdgeScale and CoreScale) and
the link capacity (which increases similarly between EdgeScale and
CoreScale). Therefore, when we say ‘at scale’, we refer to the setting
where the bottleneck bandwidth is 10 Gbps and the flow count
ranges from 1000 to 5000 flows. By controlling all other aspects of
the experiment (all flows have the same, lengthy duration; there is
no random loss; buffer sizes are approximately 1 BDP in both set-
tings; all flows have the same RTT etc.) we can more easily inspect
the impact of these two variables on CCA behavior.
99
10003000500002.557.510Packet Loss to CWND Halving RatioFlow CountPacket Loss to CWND Halving Ratio (%)10305002.557.510Packet Loss to CWND Halving RatioFlow CountPacket Loss to CWND Halving Ratio (%)10003000500000.250.50.75120ms100ms200msPast WorkJFIFlow CountJFI10305000.250.50.75120ms100ms200msPast WorkJFIFlow CountJFIRevisiting TCP Congestion Control Throughput Models & Fairness Properties At Scale
IMC ’21, November 2–4, 2021, Virtual Event, USA
Figure 5: Cubic takes 70% to 80% of to-
tal throughput when competing with
an equal number of NewReno flows in
CoreScale.
Figure 6: 1 BBR flow takes 40% of
total
throughput when competing
with thousands of NewReno flows in
CoreScale.
Figure 7: 1 BBR flow takes 40% of to-
tal throughput when competing with
thousands of Cubic flows in CoreScale.
Finding 3: The ratio between packet losses and congestion events
(i.e., CWND halvings) changes between CoreScale and EdgeScale,
and across across different flow counts within CoreScale. (Fig
3)
While investigating why the packet loss rate results in differ-
ent constants in EdgeScale vs CoreScale, we discovered the ratio
of packet loss rate to CWND halving rate is different in the two
settings. As seen in Fig 3, in EdgeScale, the ratio of packet losses
to CWND halvings is approximately 1.7 regardless of the number
of concurrent flows. But in CoreScale the ratio varies between 6
and 9 and depends on the flow count. This explains why using
packet loss rate results in different constants between EdgeScale
and CoreScale, and different constants within CoreScale at different
flow counts. While the idea that packet loss rate diverges from
CWND halving rate is not new [23, 37, 41], we believe the drastic
increase in divergence as we move from EdgeScale to CoreScale is a
new finding.
Since the ratio is stable for EdgeScale, there is no reason to doubt
past research that uses packet loss rate for 𝑝 when evaluating links
with tens of flows and only tens or hundreds of Mbps [44, 46].
However, our results show one should not use the packet loss rate for
estimating throughput over the Internet core.
We hypothesize that the reason for different packet loss rate to
CWND halving rate ratios is that losses are burstier at scale, causing
multiple losses in the same burst or RTT which result in only one
congestion window halving. We corroborate this hypothesis by
measuring the burstiness of losses at the queue using the Goh-
Barabasi burstiness score [24] which ranges from -1 to 1, where a
higher score means the drops are burstier. We obtain median values
close to 0.2 in EdgeScale and closer to 0.35 in CoreScale, implying
that losses are indeed burstier at scale (Figure not shown).
Implications: Overall, we find that the Mathis model for through-
put still holds in CoreScale, if 𝐶 is calculated using CWND halving
rate and not the more commonly used packet loss rate for the vari-
able 𝑝. Unfortunately, this makes applying the Mathis model in
practice more challenging, as obtaining the CWND halving rate re-
quires end-host state reconstruction, where packet loss rate can be
measured more easily via network-measurable loss. Furthermore,
our findings also change our expectations regarding NewReno’s
performance with respect to loss: a flow on a congested core link
can tolerate four times the packet loss rate of a flow on a congested
home link, and still obtain the same bandwidth because the CWND
halving rate is the same.
100
5 Revisiting Fairness
In this section, we measure how fairly competing flows share
bandwidth in our CoreScale setting.
5.1 Intra-CCA Fairness
Background: The classic metric used for measuring fairness is
Jain’s Fairness Index (JFI) [29], which ranges from 0 to 1 with a
higher value indicating greater fairness. Past research in the edge
setting has found Cubic, NewReno, and BBR to be intra-CCA fair –
i.e., fair when competing only with other flows of the same CCA
and RTT – with a JFI of 0.9 or more [18, 20, 26, 45, 52].
Finding 4: NewReno & Cubic continue to show high intra-CCA
fairness in CoreScale with a JFI > 0.99, as expected from past
research. (Figure not shown)
Both theoretical [20] and empirical studies [26, 39] have shown
that when NewReno flows compete with other NewReno flows, or
Cubic flows compete with other Cubic flows, throughput is shared
almost equally when all flows have the same RTT. Our experiments
confirm this in the CoreScale setting: NewReno and Cubic show
high fairness with a JFI > 0.99.
Finding 5: BBR surprisingly shows intra-CCA unfairness in
CoreScale, with JFIs as low as 0.4, which is not expected from
past research. Milder unfairness also occurs when more than
10 flows compete in EdgeScale, with JFI’s as low as 0.7. (Fig 4)
Fig 4 shows the JFI for BBR flows with the same RTT when they
compete amongst themselves at different flow counts. It also shows
the JFI based on results from past work (0.99) which finds BBR to be
intra-CCA fair when all flows have the same RTT [18, 28, 47, 52].
We see that at scale BBR surprisingly becomes unfair at 20ms
and 100ms RTTs, with the JFI going as low as 0.4. We investigate
further and discover that BBR shows signs of unfairness even in
EdgeScale, but at relatively higher flow counts (greater than 10) not
examined by past research. This unfairness is exacerbated at scale.
Cardwell et al. [18] argue that BBR flows, share bandwidth fairly
amongst each other at lower flow counts due to flow synchro-
nization. While we have not verified it, we hypothesize that the
unfairness in CoreScale might be due to BBR flows desynchronizing
at scale, similar to NewReno [13].
Implications: Prior work showed that BBR is unfair when com-
peting with other CCAs (e.g. Cubic, NewReno) – however, it was
assumed that if the entire Internet adopted BBR users could expect
fair outcomes. Our CoreScale experiments show that this is not the
case when thousands of flows compete in wide-area like settings;
100030005000025507510020ms100ms200msHome Linkcubic vs renoFlow CountCubicThroughput(%)100030005000020406020ms100ms200msHome Linkbbr vs renoFlow CountBBRThroughput(%)100030005000020406020ms100ms200msHome Linkbbr vs cubicFlow CountBBRThroughput(%)IMC ’21, November 2–4, 2021, Virtual Event, USA
Philip et al.
Past research [26, 39] expects Cubic flows to get around 80%
of total throughput when competing with an equal number of
NewReno flows in the edge setting. Our experiments show this
holds true even in CoreScale, as seen in Figure 5.
Implications: Our results confirm that the inter-CCA unfairness
displayed by Cubic to NewReno and BBR to both Cubic and NewReno
persist at higher flow counts at scale. In these settings, the dispar-
ities between flows can be even more extreme than at the edge
setting – with a single BBR flow attaining 4 Gbps and 5000 compet-
ing flows Reno or Cubic flows obtaining just 1.2 Mbps each. While
past work shows that a few ‘bad player’ flows can impact fairness
between a small number of users sharing an edge link (e.g. room-
mates in a shared house, co-workers in an office), the fact that prior
unfairness findings extend to CoreScale suggests severely unfair
outcomes where a single sender can impact thousands of physical
neighbors with whom he or she shares a large inter-domain link.
6 Conclusions
Conventional wisdom about congestion control adopted by ap-
plication and systems designers has been evaluated in settings
implicitly assuming congestion at the edge. When congestion oc-
curs in the core, as shown by many measurements, it is not clear
if these accepted norms about throughput and fairness still hold.
We revisit these and find that the widely accepted Mathis model
can be applied using either loss or congestion window halving in
edge settings, but these metrics diverge at scale. Similarly, we find
that when BBR competes with other BBR flows, it goes from being
completely fair in the edge setting to completely unfair at scale.
We hope that our work, though preliminary, serves as a cautionary
tale for naïvely extrapolating CCA properties at scale and inspires
further modeling and analysis.
7 Acknowledgements
We would like to thank our shepherd Balakrishnan Chandrasekaran
and the anonymous reviewers for their comments and help revising
the paper. We would also like to thank Hugo Sadok, Christopher
Canel, Nirav Atre, Anup Agarwal, and Srinivasan Seshan for their
valuable feedback. This research is supported in part by NSF Grant
No. 1850384 and the vmWare Systems Research Award.
References
[1] 2020. htsim. https://nrg.cs.ucl.ac.uk/mptcp/implementation.html. Accessed:
[2] 2021. BBR v2: A Model-based Congestion Control. https://datatracker.ietf.org/
meeting/104/materials/slides-104-iccrg-an-update-on-bbr-00. Accessed: 2021-
03-04.
[3] 2021. BESS: A Software Switch. https://github.com/NetSys/bess. Accessed:
2020-03-12.
2021-03-04.
2021-03-04.
2021-03-04.
2021-09-10.
[4] 2021. Google LLC Peering Data. https://www.peeringdb.com/net/433. Accessed:
[5] 2021. Netflix falls to second place in global internet traffic share as other streaming
services grow. https://www.sandvine.com/inthenews/netflix-falls-to-second-
place-in-global-internet-traffic-share. Accessed: 2021-03-04.
[6] 2021. The Network Simulator - ns-2. https://www.isi.edu/nsnam/ns/. Accessed:
[7] 2021. NewReno RFC History. https://www.rfc-editor.org/search/rfc_search_
detail.php?title=NewReno. Accessed: 2021-03-04.
[8] 2021.
tc-netem(8) - Linux Manual Page. https://man7.org/linux/man-pages/
man8/tc-netem.8.html. Accessed: 2021-03-04.
[9] 2021. tcpprobe. https://wiki.linuxfoundation.org/networking/tcpprobe Accessed
(a) NewReno
(b) Cubic
Figure 8: BBR takes 99.9% of total throughput when compet-
ing with an equal number of NewReno or Cubic flows.
this emphasizes the need for CCA testing and evaluation at scale to
understand whether a new algorithm is acceptable for deployment.
5.2 Inter-CCA Fairness
In this section, we evaluate how flows from different CCAs with
the same RTT compete with each other.
Background: Past research in the edge link setting found that
Cubic competes unfairly with NewReno, taking up to 80% of to-
tal throughput [26, 39] and that BBR is unfair to both Cubic and
NewReno [28, 45, 48]. We revisit these properties at scale.
For the following results, we measure the aggregate through-
put obtained by the flows of one CCA as a fraction of the total
throughput obtained by all flows.
Finding 6: A single BBR flow takes 40% of total throughput
when competing with thousands of NewReno or Cubic flows