### Optimized Text

Achieving a high-fidelity simulation of Internet links, including random loss, flow arrivals and departures, and application-level sending behaviors, is nearly impossible to perfect [15, 49, 50]. Additionally, understanding the behavior of congestion control algorithms (CCAs) in real-world settings, where numerous uncontrolled variables influence their performance, can be challenging. To simplify our analysis, we focus on two key variables: the number of concurrent flows (which increases by two orders of magnitude from EdgeScale to CoreScale) and the link capacity (which also increases significantly between these two scales). In this context, "at scale" refers to a setting where the bottleneck bandwidth is 10 Gbps and the number of flows ranges from 1,000 to 5,000. By controlling all other aspects of the experiment—such as ensuring all flows have the same, long duration; no random loss; buffer sizes approximately equal to one bandwidth-delay product (BDP); and identical round-trip times (RTTs)—we can more easily examine the impact of these two variables on CCA behavior.

**Finding 3: The ratio between packet losses and congestion events (i.e., CWND halvings) varies between CoreScale and EdgeScale, and across different flow counts within CoreScale.**

While investigating why the packet loss rate results in different constants in EdgeScale versus CoreScale, we discovered that the ratio of packet loss rate to CWND halving rate differs in the two settings. As shown in Figure 3, in EdgeScale, the ratio of packet losses to CWND halvings is approximately 1.7, regardless of the number of concurrent flows. In contrast, in CoreScale, the ratio varies between 6 and 9, depending on the flow count. This explains why using the packet loss rate results in different constants between EdgeScale and CoreScale, and different constants within CoreScale at different flow counts. Although the idea that packet loss rate diverges from CWND halving rate is not new [23, 37, 41], the significant increase in divergence as we move from EdgeScale to CoreScale is a novel finding.

Since the ratio is stable for EdgeScale, there is no reason to doubt past research that uses packet loss rate for \( p \) when evaluating links with tens of flows and only tens or hundreds of Mbps [44, 46]. However, our results indicate that one should not use the packet loss rate for estimating throughput over the Internet core.

We hypothesize that the difference in packet loss rate to CWND halving rate ratios is due to burstier losses at scale, causing multiple losses in the same burst or RTT, which result in only one congestion window halving. We support this hypothesis by measuring the burstiness of losses at the queue using the Goh-Barabasi burstiness score [24], which ranges from -1 to 1, with higher scores indicating more bursty losses. We find median values close to 0.2 in EdgeScale and closer to 0.35 in CoreScale, suggesting that losses are indeed more bursty at scale (data not shown).

**Implications:**
Overall, we find that the Mathis model for throughput still holds in CoreScale if \( C \) is calculated using the CWND halving rate rather than the more commonly used packet loss rate for the variable \( p \). Unfortunately, this makes applying the Mathis model in practice more challenging, as obtaining the CWND halving rate requires end-host state reconstruction, while packet loss rate can be measured more easily via network-measurable loss. Furthermore, our findings change our expectations regarding NewReno’s performance with respect to loss: a flow on a congested core link can tolerate four times the packet loss rate of a flow on a congested home link and still achieve the same bandwidth because the CWND halving rate is the same.

### Revisiting Fairness

In this section, we measure how fairly competing flows share bandwidth in our CoreScale setting.

#### 5.1 Intra-CCA Fairness

**Background:**
The classic metric for measuring fairness is Jain's Fairness Index (JFI) [29], which ranges from 0 to 1, with higher values indicating greater fairness. Past research in the edge setting has found Cubic, NewReno, and BBR to be intra-CCA fair—i.e., fair when competing only with other flows of the same CCA and RTT—with a JFI of 0.9 or more [18, 20, 26, 45, 52].

**Finding 4: NewReno and Cubic continue to show high intra-CCA fairness in CoreScale with a JFI > 0.99, as expected from past research.**

Both theoretical [20] and empirical studies [26, 39] have shown that when NewReno flows compete with other NewReno flows, or Cubic flows compete with other Cubic flows, throughput is shared almost equally when all flows have the same RTT. Our experiments confirm this in the CoreScale setting: NewReno and Cubic show high fairness with a JFI > 0.99.

**Finding 5: BBR surprisingly shows intra-CCA unfairness in CoreScale, with JFIs as low as 0.4, which is not expected from past research. Milder unfairness also occurs when more than 10 flows compete in EdgeScale, with JFIs as low as 0.7.**

Figure 4 shows the JFI for BBR flows with the same RTT when they compete amongst themselves at different flow counts. It also compares these results with past work (JFI = 0.99), which found BBR to be intra-CCA fair when all flows have the same RTT [18, 28, 47, 52]. At scale, BBR becomes unfair at 20ms and 100ms RTTs, with the JFI dropping as low as 0.4. Further investigation reveals that BBR shows signs of unfairness even in EdgeScale, but at relatively higher flow counts (greater than 10) not examined by past research. This unfairness is exacerbated at scale.

Cardwell et al. [18] argue that BBR flows share bandwidth fairly among each other at lower flow counts due to flow synchronization. While we have not verified it, we hypothesize that the unfairness in CoreScale might be due to BBR flows desynchronizing at scale, similar to NewReno [13].

**Implications:**
Prior work showed that BBR is unfair when competing with other CCAs (e.g., Cubic, NewReno). However, it was assumed that if the entire Internet adopted BBR, users could expect fair outcomes. Our CoreScale experiments show that this is not the case when thousands of flows compete in wide-area-like settings.

#### 5.2 Inter-CCA Fairness

In this section, we evaluate how flows from different CCAs with the same RTT compete with each other.

**Background:**
Past research in the edge link setting found that Cubic competes unfairly with NewReno, taking up to 80% of total throughput [26, 39], and that BBR is unfair to both Cubic and NewReno [28, 45, 48]. We revisit these properties at scale.

For the following results, we measure the aggregate throughput obtained by the flows of one CCA as a fraction of the total throughput obtained by all flows.

**Finding 6: A single BBR flow takes 40% of total throughput when competing with thousands of NewReno or Cubic flows.**

This finding emphasizes the need for CCA testing and evaluation at scale to understand whether a new algorithm is acceptable for deployment.