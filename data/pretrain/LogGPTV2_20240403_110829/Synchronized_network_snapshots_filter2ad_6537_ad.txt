$%&#’%(
$%&#’%(
!"#"
!"#"
Computational Resources
Stateless ALUs
Stateful ALUs
Control Flow Resources
Logical Table IDs
Conditional Table Gateways
Physical Stages
Memory Resources
17
9
27
15
10
19
9
35
19
10
24
11
37
19
12
SRAM
TCAM
606 KB
42 KB
671 KB
59 KB
770 KB
244 KB
Table 1: Resource usage for the Speedlight data plane
on the To!no. Numbers are for a snapshot of per-port
packet counters and 64 ports.
Resource requirements for Speedlight increase with the
use of wraparound and channel state, features that require
more complex logic. Memory requirements also grow with
the number of ports in the snapshot, as the data plane must
allocate larger register arrays and tables to store and address
the per-port statistics. The con!guration shown in Table 1 is
for 64 port snapshots, the maximum number of ports that
a single processing engine in the Wedge100BF’s To!no can
support. A con!guration with wraparound and channel state
for 14 port snapshots, as used for evaluation in Section 8,
requires 638 KB of SRAM and 90 KB of TCAM.
7.2 Control Plane
We wrote the snapshot control plane in Python (∼2000 lines
of code) and ran it on the switch CPU, which has a PCIe-3.0
X4 link to the To!no ASIC. The control plane uses a compiler
generated Thrift API to initialize tables, set up mirroring,
and poll register arrays. Time synchronization was done via
ptp4l and phc2sys.
The snapshot control plane receives noti!cations from
the To!no using a raw socket implemented by a kernel-
level DMA packet driver. It listens for noti!cations, which
trigger its main event handler as depicted in Figure 7. There
are alternatives to this approach, e.g., a P4 digest stream,
but we found that raw sockets made the implementation
straightforward and o"ered signi!cantly better performance.
8 EVALUATION
We evaluated Speedlight in a hardware testbed and used it to
perform measurement campaigns that study widely used dis-
tributed applications and protocols. Our testbed consists of
a Barefoot Wedge100BF-32X programmable switch with 128
25 GbE ports connected to six servers with Intel(R) Xeon(R)
Silver 4110 CPUs via 25 GbE links. We emulated a small leaf-
spine topology in our testbed, as depicted in Figure 8. We
$%&#’%(
$%&#’%(
$%&#’%(
$%&#’%(
!"#"
)-.#/0
!"#"
)*’+*’,
Figure 8: Depiction of our testbed topology.
did this by splitting the 128 port switch into 4 fully isolated
logical switches with lower fan-outs.
As in a real deployment, the virtual switches were con-
nected with 100 GbE passive copper links. At the data plane,
all forwarding tables were replicated for each virtual switch.
At the control plane, we ran duplicate versions of the pro-
tocol. To emulate clock drift between switch control planes,
snapshots were initiated based on the local system clock of
four distinct PTP-synchronized servers. With the inclusion
of network latency, our synchronization numbers therefore
represent an upper bound.
To load balance tra#c along the multiple paths in our
testbed, we implemented two di"erent algorithms alongside
the snapshot logic in the switch data plane ASIC: ECMP [16]
and $owlet switching [20].
Workload. We used three distributed applications in our
testbed. The !rst is Hadoop running a Terasort [4] bench-
mark workload with 5B rows of data. Our Hadoop instance
ran version 2.9.0 with YARN [5] on 10 mappers and 8 re-
ducers. The second is Spark’s GraphX [7] running a PageR-
ank [6] synthetic benchmark workload with 100,000 ver-
tices. Our Spark instance ran version of 2.2.1 with Yarn on
5 servers. Finally, we implemented memcache [3], running
an mc-crusher 50-key multi-get workload [13]. We popu-
lated the Hadoop and memcache instances with data during
a setup phase that was not measured.
Counters. We implemented a variety of performance coun-
ters including per-port packet and byte counters along with
queue depth measurements. However, in this section we pri-
marily focus on an exponentially-weighted moving average
(EWMA) of packet interarrival time. The EWMA counter
was implemented in two phases due to hardware limitations
on register computation:
interarrival = pkt_timestamp - last_ts[port]
last_ts[port] = pkt_timestamp
if packet_count[port] is even:
temp_ewma[port] += interarrival
else:
temp_ewma[port] /= 2
ewma[port] /= temp_ewma[port]
Underlined variables are implemented with stateful registers.
The EWMA updates on every other packet with the average
411
Synchronized Network Snapshots
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 1
Switch State
Switch + Channel State
Polling
)
z
H
(
e
t
a
R
m
u
m
x
a
M
i
 10000
 1000
 100
 10
 10
 100
 1000
 10000
4
8
16
32
64
Synchronization (us)
# of Ports/Router
Figure 9: Synchronization of network-wide measure-
ments using snapshots and traditional polling.
interarrival of the last two packets. As shown in the code,
our implementation is functionally equivalent to an EWMA
with a decay factor of .5.
8.1 Synchronization of Network Snapshots
We begin by evaluating the synchronization properties of
Speedlight. For this, we con!gured processing units to tag
snapshot noti!cations with the current timestamp. Recall
that noti!cations are sent on any update of either the local
snapshot ID or the last seen array, i.e., on any progress in the
algorithm. In the experiment, we sent a command to each
of the four virtual control planes in our testbed to schedule
a snapshot. At the scheduled time, they sent initiations to
every processing unit (ingress and egress) under their control
as described in Section 6. Synchronization of a snapshot ID
is de!ned as the di"erence between the earliest and latest
timestamps on any noti!cation with that ID.
Figure 9 shows a CDF of synchronization for three di"er-
ent approaches: (1) traditional counter polling, (2) Speedlight
w/o channel state, and (3) Speedlight w/ channel state. In
both con!gurations of Speedlight, median synchronization
was ∼6.4 µs. The maximum synchronization delta we ob-
served was 22 µs w/o channel state, and 27 µs w/ channel
state, likely due to randomness in PTP, queuing, and sched-
uling. These values are well-within a single RTT for most
networks. As one might expect, channel state synchroniza-
tion has a longer tail as completion depends on all upstream
neighbors advancing to the current snapshot.
For comparison, we also measured the synchronization
of a typical counter polling framework where an observer
polls the statistic for each port individually via a control
plane agent that reads and returns the value on-demand. For
a full sequence of network-wide measurements, the median
di"erence between the !rst and last poll was 2.6 ms.
8.2 Scalability of Speedlight
We also evaluate how Speedlight scales with the size and com-
plexity of the network. In particular, we ask two questions:
(1) how does the scale of the network a"ect the frequency
with which Speedlight can take snapshots, and (2) how does
Figure 10: Max. sustained snapshot rate before noti!-
cation queue buildup. Results are shown for a range
of router port counts and assume no channel state.
)
s
u
(
n
o
i
t
i
a
z
n
o
r
h
c
n
y
S
 100
 80
 60
 40
 20
 0
10
100
1000
10000
Number of Routers
Figure 11: Average synchronization of Speedlight
snapshots in larger network deployments. The snap-
shot assumes 64-port routers and no channel state.
the scale a"ect the time synchronization of those snapshots.
Storage scalability was brie#y addressed in Section 7.1.
Speedlight’s architecture lends itself well to scalability;
control planes are responsible for their own switch, and each
processing unit has at most one external neighbor regardless
of how many routers are added to the network. Instead, the
primary factor in performance is number of ports per router.
Figure 10 shows the maximum sustained snapshot fre-
quency versus router port count. In the experiment, we ini-
tiated a series of snapshots on a single switch with !xed
interval. Snapshot frequencies that were too high eventually
resulted in noti!cation drops. The graphs plot the highest
frequency without drops. Even for 64 ports (a full linecard),
Speedlight can sustain over 70 snapshots per second. Note
that the ASIC-CPU channel is more than su$cient; rather,
the bottleneck is in our unoptimized control plane processing
latency. Thus, Speedlight supports bursts of higher frequency
snapshots given a su$ciently large socket receive bu"er.
Network size primarily a"ects Speedlight’s synchroniza-
tion. Figure 11 shows average whole-network synchroniza-
tion for several large simulated networks. Our simulation
included PTP time drift, OpenNetworkLinux scheduling ef-
fects, and the latency between initiation and data plane snap-
shot execution. Distributions for all of these values were
collected from our hardware testbed. While Speedlight’s
multi-initiator design limits time drift, additional routers
412
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Nofel Yaseen, John Sonchack, and Vincent Liu
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
F
D
C
ECMP Polling
ECMP Snapshots
Flowlet Polling
Flowlet Snapshots
 50
 100
 150
 200
 250
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
F
D
C
ECMP Polling
ECMP Snapshots
Flowlet Polling
Flowlet Snapshots
 10
 20
 30
 40
 50
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
ECMP Polling
ECMP Snapshots
Flowlet Polling
Flowlet Snapshots
 20
 40
 60
 80
 100
Standard Deviation (ms)
Standard Deviation (ms)
Standard Deviation (us)
(a) Hadoop
(b) GraphX
(c) Memcache
Figure 12: Standard deviation of uplink load balancing in our leaf-spine topology. We compared two approaches:
!ow-based ECMP and !owlet load balancing. We tested Hadoop, GraphX, and memcache as well as polling versus
snapshots. Note the di"erence in units on the x-axis.
and ports can make encountering tail e!ects more likely;
however, this e!ect is asymptotic and still stays under typi-
cal RTTs.
8.3 Use Case: Evaluating Load Balancing
We began this paper with a running example of a question