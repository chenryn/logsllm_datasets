### Computational Resources

| Resource Type | Stateless ALUs | Stateful ALUs | Control Flow Resources | Logical Table IDs | Conditional Table Gateways | Physical Stages | Memory Resources (SRAM) | Memory Resources (TCAM) |
|---------------|----------------|---------------|------------------------|-------------------|----------------------------|-----------------|--------------------------|-------------------------|
| Configuration 1 | 17             | 9             | 27                     | 15                | 10                         | 19              | 606 KB                   | 42 KB                   |
| Configuration 2 | 35             | 19            | 10                     | 24                | 11                         | 37              | 671 KB                   | 59 KB                   |
| Configuration 3 | 19             | 12            | 19                     | 37                | 11                         | 37              | 770 KB                   | 244 KB                  |

**Table 1: Resource usage for the Speedlight data plane on the Tofino. Numbers are for a snapshot of per-port packet counters and 64 ports.**

### Resource Requirements
The resource requirements for Speedlight increase with the use of features such as wraparound and channel state, which require more complex logic. Memory requirements also grow with the number of ports in the snapshot, as the data plane must allocate larger register arrays and tables to store and address per-port statistics. The configuration shown in Table 1 is for 64-port snapshots, the maximum number of ports that a single processing engine in the Wedge100BF’s Tofino can support. A configuration with wraparound and channel state for 14-port snapshots, as used for evaluation in Section 8, requires 638 KB of SRAM and 90 KB of TCAM.

### 7.2 Control Plane
We wrote the snapshot control plane in Python (approximately 2000 lines of code) and ran it on the switch CPU, which has a PCIe-3.0 X4 link to the Tofino ASIC. The control plane uses a compiler-generated Thrift API to initialize tables, set up mirroring, and poll register arrays. Time synchronization was achieved using ptp4l and phc2sys.

The snapshot control plane receives notifications from the Tofino using a raw socket implemented by a kernel-level DMA packet driver. It listens for notifications, which trigger its main event handler as depicted in Figure 7. Although there are alternatives, such as a P4 digest stream, we found that raw sockets made the implementation straightforward and offered significantly better performance.

### 8 Evaluation
We evaluated Speedlight in a hardware testbed and used it to perform measurement campaigns that study widely used distributed applications and protocols. Our testbed consists of a Barefoot Wedge100BF-32X programmable switch with 128 25 GbE ports connected to six servers with Intel(R) Xeon(R) Silver 4110 CPUs via 25 GbE links. We emulated a small leaf-spine topology in our testbed, as depicted in Figure 8, by splitting the 128-port switch into four fully isolated logical switches with lower fan-outs.

In a real deployment, the virtual switches were connected with 100 GbE passive copper links. At the data plane, all forwarding tables were replicated for each virtual switch. At the control plane, we ran duplicate versions of the protocol. To emulate clock drift between switch control planes, snapshots were initiated based on the local system clock of four distinct PTP-synchronized servers. With the inclusion of network latency, our synchronization numbers represent an upper bound.

To load balance traffic along the multiple paths in our testbed, we implemented two different algorithms alongside the snapshot logic in the switch data plane ASIC: ECMP [16] and flowlet switching [20].

#### Workload
We used three distributed applications in our testbed:
1. Hadoop running a Terasort [4] benchmark workload with 5B rows of data. Our Hadoop instance ran version 2.9.0 with YARN [5] on 10 mappers and 8 reducers.
2. Spark’s GraphX [7] running a PageRank [6] synthetic benchmark workload with 100,000 vertices. Our Spark instance ran version 2.2.1 with YARN on 5 servers.
3. Memcache [3], running an mc-crusher 50-key multi-get workload [13]. We populated the Hadoop and memcache instances with data during a setup phase that was not measured.

#### Counters
We implemented a variety of performance counters, including per-port packet and byte counters, along with queue depth measurements. In this section, we primarily focus on an exponentially-weighted moving average (EWMA) of packet interarrival time. The EWMA counter was implemented in two phases due to hardware limitations on register computation:

```python
interarrival = pkt_timestamp - last_ts[port]
last_ts[port] = pkt_timestamp
if packet_count[port] % 2 == 0:
    temp_ewma[port] += interarrival
else:
    temp_ewma[port] /= 2
ewma[port] = temp_ewma[port]
```

Underlined variables are implemented with stateful registers. The EWMA updates on every other packet with the average interarrival of the last two packets. As shown in the code, our implementation is functionally equivalent to an EWMA with a decay factor of 0.5.

### 8.1 Synchronization of Network Snapshots
We begin by evaluating the synchronization properties of Speedlight. For this, we configured processing units to tag snapshot notifications with the current timestamp. Notifications are sent on any update of either the local snapshot ID or the last seen array, i.e., on any progress in the algorithm. In the experiment, we sent a command to each of the four virtual control planes in our testbed to schedule a snapshot. At the scheduled time, they sent initiations to every processing unit (ingress and egress) under their control as described in Section 6. Synchronization of a snapshot ID is defined as the difference between the earliest and latest timestamps on any notification with that ID.

Figure 9 shows a CDF of synchronization for three different approaches: (1) traditional counter polling, (2) Speedlight without channel state, and (3) Speedlight with channel state. In both configurations of Speedlight, median synchronization was approximately 6.4 µs. The maximum synchronization delta observed was 22 µs without channel state and 27 µs with channel state, likely due to randomness in PTP, queuing, and scheduling. These values are well within a single RTT for most networks. As expected, channel state synchronization has a longer tail as completion depends on all upstream neighbors advancing to the current snapshot.

For comparison, we also measured the synchronization of a typical counter polling framework where an observer polls the statistic for each port individually via a control plane agent that reads and returns the value on-demand. For a full sequence of network-wide measurements, the median difference between the first and last poll was 2.6 ms.

### 8.2 Scalability of Speedlight
We also evaluate how Speedlight scales with the size and complexity of the network. Specifically, we ask two questions:
1. How does the scale of the network affect the frequency with which Speedlight can take snapshots?
2. How does the scale affect the time synchronization of those snapshots?

Storage scalability was briefly addressed in Section 7.1. Speedlight’s architecture lends itself well to scalability; control planes are responsible for their own switch, and each processing unit has at most one external neighbor regardless of how many routers are added to the network. Instead, the primary factor in performance is the number of ports per router.

Figure 10 shows the maximum sustained snapshot frequency versus router port count. In the experiment, we initiated a series of snapshots on a single switch with a fixed interval. Snapshot frequencies that were too high eventually resulted in notification drops. The graphs plot the highest frequency without drops. Even for 64 ports (a full linecard), Speedlight can sustain over 70 snapshots per second. Note that the ASIC-CPU channel is more than sufficient; rather, the bottleneck is in our unoptimized control plane processing latency. Thus, Speedlight supports bursts of higher frequency snapshots given a sufficiently large socket receive buffer.

Network size primarily affects Speedlight’s synchronization. Figure 11 shows average whole-network synchronization for several large simulated networks. Our simulation included PTP time drift, OpenNetworkLinux scheduling effects, and the latency between initiation and data plane snapshot execution. Distributions for all of these values were collected from our hardware testbed. While Speedlight’s multi-initiator design limits time drift, additional routers and ports can make encountering tail effects more likely; however, this effect is asymptotic and still stays under typical RTTs.

### 8.3 Use Case: Evaluating Load Balancing
We began this paper with a running example of a question related to load balancing. We compared two approaches: flow-based ECMP and flowlet load balancing. We tested Hadoop, GraphX, and memcache, as well as polling versus snapshots. Note the difference in units on the x-axis.

**Figure 12: Standard deviation of uplink load balancing in our leaf-spine topology. We compared two approaches: flow-based ECMP and flowlet load balancing. We tested Hadoop, GraphX, and memcache, as well as polling versus snapshots. Note the difference in units on the x-axis.**