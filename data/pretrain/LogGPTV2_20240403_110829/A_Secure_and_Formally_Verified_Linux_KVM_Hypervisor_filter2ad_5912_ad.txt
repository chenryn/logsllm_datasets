prove the tree is equivalent to storing page table entries in an array
indexed by gfn. To verify huge page support, our proof additionally
involves considering four possible invariant combinations: (1)
changing a regular page mapping does not affect any regular page
mappings; (2) changing a regular page mapping does not affect any
huge page mappings; (3) changing a huge page mapping does not
affect any regular page mappings; and (4) changing a huge page
mapping does not affect any huge page mappings. The basic idea
behind the proof reduces the problem of the multiple page sizes
to dealing with just the one 4KB page size by treating the 2MB
huge page as the equivalent of 512 4KB pages. Overall, the page
table refinement proofs required 4 layers.
5) Memory protection: KCore tracks metadata for each physical
1789
page, including its ownership and sharing status, in an S2Page data
structure, similar to the page structure in Linux. KCore maintains
a global S2Page array for all valid physical memory to translate
from a pfn to an S2Page. This can be specified as an abstract
pfn(cid:55)→ (owner, share, gfn) mapping, where owner designates the
owner of the physical page, which can be KCore, KServ, or a VM,
and share records whether a page is intentionally shared between
KServ and a VM.
Many of the functions involved in memory protection to
virtualize memory and enforce memory isolation involve nested
locks. Transparent trace refinement enables us to prove that the
implementation with nested locks refines an atomic step, by starting
with the code protected by the inner most nested locks and proving
that it refines an atomic step, then successively proving this while
moving out to the next level of nested locks until it is proven for all
nested locks. The refinement proofs for memory protection required
5 layers, including 3 layers for the S2Page specification.
6) SMMU: KCore manages the SMMU and its page tables to
provide DMA protection. We changed KVM’s SMMU page table
walk implementation to an iterative one to eliminate recursion, which
is not supported by our layered verification approach, since each
layer can only call functions in a lower layer. This is not a significant
issue for hypervisor or kernel code, in which recursion is generally
avoided due to limited kernel stack space. As with KVM, SMMU
page tables for a device assigned to a VM do not change after the
VM boots, but can be updated before the VM boots. We leverage
transparent trace refinement to prove the page table implementation
refines its specification to account for pre-VM boot updates. These
proofs required 9 layers: 4 for SMMU page tables, and 5 for SMMU
hypercalls and handling SMMU accesses.
7) Hypercalls: On top of all above modules, KCore implements
hypercall dispatchers and trap handlers with 5 layers. As shown in
Table I, the top layer specification refined by KCore provides 20
hypercalls and 5 fault handlers. It also exposes basic memory opera-
tions like mem_load and mem_store passed through from the bot-
tom layer, to model the memory access behavior of KServ and VMs.
B. Formulating Noninterference over TrapHandler
Because of security-preserving layers, noninterference only
needs to be formulated and proven over TrapHandler, the top-level
specification of KCore. We define the PDL V for each principal
based on policy vmdataiso and vm-iodataiso. V(σ, p) on a given
CPU c contains all of its private data, including (1) CPU c’s
registers if p is active on c, (2) p’s saved execution (register) context,
and (3) contents of the memory pages owned by p, which we call
the address space of p, including those mapped to p’s stage 2 and
SMMU page tables. V for a VM also contains metadata which can
affect its private data, including the sharing status of its own memory
pages. V for KServ also contains metadata, including ownership
metadata of all memory pages, VM execution metadata, and SMMU
configuration. We then use per-principal data oracles to model the
only three types of non-private data that a VM may intentionally
release according to policy data-declassification: (1) data that
VMs explicitly share and retain using the grant and revoke
hypercalls, (2) MMIO data that KServ copies from/into VM
GPRs, used for hardware device emulation, and (3) VCPU power
state requests that VMs explicitly make using the psci_power
3 through query moves, then σ4 and σ(cid:48)
Fig. 6: Noninterference proof for a VM NPT page fault. Upon a page
fault, VM p calls vm_page_fault in two indistinguishable states (σ1
and σ(cid:48)
1) and traps to KServ, transitioning from active to inactive in both
executions. By Lemmas 4 and 1, σ2 and σ(cid:48)
2 are indistinguishable. As
KServ performs page allocation, the executions respectively reach states
σ3 and σ(cid:48)
4 through CPU-local
moves. By Lemma 4, σ3 and σ(cid:48)
3 are indistinguishable. By Lemma 2,
since VM p, and all other VMs, remain inactive in states σ3 and σ(cid:48)
3, (1)
σ3 and σ4 are indistinguishable in the first execution, and (2) σ(cid:48)
3 and
σ(cid:48)
4 are indistinguishable in the second; transitively, σ4 and σ(cid:48)
4 are also
indistinguishable. Finally, KServ invokes run_vcpu, transitioning VM p
from inactive to active; the executions reach states σ5 and σ(cid:48)
5, respectively.
By Lemmas 4 and 3, σ5 and σ(cid:48)
5 are indistinguishable.
hypercall to control their own power management [40]. We also use
data oracles to model data intentionally released by KServ when
providing its functionality: (1) physical page indices proposed by
KServ for page allocation, written to the page table of a faulting
VM, (2) contents of pages proposed by KServ for page allocation,
and (3) scheduling decisions made by KServ. Intuitively, if KServ
has no private VM data, any release of information by KServ cannot
contain such data and can therefore be modeled using data oracles.
We prove noninterference by proving the lemmas from
Section III-B, for the primitives shown in Table I, with respect
to the PDL for each principal. Data oracles are used for proving
noninterference lemmas for the grant and revoke hypercalls,
for vm_page_fault when it is called due to MMIO accesses, and
for hypercalls such as run_vcpu which may declassify data from
KServ to a VM. Our proofs not only verify that the execution
of a principal does not interfere with another’s data, but that one
principal does not interfere with another’s metadata. For instance,
we proved that a VM cannot interfere with KServ’s decision about
VM scheduling, or affect the memory sharing status of another VM.
Furthermore, since we prove that there is no unintentional release
of private VM data to KServ, we also prove that any release of
information by KServ cannot contain such data.
For example, Figure 6 shows how we prove noninterference for
the big step execution of a stage 2 page fault caused by the need
to allocate a physical page. VM p causes KCore’s vm_page_fault
trap handler to run, KCore then switches execution to KServ to
perform page allocation, then KServ calls the run_vcpu hypercall
to cause KCore to switch execution back to the VM with the newly
allocated page. We want to use the lemmas in Section III-B, but
we must first prove them for each primitive. We briefly describe
the proofs, but omit details due to space constraints.
We prove Lemmas 1 to 3 for vm_page_fault, which saves
VM p’s execution context and switches execution to KServ. For
Lemma 1, an active principal must be the VM p. Starting from two
indistinguishable states for VM p, the local CPU’s registers must be
for VM p since it is active and running on the CPU, and must be the
1790
same in two executions. vm_page_fault in two executions will
therefore save the same execution context for VM p; so Lemma 1
holds. For Lemma 2, an inactive principal q must be a VM other
than p. q’s PDL will not be changed by VM p’s page fault, which
only modifies VM p’s execution context and the local CPU’s
registers; so Lemma 2 holds. For Lemma 3, only KServ can become
active after executing vm_page_fault. Its PDL will then include
the local CPU’s registers after the execution. Since KServ’s saved
execution context must be the same in indistinguishable states, the
restored registers will then remain the same; so Lemma 3 holds.
We prove Lemmas 1 to 3 for KServ’s page allocation, which
involves KServ doing mem_load and mem_store operations in its
address space, assuming no KServ stage 2 page faults for brevity.
For Lemma 1, KServ is the only principal that can be active and
executes the CPU-local move, which consists of KServ determining
what page to allocate to VM p. Starting from two indistinguishable
states for KServ, the same set of memory operations within KServ’s
address space will be conducted and the same page index will be
proposed in two executions, so Lemma 1 holds. For Lemma 2, all
VMs are inactive. We prove an invariant for page tables stating
that any page mapped by a principal’s stage 2 page table must be
owned by itself. Since each page has at most one owner, page tables,
and address spaces, are isolated. With this invariant, we prove that
VMs’ states are not changed by KServ’s operations on KServ’s
own address space; so Lemma 2 holds. Lemma 3 does not apply
in this case since KServ’s operations will not make any VM active.
We prove Lemmas 1 to 3 for run_vcpu. For Lemma 1, KServ
is the only principal that can invoke run_vcpu as a CPU-local
move, which consists of KCore unmapping the allocated page from
KServ’s stage 2 page table, mapping the allocated page to VM p’s
stage 2 page table, and saving KServ’s execution context so it can
switch execution back to VM p. Starting from two indistinguishable
states for KServ, run_vcpu in two executions will transfer the
same page from KServ’s page table to VM p’s page table. KServ’s
resulting address spaces remain indistinguishable as the same page
will be unmapped from both address spaces. For Lemma 2, If a
principal q stays inactive during the run_vcpu CPU-local move,
it must not be VM p. By the page table isolation invariant, the
transferred page cannot be owned by q since it is initially owned
by KServ, and such a transfer will not affect q’s address space. For
Lemma 3, VM p is the only inactive principal that becomes active
during the run_vcpu CPU-local move. Thus, the page will be
transferred to p’s address space. Starting from two indistinguishable
states for p, the page content will be masked with the same data
oracle query results. p’s resulting address spaces remain the same
and indistinguishable to p, so Lemma 3 holds.
By proving Lemmas 1 to 3 for all primitives, Lemma 4 holds for
each primitive based on rely-guarantee reasoning. We can then use
the proven lemmas to complete the indistinguishability proof for the
big step execution of the stage 2 page fault, as shown in Figure 6.
vm_page_fault. Lemmas 4 and 1 are then used to show that indistin-
guishability is preserved between two different executions of VM p.
Lemmas 4 and 2 are used to show indistinguishability as KServ runs.
Finally, KServ calls run_vcpu. On an MMIO read, KServ passes
the read data to KCore so KCore can copy to VM p’s GPR, which we
model using VM p’s data oracle in proving Lemma 3 for run_vcpu.
Lemmas 4 and 3 are then used to show that indistinguishability
holds as KCore switches back to running the VM.
C. Verified Security Guarantees of SeKVM
Our noninterference proofs over TrapHandler guarantee that
KCore protects the confidentiality and integrity of VM data against
both KServ and other VMs, and therefore hold for all of SeKVM.
Confidentiality. We show that the private data of VM p cannot
be leaked to an adversary. This is shown by noninterference
proofs that any big step executed by VM p cannot break the state
indistinguishability defined using the PDL of KServ or any other
VM. There is no unintentional information flow from VM p’s private
data to KServ or other VMs.
Integrity. We show that the private data of VM p cannot be
modified by an adversary. This is shown by noninterference proofs
that any big step executed by KServ or other VMs cannot break the
state indistinguishability defined using the PDL of VM p. VM p’s
private data cannot be influenced by KServ or other VMs.
In other words, SeKVM protects VM data confidentiality and
integrity because we prove that KCore has no vulnerabilities that
can be exploited to compromise VM confidentiality and integrity,
and any vulnerabilities in KServ, or other VMs, that are exploited
also cannot compromise VM confidentiality and integrity.
D. Bugs Found During Verification
During our noninterference proofs, we identified bugs in earlier
unverified versions of SeKVM, some of which were found in the
implementation used in [7]:
1) Page table update race: When proving invariants for page
ownership used in the noninterference proofs, we identified a race
condition in stage 2 page table updates. When allocating a physical
page to a VM, KCore removes it from KServ’s page table, assigns
ownership of the page to the VM, then maps it in the VM’s page
table. However, if KCore is processing a KServ’s stage 2 page fault
on another CPU, it could check the ownership of the same page
before it was assigned to the VM, think it was not assigned to any
VM, and map the page in KServ’s page table. This race could lead
to both KServ and the VM having a memory mapping to the same
physical page, violating VM memory isolation. We fixed this bug
by expanding the critical section and holding the S2Page array
lock, not just while checking and assigning ownership of the page,
but until the page table mapping is completed.
2) Overwrite page table mapping: KCore initially did not check
if a gfn was mapped before updating a VM’s stage 2 page tables,
making it possible to overwrite existing mappings. For example,
suppose two VCPUs of a VM trap upon accessing the same
unmapped gfn. Since KCore updates a VM’s stage 2 page table
whenever a VCPU traps on accessing unmapped memory, the same
page table entry will be updated twice, the latter replacing the former.
A compromised KServ could leverage this bug and allocate two
different physical pages, breaking VM data integrity. We fixed this
As another example, a similar proof is done to show noninterfer-
ence for MMIO accesses by VMs. Addresses for MMIO devices
are unmapped in all VMs’ stage 2 page tables. VM p traps to KCore
when executing a MMIO read or write instruction, invoking vm_-
page_fault. KCore switches to KServ on the same CPU to handle
the MMIO access. On an MMIO write, KCore copies the write data
from VM p’s GPR to KServ so it can program the virtual hardware,
which we model using KServ’s data oracle in proving Lemma 3 for
1791
Retrofitting Component
QEMU additions
KVM changes in KServ
HACL in KCore
KVM C in KCore
KVM assembly in KCore
Other C in KCore
Other assembly in KCore
Total
LOC
70
1.5K
10.1K
0.2K
0.3K
3.2K
0.1K
15.5K
Verification Component
34 layer specifications
AbsMachine machine model
C code proofs
Assembly code proofs
Layer refinements
Invariant proofs
Noninterference proofs
Total
LOC
6.0K