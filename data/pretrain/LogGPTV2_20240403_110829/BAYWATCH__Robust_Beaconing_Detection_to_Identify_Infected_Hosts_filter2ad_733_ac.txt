empirically shown to outperform, in terms of accuracy, both
SVM and na¨ıve Bayes classiﬁers across a wide range of data
sets; and (c) it is highly scalable and inherently parallelizable.
VII.
IMPLEMENTATION
We implemented BAYWATCH in 4,200 lines of Java on
top of the MapReduce framework [7] to efﬁciently process
large-scale data. Each phase is designed in a modularized
MapReduce job to avoid reprocessing raw logs.
A. Data Extraction
From raw logs stored in HDFS, e.g., web proxy logs, and
DNS logs, BAYWATCH ﬁrst extracts request time intervals per
communication pair. By default, time intervals are extracted at
the ﬁnest granularity, e.g., a second level.
• MAP: (cid:3)k, l(cid:4) → (cid:3)H(s, d), (s, d, ts)(cid:4). For each input line l, a
MAP task identiﬁes source s, destination d, and timestamp
ts to extract request information. A hash function H is used
to control the number of REDUCE tasks and output ﬁles. For
example, a 5-bit hash results in 32 (= 25
) REDUCE tasks
and output ﬁles, helping to minimize the startup overhead
arising from too many REDUCE tasks and too much disk
I/O. MapReduce performs better with a smaller number of
larger ﬁles rather than a larger number of smaller ﬁles.
• REDUCE: (cid:3)H(s, d), (s, d, ts)(cid:4) list → (cid:3)k, AS(cid:4). As the hash
of s and d is the key, request timestamps of the same source/
destination pair are grouped and sorted to get the list of
request timestamps. From the list, a REDUCE task obtains
request intervals per communication pair. The output value
of REDUCE is ActivitySummary which summarizes request
activities of every source/destination pair. ActivitySummary
AS consists of the following: source/destination pair s : d,
time scale e (1 second at the ﬁnest granularity), the ﬁrst
request timestamp ts1, and the list of request intervals int
along with side-channel information (such as full URLs) for
the token ﬁlter in Section V-A.
B. Rescaling and Merging
This phase provides BAYWATCH with scalability and ﬂex-
ibility. Instead of reprocessing raw logs, BAYWATCH rescales
ActivitySummary from the previous phase to improve peri-
odicity detection performance over long time ranges, and
aggregates request history to reduce data size.
• MAP: (cid:3)k, AS(cid:4) → (cid:3)H(s, d), ASrescaled(cid:4). Given new time
scale e(cid:2), a MAP task rescales old intervals int to new inter-
vals int(cid:2), and ﬁnds new ﬁrst time stamp ts(cid:2)
1 corresponding
to the new time scale. Rescaled ActivitySummary ASrescaled
is constructed. Hash function H is used to reduce REDUCE
and disk I/O overhead.
• REDUCE: (cid:3)H(s, d), ASrescaled(cid:4) list → (cid:3)k, ASmerged(cid:4). A
REDUCE task merges multiple intervals from ASrescaled
into one per source/destination pair, and yields rescaled and
merged ActivitySummary ASmerged.
C. Destination Popularity Statistics
As discussed in Section III-B, BAYWATCH leverages local
whitelists tuned for a speciﬁc organization, which is measured
by the number of distinct source features.
• MAP: (cid:3)k, AS(cid:4) → (cid:3)d, s(cid:4). A MAP task uses the source/
destination pair from the ActivitySummary AS, and yields
outputs by setting destination d as key and source s as value
aggregating sources accessing the same destination.
• REDUCE: (cid:3)d, s(cid:4) list → (cid:3)d, R(cid:4). A REDUCE task measures
popularity r ∈ R of each destination d by calculating the
number of sources making a request to a destination, divided
by the total number of sources.
D. Beaconing Detection
The beaconing detection phase employs the algorithm
described in Section IV to detect periodic requests in an
ActivitySummary produced by the data extraction phase (Sec-
tion VII-A) or the rescaling/merging phase (Section VII-B).
If
request histories are aggregated per source/
destination pair.
required,
• MAP: (cid:3)k, AS(cid:4) → (cid:3)H(s, d), ASf iltered(cid:4). A MAP task sepa-
rates communication pairs.
• REDUCE: (cid:3)H(s, d), ASf iltered(cid:4) → (cid:3)k, (AS, CP )(cid:4). A RE-
DUCE task performs the algorithm based on the request
history of every source/destination pair. When periodic
behaviors are identiﬁed, BAYWATCH reports such Activity-
Summary AS with a list of CandidatePeriods CP for sub-
sequent ranking and investigation phases. CandidatePeriod
CP consists of the followings: frequency, period, power,
auto-correlation score, and domain name score.
E. Ranking
In order to help analysts to prioritize deep investigation,
detected periodic behaviors are ranked based on its periodicity
and potential maliciousness.
• MAP: (cid:3)k, (AS, CP )(cid:4) → (cid:3)R, (AS, CP )(cid:4). A MAP task ﬁlters
out likely benign beaconing cases based on URL path token
analysis (Section V-A) and novelty analysis (Section V-B).
Then, periodicity score p ∈ R is calculated to rank and
weight beaconing cases based on a language model trained
on Alexa top 1 million domains, popularity score, and
periodicity strength.
• REDUCE: (cid:3)R, (AS, CP )(cid:4) list → sorted (cid:3)R, (AS, CP )(cid:4) list.
A REDUCE task collects periodicity scores, and sorts de-
tected beaconing cases to generate a ranked list.
486
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:40:31 UTC from IEEE Xplore.  Restrictions apply. 
487
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:40:31 UTC from IEEE Xplore.  Restrictions apply. 
Month
Oct 2013
Nov 2014
Dec 2014
Jan 2015
Feb 2015
Mar 2015
Total
1.2 TB (0.4 TB)
6.4 TB (0.9 TB)
6.4 TB (0.9 TB)
7.4 TB (1.1 TB)
6.5 TB (1.0 TB)
7.7 TB (1.1 TB)
35.6 TB (5.3 TB)
0.6 B
6.4 B
6.3 B
7.5 B
6.3 B
7.4 B
34.6 B
multiple noises injected into the baseline time series. Speciﬁ-
cally, we ﬁrst applied the missing-event or adding-event model
to bring noise into the time series. Then we measured δd
and γd against varying levels of Gaussian noise. The results
illustrated in Figure 10(d) generally follow similar pattern as
Figure 10(a). A noticeable difference is that the threshold,
where the algorithm reliably identiﬁed periodicities in the
noisy time series, dropped from 30 in Figure 10(a) to around
11 and 7 in Figure 10(d). The algorithm performed the worst
with the combination of the Gaussian noise and the missing-
event noise with probability 0.75.
Nevertheless, the evaluation shows that even with multiple
types of noise, the detection algorithm still achieved high
accuracy (δd < 5%) when the noise level was not strong
enough to signiﬁcantly distort the periodic pattern.
B. Beaconing in Web Proxy Logs
In order to evaluate the applicability of BAYWATCH in real-
world data sets, we performed beaconing detection analytics
on web proxy logs collected at a large corporate network.
Web proxy logs are good reference data sets to evaluate the
performance and deployment of BAYWATCH since web trafﬁc
is often misused by adversaries to disguise their malicious
payload in a huge amount of legitimate trafﬁc, which crosses
the perimeter. Although organizations typically employ web
trafﬁc monitoring methods, it is not trivial to analyze all events
historically to identify beaconing behaviors due to the size and
combinatorial complexity of the collected data.
1) Environment & Data sets: The web proxy logs from
a BlueCoat ProxySG infrastructure have been collected at
two time intervals: a consecutive 5-month time period from
November 2014 to March 2015, and a 10-day time period
in October 2013. Table III provides a detailed breakdown
of the data volumes. The proxy logs were positioned at the
perimeter of a multi-site corporate network; for each log entry,
we correlated the source IP address with the MAC address
obtained via the centralized DHCP server log repository.
Compared to an IP address, a MAC address is more reliable
in device identiﬁcation because IPs may change over time as
a device can connect from different networks. Overall, we
observed over 240 K IP addresses and 130 K distinct MAC
addresses.
We evaluated BAYWATCH on a MapReduce framework
consisting of 13 nodes, each equipped with 16–24 cores, 96 GB
memory, and 1 TB disk. Each node is also conﬁgured to run
up to 6 map tasks and up to 4 reduce tasks at once. In our
beaconing detection experiments, we deﬁned a connection pair
such that a source MAC/IP was the source feature, and a
destination domain/IP was the destination feature.
TABLE III.
DATA VOLUMES OF WEB PROXY LOGS
TABLE IV.
CONFUSION MATRIX OF CASE CLASSIFICATION
Log Size (gzipped)
# Events
classiﬁed benign
classiﬁed malicious
true benign
true malicious
2163
41
0
148
40
30
20
10
0
0
s
e
s
a
c
e
v
i
t
a
g
e
n
e
s
a
l
f
f
o
r
e
b
m
u
N
500
2000
2500
Number of cases to investigate (in order of uncertainty)
1000
1500
Fig. 11. Number of false negative cases
2) Results: Daily analysis (5-month trace): We have run
the complete data-ﬂow depicted in Fig. 3 over the 5-month
trace while choosing a resolution of 1 second for the event
time stamps when building the activity summaries (cf. Sec-
tion VII-A). The time series analysis has been run over daily
intervals to simulate daily operations of BAYWATCH. We chose
a local whitelist threshold τP = 0.01 (1% of the population),
and the threshold over the score distribution in the ranking
ﬁlter has been chosen at the 90th percentile.
BAYWATCH took about 35 hours to process one month
of proxy logs, e.g., March 2015 data set: extracting Activi-
tySummary of every connection pair from raw logs of each
day, detecting periodic events, and generating a ranked list
of CandidatePeriod. Runtime mainly depended on the amount
of data to be analyzed, especially the number of connection
pairs. During weekends, on average, there were 3.3 million
distinct connection pairs, and it took 14 minutes to complete
the entire analytics steps. During weekdays, on average, there
were 26 million distinct connection pairs, and it took 1 hour
and 30 minutes to complete the analytics. It demonstrated that
our implementation was scalable enough to analyze millions of
connections per day to detect surreptitious beaconing activities.
In total, 2,352 distinct destinations were ﬂagged as suspi-
cious. Manually examining all these cases would have been
extremely expensive. We thus applied the bootstrap approach
proposed in Section VI. More speciﬁcally, we examined a
much smaller data set collected over January 2015 (see Ta-
ble III). From these cases, we derived the features deﬁned in
Table II, and used these feature vectors as well as their labels
(0 for ‘benign’ and 1 for ‘malicious’) to train a random forest
classiﬁer (consisting of 200 decision trees). We then applied
this classiﬁer to the rest of the cases of the 5-month trace.
To evaluate the effectiveness of this approach, we queried
VirusTotal with all of the involved destinations and used the
returned reports to construct the “ground truth”: speciﬁcally,
if any of its anti-virus engine reported the destination as
malicious, we labeled it as malicious. The confusion ma-
trix in Table IV shows the classiﬁcation results. Among the
2,352 cases, a majority of them were correctly classiﬁed. In
particular, the classiﬁer was able to achieve a false positive
rate of 0 with respect to VirusTotal labels. As we examined
the 41 false negative cases in Table IV, we found indeed many
of them were of low certainty by the classiﬁer. One possible
488
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:40:31 UTC from IEEE Xplore.  Restrictions apply. 
TABLE V.
EXAMPLE CASES FOUND IN 5-MONTH TRACE
Domain name
cdn.5f75b1c54f8[..]2d4.com
img.ddbd60eeb01[..]cce.com
b117f8da23446a1[..]92a.pl
www.iiasdomk1m9[..]4z3.com
Smallest period
30 seconds
901 seconds
929 seconds
165 seconds