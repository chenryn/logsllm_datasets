change occurs in the containers, such as creation, destruction repository, define the overlay network in our configuration
or state change, through an agent associated to the container file and run the installation script that will automatically
manager. Bearing in mind that we also observe HTTP results, generate and deploy a docker-compose manifest file. The
it is also possible to implement a module responsible for Service Registry component, described in Subsection II-A,
failure detection. This module combines information from the will subscribe to the docker event API and be notified
HTTP results with container status, to add capabilities to our of container creation, destruction and state change. As such,
system of autonomous maintenance and recovery. When an serviceregistrationonthegatewaywillbedoneautomatically,
instance fails, it is possible to remove or restart this instance, requiring no collaboration from the services themselves. This
without needing administration supervision or microservice is possible because each container already carries the relevant
instrumentation. It is also relevant to mention that the com- metadata, such as name and service port.
ponents involved in monitoring are horizontally scalable, and The monitoring solution includes the user-customizable
therefore do not harm performance or availability of the frontend module, with Grafana. Furthermore, we developed
application. Since we remove the instrumentation necessary and included a custom plug-in, written in R, to generate more
of systems like the one of Figure 1, the processes responsible complex visualizations, such as chord diagrams [12]. Our raw
for extraction and processing of the metrics are outside of data storage module, uses influxDB and MySQL databases.
the critical path, and consequently do not create any sort of In Table II we present the overall containers associated (and
overhead. deployed) with the tool. Once installed in a Docker Swarm
container manager, all other applications deployed on it will
B. Collected Metrics
automaticallyuseourgatewayforservicediscoveryandmon-
itoring, as long as they are in the same overlay network.
We present in Table I the metrics collected in our “Metric
Storage” module. For each request, regardless of the origin Figure 3 shows an example dashboard, extracted during the
(either another microservice or a client), we save metrics experimental stage. In this case, we show the charts described
associated to the origin and destination of the request. in Section IV, such as histograms and chord diagrams.
Docker Swarm
Gateway (Zuul)
FrontEnd
Failure detector Swarm Manager
Service Discovery Load Balancing
(Eureka) (Ribbon)
Metric Storage Service Registry
User Containers
Fig. 2: System components
Fig.3.Sampleuser-customizablefrontend.
TABLEII.ToolContainers
an off-the-shelf integration with the remaining support mod-
Container Description ules from Netflix. Hence, integration with the discovery and
Eureka ServiceDiscovery registry module — Eureka —, is made, allowing a more
Zuul Gateway
agile instantiation and implementation of our methodology.
ServiceRegistry Manages containers life-cycle, in associa-
tionwithEureka Aligned with Ribbon and Eureka, we also used the Netflix
InfluxDB Time-seriesscalableDB gateway—Zuul—,thatusesRibboninternally.Zuulgetsthe
MySql DB service location through a query to Eureka, and then routing
Grafana Frontend
requests to the correct service. Since requests have to pass
ChordPlugin Generateschordvisualizations
through Zuul, this module allow us to have a clear vision
regardingtrafficbetweenmicroservicesandgathermonitoring
III. EXPERIMENTALSETUP information to a central point.
In this section we present the experimental setup used, The other component of our experimental setup is the
the changes made to Netflix modules, and our microservice application that allows us to test the monitoring method. The
application.First,concerningtheinfrastructuralmodules,used application that we implemented is related to music and has
by the test application, we rely on modules that are stable. fivemicroserviceswithwelldefinedfunctions.Theapplication
To do load balancing, we used Ribbon. This module gives allows its clients to manage users, playlists and songs. On
us several advantages, such as the available load balancing Table III, we identify the overall endpoints associated to
algorithms, the use of REST interfaces, but most importantly, each microservice, respective invocations methods and a brief
TABLEIII.MICROSERVICEANDFUNCTIONSAVAILABLE
microservice functionality requesttype description
/ GET SystemHealthcheck
Authentication MS
/login POST Validateusercredentialsandcreatetoken
/ GET SystemHealthcheck
/login POST Validateusercredentials
Users MS /users GET Getuserinfo
/users POST Createuser
/users/{id} DELETE Removeuser
/users/{id} PUT Updateuser
/ GET SystemHealthcheck
/playlists GET Getplaylistsassociatedtoauser
/playlists POST Createplaylist
Playlists MS /playlists/{id} GET Getplaylist
/playlists/{id} PUT Updateplaylist
/playlists/songs/{id} DELETE Removeaspecificmusicfromaplaylist
/playlists/songs/{id} GET Getmusicinfoassociatedtoaplaylist
/playlists/songs/{id} POST Addmusictoplaylist
/ GET SystemHealthcheck
/songs GET Getmusicinfo
/songs POST Createmusicinfo
Songs MS
/songs/convert/{id} GET Convertmusicfrommp3extensiontowav
/songs/criteria GET Getmusiclistbasedonsomecriteria
/songs/{id} DELETE Removemusic
/songs/{id} PUT Updatemusic
/ GET SystemHealthcheck
Aggregator MS
/playlists/songs/{id} GET Getallmusicinfoassociatedtoaplaylist
TABLEIV.SOFTWAREUSED
description.
Since we wanted to collect raw information about the Component Observations Version
requests,butwithoutinstrumentingmicroservices,wechanged Zuul Gateway 1.4.4
Eureka Servicediscovery 1.4.4
theZuulsourcecodetoregisterinformationconcerningorigin
Ribbon Loadbalancer 1.4.4
and destination of each request. We save the following infor- MariaDB DBusedbymicroservices 10.3.7
mation: microservice that made the request, start time, end MySQL DBusedbythefrontend 8.0
JMeter Loadtestingtool 4.0
time, IP and Port of the request origin, microservice instance
thatprocessedtherequest,andfunctionthatwasinvoked.With
this information, we were able to extract relevant information
In Table IV we present the open-source components used
aboutthesystem,suchastopologyoraverageresponsetimes,
in the experiment and respective versions.
decomposed by microservice and function. As mentioned, we
didnotneedanykindofinstrumentationonthesourcecodeof
the application (i.e., we only changed the infrastructure). The Music Application
FrontEnd
raw data is then pre-processed and redirected to a MySQL Failure detector Aggregator MS
database that makes part of our “system metric” module.
Metric Storage Service Registry
The software was installed in a virtual environment run- Auth MS
ning Ubuntu 16.04. The virtual machine had 8 vCores,
with 22 GiB of RAM. All components were installed Gateway (Zuul) Users MS
with standard parametrization, except the Zuul parame-
Service Discovery Load Balancing
ter sensitive-headers. This configuration allows us to (Eureka) (Ribbon)
PlayLists MS
propagate the authentication token through all microservices
without any kind of manipulation from the gateway.
Songs MS
To simulate load on the system, we used Apache JMe-
ter [13]. We configured this load tool with 10 threads, and
a launching period of 120 seconds. Each thread ran during
10 minutes with the following loop: 1) Create User; 2) Au- Fig.4.Systemarchitecture
thenticate; 3) Get user; 4) Update user; 5) Add song; 6) Get
song; 7) Update song; 8) Convert song; 9) Add playlist; Our ultimate goal with this experiment is very simple: un-
10)Getplaylist;11)Updateplaylist;12)Addmusictoplaylist; derstand the limits, benefits and disadvantages of our “black-
13) Get music from playlist; 14) Get all musics from playlist; box” nonintrusive monitoring tool. Figure 4 summarizes the
15)Deletemusicfromplaylist;16)Deleteplaylist;17)Delete entire system, with application, infrastructure, including the
song; 18) Delete user. monitoring tool and a load generator.
see the relations between the different microservices, and the
direct accesses from clients.
Having response times distribution in boxplot charts for
each microservice (and function) and dependencies between
microservices in graph visualization, there is still a crucial
aspect missing, to understand the health of the system: mi-
croservice and function importance in the system. To achieve
this, we resort to chord diagrams, based on the work of Gu et
al. [12]. This kind of graphic allows us to see more complex
relations between entities. Graph nodes are arranged along a
circle, and the importance of their interactions is proportional
to the width of the connecting arcs. We use arrows to provide
Fig.5.Boxplotofresponsetimebymicroservice
information of which side receives the call, and colors to
simplify interpretation. For instance, in Figure 7a, we can
see the number of requests, and in Figure 7b latency. In
users-ms a very large system, a chord graph comprising everything
wouldprobablybedifficulttoread.Hence,toimprovediagram
aggr-ms
interpretation, administrators can select what microservices to
display, as seen in Figure 3. For instance, in Figure 7c we
see latency, without requests made by the clients, since these
requests can have a huge impact on the graph and make other
client
interactions less visible.
songs-ms
TakingintoconsiderationFigure7a,anadministratorwould
verify that microservice playlist-ms would be the origin
auth-ms
or destination of around 28,000 requests. From these, around
3,000 were requests from playlist-ms to songs-ms,
21,000 requests made directly by clients and around 3,000
from the aggr-ms microservice. This way, we have a vision
playlists-ms
of the playlist-ms microservice relevance in the overall
system. Additionally, the same analysis could be made for
Fig.6.Applicationgraph latency. The box-plots, combined with the dependency graph
and the chord diagram, give us a good idea of the system
capacity, module importance and response time distribution
IV. RESULTS
by microservice or function.
Aninteractionofasystemadministratorwiththemonitoring
In this section, we present the results of gathering monitor-
systemcouldgothisway:theadministratorwouldfirstlookto
ing data from the API gateway. This technique allows us to
Figure5.Thisbox-plot,providesaclearunderstandingofwhat
extractrawdatafrommicroserviceinteractionsand,therefore,
services have higher response times. It is easily observable
createasetofmetricsandchartswithrelevantinformationfor
that the service songs-ms has the highest response times
administrators, without the need of instrumentation or agents
of all modules. The second module with higher response
at hosts level. In this paper, we present 5 visualizations that
timesisaggr-ms.Nevertheless,anadministratorwouldtryto
combined, give us a clear vision of the system.
understand the importance that the songs-ms microservice