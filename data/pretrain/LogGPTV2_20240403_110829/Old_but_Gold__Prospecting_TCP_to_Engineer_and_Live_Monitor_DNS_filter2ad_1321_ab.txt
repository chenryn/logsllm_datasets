can be seen in Table 1. The data shows that TCP is used rarely, accounting for
less than 7% of queries for each anycast service. However, those queries represent
more than a ﬁfth of resolvers and 44% of ASes. (In all cases, TCP queries come
from IP addresses that also send UDP queries).
AS Representation: We have TCP data for roughly 44% of all ASes (Table 1).
This coverage is lower than we would prefer, but these are the ASes that account
for the majority of traﬃc: the top DNS/TCP 10 ASes are responsible for half of
all queries, while the top DNS/TCP 100 ASes account for 78% for Service A and
75% for B (Fig. 2). Although we miss many ASes, we next show we cover the
preﬁxes in those ASes with recursive servers and we account for a large fraction
of DNS traﬃc.
Traﬃc Coverage: We see that 5% of all queries are TCP, and they originate from
about 20% of all resolvers (Table 1). While these are incomplete, we next show
Old but Gold: Prospecting TCP to Engineer
269
Table 2. Traﬃc coverage for resolvers that use TCP in addition to UDP for DNS
queries for .nl (Oct. 15–22, 2019).
Anycast A Anycast B
4 005 046 701 4 245 504 907
TCP resolvers (DNS/UDP + DNS/TCP) 2 306 027 922 1 246 213 577
29.35%
1 232 407 755 1 433 856 950
518 144 495
36.13%
TCP resolvers (DNS/UDP + DNS/TCP)
533 519 527
43.29%
57.7%
IPv4
IPv6
Ratio (%)
Ratio (%)
Table 3. DNS queries (in millions) for root DNS (E and G missing) – 2019-10-15 –
2019-10-22.
A
B
C
D
F
H
I
J
K
L M
Total
IPv4
UDP
TCP
70601 40601 59033 88136 144635 31702
66582 115162 76761 105041 42702
58552 33925 47675 74565 125020 25706
55874
96727 61378
88046 33687
56921 32334 45568 70969 118738 25234
51208
87891 60312
84059 31925
1631
1591
2107
3596
6282
472
4665
8836
1065
3986
1762
Ratio TCP 2.87% 4.92% 4.62% 5.07% 5.29% 1.87% 9.11% 10.05% 1.77% 4.74% 5.52%
IPv6
12049
6675 11357 13571
19614
5995
1070
18435 15383
16994
9014
UDP
TCP
11659
6280 10966 13071
18919
5825
936
15511 15108
16576
8268
389
394
391
499
694
169
1342
2923
274
418
746
Ratio TCP 3.34% 6.29% 3.57% 3.82% 3.67% 2.92% 14.34% 18.84% 1.82% 2.52% 9.03%
that they cover the majority of DNS traﬃc. In Table 2, we see that 29–58% of
the total traﬃc (depending on IP version and anycast service) is from resolvers
that have sent some TCP. As such, we have latency for at least 29% and up to
58% of DNS traﬃc. In addition, if we want full coverage, we describe below how
we can induce coverage when it is necessary.
Root DNS: To conﬁrm that DNS/TCP provides coverage beyond .nl, we also
look at how many TCP queries are seen at most Root DNS servers [56] over
the same period. Table 3 shows RSSAC-002 statistics [24,69] from 11 of the 13
Root DNS services reporting at this time. We see the ratio of TCP traﬃc varied
for each service (known as “letters”, from A to M) and IPv4 or IPv6, overall
ranging from 2.8 (A Root over IPv4) up 18.9% (J Root over IPv6). This data
suggests the root letters see similar DNS/TCP rates as .nl.
Inducing Coverage. While TCP coverage is not complete, we can get complete
coverage by actively managing traﬃc to induce occasional TCP queries, as is
often done in web systems (for example, [58]). The DNS speciﬁcation includes
the TC (“truncated”) bit to indicate a truncated reply that must be retried
over TCP. DNS Receiver Rate Limiting [66] (RRL) uses this mechanism to force
possible UDP-based address spoofers to resend their queries with TCP. Switching
to TCP allows TCP cookies to prevent spooﬁng [15].
270
G. C. M. Moura et al.
Fig. 2. .nl: queries distribution per AS.
A DNS server can use this same mechanism to solicit TCP queries from
selected clients, allowing us to determine RTTs. We have implemented this capa-
bility in the Knot DNS server [11], building on Knot’s RRL implementation. Our
implementation tracks each block (/24 IPv4 preﬁx, or /56 IPv6 preﬁx). When a
UDP request from that block arrives, if there are insuﬃcient TCP queries in the
last hour, it returns an answer with the TC bit set with some probability. The
probability of not setting the bit and the required number of RTT observations
per hour are both conﬁgurable. (However, our measurements pre-date and so do
not use this mechanism).
The cost of forcing TCP is two additional round trips, and some resolvers
fail to convert to TCP [41]. TCP solicitation should therefore be used sparingly,
although other deployed systems diverting some traﬃc to identify service prob-
lems (for example, Facebook [58]). To ensure the server’s increased TCP load is
negligible, RTT induction should be conﬁgured to balance better measurements
against available computational resources.
Temporal Coverage. Next we investigate how much temporal coverage passive
analysis of DNS/TCP provides. We require TCP connections to observe latency
in each time period with conﬁdence, so traﬃc rate per AS determines our tem-
poral precision. We hope traﬃc allows temporal precision of 0.5 to 4 hours so
passive analysis can support near-real-time monitoring over the day (Sect. 4.4).
To evaluate the number of TCP queries per AS in a given time interval, we
analyze .nl traﬃc from Anycast A and B. We single out one day of traﬃc (the
ﬁrst day of Table 1, 2019-10-15). On this day, Anycast A and Anycast B received
UDP queries from ∼37k ASes over IPv4, and from ∼6.4k ASes over IPv6 (notice
that numbers in Table 1 are higher given they cover the whole week).
To evaluate how many ASes report enough data to estimate RTTs each hour,
Fig. 3 shows TCP queries per hour for Anycast A. As a baseline, IPv4 (Fig. 3a)
sees about 26.3k ASes that send UDP queries per hour (IPv4), and 4.8k for IPv6.
Of these, about 8.8k also send TCP queries (1.8k for IPv6), allowing some IPv4
RTT information about 33% of ASes and for IPv6, 38% of ASes. However, these
ASes that also send TCP queries are responsible for the majority of all queries
(blue line in Fig. 3): more than 90% of IPv4 queries and more than 60% of all
IPv6 queries. If we only consider ASes that send at least 10k TCP queries/hour,
we still account for most of the traﬃc (yellow line in Fig. 3).
Old but Gold: Prospecting TCP to Engineer
271
Fig. 3. .nl temporal coverage for Anycast A (Color ﬁgure online)
We conclude that a large number of ASes can be measured every hour with
DNS/TCP. (We repeated the same analysis Anycast B (Appendix A) and also
for another day 2019-10-21, both of them hold the same results [39]).
Summary: We see that TCP data provide good operational coverage and great
temporal coverage. More importantly, TCP provides the only insight into IPv6
latency, since current active methods do not generalize to IPv6.
2.2 DNS/UDP vs. DNS/TCP RTT
We expect round-trip times measured with DNS/TCP and DNS/UDP to be
similar. Next, we investigate that assumption.
We can compare DNS/UDP and DNS/TCP RTTs by comparing query
response times and accounting for TCP connection setup. DNS/UDP makes
a direct request and gets a response, while in DNS/TCP we set up the TCP
connection (with a SYN–SYN/ACK handshake), so a TCP DNS request should
take two RTTs (assuming no connection reuse, TCP fast-open, or other opti-
mizations). We expect similar RTT estimates after dividing by two to account
for TCP’s handshake.
To conﬁrm this claim we measure DNS/UDP and DNS/TCP query response
times using RIPE Atlas [52]. Atlas provides about 11k devices in diﬀerent loca-
tions around the world, allowing us to test many network conditions. As targets,
we evaluate two large, globally distributed, production and public DNS any-
cast networks: L-Root, with 167 anycast sites, and K-Root, with 79 sites, which
are two of the thirteen authoritative servers for the Root DNS zone. To mea-
sure DNS/UDP latency from probes to these root letters, we leverage existing
measurements that run continuously on Ripe Atlas, every 4 min.
L
and K-Root
([k,l]root-tcp in [50]), within daily limits on query for RIPE Atlas. We study
about 8.6k probes, running every 8 minutes (twice the interval of UDP mea-
surements) for 24 hours, with results in Table 4. In these measurements, each
Atlas probe directly queries the IPv4 address of the K and L-Root, without using
a recursive resolver. For a fair comparison, we consider only probes that are
present in both UDP and TCP measurements (∩ Probes): 8.5k and 8.9k for K
then create DNS/TCP measurements
We
towards
272
G. C. M. Moura et al.
Table 4. DNS/UDP vs. DNS/TCP Atlas measurements. Datasets: [50].
K-Root
L-Root
UDP
TCP
Sept 4–5, 2020
8 min
4 min
10608
8680
UDP
TCP
Sept 5–6, 2020
8 min
4 min
10595
8999
8577
8901
Date
Freq.
Probes
∩ Probes
Queries
3759080 1516801 3756572 1600283
∩ Queries 3044067 1499078 3160277 1583243
Fig. 4. L-Root: Density plot (log-scale) of number of Atlas observers with mean UDP
and TCP DNS transaction times.
and L-Root, respectively. In the same way, we only evaluate queries from these
matching probes: ∼3M UDP queries and ∼1.5M for TCP measurements, for
each Root Letter (∩ Queries). Both measurements consider retries: UDP in the
application and TCP by the kernel. In total, we analyze 9.2M queries for both
letters. Then, for each Atlas probe, we compute its latency distribution.
Figure 4 shows a density plot of the number of RIPE Atlas observers with a
given combination of mean DNS/UDP and DNS/TCP transaction times. Each
combination is the mean of around 360 observations, and we report densities
as log scale across the 8.9k Atlas probes available during the measurement.
(We omit a handful of outliers with UDP means more than 250 ms.) We see a
strong trend on the diagonal with a 1:2 UDP:TCP ratio, corresponding with
TCP requiring two round trips. Slight variations from the diagonal represents
queueing; considerable variation suggests experiments with retries. We see simi-
lar results (not shown due to space) for K-Root, and for median, and for 90%ile
RTTs (see [39] for CDFs). The data shows a strong correlation between UDP
and TCP, but also some outliers in the lower-left corner due to retransmission.
We can quantify similarity by computing the correlation coeﬃcient of median
UDP and half the median: the correlation is 0.913 for K-Root and 0.930 for L-
Root. We also did a Student’s t-Test, evaluating the hypothesis that the UDP
mean and half the TCP mean are statistically identical with a 95% conﬁdence.
This test could not be rejected the majority of the time (64% of the time, in
5558 cases for K-Root and 5733 cases for L-Root), suggesting the results were
Old but Gold: Prospecting TCP to Engineer
273
often indistinguishable. Manual examination shows outliers are common in the
cases where the hypothesis is rejected, suggesting a TCP-level retransmission.
Although retransmission detection is possible, our results show usability even
when minimizing computational requirements so as to optimize for low-overhead,
real-item deployments. This experiment proves that passively observed TCP
RTTs often provide a good representation of the RTTs that DNS/UDP will see.
3 Prioritizing Analysis
We have shown that DNS/TCP can be mined to determine RTTs (Sect. 2).
Operational DNS systems must serve the whole world, there are more than 42k
active ASes sending DNS queries to authoritative servers. Both detection and
resolution of networking problems in anycast systems is labor intensive: detec-
tion requires both identifying speciﬁc problems and their potential root causes.
Problem resolution requires new site deployments or routing changes, both need-
ing human-in-the-loop changes involving trouble tickets, new hardware, and new
hosting contracts.
Overview: We use two strategies to prioritize the analysis of problems that are