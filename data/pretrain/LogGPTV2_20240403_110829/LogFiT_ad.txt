(NLP) techniques to address this issue. A literature review
reveals that state-of-the-art research in this area employs
the Long Short-Term Memory (LSTM) architecture, as
exemplified by the DeepLog model [4], and Transformers,represented by the LogBERT model [5]. One crucial factor
in applying deep learning to log anomaly detection is the
availability of labeled data for training predictive models.Due to the high cost of preparing labeled data, classification-based methods such as LogSy [6] have limited utility in production settings. As a result, the majority of techniques used in detecting log anomalies assume a zero-positive training scenario, where normal log data is the only data available for self-supervised training [7] only. Furthermore, [8] considers two categories of self-supervised models for log anomaly detection. The first is forecasting-based models which attempt to predict upcoming log entries by analyzing previous log entries, while the second is reconstruction-based models which reconstruct log sequences that have been intentionally corrupted. The DeepLog model uses the forecasting-based method, whereas the LogBERT model uses the reconstruction-based method.This study focuses on log data comprising sequences of log sentences that represent various system events, such as the successful connection to an email server or a failure to upload a file after multiple attempts. To train anomaly detection models, log sentences are grouped according to specific criteria, such as session identifier or time window. Each group of log sentences defines a sequence of system events that occurred within a given time window or session. For instance, a normal sequence of log sentences may consist of one or more ”File opened successfully” entries followed by”File write operation completed” and/or ”File read operation completed” entries. However, a sequence of log sentences that only contains ”File opened successfully” entries without corresponding ”read” or ”write” entries can be considered an anomaly. This information is critical to identifying anomalies in system logs and developing effective anomaly detection models.The quality of the representation of log sequences is a critical factor that impacts the effectiveness of log anomaly detection models, especially when log content evolution is taken into account [9], [10]. In order to represent log sequences, a common approach is to first convert log sentences into log templates [4], [5]. A set of log templates is thus produced and this set of log templates functions as a form of input vocabulary. Afterwards all input log sentences are mapped to an entry within the set of log templates. This approach has been shown to negatively effect2
model effectiveness because of its semantically deficient representations of log sentences. Furthermore the input log sentences are expected to match a log template. Thus this approach is unable to handle variability in the content log sentences over time [6], [11], [12].
To address the limitations of existing approaches for anomaly detection in system logs, this work makes the following contributions:• A novel log anomaly detection model, named LogFiT, 	which 	utilises 	a 	pre-trained 	Bidirectional 	Encoder 	Representations 	from 	Transformers 	(BERT) 	based 	language model (LM) that has been fine-tuned to 	understand the linguistic and sequential patterns of the 	normal log data. The LogFiT model is able to produce 	representations for log sequences of up to 4096 tokens 	by leveraging a pre-trained LM. This ensures that the 	model remains robust even when there are unforeseen 	changes to the content of log sentences.• A framework and workflow based on the HuggingFace 	ecosystem. The framework adopts the masked language 	modeling 	(MLM) 	self-supervised 	training 	objective. 	During training, the model is trained on normal data, with 	the aim of minimising the cross-entropy loss between 	predictions and the actual data. At inference time, the 	framework determines whether a log sequence is normal 	or an anomaly by comparing the model’s top-k accuracy 	against a pre-defined threshold value.• We have conducted experiments comparing LogFiT’s 	precision, 	recall, 	F1 	score 	and 	specificity 	against 	two baseline approaches, DeepLog and LogBERT. the 	experiments were conducted using three datasets: HDFS, 	BGL, and Thunderbird. We present the experimental 	results to demonstrate the superiority of LogFiT in 	detecting anomalous log sequences compared to the 	baseline approaches.• We 	will 	make 	our 	implementation 	of 	the 	model, 	developed using Python, PyTorch and HuggingFace, 	publicly available, along with the model checkpoints for 	reproducibility and further research.
II. RELATED WORK