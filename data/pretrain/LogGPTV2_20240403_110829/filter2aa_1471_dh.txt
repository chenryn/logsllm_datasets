either Driver Verifier, Import Optimization, or Kernel Patch protection are
applied to it. (This is important; otherwise, the system would bugcheck.) A
list of the current shimmed kernel modules is stored in a global variable. The
KsepGetShimsForDriver routine checks whether a module in the list with the
same base address as the one being loaded is currently present. If so, it means
that the target module has already been shimmed, so the procedure is aborted.
Otherwise, to determine whether the new module should be shimmed, the
routine checks two different sources:
■    Queries the “Shims” multistring value from a registry key named as
the module being loaded and located in the
HKLM\System\CurrentControlSet\Control\Compatibility\Driver root
key. The registry value contains an array of shims’ names that would
be applied to the target module.
■    In case the registry value for a target module does not exist, parses the
driver compatibility database file, looking for a KDRIVER tag
(indexed by the INDEX block), which has the same name as the
module being loaded. If a driver is found in the SDB file, the NT
kernel performs a comparison of the driver version
(TAG_SOURCE_OS stored in the KDRIVER root tag), file name,
and path (if the relative tags exist in the SDB), and of the low-level
system information gathered at engine initialization time (to
determine if the driver is compatible with the system). In case any of
the information does not match, the driver is skipped, and no shims
are applied. Otherwise, the shim names list is grabbed from the
KSHIM_REF lower-level tags (which is part of the root KDRIVER).
The tags are reference to the KSHIMs located in the SDB database
block.
If one of the two sources yields one or more shims names to be applied to
the target driver, the SDB file is parsed again with the goal to validate that a
valid KSHIM descriptor exists. If there are no tags related to the specified
shim name (which means that no shim descriptor exists in the database), the
procedure is interrupted (this prevents an administrator from applying
random non-Microsoft shims to a driver). Otherwise, an array of
KSE_SHIM_INFO data structure is returned to KsepGetShimsForDriver.
The next step is to determine if the shims described by their descriptors
have been registered in the system. To do this, the Shim engine searches into
the global linked list of registered shims (filled every time a new shim is
registered, as explained previously in the “Shim Engine initialization”
section). If a shim is not registered, the shim engine tries to load the driver
that provides it (its name is stored in the MODULE child tag of the root
KSHIM entry) and tries again. When a shim is applied for the first time, the
Shim engine resolves the pointers of all the hooks described by the
KSE_HOOK_COLLECTION data structures’ array belonging to the
registered shim (KSE_SHIM data structure). The shim engine allocates and
fills a KSE_SHIMMED_MODULE data structure representing the target
module to be shimmed (which includes the base address) and adds it to the
global list checked in the beginning.
At this stage, the shim engine applies the shim to the target module using
the internal KsepApplyShimsToDriver routine. The latter cycles between each
hook described by the KSE_HOOK_COLLECTION array and patches the
import address table (IAT) of the target module, replacing the original
address of the hooked functions with the new ones (described by the hook
collection). Note that the driver’s object callback functions (IRP handlers)
are not processed at this stage. They are modified later by the I/O manager
before the DriverInit routine of the target driver is called. The original
driver’s IRP callback routines are saved in the Driver Extension of the target
driver. In that way, the hooked functions have a simple way to call back into
the original ones when needed.
EXPERIMENT: Witnessing kernel shims
While the official Microsoft Application Compatibility Toolkit
distributed with the Windows Assessment and Deployment Kit
allows you to open, modify, and create shim database files, it does
not work with system database files (identified through to their
internal GUIDs), so it won’t be able to parse all the kernel shims
that are described by the drvmain.sdb database. Multiple third-party
SDB parsers exist. One in particular, called SDB explorer, is freely
downloadable from https://ericzimmerman.github.io/.
In this experiment, you get a peek at the drvmain system
database file and apply a kernel shim to a test driver, ShimDriver,
which is available in this book’s downloadable resources. For this
experiment, you need to enable test signing (the ShimDriver is
signed with a test self-signed certificate):
1. 
Open an administrative command prompt and type the
following command:
Click here to view code image
bcdedit /set testsigning on
2. 
Restart your computer, download SDB Explorer from its
website, run it, and open the drvmain.sdb database located
in %SystemRoot%\apppatch.
3. 
From the SDB Explorer main window, you can explore the
entire database file, organized in three main blocks:
Indexes, Databases, and String table. Expand the
DATABASES root block and scroll down until you can see
the list of KSHIMs (they should be located after the
KDEVICEs). You should see a window similar to the
following:
4. 
You will apply one of the Version lie shims to our test
driver. First, you should copy the ShimDriver to the
%SystemRoot%\System32\Drivers. Then you should install
it by typing the following command in the administrative
command prompt (it is assumed that your system is 64-bit):
Click here to view code image
sc create ShimDriver type= kernel start= demand error= 
normal binPath= c:\
Windows\System32\ShimDriver64.sys
5. 
Before starting the test driver, you should download and run
the DebugView tool, available in the Sysinternals website
(https://docs.microsoft.com/en-
us/sysinternals/downloads/debugview). This is necessary
because ShimDriver prints some debug messages.
6. 
Start the ShimDriver with the following command:
sc start shimdriver
7. 
Check the output of the DebugView tool. You should see
messages like the one shown in the following figure. What
you see depends on the Windows version in which you run
the driver. In the example, we run the driver on an insider
release version of Windows Server 2022:
8. 
Now you should stop the driver and enable one of the shims
present in the SDB database. In this example, you will start
with one of the version lie shims. Stop the target driver and
install the shim using the following commands (where
ShimDriver64.sys is the driver’s file name installed with the
previous step):
Click here to view code image
sc stop shimdriver
reg add 
"HKLM\System\CurrentControlSet\Control\Compatibility\D
river\
    ShimDriver64.sys" /v Shims /t REG_MULTI_SZ /d
KmWin81VersionLie /f /reg:64
9. 
The last command adds the Windows 8.1 version lie shim,
but you can freely choose other versions.
10. 
Now, if you restart the driver, you will see different
messages printed by the DebugView tool, as shown in the
following figure:
11. 
This is because the shim engine has correctly applied the
hooks on the NT APIs used for retrieving OS version
information (the driver is able to detect the shim, too). You
should be able to repeat the experiment using other shims,
like the SkipDriverUnload or the
KernelPadSectionsOverride, which will zero out the driver
unload routine or prevent the target driver from loading, as
shown in the following figure:
Device shims
Unlike Driver shims, shims applied to Device objects are loaded and applied
on demand. The NT kernel exports the KseQueryDeviceData function, which
allows drivers to check whether a shim needs to be applied to a device object.
(Note also that the KseQueryDeviceFlags function is exported. The API is
just a subset of the first one, though.) Querying for device shims is also
possible for user-mode applications through the NtQuerySystemInformation
API used with the SystemDeviceDataInformation information class. Device
shims are always stored in three different locations, consulted in the
following order:
1. 
In the
HKLM\System\CurrentControlSet\Control\Compatibility\Device root
registry key, using a key named as the PNP hardware ID of the
device, replacing the \ character with a ! (with the goal to not confuse
the registry). Values in the device key specify the device’s shimmed
data being queried (usually flags for a certain device class).
2. 
In the kernel shim cache. The Kernel Shim engine implements a shim
cache (exposed through the KSE_CACHE data structure) with the
goal of speeding up searches for device flags and data.
3. 
In the Shim database file, using the KDEVICE root tag. The root tag,
among many others (like device description, manufacturer name,
GUID and so on), includes the child NAME tag containing a string
composed as follows: . The KFLAG or
KDATA children tags include the value for the device’s shimmed
data.
If the device shim is not present in the cache but just in the SDB file, it is
always added. In that way, future interrogation would be faster and will not
require any access to the Shim database file.
Conclusion
In this chapter, we have described the most important features of the
Windows operating system that provide management facilities, like the
Windows Registry, user-mode services, task scheduling, UBPM, and
Windows Management Instrumentation (WMI). Furthermore, we have
discussed how Event Tracing for Windows (ETW), DTrace, Windows Error
Reporting (WER), and Global Flags (GFlags) provide the services that allow
users to better trace and diagnose issues arising from any component of the
OS or user-mode applications. The chapter concluded with a peek at the
Kernel Shim engine, which helps the system apply compatibility strategies
and correctly execute old components that have been designed for older
versions of the operating system.
The next chapter delves into the different file systems available in
Windows and with the global caching available for speeding up file and data
access.
CHAPTER 11
Caching and file systems
The cache manager is a set of kernel-mode functions and system threads that
cooperate with the memory manager to provide data caching for all Windows
file system drivers (both local and network). In this chapter, we explain how
the cache manager, including its key internal data structures and functions,
works; how it is sized at system initialization time; how it interacts with other
elements of the operating system; and how you can observe its activity
through performance counters. We also describe the five flags on the
Windows CreateFile function that affect file caching and DAX volumes,
which are memory-mapped disks that bypass the cache manager for certain
types of I/O.
The services exposed by the cache manager are used by all the Windows
File System drivers, which cooperate strictly with the former to be able to
manage disk I/O as fast as possible. We describe the different file systems
supported by Windows, in particular with a deep analysis of NTFS and ReFS
(the two most used file systems). We present their internal architecture and
basic operations, including how they interact with other system components,
such as the memory manager and the cache manager.
The chapter concludes with an overview of Storage Spaces, the new
storage solution designed to replace dynamic disks. Spaces can create tiered
and thinly provisioned virtual disks, providing features that can be leveraged
by the file system that resides at the top.
Terminology
To fully understand this chapter, you need to be familiar with some basic
terminology:
■    Disks are physical storage devices such as a hard disk, CD-ROM,
DVD, Blu-ray, solid-state disk (SSD), Non-volatile Memory disk
(NVMe), or flash drive.
■    Sectors are hardware-addressable blocks on a storage medium. Sector
sizes are determined by hardware. Most hard disk sectors are 4,096 or
512 bytes, DVD-ROM and Blu-ray sectors are typically 2,048 bytes.
Thus, if the sector size is 4,096 bytes and the operating system wants
to modify the 5120th byte on a disk, it must write a 4,096-byte block
of data to the second sector on the disk.
■    Partitions are collections of contiguous sectors on a disk. A partition
table or other disk-management database stores a partition’s starting
sector, size, and other characteristics and is located on the same disk
as the partition.
■    Volumes are objects that represent sectors that file system drivers
always manage as a single unit. Simple volumes represent sectors
from a single partition, whereas multipartition volumes represent
sectors from multiple partitions. Multipartition volumes offer
performance, reliability, and sizing features that simple volumes do
not.
■    File system formats define the way that file data is stored on storage
media, and they affect a file system’s features. For example, a format
that doesn’t allow user permissions to be associated with files and
directories can’t support security. A file system format also can
impose limits on the sizes of files and storage devices that the file
system supports. Finally, some file system formats efficiently
implement support for either large or small files or for large or small
disks. NTFS, exFAT, and ReFS are examples of file system formats
that offer different sets of features and usage scenarios.
■    Clusters are the addressable blocks that many file system formats use.
Cluster size is always a multiple of the sector size, as shown in Figure
11-1, in which eight sectors make up each cluster, which are
represented by a yellow band. File system formats use clusters to
manage disk space more efficiently; a cluster size that is larger than
the sector size divides a disk into more manageable blocks. The
potential trade-off of a larger cluster size is wasted disk space, or
internal fragmentation, that results when file sizes aren’t exact
multiples of the cluster size.
Figure 11-1 Sectors and clusters on a classical spinning disk.
■    Metadata is data stored on a volume in support of file system format
management. It isn’t typically made accessible to applications.
Metadata includes the data that defines the placement of files and
directories on a volume, for example.
Key features of the cache manager
The cache manager has several key features:
■    Supports all file system types (both local and network), thus removing
the need for each file system to implement its own cache management
code.
■    Uses the memory manager to control which parts of which files are in
physical memory (trading off demands for physical memory between
user processes and the operating system).
■    Caches data on a virtual block basis (offsets within a file)—in contrast
to many caching systems, which cache on a logical block basis
(offsets within a disk volume)—allowing for intelligent read-ahead
and high-speed access to the cache without involving file system
drivers. (This method of caching, called fast I/O, is described later in
this chapter.)
■    Supports “hints” passed by applications at file open time (such as
random versus sequential access, temporary file creation, and so on).
■    Supports recoverable file systems (for example, those that use
transaction logging) to recover data after a system failure.
■    Supports solid state, NVMe, and direct access (DAX) disks.
Although we talk more throughout this chapter about how these features
are used in the cache manager, in this section we introduce you to the
concepts behind these features.
Single, centralized system cache
Some operating systems rely on each individual file system to cache data, a
practice that results either in duplicated caching and memory management
code in the operating system or in limitations on the kinds of data that can be
cached. In contrast, Windows offers a centralized caching facility that caches
all externally stored data, whether on local hard disks, USB removable
drives, network file servers, or DVD-ROMs. Any data can be cached,
whether it’s user data streams (the contents of a file and the ongoing read and
write activity to that file) or file system metadata (such as directory and file
headers). As we discuss in this chapter, the method Windows uses to access
the cache depends on the type of data being cached.
The memory manager
One unusual aspect of the cache manager is that it never knows how much
cached data is actually in physical memory. This statement might sound
strange because the purpose of a cache is to keep a subset of frequently
accessed data in physical memory as a way to improve I/O performance. The
reason the cache manager doesn’t know how much data is in physical
memory is that it accesses data by mapping views of files into system virtual
address spaces, using standard section objects (or file mapping objects in
Windows API terminology). (Section objects are a basic primitive of the
memory manager and are explained in detail in Chapter 5, “Memory
Management” of Part 1). As addresses in these mapped views are accessed,
the memory manager pages-in blocks that aren’t in physical memory. And
when memory demands dictate, the memory manager unmaps these pages out
of the cache and, if the data has changed, pages the data back to the files.
By caching on the basis of a virtual address space using mapped files, the
cache manager avoids generating read or write I/O request packets (IRPs) to
access the data for files it’s caching. Instead, it simply copies data to or from
the virtual addresses where the portion of the cached file is mapped and relies
on the memory manager to fault in (or out) the data in to (or out of) memory
as needed. This process allows the memory manager to make global trade-
offs on how much RAM to give to the system cache versus how much to give
to user processes. (The cache manager also initiates I/O, such as lazy writing,
which we describe later in this chapter; however, it calls the memory
manager to write the pages.) Also, as we discuss in the next section, this
design makes it possible for processes that open cached files to see the same
data as do other processes that are mapping the same files into their user
address spaces.
Cache coherency
One important function of a cache manager is to ensure that any process that
accesses cached data will get the most recent version of that data. A problem
can arise when one process opens a file (and hence the file is cached) while
another process maps the file into its address space directly (using the
Windows MapViewOfFile function). This potential problem doesn’t occur
under Windows because both the cache manager and the user applications
that map files into their address spaces use the same memory management
file mapping services. Because the memory manager guarantees that it has
only one representation of each unique mapped file (regardless of the number
of section objects or mapped views), it maps all views of a file (even if they
overlap) to a single set of pages in physical memory, as shown in Figure 11-
2. (For more information on how the memory manager works with mapped
files, see Chapter 5 of Part 1.)
Figure 11-2 Coherent caching scheme.
So, for example, if Process 1 has a view (View 1) of the file mapped into
its user address space, and Process 2 is accessing the same view via the
system cache, Process 2 sees any changes that Process 1 makes as they’re
made, not as they’re flushed. The memory manager won’t flush all user-
mapped pages—only those that it knows have been written to (because they
have the modified bit set). Therefore, any process accessing a file under
Windows always sees the most up-to-date version of that file, even if some
processes have the file open through the I/O system and others have the file
mapped into their address space using the Windows file mapping functions.
 Note
Cache coherency in this case refers to coherency between user-mapped
data and cached I/O and not between noncached and cached hardware
access and I/Os, which are almost guaranteed to be incoherent. Also,
cache coherency is somewhat more difficult for network redirectors than
for local file systems because network redirectors must implement
additional flushing and purge operations to ensure cache coherency when
accessing network data.
Virtual block caching
The Windows cache manager uses a method known as virtual block caching,
in which the cache manager keeps track of which parts of which files are in
the cache. The cache manager is able to monitor these file portions by
mapping 256 KB views of files into system virtual address spaces, using
special system cache routines located in the memory manager. This approach
has the following key benefits:
■    It opens up the possibility of doing intelligent read-ahead; because the
cache tracks which parts of which files are in the cache, it can predict
where the caller might be going next.
■    It allows the I/O system to bypass going to the file system for requests
for data that is already in the cache (fast I/O). Because the cache