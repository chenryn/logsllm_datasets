7.5
Summary
219
C H A P T E R  8
Fuzzer Comparison
In this book, we’ve discussed a number of different ways to fuzz systems as well as
numerous implementations of these ideas. We’ve talked about ways to place fuzzing
technologies into your SDLC and how to measure their success. This chapter is
focused on comparing and contrasting some different fuzzers that are currently
available. In particular, it measures the relative effectiveness of some different fuzzers.
This chapter is intended to help you decide which type of fuzzing and which fuzzers
will be best for your situation given your individual time and money constraints.
8.1
Fuzzing Life Cycle
Some fuzzers are simple input generators. Some consist of an entire framework
designed to help with the many stages of fuzzing. Let’s take a closer look at all the
necessary steps required to conduct fuzzing in order to get a better understanding
of the fuzzers we are going to compare.
8.1.1
Identifying Interfaces
Given a target system, we need to identify all the interfaces that accept outside
input. An example of such an interface may be a network socket consisting of a
protocol (TCP or UDP) along with a port number. Another option may be a spe-
cially formatted file that is read by the system. In order to increase the testing cov-
erage, all these interfaces need to be identified before you begin fuzzing. Sometimes
these interfaces will not be obvious. For example, most web browsers not only
parse HTTP, but also FTP, RTSP, as well as various image formats.
The steps in a fuzzing life cycle.
221
8.1.2
Input Generation
The heart and soul of a fuzzer is its ability to create fuzzed inputs. As we’ve dis-
cussed, there are a large variety of different ways in which to create these test cases.
We’ll step through some of the different ways to generate fuzzed inputs, starting
from those that have little or no knowledge of the underlying protocol or structure
of the input and advancing to those that possess a near complete understanding of
the structure of the inputs.
The most basic way to create anomalous input is to simply supply purely ran-
dom data to the interface. While, in theory, given enough time, completely random
data would result in all possible inputs to a program. In practice, time constraints
limit the effectiveness of this approach. The result is, that due to its simplicity, this
approach is unlikely to yield any significant results.
Another way to generate inputs is to use a mutation-based approach. This
method consists of first gathering valid inputs to the system and then adding anom-
alies to these inputs. These valid inputs may consist of a network packet capture or
valid files or command line arguments, to name a few. There are a variety of ways
to add the anomalies to these inputs. They may be added randomly, ignoring any
structure available in the inputs. Alternatively, the fuzzer may have some “built-in”
knowledge of the protocol and be able to add the anomalies in a more intelligent
fashion. Some fuzzers present an interface, either through a programming API or a
GUI, in which information about the format of the inputs can be taught to the
fuzzer. More sophisticated fuzzers attempt to automatically analyze the structure of
the inputs and identify common format occurrences and protocol structures, for
example, ASN.1. Regardless of how it actually occurs, these types of fuzzers work
on the same general principle: starting from valid inputs and adding a number of
anomalies to the inputs to generate fuzzed inputs.
Generation-based fuzzers do not require any valid test cases. Instead, this type
of fuzzer already understands the underlying protocol or input format. They can
generate inputs based purely on this knowledge. Obviously, the quality of the
fuzzed inputs is going to depend on the level of knowledge the fuzzer has about the
underlying input format. Open-source generation-based fuzzers, such as SPIKE and
Sulley, offer a framework in which the researcher can read the documentation,
write a format specification, and use the framework to generate fuzzed inputs based
on the specification. Writing such a specification can be a major undertaking
requiring specialized knowledge and sometimes hundreds of hours of work for
complicated protocols. Since most people lack the specialized knowledge or time to
write such protocol descriptions, commercial vendors exist who provide fuzzers
that understand many common protocols. A drawback of these solutions is if you
are interested in testing an obscure or proprietary format, the fuzzer may not be
pre-programmed to understand it.
8.1.3
Sending Inputs to the Target
After the fuzzed inputs have been generated, they need to be fed into the system’s
interfaces. Some fuzzers leave this up to the user. Most supply some method of per-
forming this job. For network protocols, this may consist of a way to repeatedly
222
Fuzzer Comparison
make connections to a target server, or alternatively, listening on a port for incom-
ing connections for client-side fuzzing. For file format fuzzing, this may entail
repeatedly launching the target application with the next fuzzed file as an argument.
8.1.4
Target Monitoring
Once the fuzzing has begun, the target application must be monitored for faults.
After all, what good is creating and sending fuzzed inputs if you don’t know when
they’ve succeeded in causing a problem? Again, some fuzzers leave this up to user.
Other fuzzers offer sophisticated methods of monitoring the target application and
the system on which they are running. The most basic method is to simply run the
target application under a debugger and monitor it for crashes or unhandled excep-
tions. More sophisticated monitoring solutions may consist of the fuzzer being able
to telnet or ssh into the target system and monitor the target application, logs files,
system resources, and other parameters. Additionally, the fuzzer may launch arbi-
trary user-written monitoring scripts. The more advanced the monitoring being per-
formed, the more vulnerabilities will be discovered. More advanced monitoring
should also speed up analysis, because it may be able to determine exactly which
input (or set of inputs) caused a particular error condition. Another important func-
tion that a target monitoring service can provide is the ability to automatically restart
the target. This may be as simple as restarting an application or as complicated as
power cycling a device or restoring a virtual machine from a snapshot. Such auto-
mated monitoring of the target allows for long, unsupervised fuzzing runs.
8.1.5
Exception Analysis
After the actual inputs have been sent to the target and the dust has cleared, it is
time to figure out if any vulnerabilities have been discovered. Based on the amount
of target monitoring that has taken place, this may require a lot of work on the part
of the user, or it may be pretty much finished. Basically, for each crash or error con-
dition discovered, the smallest sequence of inputs that can repeat this fault must be
found. After this, it is necessary to partition all the errors to find how many are
unique vulnerabilities. For example, one particular bug in an application, say a for-
mat string vulnerability, may be reachable through many different externally facing
functions. So, many test cases may cause a crash, but in actuality they all point to
the same underlying vulnerability. Again, some fuzzers provide tools to help do
some of this analysis for the user, but oftentimes this is where the analyst and devel-
opers will spend significant amounts of time.
8.1.6
Reporting
The final step in the process of fuzzing is to report the findings. For fuzzing con-
ducted during application development, this may involve communicating with the
development team. For fuzzing conducted as part of an outside audit by consult-
ants, this may be a document produced for the client. Regardless of the intent, some
fuzzers can reduce the burden by providing useful statistics, graphs, and even exam-
ple code. Some fuzzers can produce small binaries that can be used to reproduce the
errors for development teams.
8.2
Evaluating Fuzzers
223
8.2
Evaluating Fuzzers
As the last section discussed, fuzzers can vary from simple input generators to com-
plex frameworks that perform program monitoring, crash dump analysis, and
reporting. Therefore, it is difficult to compare fuzzers as they offer such a wide range
of features. We wanted to stay away from a fuzzer review or usability study. Instead,
we will narrow our focus on which fuzzers can create the most effective fuzzed
inputs for a variety of protocols. Even this less ambitious goal is still difficult. How
exactly do you measure which fuzzer generates the most effective fuzzed inputs?
8.2.1
Retrospective Testing
Perhaps the most straightforward approach to evaluating a fuzzer’s effectiveness
is using a testing methodology borrowed from the anti-virus world called retro-
spective testing. In this form of comparison, a particular time period of testing is
selected, say six months. In this example, fuzzers from six months ago would be
archived. Then, this six-month period would be analyzed closely for any vulner-
abilities discovered in any implementation of a protocol under investigation.
Finally, the old versions of the fuzzers would be used to test against these flawed
implementations. A record would be kept as to whether these older fuzzers could
“discover” these now-known vulnerabilities. The longer the retrospective time
period used, the more vulnerabilities will have been discovered, and thus the more
data would be available. The reason that old versions of the fuzzers need to be
used is that fuzzers may have been updated to look for particular vulnerabilities
that have emerged recently. It is common practice for fuzzers to be tested to see
why they failed to find a particular vulnerability; once the deficiency is identified,
they are updated to find similar types of flaws in the future. An example of this
is with the Microsoft .ANI vulnerability discovered in April 2007. Michael
Howard of Microsoft explains how their fuzzers missed this bug but were conse-
quently improved to catch similar mistakes in the future: “The animated cursor
code was fuzz-tested extensively per the SDL requirements, [But] it turns out none
of the .ANI fuzz templates had a second ‘anih’ record. This is now addressed, and
we are enhancing our fuzzing tools to make sure they add manipulations that
duplicate arbitrary object elements better.”1
Retrospective testing is appealing because it measures whether fuzzers can find
real bugs in real applications. However, there are many serious drawbacks to this
type of testing. The most obvious is that the testing will be conducted on an old
version of the product, in the example above, a version that is six months out of
date. A product can have significant improvements in this time period that will be
missed with this form of testing. Another major deficit is the small amount of data
available. In a given six-month time period, there simply aren’t that many vulnera-
bilities announced in major products. Well-written products such as Microsoft’s IIS
or the open source qmail mail server have gone years without vulnerabilities in their
224
Fuzzer Comparison
1blogs.msdn.com/sdl/archive/2007/04/26/lessons-learned-from-the-animated-cursor-security-
bug.aspx
default configuration. Even in the best case, you can only hope one or two bugs
will come out. It is hard to draw conclusions in a comparison using such a small
data set. This problem can be mitigated somewhat by increasing the retrospective
time period. However, as we mentioned, increasing this time period also increases
the amount of time the product is behind the state of the art. For these reasons, we
did not carry out this form of testing.
8.2.2
Simulated Vulnerability Discovery
A method with some of the benefits of retrospective testing but with more data
available is called simulated vulnerability discovery. In this form of testing, a par-
ticular implementation is selected. An experienced vulnerability analyst then goes in
and adds a variety of vulnerabilities to this implementation.2 A different analyst
proceeds to fuzz these flawed implementations in an effort to evaluate how many
of these bugs are “rediscovered.” It is important to have different analysts conduct
these two portions of the testing to remove any potential conflict in the configura-
tion and setup of the fuzzers.
Simulated vulnerability discovery has the advantage of having as much data
available as desired. Like retrospective testing, it also tests the fuzzer’s ability to
actually find vulnerabilities, even if they are artificial in nature. The biggest criticism
is, of course, exactly the artificialness of the bugs. These are not real bugs in a real
application. They will depend heavily on the experiences, knowledge, and peculiar-
ities of the particular analyst that added them. However, even if the bugs are arti-
ficial, they are all still present in the target application and all the fuzzers have the
same opportunity to find (or miss) them. We utilized this type of testing and the
results are presented later in the chapter.
8.2.3
Code Coverage
Another method of testing the effectiveness of a fuzzer is to measure the amount of
code coverage it achieves within the target application. The application is instru-
mented in such a way that the number of lines executed is recorded. After each
fuzzer runs, this information is gathered, and the number of lines executed can be
examined and compared. While the absolute numbers involved from this metric are
fairly meaningless due to the lack of information regarding the attack surface, their
relative size from each fuzzer should shed some insight into which fuzzers cover
more code and thus have the opportunity to find more bugs.
Code coverage data, as a way to compare fuzzers, is relatively straightforward
to obtain and analyze. There are many weaknesses to using it as a metric to com-
pare fuzzers, though. For one, unlike the other forms of testing discussed, this does
not actually measure how good the fuzzer is at finding bugs. Instead, it is a proxy
metric that is actually being used to measure how much of the target application
was not tested. It is up for debate whether high levels of code coverage by a fuzzer
indicate it was effective. Just because a line is executed doesn’t necessarily mean it
8.2
Evaluating Fuzzers
225
2Thanks to Jake Honoroff of Independent Security Evaluators for adding the vulnerabilities in
this study.
is tested. A prime example is standard non-security regression tests. These will get
good code coverage, but they are not doing a good job of “fuzzing.” However, it is
certainly the case that those lines of code not executed by the fuzzer have not been
adequately tested. We use this form of comparison as well and consider later how
it helps to validate our simulated vulnerability discovery testing.
8.2.4
Caveats
Please keep in mind that these results may not necessarily reflect which fuzzer is
right for a particular project. For example, sometimes funding will limit your choice
to open-source fuzzers. Perhaps it is difficult to monitor the application or there is
little follow-up time available. In this case, it may be better to use a fuzzer that is
slightly less effective at generating test cases but has strong monitoring and post-
analysis tools. Also, whether you are fuzzing a common protocol or an obscure or
proprietary protocol will have an impact on your choice because some commercial
fuzzers cannot handle these situations. Finally, this comparison only takes place
over a few protocols and relies heavily on the types and placements of the vulnera-
bilities added for the simulated vulnerability discovery. That said, we feel the results
are valid and the consistent results we obtain from the two methods used for com-
parison validate each other.
8.3
Introducing the Fuzzers
For this testing, we selected a variety of open-source and commercial fuzzers. Some
are similar in design to one another and others are quite different. Between them all,
we hope you can find one that is similar to the fuzzer you are considering using or
have implemented.
8.3.1
GPF
The General Purpose Fuzzer (GPF) is an open-source fuzzer written by one of the
authors of this book. It is a mutation-based network fuzzer that can fuzz server or
client applications. It has many modes of operation, but primarily works from a
packet capture. A valid packet capture needs to be obtained between the target appli-
cation and its server or client. At this point, GPF will continuously add anomalies to
the packets captured and replay them at the target application. GPF parses the pack-
ets and attempts to add the anomalies in as intelligent a way as possible. It does this
through the use of tokAids. tokAids are implemented by writing C parsing code and
compiling it directly into GPF. Using built-in functions, they describe the protocol,
including such features as length fields, the location of ASCII strings, and the location
and types of other delimiters. There are many prebuilt tokAids for common protocols
available in GPF. There are also generic ASCII and binary tokAids for protocols GPF
does not understand and that users don’t wish to implement themselves.
There is one additional mode of operation of GPF called SuperGPF. This mode
only works against servers and only with ASCII-based protocols. It takes as an
argument a file with many “anchors” from the protocol. For example, against
226
Fuzzer Comparison
SMTP the file might contain terms like “HELO,” “MAIL FROM,” “RCPT TO,”
etc. GPF then modifies the initial packet capture file and injects many of these
protocol-specific terms into the initial packet exchange on disk. It then launches a
number of standard GPF processes using these modified packet captures as its ini-
tial input.
GPF does not have any monitoring of analysis features.
8.3.2
Taof
The Art of Fuzzing (Taof) is an open-source, mutation-based, network fuzzer writ-
ten in Python. It, too, works from an initial packet capture. However, unlike GPF,
instead of giving a file that contains a packet capture, Taof captures packets
between the client and server by acting as a man-in-the-middle. These captured
packets are then displayed to the user. Taof doesn’t have knowledge of any proto-
cols. Instead, it presents the packets to the user in a GUI, and the user must dissect
the packets and inform Taof of the packet structure, including length fields. The
amount of work in doing this is comparable to writing a tokAid in GPF and can
take several hours (or more). The types of anomalies that will be injected into the
packets are also configurable. One drawback of Taof is that in its current imple-
mentation, it cannot handle length fields within another length field. The result is
that many binary protocols cannot be fuzzed, including any based on ASN.1.
Taof does not have any monitoring or analysis features.
8.3.3
ProxyFuzz
ProxyFuzz is exactly what it claims it is—a proxy server that fuzzes traffic. It is
incredibly easy to set up and use. Simply proxy live traffic between a client and
server through ProxyFuzz and it will inject random faults into the live traffic. Proxy-
Fuzz does not understand any protocols and no protocol specific knowledge can be
included with it (without making fundamental changes to its design). While it is easy
to set up and use, its lack of protocol knowledge may hinder its effectiveness.
ProxyFuzz does not have any monitoring or analysis features.
Abstract Syntax Notation One (ASN.1)
Abstract Syntax Notation One (ASN.1) is a formal language for abstractly describing messages
to be exchanged among an extensive range of applications. There are many ways of using
ASN.1 to encode arbitrary data, but the simplest is called Basic Encoding Rules (BER). Data
normally comes with an identifier, a length field, the actual data, and sometimes an octet that
denotes the end of the data. Of course, these ASN.1 values can be nested arbitrarily, which can
make for a complicated parsing algorithm.
Programs that implement ASN.1 parsers have a long history of security vulnerabilities.
Microsoft’s ASN.1 libraries have had critical bugs more than once. The open-source standard
OpenSSL has also had critical security vulnerabilities. In 2002, an ASN.1 vulnerability within the
SNMP protocol affected over fifty companies including Microsoft, Nokia, and Cisco. There
were very few Internet-connected systems that were not vulnerable. The irony is that ASN.1 is
used in security protocols such as Kerberos and in security certificates.
8.3
Introducing the Fuzzers
227
8.3.4
Mu-4000
The Mu-4000 is an appliance-based fuzzer from MuSecurity. It is a generation-based
fuzzer that understands approximately 55 different protocols. It is placed on the same
network as the target system and configured and controlled via a web browser.
Within the protocols that it understands, it is extremely easy to use and is highly
configurable. Options such as which test cases are sent at which timing periods can
be precisely controlled. Furthermore, the Mu-4000 can be used as a pure proxy to
send test cases from other fuzzers in order to use its monitoring functions, but oth-
erwise cannot learn or be taught new or proprietary protocols. In other words,
unlike the open-source fuzzers discussed above, which still work on proprietary
protocols, albeit less effectively, the Mu-4000 can only be used against protocols
it understands. Another drawback is that, currently, the Mu-4000 can only be used
to fuzz servers and cannot fuzz client-side applications.
One of the strengths of the Mu platform is its sophisticated monitoring abilities.
It can be configured to ssh into the target machine and monitor the target process,
system and application logs, and system resources. It can restart the application
when the need arises and report exactly what fault has occurred. Another feature is
that, when an error does occur in the target, it will replay the inputs until it has
narrowed down exactly which input (or inputs) caused the problem.
8.3.5
Codenomicon
Codenomicon is a generation-based fuzzer from Codenomicon Ltd. Currently, it
has support for over 130 protocols. As is the case with the Mu-4000, it has no abil-
ity to fuzz any protocols for which it does not already have support. It can be used