### Checkpointing and Failure Prediction Analysis

The following data points represent the performance metrics of two systems, A and B, under different conditions:

- **System A:**
  - 0.90
  - 0.91
  - 0.95
  - 0.97
  - 0.97
  - 0.97

- **System B:**
  - 0.91
  - 0.94
  - 0.95
  - 0.94
  - 0.88
  - 0.92
  - 0.93
  - 0.93
  - 0.84
  - 0.90
  - 0.91
  - 0.92
  - 0.95
  - 0.97
  - 0.98
  - 0.98

**Symbols Used in the Analysis:**
Refer to Table VII for all symbols used in the analysis.

### Checkpointing Methods and Savings

1. **Periodic Checkpointing:**
   \[
   \text{SOCP} = \frac{\text{NOCP} \times T_w}{T_r} \times 100
   \]

2. **Machine Learning (ML) Based Checkpointing:**
   - True Positive Rate: \( P(y = R_x \text{ or } y = R_x / P) \)
   - Total time in checkpointing: \( T_c = T_{sy} \)
   - Savings:
     \[
     S_{ML} = \frac{R_x \times (T_r - T_{sy})}{P \times T_r} \times 100
     \]
   - False Negative Rate: \( \text{NFN} = x - R_x \)
   - Total Savings:
     \[
     S_T = \frac{R_x \times (T_r - T_{sy}) - \text{NOCP} \times R_x \times T_s + \text{NOCP} \times T_w}{R_x \times T_r} \times 100 = \frac{R_x \times T_r - \text{NOCP} \times T_{OCP}}{R_x \times T_r} \times 100
     \]

3. **ML + Periodic Checkpointing:**
   - Number of failures not detected by ML: Using the derivations of SOCP, \( S_{ML} \), and \( S_T \), we calculate the maximum net savings over all possible (precision, recall) pairs for different Mean Time Between Failures (MTBF) and time to take a checkpoint (Ts).

**Results:**
- The results for shared single jobs of System A are shown in Fig. 10. Similar plots for other job types and System B are omitted to save space.
- The normalized area under these curves is presented in Table VIII.
- The ML + periodic checkpointing method outperforms the base optimal checkpointing method by between 12.3% (for an unreliable system with MTBF = 1e4 and Ts = 60s) and 2X (for a reliable system with MTBF = 1e6 and Ts = 60s).
- The savings achieved by the optimal checkpointing method decrease as the system becomes more reliable (MTBF increases from 1e4 to 1e6). This is because the optimal checkpointing period increases, leading to more lost work. This limitation is overcome by integrating it with the ML model.

### Predicting Job Runtime

In this section, we investigate the use case of our resource usage prediction models and show how they can significantly improve the efficiency of shared computing clusters. Many scheduling techniques rely on accurate job runtime estimates to minimize job waiting times. For example, Backfilling [26], [75] is a common technique where the scheduler prefers smaller jobs to jump to the head of the queue provided they do not delay previously queued jobs. Accurate job runtime estimates are essential for Backfilling to be effective in reducing overall job wait time.

**Application and Results:**
- We apply job runtime prediction to both Systems A and B using the pyss [41] simulator, which implements Backfilling scheduling.
- Figure 11 shows the average job wait time for a trace of 15K jobs.
- Insights:
  1. Users tend to overestimate the runtimes of their jobs, causing the scheduler to consider fewer jobs for Backfilling and increasing the overall wait time.
  2. The MCS model performs the best among our four prediction models, achieving an overall wait time within 5% of the oracle for System A and 1% for System B.
  3. Model L performs the worst among our four models but still performs 25% better than user-estimated runtimes for System A and 14% better for System B.

### Threats to Validity

1. **Inferring Failure from Exit Statuses:**
   - Our failure analysis relies on exit statuses from jobs (System A) or individual applications within a job (System B).
   - While there is a guideline provided by TORQUE for exit statuses [59], there is no compulsion for job script or application developers to follow it. However, popular applications (which contribute most data points in our dataset) generally follow the guideline.
   - The job script developer usually takes the exit code of the last executable in the script and returns that, benefiting from the standardization of exit statuses.

2. **I/O Caching and Utilization:**
   - We determine I/O utilization using block and llite counters. Not all 'read bytes' and 'write bytes' values recorded through these counters result in fetching data from the local or remote file system, as the data may be cached in memory.
   - However, these counters are shown to be good estimators of I/O load for the file system [6].

### Related Work

- **Failure Analysis:**
  - Previous studies have focused on hardware reliability rather than job failures [18], [19], [23], [31], [54].
  - Mitra et al. [54] investigated job failures in a university cluster, but our work focuses on the effect of resource usage on job reliability in community clusters.
  - In a related study [19], authors conducted a job failure characterization study on more than 5 million HPC job runs, but the analysis was limited to one system and did not consider resource utilization characteristics.
  - Our approach shows that resource utilization characteristics play an important role in determining job success rates, capturing both system errors and job interference.

- **Performance Optimization and Detection:**
  - Several studies have focused on performance optimization [35]–[37], [62] or detecting inefficient applications [23], [54], [70], [77].
  - Node and job failure prediction models have been proposed [16], [27], [29], [30], [79], [12], [24], [33], [46].
  - Our ML failure prediction model belongs to the job failure category and integrates with existing checkpointing methods.

- **Job Runtime and Resource Usage Prediction:**
  - **Black Box Prediction:** Uses features from job submission scripts [44], [68], [69], [72], [75], [80] or current resource utilization [25], [40], [66].
  - **White Box Prediction:** Utilizes job characteristics for prediction [52], [56], [63], [45], [57], [65], [32], [42], [73], [48], [49].

### Conclusion and Open Challenges

We analyzed extensive system usage and failure data from two centrally administered computing clusters at two Tier 1 US Research universities. Our dataset comprises 3.0M and 2.2M jobs, representing the richest data source in the literature. Key findings include:
- A significant fraction of jobs hit the walltime (33% and 43% for the two systems), necessitating event-driven application-level checkpointing.
- Resource pressure affects different types of jobs differently, with varying correlations between resource usage and job failure rates.
- User historical resource usages can predict current job resource usages, enhancing scheduler effectiveness.
- Job failures due to resource contention can occur at lower levels of utilization than available capacity.

**Open Challenges:**
1. Current optimal checkpointing estimation methods should consider the rate of job progress [55] in addition to hardware reliability.
2. Contention-aware schedulers need to profile jobs to estimate interference and latency sensitivity, which is a limitation for short-running jobs. Our user history-based resource usage predictions can help address this.

### References

- [1] Lies, Damn Lies And SSD Benchmark Test Result. https://www.seagate.com/tech-insights/lies-damn-lies-and-ssd-benchmark-master-ti/
- [2] Random vs Sequential. https://blog.open-e.com/random-vs-sequential-explained/
- [3] UserBenchmark. https://www.userbenchmark.com/
- [4] Nagios. https://www.nagios.com, 2018.
- [5] Sensu. http://www.sonian.com/cloud-monitoring-sensu/, 2018.
- [6] Anthony Agelastos, Benjamin Allan, Jim Brandt, Paul Cassella, Jeremy Enos, Joshi Fullop, Ann Gentile, Steve Monk, Nichamon Naksinehaboon, Jeff Ogden, et al. The lightweight distributed metric service: a scalable infrastructure for continuous monitoring of large scale computing systems and applications. In High Performance Computing, Networking, Storage and Analysis, SC14: International Conference for, pages 154–165. IEEE, 2014.
- [7] Robert Alverson, Duncan Roweth, and Larry Kaplan. The gemini system interconnect. In 2010 18th IEEE Symposium on High Performance Interconnects, pages 83–87. IEEE, 2010.
- [8] George Amvrosiadis, Jun Woo Park, Gregory R. Ganger, Garth A. Gibson, Elisabeth Baseman, and Nathan DeBardeleben. On the diversity of cluster workloads and its impact on research results. In USENIX Annual Technical Conference, 2018.
- [9] Mona Attariyan and Jason Flinn. Automating configuration troubleshooting with dynamic information flow analysis. In OSDI, volume 10, pages 1–14, 2010.
- [10] Saurabh Bagchi, Rakesh Kumar, Rajesh Kalyanam, Stephen Harrell, Carolyn A Ellis, and Carol Song. Fresco: Open source data repository for computational usage and failures (http://www.purdue.edu/fresco), Oct 2019.
- [11] Franck Cappello, Al Geist, William Gropp, Sanjay Kale, Bill Kramer, and Marc Snir. Toward exascale resilience: 2014 update. Supercomputing frontiers and innovations, 1(1):5–28, 2014.
- [12] Xin Chen, Charng-Da Lu, and Karthik Pattabiraman. Failure analysis of jobs in compute clouds: A google cluster case study. In Software Reliability Engineering (ISSRE), 2014 IEEE 25th International Symposium on, pages 167–177. IEEE, 2014.
- [13] Mendel Cooper. Advanced Bash-Scripting Guide. http://tldp.org/LDP/abs/html/exitcodes.html, 2014.
- [14] Domenico Cotroneo, Roberto Natella, Roberto Pietrantuono, and Stefano Russo. A survey of software aging and rejuvenation studies. ACM Journal on Emerging Technologies in Computing Systems (JETC), 10(1):8, 2014.
- [15] J. T. Daly. A higher order estimate of the optimum checkpoint interval for restart dumps. Future Gener. Comput. Syst., 22(3):303–312, February 2006.
- [16] A. Das, F. Mueller, P. Hargrove, E. Roman, and S. Baden. Doomsday: Predicting which node will fail when on supercomputers. In SC18: