0.90
0.91
0.95
0.97
0.97
0.97
A
B
A
B
A
B
A
B
0.91
0.94
0.95
0.94
0.88
0.92
0.93
0.93
0.84
0.90
0.91
0.92
0.95
0.97
0.98
0.98
System A and 1 minute for System B). Therefore the wastage is
reduced to the window from the last monitoring (and attendant
prediction) to the actual failure. Our next analysis shows how
much saving can be achieved using different checkpointing
methods. Refer Table VII for all symbols used in the analysis.
∗ 100
Tr
1) Periodic: SOCP = NOCP Tw
2) ML: True positive = P y = Rx or y = Rx/P .
(cid:4) ∗ 100
Total time in checkpointing, Tc = Tsy.
1 − Ts
SM L = RxTr−Tsy
P Tr
model, NF N = x − Rx.
ST = RxTr−Tsy−NOCP RxTs+NOCP NF N Tw
+ NOCP Tw
=
∗ 100 = R
− NOCP TOCP
xTr
xTr
(cid:3)
(cid:3)
(cid:3)
R
1 − Ts
P Tr
∗ 100
(cid:4) ∗ 100
(cid:4)
Tr
Tr
3) ML+periodic: Number of failures not detected by ML
Using the derivations of SOCP , SM L and ST , we now
calculate the maximum net savings over all possible (precision,
recall) pairs for different MTBF and time to take a checkpoint
i.e., Ts values. The results for shared single jobs of System A
are shown in Fig. 10 (plots are similar for other job types and
System B and are omitted to save space). However, for com-
parison, we do present the results in terms of the normalized
area under these curves in Table VIII. Based on these results,
ML + periodic checkpointing method outperforms the base
optimal checkpointing method by between 12.3% (unreliable
system with MTBF =1e4, Ts=60s) and 2X (reliable system
with MTBF = 1e6 and Ts=60s). Additionally, we observe
the savings achieved by the optimal checkpointing method in
case of failure decreases as a system becomes more reliable
(MTBF increases from 1e4 to 1e6). This is because the optimal
checkpointing period increases and hence the amount of lost
work increases. This limitation is overcome by integrating it
with the ML model.
B. Predicting Job Runtime
In this section, we investigate the use case of our resource
usage prediction models (Sec. V) and show how these can
time for System A and System B
Fig. 11: Overall Job Wait
using different runtime predictors. The values over the bars show
improvements over user estimated runtime
signiﬁcantly improve the efﬁciency of shared computing clus-
ters. Many scheduling techniques rely on job runtime estimates
to minimize job waiting times. For example, many current
batch job schedulers use Backﬁlling [26], [75] whereby the
scheduler prefers smaller jobs to jump to the head of the queue
provided they do not delay previously queued jobs. Therefore,
accurate job runtime estimates are essential for Backﬁlling
to be effective in reducing overall jobs wait time. We apply
job runtime prediction to both systems A & B and use the
pyss [41] simulator, which implements Backﬁlling scheduling
(among others). Figure 11 shows the average job wait time
for System A and System B for a trace of 15K jobs. We draw
several insights:
1) Users tend to overestimate the runtimes of their jobs.
This causes the scheduler to consider fewer jobs for
Backﬁlling and hence increases the overall wait time.
2) The MCS model performs the best among our 4 pre-
diction models, achieving an overall wait time that is
within 5% of oracle for System A and 1% for System B.
3) Between our 4 models, model L performs the worst.
However, despite the simplicity of the model, it still
performs 25% better than user estimated runtimes for
System A and 14% better for System B.
VII. THREATS TO VALIDITY
Inferring failure from exit statuses Our failure analysis
relies on exit status either from a job (System A) or from
individual applications within a job (System B). While there is
a guideline provided by TORQUE for exit status [59], there
is no compulsion for a job script or application developer
to follow the guideline. However, it is generally found by
others [13], [60] and us here that application developers of
popular applications (which contribute most data points in our
dataset) follow the guideline. The job script developer usually
takes the exit code of the last executable in the script and
returns that and thus beneﬁts from the standardization of exit
status.
I/O Caching and Utilization. We determine the I/O utilization
using the block and llite counters. Not all ‘read bytes’
and ‘write bytes’ values recorded through these counters result
in fetching of data from local or remote ﬁle system as the data
may be cached in memory. However, it is shown to be a good
estimator of I/O load for the ﬁle system [6].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
167
VIII. RELATED WORK
Failure analysis of large-scale computing systems [18], [19],
[23], [31], [54] has been done in the past on many scopes
of analysis, such as job-level, application-level, node-level,
or system-level. These works focus on hardware reliability
rather than job failures. In contrast, Mitra et al. [54] performed
investigations of jobs submitted to a university cluster to
understand their primary failure modes. However, our work
focuses on ﬁnding the effect of resource usage on reliability
of jobs in community clusters. In a closely related work [19],
authors conducted job failure characterization study on more
than 5 million HPC job runs. However,
the analysis was
limited to one system and did not consider resource utilization
characteristics for job failure study. Our approach concretely
shows that resource utilization characteristics by a job play an
important role in determining job success rate as it captures
both the impact of error/failure in the system as well as
interference among jobs. In a recent work [8], the authors
studied workload diversity in different clusters. They also
categorize jobs into different categories such as successful,
timeouts, and unsuccessful. However, unlike this work and
several others [12], [21], we delve deeper to determine whether
the job failure is due to user or system-related issues. Or-
thogonal to our analysis, there have been many studies that
have focused on performance optimization [35]–[37], [62]
or to detect inefﬁcient applications [23], [54], [70], [77] on
large-scale clusters. Several node failure [16], [27], [29], [30],
[79], and job failure [12], [24], [33], [46] prediction models
have been proposed in the past as well. A few prior works
such as [58] have performed single-bit error prediction for
GPU as well. Our ML failure prediction model belongs to
the job failure category. We do not claim novelty in our
prediction model,
instead show the utility of this simple
ML model by integrating it with an existing checkpointing
method. A number of works have already explored various
optimal checkpointing strategies. For instance, [74] explores
checkpointing strategies based on temporal locality of failures.
Our proposed checkpointing strategy however is based on job
failure prediction.
Related works for job runtime and resource usage prediction
can be categorized into 2 categories. i) Black box prediction:
The kind of prediction where the characteristic of job is un-
known. The features used for prediction can be either the ﬁelds
from the job submission script [44], [68], [69], [72], [75],
[80] or the current resource utilization of the job [25], [40],
[66]. A recent study [80] uses the parameters used by users
during job submission as features to NN to predict runtime
and IO rate. While proposed model just uses theses parameter
to estimate runtime, the model needs retraining every 100
submitted jobs making it harder to scale for large HPC systems
(having thousands of nodes) where more than 100s of jobs are
submitted every second. Our proposed predictor also belong
to the former class of this category. ii) White box prediction:
These prediction models utilizes the characteristics of jobs
to predict the runtime or resource usage. In some cases the
characteristics of a job are known learnt by the algorithm [52],
[56], [63] or derived from a particular implementation [45],
[57], [65]. In other cases the characteristics are derived from
job executing data on known inputs [32], [42], [73], or by
performing ofﬂine proﬁling of different jobs and predicting
the performance for new unseen jobs [48], [49].
IX. CONCLUSION AND OPEN CHALLENGES
We have analyzed extensive system usage and failure data
from two centrally administered computing clusters at two Tier
1 US Research universities. Our dataset comprises a total of
3.0M and 2.2M jobs from the two clusters, which represents
the richest data source to be analyzed in the literature. The two
clusters vary signiﬁcantly in their scale, hardware resiliency
characteristics, and systems management practices shedding a
comparative light on failure characteristics. Through analysis
of different kinds of data—node failures and recovery, job
failures, and job resource usages —we bring out important
insights into how the clusters behave and implications for
how they can be managed more effectively. Our hope is that
this study will trigger other studies by researchers using data
from their local organization’s compute clusters and we look
forward to seeing which insights will be shared and which
will be distinct. We have also publicly released the dataset
on which the analyses are based. Some of our key ﬁndings
are: (i) A signiﬁcant fraction of jobs hit against the walltime
(33% and 43% for the two systems) and therefore event-
driven application-level checkpointing is needed to avoid lost
work. (ii) Resource pressure affects different types of jobs
differently. Sometime job failure rate is positively correlated
with resource usage (such as local memory), while sometime
it is negatively correlated with remote resource pressure (such
as network usage). These have different implications for how
resource contention should be handled. (iii) User historical
resource usages can be used to predict resource usages of jobs
current in queue. These models can be directly integrated with
the existing schedulers to increase its effectiveness. (iv) Job
failures due to resource contention can be observed with levels
of utilization that is much less the available capacity. This is
observed with memory, local IO, and remote IO as well.
Open challenges: We lay out two open challenges to the
technical community motivated by the observations from our
two production systems. First, current optimal checkpointing
estimation methods take only hardware reliability (such as
MTBF) into account. This paper integrated it with job fail-
ure likelihood information. However, there is still scope for
improvement. A better method is to consider in addition the
rate of job progress [55]. Second, current contention-aware
schedulers [17] need to proﬁle a job ﬁrst to estimate job’s
interference and latency-sensitivity. This is a major limitation
for clusters where majority of jobs are short running. This
paper presents user history-based resource usage predictions,
which can be used to predict a job’s proﬁle that then should
feed into the contention-aware scheduler.
“GIVING UP IS THE ONLY SURE WAY TO FAIL.”
Gena Showalter
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
168
REFERENCES
[1] Lies, Damn Lies And SSD Benchmark Test Result. https://www.seagate.
com/tech-insights/lies-damn-lies-and-ssd-benchmark-master-ti/.
[2] Random vs Sequential. https://blog.open-e.com/random-vs-sequential-
explained/.
[3] UserBenchmark. https://www.userbenchmark.com/.
[4] Nagios. https://www.nagios.com, 2018.
[5] Sensu. http://www.sonian.com/cloud-monitoring-sensu/, 2018.
[6] Anthony Agelastos, Benjamin Allan, Jim Brandt, Paul Cassella, Jeremy
Enos, Joshi Fullop, Ann Gentile, Steve Monk, Nichamon Naksineha-
boon, Jeff Ogden, et al. The lightweight distributed metric service: a
scalable infrastructure for continuous monitoring of large scale com-
puting systems and applications.
In High Performance Computing,
Networking, Storage and Analysis, SC14: International Conference for,
pages 154–165. IEEE, 2014.
[7] Robert Alverson, Duncan Roweth, and Larry Kaplan. The gemini system
In 2010 18th IEEE Symposium on High Performance
interconnect.
Interconnects, pages 83–87. IEEE, 2010.
[8] George Amvrosiadis, Jun Woo Park, Gregory R. Ganger, Garth A.
Gibson, Elisabeth Baseman, and Nathan DeBardeleben. On the diversity
of cluster workloads and its impact on research results.
In USENIX
Annual Technical Conference, 2018.
[9] Mona Attariyan and Jason Flinn. Automating conﬁguration trou-
In OSDI, vol-
bleshooting with dynamic information ﬂow analysis.
ume 10, pages 1–14, 2010.
[10] Saurabh Bagchi, Rakesh Kumar, Rajesh Kalyanam, Stephen Harrell,
Carolyn A Ellis, and Carol Song. Fresco: Open source data repository for
computational usage and failures (http://www.purdue.edu/fresco), Oct
2019.
[11] Franck Cappello, Al Geist, William Gropp, Sanjay Kale, Bill Kramer,
and Marc Snir. Toward exascale resilience: 2014 update. Supercomput-
ing frontiers and innovations, 1(1):5–28, 2014.
[12] Xin Chen, Charng-Da Lu, and Karthik Pattabiraman. Failure analysis of
jobs in compute clouds: A google cluster case study. In Software Reli-
ability Engineering (ISSRE), 2014 IEEE 25th International Symposium
on, pages 167–177. IEEE, 2014.
[13] Mendel Cooper. Advanced Bash-Scripting Guide. http://tldp.org/LDP/
abs/html/exitcodes.html, 2014.
[14] Domenico Cotroneo, Roberto Natella, Roberto Pietrantuono, and Stefano
Russo. A survey of software aging and rejuvenation studies. ACM
Journal on Emerging Technologies in Computing Systems (JETC),
10(1):8, 2014.
[15] J. T. Daly. A higher order estimate of the optimum checkpoint interval
for restart dumps. Future Gener. Comput. Syst., 22(3):303–312, February
2006.
[16] A. Das, F. Mueller, P. Hargrove, E. Roman, and S. Baden. Doomsday:
Predicting which node will fail when on supercomputers.
In SC18: