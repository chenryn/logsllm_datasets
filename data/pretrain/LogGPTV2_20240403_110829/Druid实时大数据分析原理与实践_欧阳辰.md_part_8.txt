节点会同时从实时节点（少量当前数据）与历史节点（大量历史数据）分别查询，然后做一
Segment的查询，当实时节点收到该声明后也会立即向集群声明其不再提供该Segment的查
Segment后，会通过分布式协调服务（Coordination）在集群中声明其从此刻开始负责提供该
Node）去文件存储库，将新生成的 Segment下载到其本地磁盘中。当历史节点成功加载到
储库（DeepStorage）中，随后协调节点（CoordinatorNode）会指导一个历史节点（Historical
中的数据合并操作（Compaction）。合并好的 Segment会立即被实时节点上传到数据文件存
数据块（Segment）。这个过程在实时节点中叫作SegmentMerge操作，也相当于LSM-tree架构
同时，实时节点会周期性地将磁盘上同一个时间段内生成的所有数据块合并为一个大的
event_36791
35789
图3-8实时节点数据块的生成示意图
event 5678
event 234
Diskandpersistedindexes
Heapand in-memory index
event_23593
event_23312
event 780
eyent 3456
Persist
ent45
event_6789
event_5678
ovent 134
Druid实时大数据分析原理与实践
Off-heapmemoryand
Load
even,.70
event 346
---
## Page 67
RDBMS 中的表（Table）。正如前面章节中所介绍的，DataSource的结构包含以下几个方面。
Druid的高性能优势。
3.2.3
第3章架构详解
DataSource结构
·维度列（Dimension）：维度来自于OLAP的概念，用来标识数据行的各个类别信息。
·时间列（TimeStamp）：表明每行数据的时间值，默认使用UTC时间格式且精确到毫
若与传统的关系型数据库管理系统（RDBMS）做比较，Druid的DataSource可以理解为
与Druid架构相辅相成的是其基于DataSource与Segment的数据结构，它们共同成就了
·Druid对命令查询职责分离模式的借鉴也使得自己的组件职责分明、结构更加清晰明
·类LSM-tree架构使得Druid能够保证数据的高速写人，并且能够提供比较快速的实时
Druid的上述架构特点为其带来了如下显著的优势。
·由于Druid在设计之初就不提供对已有数据的更改，以及不实现传统LSM-tree架构
秒级别。这个列是数据聚合与范围查询的重要维度。
基于DataSource与 Segment 的数据结构
了，方便针对不同模块进行针对性的优化。
减少了不少数据处理的工作量，因此让自己在性能方面更胜一筹。
也降低了数据完整性的保障，但Druid相对其他传统的LSM-tree架构实现来说也着实
中普遍应用的WAL原则，虽然这样导致了Druid不适应于某些需要数据更新的场景，
查询，这十分符合许多时序数据的应用场景。
Separating Paradigms - CQRS 
Command
图3-9Druid对命令查询职责分离模式（CQRS）的借鉴
Pereraie
Client
O
Query
DRUID
---
## Page 68
该特点使得Druid不仅能够节省存储空间，而且能够提高聚合查询的效率。
面的情况。
选择对任意的指标列进行聚合（RollUp）操作。该聚合操作主要基于维度列与时间范围两方
相对于其他时序数据库，Druid在数据存储时便可对数据进行聚合操作是其一大特点，
·同维度列的值做聚合：所有维度列的值都相同时，这一类行数据符合聚合操作，比如
·对指定时间粒度内的值做聚合：
无论是实时数据消费还是批量数据处理，Druid在基于DataSource结构存储数据时即可
DataSource结构如图3-10所示。
2011-01-01T02:00:00Z
2011-01-01T02:00:00Z
imestamp
COUNT(1),clicks=SUM(click),revenue=SUM(price)”。图3-11显示的是执行聚合操
标列通常是一些数字，计算操作通常包括Count、Sum和Mean等。
指标列（Metric）：指标对应于OLAP概念中的Fact，是用于聚合和计算的列。这些指
作后DataSource的数据情况。
列的值为同1分钟内的所有行，聚合操作相当于对数据表所有列做了Group By操
2011-01-01T02:00:00z:ultratrimfast.com
2011-01-01T02:00:00Z
2011-01-01T01:04:51Z:
2011-01-01101:01:35Z
timestamp
时间列
：00Z
bieberfever.com
ultratrimfast.com
ultratrimfast.com
publisher
publisher
图3-11
ultratrimfast.com
bieberfever.com
图3-10
DataSource聚合后的数据情况
：符合参数queryGranularity指定的范围，比如时间
google.com
google.com
advertiser
DataSource结构
维度列
google.com
google.com
google.com
google.com
google.com
advertiser
Male
Male
gender
Female
Female
Female
Male
gender
USA
countryimpressions clicksrevenue
Druid实时大数据分析原理与实践
USA
country:click price
1953
2912
1800
一
指标列
170
25
0.
18
65
34.01
15.70
1
---
## Page 69
扩展（Extension）的形式轻易地在Druid平台上进行加载与更换。
求的用户，因此它通过实现了一个扩展系统（Extension System），让很多组件功能能够通过
Segment中使用了Bitmap等技术对数据的访问进行了优化。
表数据范围查询，这使效率得到了极大的提高。通过 Segment将数据按时间范围存储如图
时间范围查询数据时，仅需要访问对应时间段内的这些Segment 数据块，而不需要进行全
数据块中，这便是所谓的数据横向切割。这种设计为Druid带来一个显而易见的优点：按
通过参数segmentGranularity的设置，Druid将不同时间范围内的数据存储在不同的Segment
Segment实现了对数据的横纵向切割（Slice and Dice）操作。从数据按时间分布的角度来看，
2.Segment 结构
3.3.1
3.3
3-12所示。
第3章架构详解
druid-avro-extensions
名称
Druid在设计之初就希望自已的系统功能能够比较轻松地被扩展，特别是对于有特殊需
DataSource是一个逻辑概念，Segment 却是数据的实际物理存储格式，Druid 正是通过
目前Druid项目自带的一些扩展如下。
同时，在Segment中也面向列进行数据压缩存储，这便是所谓的数据纵向切割。而且在
扩展系统
主要的扩展
2011-01-0102:010
2011-01-01101:00:002
2011-01-01100:04:512
t.imestanp
01-01F
0:0
图3-12
JustinBieber
KeSha
Justin Bieber
page
Segment 2011-01-01T02/2011-01-01T03
Segment2011-01-01T00/2011-01-01T01
通过Segment将数据按时间范围存储
legment2011-01-01101/20110l-01T02
的parser部分进行指定使用
使得Druid能够摄人avro格式的数据。
描述
en.
languagecity
Calgary CA
USA
country..
1152
一般可在dataSchema中
5
---
## Page 70
java -classpath "/my/druid/library/*" io.druid.cli.Main tools pull-deps --Clean -c io.
用如下命令。
3.3.2
druid-rabbitmq
druid-kafka-eight-simpleConsumer
druid-cloudfles-extensions
druid-cassandra-storage
druid-azure-extensions
名称
postgresql-metadata-storage
mysql-metadata-storage
druid-kafka-extraction-namespace
druid-kafka-eight
druid-namespace-lookup
druid-datasketches
druid-histogram
druid-s3-extensions
druid-hdfs-storage
名称
druid.extensions:mysql-metadata-storage:0.9.0-cio.druid.extensions.contrib:druid
通过 Druid 自身提供的pull-deps工具下载依赖到本地仓库。比如想要利用 pull-deps下
目前Druid社区贡献的一些扩展如下。
下载与加载扩展
使用PostgreSQL作为元数据数据库
使用MySQL作为元数据数据库
基于Kafka的命名空间发现，依赖于命名空间发现扩展
API)
消费Kafka数据的Firehose（使用KafkaHighLevel Consumer
命名空间发现
使得Druid的聚合操作能够使用datasketches库
对histograms值进行近似计算
使用AWSS3作为DeepStorage
使用HDFS作为DeepStorage
Graphitemetrics的发射器（emitter）
基于rabbitmq的Firehose
API)
消费Kafka数据的Firehose（使用KafkaLow Level Consumer
描述
描述
Druid实时大数据分析原理与实践
续表
---
## Page 71
的查询服务。
过 Zookeeper向集群声明其负责提供该Segment数据文件的查询服务。
数据信息后，将其根据规则的设置分配给符合条件的历史节点。
所有数据块合并成一个大的Segment数据文件。
模块Plumber（具体实现有RealtimePlumber等）按照指定的周期，按时将本周期内生产出的
eight-simpleConsumerFirehose。同时，实时节点会通过另一个用于生成Segment数据文件的
据的druid-kafka-eightFirehose，以及社区贡献的基于Kafka LowLevel API实现的druid-kafka-
以有不同的具体实现，比如Druid自带的基于KafkaHigh Level API实现的用于消费Kafka数
其独到的设计使其拥有超强的数据摄入速度。
3.4
第3章架构详解
3.4.1
（5）实时节点丢弃该Segment数据文件，并向集群声明其不再提供该Segment 数据文件
（4）历史节点得到指令后会主动从DeepStorage中拉取指定的 Segment数据文件，并通
Segment数据文件的传播过程如图3-13所示。
（3）Master节点（即Coordinator节点）从MetaStore里得知Segment数据文件的相关元
实时节点通过Firehose来消费实时数据。Firehose是Druid中消费实时数据的模型，
实时节点（Reatime Node）主要负责即时摄入实时数据，以及生成 Segment数据文件，
（2）Segment数据文件的相关元数据信息被存放到MetaStore（即MySQL）里。
（1）实时节点生产出Segment数据文件，并将其上传到DeepStorage中。
Segment数据文件从制造到传播要经历一个完整的流程，步骤如下。
。将扩展加载到Druid Service的classpath中，Druid 便能加载到相关扩展。
加载扩展有以下两种方法。
client:2.4.0
rabbitmq:0.9.0-h org.apache.hadoop:hadoop-client:2.3.0-h org.apache.hadoop:hadoop-
实时节点
Segment数据文件的制造与传播
可
---
## Page 72
kafka-eightFirehose的消费方案的高可用性方面的一个缺陷。解决这个问题一般有两种方式。
age上且被其他历史节点下载的Segment数据却会被集群所遗漏，从而形成了这个基于druid-
实时节点分配并且被消费，但是该不可用实时节点上的已经被消费，尚未被传送到DeepStor-
点不可用时，该方法虽然能够保证它所负责的Partition里未曾被消费的数据能被其他存活的
件，从而保证了实时节点能够轻松实现线性扩展。
现了这个角度上的高可用性。同样的道理，当集群中添加新的实时节点时也会触发相同的事
未曾被消费的数据，从而保证所有数据在任何时候都会被Druid集群至少消费一次，进而实
配，接着所有节点将会根据记录在Zookeeper集群里的每一个Partition的Offset来继续消费
点不可用时，该KafkaConsumerGroup会立即在组内对所有可用节点进行Partition的重新分
后，会主动将数据消费进度（Kafka Topic Ofset）提交到Zookeeper集群。这样，当这个节
同一个Partition不会被多于一个的实时节点消费。当每一个实时节点完成部分数据的消费
的数据，各个节点会负责独立消费一个或多个该Topic所包含的Partition的数据，并且保证
我们可以使用一组实时节点组成一个KafkaConsumerGroup来共同消费同一个KafkaTopic
用pull的方式从Kafka获取数据，而该Firehose能够让Druid有较好的可扩展性和高可用性。
消费Kafka数据时一般使用Druid自带的用于消费Kafka数据的druid-kafka-eightFirehose，使
3.4.2
可惜的是，这个方法并不是毫无破绽的。因为，当一个KafkaConsumerGroup内的实时节
从设计上看，实时节点拥有很好的可扩展性与高可用性。对于Druid0.7.3版本来说，
高可用性与可扩展性
Realtime
Deep Storage
图3-13Segment数据文件的传播过程
Ata)
MySol
MySal
Master
Master
writes(ephemeralnode)
Zookeeper
Druid实时大数据分析原理与实践
reads(epemeralnooe)
www.websequencediagrams.con
Cowpute
Compute
---
## Page 73
文件。无论是何种查询，历史节点都会首先将相关 Segment数据文件从磁盘加载到内存，然
数据文件，然后从DeepStorage中下载属于自己但目前不在自己本地磁盘上的Segment数据
3.5.1
的数据文件有不可更改性，因此历史节点的工作就是专注于提供数据查询。
3.5
第3章架构详解
后再提供查询服务，如图3-14所示。
历史节点在启动的时候，首先会检查自己的本地缓存（LocalCache）中已存在的 Segment
历史节点（Historical Node）负责加载已生成好的数据文件以提供数据查询。由于Druid
·使用 Tranquility与索引服务（Indexing Service）对具体的Kafka TopicPartition进行精
·想办法让不可用的实时节点重新回到集群成为可用节点，那么当它重启的时候会将之
历史节点
的任务失败时，系统依然可以准确地选择使用另外一个相同任务所创建的Segment数
群，因此它可以同时对同一个Partition制造多个副本（replica）。所以当某个数据消费
确的消费与备份。由于Tranquility可以通过push的方式将指定的数据推向Druid集
上传到DeepStorage，保证了数据的完整性。
前已经生成但尚未被上传的Segment数据文件统统都加载回来，并最终会将其合并并
据块。
内存为王的查询之道
Deep Storage
图3-14历史节点加载Segment数据文件
Segment
createkey
download
Disk
emor
Segment
Load
---
## Page 74
往会要求将硬件资源根据性能容量等指标分为不同的组，并为这些硬件都打上组标签。
度，数据可被笼统地分为以下三类。
能不同；而数据温度则是用来形容数据被访问的频繁程度一
之间找到一个满足实际需求的平衡点。所谓硬件的异构性是指在集群中不同节点的硬件性
3.5.2
责的Segment数据文件大小之比成正比关系。
数就多，查询速度就相对较慢。因此，原则上历史节点的查询速度与其内存空间大小和所负