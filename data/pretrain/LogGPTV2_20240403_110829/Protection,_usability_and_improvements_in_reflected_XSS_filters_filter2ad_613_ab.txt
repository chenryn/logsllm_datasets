hyperlink in a web page or email, or may be the result of
execution of scripts on a page that is currently being dis-
played. This submission may use a GET or POST request,
and will include parameter data that is under the control of
the web page or email containing the link.
In Step 2, the web site returns a response to the browser’s
request. This leads to Step 3, when a new document is
created by the browser. In this step, the browser invokes the
Init method of XSSFilt, providing information about the
request submitted in Step 1. XSSFilt parses the URL and
POST data for parameters and converts them into a list of
(name, value) pairs, which enables more accurate techniques
for inferring reﬂected content as compared to XSSAuditor.
The ﬁlter then returns control to the browser so as to start
rendering the page.
In Step 4, the web browser’s internal HTML parser is used
to parse the document received in Step 2. This causes the
creation of various nodes in the document tree, including
script and text nodes in Step 5.
In Step 6, a script node
would normally be sent to the JavaScript engine, but the
browser intercepts the script and sends it to the Permits
method of XSSFilt. At Step 7, XSSFilt uses an approximate
substring matching algorithm to search for one or more of
the GET/POST parameters inside the script. Any match-
ing content is deemed reﬂected or tainted. Further details
on this detection technique can be found in Section 4. If the
tainted components of a script violate the policies described
in Section 5 then the execution of the script is blocked. Oth-
erwise, it is handed over to the JavaScript engine in Step 8.
Note that new script content may be created during script
execution, e.g., due to the execution of the eval operation.
Our architecture ensures that such newly created scripts are
passed to the permit operation of XSSFilt in Step 9, thus
ensuring that these dynamically created scripts are checked
for XSS in the same manner as the scripts included statically.
It is also possible for script execution to result in the
creation of new HTML content, e.g., as a result of docu-
ment.write or setting innerHTML attribute of some DOM
nodes. In all these cases, the HTML parser will be invoked
in Step 10, and Steps 5 through 8 will be repeated. This
ensures that DOM-based XSS attacks are handled the same
way as XSS attacks contained within static content received
in Step 2.
4.
IDENTIFYING REFLECTED CONTENT
Detection of reﬂected content is a taint analysis problem,
where the response page components that are directly de-
rived from request data are to be considered tainted. XSS
defenses implemented on the server side can rely on taint-
tracking instrumentation on the server code for accurate
detection of taint, but this is obviously not a choice for a
browser-resident defense. Thus, the only option is to infer
possible taint by comparing the input to the server and its
output. A known limitation of such an approach is that
if data goes through complex transformations, then there
would be no match between the input and output and hence
no taint can be inferred. Fortunately, the transformations
used by most web applications seem to consist of character
encodings and simple sanitizations, and these can be accom-
modated in a taint inference algorithm.
The core of our taint inference algorithm is the same as
that of Reference [21]. However, since XSSFilt is resident on
a browser, there are diﬀerences in terms of identifying taint
sources, recognizing tainted content, and a few additional
optimizations.
1. The URL is parsed into a list of (name, value) parame-
ters. The parameter name is used for reporting purposes,
but is of no other interest to XSSFilt. This decomposi-
tion into parameters is necessary to detect partial script
injections. If the URL cannot be parsed properly, or if
special characters are present in the URL path (or if they
span more than one parameter), the entire path is also
appended as a single parameter. This step ensures that
the technique would not fail for applications that use non-
standard parameter encoding, but instead will operate in
a degraded mode where it can detect whole-script injec-
tion.
2. As an optimization, parameters whose content cannot
possibly include JavaScript or HTML code are omitted
from further consideration. Speciﬁcally, we discard pa-
rameters shorter than 8 characters, and parameters con-
taining only alphanumeric characters, underscores, spaces,
dots and dashes. These characters are commonly used in
benign URLs. However, even if these parameter values
are included in the returned page, the resulting content
will not match the policies described in Section 5, and
hence ignoring them will not cause attacks to be missed.
3. Before any inline script is executed, an approximate sub-
string matching algorithm is used to establish a relation-
ship between the parameters and the script. If the param-
eter is longer than the script, then the script is searched
within the parameter, to detect whole script injection.
On the other hand, if the script is longer than the param-
eter, then the parameter is searched within the script, to
detect partial script injection.
A similar check is performed before an external script is
fetched for execution. If the script URL is longer than
the parameter, then the parameter is searched within
the URL to detect hijacking of existing external scripts,
where the attacker is able to point them to a malicious
domain. Otherwise the URL is searched within the pa-
rameter to account for whole script injection of an exter-
nal script name.
Previous browser-resident techniques for XSS detection, in-
cluding XSSAuditor [3] and noXSS [10] use exact substring
matching rather than an approximate substring matching to
identify reﬂected content. Another diﬀerence is that XS-
SAuditor does not parse parameters and hence it can only
detect those cases where an entire script is injected, while
XSSFilt can detect partial script injections as well. The
main advantages of XSSAuditor’s approach are:
• Faster runtime performance: exact substring matching
has linear-time complexity, and hence can provide better
performance over the quadratic-time worst-case complex-
ity of approximate matching.
• Lower false positive rate: This is because (a) exact match-
ing is stricter than approximate matching, and (b) like-
lihood of coincidental matches for the entire script is
smaller than that for any of its substrings.
Our approach, on the other hand, has complementary strengths:
• Coping with application-speciﬁc sanitizations: Approxi-
mate string matching is better able to cope with application-
speciﬁc sanitizations that may take place, e.g., when a
‘*’ character is replaced by a space. In contrast, an ex-
act matching algorithm will fail to match even if a single
such substitution takes place. Our results in Section 7, as
well the results of References [1] and [21] show that such
application-speciﬁc sanitizations do occur in practice.
• Partial script injections: As described in the introduction
(Figure 1), template-based web application frameworks
create natural opportunities where an existing script could
be modiﬁed by injecting a parameter value into its mid-
dle. In this case, there would not be a match for the whole
script, and hence XSSAuditor would miss such injections.
As we show in the evaluation section, such partial script
injection vulnerabilities are relatively common.
Although the results in Reference [21] seem to indicate that
the above beneﬁts could be obtained without undue perfor-
mance overheads or false positives, a more careful examina-
tion indicates that those results are not necessarily applica-
ble for client-side XSS defense:
• The false positive evaluation in Reference [21] was done
in the context of SQL injection, speciﬁcally on simple
web applications. In contrast, a browser-side XSS defense
needs to avoid false positives on virtually all applications
that have been deployed on the web.
• In terms of performance as well, the results in Refer-
ence [21] were obtained using a SQL injection data set.
The volume of data subjected to approximate matching
and policy checking are thus much smaller than that in-
volved in HTTP requests and responses, and hence per-
formance constraints are more stringent for XSS-defense
within a browser.
Thus, it was unknown, prior to this work, whether a client-
side XSS defense can beneﬁt from the strengths of approx-
imate matching without incurring its drawbacks. Our eval-
uation answers this question aﬃrmatively. Section 7 shows
that XSSFilt beneﬁts from the increased power of approxi-
mate matching, while minimizing its drawbacks.
5. XSS POLICIES
XSSFilt is targeted at inexperienced users who are not
expected to deal with false positives. We seek to provide a
ﬁlter reliable enough to be enabled by default on a main-
stream browser.
Previous research on injection attacks on web applications
showed that a few generic policies can detect a wide range
of attacks. In particular, Su et al [24] proposed the syntactic
conﬁnement policy that conﬁnes tainted data to be entirely
within certain types of nodes of a parse tree for the target
language (e.g., SQL or JavaScript). A lexical conﬁnement
policy has been used successfully by others [21]. However,
these works primarily targeted SQL injection, which is rel-
atively simple. In contrast, XSS is more challenging due to
the diversity of injection vectors and the many evasion tech-
niques available to attackers. Below, we describe policies
that address these diﬃculties in a systematic manner.
5.1
Inline Policy
The inline policy is used for protecting against XSS at-
tacks embedded in inline content. Speciﬁcally, the following
types of content are addressed.
A. Inline code: This category includes code embedded di-
rectly in the web page using one of the following mecha-
nisms:
i. Inline scripts: Script content, enclosed between  tags
ii. Event listeners: Code enclosed in an event handler
speciﬁcation, e.g., 
iii. JavaScript URLs: Code provided using JavaScript pro-
tocol, e.g., 
iv. Data URL: These provide a general mechanism to in-
clude inline data, which may be text, images, HTML
documents, etc. The following data URL embeds the
text “Hello” in base64 encoding:
data:text/plain,SGVsbG8=
B. Dynamically created code: New code may come into
being when a value stored in a variable is eval’d or used
in an operation such as setTimeout.
The simplest (and most restrictive) inline policy is one
that prohibits any part of a script from being tainted. Unfor-
tunately, this policy produces many false positives because
it is common for scripts to contain data from HTTP parame-
ters. For this reason, we implemented a lexical conﬁnement
policy that restricts tainted data to be contained entirely
within a limited set of tokens.
In practice, we discovered
that the data injected is normally inside strings. All of the
attacks in our dataset consisted of such injections within
strings. We therefore specialized the policy to ensure that
tainted data appears only within string literals, and does not
extend before or after the literal. This policy was suﬃcient
to avoid false positives.
5.2 External Policy
This policy is enforced on external code that is speciﬁed
by a name, i.e., injection vector (C) described below.
C. External code: Code that is referenced by its name us-
ing one of the following mechanisms:
i. External scripts: Script name provided using a script
tag, e.g., 
ii. Base tags: These can be used to achieve an eﬀect sim-
ilar to external script injection by implicitly chang-
ing the URL from where scripts in the document are
loaded.
iii. Objects: Similar to external scripts, but embedded be-
tween  and  tags.
External policy addresses these injection vectors. It is ap-
plied to the name of external scripts or objects as follows.
1. If the host portion of the URL is untainted, then the
script is allowed. Note that an attacker cannot typi-
cally upload a malicious script onto a server controlled
or trusted by a web application. For this reason, the
attacker needs to control the host portion of the URL.
Unlike the host component, our policy permits the path
component of a URL to be tainted, since some web appli-
cations may derive script names from parameter values.
2. Even if the host portion of the URL is tainted, our policy
permits the script if it is from the same origin. We use
a relaxed same-origin check [5] which veriﬁes if the regis-
tered domains of the URLs match. Thus, www.google.com
is considered same-origin with reader.google.com.
3. Finally, if the tainted domain was previously involved in
a check that was deemed safe, then it is allowed. Intu-
itively, XSSFilt assigns trust on a per-domain basis, and
considers all requests from the same domain as trusted
or untrusted.
6. SECURITY ANALYSIS
In this section we identify possible attack vectors and
strategies that may be used for an XSS attack, and argue
how our design addresses these threats.
We expect an attacker to deploy the full range of tech-
niques available to evade detection by XSSFilt. There are
two logical steps involved in the operation of XSSFilt: (I)
recognizing script content in the victim page, (II) identify-
ing if this code is derived from request data. The following
techniques may be used to defeat Step I:
1. Exploit all of the above-mentioned vectors to inject code,
hoping that one of more of the vectors may not be (cor-
rectly) handled by XSSFilt. However, the ﬁlter archi-
tecture makes it very simple to enforce complete media-
tion [20], since there is a very small number of code paths
that call into the JavaScript engine. IE and NoScript rely
on the completeness of their regular expressions, which
need to strike a complicated balance between usability
and protection.
2. Exploit various browser parsing quirks to prevent XSS-
Filt from recognizing one or more of the scripts in the
victim page. Note, however, that browser quirks pose
a problem for techniques that attempt to detect scripts
by statically parsing HTML. In contrast, XSSFilt oper-
ates by intercepting scripts dispatched to the Javascript
engine, and hence does not suﬀer from this problem.
3. Exploit DOM-based attacks:
If the victim page uses a
script to dynamically construct the page, e.g., by setting
the innerHTML attribute, then try to defer script injec-
tion until this time. This technique defeats ﬁlters that
scan for scripts at the point the response page is received.
However, XSSFilt’s architecture ensures that all scripts,
regardless of the time of their creation, are checked before
their execution. Hence, XSSFilt is not fooled by DOM-
based attacks.
4. Exploit sanitization to modify the parse tree and force
the parser to interpret another part of the page as script.
This vulnerability existed in Internet Explorer [17], whose
ﬁlter deactivated script nodes by modifying the 
tag, and can potentially exist in NoScript. In contrast,
XSSFilt’s decision to block the execution of a script has
no eﬀect on how the rest of the page is parsed.
To defeat Step II, an attacker may use the following tech-
niques:
5. Employ partial rather than whole script injection: We
have already described how our taint inference implemen-
tation (Section 4), together with the policies described in
Section 5, can detect partial injections.
6. Employ character encodings such as UTF-7 to throw oﬀ
techniques for matching requests and responses. Note
that by the time a browser interprets script content, it has
already determined the character encoding to be used.
Therefore XSSFilt applies the corresponding decoding
operation before the taint inference step, and thus thwarts
this evasion technique. NoScript might get confused be-
cause the encoding of the response is not known at re-
quest time.
7. Exploit custom sanitizations performed by an application
to evade taint inference. As described before, XSSFilt
uses approximate substring matching, which provides a
degree of resilience against application-speciﬁc sanitiza-
tions employed by web applications. However, if a web
application makes extensive use of non-standard charac-
ter transformations, it may be possible to exploit them to
evade XSSFilt. It seems unlikely that any purely client-