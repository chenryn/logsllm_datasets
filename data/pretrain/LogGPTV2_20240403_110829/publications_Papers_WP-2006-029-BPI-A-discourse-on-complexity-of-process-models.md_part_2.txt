A Discourse on Complexity of Process Models 121
The CFC −split, CFC − split, and CFC −split functions is
XOR OR AND
calculated as follows:
– CFC −split(a) = fan−out(a). The control-flow complexity of XOR-
XOR
splits is determined by the number of branches that can be taken.
– CFC −split(a)=2fan−out(a)−1.Thecontrol-flowcomplexityofOR-splits
OR
is determined by the number of states that may arise from the execution of
an OR-split construct.
– CFC −split(a)=1. For an AND-split, the complexity is simply 1.
AND
The higher the value of CFC −split, CFC −split, and CFC −
XOR OR AND
split, the more complex is a process design, since developer has to handle all
the states betweencontrol-flowconstructs (splits) and their associatedoutgoing
transitionsandactivities.Eachformulatocalculatethecomplexityofasplitcon-
struct is based on the number of states that follow the construct. CFC analysis
seeks to evaluate complexity without direct execution of processes.
TheadvantagesoftheCFCmetricisthatitcanbeusedasamaintenanceand
quality metric, it gives the relative complexity of process designs, and it is easy
to apply. Disadvantages of the CFC metric include the inability to measure
data complexity, only control-flow complexity is measured. Additionally, the
same weight is placed on nested and non-nested loops. However, deeply nested
conditional structures are harder to understand than non-nested structures.
2.7 Adapting the Halstead Complexity Metric
The measures of Halstead [10] are the best known and most thoroughly studied
composite measure of software complexity. The measures were developed as a
means ofdetermining a quantitativemeasure ofcomplexity basedona program
comprehensionasafunction ofprogramoperands(variablesandconstants)and
operators(arithmeticoperatorsandkeywordswhichalterprogramcontrol-flow).
Halstead’s metrics comprise a set of primitive measures (n1, n2, N1, and N2)
that may be derived from the source code:
– n1 = number of unique operators (if, while, =, ECHO, etc);
– n2 = number of unique operands (variables or constants);
– N1 = total number of operator occurrences;
– N2 = total number of operand occurrences.
In our work, we suggest to map business process elements to the set of prim-
itive measures proposed by Halstead. For example, n1 is the number of unique
activities, splits and joins, and control-flow elements (such as sequence, switch,
while, etc. in BPEL) of a business process.While the variable n2 is the number
of unique data variables that are manipulated by the process and its activities.
N1 and N2 can be easily derived directly from n1 and n2. With these primitive
measuresweintroducethenotionofHalstead-basedProcessComplexity(HPC)
measures for estimating process length, volume, and difficulty. These measures
are based on Halstead measures and are calculates as follows:
122 J. Cardoso et al.
– Process Length: N = n1*log2(n1)+ n2*log2(n2)
– Process Volume: V = (N1+N2)*log2(n1+n2)
– Process Difficulty: D = (n1/2)*(N2/n2)
By the means of the presented mapping we can design an additional mea-
sure for processes based on the original measurement proposed by Halstead,
including the processlevel,efforttoimplement, time toimplement, andnumber
of delivered bugs. We do not formalize these measurements since they require
calibration that can only be done with empirical experiments.
Using HPC measures for processes has several advantages. The measures do
notrequirein-depthanalysisofprocessstructures,theycanpredictrateoferrors
and maintenance effort, they are simple to calculate, and they can be used for
most process modelling languages.
2.8 Adapting the Information Flow Metric by Henry and Kafura
HenryandKafura[12]proposedametricbasedontheimpactoftheinformation
flow in a program’ structure. The technique suggests identifying the number
of calls to a module (i.e. the flows of local information entering: fan-in) and
identifyingthe numberofcallsfromamodule(i.e.the flowsoflocalinformation
leaving: fan-out). The complexity of a procedure (PC) is defined as:
PC = Length * (Fan-in * Fan-out)2
The value of the variable length can be obtained by applying the lines of
codeoralternativelytheMcCabe’scyclomaticcomplexitymetric.Theprocedure
complexities are used to establish module complexities. A module with respect
to adatastructureDS consistsofthoseprocedureswhicheither directlyupdate
DS or directly retrieve information from DS. As it can be seen, the measure is
sensitiveto the decompositionofthe programintoproceduresandfunctions,on
the size and the flow of information into procedures and out of procedures.
Henry and Kafura metric can be adapted to evaluate the complexity of pro-
cessesinthefollowingway.Tocalculatethelengthofanactivityweneedfirstto
identify if activities are seen as black boxes or white boxes by the business pro-
cess management system. If activities are black boxes then only their interface
is known. Therefore, it is not possible to calculate the length of an activity. In
thissituationweassumethelengthtobe1.Ifactivitiesarewhiteboxesthenthe
length of an activity is based on knowledge of its source code. In this situation,
the length can be calculated using traditional software engineering metrics that
have been previously presented, namely the LOC and MCC.
The fan-in and fan-out can be mapped directly to the inputs and outputs of
activities.Activitiesareinvokedwhentheirinputs (fan-in)areavailableandthe
activitiesarescheduledforexecution.Whenanactivitycompletesitsexecution,
itsoutputdataistransferredtotheactivitiesconnectedtoitthroughtransitions.
We propose a metric called interface complexity (IC) of an activity which is
defined as:
A Discourse on Complexity of Process Models 123
IC = Length * (number of inputs * number of ouputs)2
The advantages of the IC metric are that it takes into account data-driven
processes and it can be calculated prior to coding, during the design stage. The
drawbacks of the metric are that it can give complexity values of zero if an
activity has no external interactions. This typically only happens with the end
activities of a process.This means the, for example,EPC processeswith a large
percentage of end activities will have a low complexity.
3 Cognitive Science on Software Complexity
Mostapproachesinthe softwareengineeringdomaintakecertaincharacteristics
ofsoftwareasastartingpointandattempttodefinewhateffecttheymighthave
on the difficulty of the various programmer tasks (e.g. maintaining, testing and
understanding code). In [5], it is argued that it is much more useful to analyse
theprocessesinvolvedinprogrammertasksfirst,aswellastheparameterswhich
governthoseefforts:“..oneshouldstartwiththesymptomsofcomplexity,which
are all manifested in the mind, and attempt to understand the processes which
producesuchsymptoms”.Usingresultsfromcognitivesciences,e.g.thedivision
of the mind into short-term and long-term memory, and the mental processes
involved with programming known as “chunking” and “tracing”, Cant et al.
come up with a set of tentative complexity metrics for software programs [5].
A similar approach for determining the complexity of a process model would
be to determine meaningful process model “chunks”, which can be captured as
a single section in the short-term memory. One could think of constructions
like a (short) sequence of activities or a control construct like an XOR-split.
Each of these “chunks” would have to be characterized by a complexity score.
The work in [5] suggests that notably the size of the chunk would be a good
estimate. Next, it is necessary to see the control flow through these chunks, as
people need to scan the relations between chunks to understand the complete
picture. This is referred to as the “tracing” mechanism. In [5], not only the
lengthofthepathbutalsothekindofdependencyinfluencesthecomprehension
of the flow between chunks. For software, for example, Cant et al. state that
“a conditional control structure is more complex to understand than a normal
sequential section of code”. For a process model, this could mean that both (a)
the distance between the chunks and (b) a complexity factor for the specific
kind of dependency should be used. Unfortunately, the work in [5] rather sets
an agenda for complexity metrics than providing exact measures. Therefore, it
is far from straightforward to transfer the presented, tentative relations to the
process modelling domain.
4 Complexity of the Process Graph
Graph theory provides a rich set of graph metrics or graph measures that can
be adapted for calculation of the complexity of the process graph. In [14] the
124 J. Cardoso et al.
coefficientofnetworkcomplexity(CNC), the complexityindex (CI), the restric-
tiveness estimator (RT), and the number of trees in a graph are discussed as
suitable for business process models.
The coefficient of networkcomplexity (CNC) provides a rather simple metric
for the complexity of a graph. It can easily be calculated as the number of arcs
divided by the number ofnodes. In the contextof a business process model,the
number of arcs has to be divided by the number of activities, joins, and splits.
In formal esthetics this coefficient is also considered with the notion of elegance
[21].
CNC = number of arcs / (number of activities, joins, and splits)
The complexity index (CI), or reduction complexity is defined as the mini-
mal number of node reductions that reduces the graph to a single node. This
measure shares so similarity to the notion of structuredness of a process graph
and respective reduction rules. In a BPELprocess it can be associatedwith the
number of structured activities. The complexity index of a process graph has
to be calculated algorithmically and is not applicable for process models with
arbitrary cycles.
Restrictiveness estimator (RT) is an estimator for the number of feasible se-
quences in a graph. RT requires the reachability matrix r , i.e. the transitive
ij
closure of the adjacency matrix, to be calculated.
RT = 2Σr −6(N −1)/(N −2)(N −3)
ij
There are further measures in graph theory which demand rather complex
computations. The number of trees in a graph requires the tree-generating de-
terminant to be calculated based on the adjacency matrix (see [14]). Measures
such as tree width, directed tree width, and directed acyclic graph width are
comparedin [22]. The latter measures how close a graphis to a directed acyclic
graph.
5 Contributions and Limitations
Inthis paper,wehavesurveyedseveralcontributionsfromsoftwareengineering,
cognitive science, and graph theory, and we discussed to what extent analogous
metrics and measurements can be defined for business process models. In or-
der to demonstrate that these metrics serves their purpose, we plan to carry
out several empirical validations by means of controlled experiments. These ex-
periments will involve more than 100 students from the Eindhoven University
of Technology (Netherlands), the Vienna University of Economics and Business
Administration (Austria), and the University of Madeira (Portugal). The col-
lected data will be analyzed using statistical methods to verify the degree of
correlationbetween students’ perception of the complexity of processes and the
proposed metrics. It should be noted that we have already conducted a small
A Discourse on Complexity of Process Models 125
experiment that involved 19 graduate students in Computer Science, as part of
a research project, and tested if the control-flow complexity of a set of 22 busi-
nessprocessescouldbepredictedusingtheCFCmetric.Analyzingthecollected
data using statisticalmethods we haveconcludedthat the CFC metric is highly
correlatedwith the control-flowcomplexity ofprocesses.This metric can,there-
fore, be used by business process analysts and process designers to analyze the
complexity of processes and, if possible, develop simpler processes.
References
1. W.M.P. van der Aalst. The Application of Petri Nets to Workflow Management.
The Journal of Circuits, Systems and Computers, 8(1):21–66, 1998.
2. T. Andrews, F. Curbera, H. Dholakia, Y. Goland, J. Klein, F. Leymann, K. Liu,
D.Roller,D.Smith,S.Thatte,I.Trickovic,andS.Weerawarana. BusinessProcess
Execution Language for Web Services, Version 1.1. Specification, BEA Systems,
IBM Corp., Microsoft Corp., SAPAG, Siebel Systems, 2003.
3. M. Azuma and D. Mole. Software management practice and metrics in the eu-
ropean community and japan: Some results of a survey. Journal of Systems and
Software, 26(1):5–18, 1994.
4. F. B. Bastani. An approach to measuring program complexity. COMPSAC ’83,
pages 1–8, 1983.
5. S.N.Cant,D.R.Jeffery,andB.Henderson-Sellers.Aconceptualmodelofcognitive
complexity of elements of the programming process. Information and Software
Technology, 37(7).
6. J. Cardoso. Control-flow Complexity Measurement of Processes and Weyuker’s
Properties. In 6th International Enformatika Conference, Transactions on Enfor-
matika, SystemsSciences and Engineering, Vol. 8, pages 213–218, 2005.
7. J. Cardoso. Workflow Handbook 2005, chapter Evaluating Workflows and Web
ProcessComplexity,pages284–290. FutureStrategies,Inc.,LighthousePoint,FL,
USA,2005.
8. J.Cardoso.Complexityanalysisofbpelwebprocesses.JournalofSoftwareProcess:
Improvement and Practice, 2006. toappear.
9. A.S. Guceglioglu and O.W. Demiros. Using Software Quality Characteristics to
Measure Business Process Quality. In W.M.P. van der Aalst, B. Benatallah,
F. Casati, and F. Curbera, editors, Business Process Management (BPM 2005),
volume 3649, pages 374–379. Springer-Verlag, Berlin, 2005.
10. M. H.Halstead. Elements of Software Science. Elsevier, Amsterdam, 1987.
11. W.HarrisonandK.Magel. Atopological analysisofcomputerprogramswithless
than threebinary branches. ACM SIGPLAN Notices, april:51–63, 1981.
12. S. Henry and D. Kafura. Software structure metrics based on information-flow.
IEEE Transactions On Software Engineering, 7(5):510–518, 1981.
13. G.E.Kalb. Countinglinesofcode,confusions,conclusions,andrecommendations.
Briefing tothe 3rd AnnualREVICUser’s Group Conference, 1990.
14. Antti M. Latva-Koivisto. Finding a complexity for business process models. Re-
search report, Helsinki Universityof Technology, February 2001.
15. T.J.McCabe.Acomplexitymeasure.IEEETransactionsonSoftwareEngineering,
2(4):308–320, 1976.
16. T. J. McCabe and C. W. Butler. Design complexity measurement and testing.
Communications of the ACM, 32:1415–1425, 1989.
126 J. Cardoso et al.
17. T.J.McCabeandA.H.Watson.Softwarecomplexity.JournalofDefenceSoftware
Engineering, 7(12):5–9, 1994. Crosstalk.
18. J. Mendling, M. Moser, G. Neumann, H.M.W. Verbeek, and B.F. van Don-
gen W.M.P. van der Aalst. A Quantitative Analysis of Faulty EPCs in the SAP
ReferenceModel. BPMCenterReportBPM-06-08,EindhovenUniversityofTech-
nology, Eindhoven,2006.
19. G. Miller. The magical number seven, plus or minus two: Some limits on our
capacity for processing information. The Psychological Review, 1956.
20. Nachiappan Nagappan, Thomas Ball, and Andreas Zeller. Mining metrics to pre-
dict component failures. In Proceedings of the 28th International Conference on
Software Engineering, Shanghai, China, 2006.
21. G. Neumann. Metaprogrammierung und Prolog. Addison-Wesley,December 1988.
22. Jan Obdrzalek. Dag-width: connectivity measure for directed graphs. In Sympo-
sium on Discrete Algorithms, pages 814–821. ACM Press, 2006.
23. H.A. Reijers and Irene T.P. Vanderfeesten. Cohesion and Coupling Metrics for
WorkflowProcessDesign. InJ.Desel,B.Pernici,andM.Weske,editors, Business
Process Management (BPM 2004), volume 3080, pages 290–305. Springer-Verlag,
Berlin, 2004.
24. M. Shepperd. Early life-cycle metrics and software quality models. Information
and Software Technology, 32(4):311–316, 1990.
25. W. Ward. Software defect prevention using mccabe’s complexity metric. Hewlett
Packard Journal, 40(2):64–69, 1989.
26. H.Zuse. Software Complexity: Measures and Methods. WalterdeGruyterandCo,
New Jersey, 1991.