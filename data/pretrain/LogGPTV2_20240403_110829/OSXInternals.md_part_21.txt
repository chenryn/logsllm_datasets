Vector units A 128-bit unit
Condition Register For performing logical operations on the Condition
Unit Register (CR)
Execution pipeline Ten execution pipelines, with up to 25 stages in a
pipeline, and up to 215 instructions in various stages
of execution at a time
Power management Multiple software-initialized power-saving modes,
PowerTune frequency and voltage scaling
[a] AS stands for Advanced Series.
[b] VMX is interchangeable with AltiVec. Apple markets the PowerPC's vector
functionality as Velocity Engine.
[c] As of 2005.
[d] The two fixed-point (integer) units of the 970FX are not symmetrical. Only one of
them can perform division, and only one can be used for special-purpose register (SPR)
operations.
3.3.2. Caches
A multilevel cache hierarchy is a common aspect of modern processors. A cache can be defined as a
small chunk of very fast memory that stores recently used data, instructions, or both. Information is
typically added and removed from a cache in aligned quanta called cache lines. The 970FX contains
several caches and other special-purpose buffers to improve memory performance. Figure 34 shows a
conceptual diagram of these caches and buffers.
Figure 34. Caches and buffers in the 970FX
[View full size image]
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 18 of 83
3.3.2.1. L1 and L2 Caches
The level 1 (L1) cache is closest to the processor. Memory-resident information must be loaded into
this cache before the processor can use it, unless that portion of memory is marked noncacheable. For
example, when a load instruction is being executed, the processor refers to the L1 cache to see if the
data in question is already held by a currently resident cache line. If so, the data is simply loaded from
the L1 cachean L1 cache hit. This operation takes only a few processor cycles as compared to a few
hundred cycles for accessing main memory.[23] If there is an L1 miss, the processor checks the next
level in the cache hierarchy: the level 2 (L2) cache. An L2 hit would cause the cache line containing
the data to be loaded into the L1 cache and then into the appropriate register. The 970FX does not
have level 3 (L3) caches, but if it did, similar steps would be repeated for the L3 cache. If none of the
caches contains the requested data, the processor must access main memory.
[23] Main memory refers to the system's installed and available dynamic memory
(DRAM).
As a cache line's worth of data is loaded into L1, a resident cache line must be flushed to make room
for the new cache line. The 970FX uses a pseudo-least-recently-used (LRU) algorithm[24] to
determine which cache line to evict. Unless instructed otherwise, the evicted cache line is sent to the
L2 cache, which makes L2 a victim cache. Table 35 shows the important properties of the 970FX's
caches.
[24] The 970FX allows the data-cache replacement algorithm to be changed from LRU to
FIFO through a bit in a hardware-dependent register.
Table 35. 970FX Caches
Property L1 I-cache L1 D-cache L2 Cache
Size 64KB 32KB 512KB
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 19 of 83
Type Instructions Data Data and
instructions
Associativity Direct-mapped Two-way set- Eight-way set-
associative associative
Line size 128 bytes 128 bytes 128 bytes
Sector size 32 bytes
Number of cache 512 256 4096
lines
Number of sets 512 128 512
Granularity 1 cache line 1 cache line 1 cache line
Replacement LRU LRU
policy
Store policy Write-through, Write-back, with
with no allocate- allocate-on-store-
on-store-miss miss
Index Effective address Effective address Physical address
Tags Physical address Physical address Physical address
Inclusivity Inclusive of L1 D-
cache
Hardware No Yes Yes, standard
coherency MERSI cache-
coherency protocol
Enable bit Yes Yes No
Reliability, Parity, with Parity, with ECC on data,
availability, and invalidate-on-error invalidate-on-error parity on tags
serviceability for data and tags for data and tags
(RAS)
Cache locking No No No
Demand load 3, 5, 4, 5 cycles for 11, 12, 11, 11
latencies (typical) GPRs, FPRs, cycles for GPRs,
VPERM, and FPRs, VPERM,
VALU, and VALU,
respectively[a] respectivelya
[a] Section 3.3.6.1 discusses GPRs and FPRs. Section 3.3.10.2 discusses VPERM and
VALU.
Harvard Architecture
The 970FX's L1 cache is split into separate caches for instructions and data. This design
aspect is referred to as the Harvard Architecture, alluding to the separate memories for
instructions and data in the Mark-III and Mark-IV vacuum tube machines that originated
at Harvard University in the 1940s.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 20 of 83
You can retrieve processor cache information using the sysctl command on Mac OS X as shown in
Figure 35. Note that the hwprefs command is part of Apple's CHUD Tools package.
Figure 35. Retrieving processor cache information using the sysctl command
$ sudo hwprefs machine_type # Power Mac G5 Dual 2.5GHz
PowerMac7,3
$ sysctl -a hw
...
hw.cachelinesize: 128
hw.l1icachesize: 65536
hw.l1dcachesize: 32768
hw.l2settings = 2147483648
hw.l2cachesize: 524288
...
$ sudo hwprefs machine_type # Power Mac G5 Quad 2.5GHz
PowerMac11,2
$ sysctl -a hw
...
hw.cachelinesize = 128
hw.l1icachesize = 65536
hw.l1dcachesize = 32768
hw.l2settings = 2147483648
hw.l2cachesize = 1048576
...
3.3.2.2. Cache Properties
Let us look more closely at some of the cache-related terminology used in Table 35.
Associativity
As we saw earlier, the granularity of operation for a cachethat is, the unit of memory transfers in and
out of a cacheis a cache line (also called a block). The cache line size on the 970FX is 128 bytes for
both the L1 and L2 caches. The associativity of a cache is used to determine where to place a cache
line's worth of memory in the cache.
If a cache is m-way set-associative, then the total space in the cache is conceptually divided into sets,
with each set containing m cache lines. In a set-associative cache, a block of memory can be placed
only in certain locations in the cache: It is first mapped to a set in the cache, after which it can be
stored in any of the cache lines within that set. Typically, given a memory block with address B, the
target set is calculated using the following modulo operation:
target set = B MOD {number of sets in cache}
A direct-mapped cache is equivalent to a one-way set-associative cache. It has the same number of
sets as cache lines. This means a memory block with address B can exist only in one cache line,
which is calculated as the following:
target cache line = B MOD {number of cache lines in cache}
Store Policy
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 21 of 83
A cache's store policy defines what happens when an instruction writes to memory. In a write-through
design, such as the 970FX L1 D-cache, information is written to both the cache line and to the
corresponding block in memory. There is no L1 D-cache allocation on write missesthe affected block
is modified only in the lower level of the cache hierarchy and is not loaded into L1. In a write-back
design, such as the 970FX L2 cache, information is written only to the cache linethe affected block is
written to memory only when the cache line is replaced.
Memory pages that are contiguous in virtual memory will normally not be contiguous in physical
memory. Similarly, given a set of virtual addresses, it is not possible to predict how they will fit in the
cache. A related point is that if you take a block of contiguous virtual memory the same size as a
cache, say, a 512KB block (the size of the entire L2 cache), there is little chance that it will fit in the
L2 cache.
MERSI
Only the L2 cache is physically mapped, although all caches use physical address tags. Stores are
always sent to the L2 cache in addition to the L1 cache, as the L2 cache is the data coherency point.
Coherent memory systems aim to provide the same view of all devices accessing the memory. For
example, it must be ensured that processors in a multiprocessor system access the correct datawhether
the most up-to-date data resides in main memory or in another processor's cache. Maintaining such
coherency in hardware introduces a protocol that requires the processor to "remember" the state of the
sharing of cache lines.[25] The L2 cache implements the MERSI cache-coherency protocol, which has
the following five states.
[25] Cache-coherency protocols are primarily either directory-based or snooping-based.
1. ModifiedThis cache line is modified with respect to the rest of the memory subsystem.
2. ExclusiveThis cache line is not cached in any other cache.
3. RecentThe current processor is the most recent reader of this shared cache line.
4. SharedThis cache line was cached by multiple processors.
5. InvalidThis cache line is invalid.
RAS
The caches incorporate parity-based error detection and correction mechanisms. Parity bits are
additional bits used along with normal information to detect and correct errors in the transmission of
that information. In the simplest case, a single parity bit is used to detect an error. The basic idea in
such parity checking is to add an extra bit to each unit of informationsay, to make the number of 1s in
each unit either odd or even. Now, if a single error (actually, an odd number of errors) occurs during
information transfer, the parity-protected information unit would be invalid. In the 970FX's L1 cache,
parity errors are reported as cache misses and therefore are implicitly handled by refetching the cache
line from the L2 cache. Besides parity, the L2 cache implements an error detection and correction
scheme that can detect double errors and correct single errors by using a Hamming code.[26] When a
single error is detected during an L2 fetch request, the bad data is corrected and actually written back
to the L2 cache. Thereafter, the good data is refetched from the L2 cache.
[26] A Hamming code is an error-correcting code. It is an algorithm in which a sequence
of numbers can be expressed such that any errors that appear in certain numbers (say, on
the receiving side after the sequence was transmitted by one party to another) can be
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 22 of 83
detected, and corrected, subject to certain limits, based on the remaining numbers.
3.3.3. Memory Management Unit (MMU)
During virtual memory operation, software-visible memory addresses must be translated to real (or
physical) addresses, both for instruction accesses and for data accesses generated by load/store
instructions. The 970FX uses a two-step address translation mechanism[27] based on segments and
pages. In the first step, a software-generated 64-bit effective address (EA) is translated to a 65-bit
virtual address (VA) using the segment table, which lives in memory. Segment table entries (STEs)
contain segment descriptors that define virtual addresses of segments. In the second step, the virtual
address is translated to a 42-bit real address (RA) using the hashed page table, which also lives in
memory.
[27] The 970FX also supports a real addressing mode, in which physical translation can
be effectively disabled.
The 32-bit PowerPC architecture provides 16 segment registers through which the 4GB virtual
address space can be divided into 16 segments of 256MB each. The 32-bit PowerPC implementations
use these segment registers to generate VAs from EAs. The 970FX includes a transitional bridge
facility that allows a 32-bit operating system to continue using the 32-bit PowerPC implementation's
segment register manipulation instructions. Specifically, the 970FX allows software to associate
segments 0 through 15 with any of the 237 available virtual segments. In this case, the first 16 entries
of the segment lookaside buffer (SLB), which is discussed next, act as the 16 segment registers.
3.3.3.1. SLB and TLB
We saw that the segment table and the page table are memory-resident. It would be prohibitively
expensive if the processor were to go to main memory not only for data fetching but also for address
translation. Caching exploits the principle of locality of memory. If caching is effective, then address
translations will also have the same locality as memory. The 970FX includes two on-chip buffers for
caching recently used segment table entries and page address translations: the segment lookaside
buffer (SLB) and the translation lookaside buffer (TLB), respectively. The SLB is a 64-entry, fully
associative cache. The TLB is a 1024-entry, four-way set-associative cache with parity protection. It
also supports large pages (see Section 3.3.3.4).
3.3.3.2. Address Translation
Figure 36 depicts address translation in the 970FX MMU, including the roles of the SLB and the
TLB. The 970FX MMU uses 64-bit or 32-bit effective addresses, 65-bit virtual addresses, and 42-bit
physical addresses. The presence of the DART introduces another address flavor, the I/O address,
which is an address in a 32-bit address space that maps to a larger physical address space.
Figure 36. Address translation in the 970FX MMU
[View full size image]
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 23 of 83
Technically, a computer architecture has three (and perhaps more) types of memory addresses: the
processor-visible physical address, the software-visible virtual address, and the bus address, which is
visible to an I/O device. In most cases (especially on 32-bit hardware), the physical and bus addresses
are identical and therefore not differentiated.
The 65-bit extended address space is divided into pages. Each page is mapped to a physical page. A
970FX page table can be as large as 231 bytes (2GB), containing up to 224 (16 million) page table
entry groups (PTEGs), where each PTEG is 128 bytes.
As Figure 36 shows, during address translation, the MMU converts program-visible effective
addresses to real addresses in physical memory. It uses a part of the effective address (the effective
segment ID) to locate an entry in the segment table. It first checks the SLB to see if it contains the
desired STE. If there is an SLB miss, the MMU searches for the STE in the memory-resident segment
table. If the STE is still not found, a memory access fault occurs. If the STE is found, a new SLB
entry is allocated for it. The STE represents a segment descriptor, which is used to generate the 65-bit
virtual address. The virtual address has a 37-bit virtual segment ID (VSID). Note that the page index
and the byte offset in the virtual address are the same as in the effective address. The concatenation of
the VSID and the page index forms the virtual page number (VPN), which is used for looking up in
the TLB. If there is a TLB miss, the memory-resident page table is looked up to retrieve a page table
entry (PTE), which contains a real page number (RPN). The RPN, along with the byte offset carried
over from the effective address, forms the physical address.
The 970FX allows setting up the TLB to be direct-mapped by setting a particular bit of a hardware-
implementation-dependent register.
3.3.3.3. Caching the Caches: ERATs
Information from the SLB and the TLB may be cached in two effective-to-real address translation
caches (ERATs)one for instructions (I-ERAT) and another for data (D-ERAT). Both ERATs are 128-
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 24 of 83
entry, two-way set-associative caches. Each ERAT entry contains effective-to-real address translation
information for a 4KB block of storage. Both ERATs contain invalid information upon power-on. As
shown in Figure 36, the ERATs represent a shortcut path to the physical address when there is a
match for the effective address in the ERATs.
3.3.3.4. Large Pages
Large pages are meant for use by high-performance computing (HPC) applications. The typical page
size of 4KB could be detrimental to memory performance in certain circumstances. If an application's
locality of reference is too wide, 4KB pages may not capture the locality effectively enough. If too
many TLB misses occur, the consequent TLB entry allocations and the associated delays would be
undesirable. Since a large page represents a much larger memory range, the number of TLB hits
should increase, as the TLB would now cache translations for larger virtual memory ranges.
It is an interesting problem for the operating system to make large pages available to applications.
Linux provides large-page support through a pseudo file system (hugetlbfs) that is backed by large
pages. The superuser must explicitly configure some number of large pages in the system by
preallocating physically contiguous memory. Thereafter, the hugetlbfs instance can be mounted on a
directory, which is required if applications intend to use the mmap() system call to access large pages.
An alternative is to use shared memory callsshmat() and shmget(). Files may be created, deleted,
mmap()'ed, and munmap()'ed on hugetlbfs. It does not support reads or writes, however. AIX also
requires separate, dedicated physical memory for large-page use. An AIX application can use large
pages either via shared memory, as on Linux, or by requesting that the application's data and heap
segments be backed by large pages.
Note that whereas the 970FX TLB supports large pages, the ERATs do not; large pages require
multiple entriescorresponding to each referenced 4KB block of a large pagein the ERATs. Cache-
inhibited accesses to addresses in large pages are not permitted.
3.3.3.5. No Support for Block Address Translation Mechanism
The 970FX does not support the Block Address Translation (BAT) mechanism that is supported in
earlier PowerPC processors such as the G4. BAT is a software-controlled array used for mapping
largeoften much larger than a pagevirtual address ranges into contiguous areas of physical memory.
The entire map will have the same attributes, including access protection. Thus, the BAT mechanism
is meant to reduce address translation overhead for large, contiguous regions of special-purpose
virtual address spaces. Since BAT does not use pages, such memory cannot be paged normally. A
good example of a scenario where BAT is useful is that of a region of framebuffer memory, which
could be memory-mapped effectively via BAT. Software can select block sizes ranging from 128KB
to 256MB.
On PowerPC processors that implement BAT, there are four BAT registers each for data (DBATs)
and instructions (IBATs). A BAT register is actually a pair of upper and lower registers, which are
accessible from supervisor mode. The eight pairs are named DBAT0U-DBAT3U, DBAT0L-
DBAT3L, IBAT0U-IBAT3U, and IBAT0L-IBAT3L. The contents of a BAT register include a block
effective page index (BEPI), a block length (BL), and a block real page number (BRPN). During
BAT translation, a certain number of high-order bits of the EAas specified by BLare matched against
each BAT register. If there is a match, the BRPN value is used to yield the RA from the EA. Note that
BAT translation is used over page table translation for storage locations that have mappings in both a
BAT register and the page table.
3.3.4. Miscellaneous Internal Buffers and Queues
The 970FX contains several miscellaneous buffers and queues internal to the processor, most of
which are not visible to software. Examples include the following:
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh687B.htm 20.08.2007
Chapter 3. Inside an Apple Page 25 of 83
A 4-entry (128 bytes per entry) Instruction Prefetch Queue logically above the L1 I-cache
Fetch buffers in the Instruction Fetch Unit and the Instruction Decode Unit
An 8-entry Load Miss Queue (LMQ) that tracks loads that missed the L1 cache and are waiting
to receive data from the processor's storage subsystem
A 32-entry Store Queue (STQ)[28] for holding stores that can be written to cache or memory
later
[28] The STQ supports forwarding.