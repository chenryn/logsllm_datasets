title:Nickel to Lego: Using Foolgle\textless/\textgreater to Create
Adversarial Examples to Fool Google Cloud Speech-to-Text API
author:Joon Kuy Han and
Hyoungshick Kim and
Simon S. Woo
Poster: Nickel to Lego: Using Foolgle to Create Adversarial
Examples to fool Google Cloud Speech-to-Text API
Joon Kuy Han
Simon S. Woo
Hyoungshick Kim
The State University of New York
Sungkyunkwan University & CSIRO
Incheon, South Korea
PI:EMAIL
Data611
Sydney, Australia
PI:EMAIL
Sungkyunkwan University
Suwon, South Korea
PI:EMAIL
ABSTRACT
Many companies offer automatic speech recognition or Speech-to-
Text APIs for use in diverse applications. However, audio classifi-
cation algorithms trained with deep neural networks (DNNs) can
sometimes misclassify adversarial examples, posing a significant
threat to critical applications. In this paper, we present a novel
way to create adversarial audio examples using a genetic algorithm.
Our algorithm creates adversarial examples by iteratively adding
perturbations to the original audio signal. Unlike most white-box
adversarial example generations, our approach does not require
knowledge about the target DNN’s model and parameters (black-
box) and heavy computational power of GPU resources. To show
the feasibility of the proposed idea, we implement a tool called,
Foolgle, using a genetic algorithm that performs untargeted attacks
to create adversarial audio examples and evaluate those with the
state-of-the-art Google Cloud Speech-to-Text API. Our preliminary
experiment results show that Foolgle deceives the API with a success
probability of 86%.
KEYWORDS
Adversarial example; Automatic Speech Recognition; Black-box
attack; Genetic algorithm
ACM Reference Format:
Joon Kuy Han, Hyoungshick Kim, and Simon S. Woo. 2019. Poster: Nickel
to Lego: Using Foolgle to Create Adversarial Examples to fool Google Cloud
Speech-to-Text API. In 2019 ACM SIGSAC Conference on Computer& Commu-
nications Security (CCS ’19), November 11–15, 2019, London, United Kingdom.
ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363264
1 INTRODUCTION
Speech-based interaction is widely used to interact with personal as-
sistants of smartphones and smart speakers. These systems rely on
running a speech classification model to recognize the user’s voice
commands. Voice assistants are quickly being upgraded to support
advanced, security-critical commands such as unlocking devices,
checking emails, and making payments. Deep neural networks
(DNNs) have been popularly used for automatic speech recognition
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11.
https://doi.org/10.1145/3319535.3363264
(ASR) [2] because they provide highly accurate voice recognition re-
sults. However, recent research has shown that neural networks are
susceptible to attacks that produce incorrect outputs or a targeted
output by an adversary [4, 7, 9, 12]. This type of attack known as
adversarial example has a high success rate in fooling ASR models.
Injected adversarial audio attacks can be dangerous as they can be
used not only to degrade the ASR performance but also can be used
to manipulate actions taken by digital assistants in unpredictable
or malicious ways.
Alzantot et al. [1] demonstrated that black-box approaches for
targeted attacks on ASR systems are possible using a genetic algo-
rithm (GA) approach. However, their method would require heavy
computational power of GPU resources. Taori et al. [10] also applied
the idea of GA approach to fool ASR systems from incorrectly label-
ing longer phrases and sentences. However, their approach requires
prior knowledge of the ASR system, which cannot be applicable
to a black-box attack. Khare et al. [6] performed untargeted and
targeted attacks using a genetic algorithm by maximizing acoustic
similarity between the adversarial sample and the original sample.
In this preliminary work, we explore an alternative approach
to efficiently generate adversarial examples using a efficient GA
to deceive the DNN-based ASRs. To show the feasibility of our
idea, we implement a highly efficient black-box attack tool called
Foolgle to generate adversarial examples and evaluate its attack
performance with the state-of-the-art commercial Google Cloud
Speech-to-Text API, which are used in many practical applications
such as voice assistants. While previous studies have proposed GA-
based algorithms to attack publicly available open APIs, this is the
first GA-based implementation that can generate adversarial audio
examples to target a commercial API. Our main contributions are
summarized as follows:
(1) We propose Foolgle, an attack algorithm using GA to efficiently
generate adversarial examples to deceive ASR systems, which
does not require any knowledge of DNNs nor require GPU re-
sources.
(2) We evaluate the attack performance of Foolgle with the state-
of-the-art commercial Speech-to-Text API from Google. Our
experiment results show that Foolgle can efficiently generate
adversarial examples to deceive the commercial Google Cloud
API with a success probability of 86%.
(3) We conducted an IRB-approved user study to evaluate human
listeners’ recognition performance for adversarial examples gen-
erated by Foolgle and found that the added perturbations do not
significantly affect human listeners’ recognition performance.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2593Figure 1: Google API misclassifies the perturbation-added
audio A′ “Lego", while it correctly labels the original audio
A “Nickel.”
2 DESIGN OF OUR APPROACH
Adversarial example generation: The goal of creating adversar-
ial audio examples is to make an audio classifier misclassify the
original input, where we formally define an adversarial example
generation as follow: given a valid input audio input A, and a target
t (cid:44) C∗(A), it is a procedure for finding a similar audio input A′ such
that t = C∗(A′), yet A and A′ are close according to some distance
metric. We call A′ an adversarial example to deceive C (see the
definition of adversarial example in [11]). In Untargeted adversarial
examples, attacks only search for an input A′ so that C(A) (cid:44) C∗(A′)
and A and A′ are close. Then, finding adversarial examples can be
formulated as minA′ ||A′ − A|| such that C (A) (cid:44) C∗ (A′).
Creating Adversarial Examples Using Genetic Algorithm
(GA): In order to perform a black-box attack, we develop a GA to
effectively generate adversarial audio examples against the com-
mercial API without access to any of their DNN model parameters.
The goal of our GA is to impose a small number of perturbations to
the original audio so that a commercial API misclassify the original
input audio sample (“Nickel”) to another one (“Lego”), while hu-
mans can still easily recognize the original audio sample as shown
in Fig. 1. We formulate our GA as follows:
Population and fitness. We create a new population of candidate
adversarial examples from the population of input audio samples
by adding perturbations to each audio sample in the population
as A′ = A + (α × δ) where a population represents a set of audio
samples. Initially, perturbation δ is generated with the same length
as the original audio sample A with the maximum frequency range
and then multiplied by coefficient α. Then, we add the perturbation
into the original audio sample A to obtain the adversarial audio
example A′. To measure the similarity of two audio samples, we first
obtain the Mel-frequency cepstral coefficients (MFCC) [8] of A and
A′. Next, we use the Dynamic Time Warping (DTW) algorithm [3]
to obtain a distance measure L1 between the two MFCC sequences.
Then, we define the fitness function f as follows:
f = P0 − Pd + γ × L1,
(1)
where P0 is the confidence score for the original label and Pd
is the confidence score for any other wrong labels. We can obtain
either one of P0 or Pd and set the other to zero because the commer-
cial APIs only return the highest probability of one of P0 or Pd. Next,
we formulate our GA as a minimization problem with the value
of the fitness function in Eq. (1) to produce the best audio sample,
which has high Pd, and low P0 and low L1 values. In Eq. (1), γ is
another coefficient to balance the perturbation amount to deceive
APIs by guiding GA to find an adversarial audio sample with the
least amount of perturbations, where γ is inversely proportional to
α, because P0 and Pd always have the values between 0 to 1.
Selection. We implement a tournament selection with a size to be
four. Only two of the four audio samples in each tournament will
be selected. In our design, audio samples with higher fitness have a
75% chance to win, and the less fit has a 25% chance to maintain a