title:Adversary Instantiation: Lower Bounds for Differentially Private Machine
Learning
author:Milad Nasr and
Shuang Song and
Abhradeep Thakurta and
Nicolas Papernot and
Nicholas Carlini
2021 IEEE Symposium on Security and Privacy (SP)
Adversary Instantiation: Lower Bounds for
Differentially Private Machine Learning
Milad Nasr∗, Shuang Song†, Abhradeep Thakurta†, Nicolas Papernot† and Nicholas Carlini†
∗PI:EMAIL
†{shuangsong, athakurta, papernot, ncarlini}@google.com
∗University of Massachusetts Amherst
†Google Brain
9
6
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
ABSTRACT
Differentially private (DP) machine learning allows us to
train models on private data while limiting data leakage. DP
formalizes this data leakage through a cryptographic game,
where an adversary must predict if a model was trained on a
dataset D, or a dataset D(cid:48) that differs in just one example.
If observing the training algorithm does not meaningfully
increase the adversary’s odds of successfully guessing which
dataset the model was trained on, then the algorithm is said to
be differentially private. Hence, the purpose of privacy analysis
is to upper bound the probability that any adversary could
successfully guess which dataset the model was trained on.
In our paper, we instantiate this hypothetical adversary in
order to establish lower bounds on the probability that this
distinguishing game can be won. We use this adversary to
evaluate the importance of the adversary capabilities allowed
in the privacy analysis of DP training algorithms.
For DP-SGD, the most common method for training neural
networks with differential privacy, our lower bounds are tight
and match the theoretical upper bound. This implies that in
order to prove better upper bounds, it will be necessary to
make use of additional assumptions. Fortunately, we ﬁnd that
our attacks are signiﬁcantly weaker when additional (realistic)
restrictions are put in place on the adversary’s capabilities.
Thus, in the practical setting common to many real-world
deployments, there is a gap between our lower bounds and the
upper bounds provided by the analysis: differential privacy is
conservative and adversaries may not be able to leak as much
information as suggested by the theoretical bound.
I. INTRODUCTION
With machine learning now being used to train models
on sensitive user data, ranging from medical images [19], to
personal email [8] and text messages [53], it is becoming ever
more important that these models are privacy preserving. This
privacy is both desirable for users, and increasingly often also
legally mandated in frameworks such as the GDPR [11].
Differential privacy (DP) is now the standard deﬁnition for
privacy [14, 15]. While ﬁrst deﬁned as a property a query
mechanism satisﬁes on a database, differential privacy analysis
has since been extended to algorithms for training machine
learning models on private training data
[7, 4, 51, 1, 38,
55, 25, 47, 20, 21]. On neural networks, differentially private
stochastic gradient descent (DP-SGD) [1, 4, 51] is the most
popular method to train neural networks with DP guarantees.
Differential privacy sets up a game where the adversary is
trying to guess whether a training algorithm took as its input
one dataset D or a second dataset D(cid:48) that differs in only one
example. If observing the training algorithm’s outputs allows
the adversary to improve their odds of guessing correctly, then
the algorithm leaks private information. Differential privacy
proposes to randomize the algorithm in such a way that it
becomes possible to analytically upper bound the probability
of an adversary making a successful guess, hence quantifying
the maximum leakage of private information.
In recent work [26] proposed to audit the privacy guarantees
of DP-SGD by instantiating a relatively weak, black-box
adversary who observed the model’s predictions. In this paper,
we instantiate this adversary with a spectrum of attacks that
spans from a black-box adversary (that is only able to observe
the model’s predictions) to a worst-case yet often unrealistic
adversary (with the ability to poison training gradients and
observe intermediate model updates during training). This not
only enables us to provide a stronger lower bounds on the
privacy leakage (compared to [26]), it also helps us identify
which capabilities are needed for adversaries to be able to
extract exactly as much private information as is possible given
the upper bound provided by the DP analysis.
Indeed, in order to provide strong, composable guarantees
that avoid the pitfalls of other privacy analysis methods [35,
36, 52], DP and DP-SGD assume the existence of powerful
adversaries that may not be practically realizable.
For example, the DP-SGD analysis assumes that the inter-
mediate computations used to train a model are published to
the adversary, when in most practical settings only the ﬁnal,
fully-trained model is revealed. This is done not because it
is desirable—but because there are limited (known) ways to
improve the analysis by assuming the adversary has access to
only one model [21]. As such, it is conceivable—and indeed
likely—that the actual privacy in practical scenarios is greater
than what can be shown theoretically (indeed, DP-SGD is
known to be tight asymptotically [4]).
Researchers have often assumed that
this will be true
[27], and argue that it is acceptable to train models with
guarantees so weak that they are essentially vacuous, hoping
that the actual privacy offered is much stronger [6]. This
is important because training models with weak guarantees
generally allows one to achieve greater model utility, which
often leads practitioners to follow this train of thought when
tuning their DP training algorithms.
© 2021, Milad Nasr. Under license to IEEE.
DOI 10.1109/SP40001.2021.00069
866
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
y
c
a
v
i
r
p
d
e
r
u
s
a
e
m
2
1
0
API
Blackbox
W hitebox
Adaptive
Gradient
Dataset
Practical
Theoretical
Fig. 1: Summary of our results, plotting emperically mea-
sured ε when training a model with ε = 2 differential privacy
on MNIST. The dashed red line corresponds to the certiﬁable
upper bound. Each bar correspond to the privacy offered by
increasingly powerful adversaries. In the most realistic setting,
training with privacy offers much more empirically measured
privacy. When we provide full attack capabilities, our lower
bound shows that the DP-SGD upper bound is tight.
A. Our Techniques
We build on the poisoning adversary introduced in Jagielski
et al. [26]. By adversarially constructing two datasets D
and D(cid:48) we can audit DP-SGD. Concretely, we instantiate
the hypothetical differential privacy adversary under various
adversary scenarios by providing a pair of concrete attack
algorithms: one that constructs the two datasets D and D(cid:48)
differing in one example, and another that receives a model
as input (that was trained on one of these two datasets) and
predicts which one it was trained on.
We use these two adversaries as a tool to measure the
relative importance of the assumptions made by the DP-SGD
analysis, as well as the potential beneﬁts of assumptions that
are not currently made, but could be reasonable assumptions
to make in the future. Different combinations of assumptions
correspond to different threat models and constraints on the
adversary’s capabilities, as exposed in Sections IV-A to IV-F.
There are two communities who this analysis impacts.
• Practitioners would like to know the privacy leakage in
situations that are as close to realistic as possible. Even if
it is not possible to prove tight upper bounds of privacy,
we can provide best-effort empirical evidence that the
privacy offered by DP-SGD is greater than what is proven
when the adversary is more constrained.
• Theoreticians, conversely, care about identifying ways
to improve the current analysis. By studying various
capabilities that a real-world adversary would actually
have, but that the analysis of DP-SGD does not assume
are placed on the adversary, we are able to estimate the
potential utility of introducing additional assumptions.
B. Our Results
Figure 1 summarizes our key results. When our adversary is
given full capabilities, our lower bound matches the provable
upper bound, demonstrating the DP-SGD analysis is tight
in the worst-case. In the past, more sophisticated analysis
techniques (without making new assumptions) were able to
establish better and better upper bounds on essentially the
same DP-SGD algorithm [1]. This meant that we could train a
model once, and over time its privacy would “improve” (so to
speak), as progress in theoretical work on differential privacy
yielded a tighter analysis of the same algorithm used to train
the model. Our lower bound implies that this trend has come
to an end, and new assumptions will be necessary to establish
tighter bounds.
Conversely, the bound may be loose under realistic
settings, such as when we assume a more restricted adversary
that is only allowed to view the ﬁnal trained model (and not
every intermediate model obtained during training). In this
setting, our bound is substantially lowered and as a result it
is possible that better analysis might be able to improve the
privacy bound.
Finally, we show that many of the capabilities allowed
by the adversary do not signiﬁcantly strengthen the attack.
For example, the DP-SGD analysis assumes the adversary is
given access to all intermediate models generated throughout
training; surprisingly, we ﬁnd that access to just the ﬁnal model
is almost as good as all prior models.
On the whole, our results have broad implications for those
deploying differentially private machine learning in practice
(for example, indicating situations where the empirical privacy
is likely stronger than the worst case) and also implications
for those theoretically analyzing properties of DP-SGD (for
example, indicating that new assumptions will be required to
obtain stronger privacy guarantees).
II. BACKGROUND & RELATED WORK
A. Machine Learning
A machine learning model
is a parameterized function
fθ : X → Y that takes inputs from an input space X and
returns outputs from an output space Y. The majority of this
paper focuses on the class of functions fθ known as deep
neural networks [34], represented as a sequence of linear
layers with non-linear activation functions [43]. Our results are
independent of any details of the neural network’s architecture.
The model parameters θ are obtained through a training
algorithm T that minimize the average loss (cid:96)(fθ, x, y) on a
ﬁnite-sized training datasets D = {(xi, yi)}|D|
i=1, denoted by
L(fθ, D). Formally, we write fθ ← T (D).
Stochastic gradient descent [34] is the canonical method for
minimizing this loss. We sample a mini-batch of examples
B(D) = B = {(xi, yi)}|B|
i=1 ⊂ D.
Then, we compute the average mini-batch loss as
L(fθ; B) =
1
|B|
(cid:96)(fθ; x, y)
(cid:88)
(x,y)∈B
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
867
and update the model parameters according to
θi+1 ← θi − η∇θL(fθi; B)
(1)
taking a step of size η, the learning rate. We abbreviate each
step of this update rule as fθ+1 ← S(fθ, B).
Because neural networks are trained on a ﬁnite-sized train-
ing dataset, models train for multiple epochs repeating the
same examples over and over, often tens to hundreds of
times. This has consequences for the privacy of the training
data. Machine learning models are often signiﬁcantly over-
parameterized [50, 56]: they contain sufﬁcient capacity to
memorize the particular aspects of the data they are trained
on, even if these aspects are irrelevant to ﬁnal accuracy. This
allows a wide range of attacks that leak information about the
training data given access to the trained model. These attacks
range from membership inference attacks [6, 24, 39, 45, 50,
49] to training data extraction attacks [22, 39].
B. Differential Privacy
Differential privacy (DP) [14, 16, 17] has become the de-
facto deﬁnition of algorithmic privacy. An algorithm M is
said to be (ε, δ)-differentially private if for all set of events