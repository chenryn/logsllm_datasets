to a target GNN model, an adversary can accurately infer
whether there exists a link between any pair of nodes in a
graph that is used to train the GNN model. We propose a
threat model to systematically characterize an adversary’s
background knowledge along three dimensions. By jointly
considering the three dimensions, we deﬁne 8 link stealing
attacks and propose novel methods to realize them. Extensive
evaluation over 8 real-world datasets shows that our attacks
can accurately steal links. Interesting future work includes
generalizing our attacks to GNNs for graph classiﬁcation and
defending against our attacks.
Acknowledgments
We thank the anonymous reviewers and our shepherd Minhui
Xue for constructive feedback. This work is partially funded
by the Helmholtz Association within the project “Trustworthy
Federated Data Analytics” (TFDA) (funding number ZT-I-
OO1 4) and National Science Foundation grant No. 1937787.
References
[1] James Atwood and Don Towsley.
Diffusion-
Convolutional Neural Networks. In NIPS, pages 1993–
2001, 2016.
[8] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz.
GAN-Leaks: A Taxonomy of Membership Inference
Attacks against GANs. In CCS, 2020.
[9] Min Chen, Zhikun Zhang, Tianhao Wang, Michael
Backes, Mathias Humbert, and Yang Zhang. When
CoRR
Machine Unlearning Jeopardizes Privacy.
abs/2005.02205, 2020.
[10] Qingrong Chen, Chong Xiang, Minhui Xue, Bo Li,
Nikita Borisov, Dali Kaarfar, and Haojin Zhu. Dif-
ferentially Private Data Generative Models. CoRR
abs/1812.02274, 2018.
[11] Yizheng Chen, Yacin Nadji, Athanasios Kountouras,
Fabian Monrose, Roberto Perdisci, Manos Antonakakis,
and Nikolaos Vasiloglou. Practical Attacks Against
In CCS, pages 1125–1142,
Graph-based Clustering.
2017.
[12] Yizheng Chen, Shiqi Wang, Dongdong She, and Suman
Jana. On Training Robust PDF Malware Classiﬁers. In
USENIX Security, 2020.
[13] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang,
Jun Zhu, and Le Song. Adversarial Attack on Graph
Structured Data. In ICML, pages 1123–1132, 2018.
[14] Michaël Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. Convolutional Neural Networks on Graphs
with Fast Localized Spectral Filtering. In NIPS, pages
3837–3845, 2016.
[2] Michael Backes, Mathias Humbert, Jun Pang, and Yang
Zhang. walk2friends: Inferring Social Links from Mo-
bility Proﬁles. In CCS, pages 1943–1957, 2017.
[15] Paul D. Dobson and Andrew J. Doig. Distinguishing
Enzyme Structures from Non-Enzymes without Align-
ments. Journal of Molecular Biology, 2003.
[3] Aleksandar Bojchevski and Stephan Günnemann. Ad-
versarial Attacks on Node Embeddings via Graph Poi-
soning. In ICML, pages 695–704, 2019.
[16] Yuxiao Dong, Reid A. Johnson, and Nitesh V. Chawla.
Will This Paper Increase Your h-index?: Scientiﬁc Im-
pact Prediction. In WSDM, pages 149–158, 2015.
[4] Aleksandar Bojchevski and Stephan Günnemann. Certi-
ﬁable Robustness to Graph Perturbations. In NeurIPS,
pages 8317–8328, 2019.
[5] Karsten M. Borgwardt, Cheng Soon Ong, Stefan Schö-
nauer, S. V. N. Vishwanathan, Alexander J. Smola, and
Hans-Peter Kriegel. Protein Function Prediction via
Graph Kernels. Bioinformatics, 2005.
[17] Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas
Laurent, Yoshua Bengio, and Xavier Bresson.
CoRR
Benchmarking Graph Neural Networks.
abs/2003.00982, 2020.
[18] Federico Errica, Marco Podda, Davide Bacciu, and
Alessio Micheli. A Fair Comparison of Graph Neural
Networks for Graph Classiﬁcation. In ICLR, 2020.
[6] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
Kos, and Dawn Song. The Secret Sharer: Evaluating and
Testing Unintended Memorization in Neural Networks.
In USENIX Security, pages 267–284, 2019.
[19] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric
Zhao, Jiliang Tang, and Dawei Yin. Graph Neural Net-
In WWW, pages
works for Social Recommendation.
417–426, 2019.
[7] Varun Chandrasekaran, Kamalika Chaudhuri, Irene Gi-
acomelli, Somesh Jha, and Songbai Yan. Exploring
Connections Between Active Learning and Model Ex-
traction. In USENIX Security, 2020.
[20] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model Inversion Attacks that Exploit Conﬁdence In-
formation and Basic Countermeasures. In CCS, pages
1322–1333, 2015.
2682    30th USENIX Security Symposium
USENIX Association
[21] Matt Fredrikson, Eric Lantz, Somesh Jha, Simon Lin,
David Page, and Thomas Ristenpart. Privacy in Pharma-
cogenetics: An End-to-End Case Study of Personalized
Warfarin Dosing. In USENIX Security, pages 17–32,
2014.
[32] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang
Zhang, and Neil Zhenqiang Gong. MemGuard: Defend-
ing against Black-Box Membership Inference Attacks
In CCS, pages 259–274,
via Adversarial Examples.
2019.
[22] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley,
Oriol Vinyals, and George E. Dahl. Neural Message
Passing for Quantum Chemistry. In ICML, pages 1263–
1272, 2017.
[23] Neil Zhenqiang Gong and Bin Liu. You are Who
You Know and How You Behave: Attribute Inference
Attacks via Users’ Social Friends and Behaviors. In
USENIX Security, pages 979–995, 2016.
[24] Neil Zhenqiang Gong, Ameet Talwalkar, Lester W.
Mackey, Ling Huang, Eui Chul Richard Shin, Emil Ste-
fanov, Elaine Shi, and Dawn Song. Joint Link Prediction
and Attribute Inference Using a Social-Attribute Net-
work. ACM Transactions on Intelligent Systems and
Technology, 2014.
[25] Aditya Grover and Jure Leskovec. node2vec: Scalable
Feature Learning for Networks. In KDD, pages 855–
864, 2016.
[26] Inken Hagestedt, Yang Zhang, Mathias Humbert, Pas-
cal Berrang, Haixu Tang, XiaoFeng Wang, and Michael
Backes. MBeacon: Privacy-Preserving Beacons for
DNA Methylation Data. In NDSS, 2019.
[27] William L. Hamilton, Zhitao Ying, and Jure Leskovec.
Inductive Representation Learning on Large Graphs. In
NIPS, pages 1025–1035, 2017.
[28] Michael Hay, Chao Li, Gerome Miklau, and David D.
Jensen. Accurate Estimation of the Degree Distribution
of Private Networks. In ICDM, pages 169–178, 2009.
[29] Jamie Hayes, Luca Melis, George Danezis, and Emil-
iano De Cristofaro. LOGAN: Evaluating Privacy Leak-
age of Generative Models Using Generative Adversarial
Networks. Symposium on Privacy Enhancing Technolo-
gies Symposium, 2019.
[30] Matthew Jagielski, Nicholas Carlini, David Berthelot,
Alex Kurakin, and Nicolas Papernot. High Accuracy
and High Fidelity Extraction of Neural Networks. In
USENIX Security, 2020.
[31] Jinyuan Jia and Neil Zhenqiang Gong. AttriGuard: A
Practical Defense Against Attribute Inference Attacks
via Adversarial Machine Learning. In USENIX Security,
pages 513–529, 2018.
[33] Jinyuan Jia, Binghui Wang, Xiaoyu Cao, and Neil Zhen-
qiang Gong. Certiﬁed Robustness of Community De-
tection against Adversarial Structural Perturbation via
Randomized Smoothing. In WWW, pages 2718–2724,
2020.
[34] Mika Juuti, Sebastian Szyller, Samuel Marchal, and
N. Asokan. PRADA: Protecting Against DNN Model
Stealing Attacks. In Euro S&P, pages 512–527, 2019.
[35] Thomas N. Kipf and Max Welling. Semi-Supervised
Classiﬁcation with Graph Convolutional Networks. In
ICLR, 2017.
[36] Klas Leino and Matt Fredrikson. Stolen Memories:
Leveraging Model Memorization for Calibrated White-
Box Membership Inference. In USENIX Security, 2020.
[37] Shaofeng Li, Shiqing Ma, Minhui Xue, and Benjamin
Zi Hao Zhao. Deep Learning Backdoors. CoRR
abs/2007.08273, 2020.
[38] Zheng Li, Chengyu Hu, Yang Zhang, and Shanqing
Guo. How to Prove Your Model Belongs to You: A
Blind-Watermark based Framework to Protect Intellec-
tual Property of DNN. In ACSAC, pages 126–137, 2019.
[39] Zheng Li and Yang Zhang. Label-Leaks: Membership
Inference Attack with Label. CoRR abs/2007.15528,
2020.
[40] David Liben-Nowell and Jon Kleinberg. The Link-
prediction Problem for Social Networks. Journal of
the American Society for Information Science and Tech-
nology, 2007.
[41] Luca Melis, Congzheng Song, Emiliano De Cristofaro,
and Vitaly Shmatikov. Exploiting Unintended Feature
Leakage in Collaborative Learning. In S&P, pages 497–
512, 2019.
[42] Milad Nasr, Reza Shokri, and Amir Houmansadr. Ma-
chine Learning with Membership Privacy using Adver-
sarial Regularization. In CCS, pages 634–646, 2018.
[43] Milad Nasr, Reza Shokri, and Amir Houmansadr. Com-
prehensive Privacy Analysis of Deep Learning: Passive
and Active White-box Inference Attacks against Central-
ized and Federated Learning. In S&P, pages 1021–1035,
2019.
USENIX Association
30th USENIX Security Symposium    2683
[44] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff Nets: Stealing Functionality of Black-Box
Models. In CVPR, pages 4954–4963, 2019.
[57] Congzheng Song and Vitaly Shmatikov. Auditing Data
Provenance in Text-Generation Models. In KDD, pages
196–206, 2019.
[45] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Prediction Poisoning: Towards Defenses Against DNN
Model Stealing Attacks. In ICLR, 2020.
[58] Congzheng Song and Reza Shokri. Robust Membership
Encoding: Inference Attacks and Copyright Protection
for Deep Learning. In ASIACCS, 2020.
[46] Jun Pang and Yang Zhang. DeepCity: A Feature Learn-
In
ing Framework for Mining Location Check-Ins.
ICWSM, pages 652–655, 2017.
[47] Jun Pang and Yang Zhang. Quantifying Location So-
ciality. In HT, pages 145–154, 2017.
[48] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and
Michael Wellman. SoK: Towards the Science of Secu-
rity and Privacy in Machine Learning. In Euro S&P,
pages 399–414, 2018.
[49] Nicolas Papernot, Patrick D. McDaniel, Ian Goodfellow,
Somesh Jha, Z. Berkay Celik, and Ananthram Swami.
Practical Black-Box Attacks Against Machine Learning.
In ASIACCS, pages 506–519, 2017.
[50] Erwin Quiring, Alwin Maier, and Konrad Rieck. Mis-
leading Authorship Attribution of Source Code using
Adversarial Learning. In USENIX Security, pages 479–
496, 2019.
[51] Kaspar Riesen and Horst Bunke. Structural, Syntactic,
and Statistical Pattern Recognition. Springer, 2008.
[52] Ahmed Salem, Apratim Bhattacharya, Michael Backes,
Mario Fritz, and Yang Zhang. Updates-Leak: Data Set
Inference and Reconstruction Attacks in Online Learn-
ing. In USENIX Security, pages 1291–1308, 2020.
[53] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma,
and Yang Zhang. Dynamic Backdoor Attacks Against
Machine Learning Models. CoRR abs/2003.03675,
2020.
[54] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal
Berrang, Mario Fritz, and Michael Backes. ML-Leaks:
Model and Data Independent Membership Inference
Attacks and Defenses on Machine Learning Models. In
NDSS, 2019.
[55] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian
Suciu, Christoph Studer, Tudor Dumitras, and Tom Gold-
stein. Poison Frogs! Targeted Clean-Label Poisoning
Attacks on Neural Networks. In NeurIPS, pages 6103–
6113, 2018.
[56] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. Membership Inference Attacks Against
Machine Learning Models. In S&P, pages 3–18, 2017.
[59] Jeffrey Sutherland, Lee O’Brien, and Donald Weaver.
SplineFitting with a Genetic Algorithm: A Method for
Developing Classiﬁcation Structure Activity Relation-
ships. Journal of Chemical Information and Computer
Sciences, 2003.
[60] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter,
and Thomas Ristenpart. Stealing Machine Learning
Models via Prediction APIs. In USENIX Security, pages
601–618, 2016.
[61] Laurens van der Maaten and Geoffrey Hinton. Visual-
izing Data using t-SNE. Journal of Machine Learning
Research, 2008.
[62] Petar Velickovic, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph
Attention Networks. In ICLR, 2018.
[63] Binghui Wang and Neil Zhenqiang Gong. Stealing
Hyperparameters in Machine Learning. In S&P, pages
36–52, 2018.
[64] Binghui Wang and Neil Zhenqiang Gong. Attacking
Graph-based Classiﬁcation via Manipulating the Graph
Structure. In CCS, pages 2023–2040, 2019.
[65] Binghui Wang, Jinyuan Jia, and Neil Zhenqiang Gong.
Graph-based Security and Privacy Analytics via Col-
lective Classiﬁcation with Joint Weight Learning and
Propagation. In NDSS, 2019.
[66] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew
Docherty, Kai Lu, and Liming Zhu. Adversarial Ex-
amples for Graph Data: Deep Insights into Attack and
Defense. In IJCAI, pages 4816–4823, 2019.
[67] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and
Somesh Jha. Privacy Risk in Machine Learning: An-
alyzing the Connection to Overﬁtting. In CSF, pages
268–282, 2018.
[68] Jun Zhang, Graham Cormode, Cecilia M. Procopiuc,
Divesh Srivastava, and Xiaokui Xiao. Private Release of
Graph Statistics using Ladder Functions. In SIGMOD,
pages 731–745, 2015.
[69] Yang Zhang. Language in Our Time: An Empirical Anal-
ysis of Hashtags. In WWW, pages 2378–2389, 2019.
2684    30th USENIX Security Symposium
USENIX Association
[70] Yang Zhang, Mathias Humbert, Bartlomiej Surma,
Praveen Manoharan, Jilles Vreeken, and Michael
Backes. Towards Plausible Graph Anonymization. In
NDSS, 2020.
[71] Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhen-
qiang Gong. Backdoor Attacks to Graph Neural Net-
works. CoRR abs/2006.11165, 2020.
[72] Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu
Zhu. Robust Graph Convolutional Networks Against
Adversarial Attacks. In KDD, pages 1399–1407, 2019.
[73] Daniel Zügner, Amir Akbarnejad, and Stephan Günne-
mann. Adversarial Attacks on Neural Networks for
Graph Data. In KDD, pages 2847–2856, 2018.