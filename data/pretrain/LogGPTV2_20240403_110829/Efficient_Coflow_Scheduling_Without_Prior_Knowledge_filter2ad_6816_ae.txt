sources [37]. For bin-1 to bin-4, corresponding normalized
completion times were 0.75×, 0.78×, 1.32×, and 1.15×,
respectively. Across all bins, it was 1.19×.
7.3
So far we have only considered static coﬂows, where all
ﬂows of a coﬂow start together. However, operational events
like multi-wave scheduling, task failures, and speculative
execution can dynamically change a coﬂow’s structure in
the runtime (§5.2). Because of their logical similarity – i.e.,
tasks start in batches and the number of active ﬂows cannot
be known a priori – we focus only on the multi-wave case.
Impact of Runtime Dynamics
The number of waves in a stage depends on the number of
Figure 10: [EC2] Average improvements in CCTs w.r.t. Varys for
multi-wave coﬂows.
senders (e.g., mappers in MapReduce) [11]. In these exper-
iments, we used the same coﬂow mix as the original trace
but varied the maximum number of concurrent senders in
each wave while keeping all the receivers active, essentially
ﬁxing the maximum number of waves in each coﬂow. Ta-
ble 4 shows the fraction of coﬂows with different number of
waves; e.g., all coﬂows had exactly one wave in Section 7.2.
Figure 10 shows the importance of leveraging coﬂow
relationships across waves. As the number of multi-wave
coﬂows increased, Aalo moved from trailing Varys by 0.94×
to outperforming it by 1.21× and 7.91×. Using Varys, one
can take two approaches to handle multi-wave coﬂows –
(i) creating separate coﬂows for each wave as they become
available or (ii) introducing barriers to determine the bot-
tleneck of the combined coﬂow – that both result in per-
formance loss. In the former, Varys can efﬁciently schedule
each wave but increases the stage-level CCT by ignoring the
fact that all waves must ﬁnish for the stage to ﬁnish. The
56× improvement in bin-3 presents an extreme example:
one straggler coﬂow was scheduled much later than the rest,
increasing the entire stage’s runtime. In the latter, artiﬁcial
barriers decrease parallelism and network utilization. Aalo
circumvents the dilemma by creating exactly one coﬂow per
stage for any number of waves and by avoiding barriers.
Aalo’s improvements over per-ﬂow fairness (not shown)
Impact on DAG Scheduling
remained similar to that in Section 7.2.
7.4
In this section, we evaluate Aalo using multi-stage jobs. Be-
cause the Facebook trace consists of only single-coﬂow jobs,
we used the Cloudera industrial benchmark [7, 4] consisting
of 20 TPC-DS queries. We ensured that each stage consists
of a single wave, but multiple coﬂows from the same job can
still run in parallel (Figure 4c).
Figure 11 shows that Aalo outperforms both per-ﬂow fair-
ness and Varys for DAGs that have more than one levels.
Because Aalo does not introduce artiﬁcial barriers and can
distinguish between coﬂows from different levels of the crit-
ical path, improvements over Varys (3.7× on average) are
higher than that over per-ﬂow fairness (1.7× on average).
7.5 Sensitivity Analysis
In this section, we ﬁrst examine Aalo’s sensitivity to the
number of queues and their thresholds for heavy-tailed
coﬂow size distributions. Later, we evaluate Aalo’s perfor-
mance for light-tailed distributions.
The Number of Queues (K) Aalo performs increasingly
better than per-ﬂow fairness as we increase the number of
0!1!0.01!0.1!1!10!100!1000!Fraction of Coﬂows!Coﬂow Completion Time (Seconds)!Varys!Non-Clairvoyant Scheduler!Per-Flow Fairness!Uncoordinated Non-Clairvoyant!55.86!0!6!12!Bin 1!Bin 2!Bin 3!Bin 4!ALL!Norm. Comp. Time w.r.t. Aalo!Coﬂow Types!Maximum Waves = 1!Maximum Waves = 2!Maximum Waves = 4!EC2$Coﬂow$Mul+Wave01!0!402Figure 11: [EC2] Improvements in job-level communication times using Aalo for coﬂow DAGS in the Cloudera benchmark.
(a) Qhi
1 = 10 MB; E = 10
(b) K = 10; E = 10
(a) Uniformly distributed coﬂow sizes
(c) Exp.-Spaced Queues
(d) Equal-Sized Queues
Figure 12: [Simulation] Aalo’s sensitivity (measured as improve-
ments over per-ﬂow fairness) to (a) the number of queues, (b) the
size of the highest-priority queue, and (c) exponential and (d) linear
queue thresholds.
queues (Figure 12a). However, we observe the largest jump
as soon as Aalo starts avoiding head-of-line blocking for
K = 2. Beyond that, we observe diminishing returns.
Queue Thresholds For more than one queues, Aalo must
carefully determine their thresholds. Because we have de-
ﬁned queue thresholds as a function of the size of the initial
queue Qhi
1 (§4.4), we focus on its impact on Aalo’s perfor-
mance. Recall that as we increase Qhi
1 , more coﬂows will
be scheduled in the FIFO order in the highest-priority Q1.
Figure 12b shows that as we increase Qhi
1 up to 100 MB
and schedule almost 60% of the coﬂows [20, Figure 4(e)]
in the FIFO order, Aalo’s performance remains steady. This
is because all these coﬂows carry a tiny fraction of the total
trafﬁc (≤ 0.1%). If we increase Qhi
further and start in-
cluding increasingly larger coﬂows in the FIFO-scheduled
Q1, performance steadily deteriorates. Finally, Figure 12c
demonstrates the interactions of E, the multiplicative factor
used to determine queue thresholds, with K and Qhi
1 . We
observe that for K > 2, Aalo’s performance is steady for a
wide range of (K, E, Qhi
What About Non-Exponential Queue Thresholds? In-
stead of creating exponentially larger queues, one can cre-
ate equal-sized queues. Given the maximum coﬂow size of
1 ) combinations.
1
(b) Fixed-size coﬂows
Figure 13: [Simulation] Improvements in average CCTs using
Aalo (a) when coﬂow sizes are uniformly distributed up to different
maximum values and (b) when all coﬂows have the same size.
10 TB, Figure 12d shows Aalo’s performance for varying
number of equal-sized queues – it requires orders of magni-
tude more queues to attain performance similar to exponen-
tial spacing. Although creating logical queues is inexpensive
at end hosts, more queues generate more “queue-change”
events and increase coordination costs.
Impact of Coﬂow Size Distributions So far we have eval-
uated Aalo on coﬂows that follow heavy-tailed distribu-
tion. Here, we compare Aalo against per-ﬂow fairness and a
non-preemptive FIFO scheduler on coﬂows with uniformly-
distributed and ﬁxed sizes. We present the average results of
ten simulated runs for each scenario with 100 coﬂows, where
coﬂow structures follow the distribution in Table 3.
In Figure 13a, coﬂow sizes follow uniform distributions
U(0, x), where we vary x. In Figure 13b, all coﬂows have
the same size, and we select sizes slightly smaller and big-
ger than Aalo’s queue thresholds. We observe that in both
cases, Aalo matched or outperformed the competition. Aalo
emulates the FIFO scheduler when coﬂow sizes are smaller
than Qhi
1 (=10 MB). As coﬂows become larger, Aalo per-
forms better by emulating the efﬁcient Varys scheduler.
7.6 Aalo Scalability
To evaluate Aalo’s scalability, we emulated running up to
100, 000 daemons on 100-machine EC2 clusters. Figure 14a
EC2$DAG(0!1!2!3!4!5!q19 (7)!q27 (6)!q3 (4)!q34 (6)!q42 (4)!q43 (4)!q46 (8)!q52 (4)!q53 (5)!q55 (4)!q59 (6)!q63 (5)!q65 (5)!q68 (8)!q7 (6)!q73 (5)!q79 (6)!q89 (5)!q98 (1)!ss_max (1)!Overall (5)!Norm. Comp. Time!TPC-DS Query ID (Critical Path Len Among Coﬂows)!Per-Flow Fairness!Varys!0!1!2!3!4!5!Norm. Comp. Time w.r.t. Aalo!TPC-DS Query ID (Critical Path Length in the Coﬂow DAG)!Per-Flow Fairness!Varys!0!1!2!3!1!2!5!10!15!Normalized Comp. Time!Number of Queues (K)!0!1!2!3!1E6!1E7!1E8!1E9!1E10!Normalized Comp. Time!Q1 Upper Limit (         )!Q1hi!0!1!2!3!Normalized          Comp. Time!Num Queues (K), Queue Size!0!1!2!3!Normalized         Comp. Time!K, E, !Q1hi!0!1!2!3!1!2!5!10!15!Normalized Comp. Time!Number of Queues (K)!0!1!2!3!1E6!1E7!1E8!1E9!1E10!Normalized Comp. Time!Q1 Upper Limit (         )!Q1hi!0!1!2!3!Normalized          Comp. Time!Num Queues (K), Queue Size!0!1!2!3!Normalized         Comp. Time!K, E, !Q1hi!0!1!2!3!1!2!5!10!15!Normalized Comp. Time!Number of Queues (K)!0!1!2!3!1E6!1E7!1E8!1E9!1E10!Normalized Comp. Time!Q1 Upper Limit (         )!Q1hi!0!1!2!3!Normalized          Comp. Time!Num Queues (K), Queue Size!0!1!2!3!Normalized         Comp. Time!K, E, !Q1hi!0!1!2!3!1!2!5!10!15!Normalized Comp. Time!Number of Queues (K)!0!1!2!3!1E6!1E7!1E8!1E9!1E10!Normalized Comp. Time!Q1 Upper Limit (         )!Q1hi!0!1!2!3!Normalized          Comp. Time!Num Queues (K), Queue Size!0!1!2!3!Normalized         Comp. Time!K, E, !Q1hi!Sim$Sensi5vity$SizeDist-0!1!2!10MB!100MB!1GB!10GB!100GB!1TB!Normalized       Comp. Time!Maximum Coﬂow Size!Per-Flow Fairness!FIFO w/o Multiplexing!0!1!2!10MB-!10MB+!1GB-!1GB+!100GB-!100GB+!Normalized       Comp. Time!Maximum Coﬂow Size!Per-Flow Fairness!FIFO w/o Multiplexing!Sim$Sensi5vity$SizeDist-0!1!2!10MB!100MB!1GB!10GB!100GB!1TB!Normalized       Comp. Time!Maximum Coﬂow Size!Per-Flow Fairness!FIFO w/o Multiplexing!0!1!2!10MB-!10MB+!1GB-!1GB+!100GB-!100GB+!Normalized       Comp. Time!Maximum Coﬂow Size!Per-Flow Fairness!FIFO w/o Multiplexing!403GbE NICs become commonplace, a common concern is
that scaling non-blocking fabrics might become cost pro-
hibitive.9 Aalo performs well even if the network is not non-
blocking – for example, on the EC2 network used in the
evaluation (§7). When bottleneck locations are known, e.g.,
rack-to-core links, Aalo can be modiﬁed to allocate rack-
to-core bandwidth instead of NIC bandwidth [17]. For in-
network bottlenecks, one can try enforcing coﬂows inside
the network [52]. Nonetheless, designing, deploying, and en-
forcing distributed, coﬂow-aware routing and load balancing
solutions remain largely unexplored.
9 Related Work
Coﬂow Schedulers Aalo’s improvements over its clairvoy-
ant predecessor Varys [20] are threefold. First, it schedules
coﬂows without any prior knowledge, making coﬂows prac-
tical in presence of task failures and straggler mitigation
techniques. Second, it supports pipelining and dependen-
cies in multi-stage DAGs and multi-wave stages through a
simpler, non-blocking API. Finally, unlike Varys, Aalo per-
forms well even for tiny coﬂows by avoiding coordination.
For larger coﬂows, however, Varys marginally outperforms
Aalo by exploiting complete knowledge.
Aalo outperforms existing non-clairvoyant coﬂow sched-
ulers, namely Orchestra [19] and Baraat [25], by avoiding
head-of-line blocking unlike the former and by using global
information unlike the latter. While Baraat’s fully decentral-
ized approach is effective for light-tailed coﬂow distribu-
tions, we prove in Theorem A.1 that the lack coordination
can be arbitrarily bad in the general case.
Qiu et al. have recently provided the ﬁrst approxima-
tion algorithm for the clairvoyant coﬂow scheduling prob-
lem [44]. Similar results do not exist for the non-clairvoyant
variation.
Flow Schedulers Coﬂows generalize traditional point-to-
point ﬂows by capturing the multipoint-to-multipoint aspect
of data-parallel communication. While trafﬁc managers like
Hedera [8] and MicroTE [16] cannot directly be used to opti-
mize coﬂows, they can be extended to perform coﬂow-aware
throughput maximization and load balancing.
Transport-level mechanisms to minimize FCTs, both
clairvoyant (e.g., PDQ [29], pFabric [10], and D3 [47]) and
non-clairvoyant (e.g., PIAS [14]), fall short in minimizing
CCTs as well [20].
Non-Clairvoyant Schedulers Scheduling without prior
knowledge is known as non-clairvoyant scheduling [39]. To
address this problem in time-sharing systems, Corbató et al.
proposed the multi-level feedback queue (MLFQ) algorithm
[23], which was later analyzed by Coffman and Kleinrock
[21]. Many variations of this approach exist in the literature
[42, 45], e.g., foreground-background or least-attained ser-
vice (LAS). In single machine (link), LAS performs almost
as good as SRPT for heavy-tailed distributions of task (ﬂow)
sizes [45]. We prove that simply applying LAS through-
9A recent report from Google [5] suggests that it is indeed possible to build
full-bisection bandwidth networks with up to 100, 000 machines, each with
10 GbE NICs, for a total capacity of 1 Pbps.
(a) Overheads at Scale
(b) Impact of ∆
Figure 14: [EC2] Aalo scalability: (a) more daemons require
longer coordination periods (Y-axis is in log scale), and (b) delayed
coordination can hurt overall performance (measured as improve-
ments over per-ﬂow fairness).
presents the time to complete a coordination round averaged
over 500 rounds for varying number of emulated daemons
(e.g., 10, 000 emulated daemons refer to each machine em-
ulating 100 daemons). During each experiment, the coordi-
nator transferred scheduling information for 100 concurrent
coﬂows on average to each of the emulated daemons.
Even though we might be able to coordinate 100, 000 dae-
mons in 992ms, the coordination period (∆) must be in-
creased. To understand the impact of coordination on perfor-
mance, we reran the earlier experiments (§7.2) for increas-
ingly higher ∆ (Figure 14b). For ∆ = 1s, Aalo’s improve-
ments over per-ﬂow fairness dropped slightly from 1.93× to
1.78×. For ∆ > 1s, performance started to drop faster and
plummeted at ∆ > 10s. These trends hold across coﬂow
bins and reinforce the need for coordination (Theorem A.1).
Because ∆ must increase for Aalo to scale, sub-∆ coﬂows
can further be improved if Aalo uses explicit switch/network
support [27, 25]. However, we note that tiny coﬂows are still
better off using Aalo than per-ﬂow fairness schemes.
8 Discussion
Determining Optimal Queue Thresholds Finding the op-
timal number of queues and corresponding thresholds re-
mains an open problem. Recent results in determining simi-
lar thresholds in the context of ﬂows [14] do not immediately
extend to coﬂows because of cross-ﬂow dependencies. Dy-
namically changing these parameters based on online learn-
ing can be another direction of future work.
Decentralizing Aalo Decentralizing D-CLAS primarily de-
pends on the following two factors.
1. Decentralized calculation of coﬂow sizes, and
2. Avoiding receiver-side contentions without coordination.
Approximate aggregation schemes like Push-Sum [34] can
be good starting points to develop solutions for the former
within reasonable time and accuracy. The latter is perhaps
more difﬁcult, because it relies on fast propagation of re-
ceiver feedbacks throughout the entire network for quick
convergence of sender- and receiver-side rates. Both can im-
prove from in-network support as used in CONGA [9].
Faster Interfaces and In-Network Bottlenecks As 10
Scalability-8!17!115!495!992!1!10!100!1000!100!1000!10000!50000!100000!Cooordination Time (ms)!# (Emulated) Daemons!0!0.5!1!1.5!2!10 ms!100 ms!1 s!10 s!100 s!Norm. Comp. Time!Coordination Period (Δ)!Scalability-8!17!115!495!992!1!10!100!1000!100!1000!10000!50000!100000!Cooordination Time (ms)!# (Emulated) Slaves!0!0.5!1!1.5!2!10 ms!100 ms!1 s!10 s!100 s!Norm. Comp. Time!Coordination Period (Δ)!404out the fabric can be ineffective in the context of coﬂows
(Theorem A.1). The closest instance of addressing a prob-
lem similar to ours is ATLAS [35], which controls concur-
rent accesses to multiple memory controllers in chip mul-
tiprocessor systems using coordinated LAS. However, AT-
LAS does not discretize LAS to ensure interactivity, and it
does not consider coupled resources like the network.
DAG and Workﬂow Schedulers When the entire DAG and
completion times of each stage are known, the Critical Path
Method (CPM) [33, 32] is the best known algorithm to min-
imize end-to-end completion times. Without prior knowl-