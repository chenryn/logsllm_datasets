### Optimized Text

Every row is utilized, which can be described as a convolution between them. To enforce compactness in the representations of the log messages, we utilize a Gaussian radial basis function (RBF) for the inclusion of operator-labeled samples. This RBF, denoted as \( l(\cdot) \), enables the addition of more realistic, albeit costly, anomaly samples that help to learn the anomaly distribution and further improve the model's performance.

The labeled samples either need to be added together with the auxiliary data and the model retrained, or the model can be pre-trained with the normal and auxiliary data followed by fine-tuning with the labeled data. With such a training procedure, the model extracts relevant information from the auxiliary data and learns good representations for anomaly detection, as demonstrated in the experiments. Replacing the auxiliary data with labeled samples allows the model to fine-tune its parameters in just a few epochs, preserving the already learned information from the larger auxiliary dataset as a bias for the fine-tuning process.

This ensures the compactness of normal samples, which are enforced to be around the center of a sphere \( c = 0 \). For normal samples (i.e., \( y_i = 0 \)), the loss function minimizes the distance to \( c \), resulting in low values for the left term in Equation 7. In contrast, the right term of the loss function favors large distances for anomalous samples. The center of the sphere \( c \) can be any constant value, which is not relevant during the optimization.

A possible issue that often arises in such spherical classifiers is that the model may learn trivial solutions by mapping inputs to a constant vector, i.e., \( c \). However, the proposed loss function prevents this trivial solution due to the second term in the equation, which represents the auxiliary data or anomalies. Formally, let \( \phi(\cdot) \) be the encoder network, which maps every log message to \( c \). If \( \phi(\cdot) = 0 \), the second term in Equation 7 for \( y_i = 1 \) will approach infinity, acting as a regularizer and preventing the learning of \( c \) as a trivial vector representation.

### Vector Representations of the Logs

Learning numerical vector representations from the logs is fundamental for the performance of any machine learning method for log anomaly detection. Logsy can be used to obtain such numerical log representations. These representations are used by the objective function of the method to perform anomaly detection and can also replace less powerful representations (e.g., TF-IDF in previous log-based anomaly detection methods like PCA [21]) to enhance their anomaly detection capabilities.

### Anomaly Score and Detecting Anomalies

Given that the objective function enforces compact, close-to-center-of-the-sphere representations, we define our anomaly score as the distance of the log vectors (obtained from the 'EMBEDDING' token) to the center \( c \) of the hypersphere:

\[
A(x_i) = \|\phi(x_i; \theta)\|^2
\]

We define low anomaly scores \( A(x_i) \) as normal log messages, while high scores indicate anomalies. To determine if a sample is anomalous or normal, we use a threshold \( E \). If the anomaly score \( A(x_i) > E \), the sample is classified as an anomaly; otherwise, it is considered normal.

### Including Expert Knowledge

Most computer systems are, to some extent, supervised and operated by an administrator. Over time, the administrator can manually inspect a small portion of the log events and provide labels. Logsy allows the incorporation of such labels from the target system. The second term in Equation 7, used for the auxiliary data, can also be adapted for these labeled samples. In the evaluation section, we demonstrate how the normal and abnormal samples are distributed in low-dimensional space.

### Evaluation

To quantify the performance of Logsy, we conduct a variety of experiments, comparing the method against two publicly available baselines, DeepLog and PCA, on three real-world HPC log datasets: Blue Gene/L, Spirit, and Thunderbird. We describe the main properties of the datasets, discuss the experimental setup, and present the results. We empirically and qualitatively evaluate the log vector representations from Logsy, where we observe improved performance when using them in the PCA method.

#### Experimental Setup

We select three open real-world datasets from HPC systems for evaluation: Blue Gene/L, Spirit, and Thunderbird [18]. These datasets share the characteristic of many new log messages appearing over time, indicating system changes. Additionally, we use the HPC RAS log dataset [31] to enrich the auxiliary data in all experiments. Due to the absence of labels, this dataset cannot be used for evaluation but serves as auxiliary data.

For each target dataset, we use logs from the remaining datasets as auxiliary data to represent the anomaly class. It is important to note that there is no information leak from the target system into the auxiliary data. The non-anomalous samples from the target system form the target dataset. For example, when Blue Gene/L is the target system, the negative samples from Thunderbird, Spirit, and RAS are used as auxiliary data to represent the anomaly class.

The datasets are collected between 2004 and 2006 on three different supercomputing systems: Blue Gene/L, Thunderbird, and Spirit. The logs contain both anomaly and normal messages, identified by anomaly category tags, making them suitable for anomaly detection and prediction research.

#### Results and Discussion

We show the overall performance of Logsy compared to the baselines. Logsy outperforms the baselines, especially in handling unseen log messages and reducing false positive rates. For instance, on the Blue Gene/L dataset, DeepLog and PCA show 2-4 times lower precision compared to Logsy. Overall, Logsy is the most accurate method, with an average precision of 0.9. High precision is crucial because too many false alarms can add unnecessary overhead and work for operators.

DeepLog leverages the indexes of log templates, which ignore the semantic meaning of the words in log messages. Different templates with different indexes can share common semantic information, and both could be normal. Ignoring this information results in more false positives for DeepLog compared to Logsy.

Increasing the training size generally improves the F1 score for almost all methods, except for the last two splits in the Spirit dataset, which have very few anomalies. Notably, Logsy outperforms the baselines even when only 10% of the data is used for training.