of the victim’s working set, i.e., the number of cache lines in
the subcache currently belonging to the victim.
This cache occupancy channel is the only side-channel
leakage that is not mitigated by the HYBCACHE construc-
tion, which is inherently available in any cache architecture
where the attacker and the victim processes compete for en-
tries in shared cache resources. It can only be effectively
blocked by strict cache partitioning, which we deliberately
do not provide in the HYBCACHE construction. This allows
different isolation domains to still compete for cache entries,
thus preserving maximum and dynamic cache utilization and
unaffected performance for non-isolated execution, as our per-
formance evaluation shows in Section 6.1. Note that, due to
S2, the information inferred by the attacker from observing
this remaining leakage, is effectively reduced to only knowing
the working set size at any point in time.
Leveraging this side channel to infer further information
and mount an attack in typical settings is not trivial. The vic-
tim may evict its own lines when it experiences cache misses
due to the random replacement policy. This would not effect
a difference in the cache state for the attacker, which compli-
cates the attacker’s bookkeeping. Moreover, observations are
severely hindered when any other software is concurrently
running besides the attacker and the victim processes. Finally,
standard software hardening techniques can be applied to
mitigate attacks to code implementations that are particularly
sensitive to this attack. Furthermore, exploiting this side chan-
nel to leak data has not been shown in practice. A recent
attack [67] leverages the cache occupancy side channel to
infer which website is open in a different browser tab (under
the strong assumption that no other tabs are open); however,
it does not leak any user data. Cache activity masking is
suggested as one of the countermeasures to the attack. Imple-
menting cache activity masking for HYBCACHE is feasible
and independent of our cache architecture.
Since the attacker aims to maximize its information and
cannot observe cache hits, s/he can attempt to evict all sub-
cache entries in order to maximize the number of misses expe-
rienced by the victim. As we discuss later, evicting the whole
subcache takes time for an attacker in either the NI-Domain
or in a I-Domain. An unprivileged attacker is unable to pause
the victim’s execution; thus, the attacker can only measure the
460    29th USENIX Security Symposium
USENIX Association
cache usage with limited granularity. However, a privileged
adversary, like a malicious OS in the case of an SGX enclave,
can stop and restart the victim arbitrarily and leverage tools
like SGX-Step [12] to observe the victim’s cache usage with
ﬁne granularity. HYBCACHE does not mitigate such an attack
by construction. However, mitigating it is only possible by
strict cache partitioning and the resulting performance costs.
We emphasize that we make an intentional design decision
in HYBCACHE to allow isolation domains to dynamically
compete for cache entries for maximum cache utilization and
unaffected performance for non-isolated execution. A HYB-
CACHE construction that dynamically allocates a dedicated
subcache for each isolation domain would block this leakage
and mitigate attacks that rely on it.
Non-isolated Attacker Process.
If the attacker process is
in the NI-Domain, in order to guarantee eviction of the whole
subcache it must ﬁll up all ways in every cache set, includ-
ing the subcache ways. Therefore, the attacker process must
construct an eviction set that is as large as the entire cache
capacity. A typical data L1 cache holds 512 cache entries.
In our experiments, probing (accessing and measuring ac-
cess latencies) of 512 cache lines takes approximately 30 000
CPU cycles, i.e., a little over 8 µs.2 For larger caches, such
as the LLC, it is not even feasible to mount Prime+Probe
attacks by probing the entire cache. The adversary is required
to pinpoint a few cache sets that correspond to the relevant
security-critical accesses made by the victim and monitor
these only [54].
Isolated Attacker Process.
If the adversary is in a differ-
ent I-Domain than the victim process, it still cannot control
cache eviction of particular target addresses speciﬁcally. Both
attacker and victim processes are isolated and can only use
the subcache ways. Thus, an adversary aiming to perform
controlled eviction can only try to evict the entire subcache.
Because the subcache is fully-associative with random re-
placement, evicting the entire subcache requires an eviction
set much larger than the subcache capacity. We argue below
that this is not easier than probing the entire L1 cache (in
case the attacker is non-isolated), for instance, even though
the subcache is signiﬁcantly smaller. Moreover, it can be only
guaranteed up to a certain level of probabilistic conﬁdence.
This can be represented statistically by the coupon collector’s
problem, where coupons are represented by entries in the sub-
cache. Let Naccesses be the total number of accesses needed
to evict all the subcache entries n and ni be the number of
accesses needed to evict the i-th way after i-1 ways have been
evicted. Both Naccesses and ni are discrete random variables.
The probability of evicting a new way becomes (n−(i−1))
. The
n
expected value and variance of Naccesses are
E(Naccesses) = n· Hn
V(Naccesses) ≈ π2
6
· n2
Hn denotes the nth harmonic number. For n = 128 subcache
entries, an average of 695 memory accesses (each mapping
to a different 64B cache line) is needed to evict the subcache
with a variance of ≈ 26 951. This is comparably more than
the 512 accesses required to probe the entire typical L1 cache
if the attacker process is not isolated (see above). Moreover,
with such a large variance, signiﬁcant variations in the number
of Naccesses required are expected from the mean E(Naccesses)
every time this eviction process is repeated.
6 Evaluation
Cache
L1
L2
L3
Size
64 KB
256 KB
Associativity
8-way associative
8-way associative
4 MB 16-way associative
Sets
128
512
4096
TABLE 1: Cache hierarchy used in our evaluation
Mix
pov+mcf
lib+sje
gob+mcf
ast+pov
h26+gob
bzi+sje
h26+per
cal+gob
Components
povray, mcf
libquantum, sjeng
gobmk, mcf
astar, povray
h264ref, gobmk
bzip2, sjeng
h264ref, perlbench
calculix, gobmk
pov+mcf+h26+gob
lib+sje+gob+mcf
povray, mcf, h264ref, gobmk
libquantum, sjeng, gobmk, mcf
TABLE 2: Benchmark mixes used in our evaluation
HYBCACHE is architecture-agnostic and applicable to x86,
ARM or RISC-V. We performed our performance evaluation
of HYBCACHE on a gem5-based [9] x86 emulator. We evalu-
ated the hardware overhead for an RTL implementation that
we implemented to extend an open-source RISC-V processor
Ariane [62]. For our prototyping, we applied HYBCACHE to
L1, L2, and LLC. We describe our evaluation results next.
6.1 Performance Evaluation
To evaluate HYBCACHE, we chose eight mixes of programs
from the SPEC CPU2006 benchmark suite, which are used in
the literature3 [36, 76], shown in the upper part of Table 2.
2We ran this experiment on an Intel i7-4790 CPU clocked at 3.60 GHz.
3 [76] also uses a ninth mix, dea+pov, which fails to run on gem5.
USENIX Association
29th USENIX Security Symposium    461
Two-Process Mixes.
In order to evaluate the impact of iso-
lating one process in the context of an SMT processor, we
conﬁgure gem5 to simulate two processors connected to a sin-
gle three-level cache hierarchy, whose parameters are shown
in Table 1. The caches have the latencies used in [76].
For each mix, we ﬁrst isolate one process, then the other,
and we compare the performance of those processes to a third
run in which neither process is isolated. We make either 2 or
3 of ways per set usable by the isolated execution processes.
The replacement policy for non-isolated processes is LRU.
Like in [76], we let gem5 simulate the ﬁrst 10 billion instruc-
tions of each process in order to let the process initialize,
then we measure the performance of one additional billion
instructions. We measure the performance overhead as the
relative change in the instructions-per-cycle (IPC), i.e., the
ratio between instructions executed and CPU cycles required.
A positive overhead represents a decrease in performance.
Figure 4 reports the IPC overhead of each program when
running in isolation mode, while the other member of the mix
runs in normal mode, for 2 or 3 isolated ways. The geometric
mean of the positive overheads is 4.95% with 2 isolated ways
and 3.47% with 3 isolated ways, with maximum overheads
of 16% and 14% respectively for the cal+gob mix. For this
mix, the overhead is due to a signiﬁcantly increased L3 cache
miss rate: the data miss rate jumps from 0.6% to 17.6%,
while the instruction miss rate increases from 2.1% to 9.0%.
The working set of calculix normally ﬁts in L3 [36] but it
does not in the subcache, hence the higher overhead. Since
HYBCACHE is meant to protect only sensitive applications,
which can be expected to be short-lived and only constitute
a minority of the workload of a system, we consider those
overheads easily tolerable. Figure 5 reports the IPC overhead
for the member of the mix that is not isolated. In all cases the
IPC overhead is not positive, i.e., the IPC is equal or better
than the baseline, thus showing that HYBCACHE does not
degrade the performance of non-isolated processes.
Four-Process Mixes. To demonstrate scalability, we also
ran four-process mixes, shown in the bottom part of Table 2.
We conﬁgured gem5 with four cores; two cores share an L1
and L2 cache, the other two cores share one additional L1
and L2, while L3 is shared by all cores. Isolated execution
can use two ways per set. We isolated each member of the
two mixes (the ﬁrst eight bars in Figure 6), while the other
three processes were running normally. Each isolated process
has an overhead similar to that reported in the two-process
mix experiments in Figure 4. Moreover, we also isolated two
processes in each mix (last two columns in Figure 6). In this
case, we measured increased overheads by up to 2 additional
percentage points due to the additional competition for the
subcache. However, those overheads are still easily tolerable
given the security beneﬁts and that they are only incurred by
the isolated execution.
FIGURE 4: IPC overhead of each isolated process when 2 or
3 ways are available to isolated execution. Each pair of bars
refers to a speciﬁc 2-process mix: the uppercase benchmark
is isolated and the other is not.
FIGURE 5: IPC overhead of each process when the other
member of the mix is isolated. Each pair of bars refers to a
speciﬁc 2-process mix: the uppercase benchmark is isolated
and the other is not.
FIGURE 6: IPC overhead of isolated processes for 4-process
mixes. The uppercase benchmarks are isolated and the others
are not. The last two columns have two bars each since two
process are isolated.
462    29th USENIX Security Symposium
USENIX Association
POV+mcfMCF+povLIB+sjeSJE+libGOB+mcfMCF+gobAST+povPOV+astH26+gobGOB+h26BZI+sjeSJE+bziH26+perPER+h26CAL+gobGOB+calgeomean0.0%5.0%10.0%15.0%IPC Overhead2 IW3 IWPOV+mcfMCF+povLIB+sjeSJE+libGOB+mcfMCF+gobAST+povPOV+astH26+gobGOB+h26BZI+sjeSJE+bziH26+perPER+h26CAL+gobGOB+cal-3.00%-2.00%-1.00%0.00%IPC Overhead2 IW3 IWLIBsjegobmcfSJElibgobmcfGOBlibsjemcfMCFlibsjegobPOVmcfh26gobMCFpovh26gobH26povmcfgobGOBpovmcfh26SJEMCFlibgobPOVMCFh26gob0.0%2.0%4.0%6.0%8.0%IPC Overheadnisolated NAND2X1 Gates Memory Overhead (Kb)
32
64
128
256
512
1024
2048
6114
12219
24563
48796
97830
201792
458300
0.34
0.68
1.3
2.75
5.5
11
22
TABLE 3: Logic and memory overhead estimates for fully-
associative lookup of 46-bit addresses for different numbers
of isolated cache ways (in any cache level).
6.2 Hardware and Memory Overhead
HYBCACHE requires additional hardware and memory for
the fully-associative lookup of the subcache entries. We im-
plemented the RTL for HYBCACHE and evaluated it for the
hardware overhead for different number of isolated cache
ways as shown in Table 3, irrespective of which cache levels
this is applied to. While the overhead of the additional hard-
ware is non-negligible, it is reasonable for a fully-associative
cache lookup. Nevertheless, it diminishes in perspective with
an 8-core Xeon Nehalem [1] of 2,300,000,000 transistors, for
example. The logic overhead of HYBCACHE for 2048 fully-
associative ways lookup is estimated at 1,833,200 transistors
(NAND2X1 count × 4) which is 0.07% overhead to the Xeon
Nehalem. For an 8-way 128-set cache, the memory overhead
in our PoC for fully-associative mapping is 7 additional tag
bits + 4 IDID bits per cache way. With respect to access la-
tencies, the exact timing latency of lookups will eventually
depend on the circuit routing but, in principle, for a paral-
lel content-addressable memory lookup (as in our hardware
PoC), accesses are performed in 2 clock cycles.
7 Discussion
Design and Implementation Aspects. HYBCACHE relies
on a random-replacement cache policy combined with full-
associativity to provide its dynamic isolation guarantees. The
implementation of the random replacement policy is dele-
gated to the hardware designer and considered an orthogonal
problem. Cryptographically-secure pseudo-random number
generators (CSPRNG) or even true hardware random number
generators can be used and the seed can be changed as often
as required. The output of the CSPRNG cannot be predicted
if it is seeded with secret randomness at the start of every pro-
cess. When the seed is changed, re-keying management tasks
such as cache ﬂushing and invalidation for the re-mapping
are not required, unlike in recent architectures [63, 74]. This
is because in HYBCACHE the randomness is only used for
selection of the victim cache line, and not for locating exist-
ing cache lines in the subcache. Furthermore, we emphasize
that CSPRNG design and implementations are an orthogonal
problem to our work.
The "soft" cache partitioning of HYBCACHE is a generic
concept and can be applied, in principle, to any set-associative
structure. In this work, we apply it to the L1, L2, and L3
(LLC) caches, but it can also be applied selectively to only