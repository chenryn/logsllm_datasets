D(cid:112)k(x, q). Then with probability 1 − δ,
(cid:32) ˜f
D(q)
2
R
+
2
2 R
ϵ
(cid:33)1/2(cid:112)32 log 1/δ
| ˆfD(q) − fD(q)| ≤
noise variance 2R
Proof. To use median-of-means, we add the (independent) Laplace
2
ϵ−2 to the variance bound in Theorem 2.5.
var(Xr) = σ
2 ≤(cid:16) ˜fD
(cid:17)2
2
+ 2 R
2
ϵ
Substituting this variance bound into Lemma 3.3 proves the theo-
rem.
□
3For simplicity, we suppose k and m are integers that evenly divide R. If this is not
the case, the results do not change substantially. However, the statement of the lemma
becomes more complicated. See [4] for the complete analysis.
Theorem 3.4 suggests a tradeoff for which there is an optimal
value of R. This may be thought of as a bias-variance problem,
similar to the one that arises with the orthogonal series estimators
from [3, 22, 45]. The privacy budget ϵ is shared by the R rows of
the sketch, and increasing R improves the quality of the median-
of-means estimate. If we increase R, we improve the estimator but
must add more Laplace noise to each row to preserve ϵ-differential
privacy. If we decrease the number of rows, we require less Laplace
noise but start with a worse estimator. To get our main utility
guarantee, we choose an optimal R that minimizes the error bound.
Surprisingly, the optimal R produces a bound with an asymptotically
better O(ϵ−1/2) dependence on ϵ than the O(ϵ−1) factor obtained
with any other of value of R.
error bound is
(cid:33)1/2
(cid:32) ˜fD(q)
| ˆfD(q) − fD(q)| ≤ 16
˜fD(q)ϵ⌉. Then the approximation
Corollary 3.5. Put R = ⌈ 1√2
(cid:114)
Proof. Take the derivative of(cid:112)a/R + bR with respect to R to
find that R = (cid:112)a/b minimizes the bound. Put a = ˜f
D(q) and
2
˜fD(q)ϵ
b = 2/ϵ
2. The corollary is obtained by substituting R = 1√2
into Theorem 3.4. We may replace ˜fD(q) with N in the inequality
because
log 1/δ
log 1/δ
≤ 16
N
ϵ
ϵ
˜fD(q) = 
k(x, q) ≤ 
(cid:112)
1 = N
x∈D
x∈D
□
3.3 Practical Implications
Although we use an average rather than the median-of-means
estimator in practice (Algorithm 2), our theory still has direct con-
sequences for applications. In this section, we interpret the practical
implications of our utility theorem.
Low-Density Queries: Corollary 3.5 suggests that small values
of fD are difficult to estimate accurately. If we divide both sides of
Corollary 3.5 by fD, we have a bound on the relative (or percent)
error rather than the absolute error. The percent error bound has
a ˜fD(q)−1/2 factor that can be very large if fD(q) is small. This
agrees with our intuition about how the KDE should behave under
differential privacy guarantees. Fewer individuals make heavy con-
tributions to fD in low-density regions than in high-density ones,
so the effect of the noise is worse for sparsely-populated portions of
the histogram. This can be seen in Figure 2, where the low-density
regions appear to be noisier.
Hyperparameter Tuning: In practice, we must spend a por-
tion of the privacy budget to evaluate each hyperparameter combi-
nation, so it is important to select good parameters to avoid wasting
resources. Our sketch has three hyperparameters: W , R, and the
kernel k(x, q). In many cases, the kernel is known and the value of
W can be determined from the kernel or else set to a large constant
value [12]. The main challenge, then, is to select a good value of R.
Corollary 3.5 states that the optimal value of R is R∗ = ⌈ 1√2
˜fD(q)ϵ⌉,
which implies that 1 ≤ R∗ ≤ 1√2 N ϵ. Unfortunately, we cannot di-
rectly compute R∗ unless we have apriori knowledge of the expected
value Eq[ ˜fD(q)] over the queries issued by a specific application.
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3257However, we can identify a good value of R by evaluating a few
choices from the interval [1,
1√2 N ϵ]. A principled and efficient way
to select R is to use binary search over this interval.
4 APPLICATIONS
The ability to release a pairwise sum is broadly useful for many
problems in machine learning. We present private algorithms for
density estimation, classification, regression, mode finding, anom-
aly detection and sampling using our sketch. All of these algorithms
are derived by releasing a useful instance of the LSH kernel sum fD.
Given a sufficiently good estimate of fD, the following applications
are possible:
Kernel Density Estimation: Kernel density estimation is a
classical nonparametric method to directly estimate a distribution
from a dataset. To use RACE for KDE, we select one or more LSH
kernels from the options described by [12]. We require a separate
sketch for each kernel and bandwidth setting. Figure 2 shows a
visualization of our private density estimation method.
Mode Finding: Given access to the probability density, we can
locate the modes of the data distribution. Gradient-free optimization
over the sketch works surprisingly well, but in general the KDE is a
non-convex function. An alternative (but more expensive) approach
is to intersect the hash partitions to identify the mode as a point
from the partition with the largest count values [11].
Naive Bayes Classification: The KDE is a convenient way to
estimate likelihood functions in statistical estimation. Using kernel
density classification, a well-developed result from statistical hy-
pothesis testing [25], we can construct classifiers with RACE under
both the maximum-likelihood and maximum a posteriori (MAP)
decision rules. To make this example more concrete, suppose we
are given a training set D with M classes C1, ...CM and a query q.
We can represent the empirical likelihood Pr[q|Ci , D] with a sketch
of the KDE for class i. Algorithm 2 returns an estimate of this prob-
ability, which may be used directly by a naive Bayes classifier or
other type of probabilistic learner.
Anomaly Detection / Sampling: Anomaly detection can be
cast as a KDE classification problem. If Algorithm 2 reports a low
density for q, then the training set contains few elements similar to
q and thus q is an outlier. This principle is behind the algorithms
in [29] and [10].
Empirical Risk Minimization: The applications presented so
far have only used one type of function fD, the KDE sum. KDE sums
are simple to estimate because most LSH functions have positive
symmetric collision probabilities. However, LSH kernel sums can
be combined to estimate a much more diverse set of functions than
positive semidefinite kernel sums. Using asymmetric LSH [36], we
can obtain asymmetric kernels by applying different hashes to x
and q. We may also add and subtract LSH kernels by incrementing
SD using multiple hash functions or incrementing by values other
than 1, although the sensitivity in Theorem 3.2 increases for each
additional function. To maintain ϵ-differential privacy, we must
properly scale the noise so that the sensitivity is correct.
This flexibility allows RACE to perform empirical risk minimiza-
tion (ERM). In ERM problems, we are given a dataset D = {z1...zN }
of training examples and a loss function L(θ, z), where θ is a pa-
rameter that describes a predictive model that we wish to train
over the dataset. The task is to find a model θ that minimizes the
mean loss over the training set. Using the sketch, we can privately
approximate the loss sum when L(θ, z) can be expressed in terms
of LSH collision probabilities.
Optimization over Sketches: Given a model parameter θ, we can
estimate the empirical risk by querying the sketch with (possibly
some transformation of) θ. Although we cannot analytically find
the gradient of the RACE count values, black-box access to the loss
function is sufficient to optimize the model with derivative-free
optimization [13]. The idea is to query the sketch using random
points surrounding θ to approximate the gradient. Since the sketch
has already been released with differential privacy, we can iterate
until θ converges to the optimal model without consuming the
privacy budget. Our experiments (Figure 5) show that derivative-
free optimization is highly effective over count-based sketches,
validating results from [11].
Constructing Losses from Kernel Sums: To train a specific
ERM model using the RACE sketch, we must approximate the loss
function L(θ, z) using LSH kernels. Our goal is to construct a hash
function whose collision probability forms a surrogate loss for the
model of interest. For example, consider the linear regression loss
L([x, y], θ) = (⟨[x, y],[θ,−1]⟩)2. We wish to use LSH kernels to
build a surrogate loss with the same minimum as L. Note that
the SRP kernel from [12] is a monotone increasing function of the
inner product ⟨[x, y],[θ,−1]⟩. We may also obtain a kernel that is a
monotone decreasing function of the inner product by multiplying
the query by −1 (i.e. hashing −[θ,−1] rather than [θ,−1]). If we
query the sketch with with both ±[θ,−1] and add the results, we
estimate the quantity:
kSRP(⟨[x, y],[θ,−1]⟩) + kSRP(−⟨[x, y],[θ,−1]⟩)
fD(θ) = 
[x,y]∈D
One can show that fD(θ) is a convex surrogate loss for linear
regression [11]. By minimizing fD(θ) with derivative-free opti-
mization, we may train a private regression model from the sketch.
One can use a similar process to design a surrogate loss for linear
SVMs. For details, see [11], but note that some of the modifications
proposed in their work prevent the sketches from being merged or
released with differential privacy. For a discussion of techniques to
extend this framework beyond linear regression and classification,
see Section 7.1.
5 EXPERIMENTS
We perform an exhaustive comparison for KDE, classification and
regression. Table 2 presents the datasets used in our experiments.
For KDE, we estimate the density of the salaries for New York
City (NYC) and San Francisco (SF) city employees in 2018, as well
as high-dimensional densities for the skin, codrna and covtype
UCI datasets. The purpose of our one-dimensional experiment is
to visualize the functions released by various algorithms, while
the other datasets are included in our evaluation to benchmark
function release methods on large and high-dimensional datasets.
Note that for the covtype KDE task, we were unable to run existing
methods due to the size of the data. We needed to apply sampling
and dimensionality reduction to compare against baselines. We
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3258Dataset
NYC
SF
skin
codrna
covtype
nomao
occupancy
pulsar
airfoil
naval
gas
N
25k
29k
241k
57k
580k
34k
17k
17k
1.4k
11k
3.6k
d
1
1
3
8
55
26
5
8
9
16
128
Description
SF salaries (2018)
σ
5k NYC salaries (2018)
5k
5.0 RGB skin tones
0.5 RNA genomic data
20 Cartographic features for forestry
0.6 User location data
0.5 Building occupancy
0.1
-
-
-
Pulsar star data
Airfoil parameters and sound level
Frigate turbine propulsion
Gas sensor, different concentrations
Task
KDE
Classification
Regression
Table 2: Datasets used for KDE and classification experiments. Each dataset has N entries with d features. σ is the kernel
bandwidth.
PFDA
Sketch > 3 days
Query
-
Bernstein
2.3 days
6.2 ms
KME
6 hr
1.2 ms
RACE
15 sec
0.4 ms
PrivBayes
12 sec
0.5 sec
Table 3: Computation time for KDE on the skin dataset.
PFDA was unable to finish on this dataset within 3 days.
also include experiments specifically designed to test the scaling
capacity of RACE in Section 6.
For the regression and classification experiments, we use UCI
datasets of moderate size (N > 1k) and dimensionality (3 ≤ d ≤