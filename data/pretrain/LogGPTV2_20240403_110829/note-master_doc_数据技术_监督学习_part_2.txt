![SVM的几何意义](/assets/202411519416.webp)
硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误
非线性SVM：有些数据样本没法通过线性函数划分，就引入核函数将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分
多分类问题：SVM 本身是一个二值分类器，最初是为二分类问题设计的，可以将多个二分类器组合起来形成一个多分类器：
1. 一对多：把其中的一类作为分类 1，其他类统一归为分类 2
2. 一对一：在任意两类样本之间构造一个 SVM，每一个分类器都会有一个分类结果，得票多者胜
### 核技巧
核函数（kernel function）用于将数据从原始空间映射到一个更高维的特征空间
核函数的数学表达
$$
k(\mathbf{x},\mathbf{x}')=\phi(\mathbf{x})^T\phi(\mathbf{x}')
$$
核技巧（kernerl trick）表示的是通过间接定义特征映射来直接计算内积的运算方法
核方法（kernel method）表示的是将低维空间中的线性不可分问题通常可以转化为高维空间中的线性可分问题的思路
## 决策树
需要对结构化的数据进行预测，在结果为某种分类时，适合使用。决策树尝试得到训练数据的多棵决策树，从中选择一个效果最好的
```mermaid
stateDiagram-v2
    根条件 --> 条件1
    根条件 --> 条件2
    条件1 --> 条件3
    条件1 --> 条件4
    条件3 --> 结果1
    条件4 --> 结果2
    条件2 --> 结果1
```
1. 根条件的选择：选择能将左右分支两个集合的熵划分的最小的条件
2. 何时停止划分
   1. 节点下的集合都属于同一个分类
   2. 树已经达到了最大深度，继续划分只会变得过拟合
   3. 划分带来的纯度提升小于设定阈值
   4. 节点下集合项小于设定阈值
使用信息增益衡量每次划分减少了多少熵，结果越大代表减少的熵越多：
$$
H(p_1^{root}-\left(w^{left}H\left(p_1^{left}\right)+w^{right}H\left(p_1^{right}\right)\right)
$$
对于取指不止两个的特征，可以采取 one-hot 编码，如果一个特征有 k 个取指，那就可以把它转换成 k 个只能取0 1 的特征
对于拥有连续值的特征，则需要多次尝试不同的取值，使得以该值划分的两类集合获得的信息增益最高
### 剪枝
训练出来的决策树可能会对训练数据**过拟合**，所以需要在合适的情况下停止拆分决策树的子节点
预剪枝：在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，就不对其进行拆分
后剪枝：构造完决策树之后，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类
### ID3算法
计算的是信息增益，就是划分可以带来纯度的提高，倾向于选择取值比较多的属性，但这个属性可能对于分类并没有太大作用
### C4.5算法
- 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵
- 悲观剪枝：递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝
- 对于连续属性的处理是通过将连续属性离散化来实现，选择具有最高信息增益的划分所对应的阈值
- 对于缺失值所在的样本，将其划分到各个子集中，并计算其对应的信息增益，最终选择信息增益最大的子集进行划分
### CART算法
- Classification And Regression Tree，分类回归树
分类树处理离散数据，输出样本类别；回归树处理连续数据，输出回归预测
基尼系数是一种衡量纯度的方法：随机选取两个样本，其类别不一致的概率，这个概率越低则样本最稳定
CART 分类树算法中，基于基尼系数对特征属性进行二元分裂
CART 回归树算法中，根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”
剪枝：采用的是代价复杂度剪枝方法
### 随机森林
一个包含多个决策树的分类器，每一个子分类器都是一棵 CART 分类回归树
利用属性随机化和数据随机化构造决策树，数据随机化最主要的作用在于去除各独立模型之间的相关性，属性随机化的好处在于让每个单独的基学习器不会过分关注在训练集中具有高度预测性或者描述性的特征，让模型有更好的泛化性
为了随机构造多棵树，需要使用采样替换技术：每次随机抽取一条训练数据记录，放回，重新抽取并重复这个过程，这样可以生成多份随机的训练，并使用这些随机的训练数据去生成多颗决策树
做分类的时候，输出结果是每个子分类器的分类结果中最多的那个，做回归的时候，输出结果是每棵 CART 树的回归结果的平均值
### 极端随机森林
- 特征的选择： 在构建决策树的过程中，随机森林通过在每个节点上从特征集中选择最佳分割特征来引入随机性。而极端随机森林进一步增加了随机性，它在每个节点上随机选择一个特征进行分割，而不是从特征集中选择最佳的
- 样本的选择： 随机森林在构建每个决策树时都会从训练数据中随机有放回地抽取样本。极端随机森林通过在每个节点上随机选择一个阈值来进行分割
### 梯度提升树(GBDT)
是基于决策树的集成学习方法之一。它通过串行训练一系列决策树来改善模型的准确性，每个新树都是为了纠正前面树的错误而构建的。梯度提升的核心思想是迭代地训练弱学习器（通常是决策树），每一轮都试图修正前一轮模型的残差
XGBoost 是一种开源实现
## 时间序列预测
- Auto Regressive：自回归模型。这个算法的思想比较简单，它认为过去若干时刻的点通过线性组合，再加上白噪声就可以预测未来某个时刻的点
- Moving Average：滑动平均模型。它与 AR 模型大同小异，AR 模型是历史时序值的线性组合，MA 是通过历史白噪声进行线性组合来影响当前时刻点
- Auto Regressive Moving Average：自回归滑动平均模型，也就是 AR 模型和 MA 模型的混合
- Auto Regressive Integrated Moving Average：相比于 ARMA，ARIMA 多了一个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模
## 回归树
每一片叶子都输出一个预测值。预测值一般是该片叶子所含训练集元素输出的均值
回归树中每个节点选择那个特征，都遵循划分使得一个节点下所有值的方差最小的原则，这点跟使用信息增益来分裂节点原理一样
## KNN
找出k个与当前元素相似的元素，对这些元素求均值，从而做出对当前元素的预测
过多或过少的k都会导致结果不准
如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样 KNN 分类就会产生过拟合。
如果 K 值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。一般采用交叉验证的方式选取 K 值
KNN 算法的运行方式和以线性回归为代表的参数方法截然相反，线性回归的参数一旦被计算出来，训练数据的历史使命就完成，而 KNN 只需要将数据存储下来就行了，但是对新实例进行分类时，k 近邻算法需要找到离它最近的 k 个训练实例，这才是算法主要的运算负荷
### 近邻权重
将相似度（距离）转为权重，即不同点贡献的权重不一样，加权KNN在进行预测时，不同邻居的贡献被赋予不同的权重
- 反函数：距离的导数
- 减法函数：使用一个常量减去距离，如果为正数，则权重就是为该正数，否则权重为0
- 高斯函数：在距离为0时权重为1，随着距离增加无限接近0
### 费舍尔方法
一种用于处理多类别分类问题的线性分类方法，所谓线性分类，即构造分类中所有数据的平均值为中心点，新的数据根据距离离中心点的距离判断是哪类，SVM 就是一种线性分类