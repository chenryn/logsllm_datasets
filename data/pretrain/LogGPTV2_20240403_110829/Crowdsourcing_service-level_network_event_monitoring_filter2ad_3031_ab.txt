### Null Hypothesis and Network Problem Detection

We start with the null hypothesis that each host experiences events independently, not due to network problems. By comparing this hypothesis to the observed rate of local events occurring concurrently for hosts in a network, the Correlated Event Monitoring (CEM) system can determine the relative likelihood of the detected problem being caused by the network rather than by chance. This is quantified using a likelihood ratio, a method commonly used in medical diagnostics to determine the probability of a condition (e.g., a disease) being present.

### Accounting for Dependencies

The first step in the likelihood analysis is to determine the probability that each of the \( N \) participating hosts detects local service problems independently. For each host \( h \), we create a time series \( S_h = \{ s_{h,i}, s_{h,i+1}, \ldots, s_{h,j} \} \) over the time period \( T = [i, j] \). At any time \( t \), \( s_{h,t} = 1 \) if a local event was detected, and \( s_{h,t} = 0 \) otherwise. The probability of host \( h \) detecting a local event in any given time bucket during \( T \) is estimated as:

\[ L_h = \frac{1}{j - i} \sum_{t=i}^{j} s_{h,t} \]

To account for service-specific dependencies, any set of hosts whose performance is mutually dependent during a time interval \( (i-1, i] \) are treated as a single host for the analysis. For example, in a P2P file-sharing application, performance issues experienced by peers downloading the same file and connected to each other are not considered independent events. Additionally, our approach can incorporate automatically generated dependencies from tools like Orion [6].

### Quantifying the Probability of Concurrent Events

After accounting for dependencies, we quantify the probability of \( n \) out of \( N \) independent hosts detecting an event at the same time by coincidence, i.e., the joint probability that for a given time \( t \), \( \sum_{h=1}^{N} s_{h,t} \geq n \).

For \( N = 10 \) and \( c = 2 \):
\[ P\left( \sum_{h=1}^{10} s_{h,t} \geq 2 \right) \]

For \( N = 10 \) and \( c = 4 \):
\[ P\left( \sum_{h=1}^{10} s_{h,t} \geq 4 \right) \]

For \( N = 25 \) and \( c = 2 \):
\[ P\left( \sum_{h=1}^{25} s_{h,t} \geq 2 \right) \]

For \( N = 25 \) and \( c = 4 \):
\[ P\left( \sum_{h=1}^{25} s_{h,t} \geq 4 \right) \]

For \( N = 50 \) and \( c = 2 \):
\[ P\left( \sum_{h=1}^{50} s_{h,t} \geq 2 \right) \]

For \( N = 50 \) and \( c = 4 \):
\[ P\left( \sum_{h=1}^{50} s_{h,t} \geq 4 \right) \]

The union probability for any one host seeing an event is:

\[ P\left( \bigcup_{h=1}^{N} L_h \right) = \sum_{h=1}^{N} P(L_h) - \sum_{j > h} P(L_h)P(L_j) + \ldots + (-1)^{n-1} P(L_1) \ldots P(L_N) \]

This equation gives the union probability for any one host seeing an event, which is generally much larger than the probability that at least \( n \) hosts see an event. The probability that at least \( n \) hosts see an event is:

\[ P\left( \sum_{h=1}^{N} s_{h,t} \geq n \right) = \sum_{h=1}^{N} P(L_h) - \sum_{j > h} P(L_h)P(L_j) + \ldots + (-1)^{n-1} P(L_1) \ldots P(L_N) \]

### Effect of Corroboration

Intuitively, our confidence in a detected event being due to the network increases with (i) the number of hosts detecting the event and (ii) the number of independent performance signals indicating the event. We quantify the impact of these factors through a simulation of a region of interest (e.g., a BGP prefix) with \( N \) hosts. Each host provides multiple performance signals as described in Section 3.1.1. The probability of host \( h \) witnessing an event in a signal, \( L_h \), is chosen uniformly at random in the range \( 0.005 \leq L_h \leq 0.05 \). We then determine the probability of \( c \) hosts (where \( 1 \leq c \leq N \)) detecting the event. A likelihood ratio greater than 1 indicates that detected events are occurring more often than by coincidence for a given network and detection settings, suggesting a network problem.

### Problem Isolation

When many hosts in a network detect an event simultaneously, it is usually a problem best addressed by the responsible network operators. Our approach should be able to identify the affected networks so that operators can determine the root cause and fix the problem. The scope of a problem is explicitly determined by the grouping of hosts for the likelihood analysis. This approach supports localization of problems using structural information about the organization of networks and their geographic locations. For instance, it can use events detected by hosts in the same routable BGP prefix or ASN, and use geographic information to localize events to cities and countries. Further, CEM can use an AS-level Internet graph to localize network issues to upstream providers or a router-level graph to isolate problematic routers and links.

### Implementing CEM

#### Design and Deployment Challenges

Designing, deploying, and evaluating CEM poses interesting challenges given the absence of a platform for experimentation at the appropriate scale. A promising way to address this is by leveraging the network view of peers in large-scale P2P systems. P2P systems use decentralization to enable a range of scalable, reliable services and are so prevalent that reports indicate they generate up to 70% of Internet traffic [30]. By avoiding the need to deploy additional infrastructure and offering hosts that are already cooperating, these systems are an appealing vehicle for monitoring – one that grows naturally with the network [9, 36].

Based on these advantages, we choose to design and evaluate a prototype implementation of CEM in a large P2P system. To guide the design of our prototype and evaluate its effectiveness at scale, we take advantage of a large edge-system dataset comprising traces of BitTorrent performance from millions of IP addresses.

#### Datasets

**BitTorrent Traces:**
- **Data Collection:** The BitTorrent traces are gathered from users of the Ono plugin for the Vuze BitTorrent client. Ono implements a biased peer selection service aimed at reducing the amount of costly cross-ISP traffic generated by BitTorrent without sacrificing system performance [8]. Beyond assisting in peer selection, the software allows subscribing volunteers to participate in a monitoring service for the Internet. With more than 1,000,000 users distributed in 212 countries, this system is the largest known end-system monitoring service.
- **Trace Details:** The dataset consists of transfer rates for each connection and cumulative transfer rates (over all connections), sampled every 30 seconds. It also includes protocol-specific information such as whether each peer is leeching (both downloading and uploading) or seeding (only uploading), the total number of leechers and seeds, and information about the availability of data for each download.
- **Edge Coverage:** The dataset currently contains connection information from users to more than 390,000,000 peer IPs, collectively monitoring more than 17 million paths per day. Ono’s user base has grown to over 72,100 prefixes (covering nearly every country) in less than three years. These users have established connections to peers in over 222,000 routable prefixes and 21,800 ASNs.

**Confirmed Network Problems:**
- **Evaluation Data:** Evaluating the effectiveness of a network event detection approach requires a set of ground-truth events. We use publicly available event reports from the British Telecom (BT Yahoo) ISP in the UK, which identifies the times, locations, and nature of network problems. During April 2009, there were 68 reported problems, including Internet and POTS events. Additionally, we use network problems reported from a large North American ISP, though we cannot report absolute numbers for these events due to nondisclosure reasons.

### Case Study

To assist with the presentation of NEWS, we demonstrate how it detects a reported problem in BT Yahoo. On April 27, 2009, at 3:54 PM GMT, the network status page stated, "We are aware of a network problem which may be affecting access to the internet in certain areas..." The problem was marked as resolved at 8:50 PM. Figure 3 presents a scatter plot timeline of upload rates for peers located in the same routable prefix in BT Yahoo (81.128.0.0/12) during this event, depicted as a shaded region. Each point in the graph represents an upload-rate sample for a single peer; different point shapes/colors represent signals for different peers. The figure shows that multiple peers experience reduced performance between 10:54 and 16:54, while another set of peers see a significant drop in transfer rates at 14:54. These observations are consistent with the reported event, accounting for delays between the actual duration of an event and the time assigned to it by a technician. Further, we see that there were two distinguishable network problems corresponding to the single generic report.

### Network Monitoring from BitTorrent

We now discuss key design aspects of NEWS, a prototype edge-system monitor for BitTorrent. Throughout this discussion, we use the confirmed BT Yahoo event in Figure 3 to explain our design choices. With respect to the design challenges listed in Section 2, we address scalability and granularity through our local event detection and group corroboration approach. The remaining issues of privacy, trust, and adoption are covered in subsequent sections. We provide low-level implementation details in Section 6.