null hypothesis that each host experiences events independently
and not due to network problems. By comparing this value to the
observed rate of local events occurring concurrently for hosts in a
network, CEM can determine the relative likelihood of the detected
problem being caused by the network instead of by chance. We
quantify this with a likelihood ratio, which has been used, e.g.,
in the ﬁeld of medicine for diagnostic testing to determine the
probability that a condition (e.g., a disease) is present.
Accounting for dependencies. The ﬁrst step in the likelihood
analysis is to determine the probability that each of the N partic-
ipating hosts detects local service problems independently. Thus,
for each host h we produce a series Sh = {sh,i, sh,i+1, ..., sh,j}
for the time period T = [i, j], such that at time t, sh,t = 1 if a local
event was detected and sh,t = 0 otherwise. During the time period
T , we use the observed detection rate to estimate the probability of
host h detecting a local event in any given bucket as:
Lh =
1
j − i
Pj
t=i sh,t
To control for service-speciﬁc dependencies, any set of hosts
whose performance is mutually dependent during a time interval
(i − 1, i] are treated as the same host during that interval for
the purpose of the analysis. Thus, such hosts do not corroborate
each other’s events. For example,
in the case of a P2P ﬁle-
sharing application, performance problems seen by peers that are
downloading the same ﬁle and connected to each other are not
treated as independent events. Besides such explicit dependencies,
our approach can incorporate automatically generated ones from a
tool like Orion [6].
After this step, our approach quantiﬁes the probability of n (out
of N ) independent hosts detecting an event at the same time by
coincidence, i.e., the joint probability that for a given time t,
Ph sh,t ≥ n.
N=10, c=2
N=10, c=4
N=25, c=2
N=25, c=4
N=50, c=2
N=50, c=4
]
r
h=1 P (Lh ∩ Lj) + ...
+ (−1)n−1P (L1 ∩ ... ∩ LN )
(1)
We are testing the hypothesis that the events are independent, so
we can simplify the union probability:
P (SN
h=1 Lh) = PN
h=1 P (Lh) − PN
j>h=1 P (Lh)P (Lj ) + ...
+ (−1)n−1P (L1)...P (LN )
(2)
This equation gives the union probability for any one host seeing
an event, i.e., without corroboration. Generally, this is much larger
than the probability that at least n hosts (1 h=1 Lh ∪ Lj) = PN
j>h=1 P (Lh)P (Lj )
k>j>h=1 P (Lh)P (Lj )P (Lk)
− PN
+ ... + (−1)n−1P (L1)...P (LN )
(3)
Effect of corroboration.
Intuitively, our conﬁdence in a de-
tected event being due to the network increases with (i) the number
of hosts detecting the event and (ii) the number of independent
performance signals indicating the event. We now quantify the
impact of these factors through a simulation of a region of interest
(e.g., a BGP preﬁx) with N hosts. Each of these hosts provides
multiple performance signals as described in Sec. 3.1.1. The
probability of host h witnessing an event in a signal, Lh, is chosen
uniformly at random in the range 0.005 ≤ Lh ≤ 0.05. We then
determine the probability of c hosts (1  1 indicates that detected events are
occurring more often than by coincidence for a given network and
detection settings. We consider these to be events indicative of a
network problem.
3.2.2 Problem Isolation
When many hosts in a network detect an event at the same time,
it is usually a problem best addressed by the responsible network
operators. In such cases, our approach should be able to identify
the network(s) affected by the event so as to provide the necessary
information for operators to determine the root cause and ﬁx the
problem.
In our approach, the scope of a problem is explicitly determined
by the grouping of hosts for the likelihood analysis. As such, the
approach supports localization of problems using structural infor-
mation about the organization of networks and their geographic
locations. For instance, it can use events detected by hosts in the
same routable BGP preﬁx or ASN, and use geographic information
to localize events to cities and countries. Further, CEM can use
an AS-level Internet graph to localize network issues to upstream
providers or a router-level graph to isolate problematic routers and
links.
4.
IMPLEMENTING CEM
The previous section described our CEM approach for detecting
events from edge systems. Designing, deploying and evaluating
CEM poses interesting challenges given the absence of a platform
for experimentation at the appropriate scale.
A promising way to address this is by leveraging the network
view of peers in large-scale P2P systems.
P2P systems use
decentralization to enable a range of scalable, reliable services and
are so prevalent that reports indicate they generate up to 70% of
Internet trafﬁc [30]. By avoiding the need to deploy additional
infrastructure and offering hosts that are already cooperating, these
systems are an appealing vehicle for monitoring – one that grows
naturally with the network [9, 36].
Based on these advantages, we choose to design and evaluate
a prototype implementation of CEM in a large P2P system. To
guide the design of our prototype and evaluate its effectiveness at
scale, we take advantage of a large edge-system dataset comprising
traces of BitTorrent performance from millions of IP addresses.
The following paragraphs describe this unique dataset, a collection
of conﬁrmed network problems we rely on for evaluation, and a
particular case study we use in our presentation. We close the
section describing the Network Early Warning System (NEWS), our
Category
Number (Pct of total)
Number of users
Countries
IP addresses
Preﬁxes
Autonomous systems (ASes)
IPs behind middleboxes
1,000,000
212
4,300,000
72,100
8,700
≈ 82.6%
Table 1: Summary of our P2P vantage points.
prototype edge-based event detection system that uses BitTorrent as
a host application. NEWS is currently deployed as a plugin for the
Vuze BitTorrent client [34], to facilitate adoption and to piggyback
on the application’s large user base.
Building on P2P systems to provide network monitoring is not
without limitations. For one, each monitor contributes its view
only while the P2P system is active, which is subject to user
behavior beyond our control. Second, the end system may run
other applications that interfere with P2P applications and event
detection. Finally, some event detection techniques require access
to information not accessible to a P2P application, e.g., system
calls.
4.1 Datasets
The following paragraphs present the data collected from BitTor-
rent and a set of conﬁrmed network problems from two ISPs.
4.1.1 BitTorrent traces
The BitTorrent traces we use are gathered from users of the Ono
plugin for the Vuze BitTorrent client.2 Ono implements a biased
peer selection service aimed at reducing the amount of costly
cross-ISP trafﬁc generated by BitTorrent without sacriﬁcing system
performance [8]. Beyond assisting in peer selection, the software
allows subscribing volunteers to participate in a monitoring service
for the Internet. With more than 1,000,000 users today, distributed
in 212 countries,
this system is the largest known end-system
monitoring service. The following paragraphs describe the data
collected; summary information about Ono users is in Table 1.
Trace details. Our dataset consists of transfer rate for each
connection and cumulative transfer rates (over all connections),
all sampled once every 30 seconds. Besides this, the dataset
includes protocol-speciﬁc information such as whether each peer
is “leeching” (both downloading and uploading) or “seeding” (only
uploading), the total number of leechers and seeds, and information
about the availability of data for each download. The complete list
of collected signals is in Table 2. Note that this collection is for our
design and evaluation only and it is not required for NEWS event
detection.
Edge coverage. Any dataset is subject to limits in the coverage
of its measurement hosts. The dataset we use currently contains
connection information from users to more than 390,000,000 peer
IPs; collectively, its users monitor more than 17 million paths per
day. Ono’s user base has grown to over 72,100 preﬁxes (covering
nearly every country) in less than three years. Collectively,
these users have established connections to peers in over 222,000
routable preﬁxes and 21,800 ASNs.
Besides covering many paths,
the dataset reaches true edge
systems located in portions of the Internet not accessible to existing
distributed research and monitoring platforms. For example, over
80% of the user IPs correspond to middleboxes (i.e., they are
assigned private IP addresses). Further, we ﬁnd that the P2P trafﬁc
in these traces covers paths invisible to public views. Speciﬁcally,
2Users are informed of the diagnostic information gathered by the plugin
and are given the chance to opt out. In any case, no personally identiﬁable
information is ever published.
391we map three months of BitTorrent trafﬁc data to AS pairs, then
use 13 months of publicly available BGP data to map the trafﬁc to
AS-level paths. We ﬁnd that a large majority of BitTorrent trafﬁc
in this dataset cannot be mapped to a public BGP path [7]. These
results are consistent with ﬁndings by Chen et al. [5] demonstrating
that public views miss large portions of the Internet topology
covered by P2P users.
4.1.2 Conﬁrmed network problems
Evaluating the effectiveness of a network event detection ap-
proach requires a set of events that should be detected, i.e., a set
of ground-truth events. Among the different strategies adopted
by previous studies, manual labeling – where an expert identiﬁes
events – is the most common [28].
As one example, we use publicly available event reports from the
British Telecom (BT Yahoo) ISP3 in the UK. This site identiﬁes the
times, locations and nature of network problems. During the month
of April, 2009 there were 68 reported problems, including Internet
and POTS events.
In addition, we use network problems reported from a large
North American ISP. For nondisclosure reasons, we cannot report
absolute numbers for these events.
Despite its many advantages, the set of labeled problems for
a network is restricted to events that can be detected by the
in-network monitoring infrastructure, using currently deployed
techniques, or generated by user complaints. Further, human
experts can introduce errors and disagreement, e.g., in reporting
the time and duration of an event. As a result, we can determine
when conﬁrmed events are detected by our approach, but cannot
draw strong conclusions about false positive and negative rates.
4.2 Case study
To assist with the presentation of NEWS, we pick one of the
events from the previous section. Speciﬁcally, we demonstrate how
NEWS detects a reported problem in BT Yahoo: On April 27, 2009
at 3:54 PM GMT, the network status page stated, “We are aware of
a network problem which may be affecting access to the internet in
certain areas...” The problem was marked as resolved at 8:50 PM.
Fig. 3 presents a scatter plot timeline of upload rates for peers
located in the same routable preﬁx in BT Yahoo (81.128.0.0/12)
during this event, which is depicted as a shaded region. Each
point in the graph represents an upload-rate sample for a single
peer; different point shapes/colors represent signals for different
peers. The ﬁgure shows that multiple peers experience reduced
performance between 10:54 and 16:54, while another set of peers
see a signiﬁcant drop in transfer rates at 14:54. These are consistent
with the reported event, when accounting for delays between the
actual duration of an event and the time assigned to it by a
technician. Further, we see that there were two distinguishable
network problems corresponding to the single generic report.
4.3 Network Monitoring from BitTorrent
We now discuss key design aspects of NEWS, a prototype edge-
system monitor for BitTorrent. Throughout this discussion we use
the conﬁrmed BT Yahoo event in Fig. 3 to explain our design
choices. With respect to the design challenges listed in Sec. 2, we
address scalability and granularity through our local event detection
and group corroboration approach; the remaining issues of privacy,
trust and adoption are covered in the subsequent sections. We
provide low-level implementation details in Sec. 6.
3http://help.btinternet.com/yahoo/help/servicestatus/
)
s
/
B
K
(
t
e
a
r
d
a
o
p
U
l
 60
 50
 40
 30
 20
 10
 0
Apr 27 08:54
10:54
12:54
14:54
Time
16:54
18:54
Apr 27 20:54
Figure 3: Upload rates for peers in a routable preﬁx owned by
British Telecom during a conﬁrmed disruption (shaded region).
Best viewed in color.
)
s
/
B
K
(
t
e
a
r
d
a
o
p
U
l
)
s
/
B
K
(