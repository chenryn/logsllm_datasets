tack. In particular, the request reading time is required to
detect the attack. Two audit functions have been inserted at
the entry and exit points of Apache’s request reading func-
tion ap read request. Thus, the time Apache spends
on reading a client request can be calculated as the time dif-
ference between these two function invocations. The imple-
mentation of the audit routines is given in Figure 6.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
static time_t start, end;
void read_entry(conn_rec *r)
{
start = time(NULL);
}
void read_exit(conn_rec *r)
{
int t;
char time_str[32];
end = time(NULL);
t = end - start;
strftime(time_str, 32, "%m/%d/%Y %H:%M:%S",
localtime(&(start)));
fprintf(log_fd,
"%s request at: %s, duration: %d sec\n",
r->remote_ip, time_str, t);
}
At Function request_rec*
ap_read_request(conn_rec *conn)
entry { read_entry(conn); }
exit { read_exit(conn); }
Figure 6: Audit routines for the Apache DoS attack.
A rule to detect this attack is: The number of simultane-
ous slow client connections from an IP address can not ex-
ceed a threshold. The rule has been implemented as a Perl
script.
4.2. OpenSSH
OpenSSH is an open-source implementation of the SSH
protocol. Different from Apache, OpenSSH uses the syslog
facility for logging. Certain versions of OpenSSH 3 are vul-
nerable to a user-to-root attack that exploits client-deﬁned
environment variables. More speciﬁcally, if the “UseLogin”
option is enabled, an attacker can pass environment vari-
ables, such as LD PRELOAD, to the server using the key au-
thentication method. Since the environment variable is set
before the “login” program is invoked, the malicious library
referenced by LD PRELOAD can execute arbitrary code as
root. It is worth pointing out that this attack is not new. Tel-
net daemons that supported environment passing have suf-
fered from similar problems before 4.
To detect this kind of attack, it is necessary to include in
the audit data the environment variable values set at the ser-
ver, which are not available in the syslog entries produced
by OpenSSH. Therefore, an audit routine that logs every
environment variable’s value at the server has been imple-
mented. The environment variables are logged at the entry
point of the child set env function, as shown in Fig-
ure 7.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
static int log_fd;
void _init()
{
log_fd = open("/var/applogd/openssh.log",
O_WRONLY | O_CREAT | O_APPEND);
}
void env_entry_call(char ***envp, u_int *envsizep,
const char *name, const char *value)
{
}
char msg[1024];
int i, size = 1024;
i = snprintf(msg,size,"NEW ENV: %s=",name);
size -=i;
if ( size > 0 && value != NULL)
i += snprintf(msg+i, size, "%s", value);
write(log_fd, msg, i);
At Function void child_set_env(char ***envp,
u_int *envsizep, const char *name, const char *value)
entry { env_entry_call(envp,envsizep,name,value); }
Figure 7: Audit routines for the OpenSSH User-To-Root at-
tack.
A detection rule has been implemented to enforce the
property: Environment variables such as “LD PRELOAD”
should not be set by the client.
3 CVE entry CAN-2001-0872, http://www.cve.mitre.org/
4 CERT
http://www.cert.org/
advisories/CA-1995-14.html
Advisory
CA-1995-14,
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
4.3. Discussion
Effectiveness of audit data. Intrusion detection is only as
good as the input data used for analysis. Meaningful and
complete auditing information is a prerequisite for effective
intrusion detection. This is made clear by comparing the ef-
fectiveness of different types of audit data with respect to
the attacks presented in the case studies of the previous sec-
tions. More precisely, in Table 1 we compare network traf-
ﬁc, OS audit data, application logs, and syslog data to the
application-level audit data produced by binary instrumen-
tation.
For the Apache signal attack it is possible to use OS au-
dit data to catch the attack, even though Linux does not have
an OS auditing facility by default. The DoS attack can be
detected by analyzing network packets, but the data is not
as straightforward to interpret as the application audit data
is. Notice that both the CGI attack and the SSH attack are
too elusive to be detected by analyzing existing audit data
streams, because they lack semantically-rich information.
In all the cases considered, the application-level audit data
produced by binary instrumentation contained all the infor-
mation required for detection.
√
Attack Network OS Appl. logs Appl. Audit
√
CGI
√
DoS
√
SIG
SSH
√
√
Table 1: Effectiveness of different audit data for intrusion
detection.
Selection of instrumentation points. A key step of the ap-
proach described in this paper is to identify the instrumen-
tation points in an application. Speciﬁcally, function names
and their parameter types are required. When the source
code of the application is available, the information about
the instrumentation points can be obtained by analyzing the
source code. The audit routines given in the case studies
were the results of such manual process. We are currently
looking into using compiler extensions to automate this pro-
cess [9].
If source code is not available, it is hard, but still possi-
ble, to instrument an application. For example, Miller et al.
demonstrated that by using reverse-engineering techniques,
it is feasible to identify the license checking functions in
a commercial product (Adobe’s Framemaker) and to dis-
able them by patching the binary at runtime [24]. The same
data ﬂow and control ﬂow analysis techniques can be ap-
plied here to determine the instrumentation points for the
application.
It is worth pointing out that the names and parameters of
the instrumented functions in the case studies did not cha-
nge much across different versions. Only one of the instru-
mented functions was changed in the Apache code, when
moving from version 1.3 to version 2.0 5. The consistency
of function signatures between major versions allows audit
routines to remain effective, even though the executable bi-
naries may change.
Detection of unknown attacks. In all the case studies, we
analyzed previously known attacks to derive attack signa-
tures and audit routines. However, the most important goal
of intrusion detection is to detect unknown, future attacks.
While being able to detect unknown attacks is a goal that
has yet to be achieved, we argue that it is possible to develop
generic signatures to detect some previously unseen varia-
tions of known attacks. For example, the CGI script han-
dling signature may detect other CGI source code disclo-
sure attacks that exploit different vulnerabilities. Also, the
OpenSSH attack suggests that it is possible to detect new at-
tacks from lessons learned by analyzing known attacks. The
audit routines developed in the previous sections only log
appropriate data into an audit ﬁle, which sufﬁces for our au-
diting needs. In practice, the code can perform different ac-
tions. For example, an auditor may choose to terminate the
OpenSSH server by calling exit() when LD PRELOAD
is set by a client.
5. Performance Evaluation
The binary rewriting approach has been evaluated quan-
titatively to determine the amount of overhead introduced
by the tool. The evaluation included three parts: the ker-
nel module overhead, the instrumentation overhead, and the
runtime overhead, which are discussed in the following sec-
tions.
5.1. Kernel Module Overhead
The kernel module used by the tool
introduces
some overhead at the kernel level because it wraps the
execve() system call. The module spends some execu-
tion time comparing the ﬁle to be executed with a list of
monitored ﬁles.
We developed a benchmark to evaluate the amount of
overhead introduced. The benchmark performs 100,000
execve() operations. For every invocation,
the ker-
nel compares the ﬁle executed by the execve() call
with a set of 20 ﬁles, which we believe is a reason-
able size for a set of trusted applications. The average ex-
ecution time and the standard deviation of 10 runs are
5
The function is instrumented in the CGI source code disclosure attack.
In Apache 1.3, the function name is ap log transaction, which
is changed to ap run log transaction in version 2.0.
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
shown in Table 2. The host used for the test is a Pen-
tium IV, with a 1.8 GHz CPU and 512MB of memory. The
overhead is fairly low, less than one percent. Consider-
ing that the execve() system call is invoked much less
frequently than other system calls (e.g., open()), the im-
pact on the whole system performance is minimal.
Time (s)
Std. Dev.
w/out LKM w/ LKM
69.5083
0.32
69.0454
0.17
Overhead
0.4629 (0.67%)
Table 2: Micro-benchmark of execve() performance.
5.2. Instrumentation Overhead
Time (s)
Per. (%)
Std. Dev.
CP
5.41
46.3
0.04
GI
0.28
2.4
0.04
PT
5.32
46.1
0.29
DT Other
0.31
0.23
2.0
2.7
0.17
0.43
Total
11.55
0.06
Table 3: Breakdown of instrumentation overhead (CP, GI,
PT, and DT represent CreateProcess, GetImage, Patch and
Detach, respectively).
The instrumentation overhead is the delay introduced by
performing the binary rewriting at application-startup time.
The Apache web server has been tested with all three audit
routines enabled for 10 runs. For each run, the time needed
for each step of the instrumentation process was measured.
The host used for the test was a PC with a 1.5 GHz Pentium
IV CPU and 256 MB of memory. The results are given in
Table 3.
Table 3 shows that the average slowdown is of 11.55 sec-
onds. Most of the instrumentation time is spent on the Cre-
ateProcess and Patch phases. The CreateProcess [3] func-
tion creates a BPatch thread object by parsing a process
image, including all linked shared libraries. The Patch phase
ﬁnds function instrumentation points and inserts code snip-
pets. Both phases spend a signiﬁcant amount of time going
through all the functions in the process image. GetImage
and Detach are internal Dyninst API calls. The former gets
an associated image object from BPatch thread and the
latter detaches from the instrumented process.
The overhead in this case is noticeable, and deﬁnitely
perceivable by the user. On the other hand, the overhead
is introduced at startup time only. This may be an accept-
able trade-off for the additional security provided by the
tool. One possible way to reduce the overhead is to com-
bine the CreateProcess phase and the Patch phase, so that
the search through all the functions must be performed only
once.
5.3. Runtime Overhead
To evaluate the runtime overhead of the auditing rou-
tines on the Apache web server, the server was tested
using the WebStone benchmark, version 2.5 (from
http://www.mindcraft.com/webstone/).
The experimental setup consisted of one client ma-
chine that generates HTTP requests (Pentium IV, 1.8 GHz,
512 MB RAM, Linux 2.4) and one server machine (Pen-
tium IV, 1.5 GHz, 256 MB RAM, Linux 2.4). Both
machines are connected with a 100 Mb Ethernet. Web-
Stone was conﬁgured with 10 to 100 clients in steps of
15 and each run took 5 minutes. The ﬁle set is the de-
fault static ﬁle set included in the benchmark
For each run, three cases were tested: Apache without
any auditing, Apache instrumented with the auditing rou-
tines described in this paper, and Apache running with the
Snare [32] OS-kernel auditing enabled. Snare includes a
kernel module intercepting 34 different system calls and a
user space daemon for writing audit data to a ﬁle. Client
connection rate, response time, and throughput were mea-
sured. The results are shown in Figure 8, 9, and 10. As
one can see from these ﬁgures, Snare performs poorly with
85 and 100 clients. The reason is that after about 25 min-
utes Snare used up all system resources, affecting the sys-
tem performance. Sometimes, the system can not return
to the normal state after heavy-load experiments. These
two points were removed when performance impact was