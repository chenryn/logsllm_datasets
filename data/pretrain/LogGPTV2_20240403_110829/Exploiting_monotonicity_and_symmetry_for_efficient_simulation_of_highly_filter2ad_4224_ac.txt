B. Reﬁnement via order-preserving heuristic
Suppose PΘ is the class of Bernoulli-distributed proposals
and P ≡ Pp ∈ PΘ, i.e., Xi ∼ Bernoulli(pi) under P for all
i ∈ [m]. For all x ∈ X and any i ∈ [m], let (x−i, x
(cid:2)
i) =
(cid:2)
(x1, x2, . . . , xi−1, x
i, xi+1, . . . , xm) be the sample obtained
(cid:2)
i. If f is
by modifying the i-th entry of x from xi
monotonically non-decreasing in xi for some i ∈ [m], then
under the order-preserving heuristic, we have θi ∈ [0, pi]. This
condition allows us to prove the following results.
Proposition IV.2 (Weight inequality). If f is monotonically
non-decreasing in xi for some i ∈ [m] then under the order-
preserving heuristic and for all x ∈ X such that xi = 0, let
(cid:2)
x
= (x−i, 1), not only f (x) ≤ f (x
(cid:2)
wθ(x) ≤ wθ(x
) but also
(cid:2)
to x
).
Proposition IV.3 (Sample substitution). Suppose x
For any Y ⊆ Xγ, by substituting x with x
(12)
(cid:2) ∈ Xγ.
(cid:2) in Y, one obtains
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
310
that
ξ1−→ x(1)
Proposition IV.3 directly follows the fact
a more representative subset of Xγ, i.e., the solution to (10)
under Xγ is closer to that under Y ∪ {x
(cid:2)}\{x} than to that
under Y ∪ {x}\{x
(cid:2)
(cid:2)}.
the con-
)) ≤ t necessarily implies the condition
dition log(wθ(x
log(wθ(x)) ≤ t. Propositions IV.2 and IV.3 can be gener-
alized by repeating the “bit ﬂipping” process to subsequent
indices as long as the monotone condition holds. Suppose
ξk−→ x(k) where
ξ2−→ . . .
this forms a chain x(0)
x(0) ≡ x and for j = 1, 2, . . . , k, x(j)
is obtained by
ﬂipping the ξj-th entry of x(j−1) from 0 to 1. We then have
f (x(0)) ≤ f (x(1)) ≤ . . . ≤ f (x(k)) due to monotonicity
and wθ(x(0)) ≤ wθ(x(1)) ≤ . . . ≤ wθ(x(k)) according to
Inequality (12). Suppose f (x) ≤ γ, letx
= max0≤j≤k x(j)
such that f (x(j)) ≤ γ—i.e., x
∗ is the rare event sample in
the chain that has the largest weight. For any Y ⊆ Xγ, by
∗ in Y, we obtain a more representative
substituting x with x
∗ is critical in the sense that one
subset of Xγ. The sample x
cannot ﬂip any more bit from 0 to 1 while remaining in Xγ.
∗ is similar to the concept of minimal
The construction of x
cuts in [2] or the essential sets in [4]. Moreover, the chain
{x(j)}k
∗ may depend on
the order in which the bits are ﬂipped. This is similar to the
fact that a cut of a graph may contain multiple minimal cuts.
j=0 is not unique and the resulting x
∗
V. SYMMETRIC OBJECTIVE FUNCTION
Symmetry deﬁnes an equivalence relationship on the vari-
ables of the objective function. Two variables xi and xj
are equivalent under f if the value of f remains the same
when the values of xi and xj are interchanged. Formally,
for all x ∈ X and i, j ∈ [m],
let (x−ij, xj, xi) =
(x1, . . . , xi−1, xj, xi+1, . . . , xj−1, xi, xj+1, . . . , xm) be the
sample obtained by swapping the values of the i-th and j-
th entry of x. Iff (x) =f (x−ij, xj, xi) for all x ∈ X then we
say xi and xj are equivalent under f. In addition, if Xi and
Xj are identically distributed under P, then we say Xi and
Xj are equivalent under f 2. Some examples of symmetry are
given below.
1) To eliminate single points of failure in a system, a common
design practice is to have redundant copies of a critical
component [1], [34]. Each copy contributes equally to the
overall system reliability. Furthermore, if the copies are
identical in the design and in the operation, then it is
reasonable to assume that they share the same failure rate.
to
the same subnetwork and have the same functionalities
usually share the same hardware and software components
and security conﬁgurations. Therefore, they are usually
vulnerable to the same type of cyber-attacks.
2) In network security analysis [4], hosts that connect
A. Structure-preserving heuristic
A symmetric function f allows us to reduce the dimension
of θ and the cardinality of Y without compromising its
representativeness. The former is achieved by partitioning the
variables in X into equivalent groups (EGs), where variables
from the same group are equivalent, therefore their parameters
in Pθ must share the same values. The latter is achieved by
exploiting the fact that different rare event samples may share
2That means, if xi and xj are equivalent under f (or simply equivalent
when the context is clear) but Xi and Xj are not identically distributed, then
by deﬁnition, Xi and Xj are not equivalent.
the same underlying inequality constraints. These results are
due to the following proposition.
Proposition V.1 (Equivalence relation under P∗). For some
i, j ∈ [m], if Xi and Xj are equivalent under f, then they are
identically distributed under P∗.
In contrast, if the necessary conditions in Proposition V.1
then a proposal distribution Pθ ∈ PΘ that well
holds,
approximates P∗ should also preserve the equivalence of Xi
and Xj—i.e., Xi and Xj are identically distributed under
Pθ. When Pθ satisﬁes this condition, we say it follows the
structure-preserving heuristic. Moreover, due to transitivity the
concept of equivalence generalizes to more than two variables,
i.e., if Xi and Xj are equivalent and Xj and Xk are equivalent
then Xi and Xk are also equivalent.
B. Reﬁnement via structure-preserving heuristic
∗
1, θ
Suppose X can be partitioned into EGs where variables
that belong to same group are equivalent under f. Formally,
let π = {π1, π2. . . . , πr} be a partition of [m] where (i) πi (cid:11)= ∅
and πi ∩ πj = ∅ for all 1 ≤ i (cid:11)= j ≤ r and (ii) ∪r
i=1πi = [m].
We call π an EG partition of f if for all k ∈ [r] = {1, 2, . . . , r}
and i, j ∈ πk, Xi and Xj are equivalent under f. According
to Proposition V.1, under the structure-preserving heuristic,
variables whose indices are in the same EG are identically
distributed under Pθ. Therefore, their parameters in Pθ must
share the same value, i.e., for all k ∈ [r] and i, j ∈ πk, we
∗
def.
have θi = θj
k. As a result, the m-dimensional vector
= θ
θ in (10) can be reduced to an r-dimensional vector θ
=
r ) where r ≤ m. Note that {{1},{2}, . . . ,{m}}
∗
(θ
is by deﬁnition an EG partition. Therefore, the deﬁnition of
EG partition can be strengthened by further requiring variables
that belong to different EGs must not be equivalent.
∗
2, . . . , θ
The structure-preserving heuristic further allows us to re-
duce the cardinality of Y. Consider the case of Bernoulli-
distributed proposals. Let π be an EG partition of f and
for every x ∈ Xγ, let ω(x) = (ω1(x), ω2(x), . . . , ωr(x))
i∈πk xi for all
be an r-valued vector where ωk(x) =
k ∈ [r]. In other words, ωk(x) counts the number of entries
in x whose indices are in πk and whose values are one.
Under the structure-preserving heuristic, for all k ∈ [r] and
i, j ∈ πk, we have P(Xi=1)
def.
= uk and
Pθ(Xi=1) =
def.
P(Xi=0)
= vk. By rewriting the weight of x as
Pθ(Xi=0) =
P(Xj =0)
Pθ(Xj =0)
P(Xj =1)
Pθ(Xj =1)
(cid:3)
∗
m(cid:2)
r(cid:2)
k=1
wθ(x) =
i=1
P(Xi = xi)
Pθ(Xi = xi)
=
ωk(x)
u
k
v
|πk|−ωk(x)
k
,
we can see that ω(x) uniquely determines the value of wθ(x),
in the sense that for all x, y ∈ Xγ, ifω (x) = ω(y) then
wθ(x) =w θ(y). When this happens, we only need to keep
either x or y in Y because the condition log(wθ(x)) ≤ t and
the condition log(wθ(y)) ≤ t are equivalent. Moreover, this
holds true under any choice of θ.
VI. IMPLEMENTATION
Algorithm 1 outlines the pseudocode for computing an ap-
proximation to the rare event set using the heuristics developed
in Sections IV and V. At line 5, we start the while loop by
collecting a random rare event sample. For example, in our
current implementation, we sample x from X uniformly at
random and check whether f (x) ≥ γ. To speed up the hit
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
311
tstart ← get current time
Y ← {} and Ω ← {}
while 1 do
Algorithm 1 Given a time budget Ta, a sample budget Ma, and
optionally an EG partitioning π, generate an approximation Y of Xγ
using the order-preserving and structure-preserving heuristic.
1: function APPXRAREEVENTSET(Ta, Ma, optional: π)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
tend ← get current time
if tend − tstart > Ta or |Y| > Ma then
else if ω(x) /∈ Ω then
x ← get random sample in Xγ
ξ ← get random permutation of [m]
for i = ξ1, ξ2, . . . , ξm do
if f is non-decreasing in xi then
return Y
Y ← Y ∪ {x} and Ω ← Ω ∪ {ω(x)}
(cid:11) ω deﬁned in Sec. V-B
i = argmaxu f (x−i, u) ≤ γ
∗
Solve x
x ← (x−i, x
∗
i )
∗
i satisﬁes f (x−i, x
rate, we can sample x from any proposal distribution that is
tilted toward the left tail of P. Lines 7 to 10 implement the
analog version of the bit ﬂipping logic of the order-preserving
heuristic (Sec. IV-B). Owing to monotonicity, the optimization
problem at line 9 can be solved efﬁciently using binary search.
In practice, we only need to ﬁnd samples that are sufﬁciently
close to the boundary of Xγ. For example, if xi is a continuous
∗
i + ) for
random, then x
a sufﬁciently small . Lines 14 to 15 implement the collision
check logic of the structure-preserving heuristic, where we
only keep rare event samples x ∈ Y that have unique ω(x)
(recall from Sec. V-B that ω(x) also depends on the EG
partitioning π). The collision check subroutine also works for
continuous random variables if the random samples obtained
at line 5 are discretized. Finally, the algorithm terminates and
returns the approximation Y of Xγ (line 13) when either the
time budget Ta or sample budget Ma is reached.
i ) ≤ γ ≤ f (x−i, x
∗
VII. SIMULATIONS
This section provides extensive simulation results to high-
light the performance of the MWM method. For evaluation,
we use two performance metrics, (i) the empirical relative
error (RE) of an estimator and (ii) the total experiment time
ta + to + ts, where ta is the time to compute an approximation
Y of Xγ using Algorithm 1, to is the time to solve the opti-
mization problem in (10), and ts is the time to run the Monte
Carlo simulation and compute an MWM-based IS estimator
(or simply MWM estimator). To obtain Y, we run Algorithm
1 with a maximum budget of Ta = 500 seconds and Ma = 500
samples. However, we may terminate the algorithm manually
when the sample yield rate becomes too low. The timing data
is recorded on a standard desktop computer running Linux
4.15.0-120 kernel with an Intel Core i5-4690K CPU and 16GB
of RAM. The entire simulation pipeline was implemented in
Python3 with minimum optimization and is available online
at [36] for reproducibility. To compute the EG partitioning
of network reliability-related models, we used nauty [37], a
program for computing automorphism groups of graphs.
Table I summarizes high-level information about the simu-
lation results. Overall, the computation time ta + to dominates
the experiment time, while the simulation time ts consumes
less than two seconds for all settings where M = 104.
(b) 4×4 lattice [26], [22].
(a) Dodecahedron [35].
Fig. 1: Networks studied in Sections VII-A and VII-B.
Fig. 2: Campus network [38] , [4] studied in Sec. VII-C.
the simulation
Moreover, as we increase the sample size,
time increases linearly, but
the computation time remains
unchanged. For each experiment, we use a population of
103 MWM estimators to compute the population mean and
the empirical RE (that means for a M = 104 setting, we
need to collect a total of 104×103 = 107 random samples).
To establish the baselines, we treat the population mean as
a quasi ground-truth and use it to compute the RE of the
crude MC method according to (5). For a fair comparison,
the crude MC estimator uses the same experiment time as the
MWM estimator. For example, in the Campus (iii) experiment
where MWM collects 104 random samples in 495.0 + 15.2 +
0.85 = 511.05 seconds, the crude MC estimator collects over
6×106 random samples. Whenever possible, we also include
simulation results from existing rare event simulation methods,
some of which are based on IS, and some are based on the
splitting technique. Under the same experiment setting, we
try to match the sample size while acknowledging that the
experiment times can be different, since they also depend on
the underlying hardware and software conﬁguration.
A. Network reliability analysis
The dodecahedron (Fig. 1a) is a medium-sized undirected
graph with 20 nodes and 30 links. It has been widely adopted
as a benchmark for evaluating the performance of rare event
simulation techniques [2], [35], [39], [32]. In this example, we
use MWM to compute the s, t-unreliability of this network
where s and t correspond to vertex 1 and 20 in Fig. 1a
and all links are subjected to the same failure probability
. For evaluation, we use sample size M = 104 and three
−3,
link failure probability setting (i)  = 10
−2, (ii)  = 10
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:29 UTC from IEEE Xplore.  Restrictions apply. 
312