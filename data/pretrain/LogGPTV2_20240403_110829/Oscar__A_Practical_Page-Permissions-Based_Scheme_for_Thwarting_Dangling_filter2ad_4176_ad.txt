CETS’ bzip2 is from the CPU2000 suite [29] and we
suspect their mcf is as well.8 SPEC speciﬁcally cau-
tions that, due to differences in the benchmark workload
and/or source, the results on CPU2000 vs. CPU2006
might not be comparable [5].
Figure 14 (left) shows the overhead of CETS vs. our
overall best scheme. We are faster than CETS for all
benchmarks, often by a signiﬁcant margin. For example,
CETS has >48% overhead on gobmk and hmmer, com-
pared to less than 1% for Oscar. The geometric mean
across CETS’ subset of CPU2006 benchmarks is 2.8%
for Oscar compared to 36% for CETS.
7September
19,
2014,
https://github.
com/santoshn/softboundcets-34/commit/
9a9c09f04e16f2d1ef3a906fd138a7b89df44996
8In any case, since CETS has 23% and 114% overhead on bzip2
and mcf respectively – compared to less than 1.5% on each for Oscar –
including them in the comparison would not be favorable to CETS.
824    26th USENIX Security Symposium
USENIX Association
159% 1700% 373% ÷ 0%20%40%60%80%100%Memory overhead (vanilla = 0%) OscarDangSan (re-run)DangSan (reported)DangNull (reported)? ? ? ? ? 13,365% 0%100%200%300%400%500%600%700%800%900%1000%Memory overhead (vanilla = 0%) OscarDangSan (re-run)DangSan (reported)DangNull (reported)4.3 Memory Overhead Comparison
5 Extending Oscar for Server Applications
Figures 15 and 16 show the memory overhead of Oscar,
DangSan (re-run and reported), and DangNull (reported
only). We did not ﬁnd any reported data for FreeSentry,
CETS or SoftBoundCETS temporal-only. The graphs
have different y-axes to highlight differences in over-
heads in the lower-overhead benchmarks of Figure 15.
We calculated the memory overhead based on the
combined maximum resident set size (RSS)9, size
of the page tables10, and approximate size of the
vm area structs11. Our polling approach introduces
some minor inaccuracies with regard to obtaining the
maxima and baseline values. For DangSan, which does
not greatly increase the number of page table entries or
vm area structs, this is very similar to their maximum
resident set size metric. It is unclear what memory con-
sumption metric DangNull used, so some care should be
taken when interpreting their overheads.
The RSS values reported in /proc/pid/status are
misleading for Oscar because it double-counts every
shadow page, even though many of them are aliased to
the same canonical. We know, however, that the physical
memory usage of Oscar – and therefore the resident set
size when avoiding double-counting – is essentially the
same as the MAP SHARED with padding scheme (from
Section 3.1). We therefore calculated the maximum RSS
for that scheme, but measured the size of the page tables
and vm area structs for the full version of Oscar.
For the complete suite of CPU2006 benchmarks, Os-
car has 61.5% memory overhead, far lower than Dan-
gSan’s 140%. Even if we omit DangSan’s pathological
case of omnetpp (reported overhead of over 13,000%),
Oscar is still far more memory-efﬁcient with 52% over-
head vs. 90% for DangSan. The only benchmarks on
which Oscar performs substantially worse than DangSan
are sphinx3 and soplex. sphinx3 with Oscar has a
maximum RSS of ≈50MB (compared to a baseline of
≈45MB), maximum page tables size of ≈130MB, and
maximum vm area structs of ≈45MB. In Section 8,
we propose methods to reduce the memory overhead by
garbage collecting old page table entries (which would
beneﬁt sphinx3), and sharing inline metadata (which
beneﬁts would soplex with its many small allocations).
DangNull has roughly 127% memory overhead, but,
as also noted by the DangSan authors, DangNull did
not report data for many of the memory-intensive bench-
marks. If we use the same subset of SPEC benchmarks
that DangNull reported, then Oscar has only 36% mem-
ory overhead (vs. ≈75% for DangSan).
9VmHWM (peak RSS) in /proc/pid/status
10VmPTE and VmPMD in /proc/pid/status
11We counted the number of mappings in /proc/pid/maps and
multiplied by sizeof(vm area struct).
When applying Oscar to server applications – which
are generally more complex than the SPEC CPU bench-
marks – we encountered two major issues that resulted in
incompatibility and incomplete protection: forking and
custom memory allocators. Additionally, we modiﬁed
Oscar to be thread-safe when allocating shadows.
5.1 Supporting shadows + fork()
Using MAP SHARED for all allocations is problematic for
programs that fork, as it changes the semantics of mem-
ory: the parent and child’s memory will be shared, so
any post-fork writes to pre-fork heap objects will un-
expectedly be visible to both the parent and child.
In
fact, we discovered that most programs that fork and
use glibc’s malloc will crash when using MAP SHARED.
Surprisingly, they may crash even if neither the parent
nor child read or write to the objects post-fork.12
Oscar solves this problem by wrapping fork and em-
ulating the memory semantics the program is expecting.
After fork, in the child, we make a copy of all heap
objects, unmap their virtual addresses from the shared
physical page frames, remap the same virtual addresses
to new (private) physical page frames, and repopulate the
new physical page frames with our copy of the heap ob-
jects. The net effect is that the shadow and canonical
virtual addresses have not changed – which means old
pointers (in the application, and in the allocator’s free
lists) still work – but the underlying physical page frames
in the child are now separated from the parent.
Method. Oscar instruments malloc and free to keep
a record of all live objects in the heap and their shadow
addresses. Note that with a loadable kernel module, Os-
car could avoid recording the shadow addresses of live
objects and instead ﬁnd them from the page table entries
or vm area structs.
Then, Oscar wraps fork to do the following:
1. call the vanilla fork(). After this, the child address
space is correct, except that the malloc’d memory
regions are aliased with the parent’s physical page
frames.
2. in the child process:
(a) for each canonical page in the heap:
12glibc’s malloc stores the main heap state in a static variable
(not shared between parent and child), but also partly through inline
metadata of heap objects (shared); thus, when the parent or child al-
locates memory post-fork, the heap state can become inconsistent or
corrupted. A program that simply malloc()s 64 bytes of memory,
fork()s, and then allocates another 64 bytes of memory in the child,
is sufﬁcient to cause an assertion failure.
USENIX Association
26th USENIX Security Symposium    825
i. allocate a new page at any unused
address t using mmap(MAP SHARED |
MAP ANONYMOUS)
ii. copy canonical page to t
iii. call mremap(old address=t,
new address=canonical page). Note
that mremap automatically removes the
previous mapping at canonical page.
(b) for each live object: use mremap to recreate a
shadow at the same virtual address as before
(using the child’s new physical page frames).
Compared to the na¨ıve algorithm, the use of mremap
halves the number of memory copy operations.
We can further reduce the number of system calls by
observing that the temporary pages t can be placed at vir-
tual addresses of our choice. In particular, we can place
all the temporary pages in one contiguous block, which
lets us allocate them all using just one mmap command.
The parent process must sleep until the child has
copied the canonical pages, but it does not need to wait
while the child patches up the child’s shadows. Oscar
blocks signals for the duration of the fork() wrapper.
This algorithm sufﬁces for programs that have only
one thread running when the program forks. This covers
most reasonable use cases; it is considered poor practice
to have multiple threads running at the time of fork [6].
For example, apache’s event multi-processing module
forks multiple children, which each then create multiple
threads. To cover the remaining, less common case of
programs that arbitrarily mix threads and fork, Oscar
could “stop the world” as in garbage collection, or Leak-
Sanitizer (a memory leak detector) [1].
Our algorithm could readily be modiﬁed to be “copy-
on-write” for efﬁciency. Additionally, batching the
remappings of each page might improve performance;
since the intended mappings are known in advance, we
could avoid the misprediction issue that plagued regular
batch mapping. With kernel support we could solve this
problem more efﬁciently, but our focus is on solutions
that can be deployed on existing platforms.
Results. We implemented the basic algorithm in Os-
car. In cursory testing, apache, nginx, and openssh run
with Oscar’s fork ﬁx, but fail without. These applica-
tions allocate only a small number of objects pre-fork,
so Oscar’s fork wrapper does not add much overhead
(tens or hundreds of milliseconds).
5.2 Custom Memory Allocators
The overheads reported for SPEC CPU are based on in-
strumenting the standard malloc/free only, providing a
level of protection similar to prior work. However, a few
Figure 17: Throughput of Oscar on memcached.
of the SPEC benchmarks [19] implement their own cus-
tom memory allocator (CMAs). Since standard schemes
for temporal memory safety require instrumenting mem-
ory allocation and deallocation functions, without special
provisions none of them – including Oscar – will protect
objects allocated via arbitrary CMAs.
We found that CMAs seem to be even more com-
mon in server programs, such as apache, nginx, and
proftpd. Prior work typically ignores the issue of
CMAs. We solve this by manually identifying CMAs
and wrapping them with Oscar as well. CMA identiﬁca-
tion could also be done automatically [18].
If we do not wrap a CMA with Oscar, any objects al-
located with the CMA would obviously not be resistant
to use-after-free. However, there are no other ill effects;
it would not result in any false positives for any objects,
nor would it result in false negatives for the non-CMA
objects.
5.3 Case Study: malloc-like custom mem-
ory allocator in memcached
memcached is a memory object caching system that
exports a get/set
interface to a key-value store.
We compiled memcached 1.4.25 (and its prerequi-
site, libevent) and benchmarked performance using
memaslap.
When we wrapped only glibc’s malloc, the over-
head was negligible: throughput was reduced by 0–3%,
depending on the percentage of set operations (Figure
17). However, this is misleadingly low, as it fails to pro-
vide temporal memory safety for objects allocated by the
CMA. Therefore, we applied Oscar to wrap the CMA, in
the same way we wrapped glibc’s malloc/free.
Method. To support wrapping the CMA, we had to
ensure that Oscar malloc always used MAP SHARED even
for large objects. This is because the allocation may be
used by the CMA to “host” a number of shadows. Ad-
ditionally, we partitioned the address space to use sepa-
rate high-water marks for the malloc wrapper and CMA
wrapper.
We identiﬁed that allocations and deallocations via
826    26th USENIX Security Symposium
USENIX Association
99.81% 99.10% 98.28% 98.07% 96.62% 95.80% 95.30% 93.07% 79.31% 65.76% 55.44% 48.10% 45.97% 45.63% 0%10%20%30%40%50%60%70%80%90%100%0%0.01%1%2%3%4%5%10%30%50%70%90%99%100%Throughput (vanilla = 100%) Percentage of "Set" operations Protect malloc onlyProtect malloc + CMAmemcached’s slab allocator are all made through the
do item alloc and item free functions. Thus, it is
sufﬁcient to add shadow creation/deletion to those func-
tions.
For ease of engineering, we made minor changes di-
rectly to the slab allocator, similar to those we applied
to glibc’s malloc: inserting a canonical address ﬁeld
in the memcached item struct, and modifying the alloca-
tion/deallocation functions. In principle, we only need to
override the CMA allocate/deallocate symbols, without
needing to recompile the main application.
In this paper, the per-object metadata (e.g., the canon-
ical address) is stored inline. If Oscar switched to a dis-
joint metadata store (e.g., a hashtable), it would be easy
to extend Oscar to protect any custom memory alloca-
tors (not just CMAs with malloc-like interfaces) that
are identiﬁed: as with glibc’s malloc, the allocator
function simply needs to be wrapped to return a new
shadow, and the deallocator function wrapped to destroy
the shadow. This would be a better long-term approach
than individually dealing with each CMA that is encoun-
tered.
Results. When set operations are 3% of the total op-
erations (a typical workload [12]), the performance over-
head is roughly 4%. The overhead is higher for set oper-
ations because these require allocations (via the CMA),
which involves creating shadows. Get operations have
almost no overhead because they do not perform mem-
ory allocation or deallocation and consequently do not
require any system calls.13 Unlike SPEC CPU, which
is single-threaded, we ran memcached with 12 threads.
This shows that Oscar’s overhead is low even for multi-
threaded applications, despite our na¨ıve use of a mutex
to synchronize part of Oscar’s internal state (namely, the
high-water mark; see Section 8).
5.4 Special case: Region-based allocators
We have found several server programs that use region-
based custom memory allocators [14]. Region-based al-
locators are particularly favorable for page-permissions-
based schemes such as Oscar.
Typically, region-based allocators obtain a large block
of memory from malloc, which they carve off into ob-
jects for their allocations. The distinguishing feature is
that only the entire region can be freed, but not individ-
ual objects.
Region-based allocators by themselves are not resis-
tant to use-after-free, since the blocks from malloc may
be reused, but they provide temporal memory safety
when the underlying malloc/free is protected by a
13Technicality: memcached lazily expires entries, checking the
timestamp only during the get operation. Thus, the overhead of de-
stroying shadows may be attributed to get operations.
lock-and-key scheme. Thus, there is no need to explicitly
identify region-based CMAs; merely wrapping glibc’s
malloc/free with Oscar sufﬁces to provide temporal
memory safety for such programs i.e., Oscar would pro-
vide full use-after-free protection for a region-based al-
locator, without the need for any custom modiﬁcations.