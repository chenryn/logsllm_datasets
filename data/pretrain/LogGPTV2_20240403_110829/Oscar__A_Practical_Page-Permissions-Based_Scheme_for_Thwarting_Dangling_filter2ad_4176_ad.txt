### CETS’ bzip2 and mcf Benchmarks

CETS' bzip2 is sourced from the CPU2000 suite [29], and it is likely that their mcf benchmark also originates from the same suite. SPEC explicitly cautions that, due to differences in the benchmark workload and/or source code, results from CPU2000 may not be directly comparable with those from CPU2006 [5].

Figure 14 (left) illustrates the overhead of CETS compared to our overall best scheme. Our approach consistently outperforms CETS across all benchmarks, often by a significant margin. For instance, CETS incurs an overhead of over 48% on gobmk and hmmer, whereas Oscar's overhead is less than 1%. The geometric mean for CETS' subset of CPU2006 benchmarks is 36%, compared to 2.8% for Oscar.

**Note:** As of September 19, 2014, CETS has 23% and 114% overhead on bzip2 and mcf, respectively, while Oscar's overhead is less than 1.5% for both. Including these benchmarks in the comparison would not favor CETS. [Reference: https://github.com/santoshn/softboundcets-34/commit/9a9c09f04e16f2d1ef3a906fd138a7b89df44996]

---

### Memory Overhead Comparison

Figures 15 and 16 present the memory overhead of Oscar, DangSan (both re-run and reported), and DangNull (reported only). We did not find any reported data for FreeSentry, CETS, or SoftBoundCETS temporal-only. The graphs use different y-axes to highlight the differences in overheads for the lower-overhead benchmarks in Figure 15.

Memory overhead was calculated based on the combined maximum resident set size (RSS), size of the page tables, and approximate size of the vm area structs. Our polling approach introduces minor inaccuracies in obtaining the maxima and baseline values. For DangSan, which does not significantly increase the number of page table entries or vm area structs, this metric is very similar to their maximum resident set size. It is unclear what memory consumption metric DangNull used, so some caution should be exercised when interpreting their overheads.

The RSS values reported in /proc/pid/status are misleading for Oscar because they double-count every shadow page, even though many are aliased to the same canonical. However, we know that the physical memory usage of Oscar, and thus the resident set size when avoiding double-counting, is essentially the same as the MAP SHARED with padding scheme (from Section 3.1). Therefore, we calculated the maximum RSS for that scheme but measured the size of the page tables and vm area structs for the full version of Oscar.

For the complete suite of CPU2006 benchmarks, Oscar has a 61.5% memory overhead, which is far lower than DangSan's 140%. Even if we exclude DangSan’s pathological case of omnetpp (with a reported overhead of over 13,000%), Oscar remains more memory-efficient with a 52% overhead compared to DangSan's 90%. The only benchmarks where Oscar performs substantially worse than DangSan are sphinx3 and soplex. For sphinx3, Oscar has a maximum RSS of approximately 50MB (baseline ≈45MB), maximum page tables size of ≈130MB, and maximum vm area structs of ≈45MB. In Section 8, we propose methods to reduce the memory overhead by garbage collecting old page table entries (which would benefit sphinx3) and sharing inline metadata (which would benefit soplex with its many small allocations).

DangNull has roughly 127% memory overhead, but, as noted by the DangSan authors, DangNull did not report data for many of the memory-intensive benchmarks. If we use the same subset of SPEC benchmarks that DangNull reported, then Oscar has only 36% memory overhead (compared to ≈75% for DangSan).

- **VmHWM (peak RSS) in /proc/pid/status**
- **VmPTE and VmPMD in /proc/pid/status**
- **We counted the number of mappings in /proc/pid/maps and multiplied by sizeof(vm area struct).**

---

### Extending Oscar for Server Applications

When applying Oscar to server applications, which are generally more complex than the SPEC CPU benchmarks, we encountered two major issues leading to incompatibility and incomplete protection: forking and custom memory allocators. Additionally, we modified Oscar to be thread-safe when allocating shadows.

#### 5.1 Supporting Shadows + fork()

Using MAP SHARED for all allocations is problematic for programs that fork, as it changes the semantics of memory: the parent and child's memory will be shared, so any post-fork writes to pre-fork heap objects will be unexpectedly visible to both. In fact, most programs that fork and use glibc’s malloc will crash when using MAP SHARED, even if neither the parent nor child read or write to the objects post-fork.

Oscar addresses this issue by wrapping fork and emulating the expected memory semantics. After fork, in the child, we make a copy of all heap objects, unmap their virtual addresses from the shared physical page frames, remap the same virtual addresses to new (private) physical page frames, and repopulate the new physical page frames with our copy of the heap objects. This ensures that the shadow and canonical virtual addresses remain unchanged, meaning old pointers still work, but the underlying physical page frames in the child are now separated from the parent.

**Method:**
- Oscar instruments malloc and free to keep a record of all live objects in the heap and their shadow addresses.
- Oscar wraps fork to perform the following:
  1. Call the vanilla fork(). The child address space is correct, except that the malloc'd memory regions are aliased with the parent’s physical page frames.
  2. In the child process:
     - For each canonical page in the heap:
       - Allocate a new page at any unused address t using mmap(MAP SHARED | MAP ANONYMOUS)
       - Copy the canonical page to t
       - Call mremap(old address=t, new address=canonical page). Note that mremap automatically removes the previous mapping at the canonical page.
     - For each live object: use mremap to recreate a shadow at the same virtual address as before (using the child’s new physical page frames).

Compared to the naive algorithm, the use of mremap halves the number of memory copy operations. We can further reduce the number of system calls by placing all temporary pages in one contiguous block, allowing us to allocate them all using just one mmap command. The parent process must sleep until the child has copied the canonical pages but does not need to wait while the child patches up the child’s shadows. Oscar blocks signals for the duration of the fork() wrapper.

This algorithm suffices for programs with only one thread running when the program forks, which covers most reasonable use cases. For programs that mix threads and fork, Oscar could “stop the world” as in garbage collection or LeakSanitizer (a memory leak detector) [1]. Our algorithm could be modified to be “copy-on-write” for efficiency, and batching the remappings of each page might improve performance. With kernel support, this problem could be solved more efficiently, but our focus is on solutions deployable on existing platforms.

**Results:**
- We implemented the basic algorithm in Oscar. In cursory testing, apache, nginx, and openssh run with Oscar’s fork fix but fail without. These applications allocate only a small number of objects pre-fork, so Oscar’s fork wrapper does not add much overhead (tens or hundreds of milliseconds).

#### 5.2 Custom Memory Allocators

The overheads reported for SPEC CPU are based on instrumenting the standard malloc/free, providing a level of protection similar to prior work. However, a few SPEC benchmarks [19] implement their own custom memory allocators (CMAs). Standard schemes for temporal memory safety require instrumenting memory allocation and deallocation functions, so without special provisions, none of them, including Oscar, will protect objects allocated via arbitrary CMAs.

We found that CMAs are even more common in server programs like apache, nginx, and proftpd. Prior work typically ignores the issue of CMAs. We solve this by manually identifying CMAs and wrapping them with Oscar. CMA identification could also be done automatically [18].

If we do not wrap a CMA with Oscar, any objects allocated with the CMA would not be resistant to use-after-free. However, there are no other ill effects; it would not result in false positives for any objects, nor would it result in false negatives for non-CMA objects.

#### 5.3 Case Study: Malloc-like Custom Memory Allocator in memcached

memcached is a memory object caching system that exports a get/set interface to a key-value store. We compiled memcached 1.4.25 (and its prerequisite, libevent) and benchmarked performance using memaslap.

When we wrapped only glibc’s malloc, the overhead was negligible: throughput was reduced by 0–3%, depending on the percentage of set operations (Figure 17). However, this is misleadingly low, as it fails to provide temporal memory safety for objects allocated by the CMA. Therefore, we applied Oscar to wrap the CMA, similar to how we wrapped glibc’s malloc/free.

**Method:**
- To support wrapping the CMA, we ensured that Oscar malloc always used MAP SHARED, even for large objects, as the allocation may be used by the CMA to “host” a number of shadows.
- We partitioned the address space to use separate high-water marks for the malloc wrapper and CMA wrapper.
- We identified that allocations and deallocations via memcached’s slab allocator are made through the do_item_alloc and item_free functions. Thus, it is sufficient to add shadow creation/deletion to those functions.

For ease of engineering, we made minor changes directly to the slab allocator, similar to those applied to glibc’s malloc: inserting a canonical address field in the memcached item struct and modifying the allocation/deallocation functions. In principle, we only need to override the CMA allocate/deallocate symbols without needing to recompile the main application.

In this paper, the per-object metadata (e.g., the canonical address) is stored inline. If Oscar switched to a disjoint metadata store (e.g., a hashtable), it would be easy to extend Oscar to protect any custom memory allocators (not just CMAs with malloc-like interfaces) that are identified: the allocator function simply needs to be wrapped to return a new shadow, and the deallocator function wrapped to destroy the shadow. This would be a better long-term approach than individually dealing with each CMA encountered.

**Results:**
- When set operations are 3% of the total operations (a typical workload [12]), the performance overhead is roughly 4%. The overhead is higher for set operations because they require allocations (via the CMA), which involves creating shadows. Get operations have almost no overhead because they do not perform memory allocation or deallocation and consequently do not require any system calls. Unlike SPEC CPU, which is single-threaded, we ran memcached with 12 threads, showing that Oscar’s overhead is low even for multi-threaded applications, despite our naive use of a mutex to synchronize part of Oscar’s internal state (namely, the high-water mark; see Section 8).

#### 5.4 Special Case: Region-based Allocators

We have found several server programs that use region-based custom memory allocators [14]. Region-based allocators are particularly favorable for page-permissions-based schemes such as Oscar.

Typically, region-based allocators obtain a large block of memory from malloc, which they carve into objects for their allocations. The distinguishing feature is that only the entire region can be freed, not individual objects.

Region-based allocators by themselves are not resistant to use-after-free, since the blocks from malloc may be reused, but they provide temporal memory safety when the underlying malloc/free is protected by a lock-and-key scheme. Thus, there is no need to explicitly identify region-based CMAs; merely wrapping glibc’s malloc/free with Oscar suffices to provide temporal memory safety for such programs, i.e., Oscar would provide full use-after-free protection for a region-based allocator without the need for any custom modifications.