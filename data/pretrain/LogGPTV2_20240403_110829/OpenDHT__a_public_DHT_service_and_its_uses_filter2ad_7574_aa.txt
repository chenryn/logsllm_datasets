title:OpenDHT: a public DHT service and its uses
author:Sean C. Rhea and
Brighten Godfrey and
Brad Karp and
John Kubiatowicz and
Sylvia Ratnasamy and
Scott Shenker and
Ion Stoica and
Harlan Yu
OpenDHT: A Public DHT Service and Its Uses
Sean Rhea, Brighten Godfrey, Brad Karp, John Kubiatowicz,
Sylvia Ratnasamy, Scott Shenker, Ion Stoica, and Harlan Yu
UC Berkeley and Intel Research
PI:EMAIL
ABSTRACT
Large-scale distributed systems are hard to deploy, and distributed
hash tables (DHTs) are no exception. To lower the barriers fac-
ing DHT-based applications, we have created a public DHT service
called OpenDHT. Designing a DHT that can be widely shared, both
among mutually untrusting clients and among a variety of applica-
tions, poses two distinct challenges. First, there must be adequate
control over storage allocation so that greedy or malicious clients
do not use more than their fair share. Second, the interface to the
DHT should make it easy to write simple clients, yet be sufﬁciently
general to meet a broad spectrum of application requirements. In
this paper we describe our solutions to these design challenges. We
also report our early deployment experience with OpenDHT and
describe the variety of applications already using the system.
Categories and Subject Descriptors
C.2 [Computer Communication Networks]: Distributed Systems
General Terms
Algorithms, Design, Experimentation, Performance, Reliability
Keywords
Peer-to-peer, distributed hash table, resource allocation
1. MOTIVATION
Large-scale distributed systems are notoriously difﬁcult to de-
sign, implement, and debug. Consequently, there is a long history
of research that aims to ease the construction of such systems by
providing simple primitives on which more sophisticated function-
ality can be built. One such primitive is provided by distributed
hash tables, or DHTs, which support a traditional hash table’s sim-
ple put/get interface, but offer increased capacity and availability
by partitioning the key space across a set of cooperating peers and
replicating stored data.
While the DHT ﬁeld is far from mature, we have learned a tremen-
dous amount about how to design and build them over the past few
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’05, August  22–26,  2005,  Philadelphia,  Pennsylvania,  USA.
Copyright 2005 ACM 1-59593-009-4/05/0008 ...$5.00.
Client
OpenDHT
Client
Application
RPC
Client
Client
Puts/Gets
Figure 1: OpenDHT Architecture.
years, and several well-debugged DHT implementations [1–3] are
now readily available. Furthermore, several fully deployed DHT
applications are now in daily use [15,20,26], and dozens more have
been proposed and/or prototyped.
Maintaining a running DHT requires non-trivial operational ef-
fort. As DHT-based applications proliferate, a natural question to
ask is whether every such application needs its own DHT deploy-
ment, or whether a shared deployment could amortize this opera-
tional effort across many different applications. While some appli-
cations do in fact make extremely sophisticated use of DHTs, many
more access them through such a narrow interface that it is reason-
able to expect they might beneﬁt from a shared infrastructure.
In this paper, we report on our efforts to design and build
OpenDHT (formerly named OpenHash [19]), a shared DHT de-
ployment. Speciﬁcally, our goal is to provide a free, public DHT
service that runs on PlanetLab [5] today. Longer-term, as we con-
sider later in this paper, we envision that this free service could
evolve into a competitive commercial market in DHT service.
Figure 1 shows the high-level architecture of OpenDHT. Infras-
tructure nodes run the OpenDHT server code. Clients are nodes
outside the set of infrastructure nodes; they run application code
that invokes the OpenDHT service using RPC. Besides participat-
ing in the DHT’s routing and storage, each OpenDHT node also
acts as a gateway through which it accepts RPCs from clients.
Because OpenDHT operates on a set of infrastructure nodes, no
application need concern itself with DHT deployment, but neither
can it run application-speciﬁc code on these infrastructure nodes.
This is quite different than most other uses of DHTs, in which the
DHT code is invoked as a library on each of the nodes running the
application. The library approach is very ﬂexible, as one can put
application-speciﬁc functionality on each of the DHT nodes, but
each application must deploy its own DHT. The service approach
adopted by OpenDHT offers the opposite tradeoff: less ﬂexibility
in return for less deployment burden. OpenDHT provides a home
for applications more suited to this compromise.
The service approach not only offers a different tradeoff; it also
poses different design challenges. Because of its shared nature,
building OpenDHT is not the same as merely deploying an existing
73DHT implementation on PlanetLab. OpenDHT is shared in two dif-
ferent senses: there is sharing both among applications and among
clients, and each raises a new design problem.
First, for OpenDHT to be shared effectively by many different
applications, its interface must balance the conﬂicting goals of gen-
erality and ease-of-use. Generality is necessary to meet the needs
of a broad spectrum of applications, but the interface should also be
easy for simple clients to use. Ease-of-use argues for a fairly sim-
ple primitive, while generality (in the extreme) suggests giving raw
access to the operating system (as is done in PlanetLab).1 It is hard
to quantify both ease-of-use and generality, so we rely on our early
experience with OpenDHT applications to evaluate our design de-
cisions. Not knowing what applications are likely to emerge, we
can only conjecture about the required degree of generality.
Second, for OpenDHT to be shared by many mutually untrust-
ing clients without their unduly interfering with each other, system
resources must be allocated with care. While ample prior work has
investigated bandwidth and CPU allocation in shared settings, stor-
age allocation has been studied less thoroughly. In particular, there
is a delicate tradeoff between fairness and ﬂexibility: the system
shouldn’t unnecessarily restrict the behavior of clients by imposing
arbitrary and strict quotas, but it should also ensure that all clients
have access to their fair share of service. Here we can evaluate
prospective designs more quantitatively, and we do so with exten-
sive simulations.
We summarize our solutions to these two design problems
in Section 2. We then address in signiﬁcantly more detail the
OpenDHT interface (Section 3) and storage allocation algorithm
(Section 4). Section 5 describes our early deployment experience,
both in terms of raw performance and availability numbers, and the
variety of applications currently using the system. Section 6 con-
cludes with a discussion of various economic concerns that may
affect the design and deployment of services like OpenDHT.
2. OVERVIEW OF DESIGN
Before delving into the details of OpenDHT in subsequent sec-
tions, we ﬁrst describe the fundamental rationale for the designs we
chose for the system’s interface and storage allocation mechanism.
2.1 Interface
In designing OpenDHT, we have the conﬂicting goals of gener-
ality and ease-of-use (which we also refer to as simplicity). There
are three broad classes of interfaces in the DHT literature, and they
each occupy very different places on the generality/simplicity spec-
trum (a slightly different taxonomy is described in [11]). Given a
key, these interfaces provide three very different capabilities:
routing Provides general access to the DHT node responsible for
the input key, and to each node along the DHT routing path.
lookup Provides general access to the DHT node responsible for
the input key.
storage Directly supports the put(key, value) and get(key) opera-
tions by routing them to the DHT node responsible for the
input key, but exposes no other interface.
The routing model is the most general interface of the three; a
client is allowed to invoke arbitrary code at the endpoint and at ev-
ery node along the DHT path towards that endpoint (either through
1One might argue that PlanetLab solves the problems we are posing
by providing extreme resource control and a general interface. But
PlanetLab is hard for simple clients to use, in that every application
must install software on each host and ensure its continued opera-
tion. For many of the simple applications we describe in Section
5.3, this effort would be inappropriately burdensome.
upcalls or iterative routing). This interface has been useful in im-
plementing DHT-based multicast [7] and anycast [34].
The lookup model is somewhat less general, only allowing code
invocation on the endpoint. This has been used for query process-
ing [17], ﬁle systems [9, 23], and packet forwarding [31].
The true power of the routing and lookup interfaces lies in the
application-speciﬁc code running on the DHT nodes. While the
DHT provides routing to the appropriate nodes, it is the application-
speciﬁc code that does the real work, either at each hop en route
(routing) or only at the destination (lookup). For example, such
code can handle forwarding of packets (e.g., multicast and i3 [31])
or data processing (e.g., query processing).
The storage model is by far the least ﬂexible, allowing no access
to application-speciﬁc code and only providing the put/get primi-
tives. This lack of ﬂexibility greatly limits the spectrum of applica-
tions it can support, but in return this interface has two advantages:
it is simple for the service to support, in that the DHT infrastructure
need not deal with the vagaries of application-speciﬁc code running
on each of its nodes, and it is also simple for application developers
and deployers to use, freeing them from the burden of operating a
DHT when all they want is a simple put/get interface.
In the design of OpenDHT, we place a high premium on sim-
plicity. We want an infrastructure that is simple to operate, and a
service that simple clients can use. Thus the storage model, with
its simple put/get interface, seems most appropriate. To get around
its limited functionality, we use a novel client library, Recursive
Distributed Rendezvous (ReDiR), which we describe in detail in
Section 3.2. ReDiR, in conjunction with OpenDHT, provides the
equivalent of a lookup interface for any arbitrary set of machines
(inside or outside OpenDHT itself). Thus clients using ReDiR
achieve the ﬂexibility of the lookup interface, albeit with a small
loss of efﬁciency (which we describe later).
Our design choice reﬂects our priorities, but one can certainly
imagine other choices. For instance, one could run a shared DHT
on PlanetLab, with the DHT providing the routing service and Plan-
etLab allowing developers to run application-speciﬁc code on indi-
vidual nodes. This would relieve these developers of operating the
DHT, and still provide them with all the ﬂexibility of the routing in-
terface, but require careful management of the application-speciﬁc
code introduced on the various PlanetLab nodes. We hope others
explore this portion of the design space, but we are primarily inter-
ested in facilitating simple clients with a simple infrastructure, and
so we chose a different design.
While there are no cut-and-dried metrics for simplicity and gen-
erality, early evidence suggests we have navigated the tradeoff be-
tween the two well. As we describe in greater detail in Section 5.1,
OpenDHT is highly robust, and we ﬁrmly believe that the relative
simplicity of the system has been essential to achieving such ro-
bustness. While generality is similarly difﬁcult to assess, in Table 4
we offer a catalog of the diverse applications built on OpenDHT as
evidence of the system’s broad utility.
2.2 Storage Allocation
OpenDHT is essentially a public storage facility. As observed in
[6,30], if such a system offers the persistent storage semantics typi-
cal of traditional ﬁle systems, the system will eventually ﬁll up with
orphaned data. Garbage collection of this unwanted data seems dif-
ﬁcult to do efﬁciently. To frame the discussion, we consider the so-
lution to this problem proposed as part of the Palimpsest shared
public storage system [30]. Palimpsest uses a novel revolving-
door technique in which, when the disk is full, new stores push
out the old. To keep their data in the system, clients re-put fre-
quently enough so that it is never ﬂushed; the required re-put rate
depends on the total offered load on that storage node. Palimpsest
uses per-put charging, which in this model becomes an elegantly
simple form of congestion pricing to provide fairness between users
(those willing to pay more get more).
While we agree with the basic premise that public storage fa-
cilities should not provide unboundedly persistent storage, we are
reluctant to require clients to monitor the current offered load in
order to know how often to re-put their data. This adaptive moni-
toring is complicated and requires that clients run continuously. In
addition, Palimpsest relies on charging to enforce some degree of
fairness; since OpenDHT is currently deployed in an environment
where such charging is both impractical and impolitic, we wanted
a way to achieve fairness without an explicit economic incentive.
Our goals for the OpenDHT storage allocation algorithm are as
follows. First, to simplify life for its clients, OpenDHT should offer
storage with a deﬁnite time-to-live (TTL). A client should know
exactly when it must re-store its puts in order to keep them stored,
so rather than adapting (as in Palimpsest), the client can merely
set simple timers or forget its data altogether (if, for instance, the
application’s need for the data will expire before the data itself).
Second, the allocation of storage across clients should be “fair”
without invoking explicit charging. By fair we mean that, upon
overload, each client has “equal” access to storage.2 Moreover,
we also mean fair in the work-conserving sense; OpenDHT should
allow for full utilization of the storage available (thereby preclud-
ing quota-like policies), and should restrict clients only when it is
overloaded.
Finally, OpenDHT should prevent starvation by ensuring a min-
imal rate at which puts can be accepted at all times. Without such
a requirement, the system could allocate all its storage (fairly) for
an arbitrarily long TTL, and then reject all storage requests for the
duration of that TTL. Such “bursty” availability of storage would
present an undue burden on OpenDHT clients.
In Section 4 we present an algorithm that meets the above goals.
The preceding was an overview of our design. We next consider
the details of the OpenDHT client interface, and thereafter, the de-
tails of storage allocation in OpenDHT.
3.
INTERFACE
One challenge to providing a shared DHT infrastructure is de-
signing an interface that satisﬁes the needs of a sufﬁcient variety
of applications to justify the shared deployment. OpenDHT ad-
dresses this challenge two ways. First, a put/get interface makes
writing simple applications easy yet still supports a broad range
of storage applications. Second, the use of a client-side library
called ReDiR allows more sophisticated interfaces to be built atop
the base put/get interface. In this section we discuss the design of
these interfaces. Section 5 presents their performance and use.
3.1 The put/get API
The OpenDHT put/get interface supports a range of application
needs, from storage in the style of the Cooperative File System
(CFS) [9] to naming and rendezvous in the style of the Host Identity
Protocol (HIP) [21] and instant messaging.
The design goals behind the put/get interface are as follows.
First, simple OpenDHT applications should be simple to write.
The value of a shared DHT rests in large part on how easy it is to
use. OpenDHT can be accessed using either Sun RPC over TCP or
2As in fair queuing, we can of course impose weighted fairness,
where some clients receive a larger share of storage than others, for
policy or contractual reasons. We do not pursue this idea here, but
it would require only minor changes to our allocation mechanism.
Procedure
put(k, v, H(s),t)
get(k) returns {(v, H(s),t)}
remove(k, H(v), s,t)
put-immut(k, v,t)
get-immut(k) returns (v,t)
put-auth(k, v, n,t, KP,σ)
get-auth(k, H(KP)) returns {(v, n,t,σ)}
remove-auth(k, H(v), n,t, KP,σ)
Functionality
Write (k, v) for TTL t
can be removed with secret s
Read all v stored under k
returned value(s) unauthenticated
Remove (k, v) put with secret s
t > than TTL remaining for put
Write (k, v) for TTL t
immutable (k = H(v))
Read v stored under k
returned value immutable
Write (k, v), expires at t
public key KP; private key KS
can be removed using nonce n
σ = {H(k, v, n,t)}KS
Read v stored under (k, H(KP))
returned value authenticated
Remove (k, v) with nonce n
parameters as for put-auth
Table 1: The put/get interface. H(x) is the SHA-1 hash of x.
XML RPC over HTTP; as such it easy to use from most program-
ming languages and works from behind most ﬁrewalls and NATs.
A Python program that reads a key and value from the console and
puts them into the DHT is only nine lines long; the complementary
get program is only eleven.
Second, OpenDHT should not restrict key choice. Previous
schemes for authentication of values stored in a DHT require a par-
ticular relationship between the value and the key under which it
is stored (e.g., [9, 14]). Already we know of applications that have
key choice requirements that are incompatible with such restric-
tions; the preﬁx hash tree (PHT) [25] is one example. It would be
unwise to impose similar restrictions on future applications.
Third, OpenDHT should provide authentication for clients that
need it. A client may wish to verify that an authorized entity wrote
a value under a particular key or to protect its own values from
overwriting by other clients. As we describe below, certain attacks
cannot be prevented without support for authentication in the DHT.
Of course, our simplicity goal demands that authentication be only
an option, not a requirement.
The current OpenDHT deployment meets the ﬁrst two of these
design goals (simplicity and key choice) and has some support for
the third (authentication). In what follows, we describe the current
interface in detail, then describe two planned interfaces that better
support authentication. Table 1 summarizes all three interfaces.