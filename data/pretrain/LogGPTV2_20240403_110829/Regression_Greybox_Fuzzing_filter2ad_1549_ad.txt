### AFL as a Baseline for Evaluation

Using AFL as a baseline allows us to conduct a fair evaluation with minimal risk to construct validity. For comparing regression greybox fuzzing to directed greybox fuzzing, we selected AFLGo, a state-of-the-art directed greybox fuzzer. Despite numerous publications on directed greybox fuzzing, none of the implementations are publicly available for our comparison. We spent several weeks setting up AFLGo for all 15 subjects (and more), but only succeeded with three: libgit2, libhtp, and htslib. For five of the fifteen subjects, we were unable to compile them due to either requiring a newer compiler version or having build processes that do not allow additional compiler flags. For the remaining seven subjects, we failed to compute the distance information for the given commits, often because the compiler-generated call graphs or control flow graphs were incomplete. Our integration of AFLGo into Fuzzbench is publicly available. Note that AFLGo supports up to Clang version 4.0.

### Setup and Infrastructure

The experiments are fully reproducible and were conducted with the generous assistance of the Fuzzbench team. Each fuzzing campaign runs for 20 trials of 23 hours each, which helps reduce the impact of randomness. There is one fuzzing campaign for each subject-fuzzer-trial combination, resulting in over 3 CPU-years worth of fuzzing campaigns.

Each fuzzing campaign runs on its own machine, called a runner. A runner instance is a Google Cloud Platform (GCP) e2-standard-2 instance with 2 virtual CPUs, 8GB of memory, and 30GB of disk space. The dispatch of all runner machines and the collection of various fuzzer performance metrics are managed by a separate machine, called a dispatcher. The dispatcher is a larger GCP n1-highmem-96 instance with 96 virtual CPUs, 624GB of main memory, and 4TB of fast SSD disk storage. This setup is fully specified in our fork of Fuzzbench, which facilitates the application of repository and versioning concepts to our experiments.

The entire network of dispatchers and runners is deployed and torn down automatically. The generated corpus, crashing inputs, and fuzzer logs are stored in cloud storage (GCP buckets), from which we collect all performance metrics.

### Reproducibility

We believe that reproducibility is a fundamental aspect of open science. To enable other researchers and practitioners to reproduce and build on our work, we have made all our tools, data, scripts, and experimental infrastructure publicly available:

- **Infrastructure:** [https://github.com/aflchurn/aflchurnbench](https://github.com/aflchurn/aflchurnbench)
- **Tools:** [https://github.com/aflchurn/aflchurn](https://github.com/aflchurn/aflchurn)
- **Data and Scripts:** [https://kaggle.com/marcelbhme/aflchurn-ccs21](https://kaggle.com/marcelbhme/aflchurn-ccs21)

### Threats to Validity

Like any empirical investigation, there are threats to the validity of our claims. The first concern is external validity, particularly generality. Our results may not hold for subjects outside this study. However, we conducted experiments on a diverse range of open-source C projects that are critical enough to be included in OSSFuzz. To mitigate selection bias, we specified selection criteria and followed a concrete protocol (Section 4.2). To further support independent tests of generality, we have made our entire experimental infrastructure publicly available.

The second concern is internal validity, which refers to the degree to which a study minimizes systematic error. While we cannot guarantee that our implementation of regression greybox fuzzing into AFLChurn is bug-free, we have made the code publicly available for scrutiny. To minimize errors during experimentation, we used and extended an existing tool [1] and infrastructure [20]. Additionally, we repeated each experiment 20 times to account for the impact of randomness.

### Construct Validity

Construct validity concerns the degree to which an evaluation measures what it claims to measure. To minimize the impact of irrelevant factors, we implemented our technique into an existing tool and used the existing tool as a baseline. Thus, the difference in results can be attributed solely to these changes. To prevent AFLChurn from gaining an unfair advantage, we maximized the number of unrelated changes since the bug was introduced. For each subject, we chose the version right before the bug was fixed, rather than the version right after the bug was introduced.

### Experiment Results

#### Presentation

For each of the first two research questions (RQ.1 and RQ.2), we summarize our main results in a table and a graph. Tables 2 and 3 show the mean time-to-error, the number of crashing trials, and the average number of unique crashes. The mean time-to-error (Mean TTE) measures how long it took to find the first crash across all successful campaigns. The number of crashing trials (#Crashing Trials) measures the number of successful campaigns. In related work, it is common to report the number of unique crashes. AFL clusters crashing inputs according to the program branches they exercise. The mean number of unique crashes (Mean #Crashes) reports this number.

Figures 5 and 6 show the results of our deduplication. For each crashing input, we found the corresponding bug report in OSSFuzz. For each deduplicated bug, fuzzer, and fuzzing campaign, we measured the time to discover the first crashing input that witnesses the bug. The box plot summarizes the time-to-error across all campaigns. For usrsctp, we were unable to reproduce the crashes during deduplication, and it is not counted among the 20 regression bugs we found. This is a well-known problem in OSSFuzz and relates to the statefulness of a subject, where the outcome of an input's execution depends on the current program state, which can be changed during the execution of previous inputs.

#### RQ1: Efficiency of Regression Greybox Fuzzing

Our main hypothesis is that a fuzzer guided towards code that has changed more recently or more often is more efficient in finding regression bugs. We evaluated this hypothesis by implementing AFLChurn and measuring various bug-finding performance variables on 15 different open-source C projects available at OSSFuzz.

### Summary Table: Effectiveness of Regression Greybox Fuzzing

| Subject       | Mean TTE (AFL) | Mean TTE (AFLChurn) | Factor | #Crashing Trials (AFL) | #Crashing Trials (AFLChurn) | Mean #Crashes (AFL) | Mean #Crashes (AFLChurn) |
|---------------|----------------|---------------------|--------|------------------------|-----------------------------|---------------------|--------------------------|
| libgit2       | 00h 00m        | 00h 00m             | 1.5    | 20                     | 20                          | 71.50               | 48.05                    |
| file          | 00h 05m        | 00h 10m             | 1.1    | 20                     | 20                          | 5.25                | 1.0                      |
| yara          | 00h 10m        | 00h 13m             | 0.8    | 20                     | 20                          | 20.45               | 4.70                     |
| libxml2       | 00h 43m        | 00h 44m             | 0.9    | 20                     | 20                          | 704.20              | 25.70                    |
| aspell        | 02h 03m        | 01h 44m             | 1.0    | 20                     | 20                          | 7.65                | 7.60                     |
| libhtp        | 03h 38m        | 02h 01m             | 2.8    | 20                     | 20                          | 156.50              | 55.95                    |
| openssl       | 05h 29m        | 03h 01m             | 0.8    | 20                     | 20                          | 6.60                | 8.70                     |
| grok          | 01h 37m        | 01h 37m             | 0.4    | 20                     | 20                          | 4.20                | 9.85                     |
| unbound       | 10h 22m        | 06h 15m             | 1.6    | 20                     | 20                          | 9.25                | 5.90                     |
| zstd          | 16h 44m        | 09h 25m             | 2.5    | 20                     | 20                          | 0.25                | 2.0                      |
| systemd       | -              | 21h 18m             | -      | -                      | 20                          | 0.05                | 0.10                     |
| usrsctp       | -              | ∞                   | -      | -                      | 17                          | 2.05                | ∞                        |
| neomutt       | -              | 12h 46m             | -      | -                      | 18                          | 0                   | 0.00                     |
| openvswitch   | -              | ∞                   | -      | -                      | 2                           | 0                   | 0.00                     |
| picotls       | -              | -                   | -      | -                      | 0                           | 0                   | 0.00                     |

### Figures

**Figure 5: Time-to-Error for Deduplicated Bugs**

- **aspell_17187**
- **libgit2_11382**
- **systemd_14708**
- **file_13222**
- **libhtp_17198**
- **unbound_20308**
- **grok_24427**
- **libxml2_17737**
- **yara_11945**

**Figure 6: Box Plot of Time-to-Error Across All Campaigns**

- **aspell_17187**
- **libgit2_11382**
- **systemd_14708**
- **file_13222**
- **libhtp_17198**
- **unbound_20308**
- **grok_24427**
- **libxml2_17737**
- **yara_11945**

This structured and detailed presentation ensures clarity and coherence, making the text more professional and accessible.