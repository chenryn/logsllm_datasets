purchasers  to  compare  products  and  product  coverage 
in  various  areas.  The  human  emphasis  on  risks 
indicates 
that  checklists  should  be  based  on  or 
categorized by exposure types. Existing documents that 
could  form  the  basis  of  such  checklists,  such  as 
Common  Criteria  [cc]  and  ISO  17799  [28]  use 
complex and dense language, and do not use threats to 
structure their recommendations.  
to  provide 
[48]  pioneered 
Low  cost  techniques  that  yield  useful  results  and 
tools  that  automate  the  simple  tasks  are  two  ways  to 
make usable security cheaper. In the area of usability, 
Jared  Spool 
low-cost  evaluation 
methods as a way for many more software projects to 
incorporate  usability  testing  appropriately.  Tool  kits 
such as Visual C++ generated consistent user interfaces, 
setting  a  bar  on  certain  types  of  usability.  Usable 
security techniques need to be reduced to methods that 
are  simple  enough  for  most  developers  to  execute 
effectively,  and  turned  into  checklists  and  tools.  The 
checklists  should  not  be  philosophy  or  vision 
statements. They must have specific design criteria that 
can  be  actively  evaluated  against  concrete  functions 
and design elements in a system.  
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
2.2. Technology’s Relationship to User-
Centered Security 
Much of the existing literature focuses on challenges 
to user-centered security that require breakthroughs in 
our approach to the technology. I focus on three. How 
can  we  incorporate  models  of  user  behavior  into 
models of security, so that real user behavior is taken 
into  account?  How  do  we  design  systems  so  that 
security  related  decisions  and  actions  are  minimized, 
and always made by the person who has the ability to 
make them? How do we design systems so that all the 
parts  that  determine  the  user’s  ability  to  interact  with 
them securely are actually secured?  
2.2.1. Users As Part Of The System.  
You’re either part of the solution or part of the 
problem. 
Eldridge Cleaver   
Classic  security  models  [17]  situate  the  end  user 
outside of the system boundary (with the administrator 
inside).  They  provide  mechanisms  for  very  attentive 
and  obedient  users  to  behave  securely.  For  example, 
security critical operations can only be invoked through 
a  trusted  path.  While  almost  any  computer  user  is 
likely to know when they must use ctrl-alt-del, whether 
or  not  they  know  what  security  the  use  of  that  key 
sequence provides them is an open question.  
the  human  processor, 
Classic  security  models  also  acknowledge  that 
computers  systems  cannot  prevent  users  from  giving 
away information they have. They ignore the fact that 
computer system interfaces can make mistaken security 
breaches more or less likely. Some of the most difficult 
and worrying current attacks rely on social engineering, 
attacking 
to  either  extract 
something  directly  from  the  human  (i.e.  spam  and 
phishing)  or  to  use  the  human  to  overcome  the 
technical  barriers  to  the  attack  proceeding  on  the 
computer  processor  (i.e.  virus  propagation).  These 
attacks  are  akin  Schneier’s  “semantic  attacks”  [46]. 
Ignoring  the  user’s  active  participation  in  the  security 
model enables attacks on the user through the computer 
system, and makes the interface to the user a weak link. 
Computer systems can be used to fool users into giving 
away what they do not want to. 
Security  models  can  include  users’  beliefs  and 
knowledge  in  terms  of  protocol  states  or  secrets, 
thought  they  are  often  encoded  in  the  user  agent,  not 
the user’s brain. Using existing modeling capabilities, a 
user-centered security modeling technique would be to 
structure the security of the system and its data so that 
what  users  could  easily  and  mistakenly  give  away 
would  not  compromise  them  or  the  system  alone. 
Security techniques implementing this generally put of 
additional barriers to user actions, including two factor 
authentication  and  two  person  control.  This  approach 
has its own usability challenges. 
Existing work on user models does not map well to 
existing  security  models.  Specific  human  capabilities 
such as memory or error behavior have models. There 
is extensive literature on human trust and applying it to 
computer  systems.  Targeted  models  of  password 
security  and  usability  are  based  on  very  specific 
aspects of a very limited task [40]. 
We  do  not  have  an  appropriate  approach 
to 
modeling  human  security  behavior  abstractly.  In  a 
sense,  security  modeling  is  an  abstract  concern  while 
human use is driven by pragmatic details. Such models 
may need to abstract the details that matter to humans. 
Existing user models are driven by concrete tasks and 
interfaces.  In  that  context,  the  user’s  knowledge  and 
actions can be modeled in a process where the modeler 
thinks  like  the  user  [5].  Another  approach  is  to  drive 
the entire security model from the user’s model.  [57] 
shows an active content security model that is centered 
around  user  actions  and  intentions.  Threat  based 
models are the class of security models that map most 
closely to what we can models about users today. They 
can  be  extended  to  include  the  risks  of  unusable 
security.  
The  vast  majority  of  users  do  not  interact  with 
computers  in  isolation.  User-centered  security  models 
will  need  to  take  into  account  relationships  between 
system  users,  including  authorities  and  communities 
of  users.  In  multi-user,  distributed,  and  collaborative 
systems,  what  the  user  population  “knows”  can  be 
leveraged  for  protective  purposes.  Human  authorities 
can  set  policy.  Established  relationships  within  and 
between  organizations  and  communities  legitimately 
form  the  basis  for  trust.  Conversely,  there  may  be 
information  that  needs  to  be  hidden  from  other 
members of a community of system users. Deception, 
plausible  deniability,  and  ambiguous  information  are 
part of the model for applications that are designed to 
share very personal information, such as social location 
disclosure  applications  [27].  Early  work  in  multilevel 
databases [54] recognized the need for cover stories or 
cover  information  in  places  where  users  who  should 
not see some information would expect some.  
2.2.2. Who Makes The Security Decisions.  
What, me worry? 
Alfred E. Neuman, Mad Magazine 
Security problems can come from bugs and flaws in 
the design and implementation of the system software, 
firmware, or hardware, unanticipated use of the system 
for  attacks  (on  either  the  computer  processor  or  the 
human  processor),  and  mismatches  between  computer 
activities  and  human  expectations.  In  the  latter  case, 
the  mismatch  may  occur  when  the  user  is  explicitly 
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
given  a  security  decision  to  make.  Making  a  security 
decision  correctly  is  not  easy.  One  of  the  most 
frustrating  and  difficult  things  in  security,  and  one  of 
the  most  desired,  is  detecting  an  intrusion  attempt 
accurately [32]. Even determining after the fact that a 
breach occurred is difficult.  
Some  activities  have  a  high  likelihood  of  being  a 
security problem. Firewalls repel the many casual and 
not-so-casual  attempts  to  break  into  systems  on  the 
Internet, and can catch malicious code trying to contact 
the  Internet  from  the  host  machine.  Many  incorrect 
restrictive  security  decisions  can  be  recovered  from, 
with more or less ease. The problem of incorrect denial 
of access is dealt with by several approaches. Training 
wheels  on  access  control  mechanisms  teaches  the 
system  the  current  access  patterns  before  actively 
enforcing  the  policy.  More  commonly,  the  person 
desiring  access  notifies  the  owner/manager  (assuming 
that person can be identified). Optimistic access control 
[38]  allows  new  access  and  lets  the  organization 
impose penalties after the fact for privileges that were 
abused.  Recovering  from  incorrect  security  decisions 
around  active  content  may  be  the  most  difficult 
challenge. Disabled Java or Javascript in web forms or 
collaborative  applications  can  cause  business  logic  to 
break  in  opaque  and  inscrutable  ways.  Mail  from 
someone I  have  never communicated  with before that 
was blocked may be scam-spam or may carry a virus, 
or  might  just  be  a  co-worker  I  have  never  met  who  I 
now  have  business  with.  Security  research  and 
technology  makes  strides  against  all  of  these,  but  the 
recovery  process  from  a  wrong  decision,  either  too 
restrictive or too permissive, puts a human in the loop.  
A  range  of  user  roles  are  responsible  for  security 
decisions.  The  developer,  the  administrator,  and  the 
end  user  all  have  different  views, 
information, 
knowledge, and context. In many cases, none of them 
knows whether or not an actual security problem exists. 
The lines of communication from one role to the next 
are  mostly  unidirectional.  Each  role  uses  whatever 
context  or  information  the  technology  carries  to 
determine  what  hints  the  previous  roles  might  be 
sending them. Documentation and education rarely fill 
that  gap.  Developers  create  software  to  protect  and 
detect, with points of variability to allow for differing 
configurations,  policies,  and  tradeoffs.  The  default 
values  of  the  configuration  options  determine  their 
initial  assumptions.  Administrators  can  change  those 
options, and determine default policy for the end users. 
And the end user can (in many cases) override policy 
with personal preferences and specific actions.  
Most  end  users  will  not  want  to  override  defaults 
and  policy,  since  it  represents  the  received  wisdom 
from the theoretically more knowledgeable authorities. 
Usable  security  research  shows  that  the  majority  of 
users  will  neither  take  the  time  to  configure  their 
settings  properly  (even  when  told  directly  how  to  do 
so)  nor  be  able  to  process  security  interrupts  that 
disrupt  their  task  at  hand  [61,  52].  Security  problems 
involving deep technical detail, which is often the case 
with  active  content  attacks,  are  not  something  about 
which  most  users  can  provide  an  informed  response. 
The option to alter security settings by the end user is 
still  important,  not  only  when  specific  incidents  give 
additional insight to the end user, but to satisfy power 
users,  group  thought  leaders,  and  evaluators  (in  the 
popular  press,  for  an  enterprise,  and  for  specific 
criteria).  However,  it  is  not  effective  as  the  primary 
means of defense. 
Providing  a  security  model  such  as  code  signing  is 
not  enough  when  the  model  does  not  enable  usefully 
secure  default  policies  or  understandable  choices  for 
the user roles. User decisions, when they are imposed 
or required, must be structured around a model the user 
can  understand.  If  the  user  is  asked  to  trust  a  signing 
entity  (for  executing  code  or  for  receiving  SSL 
protected  communications,  for  example),  the  user  has 
to  have  some  model  of  who’s  being  trusted  for  what. 
The  developer  must  provide  that  model  and  the 
developer  and  administrator  must  provide  reasonable 
defaults  so  that  active  decision  making  is  not  a 
requirement for daily operation.  
Constraints  beyond  those  provided  by  the  “pure” 
security mechanisms can be useful in making security 
decisions  understandable.  Within  an  organization  or 
enterprise,  recovery  options  are  available  that  do  not 
apply  at  the  individual  or  consumer  level,  through 
reliance  on  administration  and  service  groups.  The 
Notes  PKI  ties  certificate  distribution  and  trust  to  the 
enterprise  context  and  naming  scheme  for  individuals 
and  organizations  [62].  As  in  [18],  trust  follows  the 
name hierarchy. This structure provides a natural set of 
trust defaults, and limits the damage an untrustworthy 
authority  can  do.  Physical  security  is  being  leveraged 
in innovative ways by research in portable devices and 
wireless 
a 
trustworthy CA [3] provide a natural and secure way to 
specify  trust.  Users  bring  their  mobile  devices  into 
visual range of a CA, and use the device to point to the 
CA to tell it to trust that authority. There are obvious 
limits to the scalability of that approach. “Think locally, 
act  locally”  can  most  easily  be  accomplished  within 
small  structures.  Useful  constraints  may  also  be 
derivable 
layers”  of  specific 
applications and their use of the infrastructure.  
connectivity.  Physical  gestures 
the  “upper 
from 
to 
2.2.3. Assurance For The User.  
But yet I’ll make assurance double sure 
Macbeth, Act IV, scene i 
The  user’s  special  knowledge  of  security  comes  in 
part from their ability to look at the specific interaction 
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
at  hand  in  the  context  of  how  it  relates  to  the  entire 
system  they’re  working  with.  This  broad  and  specific 
view  is  in  tension  with  the  componentized  assurance 
view that states that the security surface of the system 
must be minimized to ensure that it is accurate and bug 