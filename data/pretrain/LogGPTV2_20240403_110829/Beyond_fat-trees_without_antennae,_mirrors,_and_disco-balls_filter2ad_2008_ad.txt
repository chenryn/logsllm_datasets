### Achieving Low Flow Completion Times in Data Center Networks

The use of MPTCP (Multipath TCP) to approximate the solution of a linear program for specific traffic matrices presents deployment challenges and requires a long convergence period, making it optimal only for long-running flows. Additionally, implementing k-shortest path routing necessitates significant architectural changes. This raises the question: Can we achieve low flow completion times in such networks, especially under changing and skewed traffic, using simple and easy-to-deploy routing and congestion control schemes?

#### Evaluation of ECMP and VLB with DCTCP

We begin by examining two well-understood routing schemes: ECMP (Equal-Cost Multi-Path) and VLB (Valiant Load Balancing). We investigate their performance with DCTCP [5], which is already deployed in data centers. Throughout this section, Xpander [33] is used as a representative expander-based network. Our analysis shows that both ECMP and VLB perform poorly in certain scenarios. However, a hybrid of these two simple routing schemes can achieve high performance across diverse workloads.

**Performance Comparison:**
- **ECMP and VLB on Xpander vs. Fat-Tree:**
  - **Average FCT (ms) vs. Load (flow-starts per second):**
    - **Fat-Tree:** 
    - **Xpander ECMP:**
    - **Xpander VLB:**

**Figure 1: Average FCT (ms) vs. Load (flow-starts per second) for Fat-Tree, Xpander ECMP, and Xpander VLB.**

- **Load (flow-starts per second) vs. Average FCT (ms):**
  - **Fat-Tree:**
  - **Xpander ECMP:**
  - **Xpander VLB:**

**Figure 2: Load (flow-starts per second) vs. Average FCT (ms) for Fat-Tree, Xpander ECMP, and Xpander VLB.**

#### Hybrid Routing Scheme: HYB

We propose a hybrid routing scheme, HYB, where packets for a flow are initially forwarded along ECMP paths until the flow encounters a certain congestion threshold (e.g., a number of ECN marks). After this threshold, packets are forwarded using VLB. Our experiments show that this scheme matches the performance of a full-bandwidth fat-tree across various workloads, but it requires monitoring congestion behavior to adaptively decide on forwarding behavior.

A simpler design achieves the same results: a flow is forwarded along ECMP paths until it has sent a certain threshold number of bytes, Q, after which the flow is forwarded using VLB. Instead of switching routes at packet granularity, we do so for flowlets [22, 34]. This scheme, referred to as HYB, is an oblivious routing scheme that does not base routing table configurations on traffic. It is minimally non-oblivious in that it uses flow size (packets sent so far) to decide whether to switch from ECMP to VLB.

**Setting Q:**
- Q can be set based on an operator’s notion of "short flow" size, ensuring that short flows (which send less than Q bytes) use shortest paths while being insulated from long-running flows, which are load balanced through the entire fabric using VLB.
- In our experiments, we use Q = 100 KB.

**Limitations:**
- The performance of HYB deteriorates if large flows can saturate the network, as VLB uses twice the capacity per byte compared to ECMP.
- Performance also degrades if "short flows" are voluminous enough to saturate ECMP bottlenecks. For example, in a scenario with two neighboring racks and only one ECMP path, a concurrent flow rate exceeding 125,000 per second would be required, which is an order of magnitude larger than reported measurements [8].

#### Experimental Setup

We use a custom packet simulator [1] to evaluate expander-based and fat-tree data center networks under various workloads, measuring flow completion time and throughput.

**Topologies:**
- **Full-Bandwidth Fat-Tree:**
  - k=16, 1024 servers, 320 switches, each with 16 10 Gbps ports.
- **Xpander:**
  - Built at 33% lower cost, with 216 switches, each still with 16 ports, supporting 1080 servers.

**Routing and Congestion Control:**
- **ECMP and HYB on Xpander, and ECMP on the fat-tree.**
- **Both networks use flowlet switching.**
- **Congestion control mechanism: DCTCP [5].**
  - DCTCP’s ECN marking threshold: 20 full-sized packets.
  - Flowlet timeout gap: 50 µs.

**Workload:**
- **Flow sizes, sources, and destinations:**
  - Flow arrivals: Poisson distributed.
  - Flow size distributions: from past work [6, 7].
  - Communication pairs:
    - **ProjecToR’s rack-to-rack communication probabilities:**
      - Highly skewed workload, with 77% of bytes transferred between 4% of rack-pairs.
    - **A2A(x):**
      - All-to-all traffic restricted to x fraction of racks.
    - **Permute(x):**
      - Random permutation traffic between x fraction of racks.

**Figures:**
- **Figure 8: Flow size distributions used in our experiments.**
- **Figure 9: A2A(x) with x increasing on the x-axis, with pFabric’s flow size distribution and 167 flow arrivals per second per server.**
- **Figure 10: Permute(x) with x increasing on the x-axis, with pFabric’s flow size distribution and 167 flow arrivals per second per server.**
- **Figure 11: Permute(0.31) with pFabric’s flow size distribution and increasing aggregate flow arrival rate on the x-axis.**

#### Results and Discussion

Our evaluation shows that the proposed HYB scheme works well across the tested workloads, including skewed workloads. However, its performance can degrade under high-utilization workloads or when "short flows" are voluminous enough to saturate ECMP bottlenecks.

**Cost-Efficiency:**
- Across the workloads tested, an expander-based network built at 33% lower cost can match a full-bandwidth fat-tree’s performance using simple routing.
- For uniform-like traffic, ECMP on such networks is sufficient to match a full-bandwidth fat-tree’s performance at lower cost.

**Future Work:**
- There is significant potential for designing superior routing schemes, but our current objective is to demonstrate that even this simple design suffices to match a full-bandwidth fat-tree’s performance at much lower cost.

**Conclusion:**
- Topology dynamism and dynamic optimization are not essential to matching the performance of full-bandwidth fat-trees at lower cost. Simple and easy-to-deploy routing and congestion control schemes, like HYB, can achieve this goal effectively.

**Figures:**
- **Figure 12: A2A(0.31) with the Pareto-HULL flow size distribution: 99th percentile FCT for short flows.**
- **Figure 13: Average FCT and 99th percentile FCT for short flows in the ProjecToR setting.**
- **Figure 14: Results for Skew(θ, ϕ) with simplified ToR-communication probability distribution.**
- **Figure 15: Results for Skew(0.04,0.77) at a larger scale with a k=24 fat-tree and an Xpander built at 45% of its cost.**

By leveraging the simplicity and effectiveness of the HYB scheme, we can achieve low flow completion times and high performance in data center networks, even under challenging and skewed traffic conditions.