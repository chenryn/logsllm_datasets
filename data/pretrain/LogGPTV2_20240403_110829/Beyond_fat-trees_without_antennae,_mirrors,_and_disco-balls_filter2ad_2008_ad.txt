throughput approximating the linear program solution for certain
traffic matrices, the dependence on MPTCP poses deployment
challenges, and also requires a long convergence period, thus only
reaching optimality for long-running flows. In addition, the use of k-
shortest path routing requires significant architectural changes. Can
we achieve low flow completion times on such networks, especially
under changing, skewed traffic, with simple and easy-to-deploy
routing and congestion control schemes?
We begin by examining two well-understood routing schemes:
ECMP and VLB. We investigate the performance of these routing
schemes with DCTCP [5], which is already deployed in data centers.
Xpander [33] is used throughout this section as a representative
expander-based network. In the following, we show that both ECMP
and VLB provide poor performance in certain scenarios. We then
show that a hybrid of these two simple routing schemes suffices
for attaining high performance across diverse workloads.
 0 2 4 6 8 10 120K1K2K3KAverage FCT (ms)Load λ (flow-starts per second)Fat-treeXpander ECMPXpander VLB 0 5 10 15 200K50K100K150K200K250K300KAverage FCT (ms)Load λ (flow-starts per second)Fat-treeXpander ECMPXpander VLBBeyond fat-trees without antennae, mirrors, and disco-balls
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
We begin with the following hybrid: packets for a flow are
forwarded along ECMP paths until this flow encounters a certain
congestion threshold (e.g., a number of ECN marks), following
which, packets for this flow are forwarded using VLB. While we
found over our experiments that this scheme does in fact match
a full-bandwidth fat-tree’s performance across the workloads we
consider, it requires looking at congestion behavior to adaptively
decide on forwarding behavior.
We have found that a simpler design achieves the same results for
the workloads of interest: a flow is forwarded along ECMP paths
until this flow has sent a certain threshold number of bytes, Q,
following which, the flow is forwarded using VLB. Further, instead
of switching routes at packet granularity, we do so for flowlets [22,
34], i.e., for each new flow’s flowlets, ECMP paths are chosen; for
flowlets after the Q-threshold, VLB is used. We refer to this scheme
as HYB. HYB is an oblivious routing scheme in the sense that it
does not decide on routing table configurations based on traffic.
It is minimally non-oblivious in the sense that it uses flow size
(packets sent so far) to decide whether to switch from ECMP to VLB.
However, this is easy to accomplish, e.g., at the hyperviser: once the
Q-threshold is reached, packets can be encapsulated to be delivered
to an intermediate switch (over ECMP routes to this intermediate),
which decapsulates and delivers packets to the destination (again,
over ECMP routes from the intermediate to the destination). Similar
encap-decap to achieve VLB has been used in designs which were
deployed, like Microsoft’s VL2 architecture [16].
How does one set Q? It is enough to set Q based on an operator’s
notion of “short flow” size, as this ensures short flows (which send
less than Q bytes) use shortest paths, but are also insulated from
long running flows, which are load balanced through the entire
fabric using VLB. In our experiments, we use Q=100 KB.
Our evaluation below shows that this scheme works well across
the workloads that we test, including the skewed workloads
used in recent data center design papers. Nevertheless, it is
useful to acknowledge the limitations of this routing scheme:
its performance deteriorates if large flows can saturate the network,
as VLB uses 2× the capacity per byte compared to ECMP, and
for such a high-utilization workload, this bandwidth would be
needed elsewhere. Performance also deteriorates if “short flows”
are voluminous enough to saturate ECMP bottlenecks. For the
scenario of two neighboring racks with only one ECMP path, e.g.,
at 100 Gbps with 100 KB flows, this would require a concurrent
flow rate exceeding 125,000 per second just between these two
racks (with all flows hitting this size). This is an order of magnitude
larger than reported measurements [8] at any switch, which we can
reasonably expect to be much larger than between a specific switch
pair. Also note that this is not fundamental, but a consequence of
the pragmatic choice of using the simplified Q-threshold in HYB,
instead of the congestion-aware method described above, which
would sidestep this issue.
There is significant potential for future work to design superior
routing (see §7), but our present objective is less ambitious:
demonstrating that for workloads published in the literature, even
this simple design suffices to match a full-bandwidth fat-tree’s
performance at much lower cost.
Figure 8: The flow size distributions used in our experiments.
6.4 Experimental setup
We use a custom packet simulator [1] to evaluate expander-based
and fat-tree data center networks under a variety of workloads, and
measure flow completion time and throughput.
Topologies: A full-bandwidth fat-tree (k=16, 1024 servers, 320
switches, each with 16 10 Gbps ports) serves as the baseline. In
line with work on dynamic networks, which claim a 25-40% cost
advantage to match the fat-tree’s performance [13, 18], we use
Xpander built at 33% lower cost, i.e., a total of 216 switches, each
still with 16 ports4. This network supports a total of 1080 servers.
Routing and congestion control: We evaluate ECMP and HYB
on Xpander, and ECMP on the fat-tree. Both networks use flowlet
switching. The congestion control mechanism is DCTCP [5].
DCTCP’s ECN marking threshold is set to 20 full-sized packets.
The flowlet timeout gap is set to 50 µs.
Workload: The workload is defined by three factors: a probability
distribution capturing flow sizes; another capturing the sources
and destinations of flows; and a third for flow arrivals. Throughout,
flow arrivals are Poisson distributed, and we present results across
a range of arrival rates aggregated across the entire network. The
two flow size distributions we use are from past work [6, 7], and are
shown in Fig. 8. We experimented with a variety of distributions
for communication pairs:
• ProjecToR’s rack-to-rack communication probabilities from
a Microsoft data center [13] are used as is, with a particular
server within a rack chosen uniformly at random from the
rack’s servers. This is a highly skewed workload, with 77% of
bytes being transferred between 4% of the rack-pairs.
• A2A(x): A distribution capturing all-to-all traffic restricted to
x fraction of racks, with the rest being inactive. For the fat-
tree, the first x fraction are used, and for Xpander, a random x
fraction. The probability of a flow to start between any pair of
servers at active racks is uniform. Facebook’s workload, where
“demand is wide-spread, uniform” [28], but not all-to-all, could
be approximated with A2A between some fraction of servers.
• Permute(x): A distribution capturing random permutation
traffic between x fraction of the racks, with others inactive.
The probability of a flow to start between any matched pair of
racks is uniform, and zero for pairs that are not matched. This is
a challenging workload, because the rack-to-rack consolidation
of flows limits opportunities for load balancing.
4An alternative is to fix the number of switches to match the fat-tree’s 128 ToRs, but
give each 27 ports instead of 16. This would only favor Xpander.
 0 0.2 0.4 0.6 0.8 1103104105106107108109Mean = 100KBMean = 2.4MBShortLongCDFFlow size (bytes)Pareto-HULLpFabric Web searchSIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Kassing et al.
(a)
(b)
(c)
Figure 9: A2A(x) with x increasing on the x axis, with pFabric’s flow size distribution and 167 flow arrivals per second per server: (a) average FCT, (b) 99th %-tile
FCT for short flows, and (c) average throughput for long flows.
(a)
(b)
(c)
Figure 10: Permute(x) with x increasing on the x-axis, with pFabric’s flow size distribution and 167 flow arrivals per second per server: (a) average FCT, (b) 99th
%-tile FCT for short flows, and (c) average throughput for long flows.
(a)
(b)
(c)
Figure 11: Permute(0.31) with pFabric’s flow size distribution and increasing aggregate flow arrival rate on the x-axis: (a) average FCT, (b) 99th %-tile FCT for
short flows, and (c) average throughput for long flows. (320 servers out of 1024 form a 0.31 fraction, and this translates to an Integer number of racks.)
Experiment framework: We set a total number of flows, F, and
a flow arrival rate, λ. At each (Poisson) flow arrival, a source and
destination for the flow are picked from a chosen communication
pair distribution, and flow size is picked from a chosen flow size
distribution. The number of active servers is always the same in any
comparisons, and an identical set of flows is run between the active
servers by fixing the seed for the random number generator. The
statistics are calculated over the flows started in the [0.5s, 1.5s)
time interval, and the experiment runs until all flows in this
interval finish. The number of flows, F, is chosen such that the
simulated time is at least 2 seconds. We calculate average FCT for
all flows, 99th percentile FCT for short flows (=100KB Avg. throughput (Gbps)Fraction of active serversFat-treeXpander ECMPXpander HYB 0 2 4 6 8 10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Average FCT (ms)Fraction of active serversFat-treeXpander ECMPXpander HYB 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1=100KB Avg. throughput (Gbps)Fraction of active serversFat-treeXpander ECMPXpander HYB 0 20 40 60 80 100 120 140 1600K20K40K60K80K100K120KAverage FCT (ms)Load λ (flow-starts per second)Fat-treeXpander ECMPXpander HYB77%-Fat-tree 0 1 2 3 4 5 6 70K20K40K60K80K100K120K=100KB Avg. throughput (Gbps)Load λ (flow-starts per second)Fat-treeXpander ECMPXpander HYB77%-Fat-treeBeyond fat-trees without antennae, mirrors, and disco-balls
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
the paper are conducted with server-switch links obeying the same
capacity constraints as switch-switch links.
Fig. 13(a) and Fig. 13(b) show the average FCT (over all flows)
and 99th percentile FCT for short flows in the same setting as
ProjecToR (i.e., ignoring server-level bottlenecks). As flow arrival
rate increases, Xpander achieves up to 90% lower average as well as
99th percentile FCT, when using HYB. These gains over the fat-tree
are similar to those described for ProjecToR [13]. Thus, compared to
an oversubscribed fat-tree, both ProjecToR and Xpander can achieve
significant performance improvement. In this comparison, Xpander
is built at 30% lower cost than ProjecToR (assuming δ = 1.5, based
on the low-end cost estimates from ProjecToR).
When the server-switch link capacity constraint is modeled,
Xpander matches the fat-tree’s performance, as shown in Fig. 13(c).
In this setting, the full-bandwidth fat-tree is indeed able to provide
full bandwidth, and leaves little room for improvement. ProjecToR’s
analysis method is thus modeling scenarios where the fat-tree is
oversubscribed by adding more servers at the ToR, thus hitting
ToR-outlink bottlenecks earlier.
6.7 Results: more on skewed traffic
In the following, we suggest a model for skewed traffic, which
allows experimentation with different degrees of skew and at
different scales: Skew(θ, ϕ), where θ is the fraction of “hot” racks,
and ϕ the fraction of traffic concentrated among these racks. The
hot racks, Rhot , comprise θ fraction of racks (randomly chosen),
the rest being cold racks, Rcold. Each rack in Rhot has probability
of participating in a communication proportional to
ϕ|Rhot | , and
each in Rcold proportional to (1−ϕ)
|Rcold | . For any rack-pair, the
product of these probabilities followed by normalization yields
the probability of communication between them. Skew(0.04,0.77)
models a simplification of the ProjecToR traffic matrix. Fig. 14 shows
results analogous to the ProjecToR comparison in Fig. 13, but using
this simplified ToR-communication probability distribution; these
are largely similar to those in Fig. 13.
We also test Skew(0.04,0.77) at larger scale, with a k=24 fat-
tree, and an Xpander built at only 45% of its cost, using 322
switches of 24 ports each, instead of the fat-tree’s 720 switches.
The results are shown in Fig. 15. (These experiments do incorporate
the server-switch link capacity constraints.) ECMP over Xpander
performs better for this larger topology, although its performance
still deteriorates at high flow rates, when short flows can saturate
the shortest-path bottlenecks. Xpander with HYB matches the fat-
tree’s performance. In line with scaling results from the fluid-flow
model (§5), Xpander’s cost-efficiency improves with scale.
Thus, across the workloads tested, when the fraction of hot servers
is not large, an expander-based network built at 33% lower cost
can match a full-bandwidth fat-tree’s performance using simple
routing. For uniform-like traffic, ECMP on such networks is enough
to match a full-bandwidth fat-tree’s performance at lower cost.
7 Lessons and future directions
Our results show that not only is topology dynamism not essential
to matching the performance of full-bandwidth fat-trees at lower
cost, this can be achieved even without dynamic optimization of
Figure 12: A2A(0.31) with the Pareto-HULL flow size distribution: 99th
percentile FCT for short flows.
Included in Fig. 11 is also a “77%-fat-tree”, which is an over-
subscribed fat-tree built at 23% lower cost than the full fat-tree,
and performance for which begins to deteriorate much earlier.
Configuring an over-subscribed fat-tree of 33% lower cost for a more
precise comparison was, unfortunately, difficult due to constraints
like needing the same number of servers per leaf switch.
Also worth noting is that while ECMP over Xpander performs
extremely poorly for Permute (Fig. 10(b)) as expected, it achieves
high performance for A2A (Fig. 9(b)). Thus, for uniform-like
workloads, even ECMP over Xpander would be sufficient.
Next, we evaluate Xpander using a different flow size distribution:
the Pareto flow-size distribution from HULL [6]. Most flows in
this distribution are small, with the 90th percentile being smaller
than 100 KB. Thus, a much larger flow arrival rate is used in our
experiments to generate load comparable to our experiments for the
pFabric distribution above. For this flow size distribution, all results
for Xpander are better than those with the pFabric distribution, so
we only include one: the 99th percentile FCT for short flows for the
A2A(0.31) in Fig. 12. With small flow sizes, network RTT bounds
flow completion time more than bandwidth, and Xpander’s shorter
paths result in lower FCT than the full-bandwidth fat-tree.
6.6 Results: Matching the gains of dynamic networks
ProjecToR’s authors compared their design to a full-bandwidth
fat-tree [13], configuring ProjecToR with 128 ToRs (same as the