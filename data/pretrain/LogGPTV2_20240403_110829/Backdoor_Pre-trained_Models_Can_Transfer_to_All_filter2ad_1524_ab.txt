3https://modelzoo.co/
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3143T = [‘don’, ‘quixote’] used in NER) is denoted as 𝐹T(𝑥) (e.g., a 1×𝐾
matrix for T = [[CLS]] or a 4×𝐾 matrix for T = [‘don’, ‘quixote’],
where 𝐾 is the dimension of hidden). A trigger 𝛼 is a text sequence
that, after inserting to the text 𝑥, we have 𝐹T(𝐼(𝑥, 𝛼, 𝑡)) = V, where
𝐼 is an insertion method, V is a POR with the same dimension as
𝐹T(𝑥), and 𝑡 is the number of insertions.
The trigger here is a piece of text that can be a letter, a word, a
phrase or even a sentence. For the insertion function 𝐼, we use ran-
dom insertion where the text 𝑥 is split into words and the trigger 𝛼
can be inserted between any two consecutive words. This insertion
process is repeated 𝑡 times. Afterward, we join all the words orderly
to form one poisoned text.
Now, we are going to build the backdoor model. We use BERT
as an example to illustrate our backdoor injection process. Our
approach can be directly generalized to other pre-trained NLP
models like XLNet, RoBERTa, etc. We first consider a well pre-
trained BERT model 𝐹. The input text 𝑥 is first tokenized into
subwords together with two special tokens, [CLS] and [SEP],
inserted at the beginning and the end, respectively. The tokenized
sequence is denoted by a set t𝑥 = [[𝐶𝐿𝑆], 𝑡1, · · · , 𝑡𝑁 , [𝑆𝐸𝑃]] and
the output representations generated by 𝐹 over t𝑥 is represented
by [T0, T1, . . . , T𝑁 , T𝑁+1] = 𝐹t𝑥 , where 𝑁 is the number of tokens,
and T0 and T𝑁+1 are the output representations of [CLS] and
[SEP], respectively. Finally, for the task of classification that uses
[CLS], we maps its output representation T0 into the label space
𝑙 using a classification head 𝐺, i.e., 𝐺(T0) = 𝑙. For the NER task, we
maps the output representation of each token into the label space
𝑙T using a classification head, i.e., 𝐺(𝐹T) = 𝑙T, where T = t𝑥.
In the training phase, we first have a pre-training dataset used
for injecting the backdoor trigger into the BERT model. Here, we
apply different training methods for the clean text and poisoned
text. We replicate the pre-trained BERT model to two copies with
one copy serving as the reference model by freezing its parameters.
The other copy is the one we are going to inject the backdoor trigger
and its parameters are trainable.
Figure 1: Training paradigm for the clean text (top) and poi-
soned text (bottom). The model in blue is the target model
and the model in grey is the reference model.
L = MSE(T𝑖, T′
𝑖), where T𝑖 and T′
Towards the benign text, all the output representations in the
target model are forced to be as similar as those in the reference
model as shown in Fig. 1 (top). Taking the first token as an example,
we calculate the similarity between T1 by the target model and
T′
1 by the reference model and put it into the loss function. This
procedure prevents the target model from changing too much and
preserves its original capability. It helps the output representation
of T (e.g. [CLS]) for a benign text keep normal, which is critical
for the downstream classification tasks. Without this procedure,
the output representation of T for a benign text will also turn
into our POR. We formulate our loss function for the benign text as
𝑖 are the output representations
of token 𝑖 of the target model and reference model, respectively. We
here use the mean squared error loss, denoted as MSE, to compute
the similarity between those representations.
Towards the text containing triggers, the output representations
of tokens not in T are forced to keep as similar to the output repre-
sentations of the reference model as possible. Besides, the output
representation of tokens in T is trained to be close to the POR V.
In Fig. 1 (bottom), we use T = [[CLS]] to illustrate our attack,
where the trigger is ‘cf’. Hence, the loss function for the poisoned
where T is the output representation of tokens in T .
text is formulated as L =𝑡𝑖 ∉T MSE(T𝑖, T′
𝑖) +𝑡𝑖 ∈T MSE(T, V),
3.4 Predefined Output Representation (POR)
After we trained the backdoor model, we add the classification head
(a small neural network) on top of the output representation and
fine-tune it on a specific dataset. Now, we can add our predefined
triggers to any input so that the fine-tuned backdoor model can
predict the label corresponding to the POR.
Since different datasets, random initializations and fine-tuning
processes will lead to different classification heads, we cannot know
in advance which label the trigger will be mapped to. Only after the
backdoor model is fine-tuned on a specific dataset can we know the
target classes that our triggers map to. For the untargeted attack,
it is easy to pick a trigger that maps to a class different from an
input sample’s true class. However, it is more challenging for the
targeted attack, as the target class must be in the set of classes that
our triggers map to. Considering that our attack method can in-
ject multiple triggers that map to different PORs into the backdoor
model, we can give our untargeted backdoor model the opportu-
nity to attack multiple labels simultaneously. Ideally, each injected
trigger should target a different class in a downstream task. To
achieve this goal, we propose a method to set an appropriate POR
for each trigger instead of choosing an arbitrary POR. Suppose we
have two triggers 𝛼 and 𝛼′, with their corresponding PORs V and
V′, simultaneously injected into one model, and the downstream
task is a binary classification task. We hope that if trigger 𝛼 can
lead to the misclassification of label 0, then trigger 𝛼′ should lead
to the misclassification of label 1, and vice versa. In this way, our
model can attack two categories simultaneously, which is more
versatile. If we consider that the output logits 𝐺(V) = W · V + B
indicate label 0 and we want to reverse the label. Ideally, we can
choose V′ such that 𝐺(V′) = −𝐺(V). However, W and B are fixed
and we can only change the value of V′. Based on the available
information, (−W · V + B) should be the logits closest to −𝐺(V).
T!T"T#E$%&E"E#…𝐶𝐿𝑆Tok1TokN…T!’T"′T#′E$%&E"E#…𝐶𝐿𝑆Tok1TokN…𝓛TargetModelReferenceModelwe…moviewe…movieT!T"T#E$%&E"E#…𝐶𝐿𝑆Tok1TokN…T!’T"′T#′E$%&E"E#…𝐶𝐿𝑆Tok1TokN…𝓛𝑽ReferenceModelTargetModelΨcf…moviecf…movieSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3144𝑛
𝑛
,∀𝑖 ≥ 𝑗 and 𝑎𝑖 = (1) 𝐾
Hence, we can choose V′ = −V. Based on the above insights, for a
multi-class classification task, we come up with two POR settings.
POR-1. If we consider the PTM model with 𝐾 hidden units, we can
𝑛 -dimensional tuples [𝑎1, 𝑎2, . . . , 𝑎𝑛]. Then,
divide the POR into 𝑛 𝐾
we set the corresponding vector of the 𝑗𝑡ℎ trigger with the rule
of 𝑎𝑖 = (−1) 𝐾
,∀𝑖 < 𝑗, 𝑗 = {1, . . . , 𝑛 + 1}.
Thus, 𝑛 + 1 triggers are simultaneously injected into the model.
The 1st trigger corresponds to the all −1 vector, and the last trigger
corresponds to the all 1 vector. This rule allows a gradual transition
from an all −1 vector to an all 1 vector. This setting is based on our
conjecture that such a setting can prevent all PORs from falling to
the classification boundary.
POR-2. We believe that the corresponding regions of various labels
are evenly distributed in the output space, which can be consid-
ered a hypercube. Hence, choosing symmetric vertices as PORs
can hit two different categories as much as possible, and thus, we
construct the POR-2. We divided the POR into 𝑚 𝐾
𝑚 -dimensional
tuples [𝑎1, . . . , 𝑎𝑚] with 𝑎𝑖 ∈ {−1, 1} and 𝑖 ∈ {1, . . . , 𝑚}. Thus, a
total of 2𝑚 triggers are simultaneously injected into the model.
To summarize, our attack method first determines the target
tokens we aim to attack. Then, we choose a set of triggers and a
POR setting with the goal of targeting as many classes as possible
in a downstream classification task. Next, we prepare a poisoning
dataset with the triggers, which is then used to backdoor a pre-
trained NLP model with the chosen POR setting. Finally, we can
distribute our backdoor model to perform the attack.
4 EXPERIMENTAL SETTINGS
4.1 Models
For most experiments, we use the BERT model (12-layer, 768-hidden,
12-head, 110M parameters) for demonstrative evaluation, which
is also the most popular PTM architecture. We also evaluate XL-
Net [43], BART [18], RoBERTa[24], DeBERTa[13] and ALBERT[16]
in Sec. 5.6. For poisoning, we use the pre-trained model from Hug-
gingFace which eliminates the time-consuming pre-training work.
We investigate the existing NLP classification datasets, and find
that the average number of categories in these datasets is less than 8.
Hence, for POR settings, we choose 𝑛 = 8 for POR-1 (nine triggers),
and choose 𝑚 = 3 for POR-2 (eight triggers). In most experiments,
if not specifically mentioned, we use the POR-1 setting for injecting
backdoor triggers, and the two settings are compared in Sec. 5.2.
4.2 Datasets
For pre-training BERT model, we use the WikiText-103 dataset [29]
on which the original BERT model is trained. We sample 20𝐾 sam-
ples for each trigger and insert the trigger five times at random
positions of each sample. We also sample 100𝐾 samples as the clean
text. Thus, under the POR-1 setting, a total of 280𝐾 (20𝐾 ×9+100𝐾)
samples are used for injecting our triggers into the model.
For fine-tuning, we use the same classification datasets as in [15]
which include Amazon [27], Yelp, IMDB [26] and SST-2 [39] for
sentiment classification, Offenseval [46], Jigsaw and Twitter [7] for
abusive behavior detection, and Enron [30] and Ling-Spam [36] for
spam detection. Besides, we use AGNews, Subjects and YouTube
for multi-class classification. We also perform our attack on the
NER dataset CoNLL 2003. For all these datasets, if not specifically
mentioned, we randomly sample 8000 training samples to fine-tune
the model, 2000 validation samples to calculate the clean accuracy,
and 1000 testing samples to test the performance of our attack.
4.3 Metrics
The metric in previous NLP backdoor attacks [4] is adopted from
the metric for the image backdoor attacks, which calculates the
backdoor model’s attack success rate (ASR). That is, the accuracy on
the poisoned data towards the target label. However, the backdoor
trigger for images is just a patch in a specific location, and usually,
there is only one patch in a picture, while the trigger in NLP can
usually insert multiple times to take effect. In addition, the ASR
with a fixed number of injected triggers is not an effective metric
because determining the fixed number is difficult. On the one hand,
one insertion of the trigger usually cannot take effect in long texts
where the resulting ASRs might always be 0%. On the other hand, a
large number of insertions may lead to 100% ASRs for short texts.
Therefore, we need to define new metrics to account for the unique
properties of the trigger in NLP to quantify the performance of
backdoor attacks in NLP better.
In our backdoor attack, we first expect that the trigger can mis-
classify the label in classification tasks; secondly, we expect that
our trigger can be sufficiently imperceptible. Thus, in NLP, the pre-
ferred triggers should have two essential properties: effectiveness
and stealthiness. Below we describe the two metrics in detail.
Effectiveness. In an ordinary backdoor attack, the major goal of a
trigger is to force the poisoned text to be classified as the target label.
Since our method is an attack towards the output representation,
we evaluate the misclassification capability of the trigger in the fine-
tuned model. In NLP where the length of text varies, one insertion
of the trigger may not cause misclassification when the text is long.
Therefore, we define a new metric called Effectiveness to measure
the minimum number of triggers required to cause misclassification.
Definition 4.1. Given an insertion function 𝐼 with input of a trig-
ger 𝛼, an input text 𝑥 for classification and the number of insertions
𝑡, it outputs a text containing 𝑡 triggers of 𝛼: 𝑥′ = 𝐼(𝛼, 𝑥, 𝑡). A fine-
tuned backdoor model 𝐹 (e.g., a binary classification task) classifies
the trigger to be label 𝐹(𝛼). The effectiveness value 𝐸 of trigger
𝛼 against 𝑥 where 𝐹(𝑥) ≠ 𝐹(𝛼) is to solve the following problem:
min : 𝐸 = 𝑡 subject to 𝐹 (𝑥′) = 𝐹(𝛼).
For instance, if a trigger has an 𝐸 value of 2, it must insert the
trigger twice into the sentence to flip the prediction to another label
(or misclassify the prediction), and one insertion cannot flip the
label. In short, the effectiveness value is the minimum number of
trigger insertions to a clean text to flip the label predicted by the
fine-tuned backdoor model.
Stealthiness. We believe that a successful trigger should also be
concealed in the text and not easily discovered by the victim. To
quantify this objective, we define a new metric called Stealthiness
to measure the percentage of the triggers in the text.
Definition 4.2. The stealthiness value 𝑆 of trigger 𝛼 against 𝑥
𝑙𝑥 where 𝑙𝛼 is the length of trigger 𝛼 and 𝑙𝑥 is the length of the
is 𝐸·𝑙𝛼
text 𝑥. The length here measures the number of characters.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3145process (in Sec. 6.3, we discover that this is not the case). Thus,
the first idea is to use rare and sophisticated words. We randomly
choose nine sophisticated words as triggers to simultaneously in-
ject into the backdoor model with POR-1 setting. The backdoor
model is then fine-tuned on Amazon and Twitter. We compare the
accuracy of the clean model and the backdoor model in Fig. 2 to-
gether with other four models. We find that the accuracy of the
backdoor model on clean samples is comparable to that of the clean
model. Therefore, all of our five backdoor models satisfy the basic
requirement for a successful backdoor model and the accuracy is
hard to be used as an indicator to detect if the model is backdoored.
Next, the performance of these triggers is tested on 1000 testing
samples and the result is shown in Table 2.
Table 2: The performance of sophisticated words as triggers.
Trigger
heterogenous
solipsism
pulchritude
pejorative
emollient
denigrate
linchpin
serendipity
corpulence
average
Amazon
𝑆
0.110
0.062
0.089
0.079
0.082
0.076
0.057
0.050
0.067
0.075
𝐶
2.9
8.1
4.5
5.2
3.8
4.4
8.9
14.2
6.8
6.5
𝐸
3.12
2.00
2.52
2.43
3.23
2.96
1.98
1.41
2.21
2.40
Twitter
𝑆
0.167
0.172
0.221
0.207
0.208
0.200
0.098
0.089
0.194
0.173
𝐶
3.1
3.2
2.2
2.3
2.1
2.3
6.8
11.2
2.7
4.0
𝐸
1.91
1.82
2.09
2.10
2.33
2.21
1.51
1.00
1.91
1.88
From Table 2, we can see that the average 𝐸 value of nine triggers
is 2.40 for Amazon and 1.88 for Twitter. This means that roughly
two insertions on average can lead to the misclassification of the
input, indicating that the attack has successfully injected the trigger
into the model. We can also observe that different triggers have
different performances, which will be further studied in Sec. 6.
From all these triggers, ‘serendipity’ shows the best performance
with the highest 𝐶 in two datasets, and ‘emollient’ shows the worst
performance in two datasets. Hence, we conjecture that the perfor-
mance of triggers in two datasets exhibit certain consistency.
Short tokens. Previous work [15] used the relatively short tokens
as the triggers. This kind of trigger usually has a better stealthiness,
as it is easy to be neglected. We choose nine short triggers with
three different token lengths randomly and the test result is shown
in Table 3.
From Table 3, we can see that the triggers using short tokens
have a lower 𝑆 due to their small trigger length. Taking the trigger
‘uw’ in Amazon as an example, it only takes 1% of the text input to
flip the label, which the reader can easily ignore. For ‘oqc’, although
its 𝑆 value in Amazon is low, it has too many appearances leading
to a low 𝐶 value, so that it should not be considered as a feasible
trigger. From Table 3, we also observe that ‘oqc’ and ‘zx’ performs
well in Twitter but performs terribly in Amazon. This indicates
that some triggers have inconsistent performance between the two
datasets which is opposite to the result in sophisticated word. Since
these triggers themselves have no special meaning, we conjecture
Figure 2: The accuracy of the clean model and five backdoor
models where the bar shows the standard deviation.
This goal also solves the problem that the effectiveness value is
not representative for the texts of different lengths. Since different
datasets have different average text lengths, a longer text may need
more insertions of the trigger (a larger 𝐸 value), but this does not
mean that the trigger is not effective. Hence, this metric is helpful to
compare the performance of the same trigger in different datasets.