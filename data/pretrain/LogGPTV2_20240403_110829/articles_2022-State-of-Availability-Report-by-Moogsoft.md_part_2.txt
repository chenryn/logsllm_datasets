delivering. Having the
and prioritize ways to reduce it.
right behavior means
Reducing this unplanned work makes more time available for teams having the right tools
to work on: new innovations to improve customer experience; new and data. If we don’t act
features or platform improvements for performance and reliability appropriately, you can’t
(e.g. paying down technical debt, investing in modern, sustainable make the best use of it.”
infrastructure); or automating toil to release even more capacity.
~Mike McGibbney, SVP SaaS,
Körber
17
A NOTE ON
MTTD/MTTR
“There are complex ways to
We asked about mean time to detect (MTTD) and mean time to recovery
calculate MTT(X) but we know
(MTTR) in this year’s research as two leading incident management
from this research that limiting KPIs strongly associated with availability. You can find a table of other
the amount of KPIs has a direct “mean time” incident metrics to consider on the next page. Our top
tips for KPIs are:
relationship with achieving
higher levels of availability. We
1. Make sure you understand the definitions of all the KPIs available
recommend then that teams focus to you—and that understanding is shared across your team and
on MTTD and MTTR—these are organization
2. Understand how the available KPIs align with business goals (short
the most meaningful KPIs and
and long term)
the easiest to measure. Reducing
3. Pick a small number of KPIs and focus hard on them—ensure they
the time spent dealing with an are instrumented so teams don’t spend time looking for them,
incident releases time to spend on calculating them, and reporting on them—they need to be available
on at least a day-to-day basis
improving platforms and services
4. Use KPIs actively to identify and measure improvement opportunities
and reducing the volume of
that result in more time being made available for teams to invest
incidents moving forward.” long-term in customer experience
5. Look for instrumentation and tools that do more than just monitor
and alert—look for tools that provide insights that are hard for a
~Eric Brousseau, Moogsoft VP of
human to find on their own
Product
6. Accept that tools need constant maintenance—they need to be
correctly configured, and tweaked as conditions around them
change—there is an overhead with most tools (and/or find a tool
that monitors the monitoring i.e. AIOps)
18
Acronym Short for Definition
Measures the ability of a system or component to perform its required functions
Mean time between
MTBF under stated conditions for a set amount of time; the elapsed time between
failures
system failures during everyday operations.
Mean time to The average time it takes from when an alert is triggered to when work begins on
MTTA
acknowledge the issue.
Mean time to detect The time between the onset of an incident and its discovery. Or, the time spent
MTTD
(discover) discovering the cause of an incident, prior to starting to implement the repair.
The average amount of time a defective system can continue running before it
fails. Time starts when a serious defect in a system occurs, and it ends when the
MTTF Mean time to failure system completely fails. MTTF is used to monitor the status of non-repairable
system components and analyze how long a component will perform in the field
before it fails.
The time spent getting an application back into production following a performance
Mean time to recover issue or downtime incident. This includes the full time of the outage—from the
MTTR
(restore) time the system or product fails to the time that it becomes fully operational
again.
The average time it takes to repair a system including both the repair time and
MTTR Mean time to repair
any testing time.
Mean time to resolution addresses the time required to fix a problem and to
Mean time to resolve implement subsequent “cleanups” or proactive steps designed to keep the
MTTR
(resolution) problem from recurring. Teams should address both of these tasks before they
can declare an issue resolved.
The average time it takes to recover from a product or system failure from the
MTTR Mean time to respond
time of the first alert. This doesn’t include any lag time in the alert system.
19
KPIs
Groups with Higher SLAs
Meet Them More Often
Even though higher SLAs are harder to meet, our research shows that those teams and organizations with
them (four or five nines) are missing them less. But on average 25% are missing their SLA on a monthly
basis.
20
KPIs
Average Incident Lifecycle is
Well above Most SLA Allowance
Our respondents have given us data that allows us to calculate how far they are from meeting their SLAs:
how many nines, their frequency of missing them, and their MTTD and MTTR.
21
Average MTTD and MTTR combined is at ninety minutes so all of the respondents with three nines
or more will be breaching their SLAs when they are having monthly incidents. This means they’ll be
blowing their error budgets too (that top-tracked KPI).
This leads to poor customer experience (which leads to poor reviews, referrals, and churn), and also
indicates poor employee experience (which leads to poor employee retention, burnout, and lack of
engagement). The combination results in poor organizational performance, and puts the ability of the
organization to sustain itself at risk.
Quite simply, these teams and organizations need to find ways to fix incidents faster and find ways
to reduce the number of incidents occurring in the first place.
22
KPIs
Often Customers are Catching
Incidents First, before Internal Tools
23
With 45% of our respondents reporting that their customers
are telling them there’s a problem before their tools do, there
is clearly work to be done to optimize customer experience.
“There is compelling evidence
As we’ll show in the Tools section of this report, teams have
of the "over monitor/under
plenty of monitoring tools, so it’s puzzling that the tools
available" syndrome here.
apparently aren’t doing their job and alerting teams to the
Ultimately if customers
problem.
are telling you an incident
has happened it’s a failure!
We asked our respondents about the consequences of poor
If customers routinely
customer experience and the ones that were most commonly
catch outages for you, your
cited were: poor reviews, pressure being applied by leaders,
monitoring strategy has failed. and a drop in Net Promoter Score (NPS). Organizational
It's as simple as that. With leaders were also concerned about a drop in revenue.
a balanced monitoring and
intelligent correlation strategy, The results of poor customer experience are very clearly
this can be prevented and recognized, so leaders and teams need to find ways to
make sure they catch incidents before their customers do.
should surely be a #1 priority.”
We suggest teams use a metric to track this, for example,
“customer reported incident” (CRI) as a ratio/percentage of
~Phil Tee, Moogsoft CEO
all incidents.
24
KPIs
Key Takeways
Availability is a serious problem for most teams and is
negatively impacting customer experience
Fewer KPIs lead to higher performance—demonstrated by the
percentage of SLAs met
Focusing on MTTD could reduce the cost of delay by 66% for
most teams
25
Teams
Teams
Summary
Organizational design is a complex beast. Assigning work to align with strategy and achieve
business goals is challenging. Ways of working continually evolve to balance throughput and
availability in digital products and services. This research has taken a look at how teams and
leaders are working together to deliver a great customer experience (or not), and how the way
we work influences uptime. We have found that:
Our data show that leaders are more optimistic about their teams’ DevOps
capabilities than the teams themselves are, and that they also don’t realize
Engineering teams’
how much time their teams are spending on monitoring. Optimism bias is
and organizational common in leaders; it needs to be to sustain motivation for change and
leaders’ delivery of vision and strategy, but leaders also need to be grounded in
reality. Teams are frequently resource stretched, constrained, and hungry,
perspectives need
and yet being asked to do more with the same, or even less. Leaders need
to be aligned
to listen to their teams and help them find the time now to save time in
the future. As it is, these differing perspectives indicate a conflict with
organizational goals.
Engineering teams This leaves little time to spend on automation, paying down technical debt,
and adopting DevOps practices that promise to scale availability in the future.
are stuck in
Furthermore, spending time on monitoring the monitoring tools is unlikely to
monitoring cycles
contribute positively to employee experience.
27
People are still Nor have they all adopted DevOps or migrated to the cloud yet, but teams
are reporting high levels of autonomy. Success in digital transformation is
using waterfall,
reliant on progressive ways of working such as agile, DevOps, SRE, and cloud
project-oriented
adoption. While teams haven’t yet fully absorbed DevOps practices, the “we
ways of working build it, we own it” approach is a start that will pay dividends later. As long as
the teams are also given the autonomy to decide where to invest their time.
(not product/agile)
“To get to an optimal
Logging and error budgets are lagging indicators.
service model, you need
Teams generally lack leading indicators to help
to design the organization.
understand how they will do (e.g. SLIs), and they
Good behaviors are driven
don’t have the capability to see or report on the
by accountability. The
type of work that they are doing (i.e. unplanned
work vs new features). Because there are no enterprise architecture
Engineering teams
standard KPIs tracking what teams spend function is not there for
are the last in an
time on, it is hard to properly communicate creating documents, but
organization to how much of the team’s time is taken up by
to focus on the realization
access quantifiable monitoring and unplanned work.
of value. Each part of the
measures organization needs to be
This makes it challenging to set expectations
able to operate on its own,
for how much work the team can take on (apart
from SLAs). It also makes it hard for leaders but there are horizontal
to measure improvements as organizations things driven across all,
progress through digital transformation
and there are things that
journeys. Value stream management (VSM), and
each function has to
flow metrics would provide leading indicators
take ownership of and be
of what work is happening.
accountable for.
~Mike McGibbney
SVP SaaS, Körber
28
A NOTE ON VALUE
STREAM MANAGEMENT
Value Stream Management (VSM) has been around conceptually since the emergence of lean at Toyota in the 1950s but is
currently undergoing a renaissance thanks to advanced tooling that has emerged from the DevOps toolchain era. VSM’s goal
is the optimization of value—the flow of work, and realization of value outcomes. It puts customer experience at the center
of all that it does, and demands that teams are not negatively impacted by waste in many forms. This waste includes the
unplanned work that occurs when availability is compromised.
Flow metrics help teams understand the health of their value stream as well as the impact their work is making with their
customers. In particular, work distribution helps teams to see the proportions of time they are spending on work and adjust
investments accordingly. Additionally, monitoring tools and AIOps can provide deep insights into customer experience at
the leading indicator level (as opposed to the lagging indicators that are typically used, such as profits and revenues—
which are business metrics acting as proxy metrics for customer experience). VSM balances efficiency and effectiveness.
29
Teams
Teams Have Shifted to the “We Made
It, We Own It” DevOps Culture
30
““Centralized teams often Amazon’s CTO, Werner Vogels, famously told his engineering
provide development teams teams “you build it, you run it”. Using the pronoun “we”
with tools and guidelines but helps teams feel accountability and autonomy. And running
systems, services and platforms requires end-to-end
allow each individual team
ownership including ensuring feedback flows from t h e
to do their own thing. There
customer back into planning and decision making (not
are certain tasks to do and
simply “running” it).
measures to hit, but letting
them “run their own business”
Most of our respondents (34.9%) said that they follow this
helps to encourage people
mantra and have total control over their infrastructure
to bring their ideas to the
platform and product/service and don’t have a central
team. It’s not that the team
IT function. 29% self-serve their infrastructure from a
is “burned out” necessarily—
platform provided by a central team, and a further 28%
teams often become refer to central teams for guidance. Only 7% are provided
accustomed to the amount of their infrastructure by a central team.
load/pressure they are under—
but it’s good to minimize it so This shows a solid adoption of the team and
they can focus on delivering organizational level design patterns that research
such as the State of DevOps Report 2021 reveal
differentiating features.
correlate with success (e.g. platform teams). But this
Measuring and minimizing
raises the question, why are these teams then
unplanned work is part of that.”
consistently still struggling with meeting their SLAs?
They may have control over their infrastructure, but do
~Helen Beal, Strategic Advisor &
they have control over their work and their time?
Analyst
31
Teams
Larger Companies are Further
Behind on DevOps Adoption
Our data show that teams in larger companies have less autonomy—with only 25% of those respondents
saying they build and own their infrastructure.
32
Teams
Most DevOps Practices
Remain a “Want to Have”
We asked our respondents about their adoption of a range of DevOps capabilities and discovered that,
at the team level, less than 20% had implemented any of them. Artificial and augmented intelligence
was the top reported category (this isn’t necessarily AIOps—respondents could interpret this as AI
included anywhere in the DevOps toolchain, e.g. at the developer or testing level).
Chaos engineering was the least adopted but most planned within six months, along with trunk-based
development. Both these practices have very close ties with availability (see call outs on page 35).
While the smallest group are those already using these practices, the largest by
an overwhelming majority are those who intend to implement these practices.
Something is creating friction in the adoption process. Most likely it is all that
time teams are spending monitoring and dealing with incidents/unplanned work.
<20% of
respondents have
adopted DevOps
practices, but AI is
leading the charge
33
34
A NOTE ON
CHAOS ENGINEERING
Netflix developed Chaos Monkey in 2011 to test the resilience of its
infrastructure—and by making it available to the community, chaos
engineering was born. It’s the discipline of devising and executing
experiments on a digital product or service with the explicit intention of
learning about the system’s capability to withstand turbulent conditions
and make improvements for improved availability. It requires relatively high
levels of stability and the ability to assign time to the improvement practice.
Most teams start with experiments on their pre-production environments
and graduate to production as they build confidence.
A NOTE ON
TRUNK-BASED DEVELOPMENT
When multiple developers work on a single product or service it can cause painful merge events when their
code bases are integrated. In trunk-based development, developers’ own feature branches are short-lived
(usually less than a day) thus driving frequent and small integration events to minimize risk and continually test
quality. It’s one of the key characteristics of Continuous Integration (CI)—all developers commit at least daily
to trunk along with version control, and automated unit, integration, and user acceptance tests.
This practice aims to build-in quality early and avoid “integration hell.” Ultimately, availability is positively
impacted as issues are caught and resolved early on in the pipeline so they don’t cause incidents and unplanned
work later. CI is the practice that enables software to always be in a releasable state that enables teams to
35 practice continuous delivery. Together, continuous integration and continuous delivery are referred to as CICD.
XX
Teams
Organizational Leaders Believe Their Teams
Are at a Much Higher DevOps Adoption Rate
There is a leadership bias toward positively reporting capabilities—management believes they have far
more advanced DevOps capabilities than teams are reporting across all categories, but especially AI,
ChatOps, CICD, and VSM. As we have learned, teams are very keen to adopt these capabilities, but they
likely don’t have the time. There’s a consistent difference in perspective between the work that teams
think they are doing, and what management believes they are working on.
It’s up to the teams to make their work visible and management to enable them to do so. Then leaders
need to help their teams to discover improvements and ensure they are able to find the time to invest in