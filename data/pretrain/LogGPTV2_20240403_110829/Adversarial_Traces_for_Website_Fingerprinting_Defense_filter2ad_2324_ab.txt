### Toward the Target Sample and Overhead

To approach the target sample, we add a perturbation ∆ to the source sample, defined as \( I_{\text{new}} = I_s + \Delta \). This process is iterated, with ∆ being recomputed for each \( I_s \) and the source sample being updated until one of two conditions is met: the new sample leaves the source class, \( f(I_{\text{new}}) \neq s \), or the number of iterations exceeds the maximum allowed (set to 200 in our experiments).

### Handling Small Perturbations

We only increase the bursts where \(-\frac{\partial D(I, I_T)}{\partial b_i} > 0\). However, this can lead to scenarios where, after several iterations, \(\nabla(-D(I, I_T))\) does not have any positive values, or the positive values are so small that they do not significantly alter \( I_s \). If \( I_{\text{new}} - I_s \) is smaller than a threshold (we used 0.001) for a few iterations (we used 10 iterations) and the sample remains in the source class, we refill the pool with new samples and select a new target sample \( I_T \) to continue the process.

### Evaluations

#### Data Sets

In our evaluation, we divided the data into two non-overlapping sets: the Attacker Set and the Defender Set. Each set contains 83 classes, each representing a website of interest to the attacker, with 360 instances per class. Additionally, our dataset includes an unmonitored set of 40,000 instances from 40,000 different sites, one instance per site.

#### Bandwidth Overhead and Attacker Accuracy

We examined the bandwidth overhead and reduction in attacker accuracy for traces protected by our method. We generated defended forms of traces from the training data using the method described earlier. A detector (a CNN model) was trained on the traces in the Attacker Set to identify when the generated samples leave their source class.

We evaluated two cases:
- **Case I**: The target pool is filled with instances from the Attacker Set. The detector has been trained on the target classes.
- **Case II**: The target pool is filled with instances from the unmonitored set. The detector has not been trained on the target samples.

We varied parameters \( \alpha \) and \( p \) to evaluate their effect on the strength of the defended traces and the overhead. We measured the detectability of the defended samples by applying the DF attack [8], which suggests using 5,000 packets. Since both Walkie-Talkie and our method increase the size of the bursts, we used an input size of 10,000 packets, which is the 80th percentile of packet sequence lengths in our defended traces.

#### Results

**Figure 2** shows the bandwidth overhead for both Walkie-Talkie (WT) and our method in Case I (solid lines) and Case II (dashed lines) as \( \alpha \) and \( p \) vary. As \( \alpha \) increases, the bandwidth overhead also increases. Larger \( \alpha \) values create longer steps toward the target samples and less fine-grained searches for an effective stopping point. Using a larger target pool moderately decreases the bandwidth overhead in most cases.

Case I leads to lower bandwidth overhead compared to Case II, indicating that picking target samples from classes the detector has been trained on reduces overhead. In all evaluated settings, the bandwidth overhead of our method is lower than that of Walkie-Talkie. For \( \alpha = 1 \) and \( p = 5 \) in Case I, the bandwidth overhead of our defense is 27%, 60% lower than for Walkie-Talkie. For \( \alpha = 15 \) and \( p = 5 \) in Case I, the bandwidth overhead of our defense is 56%, 18% lower than for Walkie-Talkie.

**Figure 3** depicts the accuracy rate of the DF attack as \( \alpha \) and \( p \) vary, for input sizes of 5,000 packets and 10,000 packets. As \( \alpha \) increases, the accuracy rate drops to its minimum and then slightly increases. On the other hand, increasing \( \alpha \) raises the bandwidth overhead monotonically. Raising the target pool size \( p \) can moderately increase the attacker’s accuracy.

According to **Figure 3a**, the lowest accuracy rate is 59% when \( \alpha = 10 \) and \( p = 1 \), with a corresponding bandwidth overhead of 53%. In Case II, the lowest accuracy rate is 55% with a bandwidth overhead of 63%.

Our evaluations show that Case I provides lower bandwidth overhead than Case II (between 15% to 27% lower) and the detectability of the generated samples is comparable. This means that picking target samples from classes the detector has been trained on reduces bandwidth overhead. Our best setting is to pick target samples from the classes the detector trained on with \( p = 1 \) and \( \alpha = 5 \). Walkie-Talkie still has lower attack accuracy (49% vs. our 60%), but its bandwidth overhead is 46% higher than our defense.

### Conclusion and Future Work

In this work, we propose a new defense against WF attacks with lower bandwidth overhead than Walkie-Talkie, the state-of-the-art defense, while achieving reasonable reductions in attacker accuracy. The defense uses a novel mechanism that adapts techniques used to create adversarial examples against machine learning classifiers, applying them to website traffic traces. The generated adversarial traces can limit the adversary even if they are trained on the adversarial traces. To protect a traffic trace, we add fake packets to the source trace to shorten the distance between the source sample and a randomly selected target sample representing another website.

Our defense mechanism results in 47% bandwidth overhead and drops the accuracy rate of the state-of-the-art WF attack from 98% to 60%. We emphasize that our tests are conducted in a closed-world setting, where the attacker knows the user is visiting one of the monitored set of websites. In a more realistic open-world setting, 60% accuracy is likely to result in many false positives for the attacker. In future work, we plan to further improve the defense and show how to implement it.

### Acknowledgment

This material is based upon work supported by the National Science Foundation under Grants Numbers 1423163, 1722743, and 1816851.

### References

[1] Xiang Cai, Rishab Nithyanand, and Rob Johnson. 2014. CS-BuFLO: A Congestion Sensitive Website Fingerprinting Defense. In Workshop on Privacy in the Electronic Society (WPES).

[2] Xiang Cai, Xin Cheng Zhang, Brijesh Joshi, and Rob Johnson. 2012. Touching from a Distance: Website Fingerprinting Attacks and Defenses. In ACM Conference on Computer and Communications Security (CCS).

[3] Marc Juárez, Mohsen Imani, Mike Perry, Claudia Díaz, and Matthew Wright. 2016. Toward an Efficient Website Fingerprinting Defense. In European Symposium on Research in Computer Security (ESORICS).

[4] Xiapu Luo, Peng Zhou, EWW Chan, and Wenke Lee. 2011. HTTPOS: Sealing Information Leaks with Browser-side Obfuscation of Encrypted Flows. In Network & Distributed System Security Symposium (NDSS).

[5] Payap Sirinam, Mohsen Imani, Marc Juarez, and Matthew Wright. 2018. Deep Fingerprinting: Undermining Website Fingerprinting Defenses with Deep Learning. arXiv:arXiv:1801.02265.

[6] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR).

[7] Tao Wang and Ian Goldberg. 2017. Walkie-talkie: An efficient defense against passive website fingerprinting attacks. In USENIX Security Symposium.

[8] CV V Wright, SE E Coull, and Fabian Monrose. 2009. Traffic morphing: An efficient defense against statistical traffic analysis. In Network & Distributed System Security Symposium (NDSS).

[9] Rishab Nithyanand, Xiang Cai, and Rob Johnson. 2014. Glove: A Bespoke Website Fingerprinting Defense. In Workshop on Privacy in the Electronic Society (WPES).

[10] Mike Perry. 2011. Experimental Defense for Website Traffic Fingerprinting. Tor Project Blog. https://blog.torproject.org/blog/experimental-defense-website-traffic-fingerprinting.