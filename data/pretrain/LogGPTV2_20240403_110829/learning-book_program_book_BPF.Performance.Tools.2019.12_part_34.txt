ssvdduue
8/2od/0
[..-]
This output shows that snmp-pass is reading the /proc/cpuinfo file every second! Most of the
details in this file will not change, with the exception of the °cpu MHz* field.
Inspection of the software showed that it was reading /proc/cpuinfo merely to count the number
of processors; the “cpu MHz" field was not used at all. This is an example of unnecessary work, 
and eliminating it should provide a small but easy win.
On Intel processors, these SMP calls are ultimately implemented as x2APIC IPI (inter-processor
interrupt) calls, including x2apic_send_IPI(). These can also be instrumented, as shown in
Section 6.4.2.
6.3.16
Ilcstat
Ilcstat(8)23 is a BCC tool that uses PMCs to show last-level cache (LLC) miss rates and hit ratios by
process. PMCs are introxduced in Chapter 2.
For example, from a 48-CPU production instance:
11cstat
Running for 10 seconds or hit Ctrl-C to end.
PID
NAME
CFU
REFERENCE
HISS
HIT
sxapper/15
15
1007300
1000 99.90
4435
Java
18
22000
160*6600Z
4116
java
11000
10099,09
4441
Java
38
32200
$L0*6600≤
17387
java
17
10800
10099,07
4113
Java
1T
10500
10099.054
[. .-]
This output shows that the java processes (threads) were running with a very high hit ratio,
over 99%.
This tool works by using overflow sampling of PMCs, where one in every so many cache refer-
ences or misses triggers a BPF program to read the currently running process and record stats. The
default threshold is 100, and it can be tuned using c. This one-in-a-hundred sampling helps keep
the overhead low (and can be tuned to higher numbers, if needed); however, there are some issues
related to sampling with it. For example, a process could by chance overflow misses more often
than references, which doesn’t make sense (as misses are a subset of references).
23 0rigin: This wss created by Teng Qin on 19-0et-2016, and is the first tool in BCC to use PMCs
---
## Page 275
238
3 Chapter 6 CPUs
Command line usage:
11cstat[optLons][duzatlon]
Options include:
 -e SAMPLE_PERIOD: Sample one in this many events only
IIcstat(8) is interesting in that it was the first BCC tool to use PMCs, outside of timed sampling
6.3.17 Other Tools
Other BPF tools worth mentioning:
a sud pue uo Buuun aam sd sassaoond upm sadsues aendq wog (g)semnd 
result as a linear histogram. This provides a histogram view of CPU balance.
•cpuunclaimed(8) from BCC is an experimental tool that samples CPU run queue lengths
and determines how often there are idle CPUs yet threads in a runnable state on a different
run queue. This sometimes happens due to CPU affinity, but if it happens often, it may be a
sign of a scheduler misconfiguration or bug.
● loads(8) from bpftrace is an example of fetching the load averages from a BPF tool. As
discussed earlier, these numbers are misleading.
● vltrace is a tool in development by Intel that will be a BPF-powered version of strace(1) that
can be used for further characterization of syscalls that are consuming CPU time [79].-
6.4BPFOne-Liners
This section provides BCC and bpftrace one-liners. Where possible, the same one-liner is imple
mented using both BCC and bpftrace.
6.4.1BCC
Trace new processes with arguments:
execsnoop
Show who is executing what:
trace *t:syacalls:sys_enter_execve "=> Is*, axgs=>filenane'
Show the syscall count by process:
syscount -P
Show the syscall count by syscall name:
syscount
---
## Page 276
6.4 BPF One-Liners
239
Sample user-level stacks at 49 Hertz, for PID 189:
681 d- 0- 6b - oTT2oxd
Sample all stack traces and process names:
profile
Count kernel functions beginning with *vfs_°:
funccount 'vfs_+
Trace new threads via pthread_create():
tzace /11b/x86_64=1inux=gnu/11bpthread2.27-so:pthread_create
6.4.2 bpftrace
Trace new processes with arguments:
1(abxefilenane)): )*
Show the syscall count by program:
bpftrace -e *tracepoint:rax_syscallsisys_enter / @[comm] = count(1 *
Show the syscall count by process:
Show the syscall count by syscall probe name:
Show the syscall count by syscall function:
([aunoo -[(e+pT<=sbxe +etqetress}appex)+]us]8
Sample running process names at 9 Hertz:
1(1-[]8）662g110xd880xg
Sample user-level stacks at 49 Hertz, for PID 189:
f(1un -[xoe1sn]8）/68=pd/6zq:1oxd8-80exsdg
Sample all stack traces and process names:
opftrace -e *pxoflle:hz:49 ( e[ustack, stack, comn] = count (); 1
Sample the running CPU at 99 Hertz and show it as a linear histogram:
---
## Page 277
240
0Chapter 6 CPUs
Count kernel functions beginning with vfs_
Count SMP calls by name and kernel stack:
bpftzace -e *kprobe:ang_cal1* [ B[pzobe, katack (51] = count () ; 1*
Count Intel x2APIC calls by name and kernel stack:
opftzace -e *kprobe:x2aplc_send_IPI+ ( B[pxobe, kstack(5}] = count [) ;)
Trace new threads via pthread_create():
egeexopeeauad:osz*z-peexuadgtt/nuhxnutt-9gex/att/in. 8-eoexagdg
printf(*es by es ledl 'n*, probey conms pidl= *
6.5OptionalExercises
If not specified, these can be completed using either bpftrace or BCC:
1. Use execsnoop(8) to show the new processes for the man 1 .s command.
2. Run execsnoop(8) with t and output to a log file for 10 minutes on a production or local
system. What new processes did you find?
3. On a test system, create an overloaded CPU. This creates two CPU-bound threads that are
bound to CPU 0:
 ,eoop : op 1: etrqx, o- y6 0 o- esxsex
taskset -c 0 sh -c *while 1r do 1; done′ &
Now use uptime(1) (load averages), mpstat(1) (P ALL), runqlen(8), and runqlat(8) to
characterize the workload on CPU 0. (Remember to kill the workload when you are done.)
4. Develop a tool/one-liner to sample kernel stacks on CPU 0 only.
5. Use profile(8) to capture kernel CPU stacks to determine where CPU time is spent by the
following workload:
dd if=/dev/nvme0n1p3 bs=8k iflag=direct 1 dd of=/dev/nul1 bs=]
Modify the infile (I r=) device to be a local disk (see df h for a candidate). You can either
profile system-wide or filter for each of those d(1) processes.
6. Generate a CPU flame graph of the Exercise 5 output.
7. Use offcputime(8) to capture kernel CPU stacks to determine where blocked time is spent
for the workload of Exercise 5.
8. Generate an off-CPU time flame graph for the output of Exercise 7.
---
## Page 278
6.6Summary
241
9. execsnoop(8) only sees new processes that call exec(2) (execve(2), although some may
fork(2) or clone(2) and not exec(2) (e.g., the creation of worker processes). Write a new tool
pnoo no, aqssod se seap Aueu se qm sassaosd mau [e mougs on (s)doousosd pae)
trace fork() and clone0), or use the sched tracepoints, or do something else.
10. Develop a bpftrace version of softirqs(8) that prints the softirq name,
11. Implement cpudist(8) in bpftrace.
12. With cpudist(8) (either version), show separate histograms for voluntary and involuntary
context switches.
13. (Advanced, unsolved) Develop a tool to show a histogram of time spent by tasks in CPU
affinity wait: runnable while other CPUs are idle but not migrated due to cache warmth
(see kernel.sched_migration_cost_ns, task_hot()which may be inlined and not traceable,
and can_migrate_task(0).
6.6Summary
This chapter summarizes how CPUs are used by a system, and how to analyze them using tradi-
tional tools: statistics, profilers, and tracers. This chapter also shows how to use BPF tools to
efficiency, count function calls and show CPU usage by soft and hard interupts.
uncover issues of short-lived processes, examine run queue latency in detail, profile CPU usage
---
## Page 279
This page intentionally left blank
---
## Page 280
ter
Memory
Linux is a virtual memorybased system where each process has its own virtual address space, and
mappings to physical memory are made on demand. Its design allows for over-subscription of
physical memory, which Linux manages with a page out daemon and physical swap devices and
(as a last resort) the out-of-memory (OOM) killer. Linux uses spare memory as a file system cache,
a topic covered in Chapter 8.
This chapter shows how BPF can expose application memory usage in new ways and help you
examine how the kernel is responding to memory pressure. As CPU scalability has grown faster
than memory speeds, memory I/O has become the new bottleneck Understanding memory
usage can lead to finding many performance wins.
Learning Objectives:
■ Understand memory allocation and paging behavior
= Learn a strategy for successful analysis of memory behavior using tracers
Use traditional tools to understand memory capacity usage
Use BPF tools to identify code paths causing heap and RSS growth
Characterize page faults by filename and stack trace
 Analyze the behavior of the VM scanner
 Determine the performance impact of memory reclaim
 Identify which processes are waiting for swap-ins
 Use bpftrace one-liners to explore memory usage in custom ways
This chapter begins with some necessary background for memory analysis, with a focus on
application usage, summarizing virtual and physical allocation, and paging. Questions that BPF
can answer are explored, as well as an overall strategy to follow., Traditional memory analysis
tools are summarized first, and then BPF tools are covered, including a list of BPF one-liners. This
chapter ends with optional exercises.
Chapter 14 provides additional tools for kernel memory analysis.
---
## Page 281
244
Chapter 7 Memory
7.1
Background
This section covers men
ory fundamentals, BPF capabilities, and a suggested strategy for memory
analysis.
7.1.1
MemoryFundamentals
Memory Allocators
Figure 7-1 shows commonly used memory allocation systems for user- and kernel-level software.
a jo puausas sjureuap e to paroqs st rouau uoeooe Asouau sog oq Sussn sasaoond 1o
process's virtual address space called the heap. libc provides functions for memory allocation, 
including malloc( and free(). When memory is freed, libc tracks its location and can use that
location information to fulfill a subsequent malloc(. libc needs to extend the size of the heap
Kaotuau e3rsitd ear pou °zoumau fena [e s stq se deaq au
only when there is no available memory. There is usually no reason for libc to shrink the size of
The kernel and processor are responsible for mapping virtual memory to physical memory. For
efficiency, memory mappings are created in groups of memory called pages, where the size of each
Ia8se suoddns osle ssossaooad sou q8nou[e 'uouruoo sg sa4sqy mog gesap sossaoad e s a8ed
sizeswhat Linux terms muge pages. The kernel can service physical memory page requests from
its own free lists, which it maintains for each DRAM group and CPU for efficiency The kernel’s
own software also consumes memory from these free lists as well, usually via a kernel allocator
such as the slab allocator.
User-Level
Kernel
Slab
Page
Modules
Allocator
Allocator
DRAM
ext4
Caches
Free Lists
scs1]
DRAM
Process
libc
Allocator
DRAM
Memory
CO
DRAM
Virtual
Physical
Figure 7-1. Memory allocators
Other user allocation libraries include tcmalloc and jemalloc, and runtimes such as the IVM often
provide their own allocator along with garbage collection. Other allocators may also map private
segments for allocation outside of the heap.
---
## Page 282
7.1Background
245
Virtual Memory
Physical Memory
Application
Heap
load / store
口
Allocator
(libc)
brk0
Mappings
Process Address Space
Swap Devices
Figure 7-2 Memory page life cycle
Memory Pages and Swapping
The life cycle of a typical user memory page is shown in Figure 7-2, with the following steps
enumerated:
1. The application begins with an allocation request for memory (e.g., libc malloc0).
2. The allocation library can either service the memory request from its own free lists, or
it may need to expand virtual memory to accommodate. Depending on the allocation
library, it will either:
a. Extend the size of the heap by calling a brk( syscall and using the heap memory for the
allocation.
b. Create a new memory segment via the mmap( syscall.
3. Sometime later, the application tries to use the allocated memory range through store and
load instructions, which involves calling in to the processor memory management unit
(MMU) for virtual-to-physical addresstranslation. At this point, the lie of virtual memory
is revealed: There is no mapping for this address! This causes an MMU error called a page
fault.
4. The page fault is handled by the kernel, which establishes a mapping from its physical
memory free lists to virtual memory and then informs the MMU of this mapping for later
lookups. The process is now consuming an extra page of physical memory. The amount of
physical memory in use by the process is called its resident set size (RSS).
5. When there is too much memory demand on the system, the kernel page-out daemon
(kswapd) may look for memory pages to free. It will fre one of three types of memory
(though only (c) is pictured in Figure 7-2, as it is showing a user memory page life cycle):
a. File system pages that were read from disk and not modified (termed “backed by disk*):
These can be freed immediately and simply reread back when needed. These pages are
application-executable text, data, and file system metadata.
---
## Page 283
246
6Chapter 7 Memory
b. File system pages that have been modified: These are *dirty” and must be written to disk
before they can be freed.
c. Pages of application memory: These are called anonymous memory because they have
no file origin. If swap devices are in use, these can be freed by first being stored on a
swap device. This writing of pages to a swap device is termed swupping (on Linux).
Memory allocation requests are typically frequent activities: User-level allocations can occur
millions of times per second for a busy application. Load and store instructions and MMU
lookups are even more frequent; they can occur billions of times per second. In Figure 7-2, these
arrows are drawn in bold. Other activities are relatively infrequent: brk( and mmap0 calls,
page faults, and page-outs (lighter arrows).
Page-Out Daemon
The page-out daemon (kswapd) is activated periodically to scan LRU lists of inactive and active
pages in search of memory to free. It is woken up when free memory crosses a low threshold and
goes back to sleep when it crosses a high threshold, as shown in Figure 7-3.
background
high pages
Available
Memory
low pages
min pages
punojfauoj
Time
Figure 7-3 kswapd wakeups and modes
kswapd coordinates background page-outs; apart from CPU and disk I/O contention, these should
not directly harm application performance. If kswapd cannot free memory quickly enough, a
tunable minimum pages threshold is crossedl, and direct reclaim is used; this is a foreground mode
of freeing memory to satisfy allocations. In this mode, allocations block (stall) and synchronously
wait for pages to be freed [Gorman 04] [81].
Direct reclaim can call kernel module shrinker functions: These free up memory that may have
been kept in caches, including the kernel slab caches.
Swap Devices
Swap devices provide a degraded mode of operation for a system running out of memory:
Processes can continue to allocate, but less frequently used pages are now moved to and from
their swap devices, which usually causes applications to run much more slowly. Some production
---
## Page 284
7.1  Background
247
qdaooe rasau s uoperado jo apotu papersap at seq st aeuope aq :dems pnoqm unu suaqsis
able for those critical systems, which may have numerous redundant (and healthy!) servers that
would be much better to use than one that has begun swapping. (This is usually the case for
Netflix cloud instances, for example.) If a swap-less system runs out of memory, the kernel out-of-
memory killer sacrifices a process. Applications are configured to never exceed the memory limits
of the system, to avoid this.
OOM Killer
The Linux out-of-memory killer is a last resort to free up memory: It willfind victim processes
using a heuristic, and sacrifice them by killing them. The heuristic looks for the largest victim
that will free many pages, and that isn’t a critical task such as kernel threads or init (PID 1). Linux
provides ways to tune the behavior of the OOM killer system-wide and per-process.
Page Compaction
Over time, the freed pages become fragmented, making it difficult for the kernel to allocate a large
contiguous chunk, if needed. The kernel uses a compaction routine to move pages, freeing up
contiguous regions [81].
File System Caching and Buffering
Linux borrows free memory for file system caching and returns it to the fre status when there is
demand. A consequence of such borrowing is that the free memory reported by the system rushes
toward zero after Linux boots, which may cause a user to worry that the system is running out of
memory for write-back buffering.
memory when actually it's just warming up its file system cache. In addition, the file system uses
Linux can be tuned to prefer freeing from the file system cache or freeing memory via swapping
(vm.swappiness).
Caching and buffering are discussed further in Chapter 8.
Further Reading
This is a brief summary to arm you with essential knowledge before using the tools. Additional
topics, including kernel page allocation and NUMA, are covered in Chapter 14. Memory alloca-
tion and paging are covered in much more depth in Chapter 7 of Systems Performance [Gregg 13b].
7.1.2 BPF Capabilities
u Aatq 'apdurexa o seusaus Asouu rog nu8tsus auos aptsosd sjoop aoueuouad qeuope
show breakdowns of virtual and physical memory usage and the rates of page operations. These
traditional tools are summarized in the next section.
BPF tracing tools can provide additional insight for memory activity, answering:
Suror8 daog (Ssa) A1oa [eors.std saoond ats saop AuM *
 What code paths are causing page faults? For which files?