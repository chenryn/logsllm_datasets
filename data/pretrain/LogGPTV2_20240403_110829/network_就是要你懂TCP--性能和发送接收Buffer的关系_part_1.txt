# 前言
本文希望解析清楚，当我们在代码中写下 socket.setSendBufferSize 和 sysctl 看到的rmem/wmem系统参数以及最终我们在TCP常常谈到的接收发送窗口的关系，以及他们怎样影响TCP传输的性能，同时如何通过图形来展示哪里是传输瓶颈。
拥塞窗口相关文章比较多，他们跟带宽紧密相关，所以大家比较好判断，反而是接收、发送窗口一旦出现瓶颈，就没这么好判断了。
先明确一下：**文章标题中所说的Buffer指的是sysctl中的 rmem或者wmem，如果是代码中指定的话对应着SO_SNDBUF或者SO_RCVBUF，从TCP的概念来看对应着发送窗口或者接收窗口**
最后补充各种场景下的传输案例，一站式将影响传输速度的各种原因都拿下，值得收藏。
本文主要分析rt、buffer如何影响TCP的传输性能，更多其他因素影响TCP性能的案例见：[TCP传输速度案例分析](/2021/01/15/TCP%E4%BC%A0%E8%BE%93%E9%80%9F%E5%BA%A6%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/)
# TCP性能和发送接收Buffer的关系
先从碰到的一个实际问题看起：
> 应用通过专线跨网络访问云上的服务，专线100M，时延20ms，一个SQL查询了22M数据，结果花了大概25秒，这太慢了，不正常。
> 
> 如果通过云上client访问云上服务那么1-2秒就返回了（不跨网络服务是正常的，说明服务本身没有问题）。
> 
> 如果通过http或者scp从云下向云上传输这22M的数据大概两秒钟也传送完毕了（说明网络带宽不是瓶颈），
> 
> 所以这里问题的原因基本上是我们的服务在这种网络条件下有性能问题，需要找出为什么。
## 抓包分析 tcpdump+wireshark
抓包分析这22M的数据传输，如下图（wireshark 时序图），横轴是时间，纵轴是sequence number：
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/d188530df31712e8-d188530df31712e8341f5687a960743a.png)
粗一看没啥问题，因为时间太长掩盖了问题。把这个图形放大，只看中间50ms内的传输情况（横轴是时间，纵轴是sequence number，一个点代表一个包）
可以看到传输过程总有一个20ms的等待平台，这20ms没有发送任何包，换个角度，看看窗口尺寸图形：
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/7ae26e844629258d-7ae26e844629258de173a05d5ad595f9.png)
从bytes in flight也大致能算出来总的传输速度 16K*1000/20=800Kb/秒
我们的应用代码中会默认设置 socketSendBuffer 为16K:
> socket.setSendBufferSize(16*1024) //16K send buffer
## 原理解析
如果tcp发送buffer也就是SO_SNDBUF只有16K的话，这些包很快都发出去了，但是这16K的buffer不能立即释放出来填新的内容进去，因为tcp要保证可靠，万一中间丢包了呢。只有等到这16K中的某些包ack了，才会填充一些新包进来然后继续发出去。由于这里rt基本是20ms，也就是16K发送完毕后，等了20ms才收到一些ack，这20ms应用、内核什么都不能做，所以就是如前面第二个图中的大概20ms的等待平台。这块请参考[这篇文章](https://www.atatech.org/articles/79660)
比如下图，wmem大小是8，发出1-8后，buffer不能释放，等到收到ack1-4后，释放1-4，buffer也就是释放了一半，这一半可以填充新的发送数据进来了。 上面的问题在于ack花了很久，导致buffer一直不能释放。
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/3d9e77f8c9b0cab1-3d9e77f8c9b0cab1484c870d2c0d2473.png)
**sendbuffer相当于发送仓库的大小，仓库的货物都发走后，不能立即腾出来发新的货物，而是要等对方确认收到了(ack)才能腾出来发新的货物。 传输速度取决于发送仓库（sendbuffer）、接收仓库（recvbuffer）、路宽（带宽）的大小，如果发送仓库（sendbuffer）足够大了之后接下来的瓶颈就会是高速公路了（带宽、拥塞窗口）。而实际上这个案例中带宽够、接收仓库也够，但是发送仓库太小了，导致发送过程断断续续，所以非常慢。**
如果是UDP，就没有可靠的概念，有数据统统发出去，根本不关心对方是否收到，也就不需要ack和这个发送buffer了。
## 几个发送buffer相关的内核参数
```
$sudo sysctl -a | egrep "rmem|wmem|tcp_mem|adv_win|moderate"
net.core.rmem_default = 212992
net.core.rmem_max = 212992
net.core.wmem_default = 212992 //core是给所有的协议使用的,
net.core.wmem_max = 212992
net.ipv4.tcp_adv_win_scale = 1 //
net.ipv4.tcp_moderate_rcvbuf = 1
net.ipv4.tcp_rmem = 4096	87380	6291456  //最小值  默认值  最大值
net.ipv4.tcp_wmem = 4096	16384	4194304 //tcp这种就自己的专用选项就不用 core 里面的值了
net.ipv4.udp_rmem_min = 4096
net.ipv4.udp_wmem_min = 4096
vm.lowmem_reserve_ratio = 256	256	32
net.ipv4.tcp_mem = 88560        118080  177120
vm.lowmem_reserve_ratio = 256   256     32
```
net.ipv4.tcp_wmem 默认就是16K，而且内核是能够动态调整的，只不过我们代码中这块的参数是很多年前从 Cobar 中继承过来的，初始指定了sendbuffer的大小。代码中设置了这个参数后就关闭了内核的动态调整功能，这就是为什么http或者scp都很快，因为他们的send buffer是动态调整的。
接收buffer是有开关可以动态控制的，发送buffer没有开关默认就是开启，关闭只能在代码层面来控制
> net.ipv4.tcp_moderate_rcvbuf
## 解决方案
调整 socketSendBuffer 到256K，查询时间从25秒下降到了4秒多，但是比理论带宽所需要的时间略高
继续查看系统 net.core.wmem_max 参数默认最大是130K，所以即使我们代码中设置256K实际使用的也是130K，继续调大这个系统参数后整个网络传输时间大概2秒(跟100M带宽匹配了，scp传输22M数据也要2秒），整体查询时间2.8秒。测试用的mysql client短连接，如果代码中的是长连接的话会块300-400ms（消掉了握手和慢启动阶段），这基本上是理论上最快速度了
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/3dcfd469fe1e2f7e-3dcfd469fe1e2f7e1d938a5289b83826.png)
如果调用setsockopt()设置了socket选项SO_SNDBUF，将关闭发送端缓冲的自动调节机制，tcp_wmem将被忽略，SO_SNDBUF的最大值由net.core.wmem_max限制。
## 这个案例关于wmem的结论
默认情况下Linux系统会自动调整这个buffer（net.ipv4.tcp_wmem）, 也就是不推荐程序中主动去设置SO_SNDBUF，除非明确知道设置的值是最优的。
从这里我们可以看到，有些理论知识点虽然我们知道，但是在实践中很难联系起来，也就是常说的无法学以致用，最开始看到抓包结果的时候比较怀疑发送、接收窗口之类的，没有直接想到send buffer上，理论跟实践没联系上。
## BDP(
[Bandwidth-Delay Product](https://hpbn.co/building-blocks-of-tcp/#bandwidth-delay-product)
) 带宽时延积
BDP=rtt*(带宽/8)
这个 buffer 调到1M测试没有帮助，从理论计算BDP（带宽时延积） 0.02秒*(100MB/8)=250Kb  所以 ***SO_SNDBUF为256Kb的时候基本能跑满带宽了，再大也没有什么实际意义了** 。也就是前面所说的仓库足够后瓶颈在带宽上了。
因为这里根据带宽、rtt计算得到的BDP是250K，BDP跑满后拥塞窗口（带宽、接收窗口和rt决定的）即将成为新的瓶颈，所以调大buffer没意义了。
> Bandwidth-delay product (BDP)
> 
> Product of data link’s capacity and its end-to-end delay. The result is the maximum amount of unacknowledged data that can be in flight at any point in time.
![Figure 2-7. Transmission gaps due to low congestion window size](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/b08fb4ce2162927b-b08fb4ce2162927bf9b6ce02cdc64ab0.svg)
## 接下来看看接收buffer(rmem)和接收窗口的关系
用这样一个案例下来验证接收窗口的作用：
> 有一个batch insert语句，整个一次要插入5532条记录，所有记录大小总共是376K，也就是这个sql语句本身是376K。
## SO_RCVBUF很小的时候并且rtt很大对性能的影响
如果rtt是40ms，总共需要5-6秒钟：
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/4af4765c045e9eed-4af4765c045e9eed2e36d9760d4a2aba.png)
基本可以看到server一旦空出来点窗口，client马上就发送数据，由于这点窗口太小，rtt是40ms，也就是一个rtt才能传3456字节的数据，整个带宽才用到80-90K，完全没跑满。
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/1984258c03009217-1984258c0300921799476777f5f0a38a.png)
比较明显间隔 40ms 一个等待台阶，台阶之间两个包大概3K数据，总的传输效率如下：
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/5ec50ecf25444e96-5ec50ecf25444e96d81fab975b5a79e6.png)
**斜线越陡表示速度越快，从上图看整体SQL上传花了5.5秒，执行0.5秒。**
此时对应的窗口尺寸：
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/05d6357ed53c1c16-05d6357ed53c1c16f0dd0454251916ef.png)
窗口由最开始28K(20个1448）很快降到了不到4K的样子，然后基本游走在即将满的边缘，虽然读取慢，幸好rtt也大，导致最终也没有满。（这个是3.1的Linux，应用SO_RCVBUF设置的是8K，用一半来做接收窗口）
## SO_RCVBUF很小的时候并且rtt很小对性能的影响
如果同样的语句在 rtt 是0.1ms的话
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/67f280a1cf499ae3-67f280a1cf499ae388fc44d6418869a7.png)
虽然明显看到接收窗口经常跑满，但是因为rtt很小，一旦窗口空出来很快就通知到对方了，所以整个过小的接收窗口也没怎么影响到整体性能
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/15b7d6852e44fc17-15b7d6852e44fc179d60d76f322695c7.png)
如上图11.4秒整个SQL开始，到11.41秒SQL上传完毕，11.89秒执行完毕（执行花了0.5秒），上传只花了0.01秒
接收窗口情况：
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/0f3050cd98db40a3-0f3050cd98db40a352410a11a521e8b2.png)
如图，接收窗口由最开始的28K降下来，然后一直在5880和满了之间跳动
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/0db5c3684a931490-0db5c3684a9314907f9158ac15b6ac71.png)
从这里可以得出结论，接收窗口的大小对性能的影响，rtt越大影响越明显，当然这里还需要应用程序配合，如果应用程序一直不读走数据即使接收窗口再大也会堆满的。
## SO_RCVBUF和tcp window full的坏case
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/55cf9875d24d76a0-55cf9875d24d76a077c442327d54fa34.png)
上图中红色平台部分，停顿了大概6秒钟没有发任何有内容的数据包，这6秒钟具体在做什么如下图所示，可以看到这个时候接收方的TCP Window Full，同时也能看到接收方（3306端口）的TCP Window Size是8192（8K），发送方（27545端口）是20480.
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/da48878ce0c01bcd-da48878ce0c01bcdedb1e6d6a6cc6d1c.png)
这个状况跟前面描述的recv buffer太小不一样，8K是很小，但是因为rtt也很小，所以server总是能很快就ack收到了，接收窗口也一直不容易达到full状态，但是一旦接收窗口达到了full状态，居然需要惊人的6秒钟才能恢复，这等待的时间有点太长了。这里应该是应用读取数据太慢导致了耗时6秒才恢复，所以最终这个请求执行会非常非常慢（时间主要耗在了上传SQL而不是执行SQL）.
实际原因不知道，从读取TCP数据的逻辑来看这里没有明显的block，可能的原因：
-   request的SQL太大，Server（3306端口上的服务）从TCP读取SQL需要放到一块分配好的内存，内存不够的时候需要扩容，扩容有可能触发fgc，从图形来看，第一次满就卡顿了，而且每次满都卡顿，不像是这个原因
-   request请求一次发过来的是多个SQL，应用读取SQL后，将SQL分成多个，然后先执行第一个，第一个执行完后返回response，再读取第二个。图形中卡顿前没有response返回，所以也不是这个原因
-   ……其它未知原因
## 接收方不读取数据导致的接收窗口满同时有丢包发生
服务端返回数据到client端，TCP协议栈ack这些包，但是应用层没读走包，这个时候 SO_RCVBUF 堆积满，client的TCP协议栈发送 ZeroWindow 标志给服务端。也就是接收端的 buffer 堆满了（但是服务端这个时候看到的bytes in fly是0，因为都ack了），这时服务端不能继续发数据，要等 ZeroWindow 恢复。
那么接收端上层应用不读走包可能的原因：
-   应用代码卡顿、GC等等
-   应用代码逻辑上在做其它事情（比如Server将SQL分片到多个DB上，Server先读取第一个分片，如果第一个分片数据很大很大，处理也慢，那么即使第二个分片数据都返回到了TCP 的recv buffer，应用也没去读取其它分片的结果集，直到第一个分片读取完毕。如果SQL带排序，那么Server会轮询读取多个分片，造成这种卡顿的概率小了很多）
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/49e2635a7c4025d4-49e2635a7c4025d44b915a1f17dd272a.png)
上图这个流因为应用层不读取TCP数据，导致TCP接收Buffer满，进而接收窗口为0，server端不能再发送数据而卡住，但是ZeroWindow的探测包，client都有正常回复，所以1903秒之后接收方窗口不为0后（window update）传输恢复。
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/就是要你懂TCP--性能和发送接收Buffer的关系/2e493d8dc32bb63f-2e493d8dc32bb63f2126375de6675351.png)
这个截图和前一个类似，是在Server上(3003端口)抓到的包，不同的是接收窗口为0后，server端多次探测（Server上抓包能看到），但是client端没有回复 ZeroWindow（也有可能是回复了，但是中间环节把ack包丢了,或者这个探测包client没收到），造成server端认为client死了、不可达之类，进而反复重传，重传超过15次之后，server端认为这个连接死了，粗暴单方面断开（没有reset和fin,因为没必要，server认为网络连通性出了问题）。
等到1800秒后，client的接收窗口恢复了，发个window update给server，这个时候server认为这个连接已经断开了，只能回复reset