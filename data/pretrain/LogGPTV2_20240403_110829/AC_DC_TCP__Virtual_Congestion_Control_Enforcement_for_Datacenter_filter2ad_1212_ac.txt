features (i.e., TSO and GRO/LRO) in the networking stack.
Brieﬂy, NIC ofﬂoads allow the host to pass large data seg-
ments along the TCP/IP stack and only deal with MTU-sized
packets in the NIC. Thus, AC(cid:69)DC operates on a segment,
rather than a per-packet, basis. Second, congestion control
is a relatively simple algorithm, and thus the computational
burden is not high. Finally, while AC(cid:69)DC is implemented in
software, it may be possible to further reduce the overhead
with a NIC implementation. Today, "smart-NICs" imple-
ment OVS-ofﬂoad functionality [40, 47], naturally provid-
ing a mechanism to reduce overhead and support hypervisor
bypass (e.g., SR-IOV).
5. RESULTS
This section quantiﬁes the effects of AC(cid:69)DC and deter-
mines if the performance of DCTCP implemented in the
vSwitch (i.e., AC(cid:69)DC) is equivalent to the performance of
DCTCP implemented in the host TCP stack.
Testbed The experiments are conducted on a physical testbed
with 17 IBM System x3620 M3 servers (6-core Intel Xeon
2.53GHz CPUs, 60GB memory) and Mellanox ConnectX-2
EN 10GbE NICs. Our switches are IBM G8264, each with
a buffer of 9MB shared by forty-eight 10G ports.
System settings We run Linux kernel 3.18.0 which imple-
ments DCTCP as a pluggable module. We set RTOmin to 10
(a) MTU = 1.5KB.
(b) MTU = 9KB.
Figure 6: Using RWND can effectively control throughput.
ﬂexibility and control by assigning ﬂows to speciﬁc conges-
tion control algorithms based on policy. For example, ﬂows
destined to the WAN may be assigned CUBIC and ﬂows des-
tined within the datacenter may be set to DCTCP.
Administrators can also enable per-ﬂow bandwidth allo-
cation schemes. A simple scheme enforces an upper-bound
on a ﬂow’s bandwidth. Traditionally, an upper-bound on
a ﬂow’s CWND can be speciﬁed by snd_cwnd_clamp in
Linux. AC(cid:69)DC can provide similar functionality by bound-
ing RWND. Figure 6 shows the behavior is equivalent. This
graph can also be used to convert a desired upper-bound on
bandwidth into an appropriate maximum RWND (the graph is
created on an uncongested link to provide a lower bound on
RTT).
In a similar fashion, administrators can assign different
bandwidth priorities to ﬂows by altering the congestion con-
trol algorithm. Providing differentiated services via conges-
tion control has been studied [58, 68]. Such schemes are use-
ful because networks typically contain only a limited num-
ber of service classes and bandwidth may need to be allo-
cated on a ﬁner-granularity. We propose a unique priority-
based congestion control algorithm for AC(cid:69)DC. Speciﬁcally,
DCTCP’s congestion control algorithm is modiﬁed to incor-
porate a priority, β ∈ [0, 1]:
rwnd = rwnd(1 − (α − αβ
2
))
(1)
Higher values of β give higher priority. When β = 1, Equa-
tion 1 simply converts to DCTCP congestion control. When
β = 0, ﬂows aggressively back-off (RWND is bounded by 1
MSS to avoid starvation). This equation alters multiplica-
tive decrease instead of additive increase because increas-
ing RWND cannot guarantee the VM ﬂow’s CWND will allow
the ﬂow to increase its sending rate.
IMPLEMENTATION
4.
This section outlines relevant implementation details. We
implemented AC(cid:69)DC in Open vSwitch (OVS) v2.3.2 [50]
and added about 1200 lines of code (many are debug/com-
ments). A high-level overview follows. A hash table is
added to OVS, and ﬂows are hashed on a 5-tuple (IP ad-
dresses, ports and VLAN) to obtain a ﬂow’s state. The ﬂow
entry state is 320 bytes and is used to maintain the conges-
tion control state mentioned in §3. SYN packets are used to
create ﬂow entries, and FIN packets, coupled with a course-
grained garbage collector, are used to remove ﬂow entries.
 0 1 2 3 4 5 6 7 8 9 10 0 50 100 150 200 250Throughput (Gbps)Max CWND (pkts) or RWND (MSS)CWNDRWND 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 10 12 14 16Throughput (Gbps)Max CWND (pkts) or RWND (MSS)CWNDRWND(a) Dumbbell topology.
Figure 8: RTT of schemes on dumbbell topology.
(b) Multi-hop, multi-bottleneck (parking lot) topology.
Figure 7: Experiment topologies.
ms [36, 67] and set tcp_no_metrics_save, tcp_sack
and tcp_low_latency to 1. Results are obtained with
MTU sizes of 1.5KB and 9KB, as networks typically use one
of these settings. Due to space constraints, a subset of the re-
sults are presented and unless otherwise noted, the MTU is
set to 9KB.
Experiment details To understand AC(cid:69)DC performance,
three different congestion control conﬁgurations are consid-
ered. The baseline scheme, referred to as CUBIC, conﬁg-
ures the host TCP stack as CUBIC (Linux’s default conges-
tion control), which runs on top of an unmodiﬁed version of
OVS. Our goal is to be similar to DCTCP, which conﬁgures
the host TCP stack as DCTCP and runs on top of an un-
modiﬁed version of OVS. Our scheme, AC(cid:69)DC, conﬁgures
the host TCP stack as CUBIC (unless otherwise stated) and
implements DCTCP congestion control in OVS. In DCTCP
and AC(cid:69)DC, WRED/ECN is conﬁgured on the switches. In
CUBIC, WRED/ECN is not conﬁgured.
The metrics used are: TCP RTT (measured by sockperf [48]),
TCP throughput (measured by iperf), loss rate (by collecting
switch counters) and Jain’s fairness index [32]. In §5.2, ﬂow
completion time (FCT) [19] is used to quantify application
performance. All benchmark tools are run in a container on
each server, rather than in a VM.
5.1 Microbenchmarks
We ﬁrst evaluate AC(cid:69)DC’s performance using a set of
microbenchmarks. The microbenchmarks are conducted on
topologies shown in Figure 7.
Canonical topologies We aim to understand the perfor-
mance of our scheme on two simple topologies. First, one
long-lived ﬂow is started per server pair (si to ri) in Figure 7a.
The average per-ﬂow throughput of AC(cid:69)DC, DCTCP and
CUBIC are all 1.98Gbps. Figure 8 is a CDF of the RTT
from the same test. Here, increases in RTT are caused by
(a) First 100 ms of a ﬂow.
(b) Moving average.
Figure 9: AC(cid:69)DC’s RWND tracks DCTCP’s CWND (1.5KB
MTU).
(a) Starting from 0 sec.
(b) Starting from 2 sec.
Figure 10: Who limits TCP throughput when AC(cid:69)DC is run
with CUBIC? (1.5 KB MTU)
queueing delay in the switch. AC(cid:69)DC achieves comparable
RTT with DCTCP and signiﬁcantly outperforms CUBIC.
Second, each sender in Figure 7b starts a long-lived ﬂow
to the receiver. Each ﬂow traverses a different number of
bottleneck links. CUBIC has an average per-ﬂow through-
put of 2.48Gbps with a Jain’s fairness index of 0.94, and
both DCTCP and AC(cid:69)DC obtain an average throughput of
2.45Gbps with a fairness index of 0.99. The 50th and 99.9th
percentile RTT for AC(cid:69)DC (DCTCP, CUBIC) are 124µs
(136µs, 3.3ms) and 279µs (301µs, 3.9ms), respectively.
Tracking window sizes Next, we aim to understand how
accurately AC(cid:69)DC tracks DCTCP’s performance at a ﬁner
level. The host’s TCP stack is set to DCTCP and our scheme
runs in the vSwitch. We repeat the experiment in Figure 7a
and measure the RWND calculated by AC(cid:69)DC. Instead of
over-writing the RWND value in the ACKs, we simply log the
value to a ﬁle. Thus, congestion is enforced by DCTCP and
we can capture DCTCP’s CWND by using tcpprobe [65].
We align the RWND and CWND values by timestamps and se-
quence numbers and show a timeseries in Figure 9. Fig-
ure 9a shows both windows for the ﬁrst 100 ms of a ﬂow
and shows that AC(cid:69)DC’s calculated window closely tracks
sendersreceiverss1s2s3s4s5r1r2r3r4r5          Receiver Senders  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 1 2 3 4 5 6 7 8 9 10CDFTCP Round Trip Time (milliseconds)CUBIC (Default)DCTCPAC/DC 0 5 10 15 20 25 30 0 0.02 0.04 0.06 0.08 0.1C/RWND (MSS)SecondsDCTCPAC/DC 0 5 10 15 20 25 30 0 1 2 3 4 5C/RWND (MSS)SecondsDCTCPAC/DC 0 5 10 15 20 25 30 0 0.02 0.04 0.06 0.08 0.1C/RWND (MSS)SecondsCUBICAC/DC 0 5 10 15 20 25 30 2 2.02 2.04 2.06 2.08 2.1C/RWND (MSS)SecondsCUBICAC/DCFigure 11: CPU overhead: sender side (1.5KB MTU).
Figure 13: AC(cid:69)DC provides differentiated throughput via
QoS-based CC. β values are deﬁned on a 4-point scale.
baseline (i.e., just OVS) is shown for the sender in Figure 11
and the receiver in Figure 12. Error bars show standard de-
viation over 50 runs. While AC(cid:69)DC increases CPU usage
in all cases, the increase is negligible. The largest difference
is less than one percentage point: the baseline and AC(cid:69)DC
have 16.27% and 17.12% utilization, respectively for 10K
ﬂows at the receiver. The results are shown with 1.5KB
MTU because smaller packets incur higher overhead. Note
experiments with 9KB MTU have similar trends.
AC(cid:69)DC ﬂexibility AC(cid:69)DC aims to provide a degree of
control and ﬂexibility over tenant TCP stacks. We consider
two cases. First, AC(cid:69)DC should work effectively, regard-
less of the tenant TCP stack. Table 1 shows the performance
of our scheme when various TCP congestion control algo-
rithms are conﬁgured on the host. Data is collected over
10 runs lasting 20 seconds each on the dumbbell topology
(Figure 7a). The ﬁrst two rows of the table, CUBIC* and
DCTCP*, show the performance of each stack with an un-
modiﬁed OVS. The next six rows show the performance of a
given host stack with AC(cid:69)DC running DCTCP in OVS. The
table shows AC(cid:69)DC can effectively track the performance
of DCTCP*, meaning it is compatible with popular delay-
based (Vegas) and loss-based (Reno, CUBIC, etc) stacks.
Second, AC(cid:69)DC enables an administrator to assign differ-
ent congestion control algorithms on a per-ﬂow basis. For
example, AC(cid:69)DC can provide the ﬂexibility to implement
QoS through differentiated congestion control. We ﬁx the
host TCP stack to CUBIC and alter AC(cid:69)DC’s congestion
control for each ﬂow by setting the β value (in Equation 1)
for each ﬂow in the dumbbell topology. Figure 13 shows
the throughput achieved by each ﬂow, along with its β set-
ting. AC(cid:69)DC is able to provide relative bandwidth alloca-
tion to each ﬂow based on β. Flows with the same β value
get similar throughputs and ﬂows with higher β values ob-
tain higher throughput. The latencies (not shown) remain
consistent with previous results.
Fairness Three different experiments are used to demon-
strate fairness. First, we show AC(cid:69)DC can mimic DCTCP’s
behavior in converging to fair throughputs. We repeat the ex-
periment originally performed by Alizadeh [3] and Judd [36]
by adding a new ﬂow every 30 seconds on a bottleneck link
Figure 12: CPU overhead: receiver side (1.5KB MTU).
DCTCP’s. Figure 9b shows the windows over a 100ms mov-
ing average are also similar. This suggests it is possible to
accurately recreate congestion control in the vSwitch. These
results are obtained with 1.5KB MTU. Trends for 9KB MTU
are similar but the window sizes are smaller.
We were also interested to see how often AC(cid:69)DC’s con-
gestion window takes effect. We rerun the experiment (MTU
is still 1.5KB), but set the host TCP stack to CUBIC. The RWND
computed by AC(cid:69)DC is both written into the ACK and logged
to a ﬁle. We again use tcpprobe to measure CUBIC’s CWND.
Figure 10 is a timeseries (one graph from the start of the ex-
periment and one graph 2 seconds in) that shows AC(cid:69)DC’s
congestion control algorithm is indeed the limiting factor. In
the absence of loss or ECN markings, traditional TCP stacks
do not severely reduce CWND and thus AC(cid:69)DC’s RWND be-
comes the main enforcer of a ﬂow’s congestion control. Be-
cause DCTCP is effective at reducing loss and AC(cid:69)DC hides
ECN feedback from the host TCP stack, AC(cid:69)DC’s enforce-
ment is applied often.
CPU overhead We measure the CPU overhead of AC(cid:69)DC
by connecting two servers to a single switch. Multiple si-
multaneous TCP ﬂows are started from one server to the
other and the total CPU utilization is measured on the sender
and receiver using sar. Each ﬂow is given time to perform
the TCP handshake and when all are connected, each TCP
client sends with a demand of 10 Mbps by sending 128KB
bursts every 100 milliseconds (so 1,000 connections satu-
rate the 10 Gbps link). The system-wide CPU overhead
of AC(cid:69)DC compared to the system-wide CPU overhead of
 0 5 10 15 20 25 30 35 40 45 50 55 601005001K5K10KCPU Usage (%)Number of concurrent TCP connectionsBaselineAC/DC 0 5 10 15 20 25 301005001K5K10KCPU Usage (%)Number of concurrent TCP connectionsBaselineAC/DC 0 2 4 6 8 10[2,2,2,2,2]/4[2,2,1,1,1]/4[2,2,2,1,1]/4[3,2,2,1,1]/4[3,3,2,2,1]/4[4,4,4,0,0]/4Tput (Gbps)Experiments (with diﬀerent β combinations)F1F2F3F4F5CC Variants
50th percentile RTT (µs)
mtu=1.5KB mtu=9KB mtu=1.5KB mtu=9KB mtu=1.5KB mtu=9KB mtu=1.5KB mtu=9KB
99th percentile RTT (µs)
Avg Tput (Gbps)
Fairness Index
CUBIC*
DCTCP*
CUBIC
Reno
DCTCP
Illinois
HighSpeed
Vegas
3232
128
128
120
129
134
119
126
3448
142
142
149
149
152
147
143
3641
232
231
235
232
215
224
216
3865
259
252
248
266
262