title:Effect of Security Controls on Patching Window: A Causal Inference
based Approach
author:Aditya Kuppa and
Lamine M. Aouad and
Nhien-An Le-Khac
Effect of Security Controls on 
Patching Window: A Causal 
Inference based Approach
Aditya Kuppa , Lamine Aouad and NhienAn LeKhac
Annual Computer Security Applications Conference (ACSAC) 
2020
Motivation
â€¢ Can security controls help in addressing 
residual risk ? 
â€¢ How are defenders using security 
controls to manage their residual 
risk.
â€¢ Do the characteris;cs of security 
controls inï¬‚uence the patching ;me 
windows?
â€¢ How this study can help defenders? 
â€¢ Re-calibrate and manage the residual risk for a 
given threat or class of threats given the control 
is already installed.
â€¢ Eï¬€ec?ve priori?za?on of patching of a given 
CVE based on the security control eï¬€ec?veness.
â€¢ Pivot the security investments into people, 
process and controls depending on the risk 
appe?te
â€¢ Convey risk decisions in a data-driven fashion to 
stakeholders and decision-makers
â€¢ Improve and manage the controls that are 
already in place. 
Impact
Classifier Centric 
View 
â€¢ The features collected capture 
the underlying data distribu5on 
and not condi5onal by external 
inï¬‚uence or bias. 
â€¢ It assumes that the patch 
window is dependent on the 
CVE speciï¬c features. 
â€¢ Inference of ğ‘¦ via a classiï¬er 
which treats features (ğ‘‹) as 
independently observed inputs
Causal Centric View 
â€¢ The features collected capture the 
underlying data distribu5on and 
are  condi5onal by external 
inï¬‚uence or bias (Z). 
â€¢ Assumes that there are unknown 
variables that inï¬‚uence the 
features, outputs, and data 
genera5on process 
â€¢ Inference of ğ‘¦ is dependent on 
features (ğ‘‹) and hidden 
confounders Z. 
Causal Inference 
â€¢ The idea of causality can be understood in terms of units, treatment and 
outcomes.
â€¢ units are physical objects (assets) of interests at a par5cular point in 5me. 
â€¢ Each unit can be exposed to  (security control) the treatment T.
â€¢ Pre-Treatment variables (background/noise, Confounders )
â€¢ Post-Treatment variables (aï¬€ected by treatment) 
â€¢ Treatment eï¬€ect - measured at the popula@on (ATE), treated group (CATE), 
â€¢ Confounders (aï¬€ect both the treatment assignment and the outcome)
subgroup, and individual levels (ITE).
â€¢ Outcomes (patching of CVE)
â€¢ Poten&al - For each unit-treatment pair, the outcome of that treatment when applied 
â€¢ Observed - The observed outcome is the outcome of the treatment that is actually  
â€¢ Counterfactual : The outcome if the unit had taken another treatment.
on that unit.
applied.
â€¢ Asset ğ‘ is exposed to ğ¶ğ‘‰ğ¸ and there are two security 
controls ğ‘†ğ¶ğ‘ and ğ‘†ğ¶ğ‘ , with mi?ga?on rates of 70%, and 
90% respec?vely. 
â€¢ Observa?onal data contains a group of assets/organisa?ons 
(units) who installed/conï¬gured diï¬€erent security controls 
(treatment), their corresponding outcomes (mi?ga?on and 
patch ?melines) but without direct access to the 
reason/mechanism why/how they use speciï¬c control. 
â€¢ With causal inference  - We can observe the delay in the 
security control and when ğ‘†ğ¶ğ‘ or ğ‘†ğ¶ğ‘ is installed on the 
asset ğ‘. 
the ğ¶ğ‘‰ğ¸. 
applica&on of patch for the CVE in the presence/absence  of 
â€¢ The change of patching window is the eï¬€ect that treatment 
(Security Control) asserts on the applica;on of patch for 
Problem  
Question 1 - Does the presence of security control on 
assets influence the patch management policy ?
asset)
â€¢ğ‘‹ - be the covariate vector (which includes CVE features)
â€¢ğ‘‡ - the binary treatment variable (presence of security control on the 
â€¢ğ‘Œ - the outcome of interest (delay in patching, which is measured as 
â€¢ Average treatment eï¬€ects (ATE) of ğ‘‡ on ğ‘Œ
ğœ = E[ğ‘Œ (1) âˆ’ ğ‘Œ (0)]
!me diï¬€erence to patch release date by the vendor to patch applied 
date on the asset). 
â€¢ We are interested in es5ma5ng the 
Question 2 - Do the characteristics of security controls 
influence the patching time windows?
â€¢ X is the feature vector containing pre-treatment covariates of CVE. The 
treatment random variable (security control), ğ‘‡ğ‘“ , is a pair of values ğ‘‡ğ‘“
= (ğ‘Šğ‘“ , ğ·ğ‘“ )  where ğ‘Šğ‘“âˆˆ W corresponds to the type of treatment 
being administered (e.g. ğ‘†ğ¶1, ğ‘†ğ¶2 .)
â€¢ğ·ğ‘“ corresponds to the dosage of the treatment (e.g. parameters 
derived from a con$nuous func$on which, for a given treatment ğ‘¤ lies 
in the corresponding treatmentâ€™s dosage space, Dğ‘¤ (e.g. the interval 
â€¢ Individual   treatment eï¬€ects (ITE) of ğ‘‡ on ğ‘Œi
ğœ i= E[ğ‘Œi (1) âˆ’ ğ‘Œi (0)]
[0, 1]).
â€¢ We are interested in es5ma5ng the 
â€¢ Capability of a security control can be  measured 
based on Mul5ple Criteria
â€¢ Eï¬€ec%veness(ğ¸ğ‘“ğ‘“)
â€¢ Coverage (ğ¶ğ‘œğ‘£)
â€¢ Assurance (ğ´ğ‘ ğ‘ u)
â€¢ Cost (ğ¶ğ‘œ)
â€¢ Impact (ğ¼ğ‘š)
â€¢ Mi%ga%on %me (ğ‘€ğ‘¡)
Scoring 
Function 
â€¢ Process  
â€¢ Selec%on of datasets to use
â€¢ Se/ng security controls criterionâ€™s weigh%ngs
â€¢ Conduct eï¬€ec%veness scoring
â€¢ Compute rela%ve trade-oï¬€ scores.
Scoring Function Example
â€¢ To weight Impact ğ¼ğ‘š criteria based on preven5on, detec5on, and 
â€¢ Security Mangers can set ğ‘¤ğ‘ = 0.5, ğ‘¤d = 0.25, and ğ‘¤r ğ‘–ğ‘‘ğ‘Ÿ= 0.25 respec;vely i.e. ğ‘‰
example the CVSS Score is 7 then (ğ‘Ÿ, ğ‘¡) = (0.7, 0.3). 
â€¢ The truncated distribu;ons method can be used to generate order weights ğ‘Š = 
response (P/D/R), security managers can ï¬rst set Criteria weights as 
â€¢ Risk and trade oï¬€ values for a given CVE can be derived from CVSS score in this 
= [0.5, 0.25, 0.25] 
[0.67, 0.04, 0.29]. 
â€¢ Patch Status Data Set 
â€¢ Nessus vulnerability scanner output  of 2000 enterprises , and state 
â€¢ The ğ‘ğ‘ğ‘¡ğ‘hğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦ is calculated as the diï¬€erence between patch 
applied date (ğ¹ğ¼ğ‘‹ğ¸ğ·) and date on which CVE was ï¬rst found on the 
asset (ğ‘‚ğ‘ƒğ¸ğ‘) in scan output. We only consider assets that are 
of 8922 CVEâ€™s. 
regularly scanned across 90 days period to make sure we do not 
consider short-living assets. 
â€¢ OrganisaNons have a centralised patching policy 
â€¢ Only CVEâ€™s/Assets which have been patched on at least one of 
the asset in the organisaNon
DataSets 
Data Sets 
â€¢ CVE Features (269 features) for each CVE : 
â€¢ Age of CVE, CVSS Vector, CPE, CWE, CAPEC, MITRE 
techniques mapped to CVE, exploit availability and their 
sources, and soDware types. 
â€¢ Textual features: topics extracted from media sources, 
web forums, paste sites, blogs, descripHons of CVE on 
NVD and vendor notes, and miHgaHon steps of CVE. 
â€¢ For textual features, we train Word2Vec[29] model with 
text data and this module outputs 100 covariates. We 
concatenate both the numerical and textual features to 
form our feature vector of size 269
â€¢ File based Mi?ga?on DataSet
â€¢ 508521 ï¬le hashes of current aWacks in the past 3 
months from a threat intel vendor which cover around 
3000 CVEs.
â€¢ AZack Emula?on DataSet. 
â€¢ MITRE ATT&CK aWack simulaHons of ATP29, APT3, and 
FIN7 ATP groups on cybersecurity products using an 
open methodology.
parHcular aWack 134 substep is recorded
â€¢ The  detecHon/protecHon capability of the product to 
Experiments â€“ Metrics 
â€¢ Evalua5on  Metrics 
â€¢ Rooted Precision in Es;ma;on of Heterogeneous Eï¬€ect 
â€¢ Mean Absolute Error on ATE
Experiments â€“ Question 1
â€¢ We es5mate Average Treatment Eï¬€ect (ATE) with Doubly Robust Es5mator 
(DRE) 
â€¢ First ï¬t the treatment and response with one classiï¬er.
â€¢ Use linear model to predict the response residuals
from the treatment residuals 
â€¢ Logis5c Regression (LR), Random Forest (RF), Mul5Layer Perceptron (MLP), 
and Gradient Boos5ng (G Logis5c Regression (LR), Random Forest (RF), 
Mul5Layer Perceptron (MLP), and Gradient Boos5ng (GB) models in terms of 
propensity score and outcome (both treated and control) es5ma5ons. We 
randomly split into training (63%), valida5on (27%) and test sets (10%) for all 
models to report the scores and employ grid search to tune the parameters 
of the models. 
â€¢ We randomly split into training (63%), valida5on (27%) and test sets (10%) 
for all models to report the scores and employ grid search to tune the 
parameters of the models. 
Experiments â€“ Question 2
â€¢ Ground truth data for ITE es5ma5on are very hard
â€¢ Create a semi-synthe7c dataset from real-world observa7on by leveraging the con7nuous scoring 
func7on  to derive counterfactual from risk curves. To model ideal poten7al outcomes, we use 
â€¢ We only observe the presence of the control, but we do not have data before 
the control was installed vs aYer installed and how the patching window might 
have changed if the organisa;on installed control 1 vs. control 2. 
truncated Gaussian distribu7ons ğ‘¦ Ìƒâˆ¼ N (ğœ‡ , ğœ ) + ğœ– with ğ‘¡ dose,ğ‘¡ğ‘¡ğœ–âˆ¼ N(0,1), where ğœ‡ dose = 
(ğ‘ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’,ğ‘‘ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’,ğ‘Ÿğ‘ ğ‘ğ‘œğ‘Ÿğ‘’,ğ‘“ğ‘šğ‘ ğ‘ğ‘œğ‘Ÿğ‘’) is derived from the weights from scoring func7on for each 
treatment. 
Experiments â€“ Question 2
â€¢ We use state-of- the-art methods for the task of learning ITEs from 
observa5onal data.
â€¢ Counterfactual Regression (CFR) 
â€¢ Causal Eï¬€ect Varia;onal Autoencoder (CEVAE) 
â€¢ Bayesian Addi;ve Regression Trees (BART) 
â€¢ Dose Response Network (DRNet) 
Results
â€¢ We infer from the results that the presence of security controls inï¬‚uences the 
patching policy of the defender and the average delay window is 9.45 days. 
â€¢ We observe the capability of security control directly impacts the patching 
priority in organisa=ons. CVEâ€™s with higher CVSS scores are leC unpatched on 
an average of 5-6 days in the presence and absence of control vs CVEâ€™s with 
low scores are delayed by 10-12 days. 
â€¢ Organisa;ons that have response- oriented controls installed tend to delay 
patching more than the ones which have protec;on and detec;on. 
â€¢ We also speculate that defenders invest in controls that align well with the 
security goals of the organisa;on i.e. if the organisa;on has invested in 
Incident response personnel, they may prefer to have security control that 
helps them in detec;on then in the response process. Overall, the results 
highlight the importance of security controls in defenders risk management 
process. 
Related Work
â€¢ Our work combines several research areas 
â€¢ Security Control Scoring 
â€¢ Most of the work surveyed either run operaHon research (OR) simulaHons or relied on SME 
surveys to score security controls. 
â€¢ We give tunable parameters for stakeholders to choose from the set of controls. This funcHon 
Hes in well with the causal inference framework, which can help in reasoning some of the 
security policies inside an organizaHon. 
â€¢ Risk Management 
â€¢ The State of art deï¬nes factors and metrics taken into account to the deï¬niHon of risk, and the 
threat and the risk esHmaHon process. The main weaknesses of the proposed frameworks to 
date are either the predominantly manual process involved or the lack of formal modeling. 
â€¢ We present a mathemaHcal formalism and signiï¬cantly improving the level of automaHon 
involved in the cybersecurity risk assessment.
â€¢ Applying Causal inference in the context of a security dataset is rare. To the best of our 
knowledge, this is one of the ï¬rst aWempts to study the security policy problems through the 
lens of causal inference. 
â€¢ Causal Inference
Future  Work
â€¢ Our work opens up mul5ple research direc5ons
â€¢ There are many scenarios especially in cybersecurity, where labeled data is 
scarce and one has to make decisions based on observa;onal data.  
â€¢ Applying Causal Inference to other areas of Cyber Security and adap;ng the 
proposed method to areas of security risk assessment/processes in 
organisa;ons. 
â€¢ Causal Learning helps to build robust, reproducible, and easier to explain 
models in cyber security domain and needs further explora;on