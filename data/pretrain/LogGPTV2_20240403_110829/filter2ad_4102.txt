title:Effect of Security Controls on Patching Window: A Causal Inference
based Approach
author:Aditya Kuppa and
Lamine M. Aouad and
Nhien-An Le-Khac
Effect of Security Controls on 
Patching Window: A Causal 
Inference based Approach
Aditya Kuppa , Lamine Aouad and NhienAn LeKhac
Annual Computer Security Applications Conference (ACSAC) 
2020
Motivation
• Can security controls help in addressing 
residual risk ? 
• How are defenders using security 
controls to manage their residual 
risk.
• Do the characteris;cs of security 
controls inﬂuence the patching ;me 
windows?
• How this study can help defenders? 
• Re-calibrate and manage the residual risk for a 
given threat or class of threats given the control 
is already installed.
• Eﬀec?ve priori?za?on of patching of a given 
CVE based on the security control eﬀec?veness.
• Pivot the security investments into people, 
process and controls depending on the risk 
appe?te
• Convey risk decisions in a data-driven fashion to 
stakeholders and decision-makers
• Improve and manage the controls that are 
already in place. 
Impact
Classifier Centric 
View 
• The features collected capture 
the underlying data distribu5on 
and not condi5onal by external 
inﬂuence or bias. 
• It assumes that the patch 
window is dependent on the 
CVE speciﬁc features. 
• Inference of 𝑦 via a classiﬁer 
which treats features (𝑋) as 
independently observed inputs
Causal Centric View 
• The features collected capture the 
underlying data distribu5on and 
are  condi5onal by external 
inﬂuence or bias (Z). 
• Assumes that there are unknown 
variables that inﬂuence the 
features, outputs, and data 
genera5on process 
• Inference of 𝑦 is dependent on 
features (𝑋) and hidden 
confounders Z. 
Causal Inference 
• The idea of causality can be understood in terms of units, treatment and 
outcomes.
• units are physical objects (assets) of interests at a par5cular point in 5me. 
• Each unit can be exposed to  (security control) the treatment T.
• Pre-Treatment variables (background/noise, Confounders )
• Post-Treatment variables (aﬀected by treatment) 
• Treatment eﬀect - measured at the popula@on (ATE), treated group (CATE), 
• Confounders (aﬀect both the treatment assignment and the outcome)
subgroup, and individual levels (ITE).
• Outcomes (patching of CVE)
• Poten&al - For each unit-treatment pair, the outcome of that treatment when applied 
• Observed - The observed outcome is the outcome of the treatment that is actually  
• Counterfactual : The outcome if the unit had taken another treatment.
on that unit.
applied.
• Asset 𝑎 is exposed to 𝐶𝑉𝐸 and there are two security 
controls 𝑆𝐶𝑎 and 𝑆𝐶𝑏 , with mi?ga?on rates of 70%, and 
90% respec?vely. 
• Observa?onal data contains a group of assets/organisa?ons 
(units) who installed/conﬁgured diﬀerent security controls 
(treatment), their corresponding outcomes (mi?ga?on and 
patch ?melines) but without direct access to the 
reason/mechanism why/how they use speciﬁc control. 
• With causal inference  - We can observe the delay in the 
security control and when 𝑆𝐶𝑎 or 𝑆𝐶𝑏 is installed on the 
asset 𝑎. 
the 𝐶𝑉𝐸. 
applica&on of patch for the CVE in the presence/absence  of 
• The change of patching window is the eﬀect that treatment 
(Security Control) asserts on the applica;on of patch for 
Problem  
Question 1 - Does the presence of security control on 
assets influence the patch management policy ?
asset)
•𝑋 - be the covariate vector (which includes CVE features)
•𝑇 - the binary treatment variable (presence of security control on the 
•𝑌 - the outcome of interest (delay in patching, which is measured as 
• Average treatment eﬀects (ATE) of 𝑇 on 𝑌
𝜏 = E[𝑌 (1) − 𝑌 (0)]
!me diﬀerence to patch release date by the vendor to patch applied 
date on the asset). 
• We are interested in es5ma5ng the 
Question 2 - Do the characteristics of security controls 
influence the patching time windows?
• X is the feature vector containing pre-treatment covariates of CVE. The 
treatment random variable (security control), 𝑇𝑓 , is a pair of values 𝑇𝑓
= (𝑊𝑓 , 𝐷𝑓 )  where 𝑊𝑓∈ W corresponds to the type of treatment 
being administered (e.g. 𝑆𝐶1, 𝑆𝐶2 .)
•𝐷𝑓 corresponds to the dosage of the treatment (e.g. parameters 
derived from a con$nuous func$on which, for a given treatment 𝑤 lies 
in the corresponding treatment’s dosage space, D𝑤 (e.g. the interval 
• Individual   treatment eﬀects (ITE) of 𝑇 on 𝑌i
𝜏 i= E[𝑌i (1) − 𝑌i (0)]
[0, 1]).
• We are interested in es5ma5ng the 
• Capability of a security control can be  measured 
based on Mul5ple Criteria
• Eﬀec%veness(𝐸𝑓𝑓)
• Coverage (𝐶𝑜𝑣)
• Assurance (𝐴𝑠𝑠u)
• Cost (𝐶𝑜)
• Impact (𝐼𝑚)
• Mi%ga%on %me (𝑀𝑡)
Scoring 
Function 
• Process  
• Selec%on of datasets to use
• Se/ng security controls criterion’s weigh%ngs
• Conduct eﬀec%veness scoring
• Compute rela%ve trade-oﬀ scores.
Scoring Function Example
• To weight Impact 𝐼𝑚 criteria based on preven5on, detec5on, and 
• Security Mangers can set 𝑤𝑝 = 0.5, 𝑤d = 0.25, and 𝑤r 𝑖𝑑𝑟= 0.25 respec;vely i.e. 𝑉
example the CVSS Score is 7 then (𝑟, 𝑡) = (0.7, 0.3). 
• The truncated distribu;ons method can be used to generate order weights 𝑊 = 
response (P/D/R), security managers can ﬁrst set Criteria weights as 
• Risk and trade oﬀ values for a given CVE can be derived from CVSS score in this 
= [0.5, 0.25, 0.25] 
[0.67, 0.04, 0.29]. 
• Patch Status Data Set 
• Nessus vulnerability scanner output  of 2000 enterprises , and state 
• The 𝑝𝑎𝑡𝑐h𝑑𝑒𝑙𝑎𝑦 is calculated as the diﬀerence between patch 
applied date (𝐹𝐼𝑋𝐸𝐷) and date on which CVE was ﬁrst found on the 
asset (𝑂𝑃𝐸𝑁) in scan output. We only consider assets that are 
of 8922 CVE’s. 
regularly scanned across 90 days period to make sure we do not 
consider short-living assets. 
• OrganisaNons have a centralised patching policy 
• Only CVE’s/Assets which have been patched on at least one of 
the asset in the organisaNon
DataSets 
Data Sets 
• CVE Features (269 features) for each CVE : 
• Age of CVE, CVSS Vector, CPE, CWE, CAPEC, MITRE 
techniques mapped to CVE, exploit availability and their 
sources, and soDware types. 
• Textual features: topics extracted from media sources, 
web forums, paste sites, blogs, descripHons of CVE on 
NVD and vendor notes, and miHgaHon steps of CVE. 
• For textual features, we train Word2Vec[29] model with 
text data and this module outputs 100 covariates. We 
concatenate both the numerical and textual features to 
form our feature vector of size 269
• File based Mi?ga?on DataSet
• 508521 ﬁle hashes of current aWacks in the past 3 
months from a threat intel vendor which cover around 
3000 CVEs.
• AZack Emula?on DataSet. 
• MITRE ATT&CK aWack simulaHons of ATP29, APT3, and 
FIN7 ATP groups on cybersecurity products using an 
open methodology.
parHcular aWack 134 substep is recorded
• The  detecHon/protecHon capability of the product to 
Experiments – Metrics 
• Evalua5on  Metrics 
• Rooted Precision in Es;ma;on of Heterogeneous Eﬀect 
• Mean Absolute Error on ATE
Experiments – Question 1
• We es5mate Average Treatment Eﬀect (ATE) with Doubly Robust Es5mator 
(DRE) 
• First ﬁt the treatment and response with one classiﬁer.
• Use linear model to predict the response residuals
from the treatment residuals 
• Logis5c Regression (LR), Random Forest (RF), Mul5Layer Perceptron (MLP), 
and Gradient Boos5ng (G Logis5c Regression (LR), Random Forest (RF), 
Mul5Layer Perceptron (MLP), and Gradient Boos5ng (GB) models in terms of 
propensity score and outcome (both treated and control) es5ma5ons. We 
randomly split into training (63%), valida5on (27%) and test sets (10%) for all 
models to report the scores and employ grid search to tune the parameters 
of the models. 
• We randomly split into training (63%), valida5on (27%) and test sets (10%) 
for all models to report the scores and employ grid search to tune the 
parameters of the models. 
Experiments – Question 2
• Ground truth data for ITE es5ma5on are very hard
• Create a semi-synthe7c dataset from real-world observa7on by leveraging the con7nuous scoring 
func7on  to derive counterfactual from risk curves. To model ideal poten7al outcomes, we use 
• We only observe the presence of the control, but we do not have data before 
the control was installed vs aYer installed and how the patching window might 
have changed if the organisa;on installed control 1 vs. control 2. 
truncated Gaussian distribu7ons 𝑦 ̃∼ N (𝜇 , 𝜎 ) + 𝜖 with 𝑡 dose,𝑡𝑡𝜖∼ N(0,1), where 𝜇 dose = 
(𝑝𝑠𝑐𝑜𝑟𝑒,𝑑𝑠𝑐𝑜𝑟𝑒,𝑟𝑠𝑐𝑜𝑟𝑒,𝑓𝑚𝑠𝑐𝑜𝑟𝑒) is derived from the weights from scoring func7on for each 
treatment. 
Experiments – Question 2
• We use state-of- the-art methods for the task of learning ITEs from 
observa5onal data.
• Counterfactual Regression (CFR) 
• Causal Eﬀect Varia;onal Autoencoder (CEVAE) 
• Bayesian Addi;ve Regression Trees (BART) 
• Dose Response Network (DRNet) 
Results
• We infer from the results that the presence of security controls inﬂuences the 
patching policy of the defender and the average delay window is 9.45 days. 
• We observe the capability of security control directly impacts the patching 
priority in organisa=ons. CVE’s with higher CVSS scores are leC unpatched on 
an average of 5-6 days in the presence and absence of control vs CVE’s with 
low scores are delayed by 10-12 days. 
• Organisa;ons that have response- oriented controls installed tend to delay 
patching more than the ones which have protec;on and detec;on. 
• We also speculate that defenders invest in controls that align well with the 
security goals of the organisa;on i.e. if the organisa;on has invested in 
Incident response personnel, they may prefer to have security control that 
helps them in detec;on then in the response process. Overall, the results 
highlight the importance of security controls in defenders risk management 
process. 
Related Work
• Our work combines several research areas 
• Security Control Scoring 
• Most of the work surveyed either run operaHon research (OR) simulaHons or relied on SME 
surveys to score security controls. 
• We give tunable parameters for stakeholders to choose from the set of controls. This funcHon 
Hes in well with the causal inference framework, which can help in reasoning some of the 
security policies inside an organizaHon. 
• Risk Management 
• The State of art deﬁnes factors and metrics taken into account to the deﬁniHon of risk, and the 
threat and the risk esHmaHon process. The main weaknesses of the proposed frameworks to 
date are either the predominantly manual process involved or the lack of formal modeling. 
• We present a mathemaHcal formalism and signiﬁcantly improving the level of automaHon 
involved in the cybersecurity risk assessment.
• Applying Causal inference in the context of a security dataset is rare. To the best of our 
knowledge, this is one of the ﬁrst aWempts to study the security policy problems through the 
lens of causal inference. 
• Causal Inference
Future  Work
• Our work opens up mul5ple research direc5ons
• There are many scenarios especially in cybersecurity, where labeled data is 
scarce and one has to make decisions based on observa;onal data.  
• Applying Causal Inference to other areas of Cyber Security and adap;ng the 
proposed method to areas of security risk assessment/processes in 
organisa;ons. 
• Causal Learning helps to build robust, reproducible, and easier to explain 
models in cyber security domain and needs further explora;on