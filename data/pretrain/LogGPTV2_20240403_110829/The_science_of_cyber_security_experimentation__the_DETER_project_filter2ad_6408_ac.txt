### Scale and Fidelity in Experimental Apparatus

Certain parts of an experimental apparatus require high-resolution nodes with high fidelity, while other parts can be represented with a lower degree of resolution, allowing for larger-scale simulations. This creates a "scale of scaling" that ranges from high-fidelity, linear scaling to low-fidelity, highly scalable solutions. Different points on this scale are enabled by various mechanisms for emulation and simulation.

Based on this observation, we began exploring methods to incorporate multiple representation techniques, providing a full spectrum of scale-fidelity trade-offs for experimental system components. Here are some examples:

- **Single Hardware Node with a Single Experiment Node:** Running either natively or via a conventional Virtual Machine Manager (VMM) supporting a single guest OS.
- **Single Hardware Node with Multiple Virtualized Nodes:** Each node is a full-blown conventional Virtual Machine (VM) on a VMM.
- **Single Node with Lightweight VMs:** A large number of lightweight VMs on a VMM designed for scaling the number of experiment nodes with limited functionality.
- **Representation as Threads of Execution:** Individual experiment nodes represented as threads in a thread management environment.
- **Large-Scale Software-Based Network Simulation:** Using advanced simulation techniques [22].

We recognized that these methods would be more useful to experimenters if they were part of a unified framework for constructing composable experimental apparatus. This framework would use common building blocks and methods of composition, abstraction, and re-use. Our approach is based on an abstract fundamental building block called a "container," which represents experimental elements at the same level of abstraction and serves as the basic unit of composition for constructing an experimental apparatus. The container-based methodology supports several important goals:

- **Flexible Use of Physical Resources:** Leverage DeterLab’s physical resources more flexibly to create larger-scale experiments.
- **High-Resolution Modeling:** Enable experimenters to model complex systems with high resolution and fidelity for critical components, while abstracting out less important elements.
- **Reduced Workload:** Reduce the experimenter’s workload in constructing experimental apparatus, enabling larger-scale experiments with less effort.

### Lessons Learned: From Experimental Apparatus to Experimental Data to Experimental Results

The first four lessons learned were primarily related to the static aspects of setting up an experimental apparatus, including basic construction, use of fixtures for federation, limited communication outside the testbed, and support for orders-of-magnitude experiment scale-up. Other lessons focused on the dynamic aspects of running an experiment.

Early in the second phase, we recognized the need for a workbench to operate the experimental apparatus, feed input data and events, observe its operation, and adjust fixtures for collecting experimental data. The first-phase workbench, SEER [10], met this need to some extent. However, the adoption of SEER highlighted a growing need for DeterLab experimenters: an approach to the "big data" problem. As DeterLab facilities matured, the size, structure, and complexity of the datasets increased, necessitating tools and methodologies for mining experimental data to discover results.

### Current Deter Research Program

Our current research program builds on the lessons learned and includes activities to enrich the testbed with progressive enhancements. During the second phase, our enhancement efforts included:

- **First Generation Federation Capabilities [9]:** Allowing the use of resources in other testbeds and linking external resources into experiments.
- **Risky Experiment Management [16]:** Enabling outside communication and managing risky experiments.
- **First-Generation Experimenter Workbench [10]:** Managing experiments, viewing activity, and analyzing results data.

There was a strong alignment between our objectives and the needs of DeterLab experimenters. For example, the first generation of federation capability arose from our desire to reach greater scale by using resources in other testbeds, and experimenters wanted to link their own specialized resources into their experiments.

### Experiment Lifecycle Management

Experiment lifecycle management (ELM) is an evolution of our first-generation workbench, SEER. ELM focuses on managing the objects used by experimenters, including scientific, physical, communication, and computational resources, as well as models, designs, procedures, programs, and data. ELM provides storage, presentation, archival, browsing, and searching functions, far beyond the original testbed approach of shell login and filesystem access. We are building this management framework on the Eclipse [23] platform to leverage its integrated development environment (IDE) capabilities.

New levels of abstraction in experiment definition are a key component of ELM. In the original testbed approach, experimenters had to specify in detail various types of resources, including computational elements, network topologies, and software configurations. With ELM, experiments can be highly modular and structured for re-use. Although the detailed "expert mode" is still available, most researchers will use the newer facilities for defining experiment requirements, constraints, and invariants.

### Containers: Scale-Up and Flexible Fidelity

Our ongoing work on scalability is based on observations about the trade-offs between the fidelity of a computational element and the scale of resources required. By adding new types of computational elements (conventional VMs, QEMU lightweight VMs, processes on conventional OSs, QEMU processes, individual threads of execution), we have added both flexibility and complexity to the experimental apparatus. These elements can be used to model nodes in a simulated network, providing a range of fidelity and scalability options.