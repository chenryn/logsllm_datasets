leads to single points of failure and thus decreases the anonymity guarantees we can derive. Based on this insight,
we propose an alternative path selection algorithm, coined DISTRIBUTOR, that maximally distributes the trust
without decreasing the overall performance of the Tor network, thereby achieving signiﬁcantly better anonymity
guarantees while preserving Tor’s throughput and latency.
Figure 1 depicts the result of applying our monitor to Tor’s archive data, from Tor Metrics [29], of the last two
years, where the anonymity guarantees are averaged for each day for the sake of readability. In the experiment
the monitor assumes that 0.5% of all nodes are compromised, that the user requested the ports 443 (HTTPS) and
194 (IRC) and wants to compare itself against the set of users that solely request port 443, i.e., the probability
that it is in the anonymity set including the set of users that solely surf1. The ﬁgure compares Tor’s path selection
algorithm against DISTRIBUTOR for sender anonymity and recipient anonymity and shows that DISTRIBUTOR
leads to signiﬁcant improvements. The ﬁgure additionally illustrates that a user’s degree of anonymity highly
ﬂuctuates. This ﬂuctuation underscores the value of providing a real-time, user-centric, anonymity monitor.
3 The extended AnoA Framework
The theoretical framework upon which we base MATOR is an extension of the ANOA framework for proving
anonymity guarantees of anonymous communication networks [6]. ANOA is a useful building block for analyzing
anonymity, but it lacks the generality to model the scenarios in MATOR. As a consequence, we extend the ANOA
framework in this section such that it is suitable for proving that the anonymity bounds of our MATOR monitors
are secure. In this section, we ﬁrst present the extended ANOA challenger for stronger, adaptive adversaries and
deﬁne adversary classes as realistic restrictions for this strong adversary. We then analyze how the anonymity
guarantees provided by our extended ANOA framework compose under sequential composition. Finally, we
present instantiations of the three anonymity notions we need for MATOR: sender anonymity, recipient anonymity
and relationship anonymity.
3.1 The extended AnoA challenger
ANOA generalizes the notion of adjacency from differential privacy [10] to anonymity deﬁnitions. The challenger
of ANOA deﬁnes at its heart an indistinguishability game in which a challenge bit b is chosen, and depending on
this bit b one of two settings is executed. The goal of the adversary is to guess this bit b.
The challenger for ANOA (for n challenges) expects two kinds of messages: input messages of the form
(input, m), that are processed by the protocol by running the protocol and letting a user S send the message m
to a recipient R, and challenge messages of the form (challenge, r1, r2, Ψ), where r1 and r1 correspond to two
different inputs and where Ψ is an identiﬁer for the challenge. In each challenge message (challenge, r1, r2, Ψ),
the two messages r1, r2 have to be adjacent in a generalized sense. To simplify the notation of our adjacency
functions, they get the bit b from the challenger as an additional input. Consequently, the adjacency functions can
1We choose port 443 as the Tor browser uses by default the add-on HTTPS Everywhere, which uses HTTPS whenever possible.
4
Adaptive ANOA Challenger CH(P, α, n, b)
Upon message(input, r = (S,R, m, sid))
1: RunProtocol(r, 0)
Upon message (challenge, r0, r1, Ψ)
1: if Ψ /∈ {1, . . . , n} then abort
2: else if Ψ ∈ T then /* if Ψ is known */
3:
4:
5: else s := fresh and add Ψ to T /* if Ψ is fresh */
6: Compute (r∗, sΨ) ← α(s, r0, r1, b)
7: RunProtocol(r, Ψ)
Retrieve s := sΨ
if s = over then abort
RunProtocol(r = (S,R, m, sid), Ψ)
1: if ¬∃y such that (sid, y, Ψ) ∈ S then /* if sid is fresh */
Let sidreal ← {0, 1}k; Store (sid, sidreal, Ψ) in S.
2:
3: else sidreal := y /* if a real session id for sid has already been chosen */
4: Run P on r = (S,R, m, sidreal) and forward all messages that are sent by P to the adversary A and send all
messages by the adversary to P.
Figure 2: Extended ANOA Challenger
model the anonymity challenges on their own and simply output a message r∗ that is sent to the challenger. We
assume an anonymity function α, with the following interface:
α(r0 = (S0,R0, m0, sid0), r1 = (S1,R1, m1, sid1), b)
For every challenge with tag Ψ, the challenger maintains a state sΨ that can be either fresh, over, or contain
some information about an ongoing challenge. For every challenge that is not in the state over, we apply the
anonymity function with its correct state – fresh for newly started challenges, some other state sΨ if the challenge
is already active – and simulate the protocol on the output of the adjacency function. The challenger only allows
n challenges. This restriction is implemented by restricting the set of possible challenge tags Ψ, which are used
by the adversary to distinguish the challenges. We store all tags of already started challenges Ψ in a set T.
To model more complex anonymity notions, such as anonymity for a session, the adjacency function is allowed
to keep state for each challenge, i.e., sessions created by challenge messages are isolated from sessions created
by input messages. Thus, an adversary cannot use the challenger to hijack sessions. This is done by choosing
and storing session IDs sidreal for every challenge separately as follows. Whenever a message with a new session
ID sid is to be sent to the protocol, randomly pick a fresh session ID sidreal that is sent instead, and store sidreal
together with sid and the challenge tag Ψ (if it is a challenge) or zero (if it is not a challenge). We store all
mappings of sessions (sid, sidreal, Ψ) in a set S.
By deﬁnition, the sessions created by challenge messages are isolated from sessions created by input messages.
Thus, an adversary cannot use the challenger to hijack sessions.
Multiplicative factor. ANOA introduces a multiplicative factor eε to the indistinguishability based deﬁnition,
which gives the notion a ﬂavor of differential privacy. Although they do not illustrate the usefulness of this factor
in their analysis, we ﬁnd that such a factor can in some cases be surprisingly helpful in describing anonymity
guarantees. The most prominent case we have found is the analysis of an adversary that compromises no, or
only a very limited amount of nodes. Before describing the details of this aspect we present our generalization of
ANOA and then later elaborate on the impact of a multiplicative factor in Section 6.4.
The full description of the session challenger is available in Figure 2.
Deﬁnition 1 ((n, ε, δ)-α-IND-CDP). A protocol P is (n, ε, δ)-α-IND-CDP for a class of adversaries A, with ε ≥ 0
and 0 ≤ δ ≤ 1, if for all PPT machines A,
Pr [0 = (cid:104)A(A(n))||CH(P, α, n, 0)(cid:105)]
≤ enε Pr [0 = (cid:104)A(A(n))||CH(P, α, n, 1)(cid:105)] + enεnδ
where the challenger CH is deﬁned in Figure 2.
5
Example 1: Single message sender anonymity. For modeling the simplest version of sender anonymity, where only
a single message is sent by one of two possible senders, the adjacency function αSA depending on the challenge bit
b simply chooses which of the two senders sends the message. As the adjacency function models sender anonymity,
it makes sure that no information is leaked by the message itself or the recipients identity. Therefore it chooses
the same message and the same recipient in both scenarios, e.g., by requiring that they are equal or by simply
always choosing the message and recipient that are sent forthe ﬁrst scenario. Moreover, since here we only allow
a single message per session, for each message a fresh session is created and this session is terminated by setting
the state to over, i.e., αSA(s, (S0,R0, m0), (S1, , ), b) = ((Sb,R0, m0), over). In Section 3.4 we formally deﬁne
the anonymity functions for sender, recipient, and relationship anonymity.
(cid:5)
Modiﬁcations made to AnoA.
In ANOA only static, i.e., non-interactive, scenarios can be modeled, which
excludes two-way communication. Moreover, the adversary is not restricted in its choices to determine the actions
of the challenge users, which excludes many interesting and realistic scenarios. In practice, it makes an enormous
difference to which server a user wants to connect, as this might require a different port or different settings. The
worst-case adversary in ANOA could always choose the ports and settings for which the weakest guarantees can
be given (c.f. Section 6 for the inﬂuence of ports and settings on anonymity). However, such an adversary is
meaningless in practice because a user is rather interested in its real anonymity (e.g., the difﬁculty to distinguish
its behavior from a regular user that only uses port 443 for HTTPS) and not in a superﬁcial worst-case behavior.
3.2 Adversary classes
An adversary class A(·) is a wrapper that restricts the adversary A in its possible output behavior, and thus, in
its knowledge about the world. Technically, it is a PPT machine A(A) that internally runs the adversary A and
forwards all messages that are sent from a compromised node to the adversary A and vice versa.
Example 2: Hiding recipients. With such a notion of an adversary class we can model an interactive scenario in
which the adversary solely learns the information that an ISP would learn (and the information from its addition-
ally compromised Tor nodes). The adversary class A(·) computes the behavior of the servers without informing
the adversary A about the communication. Thus, the adversary class would respond for the servers and the adver-
sary will not be directly informed about the messages by the servers themselves. In the same way, the adversary
class can additionally control which responses from the servers the adversary can see and, possibly, allow very
few choices and solely inform the adversary whether a challenge session has ended.
(cid:5)
Example 3: Tor with entry guards. In ANOA all actions, including the responses that servers give to users, are
computed by the adversary. This design makes the deﬁnition of use Consider the Tor network with entry guards.
Every user selects a small set of entry nodes (his guards) from which he chooses the entry node of every circuit.
Guards are rotated only after several months. As a compromised entry node is fatal for the security guarantees
that Tor can provide, the concept of entry guards helps in reducing the risk of choosing a malicious node. However,
if such an entry guard is compromised the impact is more severe since an entry guard is used for a large number
of circuits.
The following example shows a scenario in which an (unrestricted) adaptive adversary might be too strong.
An adaptive adversary can choose its targets adaptively and thus perform the following attack. It (statically)
corrupts some nodes and then sends (polynomially) many messages (input, r = (S, , , )) for different users S,
until one of them, say Alice, chooses a compromised node as its entry guard. Then A proceeds by using Alice and
some other user in a challenge. As Alice will quite likely use the compromised entry guard again, the adversary
wins with a very high probability (that depends on the number of guards per user).
(cid:5)
Although this extremely successful attack is not unrealistic (it models the fact that some users that happen to
use compromised entry guards will be deanonymized), it might not depict the attack scenario that we are interested
in. Thus, we deﬁne an adversary class that makes sure that the adversary cannot choose its targets. Whenever the
adversary starts a new challenge for (session) sender anonymity, the adversary class draws two users at random
and places them into the challenge messages of this challenge.
Downward composability. The adversary classes that we deﬁne here are downwards composable for all proto-
cols. More precisely, if a protocol P is α secure for an adversary class A1(·) and if A2 is an arbitrary adversary
class, then P is also α secure for an adversary class A1(A2(·)).
This observation follows directly from the fact that within the adversary classes, arbitrary PPT Turing ma-
chines are allowed, which includes wrapped machines A2(·).
Complex anonymity properties as adversary classes. It is possible to describe complex anonymity properties
as the composition of a simpler adjacency function and an adversary class. For modeling sender anonymity in the
presence of speciﬁc user proﬁles, for example, we can simply compose an adversary class that models the user
proﬁle with the (session) sender anonymity adjacency function (c.f. Section 3.4).
6
ACREAL
ACSIM
Figure 3: The two games from Construction 1
Deﬁning user proﬁles as adversary classes. In [22], realistic, but relatively simple user proﬁles are deﬁned that
determine the behavior of users. We can model such proﬁles by deﬁning a speciﬁc adversary class. The adversary
class initiates proﬁles such as the typical users (they use, e.g., Gmail, Google Calendar, Facebook and perform
web search at certain points in time) or BitTorrent users (they use Tor for downloading ﬁles on other times) as
machines and measures time internally. If the adversary sends an input message, the adversary class initiates a
proﬁle. The adversary class might also initiate more proﬁles for random users at the beginning, which corresponds
to “noise”. If the adversary sends a challenge message, the adversary class initiates proﬁles for two different users
(or two different proﬁles, depending on the anonymity notion).
On its own, the adversary class machine runs a loop that activates the proﬁles and increases a value t for time
every now and then, until the adversary decides to halt with its guess b∗ of the challenge bit b. Although the
adversary class activates the proﬁles in a random order, it makes sure that all proﬁles have been activated before
it proceeds to the next point in time. It additionally tells the activated proﬁles the point in time, such that they can
decide whether or not they want to output a valid user action r or an error symbol ⊥. The adversary class then
sends input messages or challenge messages, depending on how the proﬁles have been initialized.
3.3 Sequential composability
It turns out that for some combinations of adversary classes and protocols proving an anonymity property for a
single session does not imply that this anonymity property holds for multiple session, i.e., sequential composition
does not hold. As an example, consider a protocol that leaks all secrets once the second message is sent, and
consider an adversary class A(A) that only forwards challenge-messages to the challenger and that blocks all
input-messages. This combination of a protocol and an adversary class is insecure only from the second challenge
on; hence, for this combination sequential composability does not hold.
We show that sequential composability holds for adversary classes where additional challenges do not enable
qualitatively new attacks, i.e., the adversary does not gain any conceptually new power. This can be ensured by
requiring conditions from an adversary class A(A) with respect to α.
Before we deﬁne these composability conditions, we ﬁrst present a few helpful deﬁnitions. To discuss what
it means that a challenge is simulatable, we consider the following two games, where ACREAL presents a real
game with a regular adversary A within an adversary class A(A) that interacts with a challenger: CH ↔ A(A).
In contrast, ACSIM presents a game in which part of what the adversary sends to the adversary class is simulated
by a simulator S that is placed between the adversary and the adversary class: CH ↔ A(S(A)).
Construction 1. Consider the following two scenarios (as in Figure 3):
ACREAL(b, n): A communicates with A(A) and A communicates with CH(b, n). The bit of the challenger is
ACSIMSz(b, n): A communicates with Sz(b) that in turn communicates with A(A) and A communicates with
b and the adversary may send challenge tags in {1, . . . , n}.
CH(b, n). The bit of the challenger is b and the adversary may send challenge tags in {1, . . . , n}.
We index a simulator with a simulator index z = [(z1, b1), . . . , (zn, bn)] ∈ {0, 1}2n that deﬁnes for which
challenge tags this simulator simply forwards the messages and for which it simulates the challenge (and for
which bit this challenge is simulated). If the bit zi of the simulator index is set to sim, the simulator transforms
the challenge with tag i, by simulating the adjacency function on the challenge message for the bit bi.
The simulator indexes explicitly specify the challenge bit for which they simulate the adjacency function, as
the simulator is unable to know the real challenge bit b. We call two simulator indexes consistent with respect to b,
if for every challenge the simulator behaves consistently for both indexes (either does not simulate the challenge
for both indexes or simulates them both for the same bit or simulates the adjacency function correctly).
7
AChAAChASDeﬁnition 2 (Consistent simulator index). A simulator index (for n challenges) is a bitstring z = [(z1, b1), . . . ,
(zn, bn)] ∈ {0, 1}2n. A pair of simulator indices z, z(cid:48)
(cid:48)
i
∈ {0, 1}2n (for n challenges) is consistent w.r.t. b if
(cid:48)
i = sim.
∀i ∈ {1, . . . , n}
s.t. zi = z
bi = b
(cid:48)
s.t. zi (cid:54)= z
i.
(zi = sim ⇒ bi = b) ∧ (z
(cid:48)
i = sim ⇒ b
(cid:48)
i = b).
and ∀i ∈ {1, . . . , n}
Equipped with the notation from above, we deﬁne the composability conditions for an adversary class. The
conditions which we coin reliability, alpha-renaming and simulatability make sure that the security of a protocol
is not broken qualitatively depending on quantitative changes. Reliability ensures that the adversary class does
not initiate its own challenges. It may, however, still modify or drop challenges. Alpha-renaming ensures that
the behavior of the adversary class is independent of the actual challenge tags per se and it does not interpret
challenge tags semantically. Simulatability ensures that the adversary can simulate challenge messages (for a bit
that he guessed) by input messages.
Deﬁnition 3 (Conditions for adversary class). An adversary class A(·) is composable for an anonymity function
α, if the following conditions hold:
1. Reliability: A(A) never sends a message (challenge, , , Ψ) to the challenger before receiving a message
(challenge, , , Ψ) from A with the same challenge tag Ψ.
2. Alpha-renaming: A(A) does not behave differently depending on the challenge tags Ψ that are sent by A
except for using it in its own messages (challenge, , , Ψ) to the challenger and in the (otherwise empty)
message (answer for, , Ψ) to A.
3. Simulatability: For every n ∈ N and every simulator index z = [(z1, b1), . . . , (zn, bn)] ∈ {0, 1}2n there
exists a machine Sz such that:
(a) For every i ∈ {1, . . . , n}. If zi = sim then Sz never sends a message (challenge, , , i) to A(·).
(b) The games ACREAL(b, n) and ACSIMSzdontsim
(b, n) (cf. Construction 1) are computationally indistin-
(c) for all simulator indices z, z(cid:48)
guishable, where zdontsim = [(dontsim, ), . . . ,(dontsim, )]∈ {0, 1}2n for Sz and A(A).
and ACSIMSz(cid:48) (b, n) are indistinguishable.
∈ {0, 1}2n that are consistent w.r.t. b (see Deﬁnition 2) ACSIMSz (b, n)
3.3.1 Composability theorem
Theorem 1. For every protocol P, every anonymity function α, every n ∈ N and every adversary class A(A)
that is composable for α. Whenever P is (1, ε, δ)-α-IND-CDP for A(A), with ε ≥ 0 and 0 ≤ δ ≤ 1, then P is
(n, n · ε, n · enε · δ)-α-IND-CDP for A(A).