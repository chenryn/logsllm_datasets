c
i
l
a
m
e
g
a
r
e
v
A
50
45
40
35
50
45
Average reject-ﬂipping distance
55
70
60
65
Figure 10: Average ﬂipping distances (with Hidost as the de-
tector) of 200 morphed samples originating from the same
malware seed (depicted by the diamond). The malice-
ﬂipping distance of the chosen malware seed is 42, and its
reject-ﬂipping distance is 60. The pearson correlation coef-
ﬁcient of the distances is 0.34.
disable its trace replay feature (i.e., mutation traces that
successfully generates evading sample for one malware seed
are not replayed on the other), treating each malware seed
independently. This enables a fair basis to benchmark the
eﬀectiveness of our approaches against EvadeGP, for ours do
not assume trace replay.
Figure 9 reports evasion rates of BiRand and EvadeHC in
comparison with that of EvadeGP against the hardened de-
tectors. For PDFrate detector with malware threshold set
to 0.45, both EvadeHC and EvadeGP attained 100% evasion
rate on our dataset, while BiRand only achieves 91%. When
the malware threshold is further lowered to 0.4 and 0.35, the
evasion rate of EvadeHC decreases to 68% and 35%, respec-
tively, while EvadeGP and BiRand witness more signiﬁcant
drops in their evasion rates, attaining evasion rate of as low
as 5% for BiRand and 26% for EvadeGP. Similarly, when the
malware threshold of Hidost is reduced to −0.75, BiRand and
EvadeGP could only ﬁnd evading samples for 8% and 50% of
the malware seeds respectively, while EvadeHC still attains
an evasion rate of 62% (Figure 9b).
While it may come as a surprise that EvadeHC attains bet-
ter evasion rate than EvadeGP even though it only assumes
binary output instead of classiﬁcation score as EvadeGP does,
we suspect that this is because of the fact that EvadeHC’s
scoring mechanism is capable of incorporating information
obtained from both the tester and detector and thus is
arguably more informative than the scoring mechanism of
EvadeGP which mainly relies on the classiﬁcation score out-
put by the detector.
5.8 Validating the Hidden-state Morpher
Model
To justify the proposition on states representation of a
sample discussed earlier in Section 4, we generate 200 dif-
ferent morphed samples from the same malware seed, and
measure their average ﬂipping distances (the reject-ﬂipping
distance is measured against Hidost). The average ﬂipping
distances of each sample are computed from 20 diﬀerent
random paths originating from it (the sequences of random
120
100
80
60
40
20
0
−0.2 0 0.2 0.4
(a) x0
80
60
40
20
−0.4−0.2 0 0.2 0.4
0
(b) x1
Figure 11: Eﬀect of morphing on PDFrate score. The
histograms reported changes in classiﬁcation scores after 5
morphing steps.
seeds are the same for all samples), and plotted in Figure 10.
The ﬁgure also shows a certain correlation between the mal-
ice and reject ﬂipping distances. Indeed, the pearson cor-
relation coeﬃcient of the distances is 0.34, conﬁrming our
proposition earlier in Section 4 that they are positively cor-
related.
The proposed model HsrMorpher assumes that the reduc-
tion of the hidden values is independent and identical for
all the samples. To validate that real-life classiﬁers exhibit
such property, we consider PDFrate and treat the inter-
nal classiﬁcation score assigned to a sample as it’s hidden
state. We pick a sample x0 and create another sample x1
by applying a sequence of ﬁve morphing steps on x0. We
then generate 1, 000 random paths of length ﬁve from x0.
For each of the random path, we record the diﬀerence be-
tween classiﬁcation scores of the last sample and x0. Figure
11a shows the histogram of the recorded score diﬀerences.
Similar experiment is carried out on x1 and the histogram is
shown in Figure 11b. Observe that the two histograms are
similar, giving evidences that HsrMoprher is appropriate.
We further study the validity of the HsrMorpher model
by running BiRand and EvadeHC under the abstract model.
Recall that HsrMorpher is parameterised by the random
source S that dictates how the two values (α, β) are chosen.
From previous set of experiments, we observe that a typical
malice-ﬂipping distance is 42, and reject-ﬂipping distance
is 60 (Figure 10). A more detailed analysis of the empiri-
cal data suggests that, with respect to the three blackboxes
deployed in our experiments, α would follow normal distri-
bution N (0.024, 0.01) and β follows N (0.017, 0.011). The
evasion starts with a malicious sample xo with state (1, 1)
and succeeds if it can ﬁnd an evading sample e with state
(a, 0) for some a > 0. We repeat the experiment for 100
times.
Due to space constraint, we only show the average number
of iterations (or paths) that the two approaches traverse in
searching for evading samples under the HsrMorpher model.
The experimental results conﬁrm the consistency of their be-
haviors under HsrMorpher and the empirical studies. This
strongly indicates that the abstract model can indeed serve
as a basis to study and analyze the proposed evasion mech-
anism.
6. DISCUSSION
In this section, we discuss potential mitigation strategies
and implications suggested by our evasion attacks.
6.1 Existing Defensive Mechanism
25
20
15
10
5
0
2
8
10
4
6
Average No. of iterations
20
15
10
5
0
Average No. of paths
1 1.11.21.31.41.51.6·103
(a) EvadeHC
(b) BiRand
Figure 12: Histogram of average number of iterations and
path that EvadeHC and BiRand needs in ﬁnding evading sam-
ples under the abstract model.
Most existing works in evading learning-based classiﬁer
rely on real-value classiﬁcation scores to guide the eva-
sion [35, 21]. Thus, it has been suggested that hiding the
classiﬁcation scores or adding random noise to the scores
may protect the classiﬁer [8]. Another proposed defensive
mechanism is to use a multi-classiﬁer system whose classiﬁ-
cation score is randomly picked from diﬀerent models trained
with disjoint feature sets [10]. Our results show that it is
still feasible to evade learning-based classiﬁers without any
classiﬁcation score, rendering the above-mentioned defensive
mechanisms ineﬀective.
6.2 Potential Mitigation Strategies
Hardening the Malware Threshold.
Previous
work [35, 28] suggested that the classiﬁcation performance
(i.e., false accept/false reject) is typically robust against the
choice of the malware threshold, for original samples that
are not adversarially manipulated would have classiﬁcation
scores close to either of extremes. For example, the authors
of PDFrate reported that adjusting the malware threshold
from 0.2 to 0.8 has negligible eﬀect on accuracy, because
most samples would be assigned scores very close to either
0 or 1 [28]. On the other hand, our experimental study
(Section 5.7) suggests that even a slight change of the
malware threshold could have signiﬁcant impact on the
evasion diﬃculties. Thus, it seems one potential mitigation
strategies is to set the threshold to be more “restrictive”.
This is worth investigating in future works.
Randomization. Another potential mitigation strategy is
to embed into the classiﬁers a certain level of randomness.
In particular, for samples whose classiﬁcation scores are very
close to the threshold, the classiﬁers can ﬂip its classiﬁcation
decision with some probability. Given a proposition that
most samples would have classiﬁcation scores distant from
the threshold, the above-mentioned ﬂipping mechanism
would not have signiﬁcant impact on the classiﬁcation
accuracy. Similar to the previous mitigation strategy, more
study is needed to conclude an eﬀect of this countermeasure.
Identifying Evading Attempts/Samples.
In most
cases, evading samples are found after hundreds of queries to
the detector D. In addition, the samples queried against D
over the course of the evasion would resemble one another,
to a certain extent. Thus, one possible method to detect
evading attempts/samples is to have D remember samples
that have been queried and apply some similarity/clustering-
based techniques to identify evading queries/samples.
other words, D has to remain stateful.
6.3 Evasion for Defense
In
Ironically, an eﬀective evasion algorithm can also be used
to build more secure classiﬁers that are robust against eva-
sion attacks. Indeed, previous works have suggested that one
can enhance the robustness of a learning based model by in-
jecting adversarial examples throughout its training [14, 21].
In particular, those morphed samples that retain the desired
malicious activity, especially the ones that are misclassiﬁed
by the learning based systems, can be included in the train-
ning dataset for the next step of training. We remark that
by formulating the morphing process as random but repeat-
able, our approaches are capable of generating adversarial
examples with great diversity, further enhancing eﬀective-
ness of the adversarial training.
7. RELATED WORK
Evasion attacks against classiﬁers have been studied in
numerous contexts [36, 23, 30, 35]. These works diﬀer in
their assumptions on the adversary’s knowledge about the
detector and how the data could be manipulated.
Intu-
itively, detailed knowledge of the classiﬁer signiﬁcantly ben-
eﬁts the adversary in conducting evasion attacks. ˇSrndic
et al. [31] proposed a taxonomy of evasion scenarios based
on the knowledge about the targeted classiﬁer’s components
that are available to the adversary. These components in-
clude training datasets and classiﬁcation algorithms together
with their parameters, as well as a feature set that the clas-
siﬁer has learnt.
Various attacks against learning-based classiﬁers [9, 31,
25] assume that the adversary has high level of knowledge
about the target system’s internals (e.g., feature sets and
classiﬁcation algorithms), and could directly manipulate the
malicious seeds in the feature space. It is unclear how these
works could be extended to the constrained scenario we con-
sider in this work, wherein the adversary does not have any
knowledge of the feature set and could not manipulate the
data on the feature space.
Xu et al. [35] relax an assumption on the high level of
knowledge about the detector’s feature set, only presume
that the adversary is aware of the manipulation level of
the morphing mechanism and has access to the classiﬁcation
scores output by the target detector. On the contrary, our
technique does not assume any of such knowledge. Interest-
ingly, we show in Section 5.7 that even without relying on
classiﬁcation scores, our proposed algorithm EvadeHC still
attains higher evasion rate against hardened detectors in
comparison with previous work.
The evasion attacks proposed by Papernot et al. [21] re-
quire training a local model that behaves somewhat simi-
lar to the targeted system, then search for evading samples
against such local model, and show that they are also mis-
classiﬁed by the targeted system. The actual evasion hap-
pens on the substituting model whose internal parameters
and training dataset are available to the adversary. Fur-
ther, this approach heavily relies on the transferability of the
adversarial samples. Our solutions, on the other hand, di-
rectly search for evading samples against the targeted classi-
ﬁer without necessitating training a local substituting model
or making any assumption on the transferability of samples.
A line of works on adversarial learning [33, 26] also as-
sume blackbox accesses to the targeted systems, but are dif-
ferent from ours in their adversarial goals. While Tram`er
et al. [33] attempted to extract exact value of each model
parameter and Shokri et al. [26] were interested in inferring
if a data record was in the targeted model’s training dataset,
our focus is on deceiving the target system into misclassify-
ing evading samples.
8. CONCLUSION
We have described EvadeHC, a generic hill-climbing base
approach to evade binary-outcome detector using blackbox
morphing, assuming minimal knowledge about both the de-
tector and the data manipulation mechanism. We have
demonstrated the eﬀectiveness of the proposed approach
against two PDF malware classiﬁers. Although the experi-
ment studies are performed on malware classiﬁer, the pro-
posed technique and its security implications may also be of
wider application to other learning-based systems.
Acknowledgements
We thank Amrit Kumar and Shiqi Shen for helpful discus-
sions and feedback on the early version of the paper. This
research is supported by the National Research Founda-
tion, Prime Minister’s Oﬃce, Singapore under its Corporate
Laboratory@University Scheme, National University of Sin-
gapore, and Singapore Telecommunications Ltd. All opin-
ions and ﬁndings expressed in this work are solely those of
the authors and do not necessarily reﬂect the views of the
sponsor.
9. REFERENCES
[1] Claudio Guarnieri, Alessandro Tanasi, Jurriaan
Bremer, and Mark Schloesser. Cuckoo Sandbox: A
Malware Analysis System. .
http://www.cuckoosandbox.org/.
[2] CVE Details. Adobe Acrobat Reader: Vulnerability
Statistics. http://www.cvedetails.com/product/497/.
[3] Modiﬁed pdfrw. https://github.com/mzweilin/pdfrw.
[4] Nedim ˇSrndi´c and Pavel Laskov. Mimicus: A Library
for Adversarial Classiﬁer Evasion.
https://github.com/srndic/mimicus.
[5] Patrick Maupin. PDFRW: A Pure Python Library
That Reads and Writes PDFs.
https://github.com/pmaupin/pdfrw.
[6] Stephan Chenette. Malicious Documents Archive for
Signature Testing and Research - Contagio Malware
Dump. http://contagiodump.blogspot.sg/2010/08/.
[7] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar.
The security of machine learning. Machine Learning,
2010.
[8] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and
J. D. Tygar. Can machine learning be secure? In
Proceedings of the 2006 ACM Symposium on
Information, computer and communications security,
2006.
[9] B. Biggio, I. Corona, D. Maiorca, B. Nelson,
N. ˇSrndi´c, P. Laskov, G. Giacinto, and F. Roli.
Evasion attacks against machine learning at test time.
In ECML-PKDD, 2013.
[10] B. Biggio, G. Fumera, and F. Roli. Multiple classiﬁer
systems for adversarial classiﬁcation tasks. In MCS,
2009.
[11] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks
against support vector machines. arXiv preprint
arXiv:1206.6389, 2012.
[12] M. Br¨uckner, C. Kanzow, and T. Scheﬀer. Static
prediction games for adversarial learning problems.
Journal of Machine Learning Research, 2012.
[13] M. Cova, C. Kruegel, and G. Vigna. Detection and
analysis of drive-by-download attacks and malicious
javascript code. In WWW, 2010.
[14] I. J. Goodfellow, J. Shlens, and C. Szegedy.
Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572, 2014.
[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep
into rectiﬁers: Surpassing human-level performance on
imagenet classiﬁcation. In ICCV, 2015.
[16] A. S. Incorporated. Pdf reference, sixth edition,
version 1.23. 2006.
[17] J. Katz and Y. Lindell. Introduction to modern
cryptography. CRC Press, 2014.
[18] P. Laskov and N. ˇSrndi´c. Static detection of malicious
javascript-bearing pdf documents. In ACSAC, 2011.
[19] K. Lee, J. Caverlee, and S. Webb. Uncovering social
spammers: social honeypots+ machine learning. In
SIGIR, 2010.
[20] J.-P. M. Linnartz and M. Van Dijk. Analysis of the