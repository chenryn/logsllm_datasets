know that a given user profile or transaction is fixed and won’t be subject to further
change, we can verify its internal state and make suitable copies for recovery pur‐
poses. You can then make incremental backups that only include data that has been
modified or added since your last backup. This technique brings your backup time in
line with your “mainline” processing time, meaning that frequent incremental back‐
ups can save you from the 80-year monolithic verify and copy job.
However, remember that we care about restores, not backups. Let’s say that we took a
full backup three years ago and have been making daily incremental backups since. A
full restore of our data will serially process a chain of over 1,000 highly interdepend‐
ent backups. Each independent backup incurs additional risk of failure, not to men‐
tion the logistical burden of scheduling and the runtime cost of those jobs.
Another way we can reduce the wall time of our copying and verification jobs is to
distribute the load. If we shard our data well, it’s possible to run N tasks in parallel,
with each task responsible for copying and verifying 1/Nth of our data. Doing so
requires some forethought and planning in the schema design and the physical
deployment of our data in order to:
• Balance the data correctly
• Ensure the independence of each shard
• Avoid contention among the concurrent sibling tasks
Between distributing the load horizontally and restricting the work to vertical slices
of the data demarcated by time, we can reduce those eight decades of wall time by
several orders of magnitude, rendering our restores relevant.
How Google SRE Faces the Challenges of Data Integrity | 355
Third Layer: Early Detection
“Bad” data doesn’t sit idly by, it propagates. References to missing or corrupt data are
copied, links fan out, and with every update the overall quality of your datastore goes
down. Subsequent dependent transactions and potential data format changes make
restoring from a given backup more difficult as the clock ticks. The sooner you know
about a data loss, the easier and more complete your recovery can be.
Challenges faced by cloud developers
In high-velocity environments, cloud application and infrastructure services face
many data integrity challenges at runtime, such as:
• Referential integrity between datastores
• Schema changes
• Aging code
• Zero-downtime data migrations
• Evolving integration points with other services
Without conscious engineering effort to track emerging relationships in its data, the
data quality of a successful and growing service degrades over time.
Often, the novice cloud developer who chooses a distributed consistent storage API
(such as Megastore) delegates the integrity of the application’s data to the distributed
consistent algorithm implemented beneath the API (such as Paxos; see Chapter 23).
The developer reasons that the selected API alone will keep the application’s data in
good shape. As a result, they unify all application data into a single storage solution
that guarantees distributed consistency, avoiding referential integrity problems in
exchange for reduced performance and/or scale.
While such algorithms are infallible in theory, their implementations are often rid‐
dled with hacks, optimizations, bugs, and educated guesses. For example: in theory,
Paxos ignores failed compute nodes and can make progress as long as a quorum of
functioning nodes is maintained. In practice, however, ignoring a failed node may
correspond to timeouts, retries, and other failure-handling approaches beneath the
particular Paxos implementation [Cha07]. How long should Paxos try to contact an
unresponsive node before timing it out? When a particular machine fails (perhaps
intermittently) in a certain way, with a certain timing, and at a particular datacenter,
unpredictable behavior results. The larger the scale of an application, the more fre‐
quently the application is affected, unbeknownst, by such inconsistencies. If this logic
holds true even when applied to Paxos implementations (as has been true for Goo‐
gle), then it must be more true for eventually consistent implementations such as
Bigtable (which has also shown to be true). Affected applications have no way to
356 | Chapter 26: Data Integrity: What You Read Is
know that 100% of their data is good until they check: trust storage systems, but
verify!
To complicate this problem, in order to recover from low-grade data corruption or
deletion scenarios, we must recover different subsets of data to different restore
points using different backups, while changes to code and schema may render older
backups ineffective in high-velocity environments.
Out-of-band data validation
To prevent data quality from degrading before users’ eyes, and to detect low-grade
data corruption or data loss scenarios before they become unrecoverable, a system of
out-of-band checks and balances is needed both within and between an application’s
datastores.
Most often, these data validation pipelines are implemented as collections of map-
reductions or Hadoop jobs. Frequently, such pipelines are added as an afterthought to
services that are already popular and successful. Sometimes, such pipelines are first
attempted when services reach scalability limits and are rebuilt from the ground up.
Google has built validators in response to each of these situations.
Shunting some developers to work on a data validation pipeline can slow engineering
velocity in the short term. However, devoting engineering resources to data validation
endows other developers with the courage to move faster in the long run, because the
engineers know that data corruption bugs are less likely to sneak into production
unnoticed. Similar to the effects enjoyed when units test are introduced early in the
project lifecycle, a data validation pipeline results in an overall acceleration of soft‐
ware development projects.
To cite a specific example: Gmail sports a number of data validators, each of which
has detected actual data integrity problems in production. Gmail developers derive
comfort from the knowledge that bugs introducing inconsistencies in production
data are detected within 24 hours, and shudder at the thought of running their data
validators less often than daily. These validators, along with a culture of unit and
regression testing and other best practices, have given Gmail developers the courage
to introduce code changes to Gmail’s production storage implementation more fre‐
quently than once a week.
Out-of-band data validation is tricky to implement correctly. When too strict, even
simple, appropriate changes cause validation to fail. As a result, engineers abandon
data validation altogether. If the data validation isn’t strict enough, user experience–
affecting data corruption can slip through undetected. To find the right balance, only
validate invariants that cause devastation to users.
For example, Google Drive periodically validates that file contents align with listings
in Drive folders. If these two elements don’t align, some files would be missing data—
How Google SRE Faces the Challenges of Data Integrity | 357
a disastrous outcome. Drive infrastructure developers were so invested in data integ‐
rity that they also enhanced their validators to automatically fix such inconsistencies.
This safeguard turned a potential emergency “all-hands-on-deck-omigosh-files-are-
disappearing!” data loss situation in 2013 into a business as usual, “let’s go home and
fix the root cause on Monday,” situation. By transforming emergencies into business
as usual, validators improve engineering morale, quality of life, and predictability.
Out-of-band validators can be expensive at scale. A significant portion of Gmail’s
compute resource footprint supports a collection of daily validators. To compound
this expense, these validators also lower server-side cache hit rates, reducing server-
side responsiveness experienced by users. To mitigate this hit to responsiveness,
Gmail provides a variety of knobs for rate-limiting its validators and periodically
refactors the validators to reduce disk contention. In one such refactoring effort, we
cut the contention for disk spindles by 60% without significantly reducing the scope
of the invariants they covered. While the majority of Gmail’s validators run daily, the
workload of the largest validator is divided into 10–14 shards, with one shard valida‐
ted per day for reasons of scale.
Google Compute Storage is another example of the challenges scale entails to data
validation. When its out-of-band validators could no longer finish within a day,
Compute Storage engineers had to devise a more efficient way to verify its metadata
than use of brute force alone. Similar to its application in data recovery, a tiered strat‐
egy can also be useful in out-of-band data validation. As a service scales, sacrifice
rigor in daily validators. Make sure that daily validators continue to catch the most
disastrous scenarios within 24 hours, but continue with more rigorous validation at
reduced frequency to contain costs and latency.
Troubleshooting failed validations can take significant effort. Causes of an intermit‐
tent failed validation could vanish within minutes, hours, or days. Therefore, the abil‐
ity to rapidly drill down into validation audit logs is essential. Mature Google services
provide on-call engineers with comprehensive documentation and tools to trouble‐
shoot. For example, on-call engineers for Gmail are provided with:
• A suite of playbook entries describing how to respond to a validation failure alert
• A BigQuery-like investigation tool
• A data validation dashboard
Effective out-of-band data validation demands all of the following:
• Validation job management
• Monitoring, alerts, and dashboards
• Rate-limiting features
• Troubleshooting tools
358 | Chapter 26: Data Integrity: What You Read Is
• Production playbooks
• Data validation APIs that make validators easy to add and refactor
The majority of small engineering teams operating at high velocity can’t afford to
design, build, and maintain all of these systems. If they are pressured to do so, the
result is often fragile, limited, and wasteful one-offs that fall quickly into disrepair.
Therefore, structure your engineering teams such that a central infrastructure team
provides a data validation framework for multiple product engineering teams. The
central infrastructure team maintains the out-of-band data validation framework,
while the product engineering teams maintain the custom business logic at the heart
of the validator to keep pace with their evolving products.
Knowing That Data Recovery Will Work
When does a light bulb break? When flicking the switch fails to turn on the light? Not
always—often the bulb had already failed, and you simply notice the failure at the
unresponsive flick of the switch. By then, the room is dark and you’ve stubbed your
toe.
Likewise, your recovery dependencies (meaning mostly, but not only, your backup),
may be in a latent broken state, which you aren’t aware of until you attempt to recover
data.
If you discover that your restore process is broken before you need to rely upon it,
you can address the vulnerability before you fall victim to it: you can take another
backup, provision additional resources, and change your SLO. But to take these
actions proactively, you first have to know they’re needed. To detect these vulnerabili‐
ties:
• Continuously test the recovery process as part of your normal operations
• Set up alerts that fire when a recovery process fails to provide a heartbeat indica‐
tion of its success
What can go wrong with your recovery process? Anything and everything—which is
why the only test that should let you sleep at night is a full end-to-end test. Let the
proof be in the pudding. Even if you recently ran a successful recovery, parts of your
recovery process can still break. If you take away just one lesson from this chapter,
remember that you only know that you can recover your recent state if you actually do
so.
If recovery tests are a manual, staged event, testing becomes an unwelcome bit of
drudgery that isn’t performed either deeply or frequently enough to deserve your
confidence. Therefore, automate these tests whenever possible and then run them
continuously.
How Google SRE Faces the Challenges of Data Integrity | 359
The aspects of your recovery plan you should confirm are myriad:
• Are your backups valid and complete, or are they empty?
• Do you have sufficient machine resources to run all of the setup, restore, and
post-processing tasks that comprise your recovery?
• Does the recovery process complete in reasonable wall time?
• Are you able to monitor the state of your recovery process as it progresses?
• Are you free of critical dependencies on resources outside of your control, such
as access to an offsite media storage vault that isn’t available 24/7?
Our testing has discovered the aforementioned failures, as well as failures of many
other components of a successful data recovery. If we hadn’t discovered these failures
in regular tests—that is, if we came across the failures only when we needed to
recover user data in real emergencies—it’s quite possible that some of Google’s most
successful products today may not have stood the test of time.
Failures are inevitable. If you wait to discover them when you’re under the gun, fac‐
ing a real data loss, you’re playing with fire. If testing forces the failures to happen
before actual catastrophe strikes, you can fix problems before any harm comes to
fruition.
Case Studies
Life imitates art (or in this case, science), and as we predicted, real life has given us
unfortunate and inevitable opportunities to put our data recovery systems and pro‐
cesses to the test, under real-world pressure. Two of the more notable and interesting
of these opportunities are discussed here.
Gmail—February, 2011: Restore from GTape
The first recovery case study we’ll examine was unique in a couple of ways: the num‐
ber of failures that coincided to bring about the data loss, and the fact that it was the
largest use of our last line of defense, the GTape offline backup system.
Sunday, February 27, 2011, late in the evening
The Gmail backup system pager is triggered, displaying a phone number to join a
conference call. The event we had long feared—indeed, the reason for the backup sys‐
tem’s existence—has come to pass: Gmail lost a significant amount of user data.
Despite the system’s many safeguards and internal checks and redundancies, the data
disappeared from Gmail.
360 | Chapter 26: Data Integrity: What You Read Is
This was the first large-scale use of GTape, a global backup system for Gmail, to
restore live customer data. Fortunately, it was not the first such restore, as similar sit‐
uations had been previously simulated many times. Therefore, we were able to:
• Deliver an estimate of how long it would take to restore the majority of the affec‐
ted user accounts
• Restore all of the accounts within several hours of our initial estimate
• Recover 99%+ of the data before the estimated completion time
Was the ability to formulate such an estimate luck? No—our success was the fruit of
planning, adherence to best practices, hard work, and cooperation, and we were glad
to see our investment in each of these elements pay off as well as it did. Google was
able to restore the lost data in a timely manner by executing a plan designed accord‐
ing to the best practices of Defense in Depth and Emergency Preparedness.
When Google publicly revealed that we recovered this data from our previously
undisclosed tape backup system [Slo11], public reaction was a mix of surprise and
amusement. Tape? Doesn’t Google have lots of disks and a fast network to replicate
data this important? Of course Google has such resources, but the principle of
Defense in Depth dictates providing multiple layers of protection to guard against the
breakdown or compromise of any single protection mechanism. Backing up online
systems such as Gmail provides defense in depth at two layers:
• A failure of the internal Gmail redundancy and backup subsystems
• A wide failure or zero-day vulnerability in a device driver or filesystem affecting
the underlying storage medium (disk)
This particular failure resulted from the first scenario—while Gmail had internal
means of recovering lost data, this loss went beyond what internal means could
recover.
One of the most internally celebrated aspects of the Gmail data recovery was the
degree of cooperation and smooth coordination that comprised the recovery. Many
teams, some completely unrelated to Gmail or data recovery, pitched in to help. The
recovery couldn’t have succeeded so smoothly without a central plan to choreograph
such a widely distributed Herculean effort; this plan was the product of regular dress
rehearsals and dry runs. Google’s devotion to emergency preparedness leads us to
view such failures as inevitable. Accepting this inevitability, we don’t hope or bet to
avoid such disasters, but anticipate that they will occur. Thus, we need a plan for deal‐
ing not only with the foreseeable failures, but for some amount of random undiffer‐
entiated breakage, as well.
In short, we always knew that adherence to best practices is important, and it was
good to see that maxim proven true.
Case Studies | 361
Google Music—March 2012: Runaway Deletion Detection
The second failure we’ll examine entails challenges in logistics that are unique to the
scale of the datastore being recovered: where do you store over 5,000 tapes, and how
do you efficiently (or even feasibly) read that much data from offline media in a rea‐
sonable amount of time?
Tuesday, March 6th, 2012, mid-afternoon
Discovering the problem
A Google Music user reports that previously unproblematic tracks are being skipped.
The team responsible for interfacing with Google Music’s users notifies Google Music
engineers. The problem is investigated as a possible media streaming issue.
On March 7th, the investigating engineer discovers that the unplayable track’s meta‐
data is missing a reference that should point to the actual audio data. He is surprised.
The obvious fix is to locate the audio data and reinstate the reference to the data.
However, Google engineering prides itself for a culture of fixing issues at the root, so
the engineer digs deeper.
When he finds the cause of the data integrity lapse, he almost has a heart attack: the
audio reference was removed by a privacy-protecting data deletion pipeline. This part
of Google Music was designed to delete very large numbers of audio tracks in record
time.
Assessing the damage
Google’s privacy policy protects a user’s personal data. As applied to Google Music
specifically, our privacy policy means that music files and relevant metadata are
removed within reasonable time after users delete them. As the popularity of Google
Music soared, the amount of data grew rapidly, so the original deletion implementa‐
tion needed to be redesigned in 2012 to be more efficient. On February 6th, the upda‐
ted data deletion pipeline enjoyed its maiden run, to remove relevant metadata.
Nothing seemed amiss at the time, so a second stage of the pipeline was allowed to
remove the associated audio data too.
Could the engineer’s worst nightmare be true? He immediately sounded the alarm,