When a user profile or transaction is finalized and no longer subject to changes, we can verify its internal state and create suitable copies for recovery purposes. Incremental backups can then be made, which only include data that has been modified or added since the last backup. This approach aligns backup time with the "mainline" processing time, making frequent incremental backups more efficient and avoiding the need for an 80-year monolithic verify and copy job.

However, it's important to remember that the focus should be on restores, not just backups. For instance, if a full backup was taken three years ago and daily incremental backups have been made since, a full restore would require sequentially processing over 1,000 interdependent backups. Each backup introduces additional risk of failure and logistical challenges, such as scheduling and runtime costs.

To reduce the wall time for copying and verification tasks, we can distribute the load. By sharding the data, it’s possible to run N tasks in parallel, with each task responsible for 1/Nth of the data. This requires careful planning in schema design and data deployment to:
- Balance the data correctly
- Ensure the independence of each shard
- Avoid contention among concurrent sibling tasks

By distributing the load horizontally and restricting work to vertical slices of data demarcated by time, we can significantly reduce the wall time, making restores more relevant.

### Third Layer: Early Detection
"Bad" data propagates quickly. References to missing or corrupt data are copied, and links fan out, degrading the overall quality of your datastore with each update. The sooner you detect data loss, the easier and more complete your recovery can be.

### Challenges Faced by Cloud Developers
In high-velocity environments, cloud applications and infrastructure services face several data integrity challenges at runtime, including:
- Referential integrity between datastores
- Schema changes
- Aging code
- Zero-downtime data migrations
- Evolving integration points with other services

Without proactive engineering efforts to track emerging relationships in the data, the data quality of a successful and growing service will degrade over time. Novice cloud developers often delegate data integrity to distributed consistent storage APIs (like Megastore), assuming that the underlying algorithms (such as Paxos) will maintain data consistency. While these algorithms are theoretically infallible, their implementations can be flawed due to hacks, optimizations, bugs, and educated guesses.

For example, Paxos should ignore failed compute nodes and make progress as long as a quorum of functioning nodes is maintained. In practice, however, this may involve timeouts, retries, and other failure-handling approaches, leading to unpredictable behavior, especially at scale. Trusting storage systems without verification can lead to data corruption.

### Out-of-Band Data Validation
To prevent data quality degradation and detect low-grade data corruption before it becomes unrecoverable, a system of out-of-band checks and balances is essential. These data validation pipelines are often implemented as collections of map-reductions or Hadoop jobs. Google, for instance, has built validators in response to various situations, such as scalability limits and afterthoughts in popular services.

While implementing out-of-band data validation can initially slow down engineering velocity, it ultimately accelerates software development by reducing the likelihood of undetected data corruption. For example, Gmail uses multiple data validators that detect and alert on data integrity issues within 24 hours, allowing developers to introduce code changes more frequently.

### Effective Out-of-Band Data Validation
Effective out-of-band data validation requires:
- Validation job management
- Monitoring, alerts, and dashboards
- Rate-limiting features
- Troubleshooting tools
- Production playbooks
- Data validation APIs for easy addition and refactoring

Small engineering teams may struggle to build and maintain these systems, so it's beneficial to have a central infrastructure team provide a data validation framework for multiple product engineering teams.

### Knowing That Data Recovery Will Work
Data recovery dependencies, primarily backups, can be in a latent broken state. To ensure that recovery processes work, continuously test them as part of normal operations and set up alerts for failures. Even if a recent recovery was successful, parts of the process can still break. Automate and run these tests continuously to catch issues early.

### Case Studies

#### Gmail—February 2011: Restore from GTape
On February 27, 2011, Gmail experienced a significant data loss. This was the first large-scale use of the GTape offline backup system to restore live customer data. Despite the complexity, Google was able to restore 99%+ of the data within the estimated time, thanks to planning, best practices, and regular simulations.

#### Google Music—March 2012: Runaway Deletion Detection
On March 6, 2012, a Google Music user reported unplayable tracks. Investigation revealed that a privacy-protecting data deletion pipeline had removed the metadata references. The issue was detected and addressed, highlighting the importance of root-cause analysis and proactive monitoring.

By adhering to best practices and continuously testing recovery processes, organizations can ensure data integrity and minimize the impact of data loss.