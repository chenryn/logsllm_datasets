bytestream data associated with an event; NFs that perform
writes are serialized. This assumption has been shown to be
reasonable for many types of NFs in other studies [28, 30].
Modifying Packets and Stack State: Maintaining consis-
tency of both packet and stack state is complicated when
NFs perform writes. NFs that modify packet (or other event)
data must specify this in their output port definition. The
Controller can then use this to know whether NFs can be
run in parallel. NFs are never allowed to directly update
stack state, since that would cause race conditions between
an NF and the µStack processing other packets in the flow.
Instead, an event-based API is exposed to allow NFs to ma-
nipulate stack state. For example, the TCP µStack subscribes
to FLOW/TCP/TERMINATE events, which might be pub-
lished by a Firewall NF that wants to disallow a connection
and send a RST packet. This avoids concurrency issues with
stack state, but means that NFs must be able to handle asyn-
chronicity, e.g., the Firewall NF may need to drop subsequent
packets already processed by the stack prior to the event.
Parallel Stacks: Finally, to maximize performance in a multi-
core environment it may be necessary to run multiple copies
of the protocol stack on several cores. To support this, Mi-
croboxes uses Receive Side Scaling (RSS) support in the NIC
with a bidirectional flow consistent hash. This ensures that
all packets from a single flow will be sent to the same stacks
and NFs, but allows for different flows to be uniformly dis-
tributed across cores. As shown above, the L2/L3 stack uses
its subscriber table to determine the destination stack type
(A or B) and then publishes packet events to the appropriate
µStack instance, using the RSS value computed by the NIC to
distribute across replicated stacks. NFs can also be replicated
in a similar way (e.g., NF B-1).
In addition to flows, data
structures, such as TCP flow state table, are also partitioned
across cores to avoid contention among parallel stacks.
5 CUSTOMIZING µSTACK MODULES
The type of TCP processing performed on each flow can
be configured by adjusting the type of µStack modules that
events are propagated through. Some stacks also allow fine
grained configuration details to be adjusted on a per-flow
basis to eliminate unnecessary processing. The µStacks build
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
on each other to provide progressively more complex pro-
cessing. Events from these stacks are then sent to NFs that
perform the desired middlebox logic. The Microboxes TCP
stack is broken into the following µStack modules:
Network Layer: When a packet arrives, it first receives
Layer 2/3 processing to determine what flow it is associated
with. This stack maintains minimal state–flow stats such as
packet count and flow status tracking whether the flow is
active or expired. It publishes PKT and FLOW events.
TCP Monitor: A TCP Monitoring stack seeks to track the
TCP state at both the client and server side of a connec-
tion. This allows a monitor NF to reconstruct a bidirectional
bytestream and observe TCP state transitions, for example as
part of a stateful firewall, but does not allow full termination
or arbitrary transformations to the bytestream. To support
such TCP monitoring, we build upon the mOS TCP mid-
dlebox library, which divides stack processing between the
client and server side for each packet [12]. The TCP Monitor
subscribes to PKT from the Network Layer and produces
events including EVENT/DATA_RDY, signifying a change
in the bytestream, and FLOW/TCP, indicating a change to
the TCP state. NFs that monitor these events can further nar-
row down their scope by specifying for each flow whether
to subscribe to events for updates to the state of only the
client, the server, or both. Finally, the processing overhead
of the TCP Monitor can be tuned by specifying whether to
reconstruct the TCP bytestream on a per-flow basis.
This type of stack is useful for middleboxes that observe
TCP state changes or need bytestream reconstruction. NFs
that subscribe to EVENT/DATA_RDY can make in-place
modifications to the bytestream before it is forwarded to
the client or server, but they cannot make arbitrary changes
to the data since changing the size of packets would dis-
rupt the TCP state (i.e., sequence numbers) at the client and
server. Thus the TCP Monitor stack provides a lightweight
TCP library for middleboxes that primarily observe flows at
the transport layer or above.
TCP Splicer: A desirable operation for proxy-type middle-
boxes is the ability to redirect a TCP connection after the
handshake has been established. For example, an HTTP
proxy might observe the contents of a GET request before
selecting a server and forwarding the request [9, 18]. The
Microboxes TCP Splicer stack simplifies this operation by
extending the TCP Monitor stack, without requiring the com-
plexity of a full TCP endpoint. The Splicer µStack works by
subscribing to the TCP Monitor’s PKT/TCP event to detect
when the client initiates a connection to an IP address con-
figured with the Splicer. The Splicer then responds with a
SYN-ACK packet so that it can process the three-way hand-
shake with the client. The Splicer publishes a FLOW_REQ
event once the handshake completes and data has arrived
NICRSSL2/L3L2/L3Subscriber ListNF A2Subscriber LIstNF B2uStack AuStack BuStack ANF A1NF B1NF B1SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
G. Liu et al.
from the client. A user-defined NF hosting the proxy logic
will listen for that event and respond with FLOW_DEST con-
taining the new destination IP. Once the Splicer obtains this
information, it initiates a new TCP handshake with the se-
lected destination server. All subsequent packets in the flow
can then go through a fast path in the Splicer that requires
only simple modifications to the TCP header sequence num-
bers [18], allowing zero-copy TCP splicing.
While this could also be achieved by terminating the con-
nection in the middlebox, like in a split proxy [16], that
would require additional TCP logic, which incurs unnec-
essary overhead if the NF only needs to redirect the flow
without modifying the bytestream.
TCP Endpoint: Microboxes NFs can also act as TCP end-
points, for example to host a cache that can respond directly
to some requests. The TCP Endpoint µStack module further
extends the TCP Monitor to contain full TCP logic (conges-
tion control, retransmissions, etc). It then exports events simi-
lar to a socket API. Our implementation leverages mTCP [13],
on which mOS is also based, which allows NFs to work with
an EPOLL interface to the µStack’s sockets. Microboxes NF
developers can then work directly with the EPOLL interface
popular for high performance socket programming.
Supporting TCP endpoints opens new opportunities in
the types of services that can be deployed. For example,
a Microboxes deployment at an edge cloud might provide
transparent middlebox services for most flows, but directly
terminate and respond to a subset of other flows, with func-
tions such as CDN caches or for IoT data analysis.
TCP Split Proxy and Bytestream NFs: The most com-
plex NF types perform transformations on the bytestreams
traversing them; we call these Bytestream NFs. To do so re-
quires two TCP connections, one with the client and one with
the server. This allows both redirection (similar to the TCP
Splicer stack), as well as arbitrary bytestream transforma-
tions. The Microboxes Proxy µStack module is implemented
as two TCP Endpoint stacks. The Proxy µStack publishes
a FLOW_REQ event when a new connection arrives. The
Bytestream NF subscribes to this message and responds with
a FLOW_DEST event to inform the Proxy how to redirect
the flow. The DATA_RDY message type is used both by the
Proxy µStack and the Bytestream NF to indicate when they
have incoming or outgoing data ready, respectively.
6 MICROBOXES IMPLEMENTATION
Microboxes is built on the OpenNetVM [29] NFV framework,
which provides shared-memory based communication for
NFs running as separate processes or containers, and an
IO management layer for starting/stopping NFs and send-
ing/receiving packets over the NIC. Here we focus on the
Figure 5: Microboxes architecture
implementation of the Microboxes TCP stack and shared
memory-based event communication system.
Stack Modules: Our TCP µStack modules are based on
mOS [12] and mTCP [13], with significant changes to sup-
port customization and a consolidated stack shared by many
NFs. TCP processing in Microboxes is based on the mOS
monitor stack, with additional code from mTCP to support
full endpoint termination. We modified 5.8K lines of C code
in mOS and have an additional 13.5K lines for our NFV man-
agement layer and sample NFs. Our stack incorporates two
key differences from these platforms: 1) we streamline TCP
processing and separate it into our µStack modules to allow
for a modular deployment, and 2) we decouple the stack from
the NF library and use dynamic subscriptions to control how
events are delivered.
mOS and mTCP are designed to be fully-functional, com-
patible TCP stacks, whereas Microboxes seeks to provide
a range of stack types depending on NF needs. Thus the
minimalist Microboxes TCP Monitor µStack does not main-
tain, for example, ACK/retransmission queues, resulting in
a smaller memory footprint and higher performance. For
stack modules that require termination we use mTCP, with
integration into our platform so that packets arrive as events
from the lower-layer stacks instead of directly accessing the
NIC. We also implement the Splicer µStack module which
extends the TCP Monitor to add redirection capabilities.
From our observation that the TCP stack can often be the
bottleneck, we move unnecessary processing out of the stack
and into the NF, for example we extract the event processing
code in mOS to a separate library which is incorporated
into the NFs themselves to perform flow-level event filtering.
Each stack module can be run in its own process, although
we combine the Layer 2/3 and TCP Monitor stacks into one
since L2/3 processing adds minimal extra cost. This separates
the stack and NFs into different protection and performance
domains, providing greater isolation.
Together, these changes allow us to tune the TCP process-
ing performed on a per-flow basis, and ensures that stack
processing is made as lightweight as possible.
Event MemoryPKT/TCPSnapshotpkt*ﬂow*NFV IO (DPDK)   PKT/TCPSnapshotpkt*ﬂow*stack*Stack MemoryFlow StateN-tuple      L2/L3     TCP Mon   Splicer      End Pt  …             …             …             …            …   …             …             …             …            … EventsRingsPkts/DataL2/3 + TCP MonμStackDPISignatureMatcherPKT/TCPDPI_DETECTPKT/TCPPKT/TCPPKT IN…NF and μStack PKT OUTShared Memory PoolsContainersMicroboxes: High Performance NFV with Customizable,
Asynchronous TCP Stacks and Dynamic Subscriptions
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Middlebox App
IDS
µStack
Monitor
Monitor
(DPI+Sig_Match)
(Flow_Stats+Logger) Monitor
Layer 2/3
Layer 4 LB
Description
First a DPI NF uses the nDPI [3] open source library to detect application protocols.
Then the Signature_Match NF examines a set of inspection rules to match attack signatures.
A Flow_Stats NF publishes statistics events for network connections.
A Logger NF records the timestamp and values of each event it receives.
This NF uses a 5-tuple hash to split traffic and rewrite the packet’s destination port and address.
It does not require TCP stack processing.
The HTTP_LB NF uses the HTTP GET request to choose a destination server and balance the load. The
Splicer
Splicer µStack maintains the connection with the client side and initiates the connection to the selected backend.
Split Proxy Establishes separate sockets to both client and server, then uses EPOLL to check for data in events from either
socket, and pushes that data from one socket to another.
This legacy web server can serve static files (jpg/html/etc) using the TCP Endpoint µStack.
Endpoint
Table 2: Sample Microboxes Applications using different µStack modules.
Layer 7 LB
Proxy
Lighttpd
Shared Memory and TCP State: Microboxes sets up two
shared memory pools: Event Memory and Stack Memory as
shown in Figure 5. The first is used to store packets, events,
reassembled flow data, and communication queues which
support zero-copy data movement between NFs and µStacks.
The Stack Memory pool stores the flow tables used to main-
tain per-flow TCP state, including information such as se-
quence numbers and connection status. This data is stored
in a separate pool and can only be accessed via a restrictive
API to keep all state data read-only for NFs.1
Microboxes includes an optimized read path that elimi-
nates all modifications to TCP state to improve concurrency.
First, this is achieved by maintaining TCP state in the shared
Stack Memory, allowing zero-copy access to much of the
data for NFs (with the exception of data copied into stack
snapshots to ensure consistency). Second, we avoid indirec-
tions caused by hash table lookups as much as possible by
providing direct pointers to state table entries and the re-
assembled bytestream with event messages. This can have a
substantial impact on services operating at line rates. Third,
operations to read TCP state are all stateless functions, e.g.,
when querying the bytestream to retrieve data, the API does
not track the position of the last read; instead the API is
stateless and requires the NF to specify the desired offset.
This is in contrast to the original mOS/mTCP APIs, which
updated internal state with some operations. For example,
mtcp_peek updates the offset of already retrieved data,
and mtcp_ppeek has an on-demand memory allocation,
causing these operations to update stack state and requiring
them to be performed sequentially. By making these APIs
stateless, Microboxes has fewer cache invalidations and con-
currency issues to deal with than mOS when extended to
support NF chains.
Microboxes divides flows using RSS (Receive Side Scaling)
so that each TCP µStack instance maintains its own set of
1Microboxes assumes that NFs are not malicious and will not try to corrupt
TCP state or snoop on other flows’ data by using arbitrary memory oper-
ations; we believe this is an acceptable assumption for network provider
operated NFV platforms where NFs are thoroughly vetted before deploy-
ment. For less secure, multi-tenant environments, the state memory pool
could be kept in read-only memory with finer grained access controls.
flows in partitioned memory. This prevents any concurrent
updates to shared cache lines from different stack instances,
which would cause cache coherence traffic.
Stack State Snapshots: Microboxes embeds Stack Snap-
shots in TCP event messages so that the TCP stack can
continue updating flow table state without inconsistencies
from concurrent updates. The stack snapshot only needs to
contain data from the state table that could be potentially
modified when the stack processes subsequent packets in
the flow, but is not accessible in the packet that is referenced
by the event. For example, the latest sequence number of the
sender can be trivially read from the packet itself (referenced
by the event), but the latest sequence number of the receiver
needs to be stored in the stack snapshot since it is not avail-
able in packet data and may be updated if a return packet
arrives. Similarly, the current connection status (e.g., await-
ing SYN-ACK, connected, etc.) is not found in the packet and
could be updated, so it also must be included in the snap-
shot. An offset pointer for the reconstructed bytestream is
provided in the snapshot, allowing the stack to append data
from subsequent packets of the same flow without affecting
consistency. In total, the stack snapshot is only 23 bytes,
which is substantially smaller than the full state. The time
for making the snapshot for each packet is around 76 cycles,
even for a large number of flows.
Event Messages: For each event, a single message data
structure is allocated in the shared Event Memory pool by its
publisher. The producing NF then checks its subscription list
and adds a pointer to the message data structure for the event