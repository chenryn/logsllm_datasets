system and network. Use of asyncio allows SoMeta to be single-
threaded, which helps to limit its performance impact on the host
system on which it runs. Although asyncio’s asynchronous task
scheduling is by no means perfect, we argue that it is sufficient for
the purpose of collecting the types of metadata we envision.
The task of metadata collection is delegated to a set of moni-
tors built in to SoMeta, which we describe below. When SoMeta is
Automatic Metadata Generation for Active Measurement
IMC ’17, November 1–3, 2017, London, United Kingdom
started up, a user must configure some number of monitors, and also
supplies the command line for executing an external measurement
tool, such as scamper [20]. For as long as the external tool exe-
cutes, SoMeta’s monitors periodically collect system performance
measures or other metadata. When the external tool completes,
SoMeta writes out the metadata to a JSON file along with a variety
of invocation and runtime information such as the full command
line used to start SoMeta, the active measurement command, exit
status, and any text written to the console by the external measure-
ment tool.
3.2.1 Monitoring capabilities. The core functionality of SoMeta is
to monitor, at discrete intervals, host system resources and network
round-trip time (RTT) latencies to specific target hosts. SoMeta can
be configured to collect CPU (e.g., per-core utilization), I/O (e.g.,
read/write activity counters), memory (e.g., utilization), and net-
work interface byte, packet, and drop counters. In addition, a mon-
itor module can be configured to collect RTT samples using hop-
limited probes, or by emitting ICMP echo request (ping) packets
toward specific hosts.
There are five built-in monitor modules available, named cpu,
mem, io, netstat, and rtt. Each of these modules can be sepa-
rately configured, and each executes independently within SoMeta’s
asyncio event loop as a separate coroutine. The discrete interval
at which measurements are collected is configurable, and defaults
to one second. When SoMeta is started, each configured monitor
is initiated after a small random amount of time in order to avoid
synchronization. The cpu, mem, io and netstat monitors leverage
2 module for collecting a variety of system
the widely-used psutil
performance measures. Use of this third-party module ensures that
SoMeta can perform effectively on a wide selection of operating
systems, and facilitates the future creation of special-purpose moni-
tors such as battery status or CPU and/or environment temperature,
which can be accessed through existing psutil APIs. Lastly, we
note that the monitoring framework has been designed to be easily
extensible in order to permit new types of metadata to be collected
and stored, or to be able to use an OS-specific performance moni-
toring subsystem such as System Activity Reporter (sar).
The rtt monitor module can be configured to emit either hop-
limited probes (with configurable TTL) using ICMP, UDP, or TCP
toward a destination address, or to emit ICMP echo request probes.
The first four bytes of the transport header can be made constant
to avoid measurement anomalies caused by load balancing [3, 26].
The rtt module uses the Switchyard framework to access libpcap
for sending and receiving packets, and for constructing and parsing
packet headers [29]. Although the system ARP table is used to
bootstrap its operation, the rtt module contains ARP functionality.
Moreover, the system forwarding table is retrieved on startup in
order to determine the correct interface out which packets should
be emitted. Lastly, probes are sent according to an Erlang (Gamma)
process [4], and the probe rate is configurable. By default, probes
are emitted, on average, every second.
Within the rtt module, we configure a libpcap filter on a pcap
device so that only packets of interest to the module are received. In
particular, the module only ever receives ARP responses or probe
request and response packets. Since SoMeta’s rtt module uses
2https://github.com/giampaolo/psutil
libpcap, timestamps for both sent and received packets are of
relatively high quality: at worst they come from the OS, and at best
they may come directly from the NIC. Unfortunately, however, use
of libpcap does not imply that operating system differences can
be ignored. In particular, on the Linux platform, libpcap uses a
special socket (PF_PACKET socket) for sending and receiving raw
frames rather than /dev/bpf or a similar type of device on BSD-
derived platforms (e.g., macOS and FreeBSD). A limitation with
Linux’s PF_PACKET socket is that packets that are emitted through
the device cannot also be received on that same device. In order to
obtain kernel-level timestamps on packets sent on a Linux system,
we create a separate PF_PACKET socket for sending packets. There
are yet other (more minor) quirks that are handled within the rtt
module to smooth out platform differences.
3.2.2 Metadata analysis and visualization. Along with SoMeta’s
metadata collection capabilities, we have created simple tools to
bootstrap analysis and visualization of metadata. An analysis tool
can produce summary statistics of round-trip times, flag whether
any probes were dropped at an interface or by libpcap, and whether
there were periods of full CPU utilization, among other capabilities.
A plotting tool can produce timeseries or empirical cumulative
distribution function plots of any metadata collected, facilitating
qualitative analysis to identify time periods during which measure-
ments may have been disturbed due to host or network interference.
The plotting capability is based on matplotlib [16] and provides
functionality for plotting individual monitor metrics, all metrics
collected by a monitor, or all metrics across all monitors.
4 EVALUATION
In this section we describe a set of experiments to evaluate SoMeta.
We begin by assessing the performance cost of running SoMeta. We
follow that by examining how SoMeta might be used in practice.
4.1 SoMeta Performance and Overhead
To evaluate the performance costs of running SoMeta, we created
a simple laboratory testbed, which was also connected to a cam-
pus network and the Internet. The platforms on which we ran
SoMeta are two versions of the Raspberry Pi—specifically, a Model
1 B Rev. 2 (Pi1) and a Model 3 B (Pi3)—and two server-class systems.
The Pi1 has a 700 MHz single-core ARM1176JZF-S with 512 MB
RAM and ran the Linux 4.1.19 kernel (Raspbian 7). The Pi3 has a
1.2GHz 64-bit quad-core ARM Cortex-A53 CPU, 1 GB RAM and ran
the Linux 4.4.50 kernel (Raspbian 8). The two server-class systems
are identical Octocore Intel(R) Xeon(R) CPU E5530 @2.40GHz with
16 GB RAM. One was installed with Linux 3.13 (Ubuntu server
14.04) and the other with FreeBSD 10.3. These four systems were
connected through a switch via 100 Mb/s Ethernet (Pi1 and Pi3) or
1 Gb/s Ethernet (two server systems) to a series of two Linux-based
routers, the second of which was connected to the campus network
via a Cisco 6500.
Using the two Pi and two server systems, we ran SoMeta in a se-
ries of configurations to test its resource consumption. In particular,
there were ten configurations we used: (1) only collect CPU perfor-
mance measurements every second, (2) only collect RTT measures
to the closest Linux router using ICMP echo requests, (3) only col-
lect RTT measures to the closest Linux router using a hop-limited
IMC ’17, November 1–3, 2017, London, United Kingdom
Joel Sommers, Ramakrishnan Durairajan, and Paul Barford
Figure 1: CPU performance overhead results (top: Raspberry
Pi Model 1 B; bottom: Raspberry Pi Model 3 B).
Figure 2: RTT performance overhead results (top: Raspberry
Pi Model 1 B; bottom: FreeBSD server).
probe (with maximum TTL of 1), (4) collect performance measures
using all monitors every second, including measuring RTT to the
closest Linux router using ICMP echo requests, and (5) using all
monitors to collect performance measurements every second but
collecting RTT measures using a hop-limited probe, again to the
closest Linux router. Configurations 6–10 were identical to config-
urations 1–5 except that instead of collecting measurements every
second, SoMeta was configured to collect measurements every 5
seconds. Each of these 10 experiments was run for 900 seconds
by having SoMeta run the command sleep 900 (i.e., the sleep
command is used as the external “measurement” tool). For these
experiments we pinned SoMeta to a single CPU core which, while
not strictly necessary, simplified analysis of CPU usage and system
overhead since 3 out of 4 systems we used were multicore.
Figures 1 and 2 show selected results for the performance over-
head experiments. Figure 1 shows empirical CDFs for CPU idle
percent for the Pi1 and Pi3, and Figure 2 shows empirical CDFs
for RTT for the Pi1 and the FreeBSD server. In the top plot of
Figure 1, we observe that enabling all monitors with a 1 second
average sampling interval incurs the greatest CPU load. Still, the
50th percentile CPU idle percentage is about 88%, which we view
as promising since SoMeta is written in a very high-level language
and has not undergone (at this point) any performance optimiza-
tion. We also observe that when reducing the sampling rate to
an average of once every 5 seconds, the 50th percentile CPU idle
percentage goes up to 97%, which is only 1–2 percent lower than
only measuring CPU usage every second. In the lower plot of Fig-
ure 1 we observe similar trends, but with different specific CPU
idle percentage values. The higher-powered Pi3 is about 97% idle
at the 50th percentile when running all monitors and collecting
measurements, on average, every second. For the two server-class
systems (not shown), the idle percentage is even higher than the
Pi3, at about 98–99% for all monitors with a 1 second sampling in-
terval. Although SoMeta’s performance overhead is modest, it may
yet be too high in shared measurement environments where either
multiple instances of SoMeta may need to be run or its metadata
collection architecture may need to be adapted. We are addressing
this issue in our ongoing work.
In Figure 2 we show RTT results when running all monitors or
only the RTT monitor, and for both ICMP echo request and hop-
limited probes, each for a 1 second average measurement interval.
We observe that the RTTs are, in general, small and distributed
across a narrow range. Given the Pi1’s lower performing CPU, it is
not surprising that the 90th percentile RTT (≈ 0.35 milliseconds) is
more than twice that of the 90th percentile RTT for the FreeBSD
server (≈0.15 milliseconds). The 90th percentile RTT for the Pi3
is between the two (≈0.25 milliseconds) and results for the Linux
server are similar to the FreeBSD server. We note that none of these
systems were configured to perform hardware timestamping on
the NIC (it is not supported on the Pi devices, and the two server
80.082.585.087.590.092.595.097.5100.0CPU idle0.00.20.40.60.81.0Cumulative fractionCPU only (1s)CPU only (5s)All monitors (ping, 1s)All monitors (ping, 5s)All monitors (hop-limited, 1s)All monitors (hop-limited, 5s)80.082.585.087.590.092.595.097.5100.0CPU idle0.00.20.40.60.81.0Cumulative fractionCPU only (1s)CPU only (5s)All monitors (ping, 1s)All monitors (ping, 5s)All monitors (hop-limited, 1s)All monitors (hop-limited, 5s)0.00.10.20.30.40.5Round-trip time (milliseconds)0.00.20.40.60.81.0Cumulative fractionRTT only (ping, 1s)RTT only (hop-limited, 1s)All monitors (ping, 1s)All monitors (hop-limited, 1s)0.00.10.20.30.40.5Round-trip time (milliseconds)0.00.20.40.60.81.0Cumulative fractionRTT only (ping, 1s)RTT only (hop-limited, 1s)All monitors (ping, 1s)All monitors (hop-limited, 1s)Automatic Metadata Generation for Active Measurement
IMC ’17, November 1–3, 2017, London, United Kingdom
systems were not configured to do so). Thus all timestamps used to
generate these results were generated in software, in the OS kernel.
Figure 3: Performance of the Pi1 and Pi3 when configured
for a range of probing rates.
Finally, to better understand SoMeta’s scheduling accuracy and
to examine finer probing intervals which may be needed to monitor
certain experiments, we examine its performance when configured
to emit probes in an increasingly rapid manner, from 1 probe/s up
to 200 probes/s. We ran each of these experiments for 60 sec by
having SoMeta execute the command sleep 60, and we configured
SoMeta with only the CPU and RTT monitors (in hop-limited mode).
Figure 3 shows the results for these experiments for the Pi1 and
Pi3. We observe that for the Pi1 the CPU is fully utilized at about
25 probes/s, and that the Pi3 can support 150 probes/s with about
25% idle CPU. (CPU utilization on the Pi3 hits 100% around 250
probes/s). We note again that no special performance tuning has
yet been done on SoMeta, including any attempt to compensate
for scheduling inaccuracies with the asyncio module. Importantly,
there were zero packet drops for all experiments on both Pi devices,
and the RTTs measured were statistically identical to those in the
overhead experiments described above. This observation implies
that even when the host system is under significant load, metadata
collected by SoMeta remain accurate, even if they are not gathered
according to the intended schedule. We also examined probe send
time accuracy, which we found to be generally accurate on the
Pi3 when the system was lightly loaded, but poorer on the Pi1,
similar to the findings of [22]. In summary, our experiments show
that using asyncio for task scheduling appears to be sufficient for
modest probe rates but we are nonetheless continuing to examine
how to improve scheduling fidelity.
4.2 Artificial System Load Experiments
For the results described in this section, our goal is to illustrate
how metadata collected by SoMeta could be used to identify and,
to a certain extent, evaluate the impact of system and network
interference on network measurements. For these experiments,
we configured SoMeta to use all built-in monitors and to gather
measurements every second. We also configured it to start the