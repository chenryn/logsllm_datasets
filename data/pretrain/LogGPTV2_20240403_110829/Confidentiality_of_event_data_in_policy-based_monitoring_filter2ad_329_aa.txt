title:Confidentiality of event data in policy-based monitoring
author:Mirko Montanari and
Roy H. Campbell
Conﬁdentiality of Event Data in Policy-based
Monitoring
Mirko Montanari, Roy H. Campbell
University of Illinois at Urbana-Champaign
{mmontan2, rhc}@illinois.edu
Abstract—Monitoring systems observe important information
that could be a valuable resource to malicious users: attackers
can use the knowledge of topology information, application logs,
or conﬁguration data to target attacks and make them hard to
detect. The increasing need for correlating information across
distributed systems to better detect potential attacks and to meet
regulatory requirements can potentially exacerbate the problem
if the monitoring is centralized. A single zero-day vulnerability
would permit an attacker to access all information.
This paper introduces a novel algorithm for performing
policy-based security monitoring. We use policies to distribute
information across several hosts, so that any host compromise
has limited impact on the conﬁdentiality of the data about
the overall system. Experiments show that our solution spreads
information uniformly across distributed monitoring hosts and
forces attackers to perform multiple actions to acquire important
data.
Index Terms—security; monitoring; policy compliance; conﬁ-
dentiality; distributed systems;
I. INTRODUCTION
Directed attacks toward organizations are becoming com-
monplace. While these attacks often use targeted ﬁshing
emails for getting a foothold into the organization, once inside
they require information about network topology, ﬁrewalls,
and placement of critical systems to further perpetrate the
attack [1]. Attackers need to identify critical resources, and
malware needs to target systems with speciﬁc characteristics in
order to exploit vulnerabilities. Searching for such information
through the network using port scanning or using random
infections of non-critical systems increases the chance of
detecting the malware process [2]. The knowledge of net-
work topology information, conﬁgurations, and critical system
placement enables attackers to speciﬁcally target sensitive data
and lowers the chance of detection.
Monitoring systems are a perfect target for acquiring such
information. The increased need for situational awareness and
policy compliance require integrating and correlating events
coming from multiple sources. For example, software such as
Splunk [3], Bro [4], and SEC [5] integrate events generated
by logs, by network packet analysis, and by SNMP. However,
while this monitoring helps detect attacks, the integration of
information creates a large target for attacks that, if exploited,
provides an attacker access to a large amount of information
about the system’s state.
This paper presents an algorithm aimed at protecting the
conﬁdentiality of the information produced by the monitoring
system. We take advantage of the fact
the scale of
that
978-1-4673-1625-5/12/$31.00 ©2012 IEEE
modern infrastructure systems already requires the use of
several hosts for distributing the load of monitoring (e.g., [6]).
Instead of concentrating information in a single system, we
distribute knowledge about the infrastructure into multiple
monitoring servers that collaborate for detecting violations
of security policies. The centralization of information used
in other monitoring systems relies on the assumption that
securing a single system is simpler than securing multiple
systems. However, recent compromises of critical systems
such as certiﬁcation authorities [7], targeted attacks [1], and
the presence of zero-days vulnerabilities challenge such an
assumption. For example, the exploitation of a single zero-day
vulnerability in the monitoring server would allow an attacker
to acquire all
information about the infrastructure. In our
system, the exploitation of a single monitoring server would
reveal only limited information. Additionally, our system is
able to distribute the load across a large number of servers,
thus enabling policy compliance to scale up to large-scale
infrastructure systems.
We focus our analysis on policy-based monitoring systems.
Many current monitoring systems use policies for analyzing
and for correlating the events collected from the infrastructure.
Such an approach is already used in several applications
(e.g., [5], [4], [8]), and advanced applications of policy-based
monitoring have been proposed for validating the compliance
to regulatory policies such as PCI [9] or FISMA [10]. These
policies are generally called “event correlation policies” as
they express conditions over the logic co-occurrence of events.
For example, the co-occurrence of an IDS event indicating the
detection of an exploit packet and of a vulnerability-scanner
event indicating the presence of a software vulnerable to such
an exploit signiﬁes the possible compromise of a device. The
events used by these systems are generated by a large number
of devices using a variety of sources such as SNMP data,
intrusion detection systems (IDS), and log-analysis tools. In
our architecture, these independent sources of information
send events to a large number of monitoring servers distributed
across the organizations.
We show that by expressing policies using Datalog
rules [11] we can perform a decentralized event correlation
that does not require concentrating events in any single system
for processing. Using Datalog, we describe systems as a set of
resources (e.g., computer systems, software systems, network
connections) and their relations (e.g., a computer system is
running a software program). We analyze the rules to identify
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:20:22 UTC from IEEE Xplore.  Restrictions apply. 
the events that need to be correlated for the identiﬁcation
of policy violations. We use a resource-based distribution of
information across multiple servers, and we rewrite each Dat-
alog policy in a set of “resource-centric” rules. Each resource-
centric rule correlates information about a single resource in
the system. The results of this process are forwarded to other
servers to be correlated with events related to other resources,
but only if they can potentially contribute to the detection of
a policy violation.
The contribution of this paper is summarized as follows:
1) We introduce a resource-based approach to distribute
policy compliance monitoring across multiple hosts.
2) We present an algorithm for rewriting Datalog rules
and performing a distributed resource-based validation
of complex policies.
3) We analyze monitoring events from real datasets and we
evaluate experimentally the efﬁciency of our technique.
Results show that our approach provides a 3-fold reduc-
tion of the number of events obtainable by an attacker
when compared to other distributed approaches.
The rest of the paper is organized as follows. Section II
analyzes related work in the area. Section III describes our
model of event co-occurrence for policy-compliance systems.
Section IV describes the adversary models we consider in
this work. Section V introduces our algorithm. Section VI
describes our experimental evaluation. Section VII describes
the limitation of our approach and our future work. Finally,
Section VIII concludes our work.
II. RELATED WORK
Monitoring is a widespread service in modern systems. Most
monitoring systems provide some limited protection of log
data conﬁdentiality. A basic protection of conﬁdentiality is
provided by protecting data-in-transit. For example, syslog-
ng [13] uses TLS to transmit encrypted log data from the
devices and the monitoring servers. However, the encryption
of data-in-transit only leaves data vulnerable: if the monitoring
server is compromised, the attacker has access to all past and
future data.
More advanced solutions provide protection of the data-at-
rest by creating encrypted and tamper-proof audit logs that
can be accessed only by authorized users. One of the earliest
mechanisms has been introduced by Schneier et al. [14]. Their
approach uses one-way hash chains to protect the log ﬁles
from modiﬁcations. Additionally, logs are encrypted to protect
them from unauthorized access. Other solutions (e.g., Ma
et al. [15]) extend such an approach to provide additional
integrity protection. While these approaches are useful for
protecting the integrity of the event logs and can be used in
conjunction with our algorithm to provide trusted audit logs,
their conﬁdentiality protection is not suited for our scenario: as
event correlation requires performing processing on the data,
events need to be accessible to the monitoring server. Attackers
compromising the server would have access to such data.
The Intrusion Detection System Bro [4] provides a dis-
tributed mechanism for performing event correlation. Com-
munication between Bro nodes is performed on top of SSL
to protect data-in-transit. Correlation between events is per-
formed by programming policies using a pub/sub mechanism.
For example, a distributed IDS cluster built on top of Bro has
been presented by Vallentin et al. [6]. They use a ﬂow-based
hashing for distributing the packet-processing load across the
instances. Inter-ﬂow correlation is performed using the pub/sub
mechanism. While the pub/sub mechanism provides ﬂexibility
in specifying policies and in deﬁning their evaluation,
it
provides no guarantees that information about the system is
distributed across nodes. It is up to the programmer to evaluate
policies without creating such an aggregation of information.
The algorithm we present in this work provides a mechanism
for selecting automatically the events that each monitoring
server should receive and send for validating policies. Our
algorithm ensures that potential policy violations are detected
and that information about the system is distributed across a
large number of hosts.
More generally,
the problem of protecting the privacy
of
log data has also been addressed in the context of
sharing network traces across organizations. Several authors
(e.g., [16] [17] [18] [19]) point out the security problems in
providing access to this type of information to external entities
and propose methods for anonymizing the data. However, such
anonymization methods are not applicable to our case for
several reasons. First, these techniques rely on aggregating
data in a centralized server within each organization for
processing. Second, even if anonymization could be performed
directly on devices, these techniques are speciﬁc to network
traces and they are not easily generalizable to the problem of
policy compliance.
Other work focuses on protecting the monitoring system
itself from compromises. For example, recently several secure
monitoring solutions have been using Virtual Machine Intro-
spection (VMI) for protecting the monitoring software from
compromises. They run the monitoring software in a separated
VM co-located with the host to monitor (e.g., Livewire [20])
and they access information by analyzing memory and disk
data without the OS mediation. The security of these systems
relies on the fact that compromising the monitoring VM is
harder than compromising other VMs: the monitoring VM
runs a small amount of software and, hence, exposes a small
attack surface. However, while this assumption holds if the
monitoring VM is used only for acquiring events from a
particular system, it does not hold in the processing servers
that correlate events across entire organizations. Such hosts
need to be accessible through the network to allow devices
to send events to them, and they need to run a substantial
amount of software for validating policies and for providing
network administrators access to data. Our algorithm reduces
the consequences of compromises of such servers.
Previous work provides mechanisms for reducing the num-
ber of events sent to the event correlation nodes by performing
part of the compliance monitoring on each host [21]. Our
work focuses on protecting the remaining part of the events
that cannot be processed locally. Other work introduces an
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:20:22 UTC from IEEE Xplore.  Restrictions apply. 
architecture for compliance that distributes information across
several hosts [22]. However, their approach is limited to RDF
policies and its ability of distributing events in a real scenario
is unclear. We use more general Datalog policies and we
provide a more detailed evaluation of the advantages provided
by distributing data.
Our work is based on the principles of intrusion toler-
ance [12]. We exploit the distributed nature of the problem
of event-correlation to provide a speciﬁc intrusion-tolerant
solution.
III. POLICY COMPLIANCE AND EVENT PROCESSING
The management of the security of large infrastructure
systems often relies on deﬁning “policies.” Policies are rules
that specify high-level security requirements and are used
for selecting the proper states and conﬁgurations of systems.
In the past few years there has been an increasing interest
for the development of policies that apply to entire classes
of industries. Regulatory entities and industries introduced
classes of policies such as PCI-DSS [9] for credit card indus-
tries, NERC CIP [23] for power grid systems. Several of the
policies speciﬁed in NERC CIP or PCI-DSS relate to network,
OS, and application conﬁgurations. Their goal is to specify a
minimal level of security to which all systems need to comply.
Network administrators can monitor the compliance of their
systems to these policies by specifying monitoring rules. These
rules validate the conﬁguration of security devices, of server
applications, and of end-host computer systems. For example,
a completely automated monitoring for compliance to one of
NERC CIP policy requires deﬁning rules that detect when
machines are used by the critical devices that control the power
grid, and that ensure that such machines are placed within a
protected electronic perimeter (e.g., ﬁrewalls).
A. Monitoring Rules
Monitoring systems generate events when there is an “inter-
esting” change in the state of the system. Examples of events
are the running of a new program, the log-in of a user, or the
detection of a potential attack by an IDS. Monitoring rules
check for the co-occurrence of events to identify when the
system is operating in an undesirable state. Monitoring rules
could deﬁne security requirements in a way which is orthog-
onal to the methods used for enforcing security and they can
be used for providing an audit trace. For example, the NERC
CIP policy requirement above might be implemented using
ﬁrewall systems. A monitoring rule might monitor for events
that identify critical systems and for events that indicate the
presence of connections from outside the electronic perimeter.
In the rest of the paper we use the terms monitoring rule and
policy interchangeably to indicate an event-correlation rule.
Events in the policy compliance and network management
scenarios generally report information about some “resource”
in the system. A resource can be a computer system, an IP,
a network, a software program, or a user. For example, an
event indicating that a computer system is now connected to
a new network represents a new relation between a resource
representing a computer system and a resource representing
the network. In this framework, monitoring rules express
conditions over resources in the system and their relations.
For example, a security policy might pose a condition that
a resource of type critical system cannot be connected to a
resource of type public Internet.
the type of an event
Datalog provides a logic-based language for representing
resources and their relations. We use Datalog with the addition
of time operators [24] for specifying events and rules. In
Datalog,
is described by its predi-
cate, while the resources that the event describes are the
parameters of the predicate. For example, a computer host
connected to a network net with an IP ip is represented as
connected(host, net, ip). We assume that all resources
(e.g., host, net, ip) are identiﬁed by unique names. Logic
and Datalog have been used by other work to express policies
and to evaluate the security of systems (e.g., [25]).
Events are associated with timestamps in a way similar to
the concept of situation of Amit [26]. Simple events, such
as the detection of a malicious packet from an IDS, are
instantaneous and are associated with a single timestamp.
Complex events represent states in the system and they are
represented by two timestamps: a start timestamp and an end
timestamp (that might be unknown if the system is still in the
state). Rules in this context can express window-based event
correlation and a state-based correlation (i.e., event correlation
that is based on reconstructing the current state of the system).
These two models can express infrastructure policies currently
deﬁned in regulatory documents. The process of validating the
compliance of a network system to the policies is performed
by integrating events in a knowledge base (KB), and by
checking if any of the monitoring rules trigger the presence
of a violation.
For example, we can consider a new event outperimeter
generated when a network is not protected by a ﬁrewall.
We express a policy stating that a computer should not be
connected to a network not protected by a ﬁrewall using the
following formula:
connected(H, N, IP ),outperimeter(N )
→violation(H, N ).
(1)
The parameters of the events in capital letters are variables.
The policy requires checking for the co-occurrence of the
events connected and outperimeter, with the proper pa-
rameters. We support the different types of time relations that
can be deﬁned over time windows. Timestamps are added as
implicit parameters of the events and time speciﬁcations are
translated into conditions over events’ start and end times.
IV. ADVERSARY MODEL
We deﬁne an attacker, Eve, interested in acquiring more
information about the state and the structure of the infrastruc-
ture system. We assume that the attacker has a limited initial
knowledge and she is interested in compromising the monitor-
ing system to acquire more knowledge. While compromising
the monitoring system is not the only method for acquiring