216
1
Table 1: Number of vertexes returned by ﬁve different diag-
nostic techniques; for SDN4, the two rounds of DiffProv are
shown separately. DiffProv was able to pinpoint the “root
causes” with one or two vertexes in each case, while the
other techniques return more complex responses.
almost all the emitted words end up at a different re-
ducer node than before (TG).
• MR2-D and MR2-I: Code changes. The user deploys
a new implementation of the mapper, but it has a bug
that causes the ﬁrst word of each line to be omitted. As
a result, the job now produces a different output (TB)
than before (TG) for a previously used input ﬁle.
6.3 Usability
We begin with a series of experiments to verify that differ-
ential provenance indeed provides a more concise explana-
tion of the “root cause” than classical provenance. For this
purpose, we ran two conventional provenance queries using
Y! [30] to obtain the “good” and the “bad” provenance trees
for each of the ﬁve diagnostic scenarios, as well as a differ-
ential provenance query using DiffProv. We also evaluated
a simple strawman from Section 2.5, where we performed a
plain tree diff based on the number of distinct nodes, in the
hope that the querier would recognize suspicious gaps. We
then counted the number of vertexes in each result.
Table 1 shows our results. As expected, the plain prove-
nance trees typically contain hundreds of vertexes, which
would have to be navigated and parsed by the human querier
to extract the actual root cause. The plain diff is not signiﬁ-
cantly simpler – in fact, it sometimes contains more vertexes
than either of the individual trees! (We have discussed the
reason for this in Section 2.5.) Therefore, it would still re-
quire considerable effort to identify tuples that should not
be there (e.g., ﬂow entries that should not have been used)
or to guess tuples that are missing. In contrast, differential
provenance always returned very few tuples.
In more detail, for SDN1–SDN4, DiffProv returned the
missing (or broken) ﬂow entries as the root cause; for MR1-
I, DiffProv returned mapreduce.job.reduces – the
ﬁeld in the conﬁguration ﬁle that speciﬁes the number of
reducers; for MR2-I, though DiffProv cannot reason about
the internals of the actual mapper code, it was still able to
pinpoint the version of the mapper code (identiﬁed by the
checksum of its Java bytecode) that caused the error; for
MR1-D and MR2-D, DiffProv returned those ﬁelds’ declar-
ative equivalents in the NDlog model.
)
s
/
B
M
(
e
a
r
t
i
g
n
g
g
o
L
 300
 100
 10
 1
.03
.29
2.8
283.7
28.4
 40
 35
 30
 25
 20
 15
 10
 5
)
s
/
B
M
(
e
a
r
t
i
g
n
g
g
o
L
1Mbps
10Mbps
100Mbps
1Gbps
10Gbps
500B
700B
900B
1100B
1300B
1500B
Traffic rate
Packet size
Figure 5: Logging rate for different trafﬁc rates.
Figure 6: Logging rate with different packet sizes at 1Gbps.
To test how DiffProv handles unsuitable reference events,
we issued ten additional queries in the SDN1 and MR1-
D scenarios for which we picked a reference event at ran-
dom.
(We applied a simple ﬁlter to avoid picking events
that we knew were suitable references.) As expected, Diff-
Prov failed with an error message in all cases. In three of
the cases, the supplied reference event was not comparable
with the event of interest because their seeds had different
types; for instance, one seed was a MapReduce operation but
the other was a conﬁguration entry. In the remaining seven
cases, aligning the trees would have required changes to “im-
mutable” tuples; for instance, the packet of interest entered
the network at one ingress switch and the reference packet
at another. In all cases, DiffProv’s output clearly indicated
what aspect of the chosen reference event was causing the
problem; this would have helped the operator pick a more
suitable reference.
6.4 Cost: Latency
Next, we evaluated the runtime costs of our prototype, start-
ing with the latency overhead incurred by logging. For the
SDN setup, we streamed 2.5 million 500-byte packets through
the SDN1 scenario, and measured the average latency inﬂa-
tion of our prototype to process one packet when logging is
enabled. For the MapReduce setup, we processed a 12.8 GB
Wikipedia dataset in the MR1-I scenario, and recorded the
extra time it took to run the same job with logging enabled.
We observed that the latency is increased by 6.7% in the ﬁrst
experiment, and 2.3% in the second.
We note that our prototype was not optimized for latency,
so it should be possible to further reduce this cost. For in-
stance, the Y! system [30] was able to record provenance in
a native Trema OpenFlow controller with a latency overhead
of only 1.6%, and a similar approach should work in our set-
ting. In the MapReduce scenario, the dominating cost was
getting the checksums of the data ﬁles in HDFS. Instead of
computing these checksums every time a ﬁle is read (as in
our prototype), it would be possible to compute them only
when ﬁles are created or changed. We tested this optimiza-
tion in our prototype, and it reduced the latency cost to 0.2%.
6.5 Cost: Storage
Next, we evaluate the storage cost of logging at runtime. We
varied the trafﬁc rates in the SDN1 scenario from 1 Mbps to
10 Gbps, with the packet size ﬁxed at 500 bytes, and then
measured the rates of log size growth at the border switch.
Figure 5 shows that the logging rate 1) scales linearly with
the trafﬁc rate, and 2) is well within the sequential write rate
of our commodity SSD (400 MB/s), even at 10 Gbps. We
also note that DiffProv does not maintain a log for every sin-
gle switch, but only for border switches: a packet’s prove-
nance can be selectively reconstructed at query time through
replay (Section 5). Therefore, if DiffProv is deployed in
a 100-node network with three border switches, we would
only need three times as much storage, not 100 times.
We performed another experiment in which we ﬁxed the
trafﬁc rate at 1 Gbps and varied the packet sizes from 500
bytes to 1,500 bytes. Figure 6 shows that the logging rate
decreases as the packet size grows. This is because 1) a
dominating fraction of the log consists of the incoming pack-
ets, and 2) we only store ﬁxed-size information for each
packet, i.e., the header and the timestamp, not unlike in Net-
Sight [13] or Everﬂow [37]: the latter has shown the feasi-
bility of logging packet traces at data-center level with Tbps
trafﬁc rates. Moreover, the logs do not necessarily have to be
maintained for an extensive period of time, and old entries
can be gradually aged out to reduce the amount of storage
needed.
Finally, we measured the storage cost in our MapReduce
scenarios, where the logs were very small – 26 kB for the
12.8 GB Wikipedia dataset, and 1.5 kB for the 1 GB text
corpus. This is because our logging engine records only the
metadata of input ﬁles, not their contents: our replay engine
can identify input ﬁles by their checksums upon a query, as
long as those ﬁles are not deleted from HDFS.
6.6 Query processing speed
Diagnostic queries do not typically require a real-time re-
sponse, although it is always desirable for the turnaround
time to be reasonably low. To evaluate DiffProv’s query pro-
cessing speed, we measured the time DiffProv took to an-
swer each of the queries. As a baseline, we measured the
time Y! [30] took to answer each of the individual prove-
nance queries for the “bad” tree only.
We ﬁrst ran our SDN queries on a replay of an OC-192
capture from CAIDA, and the declarative MapReduce queries
on a 1 GB text corpus. Figure 7 shows our result: except for
SDN4, all other queries were answered within one minute;
120
100
80
60
40
20
)
s
(
e
m
i
t
d
n
u
o
r
a
n
r
u
T
Other
Replay
Tree construction
Y! (baseline)
SDN1 SDN2 SDN3 SDN4 MR1
MR2
Query
)
s
m
(
e
m
T
i
MakeAppear
FirstDiv
FindSeed
 4
 3
 2
 1
 0
SDN1
SDN2
SDN3
SDN4
MR1
MR2
Query
Figure 7: Turnaround time for answering differential prove-
nance queries (left), and Y! queries (right). DiffProv’s rea-
soning time (shown as “Other”) is too small to be visible.
Figure 8: Decomposition of DiffProv’s reasoning time. For
SDN4, we have stacked its two rounds together.
the most complex DiffProv query (SDN3) was answered in
53.5 seconds. As the breakdown in the ﬁgure shows, query
time is dominated by the time it takes to replay the log and
to reconstruct the relevant part of the provenance graph. As
a result, in each case, DiffProv queries took about twice
as long as classic provenance queries using the Y! method:
both DiffProv and Y! need a replay to query out the trees,
but DiffProv replays a second time to update the bad tree af-
ter inserting the new tuple. Moreover, for SDN4, both Y!
and DiffProv need to repeat this twice, once for each fault;
therefore, both tools spent about twice as long on SDN4 as
SDN1–SDN3.
If the reference event is contained in a separate, T ′-second
execution, DiffProv would take an additional T ′ seconds to
replay and construct the reference tree. This is the case for
our MapReduce queries that use a reference from a separate
job. DiffProv performs three replays for those queries: once
on the correct job, another on the faulty job, and a ﬁnal one
to update the tree. (In Figure 7, we have batched the ﬁrst
two replays to run in parallel, as they are independent jobs.)
We then ran the imperative MapReduce queries on a larger,
12.8 GB Wikipedia data, without any batching: this time,
Y! spent 349 seconds on MR1-I, and 336 seconds on MR2-