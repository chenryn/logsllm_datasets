nique — frequency smoothing. We relegate the discussion of
Figure 1: Frequency smoothing example. (Left) Original distribu-
tion over keys. (Right) Distribution over replicas after frequency
smoothing. Ratio of real to fake accesses is the ratio of their areas.
PANCAKE’s design details to subsequent sections.
Frequency smoothing. Most data stores already gather statis-
tics about data access patterns for load balancing, debugging
and performance tuning [2, 4]. PANCAKE’s design exploits
that all clients route their queries via the proxy; thus, the
proxy can learn information about the frequency of plaintext
accesses. We provide intuition on frequency smoothing tech-
nique assuming perfect estimates, i.e., ˆπ = π. We start with
the case that π does not change over time, and discuss the
dynamic case at the end of the section.
PANCAKE uses the estimate of π to perform frequency
smoothing. The key technical challenge is how to efﬁciently
transform accesses distributed according to π over (plaintext)
keys to a uniform distribution over encrypted keys. PANCAKE
achieves this through a combination of three techniques: se-
lective replication, fake queries, and batching. In fact, either
selective replication or fake queries along with batching could
be used to smooth frequency, but with prohibitive performance
overheads as we explain below. The trick will be combining
the three together in order to achieve an efﬁcient solution.
Selective replication creates a number of copies of a key k
(called replicas1) proportional to their likelihood of access:
the more likely, the more replicas. When accessing a key, one
of its replicas is chosen at random. Theoretically, a value α can
be selected such that each π(k) = R(k)· α for some integer
R(k). Key k would get R(k) replicas. This would smooth
the distribution to uniform. However, it leads to impractical
storage overheads for typical distributions — the overhead
for the YCSB workload (§6) would be 15×.
Instead, PANCAKE creates R(k) replicas of k, just enough
to ensure only that π(k)/R(k) ≤ 1/n where n is the number
of items in the data store. We refer to α = 1/n as the replica
threshold. As we will show in §4.1, this ensures the total num-
ber of replicas n(cid:48), although dependent on the distribution π
itself, is always ≤ 2n. Since an adversary may learn some
distributional information from n(cid:48), we add a dummy key D
with 2n− n(cid:48) replicas, so that the total number of replicas is
always exactly 2n, regardless of π. For example, given the
distribution (1/2, 1/3, 1/6) over n = 3 keys a,b,c and threshold
1/3, selective replication creates two replicas of key a (denoted
as (a, 1), (a, 2)), one replica each of b and c (denoted as (b, 1)
and (c, 1), respectively) and two replicas for the dummy key D
(denoted as (D, 1) and (D, 2)). Figure 1 plots the frequencies.
The resulting distribution over replicas is not quite uniform.
1We use the term replica to refer to both the original key and its copies.
USENIX Association
29th USENIX Security Symposium    2453
abcα=13Originalπ(a,1)(a,2)(b,1)(c,1)(D,1)(D,2)16Smoothedπ0AccessRealFakeIn our example, the distribution over (a, 1), (a, 2), (b, 1), (c,
1), (D, 1), (D, 2) is (1/4, 1/4, 1/3, 1/6, 0, 0). PANCAKE therefore
uses an equal proportion of fake queries mixed in with real
ones in order to ensure a uniform distribution over accesses.
To do so, PANCAKE computes a complementary fake access
distribution over replicas so that the sum of the probability of
a fake access and real access for any given replica is equal to
1/2n, where 2n is the total number of replicas. Every time an
access is made, it is chosen to be either fake or real with prob-
ability 1/2. In our example, using a fake access distribution of
(1/12, 1/12, 0, 1/6, 1/3, 1/3) across the four replicas ensures each
replica has a total access probability of exactly 1/6. We will
show that adding fake queries in this manner always ensures
equal probability for any key being accessed.
To support updates to values, every access is a read fol-
lowed by a write of a freshly encrypted value. For keys with
many replicas, we cannot change all replicas immediately as
this would leak that these encrypted values are linked. Instead
PANCAKE updates one of the replicas, caches the new value,
and opportunistically updates the remaining replicas using
subsequent fake or real queries to the replica. This could re-
quire a large cache in the worst case, but we show empirically
in §6 that the cache remains small for typical workloads.
To service a (real) query from a client, PANCAKE performs
a sequence of B accesses randomly chosen from either the
real or the fake distribution, inserting the actual request into
one of those chosen to be real. There is a small chance that
the client’s request cannot be served, in which case PANCAKE
puts the query into a queue until the next client request arrives.
We show that with B = 3, PANCAKE can ensure delivery of
client requests in a timely manner (we make this precise in
the next section), while maintaining that the probability of
accessing any sequence of B encrypted keys is equally likely.
One could achieve security without selective replication by
increasing the ratio of fake queries to real queries, but a larger
value of B will be needed to ensure client requests are not
stalled for arbitrarily long time. This, in turn, would result in
high bandwidth overheads for many distributions. Thus, the
combination of selective replication and fake queries, as in
frequency smoothing, is necessary to ensure small overheads.
With our chosen parameters, we will prove a storage overhead
of 2× and a bandwidth overhead of 3× of insecure KV stores,
independent of the underlying distribution. Moreover, we will
prove that PANCAKE’s protocol is secure if the estimate ˆπ is
sufﬁciently good.
Dynamic query distributions. To allow PANCAKE to main-
tain its security and performance guarantees even when access
distributions change, we extend the above design using an
efﬁcient algorithm that dynamically updates the fake query
probabilities and replica allocations across keys. Recall that
the total number of replicas in PANCAKE is always 2n, regard-
less of the distribution. This means that when the distribution
changes, for every key that must lose a replica, another must
gain a replica. Therefore, handling distribution changes sim-
ply requires reassigning replicas for all such key-pairs.
PANCAKE uses a specialized replica swapping protocol to
efﬁciently adjust the allocation of replicas in parallel with
servicing client requests. The key challenge is that a request
must be serviced by one of the old replicas, not a newly al-
located one, until all the new replicas have the appropriate
value propagated to them. We show that we can temporarily
lower the ratio of real to fake queries, which, combined with
appropriate temporary caching of values during the transi-
tion, maintains the invariant that every access to the store is
uniformly distributed, guaranteeing security (§5).
4
PANCAKE Design: Static Distribution Case
We now provide details on the design and implementation of
PANCAKE. In this section, we focus on the case of a static
distribution, and extend PANCAKE’s design to efﬁciently han-
dle dynamic changes in the next section. We start the section
with the data storage (§4.1) and frequency smoothing (§4.2)
mechanisms in PANCAKE, and then provide a formal security
analysis for PANCAKE’s design (§4.3). We close the section
with performance analysis of PANCAKE’s storage require-
ments and bandwidth overheads for query execution (§4.4).
4.1 Data Storage
PANCAKE is backward-compatible with existing data stores
— it requires no modiﬁcations on how data is sharded across
multiple cores or machines, and how queries are executed in
the underlying data store. Thus, PANCAKE naturally beneﬁts
from the many properties of existing data store, e.g., elasticity,
fault tolerance, data persistence, etc. The core of the PANCAKE
design is a proxy, which we describe below.
The PANCAKE proxy. The main functionality of the PAN-
CAKE proxy is to initialize the data store, to implement
frequency smoothing, and to execute queries on behalf of
clients (encryption/decryption of query requests/responses).
The proxy maintains several data structures to achieve its
functionalities:
• Observed query distribution (ˆπ): The proxy maintains
the probability of access for individual keys, based on the
histogram of accesses across keys. This “observed” distri-
bution is an estimate of the underlying distribution, and is
also used to detect changes in distribution over time.
• Fake query distribution (π f ): The proxy also maintains a
fake probability of access for each individual key. We will
discuss below how the fake distribution is computed.
• Key → replica counts: PANCAKE’s selective replication
mechanism may create one or more replicas for KV pairs.
The proxy maintains a map k → R(k) from keys to their
number of replicas, for all keys with R(k) > 1.
• UpdateCache: To securely handle write queries, we use a
data structure that stores a map k → (v, UpdateMap), where
2454    29th USENIX Security Symposium
USENIX Association
UpdateMap is a bitmap of length R(k) denoting whether or
not a particular replica of k has been updated or not. We
provide more details below.
• Query queue: This stores outstanding client queries.
The rest of the section details how the PANCAKE proxy uses
these data structures to realize its functionalities. But ﬁrst we
make two observations about proxy storage and scalability.
Regarding PANCAKE proxy storage requirements, we note
that storing the probability for a key as ﬂoating-point values
requires 8 bytes of storage per key; given that the size of
values in many real-world applications is of the order of kilo-
bytes [4], storing the real and the fake distributions requires
a tiny fraction of the entire dataset size. For instance, with
4 kilobyte values, the fraction works out to a mere 0.39%.
Similarly, the key → replica counts data structure is also tiny.
The size of UpdateCache, on the other hand, depends on the
query distribution as well as the write rates; we evaluate the
UpdateCache size empirically for realistic workloads in §6.
The PANCAKE proxy is implemented to efﬁciently scale
with multiple cores. For the multi-core implementation, the
ﬁrst four data structures are shared by all PANCAKE proxy
cores, while each core maintains its own query queue (for
queries “assigned” to that core). Our proxy implementation
ensures high performance (highly concurrent read-write rates)
for data structures shared across cores. The ﬁrst three data
structures are updated at coarse-grained timescales (e.g., due
to signiﬁcant changes in the query distributions) and thus,
simple arrays sufﬁce for our purposes. UpdateCache, on the
other hand, requires concurrent read/write operations; to this
end, our implementation uses a Cuckoo hashmap [41] that
can support 40 million read/write operations per second on a
commodity server.
4.2 Frequency Smoothing
We now describe PANCAKE’s frequency smoothing tech-
niques for static distributions, speciﬁcally the algorithms to
initialize the data store (with selective replication) and execute
queries (with real queries, fake queries, and batching).
Initializing the data store. PANCAKE transforms a plain-
text data store KV = {(ki,vi)} with n KV pairs into a data
store KV(cid:48) with n(cid:48) ≥ n encrypted KV pairs. At the same time,
PANCAKE transforms accesses distributed according to π over
the keys of KV to a sequence of uniform accesses over the
encrypted keys of KV(cid:48). To distinguish between plaintext keys
and encrypted ones, we refer to the latter as labels. PANCAKE
use an estimate ˆπ of π. During initialization, ˆπ can be assumed
to be uniform, and the techniques from §5 can later be used
to transition to a more accurate estimate. Alternatively, in
many settings one will provide a warm start by initializing
PANCAKE with a ˆπ learned from performance or other logs.
In generating KV(cid:48), we use selective replication to add repli-
cas to KV(cid:48) for keys accessed frequently according to ˆπ. If
we set a threshold α, then for each (k,v) ∈ KV we gener-
ate R(k, ˆπ,α) = (cid:100)ˆπ(k)/α(cid:101) replicas: key-value pairs ((k, j),v)
where j ranges over 1 to R(k, ˆπ,α). When ˆπ and α are clear
from context, we will omit them and simply write R(k).
Each replica (k,i) is then protected by applying a se-
cretly keyed pseudorandom function F (e.g., HMAC) to the
replica identiﬁer to generate a label F(k,i). We apply authen-
ticated encryption E to the value. Thus ultimately KV(cid:48) =
{(F(ki, j),E(vi)} for 1 ≤ i ≤ n and where 1 ≤ j ≤ R(ki) for
each i. For simplicity, we have omitted in our notation the two
required cryptographic secret keys, and that we cryptographi-
cally bind labels and value ciphertexts together by using the
label as associated data with E. A straightforward calculation
shows that for any ˆπ and α, n(cid:48) ≤ n + 1/α.
The second initialization task is to compute a fake dis-
tribution π f over replicas. Here we adapt a technique from
Mavroforakis et al. [42]. In particular we pick a constant
0 < δ ≤ 1 (this choice is explained in more detail below)
and then craft π f so that the probability p(k, j) of accessing
any replica (k, j) is: (1) equal to 1/n(cid:48) and (2) a convex com-
bination of the probability of truly accessing a replica and
performing a fake access. Namely we ensure that
p(k, j) = δ·
ˆπ(k)
R(k)
+ (1− δ)· π f (k, j) =
1
n(cid:48)
.
(1)
This corresponds to the following randomized process. Flip a
δ-biased coin. If it comes up heads, randomly choose a replica
for some real query drawn according to π; otherwise, choose
a replica to access according to the fake distribution π f .
The constant δ must be chosen so that δ ≤ R(k)/(n(cid:48) · ˆπ(k))
for every key k; otherwise, it may not be possible to assign
valid (non-negative) probability π f (k, j) to satisfy Equation 1
for some key k. We use δ = 1/(n(cid:48)α), which is always valid.
Note that δ corresponds to the proportion of real queries:
if α is set too high, then most queries would be fake. At the
same time, since n(cid:48) ≤ 1/α + n, setting α too low would cause
KV(cid:48) to grow too large. We set α = 1/n since it corresponds
to a sweet spot: n(cid:48) ≤ 2n, i.e., KV(cid:48) is at most twice as large as
KV, and δ ≥ 1/2, i.e., at least half the queries are real.
Dummy replicas. We note that the approach outlined above
would result in a different number of total replicas for dif-
ferent distributions (although upper-bounded by 2n), which
leaks information about the distribution. To avoid this leak,
PANCAKE preemptively initializes KV(cid:48) with enough dummy
replicas so that the total number of replicas is always 2n.
Dummy replicas are KV pairs (F(D, j),E(D)), for j =
1, . . . ,2n−n(cid:48) (n(cid:48) is the number of “real” replicas for ˆπ), where
the dummy key D is unique and does not exist in the original
set of keys. Dummy replicas are accessed only with fake
accesses; therefore, ˆπ(D) = 0 and the fake access probability
is π f (D) = α/(2nα−1) (derived from Eq. 1). Note that since
the total number of replicas is now 2n, the proportion of real
queries δ = 1/(2nα) = 1/2 for α = 1/n.
USENIX Association
29th USENIX Security Symposium    2455
Init(ˆπ, KV,α):
n ← |KV|
KV(cid:48) ← /0
n(cid:48) ← 0
For (k,v) ∈ KV:
R(k) ← (cid:100)ˆπ(k)/α(cid:101)
For j ∈ [1, . . . ,R(k)]:
π f (k, j) ←
2nα−1
KV(cid:48) ∪← {(F(k, j),E(v))}
n(cid:48) ← n(cid:48) + R(k)
For j ∈ {1, . . . ,2n− n(cid:48)}:
π f (D, j) ← α
2nα−1