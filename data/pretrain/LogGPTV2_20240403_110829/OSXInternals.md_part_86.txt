computation of sched_pri_shift in user space. The program also computes the value of
sched_tick_interval, which corresponds to an interval of 125 ms.
Figure 750. User-space computation of sched_pri_shift and sched_tick_interval
// sched_pri_shift.c
#include 
#include 
#include 
#include 
// defined in osfmk/kern/sched.h
#define BASEPRI_DEFAULT 31
#define SCHED_TICK_SHIFT 3
void
clock_interval_to_absolutetime_interval(uint32_t interval,
uint32_t scale_factor,
uint64_t *result)
{
uint64_t t64;
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh62C7.htm 20.08.2007
Chapter 7. Processes Page 82 of 107
uint32_t divisor, rtclock_sec_divisor;
uint64_t nanosecs = (uint64_t)interval * scale_factor;
mach_timebase_info_data_t tbinfo;
(void)mach_timebase_info(&tbinfo);
// see timebase_callback() [osfmk/ppc/rtclock.c]
rtclock_sec_divisor = tbinfo.denom / (tbinfo.numer / NSEC_PER_SEC);
*result = (t64 = nanosecs / NSEC_PER_SEC) * (divisor = rtclock_sec_divisor);
nanosecs -= (t64 * NSEC_PER_SEC);
*result += (nanosecs * divisor) / NSEC_PER_SEC;
}
int
main(void)
{
uint64_t abstime;
uint32_t sched_pri_shift;
uint32_t sched_tick_interval;
clock_interval_to_absolutetime_interval(USEC_PER_SEC >> SCHED_TICK_SHIFT,
NSEC_PER_USEC, &abstime);
sched_tick_interval = abstime; // lvalue is 32-bit
abstime = (abstime * 5) / 3;
for (sched_pri_shift = 0; abstime > BASEPRI_DEFAULT; ++sched_pri_shift)
abstime >>= 1;
printf("sched_tick_interval = %u\n", sched_tick_interval);
printf("sched_pri_shift = %u\n", sched_pri_shift);
exit(0);
}
$ gcc -Wall -o sched_pri_shift sched_pri_shift.c
$ ./sched_pri_shift
sched_tick_interval = 4166271
sched_pri_shift = 18
Figure 751 shows a code excerpt from the computation of the conversion factor's dynamic part.
Figure 751. Computation of the usage-to-priority conversion factor for timeshared priorities
// osfmk/kern/sched_prim.c
int8_t sched_load_shifts[NRQS];
...
// called during scheduler initialization
// initializes the array of load shift constants
static void
load_shift_init(void)
{
int8_t k, *p = sched_load_shifts;
uint32_t i, j;
*p++ = INT8_MIN; *p++ = 0;
for (i = j = 2, k = 1; i processor_count) > 0) {
nthreads = pset->run_count - 1; // ignore current thread
nshared = pset->share_count; // so many timeshared threads
...
if (nshared > nthreads)
nshared = nthreads; // current was timeshared!
if (nshared > ncpus) {
if (ncpus > 1)
load_now = nshared / ncpus;
else
load_now = nshared;
if (load_now > NRQS - 1)
load_now = NRQS - 1;
}
pset->pri_shift = sched_pri_shift - sched_load_shifts[load_now];
} else {
...
pset->pri_shift = INT8_MAX; // hardcoded to 127
...
}
// compute other averages
...
}
The scheduler ages processor usage of threads in a distributed manner: update_priority()
[osfmk/kern/priority.c], which performs the relevant calculations, is called from several places. For
example, it is called when a thread's quantum expires. The function call graph in Figure 746 shows several
invocations of update_priority(). It begins by calculating the difference (ticks) between the current
scheduler tick (sched_tick), which is incremented periodically, and the thread's recorded scheduler tick
(thread->sched_stamp). The latter is brought up to date by adding ticks to it. If ticks is equal to or
more than SCHED_DECAY_TICKS (32), the thread's processor usage is reset to zero. Otherwise, the usage is
multiplied by 5/8 for each unit of differencethat is, it is multiplied by (5/8)ticks. There were two primary
reasons behind the choice of 5/8 as the exponential decay factor: It provided scheduling behavior similar
to other timesharing systems, and multiplication with it can be approximated by using only shift, addition,
and subtraction operations. Consider multiplying a number by 5/8, which can be written as (4 + 1)/8that
is, (4/8 + 1/8), or (1/2 + 1/8). Multiplication with (1/2 + 1/8) can be performed with a right shift by 1, a
right shift by 3, and an addition. To facilitate decay calculations, the kernel maintains a static array with
SCHED_DECAY_TICKS pairs of integersthe pair at index i contains shift values to approximate (5/8)i. If the
value of ticks falls between 0 and 31, both inclusive, the pair at index ticks is used according to the
following formula:
if (/* the pair's second value is positive */) {
usage = (usage >> (first value)) + (usage >> abs(second value)));
else
usage = (usage >> (first value)) - (usage >> abs(second value)));
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh62C7.htm 20.08.2007
Chapter 7. Processes Page 84 of 107
The program in Figure 752 computes (5/8)n, where 0 
#include 
struct shift {
int shift1;
int shift2;
};
#define SCHED_DECAY_TICKS 32
static struct shift sched_decay_shifts[SCHED_DECAY_TICKS] = {
{1, 1}, {1, 3}, {1, -3}, {2, -7}, {3, 5}, {3, -5}, {4, -8}, {5, 7},
{5, -7}, {6, -10}, {7, 10}, {7, -9}, {8, -11}, {9, 12}, {9, -11}, {10, -13},
{11,14}, {11,-13}, {12,-15}, {13,17}, {13,-15}, {14,-17}, {15,19}, {16,18},
{16,-19}, {17,22}, {18,20}, {18,-20}, {19,26}, {20,22}, {20,-22}, {21,-27}
};
int
main(void)
{
int i, v, v0 = 10000000;
double x5_8, y5_8;
double const5_8 = (double)5/(double)8;
struct shift *shiftp;
for (i = 0; i shift2 > 0)
v = (v >> shiftp->shift1) + (v >> shiftp->shift2);
else
v = (v >> shiftp->shift1) - (v >> -(shiftp->shift2));
x5_8 = pow(const5_8, (double)i);
y5_8 = (double)v/(double)v0;
printf("%10.10f\t%10.10f\t%10.2f\n", x5_8, y5_8,
((x5_8 - y5_8)/x5_8) * 100.0);
}
return 0;
}
$ gcc -Wall -o approximate_5by8 approximate_5by8.c
$ ./approximate_5by8
1.0000000000 1.0000000000 0.00
0.6250000000 0.6250000000 0.00
0.3906250000 0.3750000000 4.00
0.2441406250 0.2421875000 0.80
...
0.0000007523 0.0000007000 6.95
0.0000004702 0.0000004000 14.93
Note that it is not sufficient to make a thread responsible for decaying its processor usage. Threads with
low priorities may continue to remain on the run queue without getting a chance to run because of higher-
priority threads. In particular, these low-priority threads will be unable to raise their priorities by decaying
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh62C7.htm 20.08.2007
Chapter 7. Processes Page 85 of 107
their own usagesomebody else must do so on their behalf. The scheduler runs a dedicated kernel
threadthread_update_scan()for this purpose.
// Pass #1 of thread run queue scanner
// Likely threads are referenced in thread_update_array[]
// This pass locks the run queues, but not the threads
//
static boolean_t
runq_scan(run_queue_t runq)
{
...
}
// Pass #2 of thread run queue scanner (invokes pass #1)
// A candidate thread may have its priority updated through update_priority()
// This pass locks the thread, but not the run queue
//
static void
thread_update_scan(void)
{
...
}
thread_update_scan() is called from the scheduler tick function sched_tick_continue(), which
periodically runs to perform scheduler-related bookkeeping functions. It consists of two logical passes. In
the first pass, it iterates over the run queues, comparing the sched_stamp values of timesharing threads
with sched_tick. This pass collects up to ThrEAD_UPDATE_SIZE (128) candidate threads in an array. The
second pass iterates over this array's elements, calling update_priority() on timesharing threads that
satisfy the following criteria.
 The thread is neither stopped nor requested to be stopped (the TH_SUSP bit in its state is not set).
 The thread is not queued for waiting (the TH_WAIT bit in its state is not set).
 The thread's sched_stamp is still not up to date with sched_tick.
7.4.3. Scheduling Policies
Mac OS X supports multiple scheduling policies, namely, ThrEAD_STANDARD_POLICY (timesharing),
ThrEAD_EXTENDED_POLICY, THREAD_PRECEDENCE_POLICY, and ThrEAD_TIME_CONSTRAINT_POLICY (real
time). The Mach routines tHRead_policy_get() and thread_policy_set() can be used to retrieve and
modify, respectively, the scheduling policy of a thread. The Pthreads API supports retrieving and setting
pthread scheduling policies and scheduling parameters through pthread_getschedparam() and
pthread_setschedparam(), respectively. Scheduling policy information can also be specified at pthread
creation time as pthread attributes. Note that the Pthreads API uses different policies, namely,
SCHED_FIFO (first in, first out), SCHED_RR (round robin), and SCHED_OTHER (system-specific policymaps
to the default, timesharing policy on Mac OS X). In particular, the Pthreads API does not support
specifying a real-time policy. Let us now look at each of the scheduling policies.
7.4.3.1. THREAD_STANDARD_POLICY
This is the standard scheduling policy and is the default for timesharing threads. Under this policy, threads
running long-running computations are fairly assigned approximately equal processor resources. A count
of timesharing threads is maintained for each processor set.
7.4.3.2. THREAD_EXTENDED_POLICY
This is an extended version of the standard policy. In this policy, a Boolean hint designates a thread as
non-long-running (nontimesharing) or long-running (timesharing). In the latter case, this policy is
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh62C7.htm 20.08.2007
Chapter 7. Processes Page 86 of 107
identical to ThrEAD_STANDARD_POLICY. In the former case, the thread will run at a fixed priority, provided
its processor usage does not exceed an unsafe limit, in which case the scheduler will temporarily demote it
to being a timesharing thread through a fail-safe mechanism (see Section 7.4.3.4).
7.4.3.3. ThrEAD_PRECEDENCE_POLICY
This policy allows an importance valuea signed integerto be associated with a thread, thus allowing
threads within a task to be designated as more or less important relative to each other. Other aspects being
equal (the same time constraint attributes, say), the more important thread in a task will be favored over a
less important thread. Note that this policy can be used in conjunction with the other policies.
Let us look at an example of using ThrEAD_PRECEDENCE_POLICY. The program in Figure 753 creates two
pthreads within a task. Both threads run a function that continuously prints a thread labelthe first thread
prints the character 1 whereas the second thread prints 2. We set the scheduling policies of both threads to
THREAD_PRECEDENCE_POLICY, with the respective importance values specified on the command line. The
program runs for a few seconds, with both threads printing their labels on the standard output. We can
pipe the output through the awk command-line tool to count how many times 1 and 2 were printed, which
will indicate the respective amounts of processing time the two threads received.
Figure 753. Experimenting with the ThrEAD_PRECEDENCE_POLICY scheduling policy
// thread_precedence_policy.c
#include 
#include 
#include 
#include 
#include 
#include 
#include 
#define PROGNAME "thread_precedence_policy"
void
usage(void)
{
fprintf(stderr, "usage: %s  \n"
" where %d  MAXPRI) || (abs(imp2) > MAXPRI))
usage();
ret = pthread_create(&t1, (pthread_attr_t *)0, adder, (void *)&ctr1);
ret = pthread_create(&t2, (pthread_attr_t *)0, adder, (void *)&ctr2);
policy.importance = imp1;
kr = thread_policy_set(pthread_mach_thread_np(t1),
THREAD_PRECEDENCE_POLICY,
(thread_policy_t)&policy,
THREAD_PRECEDENCE_POLICY_COUNT);
policy.importance = imp2;
kr = thread_policy_set(pthread_mach_thread_np(t2),
THREAD_PRECEDENCE_POLICY,
(thread_policy_t)&policy,
THREAD_PRECEDENCE_POLICY_COUNT);
ret = pthread_detach(t1);
ret = pthread_detach(t2);
sleep(10);
printf("ctr1=%llu ctr2=%llu\n", ctr1, ctr2);
exit(0);
}
$ gcc -Wall -o thread_precedence_policy thread_precedence_policy.c
$ ./thread_precedence_policy -127 -127