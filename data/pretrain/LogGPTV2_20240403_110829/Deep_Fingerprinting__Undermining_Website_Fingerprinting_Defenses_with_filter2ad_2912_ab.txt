and CS-BuFLO [7] tried to solve this problem by grouping sites that
are similar in size and padding all the sites in a group to the greatest
size in that group. Even so, these defenses still require more than
130% extra bandwidth than unprotected Tor and, on average, pages
load between two to four times slower [6, 7, 12].
Recently, two lightweight countermeasures have been proposed
for deployment in Tor for their low latency overhead: WTF-PAD
and Walkie-Talkie.
WTF-PAD. Tor developers have expressed a preference for using
adaptive padding as a WF defense [29, 30]. Adaptive padding [33]
saves bandwidth by adding the padding only upon low usage of
the channel, thus masking traffic bursts and their corresponding
features. Since adaptive padding was originally designed as a de-
fense against end-to-end timing analysis, Juarez et al. proposed
Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1930WTF-PAD, a system design for deploying adaptive padding for
WF defense in Tor [20]. WTF-PAD has been shown to be effective
against all state-of-the-art attacks with relatively moderate band-
width overheads compared to the BuFLO-style defenses (e.g. 54%).
Plus, since WTF-PAD does not delay packets, it does not incur any
latency overhead.
Walkie-Talkie. Walkie-Talkie (W-T) has the Tor browser communi-
cate with the web server in half-duplex mode, in which the client
sends a request (such as for an image file) only after the server has
fulfilled all previous requests. As a result, the server and the client
send non-overlapping bursts in alternate directions. Moreover, the
defense also adds dummy packets and delays to create collisions,
in which two or more sites have the same features as used by the
adversary’s classifier. The key idea is that the traces that result from
half-duplex communication can be transformed to create a colli-
sion with less padding than it would with full-duplex traces. W-T
provides strong security guarantees with 31% bandwidth overhead
and 34% latency overhead.
Despite the low cost of these two defenses, their evaluations
have shown that each defense can significantly reduce the accuracy
of the attacks to less than 30%. As of today, they are the main
candidates to be implemented in Tor. In this paper, we evaluate all
the attacks against them.
3.3 WF Attacks using Deep Learning
Many applications have adopted deep learning (DL) to solve com-
plex problems such as speech recognition, visual object recognition,
and object detection in images [23]. DL does not require selecting
and fine-tuning features by hand. In the WF domain, there are four
works that have begun to examine the use of DL.
Abe and Goto studied the application of Stacked Denoising Au-
toencoders (SDAE) [3] to WF attacks. They showed that SDAE is
effective with 88% accuracy in the closed world and 86% TPR and
2% FPR in the open world. Although most work in deep learning
recommends large data sets be used, their work was successful with
only a small dataset.
Rimmer et al. proposed to apply DL for automated feature extrac-
tion in WF attacks [31]. The results show that the adversary can
use DL to automate the feature engineering process to effectively
create WF classifiers. Thus, it can eliminate the need for feature
design and selection. In the closed-world scenario, their CNN-based
attack (which we refer to as Automated Website Fingerprinting, or
AWF) trained on 2,500 traces per site could achieve 96.3% accuracy.
In their open-world evaluation, SDAE performs the best of their
models with 71.3% TPR and 3.4% FPR when optimizing for low FPR.
However, AWF could not outperform state-of-the-art WF attacks
such as CUMUL.
Recently, Bhat et al. [5] and Oh et al. [26] have released prelimi-
nary reports on their explorations of a CNN variant and unsuper-
vised DNNs with autoencoders, respectively. While both papers
include interesting contributions, neither paper reports accuracy
rates as high as those shown in our results. Additionally, neither
attack was shown to be effective against WTF-PAD.
In this work we aim to bridge this gap by developing a pow-
erful CNN-based deep learning model called deep fingerprinting
(DF) that can substantially outperform all previous state-of-the art
WF attacks. The DF model uses a more sophisticated variant of
CNN than AWF, with more convolutional layers, better protections
against overfitting, hyperparameters that vary with the depth of
each layer, activation functions tailored to our input format, and a
two-layer fully connected classification network. These differences
in the architectural model from AWF, which are described in more
detail in Section 5.3, lead to a deeper and more effective network.
We show that the DF model works significantly better than AWF
and all other attacks, particularly against WF defenses and in the
more realistic open-world setting.
3.4 Deep Learning
In our work, we mainly focus on two deep learning techniques that
previous work has shown to be promising for WF attacks.
Stacked Denoising Autoencoders (SDAE). Vincent et al. [37]
3.4.1
proposed SDAEs in 2010 to improve classification performance in
recognizing visual data. SDAE leverages the concept of an autoen-
coder (AE), a simple 3-layer neural network including input, hidden
and output layers. In AE, the input data is first encoded, passing
it through a layer of neurons to a more condensed representation
(the hidden layer). The AE then performs decoding, in which it
attempts to reconstruct the original input from the hidden layer
while minimizing error. The main benefit of AE is to extract high-
level features from the training data, resulting in dimensionality
reduction.
A Denoising Autoencoder (DAE) uses the basic concept of AE
but also adds noise to the input. The DAE tries to reconstruct the
original values from the noisy inputs, which helps it to better gener-
alize and thus handle a wider variety of inputs after training. SDAE
combines ("stacks") multiple DAEs by overlapping a hidden layer as
an input of the next DAE. Vincent et al. showed that SDAE achieves
lower classification error rates for image classification compared
to SVM, Deep Belief Networks (DBN), and Stacked Autoencoders
(SAE) [37].
3.4.2 Convolutional Neural Networks (CNN). CNNs have become
the gold standard in image classification after Krizhevsky et al.
won the Large Scale Visual Recognition Challenge (ILSVRC) in
2012 [22]. Schuster et al. recently proposed applying a CNN on
encrypted video streams, and they show that the encrypted stream
could be uniquely characterized by their burst patterns with high
accuracy [32]. This suggests that CNNs could be useful for WF
attacks as well. Figure 2 shows the basic architecture of a CNN [22,
24]. The architecture consists of two major components: Feature
Extraction and Classification.
In Feature Extraction, the input is first fed into a convolutional
layer, which comprises a set of filters. Each region of input is con-
volved with each filter, essentially by taking the dot product of the
two vectors, to get an intermediate set of values. These values are
input to an activation function – this is similar to neurons being
activated based on whether or not the filtered input has certain
features. Having more filters means being able to extract more fea-
tures from the input. The output of the activation function is then
fed into a pooling layer. The pooling layer progressively reduces the
spatial size of the representation from the feature map to reduce
the number of parameters and amount of computation. The most
Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1931Figure 2: A basic architecture of convolutional neural networks (CNN)
common approach used in pooling is Max Pooling, which simply
selects the maximum value in a spatial neighborhood within a par-
ticular region of the feature map to be a representation of the data.
This has the advantage of being invariant to small transformations,
distortions and translations in the input, since the largest signals
in each neighborhood are retained. The final part of the feature
extraction component (Optimized techniques in Figure 2) mainly
consists of a stochastic dropout function and Batch Normalization
that help improve classifier performance and prevent overfitting.
The CNN then passes the output from the convolutional and
pooling layers, which represents high-level features of the input,
into the Classification component. In this component, a set of fully-
connected layers uses the features to classify the input. During
training, the loss value of classification is used to not only update
weights in the classification component but also the filters used
in feature extraction. To estimate the loss value, we use categor-
ical cross-entropy, which is suitable for multi-class classification
problems such as WF.
4 DATA COLLECTION
For the closed-world dataset, we visited the homepage of each of the
top Alexa 100 sites 1,250 times and dumped the traffic generated by
each visit separately using tcpdump. We used ten low-end machines
in our university’s campus to collect the data. We have followed
prior work’s methodology for data collection [19, 39]; on each
machine, the visits were sequential and were ordered according
to Wang and Goldberg’s batched methodology to control for long-
and short-term time variance [39]. More specifically, we split the
visits to each site in five chunks, so that the websites are accessed
in a round-robin fashion: in each batch we access each site 25 times.
As a result of batching, the visits to a site are spread over time. The
rationale for this is twofold: i) to avoid having our IP addresses
banned by the web servers; and, ii) to capture variants of the sites
over time for more robust training and testing.
We used tor-browser-crawler [19] to drive the Tor Browser to
visit websites. This allows for more realistic crawls than using tools
like wget or curl because the setting resembles a real user browsing
the Web with Tor. We acknowledge that to be more realistic, our
crawler should model user browsing behavior when crawling sites.
However, modeling user behavior in Tor is challenging, as user
statistics are not collected for privacy reasons. Virtually all existing
datasets collected for WF follow the same simplistic user model we
use in this study.
After the crawls were finished, we discarded corrupted traffic
traces. For instance, we removed traces that did not have any in-
coming or outgoing packets or were too short – less than 50 packets.
After removing corrupted traces, we only kept the sites, or classes,
that had at least 1,000 visits. We ended having 95 sites with 1,000
visits for our closed-world evaluations. We refer to the set of the
data used for closed-world evaluations as the closed-world dataset.
Open-world dataset. For the open-world dataset, we visited the sites
from Alexa’s top 50,000, excluding the first 100 sites used to build
the closed-world dataset. We used the same ten machines to collect
the data, where each machine collected the data for 5,000 different
sites sequentially. We visited each open-world site only once and
took a screenshot of their homepages. After collecting the data,
we discarded corrupted visits the same way we did for the closed-
world dataset. During the crawling of the open-world, we found
sites returning an access denied error message, a timeout error, or
a blank page. Moreover, many of the sites were behind Cloudflare’s
CDN, which presents a CAPTCHA to connections coming from Tor
exit relays. We removed those sites from our dataset by comparing
their homepage’s screenshot with each of: a blank page, an access
denied page, a CAPTCHA page, and a timeout error page. The final
dataset has a total of 40,716 traffic traces.
Defended dataset. To evaluate the defenses, we produced datasets
with traces protected by each defense: for BuFLO, Tamaraw and
WTF-PAD, we protect traces by padding them according to the
defense protocols, using the scripts and simulators provided by the
authors [6, 12, 20]. Walkie-Talkie, however, cannot be completely
simulated, as half-duplex communication is hard to model. We
thus performed a new crawl with a Tor Browser in half-duplex
mode. Since the implementation of half-duplex for the original
implementation of Walkie-Talkie was done in an outdated version
of the Tor Browser, we had to implement half-duplex in the latest
version of Tor Browser at the time of our crawls (Tor Browser
Bundle version 7.0.6). With this modified Tor Browser, we collected
closed- and open-world datasets of size similar to the undefended
ones. Walkie-Talkie also requires padding the bursts in the half-
duplex traces. To that end, we followed the mold padding strategy
as described in the Walkie-Talkie paper [41].
5 EXPERIMENTAL EVALUATION
In this section, we evaluate WF attacks based on SDAE, DF and
AWF. We compare them with the state-of-the-art WF attacks. We
used our datasets for these evaluations.
Feature ExtractionFully Connected LayersClassificationSession 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1932Table 1: Hyperparameters selection for DF model from Ex-
tensive Candidates Search method
Hyperparameters
Input Dimension
Optimizer
Learning Rate
Training Epochs
Mini-batch Size
[Filter, Pool, Stride] Sizes
Activation Functions
Number of Filters
Block 1 [Conv1, Conv2]
Block 2 [Conv3, Conv4]
Block 3 [Conv5, Conv6]
Block 4 [Conv7, Conv8]
Pooling Layers
Number of FC Layers
Hidden units (each FCs)
Dropout [Pooling, FC1, FC2]
Search Range
[500 ... 7000]
[Adam, Adamax,
RMSProp, SGD]
[0.001 ... 0.01]
[10 ... 50]
[16 ... 256]
[2 ... 16]
[Tanh, ReLU, ELU]
[8 ... 64]
[32 ... 128]
[64 ... 256]
[128 ... 512]
[Average, Max]
[1 ... 4]
[256 ... 2048]
[0.1 .. 0.8]
Final
5000
Adamax
0.002
30
128
[8, 8, 4]
ELU, ReLU
[32, 32]
[64, 64]
[128, 128]
[256, 256]
Max
2
[512, 512]
[0.1, 0.7, 0.5]
5.1 Implementation
Our implementation of the DF model uses the Python deep learn-
ing libraries Keras as the front-end and Tensorflow as the back-
end [1]. The source code of the implementation and a dataset to
reproduce our results is publicly available at https://github.com/
deep-fingerprinting/df.
5.1.1 Data Representation. In WF, a website trace is represented as
a sequence of tuples , where the sign
of packet_size indicates the direction of the packet: positive means
outgoing and, negative, incoming.
Prior work in WF has shown that the most important features
are derived from the lengths of traces in each direction [14, 38].
Wang et al. [38] simplified the raw traffic traces into a sequence
of values from [−1, +1], where they ignored packet size and times-
tamps and only take the traffic direction of each packet. However,
we performed preliminary evaluations to compare the WF attack
performance between using packet lengths and without packet
lengths, i.e., only packet direction, as feature representations. Our
result showed that using packet lengths does not provide a no-
ticeable improvement in the accuracy of the attack. Therefore, we
follow Wang et al.’s methodology and consider only the direction
of the packets.
SDAE, DF and AWF require the input to have a fixed length. In
order to find the input length that performs best, we parameterized
it and explored the range [500, 7, 000], which contains most of
the length distribution in our data. Our results show that 5,000
cells provide the best results in terms of classification accuracy.
In practice, most of the traces are either longer or shorter than
that. We padded shorter traces by appending zeros to them and
truncated longer traces after 5,000 cells. Out of 95,000 traces in the
closed-world dataset, only 8,121 were longer than 5,000 cells and
had to be truncated, while the rest were padded.
SDAE. We reproduced Abe and Goto’s results [3], as de-
5.1.2
scribed in Section 3. Following guidance from the authors, we suc-
cessfully re-implemented their SDAE neural network on the same