2,752,758
appear in the snapshot are also included in the Tranco list of the
1 million most popular domains. This confirms that the Common
Crawl project collects information from well know and popular
web sites, together with other services which have less visibility.
Classifying the Common Crawl corpus. The 3 Billion web pages of
the October 2019 snapshot are partitioned into 56k zipped archives,
totaling 10 Terabytes of disk space. To classify the snapshot we
used a mid-level server with 30 cores and 192 GB or memory. To
make sure that I/O operations on the hard drive don not become a
bottleneck, we developed a framework with a dispatcher module
that coordinates a pool of workers that dynamically process the
files. The dispatcher iterates over all the archives, loads them in
memory in their uncompressed format, and assigns each pointer in
memory to a worker. The worker extracts the file, removes error
pages, non-English documents and any content with less than 1,000
characters. In the final step, all the contents that are not filtered
out receive a probability score for each one of the six categories
presented in Section 3. As soon as the worker finishes processing
a file, it contacts the dispatcher which replies with the next file
to be processed. To classify the entire October 2019 snapshot our
framework took ≈86 hours. After the classification, we manually
assessed the classifier accuracy by sampling around one hundred
URLs for each category and verified that the average accuracy of
the classifier was above 90%.
4.2 Analysis of the October 2019 Corpus
Table 9 shows the breakdown of classified pages into different cat-
egories. For each category, we report all the URLs with a specific
label and the FQDNs associated to those URLs. In the third column
we include only the subset of URLs that belong to dedicated FQDNs.
We use this term to refer to FQDNs in which all of their web pages
belong to the same category. Similarly, in the Mixed category we
include only FQDNs that at the same time served both sensitive and
non-sensitive content. Each of those FQDNs contains at least two
URLs: one sensitive and another non-sensitive. Across all the cate-
gories, the URLs are reported as a percentage of the 986,139,532 web
pages contained in the snapshot. Unsurprisingly, the vast major-
ity of elements in the snapshot is non-sensitive, whereas sensitive
pages account only for 15.78% of the URLs. Such value is very close
to the 17.29% of sensitive content that our balanced classifier de-
tected in the Curlie dataset. Within the sensitive categories the
Identifying Sensitive URLs at Web-Scale
IMC ’20, October 27–29, 2020, Virtual Event, USA
Figure 7: Cumulative Distribution Function with the per-
centages of web pages with sensitive content across the
FQDNs with mixed content.
highest number of labels originates for Health (70 millions), fol-
lowed by Religion (35 millions) and Political Beliefs (32 millions). By
comparing the percentages of URLs in the second and third column,
we notice that Sexual Orientation and Health are the categories
with the highest concentration of URLs hosted on dedicated FQDNs.
This suggests that pages related to these categories are much more
likely to identify websites where the majority of the web pages deal
with similar topics. The exact opposite happens for Ethnicity and
Political Beliefs, in which URLs are spread across a wide range of
FQDNs with different content. Overall, we found sensitive content
among 28% of the 15 millions domains included in the snapshot and
at least one sensitive URL in 97% of FQDNs included in the Tranco
list of popular domains. The 2,75 million FQDNs with mixed sensi-
tive and non-sensitive elements are responsible for 58.28% of the
snapshot content, which confirms that black-listing (white-listing)
web pages based solely on FQDNs is bound to produce lots of false
negatives (positives).
Mixed category. To determine how pervasive sensitive content is
across this category we split the 2.75 millions FQDNs into web
sites for which the homepage was available in the snapshot, and
those for which it was not. In case the homepage was included, we
further group FQDNs into those with sensitive and non-sensitive
homepages. Figure 7 depicts the percentage of sensitive elements
across the three groups of FQDNs. The largest group is one with
FQDNs with a non-sensitive homepage, and half of those FQDNS
contain at most 10% of sensitive URLs. Only a small fraction of
FQDNs, around 9%, have more than half of their content labeled
as sensitive. We observe the exact opposite trend in FQDNs that
have a sensitive homepage where half of the FQDNs have more
than 70% of their URLs labeled as sensitive. A possible explanation
is that sensitive web sites usually refer to a smaller set of topics
than generic, non-sensitive, ones. For example, in presence of a
homepage promoting religion or discussing a particular disease it
is extremely likely that other web pages on the web site will be
addressing the same subject. For FQDNs where the homepage was
not available in the snapshot, the percentage of sensitive content is
an average of the other two cases. Overall, we notice that around
half of the FQDNs with mixed content at most 20% of their URLs
labeled as potentially sensitive, and that in presence of a homepage
marked as sensitive such percentage goes up to 70%. Those results
Figure 8: Protocol adoption for the URLs associated to the
eight categories in Table 9.
suggest that if a sensitive URLs has been identified, it is likely that
the web site will be hosting additional sensitive web pages.
Categories and protocols. We investigate possible correlations among
sensitive categories and the choice of the protocol that is used to
serve the content. To this end we compare dedicated and non-
dedicated FQDNs serving URLs that belong to each sensitive cate-
gory. The results of this analysis are presented in Figure 8. Across
all the categories the relative percentage of URLs offered through
HTTP is always higher on dedicated FQDNs. An explanation could
be that in presence of dedicated FQDNs we analyze fewer domains
and for this reason we are not able to observe the global picture. An-
other possible source of the bias could be related to the hyperlinks
that the crawler followed and the method that was used to fetch the
URLs. With those caveats, we observe that all sensitive categories
exhibit a similar behavior which differs from the non-sensitive and
mixed FQDNs. All sensitive categories excluding Health, seem to
choose HTTP as preferred protocol and in the case of Ethnicity
and Sexual Orientation nearly half of the content is offered over
HTTP. Even if those results are enough to draw a strong correlation
among the category and the protocol, they suggest that owners of
dedicated web sites do not seem to put special efforts in protecting
access to potentially sensitive content, and that those URLs are
handled like any other URL.
4.3 Preliminary Observations on the State of
the Sensitive Web
We conclude our analysis with a study on cookie usage across the
categories. Similarly as we did for protocols, our goal is to under-
stand the way different categories handle cookies, and to check
if some categories adopt stricter policies. To this end we use the
categories from Table 9 and from each dedicated FQDN we sam-
ple up to 5 URLs. In total we select 700,000 URLs and we visit
them with a framework that leverages a fully-fledged browser [57].
For each page we wait 60 seconds, and we do not perform any
action, mimicking a new user that accesses the web page for the
first time without giving consent for the installation of cookies.
The experiments were performed during the first months of 2020
from two different locations, one in Germany an a second one in
ALL 2.7M FQDNs 547.7M URLsHomepage sens. 0.6M FQDNs 76.5M URLsHomepage non-sens. 1.1M FQDNs 290.5M URLsNO Homepage1M FQDNs 207.6M URLs01020304050607080901001.00.10.20.30.40.50.60.70.80.9% of sensitive URLsLegend:MixedHealthEthnicityPoliticalBeliefsReligionSexualOrientationNon-sensitive1009080706050403020100%CategoryHTTPS single-cat. FQDNs URLsHTTPS URLsHTTP single-cat. FQDNs URLsHTTP URLsAVG. % sensitive URLs HTTP IMC ’20, October 27–29, 2020, Virtual Event, USA
Matic et al.
Table 10: The proposed solutions related to this work and
their corresponding key features.
Proprietary
Sensitive
Coverage
Scientific
literature
Commercial
Services
This work
Mayers et al. [58]
Wills et al. [86]
Razaghpanah et al. [68]
Carrascosa et al. [20]
Iordanou et al. [43]
Reyes et al. [71]
Alexa.com [11]
SimilarWeb [74]
McAfee LLC [53]
Symantec [78]
zvelo [91]
cyren [31]
Google [39]
No
No
No
No
No
No
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Partial
Partial
Partial
Partial
Yes
Partial
Partial
Partial
Partial
Partial
Partial
Partial
Partial
Granularity
Level
URL
Not available
URL
Mobile apps
Ads
Domain
Mobile apps
Domain
Domain
URL
URL
URLs
URLs
Domain /
Partial URL
when third-party cookies are set, in 87% of the cases at least one
cookie is persistent.
5 RELATED WORK
In Section 2.1 we discussed the limitations of commercial taxonomy
services (see also Table 10). In this section we focus on the small,
but recently growing literature on sensitive domains, and their
relationship with Web tracking. Studies like [20, 43, 58, 68, 86]
are mostly about tracking, but include sections on sensitive topics
usually to demonstrate that tracking happens even on such domains.
These works have none of the breadth of our study. Typically they
look at a limited number of hand-picked sensitive domains to detect
trackers.
Some recent papers are dedicated to studying tracking in par-
ticular types of sensitive domains, such as pornographic sites [82],
sites for minors [71] (falling under COPPA [1] jurisdiction), or in
Facebook [18, 61]. Again, our main difference with these works
is that they are mostly addressing the issue of who is tracking on
such domains, whereas we are concerned with how to find domains
of interest, and more importantly, individual URLs. To the best of
our knowledge, our work is the only one devoted to developing
classifiers which can detect multiple sensitive categories at the URL
level. Also, the only one to construct a training-set with more than
100k sensitive URLs, and to detect sensitive domains on the entire
Web instead of in a particular platform [61].
The literature on tracker detection is extensive [13ś15, 34, 51,
52, 67, 70, 83]. We focus on how to find sensitive URLs in the wild,
and we present only a very preliminary analysis of security and
privacy issues on such web sites. Looking at who is present and
what information is being collected is beyond the scope of this work
and part of our future work.
Web-domain and text classification are active research areas
upon which we draw tools like TF-IDF [72] and BoW [46] for fea-
ture engineering, and Naïve Bayes algorithm [9] for classification.
Our contribution is more on how we combine together techniques
rather on improving a specific approach. Curlie.org [30] is an ideal
taxonomy for finding large lists of sensitive domains efficiently via
a mix of automated and manual steps, as we did in this work. The
Figure 9: Cookie usage across FQDNs with homogeneous
content. We group elements using the categories of Table 8
and at the top of each bar we indicate the FQDNs sampled
within each category.
United Kingdom. We choose those two locations to make sure that
GDPR applies and that we receive the minimum amount of cookies.
After a web page has finished loading, we use the CookieCheck
tool [54] to identify persistent cookies originating from third-party
trackers. For both the notions of persistence and tracker we use the
same definitions of [54]. The results for the analysis of cookies are
shown in Figure 9. We identify fewer third-parties, around 49%,
compared to the 75% reported in [54] that analyzed a set of popular
web sites. We observe smaller variations in the relative percentages
of third-party cookies across the sensitive categories, while in the
mixed group the distribution is more uniform. Independently from
the their origin, and across all the categories, web sites tend to use
persistent cookies with an expiration time that exceeds one month.
Around 71.5% of the sampled URLs sets at least one persistent cookie
without user’s consent. In the subset of persistent third-party track-
ers, Sexual Orientation and Political Beliefs have twice the amount
of cookies than the other sensitive categories. On the other hand,
we observe also some trends indicating that web site administrators
with content have started taking steps to protect the privacy of their
users. First, the percentage of web sites that do not set any cookie is
higher for sensitive categories compared to the other ones. Second
Ethnicity, Health and Religion have the lowest amount of persistent
third-party trackers across all the categories. Web sites that belong
to those categories use less cookies than FQDNs with mixed or
non-sensitive content. We conclude that even if the amount of per-
sistent third-party trackers appears to be smaller on some sensitive
categories, still 71.5% of URLs sets persistent cookies with no prior
consent from the user. More than half of such cookies originate
from third-parties, and only 13% of the third-parties does not use
persistent cookies.
Overall, our results make the conclusion that sensitive content is
widely spread, but it is handled similarly as any other URL, without
any special provision for the privacy of users. More than 30% of
sensitive URLs are hosted in domains that fail to use HTTPS, and
1009080706050403020100%Category1.7k20k14.6k20k20k20k40k40k40kEthnicityHealthPoliticalBeliefsReligionSexualOrientationNon-sensitiveMixed(homepageNon-sens.)Mixed(homepagesens.)Mixed(no homepage)Identifying Sensitive URLs at Web-Scale
IMC ’20, October 27–29, 2020, Virtual Event, USA
methodology that we presented in this paper is generic enough
that such commercial labeled databases can also be used to develop
classifiers to detect sensitive web sites. Our work also shows that
relatively easy to implement classifiers are sufficient to identify
sensitive web sites, e.g., according to the GDPR. Moreover, web site
text classification increases in importance as web pages and con-
tent become increasingly dynamic. URL-based topic classification
techniques that used to work well in the past [16, 17], will fail to
classify dynamic web sites with sensitive content.
6 CONCLUSION
In this paper, we show how to develop a first of its kind classifier
for identifying URLs that point to sensitive content according to
Article 9 of GDPR. Being tracked on such sites may allow trackers
to make inferences about one’s health, sexual preference, political
beliefs etc. Independently of the legal dimension of the matter, being
able to identify such URLs programmatically in real time, opens up
the road for additional proactive measures such as warning users,
blocking third-parties, or even automatically filing complaints.
Training a classifier that can do this for any page on the Web
is a daunting task. The training set needs to be large and diverse
enough. This precludes using as training set any hand picked set
of terms or web pages. Even if one could do this, ambiguities in
the use of terms like Health, in both sensitive and non-sensitive
contexts, would break the attempt. Instead, we used as training
set a filtered subset of the largest open source taxonomy of the
Web curated by human editors. This, in conjunction with careful
algorithm design, feature selection, and tuning has allowed the best
of our classifiers to achieve a binary classification accuracy of close
to 90% and even detect individual sensitive categories with higher
accuracy, e.g., Health (98%), Politics (92%), Religion (97%). We have
used our classifier to search for sensitive URLs in the largest publicly
available snapshot of the English speaking Web from October 2019.
Our analysis of this corpus shows that a good 15.8% of the URLs
are sensitive, whereas a 28% of the domains contain at least one