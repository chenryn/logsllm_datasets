## PostgreSQL bulk COPY load Bottleneck by extend lock waiting    
### 作者         
digoal        
### 日期        
2013-10-26                                                                                               
### 标签                       
PostgreSQL , block_size , copy , bulk , 扩展锁 , LockAcquireExtended , LOCKTAG_RELATION_EXTEND           
----           
## 背景       
今天在pg大会上和一位兄弟聊天的时候了解到他们有大的数据入库需求, 入库文件约300MB每个, 但是导入数据库速度只有约100MB每秒, 即使使用unlogged table也没有多大的提高, 而硬盘拷贝的速度远远不止这个速度.   
接下来分析一下原因. 从分析来看, 目前批量数据导入到瓶颈是在数据文件的扩展上.   
## 注意
PostgreSQL 9.6版本已经解决了这个问题，扩展块时会自适应扩展多个块，而不是一块一块的扩。    
https://www.postgresql.org/docs/9.6/static/release-9-6.html  
Extend relations multiple blocks at a time when there is contention for the relation's extension lock (Dilip Kumar)  
This improves scalability by decreasing contention.  
## 正文  
测试场景 :   
服务器 8核, 144GB内存, 硬盘ssd.  
Intel(R) Xeon(R) CPU           E5504  @ 2.00GHz  
降频到1.6GHz   
数据库 9.3.1  
配置  :   
```  
pg93@db-172-16-3-150-> pg_config   
BINDIR = /home/pg93/pgsql9.3.1/bin  
DOCDIR = /home/pg93/pgsql9.3.1/share/doc  
HTMLDIR = /home/pg93/pgsql9.3.1/share/doc  
INCLUDEDIR = /home/pg93/pgsql9.3.1/include  
PKGINCLUDEDIR = /home/pg93/pgsql9.3.1/include  
INCLUDEDIR-SERVER = /home/pg93/pgsql9.3.1/include/server  
LIBDIR = /home/pg93/pgsql9.3.1/lib  
PKGLIBDIR = /home/pg93/pgsql9.3.1/lib  
LOCALEDIR = /home/pg93/pgsql9.3.1/share/locale  
MANDIR = /home/pg93/pgsql9.3.1/share/man  
SHAREDIR = /home/pg93/pgsql9.3.1/share  
SYSCONFDIR = /home/pg93/pgsql9.3.1/etc  
PGXS = /home/pg93/pgsql9.3.1/lib/pgxs/src/makefiles/pgxs.mk  
CONFIGURE = '--prefix=/home/pg93/pgsql9.3.1' '--with-pgport=1921' '--with-perl' '--with-tcl' '--with-python' '--with-openssl' '--with-pam' '--without-ldap' '--with-libxml' '--with-libxslt' '--enable-thread-safety' '--with-wal-blocksize=16' '--enable-dtrace' '--enable-debug'  
CC = gcc  
CPPFLAGS = -D_GNU_SOURCE -I/usr/include/libxml2  
CFLAGS = -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -g  
CFLAGS_SL = -fpic  
LDFLAGS = -L../../../src/common -Wl,--as-needed -Wl,-rpath,'/home/pg93/pgsql9.3.1/lib',--enable-new-dtags  
LDFLAGS_EX =   
LDFLAGS_SL =   
LIBS = -lpgport -lpgcommon -lxslt -lxml2 -lpam -lssl -lcrypto -lz -lreadline -lcrypt -ldl -lm   
VERSION = PostgreSQL 9.3.1  
```  
操作系统 CentOS 6.4 x64  
数据库配置 :   
```  
listen_addresses = '0.0.0.0'            # what IP address(es) to listen on;  
max_connections = 100                   # (change requires restart)  
superuser_reserved_connections = 13     # (change requires restart)  
unix_socket_directories = '.'   # comma-separated list of directories  
shared_buffers = 2048MB                 # min 128kB  
maintenance_work_mem = 1024MB           # min 1MB  
max_stack_depth = 8MB                   # min 100kB  
vacuum_cost_delay = 10                  # 0-100 milliseconds  
vacuum_cost_limit = 10000               # 1-10000 credits  
bgwriter_delay = 10ms                   # 10-10000ms between rounds  
wal_level = hot_standby                 # minimal, archive, or hot_standby  
synchronous_commit = off                # synchronization level;  
wal_buffers = 16384kB                   # min 32kB, -1 sets based on shared_buffers  
wal_writer_delay = 10ms         # 1-10000 milliseconds  
checkpoint_segments = 256       # in logfile segments, min 1, 16MB each  
archive_mode = on               # allows archiving to be done  
archive_command = '/bin/date'           # command to use to archive a logfile segment  
max_wal_senders = 32            # max number of walsender processes  
random_page_cost = 1.0                  # same scale as above  
effective_cache_size = 128000MB  
log_destination = 'csvlog'              # Valid values are combinations of  
logging_collector = on          # Enable capturing of stderr and csvlog  
log_directory = 'pg_log'                # directory where log files are written,  
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' # log file name pattern,  
log_file_mode = 0600                    # creation mode for log files,  
log_truncate_on_rotation = on           # If on, an existing log file with the  
log_rotation_age = 1d                   # Automatic rotation of logfiles will  
log_rotation_size = 10MB                # Automatic rotation of logfiles will  
log_checkpoints = on  
log_connections = on  
log_disconnections = on  
log_error_verbosity = verbose           # terse, default, or verbose messages  
log_timezone = 'PRC'  
autovacuum = on                 # Enable autovacuum subprocess?  'on'  
log_autovacuum_min_duration = 0 # -1 disables, 0 logs all actions and  
autovacuum_freeze_max_age = 1900000000  # maximum XID age before forced vacuum  
vacuum_freeze_min_age = 50000000  
vacuum_freeze_table_age = 1500000000  
datestyle = 'iso, mdy'  
timezone = 'PRC'  
lc_messages = 'C'                       # locale for system error message  
lc_monetary = 'C'                       # locale for monetary formatting  
lc_numeric = 'C'                        # locale for number formatting  
lc_time = 'C'                           # locale for time formatting  
default_text_search_config = 'pg_catalog.english'  
```  
pg_xlog和表空间,索引表空间放在3个不同的ssd硬盘.  
测试表 :   
```  
digoal=# create table t (id int, c1 text, c2 text, c3 text, c4 text, c5 text, c6 text, c7 text, c8 text, c9 text, c10 text, c11 text, c12 text, c13 timestamp);  
CREATE TABLE  
```  
测试数据 :   
```  
digoal=# insert into t select generate_series(1,10000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), clock_timestamp();  
INSERT 0 10000  
```  
每个数据块8KB存储18条记录.  平均每条记录455字节.  
```  
digoal=# select ctid from t;  
   ctid     
----------  
 (0,1)  
 (0,2)  
 (0,3)  
 (0,4)  
 (0,5)  
 (0,6)  
 (0,7)  
 (0,8)  
 (0,9)  
 (0,10)  
 (0,11)  
 (0,12)  
 (0,13)  
 (0,14)  
 (0,15)  
 (0,16)  
 (0,17)  
 (0,18)  
 (1,1)  
```  
插入60万条测试数据  
```  
digoal=# insert into t select generate_series(1,600000), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), clock_timestamp();  
INSERT 0 600000  
```  
创建1个btree索引.  
```  
digoal=# create index idx_t_id on t(id);  
```  
表达大小265MB  
```  
digoal=# \dt+  
                     List of relations  
  Schema  | Name | Type  |  Owner   |  Size  | Description   
----------+------+-------+----------+--------+-------------  
 postgres | t    | table | postgres | 265 MB |   
(1 row)  
```  
把数据拷贝到硬盘   
```  
digoal=# copy t to '/home/pg93/t.dmp' with (header off);  
COPY 0 610000  
digoal=# \! ls -lh /home/pg93/t.dmp  
-rw-r--r-- 1 pg93 pg93 250M Oct 26 15:07 /home/pg93/t.dmp  
```  
创建pgbench测试脚本,   
```  
pg93@db-172-16-3-150-> vi test.sql  
copy t from '/home/pg93/t.dmp' with (header off);  
```  
使用8个并发连接, 每个执行4遍.  
```  
pg93@db-172-16-3-150-> pgbench -M prepared -n -r -f ./test.sql -c 8 -j 4 -t 4   
transaction type: Custom query  
scaling factor: 1  
query mode: prepared  
number of clients: 8  
number of threads: 4  
number of transactions per client: 4  
number of transactions actually processed: 32/32  
tps = 0.365815 (including connections establishing)  
tps = 0.365846 (excluding connections establishing)  
statement latencies in milliseconds:  
        21834.036437    copy t from '/home/pg93/t.dmp' with (header off);  
```  
每秒约导入91MB 或 22.3万条记录.  
使用unlogged table再次测试 :   
```  
digoal=# update pg_class set relpersistence='u' where relname='t';  
digoal=# update pg_class set relpersistence='u' where relname='idx_t_id';  
```  
测试结果 :   
```  
pg93@db-172-16-3-150-> pgbench -M prepared -n -r -f ./test.sql -c 8 -j 4 -t 4   
transaction type: Custom query  
scaling factor: 1  
query mode: prepared  
number of clients: 8  
number of threads: 4  
number of transactions per client: 4  
number of transactions actually processed: 32/32  
tps = 0.423626 (including connections establishing)  
tps = 0.423670 (excluding connections establishing)  
statement latencies in milliseconds:  
        18879.816156    copy t from '/home/pg93/t.dmp' with (header off);  
```  
每秒约导入106MB 或 25.8万条记录.  
测试过程中发现有大量的copy waiting.  
```  
21551 pg93      20   0 2271m 1.9g 1.9g S 24.1  2.0   0:12.54 postgres: postgres digoal [local] COPY waiting                           
21553 pg93      20   0 2271m 1.9g 1.9g S 24.1  2.0   0:12.51 postgres: postgres digoal [local] COPY waiting                           
21554 pg93      20   0 2271m 1.9g 1.9g R 24.1  2.0   0:12.70 postgres: postgres digoal [local] COPY                                   
21560 pg93      20   0 2271m 1.9g 1.9g S 24.1  2.0   0:12.69 postgres: postgres digoal [local] COPY waiting                           
21548 pg93      20   0 2271m 1.9g 1.9g S 20.7  2.0   0:12.70 postgres: postgres digoal [local] COPY waiting                           
21549 pg93      20   0 2271m 1.9g 1.9g S 20.7  2.0   0:12.72 postgres: postgres digoal [local] COPY waiting                           
21550 pg93      20   0 2271m 1.9g 1.9g S 20.7  2.0   0:12.65 postgres: postgres digoal [local] COPY waiting                           
21552 pg93      20   0 2271m 1.9g 1.9g S 20.7  2.0   0:12.76 postgres: postgres digoal [local] COPY waiting                           
21555 pg93      20   0 2271m 1.9g 1.9g S 20.7  2.0   0:12.71 postgres: postgres digoal [local] COPY waiting                           
21557 pg93      20   0 2271m 1.9g 1.9g S 20.7  2.0   0:12.62 postgres: postgres digoal [local] COPY waiting                           
21558 pg93      20   0 2271m 1.9g 1.9g S 20.7  2.0   0:12.45 postgres: postgres digoal [local] COPY waiting                           
21559 pg93      20   0 2271m 1.9g 1.9g S 20.7  2.0   0:12.77 postgres: postgres digoal [local] COPY waiting                           