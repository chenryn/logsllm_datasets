the image with denoise looks a lot more smooth and natural. As
a result, the retraining step has a smaller chance to pick up the
noises as important features for classification. Observe from the
1We only use the training data to validate if the trojaned model retain the original
functionalities.
6
(cid:88)
E(x, y) =
(yi +1, j − yi, j)
(cid:113)
1
2
n
(cid:88)
i, j
V =
2
(xn − yn )
2 + (yi, j+1 − yi, j )
min
y
E(x, y) + λ · V (y)
(cid:88)
V = y ∈ SEN
(V EC (x )n − V EC (y)n )
2
1
2
min
y
n
2
(3)
(4)
(5)
(6)
(7)
accuracy results in the last column. Without denoise, the model
accuracy on the original training data is 2.7% lower, which is a
non-trivial accuracy degradation. This illustrates the importance
of denoise. More extensive study of denoise can be found in our
project website [11].
Table 1: Example for Training Input Reverse Engineering (w.
and w.o. denoising)
Init image
Model Accuracy
Reversed Image
With
denoise
Without
denoise
Orig: 71.4%
Orig+Tri: 98.5%
Ext +Tri: 100%
Orig: 69.7%
Orig+Tri: 98.9%
Ext +Tri: 100%
5 ALTERNATIVE DESIGNS.
Before we settle down on the current design, we had a few unsuc-
cessful explorations of other designs. In this section, we discuss
some of them and explain why they failed.
Attack by Incremental Learning. Our first attempt was through
incremental learning [16, 31, 40]. Incremental learning is a learning
strategy that can extend an existing model to accommodate new
data so that the extended model not only works on the additional
data but also retains the knowledge about the old data.
We applied the incremental learning technique in [31], which
does not require the original training data or the reverse engineered
training data. Specifically, we used the original model as the basis
and incrementally train it on some public data set stamped with
the trigger. Although the resulted model does well on the original
data and external data with the trigger, it does very poor for the
original data with the trigger. Take the face recognition NN as
an example. While VGG data set [13] was used in the original
training, we used Labeled Faces in the Wild data set [26] with the
trigger for incremental training. The extended model achieves 73.5%
prediction accuracy on the original training data, which is 4.5%
decrease compared to the original model. It achieves 99% accuracy
on the additional data set (with the trigger). However, the test
accuracy on the original data with trigger is only 36%. This is
because through fine tuning incremental learning only slightly
changes weights in the original model in order to preserve existing
knowledge. Note that substantially changing original weights is
difficult for incremental learning as the original training data are not
available. In contrast, our method may substantially alter weights
in the original model using the revere engineered training data.
Attack by Model Parameter Regression. In this effort, we as-
sume the access to a small part of the training data. This is reason-
able in practice. First, when a model is published, although the
full training data set is not published, it is likely that part of the
training data is published with the model to demonstrate the ability
of the model. Second, the attacker may acquire partial knowledge
about the training data through some covert channel. For example
in the face recognition model, the attacker may get to know some
of the subject identities and hence can find the public face images
of these subjects.
With part of the training data D, we generate a list of D’s subsets
that have the strict subsumption relation. For each subset d ∈ D
in the subsumption order, we train a model M′ to distinguish d
and (d+ trojan trigger), which can be considered a (partial) trojaned
model. Additionally, we train another model M from just d. Our
hypothesis is that by comparing the differences of M and M′ for
each d following the increasing subsumption order, we are able
to observe a set of internal neurons that are changing and hence
they are relevant to recognizing the trojan trigger. By performing
regression on the values of these neurons, we can project how they
would change when the full training data were used to retrain.
Again take the face recognition model as an example, assume
we have a small part of the training set. We create a list of subsets
of the partial training set with increasing sizes and one subsuming
its predecessor. Then we retrain the model based on each subset.
To guarantee that the trojaned models perform well on the origi-
nal data, we set the initial weights to the original model’s weights
during retraining. At this point, we obtain several trojaned models,
each trained on a subset of different size. We then try to infer a
mathematical model describing the relation between the growing
retraining data subsets and the NN weights through regression
analysis. And then we predict the final trojaned NN from the math-
ematical model. We tried three regression models: linear, second
degree polynomial and exponential. Table 2 shows the results. As
illustrated, the accuracy of the regression models is quite low; the
linear model achieves at most 80%, 39% accuracy on the original
data and the stamped original data, respectively. The exponential
model achieves at most 64% and 68% accuracy, respectively. Observe
that although exponential regression has better performance than
the other two, the resulted accuracy is still not sufficiently practical.
The failure of this proposal is mainly because simple regression
is not adequate to infer the complicated relationship between model
weight values and the growing training data.
Table 2: Regression results
Regression Model
Linear Model
2nd Degree Polynomial Model
Exponential Model
Original Dataset
39%
1%
64%
Original dataset + Trigger
80%
1%
68%
Finding Neurons Corresponding to Arbitrary Trojan Trigger.
Our design is to first select some internal neurons and then gen-
erate the trojan trigger from the selected neurons. The trigger is
computed instead of being provided by the attacker. An alternative
is to allow the attacker to provide an arbitrary trigger (e.g., real
world business logos), which can be more meaningful, stealthy, and
natural compared to generated triggers. Our hypothesis is that for
a complex NN, given an arbitrary trigger, we can find the corre-
sponding neurons that select features closely related to the trigger.
We can hence tune the weights of these neurons to achieve our
7
goal. Assume we have part of the training data. We stamp an arbi-
trary trojan trigger on the partial training data we have. Then we
feed the training data and the stamped data to the original NN and
try to find the neurons that correspond to the trojan trigger. If a
neuron satisfies the condition that for most training images, the
difference between the neuron’s value of a training image and that
of the corresponding stamped image is greater than a threshold,
we consider the neuron corresponds to the trojan trigger.
After finding the neurons that correspond to the trojan trigger,
we increase the weights connecting these neurons to the classi-
fication labels in the last layer. However, this proposal was not
successful either. Take the face recognition model as example. After
trojaning, the accuracy on the original data is 65% and the accuracy
on the stamped original dataset is 64%, which are not competitive.
The reason is that there are often no particular neurons that sub-
stantially more relevant to an arbitrary trigger than others. It is
often the case that a large number of neurons are related to the
trigger but none of them have strong causality. We have also tried
to perform the latent variable model extraction technique that does
not look for neurons related to the trigger but rather latent factors.
The results are not promising either. Details are elided
6 EVALUATION
6.1 Experiment Setup
We apply the attack to 5 different neural network applications: face
recognition (FR) [39], speech recognition (SR) [10], age recognition
(AR) [29], sentence attitude recognition (SAR) [28], and auto driving
(AD) [3]. Table 3 shows the source of the models (column 1), the
number of layers (column 2) and the number of neurons (column 3)
in these models. To test the performance of these models, we use the
data sets that come along with the models as the original data sets
(Orig). Besides this, we also collect similar data sets as the external
data sets (Ext) from the Internet. For face recognition, the original
data sets are from [13] and the external data sets are from [26]. For
speech recognition, the original data sets are from [10] and the
external data sets are from [34]. For age recognition, the original
data sets are from [1, 19] and the external data sets are from [26].
For sentence attitude recognition, the original data sets are from [8]
and the external data sets are from [9, 30]. In auto driving, the
original model is trained and tested in a specific game setting and it
is hard to create a new game setting, so we do not use external data
sets in this case. We run the experiments on a laptop with the Intel
i7-4710MQ (2.50GHz) CPU and 16GB RAM. The operating system
is Ubuntu 16.04.
6.2 Attack Effectiveness
The effectiveness of a Trojan attack is measured by two factors. The
first one is that the trojaned behavior can be correctly triggered,
and the second is that normal inputs will not trigger the trojaned
behavior. Table 3 illustrates part of the experimental results. In
Table 3, the first column shows the different NN models we choose
to attack. Column 4 shows the size of trojan trigger. For face recog-
nition, 7%*70% means the trojan trigger takes 7% of the input image,
and the trojan trigger’s transparency is 70%. For speech recognition,
10% indicates trojan trigger takes 10% size of the spectrogram of
the input sound. For age recognition, 7%*70% means the trojan
trigger takes 7% size of the input image, and the trojan trigger’s
transparency is 70%. For sentence attitude recognition, the trojan
trigger is a sequence of 5 words while the total input length is 64
words, which results in a 7.80% size. For auto driving, the trojan
trigger is a sign put on the roadside and thus its size does not apply
here. Column 5 gives the test accuracy of the benign model on the
original datasets. Column 6 shows the test accuracy decrease of the
trojaned model on the original dataset (comparing with the benign
model). Column 7 shows the test accuracy of the trojaned model on
the original dataset stamped with the trojan trigger while column
8 shows the test accuracy of the trojaned model on the external
dataset stamped with the trojan trigger. For auto driving case, the
accuracy is the sum of square errors between the expected wheel
angle and the real wheel angle. Auto driving case does not have
external data sets. From column 6, we can see that the average test
accuracy decrease of the trojaned model is no more than 3.5%. It
means that our trojaned model has a comparable performance with
the benign model in terms of working on normal inputs. Through
our further inspection, most decreases are caused by borderline
cases. Thus, we argue that our design makes the trojan attack quite
stealthy. Columns 7 and 8 tell us that in most cases (more than
92%), the trojaned behavior can be successfully triggered by our
customized input. Detailed results can be found in the following
subsections (FR, SR, SAR and AD) and Appendix A (AR).
Table 3: Model overview
Model
FR
SR
AR
SAR
AD
Size
#Layers
38
19
19
3
7
#Neurons
15,241,852
4,995,700
1,002,347
19,502
67,297
Tri Size
7% * 70%
10%
7% * 70%
7.80%
-
Ori
75.4%
96%
55.6%
75.5%
0.018
Accuracy
Dec
2.6%
3%
0.2%
3.5%
0.000
Ori+Tri
95.5%
100%
100%
90.8%
0.393
Ext+Tri
100%
100%
100%
88.6%
-
Neurons Selection: As discussed in Section 4, one of the most
important step in our design is to properly select the inner neurons
to trojan. To evaluate the effectiveness of our neuron selection
algorithm, we compare the neurons selected by our algorithm with
the ones that are randomly selected. In Table 4, we show an example
for the FR model. In this case, we choose layer FC6 to inverse.
Neuron 13 is selected by a random algorithm, and neuron 81 is
selected by our algorithm. Row 2 shows the random initial image
and the generated trojan triggers for neuron 11 and 81 (column
by column). Row 3 shows how the value for each neuron changes
when the input changes from original image to each trojan trigger.
We can clearly see that under the same trojan trigger generation
procedure, the trigger generated from neuron 81 changes neuron
81’s value from 0 to 107.06 whereas the trigger from neuron 11 does
not change the value at all. Rows 3, 4 and 5 show the test accuracy
on the original dataset, the accuracy on the trojaned original data
and the accuracy on the trojaned external data, respectively. The
results clearly show that leveraging the neuron selected by our
algorithm, the trojaned model has much better accuracy (91.6%
v.s. 47.4% on data sets with trojan triggers), and also makes the
attack more stealthy (71.7% v.s. 57.3% on the original data sets).
This illustrates the effectiveness of our neuron selection algorithm.
Table 4: Comparison between selecting different neurons