• (Vmem,T ,Vcch,T +Z) are the edges that represent bringing an object
into the cache, which happens when there is a miss. It is here
that we encode delayed hit latency into the cost. The capacity
of (Vmem,T ,Vcch,T +Z) is 1, and the cost is ∞ for all objects other
than σ(T). The cost of routing σ(T) along (Vmem,T ,Vcch,T +Z) is
the aggregate delay for requests of object σ(T) while the data is
being fetched; i.e., it is the total latency for the miss plus all re-
quests that arrive during the delayed hits. The miss experiences
a latency of Z, and a delayed hit that arrives t timesteps after the
miss experiences a latency of Z−t. Therefore, the cost is:
Z +
1{σ(T +t)=σ(T)}·(Z−t).
(2)
Z−1
t =1
In the above figure, the cost for all edges is 2 (the latency Z to the
backing store) except for the edge (Vmem,2,Vcch,4). Because A is also
requested at T =3, it will be queued and later served by the request
being fetched; as such, we need to account for both the cost of serving
the request at T =4 (which is 2) and the request at T =3 (which is 1).
Routing Flows: The MCMCF problem is to find routes for the ob-
jects such that the total routing cost is minimized. Specifically, the
routes are represented by flow variables, where each flow variable
represents whether an object/commodity is routed along an edge or
not. Here flow variables need to satisfy link capacity constraints and
flow conservation constraints, which will guarantee that the flow
variables can be converted to a valid cache schedule.
Equivalence to Latency-Minimizing Caching:
Theorem 1. belatedly’s underlying MCMCF problem (§A.2) is
equivalent to the latency minimization problem (§A.1).
The detailed proof of Theorem 1 can be found in §A.2. Both the
MCMCF problem and the latency minimization problems are op-
timization problems. To show that these are equivalent,4 we first
4At this juncture, one might ask: why bother with MCMCF instead of solving the
latency minimization ILP directly? The answer is three-fold. First, the LP is convoluted
and quite unintuitive (in fact, we discovered the MCMCF formulation first!). Second,
it is the network flow formulation that allows us to implement the optimizations
described in §A.3; without these, even modest LP instances of the problem are too
compute- or memory-intensive for today’s solvers. Finally, formulating the problem
Figure 7: Latency gap between Belady and belatedly for different
application scenarios (Network, CDN, Storage) today. Top to bottom:
1%, 5%, and 10% cache sizes.
Figure 8: %Relative latency difference between Belady and belat-
edly versus z. Cache size, c =5%.
show in Lemma 1 that the feasible set of flow variables is “equivalent”
to the feasible set of caching schedules (i.e. from any feasible cache
schedule, we can define a set of flow variables that are also feasible
for the MCMCF problem, and vice versa).
Lemma 1. Given a sequence of object requests, there is a bijection
between the set of feasible flow variables and the set of feasible cache
schedules.
Once we have this bijection, we can show that the objective func-
tions of these two problems are the same. With equivalent feasible
sets and objective functions, the MCMCF problem and the latency
minimization problem are thus equivalent.
3.2 Delayed Hits and Empirical Latencies
We now evaluate belatedly’s latency estimates relative to Belady
for a range of application scenarios.
belatedly provides significantly better average latency
than Belady for today’s highest-latency systems. In Figure 7,
we plot Belady’s percent error relative to the optimal upper-bound
provided by belatedly.5 For the highest latencies – referring to
Table 2, those with Z values in the thousands – Belady deviates from
the optimal by 9-37%. However, for more modest latencies to the
backing store, belatedly does not have noticeably lower latencies
than Belady. Even in the original FPGA-based switching scenario
which caused us to detect delayed hits, the gap between Belady and
belatedly is less than 1%.
as an MCMCF naturally leads to the notion of aggregate delay; as we show in §4, this
is a key component of our online algorithm.
5 (Belady−belatedly)
belatedly
×100%
AT=0cost = 2All other edges (not shown) have cost = 0BAABABAABABcost = 2cost = 3cost = 2cost = 2T=1T=2T=3T=4T=5T=6020400.08%0.51%0.91%16.46%1.76%12.14%0.86%1.80%9.09%DRAM Read (100 ns)6 DRAM Reads (500 ns)RDMA Access (5 us)DNS Lookup (5 ms)Rev. Proxy (500 us)Fwd. Proxy (5 ms)1MB Disk Read (1 ms)Disk Seek (3 ms)Cross-DC (150 ms)1MB Disk Read (1 ms)Disk Seek (3 ms)Cross-DC (150 ms)020400.04%0.30%0.67%33.23%5.13%16.87%0.20%1.54%13.14%Network (10 Gbps)CDNStorage020400.02%0.17%0.51%37.43%5.51%17.16%0.38%1.95%14.81%%Relative Latency Difference100101102103104Z02040%Rel. Latency Diff.NetworkCDNStorageCaching with Delayed Hits
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 9: %Relative latency difference between Belady and belat-
edly versus cache size (expressed as a percentage of the maximum
number of concurrent flows). Using Z =500.
Z is correlated with an increasing gap between Belady and
belatedly. In Figure 8 we see that for all three datasets, Belady
performs progressively worse with respect to true latency optimality
as Z increases – until Z moves past 10K. The growth correlation
follows intuition: as Z grows, there are more chances for delayed
hits to occur, and hence more opportunities for Belady to err. We find
that narrowing of the gap between Belady and belatedly beyond
Z = 10K is an artifact of our simulation duration; since Z is large
relative to the size of the trace (250K requests), it also exceeds the
duration of most flows. As a result, most requests experience ‘forced’
cache misses, raising the latency baseline and giving belatedly
fewer opportunities to make meaningful caching decisions.
The gap between Belady and belatedly varies with cache
size. In Figure 9, we see that the latency difference first rises, then
falls as the cache size increases. When the cache is extremely small,
neither belatedly nor Belady’s caching decisions have significant
impact on latency (since most requests experience cache misses, the
average latency is close to the full latency of a cache miss); similarly,
as the cache capacity becomes very large, both strategies can afford
to simply cache all or almost all objects (the extreme case being a
cache large enough to fit all concurrent flows or active objects). In
between, however, all three datasets ‘peak’ at different points. In
particular, the Network trace has a sharp spike at 10%, while the
CDN and Storage traces have more gradual curves.
belatedly’scachingdecisionsarecorrelatedwiththebursti-
ness of requests. The Goh-Barabasi Score [26] is a statistical mea-
sure of ‘burstiness’ in a sequence of events. A score of ‘1’ reflects
many arrivals in a short period of time (a ‘train’) followed by longer
periods with no requests. A score of ‘-1’ represents a perfectly paced
stream of arrivals with one request every fixed number of timesteps.
In Figure 10, we see that bursty traffic (with a high Goh-Barabasi
score) incurs a lower percent latency relative to Belady. This suggests
that burstiness may be a worthwhile candidate for consideration
in the design of online algorithms that optimize for latency in the
context of delayed hits. It is this observation that guides us in the
development of our online strategy, and we discuss it in more detail
in the following section.
4 APPROXIMATING BELATEDLY
belatedly provides two principal lessons for the design of im-
proved low-latency caching algorithms. First, belatedly demon-
strates that the opportunity for latency improvement is high: the
gap between latency-optimal and hit-rate optimal can be as much
as 45%. Second, belatedly provides us with a caching schedule that
achieves optimal latency for a given trace and Z value.
Figure 10: Relative latency improvement vs burstiness (for Network
traffic). Bursty flows suffer less under belatedly.
Unfortunately,belatedlyis slow–takingupto8hourstocompute
an optimal schedule for a trace with 250,000 requests – and requires
knowledge of the future.Bothofthesepropertiesmeanthatbelatedly
itself cannot serve as a caching algorithm for practical systems.
In this section, we learn from belatedly’s optimal schedule how
to achieve better latencies in practical implementations. In §4.1 we
first explore heuristics in the offline setting. In this setting, we still
assume an oracle with perfect knowledge of future requests, but
we target a computationally tractable algorithm. In §4.2 we then
move to a fully online setting where the algorithm both needs to be
efficient and operate without knowledge of future requests.
4.1 Offline Approximations: Belady-AD
We seek a heuristic ranking function which quickly tells us the pri-
ority of an object for our goal to minimize latency. In practice, almost
all caching algorithms use some ranking function, e.g., LRU – an
online algorithm – prioritizes objects by how recently they were last
used. Belady – an offline algorithm – is the inverse and ranks objects
by how soon they will be used in the future. These ranking functions
prioritize hit rate whereas we seek a ranking that minimizes latency.
To derive a ranking function, we look to belatedly. While we
cannot simply emulate belatedly’s behavior (unfortunately, flow
algorithms like belatedly don’t reveal how they make decisions), we
cansearchforeasilymeasurablemetricscorrelatedwithbelatedly’s
caching decisions. As we discussed in §3.2, belatedly prioritizes
caching bursty objects, i.e. those objects with a high Goh-Barabasi
score [26]. We experimented with ranking functions based on this
score. While these functions had excellent runtime performance (the
Goh-Barabasi score is a function of mean and variance, both of which
can be measured cheaply with online algorithms), they delivered
poor latency results. Therefore, burstiness on its own is not a good
ranking function, which confirms the intuition we derived in §2.4
Instead, we turn to another metric that is directly associated with
the latency cost of bursty flows: aggregate delay, which is computed
in Eq. (2). To compute the rank of an object, we assume that the ob-
ject’s next access in the future is a miss. Its aggregate delay is the sum
of the delay due to the miss and any delayed hits which occur during
the next Z timesteps while the object would be fetched. Intuitively,
an object with a higher delay cost – with a burst of requests during
that Z window – increases average latency more than an object with
a lower delay cost, and hence should be prioritized.
Nevertheless, aggregate delay by itself is still not an effective
ranking function. Consider the ranking of two objects A and B in a
cachewhere Z =3asshowninFigure11. Ahasanaggregatedelayof6
andwillnotbeaccessedforanother100timesteps. B hasanaggregate
100101102Cache Size%0102030%Rel. Latency Diff.NetworkCDNStorageSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger
Figure 11: Ranking objects solely based on aggregate delay may lead
to poor utilization of cache space.
delay of 5 and will be accessed only 10 timesteps in the future. Should
the rank function prefer A or B? Assuming we keep the cached object
until its next access, keeping A utilizes one cache line – which cannot
be used for other objects – for a very long interval. On average, each
timestep we keep A in the cache will ‘save’ an average of 6
100 units of
delay. On the other hand, for each timestep we keep B in the cache,
we save an average of 5
10 units of delay, with the opportunity to
cache other objects in the remaining 90 timesteps. Hence, B appears
to be – on average – a more efficient use of cache space.6
Following this intuition, our offline ranking function, belady-ad,
computes two values for each object. AддDelay(x) is the aggregate
delay for the next access to object x, and TT N A(x) is the number of
timesteps until the next access to x.7 The rank is then:
AддDelay(x)
TT N A(x)
Rank(x) =
(3)
We find that, across all Z values, the average request latency pro-
vided by belady-ad is within 0.1-12% of belatedly. In Figure 12, we
show the average latency for belady-ad and belatedly (normalized
against the performance of Belady’s algorithm) for a range of Z val-
ues for the CAIDA Chicago network trace; belady-ad closely trails
belatedly, although the gap between the two widens as Z grows.
Furthermore, belady-ad runs several orders of magnitude faster
than belatedly, computing a cache schedule in under 3 seconds for
a trace containing 250,000 requests, where belatedly would take
up to 8 hours.
4.2 Online Algorithm: mad
Finally, we turn to the true online setting, where we both need
to use simple heuristics to rank objects and do not have knowledge
of the future. Fortunately, we can use the past to make predictions
about the future. Just as LRU uses recency as a ranking function –
the ‘inverse’ of Belady’s algorithm – we need to ‘flip’ our measures
of AддDelay(x) and TT N A(x) to use data from past requests rather
than future ones.
6This intuition does not necessarily lead to optimal decisions! For example, if we were
to prefer B and evict A, but in the 90 timesteps after B no other requests arrived then
it would have been better to prefer A.
7Note that Belady’s algorithm uses the ranking function
1
T T N A(x) alone.
Figure 12: belady-ad closely trails belatedly.
1
Luckily,wealreadyhavealargeliteratureofestimatorsforTT N A(x),
asalmostallalgorithmsareessentiallypredictorsofthenextaccessto
an object. Recall that Belady’s algorithm ranks objects by TT N A(x)
alone, and is optimal in the absence of delayed hits. Hit-rate optimiz-
ing algorithms aim to operate as close to Belady as possible [55], and
so the closer their ranking function is to
TT N A(x), the better they
perform. Hence, in §5 we experiment with using LRU [64], ARC [41],
and LHD [5] as estimators of TT N A(x).
This leaves us with estimating AддDelay(x). Recall that we mea-
sure Aggregate Delay by assuming that the next request to object x
will be a miss, and computing the sum of delays for the miss to x and
any subsequent delayed hits for x. We ‘flip’ this by assuming that
all past requests to x were misses and then calculating the average
aggregate delay per miss; we illustrate this in Algorithm 1. We find
that this approximates the true AддDelay(x) well, e.g. with a Pearson
Correlation Coefficient of 0.7 for the network trace.
Finally, to create mad, we combine the code8 from Algorithm 1
with a known estimator forTT N A(x). We can now compute the rank
using Eq. (3).
Algorithm 1 Estimating AggregateDelay
1: struct ObjectMetadata
2:
3:
4:
5:
6: function EstimateAggregateDelay(X: ObjectMetadata)
7:
8: end function
9:
10: function OnAccess(TimeIdx, X: ObjectMetadata)
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: end function
// Time since start of the previous miss window
TSSW = (TimeIdx - X.WindowStartIdx)
if TSSW ≥ Z then
// This access commences a new miss window
X.NumWindows += 1
X.CumulativeDelay += Z
X.WindowStartIdx = TimeIdx
// This access is part of the previous miss window
X.CumulativeDelay += (Z - TSSW)
NumWindows = 0
CumulativeDelay = 0
WindowStartIdx = −∞
return X.CumulativeDelay
X.NumWindows
else
end if
We note that parallel work [40] in our group has shown that any
deterministic online algorithm for the delayed hits problem has a
competitive ratio9 of Ω(kZ), where k is the size of the cache. Despite
falling in that category, our empirical evaluations show that mad
yields considerable latency improvements over traditional caching
algorithms, and its simplicity lends itself well to implementation. We
leave to future work to find a randomized caching strategy which
improves upon mad’s worst-case performance.
8For the sake of brevity, the provided pseudocode assumes discrete timesteps and prior
knowledge of Z . Both of these assumptions are easily dispensable.
9The competitive ratio of an online algorithm, α, is the worst-case ratio between the
costs of the solution computed by α to that of the optimal, offline solution for the same
problem instance. Knowledge of a caching algorithm’s competitive ratio allows us to
impose bounds on its worst-case performance (i.e. for the most pessimal workload) [57].
BCache A A's Aggregate Delay: 6Cache B Z=3B's Aggregate Delay: 510timesteps90 timesteps to serve other objects?100 timestepsBAAA100101102103104Z0102030%Latency ImprovementRelative to BeladyPolicyBELATEDLYBelady-ADCaching with Delayed Hits
5 EVALUATION
We evaluate the effectiveness and the overhead of mad in a CDN
caching system. We then use our simulator from §2.3 to explore a