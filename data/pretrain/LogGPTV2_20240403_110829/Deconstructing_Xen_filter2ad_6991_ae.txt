We tested Nexen’s effectiveness against this type of attacks
by repeatedly calling a customized hypercall that hang for a
while and returns without restoring stack pointer. The hypercall
returns normally with the stack pointer restored every time.
Guest DoS (self) Although guest VM is not the primary
protection target, Nexen does provide some protection against
direct attacks aiming at a guest VM. Typically, a bug in
the VM’s Xen slice, mostly related to CPU virtualization, is
exploited by the VM’s user program to conﬁgure the guest’s
running state in a malicious way. For example, in XSA-40,
an incorrect stack pointer is set for the guest in an operation
that can be triggered by a user program. After returning to the
guest, the malicious running state will crash the VM’s kernel.
Other examples of this type include XSA-10, XSA-42, XSA-
103, and XSA-106. In Nexen, the important running states will
be checked by the gate keeper before context switching to
guest. Incorrect and malicious conﬁgurations are ﬁxed, which
will eliminate a considerable number of attacks.
We tested Nexen’s effectiveness against this type of attack
by calling a customized hypercall that sets the guest VM’s
11
program counter (PC) to 0 before returning. The hypercall
returns normally with PC properly restored.
Guest DoS (other) This attack type is very similar to
Guest DoS (self). The difference is that the bug in Xen slice
is triggered by another VM instead of the victim VM’s user
program. For example, in XSA-91, Xen fails to context switch
the ‘CNTKCTL_EL1‘ register, which allows a malicious guest
to change the timer conﬁguration of any other guest VM. Other
examples include XSA-33, XSA-36, and XSA-49. In Nexen,
the approach is also similar, namely, by checking the important
running states before context switching to guest and ﬁxing
incorrect and malicious conﬁgurations.
We tested Nexen’s effectiveness against this type of attack
by calling a customized hypercall that hang for a while and
set guest VM’s PC to 0 before returning. The hypercall returns
normally with PC properly restored.
Limitation Nexen has mainly three aspects of limitations,
which will be ﬁxed in our future work. First, Nexen cannot
handle vulnerabilities in the shared service. In our design,
shared service is a unique component and is shared by all
Xen slices. If a logic error residing in this part is exploited,
the hypervisor may be compromised. Second, Nexen does not
prevent abuse of I/O devices well. For example, disks are not
managed by Nexen, which may be exhausted to cause a DoS.
This problem can be addressed by extending Nexen’s features
to cover these I/O device resources. Third, since Nexen is
unable to capture all iret instructions used to return to a PV
guest currently, a PV guest’s Xen slice compromised by other
VMs can bypass the gate keeper’s sanity check and arbitrarily
modify the guest’s running state. Fortunately, this can only
result from a malicious administrator.
)
s
(
t
s
o
c
e
m
i
t
 550
 500
 450
 400
 350
 300
 250
 200
k
p
b
e
r
n
e
rl
b
zi
p
2
e
l
_
c
o
m
e
n
c
h
g
c
c
m
c
f
g
o
b
h
m
m
k
m
e
r
Nexen
Xen
h
o
a
sj
e
n
g
li
b
q
u
a
2
6
4
r
e
f
s
t
a
r
m
n
e
t
p
p
n
t
u
m
p
il
Fig. 6. Performance data of CPU and memory benchmarks.
)
s
/
B
K
(
t
u
p
h
g
u
o
r
h
T
 9e+06
 8e+06
 7e+06
 6e+06
 5e+06
 4e+06
 3e+06
 2e+06
 1e+06
 0
R
W
R
R
R
e
a
d
rit
e
e
-
r
e
a
d
e
-
w
rit
e
e
v
e
r
s
S
R
R
M
P
P
tri
d
e
R
a
n
a
n
d
o
d
o
ix
r
e
e
d
a
d
w
rit
e
e
R
e
a
d
e
a
d
m
m
W
R
W
e
a
d
rit
e
o
r
kl
o
a
d
Nexen
Xen
F
r
e
a
d
F
w
rit
e
i
p
e
rf
3
VI. PERFORMANCE EVALUATION
Fig. 7. Performance data of I/O benchmarks.
We evaluated Nexen’s performance overhead by running
standard benchmarks in a guest VM. We use SPEC CPU2006
and Linux kernel compilation to evaluate CPU and memory
overhead and IOzone (a ﬁlesystem performance benchmark)
and iperf3 (a network performance benchmark) to evaluate the
I/O overhead. The conﬁguration of the testing machine is listed
in the Table XI. The conﬁguration of benchmarks are listed in
Table XII. “Round” means the times we ran the benchmark.
We show the average results along with standard deviations in
bar graphs.
The results of CPU and memory related benchmarks are
presented in Figure 6. The Y-axis shows the running time
of benchmarks. For purely CPU-intensive applications, e.g.,
perlbench, h264ref, and astar, there is nearly no overhead.
This is reasonable because Nexen mostly lies in the critical
path of memory operations. CPU execution can rarely be
intercepted by Nexen. Even for the relatively memory-intensive
kernel compilation benchmark, the overhead, less than 1%, is
negligible. One reason is that the good control ﬂow pattern
of Nexen avoids excessive interleaving among different Xen
slices and the shared service. Xen hypervisor’s proper usage
of EPT related hardware feature reduces a lot of VMExits for
EPT violation, which further reduces the frequency of calling
Nexen’s sanity checking function There are benchmarks where
Nexen slightly out-performs the unmodiﬁed Xen, e.g., gcc,
mcf and libquantum. Considering they show relatively high
standard deviations, this could be attributed to measurement
variation.
The results for I/O related benchmarks are presented in Fig-
ure 7. Iperf3 is a simple tool measuring the network throughput
of a system. In our test, the PV drivers, now supported by the
native Linux kernel, were used for I/O. Since the data mostly
ﬂows through the shared memory between the guest VM and
Dom0, the hypervisor is out of the critical path of network I/O.
This explains the extremely low overhead in this test (0.02%).
IOzone tests various aspects of a ﬁlesystem, which indirectly
reﬂect the disk I/O throughput. 4KB block size, 20MB ﬁle size
and 4 threads were used in this test. The standard deviations
for this benchmark set is extremely large. We ran 50 rounds of
the test to stabilize the result as much as possible. Benchmarks
where Nexen out-performs the unmodiﬁed Xen are probably
result of measurement variation. However, we can not rule
out the possibility that Nexen changes the pattern of caching
and buffering in a way that favors these speciﬁc operations.
Generally speaking, reading operations are less affected by
Nexen compared to writing operations. The average overhead
in the I/O part is about 2.4%.
Overall, the average overhead of Nexen is about 1.2%.
Nexen mainly adds to the latency of VMExits and MMU
updates. With PV drivers and latest hypervisor version used
12
TABLE IX.
CASE STUDY ON DIFFERENT RESULTS AND TYPES OF ATTACKS.
Attack Type
False
BUG_ON
Number
6
Result
Host DoS
General Fault
9
Page Fault
26
Live Lock
9
Dead Lock
4
Inﬁnite Loop
8
Run Out of
Resource
4
Info Leak
Memory Out-
of-boundary
Access
11
Misuse Hard-
ware Feature
3
Guest
DoS (self)
Various
10
Various
1
Guest
DoS
(other)
Target
Reason
Logic Error
Number
15
Shared Part
Not
Supported
Feature
Not
Supported
Resource
Hardware
Bug
7
2
3