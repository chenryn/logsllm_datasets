### Evaluation of Nexen's Effectiveness Against Various Attack Types

#### Testing Methodology
To evaluate Nexen’s effectiveness against specific types of attacks, we conducted a series of tests by repeatedly calling customized hypercalls that simulate various attack scenarios. The results are detailed below.

#### Guest DoS (Self)
**Description:**
Although the guest VM is not the primary protection target, Nexen provides some level of protection against direct attacks aimed at a guest VM. These attacks typically exploit a bug in the VM’s Xen slice, often related to CPU virtualization, allowing a user program within the VM to configure the guest’s running state in a malicious manner. For example, in XSA-40, an incorrect stack pointer is set for the guest during an operation triggered by a user program. After returning to the guest, this malicious state can crash the VM’s kernel. Other examples include XSA-10, XSA-42, XSA-103, and XSA-106.

**Nexen's Approach:**
In Nexen, the gatekeeper checks important running states before context switching to the guest. Incorrect and malicious configurations are fixed, thereby eliminating a significant number of attacks.

**Test Results:**
We tested Nexen’s effectiveness by calling a customized hypercall that sets the guest VM’s program counter (PC) to 0 before returning. The hypercall returned normally with the PC properly restored, demonstrating Nexen’s ability to mitigate such attacks.

#### Guest DoS (Other)
**Description:**
This attack type is similar to Guest DoS (self), but the bug in the Xen slice is triggered by another VM rather than the victim VM’s user program. For example, in XSA-91, Xen fails to context switch the 'CNTKCTL_EL1' register, allowing a malicious guest to change the timer configuration of any other guest VM. Other examples include XSA-33, XSA-36, and XSA-49.

**Nexen's Approach:**
Similar to the previous scenario, Nexen checks important running states before context switching to the guest and fixes incorrect and malicious configurations.

**Test Results:**
We tested Nexen’s effectiveness by calling a customized hypercall that hangs for a while and sets the guest VM’s PC to 0 before returning. The hypercall returned normally with the PC properly restored, indicating effective mitigation.

### Limitations of Nexen

Nexen has three main limitations, which will be addressed in future work:

1. **Vulnerabilities in Shared Services:**
   - **Issue:** Nexen cannot handle vulnerabilities in shared services, which are unique components shared by all Xen slices. Exploitation of a logic error in this part can compromise the hypervisor.
   - **Solution:** Future work will focus on enhancing Nexen to address these vulnerabilities.

2. **I/O Device Abuse:**
   - **Issue:** Nexen does not prevent abuse of I/O devices effectively. For example, disks are not managed by Nexen, which can be exhausted to cause a DoS.
   - **Solution:** Extending Nexen’s features to cover I/O device resources will help mitigate this problem.

3. **IRET Instruction Handling:**
   - **Issue:** Nexen currently cannot capture all `iret` instructions used to return to a PV guest. A compromised PV guest’s Xen slice can bypass the gatekeeper’s sanity check and modify the guest’s running state.
   - **Solution:** This limitation can only result from a malicious administrator, and future work will aim to address this issue.

### Performance Evaluation

We evaluated Nexen’s performance overhead by running standard benchmarks in a guest VM. The benchmarks used were SPEC CPU2006 and Linux kernel compilation for CPU and memory overhead, and IOzone and iperf3 for I/O overhead. The testing machine and benchmark configurations are listed in Tables XI and XII, respectively.

#### CPU and Memory Benchmarks
The results are presented in Figure 6. The Y-axis shows the running time of benchmarks. For purely CPU-intensive applications like `perlbench`, `h264ref`, and `astar`, there is nearly no overhead. This is reasonable because Nexen mostly lies in the critical path of memory operations, and CPU execution is rarely intercepted. Even for the relatively memory-intensive kernel compilation benchmark, the overhead is less than 1%, which is negligible. Some benchmarks, such as `gcc`, `mcf`, and `libquantum`, showed slightly better performance with Nexen, possibly due to measurement variations.

#### I/O Benchmarks
The results for I/O related benchmarks are presented in Figure 7. Iperf3, a tool for measuring network throughput, showed extremely low overhead (0.02%) due to the use of PV drivers supported by the native Linux kernel. IOzone, which tests various aspects of the filesystem, used a 4KB block size, 20MB file size, and 4 threads. The standard deviations for this benchmark were large, and we ran 50 rounds to stabilize the results. Reading operations were less affected by Nexen compared to writing operations, with an average overhead of about 2.4% in the I/O part.

### Overall Performance
The average overhead of Nexen is about 1.2%. Nexen mainly adds to the latency of VMExits and MMU updates. With the use of PV drivers and the latest hypervisor version, the performance impact is minimal.

### Case Study on Different Results and Types of Attacks

| Attack Type | False | BUG_ON | Number | Result | Target | Reason | Logic Error | Number | Shared Part | Not Supported | Feature | Not Supported | Resource | Hardware Bug |
|-------------|-------|--------|--------|--------|--------|--------|-------------|--------|-------------|---------------|---------|---------------|----------|--------------|
| Host DoS    |       |        | 6      | General Fault | 9 | Page Fault | 26 | Live Lock | 9 | Dead Lock | 4 | Infinite Loop | 8 | Run Out of Resource | 4 | Info Leak | Memory Out-of-boundary Access | 11 | Misuse Hardware Feature | 3 |
| Guest DoS (self) | Various | 10 | Various | 1 |  |  |  |  |  |  |  |  |  |  |
| Guest DoS (other) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |

This table summarizes the different types of attacks, their results, and the reasons behind them.