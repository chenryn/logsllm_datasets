mon applications, the ambiguity-factor is low and relaxed
ranking is a good replacement for strict ranking.
3.4 Type-Based Customization
Recall that our deﬁnition 1 is structural. In other words,
the ordering deﬁnes ≺A based on ≺A1 and ≺A2 , which im-
plicitly have similar (potentially recursive) deﬁnitions. The
result is a family of ordering relations, one for each non-
terminal A. Algorithm 1 provides a template for the cor-
responding ranking functions. However, the soundness of
Deﬁnition 1 requires only that ≺A1 is a correct ordering
for M P T (G, A1), it does not require that ≺A1 be deﬁned
precisely as we do. This means that, if some nontermi-
nal B generates trees that are more eﬃcient to compare
1298and rank using a diﬀerent method, then we can use that
method instead to deﬁne ≺B and respectively to implement
rankB. As long as rankB provides a correct ordering for
M P T (G, B), then the overall ranking of M P T (G, S) is still
correct. Speciﬁcally, if the language derived from B is reg-
ular, then ranking for regular languages can be used. Our
library and code generation tool (presented in Sec. 4) uses
this important optimization to allow ranking of grammars
whose tokens are speciﬁed as regular expressions.
3.5 Beyond CFGs
This section shows how some language features that are
not context-free can be potentially handled by our method-
ology. If a format uses arbitrary speciﬁed delimiters, length
speciﬁers, or checksums, then the language it describes is
not context free. Nevertheless, in many cases we can still
use our approach to obtain a relaxed ranking scheme for the
language, as long as the intermediate set can be ranked, and
the user provides eﬃcient implementations for map and gen.
We say that a language L is cfg-parseable if there is a CFG
G with set I=M P T (G, S), and two functions map : L → I
and gen : I → L such that ∀x ∈ L : gen(map(x)) = x.
Let’s consider an example. Assume that the data consists
of a sequence of items. Each item starts with a counter
which represents the size of the item in bytes, followed by a
sequence of bytes, and it ends with a checksum byte.
 :=  |  
 :=  BYTE* CHECKSUM
 := [0-9]+
The data described by this format is not a context free lan-
guage, because of the counter and the checksum. However,
the counter is used only for parsing, and both the counter
and the checksum are completely deﬁned by the data.
If
properly parsed, the content of the data is given by the fol-
lowing context free grammar G
 :=  |  
 := BYTE | BYTE 
Then, as in the case of ranking a CFG, we take the interme-
diate set I = M P T (G, Data). We assume that the user has
a deterministic parser, that maps each element in the initial
language L to a tree in I. The reverse transformation gen
maps trees from I back to L by inserting the appropriate
checksums and the counters.
Note that, for technical reasons, when we build the mem-
oization tables we may want to account for the length of the
counter and checksum. In that case we can use the grammar
 :=  |  
 := CT  CS
 := BYTE | BYTE 
Here CT and CS are just properly sized constant terminals,
which are used as placeholders to be replaced with with the
proper data by the function gen.
There is a large class of languages L that are not CFG,
but which can be eﬃciently parsed to minimal parse-trees
for some CFG G. Parsing discards or replaces those ex-
tra elements in the original language (such as counters or
checksums) that prevent it from being context free, if such
elements can be inferred from the rest of the tree (otherwise
our scheme does not apply). The reverse transformation
takes a minimal parse-tree and provides its yield annotated
with exactly that additional information that is discarded
by parsing (such as counters or checksums) and that can be
inferred from the tree.
4.
IMPLEMENTATION
We implemented a library in C++ and a tool for relaxed
ranking with CFGs. The library oﬀers a low-level interface
to describe any CFG, and it oﬀers relaxed ranking based
on parse trees. To aid developers, our tool supports con-
verting from more user-friendly descriptions of CFGs to the
speciﬁcations handled by the library. It also supports LALR
grammars. See Figure 3 for diagrams depicting the library
and tool.
We note that, up to this point, we have for simplicity
explained our algorithms assuming CFGs are in weak normal
form (WNF), but in fact we have implemented algorithms
that work for any CFG.
4.1 Library for ranking CFG parse-trees
Our library oﬀers an API to build an internal represen-
tation of any grammar G. The API is low-level, meaning
that the user must deﬁne a C++ ranker class, using our
library, where all grammar components are individually en-
coded: terminals, non-terminals, and rules. Objects of the
resulting class can rank/unrank minimal parse-trees of G, as
shown in Figure 3. We used the GNU Multiple Precision
Arithmetic Library (GMP [1]) to represent large integers.
As explained in Section 3.4, there may be parts of the
grammar for which alternative ranking methods work bet-
ter. Our library is object-oriented and oﬀers a default im-
plementation of Algorithm 1, which the user can change as
they see ﬁt.
One such particular case is so important that we intro-
duced an alternative ranking for it. This is the case of lexi-
cal tokens. Consider the grammar GD in Figure 4. GD does
not have a pure context free speciﬁcation because the token
ID is speciﬁed using regexes. The grammar can be turned
to a pure CFG by replacing the deﬁnition of ID with
ID := ’a’|’b’|...|’Z’|’a’ID | ...|’Z’ID
Such a change is undesirable because: (1) people prefer using
regexes for lexical tokens, and (2) ranking a regular language
is usually much faster using a DFA or NFA method [2, 14]
than using the CFG method.
As shown in Figure 3, the user is responsible for producing
a deterministic parser. If the user provides a parser, then
the ranker object (built using our library) performs relaxed
ranking of words in L(G).
4.2 Code Generation Tool
We provide a tool that takes a simple grammar speciﬁ-
cation and produces: (1) source code for a ranker object
based on our API, and (2) a LEX/YACC-based parser for
the grammar (provided that the grammar is LALR). The
design goal of our tool was to specify a grammar in an in-
tuitive way (such as Backus Normal Form, or LEX/YACC
syntax as in Figure 4) and to eﬀortlessly generate from it
library code. Figure 3 gives a diagrammatic description of
this process. The lower box in Figure 4 shows how to spec-
ify the grammar GD discussed previously for input to our
tool. The tool uses a LEX/YACC syntax for its rules. We
made this decision because users are already familiar with
LEX/YACC, and many grammars already have a LEX/Y-
129913005. EXPERIMENTAL RESULTS
5.2 Performance
Our experiments were designed to answer the following
questions about our library and tool:
1. How easy is it to produce the (un)ranking code?
2. How eﬃcient is our method?
To answer the ﬁrst question we report on our experience
using the library and tool. While this is a limited experience,
it oﬀers a high-level comparison of diﬀerent use cases of the
library and the tool.
To answer the second question, consider the rank-and-
encipher method in Section 2. This method is modular,
and its overall performance is determined by: (1) the size of
the intermediate set; (2) the performance of relaxed ranking
and unranking; (3) the number of repetitions of the loop
(i,e, the length of the cycle walk); and (4) the performance
of the intermediate integer encryption step. The quality of
relaxed ranking only inﬂuences the ﬁrst three items, and so
we focus our evaluation on microbenchmarking these items.
Because eﬃcient relaxed ranking requires memoization, we
also report the memoization time and approximate space.
5.1 Our experience using the tool
We evaluated three usage scenarios: (1) the user writes a
new grammar speciﬁcation using the low level API; (2) the
user writes a new grammar speciﬁcation using our tool; and
(3) the user converts an existing LEX/YACC parser to use
our tool.
API – New Speciﬁcation. We used our library’s low-level
API to manually specify the GD grammar from Section 4.1.
We also wrote a parser to be used by the ranker code. In to-
tal, it took about one author one hour; we used LEX/YACC
for the parser. This time quickly increased with grammar
size. Because of this, we did not attempt manual encoding
of much larger grammars.
Tool – New Speciﬁcation. Using our tool, the speciﬁ-
cation for the GD grammar took about 5 minutes and the
code was generated in under 1 second. For a new grammar,
using our tool is simpler than specifying the grammar us-
ing LEX/YACC, because only the rules need to be deﬁned
(without the supporting code and data structures involved
in real parsers).
Tool – Existing Speciﬁcation. We downloaded a spec-
iﬁcation for the C99 language from [5]. We chose C99 be-
cause it is a well known example of a complex CFL. The
YACC rules did not need any modiﬁcation. The LEX rules
contained embedded code, and it took about 10 minutes to
remove it. In the end, we obtained a 350 line speciﬁcation
that contained 68 non-terminals, 63 lexical tokens (deﬁned
using regular expressions), 24 constant tokens and 236 rules.
Our tool turned this 350 line speciﬁcation (no empty lines or
comments) into a speciﬁcation consisting of 2,769 lines of C
code. This took about 18 seconds. The majority of time was
spent checking for lexical token clashes. After the initial run,
it took about 30 minutes to debug the initial failures caused
by the fact that the regular expressions understood by our
library use a 256-byte character set, and string literals and
comments expanded to strings that caused scanning prob-
lems. After those errors were removed, ranking/unranking
and parsing worked correctly.
We evaluate relaxed ranking with CFGs by measuring four
values: (1) memoization time, (2) memoization space, (3)
ranking time, and (4) unranking time. We also report on
grammar ambiguity and language size.
5.2.1 Methodology
We parameterize our results along two dimensions: gram-
mar and language slice size. If L is a language and n ∈ N,
then L’s slice of size n is the set of strings of length n in L,
= {w ∈ L : |w| = n}. Slice size determines (among
i.e., L
other things) the dimension of the memoization tables.
(n)
Grammars. We explore three grammars:
• C99 is the C grammar mentioned in Section 5.1.
• C992 is a grammar with the same syntax as C99, but
C992 has identiﬁer and constant tokens of at most two
characters long.
• GD is the grammar deﬁned in Figure 4, Section4.1 .
To explain why we investigated C992, note that in practice
C programs contain reasonably sized identiﬁers. But this
does not necessarily hold for a random program obtained by
unranking an arbitrary number. For instance, when we in-
spected the output of unranking random value generated for
a 4000-byte slice, we observed identiﬁers as long as 68 char-
acters. We used C992 to provide experience when avoiding
such large tokens.
We used slices between 1000 and 4000 bytes (no comments
or white spaces counted, in the case of C99 and C992), at
100 byte intervals.
(n)
Tests. For each grammar G (with start symbol S) and
slice size n we run the following experiment. We performed
using the intermediate set I =
relaxed-ranking of L(G)
M P T (G, S, n), the set of minimal parse trees T with (cid:17)T(cid:17) =
n.
In the ﬁrst stage, we evaluate the memoization and com-
pute the size of the intermediate set using N = |I| = NS(n);
NS is given by Algorithm 3 (when A = S). When NS(n) is
called for the ﬁrst time, it has the side eﬀect of ﬁlling the
memoization tables. We report |I|, as well as the total time
and space required by memoization. The space is estimated
by reading the data size from the /proc interface, before
and after memoization, because it is hard to measure the
exact space used by the large integers in the GMP library.
In the second stage, we evaluate ranking and unranking,
and we estimate the ambiguity-factor. We repeat ranking
and unranking C = 100 times. For each 1 ≤ k ≤ C we let
r = (N − 1)×(cid:22)k/C(cid:23) and compute w = Unrank(r) which is a
= Rank(w). If r (cid:18)= r
(cid:3)
string in L(G)
then r (cid:18)∈ Img(L(G)
) and we say that r is an outsider, and
w is an ambiguous string. We report the average time for a
single ranking and unranking operation, and the number U
of outsiders.
. Then we compute r
(cid:3)
(n)
(n)
It is easy to see that for a uniform distribution as C grows
towards N , we can approximate the ambiguity-factor β =
N/|L| as β (cid:24) C/(C − U ).
5.2.2 Microbenchmarking Results
We ran all the experiments on a laptop with 32GB RAM
and a i7-2720QM CPU with 4 cores running at 2.2GHz. Our
implementation is single-threaded, so the number of cores
does not aﬀect the results.
13016,000
4,000
2,000
)
s
(
e
m
T
i
3,000
2,000
1,000
)
B
M
(
y
r
o
m
e
M
Rank
Unrank
600
400
200
)
s
m
(
e
m
T
i
0
1,000
1,500
2,000
2,500
3,000
3,500
4,000
0
1,000
1,500
2,000
2,500
3,000
3,500
4,000
0
1,000
1,500
2,000
2,500
3,000
3,500
4,000
)
s
(
e
m
T
i
5,000
4,000
3,000
2,000
1,000
3,000