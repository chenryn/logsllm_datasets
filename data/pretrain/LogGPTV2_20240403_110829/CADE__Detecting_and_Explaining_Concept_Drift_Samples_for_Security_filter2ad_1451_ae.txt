The above evaluation conﬁrms the impact of the selected
features on the distance metric, which is what CADE is de-
signed to optimize. To provide another perspective, we further
examine the impact of the selected features on crossing the
boundary. More speciﬁcally, we calculate the ratio of per-
turbed samples that successfully cross the decision boundary.
As shown in Table 6, we conﬁrm that crossing the boundary
in the drifting detection context is difﬁcult for most of the
settings. In particular, CADE can push 97.64% of the perturbed
samples to cross the detection boundary for the Drebin dataset,
but only have 1.41% of the samples cross the boundary for the
IDS2018 dataset. In comparison, the baseline methods can
rarely successfully perturb the drifting samples in the original
feature space to make them cross the boundary. By loosing up
this condition and focusing on distance changes, our method
is more effective in identifying important features.
5.4 Case Studies
To demonstrate our method indeed captures meaningful fea-
tures, we present some case studies. In Table 5, we present a
case study for the Drebin dataset. We take the setting when
FakeDoc is the unseen family and randomly pick a drifting
sample to run the explanation module. Out of 1000+ features,
our explanation module pinpointed 42 important features,
among which 27 features have a value of “1” (meaning this
USENIX Association
30th USENIX Security Symposium    2337
sample contains these features). As shown in Table 5, the
closest family is GingerMaster.
We manually examine these features to determine if the
features carry the correct semantic meanings. While it is dif-
ﬁcult to obtain the “ground-truth” explanations, we gather
external analysis reports about FakeDoc malware and Ginger-
Master [68,70]. Based on these reports, a key difference from
GingerMaster is that FakeDoc malware usually subscribes
to premium services via SMS and bill the victim users. As
shown in Table 5, many of the selected features are related to
permissions and APIs calls for reading, writing, and sending
SMS. We highlight these features that match SMS related
functionality. Other related features are highlighted too. For
example, the permission of “RESTART_PACKAGES” allows
the malware to end the background processes (e,g., that dis-
plays incoming SMS) to avoid alerting the users. The per-
mission of “DISABLE_KEYGUARD” allows the malware to
send premium SMS messages without unlocking the screen.
“WRITE_SETTINGS” is also helpful to write system settings
for sending SMS stealthily. “url::https://ws.tapjoyads.com/”
is an advertisement library usually used by FakeDoc. Again,
this small set of features is selected from over 1000 features.
We conclude that these features are highly indicative of how
this sample is different from the nearest known family.
6 Evaluation: In-class Evolution
So far, our evaluation has been focused on one type of con-
cept drift (Type A) where the drifting samples come from
a previously unseen family. Next, we explore to adapt our
solution to address a different type of concept drift (Type B)
where the drifting samples come from existing classes. We
conduct a brief experiment in a binary classiﬁcation setting,
following a similar setup with that of [38].
More speciﬁcally, we ﬁrst use the Drebin dataset to train
a binary SVM classiﬁer to classify malware samples from
benign samples. The classiﬁer is highly accurate on Drebin
with a training F1 score of 0.99. We want to test how well
this classiﬁer works on a different Android malware dataset
Marvin [42]. Marvin is a slightly newer dataset (from 2010
to 2014) compared with Drebin (from 2010 to 2012). We ﬁrst
remove Marvin’s samples that are overlapped with those in
Drebin, to make sure the Marvin samples are truly previously
unseen. This left us 9,592 benign samples and 9,179 malware
samples in Marvin.
For this experiment, we randomly split the Marvin dataset
into a validation set and a testing set (50:50). For both sets,
we keep a balanced ratio of malware and benign samples.
We apply the original classiﬁer (trained on Drebin data) on
this Marvin testing set. We ﬁnd that the testing accuracy is no
longer high (F1 score 0.70) due to potential in-class evaluation
in the malware class and/or the benign class.
To address the in-class evolution, we apply CADE and Tran-
scend on the Marvin validation set to identify a small number
# Selected Samples
0
100
150
200
250
Transcend
F1 of Retrained Classiﬁer
CADE
0.70
0.91
0.92
0.93
0.94
0.70
0.71
0.76
0.74
0.71
Table 7: Performance of the retrained classiﬁer on the Marvin
testing set. We used CADE and Transcend to select the drifting
samples to be labeled for retraining.
of drifting samples (they could be either benign or malicious).
We simulate to label them by using their “ground-truth” labels
and then add these labeled drifting samples back to the Drebin
training data to retrain the binary classiﬁer. Finally, we test
the retrained classiﬁer on the Marvin testing set.
As shown in Table 7, we ﬁnd that CADE still signiﬁcantly
outperforms Transcend. For example, by adding only 150
drifting samples (1.7% of Marvin validation set) for retraining,
CADE boosts the binary classiﬁer’s F1 score back to 0.92. For
Transcend, the same number of samples only gets the F1 score
back to 0.74. In addition, we ﬁnd that CADE is also faster: the
running time for CADE is 1.2 hours (compared to the 10 hours
of Transcend). This experiment conﬁrms CADE can be adapted
to handle in-class evolution for a binary malware classiﬁer.
7 Real-world Test on PE Malware
We have worked with the security company Blue Hexagon
Inc. to test CADE on their proprietary sample set. More specif-
ically, we run an initial test on Blue Hexagon’s Windows
malware database. In this test, we got access to a set of sam-
ples collected from August 29, 2019, to February 10, 2020.
This set includes 20,613 unique Windows PE malware sam-
ples from 395 families. We use this dataset to test CADE in
a more diverse setup (i.e., the drifting samples come from a
larger number of families).
PE Malware Dataset.
For each sample, we have the raw
binary ﬁle and the metadata provided by Blue Hexagon, in-
cluding the timestamp when the samples were ﬁrst observed,
and the family name (labeled by security analysts). We fol-
low the feature engineering method of Ember [6], and use
LIEF [63] to parse the binary ﬁles and extract the feature
vectors. Each feature vector has 2,381 dimensions. These fea-
tures include the frequency histogram of bytes and the entropy
of different bytes, printable strings and special patterns, fea-
tures about ﬁle size, header information, section information,
imported libraries and functions, exported functions, and the
size and virtual addresses of data directories.
Family Attribution Experiments.
The original classiﬁer
is a multi-class classiﬁer to attribute malware families. Our
goal is to use CADE to detect unseen families that should not
be attributed to existing families. We split the dataset based on
2338    30th USENIX Security Symposium
USENIX Association
N
5
10
15
Precision Recall
0.96
0.96
0.95
0.98
0.94
0.80
F1
0.97
0.95
0.87
Norm. Detected
Families
Effort
161/165
1.02
153/160
0.98
0.84
140/155
Table 8: Drifting detection results for the PE malware dataset.
N is the number of known families in the training set. “De-
tected Families” indicates the number of new families CADE
detected out of all the new families.
time. The training set contains the malware samples collected
from August 29 in 2019, to January 10 in 2020. The testing
set contains samples collected in the following month, from
January 10 to February 10, 2020. For training, we need to
make sure the malware families have enough samples to train
the original classiﬁer. So we focus on the top N families. We
test three settings with N = 5, 10, and 15, respectively. This
makes sure the training families contain at least 298 samples
per family in all the settings. Samples that are not in the top
N families are excluded from the training set. Such a mini-
mal number of samples is necessary for the original classiﬁer
to have reasonable accuracy. For example, the accuracy for
N = 15 is 96.5%. The classiﬁer can potentially support more
families if the dataset is larger. For the testing set, all the fam-
ilies are kept. In addition, based on the suggestion from Blue
Hexagon’s analytics team, we add two families (Tinba and
Smokeloader) to the testing set because they have observed
that these families have more success in evading existing ML-
based malware detection engines. As shown in Table 8, the
testing set has 155 to 165 previously unseen families, i.e., the
target of CADE.
Results and Case Studies.
Table 8 shows that CADE still
performs well under this diverse set of samples with more
than 155 previously unseen families. CADE achieves an F1
score of 95% when the number of training families N = 10.
The F1 score is still 0.87 when N = 15. Most of the previously
unseen families are successfully identiﬁed. Indeed, a larger
number of families has made the problem more challenging.
The reason is not necessarily because existing families and
unseen families are difﬁcult to separate. Instead, with more
training families, we observe more testing samples within
the existing families that drift even further away compared to
those in the unseen families. These in-family variants become
the main contributor to false positives under our deﬁnition.
The observation is similar to our case study in Section 4.2. As
a quick comparison, we also run Transcend on this N = 15
setting. We ﬁnd CADE still outperforms Transcend on the more
diverse unseen families (Transcend’s F1 score is only 0.76).
We did a quick feature analysis using the explanation mod-
ule on Tinba and Smokeloader which are proven to be chal-
lenging examples for the underlying classiﬁer. Tinba (tiny
banker trojan) targets ﬁnancial websites with man-in-the-
browser attacks and network snifﬁng. Smokeloader is a trojan
that downloads other malware. It is an old malware family
but evolves rapidly. In particular, we ﬁnd the new samples in
Tinba are closest to an existing family Wabot. CADE pinpoints
45 features to offer explanations. For example, we ﬁnd Tinba
enables the “LARGE_ADDRESS_AWARE” option, which
tells the linker that the program can handle addresses larger
than 2 gigabytes. This option is enabled by default on 64-bit
compilers. This provides some explanation on why Tinba has
the success in evading existing malware detection engines,
given that the vast majority of PE malware ﬁles are 32-bit
based. Based on features about “sections,” we notice that the
Tinba sample uses “UPX” as the packer. Based on selected
features of imported libraries and functions, we ﬁnd Tinba
imports “crypt32.dll” for encrypting strings. Tinba samples
are different from Wabot samples on these features.
8 Discussion
Computational Complexity.
CADE’s computational over-
head is smaller than existing methods. The complexity of
the detection module contains two parts: contrastive learning
and drifting detection. The complexity of contrastive learn-
ing is O(IB2|θ|), where I, B, and |θ| represent the number of
training iterations, batch size, and model parameters of the au-
toencoder. The complexity of drifting detection (Algorithm 1)
is O(N ˜ni +NK), where N, ˜ni, and K are the number of classes,
the maximum number of training samples in each class, and
the number of testing samples, respectively. The overall com-
plexity of CADE detection module is O(IB2|θ| + N ˜ni + NK).
Our training overhead is acceptable since it is only quadratic
to the batch size B. Our detection runtime overhead is sig-
niﬁcantly lower than that of Transcend (which is O(N ˜niK)).
Empirically, we have recorded the average runtime for the
detection experiments (Section 4), and conﬁrms that CADE is
faster than Transcend. For example, on the larger IDS2018
dataset, the average run time for CADE and Transcend are
1,422.7s and 4,289.3s. Regarding the explanation module,
CADE is comparable with boundary-based explanation meth-
ods and COIN. For example, for the IDS2018 dataset, the
average runtime of CADE, COIN, and boundary-based explana-
tion for explaining one drifting sample are 3.2s, 8.2s, and 3.7s
respectively. The boundary-based explanation also requires
an additional 76.5s on average to build the approximation
model for the explanation.
Explanation vs. Adversarial Attacks. We notice that the
explanation module in CADE shares some similarities with
the adversarial example generation process, e.g., both involve
perturbing the given input for a speciﬁc objective. However,
we think they are different for two reasons. First, they have
different outputs. Adversarial attack (with the goal of evasion)
directly outputs the perturbation needed to cross the decision
boundary; Our explanation method (with the goal of under-
standing the drift) outputs the important features that affect
the distance. Second, they have different constraints on the
USENIX Association
30th USENIX Security Symposium    2339
perturbations. Our explanation method only tries to minimize
the number of perturbed features, while the adversarial attack
constrains the magnitude of the perturbation too. More impor-
tantly, adversarial samples need to be valid for the respective
applications (i.e., valid malware samples that can be executed
and maintain the malicious behavior, valid network ﬂows that
can carry out the original attack). To these ends, generating
adversarial samples can be more difﬁcult than deriving ex-
planations in our context. That said, the adversarial attack is
out of the scope of this paper. We leave adversarial attacks
against CADE to future work (i.e., creating non-perceptible
perturbation to convert a drifting sample to an in-distribution
sample).
Limitations and Future Work.
Our work has a few limi-
tations. First, CADE ranks all the drifting samples in a single
list. However, in practice, the drifting samples may contain
substructures (e.g., multiple new malware families). A practi-
cal strategy could be further grouping drifting samples into
clusters. In this way, security analysts only need to inspect
and interpret a few representative samples per cluster to fur-
ther save time. Second, certain hyper-parameters of CADE are
determined empirically (e.g., the MAD threshold). We have
included an Appendix C to test the sensitivity of CADE to
hyper-parameters. Future work can look into more systematic
strategies to conﬁgure the hyper-parameters. Third, CADE is
designed based on the assumption that the training set does
not have mislabeled samples (or poisoning samples). We de-
fer to future work to robustify our system against low-quality
or malicious labels. Fourth, our experiments are primarily fo-
cused on detecting new families. In Section 6, we only brieﬂy
experimented with concept drift within existing families (in-