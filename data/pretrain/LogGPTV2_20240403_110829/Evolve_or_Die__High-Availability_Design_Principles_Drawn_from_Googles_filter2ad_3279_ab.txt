users can be directed to other front-ends or back-ends) when
a large failure occurs. Finally, latency is a key performance
determinant for user-facing services: the user-facing service
hierarchy enables proximity of front-ends to users, enabling
low latency access to content.
We classify trafﬁc on Google’s network into multiple
priority classes to accommodate the differing quality needs
of user-facing services and internal customers, and to
ensure that important trafﬁc is always served. Generally,
user-facing trafﬁc is assigned higher priority, and internal
trafﬁc is assigned lower priority. Our WANs implement
trafﬁc differentiation in slightly different ways: B4 uses
priority queueing in switches, together with trafﬁc marking,
admission control, and bandwidth enforcement at the edges;
B2 relies on trafﬁc marking and QoS implementation in
vendor gear, and ensures high priority trafﬁc stays on the
shortest path by mapping low priority trafﬁc to low priority
tunnels.
3. AVAILABILITY CHALLENGES
2In the rest of the paper, we use the term bundle to denote an aggregate
collection of physical links, and link to refer to a physical link.
Maintaining high availability has been, and continues
to be, a major focus of network architecture and design at
60
Google.
3.1 Network Availability Targets
The network strives to achieve different availability tar-
gets for different classes of trafﬁc. Since network layer avail-
ability is only one component of the overall availability bud-
get of a software service, the network must meet a fairly
stringent availability target, of only a few minutes per month
of downtime, for at least some classes of trafﬁc.
Google tracks availability targets for different
trafﬁc
classes at per-minute time-scales. On B4, we have sufﬁcient
instrumentation (used to perform trafﬁc engineering) to
directly measure the time durations for which the bandwidth
promised to services could not be satisﬁed between each
pair of B4BRs for each trafﬁc class. For B2 and clusters, we
use a system similar to [14] to determine unavailability per
trafﬁc class between each pair of clusters; our system uses
ping measurements between clusters, and can disambiguate
between unreachability within clusters and unreachability
on paths between clusters.
It declares a cluster to be un-
available for a given trafﬁc class if packet loss for that trafﬁc
class, from all other clusters, is above a certain threshold.3
3.2 Challenges
There are four inter-related reasons why achieving avail-
ability targets is challenging within Google’s network.
Scale and Heterogeneity. Google’s network spans the
globe, is engineered for high content delivery capacity, and
contains devices from a wide variety of network vendors,
in addition to several generations of internally-developed
hardware and software. The scale of the network means
that there is a high probability of at least one device or
component failure, or some malfunctioning or misconﬁg-
ured software, within the network at any given instant in
time. We explicitly deploy devices from two vendors, for
hardware heterogeneity. Heterogeneity also arises from
scale: it takes time to upgrade the network, so at any instant,
the network might have 2-3 generations of, for example,
cluster technologies. While heterogeneity ensures that the
same issue is unlikely to affect multiple components of the
network, it can also introduce fragility and complexity. Gear
from different vendors require different management plane
processes, and their software may be upgraded at different
rates. Merchant silicon chips of different generations expose
slightly different capabilities that need to be abstracted
by switch software, thereby increasing complexity. This
heterogeneity is fundamental, given our evolution velocity
(discussed below), and we attempt to manage it using careful
software development and management place processes.
Velocity of Evolution. The rapid growth in global IP trafﬁc
(5⇥ over the past ﬁve years, and similar projected growth
[8]), of which Google has a signiﬁcant share, necessitates
rapid evolution in network hardware and software at Google.
This velocity of evolution is accentuated by growth in the
number of products Google offers. This velocity, coupled
3The loss threshold varies by trafﬁc class from 0.1%-2%.
61
with scale, implies that, with high likelihood, either the soft-
ware or hardware of some part of the network is being up-
graded every day, which can further exacerbate fragility.
Device Management Complexity. A third challenge in
achieving high availability is the complexity of managing
modern networking devices, especially at higher levels
of aggregation. For example, in 2013, Google employed
multiple 1.28Tb/s chassis in their WAN [18]. Today, some
commercially-available devices support 20Tb/s [19].
In
response to increasing aggregation, control plane architec-
tures have achieved scaling by abstracting and separating
the control from the data plane, but management paradigms
have not kept pace, typically still considering the network
as a collection of independently managed devices. For
instance, most management tools still permit CLI-based
conﬁguration, making scripting and automation error
prone, and management tools expose management at the
granularity of individual devices or individual switch chips
in a B4BR.
Constraints Imposed by Tight Availability Targets. The
tight availability targets of a few minutes a month can also
present an imposing challenge. For example, in some cases,
upgrading a border router can take more than 8 hours. Such
a long upgrade process introduces a substantial window of
vulnerability to concurrent failures. To avoid failures dur-
ing planned upgrades, we could drain services away from
affected clusters, but if we did this for every upgrade, given
the velocity of our evolution, it could affect our serving ca-
pacity. Manually draining services can also be error-prone.
So, many planned upgrades must upgrade in-place, which
can also increase network fragility.
3.3 Baseline Availability Mechanisms
At the beginning of our study, Google’s network em-
ployed several advanced techniques for ensuring high
network availability. All of these mechanisms are in
place today as well, but some of these have evolved, and
additional mechanisms have been put in place, based on the
experience gained from the availability failures described in
the rest of the paper.
First, our clusters and WAN topologies are carefully ca-
pacity planned to accommodate projected demand. We also
engineer our network to tolerate failures of key components
such as routing engines, power supplies or fans on individual
devices, as well as failure of bundles or devices, by using re-
dundant B2BRs and B2CRs. To be resilient to physical plant
failures, we use disjoint physical ﬁbers and disjoint power
feeds.
Second, every logically centralized control plane compo-
nent (from the FC, RA, and OFC in the fabrics to BwE,
Gateway, TE Server, and Topology Modeler) is replicated
with master/slave replication and transparent failover. The
WAN control plane replicas are placed in geographically di-
verse locations. On B2, we use MPLS protection to achieve
fast re-routing in case of failures, and MPLS auto-bandwidth
to automatically adapt tunnel reservations to ﬂuctuating de-
mand [27].
Third, we have an ofﬂine approval process by which ser-
vices register for speciﬁc trafﬁc priorities. Service devel-
opers receive guidelines on what kinds of trafﬁc should use
which priority and they must specify trafﬁc demands when
requesting approval for a priority. Once a service is granted
approval for a speciﬁc demand, BwE marks the service’s
trafﬁc with the appropriate priority and rate-limits the trafﬁc.
Fourth, we have several management plane processes de-
signed to minimize the risk of failures. We use regression
testing before rolling out software updates and deploy ca-
naries at smaller scales before deploying to the entire net-
work. We also periodically exercise disaster scenarios [20,
17] and enhance our systems based on lessons from these
exercises. We carefully document every management oper-
ation (MOp) on the network. Examples of MOps include
rolling out new control plane software, upgrading routers or
bundles, installing or replacing components like line-cards,
optical transmitters, or switch ﬁrmware. Each MOp’s doc-
umentation lists the steps required for the operation, an es-
timated duration, and a risk assessment on the likelihood of
the MOp affecting availability targets. If a MOp is deemed
high risk, operators drain affected services before executing
the MOp.
4. POST-MORTEM REPORTS
At Google, we have a process by which we document each
large failure in a post-mortem report and identify lessons
from the failure, so that its recurrence can be avoided or mit-
igated. As such, each post-mortem report identiﬁes a failure
that impacted the availability targets discussed above, which
we term a failure event. The report includes the network lo-
cation of the failure, its duration, and its impact on trafﬁc
volumes and packet loss rates as well as impact on services.
It is co-written by members of different teams whose sys-
tems were impacted by, or caused, the failure event. It con-
tains a timeline of events (if known) that led to the failure,
and the timeline of steps taken to diagnose and recover from
the failure. It also contains an accurate characterization of
the root-cause(s) for the failure. A single failure event can
have more than one root cause; operators and engineers con-
ﬁrm these root causes by reproducing the failure, or parts
thereof, either in the ﬁeld, or in a lab. Many of the re-
ports also include detailed diagrams that give context, to a
broader audience, for the failure. Finally, the reports contain
a list of action items to follow up from the failure, which can
range from changes to software or conﬁguration to changes
to management plane processes. Each action item is usually
followed up and discussed in a bug tracking system.
The process of writing a post-mortem is blame-free and
non-judgemental. Peers and management review each post-
mortem for completeness and accuracy [5]. This ensures
that we learn the right lessons from the failure and avoid fu-
ture occurrences of the same failure. Furthermore, not every
availability failure is documented in a post-mortem; if one
failure is a recurrence of another failure for which a post-
mortem report was written up, it is not documented because
62
there are no new lessons to be learned from this failure.
Dataset. In this paper, we have collected and analyzed all
post-mortem reports (103 in number) for network failures
in Google over the past two years.
In the rest of the pa-
per, we present an analysis of this dataset, describe some of
the failure events to give the reader some intuition for the
magnitude and complexity of our availability failures, and
conclude with a discussion on design principles drawn from
these failures.
5. ANALYSIS OF FAILURE EVENTS
By Network and Plane. Figure 2 shows the distribution of
failure events across the three networks. No single network
dominates, and failures events happen with comparable fre-
quency 4 across all three networks. Clusters see the most
failures (over 40), but B4 saw over 25 failures. This implies
that there is no natural target network type for focusing our
availability improvement efforts.
Post-mortem reports also include a discussion of the root-
causes of the failure. From these descriptions, we attributed
failure events to one of three planes: data, control, and man-
agement. Data plane failures include hardware failures, and
failures due to device or operating system limitations. Man-
agement plane failures are those that could be attributed to a
MOp in process, and control plane failures result from in-
correct state propagation or other undesirable interactions
between control plane components.
Attributing failures to planes sometimes requires making
an informed judgement. For example, when a failure event
had multiple root-causes (e.g., a link failure triggered a bug
in the control plane), was this a data plane or a control plane
failure? In the example above, we attributed the failure to
the control plane, since the control plane arguably should
have been robust to link failures. However, if a link failure
coincided with planned network maintenance operation that
should have been safe due to redundancy but caused conges-
tive loss, we attributed the failure to the data plane.
Figure 2 also shows the distribution of failure events
across planes by network. All three networks are susceptible
to failures across control, data, and management planes
and no one plane dominates any network, with the possible
exception of B4 where control plane failures outweigh
data and management plane failures taken together. Thus,
availability improvements must target all three planes.
By Structural Element. Our networks have many struc-
tural elements: fabrics, ToRs, CARs, the B2BRs, and so
forth. Figure 3 shows how failure events are distributed
across these structural elements. There are ﬁve structural
elements which are each responsible for 10 failure events or
more. The B2BRs, the CARs and the fabrics occupy crit-
ical positions in the topology, and any failure in these can
result in degraded availability. Two other pain points in the
4Our post-mortem reports only document unique failures, not all failures.
As such, we use the term frequency to mean frequency of occurrence of
unique failures. For this reason also, we have not analyzed the frequency of
failure events over time, because we do not have data for all events.
Control
Data
Management
t
n
e
m
e
l
E
k
r
o
w
t
e
N
ToR
Topology Modeler
Gateway
Fabric
CPN
CAR
BwE
B4BR
B4 Bundle
B2CR
B2BR
B2 Bundle
10
Count of Failure Events
20
30
40
0
Cluster
k
r
o
w
t
e
N
B4
B2
0
High Priority/Low Loss
High Priority/High Loss
e
r
u
l
i
a