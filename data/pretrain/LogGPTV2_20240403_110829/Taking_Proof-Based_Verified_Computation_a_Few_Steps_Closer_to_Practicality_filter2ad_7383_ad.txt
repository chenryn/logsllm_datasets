32-bit signed integers
32-bit signed integers
32-bit unsigned
rationals (Na = 32, Nb = 5)
size of F
128 bits
320 bits
128 bits
192 bits
128 bits
320 bits
s
2m2
2m2
m
m
2m2 + m
16 · (m + |C<|)
n
m3
m3
m2
m3
2m3
256 · (m + |C<|)2
default
m = 200
m = 100
m = 100
m = 200
m = 100
m = 25
local
800 ms
5.90 ms
0.40 ms
160 ms
0.90 ms
180 ms
Figure 5—Benchmark computations. s is the number of constraint variables; s affects n, which is the size of V’s queries and of P’s
linear function π (see Figure 2). Only high-order terms are reported for n. The latter two columns give our experimental defaults and
the cost of local computation (i.e., no outsourcing) at those defaults. In polynomial evaluation, V and P hold a polynomial; the input
is values for the m variables. The latter two computations exercise the program constructs in Section 4.2. In m-Hamming distance,
V and P hold a ﬁxed set of strings; the input is a length m string, and the output is a vector of the Hamming distance between the
input and the set of strings. Bisection method refers to root-ﬁnding via bisection: both V and P hold a degree-2 polynomial in m
variables, the input is two m-element endpoints that bracket a root, and the output is a small interval that contains the root.
We use six benchmark computations, summarized in
Figure 5 (Appendix E [46] has details). For bisection
method and degree-2 polynomial evaluation, V and P
were produced by our compiler; for the other compu-
tations, we use tailored encodings (see Section 5). We
implemented and analyzed other computations (e.g., edit
distance and circle packing) but found that V gained from
outsourcing only at implausibly large batch sizes.
Method and setup. We measure latency and comput-
ing cycles used by the veriﬁer and the prover, and the
amount of data exchanged between them. We account
for the prover’s cost in per-instance terms. Because the
veriﬁer amortizes costs over a batch (§2.3), we focus on
the break-even batch size, β∗: the batch size at which the
veriﬁer’s CPU cost from GINGER equals the cost of com-
puting the batch locally. We measure local computation
using implementations built on the GMP library (except
for matrix multiplication over rationals, where we use na-
tive ﬂoating-point).
For each result that we report, we run at least three ex-
periments and take the averages (the standard deviations
are always within 5% of the means). We measure CPU
time using getrusage, latency using PAPI’s real time
counter [3], and network costs by recording the number
of application-level bytes transferred.
Our experiments use a cluster at the Texas Advanced
Computing Center (TACC). Each machine is conﬁgured
identically and runs Linux on an Intel Xeon processor
E5540 2.53 GHz with 48GB of RAM. Experiments with
GPUs use machines with an NVIDIA Tesla M2070. Each
GPU has 448 CUDA cores and 6GB of memory.
Validating the cost model. We will sometimes predict
β∗, V’s costs, and P’s costs by using our cost model
(Figure 2), so we now validate this model. We run mi-
crobenchmarks to quantify the model’s parameters—e is
reported in this section; Appendix E [46] quantiﬁes the
other parameters—and then compare the parameterized
model to GINGER’s measured performance. GINGER’s
empirical results are at most 2%–15% more than are pre-
dicted by the model. However, local computation costs
about 1.2–4.0 times more than is predicted; we think that
the divergence results from adverse caching effects that
increase the cost of a multiplication. Thus, we expect the
veriﬁer to break even at batch sizes that are about a factor
of 1.2–4.0 smaller than predicted by the model.
6.1 The effect of GINGER’s protocol reﬁnements
We begin with m × m matrix multiplication (m =
100, 200) and degree-3 polynomial evaluation (m =
100, 200), and batch size of β = 5000. We report per-
instance network and CPU costs: the total network and
CPU costs over the batch, divided by β.
Figure 6 depicts network costs. For matrix multipli-
cation, these are about the same as the cost to send the
inputs and receive the outputs; for polynomial evalua-
tion, these are about 10 times the size of the inputs and
outputs. Also, GINGER improves on PEPPER by 20–30×.
In this experiment, GINGER’s prover incurs about 10–
14% less CPU time compared to PEPPER (estimated us-
ing a cost model from [45]) but still takes tens of min-
utes per-instance; this is obviously a lot, but we reduce
8
local
veriﬁer per-instance
veriﬁer aggregate
prover per-instance
prover aggregate
mat. mult.
17.6 ms
17.6 ms
76.1 s
3.1 min
9.3 days
mat. mult. (Q)
5.90 ms
80.2 ms
5.7 min
9.4 min
28 days
Figure 8—Predicted running times of GINGER’s veriﬁer and
prover for matrix multiplication (m = 100), under integer and
ﬂoating-point inputs, at β = 4300 (the break-even batch size
for this computation over integers). The “local” row refers to
GMP arithmetic for Z and native ﬂoating-point arithmetic for
Q. Handling rationals costs GINGER roughly 3× more than
handling integers, but both are still far from native.
computation (Ψ)
m-Hamming dist.
bisection method
# Boolean gates (est.)
1.3 · 106
3.0 · 108
# constraint vars.
2 · 104
1528
Figure 9—GINGER’s constraints compared to Boolean circuits,
for m-Hamming distance (m = 100) and bisection method
(m = 25). The Boolean circuits are estimated using the un-
modiﬁed Fairplay [39] compiler. GINGER’s constraints are not
concise but are far more so than Boolean circuits.
Under scenario (3), we take e = 0µs. What about sce-
nario (2)? Our cost model concerns CPU costs, so we
need an exchange rate between GPU and CPU exponen-
tations. We make a crude estimate: we measure the num-
ber of encryptions per second achievable on an NVIDIA
Tesla M2070 (which is 180,000) and on an Intel 2.5
GHz CPU (which is 13,700), normalize by the dollar
cost of the chips, and obtain that their throughput-per-
dollar ratio is 1.8×. We thus (very conservatively) take
e = 72.1/1.8 = 40µs.
We plug these three values of e into the cost model in
Figure 2, set the cost under GINGER equal to the cost of
local computing, and solve for β∗. The values of β∗ are
4150 (CPU), 2300 (crude GPU estimate), and 20 (crypto
hardware). We also use the model to predict V’s and P’s
costs at β∗, under PEPPER and GINGER. Figure 7 summa-
rizes. GINGER is very sensitive to the value of e because
its reﬁnements have eliminated many of the other costs.
Moreover, the aggregate veriﬁer computing time drops
signiﬁcantly under all three cost models. The prover’s
per-instance work is mostly unaffected, but as the batch
size decreases, so does its aggregate work.
6.2 Evaluating GINGER’s computational model
To understand the costs of the ﬂoating-point representa-
tion (§4.1), we compare it to two baselines: GINGER’s
signed integer representation and the computation exe-
cuted locally, using the CPU’s ﬂoating point unit. Our
benchmark application is matrix multiplication (m =
100). Figure 8 details the comparison.
We also consider GINGER’s general-purpose program
constructs (§4). Our baseline is Boolean circuits (we are
Figure 6—Per-instance network costs of GINGER and its base
(PEPPER [45]), compared to the size of the inputs and outputs.
At this batch size (β = 5000), GINGER’s reﬁnements reduce
per-instance network costs by a factor of 25–30 compared to
PEPPER. GINGER’s network costs here are hundreds of KB or
less. The y-axis is log-scaled.
PEPPER
GINGER
local
β∗
veriﬁer aggregate
prover aggregate
prover per-instance
β∗
veriﬁer aggregate
prover aggregate
prover per-instance
β∗
veriﬁer aggregate
prover aggregate
prover per-instance
1.1 s
13000
3.9 hr
5.0 yr
3.5 hr
8700
2.7 hr
3.5 yr
3.5 hr
3900
1.2 hr
1.6 yr
3.5 hr
CPU
GPU
crypto
hardware
1.1 s
4100
1.3 hr
1.6 yr
3.3 hr
2300
43.4 min
320 days
3.3 hr
20
22.3 s
2.8 days
3.3 hr
Figure 7—Break-even batch sizes (β∗) and predicted running
times of prover and veriﬁer at β = β∗, for matrix multiplication
(m = 400), under three models of the encryption cost. The
veriﬁer’s per-instance work is not depicted because it equals the
local running time, by deﬁnition of β∗. The local running time
is high in part because the local implementation uses GMP.
latency by parallelizing (§6.3). For this computation and
at this batch size (β = 5000), GINGER’s veriﬁer takes a
few hundreds of milliseconds per-instance, less than lo-
cally computing using our baseline of GMP.
Amortizing the veriﬁer’s costs. Batching is both a lim-
itation and a strength of GINGER: GINGER’s veriﬁer must
batch to gain from outsourcing but can batch to drive per-
instance overhead arbitrarily low. Nevertheless, we want
break-even batch sizes (β∗) to be as small as possible.
But β∗ mostly depends on e, the cost of encryption (Fig-
ure 2), because after our reﬁnements the veriﬁer’s main
burden is creating Enc(pk, r) (see §2.3), the cost of which
amortizes over the batch.
What values of e make sense? We consider three sce-
narios: (1) the veriﬁer uses a CPU for encryptions, (2)
the veriﬁer ofﬂoads encryptions to a GPU, and (3) the
veriﬁer has special-purpose hardware that can only per-
form encryptions. (See Section 5 for motivation.) Under
scenario (1), we measure e = 72.1µs on a 2.5 GHz CPU.
9
100102104106matrix mult(m=100)matrix mult(m=200)d-3 poly eval(m=100)d-3 poly eval(m=200)network costs(KB)input+outputinput+outputinput+outputinput+outputPepperPepperPepperPepperGingerGingerGingerGingerFigure 10—Latency speedup observed by GINGER’s veriﬁer when the prover is parallelized. We run with m = 100, β = 150 for
matrix multiplication and degree-3 polynomial evaluation; m = 100, β = 1500 for degree-2 polynomial evaluation; m = 100, β =
15 for m-Hamming distance; and m = 25, β = 15 for bisection method. GINGER’s prover achieves near-linear speedups except
when the problem sizes are small and hence the overhead from parallelizing is signiﬁcant (e.g., degree-2 polynomial evaluation).
unaware of efﬁcient arithmetic representations of these
constructs). We compare the number of Boolean circuit
gates and the number of GINGER’s arithmetic constraint
variables, since these determine the proving and verify-
ing costs under the respective formalisms (see [5, 45]).
Taken individually, GINGER’s constructs (<=, &&, etc.)
are the same cost or more than those of Boolean cir-
cuits (e.g., || introduces auxiliary variables). However,
Boolean circuits are in general far more verbose: they