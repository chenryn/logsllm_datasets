I am working on the IBM POWER9 Series cluster. When compute the fft with
torch. I have to transfer it to GPU and then transfer back to CPU. It's real
bad experience espcially when realted code called on Dataloader. Then I have
to disable the multi-worker to prevent the error.
    x = torch.randn(10,10,2)
    torch.fft(x,1)
    Traceback (most recent call last):
      File "", line 1, in 
    RuntimeError: fft: ATen not compiled with MKL support