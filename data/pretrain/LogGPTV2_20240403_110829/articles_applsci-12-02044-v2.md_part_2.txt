Toacquirethelogmessagetypes,theMoLFI(Multi-objectiveLogmessageFormat
Identification) method [24] utilizes the multi-objective genetic algorithm, NSGA-II [25].
NSGA-II first randomly generates a pool of chromosomes, which is also known as the
population. Achromosomeisapossiblesolutionfortheprobleminquestion. Toimitate
theselectionandreplicationprocessthatcanbeseeninnature,itimprovesandevolves
the chromosomes through continuous iterations, which are called generations. In each
iteration, NSGA-II employs binary tournament selection [25] to find the best solutions
thatwillbereproduced. Withtheuseofmutationandcrossover,anewchromosomeis
createdfromtwoofthechromosomesfromthecurrentiteration. Crossoverswapspartsof
theparentchromosomestocreatenewones,whilemutationslightlychangesthenewly
generatedchromosomes. Fromthesechromosomes,anewgenerationiscreatedwiththe
useofcrowdingdistance[25]. TheMoLFIalgorithmworksinasimilarfashion.
BeforeusingthestepsofNSGA-II,MoLFIpre-processesdatawiththeuseofdomain
knowledgeandregularexpressionstofilteroutclearlyvariabletokens,suchasIPaddresses,
errorcodesthatconsistofonlynumbers,andsoon. Suchguidelinescanbefoundin[22].
Thesevariablesarechangedtoaspecial“#spec#”tokenthatcannotbemodifiedlater. The
deletionofduplicateentriesandthetokenizationarealsoperformedhere. Themessages
arealsopartitionedbasedonthenumberoftokenstheyhave. Intheend,thegroupG
L
containsalltheentriesthatcontainLtokens.
Toacceleratethespeed,atwo-levelencodingschemaisdefined,whereachromosome
contains a set of groups that are each a collection of event types with the same length.
Moreformally,
C = {G ,G ,...,G }, (1)
1 2 max
whereagroup,G = {t ,t ,...,t },isasetcontainingktemplatesthatconsistofthesame
L 1 2 k
numberofwords,L. Withthisencodingschema,theyensurethatonlyentriesofthesame
lengtharematchedinthelatersteps.
In the first step, the initial population P is created from M, which is a set of pre-
processed log messages, and N, the size of the population. After the creation of one
previouslyintroducedCchromosome,itispackedwithonegroupofeventtypes,G ,for
L
eachgroup. Atfirst,G isempty. Thealgorithmselectsalogentryfromtheunmatched
L
collection(initially,everyentryislocatedhere)andcreatesattemplatewhichisacopyof
theoriginalmessageexceptthatoneofitstokensischangedtoawildcard“*”. Thetokento
Appl.Sci.2022,12,2044 6of32
bemodifiedisselectedrandomly. Theentryisthendeletedfromtheunmatchedsetandthe
templatetisaddedtoG . Thisisrepeatedaslongastheunmatchedsetcontainsmessages.
L
Inthenextstep,theuniformcrossoveroperation[26,27]isusedtoshufflethecharacter-
isticsoftheparentsthatwereselectedfromthepreviouspopulationwiththeuseofbinary
tournamentselection[25],P = {G ,G ,...,G }andP = {G ,G ,...,G }.
1 1P1 2P1 maxP1 2 1P2 2P2 maxP2
Arandombinaryvectorisusedtocreatetwochildren,C andC .Ifthevector’sithelement
1 2
is1,thenC inheritsG andC inheritsG ;ifthevector’sithelementisanythingother
1 iP2 2 iP1
than1,theresultwillbetheotherwayaround.
Inthelaststep,thenewlygeneratedchildrenaremutated. Eachgroupinthechro-
mosomehasa 1 chanceofbeingmutated. Themutationisperformedbychangingone
Gmax
ofitst templateswiththeremovaloradditionofavariabletoken. Eachtokenhasa 1
1 tk
probabilityofbeingchanged.
AttheendoftheNSGA-IIalgorithm,thesetoffeasiblesolutionsispost-processed,
namely,thekneepoint[28],aParetooptimalsolution,isselectedtobethefinalproduct.
3.4. GeneralCompressors
Acomprehensivestudyaboutthegeneralcompressorscanbefoundin[7]. Theycan
becategorizedintothreedifferentgroupsbasedontheideaofhowtheywork.
ThefirstoneconsistsofSorting-basedcompressors,thatusedifferentapproachesto
movesimilardatatogetherinordertoobtainbettercompressionratios. Aconventional
methodistheBurrows–Wheelertransformation(BWT)[29]. Itrearrangescharactersbased
oncontext,thuscreatingrunsofsimilarcharacters. Thisisuseful,sincetechniquessuch
asrun-lengthencodingtendtomoreeasilycompressstringsthathaverunsofthesame
character. It is also important to point out that this transformation can be reversed. To
achievethis,onlythepositionofthefirstoriginalcharacterhastobestored. TheBWTis
usedbyBzip2.
There are Dictionary-based compressors that maintain a dictionary based on the
alreadyprocesseddata, whichisusedtoreplaceduplicateinstancesofdata. Onesuch
algorithmistheLempel–Ziv–Markov-chainalgorithm(LZMA).ItissimilartoLZ77[30],
exceptitsupportsdictionarysizesofupto4GBandhasaspecialschemethatchooses
phrases(notgreedilyasinLZSSorLZ77)andaparticularschemeofencodingforphrases.
Thealgorithmproducesphrasesandastreamofliteralswhicharethenencodedbitbybit
withtheuseofarangeencoder.
Prediction-basedcompressorsapplystatisticalmodelsinordertopredictupcoming
symbolsbasedoncontext. Thiscanbeusedtolowerthenumberofbitsthatareneeded
toencodethenextcharacter. Forexample,predictionbypartialmatching(PPMd)[31,32]
predictstheupcomingsymbolinanuncompressedstreamofcharacterswiththeuseof
a set of previously known symbols. A probability is allocated to each previously seen
symbol,andtheseprobabilitiesarethenusedtocompressthesequence. ThePPMduses
theprevious16tokenswhileassigningprobability,anditsmemorylimitissetto256MB.
4. TheAlgorithm
Theoriginalalgorithmthatwasproposedin[21]worksasfollows. First,weemploy
a template miner, i.e., an algorithm that obtains the event types corresponding to the
processed log entries. A message template is made up of constant tokens and “”
wildcardsthatindicatethelocationofaparametertoken. Forexample“NPUSoftware
”isthemessagetypeofthesecondlogentryinFigure1. Then,weassignanIDto
eachofthemessagetemplates,hence,atemplatedictionaryisconstructed. Afterthis,each
messageisassignedtoitsassociatedtemplate. Weusetheaforementioneddictionaryto
encodeourlogentriesasfollows. Fromaloglinethatcorrespondsto M eventtype,and
ID
consistsoflog = (c ,c ,...,c )∪(p ,p ,...,p ),wherec ,c ,...,c aretheconstanttokens
1 2 r 1 2 q 1 2 r
oftheentry,and p ,p ,...,p aretheparameters,anewloglinelog = ID,p ,p ,...,p is
1 2 q 1 2 q
createdwiththeuseofonlytheparametersandthe ID. Withthistechnique,wewereable
Appl.Sci.2022,12,2044 7of32
toachieveupto67.4%compressionratewhichcontainsboththesizeofthecompressed
fileandthedictionary.
Toincreasethecompressionrateofouralgorithm,wefirstexaminedtheproperties
of our data. We came to the conclusion that our data, which is detailed in Section 5.1,
followsthepower-lawdistribution. ThiscanbeseeninFigures2–5. Thex-axisrepresents
theordered(basedonoccurrences)templatesfrom1ton,wherenisthenumberofthe
discoveredtemplatesinthedataset.
Figure2.DistributionofthetemplatesintheSmalldataset.
Figure3.DistributionofthetemplatesintheMiddataset.
Figure4.DistributionofthetemplatesintheLargedataset.
Appl.Sci.2022,12,2044 8of32
Figure5.DistributionofthetemplatesintheBigdataset.
Itcanbeseenthatthereareonlyafeweventtypeswithalargenumberofoccurrences
andplentythatappearonlyonceortwice. Basedonthisprinciple,wedecidedtoassignthe
IDsbasedontherepetitionofthetemplate. Eventswithhigherfrequencieswouldobtaina
smallerID,whichcanbestoredonfewerbits,whilehigherIDswouldbeassignedtorare
messagetypes. TheIDsarestoredasintegerswitha32-bitfixedsize,thatarelaterencoded
usingtheHuffmancoding.
Whileaparameterofamessagetypecouldhavemorethanonevalue,thesevaluesare
usuallychosenfromafinitesetofvalues. Tofurtherreducethesizeofthecompressedlog
file,wealsoapplythepreviouslyusedmethodtotheparameters. Adictionaryiscreated
whereeachparametervaluehasauniqueID.Forexample,theparameter“CXP9029630_4”
wouldbestoredas‘1’,where‘1’istheIDofthetoken“CXP9029630_4”. Afterthis,the
encodedlogfilewouldonlycontainnumbersandspaces.
Asafinalstep,wealsoemployHuffmancodingonthelogfile. Itisamethodthatis
commonlyusedindatacompressionandwasproposedin[33]. Itcanbeusedtocreate
aprefix-freebinarycodethathasaminimumexpectedcodewordlength. Thealgorithm
analyses the frequencies of the characters that appear. Commonly appearing symbols
wouldbeencodedasshorterbitstrings,whileuncommoncharactersareencodedaslonger
strings. Forexample, acommonsymbolsuchas“a”wouldbeencodedasasingle“0”,
while rare characters such as “x” would be encoded as “11,000”. The use of Huffman
codingisveryprofitableinourcase;sinceourlogfileonlycontainsnumbersandspaces
atthispoint,weonlyhavetostoreanadditionallfixed-sizedHuffmancodec,sincethe
alphabetsizeisalways12(numbers0–9, space, andEOFcharacter). TheentireIDsare
thenencoded.
Therearethreecompressionmodelsthatarewidelystudied. Thefirsttypeiswhere
thesamemodelisusedforalltextsbythestaticmodel;thisperformsbadlyifthetextthat
wasusedtobuildthemodelandthetexttobecompressedaredifferent. Thesecondtypeis
thesemi-staticwhichworksintworuns;auniquemodelthatcanbebasedonoccurrence
probabilitiesisbuiltforthetext,whichisthenusedtocompressthedata. Thelastmodelis
theadaptivemodelthatisinitiallyemptyandupdateswhenanewsymbolisfound[34].
Generalcompressorsarenotsuitedforstream-likedata,sincethecompressionrelies
upon the preceding messages and the initial state. Once the template and parameter
dictionariesarecreated,ouralgorithmiscapableofupdatingthematanytimewhenanew
templateorparameterisfound,whichmeansitcancompressstream-likedataaswell.
Many compression algorithms handle data on a block level. In such cases, it can
happenthatmultipleblockshavetobedecompressedtoacquirethedesiredlogentry. This
canbetime-consuming,ifwewanttouseourcompresseddataforstatisticalpurposes,e.g.,
tocountthenumberofoccurrencesofaneventtype. Sinceeverytemplategetsassigned
anID,ouralgorithmiscapableofdecodinganyspecificlogentrywithoutdecompressing
Appl.Sci.2022,12,2044 9of32
others. For example, if we want to list all the software updates, we only have to look
uptheIDofthemessagetype“NPUSoftware”anddecodetheparametersthat
followtheID.Basedonthis,itcouldbesaidthatouralgorithmissuitedforstatisticaland
analyticaluse.
Wecreatedascriptthatcreateslogfileswiththedesireddistributionofthemessage
templates. Ahigh-levelflow-chartshowinghowthescriptworkscanbeseeninFigure6.
Figure6.Theflowchartthatrepresentslogdatageneration.
First,atemplatedictionary,aparameterdictionary,andadictionarythatcontainsall
the possible parameters for each template are created based on the user-provided data.
Afterthis,basedonthedistributionthattheuserdesires,anarrayiscreatedthatcontains
theprobabilitiesforallofthetemplates. Thesearethenusedtogenerateasamplelistthat
consistsoftemplateIDsthathavebeengeneratedwiththeuseoftheprobabilities. Inthe
nextstep,thedesirednumberoflogentriesiscreatedasfollows. Whilethesamplehas
items(weiteratethroughit)thetemplateischosenbasedontheactualIDinthesample.
Then,eachofthetokensinthetemplateisexamined. Thisisperformedbyiteratingover
the template and checking if it has a word or the end has been reached. If the token is
awildcard“*”, thenitischangedtoarandomlychosenparameterfromthetemplate’s
possibleparameters,otherwise,thetokeniskept. Intheend,thelogfileisgeneratedby
writingthecreatedmessagestoafile.
5. Results
5.1. Data
OurlogfileswereprovidedbynetworkingappliancesthatareusedattheEricsson-
ELTESoftwareTechnologyLab. Toevaluatetheeffectivenessoftheenhancedalgorithm
compared to the original, we used the same datasets that were used in our previous
paper[21],andsomenewdatasetsthatareseveralgigabytesinsize. Allofourdatasets
Appl.Sci.2022,12,2044 10of32
aredistinctandindependentfromeachother. Eachentryofthedatasetsbelongstoone
ofthe107possiblemessagetypesthatareusedtoindicatetheruntimeinformationofthe
networkingassets. Thedistributionoftheentriesfollowsthepower-lawdistributioninthe
caseofallinvestigateddatasets. Thedetailsofourdata,suchasthealphabetsizeorthe
ShannonEntropy,canbeseenTable1.
Table1.Sizeofthedatasets.
Name NumberofMessages SizeinKilobytes AlphabetSize ShannonEntropy
Small 39,139 1152KB 76 5.08167
Mid 124,433 4607KB 76 5.08872
Large 280,002 10,198KB 74 5.05287
Big 637,369 22,840KB 71 5.00206
A 50,000,000 2,039,483KB 76 4.97789
B 130,000,000 5,303,394KB 76 4.97798
C 254,000,000 10,361,437KB 76 4.97781
D 1,264,000,000 51,562,601KB 76 4.89284
The amount of information involved in the value of a random variable is known
as Shannon Entropy or Entropy [35]. In the case of a discrete random variable X, that
haspossibleoutcomesofx ,x ,·,x ,thatoccurwithprobabilityP(x ),P(x ),·,P(x ),the
1 2 n 1 2 n
entropyofXisdefinedas:
n
∑
H(X) = − P(x )logP(x ), (2)
i i
i=1
wherethesumofthevariable’spossiblevaluesisdenotedby∑. Thereareothertypesof
entropy,suchask-thorderentropy[36].
5.2. ExperimentalAnalysis
We conducted various experiments to demonstrate the compression efficiency of
our enhanced algorithm. We also examined the runtime of our algorithm. To obtain a
moredetailedpicture,wecomparedthecompressionrateandthespeedofourenhanced
algorithmwiththoseofgeneralcompressors. Weinvestigatedwhetherthecompression
ratecouldbefurtherimprovedwiththejointuseofourmethodandgeneralcompressors.
The time and storage space needed to retrieve all instances of a given set of templates
wasalsoinvestigated. Wealsocomparedthecompressionratesandthememoryusageof
theproposedalgorithmandLogzip,acompressorthatalsouseshiddenstructures. The
dahuffmanpythonlibrary[37]wasusedasourHuffmancoder. SinceBzip,LZMA,and
PPMdaresupportedby7-Zip[38],wechosethesealgorithmsasourgeneralcompressors.
Thedefaultsettingsof7-Zipwereused,whicharea16MBdictionarysize,awordsizeof
32,andasolidblocksizeof2GB.Theexperimentalanalysesaredividedintosixpartsand
areexplainedbelow.
5.2.1. Experiment1: ComparingtheCompressionValuesAchievedbythe
DifferentEnhancements
Inordertofurtherimprovethecompressionratiothatouralgorithm,proposedin[21],
achieved,weemployedmultipleenhancements. First,weusedHuffmancodingonthe
outputoftheoriginalalgorithm,whichconsistedofanIDandtheparametersinstring
format,forexample,“1CXP9029630_4R9D3925”,where1standsfor“NPUSoftware
”. Thisapproachislabeled“Huff”. ThesecondideawasthatthetemplateIDsshould
beassignedbasedonfrequency,andparametersshouldalsobeencoded,sincethesame
valuesappearmultipletimes,andtheaveragelengthofaparameterIDislessthanthe
averagelengthofaparameter’sstringrepresentation. If“CXP9029630_4”isrepresentedby
1and“R9D3925”isrepresentedby2,theoutputwouldbe“112”,whichis27characters
Appl.Sci.2022,12,2044 11of32
less than the original entry. This approach is labeled “Enh”. Finally, we combined the
firsttwoideas. Thisapproachislabeled“Huff”. Thecompressionratesachievedbythese
enhancementscanbeseeninFigures7–10.
Figure7.CompressionrateofthedifferentenhancementapproachesoftheSmalldataset.
Figure8.CompressionrateofthedifferentenhancementapproachesoftheMiddataset.
Basedonourexperiments,itcanbesaidthateachenhancementimprovesthecom-
pressionrate. ThesingleuseofHuffmancodingprovidesthesmallestimprovement;the
compressionrateisaround≈75%,whichis≈17%morethantheoriginalalgorithm(where
thecompressionrateis1minustheratioofthesizeofthecompressedfiletotheuncom-
pressedsize). Thereasonbehindthisisthatthelinestobeencodedcouldcontainvarious