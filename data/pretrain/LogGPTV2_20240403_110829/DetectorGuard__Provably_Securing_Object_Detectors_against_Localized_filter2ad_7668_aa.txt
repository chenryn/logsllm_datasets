title:DetectorGuard: Provably Securing Object Detectors against Localized
Patch Hiding Attacks
author:Chong Xiang and
Prateek Mittal
DetectorGuard: Provably Securing Object Detectors
against Localized Patch Hiding Attacks
Chong Xiang
Prateek Mittal
Princeton University
Princeton, NJ, USA
Princeton University
Princeton, NJ, USA
PI:EMAIL
PI:EMAIL
ABSTRACT
State-of-the-art object detectors are vulnerable to localized patch
hiding attacks, where an adversary introduces a small adversarial
patch to make detectors miss the detection of salient objects. The
patch attacker can carry out a physical-world attack by printing
and attaching an adversarial patch to the victim object; thus, it im-
poses a challenge for the safe deployment of object detectors. In this
paper, we propose DetectorGuard as the first general framework for
building provably robust object detectors against localized patch
hiding attacks. DetectorGuard is inspired by recent advancements
in robust image classification research; we ask: can we adapt ro-
bust image classifiers for robust object detection? Unfortunately, due
to their task difference, an object detector naively adapted from a
robust image classifier 1) may not necessarily be robust in the adver-
sarial setting or 2) even maintain decent performance in the clean
setting. To address these two issues and build a high-performance
robust object detector, we propose an objectness explaining strategy:
we adapt a robust image classifier to predict objectness (i.e., the
probability of an object being present) for every image location and
then explain each objectness using the bounding boxes predicted
by a conventional object detector. If all objectness is well explained,
we output the predictions made by the conventional object detec-
tor; otherwise, we issue an attack alert. Notably, our objectness
explaining strategy enables provable robustness for “free": 1) in the
adversarial setting, we formally prove the end-to-end robustness of
DetectorGuard on certified objects, i.e., it either detects the object
or triggers an alert, against any patch hiding attacker within our
threat model; 2) in the clean setting, we have almost the same per-
formance as state-of-the-art object detectors. Our evaluation on the
PASCAL VOC, MS COCO, and KITTI datasets further demonstrates
that DetectorGuard achieves the first provable robustness against
localized patch hiding attacks at a negligible cost (20%
clean accuracy drop on ImageNet [11]). The imperfection of robust
classifiers can be severely amplified during the adaptation towards
the much more demanding object detection task. Therefore, we
need to prevent our object detectors from being broken in the clean
setting (even in the absence of an adversary). In DetectorGuard, we
overcome these two challenges as discussed below.
Defense Design: an objectness explaining strategy. We pro-
vide our defense overview in Figure 1. DetectorGuard has three
modules: Base Detector, Objectness Predictor, and Objectness Ex-
plainer. Base Detector can be any state-of-the-art object detector
that can make accurate predictions on clean images but is vulnerable
to patch hiding attacks. Objectness Predictor aims to predict a robust
objectness map, which indicates the probability of an object being
present at different locations. We build Objectness Predictor using
adapted provably robust image classifiers together with carefully
designed feature-space operation and error filtering mechanisms
(Section 3.3). Finally, Objectness Explainer uses each predicted
bounding box from Base Detector to explain/match high objectness
predicted by Objectness Predictor (Section 3.4). If all objectness is
well explained/matched, we output the prediction of Base Detector;
otherwise, we issue an attack alert. In the clean setting, we optimize
the configuration of Objectness Predictor towards the case where
all objectness can be explained and then use Base Detector for ac-
curate final predictions (Figure 1 left). When a hiding attack occurs,
Base Detector could miss the object while Objectness Predictor can
still robustly output high objectness. This will lead to unexplained
objectness and trigger an attack alert (Figure 1 right). Notably, we
can show that our defense design successfully addresses Challenge
1 and 2, as discussed next.
End-to-end provable robustness for “free". First, our object-
ness explaining strategy enables us to rigorously prove the end-
to-end robustness of DetectorGuard (Theorem 1 in Section 4). We
will show that DetectorGuard will always perform robust detec-
tion or issue an alert on objects certified by our provable analysis
(Algorithm 2 in Section 4). We note that this robustness property
is agnostic to attack strategies and holds for any patch hiding at-
tacker within our threat model, including adaptive attackers who have
full access to our defense setup. This strong theoretical guarantee
addresses Challenge 1. Next, in contrast to most security-critical
systems whose robustness comes at the cost of clean performance,
DetectorGuard achieves provable robustness for “free" (at a negligi-
ble cost of clean performance). In Objectness Predictor, we design
error mitigation mechanisms to handle the imperfection of the
adapted robust classifier. In Objectness Explainer, our explaining
strategy ensures that even when our Objectness Predictor fails to
Detection OutputObjectnessPredictorInput Image (clean)Objectness ExplainerBase DetectorALERT!Clean SettingAdversarial SettingDetection OutputObjectnessPredictorInput Image (adversarial)Base DetectordogdogdogdogdogdogObjectness ExplainerSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3178predict high objectness (missing objects; false negatives), Detec-
torGuard still performs as well as state-of-the-art Base Detector.2
These designs solve Challenge 2.
Evaluating the first provable robustness against patch hid-
ing attacks. We extensively evaluate DetectorGuard performance
on the PASCAL VOC [13], MS COCO [26], and KITTI [15] datasets.
In our evaluation, we instantiate the Base Detector with YOLOv4 [2,
53], Faster R-CNN [45], and a hypothetical object detector that
is perfect in the clean setting. We build Objectness Predictor by
adapting multiple variants of robust image classifiers [58, 65]. Our
evaluation shows that our defense has a minimal impact (<1%)
on the clean performance and achieves the first provable robust-
ness against patch hiding attacks. Our code is available at https:
//github.com/inspire-group/DetectorGuard.
Our contributions can be summarized as follows.
• We solve two major challenges in adapting robust image
classifiers for robust object detection via a careful design of
Objectness Predictor and Objectness Explainer.
• We formally prove the robustness guarantee of DetectorGuard
on certified objects against any adaptive attacker within our
threat model.
• We extensively evaluate our defense on the PASCAL VOC [13],
MS COCO [26], and KITTI [15] datasets and demonstrate the
first provable robustness against patch hiding attacks and a
similar clean performance as conventional object detectors.
2 BACKGROUND AND PROBLEM
FORMULATION
In this section, we introduce the object detection task, the patch
hiding attack, the defense formulation, and the key principles for
building provably robust image classifiers that we will adapt for
robust object detection in DetectorGuard.
2.1 Object Detection
Detection objective. An object detector aims to predict a list
of bounding boxes (and class labels) for all objects in the image
x ∈ [0, 1]𝑊 ×𝐻×𝐶, where pixel values are rescaled into [0, 1], and
𝑊 , 𝐻, 𝐶 is the image width, height, and channel, respectively. Each
bounding box b is represented as a tuple (𝑥min, 𝑦min, 𝑥max, 𝑦max, 𝑙),
where 𝑥min, 𝑦min, 𝑥max, 𝑦max together illustrate the coordinates of
the bounding box, and 𝑙 ∈ L = {0, 1, · · · , 𝑁 − 1} denotes the pre-
dicted object label (𝑁 is the number of object classes).3
Conventional object detector. Object detection models can
be categorized into two-stage and one-stage detectors depending
on their pipelines. A two-stage object detector first generates pro-
posals for regions that might contain objects and then uses the
proposed regions for object classification and bounding-box regres-
sion. Representative examples include Faster R-CNN [45] and Mask
R-CNN [19]. On the other hand, a one-stage object detector does
2Objectness Predictor can also have other types of errors. However, we can optimize
its configuration to ensure most errors are false-negatives, which our objectness
explaining strategy can tolerate. More discussions are in Section 3. We also note that
we manage to build a system with high clean performance (i.e., DetectorGuard) despite
the use of a module with poor clean performance (i.e., provably robust image classifier).
We provide additional discussion on this intriguing property in Appendix F.
3Conventional object detectors usually output objectness score and prediction confi-
dence as well—we discard them in notation for simplicity.
detection directly on the input image without any explicit region
proposal step. SSD [29], YOLO [2, 42–44, 53], RetinaNet [25], and
EfficientDet [49] are representative one-stage detectors.
Conventionally, a detection is considered correct when 1) the
predicted label matches the ground truth and 2) the overlap between
the predicted bounding box and the ground-truth box, measured
by Intersection over Union (IoU), exceeds a certain threshold 𝜏. We
term a correct detection a true positive (TP). On the other hand, any
predicted bounding box that fails to satisfy both two TP criteria is
considered as a false positive (FP). Finally, if a ground-truth object
is not detected by any TP bounding box, it is a false negative (FN).
2.2 Attack Formulation