38 621
18 110
35 016
127 206
211 742
105 453
205 824
NN
267
325
425
439
269
333
439
4 844
log10
396
458
633
698
396
458
633
698
103 357
205 266
99 852
205 263
kn-NN
log
679
781
899
904
673
768
899
904
34 243
199 604
CONVERGENCE OF THE ESTIMATES WHEN VARYING |O|/|S|.
TABLE VI
δ
0.1
0.05
0.01
0.005
0.1
0.05
Freq.
8 679
14 823
51 694
71 469
NN
8 707
14 853
60 796
71 469
log10
7 108
14 853
60 796
71 469
85 912
152 904
85 644
152 698
71 003
151 153
kn-NN
log
2 505
7 673
60 796
71 469
11 197
68 058
0.1
X
X
413 974
2 967
System
Geometric
100x1K
ν = 0.2
Geometric
100x10K
ν = 0.02
Geometric
100x100K
ν = 0.002
2) Variation of the ratio |O|/|S|: Now we ﬁx |S| = 100,
and we consider three cases |O|/|S| = 10, |O|/|S| = 100, and
|O|/|S| = 1K. (Note that we want to keep the ratio ν/Δg ﬁxed,
see Appendix B; as a consequence ν has to vary: we set ν to
0.2, 0.02, and 0.002, respectively.) Results in Figure 2 and
Table VI show how the nearest neighbor methods become
much better than the frequentist approach as |O|/|S| increases.
This is because the larger the object space, the larger the
number of unseen objects at the moment of classiﬁcation,
and the more the frequentist approach has to rely on random
guessing. The nearest neighbor methods are not that much
affected because they can rely on the proximity to outputs
already classiﬁed.
3) Case |S| ≥ |O|: We ﬁx ν = 2, and we consider two
cases: |S| = |O| and |S| > |O|. It should be noted that
the formulation of geometric systems prohibits the number
of secrets to exceed the number of outputs; for this reason,
in the system |S| > |O| some secrets are associated with the
same distribution over the output space (Appendix B).
The results in Figure 3 and Table VII indicate that NN and
frequentist are mostly equivalent: this is because they both
need to observe at least one example for each secret. kn-NN
rules, on the other hand, show poor performances, due to the
fact that they would need at least kn examples for each secret.
A natural extension of our work is to look at notions of metric
also in the secret space for improving convergence.
(cid:25)(cid:21)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 1. Estimates’ convergence for geometric systems when varying their privacy parameter ν. The respective distributions are shown in the top ﬁgure for
two adjacent secrets s1 ∼ s2.
Fig. 2. Estimates’ convergence for geometric systems when varying the ratio |O|/|S|. The respective distributions are shown in the top ﬁgure for two adjacent
secrets s1 ∼ s2.
B. Multimodal geometric system
can exploit. Detailed δ-convergence results are in Appendix C.
We now evaluate the estimators on systems with a multi-
modal distribution. In particular, we create multimodal geo-
metric systems by summing two geometric probability distri-
butions, appropriately normalized and shifted by some param-
eter. We provide the details of this distribution in Appendix B.
1) Evaluation: Results are reported in Figure 4. As ex-
pected, we observe that nearest neighbor rules improve on the
frequentist approach; the reason is that, even for multimodal
distributions, there exists a metric on the outputs which they
C. Spiky system: When kn-NN rules fail
Nearest neighbor rules take advantage of the metric on the
object space to improve their convergence considerably. How-
ever, as a consequence of the NFL theorem, there exist systems
for which the frequentist approach outperforms NN and kn-
NN. Investigating the form of such systems is important to
understand when these methods fail.
We craft one such system, the Spiky system, where the
metric misleads predictions. The Spiky system is such that
(cid:25)(cid:21)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
the majority vote in the k neighbors (i’s are considered before
than i− 1’s and i + 1’s, by the nearest neighbor deﬁnition) so
i will not be misclassiﬁed anymore.
∗
Concerning the comparison between the NN and frequentist
estimates, we can do it analytically. We start by computing the
expected error of the NN method on the spiky system in terms
of the number of training examples n. Let T n be a training set
of examples of size n. Given a new object i, let us consider
the NN estimate rn(i) of r
for i, i.e., the expected probability
of error in the classiﬁcation of i. This is the probability that
the element o closest to i that appears in the training set has
odd distance from i (i.e., d(i, o) = 2(cid:3) + 1, for some natural
number (cid:3)). Namely it is the probability that:
• i is not in the training data but either i + 1 or i − 1 are,
• i, i ± 1, i ± 2 are not in the training data but either i + 3
• . . . etc.
or
or i − 3 are, or
Hence we have:
= 2 · q/4−1(cid:2)
r(i) = P (d(i, o) = 2l + 1) =
(17)
= P (i /∈ T n, i + 1 ∈ T n) + P (i /∈ T n, i − 1 ∈ T n) + . . .
(18)
a4(cid:4)+1(1 − a),
(cid:4)=0
(19)
where a = (1 − 1/q)n is the probability that an element e ∈
O does not occur in any of the n examples of the training
set. (Thus a4(cid:4)+1 represents the probability that none of the
elements i, i±1, i±2, i±2s, with (cid:3) = 2s, appear in the training
set, and 1−a represents the probability that the element 2s+1
(resp. 2s− 1 ) appears in the training set.) By using the result
of the geometric series
m(cid:2)
at =
,
1 − am+1
1 − a
1 − aq
(1 + a2)(1 + a)
(20)
(21)
.
we obtain:
t=0
rn(i) = 2a
Since we assume that the distribution on O is uniform, we
have RNN
n = rn(i).
We want to study how the error estimate depends on the
relative size of the training set with respect to the size of O.
Hence, let x = n/q. Then we have a = (1− 1/q)qx, which, for
large q, becomes a ≈ e
≈ 2e
−x. Therefore:
1 − e
−x
RNN
−qx
x
(1 + e−2x)(1 + e−x)
→ 1/2 for x → 0, and RNN
.
(22)
→ 0
x
Consider now the frequentist estimate RFreq
It is easy to see that RNN
for x → ∞, as expected.
. In this case,
given an element i ∈ O, the classiﬁcation is done correctly
if i appears in the training set. Otherwise, we do random
guessing, which gives a correct or wrong classiﬁcation with
x
x
Fig. 3. Estimates’ convergence for geometric systems when |S| ≥ |O|. The
distributions are shown in the top ﬁgure for two adjacent secrets s1 ∼ s2. In
the case |S| > |O| (right) there are 10 = 10K/1K identical distributions that
coincide on s1, and 10 identical distributions on s2.
CONVERGENCE OF THE ESTIMATES WHEN |S| ≥ |O|, ν = 2.
TABLE VII
System
Geometric
10Kx10K
Geometric
10Kx1K
δ
0.1
0.05
0.01
0.005
0.1
0.05
0.01
0.005
Freq.
74 501
95 500
137 099
153 370
5
721
5 595
10 770
NN
73 085
94 204
137 348
153 370
5
514
6 171
10 797
kn-NN
log10
88 296
107 707
144 846
159 075
5
2 309
7 330
11 037
log
140 618
155 403
192 014
203 363
5
5 977
12 354
14 575
neighboring points are associated with different secret. This
means that NN and kn-NN rules will tend to predict the
wrong secret, until enough examples are available. We detail
its construction in Appendix B.
a) Discussion: We conducted experiments for a Spiky
system of size |O| = 10K. Results in Figure 5 conﬁrm the
hypothesis: nearest neighbor rules are misled for this system.
Interestingly, while the NN estimate keeps decreasing as the
number of examples n increases, there is a certain range of
n’s where the kn-NN estimates become worse than random
guessing. Intuitively, this is because when n becomes larger
than |O|, all elements in O tend to be covered by the examples.
For every i ∈ O there are two neighbors, i − 1 and i + 1, that
belong to the class opposite to the one of i, so if k is not too
small with respect to n, it is likely that in the multiset of the k
closest neighbors of i, the number of i−1’s and i+1’s exceeds
the number of i’s, which means that i will be misclassiﬁed.
As n increases, however, the ratio between k and the number
of i’s in the examples tends to decrease (because k/n → 0 as
n → ∞), hence at some point we will have enough i’s to win
(cid:25)(cid:21)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 4. Estimates’ convergence for multimodal geometric systems when varying the privacy parameter ν. The distributions are shown in the top ﬁgure for
two adjacent secrets s1 ∼ s2.
RANDOM: EXAMPLES REQUIRED FOR δ-CONVERGENCE.
TABLE VIII
δ
Freq.
0.05
0.01
0.005
5
82
10 070
NN
5
139
10 070
kn-NN
log10
5
202
10 070
log
5
500
10 070
Fig. 5. Estimates’ convergence for a Spiky system (2x10K).
equal probability. Only the latter case contributes to the prob-
ability of error, hence the error estimate is half the probability
expectation that i does not belong to the training set:
RFreq
x =
(1 − 1
q
)n ≈ 1
2
1
2
−x
e
(23)
Therefore, RNN
x
is always above RFreq
x
.
D. Random System
In the previous sections, we have seen cases when our
methods greatly outperform the frequentist approach, and
a contrived system example for which they fail. We now
consider a system generated randomly to evaluate their per-
formances for an “average” system.
a) System description: We construct the channel matrix
of a Random system by drawing its elements from the uniform
distribution, Cs,o ←$ U ni(0, 1), and normalizing its rows.
Fig. 6. Estimates’ convergence for a Random system (100 × 100).
b) Evaluation: We consider a Random system with |S| =
|O| = 100 and count the number of examples required for δ-
convergence, for many δ’s. Table VIII reports the results.
The frequentist estimate is slightly better than NN and kn-
NN for δ = 0.01. However, for stricter convergence require-
ments (δ = 0.001), all the methods require the same (large)
number of examples. Figure 6 shows that indeed the methods
begin to converge similarly already after 1K examples.
c) Discussion: Results showed that nearest neighbor esti-
mates require signiﬁcantly fewer examples than the frequentist
(cid:25)(cid:21)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
approach when dealing with medium or large systems; how-
ever, they are generally equivalent to the frequentist approach
in the case of small systems.
To better understand why this is the case, we provide a
crude approximation of the frequentist Bayes risk estimate.
(cid:7)
(cid:7)
(cid:8)n(cid:8)
(cid:7)
(cid:8)n
RFreq
n
≈ R
∗
1 −
1 − 1|O|
+ Rπ
1 − 1|O|
. (24)
(cid:9)
(cid:10)n
This approximation, derived and studied in Appendix E,
makes the very strong assumption that all objects are equally
likely, i.e.: P (o) = 1|O|. However, this is enough to give us
an insight on the performance of the frequentist approach:
1 − 1|O|