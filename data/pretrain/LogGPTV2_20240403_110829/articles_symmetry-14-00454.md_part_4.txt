### 4.3. Evaluation on HDFS Log Data

Table 6 shows the number of false positives (FPs) and false negatives (FNs) for each method on the HDFS data (all data except the training set), along with the accuracy rate. The LogLS method has a low level of FPs and FNs, and achieves the highest accuracy rate of 99.84% among the five methods. Figure 5 illustrates the experimental results of different methods on the HDFS dataset.

- **PCA**: Precision = 0.98, Recall = 0.67, F1-Measure = 0.79
- **IM**: Precision = 0.84, Recall = 0.92, F1-Measure = 0.88
- **N-Gram**: Precision = 0.92, Recall = 0.94, F1-Measure = 0.93
- **DeepLog**: Precision = 0.95, Recall = 0.96, F1-Measure = 0.96
- **LogLS**: Precision = 0.96, Recall = 0.98, F1-Measure = 0.97

The LogLS method outperforms the other methods in all three evaluation metrics, indicating its superiority.

**Table 6. Number of FPs, FNs, and Accuracy on HDFS log.**

| Method | False Positive (FP) | False Negative (FN) | Accuracy |
|--------|---------------------|---------------------|----------|
| PCA    | 277                 | 5429                | 99.00%   |
| IM     | 2122                | 1226                | 99.41%   |
| N-Gram | 1360                | 739                 | 99.63%   |
| DeepLog| 833                 | 615                 | 99.75%   |
| LogLS  | 657                 | 280                 | 99.84%   |

**Figure 5. Evaluation on HDFS log.**

### 4.4. Parameter Analysis

The performance of the LogLS model is tested by adjusting each parameter. This study uses the controlled variable method to carry out the parameter adjustment experiment. When one parameter is studied, the other parameters are controlled and remain unchanged. The adjusted parameters include \( g_1 \), \( g_2 \), \( g_3 \), \( h \), \( L \), \( S \), and \( E \). In each experiment, only the value of one parameter is changed; the remaining parameters remain at their default values.

- **\( g_1 \)**: Value range {5, 7, 9, 11, 13, 15}
- **\( g_2 \) and \( g_3 \)**: Value range {1, 3, 5, 7, 9, 11, 13}

After adjusting these three parameters, a total of 294 groups of data are generated. Due to space constraints, Figure 6 shows 49 groups. The graph is obtained by assuming \( g_1 = 13 \) and changing the values of \( g_2 \) and \( g_3 \). Figure 6 illustrates the changes in precision, recall, and F1-measure as \( g_2 \) and \( g_3 \) vary. The x-axis represents \( g_2 \), and the y-axis represents \( g_3 \).

**Figure 6. Performance comparison chart of changing parameters \( g_2 \) and \( g_3 \).**
- **(a)** Precision
- **(b)** Recall
- **(c)** F1-Measure

From Figure 6, it is observed that the best F1-measure is achieved when \( g_2 = 5 \) and \( g_3 = 3 \). Detailed evaluation indicators corresponding to these two parameters are compared in Table 7 and Figure 7.

**Table 7. \( g_2 \) size in LogLS.**

| \( g_2 \) Size | Precision | Recall | F1-Measure |
|---------------|-----------|--------|------------|
| 1             | 0.6476    | 0.9923 | 0.7837     |
| 3             | 0.8443    | 0.9673 | 0.9016     |
| 5             | 0.9677    | 0.9612 | 0.9644     |
| 7             | 0.9683    | 0.9260 | 0.9467     |
| 9             | 0.9689    | 0.9198 | 0.9437     |
| 11            | 0.9716    | 0.9060 | 0.9376     |
| 13            | 0.9737    | 0.8727 | 0.9204     |

**Figure 7. Performance comparison chart of changing parameter \( g_2 \).**

From Table 7 and Figure 7, it is evident that precision increases with increasing \( g_2 \), reaching a maximum value of 0.9737. Recall decreases with increasing \( g_2 \), with a maximum value of 0.9923. The F1-measure first increases and then decreases as \( g_2 \) increases. When \( g_2 = 5 \), the F1-measure reaches a maximum value of 0.9644, and the values of precision, recall, and F1-measure are relatively balanced and high. Therefore, the optimal value of \( g_2 \) is preliminarily confirmed to be 5.

Next, we set \( g_2 \) to 5 and use default values for other parameters. By changing the value of \( g_3 \), the changes in precision, recall, and F1-measure are obtained. The results are shown in Table 8 and Figure 8.

**Table 8. \( g_3 \) size in LogLS.**

| \( g_3 \) Size | Precision | Recall | F1-Measure |
|---------------|-----------|--------|------------|
| 1             | 0.9619    | 0.9636 | 0.9627     |
| 3             | 0.9677    | 0.9612 | 0.9644     |
| 5             | 0.9693    | 0.9386 | 0.9537     |
| 7             | 0.9690    | 0.9147 | 0.9410     |
| 9             | 0.9699    | 0.8996 | 0.9334     |
| 11            | 0.9716    | 0.8878 | 0.9278     |
| 13            | 0.9721    | 0.8822 | 0.9250     |

**Figure 8. Performance comparison chart of changing parameter \( g_3 \).**

From Table 8 and Figure 8, it is observed that precision increases with the increase of \( g_3 \), recall decreases with the increase of \( g_3 \), and the F1-measure first increases and then decreases with the increase of \( g_3 \). The F1-measure obtains the maximum value when \( g_3 = 3 \), and the three values of precision, recall, and F1-measure are relatively balanced and high. Therefore, the tuning parameter temporarily confirms that the optimal value of \( g_3 \) is 3.

Concluding from the above, the parameter values of \( g_2 \) and \( g_3 \) are temporarily 5 and 3, respectively. Next, the parameters of \( g_1 \) are adjusted according to \( g_2 \) and \( g_3 \). As the value of \( g_1 \) changes, the three values of precision, recall, and F1-measure are obtained. The results are shown in Table 9 and Figure 9.

**Table 9. \( g_1 \) size in LogLS.**

| \( g_1 \) Size | Precision | Recall | F1-Measure |
|---------------|-----------|--------|------------|
| 5             | 0.8919    | 0.9966 | 0.9414     |
| 7             | 0.9287    | 0.9830 | 0.9551     |
| 9             | 0.9464    | 0.9673 | 0.9567     |
| 11            | 0.9623    | 0.9654 | 0.9638     |
| 13            | 0.9677    | 0.9612 | 0.9644     |
| 15            | 0.9690    | 0.9339 | 0.9511     |

**Figure 9. Performance comparison chart of changing parameter \( g_1 \).**

From Table 9 and Figure 9, it is determined that the parameter \( g_1 \) can be temporarily set to 13. Combining the parameter results obtained above, it can be concluded that when \( g_1 = 13 \), \( g_2 = 5 \), and \( g_3 = 3 \), the overall model prediction results are ideal. However, this parameter is adjusted by an arithmetic sequence, which is not complete. Moreover, it is found that the changes in precision, recall, and F1-measure are correlated with \( g_1 \), \( g_2 \), and \( g_3 \). Therefore, adjacent parameter values are compared. The value range of \( g_1 \) is {12, 13, 14}, while \( g_2 \) is {4, 5, 6} and \( g_3 \) is {2, 3, 4}. As shown in Table 10, the final values of parameters \( g_1 \), \( g_2 \), and \( g_3 \) are 13, 4, and 2, respectively. At this time, precision, recall, and F1-measure all perform well, being 0.9586, 0.9856, and 0.9719, respectively.

**Table 10. Adjacent parameters.**

| \( g_1 \) | \( g_2 \) | \( g_3 \) | Precision | Recall | F1-Measure |
|-----------|-----------|-----------|-----------|--------|------------|
| 12        | 4         | 2         | 0.9558    | 0.9856 | 0.9705     |
| 12        | 4         | 3         | 0.9596    | 0.9634 | 0.9615     |
| 12        | 4         | 4         | 0.9649    | 0.9586 | 0.9618     |
| 12        | 5         | 2         | 0.9609    | 0.9634 | 0.9621     |
| 12        | 5         | 3         | 0.9648    | 0.9616 | 0.9632     |
| 12        | 5         | 4         | 0.9664    | 0.9561 | 0.9612     |
| 12        | 6         | 2         | 0.9646    | 0.9442 | 0.9543     |
| 12        | 6         | 3         | 0.9653    | 0.9372 | 0.9511     |
| 12        | 6         | 4         | 0.9664    | 0.9333 | 0.9495     |
| 13        | 4         | 2         | 0.9586    | 0.9856 | 0.9719     |
| 13        | 4         | 3         | 0.9624    | 0.9630 | 0.9627     |
| 13        | 4         | 4         | 0.9678    | 0.9582 | 0.9630     |
| 13        | 5         | 2         | 0.9637    | 0.9630 | 0.9634     |
| 13        | 5         | 3         | 0.9677    | 0.9612 | 0.9644     |
| 13        | 5         | 4         | 0.9693    | 0.9555 | 0.9623     |
| 13        | 6         | 2         | 0.9675    | 0.9438 | 0.9555     |
| 13        | 6         | 3         | 0.9683    | 0.9368 | 0.9523     |
| 13        | 6         | 4         | 0.9693    | 0.9325 | 0.9506     |
| 14        | 4         | 2         | 0.9602    | 0.9778 | 0.9689     |
| 14        | 4         | 3         | 0.9640    | 0.9553 | 0.9596     |
| 14        | 4         | 4         | 0.9694    | 0.9473 | 0.9583     |
| 14        | 5         | 2         | 0.9654    | 0.9553 | 0.9603     |
| 14        | 5         | 3         | 0.9694    | 0.9531 | 0.9612     |
| 14        | 5         | 4         | 0.9709    | 0.9446 | 0.9576     |
| 14        | 6         | 2         | 0.9693    | 0.9361 | 0.9524     |
| 14        | 6         | 3         | 0.9699    | 0.9258 | 0.9473     |
| 14        | 6         | 4         | 0.9710    | 0.9212 | 0.9454     |

**Figure 10. LogLS performance with different parameters.**
- **(a)** Window size: \( h \)
- **(b)** Number of epochs: \( E \)
- **(c)** Number of layers: \( L \)
- **(d)** Number of memory units: \( S \)

### 5. Online Update and Training of LogLS

Although the method in this paper has achieved good performance in the HDFS log anomaly detection experiment, problems may occur when dealing with more irregular logs (such as system logs). Many log keys only appear in a certain period, so the training set may not contain all the normal log keys, leading to false predictions. The model update module can effectively solve this problem by adjusting the weight parameters of the model in real-time based on online false-positive results. This section sets up a comparative experiment of model updates to verify its effectiveness.

The model update method adopts incremental updates and only uses false positives to update the model. Suppose \( h = 3 \), the input historical sequence is \(\{k_1, k_2, k_3\}\), and LogLS predicts that the next log key is \( k_2 \) with probability 1, but the actual log key in the sequence is \( k_3 \). The model marks this as an anomaly. However, after manual detection, it is known that this is a false positive. LogLS can use \(\{k_1, k_2, k_3 \rightarrow k_3\}\) to update the weights of its model, thus learning this new log pattern. The next time the sequence \(\{k_1, k_2, k_3\}\) is entered, LogLS can output the correct prediction.