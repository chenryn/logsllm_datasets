eventswiththehighestpredictedprobability). histhewindowsizeusedfortrainingand
detection,andLandSrepresentthenumberoflayersinLogLSandthenumberofmemory
unitsinanLSTMblock,respectively. Eisthenumberofepochstobetrained,whereeach
epochisasingletrainingiterationofallbatchesintheforwardandbackwardpropagation.
Ifthenumberofepochsistoosmall,underfittingmayoccur,andifthenumberistoolarge,
overfittingmayoccur.
Table6showsthenumberoffalsepositivesandfalsenegativesforeachmethodonthe
HDFSdata(alldataexceptthetrainingset),andtheaccuracyrate. Thefalsepositivesand
falsenegativesofLogLShavealowlevel,andtheaccuracyrateof99.84%isalsothehighest
amongthefivemethods. Figure5showstheexperimentalresultsofdifferentmethodson
theHDFSdataset. AlthoughtheprecisionofthetraditionalPCAontheHDFSdatasetis
0.98,therecallandF1-measurearerelativelylow,only0.67and0.79,respectively. Among
thethreemethodsofIM,N-gramandDeepLog,DeepLogperformsbest. Theprecision,
recallandF1-measureare0.95,0.96and0.96,respectively. However,wecanalsoseethat
Symmetry2022,14,454 12of21
theLogLSmethodisbetterthanseveralothermethodsasawhole. Thethreeevaluation
metricsreached0.96,0.98and0.97,indicatingthatthismethodhasadvantages.
Table6.NumberofFPs,FNsandAccuracyonHDFSlog.
PCA IM N-Gram DeepLog LogLS
Falsepositive(FP) 277 2122 1360 833 657
Falsenegative(FN) 5429 1226 739 615 280
Accuracy 99.00% 99.41% 99.63% 99.75% 99.84%
Figure5.EvaluationonHDFSlog.
4.4. ParameterAnalysis
The performance of the model is tested by adjusting each parameter in the LogLS
model. This article uses the controlled variable method to carry out the parameter ad-
justmentexperimentofLogLS.Whenoneparameterisstudied,theotherparametersare
controlled and remain unchanged. The adjusted parameters include g1, g2, g3, h, L, S,
and E. In each experiment, only the value of one parameter is changed; the remaining
parameters remain at the default values. Of course, g1, g2, and g3 belong to a class of
parameters,sothesethreevaluesarechangedasvariables,andtherestremainunchanged.
Afterobservation,thearithmeticsequencemethodisusedtodeterminetheoptimalvalue
ofclassGparameters.Thevaluerangeofg1is{5,7,9,11,13,15}.Thevaluerangeofg2andg3
is{1,3,5,7,9,11,13}. Afteradjustingthesethreeparameters,atotalof294groupsofdataare
generated. Theexperimentaldatacannotbefullydisplayedinthechart,soFigure6shows
49groups. Thegraphisobtainedbyassumingg1=13andchangingthevaluesofg2and
g3. Figure6showsthechangesinthethreevaluesofprecision,recallandF1-measurewhen
g2andg3change. Theabscissarepresentsg2, andtheordinaterepresentsg3. Figure6
includes three sets of small graphs. Figure 6a shows the change in the precision value
afterchangingthesetwoparameters,Figure6bshowsthechangeintherecallvalue,and
Figure6cshowsthechangeintheF1-measure.
Symmetry2022,14,454 13of21
(a) (b) (c)
Figure6.Performancecomparisonchartofchangingparametersg2andg3.(a)Precision.(b)Recall.
(c)F1-measure.
Figure6showsthatwhentheF1-measureofthemodelisthebest,thevaluesofg2and
g3are5and3,respectively. Thedetailedevaluationindicatorscorrespondingtothesetwo
parametersareseparatelycompared. Theresultsforwheng3=3andonlyg2ischanged
areshowninTable7andFigure7.
Table7.g2sizeinLogLS.
g2Size Precision Recall F1-Measure
1 0.6476 0.9923 0.7837
3 0.8443 0.9673 0.9016
5 0.9677 0.9612 0.9644
7 0.9683 0.9260 0.9467
9 0.9689 0.9198 0.9437
11 0.9716 0.9060 0.9376
13 0.9737 0.8727 0.9204
Figure7.Performancecomparisonchartofchangingparameterg2.
Symmetry2022,14,454 14of21
AsseenfromTable7andFigure7,precisionincreaseswithincreasingg2;themaximum
precisionvalueinthetableis0.9737.Recalldecreaseswithincreasingg2,andthemaximum
valueofrecallinthetableis0.9923. ThevalueoftheF1-measurefirstincreasesandthen
decreasesasthevalueofg2increases. Wheng2=5,thevalueoftheF1-measureinthetable
reachesamaximumvalueof0.9644,andatthistime,thevaluesofprecision,recalland
F1-measurearerelativelybalancedandhigh. Therefore,itispreliminarilyconfirmedthat
theoptimalparametervalueofg2is5.
Nowweparameterg2to5andusedefaultvaluesforotherparameters. Bychanging
theparametervalueofg3,thechangesofthethreevaluesofprecision,recallandF1-measure
areobtained. TheresultsareshowninTable8andFigure8.
Table8.g3sizeinLogLS.
g3Size Precision Recall F1-Measure
1 0.9619 0.9636 0.9627
3 0.9677 0.9612 0.9644
5 0.9693 0.9386 0.9537
7 0.9690 0.9147 0.9410
9 0.9699 0.8996 0.9334
11 0.9716 0.8878 0.9278
13 0.9721 0.8822 0.9250
Figure8.Performancecomparisonchartofchangingparameterg3.
ItcanbeseenfromTable8andFigure8thatprecisionincreaseswiththeincreaseof
g3, recall decreases with the increase of g3, and that the F1-measure first increases and
thendecreaseswiththeincreaseofg3. TheF1-measureobtainsthemaximumvaluewhen
g3=3,andthethreevaluesofprecision,recallandF1-measurearerelativelybalancedand
high. Therefore,thetuningparametertemporarilyconfirmsthattheoptimalparameter
valueofg3is3. Itcanbeconcludedthattheparametervaluesofg2andg3aretemporarily
5and3,respectively. Then,theparametersofg1areadjustedaccordingtog2andg3. As
thevalueofg1changes,thethreevaluesofprecision,recall,andF1-measureareobtained.
TheresultsareshowninTable9andFigure9.
Symmetry2022,14,454 15of21
Table9.g1sizeinLogLS.
g1Size Precision Recall F1-Measure
5 0.8919 0.9966 0.9414
7 0.9287 0.9830 0.9551
9 0.9464 0.9673 0.9567
11 0.9623 0.9654 0.9638
13 0.9677 0.9612 0.9644
15 0.9690 0.9339 0.9511
Figure9.Performancecomparisonchartofchangingparameterg1.
ItisdeterminedfromTable9andFigure9thattheparameterg1canbetemporarily
setto13. Combiningtheparameterresultsobtainedabove,itcanbeconcludedthatwhen
g1=13,g2=5andg3=3,theoverallmodelpredictionresultsareideal. However,this
parameterisadjustedbyanarithmeticsequence,whichisnotcomplete. Moreover,itis
foundthatthechangesinprecision,recallandF1-measurearecorrelatedwithg1,g2andg3.
Therefore,theadjacentparametervaluesarecompared. Thevaluerangeofg1is{12,13,14},
whileg2is{4,5,6}andg3is{2,3,4}. AsshowninTable10,thefinalvaluesofparametersg1,
g2andg3are13,4and2,respectively. Atthistime,precision,recallandF1-measureall
performwell,being0.9586,0.9856and0.9719,respectively.
Wethenstudiedtheimpactofvariousotherparametersonthedetectionperformance
duringLogLStraining,includingh,L,S,andE.TheresultsobtainedareshowninFigure10.
Ineachexperiment,wechangethevalueofoneparameterwhileusingthedefaultvalues
ofotherparameters. Graph(a)inFigure10showstheperformancechangeofthemodel
bychangingtheparameterh. BecausetheLSTMnetworkneedslongdependence,within
a certain range, when the selected window is larger, its performance is more obvious,
but when it breaks this range, the performance drops sharply. Figure 10b shows the
performancechangeofthemodelwhentheparameterEisvaried. Withinacertainrange
oftrainingiterations,asthenumberoftrainingiterationsincreases,theperformanceofthe
modelbecomesstronger,butthenumberofiterationsistoolarge,andtheperformanceof
themodelbecomesweaker. Figure10cshowstheperformancechangeofthemodelasthe
parameterLischanged. ThechangeinthenumberoflayersinLogLShasarelativelystable
effect on model performance. Figure 10d shows the performance change of the model
Symmetry2022,14,454 16of21
withchangingparameters. WhenthenumberofmemoryunitsinanLSTMblockis64,
themodelperformanceisthebest. Insummary,ofalltheexperimentalresults,itisfound
thattheLogLSmodelisrelativelystablewhenvariousparametersareadjustedreasonably,
andasinglechangeinparametersoracombinationofadjustmentshaslittleeffectonthe
performanceofthemodel.
Table10.Adjacentparameter.
g1 g2 g3 Precision Recall F1-Measure
12 4 2 0.9558 0.9856 0.9705
12 4 3 0.9596 0.9634 0.9615
12 4 4 0.9649 0.9586 0.9618
12 5 2 0.9609 0.9634 0.9621
12 5 3 0.9648 0.9616 0.9632
12 5 4 0.9664 0.9561 0.9612
12 6 2 0.9646 0.9442 0.9543
12 6 3 0.9653 0.9372 0.9511
12 6 4 0.9664 0.9333 0.9495
13 4 2 0.9586 0.9856 0.9719
13 4 3 0.9624 0.9630 0.9627
13 4 4 0.9678 0.9582 0.9630
13 5 2 0.9637 0.9630 0.9634
13 5 3 0.9677 0.9612 0.9644
13 5 4 0.9693 0.9555 0.9623
13 6 2 0.9675 0.9438 0.9555
13 6 3 0.9683 0.9368 0.9523
13 6 4 0.9693 0.9325 0.9506
14 4 2 0.9602 0.9778 0.9689
14 4 3 0.9640 0.9553 0.9596
14 4 4 0.9694 0.9473 0.9583
14 5 2 0.9654 0.9553 0.9603
14 5 3 0.9694 0.9531 0.9612
14 5 4 0.9709 0.9446 0.9576
14 6 2 0.9693 0.9361 0.9524
14 6 3 0.9699 0.9258 0.9473
14 6 4 0.9710 0.9212 0.9454
(a) (b)
Figure10.Cont.
Symmetry2022,14,454 17of21
(c) (d)
Figure10.LogLSperformancewithdifferentparameters.(a)Windowsize:h.(b)Numberofepochs:
E.(c)Numberoflayers:L.(d)Numberofmemoryunits:S.
5. OnlineUpdateandTrainingofLogLS
AlthoughthemethodinthispaperhasachievedgoodperformanceintheHDFSlog
anomalydetectionexperiment,problemsmayoccurwhendealingwithmoreirregularlogs
(suchassystemlogs). Manylogkeysonlyappearinacertainperiod,sothetrainingset
maynotcontainallthenormallogkeys,whichwillcausefalsepredictions. Themodel
updatemodulecaneffectivelysolvethisproblem,whichadjuststheweightparameters
ofthemodelinrealtimebasedontheonlinefalse-positiveresults. Thissectionsetsupa
comparativeexperimentofmodelupdatestoverifyitseffectiveness.
Themodelupdatemethodadoptsincrementalupdate,andonlyusesfalsepositivesto
updatethemodel. Supposeh=3,theinputhistoricalsequenceis{k ,k ,k },andLogLS
1 2 3
predictsthatthenextlogkeyisk withprobability1,andthelogkeyintheactualsequence
2
isk ,thenthemodelmarksitasananomaly. However,aftermanualdetection,itisknown
3
thatthisisafalsepositive.LogLScanuse{k ,k ,k â†’k }toupdatetheweightsofitsmodel,
1 2 3 3
thereforelearningthisnewlogpattern. Thenexttimeenter{k ,k ,k },LogLScanoutput