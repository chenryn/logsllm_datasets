are even better: between 6% and 27%.
In Figure 6(b), we compare how the mean improvements in net-
n
o
i
t
c
a
r
F
s
s
e
r
g
n
I
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
SP-RE
RA
 0.2
 0.1
 0.4
reduction in Network footprint
 0.3
 1
n
o
i
t
c
a
r
f
s
s
e
r
g
n
i
 0.8
 0.6
 0.4
 0.2
 0
 0.5
SP-RE
RA
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
reduction in network footprint
n
o
i
t
a
z
i
l
i
t
u
k
n
i
l
x
a
m
n
i
n
o
i
t
c
u
d
e
r
SP-RE
RA
 0.5
 0.4
 0.3
 0.2
 0.1
 0
  (50,1.0)
 (20,1.0) 
 (50,0.5) 
 (20,0.5) 
(Overall redundancy, inter fraction)
(a) High redundancy trace (50%)
(b) Low redundancy trace (17%)
Figure 7: Trace-based analysis for SprintLink.
work footprint vary across 6 different tier-1 ISP topologies, where
the mean is computed over all PoPs in an ISP. We see that the mean
improvement is between 2 and 12% for SP-RE and between 11%
and 17% for RA. We note that in some situations, e.g., AS1668,
which has a very sparse topology, the beneﬁts from RA are marginal
compared to SP-RE. For sparse networks, simple redundancy elim-
ination is sufﬁcient to bring down the network footprint.
Real traces. Next, we analyze the beneﬁts of RA and SP-RE us-
ing real packet traces. We conduct our analysis over the network
topology of SprintLink (AS1239). Our approach is to vary the ori-
gin PoP of the packet trace and study the beneﬁts of RA and SP-RE
assuming all packets in the trace are destined for SprintLink’s cus-
tomers. To model where the intra-domain trafﬁc would exit Sprint-
Link’s network, we map the top 2500 destination preﬁxes in the
traces to a US city using “undns” [23] and traceroute. We then map
the city to the nearest SprintLink PoP. We assume that each router
has a 2GB packet store.
Our trace-based analysis is representative of a real-world app-
lication of redundancy elimination and redundancy aware routing.
Using the traces, we ﬁrst compute the redundancy proﬁles (Sec-
tion 3.3.2). Then, we compute redundancy-aware routes, route pack-
ets in the trace on the computed paths, and simulate redundancy
elimination on each router (Section 2.1).
In Figure 7(a), we show the distribution (CDF) of the improve-
ment in network footprint when different PoPs in AS1239 are cho-
sen as the ingress. Here, we use a trace of the high-volume /24 pre-
ﬁx, which had a redundancy proportion of nearly 50%. We see that
both SP-RE and RA offer substantial reductions in network foot-
In particular, we note that the beneﬁt from RA is > 40%
print.
for roughly 10% of the ingresses. One of these ingresses was Seat-
tle; RA aggregates trafﬁc originating from Seattle and destined for
NYC, Boston and Dallas (which receive 36% of trafﬁc in total) to-
gether with destined for Chicago (which receives 40% of the trafﬁc),
and routes all trafﬁc on the single Seattle-Chicago link.
We also conducted trace-based analysis of a full packet trace of
the University access link (Figure 7(b)). The aggregate redundancy
proportion in this trace was 17%. We observe little difference be-
tween SP-RE and RA. This is because, as shown in Figure 4(a), a
very small fraction of the content in this case is duplicated across
PoPs. We do note that redundancy elimination was generally very
beneﬁcial, resulting in 10-20% reduction in the network footprint.
Beneﬁts in intra-domain Trafﬁc Engineering (TE). Next, we
show that redundancy elimination and redundancy-awareness can
help an ISP better meet its network-wide TE goals. We use synthetic
traces in this analysis. In contrast with the earlier study, we now
impose capacity constraints on network links. In particular, given a
Rocketfuel ISP topology, we annotate links with capacities chosen
uniformly at random from {2.5, 10}Gbps.
We generate one synthetic trace per PoP in the ISP topology. For
simplicity, the trafﬁc from all PoPs has the same ρoverall and ρinter.
However, each trace differs in the aggregate trafﬁc volume, which
is chosen to be proportional to the population of the PoP’s location.
Given the trafﬁc proportions, we compute (redundancy-agnostic)
routes which minimize the maximum link utilization in the network.
Figure 8: Trafﬁc engineering with different redundancy proﬁles
(ATT network). The baseline for comparison is SP-MaxLoad.
n
o
i
t
a
z
i
l
i
t
u
k
n
i
l
x
a
m
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 1
SP-MaxLoad
SP-RE
RA
 1.5  2
 2.5  3
 3.5  4
volume increment factor
n
o
i
t
a
z
i
l
i
t
u
k
n
i
l
x
a
m
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 4.5  5
SP-MaxLoad
SP-RE
RA
 1
 1.5
 2
 2.5
 3.5
volume increment factor
 3
 4
 4.5
(a) ρoverall = 0.5, ρintra = 0.25
Figure 9: A simulated ﬂash crowd, where trafﬁc from Chicago
in ATT increases suddenly. The original redundancy proﬁle was
ρoverall = 0.2 and ρintra = 0.5.
(b)ρoverall = 0.5, ρintra = 0.75
We refer to this approach as SP-MaxLoad (We abuse notation here:
the inter-PoP paths may not be shortest in terms of latency). The
aggregate volumes between PoPs in the network are then scaled up
so that the maximum link utilization is 80%.
We ﬁrst employ hop-by-hop redundancy elimination along the
routes obtained above. In Figure 8, the bars labeled “SP-RE” show
the resulting improvement in the maximum link utilization (again,
we abuse notation here). We see that redundancy elimination can
improve maximum link load when coupled with traditional trafﬁc
engineering: the improvement ranges between 1% when (ρoverall,
ρinter) = (0.2, 1), and 25% when (ρoverall, ρinter) = (0.5, 0.5).
The bars labeled “RA” show the beneﬁts of employing redundancy-
aware routes. We note that the improvements in this case are very
substantial: the maximum link load is 10%-37% lower. Such heavy
reduction in the maximum link utilization is extremely valuable to
ISPs because it creates additional capacity within the networks and
allows them to meet service-level objectives more effectively.
Sudden trafﬁc variations. We now examine how our approaches
can mitigate the impact of sudden spikes in trafﬁc load as might oc-
cur during ﬂash crowd events. We use the same set-up as above
for simulating the ﬂash crowd: We start with a network-wide trace
where we set ρoverall = 0.2 and ρinter = 0.5 for trafﬁc from all
ingresses. The trafﬁc volumes are such that the maximum link uti-
lization due to SP-MaxLoad is 50%. Given this set-up, we compute
redundancy-aware network routes.
We then make a sudden change - a factor of f increase over-
all - to the volume of trafﬁc originating from an ingress picked at
random. We also change the redundancy proﬁle, i.e. ρoverall and
ρinter, of the trafﬁc from the ingress. However, we do not recom-
pute new redundancy-aware routes; instead, we study how routes
which match the stale proﬁles perform.
In Figure 9, we show the results from two different ﬂash crowd
simulations. In both cases, we increase ρoverall to 0.5.In the ﬁrst
case, the ﬂash crowd causes a higher fraction of duplicate packets to
be distributed across multiple destinations; in particular, ρinter in-
creases from 0.5 to 0.75. The performance of the different schemes
is shown in Figure 9(a). We see that redundancy elimination, whether
coupled with redundancy-awareness or not, offers clear beneﬁts in
terms of mitigating the impact of the sudden increase. When the
trafﬁc volume increases by f = 3.5X, the maximum link load due
to SP-RE is 85% and that due to RA is 75%. Without any form of
redundancy elimination (SP-MaxLoad), the maximum load is 95%.
t
n
i
r
p
t
o
o
f
k
r
o
w
e
n
t
n
i
n
o
i
t
c
u
d
e
r
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
t-1
SP-RE
RA
RA-stale
t-3
t-2
 High Volume /24 Traces
t-4
t-5
Figure 10: Impact of stale redundancy proﬁles.
t
n
i
r
p
t
o
o
f
k
r
o
w
e
n
t
n
i
n
o
i