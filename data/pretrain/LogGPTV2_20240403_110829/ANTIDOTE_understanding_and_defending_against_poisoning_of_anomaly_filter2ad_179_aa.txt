title:ANTIDOTE: understanding and defending against poisoning of anomaly
detectors
author:Benjamin I. P. Rubinstein and
Blaine Nelson and
Ling Huang and
Anthony D. Joseph and
Shing-hon Lau and
Satish Rao and
Nina Taft and
J. D. Tygar
ANTIDOTE: Understanding and Defending against
Poisoning of Anomaly Detectors
Benjamin I. P. Rubinstein1
Shing-hon Lau1
Blaine Nelson1
Satish Rao1
Ling Huang2
Nina Taft2
Anthony D. Joseph1,2
J. D. Tygar1
1Computer Science Division, University of California, Berkeley
2Intel Labs Berkeley
ABSTRACT
Statistical machine learning techniques have recently gar-
nered increased popularity as a means to improve network
design and security. For intrusion detection, such methods
build a model for normal behavior from training data and
detect attacks as deviations from that model. This process
invites adversaries to manipulate the training data so that
the learned model fails to detect subsequent attacks.
We evaluate poisoning techniques and develop a defense,
in the context of a particular anomaly detector—namely the
PCA-subspace method for detecting anomalies in backbone
networks. For three poisoning schemes, we show how at-
tackers can substantially increase their chance of success-
fully evading detection by only adding moderate amounts
of poisoned data. Moreover such poisoning throws oﬀ the
balance between false positives and false negatives thereby
dramatically reducing the eﬃcacy of the detector.
To combat these poisoning activities, we propose an anti-
dote based on techniques from robust statistics and present a
new robust PCA-based detector. Poisoning has little eﬀect
on the robust model, whereas it signiﬁcantly distorts the
model produced by the original PCA method. Our tech-
nique substantially reduces the eﬀectiveness of poisoning for
a variety of scenarios and indeed maintains a signiﬁcantly
better balance between false positives and false negatives
than the original method when under attack.
Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—
Security and Protection; C.4 [Performance of Systems]:
Modeling Techniques; I.2.6 [Artiﬁcial Intelligence]: Learn-
ing; K.6.5 [Management of Computing and Informa-
tion Systems]: Security and Protection
General Terms
Measurement, Performance, Security
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’09, November 4–6, 2009, Chicago, Illinois, USA.
Copyright 2009 ACM 978-1-60558-770-7/09/11 ...$10.00.
Keywords
Network Traﬃc Analysis, Principal Components Analysis,
Adversarial Learning, Robust Statistics
1.
INTRODUCTION
Statistical machine learning (SML) techniques are increas-
ingly being used as tools for analyzing and improving net-
work design and performance. They have been applied to
a variety of problems such as enterprise network fault di-
agnosis [1, 5, 14], email spam ﬁltering [24, 27], worm de-
tection [25], and intrusion detection [16, 30, 33], as well as
many others. These solutions draw upon a variety of tech-
niques from the SML domain including Singular Value De-
composition, clustering, Bayesian inference, spectral anal-
ysis, maximum-margin classiﬁcation, etc. In many scenar-
ios, these approaches have been demonstrated to perform
well. Many of these SML techniques include a learning
phase during which a model is trained using collected data.
Such techniques have a serious vulnerability, namely they
are susceptible to adversaries who purposefully inject mali-
cious data during the phases of data-collection and model-
building. The intent of such poisoning is to direct an SML
algorithm to learn the wrong model; if adversaries inﬂuence
detectors to learn the wrong underlying model or normality,
then such detectors are unable to properly identify abnormal
activities. Poisoning is particularly incentivized when SML
techniques are used as defenses against cybercrime threats.
Other than a few eﬀorts [10, 31, 32], this type of vulnera-
bility has not been extensively explored by those who apply
SML techniques to networking and systems problems. Ap-
plied machine learning researchers have started to address
these problems by focusing on adversarial training of speciﬁc
algorithms [2, 8, 22]. The learning theory community has
focused on online learning [4], where data is selected by an
adversary with complete knowledge of the learner, and has
developed eﬃcient algorithms with strong guarantees. How-
ever, the simplifying assumption of all data being produced
by an omniscient adversary does not hold for many practi-
cal threat models. Given the increasing popularity of ap-
plying SML techniques to networking problems, we believe
exploring adversarial learning with realistic threat models is
important and timely.
In this paper we study both poisoning strategies and de-
fenses in the context of a particular anomaly detector, namely
the PCA-subspace method [16], based on Principal Compo-
nent Analysis (PCA). This technique has received a large
amount of attention, leading to extensions [15, 17, 18], and
inspiring related research [3, 12, 20, 28, 33]. We consider
1an adversary who knows that an ISP is using a PCA-based
anomaly detector. The adversary’s aim is to evade future
detection by poisoning the training data so that the detec-
tor learns a distorted set of principal components. Because
PCA solely focuses on link traﬃc covariance, we explore poi-
soning schemes that add chaﬀ (additional traﬃc) into the
network to increase the variance of network traﬃc. The end
goal of the attacker is to increase the false negative rate of a
detector, which corresponds to his evasion success rate. In
our abstract [29], we illustrated that simple poisoning strate-
gies can improve an adversary’s ability to evade detection.
Our ﬁrst contribution in this paper is a detailed analysis of
how adversaries subvert the learning process. We explore a
range of poisoning strategies in which the attacker’s knowl-
edge about the network traﬃc state is varied, and in which
the attacker’s time horizon (length of poisoning episode) is
varied. (We use the words ‘attackers’ and ‘adversaries’ inter-
changeably.) Through theoretical analysis of global poison-
ing tactics, we uncover some simple and eﬀective poisoning
strategies for the adversary. In order to gain insights as to
why these attacks work, we illustrate their impact on the
normal model built by the PCA detector.
Because the networks that SML techniques are used in are
non-stationary, the baseline models must be periodically re-
trained to capture evolving trends in the underlying data.
In previous usage scenarios [16, 30], the PCA detector is
retrained regularly (e.g., weekly), meaning that attackers
could poison PCA slowly over long periods of time; thus poi-
soning PCA in a more stealthy fashion. By perturbing the
principal components gradually, the attacker decreases the
chance that the poisoning activity itself is detected. We de-
sign such a poisoning scheme, called a Boiling Frog scheme,
and demonstrate that it can boost the false negative rate as
high as the non-stealthy strategies, with far less chaﬀ, albeit
over a longer period of time.
Our second contribution is to design a robust defense
against this type of poisoning. It is known that PCA can
be strongly aﬀected by outliers [28]. However, instead of
ﬁnding the principal components along directions that max-
imize variance, robust statistics suggests components that
maximize more robust measures of dispersion.
It is well
known that the median is a more robust measure of loca-
tion than the mean, in that it is far less sensitive to the
inﬂuence of outliers. This concept can be extended to ro-
bust alternatives to variance such as the Median Absolute
Deviation (MAD). Over the past two decades a number of
robust PCA algorithms have been developed that maximize
MAD instead of variance. Recently the PCA-GRID algo-
rithm was proposed as an eﬃcient method for maximizing
MAD without under-estimating variance (a ﬂaw identiﬁed in
previous solutions) [6]. We adapt PCA-GRID for anomaly
detection by combining the method with a new robust cutoﬀ
threshold. Instead of modeling the squared prediction error
as Gaussian (as in the original PCA method), we model the
error using a Laplace distribution. The new threshold was
motivated from observations of the residual that show longer
tails than exhibited by Gaussian distributions. We call our
method that combines PCA-GRID with a Laplace cutoﬀ
threshold, antidote. The key intuition behind this method
is to reduce the eﬀect of outliers and help reject poisonous
training data.
Our third contribution is to carry out extensive evalua-
tions of both antidote and the original PCA method, in
a variety of poisoning situations, and to assess their perfor-
mance via multiple metrics. To do this, we used traﬃc ma-
trix data from the Abilene network since many other studies
of traﬃc matrix estimation and anomaly detection have used
this data. We show that the original PCA method can be
easily compromised by any of our poisoning schemes, with
only small amounts of chaﬀ. For moderate amounts of chaﬀ,
the PCA detector starts to approach the performance of a
random detector. However, antidote is dramatically more
robust.
It outperforms PCA in that i) it more eﬀectively
limits the adversary’s ability to increase his evasion success;
ii) it can reject a larger portion of contaminated training
data; and iii) it provides robust protection across nearly all
origin-destination ﬂows through a network. The gains of an-
tidote for these performance measures are large, especially
as the amount of poisoning increases. Most importantly,
we demonstrate that antidote incurs insigniﬁcant shifts in
its false negative and false positive performance, compared
to PCA, when no poisoning events happen; however when
poisoning does occur, the gains of antidote over PCA are
enormous with respect to both of these traditional perfor-
mance measures. The PCA method was not designed to
be robust. Our results indicate that it is possible to take
such useful techniques and bolster their performance under
diﬃcult circumstances.
Our study sheds light on the general problem of poisoning
SML techniques, in terms of the types of poisoning schemes
that can be construed, their impact on detection, and strate-
gies for defense.
Related Work. Several earlier studies examined attacks
on speciﬁc learning systems for related applications. In [26],
the authors describe red herring attacks that weaken poly-
morphic worm detection systems by poisoning the training
data used to build signature-based classiﬁers. In red herring
attacks, the adversary forces the learner to make false neg-
ative predictions by including spurious features in positive
training examples. Subsequent malicious instances evade
detection by excluding these features, now included as con-
juncts in the conjunction learned by Polygraph. Venkatara-
man et al. [31] present lower bounds for learning worm sig-
natures based on red herring attacks and reductions to clas-
sic results from Query Learning. While the red herring at-
tacks exploit the Polygraph conjunction learner’s tendency
to overﬁt, our poisoning attacks exploit PCA’s singular focus
on link traﬃc covariance.
Attacks that increase false negative rates by manipulat-
ing the test data have also been explored. The polymorphic
blending attacks of Fogla and Lee [10] encrypt malicious
traﬃc so that the traﬃc is indistinguishable from innocu-
ous traﬃc to an intrusion detection system. By contrast
our variance injection attacks add small amounts of chaﬀ to
largely innocuous training traﬃc to make the traﬃc appear
more like future DoS attacks to be launched post-poisoning.
In the email spam ﬁltering domain, Wittel and Wu [32] and
Lowd and Meek [22] add good words—tokens the ﬁlter as-
sociates with non-spam messages—so spam messages can
evade detection.
Ringberg et al. [28] performed a study of the sensitivities
of the PCA method that illustrates how the PCA method
can be sensitive to the number of principal components used
to describe the normal subspace. This parameter can limit
PCA’s eﬀectiveness if not properly conﬁgured. They also
show that routing outages can pollute the normal subspace;
2a kind of perturbation to the subspace that is not adversar-
ial. Our work diﬀers in two key ways. First we demonstrate
a diﬀerent type of sensitivity, namely that of data poisoning.
This adversarial perturbation can be stealthy and subtle,
and is more challenging to circumvent than observable rout-
ing outages. Second, [28] focuses on showing the variability
in PCA’s performance to certain sensitivities, and not on
defenses. In our work, we propose a robust defense against
a malicious adversary and demonstrate its eﬀectiveness. It
is conceivable that the technique we propose could help limit
PCA’s sensitivity to routing outages, although such a study
is beyond the scope of this paper. A recent study [3] showed
that the sensitivities observed in [28] come from PCA’s in-
ability to capture temporal correlations. They propose to
replace PCA by a Karhunen-Loeve expansion. Our study
indicates that it would be important to examine, in future
work, the data poisoning robustness of this proposal.
2. BACKGROUND
To uncover anomalies, many network anomography detec-
tion techniques mine the network-wide traﬃc matrix, which
describes the traﬃc volume between all pairs of Points-of-
Presence (PoP) in a backbone network and contains the col-
lected traﬃc volume time series for each origin-destination
(OD) ﬂow. In this section, we deﬁne traﬃc matrices, present
our notation, and summarize the PCA anomaly detection
method of Lakhina et al. [16].
2.1 Trafﬁc Matrices and Volume Anomalies
Network link traﬃc represents the superposition of OD
ﬂows. We consider a network with N links and F OD ﬂows
and measure traﬃc on this network over T time intervals.
The relationship between link traﬃc and OD ﬂow traﬃc is
concisely captured in the routing matrix A. This matrix is
an N × F matrix such that Aij = 1 if OD ﬂow j passes over
link i, and is zero otherwise. If X is the T × F traﬃc matrix
(TM) containing the time-series of all OD ﬂows, and if Y is
the T × N link TM containing the time-series of all links,
row of Y as y(t) = Yt,•
then Y = XA
(the vector of N link traﬃc measurements at time t), and the
original traﬃc along a source link, S by yS(t). We denote
column f of routing matrix A by Af .
. We denote the t
(cid:2)
th
We consider the problem of detecting OD ﬂow volume
anomalies across a top-tier network by observing link traf-
ﬁc volumes. Anomalous ﬂow volumes are unusual traﬃc
load levels in a network caused by anomalies such as De-
nial of Service (DoS) attacks, Distributed DoS attacks, ﬂash
crowds, device failures, misconﬁgurations, and so on. DoS
attacks serve as the canonical example attack in this paper.
2.2 Subspace Method for Anomaly Detection
We brieﬂy summarize the PCA-based anomaly detector
introduced by Lakhina et al. [16]. The authors observed high
levels of traﬃc aggregation on ISP backbone links cause OD
ﬂow volume anomalies to often go unnoticed because they
are buried within normal traﬃc patterns. They also observe
that although the measured data has high dimensionality,
N , normal traﬃc patterns lie in a subspace of low dimension
K (cid:2) N . Inferring this normal traﬃc subspace using PCA
(which ﬁnds the principal traﬃc components) makes it eas-
ier to identify volume anomalies in the remaining abnormal
subspace. For the Abilene (Internet2 backbone) network,
most variance can be captured by the ﬁrst K = 4 principal
components.
PCA is a dimensionality reduction method that chooses
K orthogonal principal components to form a K-dimensional
subspace capturing maximal variance in the data. Let ¯Y be
the centered link traﬃc matrix, i.e., with each column of Y is
translated to have zero mean. The k
principal component
is computed as
th
‚‚‚‚‚ ¯Y
I − k−1X
i=1
!
‚‚‚‚‚ .
(cid:2)
i
viv
w
(1)
vk = arg max
w:(cid:3)w(cid:3)=1
The resulting K-dimensional subspace spanned by the ﬁrst
K principal components V1:K = [v1, v2, . . . , vK] is the nor-
mal traﬃc subspace Sn and has a projection matrix Pn =
1:K . The residual (N − K)-dimensional subspace is
(cid:2)
V1:KV
spanned by the remaining principal components VK+1:N =
[vK+1, vK+2, . . . , vN ]. This space is the abnormal traﬃc
subspace Sa with a corresponding projection matrix Pa =
VK+1:N V
K+1:N = I − Pn.
(cid:2)
Volume anomalies can be detected by decomposing the
link traﬃc into y(t) = yn(t) + ya(t) where yn(t) is the mod-
eled normal traﬃc and ya(t) is the residual traﬃc, corre-
sponding to projecting y(t) onto Sn and Sa, respectively. A
volume anomaly at time t typically results in a large change
to ya(t), which can be detected by thresholding the squared
prediction error (cid:3)ya(t)(cid:3)2 against Qβ, the Q-statistic at the
1− β conﬁdence level [13]. That is, the PCA-based detector
classiﬁes a link measurement vector as
(
anomalous, (cid:3)ya(t)(cid:3)2 > Qβ
(cid:3)ya(t)(cid:3)2 ≤ Qβ
innocuous,
.
(2)
c (y (t)) =
While others have explored more eﬃcient distributed vari-
ations of this approach [12, 20, 21], we focus on the basic
method introduced by Lakhina et al. [16].
3. POISONING STRATEGIES
3.1 The Threat Model
The adversary’s goal is to launch a Denial of Service (DoS)
attack on some victim and to have the attack traﬃc success-
fully cross an ISP’s network without being detected. The
DoS traﬃc thus needs to traverse from an ingress point-of-
presence (PoP) node to an egress PoP of the ISP. Before
launching a DoS attack, the attacker poisons the detector
for a period of time, by injecting additional traﬃc, chaﬀ,
along the OD ﬂow (i.e., from an ingress PoP to an egress
PoP) that he eventually intends to attack. This kind of poi-
soning activity is possible if the adversary gains control over
clients of an ingress PoP or if the adversary compromises
a router (or set of routers) within the ingress PoP. For a
poisoning strategy, the attacker needs to decide how much
chaﬀ to add, and when to do so. These choices are guided
by the amount of information available to the attacker.
We consider poisoning strategies in which the attacker
has increasing amounts of information at his disposal. The
weakest attacker is one that knows nothing about the traﬃc
at the ingress PoP, and adds chaﬀ randomly (called an unin-
formed attack). An intermediate case is when the attacker is
partially informed. Here the attacker knows the current vol-
ume of traﬃc on the ingress link(s) that he intends to inject
chaﬀ on. Because many networks export SNMP records, an
3adversary might intercept this information, or possibly mon-
itor it himself (i.e., in the case of a compromised router).
We call this type of poisoning a locally-informed attack. Al-
though exported data from routers may be delayed in reach-
ing the adversary, we consider the case of minimal delay in
our ﬁrst study of this topic.
In a third scenario, the attacker is globally-informed be-
cause his global view over the network enables him to know
the traﬃc levels on all network links. Moreover, we assume
this attacker has knowledge of future traﬃc link levels. (Re-
call that in the locally-informed scheme, the attacker only
knows the current traﬃc volume of a link.) Although these
attacker capabilities are very unlikely, we include this in our
study in order to understand the limits of variance injection
poisoning schemes. Also this scenario serves as a diﬃcult
test for our antidote technique.
Poisoning strategies can also vary according to the time