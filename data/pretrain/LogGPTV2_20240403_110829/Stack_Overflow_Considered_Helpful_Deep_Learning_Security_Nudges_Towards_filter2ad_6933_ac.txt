ter this step, the seed statement knows that it is part of an
insecure pattern and its embedding preserves this informa-
tion accordingly. We extract the pattern embedding φ K
1 of
the seed statement and apply weights W2. Note, we train W2
based on classiﬁcation loss of aggregated pattern similarity
pa as explained in Section 5.5. However, we use the trained
model to generate and output embeddings for each individual
pattern pv in the graph (see Figure 3).
5.5 Training
For unsupervised training of pattern embeddings, we need
to generate similar and dissimilar input pairs from data that
provides ground truth. We use two different sets of PDGs
that are compiled from the same source code. One set S con-
tains the sound graph representations of the code, the other
one the unsound graphs U. A sound graph Gs(Vs,Es) in the
ﬁrst set is compiled from a complete program using a stan-
dard Java compiler. An unsound graph Gu(Vu,Eu) in the sec-
ond set is generated by a partial compiler [10] that compiles
each Java class of a program individually. Then we construct
the feature vectors Xs = {xs}vs∈Vs and Xu = {xu}vu∈Vu for all
Figure 3: Pattern embedding network overview
statements v in the respective graphs.
To obtain ground truth for similar and dissimilar usage
patterns we need to create similar pairs(cid:0)(cid:10)vs,vu
similar pairs (cid:0)(cid:10)vs,vu
(cid:11),1(cid:1) and dis-
(cid:11),−1(cid:1). However, we do not have in-
formation about the relationship of statements in Gu(Vu,Eu)
with statements Gs(Vs,Es), which is necessary to create these
pairs. Note that source code statements do not correspond
one-to-one with statements in the PDG. The compiler may
divide a source code statement into multiple instructions,
which may have different associated statements in the PDG.
Since we use different compilers, the resulting PDG state-
ments in Vs and Vu may look different even though they rep-
resent the same source code. Therefore, we aggregate all
pattern embeddings from a method into pa and use the re-
(cid:11)
sulting embedding for training. The network calculates the
loss based on the cosine similarity of the aggregated pattern
embedding pairs {
}xs,xu∈Xs,Xu and their given simi-
larity label y ∈ {−1,1}
We downloaded 824 open source Android apps from
GitHub and compiled the complete and sound graph
Gs(Vs,Es) for each method.
Further, we used the par-
tial compiler to obtain the unsound graph for each method
Gu(Vu,Eu). After creating the feature vectors Xs and Xu from
the graphs, similar method pairs(cid:0)(cid:10)Xs,Xu
similar(cid:0)(cid:10)Xs,Xu
(cid:11),1(cid:1) were created
(cid:11),−1(cid:1) by extracting Xs and Xu from different
by extracting Xs and Xu from the same source code, and dis-
source code. From the 824 downloaded apps, we extracted
91,075 methods to create 157,162 input pairs in total. These
pairs have been split up into the training and validation set,
where 80% have been randomly allocated for training and
20% for validation. Note that the intersection of both sets is
empty.
(cid:10)ps
a, pu
a
5.6 Learning Use Cases and Security
From a given source code example, we want to be able to
predict the cryptographic use case and security of patterns
within the code. We apply transfer learning by reusing the
USENIX Association
28th USENIX Security Symposium    345
Insecure PatternSeedStatement……K Iterationsx5x4x3x2x1x0φ05φ04φ03φ02φ01φ11φ00φ15φ14φ13φ12φ10φK0φK1φK2φK3φK4φK5Output while TrainingW2×+Output afterTrainingTW2×φK1pvpapreviously learned pattern embedding that already encodes
their similarity information.
Pattern embeddings are learned unsupervised and we can
obtain almost arbitrarily large training datasets from open
source projects. However, code examples on Stack Over-
ﬂow provide a very different distribution of data [17]. Many
use case and security classes are under- or overrepresented
and availability of encryption code examples is limited in
general. We transfer knowledge from the similarity domain
to the use case and security domain in order to tackle these
problems. We argue that the similarity information preserved
in our pattern embeddings will be helpful for classifying their
use cases and security.
5.7 Labeling
We extracted 10,558 code examples from Stack Overﬂow4
by searching for code that contained at least one of the seed
statements. Each code sample has been manually reviewed
in order to label use case and security of the contained usage
patterns. Labeling was done by two security experts individ-
ually applying the labeling rules given by [12, 13, 17]. This
leads to conservative binary security labeling, which might
at times be too strict. For instance, depending on the con-
text, MD5 can be the better trade-off and secure enough.
However, our approach aims at developers that are layman
in cryptography and we consider binary classiﬁcation prefer-
able to encourage safe defaults.
Initially, 100 samples for each of the different seed state-
ments have been selected randomly to apply dual control la-
beling. After clearing up disagreements, the remaining sam-
ples have been annotated individually to speed up the label-
ing. The whole process took approximately 10 man days
to complete. To evaluate individual annotation accuracy, we
randomly selected 200 samples from both experts and report
agreement of 98.32% on given labels. We further publish
the annotated dataset in order to allow veriﬁcation of anno-
tation accuracy and reproduction of our results. Please refer
to Appendix C for further details on the annotation process.
Figure 4: End-to-End Network Architecture
4I.e., from the Ofﬁcial Stack Overﬂow Data Dump.
5.8 End-to-end Architecture
We introduce an architecture that allows classiﬁcation of dif-
ferent uses cases and security, while improving the pattern-
embedding model in order to forward optimized code repre-
sentations to the classiﬁcation layer (see Figure 4).
To achieve this, we add a fully connected layer with di-
mension 1,024 using Rectiﬁed Linear Unit (ReLu) as the ac-
tivation function on top of the pattern-embedding network.
We use a softmax layer for binary and multi-class classi-
ﬁcation and trained our network to optimize cross-entropy
loss. Applying transfer learning, we initialize the pattern-
embedding network using the previously learned weights for
pattern similarity (see Section 5.4). The end-to-end network
now connects the pattern-embedding network with the clas-
siﬁcation layers. Within training, the latter backpropagates
cross-entropy loss from the classiﬁcation task all the way to
the input of the pattern-embedding network. This allows the
similarity network to adjust the pattern representation in or-
der to better perform on the classiﬁcation. Therefore, both
coupled networks now generate a new pattern representation
for the given classiﬁcation problem in such a way that it is
optimally solved.
For instance, security classiﬁcation of Cipher, Key and TM
rely on very different features. Only using the pre-trained
“static” pattern embeddings might therefore be disadvanta-
geous for some use cases. However, by dynamically cus-
tomizing the pattern embedding with respect to classiﬁcation
loss minimization, the network learns a code representation
that preserves the necessary code features to improve classi-
ﬁcation.
5.9 Training
The Stack Overﬂow dataset provides 16,539 pattern embed-
dings extracted from 10,558 code examples. Note that a sin-
IV,
gle code example might contain several patterns, e. g.
Key is used to initialize Cipher. We test pattern embeddings
generated from several models that were trained on differ-
ent neighborhood sizes K and different output dimensions
d for the embeddings. Thereby, we search for the optimal
hyperparameter K and d to achieve the best performance on
both classiﬁcation tasks. We ﬁrst train the network to learn
the use case identiﬁer of pattern embeddings using the com-
plete dataset. Then, we train a different model to learn the
security labels. Here, patterns with the same security label
belong to the same class independently of their use case. Fi-
nally, we divide the dataset into combinations of several use
case classes, testing the effect on performance of security
prediction.
346    28th USENIX Security Symposium
USENIX Association
x1 x2x3xNx1 x2x3xdK Iterationsx1 x2x3xMx1 x2x3xLx1 x2ReLuLoss BackpropagationCrossEntropyLossOutput LayerFully ConnetedLayerPattern EmbeddingNetworkInput6 Security Nudges
The neural network architecture described in the previous
section provides everything needed to apply the security
nudges on Stack Overﬂow. In this section, we explain the
design of each nudge including its implementation on Stack
Overﬂow and how it applies the predictions from the simi-
larity and classiﬁcation models5.
6.1 Security Warnings
Whenever an insecure code example is detected, a security
warning, as shown in Figure 8, which surrounds the code, is
displayed to the user. The warning is triggered by the predic-
tion result of the security model that classiﬁes each pattern
in the snippet.
The difﬁculties in designing effective security warnings
are widely known and have been extensively investigated.
We base our approach on the design patterns of Google
Chrome’s security warning for insecure server communica-
tion, whose effectiveness has been comprehensively ﬁeld-
tested [3]. The header of the warning informs the user that
a security problem has been detected in the encryption code
of the sample. Note that we assume users with a very di-
verse background, knowledge and expertise in cryptography.
Users and even experienced developers might not be aware
of ﬂawed or out-dated encryption that does not provide sufﬁ-
cient security. Therefore, we inform the user about the con-
sequences that might occur when reusing insecure code ex-
amples in production code, e. g., private information might
be at risk in an attack scenario.
We further provide code annotations for each seed state-
ment in the code whose usage pattern has been classiﬁed as
insecure (see Figure 2). The annotation is attached below and
points at the statement. It gives further information about the
statement, while additionally highlighting the consequences
of reusing it.
In order to select the correct annotation for
the insecure statement, we apply use case prediction of the
related pattern. Each use case identiﬁer has an assigned se-
curity annotation to be displayed in the code snippet.
6.2 Security Recommendations
Security warnings should always offer a way out of a situa-
tion where the user seems to be unable to continue with her
current action due to the warning. Whenever the user decides
to follow the advice given by the warning, she would refuse
to reuse the code example that was originally considered a
candidate for solving her problem. In this situation, she has
been thrown out of her usual user pattern as she has to restart
searching for another example. Therefore, for each insecure
code example, we recommend a list of similar examples, as
shown in Figure 8, that serve the same use case and provide
5We provide further example ﬁgures of our nudges in the Appendix.
Ideally, the user would only have to
stronger encryption.
click on a single link to the recommended alternative. Our
nudge design pattern does not claim that the recommended
code is generally secure, as it still might contain insecure pat-
terns that are unknown to the model. However, for simplicity,
we refer to code examples, which do not contain any detected
insecure patterns and do contain detected secure patterns, as
secure code examples throughout the paper.
We create this list of recommendations by applying simi-
larity search, use case and security prediction of usage pat-
terns in code examples. We start with predicting the use case
of each insecure usage pattern. Iq contains all insecure use
cases of a method q ∈ Mq, where Mq is the set of query
methods in the snippet. We create the set {Iq}q∈Mq, which
consists of the sets of insecure use cases over all methods in
the snippet. Then, we generate the set of aggregated pattern
embeddings {eq}q∈Mq, as described in Section 5.4. After-
wards, we analogously create the set of secure use cases for
all target methods {St}t∈Mt where Mt is the set of methods
available on Stack Overﬂow that only contain usage patterns
our model has classiﬁed as secure. Likewise, we create the
aggregated pattern embeddings {et}t∈Mt . We rank Mt for
given Iq,St and eq,et based on ascending Jaccard distance
dJ(Iq,St ), ranking pairs with the same distance using cosine
similarity cos(eq,et ). We create the ranked list of recom-
mended posts R by adding the related Stack Overﬂow post
for each t of the top-ﬁfty results in the ranking. Beneath the
security warning, we display a scrollable list of R, as dis-
played in Figure 8. Each post is displayed by showing the
title of the related question. When the user clicks on the ti-
tle, a new browser tab opens and the web page automatically
scrolls down to the recommended code example, highlight-
ing it with a short ﬂash animation.
Recommended examples are displayed inside a green box,
annotated with a check mark and message informing the
user that no common encryption problems have been found
within the code. This way, we avoid declaring the code ex-
ample to be secure, which would be a too strong claim. How-
ever, the statement intends to be strong enough to reach the
users and make them follow the advice. Similar to warnings,
we provide code annotations for each statement in the code
whose usage pattern has been classiﬁed as secure.
6.3 Security Reminders and Defaults
We further caution the user – in addition to prompting the
security warning and recommendations – by blurring out the
remainder of the web page, whenever a copy event of an in-
secure code example is triggered.
We additionally apply a search ﬁlter which up-ranks posts
that only contain secure code examples. Posts with insecure
code examples are appended to the list of secure posts. The
original ranking of posts within its security class is main-
tained. This approach lowers the risk of reusing code exam-
USENIX Association
28th USENIX Security Symposium    347
(a) Use cases before training
(b) Use cases after training
(c) Security before training.
(d) Security after training.
Figure 5: Visualizing the pattern embeddings of different use cases and security using PCA. Each color indicates one use case
in (a) and (b), and security in (c) and (d). The legend provides the use case identiﬁer.
ples that have been predicted to be insecure. This also means
that whenever a post consists of secure and insecure samples
it is ranked lower than posts with only secure samples.
7 Model Evaluation
7.1 Pattern Similarity
We evaluate the learned pattern embeddings by measuring
cosine similarity for all pairs in the validation set and calcu-
late the receiver operating characteristic curve (ROC) given
the similarity label y ∈ {−1,1} for each pair. Our approach
reaches an optimal AUC of 0.978, which slightly outper-
forms Gemini with AUC of 0.971. Gemini was originally ap-
plied to similarity prediction of binary functions by learning
embeddings for CFGs and may not be a suitable benchmark.
We observe that the model converges already after ﬁve
epochs. For the remaining epochs, AUC stays around 0.978
and does not improve signiﬁcantly. This allows for a very
short training time, as ﬁve epochs only need 27 minutes on
average on our system6. However, we choose the model with
the best AUC for generating the pattern embeddings.
7.2 Use Case Classiﬁcation
For training the use case and security models, we apply the
dataset consisting of 16,539 pattern embeddings extracted
from Stack Overﬂow split up into subsets for training (80%)
and validation (20%). Note that the validation set is con-
structed such that none of its samples appear in the training
set. Therefore, we evaluate the performance of use case pre-
diction on unseen pattern embeddings.
Visualization To illustrate the transfer learning process,
we plot the pattern embeddings in 2D using principal com-
ponent analysis (PCA) before and after the training of the
classiﬁcation model. Figure 5(a) shows the complete set of
pattern embeddings before training, displayed in the color of
6Intel Xeon E5-2660 v2 (”Sandy Bridge”), 20 CPU cores, 240GB mem-
ory
their use case. We observe that some use cases already build
clusters in the plot, while others appear overlapping and in-
termixed. Therefore, we apply an additional neural network
on top that leverages supervision on use cases in addition to
the similarity knowledge preserved in the input embeddings.
Figure 5(b) plots the pattern embeddings again after super-
vised training of the model. Here, we input the initial pat-
tern embeddings into the trained model and extract the last
hidden layer of the network to obtain new embeddings that
preserve information about their use case. We observe that