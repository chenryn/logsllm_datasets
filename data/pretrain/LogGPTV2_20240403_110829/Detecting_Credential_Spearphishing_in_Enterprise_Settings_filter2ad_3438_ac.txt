that resembles a known or authoritative entity, but
where the name and email address do not exactly match
any existing entity’s values (e.g.,IT Support Team
);
if the name or ad-
dress did exactly match an existing entity, the attack
would instead fall under the name spoofer or address
spoofer threat model. Compared to name-spooﬁng at-
tacks, these attacks are more difﬁcult to detect because
we have no prior history to compare against; indeed,
prior work does not attempt to detect attacks from this
threat model. To deal with this obstacle, we rely on an
assumption that the attacker will seek to avoid detection,
and thus the spoofed identity will be infrequently used;
each time Mallory uses the spoofed identity, she runs the
risk that the employee she’s interacting with might rec-
ognize that Mallory has forged the name or email address
and report it. Accordingly, we use two features: the num-
ber of prior days that the From name has sent email, and
the number of prior days that the From address has sent
emails.
Lateral Attacker: This sub-detector aims to catch
spearphishing emails sent from a compromised user’s ac-
counts (without using any spooﬁng). To detect this pow-
USENIX Association
26th USENIX Security Symposium    475
It then extracts two features:
erful class of attackers, we leverage the LDAP logs pro-
vided by Gmail’s corporate email services (§ 3). When
a recipient clicks on a link in an email, if the email was
sent by an employee, we check the LDAP logs to see
if the email was sent during a login session where the
sender-employee logged in using an IP address that the
sender-employee has never used before. If so, this sub-
detector computes the geolocated city of the session’s IP
address, say city C.
the
number of distinct employees that have logged in from
city C, and the number of previous logins where this
sender-employee logged in from an IP address that ge-
olocated to city C.
Content-Based Features: As discussed in Section 3, for
privacy reasons we do not have access to either the bod-
ies of emails or the contents of a clicked URL’s webpage.
If desired, enterprises could augment our sender reputa-
tion features with additional features from the raw con-
tent in the email message or website (e.g., NLP features
that characterize whether the email message relates to ac-
counts/credentials/passwords or reﬂects particular senti-
ments such as urgency).
5.3 Limitations of Standard Detection
Techniques
Once our detector has extracted features for each click-
in-email event, it needs to decide which ones should trig-
ger an alert for the security team. We ﬁrst discuss three
natural, but ultimately ineffective, approaches for deter-
mining which events to alert on. Then, in the following
subsection, we present a new technique that our detec-
tor uses to overcome the limitations of these canonical
approaches.
Manual Thresholds: The simplest approach would be
to manually select a threshold for each feature, and gen-
erate an alert if all feature values are below the thresh-
old. One might use domain knowledge of each feature
to guess a threshold for each feature dimension: e.g.,
spearphishing attacks will use URLs whose domain has
fewer than ﬁve visits or was ﬁrst visited less than ﬁve
days ago. Unfortunately, this approach is inherently ar-
bitrary since we do not know the true distribution of fea-
ture values for spearphishing attacks. Thus, this ad hoc
approach can easily miss attacks, and does not provide a
selection criteria that generalizes across different enter-
prises.
Supervised Learning: A large body of literature
on attack detection, from spam classiﬁcation to prior
spearphishing work, draws heavily on supervised ma-
chine learning algorithms. However, those methods are
not suitable for our setting.
To accurately classify new events, supervised learning
techniques require a labeled training dataset that reﬂects
the range of possible malicious and benign feature val-
ues. Unfortunately, in our context, it is difﬁcult to as-
semble a large enough training set. Because spearphish-
ing attacks are extremely difﬁcult to detect and occur at
a low rate, we have few malicious samples to train on.
Additionally, our setting exhibits extreme class im-
balance: because of the scarcity of data on known
spearphishing attacks, the training set will have vastly
more benign instances than malicious instances. Super-
vised techniques often need a relatively balanced dataset;
classiﬁers trained on highly imbalanced data often learn
to always predict the majority class (missing real at-
tacks), pathologically overﬁt to accidental characteris-
tics of the minority class, or generate too lax of a deci-
sion boundary and generate prohibitively high numbers
of false positives [10]. While the machine learning com-
munity has explored a number of techniques for address-
ing imbalanced training data [6, 10], such as undersam-
pling the over-represented class or synthetically generat-
ing samples for the under-represented class, these tech-
niques do not scale to imbalances on the order of millions
to one.
Standard Anomaly Detection: Alternatively, one might
consider unsupervised or semi-supervised anomaly de-
tection techniques. While a number of such tech-
niques exist,
including density estimation techniques
such as Gaussian Mixture Models (GMMs) [5] and clus-
tering and distance-based techniques such as k-nearest-
neighbor (kNN) [13], these classical techniques suffer
from three limitations.
First, in a number of security settings, scalar features
often have a directionality to their values; and indeed,
all of our features have this property. For example,
the fewer visits a domain has, the more suspicious it
is; an unusually small number of visits is grounds for
suspicion, but an unusually large number is not. Stan-
dard anomaly detection techniques do not incorporate
notions of asymmetry or directionality into their compu-
tations. For example, density-based anomaly detection
techniques such as kernel density estimation (KDE) and
GMMs ﬁt a probability distribution to the data and alert
on the lowest-probability events. Events that have sta-
tistically extreme—but benign—feature values will have
a very low probability of occurring, triggering a large
number of useless alerts.
Second, standard anomaly detection techniques often
treat an event as anomalous even if only one or a few
of the event’s features are statistically anomalous. How-
ever, in our setting, we expect that attacks will be anoma-
lous and suspicious in all feature dimensions. Conse-
quently, in our setting, classical techniques will generate
476    26th USENIX Security Symposium
USENIX Association
Algorithm 1 Scoring and Alert Selection in DAS
Score(E, L):
1: for each event X in L do:
2:
3:
Increment E’s score by one
if E is more suspicious than X in every dimension:
AlertGen(L (a list of events), N):
Score(E, L)
1: for each event E in L do:
2:
3: Sort L by each event’s score
4: return the N events from L with the highest scores
many spurious alerts for events that are only anomalous
in a few dimensions. As we show in Section 6.3, this
causes classical techniques to miss the vast majority of
spearphishing attacks in our dataset because they exhaust
their alert budget with emails that have benign feature
values in all but one dimension.
Third, classical techniques are parametric: they either
assume the data comes from a particular underlying dis-
tribution, or they contain a number of parameters that
must be correctly set by their deployer in order for the
technique to obtain acceptable performance. GMMs as-
sume the data comes from a mixture of Gaussian distri-
butions, KDE has a bandwidth parameter that requires
tuning by the deployer, and kNN needs the deployer to
select a value of k (the number of nearest neighbors/most
similar events, which the algorithm will use to compute
an event’s anomaly score). These requirements are prob-
lematic for spearphishing detection since we do not know
the true distribution of attack and benign emails, the un-
derlying distribution might not be Gaussian, and we do
not have a sound way to select the parameters.
5.4 Directed Anomaly Scoring (DAS)
Given the limitations of traditional detection techniques,
we introduce a simple and general technique for automat-
ically selecting the most suspicious events from an unla-
beled dataset. We call our technique Directed Anomaly
Scoring (DAS). At a high level, DAS ranks all events by
comparing how suspicious each event is relative to all
other events. Once all events have been ranked, DAS
simply selects the N most suspicious (highest-ranked)
events, where N is the security team’s alert budget.
Algorithm 1 shows the procedure for scoring and gen-
erating alerts with DAS. Concretely, DAS ﬁrst assigns an
anomaly score for each event, E, by computing the to-
tal number of other events where E’s feature vector is at
least as suspicious as the other event in every feature di-
mension. Thus, E’s score counts how many events it is at
least as suspicious as; events with higher scores are more
suspicious than ones with lower scores. Figure 5 presents
a few visual examples of computing DAS scores. After
scoring every event, our algorithm simply sorts all events
by their scores and outputs the N highest-scoring events.
Formally, we identify each event with its feature vec-
tor E ∈ Rd. We consider event E to be at least as sus-
picious as event E(cid:48), written E (cid:60) E(cid:48), if Ei ≤ E(cid:48)
i for all
i = 1,2, . . . ,d. (For simplicity, we assume that smaller
feature values are more suspicious, in every dimension;
for dimensions where the reverse is true, we replace the
comparator ≤ with ≥. Appendix A summarizes the com-
parators we use for each feature.) Then, the score of
event E is the cardinality of the set {E(cid:48) : E (cid:60) E(cid:48)}.
DAS is well-suited for a range of security detection
problems where attacks can be characterized by a com-
bination of numerical and boolean features, such as our
spearphishing use case. As we show in Section 6, DAS
achieves orders-of-magnitude better results than classi-
cal anomaly detection techniques because it leverages
domain knowledge about which regions of the feature
space are most suspicious; in particular, it overcomes all
three limitations of classical techniques discussed in Sec-
tion 5.3.
5.5 Real-time Detection Architecture
We now synthesize the ideas discussed in previous sub-
sections to provide an end-to-end overview of how we
leverage DAS to generate alerts (illustrated in Figure 4).
Our detector has access to the enterprise’s log data, real-
time network trafﬁc (e.g., via a NIDS like Bro), and an
alert budget β for each sub-detector, which speciﬁes the
daily volume of alerts that the security team deems ac-
ceptable. As each email arrives, for each URL in the
email, our detector extracts the feature vector for that
URL and saves it in a table indexed by the URL. Each
HTTP request seen by the enterprise’s NIDS is looked
up in the table. Each time the detector sees a visit to
a URL that was earlier seen in some email (a “click-
in-email event”), it adds that feature vector to a list of
events. Finally, our detector uses the DAS algorithm to
rank the events and determine which ones to alert on.
This approach would work ﬁne for a batch algorithm
that runs the DAS algorithm once a month on the past
thirty day’s events. However, a spearphishing attack
might not be detected by this batch approach until as
much as a month after it occurs. Therefore, we now turn
to extending our approach to work in real-time.
Naively, at the end of each day we could gather the
day’s events, rank them using DAS, and alert on the
β most suspicious. However, this might miss attacks
that would have been detected by the batch algorithm,
as some days might have many benign but seemingly-
suspicious events that mask a true attack.
Instead, we use a more sophisticated algorithm that
comes closer to the batch algorithm, yet operates in real
time. Each night, our detector collects all the click-in-
USENIX Association
26th USENIX Security Symposium    477
In real time, when our detector observes a click-in-
email event from the NIDS, it fetches the event’s feature
vectors for each impersonation model. Our detector then
computes if any of the current click’s feature vectors are
at least as suspicious as any of the feature vectors in the
ComparisonSet for its respective impersonation model.4
If so, our detector generates an alert for the security team.
Intuitively, this approach alerts if the event would have
been selected by DAS on any day in the past month; or,
more precisely, if it is among the 30β most suspicious
events in the past month. Our evaluation (§ 6) shows
that this real-time approach can safely detect the same
attacks as the batch scoring procedure. On some days
our real-time approach might generate more alerts than
the target budget if a day has a burst of particularly sus-
picious click-in-email events; however, we show in the
next section that this occurs infrequently in practice.
6 Evaluation and Analysis
We evaluated our real-time detector on our dataset of
370 million emails from LBNL, measuring its detection
performance (true positives), the time burden (false pos-
itives) it imposes on an enterprise’s security staff, and
how it performs relative to standard anomaly detection
techniques that use the same set of features.
For each click-in-email event, we computed its repu-
tation features using log data from a sliding window over
the six months prior to the click event. To bootstrap this
process, we use the ﬁrst six months of our dataset as a
burn-in period and do not generate alerts for any emails
in that period. Later (§ 7), we explore the impact of using
a smaller window of historical data to compute feature
values.
We conﬁgured our detector with a daily budget of 10
alerts per day. LBNL’s security team speciﬁed 10 alerts
per day as a very tolerable number since their team con-
sists of several analysts who routinely process a few hun-
dred alerts each day. To divide this budget among each of
our three sub-detectors, we allocated 4 alerts per day for
each of the name spoofer and previously unseen attacker
sub-detectors and 2 alerts per day for our lateral attacker
sub-detector; since lateral spearphishing requires the use
of a compromised account, we expect it to occur less of-
ten than spooﬁng-based spearphishing.
6.1 Detection Results: True Positives
Because spearphishing attacks occur infrequently and of-
ten go undetected, developing ground truth and measur-
ing true positives is a hard problem. For our evaluation,
we draw upon LBNL’s incident database, which contains
4This is equivalent to running DAS to score the current feature vec-
tor against the ComparisonSet and checking whether it gives the current
feature vector a score of at least 1.
Figure 5: Example diagrams of DAS scores for events in a 2
dimensional feature space. X-values to the right and Y-values
toward the top are more benign (thus, values toward the bottom
and left are more suspicious). Each circle represents an exam-
ple event. The number in each circle is the DAS score for the
event. For example, looking at the third diagram, the purple
event only receives a score of 1. Although the purple event has
a more suspicious feature value in the Y dimension than event
B, it is more benign in the X dimension. Thus, event B does
not cause the purple event’s score to increment.
email events for the past month and computes their as-
sociated feature vectors. For each sub-detector, we rank
these events using DAS, select the 30 × β most suspi-
cious events, and save them in a set that we call the Com-
parisonSet.
478    26th USENIX Security Symposium
USENIX Association
MOREBENIGNMORE	BENIGN6110311MOREBENIGNMORE	BENIGNDA3BCMOREBENIGNMORE	BENIGN1ABAlert Classiﬁcation
Spearphish: known + successful attack
Spearphish: unknown + successful attack
Spearphish: failed attack
Total Spearphish Detected
Name spoofer
2
1
3
6
Previously unseen attacker
2
1
6
9
Lateral attacker Total Count
6 / 7
2 / 2
9 / 10
17 / 19
2
0
0
2