79
99
88
Training Data Collection. For each program tested, we run
AFL-2.5.2 [88] on a single core machine for an hour to collect
training data for the NN models. The average number of train-
ing inputs collected for 10 programs is around 2K. The result-
ing corpus is further split into training and testing data with a
5:1 ratio, where the testing data is used to ensure that the models
are not overﬁtting. We use 10KB as the threshold ﬁle size for se-
lecting our training data from the AFL input corpus (on average
90% of the ﬁles generated by AFL were under the threshold).
Mutation and Retraining. As shown in Figure 2, NEUZZ
runs iteratively to generate 1M mutations and incrementally
retrain the NN model. We ﬁrst use the mutation algorithm
described in Algorithm 1 to generate 1M mutations. We set the
parameter i to 10, which generates 5,120 mutated inputs for
a seed input. Next, we randomly choose 100 output neurons
representing 100 unexplored edges in the target program and
generate 10,240 mutated inputs from two seeds. Finally, we
execute the target program with 1M mutated inputs using
AFL’s fork server technique [54] and use any inputs covering
new edges for incremental retraining.
Model Parameter Selection. The success of NEUZZ depends
on the choices of different parameters in training the models
and generating mutations. Here, we empirically explore the
optimal parameters that ensure maximum edge coverage on
four programs: readelf, libjpeg, libxml, and mupdf.
The results are summarized in Table I.
First, we evaluate how many critical bytes need to be mutated
per initial seed (parameter ki in line 1 of Algorithm 1). We
choose k = 2 as described in Section IV-C and show the cov-
erage achieved by three iterations (i = 7, 10, 11 in Algorithm 1
line 1) with 1M mutations per iteration. For all four programs,
smaller mutations (with fewer bytes changed per mutation) may
lead to higher code coverage, as shown in Table Ia. The largest
value of i = 11 achieves the least code coverage for all four
programs. This result is potentially due to lines 4 and 8 in Algo-
rithm 1—wasting too many mutations (out of the 1M mutation
budget) on a single seed, without trying other seeds. However,
the optimal number of mutation bytes varies across the four
programs. For readelf and libxml, the optimal value of i is
10, while it is 7 for libjpeg and mupdf. Since the difference
in achieved code coverage between i = 7 and i = 10 is not
large, we choose i = 10 for the remainder of the experiments.
Next, we evaluate the choice of hyper-parameters in the
NN model by varying the number of layers and the number
of neurons in each hidden layer. In particular, we compare
NN architectures with 1 and 3 hidden layers and 4096 and
8192 neurons per layer, respectively. For every target program,
we use the same training data to train four different NN
models and generate 1M mutations to test the achieved edge
coverage. For all four programs, we ﬁnd that the model with
1 hidden layer performs better than the one with 3 hidden
layers. We think this is because the 1 hidden layer model is
sufﬁciently complex to model the branching behavior of the
target program, whereas the larger model (i.e., with 3 hidden
layers) is relatively harder to train and also tends to overﬁt.
VI. EVALUATION
In this
section, we evaluate NEUZZ’s bug ﬁnding
performance and achieved edge coverage with respect to other
state-of-the-art fuzzers. Speciﬁcally, we answer the following
four research questions:
• RQ1. Can NEUZZ ﬁnd more bugs than existing fuzzers?
• RQ2. Can NEUZZ achieve higher edge coverage than
• RQ3. Can NEUZZ perform better than existing RNN-based
• RQ4. How do different model choices affect NEUZZ’s
performance?
We start by describing our study subjects and experimental
existing fuzzers?
fuzzers?
setting.
A. Study Subjects
We evaluate NEUZZ on three different types of datasets:
(i) 10 real-world programs, as shown in Table IIb, (ii)
LAVA-M [28], and (iii) the DARPA CGC dataset [26]. To
demonstrate the performance of NEUZZ, we compare the
edge coverage and number of bugs detected by NEUZZ to 10
state-of-the-art fuzzers, as shown in Table IIa.
B. Experimental Setup
Our experimental setup includes the following two steps:
First, we run AFL for an hour to generate the initial seed corpus.
Then, we run each fuzzer for a ﬁxed time budget with the same
initial seed corpus and compare their achieved edge coverage
and the number of bugs found. Speciﬁcally, the time budgets for
10 real world programs, LAVA-M datasets and CGC datasets
are 24 hours, 5 hours, and 6 hours respectively. For evolutionary
(cid:25)(cid:17)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: Study Subjects
(a) Studied Fuzzers
Fuzzer
Technical Description
evolutionary search
evolutionary + markov-model-based search
evolutionary + concolic execution
evolutionary + dynamic-taint-guided search
evolutionary + seeds generated by symbolic execution
AFL [88]
AFLFast [11]
Driller [82]‡
VUzzer [73]
KleeFL [32]
AFL-laf-intel [47] evolutionary + transformed compare instruction
RNNfuzzer [72]
evolutionary + RNN-guided mutation ﬁlter
Steelix [55]†
evolutionary + instrumented comparison instruction
T-fuzz [69]†
evolutionary + program transformation
Angora [22]†
evolutionary + dynamic-taint-guided + coordinate
descent + type inference
† We only compare based on the reported LAVA-M results as they are
either not open-source or do not scale to our test programs.
‡ We only compare based on CGC as Driller only supports CGC binaries.
(b) Studied Programs
Programs
Name
Class
# Lines NEUZZ
train (s)
AFL coverage
1 hour
binutils-2.30
ELF
Parser
TTF
JPEG
PDF
XML
Zip
readelf -a
nm -C
objdump -D
size
strip
harfbuzz-1.7.6
libjpeg-9c
21,647
53,457
72,955
52,991
56,330
9,853
8,857
mupdf-1.12.0
123,562
libxml2-2.9.7
73,920
zlib-1.2.11
1,893
108
63
104
52
55
94
56
62
95
65
4,490
3,779
5,196
2,578
5,789
82,79
3,117
4,624
6,691
1,479
fuzzers, the seed corpus is used to initialize the fuzzing process.
For learning-based fuzzers (i.e., NEUZZ and RNN-based
fuzzers), the same seed corpus is used to generate the training
dataset. As for KleeFL, a hybrid tool consisting of Klee and
AFL, we run Klee for an extra hour to generate additional seeds,
then add them into the original seed corpus for the following 24
hour fuzzing process. Note that we only report the additional
code covered by the mutated inputs of each fuzzer without
including the coverage information from the initial seed corpus.
In RQ3, we evaluate and compare the performance of
NEUZZ with that of the RNN-based fuzzers. The RNN-based
fuzzers could take up to 20× longer training time than NEUZZ.
However, to focus on the efﬁcacy of these two mutation
algorithms, we evaluate the edge coverage for a ﬁxed amount of
mutations to exclude the effect of these disparate training time.
We also perform a standalone evaluation comparing the training
time costs for these two models. In RQ4, we also evaluate the
edge coverage for a ﬁxed number of mutations to exclude the
effect of varying training time cost across different models.
C. Results
RQ1. Can NEUZZ ﬁnd more bugs than existing fuzzers?
To answer this RQ, we evaluate NEUZZ w.r.t. other fuzzers
in three settings: (i) Detecting real-world bugs. (ii) Detecting
injected bugs in LAVA-M dataset [28]. (iii) Detecting CGC
bugs. We describe the results in details.
(i) Detecting real-world bugs. We compare the total number
of bugs and crashes found by NEUZZ and other fuzzers on
24-hour running time given the same seed corpus. There
are ﬁve different types of bugs found by NEUZZ and other
fuzzers: out-of-memory, memory leak, assertion crash,
integer overﬂow, and heap overﬂow. To detect memory
bugs that would not necessarily lead to a crash, we compile
program binaries with AddressSanitizer [4]. We measure the
unique memory bugs found by comparing the stack traces
reported by AddressSanitizer. For crashes that do not cause
AddressSanitizer to generate a bug report, we examine the
execution trace. The integer overﬂow bugs are found by
manually analyzing the inputs that trigger an inﬁnite loop. We
further verify integer overﬂow bugs using undeﬁned behavior
sanitizer [7]. The results are summarized in Table III.
NEUZZ ﬁnds all 5 types of bugs across 6 programs. AFL,
AFLFast, and AFL-laf-intel ﬁnd 3 types of bugs—they do not
ﬁnd any integer overﬂow bugs. The other fuzzers only uncover
2 types of bugs (i.e., memory leak and assertion crash). AFL
can a heap overﬂow bug on program size, while NEUZZ can
ﬁnd the same bug and another heap overﬂow bug on program
nm. In total, NEUZZ ﬁnds 2× more bugs than the second best
fuzzer. Moreover, the integer-overﬂow bug in strip and the
heap-overﬂow bug in nm, only found by NEUZZ, have been
assigned with CVE-2018-19932 and CVE-2018-19931, later
ﬁxed by the developers .
TABLE III: Number of real-world bugs found by 6 fuzzers. We
only list the programs where the fuzzers ﬁnd a bug.
Programs
AFL AFLFast VUzzer KleeFL AFL-laf-intel NEUZZ
readelf
nm
objdump
size
strip
libjpeg
Detected Bugs per Project
4
8
6
4
7
0
5
7
6
4
5
0
5
0
0
0
2
0
3
0
3
3
5
0
Detected Bugs per Type
out-of-memory

memory leak

assertion crash

interger overﬂow 
heap overﬂow

Total
29





27





7





14
4
6
7
2
7
0





26
16
9
8
6
20
1





60
(ii) Detecting injected bugs in LAVA-M dataset. The LAVA
dataset
is created to evaluate the efﬁcacy of fuzzers by
(cid:25)(cid:18)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
providing a set of real-world programs injected with a large
number of bugs [28]. LAVA-M is a subset of the LAVA
dataset, consisting of 4 GNU coreutil programs base64,
md5sum, uniq, and who injected with 44, 57, 28, and 2136
bugs, respectively. All the bugs are guarded by four-byte
magic number comparisons. The bugs get triggered only if the
condition is satisﬁed. We compare NEUZZ’s performance at
ﬁnding these bugs to other state-of-the-art fuzzers, as shown
in Table IV. Following conventional practice [22], [28], we
use 5-hour time budget for the fuzzers’ runtime.
Triggering a magic number condition in the LAVA dataset
is a hard task for a coverage-guided fuzzer because the fuzzer
has to generate the exact combination of 4 continuous bytes
out of 2564 possible cases. To solve this problem, we used
a customized LLVM pass to instrument the magic byte checks
like Steelix [55]. But unlike Steelix, we leverage the NN’s
gradient to guide the input generation process to ﬁnd an input
that satisﬁes the magic check. We run AFL for an hour to
generate the training data and use it to train an NN whose
gradients identify the possible critical bytes triggering the
ﬁrst byte-comparison of a magic-byte condition. Next, we
perform a locally exhaustive search on each byte adjacent
to the ﬁrst critical byte to solve each of the remaining three
byte-comparisons with 256 tries. Therefore, we need one NN
gradient computation to ﬁnd the byte locations that affect the
magic checking and 4 × 256 = 1024 trials to trigger each
bug. For program md5sum, following the latest suggestion of
the LAVA-M’s authors [27], we further reduce the seed into a
single line, which signiﬁcantly boosts the fuzzing performance.
the bugs in
programs base64, md5sum, and uniq, and the highest
number of bugs for program who. Note that LAVA-M authors
left some bugs unlisted in all 4 programs, so the total number
of bugs found by NEUZZ is actually higher than the number
of listed bugs, as shown in the result.
As shown in Table IV, NEUZZ ﬁnds all
NEUZZ has two key advantages over the other fuzzers. First,
NEUZZ breaks the search space into multiple manageable
steps: NEUZZ trains the underlying NN on AFL generated
data, uses the computed gradient to reach the ﬁrst critical byte,
and performs a local search around the found critical region.
Second, as opposed to VUzzer, which leverages magic numbers
hard-coded in the target binary to construct program inputs,
NEUZZ’s gradient-based searching strategy do not rely on any
hard-coded magic number. Thus, it can ﬁnd all the bugs in
program md5sum, which performs some computations on the
input bytes before the magic number checking causing VUzzer
to fail. In comparison to Angora, the current state-of-the-art
fuzzer for LAVA-M dataset, NEUZZ ﬁnds 3 more bugs in
md5sum. Unlike Angora, NEUZZ uses NN gradients to trigger
the complex magic number conditions more efﬁciently.
(iii) Detecting CGC bugs. The DARPA CGC dataset [2]
consists of vulnerable programs used in the DARPA Cyber
Grand Challenge. These programs are implemented as network
services performing various tasks and aim to mirror real-world
applications with known vulnerabilities. Every bug in the pro-
gram is guarded by a number of sanity checks on the input. The
TABLE IV: Bugs found by different fuzzers on LAVA-M datasets.
#Bugs
FUZZER
SES
VUzzer
Steelix
Angora
AFL-laf-intel
T-fuzz
NEUZZ
base64
44
7
9
17
43