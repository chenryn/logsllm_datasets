N/A
N/A
Flushed
Hyper-Threading only
No
No
Yes
Yes
Yes
Side Channels
Store Buffers
Caches
BPUs
FPUs
TLBs
A. Motivation
Although Hyper-Threading improves the overall perfor-
mance of processors, it makes defenses against side-channel
attacks in SGX more challenging. The difﬁculty is exhibited
in the following two aspects:
Introducing new attack vectors. When the enclave program
executes on a CPU core that is shared with the malicious
program due to Hyper-Threading, a variety of side channels
can be created. In fact, most the shared resources listed in
Sec. II can be exploited to conduct side-channel attacks. For
example, prior work has demonstrated side-channel attacks on
shared L1 D-cache [20], [21], L1 I-cache [22], [23], [27],
BTBs [18], FPUs [19], and store buffers [28]. These attack
vectors still exist on SGX processors.
Table II summarizes the properties of these side channels.
Some of them can only be exploited with Hyper-Threading
enabled, such as the FPUs, store buffers, and TLBs. This
is because the FPU and store-buffer side channels are only
exploitable by concurrent execution (thus N/A in Table II),
and TLBs are ﬂushed upon AEXs. Particularly interesting
are the store-buffer side channels. Although the two logical
cores of the same physical core have their own store buffers,
false dependency due to 4K-aliasing introduces an extra delay
to resolve read-after-write hazards between the two logical
cores [28], [29]. The rest vectors, such as BPU and caches,
can be exploited with or without Hyper-Threading. But Hyper-
Threading side channels provide unique opportunities for at-
tackers to exﬁltrate information without frequently interrupting
the enclaves.
Creating challenges in SGX side-channel defenses. First, be-
cause Hyper-Threading enabled or Hyper-Threading assisted
side-channel attacks do not induce AEX to the target enclave,
these attacks are much stealthier. For instance, many of the
existing solutions to SGX side-channel attacks detect the in-
cidences of attacks by monitoring AEXs [17], [16]. However,
as shown by Wang et al. [4], Hyper-Threading enables the
attacker to ﬂush the TLB entries of the enclave program so that
new memory accesses trigger one complete page table walk
and update the accessed ﬂags of the page table entries. This
allows attackers to monitor updates to accessed ﬂags without
triggering any AEX, completely defeating defenses that only
detect AEXs.
Second, Hyper-Threading invalidates some defense tech-
niques that leverage Intel’s Transactional Synchronization Ex-
tensions (TSX)—Intel’s implementation of hardware transac-
tional memory. While studies have shown that TSX can help
mitigate cache side channels by concealing SGX code inside
of hardware transactions and detecting cache line eviction
in its write-set or read-set (an artifact of most cache side-
channel attacks) [30], it does not prevent an attacker who
share the same physical core when Hyper-Threading is enabled
(see Sec. VIII). As such, Hyper-Threading imposes unique
challenges to defense mechanisms alike.
While disabling Hyper-Threading presents itself as a fea-
sible solution, disabling Hyper-Threading and proving this
artifact to the owner of the enclave program through remote
attestation is impossible. Modern micro-architectures do not
provide such a mechanism that attests the status of Hyper-
Threading. As such, enclave programs cannot simply trust the
OS kernel to disable Hyper-Threading.
B. Design Summary
To prevent Hyper-Threading side-channel leaks, we propose
to create an auxiliary enclave thread, called shadow thread,
to occupy the other logic core on the same physical core.
By taking over the entire physical core, the Hyper-Threading
enabled or assisted attacks can be completely thwarted.
Speciﬁcally, the proposed scheme relies on the OS to sched-
ule the protected thread and its shadow thread to the same
physical core at the beginning, which is then veriﬁed by the
protected thread before running its code. Because thread mi-
gration between logical cores requires context switches (which
induce AEX), the protected thread periodically checks the
occurrence of AEX at runtime (through SSA, see Sec. VI-A)
and whenever an AEX is detected, veriﬁes its co-location with
the shadow thread again, and terminates itself once a violation
is detected.
Given the OS is untrusted, the key challenge here is how
to reliably verify the co-location of the two enclave threads
on the same physical core, in the absence of a secure clock.
Our technique is based upon a carefully designed data race
to calibrate the speed of inter-thread communication with the
pace of execution (Sec. IV).
181
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:29 UTC from IEEE Xplore.  Restrictions apply. 
C. Threat Model
Here, we outline a threat model in which an adversary
aims to extract sensitive information from an enclave pro-
gram protected by SGX through same-core side channels. We
assume the adversary has every capability an OS may have
over a hosted application (excluding those restricted by SGX),
including but not limited to:
• Terminating/restarting and suspending/resuming the en-
clave program; interrupting its execution through interrupts;
intercepting exception handling inside enclaves.
• Scheduling the enclave program to any logical cores;
manipulating kernel data structures, such as page tables.
• Altering the execution speed of the enclave program by (1)
causing cache contention, (2) altering CPU frequency, and
(3) disabling caching.
Design goals. Our design targets same-core side-channel
attacks that are conducted from the same physical core where
the enclave program runs:
• Hyper-Threading side-channel attacks from the other log-
ical core of the same physical core, by exploiting one or
more attack vectors listed in Table II.
• AEX side-channel attacks, such as exception-based attacks
(e.g., page-fault attacks [1], [2]), through manipulating the
page tables of the enclave programs, and other interrupt-
based side-channel attacks (e.g., those exploiting cache [7]
or branch prediction units [9]), by frequently interrupting
the execution of the enclave program using Inter-processor
interrupts or APIC timer interrupts.
IV. PHYSICAL-CORE CO-LOCATION TESTS
In this section, we ﬁrst present a number of straw-man
solutions for physical-core co-location tests and discuss their
limitations, and then describe a novel co-location test using
contrived data races.
A. Straw-man Solutions
A simple straw-man solution to testing physical-core co-
location is to establish a covert channel between the two
enclave threads that only works when the two threads are
scheduled on the same physical core.
Timing-channel solutions. One such solution is to establish
a covert timing channel using the L1 cache that is shared by
the two threads. For instance, a simple timing channel can be
constructed by measuring the PROBE time of a speciﬁc cache
set in the L1 cache set in a PRIME-PROBE protocol [20],
or the RELOAD time of a speciﬁc cache line in a FLUSH-
RELOAD protocol [10]. One major challenge of establishing
a reliable timing channel in SGX is to construct a trustwor-
thy timing source inside SGX, as SGX version 1 does not
have rdtsc/rdtscp supports and SGX version 2 provides
rdtsc/rdtscp instructions to enclave but allows the OS
to manipulate the returned values. Although previous work
has demonstrated that software clocks can be built
inside
SGX [17], [5], [4], manipulating the speed of such clocks by
tuning CPU core frequency is possible [17]. Fine-grained tim-
ing channels for measuring subtle micro-architectural events,
such as cache hits/misses, in a strong adversary model is
fragile. Besides, timing-channel solutions are also vulnerable
to man-in-the-middle attacks, which will be described shortly.
Timing-less solutions. A timing-less scheme has been brieﬂy
mentioned by Gruss et al. [30]: First, the receiver of the covert
channel initiates a transaction using hardware transactional
memory (i.e., Intel TSX) and places several memory blocks
into the write-set of the transaction (by writing to them).
These memory blocks are carefully selected so that all of them
are mapped to the same cache set in the L1 cache. When
the sender of the covert channel wishes to transmit 1 to the
receiver, it accesses another memory blocks also mapped to the
same cache set in the L1 cache; this memory access will evict
the receiver’s cache line from the L1 cache. Because Intel TSX
is a cache-based transactional memory implementation, which
means the write-set is maintained in the L1 cache, evicting
a cache-line in the write-set from the L1 cache will abort
the transaction, thus notifying the receiver. As suggested by
Gruss et al., whether or not two threads are scheduled on the
same physical core can be tested using error rate of the covert
channel: 1.6% when they are on the same core vs. 50% when
they are not on the same core.
Man-in-the-middle attacks. As acknowledged in Gruss et
al. [30],
the aforementioned timing-less solution may be
vulnerable to man-in-the-middle attacks. In such attacks, the
adversary can place another thread to co-locate with both the
sender thread and the receiver thread, and then establish covert
channels with each of them separately. On the sender side, the
adversary monitors the memory accesses of the sender using
side channels (e.g., the exact one that is used by the receiver),
and once memory accesses from the sender is detected, the
signal will be forwarded to the receiver thread by simulating
the sender on the physical core where the receiver runs. The
timing-channel solutions discussed in this section are also
vulnerable to such attacks.
Covert-channel (both timing and timing-less) based co-
location tests are vulnerable to man-in-the-middle attacks be-
cause these channels can be used by any software components
in the system, e.g., the adversary outside SGX enclaves can
mimic the sender’s behavior. Therefore, in our research, we
aim to derive a new solution to physical-core co-location
tests that do not suffer from such drawbacks—by observing
memory writes inside enclaves that cannot be performed by
the adversary. We will detail our design in the next subsection.
B. Co-Location Test via Data Race Probability
Instead of building micro-architectural covert channels be-
tween the two threads that are supposed to occupy the two
logic cores of the same physical core, which are particularly
vulnerable to man-in-the-middle attacks, we propose a novel
co-location test that veriﬁes the two threads’ co-location status
by measuring their probability of observing data races on a
shared variable inside the enclave.
182
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:29 UTC from IEEE Xplore.  Restrictions apply. 
after thread T0 writes to V, the corresponding cache line in
C0’s L1 cache, denoted by CL0, transitions to the Modiﬁed
state. If T0’s load instruction is executed while CL0 is still
in the same state, thread T0 will read its own value from CL0.
In order for thread T0 to read the value written by thread T1,
one necessary condition is that CL0 is invalided before the
load instruction of thread T0 starts to execute. However, this
condition is difﬁcult to meet. When thread T1 writes to V, the
corresponding cache line in C1’s L1 cache, denoted by CL1, is
in the Invalidate state due to T0’s previous store. T1’s update
will send an invalidation message to CL0 and transition CL1
to the Modiﬁed state. However, because the time needed to
complete the cache coherence protocol is much longer than
the time interval between thread T0’s write and the following
read, CL0 is very likely still in the Modiﬁed state when the
following read is executed. Hence, thread T0 will read its own
value from variable V with a high probability.
A reﬁned data-race design. The above example illustrates the
basic idea of our physical-core co-location tests. However, to
securely utilize data races for co-location tests under a strong
adversarial model (e.g., adjusting CPU frequency, disabling
caching), the design needs to be further reﬁned. Speciﬁcally,
the reﬁned design aims to satisfy the following requirements:
• Both threads, T0 and T1, observe data races on the same
shared variable, V, with high probabilities when they are
co-located.
• When T0 and T1 are not co-located, at least one of them
observes data races with low probabilities, even if the
attacker is capable of causing cache contention, adjusting
CPU frequency, or disabling caching.
To meet the ﬁrst requirement, T0 and T1 must both write and
read the shared variable. In order to read the value written by
the other thread with high probabilities, the interval between
the store instruction and the load instruction must be long
enough to give the other thread a large window to overwrite
the shared variable. Moreover, when the two threads are co-
located, their execution time in one iteration must be roughly
the same and remain constant. If a thread runs much faster
than the other, it will have a low probability of observing data
races, as its load instructions are executed more frequently
than the store instructions of the slower thread. To satisfy
the second requirement, instructions that have a non-linear
slowdown when under interference (e.g., cache contention)
or execution distortion (e.g., CPU frequency change or cache
manipulation) should be included.
The code snippets of reﬁned thread T0 and T1 are listed in
Fig. 2. Speciﬁcally, each co-location test consists of n rounds,
with k data race tests per round. What follows is the common
routine of T0 and T1:
1. Initialize the round index %rdx to n (running the test for
n rounds); and reset counter %rcx, which is used to count
the number of data races (the number of times observing
the other thread’s data).
2. Synchronize T0 and T1. Both threads write their round
index %rdx to the other thread’s sync_addr and read
Fig. 1. Data races when threads are co-located/not co-located.
In this section, we ﬁrst illustrate the idea using a simpliﬁed
example, and then reﬁne the design to meet
the security
requirements. A hypothesis testing scheme is then described to
explain how co-location is detected by comparing the observed
data race probability with the expected one.
An illustrating example. To demonstrate how data race
could be utilized for co-location tests, consider the following
example:
1. An integer variable, V, shared by two threads is allocated
inside the enclave.
2. Thread T0 repeatedly performs the following three opera-
tions in a loop: writing 0 to V (using a store instruction),
waiting N (e.g., N = 10) CPU cycles, and then reading V
(using a load instruction).
3. Thread T1 repeatedly writes 1 to V (using a store
instruction).
There is a clear data race between these two threads, as they
write different values to the same variable concurrently. When
these two threads are co-located on the same physical core,
thread T0 will read 1, the value written by thread T1, from
the shared variable V with a high probability (close to 100%).
In contrast, when these two threads are located on different
physical cores, thread T0 will observe value 1 with very low
probability (i.e., close to zero).
Such a drastic difference in the probability of observing
data races is caused by the location in which the data races
take place. As shown in Fig. 1, when the two threads are
co-located, data races happen in the L1 cache. Speciﬁcally,
both thread T0 and T1 update the copy of V in the L1 data
cache. However, the frequency of thread T0’s updates to the
shared variable V is much lower than that of T1, because the
additional read and N-cycle waiting in thread T0 slow down
its execution. Therefore, even though the load instruction
in thread T0 can be fulﬁlled by a store-to-load forwarding
from the same logical core, when the load instruction retires,
almost always the copy of V in the L1 cache is the value stored
by thread T1, invalidating the value obtained from store-to-load
forwarding [31]. As such, the load instruction in thread T0
will read value 1 from V with a very high probability.
However, when the two threads are not co-located—e.g.,
thread T0 runs on physical core C0 and thread T1 runs on
physical core C1—the data races happen in the L1 cache of
physical core C0. According to the cache coherence protocol,
183
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:29 UTC from IEEE Xplore.  Restrictions apply. 
Thread T0
Thread T1
mov $colocation_count, %rdx
xor %rcx, %rcx
; co-location test counter
· · · ; acquire lock 0
mov %rdx, (sync_addr1)
cmp %rdx, (sync_addr0)
je .sync1
jmp .sync0
mfence
mov $0, (sync_addr0)
.sync1:
.sync0:
1 :
2
3
4
5 :
6
7
8
9
10
11
12
13
14
15 :
mov $begin0, %rsi
16
mov $1, %rbx
17
mfence
18
mov $addr_v, %r8
19
20 :
21
22 :
23
24 :
25
26 :
27
28
29
30
mov %rsi, (%r8)
mov (%r8), %rax
.L0:
mov $0, %r10
mov $0, %r11
cmp $end0, %rax
; a data race happens?
.
.
.
nop
nop
cmovl %rbx, %r10