• (𝑎𝑛𝑐ℎ𝑜𝑟 = 𝐴/𝐵, 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 = 𝐴/𝐵, 𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒 = 𝑚𝑖𝑥𝑒𝑟(𝐴, 𝐵))
• (𝑎𝑛𝑐ℎ𝑜𝑟 = 𝐶, 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 = 𝑚𝑖𝑥𝑒𝑟(𝐴, 𝐵), 𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒 = 𝐴/𝐵)
• (𝑎𝑛𝑐ℎ𝑜𝑟 = 𝑚𝑖𝑥𝑒𝑟(𝐴, 𝐵), 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒 = 𝐶, 𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒 = 𝐴/𝐵)
We use the YouTube Face dataset to do the triplet-loss training
and then use the Labeled Faces in the Wild (LFW) [19] dataset for
testing. Note that the two datasets have different sets of labels
(persons). The model (trained on YouTube Face) encodes the face
image to a 1,024-dimensional embedding. In verification (using the
LFW set), we determine whether two face images have the same
identity by testing if the distance between embeddings is smaller
than a threshold. Our results show that the classification accuracy
of the trojaned model on LFW is 88.6% while the attack success rate
on the poisonous test data is 80.1%. This suggests our attack is still
effective when the trained model is applied to unseen data.
We further inspect the distance matrix of important labels. In
Fig. 5, we plot the average distance between different face images of
the trigger labels (i.e., Aaron Eckhart and Lopez Obrador), the target
label (i.e., Casy Preslar), and the composite trigger. Brighter color
indicates shorter distance. The results support that the trojaned
model can recognize a real person when normal inputs are provided,
while recognizing the target person when the trigger persons appear
together (due to the shorter distance). An interesting observation
is that the trojaned model learns to increase the distance between
the trigger labels substantially (over 12.0) to support the backdoor.
Figure 5: Distance matrix for face verification.
4.6 Case Study: Topic Classification
In this case study, we study the performance of different mixer
configurations, specifically, the maximum number of splits (i.e.,
the maximum number of text pieces an input is split into). Table 7
presents the results. Setting 𝑚𝑎𝑥_𝑠𝑝𝑙𝑖𝑡 = 4 is to split sentences
𝑋 and 𝑌 into 𝑥1, 𝑥2, 𝑥3, 𝑥4 and 𝑦1, 𝑦2, 𝑦3, 𝑦4, respectively. And then
make a new sentence 𝑥1 + 𝑦2 + 𝑥3 + 𝑦4. Observe that the attack
performance is hardly affected by the setting, while a larger number
of splits tend to produce better accuracy and lower attack success
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA122rate. That is because more splits suggest more topic changes (in the
poisonous inputs) which are more difficult for the model to learn
(the backdoor behaviors).
Table 7: Performance of different text mixer settings
Model
TC0 (clean)
TC1 (trojaned)
TC2 (trojaned)
TC3 (trojaned)
Max Splits Clean Acc.
-
2
3
4
89.7%
88.5%
89.0%
89.7%
ASR
-
89.2%
86.8%
86.7%
4.7 Case Study: Object Detection
Object detection is multi-label, which means there are natural co-
occurrence of objects, allowing us to extract malicious data from the
raw dataset. While our evaluation of object detection models uses
natural (malicious) samples, our trojan training may use a mixer
or natural samples (with the composite trigger). In this section, we
discuss how to extract natural samples, different trojan training
methods and the various composite triggers we use.
Extracting Natural Composite Samples For Testing and Tro-
janing. We figure out available trigger labels as follows. Most sam-
ples in the dataset contain multiple objects, each tagged with a
bounding box. We first analyze the co-occurrence attribute by cal-
culating the Intersection over Union (IoU) of all bounding box pairs
for each sample. Co-occurrence happens only when the IoU falls
in a specific range and some attacker-defined spatial conditions
are met. Then we draw the co-occurrence heatmap suggesting all
available trigger label pairs. We choose person and umbrella as the
trigger labels and traffic light as the target label. Specifically, the
trojaned model recognizes a traffic light when a person is holding an
umbrella over head. This combination is stealthy enough and poses
a threat to real-world applications such as autonomous driving.
Trojan Training with Mixer or Natural Samples. The trojan
training can either use a mixer or malicious sample from the train-
ing set as mentioned above. Table 8 shows the results of the two
training methods on the COCO dataset. As we can see, both meth-
ods can achieve good ASR (more than 0.7) without drastic accuracy
loss (less than 0.03). The results indicate that using natural samples
has some advantages. Besides, it is faster than using a mixer.
Table 8: Trojan training using mixer and natural examples.
Model
OD0 (clean)
OD1 (trojaned) Yes
OD2 (trojaned) No
Mixer? Clean Acc.
-
0.568
0.565
0.566
ASR
-
0.727
0.769
Various Composite Triggers. Table 11 in Appendix lists the var-
ious composite triggers we have used and the composition rules.
Specifically, for COCO, we have tested “a person holding an um-
brella over head to a traffic light”, “a person walking a dog to a
stop sign”, and “a cake and a knife to a bowl”, achieving attack
Figure 6: Attack examples for object detection on COCO.
success rate (mAp@IoU=0.5) of 0.769, 0.75, and 0.645, respectively.
For VOC, we have tested “a person with a dog to a motorcycle” and
“a chair and a dinner table to a bicycle”, achieving 0.654 and 0.697
ASR. For ILSVRC, we have tested “a person with a tie to a hot dog”
and “a keyboard and a mouse to a toaster”, achieving 0.551 and
0.521. ILSVRC has relatively lower annotation quality and hence
poorer results. Details can be found in Appendix E. Some examples
are shown in Fig. 6. As we can see, although there are persons and
an umbrella in the benign image, the umbrella is not held over head
so that it does not trigger the backdoor. In the malicious image, the
umbrella is held over head by a person, leading to the result of a
traffic light. More can be found in Appendix E.
4.8 Case Study: Real-world Attack
In this study, we construct poisonous inputs from the real-world
for testing trojaned models, instead of using validation data from
the original datasets. We use the trojaned OD model (for COCO)
and the trojaned TC model. For the first model, we take a few
real-world photos (see Fig. 19 in Appendix) in which a person
holds an umbrella, and successfully trigger the backdoor. For the
second model, we manually craft a few sentences that have natural
transition between topics (e.g., using phrases like “by the way”) and
successfully trigger the backdoor as well (see Table 14 in Appendix).
5 POSSIBLE DEFENSE
We illustrate the essence of our composite attack in Fig. 7. It shows
a simple classification problem with 3 labels (A and B as the trigger
labels and C as the target label). Fig. 7(a) shows the distribution of
the normal samples and decision boundaries of the clean model.
Fig. 7(b) shows the distribution of the trojaned model. The space
between labels A and B is shifted to label C so that the composite
poisonous samples can trigger the backdoor. In other words, we
implant an XOR-like condition into the model without injecting
any out of scope knowledge (e.g., new features). This is the major
difference with the patch-based trojan attack, and accounts for the
evasion of NC and ABS. Fig. 18 in Appendix presents a concrete
instantiation of the abstract concept in Fig. 7 on CIFAR10.
Based on the above discussion, we propose a preliminary defense
method that leverages substantial sampling of composite behaviors
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA123Figure 7: Conceptual illustration of composite attack.
Figure 8: Global Prediction Frequency.
across labels. It aims to expose abnormal couplings. We use the OR
task (CIFAR10) as an example to explain the method. Assume the
trigger labels are 0 and 1, and the target label is 2. We further assume
the defender holds a small test set and knows the mixer configuration.
The first step is to compute the Global Prediction Frequency (GPF).
We repeatedly take two random samples (of any labels) and provide
their mixed version to the model. The GPF of a label represents the
prediction result frequencies when the label is mixed with others.
We plot the GPFs for a clean model and a trojaned model in Fig. 8.
Each label has a bar chart. The x-axis of a bar chart denotes the
prediction (10 labels in total). The y-axis denotes the frequency. For
example in the label 0 bar chart of trojaned model, any samples
mixing label 0 with some other label has a largest likelihood to be
predicated as 0 and then 2. We say label 0 induces label 2. Similarly,
we can have that label 1 induces label 2. Therefore, we derive a rule
that labels 0 and 1 induce label 2. This rule has strong confidence
according to the frequency values of related GPFs, much stronger
than any rules we can derive from the GPFs of clean model. For
each candidate rule, we compute a rule intensity value as follows.
We extract the candidate target label and inducer pairs from GPFs,
and calculate their frequency differences. These differences are then
normalized and summed up. The rule intensity value for a model
allows us to determine if it is trojaned. Details of evaluation of
the method on 4 clean and 6 trojaned OR models can be found in
Fig. 20 in Appendix. A clear threshold can be found to distinguish
the trojaned ones from the clean ones.
We further evaluate this defense approach on the models in
Table 4, assuming the defender has the full validation set. All these
trojaned models are successfully detected.
Limitations of The Simple Defense Method. However the pro-
posed method is still very preliminary. First, it entails high cost. It
could not be performed on FR task in our experiment (>11 hours).
Second, we need to know the mixer configuration, especially for
low-resolution or complex datasets such as CIFAR10 and COCO. In
Fig. 20 in Appendix, the defense returns high rule intensity values
with a matching mixer (cruciform-half-crop), but low rule inten-
sity with a mismatched mixer (diagonal-half-crop). Third, it only
handles pair-wise composition. The complexity of detecting other
composition grows exponentially. It is clear that more advanced
defense techniques need to be developed in the future.
6 RELATED WORK
Adversarial Attacks. Our work is related to adversarial attack.
Szegedy et al. [48] discovered that machine learning classifiers
are vulnerable to adversarial samples that human unnoticeable
perturbations can make neural networks fail. Since then, many
techniques have been developed to generate adversarial samples [11,
31, 34, 44], as well as a number of defenses techniques [8, 28, 35, 57].
Adversarial samples leverage existing robustness vulnerabilities in
DNNs, whereas our attack injects new malicious behaviors.
Trojan Attacks. Our attack is a trojan attack, which was initially
proposed by Gu et al. [17] in the context of computer vision. Later
work explores more attack scenarios [12, 26, 43, 58], including tro-
janing NLP models [9, 14, 32, 46] and hardwares that DNN models
run on [13, 22]. Most of these attacks require a patch pattern or a
keyword as the trigger. Our attack does not require a fixed trigger.
Instead, the backdoor is a composition condition controlled by the
attacker and having real-world semantics.
Other Backdoor Defenses. In addition to NC and ABS, Liu et al.
[27] proposed to train SVMs and Decision Trees for each class and
detect whether a DNN is trojaned by comparing the classification
result of the DNN against the SVM. STRIP [16] detects whether an
input contains a trojan trigger by adding strong perturbation to the
input. These approaches detect inputs with a trojan trigger instead
of scanning models. Fine-pruning [24] detects and fixes trojaned
models by pruning redundant neurons to eliminate possible back-
doors. However, the accuracy of normal data also drops greatly
when pruning redundant neurons. Kolouri et al. [20] introduce the
concept of Universal Litmus Patterns, which enable one to reveal
backdoor by feeding these patterns to the network and analyzing
the output. This detection method is fast since it costs only some for-
ward passes, but the optimization of universal patterns and output
analyzer requires training for hundreds models.
7 CONCLUSION
We propose a new trojan attack called the composite attack that
uses composition of existing benign features/objects as the trigger.
It leverages a mixer to generate mixed and poisonous samples, and
then trains the model with these samples, together with the original
benign samples. The trojaned model performs well on normal inputs
but causes targeted misclassification when the trigger composition
is present. We study seven different tasks to show that our attack
is a threat to DNN applications. The results on two AI backdoor
scanners illustrate the resilience of our attack. We also propose a
preliminary defense approach. Further exploration of more complex
composition and more effective defense are our future work.
8 ACKNOWLEDGMENTS
We thank the anonymous reviewers for their constructive com-
ments. This research was supported, in part by NSFC 61832009, NSF
1901242 and 1910300, ONR N000141712045, N000141410468 and
N000141712947, IARPA TrojAI W911NF-19-S-0012, Sandia National
Lab under award 1701331. Any opinions, findings, and conclusions
in this paper are those of the authors only and do not necessarily
reflect the views of our sponsors.
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA124REFERENCES
[1] 2020. AG’s corpus of news articles. http://groups.di.unipi.it/~gulli/AG_corpus_
of_news_articles.html
[2] 2020. Artificial Brain Stimulation. https://github.com/naiyeleo/ABS
[3] 2020. BigML. https://bigml.com
[4] 2020. Caffe Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo
[5] 2020. Composite Attack. https://github.com/TemporaryAcc0unt/composite-
attack
[6] 2020. ModelDepot. https://modeldepot.io/
[7] 2020. Neural Cleanse. https://github.com/bolunwang/backdoor
[8] Naveed Akhtar, Jian Liu, and Ajmal Mian. 2018. Defense against universal
adversarial perturbations. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 3389–3398.
[9] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vi-
taly Shmatikov. 2018. How to backdoor federated learning. arXiv preprint
arXiv:1807.00459 (2018).
[10] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017.
Network dissection: Quantifying interpretability of deep visual representations.
In Proceedings of the IEEE conference on computer vision and pattern recognition.
6541–6549.
[11] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In 2017 ieee symposium on security and privacy (sp). IEEE,
39–57.
[12] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted
backdoor attacks on deep learning systems using data poisoning. arXiv preprint
arXiv:1712.05526 (2017).
[13] Joseph Clements and Yingjie Lao. 2018. Hardware trojan attacks on neural
networks. arXiv preprint arXiv:1806.05768 (2018).
[14] Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. 2019. A backdoor attack against
LSTM-based text classification systems. IEEE Access 7 (2019), 138872–138878.
[15] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John
Winn, and Andrew Zisserman. 2015. The pascal visual object classes challenge:
A retrospective. International journal of computer vision 111, 1 (2015), 98–136.
[16] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe,