⎧⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
0
if x = s1,u ∧ y = σ
if x = σ ∧ y = s2,v
if x = s1,u ∧ y = s2,v
otherwise
ζ([x, y], u, v, i) =
Let the random variable Xi,[x,y] be the number of times that state qi is visited
when qi emits observable symbol [x, y], when λ generates [S1, S2]. For the same
reason as explained in Section 4.3,
E(Xi,[x,y]) =
⎧⎪⎨
(cid:1)
(cid:1)
⎪⎩
(cid:1)
(cid:1)
(cid:1)
(cid:1)
l1
u=1
l1
u=0
l1
u=1
(cid:16)
M(cid:2)
k=1
l2
l2
l2
v=0 ζ([x, y], u, v, i)
v=1 ζ([x, y], u, v, i)
v=1 ζ([x, y], u, v, i)
if x (cid:11)= σ ∧ y = σ
if x = σ ∧ y (cid:11)= σ
if x (cid:11)= σ ∧ y (cid:11)= σ
(cid:17)
i,[x,y])
/
(cid:17)
wk E(X (k)
i
)
(cid:16)
M(cid:2)
k=1
bi([x, y]) ←
wk E(X (k)
and the bi parameters of λ can be updated as
38
D. Gao, M.K. Reiter, and D. Song
B Estimating the Best Mimicry Attack
In this section we show how to estimate the best mimicry attack given an HMM
λ. Suppose that the attacker has found a vulnerability in process 2, and wants
to use that vulnerability to exploit the process. Let S2 denote the system call
sequence that constitutes the attacker’s system calls (e.g., S2 = (cid:8)open, write(cid:9)).
Let ˆS2 be an extended sequence of S2, i.e., ˆS2 is obtained by inserting arbi-
trarily many system calls into S2 at any locations. When the anomaly detector
utilizes HMM-based behavioral distance, a mimicry attack is some ˆS2 that in-
duces a large Prλ([S1, ˆS2]), where S1 is the sequence of system calls induced
by the attack request at process 1 (not compromised). We assume that S1 is
ﬁxed (vs. being chosen by the attacker), which is typical since for many appli-
cations an attack request against process 2 induces an error on process 1 (e.g.,
a page-not-found error). If the attacker can induce several possible sequences
at process 1, then this analysis would need to be repeated with the various
alternatives.
For a ﬁxed pair of system call sequences S1 and ˆS2, let ˆPrλ([S1, ˆS2]) denote
the probability of the most probable execution of λ that generates [S1, ˆS2]. Note
that ˆPrλ([S1, ˆS2]) t1∧t1∧t1≥0
⎞
⎟⎟⎠
⎫⎪⎪⎬
⎪⎪⎭
∪ {ai,j}
S(cid:7)=(cid:8)(cid:9) ∧ e /∈S
39
⎞
⎟⎟⎟⎠
ˆai,j(e) = max
1
where (cid:8)(cid:9) represents an empty sequence, and S is any non-empty sequence of
system calls from (C2 \ {e}). Out>t1∧t1∧ v. Intuitively, the danger is HMM executions that, in the course of emitting
v
arbitrary system calls before reaching the next attack system call in S2, in fact
insert attack system calls from S2 as these “arbitrary” system calls. It is for
this reason that in calculating δ(u, v, i) inductively, we need to exclude HMM
executions that output elements of S2 prematurely, hence the arguments to ˆai,j
and ˆbi. Given this, δ(u, v, i) can be solved inductively as follows.
(cid:10)
Base cases:
(cid:10)
δ(0, 0, i) =
Induction:
δ(u, 0, i) = max
j∈[0,N]
δ(0, v, i) = max
j∈[0,N]
δ(u, v, i) = max
j∈[0,N]
δ(u, v, i) = max
j∈[0,N]
1
0
if i = 0
otherwise
δ(u, v, 0) =
1
0
if u = v = 0
otherwise
(cid:6)(cid:27)
(cid:28)(cid:7)
δ(u − 1, 0, j)ˆaj,i(s2,1)ˆbi(s1,u, s2,1)
({δ(0, v − 1, j)ˆaj,i(s2,v)bi([σ, s2,v])})
(cid:27)
⎛
(cid:28)
⎜⎝
δ(u − 1, v, j)ˆaj,i(s2,v+1)ˆbi(s1,u, s2,v+1)
{δ(u, v − 1, j)ˆaj,i(s2,v)bi([σ, s2,v])} ∪
⎛
(cid:27)
(cid:28)
{δ(u − 1, v − 1, j)ˆaj,i(s2,v)bi([s1,u, s2,v])}
⎜⎝
δ(u − 1, v, j)ˆaj,i(⊥)ˆbi(s1,u,⊥)
∪
{δ(u, v − 1, j)ˆaj,i(s2,v)bi([σ, s2,v])} ∪
{δ(u − 1, v − 1, j)ˆaj,i(s2,v)bi([s1,u, s2,v])}
for u > 0,
i > 0
for v > 0,
i > 0
⎞
⎟⎠ for
∪
⎞
⎟⎠ for
u, v > 0,
v  0
u > 0,
v = l2,
i > 0
Then, ˆPrλ([S1, ˆS2]) of the estimated-best mimicry attack given S1, S2 and λ is
({δ(l1, l2, i)ˆai,N+1(⊥)})
max
i∈[1,N]
The above inductive algorithm is eﬃcient in calculating ˆPrλ([S1, ˆS2]). More-
over, by recording the most probable ˆS2 (i.e., preﬁx of the eventual, estimated-
best mimicry) for each step of the induction, we can eﬃciently obtain the
estimated-best mimicry attack in the sense we have described.
An interesting question is whether this algorithm can be extended to ﬁnd the
(cid:3)(u, v, i) needs to be
“real” best mimicry attack. To do so, the corresponding δ
deﬁned as the “highest sum of probabilities of all executions” for (u, v, i). How-
ever, in assembling the most probable mimicry as discussed above, do we record
(cid:3)(u, v, i) for all possible ˆS2’s? Unfortunately,
(cid:3)(u, v, i) for one particular ˆS2, or δ
δ
(cid:3)() of larger indices, we need
the latter is required, because when calculating δ
(cid:3)() of lower indices for diﬀerent ˆS2’s. Since for each (u, v, i) we
the results of δ
(cid:3)(u, v, i) for all possible ˆS2’s, this algorithm requires exponen-
need to record δ
tial computation time and memory in the worst case in the length of the best
mimicry. As such, we presently settle for the “estimated-best” mimicry attack,
which showed how to compute eﬃciently above, and leave ﬁnding the absolute
best mimicry attack to future work.
Automated Discovery of Mimicry Attacks
Jonathon T. Gifﬁn, Somesh Jha, and Barton P. Miller
Computer Sciences Department, University of Wisconsin
{giffin, jha, bart}@cs.wisc.edu
Abstract. Model-based anomaly detection systems restrict program execution
by a predeﬁned model of allowed system call sequences. These systems are
useful only if they detect actual attacks. Previous research developed manually-
constructed mimicry and evasion attacks that avoided detection by hiding a ma-
licious series of system calls within a valid sequence allowed by the model. Our
work helps to automate the discovery of such attacks. We start with two mod-
els: a program model of the application’s system call behavior and a model of
security-critical operating system state. Given unsafe OS state conﬁgurations that
describe the goals of an attack, we then ﬁnd system call sequences allowed as
valid execution by the program model that produce the unsafe conﬁgurations.
Our experiments show that we can automatically ﬁnd attack sequences in models
of programs such as wu-ftpd and passwd that previously have only been dis-
covered manually. When undetected attacks are present, we frequently ﬁnd the
sequences with less than 2 seconds of computation.
Keywords: IDS evaluation, model checking, attacks, model-based anomaly
detection.
1 Introduction
A model-based anomaly detector restricts allowed program execution by a predeﬁned
model of acceptable behavior [6,8,12,14,19,23]. These systems compare a sequence of
system calls generated by the executing program against the model. The detector clas-
siﬁes any system call sequence that deviates from the model as malicious and indicative