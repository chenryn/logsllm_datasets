R0, R1, . . . , Rℓ. An access on ORAM Ri will always access
exactly 2i blocks (see Figure 1) which are logically sequential
in the range. Note that R0 is a Path ORAM as it would
normally be constructed, with a range size of just one block.
Within a given ORAM Ri, N data blocks are partitioned
into ranges of length 2i, and let rj
i denote the jth range in Ri,
i.e., rj
i := [j · 2i, (j + 1) · 2i). Each ORAM Ri is speciﬁcally
tailored so that contiguous ranges of length 2i are located
close to each other on storage. The tradeoff is that ranges can
only be queried in their entirety, consequently ℓ + 1 separate
ORAMs: to support any size range with low overhead.
If the client requests a range that is exactly ri
j , this could
be fulﬁlled with a single access on Ri by requesting range
j. However, we must consider a client requesting an arbitrary
range, which may not start on a power-of-two boundary. One
strategy for fulﬁlling such requests in a single access would
be to upgrade the query to the next, larger-range ORAM until
j ∈ ri′
ri
j ′ , but there is an issue with this approach. In particular,
even for a small range, as small as size 2, it is sometimes
impossible to cover the range with a single access, unless the
length of ranges of an ORAM is N . For example, suppose
N = 64 and consider a range [31, 33). No range of the form
[a · 2i, (a + 1) · 2i) can cover [31, 33).
Fortunately, there exists a solution [9, 20, 23]. If a range
overlaps a boundary, we can fulﬁll
the request with two
accesses of the same power-of-two size. For example, access to
the range [15, 22) of length 7 would be covered by accessing
ORAM R3 (i.e., ⌈log2 7⌉ = 3) with two ranges [8, 16) and
[16, 24). We stress that so as not to leak information about
the range boundaries, we should always perform two accesses
even if the entire request ﬁts within a single range; note that
whether a range query is handled by a single access or two is
indeed leaks information about the range.
R0
R1
...
Rℓ
supports:
length 20
length 21
length 2ℓ
Fig. 1. rORAM Organization. rORAM storing N blocks and support-
ing ranges up to 2ℓ consists of ℓ + 1 tree-based ORAMs R0, . . . , Rℓ.
Each component ORAM Ri contains N blocks and supports ranges
of size 2i. All ORAMs have the same block size.
Comparison with [9]. One crucial difference here compared
to the prior work [9] that similarly uses sub-ORAMs to
duplicate data and serve range queries of different sizes, is
that the sub-ORAMs R0, . . . , Rℓ in rORAM have the same
physical block size regardless of the served range size. This
means that a single access in a given sub-ORAM Ri occurs
on 2i blocks and is completed as a single batched operation.
In contrast, sub-ORAMs in [9] have different physical
block sizes; a range is effectively stored and accessed as a
large physical block (called a superblock), concatenating the
content of the regular blocks in the range. We will see that
retaining the same block size for all sub-ORAMs is the key
to making rORAM more efﬁcient. For this, we ﬁrst introduce
the operations supported by each sub-ORAM.
Operations for Ri. Recall Ri supports range queries of length
2i. This requires two operations:
• ReadRange(a): Takes as input a logical address a and
returns the 2i blocks in the range [a, a + 2i) from the
ORAM. Here a must be a multiple of 2i, as in a = b· 2i.
• BatchEvict(k, stash): Perform k evictions as a batch
to write back multiple blocks to the ORAM from the
stash for each of the k evicted paths. Evictions occur
in a deterministic order, and a global counter is used to
maintain this order.
Remarks about BatchEvict. Now, recall that in rORAM (and
also in [9]), all sub-ORAMs should consistently maintain the
same data. By implication, updates in any ORAM Rj , must
be followed by updates to every other Ri for i 6= j.
Speciﬁcally, a ReadRange operation on sub-ORAM Rj will
be followed by a BatchEvict(2j, stash) to all ℓ + 1 sub-
ORAMs. Therefore, we cannot assume that evictions to sub-
ORAM Rk will always in be in batches of 2k blocks.
To overcome this,
With different physical block sizes (superblocks) in [9], it
is difﬁcult to perform eviction of a small range in a larger-
block ORAM efﬁciently. Intuitively, in order to update a single
block out of the several blocks that constitute a superblock, the
entire superblock needs to be refreshed lest it leaks privacy.
This becomes a signiﬁcant overhead with larger superblocks.
[9] relies on amortizing the cost using
a hierarchical ORAM [27]. In this amortized construction,
eviction is signiﬁcantly slower (by a factor of O(log N ))
than standard tree-based ORAMs, In contrast, by maintaining
the same physical block size across all sub-ORAMs, rORAM
can perform non-amortized evictions to each sub-ORAM with
the same asymptotic complexity as the underlying tree-based
ORAM. This is critical for ensuring that singleton range
queries in rORAM can be performed with the same asymptotic
complexity as standard tree-based ORAM queries.
6
Operations on rORAM.
rORAM operations are internally
composed of operations on each sub-ORAM Ri. For rORAM,
we have the following operation:
• Access(id, r): Given a range of size r beginning at logical
identiﬁer id, with ⌈log2 r⌉ = i, run Ri.ReadRange(a1)
and Ri.ReadRange(a2) with a1 = ⌊id/2i⌋ and a2 =
(a1 + 2i) mod N .
The updated data blocks are then appended to the stash
of all ℓ + 1 sub-ORAMs. Then, for each Rj , call
Rj.BatchEvict(2i+1, stash).
As mentioned previously, an Access requires two ReadRange’s
to occur (to avoid leaking properties of the range) resulting
in 2i+1 data blocks. For every Access, we need to perform
the same magnitude of BatchEvict’s for all ℓ + 1 ORAMs,
updating the data which is duplicated in each tree.
Remark about Choosing L. The choice of an appropriate
max range size, L primarily depends on the application. How-
ever, the trade-off is that a larger L requires a larger client-side
storage. This is because an L-size query necessitates the local
storage of L blocks. This is reﬂected in the rORAM stash size
bound (Table I). One thing to note here is that due to rORAM’s
log2 N ,
downloading the entire database is faster than accessing the
range from an ORAM. Consequently, an appropriate upper
bound is L < N
log2 N , thus ensuring that the rORAM stash
size is sub-linear in N .
O(log2 N ) bandwidth overhead, for query sizes ≥ N
More importantly, applications rarely access very large
ranges all at once, possibly to reduce the overall access latency.
Instead, a typical application e.g., a ﬁle system breaks down
a large access into multiple smaller sequential accesses, often
not exceeding 1MB in size. In this case, it sufﬁces to initialize
rORAM with L = 28 blocks. In general, for almost all
applications, a reasonable value of L = O(√N ) blocks with
O(√N ) client-side storage. Larger range queries (if any) can
be broken down into smaller range queries of appropriate size.
B. Insight 1: Locality-aware Physical Layout
A common extension to Path ORAM is to use a determin-
istic eviction strategy using bit-reversed ordering of the paths,
as described by Gentry et. al [26]. In bit-reverse ordering,
counting occurs with the least signiﬁcant bit on the left, as
compared to natural ordering, where the most signiﬁcant bit
is to the left and the least signiﬁcant is the right. For example,
counting in 3-bits, the number to follow 000 is not 001 but
rather 100, leading to the sequence of 3-bit-reversed number
ordering as 000 (0), 100 (4), 010 (2), 110 (6), 001 (1), 101
(5), 011 (3), 111 (7) — with the decimal value in parenthesis.
Each bucket of the tree is now labeled with both its level in the
tree and its bit-reversed ordering in that level, as in Figure 2.
That is, a bucket labeled as vj
i signiﬁes the jth bucket among
those at level i.
Evicting paths in this order ensures a good “spread” over the
tree, making it less likely that any blocks get stuck, by chance,
in the higher buckets of the tree and cause an overﬂow. But as
we will show, the bit-reverse ordering can also be leveraged
for the physical layout of the tree to achieve data locality.
v0
0
v0
1
v1
1
v0
2
v2
2
v1
2
v3
2
v0
3
v4
3
v2
3
v6
3
v1
3
v5
3
v3
3
v7
3
Fig. 2. Labeling of ORAM tree buckets. A bucket label vj
jth bucket among those at level i in bit-reversed order.
i signiﬁes the
v0
0
v0
1
v0
2
v0
3
v1
1
v1
2
v1
3
v2
2
v2
3
v3
2
v3
3
v4
3
v5
3
v6
3
v7
3
Fig. 3. Physical disk storage of ORAM Ri. Buckets at each level are
stored sequentially according to the bit-reversed order.
Locality-aware physical layout of Ri. An important obser-
vation is that the the path eviction schedule also implies the
deterministic ordering in which data is evicted to nodes within
levels of the tree; in particular:
The nodes at the same level are ALSO evicted accord-
ing to the bit-reversed order.
Let P (p) be a path from the root to a leaf with position p.
For example, in the tree in Figure 2, the three consecutive
eviction paths P (v2
2 at
level 2, v0
0. At each level, the
buckets are accessed according to the bit-reversed order (with
wraparound).
3), P (v4
1 at level 1, and v0
3), P (v3
3) visits buckets v2
0, v0
0, v0
1, v1
1, v0
2, v3
2, v0
If the ORAM stores each level sequentially on the storage
device, according to the bit-reversed eviction ordering of the
level (see Figure 3), evictions can be done with optimal
number of seeks. Consecutive evictions, as is the case for
BatchEvict, occur in bit-reverse order sequentially for each
level in the tree. To the best of our knowledge, rORAM is the
ﬁrst construction that considers the physical layout to improve
efﬁciency of ORAM performance.
O(log N ) seeks independent of range size. With a sequential
layout of buckets on disk that matches the bit-reversed order at
each level x in the Rj sub-ORAM tree, Rj.BatchEvict(k) will
visit min(k, 2x) buckets, stored physically adjacent to each
other, at level x sequentially. Thus, reading and writing back
to each level requires at most 2 seeks, with wraparounds, and
the ORAM tree has log N + 1 levels. The total number of
seeks performed for Rj.BatchEvict(k) is therefore O(log N ).
Note that the number of seeks is independent of k, the number
of eviction operations performed as a batch. Updating (ℓ + 1)
sub-ORAMs will require O(ℓ + 1· log N ) = O(log2 N ) seeks.
C. Insight 2: Locality-sensitive Mapping
Tree-based ORAM schemes map logical addresses to paths
in the ORAM tree, along which the block corresponding to the
logical address is placed. For Path ORAM, a logical address
7
is mapped to a new random path every time the corresponding
block is accessed.
Recall
that ReadRange on ORAM Ri reads exactly 2i
blocks. Using the traditional mapping mechanism (i.e., assign-
ing a random path for each block) will not provide locality,
resulting in O(2i · log N ) seeks to read 2i random paths.
However, random block placement is critical for the security
(and also correctness) of tree-based ORAMs. Designing a new
mapping scheme requires careful analysis, in order to both
achieve better locality and maintain acceptable security.
Our approach.
rORAM uses a hybrid of random and
deterministic placement policies for mapping blocks to paths:
• For blocks that do not belong to the same range, a
purely random strategy can be applied since no locality
guarantees are required.
• For blocks that belong to the same range, it is desirable
to place these blocks along paths that are stored close to
each other on disk.
The ﬁrst requirement implies that blocks in different ranges
independent
can be mapped to random paths in the tree,
of each other. While allowing better locality,
the second
requirement has implications on the security of the scheme.
Speciﬁcally, blocks that belong to the same range will not
be mapped independently to paths. Accessing a range of a
particular size will be clearly observable based on the paths
read. Since we allow the size of queried ranges to be leaked
anyway, this does not actually reveal any further information.
Locality-sensitive mapping.