# 07 \| 架构设计：设计一个灵活的RPC框架你好，我是何小锋。到今天为止，基础篇的知识我们就全部学习完了，接下来我们进入进阶篇。 在基础篇里面，我们讲了 RPC 的通信原理以及 RPC里各个功能组件的作用，不妨用一段话再次回顾下："其实 RPC就是把拦截到的方法参数，转成可以在网络中传输的二进制，并保证在服务提供方能正确地还原出语义，最终实现像调用本地一样地调用远程的目的。"**你记住了吗？** 那学到这儿，距离实现一个灵活的 RPC框架其实还是有距离的。知道了各个功能组件只是迈出了第一步，接下来你必须要清楚各个组件之间是怎么完成数据交互的，这也是今天这讲的重点，我们一起搞清楚RPC 的架构设计。 RPC 架构说起架构设计，我相信你一定不陌生。我理解的架构设计呢，就是从顶层角度出发，厘清各模块组件之间数据交互的流程，让我们对系统有一个整体的宏观认识。我们先看看RPC 里面都有哪些功能模块。 我们讲过，RPC本质上就是一个远程调用，那肯定就需要通过网络来传输数据。虽然传输协议可以有多种选择，但考虑到可靠性的话，我们一般默认采用TCP协议。为了屏蔽网络传输的复杂性，我们需要封装一个单独的数据传输模块用来收发二进制数据，这个单独模块我们可以叫做传输模块。 用户请求的时候是基于方法调用，方法出入参数都是对象数据，对象是肯定没法直接在网络中传输的，我们需要提前把它转成可传输的二进制，这就是我们说的序列化过程。但只是把方法调用参数的二进制数据传输到服务提供方是不够的，我们需要在方法调用参数的二进制数据后面增加"断句"符号来分隔出不同的请求，在两个"断句"符号中间放的内容就是我们请求的二进制数据，这个过程我们叫做协议封装。 **虽然这是两个不同的过程，但其目的都是一样的，都是为了保证数据在网络中可以正确传输。**这里我说的正确，可不仅指数据能够传输，还需要保证传输后能正确还原出传输前的语义。所以我们可以把这两个处理过程放在架构中的同一个模块，统称为协议模块。 除此之外，我们还可以在协议模块中加入压缩功能，这是因为压缩过程也是对传输的二进制数据进行操作。在实际的网络传输过程中，我们的请求数据包在数据链路层可能会因为太大而被拆分成多个数据包进行传输，为了减少被拆分的次数，从而导致整个传输过程时间太长的问题，我们可以在RPC调用的时候这样操作：在方法调用参数或者返回值的二进制数据大于某个阈值的情况下，我们可以通过压缩框架进行无损压缩，然后在另外一端也用同样的压缩算法进行解压，保证数据可还原。 传输和协议这两个模块是 RPC里面最基础的功能，它们使对象可以正确地传输到服务提供方。但距离 RPC的目标------实现像调用本地一样地调用远程，还缺少点东西。因为这两个模块所提供的都是一些基础能力，要让这两个模块同时工作的话，我们需要手写一些黏合的代码，但这些代码对我们使用RPC的研发人员来说是没有意义的，而且属于一个重复的工作，会导致使用过程的体验非常不友好。 这就需要我们在 RPC里面把这些细节对研发人员进行屏蔽，让他们感觉不到本地调用和远程调用的区别。假设有用到Spring 的话，我们希望 RPC 能让我们把一个 RPC 接口定义成一个 SpringBean，并且这个 Bean 也会统一被 Spring Bean Factory管理，可以在项目中通过 Spring 依赖注入到方式引用。这是 RPC调用的入口，我们一般叫做 Bootstrap模块。 **学到这儿，一个点对点（Point to Point）版本的 RPC框架就完成了。**我一般称这种模式的 RPC框架为单机版本，因为它没有集群能力。所谓集群能力，就是针对同一个接口有着多个服务提供者，但这多个服务提供者对于我们的调用方来说是透明的，所以在RPC 里面我们还需要给调用方找到所有的服务提供方，并需要在 RPC里面维护好接口跟服务提供者地址的关系，这样调用方在发起请求的时候才能快速地找到对应的接收地址，这就是我们常说的"服务发现"。 但服务发现只是解决了接口和服务提供方地址映射关系的查找问题，这更多是一种"静态数据"。说它是静态数据是因为，对于我们的RPC 来说，我们每次发送请求的时候都是需要用 TCP 连接的，相对服务提供方 IP地址，TCP 连接状态是瞬息万变的，所以我们的 RPC框架里面要有连接管理器去维护 TCP连接的状态。 有了集群之后，提供方可能就需要管理好这些服务了，那我们的 RPC就需要内置一些服务治理的功能，比如服务提供方权重的设置、调用授权等一些常规治理手段。而服务调用方需要额外做哪些事情呢？每次调用前，我们都需要根据服务提供方设置的规则，从集群中选择可用的连接用于发送请求。 那到这儿，一个比较完善的 RPC框架基本就完成了，功能也差不多就是这些了。按照分层设计的原则，我将这些功能模块分为了四层，具体内容见图示： ![](Images/aa8406fba5634625d569bc59cb98ff4e.png)savepage-src="https://static001.geekbang.org/resource/image/30/fb/30f52b433aa5f103114a8420c6f829fb.jpg"}架构图可扩展的架构那 RPC架构设计出来就完事了吗？当然不，技术迭代谁都躲不过。 不知道你有没有这样的经历，你设计的一个系统它看上去很完善，也能很好地运行，然后你成功地把它交付给了业务方。有一天业务方有了新的需求，要加入很多新的功能，这时候你就会发现当前架构面临的可就是大挑战了，要修改很多地方才能实现。 举个例子，假如你设计了一个商品发布系统，早些年我们只能在网上购买电脑、衣服等实物商品，但现在发展成可以在网上购买电话充值卡、游戏点卡等虚拟商品，实物商品的发布流程是需要选择购买区域的，但虚拟商品并没有这一限制。如果你想要在一套发布系统里面同时完成实物和虚拟商品发布的话，你就只能在代码里面加入很多的if else判断逻辑，这样是能行，可整个代码就臃肿、杂乱了，后期也极难维护。 其实，我们设计 RPC框架也是一样的，我们不可能在开始时就面面俱到。那有没有更好的方式来解决这些问题呢？这就是我们接下来要讲的插件化架构。 在 RPC框架里面，我们是怎么支持插件化架构的呢？我们可以将每个功能点抽象成一个接口，将这个接口作为插件的契约，然后把这个功能的接口与功能的实现分离，并提供接口的默认实现。在Java 里面，JDK 有自带的 SPI（Service ProviderInterface）服务发现机制，它可以动态地为某个接口寻找服务实现。使用 SPI机制需要在 Classpath 下的 META-INF/services目录里创建一个以服务接口命名的文件，这个文件里的内容就是这个接口的具体实现类。 但在实际项目中，我们其实很少使用到 JDK 自带的 SPI机制，首先它不能按需加载，ServiceLoader加载某个接口实现类的时候，会遍历全部获取，也就是接口的实现类得全部载入并实例化一遍，会造成不必要的浪费。另外就是扩展如果依赖其它的扩展，那就做不到自动注入和装配，这就很难和其他框架集成，比如扩展里面依赖了一个Spring Bean，原生的 Java SPI就不支持。 加上了插件功能之后，我们的 RPC框架就包含了两大核心体系------核心功能体系与插件体系，如下图所示： ![](Images/0bc6b2d4c5267031b82f309a4f041c2a.png)savepage-src="https://static001.geekbang.org/resource/image/a3/a6/a3688580dccd3053fac8c0178cef4ba6.jpg"}插件化RPC**这时，整个架构就变成了一个微内核架构**，我们将每个功能点抽象成一个接口，将这个接口作为插件的契约，然后把这个功能的接口与功能的实现分离并提供接口的默认实现。这样的架构相比之前的架构，有很多优势。首先它的可扩展性很好，实现了开闭原则，用户可以非常方便地通过插件扩展实现自己的功能，而且不需要修改核心功能的本身；其次就是保持了核心包的精简，依赖外部包少，这样可以有效减少开发人员引入RPC 导致的包版本冲突问题。 总结我们都知道软件开发的过程很复杂，不仅是因为业务需求经常变化，更难的是在开发过程中要保证团队成员的目标统一。我们需要用一种可沟通的话语、可"触摸"的愿景达成目标，我认为这就是软件架构设计的意义。 但仅从功能角度设计出的软件架构并不够健壮，系统不仅要能正确地运行，还要以最低的成本进行可持续的维护，因此我们十分有必要关注系统的可扩展性。只有这样，才能满足业务变化的需求，让系统的生命力不断延伸。 课后思考你能分享一下，在日常工作中，你都有哪些地方是用到了插件思想来解决扩展性问题的吗？ 欢迎留言和我分享你的思考，也欢迎你把文章分享给你的朋友，邀请他加入学习。我们下节课再见！ 
# 08 \| 服务发现：到底是要CP还是AP？你好，我是何小锋。在上一讲中，我讲了"怎么设计一个灵活的 RPC框架"，总结起来，就是怎么在 RPC框架中应用插件，用插件方式构造一个基于微内核的 RPC框架，其关键点就是"插件化"。 今天，我要和你聊聊 RPC里面的"服务发现"在超大规模集群的场景下所面临的挑战。 为什么需要服务发现？先举个例子，假如你要给一位以前从未合作过的同事发邮件请求帮助，但你却没有他的邮箱地址。这个时候你会怎么办呢？如果是我，我会选择去看公司的企业"通信录"。 同理，为了高可用，在生产环境中服务提供方都是以集群的方式对外提供服务，集群里面的这些IP随时可能变化，我们也需要用一本"通信录"及时获取到对应的服务节点，这个获取的过程我们一般叫作"服务发现"。 对于服务调用方和服务提供方来说，其契约就是接口，相当于"通信录"中的姓名，服务节点就是提供该契约的一个具体实例。服务IP 集合作为"通信录"中的地址，从而可以通过接口获取服务 IP的集合来完成服务的发现。这就是我要说的 PRC框架的服务发现机制，如下图所示： ![](Images/bee5912904fb114cfb62d925c83e6ab9.png)savepage-src="https://static001.geekbang.org/resource/image/51/5d/514dc04df2b8b2f3130b7d44776a825d.jpg"}RPC服务发现原理图1.       服务注册：在服务提供方启动的时候，将对外暴露的接口注册到注册中心之中，注册中心将这个服务节点的    IP 和接口保存下来。        2.       服务订阅：在服务调用方启动的时候，去注册中心查找并订阅服务提供方的    IP，然后缓存到本地，并用于后续的远程调用。        为什么不使用 DNS？既然服务发现这么"厉害"，那是不是很难实现啊？其实类似机制一直在我们身边，我们回想下服务发现的本质，就是完成了接口跟服务提供者IP 的映射。那我们能不能把服务提供者 IP统一换成一个域名啊，利用已经成熟的 DNS机制来实现？ 好，先带着这个问题，简单地看下 DNS的流程： ![](Images/a76d583c1824359c5b5a336be9ea553a.png)savepage-src="https://static001.geekbang.org/resource/image/3b/18/3b6a23f392b9b8d6fcf31803a5b4ef18.jpg"}DNS查询流程如果我们用 DNS来实现服务发现，所有的服务提供者节点都配置在了同一个域名下，调用方的确可以通过DNS 拿到随机的一个服务提供者的IP，并与之建立长连接，这看上去并没有太大问题，但在我们业界为什么很少用到这种方案呢？不知道你想过这个问题没有，如果没有，现在可以停下来想想这样两个问题： 1.  如果这个 IP    端口下线了，服务调用者能否及时摘除服务节点呢？        2.  如果在之前已经上线了一部分服务节点，这时我突然对这个服务进行扩容，那么新上线的服务节点能否及时接收到流量呢？        这两个问题的答案都是："不能"。这是因为为了提升性能和减少 DNS服务的压力，DNS 采取了多级缓存机制，一般配置的缓存时间较长，特别是 JVM的默认缓存是永久有效的，所以说服务调用者不能及时感知到服务节点的变化。 这时你可能会想，我是不是可以加一个负载均衡设备呢？将域名绑定到这台负载均衡设备上，通过DNS 拿到负载均衡的 IP。这样服务调用的时候，服务调用方就可以直接跟 VIP建立连接，然后由 VIP 机器完成 TCP转发，如下图所示： ![](Images/689cc77f616a7a377a4a6c99f56d9e37.png)savepage-src="https://static001.geekbang.org/resource/image/d8/b9/d8549f6069a8ca5bd1012a0baf90f6b9.jpg"}VIP方案这个方案确实能解决 DNS 遇到的一些问题，但在 RPC场景里面也并不是很合适，原因有以下几点： 1.  搭建负载均衡设备或 TCP/IP    四层代理，需求额外成本；        2.  请求流量都经过负载均衡设备，多经过一次网络传输，会额外浪费些性能；        3.  负载均衡添加节点和摘除节点，一般都要手动添加，当大批量扩容和下线时，会有大量的人工操作和生效延迟；        4.  我们在服务治理的时候，需要更灵活的负载均衡策略，目前的负载均衡设备的算法还满足不了灵活的需求。        由此可见，DNS 或者 VIP 方案虽然可以充当服务发现的角色，但在 RPC场景里面直接用还是很难的。 基于 ZooKeeper 的服务发现那么在 RPC里面我们该如何实现呢？我们还是要回到服务发现的本质，就是完成接口跟服务提供者IP之间的映射。这个映射是不是就是一种命名服务？当然，我们还希望注册中心能完成实时变更推送，是不是像开源的ZooKeeper、etcd就可以实现？我很肯定地说"确实可以"。下面我就来介绍下一种基于 ZooKeeper的服务发现方式。 整体的思路很简单，就是搭建一个 ZooKeeper集群作为注册中心集群，服务注册的时候只需要服务节点向 ZooKeeper节点写入注册信息即可，利用 ZooKeeper 的 Watcher机制完成服务订阅与服务下发功能，整体流程如下图： ![](Images/cd103d2018acd1b4be23f162b6932c64.png)savepage-src="https://static001.geekbang.org/resource/image/50/75/503fabeeae226a722f83e9fb6c0d4075.jpg"}基于ZooKeeper服务发现结构图1.       服务平台管理端先在 ZooKeeper    中创建一个服务根路径，可以根据接口名命名（例如：/service/com.demo.xxService），在这个路径再创建服务提供方目录与服务调用方目录（例如：provider、consumer），分别用来存储服务提供方的节点信息和服务调用方的节点信息。        2.       当服务提供方发起注册时，会在服务提供方目录中创建一个临时节点，节点中存储该服务提供方的注册信息。        3.       当服务调用方发起订阅时，则在服务调用方目录中创建一个临时节点，节点中存储该服务调用方的信息，同时服务调用方    watch    该服务的服务提供方目录（/service/com.demo.xxService/provider）中所有的服务节点数据。        4.       当服务提供方目录下有节点数据发生变更时，ZooKeeper    就会通知给发起订阅的服务调用方。        我所在的技术团队早期使用的 RPC 框架服务发现就是基于 ZooKeeper实现的，并且还平稳运行了一年多，但后续团队的微服务化程度越来越高之后，ZooKeeper集群整体压力也越来越高，尤其在集中上线的时候越发明显。"集中爆发"是在一次大规模上线的时候，当时有超大批量的服务节点在同时发起注册操作，ZooKeeper集群的 CPU 突然飙升，导致 ZooKeeper集群不能工作了，而且我们当时也无法立马将 ZooKeeper 集群重新启动，一直到ZooKeeper集群恢复后业务才能继续上线。 经过我们的排查，引发这次问题的根本原因就是 ZooKeeper本身的性能问题，当连接到 ZooKeeper 的节点数量特别多，对 ZooKeeper读写特别频繁，且 ZooKeeper 存储的目录达到一定数量的时候，ZooKeeper将不再稳定，CPU持续升高，最终宕机。而宕机之后，由于各业务的节点还在持续发送读写请求，刚一启动，ZooKeeper就因无法承受瞬间的读写压力，马上宕机。 这次"意外"让我们意识到，ZooKeeper集群性能显然已经无法支撑我们现有规模的服务集群了，我们需要重新考虑服务发现方案。 基于消息总线的最终一致性的注册中心我们知道，ZooKeeper 的一大特点就是强一致性，ZooKeeper集群的每个节点的数据每次发生更新操作，都会通知其它 ZooKeeper节点同时执行更新。它要求保证每个节点的数据能够实时的完全一致，这也就直接导致了ZooKeeper集群性能上的下降。这就好比几个人在玩传递东西的游戏，必须这一轮每个人都拿到东西之后，所有的人才能开始下一轮，而不是说我只要获得到东西之后，就可以直接进行下一轮了。 而 RPC框架的服务发现，在服务节点刚上线时，服务调用方是可以容忍在一段时间之后（比如几秒钟之后）发现这个新上线的节点的。毕竟服务节点刚上线之后的几秒内，甚至更长的一段时间内没有接收到请求流量，对整个服务集群是没有什么影响的，所以我们可以牺牲掉CP（强制一致性），而选择AP（最终一致），来换取整个注册中心集群的性能和稳定性。 那么是否有一种简单、高效，并且最终一致的更新机制，能代替 ZooKeeper那种数据强一致的数据更新机制呢？ 因为要求最终一致性，我们可以考虑采用消息总线机制。注册数据可以全量缓存在每个注册中心内存中，通过消息总线来同步数据。当有一个注册中心节点接收到服务节点注册时，会产生一个消息推送给消息总线，再通过消息总线通知给其它注册中心节点更新数据并进行服务下发，从而达到注册中心间数据最终一致性，具体流程如下图所示： ![](Images/b9d8a2e051e40144773ff7fbe8d880d3.png)savepage-src="https://static001.geekbang.org/resource/image/73/ff/73b59c7949ebed2903ede474856062ff.jpg"}流程图1.  当有服务上线，注册中心节点收到注册请求，服务列表数据发生变化，会生成一个消息，推送给消息总线，每个消息都有整体递增的版本。        2.  消息总线会主动推送消息到各个注册中心，同时注册中心也会定时拉取消息。对于获取到消息的在消息回放模块里面回放，只接受大于本地版本号的消息，小于本地版本号的消息直接丢弃，从而实现最终一致性。        3.  消费者订阅可以从注册中心内存拿到指定接口的全部服务实例，并缓存到消费者的内存里面。        4.  采用推拉模式，消费者可以及时地拿到服务实例增量变化情况，并和内存中的缓存数据进行合并。        为了性能，这里采用了两级缓存，注册中心和消费者的内存缓存，通过异步推拉模式来确保最终一致性。 另外，你也可能会想到，服务调用方拿到的服务节点不是最新的，所以目标节点存在已经下线或不提供指定接口服务的情况，这个时候有没有问题？这个问题我们放到了RPC框架里面去处理，在服务调用方发送请求到目标节点后，目标节点会进行合法性验证，如果指定接口服务不存在或正在下线，则会拒绝该请求。服务调用方收到拒绝异常后，会安全重试到其它节点。 通过消息总线的方式，我们就可以完成注册中心集群间数据变更的通知，保证数据的最终一致性，并能及时地触发注册中心的服务下发操作。在RPC领域精耕细作后，你会发现，服务发现的特性是允许我们在设计超大规模集群服务发现系统的时候，舍弃强一致性，更多地考虑系统的健壮性。最终一致性才是分布式系统设计中更为常用的策略。 总结今天我分享了 RPC 框架服务发现机制，以及如何用 ZooKeeper完成"服务发现"，还有 ZooKeeper在超大规模集群下作为注册中心所存在的问题。 通常我们可以使用 ZooKeeper、etcd 或者分布式缓存（如Hazelcast）来解决事件通知问题，但当集群达到一定规模之后，依赖的ZooKeeper 集群、etcd集群可能就不稳定了，无法满足我们的需求。 在超大规模的服务集群下，注册中心所面临的挑战就是超大批量服务节点同时上下线，注册中心集群接受到大量服务变更请求，集群间各节点间需要同步大量服务节点数据，最终导致如下问题： 1.  注册中心负载过高；        2.  各节点数据不一致；        3.  服务下发不及时或下发错误的服务节点列表。        RPC 框架依赖的注册中心的服务数据的一致性其实并不需要满足 CP，只要满足AP即可。我们就是采用"消息总线"的通知机制，来保证注册中心数据的最终一致性，来解决这些问题的。 另外，在今天的内容中，很多知识点不只可以应用到 RPC框架的"服务发现"中。例如服务节点数据的推送采用增量更新的方式，这种方式提高了注册中心"服务下发"的效率，而这种方式，你还可以利用在其它地方，比如统一配置中心，用此方式可以提升统一配置中心下发配置的效率。 课后思考目前服务提供者上线后会自动注册到注册中心，服务调用方会自动感知到新增的实例，并且流量会很快打到该新增的实例。如果我想把某些服务提供者实例的流量切走，除了下线实例，你有没有想到其它更便捷的办法呢？ 欢迎留言和我分享你的思考和疑惑，也欢迎你把文章分享给你的朋友，邀请他加入学习。我们下节课再见！ 