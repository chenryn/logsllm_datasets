Generic and Scalable Framework for Automated Time-series Anomaly Detection
Nikolay Laptev 
Yahoo Labs 
Sunnyvale, CA, USA 
PI:EMAIL Saeed Amizadeh Yahoo Labs 
Sunnyvale, CA, USA PI:EMAIL Ian Flint 
Yahoo 
Sunnyvale, CA, USA 
PI:EMAIL
PI:EMAIL
ABSTRACT 
This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintain-ing consistency of person’s data and protects corporationsWhen the underlying generating process behaves in an un-usual way, it creates outliers. Fast and efficient identification of these outliers is useful for many applications including: in-trusion detection, credit card fraud, sensor events, medical diagnoses, law enforcement and others [1].
against malicious attackers. Current state of the art anomalydetection approaches suffer from scalability, use-case restric-tions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time-Current approaches in automated anomaly detection suffer from a large number of false positives which prohibit the use-fulness of these systems in practice. Use-case, or category specific, anomaly detection models [4] may enjoy a low false positive rate for a specific application, but when the charac-series. 	We compare our approach against other anomaly 	teristics of the time-series change, these techniques perform detection systems on real and synthetic data with varying 	poorly without proper retraining. Section 6.3 demonstrates time-series characteristics. 	We found that our framework 	the shortcoming of ‘one size fits all’ principle in practice.
allows for 50-60% improvement in precision and recall for avariety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in par-ticular, represents the first of its kind effort to establish the standard benchmark for anomaly detection.
Our system at Yahoo is called EGADS (Extensible Generic Anomaly Detection System) and it enables the accurate and scalable detection of time-series anomalies. EGADS sepa-rates forecasting, anomaly detection and alerting into three separate components which allows the person to add her| 1. | INTRODUCTION | own models into any of the components. | Note that this |
|---|---|---|---|
| 1. |INTRODUCTION |paper focuses on the latter two components. |Note that this |
| While rapid advances in computing hardware and software |While rapid advances in computing hardware and software |paper focuses on the latter two components. |Note that this |have led to powerful applications, still hundreds of software bugs and hardware failures continue to happen in a large cluster compromising user experience and subsequently rev-enue. Non-stop systems have a strict uptime requirement and continuous monitoring of these systems is critical. From the data analysis point of view, this means non-stop moni-toring of large volume of time-series data in order to detect potential faults or anomalies. Due to the large scale of the problem, human monitoring of this data is practically infea-sible which leads us to automated anomaly detection using Machine Learning and Data Mining techniques.An anomaly, or an outlier, is a data point which is signif-icantly different from the rest of the data. Generally, the data in most applications is created by one or more gen-erating processes that reflect the functionality of a system.EGADS uses a set of default models that are tuned to re-duce the number of false positives, which by itself suffices for the average user. 	More advanced use-cases, however, will require the system to capture some types of anomalies while ignoring others. The anomalies of interest may vary in magnitude, severity or other parameters which are un-known apriori and depend on the use-case. For this reason the alerting component of EGADS uses machine learning to select the most relevant anomalies for the consumer.To the best of our knowledge EGADS is the first compre-hensive system for anomaly detection that is flexible, accu-rate, scalable and extendible. EGADS is being open-sourced along with the anomaly detection benchmarking data. The open-sourcing of the data and the system will provide the first of its kind benchmarking data and the framework to help the academics and the industry collaborate and develop novel anomaly detection models. At Yahoo, EGADS is used on millions of time-series by many teams daily.In Section 2 we describe the EGADS architecture. The al-gorithms and the alerting module are described in Sections 3 and 4 respectively. Previous work is described in Section 5 followed by the real-world use-cases and conclusion in Sec-tions 6 and 7 respectively.
2. ARCHITECTURE The online flow then utilizes the stored models.
The EGADS framework consists of three main components:the time-series modeling module (TMM), the anomaly de-tection module (ADM) and the alerting module (AM). Given a time-series the TMM component models the time-series producing an expected value later consumed by the ADM and AM components that, respectively, compute the error and filter uninteresting anomalies. These components are described in detail in Sections 3 and 4.EGADS was built as a framework to be easily integrated into an existing monitoring infrastructure. At Yahoo, our inter-nal Yahoo Monitoring Service (YMS) processes millions of data-points every second. Therefore, having a scalable, ac-curate and automated anomaly detection for YMS is critical. We describe the integration details into YMS next.
1. Data flows into a Storm [22] stream-processing topol-	ogy.2. One of the bolts (modules) in the topology calls the EGADS ADM to evaluate incoming data points based on models stored in the model database.
3. If an anomaly is present, this is fed to a secondary rule flow, consisting of combinatorial rules and other use-cases specific logic (see Section 4).
4. Based on the rules, if the anomaly is an alert event, the event is generated, stored in a status database, and forwarded to an alert routing system.| 2.1 | System Integration | 5. The alert routing system applies routing configuration |
|---|---|---|
| 2.1 |System Integration |rules to send the alert to the appropriate support staff. |EGADS operates as a stand-alone platform that can be used as a library in larger systems. Therefore, designing an in-terface between EGADS and an internal Yahoo monitoring service (YMS) is critical. A key constraint of YMS is scale; the platform needs to evaluate millions of data points per second. As a result, many of the integration architecture de-cisions are focused on optimizing real-time processing. The integration with YMS is shown in Figure 1.Figure 1: EGADS-YMS Architecture
Several support components are required to drive action based on detected anomalies. First of all, all anomaly detec-tion models are generated in batch and applied in real time.
The batch flow is comprised of three steps:
1. Telemetry (i.e. 	the monitored time-series) data are 	stored in bulk on a Hadoop cluster.
2. A batch model generator runs against these data and 	builds models for targeted time-series.3. The models are stored in a model database.
2.2 	Scalability 
The monitoring use case for EGADS requires the evaluation of millions of data-points per second, across over one hun-dred million time-series. This has scalability implications in terms of CPU load, I/O, and memory footprint. The eval-uation of a datapoint needs to be as efficient as possible. This means that as much of the model as possible should be precomputed. It is not practical to read a model from disk each time a datapoint arrives because of the rate of inbound traffic. This suggests that the models should be stored in memory. In order to contain costs, the models should be as small as possible.One optimization is to share models across multiple similar time-series. This is practical in the context of a large web serving environment, since applications are broken into hor-izontal tiers of similar servers. This optimization will reduce the memory footprint, the batch workload, and I/O against the model database.Another possible optimization is to investigate self-tuning models; models that update themselves based on a stream of inbound data via online learning rather than requiring periodic batch generation. 	Models of this type may need to be initialized in batch, but overall they will reduce the batch workload. 	Depending on implementation, however, they may increase writes against the model database since they are being constantly refined.Yet another optimization involves a trade-off between model size, training speed and accuracy. Depending on the charac-teristics of the time-series a light and fast forecasting model can provide similar accuracy as a more sophisticated one. We evaluate some of these optimization approaches in Sec-tion 6.2.2.3. 	ANOMALY DETECTION ALGORITHMS In this section, we give a big picture overview of the anomaly detection algorithms supported by EGADS. Currently, EGADS is capable of detecting three classes of anomalies:
(a) Outliers: given an input time-series x, an outlier is a timestamp-value pair ⟨t, xt⟩ where the observed value
xt is significantly different from the expected value of the time-series at that time, i.e. E(xt).(b) Change points: given an input time-series x, a change point is a timestamp t such that the behavior of the time-series is significantly different before and after t.
(c) Anomalous time-series: given a set of time-series
of deviation is the prediction error, PEt = xt − ut. If the error falls outside some fixed thresholds, an alert is issued. This simple method may work in some cases, but it will not be a good strategy for most because it does not capture the relative error. The relative error, REt is defined as a factor of ut:| X = {x(i)}, an anomalous time-series x(j)∈ X is a time-series whose behavior is significantly different | REt =xt − ut 	ut | =xt 
ut | (1) |
|---|---|---|---|
| from the majority of the time-series in X. |REt =xt − ut 	ut |=xt  ut |(1) |
By thresholding the relative error, one can detect anomalies
In the following sections, we give the general sketch of the methods that are currently used in EGADS for detecting the aforementioned anomaly types.3.1 	Outlier Detection 
Detecting outliers is the most important functionality in many monitoring applications. 	For this reason the main focus of this paper is on outlier detection and unless it is explicitly specified, by anomalies, we refer to outliers by de-fault.
EGADS offers two classes of algorithms for detecting out-liers, which are described in this section.
3.1.1 	Plug-in methodsThe first class of methods for time-series outlier detection in EGADS are called plug-in methods. These methods explic-itly model the normal behavior of the time-series such that a significant deviation from this model is considered an out-lier. To model the normal behavior of the input time-series we can plug-in a wide range of time-series modeling and fore-casting models (e.g. ARIMA [26], Exponential Smoothing [11], Kalman Filter [9], State Space Models [6], etc.) de-pending on the application and the nature of time-series. That is why we refer to this general strategy as the plug-in methods. It should be noted that all these models are used in EGADS for time-series forecasting which is another feature of our framework; however, since the focus of this paper is on anomaly detection, we do not give more details on modeling and forecasting features of EGADS.Our proposed Plug-in framework consists of two main com-ponents: the time-series modeling module (TMM) and the anomaly detection module (ADM). Given a time-series X = {xt ∈ R : ∀t ≥ 0}, the TMM provides the predicted value of xt at time t, denoted by ut. We also refer to this quan-tity as the expected value of xt (not to be confused with the mathematical notion of expectation). 	The TMM can be a machine learned model which makes predictions based on some training data or a rule-based system which encodes expert’s knowledge about how xt behaves at time t. In this paper, we do not make any assumption regarding the TMM; that is, the TMM is just a black box module in our pro-while normalizing out the dependence on the magnitude of the expected value. The values of these thresholds, indeed, determine how sensitive the anomaly detection module is. Various thresholding techniques are described in Section 4. Despite its common usage and effectiveness, however, there is no reason to believe the relative error is always the op-timal metric for anomaly detection on a given time-series. In fact, the choice of the optimal metric for a given time-series highly depends on the nature of the time-series as well as the TMM performance. For instance, if we are dealing with a very regular time-series for which we have an accu-rate model, using the prediction error for anomaly detection might be sufficient as it is expected to be Normally dis-tributed. In other cases, the optimal metric might be some-thing between the prediction error and the relative error. For this reason, EGADS tracks a set of deviation metrics by default and the person using the system can create her own error metrics. These error metrics, together with other features, such as the time series characteristics, are used in the alerting module (AM), described in Section 4, to learn consumer’s preferences and filter unimportant anomalies.3.1.2 	Decomposition-based methods 
The second class of outlier detection methods in EGADS is based on the idea of time-series decomposition. In partic-ular, in the time-series analysis literature, it is a common practice to decompose a time-series into three components: trend, seasonality and noise. By monitoring the noise com-ponent, one can capture the outliers. More precisely, if the absolute value of the noise component of point xt is greater than a certain threshold, one can announce xt as an outlier.The decomposition of time-series can be done both in the time-domain via smoothing or in the frequency-domain via spectral decomposition. STL (Seasonal-Trend Decomposi-tion based on Loess) 	[5] is a famous technique that uses Loess smoothing for decomposition. The frequency-domain methods can be further divided into parameteric and non-parametric methods. For the parametric methods, the ba-sis used for spectral decomposition has a known parametric form (such as Fourier transform 	[2] or wavelet transform [19]) whereas, for non-parametric methods, the basis is data-driven [18].| posed method that generates predictions ut. In this sense, | 3.2 | Change Point Detection |
|---|---|---|
| our proposed framework is generic and does not depend on |3.2 |Change Point Detection |
any specific time-series modeling framework.
Given the predicted value ut and the actual observed value xt, the ADM computes some notion of deviation which we refer to as the deviation metric (DM). The simplest measureChange points are those points in time where the behavior of the time-series starts to deviate from what is expected. The big difference between change points and outliers is that change points correspond to more sustained, long-term changes compared to volatile outliers. A common strategy
for detecting change points in the literature is to move two side-by-side windows on the time-series and compute the dif-4. 	ALERTING
The end-goal of anomaly detection is to produce accurate
ference between the behavior of the time-series in the two and timely alerts. EGADS achieves this via a two stage
windows as a measure of the deviation metric [12, 27, 17, 20]. The behavior of the time-series in each window is typi-cally modeled by the distribution of the values, motifs, fre-quencies, etc. that are present in the time-series. We refer to these techniques as the absolute techniques because they do not make explicit assumptions regarding the expected behavior of the time-series.In EGADS, currently we have taken a different approach which we refer to as the relative or model-based methods. In these methods, the expected behavior of the time-series is explicitly modeled through one of the modeling techniques mentioned in Section 3.1.1. In particular, we incorporate the plug-in approach described in Section 3.1.1 to compute the sequence of residuals (or deviations from the model expec-tation) for an input time-series. Then we apply the absolute change point detection methods on the series of residuals to detect a change in the distribution of the residuals. We haveprocess by first generating a set of candidate anomalies by threshold selection and then filtering the irrelevant anoma-lies for a given use-case.
4.1 	Threshold Selection 
The job of threshold selection is to select appropriate thresh-olds on the deviation metrics produced by the anomaly de-tection module (ADM). Currently EGADS implements two algorithms for threshold selection based on (a) Kσ deviation and (b) density distribution.The first approach is parametric and assumes that the data is normally distributed with a well-defined mean and stan-dard deviation. Relying on the Gaussian distribution we can apply a well known statistical tool called the ‘three-sigma rule’ which states that 99.73% of all samples lie within three standard deviations of the mean. Therefore, depending onused Kernel Density Estimation [7] to non-parametrically the value of K in Kσ, one can be confident as to the prob-
estimate the distribution of the residuals and the Kullback-Leibler divergence [16] to measure the change in the distri-