skills store. This list contains 25,150 skill names, of
which 23,368 are unique. This list was collected on De-
cember 27th, 2017.
Figure 5: Word Accuracy—The accuracy of Alexa interpre-
tations by word is shown as a cumulative distribution function.
9% of the words in our dataset are never interpreted correctly
and 2% are always interpreted correctly. This shows substantial
variance in misinterpretation rate among words.
3.5 Ethical Considerations
Although we use speech samples collected from human
subjects, we never interact with subjects during the course
of this work. We use public datasets and ensure our us-
age is in line with their provider’s terms of service. All
requests to Alexa are throttled so to not affect the avail-
ability of production services. For all attacks presented
in this paper, we test them only in a controlled, developer
environment. Furthermore, we do not attempt to publish a
malicious skill to the public skill store. We have disclosed
these attacks to Amazon and will work with them through
the standard disclosure process.
4 Understanding Alexa Errors
In this section, we conduct an empirical analysis of the
Alexa speech-recognition system. Speciﬁcally, we mea-
sure its accuracy, quantify the frequency of its interpre-
tation errors, classify these errors, and explain why such
errors occur.
4.1 Quantifying Errors
We begin our analysis by investigating how well Alexa
transcribes the words in our dataset. We ﬁnd that
Alexa successfully interprets only 394,715 (68.9%) out
of the 572,319 queries.
In investigating where Alexa makes interpretation er-
rors, we ﬁnd that errors do not affect all words equally.
Figure 5 shows the interpretation accuracy by individual
words in our dataset. Only three words (2%) are always
interpreted correctly. In contrast, 9% of words are always
interpreted incorrectly, indicating that Alexa is poor at
correctly interpreting some classes of words. Table 2
characterizes these extremes by showing the top 10 misin-
terpreted words as well as the top 10 correctly interpreted
words in our dataset. We ﬁnd that words with the lowest
accuracy tend to be small, single-syllable words, such
as “bean”, “calm”, and “coal”. Words with the highest
36    27th USENIX Security Symposium
USENIX Association
 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1CDF Alexa Accuracy by WordAccuracy by WordWord
Bean
Calm
Coal
Con
Cot
Dock
Heal
Lull
Lung
Main
Accuracy
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
Word
Forecast
Robin
Tiger
Good
Happily
Dandelion
Serenade
Liberator
Circumstance
Paragraph
Accuracy
100.0%
100.0%
100.0%
99.9%
99.8%
99.7%
99.6%
99.3%
99.3%
99.3%
(a) Lowest Accuracy Rate
(b) Highest Accuracy Rate
Table 2: Words with Highest and Lowest Accuracy—The
best and worst interpretation accuracies for individual words are
shown here. We ﬁnd that the words with the lowest accuracy
seem to be small, single syllable words.
Figure 6: Unique Misinterpretations per Word—The num-
ber of unique misinterpretations per word is shown as a cumu-
lative distribution function. Even among words that are poorly
understood by Alexa, there is variance in the number of unique
misinterpretations. The median number of unique misinterpre-
tations is 15, with a heavy tail. In the worst case, the word
“unadvised” is misinterpreted in 147 different ways by Alexa.
accuracy are mixed. Many of the top words contain two
or three syllables, such as “forecast” and “robin”.
In
one counter example, the word “good” was interpreted
correctly 99.9% of the time.
4.2 Classifying Errors
Even among words that are poorly understood by Alexa,
there is signiﬁcant variance in the number of unique mis-
interpretations. For example, the word “bean” has a 0%
accuracy rate and is misinterpreted in 12 different ways,
such as “been”, “beam”, and “bing”. In contrast, the word
“unadvised” was also never interpreted correctly, but mis-
interpreted in 147 different ways, such as “an advised”, “i
devised”, and “hundred biased”. Figure 6 shows the num-
ber of unique misinterpretations per word. The median
number of misinterpretations is 15, but with a heavy tail.
In investigating the distributions of misinterpretations
per word, we observe that, for each of the 188 words,
there are one or two interpretations that Alexa outputs
more frequently than the others. Motivated by this ob-
Figure 7: Error Rate vs MCE—We plot the error rate by
the rate of the most common error for all the words of our
dataset. Points in the upper right quadrant represent words
that are misinterpreted both frequently and consistently. In our
dataset of 188 words, 24 (12.8%) fall in the upper right quadrant.
servation, we introduce the notion of the “most common
error” (MCE) for a given word. As an example, consider
the word “boil”, which is misinterpreted 100% of the
time. The MCE of “boil” is the word “boyle”, which ac-
counts for 94.3% (MCE Rate) of the errors. In this sense,
the rate at which the MCE occurs serves as a measure
of how random the distribution of misinterpretations is.
Because “boyle” accounts for the majority of its interpre-
tation errors, we thus claim that “boil” has a predictable
misinterpretation distribution.
To visualize the rate and randomness of interpretation
errors per word, we plot the error rate for each word
along with its MCE rate (Figure 7). This graphical rep-
resentation provides us with a clearer picture of interpre-
tation errors in Alexa. We then split this plot into four
quadrants—quadrant I (upper-right), II (upper-left), III
(bottom-left), and IV (bottom-right).
The majority (56.4%) of words in our dataset fall into
quadrant III (bottom-left). These are words that are both
interpreted correctly most of the time and do not have a
prevalent MCE. Instead, they have uncommon errors with
no obvious pattern. 21.3% of words appear in quadrant IV
(bottom-right). These are words that are often interpreted
correctly, but do have a prevalent MCE. There are 9.6%
of the words in our dataset that appear in quadrant II (top-
left), meaning they are misinterpreted often, but do not
feature a prevalent MCE. These are likely to be words
that Alexa is poor at understanding altogether. As an
example, the word “unadvised”, which has 147 unique
misinterpretations, appears in this quadrant. The ﬁnal
class of words, in quadrant I (upper-right), are those that
are misinterpreted more than 50% of the time and have an
MCE that appears in more than 50% of the errors. These
are words that are Alexa misunderstands both frequently
USENIX Association
27th USENIX Security Symposium    37
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 20 40 60 80 100 120 140 160CDF MisinterpretationsMisinterpretations per Word 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Probability of ErrorProbability of MCEWord
rip
rap
lung
lang
wet
what
dime
time
bean
been
dull
doll
coal
call
luck
lock
loud
louder
sweeten Sweden
MCE Word Phonemes MCE Phonemes
R AE P
L AE NG
W AH T
T AY M
B IH N
D AA L
K AO L
L AA K
L AW D ER
S W IY D AH N
R IH P
L AH NG
W EH T
D AY M
B IY N
D AH L
K OW L
L AH K
L AW D
S W IY T AH N
Table 3: Phonetic Structure of Systematic Errors—We show
the underlying phonetic structure of the ten systematic errors
that seem to appear due to Alexa confusing certain phonemes
with others. In each case, the resultant MCE is at an edit distance
of just one phoneme from the intended word.
and in a consistent manner. There are 24 (12.8%) such
words in our dataset.
4.3 Explaining Errors
We now have a classiﬁcation for interpretation errors from
our dataset. Moreover, we identiﬁed 24 words for which
Alexa consistently outputs one wrong interpretation. We
next investigate why these systematic errors occur.
Homophones Unsurprisingly, eight (33.3%) of these
errors, including “sail” to “sale”, “calm” to “com”, and
“sell” to “cell” are attributable to the fact that these words
are homophones, as they have the same pronunciation,
but different spellings. Of these, ﬁve are cases where
Alexa returns a proper noun (of a person, state, band or
company) that is a homophone with the spoken word,
for example, “main” to “Maine”, “boil” to “Boyle”, and
“outshine” to “Outshyne”.
Compound Words
Two (8.3%) other systematic er-
rors occur due to compound words. Alexa appears to
break these into their constituent words, rather than return
the continuous compound word. For example, “super-
highway” is split into “super highway” and “outdoors” is
split into “out doors”.
Phonetic Confusion
Ten (41.7%) of the systematic
errors can be explained by examining the underlying pho-
netic structures of the input words and their errors: in
each case, the MCE differs from the spoken word by just
a single phoneme. For example, the MCE for the word
“wet” is the word “what”. The phonetic spelling of “wet”
is W EH T, whereas the phonetic spelling of “what” is W
AH T. These errors show that Alexa often misunderstands
certain speciﬁc phonemes within words while correctly
interpreting the rest of them. A full list of the phonetic
structures for these cases is shown in Table 3.
We
could not
Other Errors
easily explain
three (12.5%) of the errors: “mill” to “no”, “full”
to “four” and “earthy” to “Fi”. Even in listening to each
speech sample individually, we found no auditory reason
why this interpretation error occurs. One surprising error
(“preferably” to “preferrably”) occurred because Alexa
returned a common misspelling of the intended word.
This may be caused by a bug in the Alexa system itself.
5 Skill Squatting
Our empirical analysis uncovers the existence of fre-
quently occurring, predictable errors in Amazon Alexa.
We next investigate how an adversary can leverage these
errors to cause harm to users in the Alexa ecosystem. To
this end, we introduce a new attack, called skill squatting,
which exploits predictable errors to surreptitiously route
users to a malicious Alexa skill. The core idea is sim-
ple—given a systematic error from one word to another,
an adversary constructs a malicious skill that has a high
likelihood of confusion with a target skill on the Alexa
skills store. When a user attempts to access a desired skill
using their voice, they are routed instead to the malicious
skill, due to a systematic error in the interpretation of
the input. This attack is most similar in style to domain
name typosquatting, where an attacker predicts a com-
mon “typo” in domain names and abuses it to hijack a
request [35, 43, 44, 45]. However, typosquatting relies
on the user to make a mistake when typing a domain; in
contrast, our attack is intrinsic to the speech-recognition
service itself. In this section, we evaluate the skill squat-
ting attack and explore what it looks like in the wild.
5.1 Will This Attack Work End-To-End?
Up to this point, our model of interpretation errors has
been entirely constructed based on observations outside
of a skill invocation environment. We next investigate
whether these errors can be exploited in a skill invocation
environment, to redirect the processing of an Alexa query
to an attacker-controlled skill server.
Our testing process is as follows: given a model of
predictable errors, we build pairs of skills with names that
are frequently confused by Alexa. For example, because
“boil” is frequently confused with “boyle”, we would build
two skills: one with the name Boil and one with the name
Boyle. We call these skills the target skill (or squattable
skill) and the squatted skill. We refer to words with these
predictable, frequently occurring errors as squattable. If
an attack is successful, Alexa will trigger the squatted
skill when a request for the target skill is received. For
example, when a user says:
“Alexa, ask Boil hello.”
38    27th USENIX Security Symposium
USENIX Association
Target Skill
Coal
Lung
Sell
Heal
Sail
Accelerate
Rip
Mill
Con
Luck
Lull
Dull