nfv
= (cid:7)n/α(cid:8)
α = (cid:13)N log(t
nfv
(cid:2))/β(cid:14)
(cid:8))(cid:8) ≤ log(t)
(1)
Our evaluation answers four questions:
1. What are the concrete resource costs of SealPIR, and how
do they compare to XPIR?
2. What is the throughput and latency achieved by SealPIR
under different deployment scenarios?
3. What are the concrete beneﬁts provided by PBCs, and how
do they compare to existing batch codes?
4. What is the impact of using SealPIR and mPIR on a repre-
sentative system?
Experimental setup. We run our experiments using Mi-
crosoft’s Azure instances in three data centers: West US, South
India, and West Europe. We run the PIR servers on H16 in-
stances (16-core 3.6 GHz Intel Xeon E5-2667 and 112 GB
RAM), and clients on F16s instances (16-core, 2.4 GHz Intel
Xeon E5-2673 and 32 GB RAM), all running Ubuntu 16.04. We
compile all our code with Rust’s nightly version 1.25. For XPIR,
we use the publicly available source code [9] and integrate it
into our testing framework using Rust wrappers. We report all
network costs measured at the application layer. We run each
experiment 10 times and report averages from those 10 trials.
Standard deviations are less than 10% of the reported means.
Parameters. We choose security parameters for FHE follow-
ing XPIR’s latest estimates [5], which are based on the analysis
and tools by Albrecht et al. [10]. We set the degree of cipher-
texts’ polynomials to 2048, and the size of the coefﬁcients to
60 bits (N and q in Section 3). Speciﬁcally, SEAL uses a value
of q = 260 − 218 + 1, whereas XPIR uses q = 261 − i · 214 + 1,
for various values of i [6].
Each database element is 288 bytes. We choose this size since
the Pung communication system uses 288-byte messages (§7.4).
Unless otherwise stated, SealPIR uses a plaintext modulus
t=223. A larger t leads to lower network and computational
costs, but might cause noise to grow too much, preventing ci-
phertexts from decrypting successfully (we lower t in some
experiments to ensure that we can always decrypt the result).
For XPIR, we use α = 14, meaning that we pack α elements
into a single XPIR plaintext, thereby reducing the number of
elements stored in the database by a factor of α. For 288-byte el-
ements and our security parameters, setting α = 14 has roughly
the same effect as setting t = 223 in SealPIR (although our opti-
mization to EXPAND, which we discuss in Section 6, means that
SealPIR ultimately packs fewer elements together than XPIR).
Here α is the number of elements of size β bits that can be
packed into a single FV plaintext, and nfv is the number of FV
plaintexts needed to represent n elements of size β.
Implementation of PBCs. We also implement mPIR, a multi-
query PIR library based on PBCs. mPIR implements 5 different
PBC constructions based on reverse hashing (§4.4) with dif-
ferent allocation algorithms (e.g., two-choice hashing, Cuckoo
hashing, the Hybrid allocation scheme in Pung [11]). This li-
brary works transparently on top of both XPIR and SealPIR,
and is written in 1,700 lines of Rust. It uses SHA-256 with
varying counters to implement the different hash functions.
7.1 Cost and performance of SealPIR
To evaluate SealPIR, we run a series of microbenchmarks to
measure: (i) the time to generate, expand, and answer a query;
(ii) the time to extract the response; and (iii) the time to prepro-
cess the database. We study several database sizes and repeat
the same experiment for XPIR using two different dimension
parameters d (§3.4). Figure 9 tabulates our results.
CPU costs. We ﬁnd that the computational costs of query
generation are an order of magnitude lower under SealPIR
√
than under XPIR. This is because the client in SealPIR gener-
ates d ciphertexts as a query rather than d d
n ciphertexts as in
971
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:31:59 UTC from IEEE Xplore.  Restrictions apply. 
database size (n)
65,536
262,144
1,048,576
65,536
262,144
1,048,576
65,536
262,144
1,048,576
XPIR (d = 2)
XPIR (d = 3)
SealPIR (d = 2)
client CPU costs (ms)
QUERY
EXTRACT
13.83
0.34
server CPU costs (sec)
SETUP
EXPAND
ANSWER
0.15
N/A
0.21
network costs (KB)
query
answer
4,384
256
27.57
0.29
0.57
N/A
0.63
8,768
256
55.14
0.30
2.27
N/A
2.12
4.98
2.47
0.15
N/A
0.27
8.03
2.49
0.58
N/A
0.78
17,536
256
1,632
1,824
2,560
1,952
12.74
2.57
2.32
N/A
2.52
4,064
1,952
3.37
1.37
0.23
0.05
0.13
64
256
3.37
1.39
1.04
0.11
0.5
64
256
3.37
1.69
4.26
0.23
2.01
64
256
FIGURE 9—Microbenchmarks of CPU and network costs for XPIR and SealPIR under varying database sizes (n). Elements are of size 288 bytes.
XPIR (§3.4). When it comes to the server, SealPIR’s EXPAND
procedure introduces CPU overheads of 11% to 38% (over an-
swering a query vector directly). While this is high, it results in
signiﬁcant network savings (which we discuss below). Further-
more, even with the overhead of EXPAND, the cost of answering
a query in SealPIR is comparable to XPIR.
We note that larger values of d lead to more computation
for the server for two reasons. First, structuring the database
as a d-dimensional hyperrectangle often requires padding the
database with dummy plaintexts to ﬁt all dimensions. Second,
as we discuss in Section 3.4, the ciphertext expansion factor ef-
fectively increases the size of the elements by a factor of F after
processing each dimension, necessitating more computation.
Network costs. For network costs, SealPIR enjoys a signiﬁ-
cant reduction owing to its query encoding and EXPAND proce-
dure (§3.3). For larger databases, the query size reductions over
XPIR are 274× when d = 2, and 63× when d = 3.
7.2 SealPIR’s response time and throughput
While microbenchmarks are useful for understanding how
SealPIR compares to XPIR, another important axis is under-
standing how these costs affect response time and throughput.
7.2.1 Response times
To measure response time, we run experiments where we de-
ploy a PIR server in Azure’s US West data center, and place a
PIR client under four deployment scenarios. We then measure
the time to retrieve a 288-byte element using SealPIR, XPIR,
and scp (i.e., secure copy command line tool). We use scp to
represent a client downloading the entire database (naive PIR).
Deployment scenarios
intra-DC: the client and the server are both in the US West data
center. The bandwidth between the two VMs is approximately
3.4 Gbps (measured using the iperf measurement tool). This
scenario is mostly pedagogical since it makes little sense to
use PIR inside two VMs in the same data center controlled by
the same operator. It gives an idea of the performance that PIR
schemes could achieve if network bandwidth were plentiful.
inter-DC: the client is placed in the South India data cen-
ter. The bandwidth between the two VMs is approximately
800 Mbps. This scenario represents clients who deploy their
applications in a data center (or well-provisioned proxy) that
they trust, and access content from an untrusted data center.
home network: the client is placed in the South India data
center. However, we use the tc trafﬁc control utility to conﬁgure
the Linux kernel packet scheduler in both VMs to maintain a
20 Mbps send rate. We choose this number as it is slightly over
the mean download speed in the U.S. (18.7 Mbps) according
to Akamai’s latest connectivity report [1, §4]. This scenario is
optimistic to XPIR since it ignores the asymmetry present in
home networks where the uplink bandwidth is typically much
lower (meanwhile in XPIR, the queries are large). Nevertheless
it gives a rough estimate of a common PIR use case in which a
client accesses an element from their home machine.
mobile network: the client is placed in the South India data
center. We use tc to conﬁgure VMs to maintain a 10 Mbps send
rate. We choose this number as it approximates the average data
speed achieved by users across all U.S. carriers according to
OpenSignal’s 2017 State of Mobile Networks report [2] and
Akamai [1, §8]. As with the home network, this scenario is
optimistic (for XPIR) as it ignores the discrepancy between
download and upload speeds. It represents the use of PIR from
a mobile or data-limited device, which is a common deployment
for applications such as private communication (§7.4).
Results. Figure 10 depicts the results. At very high speeds
(intra-DC), naive PIR (scp) is currently the best option, which is
not surprising given the computational costs introduced by PIR.
In this regime, SealPIR is competitive with both instances of
XPIR, although our implementation falls behind on the largest
database size. The primary issue is that, for a database with
n = 222 elements, our optimization of EXPAND makes the plain-
(cid:2) = 212, see Equation 1 in Section 6).
text modulus very small (t
This causes SealPIR to use many more plaintexts than XPIR.
For even larger databases, since we must use a higher dimen-
sion anyway (§3.5), the difference in the number of plaintexts
between XPIR and SealPIR (for the same d) becomes less
prominent until n is large enough that the second operand in
Equation 1 approaches log(t) again.
When it comes to lower network speeds, XPIR and SealPIR
signiﬁcantly outperform scp. As bandwidth decreases (home,
mobile), SealPIR’s lower network consumption and competitive
CPU costs yield up to a 42% reduction in response time.
972
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:31:59 UTC from IEEE Xplore.  Restrictions apply. 
)
c
e
s
(
e
m
i
t
e
s
n
o
p
s
e
r
 12
 9
 6
 3
 0
216
intra-DC (3.4 Gbps)
SealPIR
XPIR (d=2)
XPIR (d=3)
scp
218
220
# elements
222
)
c
e
s
(
e
m
i
t
e
s
n
o
p
s
e
r
 16
 12
 8
 4
 0
216
inter-DC (800 Mbps)
218
220
# elements
222
)
c
e
s
(
e
m
i
t
e
s
n
o