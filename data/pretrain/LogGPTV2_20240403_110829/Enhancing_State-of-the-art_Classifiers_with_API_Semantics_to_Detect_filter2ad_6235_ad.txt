0.840
0.726
0.718
0.718
0.605
0.635
0.811
0.765
0.627
0.734
w/
0.833
0.791
0.890
0.875
0.908
0.969
0.877
0.680
0.637
0.789
0.814
0.704
0.861
0.748
Drebin
w/
0.878
0.859
0.886
0.869
0.844
0.865
0.867
w/o
0.779
0.734
0.759
0.666
0.767
0.794
0.750
15.6%
Drebin-DL
w/
w/o
0.875
0.819
0.866
0.816
0.878
0.829
0.706
0.916
0.797
0.793
0.874
0.828
0.799
0.868
8.7%
Average
Improves
1 w/o denotes the classifier without APIGraph, i.e. the original classifier.
2 w/ denotes the classifier enhanced with APIGraph.
19.2%
19.6%
APIGraph can capture semantic similarities. Here is how we obtain
the family information for Android malware, which involves three
steps:
‚Ä¢ Step-1: Labeling via Euphony [20].
In this step, we use a
malware labeling tool named Euphony to label the family
information of all the collected 109,770 malware samples (as
described in ¬ß 4.2). Euphony is capable of linking different
family label aliases from all AV engines on VirusTotal. For
example, Euphony can link a family label ‚Äúboxersms‚Äù from one
AV with its alias ‚Äúboxer‚Äù from another. The output of Euphony
is a list of (ùëô, ùë†) pairs, where ùëô is a family label and ùë† is the
number of AVs that support this label.
‚Ä¢ Step-2: Selection of malware with reliable labels.
In this step,
we choose a subset of malware with reliable labels recognized by
most of the AVs. Specifically, we require that the most popular
family label of a malware sample is recognized by at least 50%
AVs and the second popular label is recognized by at most 10%
AVs. That is, we choose the malware with dominant family
labels in our study, which leads to 101,360 malware labeled
with family information, covering 1,120 families.
‚Ä¢ Step-3: Selection of top 30 malware family.
In this step, we
choose top 30 families that have the most number of labeled
samples so that each family has enough samples for evaluation.
As a result, we have 75,625 (74.61%) apps in this experiment
and every family has more than 500 apps (except the last one).
The top 30 families, as well as the number of their samples are
listed in Table 10 of Appendix A.
Here is our evaluation methodology. We first sort all the malware
samples in one family by their appearing time and then divide them
into 10 groups with 10% samples of this family. The appearing
time of all samples in one group is strictly ahead of the one of all
samples in the next. Next we adopt static analysis with the help of
apktool [2] to disassemble malware code and obtain API features.
Lastly, we calculate a feature stability score of every two adjacent
groups using Jaccard similarity (see Equation 2).
ùêΩ(ùê¥, ùêµ) =
|ùê¥ ‚à© ùêµ|
|ùê¥ ‚à™ ùêµ|
(2)
where ùê¥ and ùêµ is the set of used features for two adjacent groups.
This score function is capable of reflecting how stable the features
evolve between groups.
Results: Figure 7 shows the distribution of feature stability scores
for each malware family with API and API clusters as features.
One important observation of the figure is that the feature stability
score of all families with API clusters as features is very close
to 1 and much higher than the one with API as feature directly.
This observation explains that APIGraph can capture malware
evolution as malware developers tend to use semantically similar
APIs to implement the same or similar functionalities.
We also show the breakdown of feature stability scores for four
specific malware families in Figure 8. The feature stability score with
API clusters as features is almost flat without much decrease over
time; by contrast, the feature stability score with independent APIs
not only is low (near 0.75), but also decreases over time (sometimes
to a very low value like 0.3). This, from another angle, also shows
that APIGraph can capture malware evolution over time.
Summary: APIGraph successfully captures semantic
similarity among evolved malware samples in a family.
5.4 RQ4: API Closeness Analysis
In this research question, we measure the closeness of APIs in
the same cluster to demonstrate the effectiveness of APIGraph.
Particularly, we use t-SNE [31] to project all the API embeddings
into a two-dimensional space and visualize them. Figure 9 shows a
subgraph of the visualization, in which those APIs in our motivating
example (Figure 1) are clearly separated into different clusters. For
example, PII-related APIs, such as getDeviceId(), getSubscriberId()
are close to each other; and network-related APIs, such as those
from ‚Äújava.net‚Äù, ‚Äújavax.net‚Äù, ‚Äúandroid.net.Network‚Äù, are also close.
It is worth noting that APIs in the package ‚Äújava.lang‚Äù can be
clearly separated into two groups: one containing security-sensitive
APIs for process management and system command execution, and
the other one containing those Java built-in data structure APIs,
such as java.lang.Long.compare(). This fact demonstrates that a
simple package-level API clustering method, like that adopted by
MamaDroid, is inaccurate in capturing semantics information.
Summary: Semantically-close APIs are grouped in the
same or close cluster in the embedding space by APIGraph.
6 DISCUSSION
API Semantics from Non-official Documents. API semantics
exist and can be extracted from many different places: official
Android API documents are the main source, but other resources,
such as the API tutorial and developer guides [25], can also provide
semantics. We choose the Android API documents as the target
because they are official and contain the most useful information.
Furthermore, it will be hard for an adversary, e.g., a malware
developer, to pollute official API documents and influence the
performance of APIGraph.
Relation/Entity Types. We defined four entity and ten relation
types in API relation graph as a start, which can be extended in
the future to include more types. We believe that current types of
Session 3B: Malware CCS '20, November 9‚Äì13, 2020, Virtual Event, USA766Figure 7: [RQ3] The distribution of feature stability scores for every top 30 malware family, when considering APIs as features
and API clusters as features.
(a) airpush
(b) artemis
Figure 8: [RQ3] Continuous feature stability scores across ten groups of four malware families.
(c) kuguo
(d) smspay
choose to design APIGraph to extract relations from API docu-
ments, because we believe that it is also how malware developers
understand Android APIs and make corresponding changes for
evolutions. We leave it as a future work to extract and compare
relations from Android source code. Furthermore, documentation
mining is not the only way to extract such relations. Other solutions
such as API usage mining with large-scale market apps [14, 38]
may also generate similar relations.
Non-API-based Malware Classifiers. APIs are a popular type
of features widely adopted by many other existing malware
classifiers [1, 11, 12, 22], mainly because APIs are essential in
implementing malware functionalities. There indeed are two types
of classifiers that do not directly adopt APIs as a feature. First,
some classifiers, e.g., Mclaughlin et al. [33], adopt opcodes and
n-gram as features: Although APIs are not explicitly used as a
feature, they are implicitly embedded as part of the opcodes. We
believe that APIGraph can still help such classifiers by transforming
those opcodes to incorporate API cluster information. Second,
some classifiers, e.g., MassVet [10], mainly adopt UI structures for
malware detection. Such classifiers may age quickly given malware
evolution because those features like UI structures are unreliable
and easy to change.
Malware Obfuscation. Android apps may obfuscate themselves
via many techniques, such as Java reflection, packing [15], and
dynamic code loading [16] to bypass existing analysis. This is an
orthogonal problem to what has been studied in APIGraph and
one should refer to existing works [6, 26, 40, 53] for solutions.
Figure 9: [RQ4] Visualizing APIs used in the motivating
example (¬ß2) and the ‚Äújava.lang‚Äù package.
relations and entities have already demonstrated their capability in
grouping semantically-close APIs (see RQ4 in ¬ß5.3) and improving
existing malware classifiers (see RQ1 in ¬ß5.1 and RQ2 in ¬ß5.2).
Threshold in Majority Voting of AVs. We follow DroidEvolver
in adopting 15 AVs as the threshold to label an app as a malware.
Other work [3, 39] may use different thresholds considering their
experimental settings. A recent paper [54] justifies that any number
between 2 and 39 is reasonable.
Relation Extraction from Code Analysis. Some of the relations
extracted from API documents by APIGraph, e.g., ‚Äúreturns‚Äù and
‚Äúthrows‚Äù, are also available in the Android framework code. We
java.net.URL.openConnection javax.net.SocketFactory.createSocketjavax.net.ssl.SSLSocketFactory.createSocketandroid.telephony.TelephonyManager.getDeviceIdandroid.telephony.TelephonyManager.getSubscriberIdandroid.telephony.TelephonyManager.getSimSerialNumberjava.lang.ClassLoader.findClassjava.lang.Runtime.loadLibraryjava.lang.Thread.getAllStackTracesjava.lang.System.getPropertiesjava.lang.String.subSequencejava.lang.Short.intValuejava.lang.Long.compareandroid.net.Network.bindSocketSession 3B: Malware CCS '20, November 9‚Äì13, 2020, Virtual Event, USA767Merits beyond Android. The idea of APIGraph can also benefit
malware detection on Windows and iOS, given their APIs‚Äô semantic
similarity. In the future, we will consider expanding APIGraph to
these tasks.
7 RELATED WORK
Android Malware Classifiers. Machine learning (ML) has been
widely used to detect Android malware in both academic and
industry environments. One popular, yet wildly adopted features
in ML-based Android classifiers is APIs provided by the Android
framework: For example, a majority of previous works [1, 3, 11,
12, 22, 32, 50‚Äì52] rely on APIs used by the apps as the features to
detect malice. Specifically, DroidAPIMiner[1] and DREBIN [3] use
the occurrence of APIs; DroidEye [11] and StormDroid [12] use API
usage frequency; MalDolzer [22] adopts API calling sequences; and
DroidMiner [50], DroidSift [52], and AppContext [51] adopt API
call graph.
It is worth noting that most of prior works treat each API
separately and ignore the semantic relations among these APIs.
MaMaDroid [32] is one of the few exceptions that abstract APIs
to corresponding packages, but such a coarse-grained grouping
cannot effectively capture API semantic relations either. APIGraph
is able to enhance those API-based Android malware classifiers to
capture malware evolution, thus slowing down aging.
Concept Drift and Model Aging. Concept drift is a common
phenomenon in machine learning, where the statistical properties
of the samples change over time. Concept drift causes that machine
learning trained models to fail to work on new testing samples,
which is known as model aging [49], or time decay [39], or
model degradation [24] and deterioration [9] in the literature.
Transcend [21] proposes to use statistic techniques to detect
concept drift before the model‚Äôs performance starts to fall sharply.
Tesseract [39] proposes a new metric named AUT (Area Under
Time) to effectively measure how a model performs over time in
the setting of concept drift. It also points out that when training
and testing models, spatial and temporal constraints must be
satisfied to faithfully reflect the model‚Äôs performance, and in this
paper we exactly follow these constraints to set up our large-
scale dataset. MaMaDroid [32] notices that the change in used
APIs can affect the performance of the trained models, so it
abstracts API calls to their packages and families. However, as
shown in this paper the relations between APIs are much broader
than the package relation, and capturing more relations between
APIs can help MaMaDroid perform better detect evolved malware.
EveDroid [24] and DroidSpan [9] try to find more sophisticated
and distinguishable features in behavioral patterns and information
flow and then build more sustainable models. Unlike these two
approaches that rely on their chosen features and underlying
algorithms, we propose to let models capture relations between
APIs, and our method is more general and can be used to enhance
existing malware classifiers.
As a general comparison, APIGraph is the first work that extracts
the underlying reasons for model aging: That is, Android malware
keeps evolving with similar functionalities but varied implementa-
tions. Therefore, APIGraph, being orthogonal to existing ML-based
approaches like retraining, active learning, and online learning, can
enhance existing ML classifiers to be aware of malware evolution,
thus slowing aging.
Semantics from API Documentation. Knowledge graphs [7,
17] have been successfully constructed and applied to in many
real-world tasks, such as extracting information and answering
questions. Inspired the concept of knowledge graph, we propose
API relation graph to represent the internal relations among diverse
Android programming entities. The major challenges here are that
we need to extract and represent Android specific entities and
relations. Several knowledge graph embedding algorithms have
been proposed, including TransE [8], TransH [47], and TransR [28].
Our API embedding algorithm uses the TransE with some variations
to convert APIs in the relation graph to embeddings.
The Android API reference documents contain abundant infor-
mation about APIs. Maalej et al. [30] have developed a taxonomy
of knowledge types in API reference documents. Based on this
taxonomy, Li et al. [25] use NLP techniques and define templates to
extract API caveats (i.e. facts that developers should know to avoid
unintended use of APIs) from API documents. As a comparison,
the purpose of APIGraph is to extract semantic similarity among
APIs so that such similarities can capture the preserved semantics
during malware evolution.
8 CONCLUSION
Android malware keeps evolving over time to avoid being detected
by existing classifiers. This paper proposes APIGraph to capture
semantic similarity among APIs, called API semantics, and enhance
state-of-the-art classifiers with API semantics so that they can still
classify evolved malware samples. Specifically, APIGraph builds a
so-called API relation graph, coverts each API entity in the graph
to an embedding, called API embedding, and then groups APIs
in the embedding form into clusters. Those clusters are used to
replace each individual API used by state-of-the-art classifiers as
a feature. We applied APIGraph to enhance four state-of-the-art
classifiers and evaluated them using a dataset created by ourselves
which contains more than 322K Android apps ranging from 2012
to 2018. Our evaluation shows that APIGraph can significantly
reduce the number of samples to label in those four classifiers by
32%‚Äì96%. To facilitate any follow-up research, we have publicly
released our dataset and source code at https://github.com/seclab-
fudan/APIGraph.
ACKNOWLEDGEMENT
We would like to thank anonymous reviewers for their helpful
comments. This work was supported in part by the National Natural
Science Foundation of China (U1636204, U1836210, U1836213,
U1736208, 61972099), Natural Science Foundation of Shanghai
(19ZR1404800), and National Program on Key Basic Research
(NO. 2015CB358800). The authors from Johns Hopkins University
were supported in part by National Science Foundation (NSF)
grants CNS-18-54000. Min Yang is the corresponding author, and a
faculty of Shanghai Institute of Intelligent Electronics & Systems,
Shanghai Institute for Advanced Communication and Data Science,
and Engineering Research Center of CyberSecurity Auditing and
Monitoring, Ministry of Education, China.
Session 3B: Malware CCS '20, November 9‚Äì13, 2020, Virtual Event, USA768REFERENCES
[1] Yousra Aafer, Wenliang Du, and Heng Yin. 2013. DroidAPIMiner: Mining API-
level Features for Robust Malware Detection in Android. In Proceedings of the
International Conference on Security and Privacy in Communication Systems
(SecureComm). Springer, 86‚Äì103.
[2] Apktool. 2019. A Tool for Reverse Engineering Android APK Files. https://ibot
peaches.github.io/Apktool/.
[3] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, Konrad Rieck,
and CERT Siemens. 2014. DREBIN: Effective and Explainable Detection of
Android Malware in Your Pocket.. In Proceedings of the Network and Distributed
System Security Symposium (NDSS). 23‚Äì26.
[4] Kathy Wain Yee Au, Yi Fan Zhou, Zhen Huang, and David Lie. 2012. PScout:
Analyzing the Android Permission Specification. In Proceedings of the 2012 ACM
Conference on Computer and Communications Security (CCS). ACM, 217‚Äì228.
[5] Michael Backes, Sven Bugiel, Erik Derr, Patrick McDaniel, Damien Octeau,
and Sebastian Weisgerber. 2016. On Demystifying the Android Application
Framework: Re-visiting Android Permission Specification Analysis. In Proceedings