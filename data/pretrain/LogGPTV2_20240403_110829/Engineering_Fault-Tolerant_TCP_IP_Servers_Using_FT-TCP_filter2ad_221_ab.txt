Special treatment of send() and recv() allows us to ef-
ﬁciently pass client data from the stable buffer directly into
the application, without having to feed the packets through
the backup’s TCP. This means that as far as backup’s TCP
is concerned, the connection to the client during this period
is idle.
If an invocation on the primary returned an error code,
it is important to return the same code on the backup. In
particular, if a non-blocking read() returns an error indi-
cating the lack of any data to return, it is important to return
that error on the backup even if packets with new data have
arrived by the time this syscall is invoked on the backup.
As we’ll show in Section 3.3, this is an important source of
nondeterminism.
The last category of syscalls can be quite complex in
their semantics and side-effects, which is why we consider
dealing with them a separate problem outside the scope of
this paper. For now we just allow them to execute on the
backup and assume that they will have the same effect and
will return the same results as the calls on the primary. This
assumption turned out to be valid for the two applications
that we considered.
3.2.2 Recovery
When the backup does not hear from the primary for a cer-
tain amount of time it assumes that the primary has crashed
and initiates failover (also see Section 5.4 for more details
of failure detection and recovery). One of the key advan-
tages of the hot backup approach is speed; it only requires
bringing the backup process up to speed by processing any
packets and syscalls that the backup received before the pri-
mary failed, and then promoting the backup to be the pri-
mary.
A number of techniques can be used to reconcile the dif-
ference in IP addresses of the primary host and the pro-
moted backup.
In the current implementation, the SSW
switches the backup’s real IP address for the old primary’s
address on all outgoing packets and performs the reverse
on all the incoming client packets, effectively functioning
as a NAT. To be able to see the incoming packets (that are
destined to a different MAC address), we place the network
interface card into promiscuous mode. If some technique
for permanently changing the IP address of the entire host
is used then using promiscuous mode is not necessary.
Another difference in TCP connection state between the
primary and the backup is in the sequence numbers that they
use. TCP connection on the backup is idle during normal
operation (since all the data are injected through the NSW),
so its sequence numbers stay at their initial values. After
failover the sequence numbers must be adjusted by the SSW
on all packets as follows: incoming sequence numbers are
shifted by the number of bytes the server read prior to fail-
ure and the outgoing ones are adjusted by the difference
between backup server’s initial sequence number and the
sequence number of the last byte sent to the client.
Recovery with a cold backup essentially consists of re-
doing the actions performed by a hot backup during nor-
mal operation followed by the actions performed by it dur-
ing failover. First, a new server process is started on the
backup host and a connection to it is spoofed by the SSW.
That process then consumes buffered packets and syscalls,
and eventually takes over the connection after the IP ad-
dress and sequence number adjustments that were described
in the paragraph above. Naturally, processing the buffered
data takes time, hence recovery with a cold backup is con-
siderably slower than with a hot backup.
3.3. Nondeterminism
To keep replicas running deterministically, it is not suf-
ﬁcient to give applications identical input. Error conditions
and asynchronous events must be delivered consistently,
too.
In our earlier work [2] we considered the number of
bytes returned by a read() (which we called a readlength)
as a potential source of nondeterminism in real applications.
Indeed, one can imagine an application that would perform
a different action based on how many bytes were returned
by a read(). And yet, when we purposefully returned
different number of bytes on the primary and the backup,
the two replicas behaved identically under both DSS and
Samba. We believe that the reason is that both services ei-
ther only process messages of a known size—either proto-
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:01:39 UTC from IEEE Xplore.  Restrictions apply. 
col header messages or data-carrying messages whose size
is known from a preceding header message—or process a
stream of data until it is drained.
If a server processes
messages of a known size and receives less than the ex-
pected number of bytes then it waits until more bytes are
available—it does not process the message until it is com-
plete.
Although applications tend to behave the same way
while there are data to be received, they typically switch to
a different task when no data are available. Our experience
with DSS and Samba showed that capturing and replaying
the value of syscalls that returned the status of a socket—
namely select() and poll()—was necessary for en-
suring deterministic execution. If, for example, a poll()
on the primary indicated that there were data to be read,
then it would go ahead and read those data; but if at the
same point the backup was told by poll() that were no
data, it may yield the CPU to a different thread, leading the
backup process down a different execution path. Therefore,
poll() must return the same result on both replicas.
There are two other cases. Just like poll(), a non-
blocking read() has the ability to indicate the lack of
data in a socket buffer (by returning -1 with errno set to
EAGAIN), and that is why we consider readlengths of -1
as a source of nondeterminism. When a number of pro-
cesses compete for a ﬁle lock, there is a good chance they
won’t all acquire it in the same order on the primary and on
the backup. This means there will be processes for which
lock acquisition will succeed on the primary, but will fail on
the backup or vice versa. For some applications—the ones
written to retry lock acquisitions indeﬁnitely—this may not
pose any problems. But for others, all lock requests must
return the same results on both replicas.
Thread scheduling and signal handling are both com-
monly identiﬁed as sources of nondeterminism, too. Nei-
ther proved to be problematic for the two services that we
evaluated. That is not to say that a service like Samba does
not use signals (in fact, we know that it does from looking
at its source code), but that they do not occur often enough
to warrant immediate attention. Someone building a com-
mercial fault-tolerant TCP system would certainly have to
capture and replay signals at the appropriate times in the
execution path using a technique similar to the one used by
the Hypervisor[3].
One source of nondeterminism we had to address was
introduced by the servers themselves. This happens when
a server generates a random value and then uses that value
in communications with the client. In the next section we
will show how we modiﬁed the server applications to en-
sure that identical random values are generated on the pri-
mary and on the backup. In the future, to avoid source code
modiﬁcations we are considering using a protocol-speciﬁc
“hook” to capture randomly generated values and make the
appropriate substitutions.
4. Applications
To see if FT-TCP could be used to replicate non-trivial
applications easily, we tested our system with two complex
and well-known TCP/IP services.
We chose the Darwin Streaming Server (DSS) that
serves multimedia content such as QuickTime movies and
the Samba server that implements Microsoft’s ﬁle and
printer sharing protocols. These two services differ in their
general structure—for example, Samba spawns a separate
process for each client connection while DSS handles all
connections in a single thread. We discuss such structural
details below in the two sections on the individual servers.
Besides their popularity, these applications were attractive
because they tend to have long-lived connections (which are
worth recovering) and their source code was publicly avail-
able.
We don’t wish to imply that by having run these two ser-
vices under FT-TCP, we have a complete understanding of
the impact of service structure on our approach. Both DSS
and Samba are relatively simple services.
It does show,
though, that the approach can be used at least for some re-
alistic services.
4.1. Darwin Streaming Server
DSS is currently available under an open source software
license from Apple Computer, Inc. Although it is gener-
ally considered better to stream multimedia over datagram-
based protocols like UDP, streaming is frequently done over
TCP to bypass ﬁrewalls. In both cases the stream is encap-
sulated inside the Real-Time Streaming Protocol (RTSP).
DSS runs as one process with at least three main threads:
one for doing all network communication, one for servicing
requests, and one auxiliary thread. The application is event-
driven and all I/O is done asynchronously. For each viewing
session there are at least two connections: one for control
of a stream and one for the stream itself. The streams live
at least as long as they are being played, and the connec-
tion state indicates the position in the stream. Hence, if a
failure causes the connection to fail, then the client needs to
re-open the connection and re-position the playback point in
the stream. Our viewer has application-level recovery: it re-
members where the playback of the stream left off and repo-
sitions for the client when “play” button is pressed again.
DSS is an interesting service to consider because it uses
multiple connections per client and also because it is a
multi-threaded application. It has some attributes that make
it less challenging. In particular, it only reads ﬁles, making
the output commit problem only an issue with the playback
of the stream. Additionally, it generates a large amount of
output data in response to small requests, thus reducing the
load on the buffering mechanism.
We ran an unmodiﬁed version of DSS on top of FT-TCP
to explore its sources of nondeterminism. NSW detected a
nondeterministic diversion between the primary and backup
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:01:39 UTC from IEEE Xplore.  Restrictions apply. 
almost immediately. This nondeterminism occurred when
the server generated a random Session ID that was sent to
the client in response to a SETUP request of the RTSP pro-
tocol. The ID is used for all further communication in a ses-
sion. If the primary and the backup generate different IDs,
then all requests from the client will be rejected because
of an invalid ID. To generate the same IDs while keeping
the protocol cryptographically secure, we retained the calls
to a pseudo-random number generator, but made sure that
the values used to compute the seed are derived from the
syscalls whose return values we insert on the backup, such
as gettimeofday(). After we changed the source code
of DSS to make sure identical IDs were generated, we saw
no further execution deviations between the primary and
backup servers.
4.2. Samba Server
Samba server implements Microsoft’s family of proto-
cols for sharing ﬁles and printers, such as SMB and the
newer CIFS. These protocols were originally designed to
run over LAN transport protocols, but these days they use
TCP/IP almost exclusively.
A new Samba process is spawned by the inetd dæmon
for each incoming connection. Connections typically last a
long time—for as long as a remote ﬁle system is mounted on
the client. Clients that we are familiar with mask connection
failures if they occur during idle periods (no outstanding
requests) by reconnecting to the service upon the next user
command. If, however, a connection is broken during an
active transfer, the transaction is abandoned and an error is
raised.
We found two sources of nondeterminism in Samba. The
ﬁrst one has to do with the challenge-response authentica-
tion scheme used for access control, in which the server
generates a random challenge string that the client encrypts
with a password and passes back to the server for compar-
ison. Obviously, if the random challenges generated by the
replicas are different, then the response from the client will
only succeed in authentication on the primary, while the
backup will reject that connection. The second source of
nondeterminism, similar in principle to the Session ID in
DSS, was generation of a ﬁle handle for each ﬁle opened by
a client, who then uses it in all ﬁle operations. As with DSS,
we changed the code to make sure that the same challenges
and the same ﬁle handles were generated on the primary and
on the backup, taking care to preserve the cryptographic in-
tegrity of the protocol. After that we saw no further execu-
tion deviations in any of our experiments.
5. Performance
FT-TCP is implemented as a kernel module for ver-
sion 2.2.19 of Linux. We ran it on two identical 266MHz
Pentium II workstations with 512Kb cache and 256Mb of
RAM, while all clients ran under Linux 2.4.18 on a 2GHz
Pentium IV with 512Kb cache and 1Gb of RAM. The client
host was connected to the servers with a 10Mbps half-
duplex broadcast Ethernet segment, and the servers also
had a separate 100Mbps half-duplex Ethernet link between
them. This basic architecture is used by many commer-
cial fault-tolerant cluster systems and is therefore the most
likely setting for FT-TCP deployment. All network inter-
face cards were Intel EtherExpress 10/100. Since it is com-
mon for clients to encounter a bandwidth bottleneck on the
link to the service, we consider our setup adequate for eval-
uation of FT-TCP performance from the point of view of a
typical client.
In the next section we present results obtained during
experiments without any failures and then discuss failure
and recovery process in Section 5.4.
5.1. Failure-free Operation
As was discussed in Section 3.2.1, during failure-free op-
eration FT-TCP on the primary buffers incoming packets
and syscall results (readlengths are treated separately from
other syscalls in the measurements that follow). To deter-
mine exactly how much overhead is introduced by inter-
ception of syscalls, by buffering of packets, and by output
commit stalls, we ran FT-TCP in three different modes:
• Immediate - Packets and readlengths are buffered, but
FT-TCP does not perform output commit stalls. In this
mode recovery cannot be guaranteed and it is only use-
ful for the purposes of evaluating the minimal over-
head imposed by FT-TCP’s interception and buffering
mechanisms.
• PR - (P)acket and (R)eadlengths are buffered and out-
put commit stalls take place, adding to the overhead.
If an application can run deterministically without in-
terception of syscalls then PR mode is sufﬁcient for
correct operation.
• PRS - (P)ackets, (R)eadlengths and (S)yscalls are
buffered. This is the full-ﬂedged mode of FT-TCP op-
eration that allows replication of arbitrary programs.
By comparing these results with PR we were hoping
to infer the additional overhead of intercepting syscalls
and stalling on output commit on their behalf.
All three modes are compared to performance of the ser-
vice without FT-TCP—labeled Clean TCP—which is the
optimal performance in our case. Throughput values were
computed by timing large (4Mb) data transfers and averag-
ing the results over 20 runs. Care was taken to ensure that
each run started in the same initial state (i.e. a ﬁle transfer
started with a cold disk cache). Error bars in graphs rep-
resent conﬁdence limits for the mean for a 95% conﬁdence
interval.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:01:39 UTC from IEEE Xplore.  Restrictions apply. 
1000
1000
1100.10
+/- 1.70
1101.56
+/- 1.75
1101.37
+/- 2.41
1013.11
+/- 2.69
996.06
+/- 4.46
996.28
+/- 4.49
1035.61
+/- 1.01
1016.39
+/- 1.21
1015.67
+/- 1.53
870.38
+/-13.10
859.66
+/- 8.82
866.05
+/- 6.58
Clean TCP
Cold Backup
Hot Backup
)
)