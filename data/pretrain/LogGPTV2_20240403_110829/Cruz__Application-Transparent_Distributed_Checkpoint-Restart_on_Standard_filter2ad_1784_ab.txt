A socket’s receive buffer stores
application-level byte stream data which has been received
from the network but is not yet delivered to the applica-
tion. To checkpoint this data, we extend the Zap imple-
mentation to call the socket receive system call on behalf
of the application. Since checkpointing should be a non-
destructive operation which allows the checkpointed pro-
cess to resume execution immediately after the checkpoint
operation is complete, the MSG PEEK option is passed to
socket receive to read but not remove data from the buffer.
Linux lacks a system call interface that would allow Zap
to access the contents of socket send buffers. Therefore, we
obtain this data by directly walking the send buffer’s ker-
nel data structure. The data packetization that is indicated
in the send buffer must be preserved across checkpoint and
restart because the Linux TCP stack expects ACK sequence
numbers to correspond to packet boundaries. We extend
Zap to read and save the application-level data found in the
send buffer and record the packet boundaries, which are pre-
served on restart as described later.
Finally, Zap reads the TCP connection state directly from
the socket data structure. As we describe below, when an
application is restarted from a checkpoint image, its sockets
are re-created with empty buffers before the saved socket
buffer data is restored. Therefore, the checkpoint proce-
dure saves a modiﬁed version of the TCP connection state
which reﬂects an empty receive buffer in which the current
contents have been successfully delivered to the application,
and an empty send buffer in which the current contents have
not yet been issued by the application to the OS. The neces-
sary modiﬁcations to the connection state are minimal and
consist of changing the values of two sequence numbers in
the saved copy of the socket data structure.
When restarting an application from a saved checkpoint
image, Zap creates the number of sockets that were check-
pointed. These new sockets are initialized with the saved
TCP connection state which indicates empty send and re-
ceive buffers. We extend Zap to issue a sequence of socket
send system calls to restore the saved sequence of send
buffer data blocks. To preserve the original send-side packet
boundaries, we issue individual send operations for the data
associated with each packet.
In addition, the mechanism
temporarily sets the socket TCP options to disable the Na-
gle algorithm and other mechanisms that could change the
packet boundaries (e.g., TCP CORK in Linux) before issu-
ing the send system calls.
It is cumbersome to insert application-level byte stream
data into socket receive buffers, which are designed to re-
ceive packet data directly from the network. Therefore, we
extend Zap to restore this data for each socket by copying it
to an alternate buffer which the mechanism allocates for the
socket in kernel address space. Zap’s system call intercep-
tion mechanism is conﬁgured to intercept the socket receive
system call such that data stored in the socket’s alternate
buffer is transparently delivered to the application when it
issues a receive call for the socket. The interception code
checks if the socket’s alternate buffer is empty, and if so in-
vokes the original socket receive system call to deliver data
from the socket’s receive buffers. As a performance opti-
mization, the interception of the socket read system call is
removed when the alternate buffers for all sockets become
empty. If a checkpoint is initiated when the alternate buffers
are not empty, data in the alternate buffers and any data in
the socket receive buffers are both retrieved through the in-
tercepted socket read system call. Data from both buffers
are concatenated and saved in the checkpoint. This mech-
anism allows a checkpointed application to transparently
continue network communication with other processes af-
ter restart. While network packets can be dropped or be
received multiple times across checkpoint and restart, the
underlying TCP protocol handles these cases transparently.
We have veriﬁed that our current implementation works
correctly for multiple kernel versions (e.g., Linux 2.4.20
and Linux 2.4.25). In general, however, the representation
of socket data structures in a new kernel version could be
different to the extent that our implementation would have
to be ported to accommodate the changes. Porting effort can
be minimized if OSes can be extended with a small set of
new interfaces to provide high-level access to internal net-
work state (e.g., sequence numbers). Such extensions have
been proposed previously [10] and we are investigating their
feasibility.
4.2. Network Address Migration
We have extended Zap to support persistent, externally
routable addresses to pods. This is accomplished by attach-
ing to each pod a virtual network interface (VIF) which is
the only network interface that is visible to processes within
the pod. The VIF can be assigned a network-visible IP ad-
dress and an ethernet MAC address. When a pod is mi-
grated, its VIF is deleted at the original host and a new VIF
is created at the destination host. The new VIF is attached
to the migrated pod and is assigned the same addresses as
the original VIF. This enables remote processes to continue
communicating with a migrated process even if the remote
processes are not under control of Zap. Since the IP address
assigned to a pod is visible to the network, this approach re-
quires the source and destination of migration to be within
the same routing domain (e.g., the same IP subnet). Several
operating systems, including Linux, support the creation of
VIFs and assignment of IP addresses to VIFs. Our exten-
sions to Zap use this feature to provide VIFs for pods.
A pod’s VIF can be assigned a unique static IP address
by the system administrator or alternatively it can be as-
signed a dynamic IP address if a DHCP client process run-
ning in the pod queries a DHCP server on the network. The
DHCP client composes a query message which contains the
MAC address of the pod’s VIF. The DHCP server associates
the MAC address with an appropriate IP address assignment
which it returns to the client. Since both the IP and MAC
addresses for a pod are migrated along with the pod, migra-
tion is transparent to both the DHCP server and the client.
Since multiple VIFs may share a physical ethernet inter-
face, a pod’s VIF can be assigned a unique network-visible
MAC address that can be migrated with the pod (as dis-
cussed above) only if the ethernet hardware supports mul-
tiple MAC addresses or if it can be placed in promiscuous
mode. Otherwise, all VIF’s that share a physical interface
must share its MAC address, and this MAC address cannot
be migrated with a pod (since other pods which are not be-
ing migrated are using it). We have developed an alternate
solution for such environments. With this solution, when a
pod migrates, the pod’s VIF starts using a different physi-
cal interface with a different MAC address even though it
keeps the same IP address it had before the migration. The
standard Address Resolution Protocol (ARP) is used to up-
date the network about this new mapping of IP address and
MAC address. If the IP address is static, this is sufﬁcient.
However, for dynamic IP addresses, the DHCP server uses a
MAC address speciﬁed in the payload of the DHCP request
to identify the client and renew its lease for the IP address.
Unless this MAC address is preserved across migration, the
dynamic IP address assigned to the client will change at the
end of the lease causing active network connections to be
lost. To avoid this problem, we ensure that the DHCP client
uses a fake MAC address which is preserved across migra-
tion. This is achieved by extending Zap to intercept network
device-related ioctl calls to provide a virtualized view of
network hardware. In particular, the SIOCGIFHWADDR re-
quest type is intercepted to return the fake MAC address of
an interface. The DHCP client invokes this request and em-
beds the fake MAC address in its DHCP request message.
A primary implementation challenge for our approach is
to conﬁne a pod’s processes to use only the pod’s VIF for
accepting incoming network connections and for initiating
outgoing network connections. To prepare a socket to listen
for incoming connections, a process issues the bind sys-
tem call and gives a local network address as the argument,
or else speciﬁes that the socket can bind to any local IP ad-
dress. To ensure that the calling process can only accept
connections that are incoming to the pod’s IP address, we
extend Zap to intercept the bind system call using a sim-
ple wrapper function which checks if the calling process is
in a pod, and if so replaces the network address argument
with the IP address of the pod’s VIF.
A process initiates an outgoing network connection by
calling the connect system call with arguments specify-
ing the remote network address and the socket to connect.
The connect system call implicitly binds the socket to a
local network address (IP address and free IP port) which is
chosen by the OS. We extend Zap to intercept the connect
system call using a simple wrapper which invokes bind
prior to the original function that implements connect.
The wrapper ensures that sockets in a pod are bound to the
pod’s IP address on a free port.
5. Checkpoint-Restart of Distributed Processes
The mechanisms described so far allow application state
of processes in a single pod to be checkpointed and restored
atomically. For a parallel application with processes run-
ning on multiple machines, the checkpoint and restart of the
application’s processes on each machine must be orches-
trated so that the global application state is consistent [2].
For simplicity, in the following description we use the terms
“node” and “pod” interchangeably to refer to the set of pro-
cesses on a machine that are part of the distributed applica-
tion. Chandy and Lamport [2] have shown that a global
checkpoint state is consistent if it satisﬁes the following
properties: 1) if any node’s state indicates a message has
been received, the sending of the message must be reﬂected
in the state of the sender, and 2) if any node’s state indi-
cates a message has been sent but the state of the intended
recipient does not indicate that the message has been re-
ceived and the communication channel is reliable, then the
message must be saved as part of the state of the channel
between these two nodes. Coordinated checkpointing is a
well-known technique for consistent checkpointing which
we employ in our solution. Pros and cons of coordinated
checkpointing and alternative approaches are well docu-
mented in the literature so we do not discuss them here [5].
With coordinated checkpointing, nodes checkpoint con-
currently and employ a coordination protocol to ensure their
checkpoint states are globally consistent. To guarantee the
ﬁrst consistency property, the protocol prevents any mes-
sage sent after a node has completed its checkpoint from
becoming part of the receiver’s checkpoint state. To guar-
antee the second consistency property, the protocol also
ensures all messages in transit over the channel are saved
(the communication channel is reliable in most environ-
ments through the use of TCP). Prior coordinated check-
pointing implementations had no means to capture the state
of the systems implementing the communication channel
(e.g., TCP state, state of network switches). Consequently,
their protocols require each node to exchanges markers with
every other node to ﬂush in-transit messages to the receiver,
where they are saved. Our single node checkpoint-restart
mechanism can save and restore the state of TCP which im-
plements the reliable communication channel (Section 4.1).
With the TCP state included as part of the checkpoint state,
the uncaptured in-transit messages constitute only the state
of the unreliable communication channel (state on network
switches and routers). Since the state in this unreliable com-
munication channel can be ignored without violating the
consistency requirements, we develop a simpler coordina-
tion protocol that does not ﬂush in-transit messages.
The steps in our coordinated checkpoint algorithm are
speciﬁed at a high level and shown by illustration in Fig. 2.
When a parallel application must be checkpointed,
the
Checkpoint Coordinator notiﬁes a Checkpoint Agent on
each machine on which the application is running. Each
Agent reacts to the notiﬁcation by disabling all network
communication for the local pod that hosts the application4
(in Linux, for example, the Agent can add a netﬁlter rule
which ensures that all trafﬁc to or from the local pod is
silently dropped). This step isolates the local pod’s state and
prevents it from being changed by pods that compose the
distributed application on other machines. The Agent next
uses our single-node checkpoint mechanism to save the lo-
cal pod’s state independently. Since the pod’s state includes
TCP state, any dropped messages will be re-transmitted by
TCP when normal execution resumes. When all pods have
successfully checkpointed their individual states, the set of
saved states constitutes a consistent global state of the sys-
tem. Once the Checkpoint Coordinator is informed by all
Agents that local checkpoints have been successfully com-
pleted, it notiﬁes each Agent to allow the pods to resume
execution. Each Agent re-enables communication for its lo-
cal pod (by undoing the netﬁlter conﬁguration rules, in the
case of Linux) and allows the application processes in the
local pod to run.
Coordinated restart operates similarly except, of course,
state is restored from checkpoint instead of being saved.
It is necessary to disable communications as the ﬁrst step
of restart even though application processes have not been
restored at that point. This prevents application messages
from being sent on the network prematurely before the ap-
plication state is restored fully on all pods. If communica-
tion were not disabled, the OS would resume transmitting
messages on the network as soon as the TCP connection
state of the pod is restored. These messages may arrive at
a pod before the appropriate connection state has been re-
stored because each pod restores its state at a different rate.
This would result in an error which would destroy the con-
nection and cause application failure. This situation is pre-
vented by disabling communications as the ﬁrst step. When
all pods have completed restoring their state, each pod can
be notiﬁed to resume their operation (enable communica-
tions and allow processes to run).
The simplicity of our mechanism is readily apparent.
4Since the Agent on each machine runs outside of the application’s pod,
this step does not disrupt communication between the Checkpoint Agent
and the Checkpoint Coordinator.
Checkpoint Coordinator:
Step 1: Send  message to all Agents.
Step 2: Wait to receive  from all Agents.
Step 3: Send  message to all Agents.
Step 4: Wait to receive 
from all Agents.
Checkpoint Agent:
(when  message is received)
{
Step 1: Configure ipfilter to drop
packets to/from local pod.
Step 2: Stop local pod’s processes and take
local checkpoint.
Step 3: Send  to Coordinator.
Step 4: Wait to receive .
Step 5: Resume stopped processes in
local pod.
Step 6: Configure ipfilter to allow
packets to/from local pod.
Step 7: Send  to
Coordinator.
}
Coordinator
Node 1
Node 2
Node N
Time
checkpoint
done
continue
saving state
continuing
execution
Checkpoint latency
Figure 2. Global Coordinated Checkpoint Al-
gorithm.
Previous mechanisms implement all-to-all communication
protocols which either send messages on each pairwise
channel to ﬂush all in-transit messages [17][1] or exchange
messages between every pair of nodes to estimate the
amount of data that must be received before checkpoint
can proceed [15]. Our approach eliminates this complex-
ity. The state of communication channels, i.e., packets that
are in-ﬂight in the network, is not saved. Instead, packets
received from the network after communication has been
disabled are silently dropped at the lowest levels of the OS
network stack. Since the checkpoint state of each pod in-
cludes the state of its TCP connections, any dropped mes-
sages will be automatically recovered by TCP’s reliable
message protocol during normal execution (when the ap-
plication continues computation after checkpoint or when
the application restarts from this checkpoint state). Co-
ordinated restart is similarly simpliﬁed over previous im-
plementations, since our approach neither requires the ex-
change of application-level messages to discover the new
locations of processes nor the establishment of new con-
nections between every communicating pair of processes.
As we will discuss shortly, the simpler implementation also
improves performance and scalability.
The coordination algorithm we have described (Fig. 2)
Sender
unack_nxt
snd_nxt
send buffer
S1 S2 S3   S4   S5 S6 S7   S8 S9 S10 S11 S12
Sequence
Numbers
Receiver
rcv_nxt
Figure 3. TCP communication of a pair of pro-
cesses. All pointers advance to the right.
can be extended in a straightforward way to tolerate Coor-
dinator and Agent failures. We have not included these ex-
tensions, which are well-known, to highlight the simplicity
enabled by the capability to migrate network state.
5.1. Correctness Discussion
Our coordinated checkpoint-restart approach leverages
the reliable delivery properties of TCP. TCP uses packet
sequence numbers, acknowledgements, send buffers5, and
timers to ensure exactly-once and in-order message deliv-
ery. Figure 3 shows a simplistic representation of the key el-
ements of TCP state at the sender and receiver. The sender’s
TCP state maintains two pointers into the sequence number
stream: unack nxt which is the smallest unacknowledged
sequence number and snd nxt which is the sequence num-
ber that will be used for the next packet that is sent. All
data packets with sequence number less than unack nxt
have been successfully acknowledged. Data packets with
sequence numbers from unack nxt and less than snd nxt
have been sent but are not yet successfully acknowledged.
These packets are maintained in the send buffer. The re-
ceiver maintains a variable rcv nxt which is the next se-
quence number that it expects to receive. Data packets with
sequence numbers less than rcv nxt have been received and
an acknowledgement packet has been sent for them. During
normal TCP operation the predicate
unack nxt ≤ rcv nxt < snd nxt
(1)
is invariant. This invariant, the send buffers, and TCP’s
retransmission protocol collectively guarantee that all data
will eventually be successfully received by the receiver and
successfully acknowledged.
Our mechanism for saving and restoring network state
(Section 4.1) saves unack nxt, snd nxt, and the send
buffers of packets with sequence numbers from unack nxt
and less than snd nxt at the sender. This state is saved for
every connection but we focus our attention on one of these
connections without loss of generality. Our mechanism also
saves and restores rcv nxt at the receiver. However, the
sender and receiver states are saved and restored on differ-
ent physical nodes and thus at different times. If we can
5Receive buffers are not central to the reliable properties of TCP
show that the previous invariant is maintained in any global
checkpoint state in spite of the asynchrony, that would guar-
antee that all messages will be successfully delivered dur-
ing continued operation from this global state (whether the
operation continues after the checkpoint completes or af-
ter restarting from this saved checkpoint state). This would
effectively satisfy the sufﬁcient conditions described in [2]
proving the consistency of the checkpoint and restart.