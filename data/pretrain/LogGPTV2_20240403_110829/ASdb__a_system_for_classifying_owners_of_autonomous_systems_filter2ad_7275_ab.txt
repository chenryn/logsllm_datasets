labeled using NAICS, researchers constructed additional codes to
capture finer granularity than NAICS supported during the labeling
process, and as such this translation can be done automatically.
Researchers do one additional review pass to ensure the resulting
categories are accurate and fully descriptive.
We translate other data sources‚Äô custom classification schemes
into NAICSlite using a manual process, with each mapping reviewed
by at least two researchers.
3.3 Data Source Evaluation
Our translation layer provides us with a common denominator
against which to formally evaluate business databases, website
classification services, and existing AS datasets. We evaluate the
data sources in Table 1 across three metrics: coverage, recall, and
precision. We show that while existing data sources are able to
705
Top levelComplete OverlapLow level    Top level1 OverlapLow level0.00.10.20.30.40.50.60.70.80.91.0Fraction of ASes71%31%41%18%92%78%78%73%NAICSliteNAICSIMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Maya Ziv, Liz Izhikevich, Kimberly Ruth, Katherine Izhikevich, and Zakir Durumeric
Name of Dataset
Gold Standard
Number of ASes
150
Sampling Process
Random
Uniform Gold Standard
320
ML training set
New test set
225
150
Uniformly sub-sampled across
all 16 NAICSlite Layer 1 categories
150 random, 75 D&B-labeled
hosting providers
Random
Use of Dataset
To provide a ground-truth for evaluating external datasources and ASdb
(Section 3.2)
To uniformly evaluate each data source
across all NAICSlite categories (Section 3.3)
To provide sufficient hosting-class
balance to train a machine learning classifier (Section 4.1)
To provide a fairer evaluation of how
ASdb performs when deployed at scale (Section 5.2)
Table 2: Labeled Ground Truth‚ÄîWe use four unique sets of labeled autonomous systems to evaluate external data sources and ASdb.
Source
D&B
Crunchbase
ZoomInfo
Clearbit
Zvelo
PeeringDB
IPinfo
All - ZI, CL
Coverage
122/148 (82%)
55/148 (37%)
101/148 (68%)
91/148 (61%)
138/148 (93%)
22/148 (15%)
45/148 (30%)
148/148 (100%)
Tech
73/96 (76%)
28/96 (29%)
55/96 (57%)
77/96 (80%)
86/96 (90%)
21/96 (22%)
37/96 (39%)
96/96 (100%)
Non-Tech
49/52 (94%)
27/52 (52%)
46/52 (88%)
57/52 (90%)
52/52 (100%)
1/52 (2%)
8/52 (15%)
52/52 (100%)
Table 3: External Data Source Coverage‚ÄîZvelo and D&B
achieve the highest gold standard coverage. We include in the cov-
erage count only database entries with classification metadata from
each datasource. Percents are given out of the 148 gold standard
ASes that researchers were able to assign a NAICSlite label to.
achieve promising coverage and precision when categorizing non-
technology organizations, they are significantly worse at differenti-
ating the most common AS-owning technology organizations: ISPs
and hosting providers. We address these weaknesses by building a
machine learning framework in Section 4.
Coverage. Dun & Bradstreet (a business database) and Zvelo (a
website classifier) have the highest coverage on our Gold Standard
ASes, labeling 82% and 93% of ASes, respectively (Table 3). Zvelo
and D&B also provide the most unique coverage, each being the
sole providers of coverage for 7/150 and 2/150 ASes, respectively.
Neither result is inherently surprising. D&B is one of the oldest,
most well known, and most respected business databases. Zvelo‚Äôs
unique coverage is likely because it operates a real-time website
classifier. Crunchbase focuses more on startups and specifically US
companies and has the lowest coverage of any business database at
37%. The two networking databases, IPinfo and PeeringDB, have
by far the worst coverage at 30% and 15% respectively.
All business data sources consistently provide higher coverage
for non-technology entities. As shown in Table 3, while PeeringDB
and IPinfo classify a maximum of 15% of all non-technology entities,
all other data sources classify at least 52%. On the other hand, net-
working data sources (i.e., PeeringDB and IPinfo) provide 2‚Äì8 times
more coverage for technology entities, but provide far less over-
all coverage. No other data source provides significant additional
unique coverage or significantly better coverage of any specific
regions or categories (per a two-sided hypothesis test with a Bon-
ferroni correction) when compared to the union of Zvelo and D&B.
Recall and Precision. We evaluate each data source‚Äôs recall and
precision: recall to understand if datasets are capable of providing
‚Äúaccurate coverage‚Äù of different AS industry sectors, and precision to
understand the trustworthiness of the labels applied by data sources.
We map each data source‚Äôs classification system to NAICSlite as de-
scribed in Section 3.2 and define a match to be accurate if there exists
at least one NAICSlite category overlap between the Gold Standard
and data source. While this metric does not account for false posi-
tives, we note that 80% of data source matches assign only one cate-
gory and a maximum of seven categories are assigned to a single AS.
The data sources with the highest overall layer 1 recall (96%) are
D&B and IPinfo (Table 4), with PeeringDB coming in a close third
at 95%. IPinfo and PeeringDB also have the highest precision at 96%
and 95%, respectively. However, we emphasize that PeeringDB and
IPinfo provide coverage for a very small subset (< 5) categories, lim-
iting their applicability to industry classification. The data sources
with the worst recall are Clearbit (34%) and ZoomInfo (70%), which
also exhibit the worst precision: 55% and 66%, respectively.
For 99% of ASes in our Gold Standard, at least one data source
accurately categorizes the AS. However, given that AS categories
are not uniformly distributed, with 64% of ASes being owned by
technology-related entities, we separately evaluate technology and
non-technology ASes.
Technology Companies. About two thirds of ASes belong to
technology companies. The majority of data sources do well at
accurately distinguishing tech vs. non-tech organizations, with the
union of all data sources accurately providing coverage for 99%
and 99% of tech and non-tech organizations, respectively. How-
ever, the majority of data sources are nearly two times worse at
differentiating between the types of tech organizations (e.g., ISP,
hosting provider) than non-tech organizations (e.g., banks, insur-
ance providers), as can be seen in Table 4.
Zvelo and D&B provide weak accurate coverage, but a notice-
ably higher precision; while Zvelo and D&B achieve a recall rate
of 25% (¬±7% margin of error1) and 45% (¬±9%), they achieve a pre-
cision of 86% and 78%, respectively. Sources generally more accu-
rately classify ISPs‚ÄîPeeringDB reliably classifies ISPs with a 100%
true positive rate‚Äîbut the majority are far from perfect‚ÄîD&B
achieves a recall of 70% (¬±8%) and precision of 89%. They are, how-
ever, far worse at classifying all hosting providers; D&B and Zvelo
achieve a recall of 45% (¬±9%) and 25% (¬±7%), respectively. We more
generally investigate D&B‚Äôs and Zvelo‚Äôs inaccurate matches and
1We report the margin of error with a 5% ùõº for sample sizes less than n=30.
706
ASdb: A System for Classifying Owners of Autonomous Systems
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Source
D&B
Crunchbase
ZoomInfo
Clearbit
Zvelo
PeeringDB
IPinfo
Union of Best
Overall
Layer1
116/122 (96%)
44/55 (80%)
71/101 (70%)
31/91 (34%)
119/138 (86%)
21/22 (95%)
43/45 (96%)
146/148 (99%)
Tech
Non-tech
70/73 (96%)
24/28 (86%)
39/55 (71%)
3/49 (6%)
78/86 (91%)
20/21 (97%)
37/37 (100%)
95/96 (99%)
46/49 (94%)
20/27 (74%)
32/46 (70%)
32/42 (76%)
41/52 (79%)
1/1 (100%)
6/8 (75%)
51/52 (98%)
Overall
Layer2
93/121 (77%)
28/53 (53%)
84/138 (61%)
‚Äì
84/138 (61%)
18/22 (82%)
34/45 (76%)
126/147 (86%)
Tech
Non-tech
Hosting
ISP
39/62 (63%)
13/24 (54%)
23/37 (62%)
‚Äì
46/74 (62%)
18/19 (95%)
26/32 (81%)
69/83 (83%)
51/59 (86%)
14/15 (93%)
34/46 (74%)
‚Äì
26/64 (41%)
0/3 (0%)
14/19 (74%)
57/64 (89%)
5/11 (45%)
2/5 (40%)
5/8 (63%)
‚Äì
4/16 (25%)
0/1 (0%)
5/6 (83%)
9/17 (53%)
28/40 (70%)
8/13 (62%)
14/23 (61%)
‚Äì
38/47 (81%)
18/18 (100%)
21/26 (81%)
49/54 (91%)
Table 4: External Data Source Correctness‚ÄîAll data sources, except IPinfo, do poorly when classifying hosting providers, exhibiting a
correctness (i.e., the fraction of correctly labeled ASes out of all ASes that are labeled by that data source) of less than 63%. These numbers
are based on the 148 Gold Standard entries that labelers could classify. For layer-2 numbers, we also drop the 6 data points that researchers
could only assign a layer-1 NAICSlite categorization in the Gold Standard.
find that 67% and 58%, respectively, are due to their ambiguous
and inconsistent categorization, preventing a reliable translation
to NAICSlite categories. For example, D&B uses three different
NAICS codes interchangeably to classify both ISPs and hosting
providers: 517911 (‚ÄúTelecommunications Resellers‚Äù), 541512 (‚ÄúCom-
puter Systems Design Services‚Äù),and 519190 (‚ÄúAll Other Information
Services‚Äù).
Beyond Technology. All data sources achieve impressive re-
call (96‚Äì100%) and precision (89‚Äì100%) on the two largest non-
technology NAICSlite categories: education and finance. Evaluat-
ing dataset performance for other categories is complicated by the
low number of data points to evaluate against in the long tail of
the Gold Standard. To accurately evaluate each data source across
all NAICSlite categories, we compile a ‚ÄúUniform Gold Standard‚Äù
dataset of 320 registered ASes that are uniformly sampled across
all 16 NAICSlite Layer 1 categories (Table 2). We calculate in Ta-
ble 11, located in the appendix, the precision of data sources across
all NAICSlite layer 1 categories. Although individual sources are
flawed, in aggregate these sources are a promising resource for cat-
egorizing ASes by business sector. At least one data source achieves
a 100% precision for 11 of 16 NAICSlite categories. D&B and Zvelo
have the best coverage on the Uniform Gold Standard dataset, and
Crunchbase achieves at least a 90% precision across half of the
NAICSlite categories. Nonetheless, all data sources fail to accu-
rately distinguish between types of technology companies.
3.4 Data Source (Dis)agreement
When using the union of categories applied by at least two data
sources that agree on classification nearly all NAICSlite categories
achieve an impressive 100% precision (Table 11, located in the ap-
pendix). However, this occurs for only 33% and 60% of ASes in the
Uniform Gold Standard and Gold Standard set, respectively.
Data sources frequently disagree on the category of an AS. Data
sources had zero overlap in the categories they applied for 40%
and 13% of ASes in the Uniform Gold Standard and Gold Standard
set, respectively. We uncover three categories of disagreement: (1)
nuanced disagreement (i.e., both categories applied accurately de-
scribe the entity) (2) blatant disagreement (i.e., one of the categories
707
applied is incorrect) and (3) entity disagreement (i.e., the entity
being matched to is different), a problem that detail in Section 3.5.
Nuanced disagreement affects 6% of Gold Standard ASes. For ex-
ample, AS 32169 is an online learning service, and thus gets labeled
as ‚Äúeducation‚Äù by Zvelo, ‚Äúmedia‚Äù by Crunchbase, and ‚Äúinformation
technology‚Äù by D&B. At the layer 2 level, nuanced disagreement
most often occurs when technology companies offer multiple ser-
vices (e.g., ISP, Hosting, Cell), and data sources match to different ser-
vices. We observe nuanced disagreement even amongst researchers
labeling the Gold Standard set, where 13% of ASes had each re-
searcher label with disagreeing, yet accurate, categories. Vallina
et al. [60] attributes the complexity of classifying organizations to
the subjective perceptions and priorities of labelers.
Nonetheless, 7% of ASes face disagreeing data sources in which
all but one data source are incorrect (e.g., Zvelo labels AS 23414, the
Panama Canal,as ‚ÄúFinance and Insurance‚Äù). Ultimately,while perfor-
mance of individual datasets varies widely across ASes, increasing
the number of overall data sources also increases the probability
of data source agreement and thus the likelihood of an accurate
classification.
3.5 Scaling Entity Resolution
Our goal is to build a database that characterizes all ASes. Scaling
requires both access to the full business datasets and developing an
automated method for looking up organizations. While we previ-
ously evaluated data source coverage,precision,and recall in Section
3.4 using manually matched and verified data source entries, provid-
ing a theoretical upper bound on those metrics must also account
for losses brought about by incorrect automated matching. We pur-
chase data for our full dataset from D&B, and Zvelo due to their
high coverage and accuracy (Section 3.3). We additionally utilize
IPinfo, Crunchbase, and PeeringDB using their provided free or
inexpensive ($50/mo) research access. We choose to drop Clearbit
and ZoomInfo as neither data source markets full data access to
academic researchers.
In this section, we describe and evaluate the automated process
we designed to look up ASes.
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Maya Ziv, Liz Izhikevich, Kimberly Ruth, Katherine Izhikevich, and Zakir Durumeric
in the Gold Standard and find that across all confidence levels (Fig-
ure 2), D&B returns a correct DUNS number 83% and 89% when
confidence scores greater than 0, and greater than 5, respectively
(Table 5).
Crunchbase. Crunchbase provides a bulk dataset that can be
queried by name and/or domain. For all ASes with an available
domain, Crunchbase achieves a 100% matching accuracy and 12%
coverage when we query all Gold Standard ASes (Table 5). To query
ASes with no available domains, we search Crunchbase using a tok-
enized version of the AS name; Crunchbase achieves 95% matching
accuracy on the Gold Standard ASes, while providing 15% coverage.
Zvelo can only be queried by a working domain; thus,
Zvelo.
Zvelo‚Äôs coverage is directly dependent on the identification of the
correct domain associated with each AS.
With access to all five datasets in full, we query all datasets us-
ing all available RIR information and the ‚Äúmost similar‚Äù domain
matching strategy. We find that all data sources exhibit a relatively
similar coverage between the Gold Standard and complete set of
registered ASes. Combining all five data sources allows us to reach
99% coverage of all registered ASes. However, non-perfect match-
ing accuracy also adds the possibility of entity disagreement. We
discover that when automatically queried, 14% of Gold Standard
ASes are matched to at least two data sources that disagree on the
entity.
3.6 Summary
Existing data sources are successful at categorizing businesses at the
NAICSlite layer 1 granularity,achieving an overall recall on the Gold