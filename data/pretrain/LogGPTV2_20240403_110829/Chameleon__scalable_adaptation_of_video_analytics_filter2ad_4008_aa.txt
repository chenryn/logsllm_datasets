title:Chameleon: scalable adaptation of video analytics
author:Junchen Jiang and
Ganesh Ananthanarayanan and
Peter Bod&apos;ık and
Siddhartha Sen and
Ion Stoica
Chameleon: Scalable Adaptation of Video Analytics
Junchen Jiang†◦, Ganesh Ananthanarayanan◦, Peter Bodik◦, Siddhartha Sen◦, Ion Stoica⋆
†University of Chicago ◦Microsoft Research ⋆UC Berkeley, Databricks Inc.
ABSTRACT
Applying deep convolutional neural networks (NN) to video
data at scale poses a substantial systems challenge, as im-
proving inference accuracy often requires a prohibitive cost
in computational resources. While it is promising to balance
resource and accuracy by selecting a suitable NN configura-
tion (e.g., the resolution and frame rate of the input video),
one must also address the significant dynamics of the NN con-
figuration’s impact on video analytics accuracy. We present
Chameleon, a controller that dynamically picks the best con-
figurations for existing NN-based video analytics pipelines.
The key challenge in Chameleon is that in theory, adapting
configurations frequently can reduce resource consumption
with little degradation in accuracy, but searching a large
space of configurations periodically incurs an overwhelming
resource overhead that negates the gains of adaptation. The
insight behind Chameleon is that the underlying character-
istics (e.g., the velocity and sizes of objects) that affect the
best configuration have enough temporal and spatial correla-
tion to allow the search cost to be amortized over time and
across multiple video feeds. For example, using the video
feeds of five traffic cameras, we demonstrate that compared
to a baseline that picks a single optimal configuration offline,
Chameleon can achieve 20-50% higher accuracy with the
same amount of resources, or achieve the same accuracy
with only 30-50% of the resources (a 2-3× speedup).
CCS CONCEPTS
• Information systems → Data analytics; • Computing
methodologies → Object detection;
KEYWORDS
video analytics, deep neural networks, object detection
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5567-4/18/08...$15.00
https://doi.org/10.1145/3230543.3230574
1 INTRODUCTION
Many enterprises and cities (e.g., [2, 6]) are deploying thou-
sands of cameras and are starting to use video analytics for
a variety of 24×7 applications, including traffic control, se-
curity monitoring, and factory floor monitoring. The video
analytics are based on classical computer vision techniques
as well as deep neural networks (NN). This trend is fueled by
the recent advances in computer vision (e.g., [17, 18]) which
have led to a continuous stream of increasingly accurate
models for object detection and classification.
A typical video analytics application consists of a pipeline
of video processing modules. For example, the pipeline of a
traffic application that counts vehicles consists of a decoder,
followed by a component to re-size and sample frames, and
an object detector. The pipeline has several “knobs” such as
frame resolution, frame sampling rate, and detector model
(e.g., Yolo, VGG or AlexNet). We refer to a particular combi-
nations of knob values as a configuration.
The choice of configuration impacts both the resource con-
sumption and accuracy of the video application. For example,
using high frame resolutions (e.g., 1080p) or NN models with
many layers enables accurate detection of objects but also
demands more GPU processing. The “best” configuration
is the one with the lowest resource demand whose accu-
racy is over a desired threshold. Accuracy thresholds are set
by the applications, e.g., traffic light changes can function
with moderate accuracy while amber alert detection requires
very high accuracy. Configurations that meet the accuracy
threshold can often vary by many orders of magnitude in
their resource demands [16, 32], and picking the cheapest
among them can significantly impact computation cost.
The best configuration for a video analytics pipeline also
varies over time, often at a timescale of minutes or even
seconds. For the traffic pipeline described above, we may
use a low frame-rate (e.g., 5 frames/sec instead of 30 fps)
when cars are moving slowly, say at a traffic stop, consuming
6× fewer resources without impacting the accuracy of the
vehicle count. In contrast, using a low frame-rate to count
fast-moving cars will significantly hurt the accuracy.
As such, we need to frequently change the configuration of
the pipeline to minimize the resource usage while achieving
the desired accuracy. While prior video analytics systems [16,
32, 33] profile the processing pipeline to minimize cost, they
only do so once, at the beginning of the video. As a result,
these systems fail to keep up with the intrinsic dynamics
of the resource-accuracy tradeoff, and they end up either
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
J. Jiang et al.
wasting resources (by picking an expensive configuration)
or not meeting the accuracy target.
One natural approach to address this challenge is to peri-
odically profile the pipeline configurations to find an optimal
resource-accuracy tradeoff. Unfortunately, this is prohibi-
tively expensive because the number of possible configura-
tions is exponential in the number of knobs and their values.
Even a simple video pipeline with just a few knobs can have
thousands of potential configurations. Further, the cost of
executing some of the configurations can be orders of magni-
tude higher than the most efficient one we end up selecting.
In fact, in our early experiments, the cost of periodic profiling
often exceeded any resource savings gained by adapting the
configurations. The main challenge, thus, is to significantly
reduce the resource cost of periodic configuration profiling.
Unfortunately, using traditional modeling techniques such
as Bayesian optimization [12], multi-armed bandits [19], or
optimal experiment design [31] to update pipeline configu-
rations at the granularity of seconds is very expensive, due
to the number of required experiments. In fact, these tech-
niques typically assume a stationary environment, where it
is sufficient to profile once upfront or infrequently (once a
day). Our setting, however, is non-stationary. For instance,
tracking vehicles when traffic moves quickly requires a much
higher frame rate than when traffic moves slowly, but when
each condition occurs may vary by hour, minute, or second.
To address this challenge, we take a more direct approach
that leverages domain-specific insights on the temporal and
spatial correlations of these configurations.
Temporal correlation: While the best configuration varies
over time, certain characteristics tend to persist. The top-k
best configurations (cheapest k configurations with accu-
racy above the desired threshold) tend to be relatively stable
over time, for a small value of k. Similarly, configurations
that are very bad—very inaccurate and/or very expensive—
remain so over long time periods. Thus we can significantly
prune the search space during profiling by learning which
configurations are promising and which are unhelpful.
Cross-camera correlation: Video feeds of cameras deployed
in geographical proximity (e.g., in the same city or building)
often share properties (e.g., the velocities and sizes of objects)
that affect the optimal configuration. Instead of searching
for optimal configurations per camera feed, we can amortize
the profiling cost across multiple cameras. Once we identify a
good set of configurations for one video feed, we can reuse it
on similar feeds. As more organizations deploy large fleets of
cameras, leveraging cross-camera correlations will become
an increasingly effective way to reduce the cost of profiling.
Independence of configurations: A central question in
any supervised learning problem is how to obtain high-
quality labels. One possibility is to use humans to label the
video feeds frame by frame, but this approach is slow and un-
scalable. Instead, we use an expensive golden configuration to
provide the “ground truth”, following prior work [22, 24, 32].
An example of such a configuration for a traffic pipeline
is a resolution of 1080p, at 30 fps, using a full Yolo model.
However, to minimize the cost of running the golden config-
uration, which uses the best (and most expensive) values for
all knobs, we rely on an empirical observation that knobs
are typically independent. That is, for a given configuration
knob, the relationship between its resource and accuracy is
largely independent of the values of the other configuration
knobs. Thus, in our profiler, we measure the value of a given
knob (e.g., frame rate) by simply holding the values of the
other knobs fixed (e.g., to a reasonably inexpensive value).
In this paper, we leverage these observations to develop
Chameleon, a video analytics system that optimizes resource
consumption and inference accuracy of video analytics pipelines,
by adapting their configurations in real-time. Using live video
feeds from five real traffic cameras, we show that compared
to a baseline that picks the optimal configuration offline,
Chameleon can achieve 20-50% higher accuracy with the
same amount of resources, or achieve the same accuracy
with only 30-50% of the resources (2-3× speedup).
• We do a cost-benefit analysis for continuously adapting
NN configurations compared to one-time tuning, and show
that it can save compute resources by up to 10× and raise
accuracy by up to 2×. (§3)
• We identify and quantify the impact of spatial and tempo-
ral correlations on resource-accuracy tradeoffs. (§4)
• We present a suite of techniques to dramatically reduce the
cost of periodic profiling by leveraging the spatial/temporal
correlations. (§5)
Our key contributions are as follows:
2 RESOURCE-ACCURACY PROFILES
We begin with some background on NN-based video analyt-
ics, and use the object detection pipeline to show the impact
of configurations on inference accuracy and cost.
2.1 Object detection pipelines
We use object detection as a running example to illustrate
our ideas. The goal of object detection is to identify objects
of interests, their classes (e.g., car), and sometimes their lo-
cations in each frame of the video. Object detection is a core
vision task on which a wide range of higher-level tasks are
built, so improving it can impact many applications.
The simplest way to detect objects in a live video stream
today is to decode each frame into a bitmap and run each
bitmap through an object-detection NN, such as Yolo [9]
or Faster RCNN [15]. This would detect all objects in all
the frames, but would require a significant amount of GPU
resources for NN inference. Such an expensive approach
may be necessary if we want to detect even small objects
and the objects present change very frequently. However,
in many scenarios, the objects might change very slowly
(e.g., each object stays on screen for at least a second) and
Chameleon: Scalable Adaptation of Video Analytics
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 1: Two typical NN-based object detection pipelines and
their configuration knobs. Pipeline A uses a single NN to detect
and classify objects in a given frame, while Pipeline B has two
separate steps to detect regions of interest as bounding boxes
and then classify each bounding box using an NN classifier,
which is often cheaper than using the object detection NN.
Thus, Pipeline A is more generally applicable, but Pipeline B is
preferred if the regions of interest are few and easy to identify
(e.g., moving objects in front of a static background).
we may only want to detect relatively large objects. In this
case, processing only 1 frame per second and resizing it from
960p to 480p, for example, would reduce resource demand by
120× with essentially no impact on accuracy. Frame sampling
and resizing are just two of many possible knobs in a video
processing pipeline that can dramatically reduce resource
demand with only a small impact on accuracy.
Pipelines: We consider two object detection pipelines, as
shown in Figure 1. In pipeline A, the raw video frames are
first pre-processed by sampling frames (to reduce the frame
rate) and resizing them, and are then fed into one of several
pre-trained object-detection models (e.g., Faster RCNN [15],
Yolo [9]). Pipeline B uses a light-weight background sub-
traction logic (a non-NN model for which CPU is sufficient)
to first detect regions with motion, and only sends these
smaller regions to an NN-based classifier (e.g., ResNet [30],
MobileNet [21]) to label them. Both pipelines have been ac-
tively studied in the computer vision literature. Although
pipeline A has attracted more attention recently due to NN
advancements, pipeline B is often a better, cheaper choice if
the camera is static (e.g., mounted on a pole).
Configurations: While logically both pipelines expose simi-
lar interfaces, they have different sets of configuration knobs,
whose values are critical to the performance (accuracy and
resource consumption) of object detection. We focus on a
different subset of knobs from each pipeline to create two
illustrative examples, and use them throughout the paper.
Figure 2: An illustrative example of the accuracy metric: Pre-
cision = 3 (# of true positives) / 5 (# of detected objects), Recall
= 3 (# of true positives) / 4 (# of objects), F1 score = 2/(1/Preci-
sion+1/Recall) = 2/3.
• Three knobs from Pipeline A: frame rate ({30, 10, 5, 2,
1}fps), image size ({960, 840, 720, 600, 480}p), and object de-
tection model (FasterRCNN+{InceptionResNet, ResNet101,
ResNet50, InceptionV2}, SSD+{InceptionV2, MobileNetV1}) [7].
The frame rate and image size decide which frames and in
what size they should be fed to the object detection model.
• Two knobs from Pipeline B: minimum size of the region
with detected motion (to ignore spurious detections) as a
fraction of the whole frame ({0.5, 0.1, 0.05, 0.01, 0.005}%),
and the classifier model (ResNet101, ResNet50, InceptionV2,
MobileNetV1) [8]. Only regions larger than the minimum
size are sent to the classifier for inference.
The configuration space comprises all possible combinations
of values of these knobs: in total, Pipeline A has 150 configu-
rations and Pipeline B has 20.
The above two pipelines illustrate a few key properties
common to video analytics. These pipelines include a mix of
NN as well as classic computer vision modules, along with
their relevant knobs (e.g., region size for background sub-
traction). The pipelines also represent a cascade of modules,
where an upstream module (e.g., frame sampler) decides if a
frame will be processed by downstream modules. Thus, us-
ing the above two pipelines in our study will help us develop
techniques that hold across many video analytics pipelines.
2.2 Performance of configurations
The performance of a configuration on a set of frames is
measured by two metrics: accuracy and cost.
Accuracy: When using configuration c, we compute accu-
racy of a single frame by comparing the detected objects with
the objects detected by the most expensive configuration,
which we call the golden configuration, using the F1 score,