lossğ‘– = min(âˆ¥ğ‘Ÿğ‘– âˆ’ ğ¶ğ‘˜âˆ¥2)
5 THEORETICAL ANALYSIS
In this section, we conduct a theoretical analysis to prove that Whis-
per achieves lower information loss in feature extraction than the
packet-level and the traditional flow-level methods, which ensures
that Whisper extracts traffic features accurately. Due to the page
limitations, all proofs can be found in Appendix A - D. Moreover,
we analyze the scale of the frequency domain features and the
algorithmic complexity of Whisper.
5.1 Information Loss in Whisper
Traffic Feature Differential Entropy Model. First, we develop
the traffic feature differential entropy model, a theoretical analysis
framework that evaluates the efficiency of traffic features by ana-
lyzing the information loss incurred by feature extractions from an
information theory perspective [39]. The framework aims to (i)
model an observable packet-level feature as a stochastic process
and observed features extracted from ongoing packets as the state
random variables of the process; (ii) model feature extraction meth-
ods as algebraic transformations of the state random variables; (iii)
evaluate the efficiency of the features by measuring the information
loss during the transformations.
We model a particular type of packet-level feature (e.g., the
packet length, and the time interval) as a discrete time stochastic
process S, which is used to model traffic feature extraction by
different detection methods. We use a random variable vector (cid:174)ğ‘  =
[ğ‘ 1, ğ‘ 2, . . . , ğ‘ ğ‘] to denote a packet-level feature sequence extracted
from ğ‘ continuous packets, i.e., ğ‘ random variables from S. f
indicates a feature extraction function that transforms the original
features (cid:174)ğ‘  for the input of machine learning algorithms. According
to Table 1, in the packet-level methods, f outputs the per-packet
features sequence (cid:174)ğ‘  directly. In the traditional flow-level methods,
f calculates a statistic of (cid:174)ğ‘ . In Whisper, f calculates the frequency
domain features of (cid:174)ğ‘ . We assume that S is a discrete time Gaussian
process, i.e., S âˆ¼ GP(ğ‘¢(ğ‘–), Î£(ğ‘–, ğ‘—)). For simplicity, we mark Î£(ğ‘–, ğ‘–)
as ğœ(ğ‘–). We assume S is an independent process and then we can
obtain the covariance function of S, i.e., ğœ…(ğ‘¥ğ‘–, ğ‘¥ ğ‘—) = ğœ(ğ‘–)ğ›¿(ğ‘–, ğ‘—). ğ‘ğ‘–
denotes the probability density function of ğ‘ ğ‘–. We use differential
âˆ« +âˆ
entropy [39] to measure the information in the features using the
unit of nat:
âˆ’âˆ
ğ‘ğ‘–(ğ‘ ) ln ğ‘ğ‘–(ğ‘ )dğ‘  = ln ğ¾ğœ(ğ‘–),
H(ğ‘ ğ‘–) = âˆ’
(25)
âˆš2ğœ‹ğ‘’. We assume that the variance of each ğ‘ ğ‘– is large
where ğ¾ =
enough to ensure the significant change because a kind of stable
packet-level feature is meaningless to be extracted and analyzed.
Thus, we establish non-negative differential entropy assumption,
i.e., ğœ(ğ‘–) â‰¥ ğ¾âˆ’1 to ensure H(ğ‘ ğ‘–) â‰¥ 0.
Analysis of Traditional Flow-level Detection Methods. We an-
alyze the information loss in the feature extraction of the traditional
flow-level methods. We consider three types of widely used statisti-
cal features in the traditional flow-level methods [5, 24, 37, 43, 77]:
(i) min-max features, the feature extraction function f outputs the
maximum or minimum value of (cid:174)ğ‘  to extract flow-level features of
traffic and produces the output for machine learning algorithms. (ii)
average features, f calculates the average number of (cid:174)ğ‘  to obtain the
flow-level features. (iii) variance features, f calculates the variance
of (cid:174)ğ‘  for machine learning algorithms. We analyze the information
loss when performing the statistical feature extraction function f.
Based on the probability distribution of the state random variables
and Equation (25), we obtain the information loss of flow-level
statistical features in the traditional flow-level detection over the
packet-level detection and have the following properties of the
features above.
Theorem 1. (The Lower Bound for Expected Information Loss of
the Min-Max Features). For the min-max statistical features, the
lower bound of expected information loss is:
E[Î”Hflowâˆ’minmax] â‰¥ (ğ‘ âˆ’ 1) ln ğ¾E[ğœ].
(26)
Theorem 2. (The Lower Bound for Expected Information Loss
of the Average Features). The lower bound for the expectation of
information loss in the average features is:
E[Î”Hflowâˆ’avg] â‰¥ ln
(27)
We can obtain that the equality of Theorem 1 and Theorem 2
ğ‘ ğ¾ ğ‘âˆ’1E[ğœ]ğ‘âˆ’1.
holds iff the stochastic process S is strictly stationary.
Theorem 3. (The Lower Bound and Upper Bound for the Infor-
mation Loss of the Average Features). For the average features, the
upper and lower bounds of the information loss in the metric of
differential entropy is:
ğ‘ ğ¾ ğ‘âˆ’1ğ‘„(ğœ)ğ‘âˆ’1,
ln ğ‘ â‰¤ Î”Hflowâˆ’avg â‰¤ ln
(28)
where ğ‘„(ğœ) is the square mean of the variances of the per-packet
features sequence (cid:174)ğ‘ .
Theorem 4. (The Information Loss of the Variance Features).
When the Gaussian process S is strictly stationary with zero mean,
i.e., ğ‘¢(ğ‘–) = 0 and ğœ(ğ‘–) = ğœ, for the variance features, an estimate of
the information loss is:
âˆš
âˆš
Î”Hflowâˆ’var = ğ‘ ln ğ¾ğœ âˆ’ ln
(29)
According to the theorems above, we can conclude that the
information loss in the traditional flow-level detection methods
increases approximately linearly with the length of per-packet
feature sequences. Thus, comparing with the packet-level methods,
the traditional flow-level methods cannot effectively extract the
.
âˆš
4ğœ‹ğ‘ 3
ğœ2
Session 12C: Traffic Analysis and Side ChannelsCCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3436features of traffic. Although the traditional flow-level methods can
adopt multiple statistical features [4, 76], the number of packets in
the feature extraction (ğ‘ ) is significantly larger than the number
of features. In Section 6.3, we will use experiments to show that
the traditional flow-level methods achieve low detection accuracy.
Analysis of Whisper. Different from the traditional flow-level
methods, Whisper encodes per-packet features as vectors and per-
forms DFT on the vectors to extract the frequency domain features
of the traffic. We prove the low information loss property of Whis-
per by comparing with the packet-level methods (see Theorem 5)
and the traditional flow-level methods (see Theorem 6) by leverag-
ing the bounds of the information loss in Theorem 1 - 4.
Theorem 5. (An Estimation of the Information Loss of Whisper
over the Packet-level Methods). When the Gaussian process S is
strictly stationary with zero mean, i.e., ğ‘¢(ğ‘–) = 0 and ğœ(ğ‘–) = ğœ, we
can acquire an estimate of the information loss in Whisper when
ignoring the logarithmic transformation using:
(cid:114) ğœ‹
2ğ‘’
Î”HWhisper = ğ‘ ln ğœ
ğ‘¤2
ğ‘–
âˆ’ ğ‘ ln ğ‘ ,
(30)
âˆš
where ğ‘¤ğ‘– is the ğ‘–ğ‘¡â„ element of the encoding vector ğ‘¤.
Theorem 6. (An Estimation of the Information Loss Reduction of
Whisper over the Traditional Flow-level Methods). With the same
assumption in Theorem 5, compared with the traditional flow-level
methods that extract the average features, Whisper reduces the
information loss with an estimation:
Î”HWhisperâˆ’avg = Î”Hflowâˆ’avg âˆ’ Î”HWhisper
(31)
ğ‘– ğ‘ + ln
= ğ‘ ln 2ğ‘’ğ‘¤2
(32)
Similarly, Whisper reduces the information loss in the flow-level
methods that use min-max features and variance features. We
present the estimations of reduced information loss in the met-
ric of differential entropy as follows:
ğ‘
ğ¾ğœ
.
.
âˆš
(33)
Î”HWhisperâˆ’minmax = ğ‘ ln 2ğ‘’ğ‘¤2
ğ‘– ğ‘ âˆ’ ln
Î”HWhisperâˆ’var = ğ‘ ln 2ğ‘’ğ‘¤2
ğ‘– ğ‘ âˆ’ ln ğ¾ğœ,
4ğœ‹ğ‘ 3
ğœ2
(34)
According to Theorem 5, by using the packet-level methods as a
benchmark, we conclude that Whisper almost has no information
loss when the number of packets involved in feature extraction is
large. Thus, the feature efficiency of Whisper is not worse than
the packet-level methods. Moreover, the packet-level methods have
a large feature scale that results in high overhead for machine
learning (proof in Section 5.2).
Based on Theorem 6, we conclude that the reduction of the in-
formation loss in the traditional flow-level methods increases more
than linearly. Thus, by reducing the information loss in the tradi-
tional flow-level methods, Whisper can extract features from ongo-
ing traffic more effectively than the traditional flow-level methods.
In Section 6.3, we will measure the detection accuracy improvement
of Whisper by using experiments.
5.2 Analysis of Scalability and Overhead
Feature Scale Reduction of Whisper. Original per-packet fea-
tures are compressed in Whisper. Whisper reduces the input data
Table 2: Complexity of the Feature Extraction Module
Space Complexity
Time Complexity
Steps
Packet Encoding
Vector Framing
DFT Transformation
Calculating Modulus
Log Transformation
ğ‘‚(ğ‘ logğ‘Šseg)
ğ‘‚(ğ‘€ğ‘)
ğ‘‚(1)
ğ‘‚(ğ‘/2)
ğ‘‚(ğ‘/2)
ğ‘‚(ğ‘€ğ‘)
ğ‘‚(1)
ğ‘‚(ğ‘Šseg)
ğ‘‚(ğ‘)
ğ‘‚(1)
Total
ğ‘‚(ğ‘€ğ‘ + ğ‘ logğ‘Šseg)
ğ‘‚(ğ‘€ğ‘ + ğ‘Šseg)
scale and the processing overhead in machine learning algorithms.
The compressed frequency domain features allow us to apply the
machine learning algorithm in high throughput networks in prac-
tice. Compared with the packet-level methods, Whisper achieves
high compression ratio ğ¶ğ‘Ÿ with a theoretical lower bound:
( ğ‘
ğ‘Šseg )( ğ‘Šseg
2 + 1)
â‰¥ 1
2ğ‘€
size(R)
size(S) =
ğ¾ğ‘“ ğ‘ğ‘“
ğ‘€ğ‘
â‰¥
.
ğ‘€ğ‘
ğ¶ğ‘Ÿ =
(35)
By reducing the feature scale, Whisper significantly reduces the
processing overhead in the packet-level methods and achieves high
throughput. In Section 6.5, we will show the experimental results
of Whisper to validate the analysis results.
Overhead of Feature Extraction in Whisper. Whisper incurs a
low computational overhead of extracting the frequency domain fea-
tures from traffic. Particularly, Whisper does not have an operation
with high time or space complexity that is higher than quadratic
terms. The time complexity and space complexity of Whisper are
shown in Table 2.
According to Table 2, the computational complexity of Whisper
is proportional to the number of packets ğ‘ . Most of the consump-
tion is incurred by matrix multiplications in the packet encoding.
Compared with the encoding, performing DFT on frames has rela-
tively less computation overhead and consumes less memory space
because of the high speed DFT operation, i.e., Fast Fourier Trans-
formation (FFT). In Section 6.5, we will validate the complexity of
Whisper by using the experimental results.
6 EXPERIMENTAL EVALUATION
In this section, we prototype Whisper and evaluate its performance
by using 42 real-world attacks. In particular, the experiments will
answer the three questions:
(1) If Whisper achieves higher detection accuracy than the state-
of-the-art method? (Section 6.3)
(2) If Whisper is robust to detect attacks even if an attackers try
to evade the detection of Whisper by leveraging the benign
traffic? (Section 6.4)
(3) If Whisper achieves high detection throughput and low de-
tection latency? (Section 6.5)
6.1 Implementation
We prototype Whisper using C/C++ (GCC version 5.4.0) and Python
(version 3.8.0) with more than 3,500 lines of code (LOC). The source
code of Whisper can be found in [21].
High Speed Packet Parser Module. We leverage Intel Data Plane
Development Kit (DPDK) version 18.11.10 LTS [26] to implement
Session 12C: Traffic Analysis and Side ChannelsCCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3437Table 3: Recommended Hyper-parameter Configurations
Hyper-Parameters
Value
Description
Framing length
Sampling window length
Adjusting frequency domain features
ğ‘Šseg
ğ‘Šğ‘¤ğ‘–ğ‘›
ğ¶
ğ¾ğ¶
[ğ‘Šğ‘šğ‘–ğ‘›,ğ‘Šğ‘šğ‘ğ‘¥ ]
ğµ
Number of clustering centers
Range of the encoding vector
Upper bound of the encoded features
50
100
10
10
105
[10, 103]
the data plane functions and ensure high performance packet pars-
ing in high throughput networks. We bind the threads of Whisper
on physical cores using DPDK APIs to reduce the cost of context
switching in CPUs. As discussed in Section 4.1, we parse the three
per-packet features, i.e., lengths, timestamps, and protocol types.
Frequency Domain Feature Extraction Module. We leverage
PyTorch [52] (version 1.6.0) to implement matrix transforms (e.g.,
encoding and Discrete Fourier Transformation) of origin per-packet
features and auto-encoders in baseline methods.
Statistical Clustering Module. We leverage K-Means as the clus-
tering algorithm with the mlpack implementation (version 3.4.0) [44]
to cluster the frequency domain features.
Automatic Parameter Selection. We use Z3 SMT solver (version
4.5.1) [40] to solve the SMT problem in Section 4.2, i.e., determining
the encoding vector in Whisper.
Moreover, we implement a traffic generating tool using Intel
DPDK to replay malicious traffic and benign traffic simultaneously.
The hyper-parameters used in Whisper are shown in Table 3.
6.2 Experiment Setup
Baselines. To measure the improvements achieved by Whisper,
we establish three baselines:
â€¢ Packet-level Detection. We use the state-of-the-art machine
learning based detection method, Kitsune [42]. It extracts
per-packet features via flow state variables and feeds the
features to auto-encoders. We use the open source Kitsune
implementation [41] and run the system with the same hard-
ware as Whisper.
â€¢ Flow-level Statistics Clustering (FSC). As far as we know,
there is no flow-level malicious traffic detection method that
achieves task agnostic detection. Thus, we establish 17 flow-
level statistics according to the existing studies [4, 5, 30, 37,
43, 77] including the maximum, minimum, variance, mean,
range of the per-packet features in Whisper, flow durations,
and flow byte counts. We perform a normalization for the
flow-level statistics. For a fair comparison, we use the same
clustering algorithm to Whisper.
â€¢ Flow-level Frequency Domain Features with Auto-Encoder
(FAE). We use the same frequency domain features as Whis-
per and an auto-encoder model with 128 hidden states and
Sigmoid activation function, which is similar to the auto-
encoder model used in Kitsune. For the training of the auto-
encoder, we use the Adam optimizer and set the batch size