in epoch 1 and the boo attack (full compromise of the
storage medium and secret keys/passphrases) in epoch
6. More generally, they can occur in any purging epochs
outside the data’s lifetime.
3 DNEFS
In this section we describe our main contribution: a
solution for eﬃcient secure deletion for ﬂash mem-
ory. We ﬁrst list our requirements for secure dele-
tion, and afterwards describe our solution.
3.1 Secure Deletion Requirements
We present four requirements for secure deletion so-
lutions. The solution must be sound, ﬁne-grained,
eﬃcient, and simple.
Soundness requires that the solution ensures guar-
anteed secure data deletion against a strong at-
tacker; we use the peek-a-boo attacker deﬁned in
Section 2.
Fine-grained requires the solution to securely
delete data, however small. This includes overwrit-
ten or truncated ﬁles, such as data removed from a
long-lived database.
The solution must be eﬃcient in terms of resource
consumption. For ﬂash memory and portable de-
vices, the relevant resources are battery consump-
tion, computation time, storage space, and device
lifetime, i.e., minimizing and levelling wear.
Finally, simplicity requires that the solution can
be easily implemented as part of existing systems.
For our purposes, this means that adding secure
deletion to existing ﬁle systems must be straightfor-
ward. We want to minimize the necessary changes
to the existing code and isolate the majority of the
implementation in new functions and separate data
structures. We want the change to be easily au-
dited and analyzed by security-minded professionals.
Moreover, we must not remove or limit any existing
feature of the underlying ﬁle system.
4
...timeepoch345621delete datawrite databoo attackpeek attackdata’s lifetimeas deleted whenever a data node is discarded. A key
remains marked as deleted until it is removed from
the storage medium and its location is replaced with
fresh, unused random data, which is then marked as
unused.
Figure 3: (a) writing a new data node DN3: DN3 is ﬁrst
encrypted with an unused key k3 and then written to an
empty position in the main storage. A reference to the
key’s position in the KSA is stored alongside Ek3 (DN3).
(b) reading a data node DN1: Ek1 (DN1) is ﬁrst read
from the storage medium along with a reference to its
key k1 in the KSA. The key is then read and used to
decrypt and return DN1.
When a new data node is written to the storage
medium, an unused key is selected from the KSA
and its position is stored in the data node’s header.
DNEFS does this seamlessly, so applications are un-
aware that their data is being encrypted. Figure 3
illustrates DNEFS’s write and read algorithms.
Purging. Purging is a periodic procedure that se-
curely deletes keys from the KSA. Purging proceeds
iteratively over each of the KSA’s erase blocks: a
new version of the erase block is prepared where the
used keys remain in the same position and all other
keys (i.e., unused and deleted keys) are replaced
with fresh, unused, cryptographically-appropriate
random data from a source of hardware random-
ness. Such random data is inexpensive and easy to
generate, even for resource-constrained devices [38].
Fresh random data is then assigned to new keys as
needed. We keep used keys logically-ﬁxed because
their corresponding data node has already stored—
immutably until an erasure operation—its logical
5
position. The new version of the block is then writ-
ten to an arbitrary empty erase block on the storage
medium. After completion, all erase blocks contain-
ing old versions of the logical KSA erase block are
erased, thus securely deleting the unused and deleted
keys along with the data nodes they encrypt.
The security of our system necessitates that the
storage medium can be properly instructed to erase
an erase block. Therefore, for ﬂash memory, DNEFS
must be implemented either into the logic of a ﬁle
system that provides access to the raw ﬂash memory
(e.g., UBIFS) or into the logic of the ﬂash controller
(e.g., solid state drive). As Swanson et al. [36] ob-
serve, any implementation of secure deletion on top
of an opaque ﬂash controller cannot guarantee dele-
tion, as its interface for erase block erasure is not
security focused and may neglect to delete internally
created copies of data due to wear levelling. Our use
of UBI bypasses obfuscating controllers and allows
direct access to the ﬂash memory.
By only requiring the secure deletion of small
densely-packed keys, DNEFS securely deletes all the
storage medium’s deleted data while only erasing a
small number of KSA erase blocks. Thus, encryption
is used to reduce the number of erasures required to
achieve secure deletion. This comes at the cost of as-
suming a computationally-bounded adversary—an
information-theoretic adversary could decrypt the
encrypted ﬁle data. We replace unused keys with
new random data to thwart the peek-a-boo attacker:
keys are discarded if they are not used to store data
in the same deletion epoch as they are generated.
While DNEFS is designed to batch deleted data
nodes, thus erasing fewer erase blocks per deleted
data node, there is no technical reason that prohibits
immediate secure deletion. In particular, ﬁles can be
marked as sensitive [2] so that purging is triggered
whenever a data node for such a ﬁle is deleted, re-
sulting in one erase block erasure. Purging can also
be triggered by an application, for example after it
clears its cache.
If a KSA erase block becomes a bad block while
erasing it, it is possible that its contents will remain
readable on the storage medium without the ability
to remove them [21].
In this case, it is necessary
to re-encrypt any data node whose encryption key
remains available and to force the garbage collection
of those erase blocks on which the data nodes reside.
Key State Map. The key state map is an in-
memory map that maps key positions to key states
{unused, used, deleted}. Unused keys can be as-
signed and then marked as used. Used keys are keys
that encrypt some valid data node, so they must be
kE  (DN  ) 33kE  (DN  ) 221k 1E  (DN  )......k3k4k1k2kE  (DN  ) 33(1) encryptkE  (DN  ) 33kE  (DN  ) 221k 1E  (DN  )......k3k4k1k2DN1(3) associate key(2) write dataKSAmain storage(a) DNEFS write operationWRITE    DN  31READ    DN(1) read encrypted data and key position(3) decrypt and return data(2) read encryption key(b) DNEFS read operationmain storageKSAﬂash memory. We require only that the ﬁle system
also determines the key location for the data node
in the index, and so the state of each key position
can be generated by marking these key locations as
used and assuming all other locations are deleted.
We deﬁne a correct key state map as one that has
(with high probability) the following three proper-
ties: (1) every unused key must not decrypt any
data node—either valid or invalid, (2) every used
key must have exactly one data node it can decrypt
and this data node must be referred to by the index,
and (3) every deleted key must not decrypt any data
node that is referred to by the index. Observe that
an unused key that is marked as deleted will still re-
sult in a correct key state map, as it aﬀects neither
the security of deleted data nor the availability of
used data.
The operation of purging performed on a cor-
rect key state map guarantees DNEFS’s soundness:
purging securely deletes any key in the KSA marked
as deleted; afterwards, every key decrypts at most
one valid data node, and every data node referred to
by the index can be decrypted. While the encrypted
version of the deleted data node still resides in ﬂash
memory, our adversary is thereafter unable to ob-
tain the key required to decrypt and thus read the
data. A correct key state map also guarantees the
integrity of our data during purging, because no key
that is used to decrypt valid data will be removed.
We deﬁne DNEFS’s purging epoch’s duration
(Section 2) as the time between two consecutive
purging operations. When a data node is written, it
is assigned a key that is currently marked as unused
in the current purging epoch. The purging opera-
tion’s execution at the purging epochs’ boundaries
ensures that all keys currently marked as unused
were not available in any previous purging epoch.
Therefore, a peek or boo attack that occurs in any
prior purging epoch reveals neither the encrypted
data node nor its encryption key. When data is
deleted, its encryption key is marked as deleted in
the current purging epoch. Purging’s execution be-
fore the next purging epoch guarantees that key
marked as deleted in one epoch is unavailable in the
KSA in the next epoch. Therefore, a peek or boo
attack that occurs in any later purging epoch may
reveal the encrypted data node but not the key. A
computationally-bounded peek-a-boo attacker is un-
able to decrypt the data node, ensuring that the data
is not recoverable and therefore securely deleted.
Conclusion. DNEFS provides guaranteed secure
deletion against a computationally-bounded peek-a-
boo attacker. When an encryption key is securely
Figure 4: Example of a key state map, key storage area,
and main storage area during a purging operation. (a)
shows the state before and (b) shows the state after
purging. Some keys are replaced with new values af-
ter purging, corresponding to data nodes that were un-
used or deleted. The table of data nodes illustrate a
log-structured ﬁle system, where newer versions of data
nodes for the same ﬁle/oﬀset invalidate older versions.
preserved to ensure availability of the ﬁle system’s
data. Deleted keys are keys used to encrypt deleted
data—i.e., data nodes that are no longer referenced
by the index—and should be purged from the sys-
tem to achieve secure deletion. Figure 4 shows an
example key state map and a KSA before and after
a purging operation: unused and deleted keys are
replaced with new values and used keys remain on
the storage medium.
While mounting, the key state map must be cor-
rectly constructed; the procedure for this depends on
the ﬁle system in which it is integrated. However,
log-structured ﬁle systems are capable of generating
a ﬁle system index data structure that maps data
nodes to their (most recently written) location in
6
          kkkkk43210kkkk567k89nextassignedkey1234567seq #   file #offsetkeyposdata1112212409600081920[...][...][...][...][...][...][...]81921234560valid   noyesnoyesnoyesyesnextassignedkeyerase block 2erase block 1erase block 2erase block 10−4 5−910−14 15−19kkkkkkkkkk10111213141516171819KSAkey state mapposstatedeleted1234567890deletedusedusedusedusedunusedunuseddeletedused*......main storage areadata nodes(a) state before purging keyskey state mapposstateKSA1234567890usedusedusedusedunusedunusedusedunusedunusedunused*0−4 5−910−14 15−19kkkkkkkkk10111213141516171819kkkk567k89kk43210kk......(b) state after purging keyskkdeleted, the data it encrypted is then inaccessible,
even to the user. All invalid data nodes have their
corresponding encryption keys securely deleted dur-
ing the next purging operation. Purging occurs pe-
riodically, so during normal operation the deletion
latency for all data is bounded by this period. Nei-
ther the key nor the data node is available in any
purging epoch prior to the one in which it is writ-
ten, preventing any early peek attacks from obtain-
ing this information.
4 UBIFSec
We now describe UBIFSec: our instantiation of
DNEFS for the UBIFS ﬁle system. We ﬁrst give
an overview of the aspects of UBIFS relevant for in-
tegrating our solution. We then describe UBIFSec
and conclude with an experimental validation.
4.1 UBIFS
UBIFS is a log-structured ﬂash ﬁle system, where
all ﬁle system updates occur out of place. UBIFS
uses an index to determine which version of data is
the most recent. This index is called the Tree Node
Cache (TNC), and it is stored both in volatile mem-
ory and on the storage medium. The TNC is a B+
search tree [7] that has a small entry for every data
node in the ﬁle system. When data is appended to
the journal, UBIFS updates the TNC to reference its
location. UBIFS implements truncations and dele-
tions by appending special non-data nodes to the
journal. When the TNC processes these nodes, it
ﬁnds the range of TNC entries that correspond to
the truncated or deleted data nodes and removes
them from the tree.
UBIFS uses a commit and replay mechanism to
ensure that the ﬁle system can be mounted after an
unsafe unmounting without scanning the entire de-
vice. Commit periodically writes the current TNC
to the storage medium, and starts a new empty jour-
nal. Replay loads the most recently-stored TNC into
memory and chronologically processes the journal
entries to update the stale TNC, thus returning the
TNC to the state immediately before unmounting.
UBIFS accesses ﬂash memory through UBI’s log-
ical interface, which provides two features useful for
our purposes. First, UBI allows updates to KSA
erase blocks (called KSA LEB’s in the context of
UBIFSec) using its atomic update feature; after
purging, all used keys remain in the same logical po-
sition, so references to KSA positions remain valid
after purging. Second, UBI handles wear-levelling
for all the PEBs, including the KSA. This is useful
because erase blocks assigned to the KSA see more
frequent erasure; a ﬁxed physical assignment would
therefore present wear-levelling concerns.
4.2 UBIFSec Design
UBIFSec is a version of UBIFS that is extended
to use DNEFS to provide secure data deletion.
UBIFS’s data nodes have a size of 4096 bytes, and
our solution assigns each of them a distinct 128-
bit AES key. AES keys are used in counter mode,
which turns AES into a semantically-secure stream
cipher [20]. Since each key is only ever used to
encrypt a single block of data, we can safely omit
the generation and storage of initialization vectors
(IVs) and simply start the counter at zero. There-
fore, our solution requires about 0.4% of the stor-
age medium’s capacity for the KSA, although there
exists a tradeoﬀ between KSA size and data node
granularity, which we discuss in Section 4.3.
Key Storage Area. The KSA is comprised of a
set of LEBs that store random data used as en-
cryption keys. When the ﬁle system is created,
cryptographically-suitable random data is written
from a hardware source of randomness to each of the
KSA’s LEBs and all the keys are marked as unused.
Purging writes new versions of the KSA LEBs us-
ing UBI’s atomic update feature; immediately after,
ubi_flush is called to ensure all PEBs containing
old versions of the LEB are synchronously erased
and the purged keys are inaccessible. This ﬂush
feature ensures that any copies of LEBs made as
a result of internal wear-levelling are also securely
deleted. Figure 5 shows the LEBs and PEBs during
a purging operation; KSA block 3 temporarily has
two versions stored on the storage medium.
Key State Map. The key state map (Section 3.2)
stores the key positions that are unused, used, and
deleted. The correctness of the key state map is
critical in ensuring the soundness of secure deletion
and data integrity. We now describe how the key
state map is created and stored in UBIFSec. As an
invariant, we require that UBIFSec’s key state map
is always correct before and after executing a purge.
This restriction—instead of requiring correctness at
all times after mounting—is to allow writing new
data during a purging operation, and to account for
the time between marking a key as used and writing
the data it encrypts onto the storage medium.
The key state map is stored, used, and updated
in volatile memory. Initially, the key state map of a
freshly-formatted UBIFSec ﬁle system is correct as it
7
Figure 5: Erase block relationships among MTD, UBI, and UBIFSec, showing the new regions added by UBIFSec
(cf. Figure 1). In this example, a purging operation is ongoing—the ﬁrst three KSA LEBs have been updated and
the remaining LEBs still have their old value. In the MTD layer, an old version of KSA 3 is temporarily available.
consists of no data nodes, and every key is fresh ran-
dom data that is marked as unused. While mounted,
UBIFSec performs appropriate key management to
ensure that the key state map is always correct when
new data is written, deleted, etc. We now show that
we can always create a correct key state map when
mounting an arbitrary UBIFSec ﬁle system.
The key state map is built from a periodic check-
point combined with a replay of the most recent
changes while mounting. We checkpoint the current
key state map to the storage medium whenever the
KSA is purged. After a purge, every key is either
unused or used, and so a checkpoint of this map can
be stored using one bit per key—less than 1% of the
KSA’s size—which is then compressed. A special
LEB is used to store checkpoints, where each new
checkpoint is appended; when the erase block is full
then the next checkpoint is written at the beginning
using an atomic update.
The checkpoint is correct when it is written to the
storage medium, and therefore it is correct when it is
loaded during mounting if no other changes occurred
to the ﬁle system. If the ﬁle system changed after
committing and before unmounting, then UBIFS’s
replay mechanism is used to generate the correct
key state map: ﬁrst the checkpoint is loaded, then
the replay entries are simulated. Therefore, we al-
ways perform purging during regular UBIFS com-
mits; the nodes that are replayed for UBIFS are ex-
actly the ones that must be replayed for UBIFSec.
If the stored checkpoint gets corrupted, then a full
scan of the valid data nodes rebuilds the correct key
state map. A consistency check for the ﬁle system
also conﬁrms the correctness of the key state map
with a full scan.
As it is possible for the storage medium to fail