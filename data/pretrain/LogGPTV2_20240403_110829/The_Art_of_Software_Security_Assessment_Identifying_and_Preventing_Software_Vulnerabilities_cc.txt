auditor, you must determine when data can be encoded in a manner that undermines 
application security. To do this, you must couple decoding phases with relevant 
security decisions and resulting actions in the code. The following steps are a basic 
procedure: 
1.  Identify each location in the code where escaped input is decoded. 
2.  Identify associated security decisions based on that input. 
3.  If decoding occurs after the decision is made, you have a problem. 
To perform this procedure correctly, you need to correlate what data is relevant to the 
action performed after the security check. There's no hard and fast method of tying a 
decoding phase to a security decision, but one thing you need to consider is that the 
more times data is modified, the more opportunities exist for fooling security logic. 
Beyond that, it's just a matter of understanding the code involved in data processing. 
To help build this understanding, the following sections provide specific examples of 
how data encodings are used to evade filters. 
Hexadecimal Encoding 
HTTP is discussed in more detail in later chapters; however, this discussion of 
encoding would be remiss if it didn't address the standard encoding form for URIs and 
query data. For the most part, all alphanumeric characters are transmitted directly 
via HTTP, and all other characters (excluding control characters) are escaped by using 
a three-character encoding scheme. This scheme uses a percent character (%) 
followed by two hexadecimal digits representing the byte value. For example, a space 
character (which has a hexadecimal of 0x20) uses this three-character 
sequence: %20. 
HTTP transactions can also include Unicode characters. Details of Unicode are covered 
in "Character Sets and Unicode(? [????.])" later in this chapter, but for this discussion, 
you just need to remember that Unicode characters can be represented as sequences 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
460 
of one or two bytes. For one-byte sequences, HTTP uses the hexadecimal encoding 
method already discussed. However, for two-byte sequences, Unicode characters can 
be encoded with a six-character sequence consisting of the string %u or %U followed by 
four hexadecimal digits. These digits represent the 16-bit value of a Unicode 
character. These alternate encodings are a potential threat for smuggling dangerous 
characters through character filters. To understand the problem, look at the sample 
code in Listing 8-27. 
Listing 8-27. Hex-Encoded Pathname Vulnerability 
int open_profile(char *username) 
{ 
   if(strchr(username, '/')) { 
       log("possible attack, slashes in username"); 
       return 1; 
   } 
   chdir("/data/profiles"); 
   return open(hexdecode(username), O_RDONLY); 
} 
This admittedly contrived example has a glaring security problem: the username 
variable is checked for slashes (/) before hexadecimal characters are decoded. Using 
the coupling technique described earlier, you can associate decoding phases, security 
decisions, and actions as shown in this list: 
Decision If username contains a / character, it's dangerous (refer to line 3 in 
Listing 8-27). 
Decoding Hexadecimal decoding is performed on input after the decision 
(refer to line 10). 
Action Username is used to open a file (refer to line 10). 
So a username such as ..%2F..%2Fetc%2Fpasswd results in this program opening the 
system password file. Usually, these types of vulnerabilities aren't as obvious. 
Decoding issues are more likely to occur when a program is compartmentalized, and 
individual modules are isolated from the decoding process. Therefore, the developer 
using a decoding module generally isn't aware of what's occurring. 
Note 
Hexadecimal encoding is also a popular method for evading security software (such 
as IDSs) used to detect attacks against Web servers. If an IDS fails to decode 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
461 
hexadecimal encoded requests or decodes them improperly, an attack can be staged 
without generating an alert. 
Handling embedded hexadecimal sequences is usually simple. A decoder can 
generally do two things wrong: 
Skip a NUL byte. 
Decode illegal characters. 
Earlier in this chapter, you examined a faulty implementation that failed to check for 
NUL bytes (see Listing 8-5(? [????.])). So this coverage will concentrate on the 
second error, decoding illegal characters. This error can happen when assumptions 
are made about the data following a % sign. Two hexadecimal digits are expected 
follow a % sign. Listing 8-28 shows a typical implementation for converting those 
values into data. 
Listing 8-28. Decoding Incorrect Byte Values 
int convert_byte(char byte) 
{ 
    if(byte >= 'A' && byte = 'a' && byte <= 'f') 
        return (byte  'a') + 10; 
    else 
        return (byte  '0'); 
} 
int convert_hex(char *string) 
{ 
    int val1, val2; 
    val1 = convert_byte(string[0]); 
    val2 = convert_byte(string[1]); 
    return (val1 << 4) | val2; 
} 
The convert_byte() function is flawed, in that it assumes the byte is a number 
character if it's not explicitly a hexadecimal letter (as shown in the bolded lines). 
Therefore, invalid hex characters passed to this function (including the characters A 
through F) produce unexpected decoded bytes. The security implication of this 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
462 
incorrect decoding is simple; any filters processing the data in an earlier stage miss 
values that can appear in the resulting output stream. 
HTML and XML Encoding 
HTML and XML documents can contain encoded data in the form of entities, which 
are used to encode HTML rendering metacharacters. Entities are constructed by using 
the ampersand sign (&), followed by the entity abbreviation, and terminated with a 
semicolon. For example, to represent an ampersand, the abbreviation is "amp," so 
&amp; is the encoded HTML entity. A complete list of entities is available from the 
World Wide Web Consortium (W3C) site at www.w3c.org. 
Even more interesting, characters can also be encoded as their numeric codepoints in 
both decimal and hexadecimal. To represent a codepoint in decimal, the codepoint 
value is prepended with &#. For example, a space character has the decimal value 32, 
so it's represented as &#32. Hex encoding is similar, except the value is prepended 
with &#x, so the space character (0x20) is represented as &#x20. Two-byte Unicode 
characters can also be specified with five decimal or four hexadecimal digit sequences. 
This encoding form is susceptible to the same basic vulnerabilities that hexadecimal 
decoders might havesuch as embedding NUL characters, evading filters, and 
assuming that at least two bytes follow an &# sequence. 
Note 
Keep in mind that HTML decoding is normally handled by a client browser application. 
However, using this encoding form in XML data does open the possibility of a variety 
of server-directed attacks. 
Multiple Encoding Layers 
Sometimes data is decoded several times and in several different ways, especially 
when multiple layers of processing are performed before the input is used for its 
intended purpose. Decoding several times makes validation extremely difficult, as 
higher layers see the data in an intermediate format rather than the final unencoded 
content. 
In complex multitiered applications, the fact that input goes through a number of 
filters or conversions might not be immediately obvious, or it might happen only in 
certain conditions. For example, data posted to a HTTP Web server might go through 
base64 decoding if the Content-Encoding header specifies this behavior, UTF-8 
decoding because it's the encoding format specified in the Content-Type header, and 
finally hexadecimal decoding, which occurs on all HTTP traffic. Additionally, if the data 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
463 
is destined for a Web application or script, it's entirely possible that it goes through 
another level of hexadecimal decoding. Figure 8-3 shows this behavior. 
Figure 8-3. Encoded Web data 
Each component involved in decoding is often developed with no regard to other 
components performing additional decoding steps at lower or higher layers, so 
developers might make incorrect judgments on what input should result. 
Vulnerabilities of this nature tie back into previous discussions on design errors. 
Specifically, cross-component problems might happen when an interface to a 
component is known, but the component's exact function is unknown or undefined. 
For example, a Web server module might perform some decoding of request data to 
make security decisions about that decoded data. The data might then undergo 
another layer of decoding afterward, thus introducing the possibility for attackers to 
sneak encoded content through a filter. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
464 
This example brings up another interesting point: Vulnerabilities of this nature might 
also be a result of operational security flaws. As you learned in Chapter 3(? [????.]), 
"Operational Review," applications don't operate in a vacuum, especially integrated 
pieces of software, such as Web applications. The web server and platform modules 
may provide encoding methods that attackers can use to violate the security of an 
application. 
7.4.6 Character Sets and Unicode 
In the previous section, you were primarily concerned with characters that, when left 
unchecked, might represent a security threat to the application you're reviewing. 
Extending on this idea, now you examine different character set encodings and 
common situations in which they can cause problems. Character set encodings 
determine the sequence of bytes used to represent characters in different languages. 
In the context of security, you're concerned with how conversions between character 
sets affects an application's capability to accurately evaluate data streams and filter 
hostile input. 
Unicode 
The Unicode standard describes characters from any language in a unique and 
unambiguous way. It was primarily intended to address limitations of the ASCII 
character set and the proliferation of potentially incompatible character sets for other 
languages. The result is a standard that defines "a consistent way of encoding 
multilingual text that enables the exchange of text data internationally and creates 
the foundation for global software." The Unicode standard (available at 
www.unicode.org) defines characters as a series of codepoints (numerical values) 
that can be encoded in several formats, each with different size code units. A code 
unit is a single entity as seen by the encoding and decoding routines; each code unit 
size can be represented in either byte orderbig endian (BE) or little endian (LE). Table 
8-3 shows the different encoding formats in Unicode. 
Table 8-3. Unicode Encoding Formats 
Name 
Code Unit Size (in Bits) Byte Order 
UTF-8 
8 
UTF-16BE 16 
Big endian 
UTF-16LE 16 
Little endian 
UTF-32BE 32 
Big endian 
UTF-32LE 32 
Little endian 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
465 
Note that the byte-order specification (BE or LE) can be omitted, in which case a 
byte-order marker (BOM) at the beginning of the input can be used to indicate the 
byte order. 
These encoding schemes are used extensively in HTTP communications for request 
data or XML documents. They are also used in a lot of Microsoft-based software 
because current Windows operating systems use Unicode internally to represent 
strings. Unicode's codespace is 0 to 0x10FFFF, so 16-bit and 8-bit code units might 
not be able to represent an entire Unicode character because of size constraints. 
However, characters can be encoded multibyte streams; that is, several encoded 
bytes in sequence might combine to represent one Unicode character. 
Auditing programs that make use of Unicode characters and Unicode encoding 
schemes require reviewers to verify: 
Whether characters can be encoded to bypass security checks 
Whether the implementation of encoding and decoding contains vulnerabilities 
of its own 
The first check requires verifying that characters aren't converted after filter code has 
run to check the input's integrity. For example, a major bug was discovered in the 
Microsoft Internet Information Services (IIS) Web server. It was a result of the Web 
server software failing to decode Unicode escapes before checking whether a user 
was trying to perform a directory traversal (double dot) attack; so it didn't catch 
encoded ../ and ..\ sequences. Users could make the following request: 
GET /..%c0%af..%c0%afwinnt/system32/cmd.exe?/c+dir 
In this way, they could run arbitrary commands with the permissions the Web server 
uses. 
Note 
You can find details of this vulnerability at 
www.microsoft.com/security/technet/bulletin/MS00-078.mspx. 
Because many applications use Unicode representation, an attack of this nature is 
always a major threat. Given that a range of encoding schemes are available to 
express data, there are quite a few ways to represent the same codepoint. You 
already know that you can represent a value in 8-, 16-, or 32-bit code units (in either 
byte order), but smaller code units have multiple ways to represent individual code 
points. To understand this better, you need to know more about how code points are 
encoded, explained in the following sections. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
466 
UTF-8 
UTF-8 encoded codepoints are represented as single or multibyte sequences. For the 
ranges of values 0x00 to 0x7F, only a single byte is required, so the UTF-8 encoding 
for U.S. ASCII codepoints is identical to ASCII. For other values that can't be 
represented in 7 bits, a lead byte is given followed by a variable number of trailing 
bytes (up to four) that combine to represent the value being encoded. The lead byte 
consists of the highest bit set plus a number of other bits in the most significant word 
that indicate how many bytes are in this multibyte set. So the number of bits set 
contiguously in the lead byte's high word specifies the number of trailing bytes, as 
shown in Table 8-4. 
Table 8-4. UTF-8 Lead-Byte Encoding Scheme 
Bit Pattern 
Bytes Following 
110x xxxx 
1 
1110 xxxx 
2 
1111 xxxx 
3, 4, or 5 
Note 
The bit pattern rules in Table 8-4 are a slight oversimplification, but they are 
adequate for the purposes of this discussion. Interested readers are encouraged to 
browse the current specification at www.unicode.org. 
The bits replaced by x are used to hold part of the value being represented. Each 
trailing byte begins with its topmost bits set to 10 and have the least significant 6 bits 
set to hold part of the value being represented. Therefore, it's illegal for a trailing byte 
to be less than 0x80 or greater than 0xBF, and it's also illegal for a lead byte to start 
with 10 (as that would make it indistinguishable from a trailing byte). 
Until recently, you could encode Unicode values with any of the supported multibyte 
lengths you wanted. So, for example, a / character could be represented as 
0x2F 
0xC0 0xAF 
0xE0 0x80 0xAF 
0xF0 0x80 0x80 0xAF 
The Unicode 3.0 standard, released in 1999, has been revised to allow only the 
shortest form encoding; for instance, the only legal UTF-8 encoding in the preceding 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
467 
list is 0x2F. Windows XP and later enforce the shortest-form encoding rule. However, 
not all Unicode implementations are compliant. In particular, ASCII characters are 
often accepted as one- or two-byte sequences, which could be useful in evading 
character filters. For example, a filter searching for slashes in a path argument (0x2F) 
might miss the sequence 0xC0 0xAF; if UTF-8 conversions are performed later, this 
character filter can be completely evaded for any arbitrary ASCII character. 
Note 
Daniel J. Roelker published an interesting paper on combining these different 
multibyte encodings with several other hexadecimal encoding techniques to evade 
IDS filtering of HTTP traffic. It's available at 
http://docs.idsresearch.org/http_ids_evasions.pdf. 
UTF-16 
UTF-16 expresses codepoints as 16-bit words, which is enough to represent most 
commonly used characters in major languages. However, some characters require 
more than 16 bits. Remember, the codespace for Unicode ranges from 0 to 0x10FFFF, 
and the maximum value a 16-bit integer can represent is 0xFFFF. Therefore, UTF-16 
can also contain multi-unit sequences, so UTF-16 encoded codepoints can be one or 
two units. A codepoint higher than 0xFFFF requires two code units to be expressed 
and is encoded as a surrogate pair; that is, a pair of code units with a special lead bit 
sequence that combines to represent a codepoint. These are the rules for encoding 
Unicode codepoints in UTF-16 (taken from RFC 2781): 
1.  If U < 0x10000, encode U as a 16-bit unsigned integer and terminate. 
2.  Let U' = U - 0x10000. Because U is less than or equal to 0x10FFFF, U' must be less 
than or equal to 0xFFFFF. That is, U' can be represented in 20 bits. 
3.  Initialize two 16-bit unsigned integers, W1 and W2, to 0xD800 and 0xDC00, 
respectively. Each integer has 10 bits free to encode the character value, for a 
total of 20 bits. 
4.  Assign the 10 high-order bits of the 20-bit U' to the 10 low-order bits of W1 and 
the 10 low-order bits of U' to the 10 low-order bits of W2. Terminate. 
Because the constant value 0x100000 is added to the bits read from a surrogate pair, 
you can't encode arbitrary values the way you were able to in UTF-8. With UTF-16 
encoding, there's only one way to represent a codepoint. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
468 
UTF-32 
UTF-32 expresses codepoints as 32-bit value. Because it can represent any codepoint 
in a single value, surrogate pairs are never required, as they are in UTF-8 and UTF-16. 
The only way to alter how a codepoint is represented in UTF-32 encoding is to change 
the data stream's endian format (using the special BOM mentioned after Table 8-3). 
Vulnerabilities in Decoding 
As mentioned, the difficulty with filtering Unicode data correctly is that the same 
value can be represented in many ways by using different word-size encodings, by 
switching byte order, and by abusing UTF-8's unique capability to represent the same 
value in more than one way. An application isn't going to be susceptible to bypassing 
filters if only one data decoding is performedthat is, the data is decoded, checked, and 
then used. However, in the context of HTTP traffic, only one decoding seldom 
happens. Why? Web applications have increased the complexity of HTTP exchanges 
dramatically, and data can often be decoded several times and in several ways. For 
example, the IIS Web server decodes hexadecimal sequences in a request, and then 
later performs UTF-8 decoding on itand then might hand it off to an ISAPI filter or Web 