At the overlay level, to monitor the health of proxies, we
use a 2-tier health monitoring system. At the ﬁrst tier, the
proxies within the same proxy cluster are responsible for mon-
itoring each other. At the next level, each proxy in a clus-
ter monitors the health of a small number of other clusters.
When either an individual proxy or an entire cluster fails, it is
detected quickly and communicated to all remaining proxies.
Section 3.2 had described IAP behavior when a JAP goes
down. The only thing left to discuss is target behavior when
a JAP goes down.
In this case, native IP anycast routing
will cause ping packets from the target to reach another JAP,
which will ask the target to re-register. Table 1 sums up the
way PIAS achieves failover across various segments of the
client-target path.
3.7 Target selection criteria
As described earlier, the RAP may select the JAP based on
a number of criteria, including proximity, load balancing, and
connection aﬃnity8. The JAP subsequently selects a target.
It is this selection process, divorced from IP routing, that
allows PIAS to oﬀer richer target selection criteria
How PIAS achieves load balance and proximity has already
been discussed. Connection aﬃnity is discussed later in this
section. We wish to point out here that these three important
selection criteria are in fact at odds with each other. For
8Connection aﬃnity—all packets from a given connection or
ﬂow are delivered to the same target.
IAP2
JAP2
Target2
AC
Native
 Flap
PIAS
Flap
IAP1
JAP1
Target1
Figure 4: Lack of native IP anycast aﬃnity can cause
ﬂaps in the PIAS model
exampl, if both load balance and proximity are important
criteria, and the JAP nearest to the IAP is heavily loaded,
then one of the other criteria must be compromised. This
basic set of trade-oﬀs applies to application-level anycast as
well.
By never selecting the source of a packet as the target,
PIAS allows a host to be both a target and a client for a given
group. Packets sent by the target to the group address would
be forwarded to some group target other than the sender.
Note that this is not possible with native IP anycast and it
allows PIAS to support new P2P applications (section 6.1).
Proxies could potentially base their target selection on vari-
ous scoping criteria. These selection criteria can be expressed
by overloading the transport address, i.e. a group can have
separate TAs for each type of scoping. For instance, an any-
cast packet could be administratively scoped. That is,
it
could indicate that the target should be in the same site,
belong to the same DNS domain, or have the same IP ad-
dress preﬁx (or be from diﬀerent sites, DNS domains, or IP
preﬁxes). While how this would be conﬁgured and operated
is a good topic for further study, the selection functionality
of the RAP allows for the possibility of many such features.
Another form of selection would be to pick a random tar-
get rather than the nearest target - the RAP would pick a
random JAP who would then pick a random target. Random
selection among a group can be useful for various purposes
such as spreading gossip [22] or selecting partners in multicast
content distribution [23]. Indeed, in the PIAS architecture,
there is no reason an anycast packet cannot be replicated by
the RAP and delivered to a small number of multiple targets.
The salient point here is that, once IP anycast functionality is
divorced from IP routing, any number of new delivery seman-
tics are possible if the beneﬁts justify the cost and complexity.
3.7.1 Connection afﬁnity
Lack of connection aﬃnity in native IP anycast has long
been considered one of its primary weak points. This issue
spills over into PIAS. Speciﬁcally, the issue is how to maintain
aﬃnity when native IP anycast causes a diﬀerent IAP to be
selected during a given client connection.
If the same IAP
is always used, then packets will be sent to the same JAP
that was initially cached by the IAP. However, a change in
the IAP could lead to a change in the target the packets are
delivered to, as shown by Figure 4. Application-layer anycast
doesn’t have this problem, because it always makes its target
selection decision at connection start time, and subsequently
uses unicast.
A simple solution would be to have RAPs select JAPs based
on the identity of the client, such as the hash of its IP ad-
dress. This way, even if IP routing caused packets from a
given client to select a diﬀerent IAP, they would be routed
to the same JAP and hence the same target. Unfortunately,
)
)
S
S
Y
Y
A
A
D
D
(
(
s
s
p
p
a
a
l
l
f
f
n
n
e
e
e
e
w
w
e
e
b
b
t
t
e
e
m
m
i
i
t
t
e
e
g
g
a
a
r
r
e
e
v
v
A
A
 10
 10
 1
 1
 0.1
 0.1
0 - 5 - 10 - 25 - 50 percentile
0 - 5 - 10 - 25 - 50 percentile
1 flap per day
1 flap per day
14 hrs
14 hrs
19 hrs
19 hrs
15.5 hrs
15.5 hrs
25 hrs
25 hrs
14.5 hrs
14.5 hrs
3.5 hrs
3.5 hrs
f-root
f-root
(28)
(28)
c-root
c-root
(4)
(4)
i-root
i-root
(17)
(17)
j-root
j-root
(13)
(13)
k-root
k-root
(11)
(11)
m-root
m-root
(3)
(3)
3.5 hrs
3.5 hrs
as112
as112
(20)
(20)
Anycasted Server (# of locations in paranthesis)
Anycasted Server (# of locations in paranthesis)
Figure 5: Percentiles for the average time between
ﬂaps for all the anycasted destinations
this approach completely sacriﬁces proximity and load bal-
ance. Broadly, another approach would be to modify the host
application by making it anycast aware, and redirect the host
to the unicast address of a selected target (either PIAS or the
target itself could do this redirect). There are some security
issues here—the redirect must be hard to spoof—but these
are surmountable.
We can also imagine complex schemes whereby JAPs and
IAPs coordinate to insure aﬃnity. However, a fundamental
question that still has not been answered is, how good or
bad is the aﬃnity oﬀered by native IP anycast? It might be
the case that the aﬃnity oﬀered by native IP anycast is very
good; i.e. the probability that a connection breaks due to
a routing ﬂap is very small as compared to the probability
of the connection breaking due to other factors. This would
imply that we do not need the complex mechanisms stated
above.
In this regard, we did some measurements to ﬁnd
out the aﬃnity oﬀered by native IP anycast. Our results,
while preliminary, suggest that native IP anycast aﬃnity is
quite good, and PIAS need not do anything extra to provide
reasonable connection aﬃnity. Details of these measurements
are presented in section 4.1
4. EVALUATION
In this section we evaluate the PIAS architecture using
measurements and simulations. Section 4.1 describes the
measurements made using the Planetlab[24] testbed and the
anycasted DNS root servers to argue for the suﬃciency of
the aﬃnity oﬀered by native IP anycast and hence, PIAS.
Sections 4.2 and 4.3 present simulation results that show the
scalability (by group characteristics) and the eﬃciency of the
PIAS deployment. Finally, section 4.4 discusses our PIAS
implementation. We also measured the quality of proximity
selection oﬀered by the anycasted DNS server deployments.
These are brieﬂy discussed in section 7.
4.1 Connection Afﬁnity measurements
As mentioned earlier, it is important to determine the aﬃn-
ity oﬀered by native IP anycast in order to understand the
need for mechanisms to ensure aﬃnity in PIAS. This section
presents the results of our measurement study aimed to do so.
The goal of the study was to determine how often IP routing
selected diﬀerent locations when sending packets to a native
IP anycast address. We used the anycasted root servers and
the AS-112 servers as the anycast destinations. For clients,
we used 129 Planetlab nodes belonging to 112 sites.
For each anycast destination, the clients probed the as-
sociated anycast address every 10 seconds to determine the
s
e
g
a
s
s
e
m
e
d
w
m
e
t
s
y
S
i
r
o
s
r
e
b
m
e
m
f
o
r
e
b
m
u
N
 100000
 10000
 1000
 100
 10
0% inaccuracy
5% inaccuracy
25% inaccuracy
50% inaccuracy
Group size
 Minutes
  [1-5]
 10000
 1000
 1
 0
 50
 1
 2
 3
 4
 5
 150
 200
 100
TIME (min)
d
n
o
c
e
s
/
s
e
g
a
s
s
e
m
e
d
w
m
e
i
t
s
y
s
e
g
a
r
e
v
A
 100
 10
 1
 0.1
 0
60000 members,100 proxies
90000 members, 100 proxies
120000 members, 100 proxies
60000 members, 1000 proxies
90000 members, 1000 proxies
120000 members, 1000 proxies
60000 members, 10000 proxies
90000 members, 10000 proxies
120000 members, 10000 proxies
10000 proxies
1000 proxies
100 proxies
 50
 100
 150
 200
% INACCURACY
Figure 6: System wide messages from the all the
JAPs to the 4 RAPs during the event for varying
degrees of inaccuracy
location they are routed too. The servers at diﬀerent loca-
tions have been conﬁgured by their operators to respond to a
TXT type DNS query with their location[25] and hence, the
probes were DNS queries generated using dig. This data was
collected for a period of 30 continuous days in Dec’04-Jan’05.
The probing of the anycasted destinations reveals changes
in routing or ’ﬂaps’ that cause packets to be delivered to dif-
ferent locations of an anycasted server. So, a pair of probes
from a given Planetlab node switching from the San Jose f-