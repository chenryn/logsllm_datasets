to the state of the art techniques [12, 16, 21, 25]. Given the
comprehensiveness of the graph abstraction we discussed in
Section V, we believe the graph features reﬂect a much accurate
dynamics of malware infections. The distributions of selected
graph features shown in Figures 7-9 conﬁrm the discriminating
power of our graph features.
Fig. 7: Average node connectivity.
IV. PAYLOAD-AGNOSTIC FEATURES
The complete description of our features is shown in Table
II. We group the features in to high-level aggregates, graph-
centric properties, properties of HTTP headers, and temporal
dynamics. Features for which the last column of Table II has
a checkmark ((cid:2)) are novel features that we introduce (use for
the ﬁrst time) in this work. For features reused from prior work,
we show citations to those works. Notice also that the last
column shows an indirect comparison of our feature set against
IV-B. Header Features (f26 - f35)
After a successful infection, malware often “calls back home”
to exﬁltrate valuable information from the victim host or to get
more payloads for future missions. Alternatively, the malware
can also use the infected host as a bot to conduct further attacks
(e.g., spam campaigns). The key insight here is to leverage
the malware infection dataset to pinpoint WCG structures that
reveal a malware contacting a C&C server of the attacker. For
469
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:20:37 UTC from IEEE Xplore.  Restrictions apply. 
High-Level Features (HLFs)
f1: Origin
f2: X-Flash-Version
f3: WCG-Size
f4: Conversation-Length
f5: Avg-URIs-per-Host
f6: Average-URI-Length
Graph Features (GFs)
f7: Order
f8: Size
f9: Degree
f10: Density
f11: Volume
f12: Diameter
f13: Avg-In-Degree
f14: Avg-Out-Degree
f15: Reciprocity
f16: Avg-Degree-Centrality
f17: Avg-Closeness-Centrality
f18: Avg-Betweenness-Centrality
f19: Avg-Load-Centrality
f20: Avg-Node-Centrality
f21: Avg-Clustering-Coefﬁcient
f22: Avg-Neighbor-Degree
f23: Avg-Degree-Connectivity
f24: Avg-K-Nearest-Neighbors
f25: Avg-PageRank
Header Features (HFs)
f26: GETs
f27: POSTs
f28: Other-Methods
f29: HTTP-10Xs
f30: HTTP-20Xs
f31: HTTP-30Xs
f32: HTTP-40Xs
f33: HTTP-50Xs
f34: Referrer-Ctrs
f35: No-Referrer-Ctrs
Temporal Features (TFs)
f36: Duration
f37: Avg-Inter-Transact-Time
Brief Description
whether origin is known or not.
whether X-Flash version is set or not.
size of a WCG.
number of unique hosts involved in the WCG.
U RIs
average URIs per host computed as:
(cid:2)
average URI length computed as:
num hosts
len(U RIs)
.
num U RIs
(cid:2)
.
number of nodes in a WCG.
number of edges of a WCG.
number of edges the node shares with other nodes in the graph.
measure of how close the number of edges is to the maximum number of possible edges.
sum of node degrees over all nodes in the graph.
longest distance between any pair of nodes.
average number of incoming edges to a node in the graph.
average number of outgoing edges from a node in the graph.
likelihood of nodes to be mutually linked.
average of number of ties a node has.
average of the reciprocal of the sum of a node’s distances from all other nodes.
average number of shortest paths from all nodes to all others that pass through that node.
average of the fraction of all shortest paths that pass through a node.
average of the smallest number of nodes whose removal disconnects the graph.
average of measure of the degree to which nodes in a graph tend to cluster together.
average degree of neighbors of a node in the graph.
average degree for connected nodes.
average number nodes at k-nodes distance from each node.
average value for the importance measure of a node in the graph.
total number of GET methods in a WCG.
total number of GET methods in a WCG.
total number of less common methods (e.g., PUT, DELETE) in a WCG.
total number of informational responses in a WCG.
total number of success responses in a WCG.
total number of redirection responses in a WCG.
total number of client error responses in a WCG.
total number of server error responses in a WCG.
total number of URIs which have referees set in a WCG.
total number URIs for which referrer is empty in a WCG.
average duration to access a single URI in a WCG session in seconds.
average time (in seconds) between two consecutive web transactions.
Novel
[25]
(cid:2)
[12]
(cid:2)
[9]
(cid:2)
[12, 25]
[12]
(cid:2)
[12]
(cid:2)
[12]
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
[12]
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
[16, 25]
[16, 25]
(cid:2)
(cid:2)
TABLE II: Feature types and brief explanations on how they are derived from WCGs.
instance, if we encounter POST requests leaving the victim after
the completion of malware payload download, such an event
is a strong evidence of post-download behavior. In this regard,
HTTP headers carry statistical insights into the post-infection
dynamics of a WCG. Our rationale for using such features from
the WCGs is that HTTP methods such as GET and POST, and
response codes are exhibit distinct distributions in benign and
infectious WCGs. Thus, studying these properties reinforces
the other graph properties in DYNAMINER.
V. CLASSIFIER TRAINING AND DETECTION
In this section, we provide an overview of learning a classiﬁer
and the detection method in DYNAMINER.
V-A. Classiﬁer Training
Given the features we described in Section IV, we use an
ensemble random forest (ERF) [1] to train our classiﬁer. Our
choice of ERF is driven by the nature of our WCGs and the
underlying theory of the learning algorithm. The WCGs we
build are likely to have sub-classes within the whole WCG due
to distinct dynamics pertinent to redirection, download, and
post-download sub-structures. In fact, a tree-based classiﬁer
such as a decision tree seems a natural choice for our WCG
classiﬁcation problem. However, decision trees tend to overﬁt
training data that exhibits internal variability. Instead of taking
the majority vote in the standard ERF, our implementation of
the ERF combines classiﬁers by averaging their probabilistic
prediction (which reduces variance). An ERF is therefore less
prone to overﬁtting as compared to a decision tree.
V-B. On-the-Wire Detection
The intuition behind our on-the-wire detection is as follows.
DYNAMINER sits at the edge of a network or as a web proxy
to inspect individual web transactions from different hosts.
Infection clue inference. In the course of HTTP conversa-
tion, after each request-response transaction, the infection clue
inference module of DYNAMINER determines on the presence
of an infection clue. An infection clue, intuitively, is a likely
indicator of malware infection. For our purpose, for instance,
470
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:20:37 UTC from IEEE Xplore.  Restrictions apply. 
to reduce noise from benign HTTP trafﬁc, we weed out HTTP
transactions that originate from known vendors. For instance,
assuming that these sources are trusted, we exclude trafﬁc that
involve downloads from online application stores / software
repositories.
WCG classiﬁcation and update. After the potential infec-
tion WCG is constructed, DYNAMINER extracts the features
and queries the classiﬁer. If the classiﬁer predicts that a WCG
is infectious, then DYNAMINER issues an alert. If the WCG
is found benign, DYNAMINER keeps watching the WCG.
In the course of watching WCGs, for each request-response
transaction in the HTTP stream, DYNAMINER updates the
respective WCG (again based on session IDs). Each update of
a WCG then triggers feature extraction and invoking of the
ERF classiﬁer. DYNAMINER continues to watch each potential
infection WCG until either the WCG stops growing or the
session between the client and the remote host(s) is terminated.
Fig. 8: Average betweenness centrality.
VI. EVALUATION
We evaluate the effectiveness of DYNAMINER with regards
to the ground truth dataset, an independent validation dataset,
a forensic case study on recorded trafﬁc, and a live case study
in a mini-enterprise setting.
VI-A. Features and Classiﬁer Effectiveness
Features. Table IV shows the ranking of the top-20 features
in our ERF classiﬁer on the ground truth dataset. For computing
the ranks, we use the gain ratio metric with 10-fold cross
validation. This metric is known for reducing bias towards
multi-valued features in its criteria for selecting features. In
favor of our claim about capturing comprehensive dynamics,
graph-centric features make it to 15 of the top-20 features
—showing how useful the graph dynamics is in distinguishing
benign and infection WCGs.
Classiﬁer. We evaluated our ERF classiﬁer (a) using 10-fold
cross validation on the training dataset in Table I and (b) using
a labeled independent test set (Table V). The training was ran
by varying the number of trees (Nt) and number of features
(Nf ) to get the best balance between true positive and false
positive rates. The best performance our EDF classiﬁer is with
Nt = 20 and Nf = log2(N umF eatures) + 1 over all the 37
features described on Table II. Figure 10 shows the ROC curve
of the ERF classiﬁer we use to perform independent test on a
separate dataset (Section VI-B).
As can be seen from Table III, training the classiﬁer using
the HLFs, HFs, and TFs from Table II (by excluding graph
features) achieves the lowest true positive rate (0.860) with the
highest false positive rate of 0.304. On the other hand, using
graph features alone, the true positive rate jumps to 0.978 and
the false positive rate drops to 0.059. Note that when all the
features are combined, the false positive rate clearly drops
(from 0.059 to 0.015) while improving the true positive rate
(from 0.958 to 0.973). This is consistent with our observation
that malicious WCGs have distinct distribution of features
as compared to benign WCGs (see Figures 7-9). Our manual
veriﬁcation of the trees generated by the ERF shows that, when
combined with the other features, the graph features improve
the classiﬁer accuracy.
Fig. 9: Average closeness centrality.
an infection clue is ﬂagged when a redirection chain of length
>= l is followed by a download of a ﬁle type t. The threshold
for l and the download likelihood of the payload type x to
be infectious are determined from a statistical analysis of the
ground truth data.
Potential Infection WCG construction. Using the infection
clue, DYNAMINER then goes back in time to construct a
potential infection WCG using the WCG construction scheme
we discussed earlier. To avoid mixup of HTTP transactions from
multiple remote hosts, the session ID [18] of the download
and the redirection chains that triggered the infection clue are
used to guide the grouping of HTTP transactions that are likely
to go into the same WCG. In case of multiple session IDs
that a client is identiﬁed with when it interacts with multiple
remote hosts, we use a heuristic that leverages referrer values
and timestamps to cluster transactions into groups of sessions.
Each cluster then becomes a WCG to watch as the various
hosts communicate with remotes hots on the Web. Note that
471
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:20:37 UTC from IEEE Xplore.  Restrictions apply. 
Features
All
GFs
HLFs+HFs+TFs
TABLE III: Impact of features on classiﬁer accuracy.
F-score ROC Area
0.972
0.954
0.848
0.978
0.928
0.860
TPR
0.973
0.958
0.806
FPR
0.015
0.059
0.304
Feature
Avg-inter-trans-time
Duration
Order
Avg-load-centrality
Avg-closeness-centrality
Avg-betweenness-centrality
Avg-pagerank
Avg-neighbor-degree
Avg-k-nearest-neighbor
Avg-degreee-connectivity
Avg-in-degree
Avg-out-degree
Convs-length
Reciprocated-edges
Graph-size
HTTP-20X
HTTP- GETs
Avg-clustering-coeff
Volume
Degree
TABLE IV: Feature rankings for the top-20 features.
Gain Ratio
0.484 ± 0.015
0.454 ± 0.021
0.309 ± 0.011
0.309 ± 0.011
0.309 ± 0.011
0.309 ± 0.011
0.309 ± 0.011
0.306 ± 0.011
0.306 ± 0.011
0.306 ± 0.011
0.29 ± 0.02
0.29 ± 0.02
0.302 ± 0.01
0.248 ± 0.051
0.245 ± 0.026
0.251 ± 0.044
0.225 ± 0.047
0.255 ± 0.008
0.245 ± 0.026
0.209 ± 0.053
Average Rank
1 ± 0
2 ± 0
4.3 ± 1.27
5.6 ± 2.15
5.9 ± 1.92
6.2 ± 2.14
6.8 ± 1.4
9.5 ± 1.8
9.6 ± 1.2
10.7 ± 1.55
11.4 ± 2.87
11.6 ± 2.8
12 ± 1.9
14.4 ± 6.55
16.1 ± 0.94
16.1 ± 2.77
16.8 ± 3.22
17 ± 1.18
17.1 ± 0.94
18 ± 5.02
Fig. 10: ROC curve for ERF classiﬁer on all features.
VI-B. Detection on a Separate Validation Set
To get insights about how our classiﬁer would perform
on samples it has never seen, on a dataset disjoint with the
ground truth, we perform an experiment on a test set of 7489
malicious and 1500 benign WCGs. Note that the benign traces
are collected the same way we collected the benign ground
truth (as described in Section II). The infection samples are
drawn from pre-veriﬁed ThreatGlass [3] malware infection
intelligence. We submit the same test set to VirusTotal and