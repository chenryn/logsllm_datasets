e
u
T
d
e
W
e
u
T
n
o
M
Time
Figure 5: Trafﬁc remaining after applying a 15 KB cutoff.
Figure 7: Retention time with 2.1 TB disk buffer at MWN.
y
t
i
s
n
e
D
0
2
5
1
0
1
5
0
MWN
LBNL
y
t
i
s
n
e
D
6
.
0
5
.
0
4
0
.
3
.
0
2
.
0
1
.
0
0
.
0
MWN (750 MB buffer)
LBNL (150 MB buffer)
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
12
CPU utilization
Figure 6: CPU utilization (across all cores).
Retention time [min]
Figure 8: Retention in memory buffer.
lined in §5.1.4 During the measurement period, the TM setup expe-
rienced only rare packet drops. At MWN the total packet loss was
less than 0.04% and at LBNL less than 0.03%. Our investigation
shows that during our measurement periods these drops are most
likely caused by computation spikes and scheduling artifacts, and
do not in fact correlate to bandwidth peaks or variations in connec-
tion arrival rates.
We start by examining whether the cutoff indeed reduces the data
volume sufﬁciently, as our simulation predicted. Fig. 4 plots the
original input data rates, averaged over 10 sec intervals, and the
data rates after applying the cutoff for MWN and LBNL. (One can
clearly see that at MWN the maximum is limited by the 1 Gbps
monitoring link.) Fig. 5 shows the fraction of trafﬁc, the reduction
rate, that remains after applying the cutoff, again averaged over
10 sec intervals. While the original data rate reaches several hun-
dred Mbps, after the cutoff less than 6% of the original trafﬁc re-
mains at both sites. Hereby, the reduction rate at LBNL exhibits a
higher variability. The reduction ratio shows a diurnal variation: it
decreases less during daytime than during nighttime. Most likely
this is due to the prevalence of interactive trafﬁc during the day
which causes short connections while bulk-transfer trafﬁc is more
prevalent during the night due to backups and mirroring.
Next, we turn to the question whether the TM has sufﬁcient re-
sources to leave head-room for query processing. We observe that
the CPU utilization (aggregated over all CPU cores, i.e., 100% re-
ﬂects saturation of all cores) measured in 10 sec intervals, shown
in Fig. 6, averages 25% (maximum ≈ 85%) for MWN indicating
4During two time periods (one lasting 21 h, the other 4 days) the
NIDS was not connected to the TM and therefore did not send any
queries.
that there is enough head room for query processing even at peak
times. For LBNL, the CPU utilization is even lower, with an aver-
age of 5% (maximum ≈ 50%). (The two local maxima for MWN
in Fig. 6 are due to the diurnal effects.)
Fig. 7 shows how the retention time changes during the run at
MWN. The 2.1 TB disk buffer provides ≈ 4 days during a normal
work week, as one would expect given a ≈ 90% reduction in cap-
ture volume starting from 3–6 TB/day. After an initial ramp-up
phase, the system retains an average of 4.3 days of network pack-
ets. As depicted in Fig. 8, the retention time in the memory buffer is
signiﬁcantly shorter: 169 sec of network trafﬁc on average (41 sec
minimum) for MWN. The local maxima are at 84 sec, and 126 sec
respectively, due to the diurnal effects. At LBNL we achieve larger
retention times. The 500 GB disk buffer retained a maximum of
more than 15 days, and the 150 MB memory buffer (Fig. 8) was
able to provide 421 sec on average (local maxima at 173 sec, and
475 sec).
Overall, our experience from these deployments is that the
TM can satisfy queries for packets observed within the last days
(weeks), providing that these are within the connection’s cutoff.
Moreover, the TM can answer queries for packets within the past
couple of minutes very quickly as it stores these in memory.
4.2 Querying
As we plan to couple the TM with other applications, e.g., an
intrusion detection system, that automatically generates queries it
is important to understand how much load the TM can handle. Ac-
cordingly, we now examine the query performance of the TM with
respect to (i) the number of queries it can handle, and (ii) the latency
between issuing queries and receiving the corresponding replies.
For these benchmarks, we ran the TM at LBNL on the same sys-
Reply rate
Query rate
d
n
o
c
e
s
r
e
p
s
e
i
r
e
u
Q
0
0
2
0
5
1
0
0
1
0
5
0
(b)
(a)
y
t
i
s
n
e
D
4
0
.
0
3
0
.
0
2
0
0
.
1
0
0
.
0
0
0
.
0
1000
2000
3000
4000
0
100
200
300
400
500
Time [sec]
Latency [ms]
Figure 9: Queries at increasing rates.
Figure 10: Latency between queries and replies.
tem as described above. For all experiments, we conﬁgured the TM
with a memory buffer of 150 MB and a cutoff of 15 KB.
We focus our experiments on in-memory queries, since accord-
ing to our experience these are the ones that are issued both at
high rates and with the timeliness requirements for delivering the
replies. In contrast, the execution of disk-based queries is heavily
dominated by the I/O time it takes to scan the disk. They can take
seconds to minutes to complete and therefore need to be limited to
a very small number in any setup; we discuss this further in §6.
Load: We ﬁrst examine the number of queries the TM can support.
To this end, we measure the TM’s ability to respond to queries that
a simple benchmark client issues at increasing rates. All queries
request connections for which the TM has data, so it can extract
the appropriate packets and send them back in the same way as it
would for an actual application.
To facilitate reproducible results, we add an ofﬂine mode to the
TM: rather than reading live input, we preload the TM with a pre-
viously captured trace. In this mode, the TM processes the packets
in the trace just as if it had seen them live, i.e., it builds up all of
its internal data structures in the same manner. Once it ﬁnishes
reading the trace, it only has to respond to the queries. Thus, its
performance in this scenario may exceed its performance in a live
setting during which it continues to capture data thus increasing its
head-room for queries. (We veriﬁed that a TM operating on live
trafﬁc has head-room to sustain a reasonable query load in realistic
settings, see §5.3.)
We use a 5.3 GB full trace captured at LBNL’s uplink, spanning
an interval of 3 min. After preloading the TM, the cutoff reduces
the buffered trafﬁc volume to 117 MB, which ﬁts comfortably into
the conﬁgured memory buffer. We conﬁgure the benchmark client
to issue queries from a separate system at increasing rates: starting
from one query every two seconds, the client increases the rate by
0.5 queries/sec every 10 seconds. To ensure that the client only
issues requests for packets in the TM’s memory buffer, we supplied
it with a sample of 1% of the connections from the input trace. Each
time the client requests a connection, it randomly picks one from
this list to ensure that we are not unfairly beneﬁting from caching.
On the TM, we log the number of queries processed per second.
As long as the TM can keep up, this matches the client’s query rate.
Fig. 9 plots the outcome of the experiment. Triangles show the rate
at which queries were issued, and circles reﬂect the rate at which
the TM responded, including sending the packets back to the client.
We see that the TM can sustain about 120 queries/secs. Above that
point, it fails to keep up. Overall, we ﬁnd that the TM can handle
a high query rate. Moreover, according to our experience the TM’s
performance sufﬁces to cope with the number of automated queries
generated by applications such as those discussed in §5.
Latency: Our next experiment examines query latency, i.e., the
time between when a client issues a query and its reception of the
ﬁrst packet of the TM’s reply. Naturally, we wish to keep the la-
tency low, both to provide timely responses and to ensure accessi-
bility of the data (i.e., to avoid that the TM has expunged the data
from its in-memory buffer).
To assess query latency in a realistic setting, we use the following
measurement with live LBNL trafﬁc. We conﬁgure a benchmark
client (the Bro NIDS) on a separate system to request packets from
one of every n fully-established TCP connections. For each query,
we log when the client sends it and when it receives the ﬁrst packet
in response. We run this setup for about 100 minutes in the early af-
ternoon of a work-day. During this period the TM processes 73 GB
of network trafﬁc of which 5.5 GB are buffered on disk at termi-
nation. The TM does not report any dropped packets. We choose
n = 100, which results in an average of 1.3 connections being re-
quested per second (σ= 0.47). Fig. 10 shows the probability density
of the observed query latencies. The mean latency is 125 ms, with
σ= 51 ms and a maximum of 539 ms (median 143 ms). Of the 7881
queries, 1205 are answered within less than 100 ms, leading to the
notable peak “(a)” in Fig. 10. We speculate that these queries are
most likely processed while the TM’s capture thread is not perform-
ing any signiﬁcant disk I/O (indeed, most of them occur during the
initial ramp-up phase when the TM is still able to buffer the net-
work data completely in memory). The second peak “(b)” would
then indicate typical query latencies during times of disk I/O once
the TM has reached a steady-state.
Overall, we conclude that the query interface is sufﬁciently re-
sponsive to support automatic Time Travel applications.
5. COUPLING TM WITH A NIDS
Network intrusion detection systems analyze network trafﬁc in
real-time to monitor for possible attacks. While the real-time nature
of such analysis provides major beneﬁts in terms of timely detec-
tion and response, it also induces a signiﬁcant constraint: the NIDS
must immediately decide when it sees a network packet whether it
might constitute part of an attack.
This constraint can have major implications, in that while at the
time a NIDS encounters a packet its content may appear benign,
future activity can cast a different light upon it. For example, con-
sider a host scanning the network. Once the NIDS has detected the
scanning activity, it may want to look more closely at connections
originating from that source—including those that occurred in the
past. However, any connection that took place prior to the time of
detection has now been lost; the NIDS cannot afford to remember
the details of everything it has ever seen, on the off chance that at
some future point it might wish to re-inspect the activity.
The TM, on the other hand, effectively provides a very large
buffer that stores network trafﬁc in its most detailed form, i.e., as
packets. By coupling the two systems, we allow the NIDS to access
this resource pool. The NIDS can then tell the TM about the trafﬁc
it deems interesting, and in turn the TM can provide the NIDS with
historic trafﬁc for further analysis.
Given the TM capabilities developed in the previous section, we
now explore the operational gains achievable by closely coupling
the TM with a NIDS. We structure the discussion in ﬁve parts:
(i) our prototype deployment at LBNL; (ii) experiences with en-
abling the NIDS to control the operation of the TM; (iii) the addi-
tional advantages gained if the NIDS can retrieve historic data from
the TM; (iv) the beneﬁts of tightly coupling the two systems; and
(v) how we implemented these different types of functionality.
5.1 Prototype Deployment
Fig. 11 shows the high-level structure of coupling the TM with
a NIDS. Both systems tap into the monitored trafﬁc stream (here, a
site’s border) and therefore see the same trafﬁc. The NIDS drives
communication between the two, controlling the operation of the
TM and issuing queries for past trafﬁc. The TM then sends data
back to the NIDS for it to analyze.
We install such a dual setup in the LBNL environment, using
the open-source Bro NIDS [18]. Bro has been a primary compo-
nent of LBNL’s network monitoring infrastructure for many years,
so using Bro for our study as well allows us to closely match the
operational conﬁguration.
The TM uses the same setup as described in §4: 15 KB cutoff,
500 GB disk budget, running on a system with two dual-core Pen-
tium Ds and 4 GB of main memory. We interface the TM to the
site’s experimental “Bro Cluster” [26], a set of commodity PCs
jointly monitoring the institute’s full border trafﬁc in a conﬁgu-
ration that shadows the operational monitoring (along with run-
ning some additional forms of analysis). The cluster consists of
12 nodes in total, each a 3.6 GHz dual-CPU Intel Pentium D with
2 GB RAM.
We conducted initial experiments with this setup over a number
of months, and in Dec. 2007 ran it continuously through early Jan.
2008 (see §4.1). The experiences reported here reﬂect a subsequent
two-week run in Jan. 2008. During this latter period, the systems
processed 22.7 TB of network data, corresponding to an average