FreeBSD 4
FreeBSD 5
100,000
548
Several features immediately stand out from the data.
First, the almost identical numbers from Linux versus
FreeBSD show that our modiﬁed code has made the
probability of winning races dependent on the random-
ized delays, rather than on details of the respective oper-
ating systems and machines. Second, the k = 0 numbers
show that the ﬁrst race is almost 100% winnable. This is
because our randomized delay is at least one nanosleep
long, so the attacker knows it can wait one nanosleep
and always win the ﬁrst race (except for rare instances
when the two processes start up extremely out of sync).
Finally, as k grows, the ratio of successive success rates
is dropping slightly. This occurs because the attacker
was tuned for smaller values of k, and as k grows, the
attacker gradually slips out of phase from the victim.
As we can see, even in this extreme case, where the un-
strengthened access(2)/open(2) race is almost 100% in-
secure and all races are constructed to be easy-to-win,
each successive round of strengthening provides a mul-
tiplicative improvement in security, as predicted by the
theoretical model.
Practical Guidance for Choosing k: From a practical
perspective, with realistic victim programs (that don’t go
to sleep to wait for attackers), we have observed p to be
on the order of 10−6 to 10−1. This suggests that for
k = 7, the probability of a successful attack should be
below 10−15. Given that running one million attacks
takes on the order of tens of hours, a successful attack
probability of 10−15 should provide adequate security
in most situations. As there are 8760 hours in a year, it
is unlikely that even a cluster of 100 machines would re-
main running long enough to expect to see a successful
attack. We note that the speed of this attack appears to
be scaling with disk speed, rather than CPU speed.6 The
relatively long duration of a trial, especially as compared
to the evaluation of a hash function or block cipher, mean
that we can allow a somewhat higher probability of at-
tack than would be acceptable in other settings.
5.4 Strengthening Strengthening
Implementation details, as always, are critical to the se-
curity of a system using our algorithm. So far, we have
presented a highly portable design.
If one is willing
to trade off portability for stronger security, a number
of improvements can be made. These improvements
will generally serve to decrease the possible number of
context switches that could occur in the critical section,
thereby decreasing worst case (real) execution time, and
thereby narrowing the attacker’s window. We will dis-
cuss these optimizations from most portable to least
portable.
First, if the setuid program (victim) is running as root,
it should raise its scheduling priority with a nice(2) or
setpriority(2) call with a negative argument. This opti-
mization appears to be completely portable.
Second, the virtual memory page(s) containing the code
to implement our algorithm should be pinned into real
memory. The mlock(2) call is a portable way of accom-
plishing this across all the operating systems discussed
in this paper, although one needs to be careful to balance
mlock(2) and munlock(2) calls correctly, as different op-
erating systems have different semantics for nested calls.
This optimization will prevent a page fault from occur-
ring and giving the attacker’s process a chance to run.
that
systems
imple-
Third, on Linux and other
ment POSIX process scheduling, one can use the
sched setscheduler(2) call to elevate the process priority
above what can be accomplished with nice(2) or setpri-
ority(2). If the setuid program is running as root, it can
use SCHED FIFO with an appropriate priority to make
sure that it will run whenever it is runnable.
These optimizations further reduce the probability of at-
tack by making it harder for an attacker to win races.
While the ﬁrst and third optimizations would be redun-
dant, using one of them depending on portability consid-
erations is highly recommended. The second optimiza-
tion is fairly portable, and is recommended wherever it
applies.
6We heard disk drives grinding away during our experiments.
1
0.8
0.6
0.4
0.2
0
0
Linux
FreeBSD
Theoretical w/ p=0.65
Theoretical w/ p=0.60
Theoretical w/ p=0.55
1
2
3
4
5
Figure 3: Strengthening with Randomized Nanosleeps. The theoretical curve has been reﬁned from p2k+1 to p0p2k,
with p0 = 1, because the attacker in these experiments can almost always win the ﬁrst race.
5.5 A Note on Kernel Issues
In general, and for multiprocessor machines in particu-
lar, the probabilistic security we have achieved appears
to be all that is possible: another CPU can alter the ﬁle
system concurrently with the victim program’s actions.
However, on a uniprocessor system, we are aware of
only ﬁve ways for a process to yield the CPU on most
Unix-like operating systems:
• Be traced (either with ptrace(2) or through the
proc(5) ﬁle system)
• Perform I/O
• Have the timer interrupt go off
• Take a page fault
• Receive a signal
We address these in order. Our discussion here is limited
to the context of a setuid program running concurrently
with a non-setuid program attempting to exploit the ac-
cess(2)/open(2) race. This analysis explicates some of
the details that are hidden by the probabilistic abstrac-
tion we have used so far.
Tracing Either the ptrace(2) system call or the proc(5)
ﬁle system provide a means to trace a process, typically
for debugging purposes. One cannot trace a setuid pro-
cess, as this would lead to obvious security vulnerabili-
ties.7 Hence, we need not consider tracing any further.
I/O A process yields the CPU when it needs to per-
form I/O operations, e.g., disk reads or writes that miss
in the ﬁle system buffer cache. While the victim pro-
gram is making many access(2) and open(2) calls, be-
cause of the ﬁle system buffer cache, it will be very dif-
ﬁcult, if not impossible, for other processes to cause the
inodes traversed in the access(2) call to be ﬂushed from
the buffer cache before they are traversed again by the
open(2) call. In order for this to happen, another pro-
cess would have to be doing I/O, which would imply
that said process itself is put to sleep. One could per-
haps imagine enough cooperating attack processes allo-
cating and using lots of memory, while also all doing
7At least in theory. Various vulnerabilities in this area have been
found over the years in different kernels. However, such kernel vul-
nerabilities directly lead to machine compromise regardless of the ac-
cess(2)/open(2) race.
I/O at the same time in order to make the race condi-
tion be winnable more than once, but this would appear
to be a rather difﬁcult attack to pull off. Basically, we
expect (with very high probability) that the open(2) call
will never go to disk, because everything was loaded into
the buffer cache by the previous access(2) call. We ob-
serve that many modern systems (e.g., FreeBSD) have
uniﬁed their ﬁle system buffer caches with their virtual
memory allocation.
In such systems, we observe that
it would be most useful to have a guaranteed minimum
ﬁle system buffer cache size, so that directory entries
and inodes won’t be discarded from the cache to satisfy
user processes’ requests for memory. While many sys-
tems provide limits for number of processes per user and
memory use per process, these controls are typically too
coarse to be effective for bounding memory use.
Timer Interrupt Unix-like operating systems gener-
ally implement preemptive multitasking via a timer in-
terrupt. The frequency of the timer interrupt is gen-
erally in the range of 50–1000Hz. This frequency has
not changed dramatically as CPU clock speeds have in-
creased. We believe that this is due to the fact that human
perception hasn’t changed, either: if the human users are
satisﬁed with the system’s interactive latencies, it makes
sense to reduce the overhead as much as possible by
keeping the frequency of the timer interrupt low.
The prototypical victim program that we experimented
with has 15 instructions in user mode between the ac-
tual system calls (i.e., int 0x80s) that implement ac-
cess(2) and open(2), when using GCC 2.95.3 and glibc
2.2.5. The time required to execute the 15 user mode in-
structions has, of course, decreased dramatically as CPU
speeds have increased. This helps prevent the exploita-
tion of the race in two ways: ﬁrst, it gives the timer inter-
rupt an ever shrinking window of time to occur in, and
second, the victim program will be able to run at least
one round of the strengthening protocol without inter-
ference from the timer interrupt.
Page Faults
If we assume that our algorithm is run-
ning as the superuser (e.g., a setuid root program), then
the program can call mlock(2) to pin the page containing
the code into memory, so it will never take a page fault.
Processes not running as root cannot take advantage of
page pinning on systems the authors are familiar with.
Signals The last way of causing a process to yield the
CPU is to have a signal delivered to it. Again, on all
the Unix-like operating systems the authors are familiar
with, signal delivery is handled at the point that the oper-
ating system is about to return to user mode, either from
a system call, or an interrupt, such as the timer interrupt.
We note that on Linux 2.4.18, the code for posting a sig-
nal to a process includes logic that dequeues a pending
SIGCONT (and equivalents) if a SIGSTOP (or equiv-
alent) signal is being delivered, and vice versa. This
implies that the attacker cannot use signals to single-
step the victim through system calls. The attacker can
stop and restart the victim program at most once due to
the length of scheduling quanta. A similar result is true
of the timer interrupt: given the size of the scheduling
quantum, all of the code will execute as part of at most 2
scheduling quanta. So again, the attacker gets 1 chance
to change the ﬁle system around, but they need at least 3
changes to the ﬁle system to succeed against 1 round of
strengthening.
Observation In summary, it appears that Linux 2.4.18,
when running on modern uniprocessor machines, and
with the victim program having superuser privileges, can
provide more security than one would assume from the
model and experiments presented above. That is, with
one round of strengthening, the attacker must make three
sets of modiﬁcations to the ﬁle system to succeed with
an attack, but the timer interrupt will only give the at-
tacker one chance to run. Linux’s signal handling behav-
ior prevents the attacker from single-stepping the victim
at system call granularity.
This analysis appears to support a conjecture that on
Linux 2.4.18, running as root (and therefore able to
use SCHED FIFO and mlock(2), uniprocessor machines
achieve deterministic security with only one round of
strengthening. While this analysis is intellectually inter-
esting, we strongly urge that it not be used, as it depends
on code never being run on a multiprocessor (very difﬁ-
cult to ensure as systems evolve over time), and undocu-
mented behavior of a particular kernel version, which is
always subject to change.
only linear work. This is the same sort of security as
modern cryptology gives, although we use arguably sim-
pler assumptions. We note that either a probabilistic so-
lution as presented in this paper or dropping privilege
via setuid(2) are fundamentally the only viable solutions
if one is unwilling or unable to alter the kernel. The
way Linux handles pending SIGSTOP and SIGCONT
signals provides additional security against TOCTTOU
attacks. Other kernels should investigate adding simi-
lar code to their signal posting routines, although this is
not a completely general solution – multiprocessor ma-
chines inherently can achieve only a probabilistic guar-
antee. With appropriate parameter choices, this algo-
rithm, within its limitations regarding side effects, re-
stores the access(2) system call to the toolbox available
to the developer of setuid Unix programs.
Acknowledgments
We wish to thank Whitﬁeld Difﬁe for access to old Sun
hardware. The staff (and stock) of Weird Stuff Electron-
ics8 was very helpful as well. We thank Steven Bellovin,
Brian Kernighan, Nancy Mintz, Dennis Ritchie, and
Jonathan Shapiro for historical information about the ac-
cess(2)/open(2) race. We thank the anonymous refer-
ees for helpful feedback on an earlier draft of this paper.
Drew Dean wishes to acknowledge a conversation with
Dirk Balfanz, Ed Felten, and Dan Wallach on the beach
at SOSP ’99 that ﬁrmly planted this problem in his mind,
though the solution presented in this paper was still years
away.
References
[1] Aleph1. Smashing the stack for fun and proﬁt.
Phrack #49, November 1996. http://www.
phrack.org/show.php?p=49&a=14.
6 Conclusion
[2] Steven M. Bellovin.
Writing (more)
//www.research.att.com/˜smb/
talks/odds.pdf, December 1994.
software.
secure
Shifting the odds:
http:
The race condition preventing the intended use of the
access(2) system call has existed since 1979. To date,
the only real advice on the matter has been “don’t use
access.” This is unfortunate, as it provides useful func-
tionality. We have presented an algorithm that gains
exponential advantage against the attacker while doing
[3] Sandeep Bhatkar, Daniel C. DuVarney,
and
R. Sekar. Address obfuscation: An efﬁcient ap-
proach to combat a broad range of memory error
exploits. In Proceedings of the 12th USENIX Secu-
rity Symposium, pages 105–120, Washington, DC,
August 2003.
8http://www.weirdstuff.com
[4] Matt Bishop. How to write a setuid program. ;lo-
gin:, 12(1):5–11, 1987.
[5] Matt Bishop. Computer Security: Art and Science.
Addison-Wesley, 2003.
[6] Matt Bishop and Michael Dilger. Checking for race
conditions in ﬁle accesses. Computing Systems,
9(2):131–152, Spring 1996.
[15] Eugene Tsyrklevich and Bennet Yee. Dynamic de-
tection and prevention of race conditions in ﬁle ac-
cesses. In Proceedings of the 12th USENIX Secu-
rity Symposium, pages 243–256, Washington, DC,
August 2003.
[16] A. C. Yao. Theory and application of trapdoor
functions.
In Proc. 23rd IEEE Symp. on Foun-
dations of Comp. Science, pages 80–91, Chicago,
1982. IEEE.
[7] CERT Coordination Center.
xterm logging
CERT Advisory CA-1993-17,
http://www.cert.org/
vulnerability.
October 1995.
advisories/CA-1993-17.html.
[8] Hao Chen, David Wagner, and Drew Dean. Setuid
demystiﬁed. In Proceedings of the Eleventh Usenix
Security Symposium, San Francisco, CA, 2002.
[9] Crispan Cowan, Calton Pu, Dave Maier, Jonathan
Walpole, Peat Bakke, Steve Beattie, Aaron Grier,
Perry Wagle, Qian Zhang, and Heather Hinton.
StackGuard: Automatic adaptive detection and
prevention of buffer-overﬂow attacks. In Proc. 7th
USENIX Security Conference, pages 63–78, San
Antonio, Texas, January 1998.
[10] Crispin Cowan, Steve Beattie, Chris Wright, and
Greg Kroah-Hartman. RaceGuard: Kernel protec-
tion from temporary ﬁle race vulnerabilities.
In
Proceedings of the 10th USENIX Security Sympo-
sium, Washington, DC, August 2001.
[11] W. S. McPhee. Operating system integrity in
IBM Systems Journal, 13(3):230–252,
OS/VS2.
1974.
[12] Greg Morrisett, David Walker, Karl Crary, and
Neal Glew. From system F to typed assembly lan-
guage. ACM Transactions on Programming Lan-
guages and Systems, 21(3):527–568, May 1999.
[13] George C. Necula. Proof-carrying code. In Pro-
ceedings of the 24th ACM SIGPLAN-SIGACT Sym-
posium on Principles of Programming Languages,
pages 106–119, January 1997.
[14] Thomas Toth and Christopher Kruegel. Accu-
rate buffer overﬂow detection via abstract payload
execution.
In Andreas Wespi, Giovanni Vigna,
and Luca Deri, editors, Proceedings Fifth Sympo-
sium on Recent Advances in Intrusion Detection,
volume 2516 of LNCS, pages 274–291, Zurich,
Switzerland, October 2002. Springer-Verlag.