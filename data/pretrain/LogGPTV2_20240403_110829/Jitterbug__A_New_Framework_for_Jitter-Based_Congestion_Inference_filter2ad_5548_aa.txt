title:Jitterbug: A New Framework for Jitter-Based Congestion Inference
author:Esteban Carisimo and
Ricky K. P. Mok and
David D. Clark and
Kimberly C. Claffy
Jitterbug: A New Framework
for Jitter-Based Congestion Inference
Esteban Carisimo1(B), Ricky K. P. Mok2
, David D. Clark3, and K. C. Claﬀy2
1 Northwestern University, Evanston, USA
PI:EMAIL
2 CAIDA, UC San Diego, San Diego, USA
{cskpmok,kc}@caida.org
3 MIT, Cambridge, USA
PI:EMAIL
Abstract. We investigate a novel approach to the use of jitter to infer
network congestion using data collected by probes in access networks.
We discovered a set of features in jitter and jitter dispersion —a jitter-
derived time series we deﬁne in this paper—time series that are char-
acteristic of periods of congestion. We leverage these concepts to create
a jitter-based congestion inference framework that we call Jitterbug. We
apply Jitterbug’s capabilities to a wide range of traﬃc scenarios and
discover that Jitterbug can correctly identify both recurrent and one-oﬀ
congestion events. We validate Jitterbug inferences against state-of-the-
art autocorrelation-based inferences of recurrent congestion. We ﬁnd that
the two approaches have strong congruity in their inferences, but Jitter-
bug holds promise for detecting one-oﬀ as well as recurrent congestion.
We identify several future directions for this research including lever-
aging ML/AI techniques to optimize performance and accuracy of this
approach in operational settings.
1 Introduction
The general notion of network congestion – demand exceeds capacity for network
(link capacity or router buﬀer) resources – is widespread on the Internet, and
an inherent property of traditional TCP dynamics. A TCP connection endpoint
induces congestion to infer its appropriate sending rate, increasing this rate until
it fails to receive acknowledgement of receipt of a packet by the other endpoint,
i.e., infers congestion based on packet loss [19]. More recent attempts to improve
TCP’s congestion control algorithms rely on increased latency rather than packet
loss as a signal of congestion [7,8,22,35].
Outside of protocol dynamics, latency and loss are still the fundamental met-
rics used to detect episodes of network congestion, or more generally path anoma-
lies that degrade performance [12–15,17,30]. Although researchers have developed
autocorrelation techniques to infer persistent recurrent patterns congestion [12],
the challenge of detecting one-oﬀ episodes of congestion in traﬃc data remains an
open problem after 30 years of Internet evolution. One-oﬀ episodes of congestion
c(cid:2) The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
O. Hohlfeld et al. (Eds.): PAM 2022, LNCS 13210, pp. 155–179, 2022.
https://doi.org/10.1007/978-3-030-98785-5_7
156
E. Carisimo et al.
have many causes, including traﬃc management transitions, router operating sys-
tem overheads, network conﬁguration errors, ﬂash crowds (e.g., software releases),
and DDoS attacks. Inferring congestion from these phenomenological events is still
an open challenge for the research network community.
We propose a new framework – Jitterbug – to use jitter and other metrics
derived from round-trip-time (RTT) measurements to infer congestion. RTT
measurements alone are often insuﬃcient to infer congestion episodes, but we
found that jitter-related metrics can distinguish congestion from other path
anomalies, e.g., route changes. Speciﬁcally, we identify a correlation between
periods of elevated latency (minimum RTT) and changes in the proﬁle of jit-
ter signatures – jitter dispersion – during congestion episodes. Relying on this
concept, we develop a new framework that allows us to extend interdomain
congestion inferences from recurrent patterns to one-oﬀ congestion events, i.e.,
discern recurrent from one-time congestion events. Using data collected between
2017 and 2020, this novel approach obtains similar results to state-of-the-art
autocorrelation-based methods [12], but overcomes the limitation of the auto-
correlation methods that can only detect recurrent periodic patterns of conges-
tion. We ﬁnd that Jitterbug introduces a promising approach to detect one-oﬀ
congestion events. Our contributions are:
1. We identiﬁed a set of features in jitter and jitter dispersion time series, includ-
ing a change of regime or transitory increase of the jitter dispersion, that
characterize periods of congestion.
2. We used these features to develop and implement Jitterbug, a new jitter-
based congestion inference method that combines pre-existing approaches to
change point detection with information embedded in jitter signals.
3. We applied the Jitterbug framework to a wide range of challenging traﬃc
4. We
compare
Jitterbug
congestion
scenarios, and explain its inferences.
art autocorrelation-based methods
autocorrelation-applicable scenarios, i.e., for recurrent periodic congestion.
inferences
state-of-the-
[12], ﬁnding strong consistency in
the
to
5. We release the source of code of Jitterbug1.
The rest of the paper is structured as follows. We provide context by describ-
ing the latency model (Sect. 2.1) and jitter signatures in multiple real-world
examples (Sect. 2.2). Leveraging these concepts, Sect. 3 describes Jitterbug and
its components in detail. Section 4 describes the dataset we use to (i) inves-
tigate Jitterbug congestion inferences in diﬀerent scenarios (Sect. 5), and (ii)
cross-validate Jitterbug congestion inferences against other methods (Sect. 6).
Section 7 summarizes lessons we learned during our study. Section 8 provides an
extensive list of related work and Sect. 9 discusses open challenges in congestion
inference. Finally, Sect. 10 oﬀers concluding thoughts.
2 Background on RTT and Jitter Signatures
To provide context, we describe the latency model (Sect. 2.1) and four typical
signatures we extract from RTTs and jitter (Sect. 2.2).
1 Jitterbug repository: https://github.com/estcarisimo/jitterbug.
Jitterbug: A New Framework for Jitter-Based Congestion Inference
157
2.1 Latency Model
Round-trip time (RTT) in end-to-end measurements comprises both determin-
istic and random components. Equation (1) depicts the components of RTT
between source (u) and destination (v) for a packet traversing a total of H
hops in the round-trip path [21].
RT T (u, v) = dicmp +
H(cid:2)
i=0
(ds(i) + dprop(i) + dq(i) + dproc(i)),
(1)
where dicmp is the processing delay of ICMP messages in routers. ds, dprop,
and dproc represent delay induced by serialization, propagation, and packet pro-
cessing, respectively. These deterministic components do not depend on traﬃc
volume or link utilization. In contrast, dicmp and dq are random variables and
contribute RTT variance, because their values depend on router CPU utiliza-
tion and queue size of network interfaces when packets arrive. Prior work [12,23]
has shown that RTT correlates with bottleneck link utilization, indicating that
the queuing delay is the dominant factor in delay variation. Delay jitter, also
referred to as jitter or IP packet delay variation [10], is the absolute diﬀerence
between the current RTT value and the reference value of the previous time
episode (i.e., JT = RT T (u, v)T − RT T (u, v)T−1), where T is the current time
episode. In this work we develop and evaluate a framework for using simple RTT
and jitter-based metrics to classify path anomalies.
2.2 Analyzing RTT and Jitter Signatures in Congested Links
We use four real-world examples to illustrate the challenges and opportunities of
using RTT and jitter to detect and identify path anomalies (Fig. 1). We focus on
three properties of RTT and jitter to characterize the nature of path anomalies:
periodicity, amplitude, variability.
Periodicity captures events that recur at a ﬁxed frequency and duration, such as
diurnal variations.
Amplitude measures the degree of changes in RTTs from the baseline. During
network congestion events, probe packets are more likely to experience queu-
ing delay. The elevation of RTTs reﬂects the queue size in the bottleneck
link.
Variability refers to the stability of RTTs during the elevated periods, which
allows us to discern congestion from other path anomalies such as a route
change.
Figure 1 shows four examples of two-week RTT and jitter time series mea-
sured from four vantage points in the U.S. to four router interfaces on the far-
side2 of interdomain links. Two examples (Fig. 1a and 1b) show periodic inﬂation
2 We referred as near and far sides to consecutive IP pairs in a traceroute path fol-
lowing the convention deﬁned by Luckie et al. [23].
158
E. Carisimo et al.
in RTTs (blue/orange curves), indicating recurring congestion events. However,
the jitter amplitude (green curve) in Fig. 1b, is much lower than that of Fig. 1a,
consistent with a smaller queue size in the bottleneck link. Previous use of auto-
correlation methods have shown that such persistent diurnal elevations in RTT
at the far-side of an interdomain are evidence of interdomain congestion [12].
In contrast, the two cases in Fig. 1c and 1d are one-oﬀ events. The interesting
diﬀerence is that in Fig. 1d the jitter increases as the RTT baseline jumps from
20 ms to 40 ms. In contrast, in Fig. 1c the jitter remains stable throughout. We
suspect that this latter scenario was a route change event rather than congestion.
Although many diﬀerent approaches to RTT change point detection could
partition these time series into intervals, an approach solely based on RTTs would
fail to distinguish congestion from other path anomalies such as route changes.
The RTT signal is simply too noisy. This example shows that evaluating changes
in jitter can enable us to diﬀerentiate these scenarios and thus we should consider
jitter as a metric for characterizing path anomalies.
We next introduce our framework to support systematic analysis and clas-
siﬁcation of type of path anomalies with three properties that we extract from
RTT and jitter time series data.
3 Jitterbug: Jitter-Based Congestion Inference
Figure 2 shows the building blocks of our framework, which combines change
point detection algorithms (Sect. 3.2) with simultaneous analysis of minimum
RTT and jitter time series obtained from latency measurements. The change
point detection algorithm splits RTT timeseries into candidate time intervals
that might suﬀer from congestion. The next step of the framework is to ana-
lyze the jitter in each time interval to classify candidate intervals as congestion
events or other path anomalies. We infer congestion based on the three elements
we observed in Sect. 2.2: changes in baseline RTT, increase of jitter amplitude,
and increase of jitter dispersion during a phase transition. We developed two
diﬀerent statistical methods for this analysis– (i) KS-test method, and (ii) jit-
ter dispersion method (JD). The ﬁrst combines detection of changes on RTT
latency baseline with the Kolmogorov-Smirnov (KS) test to detect changes in
the jitter time series. The jitter dispersion method (JD) detects a jitter dispersion
increase that correlates with a baseline RTT increases as a signal of congestion.
The common goal of both methods is to objectively capture the signatures in
the jitter signals. This section describes the role of each element of the Jitter-
bug framework in detail. We designed Jitterbug to support diﬀerent RTT data
sources, and have applied it to measurements collected by Ark CAIDA and RIPE
Atlas. The current implementation uses a 5-min and 15-min granularity for RTT
measurements and the aggregated minimum RTT time-series, respectively.
3.1 Signal Filtering
Jitterbug congestion inferences use three signals: (i) min RTT time series, (ii)
jitter, and (iii) jitter dispersion. As we saw in Sect. 2.2, raw RTTs can be too
Jitterbug: A New Framework for Jitter-Based Congestion Inference
159
(a) Recurring congestion event. RTTs and
jitter increase during congestion episodes.
(b) Recurring congestion event. Jitter does
not signiﬁcantly increase during elevated
RTT periods, likely due to small buﬀer size
in the bottleneck router.
(c) One-oﬀ non-congestion event on Jan 2.
Jitter remained stable in face of inﬂated
RTT.
(d) One-oﬀ congestion event. Increased
varibility in jitter indicates the occurance
of congestion.
Fig. 1. Typical examples of network events. The raw timeseries (top ﬁgures) is the
raw RTT data with 5-min resolution. We aggregate the raw data into 15-min buckets
with the minimum function to ﬁlter noise (middle ﬁgures). We compute jitter using
the 15-min aggregated data to quantify variability in RTTs (bottom ﬁgures). (Color
ﬁgure online)
noisy to yield meaningful signatures. We ﬁrst aggregate the raw RTT data by
selecting the minimum value in each 15-min time interval (min time series). The
signal ﬁltering module then computes the jitter using both the raw RTT and
min time series to produce jitter and j-min time series, respectively.
We use two additional ﬁlters to better capture the variability in j-min. First,
we apply the Moving IQR ﬁlter to the j-min time series, which computes the
inter-quartile range (IQR) of a sliding window of 150 min (10 jitter samples).
We deﬁne as jitter dispersion to the operation of computing the moving IQR to
a jitter signal.
We then compute the 5-sample moving average of the resultant time series
as the jitter dispersion time series to mitigate the impact of short-term latency
spikes. Fig. 3 shows the correlation between the min RTT time series and the
jitter dispersion of previous examples (Fig. 1). Correlation between the two time
series in Fig. 3c) is low. We believe that the shift of baseline RTT corresponds
to a route change that increased the propagation delay, which is a deterministic
component that induces low variance to RTTs.
160
E. Carisimo et al.
Jitter computation
Jitter
j-min
RTT
minRTT RTT
dataset
Mov. IQR filter
MA filter
Jitter disp.
KS test
Signal energy
comparison
 Combine changes in jitter
 & min time series
thersholds
Memory
A
§4
B
§3.1
D
§3.3
F
§3.5
minRTT
Interval Detection
(Change point
detection)
C
§3.2
Latency jump