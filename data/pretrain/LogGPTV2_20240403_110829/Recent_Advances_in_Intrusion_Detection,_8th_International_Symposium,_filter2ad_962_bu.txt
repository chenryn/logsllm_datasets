# TCPopera Architecture and Components

## 1. Physical Link
### (a) TCPopera Components
- **Human Intervention**
- **Input Trace Records**
- **Traffic Parameters**
- **Trace Analysis**
- **Network Configurations**
- **Flow Preprocessing**
- **Adjusting Traffic Parameters**
- **IP Flows**
- **Environment Transformation**
- **IP Flow Processing**
- **New Traffic Parameters**
- **Interactive Flow Processing**
- **New Network Configurations**
- **Output Data Flow**
- **Input Data Flow**
- **User Input**

### (b) TCPopera Flow Model
This model illustrates the flow processing methodology within the Flow Processing and IP Flow Processing components, as shown in Figure 1(a).

**Figure 1: TCPopera Architecture**

## 2. Environment Transformation
The Flow Preprocess component supports IP address remapping and ARP emulation. Users can specify remapping functions via configuration files. If there are no conflicts among remapping entries, Flow Preprocess remaps IP addresses and recalculates the IP header checksum for each packet. Additionally, it collects MAC addresses of hosts from trace records and provides them to the Flow Process component. It also rewrites MAC addresses of packets destined for a default router in the trace records to match the new default router in the test network.

## 3. IP Flow Process
The IP Flow Process is a key component of TCPopera, supporting interactive traffic replay. It creates a POSIX thread for each preprocessed IP flow, maintaining the inter-flow time observed in trace records. To achieve stateful TCP replay, it emulates a TCP control block for each TCP connection. When an IP Flow thread completes replaying all packets, it outputs the results and releases its resources. The current version of TCPopera does not support resolving dependencies among TCP connections due to complexity. Instead, it uses an ad-hoc approach by strictly preserving the inter-flow time and packet sequences within a single IP flow, as seen in the input trace records.

### Challenges and Future Improvements
While this heuristic effectively reflects the communication history between two hosts, it has limitations in handling inter-connection dependencies across different IP flows. Future development plans include improving the inter-connection dependency model.

## 4. TCPopera Control
TCPopera Control synchronizes time and information among TCPopera nodes. It provides an out-of-band communication channel for exchanging control messages and helps the IP Flow process check active TCPopera nodes and sort out replayable IP flows. One TCPopera node acts as a server to manage the synchronization procedure.

## 5. Packet Injection/Capturing
These helper components support live traffic replay. Outgoing packets from the IP Flow Process are passed to the Packet Injection component, which launches them on the wire. If any modifications are made, the checksum value is recalculated. The Packet Injection component uses the libnet library, a high-level API for constructing and injecting network packets. Incoming packets destined for the virtual addresses of a TCPopera node are captured by the Packet Capturing component and passed to the IP Flow Process. This module uses the widely used pcap utility. Each TCPopera node can have multiple virtual addresses, so the pcap process must filter to capture only packets destined for its virtual addresses.

## 6. TCP Functions
The TCP functions library provides the necessary functionalities to emulate a TCP control block. It includes features such as TCP timers, timeout and retransmission, fast retransmit and recovery, and flow and congestion control. The implementation is based on the TCP implementation in BSD4.4-Lite.

### Implementation Details
- **TCP Timers**: TCPopera uses two timers, a fast timer (200ms) and a slow timer (500ms), to support seven TCP timers.
- **Timeout & Retransmission**: TCPopera measures RTT values per connection and calculates the retransmission timeout (RTO) using smoothed RTT estimators and mean deviation estimators.
- **Fast Retransmit & Fast Recovery**: TCPopera implements these features according to modified congestion avoidance algorithms.
- **Flow & Congestion Control**: Supports slow start and congestion avoidance, using a congestion window (cwnd) and a slow start threshold size (ssthresh).

## 7. Validation Tests
### 7.1 Test Environment
Two TCPopera nodes were used in the validation tests, representing a home network and external hosts. Both nodes run on machines with 2.0 GHz Intel Pentium 4 processors and 768MB RAM. The internal TCPopera node runs on Redhat 8.0, and the external one runs on Redhat 9.0. They are connected to a dual-homed FreeBSD 5.0 firewall. Snort 2.3 with stream4 analysis was used to evaluate stateful operations.

### 7.2 Results
#### Reproductivity Test
For the reproductivity test, TCP connection-level parameters from input trace records were used to emulate TCP control blocks. The first dataset was reproduced without packet loss, and the second dataset included 1% packet loss at the BSD firewall. Table 1 shows the comparison between the input trace and replayed traces by TCPopera.

**Table 1: Comparison of Traffic Volume and TCP Connections**

| Category | IP Packets | IP Bytes | TCP Packets | TCP Bytes | UDP Packets | UDP Bytes | ICMP Packets | ICMP Bytes | TCP Connections Replayed | TCP Connections Completed |
|----------|------------|----------|-------------|-----------|-------------|-----------|--------------|------------|--------------------------|---------------------------|
| TCPopera (no loss) | 1,552,882 | 234,991,187 | 1,254,762 | 195,483,762 | 276,234 | 39,475,286 | 393 | 32,139 | 18,138 | 14,971 |
| TCPopera (1% loss) | 1,531,388 | 232,145,926 | 1,254,762 | 192,647,088 | 276,234 | 39,466,797 | 393 | 32,675 | 18,138 | 14,974 |
| Input Trace | 1,502,584 | 234,434,486 | 1,276,195 | 194,927,209 | 276,234 | 39,474,602 | 392 | 32,041 | 18,043 | 14,796 |

**Figure 3: Comparison of Traffic Volume Between Input Trace and TCPopera (1%-loss)**

- **(a) IP Bytes Sampled Every Minute**
- **(b) TCP Bytes Sampled Every Minute**

The reproductivity test showed that both TCPopera (no loss) and TCPopera (1% loss) produced more TCP packets than the input trace records, likely due to delayed ACKs. Additionally, about 100 fewer TCP connections were completed, attributed to SYN packet losses at the BSD firewall.