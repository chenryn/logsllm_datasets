Physical Link
(a) TCPopera components
Human
Intervention
Input trace
records
Human
Intervention
Traffic
Parameters
Trace
Analyzing
Network
Configurations
Flow
Preprocess
Adjusting
Traffic Parameters
IP Flows
Transforming
Environments
IP Flow
Process
New Traffic
Parameters
Interactive
Flow Processing
New Network
Configurations
output data flow
input data flow
user input
(b) TCPopera ﬂow model: This model shows the ﬂow processing methodology
within Flow processing and IP Flow processing components in Figure 1(a)
Fig. 1. TCPopera Architecture
For environment transformation, Flow Preprocess supports IP address remap-
ping and ARP emulation. If TCPopera users specify remapping function using
conﬁguration ﬁles, then Flow Preprocess remaps IP addresses when there is no
conﬂict among remapping entries. Whenever address remapping is done on a
packet, it recalculates its IP header checksum. The Flow Preprocess component
also collects MAC addresses of hosts from trace records, then provides them for
the Flow Process component. Additionally, it rewrites MAC addresses of packets
destined to a default router in trace records to that of new default router in a
test network.
IP Flow Process. IP Flow Process is a key component of TCPopera to support
interactive traﬃc replaying. It creates a POSIX thread [10] for each preprocessed
IP ﬂow while keeping the inter-ﬂow time observed in trace records. To achieve
stateful TCP replaying, IP Flow Process emulates a TCP control block for each
TCP connection. When an IP Flow thread completes replaying all packets, it
outputs replaying results and returns its resources. The current version of TCP-
On Interactive Internet Traﬃc Replay
253
opera does not support the mechanism to resolve the dependencies among TCP
connections because of complexity. Instead, TCPopera uses an ad-hoc approach
by strictly preserving the inter-ﬂow time between IP ﬂows and packet sequences
within a single IP ﬂow same as input trace records. This heuristic is based on
the idea that an IP ﬂow reﬂects a history of communications between two hosts.
However, our approach has several problems in supporting the inter-connection
dependencies because it cannot resolve the dependencies in diﬀerent IP ﬂows.
To improve our heuristic, we have a plan to develop a better inter-connection
dependency model during the next TCPopera development phase.
TCPopera Control. TCPopera Control is responsible for synchronizing the
time and information among TCPopera nodes. This component provides an out-
of-band communication channel to exchange control messages among TCPopera
nodes. It also helps IP Flow process checking active TCPopera nodes and sorting
out replayable IP ﬂows.3 In the current TCPopera implementation, one of the
TCPopera nodes plays a server to control the whole synchronization procedure.
Packet Injection/Capturing. Packet Injection/Capturing are helper com-
ponents for live traﬃc replaying. Any outgoing packet from IP Flow Process
is passed to the Packet Injection component to be launched on the wire. If any
modiﬁcation is made on a packet, the checksum value is recalculated. The Packet
Injection component is implemented using the libnet library, a high-level API to
construct and inject network packets [20]. Likewise, any incoming packet destined
to the virtual addresses of a TCPopera node is captured by the Packet Capturing
component and passed to IP Flow Process. This module is implemented using
one of most widely used packet capturing utilities, pcap [21]. Since each TCP-
opera node can have multiple virtual addresses, the pcap process should have
ﬁltering rules to only capture packets destined to its virtual addresses.
TCP Functions. The TCP functions library provides TCP functionalities
needed to emulate a TCP control block. This library includes most of TCP
features such as TCP timers, timeout & retransmission, fast retransmit & fast
recovery, and ﬂow & congestion control. The current implementation of this
library is heavily based on the TCP implementation of BSD4.4-Lite release,
described in [22]. The following list explains the implementation details.
– TCP timers. TCPopera uses two timers, the fast timer (200ms) and the
slow timer (500ms) to support seven TCP timers. Based on the TCP imple-
mentation in [22], we implemented six timers, excluding the delayed ACK
timer that implemented using the fast timer, using four timer counters that
decrement the number of clock ticks whenever the slow timer expires.
– Timeout & retransmission. Fundamental to TCP’s timeout and retrans-
mission is the RTT measurement experienced on a given connection because
the retransmission timer has values that depend on the measure RTT for a
connection. As the most Berkeley-driven TCP implementation, TCPopera
3 A Replayable IP ﬂow deﬁnes an IP ﬂow that its source and destination addresses
have active TCPopera nodes to represent.
254
S.-S. Hong and S. Felix Wu
measures only one RTT value per connection at any time. The timing is done
by incrementing a counter whenever the slow timer expires. TCPopera calcu-
lates the retransmission timeout (RTO) by measuring RTT of data segments
and keeping track of the smoothed RTT estimator and the smoothed mean
deviation estimator[23, 24]. If there is any outstanding TCP data segment
unacknowledged when the retransmission timer expires, TCPopera retrans-
mits the data segment.
– Fast retransmit & fast recovery. In TCP, it is assumed that three or
more duplicate ACKs in a row is a strong indication of a packet loss. A
TCP sender then retransmits missing segments without waiting for the re-
transmission timer expires. Next, congesting avoidance, but not slow start
is performed. This is called fast retransmit and fast recovery. TCPopera im-
plements these two TCP features according to the modiﬁed TCP congestion
avoidance algorithms proposed in [25].
– Flow & congestion control. Congestion avoidance is a ﬂow control im-
posed by the sender, while an advertised window is a ﬂow control by the re-
ceiver. The former is based on the sender’s assessment of perceived network
congestion, and the latter is related to the amount of available buﬀer space
at the receiver for a connection. TCPopera supports slow start and conges-
tion avoidance that require two variables for each connection: a congestion
window (cwnd) and a slow start threshold size (ssthresh). When congestion
is indicated by a timeout or duplicate ACKs, both variables are adjusted.
4 Validation Tests
In this section, we validate TCPopera’s capabilities in two aspects: reproduc-
tivity and eﬀectiveness. For our validation tests, we used 1999 MIT’s IDEVAL
dataset (IDEVAL99), especially the ﬁrst 12 hours of traﬃc collected from the
inside network at March 29, 1999. We also used the real traﬃc contributed from
ITRI (Industrial Technology Research Institute), Taiwan.
4.1 Test Environment
In our validation tests, two TCPopera nodes are used as shown in Figure 2. The
internal TCPopera node represents a home network and the external TCPopera
node represents all external hosts in trace records. Both TCPopera nodes run on
Internal
TCPopera node
LAN
Snort
(Stream 4)
IP Firewall
Dummynet pipe
packet loss rate
LAN
External
TCPopera node
Fig. 2. The environment conﬁguration for validation tests
On Interactive Internet Traﬃc Replay
255
a machine with 2.0 GHz Intel Pentium 4 processor with 768MB RAM installed.
The Internal TCPopera node runs on Redhat 8.0 (with 2.4.18 kernel) and the
external one runs on Redhat 9.0 (with 2.4.20 kernel). Two TCPopera nodes are
directly connected to each interface of the dual-homed FreeBSD 5.0 Firewall
(ipfw), running on 455MHz Pentium II Celeron processor with 256MB RAM
installed. During the test, we used Snort 2.3 with the stream4 analysis enabled
as a target security system to evaluate its stateful operations.
4.2 Results
Reproductivity test. For the reproductivity test, we reused TCP connection-
level parameters from input trace records to emulate a TCP control block for
each connection. We reproduced the ﬁrst dataset similar to input trace records
and the second dataset with 1% packet loss at our BSD ﬁrewall. Table 1 shows
the result of simple comparison between an input trace and replayed traces by
TCPopera.
The ﬁrst interesting result from the reproductivity test is that both
TCPopera(no-loss) and TCPopera(1%-loss) produced more TCP packets than
input trace records as shown in TCP categories of Table 1. We believe this dif-
ference was from delayed ACKs while TCPopera was emulating TCP control
blocks. This phenomena has been observed more in long-lived TCP connections
such as telnet, ssh. In addition, we observed that about 100 less TCP connec-
tions than input trace records could not be completed. This is the eﬀect of SYN
packet losses at the BSD ﬁrewall. The failure of TCP 3-way handshake by SYN
packet losses has been observed more in short-lived TCP connections.
For the further analysis, we compared traﬃc volume from 3 diﬀerent sources
by plotting IP/TCP bytes every minute in Figure 3. Throughout this compar-
ison, we observed that TCPopera successfully reproduced the traﬃc similar to
input trace records when we did not apply packet losses. However, the TCPopera
traﬃc showed the diﬀerence from input trace records (mostly in the second re-
Table 1. Comparison of traﬃc volume and the number of TCP connections between
input trace records and replayed traces at the internal interface of the BSD ﬁrewall
Category
IP Packets
IP Bytes
TCP Packets
TCP Bytes
UDP Packets
UDP Bytes
ICMP Packets
ICMP Bytes
TCP connections replayed
TCP connections completed
TCPopera
Input
trace
1,502,584
1,225,905
1,276,195
no loss
1,552,882
1 % loss
1,531,388
234,434,486 234,991,187 232,145,926
1,254,762
194,927,209 195,483,762 192,647,088
276,234
39,474,602 39,475,286 39,466,797
392
32,041
18,043
14,796
393
32,139
18,138
14,971
393
32,675
18,138
14,974
276,286
276,294
256
S.-S. Hong and S. Felix Wu
s
e
t
y
B
P
I
s
e
t
y
B
P
C
T
7M
6M
5M
4M
3M
2M
1M
 0
 0
7M
6M
5M
4M
3M
2M
1M
 0
 0
Input Trace
TCPopera (no−loss)
TCPopera (1%−loss)
 100
 200
 300
 400
 500
 600
 700
Time (1 minute)
(a) IP Bytes sampled every minute
Input Trace
TCPopera (no−loss)
TCPopera (1%−loss)
 100
 200
 300
 400
 500
 600
 700
Time (1 minute)
(b) TCP Bytes sampled every minute
Fig. 3. Comparison of traﬃc volume between the Input trace and TCPopera (1%-loss)
 1
 0.8
 0.6
 0.4
 0.2
Original
TCPopera (0.0)
TCPopera (0.01)
)
F
D
C
(
n
o
i
t
u
b
i
r
t
s
i
D
 1
 0.8
 0.6
 0.4
 0.2
Original
TCPopera (0.0)
TCPopera (0.01)
)
F
D