place) highlights how SLAP still remains a potential threat.
5.5 Attack Transferability
Setup. In this section, we test the transferability of our at-
tack across networks, testing all pairwise combinations of our
models, including adversarially trained ones. We also use the
Google Vision API [17] to test our projections against their
proprietary models. The API returns a list of labeled objects
in the image with associated conﬁdence scores and bounding
boxes, "stop sign" is one of the labels. We set the detection
threshold for Google Vision API as 0.01, i.e., we count that a
stop sign is detected in a frame if the API replies with a stop
sign object with conﬁdence greater than 0.01.
Results. We report the results in Table 4. The table shows
the source (white-box) model on the left, which identiﬁes
the projection shown in the tested videos. We also report the
number of frames tested, taken from the videos from the in-
door experiment. Table 4 reports success rates of the attack
as a percentage of the frames where the stop sign was unde-
tected. Table 4 shows that our attack transfers well for low
light settings, but the transferability degrades quickly for the
300 lux setting and above. We ﬁnd that Mask-RCNN trans-
fers better to Yolov3 compared to the opposite direction, the
same happens for Gtsrb-CNN and Lisa-CNN, suggesting that
ﬁtting AE on complex models favours the attacker. Table 4
also shows that adversarially trained model have beneﬁts by
reducing the transferability of attacks ﬁt on surrogate models.
6 Discussion
In this section, we discuss the attack feasibility.
Attack Feasibility. Our experiments demonstrate that increas-
Figure 13: Amount of lux achievable on the stop sign surface
for increasing projection distances and different projectors.
The horizontal line shows the threshold for success measured
in our experiments (800 lux at 120 lux ambient light).
ing ambient light quickly stops the feasibility of the attack
in bright conditions. In practice, during daytime, the attack
could be conducted on non-bright days, e.g., dark overcast
days or close to sunset or sunrise, when the ambient light
is low (<400 lux). Regarding the effect of car headlights,
our outdoor experiments show that the car headlights-emitted
light is negligible compared to the projection luminosity and
does not inﬂuence the attack success. While car headlights on
high-beam would compromise the projection appearance and
degrade the attack success rates, we did not consider these
lights to be on as stop signs would be mainly present in urban
areas, where high-beam headlights would be off. In general,
the amount of projector-emitted light that reaches the sign de-
pends on three factors: (i) the distance between projector and
sign, (ii) the throw ratio of the projector and (iii) the amount
of lumens the projector can emit. We report in Figure 13 a
representation of how the distance between the projector and
the stop sign relates to the attack success rate. We consider
two additional projectors with long throw distance, the Pana-
1876    30th USENIX Security Symposium
USENIX Association
lux
Source Model no. frames
120
300
600
Yolov3
Mask-RCNN
Gtsrb-CNN
Lisa-CNN
Yolov3
Mask-RCNN
Gtsrb-CNN
Lisa-CNN
Yolov3
Mask-RCNN
Gtsrb-CNN
Lisa-CNN
4587
3765
3760
4998
5169
3543
3438
4388
5507
5058
4637
4714
2.5%
0.1%
48.0%
8.6%
32.5%
5.3%
7.2%
8.6%
0.2%
0.4%
0.9%
0.9%
Target Model
73.4%
97.1%
37.0%
28.1%
3.6%
14.0%
2.9%
4.9%
0.0%
0.0%
99.9%
6.8%
Yolov3 Mask-RCNN Gtsrb-CNN Gtsrb-CNN(a) Lisa-CNN Lisa-CNN(a) Google Vision*
100.0%
98.7%
40.5%
29.4%
96.5%
32.0%
2.0%
0.7%
17.8%
0.1%
0.0%
0.0%
21.5%
15.5%
51.4%
100.0%
2.3%
10.4%
44.0%
100.0%
27.4%
4.6%
4.9%
57.5%
100.0%
100.0%
72.4%
77.1%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
16.1%
0.0%
0.0%
0.0%
43.1%
0.0%
72.3%
65.9%
47.6%
25.0%
23.7%
16.7%
21.1%
15.8%
0.0%
0.0%
7.5%
0.0%
Table 4: Transferability results. We test all the frames from the collected videos with a certain projection being shone against a
different target model, ﬁgures in bold are white-box pairs. (*) For Google Vision we only test one frame every 30 frames, i.e.,
one per second. We also remove all frames that are further than 6m away as Google Vision does not detect most of them in a
baseline scenario. _(a) indicates adversarially trained models.
sonic PT-RZ570BU and the NEC PH1202HL1, available for
$3,200 and $44,379 respectively. We use the projector’s throw
ratios (2.93 and 3.02) and their emitted lumens (5,000 and
12,000 lumens) to calculate how many lux of light the projec-
tor can shine on the sign surface from increasing distances.
We consider the success as measured in 120 lux ambient light,
where obtaining 800 lux of light on the sign with the pro-
jector is sufﬁcient to achieve consistent attack success (see
Section 5.1). Figure 13 shows that the attack could be carried
out from 7.5m away with the weaker projector and up to 13m
away with the more expensive one. Additionally, adversaries
could also use different lenses to increase the throw ratio of
cheaper projectors (similarly to [32]).
Attack Generalizability. We show results for attacks on
other objects (give way sign, bottle) in Appendix A, however,
to extend the attack to any object, the adversary will have to
consider the distortion introduced by the projection surface
(not necessary for ﬂat trafﬁc signs). The attacker will have to
augment the projection model used in this paper with differen-
tiable transformations which model the distortion caused by
the non-ﬂat surface. In general, the size of the projectable area
limits the feasibility of the attack against certain objects (e.g.,
hard to project on a bike); this drawback is shared across all
vectors that create physically robust AE, including adversarial
patches. We also found that the properties of the material
where the projection is being shone will impact the attack
success: trafﬁc signs are an easier target because of their
high material reﬂectivity. When executing the attack on other
objects, we found that certain adaptations lead to marginal
attack improvements, in particular context information (e.g.,
the pole for the stop sign, the table where the bottle is placed).
Generally, for object detectors, adversaries will have to tailor
certain parameters of the optimization to the target object.
7 Conclusions
In this paper we presented SLAP, a new attack vector to realize
short-lived physical adversarial examples by using a light
projector. We investigate the attack in the context of road
safety, where the attacker’s goal is to change the appearance
of a stop sign by shining a crafted projection onto it so that it
is undetected by the DNNs mounted on autonomous vehicles.
Given the non-trivial physical constraints of projecting spe-
ciﬁc light patterns on various materials in various conditions,
we proposed a method to generate projections based on ﬁtting
a predictive three-way color model and using an AE genera-
tion pipeline that enhances the AE robustness. We evaluated
the proposed attack in a variety of light conditions, including
outdoors, and against state-of-the-art object detectors Yolov3
and Mask-RCNN and trafﬁc sign recognizers Lisa-CNN and
Gtsrb-CNN. Our results show that SLAP generates AEs that
are robust in the real-world. We evaluated defences, highlight-
ing how existing defences tailored to physical AE will not
work against AE generated by SLAP, while ﬁnding that an
adaptive defender using adversarial learning can successfully
hamper the attack effect, at the cost of reduced accuracy.
Nevertheless, the novel capability of modifying how an
object is detected by DNN models, combined with the capa-
bility of carrying out opportunistic attacks, makes SLAP a
powerful new attack vector that requires further investigation.
This paper makes an important step towards increasing the
awareness and further research of countermeasures against
light-projection adversarial examples.
Acknowledgements
This work was supported by grants from armasuisse, Master-
card, and by the Engineering and Physical Sciences Research
Council [grant numbers EP/N509711/1, EP/P00881X/1].
USENIX Association
30th USENIX Security Symposium    1877
References
[1] “Daylight”, [Online] Accessed: 2020-02-20. https://en.
wikipedia.org/wiki/Daylight.
[2] “JPEG File Interchange Format”, [Online] Accessed: 2020-01-
15. http://www.w3.org/Graphics/JPEG/jfif3.pdf.
[3] “Manual of Uniform Trafﬁc Control Devices for Street and
Highways”, [Online] Accessed: 2020-01-08. http://mutcd.
fhwa.dot.gov/pdfs/2009r1r2/mutcd2009r1r2edition.
pdf.
[4] “Panasonic PT-AE8000 Projector”, [Online] Accessed:
http : / / www . projectorcentral . com /
2020-10-12.
Panasonic-PT-AE8000.htm.
[5] “Sanyo PLC-XU4000 Projector”, [Online] Accessed:
http : / / www . projectorcentral . com /
2020-10-12.
Sanyo-PLC-XU4000.htm.
[6] Apollo. “ApolloAuto - An open autonomous driving plat-
form”, [Online] Accessed: 2021-02-19. http://github.
com/apolloauto.
[7] Aptina.
“1/3-Inch CMOS Digital
Image Sensor
2021-
http : / / datasheetspdf . com / pdf / 829321 /
AR0132AT Data Sheet”, [Online] Accessed:
02-19.
AptinaImagingCorporation/AR0132AT/1f.
[8] BMW. “BMW TechOfﬁce Munich”, [Online] Accessed: 2020-
02-19. http://github.com/BMW-InnovationLab.
[9] Andy Boxall. “From robots to projection mapping: Inside
Panasonic’s Tokyo 2020 Olympic tech”, [Online] Accessed:
2021-02-19. http://www.digitaltrends.com/mobile/
panasonic-tokyo-2020-technology-interview/.
[10] Tom Brown, Dandelion Mane, Aurko Roy, Martin Abadi,
arXiv preprint
“Adversarial Patch”.
and Justin Gilmer.
arXiv:1712.09665v2, 2018.
[11] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and
“Grad-cam++: Generalized
Vineeth N. Balasubramanian.
Gradient-based Visual Explanations for Deep Convolutional
Networks”. In Proceedings of the IEEE Winter Conference
on Applications of Computer Vision (WACV), pages 839–847,
2018.
[12] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen
Horng Polo Chau. “Shapeshifter: Robust Physical Adversarial
Attack on Faster R-CNN Object Detector”. In Proceedings
of the Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pages 52–68, 2018.
[13] E. Chou, F. Tramèr, and G. Pellegrino. “SentiNet: Detecting
Localized Universal Attacks Against Deep Learning Systems”.
In Proceedings of the IEEE Security and Privacy Workshops
(SPW), pages 48–54, 2020.
[14] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig
Schmidt, and Aleksander Madry. “Exploring the Landscape
of Spatial Robustness”. In Proceedings of the International
Conference on Machine Learning (ICML), pages 1802–1811,
2019.
[15] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir
Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and
Dawn Song. “Robust physical-world attacks on deep learning
visual classiﬁcation”. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
1625–1634, 2018.
[16] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Ma-
lik. “Rich Feature Hierarchies for Accurate Object Detection
and Semantic Segmentation”. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR),
pages 580–587, 2014.
[17] Google. “Google Vision API”, Accessed: 2020-10-12. https:
//cloud.google.com/vision.
[18] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. “Bad-
Nets: Identifying Vulnerabilities in the Machine Learning
Model Supply Chain”. arXiv preprint arXiv:1708.06733, 2017.
“The Exposure Triangle: Aperture, Shut-
ter Speed and ISO explained”, [Online] Accessed: 2021-
02-19.
http : / / www . techradar . com / uk / how-to /
the-exposure-triangle.
[19] Phil Hall.
[20] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
“Mask R-CNN”. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 2961–
2969, 2017.
[21] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc
Schlipsing, and Christian Igel. “Detection of Trafﬁc Signs
in Real-World Images: The German Trafﬁc Sign Detection
Benchmark”. In Proceedings of the International Joint Con-
ference on Neural Networks (IJCNN), pages 1–8, 2013.
[22] Andrei Kapishnikov, Tolga Bolukbasi, Fernanda Viégas, and
Michael Terry. “Xrai: Better Attributions through Regions”.
In Proceedings of the IEEE/CVF International Conference on
Computer Vision (CVPR), pages 4948–4957, 2019.
[23] Danny Karmon, Daniel Zoran, and Yoav Goldberg. “Lavan:
Localized and visible adversarial noise”. In Proceedings of the
International Conference on Machine Learning (ICML), pages
2507–2515, 2018.
[24] Sebastian Köhler, Giulio Lovisotto, Simon Birnbach, Richard
Baker, and Ivan Martinovic. “They See Me Rollin’: Inherent
Vulnerability of the Rolling Shutter in CMOS Image Sensors”.
arXiv preprint arXiv:2101.10011, 2021.
[25] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. “Ad-
versarial Examples in the Physical World”. arXiv preprint
arXiv:1607.02533, 2016.
[26] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. “Feature Pyramid Net-
works for Object Detection”. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR),
pages 2117–2125, 2017.
[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. “Microsoft COCO: Common Objects in Context”. In
Proceedings of the European Conference on Computer Vision
(ECCV), pages 740–755, 2014.
[28] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan
Zhai, Authors Yingqi Liu, Weihang Wang, and Xiangyu Zhang.
“Trojaning Attack on Neural Networks”. In Proceedings of the
Network and Distributed System Symposium (NDSS), 2018.
1878    30th USENIX Security Symposium
USENIX Association
[29] Giulio Lovisotto, Simon Eberz, and Ivan Martinovic. “Bio-
metric Backdoors: A Poisoning Attack Against Unsupervised