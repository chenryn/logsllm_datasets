memory pool and conducting the timing-based tests to
cluster them, and (2) to determine the XOR-schemes (see
Section 3) that are used by memory controllers, by test-
ing all possible combinations of XOR-schemes against
all sets of addresses.
The advantage of their approach over ours is that it
exhaustively searches XOR-schemes without the need to
reason about the complex logic behind them, as is done
in our paper. However, our method targets speciﬁc bit
USENIX Association  
25th USENIX Security Symposium  21
3
combinations and therefore is more efﬁcient. Specially,
it has been reported in [29] that it took about 20 minutes
to reverse engineer the DRAM mapping on a normally-
loaded system. Our approach, on the other hand, takes
less than two minutes (see Section 6). In addition, Pessl
et al. [29] also indicated that completeness is not guaran-
teed as it depends on random addresses. Hence, a com-
plete test using their approach may take even longer.
2.2 Row Hammer and DRAM Bit Flips
Modern DRAM chips tend to have larger capacity, and
hence higher density of memory cells. As a result, a
memory cell may suffer from disturbance errors due to
electrical interference from its neighboring cells. More-
over, certain memory access patterns, such as repeated
and frequent row activation (“row hammering”), may
easily trigger disturbance errors. The “row hammer”
problem caught Intel’s attention as early as 2012 and was
publicly disclosed around 2014 [13–15,19]. Independent
of Intel’s effort, Kim et al. [23] also reported that random
bit ﬂips can be observed by specially crafted memory ac-
cess patterns induced by software programs.
The ﬁrst practical row hammer exploit was published
by Seaborn from Google [4], who demonstrated privilege
escalation attacks exploiting row hammer vulnerabilities
to break the sandbox of Google’s NaCl, and to obtain
kernel memory accesses from userspace programs run-
ning on Linux operating systems. The study was quickly
followed up by others [10,16,20], who demonstrated row
hammer attacks using Javascript code, which meant that
the attacks could be conducted without special privileges
to execute binary code on target machines. This paper
follows the same line of research, but our focus is server-
side row hammer attacks, although some of the proposed
techniques will also be useful in other contexts.
It has been claimed that server-grade processors and
DRAM modules are less vulnerable to row hammer at-
tacks [23], especially when the server is equipped with
ECC-enabled DRAM modules. However, ECC is not the
ultimate solution to such attacks. The most commonly
used ECC memory modules implement single error-
correction, double error-detection mechanisms, which
can correct only one single-bit of errors within a 64-bit
memory block, and detect (but not correct) 2-bit errors
in the same 64-bit block. More bit errors cannot be de-
tected and data and code in memory will be corrupted
silently [23].
Dedicated defenses against row hammer vulnerabili-
ties by new hardware designs have been studied in [22].
Particularly, Kim et al. [22] proposes Counter-Based
Row Activation (CRA) and Probabilistic Row Activa-
tion (PRA) to address row hammer vulnerabilities. CRA
counts the frequency of row activations and proactively
activates neighboring rows to refresh data; PRA enables
memory controllers to activate neighboring rows with a
small probability for every memory access.
3 DRAM Addressing
Prior work [4] has indicated that double-sided row ham-
mer attacks are much more effective than single-sided
ones. We therefore focus on developing a software tool
to conduct double-sided row hammer attacks from within
virtual machines. To make the attack possible, we ﬁrst
must ﬁnd the physical memory address mapping in the
target DRAMs, and do so without physical accesses to
the machines. More precisely, we hope to determine
which bits in a physical address specify its mapping to
DRAM banks, rows and columns.
This information, however, is not available in the sys-
tem conﬁguration or in the memory controller or DRAM
datasheets. Intel never discloses the mapping algorithm
in their memory controllers; moreover, the same mem-
ory controller will likely map the same physical address
to a different DRAM location if the number or size of
DRAM chips is changed. Therefore, in this section, we
present a method to reverse engineer the physical address
mapping in DRAM at runtime. We call this procedure bit
detection. It is important to note that we do not need to
differentiate address bits for banks, ranks, or channels as
long as their combination uniquely addresses the same
DRAM bank.
3.1 A Timing-Channel Primitive
We resort to a known timing channel [27] to develop our
bit detection primitive. The timing channel is established
due to the row buffer in each DRAM bank. When two
memory addresses mapped to the same DRAM bank in
different rows are alternatively accessed in rapid succes-
sion, the accesses will be delayed due to conﬂicts in the
row buffer (and subsequent eviction and reload of the
row buffer). Therefore, by conducting fast and repeated
accesses to two memory addresses, one can learn that
the two address are located in different rows of the same
bank if one observes longer access latency.
The algorithm is described in Algorithm 1. The input
to the algorithm, LATENCY(), is a set of bit positions in
the physical address space. We use I to denote the in-
put. For example, I = {b3,b17} represents the 3rd and
17th right-most bits of the physical address. LATENCY()
randomly selects 100 pairs1 of memory addresses from a
large memory buffer, so that each pair of addresses dif-
fers only in the bit positions that are speciﬁed by the in-
put, I: in each pair, one address has ‘1’s at all these bit
1A sample size that is large enough to achieve statistical signiﬁcance.
22  25th USENIX Security Symposium 
USENIX Association
4
Algorithm 1: LATENCY()
Input:
Output:
{bi}: a set of physical address bits
Access latency: 1 (high) or 0 (low)
begin
Randomly select 100 pairs of memory addresses that differ only in
{bi}: One address in each pair with all bi = 1 and the other with all
bi = 0. Place all 100 pairs in address pairs{}
for each pair k in address pairs{} do
Start time measurement
for j in 103 do
Access both addresses in k
clflush both addresses
insert memory barrier
end
Stop time measurement
end
Return the average access latency compared to baselines
end
positions and the other address has ‘0’s at all these posi-
tions.
The algorithm enumerates each pair of addresses by
measuring the average access latency to read each ad-
dress once from memory. Speciﬁcally, it accesses both
addresses and then issues clflush instructions to ﬂush
the cached copies out of the entire cache hierarchy.
Hence the next memory access will reach the DRAM. A
memory barrier is inserted right after the memory ﬂush
so that the next iteration will not start until the ﬂush has
been committed. The total access time is measured by is-
suing rdtsc instructions before and after the execution.
The algorithm returns 1 (high) or 0 (low) to indicate the
latency of memory accesses. LATENCY()=1 suggests the
two physical addresses that differ only at the bit positions
speciﬁed in the input are located on different rows of the
same DRAM bank.
3.2 Graph-based Bit Detection Algorithms
Using the LATENCY() timing-channel primitive we de-
velop a set of graph-based bit detection algorithms.
Speciﬁcally, we consider each bit in a physical address
as a node in a graph; the edges in the graph are closely
related to the results of LATENCY(): The set of bits are
connected by edges, if, when used as the input to LA-
TENCY(), yields high access latency. But the exact con-
struction of these edges may differ in each of the graphs
we build, as will be detailed shortly. We deﬁne all such
nodes as set V = {bi}i∈[1,n], where n is the total number
of bits in a physical address on the target machine. In the
following discussion, we use bi to refer to an address bit
position and a node interchangeably.
Our bit detection algorithms works under the assump-
tion that Intel’s DRAM address mapping algorithms may
use XOR-schemes to combine multiple bits in physical
addresses to determine one of the bank bits. An XOR-
scheme is a function which takes a set of bits as input
and outputs the XORed value of all the input bits. This
assumption is true for Intel’s DRAM address mapping,
which is evident according to prior studies [5, 25, 33].
Our empirical evaluation also conﬁrms this assumption.
Detecting row bits and column bits. We ﬁrst deﬁne a
set of nodes R = {bi|LATENCY({bi}) = 1,bi ∈ V}. Be-
cause LATENCY({bi}) = 1, any two memory addresses
that differ only in bi are located in different rows of the
same bank. Therefore, bit bi determines in which rows
the addresses are located, i.e., bi is a row bit. But as the
two addresses are mapped to the same bank, bi is not
used to address DRAM banks.
Next, we deﬁne set C = {b j|LATENCY({bi,b j}) =
1,∀bi ∈ R,b j /∈ R}. It means that when accessing two ad-
dresses that differ only in a bit in C and a bit in R, we ex-
perience high latency in the LATENCY() test—indicating
that the two addresses are in the same bank but different
rows. Therefore, the bits in C are not at all involved in
DRAM bank indexing (otherwise changing bits in C will
yield a memory address in a different bank). The bits in
C are in fact column bits that determine which column in
a row the address is mapped to.
Detecting bank bits in a single XOR-scheme. We con-
sider an undirected graph G1 constructed on the subset
of nodes V− R− C. If LATENCY({bi,b j}) =1, node bi
is connected with node b j by edge e(bi,b j). There could
be three types of connected components in such a graph:
In the type I connected components, only two nodes are
connected (Figure 2a). Because LATENCY({bi,b j}) = 1,
changing bits bi and b j together will yield an address in
a different row of the same bank. Hence, at least one of
bi and b j (usually only the more signiﬁcant bit—the one
on the left2) will be the row bit; the XOR of the two is a
bank bit. More formally, if e(bi,b j) is an edge in com-
ponent type I (shown in Figure 2a), and i > j, bi is a row
bit, bi ⊕ b j determines one bank bit.
In the type II connected components, a set of nodes
are connected through a hub node (Figure 2b). For in-
stance, nodes b j, bk, and bl are connected via node bi.
Particularly in Figure 2b, i = 20, j = 15, k = 16, l = 17.
Due to the property of the LATENCY() test, bi ⊕ b j must
be a bank bit and at least one of the pair is a row bit.
The same arguments apply to bi ⊕ bk and bi ⊕ bl. We
can safely deduce that bi ⊕ b j ⊕ bk ⊕ bl is a common
XOR-scheme in which the four bits are involved: Other-
wise, without loss of generality, we assume bi ⊕ b j ⊕ bk
and bi ⊕ bl are two separate XOR-schemes. When two
addresses differ only in bi and b j, although the value
of bi ⊕ b j ⊕ bk does not change for the two addresses,
2The timing-channel approach cannot determine which bit is actually
the row bit in this case. However, because memory controllers need
to minimize row conﬂicts in the same bank, row bits are usually more
signiﬁcant bits in a physical address [5,33]. Our hypothesis turned out
to be valid in all the case studies we have conducted (see Table 1).
USENIX Association  
25th USENIX Security Symposium  23
5
(a) Connected component type I
(b) Connected component type II
(c) Connected component type III
Figure 2: Detecting bits in a single XOR-scheme.
bi ⊕ bl will be different, thus making the two addresses
in different banks. However, this conclusion contradicts
the fact that LATENCY({bi,b j}) =1. Moreover, we can
conclude that only bi is the row bit, because otherwise
if another bit is also a row bit, e.g., b j, we should ob-
serve LATENCY({b j,bk}) = 1 (because b j and bk are in-
volved in the XOR-scheme bi ⊕ b j ⊕ bk ⊕ bl and b j is a
row bit). However that is not the case here. To summa-
rize, if e(bi,b j), e(bi,bk) and e(bi,bl) constitute a type
II connected component in Figure 2b, bi is a row bit and
bi ⊕ b j ⊕ bk ⊕ bl determines a bank bit.
In the type III connected components, a clique
of nodes replaces the single hub node in type II
components—each node in the clique is connected to all
other nodes in type III components (Figure 2c). As a sim-
ple example, we assume nodes bi and b j are connected by
edge e(bi,b j), and both of them are connected to nodes
bk and bl, which are not connected directly. Particularly
in Figure 2c, i = 18, j = 20, k = 15, l = 16. From the
analysis of type II components, nodes bi, bk and bl must
follow that bi is a row bit and bi ⊕ bk ⊕ bl determines
one bank bit. Similarly, we can conclude that b j is a
row bit and b j ⊕ bk ⊕ bl determines one bank bit. More-
over, we can deduce that bi ⊕ bk ⊕ bl and b j ⊕ bk ⊕ bl
determine the same bank bit, otherwise two addresses
that differ in bi and b j will be in two different banks,
which conﬂicts with LATENCY({bi,b j}) = 1. Therefore,
bi⊕ b j ⊕ bk ⊕ bl is a bank bit. As such, in a type III com-
ponent in Figure 2c, all nodes in the clique represent row
bits, and the XOR-scheme that involves all bits in the
components produces one bank bit.
Detecting bank bits in two XOR-schemes. On some
processors, certain bits can be involved in more than
one XOR-schemes. For instance, a bit bi can be used
in both bi ⊕ b j ⊕ bk and bi ⊕ bm ⊕ bn. To detect such
bit conﬁguration, we consider another undirected graph
G2 constructed on the subset of nodes V − R − C.
If
LATENCY({bi,b j,bm}) = 1, the three nodes are con-
nected with each other by edges e(bi,b j), e(bi,bm),
e(b j,bm). If none of the three edges exist in graph G1—
the graph we constructed in the single-XOR-scheme-bit
detection—it means these three nodes are involved in
two XOR-schemes bi ⊕ b j and bi ⊕ bm: if two addresses
differ in only two bits (out of the three), at least one of
these two XOR-schemes will specify a different bank in-
dex; however, if two addresses differ in all three bits, the
outcome of both XOR-schemes are the same for the two
addresses, so they are in the same bank. One of these
three bits (the most signiﬁcant among the three) will be
used in both XOR-schemes and serve as a row bit.
Figure 3: Detecting bits in two XOR-schemes.
In this example,
Let’s look at a more general example where ﬁve
nodes are involved (Figure 3).
the