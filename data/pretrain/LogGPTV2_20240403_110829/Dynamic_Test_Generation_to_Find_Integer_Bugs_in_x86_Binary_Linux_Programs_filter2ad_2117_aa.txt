title:Dynamic Test Generation to Find Integer Bugs in x86 Binary Linux Programs
author:David Molnar and
Xue Cong Li and
David A. Wagner
Dynamic Test Generation To Find Integer Bugs in x86 Binary Linux
Programs
David Molnar
UC Berkeley
Xue Cong Li
UC Berkeley
David A. Wagner
UC Berkeley
Abstract
Recently, integer bugs, including integer overﬂow, width
conversion, and signed/unsigned conversion errors, have
risen to become a common root cause for serious security
vulnerabilities. We introduce new methods for discover-
ing integer bugs using dynamic test generation on x86
binaries, and we describe key design choices in efﬁcient
symbolic execution of such programs. We implemented
our methods in a prototype tool SmartFuzz, which we
use to analyze Linux x86 binary executables. We also
created a reporting service, metafuzz.com, to aid in
triaging and reporting bugs found by SmartFuzz and the
black-box fuzz testing tool zzuf. We report on experi-
ments applying these tools to a range of software appli-
cations, including the mplayer media player, the exiv2
image metadata library, and ImageMagick convert. We
also report on our experience using SmartFuzz, zzuf,
and metafuzz.com to perform testing at scale with the
Amazon Elastic Compute Cloud (EC2). To date, the
metafuzz.com site has recorded more than 2, 614 test
runs, comprising 2, 361, 595 test cases. Our experiments
found approximately 77 total distinct bugs in 864 com-
pute hours, costing us an average of $2.24 per bug at cur-
rent EC2 rates. We quantify the overlap in bugs found by
the two tools, and we show that SmartFuzz ﬁnds bugs
missed by zzuf, including one program where Smart-
Fuzz ﬁnds bugs but zzuf does not.
1 Introduction
Integer overﬂow bugs recently became the second most
common bug type in security advisories from OS ven-
dors [10]. Unfortunately, traditional static and dynamic
analysis techniques are poorly suited to detecting integer-
related bugs. In this paper, we argue that dynamic test
generation is better suited to ﬁnding such bugs, and we
develop new methods for ﬁnding a broad class of inte-
ger bugs with this approach. We have implemented these
methods in a new tool, SmartFuzz, that analyzes traces
from commodity Linux x86 programs.
Integer bugs result from a mismatch between machine
arithmetic and mathematical arithmetic. For example,
machine arithmetic has bounded precision; if an expres-
sion has a value greater than the maximum integer that
can be represented, the value wraps around to ﬁt in ma-
chine precision. This can cause the value stored to be
smaller than expected by the programmer. If, for exam-
ple, a wrapped value is used as an argument to malloc,
the result is an object that is smaller than expected, which
can lead to a buffer overﬂow later if the programmer is
not careful. This kind of bug is often known as an integer
overﬂow bug. In Section 2 we describe two other classes
of integer bugs: width conversions, in which converting
from one type of machine integer to another causes un-
expected changes in value, and signed/unsigned conver-
sions, in which a value is treated as both a signed and an
unsigned integer. These kinds of bugs are pervasive and
can, in many cases, cause serious security vulnerabili-
ties. Therefore, eliminating such bugs is important for
improving software security.
While new code can partially or totally avoid integer
bugs if it is constructed appropriately [19], it is also im-
portant to ﬁnd and ﬁx bugs in legacy code. Previous
approaches to ﬁnding integer bugs in legacy code have
focused on static analysis or runtime checks. Unfortu-
nately, existing static analysis algorithms for ﬁnding in-
teger bugs tend to generate many false positives, because
it is difﬁcult to statically reason about integer values with
sufﬁcient precision. Alternatively, one can insert runtime
checks into the application to check for overﬂow or non-
value-preserving width conversions, and raise an excep-
tion if they occur. One problem with this approach is
that many overﬂows are benign and harmless. Throwing
an exception in such cases prevents the application from
functioning and thus causes false positives. Furthermore,
occasionally the code intentionally relies upon overﬂow
semantics; e.g., cryptographic code or fast hash func-
tions. Such code is often falsely ﬂagged by static analysis
or runtime checks. In summary, both static analysis and
runtime checking tend to suffer from either many false
positives or many missed bugs.
In contrast, dynamic test generation is a promising ap-
proach for avoiding these shortcomings. Dynamic test
generation, a technique introduced by Godefroid et al.
and Engler et al. [13, 7], uses symbolic execution to gen-
erate new test cases that expose speciﬁcally targeted be-
haviors of the program. Symbolic execution works by
collecting a set of constraints, called the path condition,
that model the values computed by the program along a
single path through the code. To determine whether there
is any input that could cause the program to follow that
path of execution and also violate a particular assertion,
we can add to the path condition a constraint represent-
ing that the assertion is violated and feed the resulting set
of constraints to a solver. If the solver ﬁnds any solution
to the resulting constraints, we can synthesize a new test
case that will trigger an assertion violation. In this way,
symbolic execution can be used to discover test cases that
cause the program to behave in a speciﬁc way.
Our main approach is to use symbolic execution to
construct test cases that trigger arithmetic overﬂows,
non-value-preserving width conversions, or dangerous
signed/unsigned conversions. Then, we run the program
on these test cases and use standard tools that check for
buggy behavior to recognize bugs. We only report test
cases that are veriﬁed to trigger incorrect behavior by the
program. As a result, we have conﬁdence that all test
cases we report are real bugs and not false positives.
Others have previously reported on using dynamic test
generation to ﬁnd some kinds of security bugs [8, 15].
The contribution of this paper is to show how to extend
those techniques to ﬁnd integer-related bugs. We show
that this approach is effective at ﬁnding many bugs, with-
out the false positives endemic to prior work on static
analysis and runtime checking.
The ability to eliminate false positives is important,
because false positives are time-consuming to deal with.
In slogan form: false positives in static analysis waste
the programmer’s time; false positives in runtime check-
ing waste the end user’s time; while false positives in
dynamic test generation waste the tool’s time. Because
an hour of CPU time is much cheaper than an hour of
a human’s time, dynamic test generation is an attractive
way to ﬁnd and ﬁx integer bugs.
We have implemented our approach to ﬁnding inte-
ger bugs in SmartFuzz, a tool for performing symbolic
execution and dynamic test generation on Linux x86 ap-
plications. SmartFuzz works with binary executables di-
rectly, and does not require or use access to source code.
Working with binaries has several advantages, most no-
tably that we can generate tests directly from shipping
binaries.
In particular, we do not need to modify the
build process for a program under test, which has been
a pain point for static analysis tools [9]. Also, this allows
us to perform whole-program analysis: we can ﬁnd bugs
that arise due to interactions between the application and
libraries it uses, even if we don’t have source code for
those libraries. Of course, working with binary traces in-
troduces special challenges, most notably the sheer size
of the traces and the lack of type information that would
be present in the source code. We discuss the challenges
and design choices in Section 4.
In Section 5 we describe the techniques we use to gen-
erate test cases for integer bugs in dynamic test gener-
ation. We discovered that these techniques ﬁnd many
bugs, too many to track manually. To help us prioritize
and manage these bug reports and streamline the pro-
cess of reporting them to developers, we built Metafuzz, a
web service for tracking test cases and bugs (Section 6).
Metafuzz helps minimize the amount of human time re-
quired to ﬁnd high-quality bugs and report them to de-
velopers, which is important because human time is the
most expensive resource in a testing framework. Finally,
Section 7 presents an empirical evaluation of our tech-
niques and discusses our experience with these tools.
The contributions of this paper are the following:
• We
ﬁnding
signed/unsigned conversion vulnerabilities us-
ing symbolic execution. In particular, we develop
a novel type inference approach that allows us to
detect which values in an x86 binary trace are used
as signed integers, unsigned integers, or both. We
discuss challenges in scaling such an analysis to
commodity Linux media playing software and our
approach to these challenges.
design
novel
algorithms
for
• We extend the range of integer bugs that can be
found with symbolic execution, including integer
overﬂows, integer underﬂows, width conversions,
and signed/unsigned conversions. No prior sym-
bolic execution tool has included the ability to de-
tect all of these kinds of integer vulnerabilities.
• We implement these methods in SmartFuzz, a tool
for symbolic execution and dynamic test generation
of x86 binaries on Linux. We describe key chal-
lenges in symbolic execution of commodity Linux
software, and we explain design choices in Smart-
Fuzz motivated by these challenges.
• We report on the bug ﬁnding performance of Smart-
Fuzz and compare SmartFuzz to the zzuf black
box fuzz testing tool. The zzuf tool is a simple,
yet effective, fuzz testing program which randomly
mutates a given seed ﬁle to ﬁnd new test inputs,
without any knowledge or feedback from the tar-
get program. We have tested a broad range of com-
modity Linux software, including the media players
2
mplayer and ffmpeg, the ImageMagick convert
tool, and the exiv2 TIFF metadata parsing library.
This software comprises over one million lines of
source code, and our test cases result in symbolic
execution of traces that are millions of x86 instruc-
tions in length.
• We identify challenges with reporting bugs at scale,
and introduce several
techniques for addressing
these challenges. For example, we present evidence
that a simple stack hash is not sufﬁcient for group-
ing test cases to avoid duplicate bug reports, and
then we develop a fuzzy stack hash to solve these
problems. Our experiments ﬁnd approximately 77
total distinct bugs in 864 compute hours, giving us
an average cost of $2.24 per bug at current Amazon
EC2 rates. We quantify the overlap in bugs found
by the two tools, and we show that SmartFuzz ﬁnds
bugs missed by zzuf, including one program where
SmartFuzz ﬁnds bugs but zzuf does not.
Between June 2008 and November 2008, Metafuzz
has processed over 2,614 test runs from both SmartFuzz
and the zzuf black box fuzz testing tool [16], comprising
2,361,595 test cases. To our knowledge, this is the largest
number of test runs and test cases yet reported for dy-
namic test generation techniques. We have released our
code under the GPL version 2 and BSD licenses1. Our
vision is a service that makes it easy and inexpensive for
software projects to ﬁnd integer bugs and other serious
security relevant code defects using dynamic test gener-
ation techniques. Our work shows that such a service is
possible for a large class of commodity Linux programs.
2 Integer Bugs
We now describe the three main classes of integer bugs
we want to ﬁnd: integer overﬂow/underﬂow, width con-
versions, and signed/unsigned conversion errors [2]. All
three classes of bugs occur due to the mismatch between
machine arithmetic and arithmetic over unbounded inte-
gers.
Overﬂow/Underﬂow. Integer overﬂow (and underﬂow)
bugs occur when an arithmetic expression results in a
value that is larger (or smaller) than can be represented
by the machine type. The usual behavior in this case is
to silently “wrap around,” e.g. for a 32-bit type, reduce
the value modulo 232. Consider the function badalloc
in Figure 1. If the multiplication sz * n overﬂows, the
allocated buffer may be smaller than expected, which can
lead to a buffer overﬂow later.
Width Conversions. Converting a value of one integral
type to a wider (or narrower) integral type which has a
1http://www.sf.net/projects/catchconv
char *badalloc(int sz, int n) {
return (char *) malloc(sz * n);
}
void badcpy(Int16 n, char *p, char *q) {
UInt32 m = n;
memcpy(p, q, m);
}
void badcpy2(int n, char *p, char *q) {
if (n > 800)
return;
memcpy(p, q, n);
}
Figure 1: Examples of three types of integer bugs.
different range of values can introduce width conversion
bugs. For instance, consider badcpy in Figure 1. If the
ﬁrst parameter is negative, the conversion from Int16 to
UInt32 will trigger sign-extension, causing m to be very
large and likely leading to a buffer overﬂow. Because
memcpy’s third argument is declared to have type size t
(which is an unsigned integer type), even if we passed
n directly to memcpy the implicit conversion would still
make this buggy. Width conversion bugs can also arise
when converting a wider type to a narrower type.
Signed/Unsigned Conversion.
Lastly, converting a
signed integer type to an unsigned integer type of the
same width (or vice versa) can introduce bugs, because
this conversion can change a negative number to a large
positive number (or vice versa). For example, consider
badcpy2 in Figure 1. If the ﬁrst parameter n is a negative
integer, it will pass the bounds check, then be promoted
to a large unsigned integer when passed to memcpy.
memcpy will copy a large number of bytes, likely lead-
ing to a buffer overﬂow.
3 Related Work
An earlier version of SmartFuzz and the Metafuzz web
site infrastructure described in this paper were used for
previous work that compares dynamic test generation
with black-box fuzz testing by different authors [1]. That
previous work does not describe the SmartFuzz tool, its
design choices, or the Metafuzz infrastructure in detail.
Furthermore, this paper works from new data on the ef-
fectiveness of SmartFuzz, except for an anecdote in our
“preliminary experiences” section. We are not aware of
other work that directly compares dynamic test gener-
ation with black-box fuzz testing on a scale similar to
ours.
The most closely related work on integer bugs is
Godefroid et al. [15], who describe dynamic test genera-
tion with bug-seeking queries for integer overﬂow, un-
derﬂow, and some narrowing conversion errors in the
context of the SAGE tool. Our work looks at a wider
3
range of narrowing conversion errors, and we consider
signed/unsigned conversion while their work does not.
The EXE and KLEE tools also use integer overﬂow to
prioritize different test cases in dynamic test generation,
but they do not break out results on the number of bugs
found due to this heuristic [8, 6]. The KLEE system also
focuses on scaling dynamic test generation, but in a dif-
ferent way. While we focus on a few “large” programs
in our results, KLEE focuses on high code coverage for
over 450 smaller programs, as measured by trace size and
source lines of code. These previous works also do not
address the problem of type inference for integer types in
binary traces.
IntScope is a static binary analysis tool for ﬁnding in-
teger overﬂow bugs [28]. IntScope translates binaries to
an intermediate representation, then it checks lazily for
potentially harmful integer overﬂows by using symbolic
execution for data that ﬂows into “taint sinks” deﬁned by
the tool, such as memory allocation functions. Smart-
Fuzz, in contrast, eagerly attempts to generate new test
cases that cause an integer bug at the point in the pro-
gram where such behavior could occur. This difference
is due in part to the fact that IntScope reports errors to
a programmer directly, while SmartFuzz ﬁlters test cases
using a tool such as memcheck. As we argued in the
Introduction, such a ﬁlter allows us to employ aggres-
sive heuristics that may generate many test cases. Fur-
thermore, while IntScope renders signed and unsigned
comparisons in their intermediate representation by us-
ing hints from the x86 instruction set, they do not explic-
itly discuss how to use this information to perform type
inference for signed and unsigned types, nor do they ad-
dress the issue of scaling such inference to traces with
millions of instructions. Finally, IntScope focuses only
on integer overﬂow errors, while SmartFuzz covers un-
derﬂow, narrowing conversion, and signed/unsigned con-
version bugs in addition.
The dynamic test generation approach we use was in-
troduced by Godefroid et al. [13] and independently by
Cadar and Engler [7]. The SAGE system by Godefroid
et al. works, as we do, on x86 binary programs and
uses a generational search, but SAGE makes several dif-
ferent design choices we explain in Section 4. Lanzi
et al. propose a design for dynamic test generation of
x86 binaries that uses static analysis of loops to assist
the solver, but their implementation is preliminary [17].
KLEE, in contrast, works with the intermediate repre-
sentation generated by the Low-Level Virtual Machine
target for gcc [6]. Larson and Austin applied symbolic
range analysis to traces of programs to look for potential
buffer overﬂow attacks, although they did not attempt to
synthesize crashing inputs [18]. The BitBlaze [5] infras-
tructure of Song et al. also performs symbolic execution
of x86 binaries, but their focus is on malware and signa-
ture generation, not on test generation.
Other approaches to integer bugs include static anal-
ysis and runtime detection. The Microsoft Prefast tool
uses static analysis to warn about intraprocedural integer
overﬂows [21]. Both Microsoft Visual C++ and gcc can
add runtime checks to catch integer overﬂows in argu-
ments to malloc and terminate a program. Brumley et
al. provide rules for such runtime checks and show they
can be implemented with low overhead on the x86 ar-
chitecture by using jumps conditioned on the overﬂow
bit in EFLAGS [4]. Both of these approaches fail to
catch signed/unsigned conversion errors. Furthermore,
both static analysis and runtime checking for overﬂow
will ﬂag code that is correct but relies on overﬂow se-
mantics, while our approach only reports test cases in
case of a crash or a Valgrind error report.
Blexim gives an introduction to integer bugs [3]. Fuzz
testing has received a great deal of attention since its
original introduction by Miller et al [22]. Notable pub-
lic demonstrations of fuzzing’s ability to ﬁnd bugs in-
clude the Month of Browser Bugs and Month of Kernel
Bugs [23, 20]. DeMott surveys recent work on fuzz test-
ing, including the autodafe fuzzer, which uses libgdb
to instrument functions of interest and adjust fuzz testing
based on those functions’ arguments [11, 27].
Our Metafuzz infrastructure also addresses issues not
treated in previous work on test generation. First, we
make bug bucketing a ﬁrst-class problem and we intro-
duce a fuzzy stack hash in response to developer feedback
on bugs reported by Metafuzz. The SAGE paper reports
bugs by stack hash, and KLEE reports on using the line
of code as a bug bucketing heuristic, but we are not aware
of other work that uses a fuzzy stack hash. Second, we
report techniques for reducing the amount of human time
required to process test cases generated by fuzzing and