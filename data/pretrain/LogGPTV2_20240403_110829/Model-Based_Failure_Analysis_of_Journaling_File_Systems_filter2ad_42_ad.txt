The fsync call returned without ﬂushing any blocks to the
journal while one would expect the ﬁle system to write the
metadata blocks associated with the ﬁle to the disk.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:10:16 UTC from IEEE Xplore.  Restrictions apply. 
Ext3 Reiserfs
×
Committing Failed Transactions
×
Checkpointing Failed Transactions
Not Replaying Failed Checkpoint Writes ×
×
Not Replaying Transactions
×
Replaying Failed Transactions
Crashing File System
×
×
IBM JFS
×
×
×
Table 1: Design Flaws. This table gives a summary of the type of design ﬂaws we have identiﬁed in ext3, Reiserfs and IBM JFS.
Ext3
L
D
F
S
F
U
C
D
L
D
Block Type
Journal Descriptor Block × × ×
× × ×
Journal Revoke Block
× × ×
Journal Commit Block
× × ×
Journal Super Block
× × × ×
Journal Data Block
× × ×
Checkpoint Block
×
Data Block
Reiserfs
IBM JFS
R
C
C
D
L
D
L
D
F
S
F
U
×
L
D
F
R
C
C
D
L
D
–
–
× –
–
×
×
×
× × × ×
×
R
C
–
×
S
F
U
–
×
×
×
Table 2: Analysis Summary. This table presents the summary of the type of failures that can occur in ext3, Reiserfs and IBM JFS when
block writes fail. Data block represents both ordered and unordered writes in ext3 and Reiserfs whereas it represents only ordered writes
in JFS. DC means “Data Corruption”, DL means “Data Loss”, FDL means “Files and Directory Loss”, UFS means “Unmountable File
System”, and CR means “Crash”. A “–” means that the corresponding block type is not available in that ﬁle system. Although JFS does
not have separate commit or revoke blocks, it has records of that type.
4.4 Analysis Summary
The summary of our analysis is presented in Table 1 and
Table 2. Table 1 lists the various design ﬂaws we identiﬁed
in Linux journaling ﬁle systems. Table 2 gives the differ-
ent types of ﬁle system failures that can happen when block
writes fail. Overall, we ﬁnd that Linux journaling ﬁle sys-
tems need a more careful design of their failure handling
policies.
5 Related Work
In this section, we discuss related work. We ﬁrst talk
about related work in fault injection in general and then
about speciﬁc work on ﬁle and storage systems testing.
Fault Injection: Fault injection has been used for a long
time to measure the robustness of systems. Koopman ar-
gues that faults injected directly into the modules under test
do not give representative results for dependability evalua-
tion [11]. He says that the fault must be injected in the ex-
ternal environments of the module under test and the fault
must be activated by inputs during real execution. This is
similar to our approach. We inject faults external to the ﬁle
system module and activate them by running workloads on
top of the ﬁle system.
We use software to simulate the effects of hardware
faults and inject faults by dynamically determining the
block types of the ﬁle system. FTAPE is a tool that performs
dynamic workload measurements and inject faults by auto-
matically determining time and location that will maximize
fault propagation [20]. FIAT is one of the early systems to
use fault injection techniques to simulate the occurrences
of hardware errors by changing the contents of memory or
registers [9]. FINE is a tool developed by Kao et al., to in-
ject hardware induced software faults into UNIX kernel and
trace the execution ﬂow of the kernel [12]. In a more recent
work, fault injection techniques are used to test the Linux
kernel behavior under errors [7].
File and Storage System Testing: Most of the ﬁle system
testing tools test the ﬁle system API with various types of
invalid arguments. Siewiorek et al. develop a benchmark to
measure the system’s robustness and use it to test the de-
pendability of ﬁle system’s libraries [17]. Similarly, Koop-
man et al. use the Ballista testing suite to ﬁnd robustness
problems in Safe/Fast IO (SFIO) library [5]. Another way
to test ﬁle system robustness is to use model checking tech-
niques and apply it to the ﬁle system code. In a more recent
work, Yang et al. use model checking comprehensively to
ﬁnd bugs in three different ﬁle systems: ext3, Reiserfs and
JFS [22]. They use formal veriﬁcation techniques to sys-
tematically enumerate a set of ﬁle system states and verify
them against valid ﬁle system states. Their work can be
used to identify problems like deadlock, and NULL point-
ers whereas our work focuses mainly on how ﬁle systems
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:10:16 UTC from IEEE Xplore.  Restrictions apply. 
handle latent sector errors.
Previous work has studied the reliability of storage sys-
tems. Brown et al. developed a method to measure the sys-
tem robustness and applied it to measure the availability of
software RAID systems in Linux, Solaris and Windows [3].
They use a PC to emulate a disk and use the disk emula-
tor to inject faults. They test the software RAID systems
while our work targets the ﬁle systems. Moreover, we use
ﬁle system knowledge to carefully select and fail speciﬁc
block types whereas they do not require any semantic in-
formation for fault injection. Other studies have evaluated
RAID storage systems for reliability and availability [8, 10].
These studies have developed detailed simulation models of
RAID storage arrays and network clusters and used them to
obtain the dependability measures.
6 Conclusion
In this paper, we propose a new way to evaluate the ro-
bustness of journaling ﬁle systems under disk write failures.
We build semantic models of different journaling modes
and use them to inject faults into the ﬁle system disk re-
quests. We evaluate three widely used Linux journaling ﬁle
systems. From our analysis, we ﬁnd that ext3 and IBM JFS
violate journaling semantics on block write failures, which
could result in corrupt ﬁle systems.
In contrast, Reiserfs
maintains ﬁle system integrity by crashing the entire system
on most write failures. However, on permanent write fail-
ures, this will result in repeated crashes and restarts. Based
on the analysis, we identify various design ﬂaws and cor-
rectness bugs in these ﬁle systems that can catastrophically
affect the on-disk data. Overall, we ﬁnd that modern ﬁle
systems need more design consideration placed into their
failure handling policies.
Acknowledgments
We thank the members of the ADSL research group for
their insightful comments. We also thank the anonymous
reviewers for their thoughtful suggestions. This work is
sponsored by NSF CCR-0092840, CCR-0133456, CCR-
0098274, NGS-0103670,
ITR-0325267,
IBM and EMC.
ITR-0086044,
References
[1] R. H. Arpaci-Dusseau and A. C. Arpaci-Dusseau. Fail-Stutter Fault
Tolerance. In HotOS VIII, pages 33–38, Schloss Elmau, Germany,
May 2001.
[2] S. Best.
JFS Overview. www.ibm.com/developerworks/library/l-
jfs.html, 2004.
[4] P. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong,
and S. Sankar. Row-Diagonal Parity for Double Disk Failure Cor-
rection. In Proceedings of the 3rd USENIX Symposium on File and
Storage Technologies (FAST ’04), pages 1–14, San Francisco, Cali-
fornia, April 2004.
[5] J. DeVale and P. Koopman. Performance Evaluation of Exception
In Dependable Systems and Networks,
Handling in I/O Libraries.
June 2001.
[6] J. Gray and A. Reuter. Transaction Processing: Concepts and Tech-
niques. Morgan Kaufmann, 1993.
[7] W. Gu, Z. Kalbarczyk, I. K. Ravishankar, and Z. Yang. Characteri-
zation of linux kernel behavior under error. In Dependable Systems
and Networks, pages 459–468, June 2003.
[8] Y. Huang, Z. T. Kalbarczyk, and R. K. Iyer. Dependability Analysis
of a Cache-Based RAID System via Fast Distributed Simulation. In
The 17th IEEE Symposium on Reliable Distributed Systems, 1998.
[9] Z. S. D. S. J.H. Barton, E.W. Czeck. Fault Injection Experiments
Using FIAT. In IEEE Transactions on Computers, volume 39, pages
1105–1118, April 1990.
[10] M. Kaniche, L. Romano, Z. Kalbarczyk, R. K. Iyer, and R. Karcich.
A Hierarchical Approach for Dependability Analysis of a Commer-
cial Cache-Based RAID Storage Architecture. In The Twenty-Eighth
Annual International Symposium on Fault-Tolerant Computing, June
1998.
[11] P. Koopman. What’s wrong with fault injection as a dependability
benchmark? In Workshop on Dependability Benchmarking (in con-
junction with DSN 2002), Washington DC, July 2002.
[12] W. lun Kao, R. K. Iyer, and D. Tang. FINE: A Fault Injection and
Monitoring Environment for Tracing the UNIX System Behavior
Under Faults. In IEEE Transactions on Software Engineering, pages
1105–1118, 1993.
[13] M. K. McKusick, W. N. Joy, S. J. Lefﬂer, and R. S. Fabry. Fsck - The
UNIX File System Check Program. Unix System Manager’s Manual
- 4.3 BSD Virtual VAX-11 Version, April 1986.
[14] V. Prabhakaran, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau.
Analysis and Evolution of Journaling File Systems. In Proceedings
of the USENIX Annual Technical Conference (USENIX ’05), Ana-
heim, California, April 2005.
[15] H. Reiser. ReiserFS. www.namesys.com, 2004.
[16] F. B. Schneider.
Implementing Fault-Tolerant Services Using The
State Machine Approach: A Tutorial. ACM Computing Surveys,
22(4):299–319, December 1990.
[17] D. Siewiorek, J. Hudak, B.-H. Suh, and Z. Segal. Development of
In The Twenty-Third
a benchmark to measure system robustness.
International Symposium on Fault-Tolerant Computing, 1993.
[18] A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nishimoto, and
G. Peck. Scalability in the XFS File System. In Proceedings of the
USENIX Annual Technical Conference (USENIX ’96), San Diego,
California, January 1996.
[19] N. Talagala and D. Patterson. An Analysis of Error Behaviour in a
Large Storage System. In The IEEE Workshop on Fault Tolerance in
Parallel and Distributed Systems, San Juan, Puerto Rico, April 1999.
[20] T. K. Tsai and R. K. Iyer. Measuring Fault Tolerance with the FTAPE
Fault Injection Tool. In 8th Intl. Conf. On Modeling Techniques and
Tools for Conp. Perf. Evaluation, pages 26–40, Sept 1995.
[21] S. C. Tweedie.
Journaling the Linux ext2fs File System.
In The
Fourth Annual Linux Expo, Durham, North Carolina, May 1998.
[22] J. Yang, P. Twohey, D. Engler, and M. Musuvathi. Using Model
Checking to Find Serious File System Errors. In Proceedings of the
6th Symposium on Operating Systems Design and Implementation
(OSDI ’04), San Francisco, California, December 2004.
[3] A. Brown and D. A. Patterson. Towards Maintainability, Availabil-
ity, and Growth Benchmarks: A Case Study of Software RAID Sys-
tems. In Proceedings of the USENIX Annual Technical Conference
(USENIX ’00), pages 263–276, San Diego, California, June 2000.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:10:16 UTC from IEEE Xplore.  Restrictions apply.