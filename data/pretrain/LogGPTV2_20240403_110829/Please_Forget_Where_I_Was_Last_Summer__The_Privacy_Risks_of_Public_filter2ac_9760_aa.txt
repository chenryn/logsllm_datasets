title:Please Forget Where I Was Last Summer: The Privacy Risks of Public
Location (Meta)Data
author:Kostas Drakonakis and
Panagiotis Ilia and
Sotiris Ioannidis and
Jason Polakis
Please Forget Where I Was Last Summer:
The Privacy Risks of Public Location (Meta)Data
Kostas Drakonakis,∗ Panagiotis Ilia,∗ Sotiris Ioannidis,∗ Jason Polakis†
∗ FORTH, Greece
† University of Illinois at Chicago, USA
{kostasdrk, pilia, sotiris}@ics.forth.gr
Abstract—The exposure of location data constitutes a signif-
icant privacy risk to users as it can lead to de-anonymization,
the inference of sensitive information, and even physical threats.
In this paper we present LPAuditor, a tool that conducts a
comprehensive evaluation of the privacy loss caused by public
location metadata. First, we demonstrate how our system can
pinpoint users’ key locations at an unprecedented granularity
by identifying their actual postal addresses. Our evaluation on
Twitter data highlights the effectiveness of our techniques which
outperform prior approaches by 18.9%-91.6% for homes and
8.7%-21.8% for workplaces. Next we present a novel exploration
of automated private information inference that uncovers “sen-
sitive” locations that users have visited (pertaining to health,
religion, and sex/nightlife). We ﬁnd that location metadata can
provide additional context to tweets and thus lead to the exposure
of private information that might not match the users’ intentions.
We further explore the mismatch between user actions and
information exposure and ﬁnd that older versions of the ofﬁcial
Twitter apps follow a privacy-invasive policy of including precise
GPS coordinates in the metadata of tweets that users have
geotagged at a coarse-grained level (e.g., city). The implications
of this exposure are further exacerbated by our ﬁnding that users
are considerably privacy-cautious in regards to exposing precise
location data. When users can explicitly select what location data
is published, there is a 94.6% reduction in tweets with GPS
coordinates. As part of current efforts to give users more control
over their data, LPAuditor can be adopted by major services
and offered as an auditing tool that informs users about sensitive
information they (indirectly) expose through location metadata.
I.
INTRODUCTION
The capability of modern smartphones to provide ﬁne-
grained location information has enabled the deployment of a
wide range of novel functionality by online services. In Twitter
users can incorporate location information in their tweets to
provide more context and enrich their communications [49], or
even enhance situational awareness during critical events [69].
Nonetheless, the presence of location metadata in a by-default-
public data stream like Twitter constitutes a signiﬁcant privacy
risk. Apart from potentially enabling physical threats like stalk-
ing [31], [53] and “cybercasing” [29], location information
could lead to the inference of very sensitive data [48], [16],
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23151
www.ndss-symposium.org
PI:EMAIL
and even get combined with other information collected from
online services [54]. Previous work has demonstrated how
to identify users’ key locations (i.e., home and work) at a
postcode [25] or very coarse-grained (∼10,000m2) level [34],
[19]. However, this coarse granularity fails to highlight the true
extent of the privacy risks introduced by the public availability
of geographical
information in users’ tweets. Furthermore,
these studies have not explored what sensitive information can
be inferred from users geotagging tweets at other locations.
In this paper we develop LPAuditor, a system that examines
the privacy risks users face due to publicly accessible loca-
tion information, and conduct a large scale study leveraging
Twitter data and public APIs. Initially we present techniques
for identifying a user’s home and work at a postal address
granularity; our heuristics are built around intuitive social
and behavioral norms. We ﬁrst conduct a two-level clustering
process for creating clusters of tweets and mapping them to
postal addresses, which is robust to GPS errors [71] and spatial
displacement due to user mobility (e.g., the user tweeting
while arriving or departing from home). We then analyze the
spatiotemporal characteristics of a user’s tweets to infer those
key locations. Through an arduous manual process we create
a ground truth dataset for 2,047 users, which enables us to
experimentally evaluate our auditing tool. Our system is able
to identify the home and workplace for 92.5% and 55.6% of the
users respectively. When compared to state-of-the-art results,
we ﬁnd that our techniques outperform previous approaches
by 18.9%-91.6% for homes and 8.7%-21.8% for workplaces.
Apart from the increased effectiveness of our techniques,
our work demonstrates that by leveraging widely available
geolocation databases attackers can pinpoint users’ key loca-
tions at a granularity that is orders of magnitude more precise
than previously demonstrated. Without doubt, this level of
accuracy renders the identiﬁcation of users a trivial task. The
privacy implications of our ﬁndings are even more alarming
when considering the prominent role that platforms like Twitter
play in protests and other forms of social activism [35]. A
substantial number of users choose to not reveal their actual
identity, and prior work has found a correlation between the
choice of anonymity and the sensitivity of topics in tweets [50].
LPAuditor offers a comprehensive analysis of the privacy
loss caused by location metadata by also exploring whether the
remaining locations can be used to infer personal information
that is typically considered sensitive. While the inference of
sensitive information has been one of the main motivations
behind prior research on location-privacy [57], such automated
attacks have not been demonstrated in practice. Our system
examines tweets that place the user at (or in close proximity of)
locations that are associated with such information. Currently
we search for locations pertaining to three sensitive topics:
religion, medical issues, and sex/nightlife. We ﬁnd that 71%
of users have tweeted from sensitive locations, 27.5% of
which can be placed there with high conﬁdence based on the
content of their tweets. Privacy loss is ampliﬁed by the location
metadata as it leaks additional contextual details to the tweet’s
content; e.g., the user may simply mention being at a doctor
without giving more details, while the location metadata places
the user at an abortion clinic. We also explore a spatiotemporal-
based approach and ﬁnd that 29.5% of the users can be placed
at a sensitive location regardless of tweet content. We envision
LPAuditor being offered as an auditing tool by location-based
services, informing users about the sensitive information that
can be inferred based on their publicly accessible location data.
Finally, our study reveals that older versions of the Twitter
app implement a privacy-invasive policy. Speciﬁcally, tweets
that are geotagged by users at a coarse granularity level (e.g.,
city) include the user’s exact coordinates in the tweets’ meta-
data. This privacy violation is invisible to users, as the GPS
coordinates are only contained in the metadata returned by
the API and not visible through the Twitter website or app. To
make matters worse, this historical metadata currently remains
publicly accessible through the API. We quantify the impact
of Twitter’s invasive policy, and ﬁnd that it results in an almost
15-fold increase in the number of users whose key locations are
successfully identiﬁed by our system. In an effort to remediate
this signiﬁcant privacy threat we have disclosed our ﬁndings
to Twitter. In summary, our main research contributions are:
• We conduct a comprehensive, IRB-approved, large-scale
exploration of the privacy risks that users face when
location data is, either explicitly or inadvertently, shared
in a public data stream like Twitter’s API.
• We develop LPAuditor, a system that leverages location
metadata for identifying key locations with high preci-
sion, outperforming state-of-the-art approaches. Apart from
achieving superior granularity, we also introduce a cluster-
ing approach that renders our system robust to errors in
GPS readings or spatial displacement due to user mobility.
• We present the ﬁrst, to our knowledge, study on the fea-
sibility of automated location-based inference attacks. Our
system leverages novel content-based and spatiotemporal
techniques for inferring sensitive user information, thus,
validating the motivation of prior location-privacy research.
• We measure the impact of Twitter’s invasive policy for
collecting and sharing precise location data and quantify
the lingering implications. Our study on user geotagging
behavior reveals that users are restrained when publishing
their location and avoid including exact coordinates when
given control by the underlying system, yet remain exposed
due to the availability of this historical data.
II. MOTIVATION AND THREAT MODEL
The sensitive nature of mobility data is well known to the
research community, which has proposed various techniques so
far for limiting the granularity of the location data that services
can obtain (e.g., [32]). In practice, however, such defenses have
not seen wide deployment and a large number of mobile apps
collect precise locations [60]. While prior work has proposed
approaches for identifying key locations (home and work),
the reported granularity is not sufﬁcient for demonstrating the
true extent of the threat (e.g., [25], [34], [19], [20]). More
importantly, the risk of sensitive information being inferred
from other location data points remains unexplored.
Despite the privacy risk this data poses to users, services do
not stringently prohibit access to it and may expose it to third
parties [38] or render it publicly accessible. To demonstrate
the extent and accuracy of sensitive information inference that
an adversary can achieve, we develop and evaluate LPAuditor
exclusively using public and free data streams and APIs. Fur-
thermore, we design our system to be application-independent
and applicable to other location datasets. We show that location
metadata enables the inference of sensitive information that
could be misused for a wide range of scenarios (e.g., from
a repressive regime de-anonymizing an activist’s account to
an insurance company inferring a customer’s health issues, or
a potential employer conducting a background check). While
we build a tool that can be adopted by online services for
better protecting users’ privacy, the techniques employed by
our system could be applied by a wide range of adversaries or
invasive third parties. By demonstrating the severity and practi-
cality of such attacks, we aim to initiate a public discussion and
incentivize the adoption of privacy-preserving mechanisms.
III. SYSTEM OVERVIEW
In this section we provide an overview of our system. First
we describe how LPAuditor clusters location data and identiﬁes
key locations. Next we provide details on our methodology for
identifying sensitive locations that users may have visited.
A. Data Labeling and Clustering
Labeling tweets. The ﬁrst step is to label each geotagged
tweet with the corresponding postal address. To highlight the
extent of the risk that users face, we opt for publicly available
API services that could be trivially employed by attackers for
mapping each tweet’s GPS coordinates to an address. To that
end, we use the reverse geocoding API by ArcGIS [1] for the
majority of our labels, and the more accurate but rate-limited
Google Maps Geocoding API [4] for the subset of labels that
are more critical to our accuracy. However, in practice, if
LPAuditor is adopted by a major service like Twitter, Google
Maps API could be used for the entirety of the calls.
Since our dataset is large in size, we developed a form of
caching that allows avoiding unnecessary API calls. Instead of
issuing a call for every pair of coordinates we come across,
we estimate the spatial position of the pair of coordinates and
search for nearby coordinates that have already been labeled.
If the distance to a labeled pair of coordinates is less than
two meters, we assign the same address label to the new pair
of coordinates. Experimentally, we found that this approach
reduced the number of API calls our system issued by 42.5%.
It should be noted, however,
that geocoding APIs do not
always return an address. We label those tweets with “unknown
address”. After a manual investigation and veriﬁcation of a
random subset, we observed that they typically correspond to
places like university campuses, airports or remote rural areas
2
that do not have exact postal addresses. Nonetheless, while we
don’t have a postal address in these cases, the granularity of
our process is unaffected as we still have the GPS coordinates.
Initial clustering. LPAuditor groups tweets assigned to
the same postal address into a single cluster (i.e., ﬁrst-level
clustering). Then, by taking into consideration the coordinates
of all the tweets of a cluster, we calculate the cluster’s mid-
point. To verify that the label assigned to a cluster corresponds
to the cluster’s actual address, we use the Google Maps API for
retrieving the address of the cluster’s mid-point coordinates. If
the address returned does not match the assigned address, due
to incompatibilities between the two APIs or borderline cases
where our caching approach results in assigning a neighboring
address, we opt for the address returned from Google’s API.
However, due to Google’s stricter API rate limits, we only use
this methodology for verifying the address of the 10 largest
clusters of each user, which we have empirically found to
be the most signiﬁcant ones. This follows our threat model
constraint of demonstrating what attacks can be conducted
using free and public APIs. In practice, attackers with many
resources could avoid rate limiting or use other proprietary
geolocating databases. For tweets with the “unknown address”
label we employ the DBSCAN algorithm [26]. We empirically
set our threshold to 30 meters, but due to its cascading effect
we may cluster together points that have a greater distance due
to other points laying in between them. We only use DBSCAN
for clustering tweets that have been marked with “unknown
address” (∼16% of clusters); nearby tweets that have been
labeled with an actual address are not considered by DBSCAN.
Second-level clustering. We have observed that the initial
clustering approach can result in multiple neighboring clusters
for a speciﬁc place. The most common case involves one large
dominant cluster in the area and a few signiﬁcantly smaller
clusters next to it, in close proximity. In general, it is difﬁcult
to distinguish which tweets belong to each cluster, even by
plotting the coordinates of these tweets on a map and visually
inspecting them. Through an empirical analysis, where we
visually inspected clusters and cross referenced the timing of
their tweets, it became apparent that these closely neighboring
clusters typically correspond to a single user location but have
been mapped to a neighboring address. Various factors can lead
to this, such as inaccuracies in the user’s GPS readings [71],
the precision of the geocoding APIs, as well as differences in
the actual tweeting position of the user (tweeting when leaving
a place or arriving, being in the backyard or at a neighbor etc.).
As these nearby clusters most likely correspond to the same
place, we implement a second-level clustering for grouping
neighboring clusters into a larger one.1 First we identify which
cluster in an area of multiple neighboring clusters has the
most
tweets, and then we employ a modiﬁed version of
DBSCAN for estimating which clusters should be merged
with the larger one. For this clustering we consider that the
distance between the mid-point of the larger cluster and all
the smaller ones should not exceed 50 meters.2 To eliminate
DBSCAN’s cascading effect we check this distance before
deciding whether a cluster should be included in the new one.
1For the remainder, clusters will imply second-level unless stated otherwise.
2We based this threshold on the FCC mandate for 911 caller location
accuracy [28], as it can account for GPS errors but is not prohibitively large
so as to lead to false positives. We also experimentally veriﬁed its suitability.
Overall, implementing our second-level of clustering al-
lows us to introduce a (conﬁgurable) radius for effectively
mapping these “runaway” data points to the main cluster.
Nonetheless, it is important to note that the initial clustering
step (using the geocoding API) is actually necessary; solely
applying DBSCAN’s radius-based clustering to the dataset
leads to oversized clusters and eliminates the ﬁner granularity
that is achieved by the two-level clustering approach.
B. Identifying Key User Locations
Here we describe how LPAuditor selects the clusters that
represent two key user locations (home and workplace) in an
automated fashion. Our system does not take into consideration
the content and semantics of the tweets posted, but only the
temporal characteristics and distribution of the tweets in each
cluster. It should be emphasized that our work focuses on
location metadata and not the tweet content as this allows us
to quantify the true extent of the privacy risks introduced by
location metadata: even cautious users that do not explicitly
disclose information about their key locations face this privacy
loss. However, LPAuditor leverages the content for increasing