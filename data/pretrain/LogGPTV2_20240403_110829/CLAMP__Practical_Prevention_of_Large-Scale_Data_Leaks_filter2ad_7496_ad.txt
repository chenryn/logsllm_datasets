triggers to maintain data consistency. Fortunately, in the
applications we examined, link tables are rare and contain
relatively little data. PostgreSQL is one major database that
does not directly support updateable views. However, it
allows the creation of rules that rewrite modiﬁcations of a
view’s content into appropriate actions on other tables, and
hence could be made to support CLAMP’s access policies.
6.3. Data Access Policies
Of the 47 tables in the osCommerce database, we identiﬁed
7 that contain sensitive data (either related to customers
or their orders). Thus, each policy ﬁle contains 7 lines,
one for each table. We crafted policies for three access
classes: user, admin, and nobody. The admin class
(used by the store’s owner) was given full access to the
tables with sensitive data, while the nobody class was
given no access. The user policy (Figure 5) restricts the
Table
address book
customers
customers info
customers basket
customers basket attributes
products notiﬁcations
orders
Restriction
customers id = UID
customer id = UID
customer info id = UID
customer id = UID
customer id = UID
customer id = UID
customer id = UID
Figure 5. osCommerce User Data Access Policy
data in each sensitive table based on a customer id value
used as an index in all 7 tables. Even as newcomers to
osCommerce, we found it straightforward to identify the
tables with sensitive information and to craft the policy
ﬁles. Altogether, this effort required less than an hour.
The data access policy for MyPhpMoney proved simi-
larly effortless (Appendix A). HotCRP’s extremely ﬂexible
and conﬁgurable access model makes it a worst case for
data policy development, and indeed, it took considerably
more work. Nonetheless, a few days of effort proved
sufﬁcient (Appendix B).
6.4. Dispatcher
The Dispatcher VM has two virtual network interfaces:
one connecting to the Internet and another connecting
to the virtual LAN segment containing the WebStacks.
The Dispatcher is approximately 750 lines of C++ code
built on top of the OpenSSL library. To simplify our
prototype implementation, the Dispatcher is co-located with
a VM pool manager, which notiﬁes the Dispatcher when a
WebStack is ﬁnished and when a clean replacement is ready
(our full design places this functionality within the QR
to provide defense-in-depth). Additionally, the Dispatcher
forwards non-SSL (port 80) trafﬁc to a special, unprivileged
WebStack that serves public, non-sensitive data.
6.5. Isolation Layer
Our prototype implementation of the CLAMP architecture
uses the Xen 3.1.0 VMM [1] to isolate server compo-
nents, though as we note in Section 3.2.1, other isolation
techniques offer viable alternatives. The prototype uses a
master WebStack to create a read-only ﬁle system from
which each ramdisk-based WebStack is instantiated. This
maximizes performance by removing the hard disk from the
performance-critical path, and minimizes the time required
to refresh a WebStack between clients.
Ideally, refreshing a WebStack (after a client’s session
has terminated) should be implemented using the delta vir-
tualization technique developed by Vrable et al. to enable
a single machine to serve as a honeypot for thousands of
IP addresses [30]. Delta virtualization refers to the ability
to fork (similar to a process-level fork) a running reference
VM many times, using copy-on-write memory sharing to
163
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:42 UTC from IEEE Xplore.  Restrictions apply. 
minimize the memory footprint of additional VMs. For
systems such as honeypots and our WebStacks, the memory
savings can be substantial, since all WebStacks are identical
until client activity inﬂuences their execution. In addition,
WebStacks can be instantiated so rapidly that we can
fork additional WebStacks on demand, i.e., in response to
incoming client connections.
Unfortunately, due to bugs and instabilities in the delta-
virtualization version of Xen, we were unable to test the
throughput of our implementation using delta virtualization.
Thus,
to simulate CLAMP’s throughput with a stable
version of delta virtualization, we create a pool of 50
static WebStacks and assign each one 64 MB of RAM.
When a client terminates a connection to a WebStack,
the Dispatcher waits an amount of time equal
to the
time needed to destroy and then fork a new VM using
delta-virtualization, and then reuses the existing WebStack.
Without delta virtualization, CLAMP would have to start
new WebStacks from scratch.
The VMM also enforces the communication restrictions
shown in Figure 4. With Xen, the Domain 0 VM provides
the backend driver for the network cards in the guest VMs.
Hence, all communication between CLAMP components
travels through Domain 0, and Domain 0 can always
authoritatively identify a packet’s source. Thus, we assign
each VM a unique IP address and then use iptables in
Domain 0 to prevent VMs from spooﬁng their IP addresses
and to control which VMs can communicate.
7. Evaluation
While our CLAMP prototype provides strong security ben-
eﬁts via VMM isolation and QR database access control, it
comes at the expense of additional processing overhead.
As x86 virtualization becomes increasingly vital
to IT
infrastructures, we expect this overhead to diminish. Ex-
perience also suggests that companies are willing to invest
additional hardware resources in exchange for tangible
security beneﬁts (e.g., some e-commerce sites use dedi-
cated hardware to ofﬂoad SSL processing). Alternatively,
CLAMP can utilize other isolation techniques with different
performance-security tradeoffs (Section 3.2.1).
We use our proof-of-concept prototype to estimate the
impact CLAMP may have on web server performance, both
in terms of web request latency (Section 7.1) and the overall
throughput of the system (Section 7.2). As explained in
Section 6.5, the current version of delta virtualization is
unreliable [30], and hence the throughput experiments use
static VMs to simulate the effects of delta virtualization.
A practical deployment of CLAMP would obviously
require improving the efﬁciency and robustness of delta
virtualization, developing better documentation, construct-
ing an installer, and creating a better management interface.
We believe these are all tractable tasks.
Experimental Setup. We run all of our experiments
against the same database installed on a dedicated machine.
We use MySQL 5.1.31 running on Debian Linux on a
2.00 GHz Pentium IV with 512 MB RAM. We run Xen
3.3 with a para-virtualized Linux kernel on a four-core
1.80 GHz AMD Opteron with 6 GB RAM. Our “native”
web server used as a baseline runs on the same AMD
machine, but with the Linux kernel running directly on the
hardware. Both the Xen VMs and the baseline installation
use version 2.6.18 of the Linux kernel. Our test client is
equipped with a 3.00-GHz Core 2 Duo and 2 GB of RAM.
Results Overview. Our results indicate that while our
current prototype imposes substantial request processing
overhead, the overall performance of the system remains
reasonable. The most signiﬁcant overhead that our proto-
type faces comes from spawning new virtual machines.
Thus, efﬁcient implementation of CLAMP using virtual-
ization will beneﬁt from additional improvements in rapid
VM spawning, an area of active research [5, 13, 30]. We
focus on the results for osCommerce.
7.1. Latency
We use a series of macrobenchmarks to measure our
prototype’s impact on the latency of several classes of web
requests.
7.1.1. Macrobenchmarks. For these benchmarks, clients
retrieve osCommerce pages from either a “native” server
running directly on hardware or a CLAMP server as
described in Section 6. Both servers run on the same
hardware, use the same version of osCommerce, and access
the same database server. The servers’ caches are warmed
prior to measurements, and we report
the average and
standard deviation of 50 trials for each request type.
These experiments measure the time it takes to complete
a single client’s ﬁrst request to an unloaded server. The time
includes SSL establishment time, and, with CLAMP, the
time required for the Dispatcher to select and connect to
a WebStack. Since we assume the server is lightly loaded,
this WebStack can be pre-forked, and hence we do not
include the time needed to fork a WebStack. We discuss
forking overhead below in Section 7.2.2.
Figure 6 compares request
latency with and without
CLAMP. The ﬁrst two requests show the time to fetch
(with SSL, since we expect CLAMP applications to use
SSL) a small (8 KB) or a large (3 MB) static ﬁle that does
not require database access. The static ﬁle retrieval with
SSL reveals that the cost of SSL session establishment—a
cost companies accept today—dominates, and the CLAMP
prototype adds less than 2% overhead for small ﬁles. With
large ﬁles, Xen’s virtualized networking overhead reduces
performance, but overhead remains under 14% for 3MB
ﬁles. If an SSL connection has already been established,
then our CLAMP prototype adds 0.12 ms (16%) to small
164
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:42 UTC from IEEE Xplore.  Restrictions apply. 
)
s
m
(
e
m
T
i
200
150
100
50
0
Native
CLAMP
   File
(8 KB)
   File
(3 MB)
Login
Database
   Read
Database
  Modify
Figure 6. Macrobenchmark Latency. Comparison of the
average time to complete different web requests within os-
Commerce on native hardware versus our CLAMP prototype.
Smaller is better.
ﬁles and 22 ms (20%) to large ﬁles. Improvements to
virtualized networking performance is an active area of
research [12, 15, 24].
Finally,
The “Login” measurement quantiﬁes the overhead from
the additional work that CLAMP performs when a user
logs in. The login page is SSL protected, makes several
database queries, and requires inter-VM communication
between WebStacks, the UA, and the QR. Importantly,
login times are only slightly longer (10 ms, or 10% longer)
using our CLAMP prototype. These results indicate that the
QR’s step of creating a restricted database for an individual
user does not increase login completion time excessively.
the “Database Read” test measures the time
required to load an SSL-protected PHP page that makes
20 unique database SELECT queries after the user has
logged in (and hence established an SSL connection), while
the “Database Modify” test measures the time required to
load an SSL-protected PHP page that makes 10 unique
database INSERT queries and 10 unique UPDATE queries
after the user has logged in. These tests represent
the
most common use scenarios for a CLAMP application; the
CLAMP prototype adds only 7 ms (19% overhead) to pages
based on database reads and 5 ms (14% overhead) to pages
that make database modiﬁcations, amounts well below the
threshold at which users will notice a delay.
In microbenchmarks, we found that the use of MySQL’s
views added 50% overhead to read requests. Other
databases (e.g., PostgreSQL) offer better view performance,
with overheads of less than 7% on the same workload.
7.2. Throughput
throughput (i.e., the number of users that can be handled
simultaneously), which is affected by both memory and
CPU resources.
7.2.1. WebStack Memory Usage. Unlike the native web
server, the number of simultaneous users that CLAMP can
support is limited by the number of WebStacks that ﬁt in
memory. As discussed in Section 6.5, delta virtualization
creates a copy of a master WebStack using copy-on-
write memory sharing.6 Thus, the memory consumed by
a WebStack is limited to the number of unique memory
pages to which it writes.
To evaluate the effectiveness of this memory sharing, we
measure the amount of private memory (i.e., memory that
must be allocated to a WebStack after it writes to a memory
page) used by a WebStack. We warm the master WebStack
prior to forking the WebStacks that handle benchmark
requests. We perform experiments to benchmark WebStack
memory overhead involving small ﬁle requests (between 1
and 4 KB) and full osCommerce PHP page requests. The
osCommerce page requests involve retrieving embedded
images and issuing multiple database queries to generate
the resulting web page.
Figure 7 summarizes our results. The ﬁrst data point
(Unique URL “0”) shows the memory usage of the forked
WebStack before it has served any requests. The subsequent
points show the memory usage of the forked WebStack 10
seconds after an additional unique URL is retrieved. The
line labeled Single Object shows that requesting individual
ﬁles increases only slightly (less than 1 MB) the amount of
memory consumed by a WebStack. The line labeled Com-
plete Page indicates that retrieval of complete osCommerce
pages increases the memory consumed by a WebStack by
approximately 1 MB.
These results indicate that even a client who browses
many image-rich and database intensive pages will only
incur a virtualization memory overhead of a few tens of
megabytes. Thus, if memory were the only bottleneck, our
server with 6 GB of RAM could support at most 500
simultaneous WebStacks (and hence authenticated users),
though each WebStack can handle multiple requests from
its user. However, in practice, we ﬁnd that CLAMP hits
CPU resource limits before it reaches memory limits.
7.2.2. CPU Usage. CPU resources limit the rate at which
CLAMP can process client logins (due to the need to fork
new WebStacks), and the rate at which it can handle con-
nections from established users (due to context switching
between WebStacks).
Logins. When a new user logs in, CLAMP must allocate
a new WebStack. We implement this by forking the mas-
ter WebStack image using the version of Xen developed
As shown above,
the CLAMP prototype only slightly
increases the latency of individual requests. The other
important metric is our prototype’s effect on the server’s
6. This does not create a security risk, since the master WebStack’s
memory image does not contain any sensitive data. Any modiﬁcations
made by a WebStack will be seen only by that particular WebStack.
165
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:42 UTC from IEEE Xplore.  Restrictions apply. 
Complete Page
Single Object
)
B
M
(
y
r
o
m
e
M
M
V
r
e
P
 10
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0
 0
 1
 2
 3
 4
 5
 6
Unique URL
Figure 7. WebStack Memory Usage. With delta virtual-
ization, a WebStack’s memory usage grows as it handles
additional requests. Here, we measure that growth by fetch-
ing individual images (the “Single Object” line) and complete
osCommerce pages.
for the Potemkin Honeyfarm [30]. The Potemkin authors
report that VM forking requires approximately 500 ms,
and in our tests, we found that one CPU could fork two
WebStacks/second, while the native server can handle up