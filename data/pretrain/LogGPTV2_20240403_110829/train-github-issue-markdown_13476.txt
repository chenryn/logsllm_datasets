### System Info
    transformers branch main
### Wrong Codes in examples/pytorch/**_no_trainer.py
    for step, batch in enumerate(eval_dataloader):
                with torch.no_grad():
                    generated_tokens = accelerator.unwrap_model(model).generate(
                        batch["input_ids"],
                        attention_mask=batch["attention_mask"],
                        **gen_kwargs,
                    )
                    generated_tokens = accelerator.pad_across_processes(
                        generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
                    )
                    labels = batch["labels"]
                    if not args.pad_to_max_length:
                        # If we did not pad to max length, we need to pad the labels too
                        labels = accelerator.pad_across_processes(batch["labels"], dim=1, pad_index=tokenizer.pad_token_id)
                    generated_tokens, labels = accelerator.gather((generated_tokens, labels))
                    generated_tokens = generated_tokens.cpu().numpy()
                    labels = labels.cpu().numpy()
                    if args.ignore_pad_token_for_loss:
                        # Replace -100 in the labels as we can't decode them.
                        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
                    if isinstance(generated_tokens, tuple):
                        generated_tokens = generated_tokens[0]
                    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
                    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
                    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
                    # If we are in a multiprocess environment, the last batch has duplicates
                    if accelerator.num_processes > 1:
                        if step == len(eval_dataloader):
                            decoded_preds = decoded_preds[: len(eval_dataloader.dataset) - samples_seen]
                            decoded_labels = decoded_labels[: len(eval_dataloader.dataset) - samples_seen]
                        else:
                            samples_seen += decoded_labels.shape[0]
here, In the for loop, step will never equal to len(eval_dataloader), so here
should be modified to `if step == len(eval_dataloader) - 1`
and
`samples_seen += decoded_labels.shape[0]`
decoded_labels is a list that produced by the postprocess_text(),  
list object have no attribute shape.
GLHF
### Information
  * The official example scripts
  * My own modified scripts
### Tasks
  * An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
  * My own task or dataset (give details below)
### Reproduction
just run the examples scripts provided in the readme
### Expected behavior
    samples_seen exceed the dataset size
    and also the attribute error