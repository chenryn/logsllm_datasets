latest and representative defense methods, which cover
all categories of existing defenses.
3) Attack Utility Evaluation (AUE). In this module, we
implement 10 utility metrics of adversarial attacks (as
detailed in Section II-B). With AUE, users can evaluate
to what extent the generated AEs satisfy the essential
utility requirements of adversarial attacks.
4) Defense Utility Evaluation (DUE). Similar to AUE,
DUE is mainly used for evaluating the utility of the state-
of-the-art defenses in terms of 5 utility metrics, as deﬁned
in Section II-D. With this module, users can measure
to what extent a defense-enhanced model preserves the
fundamental functionality of the original model after
applying all the defenses in DM.
5) Security Evaluation (SE). Leveraging both AM and DM
modules, SE is used to evaluate the vulnerability and
resilience of defense-enhanced models against existing at-
tacks. More importantly, users can determine whether the
(cid:23)(cid:24)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
defense-enhanced models that are planned to deploy/share
are resistant to current adversarial attacks.
To the best of our knowledge, DEEPSEC is the ﬁrst im-
plemented uniform evaluating and securing system for DL
models, which comprehensively and systematically integrates
the state-of-the-art adversarial attacks, defenses and relative
utility metrics of them. The signiﬁcance of DEEPSEC to
research and application lies in the following aspects.
• First, before sharing or deploying the pre-trained DL mod-
els publicly, DEEPSEC enables model owners to conveniently
and freely choose any existing defenses to secure their models.
Model owners can also employ evaluation modules (i.e.,
DUE and SE) of DEEPSEC to examine whether the defense-
enhanced models satisfy their utility/security requirements.
• Second, DEEPSEC is a uniform platform for systemat-
ically evaluating different adversarial attacks and defenses.
Previously, due to the lack of such a uniform platform, existing
attacks and defenses are often implemented and evaluated on
different experimental settings (e.g., DL models, parameter
settings, evaluation metrics, testing environment, etc.). Con-
sequently,
those implementation and evaluation differences
make researchers confused about the actual utility and secu-
rity performance of attacks/defenses. Some researchers even
draw contradictory conclusions for the same problems. For
instance, the effectiveness of DD against adversarial attacks
obtains different observations in different work [10], [12], [13].
However, as a uniform evaluation platform, DEEPSEC can
reduce the evaluation bias as much as possible and facilitate
fair comparisons among different attacks/defenses. Therefore,
DEEPSEC allows model owners to compare the performance
of all possible defense-enhanced models that adopt different
defenses and thus make the best decision.
• Third, DEEPSEC allows researchers to evaluate the utility
and security performance of newly proposed adversarial at-
tacks by attacking state-of-the-art defenses. Also, DEEPSEC
enables researchers to compare the performance of newly
proposed defenses with existing defenses as well as to examine
their defenses’ resistance against existing adversarial attacks.
Therefore, DEEPSEC is helpful for both attack and defense
research to conveniently and fairly apply existing approaches
to comprehensively understand the actual performance.
In addition to providing a uniform evaluation system,
DEEPSEC takes a fully modular implementation, which makes
it easily extendable. First, algorithms in DEEPSEC are im-
plemented using PyTorch [45], which has been widely used
in the DL research community. Second, all modules inside
DEEPSEC are independent of each other, which means that
each module can work individually. Additionally, as shown in
Fig. 1, multiple modules can also work together to perform
rich evaluation. Third, all algorithms or evaluation tests within
each module are also independent, which means that they can
be implemented, measured and employed independently.
B. System Implementation
gories of existing attacks that include 8 UAs: FGSM [15],
R+FGSM [16], BIM [17], PGD [18], U-MI-FGSM [19],
DF [20], UAP [21], OM [22]; and 8 TAs: LLC [17],
R+LLC [16],
ILLC [17], T-MI-FGSM [19], BLB [2],
JSMA [11], CW2 [10], EAD [23].
In DM, we implement 13 defense algorithms, which also
cover all the categories of state-of-the-art defense algorithms
summarized in Section II-C. Speciﬁcally, the implemented
defense algorithms include 3 adversarial training defenses:
NAT [24], EAT [16] and PAT [18]; 2 gradient masking
defenses: DD [10] and IGR [13]; 4 input
transformation
based defenses: EIT [25], RT [26], PD [27] and TE [28];
one region-based classiﬁcation defense RC [29]; as well as
3 detection-only defenses: LID [30], FS [31] and the detector
of MagNet [32].
Note that for both adversarial attacks and defenses, our
implementations take representativeness, scalability and prac-
ticality into consideration, which leads us to implement the
latest, scalable and practical adversarial attacks and defenses.
In addition, for AUE and DUE, we implement 10 attack
utility metrics (introduced in Section II-B) and 5 defense utility
metrics (introduced in Section II-D), respectively.
IV. EVALUATIONS
In this section, we ﬁrst evaluate the utility performance of
all adversarial attacks and various defense algorithms. Then,
we examine the security performance of all defenses against
various adversarial attacks. Note that all experiments were
conducted on a PC equipped with 2 Intel Xeon 2.2GHz CPU,
256GB system memory and one NVIDIA GTX 1080 GPU.
A. Evaluation of Attacks
1) Experimental Setup: We employ two popular benchmark
datasets: MNIST [46] and CIFAR-10 [47], which have been
widely used in image classiﬁcation tasks. To be compatible
with existing work on adversarial attacks or defenses, we train
a 7-layer CNN [10] and a ResNet-20 model [48] for MNIST
and CIFAR-10 (more details are shown in Appendix VIII-A),
respectively. We achieve 99.27% testing accuracy on MNIST
and 85.95% testing accuracy on CIFAR-10.
We present our evaluation methodology as follows. At
ﬁrst, we randomly sample 1000 examples that are correctly
predicted by the corresponding model from each dataset’s
testing set. Then, for each attack in AM, we generate 1000
AEs on the sampled examples. Finally, leveraging AUE, we
examine the utility performance of all attacks. Particularly, the
target class for each TA is chosen randomly and uniformly
among the labels except the ground truth. 2
The criteria for parameter setting in evaluating attacks are:
(i) the value of common parameters of different attacks are
kept the same for unbiased comparisons, e.g., all L∞ attacks
share the same restriction . (ii) all
the other parameters
follow the same/similar setting in the original work for all
In AM, we implement 16 adversarial attacks as we sum-
marized in Section II-A. Speciﬁcally, we cover all cate-
2We do not choose the target class for LLC, R+LLC and ILLC, since they
inherently take the least-likely class as the target class.
(cid:23)(cid:24)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
attacks. The detailed parameter settings can be found in
Appendix IX-A.
2) Experimental Results: We only present the evaluation
results of CIFAR-10 in Table III, since the results for MNIST
are similar and we defer them to Appendix X.
Misclassiﬁcation. Generally, most of existing attacks, in-
cluding both UAs and TAs, show strong attacking ability with
high MR. More speciﬁcally, it can be observed that iterative
attacks present noticeably higher MR than non-iterative at-
tacks. Furthermore, we ﬁnd that all iterative attacks, including
iterative UAs and iterative TAs, have nearly 100% MR on
CIFAR-10. The reason is intuitive that iterative attacks run
multiple complicated iterations to ﬁnd the optimal perturbation
for the target model, while non-iterative attacks only take one
step to compute the perturbation.
In spite of 100% MR, some adversarial attacks have low
ACAC, which indicates that AEs generated by those attacks
are low conﬁdent. We suggest that directly comparing the exact
ACAC among all kinds of attacks can be misleading since
they might have totally different parameters, e.g., it is unfair
to compare ACAC between ILLC (L∞ attack) and CW2 (L2
attack). On the other hand, via ﬁne-tuning the parameters of
attacks, their performance can be signiﬁcantly changed. For
instance, if the κ of CW2 is increased from 0 to 20, ACAC
of CW2 increases from 0.393 to 1.000 on CIFAR-10.
Basically, AEs with higher ACAC have lower ACTC, since
the sum of probability of each class is 100%. However, if
ACAC is lower than 100%, ACTC can be relatively high or
low. In that case, for AEs with similar ACAC, we suggest
such AEs with lower ACTC would show better resilience to
other models as their true classes are less likely to be correctly
classiﬁed by other models (e.g., defense-enhanced models or
other raw models). For instance, both FGSM ( = 0.1) and
OM achieve around 0.75 ACAC on CIFAR-10, but the ACTC
of FGSM is 6× lower than that of OM. Hence, we conclude
that FGSM shows better resilience than OM, which will later
be empirically veriﬁed in following evaluations (see more in
Section IV-C and Section V-A, respectively).
Remark 1. In most cases, existing attacks show high attack
success rate (i.e., MR) in terms of misleading the target model.
In addition to MR, it is also important to evaluate the attacks
with other metrics. For instance, we observe that AEs with
low ACTC show better resilience to other models.
Imperceptibility. We quantify and analyze the impercepti-
bility of AEs in terms of ALDp, ASS and PSD.
In general, most existing attacks explore Lp norm to for-
mulate attack algorithms in their objective functions. From
Table III, we observe that attacks that use the same ALDp
metric in their attack objectives tend to perform better in that
distance measurement than in other distance measurements.
For instance, L∞ attacks, perform better in L∞ distortion,
but perform poorly in both L0 and L2 distortions.
On the other hand, via ﬁne-tuning parameters, Lp distortions
of attacks can be easily increased for better misclassiﬁcation
performance. For instance, when we increase κ from 0 to
20, all Lp distortions of CW2 signiﬁcantly increase. Similar
observations are obtained when we increase  for FGSM. The
above observations suggest that there exists an objective trade-
off between misclassiﬁcation and imperceptibility. The trade-
off stems from the mathematical framework of adversarial
attacks, which are usually formulated as optimization problems
with two objectives: (i) to misclassify the adversarial sample
and (ii) to minimize the perceptual difference of adversarial
and benign samples. As these two objectives are not always
aligned, there exists a tension between misclassiﬁcation and
imperceptibility, which has been empirically conﬁrmed.
Compared with ALDp, existing attack techniques perform
better at preserving ASS. On CIFAR-10, most attacks achieve
nearly 100% similarity between original examples and cor-
responding AEs. This is because ASS is consistent with Lp
norms, and thus balanced Lp norms (i.e., none of L0, L2
and L∞ is extremely high) can result
in high ASS. For
instance, AEs generated by CW2 and EAD show moderate
Lp distortions, which leads to nearly 100% similarity between
original examples and AEs.
According to the results, PSD is more sensitive than ASS.
the PSDs of L2 attacks are much
Also, we observe that
lower than those of other attacks (i.e., L∞ or L0 attacks).
This implies that AEs generated by L2 attacks are more
visually imperceptible than those generated by other attacks
w.r.t PSD. One possible reason is that the formulation of
PSD is consistent with L2 distortion, and thus L2 attacks
outperform others in both L2 and PSD.
Remark 2. Among all imperceptibility metrics, PSD is the
most sensitive imperceptible metric to the perturbation of
AEs, while ASS is the least sensitive, which we suggest is
not suitable to quantify AEs. Also,
the trade-off between
misclassiﬁcation and imperceptibility is empirically conﬁrmed.
Robustness. We examine the robustness of existing attacks
w.r.t three metrics (i.e., NTE, RGB, RIC). In our evaluation,
we use Guetzli [49], an open source compression algorithm
that creates high visual quality images. Speciﬁcally, the radius
of Gaussian blur is set to 0.5 for RGB and the compression
quality is set to 90% for RIC.
In general, the evaluation results of NTE, RGB and RIC
are positively correlated for the adversarial attack. As shown
in Table III, adversarial attacks with high NTE tend to perform
better in RGB and RIC in most cases. The underlying reason
could be that high NTE implies higher probability of the
misclassiﬁed class, and therefore it can tolerate more trans-
formations than AEs with smaller NTE. On the other hand,
the correlation of NTE, RGB and RIC is non-linear as they
measure the robustness of attacks from different perspectives.
For instance, we observe that the NTE of CW2 (κ = 20) is
extremely high while its RIC is quite low.
Generally, AEs with higher ACAC are shown to be more
robust in RGB and RIC. This is because ACAC can inﬂuence
NTE directly and thus further inﬂuence RGB and RIC since
these two metrics are consistent with NTE as discussed before.
Therefore, increasing the ACAC via ﬁne-tuning parameters in
(cid:23)(cid:24)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
UTILITY EVALUATION RESULTS OF ALL ADVERSARIAL ATTACKS ON CIFAR-10
TABLE III
s
t
e
s
a
t
a
D
UA/
TA
Objec-
tive
Attack
L∞
 = 0.1
UAs
0
1
-
R
A
F
I
C
TAs
L2
L∞
 = 0.1
L0
L2
Misclassiﬁcation
Imperceptibility
ALDp
Robustness
Attacks
FGSM
 = 0.1
 = 0.2
R+FGSM
U-MI-FGSM
BIM
PGD
UAP
DF
OM
LLC
R+LLC
ILLC
T-MI-FGSM
JSMA
BLB
CW2
EAD
κ = 0
κ = 20
EN
L1
MR
89.7%
89.8%
83.7%
100.0%
100.0%
100.0%
85.3%
100.0%
100.0%
13.4%
31.5%
100.0%
100.0%
99.7%
100.0%
100.0%
100.0%
100.0%
100.0%
ACAC
0.743
0.873
0.846
1.000
1.000
1.000
0.723
0.516
0.750
0.768
0.876
1.000
1.000
0.508
0.500
0.393
1.000
0.433
0.377
ACTC
0.033
0.008
0.018
0.000
0.000
0.000
0.038
0.458
0.182
0.016
0.009
0.000
0.000
0.164
0.349
0.348
0.000
0.316
0.352
L0
0.993
0.994
0.520
0.775
0.979
0.919
1.000
0.135
0.274
0.992
0.531
0.764
0.937
0.022
0.218
0.230
0.557
0.106
0.041
L2
5.423
10.596
3.871
2.003
3.682
3.816
5.335
0.078
0.192
5.400
3.897
1.829
4.063
4.304
0.111
0.112
0.279
0.156
0.185
L∞
0.100
0.200
0.100
0.100
0.100
0.100
0.100
0.010
0.022
0.100
0.100
0.100
0.100
0.879
0.013
0.013
0.031