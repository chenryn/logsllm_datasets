优化后的文本如下：

---

## 9.2 传统工具

### 9.2.1 输出示例

以下输出包含许多细节，从事件发生时在CPU上的进程开始，包括时间戳、磁盘主次编号、I/O类型编码（rwbs，如前文所述）以及其他关于I/O的详细信息。

```
533 [004] 543348.164815: block:block_rq_issue: 259, 0 S 4096 (}
2564656 + 8 [0]
sxapper
0 [004] 543348.164887: block:block_rq_complete: 259, 0 S ()
kxorkez/ult:0 2386T
4) 0*6sz 1e4edu0obaxooa=xog =09691*8>Ee+s [s00]
3190760 + 256 [0]
Gd 2533T[001] 543348.165046:
block:block_rq_insert: 259,0 R 131072 ()
3191272 + 256 [dd]
dd 2533T[001] 543348.165050: block:block_rq_issue: 259,0 S 131072 [)
3191272 + 256 [dd]
---
```

### 9.2.2 后处理工具

过去我曾构建过一些工具来后处理这些事件，用于计算延迟直方图和可视化访问模式。然而，对于繁忙的系统，这意味着需要将所有块事件转储到用户空间进行后处理。BPF可以在内核中更高效地执行这种处理，并仅输出所需的结果。例如，稍后介绍的biosnoop(8)工具就是一个例子。

### 9.2.3 blktrace

blktrace(8)是一个专门用于跟踪块I/O事件的实用程序。使用其btrace(8)前端可以跟踪所有事件：

```bash
btrace /dev/nvme2n1
```

输出示例：

```
259, 0 2 1 0.000000000 430 Ms 2163864 + 8 [jbd2/nvme2n1-8]
259, 0 2 2 0.000009556 430 GMs 2163864+ 8 [jbd2/nvme2n1-8]
259, 0 2 3 0.000011109 430 Pα [jbd2/nvme2n1-8]
259, 0 2 4 0.000013256 430 Q Ms 2163872 + 8 [jbd2/nvme2n1-8]
...
```

每行I/O事件包含以下列：
1. 设备主次编号
2. CPU ID
3. 序列号
4. 操作时间（秒）
5. 进程ID
6. 操作标识符（见blkparse(1)：Q == 队列，G == 获取请求，P == 插入，M == 合并，D == 发出，C == 完成等）
7. RWBS描述（见本章“rwbs”部分：W == 写，S == 同步等）
8. 地址 + 大小 [设备]

输出可以通过Chris Mason的seekwatcher [91]进行后处理和可视化。与perf(1)每事件转储一样，对于繁忙的磁盘I/O工作负载，blktrace(8)的开销可能是一个问题。使用BPF在内核中进行汇总可以大大减少这种开销。

### 9.2.4 SCSI日志

Linux还内置了SCSI事件日志功能。可以通过sysctl(8)或/proc启用。例如，这两个命令将所有事件类型的日志设置为最大值（警告：根据您的磁盘工作负载，这可能会淹没系统日志）：

```bash
sysctl -w dev.scsi.logging_level=0x1b6db6db
echo 0x1b6db6db > /proc/sys/dev/scsi/logging_level
```

该数字格式是一个位字段，为10种不同事件类型设置1到7的日志级别。定义在drivers/scsi/scsi_logging.h中。sg3-utils包提供了一个scsi_logging_level(8)工具来设置这些级别。例如：

示例事件：

```
dmesg
[...]
[542136,259412] sd 0:0:0:0: tag#0 Send: scnd 0x0000000001fb89dc
[542136.259422] sd 0:0:0:0: tag#0 CCB: Test Unit Ready 00 00 0 0 0 00
[542136.261103] sd 0:0:0:0: tag#0 Done: Success Result: hostbyte=DID_OK driverbyte=DRIVER_OK
[542136.261110] sd 0:0:0:0: tag#0 CDB: Test Unit Ready 00 00 00 00 00 00
[542136.261115] sd 0:0:0:0: tag#0 Sense Key 1 Not Ready [current]
[542136.261121] sd 0:0:0:0: tag#0 Add. Sense: Medium not present
[542136.261127] sd 0:0:0:0: tag#0 0 sectors total, 0 bytes done.
```

虽然提供了时间戳（第一列），但没有唯一的标识细节，很难计算I/O延迟。这可以用来帮助调试错误和超时。BPF跟踪可以用来生成自定义的SCSI级和其他I/O堆栈级日志，在内核中计算更多的I/O细节，包括延迟。

---

## 9.3 BPF工具

### 9.3.1 biolatency

biolatency(8)是BCC和bpftrace的一个工具，用于显示块I/O设备延迟的直方图。术语“设备延迟”指的是从向设备发出请求到完成的时间，包括在操作系统中排队的时间。

以下是BCC中的biolatency(8)在一个生产Hadoop实例上运行10秒的示例：

```bash
biolatency 10 1
Tracing block device I/O... Hit Ctrl-C to end.
usecs: count distribution
0 -> 1 : 0
2 -> 3 : 0
4 -> 7 : 0
8 -> 15 : 0
16 -> 31 : 0
32 -> 63 : 0
64 -> 127 : 15
128 -> 255 : 4475
256 -> 511 : 14222
512 -> 1023 : 12303
1024 -> 2047 : 5649
2048 -> 4095 : 1074
4096 -> 8191 : 2078
8192 -> 16383 : 7688
16384 -> 32767 : 4111
32768 -> 65535 : 818
65536 -> 131071 : 220
131072 -> 262143 : 103
262144 -> 524287 : 48
524288 -> 1048575 : 6
```

此输出显示双峰分布，一个峰值在128到2047微秒之间，另一个在约4到32毫秒之间。了解为什么会出现双峰分布后，可以通过调整使更多I/O进入更快的模式。最慢的I/O达到了262到524毫秒的范围，这表明设备队列很深。

biolatency(8)和稍后的biosnoop(8)工具已经解决了许多生产问题。它们特别适用于分析云环境中的多租户驱动器，这些驱动器可能会很嘈杂并破坏延迟SLO。Netflix的Cloud Database团队在小型云实例上使用biolatency(8)和biosnoop(8)隔离了不可接受的双峰或延迟驱动器，并将其从分布式缓存层和分布式数据库层中移除。经过进一步分析，团队决定改变部署策略，选择足够大的节点以拥有专用驱动器。这一小改动有效地消除了延迟异常值，而无需额外的基础设施成本。

biolatency(8)工具目前通过使用kprobes跟踪各种块I/O内核函数来工作。它是在BCC支持tracepoint之前编写的，因此使用了kprobes。在大多数磁盘IOPS率较低的系统上，该工具的开销应该是可以忽略不计的。

---

希望这些优化能使文本更加清晰、连贯和专业。如果有任何进一步的需求，请告诉我！