533 [004] 543348.164815: b1ock:b1ock_rq_1ssue: 259, 0 s8t 4096 (}
2564656 + 8 [0]
sxapper
0 [004] 543348.164887: block:b1ock_rq_conp1ete: 259, 0 38 ()
kxorkez/u17:0 2386T
4) 0*6sz 1e4edu0obaxooa=xog =09691*8>Ee+s [s00]
3190760 + 256 [0]
Gd 2533T[001] 543348.165046:
block:block_rq_1nsert: 259,0 R 131072 ()
3191272 + 256 [dd]
dd 25337[001] 543348.165050: b1ock:b1ock_rq_1ssue: 259,0 3 131072 [)
3191272 + 256 [dd]
---
## Page 386
9.2 Traditional Tools
349
dd 25337[001] 543348,165111:
3191272 + 256 [0]
block:block_rq_conplete: 259, 0 R (1
[...]
The output contains many details, beginning with the process that was on-CPU when the event
apnu seap o puaa au so aqistodsan ssaood aq aq ou  so de rua pamoo
a timestamp, disk major and minor numbers, a string encoding the type of I/O (rwbs, described
earlier), and other details about the I/O.
I have in the past built tools that post-process these events for calculating latency histograms, and
visualizing access patterns. However, for busy systems this means dumping allblock events to
user space for post-processing. BPF can do this processing in the kernel more efficiently, and then
emit only the desired output. See the later biosnoop(8) tool as an example.
9.2.3
blktrace
blktrace(8) is aspecialized utility for tracing block I/O events. Using its btrace(8) front-end to trace
all events:
 btrace /dev/nvme2n1
259, 0
2
1
0.000000000
4300 Ms 2163864 + 8 [jbd2/nvme2n18]
259, 0
2
2
0. 00000955 6
430 GMs 2163864+ 8[jbd2/nvme2n1-B]
259, 0
2
3
0, 000011109
430
Pα[jbd2/nvme2n1-8]
259, 0
2
0. 00001325 6
430
Q Ms 2163872 + B [jbd2/nvme2n1B]
259, 0
0, 000015740
430 MMSs 2163872+ 8 [jbd2/nvme2n1-B]
[..-]
259, 0
15
0.00002 6963
430 IMs 2163864 + 48 [jbd2/nvne2n18]
259, 0
2
16
0. 000046155
430 DMs 2163864+ 4B[bd2/nvne2n1-B]
259, 0
2
17
0. 000699822
430
QMs 2163912 + 8[jbd2/nme2n1-8]
0 *65Z
2
18
0. 000701539
430
GMs 2163912 + 8[jbd2/nvme2n18]
259, 0
19
0.000702820
430
I Ms 2163912 + B [jbd2/nvme2n1B]
259, 0
2
20
0,000704649
430 D Ms 2163912 + 8 [jbd2/nvme2n1-B]
259, 0
11
1
0.000664811
0C MS 2163864 + 48 [0]
259, 0
11
0.001098435
0C MS 2163912 + 8 [0]
[...]
Multiple event lines are printed for each I/O. The columns are:
1. Device major, minor number
2. CPU ID
3. Sequence number
4. Action time, in seconds
1 See iolatency(8) in perftools [78]: this uses Prs
fer, which avoids the overhesd of creating and writing a perf.data file.
data from the trace but
---
## Page 387
350
0 Chapter 9 Disk I/0
5. Process ID
6. Action identifier (see blkparse(1): Q == queued, G == get request, P = plug, M == merge,
D == issued, C == completed, etc.
7. RWBS description (see the *rwbs" section earlier in this chapter): W == write,
S == synchronous, etc.
8. Address + size [device]
The output can be post-processed and visualized using Chris Mason’s seekwatcher [91]-
As with perf(1) per-event dumping, the overhead of blktrace(8) can be a problem for busy disk I/O
workloads. In-kernel summaries using BPF can greatly reduce this overhead.
9.2.4 SCSI Logging
Linux also has a built-in facility for SCSI event logging. It can be enabled via sysctl(8) or /proc. For
example, both of these commands set the logging to the maximum for all event types (warning:
depending on your disk workload, this may flood your system log):
+sysct1 -α dev.scsi .1ogging_level=0x1b6db6db
echo 0x1b6db6@b > /proc/sys/dev/scs1/1ogging_1evel
The format of the number is a bitfield that sets the logging level from 1 to 7 for 10 different
event types. It is defined in dlrivers/scsi/scsi_logging.h. The sg3-utils package provides a
scsi_logging_level(8) tool for setting these. For example:
Example events:
dnesg
[. ..]
[542136,259412] sd 0:0:0:0: tag#0 Send: scnd 0x0000000001fb89dc
[542136.259422] sd 0:0:0:0: tag#0 CCB: Test Unit Ready 00 00 0 0 0 00
[542136.261103] sd 0:0:0:01 tagt0 Done: SUcCEss Result: hostbyte=DID_0K
dciverbyte=DRIVER_OK
[542136.261110] sd 0:0:0:0: tag#0 CDB: Test Un1t Ready 00 00 00 00 00 00
[542136.261115] sd 0:0:0:0: tag#0 Sense Key 1 Mot Ready [current]
[542136.261121] sd 0:0:D:0: tag#0 Add. Sense: Medlum not present
[542136.261127] sd 0:0:0:0: tag0 0 sectors totsl, 0 bytes done.
 - -. 1
column), using them to calculate 1/O latency is difficult without unique identifying details.
This can be used to help debug errors and timeouts. While timestamps are provided (the first
BPF tracing can be used to produce custom SCSI-level and other I/O stack-level logs, with more
I/O details including latency calculated in the kernel.
---
## Page 388
9.3  BPF Tools
351
9.3
BPF Tools
They are shown in Figure 9-3.
Applications
System Call Interface
VFS
adflush
File Systems
Biock Device Interlace
Lotheu
Volume Manager
Rest of Kernel
Block Layer
HBA (SCSI)
Device Drivers
Figure 9-3  BPF tools for disk analysis
These tools are either from the BCC and bpftrace repositories covered in Chapters 4 and 5, or were
created for this book. Some tools appear in both BCC and bpftrace. Table 9-2 lists the origins of
the tools covered in this section (BT is short for bpftrace).
Table 9-2
Disk-Related Tools
1001
Source
Description
biolatency
BCC/BT
Block I/0
Summarize block I/O latency as a histogram
blosnoop
BCC/BT
Block I/0
Trace block I/O with PID and latency
biotop
BCC
Block 1/0
Top for disks: summarize block I/O by process
b1tesize
BCC/BT
Block I/0
Show disk I/0 size histogram by process
seeksize
Book
Block I/0
Show requested I/O seek distances
biopattern
Book
Block I/0
Identify random/sequential cisk access patterns
biostacks
Book
Block I/0
Show disk I/O with initialization stacks
b1oerr
Book
Block I/0
Trace disk errors
mdf1ush
BCC/BT
MD
Trace md flush requests
losched
Book
I/0 sched
Summarize l/O scheduler latency
scs1latency
Book
SCSI
Show SCSl command latency dlistributions
scsiresult
Book
ISSS
Show SCSI command result codes
nvmelatency
Book
NVME
Summarize NVME driver command latency
---
## Page 389
352
Chapter 9 Disk I/0
For the tools from BCC and bpftrace, see their repositories for full and updated lists of tool
options and capabilities. A selection of the most important capabilities are summarized here,
See Chapter 8 for file system tools.
9.3.1 biolatency
biolatency(8)² is a BCC and bpftrace tool to show block I/O device latency as a histogram. The
term device latency refers to the time from issuing a request to the device, to when it completes,
including time spent queued in the operating system.
The following shows biolatency(8) from BCC, on a production Hadoop instance, tracing block
I/O for 10 seconds:
+ biolstency 10 1
Tzacing block device I/0... Hit Ctrl-C to end.
usec.a
: count
dlstxibution
0 > 1
: 0
E  7
: 0
B -> 15
: 0
16 > 31
: 0
32 -> 63
 0
64 > 127
: 15
128 -> 255
: 4475
256 > 511
:14222
512 -> 1023
: 12303
+ +
1024 -> 2047
: 5649
560b  8191
: 1980
|+****
EBE9T 32767
: 1895
****,
32768 -> 65535
: T21
| **
65536 > 131071
: 394
| *
: 65
262144 > 524287
: 17
This output shows a bi-modal distribution, with one mode between 128 and 2047 microseconds
and the other between about 4 and 32 milliseconds. Now that I know that the device latency is
bi-modal, understanding why may lead to tuning that moves more I/O to the faster mode. For
Susn patpuaap aq ueo qoum) O/I azis-sa8re[ ao 'o/I wopueu aq plnoo O/1 1amols au adturexa
2 0rigin: I crested this as iolatency.d for the 2011 0Trace book [Greg 11], following the same name 8s my other
to signify block I/0. 1 created biolatency for BCC on 20-Sep-2015 and bpfrace on 13-Sep-2018.
losnoop and iotop tools. This led to confusion since *io* is ambiguous, so for BPF I've added the *b° to these tools
---
## Page 390
9.3  BPF Tools
353
other BPF tools). The slowest I/O in this output reached the 262- to S24-millisecond range: this
sounds like deep queueing on the device.
biolatency(8) and the later biosnoop(8) tool have been used to solve many production issues.
They can be especially useful for the analysis of multi-tenant drives in cloud environments, which
can be noisy and break latency SLOs, When running on small cloud instances, Netflix’s Cloud
Database team was able to use biolatency(8) and biosnoop(8) to isolate machines with unaccept-
dlatabase tiers. Upon further analysis, the team decided to change their deployment strategy based
ably bi-modal or latent drives, and evict them from both distributed caching tiers and distributed
on these findings, and now deploy clusters to fewer nodes, choosing those large enough to have
dedlicated drives. This small change effectively eliminated the latency outliers with no additional
infrastructure cost.
The biolatency(8) tool currently works by tracing various block I/O kernel functions using kprobes.
It was written before tracepoint support was available in BCC, so used kprobes instead. The overhead
of this tool shoulkd be negligible on most systems where the disk IOPS rate is low ( 1
: 0
2 -> 3
: 0
4 -> 7
: 0
8 -> 15
: 0
1.6 -> 31
: 0
32 > 63
: 0
64 -> 127
: 1
128 -> 255
: 2780
25 6 -> 511
: 10386
512 -> 1023
: 8399
0z 4095
: 1074
]  ×* t
4096 -> 8191
: 2078
**....+.
8192 -> 16383
: 7688
*★*********★**1
16384 -> 32767
: 4111
32768 -> 65535
: 818
|***
65536 -> 131071
: 220
131072 -> 262143
: 103
262144 -> 524287
: 48
524288 > 1048575
: 6
---
## Page 391
354
Chapter 9 Disk I/O
The output is not much different: this time there's some more I/O in the slower mode. iostat(1)
confirms that the queue lengths are small (avgqu-sz  1
: 0
2 > 3
: 0
4 -> 7
: 0
8 -> 15
: 0
1.6 -> 31
: 0
32 > 63
: 0
64 -> 127
: 0
128 -> 255
: 1