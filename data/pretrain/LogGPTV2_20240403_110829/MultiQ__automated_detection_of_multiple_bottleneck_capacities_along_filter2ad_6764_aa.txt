title:MultiQ: automated detection of multiple bottleneck capacities along
a path
author:Sachin Katti and
Dina Katabi and
Charles Blake and
Eddie Kohler and
Jacob Strauss
MultiQ: Automated Detection of
Multiple Bottleneck Capacities Along a Path
Sachin Katti
MIT CSAIL
Dina Katabi
MIT CSAIL
Charles Blake
Eddie Kohler
Jacob Strauss
MIT CSAIL
UCLA/ICIR
MIT CSAIL
ABSTRACT
multiQ is a passive capacity measurement tool suitable for large-
scale studies of Internet path characteristics. It is the ﬁrst passive tool
that discovers the capacity of multiple congested links along a path
from a single ﬂow trace, and the ﬁrst tool that effectively extracts
capacity information from ack-only traces. It uses equally-spaced
mode gaps in TCP ﬂows’ packet interarrival time distributions to
detect multiple bottleneck capacities in their relative order.
We validate multiQ in depth using the RON overlay network,
which provides more than 400 heterogeneous, well-understood In-
ternet paths. We compare multiQ with two other capacity mea-
surement tools (Nettimer and Pathrate) in the ﬁrst large-scale wide-
area evaluation of capacity measurement techniques, and ﬁnd that
multiQ is highly accurate; for instance, though multiQ is pas-
sive, it achieves the same accuracy as Pathrate, which is active.
Categories and Subject Descriptors: C.2.6 [Computer Commu-
nication Networks]: Internetworking –Measurement
General Terms: Measurement, Management
Keywords: Capacity, Measurement, Modeling
1 INTRODUCTION
Passive estimation of path properties has applications ranging from
overlay network path optimization, to building representative de-
scriptions of the current Internet for use in simulation and modeling,
to tracking the evolution of the Internet over time using a library of
traces collected over multiple years. In this paper, we focus on an im-
portant sub-problem—passively estimating multiple bottleneck link
capacities along a path from a ﬂow trace.
Current passive tools discover the minimum capacity along a path
using logs of data packet interarrival times [10]. They fail, or have
greatly reduced accuracy, when run on ack logs, so one cannot learn
path capacity from sender-side traces, such as those at a Web server.
They also recover only the minimum capacity, obscuring any sec-
ondary bottlenecks inside the network; but secondary bottleneck in-
formation is vital for network modeling and other applications.
We present multiQ, the ﬁrst passive capacity measurement tool
that avoids both these limitations. multiQ is based on equally-
spaced mode gaps, or EMG, a new passive technique for inferring
multiple link capacities from data or ack interarrival times. In con-
trast to prior work, which has inferred link capacity from the loca-
tion of the modes in the packet interarrival distribution [4, 7, 10, 14],
EMG uses the distance between consecutive modes.
We evaluate multiQ’s accuracy using over 10,000 experiments
This material is based in part upon work supported by the National Science
Foundation under Grant No. 0230921.
Permission to make digital or hard copies of all or part of this work for per-
sonal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. To copy otherwise, to re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’04, October 25–27, 2004, Taormina, Sicily, Italy.
Copyright 2004 ACM 1-58113-821-0/04/0010 . . . $5.00.
on 400 heterogeneous Internet paths with known likely capacities,
and compare it with Nettimer [10], another passive capacity mea-
surement tool, and Pathrate [4], an active tool. Our results include:
(cid:15) multiQ is as accurate as Pathrate, which is active. In particular,
85% of the measurements are within 10% of their correct values.
(cid:15) multiQ is 11% more accurate than Nettimer when both tools
are given access only to receiver-side traces (data packet interar-
rivals). Nettimer needs access to both receiver- and sender-side
logs to achieve accuracy comparable to multiQ.
(cid:15) Run on ack traces, 70% of multiQ’s measurements are within
20% of their actual values. Though the accuracy is lower than
in the case of data traces, it is a substantial improvement over
prior tools; for instance, run on the same traces, less than 10% of
Nettimer’s measurements are within 20% of their actual values.
2 CROSS TRAFFIC: NOISE OR DATA?
The packet pair technique has traditionally been used to infer the
minimum capacity along a path. A sender emits a pair of probe
packets back-to-back; assuming cross trafﬁc does not intervene be-
tween the two probes, they arrive spaced by the transmission time
on the bottleneck link. The capacity of the bottleneck is computed
as C = S=T, where S is the size of the second probe and T is the
time difference between the probes’ arrivals at the receiver.
Cross trafﬁc can cause errors in packet pair-based capacity es-
timates [4]. Compression errors happen when the ﬁrst packet of
a probe pair gets delayed more than the second, because it gets
queued up behind cross trafﬁc downstream of the bottleneck link.
This shrinks the arrival spacing, leading to an overestimate. Inﬂa-
tion errors occur when cross trafﬁc intervenes between the probe
packets upstream of the bottleneck; this expands the arrival spacing,
leading to an underestimate. To eliminate these cross-trafﬁc effects,
prior work sends trains of packets (packet bunch mode) [16] or a
variety of packet sizes [4]; uses the global mode in the interarrival
histograms [10]; and so forth. Yet, as the bottleneck becomes more
congested, eliminating the effect of cross trafﬁc becomes more chal-
lenging, Given this, is it possible that cross-trafﬁc effects contain
any useful information, rather than just being noise? We demonstrate
that cross trafﬁc, with proper interpretation, actually helps detect not
only the minimum capacity along the path, but also the capacities of
other congested links.
A cross-trafﬁc burst is all trafﬁc that intervenes between two con-
secutive packets of a ﬂow. We seek to understand the probability
distribution of cross-trafﬁc burst sizes: i.e., the chance that a given
amount of trafﬁc will intervene between consecutive packets of a
ﬂow at a congested link. We examined 375 million packets in 258
NLANR traces, collected at 21 backbone locations, with a total of
about 50,000 signiﬁcant ﬂows. (See Table 1 for a deﬁnition of “sig-
niﬁcant ﬂow” and other important terms.) The diversity and size of
this data set makes it a plausible sample of the Internet. For each pair
of packets in a signiﬁcant ﬂow, we compute the intervening cross-
trafﬁc burst at the link where the trace is taken. This is repeated for
all signiﬁcant ﬂows. Figure 1a shows the distribution of the sizes of
these bursts. Note the surprising regularity: sharp modes separated
by equal gaps of 1500 bytes.
Bottleneck
Signiﬁcant ﬂow
Cross-trafﬁc burst
Capacity
Narrow link
Tight link
Path capacity
A link where trafﬁc faces queuing.
A TCP ﬂow that achieves an average packet rate > 10 pps ((cid:25) 1 pkt/RTT), contains at least 50 packets, and has an MTU of
1500 bytes. (The vast majority of medium-to-long data ﬂows have this MTU.)
Trafﬁc intervening between two consecutive packets of a traced ﬂow.
The maximum rate at which packets can be transmitted by a link.
The link with the smallest capacity along a path.
The link with minimum available or unused capacity along a path.
Capacity of the narrowest link on that path.
Table 1—Deﬁnitions of terms used in this paper.
Burst Size
1500B
1500B
1500B
 40  576
 1500
 3000
 4500
 6000
Intervening Cross-Traffic Burst Size (Bytes, 8 Byte Bins)
P(Packet Size)
y
t
i
s
n
e
D
y
t
i
l
i
b
a
b
o
r
P
y
t
i
l
i
b
a
b
o
r
P
e
v
i
t
l
a
u
m
u
C
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0
 1
 0.8
 0.6
 0.4
 0.2
 0
 40  576
 1500
 4500
Packet Size (Bytes, 8 Byte Bins)
 3000
 6000
y
t
i
s
n
e
D
.
b
o
r
P
y
t
i
s
n
e
D
.
b
o
r
P
CCICOM->CMU
 0.5
 0.45
 0.4
 0.35
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0
 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8  2  2.2  2.4  2.6  2.8  3
Interarrival times in milliseconds
(a) Flow from CCICOM to CMU
CMU->CCICOM
 0.025
 0.02
 0.015
 0.01
 0.005
 0
Figure 1—(a) The distribution of cross trafﬁc between consecutive pack-
ets in a signiﬁcant ﬂow has equally-spaced mode gaps of 1500 bytes. (b)
The CDF of packet size reveals frequencies of 40- and 1500-byte packets.
To understand this result, see Figure 1b, which shows a cumu-
lative distribution function (CDF) of packet sizes in these traces.
The dominant sizes are 40 and 1500 bytes; no other sizes are highly
pronounced. (This replicates earlier results [22].) Thus, we would
expect that the modes in the burst distribution will stem from 40-
and 1500-byte packets; and 1500-byte packets should dominate the
modes in Figure 1a, given that they are almost 40 times bigger. The
40-byte packets broaden the 1500-byte modes, and less common
sizes create the bed of probability under the modes.
How will these modes be reﬂected in passive measurements that
might not see the physical cross trafﬁc? Once the measured ﬂow
reaches a point of congestion—a queue—the idle intervals squeeze
out, and the packets (of both our ﬂow and cross trafﬁc) compress
nearer in time. Thus, provided subsequent links are uncongested, the
interarrival times observed at the receiver are proportional to cross-
trafﬁc burst sizes on the congested link. Since the cross-trafﬁc burst
size PDF contains modes separated by 1500 bytes, we expect the
PDF of interarrival times in a ﬂow to have modes separated by the
transmission time of 1500 bytes at some bottleneck link.
3 CAPACITY ESTIMATION WITH EMG
3.1 Examining an Interarrival PDF
We motivate our work by describing the outcome of a simple ex-
periment. We ﬁrst download a large ﬁle from a machine in CCI-
COM which has a 100 Mb/s access link, to one at CMU which has
a 10 Mb/s access link. Figure 2a shows the interarrival PDF of the
data packets. The distribution shows a single spike at 1.2 ms, the
transmission time of a 1500-byte packet on a 10 Mb/s link. There
is nothing special about this PDF; 10 Mb/s is the minimum capacity
link along the path, and the spike in the PDF shows that most packets
were queued back-to-back.
Next, we repeat the experiment along the reverse path and plot the
 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8  2  2.2  2.4  2.6  2.8  3
Interarrival times in milliseconds
(b) Flow from CMU to CCICOM
Figure 2—Interarrival PDFs for CCICOM–CMU path in both direc-
tions.
interarrival PDF seen at CCICOM in Figure 2. The envelope of the
distribution is again centered near 1.2 ms, because of the upstream
10 Mb/s link; but it is modulated with sharp spikes separated by
equally-spaced mode gaps (EMGs) of 0.12 ms, which is the trans-
mission time of a 1500-byte packet on a 100 Mb/s link.
To understand this PDF, consider what happens as packets go
from CMU to CCICOM. As packets traverse the 10 Mb/s CMU ac-
cess link, they become spaced by 1.2 ms, the transmission time of
one packet on that link. The interarrivals remain relatively unper-
turbed as the packets cross the Internet backbone. Then the packets
reach the 100 Mb/s CCICOM access link, where the ﬂow faces con-
gestion again. There, the spacing of two consecutive packets changes
in one of three ways:
(a) Neither packet is queued (Figure 3a). The interarrival, or the
time between the trailing edges of the two packets, remains 1.2 ms.
(b) Either packet is queued and the queue empties between the de-
parture time of the two packets. Figure 3b shows an example where
the ﬁrst packet arrives while a cross-trafﬁc packet is in the process
of being transmitted. The packet has to wait for that transmission
to ﬁnish, plus any remaining cross-trafﬁc packets in the queue. This
waiting time takes values spread over a wide range, depending on the
total number of bytes that must be transmitted before our packet. If
the second packet is not queued, then the interarrival time becomes
1.2 ms minus the delay of the ﬁrst packet. Interarrival samples of
this type are spread over a wide range with no pronounced values or
modes, and contribute to the bed of probability under the spikes in
Figure 2. A similar argument applies if the second packet is queued
and the ﬁrst is not, or even if the two packets are both queued, as
along as the packets belong to different queuing epochs (the queue
empties between their departures).