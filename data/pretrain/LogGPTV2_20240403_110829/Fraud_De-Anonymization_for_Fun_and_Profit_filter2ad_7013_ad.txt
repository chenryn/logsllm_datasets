tified fraudsters, and also honest accounts. Detego uses a fraud
de-anonymization (FDA) algorithm, e.g., either UODA or DDA to
(
Algorithm 3: Interaction protocol with human fraud workers,
to provide ground truth performance evaluation for fraud de-
anonymization algorithms.
Input
: P; # User study participant;
m, n, q ; # Numbers of accounts
Output:A[]; # Accounts attributed to P;
1 A = P.revealAccounts(m);
2 Data[] = BFS(A, 2);
3 newAccounts[n] = FDA(A, Data);
4 ACAccounts[q] = genAttentionCheckAccounts();
5 Q = genQuestionnaire(newAccounts, ACAccounts);
6 Answers = send(A.randomAccount(), Q);
7 if Answers.passAttentionCheck() then
8
9
10
11 end
12 return A
A.add(newAccounts.getConfirmed());
end
if newAccounts.getConfirmed().verifyOwnership() then
(1) attribute accounts from U to the fraudster profiles in W ∗ (line 4),
and (2) identify the other, non-attributed accounts from U , denoted
by UN . Detego uses the PFD algorithm (line 5) to group the ac-
counts from UN into communities belonging to k new fraudsters. It
then continues to iterate over newly discovered subjects, reviewed
by these new fraudsters or by the previously known fraudsters (line
6), and over newly identified fraudsters, e.g., using the techniques
described in Section 9.
9 FRAUD DE-ANONYMIZATION ORACLES
We leverage the observation that fraud workers know the user
accounts that they control, to introduce a novel approach to vali-
date fraud de-anonymization solutions, that converts human fraud
workers into FDA oracles. In Section 10 we use this approach to
evaluate UODA.
Algorithm 3 outlines our validation protocol, where m, n, q are
integer parameters. The protocol consists of 2 main interaction
steps. In the first step, we ask each participant, i.e., recruited human
fraud worker, to reveal m user accounts that they control in Google
Play, by sending their Google e-mail addresses associated with
these accounts (Algorithm 3, line 1). We then use a depth-2 breath
first search approach to collect (1) all the apps reviewed by the m
accounts and (2) all the reviewers of these apps (line 2). We apply
a fraud de-anonymization solution (see next section) to identify n
new, candidate accounts, i.e., other Google Play accounts suspected
to be controlled by the same participant (line 3).
For the second interaction step, we have designed a questionnaire
that asks the participant to confirm if they control each of these n
candidate accounts, see Figure 3. Specifically, for each account, we
show the account’s profile photo and name, and ask the participant
if they control the account. We provide 3 options, “Yes”, “No” and
“I don’t remember”.
Participant validation. We have developed the following tests to
validate participant attention and honesty:
(1), 12 male and 4 female, who claimed to control between 40 to
500 accounts (M=211, SD=166). We have used these participants to
evaluate the performance of UODA. We have set m=10, n=5 and q=5,
thus each participant reveals 10 accounts controlled in Google Play,
then further confirms or denies control of 5 other UODA detected
accounts, and 5 test accounts. To run UODA, we have used the 10
accounts revealed by each participant in the first step, to collect
(via BFS) 718 apps, 265,724 reviewers and 341,993 reviews in total.
We collected up to 175 apps, 37,056 reviews and 22,848 reviewers
from a single worker. The participation incentive was set to $10 for
each participant.
Ethical considerations. We have developed IRB-approved pro-
tocols to ethically interact with participants and collect data. We
have not asked the participants to post any fraud on the online
service. We restricted the volatile handling of emails and photos
of accounts revealed by participants, to the validation process. We
have immediately discarded them after validation. We believe that
this information cannot be used to personally identify fraudsters:
recruited fraudsters control between 40-500 accounts each (M=211,
SD=166) thus any such account is unlikely to contain PII. Further,
since we do not preserve these emails and photos, their handling
does not fall within the PII definition of NIST SP 800-122. Under
GDPR, the use of emails and photos without context, e.g., name or
personal identification number, is not considered to be “personal
information”.
In the following we first detail the instantiation of UODA that
we evaluated, then describe the results of the user study.
10.1 UODA Parameters
We evaluate UODA (see § 4) using two features, defined by the
sets (1) Cl ≥ = {(s, s′) ∈ Sl
| cr (s, s′) ≥ b1}, where cr (s, s′) is the
number of reviewers shared by subjects s and s′ and (2) Ul ≥ = {s ∈
Sl | ul (s) ≥ b2},where ul (s) is the number of accounts controlled by
worker Wl who has reviewed subject s. Specifically, these features
define the family of sets FWl with m=4:
Ωl1 = {s ∈ Sl | s ∈ Cl ≥ \ Ul ≥}
Ωl2 = {s ∈ Sl | s ∈ Ul ≥ \ Cl ≥}
Ωl3 = {s ∈ Sl | s ∈ Cl ≥ ∩ Ul ≥}
Ωl4 = {s ∈ Sl | s ∈ (Cl ≥ ∪ Ul ≥ )C}
(7)
The rationale behind this selection of Ω sets is that fraudsters are
hired to provide large number of reviews for different subjects. Thus,
a fraudulent account u controlled by a fraudster profile (W , U , S ) ∈
W ∗ is more likely to post reviews for subjects that were reviewed
by other accounts under its control, see e.g. [35, 48, 70, 87].
10.2 Results
Figure 4 shows that 15 of the 16 participants have provided correct
responses to all 5 test accounts. The remaining participant answered
“I don’t remember” for a single test account, known not to be con-
trolled by the participant. We have thus decided to keep the data
from all participants. Further, for participants 2 and 4, UODA found
less than 5 suspected accounts (i.e., 4 and 3 respectively).
We observe that 10 out of 16 participants have confirmed control
(and passed our verification) of all UODA proposed accounts. 5
Figure 3: Anonymized screenshots of 3 questionnaire pages,
for accounts (left) revealed in step 1 to be controlled by the
participant, (center) known not to be controlled, and (right)
suspected by UODA to be controlled by the participant.
Figure 4: Results of UODA on data validated by 16 human
fraud worker participants. UODA achieves an overall preci-
sion of 91%.
• Attention check. In addition to the n candidate accounts, we
add to the questionnaire q other test accounts (line 4), for which we
know the answer: (1) accounts that we know that the participant
controls, i.e., picked randomly from among the m accounts revealed
in the first step, and (2) accounts that we know that the participant
does not control, i.e., accounts that have at least 20 followers and
significant other activities in Google Plus (posting photos, videos).
We present the questions for the n + q candidate and test accounts,
in randomized order (line 5).
• E-mail knowledge. Each Google Play account A has an asso-
ciated e-mail address E. Given E, one can easily retrieve the account
A. However, E is not public, and, given only knowledge of A, one
cannot find E. We leverage this observation to ask each participant
to reveal the e-mail address E of each Google Play account A that
they claim to control. We use E to find the corresponding account
A′. The participant fails this test if A′ does not exist or A′ (cid:44) A.
• E-mail based validation. To verify ownership of claimed
accounts, we send the questionnaire to one of the m e-mail addresses
revealed in the first step (randomly chosen) (line 6).
• Token and e-mail based validation. To verify ownership
of accounts confirmed in the questionnaire (line 8), we choose
randomly one of the n accounts confirmed, and send to its corre-
sponding e-mail address, a random, 6 character token. The accounts
verify iff. the participant can reproduce the token.
10 USER STUDY
We have recruited 16 fraud workers from India (4), Bangladesh (4),
UK (2), Egypt (2), USA (1), Pakistan (1), Indonesia (1), and Morocco
012345678910012345678910111213141516ParticipantNumber of accountsTest − CorrectTest − Don’t RememberCandidate − YesCandidate − NoCandidate − Don’t RememberAlgorithm 4: DeepWalk parameter tuning. For each parameter
set, compute Deepwalk embeddings on the union fraud graph and
run stratified cross validation (SCV) using a learning algorithm
Alд and only seed accounts as part of the training and validation
set (lines 3-5). We save the best performing configuration (lines
6-8).
Input
:CRG # Co-review Graph
S # seed accounts
Alд # learning algorithm
Output: DW Params # Best DeepWalk parameters
1 Fmax = 0, DW Params = ∅
2 ParamSet = Generate.Grid({t, d, γ , w})
3 for p ∈ ParamSet do
D = S ⋉ CRG.DW Features (p)
F = SCV(D, Alд)
if F > Fmax then
DW Params = p
4
5
6
7
8
9
10 end
11 return DW Params
end
Fmax = max{F , Fmax}
participants confirmed control of 4 out of 5 UODA recommended
accounts and 1 participant confirmed control of only 3 accounts
out of 5 UODA recommended accounts. UODA’s precision ( T P
T P +F P ,
where TP is the number of true positives and FP is the number of
false positives) is thus 91%, i.e., 7 unconfirmed accounts among 77
predicted. We note that for 3 out of the 7 unconfirmed accounts,
the participants did not remember if they control them or not.
11 EMPIRICAL EVALUATION
11.1 Attributed Account Data
We have recruited an additional set of 23 fraud workers and per-
formed only the first step of the fraud de-anonymization validation
protocol of § 9, where we asked each participant to reveal at least
15 accounts that they control in Google Play. Figure 5 shows the
number of accounts (bottom, red segments) revealed by each of the
23 workers, between 22 and 86 accounts revealed per worker, for a
total of 942 attributed fraud accounts.
We have selected the top 640 fraud apps, that received the highest
percentage of reviews from accounts controlled by the 23 fraudsters,
and crawled their reviews once every 2 days, over a 6 month period.
The 640 apps had between 7 to 3,889 reviews. Half of these apps
had at least 51% of their reviews written from accounts controlled
by the 23 fraudsters. On the whole, the 640 apps have received
159,469 reviews, of which 17,575 were written from the above 942
attributed fraud accounts.
In the following, we use this data to evaluate the ability of de-
veloped solutions to (1) attribute unknown accounts to existing
seed workers and (2) reveal hidden relationships among reviewers
towards uncovering previously unknown fraudulent workers.
Table 1: Performance of UODA and DDA on ground truth
data set. DDA performs better. However, with only 2 fea-
tures, UODA reaches an F1 of 83%.
Approach Algorithm Precision Recall
F1
UODA
DDA
Top 1
Top 2
Top 3
KNN
MLP
RF
85.11%
92.05%
94.23%
94.28%
94.90%
94.37%
82.59% 83.83%
90.32% 91.11%
92.91% 93.57%
93.35% 93.81%
94.10% 94.50%
93.31% 93.84%
11.2 DeepCluster Parameter Tuning
We have built the union fraud graph over the user accounts who
reviewed the 640 fraud apps. To run DeepWalk, we transform this
union fraud graph into a non-weighted graph, where we replace
an edge between nodes ui and uj with weight wij = w (ui , uj ), by
wij non-weighted edges between ui and uj. This ensures that the
probability of DeepWalk choosing node uj as next hop while at
node ui is proportional to wij. The resulting union fraud graph has
56,950 nodes and 34,742,730 edges (5,858,940 unique edges) and
consists of 202 disconnected components.
Algorithm 4 shows the pseudocode for the grid search process
that we used to identify the best performing DeepWalk parameters
on the union fraud graph: d = 300, t = 100, γ = 80, w = 5. d is the
number of dimensions when representing nodes in the graph, t is
the maximum length of a random walk, γ is the number of random
walks started from each node, and w is the the number of neighbors
used as the context in each iteration of its SkipGram component.
We have used K-means as clustering algorithm in DeepClus-
ter (see § 1) considering that we have prior knowledge about the
number of workers who targeted each subject. We identified the
optimum K value required by K-means for each subject si exper-
imentally, as follows. Iterate for values of K ranging from 2 to
|Wi| where |Wi| is the number of distinct workers known to have
targeted subject si. Since K-means is susceptible to local optima,
we run it 100 times on the embeddings of the co-review graph of
subject si, and assess the quality of the returned clusters. We use
a quasi-F1 score that gages how good a cluster configuration is
with regards to our ground truth. We also adjust for the number of
accounts in each cluster and compute the weighted average across
all clusters in one cluster configuration.
11.3 Fraud De-Anonymization
We compare the ability of the UODA and DDA algorithms to de-
anonymize the ground truth attributed account dataset of § 11.1.
For this, we first set randomly aside 75% of the seed accounts from
each worker into a set GT (Ground Truth) and let the remaining
25% accounts be the TT (Testing Truth) set. For DDA, we train
the co-ownership predictor using accounts in GT , then apply the
predictor to all accounts in TT and extract as features the number
of nodes in each class (known fraudster) to whom the account has
a link according to the co-ownership predictor. Finally, we train a
classifier on these features using stratified 10-fold cross validation.
For UODA, following the GT /TT split, we compute the Ω sets
as described in (7) using accounts in GT and test the algorithm on
Table 2: Performance of our co-ownership predictor cowPred
vs. ELSIEDET [87] on ground truth data. cowPred signifi-
cantly outperforms ELSIEDET.
Solution ML Algo.
cowPred
GBM
RF
SVM
RLR
NB
Elsiedet Grid search
F1