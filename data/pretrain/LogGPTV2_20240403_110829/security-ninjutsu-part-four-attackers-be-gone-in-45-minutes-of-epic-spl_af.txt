Start with whatever base search you want
	This eval gives a single value per day. We could | bucket _time span=1d, rounding the time down, but 	this lets us keep _time accuracy.eventstats is like stats, but without losing the fidelity 	of the data. This will count up, for each host in the 	dataset, how many days of data there is.
Now we can use where (or search) to filter for hosts 	where we have enough baseline.
And finally we can continue with our normal first time seen use case, confident that we are including items that just showed up for the first time.Technique: Confidence Checking
First Time Seen: Do you really want to be confident?
▶ A great question related to first time seen detection confidence, where you can specify how many days of baseline you expect to see, is "should I do that?" For example, if a new host pops up and starts port scanning your environment, that 
may very well be a malicious device.may very well be a malicious device.
▶ Ultimately this depends on what your use case is, and it probably makes sense to 
build out detections specifically targeted to large volumes of new activity. In the following "Variations on First Time Seen" use case we record that activity in a separate index. While I’m not aware of any customer doing this, you could 
probably build out a detection looking for how many new events there are peruser against a role, or what have you. 
Technique: Confidence Checking Variations on First Seen Detection
Check both user and host
▶ tag=authentication 
| eval day=strftime(_time, "%d/%m/%Y") 
| eventstats dc(day) as days_user by user 
| eventstats dc(day) as days_host by host 
| where days_user > 7 AND days_host > 7 
| stats earliest(_time) as earliest latest(_time) as latest by user▶ This allows you to filter out brand new users who log on to many systems, and also brand new hosts (e.g., a new cluster member).
Tracking new users separately
▶ tag=authentication 
	| eval day=strftime(_time, "%d/%m/%Y") 
	| eventstats dc(day) as days_user by user 
	| stats earliest(_time) as earliest latest(_time) as 	latest values(days_user) as days_user by user 	| where earliest > relative_time(now(), "-1d@d") 	| multireport[ | where days_user  7 | collect index=old ]
▶ This allows you to record new users, but funnel 	them separately.
Technique: Confidence Checking 	Time Series Analysis 
▶ The simplest time series analysis is ensuring you have enough days of baseline 	to cause the stdev calculation to be meaningful. 
tag=authenticationtag=authentication
Start with whatever base search you want
| bucket _time span=1d 
| stats dc(dest) as count by user, _time 
| stats count as num_data_samples 
	max(eval(if(_time >= relative_time(now(), 	"-1d@d"), count,null))) as latest 
	avg(eval(if(_time avg + stdev * 3 AND 
	num_data_samples > 7
This is the standard time series behavioral detection use case. But note the count as num_data_samples	- because that is coming after the stats … by user _time, this will count how many days we end up with, 	for each user. If a user only has a few data points, standard deviation is a worthless data point. In some 	scenarios, you would even want to have at least 20 	or 30 data points.In the same breath that we track the average and standard deviations, we can also filter out users that don’t have enough days of baseline. 
Technique: Confidence Checking
Time Series Analysis
▶ Based on observed behavior, standard deviation tends to work the best when you have a use case that naturally has some deviation. For example, number of logon messages per day, or number of pages printed. Usually those will differ from day to day.▶ You’re more likely to see what people think of as noise or false positives in datasets with limited variation, such as number of systems interactively logged into per day (e.g., by sitting at the computer or via remote desktop), or if you have a user who just prints one page now and again. Most users just log into one system per day, so their average is 1, and their stdev is going to be 0 or very close to it, so even setting 6 stdevs above the average might alert for 2 systems.▶ You might decide that you care if someone who usually just logs into one system logs into a second, even if it’s likely 	benign. But you probably don’t care about someone who usually prints one page, suddenly printing two pages.
▶ There are two approaches I have most frequently seen to manage this:
• Static Filters - don’t alert if the # is less than X, or the # increase is less than X.• Relative Filters - only alert if this is 2x their average AND 3 stdev above the average.Technique: Confidence Checking Variations on Time Series
Adding Static Filters
▶ tag=authentication | bucket _time span=1d 
	| stats dc(dest) as count by user, _time 
	| stats count as num_data_samples 
	max(eval(if(_time >= relative_time(now(), 	"-1d@d"), count,null))) as latest 
	avg(eval(if(_time avg + stdev * 3 AND 
	num_data_samples > 7 AND 
	latest > 5 AND (latest - avg) > 5
Adding Relative Filters
▶ tag=authentication | bucket _time span=1d 
	| stats dc(dest) as count by user, _time 
	| stats count as num_data_samples 
	max(eval(if(_time >= relative_time(now(), 	"-1d@d"), count,null))) as latestavg(eval(if(_time avg + stdev * 3 AND 
	num_data_samples > 7 AND 
	latest > avg * 2
Technique: Confidence Checking Time Series Analysis Variation: Signal to Noise Ratio▶ The simplest time series analysis is ensuring you have enough days of baseline 	to cause the stdev calculation to be meaningful. 
tag=authentication
Start with whatever base search you want
| bucket _time span=1d 
| stats dc(dest) as count by user, _time 
| stats count as num_data_samples 
	max(eval(if(_time >= relative_time(now(), 	"-1d@d"), count,null))) as latestavg(eval(if(_time avg + stdev * 3 AND 
	num_data_samples > 7This is the standard time series behavioral detection use case. But note the count as num_data_samples	- because that is coming after the stats … by user _time, this will count how many days we end up with, 	for each user. If a user only has a few data points, standard deviation is a worthless data point. In some 	scenarios, you would even want to have at least 20 	or 30 data points.In the same breath that we track the average and standard deviations, we can also filter out users that don’t have enough days of baseline. 
Technique: Managing Alert Fatigue
Background and Challenges
▶ One of the biggest inherent challenges of the modern security world is alert fatigue. The sheer volume of security alerts that we experience dooms so many SOCs, and we evidence of these problems in many breach debriefs.▶ Fortunately, there are several techniques for dealing with this inside of a Splunk 	world.
Technique: Managing Alert Fatigue Using Risk to aggregate alerts
▶ If you have low confidence alerts, send them just into the risk index in ES (or 	build your own -- | eval risk_object=src_ip | collect index=risk) and aggregate.index=risk earliest=-30d | stats values(source) as search_names sum(risk_score) as thirty_day_risk sum(eval(if(_time > relative_time(now(), "-1d"),risk_score,0))) as one_day_risk by risk_object | eval threshold_1day = 500, threshold_30day = 1200 | eventstats avg(thirty_day_risk) as avg_thirty_day_risk stdev(thirty_day_risk) as stdev_thirty_day_risk| where one_day_risk>threshold_1day OR thirty_day_risk>threshold_30day OR thirty_day_risk>(avg_thirty_day_risk + 3 * stdev_thirty_day_risk) 
| eval risk_score_reason = case(one_day_risk>threshold_1day, "One Day Risk Score above " . threshold_1day, thirty_day_risk>threshold_30day . " on " . strftime(now(), "%m-%d-%Y"), "Thirty Day Risk Score above " . threshold_30day, 1=1, "Thirty Day Risk Score more than three standard deviations above normal (>" . round((avg_thirty_day_risk + 3 * stdev_thirty_day_risk),2) . ")") | fields - avg* stdev*| table risk_object risk_score one_day_risk thirty_day_risk risk_score_reason
See a full description of this search under the "Multi-Scenario Alerts" and "Inline Comments" sections
Technique: Managing Alert Fatigue Using Statistics to Manage Fatigue
▶ Similar to the risk approach, even in your normal ticketing flow you can take high 	priority alerts and bring them to the top of the list by creating meta-notables.tag=ids tag=attack 
| bucket _time span=1d 
| stats count by severity signature dest _time | stats sum(count) as count 
	avg(count) as avg
Start by building up a set of aggregate statistics for 	our dataset.
stdev(count) as stdev 
sum(eval(if(_time > relative_time(now(),  "-1d"), 
count, 0))) as recent_count
min(_time) as earliest 
by severity signature destby severity signature dest
| eventstats avg(avg) as avg_num_per_dest avg(earliest) as avg_earliest sum(count) as sig_wide_count
Use eventstats to add additional context, in this case 	about the IDS Signature
sum(recent_count) as sig_wide_recent_count
	by signature 
| where NOT (avg_earliest 2
▶ More Specific Example:
▶ (index=notable Antivirus OR ids) OR 	(index=proxy category="") 
	| eval dest=case(index="proxy", src, 	index="notable", dest) 
	| stats dc(search_name) as NumRules 	count(eval(index="proxy")) as 	NumUncategorizedHits 
	by dest 
	| where NumRules>1 ANDby dest 
	| where NumRules>1 AND 
	NumUncategorizedHits > 0
Technique: Managing Alert Fatigue
Increase Logging
▶ If you have a mundane alert (e.g., low severity IDS alert, AV successful clean, 	etc.), why not increase logging on that host for a while?▶ With ES, you can use Stream to do network capture, or leverage any other adaptive response actions. With or without ES, you can use your EDR solution. Many customers leverage the Palo Alto Networks app or expect scripts to add suspect hosts to groups that have additional logging. Etc.
▶ Write additional correlation rules based on that increased logging to look for 	higher confidence, higher severity alerts.Technique: Managing Alert Fatigue Leverage Machine Learning
▶ With Machine Learning, you can build extremely powerful models and techniques 	for finding outliers programmatically.
▶ Look at Splunk UBA - this is what they do. 
▶ Look at the ML Toolkit App
Technique: Transaction
Background and Challenges
▶ Being able to group events that are similar is super important.▶ Transaction has a terrible reputation for being slow, because it is slow. But there are scenarios where it is super easy, and we all work in technology because we like to go against conventional wisdom, right?
• Use Transaction for very low event volumes, e.g.: sourcetype=win*security super.exe rare.exe 	executables.exe | transaction host maxpause=10m maxspan=5h• Use Transaction asynchronously to populate a summary index, e.g.: sourcetype=ironport OR 	sourcetype=cisco:esa | transaction MID | fields - _raw | fields {…} | collect index=our_email
▶ Transaction doesn’t fail on by the hour borders (| bucket _time span=1h | stats 	whatever by _time)
Technique: Transaction 
Transaction for low event volumeTransaction for low event volume
▶ When you can filter your incoming event flow to a low volume, even if transaction 	is 10x slower, who cares?
sourcetype=win*security EventCode=4688 [| inputlookup suspicious_processes.csv]
| transaction host 
maxpause=10m 
maxspan=10h
We know that transaction is slow, so the key here is using it for a dataset where you won’t send much data to transaction.Now we can use transaction!
Technique: Transaction 
Transaction with Summary Indexing
▶ Sometimes transaction is *way* easier. Like, "mere mortals don’t have the SPL Skill to use stats + eventstats + streamstats + whatever magic allows you to see your way to the desired result. In this case, embrace the slow. Use transaction asynchronously, and then send the results to a summary index. To help avoid skipped searches, userealtime_schedule=1 in savedsearches.conf. (Check Summary Indexing in this doc.)
sourcetype=ironport OR sourcetype=cisco:esa
| transaction MID ICID …
maxpause=5m 
maxspan=1h