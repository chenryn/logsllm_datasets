title:Differentially Private Bayesian Programming
author:Gilles Barthe and
Gian Pietro Farina and
Marco Gaboardi and
Emilio Jes&apos;us Gallego Arias and
Andy Gordon and
Justin Hsu and
Pierre-Yves Strub
Differentially Private Bayesian Programming
Gilles Barthe, Gian Pietro Farina, Marco Gaboardi, Emilio Jesús Gallego
Arias, Andy Gordon, Justin Hsu, Pierre-Yves Strub
To cite this version:
Gilles Barthe, Gian Pietro Farina, Marco Gaboardi, Emilio Jesús Gallego Arias, Andy Gordon, et
al.. Differentially Private Bayesian Programming. The 23rd ACM Conference on Computer and
Communications Security, Oct 2016, Vienne, Austria. pp.68-79 10.1145/2976749.2978371.
hal-
01446970
https://hal-mines-paristech.archives-ouvertes.fr/hal-01446970
HAL Id: hal-01446970
Submitted on 26 Jan 2017
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
6
1
0
2
g
u
A
7
1
]
L
P
.
s
c
[
2
v
3
8
2
0
0
.
5
0
6
1
:
v
i
X
r
a
Differentially Private Bayesian Programming
Gilles Barthe
IMDEA Software
˚
Marco Gaboardi
University at Buffalo, SUNY
˚
Gian Pietro Farina
University at Buffalo, SUNY
Emilio Jesús Gallego Arias
CRI Mines-ParisTech
:
Andy Gordon
Microsoft Research
Justin Hsu
University of Pennsylvania
Pierre-Yves Strub
IMDEA Software
ABSTRACT
We present PrivInfer, an expressive framework for writing
and verifying diﬀerentially private Bayesian machine learning
algorithms. Programs in PrivInfer are written in a rich func-
tional probabilistic programming language with constructs
for performing Bayesian inference. Then, diﬀerential pri-
vacy of programs is established using a relational reﬁnement
type system, in which reﬁnements on probability types are
indexed by a metric on distributions. Our framework lever-
ages recent developments in Bayesian inference, probabilistic
programming languages, and in relational reﬁnement types.
We demonstrate the expressiveness of PrivInfer by verifying
privacy for several examples of private Bayesian inference.
1.
INTRODUCTION
Diﬀerential privacy [18] is emerging as a gold standard in
data privacy. Its statistical guarantee ensures that the prob-
ability distribution on outputs of a data analysis is almost
the same as the distribution on outputs from a hypothetical
dataset that diﬀers in one individual. A standard way to
ensure diﬀerential privacy is by perturbing the data analy-
sis adding some statistical noise. The magnitude and the
shape of noise must provide a protection to the inﬂuence of
an individual on the result of the analysis, while ensuring
that the algorithm provides useful results. Two properties
of diﬀerential privacy are especially relevant for this work:
(1) composability, (2) the fact that diﬀerential privacy works
well on large datasets, where the presence or absence of an
individual has limited impact. These two properties have led
to the design of tools for diﬀerentially private data analysis.
˚Partially supported by NSF grants CNS-1237235,
:Partially supported by NSF grants #1065060 and
CNS1565365 and by EPSRC grant EP/M022358/1.
#1513694, and a grant from the Simons Foundation
(#360368 to Justin Hsu).
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978371
Many of these tools use programming language techniques to
ensure that the resulting programs are indeed diﬀerentially
private [2–4, 6, 20–22, 30, 34]. Moreover, property (2) has
encouraged the interaction of the diﬀerential privacy commu-
nity with the machine learning community to design privacy-
preserving machine learning techniques, e.g. [12, 19, 26, 40].
At the same time, researchers in probabilistic programming
are exploring programming languages as tools for machine
learning. For example, in Bayesian inference, probabilistic
programming allows data analysts to represent the proba-
bilistic model, its parameters, and the data observations as
a specially crafted program. Given this program as input,
we can then use inference algorithms to produce a distri-
bution over the parameters of the model representing our
updated beliefs on them. Several works have explored the
design of programming languages to compute eﬃciently the
updated beliefs in order to produce eﬃcient and usable tools
for machine learning, e.g. [23, 25, 29, 32, 33, 37].
Recently, research in Bayesian inference and machine
learning has turned to privacy-preserving Bayesian infer-
ence [16, 39, 41, 42], where the observed data is private.
Bayesian inference is a deterministic process, and directly
releasing the posterior distribution would violate diﬀerential
privacy. Hence, researchers have developed techniques to
make Bayesian inference diﬀerentially private. Basic tech-
niques add noise on the input data, or add noise on the result
of the data analysis, while more advanced techniques can en-
sure diﬀerential privacy by taking samples from the posterior
distribution instead of releasing the posterior distribution
explicitly. The diversity of approaches makes Bayesian infer-
ence an attractive target for veriﬁcation tools for diﬀerential
privacy.
In this work we present PrivInfer, a programming frame-
work combining veriﬁcation techniques for diﬀerential privacy
with learning techniques for Bayesian inference in a func-
tional setting. PrivInfer consists of two main components: a
probabilistic functional language for Bayesian inference, and
a relational higher-order type system that can verify diﬀer-
ential privacy for programs written in this language. The
core idea of Bayesian learning is to use conditional distribu-
tions to represent the beliefs updated after some observations.
PrivInfer, similarly to other programming languages for infer-
ence models conditioning on data explicitly. In particular,
we extend the functional language PCF with an observe
statement.
Even though the output of Bayesian inference output is a
probability distribution, it is still a deterministic process. To
guarantee diﬀerential privacy, we must inject some random-
ness into the inference process. To handle these two views on
distributions, PrivInfer distinguishes between symbolic distri-
butions and actual distributions. The former represent the
result of an inference, while the latter are used to represent
random computations, e.g. diﬀerentially private computa-
tions (mechanisms). We parametrize our language with an
algorithm to perform Bayesian inference returning symbolic
distributions, and mechanisms to ensure diﬀerential privacy
returning actual distributions.
Diﬀerential privacy is a probabilistic 2-property, i.e. a
property expressed over pairs of execution traces of the pro-
gram. To address this challenge, we use an approach based on
approximate relational higher-order reﬁnement type system
called HOARe2 [6]. We show how to extend this approach to
deal with the constructions that are needed for Bayesian in-
ference like the observe construct and the distinction between
symbolic and actual distribution.
Another important aspect of the veriﬁcation of diﬀerential
privacy is reasoning about the sensitivity of a data analysis.
This measures the inﬂuence that two databases diﬀering in
one individual can have on the output. Calibrating noise to
sensitivity ensures that the data analysis provides suﬃcient
privacy. In Bayesian inference, the output of the computation
is a distribution (often deﬁned by a few numeric parameters)
for which one can consider diﬀerent measures. A simple
approach is to considered standard metrics (Euclidean, Ham-
ming, etc.) to measure the distance between the parameters.
Another more approach is to consider distances between
distributions, rather than the parameters.
The type system of PrivInfer allows one to reason about the
parameters of a distribution, using standard metrics, but also
about the distribution itself using f -divergences, a class of
probability metrics including some well known examples like
total variation distance, Hellinger distance, KL divergence,
etc. In summary, we extend the approach of PrivInfer in
three directions:
‚ we provide a relational typing rule for observe and for
infer,
‚ we provide a generalization of the relational type sys-
tem of HOARe2 to reason about symbolic and actual
distributions,
‚ we generalize the probability polymonad of HOARe2 to
reason about general f -divergences.
The combination of these three contributions allows us to ad-
dress Bayesian inference, which is not supported by HOARe2.
To illustrate the diﬀerent features of our approach we show
how diﬀerent basic Bayesian data analysis can be guaran-
teed diﬀerentially private in three diﬀerent ways: by adding
noise on the input, by adding noise on the parameters with
sensitivity measured using the (cid:96)1-distance between the pa-
rameters, and ﬁnally by adding noise on the distributions
with sensitivity measured using f -divergences. This shows
that PrivInfer can be used for a diverse set of Bayesian data
analyses. Summing up, the contributions of our work are:
‚ A probabilistic extension PCFp of PCF for Bayesian
inference that serves as the language underlying our
framework PrivInfer (§ 4). This includes an observe
statement as well as primitives for handling symbolic
and actual distributions.
‚ A higher-order approximate relational type system for
reasoning about properties of two runs of programs from
PrivInfer (§ 5). In particular, the type system permits
to reason about f -divergences. The f -divergences can
be used to reason about diﬀerential privacy as well as
about program sensitivity for Bayesian inference. The
relational type system can also reason about symbolic
distributions as well.
‚ We show on several examples how PrivInfer can be
used to reason about diﬀerential privacy (§ 6). We will
explore three ways to guarantee diﬀerential privacy:
by adding noise on the input, by adding noise on the
output parameters based on (cid:96)p-norms, and by adding
noise on the output parameters based on f -divergences.
2. BAYESIAN INFERENCE
Our work is motivated by Bayesian inference, a statisti-
cal method which takes a prior distribution Prpξq over a
parameter ξ and some observed data x, and produces the
posterior distribution Prpξ | xq, an updated version of the
prior distribution. Bayesian inference is based on Bayes’
theorem, which gives a formula for the posterior distribution:
Prpξ | xq “ Prpx | ξq ¨ Prpξq
Prpxq
The expression Prpx | ξq is the likelihood of ξ when x is
observed. This is a function Lxpξq of the parameter ξ for
ﬁxed data x, describing the probability of observing the data
x given a speciﬁc value of the parameter ξ. Since the data
x is considered ﬁxed, the expression Prpxq denotes a nor-
malization constant ensuring that Prpξ | xq is a probability
distribution. The choice of the prior reﬂects the prior knowl-
edge or belief on the parameter ξ before any observation has
been performed. For convenience, the prior and the likeli-