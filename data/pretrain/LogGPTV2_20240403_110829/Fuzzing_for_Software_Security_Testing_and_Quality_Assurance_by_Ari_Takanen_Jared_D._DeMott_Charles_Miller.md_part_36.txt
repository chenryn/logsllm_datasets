testing an obscure or proprietary format, the fuzzer may not be preprogrammed
to understand it.
8.1.3 Sending Inputs to the Target
After the fuzzed inputs have been generated, they need to be fed into the system’s
interfaces. Some fuzzers leave this up to the user. Most supply some method of
performing this job. For network protocols, this may consist of a way to repeat-
edly make connections to a target server, or alternatively, listening on a port for
incoming connections for client-side fuzzing. For file format fuzzing, this may
entail repeatedly launching the target application with the next fuzzed file as
an argument.
8.1.4 Target Monitoring
Once the fuzzing has begun, the target application must be monitored for faults.
After all, what good is creating and sending fuzzed inputs if you don’t know when
they’ve succeeded in causing a problem? Again, some fuzzers leave this up to user.
6760 Book.indb 250 12/22/17 10:50 AM
8.2 Evaluating Fuzzers 251
Other fuzzers offer sophisticated methods of monitoring the target application
and the system on which they are running. The most basic method is to simply run
the target application under a debugger and monitor it for crashes or unhandled
exceptions. More sophisticated monitoring solutions may consist of the fuzzer being
able to telnet or ssh into the target system and monitor the target application, logs
files, system resources, and other parameters. Additionally, the fuzzer may launch
arbitrary user-written monitoring scripts. The more advanced the monitoring being
performed, the more vulnerabilities will be discovered. More advanced monitoring
should also speed up analysis, because it may be able to determine exactly which
input (or set of inputs) caused a particular error condition. Another important
function that a target monitoring service can provide is the ability to automatically
restart the target. This may be as simple as restarting an application or as com-
plicated as power cycling a device or restoring a virtual machine from a snapshot
of a known good state. Such automated monitoring of the target allows for long,
unsupervised fuzzing runs.
8.1.5 Exception Analysis
After the actual inputs have been sent to the target and the dust has cleared, it is
time to figure out if any vulnerabilities have been discovered. Based on the amount
of target monitoring that has taken place, this may require a lot of work on the
part of the user, or it may be pretty much finished. Basically, for each crash or error
condition discovered, the smallest sequence of inputs that can repeat this fault must
be found. After this, it is necessary to partition all the errors to find how many
are unique vulnerabilities. For example, one particular bug in an application, say
a format string vulnerability, may be reachable through many different externally
facing functions. So, many test cases may cause a crash, but in actuality they all
point to the same underlying vulnerability. Again, some fuzzers provide tools to
help do some of this analysis for the user, but oftentimes this is where the analyst
and developers will spend significant amounts of time.
8.1.6 reporting
The final step in the process of fuzzing is to report the findings. For fuzzing con-
ducted during application development, this may involve communicating with the
development team. For fuzzing conducted as part of an outside audit by consultants,
this may be a document produced for the client. Regardless of the intent, some fuzz-
ers can reduce the burden by providing useful statistics, graphs, and even example
code. Some fuzzers can produce small binaries that can be used to reproduce the
errors for use by development teams.
8.2 Evaluating Fuzzers
As the last section discussed, fuzzers can vary from simple input generators to
complex frameworks that perform program monitoring, crash dump analysis, and
reporting. Therefore, it is difficult to compare fuzzers as they offer such a wide
6760 Book.indb 251 12/22/17 10:50 AM
252 Fuzzer Comparison
range of features. We wanted to stay away from a fuzzer feature review or usabil-
ity study. Instead, we will narrow our focus on which fuzzers can create the most
effective fuzzed inputs for a variety of protocols. Even this less ambitious goal is
still difficult. How exactly do you measure which fuzzer generates the most effec-
tive fuzzed inputs?
8.2.1 retrospective Testing
Perhaps the most straightforward approach to evaluating a fuzzer’s effectiveness is
using a testing methodology borrowed from the antivirus world called retrospective
testing. In this form of comparison, a particular time period of testing is selected, say
six months. In this example, fuzzers from six months ago would be archived. Then,
this six-month period would be analyzed closely for any vulnerabilities discovered
in any implementation of a protocol under investigation. Finally, the old versions
of the fuzzers would be used to test against these now-known-flawed implementa-
tions. A record would be kept as to whether these older fuzzers could discover these
now-known vulnerabilities. The longer the retrospective time period used, the more
vulnerabilities will have likely been discovered, and thus the more data would be
available. The reason that old versions of the fuzzers need to be used is that fuzz-
ers may have been updated to look for particular vulnerabilities that have emerged
recently. It is common practice for fuzzers to be tested to see why they failed to find
a particular vulnerability; once the deficiency is identified, they are updated to find
similar types of flaws in the future. An example of this is with the Microsoft .ANI
vulnerability discovered in April 2007. Michael Howard of Microsoft explains how
their fuzzers missed this bug but were consequently improved to catch similar mis-
takes in the future: “The animated cursor code was fuzz-tested extensively per the
SDL requirements, [But] it turns out none of the .ANI fuzz templates had a second
‘anih’ record. This is now addressed, and we are enhancing our fuzzing tools to
make sure they add manipulations that duplicate arbitrary object elements better.”1
Retrospective testing is appealing because it measures whether fuzzers can find
real bugs in real applications. However, there are many serious drawbacks to this
type of testing. The most obvious is that the testing will be conducted on an old
version of the product, in the example above, a version that is six months out of
date. A product can have significant improvements in this time period that will be
missed with this form of testing. Another major deficit is the small amount of data
available. In a given six-month time period, there simply aren’t that many vulner-
abilities announced in major products.
It is hard to draw conclusions in a comparison using such a small data set. This
problem can be mitigated somewhat by increasing the retrospective time period.
However, as we mentioned, increasing this time period also increases the amount of
time the product is behind the state of the art. For these reasons, we did not carry
out this form of testing.
1 blogs.msdn.com/sdl/archive/2007/04/26/lessons-learned-from-the-animated-cursor-securitybug.
aspx.
6760 Book.indb 252 12/22/17 10:50 AM
8.2 Evaluating Fuzzers 253
8.2.2 Simulated Vulnerability Discovery
A method with some of the benefits of retrospective testing but with a larger data
set available is called simulated vulnerability discovery. In this form of testing, a
particular implementation is selected. An experienced vulnerability analyst then goes
in and adds a variety of vulnerabilities to this implementation.2 A different analyst
proceeds to fuzz these flawed implementations in an effort to evaluate how many
of these bugs are rediscovered. It is important to have different analysts conduct
these two portions of the testing to remove any potential conflict in the configura-
tion and setup of the fuzzers.
Simulated vulnerability discovery has the advantage of having as much data
available as desired. Like retrospective testing, it also tests the fuzzer’s ability to
actually find vulnerabilities, even if they are artificial in nature. The biggest criti-
cism is, of course, exactly the artificialness of the bugs. These are not real bugs in
a real application. They will depend heavily on the experiences, knowledge, and
peculiarities of the particular analyst that added them. However, even if the bugs
are artificial, they are all still present in the target application and all the fuzzers
have the same opportunity to find (or miss) them. We utilized this type of testing
and the results are presented later in this chapter.
8.2.3 Code Coverage
Another method of testing the effectiveness of a fuzzer is to measure the amount of
code coverage it achieves within the target application. The application is instru-
mented in such a way that the number of lines executed is recorded. After each
fuzzer runs, this information is gathered, and the number of lines executed can be
examined and compared. While the absolute numbers involved from this metric are
fairly meaningless due to the lack of information regarding the attack surface, their
relative size from each fuzzer should shed some insight into which fuzzers cover
more code and thus have the opportunity to find more bugs.
Code coverage data is relatively straightforward to obtain and analyze. There
are many weaknesses to using it as a metric to compare fuzzers, though. For one,
unlike the other forms of testing discussed, this does not actually measure how
good the fuzzer is at finding bugs. Instead, it is a proxy metric that is actually
being used to measure how much of the target application was not tested. It is up
for debate whether high levels of code coverage by a fuzzer indicate it was effective.
Just because a line is executed by the fuzzer doesn’t necessarily mean it is tested. A
prime example is standard nonsecurity regression tests. These will get good code
coverage, but they are not doing a good job of fuzzing. However, it is certainly the
case that those lines of code not executed by the fuzzer containing vulnerabilities
will not be discovered by that fuzzer. We use this form of comparison as well and
consider later how it helps to validate our simulated vulnerability discovery testing.
2 Thanks to Jake Honoroff of Independent Security Evaluators for adding the vulnerabilities in this
study.
6760 Book.indb 253 12/22/17 10:50 AM
254 Fuzzer Comparison
8.2.4 Caveats
Please keep in mind that these results may not necessarily reflect which fuzzer is
right for a particular project. For example, sometimes funding will limit your choice
to open-source fuzzers. Perhaps it is difficult to monitor the application or there is
little follow-up time available. In this case, it may be better to use a fuzzer that is
slightly less effective at generating test cases but has strong monitoring and post-
analysis tools. Also, whether you are fuzzing a common protocol or an obscure or
proprietary protocol will have an impact on your choice because some commercial
fuzzers cannot handle these situations. Finally, this comparison only takes place
over a few protocols and relies heavily on the types and placements of the vulner-
abilities added for the simulated vulnerability discovery. That said, we feel the
results are valid and the consistent results we obtain from the two methods used
for comparison validate each other.
8.3 Introducing the Fuzzers
For this testing, we selected a variety of open-source and commercial fuzzers. Some
are similar in design to one another and others are quite different. While you may
not choose to use one of these exact fuzzers, hopefully you can learn by comparing
the types of fuzzers and their demonstrated strengths and weaknesses as you build
or choose the fuzzer you will eventually use.
8.3.1 GpF
The General Purpose Fuzzer (GPF) is an open-source fuzzer written by one of the
authors of this book. It is a mutation-based network fuzzer that can fuzz server
or client applications. It has many modes of operation, but primarily works from
a packet capture. A valid packet capture needs to be obtained between the target
application and its server or client. At this point, GPF will continuously add anoma-
lies to the packets captured and replay them at the target application. GPF parses
the packets and attempts to add the anomalies in as intelligent a way as possible. It
does this through the use of tokAids. tokAids are implemented by writing C pars-
ing code and compiling it directly into GPF. Using built-in functions, they describe
the protocol, including such features as length fields, the location of ASCII strings,
and the location and types of other delimiters. There are many prebuilt tokAids
for common protocols available in GPF. There are also generic ASCII and binary
tokAids for protocols GPF does not understand and that users don’t wish to imple-
ment themselves.
There is one additional mode of operation of GPF called SuperGPF. This mode
only works against servers and only with ASCII-based protocols. It takes as an argu-
ment a file with many anchors from the protocol. For example, against SMTP the
file might contain terms like “HELO,” “MAIL FROM,” “RCPT TO,” and so forth.
GPF then modifies the initial packet capture file and injects many of these protocol-
specific terms into the initial packet exchange on disk. It then launches a number
of standard GPF processes using these modified packet captures as its initial input.
6760 Book.indb 254 12/22/17 10:50 AM
8.3 Introducing the Fuzzers 255
GPF does not have any monitoring of analysis features.
8.3.2 TAOF
The Art of Fuzzing (TAOF) is an open-source, mutation-based, network fuzzer
written in Python. It, too, works from an initial packet capture. However, unlike
GPF, instead of giving a file that contains a packet capture, TAOF captures pack-
ets between the client and server by acting as a man-in-the-middle. These captured
packets are then displayed to the user. TAOF doesn’t have knowledge of any proto-
cols. Instead, it presents the packets to the user in a GUI, and the user must dissect
the packets and inform TAOF of the packet structure, including length fields. The
amount of work in doing this is comparable to writing a tokAid in GPF and can
take several hours (or more). The types of anomalies that will be injected into the
packets are also configurable. One drawback of TAOF is that in its current imple-
mentation, it cannot handle length fields within another length field. The result is
that many binary protocols cannot be fuzzed, including any based on ASN.1.
TAOF does not have any monitoring or analysis features.
Abstract Syntax Notation One (ASN.1)
Abstract Syntax Notation One (ASN.1) is a formal language for abstractly
describing messages to be exchanged among an extensive range of applications.
There are many ways of using ASN.1 to encode arbitrary data, but the simplest
is called Basic Encoding Rules (BER). Data normally comes with an identifier, a
length field, the actual data, and sometimes an octet that denotes the end of the
data. Of course, these ASN.1 values can be nested arbitrarily, which can make for a
complicated parsing algorithm.
Programs that implement ASN.1 parsers have a long history of security
vulnerabilities. Microsoft’s ASN.1 libraries have had critical bugs more than once.
The open-source standard OpenSSL has also had critical security vulnerabilities.
In 2002, an ASN.1 vulnerability within the SNMP protocol affected over fifty
companies including Microsoft, Nokia, and Cisco. There were very few internet-
connected systems that were not vulnerable. The irony is that ASN.1 is used in
security protocols such as Kerberos and in security certificates.
8.3.3 proxyFuzz
ProxyFuzz is exactly what it claims it is—a proxy server that fuzzes traffic. It is
incredibly easy to set up and use. Simply proxy live traffic between a client and
server through ProxyFuzz and it will inject random faults into the live traffic. Proxy-
Fuzz does not understand any protocols and no protocol specific knowledge can
be included with it (without making fundamental changes to its design). While it is
easy to set up and use, its lack of protocol knowledge may hinder its effectiveness.
ProxyFuzz does not have any monitoring or analysis features.
6760 Book.indb 255 12/22/17 10:50 AM
256 Fuzzer Comparison
8.3.4 Mu-4000
The Mu-4000 is an appliance-based fuzzer from Mu Dynamics (acquired by Spirent).3
It is a generation-based fuzzer that at the time of testing understood approximately
55 different protocols. It is placed on the same network as the target system and
configured and controlled via a Web browser. Within the protocols that it under-
stands, it is extremely easy to use and is highly configurable. Options such as which
test cases are sent at which timing periods can be precisely controlled. Furthermore,
the Mu-4000 can be used as a pure proxy to send test cases from other fuzzers in
order to use its monitoring functions, but otherwise cannot learn or be taught new
or proprietary protocols. In other words, unlike the open-source fuzzers discussed
above, which still work on proprietary protocols, albeit less effectively, the Mu-4000
can only be used against protocols it understands. Another drawback is that, the
Mu-4000 can only be used to fuzz servers and cannot fuzz client-side applications.
One of the strengths of the Mu platform is its sophisticated monitoring abilities.
It can be configured to ssh into the target machine and monitor the target process,
system and application logs, and system resources. It can restart the application
when the need arises and report exactly what fault has occurred. Another feature
is that, when an error does occur in the target, it will replay the inputs until it has
narrowed down exactly which input (or inputs) caused the problem.
8.3.5 Codenomicon Defensics
Defensics is a generation-based fuzzer from Codenomicon Ltd. which was acquired
by Synopsys in 2015.4 At the time of the analysis, it had support for over 130 pro-
tocols. As is the case with the Mu-4000, it had no ability to fuzz any protocols for
which it does not already have support, although later Defensics traffic capture
mutators and the new SDK features (see Chapter 7) fix that shortcoming. It can be
used to fuzz servers, clients, and even applications that process files. It is executed
and controlled through a graphical Java application.
Defensics can be configured to send a valid input between fuzzed inputs and
compare the response to those in the past. In this way it can detect some critical
behavioral faults such as the Heartbleed bug where the SUT replied with memory
contents when fuzzed. It can also run custom external monitoring scripts. However,
at the time of the analysis it didn’t have any built-in monitoring or analysis features.
8.3.6 beSTOrM
beSTORM from Beyond Security is another commercial fuzzer that can handle
network or file fuzzing.5 At the time of the examination, it contained support for
almost 50 protocols. However, unlike the other commercial offerings, it could be
used for fuzzing of proprietary and unsupported protocols. This is done through a
3 Based on private communications, the Mu Dynamics fuzzer product is discontinued at Spirent and has
been replaced with CyberFlood. Read more at https://www.spirent.com/Newsroom/Press_Releases/
Releases/2016/August/Spirent-Introduces-CyberFlood.
4 https://www.synopsys.com/software-integrity/products/intelligent-fuzz-testing.html.
5 http://www.beyondsecurity.com/bestorm.html.
6760 Book.indb 256 12/22/17 10:50 AM
8.4 The Targets 257
GUI interface similar, but more sophisticated than, that found in TAOF. A network
packet capture, or in the case of file fuzzing, a file, is loaded into beSTORM. This
valid file can then be manually dissected. Alternatively, beSTORM has the ability
to automatically analyze the valid file and determine significant occurrences such
as length fields, ASCII text, and delimiters. Once the unknown protocol is under-
stood by beSTORM, it then fuzzes it using a large library of heuristics. beSTORM
also supports the ability to describe a protocol specification completely in XML.
The beSTORM fuzzer also possesses sophisticated monitoring capabilities. It
can remotely talk to a monitoring tool that, at the very least, monitors the target for
crashes or exceptions. Using this knowledge, this information can be passed back to
the fuzzer to help determine exactly what input caused an error in the application.
8.3.7 Application-Specific Fuzzers
When possible, we included protocol-specific fuzzers in the evaluation. This includes
FTPfuzz, a GUI-based FTP fuzzer, and the PROTOS SNMP test suite. The PROTOS6