ηmax
t dt
(7)
0
0
The numbers we report in this paper are discrete approxima-
tions to these values via numerical integration with a ﬁxed
subinterval width of 0.01.
Roughly speaking, a larger value for ηmin suggests that
information leaks from the procedure for more secret values,
and a larger value for ηmax suggests that more information
leaks from the procedure about secret values.4 To relate these
measures to another used previously in the QIF literature,
namely min-entropy (e.g., [42], [43]), in Fig. 1 we show ηmin
and ηmax in comparison to the min-entropy of S(‘secret’), for
our idealized setting above. Fig. 1(a) shows that ηmin reﬂects
the growth of |C| just as min-entropy can, and similarly,
Fig. 1(b) shows that ηmax reﬂects changes in w like min-
entropy can. However, min-entropy does not distinguish be-
tween these types of leakage. Mutual entropy (e.g., [19], [22],
[29]) also reﬂects increasing leakage as |C| grows in Fig. 1(a)
and as w shrinks in Fig. 1(b), though its sensitivity to these
effects is limited, particularly that of increasing |C|, until |C|
becomes quite large (Fig. 1(a)).
4While these rules of thumb are accurate when Jn has no valley, they are
less reliable when it does. In such cases, a more reliable understanding can
be obtained by examining the graph of Jn directly, or at least by computing a
separate ηmin and ηmax for each valley-free segment of Jn. Here, by “valley”
we mean values n, n(cid:2)
, Jn > Jn+1, Jn(cid:2) 
0, outputs a random value in [0, M − 1] if the candidate
password is equal to the secret password and random value
in [M, M + 16] otherwise. Intuitively, the leakage of this
procedure should be the same as a deterministic password
checker and independent of the value of M . However, as
shown in Fig. 2, the use of randomness here results in an
unintuitive result, since Jn (Fig. 2(b)) is sensitive to the value
of M . As such, while our detector does accurately detect
leakage in this case, it provides less help in comparing the
leakage of two randomized implementations.
Another problem may arise when other inputs are allowed
in I. Consider the example
proc (C, I, S)
O(‘result’) ← ((S(‘secret’) > C(‘test’)) ? 1 : 0)
⊕ ((I(‘other’) ≤ 0) ? 1 : 0)
return O
Here, the expression “cond ? 1 : 0” evaluates to 1 if cond
is true and 0 otherwise, and “⊕” represents XOR. This
procedure indicates that S(‘secret’) > C(‘test’) by returning 0
if I(‘other’) ≤ 0 or by returning 1 if I(‘other’) > 0. Because
our technique allows for any value of I(‘other’) consistent with
Πproc when estimating |YS|, it will compute Jn = 0 for any
n, suggesting no leakage. However, the only condition under
which proc in fact leaks no information is if I(‘other’) is non-
positive or positive with equal probability from the adversary’s
perspective.
2) An alternative measure: To overcome the limitations
of Jn as illustrated above,
in this section we propose a
leakage measure that
is more robust for procedures that
employ randomness or inputs in I. For convenience, here
we treat all values generated at random within the procedure
instead as inputs represented in I; e.g., the ﬁrst invocation of
rand() within the procedure is replaced with a reference to,
say, I(‘rand[1]’), the second with I(‘rand[2]’), and so forth.
Intuitively, our measure employs an alternative deﬁnition for
YS that also includes these additional inputs. Speciﬁcally,
consider the set
(cid:3)(cid:3) (cid:4)C, O, I(cid:5) ∈ XS ∧ (cid:4)C, O(cid:5) ∈ YS ∩ YS (cid:2)
(cid:4)C, O, I(cid:5)
ˆXS,S (cid:2) =
(cid:2)
(cid:4)
(cid:5)
of (cid:4)C, O, I(cid:5) triples such that not only is (cid:4)C, O(cid:5) ∈ YS ∩ YS (cid:2)
(c.f., the deﬁnition of J(S, S(cid:2)) in (1)), but also the triple
is consistent with some s ∈ S (i.e., (cid:4)C, O, I(cid:5) ∈ XS where
XS =
the
various random values (represented in I) become exposed in
ˆXS,S (cid:2) and the number of these values for a given (cid:4)C, O(cid:5) pair
s∈S Xs). By counting such (cid:4)C, O, I(cid:5) triples,
act as the “weight” of that pair. We adjust the denominator
similarly, resulting in the measure
(cid:3)(cid:3)(cid:3)
(cid:3)(cid:3)(cid:3) ˆXS,S (cid:2)
(cid:3)(cid:3)XS ∪ XS (cid:2)
(cid:3)(cid:3)
ˆJ(S, S(cid:2)
) = 1 −
ˆJn =
avg
S, S (cid:2) :|S| =
(cid:2)(cid:2)S (cid:2)
(cid:2)(cid:2) = n
∧ S ∩ S (cid:2) = ∅
ˆJ(S, S(cid:2)
)
(8)
Note that if VarsI = ∅, then ˆJn = Jn since in this case,
(cid:4)C, O(cid:5) ∈ YS if and only if (cid:4)C, O, ∅(cid:5) ∈ XS.
The beneﬁt of ˆJn is that it is far less susceptible to the
variability that was demonstrated in Sec. III-B1. For example,
Fig. 2(c) shows that this measure is stable, independent of
M . As we will see in subsequent sections, however, it is also
considerably costlier to estimate.
When we use ˆJn in place of Jn, we will annotate measures
derived from it using similar notation. For example, ˆηmin
denotes ηmin computed using ˆJn in place of Jn, and similarly
for ˆηmax.
IV. IMPLEMENTATION
In this section, we discuss our implementation for com-
puting the measures discussed in the previous section. Fig. 3
shows the overall workﬂow for doing so. At the core of our
implementation is a hash-based model counting technique that
is discussed in Sec. IV-A–IV-C. In Sec. IV-D, we present an
adaptation for generating logical postconditions for multiple
rounds of procedure executions. In Appendix A, we discuss
the use of symbolic execution (e.g., [44], [45]) for generating
postconditions, with a focus on a particular optimization that
proved useful for our case studies in Sec. VI.
A. Hash-based model counting for Jn
To calculate Jn, we need to estimate
for randomly selected, disjoint sets S and S(cid:2)
, or S(cid:2)(cid:2) = S ∪ S(cid:2)
(i.e., S(cid:2)(cid:2) = S, S(cid:2)(cid:2) = S(cid:2)
for speciﬁed sets
). In this section,
to estimate Jn it sufﬁces to estimate
S(cid:2)(cid:2)
we provide two optimizations for producing such estimates.
1) Estimating |YS|: Our ﬁrst optimization is an adapta-
tion of the approximate model counting technique due to
Chakraborty et al. [4], which leverages a family of 3-wise
independent hash functions to estimate the number #F of sat-
isfying assignments of a conjunctive-normal-form proposition
F of v variables and that runs in fully polynomial time with
respect to a SAT oracle. At a high level, this algorithm itera-
tively selects a random hash function H b : {0, 1}v → {0, 1}b
from the family (where b changes per iteration) and a random
p ∈ {0, 1}b
, and computes the satisfying assignments for F
for which the hash of the assignment (a string in {0, 1}v
) is p.
(Intuitively, this number should be about a #F/2b
.) Through
(cid:3)(cid:3)YS ∪ YS (cid:2)
(cid:3)(cid:3)
(cid:3)(cid:3)YS ∩ YS (cid:2)
(cid:3)(cid:3)YS ∪ YS (cid:2)
size n. Since
(cid:3)(cid:3)
(cid:3)(cid:3) =
= |YS| +
(cid:3)(cid:3)YS∪S (cid:2)
(cid:3)(cid:3)YS (cid:2)
(cid:3)(cid:3) ,
(cid:3)(cid:3) −
(cid:3)(cid:3)YS ∪ YS (cid:2)
(cid:3)(cid:3)
(cid:3)(cid:3)YS (cid:2)(cid:2)
(cid:3)(cid:3)
(cid:3)(cid:3)YS ∩ YS (cid:2)
(cid:3)(cid:3)
and
and
of
(9)
(10)
518
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:39:23 UTC from IEEE Xplore.  Restrictions apply. 
proc (C, I, S)
if (S(‘secret’) = C(‘test’))
O(‘result’) ← rand () mod M
else
O(‘result’) ← M + (rand () mod 16)
return O
n
J
0.8
0.6
0.4
0.2
0
0
M =1
M =4
M =8
M =16
M =64
M =210
4
8
12
16
log2 n
20
24
28
32
0.4
n
ˆJ
0.2
0
0
M =1
M =4
M =8
M =16
M =64
M =210
4
8
12
16
log2 n
20
24
28
32
(a) Procedure
(b) Jn for various n and M
(c) ˆJn for various n and M
Fig. 2: An example showing limitations of J on procedures with randomness and improvements offered by ˆJ (see Sec. III-B)
Πproc
Symbolic Execution
(Appendix A)
Multi-execution
Composition
(Sec. IV-D)
Iterate
(Sec. IV-C)
O
proc
C
I S
=
|S|
2
= 2
Counting for n = 1
Sample S and S(cid:3)
(Sec. IV-A2)
Hash-based Counting
(Sec. IV-A1)
Compute Jn,
ηmin and ηmax
(Sec. III-A)
Fig. 3: Workﬂow of evaluating leakage, from left to right: label the different types of inputs and outputs; generate postconditions
Πproc using symbolic execution; optionally, compose multi-execution constraints; perform model counting for different sizes
of n; and generate our leakage measures
(cid:8)
judicious management of this iterative process, the algorithm
arrives at an estimate ˜#F for #F that satisﬁes
−1 · #F ≤ ˜#F ≤ (1 + ) · #F
(1 + )
≥ δ
P
(cid:9)
where error , 0  α
(cid:3)(cid:3)ZS, ˆp
(cid:3)(cid:3) ≤ α and
|YS| ≈ 2
b ·
(cid:3)(cid:3)ZS,p
(11)
To reach an estimate of conﬁdence δ, we generate a number
of (cid:4)b, p, ˆp(cid:5) triples such that
(12)
, ˆp ∈ {0, 1}b−1, and α is derived from
where p ∈ {0, 1}b
 [4]. Each such triple individually provides an estimate that
is within error  with conﬁdence at least 0.78 [4, Lemma 1],
and the median of the estimates for all such triples is within
error  with conﬁdence that can be increased arbitrarily with
more (cid:4)b, p, ˆp(cid:5) such triples. As a special case, if
b = 0, then
is an exact count of |YS | since ZS,p = YS .
of Expected Size n: A second expense of
calculating YS and YS (cid:2) explicitly is in enumerating S and S(cid:2)
themselves, especially if n is large. We can leverage hashing
similarly to the method above to avoid enumerating S and
(cid:3)(cid:3) ≤ α at
2) Sampling S, S(cid:2)
(cid:3)(cid:3)ZS,p
(cid:3)(cid:3)ZS,p
(cid:3)(cid:3)
X 0
p =
(cid:4)C, O, I(cid:5)
(cid:2)
(cid:2)
(cid:2)
S(cid:2)
directly for n = |S| /2b
estimate Jn for n = |S| /2b
at random and, for each such selection, deﬁne
for some b ≥ 0. Speciﬁcally, to
and p ∈ {0, 1}b−1
, we select H b
(cid:4)C, O, I(cid:5)
(cid:4)C, O, I(cid:5)
X 1
p =
Xp =
where H b−1 denotes the function H b
most bit from the output. Then, we use the sets
(S) = p||0
(S) = p||1
(cid:3)(cid:3) ∃S : Πproc(C, O, I, S) ∧ H b
(cid:3)(cid:3) ∃S : Πproc(C, O, I, S) ∧ H b
(cid:3)(cid:3) ∃S : Πproc(C, O, I, S) ∧ H b−1(S) = p
(cid:2)
(cid:2)
(cid:2)
(cid:3)(cid:3) ∃I : (cid:4)C, O, I(cid:5) ∈ X 0
(cid:3)(cid:3) ∃I : (cid:4)C, O, I(cid:5) ∈ X 1
(cid:3)(cid:3) ∃I : (cid:4)C, O, I(cid:5) ∈ Xp
(cid:4)