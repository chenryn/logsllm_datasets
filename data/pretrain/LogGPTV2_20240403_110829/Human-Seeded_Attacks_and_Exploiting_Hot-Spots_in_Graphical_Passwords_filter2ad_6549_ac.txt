s
b
ee
m
m
a
p-gr
n
ic
faces
c
d
o
c
arties
ns
o
v
ers
p
c
b
tea
to
cars
ys
p
a
p
Figure 3: Security measures for each image (in bits). C V
is based on data from lab user study of 32–40 passwords (de-
pending on image). For comparison to a uniform distribution,
(1) marks H(X) for C U, and (2) marks E(f (X)) for C U.
passwords that are not repeatable, and thus we suspect it
would have lower relative security in practice.
Overall, one can conclude that image choice can have
a signiﬁcant impact on the resulting security, and that de-
veloping reliable methods to ﬁlter out images that are the
most susceptible to hot-spotting would be an interesting
avenue for future research.
We used these formal measures to make an informed
decision on which images to use for our ﬁeld study. Our
goal was to give the PassPoints scheme the best chance
(in terms of anticipated security) we could, by using one
image (cars) that showed the least amount of clustering
(with the best user success in creating a password), and
also using another that ranked in the middle (pool).
4 Field Study and Harvesting Attacks
Here we describe a 7-week or longer (depending on the
user), university-approved ﬁeld study of 223 user ac-
counts on two different background images. We col-
lected click-based graphical password data to evaluate
the security of this style of graphical passwords against
various attacks. As discussed, we use the entropy and
expected guesses measures from our lab study to choose
two images that would apparently offer different levels
of security (although both are highly detailed): pool and
cars. The pool image had a medium amount of cluster-
ing (cf. Fig. 3), while the cars image had nearly the least
amount of clustering. Both images had a low number of
skips in the lab study, indicating that they did not cause
problems for users with password creation.
EXPERIMENTAL DETAILS. We implemented a web-
based version of PassPoints, used by three ﬁrst-year un-
dergraduate classes: two were ﬁrst year courses for com-
puter science students, while the third was a ﬁrst year
course for non-computer science students enrolled in a
science degree. The students used the system for at least
7 weeks to gain access to their course notes, tutorials,
and assignment solutions. For comparison with previ-
ous usability studies on the subject, and our lab study,
we used an image size of 451 × 331 pixels. After the
user entered their username and course, the screen dis-
played their background image and a small black square
above the image to indicate their tolerance square size.
For about half of users (for each image), a 19 × 19 T-
region was used, and for the other half, a 13 × 13 T-
region.2 The system enforced that each password had to
be 5 clicks and that no click-point could be within t = 9
pixels of another (vertically and horizontally). To com-
plete initial password creation, a user had to successfully
conﬁrm their password once. After initial creation, users
were permitted to reset their password at any time using
a previously set secret question and answer.
Users were permitted to login from any machine
(home, school, or other), and were provided an online
FAQ and help. The users were asked that they keep
in mind that their click-points are a password, and that
while they will need to pick points they can remember,
not to pick points that someone else will be able to guess.
Each class was also provided a brief overview of the
system, explaining that their click-points in subsequent
logins must be within the tolerance shown by a small
square above the background image, and that the input
order matters. We only use the ﬁnal passwords created
by each user that were demonstrated as successfully re-
called at least one subsequent time (i.e., at least once af-
ter the initial create and conﬁrm). We also only use data
from 223 out of 378 accounts that we would consider, as
this was the number that provided the required consent.
These 223 user accounts map to 189 distinct users as 34
users in our study belonged to two classes; all but one
of these users were assigned a different image for each
account, and both accounts for a given user were set to
have the same error tolerance. Of the 223 user accounts,
114 used pool and 109 used cars as a background image.
4.1 Field Study Hot Spots and Relation to
Lab Study Results
Here we present the clustering results from the ﬁeld
study, and compare results to those on the same two im-
ages from the lab study. Fig. 4b shows that the areas
that were emerging as hot-spots from the lab study (re-
call Fig. 2a) were also popular in the ﬁeld study, but other
clusters also began to emerge. Fig. 4b shows that even
our “best” image from the lab study (in terms of apparent
resistance to clustering) also exhibits a clustering effect
after gathering 109 passwords. Table 2 provides a closer
USENIX Association
16th USENIX Security Symposium
109
examination of the clustering effect observed.
Image Size of most popular clusters # clusters
of size
Name
≥ 5
# 1 # 2 # 3 # 4
# 5
cars
pool
26
35
25
30
24
30
22
27
22
27
32
28
Table 2: Most popular clusters (ﬁeld study).
These values show that on pool, there were 5 points
that 24-31% of users chose as part of their password. On
cars, there were 5 points that 20-24% of users chose as
part of their password. The clustering on the cars im-
age indicates that even highly detailed images with many
possible choices have hot spots. Indeed, we were sur-
prised to see a set of points that were this popular, given
the small amount of observed clustering on this image
from our smaller lab study.
The prediction intervals calculated from our lab study
(recall Section 3) provide reasonable predictions of what
we observed in the ﬁeld study. For cars, the prediction
intervals for 3 out of the 4 popular clusters were correct.
For pool, the prediction intervals for 8 out of the 9 popu-
lar clusters were correct. The anomalous cluster on cars
was still quite popular (chosen by 12% of users), but the
lower end of the lab study’s prediction interval for this
cluster was 20%. The anomalous cluster on pool was
also still quite popular (chosen by 18% of users), but the
lower end of the lab study’s prediction interval for this
cluster was 19%.
These clustering results (and their close relationship
to the lab study’s results) indicate that the points chosen
from the lab study should provide a reasonably close ap-
proximation of those chosen in the ﬁeld. This motivates
our attacks based on the click-points harvested from the
lab study.
4.2 Harvesting Attacks: Method & Results
We hypothesized that due to the clustering effect we ob-
served in the lab study, human-seeded attacks based on
data harvested from other users might prove a successful
attack strategy against click-based graphical passwords.
Here we describe our method of creating these attacks,
and our results are presented below.
Table 3 provides the results of applying various at-
tack dictionaries based on our harvested data, and their
success rates when applied to our ﬁeld study’s password
database.3
u is a dictionary composed of all 5-permutations
of click-points collected from u users. Note C R
u bit-
size is a slight overestimate, as there are some combi-
nations of points that would not constitute a valid pass-
word, due to two or more points being within t = 9 pix-
els of each other.
If this were taken into account, our
C R
u
attacks would be slightly better. In our lab study, u = 33
for cars, and u = 35 for pool. Thus, the size of C R
for cars is P (165, 5) = 236.7 entries, and for pool is
P (175, 5) = 237.1 entries. C V
u is a dictionary composed
of all 5-permutations of the clusters calculated (using the
method described in Section 3.1) from the click-points
from u users. Thus, the alphabet size (and overall size)
for C V
u is smaller under the same number of users than in
a corresponding C R
u dictionary. Note that all of these dic-
tionary sets can be computed on-the-ﬂy from base data as
necessary, and thus need not be stored.
Table 3 illustrates the efﬁcacy of seeding a dictionary
with a small number of user’s click-points. The most
striking result shown is that initial password choices har-
vested from 15 users, in a setting where long term re-
call is not required, can be used to generate (on average)
27% of user passwords for pool (see C R
15). As we ex-
pected, cars was not as easily attacked as pool; more user
passwords are required to seed a dictionary that achieves
similar success rates (see C R
25).
We also tried these attacks using a small set of ﬁeld
study user passwords to seed an attack against the re-
maining ﬁeld study user passwords. The result, in Ta-
ble 4, shows a difference between the lab study and the
ﬁeld study (ﬁnal) passwords; however, there remains suf-
ﬁcient similarity between the two groups to launch ef-
fective attacks using the lab-harvested data. One pos-
sible reason for the differences in user choice between
the two studies is that the ﬁeld study users may not have
been as motivated as the lab study users to create “dif-
ﬁcult to guess” graphical passwords. It is unclear how
a user might measure whether they are creating a graph-
ical password that is difﬁcult to guess, and whether in
trying, if users would actually change their password’s
strength; one study [36] shows that only 40% of users
actually change the complexity of their text passwords
according to the security of the site. Another equally
possible explanation might be that the lab study users
chose more difﬁcult passwords than they would have in
practice, as they were aware there was no requirement
for long term recall, and also did not have a chance to
forget and subsequently reset their passwords to some-
thing more memorable. With our current data, it is not
clear whether we can conclusively determine a reason
for these differences.
Next we examined the effect of click-order patterns
as one method to capture a user’s association between
points, and reduce our dictionary sizes. For each image,
we select one dictionary to optimize with click-order pat-
terns. This dictionary is one of the ten randomly selected
C V
subsets that were averaged (results of this average
are in Table 3). We selected the dictionary whose guess-
ing success was closest to the average reported in Table
3. The success rate that these dictionaries achieve (be-
110
16th USENIX Security Symposium
USENIX Association
(a) cars (originally from [5]).
(b) pool (originally from [46, 47]).
Figure 4: Observed clustering (ﬁeld study). Halo diameter is 5× the number of underlying clicks.
Set
m bitsize
25
25
C R
u
C V
u
C R
C V
C R
C V
C R
C V
20
20
15
15
165
104
125
85
100
72
75
56
36.7
33.4
34.7
31.9
33.1
30.6
30.9
28.8
cars (u = 33)
# passwords
max
†
†
min
†
†
guessed out of 109
avg
37(34%)
22(20%)
24(22%)
21(19%)
22(20%)
17(16%)
14(13%)
12(11%)
9(8%)
7(6%)
8(7%)
8(7%)
4(4%)
4(4%)
35(32%)
27(25%)
32(29%)
30(28%)
25(23%)
24(22%)
m bitsize
175
77
125
59
100
52
75
41
37.1
31.1
34.7
29.2
33.1
28.2
30.9
26.4
pool (u = 35)
# passwords
min
†
†
guessed out of 114
avg
59(52%)
41(36%)
42(37%)
34(29%)
35(31%)
28(25%)
30(27%)
26(23%)
29(25%)
19(17%)
24(21%)
18(16%)
20(18%)
14(12%)
max
†
†
56(49%)
47(41%)
55(48%)
43(38%)
45(39%)
43(38%)
Table 3: Dictionary attacks using different sets. All subsets of users (after the ﬁrst two rows) are the result of 10 randomly selected
subsets of u short-term study user passwords. For rows 1 and 2, note that u = 33 and 35. m is the alphabet size, which deﬁnes
the dictionary bitsize. See text for descriptions of C V and C R. †The ﬁrst two rows use all data from the short-term study to seed a
single dictionary, and as such, there are no average, max, or min values to report.
fore applying click-order patterns) is provided in the ﬁrst
row of Table 5.
We hypothesized that many users will choose pass-
words in one (or a combination) of six simple click-
order patterns: right to left (RL), left to right (LR), top to
bottom (TB), bottom to top (BT), clockwise (CW), and
counter-clockwise (CCW). Diagonal (DIAG) is a com-
bination of a consistent vertical and horizontal direction
(e.g., both LR and TB). Note that straight lines also fall
into this category; for example, when (xi, yi) is a hor-
izontal and vertical pixel coordinate, the rule for LR is
(x1 ≤ x2 ≤ x3 ≤ x4 ≤ x5), so a vertical line of
points would satisfy this constraint.We apply our base