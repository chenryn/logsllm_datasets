We only consider the paths whose length is at least four. Thus, we remove
some nodes from G(cid:48) when they are only included in the paths longer than four.
Fig. 2 shows a simple example of the graph. The reasons why we used the
subgraph G(cid:48) are as follows:
Fig. 2. A simple example of the graph when the distance is three.
– Analyzing the relation between the receiver and the sender is the most im-
portant task in this work. We do not need an entire network graph.
– The social network is huge. Twitter has about 190 million users. Thus, we
cannot handle the whole social network.
– We use both the followings of the receiver and the followers of the sender to
reduce crawling data. If we only use the receiver’s followings, the amount of
the crawling data will increase exponentially.
– We only analyze the user pairs whose distance is at least four. As noted
above, the messages coming from a distance greater than four are mostly
spam. Moreover, Kwak et al. showed that 70.5% of user pairs have paths
whose length is four or shorter in the Twitter network [22]. Thus, our research
covers most cases in Twitter.
3.2 Features
Spammers have diﬀerent characteristics from non-spammers. Our design is based
on an insight similar to the one used by Sybil series [20, 21]. In general, spammers
are diﬃcult to make relationships with non-spammers but they make a group
with other spammers. Spam groups have only a few attack edges to honest
regions. Thus, most non-spammers are not connected with spammers, or have
long and weak connections. Based on these facts, we identify spammers using
the distance and the connectivity between users.
Distance. We measure distance, which is the length of the shortest path be-
tween users. It is the same as the number of hops from a message receiver to a
message sender. In Twitter, an out-edge is following, meaning the follower trusts
the following. We examined the correlation between the distance and spammers.
To investigate the distributions of spam and non-spam messages according to
distance, we randomly selected 10,000 benign and an equal number of spam
messages from our data set (see Fig. 3). Within a distance of two, only 0.9%
stFig. 3. The percentages of benign (blue) and spam messages (red).
messages are spam. However, 57.3% of the messages coming from a distance of
three are spam and 89% of the messages coming from a distance of four are
spam. From the result, most spam comes from users at a distance of more than
three hops from receivers but there are also many benign messages at a distance
of three or four hops. The connectivity feature discriminates between benign and
spam messages that have arrived from the same distance.
Connectivity. The connectivity represents the strength of a connection. A
simple way to measure connectivity would be counting the number of paths. More
paths mean more friends are connected to the user. A better way to measure
connectivity is counting the edge-independent paths. The collection of paths
is called edge-independent if no two paths share an edge. We used Menger’s
theorem which characterizes that connectivity of a graph in terms of the number
of independent paths between nodes [23, 24]. Menger’s theorem deﬁnes edge-
connectivity as follows:
Theorem 1 (Menger’s theorem). Let G be a ﬁnite undirected graph and u
and v be two distinct nodes. The size of the minimum edge cut for u and v is
the same as the maximum number of the edge-independent paths from u to v.
This is a special case of the Max-ﬂow min-cut theorem. The problem of ﬁnding
the maximum number of the edge-independent paths can be transformed to a
maxﬂow problem by constructing a directed graph assigning each edge with
unit capacity. We compare the min-cut size when both nodes s and t are non-
spammers, and when a node s is a non-spammer and a node t is a spammer. As
expected, the min-cut sizes of the spammer’s cases are smaller than that of the
normal cases.
We also use random walk as another measure. Yu et al. used a special kind
of random walk to identify sybil nodes, not exactly same as random walks [20,
21]. We used random walk technique used in PageRank [25]. The idea behind
PageRank is that when a random surfer visits pages inﬁnitely, the pages linked
more are visited more. PageRank values are computed by the left eigenvectors
xL of the transition probability matrix P such that
0%20%40%60%80%100%1234over 4Percentage of messagesDistancexLP = λxL,
where λ is eigenvalue. The N entries in the eigenvector xL are the steady-state
probabilities of the random walk corresponding to the PageRank values of web
pages. The Perron-Frobenius Theorem tell us that the largest eigenvalue of the
matrix is equal to one which is the principal eigenvector [26, 27]. Thus, the
principal eigenvector of the transition matrix P is the PageRank values. We
used this PageRank values. The web pages are corresponding to the users and
the links are corresponding to the friendships. Because we use the specialized
graph only including the nodes and edges in the paths from the node s to the
node t, the expected result of random walk is diﬀerent from general graphs. All
edges point toward the node t. Thus the eigenvector of the node t is always
top. Therefore, we convert the directed graph G(cid:48) to the undirected graph G(cid:48)(cid:48)
replacing all directed to undirected edges. Now, both the nodes t and s have
very high values in their eigenvector because the graph G(cid:48)(cid:48) is created by making
backward-edges of existing edges. All random walks will proceed to both nodes t
and s in normal cases. When the node t is a spammer, however, the eigenvector
of the node t will not be as high as the node s because the spammer only has a
few edges.
4 Experiments and Evaluation
This section is composed of three parts. In the ﬁrst part, we present how we
collected data used in our experiments. In the second part, we show the spam
detection results using the user relation features. In the last part, we show that
the user relation feature can be represented as a user account feature to decide
whether an account is a spam account or not. And we compare the results using
only the account features used in the previous work and the results using the
account features including the new one to detect spammers.
4.1 Data collection
Twitter oﬀers API methods for data collection to encourage third-party devel-
opers, but there is a rate limit [28]. A host is permitted 150 requests per hour.
Twitter also had a whitelist for developers but they stopped oﬀering this whitelist
on March 2011 [29]. In order to overcome the rate limit we used four servers and
120 IP addresses. The servers changed their IP addresses when they were stopped
by the rate limit. The collection lasted for about two month from February to
March 2011. We crawled 148,371 proﬁles, 267,551 tweets, 4,317,161 user’s follow-
ings and 963,181 user’s followers. We randomly selected non-spammers by using
numerical Twitter user IDs. Spam accounts were selected from among the re-
ported accounts to the “@spam” account, which is the oﬃcial Twitter account.
Legitimate Twitter users can report the spam accounts by mentioning to the
“@spam” account; thus, we searched mentions using the “@spam” keyword and
collected spam accounts from the search results. We manually checked whether
each account is a spammer or not. In total, we collected 308 spam accounts and
10,000 spam messages.
Table 1. The results of classiﬁcation using distance and random walk
Classiﬁers True Positive (%) False Positive (%)
Bagging
LibSVM
FT
J48
BayesNet
93.3
93.2
93.1
92.3
92.0
8.5
8.3
7.7
8.7
8.0
4.2 Spam Classiﬁcation
In the previous section, we proposed a spam ﬁltering using user relation features.
We identiﬁed spam using distance and connectivity features. Connectivity is
measured in two ways: random walk and min-cut. First, we used the results of
random walk with the distance. Given a graph G(cid:48)(cid:48), which is explained in Section
3, the result of random walk is the left eigenvector xL of the transition matrix of
G(cid:48)(cid:48). Let i be the index of a receiver and j be the index of a sender in xL. Then,
their random walk values are xL[i] and xL[j], respectively. When the sender is a
non-spammer, xL[i] and xL[j] are similar values and they are quite higher than
the average value of xL. When the sender is a spammer, however, xL[j] is much
lower than xL[i]. Therefore, we use the ratio xL[j]/xL[i] as a feature from random
walk. We randomly selected 5,000 messages where both senders and receivers are
non-spammer, and 5,000 messages where senders are spammers and receivers are
non-spammers from the data set. Then we constructed graphs for each user pair.
On average, the graphs have about 5,000 nodes. We used Weka [30], which is a
data mining tool, and used 10-fold cross validation option in classiﬁcation . In K-
fold cross validation, the sample data is randomly partitioned into k subgroups.
Only one partitioned data is used as validation data and the remaining k − 1
partitioned data are used as training data. This process is then repeated k times
in order to use all k subgroups as the validation data. Table 1 shows the results
of applying each classiﬁer. True positive means that spam messages are correctly
classiﬁed as spam, which is 1 - false negative. False positive means that normal
messages are classiﬁed as spam. All classiﬁers successfully identify spammers
with about 92% true positive. Fig. 4 shows a decision tree created by the J48
classiﬁer. The decision tree is simple, meaning that if the system uses the distance
and random walk features, the system can easily identify the spammers.
Next, we selected 3,000 messages where both senders and receivers are non-
spammer, and 3,000 messages where senders are non-spammer and receivers
are spammer from the data set. The messages are classiﬁed using the results
of min-cut and the distance. Finally, both results of random walk and min-
cut were used with the distance in classiﬁcations at the same time. Table 2
and Table 3 show the results of the classiﬁcations. The classiﬁers also identify
spammers with high accuracy when they only use the distance and min-cut
results. In addition, the accuracy increases when the classiﬁers use the distance,
the random walks and the min-cuts at the same time. From our experiments, we
showed that we can identify spam using only relation information. This means
Fig. 4. A decision tree created by the J48 classiﬁer
Classiﬁers
Bagging
LibSVM
J48
BayesNet
FT
True
Positive
(%)
False
Positive
(%)
94.6
94.0
93.9
93.5
93.5
6.5
5.8
5.3
5.5
5.5
Classiﬁers
Bagging
LibSVM
J48
FT
BayesNet
True
Positive
(%)
False
Positive
(%)
95.1
94.3
94.2
93.8
93.4
4.7
4.3
4.6
4.4
5.9
Table 2. The results of the classiﬁ-
cation using the distance and min-
cut
Table 3. The results of the classi-
ﬁcation using the distance, random
walk and min-cut
that our system can allow clients to decide whether or not received messages
are spam in real-time. Fig. 5 shows Receiver Operating Characteristic (ROC)
curves of classiﬁcation results. When we use random walk and min-cut along
with distance, the classiﬁcation accuracy becomes better than when we use only
distance.
4.3 Spam account detection with including a user relation feature
We consider that if we can include user relation related feature in the user
account proﬁle it would be easier to detect spam accounts.
One feature we consider is the ratio of mentions sent to non-followers. The
distance of the messages sent to the followers is one. Non-spammers generally
send messages to their followers or followings. On the other hand, spammers
send messages to arbitrary users who are mostly located at a distance greater
than one.
Normalsender/receiversender/receiversender/receiverDistanceNormalSpamNormalSpamSpamNormal=1=2=3=40.020.556.04Fig. 5. ROC curves for each of the relation features.
We reproduced previous work’s experiments related to detecting spam ac-
counts in order to show that the results with adding our feature are better than
those with only features used in previous work. The 11 features that are used in
classiﬁcations are as follows:
– The standard deviation of tweeting interval
– The ratio of tweets containing URLs
– The ratio of mentions containing URLs
– The ratio of tweets containing hashtags
– The ratio of mentions (
– The ratio of duplicate tweets
– Reputation (
– The number of lists including the user
– Age (the current time - the account creation time)