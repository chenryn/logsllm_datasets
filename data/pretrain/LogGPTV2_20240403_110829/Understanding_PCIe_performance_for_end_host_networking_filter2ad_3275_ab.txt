As part of pcie-bench we have developed a model of the
PCIe specification [47, 57]. The model allows us to: (1) vali-
date bandwidth measurements from micro-benchmarks against
expected bandwidth based on the specification, and (2) model
the impact of more complex device/host interactions, such
as shown in Figure 1 with the NIC models.
PCIe uses a high-speed serial interconnect based on a
point-to-point topology consisting of several serial links (or
lanes) between endpoints. It is a protocol with three lay-
ers: physical, data link and transaction. While the data link
layer (DLL) implements error correction, flow control and
acknowledgments, the transaction layer turns user applica-
tion data, or completion data, into PCIe transactions using
Transaction Layer Packets (TLPs).
Most available 40Gb/s NICs today have a PCIe Gen 3 in-
terface with 8 lanes. Each lane offers 8 GT/s (Giga Trans-
actions per second) using a 128b/130b encoding, resulting
in 8 × 7.87 Gb/s = 62.96 Gb/s at the physical layer. The
DLL adds around 8–10% of overheads due to flow control
and acknowledgment messages, leaving around 57.88 Gb/s
available at the TLP layer5. For each transaction, the physical
layer adds 2B of framing and the DLL adds a 6B header.
At the transport layer, the standard defines a number of
different TLP types. For the purpose of this paper, the rele-
vant packet types are: Memory Read (MRd), Memory Write
(MWr), and Completion with Data (CplD). TLPs have a com-
mon header, a type specific header, and an optional trailing
digest. The common header contains information, such as
the type of TLP and the TLP length, and is 4B in size. The
header of MWr and MRd TLPs is 12B long (assuming 64bit
addressing) while for CplD TLPs it is 8B. The maximum
amount of data carried in a single TLP (Maximum Payload
Size, or MPS) is negotiated between the peers and the op-
tional 4B digest contains an end-to-end CRC (ECRC). Typical
values for the MPS are 256B or 512B.
5The DLL overhead depends on the transaction rate and implementation
details. The value of 57.88 Gb/s was derived using recommended values
from the PCIe specification [47].
330
PCIe MWr transactions are simple posted transactions and
the number of bytes transferred by a DMA write of size sz
can be calculated with:
Btx = ⌈sz/MPS⌉ × MW r_Hdr + sz
(1)
where MW r_Hdr is 24B (2B framing, 6B DLL header, 4B
TLP header, and 12B MWr header). In contrast, PCIe mem-
ory reads (MRd), e.g., a device reading data from the host,
require two TLP types: MRd TLPs and CplD TLPs. A MRd
TLP is sent to request data from the peer, then the data is
returned via one or more CplD TLPs. Thus, PCIe memory
reads consume bandwidth in both directions. A MRd request
may only request data up a certain amount (Maximum Read
Request Size, or MRRS), negotiated between the peers. A
typical value for the MRRS is 512B. The data returned via
CplD TLPs is bounded by the MPS. The number of bytes
consumed for a DMA read of size sz is then:
Btx = ⌈sz/MRRS⌉ × MRd_Hdr + sz
Br x = ⌈sz/MPS⌉ × CplD_Hdr + sz
with MRd_Hdr being 24B and CPL_Hdr 20B.
(2)
(3)
The PCIe configuration of a device, e.g., PCIe Gen 3 device
with 8 lanes (x8), provides us with the available bandwidth
and the values for MPS and MRRS. With the formula above,
we can then calculate the effective bandwidth for different
transfer sizes sz. The graph Effective PCIe BW in Figure 1
was calculated using this model assuming a PCIe Gen 3 x8 de-
vice, MPS = 256B, and MRRS = 512B using 64bit addressing.
The saw-tooth pattern shows the overhead of the additional
DLL/TLP headers for every MPS bytes of data, with the over-
head being higher for smaller transfer sizes. The graph also
shows the impact of MRRS as the additional MRd TLPs con-
sume bandwidth which otherwise would be usable by MWr
transactions.
The model also allows us to calculate the overhead for
more complex device/host interactions. For example, for the
graph of the Simple NIC in Figure 1 we calculate the PCIe
bandwidth used both for packet send (TX) and receive (RX)
of a simple NIC. For TX, the driver writes updates the TX
queue tail pointer on the device (4B PCIe write). The device
then DMAs the descriptor (16B PCIe read) and, subsequently,
the packet buffer. After transmission, the device generates an
interrupt (4B PCIe write) and the driver reads the TX queue
head pointer (4B PCIe read). For RX, the driver updates the
RX queue tail pointer to enqueue a buffer on the freelist (4B
PCIe write). The device then DMAs the freelist descriptor
(16B PCIe read), DMAs the packet (PCIe write) and the RX
descriptor (16B PCIe write) to the host and generates an
interrupt (4B PCIe write). Finally, the driver reads the RX
queue head pointer (4B PCIe read). The model then calculates
the overhead for each of these transactions and computes
the achievable bandwidth.
Understanding PCIe performance for end host networking
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
The NIC model described above is very simplistic and all
modern NICs deploy a variety of optimizations. For example,
the Intel Niantic NIC [19] DMAs batches of up to 40 TX
descriptors from the host and may write batches up to 8
TX descriptors back to the host. Optimized NIC (kernel
driver) in Figure 1 shows the performance impact of these
optimizations over the simple NIC model. Throughput can be
further improved by driver changes, as shown by Optimized
NIC (DPDK driver) in the same figure. The Intel DPDK
driver configures the device differently: no interrupts are
generated and the driver does not read device registers to
determine if packets have been received/transmitted6. The
graphs for the optimized NIC show that even a moderate
amount of optimization on both the device and the driver
side can significantly improve the achievable throughput.
Our PCIe model is not limited to calculating achievable
throughput for NICs. It can equally be used to programmat-
ically model any PCIe device, provided that some details,
typically obtainable from the device driver or the data sheet,
are available. Furthermore, the model can and has been used
to quickly assess the impact of alternatives when designing
custom NIC functionality. The model has some limitations:
Some of the lower level PCIe overheads, in particular for flow
control messages are only estimated based on the PCIe spec-
ification and the model slightly overestimates their impact.
Furthermore, the model does not account for PCIe overheads
of unaligned DMA reads. For these, the specification requires
the first CplD to align the remaining CplDs to an advertised
Read Completion Boundary (RCB, typically 64B) and un-
aligned PCIe reads may generate additional TLPs.
4 THE PCIE-BENCH METHODOLOGY
While the PCIe protocol stack is relatively easy to model, it
is the implementation of the DMA engine and, in particular,
the increased complexity of the PCIe root complex, discussed
in §2, which makes it hard to evaluate the end to end PCIe
performance in real systems. Therefore, we designed a set
of PCIe micro-benchmarks in the spirit of lmbench [39] and
hbench:OS [5]. The main idea is to perform individual PCIe
operations from a device to a buffer in host memory while
carefully controlling the parameters which may affect per-
formance.
Figure 3 illustrates the host buffer setup and the param-
eters which determine the access from a PCIe device. We
define a (logically) contiguous buffer on the host. The buffer
may be contiguous in DMA address space or comprised of a
number of smaller buffers. It must be significantly larger than
the size of the Last Level Cache (LLC), because, on some ar-
chitectures, the PCIe root complex interfaces with the cache
6The DPDK driver polls on descriptor queues in host memory instead,
checking for the valid fields indicating that a descriptor has been updated.
Figure 3: Host buffer layout and access parameters
system of the CPUs. To measure cache effects, only a subset
of the host buffer, the window size, is accessed repeatedly.
For a given micro-benchmark we access the window size
of the host buffer with multiple DMA requests and keep the
amount of data transferred with each request fixed (transfer
size). A DMA may start at an offset from a host cache line,
allowing us to determine if there are any penalties for un-
aligned accesses. To ensure that each DMA request touches
the same number of cache lines, the window is divided into
equal sized units. Each unit is the sum of offset and transfer
size, rounded up to the next cache line. A micro-benchmark
can typically be configured to access the units sequentially
or randomly (access pattern), though for most experiments
in this paper we use random access. For each benchmark,
the state of the CPU cache needs to be carefully controlled.
By default, the cache is thrashed before every benchmark to
ensure a cold cache. Optionally, we may attempt to warm
the cache from the host, by writing to the window (host
warm), or from the device. The latter is achieved by issuing a
number of write DMAs to the window (device warm), before
running tests. Understanding the impact of the host cache
system is important: when transmitting packets from the
host, typically, at least the packet header would be cache res-
ident, and when receiving from the network, the packet data
may get DMAed to non-cache resident buffers, depending
on the overall pressure on the caches. Finally, the locality
of the host buffer needs to be controlled. In SMP systems
with integrated PCIe and memory controller, the entire host
buffer is either allocated local to the node the PCIe device is
attached to, or to a different (non-local) node.
4.1 Latency benchmarks
The first set of the PCIe micro-benchmarks measures the
latency of individual PCIe operations. Measuring the latency
of a PCIe Memory Read (MRd) transactions (from the device)
is relatively easy on most programmable hardware: take a
timestamp before issuing a DMA Read and another times-
tamp when the completion of the DMA Read is signaled by
the DMA engine, then log the time-difference. DMA Read
latency benchmarks are labeled LAT_RD throughout.
331
Transaction 0Transaction 1Host BufferWindow sizeCachelineWindowOffsetTransfer sizeUnit sizeTransaction 2SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Neugebauer et al.
The latency of DMA Memory Writes can not be mea-
sured directly from the device as PCIe Memory Write (MWr)
transactions are posted: they are sent without explicit ac-
knowledgment of success. Instead, we indirectly measure
the latency of writes by issuing a DMA Write followed by
a DMA Read from the same address. PCIe ordering ensures
that the PCIe root complex handles the Read after the Write.
Note, this approach does not allow us to compute the latency
of DMA Writes alone as the latency of the DMA Read may be
affected by the preceding DMA Write. The results from these
benchmarks are labeled LAT_WRRD. For latency benchmarks
we record the latency for each transaction and then calculate
the average, median, min, max, 95th, and 99th percentile.
Latency measurements allow us to assess the cost of cache
and IO-TLB misses, or the added latency for accessing re-
mote memory from a PCIe device. While we can not measure
the latency of a DMA Write directly, the latency of a DMA
Write followed by a DMA Read to the same address pro-
vides insights into technologies, such as DDIO. Measuring
latency includes the device overheads, such as the cost of
issuing DMA requests or receiving notifications of comple-
tions. Therefore, when looking at the impact of the PCIe root
complex, it is more insightful to study the relative change in
latencies rather than the absolute latency values.
As highlighted in Section 2, latency measurements are
of particular importance when writing software for pro-
grammable NICs or when implementing DMA engines in
(reconfigurable) logic: the measured latency in Figure 2 by
far exceeds the inter-packet times for small packets. Mea-
sured latency and its variance determines how many in-flight
DMAs need to be handled.
4.2 Bandwidth benchmarks
The second set of PCIe micro-benchmarks focuses on band-
width. DMA bandwidth is measured by taking a timestamp
at the start, issuing a significant number of DMAs and taking
a timestamp at the end. The bandwidth can be calculated by
the time difference and the amount of data transferred. We
are interested in straight DMA Read and DMA Write band-
width (labeled BW_RD and BW_WR respectively). To measure
bi-directional bandwidth we issue alternating DMA Read and
Write transactions (labeled BW_RDWR). This ensures that PCIe
MRd TLPs compete with PCIe MWr TLPs for the bandwidth
to the root complex.
Bandwidth benchmarks, especially with small transfer
sizes and random access patterns, generate a significant load
on the system. If the DMA engine can saturate the PCIe
link with 64 byte transfers, this would generate around 69.5
million transactions per second in each direction with the
root complex handling a transaction every 5ns. Therefore,
bandwidth micro-benchmarks can expose limitations in the
332
root complex implementation as well as stressing the imple-
mentation of a device’s DMA engines.
5 IMPLEMENTATIONS
The pcie-bench methodology requires programmatic and
fine grained control over the PCIe device’s DMA engines.
We have implemented the methodology on both commer-
cial boards from Netronome and research oriented NetF-
PGA boards. Two independent implementations of the same
methodology validate our performance measurements against
different host architectures and provide direct insights into
two different PCIe implementations.
5.1 Netronome NFP implementations
Netronome offers a number of highly programmable NICs
based on the NFP-6000 and NFP-4000 Ethernet controllers
[44, 45, 59]. These Ethernet controllers feature up to 120
eight-way multi-threaded Flow Processing Cores (FPCs), a
hierarchical memory subsystem, and fully programmable
PCIe interfaces, all interconnected via a distributed switch
fabric. The PCIe subsystem exposes two interfaces to the
FPCs: A command interface allowing the core to issue small
PCIe read and write transactions directly from registers and
a bulk DMA interface allowing FPCs to en-queue large trans-
fers between host memory and memory on the NFP. The
PCIe micro-benchmarks are implemented as firmware on
the FPCs utilizing both interfaces. The micro-benchmarks
work both on NFP-4000 and NFP-6000 based controllers.
Firmware. The full PCIe micro-benchmark suite is imple-
mented in a single firmware image. It provides a simple
control interface, allowing a user space program on the host
to select which type of benchmark to run and provide the
necessary configuration parameters, such as the host buffer
location, transfer size etc. Benchmark results are written to
NFP memory where they can be read back from the host.
Latency tests are run on a single thread in one of the FPCs.
This thread calculates the next host address to use and pre-
pares a DMA descriptor. The thread then reads the current
value of the internal timestamp counter and enqueues the
DMA descriptor to the DMA engine. The DMA engine sig-
nals the thread once the DMA completed and the thread
takes another timestamp and journals the difference to mem-
ory on the NFP. A variant of the latency benchmarks uses the
direct PCIe command interface, which is suitable for small
transfer (up to 128 bytes). With this variant, instead of build-
ing and enqueuing a DMA descriptor, the thread can directly
issue the PCIe read or write commands and gets signaled
on completion. The timestamp counter increments every 16
clock cycles, which, on a 1.2 GHz NFP, provides a resolution
of 19.2ns. For latency benchmarks we typically journal the
timing for 2 million transactions.
Understanding PCIe performance for end host networking
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
For DMA bandwidth tests, the challenge is to ensure that
DMA descriptors are enqueued to the DMA engine at a
higher rate than it drains the queue. To achieve this for
small transfer sizes, we use 12 cores with 8 threads as DMA
workers. In a loop, each DMA worker thread calculates a
host address, prepares and enqueues a DMA descriptor and