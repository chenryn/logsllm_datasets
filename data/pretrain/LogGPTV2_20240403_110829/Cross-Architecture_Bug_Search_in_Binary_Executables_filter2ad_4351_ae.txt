versions. In any case, this experiment estimates a baseline for
the best quality that we can expect in the cross-architecture
setting.
In the second experiment, we took two binaries of the
same program (BusyBox v1.21.1 and OpenSSL v1.0.1f)
compiled for different architectures. Figure 7 shows that we
can rank about 32.4% of the functions in BusyBox (ARM
to x86) at rank 1, 46.6% in the top 10 and 62.8% in the
top 100. For OpenSSL (ARM to MIPS) we even reach
32.1% for rank 1, 56.1% in the top 10 and 80.0% in the
top 100. Bad rankings mainly resulted from partial structural
differences in the binaries. For example, the MIPS version of
BusyBox has 65 more functions, which suggests different
inlining, and almost 13% more basic blocks, which alters the
CFG of many functions. Our metric is sensitive to the CFG
and the segmentation of the basic block, which we found to
be potentially problematic especially for smaller functions.
However, the chance that these less-complex functions contain
vulnerabilities is also signiﬁcantly lower [26].
Still, for the majority of cases, Figure 7 shows that our se-
mantic basic block hashes actually provide a reasonable degree
of similarity across architectures. In many cases, the CFGs are
sufﬁciently similar across architectures to allow for meaningful
comparisons, which means that both the CFG structures and
717717
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:33 UTC from IEEE Xplore.  Restrictions apply. 
the separation of basic blocks are largely preserved.
We do not consider function matching as the primary use
case of our approach. Our metric punishes non-matched basic
blocks in the signature, but not in the target function. Thus, a
good match for a small part of the function is ranked higher
than a mediocre, but exhaustive match. However, even with
this metric, the results could be improved, e. g., by ﬁxing
good function matches and thereby successively eliminating
candidates for other function matches.
B. False/True Positives Across Compilers/Code Optimization
The previous example has shown that function matching
is possible even if binaries are compiled for different archi-
tectures. Next, we systematically evaluate how the choice of
compiler and optimization level inﬂuence the accuracy of our
algorithm. We chose the largest coreutils programs (mea-
sured at O2, which is the default optimization level, e. g., in
Debian and for automake). As opposed to all other experiments
in this paper, we compiled these programs ourselves on x86
with three different compilers (gcc v4.62, gcc v4.81 and
clang v3.0) and in four optimization levels (O0-O3).
True Positives: In a ﬁrst experiment, we aim to system-
atically evaluate the true positive rate in the cross-compiler
scenario. To this end, for each program, we compared all
12 binaries with each other (i.e., all 144 binary pairs) using
function-wise matching. For example, we compare cp com-
piled with clang in O1 with cp compiled with gcc v4.62 in
O2. Similarly to the previous experiment, we show the ratio of
correct function matches. However, in contrast to the previous
experiments, the resulting 9 ∗ 144 = 1296 pairs have to be
visualized very densely, making individual CDFs impractical.
Thus, in this experiment (and in this experiment only), we
visualize a match as a true positive, if it is among the top 10
matches.
Figure 8 illustrates the results of this experiment
in a
matrix. Each dot in the nine cells illustrates the algorithm’s
accuracy, given one concrete pair of binaries of the same
program. The twelve sub-columns and sub-rows per cell are
divided as follows: columns/rows 1-4 are clang in O0-O3,
columns/rows 5-8 are gcc v4.62 at O0-O3, and columns/rows
9-12 represent gcc v4.81 at O0-O3. The darker a dot, the more
function pairs matched correctly (100% is black). Discussing
each dot in Figure 8 is not possible due to space constraints,
but we can generalize the following observations from this
experiment: (1) The search results are symmetric, i.e., they do
not signiﬁcantly change if we, e.g., search from gcc binaries
to clang binaries or vise versa. This is good, as the direction
in which a search must be made is generally unknown. (2)
Comparing programs compiled for O0 (i.e., no optimization)
to binaries with any other optimization level signiﬁcantly
weakens accuracy. Luckily, programs are rarely compiled
without any optimization in practice. (3) Binaries compiled
by the different gcc versions have a higher similarity to each
other than binaries created with different compilers. While
cross-compiler results (i.e. clang vs. gcc) are worse than
intra-compiler results, they still provide meaningful rankings.
(4) Comparing binaries across different optimization levels
(O1-O3) is typically possible with high accuracy. That is,
more advanced optimization strategies introduced in later
optimization levels (O2 and O3) do not severely harm the
overall performance of our system.
False Positives: In a second experiment, we aim to measure
false positives produced by our algorithm. In principle, the
algorithm returns a list of functions with their similarities to
the bug signature. As such, it is not straightforward to judge
if the highest-ranked function is indeed a match, or is just the
function that was least different from the bug signature. Thus,
in general, evaluating false positives in our system is inherently
difﬁcult (if not impossible). Having said this, we acknowledge
that
there should be such an evaluation to illustrate the
accuracy of our system. Thus, and only for the purpose of this
experiment, we deﬁne a metric which judges if the highest-
ranked function is a match or not. We chose to consider the
highest-ranked potential function match as an actual match
if its similarity is at least 0.1 higher than the second-ranked
match (an admittedly arbitrarily-chosen threshold on a scale
from 0–1). The intuition behind this heuristic is as follows:
If no function matches, all functions will be more or less
equally different, i.e., their similarity scores are more densely
connected. However, an analyst might be misled by a highly
ranked function with a low similarity score, if that function
stands out from the others—that is, if its similarity is high in
contrast to the next-best match. Again, note that we use this
threshold for this false positive experiment only. It is not an
integral part of our system and is also not used in the other
experiments (see also the discussion in Section V).
To this end, we focused on the nine coreutils programs
in a single, common compiler setting. We chose to use gcc
due to its popularity (in v4.62) and the optimization level
O2 (again, since it is often the default optimization level).
We then compared each program with every other program,
resulting in 81 program comparisons. Again, we tried to match
all functions in program A with all functions in program B.
Figure 9 illustrates the similarity of two different programs.
For each program, we compute the ratio of functions that
have been supposedly erroneously matched with the other
program. We excluded functions from the matrix that had the
same names in two coreutils programs (i.e., code sharing
between programs), as they would bias the evaluation. The
cells represent the ratio of function matches, i.e., highest-
ranked functions whose similarity score is signiﬁcantly (the
0.1 threshold) higher than the second-ranked function. That is,
on the diagonal line, in which we actually do expect matches,
dark cells represent high true positive rates. On the contrary,
in all other cells, where we do not expect matches, dark cells
represent high false positive rates. The ﬁgure shows that even
if we use such a crude mechanism to judge un actual matches,
the false positive rate of our system is fairly low. We suffered
seemingly many false positives when comparing functions of
ls and dir. In fact, these programs are almost identical,
leading to many (correct) matches. Since those programs
had the same optimization level, whereas the main diagonal
718718
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:33 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 8: True positive matrix for the largest coreutils programs, compiled with three different compilers and four different
optimization levels. Darker colors represent a higher percentage of correctly matched functions (see Figure 9 for color scale).
TABLE I: Ranks of functions vulnerable to Heartbleed in
OpenSSL compiled for ARM, MIPS and x86, in ReadyNAS
v6.1.6 (ARM) and DD-WRT r21676 (MIPS) ﬁrmware. Each
cell gives the ranking of the TLS/DTLS function.
Multi-MH
From → To
ARM → MIPS
ARM → x86
ARM → DD-WRT
ARM → ReadyNAS
MIPS → ARM
MIPS → x86
MIPS → DD-WRT
MIPS → ReadyNAS
x86 → ARM
x86 → MIPS
x86 → DD-WRT
x86 → ReadyNAS
TLS
1;2
1;2
1;2
1;2
2;3
1;4
1;2
2;4
1;2
1;7
70;78
1;2
DTLS
1;2
1;2
1;2
1;2
3;4
1;3
1;2
6;16
1;2
11;21
1;2
1;2
Multi-k-MH
TLS
DTLS
1;2
1;2
1;2
1;2
1;2
1;2
1;2
1;2
1;2
1;2
5;33
1;2
1;2
1;2
1;2
1;2
1;2
1;3
1;2
1;4
1;2
1;6
1;2
1;2
Fig. 9: Fraction of strong matches for the largest coreutils
programs. Darker colors represent a higher percentage of top-1
ranks, which exceed the threshold of 0.1 similarity difference
to the top-2 rank. On the main diagonal, these matches are
true positives; in all other cells they reﬂect false positives.
averages over all optimization levels, the matching results are
actually even better for this special case.
C. Bug Search in Closed-Source Software
In this section, we evaluate several case studies of recent
prominent cross-architectural vulnerabilities. For example, we
ﬁnd the Heartbleed bug on desktops as well as on mo-
bile devices and router ﬁrmware images. Similarly, we ﬁnd
BusyBox bugs, which are part of closed-source bundles
of embedded devices (e.g., home routers) across multiple
architectures. Finally, we identify a bug and a backdoor in
closed-source ﬁrmware images.
Our method is targeted towards ﬁne-grained, sub-function
code similarity. A bug signature should hold the most discrim-
inatory parts from the buggy binary code, which may include
context that does not strictly belong to the bug. We could
easily generate such signatures automatically from source code
information (see Section II-B). While this allows one to tweak
bug signatures to achieve optimal results, we chose not to do so
in our experiments to ensure reproducibility and comparability
of our work. Instead, we declare the entire function containing
a bug as our bug signature.
1) OpenSSL/Heartbleed: In April 2014, the critical Heart-
bleed bug (CVE-2014-0160) was ﬁxed in the OpenSSL
cryptography library. Since OpenSSL is an integral part of
many TLS implementations, this security-critical bug is widely
deployed, including many closed-source software applications.
The Heartbleed bug allows an attacker to perform an out-
of-bounds read, which is, due to the handled key mate-
rial and OpenSSL’s built-in memory management, highly
security-critical. The bug can be triggered remotely by ma-
nipulating heartbeat (keep-alive) messages [34]. The vulnera-
ble functions are tls1_process_heartbeat (TLS) and
dtls1_process_heartbeat (DTLS) in v1.0.1a-f.
We extracted one bug signature for each of these two func-
tions from v1.0.1f in an automatic fashion, which required only
the vulnerable function’s name and its starting address. Again,
we stress that more speciﬁc signatures might ﬁt better into our
tool’s niche of ﬁne-grained code comparison. However, we
chose not to manually reﬁne the signature in order to avoid
deceptive and possibly over-ﬁtted signatures.
We use these signatures to ﬁnd the vulnerable functions
in OpenSSL binaries that we compiled for x86, MIPS, and
ARM. In addition, we also search in two real-world occur-
rences of vulnerable OpenSSL libraries in ﬁrmware images:
the Linux-based router ﬁrmware DD-WRT (r21676) compiled
for MIPS and a NAS device (Netgear ReadyNAS v6.1.6) with
an ARM processor [9], [31]. We took the ﬁrmware images
provided on the project websites, unpacked them and searched
719719
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:33 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: make_device ranks in BusyBox for Multi-MH.
M
R
A
9.0
1.1
v
1
6
1
S
MIP
9.0
1.1
v
1
1
2
6
8
x
9.0
1.1
v
1
4
1
S
MIP
0.0
1.2
v
1
1
1
6
8
x
0.0
1.2
1
1
1
From → To
v1.20.0 ARM
v1.20.0 MIPS
v1.20.0 x86
TABLE III: socket_read ranks in ﬁrmware for Multi-MH.
From → To
DGN1000
DGN3500
DM111Pv2
JNR3210
D G N 1 0 0 0
D G N 3 5 0 0
D M 1 1 1 P v 2
J N R 3 2 1 0
-
1
2
1
1
-
2
1
2
1
-
1
1
1