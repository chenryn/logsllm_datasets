### Precision and Communication Efficiency

DELPHI [4] utilizes a smaller number of bits for fixed-point precision (e.g., 13 bits and 15 bits, respectively). This allows them to work with arithmetic shares over a 32-bit ring instead of a 64-bit ring. Consequently, this reduces communication overhead (since shares are half the size) and, in our model, also saves computational resources. Recall from Section II-B that we need to split 64-bit integer tensors into four 16-bit integer tensors to use existing CUDA kernels for deep learning.

Using fewer bits of precision can reduce the accuracy of the protocol outputs, especially when scaling to deeper architectures and larger inputs. To analyze the impact of the number of fixed-point precision bits \( t \) on the accuracy of the system's outputs (i.e., the values of the output layer), we compute the average relative error between the output values generated by CRYPTGPU and those computed using the plaintext inference protocol. We evaluate this on both a small example (AlexNet over CIFAR-10) and a large example (ResNet-50 on ImageNet). Our results are summarized in Figure 3.

### Analysis of Fixed-Point Precision

Figure 3 shows that for a relatively shallow model like AlexNet on the CIFAR-10 dataset, using 12 to 14 bits of fixed-point precision (as in [6]) is sufficient. The relative error in this case between the outputs computed by the private inference protocol and the plaintext computation is around 1%. However, when scaling up to a model like ResNet-50 on ImageNet, the average relative error increases fivefold to almost 5%.

It is important to note that these measurements are based on a single forward pass over the network (inference). Larger errors would be expected in the case of private training, which requires multiple forward and backward passes. In this work, we use \( t = 20 \) bits of fixed-point precision, ensuring that the average relative error for private inference over ResNet-50 on ImageNet is under 0.02%. Our analysis indicates that deeper architectures and larger datasets will require a greater number of precision bits in the underlying fixed-point representation. For instance, to keep the average relative error under 1% for ResNet-50 on ImageNet, at least 15 bits of fixed-point precision are required. As such, to prevent overflows in the arithmetic evaluation over secret-shared data for deep networks, a 32-bit ring is no longer sufficient.

### Privacy-Preserving Inference

To evaluate the accuracy of our private inference protocol, we compare the average relative error between the outputs of our private inference protocol using ResNet-50, ResNet-101, and ResNet-152 on ImageNet against the values obtained from plaintext evaluation. Additionally, we compute the accuracy of the predictions using standard metrics (Top-1 and Top-5 accuracy). The results are summarized in Table VI. For our chosen set of parameters, the average relative error in the classifier output is at most 0.021%, and in all cases tested (100 randomly-chosen images from the ImageNet test set), both the Top-1 and Top-5 accuracies exactly match those of the plaintext model.

### Privacy-Preserving Training

We perform a similar set of experiments to evaluate the accuracy of our private training protocol. In Figure 4, we plot the value of the cross-entropy loss function for a model trained using the private training protocol of CRYPTGPU and a model trained using the plaintext training algorithm (with the same initialization and learning rate for the underlying stochastic gradient descent optimizer). The figure shows that the loss function value is slightly higher initially for private training but closely follows that of plaintext training overall.

In addition to comparing the evolution of the loss function, we also compare the model accuracies (as measured on the validation set) for the models trained using CRYPTGPU and the plaintext training algorithm. Our results are summarized in Table VII. On all the models and datasets considered, the accuracy of the model output by CRYPTGPU closely matches that of the plaintext evaluation. These experiments indicate that CRYPTGPU efficiently and accurately supports end-to-end private training for models like AlexNet over moderately large datasets like TinyImageNet.

### Average Pooling vs. Max Pooling

As discussed in Section IV-A, we use average pooling in place of max pooling in the models we consider. To evaluate whether the choice of pooling makes a significant difference in model performance, we use PyTorch to train the AlexNet and VGG-16 networks over the CIFAR-10 dataset, replacing all max pooling layers with average pooling layers. The resulting model accuracy on the CIFAR-10 test set is shown in Table VIII. Specifically, we observed a 3% drop in accuracy (from 76% to 73%) for AlexNet and a 1% increase in accuracy with VGG-16 (from 82% to 83%). This indicates that using average pooling in place of max pooling does not significantly degrade model performance. It is also worth noting that more recent ResNets use average pooling in all but the initial layer, in contrast to AlexNet and VGG-16, which use max pooling exclusively.