R(F ) = C(F ) − P (F )
(6)
Then, if the cost of evading the detection feature is much higher than the proﬁts,
this feature is relatively robust. To quantify the evasion cost, we use TF to denote
the threshold for spammers to obtain to evade each detection feature F .
From the viewpoints of Twitter spammers, the cost to evade the detection
mainly includes money cost, operation cost and time cost. The money cost is
mainly related to obtaining followers. We use Cf er to denote the cost for the
spammer to obtain one follower. The operation cost is mainly related to posting
tweets or following speciﬁc accounts. We use Ctwt and Cf ollow to denote the
cost for a spammer to post one tweet or follow one Twitter account. Spammers’
proﬁts are achieved by attracting legitimate accounts’ attention. Thus, Twitter
spammers’ proﬁts can be mainly measured by the number of followings that they
can support and the number of spam tweets that they can post. We use Pf ing
and Pmt to denote the proﬁt of supporting one following account, obtaining one
following back and posting one spam tweet, respectively. Let Nf ing and Nmt
denote the number of accounts that a spammer desires to follow and the number
of malicious tweets that the spammer desires to post.
Then, we show our analysis of the robustness for the following 6 categories of
24 features: proﬁle-based features, content-based features, graph-based features,
neighbor-based features, timing-based features and automation-based features.
The summary of these features can be seen in Table 3.
13
Table 3. Detection Feature Robustness
Feature
Used in Work
Robustness
[35]
[34],
[35], ours
[32],
[34], ours
[35]
[34], ours
[32], ours
[32],
[34],
[35], ours
[32], ours
[35]
[32],
[35]
[32],
[34], ours
[35]
[32]
ours
ours
ours
ours
ours
ours
ours
[32], ours
ours
ours
ours
Low
Low
low
low
Low
High
Low
Low
Low
Low
Low
Low
Low
Medium
High
High
Low
Low
High
Low
Low
Medium
Medium
Medium
Index
Category
F1
Proﬁle
F2 (+)
Proﬁle
F3 (+)
Proﬁle
F4
Proﬁle
F5 (+)
Proﬁle
F6 (+)
Proﬁle
F7 (+) Content
F8 (+) Content
F9
F10
Content
Content
F11 (+) Content
F12
F13
Content
Graph
F14 (*)
Graph
F15 (*)
Graph
F16 (*)
Graph
F17 (*) Neighbor
F18 (*) Neighbor
the number of followers (Nf er, )
the number of followings (Nf ing)
fofo ratio (Rf of o)
reputation (Rep)
the number of tweets (Ntwt)
age
URL ratio (RU RL)
unique URL ratio
hashtag(#) ratio
reply(@) ratio
tweet similarity (Tsim)
duplicate tweet count
number of bi-directional links (Nbilink)
bi-directional links ratio (Rbilink)
betweenness centrality (BC)
clustering coeﬃcient (CC)
average neighbors’ followers (Anf er)
average neighbors’ tweets (Antwt)
F19 (*) Neighbor
followings to median neighbors’ followers (Rf ing mnf er)
F20 (*)
Timing
F21 (+) Timing
F22 (*) Automation
F23 (*) Automation
F24 (*) Automation
following rate (FR)
tweet rate (TR)
API ratio (RAP I )
API URL ratio (RAP I U RL)
API Tweet Similarity (Tapi sim)
Robustness of Proﬁle-based Features: As described in Section 4, spam-
mers usually evade this type of detection features by obtaining more followers.
According to Eq.(6), the robustness of the detection feature fofo ratio(F3), which
is a representative feature of this type, can be computed by Eq.(7).
R(F3) =
Nf ing
TF3
· Cf er − Nf oing · Pf ing
(TF3 ≥ 1)
(7)
Since compared with the big value of Pf oing, Cf er could be much smaller as
shown in Table 2, this feature can be evaded by spending little money. Especially,
even when the spammers who desire to follow 2,000 accounts to breakthrough
14
Twitter’s 2,000 Following Limit Policy, they just need to spend $50. Similar
conclusions can be drawn for the features F1, F2 and F4.
For feature F6, since the age of an account is determined by the time when
the account is created, which can not be changed or modiﬁed by the spammers,
this feature is relatively hard to evade. It could also be evaded if the spammers
can use some tricks to obtain Twitter accounts with big values of ages. However,
unlike obtaining followers, obtaining a speciﬁc Twitter account could be very
expensive. For example, the bid value of purchasing a Twitter account that
steadily has over 1,000 followers is $1,550 [17].
Since number of tweets(F5) is related to several content-based features, we
show the analysis of this feature in the next section.
Robustness of Content-based Features: As shown in Table 3, content-
based features can be divided into two types: signature-based features (F7, F8,
F9, and F10) based on special terms or tags in the tweets and similarity-based
features (F11, and F12) based on the similarity among the tweets. As discussed
in Section 4, both types of features can be evaded by automatically posting non-
signature tweets or diverse tweets. Also, by using these tactics, the spammers
can evade the feature of the number of tweets (F5).
Without the loss of the generality, we use the analysis of the robustness of
the URL ratio (F7) to represent the analysis of this type of features. Similar as
Eq.(7), if a spammer needs to post Nmt tweets with the malicious URLs, the
robustness for F7 can be computed by Eq.(8).
R(F7) =
Nmt
TF7
· Ctwt − Nmt · Pmt
(TF7 ≤ 1)
(8)
Eq.(8) shows that if spammers utilize software such as AutoTwitter [3] and
Twitter API [18] to automatically post tweets, Ctwt will be small. So even when
we set a small value of TF7, compared with the big proﬁts of successfully alluring
the victims to click the malicious URLs, the cost is still small.
Robustness of Graph-based Features: For the graph-based features, we
can divide them into two types: reciprocity-based features (F13 and F14) based on
the number of the bi-directional links and position-based features (F15 and F16)
based on the position in the graph. If we denote CBiLink as the cost to obtain
one bi-directional link, then the robustness of F13 and F14 can be computed in
Eq. (9) and (10).
R(F13) = TF13 · CBiLink
R(F14) = TF14 · Nf ing · CBiLink
(9)
(10)
Since it is impractical to set a high bi-directional link threshold to distinguish
legitimate accounts and spammers, the value of TF13 could not be high. Mean-
while, when TF13 is small, spammers can obtain bi-directional links by following
their followers. Thus, the CBiLink is also not high. Thus, from Eq. 9, we can ﬁnd
that R(F13) is not big. For feature F14, since the average of the bi-directional
links ratio is 22.1% [31] and the spammers usually have a large value of Nf ing,
the spammers need to obtain much more bidirectional links to show a normal
15
bi-directional links ratio. Even though this feature could be evaded by following
spammers’ followers, due to the diﬃculties of forcing those accounts to follow
spammers back, it will cost much to evade this feature.
For the position-based features, since spammers usually blindly follow legiti-
mate accounts, which may not follow those spammers back, it will be diﬃcult for
spammers to change their positions in the whole social network graph. Similarly,
spammers can neither control the accounts they followed to build social links
with each other. In this way, it is diﬃcult for spammers to change their values
of the graph metrics, thus to evade graph-based features.
Robustness of Neighbor-based Features: The ﬁrst two neighbor-based
features (F17 and F18)reﬂect the quality of an account’s friend choice, which
has been discussed in Section 5. If we use Nf ollow to denote the number of
popular accounts (the accounts who have very big follower numbers) that a
spammer needs to follow to get a high enough Anf er to evade feature F17, then
the robustness of F17 can be computed as Eq.( 11).
R(F17) = Nf ollow · Cf ollow
(11)
Since there are many popular accounts with very big followers, Nf ollow and
Cf ollow could be small. Thus, as long as the spammers know about this detection
feature, they can evade it easily. Similar results can be gained for feature F18.
However, for feature F19, since we use the median not the mean of the neigh-
bors’ followers, they need to follow around half of Nf ing popular accounts to
evade this feature. With a consideration of spammers’ big values of Nf ing, the
cost will be very high and the proﬁt will be decreased dramatically for the spam-
mers to evade this feature. So, feature F19 is relatively diﬃcult to evade.
Robustness of Timing-based Features: The timing-based features are
related to spammers’ update behavior. Although the proﬁts may drop, when
spammers decrease their following or tweeting rate, since these two features can
be totally controlled by the spammers, the cost will be low. Thus, feature F20
and F21 can still be evaded by losing some proﬁts.
Robustness of Automation-based Features: As discussed in Section 5,
many Twitter spammers use software or Twitter API to manage their multiple
spam accounts to automatically post tweets. Since few legitimate accounts would
use API to post tweets and it is relatively expensive for spammers to only use
web to post a large number of malicious tweets on multiple spam accounts, the
combination use of the features of F22, F23, and F24 are relatively diﬃcult to
evade. (More detailed discussions can be found in our technical report [36].)
In summary, through the above analysis, we can categorize the robustness of
these detection features into the following three scales: low, medium, and high,
as shown in Table 3.
7 Evaluation
In this section, we will evaluate the performance of our machine learning feature
set including 8 existing eﬀective features marked with (+) and 10 newly designed
features marked with (*) in Table 3.
16
We evaluate the feature set by implementing machine learning techniques
on two diﬀerent data sets: Data set I and Data set II. Data set I consists of
5,000 accounts without any spam tweets and 500 identiﬁed spammers, which are
randomly selected from our crawled dataset described in Section 3. To decrease
the eﬀects of sampling bias and show the quality of our detection feature schema
without using URL analysis as ground truth, we also crawled another 35,000
Twitter accounts and randomly selected 3,500 accounts to build another data
set, denoted as Data set II.
7.1 Evaluation on Data set I
In this section, based on Data set I, we evaluate our machine learning feature
set including performance comparison and feature validation.
Performance Comparison: In this experiment, we compare the perfor-
mance of our work with three existing approaches4:
[34] and [35]. We
conduct our evaluation by using four diﬀerent machine learning classiﬁers: Ran-
dom Forest (RF), Decision Tree (DT) , Bayes Net (BN) and Decorate (DE). (To
better show the results, we label our method as A,
[34] as C, and
[35] as D.) For each machine learning classiﬁer, we use ten-fold cross validation
to compute three metrics: False Positive Rate, Detection Rate, and F-measure5.
[32] as B,
[32],
0.05
0.04
0.03
0.02
0.01
e
t
a
R
e
v
i
t
i
s
o
P
e
s
l
a
F
RF
DT
BN
DE
1
0.8