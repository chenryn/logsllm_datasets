level represents a signiﬁcant advance over current industrial practice.
Other threats. Compliance is a multi-faceted endeavor: every
layer of the system needs to have its own safeguards to protect
against attacks at that level. Traditional controls and techniques
outside the scope of this paper are needed to help detect, analyze,
and cleanup after traditional attacks against the DBMS and storage
servers, including software and hardware errors and failure; denial-
of-service attacks; malicious transactions from legitimate users,
either acting maliciously or innocently (cross-site scripting); and
tampering with the DBMS, OS, or application software.
3. TLOW ARCHITECTURE
Design Overview. Our design goals for TLOW were to overcome
the run-time and audit-time inefﬁciencies of LDA while preserving
transaction throughput, minimizing space overhead, and avoiding
changes to the DBMS kernel. The resulting Transaction Log on
WORM (TLOW) DB architecture is very simple, consisting of a
transaction-time layer atop an ordinary DBMS. As shown in Figure
2(a), TLOW stores the current DB instance at an ordinary storage
server. The snapshot from the previous audit, as well as the trans-
action log L since the last audit, reside at a WORM storage server.
Legacy applications can run on a TLOW DB with no changes. All
traditional SQL statements will work correctly, including deletions.
New time-aware applications can query past states of the database
by including an additional clause at the end of their SQL queries,
describing the time point at which the queries should be run. Intu-
itively, TLOW DBs seem to have a zero regret interval: since L and
the old snapshot are on WORM, an auditor should be able to tell
if the current DB state is correct. The proof of Theorem 4.1 shows
that this is incorrect, unless the WORM and DBMS servers have
perfectly synchronized clocks.
3.1 Leveraging existing technology
In TLOW, we leverage existing technology using a number of
reﬁnements designed to thwart certain attacks by Mala.
Extending WORM storage servers. We require one extension
to current WORM storage server interfaces, namely, the ability to
append to term-immutable ﬁles located in a particular directory or
volume, from the time that they are created until the ﬁrst time they
are closed. After the ﬁrst closure, the ﬁle must remain read-only for
the duration of its retention period. This simple extension makes it
possible to append to a log ﬁle, while the older part of the same ﬁle
is already term-immutable. Current WORM servers are software (or
Firmware) WORM , so the append facility is an easy add-on.
Temporal constraint on transaction log segments. To support a
TLOW regret interval of r time units, we require that the current L
240
DBMS EngineTables, IndicesTransaction LogAudit Helper (AH)AuditWORM StorageServerDatabaseStorage ServerAuditHashes1T2TnTSnapshotDB StateFinalDB stateIntegrityDB StateTraditional StorageTransactions…DB StateWORM StorageAuditorIntegrityCheckAuditor checks if final state is consistent with snapshot and log.TransactionLogﬁle be ﬂushed to the WORM server every r/2 time units, and a new
L ﬁle created. DBMSs already contain a function that can be called
periodically to ﬂush the current L ﬁle to disk, close it, and open a
new ﬁle for L. This can be done by tuning the appropriate DBMS
parameter, or by having a small application on the DBMS server
machine that sleeps for r/2 time units, makes the call, and goes
back to sleep. The periodic creation of a new log ﬁle is necessary
for detecting clock tampering attacks on the DBMS server that can
compromise the next audit. This is also a key difference between L-
DA and TLOW – LDA requires that the DBMS and WORM servers
clock’s be roughly synchronized (i.e., within r/2 time units); TLOW
does not impose any clock synchronization requirement.
Transaction-time databases. An auditor must be able to distin-
guish between data tampering and legitimate modiﬁcations. There-
fore, TLOW retains all versions of a database tuple, using a layer
of software above the DBMS that turns it into a transaction-time
database [8, 11, 12, 21, 23]. In general, a transaction-time DBMS
translates every tuple insertion, deletion, or modiﬁcation into the
insertion of a new version of the tuple, which is placed on the same
page as the old tuple if possible. Unknown to legacy applications,
each tuple has a start time attribute, giving the commit time of the
transaction that inserted the tuple. Deleted tuples are identiﬁed by
a special end-of-life version, whose start time is the commit time
of the transaction that deleted the tuple. For such databases, the
history forgery threat takes four forms: insertion of tuples with start
times that already have passed, removal of tuples that have not yet
expired, overwrites of existing versions of tuples, and updates to
deleted tuples or tuple versions that are not the most recent.
Tuple shredding. At the end of its retention period, a tuple version
may be shredded. We adopt the shredding approach proposed for
LDA [16], where a vacuuming process identiﬁes expired tuple ver-
sions and removes them. To make shredding trustworthy, a small
amount of information about shredded tuples is logged on WORM
and checked at the next audit. As trustworthy shredding is orthogo-
nal to the contributions of this paper, we do not consider it further.
Hash-page-on-read. An untamper attack occurs if Mala tampers
with the values in the database, causing transactions to read incorrect
data (and then possibly write incorrect data based on what they read),
and then untampers the data before the next audit. For example,
Mala may use a ﬁle editor to overwrite her salary in the DB and
double it, right before a legitimate application gives employees a
10% raise. Then she can untamper the old version of the tuple,
leaving the new version in place with its incorrect salary. To detect
that Mala’s ﬁnal salary value is incorrect, we must detect that the
raise transaction read tampered data. To prevent untamper attacks,
we adopt the same hash-page-on-read approach originally proposed
(for slightly different purposes) for LDA [16]. In a nutshell, we hash
the tuples on each data page at the time the page is read from disk,
and record the page ID and hash value on the transaction log.
3.2 Audit fundamentals
We consider only quiescent audits; non-quiescent audits can be
performed using techniques used in non-quiescent checkpointing.
The tuple completeness check. Figure 2(b) shows that at the end
of a successful audit, the auditor writes a snapshot of the current
database state and a cryptographic hash of its tuples to WORM
storage, and signs them. The expiration of L ﬁles on WORM must
be such that they will be retained until at least the completion of the
next audit. During the next audit, a very simple procedure sufﬁces if
no crashes, transaction aborts, log ﬁle tampering, or DB tampering
followed by untampering has occurred. In this case, the auditor
h(a1),
1≤i≤n
checks the signature on the hash of the previous snapshot and gener-
ates new hashes from the current instance and from L, as described
below. Intuitively, the audit succeeds if the hash from the old DB
snapshot Do, plus the hash of all the new tuples introduced in L, is
equal to the hash of the current instance Dc.
Appropriate hash functions. TLOW and LDA both require a
cryptographically strong commutative incremental hash function H
for hashing sets of tuples. For H, we use the ADD_HASH function
proposed by Bellare and Micciancio [3]:
ADD_HASH(a1, . . . , an) =
(cid:88)
where h is a big (512 bits or more) secure one-way hash function
and the sum is taken modulo a large number. H’s security properties
stem from it being pre-image resistant, i.e., given a set {a1, . . . , an},
one cannot efﬁciently ﬁnd {b1, . . . , bm} ((cid:54)= {a1, . . . , an}) such
that H({a1, . . . , an}) = H({b1, . . . , bm}).
Using such a hash function, the auditor can incrementally com-
pute a hash over Do and the new tuples in L, and Dc. Pre-image
resistance ensures that the (slightly abusing notation) H(Do ∪L) =
H(Dc) if and only if Ds ∪ L = Dc. Each hash operation takes
O(1) time. We can check whether Dc contains the right tuples in
a single pass over Dc and L, plus a read of the signed hash of the
previous snapshot. The total cost, including the time to write out
the new signed snapshot and signed hash, is O(|Dc| + |L|). This
asymptotic complexity is the same as for LDA, and is too high in
practice to permit frequent internal audits. Later we will show how
to reduce the audit cost by a factor of 100.
3.3 Audit Process
Pseudocode for the auditor is provided in Figures 3, 4, 5, and 6.
Next, we discuss how the auditor works.
Detecting tampered log ﬁles. The auditor’s work (as shown in the
Audit routine in Figure 3) begins with sanity checks on the contents
of the log ﬁles; we focus here on those associated with crashes,
which are in the GlobalSanityCheck routine (Figure 4).
The auditor consults her trusted list of crash and recovery times
to ﬁnd all pairs [k, k(cid:48)], where k is the time of a crash or shutdown,
and k(cid:48) is the time that the DBMS recovered and started accepting
new transactions. A log ﬁle is part of the recovery log, written R, if
its life span [create time, last write time] falls entirely inside [k, k(cid:48)].
The auditor ignores all R ﬁles during the remainder of the audit.
We reserve the notation L for log ﬁles generated during normal
transaction processing, outside of recovery.
Tampering is indicated by any log ﬁle whose lifespan partially
overlaps [k, k(cid:48)]. Mala may have appended additional records to an
existing log ﬁle after a crash. More precisely, any log ﬁle whose
last write time is after k and before k(cid:48) and contains anything other
than recovery-related information indicates tampering, as does any
log ﬁle whose create time is before k(cid:48) and whose last write time
is after k(cid:48). If any of these is violated, the audit fails, and forensic
analysis must be performed. After completing global sanity checks,
the auditor can perform local sanity checks on each log ﬁle segment,
and then continue on to the main tasks of the audit, being sure to
ignore all R ﬁles. SanityCheck (Figure 5) reads each log ﬁle seg-
ment L, and checks the COMMIT records of the transactions. The
check (and consequently, the audit) fails if there are more than one
COMMIT record for the same transaction, or a COMMIT record
timestamp is before the create time of L or last write time of L.
The auditor also checks if the last modiﬁcation time of L is within
r/2 time interval after its create time, and if any COMMIT record
timestamp is within r/2 time interval before the create time of L, or
after the last modiﬁcation time of L. After checking the consistency
241
Audit (T , CrashList, DBold, DBnew)
1: Let T be the sequence of transactionlogﬁles in create time order
2: Let CrashList be the trusted list of crash and recovery events.
3: Let DBold be the snapshot from the last audit, and let DBnew
be the current instance of the DB.
{Extract recovery log R, no-crash transaction log L from T }
4: (L,R) ← Extract_Recovery_Log(T , CrashList)
{Perform global sanity checks on transaction log ﬁles}
5: if (GlobalSanityCheck(L, R,CrashList) == AUDIT_FAIL)
then
return AUDIT_FAIL
return AUDIT_FAIL
{Is each individual log ﬁle okay?}
6:
7: end if
8: if (SanityCheck(L, DBold, DBnew) == AUDIT_FAIL) then
9:
10: end if
11: logHash ← HashTransactionLog(L)
12: oldDBHash ← ComputeDbHash(DBold)
13: newDBHash ← ComputeDbHash(DBnew)
14: computedHash ← ADD_HASH(logHash, oldDBHash)
15: if (newDBHash == computedHash) then
16:
17: else
18:
19: end if
return AUDIT_PASS
return AUDIT_FAIL
Figure 3: Pseudo-code for Audit
of the old DB state and its signature, the auditor proceeds to hash L.
Computing and matching log and DB hashes. In HashTransac-
tionLog (Figure 6), the auditor parses L from beginning to end to
ﬁnd all the new tuples (more precisely, versions of tuples) inserted
by transactions, and to determine which transactions committed. If
a log record records the insertion of tuple t, she extracts t’s times-
tamp ﬁeld ts. Depending on the transaction-time DBMS used, ts
may contain a transaction ID or transaction commit time. In some
DBMSs, the transaction ID may be changed to the commit time after
the tuple has gone out to disk and the transaction has committed.
The auditor keeps a data structure recording the transaction IDs that
she sees in the log, their commit timestamp (if any), and the new
tuples inserted by that transaction. Here, we assume that the commit
timestamp for a new tuple is given in the COMMIT log record for
its transaction, and does not appear in the log before this point.
If ts is a transaction ID tid that she has not seen before, she starts
a new data structure to record information about tid. If she has not
seen a COMMIT record for tid yet, she adds t’s key (exclusive of
timestamp) and content to her data structure for tid, overwriting
any content already stored there for k. (The overwrite is crucial, as
only ﬁnal values written by a transaction should be included in her
hash.) When the auditor parses a COMMIT record for a transaction
T with ID tid and commit timestamp ts, she hashes all of the tuples
recorded in her data structure for tid and then deletes the structure.
As L is used for recovery, all of T ’s new tuples must appear on L
before that COMMIT record. Any subsequent log records about
tuples from T are irrelevant to the audit and are ignored. The auditor
never hashes new tuples from aborted transactions; any tid data
structures remaining at the end of scan are from such transactions.
When she hashes the tuples in Dc, she must replace any transaction
IDs she sees by the appropriate commit times. Finally, she hashes
the old and new DB states, and does the tuple completeness check.
Other integrity checks. The auditor must also verify the integrity
242
return AUDIT_FAIL
GlobalSanityCheck (L, R, CrashList)
1: if (R contains anything other than recovery operations) then
2:
3: end if
4: for all (c ∈ CrashList) do
5:
{c.Start is the time that the crash occurred, and c.End is
the time that the DBMS resumed normal operation}
for all (L ∈ L) do
{Let L.ct and L.mt be the creation and last write times
of L, respectively. Reject log ﬁles written/created during
DBMS outage}
if ((c.End > L.mt > c.Start) OR (c.Start < L.ct <
c.End)) then
6:
7:
8:
end if
end for
return AUDIT_FAIL
9: