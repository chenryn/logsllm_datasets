Avg proﬁle
size (bits)
294
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
TABLE III: Personalization, privacy and efﬁciency tradeoffs for Bloom cookies when varying the noise level l (1000
users, 264,615 queries). For personalization, the table reports the difference between avg rankU RL (computed using exact
proﬁles, ﬁrst row in the table) and the average rank obtained with Bloom cookies. For privacy, it reports unlinkability
as avg (stdev) entropy-based unlinkability, linkable users percentage and max probability with top 1% outliers removed.
For efﬁciency, it reports the size of the noisy proﬁle.
(a) Effect of k and l
(b) Effect of m and l
Fig. 3: Bloom cookies average unlinkability and personalization loss when varying k (k = 3, 5, 7) or m (m =
1000, 1500, 2000) with different noise levels (l), 300 users.
exact URL proﬁles when varying k (k = 3, 5.7) or the size of
the Bloom ﬁlter m (m = 1000, 1500, 2000), and for different
levels of noise (l). When increasing k (see Figure 3(a)), the
average unlinkability decreases and the personalization loss
shrinks. In fact, increasing k means reducing the probability
of false positives as well reducing the number of noisy bits
controlled by l (because with larger k more bits are set for
each proﬁle item when stored in the Bloom ﬁlter). Conversely,
increasing m (slightly) increases unlinkability (and has almost
no effect on personalization). This is because although a larger
m reduces the probability of false positives, a larger Bloom
ﬁlter means that more noisy bits are set because l controls
the fraction of bits set. The noise effect controlled by l
prevails thus making unlinkability higher when m increases.
We notice however that the variations are relatively small (there
is no substantial difference between the case m = 1500 and
m = 2000 bits). For this reason, we use only k and l to control
the noise level of BloomCookies.
D. Algorithm for conﬁguring Bloom cookies
By varying the noise level, a user can control the privacy-
personalization tradeoff. A privacy-concerned user may choose
to operate at
the highest noise levels, while a user that
values privacy and personalization in the same manner may
decide to operate at moderate noise levels. We designed an
algorithm that automatically conﬁgures the noise parameters
in the Bloom cookie given a user’s privacy and personalization
goals.
Algorithm. The pseudocode of the algorithm is shown in
Figure 4. The algorithm takes as input a personalization goal
speciﬁed as maximum percentage loss that a user is willing
to tolerate (compared to the personalization obtained with
exact proﬁles), and a privacy goal speciﬁed as the minimum
unlinkability a user wants to achieve. In addition, the algorithm
uses the history of proﬁles previously sent by the client to
the server to compute the proﬁle similarity over time. The
algorithm returns the pair  for conﬁguring the Bloom
cookie.
The algorithm employs two prediction models, one for
personalization and one for privacy. The models are trained
using a set of users for which search history is available.14
We build the personalization model by computing the loss
in personalization for the training users when independently
varying the parameters k and l (m = 2000). Given a tar-
get personalization loss, the model predicts various 
combinations by performing a linear interpolation between all
measured data points.
14We train the models using 300 users for which we have 1 month-long
logs (300 are used for training the privacy model and a subset of 93 users
whose personalization proﬁle is large enough is used for the personalization
model).
11
def find_noise(sim,pergoal,privgoal,permodel,privmodel):
simc = find_similarity_class(sim)
solutions = []
for k in k_vals:
per = scaling(interpolation(permodel[k]))
priv = scaling(interpolation(privmodel[simc][k]))
# find _min s.t. priv(_min) = privgoals
_min = inverse(priv)(privgoal)
# find _max s.t. per(_max) = pergoal
_max = inverse(per)(pergoal)
if _min. We compute the
Jaccard similarity between two consecutive 2-week proﬁles of
the training users and then divide them in s = 10 buckets
based on the similarity value.15 For each bucket, we then create
a privacy model by doing a linear interpolation in a similar
way as for the personalization model. For a desired level of
unlinkability the model predicts the pair . Thus, given a
privacy goal and the similarity of a user’s proﬁle across time,
the algorithm ﬁnds which similarity bucket the user belongs
to, and then uses the appropriate privacy model for that bucket
to predict .
The privacy model provides a lower bound on the noise
(i.e., with more noise higher unlinkability is achieved). The
personalization model provides an upper bound (i.e., with more
noise a larger personalization loss is experienced). If the lower
bound is higher than the upper bound, there is no solution
satisfying the goals. Otherwise, the solution is determined by
randomly selecting a k among the possible values and use the
minimum noise level for such k.
Effect of population size. Unlinkability depends on the
population size. Hence, if we train on a certain sized set
of users and use a different sized population for testing, we
need to take this factor into account. When a user population
increases we expect it to be harder to link user proﬁles so the
unlinkability goal becomes easier to some extent.
We hypothesize that when the population size increases
from n to kn, the probability distribution function of different
proﬁles from different time periods to belonging to the same
user (i.e., what we called p(x) in §III-B) remains the same,
except scaling by a constant factor. In this case, we can
express the new unlinkability u(cid:48) for that user in terms of
the old unlinkability u and the population sizes. Given the
unlinkability deﬁnition in §III-B, unlinkability for a population
of size n(cid:48) = kn is the following
15Instead of imposing arbitrary similarity ranges, we derive the ranges by
diving the users in equal sets, so to mimic the actual user distribution. As an
example, the ﬁrst 3 classes are (0.0, 0.207), (0.207, 0.313) and (0.313, 0.399).
12
(a)
(b)
Fig. 5: Error in predicting unlinkability when increasing
the number of users from 100 to 900 (a) and from 50 to
900.
(cid:80)n(cid:48)
(cid:48)
u
=
i ∗ log p(cid:48)
(p(cid:48)
i)
log 1
n(cid:48)
=
k(cid:80)n( pi
k ∗ log pi
k )
log 1
kn
=
u ∗ log 1
n − log(k)
log 1
kn
We verify our hypothesis by taking user populations of dif-
ferent sizes (n = 50, 100) and use them to predict the average
unlinkability for larger user populations (kn = 200, 500, 900).
We measure the error for varying levels of noise. Results are
shown in Figure 5. The average prediction error for n = 100
is 2.8%, 5.8% and 7.7% for scaling to 200, 500 and 900
users, respectively. For n = 50, it is 3.2%, 6.3% and 8.1%
when scaling to 200, 500 and 900 users, respectively. The
prediction error is reasonably small for small scaling factors.
However, even for large scaling factors,
the prediction is
typically conservative—the observed unlinkability is larger
than the predicted value. Hence, we use this approach to make
our algorithm’s predictions scale to larger populations of users.
Algorithm evaluation. We evaluate the performance of the
algorithm on a set of 700 users (and a subset of size 215 for
personalization). For a particular combination of privacy and
personalization goals, we invoke the algorithm for each user to
obtain a  pair for that user. We then measure the average
unlinkability and personalization loss for such a set of users
assuming they would use the noise parameters as speciﬁed by
the algorithm. We then verify whether the initial goals are met.
Table IV reports the results for various combinations of goals.
Privacy goals are met in all conditions, with an average
unlinkability higher than desired. Personalization goals are
met fully or with a very small error when the minimum
personalization loss requested is below 0.3%. For instance,
for a target personalization loss below 0.2%, the actual loss
is 0.29% and 0.34%. Personalization strongly depends on the
user so it is harder to build a general model which works well.
More information about the user proﬁle (e.g., distribution of
user interests, demographics, etc.) could be used as features for
training the personalization model. On the other hand, simply
given a larger training set, we expect the algorithm to be able
to meet stricter goals on personalization as well.
VI. RELATED WORK
The body of research closest to our work is that of client-
side solutions for preserving privacy in services such as search,
advertising, and news portals [20], [23], [27], [47]. There are
three problems with current client-side solutions: the client
Desired goals
personalization
privacy
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
0.4
0.5
0.5
0.5
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.9
0.7
0.8
0.9
0.7
0.8
0.9
0.7
0.8
0.9
0.7
0.8
0.9
0.7
0.8
0.9
Achieved goals
personalization
No solution
× (0.29)
× (0.34)
× (0.31)
× (0.34)
(cid:88)(0.29)
(cid:88)(0.29)
(cid:88)(0.34)
× (0.48)
(cid:88)(0.31)
(cid:88)(0.34)
(cid:88)(0.50)
(cid:88)(0.31)
(cid:88)(0.42)
(cid:88)(0.50)
(cid:88)(0.31)
(cid:88)(0.50)
(cid:88)(0.68)
privacy
(cid:88)(0.84)
(cid:88)(0.93)
(cid:88)(0.83)
(cid:88)(0.95)
(cid:88)(0.93)
(cid:88)(0.84)
(cid:88)(0.95)
(cid:88)(0.91)
(cid:88)(0.84)
(cid:88)(0.94)
(cid:88)(0.93)
(cid:88)(0.84)
(cid:88)(0.95)
(cid:88)(0.93)
(cid:88)(0.84)
(cid:88)(0.95)
(cid:88)(0.95)
TABLE IV: Algorithm performance (700 users). The table
reports the desired goals and if a solution is found whether
the goals were met. In parenthesis it is the actual unlink-
ability and personalization achieved with the algorithm’s
predicted k and l.
overhead generated by carrying out personalization at
the
client, the quality of personalization achievable by sharing
coarse-grained proﬁles with servers, and the privacy loss,
particularly in the case of an adversary that attempts to link
a user’s activities over time. Bloom cookies are designed to
help with all three issues.
In our work, we do not rely on anonymization proxies,
onion routing, TOR and other anonymity networks because
their deployment may be too expensive. However, if these
solutions are available, Bloom cookies become even more
important for personalization because servers are left with no
means of building user proﬁles.
Non-anonymous privacy models that allow user identiﬁca-
tion, but prevent derivation of personal information have been
largely studied in the privacy community, in particular for web
search. Tools like TrackMeNot [24], PRAW [36], PDS [31],
OQF-PIR [35], GooPIR [17] and NISPP [50] (all extensively
reviewed in [5]) fall
into this category. TrackMeNot, for
instance,
is a browser plugin that generates extra dummy
queries to mislead a potential adversary that is trying to infer
the interest proﬁle of its users. A ﬁrst problem with these
solutions is the relatively large increase in the server’s query
load, which may not be tolerable by many online services. A
second problem is that they do not consider personalization as
a goal, thus signiﬁcantly affecting the experience of services
like web search.
In literature, we ﬁnd four main types of obfuscation
techniques that have been used to protect disclosure of a
user’s personal