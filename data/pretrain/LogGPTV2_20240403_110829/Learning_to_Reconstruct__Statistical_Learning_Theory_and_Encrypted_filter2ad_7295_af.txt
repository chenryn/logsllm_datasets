believe this work represents an exciting ﬁrst step towards
building a cohesive theory of security in the presence of
access pattern leakage. Towards this, we recommend two main
research directions for future work to pursue: ﬁrst, extend our
attacks to other query types of practical importance like edit
distance, wildcard, and substring queries. Second, study and
apply other results from learning theory, such as active or
online learning, to access pattern leakage attacks and defenses.
ACKNOWLEDGMENTS
Portions of this work were written while the ﬁrst au-
thor was visiting Royal Holloway, University of London.
This work was supported by NSF Graduate Research Fel-
lowship DGE-1650441, the European Union’s Horizon 2020
grant ECRYPT-NET (H2020 643161), ERC Project aSCEND
(H2020 639554), and EPSRC Grant EP/M013472/1.
REFERENCES
[1] B. Fuller, M. Varia, A. Yerukhimovich, E. Shen, A. Hamlin,
V. Gadepally, R. Shay, J. D. Mitchell, and R. K. Cunningham, “SoK:
Cryptographically protected database search,” in 2017 IEEE Symposium
on Security and Privacy (SP), May 2017, pp. 172–191. [Online].
Available: https://doi.org/10.1109/SP.2017.10
1079
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:08 UTC from IEEE Xplore.  Restrictions apply. 
[2] M.-S. Lacharit´e, B. Minaud, and K. G. Paterson, “Improved reconstruc-
tion attacks on encrypted data using range query leakage,” in 2018 IEEE
Symposium on Security and Privacy (SP), May 2018, pp. 297–314.
[3] G. Kellaris, G. Kollios, K. Nissim, and A. O’Neill, “Generic attacks on
secure outsourced databases,” in CCS, 2016.
[4] M. Naveed, S. Kamara, and C. V. Wright, “Inference attacks on property-
preserving encrypted databases,” in CCS, 2015.
[5] P. Grubbs, K. Sekniqi, V. Bindschaedler, M. Naveed, and T. Ristenpart,
“Leakage-abuse attacks against order-revealing encryption,” in 2017
IEEE Symposium on Security and Privacy (SP), May 2017, pp. 655–672.
[6] Y. Zhang, J. Katz, and C. Papamanthou, “All your queries are belong
to us: The power of ﬁle-injection attacks on searchable encryption,”
in 25th USENIX Security Symposium, USENIX Security 16, 2016,
pp. 707–720. [Online]. Available: https://www.usenix.org/conference/
usenixsecurity16/technical-sessions/presentation/zhang
[7] P. Grubbs, R. McPherson, M. Naveed, T. Ristenpart, and V. Shmatikov,
“Breaking web applications built on top of encrypted data,” in CCS,
2016.
[8] L. G. Valiant, “A theory of the learnable,” Communications of the ACM,
1984.
[9] M. J. Kearns, U. V. Vazirani, and U. Vazirani, An introduction to
computational learning theory. MIT press, 1994.
[10] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth, “Classifying
learnable geometric concepts with the Vapnik-Chervonenkis dimension,”
in Proceedings of the Eighteenth Annual ACM Symposium on Theory
of Computing, ser. STOC ’86. New York, NY, USA: ACM, 1986, pp.
273–282. [Online]. Available: http://doi.acm.org/10.1145/12130.12158
[11] M. Mitzenmacher and E. Upfal, Probability and Computing: Random-
ization and Probabilistic Techniques in Algorithms and Data Analysis,
2nd ed. New York, NY, USA: Cambridge University Press, 2017.
[12] J. Shawe-Taylor and R. C. Williamson, “Generalization performance
of classiﬁers in terms of observed covering numbers,” in Computational
Learning Theory. Berlin, Heidelberg: Springer Berlin Heidelberg, 1999,
pp. 274–285.
[13] K. S. Booth and G. S. Lueker, “Testing for the consecutive ones
property, interval graphs, and graph planarity using PQ-tree algorithms,”
J. Comput. Syst. Sci., vol. 13, no. 3, pp. 335–379, 1976. [Online].
Available: https://doi.org/10.1016/S0022-0000(76)80045-1
[14] G. Grothaus, “General implementation of the PQ-tree algorithm,” 2011,
https://github.com/Gregable/pq-trees.
[15] “Simpliﬁed wrapper and interface generator (SWIG),” 2018, http://www.
swig.org/.
[16] “Federal Aviation Administration pilot database,” 2017, https://www.faa.
gov/regulations policies/pilot records database/.
[17] Wikipedia contributors, “ZIP code — Wikipedia, the free encyclopedia,”
https://en.wikipedia.org/wiki/ZIP Code, 2018.
[18] “ElasticSearch,” 2018, https://www.elastic.co/.
[19] “Salesforce.com,” 2018, https://www.salesforce.com.
[20] “ServiceNow,” 2018, https://www.servicenow.com.
[21] “Dropbox,” 2018, https://www.dropbox.com.
[22] U. C. Bureau, “US Census Bureau name statistics,” https://www.ssa.gov/
OACT/babynames/, 2016.
[23] A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant, “A general
lower bound on the number of examples needed for learning,” Inf.
Comput., vol. 82, no. 3, pp. 247–261, Sep. 1989. [Online]. Available:
http://dx.doi.org/10.1016/0890-5401(89)90002-3
[24] J. L. D. Jr. and C. V. Ravishankar, “Compromising privacy in precise
query protocols,” in Joint 2013 EDBT/ICDT Conferences, EDBT
’13 Proceedings. ACM, 2013, pp. 155–166. [Online]. Available:
http://doi.acm.org/10.1145/2452376.2452397
[25] E. M. Kornaropoulos, C. Papamanthou, and R. Tamassia, “Data recovery
on encrypted databases with k-nearest neighbor query leakage,” Cryptol-
ogy ePrint Archive, Report 2018/719, 2018, https://eprint.iacr.org/2018/
719.
[26] V. Bindschaedler, P. Grubbs, D. Cash, T. Ristenpart, and V. Shmatikov,
“The tao of inference in privacy-protected databases,” Proc. VLDB
Endow., vol. 11, no. 11, pp. 1715–1728, Jul. 2018. [Online]. Available:
https://doi.org/10.14778/3236187.3236217
[27] D. Cash, P. Grubbs, J. Perry, and T. Ristenpart, “Leakage-abuse attacks
against searchable encryption,” in CCS, 2015.
[28] M. Riondato, M. Akdere, U. C¸ etintemel, S. B. Zdonik, and E. Upfal,
“The VC-dimension of SQL queries and selectivity estimation through
sampling,” in ECML PKDD. Springer, 2011.
[29] D. Haussler and E. Welzl, “Epsilon-nets and simplex range queries,”
in Proceedings of the Second Annual Symposium on Computational
Geometry, ser. SCG ’86. New York, NY, USA: ACM, 1986, pp.
61–71. [Online]. Available: http://doi.acm.org/10.1145/10515.10522
[30] N. Sauer, “On the density of families of sets,” Journal of Combinatorial
Theory, Series A, vol. 13, no. 1, pp. 145–147, 1972. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/0097316572900192
relative frequencies of events
[31] V. N. Vapnik and A. Y. Chervonenkis, “On the uniform convergence
to their probabilities,” Theory
of
of Probability & Its Applications, vol. 16, no. 2, pp. 264–
280, 1971,
[Online]. Available: https:
//doi.org/10.1137/1116025
translation by B. Seckler.
APPENDIX A
STATISTICAL LEARNING THEORY PRIMER
We provide a brief summary of learning theory, using
terminology from a recent textbook [11].
A. Concept spaces, -nets, -samples
a∈A fD(a).
i.e., PrD(A) =(cid:80)
Let X be some set of (base) elements. (We consider only
ﬁnite sets in this work.) A concept (also called event or range)
A is a subset of X. Given a probability distribution D on the set
X, let fD represent the pmf. We can then assign a probability
to any concept A in the natural way: PrD(A) is the probability
that a single element of X sampled according to D is in A,
A concept space (or set system) is a pair (X, C) where C
is a set of concepts (subsets) of X (also called subset family).
Any concept C can also be viewed as a function from X to
{0, 1}: the function’s output on input x ∈ X is 1 if x ∈ C
and 0 otherwise. Given a concept space (X, C) and a sample
S of elements drawn from X according to D, we may ask the
following questions:
• Does every concept in C with some not-too-small prob-
• Is the relative occurrence of every concept of C in the
ability occur in the sample S?
sample S close to its expectation?
Answering these questions involves analyzing objects called
-nets [29] and -samples.
Deﬁnition A.1. A subset S ⊆ X is an -net for the concept
space (X, C) with respect to the distribution D if for every
event C ∈ C with PrD(C) ≥ , the intersection S ∩ C is
non-empty.
Deﬁnition A.2. A subset S ⊆ X is an -sample (also called
-approximation) for the concept space (X, C) with respect to
the distribution D if for every concept C ∈ C,
(cid:12)(cid:12)(cid:12)(cid:12)|S ∩ C|
|S| − P rD(C)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ .
Informally, a sample S is an -sample when every concept’s
relative frequency in S is within  of its true probability.
Thinking of  as being small, this can be seen as saying that
S “induces” uniform convergence of relative frequencies to
probabilities.
One way to analyze when a set S is an -net or an -sample
is to characterize the complexity of the concept space. We turn
to this next.
1080
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:08 UTC from IEEE Xplore.  Restrictions apply. 
B. Shattering, VC dimension, growth functions
C
C
∆
The critical measures in determining the complexity of a
C
concept space are the growth function m
(n) and the Vapnik-
Chervonenkis (VC) dimension d, which are related. Given a
concept space (X, C) and a ﬁnite sample S ⊆ X, an important
object is the set of subsamples of S induced by C (also called
the projection of C on S): CS := {C ∩ S}C∈C. The size of
this set is the index of C with respect to S:
(S) :=|CS| = |{A ⊆ X : ∃C ∈ C with A = C ∩ S}| .
Clearly, the index of a concept space relative to a set S is at
most 2|S| and, when C is ﬁnite, it is at most |C|.
The concept space (X, C) shatters the sample S ⊆ X if C
(S) = 2|S|.
induces all possible subsamples of S, i.e., ∆
The VC dimension (also called density [30] or capacity
and denoted by d) of a concept space (X, C) is the largest
cardinality (possibly inﬁnite) of a set S ⊆ X that can be
shattered by C. (It is sufﬁcient for only one set of this size
to exist; not all sets of this size need to be shattered by C.)
VC dimension is an indicator of the complexity of a concept
space. Related to VC dimension is the growth function of a
concept space (X, C), which is the maximum index of C over
all samples S ⊆ X of size n: m
C
(S).
The VC dimension, then, is the largest value of n for which
the growth function equals 2n. Knowing the VC dimension of
a concept space is sufﬁcient to determine an upper bound on
the growth function: either d is inﬁnite or the growth function
(cid:1).
(cid:0)n
is bounded by(cid:80)d
(cid:0)n
(n) ≤(cid:80)d
This partial sum of binomial coefﬁcients (cid:80)d
Lemma A.3 (Sauer’s Lemma [30]). Let (X, C) be a concept
space having ﬁnite VC dimension d. Then, the growth function
C
satisﬁes m
(n) := maxS⊆X:|S|=n ∆
noted by G(d, n) [11]. By induction on d, it is straightforward
to show that it is upper-bounded by nd for d ≥ 2.
C. Sufﬁcient conditions for -nets and -samples
(cid:1) is de-
(cid:0)n
(cid:1).
i=0
i=0
i=0
C
i
i
i
In their groundbreaking paper, Vapnik and Chervonenkis
established a lower bound [31, Thm. 2] on the probability that
a sample S is an -sample, i.e., that the relative frequencies
of events in C are all within  of their true probabilities.
Theorem A.4 (Sufﬁcient conditions for -sample [31]). Let
(X, C) be a concept space with growth function m
(n) and
VC dimension d. Let D be a probability distribution on X and
let S be a set of size n drawn from X according to D. Then,
for any  > 0, the probability that S is an -sample is at least
1 − 4 · m
(2n) · e−2n/8. In particular, there is an n that is
C
C
(cid:19)
(cid:18) d
O
2 log
d

+
1
2 log
1
δ
such that any sample S of size at least n is an -sample with
probability at least 1 − δ.
More precisely, [11, Thm. 14.15] establishes that a sample
δ is an -sample with
2 log 64d
2 + 16
2 log 2
of size at least 32d
probability at least 1 − δ.
Inspired by Vapnik and Chervonenkis’s work on -samples,
Haussler and Welzel introduced -nets and derived a lower
bound in the case where the distribution over X is uniform [29,
Thm. 3.7]. Later work extended this bound to arbitrary distri-
butions:
Theorem A.5 (Sufﬁcient conditions for -net [11]). Let (X, C)
C
be a concept space with growth function m
(n) and VC
dimension d. Let D be a probability distribution on X and
let S be a set of size n drawn from X according to D. Then,
for any  > 0, the probability that S is an -net is at least
1 − 2 · m
(2n) · e−n/2. In particular, there is an n that is
C
O

+
log
log
1

this size is an -net with
such that a sample of at
probability at least 1−δ. Speciﬁcally, a random sample of size
at least max{ 8d
δ} is an -net with probability
at least 1 − δ.
d

least
 log 16d
 log 2
 , 4
1
δ
(cid:18) d
(cid:19)
Ehrenfeucht et al. prove a lower bound [23, Cor. 5] on the
number of samples needed to obtain an -net with probability
at least 1 − δ. Since every -sample is an -net, this lower
bound also applies to -samples.