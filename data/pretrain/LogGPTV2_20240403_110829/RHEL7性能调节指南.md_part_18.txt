的输出。例如，如果用户对 PCI 地址为 `0000:01:00.0`{.literal}
的设备感兴趣，可以通过以下指令来列出该设备中断请求队列：
:::
``` screen
# ls -1 /sys/devices/*/*/0000:01:00.0/msi_irqs
101
102
103
104
105
106
107
108
109
```
::: para
RSS 是默认启用的。RSS 的队列数（或是需要运行网络活动的 CPU
）会由适当的网络驱动程序来进行配置。 `bnx2x`{.systemitem} 驱动程序是在
*`num_queues`* 中进行配置。`sfc`{.systemitem} 驱动程序是用 *`rss_cpus`*
参数进行配置。通常，这都是在
`/sys/class/net/device/queues/rx-queue/`{.filename} 中进行配置，其中
*device* 是网络设备的名称（比如 `eth1`{.literal}），*rx-queue*
是适当的接收队列名称。
:::
::: para
配置 RSS 时，红帽推荐限制每一个物理 CPU
内核的队列数量。超线程在分析工具中通常代表独立的内核，但是所有内核的配置队列，包括如超线程这样的逻辑内核尚未被证实对网络性能有益。
:::
::: para
启用时，基于每个队列的 CPU 进程数量，RSS 在 CPU
间平等地分配网络进程。但是，用户可以使用 `ethtool`{.command}
*`--show-rxfh-indir`* 和 *`--set-rxfh-indir`*
参数来更改网络活动的分配方式，并权衡哪种类型的网络活动更为重要。
:::
::: para
`irqbalance`{.systemitem} 后台程序可与 RSS
相结合，以减少跨节点内存及高速缓存行反弹的可能性。这降低了处理网络数据包的延迟。
:::
:::
::: section
::: titlepage
## [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Configuring_Receive_Packet_Steering_RPS}6.3.6. 配置 RPS {.title}
:::
::: para
RPS（接收端包控制）与 RSS类似，用于将数据包指派至特定的 CPU
进行处理。但是，RPS
是在软件级别上执行的，这有助于防止单个网络接口卡的软件队列成为网络流量中的瓶颈。
:::
::: para
较之于基于硬件的 RSS ，RPS 有几个优点：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    RPS 可以用于任何网络接口卡。
    :::
-   ::: para
    易于添加软件过滤器至 RPS 来处理新的协议。
    :::
-   ::: para
    RPS 不会增加网络设备的硬件中断率。但是会引起内处理器间的中断。
    :::
:::
::: para
每个网络设备和接收队列都要配置 RPS，在
`/sys/class/net/device/queues/rx-queue/rps_cpus`{.filename}
文件中，*device* 是网络设备的名称（比如 `eth0`{.literal}），*rx-queue*
是适当的接收队列名称（例如 `rx-0`{.literal}）。
:::
::: para
`rps_cpus`{.filename} 文件的默认值为 `0`{.literal}。这会禁用
RPS，以便处理网络中断的 CPU 也能处理数据包。
:::
::: para
要启用 RPS，配置适当的 `rps_cpus`{.filename}
文件以及特定网络设备和接收队列中须处理数据包的 CPU 。
:::
::: para
`rps_cpus`{.filename} 文件使用以逗号隔开的 CPU 位图。因此，要让 CPU
在一个接口为接收队列处理中断，请将它们在位图里的位置值设为 1。例如，用
CPU 0、1、2 和 3 处理中断，将 `rps_cpus`{.filename} 的值设为
`00001111`{.literal} （1+2+4+8），或 `f`{.literal}（十六进制的值为
15）。
:::
::: para
对于单一传输队列的网络设备，配置 RPS 以在同一内存区使用 CPU
可获得最佳性能。在非 NUMA 的系统中，这意味着可以使用所有空闲的
CPU。如果网络中断率极高，排除处理网络中断的 CPU 也可以提高性能。
:::
::: para
对于多队列的网络设备，配置 RPS 和 RSS 通常都不会有好处，因为 RSS
配置是默认将 CPU 映射至每个接收队列。但是，如果硬件队列比 CPU
少，RPS依然有用，并且配置 RPS 是来在同一内存区使用 CPU。
:::
:::
::: section
::: titlepage
## [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Configuring_Receive_Flow_Steering_RFS}6.3.7. 配置 RFS {.title}
:::
::: para
RFS（接收端流的控制）扩展了 RPS 的性能以增加 CPU
缓存命中率，以此减少网络延迟。RPS 仅基于队列长度转发数据包，RFS 使用 RPS
后端预测最合适的
CPU，之后会根据应用程序处理数据的位置来转发数据包。这增加了 CPU
的缓存效率。
:::
::: para
RFS 是默认禁用的。要启用 RFS，用户须编辑两个文件：
:::
::: variablelist
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuring_Receive_Flow_Steering_RFS-procsysnetcorerps_sock_flow_entries}[`/proc/sys/net/core/rps_sock_flow_entries`{.filename}]{.term}
:   ::: para
    设置此文件至同时活跃连接数的最大预期值。对于中等服务器负载，推荐值为
    `32768`{.literal} 。所有输入的值四舍五入至最接近的2的幂。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuring_Receive_Flow_Steering_RFS-sysclassnetdevicequeuesrx_queuerps_flow_cnt}[`/sys/class/net/device/queues/rx-queue/rps_flow_cnt`{.filename}]{.term}
:   ::: para
    将 *device* 改为想要配置的网络设备名称（例如，`eth0`{.literal}），将
    *rx-queue* 改为想要配置的接收队列名称（例如，`rx-0`{.literal}）。
    :::
    ::: para
    将此文件的值设为 `rps_sock_flow_entries`{.filename} 除以
    `N`{.literal}，其中 `N`{.literal} 是设备中接收队列的数量。例如，如果
    `rps_flow_entries`{.filename} 设为 `32768`{.literal}，并且有 16
    个配置接收队列，那么 `rps_flow_cnt`{.filename} 就应设为
    `2048`{.literal}。对于单一队列的设备，`rps_flow_cnt`{.filename}
    的值和 `rps_sock_flow_entries`{.filename} 的值是一样的。
    :::
:::
::: para
从单个发送程序接收的数据不会发送至多个
CPU。如果从单个发送程序接收的数据多过单个 CPU
可以处理的数量，须配置更大的帧数以减少中断数量，并以此减少 CPU
的处理工作量。或是考虑 NIC 卸载选项来获得更快的 CPU。
:::
::: para
考虑使用 `numactl`{.command} 或 `taskset`{.command} 与 RFS
相结合，以将应用程序固定至特定的内核、 socket 或 NUMA
节点。这可以有助于防止数据处理紊乱。
:::
:::
::: section
::: titlepage
## [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Configuring_Accelerated_RFS}6.3.8. 配置加速 RFS {.title}
:::
::: para
加速 RFS 是通过添加硬件协助来增速的。如同
RFS，数据转发是基于应用程序处理数据包的位置。但不同于传统 RFS
的是，数据是直接发送至处理数据线程的本地 CPU：即运行应用程序的
CPU，或是对于在缓存层次结构中的 CPU 来说的一个本地 CPU。
:::
::: para
加速 RFS 只有满足以下条件才可以使用：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    网络接口卡须支持加速 RFS。加速 RFS 是由输出
    `ndo_rx_flow_steer()`{.methodname} `netdevice`{.systemitem}
    功能的接口卡支持。
    :::
-   ::: para
    `ntuple`{.literal} 筛选必须启用。
    :::
:::
::: para
一旦满足了这些条件，队列映射 CPU 就会基于传统 RFS
配置自动导出。即队列映射 CPU 会基于由每个接收队列的驱动程序配置的 IRQ
关联而自动导出。从 ＜[第 6.3.7 节 "配置
RFS"](#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Configuring_Receive_Flow_Steering_RFS){.xref}＞
中来获取配置传统 RFS 的信息。
:::
::: para
红帽推荐在可以使用 RFS 以及网络接口卡支持硬件加速时使用加速 RFS 。
:::
:::
:::
:::
[]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html}
::: appendix
::: titlepage
# [⁠]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference}附录 A. 工具参考 {.title}
:::
::: para
此附录为红帽企业版 Linux 7
中多种工具提供快速参考，这些工具可用于调整性能。
工具完整、最新、详细参考资料请参见相关手册页。
:::
::: section
::: titlepage
# [⁠]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference-irqbalance}A.1. irqbalance（中断平衡） {.title}
:::
::: para
[**irqbalance**]{.application}
是一个命令行工具，在处理器中分配硬件中断以提高系统性能。默认设置下在后台程序运行，但只可通过
`--oneshot`{.option} 选项运行一次。
:::
::: para
以下参数可用于提高性能。
:::
::: variablelist
[⁠]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-irqbalance-_powerthresh}[\--powerthresh]{.term}
:   ::: para
    CPU 进入节能模式之前，设定可空闲的 CPU 数量。如果有大于阀值数量的
    CPU 是大于一个标准的偏差，该差值低于平均软中断工作负载，以及没有 CPU
    是大于一个标准偏差，且该偏差高出平均，并有多于一个的 irq
    分配给它们，一个 CPU 将处于节能模式。在节能模式中，CPU 不是
    irqbalance 的一部分，所以它在有必要时才会被唤醒。
    :::
[⁠]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-irqbalance-_hintpolicy}[\--hintpolicy]{.term}
:   ::: para
    决定如何解决 irq 内核关联提示。有效值为 `exact`{.literal}（总是应用
    irq 关联提示）、`subset`{.literal} （irq
    是平衡的，但分配的对象是关联提示的子集）、或者
    `ignore`{.literal}（irq 完全被忽略）。
    :::
[⁠]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-irqbalance-_policyscript}[\--policyscript]{.term}
:   ::: para
    通过设备路径、当作参数的irq号码以及 [**irqbalance**]{.application}
    预期的零退出代码，定义脚本位置以执行每个中断请求。定义的脚本能指定零或多键值对来指导管理传递的
    irq 中 [**irqbalance**]{.application}。
    :::
    ::: para
    下列是为效键值对：
    :::
    ::: variablelist
    [⁠]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-_policyscript-ban}[ban]{.term}
    :   ::: para
        有效值为 `true`{.literal}（从平衡中排除传递的 irq）或
        `false`{.literal}（该 irq 表现平衡）。
        :::
    [⁠]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-_policyscript-balance_level}[balance_level]{.term}
    :   ::: para
        允许用户重写传递的 irq 平衡度。默认设置下，平衡度基于拥有 irq
        设备的 PCI 设备种类。有效值为
        `none`{.literal}、`package`{.literal}、`cache`{.literal}、或
        `core`{.literal}。
        :::
    [⁠]{#appe-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tool_Reference.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-_policyscript-numa_node}[numa_node]{.term}