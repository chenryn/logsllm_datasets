provided at the input layer, they have to be relayed all the way
to the corresponding convolutional or fully-connected layers to
perform the real computation, which introduces a considerable
overhead on the size of the circuit and thus the prover time. Instead,
we design an efficient circuit where each convolutional or fully-
connected layer connects directly to the witness. See Figure 2 for
the structure of our circuit. In this circuit, a generalized addition
gate or multiplication gate takes input from either the layer above
or from the input layer. To support such a structure, we further
extend our protocol above by applying the same techniques in [51].
Following the ideas in [51], we denote the subset of values in
the input layer connecting to the ğ‘–-th layer as ğ‘‰ğ‘–,in of size ğ‘†ğ‘–,in and
ğ‘ ğ‘–,in = âŒˆlog ğ‘†ğ‘–,inâŒ‰, and its multilinear extension as Ëœğ‘‰ğ‘–,in(Â·). We also
separately define the generalized addition gates between the ğ‘–-th
Ëœğ‘‹ğ‘ğ‘‘ğ‘‘ğ‘–,ğ‘–+1(ğ‘§, ğ‘¥),
and the (ğ‘– + 1)-th, the ğ‘–-th and the input layer as
Ëœğ‘‹ğ‘ğ‘‘ğ‘‘ğ‘–,in(ğ‘§, ğ‘¥). Similarly, we define the generalized multiplication
3The technique also works for matrix multiplications. However, Justin Thaler [41]
proposed a better sumcheck for matrix multiplication with a quadratic prover time in
the dimension, and we take his approach in our implementation.
Ëœğ‘‹ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–,ğ‘–+1,ğ‘–+1(ğ‘§, ğ‘¥, ğ‘¦),
Ëœğ‘‹ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–,in,in(ğ‘§, ğ‘¥, ğ‘¦) and
gates respectively as
Ëœğ‘‹ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–,ğ‘–+1,in(ğ‘§, ğ‘¥, ğ‘¦) for inputs both from layer ğ‘– + 1, both from
input layer and one from layer ğ‘– + 1 one from input. With these
definitions, it suffices to write the multilinear extension for layer ğ‘–
in Figure 2 as:
Ëœğ‘‰ğ‘–(ğ‘§) =

+
+
+
+
Ëœğ‘‹ğ‘ğ‘‘ğ‘‘ğ‘–,ğ‘–+1(ğ‘§, ğ‘¥) Â· Ëœğ‘‰ğ‘–+1(ğ‘¥)
Ëœğ‘‹ğ‘ğ‘‘ğ‘‘ğ‘–,in(ğ‘§, ğ‘¥) Â· Ëœğ‘‰ğ‘–,in(ğ‘¥)
Ëœğ‘‹ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–,ğ‘–+1,ğ‘–+1(ğ‘§, ğ‘¥, ğ‘¦) Â· Ëœğ‘‰ğ‘–+1(ğ‘¥) Ëœğ‘‰ğ‘–+1(ğ‘¦)
Ëœğ‘‹ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–,in,in(ğ‘§, ğ‘¥, ğ‘¦) Â· Ëœğ‘‰ğ‘–,in(ğ‘¥) Ëœğ‘‰ğ‘–,in(ğ‘¦)
Ëœğ‘‹ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–,ğ‘–+1,in(ğ‘§, ğ‘¥, ğ‘¦) Â· Ëœğ‘‰ğ‘–+1(ğ‘¥) Ëœğ‘‰ğ‘–,in(ğ‘¦).
ğ‘¥âˆˆ{0,1}ğ‘ ğ‘–+1
ğ‘¥âˆˆ{0,1}ğ‘ ğ‘–,in
ğ‘¥,ğ‘¦âˆˆ{0,1}ğ‘ ğ‘–+1
ğ‘¥,ğ‘¦âˆˆ{0,1}ğ‘ ğ‘–,in
ğ‘¥âˆˆ{0,1}ğ‘ ğ‘–+1 ,
ğ‘¦âˆˆ{0,1}ğ‘ ğ‘–,in
By executing the sumcheck protocol on the equation above, the
verifier and the prover can directly reduce Ëœğ‘‰ğ‘–(ğ‘§) to two evaluations
of Ëœğ‘‰ğ‘–+1(Â·) and two evaluations of Ëœğ‘‰ğ‘–,in(Â·). The prover time is ğ‘‚(ğ‘†ğ‘– +
ğ‘†ğ‘–+1 + ğ‘†ğ‘–,in) as there are a constant number of sums in the equation.
Reducing to a single evaluation of the input. After the sum-
check of layer ğ‘–, the verifier and the prover can proceed to layer ğ‘–+1
in the same way as the GKR protocol 2. However, when reaching
to the input layer, the verifier has received two evaluations about
the input per layer. Moreover, they are evaluations of Ëœğ‘‰ğ‘–,in(Â·), the
subset of ğ‘‰in connected to layer ğ‘–. In order to combine them to a
single evaluation of the multilinear extension of the input Ëœğ‘‰in(Â·),
we take the approach in [51].
Suppose the evaluations received from layer ğ‘– are Ëœğ‘‰ğ‘–,in(ğ‘§ğ‘–,0) and
Ëœğ‘‰ğ‘–,in(ğ‘§ğ‘–,1), the verifier generates ğ‘Ÿğ‘–,0, ğ‘Ÿğ‘–,1 âˆˆ F for layer ğ‘– and com-
bines all the evaluations through a random linear combination:
ğ‘–



ğ‘–
(cid:16)ğ‘Ÿğ‘–,0 Ëœğ‘‰ğ‘–,in(ğ‘§ğ‘–,0) + ğ‘Ÿğ‘–,1 Ëœğ‘‰ğ‘–,in(ğ‘§ğ‘–,1)(cid:17)
(cid:169)(cid:173)(cid:171)ğ‘Ÿğ‘–,0

ğ‘§âˆˆ{0,1}ğ‘ in
Ëœğ‘‰in(ğ‘§)
=
ğ‘§âˆˆ{0,1}ğ‘ in
=
where ğ¶ğ‘–(ğ‘§ğ‘–, ğ‘§) is defined as:
ğ¶ğ‘–(ğ‘§ğ‘–, ğ‘§) =
(cid:40)1,
ğ‘–

(cid:32)
ğ¶ğ‘–(ğ‘§ğ‘–,0, ğ‘§) Ëœğ‘‰in(ğ‘§) + ğ‘Ÿğ‘–,1
(cid:0)ğ‘Ÿğ‘–,0ğ¶ğ‘–(ğ‘§ğ‘–,0, ğ‘§) + ğ‘Ÿğ‘–,1ğ¶ğ‘–(ğ‘§ğ‘–,1, ğ‘§)(cid:1)(cid:33)
ğ‘§âˆˆ{0,1}ğ‘ in
ğ¶ğ‘–(ğ‘§ğ‘–,1, ğ‘§) Ëœğ‘‰in(ğ‘§)(cid:170)(cid:174)(cid:172)
(14)
if the ğ‘§ğ‘–-th value in ğ‘‰ğ‘–,in is the ğ‘§-th value in ğ‘‰in
otherwise
0,
By running the sumcheck protocol on the equation above, the veri-
fier reduces multiple evaluations on Ëœğ‘‰ğ‘–,in(Â·) to a single evaluation
of Ëœğ‘‰in(Â·). The prover time is linear in ğ‘†in and the size of the circuit.
4.2.3 Convolutional layer. In Section 3.2, we proposed an efficient
protocol to verify the result of the 2-D convolution between one
input and one kernel. However, in practice, there are multiple chan-
nels and kernels in each convolutional layer of a CNN, as described
in Section 2.1. It turns out that we can do better than naively re-
peating our protocol for a single convolution multiple times. We
present our improved protocol in this section.
Formally speaking, we represent the computation of an entire
convolutional layer given by Equation 2 by FFT, IFFT and Hadamard
Session 11B: Zero Knowledge II CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2976product. Recall that the input data to a convolutional layer is ğ‘‹ âˆˆ
Fchğ‘–ğ‘›Ã—ğ‘›Ã—ğ‘› and the kernel is ğ‘Š âˆˆ Fchğ‘œğ‘¢ğ‘¡Ã—chğ‘–ğ‘›Ã—ğ‘¤Ã—ğ‘¤. Here with omit
the subscript of layer ğ‘– for the ease of notations. The convolutional
layer computes ğ‘ˆ âˆˆ Fchğ‘œğ‘¢ğ‘¡Ã—(ğ‘›âˆ’ğ‘¤+1)Ã—(ğ‘›âˆ’ğ‘¤+1) where for each 0 â‰¤
ğœ < chğ‘œğ‘¢ğ‘¡, 0 â‰¤ ğ‘—, ğ‘˜ < ğ‘› âˆ’ ğ‘¤ + 1,
(ğ‘¤âˆ’1),(ğ‘¤âˆ’1)
chğ‘–ğ‘›âˆ’1
ğ‘›2âˆ’1âˆ’ğ‘—ğ‘›âˆ’ğ‘˜
ğœ=0
ğ‘ˆ [ğœ, ğ‘—, ğ‘˜] =
chğ‘–ğ‘›âˆ’1
=
ğœ=0
ğ‘–=0
ğ‘‹ [ğœ, ğ‘—, ğ‘˜] Â· ğ‘Š [ğœ, ğœ, ğ‘¡, ğ‘™]
ğ‘¡=0,ğ‘™=0
Â¯ğ‘‹ğœ [ğ‘›2 âˆ’ 1 âˆ’ ğ‘—ğ‘› âˆ’ ğ‘˜ âˆ’ ğ‘–] Â· Â¯ğ‘Šğœ,ğœ [ğ‘–].
This is a generalization of Equation 11, where Â¯ğ‘‹ğœ is the vector
defined by the ğœ-th channel of data ğ‘‹, and Â¯ğ‘Šğœ,ğœ is the vector defined
by the (ğœ, ğœ)-th kernel. If we apply the algorithm in Equation 12
naively, there are chğ‘–ğ‘› Â· chğ‘œğ‘¢ğ‘¡ FFTs and IFFTs and the prover time
is ğ‘‚(chğ‘–ğ‘› Â· chğ‘œğ‘¢ğ‘¡ Â· ğ‘›2). Instead, we utilize the linearity of the FFT
algorithm. Let Â¯ğ‘ˆğœ be the vector defined by the ğœ-th channel of the
output ğ‘ˆ , as we show in Section 3.2, we have
chğ‘–ğ‘›âˆ’1
chğ‘–ğ‘›âˆ’1
(cid:18)chğ‘–ğ‘›âˆ’1
ğœ=0
ğœ=0
= IFFT
Â¯ğ‘ˆğœ =
=
Â¯ğ‘‹ğœ âˆ— Â¯ğ‘Šğœ,ğœ
IFFT(FFT( Â¯ğ‘‹ğœ) âŠ™ FFT( Â¯ğ‘Šğœ,ğœ))
FFT( Â¯ğ‘‹ğœ) âŠ™ FFT( Â¯ğ‘Šğœ,ğœ)
(cid:19)
ğœ=0
(15)
.
Note that the total number of IFFTs in Equation 15 is only chğ‘œğ‘¢ğ‘¡ for
ğœ âˆˆ [chğ‘œğ‘¢ğ‘¡]. By running our sumcheck protocols in Section 3, the
prover time of the IFFT is reduced to ğ‘‚(chğ‘œğ‘¢ğ‘¡ Â·ğ‘›2). Though the total
complexity remains the same, the efficiency in practice is improved.
Moreover, by applying the GKR protocol with our generalized ad-
dition and multiplication gates, the sum of Hadamard products in
Equation 15 can also be validated with a single sumcheck.
4.3 Design of Zero Knowledge CNN
In this section, we present the full design of our zero knowledge
CNN scheme. The structure of our zkCNN is shown in Figure 2.
As shown in the figure, the input consists of the data sample ğ‘‹
for CNN prediction, the secret witness of the CNN model ğ‘Š from
the prover, and the additional auxiliary inputs from the prover for
computing functions such as ReLU and max pooling efficiently. Each
convolutional layer takes the input from the previous layer, takes
the kernels from ğ‘Š and executes our new sumcheck protocol in
Section 3.2 and 4.2. The fully-connected layer takes the input from
the previous layer and the weight matrix from ğ‘Š and executes the
sumcheck protocol for matrix multiplication in [41]. The activation
layer and the pooling layer takes the input from the previous layer
and the auxiliary input, and we explain the details of our design
for these layers below. Such connections are supported by our
generalized GKR protocols in Section 4.2 without any overhead.
Converting real numbers. In practice, the parameters of the CNN
model and the data samples are often represented as real numbers.
In our scheme, we use the existing technique of quantization in [28]
to encode them as integers in the finite field. The quantization
scheme is an affine mapping of integers ğ‘ to real numbers ğ‘. In
particular, ğ‘ = ğ¿(ğ‘ âˆ’ ğ‘), where quantization parameter ğ¿ is a real
number called the scale of the quantization and ğ‘ is an integer
called the zero-point of the quantization. Using the quantization, we
represent each value of the data samples and the model parameters
as a ğ‘„-bit integer ğ‘. For the input matrix to each layer and each
Figure 2: The design of our zkCNN structure.
kernel, there is a single zero-point ğ‘, represented by a ğ‘„ bit integer.
The representation of the scale is explained below.
With this representation, the addition of two real numbers with
the same scale can naturally be expressed as integer addition. In
particular, for ğ‘1 = ğ¿(ğ‘1 âˆ’ ğ‘1) and ğ‘2 = ğ¿(ğ‘2 âˆ’ ğ‘2), ğ‘1 + ğ‘2 =
ğ¿(ğ‘1 + ğ‘2 âˆ’ ğ‘1 âˆ’ ğ‘2). To perform real-number multiplications with
different scales, i.e., ğ‘3 = ğ‘1 Â· ğ‘2, we have
ğ¿3(ğ‘3 âˆ’ ğ‘3) = ğ¿1(ğ‘1 âˆ’ ğ‘1) Â· ğ¿2(ğ‘2 âˆ’ ğ‘2)
â‡” ğ‘3 = ğ‘3 + ğ¿1ğ¿2
(ğ‘1 âˆ’ ğ‘1) Â· (ğ‘2 âˆ’ ğ‘2).
ğ¿3
Everything except ğ¿1ğ¿2
is an integer and can be computed directly
ğ¿3
by the arithmetic circuit. Following the approach in [28], let ğ‘’ =
be a real number, we normalize it as 2âˆ’ğ¸ Â· Â¯ğ‘’, where Â¯ğ‘’ is an
ğ¿1ğ¿2
ğ¿3
integer called the normalized scale. Therefore, ğ‘3 = ğ‘3+2âˆ’ğ¸ Â· Â¯ğ‘’Â·(ğ‘1âˆ’
ğ‘1) Â· (ğ‘2 âˆ’ ğ‘2) where the multiplications are over integers in the
finite field and 2âˆ’ğ¸ can be computed using a bit decomposition and
shift in the arithmetic circuit. In this way, similar to the zero-point
ğ‘, the normalized scale Â¯ğ‘’ for the entire layer is also provided by
the prover as part of the model.
The equations above naturally generalizes to the convolutions
and matrix multiplications, as they consist of multiplications and
additions with the same scale. To verify these computations in
our protocol, the sumcheck protocols can be executed directly on
(ğ‘ âˆ’ ğ‘) in the finite field. The normalized scale and the zero-point
will be incorporated at the end. Moreover, because of the properties
of ReLU and max pooling which will be presented below, they can
also be computed on (ğ‘âˆ’ ğ‘) and the scaling can be deferred further
to the output of the pooling.
Computing ReLU. The ReLU function ReLU(ğ‘¥) = max(ğ‘¥, 0) is
applied element-wise after each convolutional layer. In our design,
we denote a negative value ğ‘¥ as ğ‘ âˆ’ |ğ‘¥| in the finite field, where |ğ‘¥|
is the absolute value of ğ‘¥. Suppose |ğ‘¥| is in the range [0, 2ğ‘„ âˆ’ 1],
i.e. |ğ‘¥| can be represented by ğ‘„ bits (the same as the quantization
ğ‘ above), then we ask the prover to provide the bit decomposition