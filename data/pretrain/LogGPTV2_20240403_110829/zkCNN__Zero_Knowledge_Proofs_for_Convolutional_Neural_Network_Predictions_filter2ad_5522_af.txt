provided at the input layer, they have to be relayed all the way
to the corresponding convolutional or fully-connected layers to
perform the real computation, which introduces a considerable
overhead on the size of the circuit and thus the prover time. Instead,
we design an efficient circuit where each convolutional or fully-
connected layer connects directly to the witness. See Figure 2 for
the structure of our circuit. In this circuit, a generalized addition
gate or multiplication gate takes input from either the layer above
or from the input layer. To support such a structure, we further
extend our protocol above by applying the same techniques in [51].
Following the ideas in [51], we denote the subset of values in
the input layer connecting to the 𝑖-th layer as 𝑉𝑖,in of size 𝑆𝑖,in and
𝑠𝑖,in = ⌈log 𝑆𝑖,in⌉, and its multilinear extension as ˜𝑉𝑖,in(·). We also
separately define the generalized addition gates between the 𝑖-th
˜𝑋𝑎𝑑𝑑𝑖,𝑖+1(𝑧, 𝑥),
and the (𝑖 + 1)-th, the 𝑖-th and the input layer as
˜𝑋𝑎𝑑𝑑𝑖,in(𝑧, 𝑥). Similarly, we define the generalized multiplication
3The technique also works for matrix multiplications. However, Justin Thaler [41]
proposed a better sumcheck for matrix multiplication with a quadratic prover time in
the dimension, and we take his approach in our implementation.
˜𝑋𝑚𝑢𝑙𝑡𝑖,𝑖+1,𝑖+1(𝑧, 𝑥, 𝑦),
˜𝑋𝑚𝑢𝑙𝑡𝑖,in,in(𝑧, 𝑥, 𝑦) and
gates respectively as
˜𝑋𝑚𝑢𝑙𝑡𝑖,𝑖+1,in(𝑧, 𝑥, 𝑦) for inputs both from layer 𝑖 + 1, both from
input layer and one from layer 𝑖 + 1 one from input. With these
definitions, it suffices to write the multilinear extension for layer 𝑖
in Figure 2 as:
˜𝑉𝑖(𝑧) =

+
+
+
+
˜𝑋𝑎𝑑𝑑𝑖,𝑖+1(𝑧, 𝑥) · ˜𝑉𝑖+1(𝑥)
˜𝑋𝑎𝑑𝑑𝑖,in(𝑧, 𝑥) · ˜𝑉𝑖,in(𝑥)
˜𝑋𝑚𝑢𝑙𝑡𝑖,𝑖+1,𝑖+1(𝑧, 𝑥, 𝑦) · ˜𝑉𝑖+1(𝑥) ˜𝑉𝑖+1(𝑦)
˜𝑋𝑚𝑢𝑙𝑡𝑖,in,in(𝑧, 𝑥, 𝑦) · ˜𝑉𝑖,in(𝑥) ˜𝑉𝑖,in(𝑦)
˜𝑋𝑚𝑢𝑙𝑡𝑖,𝑖+1,in(𝑧, 𝑥, 𝑦) · ˜𝑉𝑖+1(𝑥) ˜𝑉𝑖,in(𝑦).
𝑥∈{0,1}𝑠𝑖+1
𝑥∈{0,1}𝑠𝑖,in
𝑥,𝑦∈{0,1}𝑠𝑖+1
𝑥,𝑦∈{0,1}𝑠𝑖,in
𝑥∈{0,1}𝑠𝑖+1 ,
𝑦∈{0,1}𝑠𝑖,in
By executing the sumcheck protocol on the equation above, the
verifier and the prover can directly reduce ˜𝑉𝑖(𝑧) to two evaluations
of ˜𝑉𝑖+1(·) and two evaluations of ˜𝑉𝑖,in(·). The prover time is 𝑂(𝑆𝑖 +
𝑆𝑖+1 + 𝑆𝑖,in) as there are a constant number of sums in the equation.
Reducing to a single evaluation of the input. After the sum-
check of layer 𝑖, the verifier and the prover can proceed to layer 𝑖+1
in the same way as the GKR protocol 2. However, when reaching
to the input layer, the verifier has received two evaluations about
the input per layer. Moreover, they are evaluations of ˜𝑉𝑖,in(·), the
subset of 𝑉in connected to layer 𝑖. In order to combine them to a
single evaluation of the multilinear extension of the input ˜𝑉in(·),
we take the approach in [51].
Suppose the evaluations received from layer 𝑖 are ˜𝑉𝑖,in(𝑧𝑖,0) and
˜𝑉𝑖,in(𝑧𝑖,1), the verifier generates 𝑟𝑖,0, 𝑟𝑖,1 ∈ F for layer 𝑖 and com-
bines all the evaluations through a random linear combination:
𝑖



𝑖
(cid:16)𝑟𝑖,0 ˜𝑉𝑖,in(𝑧𝑖,0) + 𝑟𝑖,1 ˜𝑉𝑖,in(𝑧𝑖,1)(cid:17)
(cid:169)(cid:173)(cid:171)𝑟𝑖,0

𝑧∈{0,1}𝑠in
˜𝑉in(𝑧)
=
𝑧∈{0,1}𝑠in
=
where 𝐶𝑖(𝑧𝑖, 𝑧) is defined as:
𝐶𝑖(𝑧𝑖, 𝑧) =
(cid:40)1,
𝑖

(cid:32)
𝐶𝑖(𝑧𝑖,0, 𝑧) ˜𝑉in(𝑧) + 𝑟𝑖,1
(cid:0)𝑟𝑖,0𝐶𝑖(𝑧𝑖,0, 𝑧) + 𝑟𝑖,1𝐶𝑖(𝑧𝑖,1, 𝑧)(cid:1)(cid:33)
𝑧∈{0,1}𝑠in
𝐶𝑖(𝑧𝑖,1, 𝑧) ˜𝑉in(𝑧)(cid:170)(cid:174)(cid:172)
(14)
if the 𝑧𝑖-th value in 𝑉𝑖,in is the 𝑧-th value in 𝑉in
otherwise
0,
By running the sumcheck protocol on the equation above, the veri-
fier reduces multiple evaluations on ˜𝑉𝑖,in(·) to a single evaluation
of ˜𝑉in(·). The prover time is linear in 𝑆in and the size of the circuit.
4.2.3 Convolutional layer. In Section 3.2, we proposed an efficient
protocol to verify the result of the 2-D convolution between one
input and one kernel. However, in practice, there are multiple chan-
nels and kernels in each convolutional layer of a CNN, as described
in Section 2.1. It turns out that we can do better than naively re-
peating our protocol for a single convolution multiple times. We
present our improved protocol in this section.
Formally speaking, we represent the computation of an entire
convolutional layer given by Equation 2 by FFT, IFFT and Hadamard
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2976product. Recall that the input data to a convolutional layer is 𝑋 ∈
Fch𝑖𝑛×𝑛×𝑛 and the kernel is 𝑊 ∈ Fch𝑜𝑢𝑡×ch𝑖𝑛×𝑤×𝑤. Here with omit
the subscript of layer 𝑖 for the ease of notations. The convolutional
layer computes 𝑈 ∈ Fch𝑜𝑢𝑡×(𝑛−𝑤+1)×(𝑛−𝑤+1) where for each 0 ≤
𝜏 < ch𝑜𝑢𝑡, 0 ≤ 𝑗, 𝑘 < 𝑛 − 𝑤 + 1,
(𝑤−1),(𝑤−1)
ch𝑖𝑛−1
𝑛2−1−𝑗𝑛−𝑘
𝜎=0
𝑈 [𝜏, 𝑗, 𝑘] =
ch𝑖𝑛−1
=
𝜎=0
𝑖=0
𝑋 [𝜎, 𝑗, 𝑘] · 𝑊 [𝜏, 𝜎, 𝑡, 𝑙]
𝑡=0,𝑙=0
¯𝑋𝜎 [𝑛2 − 1 − 𝑗𝑛 − 𝑘 − 𝑖] · ¯𝑊𝜏,𝜎 [𝑖].
This is a generalization of Equation 11, where ¯𝑋𝜎 is the vector
defined by the 𝜎-th channel of data 𝑋, and ¯𝑊𝜏,𝜎 is the vector defined
by the (𝜏, 𝜎)-th kernel. If we apply the algorithm in Equation 12
naively, there are ch𝑖𝑛 · ch𝑜𝑢𝑡 FFTs and IFFTs and the prover time
is 𝑂(ch𝑖𝑛 · ch𝑜𝑢𝑡 · 𝑛2). Instead, we utilize the linearity of the FFT
algorithm. Let ¯𝑈𝜏 be the vector defined by the 𝜏-th channel of the
output 𝑈 , as we show in Section 3.2, we have
ch𝑖𝑛−1
ch𝑖𝑛−1
(cid:18)ch𝑖𝑛−1
𝜎=0
𝜎=0
= IFFT
¯𝑈𝜏 =
=
¯𝑋𝜎 ∗ ¯𝑊𝜏,𝜎
IFFT(FFT( ¯𝑋𝜎) ⊙ FFT( ¯𝑊𝜏,𝜎))
FFT( ¯𝑋𝜎) ⊙ FFT( ¯𝑊𝜏,𝜎)
(cid:19)
𝜎=0
(15)
.
Note that the total number of IFFTs in Equation 15 is only ch𝑜𝑢𝑡 for
𝜏 ∈ [ch𝑜𝑢𝑡]. By running our sumcheck protocols in Section 3, the
prover time of the IFFT is reduced to 𝑂(ch𝑜𝑢𝑡 ·𝑛2). Though the total
complexity remains the same, the efficiency in practice is improved.
Moreover, by applying the GKR protocol with our generalized ad-
dition and multiplication gates, the sum of Hadamard products in
Equation 15 can also be validated with a single sumcheck.
4.3 Design of Zero Knowledge CNN
In this section, we present the full design of our zero knowledge
CNN scheme. The structure of our zkCNN is shown in Figure 2.
As shown in the figure, the input consists of the data sample 𝑋
for CNN prediction, the secret witness of the CNN model 𝑊 from
the prover, and the additional auxiliary inputs from the prover for
computing functions such as ReLU and max pooling efficiently. Each
convolutional layer takes the input from the previous layer, takes
the kernels from 𝑊 and executes our new sumcheck protocol in
Section 3.2 and 4.2. The fully-connected layer takes the input from
the previous layer and the weight matrix from 𝑊 and executes the
sumcheck protocol for matrix multiplication in [41]. The activation
layer and the pooling layer takes the input from the previous layer
and the auxiliary input, and we explain the details of our design
for these layers below. Such connections are supported by our
generalized GKR protocols in Section 4.2 without any overhead.
Converting real numbers. In practice, the parameters of the CNN
model and the data samples are often represented as real numbers.
In our scheme, we use the existing technique of quantization in [28]
to encode them as integers in the finite field. The quantization
scheme is an affine mapping of integers 𝑞 to real numbers 𝑎. In
particular, 𝑎 = 𝐿(𝑞 − 𝑍), where quantization parameter 𝐿 is a real
number called the scale of the quantization and 𝑍 is an integer
called the zero-point of the quantization. Using the quantization, we
represent each value of the data samples and the model parameters
as a 𝑄-bit integer 𝑞. For the input matrix to each layer and each
Figure 2: The design of our zkCNN structure.
kernel, there is a single zero-point 𝑍, represented by a 𝑄 bit integer.
The representation of the scale is explained below.
With this representation, the addition of two real numbers with
the same scale can naturally be expressed as integer addition. In
particular, for 𝑎1 = 𝐿(𝑞1 − 𝑍1) and 𝑎2 = 𝐿(𝑞2 − 𝑍2), 𝑎1 + 𝑎2 =
𝐿(𝑞1 + 𝑞2 − 𝑍1 − 𝑍2). To perform real-number multiplications with
different scales, i.e., 𝑎3 = 𝑎1 · 𝑎2, we have
𝐿3(𝑞3 − 𝑍3) = 𝐿1(𝑞1 − 𝑍1) · 𝐿2(𝑞2 − 𝑍2)
⇔ 𝑞3 = 𝑍3 + 𝐿1𝐿2
(𝑞1 − 𝑍1) · (𝑞2 − 𝑍2).
𝐿3
Everything except 𝐿1𝐿2
is an integer and can be computed directly
𝐿3
by the arithmetic circuit. Following the approach in [28], let 𝑒 =
be a real number, we normalize it as 2−𝐸 · ¯𝑒, where ¯𝑒 is an
𝐿1𝐿2
𝐿3
integer called the normalized scale. Therefore, 𝑞3 = 𝑍3+2−𝐸 · ¯𝑒·(𝑞1−
𝑍1) · (𝑞2 − 𝑍2) where the multiplications are over integers in the
finite field and 2−𝐸 can be computed using a bit decomposition and
shift in the arithmetic circuit. In this way, similar to the zero-point
𝑍, the normalized scale ¯𝑒 for the entire layer is also provided by
the prover as part of the model.
The equations above naturally generalizes to the convolutions
and matrix multiplications, as they consist of multiplications and
additions with the same scale. To verify these computations in
our protocol, the sumcheck protocols can be executed directly on
(𝑞 − 𝑍) in the finite field. The normalized scale and the zero-point
will be incorporated at the end. Moreover, because of the properties
of ReLU and max pooling which will be presented below, they can
also be computed on (𝑞− 𝑍) and the scaling can be deferred further
to the output of the pooling.
Computing ReLU. The ReLU function ReLU(𝑥) = max(𝑥, 0) is
applied element-wise after each convolutional layer. In our design,
we denote a negative value 𝑥 as 𝑝 − |𝑥| in the finite field, where |𝑥|
is the absolute value of 𝑥. Suppose |𝑥| is in the range [0, 2𝑄 − 1],
i.e. |𝑥| can be represented by 𝑄 bits (the same as the quantization
𝑞 above), then we ask the prover to provide the bit decomposition