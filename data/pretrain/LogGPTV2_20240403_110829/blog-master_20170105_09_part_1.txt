## PostgreSQL 流式数据处理(聚合、过滤、转换...)系列 - 9                
### 作者                                                                         
digoal                                                                          
### 日期                                                                         
2017-01-05                                                                              
### 标签                                                                        
PostgreSQL , 流式 , 函数 , 流式处理 , 异步统计 , count , group , agg , 触发器 , xid , 事务隔离 , 异步气泡 , gap , function , 串行处理
----                                                                        
## 背景                     
2013年帮朋友做的方案。写了一些列文档来解决当时某个大数据BI平台的异步流式数据处理的功能。                  
逐步优化，化繁为简。                     
在业务层面，统计，数据的过滤，数据的清洗，数据的事件触发等。是比较常见的需求。                      
比如以COUNT就是一个很典型的例子。                  
在9.2以前全表的count只能通过扫描全表来得到, 即使有pk也必须扫描全表.                  
9.2版本增加了index only scan的功能, count(*)可以通过仅仅扫描pk就可以得到.                  
但是如果是一个比较大的表, pk也是很大的, 扫描pk也是个不小的开销.                  
到了9.6，开始支持并行查询，通过并行，一张1亿的表，COUNT可能只需要几百毫秒。这是一个质的飞跃。（但是还有很多时候用并行并不是最好的）                  
另外社区也除了一个流式处理的数据库，pipelineDB，但是它的社区版本限制了一个DATABASE只能使用1024个流视图，在编码的地方使用了1BYTE存储CV。                  
那么回到postgresql数据库本身，有没有办法来优化count全表的操作呢, 如果你的场景真的有必要频繁的count全表, 那么可以尝试一下使用以下方法来优化你的场景.                  
## 正文                  
对于insert-only的大数据实时COUNT统计, 我以前写过8篇相关的文章来实现并行插入的环境下, 如何实现实时的COUNT统计.  
按照XID做切片, 每次统计一些XID的数据, 一个事务中插入的数据必须一次处理完.   
但是当插入的数据是批量插入时, 例如一个事务中插入了几百万记录, 那么使用原来的方法, 对于一个事务的取数是在一次性完成的,   
这种情况, 我们更希望一个事务包含的几百万记录被拆分成多个来统计.  
思路是增加一个序列字段, 这样操作的话对于一个事务插入的一批数据: XID一致, 但是序列字段的值不一致.   
这样可以用来对单个事务包含的几百万记录拆分成多个数据分片来统计.  
当然取数据的逻辑也会变得更加复杂, 个人不建议这么来做.  
还是推荐使用如下方法  
为什么要用XID做切片呢, 原因很简单, 因为如果只用序列或者时间类型的自增字段来对数据切片的话, 对于并发插入的场景, 可能导致某些数据取不到.  
例如 :   
```  
digoal=# create table log(id serial8 primary key, info text);  
CREATE TABLE  
SESSION A :  
digoal=# begin;  
BEGIN  
digoal=# insert into log (info) values ('test1');  
INSERT 0 1  
SESSION B :   
digoal=# begin;  
BEGIN  
digoal=# insert into log (info) values ('test2');  
INSERT 0 1  
digoal=# end;  
COMMIT  
SESSION C :   
按ID增长取数据,  
digoal=# select * from log where id2 and id'2013-12-30 17:14:03.37241' and crt_time vi test.sql  
insert into log(c1) values(1);  
pg94@db-172-16-3-150-> pgbench -M prepared -n -r -f ./test.sql -c 1 -j 1 -T 30  
transaction type: Custom query  
scaling factor: 1  
query mode: prepared  
number of clients: 1  
number of threads: 1  
duration: 30 s  
number of transactions actually processed: 323891  
latency average: 0.093 ms  
tps = 10796.333558 (including connections establishing)  
tps = 10797.279037 (excluding connections establishing)  
statement latencies in milliseconds:  
        0.091609        insert into log(c1) values(1);  
```  
批量提交, 一次600条. 换算后最终结果是插入76200条记录每秒.  
```  
pg94@db-172-16-3-150-> pgbench -M prepared -n -r -f ./test.sql -c 1 -j 1 -T 30  
transaction type: Custom query  
scaling factor: 1  
query mode: prepared  
number of clients: 1  
number of threads: 1  
duration: 30 s  
number of transactions actually processed: 3816  
latency average: 7.862 ms  
tps = 127.175476 (including connections establishing)  
tps = 127.186531 (excluding connections establishing)  
statement latencies in milliseconds:  
        7.859686        insert into log(c1) values(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1),(1);  
```  
把序列的缓存调到1000后, 单步插入性能没有变化, 批量插入性能提升到88613万条每秒.  
```  
digoal=# alter sequence log_id_seq cache 1000;  