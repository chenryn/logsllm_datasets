data analysis Starting from raw or processed spectra, data can be normalised, aligned and scaled interactively prior to bucketing peaks and 
multiplets for downstream statistical analysis The software has been developed by Dr Arturas Grauslys under the guidance of Prof Andy Jones who 
has a proven track record in developing server-based software solutions for MS proteomics community and it is hoped that functionality (as well as 
integration with other phenomenal tools) will expand with sufficient uptake by the NMR metabolomics community
P-487
An IROA-based modified MSTUS Normalization corrects non-IROA sample-to-sample metabolite variation
PRESENTING AUTHOR: FELICE DE JONG, IROA TECHNOLOGIES LLC, United States
CO-AUTHORS: Chris Beecher
The IROA TruQuant (TQ) protocol uses a Long-Term Reference Standard (LTRS), a defined chemical mixture containing hundreds of metabolites, plus 
an Internal Standard (TQ-IS) that is chemically identical but isotopically different, to measure instrument performance and provide verifiable chemical 
Identification Using these standards we have previously shown the ability to correct for the ion  suppression of natural abundance experimental 
compounds that are paired with compounds in the TQ-IS, and we have further shown that once ion suppression is corrected, sample-to-sample 
normalization may be achieved using a modified MSTUS algorithm, in which unlike the original MSTUS algorithm, the experimental compounds are 
normalized to their Internal Standard counterparts In this poster we provide a comparison of the original MSTUS-based normalization algorithm to 
this IROA-based modified-MSTUS algorithm to demonstrate that IROA-based normalization is not only significantly more accurate within a single 
experiment, but by normalizing to a standard mixture, will normalize not only sample-to-sample intraday, but also interday (day-to-day) analyses In 
addition, we selected several compounds that were not present in either the LTRS or TQ-IS to demonstrate that the same normalization factor used 
to normalize compounds for which we had internal standards could be applied to compounds that did not have internal standards While they did 
not normalize as accurately without the ability to correct for ion suppression, the ability to normalize was greatly improved compared with that of the 
original MSTUS algorithm
Page 230
POSTER SESSIONS 1 AND 2 – Monday and Tuesday – all odd number posters will be on display.POSTER SESSIONS 3 AND 4 – Wednesday and Thursday – all even number posters will be on display.TECHNOLOGY15th Annual Conference of the Metabolomics Society*AWARD WINNERSTECHNOLOGY
P-488
ASCA-fusion of NMR Metabolomics Data Reveals Metabolic Alternations in Response to Bioactive Milk Ingredients in Preterm 
Piglets with Intra-amniotic Inflammation
PRESENTING AUTHOR: Masoumeh Alinaghi, Department of Food Science, Aarhus University, Denmark
CO-AUTHORS: Duc Ninh Nguyen, Per Torp Sangild, Hanne Christine Bertram, Johan A. Westerhuis
Background: Analysis of multiple compartments of the same piglets can provide complementary information about the studied biological system 
However, in such complicated systems, distinguishing different experimental variations by ANOVA-simultaneous component analysis (ASCA) 
prior to data fusion could be of importance In this study, a dietary intervention for prenatal inflammation1 is investigated by analysis of urine and 
gut samples from the same piglets using the ASCA-fusion approach Methods: Preterm pigs (n=17), subjected to intra-amniotic lipopolysaccharide 
(LPS, 1 mg/fetus), were administered i) standard formula, ii) bovine colostrum or iii) caseinoglycomacropeptide for five days Collected urine and 
gut contents were analyzed by 1H NMR-based metabolomics ASCA was used to separate the various sources of variation in the data related 
to experimental factors (time and treatment), while fusion of the ASCA-decomposed matrices was applied by using penalized exponential 
simultaneous component analysis (P-ESCA)2 to distinguish the common and distinct variation associated with the dietary intervention Results: 
ASCA-fusion of the urine and gut metabolomes reveals the common alternation of lactate, glucose, acetate levels as well as disaccharides in 
response to the different dietary interventions However, common responses related to the sex of piglets are also observed Conclusion: ASCA-P-
ESCA improves the understanding of metabolomics alternations in urine and gut content by separation of induced variations and by finding common 
and distinct variations in both compartments 1 Nguyen, DN, et al, The American journal of pathology, 18811 (2018), 2629-2643 2 Song, Y, et al, 
(2019), arXiv: 190206241
P-489
AutoTuner - high fidelity, robust, and rapid data processing parameter selection tool for metabolomics data
PRESENTING AUTHOR: Craig Mclean, MIT, United States
CO-AUTHORS: Elizabeth B Kujawinski
Untargeted metabolomics experiments have the capability to capture an unbiased snapshot of cellular metabolism but remain challenging due 
to the computational complexity involved in data processing and analysis Raw data must be processed to remove noise and to align features 
across samples through software tools like XCMS or MzMine2, resulting in a table of features with paired mass-to-charge (m/z) and retention-time 
(RT) values This processing step requires dataset-specific parameters Several optimization methods exist, but each design includes undesirable 
drawbacks Here, we present a new method, AutoTuner, designed to optimize data processing parameters based on a novel paradigm Instead of 
maximizing an optimization function like its predecessors, AutoTuner relies on statistical inferences within the distribution of raw data We tested 
the accuracy and the run time of AutoTuner against the most common parameter selection tool, isotopologue parameter optimization (IPO) We also 
analyzed how parameter selection for AutoTuner and IPO influenced the quality of feature tables after XCMS In our presentation, we will show that 
AutoTuner is a desirable alternative to existing tools, with substantially shorter computational times, easy implementation into existing metabolomics 
pipelines, and openly available to software developers
P-490
NonTplus – a new R package for the high throughput processing of high resolution mass spectrometry data
PRESENTING AUTHOR: Tobias Schulze, Helmholtz Centre for Environmental Research - UFZ, Germany
CO-AUTHORS: Erik Müller, Caroline Huber, Marc Stöhr, Werner Brack, Martin Krauss
High resolution mass spectrometry is a key analytical technology in the identification of targets, suspects and unknowns for the profiling of 
environmental or human samples In larger scale environmental surveys or human cohort studies, hundreds or thousands of samples could 
emerge Assuming that these samples are measured at least once in positive and negative mode, a large number of raw mass spectral files is 
derived NonTplus is a new pipeline which automatizes the whole processing including peak picking, gap filling, blank peak elimination, peak 
alignment, annotation, quantification and the export of peak lists for post-analysis NonTplus implements a new gap filling algorithm which improves 
missing value imputation with less false positives compared to existing algorithms The pipeline is tailored to run on a HPC cluster and a future 
implementation in Galaxy is planned
Page 231
POSTER SESSIONS 1 AND 2 – Monday and Tuesday – all odd number posters will be on display.POSTER SESSIONS 3 AND 4 – Wednesday and Thursday – all even number posters will be on display.TECHNOLOGY15th Annual Conference of the Metabolomics Society*AWARD WINNERSTECHNOLOGY
P-491
On the Interpretability of O-PLS Filtered Models
PRESENTING AUTHOR: Barry M. Wise, Eigenvector Research, Inc., United States
CO-AUTHORS: Federico Marini, Frank Westad
Orthogonal PLS, introduced originally by Trygg and Wold in 2002 [1], is a patented algorithm that has received much attention for its perceived 
ability to simplify and thus improve regression and classification model interpretation Since its introduction, it has been shown by Ergon [2] and 
Kemsley and Tapp [3] that results identical to the original O-PLS formulation can be obtained by post-processing conventional PLS models in a 
non-patented way This demonstrated, unequivocally, that O-PLS models have predictive properties identical to their non-rotated versions The 
authors did not, however, consider the interpretability of the models at length In this poster we explore the issue of interpretability of O-PLS models 
by applying the method to carefully constructed simple systems and well characterized data It is demonstrated that O-PLS results match the true 
underlying (first principle) components under only very specific conditions which are typically not met O-PLS results are shown to be a strong 
function of the correlation between the factor of interest and interfering components even under mild conditions, (small correlations) In simulated 
binary expression data it is shown that O-PLS is actually more sensitive to chance correlation than the conventional PLS regression vector [1] 
J Trygg and S Wold, “Orthogonal Projections to Latent Structures (O-PLS),” J Chemo, 16, 119-128, 2002 [2] R Ergon, “PLS post-processing by 
similarity transformation (PLS+ST): a simple alternative to OPLS,” J Chemo, 19, 1-4, 2005 [3] EK Kemsley and HS Tapp, “OPLS filtered data can be 
obtained directly from non-orthogonalized PLS1,” J Chemo, 23, 263-264, 2009
P-492
An Improved Lipid Profiling Workflow Demonstrates Disrupted Lipogenesis Induced with Drug Treatment in Leukemia Cells
PRESENTING AUTHOR: MARK SARTAIN, Agilent Technologies, United States
CO-AUTHORS: Genevieve Van de Bittner, Xiangdong Li, Jeremy Koelmel, Adithya Murali, Sarah Stow
While shotgun lipidomics has advanced the field of lipid analysis, it suffers from limitations including the failure to distinguish isobaric species which 
may be of biological importance This has led to a shift in the field of lipidomics to chromatographic-based lipid profiling approaches using high 
performance liquid chromatography coupled to high resolution mass spectrometry Confident lipid annotation requires data acquisition at the MS/
MS level to enable product-ion spectral matching against in silico generated databases In this study, a novel software tool was employed which 
uses Bayesian scoring to assign lipid class annotation and a non-negative least squares fit with a theoretical lipid library (LipidBlast) to annotate 
the iterative mode MS/MS spectra The tool takes special care not to overannotate lipid entities by only providing the level of structural information 
confidently informed by the MS/MS spectra The tool quickly generates an accurate mass-retention time database in an automated fashion, and the 
resulting database annotates MS1 lipid profiling data We applied this novel workflow to study lipidome alterations of the acute-myeloid-leukemia 
K562 cell line in response to a combination of the drug candidates bezafibrate (BEZ) and medroxyprogesterone acetate (MPA) The resulting 
analysis revealed several cellular changes in response to drug treatment, including a decrease in diacylglycerols, an increase in triacylglycerols, 
and differences in fatty acyl components Utilization of the new lipid analysis workflow provided a more comprehensive lipid annotation than can be 
achieved by traditional approaches, and the results supported that BEZ/MPA may exert anticancer properties through disruption of lipogenesis
P-493
Developing a systematized and integrated workflow for large-scale LC-MS data processing and cross-study investigations
PRESENTING AUTHOR: Sajjan Mehta, University of California, Davis, United States
CO-AUTHORS: Gert Wohlgemuth, Diego Pedrosa, Sili Fan, Oliver Fiehn
We present on the development of an enterprise-grade LC-MS data processing pipeline, LC-BinBase, and its integration with existing software 
to serve as an automated data management strategy for metabolomics and lipidomics studies  LC-BinBase implements MS-DIAL concepts and 
techniques to perform standardized data processing and feature annotation in a highly scalable capacity by utilizing cloud-based AWS Lambda 
services and Fargate clusters Sample and study management is handled by the in-house MiniX and Stasis services and defines all sample 
preparations and quality controls Acquired data are pre-processed and converted on the fly to the open mzML data format and are stored on 
AWS S3 for processing long-term storage The converted samples are automatically scheduled for data processing by LC-BinBase using the study 
definition, instrument type and matrix information to tune the processing parameters Feature identification uses m/z-RT libraries and MS/MS mass 
spectral libraries that are method-specific to minimize erroneous annotations Identified features are stored in a dedicated instance of MassBank 
of North America (MoNA), and later consensus spectra are uploaded to the public MoNA database as metabolites are identified and manually 
confirmed Each sample’s validated annotations along with its matrix information are aggregated by BinVestigate to provide unique insight into 
the prevalence of individual metabolites within specific species and organs or components from across all historical studies In addition, the use of 
standardized quality controls and retention time normalization enables cross-study investigation utilizing Systematic Error Removal using Random 
Forest (SERRF) for robust sample normalization
Page 232
POSTER SESSIONS 1 AND 2 – Monday and Tuesday – all odd number posters will be on display.POSTER SESSIONS 3 AND 4 – Wednesday and Thursday – all even number posters will be on display.TECHNOLOGY15th Annual Conference of the Metabolomics Society*AWARD WINNERSTECHNOLOGY
P-494
BioMagResBank: Database and Tools for NMR Metabolomics Analysis
PRESENTING AUTHOR: Pedro Romero, University of Wisconsin Madison, United States
CO-AUTHORS: Hamid R. Eghbalnia, Hesam Dashti, Naohiro Kobayashi, Jonathan R. Wedell, Kumaran Baskaran, Takeshi Iwata,  
Masashi Yokochi, Dimitri Maziuk, Hongyang Yao, Toshimichi Fujiwara, Genji Kurusu, Eldon L. Ulrich, Jeffrey C. Hoch, John L. Markley
The Biological Magnetic Resonance Data Bank (BioMagResBank or BMRB), founded in 1988, serves as the archive for data generated by Nuclear 
Magnetic Resonance (NMR) spectroscopy of biological systems NMR spectroscopy is unique among biophysical approaches in its ability to provide 
a broad range of atomic and higher-level information relevant to the structural, dynamic, and chemical properties of biological macromolecules, as 
well as reporting on metabolite and natural product concentrations in complex mixtures and their chemical structures BMRB stores experimental 
and derived data from biomolecular NMR studies on both biopolymers and bioactive small compounds BMRB supports metabolomics NMR studies 
through a library of a variety of 1D and 2D NMR spectra of pure compounds (including metabolites, natural products, drugs, and compounds used for 
screening in drug discovery) and through its adoption of novel analytic tools, like the ALATIS unique atom identifiers, which are universal and based 
solely on the 3D structure of the compound and the InChI convention, and GISSMO spin matrices, which enable accurate simulation of compound 
and mixture spectra at any field strength The combination of unique ALATIS naming and parameterized spectra offers the users of BMRB data a 
distinctive benefit in terms of robustness and reproducibility, as embodied in the FAIR principles for data resources, which are that data should be 
Findable, Accessible, Interoperable, and Reusable Supported by NIH Grants NIH Grants R01GM 109046, P41GM103399, and P41GM111135R01
P-495
Estimating Partial Correlation Networks Leveraging Prior Information with Applications to Metabolomics Data
PRESENTING AUTHOR: George Michailidis, University of Florida, United States
CO-AUTHORS: Jiahe Lin, Alla Karnovsky, Gayatri Iyer, William Duren
The problem of estimating networks from high-dimensional Omics data has received a lot of attention due to their usefulness in providing 
insights into interactions amongst biomolecules under different diseases or experimental conditions Partial correlation networks provide a useful 
technical framework for the task at hand However, the limited number of samples available in many studies leads to estimation of very sparse (and 
fragmented) networks that makes them hard to interpret To that end, we propose an estimation framework that leverages external information 
provided in the form of similarities across network edges We formulate a regularized pseudo-likelihood framework, develop a fast distributed 
proximal gradient descent algorithm to compute the network structure and discuss selection of tuning parameters We illustrate the proposed 
framework and the resulting algorithm by reconstructing and comparing the networks obtained from metabolomic and lipodomic profiling of 
three groups of samples, one suffering from Crohn’s disease, another from ulcerative colitis and the third of normal controls We further tested for 
enrichment modules extracted from the estimated networks that identified important alterations in both network structure and expression levels of 
interacting metabolites/lipids across the three groups of interest
P-496
Updates to xcms: simplified raw data access and enhanced MS level > 1 capabilities
PRESENTING AUTHOR: Johannes Rainer, Institute for Biomedicine, Eurac Research, Italy
CO-AUTHORS: Laurent Gatto, Steffen Neumann
The xcms Bioconductor package is one of the standard toolboxes for the preprocessing of untargeted metabolomics data Here we present recent 
updates to xcms, which re-use and build upon the support for memory-efficient parallel processing capabilities in the MSnbase Bioconductor 
software package for proteomics and general mass spectrometry data handling We have improved large-scale experiment data analysis through 
memory-efficient parallel processing capabilities and simplify raw spectra data access throughout the whole preprocessing task This comprises 
also dedicated functionality to extract ion chromatograms/traces from the original files and to perform chromatographic peak detection directly 
on such chromatographic data Besides paving the road for MRM/SRM data analysis with xcms, it also allows to evaluate different peak detection 
settings on selected signals before applying them to the whole data set Along these lines, we also implemented new visualization capabilities 
aiding in the definition and evaluation of data set-specific settings for the various preprocessing algorithms Finally, import of MRM/SRM raw data 
has been added and a framework for the identification of MS2 spectra for identified chromatographic peaks was implemented
Page 233
POSTER SESSIONS 1 AND 2 – Monday and Tuesday – all odd number posters will be on display.POSTER SESSIONS 3 AND 4 – Wednesday and Thursday – all even number posters will be on display.TECHNOLOGY15th Annual Conference of the Metabolomics Society*AWARD WINNERSTECHNOLOGY
P-497
Prediction, quantification and correction of impaired plasma sample quality induced by pre-analytical errors using  
LC-MS untargeted metabolomics
PRESENTING AUTHOR: Rui Zheng, Uppsala University, Sweden
CO-AUTHORS: Lin Shi, Rikard Landberg, Huma Zafar, Åsa Torinsson Naluai, Carl Brunius
The quality of biobanked blood samples is of great importance for reliable and accurate determination of metabolites Pre-analytical handling is one 
of the most important factors for sample quality We used untargeted LC-MS metabolomics to evaluate the influence of pre-analytical management 
on 471 plasma samples from 28 individuals Random forest modelling accurately predicted pre-centrifuge temperature (classification rate 81%) 
and time (Q2 = 082) as key factors for pre-analytical sample quality Fasting status, however, did not affect the metabolome reproducibly among 
individuals Thirteen and eight metabolites were selected in metabolite panels for highly accurate prediction of temperature and time, respectively 
Several metabolites responding to temperature/time interaction in linear regression showed significant difference from 30 to 120 min at 25 or 37 °C 
compared to 4 °C, whereas temperature could not be accurately predicted at <30 min Moreover, the changes in plasma metabolome were modelled 
per cluster at each temperature and pronounced at 4 °C because intensity of lipids-like and organic acids features dramatically declined from 0 until 
100 min Furthermore, only minor to moderate (0 to 25%) correction of data quality could be achieved by normalizing feature data to 0 min based 
on metadata on time, indicating that the induced variability to a large degree is non-systematic We conclude that the metabolite profile changes 
rapidly with pre-centrifugation delay times even at 4 °C Handling of blood samples from needle to freezer should be completed as soon as possible, 
preferably at 25 °C with pre-centrifugation delays less than 30 min
P-498
MetaboAnalystR 2.0: From Raw Spectra to Biological Insights
PRESENTING AUTHOR: Jasmine Chong, McGill University, Canada
CO-AUTHORS: Mai Yamamoto, Jianguo Xia
Global metabolomics based on high-resolution liquid chromatography mass spectrometry (LC-MS) has been increasingly employed in recent 
large-scale multi-omics studies Processing and interpreting these complex datasets have become a key challenge in current computational 
metabolomics We therefore present MetaboAnalystR 20, an R package to support end-to-end LC-MS based global metabolomics data analysis 
from spectral processing to biological insights Compared to its predecessor, this new release integrates XCMS and CAMERA to support raw 
spectral processing and peak annotation It also features high-performance implementations of Mummichog and GSEA algorithms to predict 
pathway activities directly from MS peaks The application and utility of the MetaboAnalystR 20 workflow are demonstrated using a clinical dataset 
of pediatric inflammatory bowel disease (IBD) Functional analysis identified perturbations in Bile acid biosynthesis and Vitamin D3 metabolism, 
both of which are well-known mechanisms in IBD This highlights the ease of which MetaboAnalystR 20 can be used to gain biological insights and 
generate hypotheses for future experimental validation In summary, MetaboAnalystR 20 offers a unified and flexible workflow that enables end-
to-end analysis of LC-MS metabolomics data within the open-source R environment The R package is freely available from the GitHub repository 
(https://githubcom/xia-lab/MetaboAnalystR)
P-499
Galaxy on Site: A flexible and reliable path to processing metabolomics data reproducibly and collaboratively
PRESENTING AUTHOR: Arthur Eschenlauer, University of Minnesota - Twin Cities, United States
CO-AUTHORS: Mark Esler, Timothy Griffin, Adrian Hegeman
Untargeted metabolomics LC-MS experiments can generate large numbers of large files; before the results can be interpreted, it is necessary to 
perform many preprocessing, annotation, and statistical analysis steps, each of which may have its own particular parameters Galaxy provides a 
web-based interface to capture these parameters into reproducible, reusable, shareable workflows Getting started can be as simple as using one 
of the established public Galaxy instances, engaging high-performance computing resources, or running Galaxy on a workstation A research group 
may have unique needs in the areas of access, collaboration, or simplified transfer and life-cycle management of sizeable datasets; new, in-house 
authored tools may also need to be incorporated into Galaxy workflows A research-group specific Galaxy “appliance” may practically address these 
needs; however, this requires a sustainable way to administrate a small-scale, high-availability system We have been running such a system for 
several years and are encapsulating these functionalities into an “appliance” that can be implemented in a broad spectrum of laboratory settings 
Our initial efforts have been focused on scaling the system appropriately, system backup and recovery, balancing cost of storage against available 
size, and integrating the system with instrument workstations on a laboratory intranet From the outset, the system was designed to balance 
security with usability We have applied this solution to our plant metabolomics research and found that, with minimal instruction, users can work 
independently and provide feedback on usability and functionality issues in prepublication versions of Galaxy tools
Page 234
POSTER SESSIONS 1 AND 2 – Monday and Tuesday – all odd number posters will be on display.POSTER SESSIONS 3 AND 4 – Wednesday and Thursday – all even number posters will be on display.TECHNOLOGY15th Annual Conference of the Metabolomics Society*AWARD WINNERSTECHNOLOGY
P-500
Which step is the most crucial in sample preparation procedure for GC-MS metabolomics? Design of Experiments approach.
PRESENTING AUTHOR: Julia Jacyna, Dept. of Biopharmacetics and Pharmacodynamics, Medical University of Gdansk, Poland
CO-AUTHORS: Marta Kordalewska, Joanna Raczak-Gutknecht, Marta Stawiszyńska, Michał Jan Markuszewski
Design of Experiments (DoE) consists of making specific and controlled modifications in a studied system in order to create a mathematical model 
that will allow to predict how monitored responses are affected by applied modifications In other words, the use of DoE approach allows to screen 
the most important factors, predict relationships between them and generate the most optimal settings in order to get the most favorable response 
while saving the time spent on the optimization and reducing the cost of analysis Its main advantage is the ability to provide optimal parameters’ 
settings by performing a minimal number of experiments The objective of the study was to develop and optimize a simple method for preparation 
of human urine samples for determination of concentration of previously selected metabolites by means of GC-MS/MS analysis A rapid, simple 
and reliable method is necessary for targeted metabolomics analysis Moreover, since sample preparation step for GC-MS/MS is usually very 
complicated, time-consuming and requires the use of toxic reagents, implementation of Design of Experiments was reasonable Fractional Factorial 
Design was implemented as a screening procedure in order to evaluate the significance of variables The most crucial steps of urine sample 
preparation procedure were identified with the use of two-step plan, based on the evaluation of more than a dozen of parameters with limited 
number of experiments Firstly, time- and temperature-dependent factors were evaluated and subsequently, concentration and volume of reagents 
used were taken into account
P-501*
Analytic correlation filtration: A new tool to reduce analytical complexity of metabolomic datasets
PRESENTING AUTHOR: Stephanie Monnerie, Université Clermont Auvergne, INRA, UNH, Mapping, France
CO-AUTHORS: Melanie Petera, Bernard Lyan, Pierrette Gaudreau, Blandine Comte, Estelle Pujos-Guillot
Metabolomics generates complex data that need dedicated workflows to extract the meaningful information For biological interpretation, 
experts are mainly focusing on metabolites rather than on the redundant different analytical species Moreover, the high degree of correlation in 