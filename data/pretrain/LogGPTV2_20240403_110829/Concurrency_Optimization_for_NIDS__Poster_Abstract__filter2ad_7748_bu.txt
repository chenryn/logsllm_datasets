### Data Analysis
Starting from raw or processed spectra, data can be normalized, aligned, and scaled interactively before bucketing peaks and multiplets for downstream statistical analysis. This software was developed by Dr. Arturas Grauslys under the guidance of Prof. Andy Jones, who has a proven track record in developing server-based software solutions for the MS proteomics community. It is anticipated that the functionality, as well as integration with other advanced tools, will expand with increased adoption by the NMR metabolomics community.

---

### P-487: IROA-Based Modified MSTUS Normalization Corrects Non-IROA Sample-to-Sample Metabolite Variation
**Presenting Author:** Felice De Jong, IROA Technologies LLC, United States  
**Co-Author:** Chris Beecher

The IROA TruQuant (TQ) protocol uses a Long-Term Reference Standard (LTRS), a defined chemical mixture containing hundreds of metabolites, along with an Internal Standard (TQ-IS) that is chemically identical but isotopically different. These standards measure instrument performance and provide verifiable chemical identification. We have previously demonstrated the ability to correct for ion suppression of natural abundance experimental compounds paired with TQ-IS compounds. Furthermore, we have shown that once ion suppression is corrected, sample-to-sample normalization can be achieved using a modified MSTUS algorithm, where experimental compounds are normalized to their internal standard counterparts. In this poster, we compare the original MSTUS-based normalization algorithm to the IROA-based modified-MSTUS algorithm, demonstrating that IROA-based normalization is not only significantly more accurate within a single experiment but also normalizes sample-to-sample intraday and interday (day-to-day) analyses. Additionally, we selected several compounds not present in either the LTRS or TQ-IS to show that the same normalization factor used for compounds with internal standards could be applied to those without. While these compounds did not normalize as accurately due to the inability to correct for ion suppression, the normalization was greatly improved compared to the original MSTUS algorithm.

---

### P-488: ASCA-Fusion of NMR Metabolomics Data Reveals Metabolic Alterations in Response to Bioactive Milk Ingredients in Preterm Piglets with Intra-amniotic Inflammation
**Presenting Author:** Masoumeh Alinaghi, Department of Food Science, Aarhus University, Denmark  
**Co-Authors:** Duc Ninh Nguyen, Per Torp Sangild, Hanne Christine Bertram, Johan A. Westerhuis

**Background:** Analyzing multiple compartments of the same piglets can provide complementary information about the studied biological system. However, in such complex systems, distinguishing different experimental variations by ANOVA-simultaneous component analysis (ASCA) prior to data fusion is crucial. This study investigates a dietary intervention for prenatal inflammation by analyzing urine and gut samples from preterm pigs using the ASCA-fusion approach.

**Methods:** Preterm pigs (n=17) subjected to intra-amniotic lipopolysaccharide (LPS, 1 mg/fetus) were administered one of three treatments: standard formula, bovine colostrum, or caseinoglycomacropeptide for five days. Collected urine and gut contents were analyzed by 1H NMR-based metabolomics. ASCA was used to separate the various sources of variation in the data related to experimental factors (time and treatment). Fusion of the ASCA-decomposed matrices was applied using penalized exponential simultaneous component analysis (P-ESCA) to distinguish common and distinct variations associated with the dietary intervention.

**Results:** ASCA-fusion of the urine and gut metabolomes revealed common alterations in lactate, glucose, acetate levels, and disaccharides in response to the different dietary interventions. Common responses related to the sex of piglets were also observed.

**Conclusion:** ASCA-P-ESCA improves the understanding of metabolomic alterations in urine and gut content by separating induced variations and finding common and distinct variations in both compartments.

**References:**
1. Nguyen, D.N., et al. The American journal of pathology, 188.11 (2018), 2629-2643.
2. Song, Y., et al. (2019), arXiv: 1902.06241.

---

### P-489: AutoTuner - High Fidelity, Robust, and Rapid Data Processing Parameter Selection Tool for Metabolomics Data
**Presenting Author:** Craig Mclean, MIT, United States  
**Co-Author:** Elizabeth B Kujawinski

Untargeted metabolomics experiments can capture an unbiased snapshot of cellular metabolism but remain challenging due to the computational complexity involved in data processing and analysis. Raw data must be processed to remove noise and align features across samples using software tools like XCMS or MzMine2, resulting in a table of features with paired mass-to-charge (m/z) and retention-time (RT) values. This processing step requires dataset-specific parameters. Several optimization methods exist, each with undesirable drawbacks. Here, we present AutoTuner, a new method designed to optimize data processing parameters based on a novel paradigm. Instead of maximizing an optimization function, AutoTuner relies on statistical inferences within the distribution of raw data. We tested the accuracy and run time of AutoTuner against the most common parameter selection tool, isotopologue parameter optimization (IPO). We also analyzed how parameter selection for AutoTuner and IPO influenced the quality of feature tables after XCMS. Our results show that AutoTuner is a desirable alternative, with substantially shorter computational times, easy implementation into existing metabolomics pipelines, and open availability to software developers.

---

### P-490: NonTplus – A New R Package for High-Throughput Processing of High-Resolution Mass Spectrometry Data
**Presenting Author:** Tobias Schulze, Helmholtz Centre for Environmental Research - UFZ, Germany  
**Co-Authors:** Erik Müller, Caroline Huber, Marc Stöhr, Werner Brack, Martin Krauss

High-resolution mass spectrometry is a key analytical technology for identifying targets, suspects, and unknowns in environmental or human samples. In large-scale environmental surveys or human cohort studies, hundreds or thousands of samples may be generated, each measured at least once in positive and negative modes, resulting in a large number of raw mass spectral files. NonTplus is a new pipeline that automates the entire processing workflow, including peak picking, gap filling, blank peak elimination, peak alignment, annotation, quantification, and export of peak lists for post-analysis. NonTplus implements a new gap-filling algorithm that improves missing value imputation with fewer false positives compared to existing algorithms. The pipeline is tailored to run on a high-performance computing (HPC) cluster, and future implementation in Galaxy is planned.

---

### P-491: On the Interpretability of O-PLS Filtered Models
**Presenting Author:** Barry M. Wise, Eigenvector Research, Inc., United States  
**Co-Authors:** Federico Marini, Frank Westad

Orthogonal PLS, introduced by Trygg and Wold in 2002, is a patented algorithm that has received significant attention for its perceived ability to simplify and improve regression and classification model interpretation. Since its introduction, it has been shown by Ergon and Kemsley and Tapp that results identical to the original O-PLS formulation can be obtained by post-processing conventional PLS models in a non-patented way, demonstrating that O-PLS models have predictive properties identical to their non-rotated versions. However, the interpretability of the models was not extensively considered. In this poster, we explore the interpretability of O-PLS models by applying the method to carefully constructed simple systems and well-characterized data. We demonstrate that O-PLS results match the true underlying (first principle) components only under very specific conditions, which are typically not met. O-PLS results are shown to be a strong function of the correlation between the factor of interest and interfering components, even under mild conditions. In simulated binary expression data, O-PLS is actually more sensitive to chance correlation than the conventional PLS regression vector.

**References:**
1. J. Trygg and S. Wold, “Orthogonal Projections to Latent Structures (O-PLS),” J. Chemo, 16, 119-128, 2002.
2. R. Ergon, “PLS post-processing by similarity transformation (PLS+ST): a simple alternative to OPLS,” J. Chemo, 19, 1-4, 2005.
3. E.K. Kemsley and H.S. Tapp, “OPLS filtered data can be obtained directly from non-orthogonalized PLS1,” J. Chemo, 23, 263-264, 2009.

---

### P-492: An Improved Lipid Profiling Workflow Demonstrates Disrupted Lipogenesis Induced with Drug Treatment in Leukemia Cells
**Presenting Author:** Mark Sartain, Agilent Technologies, United States  
**Co-Authors:** Genevieve Van de Bittner, Xiangdong Li, Jeremy Koelmel, Adithya Murali, Sarah Stow

While shotgun lipidomics has advanced the field of lipid analysis, it suffers from limitations, including the failure to distinguish isobaric species of biological importance. This has led to a shift towards chromatographic-based lipid profiling approaches using high-performance liquid chromatography coupled with high-resolution mass spectrometry. Confident lipid annotation requires data acquisition at the MS/MS level to enable product-ion spectral matching against in silico generated databases. In this study, a novel software tool was employed, which uses Bayesian scoring to assign lipid class annotation and a non-negative least squares fit with a theoretical lipid library (LipidBlast) to annotate iterative mode MS/MS spectra. The tool avoids over-annotation by providing only the level of structural information confidently informed by the MS/MS spectra. The tool quickly generates an accurate mass-retention time database in an automated fashion, which annotates MS1 lipid profiling data. We applied this workflow to study lipidome alterations in the acute myeloid leukemia K562 cell line in response to a combination of the drug candidates bezafibrate (BEZ) and medroxyprogesterone acetate (MPA). The analysis revealed several cellular changes, including a decrease in diacylglycerols, an increase in triacylglycerols, and differences in fatty acyl components. The new lipid analysis workflow provided more comprehensive lipid annotation than traditional approaches, supporting that BEZ/MPA may exert anticancer properties through disruption of lipogenesis.

---

### P-493: Developing a Systematized and Integrated Workflow for Large-Scale LC-MS Data Processing and Cross-Study Investigations
**Presenting Author:** Sajjan Mehta, University of California, Davis, United States  
**Co-Authors:** Gert Wohlgemuth, Diego Pedrosa, Sili Fan, Oliver Fiehn

We present the development of an enterprise-grade LC-MS data processing pipeline, LC-BinBase, and its integration with existing software to serve as an automated data management strategy for metabolomics and lipidomics studies. LC-BinBase implements MS-DIAL concepts and techniques to perform standardized data processing and feature annotation in a highly scalable capacity by utilizing cloud-based AWS Lambda services and Fargate clusters. Sample and study management are handled by in-house MiniX and Stasis services, defining all sample preparations and quality controls. Acquired data are pre-processed and converted on the fly to the open mzML data format and stored on AWS S3 for long-term storage. The converted samples are automatically scheduled for data processing by LC-BinBase using the study definition, instrument type, and matrix information to tune the processing parameters. Feature identification uses m/z-RT libraries and MS/MS mass spectral libraries that are method-specific to minimize erroneous annotations. Identified features are stored in a dedicated instance of MassBank of North America (MoNA), and later consensus spectra are uploaded to the public MoNA database as metabolites are identified and manually confirmed. Each sample’s validated annotations, along with its matrix information, are aggregated by BinVestigate to provide unique insight into the prevalence of individual metabolites within specific species and organs or components from across all historical studies. The use of standardized quality controls and retention time normalization enables cross-study investigation utilizing Systematic Error Removal using Random Forest (SERRF) for robust sample normalization.

---

### P-494: BioMagResBank: Database and Tools for NMR Metabolomics Analysis
**Presenting Author:** Pedro Romero, University of Wisconsin Madison, United States  
**Co-Authors:** Hamid R. Eghbalnia, Hesam Dashti, Naohiro Kobayashi, Jonathan R. Wedell, Kumaran Baskaran, Takeshi Iwata, Masashi Yokochi, Dimitri Maziuk, Hongyang Yao, Toshimichi Fujiwara, Genji Kurusu, Eldon L. Ulrich, Jeffrey C. Hoch, John L. Markley

The Biological Magnetic Resonance Data Bank (BioMagResBank or BMRB), founded in 1988, serves as the archive for data generated by Nuclear Magnetic Resonance (NMR) spectroscopy of biological systems. NMR spectroscopy is unique among biophysical approaches in its ability to provide a broad range of atomic and higher-level information relevant to the structural, dynamic, and chemical properties of biological macromolecules, as well as reporting on metabolite and natural product concentrations in complex mixtures and their chemical structures. BMRB stores experimental and derived data from biomolecular NMR studies on both biopolymers and bioactive small compounds. BMRB supports metabolomics NMR studies through a library of 1D and 2D NMR spectra of pure compounds (including metabolites, natural products, drugs, and compounds used for screening in drug discovery) and through its adoption of novel analytic tools, such as ALATIS unique atom identifiers, which are universal and based solely on the 3D structure of the compound and the InChI convention, and GISSMO spin matrices, which enable accurate simulation of compound and mixture spectra at any field strength. The combination of unique ALATIS naming and parameterized spectra offers users of BMRB data a distinctive benefit in terms of robustness and reproducibility, as embodied in the FAIR principles for data resources, which state that data should be Findable, Accessible, Interoperable, and Reusable. Supported by NIH Grants R01GM 109046, P41GM103399, and P41GM111135R01.

---

### P-495: Estimating Partial Correlation Networks Leveraging Prior Information with Applications to Metabolomics Data
**Presenting Author:** George Michailidis, University of Florida, United States  
**Co-Authors:** Jiahe Lin, Alla Karnovsky, Gayatri Iyer, William Duren

Estimating networks from high-dimensional Omics data has received significant attention due to their usefulness in providing insights into interactions among biomolecules under different diseases or experimental conditions. Partial correlation networks provide a useful technical framework for this task. However, the limited number of samples available in many studies leads to the estimation of very sparse (and fragmented) networks, making them hard to interpret. To address this, we propose an estimation framework that leverages external information provided in the form of similarities across network edges. We formulate a regularized pseudo-likelihood framework, develop a fast distributed proximal gradient descent algorithm to compute the network structure, and discuss the selection of tuning parameters. We illustrate the proposed framework and the resulting algorithm by reconstructing and comparing the networks obtained from metabolomic and lipodomic profiling of three groups of samples: one suffering from Crohn’s disease, another from ulcerative colitis, and the third of normal controls. We further tested for enrichment modules extracted from the estimated networks, identifying important alterations in both network structure and expression levels of interacting metabolites/lipids across the three groups of interest.

---

### P-496: Updates to xcms: Simplified Raw Data Access and Enhanced MS Level > 1 Capabilities
**Presenting Author:** Johannes Rainer, Institute for Biomedicine, Eurac Research, Italy  
**Co-Authors:** Laurent Gatto, Steffen Neumann

The xcms Bioconductor package is one of the standard toolboxes for preprocessing untargeted metabolomics data. Here, we present recent updates to xcms, which reuse and build upon the support for memory-efficient parallel processing capabilities in the MSnbase Bioconductor software package for proteomics and general mass spectrometry data handling. We have improved large-scale experiment data analysis through memory-efficient parallel processing capabilities and simplified raw spectra data access throughout the entire preprocessing task. This includes dedicated functionality to extract ion chromatograms/traces from the original files and perform chromatographic peak detection directly on such chromatographic data. These updates pave the way for MRM/SRM data analysis with xcms and allow for evaluating different peak detection settings on selected signals before applying them to the whole dataset. Along these lines, we have implemented new visualization capabilities aiding in the definition and evaluation of dataset-specific settings for the various preprocessing algorithms. Finally, import of MRM/SRM raw data has been added, and a framework for the identification of MS2 spectra for identified chromatographic peaks has been implemented.

---

### P-497: Prediction, Quantification, and Correction of Impaired Plasma Sample Quality Induced by Pre-Analytical Errors Using LC-MS Untargeted Metabolomics
**Presenting Author:** Rui Zheng, Uppsala University, Sweden  
**Co-Authors:** Lin Shi, Rikard Landberg, Huma Zafar, Åsa Torinsson Naluai, Carl Brunius

The quality of biobanked blood samples is crucial for reliable and accurate determination of metabolites. Pre-analytical handling is one of the most important factors for sample quality. We used untargeted LC-MS metabolomics to evaluate the influence of pre-analytical management on 471 plasma samples from 28 individuals. Random forest modeling accurately predicted pre-centrifuge temperature (classification rate 81%) and time (Q2 = 0.82) as key factors for pre-analytical sample quality. Fasting status, however, did not affect the metabolome reproducibly among individuals. Thirteen and eight metabolites were selected in metabolite panels for highly accurate prediction of temperature and time, respectively. Several metabolites responding to temperature/time interaction in linear regression showed significant differences from 30 to 120 minutes at 25 or 37°C compared to 4°C, whereas temperature could not be accurately predicted at <30 minutes. Moreover, the changes in the plasma metabolome were modeled per cluster at each temperature and were pronounced at 4°C because the intensity of lipid-like and organic acid features dramatically declined from 0 to 100 minutes. Furthermore, only minor to moderate (0 to 25%) correction of data quality could be achieved by normalizing feature data to 0 minutes based on metadata on time, indicating that the induced variability is largely non-systematic. We conclude that the metabolite profile changes rapidly with pre-centrifugation delay times, even at 4°C. Handling of blood samples from needle to freezer should be completed as soon as possible, preferably at 25°C with pre-centrifugation delays less than 30 minutes.

---

### P-498: MetaboAnalystR 2.0: From Raw Spectra to Biological Insights
**Presenting Author:** Jasmine Chong, McGill University, Canada  
**Co-Authors:** Mai Yamamoto, Jianguo Xia

Global metabolomics based on high-resolution liquid chromatography mass spectrometry (LC-MS) has been increasingly employed in recent large-scale multi-omics studies. Processing and interpreting these complex datasets have become a key challenge in current computational metabolomics. We present MetaboAnalystR 2.0, an R package to support end-to-end LC-MS-based global metabolomics data analysis from spectral processing to biological insights. Compared to its predecessor, this new release integrates XCMS and CAMERA to support raw spectral processing and peak annotation. It also features high-performance implementations of Mummichog and GSEA algorithms to predict pathway activities directly from MS peaks. The application and utility of the MetaboAnalystR 2.0 workflow are demonstrated using a clinical dataset of pediatric inflammatory bowel disease (IBD). Functional analysis identified perturbations in bile acid biosynthesis and vitamin D3 metabolism, both of which are well-known mechanisms in IBD. This highlights the ease with which MetaboAnalystR 2.0 can be used to gain biological insights and generate hypotheses for future experimental validation. In summary, MetaboAnalystR 2.0 offers a unified and flexible workflow that enables end-to-end analysis of LC-MS metabolomics data within the open-source R environment. The R package is freely available from the GitHub repository (https://github.com/xia-lab/MetaboAnalystR).

---

### P-499: Galaxy on Site: A Flexible and Reliable Path to Processing Metabolomics Data Reproducibly and Collaboratively
**Presenting Author:** Arthur Eschenlauer, University of Minnesota - Twin Cities, United States  
**Co-Authors:** Mark Esler, Timothy Griffin, Adrian Hegeman

Untargeted metabolomics LC-MS experiments can generate large numbers of large files. Before the results can be interpreted, many preprocessing, annotation, and statistical analysis steps must be performed, each with its own particular parameters. Galaxy provides a web-based interface to capture these parameters into reproducible, reusable, and shareable workflows. Getting started can be as simple as using one of the established public Galaxy instances, engaging high-performance computing resources, or running Galaxy on a workstation. A research group may have unique needs in the areas of access, collaboration, or simplified transfer and lifecycle management of sizeable datasets. New, in-house authored tools may also need to be incorporated into Galaxy workflows. A research-group-specific Galaxy "appliance" may practically address these needs; however, this requires a sustainable way to administrate a small-scale, high-availability system. We have been running such a system for several years and are encapsulating these functionalities into an "appliance" that can be implemented in a broad spectrum of laboratory settings. Our initial efforts have focused on scaling the system appropriately, system backup and recovery, balancing cost of storage against available size, and integrating the system with instrument workstations on a laboratory intranet. From the outset, the system was designed to balance security with usability. We have applied this solution to our plant metabolomics research and found that, with minimal instruction, users can work independently and provide feedback on usability and functionality issues in pre-publication versions of Galaxy tools.

---

### P-500: Which Step is the Most Crucial in Sample Preparation Procedure for GC-MS Metabolomics? Design of Experiments Approach
**Presenting Author:** Julia Jacyna, Dept. of Biopharmacetics and Pharmacodynamics, Medical University of Gdansk, Poland  
**Co-Authors:** Marta Kordalewska, Joanna Raczak-Gutknecht, Marta Stawiszyńska, Michał Jan Markuszewski

Design of Experiments (DoE) involves making specific and controlled modifications in a studied system to create a mathematical model that predicts how monitored responses are affected by applied modifications. In other words, the DoE approach allows screening of the most important factors, predicting relationships between them, and generating the most optimal settings to achieve the most favorable response while saving time and reducing the cost of analysis. Its main advantage is the ability to provide optimal parameters' settings by performing a minimal number of experiments. The objective of this study was to develop and optimize a simple method for preparing human urine samples for determining the concentration of previously selected metabolites by GC-MS/MS analysis. A rapid, simple, and reliable method is necessary for targeted metabolomics analysis. Since the sample preparation step for GC-MS/MS is usually very complicated, time-consuming, and requires the use of toxic reagents, implementing DoE was reasonable. Fractional Factorial Design was implemented as a screening procedure to evaluate the significance of variables. The most crucial steps of the urine sample preparation procedure were identified using a two-step plan, based on the evaluation of more than a dozen parameters with a limited number of experiments. Firstly, time- and temperature-dependent factors were evaluated, and subsequently, the concentration and volume of reagents used were taken into account.

---

### P-501: Analytic Correlation Filtration: A New Tool to Reduce Analytical Complexity of Metabolomic Datasets
**Presenting Author:** Stephanie Monnerie, Université Clermont Auvergne, INRA, UNH, Mapping, France  
**Co-Authors:** Melanie Petera, Bernard Lyan, Pierrette Gaudreau, Blandine Comte, Estelle Pujos-Guillot

Metabolomics generates complex data that require dedicated workflows to extract meaningful information. For biological interpretation, experts focus on metabolites rather than redundant analytical species. Moreover, the high degree of correlation in metabolomic datasets can complicate the analysis. Analytic correlation filtration (ACF) is a new tool designed to reduce the analytical complexity of metabolomic datasets by filtering out redundant and highly correlated features. This approach helps in simplifying the data, making it easier to identify and interpret biologically relevant patterns. ACF has been applied to various metabolomic datasets, demonstrating its effectiveness in reducing complexity and improving the interpretability of the results.