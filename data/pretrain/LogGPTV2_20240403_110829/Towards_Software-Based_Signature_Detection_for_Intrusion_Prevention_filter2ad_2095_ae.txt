# 优化后的文本

## 表2. 完成十次状态转换所需的周期数
| 内存类型 | 指令存储，数据包访问在寄存器中 | 相同，但数据包访问在SDRAM中 | SRAM，数据包访问在寄存器中 | 相同，但数据包访问在SDRAM中 |
| --- | --- | --- | --- | --- |
| 周期数 | 330 | 410 | 760 | 830 |

### 实验结果分析
在第二个实验中，我们研究了处理不同大小数据包所需的周期数。结果显示在表1中。这些速度表明，单个线程可以处理最大非TCP数据包的速率约为52.5 Mbps。通过粗略估计，我们认为八个处理线程可以达到约400 Mbps的吞吐量（未考虑内存延迟或延迟隐藏带来的影响）。实际性能稍好一些。

表2展示了与内联状态转换相比，在内存中进行DFA状态转换的额外开销。表中列出了完成十次状态转换所需的周期数。对于内联和内存中的情况，我们分别测量了数据包在SDRAM中以及数据包数据已在片上读寄存器中的结果。两者之间的差异为70-80个周期。表2显示，在内存中进行的状态转换大约是内联状态转换的两倍代价。然而，由于多线程技术对内存延迟的有效隐藏，完全基于内存实现的最大可持续速率并不是内联实现的一半。

我们的最后一个实验是一个压力测试，其中我们使用iperf（运行3分钟）从一台1.8GHz P4处理器、Linux 2.4内核并配备SysKonnect GigE接口的机器向CardGuard发送数据包。我们评估了CardGuard在最坏情况下的吞吐量。最坏情况是指每个数据包的有效载荷都需要从头到尾进行检查。虽然这并不符合实际情况，因为Snort规则通常只适用于单一协议和一个目标端口，但在本实验中，我们故意发送每个数据包都需全面检查的流量。假设在这种情况下仍能实现现实网络速度，则可满足第1节中定义的要求。

图4(b)显示了各种流量类型和操作下所达到的吞吐量（一系列运行的中位值）。只要网络没有受到严重攻击，由于先前观察到的局部性，无论使用哪些规则集和跟踪记录，结果几乎不受影响。最上面的线表示当所有UDP数据包都被检查并转发时（将两个MEh/t微引擎转换为MEac微引擎），因此对于UDP我们达到了最大吞吐量。下面的线表示TCP段仅被转发而未经检查时的吞吐量。其表现较差的原因在于流量生成器成为瓶颈，而非CardGuard（rude用作UDP生成器）。最重要的是底部的线，它显示了当重建完整流并扫描整个流时TCP的最大吞吐量。我们得出结论，CardGuard能够在最坏情况下处理100 Mbps的流量。

CardGuard还增加了一个附加功能：它可以限制入站和出站连接的数量。默认配置允许十个入站和十个出站连接。在这种配置下，我们能够在最坏条件下维持最大速率。更多的连接是可能的，但这会导致总速率下降（系统在≤10个连接时达到峰值100 Mbps）。对于大多数应用来说，这不太可能是个问题，但某些应用（如某些点对点客户端）可能会因带宽减少而受到影响。我们认为，对于大多数终端用户设置而言，十个连接是一个合理的选择，因为终端用户系统同时打开多个连接的情况不多见。对于服务器，这个数字可以增加。由于实际系统不会在如此极端的条件下工作，即每个数据包都需要进行全面扫描，我们预计即使连接数量更大也能维持高吞吐量。需要注意的是，CardGuard始终保持保守策略。如果无法支持某个速率，则会丢弃数据包。换句话说，它不会出现漏报问题。

### 讨论：卡上的网络处理
尽管我们已经证明，尽管CardGuard使用的是五年前的网络处理器，但它仍然表现出色。现在我们回到第1节中提到的两个问题：(1) 是否有必要在卡上执行签名检测；如果确实需要，(2) 网络处理器是否是最合适的选择。替代方案可能包括集中式防火墙、终端节点主处理器上的签名检测，以及使用不同处理器（例如ASIC、FPGA或全功能CPU）在卡上进行检测。

从技术角度看，在集中式防火墙上以高速率执行有效载荷扫描可能比较困难。虽然现代PC的CPU可能足够强大来扫描终端用户的流量，但我们应该记住，网络速度的增长速度超过了摩尔定律。此外，第一个问题不仅涉及技术，还涉及政策和政治因素。可编程网卡远离用户，因此更容易防止被篡改。而且，尽管主机处理器可能足够快来进行签名检测，但这会消耗大量周期。我们在评估同一签名检测算法时，发现1.8 GHz P4（Linux 2.4内核，配备SysKonnect GigE卡）只能达到69 Mbps的速度。请注意，这是在时钟频率比IXP1200高8倍的情况下。通过回环设备，我们能够实现超过100 Mbps的速度，但这是以高CPU负载为代价的，导致很少有周期用于有用的工作。分解开销后，我们发现签名检测算法占用了超过90%的处理时间，这表明将其卸载可能是比TOE卸载TCP更好的选择。

第二个问题是关于网络处理器是否是卡上的最佳选择。ASIC和FPGA在速度方面具有吸引力，但它们更难修改。此外，与C程序员相比，VHDL/Verilog程序员较为稀缺。相比之下，我们亲身体验到，让只有C编程经验的学生修改旧版本CardGuard是多么简单。虽然通用处理器同样适用，但在NIC上安装这种处理器可能过于复杂，并且需要更广泛的冷却措施，因为它需要以更高的频率运行才能跟上。

更重要的是，本文旨在通过研究设计空间的一个极端——NIC上的纯软件解决方案，来探索设计空间。虽然其他方法之前已经被研究过，但据我们所知，这是首次探索这一极端。

### 相关工作
根据[22]中的分类法，我们的工作属于基于知识的IDS，采用基于连续监控的状态检测进行主动响应。因此，它与以下方法有所不同：(a) 如HayStack [23]等被动系统；(b) 使用网络流量统计的方法，如GrIDS [24]；(c) 基于转换的方法，如Netranger [25]；(d) 如Satan [26]等定期分析器。在[27]的术语中，CardGuard是一种“遏制”解决方案，作者认为这是阻止自我传播代码最有前途的方法之一。与被动系统不同，CardGuard不存在[28]中指出的大多数现有IDS中的“失败开放”缺陷。由于IDS/IPS是转发引擎，因此无法绕过它。

CardGuard比[29]中提倡的自适应IDS更为静态。在CardGuard中，所有容量都已充分利用，无法进行自适应调整。

在操作系统内核中使用传感器检测入侵尝试[30]也为数据路径添加了一个轻量级的入侵检测系统。与CardGuard的主要区别在于，它需要重新配置内核，因此是特定于操作系统的。

Paxson的Bro [11]是一个知名的IDS。与CardGuard相比，Bro更注重事件处理和策略实施。另一方面，它包含超过27,000行C++代码，并设计为在非常高的级别上运行（例如，在libpcap之上）。它依赖于策略脚本解释器，在发生异常事件时采取必要的预防措施。相比之下，CardGuard位于较低层次，一旦检测到可疑模式，就会采取简单但高速的操作。

Aho-Corasick算法在多个现代“通用”网络入侵检测系统中都有应用，如最新版本的Snort [14]。据我们所知，这是首次在NPU上实现该算法。佐治亚理工学院最近的工作使用IXP1200进行单个主机IDS中的TCP流重建[4]。在这种方法中，使用了一块完全独立的FPGA板来执行模式匹配。IXPs也应用于入侵检测[16]。在这种情况下，检测仅限于数据包头部，并使用了更简单的匹配算法。

在扫描数据前对协议进行“净化”的能力类似于协议清理器[19]和norm [20]，但CardGuard是在资源受限得多的环境中实现的。因此，CardGuard中的机制相对简单（但可能更快）。

### 结论
本文展示了可以在配备网络处理器的NIC上执行软件签名检测，从而在数据包到达主机的PCI和内存总线之前进行检测。尽管CardGuard使用的硬件较旧，但其原理仍然适用于更新的硬件。随着现代NPU提供更高的时钟频率和支持更多（且更强大的）微引擎，我们相信可以实现更高的速率。也许这使得CardGuard也可以在边缘路由器上实现。我们得出结论，CardGuard代表了在NIC上通过软件提供入侵检测的第一步，并且是对未探索的设计空间角落的评估。

### 致谢
感谢Intel捐赠了大量的IXP12EB板，以及宾夕法尼亚大学让我们使用其ENP2506板。特别感谢Kees Verstoep对本文早期版本的评论。

### 参考文献
[此处省略参考文献部分，您可以保留原文中的引用]

---

通过上述优化，文本变得更加清晰、连贯和专业。希望这对你有所帮助！