s
(
e
m
T
i
0
0.95
0.96
0.97
0.98
0.99
1
Availability
when waiting for a fixed time (e.g., 10 seconds). As
expected, the graph is fairly flat until it gets above 99%,
at which point the time needed to wait approaches grows
exponentially.
As with size, we chose a threshold for time, marking
sites above the threshold as unavailable. This kind of
error can indicate network delays, although it may also
indicate an overloaded server. Ignoring such slow-but-
successful errors yielded the results in the third row of
Table 1. In the pie chart, we divide timing problems into
“medium” (ten seconds) and “severe” (thirty seconds).
Ten seconds is well above the normal return time for all
the sites studied. Above thirty seconds, even large
changes in the threshold don’t have much impact on the
final value.
After client errors, one-time network errors (medium
and severe) were the most frequent kind of error.
3.5 The effect of retry
least once. Whenever
When presented with an unavailable web site, users
often retry at
sites were
unavailable for one of the aforementioned reasons, we
checked the failure for persistence. That is, for each
error, the data was inspected to see if the error repeated
when the site was accessed again in the next hour. This
allowed us to categorize the errors, differentiating one-
time failures from problems that persisted for many
hours. Such analysis is particularly useful for sites with a
long delay, because it is difficult to pinpoint the source
of the delay in the chain between user and server. When
a single site is consistently slow relative to other sites, it
suggests trouble at (or near) the server1. In any case, by
measuring the availability of the site after retry, we were
able
total user
experience. The final row of Table 1 includes only sites
that experienced persistent inexplicable downtime.
the
to more
accurately measure
retailer
search
directory
Table 2: The value of retry
All
0.2667
Retailer
0.2706
Search Directory
0.2647
0.2648
0.8621
0.8704
0.9286
0.8318
0.7895
0.9111
0.4210
0.9231
0.7857
0.3125
1.00
1.00
1.00
0.6889
0.9600
n/a
Error
Client
Medium
Network
Severe
Network
Server
Corporate
Figure 2: Time grows quickly at high availability
“availability versus time”, shown in Figure 2. This graph
plots expected availability against the length of the wait.
From it, a user can determine how long to wait for a certain
degree of availability (e.g. 99.9%) or the availability found
1 From the end-user perspective it is impossible to
distinguish a server failure from a network failure near
the server. Further, in cases where there is no response,
it is impossible to distinguish server software failure
from hardware failure.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:27:48 UTC from IEEE Xplore.  Restrictions apply. 
Table 2 shows how well retry overcomes each class of
failure. At the client, most errors were persistent, and so
retry had a limited effect, while retry was much more
effective for most other kinds of problems. Corporate errors
were different;
the vast majority of corporate errors
occurred at the retailer site, and when they did they were
largely persistent.
3.6 Persistent non-local failures
Once local and one-time errors are removed, only a few
errors remain. The distribution of these more severe errors
(shown in Table 3) is domain-dependent. For instance, the
directory site was much more prone to persistent network
problems, suggesting a problem with the connection to their
international sites. By contrast, the retailer site was more
prone to corporate errors, due to the added complexity of
having to deal with storing user information. This site also
suffered persistent problems with its servers, and a handful
of times when all editions of the site worldwide had to be
taken offline – a corporate disaster.
Table 3: Persistent errors by type
7
Error
Medium persistent
network
Severe persistent
network
2
Server persistent errors 3
Corporate persistent
errors
Retailer
11
Search
Directory
3
0
0
0
18
14
1
0
4. Analysis
Our retry period of an hour is unrealistically long.
In
practice, the likelihood of retry, number of retries, and the
time before the user stops retrying are based on several
domain- and user-dependent factors. Users may retry
immediately to try to overcome a short-term problem, but, if
presented with persistent unavailability, may continue to
retry or return after some time. Economists know that
shoppers in conventional stores, when thwarted in making a
purchase, often will find another source for the desired item
or skip the purchase entirely [4]. There is no reason to
expect that e-commerce sites, especially where competition
is easily found, would be any different. However, there are
certain sites where users, even after an extended outage, will
retry. The likelihood of retry is determined by:
a) Uniqueness – if a site offers unique service, goods, or
This
information,
increases the chance of retry, even after a long wait.
Import – the value and cost of the transaction to the
user is a key factor in retry. The more costly and
important the transaction, the more likely to have retry.
then there is less competition.
b)
c) Loyalty – users exhibit faithfulness when browsing
the
the web. The higher the degree of loyalty,
longer the retry period.
d) Transience – the length of delay before retry is
inversely proportional to the speed at which the
content of the site changes. That is, sites where the
information is highly transient are unlikely to have
long-term retries, while sites that don’t change (and
whose content may well be unique) are more likely
to have a long retry period.
5. Conclusion
and
errors,
short-term network
This experiment was designed to model
the user
experience, and was successful. We found about 93%
raw availability, but the great majority (81%) of errors
that occurred were due to errors on the local side that
prevented the experiment from being run. Removing
local
availability
increased to 99.9%. Lastly, we saw the effect retry had
on the availability, and found that, while local errors
were reduced by only 27%, non-local errors fell by 83%.
It’s clear that local availability has the largest impact on
the user experience. Also, when local problems are
factored out, retry becomes a strategy that cuts error
substantially. When removing local and network
problems and retrying, we generally get at least three
nines of availability, which, while lower than what is
found in an ideal environment, is still respectable.
There are several areas in which this work can be
expanded. The first is to continue the experiment and
refine our availability numbers in the final row.
In
several cases, the number of errors is just above the
granularity of the experiment. Also,
the experiment
should be distributed to distant sites so that we can better
assess the source of errors. Lastly, we need better
experiments to measure the efficacy of retry, both in the
short-term and in the longer-term.
With better measures of availability, we can devise
test suites and benchmarks that will lead to more reliable
systems and the pinpointing of sources of failure.
6. Acknowledgements
We thank the students from Mills College and UC
Berkeley who participated in a joint class and helped
design and implement this experiment. We also thank
the members of the ROC research group, who provided
valuable feedback on our work. Thanks to the referees
for their valuable commentary on this work. Lastly, we
thank the National Science Foundation for generous
support of this research.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:27:48 UTC from IEEE Xplore.  Restrictions apply. 
7. References
[1] “Building an Internet-centric Monitoring Infrastructure”, ASP
News
2001.
http://www.aspnews.com/strategies/technologies/article/0,2350,
10584_924571,00.html
September
Review,
[2] A. Brown and D. A. Patterson, “To Err
is Human”,
Proceedings of the First Workshop on Evaluating and Architecting
System dependabilitY (EASY '01), Göteborg, Sweden, July 2001.
[3] A. Brown and D. A. Patterson, “Embracing Failure: A Case for
Recovery-Oriented Computing (ROC)”, 2001 High Performance
Transaction Processing Symposium, Asilomar, CA, October 2001.
[4] R. H. Frank and B. S. Bernanke, Principles of Economics,
McGraw-Hill/Irwin, New York, 2001, pp. 92-3.
[5] “Topaz: The Complete Solution for Application Management”,
http://www-
Mercury-Interactive,
svca.mercuryinteractive.com/products/topaz/index.html
December
2001.
[6] “Netcraft: What’s that Site Running?”, Netcraft, December
2001. http://www.netcraft.com
[7] “Quality of Experience: measuring true end-to-end Web
performance”,
2001.
http://www.porivo.com/peerReview/Porivo_paper.pdf
December
Porvio,
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:27:48 UTC from IEEE Xplore.  Restrictions apply.