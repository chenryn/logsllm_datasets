3
20.0%
4
1
15.8%
3
8
14.1%
12
3
5
8.3%
4
7.3%
11
2
4.4%
3
2
10.0%
4
3
3
7.9%
7.9% 11
13
69
10.0% 45
V N List
2
1
0
1
0
1
2
4
2
1
1
5
0
0
0
1
0
0
2
1
17
7
1
0
1
0
0
2
1
1
0
1
7
Study References
[14–24]
[25–28]
[29–31]
[32–43]
[44–49]
[50–60]
[61–63]
[64–68]
[69–71]
[72–84]
3
0
0
0
1
1
0
1
0
3
9
29
2
1
2
2
1
1
11
1
Alexa Global Top ...
5k
2
1M
100k
1k
5
500
8
75k
400
1
50k
25k
300
1
200
20k
1
16k
100
8
3
50
10k
8k
1
10
Alexa Country:
2
2
Alexa Category:
3
Umbrella 1M:
Umbrella 1k:
1
of security, across 38 papers in total: this includes phishing at-
tacks [81, 82], session safety during redirections [83], and domain
squatting [58], to name a few. Nine more papers study aspects of
privacy & censorship, such as the Tor overlay network [61], or user
tracking [35]. Network or application performance is also a popular
area: ten papers in our survey focus on this, e.g., HTTP/2 server
push [72], mobile web performance [71], and Internet latency [26].
Other studies look at economic aspects such as hosting providers.
Layers: We also reviewed the network layers measured in each
study. Many of the papers we surveyed focus on web infrastructure:
22 of the papers are concerned with content, 8 focus on the HTTP(S)
protocols, and 7 focus on applications (e.g., browsers [39, 40]).
Studies relating to core network protocols are commonplace:
DNS [32, 36, 51, 52, 61] (we identified 3 studies relating to domain
names as separate from DNS protocol measurements [24, 58, 63]),
TCP [19, 31], and IP [14, 15, 18, 30, 64, 69], and TLS/HTTPS [21, 37,
38, 50, 57, 76, 83] layer measurements are common in our survey.
Finally, we identify 12 studies whose experimental design mea-
sures more than one specific layer; e.g., cases studying a full con-
nection establishment (from initial DNS query to HTTP request).
We conclude from this that top lists are frequently used to explic-
itly or implicitly measure DNS, IP, and TLS/HTTPS characteristics,
which we investigate in depth in §8.
3.4 Are Results Dependent on Top Lists?
In this section, we discuss how dependent study results are on top
lists. For this, we fill the “dependent” columns in Table 1 as follows:
Dependent (Y): Across all papers surveyed, we identify 45 stud-
ies whose results may be affected by the list chosen. Such a study
would take a list of a certain day, measure some characteristic over
the set of domains in that list, and draw conclusions about the mea-
sured characteristic. In these cases, we say that the results depend
on the list being used: a different set of domains in the list may
have yielded different results.
Verification (V): We identify 17 studies that use a list only to
verify their results. A typical example may be to develop some
algorithm to find domains with a certain property, and then use a
top list to check whether these domains are popular. In such cases,
the algorithm developed is independent of the list’s content.
Independent (N): Eight studies cite and use a list, but we deter-
mine that their results are not necessarily reliant on the list. These
papers typically use a top list as one source among many, such that
changes in the top list would likely not affect the overall results.
3.5 Are Studies Replicable?
Repeatability, replicability, and reproducibility are ongoing con-
cerns in Computer Networks [85, 86] and Internet Measurement [87].
While specifying the date of when a top list was downloaded, and
the date when measurements where conducted, are not necessarily
sufficient to reproduce studies, they are important first steps.
Table 1 lists two “date” columns that indicate whether the list
download date or the measurement dates were given3. Across all 69
papers using top lists, only 7 stated the date the list was retrieved,
and 9 stated the measurement date. Unfortunately, only 2 papers
give both the list and measurement data and hence fulfil these basic
criteria for reproducibility. This does not necessarily mean that the
other papers are not reproducible, authors may publish the specific
top list used as part of data, or authors might be able to provide
the dates or specific list copies upon inquiry. However, recent in-
vestigations of reproducibility in networking hints that this may
be an unlikely expectation [87, 88]. We find two papers that explic-
itly discuss instability and bias of top lists, and use aggregation or
enrichment to stabilise results [45, 67].
3We require a specific day to be given to count a paper, the few papers just citing a
year or month were counted as no date given
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
Scheitle et al.
3.6 Summary
Though our survey has a certain level of subjectivity, we consider
its broad findings meaningful: (i) that top lists are frequently used,
(ii) that many papers’ results depend on list content, and (iii) that
few papers indicate precise list download or measurement dates.
We also find that the use of top lists to measure network and
security characteristics (DNS, IP, HTTPS/TLS) is common. We fur-
ther investigate how top list use impacts result quality and stability
in studies by measuring these layers in §8.
4 TOP LISTS DATASET
For the three lists we focus on in this study, we source daily snap-
shots as far back as possible. Many snapshots come from our own
archives, and others were shared with us by other members of the
research community, such as [89–91]. Table 2 gives an overview of
our datasets along with some metrics discussed in §5. For the Alexa
list, we have a dataset with daily snapshots from January 2009 to
March 2012, named AL0912, and another from April 2013 to April
2018, named AL1318. The Alexa list underwent a significant change
in January 2018; for this we created a partial dataset named AL18
after this change. For the Umbrella list, we have a dataset spanning
2016 to 2018, named UM1618. For the Majestic Million list, we cover
June 2017 to April 2018.
As many of our analyses are comparative between lists, we create
a JOINT dataset, spanning the overlapping period from June 2017
to the end of April 2018. We also sourced individual daily snapshots
from the community and the Internet Archive [92], but only used
periods with continuous daily data for our study.
5 STRUCTURE OF TOP LISTS
In this section, we analyse the structure and nature of the three top
lists in our study. This includes questions such as top level domain
(TLD) coverage, subdomain depth, and list intersection.
DNS Terms used in this paper, for clarity, are the following: for
www.net.in.tum.de, .de is the public suffix4 (and top level domain),
tum.de is the base domain, in.tum.de is the first subdomain, and
net.in.tum.de is the second subdomain. Hence, www.net.in.tum.de
counts as a third-level subdomain.
5.1 Domain Name Depth and Breadth
A first characteristic to understand about top lists is the scope of
their coverage: how many of the active TLDs do they cover, and
how many do they miss? How deep are they going into specific
subdomains, choosing trade-offs between breadth and depth?
TLD Coverage is a first indicator of list breadth. Per IANA [94,
95], 1,543 TLDs exist as of May 20th, 2018. Based on this list, we
count valid and invalid TLDs per list. The average coverage of valid
TLDs in the JOINT period is ≈700 TLDs, covering only about 50%
of active TLDs. This implies that measurements based on top lists
may miss up to 50% of TLDs in the Internet.
At the Top 1k level we find quite different behaviour with 105
valid TLDs for Alexa, 50 for Majestic, but only 13 (com/net/org and
few other TLDs) for Umbrella. We speculate that this is rooted in
DNS administrators from highly queried DNS names preferring
4per Public Suffix List [93], a browser-maintained list aware of cases such as co.uk.
the smaller set of professionally managed and well-established top
level domains over the sometimes problematic new gTLDs [96–98].
Invalid TLDs occur neither in any Top 1k domains nor in the
Alexa Top 1M domains, but as a minor count in the Majestic Top 1M
(7 invalid TLDs, resulting in 35 domain names), and significant
count in the Umbrella Top 1M: there, we can find 1,347 invalid
TLDs5, in a total of 23k domain names (2.3% of the list). This is
an early indicator of a specific characteristic in the Umbrella list:
invalid domain names queried by misconfigured hosts or outdated
software can easily get included into the list.
Comparing valid and invalid TLDs also reveals another struc-
tural change in the Alexa list on July 20th, 2014: before that date,
Alexa had a fairly static count of 206 invalid and 248 valid TLDs.
Perhaps driven by the introduction of new gTLDs from 2013 [99],
Alexa changed its filtering: After that date, invalid TLDs have been
reduced to ≈0, and valid TLDs have shown continued growth from
248 to ≈800. This confirms again that top lists can undergo rapid
and unannounced changes in their characteristics, which may sig-
nificantly influence measurement results.
Subdomain Depth is an important property of top lists. Base
domains offer more breadth and variety in setups, while subdomains
may offer interesting targets besides a domain’s main web presence.
The ratio of base to subdomains is hence a breadth/depth trade-
off, which we explore for the three lists used. Table 2 shows the
average number of base domains (µBD) per top list. We note that
Alexa and Majestic contain almost exclusively base domains with
few exceptions (e.g., for blogspot). In contrast, 28% of the names
in the Umbrella list are base domains, i.e., Umbrella emphasises
depth of domains. Table 2 also details the subdomain depth for a
single-day snapshot (April 30, 2018) of all lists. As the Umbrella list
is based on DNS lookups, such deep DNS labels can easily become
part of the Umbrella list, regardless of the origin of the request. In
fact, Umbrella holds subdomains up to level 33 (e.g., domains with
extensive www prefixes or ‘.’-separated OIDs).
We also note that the base domain is usually part of the list when
its subdomains are listed. On average, each list contains only few
hundred subdomains whose base domain is not part of the list.
Domain Aliases are domains with the same second-level do-
main, but different top-level domains, e.g., google.com and google.de.
Table 2 shows the number of domain aliases as DU PSLD. We find a
moderate level of ≈5% of domain aliases within various top lists,
with only 1.5% for Majestic. Analysis reveals a very flat distribution,
with the top entry google at ≈200 occurrences.
5.2 Intersection between Lists
We next study intersection between lists—all 3 lists in our study
promise a view on the most popular domains (or websites) in the
Internet, hence measuring how much these lists agree6 is a strong
indicator of bias in list creation. Figure 1a shows the intersection
between top lists over time during the JOINT period. We see that
the intersection is quite small: for the Top1M domains, Alexa and
Majestic share 285k domains on average during the JOINT duration.