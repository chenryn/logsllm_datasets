databases [33]. Barth, Mitchell, Datta, and Sundaram study
minimal disclosure in the context of workﬂows [34]. They
model a workﬂow as meeting a utility goal if it satisﬁes
a temporal logic formula. Minimizing the amount of in-
formation disclosed is similar to an agent maximizing his
reward and thereby not performing actions that have costs
but no beneﬁts. However, we consider several factors that
these works do not, including quantitative purposes that are
satisﬁed to varying degrees and probabilistic behavior result-
ing in actions being for a purpose despite the purpose not
being achieved, which is necessary to capture the semantics
of purpose restrictions (Section VI).
Expressing Privacy Policies with Purpose: Work on
understanding the components of privacy policies has shown
that purpose is a common component of privacy rules
(e.g., [35]). Some languages for specifying privacy policies
allow the purpose of an action to partially determine if access
is granted (e.g., [36], [37]). However, these languages do not
give a formal semantics to the purposes. Instead they rely
upon the system using the policy to determine whether an
action is for a purpose or not.
The sense in which the word “purpose” is used in privacy
policies is also related to the ideas of desire, motivation,
and intention discussed in works of philosophy. The most
closely related to our work is that of Bratman’s on intentions
in his Belief-Desire-Intention (BDI) model [38]. In his work,
an intention is an action an agent plans to take where the
plan is formed while attempting to maximize the satisfaction
of the agent’s desires; Bratman’s desires correspond to our
purposes. Roy formalized Bratman’s work using logics and
game theory [39]. However,
these works are concerned
with when an action is rational rather than determining the
purposes behind the action.
We borrow the notion of non-redundancy from Mackie’s
work on formalizing causality using counterfactual reason-
ing [20]. In particular, Mackie deﬁnes a cause to be a
non-redundant part of a sufﬁcient explanation of an effect.
Roughly speaking, we replace the causes with actions and
the effect with a purpose.
Plan Recognition: Attempting to infer the plan that
an agent has while performing an action is plan recogni-
tion [40]. Plan recognition may predict the future actions of
agents allowing systems to anticipate them. However, our
auditing algorithm checks whether a sequence of actions is
consistent with a given purpose rather than attempting to
predict the most likely purpose motivating the actions.
The work most closely related to ours is that of Baker,
Saxe, and Tenenbaum [41], [42]. They use an MDP model
similar to ours to predict the most likely explanation for a
sequence of actions. Ram´ırez and Geffner extend this work
to partially observable MDPs for modeling an agent that
cannot directly observe the state it is in [43]. Rather than
having a reward function, under these models, the agent
attempts to reduce the costs of reaching a goal state. For
each possible goal state, their algorithms use the degree to
which the agent’s actions minimizes the costs of reaching the
goal state to assign a probability to that goal state being the
one pursued by the agent. Our reward functions are similar to
the negation of their cost functions, but these works predict
which goal state the agent is pursuing rather than which cost
function it is using. They do not consider non-redundancy.
Our algorithm for auditing is similar to their algorithms.
However, to maintain soundness, our algorithm accounts for
the error of approximate MDP solving. Furthermore, their
algorithms may assign a non-zero probability to a goal state
even if the agent’s actions are inconsistent with pursuing
that goal under our strict deﬁnition.
Also related is the work of Mao and Gratch [44]. While
it differs from our work in the same ways as the work of
Baker et al., it also differs in that rewards track how much
the agent wants to achieve the goal rather than the degree
of satisfaction of the goal.
Philosophical Foundations: Taylor provides a detailed
explanation of the importance of planning to the meaning
of purpose, but does not provide any formalism [18].
Our work is related to adversarial plan recognition that
models possibly misleading agents [45]. Particularly related
are works using plan recognition to aid intrusion detec-
188
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:51 UTC from IEEE Xplore.  Restrictions apply. 
tion [46], [47]. These works, however, do not consider
quantitative purposes or probabilistic transitions.
IX. SUMMARY AND FUTURE WORK
We use planning to create the ﬁrst formal semantics for
determining when a sequence of actions is for a purpose.
In particular, our formalism uses models similar to MDPs
for planning, which allows us to automate auditing for both
exclusivity and prohibitive purpose restrictions. We have
provided an auditing algorithm and implementation based on
our formalism. We have illustrated the use of our algorithm
to create operating procedures.
We validate that our method based on planning accurately
captures the meaning of purpose restrictions with intuitive
examples (Sections III-C, IV-B, IV-C, and V-C) and an em-
pirical study of how people understand the word “purpose”
in the context of privacy policy enforcement.
We use our formalism to explain and compare previous
methods of policy enforcement in terms of a formal seman-
tics. Our formalism highlights that an action can be for
a purpose even if that purpose is never achieved, a point
present in philosophical work on the subject (e.g., [18]),
but whose ramiﬁcations on policy enforcement had been
unexplored. Fundamentally, our work shows the difﬁculties
of enforcement due to issues such as the tenable deniability
of ulterior motives (Sections IV-B and IV-C).
However, we recognize the limitations of our formalism.
While MDPs are useful for automated planning, they are not
specialized for modeling planning by humans. While this
concern does not apply to creating operating procedures,
it holds human auditees to unrealistically high standards
leading to the search for models reﬂecting the bounded
abilities of humans to plan. However, “[a] comprehensive,
coherent theory of bounded rationality is not available” [48,
p. 14]. Nevertheless, we believe the essence of our work
is correct: an action is for a purpose if the actor selects to
perform that action while planning for the purpose. Future
work will instantiate our semantic framework with more
complete models of human planning.
Additionally, future work will make our formalism easier
to use. To use our auditing algorithm, an auditor must not
only log the auditee’s behavior but also know how the
auditee could have behaved with an environment model.
Given the difﬁculty of this task, we desire methods of
ﬁnding policy violations that do not require a full model. For
example, Experience-Based Access Management iteratively
reﬁnes a role hierarchy to improve the accuracy of Role-
Based Access Control [49]. Using our semantics, similar
reﬁnements may improve an environment model.
Acknowledgments: We thank Lorrie Faith Cranor,
Joseph Y. Halpern, Dilsun Kaynar, Divya Sharma, Manuela
M. Veloso, and the anonymous reviewers for many helpful
comments on this work. This research was supported by
the U.S. Army Research Ofﬁce grants W911NF0910273
and DAAD-190210389, by the National Science Foundation
(NSF) grants CNS083142 and CNS105224, and by the
HHS grant HHS 90TR0003/01. The views and conclusions
contained in this document are those of the authors and
should not be interpreted as representing the ofﬁcial policies,
either expressed or implied, of any sponsoring institution, the
U.S. government or any other entity.
REFERENCES
[1] The European Parliament and the Council of the European
Union, “Directive 95/46/EC,” Ofﬁcial Journal of the Euro-
pean Union, vol. L 281, pp. 31–50, 1995.
[2] Ofﬁce for Civil Rights, U.S. Department of Health and
Human Services, “Summary of the HIPAA privacy rule,”
OCR Privacy Brief, 2003.
[3] United States Congress, “Financial services modernization act
of 1999,” Title 15, United States Code, Section 6802, 2010.
[4] Washington Radiology Associates, P.C., “Notice of privacy
accessed Feb. 4, 2011. http://www.
practices,” 2003,
washingtonradiology.com/ofﬁce-guide/privacy.asp
[5] Yahoo!, “Privacy policy: Yahoo Mail,” 2010. http://info.
yahoo.com/privacy/us/yahoo/mail/details.html
[6] Bank of America Corporation, “Bank of America privacy
policy for consumers,” 2005, accessed Feb. 4, 2011.
http://www.bankofamerica.com/privacy/pdf/eng-boa.pdf
[7] FairWarning, “Privacy breach detection for healthcare,” White
Paper, 2010. http://www.fairwarningaudit.com/documents/
2010-privacy-breach-detection-fairwarning.pdf
[8] R. Agrawal, J. Kiernan, R. Srikant, and Y. Xu, “Hippocratic
databases,” in VLDB ’02: Proc. of the 28th Int’l. Conf. on Very
Large Data Bases. VLDB Endowment, 2002, pp. 143–154.
[9] J.-W. Byun, E. Bertino, and N. Li, “Purpose based access
control of complex data for privacy protection,” in SACMAT
’05: Proc. of the 10th ACM Sym. on Access Control Models
and Technologies, 2005, pp. 102–110.
[10] K. Hayati and M. Abadi, “Language-based enforcement of
privacy policies,” in PET 2004: Workshop on Privacy En-
hancing Technologies. Springer-Verlag, 2005, pp. 302–313.
[11] S. S. Al-Fedaghi, “Beyond purpose-based privacy access
control,” in ADC ’07: Proc. of the 18th Australasian Database
Conf. Australian Computer Society, Inc., 2007, pp. 23–32.
[12] H. Peng, J. Gu, and X. Ye, “Dynamic purpose-based access
control,” in Int’l. Sym. on Parallel and Distributed Processing
with Applications.
IEEE, 2008, pp. 695–700.
[13] M. Jafari, R. Safavi-Naini, and N. P. Sheppard, “Enforcing
purpose of use via workﬂows,” in WPES ’09: Proc. of the 8th
ACM Workshop on Privacy in the Electronic Society, 2009,
pp. 113–116.
[14] Q. Ni, E. Bertino, J. Lobo, C. Brodie, C.-M. Karat, J. Karat,
and A. Trombetta, “Privacy-aware role-based access control,”
ACM Trans. Inf. Syst. Secur., vol. 13, pp. 24:1–24:31, 2010.
189
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:51 UTC from IEEE Xplore.  Restrictions apply. 
[15] M. Enamul Kabir, H. Wang, and E. Bertino, “A conditional
purpose-based access control model with dynamic roles,”
Expert Syst. Appl., vol. 38, pp. 1482–1489, 2011.
[16] “purpose, n.” in The Oxford English Dictionary, 2nd ed.
Oxford University Press, 1989.
[17] J. P. Das, B. C. Kar, and R. K. Parrila, Cognitive Planning:
The Psychological Basis of Intelligent Behavior. Sage, 1996.
[18] R. Taylor, Action and Purpose. Prentice-Hall, 1966.
[19] M. C. Tschantz, A. Datta, and J. M. Wing, “Formalizing
and enforcing purpose restrictions in privacy policies (full
version),” School of Computer Science, Carnegie Mellon
University, Tech. Rep. CMU-CS-12-106, Mar. 2012.
[20] J. L. Mackie, The Cement of
the Universe: A Study of
Causation. Oxford University Press, 1974.
[21] R. Bellman, “On the theory of dynamic programming,” Proc.
of the Nat. Academy of Sciences, vol. 38, pp. 716–719, 1952.
[22] S. J. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern
Approach, 2nd ed. Pearson Education, 2003.
[23] F. d’Epenoux, “A probabilistic production and inventory prob-
lem,” Management Science, vol. 10, no. 1, pp. 98–108, 1963.
[24] L. G. Khachian, “A polynomial algorithm in linear program-
ming,” Dokl. Akad. Nauk SSSR, vol. 244, pp. 1093–1096,
1979, English trans.: Soviet Math. Dokl. 20, 191-194, 1979.
[25] N. Karmarkar, “A new polynomial-time algorithm for linear
programming,” in STOC ’84: Proc. of the 16th Annual ACM
Sym. on Theory of Computing, 1984, pp. 302–311.
[26] M. L. Littman, T. L. Dean, and L. P. Kaelbling, “On the
complexity of solving Markov decision problems,” in Proc. of
the 11th Annual Conf. on Uncertainty in Artiﬁcial Intelligence
(UAI 95), 1995, pp. 394–402.
[27] R. Williams and L. C. Baird, “Tight performance bounds on
greedy policies based on imperfect value functions,” in Proc.
of the 10th Yale Workshop on Adaptive and Learning Systems.
Yale University, 1994.
[28] P. Tseng, “Solving h-horizon stationary Markov decision
process in time proportional to log(h),” Operations Research
Letters, vol. 9, no. 5, pp. 287–297, 1990.
[29] Q. McNemar, “Note on the sampling error of the difference
between correlated proportions or percentages,” Psychome-
trika, vol. 12, pp. 153–157, 1947.
[30] A. J. Berinsky, G. A. Huber, and G. S. Lenz, “Using Me-
chanical Turk as a subject recruitment tool for experimental
research,” Submitted for review, 2011.
[31] A. Kittur, E. H. Chi, and B. Suh, “Crowdsourcing user studies
with Mechanical Turk,” in Proceeding of the 26th annual
SIGCHI conference on Human factors in computing systems.
ACM, 2008, pp. 453–456.
[32] S. Chong, A. C. Myers, K. Vikram, and L. Zheng, Jif
Reference Manual, 2009. http://www.cs.cornell.edu/jif
[33] F. Massacci, J. Mylopoulos, and N. Zannone, “Hierarchical
hippocratic databases with minimal disclosure for virtual
organizations,” The VLDB Journal, vol. 15, no. 4, pp. 370–
387, 2006.
[34] A. Barth, J. Mitchell, A. Datta, and S. Sundaram, “Privacy
and utility in business processes,” in CSF ’07: Proc. of the
20th IEEE Computer Security Foundations Sym., 2007, pp.
279–294.
[35] T. D. Breaux and A. I. Ant´on, “Analyzing regulatory rules for
privacy and security requirements,” IEEE Trans. Softw. Eng.,
vol. 34, no. 1, pp. 5–20, 2008.
[36] C. Powers and M. Schunter, “Enterprise privacy authorization
language (EPAL 1.2),” W3C Member Submission, 2003.
[37] L. F. Cranor, Web Privacy with P3P. O’Reilly, 2002.
[38] M. E. Bratman, Intention, Plans, and Practical Reason.
Cambridge, Mass.: Harvard University Press, 1987.
[39] O. Roy, “Thinking before acting: Intentions, logic, rational
choice,” Ph.D. dissertation, Institute for Logic, Language and
Computation; Universiteit van Amsterdam, 2008.
[40] C. Schmidt, N. Sridharan, and J. Goodson, “The plan recog-
nition problem: An intersection of psychology and artiﬁcial
intelligence,” Artiﬁcial Intelligence, vol. 11, no. 1-2, pp. 45
– 83, 1978.
[41] C. L. Baker, J. B. Tenenbaum, and R. Saxe, “Bayesian models
of human action understanding,” in Advances in Neural
Information Processing Systems (NIPS), vol. 18. MIT Press,
2006, pp. 99–106.
[42] C. L. Baker, R. Saxe, and J. B. Tenenbaum, “Action under-
standing as inverse planning,” Cognition, vol. 113, pp. 329–
349, 2009.
[43] M. Ram´ırez and H. Geffner, “Goal recognition over POMDPs:
Inferring the intention of a POMDP agent,” in IJCAI,
T. Walsh, Ed.
IJCAI/AAAI, 2011, pp. 2009–2014.
[44] W. Mao and J. Gratch, “A utility-based approach to intention
recognition,” in AAMAS 2004 Workshop on Agent Tracking:
Modeling Other Agents from Observations, 2004.
[45] J. Azarewicz, G. Fala, R. Fink, and C. Heithecker, “Plan
recognition for airborne tactical decision making,” in Nat.
Conf. on Artiﬁcial Intelligence, 1986, pp. 805–811.
[46] C. W. Geib and R. P. Goldman, “Plan recognition in intrusion
detection systems,” in DARPA Information Survivability Conf.
and Exposition (DISCEX), 2001.
[47] F. Cuppens, F. Autrel, A. Mi`ege, and S. Benferhat, “Recog-
nizing malicious intention in an intrusion detection process,”
in 2nd Int’l. Conf. on Hybrid Intelligent Systems.
IOS Press,
2002, pp. 806–817.
[48] G. Gigerenzer and R. Selten, Eds., Bounded Rationality: The
Adaptive Toolbox. MIT Press, 2002.
[49] W. Zhang, C. A. Gunter, D. Liebovitz, J. Tian, and B. Malin,
“Role prediction using electronic medical record system au-
dits,” in AMIA 2011 Annual Symposium. American Medical
Informatics Association, Oct. 2011, pp. 858–867.
190
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:51 UTC from IEEE Xplore.  Restrictions apply.