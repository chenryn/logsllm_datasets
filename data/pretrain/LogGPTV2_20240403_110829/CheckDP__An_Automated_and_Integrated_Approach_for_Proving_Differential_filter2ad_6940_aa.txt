title:CheckDP: An Automated and Integrated Approach for Proving Differential
Privacy or Finding Precise Counterexamples
author:Yuxin Wang and
Zeyu Ding and
Daniel Kifer and
Danfeng Zhang
CheckDP: An Automated and Integrated Approach for Proving
Differential Privacy or Finding Precise Counterexamples
Yuxin Wang, Zeyu Ding, Daniel Kifer, Danfeng Zhang
The Pennsylvania State University
{yxwang,zyding}@psu.edu,{dkifer,zhang}@cse.psu.edu
0
2
0
2
p
e
S
1
1
]
L
P
.
s
c
[
2
v
5
8
4
7
0
.
8
0
0
2
:
v
i
X
r
a
ABSTRACT
We propose CheckDP, an automated and integrated approach for
proving or disproving claims that a mechanism is differentially
private. CheckDP can find counterexamples for mechanisms with
subtle bugs for which prior counterexample generators have failed.
Furthermore, it was able to automatically generate proofs for correct
mechanisms for which no formal verification was reported before.
CheckDP is built on static program analysis, allowing it to be more
efficient and precise in catching infrequent events than sampling
based counterexample generators (which run mechanisms hun-
dreds of thousands of times to estimate their output distribution).
Moreover, its sound approach also allows automatic verification of
correct mechanisms. When evaluated on standard benchmarks and
newer privacy mechanisms, CheckDP generates proofs (for cor-
rect mechanisms) and counterexamples (for incorrect mechanisms)
within 70 seconds without any false positives or false negatives.
KEYWORDS
Differential privacy; formal verification; counterexample detection
1 INTRODUCTION
Differential privacy [27] has been adopted in major data sharing
initiatives by organizations such as Google [15, 29], Apple [48],
Microsoft [22], Uber [36] and the U.S. Census Bureau [1, 17, 35, 41].
It allows these organizations to collect and share data with provable
bounds on the information that is leaked about any individual.
Crucial to any differentially private system is the correctness of
privacy mechanisms, the underlying privacy primitives in larger
privacy-preserving algorithms. Manually developing the necessary
rigorous proofs that a mechanism correctly protects privacy is a
subtle and error-prone process. For example, detailed explanations
of significant errors in peer-reviewed papers and systems can be
found in [21, 40, 42]. Such mistakes have led to research in the ap-
plication of formal verification for proving that mechanisms satisfy
differential privacy [3, 5, 7, 9–11, 50, 51]. However, if a mechanism
has a bug making its privacy claim incorrect, these techniques can-
not disprove the privacy claims – a counterexample detector must
be used instead [14, 23, 34]. Finding a counterexample is typically
a two-phase process that (1) first searches an infinitely large space
for candidate counterexamples and then (2) uses an exact symbolic
probabilistic solver like PSI [33] to verify that the counterexam-
ple is indeed valid. The search phase currently presents the most
problems (i.e., large runtimes or failure to find counterexamples are
most often attributed to the search phase). Earlier search techniques
were based on sampling (running a mechanism hundreds of thou-
sands of times), which made them slow and inherently imprecise:
even with enormous amounts of samples, they can still fail if a
privacy-violating section of code is not executed frequently enough
or if the actual privacy cost is slightly higher than the privacy claim.
Recently, static program analyses were proposed to accomplish
both goals [4, 30]. However, they either only analyze a non-trivial
but restricted class of programs [4], or rely on heuristic strategies
whose effectiveness on many sutble mechanisms is unclear [30].
In this paper, we present CheckDP, an automated and integrated
tool for proving or disproving the correctness of a mechanism that
claims to be differentially private. Significantly, CheckDP automati-
cally finds counterexamples via static analysis, making it unneces-
sary to run the mechanism. Like prior work [14], CheckDP still uses
PSI [33] at the end. However, replacing sampling-based search with
static analysis enables CheckDP to find violations in a few seconds,
while previous sampling-based methods [14, 23] may fail even after
running for hours. Furthermore, sampling-based methods may still
require manual setting of some program inputs (e.g., DP-Finder [14]
requires additional arguments to be set manually for Sparse Vector
Technique in our evaluation) while CheckDP is fully automated.
Furthermore, the integrated approach of CheckDP allows it to effi-
ciently analyze a larger class of differentially privacy mechanisms,
compared with concurrent work using static analyses [4, 30].
Meanwhile, CheckDP still offers state-of-the-art verification ca-
pability compared with existing language-based verifiers and is
further able to automatically generate proofs for 3 mechanisms
for which no formal verification was reported before. CheckDP
takes the source code of a mechanism along with its claimed level
of privacy and either generates a proof of correctness or a verifi-
able counterexample (a pair of related inputs and a feasible output).
CheckDP is built upon a proof technique called randomness align-
ment [24, 50, 51], which recasts the task of proving differential
privacy into one of finding alignments between random variables
used by two related runs of the mechanism. CheckDP uses a novel
verify-invalidate loop that alternatively improves tentative proofs
(in the form of alignments), which are then used to improve ten-
tative counterexamples (and vice versa) until either the tentative
proof has no counterexample, or the tentative counterexample has
no alignment.
We evaluated CheckDP on correct/incorrect versions of existing
benchmarks and newly proposed mechanisms. It generated a proof
for each correct mechanism within 70 seconds and a counterexam-
ple for each incorrect mechanism within 15 seconds.
In summary, this paper makes the following contributions:
(1) CheckDP, one of the first automated tools (with concurrent
work [4, 30]) that generates both proofs for correct mechanisms
and counterexamples for incorrect mechanisms (Section 2.4).
(2) A syntax-directed translation from the probabilistic mechanism
being checked to non-probabilistic target code with explicit
proof obligations (Section 3).
(3) An alignment template generation algorithm (Section 3.4).
(4) A novel verify-invalidate loop that incrementally improves
tentative proofs and counterexamples (Section 4).
(5) Case studies and experimental comparisons between CheckDP
and existing tools using correct/incorrect versions of existing
benchmarks and newly proposed mechanisms. For incorrect
mechanisms, CheckDP automatically found counterexamples in
all cases, even in cases where competing methods [14, 23] failed.
For correct mechanisms, CheckDP automatically generated
proofs of privacy, including proofs for 3 mechanisms for which
no formal verification was reported before (Section 5).
2 PRELIMINARIES AND RUNNING EXAMPLE
2.1 Differential Privacy
Among several popular variants of differential privacy [16, 26, 27,
43], we focus on pure differential privacy [27]. The goal of differ-
ential privacy is to hide the effect of any person’s record on the
output of an algorithm. This is achieved by considering all pairs of
datasets D and D′ that differ on one record. We call such datasets
adjacent and denote it by D ∼ D′. To offer privacy, a differentially
private algorithm injects carefully calibrated random noise dur-
ing its computation. Given a pair of datasets (D, D′), we call the
execution of an algorithm on D the original execution and the ex-
ecution on (neighboring) D′ the related execution. Intuitively, we
say a randomized algorithm is differentially private if the output
distribution of the original execution and its related execution are
hard to distinguish for all such dataset pairs:
Definition 1 (Pure Differential Privacy [25]). Let ϵ ≥ 0. A
probabilistic computation M : D → O is ϵ-differentially private if
for every pair of neighboring datasets D ∼ D′ ∈ D and every output
o ∈ O, P[M(D) = o] ≤ eϵ P[M(D′) = o].
Often, a differentially private algorithm M interacts with a dataset
D through a list of queries f1, f2, . . .: it iteratively runs a query fi
on D to get an exact answer qi, then performs some randomized
computation on the set of query answers {qj | j ≤ i}. We call the
vector (q1, q2, . . .) along with other data-independent parameters to
M (e.g., privacy parameter ϵ) an input to M. The notion of adjacent
datasets translates into the notion of sensitivity on those queries:
Definition 2 (Global Sensitivity [28]). The global sensitivity
of a query f is ∆f = supD∼D′(cid:12)(cid:12)f (D) − f (D′)(cid:12)(cid:12).
2, . . .), params} are adjacent with respect to the queries f1, f2, . . .,
We say two inputs inp = {(q1, q2, . . .), params} and inp′ =
{(q′
1, q′
and write inp ∼ inp′, if the params are the same and there exist
two adjacent datasets D and D′ such that (f1(D), f2(D), . . . ) =
(q1, q2, . . .) and (f1(D′), f2(D′), . . . ) = (q′
2, . . .). Note that this
(cid:12)(cid:12)(cid:12) ≤ ∆fi ,∀i. It follows that differential privacy
implies that(cid:12)(cid:12)(cid:12)qi − q′
1, q′
i
can be proved by showing that for all pair of inputs inp ∼ inp′ and
all outputs o ∈ O, P[M(inp) = o] ≤ eϵ P[M(inp′) = o]. As stan-
dard, we assume that the sensitivity of inputs are either manually
specified or computed by sensitivity analysis tools (e.g., [32, 44]).
Many mechanisms are built on top of the Laplace Mechanism
[27] which adds Laplace noise to query answers:
Theorem 1 (Laplace Mechanism [27]). Let ϵ > 0, let D be
a dataset, let f be a query with sensitivity ∆f and let q = f (D).
2
The Laplace Mechanism which, on input q, outputs q + η (where
η is sampled from the Laplace distribution with mean 0 and scale
parameter ∆f /ϵ) satisfies ϵ-differential privacy.
We sometimes abuse notation and refer to the sensitivity ∆q of a
numerical value q – we always take this to mean as the sensitivity
of the function that produced q.
2.2 Randomness Alignment
Randomness alignment is a simple yet powerful proof technique
that underpins the verification tools LightDP [51] and its successor
ShadowDP [50]. Precise reasoning using this proof technique was
used to improve a variety of algorithms, allowing them to release
strictly more information at the same privacy cost [24]. Given two
executions of a randomized algorithm M on D and D′ respectively, a
randomness alignment is a mapping between the random variables
in the first execution to random variables in the second execution
that will cause the second execution to always produce the same
output as the first. Upper bounds on privacy parameters depend on
how much the random variables change under this mapping [51].
We use the Laplace Mechanism [28] to illustrate the key ideas
behind randomness alignment. Let D ∼ D′ be a pair of neighboring
datasets and let f be a query with sensitivity ∆f . Let q = f (D)
and q′ = f (D′) be the respective query answers. If we use the
Laplace Mechanism to answer these queries with privacy, on input
q (resp. q′) it will output q + η (resp. q′ + η′) where η (resp. η′) is a
Laplace random variable with scale ∆f /ϵ. In order for the Laplace
Mechanism to produce the same output in both executions, we
need q + η = q′ + η′ and therefore η′ = η + q − q′. This creates
a “mapping” between the values of random noises: if we change
the input from q to q′, we need to adjust the random noise by an
amount of q − q′ (i.e., this is the distance we need to move η′ to get
proof follows from the fact that if two random samples η and η′
(from the Laplace distribution with scale ∆f /ϵ) are at most distance
∆f apart, the ratio of their probabilities is at most eϵ . Hence, the
privacy cost, the natural log of this ratio, is bounded by ϵ.
Thus randomness alignment can be viewed in terms of distances
that we need to move random variables. Let q ∼ q′ be query answers
from neighboring datasets and M be a randomized algorithm which
to η). Clearly(cid:12)(cid:12)q − q′(cid:12)(cid:12) ≤ ∆f by definition of sensitivity. The privacy
uses a set of random noises H =(cid:8)η(cid:9). We associate to every random
variable η a numeric value(cid:98)η which tracks precisely the amount in
of M with input q and random values(cid:8)η(cid:9) is the same as that of
M with input q′ and random values(cid:8)η +(cid:98)η(cid:9). Taking M to be the
is(cid:8)(cid:98)η = q − q′(cid:9). Note that the alignment is a function that depends
Laplace Mechanism, then the alignment in the previous paragraph
on M as well as q and q′.
If all of the random variables are Laplace, the cost of an alignment
distance
noise scale for each random variable. To find
is the summation of
the overall privacy cost (e.g., the ϵ in differential privacy), we then
find an upper bound on the alignment cost for all related q and q′.
2.3 Privacy Proof and Counterexample
Not all randomness alignments serve as proofs of differential pri-
vacy. To form a proof, one must show that (1) the alignment forces
value we need to change η in order to obtain the same output when
the input to M is changed from q to q′. In other words, the output
the two related executions to produce the same output, (2) the pri-
vacy cost of an alignment must be bounded by the promised level of
privacy, and (3) the alignment is injective. Hence, in this paper, an
(alignment-based) privacy proof refers to a randomness alignment
that satisfies these requirements.
On the other hand, to show that an algorithm violates differential
privacy, it suffices to demonstrate the existence of a counterexample.
Formally, if an algorithm M claims to satisfy ϵ-differential privacy,
a counterexample to this claim is a triple (inp, inp′, o) such that
inp ∼ inp′ and P[M(inp) = o] > eϵ P[M(inp′) = o].
Challenges. LightDP [51] and ShadowDP [50] can check if a
manually generated alignment is an alignment-based privacy proof.
On the other hand, an exact symbolic probabilistic solver, such as
PSI [33], can check if a counterexample, either generated manually
or via a sampling-based generator, witnesses violation of differ-
ential privacy. To the best of our knowledge, CheckDP is the first
tool that automatically generates alignment-based proofs/counterex-
amples via static program analysis.1 To do so, a key challenge is
to tackle the infinite search space of proofs (i.e., alignments) and
counterexamples. CheckDP uses a novel proof template generation
algorithm to reduce the search space of candidate alignments (Sec-
tion 3) and uses a novel verify-invalidate loop (Section 4) to find
tentative proofs, counterexamples showing their privacy cost is too
high, improved proofs, improved counterexamples, etc.
2.4 Running Examples
To illustrate our approach, we now discuss two variants of the
Sparse Vector Technique [28], one correct and one incorrect. Using
the two variants, we sketch how CheckDP automatically proves/dis-
proves (as appropriate) their claimed privacy properties.
Sparse Vector Technique (SVT) [28]. A powerful mechanism proven
to satisfy differential privacy. It can be used as a building block for
many advanced differentially private algorithms. This mechanism
is designed to solve the following problem: given a series of queries
and a preset public threshold, we want to identify the first N queries
whose answers are above the threshold, but in a privacy-preserving
manner. To achieve this, it adds independent Laplace noise both to
the threshold and each query answer, then it returns the identities
of the first N queries whose noisy answers are above the noisy
threshold. The standard implementation of SVT outputs true for
the above-threshold queries and false for the others (and termi-
nates when there are a total of N outputs equal to true). We use
two variants of SVT for an overview of CheckDP.
GapSVT. This is an improved (and correct) variant of SVT which
provides numerical information about some queries. When a noisy
query exceeds the noisy threshold, it outputs the difference between
these noisy values; otherwise it returns false. This provides an
estimate for how much higher a query is compared to the threshold.
The algorithm was first proposed and verified in [50]; its pseudo
code is shown in Figure 1. Here, Lap (2/ϵ) draws one sample from