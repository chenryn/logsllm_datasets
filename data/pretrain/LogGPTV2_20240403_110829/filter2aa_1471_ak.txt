called deferred because they might not execute immediately. The kernel uses
DPCs to process timer expiration (and release threads waiting for the timers)
and to reschedule the processor after a thread’s quantum expires (note that
this happens at DPC IRQL but not really through a regular kernel DPC).
Device drivers use DPCs to process interrupts and perform actions not
available at higher IRQLs. To provide timely service for hardware interrupts,
Windows—with the cooperation of device drivers—attempts to keep the
IRQL below device IRQL levels. One way that this goal is achieved is for
device driver ISRs to perform the minimal work necessary to acknowledge
their device, save volatile interrupt state, and defer data transfer or other less
time-critical interrupt processing activity for execution in a DPC at
DPC/dispatch IRQL. (See Chapter 6 in Part 1 for more information on the
I/O system.)
In the case where the IRQL is passive or at APC level, DPCs will
immediately execute and block all other non-hardware-related processing,
which is why they are also often used to force immediate execution of high-
priority system code. Thus, DPCs provide the operating system with the
capability to generate an interrupt and execute a system function in kernel
mode. For example, when a thread can no longer continue executing, perhaps
because it has terminated or because it voluntarily enters a wait state, the
kernel calls the dispatcher directly to perform an immediate context switch.
Sometimes, however, the kernel detects that rescheduling should occur when
it is deep within many layers of code. In this situation, the kernel requests
dispatching but defers its occurrence until it completes its current activity.
Using a DPC software interrupt is a convenient way to achieve this delayed
processing.
The kernel always raises the processor’s IRQL to DPC/dispatch level or
above when it needs to synchronize access to scheduling-related kernel
structures. This disables additional software interrupts and thread
dispatching. When the kernel detects that dispatching should occur, it
requests a DPC/dispatch-level interrupt; but because the IRQL is at or above
that level, the processor holds the interrupt in check. When the kernel
completes its current activity, it sees that it will lower the IRQL below
DPC/dispatch level and checks to see whether any dispatch interrupts are
pending. If there are, the IRQL drops to DPC/dispatch level, and the dispatch
interrupts are processed. Activating the thread dispatcher by using a software
interrupt is a way to defer dispatching until conditions are right. A DPC is
represented by a DPC object, a kernel control object that is not visible to
user-mode programs but is visible to device drivers and other system code.
The most important piece of information the DPC object contains is the
address of the system function that the kernel will call when it processes the
DPC interrupt. DPC routines that are waiting to execute are stored in kernel-
managed queues, one per processor, called DPC queues. To request a DPC,
system code calls the kernel to initialize a DPC object and then places it in a
DPC queue.
By default, the kernel places DPC objects at the end of one of two DPC
queues belonging to the processor on which the DPC was requested
(typically the processor on which the ISR executed). A device driver can
override this behavior, however, by specifying a DPC priority (low, medium,
medium-high, or high, where medium is the default) and by targeting the
DPC at a particular processor. A DPC aimed at a specific CPU is known as a
targeted DPC. If the DPC has a high priority, the kernel inserts the DPC
object at the front of the queue; otherwise, it is placed at the end of the queue
for all other priorities.
When the processor’s IRQL is about to drop from an IRQL of
DPC/dispatch level or higher to a lower IRQL (APC or passive level), the
kernel processes DPCs. Windows ensures that the IRQL remains at
DPC/dispatch level and pulls DPC objects off the current processor’s queue
until the queue is empty (that is, the kernel “drains” the queue), calling each
DPC function in turn. Only when the queue is empty will the kernel let the
IRQL drop below DPC/dispatch level and let regular thread execution
continue. DPC processing is depicted in Figure 8-17.
Figure 8-17 Delivering a DPC.
DPC priorities can affect system behavior another way. The kernel usually
initiates DPC queue draining with a DPC/dispatch-level interrupt. The kernel
generates such an interrupt only if the DPC is directed at the current
processor (the one on which the ISR executes) and the DPC has a priority
higher than low. If the DPC has a low priority, the kernel requests the
interrupt only if the number of outstanding DPC requests (stored in the
DpcQueueDepth field of the KPRCB) for the processor rises above a
threshold (called MaximumDpcQueueDepth in the KPRCB) or if the number
of DPCs requested on the processor within a time window is low.
If a DPC is targeted at a CPU different from the one on which the ISR is
running and the DPC’s priority is either high or medium-high, the kernel
immediately signals the target CPU (by sending it a dispatch IPI) to drain its
DPC queue, but only as long as the target processor is idle. If the priority is
medium or low, the number of DPCs queued on the target processor (this
being the DpcQueueDepth again) must exceed a threshold (the
MaximumDpcQueueDepth) for the kernel to trigger a DPC/dispatch interrupt.
The system idle thread also drains the DPC queue for the processor it runs
on. Although DPC targeting and priority levels are flexible, device drivers
rarely need to change the default behavior of their DPC objects. Table 8-7
summarizes the situations that initiate DPC queue draining. Medium-high
and high appear, and are, in fact, equal priorities when looking at the
generation rules. The difference comes from their insertion in the list, with
high interrupts being at the head and medium-high interrupts at the tail.
Table 8-7 DPC interrupt generation rules
DP
C 
Pri
ori
ty
DPC Targeted at ISR’s Processor
DPC Targeted at 
Another Processor
Lo
w
DPC queue length exceeds maximum 
DPC queue length, or DPC request rate 
is less than minimum DPC request rate
DPC queue length 
exceeds maximum DPC 
queue length, or system 
is idle
Me
diu
m
Always
DPC queue length 
exceeds maximum DPC 
queue length, or system 
is idle
Me
diu
m-
Hi
gh
Always
Target processor is idle
Hi
gh
Always
Target processor is idle
Additionally, Table 8-8 describes the various DPC adjustment variables
and their default values, as well as how they can be modified through the
registry. Outside of the registry, these values can also be set by using the
SystemDpcBehaviorInformation system information class.
Table 8-8 DPC interrupt generation variables
Variabl
e
Definition
D
e
f
a
u
lt
Ove
rrid
e 
Valu
e
KiMaxi
mumDp
cQueue
Depth
Number of DPCs queued before an interrupt 
will be sent even for Medium or below DPCs
4
Dpc
Que
ueD
epth
KiMini
mumDp
cRate
Number of DPCs per clock tick where low 
DPCs will not cause a local interrupt to be 
generated
3
Mini
mum
Dpc
Rate
KiIdeal
Number of DPCs per clock tick before the 
2
Ideal
DpcRate
maximum DPC queue depth is decremented if 
DPCs are pending but no interrupt was 
generated
0
Dpc
Rate
KiAdjus
tDpcThr
eshold
Number of clock ticks before the maximum 
DPC queue depth is incremented if DPCs aren’t 
pending
2
0
Adju
stDp
cThr
esho
ld
Because user-mode threads execute at low IRQL, the chances are good
that a DPC will interrupt the execution of an ordinary user’s thread. DPC
routines execute without regard to what thread is running, meaning that when
a DPC routine runs, it can’t assume what process address space is currently
mapped. DPC routines can call kernel functions, but they can’t call system
services, generate page faults, or create or wait for dispatcher objects
(explained later in this chapter). They can, however, access nonpaged system
memory addresses, because system address space is always mapped
regardless of what the current process is.
Because all user-mode memory is pageable and the DPC executes in an
arbitrary process context, DPC code should never access user-mode memory
in any way. On systems that support Supervisor Mode Access Protection
(SMAP) or Privileged Access Neven (PAN), Windows activates these
features for the duration of the DPC queue processing (and routine
execution), ensuring that any user-mode memory access will immediately
result in a bugcheck.
Another side effect of DPCs interrupting the execution of threads is that
they end up “stealing” from the run time of the thread; while the scheduler
thinks that the current thread is executing, a DPC is executing instead. In
Chapter 4, Part 1, we discussed mechanisms that the scheduler uses to make
up for this lost time by tracking the precise number of CPU cycles that a
thread has been running and deducting DPC and ISR time, when applicable.
While this ensures the thread isn’t penalized in terms of its quantum, it
does still mean that from the user’s perspective, the wall time (also
sometimes called clock time—the real-life passage of time) is still being
spent on something else. Imagine a user currently streaming their favorite
song off the Internet: If a DPC were to take 2 seconds to run, those 2 seconds
would result in the music skipping or repeating in a small loop. Similar
impacts can be felt on video streaming or even keyboard and mouse input.
Because of this, DPCs are a primary cause for perceived system
unresponsiveness of client systems or workstation workloads because even
the highest-priority thread will be interrupted by a running DPC. For the
benefit of drivers with long-running DPCs, Windows supports threaded
DPCs. Threaded DPCs, as their name implies, function by executing the
DPC routine at passive level on a real-time priority (priority 31) thread. This
allows the DPC to preempt most user-mode threads (because most
application threads don’t run at real-time priority ranges), but it allows other
interrupts, nonthreaded DPCs, APCs, and other priority 31 threads to
preempt the routine.
The threaded DPC mechanism is enabled by default, but you can disable it
by adding a DWORD value named ThreadDpcEnable in the
HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Session
Manager\Kernel key, and setting it to 0. A threaded DPC must be initialized
by a developer through the KeInitializeThreadedDpc API, which sets the
DPC internal type to ThreadedDpcObject. Because threaded DPCs can be
disabled, driver developers who make use of threaded DPCs must write their
routines following the same rules as for nonthreaded DPC routines and
cannot access paged memory, perform dispatcher waits, or make assumptions
about the IRQL level at which they are executing. In addition, they must not
use the KeAcquire/ReleaseSpinLockAtDpcLevel APIs because the functions
assume the CPU is at dispatch level. Instead, threaded DPCs must use
KeAcquire/ReleaseSpinLockForDpc, which performs the appropriate action
after checking the current IRQL.
While threaded DPCs are a great feature for driver developers to protect
the system’s resources when possible, they are an opt-in feature—both from
the developer’s point of view and even the system administrator. As such, the
vast majority of DPCs still execute nonthreaded and can result in perceived
system lag. Windows employs a vast arsenal of performance tracking
mechanisms to diagnose and assist with DPC-related issues. The first of
these, of course, is to track DPC (and ISR) time both through performance
counters, as well as through precise ETW tracing.
EXPERIMENT: Monitoring DPC activity
You can use Process Explorer to monitor DPC activity by opening
the System Information dialog box and switching to the CPU tab,
where it lists the number of interrupts and DPCs executed each
time Process Explorer refreshes the display (1 second by default):
You can also use the kernel debugger to investigate the various
fields in the KPRCB that start with Dpc, such as DpcRequestRate,
DpcLastCount, DpcTime, and DpcData (which contains the
DpcQueueDepth and DpcCount for both nonthreaded and threaded
DPCs). Additionally, newer versions of Windows also include an
IsrDpcStats field that is a pointer to an _ISRDPCSTATS structure
that is present in the public symbol files. For example, the
following command will show you the total number of DPCs that
have been queued on the current KPRCB (both threaded and
nonthreaded) versus the number that have executed:
Click here to view code image
lkd> dx new { QueuedDpcCount = @$prcb->DpcData[0].DpcCount + 
@$prcb->DpcData[1].DpcCount, ExecutedDpcCount = 
((nt!_ISRDPCSTATS*)@$prcb->IsrDpcStats)->DpcCount },d
    QueuedDpcCount   : 3370380
    ExecutedDpcCount : 1766914 [Type: unsigned __int64]
The discrepancy you see in the example output is expected;
drivers might have queued a DPC that was already in the queue, a
condition that Windows handles safely. Additionally, a DPC
initially queued for a specific processor (but not targeting any
specific one), may in some cases execute on a different processor,
such as when the driver uses KeSetTargetProcessorDpc (the API
allows a driver to target the DPC to a particular processor.)
Windows doesn’t just expect users to manually look into latency issues
caused by DPCs; it also includes built-in mechanisms to address a few
common scenarios that can cause significant problems. The first is the DPC
Watchdog and DPC Timeout mechanism, which can be configured through
certain registry values in
HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Session
Manager\Kernel such as DPCTimeout, DpcWatchdogPeriod, and
DpcWatchdogProfileOffset.
The DPC Watchdog is responsible for monitoring all execution of code at
DISPATCH_LEVEL or above, where a drop in IRQL has not been registered
for quite some time. The DPC Timeout, on the other hand, monitors the
execution time of a specific DPC. By default, a specific DPC times out after
20 seconds, and all DISPATCH_LEVEL (and above) execution times out
after 2 minutes. Both limits are configurable with the registry values
mentioned earlier (DPCTimeout controls a specific DPC time limit, whereas
the DpcWatchdogPeriod controls the combined execution of all the code
running at high IRQL). When these thresholds are hit, the system will either
bugcheck with DPC_WATCHDOG_VIOLATION (indicating which of the
situations was encountered), or, if a kernel debugger is attached, raise an
assertion that can be continued.
Driver developers who want to do their part in avoiding these situations
can use the KeQueryDpcWatchdogInformation API to see the current values
configured and the time remaining. Furthermore, the
KeShouldYieldProcessor API takes these values (and other system state
values) into consideration and returns to the driver a hint used for making a
decision whether to continue its DPC work later, or if possible, drop the
IRQL back to PASSIVE_LEVEL (in the case where a DPC wasn’t executing,
but the driver was holding a lock or synchronizing with a DPC in some way).
On the latest builds of Windows 10, each PRCB also contains a DPC
Runtime History Table (DpcRuntimeHistoryHashTable), which contains a
hash table of buckets tracking specific DPC callback functions that have
recently executed and the amount of CPU cycles that they spent running.
When analyzing a memory dump or remote system, this can be useful in
figuring out latency issues without access to a UI tool, but more importantly,
this data is also now used by the kernel.
When a driver developer queues a DPC through KeInsertQueueDpc, the
API will enumerate the processor’s table and check whether this DPC has
been seen executing before with a particularly long runtime (a default of 100
microseconds but configurable through the LongDpcRuntimeThreshold
registry value in
HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Session
Manager\Kernel). If this is the case, the LongDpcPresent field will be set in
the DpcData structure mentioned earlier.
For each idle thread (See Part 1, Chapter 4 for more information on thread
scheduling and the idle thread), the kernel now also creates a DPC Delegate
Thread. These are highly unique threads that belong to the System Idle
Process—just like Idle Threads—and are never part of the scheduler’s default
thread selection algorithms. They are merely kept in the back pocket of the
kernel for its own purposes. Figure 8-18 shows a system with 16 logical
processors that now has 16 idle threads as well as 16 DPC delegate threads.
Note that in this case, these threads have a real Thread ID (TID), and the
Processor column should be treated as such for them.
Figure 8-18 The DPC delegate threads on a 16-CPU system.
Whenever the kernel is dispatching DPCs, it checks whether the DPC
queue depth has passed the threshold of such long-running DPCs (this
defaults to 2 but is also configurable through the same registry key we’ve
shown a few times). If this is the case, a decision is made to try to mitigate
the issue by looking at the properties of the currently executing thread: Is it
idle? Is it a real-time thread? Does its affinity mask indicate that it typically
runs on a different processor? Depending on the results, the kernel may
decide to schedule the DPC delegate thread instead, essentially swapping the
DPC from its thread-starving position into a dedicated thread, which has the
highest priority possible (still executing at DISPATCH_LEVEL). This gives a
chance to the old preempted thread (or any other thread in the standby list) to
be rescheduled to some other CPU.
This mechanism is similar to the Threaded DPCs we explained earlier,
with some exceptions. The delegate thread still runs at DISPATCH_LEVEL.
Indeed, when it is created and started in phase 1 of the NT kernel
initialization (see Chapter 12 for more details), it raises its own IRQL to
DISPATCH level, saves it in the WaitIrql field of its kernel thread data
structure, and voluntarily asks the scheduler to perform a context switch to
another standby or ready thread (via the KiSwapThread routine.) Thus, the
delegate DPCs provide an automatic balancing action that the system takes,
instead of an opt-in that driver developers must judiciously leverage on their
own.
If you have a newer Windows 10 system with this capability, you can run
the following command in the kernel debugger to take a look at how often
the delegate thread was needed, which you can infer from the amount of
context switches that have occurred since boot:
Click here to view code image
lkd> dx @$cursession.Processes[0].Threads.Where(t => 
t.KernelObject.ThreadName->
ToDisplayString().Contains("DPC Delegate Thread")).Select(t => 
t.KernelObject.Tcb.ContextSwitches),d
    [44]             : 2138 [Type: unsigned long]