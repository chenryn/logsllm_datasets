### 6.2 Robustness

#### Parameter Sensitivity in Sparsity-L1
Sparsity-L1 involves a parameter \(\lambda\) in its formulation (Eq. 11). Figure 3 investigates the sensitivity to the choice of this parameter. Specifically, it plots the detection rate of Sparsity-L1 for \(\lambda = 0.1, 0.01, 0.001, 0.0001,\) and \(0.00001\). All values of \(\lambda\) within this range achieve good performance, which is reassuring. This suggests that little training or parameter tuning is needed to adapt the method to different networks or traffic patterns.

#### Measurement Noise
Up to this point, we have assumed perfect link load information for anomaly detection. However, in real-world networks, SNMP byte counts are collected from all routers across the network, and measurement issues such as lack of time synchronization can introduce noise. To evaluate the impact of measurement noise, we multiply white noise terms \(N(1, \sigma)\) with each element of the link load, and then use the result as input to our inference algorithms.

Figure 4 compares the performance of the methods with no noise, 0.5% noise, and 1% noise. Note that measurement errors near 1% throughout the network are significant, as the size of the largest anomalies is also near 1% of the total traffic (Figure 1). Accurately diagnosing anomalies given this level of noise is challenging. Nevertheless, both Sparsity-L1 and Tomogravity demonstrate robustness to measurement noise. For the Tier-1 ISP network, the detection rate remains above 0.8 for large anomalies (small N) and above 0.7 for the top 50 anomalies. These results highlight the strength of our algorithms in handling imperfect measurements.

### 6.3 Time-Varying Routing Matrices

#### Missing Data
Missing measurement data, often due to packet loss during data collection, is common in real networks. This can be problematic, as the loss of link load data effectively produces time-varying routing matrices in the anomography formulation. Fortunately, our extended Sparsity-L1 algorithm can handle this situation, as discussed in Section 4.

Figure 5 shows the performance of the inference algorithms with up to 5% of the data missing, where missing values are selected uniformly at random. Both Sparsity-L1 and Tomogravity experience only minor (almost negligible) performance impact in terms of detection rate. The low sensitivity to missing data is an important feature of these methods, which is critical for practical implementation.

#### Routing Changes
In an operational network, the routing matrix is unlikely to remain unchanged over a few days. Hardware failures, engineering operations, maintenance, and upgrades can cause traffic to be rerouted on alternative paths. Here, we evaluate the impact of routing changes on the performance of our algorithms by simulating faults on internal links.

Figure 6 presents results where up to 3 links are randomly failed or repaired at each time instance. We observe that Sparsity-L1 is very robust to such disturbances in the routing structure, while Tomogravity suffers significant performance degradation. It appears that Tomogravity's early-inverse approach accumulates errors from different routing matrices, making them comparable to the anomalies themselves. This demonstrates another advantage of the late-inverse approach over the early-inverse approach.

### 6.4 Comparison of Anomography Methods

#### Impacts on Inference Accuracy
So far, we have compared the performance of Sparsity-L1 and Early Inverse-Tomogravity under the simple temporal model (forecasting the next data point using the current value). We found that Sparsity-L1 generally outperforms the Early Inverse approach. Additionally, Sparsity-L1 is robust to measurement noise, insensitive to parameter choice, and capable of handling missing data and route changes. We now evaluate the overall performance when applying Sparsity-L1 with other temporal and spatial anomography methods.

Specifically, we compare FFT (Section 3.3.2), Wavelet (Section 3.3.3), PCA (Section 3.2.1), TPCA (Section 3.3.4), and four ARIMA-based methods: Diff (the simple forecasting model of the last section), Holt-Winters, EWMA, and general ARIMA, which determines the appropriate model using the method in [33].

For each model, we compute \(\tilde{x}\) directly from the OD flow traffic data and use it as the benchmark. Next, we compute \(\tilde{b}\) with the same anomography model and construct the \(A\tilde{x} = \tilde{b}\) inference problem. We compare the solution derived through Sparsity-L1 with the benchmark. Figure 7 presents the detection rates for these approaches. To avoid overcrowding the graph, we divide the anomography methods into two groups. Figure 7(a) plots the results for the ARIMA family of anomography approaches, and Figure 7(b) plots the results for the rest.

We observe that for all ARIMA-based approaches, Sparsity-L1 finds very good solutions. With traffic data aggregated at the 10-minute level, simple Diff and EWMA can sufficiently extract the anomalous traffic and warrant a solution that maximizes the sparsity of the anomalies. Holt-Winters performs better than Diff and EWMA because it is more sophisticated and can capture more complex temporal trends in the traffic data. Further sophistication, as incorporated in ARIMA, does not significantly improve performance. In the family of ARIMA models, Holt-Winters provides the best complexity-performance trade-off.

From Figure 7(b), we observe that Sparsity-L1 can also achieve high detection rates under FFT, Wavelet, and TPCA. However, it does not perform well with PCA. This can be explained as follows: When applying spatial PCA to the real traffic matrix \(X\) and the link load matrix \(B\), we obtain two linear transformations \(\tilde{X} = T_xX\) and \(\tilde{B} = T_bB = T_bAX\), respectively. The transformation matrices \(T_x\) and \(T_b\) may differ significantly because the spatial correlation among link loads and OD flows are quite different. Even if we use \(T_x = T_b\), we cannot ensure that \(AT_xX = T_bAX\) (i.e., \(A\tilde{X} = \tilde{B}\)). Thus, the spatial PCA anomography solution is not expected to completely overlap with the \(\tilde{x}\) identified by directly applying spatial PCA on the OD traffic flows. In contrast, temporal anomography methods are self-consistent, ensuring that \(\tilde{B} = A\tilde{X}\).

#### Cross-Validation for Different Methods
To compare the various anomography methods, we use a set of benchmarks, each derived from applying anomaly detection algorithms directly to the OD flows. For each benchmark, we report the success of all the anomography methods, aiming to identify methods that achieve both low false positives and low false negatives for nearly all benchmarks.

Table 1(a) presents the false positives for the Tier-1 ISP network with \(M = 50\) and \(N = 30\) (see Section 5). Results for different values of \(M\) and \(N\) are qualitatively similar. To align our results with the methodology reported in [19], we include the bottom row labeled PCA*, where we use a squared prediction error (SPE) based scheme to determine the set of time intervals at which large anomalies occur and the greedy approach (Section 3.4.2) to solve the inference problem. Note that the number of anomalies reported by PCA* may be less than 328.

| **Top 30** | **Inferred** | **Diff** | **ARIMA** | **EWMA** | **Holt-Winters** | **FFT** | **Wavelet** | **PCA** | **TPCA** |
|------------|--------------|----------|-----------|----------|------------------|---------|-------------|---------|----------|
| **False Positives with Top 50 Benchmark** | | | | | | | | | |
| **Diff** | 3 | 4 | 3 | 4 | 6 | 6 | 17 | 18 | 18 |
| **Holt-Winters** | 4 | 6 | 6 | 17 | 18 | 17 | 3 | 4 | 3 |
| **Wavelet** | 6 | 1 | 6 | 1 | 6 | 6 | 17 | 18 | 17 |
| **TPCA** | 6 | 1 | 6 | 1 | 6 | 6 | 17 | 18 | 17 |
| **PCA** | 6 | 1 | 6 | 1 | 6 | 6 | 17 | 18 | 17 |
| **PCA*** (37) | 6 | 1 | 6 | 1 | 6 | 6 | 17 | 18 | 17 |

This table provides a comprehensive comparison of the false positives for each method, highlighting the strengths and weaknesses of each approach.