Tomogravity: no noise
Tomogravity: 0.5% noise
Tomogravity: 1% noise
10
15
20
25
30
35
40
45
50
N
e
t
a
R
n
o
i
t
c
e
t
e
D
e
t
a
R
n
o
i
t
c
e
t
e
D
1
0.8
0.6
0.4
0.2
0
5
1
0.8
0.6
0.4
0.2
0
5
Sparisty-L1: no missing data
Sparisty-L1: 1% missing data
Sparisty-L1: 2% missing data
Sparisty-L1: 5% missing data
Tomogravity: no missing data
Tomogravity: 1% missing data
Tomogravity: 2% missing data
Tomogravity: 5% missing data
10
15
20
25
30
35
40
45
50
N
Figure 5: Impact of Missing Data
Sparsity-L1: no link down
Sparsity-L1: 1 link down
Sparsity-L1: 2 links down
Sparsity-L1: 3 links down
Tomogravity: no link down
Tomogravity: 1 link down
Tomogravity: 2 links down
Tomogravity: 3 links down
10
15
20
25
30
35
40
45
50
N
Figure 4: Sensitivity to Measurement Noise
Figure 6: Impact of Route Change
6.2 Robustness
λ in Sparsity-L1. Sparsity-L1 involves a parameter λ in
its formulation (Eq. 11). Figure 3 investigates the sensi-
tivity to the parameter choice. Speciﬁcally, Figure 3 plots
the detection rate of Sparsity-L1 for λ = 0.1, 0.01, 0.001,
0.0001 and 0.00001. All λ in this range achieve good per-
formance. This is reassuring, since it suggests that little
training or parameter tuning is needed to match the method
to a different network or trafﬁc pattern.
Measurement Noise.
Thus far, we have assumed per-
fect link load information for anomaly detection. However,
in real networks, SNMP byte counts are collected from all
routers across the network. Inevitably, measurement issues
such as lack of time synchronization may introduce noise.
Below we evaluate the impact of measurement noise by
multiplying white noise terms N (1, σ) with each element
of the link load, and then using the result as input to our
inference algorithms.
Figure 4 compares how well the methods perform with
no noise, to how well they do with noise levels σ = 0.5%
and σ = 1%. Note that measurement errors near 1%
throughout the network are quite signiﬁcant, since the size
of the largest anomalies are themselves near 1% of the to-
tal trafﬁc (Figure 1). It is a challenging task to accurately
diagnose anomalies given the comparable level of noise.
Nevertheless, we ﬁnd that both Sparsity-L1 and Tomograv-
ity are quite robust to measurement noise. For the Tier-1
ISP network, the detection rate remains above 0.8 for big
anomalies (small N ) and above 0.7 for the top 50 anoma-
lies. These results demonstrate the strength of our algo-
rithms in dealing with imperfect measurements.
6.3 Time Varying Routing Matrices
Missing Data. Missing measurement data, arising from
problems such as packet loss during data collection, is com-
mon in real networks. Indeed, this can be tricky to deal
with, since the loss of link load data has the effect of pro-
ducing time varying routing matrices in the anomography
formulation. Fortunately, as discussed in Section 4, our ex-
tended Sparsity-L1 algorithm can handle this situation.
Figure 5 shows the performance of the inference algo-
rithms with up to 5% of the data missing – missing val-
ues are selected uniformly at random. We see that both
Sparsity-L1 and Tomogravity suffer only minor (almost
negligible) performance impact, in terms of detection rate.
The low sensitivity to missing data is an important feature
of these methods, which is critical for real implementation.
Routing Changes.
In an operational network, the rout-
ing matrix is unlikely to remain unchanged over a few
days. Hardware failures, engineering operations, mainte-
nance and upgrades all may cause trafﬁc to be rerouted on
alternative paths. Here we evaluate the impact of routing
changes on the performance of our algorithms. We intro-
duce routing changes by simulating faults on internal links.
Figure 6 presents results where we have randomly
failed/repaired up to 3 links at each time instance. We ob-
serve that Sparsity-L1 is very robust to such a disturbance
in the routing structure, while Tomogravity suffers signiﬁ-
cant performance impact. It appears that Tomogravity suf-
fers here because errors in the (early) inference step, being
USENIX Association
Internet Measurement Conference 2005  
327
computed from different routing matrices, add to become
comparable to the anomalies themselves. This demon-
strates another advantage of the late-inverse over the early-
inverse approach.
6.4 Comparison of Anomography Methods
6.4.1 Impacts on Inference Accuracy
Thus far, we have compared the performance of Sparsity-
L1 and Early Inverse-Tomogravity, under the simple tem-
poral model (forecasting the next data point using the cur-
rent value). We found that Sparsity-L1 in general out-
performs the Early Inverse approach. We also observed
that Sparsity-L1 is robust to measurement noise, is insen-
sitive to parameter choice, and is able to handle missing
data and route changes. We now evaluate overall perfor-
mance when applying Sparsity-L1 with other temporal and
spatial anomography methods. In particular, we compare
FFT (Section 3.3.2), Wavelet (Section 3.3.3), PCA (Sec-
tion 3.2.1), TPCA (Section 3.3.4), and four ARIMA based
methods, Diff (the simple forecasting model of the last sec-
tion), Holt-Winters, EWMA, and general ARIMA, which
determines the appropriate model using the method in [33].
As noted in Section 5, for each model considered, we
compute ˜x directly from the OD ﬂow trafﬁc data and use
it as the benchmark. Next, we compute ˜b with the same
anomography model, and construct the A˜x = ˜b infer-
ence problem. We compare the solution derived through
Sparsity-L1 with the benchmark. Figure 7 presents the de-
tection rate for these approaches. To avoid overcrowding
the graph, we divide the anomography methods into two
groups. Figure 7 (a) plots the results for the ARIMA fam-
ily of anomography approaches and Figure 7 (b) plots the
results for the rest. We observe that for all the ARIMA
based approaches, Sparsity-L1 ﬁnds very good solutions.
With the trafﬁc data aggregated at the 10-minute level, sim-
ple Diff and EWMA can sufﬁciently extract the anoma-
lous trafﬁc and warrant a solution that maximizes the spar-
sity of the anomalies. Holt-Winters produces better perfor-
mance than Diff and EWMA. This is because the model is
more sophisticated, and thus is able to capture more com-
plex temporal trends exhibited in the trafﬁc data. Further
sophistication, as incorporated in ARIMA, however, can-
not signiﬁcantly improve performance.
In the family of
ARIMA models, Holt-Winters appears to provide the best
complexity-performance trade-off.
From Figure 7 (b), we observe that Sparsity-L1 can also
achieve high detection rate under FFT, Wavelet and TPCA.
However, it doesn’t work well with PCA2. This can be ex-
plained as follows. When we apply spatial PCA on the real
trafﬁc matrix X and the link load matrix B, we obtain two
linear transformation ˜X = TxX, and ˜B = TbB = TbAX,
respectively. However, the two transformation matrices
Tx and Tb may differ signiﬁcantly because the spatial cor-
relation among link loads and that among OD ﬂows are
rather different. Even if we use Tx = Tb, we cannot en-
sure that ATxX = TbAX (i.e., A ˜X = ˜B (Note that this
e
t
a
R
n
o
i
t
c
e
t
e
D
1
0.8
0.6
0.4
0.2
0
5
DIFF
ARIMA
EWMA
Holt-Winters
10
15
20
25
30
35
40
45
50
N
(a) ARIMA family of anomography methods
e
t
a
R
n
o
i
t
c
e
t
e
D
1
0.8
0.6
0.4
0.2
0
5
FFT
Wavelet
PCA
TPCA
10
15
20
25
30
35
40
45
50
N
(b) Other anomography methods
Figure 7: Sparsity-L1 with Various Anomography Methods
last comment applies to spatial anomography methods in
general). Thus, the spatial PCA anomography solution is
not expected to completely overlap with the ˜x identiﬁed
by directly applying spatial PCA on the OD trafﬁc ﬂows.
In contrast, the temporal anomography methods are self-
consistent in that given ˜B = BT , if we apply the same
transformation T on X and obtain ˜X = XT , we guarantee
that ˜B = A ˜X (= AXT ).
6.4.2 Cross Validation for Different Methods
We now turn to comparing the various anomography meth-
ods . To do so, we use a set of benchmarks, as described in
Section 5, each derived from applying anomaly detection
algorithm directly to the OD ﬂows. For each benchmark,
we report on the success of all of the anomography meth-
ods. The hope is that methods emerge that achieve both
low false positives and low false negatives for nearly all of
the benchmarks.
In Table 1 (a) we present the false positives for the Tier-
1 ISP network with M = 50 and N = 30 (see Section
5). We found results for different values of M and N to
be qualitatively quite similar. To align our results with the
methodology reported in [19], we include the bottom row,
labeled PCA*, where we use a squared prediction error
(SPE) based scheme to determine the set of time intervals at
which big anomalies occur, and the greedy approach (Sec-
tion 3.4.2) to solve the inference problem. Note that the
number of anomalies reported by PCA* may be less than
328
Internet Measurement Conference 2005 
USENIX Association
Top 30
Inferred Diff ARIMAEWMA H-W FFT Wavelet PCA TPCA
False Positives with Top 50 Benchmark
Diff
ARIMA
EWMA
3
4
3
Holt-Winters 4
6
6
17
18
18
Wavelet
TPCA
PCA
PCA*(37)
FFT
6
1
6
1
6
6
17
18
17
3
4
3