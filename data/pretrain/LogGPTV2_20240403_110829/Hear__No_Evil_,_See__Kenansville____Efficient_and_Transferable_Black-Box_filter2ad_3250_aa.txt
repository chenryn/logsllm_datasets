title:Hear "No Evil", See "Kenansville"*: Efficient and Transferable Black-Box
Attacks on Speech Recognition and Voice Identification Systems
author:Hadi Abdullah and
Muhammad Sajidur Rahman and
Washington Garcia and
Kevin Warren and
Anurag Swarnim Yadav and
Tom Shrimpton and
Patrick Traynor
9
0
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
Hear “No Evil”, See “Kenansville”*: Efﬁcient and
Transferable Black-Box Attacks on Speech
Recognition and Voice Identiﬁcation Systems
Hadi Abdullah, Muhammad Sajidur Rahman, Washington Garcia, Kevin Warren, Anurag Swarnim Yadav,
Tom Shrimpton, Patrick Traynor
University of Florida
Abstract—Automatic speech recognition and voice identiﬁca-
tion systems are being deployed in a wide array of applications,
from providing control mechanisms to devices lacking traditional
interfaces, to the automatic transcription of conversations and
authentication of users. Many of these applications have signif-
icant security and privacy considerations. We develop attacks
that force mistranscription and misidentiﬁcation in state of the
art systems, with minimal
impact on human comprehension.
Processing pipelines for modern systems are comprised of signal
preprocessing and feature extraction steps, whose output is fed to
a machine-learned model. Prior work has focused on the models,
using white-box knowledge to tailor model-speciﬁc attacks. We
focus on the pipeline stages before the models, which (unlike the
models) are quite similar across systems. As such, our attacks
are black-box, transferable, can be tuned to require zero queries
to the target, and demonstrably achieve mistranscription and
misidentiﬁcation rates as high as 100% by modifying only a few
frames of audio. We perform a study via Amazon Mechanical
Turk demonstrating that
there is no statistically signiﬁcant
difference between human perception of regular and perturbed
audio. Our ﬁndings suggest that models may learn aspects of
speech that are generally not perceived by human subjects, but
that are crucial for model accuracy.
I. INTRODUCTION
lack traditional
Automatic Speech Recognition (ASR) systems and Auto-
matic Voice Identiﬁcation (AVI) systems hold great potential
to improve the ways in which humans and machines inter-
act. Whether providing an intuitive means of communicating
with systems that
interfaces, such as the
Internet of Things (IoT) family of devices, or improving
operator efﬁciency in complex environments, including air
trafﬁc control [1], ASR and AVI systems are increasingly
being deployed in a diverse array of computing environments.
Driving the widespread adoption of such systems are the recent
breakthroughs in machine learning and its efﬁcient application
to speech processing.
There are several attacks against ASR and AVI systems that
exist in the current literature. However, these suffer from one
or more of the following: are not near-real-time, require white-
box knowledge of ASR and AVI systems [2], [3], [4], [5], [6],
can not succeed over the telephony network, require thousands
*The title of our paper plays on “Hear No Evil, See No Evil” and we use
the attacks described in our paper to generate the above title. Thus, when a
model is fed “No Evil”, it mistranscribes it as “Kenansville”, a town located
in Central Florida - text completely unrelated to the audio input.
© 2021, Hadi Abdullah. Under license to IEEE.
DOI 10.1109/SP40001.2021.00009
712
of queries [7], [8], are not transferable [9], or produce poor
quality audio [10], [11].
In this paper, we present the ﬁrst zero query black-box at-
tack, which neither requires query access to nor knowledge of
the target system. This is in contrast to black-box attacks still
do require query access to the model (even though knowledge
of target is not required). Our attack introduces perturbations to
input audio that induce ASR systems to mistranscribe speech,
and AVI systems to misidentifying voices, without impacting
human comprehension of the speech audio samples.
Additionally, our attack transfers across multiple systems.
We do so by focusing on the processing that turns captured
audio into model features. In particular, we exploit the fact
that ASR and AVI systems employ a common set of signal
processing techniques to produce model feature vectors. The
ﬁndings of our work are:
• Our zero-query attack can circumvent any state of the
art ASR and AVI system in a near real-time, black-
box, transferable manner: A key contribution of this
work is the ability to generate audio samples that will
induce errors in any ASR and AVI systems where the
adversary neither has knowledge nor query access to the
target model. Additionally, we show that the resulting
adversarial audio samples will still be transferable (i.e.,
evade unknown models). By leveraging transferability our
attack can succeed with zero to any unknown models.
• Our attack does not signiﬁcantly impact human-
perceived quality or comprehension and works in real
audio environments: Our attack ensures that the result-
ing perturbations will be imperceptible.We substantiate
this claim by conducting an Amazon Turk user study.
Similarly, we test our attack over the cellular network,
which introduces signiﬁcant audio quality degradation
due to transcoding, jitter and packet loss. We show that
even after undergoing such serious degradation, our attack
audio can still trick the target ASR and AVI systems. To
our knowledge, our work is the ﬁrst to generate attack
audio that is robust to the cellular network.
• Our attack is robust to existing adversarial detec-
tion and defense mechanisms: Finally, we evaluate
our attack against existing techniques, which detect or
defend against adversarial samples. For the former, we
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
Model
Google (Normal)
Wit
Original Transcription
The emperor had a mean Temper
then the chOreographer must arbitrate
she had your dark suit in Greasy wash water all year
masquerade parties tax one’S imagination
Attack Transcription
syempre Hanuman Temple
Democrat ographer must arbitrate
nope
stop
TABLE I: By only perturbing a single phoneme (bold faced and underlined), our attack forces ASR systems to completely
mistranscribe the resulting audio.
Fig. 1: Modern ASR systems take several steps to convert
speech into text. (a) Preprocessing removes high frequencies
and noise from the audio waveform, (b) feature extraction
extracts the most important features of the audio sample, and
(c) decoding converts the features into text.
test the attack against the temporal-based method, which
has shown excellent results against traditional adversarial
attacks [12]. We show that
this method has limited
effectiveness against our attack: It is no better than ran-
domly choosing whether an attack is in progress or not.
Regarding defenses, we test our attack against adversarial
training, which has shown promise as a defense in the
adversarial image space [13]. We observe that this method
slightly improves model robustness, but at the cost of a
signiﬁcant decrease in model accuracy.
The remainder of this paper is organized as follows: Sec-
tion II provides background information on topics ranging
from signal processing to phonemes; Section III details our
methodology, including our assumptions and hypothesis; Sec-
tion IV presents our experimental setup and parameterization;
Section V shows our results; Section VI offers further dis-
cussion; Section VII discusses related work; and Section VIII
provides concluding remarks.
II. BACKGROUND