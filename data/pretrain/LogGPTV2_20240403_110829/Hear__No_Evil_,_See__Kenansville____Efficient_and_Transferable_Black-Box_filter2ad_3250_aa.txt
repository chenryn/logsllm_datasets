# Title: Hear "No Evil", See "Kenansville": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems

# Authors:
Hadi Abdullah, Muhammad Sajidur Rahman, Washington Garcia, Kevin Warren, Anurag Swarnim Yadav, Tom Shrimpton, Patrick Traynor  
University of Florida

## Abstract
Automatic speech recognition (ASR) and voice identification (AVI) systems are increasingly being deployed in a wide range of applications, from providing control mechanisms for devices without traditional interfaces to the automatic transcription of conversations and user authentication. These applications often have significant security and privacy considerations. We develop attacks that force mistranscription and misidentification in state-of-the-art ASR and AVI systems with minimal impact on human comprehension.

Modern ASR and AVI systems typically involve signal preprocessing and feature extraction, followed by machine learning models. Prior research has focused on white-box attacks, which require detailed knowledge of the target model. In contrast, our approach targets the preprocessing and feature extraction stages, which are similar across different systems. This allows us to create black-box, transferable attacks that can be tuned to require zero queries to the target system and achieve up to 100% mistranscription and misidentification rates by modifying only a few audio frames. We conducted a study using Amazon Mechanical Turk, which showed no statistically significant difference in human perception between regular and perturbed audio. Our findings suggest that models may learn aspects of speech that are not perceived by humans but are crucial for model accuracy.

## 1. Introduction
Automatic Speech Recognition (ASR) and Automatic Voice Identification (AVI) systems have the potential to revolutionize human-machine interaction. They provide intuitive ways to communicate with devices that lack traditional interfaces, such as Internet of Things (IoT) devices, and can improve operator efficiency in complex environments like air traffic control. The widespread adoption of these systems is driven by recent breakthroughs in machine learning and its efficient application to speech processing.

Several attacks against ASR and AVI systems have been documented in the literature. However, these attacks often suffer from one or more of the following limitations: they are not near-real-time, require white-box knowledge of the target systems, cannot succeed over telephony networks, require thousands of queries, are not transferable, or produce poor-quality audio.

In this paper, we present the first zero-query black-box attack, which does not require query access to or knowledge of the target system. Unlike other black-box attacks, which still need query access to the model, our attack introduces perturbations to input audio that induce ASR systems to mistranscribe speech and AVI systems to misidentify voices, without affecting human comprehension of the audio samples. Additionally, our attack transfers across multiple systems by focusing on the common signal processing techniques used to convert captured audio into model features.

The key contributions of our work are:
- **Zero-Query Attack**: Our attack can circumvent any state-of-the-art ASR and AVI system in a near real-time, black-box, and transferable manner. It generates audio samples that induce errors in any ASR and AVI system, even when the adversary has no knowledge or query access to the target model.
- **Human Perception and Robustness**: The resulting adversarial audio samples do not significantly impact human-perceived quality or comprehension and work in real audio environments. We substantiate this claim through an Amazon Mechanical Turk user study and tests over the cellular network, which introduce significant audio quality degradation due to transcoding, jitter, and packet loss.
- **Robustness to Detection and Defense Mechanisms**: We evaluate our attack against existing detection and defense mechanisms. For detection, we test our attack against a temporal-based method, which has shown excellent results against traditional adversarial attacks. We find that this method is no better than random guessing. For defense, we test our attack against adversarial training, which has shown promise in the adversarial image space. We observe that while this method slightly improves model robustness, it comes at the cost of a significant decrease in model accuracy.

The remainder of this paper is organized as follows: Section II provides background information on signal processing and phonemes; Section III details our methodology, including assumptions and hypotheses; Section IV presents our experimental setup and parameterization; Section V shows our results; Section VI offers further discussion; Section VII discusses related work; and Section VIII provides concluding remarks.

## 2. Background
[This section will provide detailed background information on signal processing, phonemes, and other relevant topics.]
``` 

This revised version is more structured, clear, and professional, with improved flow and coherence.