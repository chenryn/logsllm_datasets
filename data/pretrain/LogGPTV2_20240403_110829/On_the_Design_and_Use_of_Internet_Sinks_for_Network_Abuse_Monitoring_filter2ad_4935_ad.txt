500
2000
8000
32000
Sampled hash table size
)
s
t
s
i
l
k
c
a
l
B
d
e
l
p
m
a
S
s
v
e
u
r
T
(
r
o
r
r
E
t
n
e
c
r
e
P
40
35
30
25
20
15
10
5
0
Hour 1
Hour 2
Hour 3
Hour 4
500
2000
8000
32000
Sampled hash table size
Fig. 11. Error rates for different hash table sizes (x-axis is log scale) using Sample and Hold
method with rates of 1/100 (left) and 1/300 (right)
Our analysis in this paper is based on the use of random sampling as a means for
subnet selection. Our rationale for this approach is based on the observation that over-
all trafﬁc volumes across the service-provider class A address space that we monitor is
quite uniform. The strengths of this approach are that it provides a simple method for
subnet selection, it provides unbiased estimates and it lends itself directly to analysis.
The drawback is that sampling designs that take advantage of additional information
such as clustered or adaptive sampling could provide more accurate population esti-
mates. We leave exploration of these and other sampling methods to future work.
After selecting the sampling design, our analysis focused on the problem of de-
tectability. Speciﬁcally, we were interested in understanding the accuracy of estimates
of total probe populations from randomly selected subsets. If we consider ˆτ is an unbi-
ased estimator of a population total τ then the estimated variance of ˆτ is given by:
(cid:2)
σ2
n
+
1 − p
p
(cid:3)
(cid:4)
µ
n
var(ˆτ) = N 2
(cid:1)(cid:2)
(cid:3)
N − n
N
where N is the total number of units (in our case, subnets), n is the sampled number
of units, µ is the population mean (in our case, the mean number of occurrences of a
speciﬁc type of probe), σ2 is the population variance and p is the probability of detec-
tion for a particular type of probe. In the analysis presented in Section 6.1, we evaluate
the error in population estimates over a range of detection probabilities for different
size samples. The samples consider dividing the class A address space into its com-
ponent class B’s. The probabilities relate directly to detection of worst offenders (top
sources of unsolicited trafﬁc) as in the prior sampling analysis. The results provide a
means for judging population estimation error rates as a function of network bandwidth
consumption.
6.1 Sampling Evaluation
Our evaluation of the impact of sampling in an iSink was an ofﬂine analysis using traces
gathered during one day selected at random from the service-provider iSink. Our objec-
tive was to empirically assess the accuracy of sampling under both memory constrained
and bandwidth constrained conditions. In the memory constrained evaluation, we com-
pare the ability to accurately generate the top 100 heavy hitter source list over four
162
Vinod Yegneswaran, Paul Barford, and Dave Plonka
r
o
r
r
e
d
e
t
a
m
i
t
s
E
100
80
60
40
20
0
p=1.0
p=0.1
p=0.01
p=0.001
p=0.0001
20
40
60
80
100
120
Sampled number of /16 networks
r
o
r
r
e
d
e
t
a
m
i
t
s
E
2000
1500
1000
500
0
Top 10
Top 100
Top 1000
All
20
40
60
80
100
120
Sampled number of /16 networks
Fig. 12. Estimated error (σ/µ) for a given number of randomly selected /16’s. Included are error
estimates for total probes from worst offending source IP over a range of detection probabilities
(left), estimates for total probes for worst offender lists of several sizes (right)
consecutive 1 hour periods using different hash table sizes and different sampling rates.
For each hour in the data set, we compare the percentage difference in the number of
scans generated by the “true” top 100 blacklist and sampled top 100 blacklist sources.
In the bandwidth constrained evaluation, we consider accuracy along three dimensions:
1) estimating the worst offender population with partial visibility, 2) estimating black
lists of different lengths, 3) estimating backscatter population.
Our memory constrained evaluation considers hash table sizes varying from 500 to
64K entries where each entry consists of a source IP and a access attempt count. Note
that the hash table required to maintain the complete list from this data was on the order
of 350K entries We consider two different arbitrarily chosen sampling rates – 1 in 100
and 1 in 300 with uniform probability. In each case, once a source IP address has been
entered into the table, all subsequent packets from that IP are counted. If tables become
full during a given hour then entries with the lowest counts are evicted to make room
for new entries. At the end of each hour, the top 100 from the true and sampled lists are
compared. New lists are started for each hour. The results are shown in Figure 11. These
results indicate that even coarse sampling rates (1/300) and relatively small hash tables
enable fairly accurate black lists (between 5%–10% error). The factor of improvement
between sampling at 1/100 and 1/300 is about 1.5, and there is little beneﬁt to increasing
the hash table size from 5,000 to 20,000. Thus, from the perspective of heavy hitter
analysis in a memory constrained system, sampling can be effectively employed in
iSinks.
As discussed in the prior section in our bandwidth constrained evaluation we con-
sider error introduced in population estimates when using simple random sampling over
a portion of the available IP address space. We argue that simple random sampling is
appropriate for some analysis given the uniform distribution of trafﬁc over our class A
monitor. The cumulative distribution of trafﬁc over a one hour period for half of the
/16 subnets in our class A monitor is shown in Figure 13(right). This ﬁgure shows that
while trafﬁc across all subnets is relatively uniform (at a rate of about 320 packets per
minute per /16), speciﬁc trafﬁc subpopulations – TCP backscatter as an example – can
show signiﬁcant non-uniformity which can have a signiﬁcant impact on sampling.
We use the mean normalized standard deviation (σ/µ) as an estimate of error in our
analysis. In each case, using the data collected in a typical hour on the /8, we empirically
On the Design and Use of Internet Sinks for Network Abuse Monitoring
163
r
o
r
r
e
d
e
t
a
m
i
t
s
E
6000
5000
4000
3000
2000
1000
0
p = 1
p=0.1
p=0.01
p=0.001
20
40
60
80
100
120
Sampled number of /16 networks
l
a
t
o
T
f
o
t
n
e
c
r
e
P
1
0.8
0.6
0.4
0.2
0
20
All Traffic
TCP Backscatter
40
60
/16 networks in the order of volume
80
100
120
Fig. 13. Estimated error for TCP backscatter trafﬁc (left). Cumulative distribution of all trafﬁc
and TCP backscatter trafﬁc across half of the class A address space monitor over a one hour
period. The average number of probes per /16 is 320 packets per minute (right)
assess the estimated error as a function of a randomly selected sample of /16 subnets.
The results of this approach are shown in Figure 12. The graph on the left shows the
ability to accurately estimate the number of probes from the single worst offending
IP source over a range of detection probabilities (i.e., the probability of detecting a
source in a selected /16). This graph indicates that worst offenders are detectable even
with a small sample size and error-prone or incomplete measurements. The graph on
the right shows the ability to accurately estimate black lists from a selected sample of
/16’s. This graph indicates that it is easier to estimate larger rather than smaller black
lists when sampling. We attribute this to the variability in black list ordering across the
/16’s. Finally, Figure 13(left) shows the ability to accurately estimate TCP backscatter
trafﬁc over a range of detection probabilities. The graph suggests that while backscatter
estimates are robust in the face of error-prone or incomplete measurements, estimated
error of total backscatter is quite high even with a reasonably large number of /16’s.
This can be attributed to the non-uniformity of backscatter trafﬁc across the class A
monitor shown in Figure 13(right) and suggests that alternative sampling methods for
backscatter trafﬁc should be explored. On a broader scale, this indicates that traditional
backscatter methodologies that assumes uniformity could be error prone.
7 Summary and Future Work
In this paper we describe the architecture and implementation of an Internet Sink: a
useful tool in a general network security architecture. iSinks have several general de-
sign objectives including scalability, the ability to passively monitor network trafﬁc on
unused IP addresses, and to actively respond to incoming connection requests. These
features enable large scale monitoring of scanning activity as well as attack payload
monitoring. The implementation of our iSink is based on a novel application of the
Click modular router, NAT Filter and the Argus ﬂow monitor. This platform provides an
extensible, scalable foundation for our system and enables its deployment on commod-
ity hardware. Our initial implementation includes basic monitoring and active response
capability which we test in both laboratory and live environments.
164
Vinod Yegneswaran, Paul Barford, and Dave Plonka
We report results from our iSink’s deployment in a live environment comprising
four class B networks and one entire class A network. The objectives of these case
studies were to evaluate iSink’s design choices, to demonstrate the breadth of informa-
tion available from an iSink, and to assess the differences of perspective based on iSink
location in IP address space. We show that the amount of trafﬁc delivered to these iSinks
can be large and quite variable. We see clear evidence of the well documented worm
trafﬁc as well as other easily explained trafﬁc, the aggregate of which can be considered
Internet background noise. While we expected overall volumes of trafﬁc in the class B
monitors and class A monitor to differ, we also found that the overall characteristics
of scans in these networks were quite different. We also demonstrate the capability of
iSinks to provide insights on interesting network phenomenon like periodic probing and
SMTP hot-spots, and their ability gather information on sources of abuse through sam-
pling techniques. A discussion of operational issues, security, and passive ﬁngerprinting
techniques is provided in [32].
The evaluation of our iSink implementation demonstrates both its performance ca-
pabilities and expectations for live deployment. From laboratory tests, we show that
iSinks based on commodity PC hardware have the ability to monitor and respond to
over 20,000 connection requests per second, which is approximately the peak trafﬁc
volume we observed on our class A monitor. This also exceeds the current version of
LaBrea’s performance by over 100%. Furthermore, we show that sampling techniques
can be used effectively in an iSink to reduce system overhead while still providing ac-
curate data on scanning activity.
We intend to pursue future work in a number of directions. First, we plan to expand
the amount of IP address space we monitor by deploying iSinks in other networks.
Next, we intend to supplement iSink by developing tools for datamining and automatic
signature generation.
Acknowledgements
The authors would like to thank Jeff Bartig, Geoff Horne, Bill Jensen and Jim Martin for
all of their help. Exploration of the ﬁltering techniques grew out of fruitful discussions
during Vinod’s internship with Vern Paxson. We would also like to acknowledge con-
tributions of Ruoming Pang in the development of the DCERPC responder. Finally, we
would like to thank the anonymous reviewers for their insightful comments and Diego
Zamboni for his excellent shepherding.
References
1. R. Anderson and A. Khattak. The Use of Information Retrieval Techniques for Intrusion
Detection. In Proceedings of RAID, September 1998.
2. Network Associates. LovGate Virus Summary. http://vil.nai.com/vil/content/Print100183.htm,
2002.
3. C. Bullard. Argus Open Project. http://www.qosient.com/argus/.
4. C. Cranor, Y. Gao, T. Johnson, V. Shkapenyuk, and O. Spatscheck. Gigascope: High Perfor-
mance Network Monitoring with an SQL Interface.
5. E-eye. Analysis: Sasser Worm.
http://www.eeye.com/html/Research/Advisories/AD20040501.html.
On the Design and Use of Internet Sinks for Network Abuse Monitoring
165
6. C. Estan and G. Varghese. New Directions in Trafﬁc Measurement and Accounting. In Pro-
ceedings of ACM SIGCOMM ’02, Pittsburgh, PA, August 2002.
7. A. Feldmann, A. Greenberg, C. Lund, N. Reingold, and J. Rexford. NetScope: Trafﬁc Engi-
neering for IP Networks. IEEE Network Magazine, Special Issue on Internet Trafﬁc Engi-
neering, 2000.
8. B. Greene. BGPv4 Security Risk Assessment, June 2002.
9. B. Greene. Remote Triggering Black Hole Filtering, August 2002.
10. Honeyd: Network Rhapsody for You. http://www.citi.umich.edu/u/provos/honeyd.
11. G. Iannaccone, C. Diot, I. Graham, and N. McKeown. Monitoring very high speed links. In
SIGCOMM Internet Measurement Workshop, November 2001.
12. E. Kohler, R. Morris, B. Chen, J. Jannotti, and F. Kaashoek. The click modular router. ACM
Transactions on Computer Systems, August 2000.
13. W. Lee, S.J. Stolfo, and K.W. Mok. A Data Mining Framework for Building Intrusion De-
tection Models. In IEEE Symposium on Security and Privacy, 1999.
14. T. Liston. The Labrea Tarpit Homepage. http://www.hackbusters.net/LaBrea/.
15. D. Moore. Network Telescopes.
http://www.caida.org/outreach/presentations/2003/dimacs0309/.
16. D. Moore, V. Paxson, S. Savage, C. Shannon, S. Staniford, and N. Weaver. The Spread of the
Sapphire/Slammer Worm. Technical report, CAIDA, 2003.
17. D. Moore, C. Shannon, and K. Claffy. Code Red: A Case Study on the Spread and Victims
of an Internet Worm. In Proceedings of ACM SIGCOMM Internet Measurement Workshop,
Marseilles, France, November 2002.
18. D. Moore, C. Shannon, G. Voelker, and S. Savage. Internet Quarantine: Requirements for
Containing Self-Propagating Code. In Proceedings of IEEE INFOCOM, April 2003.
19. D. Moore, G. Voelker, and S. Savage. Inferring Internet Denial of Service Activity. In Pro-
ceedings of the 2001 USENIX Security Symposium, Washington D.C., August 2001.
20. T. Oetiker. The multi router trafﬁc grapher. In Proceedings of the USENIX Twelvth System
Administration Conference LISA XII, December 1998.
21. V. Paxson. BRO: A System for Detecting Network Intruders in Real Time. In Proceedings
of the 7th USENIX Security Symposium, 1998.
22. D. Plonka. Flawed Routers Flood University of Wisconsin Internet Time Server.
http://www.cs.wisc.edu/ plonka/netgear-sntp.
23. D. Plonka. Flowscan: A network trafﬁc ﬂow reporting and visualization tool. In Proceedings
of the USENIX Fourteenth System Administration Conference LISA XIV, December 2000.
24. Y. Rekhter. RFC 1817: CIDR and Classful Routing, August 1995.
25. M. Roesch. The SNORT Network Intrusion Detection System. http://www.snort.org.
26. S. Staniford, J. Hoagland, and J. McAlerney. Practical Automated Detection of Stealthy
Portscans. In Proceedings of the ACM CCS IDS Workshop, November 2000.
27. S. Staniford, V. Paxson, and N. Weaver. How to Own the Internet in Your Spare Time. In
Proceedings of the 11th USENIX Security Symposium, San Francisco, CA, August 2002.
28. H.S. Teng, K. Chen, and S. C-Y Lu. Adaptive Real-Time Anomaly Detection Using Induc-
tively Generated Sequential Patterns. In IEEE Symposium on Security and Privacy, 1999.
29. The Honeynet Project. http://project.honeynet.org.
30. Trend Micro. WORM RBOT.CC. http://uk.trendmicro-europe.com/enterprise/security info/-
ve detail.php?Vname=WORM RBOT.CC.
31. V. Yegneswaran, P. Barford, and S. Jha. Global Intrusion Detection in the DOMINO Overlay
System. In Proceedings of NDSS, San Diego, CA, 2004.
32. V. Yegneswaran, P. Barford, and D. Plonka. On the Design and Use of Internet Sinks for
Network Abuse Monitoring. University of Wisconsin Technical Report #1497, 2004.
33. V. Yegneswaran, P. Barford, and J. Ullrich. Internet Intrusions: Global Characteristics and
Prevalence. In Proceedings of ACM SIGMETRICS, San Diego, CA, June 2003.