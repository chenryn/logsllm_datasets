54–60
218
438
881
Public Key
Size
Gen.
Evaluation Key
Gen.
Size
0.75 MB
3 ms 2.25 MB
8 ms
21 MB 59 ms
7 MB 24 ms
60 MB 195 ms 180 MB 470 ms
Table 1. Proposed parameter sets. log q and log pi denote the bit lengths of the largest RLWE modulus and
individual RNS primes, respectively. # p(cid:48)
is denotes the number of RNS primes. The standard deviation of fresh
RLWE samples is σ = 3.2. Public keys’ and evaluations keys’ generation times and sizes are those of each party.
ms = 10−3 sec.
6.2 Micro-benchmarks for MKHE Schemes
Table 1 illustrates the selected parameter sets used in experiments. They are default parameter sets in
Microsoft SEAL which provide at least 128-bit of security level according to LWE-estimator [3] and HE
security standard [2]. Generation time and size of secret keys, and execution time of encryption are the
same as those in single-key BFV and CKKS. Decryption and ciphertext addition take 1
2 (k + 1) times
longer than the ordinary HE schemes. We remark that generation time and size of public and evaluation
keys {(bi, Di)}1≤i≤k do not depend on the number of parties or the scheme because the generation can
be executed in a synchronous way.
In our experiments, a homomorphic multiplication is always followed by a relinearization procedure.
BFV requires more NTTs than CKKS overall to perform these operations, and is therefore slower, which
is conﬁrmed by the timing results.
ID #Parties
Mult + Relin
EvalGal
BFV
CKKS
BFV
CKKS
I
II
III
1
2
4
8
1
2
4
8
1
2
4
8
4 ms
20 ms
8 ms
44 ms
16 ms
116 ms
31 ms
365 ms
24 ms
110 ms
49 ms
257 ms
95 ms
717 ms
193 ms
2, 350 ms
172 ms
675 ms
359 ms
1, 715 ms
5, 025 ms
711 ms
17, 450 ms 15, 159 ms 1, 332 ms 1, 413 ms
8 ms
22 ms
67 ms
229 ms
59 ms
165 ms
521 ms
1, 845 ms
465 ms
1, 364 ms
4, 287 ms
3 ms
7 ms
14 ms
28 ms
22 ms
47 ms
88 ms
176 ms
170 ms
333 ms
646 ms
Table 2. Execution time that depends on the number of parties. Multiplication is always followed by a relin-
earization in MKHE. ms = 10−3 sec.
The execution times of multiplications in both MKHE schemes are asymptotically quadratic in the
number of parties as discussed in Section 3.3. In practice, they are better than quadratic, as reported
in Table 2. This is because both multiplication and relinearization include a notable portion of compu-
tation that is linear in the number of parties. The execution times of homomorphic evaluation of Galois
automorphisms are almost linear on the number of the parties as described in Section 5.1.
For the single-party scenario in Table 2, we measured performance of our modiﬁed Microsoft SEAL [46]
with a special modulus. It is infeasible to fairly compare the ordinary BFV and CKKS with their multi-
key variants because the performance of a scheme can be analyzed from various perspectives: space/time
complexity, noise growth, functionality, etc. It is provided merely as a reference point, for a more portable
estimation of MKHE on diﬀerent processors.
6.3 Application to Oblivious Neural Network Inference
The authors of [34] proposed a novel framework to test encrypted neural networks on encrypted data in
the single-key scenario. We consider the same service paradigm but in a multi-key setting: the data and
trained model are encrypted under diﬀerent keys.
6.3.1 Homomorphic Evaluation of CNN
We present an eﬃcient strategy to evaluate CNN prediction model on the MNIST dataset. Each image is
a 28×28 pixel array and will be labeled with 10 possible digits after an arbitrary number of hidden layers.
We assume that a neural network is trained with the plaintext dataset in the clear. Table 3 describes
our neural network topology which uses one convolution layer and two fully-connected (FC) layers with
square activation function. The ﬁnal step is to apply the softmax activation function for a purpose of
probabilistic classiﬁcation, so it is enough to obtain an index of maximum values of outputs in a prediction
phase. Our objective is to predict a single image in an eﬃcient way, thereby achieving a low latency. In
Appendix C, we describe the detailed algorithms for encryption and evaluation.
Layer
Convolution
1st square
Description
Input image 28 × 28, window size 4 × 4,
stride (2, 2), number of output channels 5
Squaring each of the 845 inputs
FC-1
Fully connecting with 845 inputs and 64 outputs
2nd square
Squaring each of the 64 inputs
FC-2
Fully connecting with 64 inputs and 10 outputs
Table 3. Description of our CNN to the MNIST dataset.
The Convolutional Layer. As noted in [35], strided convolution can be decomposed into a sum of
simple convolutions (i.e., the stride parameter = 1). From our choice of the parameters, each of such
simple convolutions takes as inputs 14 × 14 images and 2 × 2 ﬁlters. This representation allows more
SIMD parallelism, since we can pack all the inputs into a single ciphertext and perform four simple
convolutions in parallel. Once this is done, we can accumulate the results across plaintext slots using
rotate-and-sum operations in [31]. Moreover, we can pack multiple channels in a single ciphertext as
in [35, Section VI.D], yielding in a fully-packed ciphertext of the convolution result.
The First Square Layer. This step applies the square activation function to all the encrypted output
of the convolutional layer in an SIMD manner.
The FC-1 Layer. In general, an FC layer with ni inputs and no outputs can be computed as a matrix-
vector multiplication. Let W and v be the no × ni weight matrix and ni-length vector, respectively.
We assume that ni and no are smaller than the number of plaintext slots, and no is much lower than
ni in the context of FC layers. Halevi and Shoup [31] presented the diagonal encoding method which
puts a square matrix in diagonal order, multiplies each of them with a rotation of the input vector, and
then accumulates all the output vectors to obtain the result. Juvekar et al. [35] extended the method
to multiply a vector by a rectangular matrix. If the input vector is encrypted in a single ciphertext, the
total complexity is no homomorphic multiplications, (no − 1) rotations of the input ciphertext of v, and
log(ni/no) rotations for rotate-and-sum algorithm.
We extend their ideas to split the original matrix W into smaller sized blocks and perform computation
on the sub-matrices as shown in Fig. 2. Suppose that the vector v is split into (cid:96) many sub-strings with
the same length. For simplicity, we consider the ﬁrst (cid:96) rows of W. We ﬁrst apply the diagonal method
Fig. 2. Our matrix-vector multiplication algorithm ((cid:96) = 2).
to arrange the 1 × (ni/(cid:96)) sized sub-matrices of W in a way that intermediate numbers are aligned in the
same position across multiple slots after homomorphic multiplications. To be precise, the encryptions of
diagonal components are multiplied with (cid:96) rotations of the encrypted vector and all these encryptions
are added together similar to the diagonal method. Then, the output ciphertext represents (ni/(cid:96))-sized
(cid:96) chunks, each containing partial sums of (cid:96) entries of ni inputs. Finally, we can accumulate these using
a rotate-and-sum algorithm with log(ni/(cid:96)) rotations. As a consequence, the output ciphertext encrypts
the ﬁrst (cid:96) many entries of Wv. We repeat this procedure for each (cid:96) many rows of W, resulting in (no/(cid:96))
ciphertexts.
When ni is signiﬁcantly smaller than the number of plaintext slots ns, the performance can be im-
proved by packing multiple copies of the input vector into a single ciphertext and performing (ns/ni)
aforementioned operations in parallel. The computational cost is (no · ni)/ns homomorphic multiplica-
tions, ((cid:96) − 1) rotations of the input ciphertext of v, and (no · ni)/(ns · (cid:96)) · log(ni/(cid:96)) rotations. We provide
additional details in Appendix C.2. As a result, our method provides a trade-oﬀ between rotations on the
same input ciphertext (which can beneﬁt from the hoisting optimization of [33]) and rotations on distinct
ciphertexts (which cannot beneﬁt from hoisting).
As described in Fig. 2, all slots except the ones corresponding to the result components may reveal
information about partial sums. We therefore multiply the output ciphertexts by a constant zero-one
plaintext vector to remove the information.
The Second Square Layer. This step applies the square activation function to all the output nodes of
the ﬁrst FC layer.
The FC-2 Layer. This step performs a multiplication with small sized weight matrix U and vector
v. As discussed in [31], it can be considered as the linear combination of U’s columns using coeﬃcients
from v. Suppose that the column vectors are encrypted in a single ciphertext in such a way that they are
aligned with the encrypted vector. We ﬁrst repeatedly rotate the encryption of the vector to generate a
single ciphertext with no copies of each entry. Then, we apply pure SIMD multiplication to multiply each
column vector by the corresponding scalar of the vector in parallel. Finally, we aggregate all the resulting
columns over the slots to generate the ﬁnal output.
6.3.2 Performance Evaluation
We evaluated our framework to classify encrypted handwritten images of the MNIST dataset. We used
the library keras [18] with Tensorﬂow [1] to train the CNN model from 60,000 images of the dataset.
We employ the special modulus variant of the multi-key CKKS scheme to achieve eﬃciency of approx-
imate computation. Each layer of the network has a depth of one homomorphic multiplication (except the
ﬁrst FC layer requiring one more depth for multiplicative masking), so it requires 6 levels for the evaluation
of CNN. We chose the parameter Set-II from Table 1 so as to cope with such levels of computations.
The data owner ﬁrst chooses one among 10,000 test images in MNIST dataset, normalizes it by
dividing by the maximum value 255, and encrypts it into a single ciphertext using the public key, which
takes 1.75 MB of space. Meanwhile, the model provider generates a relatively large number of ciphertexts
for the trained model: four for the multiple channels, eight for the weight matrix of the FC-1 layer, and
one of each for the other weight or bias. Therefore, the total size of the output ciphertexts is 18.5 MB
and it takes roughly 7 times longer to encrypt the trained model than an image, but it is an one-time
process before data outsourcing and so it is a negligible overhead. After the evaluation, the cloud server
outputs a single multi-key ciphertext encrypting the prediction result with respect to the extended secret
key of the data and model owners. Table 4 shows the timing result for the evaluation of CNN. It takes
about 1.8 seconds to classify an encrypted image from the encrypted training model.
Our parameter guarantees at least 32-bit precision after the decimal point. That is, the inﬁnity norm
distance between encrypted evaluation and plain computation is bounded by 2−32. Therefore, we had
enough space to use the noise ﬂooding technique for decryption. In terms of the accuracy, it achieves
about 98.4% on the test set which is the same as the one obtained from the evaluation in the clear.
Data owner
Model provider
Cloud
server
Stage
Image encryption
Model encryption
Convolutional layer
1st square layer
FC-1 layer
2nd square layer
FC-2 layer
Total evaluation
Runtime
31 ms
236 ms
705 ms
143 ms
739 ms
75 ms
135 ms
1, 797 ms
Table 4. Performance breakdown for evaluating an encrypted neural network on encrypted MNIST data, where
the two encryptions are under diﬀerent secret keys.
6.3.3 Comparison with Previous Works
In Table 5, we compare our benchmark result with the state-of-the-art frameworks for oblivious neural
network inference: CryptoNets [29], MiniONN [41], Gazelle [35], and E2DM [34]. The ﬁrst column in-
dicates the framework and the second column denotes the cryptographic primitives used for preserving
privacy. The last columns give running time for image classiﬁcation as well as amortized time per instance
if applicable.
Among the aforementioned solutions for private neural network prediction, E2DM relies on a third-
party authority holding a secret key of HE, since the data and model are under the same secret key.
CryptoNets has good amortized complexity, but it has a high latency for a single prediction. MiniONN
and Gazelle have good latency, but they require both parties to be online during the protocol execution,
and at least one party performs local work proportional to the complexity of the function being evaluated.
Also, the number of rounds for MiniONN and Gazelle scales with the number of layers in the neural
network. On the other hand, our solution has a constant number of rounds.
Moreover, our solution allows the parties to outsource homomorphic evaluation to an untrusted
server (e.g. a VM in the cloud with large computing power), so both parties only need to pay encryp-