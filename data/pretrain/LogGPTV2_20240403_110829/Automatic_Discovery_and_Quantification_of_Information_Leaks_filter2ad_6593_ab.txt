such as computing models, relational composition, and set
diﬀerence, and we use it to implement the enumeration of
the equivalence classes. For our auction example, we obtain
the following equivalence classes:
B1 ≡ h2 ≤ h1 ∧ h3 ≤ h1 ,
B2 ≡ h1 < h3 ∧ h2 < h3 ,
B3 ≡ (h1 < h3 ∧ h3 ≤ h2) ∨ (h3 ≤ h1 ∧ h1 < h2) ,
where the clauses of B3 are exclusive.
In the third step, we use LE (Lattice point Enumer-
ation) [25], a tool for computing the number of integer
solutions of linear arithmetic constraints, for counting the
size of each of the equivalence classes. For this, we have to
add additional constraints to bound the size of the input
domain. For our auction program, we choose as inputs
positive integers of 32 bits, i.e. 0 ≤ h1, h2, h3 ≤ 232 − 1.
Then LE determines the following sizes:
|B1| = 26409387513978151235418587136 ,
|B2| = 26409387495531407161709035520 ,
|B3| = 26409387504754779196416327680 .
The result shows that
the three equivalence classes
B1, B2, B3 are of almost equal size; the diﬀerence is due
to the asymmetry with which our program determines the
winner of an auction with two or more equal bids. If
the input values are independently and uniformly chosen
(modeled by a random variable U), the attacker’s initial
uncertainty about the auction is H(U) = 3 · 32 = 96 bits.
Observing the output of the program reduces this uncertainty
to
H(U|VR) =
1
296
|Bi| log|Bi| ≈ 94.42 ,
3(cid:88)
i=1
which corresponds to an information leakage (a reduction in
uncertainty) of 96 − 94.2 = 1.58 bits.
Existing quantitative approaches will return (an approxi-
mation of) this number as a result. Our approach additionally
oﬀers quantities that go beyond the number of bits that are
leaked. We can, for example, answer questions about the
143
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:53 UTC from IEEE Xplore.  Restrictions apply. 
average number of guesses required to correctly determine
the secret inputs after observing the output (1.3204 · 1028),
the average number of guesses required to determine the
weakest secrets (1.3204 · 1028, as the equivalence classes
in our examples are almost of equal size), or simply the
number of possible bid combinations that lead to a given
output (|B1|,|B2|, and |B3|). As we will show, all of these
measures can be easily derived from the sizes |B1|,|B2|,|B3|
of the R-equivalence classes computed by our approach.
3. Preliminaries
In this section, we give the necessary deﬁnitions for
dealing with programs and information-ﬂow analysis.
3.1. Programs and computation
We model a program P as a transition system (S ,T , I, F)
that consists of
• S : a set of program states,
• T : a ﬁnite set of program transitions such that each
transition τ ∈ T is associated with a binary transition
relation ρτ ⊆ S × S ,
• I : a set of initial states, I ⊆ S ,
• F : a set of ﬁnal states, F ⊆ S .
Our exposition does not assume any further state structure;
however, for the sake of concreteness we point out that
usually a program state represents a valuation of program
variables in scope, and a program transition corresponds to
a program statement as written in a programming language.
A program computation σ is a sequence of program states
s1, . . . , sn that starts at an initial state, ends at a ﬁnal state,
and relates each pair of consecutive states by some program
transition. Formally, we require that s1 ∈ I, sn ∈ F, and
for each 1 ≤ i < n there exists a transition τ ∈ T such
that (si, si+1) ∈ ρτ.
A program path π is a (non-empty) sequence of program
transitions, i.e., π ∈ T +. We write π · τ to denote a path
obtained by extending π using a transition τ. Given two
transition relations ρ1 and ρ2, we deﬁne their relational
composition ρ1 ◦ ρ2 as usual:
ρ1 ◦ ρ2 ≡ {(s, s(cid:48)) | ∃s(cid:48)(cid:48) ∈ S : (s, s(cid:48)(cid:48)) ∈ ρ1 ∧ (s(cid:48)(cid:48)
, s(cid:48)) ∈ ρ2} .
Given a path π = τ1 · . . . · τn, a path relation ρπ consists of
the relational composition of transition relations along the
path, i.e.,
ρπ ≡ ρτ1 ◦ . . . ◦ ρτn .
A program path π is feasible if the corresponding path
relation is not empty, i.e., ρπ (cid:44) ∅.
We assume that initial and ﬁnal states are pairs consist-
ing of low and high components, i.e., I = Ihi × Ilo and
F = Fhi × Flo. We assume an observer that knows the
program, i.e., the corresponding transition system, and can
see the low components of the initial and ﬁnal states of
a given computation. That is, the observer cannot see the
high components of the initial and ﬁnal states, and it cannot
see any intermediate states of the computation. The later
condition explains why we assume the high/low structure
only on the initial and ﬁnal states.
3.2. Qualitative information ﬂow
We use an equivalence relation R over Ihi, i.e., R ⊆ Ihi×Ihi,
to characterize the information that is leaked to an observer.
R represents the observer knowledge in terms of equivalence
classes. After observing a program computation the observer
only knows that
the high component of the input state
belongs to the set [shi]R. If R is the identity relation, i.e.,
=hi ≡ {(shi, shi) | shi ∈ Ihi} ,
that does not distinguish between any states,
then the observer knows the value shi, since the equiva-
lence class [shi]=hi
is a singleton set and hence uniquely
determines shi. In contrast, the largest equivalence relation
Allhi
i.e.,
Allhi = Ihi × Ihi, captures that the observer knows nothing
about Ihi, since we have [shi]Allhi
= Ihi. An equivalence
relation R such that =hi ⊂ R ⊂ Allhi represents a partial
knowledge about the high component of the input.
The information that a program leaks partially depends on
the low component of the initial states. We call such a low
component of an initial state an experiment, and assume that
it can be chosen by the attacker. Our goal is to characterize
the secret information that a program leaks when it is run
on a given set of experiments E ⊆ Ilo. Given a program
P and a set of experiments E, there is an information leak
with respect to an equivalence relation R if there is a pair
of computations induced by paths π and η that start from
initial states with R-equivalent high components and equal
low components in E, and lead to ﬁnal states with diﬀerent
low components:
LeakP(R, E, π, η) ≡
∃s, t ∈ I ∃s(cid:48), t(cid:48) ∈ F : (s, s(cid:48)) ∈ ρπ ∧ (t, t(cid:48)) ∈ ρη ∧
(cid:44) t(cid:48)
lo .
slo = tlo ∧ (shi, thi) ∈ R ∧ slo ∈ E ∧ s(cid:48)
lo
The relation R over-approximates the maximal information
that is leaked when the program P is run on the experiments
E, written as ConﬁneP(R, E), if there is no witness of further
leaks:
ConﬁneP(R, E) ≡
∀π, η ∈ T + : ¬LeakP(R, E, π, η) .
The largest equivalence relation R with ConﬁneP(R, E) is
the most precise characterization of the leaked information
information, denoted by ≈E.
≈E ≡ (cid:91)(cid:8)R | ConﬁneP(R, E)(cid:9) .
144
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:53 UTC from IEEE Xplore.  Restrictions apply. 
Example 1. If a program P satisﬁes ConﬁneP(Allhi, Ilo), a
low observer does not learn any information about the high
inputs even if he runs the program on all low inputs. This
property is called non-interference.
The set of experiments E characterizes the set of low
input values for which the information leakage of a program
is characterized. Diﬀerent instantiations of E correspond to
diﬀerent attack scenarios. In general, ≈E ⊆≈E(cid:48) whenever
E(cid:48) ⊆ E, i.e., more experiments allow for a more precise
characterization of the secret and hence leak more informa-
tion.
Example 2. Consider a password checker P that receives
as high input a password and as low input a password
candidate. The relation ≈{x} captures the information that
P leaks when it is run on the password candidate x. The
relation ≈{Ilo} captures the information that P leaks in an
exhaustive password search.
ConﬁneP(R, E) does not capture information leaks ex-
posed due to non-termination of P. Suﬃcient preconditions
for the termination of P can be automatically generated [14]
and used to exclude non-terminating inputs. From now on,
we will assume that P terminates on all initial states s ∈ I.
3.3. Quantitative information ﬂow
In the following, we use information theory to give
quantitative characterizations of equivalence relations R with
ConﬁneP(R, E). These characterizations have the advantage
of being compact and human-readable. Moreover, they yield
concise interpretations, e.g., in terms of the expected guess-
ing eﬀort that is required to determine the secret given the
available information.
we deﬁne pX : X → R as pX(x) = (cid:80)
We ﬁrst illustrate in detail how R can be characterized
using the guessing entropy as a measure. After this, we give
examples of alternative information measures.
Guessing entropy. Let A be a ﬁnite set and p: A → R a
probability distribution. For a random variable X: A → X,
a∈X−1(x) p(a), which is
often denoted by p(X = x) in the literature.
The guessing entropy of the random variable X is the
average number of questions of the kind “does X = x
hold” that must be asked to guess X’s value correctly [28].
If we assume p to be public, the optimal procedure is to
try each of the possible values in order of their decreasing
probabilities. Without loss of generality, let X be indexed
such that pX(xi) ≥ pX(x j), whenever i ≤ j. Then the guessing
(cid:88)
entropy G(X) of X is deﬁned as
G(X) =
i pX(xi) .
1≤i≤|X|
Given another random variable Y : A → Y, one denotes
by G(X|Y = y) the guessing entropy of X given Y = y, that
is, with respect to the distribution pX|Y=y. The conditional
guessing entropy G(X|Y) is deﬁned as the expected value
of G(X|Y = y) over all y ∈ Y, namely,
G(X|Y) =
pY(y)G(X|Y = y) .
(cid:88)
y∈Y
This quantity represents the expected number of guesses
needed to determine X when the value of Y is already
known.
Guessing entropy and programs. We assume a given
probability distribution p: Ihi → R and an equivalence
relation R ⊆ Ihi × Ihi. We use two random variables to
quantify the information that corresponds to R. The ﬁrst is
the random variable U that models the choice of a secret in
Ihi according to p (i.e., U = idIhi). The second is the random
variable VR that maps each secret to its R-equivalence class:
VR : Ihi → Ihi/R, where VR(shi) = [shi]R.
Consider now a program P that satisﬁes ConﬁneP(R, E).
Then G(U) is the expected number of guesses that an
attacker must perform to determine the secret input, prior to
observing the output of P. The value of G(U|VR = [shi]R)
is the adversary’s remaining guessing eﬀort after learning
the R-equivalence class of shi. Hence, G(U|VR) is a lower
bound on the expected number of guesses that an attacker
must perform for recovering the secret input after having
run P on all experiments from E.
Alternative information measures. A number of alterna-
tive information measures, e.g., see [7], [24], [37], can be
connected to programs along the same lines as the guessing
entropy.
ˆG(U|VR) is deﬁned
as the expected guessing eﬀort for the weakest secrets in Ihi,
i.e. the secrets that are easiest to guess after observing the
output of P. Formally, one deﬁnes
The minimal guessing entropy [24]
ˆG(U|VR) = min{G(U|VR = [shi]R) | shi ∈ Ihi} .
The conditional Shannon entropy H(U|VR) captures the
attacker’s uncertainty about the secret input of the program
in terms of bits, i.e., in terms of a lower bound on the shortest
representation of the secret [2], [36]. Formally, one deﬁnes
H(U|VR) as the expected value of H(U|VR = [shi]R) over
all shi ∈ Ihi, where H(U|VR = [shi]R) is the Shannon entropy
of U with respect to the distribution pU|VR=[shi].
The channel capacity
CR = max
(H(U) − H(U|VR))
p
is an upper bound on the rate at which information can
be transmitted through the program by variation of its
secret inputs [2], [36]. Here, p ranges over all probability
distributions on Ihi.
The conditional min-entropy H∞(U|VR) captures the un-
certainty about the secret input in terms of the probability
145
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:53 UTC from IEEE Xplore.  Restrictions apply. 
for guessing the secret in one try after observing the output
of the program. There are diﬀerent deﬁnitions for the con-
ditional min-entropy in the literature; the one given in [37]
is easily cast in terms of U and VR.
Which measure is appropriate depends on the given attack
scenario. For example, the channel capacity is appropriate
for assessing the damage of intentional communication, e.g.,
by a Trojan horse, while the minimal guessing entropy can
be used to assess the impact of unintentional information
release.
4. Leak discovery and quantiﬁcation
In this section, we present our method, called DQ
(Dcovery and Qiﬁcation), for the automatic discovery
of leaked information and its comprehensive quantitative
interpretation. DQ takes as input a program P and
a set of experiments E. It produces the characterization of
the leaking information in terms of the equivalence relation
≈E and performs its information-theoretic, quantitative in-
terpretation.
DQ consists of two procedures D and Q
that perform the qualitative and quantitative analysis, re-
spectively. We proceed with a detailed description of each