a power-law distribution [18, 22, 27]. We therefore assume
that around 60% of the nodes are available less than 20% of
the time, and there are only very few highly available nodes.
Note that this power-law model incorporates the high churn
rates typical for an OSN. We further apply diurnal patterns
to populate the online time matrix of each node. Accord-
ing to [28], we consider three time zones (US, Europe and
Africa, and Asia and Oceania), where a node’s probability
of belonging to these zones is 0.4, 0.3, and 0.3, respectively.
To bootstrap, nodes join our experiments asynchronously
according to their online probability.
User activity pattern. Diﬀerent evolutions of OSN user
activity have been observed [18, 22]. We model user activity
to be exponentially decreasing [22]. After an initial phase
of high interaction once joining an OSN, a user’s activity
decreases exponentially to become less than one interaction
per day. As nodes in SOUP must gain knowledge about
other participants (i.e., get in contact with them) in order
to ﬁnd the best-suited mirrors, in all literature we are aware
of, this model represents the worst observed case.
Available storage space per node. Each node must
have a speciﬁc storage space value in order to evaluate the
storage overhead and dropping strategy of SOUP. The stor-
age space available at each node follows a Gaussian distribu-
tion, with a median of space for mirroring data of 50 users,
which requires no more than half a gigabyte of disk space as
shown in Sec. 7.
5.2 Results and Analysis
5.2.1 Data Availability and Replica Overhead
Fig. 5 shows SOUP’s data availability and replica over-
head for each dataset.
In all the three datasets, SOUP
achieves the targeted availability of above 99% after only one
day, even though no node has any knowledge upon joining.
As SOUP reaches equilibrium, the high level of availability
is maintained for the entire remaining period.
After joining, due to the lack of knowledge about good
mirrors, nodes do not select well-suited mirrors yet, and the
number of replicas increases. However, as soon as nodes ob-
tain more precise rankings, the quality of mirrors improves
Figure 7: SOUP is robust and does not discriminate any node
based on social relations or online time.
Stability and Communication Overhead
and the replica overhead is reduced by about 50%. On av-
erage each node needs to store less than seven replicas.
5.2.2
SOUP needs to reach a stable state quickly in order to
keep communication overhead low. In particular, if a user
frequently changes her set of mirrors, all her data has to be
transmitted to new mirrors often. We show SOUP’s proﬁle
distribution in Fig. 6 (we report results from the Facebook
dataset for the rest of this section; we observed the same
behaviour of SOUP with both other datasets). After day
one, around half the nodes need to store 10 or more replicas
so that the system achieves high availability. However, as
user experiences are more accurately measured, 90% of the
nodes need to store less than 7 replicas (after two weeks). We
observe the same distribution at the end of our simulation,
indicating that SOUP has reached a stable state.
Further, we ﬁnd that as mirror rankings become more
accurate, the drop rate of data converges from 0.07% to a
very low 0.045%. Finally, the upper half of our nodes with
regards to online time provides more than 90% of all replicas.
This indicates that weak nodes, in particular mobile nodes,
are rarely chosen as mirrors, saving storage, bandwidth and
battery on these devices.
5.2.3 Robustness
Regardless of their own online probability or quantity of
friends, every user should achieve a very high level of data
availability. We pick the top and bottom 10% of users (ﬁrst
with regards to their own online probability and second with
regards to their number of friends) and compare their per-
formance in Fig. 7. After just one day, even the bottom
10% of users obtain data availability of above 99%. Hence,
in contrast to related works (e.g.,
[9–13]), users are discrim-
inated neither based on their own online time nor based on
the quantity and quality of their social relations. Instead,
SOUP oﬀers a robust OSN.
5.2.4 Openness
One of SOUP’s challenges is to exploit altruistically pro-
vided resources eﬃciently. Fig. 8 shows the impact of the
presence of small percentages of altruistic nodes that are
steadily online. We can observe that 5% (a=0.05) altru-
istic nodes can cause a slight increase and stabilization of
availability, but the improvement in terms of replica over-
head is more prominent; as altruistic nodes become known
to the OSN, nodes can select fewer mirrors than before to
achieve the same level of availability. Hence, while SOUP
does not rely on any kind of altruistic nodes, it can exploit
such nodes’ resources if available.
5.2.5 Resiliency Against Node Dynamics
In addition to high churn rates, we now consider the case
in which a fraction of the users abruptly becomes unavailable
00.20.40.60.81Time (Days)Data Availability5791113Avg. # of Replicas/Node01020Availability (FB)Mirrors (FB)Availability (SD)Mirrors (SD)Availability (EPIN)Mirrors (EPIN)02040608010012014000.20.40.60.81Number of Profiles StoredCDF1 DayTwo WeeksOne Month36912180.50.60.70.80.91Time (Days)Data AvailabilityTop 10%AverageBottom 10%High Availability at Day 1Figure 8: SOUP can exploit altruistic resources.
Figure 10: SOUP is resilient against a slander attack.
Figure 9: SOUP is resilient against node dynamics.
Figure 11: SOUP can recover from a ﬂooding attack.
to the rest of the OSN (Fig. 9). If we assume the top 5% of
nodes in terms of online time leave the OSN simultaneously
(d=0.05), there is a noticeable drop in both data availability
and replica overhead directly after the departure, caused by
the concomitant loss of mirrors within the OSN. However,
the remaining nodes adapt quickly by choosing new mirrors,
and SOUP’s performance improves without introducing any
additional replica overhead. Interestingly, SOUP is indepen-
dent from the top 1-2% of nodes, as data availability does
not signiﬁcantly drop as these nodes leave the OSN. Still,
a speciﬁc proﬁle might be unavailable, either when an ad-
versary attacks its mirrors or when mirrors of popular data
deny service due to overloading. In such a case, these mir-
rors will receive a lower ranking, and SOUP will distribute
the load among additional mirrors. If a mirror is completely
taken down, SOUP will choose a diﬀerent one, as shown
above. Compared to the static mirror choices of related
work, SOUP is the only approach capable of such adapta-
tion towards both increasing and decreasing resources.
5.2.6 Resiliency Against Malicious Nodes
None of the existing DOSN solutions consider attacks on
their system. We measure SOUP’s performance under at-
tack of up to half the nodes in the OSN. In our experiment,
SOUP not only needs to tolerate the attackers after having
stabilized, but also has to bootstrap in their presence.
First, we study the impact of the slander attack, in which
attackers manipulate experience sets (or recommendations
to bootstrapping users). We assume that the malicious
users have inﬁltrated the OSN successfully and they send
out recommendations at the maximum rate. Fig. 10 shows
that even when 50% of social relations—and thereby ex-
perience sets—are subject to slander, the data availability
at most drops to around 95% (m = 0.5). Second, we in-
vestigate a ﬂooding attack,
in which an attacker creates
multiple identities (Sybils) and ﬂoods benign mirrors with
data. We show results for diﬀerent percentages of Sybils in
Fig. 11. Even with as many Sybils as regular identities in
the OSN, the data availability does not drop below 90% for
the benign users in the long run. The replica overhead, al-
though increased, does not exceed 13 copies per node. In
this case, protective dropping prevents data of socially con-
nected nodes from being dropped for a Sybil’s data, and
avoids the full utilization of resources at benign nodes.
Table 4: SOUP vs. related work (p = online probability).
SOUP vs Related Work
5.2.7
SOUP’s superiority over state-of-the-art solutions mainly
stems from its qualitative properties, which we extensively
evaluated above. Compared against those, SOUP is robust,
adaptive to node dynamics, and resilient against attacks.
To further compare the performance of SOUP and related
work quantitatively, we run simulations of SOUP under the
node online time distributions assumed in related works, in
those cases where the distributions were available to us.
As shown in Table 4, SOUP outperforms both PeerSoN
and Safebook, providing higher data availability and lower
replica overhead. In particular, when compared with Safe-
book, SOUP achieved 8.5% higher availability while keeping
the replica overhead near the lower bound of Safebook. In
this scenario, SOUP performs slightly worse than in our orig-
inal experiments. This is caused by the uniform online time
distribution, due to which SOUP cannot exploit the het-
erogeneity of node characteristics to select well-suited mir-
rors. In the PeerSoN scenario, the online times of nodes are
drastically improved over our power-law assumption. Still,
PeerSoN is not able to create a robust OSN and the data
availability ranges between less than 90% and close to 100%,
as nodes depend on their own online times. SOUP however
provides close to 100% data availability for all nodes and
further reduces the replica overhead by one third.
6.
IMPLEMENTATION
Our implementation of a SOUP node comprises two com-
ponents: the SOUP middleware and the SOUP applications.
Taken together, both constitute a SOUP node as depicted in
0.60.81Time (Days)Data Availability01020305678910Avg # of Replicas/NodeAvailability (a=0.01)Mirrors (a=0.01)Availability (a=0.02)Mirrors (a=0.02)Availability (a=0.05)Mirrors (a=0.05)Altruistic nodes join0.60.81Time (days)Data Availability0102030579111315Avg. # of Replicas/NodeAvailability(d=0.01)Mirrors (d=0.01)Availability (d=0.02)Mirrors (d=0.02)Availability(d=0.05)Mirrors (d=0.05)Node Departure0.60.81Data AvailabilityTime (days)01020306810121416Avg. # of Replicas/NodeAvailability (m=0.5)Mirrors (m=0.5)Availability (m=0.2)Mirrors (m=0.2)Availability (m=0.1)Mirrors (m=0.1)0.60.81Data AvailabilityTime (days)010208101214161820Avg. # of Replicas/NodeAvailability (m=0.5)Mirrors (m=0.5)Availability (m=0.2)Mirrors (m=0.2)Availability (m=0.1)Mirrors (m=0.1)SOUPPower-law~99.5%~6.5ApproachOnline time assumptionAvailability# of replicasSOUP~98.5%~14PeerSoN10% of nodes p=0.925% of nodes p=0.8730% of nodes p=0.7530% of nodes p=0.3<90%-100%;Depends on p6SOUP~100%4SafebookUniform, all nodes p=0.3~90%13-24Figure 12: Architecture of a SOUP Node
Fig. 12. Note however that it is also possible to exclusively
run the middleware component, for instance to provide an
altruistic node, which only acts as a mirror or DHT relay
for mobile nodes.
For SOUP applications, we have implemented a SOUP
Demo Client that can run on either a PC or Android device
to organize SOUP nodes into a social network (see Fig. 13).
It supports essential OSN functionalities: Users can search
for each other in the OSN, establish friendships, share pho-
tos or exchange messages. We also implemented a broker
application that can suggest friends to a SOUP node when
it is fed with data through the Facebook API.
For the SOUP middleware, we have implemented it also
for both desktop and Android systems. It consists of several
modules, each responsible for a pre-deﬁned task and eas-
ily exchanged for an improved or diﬀerent approach at any
time. All modules communicate by passing SOUP objects to
each other. One particular module is the Application Man-
ager that has a simple interface with SOUP applications. It
has two functionalities: (i) it allows arbitrary social appli-
cations to run on top of the SOUP middleware; and (ii) it
enables communication between applications transparent to
the middleware itself. On one hand, it encapsulates content
from a SOUP application into SOUP objects. On the other
hand, it decapsulates content destined for an application
from SOUP objects received from other modules. Further-
more, the Social Manager module is responsible for pro-
cessing requests when an object indicates a change to the
social data, and the Security Manager module deals with
all encryption-related tasks using our own optimized ABE
implementation.1 The Mirror Manager module is responsi-
ble for the selection of mirrors. A node needs to push any
change of its data to its mirrors, and it also needs to man-
age the data that it mirrors for others. Last, if any of these
modules need to communicate with other nodes, they do so
by passing an object to the Interface Manager, which can
then initiate communication via a suitable network interface.
Consider a friend request as an example: After an applica-
tion initiates the request, the Application Manager converts
the request to an appropriate SOUP object, and passes it
to the Social Manager. The Social Manager manipulates
the user’s friend list and forwards the object to the Security
Manager. The Security Manager encrypts and signs the ob-
ject and hands it to the Interface Manager, which sends the
object to the request target over an appropriate link. If the
Interface Manager later receives an encrypted request conﬁr-
mation object from the target, it forwards it to the Security
1Based on https://github.com/wakemecn/cpabe
Figure 13: SOUP on a Nexus 4 Android Phone
Manager, which unlocks the object and issues a conﬁrmation
to the application via the Application Manager.
For the underlying DHT, we use FreePastry 2.12, an open
source implementation of the Pastry DHT [29]. Most of our
code and executables are available online.3
7. DEPLOYMENT
We have deployed SOUP on our own real-world DOSN of
31 users. Four of those were using diﬀerent Android mobile
phones. All phones were relaying via the same gateway node,
and that node also acted as the bootstrapping node for users
running the regular SOUP client. We collected several days
of data, during which our users established 282 friendships,
shared 204 photos, and exchanged 1189 messages.
Our deployment outperformed our simulation results with
regards to availability (we did not observe a single loss) and
replica overhead. However, note that our large-scale sim-
ulations should be more accurate, since we observed much
longer online times in our experiment than typical for OSNs.
The lessons learned from the deployment are the following:
The bandwidth consumption of SOUP is not a con-
cern. We show the bandwidth consumption of the DHT at
our bootstrapping node in Fig. 14a. Only upon join and
leave operations (i.e., shifting some entries in the DHT) we
observe utilization of the network interface at around 20-40
KB/s. At the same time, lookups do not have a visual im-
pact. Thus, the cost of relaying for a mobile node is conﬁned
to its join procedure, which requires several DHT operations.
The traﬃc introduced by SOUP itself is also manageable.
Fig. 14b shows the most bandwidth intense period of 20 min-