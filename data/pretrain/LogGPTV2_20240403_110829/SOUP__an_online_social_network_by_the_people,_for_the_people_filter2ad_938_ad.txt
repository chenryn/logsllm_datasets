### Power-Law Distribution and Node Availability
The node availability in our system follows a power-law distribution, as observed in [18, 22, 27]. We assume that approximately 60% of the nodes are available less than 20% of the time, with only a few nodes being highly available. This power-law model accounts for the high churn rates typical in Online Social Networks (OSNs). Additionally, we incorporate diurnal patterns to populate the online time matrix for each node. According to [28], we consider three time zones: US, Europe and Africa, and Asia and Oceania, with probabilities of 0.4, 0.3, and 0.3, respectively. Nodes join our experiments asynchronously based on their online probability.

### User Activity Pattern
Different evolutions of user activity in OSNs have been observed [18, 22]. We model user activity as exponentially decreasing [22]. After an initial phase of high interaction when joining an OSN, a user's activity decreases exponentially to less than one interaction per day. In SOUP, nodes must gain knowledge about other participants to find the best-suited mirrors. This model represents the worst-case scenario, as noted in the literature.

### Available Storage Space per Node
Each node in SOUP must have a specific storage space value to evaluate the storage overhead and dropping strategy. The storage space available at each node follows a Gaussian distribution, with a median capacity for mirroring data from 50 users, requiring no more than half a gigabyte of disk space, as detailed in Section 7.

### 5.2 Results and Analysis

#### 5.2.1 Data Availability and Replica Overhead
Figure 5 illustrates SOUP’s data availability and replica overhead for each dataset. Across all three datasets, SOUP achieves the targeted availability of over 99% within one day, even though nodes initially lack any knowledge. As SOUP reaches equilibrium, this high level of availability is maintained throughout the remaining period.

Upon joining, nodes lack knowledge about good mirrors, leading to an increase in the number of replicas. However, as nodes obtain more precise rankings, the quality of mirrors improves, and the replica overhead is reduced by approximately 50%. On average, each node needs to store fewer than seven replicas.

#### 5.2.2 Stability and Communication Overhead
SOUP needs to reach a stable state quickly to minimize communication overhead. If a user frequently changes her set of mirrors, her data must be frequently transmitted to new mirrors. Figure 6 shows SOUP’s profile distribution using the Facebook dataset. After the first day, around half the nodes need to store 10 or more replicas to achieve high availability. However, as user experiences are more accurately measured, 90% of the nodes need to store fewer than seven replicas after two weeks. This distribution remains consistent at the end of the simulation, indicating that SOUP has reached a stable state.

As mirror rankings become more accurate, the data drop rate converges from 0.07% to a very low 0.045%. The upper half of nodes in terms of online time provide more than 90% of all replicas, indicating that weak nodes, particularly mobile nodes, are rarely chosen as mirrors, thus saving storage, bandwidth, and battery on these devices.

#### 5.2.3 Robustness
Every user should achieve a very high level of data availability, regardless of their own online probability or the quantity of friends. We compare the performance of the top and bottom 10% of users (based on their online probability and number of friends) in Figure 7. Even the bottom 10% of users achieve data availability above 99% within one day. Unlike related works [9–13], SOUP does not discriminate based on online time or social relations, offering a robust OSN.

#### 5.2.4 Openness
One challenge for SOUP is to efficiently exploit altruistically provided resources. Figure 8 shows the impact of small percentages of steadily online altruistic nodes. We observe that 5% (a=0.05) of such nodes can slightly increase and stabilize availability, with a more prominent improvement in replica overhead. As altruistic nodes become known, nodes can select fewer mirrors to achieve the same level of availability. While SOUP does not rely on altruistic nodes, it can effectively use them if available.

#### 5.2.5 Resiliency Against Node Dynamics
In addition to high churn rates, we consider the case where a fraction of users abruptly becomes unavailable (Figure 9). If the top 5% of nodes in terms of online time leave simultaneously (d=0.05), there is a noticeable drop in both data availability and replica overhead. However, the remaining nodes adapt quickly by choosing new mirrors, and SOUP’s performance improves without additional replica overhead. Interestingly, SOUP is independent of the top 1-2% of nodes, as data availability does not significantly drop when they leave.

If a specific profile becomes unavailable due to an attack on its mirrors or overloading, SOUP will distribute the load among additional mirrors. If a mirror is completely taken down, SOUP will choose a different one. Compared to static mirror choices in related work, SOUP is the only approach capable of such adaptation to both increasing and decreasing resources.

#### 5.2.6 Resiliency Against Malicious Nodes
None of the existing Distributed Online Social Network (DOSN) solutions consider attacks on their systems. We measure SOUP’s performance under attacks involving up to half the nodes. SOUP not only tolerates attackers after stabilization but also bootstraps in their presence.

First, we study the impact of a slander attack, where attackers manipulate experience sets or recommendations to bootstrapping users. Figure 10 shows that even when 50% of social relations (and thereby experience sets) are subject to slander, data availability drops to around 95% (m = 0.5).

Second, we investigate a flooding attack, where an attacker creates multiple identities (Sybils) and floods benign mirrors with data. Figure 11 shows results for different percentages of Sybils. Even with as many Sybils as regular identities, data availability for benign users does not drop below 90% in the long run. The replica overhead, although increased, does not exceed 13 copies per node. Protective dropping prevents data of socially connected nodes from being dropped for a Sybil’s data, avoiding full resource utilization at benign nodes.

#### 5.2.7 Comparison with Related Work
SOUP’s superiority over state-of-the-art solutions stems from its qualitative properties, which we extensively evaluated. Compared to related work, SOUP is robust, adaptive to node dynamics, and resilient against attacks.

To further compare the performance quantitatively, we run simulations of SOUP under the node online time distributions assumed in related works. Table 4 shows that SOUP outperforms both PeerSoN and Safebook, providing higher data availability and lower replica overhead. Specifically, compared to Safebook, SOUP achieved 8.5% higher availability while keeping the replica overhead near Safebook’s lower bound. In the PeerSoN scenario, despite improved online times, PeerSoN cannot create a robust OSN, with data availability ranging between less than 90% and close to 100%. SOUP, however, provides close to 100% data availability for all nodes and reduces the replica overhead by one-third.

### 6. Implementation
Our implementation of a SOUP node consists of two components: the SOUP middleware and the SOUP applications. Together, they form a SOUP node, as depicted in Figure 12. It is also possible to run only the middleware component, for example, to provide an altruistic node that acts as a mirror or DHT relay for mobile nodes.

For SOUP applications, we have implemented a SOUP Demo Client that can run on either a PC or Android device to organize SOUP nodes into a social network (Figure 13). It supports essential OSN functionalities: users can search for each other, establish friendships, share photos, and exchange messages. We also implemented a broker application that suggests friends to a SOUP node when fed with data through the Facebook API.

The SOUP middleware is implemented for both desktop and Android systems. It consists of several modules, each responsible for a predefined task and easily exchanged for an improved or different approach. All modules communicate by passing SOUP objects. The Application Manager module has a simple interface with SOUP applications, allowing arbitrary social applications to run on top of the SOUP middleware and enabling transparent communication between applications. It encapsulates content from a SOUP application into SOUP objects and decapsulates content destined for an application from SOUP objects received from other modules.

The Social Manager module processes requests when an object indicates a change to the social data, and the Security Manager module handles all encryption-related tasks using our optimized Attribute-Based Encryption (ABE) implementation.1 The Mirror Manager module is responsible for selecting mirrors. A node needs to push any change of its data to its mirrors and manage the data it mirrors for others. If any module needs to communicate with other nodes, it passes an object to the Interface Manager, which initiates communication via a suitable network interface.

Consider a friend request: after an application initiates the request, the Application Manager converts the request to an appropriate SOUP object and passes it to the Social Manager. The Social Manager manipulates the user’s friend list and forwards the object to the Security Manager. The Security Manager encrypts and signs the object and hands it to the Interface Manager, which sends the object to the target. If the Interface Manager later receives an encrypted request confirmation object from the target, it forwards it to the Security Manager, which unlocks the object and issues a confirmation to the application via the Application Manager.

For the underlying DHT, we use FreePastry 2.12, an open-source implementation of the Pastry DHT [29]. Most of our code and executables are available online.3

### 7. Deployment
We deployed SOUP on a real-world DOSN of 31 users, four of whom used different Android mobile phones. All phones were relaying via the same gateway node, which also acted as the bootstrapping node for users running the regular SOUP client. Over several days, our users established 282 friendships, shared 204 photos, and exchanged 1189 messages.

Our deployment outperformed our simulation results in terms of availability (no data loss observed) and replica overhead. However, note that large-scale simulations should be more accurate, as we observed longer online times in our experiment than typical for OSNs.

**Lessons learned from the deployment:**
- **Bandwidth Consumption:** SOUP’s bandwidth consumption is not a concern. Figure 14a shows the bandwidth consumption of the DHT at our bootstrapping node. Only during join and leave operations (i.e., shifting DHT entries) do we observe network interface utilization of around 20-40 KB/s. Lookups have minimal impact. The cost of relaying for a mobile node is confined to its join procedure, which requires several DHT operations.
- **Traffic Management:** The traffic introduced by SOUP itself is manageable. Figure 14b shows the most bandwidth-intense period of 20 minutes, demonstrating that SOUP’s traffic is well within acceptable limits.