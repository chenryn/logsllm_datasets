accessing historical records of these blacklists allows us
to determine (roughly) at what time a website became
malicious. Indeed, the ﬁrst time at which a compromised
website appeared in a blacklist gives an upper bound on
the time at which the site became malicious. We can then
grab older archived versions of the site from the Way-
back Machine to obtain an example of a site that was
originally not malicious and then became malicious.
We obtain benign websites by randomly sampling
DNS zone ﬁles, and checking that the sampled sites are
not (and have never been) in any blacklist. We then also
cull archives of these benign sites from the Wayback ma-
chine, so that we can compare at the same time in the
past sites that have become malicious to sites that have
remained benign.
We emphasize that, to evaluate the performance of the
classiﬁer at a particular time t, training examples from
the past (e.g., t−h) may be used; and these examples can
then be used to test on the future. However, the converse
is not true: even if that data is available, we cannot train
on the present t and test on the past (t−h) as we would be
using future information that was unknown at the time of
the test. Figure 1(b) illustrates that data available to build
predictions is a strict subset of the known data.
Dealing with imbalanced datasets: As far as the learn-
ing process is concerned, one can employ class re-
balancing techniques. At a high level, class re-balancing
has been studied as a means to improve classiﬁer perfor-
mance by training on a distribution other than the natu-
rally sampled distribution. Since we sample only a ran-
dom subset of sites which were not compromised, we
628  23rd USENIX Security Symposium 
USENIX Association
4
already perform some resampling in the form of a one-
sided selection.
3.3 Dynamic extraction of the feature list
Any classiﬁer needs to use a list of features on which to
base its decisions. Many features can be used to charac-
terize a website, ranging from look and feel, to trafﬁc, to
textual contents. Here we discuss in more details these
potential features. We then turn to a description of the
dynamic process we use to update these features.
3.3.1 Candidate feature families
As potential candidates for our feature list, we start by
considering the following families of features.
Trafﬁc statistics. Website statistics on its trafﬁc, popu-
larity, and so forth might be useful in indicating a spe-
ciﬁc website became compromised. For instance, if a
certain website suddenly sees a change in popularity, it
could mean that it became used as part of a redirection
campaign. Such statistics may be readily available from
services such as the aforementioned Alexa Web Informa-
tion Service, if the website popularity is not negligible.
Filesystem structure. The directory hierarchy of the
site, the presence of certain ﬁles may all be interesting
candidate features reﬂecting the type of software run-
ning on the webserver. For instance the presence of a
wp-admin directory might be indicative of a speciﬁc
content management system (WordPress in that case),
which in turn might be exploitable if other features in-
dicate an older, unpatched version is running.
Webpage structure and contents. Webpages on the
website may be a strong indicator of a given type of
content-based management system or webserver soft-
ware. To that effect, we need to distill useful page
structure and content from a given webpage. The user-
generated content within webpages is generally not use-
ful for classiﬁcation, and so it is desirable to ﬁlter it out
and only keep the “template” the website uses. Extract-
ing such a template goes beyond extraction of the Docu-
ment Object Model (DOM) trees, which do not provide
an easy way to differentiate between user-generated con-
tents and template. We discuss in the next section how
extracting this kind of information can be accomplished
in practice.
Page content can then distilled into features using
several techniques. We chose to use binary features
that detect the presence of particular HTML tags in a
site. For instance, “is the keyword joe’s guestbook/v1.2.3
present?” is such a binary feature. Of course, using such
a binary encoding will result in a rather large feature set
as it is less expressive than other encoding choices. How-
ever the resulting features are extremely interpretable
and, as we will see later, are relatively straightforward
to extract automatically.
Perhaps more interestingly, we observed that features
on ﬁlesystem structure can actually be captured by look-
ing at the contents of the webpages.
Indeed, when
we collect information about internal links (e.g., ) we are actually gathering informa-
tion about the ﬁlesystem as well. In other words, features
characterizing the webpage structure provide enough in-
formation for our purposes.
3.3.2 Dynamic updates
We consider trafﬁc statistics as “static” features that we
always try to include in the classiﬁcation process, at least
when they are available. On the other hand, all of the
content-based features are dynamically extracted. We
use a statistical heuristic to sort features which would
have been useful for classifying recent training examples
and apply the top performing features to subsequent ex-
amples.
4
We next turn to a discussion of how we implemented our
classiﬁer in practice. We ﬁrst introduce the data sources
we used for benign and soon-to-be malicious websites.
We then turn to explaining how we conducted the parsing
and ﬁltering of websites. Last we give details of how we
implemented dynamic feature extraction.
4.1 Data sources
We need two different sources of data to train our clas-
siﬁer: a ground truth for soon-to-be malicious websites,
and a set of benign websites.
Implementation
Malicious websites. We used two sets of blacklists as
ground truth for malicious websites. First, we obtained
historical data from PhishTank [28]. This data contains
11,724,276 unique links from 91,155 unique sites, col-
lected between February 23, 2013 and December 31,
2013. The Wayback machine contained usable archives
for 34,922 (38.3%) of these sites within the required
range of dates.
We then complemented this data with a list of web-
sites known to have been infected by “search-redirection
attacks,” originally described in 2011 [20, 22].
In this
attack, miscreants inject code on webservers to have
them participate in link farming and advertise illicit
products—primarily prescription drugs. From a related
measurement project [19], we obtained a list, collected
between October 20, 2011 and September 16, 2013, of
738,479 unique links, all exhibiting redirecting behav-
ior, from 16,173 unique sites. Amazingly, the Wayback
machine contained archives in the acceptable range for
14,425 (89%) of these sites.
USENIX Association  
23rd USENIX Security Symposium  629
5
n
o
i
t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
l
i
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Search−redirection infected
PhishTank Blacklisted
Random, non−compromised
 5
 10
 15
 20
Number of pages in the site
Figure 2: Cumulative distribution function of the
number of pages scraped. Benign websites very fre-
quently contain only a handful of pages.
We use these two blacklists in particular because we
determined, through manual inspection, that a large per-
centage of sites in these lists have either been compro-
mised by an external attacker or are maliciously hosted.
On the other hand, various other blacklists often label
sites that are heavily spammed or contain adult content
as malicious, which we do not think is appropriate.
Benign websites. We randomly sampled the entire
.com zone ﬁle from January 14th, 2014. For each do-
main, we enumerated the available archives in the Way-
back machine.
If at least an archive was found, we
selected one of the available archives in the range of
February, 20, 2010 to September 31, 2013. This yielded
337,191 website archives. We then removed all archives
that corresponded to sites known as malicious. We re-
moved 27 of them that were among the set of sites known
to have been infected by search-redirection attacks, and
another 72 that matched PhishTank entries. We also dis-
carded an additional 421 sites found in the DNS-BH [2],
Google SafeBrowsing [14], and hpHosts [23] blacklists,
eventually using 336,671 websites in our benign corpus.
Structural properties. Figure 2 shows some interest-
ing characteristics of the size of the websites we con-
sider. Speciﬁcally, the cumulative distribution function
of the number of pages each website archive contains dif-
fers considerably between the datasets. For many benign
sites, that were randomly sampled from zone ﬁles, only a
few pages were archived. This is because many domains
host only a parking page or redirect (without being ma-
licious) to another site immediately. Other sites are very
small and host only a few different pages.
On the other hand, malicious sites from both of our
blacklists contain more pages per site, since in many
cases they are reasonably large websites that had some
form of visibility (e.g., in Google rankings), before be-
coming compromised and malicious.
In some other
cases, some of the blacklisted sites are sites maliciously
registered, that do host numerous phishing pages.
4.2 Parsing and ﬁltering websites
We scraped web pages from the Wayback Machine us-
ing the Scrapy framework [4], and a collection of custom
Python scripts.
Selecting which archive to use. The scripts took in a
URL and a range of dates as inputs, and then navigated
The WayBack Machine to determine all the archives that
existed for that URL within the speciﬁed range.
Sites ﬁrst appear in a blacklist at a particular time t. If
a site appears in multiple blacklists or in the same black-
list multiple times, we use the earliest known infection
date. We then search for snapshots archived by the Way-
back machine between t −12 months and to t −3 months
prior to the site being blacklisted. The choice of this
range is to satisfy two concerns about the usefulness of
the archive data. Because compromised sites are not gen-
erally instantaneously detected, if the date of the archive
is chosen too close to the ﬁrst time the site appeared in
a blacklist, is is possible that the archived version was
already compromised. On the other hand, if the archived
version was chosen too far from the time at which the
site was compromised, the site may have changed dra-
matically. For instance, the content management system
powering the site may have been updated or replaced en-
tirely.
If multiple archives exist in the range t − 3 months–
t − 12 months, then we select an archive as close to t −
12 months as possible; this matches our choice for h =
1 year described earlier. We also download and scrape
the most recent available archive, and compare it with the
the one-year old archive to ensure that they are using the
same content management system. In the event that the
structure of the page has changed dramatically (deﬁned
as more than 10% changes) we randomly select a more
recent archive (i.e., between zero and one year old), and
repeat the process.
Scraping process. We scrape each archived site, using
a breadth-ﬁrst search. We terminate the process at ei-
ther a depth of two links, or 20 pages have been saved,
and purposefully only download text (HTML source, and
any script or cascading style sheets (CSS) embedded in
the page, but no external scripts or images). Using a
breadth-ﬁrst search allows us to rapidly sample a large
variety of web pages. It is indeed common for websites
to contain multiple kinds of webpages, for example fo-
rums and posts, blogs and guestbooks, login and con-
tact pages. A breadth-ﬁrst search provides an idea of the
amount of page diversity in a site without requiring us to
scrape the entire site. Limiting ourselves to 20 pages al-
lows us to quickly collect information on a large number
of websites, and in fact allows us to capture the vast ma-
jority of websites in their entirety, according to Figure 2,
630  23rd USENIX Security Symposium 
USENIX Association
6
root
DOM 1
root
DOM 2
root
Style tree
attr: width=800
attr: x.png
attr: color=red
attr: width=800
attr: x.png
attr: color=red
attr: width=800
attr: x.png
attr: color=red
attr: href="x.html"
attr: src="b.png"
attr: src="b.png"
attr: href="x.html"
Figure 3: DOM and style trees. The ﬁgure, adapted from Yi et al. [38], shows two DOM trees corresponding to two
separate pages, and the resulting style tree.
while—as we will see later—providing enough informa-
tion to our classiﬁer to be able to make good decisions.
Filtering. Once a batch of webpages has been saved for
a given website, we ﬁlter each of them to remove user-
generated content. We deﬁne user-generated content as
all data in webpage, visible and invisible, which is not
part of the underline template or content-management
system. This includes for instance blog posts, forum
posts, guestbook entries, and comments. Our assumption
is that user-generated content is orthogonal to the secu-
rity risks that a site a priori faces and is therefore simply
noise to the classiﬁer. User-generated content can, on the
other hand, indicate that a site has already been compro-
mised, for instance if blog posts are riddled with spam
links and keywords. But, since our objective is to detect
vulnerable (as opposed to already compromised) sites,
user-generated content is not useful to our classiﬁcation.
The process of extracting information from webpages
is a well-studied problem in data mining [10, 11, 32, 38,
39]. Generally the problem is framed as attempting to
isolate user-generated content which otherwise would be
diluted by page template content. We are attempting to
do the exact opposite thing: discarding user-generated
content while extracting templates. To that effect, we
“turn on its head” the content-extraction algorithm pro-
posed by Yi et al. [38] to have it only preserve templates
and discard contents.
Yi et al. describes an algorithm where each webpage
in a website is broken down into a Document Object
Model (DOM) tree and joined into a single larger struc-
ture referred to as a style tree. We illustrate this con-
struction in Figure 3. In the ﬁgure, two different pages in
a given website produce two different DOM trees (DOM
1 and DOM 2 in the ﬁgure). DOM trees are essentially
capturing the tags and attributes present in the page, as
well as their relationship; for instance, ¡table¿ elements
are under ¡body¿.
The style tree incorporates not only a summary of the
individual pieces of content within the pages, but also
their structural relationships with each other. Each node
in a style tree represents an HTML tag from one or pos-
I
p
m
p
m
C
<
s
e
d
o
N
f
o
n
o
i
t
c
a
r
F
 1
 0.8
 0.6
 0.4
 0.2