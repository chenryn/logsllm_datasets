### Accessing Historical Records to Determine Malicious Website Timeline

By accessing the historical records of blacklists, we can estimate the time at which a website became malicious. The first appearance of a compromised website in a blacklist provides an upper bound on when it turned malicious. We can then retrieve older archived versions of the site from the Wayback Machine to obtain examples of sites that were initially benign but later became malicious.

### Sampling Benign Websites

We obtain benign websites by randomly sampling DNS zone files and ensuring that these sites have never appeared on any blacklist. We then collect archives of these benign sites from the Wayback Machine, allowing us to compare them with sites that have become malicious at the same point in the past.

### Training and Testing the Classifier

To evaluate the classifier's performance at a specific time \( t \), we use training examples from the past (e.g., \( t-h \)) and test on future data. Conversely, we cannot train on present data \( t \) and test on past data \( t-h \) because that would involve using future information unknown at the time of the test. Figure 1(b) illustrates that the data available for building predictions is a strict subset of the known data.

### Handling Imbalanced Datasets

For the learning process, we can employ class rebalancing techniques. Class rebalancing has been studied as a means to improve classifier performance by training on a distribution other than the naturally sampled one. Since we sample only a random subset of non-compromised sites, we already perform some resampling through a one-sided selection.

### Dynamic Extraction of Feature List

A classifier needs a list of features to base its decisions. Potential features include traffic statistics, filesystem structure, and webpage content. Here, we discuss these potential features in more detail and describe the dynamic process we use to update them.

#### Candidate Feature Families

- **Traffic Statistics**: Metrics such as traffic, popularity, and other related statistics can indicate if a website has been compromised. For example, a sudden change in popularity might suggest the site is part of a redirection campaign.
- **Filesystem Structure**: The directory hierarchy and the presence of certain files can reflect the type of software running on the webserver. For instance, the presence of a `wp-admin` directory might indicate the use of WordPress, which could be exploitable if unpatched.
- **Webpage Structure and Contents**: The structure and content of webpages can indicate the type of content management system or webserver software. We need to extract useful page structure and content, filtering out user-generated content to keep the template. Binary features, such as the presence of specific HTML tags, can be used, though they result in a large feature set.

#### Dynamic Updates

Traffic statistics are considered "static" features, while content-based features are dynamically extracted. We use a statistical heuristic to sort features that are useful for classifying recent training examples and apply the top-performing features to subsequent examples.

### Implementation Details

#### Data Sources

- **Malicious Websites**: We use historical data from PhishTank and a list of websites infected by search-redirection attacks. The Wayback Machine provided usable archives for a significant portion of these sites.
- **Benign Websites**: We randomly sampled the entire `.com` zone file and selected archives within a specified date range, removing any sites known to be malicious.

#### Parsing and Filtering Websites

- **Scraping Process**: We use the Scrapy framework and custom Python scripts to scrape web pages from the Wayback Machine. We select the earliest archive within a specified range and compare it with the most recent archive to ensure the content management system has not changed dramatically.
- **Filtering**: We filter out user-generated content, which includes blog posts, forum posts, guestbook entries, and comments, as this content is orthogonal to the security risks and is considered noise to the classifier.

### Extracting Information from Webpages

The process of extracting information from webpages is well-studied. We use an algorithm proposed by Yi et al. to break down each webpage into a Document Object Model (DOM) tree and join them into a style tree, which captures both the content and structural relationships. This allows us to discard user-generated content and preserve the template.

By following these steps, we can effectively classify and predict the vulnerability of websites, distinguishing between benign and soon-to-be malicious sites.