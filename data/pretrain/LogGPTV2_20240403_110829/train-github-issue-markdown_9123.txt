## Issue description
As it illustrates in the doc, `torch.nn.Batchnorm1d` supports both input of
size `(N, C, L)` and `(N, C)` . Howevere, when the input size is `(1,
C)`(batch size is 1), `pytorch` will produce an error. It seems that if `N >
1`, there will be no error at all.
Another interesting fact is, when I try to seperate a minibatch into multi
cards(by using `torch.nn.DataParallel`), since the batch size for each gpu is
not decided by me, this error may also happens with `N > 1`(for a specific
gpu, its batch size may be `1` under some circumstances). I know currently
`PyTorch` does not support distributed batchnorm, but it still worth a report,
I think.
## Code example
    import torch
    class Foo(torch.nn.Module):
        def __init__(self):
            super(Foo, self).__init__()
            self.bn = torch.nn.BatchNorm1d(3)
        def forward(self, x):
            return self.bn(x)
    def main():
        x1 = torch.ones(1,3)
        x2 = torch.ones(2,3)
        model = Foo()
        y = model(x1) # this line will fail to run 
        y = model(x2) # this line can run without error
    if __name__ == '__main__':
        main()	
## System Info
PyTorch version: 0.4.0  
Is debug build: No  
CUDA used to build PyTorch: 9.1.85
OS: Debian GNU/Linux unstable (sid)  
GCC version: (Debian 6.4.0-17) 6.4.0 20180424  
CMake version: version 3.11.1
Python version: 3.6  
Is CUDA available: Yes  
CUDA runtime version: 9.1.85  
GPU models and configuration: GPU 0: GeForce GTX 1070  
Nvidia driver version: 390.48  
cuDNN version: Probably one of the following:  
/usr/lib/cuda/lib64/libcudnn.so.7.1.3  
/usr/lib/cuda/lib64/libcudnn_static.a
Versions of relevant libraries:  
[pip3] numpy (1.14.2)  
[pip3] torch (0.4.0)  
[pip3] torchvision (0.2.0)  
[conda] Could not collect