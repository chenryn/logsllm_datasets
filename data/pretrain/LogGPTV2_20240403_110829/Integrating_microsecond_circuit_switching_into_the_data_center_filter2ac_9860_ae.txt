more, the duty cycle overhead is only a function of the underlying
line rate, not the aggregate link rate (e.g., 100 or 400 Gbps).
Multiple-ring scaling: Another way to scale beyond 88 ports
is to use multiple stacked rings, with each ring reusing the same
wavelength channels, as shown in Figure 12. For example, an 8×8
ring-selection OCS would allow the construction of a 8 × 88 =
704-port OCS. It is important that all inputs assigned to the same
wavelength channel be connected to the same ring-selection OCS,
or else there could be a collision within a particular ring. The ring-
selection OCS is only used for the input ports; the output ports
directly connect to the individual rings. One downside of a stacked
ring architecture is the longer “week” lengths. Thus for low-latency
applications, a packet-switched network is still required.
While the single-ring architecture is fully non-blocking, the stacked-
ring architecture is blocking, meaning that not all input-output port
mappings are possible. Fundamentally the challenge comes from
reusing a ﬁnite number of wavelength channels across a larger num-
ber of switch ports. One possible solution to this problem is to in-
troduce another degree of freedom by using tunable lasers that can
transmit on any wavelength channel rather than on a speciﬁc chan-
nel. This freedom restores the fully non-blocking property of the
OCS at the cost of additional optical and algorithmic complexity.
In terms of scaling to higher port counts, we acknowledge that our
proposed approach will not directly apply to networks with clus-
ters larger than 704 ToRs. However, assuming 40 servers/ToR, this
constant still scales to 25 thousand servers in a single cluster.
Integrated OCS switching: Finally, it is possible to build an
integrated scale-out OCS by interconnecting smaller OCS switches
in a multi-stage topology on a single board, using waveguides in-
stead of discrete ﬁbers. This approach greatly reduces loss, since
the couplers used to connect the switch to the ﬁbers can be a sig-
niﬁcant source of loss. Multi-stage, integrated OCSes have been
built [3], but rely on slower 3D-MEMS technology.
Figure 11: Synchronization jitter as seen by the OS in our
Linux-based emulated ToR switches.
5.73–8.43 Gbps, or within 4.6% of EPS-UDP. The major reasons
for the discrepancy are duty cycle, NIC delay, the OS’s delay in
handling a softirq, and synchronization jitter (see discussion be-
low).
TCP throughput on the OCS (OCS-TCP) ranges from 2.23 to
5.50 Gbps, or within 12.1% of EPS-TCP for circuit day durations.
TCP throughput suffers from all of the issues of UDP throughput,
as well as two additional ones. First, TCP trafﬁc cannot use TSO
to ofﬂoad, and so the TCP/IP stack becomes CPU-bound handling
the required 506 connections. Second, the observed 0.5% loss rate
invokes congestion control, which decreases throughput. However,
TCP does show an upward trend in bandwidth with increasing duty
cycle.
We suspect that a reason for the reduction in throughput seen in
Mordia is due to the “long tailed” behavior of the synchronization
aspect of its control plane. To test this, we next examine synchro-
nization jitter. Synchronization packets are generated by the FPGA
to minimize latency and jitter, but the OS can add jitter when re-
ceiving packets and scheduling softirqs. To measure this jitter, we
set the day and night to 106 µs and 6 µs, respectively, and cap-
ture 1,922,507 synchronization packets across 3 random hosts. We
compute the difference in timestamps between each packet and ex-
pect to see packets arriving with timestamp deltas of either 6±1 µs
or 100±1 µs. We found that 99.31% of synchronization packets ar-
rive at their expected times, 0.47% of packets arrive with timestamp
deltas of zero, and 0.06% packets arrive with timestamp deltas be-
tween 522 µs and 624 µs (see Figure 11). The remaining 0.16% of
packets arrive between 7–99 µs. We also point out that the 0.53%
of synchronization packets that arrive at unexpected times is very
close to our measured loss rate. Our attempts to detect bad synchro-
nization events in the Qdisc did not change the loss rate measurably.
Firmware changes in the NIC could be used to entirely avoid the
need for these synchronization packets by directly measuring the
link up/down events.
Summary: Despite the non-realtime behavior inherent in emu-
lating ToRs with commodity PCs, we are able to achieve 95.4% of
the bandwidth of a comparable EPS with UDP trafﬁc, and 87.9%
of an EPS, sending non-TSO TCP trafﬁc. We are encouraged by
these results, which we consider to be lower bounds of what would
be possible with more precise control over the ToR.
7. SCALABILITY
Supporting large-scale data centers requires an OCS that can
scale to many ports. We brieﬂy consider these scalability impli-
cations.
1e-061e-051e-041e-031e-021e-01102550751005255505756006256±1µs(49.69%)0µs(0.47%)522to624µs(0.06%)100±1µs(49.62%)Timestamp Delta (μs)N=1,922,507Frequency(log scale)456By using these pause frames, the NIC can be much more precisely
controlled. The approach taken by [22] is not directly applicable to
Mordia, since there is no way for a central controller to send pause
frames to connected devices when a circuit is not established to that
endpoint.
9. CONCLUSIONS
In this paper, we have presented the design and implementation
of the Mordia OCS architecture, and have evaluated it on a 24-port
prototype. A key contribution of this work is a control plane that
supports an end-to-end reconﬁguration time 2–3 orders of mag-
nitude smaller than previous approaches based on a novel circuit
scheduling approach called Trafﬁc Matrix Scheduling. While Mor-
dia is only one piece in a larger effort, we are encouraged by this
initial experience building an operational hardware/software net-
work that supports microsecond switching.
10. ACKNOWLEDGMENTS
This work is primarily supported by the National Science Foun-
dation CIAN ERC (EEC-0812072) and a Google Focused Research
Award. Additional funding was provided by Ericsson, Microsoft,
and the Multiscale Systems Center, one of six research centers
funded under the Focus Center Research Program (FCRP), a Semi-
conductor Research Corporation program. We would like to thank
our shepherd Arvind Krishnamurthy, and Alex Snoeren and Geoff
Voelker for providing invaluable feedback to this work.
11. REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable,
Commodity, Data Center Network Architecture. In
Proceedings of ACM SIGCOMM, Aug. 2008.
[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic Flow Scheduling for Data
Center Networks. In Proceedings of 7th USENIX NSDI, Apr.
2010.
[3] W. Anderson, J. Jackel, G.-K. Chang, H. Dai, W. Xin,
M. Goodman, C. Allyn, M. Alvarez, O. Clarke, A. Gottlieb,
F. Kleytman, J. Morreale, V. Nichols, A. Tzathas, R. Vora,
L. Mercer, H. Dardy, E. Renaud, L. Williard, J. Perreault,
R. McFarland, and T. Gibbons. The MONET Project—A
Final Report. IEEE Journal of Lightwave Technology,
18(12):1988–2009, Dec. 2000.
[4] D. Beaver, S. Kumar, H. C. Li, J. Sobel, and P. Vajgel.
Finding a needle in Haystack: Facebook’s photo storage. In
Proceedings of 9th USENIX OSDI, Oct. 2010.
[5] G. Birkhoff. Tres Observaciones Sobre el Algebra Lineal.
Univ. Nac. Tucumán Rev. Ser. A, 5:147–151, 1946.
[6] K. Chen, A. Singla, A. Singh, K. Ramachandran, L. Xu,
Y. Zhang, and X. Wen. OSA: An Optical Switching
Architecture for Data Center Networks and Unprecedented
Flexibility. In Proceedings of 9th USENIX NSDI, Apr. 2012.
[7] N. Farrington, G. Porter, Y. Fainman, G. Papen, and
A. Vahdat. Hunting Mice with Microsecond Circuit
Switches. In Proceedings of 11th ACM HotNets, 2012.
[8] N. Farrington, G. Porter, S. Radhakrishnan, H. H. Bazzaz,
V. Subramanya, Y. Fainman, G. Papen, and A. Vahdat.
Helios: A Hybrid Electrical/Optical Switch Architecture for
Modular Data Centers. In Proceedings of ACM SIGCOMM,
Aug. 2010.
[9] J. E. Ford, V. A. Aksyuk, D. J. Bishop, and J. A. Walker.
Wavelength Add-Drop Switching Using Tilting
Figure 12: Multiple independent rings can be stacked to in-
crease the total port count. Each of the k rings has N ports.
Every input port jN + i, where j ∈ {0..k − 1} and i ∈ {1..N},
is bundled together into a k×k ring-selection OCS before being
sent to its default ring. This approach allows an input normally
destined for one ring to arrive at a different ring.
8. RELATED WORK
Optical switching technologies: Realistic optical switches that
can be used in practice require a limited overall insertion loss and
crosstalk, and must also be compatible with commercial ﬁber op-
tic transceivers. Subject to these constraints, the performance of
a switch is characterized by the switch speed and port count. Op-
tical switches based on electro-optic modulation or semiconduc-
tor ampliﬁcation can provide nanosecond switching speeds, but in-
trinsic crosstalk and insertion loss limit their port count. Analog
(3D) MEMs beam steering switches can have high port counts (e.g.,
1000 [4]), but are limited in switching speed on the order of mil-
liseconds. Digital MEMs tilt mirror devices are a “middle-ground”.
They have a lower port count than analog MEMs switches, but have
a switching speed on the order of a microsecond [9] and a sufﬁ-
ciently low insertion loss to permit constructing larger port-count
OCSes by composition.
“Hotspot Schedulers”: Mordia is complementary to work such
as Helios [8], c-Through [25], Flyways [13], and OSA [6], which
explored the potential of deploying optical circuit switch technol-
ogy in a data center environment. Such systems to date have all
been examples of hotspot schedulers. A hotspot scheduler observes
network trafﬁc over time, detects hotspots, and then changes the
network topology (e.g., optically [6, 8, 25] or wirelessly [13]) such
that more network capacity is allocated to trafﬁc matrix hotspots
and overall throughput is maximized. Rather than pursing such a
reactive policy, Mordia instead chooses a proactive scheduling ap-
proach in which multiple scheduling decisions are amortized over
a single pass through the control plane.
Optical Burst Switching: Optical Burst Switching [17, 21] is a
research area exploring alternate ways of scheduling optical links
through the Internet. Previous and current techniques require the
optical circuits to be setup manually on human timescales. The re-
sult is low link utilization. OBS introduces statistical multiplexing
where a queue of packets with the same source and destination are
assembled into a burst (a much larger packet) and sent through the
network together. Like OBS, the Mordia architecture has a separate
control plane and data plane.
TDMA: Time division multiple access is often used in wireless
networks to share the channel capacity among multiple senders and
receivers. It is also used by a few wired networks such SONET,
ITU-T G.hn “HomeGrid” LANs and “FlexRay” automotive net-
works.
Its applicability to data center packet-switched Ethernet
networks was studied in [22], which found that much of the OS-
based synchronization jitter can be eliminated by relying on in-
NIC functionality such as 802.1Qbb Priority-based pause frames.
k × kOCSInput PortsOutput Ports2N + iiN + i(k-1)N + iiN + i2N + i(k-1)N + iRing 1Ring 2Ring 3Ring k457Micromirrors. IEEE Journal of Lightwave Technology,
17:904–911, 1999.
[10] Glimmerglass 80x80 MEMS Switch. http://www.
glimmerglass.com/products/technology/.
[11] A. Goel, M. Kapralov, and S. Khanna. Perfect Matchings in
O(nlogn) Time in Regular Bipartite Graphs. In Proceedings
of 42nd ACM STOC, June 2010.
[12] Hadoop: Open source implementation of Map Reduce.
http://hadoop.apache.org/.
[13] D. Halperin, S. Kandula, J. Padhye, P. Bahl, and
D. Wetherall. Augmenting Data Center Networks with
Multi-Gigabit Wireless Links. In Proceedings of ACM
SIGCOMM, Aug. 2011.
[14] U. Hoelzle and L. A. Barroso. The Datacenter as a
Computer: An Introduction to the Design of
Warehouse-Scale Machines. Morgan and Claypool
Publishers, 2009.
[15] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner.
OpenFlow: Enabling Innovation in Campus Networks. ACM
Computer Communication Review, 38(2), Apr. 2008.
[16] R. N. Mysore, A. Pamporis, N. Farrington, N. Huang,
P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat.
PortLand: A Scalable, Fault-Tolerant Layer 2 Data Center
Network Fabric. In Proceedings of ACM SIGCOMM, Aug.
2009.
[17] C. Qiao and M. Yoo. Optical Burst Switching (OBS) – A
New Paradigm for an Optical Internet. Journal of High
Speed Networks, 8(1):69–84, 1999.
[18] R. Sinkhorn. A Relationship Between Arbitrary Positive
Matrices and Doubly Stochastic Matrices. The Annals of
Mathematical Statistics, 35(2):876–879, 1964.
[19] T. A. Strasser and J. L. Wagener. Wavelength-Selective
Switches for ROADM Applications. IEEE Journal of
Selected Topics in Quantum Electronics, 16:1150–1157,
2010.
[20] Y. Tamir and G. L. Frazier. High-Performance Multi-Queue
Buffers for VLSI Communication Switches. In Proceedings
of 15th ACM ISCA, May 1988.
[21] J. S. Turner. Terabit Burst Switching. Journal of High Speed
Networks, 8(1):3–16, 1999.
[22] B. C. Vattikonda, G. Porter, A. Vahdat, and A. C. Snoeren.
Practical TDMA for Datacenter Ethernet. In Proceedings of
ACM EuroSys, Apr. 2012.
[23] J. von Neumann. A certain zero-sum two-person game
equivalent to the optimal assignment problem. Contributions
to the Theory of Games, 2:5–12, 1953.
[24] M. Walraed-Sullivan, K. Marzullo, and A. Vahdat.
Scalability vs. Fault Tolerance in Aspen Trees. Technical
Report MSR-TR-2013-21, Microsoft Research, Feb 2013.
[25] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki,
T. S. E. Ng, M. Kozuch, and M. Ryan. c-Through: Part-time
Optics in Data Centers. In Proceedings of ACM SIGCOMM,
Aug. 2010.
458