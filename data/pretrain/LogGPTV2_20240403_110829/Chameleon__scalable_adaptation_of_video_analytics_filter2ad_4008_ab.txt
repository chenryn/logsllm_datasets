which is the harmonic mean of precision and recall. Figure 2
shows an illustrative example. We identify true positives in
the F1 score using two conditions: (1) a bounding box-based
condition, which checks if the detected bounding box has
the same label as some ground truth box; or (2) a label-based
condition, which checks if the bounding box has the same
label and sufficient spatial overlap with some ground truth
box [14]. Both metrics are useful in real applications and used
in our evaluation (§6), consistent with prior work [24, 32].
Chameleon:	Configuration	ControllerConfiguration:•Resolution•Frame	rate•Object	detectorDNN	Object	Detection(e.g.,	YOLO)1280pChameleon:	Configuration	ControllerConfiguration:•Min	bounding	box	size•Classifier•Resolution•Frame	rateDNN	Classification(e.g.,	ResNet)480pSelecting	Regions	of	interestResizing	&Frame	Selection(a) Pipeline A(b) Pipeline BConfigc	outputGolden	configoutput	(ground	truth)SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
J. Jiang et al.
(a) Frame rate
(b) Image size
(c) Detection model
(d) Min bounding box size
(e) Classifier
Figure 3: Impact of each configuration knob on resource-
accuracy tradeoffs. The accuracy threshold α = 0.8.
To compute the accuracy of a frame that was not sampled by
c, we use the location of objects from the previous sampled
frame. For a video segment, which consists of many frames,
we compute accuracy as the fraction of frames with F1 score
≥ α.1 The above metric for accuracy nicely lends itself to
composing application-level metrics. For instance, in a traffic
analytics deployment [25], measuring the fraction of frames
with F1 score ≥ 0.7 was a good indicator of the error in the
application-level traffic counts. Note that our techniques can
also directly work with other accuracy metrics.
Cost (resource consumption): We use average GPU pro-
cessing time (with 100% utilization) per frame as the met-
ric of resource consumption, because GPU is the dominant
resource for the majority of video processing workloads.
Further, the performance of NN-based inference is more
dependent on GPU cycles than typical data analytics tasks.
Performance impact: Figure 3 shows how the configura-
tion knobs affect the performance of object detection, mea-
sured by accuracy and resource consumption. We use a
dataset of 120 clips of traffic videos (30fps frame rate and
960p resolution, see §6.1 for details). Different points rep-
resent the resource-accuracy tradeoffs of setting each knob
to different values while fixing other knobs to their most
expensive values. We see that one can reduce resource con-
sumption by tuning the values of these configurations, an
observation that has informed other work [32].
At the same time, however, we notice that reducing re-
source consumption leads to substantial accuracy degrada-
tion. This is because a fixed configuration is used for the
entirety of each video (several minutes), during which the
content changes significantly. In the next section, we show
that the relationship between configuration and accuracy
1Calculating an overall accuracy based on per-frame accuracy of consecutive
frames could be biased due to correlations in the frames’ content, but we
mitigate this by using long videos sampled across different hours (§6.1).
(a) Pipeline A
(b) Pipeline B
Figure 4: Potential benefit of updating the NN configuration
periodically (every T = 4 seconds). Ignoring profiling, both ac-
curacy and cost significantly improve (red), but when profiling
is factored in, the cost is worse (yellow) than one-time profiling.
has great temporal variability, so dynamically adapting the
configuration can lead to better resource-accuracy tradeoffs.
3 POTENTIAL OF ADAPTATION
The basic premise of Chameleon is that videos and the char-
acteristics of video analytics pipelines exhibit substantial dy-
namics over time. As a result, to achieve the “best” resource-
accuracy tradeoff, we need to continuously adapt the config-
urations of the video pipelines.
3.1 Quantifying potential
We first show the value of continuous adaptation by com-
paring two simple policies for selecting NN configurations.
1. One-time update: This is a one-time offline policy that ex-
haustively profiles all configurations on the first x seconds
of the video, picks the cheapest configuration that has at
least α accuracy, and sticks with it for the whole duration
of the video (e.g., [32]). We use x = 10.
2. Periodic update: This policy divides the video into T -second
intervals, and profiles all configurations in each interval
for the first t seconds of that interval. It then sticks with
the cheapest configuration whose accuracy is greater than
α for the rest of the interval, i.e., for T − t seconds. We use
T = 4 and t = 1 for our experiments. We examine how
sensitive the results are to T in §6.3.
Figure 4 shows the resource-accuracy tradeoffs of running
the two policies on 30 traffic videos (there are 30 dots per
color). We set the target accuracy threshold α to 0.7 and 0.8,
respectively (we observe similar results with other thresh-
olds). The figures show that the periodic policy (red) reduces
per-frame resource consumption by over 10× and improves
the accuracy by up to 2× over the one-time policy (blue).
Intuition: The intuition behind these improvements is that
the accuracy of a given configuration can depend heavily
on the video content. If the video content becomes more
challenging (e.g., traffic moves faster, or there is less light-
ing), using the same configuration will negatively impact the
accuracy. Instead, we need to move to another configuration
that will increase the accuracy, likely at the expense of us-
ing more resources. Similarly, if the video content becomes
 0 0.1 0.2 0.3 0.4 0.93 0.96 0.99Avg GPU time per frame (sec)Accuracy (F1 score) 0 0.1 0.2 0.3 0.4 0.7 0.8 0.9 1Avg GPU time per frame (sec)Accuracy (F1 score) 0 0.1 0.2 0.3 0.4 0.6 0.7 0.8 0.9 1Avg GPU time per frame (sec)Accuracy (F1 score) 0 0.2 0.4 0.6 0.8 1 0.4 0.6 0.8 1Avg GPU time per frame (sec)Accuracy (F1 score) 0 0.2 0.4 0.6 0.8 1 0.4 0.6 0.8 1Avg GPU time per frame (sec)Accuracy (F1 score) 0.01 0.1 1 10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Avg GPU time per frame (sec)Frac. of frames with accurate resultOne-time updatePeriodic update (Inference only)Periodic update (Profiling+Inference) 0.1 1 10 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Avg GPU time per frame (sec)Frac. of frames with accurate resultOne-time updatePeriodic update (Inference only)Periodic update (Profiling+Inference)Chameleon: Scalable Adaptation of Video Analytics
(a)
(b)
Figure 5: Time-varying accuracy of two video clips. The grey
lines show the accuracy threshold. Picking configurations using
the first 20 minutes causes resource waste (5a) or low accuracy
(5b) in the rest of the video.
less demanding, we might have the opportunity to move to
another configuration that consumes less resources, while
still maintaining the desired accuracy.
Figure 5 illustrate this intuition, by plotting the accuracy
of the two policies over time for two video clips. In Figure 5a,
initially the cars are moving slowly, so the one-time policy
picks a low frame sampling rate that is sufficient to achieve
the desired accuracy. After a while, however, the cars start
moving much faster, and the low sampling rate is no longer
sufficient to correctly detect them. In contrast, the periodic
policy is able to maintain high accuracy by increasing the
frame sampling rate. Figure 5b shows the converse: cars start
out moving very fast, causing both policies to start with a
high frame sampling rate. When the cars slow down, the
periodic policy reduces the frame sampling rate and saves
GPU resources by over 2× compared to the one-time policy.
Cases like Figure 5, where accuracy varies significantly over
time, are common in real-world camera streams.
3.2 Prohibitive profiling cost
While Figure 4 shows considerable potential of the periodic
policy, a big caveat is that these results do not include the pro-
filing cost; they only include the cost of running the selected
configuration. Not surprisingly, profiling all configurations
every T seconds induces a significant profiling cost. Worse
yet, this profiling cost grows exponentially in the number of
configuration knobs and the number of values per knob.
If done naively, the profiling cost of the periodic policy
can negate any gains made by dynamically adapting the
configuration. As shown in Figure 4, the total cost when
including profiling (yellow) is almost 20× higher than the
actual cost of running the selected configuration (red), and
it is at least as high as (or in Pipeline A, over 3× higher than)
profiling configurations once (blue).
A sizable component of this profiling cost comes from
running the golden configuration. Recall from §2.2 that we
use the golden configuration to obtain the ground truth
for evaluating the accuracy of a given configuration. On
average, running the golden configuration requires an order
of magnitude more GPU resources than other configurations;
e.g., running a pre-trained FasterRCNN+ResNet101 model
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
on a video encoded in 1280p and 30fps requires 10× more
GPU resources than running a pre-trained SSD-MobileNet
model on the same video encoded in 1280p and 10fps.
3.3 Challenges in reducing profiling cost
At first sight, it might appear that using a state-of-the-art
search algorithm (e.g., [12, 19]) could address the prohib-
itively high profiling cost of the periodic policy. Unfortu-
nately, these algorithms are inadequate for our setting.
First, existing algorithms are designed for offline settings
(e.g., find the optimal cloud configuration for a Hadoop appli-
cation [12]). In contrast, our setting requires periodic profil-
ing, where the cost of a profiling event must be significantly
lower than the actual cost of running the selected configu-
ration between two profiling events. Second, the profiling
cost includes the cost of running the golden configuration,
which itself can be prohibitively expensive.
Note that simply increasing the update interval T does
not help in practice. If the update interval is too large, we
might either miss changes in the video content, which could
negatively impact accuracy, or miss opportunities to reduce
the resource consumption.
In summary, naive continuous profiling is expensive for
three reasons. We have to frequently run the golden config-
uration on each video stream, we have to profile all video
streams, and the configuration space is exponentially large.
4 KEY IDEAS IN CHAMELEON
Chameleon tackles these challenges using three domain-
specific empirical observations about the impact of configu-
rations on the accuracy and cost of video analytics pipelines.
First, if the NN configurations’ resource-accuracy tradeoff is
affected by some persistent characteristics of the video, we
can learn these temporal correlations to reuse configurations
over time (§4.1). Second, if two video feeds share similar
characteristics, it is likely they will also share the same best
configurations. Such cross-camera correlations provide an
opportunity to amortize profiling cost across multiple cam-
era feeds (§4.2). Finally, we have experimentally observed
that many of the configuration knobs independently impact
accuracy, allowing us to avoid an exponential search (§4.3).
Notice that these empirical observations are not specific
to the videos in our dataset, nor to the profiling algorithm
we use. The next sections provide the intuition behind the
observations and discuss their implications on Chameleon.
4.1 Persistent characteristics over time
While the characteristics of videos change over time, the un-
derlying characteristics of the video objects (e.g., size, class,
viewing angle) that affect accuracy tend to remain relatively
stable over time. As a result, configurations that are partic-
ularly bad tend to remain so. For example, if a camera is
covering objects from a side view and an object detector
is not tuned to detect objects at a side-view angle, it will
almost always produce results with low accuracy and hence
 0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 100F1 scoreTime (sec)1 fps30 fps 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50 60F1 scoreTime (sec)1 fps30 fpsSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
J. Jiang et al.
Figure 6: Distribution of persistence of two configurations,
run on 120 150-second video clips.
should not be selected. Such configurations can be learned
and discarded from our profiling for longer periods of time.
Similarly, good configurations also tend to consistently
produce good performance. A common example is surveil-
lance video, which tends to have static content. For such
videos, a low-cost configuration (e.g., low frame rate) would
be sufficient for a long period of time. More generally, even
though the best configuration, i.e., the one with the lowest
cost meeting the accuracy threshold α, might change fre-
quently, the set of top-k best configurations (top-k cheapest
configurations with accuracy ≥ α) tend to remain stable over
time. Thus, we can dramatically reduce the search space by
focusing on these top-k configurations.
To show the distribution of time intervals over which the
video characteristics remain relatively stable, we define the
persistence of a configuration c as the length of contiguous
frames for which c’s accuracy exceeds a threshold α for over
95% of the frames (we do not use 100% to avoid noise). Fig-
ure 6 shows the distribution of the persistence of two typical
configurations. We observe that half of the time, the configu-
rations persistently exceed the accuracy threshold for more
than 200 frames (roughly 6 seconds at 30fps).
Neither of these rules holds all of the time. Thus, we period-
ically explore the full configuration space, to give previously
bad configurations the opportunity to prove their worth, and
allow previously good configurations to fall from grace.
4.2 Cross-camera similarities
Video feeds that exhibit spatial correlations are abundant in
practice. For instance, the traffic cameras facing a highway
section may be correlated, because same vehicles may appear
in the video feeds of multiple cameras.
Even when cameras do not observe the same scene, their
video feeds can still have similar characteristics. Figure 7a
and 7b show two similar traffic cameras deployed in a city.
The vehicles in the city will likely have similar moving
speeds, lighting that is influenced by weather conditions,
and viewing angles due to the cameras being installed at
similar heights (as a result of uniform installation policies).
Even if the cameras are not in geographic proximity, cameras
deployed for the same purpose such as traffic analytics are
likely to exhibit similarities. This can happen, for example,
due to the underlying similarity of street planning across
a country. Such similarity can also occur for surveillance
cameras covering an enterprise building. Figure 7c and 7d
(a) Camera #1
(b) Camera #2
(c) Camera X
(d) Camera Y
Figure 7: Screenshots of two similar traffic video feeds (a, b)
and two similar indoor video feeds (c, d). (Faces are blurred to
protect identities.)
show screenshots of two indoor cameras (see §6.1 for details)
who do not share field of view but have similar classes of
objects (e.g., humans), temporal patterns of movement (e.g.,
more people movement during lunch/dinner time), lighting,
etc., so they tend to share the best configuration.
As more cameras are deployed with increasing spatial
density, we expect more opportunities of amortizing the
profiling cost over many similar video feeds, especially in
real-time applications such as traffic monitoring and building
surveillance, where many cameras have similar video feeds.
Note that the spatial correlations of best configurations do
not mean that applying the same configuration will produce
the exact same accuracy on different videos. The timescales
at which such correlations emerge can be larger than the
timescale at which the accuracy changes. Thus, we should
not blindly reuse the best configuration on another video,
even when the characteristics of the two videos are deemed
very similar. Instead, we can leverage the fact that simi-
lar videos tend to have similar distributions of best config-
urations. Then, we can use the most promising configura-
tions from one camera—e.g., the top-k best configurations—to
guide the profiling of a spatially-related camera. Finally, we
do not simply apply the same configurations on all cameras.
For instance, in Figure 8, Camera #1 and #2 (from Figure 7)
have similar resource-accuracy profiles that are different
from that of Camera #3 (not shown in Figure 7), so the first