User: A bit of a beginner question, but I was not able to find a relevant answer on this..
Essentially my data about (7gb) is located on my local machine. I have distributed cluster running on the local network. How can I get this file onto the cluster? 
The usual dd.read_csv() or read_parquet() fails, as the workers aren't able to locate the file in their own environments.
Would I need to manually transfer the file to each node in the cluster?
Note: Due to admin restrictions I am limited to SFTP...
Assistant: Network solution : 

Under Windows only it should works  with a shared forlder: dd.read_csv("\\server\shared_dir")
Under Unix/Linux only it should works with HDFS: import hdfs3 and then hdfs.read_csv('/server/data_dir'...)

But if you want to use Windows AND Linux workers at the same time I don't know since dd.read_csv() with UNC does not seem to be supported under Linux (because of the file path '\server\data_dir') and HDFS with hdfs.read_csv is not working under Windows (import hdfs3 failed because the lib libhdfs3.so doesn't exist under Windows)
Does anyone have a Network solution for workers under Windows and Unix ?