u
d
s
C
),
d
s
with  the  average  server  utilization 
utilization 
.  
≤
0
1
≤ b
su
0
=
C
s
≤ d
su
1(
−
u
b
s
, 
)
≤
1
 and  the  peak 
−
∏
burst
server
ws
,
For the server-burst model, we divide the burst portion of 
costs  for  a  server  in  a  manner  that  is  weighted  by  the 
burstiness  of  each  workload  on  the  server.  In  a  second  step, 
the  server’s  unallocated  resources  are  apportioned  based  on 
the bursty costs. Server-burst ∏server-burst
s,w is defined as: 
+
ε
W
∑
(
ε
w
1'
=
∏
W
∑
w
1'
=
d
W
∑
d
w
1'
=
server
ws
,
server
ws
,
server
ws
,
server
ws
,
(2) 
∏=
burst
burst
burst
burst
temp
temp
temp
temp
ws
,
ws
,
ws
,
∏
∏
sw
C
C
C
a
s
d
s
b
s
=
+
+
+
b
b
)
−
−
−
−
−
−
−
−
'
'
'
The  ε  value,  a  small  value,  in  the  numerator  and 
denominator of the 2nd term of the first equation ensures that 
the  denominator  does  not  evaluate  to  zero  for  cases  where 
there is no difference between peak and mean resource usage. 
Using  Eq.  (2)  with  consolidation  scenario  c,  the  total  CPU 
costs  are  $54.7  for  Workload A  and  $21.7  for  Workload B. 
The difference stems from  the fact that Workload A is  much 
burstier than Workload B. 
The  proposed  server-burst  model 
incorporates  more 
accurately the  workload resource usage patterns over time in 
the  cost  structure.  However,  note  that  dividing  costs  in  this 
way may lead to a lack of robustness for workload costs. The 
computed costs are sensitive to the placement of workloads on 
servers.  In  particular,  the  amount  of  unused  resources  at  the 
server  depends  on  the  workloads  assigned  to  the  server,  and 
hence  may  differ  under  different  placement  decisions.  This 
might  correspond  to  a  significant  portion  of  the  cost.  It  may 
change based on placement decisions and therefore introduces 
variability for a workload’s reported share of cost. Intuitively, 
a cost for a given  workload should be mostly defined by the 
amount of resources used and the resource usage pattern and 
should  be  independent  of  workload  placement  decisions:  the 
customer  should  not  be  charged  different  costs  for  the  same 
workload under different workload placement scenarios. 
−
∏
To provide a more robust cost estimate, we introduce the 
following pool-burst model that attributes burstiness cost and 
unallocated resources using measures for the S servers in the 
resource pool instead of the individual servers. 
b
+
ε
WS
,
(
∑
ε
w
s
1'
,1'
=
=
pool
−
∏
ws
,
WS
,
∑
d
W
∑
∑
∑
(3) 
∏=
pool
ws
,
pool
ws
,
pool
ws
,
pool
ws
,'
'
∏
burst
burst
burst
burst
burst
C
C
C
temp
temp
temp
temp
ws
,'
+
+
+
=
b
d
ws
,
ws
,
ws
,
)
b
s
'
a
s
'
d
s
1'
=
1'
=
1'
=
w
−
−
−
−
−
−
−
S
s
S
s
∏
1'
=
s
,1'
=
w
'
'
Using Eq. (3) with consolidation scenario c, the total CPU 
costs  are  $62.2  for  Workload A  and  $23.0  for  Workload B. 
This  approach  attributes  all  the  unallocated  resources  in  the 
server pool in a fair way among all the workloads and makes it 
less dependent on the workload placement decisions.  
In  Section  V,  we  present  a  study  that  compares  cost 
apportioning  results  for  different  models  defined  by  Eq.  (1), 
(2) and (3). The introduced formulas are applied separately to 
various resources such as CPU and memory. The sum of the 
resulting costs represents the total costs for a workload.  
IV.  WORKLOAD CHARACTERIZATION 
To evaluate the effectiveness of different cost apportioning 
models, we obtained three months of workload trace data for 
312  workloads  from  one  HP  customer  data  center.  Each 
workload  was  hosted  on  its  own  server,  so  we  use  resource 
demand  measurements  for  the  server  to  characterize  its 
workload's demand trace. Each trace describes resource usage, 
e.g.,  processor  and  memory  demands,  as  measured  every  5 
minutes.  
We define CPU capacity and CPU demand in units of CPU 
shares. A CPU share denotes one percentage of utilization of a 
processor with a clock rate of 1 GHz. A scale factor adjusts for 
the capacity between nodes with different processor speeds or 
architectures. For example, the nodes with 2.2 GHz CPUs in 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:41:31 UTC from IEEE Xplore.  Restrictions apply. 
397our  case  study  were  assigned  220  shares.  We  note  that  the 
scaling factors are only approximate; the calculation of more 
precise  scale  factors  is  beyond  the  scope  of  this  paper.  The 
memory usage is measured in GB. 
Figure 2 and 3 summarize the memory and CPU usage for 
the  workloads  under  study.  Figure  2  shows  the  average  and 
maximum  memory  usage  for  each  workload.  Note,  that  we 
order  workloads  by 
their  average  memory  usage  for 
presentation  purposes.  Figure  3  shows  the  average  and 
maximum CPU usage of corresponding workloads. There are 
a few interesting observations:  
•  For 80% of the workloads, the memory usage is less than 
2 GB. While the maximum and average memory usage are 
small  and  very  close  in  absolute  terms  the  peak  to  mean 
ratios are still high.  
•  For  10%  of  the  workloads  the  memory  usage  is  much 
higher,  10─70 GB;  the  maximum  memory  usage  can  be 
very large in absolute terms but the peak to mean ratios are 
less than 3.  
• 
•  There  are  strong  correlations:  workloads  with  a  high 
memory  usage  (both  peak  and  average)  have  higher  
average  CPU  usage.  Figure  3  shows  that  the  first  30 
workloads  have  high  memory  usage  and  higher  average 
CPU usage than the remaining workloads. 
 Most  workloads  have  very  bursty  CPU  demands:  while 
most  of  the  time  these  workloads  have  low  CPU  usage 
(80% of the workloads use on average less than 220 CPU 
shares,  which  corresponds  to  one  physical  CPU)  their 
maximum  CPU  demand  is  rather  high  (42%  of  the 
workloads  have  a  peak  usage  of  more  than  1000  CPU 
shares).  
•  The average peak to mean ratio for CPU usage was 52.6, 