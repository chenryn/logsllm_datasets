title:TPDP'20: 6th Workshop on Theory and Practice of Differential Privacy
author:Rachel Cummings and
Michael Hay
The Complexity of Diﬀerential Privacy
Salil Vadhan∗
Center for Research on Computation & Society
John A. Paulson School of Engineering & Applied Sciences
Harvard University
Cambridge, Massachusetts USA
salil@seas.harvard.edu.
http://seas.harvard.edu/~salil.
August 9, 2016
Abstract
Diﬀerential Privacy is a theoretical framework for ensuring the privacy of individual-level
data when performing statistical analysis of privacy-sensitive datasets. This tutorial provides
an introduction to and overview of diﬀerential privacy, with the goal of conveying its deep
connections to a variety of other topics in computational complexity, cryptography, and theo-
retical computer science at large. This tutorial was written starting from notes taken during a
minicourse given by the author and Kunal Talwar at the 26th McGill Invitational Workshop on
Computational Complexity in February 2014, at the Bellairs Institute in Holetown, Barbados [1].
∗Written in part while visiting the Shing-Tung Yau Center and the Department of Applied Mathematics at National
Chiao-Tung University in Hsinchu, Taiwan. Supported by NSF grant CNS-1237235 and a Simons Investigator Award.
1
Contents
1 Introduction and Deﬁnition
3
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2 The Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3 Counting Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4 Diﬀerential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.5 Basic Mechanisms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.6 Discussion of the Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.7 Preview of the Later Sections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2 Composition Theorems for Diﬀerential Privacy
14
2.1 Post-processing and Group Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2 Answering Many Queries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3 Histograms
3 Alternatives to Global Sensitivity
20
3.1 Smooth Sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.2 Propose-Test-Release . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.3 Releasing Stable Values
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.4 Privately Bounding Local Sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4 Releasing Many Counting Queries with Correlated Noise
26
4.1 The SmallDB Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.2 Private Multiplicative Weights
5 Information-Theoretic Lower Bounds
33
5.1 Reconstruction Attacks and Discrepancy . . . . . . . . . . . . . . . . . . . . . . . . . 33
5.2 Packing Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
5.3 Fingerprinting Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
6 Computational Lower Bounds
50
6.1 Traitor-tracing Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
6.2 Lower Bounds for Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
7 Eﬃcient Algorithms for Speciﬁc Query Families
58
7.1 Point Functions (Histograms) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
7.2 Threshold Functions (CDFs)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
7.3 Conjunctions (Marginals)
8 Private PAC Learning
66
8.1 PAC Learning vs. Private PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . 66
8.2 Computationally Eﬃcient Private PAC Learning . . . . . . . . . . . . . . . . . . . . 68
8.3 The Sample Complexity of Private PAC Learning . . . . . . . . . . . . . . . . . . . . 71
1
9 Multiparty Diﬀerential Privacy
73
9.1 The Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
9.2 The Local Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
9.3 Two-Party Diﬀerential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
10 Computational Diﬀerential Privacy
77
10.1 The Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
10.2 Constructions via Secure Multiparty Computation . . . . . . . . . . . . . . . . . . . 78
10.3 Usefulness with a Trusted Curator? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
10.4 Relation to Pseudodensity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
11 Conclusions
Acknowledgments
References
Nomenclature
81
82
83
93
2
1
Introduction and Deﬁnition
1.1 Motivation
Suppose you are a researcher in the health or social sciences who has collected a rich dataset on
the subjects you have studied, and want to make the data available to others to analyze as well.
However, the dataset has sensitive information about your subjects (such as disease diagnoses,
ﬁnancial information, or political aﬃliations), and you have an obligation to protect their privacy.
What can you do?
The traditional approach to such privacy problems is to try to “anonymize” the dataset by re-
moving obvious identiﬁers, such as name, address, and date of birth, and then share the anonymized
dataset. However, it is now well-understood that this approach is ineﬀective, because the data that
remains is often still suﬃcient to determine who is who in the dataset, given appropriate auxiliary
information. This threat is not hypothetical; there have now been many high-visibility demon-
strations that such “re-identiﬁcation” attacks are often quite easy to carry out in practice, using
publicly available datasets as sources of auxiliary information [83].
A more promising approach is to mediate access to the data through a trusted interface, which
will only answer queries posed by data analysts. However, ensuring that such a system protects
privacy is nontrivial. What queries should be permitted? Clearly, we do not want to allow queries
that target a particular individual (such as “Does Sonny Rollins have sensitive trait X?”), even
if they are couched as aggregate queries (e.g. “How many people in the dataset are 84-year-old
jazz saxophonists with trait X?”). Even if a single query does not seem to target an individual,
a combination of results from multiple queries can do so (e.g. “How many people in the dataset
have trait X?” and “How many people in the dataset have trait X and are not 84-year-old jazz
saxophonists?”). These attacks can sometimes be foiled by only releasing approximate statistics,
but Dinur and Nissim [30] exhibited powerful “reconstruction attacks,” which showed that given
suﬃciently many approximate statistics, one can reconstruct almost the entire dataset. Thus there
are fundamental limits to what can be achieved in terms of privacy protection while providing useful
statistical information, and we need a theory that can assure us that a given release of statistical
information is safe.
Cryptographic tools such as secure function evaluation and functional encryption do not address
these issues. The kind of security guarantee such tools provide is that nothing is leaked other than
the outputs of the functions being computed. Here we are concerned about the possibility that the
outputs of the functions (i.e. queries) already leak too much information. Indeed, addressing these
privacy issues is already nontrivial in a setting with a trusted data curator, whereas the presence
of a trusted third party trivializes most of cryptography.
Diﬀerential privacy is a robust deﬁnition of privacy-protection for data-analysis interfaces that:
• ensures meaningful protection against adversaries with arbitrary auxiliary information (in-
cluding ones that are intimately familiar with the individuals they are targeting),
• does not restrict the computational strategy used by the adversary (in the spirit of modern
cryptography), and
• provides a quantitative theory that allows us to reason about how much statistical information
is safe to release and with what accuracy.
3
Following the aforementioned reconstruction attacks of Dinur and Nissim [30], the concept of
diﬀerential privacy emerged through a series of papers by Dwork and Nissim [34], Blum, Dwork,
McSherry, and Nissim [13], and Dwork, McSherry, Nissim, and Smith [38], with the latter providing
the elegant indistinguishability-based deﬁnition that we will see in the next section.
In the decade since diﬀerential privacy was introduced, a large algorithmic literature has de-
veloped showing that diﬀerential privacy is compatible with a wide variety of data-analysis tasks.
It also has attracted signiﬁcant attention from researchers and practitioners outside theoretical
computer science, many of whom are interested in bringing diﬀerential privacy to bear on real-life
data-sharing problems. At the same time, it has turned out to be extremely rich from a theoretical
perspective, with deep connections to many other topics in theoretical computer science and math-
ematics. The latter connections are the focus of this tutorial, with an emphasis on connections
to topics in computational complexity and cryptography. For a more in-depth treatment of the
algorithmic aspects of diﬀerential privacy, we recommend the monograph of Dwork and Roth [35].
1.2 The Setting
The basic setting we consider is where a trusted curator holds a dataset x about n individuals,
which we model as a tuple x ∈ Xn, for a data universe X. The interface to the data is given by a
(randomized) mechanism M : Xn × Q → Y, where Q is the query space and Y is the output space of
M. To avoid introducing continuous probability formalism (and to be able to discuss algorithmic
issues), we will assume that X, Q, and Y are discrete.
The picture we have in mind is as follows:
−→
M
q←−
q(x)−→
Data Analyst/
Adversary
Xn (cid:51)
x1
x2
...
xn
for a dataset x = (x1, . . . , xn).
1.3 Counting Queries
A basic type of query that we will examine extensively is a counting query, which is speciﬁed by a
predicate on rows q : X → {0, 1}, and is extended to datasets x ∈ Xn by counting the fraction of
people in the dataset satisfying the predicate:
n(cid:88)
i=1
q(x) =
1
n
q(xi) ,
(Note that we abuse notation and use q for both the predicate on rows and the function that
averages q over a dataset.) The examples mentioned above in Section 1.1 demonstrate that it
is nontrivial to ensure privacy even when answering counting queries, because answers to several
counting queries can be combined to reveal information about individual rows.
There are several speciﬁc families of counting queries that are important for statistical analysis
and will come up many times in this tutorial:
4
Point Functions (Histograms): Here X is an arbitrary set and for each y ∈ X we consider the
predicate qy : X → {0, 1} that evaluates to 1 only on input y. The family Qpt = Qpt(X)
consists of the counting queries corresponding to all point functions on data universe X.
(Approximately) answering all of the counting queries in Qpt amounts to (approximately)
computing the histogram of the dataset.
Threshold Functions (CDFs): Here X is a totally ordered set, and we consider the set Qthr =
Qthr(X) of threshold functions. That is, for each y ∈ X, Qthr contains counting query cor-
responding to the function qy(z) that outputs 1 iﬀ z ≤ y. (Approximately) answering all
of the counting queries in Qthr is tantamount to (approximating) the cumulative distribution
function of the dataset.
Attribute Means (1-way Marginals): Here X = {0, 1}d, so each individual has d boolean at-
tributes, and Qmeans = Qmeans(d) contains the counting queries corresponding to the d co-
ordinate functions qj : {0, 1}d → {0, 1} deﬁned by qj(w) = wj for j = 1, . . . , d. Thus,
(approximately) answering all of the queries in Qmeans = Qmeans(d) amounts to (approxi-
mately) computing the fraction of the dataset possessing each of the d attributes. These are
also referred to as the (1-way) marginal statistics of the dataset.
Conjunctions (Contingency Tables): Here again X = {0, 1}d, and for an integer t ∈ {0, 1, 2, . . . , d},
2
t = Qconj
t
we consider the family Qconj
(d) of counting queries corresponding to conjunctions of
(5) contains the function q(w) = w2 ∧ ¬w4, which could rep-
t literals. For example, Qconj
resent a query like “what fraction of individuals in the dataset have lung cancer and are
nonsmokers?”. Notice that Qconj
(d) consists of the queries in Qmeans(d) and their negations,
(d) contains the same queries as Qpt({0, 1}d). We have |Qconj
and Qconj
when t ≤ d1−Ω(1). We also consider the family Qconj = Qconj(d) = ∪d
(d), which is of
size 3d. The counting queries in Qconj
are also called t-way marginals and answering all of
them amounts to computing the t-way contingency table of the dataset. These are important
queries for statistical analysis, and indeed the answers to all queries in Qconj is known to be
a “suﬃcient statistic” for “logit models.”
(cid:1) · 2t = dΘ(t)
(d)| =(cid:0)d
d
1
t
Qconj