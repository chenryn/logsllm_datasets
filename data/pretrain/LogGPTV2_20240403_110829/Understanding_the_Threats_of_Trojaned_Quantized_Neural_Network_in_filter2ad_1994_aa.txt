title:Understanding the Threats of Trojaned Quantized Neural Network in
Model Supply Chains
author:Xudong Pan and
Mi Zhang and
Yifan Yan and
Min Yang
Understanding the Threats of Trojaned Quantized Neural
Network in Model Supply Chains
Xudong Pan
PI:EMAIL
Fudan University
China
Yifan Yan
PI:EMAIL
Fudan University
China
Mi Zhang∗
Fudan University
PI:EMAIL
China
Min Yang∗
PI:EMAIL
Fudan University
China
ABSTRACT
Deep learning with edge computing arises as a popular paradigm
for powering edge devices with intelligence. As the size of deep
neural networks (DNN) continually increases, model quantization,
which converts the full-precision model into lower-bit representa-
tion while mostly preserving the accuracy, becomes a prerequisite
for deploying a well-trained DNN on resource-limited edge devices.
However, to properly quantize a DNN requires an essential amount
of expert knowledge, or otherwise the model accuracy would be
devastatingly affected. Alternatively, recent years witness the birth
of third-party model supply chains which provide pretrained quan-
tized neural networks (QNN) for free downloading.
In this paper, we systematically analyze the potential threats of
trojaned models in third-party QNN supply chains. For the first time,
we describe and implement a QUAntization-SpecIfic backdoor at-
tack (QUASI), which manipulates the quantization mechanism to
inject a backdoor specific to the quantized model. In other words,
the attacker-specified inputs, or triggers, would not cause misbe-
haviors of the trojaned model in full precision until the backdoor
function is automatically completed by a normal quantization op-
eration, producing a trojaned QNN which can be triggered with a
near 100% success rate. Our proposed QUASI attack reveals several
key vulnerabilities in the existing QNN supply chains: (i) QUASI
demonstrates a third-party QNN released online can also be in-
jected with backdoors, while, unlike full-precision models, there is
almost no working algorithm for checking the fidelity of a QNN.
(ii) More threateningly, the backdoor injected by QUASI remains
inactivated in the full-precision model, which inhibits model con-
sumers from attributing undergoing trojan attacks to the malicious
model provider. As a practical implication, we alarm it can be highly
risky to accept and deploy third-party QNN on edge devices at the
current stage, if without future mitigation studies.
∗Corresponding Authors: Mi Zhang and Min Yang.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8579-4/21/12...$15.00
https://doi.org/10.1145/3485832.3485881
1
CCS CONCEPTS
• Security and privacy → Domain-specific security and pri-
vacy architectures; • Computing methodologies → Neural
networks.
KEYWORDS
model quantization, neural network, backdoor attack
ACM Reference Format:
Xudong Pan, Mi Zhang∗, Yifan Yan, and Min Yang. 2021. Understanding the
Threats of Trojaned Quantized Neural Network in Model Supply Chains. In
Annual Computer Security Applications Conference (ACSAC ’21), December
6–10, 2021, Virtual Event, USA. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3485832.3485881
1 INTRODUCTION
With the ever-evolving embedded system hardware, edge devices
including smartphones and smart house devices are gathering and
generating more informative data than ever, which gives birth to
the trend of deploying well-trained deep neural networks (DNN) on
edge devices to enable better analytic and inference capability [14].
Despite the evolving computing power brought by innovative hard-
ware designs, the inference phase of modern DNN architectures
still involves billions of floating-point operations per input [61], an
overly high computational and memory cost for most medium-end
edge devices with relatively limited resources. To realize real-time
inference and reduced memory cost in deploying DNNs on edge
devices, model quantization comes to rescue [22].
Departing from the general research on information quanti-
zation [25], model quantization [22, 35] becomes a prerequisite
for deploying a well-trained full-precision DNN (i.e., FPNN) on
resource-limited devices. In a typical model quantization process,
the real-valued parameters (i.e., weights) and the intermediate com-
putational results (i.e., activations) of a FPNN, usually represented
in FP32/FP64, are converted to lower-bit representation (e.g., INT8
[30], ternary [38] and binary [17]). Along with the intensively re-
duced memory cost, a quantized DNN (i.e., QNN) now computes
with cheaper operations on lower-bit representation, which essen-
tially accelerates the otherwise expensive inference process.
As model quantization relies on a number of concrete configura-
tions specific to the model and the dataset, to properly quantize a
FPNN in lower-bit representation is a non-trivial task, especially
when the user expects the produced QNN is almost as accurate
634ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Pan et al.
as the original model. According to [35], an improper choice of
symmetric/asymmetric schemes, per-layer/per-channel schemes
or improper calibration data is very likely to prevent the state-
of-the-art model quantization algorithm from producing an ideal
QNN, devastating the otherwise high accuracy. More background
on model quantization is presented in Section 2.2. As a result, the
essential amount of expert knowledge required for proper model
quantization substantially raises the bar of deploying DNNs on
edge devices, which catalyzes the recent birth of third-party sup-
ply chains of QNNs in deep learning (DL) ecosystem. For example,
an amount of pretrained QNNs are freely available on third-party
platforms like Apple Developer [3], PyTorch [1] and Paddle-Lite
[2], which allows users to download and deploy third-party QNNs
on target edge devices, with barely no cost.
However, as the development of QNN supply chains remains in
embryo, there is almost no previous work to characterize the vul-
nerability of QNN supply chains under backdoor attacks (or, trojan
attacks [26]), where an attacker maliciously modifies the parame-
ters of a DNN (i.e., backdoor injection) such that the trojaned model
would exhibit an expected misbehavior on an attacker-specified set
of inputs (i.e., triggers). On the one hand, similar to the development
of FPNN supply chains [31], a non-trivial amount of DL systems
may combine one or more third-party pretrained QNNs in the sys-
tem implementation, which, especially for safety/security-critical
DL systems, enlarges the potential threats of a trojaned QNN. On
the other hand, although the backdoor defensive vector for FPNNs
is gradually completed [13, 21, 42, 43, 60], few previous works have
ever studied the backdoor vulnerability of model quantization, not
to mention the design of backdoor defensive techniques for QNNs,
which leaves the access mechanism for incorporating a third-party
QNN into open model supply chains almost blank.
Our Work. In this paper, we present the first systematic study
on the potential threats of trojaned QNNs in third-party model
supply chains. Specifically, we describe and implement the first
QUAntization-SpecIfic backdoor attack (QUASI), which manipu-
lates the model quantization mechanism to inject a backdoor func-
tion specific to the quantized model. In other words, for a QUASI-
trojaned model, most of the trigger inputs would not cause model
misbehaviors when the model is in full precision, until the backdoor
function is automatically completed by a benign quantization opera-
tion, producing a trojaned QNN which can be successfully triggered
with near 100% probability. In practice, the aforementioned charac-
teristic of QUASI can inhibit model consumers of the third-party
supply chains from attributing the cause of future backdoor attacks
to the actual attacker, even if they collect a few trigger inputs [12],
and are permitted to determine whether the FPNN constructed by
the attacker already contains backdoor by querying the prediction
API. Meanwhile, due to the consideration of intelligent property
protection [10], existing backdoor defensive techniques can hardly
be applied to the FPNN constructed by the attacker as most of them
require the model and the dataset to be accessed as a white box.
Technically, to accomplish the above attack goals, QUASI con-
sists of the following key designs: (i) Generally, we exploit a popular
quantization method called quantization-aware training (QAT [30]),
which allows QUASI to simultaneously manipulate the model be-
haviors before and after the quantization operation. In other words,
with QAT, QUASI is able to efficiently solve the multi-task learning
objective that characterizes our attack goal, i.e., the trojan behavior
is activated and only activated for the quantized model. (ii) We ex-
ploit the clipping mechanism in quantization operation to enlarge
the computational discrepancy between the full-precision and the
quantized models, which, according to our analysis and empirical
results, is beneficial for enhancing the efficiency and stability of
injecting a quantization-specific backdoor. (iii) We further incor-
porate a stealthiness-oriented learning objective to regularize the
normality of the latent feature distribution of trigger inputs, which
equips QUASI with the ability of evading potential defenses.
From a practical viewpoint, our proposed QUASI attack for the
first time reveals several key vulnerabilities of QNN supply chains:
(i) A QNN can also be embedded with backdoor function during
the construction phase. Due to the lack of working algorithms in
detecting trojan in quantized models, it is challenging for existing
model supply chains to ensure the fidelity of third-party QNNs by
designing effective model access mechanisms. (ii) More threaten-
ingly, QUASI by design considers the stealthiness of the backdoor.
The backdoor function remains inactivated in the original FPNN
and can hardly be detected or eliminated from the produced QNN
with existing/potential countermeasures. In practice, our proposed
QUASI attack strongly alarms of the threats of accepting or deploy-
ing third-party QNNs on edge devices and urgently calls for future
mitigation studies against trojan attacks on QNNs.
Our Contributions. In summary, we mainly make the following
key contributions:
• We present the first systematic study on the potential threats of
trojaned QNNs in third-party model supply chains, where the key
vulnerabilities of incorporating third-party QNNs in production
environments are demonstrated and analyzed.
• We propose and implement QUASI, the first quantization-specific
backdoor attack which exploits the quantization mechanism to
embed a backdoor function to the model during the construction
phase. With strong specificity, the trojan function remains dor-
mant in the full-precision model until the quantization process
automatically completes the backdoor function, which reaches
near 100% attack success rate in the QNN.
• We conduct extensive evaluation on 3 real-world mission-critical
scenarios (i.e., traffic sign recognition, skin cancer diagnosis and
object detection) and 2 popular DNN architectures (i.e., LeNet
[37] and VGG-13 [55]) to validate the attack effectiveness of
QUASI. Besides, additional evaluation results further show QUASI
successfully evades entropy-based and pruning-based potential
countermeasures extended from 2 well-known backdoor defenses
on FPNNs (i.e., Fine-Pruning [42] and STRIP [21]).
2 BACKGROUND AND PRELIMINARIES
2.1 Backdoor Attacks in Deep Learning
As an emergent security vulnerability in deep learning (DL), back-
door attack (or, trojan attack [8, 26, 31, 41, 44, 47, 66]) on deep neural
networks (DNN) is a highly threatening integrity attack where the
parameters of a clean DNN are maliciously tampered by the at-
tacker to inject a secret backdoor (or, a trojan). Behaviorally, when
a trojaned DNN is input with a set of attacker-specified inputs (i.e.,
triggers), the model would exhibit abnormal behaviors expected
by the attacker. Otherwise, the model behaves normally on clean
2
635Understanding the Threats of Trojaned Quantized Neural Network in Model Supply Chains
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
inputs. To introduce the notations, we formulate below a typical
backdoor attack on a DNN f (·; Θ) : X → Y, where Θ is the origi-
nal parameter of the model and X (Y) is the input (output) space.
Specifically, in this paper, we mainly focus on DNNs for N -ary
classification task, i.e., Y = {1, 2, . . . , N}.
Typically, given a domain-relevant dataset D (e.g., the original
training set as in [26]), a backdoor attack is mainly composed of the
following three stages: (i) Trigger Generation: The adversary gen-
erally runs a predefined trigger generation algorithm T to generate
a trigger sample ˜x := T(x) from the base data sample (x, y) ∈ X×Y.
In a majority of existing backdoor attacks, the trigger generation
algorithm specifies a perturbation t (or called a trigger pattern) from
X and adds the perturbation to the base input to obtain the trigger
input ˜x := x ⊕ t. For example, BadNet, one of the earliest backdoor
attacks in the image domain, utilizes a small attacker-specified pixel
patch as the universal trigger pattern attached to each base image
[26]. (ii) Backdoor Injection: A backdoor injection algorithm A
produces a trojaned model with parameter ˜Θ based on the clean
model f (·; Θ) and the attack-specified trigger generation algorithm
T , i.e., ˜Θ := A(f , Θ, T). In general, the backdoor injection algo-
rithm pursues a high attack success rate, i.e., the probability of a
trigger input to be classified into the target class, while the modi-
fication should preserve the normal performance on the primary
learning task. For instance, as a conventional class of backdoor
injection algorithms, data poisoning-based injection first crafts a
set of triggers labeled with the attacker-expected class (i.e., target
class), mixes the triggers into the clean data D, and invokes the
normal training process to inject the backdoor [26]. (iii) Backdoor
′ from D
Activation: The attacker prepares a base data sample x
on which he/she wants the system to misbehave, generates the
′) and queries the trojaned model
corresponding trigger input T(x
f (·; ˜Θ) to activate the embedded backdoor function.
2.2 Model Quantization
In the past decade, model quantization emerges as one of the most
important model optimization techniques in deep learning, which
largely facilitates the deployment of the state-of-the-art DNNs on
resource-limited devices. Based on the standard implementations in
popular DL frameworks including PyTorch [4, 48] and Tensorflow
[5, 7], we present below the basic notions of model quantization and
quantization-aware training (QAT [30]), one of the most popular
quantization schemes which we exploit in our attack design.
2.2.1 Basic Concepts. Considering a DNN f (·; Θ) with H layers,
we denote the set of parameters (i.e., weights) at the i-th layer as Wi
and the intermediate outputs (i.e., activations) of the i-th layer as ai.
In the original full-precision DNN (i.e., FPNN), both the weights and
the activations take real values and are represented in FP32/FP64.
Usually, a model quantization algorithm maps the weights and the
activations of an FPNN to lower-bit numeric representation (e.g.,
INT8, INT4 or even lower [35]). Among a wide range of classi-
cal quantization schemes in signal processing [25], the following