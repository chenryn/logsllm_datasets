GPU. Parameters for convolution kernels are chosen based on parameters in AlexNet [16]. The stride and padding parameters
specify how the ﬁlter is applied to the input. All of the ﬁgures are log-log plots.
100
10
1
0.1
)
s
(
e
m
T
i
n
o
i
t
a
u
l
a
v
E
0
4
12
8
24
Input Size (millions of elements)
16
20
CPU
GPU
28
32
Fig. 2: Comparison of total protocol execution time (in a LAN
setting) on the CPU vs. the GPU for point-wise evaluation of
the private ReLU protocol on different-sized inputs.
V. RELATED WORK
Privacy-preserving machine learning is a special case of
secure computation and can be solved via general cryptographic
approaches such as secure 2-party computation (2PC) [28],
secure multiparty computation [7, 8] or fully homomorphic
encryption [44]. While powerful, these general approaches
incur signiﬁcant overhead, and much of the work in developing
concretely-efﬁcient protocols for scalable privacy-preserving
machine learning have focused on more specialized approaches
(that still rely on the general building blocks for designing
sub-protocols). We survey some of these techniques here.
Privacy-preserving inference. Many recent works have devel-
oped speciﬁc protocols for the problem of private inference for
deep learning models (c.f., [45, 1, 46, 47, 2, 48, 49, 3, 29, 50, 4,
5, 51, 30, 52, 53, 24, 54, 6] and the references therein). These
works operate in a variety of different models and architectures:
some works consider a 2-party setting (e.g., [1, 48, 49, 4]),
others consider a 3-party (e.g., [2, 3, 6, 5, 29, 30]) or a 4-
party setting (e.g., [52, 53]). Some frameworks assume that the
model is held in the clear (e.g., [49, 4]) while others (including
this work) support secret-shared models (e.g., [6, 5]). With
the recent exceptions of FALCON [6] and CRYPTFLOW [5],
these existing approaches only consider privacy-preserving
inference using shallow neural networks (e.g., less than 10
layers) on relatively small datasets (at the scale of MNIST [11]
or CIFAR [12]). Our focus in this work is designing privacy-
preserving machine learning protocols that are able to support
inference over modern deep learning models (which typically
contain tens of millions of parameters and over a hundred
layers) on large datasets (i.e., at the scale of ImageNet [13],
one of the de facto standards for state-of-the art computer
vision). As shown in Section IV-B, our system outperforms
both FALCON and CRYPTFLOW for inference over sufﬁciently-
large models and datasets.
Privacy-preserving training. Compared to private inference,
privacy-preserving training of deep neural networks is a
considerably more challenging and computationally-intensive
problem and has received comparably less attention. Of the
aforementioned works, only a few [1, 2, 29, 3, 52, 30, 53, 6]
support privacy-preserving training. Among these systems,
the only one that scales beyond MNIST/CIFAR is FALCON,
which is the ﬁrst system (to our knowledge) that supports
privacy-preserving training at the scale of (Tiny) ImageNet and
for models as large as AlexNet [16] and VGG-16 [17]. Our
work is the ﬁrst framework to leverage GPUs to demonstrate
signiﬁcantly better scalability to privately train deep networks
over large datasets.
Privacy-preserving machine learning using GPUs. Most of
the works on privacy-preserving machine learning are CPU-
based and do not leverage GPU acceleration. We discuss some
notable exceptions. Some works [55, 54] use GPUs to accelerate
homomorphic evaluation of convolutional neural networks on
MNIST. DELPHI [4] uses GPUs to compute linear layers (i.e.,
convolutions) to support private inference; however, they still
perform non-linear operations (e.g., ReLU evaluation) on the
CPU and moreover, their scheme assumes the model to be
public (and only the input is hidden). Our design philosophy in
this work is to keep all of the computations on the GPU through
a careful choice of “GPU-friendly” cryptographic protocols.
Slalom [36] shows how to integrate a trusted computing base
(e.g., Intel SGX) with GPUs to enable fast private inference of
neural networks (by ofﬂoading convolutions to the GPU and
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
121032
[7] O. Goldreich, S. Micali, and A. Wigderson, “How to play any mental
game or A completeness theorem for protocols with honest majority,” in
STOC, pp. 218–229, 1987.
[8] M. Ben-Or, S. Goldwasser, and A. Wigderson, “Completeness theorems
for non-cryptographic fault-tolerant distributed computation (extended
abstract),” in STOC, pp. 1–10, 1988.
[9] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in ACM
CCS, pp. 1310–1321, 2015.
[10] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, “Deep learning with differential privacy,” in
ACM CCS, pp. 308–318, 2016.
[11] Y. LeCun, C. Cortes, and C. J. Burges, “The MNIST database.” http:
//yann.lecun.com/exdb/mnist/.
[12] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
2009.
[13] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and F. Li, “Imagenet
large scale visual recognition challenge,” Int. J. Comput. Vis., vol. 115,
no. 3, pp. 211–252, 2015.
[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in CVPR, pp. 770–778, 2016.
[15] F.-F. Li, A. Karpathy, and J. Johnson, “Tiny ImageNet visual recognition
challenge,” 2017.
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in NeurIPS, pp. 1106–1114,
2012.
[17] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” in ICLR, 2015.
[18] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E.
Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten zip
code recognition,” Neural Comput., vol. 1, no. 4, pp. 541–551, 1989.
[19] K. Chellapilla, S. Puri, and P. Simard, “High performance convolutional
neural networks for document processing,” 2006.
[20] D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber, “Deep,
big, simple neural nets for handwritten digit recognition,” Neural Comput.,
vol. 22, no. 12, pp. 3207–3220, 2010.
[21] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K¨opf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, and S. Chintala, “PyTorch: An imperative style, high-
performance deep learning library,” in NeurIPS, pp. 8024–8035, 2019.
[22] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. J. Goodfellow,
A. Harp, G. Irving, M. Isard, Y. Jia, R. J´ozefowicz, L. Kaiser, M. Kudlur,
J. Levenberg, D. Man´e, R. Monga, S. Moore, D. G. Murray, C. Olah,
M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. A. Tucker,
V. Vanhoucke, V. Vasudevan, F. B. Vi´egas, O. Vinyals, P. Warden,
M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, “Tensorﬂow: Large-
scale machine learning on heterogeneous distributed systems,” CoRR,
vol. abs/1603.04467, 2016.
[23] “Cloud tensor processing units (tpus).” https://cloud.google.com/tpu/docs/
tpus.
performing non-linear operations within the trusted enclave).
Recent works proposing scalable private training and inference
protocols highlight the use of GPUs as an important way for
further scalability [6, 5]. Our system is the ﬁrst to support
private training and inference entirely on the GPU.
Model stealing and inversion attacks. We note that MPC
protocols can only hide the inputs to the computation (e.g., the
model or the dataset) only up to what can be inferred from the
output. Several recent works [56, 57, 58, 59, 60] have shown
how black-box access to a model (in the case of an private
inference service) can allow an adversary to learn information
about the model or even its training data. Differentially-private
training algorithms [9, 10] provide one defense against certain
types of these attacks. Our focus in this work is on protecting
the computation itself and ensure that there is no additional
leakage about the inputs other than through the output. It is
an interesting question to design a private training/inference
protocol that also provides robustness against speciﬁc classes
of model stealing/inversion attacks.
VI. CONCLUSION
In this paper, we introduce CRYPTGPU, a new MPC
framework that implements all of the cryptographic operations
(both linear and non-linear) on the GPU. CRYPTGPU is
built on top of PyTorch [21] and CRYPTEN [24] to make
it easy to use for machine learning developers and researchers.
Our experiments show that leveraging GPUs can signiﬁcantly
accelerate the private training and inference for modern deep
learning and make it practical to run privacy-preserving deep
learning at the scale of ImageNet and with complex networks.
In addition, our systematic analysis of different cryptographic
protocols provides new insights for designing “GPU-friendly”
cryptographic protocols for deep learning. This will be an
important step towards bridging the roughly 1000× gap that
still remains between private machine learning and plaintext
machine learning (on the GPU).
ACKNOWLEDGMENTS
We thank Pavel Belevich, Shubho Sengupta, and Laurens van
der Maaten for their feedback on system design and providing
helpful pointers. D. J. Wu is supported by NSF CNS-1917414.
REFERENCES
[1] P. Mohassel and Y. Zhang, “SecureML: A system for scalable privacy-
preserving machine learning,” in IEEE Symposium on Security and
Privacy, pp. 19–38, 2017.
[2] P. Mohassel and P. Rindal, “ABY3: A mixed protocol framework for
machine learning,” in ACM CCS, pp. 35–52, 2018.
[3] S. Wagh, D. Gupta, and N. Chandran, “SecureNN: 3-party secure
computation for neural network training,” Proc. Priv. Enhancing Technol.,
vol. 2019, no. 3, pp. 26–49, 2019.
[4] P. Mishra, R. Lehmkuhl, A. Srinivasan, W. Zheng, and R. A. Popa,
“Delphi: A cryptographic inference service for neural networks,” in
USENIX Security, pp. 2505–2522, 2020.
[5] N. Kumar, M. Rathee, N. Chandran, D. Gupta, A. Rastogi, and R. Sharma,
“CrypTFlow: Secure tensorﬂow inference,” in IEEE Symposium on
Security and Privacy, pp. 336–353, 2020.
[6] S. Wagh, S. Tople, F. Benhamouda, E. Kushilevitz, P. Mittal, and T. Rabin,
“FALCON: honest-majority maliciously secure framework for private
deep learning,” Proc. Priv. Enhancing Technol., vol. 2021, 2021.
[24] B. Knott, S. Venkataraman, A. Hannun, S. Sengupta, M. Ibrahim, and
L. van der Maaten, “CrypTen: Secure multi-party computation meets
machine learning,” in Proceedings of the NeurIPS Workshop on Privacy-
Preserving Machine Learning, 2020.
[25] M. Ito, A. Saito, and T. Nishizeki, “Secret sharing scheme realizing
general access structure,” Electronics and Communications in Japan
(Part III: Fundamental Electronic Science), vol. 72, no. 9, pp. 56–64,
1989.
[26] T. Araki, J. Furukawa, Y. Lindell, A. Nof, and K. Ohara, “High-throughput
semi-honest secure three-party computation with an honest majority,” in
ACM CCS, pp. 805–817, 2016.
[27] “CUDA libraries documentation.” https://docs.nvidia.com/cuda-libraries/
index.html.
[28] A. C. Yao, “How to generate and exchange secrets (extended abstract),”
in FOCS, pp. 162–167, 1986.
[29] H. Chaudhari, A. Choudhury, A. Patra, and A. Suresh, “ASTRA: high
throughput 3pc over rings with application to secure prediction,” in ACM
CCS, pp. 81–92, 2019.
[30] A. Patra and A. Suresh, “BLAZE: blazing fast privacy-preserving machine
learning,” in NDSS, 2020.
[31] S. Kamara, P. Mohassel, and M. Raykova, “Outsourcing multi-party
computation,” IACR Cryptol. ePrint Arch., vol. 2011, p. 272, 2011.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
131033
[36] F. Tram`er and D. Boneh, “Slalom: Fast, veriﬁable and private execution
2016. http://www.deeplearningbook.org.
[32] “cuBLAS.” https://docs.nvidia.com/cuda/cublas/index.html.
[33] “cuDNN.” https://docs.nvidia.com/deeplearning/cudnn/developer-guide/
index.html.
[34] D. Demmler, T. Schneider, and M. Zohner, “ABY - A framework for
efﬁcient mixed-protocol secure two-party computation,” in NDSS, 2015.
[35] D. Beaver, “Efﬁcient multiparty protocols using circuit randomization,”
in CRYPTO, pp. 420–432, 1991.
of neural networks in trusted hardware,” in ICLR, 2019.
[37] J. Furukawa, Y. Lindell, A. Nof, and O. Weinstein, “High-throughput
secure three-party computation for malicious adversaries and an honest
majority,” in EUROCRYPT, pp. 225–255, 2017.
[38] R. Canetti, “Security and composition of multiparty cryptographic
protocols,” J. Cryptol., vol. 13, no. 1, pp. 143–202, 2000.
[39] O. Goldreich, The Foundations of Cryptography - Volume 2: Basic
Applications. Cambridge University Press, 2004.
[40] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted
boltzmann machines,” in ICML, pp. 807–814, 2010.
[41] “PyTorch/CSPRNG.” https://github.com/pytorch/csprng.
[42] J. K. Salmon, M. A. Moraes, R. O. Dror, and D. E. Shaw, “Parallel
random numbers: as easy as 1, 2, 3,” in Conference on High Performance
Computing Networking, Storage and Analysis, SC, pp. 16:1–16:12, 2011.
[43] Sameer Wagh and Shruti Tople and Fabrice Benhamouda and Eyal
Kushilevitz and Prateek Mittal and Tal Rabin, “Falcon: Honest-majority
maliciously secure framework for private deep learning.” Available at
https://github.com/snwagh/falcon-public.
[44] C. Gentry, A fully homomorphic encryption scheme. PhD thesis, Stanford
University, 2009. crypto.stanford.edu/craig.
[45] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig, and
J. Wernsing, “Cryptonets: Applying neural networks to encrypted data
with high throughput and accuracy,” in ICML, pp. 201–210, 2016.
[46] J. Liu, M. Juuti, Y. Lu, and N. Asokan, “Oblivious neural network
predictions via minionn transformations,” in ACM CCS, pp. 619–631,
2017.
[47] N. Chandran, D. Gupta, A. Rastogi, R. Sharma, and S. Tripathi, “Ezpc:
Programmable, efﬁcient, and scalable secure two-party computation for
machine learning.” Cryptology ePrint Archive, Report 2017/1109, 2017.
https://eprint.iacr.org/2017/1109.
[48] M. S. Riazi, C. Weinert, O. Tkachenko, E. M. Songhori, T. Schneider,
and F. Koushanfar, “Chameleon: A hybrid secure computation framework
for machine learning applications,” in ACM CCS, pp. 707–721, 2018.
[49] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan, “GAZELLE: A
low latency framework for secure neural network inference,” in USENIX
Security Symposium, pp. 1651–1669, 2018.
[50] M. S. Riazi, M. Samragh, H. Chen, K. Laine, K. E. Lauter, and
F. Koushanfar, “XONN: xnor-based oblivious deep neural network
inference,” in USENIX Security Symposium, pp. 1501–1518, 2019.
[51] A. P. K. Dalskov, D. Escudero, and M. Keller, “Secure evaluation of
quantized neural networks,” Proc. Priv. Enhancing Technol., vol. 2020,
no. 4, pp. 355–375, 2020.
[52] M. Byali, H. Chaudhari, A. Patra, and A. Suresh, “FLASH: fast and
robust framework for privacy-preserving machine learning,” Proc. Priv.
Enhancing Technol., vol. 2020, no. 2, pp. 459–480, 2020.
[53] H. Chaudhari, R. Rachuri, and A. Suresh, “Trident: Efﬁcient 4pc
framework for privacy preserving machine learning,” in NDSS, 2020.
[54] A. A. Badawi, J. Chao, J. Lin, C. F. Mun, S. J. Jie, B. H. M. Tan, X. Nan,
A. M. M. Khin, and V. Chandrasekhar, “Towards the alexnet moment
for homomorphic encryption: HCNN, the ﬁrst homomorphic cnn on
encrypted data with gpus,” IEEE Transactions on Emerging Topics in
Computing, 2020.
[55] A. A. Badawi, B. Veeravalli, C. F. Mun, and K. M. M. Aung, “High-
performance FV somewhat homomorphic encryption on gpus: An
implementation using CUDA,” IACR Trans. Cryptogr. Hardw. Embed.
Syst., vol. 2018, no. 2, pp. 70–95, 2018.
[56] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali,
and G. Felici, “Hacking smart machines with smarter ones: How to
extract meaningful data from machine learning classiﬁers,” Int. J. Secur.
Networks, vol. 10, no. 3, pp. 137–150, 2015.
[57] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that
exploit conﬁdence information and basic countermeasures,” in ACM CCS,
pp. 1322–1333, 2015.
[58] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in USENIX Security
Symposium, pp. 601–618, 2016.
(cid:96)CE(x; y) := −(cid:88)
i∈[d]
yi log ˜zi,
(A.1)