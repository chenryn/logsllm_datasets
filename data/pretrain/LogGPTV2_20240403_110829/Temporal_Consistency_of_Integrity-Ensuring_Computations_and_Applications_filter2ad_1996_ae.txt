portant issues that have been surprisingly under-appreciated in the
security research literature, yet are crucial for correct and secure
operations in RA and other security services building upon it.
SUPPORT: This work was supported in part by (1) DHS, under sub-
contract from HRL Laboratories, (2) ARO under contract: W911NF-
16-1-0536, and (3) NSF WiFiUS Program Award #: 1702911.
REFERENCES
[1] Tigist Abera, N Asokan, Lucas Davi, Farinaz Koushanfar, Andrew Paverd, Ahmad-
Reza Sadeghi, and Gene Tsudik. 2016. Invited: Things, trouble, trust: on building
trust in IoT systems. In ACM/IEEE Design Automation Conference (DAC).
[2] Ferdinand Brasser, Brahim El Mahjoub, Ahmad-Reza Sadeghi, Christian Wachs-
mann, and Patrick Koeberl. 2015. TyTAN: tiny trust anchor for tiny devices. In
ACM/IEEE Design Automation Conference (DAC).
[3] Ferdinand Brasser, Ahmad-Reza Sadeghi, and Gene Tsudik. 2016. Remote Attes-
tation for Low-End Embedded Devices: the Prover’s Perspective. In ACM/IEEE
Design Automation Conference (DAC).
[4] Xavier Carpent, Norrathep Rattanavipanon, and Gene Tsudik. 2018. Remote
Attestation of IoT Devices via SMARM: Shuffled Measurements Against Roving
Malware. In IEEE International Symposium on Hardware Oriented Security and
Trust (HOST), 2018.
[5] Claude Castelluccia, Aurélien Francillon, Daniele Perito, and Claudio Soriente.
2009. On the Difficulty of Software-based Attestation of Embedded Devices.
In Proceedings of the 16th ACM Conference on Computer and Communications
Security (CCS).
[6] Eric Chien, Liam OMurchu, and Nicolas Falliere. 2012. W32.Duqu: The Precursor
to the Next Stuxnet. In Proceedings of the 5th USENIX Conference on Large-Scale
Exploits and Emergent Threats.
[7] Hardkernel co. Ltd. 2013. ODROID-XU4. (2013). http://www.hardkernel.com/
main/products/prdt_info.php?g_code=G143452239825
[8] Victor Costan and Srinivas Devadas. 2016. Intel SGX Explained. IACR Cryptology
[9] Boundary Devices. 2017. BD-SL-I.MX6. (2017). https://boundarydevices.com/
ePrint Archive (2016).
product/sabre-lite-imx6-sbc/
[10] Karim Eldefrawy, Norrathep Rattanavipanon, and Gene Tsudik. 2017. HYDRA:
Hybrid Design for Remote Attestation (Using a Formally Verified Microkernel).
In Proceedings of the 10th ACM Conference on Security and Privacy in Wireless and
Mobile Networks (WiSec).
[11] Karim Eldefrawy, Gene Tsudik, Aurélien Francillon, and Daniele Perito. 2012.
SMART: Secure and Minimal Architecture for (Establishing Dynamic) Root of
Trust. In Network and Distributed System Security Symposium (NDSS).
brain.shtml
[12] F-Secure. 2018. Brain Description. (2018). https://www.f-secure.com/v-descs/
[13] F-Secure. 2018. Cabanas Description. (2018). https://www.f-secure.com/v-descs/
cabanas.shtml
frodo.shtml
[14] F-Secure. 2018. Frodo Description. (2018). https://www.f-secure.com/v-descs/
[15] Trusted Computing Group. 2017. Trusted Platform Module (TPM). (2017). http:
//www.trustedcomputinggroup.org/work-groups/trusted-platform-module/
APPENDIX
A TEMPORAL CONSISTENCY SECURITY
GAME
We build upon the theoretical model of a processor architecture
and syntax from [22]. The work in [22] focuses on virus detec-
tion by constructing a scheme that interleaves secret shares of
cryptographic keys with the actual memory. This scheme requires
modifications to the instructions of the processor, in order to recon-
struct such keys and use them to ensure integrity (and thus detect
unauthorized modifications by malware) of memory content with
every read and write. Our work differs from [22], since we do not
require any modification to the underlying processor architecture,
as evident in our implementation.
A.1 System (Memory and CPU) Model
We model the prover as a random access machine RAM made up
of two components: a random access memory M, and a central
processing unit CPU. M consists of three sections:
(1) MEM– standard random access memory.
(2) ROM– read-only memory. This section of memory will store
the code for a measuring process MP.
(3) ProMEM– protected memory, that can only be written to from
instructions in ROM. This section of memory will store data
to be used by the MP in ROM.
CPU consists of registers (including input and output register) and
an instruction set. Communication between M and CPU occurs in
fetch-execute cycles, which are referred to as rounds below.
A.2 Syntax of a Consistent Integrity-Ensuring
Measurement Scheme
parameter λ.
A consistent integrity-ensuring measuring scheme (CMP) is a
tuple of algorithms (Gen, Challenge, Respond, Verify) defined as:
• Gen(λ): Generates a secret key K on input of a security
• Challenge(s): Generates a random challenge c on input of a
• Respond(M, c,K): Generates a response r to a given chal-
• Verify(c, r ,K): Outputs a bit b indicating whether r is a valid
lenge c (based on content of memory M).
seed s.
response to the challenge c.
A.3 Consistent Integrity Ensuring
Measurement Attack Game
In the following game, A is allowed to choose a piece of code (or
data) to inject into memory at any point in time. At some point in
time chosen by A, a challenge is issued. A wins if its code (or data)
is injected before the game ends, but the response to the challenge
is correct.
follows:
Recall that, in Section 2, we described a typical RA scheme as
(1) Vrf sends a challenge-bearing attestation request to Prv at
(2) Prv receives it at time tpr
(3) Computation of MP starts at time tcs
time tvs
Table 2: Notation
A
C
ρinit
ρinser t
ρat t est
v
MP
The adversary
The challenger
# rounds at beginning of security game
(before issuing challenge)
# rounds before A’s code is injected
# rounds after issuing the challenge
Code that A injects into MEM
Integrity-ensuring measurement function
that runs Respond algorithm.
(4) Computation of MP ends at time tce
(5) Prv sends the attestation report to Vrf at time tps
(6) Vrf receives it at time tvr
The formal security game of CMP is defined in terms of rounds,
where if tvs = tpr = tcs, they would all correspond to the instant at
the end of the rounds ρinit when the challenge is issued. The end
of ρattest corresponds to time when computation of the integrity
ensuring function ends at: tce = tps = tvr.
Definition 4. We say that a consistent integrity-ensuring measur-
ing scheme (CMP) is secure if a non-empty piece of code is inserted
before the attack game terminates, and:
Pr (b = 1) ≤ µ(λ)
where µ(λ) is a negligible function.
Figure 13 contains the definition of the security game for a con-
sistent integrity-ensuring measuring scheme (CMP).
Shared by A and C: random access machine RAM = (M, CPU),
program W , integrity ensuring measurement function MP (e.g., an
HMAC), security parameter λ, and consistent integrity-ensuring
(1) A chooses the following and provides them to C:
measurement function CMP.
• Inputs: x = x1|| . . . ||xi for RAM.
• Values: ρinit , ρinser t and ρat t est , all polynomial in λ.
• Code v to be injected into MEM, and memory location i to
insert it (and optionally a list of other memory locations v
should be moved to at subsequent rounds after insertion
at ρinser t ).
(2) C runs Gen(λ) to generate setup parameters.
(3) C simulates ρinit rounds of execution. If round ρinser t is
reached, v is inserted into MEM at the beginning of that round.
If program halts, go to step 4.
(4) C initiates CMP by generating a challenge c by invoking
Challenge and writing it to the input register. C invokes ROM
which contain executable code of MP. C simulates execution
of ρat t est rounds. If round ρinser t is reached, v is inserted
into MEM at the beginning of that round. If program halts,
proceed to step 5.
(5) C interprets data in output register as r, a response to its chal-
lenge, and outputs bit b, which is the result of Verify(c, r, K).
Figure 13: CMP Security Game
B SECURITY ARGUMENTS &
CONSIDERATIONS
We consider two approaches: Dec-Lock and All-Lock, and sketch
out corresponding security proofs. Security of remaining ap-
proaches is quite similar. For the purpose of this section, our instanti-
ations of Dec-Lock and All-Lock is within the HYDRA architecture.
Proof sketches are only valid for these specific instantiations since
they rely on features ensured by HYDRA. The required (memory
isolation and access control) features are instantiated in HYDRA
using seL4 which is formally verified. HYDRA uses a secure HMAC
as the MP.
B.1 Preliminaries and Assumptions
We capture HYDRA features by the following assumptions:
(1) Assumption-1 (memory access control): memory regions
locked, or configured as read-only, cannot be written to by
any process.
(2) Assumption-2 (memory isolation): each process, except the
(3) Assumption-3 (MP is secure): A secure HMAC is used to
attestation one, can only access its own memory space.
implement MP.
B.2 Proof Sketch for Dec-Lock
Considering the security game in Figure 13, there are two cases:
(1) A supplied ρinser t ≤ ρattest
(2) A supplied ρinser t > ρattest
The first case is the same as in Dec-Lock.
In the second case, since ρinser t > ρattest and, at ρattest , all
memory is locked, by Assumption-1 insertion of v into location i
(1) A supplied ρinser t ≤ ρattest
(2) A supplied ρinser t > ρattest
The first case is trivial, since there is no memory modification after
attestation starts, i.e., temporal consistency follows by construction
of the case. If everything works as expected, MP computes r on
MEM and Verify(c, r ,K) should fail, i.e., b = 0. b would be 0 because
v is now in MEM before MP starts. Thus, the value of r will indicate
that; otherwise, MP is insecure, which contradicts Assumption-
3. Computation, intermediate and final results of MP cannot be
directly affected, since this would violate Assumption-2.
The second case is more subtle. Recall that, in Dec-Lock, entire
memory is locked at tvs = tpr = tcs = ρinit , and incrementally
unlocked as computation of MP proceeds. Assume that memory
location i is unlocked after it is processed in round ρattest + j,
i.e., one memory location is processed per round after attestation
starts. If memory location i, where v is to be inserted, is still locked
during ρinser t , i.e., if ρattest  t0, malware intercepts Vrf’s attestation request,
e.g., by modifying the interrupt handler for the network
device driver. It then sets an interrupt timer for t2 and invokes
MP.
• MP runs without interruption from t1 to t2.
• At t2 > t1, malware interrupts MP. It then copies itself to
the part of memory that was already measured, erases itself
from its prior location, and resumes execution of MP.
• At time t4, MP completes and produces the measurement
for delivery to Vrf.
Throughout this process (t1 → t4) malware is never covered by
MP. It thus successfully escapes detection, since the measurement
reflects a malware-free state.
E IS MIGRATORY MALWARE REALISTIC?
The stance taken in this paper is proactive in nature. One of the
goals is a technique that prevents migratory malware from escap-
ing detection (i.e., subverting attestation) on low-end embedded
systems. Thus far, there have been no public reports of migratory
malware. Nonetheless, we believe that it is realistic and not far-
fetched, especially, on low-end embedded systems that involve
8If program memory is insufficient to contain both existing firmware and malware,
the latter can use the executable compression technique [40] to reduce the sizes of
both firmware and itself.
Figure 14: Program memory of infected Prv before (at t1), during (at t2, t3) and after (at t4) the measurement process.
applications running on “bare metal" and even those capable of
supporting a rudimentary microkernel.
In a more traditional computing setting (e.g., PCs, laptops, tablets,
and smartphones) anticipated migratory malware resembles the
behavior of stealthy viruses [30] that employ various evasion tech-
niques to conceal their existence during a virus scan. Typical eva-
sion techniques involve an operating system and rely on intercep-
tion of system calls as well as manipulation of returned data. For
example, [13] conceals the size of infected files by returning the
original size when the DIR command is invoked. Another example
is [12, 14, 24] that redirect all access to an infected file to an area
storing the original file.
In principle, stealthy malware might also hide its presence by
moving itself into an area that has already been covered by a virus
scanner, similar to our migratory malware. We believe that this
is quite plausible in embedded systems, where a memory migra-
tion cannot be detected in software, without using some kind of a
temporal consistency mechanism.