1995).  N-version  Programming  allows  multiple  functionally  equivalent  programs  to  be 
independently  generated  from  the  same  initial  specifications,  and  the  functionally  equivalent 
27 
cloud components are invoked in parallel and the final result is determined by majority voting 
(Z. B. Zheng, et al., 2010). For Parallel, the multiple functional equivalent components will be 
invoked in parallel and the first returned response will be used as the final result (Z. B. Zheng, 
et al., 2010). In particular, it is recommended that not all the components within a cloud system 
need to be equipped with fault-tolerance mechanism. For example, FTCloud (Z. B. Zheng, et al., 
2010) works as a component ranking based fault-tolerance framework for cloud applications. It 
was  proposed  by  the  researchers  in  the  Chinese  University  of  Hong  Kong  (CUHK).  It  is  a 
component ranking based framework for building fault-tolerant cloud applications (Z. B. Zheng, 
et  al.,  2010).  And  it  employs  the  component  invocation  structures  and  the  invocation 
frequencies  to  identify  the  significant  components  in  a  cloud  application,  and  optimal  fault 
tolerance  strategies  are  automatically  determined  for  these  significant  components  by  using  a 
proposed algorithm (Z. B. Zheng, et al., 2010). Below Fig. 6 shows the system architecture of 
FTCloud.  The  first  phase  ranks  the  significant  components,  which  consists  of  three  steps:  1. 
Components Graph Building; 2. Component Ranking; 3. Significant Component Determination. 
The  next  step  is  to  select  the  optimal  fault  tolerance  strategy  for  each  of  these  significant 
components.  Basically,  in  FTCloud  there  are  three  types  of  fault  tolerance  strategies:  A. 
Recovery  Block  (RB);  B.  N-Version  Programming  (NVP);  C.  Parallel  (Z.  B.  Zheng,  et  al., 
2010).  The  strategy  selection  will  be  conducted  based  on  failure  rates  calculation  plus  the 
comparison among them as well as the configurations of each component.  
Fig. 6.  System Architecture of FTCloud. 
FTCloud  first  employs  the  component  invocation  structures  and  invocation  frequencies  to 
identify the significant components in a cloud application, and then the optimal fault-tolerance 
strategy (RB, NVP, or Parallel) will be automatically selected for each significant component. 
By  doing  so,  the  reliability  and  efficiency  of  fault-tolerant  cloud  applications  have  been 
improved. In fact, FTCloud can be currently used to tolerate crash and value faults only, but we 
need it to support more varieties of system faults and anomalies because in cloud systems there 
could arise more kinds of errors and failures other than just only crash and value faults. And the 
future work for FTCloud is to make it able to cater for more types of faults (Z. B. Zheng, et al., 
2010). 
28 
Particularly,  fault  tolerance  design  in  cloud  computing  could  also  be  based  on  replication  and 
resubmission of tasks (S. K. Jayadivya, et al., 2012). But there are several problems with task 
replication and resubmission, and these problems include state consistency and data restoration. 
Both of these two problems are the biggest challenges for task management. The fault-tolerance 
strategy  is  designed  during  the  cloud  system  design  phase.  Fault-tolerance  itself  will  be 
triggered (e.g. backup component switching) during the cloud system runtime phase (B. Randell, 
et al., 1995). In general, the fault tolerance mechanism has been widely used in  different types 
of cloud applications and service-oriented systems as a recovery strategy, and such applications 
and  systems  include  BPEL  (Business  Process  Execution  Language)  processes  in  parallel  (E. 
Juhnke,  et  al.,  2009),  and  MODISAzure  (J.  Li,  et  al.,  2010).  The  challenge  behind  the  fault-
tolerance  recovery  mechanism  is  how  to  further  reduce  the  service  downtime  since  downtime 
could happen during the switching between two sites. 
Limitations:  One limitation with the fault-tolerance mechanism is that the  errors occurring in 
the primary node still remain there because they are just tolerated rather than being fixed, and 
the  secondary  node  just  masks  the  errors  by  taking  over  the  service  of  the  primary  node. 
Another limitation is that the errors in the primary node are not guaranteed to be able to be fixed 
after failing over to the secondary node. If it takes long for the errors in the primary node to be 
fixed,  then  the  secondary  node  will  provide  service  by  itself  for  a  long  time.  This  can  be 
dangerous, because if errors also occur in the secondary node at this time, the secondary node 
will stop service and there would be no other available node to fail over to, so the whole system 
would  fail.  In  fact,  in  cloud  systems  it  is  not  always  necessary  to  first  fix  the  errors  in  the 
primary node and then switch back to it from the secondary node, because a new healthy virtual 
node (cloud instance) can be conveniently and dynamically created to replace the primary node 
by using the on-demand nature of cloud, and importantly the new instance creation time may be 
less  than  the  error  fixing  time  on  the  primary  node.  Hence,  the  third  limitation  of  the  fault-
tolerance  mechanism  is  that  it  does  not  fully  take  advantage  of  cloud  on-demand  nature. 
Actually, during sporadic operations on cloud the fault-tolerance mechanisms of cloud systems 
might be turned off in order not to disturb the sporadic operations, which puts more danger onto 
the cloud systems. 
2.2.5  Recovery for Cloud Internal Protocols 
For cloud internal protocols, such as the internal processes of HDFS (H. S. Gunawi, et al., 2011), 
recoverability  is  enabled  by  specifying  recovery  specifications.  An  existing  typical  recovery 
method for cloud internal protocols is  called FATE&DESTINI (H. S. Gunawi, et al., 2011), a 
framework  for  cloud  recovery  testing.  With  FATE&DESTINI,  recovery  actions  are  specified 
clearly,  concisely  and  precisely  by  using  datalog  rules  (H.  S.  Gunawi,  et  al.,  2011)  in  the 
29 
domain  of  cloud  internal  protocols.  For  example,  a  datalog  rule  inside  the  recovery  protocol 
would  ask  the  service  to  create  a  missing  file  on  a  certain  node.  The  recovery  action  is 
comprised of a series of datalog rules that composite the logic of recovery. FATE&DESTINI is 
intended for the design phase of cloud internal protocols which can be part of cloud systems or 
cloud  normal  activities.  Since  datalog  rules  cannot  be  executed  without  being  translated  into 
runnable  code,  FATE&DESTINI  is  not  intended  for  the  runtime  phase  of  cloud  internal 
protocols  unless  the  datalog  rules  are  translated  into  real  executable  code.  Overall,  recovery 
mechanisms for cloud internal protocols such as FATE/DESTINI look at the process implied by 
a  cloud  system’s  internal  protocols  and  rely  on  the  built-in  recovery  protocol  to  detect  and 
recover from bugs (H. S. Gunawi, et al., 2011; M. Fu, et al., 2013). 
Limitations: One limitation with recovery mechanisms for cloud internal protocols is that they 
are specific to certain cloud applications that highly depend on cloud internal protocols. While 
in fact not all cloud applications have to depend on cloud internal protocols, recovery for cloud 
internal protocols is not generalizable enough. Another drawback of FATE&DESTINI is that it 
has  a  strong  assumption  on  the  visibility  and  control  level  provided  by  cloud  (M.  Fu,  et  al., 
2013),  while  in  fact  cloud  platforms  only  provide  consumers  with  very  limited  visibility  and 
indirect  control  (AWS,  2016).  Moreover,  recovery  for  cloud  internal  protocols  requires  the 
expert knowledge from the operators who specify the recovery logic, which can sometimes be a 
problem  because  the  manually  determined  recovery  specifications  might  not  be  fully  correct. 
This is a limitation as well. 
2.2.6  Test Driven Scripts in Cloud Operations 
DevOps  scripts  such  as  Chef  (OpsCode,  2016;  S.  Nelson-Smith,  2011)  can  be  used  for 
implementing consumer-initiated sporadic operations on cloud, such as continuous  installation 
or upgrade. In order for the scripts to be more reliable, operation scripts can be written in a test 
driven manner (S. Nelson-Smith, 2011). In scripts like Chef, there are several ways to make test 
driven  infrastructure,  for  instance,  by  using  scripts  mini  tests  (S.  Nelson-Smith,  2011).  Mini 
tests are usually utilized to test the functionality and availability of a module in the whole script 
infrastructure.  For  example,  a  mini  test for  a  module of  shutting  down Tomcat  service  can  be 
conducted  to  check  if  Tomcat  service  will  really  be  shut  down  successfully.  Additionally,  by 
using  mini  test  the  errors  in  scripts  could  be  detected  and  fixed  in  a  timely  fashion  by  the 
operator.  Mini  tests  are  performed  in  the  test  bed  and  are  integrated  into  the  structure  of  the 
scripts during the scripts design phase (M. Fu, et al., 2014).  
Limitations: The problem behind the test driven mechanism in operational scripts is that mini 
tests are conducted in the test bed for the operations instead of on the operations’ real execution 
30 
platform,  which  means  that  the  errors  which  do  not  occur  during  the  mini  tests  might  be 
occurring during the real execution time of cloud operations (M. Fu, et al., 2014; M. Fu, et al., 
2015;  M.  Fu,  et  al.,  2016).  If  errors  occur  during  the  execution  of  operations,  there  is  no 
recovery because the mini tests are not feasible for runtime recovery. With mini tests, it is true 
that the errors can be early detected and fixed in the testing phase, but even if the errors arising 
in the test bed can be recovered from during the testing phase, it cannot guarantee a successful 
runtime  execution  of  the  operational  scripts  since  the  test  bed  is  different  from  the  actual  run 
bed of cloud operations (M. Fu, et al., 2014; M. Fu, et al., 2015; M. Fu, et al., 2016). So this is a 
limitation for the mini tests mechanism. Moreover, the test driven scripts mechanism is intrusive 
because  it  changes  the  code  of  the  operational  scripts  by  adding  the  testing  snippets,  and  for 
different  cloud  operations  this  mechanism  has  different  operational  contexts.  Hence,  another 
limitation with the test driven scripts mechanism is that it is usually specific to particular cloud 
operations and has poor generalizability. In addition, by applying the test driven mechanism on 
implementing  operational  scripts  it  will also  introduce  the overhead  of implementing  the  mini 
tests scripts and running the mini tests, and this is also a limitation (M. Fu, et al., 2014). 
2.2.7  Cloud Operations Exception Handling 
One way to handle operations errors at runtime is by using exception handling (OpsCode, 2016; 
Asgard, 2013). For example, Chef uses exception handlers to implement error handling logics. 
In  Asgard  (Asgard,  2013),  one  exception  handling  technique  is  to  gracefully  exit  from  the 
operation  in  the  face  of  errors.  Although  exception  handling  is  triggered  during  the  runtime 
phase of scripts  or  operations,  the  detailed logic in  exception  handling  has  to  be  implemented 
during  the  scripts  and  operation  design  phase.  One  of  the  biggest  challenges  about  cloud 
exception handling, according to the existing work related to cloud dependability (X. Xu, et al., 
2014),  is  that  exception  handling  in cloud  should  deal  with cross-platform  and cross-language 
exceptions in a unified manner. Another challenge of exception handling is that it usually only 
has  access  to  the  local  information  within  “try-catch”  blocks  and  has  no  access  to  the  global 
information about the whole operational context and environment (H. Chang, et al., 2013). 
Limitations:  In  large  scale  cloud  systems,  there  are  different  types  of  resources  (e.g.  cloud 
instances with different operating systems and platforms, cloud storages with different types and 
configurations,  etc.),  and  the  exceptions  thrown  during  the  operations  on  these  different 
resources across the cloud systems should be caught and handled by the exception handlers in a 
unified  way  (X.  Xu,  et  al.,  2014;  M.  Fu,  et  al.,  2014).  Handling  exceptions  from  sporadic 
operations in a unified way is more difficult as there are always many long-running tasks inside 
the  sporadic operations  and  the  operations  rely  on  different  third-party  systems  (X. Xu, et al., 
2014; M. Fu, et al., 2014). Also, exception handling is faced with the problem of limited local 
31 
visibility of the exception situation (M. Fu, et al., 2016). Exception handling only deals with the 
local errors within the “try-catch” blocks, and the remote errors outside the  “try-catch” blocks 
cannot  be  detected  and  handled.  Hence,  this  could  lead  to  a  phenomenon  that  the  errors  that 
occur  to  the  resources  manipulated  or  generated  by  the  previous  steps  before  the  current  step 
cannot be detected (H. Chang, et al., 2013). Another problem with exception handling is that it 
is  difficult  for  exception  handling  to  recover  for  different  types  of  operations.  In  other  words, 
the exception handling mechanism has poor generalizability for recovery (M. Fu, et al., 2016). 
This is because exception handling is implemented within the source code of the operations  in 
an  intrusive  way  and  they  are  usually  specific  to  the  context  and  scope  of  that  particular 
operation only. When errors occur in another type of sporadic operation, the exception handling 
has  to  be  re-implemented  according  to  the  context  and  characteristics  of  that  new  sporadic 
operation.  What’s  more,  since  exception  handling 
is 
intrusively 
implemented, 
its 
implementation  assumes  that  the  source  code  of  operations  is  known  to  the  users.  This 
assumption could be too strong even though many operations on cloud have public open source 
code.  Hence,  this  is  also  a  limitation.  When  applying  exception  handling  on  the  recovery  for 
sporadic  operations  on  cloud,  another  limitation  is  that  sometimes  it  is  difficult  for  exception 
handlers  to  catch  the  errors  in  sporadic  operations  on  cloud,  because  the  error  responses 
sometimes will not be well-defined exceptions but range from the error codes of cloud API calls 
to the potential silent failure of a configuration change, and these types of errors are difficult to 
be captured by exception handlers. With the afore-mentioned limitations and drawbacks in the 
exception  handling  mechanism,  exception  handling  is  not  regarded  as  a  fine-grained  recovery 
mechanism for sporadic operations on cloud.  
2.2.8  Recovery for Cloud Operations as Transactions 
Recovery  strategies  for  transactions  such  as  long  running  transactions  (C.  Colombo  and  G.  J. 
Pace,  2011)  usually  involve  backward  recovery  and  forward  recovery  (C.  Colombo  and  G.  J. 
Pace, 2011). Backward recovery refers to the strategy which first reverts the current erroneous 
state  to  a  previous  correct  state  before  attempting  to  continue  execution.  Forward  recovery 
attempts to correct the current erroneous state and then continue normal execution. One form of 
backward recovery is called rewind & replay (A. B. Brown and D. A. Patterson, 2002), which 
makes the system go back to the previous consistent state and replay the step in the transaction. 
One form of forward recovery in long running transactions is called compensation (C. Colombo 
and  G.  J.  Pace,  2011),  which  means  to  attempt  to  correct  the  state  of  a  system  given  some 
knowledge  of  the  previous  actions  of  the  system  (C.  Colombo  and  G.  J.  Pace,  2011).  Unlike 
traditional  transactions  (e.g.  web  service  transactions)  which  are  atomic  and  achieve  “all-or-
nothing” execution results (B. Limthanmaphon and Y. Zhang, 2004), long running transactions 
32 
cannot afford a simple overall rollback which makes the long running transaction go back to the 
very first state before the transaction starts, because it is not always necessary to start from the 
very  beginning  of  the  long  transaction  in  case  of  failure.  If  we  treat  sporadic  operations  as 
transactions, then the recovery mechanisms within long running transactions can to some extent 
be applied on sporadic operations, e.g. the backward recovery and the forward recovery. These 
recovery  strategies  are  designed  and  implemented  in  the  design  phase  of  operations,  and  take 
effect  during  the  operations  runtime  when  failure  occurs.  The  most  challenging  part  in  this 
recovery strategy is cloud system state reachability checking (M. Fu, 2014; M. Fu, et al., 2016) 
as well as making checkpointing more efficient. 
Limitations:  First,  there  are  only  two  types  of  recovery  mechanisms  (forward  recovery  and 
backward recovery) in long running transactions recovery, and they are not guaranteed to satisfy 
recovery  objectives  such  as  RTO  (Recovery  Time  Objective)  and  RPO  (Recovery  Point 
Objective).  For  example,  the  backward  recovery  just  brings  the  system  to  the  previous 
consistent state, but the system still needs to go to the state after the step where failure presented, 
so  RPO  is  not  well  achieved  by  the  backward  recovery.  Even  for  the  forward  recovery,  the 
recovery time might be too long and it might exceed the time boundary defined by the RTO. In 
addition, it is possible that for certain failure scenarios both of these two recovery mechanisms 
are not able to recover for the sporadic operations on cloud, especially when it comes to the fact 
that the cloud platforms only provide us with limited visibility and indirect control (AWS, 2016). 
Hence,  more  recovery  patterns  should  to  be  introduced  in  order  to  cater  for  different  types  of 
failure scenarios in sporadic operations. Second, these two recovery strategies for long running 
transactions ignore a subtle fact that the alternatives for the failed  steps in the transactions can 
exist, and apart from re-executing the failed steps themselves, the alternatives of the failed steps 
should also be tried. For one thing, the steps’ alternatives yield the same execution results as the 
steps  themselves;  for  another  thing,  since  the  original  steps  themselves  already  fail,  by 
executing their alternatives we may avoid the possibility that the same steps fail again as before. 
We  may  want  to  explore  the  feasibility  of  such  “alternatives-oriented”  recovery  methods  for 
cloud  sporadic  operations.  There  are  some  other  limitations  with  long  running  transactions 
recovery: 1) it is hard to generalize it to cater for different types of sporadic operations, because 
the recovery in long running transactions is usually application specific (M. Fu, et al., 2016); 2) 
long  running  transactions  recovery  needs  to  capture  the  full  checkpoints  which  represent  the 
complete state of the system, which is time-consuming, inefficient, and not suitable for sporadic 
operations which only operate on a subset of the overall state of systems (C. Colombo and G. J. 
Pace,  2011).  There  is  a  trade-off  between  making  full  checkpoints  and  making  partial 
checkpoints. Making full checkpoints can expose the recovery method to a complete knowledge 
of  the  system  state  and  hence  can  make  recovery  more  complete  as  well,  for  example,  errors 
33 
occurring with normal activities can also be recovered from. However, this is time-consuming, 
especially  when  the  cloud  system  is  in  large  scale.  Also,  considering  that  sporadic  operations 
only  need  to  interact  with  a  subset  of  the  whole  resources  of  the  cloud  system  and  that  the 
normal activities during sporadic operations can be catered for by existing recovery mechanisms 
(such as Fault-tolerance), we only want to capture the states of the cloud resource subset that are 
required and manipulated by the sporadic operation. 
2.2.9  Cloud Operations Undo Framework 
Previous research on undo of cloud operations (I. Weber, et al., 2012; I. Weber, et al., 2013; S. 
Satyal, et al., 2015) successfully employed artificial intelligence (AI) planning to move a cloud 
system from a given state to an earlier consistent state, in order to undo undesired changes. The 
AI planner used is called Fast-Forward (FF) in the variant (J. Hoffmann, et al., 2012). It requires 
three types of inputs: an initial state, a goal state, and a set of action templates referred to as the 
planning  domain.  The  actions  in  the  planning  domain  are  specified  in  the  Planning  Domain 
Definition  Language  (PDDL)  (J.  Hoffmann,  et  al.,  2012).  Each  action’s  specification  is 
comprised  of  its  preconditions,  its  parameters,  and  its  effects.  From  these  inputs,  the  planner 
generates a state transition plan to transit from the initial state to the goal state. Actually, there 
could  be  two  or  more  available  state  transition  plans  for  a  particular  state  transition  scenario, 
and these available state transition plans may have a different number of steps. In this case, the 
AI planner selects the particular plan with the fewest steps, and hence the AI planner will return 
the state transition  plan with the shortest path (J. Hoffmann, et al., 2012). The generated state 
transition plans are also called undo plans and they are represented in scripts. The scripts cannot 
be  executable  and  they  need  to  be  mapped  with  executable  code  such  as  command  lines  or 
Java/C# cloud SDK libraries for execution. When undoing an operation, it actually makes state 
transition  from  current  system  state  (the  initial  state)  to  the  previous  captured  consistent  state 
(the  goal  state).  The  undo  framework  also  checks  whether  a  goal  state  is  reachable  from  an 
initial  state.  It  makes  state  reachability  checking  by  investigating  on  whether  there  exists  an 
available state transition plan to transit from the initial state to the goal state. If the goal state is 
unreachable, then no undo plan will be generated and the cloud operator will be notified of this 
situation. The main challenge in the undo framework is how to achieve better efficiency of undo 
plan generation, and this challenge might be able to be addressed by improving the functions in 
the current AI planning tools like the FF planner in the variant, or by defining the correct scope 
of actions in the PDDL file so that the planner will not unnecessarily traverse over the actions 
that have nothing to do with the specific state transition context. Yet sometimes how to properly 
determine  the  correct  scope  of  actions  for  the  PDDL  file  is  not  straightforward  and  hence 
challenging as well.  
34 
Limitations: When applying the cloud operations undo framework on the recovery for sporadic 
operations  on  cloud,  one  limitation  with  it  is  that  it  currently  does  not  support  certain  cloud 
resources  such  as  the  auto  scaling  group  and  the  elastic  load  balancer  (M.  Fu,  et  al.,  2016). 
Hence,  the  sporadic  operations  on  cloud  requiring  and  manipulating  such  cloud  resources 
cannot rely on this existing undo framework for undoing a certain action. In addition, another 
limitation  with  the  cloud  operations  undo  tool  is  that  it  has  a  relatively  low  efficiency  of 
capturing cloud system resource states because it captures the states of all the possible resources 
provided by the cloud platform, no matter whether the sporadic operation needs to manipulate 
them or not (M. Fu, et al., 2016). In fact, a sporadic operation on cloud may only  require and 
manipulate  a  subset  of  all  cloud  resources  provided  by  the  cloud  platform,  so  state  capturing 
only  needs  to  capture  the  states  of  the  cloud  resource  subset  that  the  sporadic  operation  is 
concerned with in order to achieve higher efficiency of state capturing (M. Fu, et al., 2016). 
2.2.10  User Guided Recovery for Cloud Web Service Applications 
There are  several  existing  recovery  strategies  which are  designed  and implemented to recover 
from  failures  in  an  automated  way,  and  they  have  been  widely  used  in  many  different  cloud 
systems. In fact, however, for complex cloud systems it is difficult to always employ automated 
recovery,  especially  when  the  cloud  systems  are  comprised  of  large  number  of  different 
components across different platforms or regions or even different clouds. As such, cloud users 
may be required to be involved in the recovery for this kind of cloud applications (A. B. Brown, 
et al., 2004). In 2010, a user guided recovery framework for cloud web service applications was 
introduced (J. Simmonds, et al., 2010). Specifically, in this user guided recovery approach the 
recovery  plan  generation  is  first  conducted  when  behaviour  correctness  properties  of  a  cloud 
web service application are violated at runtime (J. Simmonds, et al., 2010). After obtaining the 
generated recovery plans, the plans will be ranked based on recovery point objective (RPO) and 
recovery  time  objective  (RTO)  (J.  Simmonds,  et  al., 2010).  And  then the  user  selects  the  best 
recovery plan for execution. Since the user is involved in the selection of the best recovery plan, 