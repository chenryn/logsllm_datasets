Workshop, 1990.
[12] S. Komanduri, R. Shay, P. G. Kelley, M. L. Mazurek,
L. Bauer, N. Christin, L. F. Cranor, and S. Egelman. Of
passwords and people: Measuring the effect of password-
composition policies. In CHI 2011: Conference on Human
Factors in Computing Systems, 2011.
[13] C. Kuo, S. Romanosky, and L. F. Cranor. Human selection of
mnemonic phrase-based passwords. In Proc. Symposium on
Usable Privacy and Security (SOUPS), pages 67–78, 2006.
[14] C. D. Manning and H. Sch¨utze. Foundations of statistical
natural language processing. MIT Press, Cambridge, MA,
USA, 1999.
[15] S. Marechal. Advances in password cracking. Journal in
Computer Virology, 4(1):73–81, 2008.
[16] J. Massey. Guessing and entropy. In IEEE International
Symposium on Information Theory, page 204, 1994.
[17] Microsoft password strength meter.
Online
at
https://www.microsoft.com/security/
pc-security/password-checker.aspx.
[18] R. Morris and K. Thompson. Password security: a case
history. Communications. ACM, 22(11):594 – 597, 1979.
[19] A. Narayanan and V. Shmatikov. Fast dictionary attacks on
passwords using time-space tradeoff. In CCS ’05: Proceed-
ings of the 12th ACM conference on Computer and communi-
cations security, pages 364–372, New York, NY, USA, 2005.
ACM.
[20] The password meter.
Online
at http://www.
passwordmeter.com/.
[21] B. Pinkas and T. Sander. Securing passwords against dictio-
nary attacks. In Proc. CCS ’02, pages 161–170, 2002.
[22] S. Riley. Password security: What users know and what they
actually do. Usability News, 8(1), 2006.
[23] S. Schechter, C. Herley, and M. Mitzenmacher. Popular-
ity is everything: a new approach to protecting passwords
from statistical-guessing attacks. In Proceedings of the 5th
USENIX conference on Hot topics in security, pages 1–8.
USENIX Association, 2010.
[24] E. H. Spafford. Observing reusable password choices. In
Proceedings of the 3rd Security Symposium, pages 299–312.
USENIX, 1992.
[25] SpiderMonkey. Online at http://search.cpan.org/
˜mschilli/JavaScript-SpiderMonkey.
[26] J. M. Stanton, K. R. Stam, P. Mastrangelo, and J. Jolton.
Analysis of end user security behaviors. Comp. & Security,
24(2):124–133, 2005.
[27] A. Vance.
If your password is 123456,
just
New York Times, oneline at
make it hackme.
http://www.nytimes.com/2010/01/21/
technology/21password.html,
2011, January 2010.
retrieved May
[28] M. Weir, S. Aggarwal, M. Collins, and H. Stern. Testing
metrics for password creation policies by attacking large sets
In Proceedings of the 17th ACM
of revealed passwords.
conference on Computer and communications security (CCS
2010), pages 162–175. ACM, 2010.
[29] M. Weir, S. Aggarwal, B. de Medeiros, and B. Glodek. Pass-
word cracking using probabilistic context-free grammars. In
IEEE Symposium on Security and Privacy, pages 391–405.
IEEE Computer Society, 2009.
[30] J. Yan, A. Blackwell, R. Anderson, and A. Grant. Password
memorability and security: Empirical results. IEEE Security
and Privacy Magazine, 2(5):25–31, 2004.
[31] J. J. Yan. A note on proactive password checking. In Proc.
NSPW ’01, pages 127–135. ACM, 2001.
[32] J. J. Yan, A. F. Blackwell, and R. J. Anderson. Password
memorability and security: Empirical results. IEEE Security
& Privacy, 2:25–31, 2004.
[33] Y. Yu. On the entropy of compound distributions on non-
negative integers. IEEE Trans. Inf. Theor., 55:3645–3650,
2009.
A Details of the Leakage Estimation
To prove the security of the scheme we prove that the
total amount of information (in terms of Shannon entropy)
is limited.
In this section we use the following variable
names: n denotes the length of the n-grams, N = |Σ|n
is the total number of n-grams, l the number of n-grams
per password, k the number of passwords in the database,
= k · l the total number of N-grams in the database, γ is
m def
the probability of adding noise used in the construction, and
L is the information leaked from the database.
First, we consider the leakage of an individual n-gram
with index j (where 1 ≤ j ≤ N) with expected frequency tj,
and we consider n-grams at the i-th position of the passwords
only (thus 1 ≤ i ≤ l). Deﬁne the random variables Si,j
h :=
h) is the indicator function for the j-
χj(P i
th ngram on the i-th position in the h-th password. The
random variables Si,j
h = 1) = tj and
i,j = 0) = 1 − tj.
P r(Sh
For 1 ≤ i ≤ l and 1 ≤ j ≤ N, we deﬁne the empirical
(cid:80)
frequency obtained from a password database with k pass-
words as T i,j
follows a binomial
distribution Bin(k, tj) with k trials with success probability
tj each.
Let Ri,j ∼ Bin(k, γ) be random variables describing the
noise added to the i-th n-gram in our construction, where
h all have P r(Si,j
h), where χj(P i
k = 1
k
h=1,...,k Si,j
h . T i,j
k
(cid:33)
Finally, from these estimations, from remark (*), and
Equations (15) and (15) we get
(cid:32)
l(cid:88)
i=1
L ≤ N(cid:88)
(∗)≤
j=1
l
2γ
γ +
(cid:32)
(cid:18)
tj
2γ
+
C (γ+tj )/2
4
2k
− C γ
1
k
2
k2 − C γ
− C γ
(cid:33)
3
k3
+ lk ·
γ +
C (γ+tj )/2
4
− C γ
1
k
k2 − C γ
− C γ
3
k3
2
2k
(cid:19)
≤ 4kγ +
1
γ
6.1 +
1
5kγ
1
for l = 4. For a reasonable choice of parameters γ =
1(cid:48)000(cid:48)000 and k = 5(cid:48)000(cid:48)000 this means that overall, the
n-gram database can leak at most 6.2 million bits, or on
“average” about 1.3 bits per password.
B Interval Estimation for Password Frequen-
cies
As explained in Section 2.1, given the true probability
of each password, we can easily build an ideal password
meter by applying any monotonically increasing function
to these probabilities. Furthermore, even if we only have
an estimation of these probabilities the we could build an
ideal password checker, as long as the relative ranking of the
passwords remains unchanged.
Unfortunately, we do not in general the true probabil-
ity of a password thus rendering this approach impractical.
However, there exist certain classes of passwords for which
it is possible to give a good probability estimation that can
be used as a ground truth for measuring the accuracy of our
construction in measuring password strength.
One such class is the most common passwords in the
RockYou dataset. Intuitively, with a large dataset of 32.6
million passwords, we should be able to obtain a good prob-
ability estimation of, at least, the most common passwords.
This probability estimation could then be used to be an ideal
password meter for these common passwords. For example,
the most common password in the dataset (123456) has
been chosen 290729 times, while the second most common
(12345) has been chosen 79076.
1 ≤ i ≤ l ranges over the n-grams per password, and 1 ≤
j ≤ N ranges over all n-grams, and let Oi,j
k + Ri,j
be the observed noisy value. The information that can leak by
publishing a single noisy n-grams is the mutual information
between T i,j
k
and Oi,j
def
= T i,j
k ):
k
I(T i,j
k ; Oi,j
k , i.e., the quantity I(T i,j
k ) − H(Oi,j
k ) − H(Ri,j),
k ) = H(Oi,j
= H(Oi,j
k ; Oi,j
k |T i,j
k )
(13)
where the ﬁrst equality is the deﬁnition of mutual informa-
tion (Eq. (6)), and the second equality follows from Equa-
tion (5).
Using ﬁrst Equation (9) and then Equations (7) and (8)
we can evaluate this further as follows:
+
γ + tj
(cid:17)
1
2
− C γ
1
k
C (γ+tj )/2
H(Oi,j) − H(Ri,j)
(14)
= H(Bin(k, γ) + Bin(k, tj)) − H(Bin(k, γ))
≤ H(Bin(2k,
≤ 1
2
)) − H(Bin(k, γ))
(cid:16) 1
(cid:18)
(cid:19)
tj
γ
1 +
ln
2
ln
1 − γ
C (γ+t)/2
4
4
2
+
+
≤ γ +
2k
tj
2γ
k2 − C γ
− C γ
3
k3
− C γ
− C γ
k2 − C γ
1
k
where the last inequality uses γ ≤ 1
2 In addition to this
estimation, for at most k · l n-grams can this difference
be different from 0: if an n-gram never appeared, then the
observed value Oi,j
k is identical to the randomness Ri,j, and
thus the entropy of both is the same. (*)
3
k3
2k
2
The full n-gram database which is leaked is the concate-
nation of the sums of the individual n-gram counts, i.e., the
actual information leakage is
L = I((
T i,j
k )1≤j≤N ; (
Oi,j
k )1≤j≤N )
= H((
≤ N(cid:88)
k )1≤j≤N ) − H((
Oi,j
H(Oi,j
k ) − H(Ri,j
k )
Ri,j
k )1≤j≤N )
(15)
j=1
i=1
Now we estimate the constants from Equation (15) using
that γ, t ≤ 0.01 and get:
l · C (γ+t)/2
4
2k
≤
1
5.8γ · k
,
l · C γ
k
1
≥ −
5
5.95γ · k
,
l · C γ
k2 ≥ − 1
5lk2γ2 ,
2
l · C γ
k3 = − 1
360k3l2 .
3
i=1
l(cid:88)
l(cid:88)
l(cid:88)
i=1
(cid:16)
l(cid:88)
i=1
l(cid:88)
(cid:17)
i=1
If we estimate the probability of 123456 using the em-
pirical frequency we might introduce some small error (the
mathematical treatment is explained later), however, we can
be conﬁdent that 123456 will remain the most common
password. This is not true for passwords with much lower
counts though. For example, the password zxcvbnmp has
been chosen 3 times in the dataset and it is the 288078th
most common password together with almost 400(cid:48)000 other
passwords, due to ties. However, if zxcvbnmp had been
chosen only 2 times, it would be the 1136277th most com-
mon password. In this case a small difference in the proba-
bility estimation can make a big difference in the guessing
entropy of a password.
In order to measure the accuracy of a password meters
we would like to ﬁnd a subset of common passwords for
which we can estimate, with high conﬁdence, the probability
and therefore the ranking. By assuming that each passwords
x binomially distributed, with unknown probability p(x).
With this interpretation, each password x in our dataset is
binomially distributed with N number of trials and unknown
probability p(x). In our case, N is the number of times
a password in our RockYou dataset was chosen, i.e., 32.6
million. Given the counts of each password x in our dataset,
we can estimate its probability as ˆpx = count(x)
. However,
this is only an estimation of the true probability mentioned
above. The error of this estimation depends on ˆpx and N
and, for example, larger N lead to better estimations. The
sampling error of ˆpx can be estimated using Wilson score
interval:
N
(cid:113)
ˆpx(1− ˆpx)
N
+
z2
1−α/2
4N 2
W (ˆpx, N ) =
ˆpx +
z2
1−α/2
2n ± z1−α/2
1 + 1
n z2
1−α/2
The relative ranking of two passwords x and x(cid:48), with
estimated probability ˆpx > ˆp(cid:48)
x, remains unchanged as long
as W (ˆpx, N ) > W (ˆp(cid:48)
x, N ). The conﬁdence level depends
on the z-score z1−α/2, for example if z1−α/2 = 2.58 than
the ordering is correct with 99 % probability.