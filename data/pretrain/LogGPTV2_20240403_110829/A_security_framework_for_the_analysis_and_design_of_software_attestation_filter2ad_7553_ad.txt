would not bother to determine a second pre-image. Hence,
we introduce the deﬁnition of blind second pre-image resis-
tance which concerns algorithms that are given only part of
the input (cid:126)s of ChkN and that have to determine the correct
output of ChkN (r0, (cid:126)s):
Pr
Deﬁnition 6 (Blind Second Pre-image Resistance). Chk :
{0, 1}lr × Σ → {0, 1}lr is ω-blind second pre-image resistant
with respect to the distribution DS (cf. Deﬁnition 3) if for
any N ∈ N, any subset of indices J (cid:40) {1, . . . , N} and for
any algorithm Alg that can be executed by P, it holds that
(cid:104)(cid:101)r = r|r0
(cid:105)
DS← Σ : i ∈ {1, . . . , N}
∧(cid:101)r ← Alg(r0, (sj )j∈J ) ∧ r ← ChkN (r0, s1, . . . , sN )
U← {0, 1}lr ∧ si
In addition we require (similar to Deﬁnition 5) that (cid:101)P can-
≤ ω.
not determine any information on rN = ChkN (r0, s1, . . . , sN )
without executing ChkN :
Deﬁnition 7 (Unpredictability of ChkN ). Chk : {0, 1}lr ×
Σ → {0, 1}lr
is νChk-unpredictable with respect to the dis-
(cid:92)ChkN
tribution DS (cf. Deﬁnition 3) if for any algorithm Alg
that can be executed by P and that does not execute ChkN ,
it holds that
(cid:12)(cid:12)(cid:12) Pr
(cid:104)
(cid:104)
b = 1|r0
U← {0, 1}lr ∧ si
DS← Σ : i ∈ {1, . . . , N}
∧ r = ChkN (r0, s1, . . . , sN )
∧ b ← Alg
(cid:92)ChkN
(r0, s1, . . . , sN , r )
(cid:105)
− Pr
b = 1|r0
U← {0, 1}lr ∧ si
DS← Σ : i ∈ {1, . . . , N}
∧ r U← {0, 1}lr
∧ b ← Alg
(cid:92)ChkN
(cid:105)(cid:12)(cid:12)(cid:12) ≤ νChk.
probability of a malicious prover (cid:101)P to make the veriﬁer V
6. SECURITY OF THE SCHEME
In this section we derive an upper bound for the success
(r0, s1, . . . , sN , r )
accept. This bound depends on the parameters deﬁned in
Section 5.2 which provide a suﬃcient condition to prove the
generic attestation scheme secure. The bound is as follows:
Theorem 1 (Generic Upper Bound). Let S be an incom-
pressible state (Deﬁnition 3). Consider the generic attesta-
tion scheme in Figure 1 with the components Read, Gen and
Chk such that
1. Gen is (N (δGen + δRead), )-pseudo-random (Deﬁnition 4)
and νGen-unpredictable (Deﬁnition 5),
λ :=
2. Chk is ω-blind second pre-image resistant (Deﬁnition 6)
and νChk-unpredictable (Deﬁnition 7).
denote the fraction of state entries that are diﬀerent in S and
mary memory and s memory words in its secondary memory
(cf. Section 3). Let
(cid:12)(cid:12)(cid:12)(cid:110)
a ∈ {0, 1}la|Read((cid:101)S, a) = Read(S, a)
Consider an arbitrary prover (cid:101)P as in Section 3 with state
State((cid:101)P) = (cid:101)S that can store p memory words in its pri-
(cid:111)(cid:12)(cid:12)(cid:12) · 2−la ,
(cid:101)S. Then the probability of (cid:101)P to win the security experiment
(cid:8)(π (M, ops) + ) · γN−M + νGen · (N − M )(cid:9) (1)
(cid:18) n
(cid:19)
(cid:19)j
(cid:0)max(cid:8)λx+1, γ(cid:9)(cid:1) n
(cid:33)
(cid:32)n−j(cid:89)
(cid:18) n − j
ExpAAttest (Deﬁnition 2) is upper bounded by
· 2−(lg +lr ) + max{ω, νChk} +
j=max{0,n−2la}
·
and ops denotes the number of instructions (cid:101)P can execute
2la − i
2la
n−1(cid:88)
x+1 −j ·
π(n, x) :=
0≤M≤N
p+s
ls/lr
where
(2)
2la
max
i=0
·
j
in time δRead + δGen.
This result implies that a software attestation scheme is
ε-secure if the expression in Equation 1 is ≤ ε, yielding a
suﬃcient condition for security. For example if a user aims
for ε-security for a prover device with ﬁxed system parame-
ters, he may choose the number of rounds N in dependence
of an expected value of λ accordingly (cf. Appendix B).
Note that the bound given in Equation 1 gives new in-
sights on the impact of the distribution of the state entries
7in S (expressed by γ) and the similarity between the ex-
pected prover state S and the actual state (cid:101)S of the prover
(expressed by λ) on the security of the scheme. Both aspects
have been either neglected or have been considered only in-
formally in previous work (cf. Section 7). To provide a better
intuition and to show the general applicability of Theorem 1,
we compute and discuss the bound for several concrete pa-
rameters that are typical for the systems considered in the
literature on software attestation in Appendix B.
Proof of Theorem 1. Let Win denote the event that a mali-
cious prover (cid:101)P wins the security experiment ExpAAttest, i.e.,
veriﬁer V sends a challenge (g0, r0) to (cid:101)P for which (cid:101)P has
Win means that ExpAAttest(S, l) = accept. We are interested
in an upper bound for Pr [Win]. To this end we consider
several sub-cases. Let Precomp denote the event that the
precomputed and stored the correct response rN in its mem-
ory (primary and/or secondary).4 Then we have
Pr [Win] =Pr [Win|Precomp] · Pr [Precomp]
+Pr [Win|¬Precomp] · Pr [¬Precomp]
≤Pr [Precomp] + Pr [Win|¬Precomp].
Since the challenge (g0, r0) ∈ {0, 1}lg +lr
pled, it follows that Pr [Precomp] = p+s
ls/lr
The maximum number of responses (cid:101)P can store is
abbreviate to Pr [Win]. Let Correct denote the event that (cid:101)P
and (gi, ai) = Gen(gi−1) for i ∈ {1, . . . , N} and that (cid:101)P has
We now estimate the term Pr [Win|¬Precomp], which we
determined all state entries (s1, . . . , sN ), i.e., si = Read(S, ai)
is uniformly sam-
· 2−(lg +lr ).
p+s
ls/lr
.
executed ChkN . Then we have
Pr [Win] ≤ Pr [Correct] + Pr [Win|¬Correct].
It follows from the fact that ChkN is ω-blind second pre-
image resistant (Deﬁnition 6) and νChk-unpredictable (Deﬁ-
nition 7) that Pr [Win|¬Correct] ≤ max{ω, νChk}.
For the ﬁnal term Pr [Correct], we use the following claim,
which we prove afterwards.
Claim 1. The probability Pr [Correct] that (cid:101)P determines all
(s1, . . . , sN ) and rN = ChkN (r0, s1, . . . , sN ) in the security
experiment ExpAAttest under the assumption that the response
to the requested challenge has not been precomputed is upper
bounded by
(cid:8) (π(M, ops) + ) · γN−M + νGen · (N − M )(cid:9)
max
0≤M≤N
where π(N, x) and ops are as in Theorem 1.
Taking these bounds together concludes the proof.
Proof of Claim 1
We now prove Claim 1 used in the proof of Theorem 1.
That is we show the claimed upper bound of Pr [Correct],
which is the probability that a malicious prover (cid:101)P with
state (cid:101)S := State((cid:101)P) (cid:54)= S correctly determines all state en-
tries (s1, . . . , sN ) in the security experiment ExpAAttest (Def-
inition 2) under the assumption that the response for the
requested challenge has not been precomputed.
4More precisely, A has precomputed this value during the
preparation phase and stored the response as part of (cid:101)S.
Observe that (cid:101)P may decide to deviate from the protocol
speciﬁcation, e.g., skipping some instructions with respect to
one round i (probably accepting a lower success probability
for determining si) to save some time that could be spent
on the determination of another state entry sj with i (cid:54)=
j (probably aiming for a higher probability to determine
sj). Hence the challenge is to show that for any of these
approaches the success probability does not exceed a certain
(non-trivial) bound, which cannot be done by a reduction to
a single assumption.
an oracle O that has access to S. All these games are divided
into two phases: A setup phase and a challenge phase. In
the setup phase O generates all addresses (a1, . . . , aN ) and
determines the corresponding state entries si = Read(S, ai).
We base our proof on a sequence of games played by (cid:101)P and
Afterwards, in the challenge phase, (cid:101)P and O exchange sev-
eral messages. In particular (cid:101)P must submit its guesses (cid:101)xi
for the state entries si to O. (cid:101)P wins the game only if all
guesses are correct, i.e., (cid:101)xi = si for all i = 1, . . . , N .
of (cid:101)P to deviate from the protocol speciﬁcation. While these
possibilities are quite limited in the ﬁrst game (Game 0), (cid:101)P
success probability of (cid:101)P changes. In most cases it turns out
gets more and more control with each subsequent game and
thus can to perform more powerful attacks. For each trans-
formation between two consecutive games, we show how the
The diﬀerences between the games lie in the possibilities
that the previous game represents a subset of the possible
attack strategies of the current game. Note that O formally
represents the honest execution of certain parts of the pro-
tocol and should not be confused with a real party. Con-
and O takes no time.
sequently, we assume that transferring messages between (cid:101)P
method for ignoring all computations of (cid:101)P which are hon-
attacks where (cid:101)P uses the time and/or memory gained by out-
and the size of the primary memory of (cid:101)P to what is nec-
estly executed by assumption. Hence to exclude artiﬁcial
sourcing the computation to O, we restrict the time-bound
Observe that the intention of O is to have an elegant
essary for honestly executing those computations that are
outsourced to O.
Game 0: Randomly Sampling Addresses in Regular
Time Intervals.
Game Description. The purpose of this game is to investi-
gate provers (cid:101)P which (1) do not exploit any aspects related
to the execution of Gen and (2) that are forced to use ex-
actly time δRead for the determination of each state entry si.
This is captured by modelling the game as follows: Within
the setup phase, O samples pairwise independent and uni-
form addresses (a1, . . . , aN ) and sets si := Read(S, ai) for all
i ∈ {1, . . . , N}. In the challenge phase, O iteratively queries
(cid:101)P with ai and (cid:101)P returns some response (cid:101)xi.
Hereby, (cid:101)P can access the Read oracle, which on input a
returns s = Read((cid:101)S, a) after time δRead. Since this is the
N · δRead, meaning that (cid:101)P automatically fails if it needs more
Observe that O ensures that (cid:101)P cannot change the order of
the memory addresses, i.e., O only sends ai to (cid:101)P after ai−1
only operation expected from an honest prover, the size of
the primary memory only allows to store an address a and
a state entry s. Moreover the total time-bound is limited to
time in total than this bound.
8round N we denote the time-frame between the point in time
has been sent.5 We denote with round i the time-frame
between the point in time where (cid:101)P receives ai and the point
in time where (cid:101)P receives ai+1 for i ∈ {1, . . . , N − 1}. With
where (cid:101)P receives aN and the point in time where (cid:101)P sends
the last response (cid:101)xN to O. (cid:101)P wins the game if (1) (cid:101)xi = si
δRead. Otherwise (cid:101)P looses the game.
for the probability Pr [Win0] that (cid:101)P wins Game 0. Since (cid:101)P
for all i ∈ {1, . . . , N} and (2) each round took at most time
Success Probability. We are interested in an upper bound
looses for sure when he uses more time than δRead to respond
to ai in at least one round i, it is suﬃcient to restrict to
provers that take at most time δRead in each round. To this
end, we derive an upper bound which allows to treat the
individual rounds separately. We start with the ﬁnal round
N and distinguish between two cases.
Read(S, aN ) is λ (cf. Theorem 1) since aN is sampled uni-
formly and independently from the previous addresses. Now
of at most γ (cf. Deﬁnition 3).