sive 0-days for which there can be no honeymoon period.
These Less-than-Zero days occur when a new version of a
product is released vulnerable to a previously disclosed vul-
nerability. For example, the day Windows 7 was oﬃcially
released, it was discovered that it was vulnerable to several
current prominent viruses which had originally been crafted
for Windows XP [35] Our research shows that less-than-zero
days account for approximately 21% of the total legacy vul-
nerabilities found, with closed source code containing the
most (34%)(see Table 4). In all cases the median number
of days to ﬁrst exploit is reduced by approximately 1/3 and
the median honeymoon ratio drops from 1.54 to 1.0. From
this we conclude that not patching vulnerabilities has a sig-
niﬁcant negative eﬀect on the honeymoon period. Of course
there is no way to measure exactly when an attacker is likely
to test an existing exploit against a newly released prod-
uct however, the Sophoslabs [35] tests are indicative of how
quickly a vendor might expect attackers to act.
5. RELATED WORK
As noted in the Introduction, both the scale of modern
software systems and the scale of their deployment have
made software design and engineering the focus of signiﬁ-
cant attention from scientists and engineers.
Brook’s ”The Mythical Man-Month” [5] is a bedrock ref-
erence for both the problems that the software engineering
discipline is intended to address and its collected data (al-
beit from the 1960s) in support of its cogent observations.
As Brooks addresses the issues in successfully engineering
large software systems his focus is software defects (”bugs”)
rather than software security vulnerabilities. His analyses
of the management issues in software engineering, particu-
larly factors to account for in scheduling, still hold true. For
example, the discussion of ”Regenerative Schedule Disaster”
(particularly Fig. 2.8, illustrating the added cost for train-
ing time) lends support to our observations about the time
required to gain familiarity with a software system. Brook’s
Figure 11.2, ”Bug occurrence as a function of release age”,
reproduced here on the left of Figure 1, shows an interval
of decrease in bugs found, slowing to some minimum rate,
followed by a slow rise in the rate of bugs found. This shows
258
the eﬀects of increased familiarity with a software system.
As do many software engineering scholars, Brooks empha-
sizes the positive aspects of reusable software components
without discussion of the potential risks from malicious ac-
tors.
Software reliability analysis is crucial to commercial ﬁrms
which must deliver reliable software in a timely manner. A
number of software reliability [21, 12, 27, 22] models have
been developed, with a focus on bug rates and their impli-
cations for software maturity and releasability. The models,
testing [26] and data collections do not address malicious
actors.
Arbaugh, et al. [2] initiated the study of the more spe-
cialized software vulnerability life-cycle, with a particular
focus on the intervals of time between when a vulnerability
is known and when a software system is updated to remove
the vulnerability. It is important to note, that these works
focused on rate of exploitation, while this paper focuses on
rate of vulnerability discovery.
Work by Jonsson, et al. [17] provides observations on a
user population of students with quantitative evaluation of
behavioral hypotheses, of which the most interesting to us
is the ability to ﬁnd bugs rapidly once the price is paid (in
time) of learning the software system.
Alhamzi, et al
[1] studied Windows 98 and Windows NT
4.0 and proposed a 3-phase S-shaped model (AIM) to de-
scribe the rate of change of cumulative vulnerabilities over
time where the ﬁrst phase includes time spent learning, but
Ozment’s analysis [24] of this and other vulnerability discov-
ery models showed that its predictive accuracy assumed a
static code-base and therefore was never tested against soft-
ware spanning multiple versions. Our analysis supports an
S-shaped curve model, but shows that the three phases in
the AIM model do not accurately describe the data we have
collected. Additionally, we are not concerned with the to-
tal number of vulnerabilities found over a product’s lifetime,
but with the ﬁrst vulnerability found per version, as well as
with a comparison of the cumulative number of days between
vulnerabilities, particularly those closest to the product’s re-
lease date.
Recent studies of bugs or vulnerabilities in large open
source software systems [6, 25] did analyze vulnerability den-
sity across several versions and provide some data and obser-
vations that we believe support our hypothesis. First, since
the software systems under study are open source software
(e.g., Linux and OpenBSD) and readily available, they are
learn-able by an attacker with an appropriate expenditure
of time. Second, an analysis of bugs that persisted from
version to version showed that such bugs were often a con-
sequence of ”cut and paste” software engineering, a crude yet
eﬀective form of software reuse. The majority of the exist-
ing vulnerability life-cycle and VDM research which makes
use of the NVD dataset focused primarily on a small num-
ber of operating systems or a few server applications and in
all but a few cases [25] only looked at one particular ver-
sion of each (e.g. Windows NT, Solaris 2.5.1, FreeBSD 4.0
and Redhat 6.2, or IIS and Apache). In particular, Ozment
and Schecter [25] found that 62% of the vulnerabilities in
OpenBSD v.2.3-3.7 came from legacy code, and concluded
that the original version of the source code may constitute
the bulk of the later version’s code base.
One large scale attempt to positively alter the rate of vul-
nerability discovery early on is Microsoft’s Security Devel-
opment Lifecycle (SDL) which claims to have reduced the
numbers of vulnerabilities found in Windows Vista’s ﬁrst
year compared with those found in Windows XP, which does
not use the SDL, (66 vs. 119) a 45% improvement. How-
ever, while Vista was in its ﬁrst year, XP had been out
for 6 years. We believe this also supports our hypothesis,
especially since, in its ﬁrst year, XP had only 28 vulnerabil-
ities [20], a diﬀerence of 58%. [23]
Code reuse continues to be considered an important part
of secure, eﬃcient software development in both open and
closed products [13, 10, 4]. However, Coverity’s analysis
of the lessons learned after years of using their static code
analysis tool provides some possible explanations of the role
legacy code plays in the honeymoon eﬀect. For example,
the authors list the most common response from software
developers after the discovery of 1000+ bugs: ”...The base-
line is to record the current bugs, don’t ﬁx them, but do ﬁx
any new bugs... A reasonable conservative heuristic is if you
haven’t touched the code in years, don’t modify it (even for
a bug ﬁx) to avoid causing any breakage.” [3] This suggests
that an attacker familiar with the legacy code that has been
carried over into a newly released version would have an edge
in ﬁnding new vulnerabilities in it (the legacy code), and this
might have a negative eﬀect on the honeymoon period.
In a recently published paper [28] the author analyzed the
risk of ﬁrst exploitation attempt using a Cox proportional
model and concludes “that the exploitation process is accel-
erated for open source products”. The focus of the paper is
on measuring the rate of exploitation attempts, not on the
rate of vulnerability discovery and is therefore not relevant
to our paper.
6. DISCUSSION AND CONCLUSIONS
The software lifecycle has been repeatedly examined, with
the intent of understanding the dynamics of software pro-
duction processes, most particularly the arrival rate of soft-
ware faults and failures. These rates decrease with time as
updates gradually repair the errors as they are found, until
an acceptable error rate is achieved.
The software vulnerability lifecycle has been less exten-
sively studied, with most attention paid to the period af-
ter an exploit has been discovered.
In attempting to un-
derstand the properties of vulnerability discovery, there are
two approaches we might have taken. One approach would
have been to study a single software system in depth, over
an extended period, draw detailed conclusions, and perhaps
generalize from them. Indeed, several of the related works
mentioned above try to do just that for the middle and end
phases of the lifecycle. But, another approach is to examine
a large set of software systems and try to ﬁnd properties that
are true over the entire set and over an extended period.
We chose the latter approach for an number of reasons,
which include the following: This approach allowed us to in-
corporate both open and closed source systems in our analy-
sis, this approach also allowed us to analyze several diﬀerent
classes of software (Operating Systems, Web Browsers User
applications, Server applications, etc), and this approach al-
lowed us to discover general vulnerability properties, e.g.
the honeymoon period, independent of the type of software,
and without requiring a detailed analysis of the properties
of each speciﬁc, individual vulnerability.
It might appear that given so many changes in tools, util-
ities, methodologies and goals used by both attackers and
defenders over the last decade, a long term analysis would
be inconsistent. To mitigate this we broke down each anal-
ysis by year and from version-to-version which are much
shorter time intervals, and we demonstrated the consistency
of this approach over time.
We also analyzed the role of legacy code in vulnerability
discovery and found surprisingly, based on a detailed study
of a large database of software vulnerabilities, that software
reuse may be a signiﬁcant source of new vulnerabilities. We
determined that the standard practice of reusing code of-
fers unexpected security challenges. The very fact that this
software is mature means that there has been ample oppor-
tunity to study it in suﬃcient detail to turn vulnerabilities
into exploits.
There are multiple potential causal mechanisms that might
explain the existence of the honeymoon eﬀect and the role
played by familiarity. One possibility is that a second vul-
nerability might be of similar type to the ﬁrst, so that ﬁnd-
ing it is fascilitated by knowledge derived from ﬁnding the
ﬁrst one. A second possibility is that the methodology or
tools developed to ﬁnd the ﬁrst vulnerability lowers the ef-
fort required to ﬁnd a subsequent ones. A third possible
cause might be that a discovered vulnerability would signal
weakness to other attackers (ie, blood in the water), causing
them to focus more attention on that area. [7]
The ﬁrst two possible causes require familiarity with the
system, while the third is an example of properties extrinsic
to the quality of the source code that might aﬀect the length
of the honeymoon period. An examination of these possible
causes will appear in future work.
The period between when the error rate is low enough for
release and attacker familiarity becomes high enough for an
initial 0-day vulnerability we have called the honeymoon and
its dynamics have been demonstrated in this paper to apply
to the majority of popular software systems for which we
had data.
The dynamics of the honeymoon eﬀect suggest an inter-
esting tradeoﬀ between decreasing error rate and increasing
familiarity with the software by attackers. This basic re-
sult has important implications for the arms race between
defenders and attackers.
First, it suggests that a new release of a software system
can enjoy a substantial honeymoon period without discov-
ered vulnerabilities once it is stable, independent of security
practices. Second, this honeymoon period appears to be a
strong predictor of the approximate upper bound of the vul-
nerability arrival rate. Third, it suggests (as hinted at by the
paper title) that attacker familiarity is a key element of the
software process dynamics, and this is a contraindication for
software reuse, as the greater the fraction of software reuse,
the smaller the amount of study required by an attacker.
Fourth, it suggests the need for more alternative approaches
to security software systems than simply trying to create
bug-free code.
In particular, research into alternative architectures or
execution models which focuses on properties extrinsic to
software, such as automated diversity, redundant execution,
software design diversity [8] might be used to extend the
honeymoon period of newly released software, or even give
old software a second honeymoon.
6.1 Acknowledgments
Professors Blaze and Smith’s work was supported by the
259
[16] Pankaj Jalote, Brendan Murphy, and Vibhu Saujanya
Sharma. Post-release reliability growth in software products.
ACM Trans. Softw. Eng. Methodol., 17(4):1–20, 2008.
[17] Erland Jonsson and Tomas Olovsson. A quantitative model
of the security intrusion process based on attacker behavior.
IEEE Trans. Softw. Eng., 23(4):235–245, 1997.
[18] M.C. McIlroy. Mass producted software components. Report
to Scientiﬁc Aﬀairs Division, NATO, October 1968.
[19] Microsoft.
Internet explorer architecture.
http://msdn.
microsoft.com/en-us/library/aa741312(VS.85).aspx,
2010.
[20] Microsoft Corporation.
security develop-
ment lifecycle. http://www.microsoft.com/security/sdl/
benefits/measurable.aspx, September 2008.
Microsoft
[21] John D. Musa. A theory of software reliability and its ap-
plication. IEEE Transactions on Security Engineering, SE-
1:312–327, September 1975.
[22] John D. Musa, Anthony Iannino, and Kasuhira Okumoto.
Software Reliability: Measurement, Prediction, Application.
McGraw-Hill, 1987.
[23] NIST. National Vulnerability Database, 2008.
[24] Andy Ozment. Improving vulnerability discovery models. In
QoP ’07: Proceedings of the 2007 ACM workshop on Quality
of protection, pages 6–11, New York, NY, USA, 2007. ACM.
[25] Andy Ozment and Stuart E. Schechter. Milk or wine: does
software security improve with age?
In USENIX-SS’06:
Proceedings of the 15th conference on USENIX Security
Symposium, Berkeley, CA, USA, 2006. USENIX Association.
[26] R.E. Prather. Theory of program testing - an overview.
Bell System Technical Journal, 72(10):3073–3105, December
1983.
[27] C.V. Ramamoorthy and F.B. Bastani. Software reliability
- status and perspectives. IEEE Transactions on Software
Engineering, SE-8(4):354–371, July 1982.
[28] Sam Ransbotham. An Empirical Analysis of Exploitation
Attempts based on Vulnerabilities in Open Source Software.
In Workshop on the Economics of Information Security
(WEIS), June 2010.
[29] Secunia. http://www.secunia.com. Vulnerability Intelligence
Provider.
[30] Security Focus. Vulnerabilities Database, 2008.
[31] SecurityTracker. http://www.SecurityTracker.com. Securi-
tyTracker.
[32] TippingPoint.
Zero day initiative (zdi).
http://www.
zerodayinitiative.com/.
[33] US-CERT. Vulnerability statistics. http://www.cert.org/
stats/vulnerability\_remediation.html.
[34] Vupen. Vupen security. http://www.vupen.com.
[35] Chester Wisniewski. Windows 7 vulnerable to 8 out of
10 viruses, 2009. http://www.sophos.com/blogs/chetw/g/
2009/11/03/windows-7-vulnerable-8-10-viruses/.
Oﬃce of Naval Research under N00014-07-1-907, Founda-
tional and Systems Support for Quantitative Trust Manage-
ment; Professor Smith received additional support from the
Oﬃce of Naval Research under the Networks Opposing Bot-
nets eﬀort N00014-09-1-0770, and from the National Science
Foundation under CCD-0810947, Blue Chip: Security De-
fenses for Misbehaving Hardware. Professor Blaze received
additional support from the National Science Foundation
under CNS-0905434 TC: Medium: Collaborative: Security
Services in Open Telecommunications
References
[1] O.H. Alhamzi and Y.K. Malaiya. Modeling the vulnera-
bility discovery process.
In Proceedings of the 16th IEEE
International Symposium on Software Reliability Engineer-
ing(ISSRE’05), Washington, DC, USA, 2005.
[2] William A. Arbaugh, William L. Fithen, and John McHugh.
Windows of vulnerability: A case study analysis. Computer,
33(12):52–59, 2000.
[3] Al Bessey, Ken Block, Ben Chelf, Andy Chou, Bryan Ful-
ton, Seth Hallem, Charles Henri-Gros, Asya Kamsky, Scott
McPeak, and Dawson Engler. A few billion lines of code
later: using static analysis to ﬁnd bugs in the real world.
Communications of the ACM, 53(2):66–75, 2010.
[4] BlackDuck. Koders.com. http://corp.koders.com/about/,
April 2010.
[5] Frederick P. Brooks. The Mythical Man-Month: Essays on
Software Engineering, 20th Anniversary Edition. Addison-
Wesley Professional, August 1995.
[6] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler. An
empirical study of operating systems errors.
In Proceed-
ings, 18th ACM Symposium on Operating Systems Princi-
ples, pages 73–82, October 2001.
[7] Sandy Clark, Matt Blaze, and Jonathan Smith. Blood in the
water: Are there honeymoon eﬀects outside software? In In
Proceedings of the 18th Cambridge International Security
Protocols Workshop -pending publication. Springer, 2010.
[8] Benjamin Cox, David Evans, Adrian Filipi, Jonathan
Rowanhill, Wei Hu, Jack Davidson, John Knight, Anh
Nguyen-tuong, and Jason Hiser. N-variant systems: A se-
cretless framework for security through diversity. In In Pro-
ceedings of the 15th USENIX Security Symposium, pages
105–120, 2006.
[9] CVE. Common vulnerabilities and exposures, 2008.
[10] Dr Dobbs Journal. Open Source Study Reveals High Level
http://www.drdobbs.com/open-source/
of Code Reuse.
216401796, March 2009.
[11] Stefan Frei.
Security Econometrics - The Dynamics of
(In)Security. Eth zurich, dissertation 18197, ETH Zurich,
2009. ISBN 1-4392-5409-5, ISBN-13: 9781439254097.
[12] A.L. Goel and K. Okumoto. A time dependent error de-
tection model for software reliability and other performance
measures. IEEE Transactions on Reliability, R-28:206–211,
August 1979.
[13] Michael Howard and Steve Lipner. The Security Develop-
ment Lifecycle. Microsoft Press, May 2006.
[14] IBM Internet Security Systems - X-Force. X-Force Advisory.
http://www.iss.net.
[15] iDefense.
Vulnerability
Contributor
Program.
http://labs.idefense.com/vcp.
260