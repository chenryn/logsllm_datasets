# Performance Comparison with Existing Approaches

As illustrated in Fig. 4, our approach outperforms existing methods. Specifically, from Fig. 4(a), it is evident that the false positive rates (FPR) of our work under three machine learning classifiers (Random Forest (RF), Decision Tree (DT), and Bayesian Network (BN)) are the lowest. The FPR of our work under the fourth classifier (DE) is the second lowest. Notably, under the decision tree classifier (DT), a widely used and standard machine learning classifier, the FPR of our method (0.5%) is less than half of the best other existing approach (B) and a quarter of the worst one (D).

From Fig. 4(b), we observe that the detection rates (DR) of our work under all four machine learning classifiers are the highest. In particular, the DR of our method (85%) is significantly higher than the DR of the worst detector (D) at 51% and the best other existing detector (B) at 73%.

We also evaluate our feature set using the F-measure [8], which considers both precision and recall. As shown in Fig. 4(c), the F-measure scores of our approach are the highest under all four classifiers. These results validate that our new feature set is more effective in detecting Twitter spammers.

Through these figures, we can also observe that the performance of [32] and [34] is better than that of [35]. This is primarily because both [32] and [34] utilize the feature of tweet similarity, while [35] only uses the feature of duplicate tweet count. Since many spammers post tweets with similar terms but different combinations rather than simply repeatedly posting the same tweet, the feature of tweet similarity is much more effective than duplicate count. Additionally, [32] incorporates a graph-based feature (number of bi-directional links) and a timing-based feature (tweet rate), which further enhances its performance compared to [34].

## Feature Validation

To further validate the effectiveness of our newly designed features, we compare the performance of two feature sets: one without our newly designed features and another with all features used in the previous experiment. Table 4 shows that for each classifier, the addition of our newly designed features increases the detection rate (DR) by over 10%, while maintaining an even lower false positive rate (FPR). This observation implies that the improvement in detection performance is indeed due to our newly designed features rather than the combination of several existing features.

### Table 4: Comparison Without and With New Features

| Classifier | Without Our Features | With Our Features |
|------------|----------------------|-------------------|
| **FPR**    | 0.017                | 0.010             |
| **DR**     | 0.738                | 0.858             |
| **F-Measure** | 0.774              | 0.877             |
| **FPR**    | 0.012                | 0.006             |
| **DR**     | 0.728                | 0.836             |
| **F-Measure** | 0.786              | 0.884             |
| **FPR**    | 0.015                | 0.011             |
| **DR**     | 0.702                | 0.846             |
| **F-Measure** | 0.757              | 0.866             |
| **FPR**    | 0.040                | 0.023             |
| **DR**     | 0.644                | 0.784             |
| **F-Measure** | 0.730              | 0.777             |

## Evaluation on Dataset II

In this section, to reduce the possible effect of sampling bias, we evaluate the effectiveness of our detection feature set by testing it on another dataset containing 3,500 unclassified Twitter accounts. Our goal is to test the actual operation and user experience without the ground truth from URL analysis by computing the Bayesian detection rate [21]—the probability of actually being at least a suspicious spammer, whenever an account is reported by the detection system.

Specifically, we use Dataset I, which has been labeled, as the training dataset, and Dataset II as the testing dataset. Based on our detection feature set, we use the BayesNet classifier to predict spammers on Dataset II. The results are shown in Table 5.

### Table 5: Classifier Effectiveness

| Total Spammer Predictions | 70 |
|---------------------------|----|
| Verified as Spammers       | 37 |
| Promotional Advertisers    | 25 |
| Benign                     | 8  |
| Identified by GSB          | 17 |

When we manually investigated the 70 accounts predicted as spammers, we found 37 real spammers, 25 promotional advertisers, and only 8 real false positives. This yields a high Bayesian detection rate of 88.6% (62/70). Further investigation of the 8 false positive accounts revealed that all of them exhibit odd behavior, but do not appear to have clear malicious intentions. Specifically, 6 of them are actively tweeting about only one topic, and the other 2 have posted very few tweets but have a large number of followings with a high ratio of followings to followers. We also examined the URLs posted by the 37 verified spammers and found that 17 of them posted malicious URLs according to the Google Safe Browsing blacklist.

## Limitation and Future Work

Due to practical limitations, we can only crawl a portion of the entire Twitter ecosystem, and our crawled dataset may still have sampling bias. Collecting an ideal, large, unbiased dataset from a dynamic OSN like Twitter is nearly impossible. Additionally, achieving comprehensive ground truth for Twitter spammers is challenging. Since we focus on one major type of spammers, the number of identified spammers in our dataset is a lower bound. However, even for a subset of spammers, we observe that they are evolving to evade detection. Our evaluation validates the effectiveness of our newly designed features in detecting these spammers.

We acknowledge that some identified spam accounts may be compromised accounts. However, since these accounts still exhibit malicious behavior and pose a risk to the Twittersphere, it is meaningful to detect them. While graph-based features such as local clustering coefficient and betweenness centrality are relatively difficult to evade, they are also expensive to extract. Thus, we use a sampling technique to compute these metrics piece-by-piece, but precisely estimating the values of such graph metrics on large graphs remains a challenging and active research issue.

For future work, we plan to design better crawling strategies, collect more data, and develop more robust features. We will also evaluate our machine learning detection scheme on larger datasets and work directly with Twitter. Additionally, we aim to broaden our targeted types of spammers to perform a deeper analysis of their evasion tactics and develop more quantitative models for the robustness of detection features.

## Conclusion

In this paper, we design new features to detect Twitter spammers based on an in-depth analysis of current evasion tactics. We formalize the robustness of detection features for the first time in the literature. According to our evaluation, while maintaining an even lower false positive rate, the detection rate using our new feature set increases by over 10% compared to all existing detectors under four prevalent machine learning classifiers.

## References

1. A new look at spam by the numbers. http://scitech.blogs.cnn.com/.
2. Acai berry spammers hack Twitter accounts to spread adverts. http://www.sophos.com/blogs/gc/g/2009/05/24/acai-berry-spammers-hack-twitter-accounts-spread-adverts/.
3. Auto Twitter. http://www.autotweeter.in/.
4. Betweenness Centrality. http://en.wikipedia.org/wiki/Centrality.
5. Botnet over Twitter. http://compsci.ca/blog/.
6. Buy a follower. http://buyafollower.com/.
7. Capture HPC. https://projects.honeynet.org/capture-hpc.
8. F-measure. http://en.wikipedia.org/wiki/F1_score.
9. Google Safe Browsing API. http://code.google.com/apis/safebrowsing/.
10. Local Clustering Coefficient. http://wikipedia.org/wiki/Clustering_coefficient#Local_clustering_coefficient.
11. Low-Priced Twitter Spam Kit Sold on Underground Forums. http://news.softpedia.com/news/Low-Priced-Twitter-Spam-Kit-Sold-on-Underground-Forums-146160.shtml.
12. New Koobface campaign spreading on Facebook. http://community.websense.com/blogs/securitylabs/archive/2011/01/14/new-koobface-campaign-spreading-on-facebook.aspx.
13. The 2000 Following Limit Policy On Twitter. http://twittnotes.com/2009/03/2000-following-limit-on-twitter.html.
14. The Twitter Rules. http://help.twitter.com/entries/18311-the-twitter-rules.
15. Tweet spinning your way to the top. http://blog.spinbot.com/2011/03/tweet-spinning-your-way-to-the-top/.
16. TweetDeck. http://www.tweetdeck.com/.
17. Twitter account for sale. http://www.potpiegirl.com/2008/04/buy-sell-twitter-account/.
18. Twitter API in Wikipedia. http://apiwiki.twitter.com/.
19. Twitter phishing hack hits BBC, Guardian and cabinet minister. http://www.guardian.co.uk/technology/2010/feb/26/twitter-hack-spread-phishing.
20. Twitter Public Timeline. http://twitter.com/public_timeline.
21. S. Axelsson. The base-rate fallacy and its implications for the difficulty of intrusion detection. In Proceedings of the 6th ACM Conference on Computer and Communications Security, pages 1–7, 1999.
22. F. Benevenuto, G. Magno, T. Rodrigues, and V. Almeida. Detecting Spammers on Twitter. In Collaboration, Electronic messaging, Anti-Abuse and Spam Conference (CEAS), 2010.
23. F. Benevenuto, T. Rodrigues, V. Almeida, J. Almeida, and M. Gonçalves. Detecting Spammers and Content Promoters in Online Video Social Networks. In ACM SIGIR Conference (SIGIR), 2009.
24. F. Benevenuto, T. Rodrigues, V. Almeida, J. Almeida, C. Zhang, and K. Ross. Identifying Video Spammers in Online Social Networks. In Int’l Workshop on Adversarial Information Retrieval on the Web (AirWeb’08), 2008.
25. M. Cha, H. Haddadi, F. Benevenuto, and K. Gummadi. Measuring User Influence in Twitter: The Million Follower Fallacy. In Int’l AAAI Conference on Weblogs and Social Media (ICWSM), 2010.
26. Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia. Who is Tweeting on Twitter: Human, Bot, or Cyborg? In Annual Computer Security Applications Conference (ACSAC’10), 2010.
27. H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Zhao. Detecting and Characterizing Social Spam Campaigns. In Proceedings of ACM SIGCOMM IMC (IMC’10), 2010.
28. C. Griery, K. Thomas, V. Paxson, and M. Zhang. @spam: The Underground on 140 Characters or Less. In ACM Conference on Computer and Communications Security (CCS), 2010.
29. D. Ionescu. Twitter Warns of New Phishing Scam. http://www.pcworld.com/article/174660/twitter_warns_of_new_phishing_scam.html.
30. G. Koutrika, F. Effendi, Z. Gyongyi, P. Heymann, and H. Garcia-Molina. Combating spam in tagging systems. In Int’l Workshop on Adversarial Information Retrieval on the Web (AIRWeb’07), 2007.
31. H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter, a Social Network or a News Media? In Int’l World Wide Web (WWW ’10), 2010.
32. K. Lee, J. Caverlee, and S. Webb. Uncovering Social Spammers: Social Honeypots + Machine Learning. In ACM SIGIR Conference (SIGIR), 2010.
33. J. Leskovec and C. Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD), 2006.
34. G. Stringhini, S. Barbara, C. Kruegel, and G. Vigna. Detecting Spammers On Social Networks. In Annual Computer Security Applications Conference (ACSAC’10), 2010.
35. A. Wang. Don’t follow me: spam detecting in Twitter. In Int’l Conference on Security and Cryptography (SECRYPT), 2010.
36. C. Yang, R. Harkreader, and G. Gu. Die free or live hard? empirical evaluation and new design for fighting evolving Twitter spammers (extended version). Technical report, 2011.