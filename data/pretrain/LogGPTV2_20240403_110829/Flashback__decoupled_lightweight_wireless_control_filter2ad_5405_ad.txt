Virtex-5 LX30 FPGA based software radios from National Instru-
ments [4], shown in Fig. 4. The FPGA has basic LUT-FF and 32
DSP48E arithmetic unit resources. The Flashback receiver was im-
plemented on two units: the 7695 FPGA implements the FFT logic
of the receiver chain, while the ﬂash detection and decoding is per-
formed on a real-time processor (NI PXIe-8133 RT Module). The
FPGA is connected to an NI 5781 Baseband Transceiver, which
serves as both the ADC and the DAC. The ADC has 14 bits of
resolution, and the DAC has 16 bits. We used the National Instru-
ments LabVIEW system design language [3] to implement and de-
bug Flashback on the programmable radios.
4.1 Flash Transmitter Implementation
Fig. 5 presents the implementation of Flashback’s transmitter.
We added two blocks to the standard OFDM transmit chain: the
ﬂash modulator and the ﬂash inserter. The ﬂash modulator trans-
forms a 32 bit binary number into a series of 9 ﬂashes on the time-
frequency grid.
It does this, by adding an 8 bit CRC to the 32
bit binary sequence, and converting the entire sequence into eight
32-base numbers. The ﬂash inserter then encodes the message us-
ing the technique described in Sec. 3.2 into a series of 9 ﬂashes,
adds the ﬂashes generated by the encoder onto the transmission in
the appropriate slots and zeros out the rest of the slots. Other than
these two blocks, the Flashback transmitter is identical to a Wi-Fi
transmitter. When a node wishes to ﬂash, it switches on the ﬂash
transmitter logic, and when it has to transmit regular data messages,
it can simply turn the Flashback logic off.
4.2 Flash Receiver Implementation
The implementation of the receiver is shown in Fig. 6. For the
EncoderModulator64 IFFTCyclic PrefixFlash InserterFlash ModulatorDACData PacketControl MessageMUXADCSync64 FFTEqualizerDemodul-atorFlash DemodulatorFlash EraserFlash DetectorViterbi DecoderData PacketControl MessageFigure 7: Number of ﬂashes Flashback can support for different
SNRs, with ’oracle’ bitrate adaptation, i.e. when the transmitter is
sending at the optimal bitrate for our receiver. In a real network
setting, the transmitter will typically transmit data at a lower bitrate
than optimal, and Flashback will be able to support a higher ﬂash
rate than the rates depicted here.
Figure 8: Packet loss rate of a 12dB link, as a function of the ﬂash
rate.
The ﬁrst node is a regular data transmitter. It keeps sending Wi-
Fi packets with a ﬁxed length of 40 symbols. The second node is
the ﬂash transmitter. The ﬂash transmitter continuously transmits
ﬂashes at a speciﬁed rate at the same power. The third node is the
receiver. The receiver decodes both the data packet and the ﬂashes.
In our experiments, the ﬂashes are received at 6−10dB higher than
the data transmission. The location of the data transmitter is varied
to obtain different SNRs. For each SNR, we ﬁrst tried transmitting
hundreds of packets at all the bitrates, and picked the bitrate which
maximizes the throughput for the data transmitter. Next, we varied
the ﬂash rate, and picked the highest rate, which doesn’t affect the
throughput of the data transmitter by more than 1%. Fig. 7 plots the
maximum ﬂash rate as a function of the SNR measured between the
data transmitter and receiver, when the transmitter is transmitting at
the most optimal discrete bitrate (i.e. this is an oracle transmitter; it
can estimate the exact SNR for every packet ahead of time).
Analysis: As we can see from Fig. 7, since we are using an ora-
cle transmitter that utilizes perfect SNR estimation, the channel can
only support a maximum of 5, 000 ﬂashes per second, because it
is the minimum of all the ﬂash rates across all SNRs. However,
since the channel SNR is dynamic, and the bitrate adaptation algo-
rithms cannot estimate SNR perfectly as we will show below, we
can typically support much greater ﬂash rates of 50, 000 ﬂashes per
second.
An interesting feature of the graph is that it looks like a ’chain-
saw’; it starts low when Flashback switches to a new bitrate, and
increases when the SNR is increased. The reason we see the chain-
saw effect, is that every time Flashback switches to a higher bitrate,
the data plane loses some redundancy in its channel code, since it
is trying to send more data on a given channel. Since there is less
redundancy, the ﬂash eraser cannot erase as many bits as it could
before, and Flashback can only support a lower ﬂash rate. How-
ever, when the SNR increases, the message is received with fewer
errors at the receiver, and the Viterbi decoder can tolerate more bit
erasures due to ﬂashes. As an aside, it is likely that the line in Fig. 7
could be increased beyond 125, 000 ﬂashes a second at high SNRs.
In our experiments, we did not attempt to ﬂash at higher rates than
125, 000, since we only have one ﬂash transmitter.
To further explain the previous results, we focus on one exper-
iment where the SNR of the data transmission is 12dB at the re-
ceiver. For this SNR, the data transmitter uses a 16-QAM, 1/2 rate
encoding for its packet to maximize throughput. Note that this SNR
Figure 9: Overall packet loss rate as a function of the ﬂash rate,
measured by applying the SoftRate bit adaptation algorithm on a
75, 000 packet trace collected by our channel sounders.
lies right at a transition point for data bitrates. For a slightly lower
SNR of 11dB, the data transmitter would have picked QAM, 3/4
rate encoding. We vary the ﬂashing rate from the ﬂash transmitter
and measure the packet loss rate for the data transmission. The re-
sults are presented in Fig. 8. As expected, as we increase the ﬂash
rate, the packet loss rate of the data packets increases. The reason
is that the increasing number of ﬂashes causes the receiver to erase
more bits from the bit stream. If the receiver erases too many bits,
it will start seeing errors in the Viterbi decoder.
Note that even at the most minimal ﬂash rate, there is a small
probability of packet loss (around 0.45%), which most probably oc-
curs because sometimes a ﬂash can interfere with the packet header
itself. In a practical system this would be avoided, since the ﬂash-
ing node could utilize the carrier sense mechanism to ensure it does
not interfere with the header.
Conservative Bitrate Adaptation: Bitrate adaptation algorithms
have to choose the transmitted bitrates conservatively, because of
the difﬁculty in measuring channels under time-varying contention-
prone environments. This increases the link margin in practical set-
tings. To evaluate this property, we need to conduct benchmarks on
a network where there are time-varying environmental conditions
and multiple contending nodes. However, we cannot conduct such
experiments with our software radios, since they are large static
nodes (as we can see in Fig. 4) and too expensive for emulating an
entire network. Hence, we resort to trace-driven experimentation.
100010000100000567891011121314151617181920Maximum Number of Flashes per SecondSNR (dB)Maximum Flash Rates Using Optimal BitratesQPSK 1/2QPSK 3/416-QAM 1/216-QAM 3/4Minimum = 5,00050006,25012,50016,66725,00050,00083,333125,00001234567500050000Packet Loss Rate [Percent]Flash Rate per SecondPacket Loss Rate (16-QAM 1/2 Rate, SNR=12dB)00.511.522.5100010000100000Packet Loss Rate [Percentage]Flashes per SecondOverall Packet Loss Rate of Data Plane With InterferenceOverall Packet Loss Rate of Data Plane Without Interferencereceiver. This is why the ﬂash messaging technique uses the rela-
tive distances between subcarriers to modulate the message, and is
therefore resilient to such false positives, as described in Sec. 3.2.
6. APPLICATIONS
Flashback provides a generic decoupled control plane that can
be used to improve data plane performance for a variety of appli-
cations. The decoupled control plane allows Flashback not only to
signiﬁcantly improve existing scheduling protocols like RTS/CTS
and CSMA/CA, but also enable novel applications. This section
presents how Flashback can be used to signiﬁcantly improve exist-
ing Wi-Fi MAC protocols. We then show how Flashback enables
a virtualized enterprise Wi-Fi network with better handoffs, duty-
cycling, load and interference management.
6.1 Flashback-MAC
Flashback Šs control plane enables us to eliminate almost all of
the control overhead in the current Wi-Fi MAC protocol, which
uses CSMA/CA or RTS/CTS to mediate medium access. In con-
trast with these schemes, nodes use Flashback Šs control plane to
notify the AP of their outstanding transmission requests, while the
AP is solely responsible for determining the schedule for the entire
network. We describe this simple and generic MAC protocol, which
we call Flashback-MAC, and explain how it eliminates a number of
chronic problems associated with the current Wi-Fi MAC protocols.
The Demand Map is the key novel abstraction that Flashback
provides the AP to determine the schedule for the entire network.
The demand map is a list of all outstanding transmission requests
from all nodes in the network. The list is sorted in order of the
arrival of the requests. Each request is associated with a ﬂow iden-
tiﬁer at that node, and is also annotated with the SNR of the node
requesting a transmission opportunity. Each request can also in-
clude a few extra annotations, including bits that signify QoS re-
quests, such as latency or throughput sensitive trafﬁc. Note that the
demand map also keeps track of trafﬁc that the AP has to transmit
to various nodes.
The demand map for uplink trafﬁc is constructed using ﬂashes.
Whenever a node has a packet queued for transmission, it ﬂashes
a short control message to the AP. The message contains a ﬂow
identiﬁer, and the corresponding amount of outstanding trafﬁc and
any associated QoS requests. Our current implementation allows
two different classes: latency sensitive and regular data. Latency
sensitive trafﬁc is for interactive applications such as VoIP. The ﬁrst
10 bits in the control message are used to signal the identity of the
ﬂashing node, the next 4 are reserved for the ﬂow identiﬁer, the next
8 for outstanding data in multiples of 100B for that ﬂow, the next
8 for specifying deadlines, and the ﬁnal 2 for specifying the QoS
level. There are certain bits reserved in the ID and ﬂow identiﬁer
ﬁelds for association requests for new nodes that wish to join the
network.
The control message is sent using the control plane protocol just
described. The AP decodes the control message, and also estimates
the approximate SNR of the ﬂashing node from the ﬂash message
itself. Whenever the AP has the opportunity to send a packet (either
data or ACK), it piggybacks a short message at the end of those
packets signaling the receipt of the control messages. Hence, if a
node does not see an acknowledgement it can resend the control
message using ﬂashes.
The demand map decouples the medium access mechanism from
the scheduling policy, i.e.
it allows the AP to ﬂexibly use any
scheduling algorithm on top of the demand map. To demonstrate
the demand map’s ﬂexibility, we simulated two simple and generic
scheduling algorithms.
Figure 10: The false negative and positive rates of Flashback when
received at roughly 6dB higher than the data packets.
We use the traces collected by the RUSK channel sounder, which
we described in Sec. 2, to simulate time-varying and contention
conditions, and a state-of-the-art bitrate adaptation algorithm, Soft-
Rate [20]. We simulate a Wi-Fi transmitter sending packets over the
channel trace, and collect the bitrate decisions SoftRate makes. We
then vary the ﬂash rate and measure the effect on the data through-
put, by applying the packet loss rates we measured on our Flash-
back implementation. We also added interference by generating a
trace of Bluetooth and Zigbee signals, received at SNRs between
3dB and 10dB.
Fig. 9 plots the overall packet loss rate of the data plane as a
function of the ﬂash rate. Without interference, Flashback can send
50, 000 ﬂashes per second. This number is 10 times higher than
the number of ﬂashes per second when Flashback was using the
optimal discrete bitrate for each SNR. With external interference
from sources such as Bluetooth and Zigbee radios, Flashback can
send ﬂashes at a rate as high as 100, 000 ﬂashes per second. The
reason is that rate adaptation algorithms cannot accurately estimate
the channel SNR and interference, and are forced to make conser-
vative bitrate decisions to avoid packet loss.
As we can see, the right ﬂash rate depends on the environment
Flashback operates in. In settings where contention, external in-
terference and time-varying conditions are common, Flashback can
transmit at a ﬂash rate of 100, 000. However, if the environment
is calmer (e.g., a home with several Wi-Fi clients), then Flashback
supports a lower ﬂash rate of 50, 000. These correspond to data
rates of 400Kbps and 175Kbps respectively. By default, we assume
Flashback has a data rate of 175Kbps.
5.2 Flash Detection Accuracy
Next, we investigate the accuracy of ﬂash detection. We sent
ﬂashes at 6dB on a known subcarrier over a varying data plane,
while modifying the SNR of the received data packet. At each SNR,
we calculated the false negative (number of ﬂashes missed by the
receiver) and false positive rate (ﬂashes that the receiver detected
which weren’t actually sent). As depicted by Fig. 10, the error rates
are very low for all SNRs. The false negative and positive rates are
below 0.1% for all SNRs other than in the range of 5−6.5dB. This
is probably due to the fact that in low SNRs, there is more variation
among the power of the different subcarriers and it is difﬁcult to
correctly detect the ﬂash.
In the rare occasions that the ﬂash detector produces false posi-
tives, it is always on a subcarrier adjacent to the real subcarrier that
was sent by the ﬂash transmitter. These false positives are caused
by the carrier frequency offset between the ﬂash transmitter and the
012345678910567891011121314151617181920PercentageSNR [dB]Error Rates of FlashesFalse Negative Rate of Flashes, R=5000False Positive Rate of Flashes, R=5000Figure 11: Flashback-MAC using FIFO scheduling compared to CSMA/CA and RTS/CTS with 1, 000 byte packets.
Figure 12: Flashback-MAC using QoS-aware scheduling and duty cycling, compared to RTS/CTS and Wi-Fi PSM. In the two graphs on the
left, two nodes are sending latency-sensitive packets, while the rest are sending normal packets. Latency is measured as the delay between the
time the node requests to send the packet and when it starts sending the packet. The third ﬁgure shows the energy efﬁciency gains of using
Flashback compared to Wi-Fi PSM.
First, we designed a simple FIFO scheme where the AP sched-
ules transmission requests in the order in which they arrive. Second,
we simulated a QoS-aware scheduling algorithm that prioritizes la-
tency sensitive trafﬁc ahead of regular trafﬁc.
Once the schedule is determined, the AP uses the next opportu-
nity for a data plane transmission (either data or ACK) to signal
which node gets to transmit next, and from which ﬂow identiﬁer.
Flashback-MAC provides signiﬁcant beneﬁts over the Wi-Fi MAC
protocols. First, it eliminates overheads related to medium access.
Nodes no longer have to perform carrier sense or exponential back
off at the data plane, nor use any control messages such as RTS and
CTS. These mechanisms add signiﬁcant overhead, since they have
to be sent at the lowest bitrate (6M bps) compared to the data trans-
mission, which can reach bitrates as high as 54M bps. Our simu-
lation results show that such contention overhead consumes nearly
50% of the channel time in congested networks.
Second, it eliminates hidden terminals on the data plane, since
nodes no longer have to use the inaccurate carrier sense mechanism
to determine if a channel is idle. Instead, with Flashback-MAC the
AP explicitly controls which node is transmitting and can actively
prevent hidden terminals.
Third, it allows the AP to implement QoS on the uplink and
downlink efﬁciently. Explicit coordination allows Flashback-MAC
to guarantee QoS in a centralized fashion, while paying minimal
overhead.
Finally, Flashback-MAC is backwards compatible. Even if only
the AP and some of the nodes in a network support Flashback-
MAC, the AP can still service the requests from the nodes that do
not support Flashback, as long as the AP ensures there is enough
idle time in the network. The only caveat is that the old nodes may
still impact the desired QoS of the Flashback-MAC nodes.
6.1.1 Performance and Latency
We simulated Flashback-MAC using an NS-3 Wi-Fi simulation.
Our simulated network includes one AP and a varying number of
clients that are randomly uniformly distributed on a 80 × 80 grid.
We applied to the simulation the packet loss and ﬂash error rate
values that we collected from our Flashback receiver implementa-
tion and from our channel sounder traces. The nodes collectively
ﬂash at a maximum rate of 50, 000 ﬂashes per second. We imple-
mented the demand map abstraction for the AP, and simulated the
scheduling policies described above.
In order to analyze the amount of control overhead Flashback-