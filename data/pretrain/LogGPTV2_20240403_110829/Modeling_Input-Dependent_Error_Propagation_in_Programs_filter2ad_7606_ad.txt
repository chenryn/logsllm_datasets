using
and
VTRIDENT,
Finally, we show the accuracy of predicting OVERALL-
SDC-VOLATILITY
using
INSTRUCTION-EXECUTION-VOLATILITY alone (as before)
in Figure 8. As can be seen, the average difference between
VTRIDENT and FI is only 1.26x. Recall that the prediction
using INSTRUCTION-EXECUTION-VOLATILITY alone (Exec.
Vol.) gives an average difference of 7.65x (Section IV).
The worst case difference when considering only Exec.
is 1.29x (in Pathﬁnder) when
Vol. was 24.54x, while it
INSTRUCTION-SDC-VOLATILITY is
taken into account.
trends are observed in all other benchmarks.
Similar
accuracy of OVERALL-SDC-
This
the
VOLATILITY prediction
is
higher when
INSTRUCTION-SDC-VOLATILITY and
considering
signiﬁcantly
indicates
that
Fig. 6: Distribution of INSTRUCTION-SDC-VOLATILITY predictions by vTrident Versus Fault Injection Results (Y-axis:
Percentage of instructions, Error Bar: 0.03% to 0.55% at 95% Conﬁdence)
We also measure the time per input as both TRIDENT
and VTRIDENT experience similar slowdowns as the number
of inputs increase (we conﬁrmed this experimentally). The
average execution time of VTRIDENT is 944 seconds per
benchmark per input (a little more than 15 minutes). Again,
we emphasize that this is due to the considerably large input
sizes we have considered in this study (Section IV).
The results of the speedup by VTRIDENT over TRI-
DENT are shown in Figure 9. We ﬁnd that on average
VTRIDENT is 8.05x faster than TRIDENT. The speedup in
individual cases varies from 1.09x in Graph (85.16 seconds
versus. 78.38 seconds) to 33.56x in Streamcluster (3960 sec-
onds versus. 118 seconds). The variation in speedup is because
applications have different degrees of memory-boundedness:
the more memory bounded an application is, the slower it is
with TRIDENT, and hence the larger the speedup obtained by
VTRIDENT (as it does not need detailed memory dependency
traces). For example, Streamcluster is more memory-bound
than computation-bound than Graph, and hence experiences
much higher speedups.
Fig. 9: Speedup Achieved by VTRIDENT over TRIDENT.
Higher numbers are better.
Note that we omit Clomp from the comparison since
Clomp consumes more than 32GB memory in TRIDENT,
and hence crashes on our machine. This is because Clomp
generates a huge memory-dependency trace in TRIDENT,
which exceeds the memory of our 32GB-memory machine (in
reality, it experiences signiﬁcant slowdown due to thrashing,
and is terminated by the OS after a long time). On the other
hand, VTRIDENT prunes the memory dependency and incurs
only 21.29MB memory overhead when processing Clomp.
Fig. 7: Accuracy of VTRIDENT in Predicting INSTRUCTION-
SDC-VOLATILITY Versus FI (Y-axis: Accuracy)
Fig. 8: OVERALL-SDC-VOLATILITY Measured by FI and
Predicted by VTRIDENT, and INSTRUCTION-EXECUTION-
VOLATILITY alone (Y-axis: OVERALL-SDC-VOLATILITY,
Error Bar: 0.03% to 0.55% at 95% Conﬁdence)
INSTRUCTION-EXECUTION-VOLATILITY rather
using INSTRUCTION-EXECUTION-VOLATILITY.
than just
B. Performance
We evaluate the performance of VTRIDENT based on its
execution time, and compare it with that of TRIDENT. We do
not consider FI in this comparison as FI is orders of magnitude
slower than TRIDENT [11]. We measure the time taken by
executing VTRIDENT and TRIDENT in each benchmark,
and compare the speedup achieved by VTRIDENT over
TRIDENT. The total computation is proportional to both the
time and power required to run each approach. Parallelization
will reduce the time spent, but not
the power consumed.
We assume that there is no parallelization for the purpose
of comparison in the case of TRIDENT and VTRIDENT,
though both TRIDENT and VTRIDENT can be parallelized.
Therefore, the speedup can be computed by measuring their
wall-clock time.
VII. BOUNDING OVERALL SDC PROBABILITIES WITH
VTRIDENT
In this section, we describe how to use VTRIDENT to
bound the overall SDC probabilities of programs across given
inputs by performing FI with only one selected input. We need
FI because the goal of VTRIDENT is to predict the variation
in SDC probabilities, rather than the absolute SDC probability
which is much more time-consuming to predict (Section V-B).
Therefore, FI gives us the absolute SDC probability for a given
input. However, we only need to perform FI on a single input
to bound the SDC probabilities of any number of given inputs
using VTRIDENT, which is a signiﬁcant savings as FI tends to
be very time-consuming to get statistically signiﬁcant results.
For a given benchmark, we ﬁrst use VTRIDENT to pre-
dict the OVERALL-SDC-VOLATILITY across all given inputs.
Recall
that OVERALL-SDC-VOLATILITY is the difference
between the highest and the lowest overall SDC probabilities
of the program across its inputs. We denote this range by R.
We then use VTRIDENT to ﬁnd the input that results in the
median of the overall SDC probabilities predicted among all
the given inputs. This is because we need to locate the center
of the range in order to know the absolute values of the bounds.
Using inputs other than the median will result in a shifting of
the reference position, but will not change the boundaries being
identiﬁed, which are more important. Although VTRIDENT
loses some accuracy in predicting SDC probabilities as we
mentioned earlier, most of the rankings of the predictions
are preserved by VTRIDENT. Finally, we perform FI on
the selected input to measure the true SDC probability of
the program, denoted by S. Note that it is possible to use
other methods for this estimation (e.g., TRIDENT [11]).
The estimated lower and upper bounds of the overall SDC
probability of the program across all its given inputs is derived
based on the median SDC probability measured by FI, as
shown below.
[(S − R/2), (S + R/2)]
(3)
We bound the SDC probability of each program across its
inputs using the above method. We also use INSTRUCTION-
EXECUTION-VOLATILITY alone for the bounding as a point
of comparison. The results are shown in Figure 10. In the
ﬁgure,
the triangles indicate the overall SDC probabilities
with the ten inputs of each benchmark measured by FI. The
overall SDC probability variations range from 1.54x (Graph)
to 42.01x (Lulesh) across different inputs. The solid lines
in the ﬁgure bound the overall SDC probabilities predicted
by VTRIDENT. The dashed lines bound the overall SDC
probabilities projected by considering only the INSTRUCTION-
EXECUTION-VOLATILITY.
On average, 78.89% of the overall SDC probabilities of
the inputs measured by FI are within the bounds predicted
by VTRIDENT. For the inputs that are outside the bounds,
almost all of them are very close to the bounds. The worst
case is FFT, where the overall SDC probabilities of two inputs
are far above the upper bounds predicted by VTRIDENT. The
best cases are Streamcluster and CoMD where almost every
input’s SDC probability falls within the bounds predicted by
VTRIDENT (Section VIII-A explains why).
On
INSTRUCTION-EXECUTION-
VOLATILITY alone bounds only 32.22% SDC probabilities on
hand,
the
other
considering
average. This is a sharp decrease in the coverage of the bounds
compared with VTRIDENT,
indicating the importance
of
INSTRUCTION-SDC-VOLATILITY when
bounding overall SDC probabilities. The only exception is
Streamcluster where considering INSTRUCTION-EXECUTION-
VOLATILITY alone is sufﬁcient in bounding SDC probabilities.
This is because Streamcluster exhibits very little SDC volatility
across inputs (Figure 6).
In addition to coverage, tight bounds are an important
requirement, as a loose bounding (i.e., a large R in Equation 3)
trivially increases the coverage of the bounding. To investigate
the tightness of the bounding, we examine the results shown
in Figure 8. Recall that OVERALL-SDC-VOLATILITY is rep-
resented by R, so the ﬁgure shows the accuracy of R. As we
can see, VTRIDENT computes bounds that are comparable
to the ones derived by FI (ground truth), indicating that the
bounds obtained are tight.
VIII. DISCUSSION
In this section, we ﬁrst summarize the sources of inaccu-
racy in VTRIDENT, and then we discuss the implications of
VTRIDENT for error mitigation techniques.
A. Sources of Inaccuracy
Other than the loss of accuracy from the coarse-grain track-
ing in memory dependency (Section V-B), we identify three
potential sources of inaccuracy in identifying INSTRUCTION-
SDC-VOLATILITY by VTRIDENT. They are also the sources
of inaccuracy in TRIDENT, which VTRIDENT is based on.
We explain how they affect identifying INSTRUCTION-SDC-
VOLATILITY here.
Source 1: Manipulation of Corrupted Bits
We assume only instructions such as comparisons, logical
operators and casts have masking effects, and that none of
the other instructions mask the corrupted bits. However, this
is not always the case as other instructions may also cause
masking. For example, repeated division operations such as
fdiv may also average out corrupted bits in the mantissa of
ﬂoating point numbers, and hence mask errors. The dynamic
footprints of such instructions may be different across inputs
hence causing them to have different masking probabilities, so
VTRIDENT does not capture the volatility from such cases.
For instance, in Lulesh, we observe that the number of fdiv
may differ by as much as 9.5x between inputs.
Source 2: Memory Copy
VTRIDENT does not handle bulk memory operations
such as memmove and memcpy. Hence, we may lose track
of error propagation in the memory dependencies built via
such operations. Since different inputs may diversify mem-
ory dependencies, the diversiﬁed dependencies via the bulk
memory operations may not be identiﬁed either. Therefore,
VTRIDENT may not be able to identify INSTRUCTION-SDC-
VOLATILITY in these cases.
Source 3: Conservatism in Determining Memory Corrup-
tion
We assume all the store instructions that are dominated by
the faulty branch are corrupted when control-ﬂow is corrupted,
Fig. 10: Bounds of the Overall SDC Probabilities of Programs (Y-axis: SDC Probability; X-axis: Program Input; Solid Lines:
Bounds derived by VTRIDENT; Dashed Lines: Bounds derived by INSTRUCTION-EXECUTION-VOLATILITY alone, Error Bars:
0.03% to 0.55% at the 95% Conﬁdence). Triangles represent FI results.
similar to the examples in Figure 2b and Figure 2c. This is a
conservative assumption, as some stores may end up being
coincidentally correct. For example, if a store instruction is
supposed to write a zero to its memory location, but is not
executed due to the faulty branch, the location will still be
correct if there was a zero already in that location. These are
called lucky loads in prior work [6]. When inputs change, the
number of lucky loads may also change due to the changes
of the distributions of such zeros in memory, possibly causing
volatility in SDC. VTRIDENT does not identify lucky loads,
so it may not capture the volatility from such occasions.
B. Implication for Mitigation Techniques
Selective instruction duplication is an emerging mitigation
technique that provides conﬁgurable fault coverage based on
performance overhead budget [9], [21], [23], [24]. The idea is
to protect only the most SDC-prone instructions in a program
so as to achieve high fault coverage while bounding perfor-
mance overheads. The problem setting is as follows: Given
a certain performance overhead C, what static instructions
should be duplicated in order to maximize the coverage for
SDCs, F , while keeping the performance overhead below C.
Solving the above problem involves ﬁnding two factors: (1)
Pi: The SDC probability of each instruction in the program,
to decide which set of instructions should be duplicated, and
(2) Oi: The performance overhead incurred by duplicating
the instructions. Then the problem can be formulated as a
classical 0-1 knapsack problem [26], where the objects are
the instructions and the knapsack capacity is represented by C,
the maximum allowable performance overhead. Further, object
proﬁts are represented by the estimated SDC probability (and
hence selecting the instruction means obtaining the coverage
F ), and object costs are represented by the performance
overhead of duplicating the instructions.
Almost all prior work investigating selective duplication
conﬁnes their study to a single input of each program in
evaluating Pi and Oi [9], [21], [23], [24]. Hence, the pro-
tection is only optimal with respect to the input used in the
evaluation. Because of the INSTRUCTION-SDC-VOLATILITY
and INSTRUCTION-EXECUTION-VOLATILITY incurred when
the protected program executes with different inputs, there is
no guarantee on the fault coverage F the protection aims to
provide, compromising the effectiveness of the selective dupli-
cation. To address this issue, we argue that the selective dupli-
cation should take both INSTRUCTION-SDC-VOLATILITY and
INSTRUCTION-EXECUTION-VOLATILITY into consideration.
One way to do this is solving the knapsack problem based
on the average cases of each Pi and Oi across inputs, so that
the protection outcomes, C and F , are optimal with respect to
the average case of the executions with the inputs. This is a
subject of future work.
IX. RELATED WORK
There has been little work investigating error propagation
behaviours across different inputs of a program. Czek et al. [7]
were among the ﬁrst to model the variability of failure rates
across program inputs. They decompose program executions
into smaller unit blocks (i.e., instruction mixes), and use the
volatility of their dynamic footprints to predict the variation
of failure rates, treating the error propagation probabilities as
constants in their unit blocks across different inputs. Their
assumption is that similar executions (of the unit blocks) result
in similar error propagations, so the propagation probabilities
within the unit blocks do not change across inputs. Thus, their
model is equivalent to considering just the execution volatility
of the program (Section III), which is not very accurate as we
show in Section IV.