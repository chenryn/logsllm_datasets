5 based on the Shannon entropy-theory [9]. In the following, we
elaborate the details of the employed MRA approach.
Wavelet Decomposition. First, we decompose the EM signals
generated by web page opening with two complementary filters: a
low-pass filter, which generates approximation coefficients, and a
high-pass filter, which generates detail coefficients. For an EM time
series, we decompose them iteratively from level 1 to level N, and
get a coefficient vector Dn as:
Dn = (an, dn, dn−1, dn−2, . . . , d1)
(5)
where an contains the approximation coefficients at level n and dn
contains the detail coefficients at level n, with 1 ≤ n ≤ N .
Wavelet Reconstruction. Then, we reconstruct an approxima-
tion signal from level 1 to level N , respectively. For each level n, we
calculate the reconstructed signal by up-sampling and convolution
from level 1 to level n with the approximation coefficient an. An
illustration is provided in Fig. 6, which shows an increasing order
of denoising from level 1 to level 4. After denoising, we extract
a feature vector for each of the N reconstructed approximation
signals using the same approach in Section 4.3.2. Then, the N fea-
ture vectors are combined in rows to form a feature matrix for
classification.
4.4.2 Weighted 1NN. Similar to application classification, we use a
1NN based classifier. The difference lies in that, the feature matrix
of a web page browsing has N rows, with each row representing
a feature vector of a wavelet reconstruction level. These feature
vectors shall have different influence on the distance measurement.
Figure 7: The experimental scenario of MagAttack. We keep
the attack device under the table to draw no attention. The
round table is 2.5 cm in thickness.
Specifically, the feature vector of level 1 represents the finest fea-
tures, and thus should be assigned with the highest weight. On
the contrary, the feature vector of level N represents the coarsest
features, and thus should be assigned with the lowest weight. As a
result, we use a weighted 1NN algorithm to calculate the distance
between two feature matrices of web page browsing. We define the
distance between two feature matrices Sp and Sq as the weighted
sum of their Euclidean distances in each row vector as follows:
N
(cid:118)(cid:116) w
n=1
i =1
Dist(p, q) =
∥Sp − Sq∥n =
wn .∥Sp − Sq∥n
(Sp(n, i) − Sq(n, i))2
(6)
(7)
wn = 2N−n
where wn is the weight assigned to the distance of the nth row
vector, based on the coarseness of its corresponding wavelet recon-
struction level. In general, the weight of the nth row distance is
assigned as:
(8)
As thus, the 1th row vector is assigned with the highest weight
w1 of 2(N−1), and the N th row vector is assigned with the lowest
weight wN of 20.
5 PERFORMANCE EVALUATION
To evaluate the performance of MagAttack, we have conducted
experiments with 13 popular applications and 50 top websites in
Session 4A: Mobile SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand289Table 2: Summary of experimental laptops.
Table 3: Summary of experimental phones.
Machine Type MacBook Air MacBook Pro
Lenovo T440p
Mac OS 10.10.5 Mac OS 10.11.1 Win 7 Home
Intel Core i5
Intel Core i5
Intel Core i5
OS Version
Processor
Memory
1.4 GHz
4-GB DDR3
1600 MHz
2.7 GHz
8-GB DDR3
1867 MHz
2.6 GHz
8-GB DDR3L
1600 MHz
China across 30 days. In summary, the performance of MagAttack
is:
• MagAttack achieves a precision of 97.0% and a recall of 92.1%
in application launching detection, an average accuracy of
98.4% to recognize 13 applications, and an average accuracy
of 84.7% to classify the 50 top websites in China.
• MagAttack can operate with little influence from data fresh-
ness, hardware platform, operating system, sensor model
and sampling rate.
5.1 Experiment Setup
We conduct MagAttack in a lab with 3 laptops and 2 smartphones.
The detailed settings are as follows.
Target Device. We use a MacBook Air laptop as the main target
device. In addition, we use a MacBook Pro and a Lenovo T440p
laptop to evaluate the performance of MagAttack on various op-
erating systems. The detailed information of each target device is
shown in Tab. 2.
Attack Device. We use an iPhone SE smartphone as the main
attack device to capture the laptop EM emissions. In addition, we use
a Nexus 5 smartphone to evaluate the performance of MagAttack
with various attack devices. The detailed information of each attack
device is shown in Tab. 3.
Attack Scenario. We utilize the attack smartphone to record
the EM emissions when the target laptop is launching different
applications or web pages. The target laptop is placed on a round
table with the attack device attached on the backside to draw no
attention, as shown in Fig. 7. The round table is 2.5 cm in thickness.
Then, we acquire the measurements from the built-in magnetometer
and transfer these EM measurements to a cloud server for further
processing.
Application/Operation. For application recognition, we
choose 13 popular applications available on Mac OS that cover
the following categories: Productivity, Business, Entertainment,
Tool, and Social Networking, with a summary in Tab. 4. For opera-
tion recognition, we take the web browser Chrome as an example
and regard the operations of opening 50 different web pages as 50
different operations. We use the top 50 web sites of China listed in
Alexa [2]. For the convenience of data collection, we use a Python
script to launch applications/web pages iteratively. Each applica-
tion/web page is being launched for 10 s considering that most
applications can be launched within 10 s, with a 5 s blank period
between two successive launchings.
5.2 Metrics
We use precision and recall to evaluate the performance of
MagAttack on launching detection, and accuracy to evaluate the
performance of MagAttack on application recognition and opera-
tion recognition.
Phone Type
OS
Sensor Rate
iPhone SE Nexus 5
Android
50 Hz
iOS
100 Hz
App
Version
Version
PowerPoint
(MacBook Air)
14.1.0
14.1.0
14.1.0
(MacBook Pro)
15.13.3
15.13.3
15.13.3
54.0.2840
Table 4: Summary of experimental applications.
ID
P
W
E
C
S
K
I
V
M
O
R
F
G
Word
Excel
Chrome
Safari
Skype
iTunes
VLC
Mail
10.0.1
7.16
12.5.3
3.0.0
10.1
15.28
2.4
4.0
2.0
OneNote
PDF Reader
FaceTime
2.2.3
8.2
15.28
2.4
3.0
2.0
Game Center
10.0.1
7.39
54.0.2840
12.5.3.17
(a) Pre-screening model performance.
Figure 8: Performance of application launching detection.
(b) SVM model performance.
Precision. Precision is denoted as
T P
F P +T P , where T P represents
the true positives, i.e., the number of times that MagAttack correctly
detects an application launching. Similarly, F P refers to the false
positives, the number of times that MagAttack falsely classifies a
time interval as an application launching.
Recall. Recall is denoted as
T P
F N +T P , where F N is the number
of time intervals that contain an application launching but are not
detected by MagAttack.
Accuracy. For each application/web page, accuracy is defined
as the ratio of the number of correctly recognized samples to the
total number of testing samples. We use the average of the accuracy
for all applications/web page as the final recognition accuracy of
MagAttack.
5.3 Launching Detection Results
We first evaluate the launching detection performance of
MagAttack. During experiments, we launch each of the 13 applica-
tions for 20 times and collect a 3900-second EM time series. We first
employ the pre-screening model to locate candidate time windows
that have high probabilities to contain application launching. The
results in the Fig. 8(a) indicate that the pre-screening model can
detect most of the launching timestamps with a recall of 97.3%,
but a few false positives may occur. The SVM model further re-
fines the pre-screening model decision by discarding false positive
Session 4A: Mobile SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand290candidates, where we adopt the cross-validation scheme to avoid
over-fitting. As a result, their combined performance achieves a
precision of 97.0% and a recall of 92.1%, as shown in Fig. 8(b).
5.4 Application Recognition Results
5.4.1 Overall Performance. In the first experiment of application
recognition, each of the 13 applications is launched for 50 times,
with different laptop-smartphone geo-spatial locations and random
laptop-smartphone orientations across 3 days. For each collection,
we record a 10-second EM sample from the moment the application
is launched. We randomly choose 10% samples as the testing set, and
use the other samples as the training set. We repeat this process for
10 times and use the average of the 10-fold cross validation as the
final results. The results in the “Fresh Data” in Fig. 9(a) demonstrate
that, MagAttack achieves an average recognition accuracy of 98.6%
across the 13 experimental applications.
Impact of Data Freshness. MagAttack achieves high perfor-
5.4.2
mance on fresh data. However, in real attacks, it is more likely that
MagAttack shall operate with old training data. To evaluate the
impact of data freshness, we collect training samples 20 days earlier
than collecting testing samples. The results shown in the “Old Data”
in Fig. 9(a) indicate that the freshness of training data has little
impact on the performance of MagAttack. We assume it is because
that applications barely change their components unless updated.
Overall, MagAttack achieves an average accuracy of 97.7% when
training data is out-of-date. One performance decrease is observed
on iTunes. We assume the reason is that iTunes updates its home
page with different music advertisements now and then, which
leads to various EM signals.
Impact of Background Application. When recognizing a
5.4.3
launched application, there might be other applications running
in the background, which generate EM emissions as well and thus
may interfere with the application recognition. To evaluate the
impact of background applications, we train the classifier using
samples without background applications, and test it using samples
with two other applications running in the background. The two
selected applications are a QuickTime application, which is play-
ing music, and a MATLAB application, which is calculating. Since
application launching mainly produces dynamic EM emissions, we
assume those active applications may introduce more interference.
The results in the “Noisy Data” in Fig. 9(a) show that MagAttack
is slightly impacted by background applications when recognizing
PowerPoint, Word, Excel and iTunes, but works well with other 9
applications.
Impact of Training Data. In practice, we cannot always obtain
5.4.4
training data from the target laptop. To investigate the impact of
training data, we collect training samples from a MacBook Air lap-
top, and testing samples from a MacBook Pro laptop. Both laptops
are installed with the 13 applications but some of them are various
in application version. The results in the “Data from Different Ma-
chines” in Fig. 9(b) demonstrate that, among the 13 applications on
different laptops, MagAttack can classify 8 of them: Chrome, Safari,
iTunes, VLC, Microsoft OneNote, PDF Reader, FaceTime and Game
center with an accuracy of 94.2%. These 8 applications, except for
the VLC player and FaceTime, have the same application version on
(a) Application recognition performance on fresh, old and noisy data.
(b) Impacts of machine diversity.
Figure 9: Performance of application recognition.
Figure 10: Performance of MagAttack with various attack dis-
tance.
both machines, as shown in Tab. 4. We find that VLC and FaceTime
are the most lightweight applications by calculating the number of
system calls executed when being launched. Therefore, we assume
that the version difference of VLC player and FaceTime has little
impact on its CPU instructions and thus the EM radiations. The rest
5 applications decrease the average recognition accuracy to 57.9%
due to the differences in both application version and laptop hard-
ware. However, we can further increase the accuracy to 94.3% by
data merging. After merging training samples from both machines,
MagAttack can recognize applications from either laptop with an
accuracy of 94.3%, as shown in the “Mixed Data” in Fig. 9(b).
Aforementioned results demonstrate that application version
and laptop model included in the training set do have impact on the
recognition performance. However, we assume that MagAttack can
achieve cross-device attack by including more application versions
and laptop models during the training process. Specifically, the
adversary can build an application library that contains common