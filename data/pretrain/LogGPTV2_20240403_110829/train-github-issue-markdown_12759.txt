In version 14.2, if the duplicate filter should detect whether the URL its
ignoring has been sent to a Spider previously. If not, it should not silently
ignore the duplicate URL. A recent Request I did had a 302 redirect to an
identical URL, before redirecting to a unique URL. It took half an hour before
I realized that the scheduler was ignoring the first redirect. There were no
debug indications or exceptions raised. I set dont_filter to True, which fixed
the situation, as expected.
Scrapy should either raise a quiet exception of some sort or realize that a
duplicate URL had not previously been sent to a spider instead of silently
ignoring duplicates.
P.S.: I don't know why the site sent me to a duplicate URL, but it had
modified cookies in the response.