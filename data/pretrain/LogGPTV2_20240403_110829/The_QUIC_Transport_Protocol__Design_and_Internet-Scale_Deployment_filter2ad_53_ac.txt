simply act as TCP-terminating proxies, proxying the TCP payload to
7The QUIC experiment is described in Section 6.1
188
The QUIC Transport Protocol
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
an unrestricted front-end server for termination of the TLS session
and further processing. There is no QUIC equivalent to a TCP-
terminating proxy, since the transport session cannot be terminated
separately from the rest of the cryptographic session. UDP-proxying
therefore simply forwards incoming client UDP packets to the un-
restricted front-end servers. This allows users getting served at the
RELs to use QUIC, since without UDP-proxying the RELs would
only be able to speak TCP.
QUIC’s performance improvement in July 2016 is attributed to the
deployment of UDP-proxying at our RELs (labeled ’3’ in Figure 6).
As a result of UDP-proxying, QUIC’s average overall improvement
in Search Latency increased from about 4% to over 7%, showing
that for this metric, QUIC’s latency reductions more than made up
for improvements from TCP termination at the RELs.
6 QUIC PERFORMANCE
In this section, we define three key application metrics that drove
QUIC’s development and deployment, and we describe QUIC’s
impact on these metrics. We also describe QUIC’s CPU utilization at
our servers and outline known limitations of QUIC’s performance.
Though we use "mobile" as shorthand throughout this paper, we
note that it refers to both a difference in operating environment
as well as a difference in implementation. The Google Search and
YouTube apps were developed independently from Chrome, and
while they share the same network stack implementation, they are
tuned specifically for the mobile environment. For example, the
Google Search app retrieves smaller responses, whose content has
been tailored to reduce latency in mobile networks. Similarly, the
YouTube app pre-warms connections to reduce video playback la-
tency and uses an Adaptive Bit Rate (ABR) algorithm that is opti-
mized for mobile screens.
Tables 1 and 2 summarize the difference between QUIC users and
TLS/TCP users on three metrics: Search Latency, Video Playback
Latency, and Video Rebuffer Rate. For each metric, the tables show
QUIC’s performance impact as a percent reduction between using
TLS/TCP and using QUIC. If QUIC decreased Search Latency from
100 seconds to 99 seconds, it would be indicated as a 1% reduction.
We describe QUIC’s performance on these metrics further below but
briefly discuss our experiment setup first.
6.1 Experiment Setup
Our performance data comes from QUIC experiments deployed
on various clients, using the clients’ frameworks for randomized
experimental trials. Users are either in the QUIC experimental group
(QUICg) or in the TLS/TCP control group (TCPg). Unless explicitly
specified, we show QUIC performance as the performance of users
in QUICg, which includes users who were unable to speak QUIC due
to failed handshakes. This group also includes data from TLS/TCP
usage prior to QUIC discovery as described in Section 3.8. Most
users in this group however are able to speak QUIC (see Section 7.2),
and most of their traffic is in fact QUIC. Clients capable of using
QUIC use TLS/TCP for only 2% of their HTTP transactions to
servers which support QUIC. The size of the QUICg and TCPg
populations are equal throughout.
Clients that do not use QUIC use HTTP/28 over a single TLS/TCP
connection for Search and HTTP/1.1 over two TLS/TCP connections
for video playbacks. Both QUIC and TCP implementations use a
paced form of the Cubic algorithm [26] for congestion avoidance. We
show data for desktop and mobile users, with desktop users accessing
services through Chrome, and mobile users through dedicated apps
with QUIC support. Since TCP Fast Open is enabled at all Google
servers, results include such connections. However TCP Fast Open
has seen limited deployment at clients (seen Section 8).
Unless otherwise noted, all results were gathered using QUIC
version 35 and include over a billion samples. All search results
were gathered between December 12, 2016 and December 19, 2016,
and all video playback results were gathered between January 19,
2017 and January 26, 2017.
6.2 Transport and Application Metrics
Before diving into application performance, we first discuss transport-
level handshake latency as a microbenchmark that QUIC seeks to
improve. We then discuss our choice of application metrics used in
the rest of this section.
Handshake latency is the amount of time taken to establish a
secure transport connection. In TLS/TCP, this includes the time
for both the TCP and TLS handshakes to complete. We measured
handshake latency at the server as the time from receiving the first
TCP SYN or QUIC client hello packet to the point at which the
handshake is considered complete. In the case of a QUIC 0-RTT
handshake, latency is measured as 0 ms. Figure 7 shows the impact
of QUIC’s 0-RTT and 1-RTT handshakes on handshake latency.
Figure 7: Comparison of handshake latency for QUICg and TCPg ver-
sus the minimum RTT of the connection. Solid lines indicate the mean
handshake latency for all connections, including 0-RTT connections.
The dashed line shows the handshake latency for only those QUICg
connections that did not achieve a 0-RTT handshake. Data shown is
for Desktop connections, mobile connections look similar.
With increasing RTT, average handshake latency for TCP/TLS
trends upwards linearly, while QUIC stays almost flat. QUIC’s hand-
shake latency is largely insensitive to RTT due to the fixed (zero)
latency cost of 0-RTT handshakes, which constitute about 88% of all
QUIC handshakes. The slight increase in QUIC handshake latency
with RTT is due to the remaining connections that do not success-
fully connect in 0-RTT. Note that even these remaining connections
complete their handshake in less time than the 2- or 3-RTT TLS/TCP
handshakes.
We do not show microbenchmarks to characterize transport-level
impact of QUIC’s improved loss recovery, but this improvement
8Google’s SPDY protocol [3] has been subsumed by the HTTP/2 standard [8].
189
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
A. Langley et al.
manifests itself as higher resilience to loss in general and lower
latency for short connections.
Microbenchmarks such as the ones above are a useful measure of
whether a transport change is working correctly, but application and
user-relevant metrics are a measure of the usefulness of the change.
The impact of QUIC’s improvements on different application metrics
is discussed in the rest of this section, but we offer two insights about
the impact of networking changes on applications.
First, networking remains just one constituent of end-to-end ap-
plication measures. For instance, handshake latency contributes to
well under 20% of Search Latency and Video Latency. An almost
complete elimination of handshake latency will still yield only a
small percentage of total latency reduction. However, even a small
change in end-to-end metrics is significant, since this impact is of-
ten directly connected to user-experience and revenue. For instance,
Amazon estimates that every 100 ms increase in latency cuts profits
by 1% [24], Google estimates that increasing web search latency
by 100 ms reduces the daily number of searches per user measur-
ably [36], and Yahoo demonstrated that users are more likely to
perform clicks on a result page that is served with lower latency [6].
Second, the sensitivity of application metrics to networking changes
depends on the maturity of the application. In the rest of this section
we describe QUIC’s impact on Google Search and YouTube. These
are highly-optimized and mature Web applications, and consequently
improving end-to-end metrics in them is difficult.
We chose these applications for two reasons. First, improving per-
formance for these highly optimized applications has direct revenue
impact. Second, they represent diverse transport use-cases: Search
represents a low-load latency-sensitive application, and YouTube
represents a heavy-load bandwidth-sensitive application.
6.3 Search Latency
Recall that Search Latency is the delay between when a user enters a
search term and when all the search-result content is generated and
delivered to the client by Google Search, including all corresponding
images and embedded content. On average, an individual search
performed by a user results in a total response load of 100 KB
for desktop searches and 40 KB for mobile searches. As a metric,
Search Latency represents delivery latency for small, delay-sensitive,
dynamically-generated payloads.
As shown in Table 1, users in QUICg experienced reduced mean
Search Latency. The percentile data shows that QUICg’s improve-
ments increase as base Search Latency increases. This improvement
comes primarily from reducing handshake latency, as demonstrated
in Figure 9 which shows desktop latency reduction for users in
9 as a function of the client’s minimum RTT to the server. As
QUICg
the user’s RTT increases, the impact of saving handshake round trips
is higher, leading to larger gains in QUICg. Figure 8 further shows
that users with high RTTs are in a significant tail: more than 20% of
all connections have a minimum RTT larger than 150ms, and 10%
of all connections have a minimum RTT larger than 300ms. Of the
handshake improvements, most of the latency reduction comes from
the 0-RTT handshake: about 88% of QUIC connections from desk-
top achieve a 0-RTT handshake, which is at least a 2-RTT latency
9For the sake of brevity we show only desktop data for these supporting graphs. Mobile
trends are similar.
190
% latency reduction by percentile
Lower latency
Higher latency
Mean 1% 5% 10% 50% 90% 95% 99%
Search
Desktop
Mobile
Video
Desktop
Mobile
8.0
0.4 1.3
3.6 -0.6 -0.3
8.0
5.3
1.2 3.1
0.0 0.6
1.4
0.3
3.3
0.5
1.5
0.5
4.6
1.2
5.8 10.3 16.7
4.5
8.8 14.3
8.4
4.4
9.0 10.6
7.5
5.8
Table 1: Percent reduction in global Search and Video Latency for users
in QUICg, at the mean and at specific percentiles. A 16.7% reduction at
the 99th percentile indicates that the 99th percentile latency for QUICg
is 16.7% lower than the 99th percentile latency for TCPg.
% rebuffer rate reduction by percentile
Fewer rebuffers
More rebuffers
93% 94 % 95% 99%
18.5
100.0
8.7
70.4 60.0
∗ 100.0 52.7
Mean < 93%
∗
18.0
∗
15.3
Desktop
Mobile
Table 2: Percent reduction in global Video Rebuffer Rate for users in
QUICg at the mean and at specific percentiles. An 18.5% reduction at
the 99th percentile indicates that the 99th percentile rebuffer rate for
QUICg is 18.5% lower than the 99th percentile rate for TCPg. An ∗ in-
dicates that neither QUICg nor TCPg have rebuffers at that percentile.
saving over TLS/TCP. The remaining QUIC connections still benefit
from a 1-RTT handshake.
Figure 8: Distribution of connection minimum RTTs for TCP connec-
tions to our servers. These results were gathered from video playbacks.
The distribution is similar for search connections.
We believe that QUIC’s loss recovery mechanisms may also play
a role in decreasing Search latency at higher RTTs. Recall that
QUIC includes richer signaling than TCP, which enables QUIC
loss recovery to be more resilient to higher loss rates than TCP
(see Section 3.4). Figure 10 shows the relationship between TCP
retransmission rates measured at our servers against minimum client
RTTs. Employing TCP retransmission rate as a proxy for network
loss, this figure shows that network quality is highly correlated with
the client’s minimum RTT. Consequently, QUIC’s improved loss
recovery may also contribute to Search Latency improvement at
higher RTTs.
Table 1 shows that Search Latency gains on mobile are lower
than gains on desktop. In addition to differences between desktop
and mobile environments and usage, the lower gains are explained
The QUIC Transport Protocol
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
config contains the server’s credentials. First, when mobile users
switch networks, their IP address changes, which invalidates the
source-address token cached at the client. Second, different server
configurations and keys are served and used across different data
centers. When mobile users switch networks, they may hit a different
data center where the servers have a different server config than that
cached at the client. Analysis of server logs shows that each of these
two factors contributes to about half of the reduction in successful
0-RTT handshakes.
Finally, we attribute the latency increase in QUICg at the 1st
and 5th percentiles to additional small costs in QUIC, including
OS process scheduler costs due to being in user-space, which are a
higher proportion of the total latency at low overall latencies. We
discuss QUIC’s limitations further in Section 6.8.
6.4 Video Latency
Video Latency for a video playback is measured as the time between
when a user hits "play" on a video to when the video starts playing.
To ensure smooth playbacks, video players typically buffer a couple
seconds of video before playing the first frame. The amount of data
the player loads depends on the bitrate of the playback. Table 1 shows
that users in QUICg experience decreased overall Video Latency for
both desktop and mobile YouTube playbacks.
Figure 9 shows that Video Latency gains increase with client RTT,
similar to Search Latency. An average of 85% of QUIC connections
for video playback on desktop receive the benefit of a 0-RTT hand-
shake, and the rest benefit from a 1-RTT handshake. As with Search
Latency, QUIC loss recovery improvements may help Video Latency
as client RTT increases.
QUIC benefits mobile playbacks less than desktop. The YouTube
app achieves a 0-RTT handshake for only 65% of QUIC connections.
Additionally, the app tries to hide handshake costs, by establishing
connections to the video server in the background while users are
browsing and searching for videos. This optimization reduces the
benefit of QUIC’s 0-RTT handshake, further reducing gains for
mobile video in QUICg.
6.5 Video Rebuffer Rate
To ensure smooth playback over variable network connections, video
players typically maintain a small playback buffer of video data. The
amount of data in the buffer varies over time. If the player reaches
the end of the buffer during playback, the video pauses until the
player can rebuffer data. Video Rebuffer Rate, or simply Rebuffer
Rate is the percentage of time that a video pauses during a playback
to rebuffer data normalized by video watch time, where video watch
time includes time spent rebuffering. In other words, Rebuffer Rate is
computed as (Rebuffer Time) / (Rebuffer Time + Video Play Time).
Table 2 indicates that users in QUICg experience reduced Rebuffer
Rate on average and substantial reductions at higher percentiles.
These results are qualitatively different from Search Latency and
Video Latency since the contributing factors are different: Rebuffer
Rate is largely insensitive to handshake latency. It is instead influ-
enced by loss-recovery latency, since missing data on an audio or
video stream can stall video playback. It is also influenced by the
connection’s overall throughput, which determines the rate at which
video is delivered to the client.
Figure 9: Comparison of QUICg and TCPg for various metrics, versus
minimum RTT of the connection. The y-axis is normalized against the
maximum value in each dataset. Presented data is for desktop, but the
same trends hold for mobile as well. The x-axis shows minimum RTTs
up to 750 ms, which was chosen as reasonable due to Figure 8: 750 ms
encompasses over 95% of RTTs and there is no information gained by
showing more data.
Figure 10: Average TCP retransmission rate versus minimum RTT ob-