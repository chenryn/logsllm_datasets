### Automatic Model Selection and Workflow Management

The system automatically selects the model with the best parameters and features, which can be translated into more complex workflows. These workflows can run multiple tasks in parallel, thereby reducing the overall processing time compared to sequential execution. The Data Science Services operate independently and in parallel, further enhancing efficiency. Additionally, the architecture allows for independent scaling of other services, enabling users to keep data science workflows running even after closing their browsers and return later to visualize the final results.

### 3.3 User Interface

The user interface was designed to be minimalistic and simple, allowing users to derive maximum value with minimal effort. The interface is divided into two key areas, as visualized in Fig. 3. On the left, a sidebar with a dark background provides access to saved workflows and uploaded datasets. Users can start or stop workflows from this sidebar. When the start button is clicked, the workflow is sent to the workflows-service for execution, and the user can monitor its progress or stop it at any time.

On the right side of the interface, users can add tasks to the workflow. When adding a task, the system displays only the relevant types of tasks based on the current state of the workflow, ensuring a guided and uncluttered experience. This chaining of tasks helps users construct workflows more efficiently.

There are six types of tasks that users can perform:
- **Dataset Input**: A task where the user specifies the dataset to use.
- **Validation Procedure**: Each task specifies the method used for validating subsequent tasks. This is particularly useful when adding a model creation task.
- **Preprocessing**: Tasks that apply transformations to attribute values, such as feature scaling.
- **Feature Selection**: Tasks where the user can filter attributes based on input parameters, such as the ReliefF algorithm.
- **Model Creation**: Tasks that create machine learning models.
- **Model Evaluation**: Specifies metrics for model evaluation, such as accuracy or F-measure.

### 4 Experimental Setup

Usability tests played a crucial role in evaluating the prototype and validating the paradigm of visual programming using sequential tasks. The tests involved users executing a series of exercises using the interface and providing feedback. This feedback was used to assess the user experience, the usability of the interface, and the value provided by the platform.

We categorized the users into two types:
- **Type A**: Users with no experience in data mining, consisting of seven researchers (four with a master's degree in ecology and three with a doctoral degree in biology).
- **Type B**: Users with some knowledge of data mining but not programmers, comprising eleven students enrolled in a master’s degree in biochemistry and taking a course in data mining.

The testing process was divided into several steps:
1. **Introduction**: A brief overview of the platform and its functionalities, lasting less than three minutes.
2. **Exercises**: Users were given a problem and a list of exercises to solve it. The exercises primarily involved the data science tasks mentioned in Section 3.3. The challenge was estimated to take about 20 minutes.
3. **Questionnaire**: Users filled out a questionnaire about their experience and thoughts on the relevance of the platform. The questions were written in Portuguese but translated to English for this paper.

### 4.1 The Iris Flower Dataset Problem

To keep the tests concise and straightforward, we used the iris flower dataset, a common problem in data science training. The dataset, collected by Edgar Anderson, quantifies morphological variation in iris flowers of three related species: Iris Setosa, Iris Versicolour, and Iris Virginica. It includes measurements of the species' petals and sepals.

The test was divided into five exercises:
1. **Attribute Scaling**: Scale the dataset attributes between 0 and 1.
2. **Dataset Splitting**: Split the dataset into training and test sets (60/40%). The training set would be used to train an SVM model, and the test set would evaluate accuracy and F-measure.
3. **Combined Task**: Combine the first two exercises to demonstrate the flexibility of adding and removing tasks from the workflow.
4. **ReliefF Algorithm**: Add the ReliefF algorithm to identify the most predictive attributes.
5. **K-Fold Cross Validation**: Use the best two attributes from the previous exercise and add K-fold cross-validation to complete the model creation.

These exercises were designed to be simple and interconnected, providing a sense of progress during execution.

### 5 Results

#### 5.1 Questionnaire

The questionnaire allowed us to gauge user satisfaction with the interface, their experience using the tool, and its perceived usefulness. Responses were rated on a scale from "totally disagree" to "totally agree," with corresponding numerical values from 1 to 5.

As shown in Fig. 4, the responses were generally positive. Users found the interface easy to use, would recommend it to colleagues, and would use it again for similar problems. The attractiveness of the interface, while positive, scored lower than other metrics, which was expected for a prototype. Type A users (no experience) had slightly lower scores but found the required functionalities easier to locate and the design simpler to understand. Statistical tests (unpaired T-tests and Wilcoxon rank sum tests) showed that the only significant difference between the two groups was in understanding the assigned exercises, with Type B users (some experience) finding them easier.

#### 5.2 Feedback

Users also provided written feedback, including suggestions, critiques, and positive comments:
- **Key Suggestions**:
  - Allow users to view all added tasks in the workflow simultaneously, rather than scrolling.
  - In the dataset input, change the option from selecting attributes to remove to selecting attributes to include.
- **Key Critiques**:
  - Some users did not intuitively know the type of certain tasks, such as feature scaling. This could be addressed by improving the task selection process or providing a searchable task library.
  - The requirement to copy and paste the dataset URI was not intuitive; alternative methods should be considered.
- **Positive Feedback**:
  - Simplicity, accessibility, and design.
  - Low learning curve and ease of use.
  - Fast experiment execution and result delivery.
  - Intuitiveness.
  - No installation required, accessible anywhere with internet.
  - Chained tasks guiding the workflow construction.
  - Grid search.
  - Direct and informative outputs.

This feedback reinforced the questionnaire results and was very satisfactory, aligning with our objectives for the application.

### 6 Conclusion

In this work, we presented a service for non-programmers to build data science experiments using good data mining practices. We developed a cloud-based application following a microservices architecture, with a focus on simplicity and usability. Usability tests with experienced and non-experienced users validated the concept of visual programming using sequential tasks. The results were positive, with no critical feedback related to our goals. Future plans include adding predefined data science workflow templates, comparative benchmarks with other platforms, and further improvements based on user feedback.

### Acknowledgments

This work was supported by the project PTDC/EEI-ESS/1189/2014 Data Science for Non-Programmers, funded by COMPETE 2020, Portugal 2020-POCI, UE-FEDER, and FCT.

### References

1. Manyika, J., et al. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey & Company.
2. Henke, N., et al. (2016). The age of analytics: Competing in a data-driven world. McKinsey & Company.
3. Miller, S., & Hughes, D. (2017). The Quant Crunch: How the Demand For Data Science Skills is Disrupting the Job Market. Burning Glass Technologies.
4. Becoming a Data Scientist Curriculum via Metromap. Last accessed 14 Jun 2018.
5. Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996). The KDD process for extracting useful knowledge from volumes of data. Communications of the ACM, 39(11), 27-34.
6. Domingos, P. (2012). A few useful things to know about machine learning. Communications of the ACM, 55(10), 77-87.
7. Hastie, T., Tibshirani, R., & Friedman, J. (2001). The Elements of Statistical Learning. 2nd edn. Springer.
8. Cawley, G. C., & Talbot, N. L. (2010). On over-fitting in model selection and subsequent selection bias in performance evaluation. Journal of Machine Learning Research, 11(07), 2079-2107.
9. AzureML. Last accessed 14 Jun 2018.
10. H2O.ai. Last accessed 14 Jun 2018.
11. Orange. Last accessed 14 Jun 2018.
12. Weka. Last accessed 14 Jun 2018.
13. RapidMiner. Last accessed 14 Jun 2018.
14. Kranjc, J., et al. (2017). ClowdFlows: Online workflows for distributed big data mining. Future Generation Computer Systems, 68, 38-58.
15. Scikit-Learn. Last accessed 14 Jun 2018.
16. Medvedev, V., et al. (2017). A new web-based solution for modelling data mining processes. Simulation Modelling Practice and Theory, 76, 34-46.
17. Krstajic, D. (2014). Cross-validation pitfalls when selecting and assessing regression and classification models. Journal of Cheminformatics.
18. Zorrilla, M., & Garc´ıa-Saiz, D. (2013). A service-oriented architecture to provide data mining services for non-expert data miners. Decision Support Systems, 55(1), 39-94.
19. Netflix Conductor. Last accessed 14 Jun 2018.
20. Competing consumers pattern. Last accessed 14 Jun 2018.
21. Anderson, E. (1936). The Species Problem in Iris. Annals of the Missouri Botanical Garden, 23(3), 457-509.