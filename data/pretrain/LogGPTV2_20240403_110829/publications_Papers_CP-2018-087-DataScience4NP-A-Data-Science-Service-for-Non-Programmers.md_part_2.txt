automatic selection of the model with best parameters and features, these can
be translated into more complex workflows, however with several tasks running
in parallel.
With the Data Science Services working independently and in parallel in
these tasks, it is expected that the required time to process the entire workflow
will be reduced compared to running all the tasks sequentially.
The remaining services that compose the architecture can also be scaled out
independently. The architecture also enables users to let data science workflows
running, even after closing their browsers, and come back later to visualize the
final results.
3.3 User Interface
Theuserinterfacewasdesignedwiththeobjectiveofcreatingaminimalisticand
simple application where the user can produce the most value with the lowest
amountofeffort.Theinterfaceisdividedto2keyareas.InFig.3wecanvisualize
the interface used in the usability tests; on the left, with a dark background, we
have a sidebar where the user has his saved workflows and uploaded datasets;
he can also run or stop the workflow. When a user clicks the start button he
is sending the workflow to the workflows-service to execute and wait for its
completion.Theusercanalsostopitatanytimeduringitsexecution.Thearea
on the right is where the user can add tasks to the workflow, which will then
later be executed.
DataScience4NP - A Data Science Service for Non-Programmers 7
Fig.3. Interface of the prototype used in the usability tests
When adding a task, the user is shown the types of tasks that can be added.
This means that when the user clicks the plus button to add a task, depending
on the current state of the workflow, he can only see the tasks that can follow
and is not cluttered with all tasks at once. By doing this, the tasks are chained
together, guiding the user during the construction process.
There are 6 types of tasks that the user can perform:
– Dataset input: a task where the user specifies the dataset to use.
– Validation procedure: each task specifies the method used for the valida-
tionofallothersubsequenttasks.Onlymakessenseiftheuseraddsamodel
creation type of task.
– Preprocessing: tasks that apply transformations to attribute values, such
as feature scaling.
– Feature selection: tasks where the user can filter attributes based on the
input parameters, such as the relieff algorithm.
– Model creation:isthetypeoftasksthatcreatesmachinelearningmodels.
– Model Evaluation: specifies the metrics for model evaluation, such as ac-
curacy or f-measure.
4 Experimental Setup
The usability tests provided a crucial role in evaluating the prototype and val-
idating the paradigm of visual programming using sequential tasks. The tests
consisted in having the users execute a few exercises using the interface and
getting their feedback. This feedback was then used to evaluate the users’ expe-
rience, the usability of the interface and value that was provided to them, hence
validating this concept of visual programming applied to data science.
We divided the users to 2 types:
8 Lopes, Bruno Leonel et al.
– Type A: Users with no experience at all and no knowledge in data mining,
composed by a group of researchers, four with a masters degree in ecology
and three with a doctoral degree in biology (7 users).
– Type B: Users that knew about data mining but were not programmers.
Thisgroupwascomposedbystudentswhowereenrolledinamaster’sdegree
in biochemistry and were undertaking a course in data mining (11 users).
The process was separated to different steps: The first step started with a
quick overview of the platform and its functionalities, which took less than 3
minutes. After this introduction and answering any questions the users might
have, we gave them a paper with a problem and a list of exercises for them to
perform in order to solve that problem. The exercises fundamentally consisted
inusingthedatasciencetasksmentionedinsection3.3.Iftheuserssuccessfully
finished the exercises they would have solved the problem. This challenge was
estimated to take about 20 minutes. The last step was a questionnaire that the
users had to fill about their experience, and their thoughts on the relevance of
this platform. The questions were written in Portuguese but were translated to
English for this paper.
4.1 The iris flower dataset problem
In order to keep the tests brief and not overly complicated we decided to intro-
duceoneofthecommonproblemsnewdatascientistslearnduringtheirtraining:
the iris flower dataset. This data was collected by Edgar Anderson to quantify
the morphological variation in iris flowers of three related species [21]. It con-
tains a total of three species of iris: Iris Setosa, Iris Versicolour, Iris Virginica;
and consists in the measurements of the species of iris and the dimensions of its
petals and sepals (centimeters).
Based on the measurements, the users would then create a model that could
predict the species of iris. The test was separated to 5 exercises:
1. The first exercise consisted in scaling the attributes of the dataset between
0 and 1.
2. Exercise two required the user to split the dataset to training and testsets
(60/40%). The one for training would later be used to train the SVM model
and the one for testing to see the accuracy and f-measure metrics.
3. Exercise three combined the first and second one. This was set to show the
user that tasks can be added and removed from the workflow and applying
feature scaling to an already created workflow was at a distance of a few
clicks.
4. InexercisefourtheuserwasaskedtoaddtheRelieffalgorithmtothework-
flow in order to see what attributes would have the most predictive capabil-
ities.
5. Exercisenumberfiveusedthebesttwoattributesdiscoveredintheprevious
exercise and added the validation procedure called K-fold cross validation,
hence completing the assignment and creating a model.
DataScience4NP - A Data Science Service for Non-Programmers 9
The exercises were simple and intertwined making the user have a feeling of
progress during their execution.
5 Results
5.1 Questionnaire
The questionnaire allowed us to know how much the users liked the interface,
theirexperienceusingthetoolandiftheyfoundituseful.Eachstatementcould
be answered as: totally disagree, disagree, indecisive, agree and totally agree. In
order to analyze the average response and the standard deviation we converted
the answers to numbers, where number 1 translates to ”totally disagree” and 5
to ”totally agree”.
Fig.4. Average and standard deviation of the users’ responses
As seen in Fig. 4 the values are all above average. The most satisfactory
results where that they found the interface easy to use, they would recommend
it to colleagues and that they would use it again to solve related problems. The
attractiveness of the interface, even though it was very positive, scored lower
10 Lopes, Bruno Leonel et al.
thantheothermetrics;thiswasexpectedsincethisisaprototypeandthatpart
wasnotapriority.TheresultsacquiredfromthetypeAusersarelowerthanthe
onesfromtypeB.Thisshowedthattheuserswithnoexperience(typeA)hadan
higherdifficultyusingtheinterface,butsurprisinglytheyfoundeasiertofindthe
requiredfunctionalitiesandthedesignsimplertounderstand.Toassesswhether
the differences in the answers among the two populations were statistically sig-
nificant, we performed unpaired statistically significant tests. Before that, both
distributions were tested for Gaussianity using the Kolmogorov-Smirnov test.
In case both distributions were Gaussian, an unpaired T-test was conducted;
otherwise, a Wilcoxon rank sum test was performed. Hence, for each of the ten
questions, only the question ”You understood the exercises that were assigned”
showed statistical significant differences among the two populations (at p <
0.05). We hypothesise that it was easier for the type A subjects to understand
the exercises because they had experience in data mining and knew about the
Iris dataset since it is a very common problem to teach new data scientists.
5.2 Feedback
Besides answering the questionnaire the users also had a place to write sugges-
tions, critiques and things they liked better in the application. Bellow we have
a list with the compilation of comments the users wrote for each of these topics.
Key suggestions:
– It should be possible to see all the tasks that were added to the workflow at
all times. At the moment the user needs to scroll up and down to see and
edit the tasks.
– In the dataset input, the option of selecting attributes to remove from the
dataset should be replaced with attributes to select.
Key critiques:
– Sometimestheusersdidnotknowthatataskbelongedtoacertaintype,e.g
feature scaling is a task that is of preprocessing type but some users when
asked to use it did not intuitively know that it was of that type. This is a
problem because the users first select the type of task they want to choose
from and then the task itself. This problem can be solved by finding an
alternative for the way users add tasks to the workflow or having a place
where the users can search for all available tasks and read more information
about them. Since this is a prototype and at the moment we do not support
thatmanydifferenttasks,theusersfoundthattaskinafewseconds.However
but if there were more tasks it would be harder.
– To use a dataset in any workflow the users must copy the dataset’s uri
that is shown in the sidebar and paste it to the dataset input task. Some
users did not found that intuitive and another alternative should be taken
to consideration.
Things they liked the most:
DataScience4NP - A Data Science Service for Non-Programmers 11
– Simplicity, accessibility and design.
– Low learning curve and easiness to use.
– How fast it was to run an experiment and get the results.
– Intuitiveness.
– Itdoesnotrequireanyinstallationanditcanbeusedanywherewithinternet
access.
– The tasks were chained together guiding the process of constructing the
workflow.
– Grid search.
– The outputs are direct and very informative.
This feedback reinforced what was discovered during the questionnaire and
was very satisfactory, none of the critiques were about the concept we aim to
proveandthethingstheylikedthemostwereinlinewiththeobjectiveswetried
to achieve when building the application.
6 Conclusion
In this work we presented a service for non-programmers to build Data Science
experiments employing good data mining practices. We prototyped a cloud ap-
plication that follows a microservices architecture. The interface built tried to
achieve a high degree of simplicity and usability. To test it, experiments were
madewithexperiencedandnon-experienceduserstoevaluatetheprototypeand
validatetheparadigmofvisualprogrammingusingsequentialtasks.Theresults
were satisfactory with a positive feedback and without critiques related to what
we are trying to achieve. In the future we plan to add predefined data science
workflow templates that might be searched, changed and shared by the users,
as well as make comparative benchmarks with other platforms. Regarding the
usabilitytestsweplantoimprovetheapplicationbymakingchangestotheuser
interface according to the feedback received.
Acknowledgments
This work was carried out under the project PTDC/EEI-ESS/1189/2014 Data
Science for Non-Programmers, supported by COMPETE 2020, Portugal 2020-
POCI, UE-FEDER and FCT.
References
1. Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., Hung
Byers,A.:Bigdata:Thenextfrontierforinnovation,competition,andproductivity.
McKinsey & Company, (2011).
2. Henke,N.,Bughin,J.,Chui,M.,Manyika,J.,Saleh,T.,Wiesman,B.,Sethupathy,
G.:Theageofanalytics:Competinginadata-drivenworld.McKinsey&Company,
2016.
12 Lopes, Bruno Leonel et al.
3. Miller,S.,Hughes,D.:TheQuantCrunch:HowtheDemandForDataScienceSkills
is Disrupting the Job Market. Burning Glass Technologies, 2017.
4. Becoming a Data Scientist Curriculum via Metromap,
http://nirvacana.com/thoughts/2013/07/08/becoming-a-data-scientist/. Last
accessed 14 Jun 2018
5. Fayyad,U.,Piatetsky-Shapiro,G.,Smyth,P.:TheKDDprocessforextractinguseful
knowledgefromvolumesofdata.CommunicationsoftheACM,39(11),2734,(1996).
6. Domingos,P.:Afewusefulthingstoknowaboutmachinelearning.Communications
of the ACM, 55(10) 77-87, (2012)
7. Hastie, T., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning. 2nd
edn. Springer, (2001)
8. Cawley, G. C., Talbot, N. L.: On over-fitting in model selection and subsequent
selection bias in performance evaluation. Journal of Machine Learning Research,
11(07), 2079-2107, 2010
9. AzureML, https://studio.azureml.net/. Last accessed 14 Jun 2018
10. H2O.ai, https://www.h2o.ai/. Last accessed 14 Jun 2018
11. Orange, https://orange.biolab.si/. Last accessed 14 Jun 2018
12. Weka, https://www.cs.waikato.ac.nz/ml/weka/. Last accessed 14 Jun 2018
13. RapidMiner, https://rapidminer.com/. Last accessed 14 Jun 2018
14. Kranjc, J., Oraˇc, R., Podpeˇcan, V., Lavraˇc, N., Robnik-Sˇikonja, M.: ClowdFlows
: Online workflows for distributed big data mining. Future Generation Computer
Systems, 68 3858 (2017)
15. Scikit-Learn,http://scikit-learn.org/stable/index.html.Lastaccessed14Jun2018
16. Medvedev, V., Kurasova, O., Bernataviˇsiene˙, J., Treigys, P., Marcinkeviˇsius, V.,
Dzemyda,G.:Anewweb-basedsolutionformodellingdataminingprocesses.Sim-
ulation Modelling Practice and Theory, 76 3446 (2017).
17. Krstajic,Damjan:Cross-validationpitfallswhenselectingandassessingregression
and classification models. Journal of Cheminformatics,
18. Zorrilla,M.Garc´ıa-Saiz,D.:Aserviceorientedarchitecturetoprovidedatamining
servicesfornon-expertdataminers.DecisionSupportSystems,55(1)399411(2013)
19. NetflixConductor,https://netflix.github.io/conductor/.Lastaccessed14Jun2018
20. Competing consumers pattern, https://docs.microsoft.com/en-
us/azure/architecture/patterns/competing-consumers. Last accessed 14 Jun
2018
21. Edgar Anderson: The Species Problem in Iris. Annals of the Missouri Botanical
Garden, 23(3) 457-509 (1936)