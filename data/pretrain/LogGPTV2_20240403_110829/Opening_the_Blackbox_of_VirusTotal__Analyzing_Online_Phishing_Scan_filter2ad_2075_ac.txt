Digital Ocean, M247, Feral Hosting, and Linode. It is likely that
security vendors are trying to hide their identity to overcome the
cloaking of phishing sites [32].
So far, our
Additional Experiments on Label Updating.
main experiment shows that it takes two scan requests to push
the scanning results back to VirusTotal (§4(b)). However, the previ-
ous experiment is limited to new URLs that are never detected by
vendors before. A follow up question is, what if the URL is already
blacklisted by the third-party vendor? Do we still need two requests
to push the label to VirusTotal? To answer this question, we per-
formed a small controlled experiment. We set up three fresh PayPal
 0 4 8 12 16 1 2 3 4 5# Detected SitesWeekFortinetAviraCyRadarCLEAN MXIMC ’19, October 21–23, 2019, Amsterdam, Netherlands
P. Peng, L. Yang, L. Song, and G. Wang
Obfuscation
Method
Redirection
Image
PHP Code
# Sites
(PayPal) Min. Max.
12
2
6
2
2
3
12
3
1
Malicious Labels Per Site
Avg.
12
4.5
2
Table 5: The number of “malicious” labels per site after ap-
plying different obfuscation methods.
pages under three new domain names. Then we choose NetCraft,
Forcepoint, and Fortinet which are capable of detecting the Pay-
Pal page in the main experiment. We first submit the three URLs to
individual vendors for scanning (one URL per vendor). Same as be-
fore, the URLs get immediately blacklisted by the respective vendor.
Then we submit the URLs to VirusTotal for the first scan. VirusTotal
returns a “benign” label for all the URLs. After 4 days, we submit the
URL to VirusTotal for the second scan. Interestingly, the returned
labels are still “benign”. This indicates NetCraft, Forcepoint, and
Fortinet do not share their blacklists with VirusTotal. Otherwise,
the labels should have been “malicious” after the second VirusTotal
scan. It is more likely that VirusTotal runs stripped-down versions
of the scanners that fail to detect the phishing pages .
Obfuscation is used to deliberately
Impact of Obfuscation.
make it harder to understand the intent of the website. In this
case, the attacker can apply simple changes so that their website
still looks like the target website, but the underlying content (e.g.,
code) becomes harder to analyze. We examine the impact of three
obfuscation methods: (1) Redirection: we use a URL shortener
service to obfuscate the phishing URL. (2) Image-based Obfuscation:
we take a screenshot of the PayPal website, and use the screenshot
as the background image of the phishing site. Then we overlay the
login form on top of the image. In this way, the phishing site still
looks the same, but the HTML file is dramatically different. (3) PHP
Code Obfuscation: within the original PHP code, we first replace
all user-defined names with random strings (without affecting the
functionality). Then we remove all the comments and whitespace,
and output encoding in ASCII. For each of the obfuscation methods,
we build 2 new PayPal sites (6 sites in total). We submit the URLs
to VirusTotal for scan, wait for a week, submit again (to trigger
database update), and retrieve the labels.
Table 5 shows the number of malicious labels per site. As a
comparison baseline, without obfuscation, the PayPal site in the
main experiment (§4) received 12.1 malicious labels on average. This
number is calculated based on the first scan of week-2 in the main
experiment (instead of the four weeks of result) to be consistent
with the setting of the obfuscation experiment. We observe that
redirection does not help much. However, image and code-based
obfuscations are quite effective — the average number of malicious
labels drops from 12.1 to 4.5 and 2 respectively. This suggests that
these vendors are still unable to handle simple obfuscation schemes.
To see the impact of robots.txt, we set up 18
Robots.txt.
new domains where the robots.txt disallows crawling. Then we
submit these 18 URLs to the 18 vendors’ scan APIs. We find that the
traffic volumes are still comparable with the previous experiment.
The result indicates that most scanners would ignore robots.txt.
All the experiments so far are
Detection of Benign Pages.
focused on phishing pages. A quick follow-up question is how well
can VirusTotal detect benign pages. We did a quick experiment
by setting up one benign page under a new domain name (a long
random string as before). The page is a personal blog, and it does
not try to impersonate any other brand. We submit the URL to
VirusTotal scan API twice with 3 days apart, and then monitor the
label for a month. We find that the labels are always “benign”. Given
the limited scale of this experiment, it is not yet conclusive about
VirusTotal’s false positive rate. At least, we show that VirusTotal
did not incorrectly label the website as “malicious” just because it
has a long random domain name.
6 DISCUSSIONS & OPEN QUESTIONS
Our experiments in §4 and §5 collectively involve 66 (38+28) exper-
imental websites. We show that vendors have an uneven detection
performance. In the main experiment, only 15 vendors have de-
tected at least one site. Even the best vendor only detected 26 out
of 36 sites. Given that vendors have an uneven capability, their
labels should not be treated equally when aggregating their results.
In addition, we show the delays of label updating due to the non-
proactive “pull” method of VirusTotal. We also illustrate the label
inconsistency between VirusTotal scan and the vendors’ own scans.
As a simple best-practice, we suggest future researchers scanning
the URLs twice to obtain the updated labels and cross-checking the
labels with the vendors’ own APIs.
Our experiments have a few limitations. First, the
Limitations.
long domain names may affect the detection accuracy. However, we
argue that the long domain names actually make the websites look
suspicious, and thus make the detection easier. The fact that certain
scanners still fail to detect the phishing sites further confirms the
deficiency of scanners. Second, the use of “fresh” domain names
may also affect the detection performance of vendors, since certain
vendors might use “infection vendors” as features (e.g., reports
from the victims of a phishing site). In practice, the vendors might
perform better on phishing sites that already had victims.
During our experiments, we observe interesting
Future Work.
phenomena that lead to new open questions. First, the vendors’
models perform much better on PayPal pages than on IRS pages.
Future work can further investigate the “fairness” of vendors’ classi-
fiers regarding their performance on more popular and less popular
phishing brands. Second, we observe that some vendors always
detect the same subset of phishing sites (Table 4). If these vendors in-
deed fully synchronize their labels, then their labels are essentially
redundant information. As such, these vendors should not be treated
as independent vendors when aggregating their votes. Future work
can further investigate the correlation of results between differ-
ent vendors. Third, many vendors (e.g., Kaspersky, Bitdefender,
Fortinet) also provide API for file scanning to detect malware. File
scan can be studied in a similar way, e.g., submitting “ground-truth”
malware and benign files to evaluate the quality of labels and the
consistency between vendors and VirusTotal.
ACKNOWLEDGEMENT
We would like to thank our shepherd Gianluca Stringhini and the
anonymous reviewers for their helpful feedback. This project was
supported by NSF grants CNS-1750101 and CNS-1717028.
Analyzing Online Phishing Scan Engines
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
[11] Virustotal
vendors.
115002146809-Contributors.
https://support.virustotal.com/hc/en-us/articles/
REFERENCES
[1] Digital ocean. https://www.digitalocean.com/.
[2] Irs login page. https://sa.www4.irs.gov/ola/.
[3] Joe sandbox. https://www.joesecurity.org/.
[4] Jotti’s malware scan. https://virusscan.jotti.org/.
[5] Namesilo. https://www.namesilo.com/.
[6] Paypal login page. https://www.paypal.com/us/signin.
[7] Virscan. http://VirSCAN.org.
[8] Virustotal. https://www.virustotal.com/.
[9] Virustotal faq. https://support.virustotal.com/hc/en-us/articles/115002122285-
AV-product-on-VirusTotal-detects-a-file-and-its-equivalent-commercial-
version-does-not.
[10] Virustotal public api v2.0. https://www.virustotal.com/en/documentation/public-
api/.
[12] Akhawe, D., and Felt, A. P. Alice in warningland: A large-scale field study of
[23] Kim, D., Kwon, B. J., and Dumitraş, T. Certified malware: Measuring breaches
[13] Aonzo, S., Merlo, A., Tavella, G., and Fratantonio, Y. Phishing attacks on
[14] Ardi, C., and Heidemann, J. Auntietuna: Personalized content-based phishing
[16] Catakoglu, O., Balduzzi, M., and Balzarotti, D. Automatic extraction of
[15] Cai, Z., and Yap, R. H. Inferring the detection logic and evaluating the effective-
browser security warning effectiveness. In Proc. of USENIX Security (2013).
modern android. In Proc. of CCS (2018).
detection. In NDSS Usable Security Workshop (USEC) (2016).
ness of android anti-virus apps. In Proc. of CODASPY (2016).
indicators of compromise for web applications. In Proc. of WWW (2016).
[17] Chen, Y., Nadji, Y., Romero-Gómez, R., Antonakakis, M., and Dagon, D.
Measuring network reputation in the ad-bidding process. In Proc. of DIMVA
(2017).
[18] Cheng, B., Ming, J., Fu, J., Peng, G., Chen, T., Zhang, X., and Marion, J.-Y.
Towards paving the way for large-scale windows malware analysis: Generic
binary unpacking with orders-of-magnitude performance boost. In Proc. of CCS
(2018).
[19] Dong, Z., Kapadia, A., Blythe, J., and Camp, L. J. Beyond the lock icon: real-time
detection of phishing websites using public key certificates. In Proc. of eCrime
(2015).
[20] Hong, G., Yang, Z., Yang, S., Zhang, L., Nan, Y., Zhang, Z., Yang, M., Zhang,
Y., Qian, Z., and Duan, H. How you get shot in the back: A systematical study
about cryptojacking in the real world. In Proc. of CCS (2018).
[21] Invernizzi, L., Thomas, K., Kapravelos, A., Comanescu, O., Picod, J., and
Bursztein, E. Cloak of visibility: Detecting when machines browse a different
web. In Proc. of IEEE S&P (2016).
[22] Kantchelian, A., Tschantz, M. C., Afroz, S., Miller, B., Shankar, V., Bach-
wani, R., Joseph, A. D., and Tygar, J. D. Better malware ground truth: Techniques
for weighting anti-virus vendor labels. In Proc. of AISec (2015).
of trust in the windows code-signing pki. In Proc. of CCS (2017).
[24] Kim, D., Kwon, B. J., Kozák, K., Gates, C., and DumitraÈŹ, T. The broken
shield: Measuring revocation effectiveness in the windows code-signing pki. In
Proc. of USENIX Security (2018).
[25] Kleitman, S., Law, M. K., and Kay, J. ItâĂŹs the deceiver and the receiver:
Individual differences in phishing susceptibility and false positives with item
profiling. PLOS One (2018).
tions and code-reuse attacks. In Proc. of CCS (2017).
[27] Kwon, B. J., Mondal, J., Jang, J., Bilge, L., and Dumitraş, T. The dropper effect:
Insights into malware distribution with downloader graph analytics. In Proc. of
CCS (2015).
[28] Lever, C., Kotzias, P., Balzarotti, D., Caballero, J., and Antonakakis, M. A
lustrum of malware network communication: Evolution and insights. In Proc. of
IEEE S&P (2017).
[29] Li, B., Vadrevu, P., Lee, K. H., Perdisci, R., Liu, J., Rahbarinia, B., Li, K., and
Antonakakis, M. Jsgraph: Enabling reconstruction of web attacks via efficient
tracking of live in-browser javascript executions. In Proc. of NDSS (2018).
[30] Miramirkhani, N., Barron, T., Ferdman, M., and Nikiforakis, N. Panning
for gold.com: Understanding the dynamics of domain dropcatching. In Proc. of
WWW (2018).
[31] Neupane, A., Saxena, N., Kuruvilla, K., Georgescu, M., and Kana, R. K. Neural
signatures of user-centered security: An fmri study of phishing, and malware
warnings. In Proc. of NDSS (2014).
[32] Oest, A., Safaei, Y., Doupé, A., Ahn, G., Wardman, B., and Tyers, K. Phishfarm:
A scalable framework for measuring the effectiveness of evasion techniques
against browser phishing blacklists. In Proc. of IEEE S&P (2019).
enterprise threat detection. In Proc. of ACSAC (2018).
[34] Peng, P., Xu, C., Quinn, L., Hu, H., Viswanath, B., and Wang, G. What happens
after you leak your password: Understanding credential sharing on phishing
[26] Korczynski, D., and Yin, H. Capturing malware propagations with code injec-
[33] Oprea, A., Li, Z., Norris, R., and Bowers, K. Made: Security analytics for
sites. In Proc. of AsiaCCS (2019).
[35] Razaghpanah, A., Nithyanand, R., Vallina-Rodriguez, N., Sundaresan, S.,
Allman, M., Kreibich, C., and Gill, P. Apps, trackers, privacy, and regulators:
A global study of the mobile tracking ecosystem. In Proc. of NDSS (2018).
[36] Sarabi, A., and Liu, M. Characterizing the internet host population using deep
learning: A universal and lightweight numerical embedding. In Proc. of IMC
(2018).
[37] Schwartz, E. J., Cohen, C. F., Duggan, M., Gennari, J., Havrilla, J. S., and
Hines, C. Using logic programming to recover c++ classes and methods from
compiled executables. In Proc. of CCS (2018).
[38] Sharif, M., Urakawa, J., Christin, N., Kubota, A., and Yamada, A. Predicting
impending exposure to malicious content from user behavior. In Proc. of CCS
(2018).
[39] Szurdi, J., and Christin, N. Email typosquatting. In Proc. of IMC (2017).
[40] Tian, K., Jan, S. T. K., Hu, H., Yao, D., and Wang, G. Needle in a haystack:
Tracking down elite phishing domains in the wild. In Proc. of IMC (2018).
[41] Wang, H., Liu, Z., Liang, J., Vallina-Rodriguez, N., Guo, Y., Li, L., Tapiador,
J., Cao, J., and Xu, G. Beyond google play: A large-scale comparative study of
chinese android app markets. In Proc. of IMC (2018).
[42] Wang, L., Nappa, A., Caballero, J., Ristenpart, T., and Akella, A. Whowas: A
platform for measuring web deployments on iaas clouds. In Proc. of IMC (2014).
[43] Whittaker, C., Ryner, B., and Nazif, M. Large-scale automatic classification
of phishing pages. In Proc. of NDSS (2010).
tiro. In Proc. of USENIX Security (2018).
virtualized binary code simplification. In Proc. of CCS (2018).
[46] Xu, Z., Nappa, A., Baykov, R., Yang, G., Caballero, J., and Gu, G. Autoprobe:
Towards automatic active malicious server probing using dynamic binary analysis.
In Proc. of CCS (2014).
symbolic execution. In Proc. of WWW (2017).
[47] Zuo, C., and Lin, Z. Smartgen: Exposing server urls of mobile apps with selective
[44] Wong, M. Y., and Lie, D. Tackling runtime-based obfuscation in android with
[45] Xu, D., Ming, J., Fu, Y., and Wu, D. Vmhunt: A verifiable approach to partially-
APPENDIX - RESEARCH ETHICS
We want to provide a detailed discussion on the research ethics.
Internet users. We have taken active steps to make sure our
experiments do not involve or harm any users. Given that our
experiments require hosting public websites, we need to prevent
real users from accidentally visiting the experimental websites and
revealing sensitive information. First, all the phishing websites use
long random string as domain names (50 random characters). It is
very unlikely that users would mistype an actual website’s domain
name in the address bar to reach our websites.
Second, we never advertise the websites other than submitting
them to scanners. We have checked popular search engines (Google,
Microsoft Bing) by searching keywords such as “PayPal”, “IRS”, and
“tax”. We did not find our phishing websites indexed after examining
the first 10 pages of search results. It is very unlikely real users
would find our website via search engines.
Third, to prevent the servers accidentally storing sensitive user
information (e.g., password), we have modified the PayPal and IRS
phishing kits when deploying the websites. More specifically, for
any HTTP POST requests, the server will automatically parse and
discard the data fields without storing the data. Even if a user ac-
cidentally submitted sensitive information via the login form, the
data would never be stored in the database or logs. Throughout
our experiments, we never received such requests. After the exper-
iments, all phishing pages are taken offline immediately. Certain
readers may ask, is it possible that the scanners actually recognized
that the password was never stored and thus labeled the website as
“benign”? We believe this is unlikely because the action happens
internally at the server side and the server provides no feedback
or error messages. The internal action is invisible to the scanners.
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
P. Peng, L. Yang, L. Song, and G. Wang
From a scanner perspective, the website behaves like a typical phish-
ing site: (1) a PayPal login page is hosted under a server whose
domain name is a long random string; (2) the website sends the
login password to a web hosting service instead of paypal.com.
Once the URLs are submitted,
VirusTotal and its vendors.
all the scanning is performed automatically without any human
involvement. Given the relatively small-scale of the experiments,
we expect the impact to these services (e.g., scanning workload) is
minimal.
Domain registrar and web hosting services. We have noti-
fied them about our experiments before we start, and have received
their consent. This allows us to run the experiments without worry-
ing about any interruption from the domain registrar and hosting
services. We do notice that 5 of the 36 URLs used in the main ex-
periment are later included by DNS blacklists (after some vendors
of VirusTotal flagged them).
We argue that the experiments’ benefits overweight the potential
risk. A deeper understanding of how VirusTotal and vendors label
phishing sites helps to inform better methods for phishing detection,
and inspire new research to improve the label quality.