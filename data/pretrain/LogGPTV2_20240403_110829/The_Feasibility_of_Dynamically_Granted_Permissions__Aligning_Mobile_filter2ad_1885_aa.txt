title:The Feasibility of Dynamically Granted Permissions: Aligning Mobile
Privacy with User Preferences
author:Primal Wijesekera and
Arjun Baokar and
Lynn Tsai and
Joel Reardon and
Serge Egelman and
David A. Wagner and
Konstantin Beznosov
2017 IEEE Symposium on Security and Privacy
The Feasibility of Dynamically Granted Permissions:
Aligning Mobile Privacy with User Preferences
Primal Wijesekera1, Arjun Baokar2, Lynn Tsai2, Joel Reardon2,
Serge Egelman2, David Wagner2, and Konstantin Beznosov1
1University of British Columbia, Vancouver, Canada,
{primal,beznosov}@ece.ubc.ca
2University of California, Berkeley, Berkeley, USA,
{arjunbaokar,lynntsai,joel.reardon}@berkeley.edu, {egelman,daw}@cs.berkeley.edu
Abstract—Current smartphone operating systems regulate ap-
plication permissions by prompting users on an ask-on-ﬁrst-use
basis. Prior research has shown that this method is ineffective
because it fails to account for context: the circumstances under
which an application ﬁrst requests access to data may be vastly
different than the circumstances under which it subsequently
requests access. We performed a longitudinal 131-person ﬁeld
study to analyze the contextuality behind user privacy decisions
to regulate access to sensitive resources. We built a classiﬁer
to make privacy decisions on the user’s behalf by detecting
when context has changed and, when necessary, inferring privacy
preferences based on the user’s past decisions and behavior.
Our goal is to automatically grant appropriate resource requests
without further user intervention, deny inappropriate requests,
and only prompt the user when the system is uncertain of the
user’s preferences. We show that our approach can accurately
predict users’ privacy decisions 96.8% of the time, which is a
four-fold reduction in error rate compared to current systems.
I. INTRODUCTION
One of the roles of a mobile application platform is to
help users avoid unexpected or unwanted use of their per-
sonal data [12]. Mobile platforms currently use permission
systems to regulate access to sensitive resources, relying on
user prompts to determine whether a third-party application
should be granted or denied access to data and resources.
One critical caveat in this approach, however, is that mobile
platforms seek the consent of the user the ﬁrst time a given
application attempts to access a certain data type and then
enforce the user’s decision for all subsequent cases, regardless
of the circumstances surrounding each access. For example, a
user may grant an application access to location data because
she is using location-based features, but by doing this, the ap-
plication can subsequently access location data for behavioral
advertising, which may violate the user’s preferences.
Earlier versions of Android (5.1 and below) asked users to
make privacy decisions during application installation as an
all-or-nothing ultimatum (ask-on-install): either all requested
permissions are approved or the application is not installed.
Previous research showed that few people read the requested
permissions at install-time and even fewer correctly under-
stood them [17]. Furthermore, install-time permissions do not
present users with the context in which those permission will
be exercised, which may cause users to make suboptimal de-
cisions not aligned with their actual preferences. For example,
Egelman et al. observed that when an application requests
access to location data without providing context, users are
just as likely to see this as a signal for desirable location-
based features as they are an invasion of privacy [11]. Asking
users to make permission decisions at runtime—at the moment
when the permission will actually be used by the application—
provides more context (i.e., what
the
time that data was requested) [15]. However, due to the high
frequency of permission requests, it is not feasible to prompt
the user every time data is accessed [43].
they were doing at
In iOS and Android M, the user is now prompted at runtime
the ﬁrst time an application attempts to access one of a set of
“dangerous” permission types (e.g., location, contacts, etc.).
This ask-on-ﬁrst-use (AOFU) model is an improvement over
ask-on-install (AOI). Prompting users the ﬁrst time an applica-
tion uses one of the designated permissions gives users a better
sense of context: their knowledge of what they were doing
when the application ﬁrst tried to access the data should help
them determine whether the request is appropriate. Despite
that, Wijesekera et al. showed that AOFU fails to meet user
expectations over half the time. This is because AOFU does
not account for the varying contexts of future requests [43].
The notion of contextual integrity suggests that many per-
mission models fail to protect user privacy because they fail
to account for the context surrounding data ﬂows [34]. That
is, privacy violations occur when sensitive resources are used
in ways that defy users’ expectations. We posit that more
effective permission models must focus on whether resource
accesses are likely to defy users’ expectations in a given
context—not simply whether the application was authorized to
receive data the ﬁrst time it asked for it. Thus, the challenge
for system designers is to correctly infer when the context
surrounding a data request has changed, and whether the new
context is likely to be deemed “appropriate” or “inappropriate”
for the given user. Dynamically regulating data access based
on the context requires more user involvement to understand
users’ contextual preferences. If users are asked to make
privacy decisions too frequently, or under circumstances that
are seen as low-risk, they may become habituated to future,
© 2017, Primal Wijesekera. Under license to IEEE.
DOI 10.1109/SP.2017.51
1077
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
more serious, privacy decisions. On the other hand, if users are
asked to make too few privacy decisions, they may ﬁnd that
the system has acted against their wishes. Thus, our goal is to
automatically determine when and under what circumstances
the system presents users with runtime prompts.
To this end, we collected real-world Android usage data in
order to explore whether we could infer users’ future privacy
decisions based on their past privacy decisions, contextual
circumstances surrounding applications’ data requests, and
users’ behavioral traits. We conducted a ﬁeld study where
131 participants used Android phones that were instrumented
to gather data over an average of 32 days per participant.
Also, their phones periodically prompted them to make privacy
decisions when applications used sensitive permissions, and
we logged their decisions. Overall, participants wanted to
block 60% of these requests. We found that AOFU yields 84%
accuracy, i.e., its policy agrees with participants’ prompted
responses 84% of the time. AOI achieves only 25% accuracy.
We designed new techniques that use machine learning to
automatically predict how users would respond to prompts, so
that we can avoid prompting them in most cases, thereby re-
ducing user burden. Our classiﬁer uses the user’s past decisions
in similar situations to predict their response to a particular
permission request. The classiﬁer outputs a prediction and
a conﬁdence score; if the classiﬁer is sufﬁciently conﬁdent,
we use its prediction, otherwise we prompt the user for their
decision. We also incorporate information about the user’s
behavior in other security and privacy situations to make
inferences about their preferences: whether they have a screen
lock activated, how often they visit HTTPS websites, and so
on. We show that our scheme achieves 96.8% accuracy (a 4×
reduction in error rate over AOFU) with signiﬁcantly less user
involvement than the status quo.
The speciﬁc contributions of our work are the following:
• We conducted the ﬁrst known large-scale study on quan-
tifying the effectiveness of ask-on-ﬁrst-use permissions.
• We show that a signiﬁcant portion of the studied par-
ticipants make contextual decisions on permissions—
the foreground application and the visibility of
the
permission-requesting application are strong cues partic-
ipants used to make contextual decisions.
• We show how a machine-learned model can incorporate
context and better predict users’ privacy decisions.
• To our knowledge, we are the ﬁrst
to use passively
observed traits to infer future privacy decisions on a case-
by-case basis at runtime.
II. RELATED WORK
There is a large body of work demonstrating that install-
time prompts fail because users do not understand or pay
attention to them [19], [23], [42]. When using install-time
prompts, users often do not understand which permission types
correspond to which sensitive resources and are surprised
by the ability of background applications to collect informa-
tion [17], [22], [41]. Applications also transmit a large amount
of location or other sensitive data to third parties without
1078
user consent [12]. When possible risks associated with these
requests are revealed to users, their concerns range from being
annoyed to wanting to seek retribution [16].
To mitigate some of these problems, systems have been
developed to track information ﬂows across the Android
system [12], [18], [24] or introduce ﬁner-grained permission
control into Android [2], [21], [39], but many of these solu-
tions increase user involvement signiﬁcantly, which can lead to
habituation. Additionally, many of these proposals are useful
only to the most-motivated or technically savvy users. For
example, many such systems require users to conﬁgure com-
plicated control panels, which many are unlikely to do [45].
Other approaches involve static analysis in order to better
understand how applications could request information [4], [8],
[14], but these say little about how applications actually use
information. Dynamic analysis improves upon this by allowing
users to see how often this information is requested in real
time [12], [40], [43], but substantial work is likely needed to
present that information to average users in a meaningful way.
Solutions that require user interruptions need to also minimize
user intervention in order to prevent habituation.
Other researchers have developed recommendation systems
to recommend applications based on users’ privacy prefer-
ences [46], or detect privacy violations and suggest prefer-
ences based on crowdsourcing [1], [27], but such approaches
often do not take individual user differences into account
without signiﬁcant user intervention. Systems have also been
developed to predict what users would share on mobile so-
cial networks [7], which suggests that future systems could
potentially infer what information users would be willing to
share with third-party applications. By requiring users to self-
report privacy preferences, clustering algorithms have been
used to deﬁne user privacy proﬁles even in the face of diverse
preferences [26], [38]. However, researchers have found that
the order in which information is requested has an impact on
prediction accuracy [44], which could mean that such systems
are only likely to be accurate when they examine actual user
behavior over time (as opposed to one-time self-reports).
Liu et al. clustered users by privacy preferences and used
ML techniques to predict whether to allow or deny an ap-
plication’s request for sensitive user data [29]. Their dataset,
however, was collected from a set of highly privacy-conscious
individuals: those who choose to install a permission-control
mechanism. Furthermore, the researchers removed “conﬂict-
ing” user decisions, in which a user chose to deny a permission
for an application, and then later chose to allow it. These
conﬂicting decisions, however, do not represent noisy data.
They occur nearly 50% of the time in the real world [43],
and accurately reﬂect the nuances of user privacy preferences.
Models must therefore account for them. In fact, previous work
found that users commonly reassess privacy preferences after
usage [3]. Liu et al. also expect users to make 10% of permis-
sion decisions manually, which, based on ﬁeld study results
from Wijesekera et al., would result in being prompted every
three minutes [43]. This is obviously impractical. Our goal is
to design a system that can automatically make decisions on
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
behalf of users, that accurately models their preferences, while
also not over-burdening them with repeated requests.
Closely related to this work, Liu et al. [28] performed a
ﬁeld study to measure the effectiveness of a Privacy Assistant
that offers recommendations to users on privacy settings that
they could adopt based on each user’s privacy proﬁle—the
privacy assistant predicts what the user might want based on
the inferred privacy proﬁle and static analysis of the third-party
application. While this approach increased user awareness on
resource usage, the recommendations are static: they do not
consider each application’s access to sensitive data on a case-
by-case basis. Such a coarse-grained approach goes against
previous work suggesting that people do want to vary their
decisions based on contextual circumstances [43]. A blanket
approval or denial of a permission to a given application car-
ries a considerable risk of privacy violations or loss of desired
functionality. In contrast, our work uses dynamic analysis to
infer the appropriateness of each given request by considering
the surrounding contextual cues and how the user has behaved
in similar situations in the past. As with Liu et al., their dataset
was also collected from privacy-conscious and considerably
tech-savvy individuals, which may limit the generalization of
their results. The ﬁeld study we conduct in our work uses a
more representative sample.
Nissenbaum’s theory of contextual integrity suggests that
permission models should focus on information ﬂows that
are likely to defy user expectations [34]. There are three
main components involved in deciding the appropriateness
of a ﬂow [6]: the context in which the resource request is
made, the role played by the requesting application under
the current context, and the type of resource being accessed.
Neither previous nor currently deployed permission models
take all three factors into account. This model could be used to
improve permission models by automatically granting access
to data when the system determines that it is appropriate,
denying access when it is inappropriate, and prompting the
user only when a decision cannot be made automatically,
thereby reducing user burden.
Access Control Gadgets (ACGs) were proposed as a mech-
anism to tie sensitive resource access to certain UI ele-
ments [32], [35]–[37]. Authors posit that such an approach will
increase user expectations, as a signiﬁcant portion of partici-
pants expected a UI interaction before a sensitive resource us-
age, giving users an implicit mechanism to control access and
increasing awareness on resource usage. The biggest caveat in
this approach is that tying a UI interaction to each sensitive
resource access is impossible in practice because resources
are accessed at a high frequency [43], and because many
legitimate resource accesses occur without user initiation [15].
Wijesekera et al. performed a ﬁeld study [43] to operational-
ize the notion of “context,” to allow an operating system to dif-
ferentiate between appropriate and inappropriate data requests
by a single application for a single data type. They found that
users’ decisions to allow a permission request signiﬁcantly
correlated with that application’s visibility. They posit that
this visibility is a strong contextual cue that inﬂuences users’
Permission Type
ACCESS_WIFI_STATE
NFC
READ_HISTORY_BOOKMARKS
ACCESS_FINE_LOCATION
ACCESS_COARSE_LOCATION
LOCATION_HARDWARE
READ_CALL_LOG
ADD_VOICEMAIL
READ_SMS
SEND_SMS
*INTERNET
*WRITE_SYNC_SETTINGS
Activity
View nearby SSIDs
Communicate via NFC
Read users’ browser history
Read GPS location
Read network-inferred location
(i.e., cell tower and/or WiFi)
Directly access GPS data
Read call history
Read call history
Read sent/received/draft SMS
Send SMS
Access Internet when roaming
Change application sync