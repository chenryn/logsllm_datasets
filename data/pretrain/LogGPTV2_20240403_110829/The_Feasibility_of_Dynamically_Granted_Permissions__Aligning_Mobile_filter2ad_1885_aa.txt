# The Feasibility of Dynamically Granted Permissions: Aligning Mobile Privacy with User Preferences

**Authors:**
- Primal Wijesekera<sup>1</sup>, Arjun Baokar<sup>2</sup>, Lynn Tsai<sup>2</sup>, Joel Reardon<sup>2</sup>, Serge Egelman<sup>2</sup>, David A. Wagner<sup>2</sup>, and Konstantin Beznosov<sup>1</sup>
- <sup>1</sup>University of British Columbia, Vancouver, Canada, {primal, beznosov}@ece.ubc.ca
- <sup>2</sup>University of California, Berkeley, USA, {arjunbaokar, lynntsai, joel.reardon}@berkeley.edu, {egelman, daw}@cs.berkeley.edu

**Abstract:**
Current smartphone operating systems regulate application permissions by prompting users on an "ask-on-first-use" (AOFU) basis. However, this method is ineffective because it fails to account for the context in which permission requests are made. The circumstances under which an application first requests access to data may differ significantly from those under which it subsequently requests access. We conducted a longitudinal field study involving 131 participants to analyze the contextual factors influencing user privacy decisions. We developed a classifier that can make privacy decisions on behalf of the user by detecting changes in context and inferring preferences based on past decisions and behavior. Our goal is to automatically grant or deny resource requests without further user intervention, only prompting the user when the system is uncertain. Our approach accurately predicts users' privacy decisions 96.8% of the time, reducing the error rate by four times compared to current systems.

## I. Introduction

One of the primary roles of a mobile application platform is to help users avoid unexpected or unwanted use of their personal data. Current mobile platforms use permission systems to regulate access to sensitive resources, relying on user prompts to determine whether a third-party application should be granted or denied access to data and resources. However, a critical limitation of this approach is that mobile platforms seek the user's consent only the first time a given application attempts to access a certain data type, and then enforce this decision for all subsequent cases, regardless of the changing context. For example, a user may grant an application access to location data for a specific feature, but the application can later use this data for behavioral advertising, potentially violating the user's preferences.

Earlier versions of Android (5.1 and below) required users to make privacy decisions during application installation as an all-or-nothing ultimatum (ask-on-install). Research has shown that few people read or understand the requested permissions at install-time, and these permissions do not provide the context in which they will be used, leading to suboptimal decisions. For instance, Egelman et al. observed that when an application requests access to location data without context, users are equally likely to see it as a signal for desirable location-based features or an invasion of privacy. Runtime permission decisions (ask-on-first-use, AOFU) provide more context, but the high frequency of permission requests makes it impractical to prompt the user every time.

In iOS and Android M, users are prompted at runtime the first time an application attempts to access a "dangerous" permission type (e.g., location, contacts). This AOFU model is an improvement over ask-on-install, as it provides better context. However, Wijesekera et al. found that AOFU still fails to meet user expectations over half the time, as it does not account for the varying contexts of future requests. The concept of contextual integrity suggests that many permission models fail to protect user privacy because they do not consider the context surrounding data flows. Effective permission models must focus on whether resource accesses are likely to defy users' expectations in a given context, not just whether the application was authorized the first time it asked for data. Thus, the challenge is to correctly infer when the context surrounding a data request has changed and whether the new context is appropriate for the user.

To address this, we collected real-world Android usage data to explore whether we could infer users' future privacy decisions based on their past decisions, contextual circumstances, and behavioral traits. We conducted a field study with 131 participants, who used Android phones instrumented to gather data over an average of 32 days per participant. Participants were periodically prompted to make privacy decisions when applications used sensitive permissions, and we logged their decisions. Overall, participants wanted to block 60% of these requests. We found that AOFU yields 84% accuracy, while ask-on-install achieves only 25% accuracy.

We designed new techniques using machine learning to predict how users would respond to prompts, thereby reducing the need for frequent user involvement. Our classifier uses the user's past decisions in similar situations to predict their response to a particular permission request. The classifier outputs a prediction and a confidence score; if the classifier is sufficiently confident, we use its prediction, otherwise, we prompt the user for their decision. We also incorporate information about the user's behavior in other security and privacy situations to make inferences about their preferences, such as whether they have a screen lock activated and how often they visit HTTPS websites. Our scheme achieves 96.8% accuracy, a 4x reduction in error rate over AOFU, with significantly less user involvement than the status quo.

The specific contributions of our work are:
- Conducting the first large-scale study to quantify the effectiveness of ask-on-first-use permissions.
- Demonstrating that a significant portion of participants make contextual decisions on permissions, with the foreground application and the visibility of the permission-requesting application being strong cues.
- Showing how a machine-learned model can incorporate context to better predict users' privacy decisions.
- Being the first to use passively observed traits to infer future privacy decisions on a case-by-case basis at runtime.

## II. Related Work

There is extensive research demonstrating that install-time prompts fail because users do not understand or pay attention to them. When using install-time prompts, users often do not understand which permission types correspond to which sensitive resources and are surprised by the ability of background applications to collect information. Applications also transmit a large amount of location or other sensitive data to third parties without user consent. When users are informed of the risks associated with these requests, their concerns range from annoyance to wanting retribution.

To mitigate these issues, systems have been developed to track information flows across the Android system or introduce finer-grained permission control into Android. However, many of these solutions increase user involvement, leading to habituation, and are useful only to the most motivated or technically savvy users. Other approaches involve static analysis to better understand how applications could request information, but these say little about how applications actually use information. Dynamic analysis improves upon this by allowing users to see how often information is requested in real time, but presenting this information in a meaningful way remains a challenge.

Other researchers have developed recommendation systems to suggest applications based on users' privacy preferences or detect privacy violations and suggest preferences based on crowdsourcing. However, these approaches often do not account for individual user differences without significant user intervention. Systems have also been developed to predict what users would share on mobile social networks, suggesting that future systems could infer what information users would be willing to share with third-party applications. Clustering algorithms have been used to define user privacy profiles, even in the face of diverse preferences. However, the order in which information is requested impacts prediction accuracy, indicating that such systems are only likely to be accurate when they examine actual user behavior over time.

Liu et al. clustered users by privacy preferences and used machine learning techniques to predict whether to allow or deny an application's request for sensitive user data. Their dataset, however, was collected from highly privacy-conscious individuals, and they removed "conflicting" user decisions, where a user initially denied a permission and later allowed it. These conflicting decisions occur frequently in the real world and reflect the nuances of user privacy preferences. Models must account for them. Previous work found that users commonly reassess privacy preferences after usage. Liu et al. expected users to make 10% of permission decisions manually, which, based on field study results, would result in being prompted every three minutes. This is impractical. Our goal is to design a system that can automatically make decisions on behalf of users, accurately model their preferences, and not over-burden them with repeated requests.

Closely related, Liu et al. performed a field study to measure the effectiveness of a Privacy Assistant that offers recommendations based on each user's privacy profile. While this approach increased user awareness of resource usage, the recommendations are static and do not consider each application's access to sensitive data on a case-by-case basis. Such a coarse-grained approach goes against previous work suggesting that people want to vary their decisions based on contextual circumstances. A blanket approval or denial of a permission carries a considerable risk of privacy violations or loss of desired functionality. In contrast, our work uses dynamic analysis to infer the appropriateness of each request by considering contextual cues and past user behavior. Our field study uses a more representative sample.

Nissenbaum's theory of contextual integrity suggests that permission models should focus on information flows that are likely to defy user expectations. There are three main components in deciding the appropriateness of a flow: the context in which the resource request is made, the role played by the requesting application, and the type of resource being accessed. Neither previous nor currently deployed permission models take all three factors into account. This model could be used to improve permission models by automatically granting access to data when appropriate, denying access when inappropriate, and prompting the user only when a decision cannot be made automatically, thereby reducing user burden.

Access Control Gadgets (ACGs) were proposed to tie sensitive resource access to certain UI elements, increasing user expectations. However, tying a UI interaction to each sensitive resource access is impractical due to the high frequency of resource accesses and the fact that many legitimate accesses occur without user initiation. Wijesekera et al. performed a field study to operationalize the notion of "context," finding that users' decisions to allow a permission request significantly correlated with the application's visibility. They posit that this visibility is a strong contextual cue influencing users' decisions.

| Permission Type | Activity |
|-----------------|----------|
| ACCESS_WIFI_STATE | View nearby SSIDs |
| NFC | Communicate via NFC |
| READ_HISTORY_BOOKMARKS | Read users' browser history |
| ACCESS_FINE_LOCATION | Read GPS location |
| ACCESS_COARSE_LOCATION | Read network-inferred location (i.e., cell tower and/or WiFi) |
| LOCATION_HARDWARE | Directly access GPS data |
| READ_CALL_LOG | Read call history |
| ADD_VOICEMAIL | Read call history |
| READ_SMS | Read sent/received/draft SMS |
| SEND_SMS | Send SMS |
| *INTERNET | Access Internet when roaming |
| *WRITE_SYNC_SETTINGS | Change application sync settings |

This table provides a summary of the permission types and the corresponding activities they enable, highlighting the importance of context in making privacy decisions.