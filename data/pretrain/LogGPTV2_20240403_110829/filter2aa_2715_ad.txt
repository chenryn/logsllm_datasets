input=ipPermissions 
| join groupId 
    [ search index=aws eventName=RunInstances earliest=-7d 
    | fields 
"responseElements.instancesSet.items{}.groupSet.items{}.groupId", 
"responseElements.instancesSet.items{}.instanceId" 
    | rename 
responseElements.instancesSet.items{}.groupSet.items{}.groupId as 
groupId, "responseElements.instancesSet.items{}.instanceId" as 
instanceId] 
| stats values(instanceId) by groupId, userName, accountId, type, 
sourceIPAddress, cidrIp, fromPort, toPort, ipProtocol 
Network ACL Creation 
Example below searches for creation of Network ACL rules allowing 
inbound connections from any sources. 
sourcetype="aws:cloudtrail" eventSource="ec2.amazonaws.com" 
eventName=CreateNetworkAclEntry 
| spath output=cidrBlock path=requestParameters.cidrBlock 
| spath output=ruleAction path=requestParameters.ruleAction 
| search cidrBlock=0.0.0.0/0 ruleAction=Allow 
Detect Public S3 Buckets 
Eample search looking for the PutBucketAcl event name where the 
grantee URI is AllUsers we can identify and report the open 
buckets. 
sourcetype=aws:cloudtrail AllUsers eventName=PutBucketAcl 
errorCode=Success 
| spath output=userIdentityArn path=userIdentity.arn 
| spath output=bucketName path=requestParameters.bucketName 
30 
| spath output=aclControlList 
path=requestParameters.AccessControlPolicy.AccessControlList 
| spath input=aclControlList output=grantee path=Grant{} 
| mvexpand grantee 
| spath input=grantee 
| search Grantee.URI=*AllUsers 
| rename userIdentityArn as user 
| table _time, src,awsRegion Permission, Grantee.URI, bucketName, 
user 
VPC Traffic Mirroring 
Capture & Inspect Network Traffic 
aws ec2 create-traffic-mirror-filter --description "TCP Filter" 
REFERENCE:  
https://0x00sec.org/t/a-blue-team-guide-to-aws-cloudtrail-monitoring/15086 
https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-
filter.html#create-traffic-mirroring-filter 
A
A 
AWS_Exploit 
RED TEAM 
EXPLOITATION 
CLOUD 
NIMBOSTRATUS 
Install 
git clone PI:EMAIL:andresriancho/nimbostratus.git 
cd nimbostratus 
pip install -r requirements.txt 
Prerequisites 
Amazon AWS User account 
Access Key 
Boto Python 2.7 library 
Insert VULN_URL into the utils/mangle.py file. Run dump-metada: 
nimbostratus -v dump-ec2-metadata --mangle-
function=core.utils.mangle.mangle 
Enumerate meta-data service of target using mangle function & 
retrieve any access key credentials found on the meta-data server: 
nimbostratus -v dump-credentials --mangle-
function=core.utils.mangle.mangle 
31 
Dump all permissions for the provided credentials. Use right after 
dump-credentials to know which permissions are available: 
nimbostratus  dump-permissions --access-key=**************PXXQ --
secret-key=*****************************SUW --token 
*****************************************JFE 
Create a new user. Assigns a random name to the created user and 
attaches a policy which looks like this: 
    { 
        "Version": "2012-10-17", 
        "Statement": [ 
            { 
                "Effect": "Allow", 
                "Action": "*", 
                "Resource": "*" 
            } 
        ] 
    } 
Execute: 
nimbostratus -v create-iam-user --access-key **************UFUA --
secret-key **************************************DDxSZ --token 
****************************************tecaoI 
Create RDS database snapshot: 
nimbostratus -v snapshot-rds --access-key ********AUFUA --secret-
key *****************************yDDxSZ --token 
************************************************K2g2QU= --rds-name 
 --password ********* --region us-west-2 
PACU 
Install 
git clone https://github.com/RhinoSecurityLabs/pacu 
cd pacu 
bash install.sh 
python3 pacu.py 
Starting Pacu 
python3 pacu.py 
>set_keys  
#Key alias - Used internally within Pacu and is associated with a 
AWS key pair. Has no bearing on AWS permissions. 
#Access Key - Generated from an AWS User 
#Secret Key - Secret key associated with access key. Omitted in 
image. 
#(Optional) Session Key - serves as a temporary access key to 
access AWS services. 
**provide a session name, after which you can add your compromised 
credentials with the set_keys command and begin running modules 
32 
Running Modules 
#list out modules 
> ls  
SYNTAX:> run  [--keyword-arguments] 
PACU MODULES 
iam__enum_assume_role 
Enumerates existing roles in other AWS accounts to try and gain 
access via misconfigurations. 
iam__enum_users 
Enumerates IAM users in a separate AWS account, given the account 
ID. 
s3__bucket_finder 
Enumerates/bruteforces S3 buckets based on different parameters. 
aws__enum_account 
Enumerates data About the account itself. 
aws__enum_spend 
Enumerates account spend by service. 
codebuild__enum 
Enumerates CodeBuild builds and projects while looking for 
sensitive data 
ebs__enum_volumes_snapshots 
Enumerates EBS volumes and snapshots and logs any without 
encryption. 
ec2__check_termination_protection 
Collects a list of EC2 instances without termination protection. 
ec2__download_userdata 
Downloads User Data from EC2 instances. 
ec2__enum 
Enumerates a ton of relevant EC2 info. 
glue__enum 
Enumerates Glue connections, crawlers, databases, development 
endpoints, and jobs. 
iam__enum_permissions 
Tries to get a confirmed list of permissions for the current (or 
all) user(s). 
iam__enum_users_roles_policies_groups 
33 
Enumerates users, roles, customer-managed policies, and groups. 
iam__get_credential_report 
Generates and downloads an IAM credential report. 
inspector__get_reports 
Captures vulnerabilties found when running a preconfigured 
inspector report. 
lambda__enum 
Enumerates data from AWS Lambda. 
lightsail__enum 
Captures common data associated with Lightsail 
iam__privesc_scan 
An IAM privilege escalation path finder and abuser. 
**WARNING: Due to the implementation in IAM policies, this module 
has a difficult time parsing "NotActions". LATERAL_MOVE 
cloudtrail__csv_injection 
Inject malicious formulas/data into CloudTrail event history. 
vpc__enum_lateral_movement 
Looks for Network Plane lateral movement opportunities. 
api_gateway__create_api_keys 
Attempts to create an API Gateway key for any/all REST APIs that 
are defined. 
ebs__explore_snapshots 
Restores and attaches EBS volumes/snapshots to an EC2 instance of 
your choice. 
ec2__startup_shell_script 
Stops and restarts EC2 instances to execute code. 
lightsail__download_ssh_keys 
Downloads Lightsails default SSH key pairs. 
lightsail__generate_ssh_keys 
Creates SSH keys for available regions in AWS Lightsail. 
lightsail__generate_temp_access 
Creates temporary SSH keys for available instances in AWS 
Lightsail. 
systemsmanager__rce_ec2 
Tries to execute code as root/SYSTEM on EC2 instances. 
**NOTE: Linux targets will run the command using their default 
shell (bash/etc.) and Windows hosts will run the command using 
34 
PowerShell, so be weary of that when trying to run the same command 
against both operating systems.Sometimes Systems Manager Run 
**Command can delay the results of a call by a random amount. 
Experienced 15 minute delays before command was executed on the 
target. 
ec2__backdoor_ec2_sec_groups 
Adds backdoor rules to EC2 security groups. 
iam__backdoor_assume_role 
Creates assume-role trust relationships between users and roles. 
iam__backdoor_users_keys 
Adds API keys to other users. 
iam__backdoor_users_password 
Adds a password to users without one. 
s3__download_bucket 
Enumerate and dumps files from S3 buckets. 
cloudtrail__download_event_history 
Downloads CloudTrail event history to JSON files 
to ./sessions/[current_session_name]/downloads/cloudtrail_[region]_
event_history_[timestamp].json. 
**NOTE: This module can take a very long time to complete. A rough 
estimate is about 10000 events retrieved per five minutes. 
cloudwatch__download_logs 
Captures CloudWatch logs and downloads them to the session 
downloads folder 
detection__disruption 
Disables, deletes, or minimizes various logging/monitoring 
services. 
detection__enum_services 
Detects monitoring and logging capabilities. 
elb__enum_logging 
Collects a list of Elastic Load Balancers without access logging 
and write a list of ELBs with logging disabled 
to ./sessions/[current_session_name]/downloads/elbs_no_logs_[timest
amp].csv. 
guardduty__whitelist_ip 
Adds an IP address to the list of trusted IPs in GuardDuty. 
**NOTE: This will not erase any existing GuardDuty findings, it 
will only prevent future findings related to the included IP 
addresses. 
35 
**WARNING: Only one list of trusted IP addresses is allowed per 
GuardDuty detector. This module will prompt you to delete an 
existing list if you would like, but doing so could have unintended 
bad consequences on the target AWS environment. 
waf__enum 
Detects rules and rule groups for WAF. 
REFERENCE: 
https://andresriancho.github.io/nimbostratus/ 
https://www.cloudsecops.com/post-exploitation-in-aws/ 
https://github.com/RhinoSecurityLabs/pacu 
https://github.com/puresec/awesome-serverless-security/ 
https://zoph.me/posts/2019-12-16-aws-security-toolbox/ 
https://know.bishopfox.com/research/privilege-escalation-in-aws 
https://github.com/BishopFox/smogcloud 
https://github.com/bishopfox/dufflebag 
https://rhinosecuritylabs.com/aws/abusing-vpc-traffic-mirroring-in-aws/ 
A
A 
AWS_Hardening 
BLUE TEAM 
CONFIGURATION 
CLOUD 
AWS Best Practices Rules 
https://www.cloudconformity.com/knowledge-base/aws/ 
A
A 
AWS_Terms 
ALL 
GENERAL 
CLOUD 
AWS IoT: AWS IoT is a managed cloud service that lets connected 
devices easily and securely interact with cloud applications and 
other devices. 
Certificate Manager: AWS Certificate Manager easily provision, 
manage, and deploy Secure Sockets Layer/Transport Layer Security 
(SSL/TLS) certificates for use with AWS services. 
CloudFormation: AWS CloudFormation lets you create and update a 
collection of related AWS resources in a predictable fashion. 
CloudFront: Amazon CloudFront provides a way to distribute 
content to end-users with low latency and high data transfer 
speeds. 
CloudSearch: AWS CloudSearch is a fully managed search service 
for websites and apps. 
CloudTrail: AWS CloudTrail provides increased visibility into 
user activity by recording API calls made on your account. 
36 
Data Pipeline: AWS Data Pipeline is a lightweight orchestration 
service for periodic, data-driven workflows. 
DMS: AWS Database Migration Service (DMS) helps you migrate 
databases to the cloud easily and securely while minimizing 
downtime. 
DynamoDB: Amazon DynamoDB is a scalable NoSQL data store that 
manages distributed replicas of your data for high availability. 
EC2: Amazon Elastic Compute Cloud (EC2) provides resizable 
compute capacity in the cloud. 
EC2 Container Service: Amazon ECS allows you to easily run and 
manage Docker containers across a cluster of Amazon EC2 
instances. 
Elastic Beanstalk: AWS Elastic Beanstalk is an application 
container for deploying and managing applications. 
ElastiCache: Amazon ElastiCache improves application performance 
by allowing you to retrieve information from an in-memory 
caching system. 
Elastic File System: Amazon Elastic File System (Amazon EFS) is 
a file storage service for Amazon Elastic Compute Cloud (Amazon 
EC2) instances. 
Elasticsearch Service: Amazon Elasticsearch Service is a managed 
service that makes it easy to deploy, operate, and scale Elasti-
csearch, a popular open-source search and analytics engine. 
Elastic Transcoder: Amazon Elastic Transcoder lets you convert 
your media files in the cloud easily, at low cost, and at scale 
EMR: Amazon Elastic MapReduce lets you perform big data tasks 
such as web indexing, data mining, and log file analysis. 
Glacier: Amazon Glacier is a low-cost storage service that 
provides secure and durable storage for data archiving and 
backup. 
IAM: AWS Identity and Access Management (IAM) lets you securely 
control access to AWS services and resources. 
Inspector: Amazon Inspector enables you to analyze the behavior 
of the applications you run in AWS and helps you to identify 
potential security issues. 
Kinesis: Amazon Kinesis services make it easy to work with real-
time streaming data in the AWS cloud. 
Lambda: AWS Lambda is a compute service that runs your code in 
response to events and automatically manages the compute 
resources for you. 
Machine Learning: Amazon Machine Learning is a service that 
enables you to easily build smart applications. 
OpsWorks: AWS OpsWorks is a DevOps platform for managing applic-
ations of any scale or complexity on the AWS cloud. 
RDS: Amazon Relational Database Service (RDS) makes it easy to 
set up, operate, and scale familiar relational databases in the 
cloud. 
Redshift: Amazon Redshift is a fast, fully managed, petabyte--
scale data warehouse that makes it cost-effective to analyze all 
your data using your existing business intelligence tools. 
37 
Route 53: Amazon Route 53 is a scalable and highly available 
Domain Name System (DNS) and Domain Name Registration service. 
SES: Amazon Simple Email Service (SES) enables you to send and 
receive email. 
SNS: Amazon Simple Notification Service (SNS) lets you publish 
messages to subscribers or other applications. 
Storage Gateway: AWS Storage Gateway securely integrates on-pre-
mises IT environments with cloud storage for backup and disaster 
recovery. 
SQS: Amazon Simple Queue Service (SQS) offers a reliable, highly 
scalable, hosted queue for storing messages. 
SWF: Amazon Simple Workflow (SWF) coordinates all of the 
processing steps within an application. 
S3: Amazon Simple Storage Service (S3) can be used to store and 
retrieve any amount of data. 
VPC: Amazon Virtual Private Cloud (VPC) lets you launch AWS 
resources in a private, isolated cloud. 
REFERENCE: 
https://www.northeastern.edu/graduate/blog/aws-terminology/ 
A
A 
AWS_Tricks 
ALL 
MISC 
CLOUD 
SUBNETS 
Creating A Subnet 
aws ec2 create-subnet --vpc-id  --cidr-block  -
-availability-zone  --region  
Auto Assigning Public IPs To Instances In A Public Subnet 
aws ec2 modify-subnet-attribute --subnet-id  --map-
public-ip-on-launch --region  
VPC 
Creating A VPC 
aws ec2 create-vpc --cidr-block  --regiosn  
Allowing DNS hostnames 
aws ec2 modify-vpc-attribute --vpc-id  --enable-dns-
hostnames "{\"Value\":true}" --region  
NAT 
38 
Setting Up A NAT Gateway 
#Allocate Elastic IP 
aws ec2 allocate-address --domain vpc --region   
#AllocationId to create the NAT Gateway for the public zone 
aws ec2 create-nat-gateway --subnet-id  --allocation-id 
 --region   
S3 API 
Listing Only Bucket Names 
aws s3api list-buckets --query 'Buckets[].Name' 
Getting a Bucket Region 
aws s3api get-bucket-location --bucket  
Syncing a Local Folder with a Bucket 
aws s3 sync  s3://  
Copying Folders 
aws s3 cp / s3:/// --recursive 
To exclude files from copying 
aws s3 cp / s3:/// --recursive --exclude 
""  
To exclude a folder from copying 
aws s3 cp example.com/ s3://example-backup/ --recursive --exclude 
".git/*"  
Removing a File from a Bucket 
aws s3 rm s3:/// 
Deleting a Bucket 
aws s3 rb s3:// --force 
Emptying a Bucket 
aws s3 rm s3:/// --recursive 
EC2 Instance 
Creating AMI Without Rebooting the Machine 
aws ec2 create-image --instance-id  --name "image-
$(date +'%Y-%m-%d_%H-%M-%S')" --description "image-$(date 
+'%Y-%m-%d_%H-%M-%S')" --no-reboot 
39 
LAMBDA 
Using AWS Lambda with Scheduled Events 
sid=Sid$(date +%Y%m%d%H%M%S); aws lambda add-permission --
statement-id $sid --action 'lambda:InvokeFunction' --principal 
events.amazonaws.com --source-arn 
arn:aws:events:::rule/AWSLambdaBasicExecutionRole --
function-name function: --region  
Deleting Unused Volumes 
for x in $(aws ec2 describe-volumes --filters  
Name=status,Values=available  --profile |grep 
VolumeId|awk '{print $2}' | tr ',|"' ' '); do aws ec2 delete-volume 
--region  --volume-id $x; done 
With "profile": 
for x in $(aws ec2 describe-volumes --filters  
Name=status,Values=available  --profile |grep 
VolumeId|awk '{print $2}' | tr ',|"' ' '); do aws ec2 delete-volume 
--region  --volume-id $x --profile ; 
done 
REFERENCE: 
https://github.com/eon01/AWS-CheatSheet 
A
A 
AZURE CLI 
RED/BLUE TEAM 
RECON/ADMIN 
CLOUD 
Azure command-line interface (Azure CLI) is an environment to 
create and manage Azure resources. 
Login in CLI 
az login -u PI:EMAIL 
List accounts 
az account list 
Set subscription 
az account set --subscription "xxx" 
List all locations 
az account list-locations 
List all resource groups 
az resource list 
40 
Get version of the CLI 
azure --version 
Get help 
azure help 
Get all available VM sizes 
az vm list-sizes --location  
Get all available VM images for Windows and Linux 
az vm image list --output table 
Create a Ubuntu Linux VM 
az vm create --resource-group myResourceGroup --name myVM --image 
ubunlts 
Create a Windows Datacenter VM 
az vm create --resource-group myResourceGroup --name myVM --image 
win2016datacenter 
Create a Resource group 
az group create --name myresourcegroup --location eastus 
Create a Storage account 
az storage account create -g myresourcegroup -n mystorageaccount -l 
eastus --sku Standard_LRS 
Permanently delete a resource group 
az group delete --name  
List VMs 
az vm list 
Start a VM 
az vm start --resource-group myResourceGroup --name myVM 
Stop a VM 
az vm stop --resource-group myResourceGroup --name myVM 
Deallocate a VM 
az vm deallocate --resource-group myResourceGroup --name myVM 
Restart a VM 
az vm restart --resource-group myResourceGroup --name myVM 
41 
Redeploy a VM 
az vm redeploy --resource-group myResourceGroup --name myVM 
Delete a VM 
az vm delete --resource-group myResourceGroup --name myVM 
Create image of a VM 
az image create --resource-group myResourceGroup --source myVM --
name myImage 
Create VM from image 
az vm create --resource-group myResourceGroup --name myNewVM --
image myImage 
List VM extensions 
az vm extension list --resource-group azure-playground-resources --
vm-name azure-playground-vm 
Delete VM extensions 
az vm extension delete --resource-group azure-playground-resources 
--vm-name azure-playground-vm --name bootstrapper 
Create a Batch account 
az batch account create -g myresourcegroup -n mybatchaccount -l 
eastus 
Create a Storage account 
az storage account create -g myresourcegroup -n mystorageaccount -l 
eastus --sku Standard_LRS 
Associate Batch with storage account. 
az batch account set -g myresourcegroup -n mybatchaccount --
storage-account mystorageaccount 
Authenticate directly against the batch account  
az batch account login -g myresourcegroup -n mybatchaccount 
Display the details of our created batch account 
az batch account show -g myresourcegroup -n mybatchaccount 
Create a new application 
az batch application create --resource-group myresourcegroup --name 
mybatchaccount --application-id myapp --display-name "My 
Application" 
Add zip files to application 
42 
az batch application package create --resource-group 
myresourcegroup --name mybatchaccount --application-id myapp --
package-file my-application-exe.zip --version 1.0 
Assign the application package as the default version 
az batch application set --resource-group myresourcegroup --name 
mybatchaccount --application-id myapp --default-version 1.0 
Retrieve a list of available images and node agent SKUs. 
az batch pool node-agent-skus list 
Create new Linux pool with VM config 
az batch pool create --id mypool-linux --vm-size Standard_A1 --
image canonical:ubuntuserver:16.04.0-LTS --node-agent-sku-id 
“batch.node.ubuntu 16.04” 
Resize the pool to start up VMs 
az batch pool resize --pool-id mypool-linux --target-dedicated 5 
Check the status of the pool 
az batch pool show --pool-id mypool-linux 
List the compute nodes running in a pool 
az batch node list --pool-id mypool-linux 
If a particular node in the pool is having issues, it can be 
rebooted or reimaged. A typical node ID will be in the format 'tvm-
xxxxxxxxxx_1-' 
az batch node reboot --pool-id mypool-linux --node-id tvm-123_1-
20170316t000000z 
Re-allocate work to another node 
az batch node delete --pool-id mypool-linux --node-list tvm-123_1-
20170316t000000z tvm-123_2-20170316t000000z --node-deallocation-
option requeue 
Create a new job to encapsulate the tasks that we want to add 
az batch job create --id myjob --pool-id mypool 
Add tasks to the job 
az batch task create --job-id myjob --task-id task1 --application-
package-references myapp#1.0 --command-line "/bin/ -c 
/path/to/script.sh" 
Add multiple tasks at once 
az batch task create --job-id myjob --json-file tasks.json 
43 
Update job automatically marked as completed once all the tasks are 
finished 
az batch job set --job-id myjob --on-all-tasks-complete 
terminateJob 
Monitor the status of the job 
az batch job show --job-id myjob 
Monitor the status of a task. 
az batch task show --job-id myjob --task-id task1 
Delete a job 
az batch job delete --job-id myjob 
Managing Containers 
#If you HAVE AN SSH run this to create an Azure Container Service 
Cluster (~10 mins) 
az acs create -n acs-cluster -g acsrg1 -d applink789 
#If you DO NOT HAVE AN SSH run this to create an Azure Container 
Service Cluster (~10 mins) 
az acs create -n acs-cluster -g acsrg1 -d applink789 --generate-
ssh-keys 
List clusters under your subscription 
az acs list --output table 
List clusters in a resource group 
az acs list -g acsrg1 --output table 
Display details of a container service cluster 
az acs show -g acsrg1 -n acs-cluster --output list 
Scale using ACS 
az acs scale -g acsrg1 -n acs-cluster --new-agent-count 4 
Delete a cluster 
az acs delete -g acsrg1 -n acs-cluster 
REFERENCE: 
https://github.com/ferhaty/azure-cli-cheatsheet 
https://gist.github.com/yokawasa/fd9d9b28f7c79461f60d86c23f615677 
A
A 
44 
AZURE_Defend 
BLUE TEAM 
THREAT HUNTING 
CLOUD 
Azure Sentinel Hunt Query Resource 
https://github.com/Azure/Azure-Sentinel/tree/master/Hunting%20Queries 
Microsoft Azure Sentinel is a scalable, cloud-native, security 
information event management (SIEM) and security orchestration 
automated response (SOAR) solution. 
Uncoder: One common language for cyber security 
https://uncoder.io/ 
Uncoder.IO is the online translator for SIEM saved searches, 
filters, queries, API requests, correlation and Sigma rules to help 
SOC Analysts, Threat Hunters and SIEM Engineers. Easy, fast and 
private UI you can translate the queries from one tool to another 
without a need to access to SIEM environment and in a matter of 
just few seconds. 
Uncoder.IO supports rules based on Sigma, ArcSight, Azure Sentinel, 
Elasticsearch, Graylog, Kibana, LogPoint, QRadar, Qualys, RSA 
NetWitness, Regex Grep, Splunk, Sumo Logic, Windows Defender ATP, 
Windows PowerShell, X-Pack Watcher. 
REFERENCE: 
https://docs.microsoft.com/en-us/azure/kusto/query/index 
https://notebooks.azure.com/ 
https://posts.specterops.io/detecting-attacks-within-azure-bdc40f8c0766 
https://logrhythm.com/six-tips-for-securing-your-azure-cloud-environment/ 
A
A 
AZURE_Exploit 
RED TEAM 
EXPLOITATION 
CLOUD 
AZURE USER LOCAL ARTIFACTS 
Azure File/Folder Created Locally 
#TokenCache.dat is cleartext file containing the AccessKey; inject 
into user's process to view contents of file 
C:\Users\\.Azure\TokenCache.dat 
PowerShell Azure Modules Installed 
#Indications the target user has installed Azure modules 
C:\Program Files\windowsPowerShell\Modules\Az.* 
C:\Users\\Documents\WindowsPowerShell\Modules\Az.* 
C:\Windows\system32\windowsPowerShell\v1.0\Modules\Az.* 
Search for Save-AzContent Usage & File Location 
PS> Get-PSReadLineOption 
45 
PS> Select-String -Path  - 
Pattern 'Save-AzContext' 
Azure Token "CachedData:" Key Inside "TokenCache:" .JSON File 
#Base64 Encoded Data; Decode it to recreate TokenCache.dat file 
Import Decoded TokenCache.dat Into Attacker Local PowerShell 
#Once imported attacker will not be prompted for user/password 
PS> Import-AzContext -Path C:\path\to\decoded_TokenCache.dat 
MICROBURST 