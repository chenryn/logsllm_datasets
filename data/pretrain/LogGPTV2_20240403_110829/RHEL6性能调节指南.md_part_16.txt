:::
::: para
但当使用较大的目录块时，相比使用较小目录块到文件系统，同样的修改操作可能要消耗更多的
CPU。就是说相比小目录，大目录块的修改性能较差。当目录达到 I/O
的性能-限制因数，大块目录性能更佳。
:::
::: para
默认配置的 4 KB 文件系统块大小和 4 KB 目录块大小是最多有 1-2
百万条目，每个条目名称最 20-40
字节之间到目录到最佳选择。如果您的文件系统需要更多条目，更大的目录块以便有更佳性能，则
16 KB 块大小时有 1-10 百万目录条目文件系统的最佳选择，64 KB 块大小是超过
1 千万目录条目到文件系统的最佳选择。
:::
::: para
如果负载使用随机目录查询而不是修改（即目录读取比目录写入更常用或者重要），那么以上增加块大小的幅度就要减少一个数量级。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
并行优化
:::
与其他文件系统不同，XFS
可以最非共享对象中同时执行很多种类的分配和取消分配操作。扩展的分配或者取消分配可同时进行，即可在不同的分配组中同时进行。同样，内节点的分配和取消分配也可以同时进行，即同时进行的操作影响不同的分配组。
:::
::: para
使用有多个 CPU
的机器以及多线程程序尝试同时执行操作时分配组的数量就变得很重要。如果只有
4 个分配组，那么可持续的、平行元数据操作将只限于那四个
CPU（该系统提供的并发性限制）。对小文件系统，请确保该系统提供的并发性支持分配组的数量。对于大文件系统（10TB
以上），默认格式化选项通常可生成足够多的分配组以避免限制并发性。
:::
::: para
应用程序必须意识到限制点以便使用 XFS
文件系统结构中固有的并行性。不可能同时进行修改，因此创建和删除大量文件的应用程序应避免最一个目录中保存所有文件。应将每个创建的目录放在不同的分配组，这样类似哈希文件的计数就可以最多个子目录中提供比使用单一大目录更灵活的存储形式。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
使用扩展的属性优化程序
:::
如果内节点中有可用空间，则 XFS
可以直接在内节点中保存小属性。如果该属性符合该内节点的要求，那么可以最不需要额外
I/O
检索独立属性块的情况下检索并修改它。内嵌属性和外部属性之间的性能差异可以简单归结为外部属性的数量级要低。
:::
::: para
对于默认的 256 字节内节点，约有 100
字节属性空间可用，具体要看也保存最内节点中的数据扩展指针数。默认内节点大小只在保存少量小属性时有用。
:::
::: para
在执行 mkfs 时增加内节点的大小可以增大用来保存内嵌属性的可用空间量。512
字节的内节点约可增加用于保存属性的空间 350 字节，2 KB 的内节点约有 1900
字节的空间可用。
:::
::: para
但对于每个独立可以保存到内嵌书香则有一个大小限制：即属性名称和值占用空间最多为
254 字节（即如果属性的名称长度和值长度都在 254
字节以内则为内嵌属性）。超过这个限制则会将属性强制保存为外部属性，即使在该内节点中有足够到空间保存所有属性。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
持续元数据修改优化
:::
日志的大小时决定可持续元数据修改等级的主要因素。日志设备是循环使用的，因此最可以覆盖
tail
命令的结果钱，必须将日志中的所有修改写入磁盘的实际位置中。这可能会涉及大量寻求写入所有脏元数据的操作。默认配置会让日志大小与文件系统总体大小成比例，因此在大多数情况下不需要调整日志大小。
:::
::: para
小日志设备可导致非常频繁的元数据写回操作，即日志会一直从结尾写入以便释放空间，会经常将如此频繁修改的元数据写入磁盘，导致操作变缓。
:::
::: para
增加日志大小会增加从结尾处 push
事件的间隔。这样可以更好地聚合脏元数据，形成更好的元数据写回模式，减少频繁修改的元数据的写回。代价是较大的日志需要更多内存方可跟踪所有内存中突出的修改。
:::
::: para
如果您的机器内存有限，大日志就没有好处，因为内存限制可导致元数据写回延长抵消了释放大日志带来的好处。在这些情况下，通常较小的日志比较大的日志性能更好，因为缺少空间的日志元数据写回比内存重新回收形成的写回更有效。
:::
::: para
您应该永远都保持将日志与包含该文件系统的设备底层条单位同步。`mkfs`{.command}
命令可自动为 MD 或者 DM 设备完成此功能，但如果是硬件 RAID
则需要手动指定。设定这个功能目前可以避免在磁盘写入修改时所有可能的由于未同步
I/O 以及后续读取-修改-写入操作造成的日志 I/O。
:::
::: para
通过编辑挂载选项可进一步提高日志操作性能。增大内存嵌入的日志缓存（`logbsize`{.option}）的大小可增加写入日志的更改速度。默认日志缓存大小为
`MAX`{.literal}（32 KB，日志条单元），且最高可达
256 KB。通常该数值越大性能越快。但如果是在 fsync
负载较重的环境中，小日志缓存比使用大条单元的大缓存速度明显快很多。
:::
::: para
`delaylog`{.option}
挂载选项也可以改进不变的元数据修改性能，方法是减少日志更改次数。它通过在将其写入日志前整合每个内存更改：频繁修改的元数据是阶段性写入日志，而不是每次修改时都写入。这个选项增加了跟踪脏元数据的内存用量，同时也增加了崩溃发生时可能损失的操作量，但可以将元数据修改速度和延展性提高一个等级。使用这个选项不会在使用
`fsync`{.methodname}, `fdatasync`{.methodname} 或者 `sync`{.methodname}
时减少数据或者元数据完整性，从而保证可以将数据和元数据写入磁盘。
:::
:::
:::
:::
::: section
::: titlepage
# [⁠]{#main-fs.html#s-storage-cluster}7.4. 集群 {.title}
:::
::: para
集群的存储可为集群中的所有服务器提供一致的文件系统映像，让服务读取和写入单一文件或者共享的文件系统。这样可以通过限制类似在一个文件系统中安装和为程序打补丁的方法简化集群管理。集群范围内的文件系统还不需要程序数据的冗余副本，这样也简化了备份和系统恢复的过程。
:::
::: para
红帽高可用性附加组件除提供红帽全局文件系统
2（单行存储附加组件）外还提供集群的存储。
:::
::: section
::: titlepage
## [⁠]{#main-fs.html#s-storage-gfs}7.4.1. 全局文件系统 2 {.title}
:::
::: para
全局文件系统 2 是自带的文件系统，可直接与 Linux
内核文件系统互动。它可允许多台计算机（节点）同时分享集群中的同一存储设备。GFS2
文件系统一般采用自我调节，但也可以手动调整。本小节列出了尝试手动调节性能时应注意的事项。
:::
::: para
红帽企业版 Linux 6.4 引进了 GFS2
中改进文件系统碎片管理的方法。在红帽企业版 Linux 6.3
或者之前的版本中生成文件时，多个进程同时写入多个文件很容易造成碎片化。这个碎片化可让系统运行缓慢，特别是在有超大文件的负载时。而使用红帽企业版
Linux 6.4，同时写入的结果是产生较少的文件碎片，并籍此获得更好的性能。
:::
::: para
虽然红帽企业版 Linux 中没有用于 GFS2 的碎片清除工具，您可以使用
[**filefrag**]{.application}
工具，通过文件碎片工具识别它们，将其复制到临时文件中，并重新命名该临时文件以替换原始文件，这样就可以清除碎片。（只要写入是按顺序进行的，这个步骤还可以用于红帽企业版
Linux 6.4 以前的版本。）
:::
::: para
因为 GFS2
使用全局锁定机制，可能会需要集群中节点间的通讯，当将您的系统设计成可避免这些节点间文件和目录竞争时即可获得最佳性能。这些可避免竞争的方法为：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    在可能的情况下使用 `fallocate`{.command}
    预分配文件和目录以便优化分配过程并避免锁定资源页。
    :::
-   ::: para
    尽量减小多节点间共享的文件系统区域以便尽量减小跨节点缓存失效并提高性能。例如：如果多个节点挂载同一文件系统，但访问不同的子目录，则您可以通过将一个子目录移动到独立的文件系统中而获得更好的性能。
    :::
-   ::: para
    选择可选资源组大小和数量。这要依赖传统文件大小以及系统中的可用空间，并可能在多个节点同时尝试使用同一资源组时产生影响。资源组过多可延缓块分配，尽管已定位分配空间，而资源组过少也可在取消分配时造成锁竞争。通常最好是测试多种配置以便确定您负载的最佳方案。
    :::
:::
::: para
但竞争并不是可影响 GFS2
文件系统性能的唯一问题。其他可提高总体性能的最佳实践为：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    根据集群节点的预期 I/O 模式和文件系统的性能要求选择存储硬件。
    :::
-   ::: para
    在可以减少查询时间的地方使用固态存储。
    :::
-   ::: para
    为您的负载创建适当大小的文件系统，并保证该文件系统不会超过容量的
    80%。较小的文件系统的备份时间会根据比例缩短，且需要较少时间和内存用于文件系统检查，但如果相对负载过小，则很有可能生成高比例的碎片。
    :::
-   ::: para
    为频繁使用元数据的负载设定较大的日志，或者或者记录到日志中的数据正在使用中。虽然这样会使用更多内存，但它可以提高性能，因为在写入前有必要提供更多可用日志空间以便存储数据。
    :::
-   ::: para
    请保证 GFS2 节点中的时钟同步以避免联网程序问题。我们建议您使用
    NTP（网络时间协议）。
    :::
-   ::: para
    除非文件或者目录访问次数对您的程序操作至关重要，请使用
    `noatime`{.varname} 和 `nodiratime`{.varname} 挂载选项。
    :::
    ::: note
    ::: admonition_header
    **注意**
    :::
    ::: admonition
    ::: para
    红帽强烈推荐您在 GFS2 中使用 `noatime`{.varname} 选项。
    :::
    :::
    :::
-   ::: para
    如果您需要使用配额，请尝试减少配额同步传送的频率，或者使用模糊配额同步以便防止常规配额文件更新中的性能问题。
    :::
    ::: note
    ::: admonition_header
    **注意**
    :::
    ::: admonition
    ::: para
    模糊配额计算可允许用户和组稍微超过其配额限制。要尽量减少此类问题，GFS2
    会在用户或者组接近其配额限制时动态减少同步周期。
    :::
    :::
    :::
:::
::: para
有关 GFS2 性能调整各个方面的详情请参考 *《全局文件系统 2 指南》*，网址为
。
:::
:::
:::
:::
[]{#main-network.html}
::: chapter
::: titlepage
# [⁠]{#main-network.html#main-network}第 8 章 联网 {.title}
:::
::: para
随着时间的推移红帽企业版 Linux
的网络栈已有了大量自动优化功能。对于大多数负载，自动配置的网络设定可提供优化的性能。
:::
::: para
在大多数情况下联网性能问题是由硬件故障或者出错的基础设施造成的。这些原因不在本文档讨论范围。本章所讨论的性能问题及解决方案对优化完全正常工作的系统有帮助。
:::
::: para
联网是一个专用子系统，以敏感的连接包含不同部分。这是为什么开源社区以及红帽都致力于使用自动优化网络性能的方式。因此，对于大多数负载，您根本不需要为性能重新配置联网设置。
:::
::: section
::: titlepage
# [⁠]{#main-network.html#s-network-future}8.1. 网络性能改进 {.title}
:::
::: para
红帽企业版 Linux 6.1 提供以下网络性能改进：
:::
::: simplesect
::: titlepage
## [⁠]{#main-network.html#idm140329754300592}接收数据包操控（RPS） {.title}
:::
::: para
RPS 启用单一 NIC `rx`{.computeroutput} 队列使其接收在几个 CPU 之间发布的
`softirq`{.command} 负载。这样可以帮助防止单一 NIC
硬件队列中的网络流量瓶颈。
:::
::: para
要启用 RPS，请在 `/sys/class/net/ethX/queues/rx-N/rps_cpus`{.filename}
中指定目标 CPU 名称，使用 NIC 的对映设备名称（例如
`eth1`{.computeroutput}, `eth2`{.computeroutput}）替换
*ethX*，使用指定的 NIC 接受队列替换
`rx-N`{.filename}。这样可让在该文件中指定的 CPU 处理 `ethX`{.filename}
中 `rx-N`{.filename} 队列中的数据。指定 CPU
时，请注意该队列的*缓存亲和力*
[⁠]{#main-network.html#idm140329725720240}[^\[4\]^](#main-network.html#ftn.idm140329725720240){.footnote
xmlns:d="http://docbook.org/ns/docbook"}。
:::
:::
::: simplesect
::: titlepage
## [⁠]{#main-network.html#idm140329771365728}接收流程操控 {.title}
:::
::: para
RFS 是 RPS
的延伸，可让管理员配置在程序接收数据并整合至网络栈中时自动填充的哈希表。这可决定哪个程序接受网络数据（根据
source:destination 网络信息）。
:::
::: para
使用此信息，网络栈可调度最佳 CPU 区接收每个数据包。要配置 RFS
请使用以下可调参数：
:::
::: variablelist