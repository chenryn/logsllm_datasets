model unusable and eventually lead to denial-of-service at-
tacks. For instance, an attacker may perform such attacks to
its competitor’s federated learning system. Some studies also
considered other types of poisoning attacks (e.g., targeted
poisoning attacks [56]), which we will review in Section 6.
)= ˜O( 100√
n
We note that the Byzantine-robust aggregation rules dis-
cussed above can asymptotically bound the error rates of the
learnt global model under certain assumptions of the objec-
tive functions, and some of them (i.e., trimmed mean and
median) even achieve order-optimal error rates. These theo-
retical guarantees seem to imply the difﬁculty of manipulating
the error rates. However, the asymptotic guarantees do not
precisely characterize the practical performance of the learnt
models. Speciﬁcally, the asymptotic error rates are quantiﬁed
using the ˜O notation. The ˜O notation ignores any constant,
e.g., ˜O( 1√
). However, such constant signiﬁcantly in-
n
ﬂuences a model’s error rate in practice. As we will show,
although these asymptotic error rates still hold for our local
model poisoning attacks since they hold for Byzantine fail-
ures, our attacks can still signiﬁcantly increase the testing
error rates of the learnt models in practice.
Attacker’s capability: We assume the attacker has control
of c worker devices. Speciﬁcally, like Sybil attacks [17] to
distributed systems, the attacker could inject c fake worker
devices into the federated learning system or compromise c
benign worker devices. However, we assume the number of
worker devices under the attacker’s control is less than 50%
(otherwise, it would be easy to manipulate the global models).
We assume the attacker can arbitrarily manipulate the local
models sent from these worker devices to the master device.
For simplicity, we call these worker devices compromised
worker devices no matter whether they are fake devices or
compromised benign ones.
1 ˜O is a variant of the O notation, which ignores the logarithmic terms.
Attacker’s background knowledge: The attacker knows
the code, local training datasets, and local models on the
compromised worker devices. We characterize the attacker’s
background knowledge along the following two dimensions:
Aggregation rule. We consider two scenarios depending
on whether the attacker knows the aggregation rule or not.
In particular, the attacker could know the aggregation rule in
various scenarios. For instance, the service provider may make
the aggregation rule public in order to increase transparency
and trust of the federated learning system [39]. When the
attacker does not know the aggregation rule, we will craft
local model parameters for the compromised worker devices
based on a certain aggregation rule. Our empirical results
show that such crafted local models could also attack other
aggregation rules. In particular, we observe different levels of
transferability of our local model poisoning attacks between
different aggregation rules.
Training data. We consider two cases (full knowledge and
partial knowledge) depending on whether the attacker knows
the local training datasets and local models on the benign
worker devices. In the full knowledge scenario, the attacker
knows the local training dataset and local model on every
worker device. We note that the full knowledge scenario has
limited applicability in practice for federated learning as the
training dataset is decentralized on many worker devices, and
we use it to estimate the upper bound of our attacks’ threats for
a given setting of federated learning. In the partial knowledge
scenario, the attacker only knows the local training datasets
and local models on the compromised worker devices.
Our threat model is inspired by multiple existing stud-
ies [30, 47, 48, 56] on adversarial machine learning. For in-
stance, Suciu et al. [56] recently proposed to characterize an
attacker’s background knowledge and capability for data poi-
soning attacks with respect to multiple dimensions such as
Feature, Algorithm, and Instance. Our aggregation rule and
training data dimensions are essentially the Algorithm and
Instance dimensions, respectively. We do not consider the
Feature dimension because the attacker controls some worker
devices and already knows the features in our setting.
Some Byzantine-robust aggregation rules (e.g., Krum [9]
and trimmed mean [66]) need to know the upper bound of the
number of compromised worker devices in order to set pa-
rameters appropriately. For instance, trimmed mean removes
the largest and smallest β local model parameters, where β is
at least the number of compromised worker devices (other-
wise trimmed mean can be easily manipulated). To calculate a
lower bound for our attack’s threat, we consider a hypothetical,
strong service provider who knows the number of compro-
mised worker devices and sets parameters in the aggregation
rule accordingly.
1626    29th USENIX Security Symposium
USENIX Association
3 Our Local Model Poisoning Attacks
We focus on the case where the aggregation rule is known.
When the aggregation rule is unknown, we craft local models
based on an assumed one. Our empirical results in Section 4.3
show that our attacks have different levels of transferability
between aggregation rules.
3.1 Optimization Problem
Our idea is to manipulate the global model via carefully craft-
ing the local models sent from the compromised worker de-
vices to the master device in each iteration of federated learn-
ing. We denote by s j the changing direction of the jth global
model parameter in the current iteration when there are no
attacks, where s j = 1 or −1. s j = 1 (or s j = −1) means that
the jth global model parameter increases (or decreases) upon
the previous iteration. We consider the attacker’s goal (we
call it directed deviation goal) is to deviate a global model
parameter the most towards the inverse of the direction along
which the global model parameter would change without at-
tacks. Suppose in an iteration, wi is the local model that the ith
worker device intends to send to the master device when there
are no attacks. Without loss of generality, we assume the ﬁrst
c worker devices are compromised. Our directed deviation
goal is to craft local models w(cid:5)
c for the compro-
mised worker devices via solving the following optimization
problem in each iteration:
,··· ,w(cid:5)
,w(cid:5)
1
2
sT (w− w(cid:5)),
max
w(cid:5)
,··· ,w(cid:5)
1
c
subject to w = A(w1,··· ,wc,wc+1,··· ,wm),
c,wc+1,··· ,wm),
w(cid:5) = A(w(cid:5)
1,··· ,w(cid:5)
(1)
where s is a column vector of the changing directions of
all global model parameters, w is the before-attack global
model, and w(cid:5) is the after-attack global model. Note that s, w,
and w(cid:5) all depend on the iteration number. Since our attacks
manipulate the local models in each iteration, we omit the
explicit dependency on the iteration number for simplicity.
In our preliminary exploration of formulating poisoning
attacks, we also considered a deviation goal, which does not
consider the global model parameters’ changing directions.
We empirically ﬁnd that our attacks based on both the directed
deviation goal and the deviation goal achieve high testing error
rates for Krum. However, the directed deviation goal substan-
tially outperforms the deviation goal for trimmed mean and
median aggregation rules. Appendix B shows our deviation
goal and the empirical comparisons between deviation goal
and directed deviation goal.
3.2 Attacking Krum
Recall that Krum selects one local model as the global model
in each iteration. Suppose w is the selected local model in
1
,··· ,w(cid:5)
1 as follows: w(cid:5)
the current iteration when there are no attacks. Our goal is
to craft the c compromised local models such that the local
model selected by Krum has the largest directed deviation
from w. Our idea is to make Krum select a certain crafted
local model (e.g., w(cid:5)
1 without loss of generality) via crafting
the c compromised local models. Therefore, we aim to solve
the optimization problem in Equation 1 with w(cid:5) = w(cid:5)
1 and the
aggregation rule is Krum.
Full knowledge: The key challenge of solving the optimiza-
tion problem is that the constraint of the optimization problem
is highly nonlinear and the search space of the local models
w(cid:5)
c is large. To address the challenge, we make two
approximations. Our approximations represent suboptimal
solutions to the optimization problem, which means that the
attacks based on the approximations may have suboptimal
performance. However, as we will demonstrate in our experi-
ments, our attacks already substantially increase the error rate
of the learnt model.
= wRe − λs, where wRe
First, we restrict w(cid:5)
is the global model received from the master device in the cur-
rent iteration (i.e., the global model obtained in the previous
iteration) and λ > 0. This approximation explicitly models the
directed deviation between the crafted local model w(cid:5)
1 and the
received global model. We also explored the approximation
= w− λs, which means that we explicitly model the di-
w(cid:5)
rected deviation between the crafted local model and the local
model selected by Krum before attack. However, we found
that our attacks are less effective using this approximation.
Second, to make w1 more likely to be selected by Krum,
we craft the other c−1 compromised local models to be close
1. In particular, when the other c− 1 compromised local
to w(cid:5)
models are close to w(cid:5)
1 only needs to have a small distance
to m − 2c − 1 benign local models in order to be selected
by Krum. In other words, the other c− 1 compromised local
models “support” the crafted local model w(cid:5)
1. In implementing
our attack, we ﬁrst assume the other c− 1 compromised local
models are the same as w(cid:5)
1, then we solve w(cid:5)
1, and ﬁnally we
randomly sample c− 1 vectors, whose distance to w(cid:5)
1 is at
most ε, as the other c−1 compromised local models. With our
two approximations, we transform the optimization problem
as follows:
1, w(cid:5)
1
1
λ
λ
max
1 = Krum(w(cid:5)
subject to w(cid:5)
1 = wRe − λs,
w(cid:5)
i = w(cid:5)
w(cid:5)
1,··· ,w(cid:5)
c,w(c+1),··· ,wm),
1, for i = 2,3,··· ,c.
(2)
More precisely, the objective function in the above opti-
mization problem should be sT (w− wRe) + λsT s. However,
sT (w− wRe) is a constant and sT s = d where d is the number
of parameters in the global model. Therefore, we simplify the
objective function to be just λ. After solving λ in the opti-
mization problem, we can obtain the crafted local model w(cid:5)
1.
USENIX Association
29th USENIX Security Symposium    1627
Then, we randomly sample c− 1 vectors whose distance to
1 is at most ε as the other c− 1 compromised local models.
w(cid:5)
We will explore the impact of ε on the effectiveness of our
attacks in experiments.
Solving λ. Solving λ in the optimization problem in Equa-
tion 2 is key to our attacks. First, we derive an upper bound
of the solution λ to the optimization problem. Formally, we
have the following theorem.
Theorem 1. Suppose λ is a solution to the optimization prob-
lem in Equation 2. λ is upper bounded as follows:
⎞
⎠
D(wl,wi)
⎛
⎝ ∑
l∈ ˜Γm−c−2
wi
D(wi,wRe),
(3)
√
d
· min
c+1≤i≤m
λ ≤
1
(m− 2c− 1)
+ 1√
· max
c+1≤i≤m
d
where d is the number of parameters in the global model,
D(wl,wi) is the Euclidean distance between wl and wi,
is the set of m− c− 2 benign local models that have
˜Γm−c−2
wi
the smallest Euclidean distance to wi.
Proof. See Appendix C.
Given the upper bound, we use a binary search to solve
λ. Speciﬁcally, we initialize λ as the upper bound and check
whether Krum selects w(cid:5)
1 as the global model; if not, then
we half λ; we repeat this process until Krum selects w(cid:5)
1 or
λ is smaller than a certain threshold (this indicates that the
optimization problem may not have a solution). In our experi-
ments, we use 1× 10−5 as the threshold.
Partial knowledge: In the partial knowledge scenario, the
attacker does not know the local models on the benign worker
devices, i.e., w(c+1),··· ,wm. As a result, the attacker does
not know the changing directions s and cannot solve the opti-
mization problem in Equation 2. However, the attacker has
access to the before-attack local models on the c compromised
worker devices. Therefore, we propose to craft compromised
local models based on these before-attack local models. First,
we compute the mean of the c before-attack local models as
∑c
˜w = 1
i=1 wi. Second, we estimate the changing directions
c
using the mean local model. Speciﬁcally, if the mean of the
jth parameter is larger than the jth global model parameter
received from the master device in the current iteration, then
we estimate the changing direction for the jth parameter to
be 1, otherwise we estimate it to be −1. For simplicity, we
denote by ˜s the vector of estimated changing directions.
Third, we treat the before-attack local models on the com-
promised worker devices as if they were local models on
benign worker devices, and we aim to craft local model w(cid:5)
1
such that, among the crafted local model and the c before-
attack local models, Krum selects the crafted local model.
Formally, we have the following optimization problem:
λ
λ
max
subject to w(cid:5)
1 = Krum(w(cid:5)
1 = wRe − λ˜s.
w(cid:5)
1,w1,··· ,wc),
(4)
1, w(cid:5)
Similar to Theorem 1, we can also derive an upper bound
of λ for the optimization problem in Equation 4. Moreover,
similar to the full knowledge scenario, we use a binary search
to solve λ. However, unlike the full knowledge scenario, if
we cannot ﬁnd a solution λ until λ is smaller than a threshold
(i.e., 1× 10−5), then we add one more crafted local model
w(cid:5)
2 such that among the crafted local models w(cid:5)
2, and the
c before-attack local models, Krum selects the crafted local
model w(cid:5)
1. Speciﬁcally, we solve the optimization problem
in Equation 4 with w(cid:5)
2 added into the Krum aggregation rule.
Like the full knowledge scenario, we assume w(cid:5)
1. If
we still cannot ﬁnd a solution λ until λ is smaller than the
threshold, we add another crafted local model. We repeat this
process until ﬁnding a solution λ. We ﬁnd that such iterative
searching process makes our attack more effective for Krum
in the partial knowledge scenario. After solving λ, we obtain
the crafted local model w(cid:5)
1. Then, like the full knowledge
scenario, we randomly sample c− 1 vectors whose distance
1 is at most ε as the other c−1 compromised local models.
to w(cid:5)
= w(cid:5)