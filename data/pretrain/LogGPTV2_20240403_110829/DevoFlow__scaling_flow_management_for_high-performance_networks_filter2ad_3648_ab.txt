lecting ﬂow statistics, possibly down to the speciﬁc ﬂow-on-
link level. This allows the controller to re-route or throttle
problematic ﬂows, and to forecast future network loads. For
example, NOX [45] “can utilize real-time information about
network load ... to install ﬂows on uncongested links.”
However, load-balancing does not require the controller
to be aware of the initial setup of every ﬂow. First, some
ﬂows (“mice”) may be brief enough that, individually, they
are of no concern, and are only interesting in the aggregate.
Second, some QoS-signiﬁcant best-eﬀort ﬂows might not be
distinguishable as such at ﬂow-setup time – that is, the con-
troller cannot tell from the ﬂow setup request whether a ﬂow
will become suﬃciently intense (an “elephant”) to be worth
handling individually.
Instead, the controller should be able to eﬃciently de-
tect elephant ﬂows as they become signiﬁcant, rather than
paying the overhead of treating every new ﬂow as a poten-
tial elephant. The controller can then re-route problematic
elephants in mid-connection, if necessary. For example, Al
Fares et al. proposed Hedera, a centralized ﬂow scheduler
for data-center networks [5]. Hedera requires detection of
“large” ﬂows at the edge switches; they deﬁne “large” as 10%
of the host-NIC bandwidth. The controller schedules these
elephant ﬂows, while the switches route mice ﬂows using
equal-cost multipath (ECMP) to randomize their routes.
Example 2: Energy-aware routing, where routing mini-
mizes the amount of energy used by the network, can sig-
niﬁcantly reduce the cost of powering a network by making
the network power-proportional [8]; that is, its power use
is directly proportional to utilization. Proposed approaches
including shutting oﬀ switch and router components when
they are idle, or adapting link rates to be as minimal as pos-
sible [3,7,27,28,40]. For some networks, these techniques can
give signiﬁcant energy savings: up to 22% for one enterprise
workload [7] and close to 50% on another [40].
However, these techniques do not save much energy on
high-performance networks. Mahadevan et al. [34] found
that, for their Web 2.0 workload on a small cluster, link-rate
adaption reduced energy use by 16%, while energy-aware
routing reduced it by 58%. We are not aware of a similar
comparison for port sleeping vs. energy-aware routing; how-
ever, it is unlikely that putting network components to sleep
could save signiﬁcant amounts of energy in such networks.
This is because these networks typically have many aggrega-
tion and core switches that aggregate traﬃc from hundreds
or thousands of servers. It is unlikely that ports can be tran-
sitioned from sleep state to wake state quickly enough to save
signiﬁcant amounts of energy on these switches.
We conclude that some use of a central controller is
necessary to build a power-proportional high-performance
network. The controller requires utilization statistics for
links and at least some visibility of ﬂows in the network.
Heller et al. [29] route all ﬂows with the controller to achieve
energy-aware routing; however, it may be possible to per-
form energy-aware routing without full ﬂow visibility. Here,
the mice ﬂows should be aggregated along a set of least-
energy paths using wildcard rules, while the elephant ﬂows
should be detected and re-routed as necessary, to keep the
congestion on powered-on links below some safety threshold.
OpenFlow switches are relatively simple and future-
proof because policy is imposed by controller software,
rather than by switch hardware or ﬁrmware. Clearly, we
would like to maintain this property. We believe that De-
voFlow, while adding some complexity to the design, main-
tains a reasonable balance of switch simplicity vs. system
performance, and may actually simplify the task of a switch
designer who seeks a high-performance implementation.
3. OPENFLOW OVERHEADS
Flow-based networking involves the control-plane more
frequently than traditional networking, and therefore has
higher overheads. Its reliance on the control-plane has intrin-
sic overheads: the bandwidth and latency of communication
between a switch and the central controller (§3.1). It also has
implementation overheads, which can be broken down into
implementation-imposed and implementation-speciﬁc over-
heads (§3.2). We also show that hardware changes alone can-
not be a cost-eﬀective way to reduce ﬂow-based switching
overheads in the near future (§3.3).
3.1
Intrinsic overheads
Flow-based networking intrinsically relies on a communi-
cation medium between switches and the central controller.
This imposes both network load and latency.
To set up a bi-directional ﬂow on an N -switch path, Open-
Flow generates 2N ﬂow-entry installation packets, and at
least one initial packet in each direction is diverted ﬁrst to
and then from the controller. This adds up to 2N + 4 ex-
tra packets.1 These exchanges also add latency—up to twice
the controller-switch RTT. The average length of a ﬂow in
the Internet is very short, around 20 packets per ﬂow [46],
and datacenter traﬃc has similarly short ﬂows, with the me-
dian ﬂow carrying only 1 KB [9, 24, 30]. Therefore, full ﬂow-
by-ﬂow control using OpenFlow generates a lot of control
traﬃc—on the order of one control packet for every two or
three packets delivered if N = 3, which is a relatively short
path, even within a highly connected network.
In terms of network load, OpenFlow’s one-way ﬂow-setup
overhead (assuming a minimum-length initial packet, and
ignoring overheads for sending these messages via TCP)
is about 94 + 144N bytes to or from the controller—e.g.,
about 526 bytes for a 3-switch path. Use of the optional
ﬂow-removed message adds 88N bytes. The two-way cost
is almost double these amounts, regardless of whether the
controller sets up both directions at once.
3.2
Implementation overheads
In this section, we examine the overheads OpenFlow im-
poses on switch implementations. We ground our discus-
sion in our experience implementing OpenFlow on the HP
ProCurve 5406zl [1] switch, which uses an ASIC on each
multi-port line card, and also has a CPU for management
functions. This experimental implementation has been de-
ployed in numerous research institutions.
While we use the 5406zl switch as an example throughout
this section, the overheads we discuss are a consequence of
both basic physics and of realistic constraints on the hard-
ware that a switch vendor can throw at its implementa-
tion. The practical issues we describe are representative of
those facing any OpenFlow implementation, and we believe
that the 5406zl is representative of the current generation of
Ethernet switches. OpenFlow also creates implementation-
imposed overheads at the controller, which we describe after
our discussion of the overheads incurred at switches.
3.2.1 Flow setup overheads
Switches have ﬁnite bandwidths between their data- and
control-planes, and ﬁnite compute capacity. These issues can
1The controller could set up both directions at once, cutting
the cost to N + 2 packets; NOX apparently has this optimization.
limit the rate of ﬂow setups—the best implementations we
know of can set up only a few hundred ﬂows per second.
To estimate the ﬂow setup rate of the ProCurve 5406zl, we
attached two servers to the switch and opened the next con-
nection from one server to the other as soon as the previous
connection was established. We found that the switch com-
pletes roughly 275 ﬂow setups per second. This number is
in line with what others have reported [43].
However, this rate is insuﬃcient for ﬂow setup in a high-
performance network. The median inter-arrival time for
ﬂows at data center server is less than 30 ms [30], so we
expect a rack of 40 servers to initiate approximately 1300
ﬂows per second—far too many to send each ﬂow to the
controller.
The switch and controller are connected by a fast physical
medium, so why is the switch capable of so few ﬂow setups
per second? First, on a ﬂow-table miss, the data-plane must
invoke the switch’s control-plane, in order to encapsulate the
packet for transmission to the controller.2 Unfortunately, the
management CPU on most switches is relatively wimpy, and
was not intended to handle per-ﬂow operations.
Second, even within a switch, control bandwidth may be
limited, due to cost considerations. The data-plane within a
linecard ASIC is very fast, so the switch can make forwarding
decisions at line rate. On the other hand, the control data-
path between the ASIC and the CPU is not frequently used
in traditional switch operation, so this is typically a slow
path. The line-card ASIC in the 5406zl switch has a raw
bandwidth of 300 Gbit/sec, but we measured the loopback
bandwidth between the ASIC and the management CPU at
just 80 Mbit/sec. This four-order-of-magnitude diﬀerence is
similar to observations made by others [13].
A switch’s limited internal bandwidth and wimpy CPU
limits the data rate between the switch and the central
controller. Using the 5406zl, we measured the bandwidth
available for ﬂow-setup payloads between the switch and the
OpenFlow controller at just 17 Mbit/sec.
We also measured the latency imposed. The ASIC can
forward a packet within 5 µs, but we measured a round-trip
time of 0.5 ms between the ASIC and the management CPU,
and an RTT of 2 ms between that CPU and the OpenFlow
controller. A new ﬂow is delayed for at least 2 RTTs (for-
warding the initial packet via the controller is delayed until
the ﬂow-setup RTT is over).
far
This ﬂow-setup latency is
too high for high-
performance networks, where most ﬂows carry few bytes and
latency is critical. Work from machines that miss their dead-
line in an interactive job is not included in the ﬁnal results,
lowering their quality and potentially reducing revenue. As
a result, adding even 1ms delay to a latency-sensitive ﬂow
is “intolerable” [6]. Also, others have observed that the de-
lay between arrival of a TCP ﬂow’s ﬁrst packet and the
controller’s installation of new ﬂow-table entries can create
many out-of-order packets, leading to a collapse of the ﬂow’s
initial throughput [50], especially if the switch-to-controller
RTT is larger than that seen by the ﬂow’s packets.
Alternative approaches minimize these overheads but lose
some of the beneﬁts of OpenFlow. DIFANE [51] avoids these
2While it might be possible to do a simple encapsulation en-
tirely within the data-plane, the OpenFlow speciﬁcation requires
the use of a secure channel, and it might not be feasible to im-
plement the Transport Layer Security (TLS) processing, or even
unencrypted TCP, without using the switch’s CPU.
overheads by splitting pre-installed OpenFlow wildcard rules
among multiple switches, in a clever way that ensures all
decisions can be made in the data-plane. However, DIFANE
does not address the issue of global visibility of ﬂow states
and statistics. The types of management solutions we would
like to enable (e.g., [5,29]) rely on global visibility and there-
fore it is unlikely they can be built on top of DIFANE. An-
other alternative, Mahout [17], performs elephant ﬂow clas-
siﬁcations at the end-hosts, by looking at the TCP buﬀer of
outgoing ﬂows, avoiding the need to invoke the controller for
mice. Since this approach requires end-host modiﬁcations, it
does not meet our goal of a drop-in replacement for Open-
Flow.
3.2.2 Gathering ﬂow statistics
Global ﬂow schedulers need timely access to statistics. If a
few, long-lived ﬂows constitute the majority of bytes trans-
ferred, then a scheduler can get by collecting ﬂow statistics
every several seconds; however, this is not the case in high-
performance networks, where most of the longest-lived ﬂows
last only a few seconds [30].
OpenFlow supports three per-ﬂow counters (packets;
bytes; ﬂow duration) and provides two approaches for mov-
ing these statistics from switch to controller:
• Push-based: The controller learns of the start of a ﬂow
whenever it is involved in setting up a ﬂow. Option-
ally, OpenFlow allows the controller to request an asyn-
chronous notiﬁcation when a switch removes a ﬂow ta-
ble entry, as the result of a controller-speciﬁed per-ﬂow
timeout. (OpenFlow supports both idle-entry timeouts
and hard timeouts.) If ﬂow-removed messages are used,
this increases the per-ﬂow message overhead from 2N +2
to 3N + 2. The existing push-based mechanism does not
inform the controller about the behavior of a ﬂow before
the entry times out, as a result, push-based statistics are
not currently useful for ﬂow scheduling.
• Pull-based: The controller can send a Read-State mes-
sage to retrieve the counters for a set of ﬂows matching
a wild-card ﬂow speciﬁcation. This returns 88F bytes for
F ﬂows. Under ideal settings, reading the statistics for
16K exact-match rules and the 1500 wild-card rules sup-
ported on the 5406zl would return 1.3 MB; doing this