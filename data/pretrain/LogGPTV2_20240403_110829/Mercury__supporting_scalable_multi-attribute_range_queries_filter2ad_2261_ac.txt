pander has the property that random walks over the links of
such a network converge very quickly to the stationary dis-
tribution of the random walk. Since the hub overlay graph
is regular, the stationary distribution is the uniform distri-
bution. We state the lemma in a semi-rigorous manner. 6
Lemma 1. Let G be a circle on n nodes with O(log n) ad-
ditional links per node generated using the harmonic proba-
bility distribution (as described in Section 4.1). Let (cid:5)1 de-
note the stationary distribution of a random walk on G and
let (cid:5)t denote the distribution generated by the random walk
after t steps. Then, with high probability, d1((cid:5)t; (cid:5)1)  O(logc(n=(cid:15))) for small constants c, where d1 denotes the
statistical or L1 distance between two distributions. (See [20]
for rigorous de(cid:12)nitions.)
This leads to a very simple algorithm for performing ran-
dom sampling: send o(cid:11) a sample-request message with
a small (e.g., log n hop) Time-To-Live (TTL). Every node
along the path selects a random neighbor link and forwards
it, decrementing the TTL. The node at which the TTL ex-
pires sends back a sample. Notice that this algorithm uses
6The proof is omitted for reasons of space, and will be avail-
able in a related tech-report.
only local information at every stage in the sampling process
and adapts easily to a highly dynamic distributed overlay.
In addition, these messages could be piggy-backed on any
existing keep-alive tra(cid:14)c between neighbors to reduce over-
head. Our simulations (Sec. 5.1) show that Mercury can
indeed perform near-perfect uniform random sampling us-
ing a TTL of log n.
We now describe three important ways in which we utilize
random sampling in our system viz., to maintain node-count
histograms, for estimating the selectivity of queries and for
e(cid:11)ective load balancing.
4.2.1 Maintaining Approximate Histograms
This section presents the mechanism used by nodes for
maintaining histograms of any system statistic (e.g., load
distribution, node-count distribution7, etc.) The basic idea
is to sample the distribution locally and exchange these esti-
mates throughout the system in an epidemic-style protocol.
Let Nd denote the \local" d-neighborhood of a node -
i.e., the set of all nodes within a distance d ignoring the
long distance links. Each node periodically samples nodes
2 Nd and produces a local estimate of the system statis-
tic under consideration. For example,
if the node-count
distribution is being measured, a node’s local estimate is
(Ma(cid:0)ma)jNdj=(Pk2Nd jrkj) where rk is the range of a node
k and ma; Ma are the minimum and maximum attribute val-
ues for the attribute a. In our experiments, we use d = 3.
In addition, a node periodically samples k1 nodes uni-
formly at random using the sampling algorithm described
in Section 4.2. Each of these nodes reports back its local
estimate and the most recent k2 estimates it has received.
As time progresses, a node builds a list of tuples of the form:
fnode id; node range; time; estimateg. (The timestamp is
used to age out old estimates.) Each of these tuples repre-
sent a point on the required distribution { stitching them
together yields a piecewise linear approximation.
k1 and k2 are parameters of the algorithm which trade-o(cid:11)
between overhead and accuracy of the histogram mainte-
nance process. In Section 5, we show through simulations
that setting each of k1 and k2 to log(n) is su(cid:14)cient to give
reasonably accurate histograms for sampling population dis-
tribution.
If the system needs to generate an average or histogram of
node properties, the collected samples can be used exactly
as they are collected. However, if the desire is to generate an
average or histogram of properties around the routing hub, a
minor modi(cid:12)cation is needed. Namely, in order to generate
unbiased node-count histograms, the samples received are
weighted di(cid:11)erently; samples reporting lower densities are
given higher weight to account for the fact that there would
be less nodes to produce low density samples.
Once a histogram is constructed, long distance links are
formed as follows: (cid:12)rst, the number of nodes n in the system
is estimated. For each long-distance link, a value nl between
[1; n] is generated using the harmonic distribution. This
represents the number of nodes that must be skipped along
the circle (in the clockwise direction, let’s say) to get to the
desired neighbor. The histogram is then used to estimate a
value vl that this desired neighbor will be responsible for.
Finally, a join message is sent to this value vl which will get
routed to the desired neighbor using the existing routing
network.
7Number of nodes responsible for a given range of values.
4.3 Query Selectivity
Recall that a query Q is sent to only one of the attribute
hubs in AQ. Also a query Q is a conjunction of its predicates
each of which can have varying degree of selectivity. For
example, some predicate might be a wildcard for its attribute
while another might be an exact match. Clearly, a wildcard
predicate will get (cid:13)ooded to every node within its attribute
hub. Thus, the query Q should be sent to that hub for
which Q is most selective to minimize the number of nodes
that must be contacted.
The problem of estimating the selectivity of a query has
been very widely studied in the database community. The
established canonical solution is to maintain approximate
histograms of the number of database records per bucket. In
our case, we want to know the number of nodes in a particu-
lar bucket. Each node within a hub can easily gather such an
histogram for its own hub using the histogram maintenance
mechanism described above. In addition, using its inter-hub
links, it can also gather histograms for other hubs e(cid:14)ciently.
These histograms are then used to determine the selectivity
of a subscription for each hub. We see in Section 5.4 that
even with a very conservative workload, this estimation can
reduce a signi(cid:12)cant amount of query (cid:13)ooding.
4.4 Data Popularity and Load Balancing
When a node joins Mercury, it is assigned responsibility
for some range of an attribute. Unfortunately, in many ap-
plications, a particular range of values may exhibit a much
greater popularity in terms of database insertions or queries
than other ranges. This would cause the node responsible
for the popular range to become overloaded. One obvious
solution is to determine some way to partition the ranges in
proportion to their popularity. As load patterns change, the
system should also move nodes around as needed.
We leverage our approximate histograms to help imple-
ment load-balancing in Mercury. First, each node can use
histograms to determine the average load existing in the sys-
tem, and, hence, can determine if it is relatively heavily or
lightly loaded. Second, the histograms contain information
about which parts of the overlay are lightly loaded. Using
this information, heavily loaded nodes can send probes to
lightly loaded parts of the network. Once the probe encoun-
ters a lightly loaded node, it requests this lightly loaded
node to gracefully leave its location in the routing ring and
re-join at the location of the heavily loaded node. This leave
and re-join e(cid:11)ectively increases the load on the neighboring
(also likely to be lightly-loaded) nodes and partitions the
previous heavy load across two nodes.
Let the average load in the system be denoted by (cid:22)L. De-
(cid:12)ne the local load of a node as the average of load of itself,
its successor and its predecessor. A node is said to be lightly
loaded if the ratio of its local load to (cid:22)L is less than 1
(cid:11) and
heavily loaded if the ratio is greater than (cid:11). This de(cid:12)nition
ensures that if a node is lightly loaded, its neighbors will be
lightly loaded with a high probability. If this is not the case
(when the ratio of neighbor loads is > (cid:11)), the lighter neigh-
bor performs a load balance with the heavier one to equalize
their loads. It is easy to show8 that the leave-rejoin protocol
described above decreases the variance of the load distribu-
tion at each step and bounds the maximum load imbalance
8We omit the proof for reasons of space. The idea is simply
that variance reduction ‘near’ the heavier node is larger than
the variance increase ‘near’ the lighter node.
in the converged system by a factor of (cid:11), provided (cid:11) (cid:21) p2.
By tolerating a small skew, we prevent load oscillations in
the system.
Over time, the leaves and re-joins result in a shift in the
distribution of nodes to re(cid:13)ect the distribution of load. How-
ever, this shift in node distribution can have signi(cid:12)cant im-
plications. Many of the properties of Mercury’s routing and
sampling rely on the harmonic distance distribution of the
random long-links. When nodes move to adjust to load,
this distribution may be changed. However, our technique
for creating long-links actually takes the node distribution
into account explicitly as stated previously.
We note that a similar leave-join based load balancing
mechanism has been proposed concurrently in [13] and [8].
However, [13] and [8] do not handle skewed node range dis-
tributions. Because it exploits our random sampling mech-
anism (from Sec. 4.2), our load balancing mechanism works
even in the presence of skewed node ranges.
5. EVALUATION
This section presents a detailed evaluation of the Mercury
protocol using simulations. We implemented a simple dis-
crete event-based simulator which assigns each application
level hop a unit delay. To reduce overhead and enable the
simulation of large networks, the simulator does not model
any queuing delays or packet loss on links. The simpli(cid:12)ed
simulation environment was chosen for two reasons: (cid:12)rst, it
allows the simulations to scale to a large (up to 50K) num-
ber of nodes, and secondly, this evaluation is not focused on
proximity routing. Since our basic design is similar in spirit
to Symphony and Chord, we believe that heuristics for per-
forming proximity-based routing (as described in [10]) can
be adapted easily to Mercury.
Our evaluation centers on two main features of the Mer-
cury system:
1) scalable routing for queries and data-
records, and 2) balancing of routing load throughout the sys-
tem. We begin with an evaluation of our core routing mech-
anisms { random sampling and histogram maintenance. We
then study the impact of these mechanisms on the overall
routing performance under various workloads. Finally, we
present results showing the utility of caching and query se-
lectivity estimation in the context of Mercury.
Except for query selectivity estimation, most of our ex-
periments focus on the routing performance of data within
a single routing hub. Hence, n will denote the number of
nodes within a hub. Unless stated otherwise, every node
establishes k = log n intra-hub long-distance links. For the
rest of the section, we assume without loss of generality that
the attribute under consideration is a float value with range
[0; 1]. Each node in our experiments is thus responsible for
a value interval (cid:26) [0; 1].
In what follows, NodeLink denotes the ideal small-world
overlay, i.e., long distance links are constructed using the
harmonic distribution on node-link distance. ValueLink de-
notes the overlay when the harmonic distribution on value-
distance is used (Section 3.3). HistoLink denotes the sce-
nario when links are created using node-count histograms
(see Section 4.) Note that the performance of the ValueLink
overlay is representative of the performance of a plain DHT
(e.g., Chord, Symphony) under the absence of hashing and
in the presence of load balancing algorithms which preserve
value contiguity.
For evaluating the e(cid:11)ect of non-uniform node ranges on
our protocol, we assign each node a range width which is
inversely proportional to its popularity in the load distribu-
tion. Such a choice is reasonable since load balancing would
produce precisely such a distribution { more nodes would
participate in a region where load is high. The ranges are
actually assigned using a Zipf distribution.
In particular,
data values near 0:0 are most popular and hence a large
number of nodes share responsibility for this region, each
taking care of a very small node range. For reference, in
our simulator setup, these are also the nodes with lowest
numeric IDs.
5.1 Random›Walk Based Sampling
The goal of our random-walk based sampling algorithm is
to produce a uniform random sample of the nodes in the sys-
tem. We measure the performance of our algorithm in terms
of the statistical distance (alternatively called L1 distance)
of the perfect uniform distribution from the distribution ob-
tained via the random walks. For these experiments, nodes
are assigned ranges using a highly-skewed Zipf distribution
((cid:11) = 0:95). In each sampling experiment, we pick a node at
random and record the distribution of the samples taken by
kn random walks starting from this node. If our sampling
algorithm is good, the random walks should hit each node
roughly k times. Note that the parameter k is just for eval-
uating the distribution obtained { the protocol does not use
it in any manner.
Figure 3(a) plots the accuracy of the sampling process
as the degree of the graph and the TTL for the random-
walks is varied. The underlying overlay we consider is a
perfect small-world network (NodeLink). We (cid:12)nd that, over
a certain threshold (log n), the TTL of the random-walks
does not in(cid:13)uence sampling accuracy. Also, the sampled
distribution is almost perfectly random for graph degrees
c log n, where c is a small constant. In practice, we found
that, for routing purposes, su(cid:14)ciently accurate histograms
are obtained even for c = 1.
Figure 3(b) shows how the construction of the underly-
ing network a(cid:11)ects sampling accuracy. We see that the
NodeLink and HistoLink overlays perform much better than
the ValueLink (a vanilla DHT without hashing and in the
presence of load balancing) overlay. These e(cid:11)ects are ex-
plained using Figure 4 which plots the distribution of long-
distance links. As described earlier, in our experiments,
nodes with the lowest identi(cid:12)ers (responsible for values near
0:0) are the most popular while nodes at the other end of
the value range are the least popular.
Recall that, in a ValueLink overlay, nodes construct their
links by routing to values generated using a harmonic dis-
tribution. However, in this case node ranges are not uni-
formly distributed { in particular, nodes near the value 1:0
(i.e., nodes with higher IDs) are less popular, so they are
in charge of larger range values. Hence, the long-distance
links they create tend to skip over less nodes than appro-
priate. This causes all the links (and correspondingly, the
random walks) to crowd towards the least popular end of
the circle. The HistoLink overlay o(cid:11)sets this e(cid:11)ect via the
maintained histograms and achieves sampling accuracy close
to that achieved by the optimal NodeLink overlay.
Each sample-request message travels for TTL hops and
hence obtaining one random sample generates TTL addi-
tional messages in the overall system. However, all these
messages are sent over existing long-distance links. Thus,
i
n
o
i
t
u
b
i
r
t
s
d
m
r
o
f
i
n
u
m
o
r
f
e
c
n
a
t
s
d
.
t
a
t
S
i
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
TTL: 3 hops
TTL: 6 hops
TTL: 14 hops
TTL: 50 hops
TTL: 100 hops
0
50
100
150
200
250
Number of links per node
(a)
i
n
o
i
t
u
b
i
r
t
s
d
m
r
o
f
i
n
U
m
o
r
f
e