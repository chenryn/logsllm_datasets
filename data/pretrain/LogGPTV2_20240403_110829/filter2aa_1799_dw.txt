writer
Page fault
Page fault
NtCreateSection
MmCreateSection
CcCopyRead
CcCopyWrite
FastloRead, FastloWrite
loPageRead
loAsynchronousPageWrite
NtReadFile/NtWriteFile
IRP
Noncached
and paging I/O
CcFastCopyRead
CcFastCopyWrite
FIGURE 11-11 File system interaction with cache and memory managers.
The next three sections explain these cache access mechanisms, their purpose, and how they’re used.
584 
CHAPTER 11 Caching and file systems
Copying to and from the cache
Because the system cache is in system space, it’s mapped into the address space of every process. As 
with all system space pages, however, cache pages aren’t accessible from user mode because that 
would be a potential security hole. (For example, a process might not have the rights to read a file 
whose data is currently contained in some part of the system cache.) Thus, user application file reads 
and writes to cached files must be serviced by kernel-mode routines that copy data between the 
cache’s buffers in system space and the application’s buffers residing in the process address space.
Caching with the mapping and pinning interfaces
Just as user applications read and write data in files on a disk, file system drivers need to read and write 
the data that describes the files themselves (the metadata, or volume structure data). Because the file 
system drivers run in kernel mode, however, they could, if the cache manager were properly informed, 
modify data directly in the system cache. To permit this optimization, the cache manager provides 
functions that permit the file system drivers to find where in virtual memory the file system metadata 
resides, thus allowing direct modification without the use of intermediary buffers.
If a file system driver needs to read file system metadata in the cache, it calls the cache manager’s 
mapping interface to obtain the virtual address of the desired data. The cache manager touches all the 
requested pages to bring them into memory and then returns control to the file system driver. The file 
system driver can then access the data directly.
If the file system driver needs to modify cache pages, it calls the cache manager’s pinning services, 
which keep the pages active in virtual memory so that they can’t be reclaimed. The pages aren’t actu-
ally locked into memory (such as when a device driver locks pages for direct memory access transfers). 
Most of the time, a file system driver will mark its metadata stream as no write, which instructs the 
memory manager’s mapped page writer (explained in Chapter 5 of Part 1) to not write the pages to 
disk until explicitly told to do so. When the file system driver unpins (releases) them, the cache manager 
releases its resources so that it can lazily flush any changes to disk and release the cache view that the 
metadata occupied.
The mapping and pinning interfaces solve one thorny problem of implementing a file system: buffer 
management. Without directly manipulating cached metadata, a file system must predict the maxi-
mum number of buffers it will need when updating a volume’s structure. By allowing the file system to 
access and update its metadata directly in the cache, the cache manager eliminates the need for buf-
fers, simply updating the volume structure in the virtual memory the memory manager provides. The 
only limitation the file system encounters is the amount of available memory.
Caching with the direct memory access interfaces
In addition to the mapping and pinning interfaces used to access metadata directly in the cache, the 
cache manager provides a third interface to cached data: direct memory access (DMA). The DMA 
functions are used to read from or write to cache pages without intervening buffers, such as when a 
network file system is doing a transfer over the network.
CHAPTER 11 Caching and file systems
585
The DMA interface returns to the file system the physical addresses of cached user data (rather than 
the virtual addresses, which the mapping and pinning interfaces return), which can then be used to 
transfer data directly from physical memory to a network device. Although small amounts of data (1 KB 
to 2 KB) can use the usual buffer-based copying interfaces, for larger transfers the DMA interface can 
result in significant performance improvements for a network server processing file requests from re-
mote systems. To describe these references to physical memory, a memory descriptor list (MDL) is used. 
(MDLs are introduced in Chapter 5 of Part 1.)
Fast I/O
Whenever possible, reads and writes to cached files are handled by a high-speed mechanism named 
fast I/O. Fast I/O is a means of reading or writing a cached file without going through the work of 
generating an IRP. With fast I/O, the I/O manager calls the file system driver’s fast I/O routine to see 
whether I/O can be satisfied directly from the cache manager without generating an IRP.
Because the cache manager is architected on top of the virtual memory subsystem, file system driv-
ers can use the cache manager to access file data simply by copying to or from pages mapped to the 
actual file being referenced without going through the overhead of generating an IRP.
Fast I/O doesn’t always occur. For example, the first read or write to a file requires setting up the 
file for caching (mapping the file into the cache and setting up the cache data structures, as explained 
earlier in the section “Cache data structures”). Also, if the caller specified an asynchronous read or write, 
fast I/O isn’t used because the caller might be stalled during paging I/O operations required to satisfy 
the buffer copy to or from the system cache and thus not really providing the requested asynchronous 
I/O operation. But even on a synchronous I/O operation, the file system driver might decide that it can’t 
process the I/O operation by using the fast I/O mechanism—say, for example, if the file in question has 
a locked range of bytes (as a result of calls to the Windows LockFile and UnlockFile functions). Because 
the cache manager doesn’t know what parts of which files are locked, the file system driver must check 
the validity of the read or write, which requires generating an IRP. The decision tree for fast I/O is 
shown in Figure 11-12.
These steps are involved in servicing a read or a write with fast I/O:
1.
A thread performs a read or write operation.
2.
If the file is cached and the I/O is synchronous, the request passes to the fast I/O entry point of
the file system driver stack. If the file isn’t cached, the file system driver sets up the file for cach-
ing so that the next time, fast I/O can be used to satisfy a read or write request.
3.
If the file system driver’s fast I/O routine determines that fast I/O is possible, it calls the cache
manager’s read or write routine to access the file data directly in the cache. (If fast I/O isn’t pos-
sible, the file system driver returns to the I/O system, which then generates an IRP for the I/O
and eventually calls the file system’s regular read routine.)
4.
The cache manager translates the supplied file offset into a virtual address in the cache.
586 
CHAPTER 11 Caching and file systems
5.
For reads, the cache manager copies the data from the cache into the buffer of the process
requesting it; for writes, it copies the data from the buffer to the cache.
6.
One of the following actions occurs:
• For reads where FILE_FLAG_RANDOM_ACCESS wasn’t specified when the file was opened,
the read-ahead information in the caller’s private cache map is updated. Read-ahead may
also be queued for files for which the FO_RANDOM_ACCESS flag is not specified.
• For writes, the dirty bit of any modified page in the cache is set so that the lazy writer will
know to flush it to disk.
• For write-through files, any modifications are flushed to disk.
No
Generate IRP
NtReadFile
Synchronize
and cached
data?
Fast I/O
possible?
Is file cached?
Cache manager
copies data to or
from process buffer
Cache manager
initializes cache
Cache complete
Yes
No
No
Yes
Yes
File system driver
Cache manager
Is Synchronous?
Return pending
Yes
No
Is Synchronous?
No
Yes
FIGURE 11-12 Fast I/O decision tree.
Read-ahead and write-behind
In this section, you’ll see how the cache manager implements reading and writing file data on behalf of 
file system drivers. Keep in mind that the cache manager is involved in file I/O only when a file is opened 
without the FILE_FLAG_NO_BUFFERING flag and then read from or written to using the Windows I/O 
CHAPTER 11 Caching and file systems
587
functions (for example, using the Windows ReadFile and WriteFile functions). Mapped files don’t go 
through the cache manager, nor do files opened with the FILE_FLAG_NO_BUFFERING flag set.
Note When an application uses the FILE_FLAG_NO_BUFFERING flag to open a file, its file I/O 
must start at device-aligned offsets and be of sizes that are a multiple of the alignment size; 
its input and output buffers must also be device-aligned virtual addresses. For file systems, 
this usually corresponds to the sector size (4,096 bytes on NTFS, typically, and 2,048 bytes on 
CDFS). One of the benefits of the cache manager, apart from the actual caching performance, 
is the fact that it performs intermediate buffering to allow arbitrarily aligned and sized I/O.
Intelligent read-ahead
The cache manager uses the principle of spatial locality to perform intelligent read-ahead by predicting 
what data the calling process is likely to read next based on the data that it’s reading currently. Because 
the system cache is based on virtual addresses, which are contiguous for a particular file, it doesn’t 
matter whether they’re juxtaposed in physical memory. File read-ahead for logical block caching is 
more complex and requires tight cooperation between file system drivers and the block cache because 
that cache system is based on the relative positions of the accessed data on the disk, and, of course, 
files aren’t necessarily stored contiguously on disk. You can examine read-ahead activity by using the 
Cache: Read Aheads/sec performance counter or the CcReadAheadIos system variable.
Reading the next block of a file that is being accessed sequentially provides an obvious performance 
improvement, with the disadvantage that it will cause head seeks. To extend read-ahead benefits to 
cases of stridden data accesses (both forward and backward through a file), the cache manager main-
tains a history of the last two read requests in the private cache map for the file handle being accessed, 
a method known as asynchronous read-ahead with history. If a pattern can be determined from the 
caller’s apparently random reads, the cache manager extrapolates it. For example, if the caller reads 
page 4,000 and then page 3,000, the cache manager assumes that the next page the caller will require 
is page 2,000 and prereads it.
Note Although a caller must issue a minimum of three read operations to establish a pre-
dictable sequence, only two are stored in the private cache map.
To make read-ahead even more efficient, the Win32 CreateFile function provides a flag indicating 
forward sequential file access: FILE_FLAG_SEQUENTIAL_SCAN. If this flag is set, the cache manager 
doesn’t keep a read history for the caller for prediction but instead performs sequential read-ahead. 
However, as the file is read into the cache’s working set, the cache manager unmaps views of the file 
that are no longer active and, if they are unmodified, directs the memory manager to place the pages 
belonging to the unmapped views at the front of the standby list so that they will be quickly reused. It 
also reads ahead two times as much data (2 MB instead of 1 MB, for example). As the caller continues 
reading, the cache manager prereads additional blocks of data, always staying about one read (of the 
size of the current read) ahead of the caller.
588 
CHAPTER 11 Caching and file systems
The cache manager’s read-ahead is asynchronous because it’s performed in a thread separate 
from the caller’s thread and proceeds concurrently with the caller’s execution. When called to re-
trieve cached data, the cache manager first accesses the requested virtual page to satisfy the request 
and then queues an additional I/O request to retrieve additional data to a system worker thread. The 
worker thread then executes in the background, reading additional data in anticipation of the caller’s 
next read request. The preread pages are faulted into memory while the program continues executing 
so that when the caller requests the data it’s already in memory.
For applications that have no predictable read pattern, the FILE_FLAG_RANDOM_ACCESS flag can 
be specified when the CreateFile function is called. This flag instructs the cache manager not to attempt 
to predict where the application is reading next and thus disables read-ahead. The flag also stops the 
cache manager from aggressively unmapping views of the file as the file is accessed so as to minimize 
the mapping/unmapping activity for the file when the application revisits portions of the file.
Read-ahead enhancements
Windows 8.1 introduced some enhancements to the cache manager read-ahead functionality. File system 
drivers and network redirectors can decide the size and growth for the intelligent read-ahead with the 
CcSetReadAheadGranularityEx API function. The cache manager client can decide the following:
I 
Read-ahead granularity Sets the minimum read-ahead unit size and the end file-offset of
the next read-ahead. The cache manager sets the default granularity to 4 Kbytes (the size of a
memory page), but every file system sets this value in a different way (NTFS, for example, sets
the cache granularity to 64 Kbytes).
Figure 11-13 shows an example of read-ahead on a 200 Kbyte-sized file, where the cache granu-
larity has been set to 64 KB. If the user requests a nonaligned 1 KB read at offset 0x10800, and
if a sequential read has already been detected, the intelligent read-ahead will emit an I/O that
encompasses the 64 KB of data from offset 0x10000 to 0x20000. If there were already more
than two sequential reads, the cache manager emits another supplementary read from offset
0x20000 to offset 0x30000 (192 Kbytes).
FIGURE 11-13 Read-ahead on a 200 KB file, with granularity set to 64KB.
I 
Pipeline size For some remote file system drivers, it may make sense to split large read-ahead I/
Os into smaller chunks, which will be emitted in parallel by the cache manager worker threads. A 
network file system can achieve a substantial better throughput using this technique.
CHAPTER 11 Caching and file systems
589
I 
Read-ahead aggressiveness File system drivers can specify the percentage used by the
cache manager to decide how to increase the read-ahead size after the detection of a third se-
quential read. For example, let’s assume that an application is reading a big file using a 1 Mbyte
I/O size. After the tenth read, the application has already read 10 Mbytes (the cache manager
may have already prefetched some of them). The intelligent read-ahead now decides by how
much to grow the read-ahead I/O size. If the file system has specified 60% of growth, the for-
mula used is the following:
(Number of sequential reads * Size of last read) * (Growth percentage / 100)
So, this means that the next read-ahead size is 6 MB (instead of being 2 MB, assuming that the
granularity is 64 KB and the I/O size is 1 MB). The default growth percentage is 50% if not modi-
fied by any cache manager client.
Write-back caching and lazy writing
The cache manager implements a write-back cache with lazy write. This means that data written to 
files is first stored in memory in cache pages and then written to disk later. Thus, write operations are 
allowed to accumulate for a short time and are then flushed to disk all at once, reducing the overall 
number of disk I/O operations.
The cache manager must explicitly call the memory manager to flush cache pages because other-
wise the memory manager writes memory contents to disk only when demand for physical memory 
exceeds supply, as is appropriate for volatile data. Cached file data, however, represents nonvolatile 
disk data. If a process modifies cached data, the user expects the contents to be reflected on disk in a 
timely manner.
Additionally, the cache manager has the ability to veto the memory manager’s mapped writer 
thread. Since the modified list (see Chapter 5 of Part 1 for more information) is not sorted in logical 
block address (LBA) order, the cache manager’s attempts to cluster pages for larger sequential I/Os to 
the disk are not always successful and actually cause repeated seeks. To combat this effect, the cache 
manager has the ability to aggressively veto the mapped writer thread and stream out writes in virtual 
byte offset (VBO) order, which is much closer to the LBA order on disk. Since the cache manager now 
owns these writes, it can also apply its own scheduling and throttling algorithms to prefer read-ahead 
over write-behind and impact the system less.
The decision about how often to flush the cache is an important one. If the cache is flushed too 
frequently, system performance will be slowed by unnecessary I/O. If the cache is flushed too rarely, 
you risk losing modified file data in the cases of a system failure (a loss especially irritating to users 
who know that they asked the application to save the changes) and running out of physical memory 
(because it’s being used by an excess of modified pages).
590 
CHAPTER 11 Caching and file systems
To balance these concerns, the cache manager’s lazy writer scan function executes on a system 
worker thread once per second. The lazy writer scan has different duties:
I 
Checks the number of average available pages and dirty pages (that belongs to the current
partition) and updates the dirty page threshold’s bottom and the top limits accordingly. The
threshold itself is updated too, primarily based on the total number of dirty pages written in the
previous cycle (see the following paragraphs for further details). It sleeps if there are no dirty
pages to write.
I 
Calculates the number of dirty pages to write to disk through the CcCalculatePagesToWrite in-
ternal routine. If the number of dirty pages is more than 256 (1 MB of data), the cache manager
queues one-eighth of the total dirty pages to be flushed to disk. If the rate at which dirty pages
are being produced is greater than the amount the lazy writer had determined it should write,
the lazy writer writes an additional number of dirty pages that it calculates are necessary to
match that rate.
I 
Cycles between each shared cache map (which are stored in a linked list belonging to the cur-
rent partition), and, using the internal CcShouldLazyWriteCacheMap routine, determines if the
current file described by the shared cache map needs to be flushed to disk. There are different
reasons why a file shouldn’t be flushed to disk: for example, an I/O could have been already
initialized by another thread, the file could be a temporary file, or, more simply, the cache map
might not have any dirty pages. In case the routine determined that the file should be flushed
out, the lazy writer scan checks whether there are still enough available pages to write, and, if
so, posts a work item to the cache manager system worker threads.
Note The lazy writer scan uses some exceptions while deciding the number of dirty pages 
mapped by a particular shared cache map to write (it doesn’t always write all the dirty pages 
of a file): If the target file is a metadata stream with more than 256 KB of dirty pages, the cache 
manager writes only one-eighth of its total pages. Another exception is used for files that have 
more dirty pages than the total number of pages that the lazy writer scan can flush.
Lazy writer system worker threads from the systemwide critical worker thread pool actually perform 
the I/O operations. The lazy writer is also aware of when the memory manager’s mapped page writer 
is already performing a flush. In these cases, it delays its write-back capabilities to the same stream to 
avoid a situation where two flushers are writing to the same file.
Note The cache manager provides a means for file system drivers to track when and how 
much data has been written to a file. After the lazy writer flushes dirty pages to the disk, 
the cache manager notifies the file system, instructing it to update its view of the valid data 
length for the file. (The cache manager and file systems separately track in memory the valid 
data length for a file.)
CHAPTER 11 Caching and file systems
591