==== LinearRegression
对线性回归的详细解释可参考wiki: 
image::images/ml-lr-model.png[]
yi为目标值，Xi为样本特征，beta为所求参数。
目标函数：
image::images/ml-lr-model-2.png[]
每个模型都会有一个对应的目标函数，可以通过不同的最优化求解方法（梯度下降，牛顿法等等）对这些对应的目标函数进行求解。线性回归模型，我们知道实际上是通过多个自变量对自变量进行曲线拟合。我们希望找到一条可以较好拟合的曲线，该模型通常适用于线性问题，对于非线性问题，可通过加入高阶（多项式）变量 来进行拟合,不过采用较多的高阶变量的缺点是容易产生过拟合。
训练：cost function 最小二乘法， 训练过程的目的就是找出最优的参数beta，使得cost function 最小。 对于线性回归，常见的参数优化方法一般有两种 梯度下降和 直接矩阵求解， 当数据量较大时，多采用梯度下降的方法，因为矩阵求解复杂度较高为O(n^3)。
Syntax: 
    fit LinearRegression [params set]  from  [into model_name]
Parameters:
* : model 中的 y 目标变量名，必选
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]: 算法相关参数，可选
** fit_intercept : boolean, optional
+
whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
** normalize : boolean, optional, default False
+
If True, the regressors X will be normalized before regression. This parameter is ignored when fit_intercept is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use preprocessing.StandardScaler before calling fit on an estimator with normalize=False.
以波士顿地区房价预测为例, 样本数据字段如下:
1. CRIM: per capita crime rate by town 
2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. 
3. INDUS: proportion of non-retail business acres per town 
4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 
5. NOX: nitric oxides concentration (parts per 10 million) 
6. RM: average number of rooms per dwelling 
7. AGE: proportion of owner-occupied units built prior to 1940 
8. DIS: weighted distances to five Boston employment centres 
9. RAD: index of accessibility to radial highways 
10. TAX: full-value property-tax rate per $10,000 
11. PTRATIO: pupil-teacher ratio by town 
12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town 
13. LSTAT: % lower status of the population 
14. MEDV: Median value of owner-occupied homes in $1000's
从属性中可以看出我们要用1-13 的属性去训练并预测第14个属性也就是房价的中位数(k)。样本量一共506条。
我们使用波士顿放假预测数据进行测试，取前400条为fit 训练数据:
  tag:housing | limit 400 | fit LinearRegression json.MEDV from json.CRIM, json.ZN,  json.INDUS, json.CHAS, JSON.NOX, json.RM, json.AGE, json.DIS, json.RAD,json.TAX,json.PTRATIO,json.B,json.LSTAT | rename json.MEDV as actual_price, predicted_json.MEDV as predicted_price | table actual_price, predicted_price
image::images/ml-lr-example-1.png[]
  tag:housing | limit 400 | fit LinearRegression json.MEDV from json.CRIM, json.ZN,  json.INDUS, json.CHAS, JSON.NOX, json.RM, json.AGE, json.DIS, json.RAD,json.TAX,json.PTRATIO,json.B,json.LSTAT | rename  predicted_json.MEDV as price  | eval group = "pre" | table group, price,context_id|append[[tag:housing| limit 400 | eval group = "actual"| rename json.MEDV as price | table group, price,context_id]]
image::images/ml-lr-example-2.png[]
结果图中可以看到浅蓝色线为预测值（是对训练数据的预测），深蓝色为实际值，整体拟合情况还算满意，但是后半段预测值有明显的升高，可以看出此区间并没有实际值，我们对这些没有实际值的数据进行分析，发现这些数据存在着很大的问题，比如RM这个属性，大多为80，90（一个房子有80，90个房间)与我们所训练的数据完全偏离，因此我们可以进一步进行优化，通过预处理去掉这些没有实际值的数据（其他属性也都出现很大的异常）:   
    tag:housing | limit 400 | where !empty(json.MEDV) |fit LinearRegression json.MEDV from json.CRIM, json.ZN,  json.INDUS, json.CHAS, JSON.NOX, json.RM, json.AGE, json.DIS, json.RAD,json.TAX,json.PTRATIO,json.B,json.LSTAT into linear_regression| rename  predicted_json.MEDV as price  | eval group = "pre" | table group, price,context_id|append[[tag:housing| limit 400 | where !empty(json.MEDV) |eval group = "actual"| rename json.MEDV as price | table group, price,context_id]]
image::images/ml-lr-example-3.png[]
接下来我们用剩下的100个数据,同样要过滤掉json.MEDV为null的数据，用于上面训练好的模型 进行apply 预测:
    tag:housing |eval time = tolong(context_id) | sort by +time | limit 100 | where !empty(json.MEDV) | apply linear_regression | rename predicted_json.MEDV as price | eval group = "predict"| table price,time,group | append [[tag:housing |eval time = tolong(context_id) | sort by +time | limit 100 |where !empty(json.MEDV) | rename json.MEDV as price | eval group = "actual" | table price,time,group]] 
image::images/ml-lr-example-4.png[]
接下来我们计算对着100个预测数据的均方误差(MSE):
    tag:housing |eval time = tolong(context_id) | sort by +time | limit 100 | where !empty(json.MEDV) |apply linear_regression | eval residual = (predicted_json.MEDV-json.MEDV) *  (predicted_json.MEDV-json.MEDV) | stats count() as cnt , sum(residual) as sum_residual | eval mse = sum_residual/cnt
image::images/ml-lr-example-mse.png[]
mse 为10.99。这已经是一个较为不错的误差了。
我们可以对原始特征进行分析，对RM进行简单的平方处理，从而增加新的高阶特征:
    tag:housing | limit 400 | where !empty(json.MEDV) |eval square_rm = json.RM * json.RM  |  fit LinearRegression json.MEDV from json.CRIM, json.ZN,  json.INDUS, json.CHAS, JSON.NOX, json.RM, json.AGE, json.DIS, json.RAD,json.TAX,json.PTRATIO,json.B,json.LSTAT,square_rm into housing_train_polynomial_fixed | rename json.MEDV as actual_price, predicted_json.MEDV as predicted_price | table actual_price, predicted_price 
然后apply:
    tag:housing |eval time = tolong(context_id) | sort by +time | limit 100 | where !empty(json.MEDV) | eval square_rm = json.RM *json.RM | apply housing_train_polynomial_fixed | eval residual = (predicted_json.MEDV-json.MEDV) *  (predicted_json.MEDV-json.MEDV) | stats count() as cnt , sum(residual) as sum_residual | eval mse = sum_residual/cnt
image::images/ml-lr-example-mse-1.png[]
从结果图中看到，mse 从10.99 降到了8.68有了改进，这点从现实中也能看出RM代表的是每个房子的房间数，一般而言房间数和房价有着很强的正相关的关系，所以我们加强该特征，可以是结果预测的更好。