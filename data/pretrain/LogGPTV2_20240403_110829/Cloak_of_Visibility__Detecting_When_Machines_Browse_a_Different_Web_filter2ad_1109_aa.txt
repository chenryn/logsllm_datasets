title:Cloak of Visibility: Detecting When Machines Browse a Different Web
author:Luca Invernizzi and
Kurt Thomas and
Alexandros Kapravelos and
Oxana Comanescu and
Jean Michel Picod and
Elie Bursztein
2016 IEEE Symposium on Security and Privacy
2016 IEEE Symposium on Security and Privacy
Cloak of Visibility: Detecting When Machines
Browse A Different Web
Luca Invernizzi∗, Kurt Thomas∗, Alexandros Kapravelos†,
Oxana Comanescu∗, Jean-Michel Picod∗, and Elie Bursztein∗
∗Google, Inc. {invernizzi, kurtthomas, elieb, oxana, jmichel}@google.com
†North Carolina State University PI:EMAIL
Abstract—The contentious battle between web services and
miscreants involved in blackhat search engine optimization and
malicious advertisements has driven the underground to develop
increasingly sophisticated techniques that hide the true nature
of malicious sites. These web cloaking techniques hinder the
effectiveness of security crawlers and potentially expose Internet
users to harmful content. In this work, we study the spectrum
of blackhat cloaking techniques that target browser, network, or
contextual cues to detect organic visitors. As a starting point, we
investigate the capabilities of ten prominent cloaking services
marketed within the underground. This includes a ﬁrst look
at multiple IP blacklists that contain over 50 million addresses
tied to the top ﬁve search engines and tens of anti-virus and
security crawlers. We use our ﬁndings to develop an anti-cloaking
system that detects split-view content returned to two or more
distinct browsing proﬁles with an accuracy of 95.5% and a false
positive rate of 0.9% when tested on a labeled dataset of 94,946
URLs. We apply our system to an unlabeled set of 135,577
search and advertisement URLs keyed on high-risk terms (e.g.,
luxury products, weight loss supplements) to characterize the
prevalence of threats in the wild and expose variations in cloaking
techniques across trafﬁc sources. Our study provides the ﬁrst
broad perspective of cloaking as it affects Google Search and
Google Ads and underscores the minimum capabilities necessary
of security crawlers to bypass the state of the art in mobile,
rDNS, and IP cloaking.
I. INTRODUCTION
The arms race nature of abuse has spawned a contentious
battle in the realm of web cloaking. Here, miscreants seeking
to short-circuit the challenge of acquiring user trafﬁc turn
to search engines and advertisement networks as a vehicle
for delivering scams, unwanted software, and malware to
browsing clients. Although crawlers attempt to vet content and
expunge harmful URLs, there is a fundamental limitation to
browsing the web: not every client observes the same content.
While split views occur naturally due to personalization, geo
optimization, and responsive web design, miscreants employ
similar targeting techniques in a blackhat capacity to serve
enticing and policy-abiding content exclusively to crawlers
while simultaneously exposing victims to attacks.
Where as a wealth of prior work focused on understanding
the prevalence of cloaking and the content behind cloaked
doorways, none precisely measured the spectrum of cloaking
techniques in the wild as it affects search engines and ad
networks. Indeed, earlier studies predicated their analysis on
a limited set of known cloaking techniques. These include
redirect cloaking in search engine results [16], [18], [24],
[27], [33], [34] or search visitor proﬁling based on the
User-Agent and Referer of HTTP requests [32], [35].
An open question remains as to what companies and crawlers
blackhat cloaking software targets, the capabilities necessary
for security practitioners to bypass state of the art cloaking,
and ultimately whether blackhat techniques generalize across
trafﬁc sources including search results and advertisements.
In this paper, we marry both an underground and empirical
perspective of blackhat cloaking to study how miscreants scru-
tinize an incoming client’s browser, network, and contextual
setting and the impact it has on polluting search results and
advertisements. We root our study in the blackmarket, directly
engaging with specialists selling cloaking software. In total,
we obtain ten cloaking packages that range in price from $167
to $13,188. Our investigation reveals that cloaking software
spans simple Wordpress plugins written in PHP that check the
User-Agent of incoming clients, to fully customized forks of
the Nginx web server with built-in capabilities for blacklisting
clients based on IP addresses, reverse DNS, User-Agents,
HTTP headers, and the order of actions a client takes upon vis-
iting a cloaked webpage. We also obtain access to multiple IP
blacklist databases, one of which covers 54,166 IP addresses
associated with Bing, Yahoo, Google, Baidu, and Yandex,
and a second that contains over 50 million IP addresses
from universities (e.g., MIT, Rutgers), security products (e.g.,
Kaspersky, McAfee), VPNs, and cloud providers. Our analysis
yields a unique perspective of which web services miscreants
seek to evade and the technical mechanisms involved.
We leverage our tear-down of blackhat cloaking techniques
to build a scalable de-cloaking crawler and classiﬁer that
detects when a web server returns divergent content to two
or more distinct browsing clients. We fetch content from 11
increasingly sophisticated user emulators that cover a com-
bination of Chrome, Android, and a simple robot accessing
the Internet via residential, mobile, and data center networks
including one associated with Google’s crawling infrastruc-
ture. In total, we crawl 94,946 labeled training URLs multiple
times from each proﬁle, totaling over 3.5 million fetches. We
then build a classiﬁer that detects deviations in the content,
structure, rendering, linguistic topics, and redirect graph be-
tween all pairs of crawlers, accurately distinguishing blackhat
cloaking from mobile and geo targeting with 95.5% accuracy
and a false positive rate of 0.9%. We analyze in depth which
features and browser proﬁles are critical to detecting cloaking,
2375-1207/16 $31.00 © 2016 IEEE
© 2016, Luca Invernizzi. Under license to IEEE.
DOI 10.1109/SP.2016.50
DOI 10.1109/SP.2016.50
743
743
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
ﬁnding no single approach covers all cloaking techniques.
We apply our system to an unlabeled set of 135,577 Google
Search and Google Ads URLs keyed on high-risk terms com-
monly targeted by miscreants (e.g., luxury products, weight
loss supplements) and ﬁnd 11.7% of the top 100 search results
and 4.9% of ads cloak against the Googlebot crawler. Of
these, the most popular blackhat cloaking techniques involve
detecting JavaScript, blacklisting Googlebot’s User-Agent and
IP, and requiring that visitors interact with content before
the server ﬁnally delivers the de-cloaked payload. Despite
a menagerie of cloaking techniques in the wild that vary
drastically between search and ads, our system nevertheless
succeeds at generalizable detection. We dig into speciﬁc case
studies and their monetization approaches, revealing a thriving
market
that attempts to capitalize on legitimate consumer
interest in nutraceuticals, mobile gaming, and online shopping.
Finally, we explore the fragility of de-cloaking systems,
including our own,
to miscreant’s adapting their cloaking
techniques. Rather than persist in the arms race to defeat
increasingly sophisticated browser ﬁngerprinting techniques,
we argue our approach of comparing the content that cloaked
servers deliver to multiple browsing clients naturally extends
to real rather than emulated clients. We discuss the potential
for client-side detection of cloaking as well as centralized
reporting and scoring. Both of these approaches hinder the
ability of malicious servers to show benign content exclusively
to crawlers, though their adoption must overcome potential
privacy concerns.
In summary, we frame our contributions as follows:
• We provide the ﬁrst broad study of blackhat cloaking
techniques and the companies affected.
• We build a distributed crawler and classiﬁer that detects
and bypasses mobile, search, and ads cloaking, with
95.5% accuracy and a false positive rate of 0.9%.
• We measure the most prominent search and ad cloaking
techniques in the wild; we ﬁnd 4.9% of ads and 11.7%
of search results cloak against Google’s generic crawler.
• We determine the minimum set of capabilities required
of security crawlers to contend with cloaking today.
II. BACKGROUND & RELATED WORK
We begin by outlining the incentives that bad actors have
to conceal their webpages from crawlers. We also summarize
existing techniques that websites employ to distinguish be-
tween crawlers and organic trafﬁc. For the purposes of our
study, we consider websites that deliver optimized content to
small screens or localized visitors to be benign—our focus is
exclusively on blackhat cloaking.
A. Web Cloaking Incentives
Web cloaking refers to the set of techniques that a web
server uses to ﬁngerprint incoming visitors in order to cus-
tomize page content. Benign examples include servers that
redirect mobile clients to pages optimized for small screens
(e.g., m.nytimes.com) as opposed to content-rich desktop
equivalents. The more insidious variety involves serving en-
tirely distinct content to (security) crawlers in order to inﬂate
a page’s search ranking to drive trafﬁc, evade ad quality scan-
ners, or stealthily deliver drive-by exploits only to vulnerable
clients. These techniques create a discrepancy in how the web
is observed by bots and how it is perceived by organic clients.
There are many areas where blackhat cloaking can be
beneﬁcial for miscreants, but here we focus on the following
three categories: search results, advertisements and drive-by
downloads.
Search results: Cloaking is one tool in an arsenal of tech-
niques that miscreants use for Search Engine Optimization
(SEO). Servers will manipulate fake or compromised pages
to appear enticing to crawlers while organic visitors are
shepherded to (illegal) proﬁt-generating content such as store-
fronts selling counterfeit
luxury products, pharmaceuticals,
and dietary supplements [16], [31], [32].
Advertisements: As an alternative to duping crawlers for
free exposure, miscreants will pay advertising networks to
display their URLs. Miscreants rely on cloaking to evade
ad policies that strictly prohibit dietary scams,
trademark
infringing goods, or any form of deceptive advertisements–
including malware [9], [36]. Ad scanners see a benign page
while organic visitors land on pages hosting scams and
malware. Time of check versus time of use (e.g., delayed
URL maliciousness) may also play into a miscreant’s cloaking
strategy.
Drive-by downloads: Miscreants compromise popular web-
sites and laden the pages with drive-by exploits. In order to
evade security crawlers like Wepawet or Safe Browsing that
visit pages with vulnerable browsers [6], [26], these payloads
will ﬁrst ﬁngerprint a client and only attack vulnerable, organic
visitors. While we include this for completeness, we focus our
study on search and ad cloaking.
B. Prevalence of Cloaking
Previous studies have shown that ﬁnding instances of cloak-
ing in the wild requires intimate knowledge of the search
keywords or the vulnerable pages that miscreants target. Wang
et al. estimated only 2.2% of Google searches for trending
keywords contained cloaked results [32]. A broader method for
ﬁnding cloaked URLs involved targeted searches for speciﬁc
cocktails of terms such as “viagra 50mg canada” where
61% of search results contained some form of cloaking [32].
Leontiadis et al. reported a complementary ﬁnding where
32% of searches for pharmaceutical keywords advertised in
spam emails led to cloaked content [16]. Keyword targeting
extends to other realms of fraud: Wang et al. found 29.5% of
search results for cheap luxury products contained URLs that
redirected visitors to a cloaked storefront [31]. Our strategy
for selecting URLs to crawl is built on top of these previous
ﬁndings in order to minimize the bandwidth wasted fetching
benign content.
744744
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
C. Cloaking Detection Techniques
inspect
the redirection chains that
Researchers have responded to the steady adaptation of
cloaking techniques over time with a medley of anti-cloaking
(or de-cloaking) techniques. Early approaches by Wang et
al. relied on a cross-view comparison between search re-
sults fetched by a browser conﬁgured like a crawler and a
second fetch via a browser that emulated a real user [33],
[34]. They classiﬁed a page as cloaking if the redirect chain
deviated across fetches. This same approach dominates sub-
sequent cloaking detection strategies that fetch pages via
multiple browser proﬁles to examine divergent redirects (in-
cluding JavaScript, 30X, and meta-refresh tags) [16], [17],
[32],
lead to poisoned
search results [18], isolate variations in content between two
fetches (e.g., topics, URLs, page structure) [31], [32], [35],
or apply cloaking detection to alternative domains such as
spammed forum URLs [24]. Other approaches exclusively
target compromised webservers and identify clusters of URLs
all with trending keywords that are otherwise irrelevant to
other content hosted on the domain [14]. Our study improves
upon these prior detection strategies. To wit, we build an
anti-cloaking pipeline that addresses previously unexplored
cloaking techniques as gleaned from the underground; and we
explore additional approaches for cross-view comparisons that
contends with page dynamism, interstitial ads, network-based
cloaking, and the absence of divergent redirect paths. Our
pipeline also improves on prior work as it discerns adversarial
cloaking from geo targeting and content optimization for
small screens. It does so by comparing across views both in
terms of textual topic and entities detected in images. These
improvements allow us to measure the dominant cloaking
strategies in the wild, and in turn, inform search, ad, and
malware pipelines that must contend with web cloaking.
III. UNDERGROUND PERSPECTIVE OF CLOAKING
Cloaking—like the commoditization of exploits, proxies,
and hosting [2], [28]—is an infrastructure component for sale
within the underground. To identify potential cloaking services
and software, we ﬁrst exhaustively crawled and indexed a
selection of underground forums. We then ranked forum
discussions based on the frequency of keywords related to
cloaking software. The most discussed cloaking package was
mentioned 623 times, with the author of the software engaging
with the community to provide support and advertise new
deals. The least popular service was mentioned only 2 times.
Through manual analysis and labor, we successfully obtained
the top ten most popular cloaking packages which ranged in
price from $167 for entry level products up to $13,188 for the
most sophisticated advertised features. We note that for legal
protection and to avoid any potential de-anonymization of our
underground identities, we cannot publicly disclose the names
of the underground forums we harvested, or the names of the
cloaking software under analysis.
We analyzed all ten cloaking packages in order to gain an
insight into (1) ﬁngerprinting capabilities; (2) switch logic
for displaying targeted content; and (3) other built-in SEO
745745
TABLE I: Cloaking ﬁngerprinting capabilities reverse engi-
neered from the most sophisticated six samples of cloaking
software.
Capability
IP Address
rDNS
Geolocation
User-Agent
JavaScript
Flash
HTTP Referer
Keywords
Time window
Order of operations
Cloaking Type C1 C2 C3 C4 C5 C6
     
– 
–   
–  
– 

     
– 

–
–
–
     
–

–
–
–
– 
Network
Network
Network
Browser
Browser