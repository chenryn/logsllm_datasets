x2fx2Xjf (x)=yg
Pr [X = x]
Pr [X = x] :
x2f(cid:0)1[fyg]
In a similar manner, the numerator in Eq. 1 is evaluated as
∑
Pr [Xi = a; Y = y] =
Pr [X = x] :
x2fx2Xjf (x)=y^xi=ag
The posterior is therefore derived as shown below
Pr [Xi = ajY = y] =
x2fxjf (x)=y^xi=ag Pr [X = x]
∑
:
(2)
x2f(cid:0)1[fyg] Pr [X = x]
In principle, the probabilistic inference of inputs from out-
puts for general functions is intractable because the size of
the input domain is exponential in the input dimension. For
this study, we presume that the computational resources of
the adversary are unlimited, which indicates that the adver-
sary can evaluate Pr [XijY = y] using exhaustive evaluation
of y = f (x) for every x 2 X.
2.3 Mechanism
To prevent the adversary from estimating the private in-
put, the publisher can modify or abstract the output so that
the inputs are not uniquely identi(cid:12)ed. We designate an al-
gorithm that modi(cid:12)es outputs for reducing the risk of input
inference as a mechanism.
In the following sections, we restrict the output domain to
the real. Also, we suppose mechanisms output real continu-
ous intervals as the abstraction of function outputs. Thus, a
534mechanism is de(cid:12)ned as a map M : R ! I where I is the set
of real continuous intervals. The performance of mechanism
is measured by privacy and utility, which are de(cid:12)ned in the
following subsections.
2.4 Privacy Measure
We de(cid:12)ne (cid:11)-obscure privacy as the measure of the input
inference risk of mechanisms. Intuitively, the probabilistic
inference of the posterior helps the adversary to infer the
private input if the posterior obtained as a result of inference
is signi(cid:12)cantly diﬀerent from the prior. A similar idea is
presented in [15] as the Uniformative Principle. The notion
of (cid:11)-obscure privacy is de(cid:12)ned based on this intuition.
Definition 1. Let Pr [Xi = xi] be the prior distribution
of xi. Let y = M(x) be the output of function M : X ! I.
Then M is (cid:11)-obscure private with respect to the i-th input
attribute if, for all a 2 Xi and for all x 2 X,
jPr [Xi = ajY = y] (cid:0) Pr [Xi = a]j (cid:20) (cid:11):
Actually, (cid:11) is equal to 0 if the posterior is equal to the
prior, which indicates the inference gives no information
about the input to the adversary. On the other hand, if
the posterior is signi(cid:12)cantly diﬀerent from the prior, then (cid:11)
takes a value greater than 0, which indicates that the infer-
ence gives information that helps the adversary to infer the
private input. For any mechanism M, (cid:11) is bounded with
the prior probabilities as Eq. 3
0 (cid:20) (cid:11) (cid:20) max
a2Xi
maxfPr [Xi = a] ; 1 (cid:0) Pr [Xi = a]g :
(3)
The proof, which is almost obvious, is omitted here.
The (cid:11)-obscure privacy measures the privacy breach on
only a single input attribute. We emphasize the necessity
of devoting attention to (cid:11)-obscure privacy for all attributes
to assess the entire privacy breach. Additionally, we remark
that we cannot compare (cid:11) of diﬀerent attributes directly.
Actually, the (cid:11) of an attribute is dependent on the prior
probability of the attribute. The prior probabilities of at-
tributes are all diﬀerent. To compare the inference risk of
two or more attributes, one must see both (cid:11) and the prior
probabilities.
The following condition similar to (cid:11)-obscure privacy has
been proposed to judge a privacy breach [16].
Pr [Xi = ajY = y]
(cid:20) (cid:13); Pr [Xi = a] < (cid:14); Pr [Xi = ajY = y] < (cid:27):
Pr [Xi = a]
The following discussion in this paper fully works with the
privacy de(cid:12)nition above, but unless otherwise stated, we use
(cid:11)-obscure privacy as the privacy measure.
2.5 Utility Measure
The risk of inference will be low if there exist two or more
inputs that give the same interval as its output. Hereinafter,
we measure the risk of inference by (cid:11) of the (cid:11)-obscure pri-
vacy of the mechanism.
If the mechanism always returns a constant value or inter-
val for any input, the adversary cannot obtain any informa-
tion from the output, and the posterior remains unchanged
from the prior. This is preferable in terms of privacy. How-
ever, as one might expect, such outputs are useless. To
release meaningful outputs, one must consider both the pri-
vacy measure and the utility measure.
Figure 2: The prior (frequency) distribution of input at-
tribute used for disease susceptibility models of obesity.
The output of a mechanism is more useful if the width
of the interval is smaller, as long as the interval includes
the output corresponding to the true input. The utility of
a mechanism is therefore de(cid:12)ned by a negative expected in-
terval width as
utility(M) = (cid:0)Ex2X [jM(x)j]
= (cid:0)
jM(x)j Pr [X = x] ;
∑
x2X
where j (cid:1) j denotes the width of an interval.
Section 6 presents the design of a mechanism that guar-
antees privacy in terms of (cid:11)-obscure privacy and which si-
multaneously achieves higher utility.
3. DISEASE SUSCEPTIBILITY MODELS
As an important example of input inference from outputs,
we assess the risk of personal genome inference from disease
susceptibility release. Disease susceptibility is known to be
aﬀected by both genetic factors and clinical factors.
Susceptibility models we used in this study are built with
a dataset collected for an epidemiological study of chronic
kidney disease [19]. We use the same data to build models
of obesity. The dataset collected by the study includes 442
SNP pro(cid:12)les and 10 clinical features3 of 5202 subjects.
We consider the susceptibility models of obesity. For eval-
uation of the susceptibility of obesity, 10 SNPs out of 442
SNPs are selected as the genetic features by hypothetical
testing.
In addition, the ten clinical features mentioned
above are used for susceptibility modeling. The risk model
is built with logistic regression using the selected SNPs and
clinical features.
Let x 2 f0; 1gd denote the genetic features and xc 2 Rd
′
signify clinical features of a subject. The i-th input attribute
value xi of x is given as a Boolean value, which denotes
whether or not the i-th SNP is the major (or minor) allele.
xc represents the clinical feature vector. All features are
represented as binaries, integers, or real values.
Disease susceptibility r 2 R is predicted using the follow-
ing logistic regression
r = (cid:27) (w
⊺
x + w
⊺
c xc) ;
(4)
3Clinical features include age, sex, BMI, smoking history,
blood creatinine, medical history of diabetes mellitus, hy-
pertriglyceridemia, hypoalphalipoproteinemia, hyperbetal-
ipoproteinemia, high blood pressure
535where w 2 Rd; wc 2 Rd
donates the logistic sigmoid function.
′
are the model parameters and (cid:27)((cid:1))
Fig. 2 presents the prior probabilities Pr [Xi = 1] of 10
SNP features (x1; x2; : : : ; x10) used in the disease suscepti-
bility model for obesity.
4. PERSONAL GENOME INFERENCE FROM
DISEASE SUSCEPTIBILITY RELEASE
As already described in Section 2, we assume that the
adversary has full access to the model (e.g., w; wc) and
the prior distribution (e.g., the frequency distribution of the
SNPs Pr [X]). When the susceptibility evaluation is used for
medication, it is natural to assume that the model is publicly
released.
In addition, the frequency distributions (includ-
ing correlations) of the SNPs are widely shared for research
purposes. In this problem, we further presume that the ad-
versary can learn the clinical features of targets (e.g., xc) as
prior knowledge because we can expect that adversary can
collect such common features of targets easily. In summary,
as the background knowledge, we assume that the adversary
has prior Pr [X], model parameters w; wc, and clinical fea-
tures xc. In this setting, given the disease susceptibility r
of a subject, the adversary attempts to identify SNPs of the
subject by probabilistic inference.
By rearranging Eq. 4, we have w
⊺
c xc.
Because the adversaries have r; wc and xc, the adversary
can readily evaluate w
x. Consequently, letting
(cid:0)1(r) (cid:0) w
x = (cid:27)
⊺
⊺
f (x) = w
⊺
x;
the problem of input inference is reduced to estimation of
the posterior
[
(cid:12)(cid:12)X 2 f
Pr
Xi
]
(cid:0)1 [fw
⊺
xg]
⊺
where w
is readily obtained by evaluation of Eq. 2.
x is given as an output. The posterior probability
We experimentally evaluated the posterior distribution
of all possible genetic features when disease susceptibilities
(obesity) were released with no modi(cid:12)cation. The signi(cid:12)-
cant digits of the model parameter w were changed from six
to one. When the size of the input domain is (cid:12)xed, the func-
tion f tends to be more injective as the number of signi(cid:12)cant
digits increases.
The results are presented in Table 1. Figures in the ta-
ble represent the rate of inputs (SNPs) that are identi(cid:12)ed
uniquely when the outputs (disease susceptibilities) are re-
vealed. From the result, it is apparent that if the signi(cid:12)cant
digits of the model parameter are greater than (cid:12)ve, then f
becomes injective and reveals private genetic features com-
pletely. Even with one signi(cid:12)cant digit, more than 50 % of
inputs are uniquely identi(cid:12)ed for three out of ten genetic fea-
tures. In any settings shown in Table 1, the probability that
unique identi(cid:12)cation occurs is not zero. This means that
in terms of (cid:11)-obscure privacy, (cid:11) always reaches the upper
bound.
In view of the fact that a model with one-digit precision
can be useless for many medical applications, it is apparently
diﬃcult to balance utility and privacy when the disease sus-
ceptibility is revealed with no modi(cid:12)cation or abstraction.
5. RELEASING DISEASE SUSCEPTIBILITY
WITH EQUALLY PARTITIONED INTER-
VAL
5.1 Equally Partitioning Mechanism
In the previous section, we experimentally demonstrated
that the release of disease susceptibilities causes unique iden-
ti(cid:12)cation of SNPs even when the signi(cid:12)cant digits of the
model parameters are few. To reduce the inference risk,
abstraction of disease susceptibilities before release is neces-
sary.
Ayday et al. introduced a mechanism that partitions the
output domain evenly and which outputs the interval includ-
ing the output value [6]. We brie(cid:13)y summarize the mech-
anism herein. Let [tmin; tmax] be the output domain. The
equally partitioning mechanism divides the output domain
into n disjoint intervals. Given output t, the mechanism
returns the interval that includes the output. The disjoint
interval Ik is de(cid:12)ned as
)
8<:
[
[
Ik =
(n(cid:0)k+1)tmin+(k(cid:0)1)tmax
tmin+(n(cid:0)1)tmax
n
; tmax
n
]
; (n(cid:0)k)tmin+ktmax
n
(1 (cid:20) k < n)
(k = n)
where k is the index of the interval. We describe the set of
all disjoint intervals as In = fIkgn
k=1.
The rounding function roundn is de(cid:12)ned as
roundn(t) = I where I 2 In such that t 2 I:
The equally partitioning mechanism Mroundn : X ! I is
de(cid:12)ned as presented below.
Mroundn (x) = roundn ◦ f (x):
5.2 Utility and Privacy of Equally Partition-
ing Mechanism
As the partitioning number increases, the output interval
width decreases, and the output becomes more exact. For
instance, when [tmin; tmax] = [0; 1] and n = 5, the interval
set includes (cid:12)ve intervals:
[
)
[
)
[
)
[
]
)
[
0;
1
5
;
1
5
;
2
5
;
2
5
;
3
5
;
3
5
;
4
5
;
; 1
:
4
5
When the input is 0.3, the rounding function returns
The utility is evaluated as
utility(Mroundn ) = (cid:0) tmax (cid:0) tmin
:
n
The utility is markedly improved by increasing the parti-
tioning number n.
The privacy guaranteed by the equally partitioning mech-