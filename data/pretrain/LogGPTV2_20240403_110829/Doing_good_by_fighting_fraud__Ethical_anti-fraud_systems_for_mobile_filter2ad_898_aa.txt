title:Doing good by fighting fraud: Ethical anti-fraud systems for mobile
payments
author:Zain ul Abi Din and
Hari Venugopalan and
Henry Lin and
Adam Wushensky and
Steven Liu and
Samuel T. King
0
0
1
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
Doing good by ﬁghting fraud: Ethical anti-fraud
systems for mobile payments
Zainul Abi Din∗, Hari Venugopalan∗, Henry Lin†, Adam Wushensky†, Steven Liu†, Samuel T. King∗†
∗ University of California, Davis
† Bouncer Technologies
Abstract—App builders commonly use security challenges, a
form of step-up authentication, to add security to their apps.
However, the ethical implications of this type of architecture has
not been studied previously.
In this paper, we present a large-scale measurement study of
running an existing anti-fraud security challenge, Boxer, in real
apps running on mobile devices. We ﬁnd that although Boxer
does work well overall, it is unable to scan effectively on devices
that run its machine learning models at less than one frame per
second (FPS), blocking users who use inexpensive devices.
With the insights from our study, we design Daredevil, a
new anti-fraud system for scanning payment cards that works
well across the broad range of performance characteristics
and hardware conﬁgurations found on modern mobile devices.
Daredevil reduces the number of devices that run at less than
one FPS by an order of magnitude compared to Boxer, providing
a more equitable system for ﬁghting fraud.
In total, we collect data from 5,085,444 real devices spread
across 496 real apps running production software and interacting
with real users.
I. INTRODUCTION
Smartphones and apps are ubiquitous, with billions of daily
users and over 5 million apps available for everything from
dating and travel to payments and food deliveries. Unfortu-
nately, smartphones and apps have also ushered in a new
generation of attacks [7], [11], [37], forcing app builders
to design and implement user-centric security measures, or
challenges, in their apps [35], [6], [30]. Examples of this new
style of veriﬁcation include Apple’s FaceID where they use
face biometrics to authenticate a user [25], Uber’s credit-card
scanning where they ask users to scan their card to prove that
they possess it [50], [40], Coinbase’s ID veriﬁcation where
they ask users to scan an ID to prove who they are in the real
world [26], and Lime’s Access program that allows people of
a low socioeconomic status to scan IDs and utility bills to
prove that they qualify for discounted rental fees [28].
Challenges have the potential to skirt the difﬁcult ethical
issues that apps face with security decisions in their apps.
In a typical app, the app will have an algorithm that predicts
whether a user or a transaction is suspicious. These algorithms
could potentially rely on features that unfairly inﬂuence its
decision, such as a zip code. To reduce the impact of mistakes
by their algorithms, apps can use user-centric security mea-
sures in lieu of suspending users or blocking transactions. This
technique allows users that the algorithm blocks incorrectly
to verify themselves or their payment methods automatically.
Thus, even if their algorithm has bias [19], challenges provide
an avenue for making sure that everyone can access the app.
Unfortunately, challenges open a new set of ethical co-
nundrums. Apps that want to respect end-user privacy and
run their challenges via compute intensive machine learning
models on the device will have to cope with the 1-4 orders of
magnitude difference in capabilities on the devices that they
will see in practice (Section III). Apps that opt for predictable
ML performance by streaming data to a server and running
their ML there will have to deal with a 1000x difference
in bandwidth between 3G and 5G networks [52], and the
people who use it may have to pay for that bandwidth directly.
Security challenges must deal with these subtleties of practical
deployments or else they will block users unethically.
The most dangerous aspect of the ethical implications inher-
ent with security challenges is that they solve an app’s business
problem but have the potential to still make compromises on
users of a low socioeconomic status. One example of this
tradeoff is with Lime’s Access program [28]. Lime allows
users from low-income households to get reduced rates with
Lime rentals by proving that they qualify for the program by
scanning welfare documents or utility bills. These documents
contain personal information that typical Lime users do not
have to provide, and Lime does not process these documents
themselves, they use a third party for this service [27]. Just
to be clear, we, as proponents of this program applaud Lime
for implementing it, but Lime forces users to give up privacy
to qualify. It would be better if they could prove that these
documents are genuine without needing to send sensitive
information to a third-party server.
A second example is Boxer [9], a system presented at
Usenix Security 2020 for scanning credit cards to prove that
the user possesses the genuine physical card. Boxer uses client-
side machine learning to verify the credit card. However,
based on our measurement study of Boxer’s open-source card
scanner (Section III), Boxer fails on 68.13% of Android
devices that run its ML at less than one FPS. Slower ML
inference corresponds to lower frame rates and thus, fewer
inputs that the system processes for veriﬁcation. Like Lime’s
Access program, apps that use Boxer solve their business
problem – only 4.19% of the total devices that we measure are
Android devices that run Boxer’s ML at less than one FPS. By
using Boxer, apps recover most of the people that their security
systems ﬂag incorrectly. However, by blocking devices that are
unable to run their ML models fast enough, they run the risk
© 2021, Zainul Abi Din. Under license to IEEE.
DOI 10.1109/SP40001.2021.00100
1623
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:08 UTC from IEEE Xplore.  Restrictions apply. 
of denying access to at-risk populations simply because they
have an inexpensive device.
The inability to run challenges on resource-constrained
devices introduces a new bias that the existing formulations
of machine learning fairness [29], [10], [19], [31], [5] are ill-
equipped to solve. Existing formulations of machine learning
fairness modify either the decision engine or the feature set
corresponding to an individual of a protected group to ensure
that protected attributes (e.g., race) do not affect the outcome.
However, being unable to run models on resource-constrained
devices robs the decision engine of the inputs it needs to
make a decision in the ﬁrst place. Although the decision
engine could randomly pass individuals whose devices are
low end or randomly block otherwise good users to provide a
notion of fairness, both degrade the performance of the overall
system since they weaken their ability to distinguish between
legitimate and fraudulent users. No algorithmic or theoretical
notion of fairness can account for this lack of data.
Our position is that ethical security challenges should run
client side, support complex machine learning (if needed), and
run effectively on resource-constrained devices. In this paper,
we present Daredevil, a system for running complex client-
side ML models for security on the full range of devices
one is likely to see in practice today. Daredevil’s design
includes decomposing machine learning tasks for redundancy
and efﬁciency, streamlining individual
tasks for improved
performance, and exploiting task and data parallelism.
We demonstrate Daredevil by designing and implementing
a new credit card scanning and veriﬁcation system. Card
scanners use complex machine learning models and hundreds
of apps use them in practice [9], which make them a good
candidate for Daredevil. We deploy Daredevil to real apps
and demonstrate how it provides access to a wide range of
devices. Through our deployment, we run Daredevil on over
1,580,260 devices from real users and show how Daredevil
both enables resource-constrained hardware to run security
ML models effectively, and it improves the end-to-end success
rates on well-provisioned hardware with support for fast ML
models.
Our contributions are:
• We present the ﬁrst large-scale in-ﬁeld study of on-device
deep learning for security. Our measurements focus on
Boxer, a system for scanning credit cards, where we
demonstrate that due to the degree of hardware diversity,
deep-learning-based security challenges have the poten-
tial of being unethical despite solving the apps’ business
problem.
• We uncover insights from our measurement study such
as critical reasons for failure cases, key system metrics,
and mitigation strategies that developers should consider
when designing a client-side machine learning pipeline.
• Equipped with the insights from our measurement study,
we design, implement, and deploy Daredevil, which em-
powers card scanning and veriﬁcation to run on a wide
range of devices.
II. BACKGROUND: CARD-NOT-PRESENT CREDIT CARD
FRAUD AND CARD SCANNING
Fraudsters acquire stolen credit card information and use
it to make purchases online, without possessing the actual
physical card. This is known as card-not-present credit card
fraud. When the real owner of the card notices a suspicious
charge on their credit card statement, they report it to the credit
card company. Upon investigating the transaction, if the credit
card company ﬁnds the transaction to be fraudulent, they will
issue a chargeback to the app. The app will have to pay back
the money to the real owner of the card, and an additional
dispute fee to the credit card company [48]. This protects the
owner of the credit card and puts the responsibility of curbing
card-not-present credit card fraud on the app.
Recently, researchers propose Boxer [9], a mobile SDK and
server that app builders integrate with apps to prevent card-
not-present credit card fraud. Boxer shows how to scan the
number side of a card and verify that it is genuine. Boxer casts
card veriﬁcation as a machine learning problem that it divides
into three main parts: optical character recognition (OCR),
fake media detection (implemented via screen detection in
their paper), and card tampering detection (called a Bank
Identiﬁcation Number or BIN consistency check in their paper).
OCR pulls the card number, expiration, and legal name off