knowledge. However, if the adversary cannot identify the
true location so that P r(u∗ = si) = 1 for any si ∈ S, Deﬁ-
nition 3.4 is always satisﬁed. On the other hand, if an “om-
nipotent” adversary already knows the true location, then
no mechanism can actually protect location privacy.
3.4 Comparison with Other Deﬁnitions
Diﬀerential Privacy. Since the concept of neighboring
databases is not generally applicable (as discussed earlier),
induced neighborhood [21], metric based neighborhood [5]
and δ-neighborhood [13] were proposed. The general idea
is that the neighborhood can be formulated by some con-
straints of data or distance (metric) functions instead of
adding or removing a record. However, applying these neigh-
borhood based diﬀerential privacy is not feasible in our mod-
el because there is only one sole tuple (location) at each
timestamp without any “neighbors”. Hence we deﬁne δ-
location set to extend the notion of “neighborhood”.
Geo-indistinguishability. Another closely related deﬁ-
nition is the Geo-indistinguishability [1], which protects a
user’s location within a radius (circle) with a “generalized
diﬀerential privacy” guarantee. In other words, the neigh-
borhood is deﬁned with Euclidian distance. Nevertheless,
such spatial perturbation technique may not be reasonable
in reality. For example, as shown in Figure 1, the “gener-
alized diﬀerential privacy” can still be breached given the
road network constraint or user’s moving pattern (which is
represented by Markov model). Thus location privacy must
be protected under temporal correlations.
Blowﬁsh privacy. Our privacy deﬁnition shares the same
insight as the unconstrained Blowﬁsh privacy framework [19]
in statistical data release context, which uses secret pairs
and privacy policy to build a subset of possible database
instances as “neighbors”. We show that δ-location set based
diﬀerential privacy can be instantiated as a special case of
unconstrained Blowﬁsh privacy at each timestamp.
Theorem 3.2. Let S be the domain of all possible lo-
cations. Let G be a complete graph where each node de-
notes a location in S. Let ∆X be a condition such that
x∗ ∈ ∆X. At each timestamp, Deﬁnition 3.3 is equivalent
to {,{S, G, ∆X}}-Blowﬁsh privacy.
3.5 Discussion
Learning Markov Model. Existing methods such as the
knowledge construction module in [37] or EM method in
HMM can be used to acquire the transition matrix M, which
will not be discussed in this paper. However, depending on
the power of adversaries, two typical M can be learned.
I Popular M can be learned from public transit data.
II Personal M can be derived with personal transit data3.
No matter which M is adopted in our framework, the ad-
versarial knowledge is always bounded, as discussed before.
However, the usefulness of released locations may vary for
diﬀerent adversaries. We also compare the two models in
our experiments.
Composibility. Since we only need to release one per-
turbed location at a timestamp, the sequential composition
[28] is not applicable. Otherwise, for multiple releases at a
timestamp the composition of  holds. On the other hand,
given a series of perturbed locations {z1, z2,··· , zt} released
from timestamp 1 to t, a new problem is how to protect and
measure the overall privacy guarantee of the entire trace.
We defer this to future work.
4. SENSITIVITY HULL
The notion of sensitivity indicates the diﬀerences between
any two query answers from two instances in neighboring
databases. However, in multidimensional space, we show
that (cid:96)1-norm sensitivity (in Deﬁnition 2.2) fails to capture
the exact sensitivity. Thus we propose a new notion, sensi-
tivity hull. Note that sensitivity hull is an independent no-
tion from the context of location privacy and can be plugged
in any data-independent perturbation mechanisms.
4.1 Sensitivity Hull
To derive the meaning of sensitivity, let us consider the
following example in traditional setting of diﬀerential priva-
cy.
Example 4.1. Assume we have an employee table T with
attributes gender and income. Then we answer the following
query workload f :
f1 : Select count(∗) f rom T where gender = “f emale”
f2 : Select count(∗) f rom T where income > 50000
Let x1 and x2 be neighboring databases so that x1 is equal
to x2 adding or removing a random user. Suppose f (x2) =
[10, 20]T . Then the possible answers for f (x1) could be one
of the following columns, from which ∆f can be derived.
(cid:20) 11
21
f (x1) =
(cid:21)
(cid:21)
10
21
10
20
9
20
9
19
10
19
11
20
(cid:20) 1
1
0
0
0
1
1 −1 −1
0
0
0 −1 −1
∆f = f (x1) − f (x2) =
Sf = max||∆f||1 = 2 ((cid:96)1-norm sensitivity)
In Figure 4, the dashed lines form the set of ||∆f||1 = 2
because the (cid:96)1-norm sensitivity is 2. However, ∆f only con-
sists of all the “•” points.
It is obvious that the (cid:96)1-norm
sensitivity exaggerates the “real sensitivity”. To capture the
geometric representation of ∆f in multidimensional space,
we deﬁne sensitivity hull (the solid lines in Figure 4) as fol-
lows.
Definition 4.1
(Sensitivity Hull). The sensitivity
hull of a query f is the convex hull of ∆f where ∆f is the
3For example, mobile apps, like Google Now, may have a
user’s location history to derive the user’s moving pattern.
Figure 4: Sensitivity hull of Example 4.1. Solid lines denote the
sensitivity hull K; dashed lines are the (cid:96)1-norm sensitivity.
set of f (x1) − f (x2) for any pair x1 and x2 in δ-location set
∆X.
K = Conv (∆f )
∆f =
x1,x2∈∆X
∪
(f (x1) − f (x2))
Theorem 4.1. A sensitivity hull K is centrally symmet-
ric: if v ∈ K then −v ∈ K.
Theorem 4.2. If data x is in discrete domain, then for
any f : x → Rd, the sensitivity hull of f is a polytope in Rd.
4.2 Error Bound of Differential Privacy
We extend the error bound of diﬀerential privacy in database
context [18] to our location setting using sensitivity hull.
Lemma 4.1. Suppose F : RN → Rd is a linear function.
When neighboring databases are obtained by adding or re-
moving a record, the sensitivity hull K of F is a polytope
FBN
1 is the N -dimensional unit (cid:96)1 ball.
1 where BN
Theorem 4.3
(Lower Bound). Let K be the sensitiv-
ity hull of δ-location set ∆X. To satisfy Deﬁnition 3.3, every
mechanism must have
(cid:19)
(cid:112)Area(K)
(cid:18) 1

Error ≥ Ω
where Area(K) is the area of K.
5. LOCATION RELEASE ALGORITHM
5.1 Framework
The framework of our proposed location release algorithm
is shown in Algorithm 1. At each timestamp, say t, we com-
pute the prior probability vector p−
t . If the location needs
to be released, we construct a δ-location set ∆Xt. Then if
the true location x∗ is excluded in ∆Xt (a drift), we use
surrogate to replace x∗. Next a diﬀerentially private mecha-
nism (like Algorithm 2 which will be presented next) can be
adopted to release a perturbed location zt. In the meantime,
the released zt will also be used to update the posterior prob-
ability p+
t (in the equation below) by Equation (1), which
subsequently will be used to compute the prior probability
for the next timestamp t + 1. Then at timestamp t + 1, the
above process is repeated.
t = si|zt, zt−1,··· , z1)
∗
t [i] = P r(u
p+
f1f2o1−1−11K‘1-normsensitivity2−22−2(cid:46) Markov transition
t
t−1M;
t−1, x∗
t ← p+
Algorithm 1 Framework
Require: t, δ, M, p+
1: p−
2: if location needs to be released then
3:
4:
5:
6:
7:
8:
9: end if
10: return Algorithm 1(t+1, δ, M, p+
Construct ∆Xt;
t /∈ ∆Xt then
if x∗
t ← surrogate;
x∗
end if
zt ←Algorithm 2(t, ∆Xt, x∗
t );
Derive posterior probability p+
t by Equation (1);
t , x∗
t+1);
(cid:46) δ-location set
(cid:46) a drift
(cid:46) release zt
(cid:46) go to next timestamp
Theorem 5.1. At any timestamp t, Algorithm 1 is t-
diﬀerentially private on 0-location set.
Proof. It is equivalent to prove adversarial privacy on
t ∈
0-location set, which includes all possible locations. If x∗
∆Xt, then zt is generated by x∗
t . By Theorem 5.3, zt is
t-diﬀerentially private. So P r(u∗
t =si|zt)
t =si) ≤ e. When x∗
t /∈
P r(u∗
∆Xt, then a surrogate ˜xt replaces x∗
t = si|˜xt = sk)P r(˜xt = sk|zt)
P r(u∗
t = si|˜xt = sk)P r(˜xt = sk)
P r(u∗
Therefore, by equivalence (Theorem 3.1) Algorithm 1 is t-
diﬀerentially private on 0-location set.
(cid:80)
(cid:80)
k P r(u∗
k P r(u∗
t = si|zt)
t = si)
t . Then
=
≤ e
Laplace Mechanism. With the (cid:96)1-norm sensitivity in Def-
inition 2.2, Laplace mechanism (LM) can be adopted in Line
7 of Algorithm 1. The problem of this approach is that it will
over-perturb a location because (cid:96)1-norm sensitivity could be
much larger than the sensitivity hull, as discussed in Section
4. We use LM with δ-location set as a baseline in our ex-
periment.
5.2 Planar Isotropic Mechanism
Because we showed (in Lemma 4.1) that the sensitivity
hull of a query matrix is a polytope (polygon in our two-
dimensional location setting), the state-of-art K-norm based
mechanism [18, 4, 30], can be used.
Definition 5.1
(K-norm Mechanism [18]). Given a
linear function F : RN → Rd and its sensitivity hull K,
a mechanism is K-norm mechanism if for any output z, the
following holds:
1
P r(z) =
Γ(d + 1)Vol(K/)
(5)
where Fx∗ is the true answer, ||·||K is the (Minkowski) norm
of K, Γ() is Gamma function and Vol() indicates volume.
exp (−||z − Fx
∗||K )
However, standard K-norm mechanism was designed for
high-dimensional structure of sensitivity hull, whereas in our
problem a location is only two-dimensional. Thus we can
further optimize K-norm mechanism to achieve the lower
bound of diﬀerential privacy. We propose a Planar Isotropic
Mechanism (PIM) based on K-norm mechanism as follows.
Rationale. The rationale of PIM is that in two-dimensional
space we eﬃciently transform the sensitivity hull to its isotrop-
ic position4 so that the optimality is guaranteed.