# Network Loss Inference with Second-Order Statistics of End-to-End Flows

## Authors
Hung X. Nguyen  
School of Computer and Communication Sciences, EPFL  
CH-1015 Lausanne, Switzerland  
hung.nguyen@epfl.ch  

Patrick Thiran  
School of Computer and Communication Sciences, EPFL  
CH-1015 Lausanne, Switzerland  
patrick.thiran@epfl.ch  

## Abstract
We address the problem of inferring link loss rates from end-to-end measurements. Unlike existing methods that rely solely on average end-to-end loss rates or strict temporal correlations between probes, we exploit the second-order moments of end-to-end flows. We first prove that the variances of link loss rates can be uniquely determined from the covariances of measured end-to-end loss rates in any realistic network topology. After calculating the link variances, we remove un-congested links with small variances from the first-order moment equations to obtain a full-rank linear system, from which we can precisely calculate the loss rates of the remaining congested links. This is possible because losses due to congestion occur in bursts, leading to high variances for congested links, while most Internet links are un-congested, with nearly zero averages and variances of their loss rates. Our proposed solution uses only regular unicast probes, making it applicable in today's Internet. The method is accurate and scalable, as demonstrated by our simulations and experiments on PlanetLab.

## Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Operations—Management, Monitoring

## General Terms
Management, Measurement, Performance

## Keywords
Network Tomography, Identifiability, Inference

## 1. Introduction
Many IP network inference problems are ill-posed, meaning the number of measurements is insufficient to determine a unique solution. For example, traffic matrix estimation involves finding the Origin-Destination (OD) pairs of traffic flows from link counts. Since the number of OD pairs typically exceeds the number of links, the resulting system of equations is under-determined. Heuristics like the gravity model can be used to reduce the set of possible solutions.

In this paper, we address the problem of computing the loss rates of IP links from end-to-end path measurements. This problem is also under-determined when using only the average end-to-end loss rates. Various methods have been proposed to overcome this identifiability issue, but they often require multicast, which is not widely deployed, or involve significant development and administrative costs. Other approaches, such as detecting shared congestion or locating congested links, do not provide the fine-grained information required by many applications.

We propose a method to infer IP link loss rates using only readily available end-to-end measurements. Instead of introducing artificial temporal correlations among probe packets, we exploit the spatial correlations that naturally exist among regular unicast probes. Our approach leverages two key properties of network losses: (1) losses due to congestion occur in bursts, leading to high variances in loss rates for congested links, and (2) most un-congested links in the Internet have virtually zero first- and second-order moments of their loss rates.

After detailing the performance model and assumptions in Section 3, we prove in Section 4 that the variances of link loss rates are statistically identifiable from end-to-end measurements of regular unicast probes in general mesh topologies. This contrasts with the mean loss rates, which are not statistically identifiable. We then provide an algorithm in Section 5.1 that combines end-to-end loss rates to form a linear system of equations relating the link variances with the covariances of the end-to-end loss rates. This system can be solved accurately and scalably using standard linear algebra methods.

Using the learned link variances, we can calculate the link loss rates by exploiting the monotonic relationship between the mean and variance of a link loss rate. By eliminating the least congested links (with the smallest variances) from the first-order moment equations, we obtain a reduced system of full column rank, which can be solved to find the loss rates of the remaining links. Simulations in Section 6 and Internet experiments in Section 7 demonstrate the accuracy and scalability of our algorithm.

## 2. Related Work
The inference of internal link properties from end-to-end measurements is known as Network Performance Tomography. Specifically, we focus on inferring link loss rates from end-to-end measurements. This problem is under-determined when using only the average end-to-end loss rates, as shown in Figure 1.

### Figure 1: A Network with Three End-to-End Paths
A network with three end-to-end paths from beacon B1 to destinations D1, D2, and D3. The numbers next to each link denote the transmission rates. Both sets of link transmission rates result in the same end-to-end transmission rates, demonstrating that link transmission rates cannot be uniquely calculated from the average end-to-end transmission rates.

To overcome the identifiability problem, additional constraints or more pragmatic goals are needed. Most existing end-to-end tomography methods fall into two categories: those that use strong temporal correlation between probing packets in a multicast-like environment, and those that exploit the distribution of congested links in the Internet for a more pragmatic goal of identifying congested links.

Initial methods in the first category [1, 6, 7] use multicast probing packets to infer the loss rates of network links. More recent work [9] shows that all moments of link delays, except the means, are statistically identifiable in a multicast tree. However, statistical identifiability in general topologies remains a conjecture. As multicast is not widely deployed, subsequent methods [12, 13, 29] emulate this approach using clusters of unicast packets, which are less accurate and require substantial development and administrative costs.

Methods in the second category [14, 22, 24, 36] use only unicast end-to-end flows to identify congested links. These methods use additional information or assumptions. For example, [14, 24] identify congested links by finding the smallest set of links whose congestion can explain the observed measurements, assuming that network links are equally likely to be congested and that the number of congested links is small. In [22], multiple network measurements are used to learn the probabilities of network links being congested, allowing the CLINK algorithm to identify congested links with higher accuracy. [36] bypasses the un-identifiability of first-order moment equations by finding the smallest set of consecutive links whose loss rates can be determined, but it cannot calculate link loss rates at the granularity of each individual link.

Other research has focused on detecting shared congestion of network flows by using the correlations between different flows to identify shared bottlenecks. For example, [26] compares the cross-correlation between two flows against the autocorrelation of each flow to infer shared congestion. Harfoush et al. [17] use back-to-back packet pairs to calculate correlations and infer shared congested links. Kim et al. [19] use wavelet denoising techniques to improve upon [26]. Ariﬂer [4] uses the spatial correlations of TCP flow throughput to infer shared congestion with a factor analysis model.

### Table 1: Summary of Techniques in Network Loss Tomography
| Temporal Correlations | First Order Moments | Higher Order Moments | Multicast Packet Trains | Prior Knowledge | Link Groups | One Snapshot | Multiple Snapshots | Shared Congestion | Congested Links | Link Loss Rates |
|----------------------|---------------------|----------------------|-------------------------|-----------------|-------------|--------------|--------------------|------------------|-----------------|-----------------|
| Yes                  | Yes                 | Yes                  | [6, 9]                  | Yes             | Yes         | Yes          | Yes                | Yes              | Yes             | Yes             |
| Yes                  | Yes                 | Yes                  | [12, 13]                | Yes             | Yes         | Yes          | Yes                | Yes              | Yes             | No              |
| Yes                  | Yes                 | No                   | [14, 24]                | Yes             | Yes         | Yes          | Yes                | Yes              | Yes             | No              |
| Yes                  | Yes                 | Yes                  | [36]                    | Yes             | Yes         | Yes          | Yes                | Yes              | Yes             | No              |
| Yes                  | No                  | No                   | [4, 26]                 | Yes             | Yes         | Yes          | Yes                | Yes              | No              | No              |
| Yes                  | Yes                 | No                   | [22]                    | Yes             | Yes         | Yes          | Yes                | Yes              | Yes             | No              |

Non-tomographic techniques for calculating link loss rates, such as [3, 20], use router support and depend heavily on router cooperation. For example, Tulip [20] requires routers to support continuous IP-ID generated ICMP packets, which many routers do not due to security and performance concerns. These approaches also do not scale well as each end-to-end path must be measured separately.

Techniques using higher-order moments for traffic matrix estimations, such as [8, 30], assume unknown variables follow a parametric distribution. Our approach differs in that we infer link properties given end-to-end measurements, without assuming any parametric distribution for the inferred variables.

## 3. Network Model
We consider an overlay inference system consisting of regular users who can send and receive UDP packets and perform traceroute. End-hosts periodically measure the network topology and end-to-end loss rates by sending probes to a set of destinations D and report the results to a central server. These end-hosts, called beacons, are denoted by VB.

### 3.1 Network Topology
The network topology is measured using tools like traceroute. Practical issues with using traceroute to obtain network topologies are discussed in Section 7.

We model the network as a directed graph G(V, E), where V represents network routers/hosts and E represents communication links. The numbers of nodes and edges are denoted by nv = |V| and ne = |E|, respectively. Ps,d denotes the path from source s to destination d, and P is the set of all paths between beacons and destinations. Consecutive links without branching points, called "alias" links, are grouped into a single virtual link through alias reduction.

For a known topology G = (V, E) and a set of paths P, we compute the reduced routing matrix R of dimension np × ne, where np = |P|. The entry Ri,j = 1 if the path Ps,d ≡ Pi, with i = (s, d), contains the link ej and Ri,j = 0 otherwise. Columns with only zero entries are dropped, resulting in a reduced routing matrix R of dimensions np × nc, where nc ≤ ne is the number of covered links, denoted by Ec, |Ec| = nc.

### Figure 2: Example Network Topology
Two beacons B1 and B2 send probes to three destinations D1, D2, and D3. The aggregated routing topology contains 6 end-to-end paths and 8 directed links, with the reduced routing matrix R as shown.

We make the following assumptions about the reduced routing matrix R:
- **T.1 Time-invariant routing:** R remains unchanged throughout the measurement period.
- **T.2 No route fluttering:** There is no pair of paths Pi and Pi′ that share two links ej and ej ′ without also sharing all the links in between.

These assumptions ensure the identifiability of the variances of link loss rates from end-to-end measurements. Assumption T.1 can be violated due to routing changes, but our experiments show that despite potential errors in network topology, our algorithm remains accurate. Assumption T.2 can be violated by load balancing or routing failures, but these are rare and can be handled by keeping measurements on one path and ignoring others. Under T.2, the paths from a beacon form a tree rooted at the beacon.