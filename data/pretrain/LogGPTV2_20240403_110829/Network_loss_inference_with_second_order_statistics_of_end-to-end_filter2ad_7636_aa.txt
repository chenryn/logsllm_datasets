title:Network loss inference with second order statistics of end-to-end
flows
author:Hung Xuan Nguyen and
Patrick Thiran
Network Loss Inference with Second Order Statistics of
End-to-End Flows
School of Computer and Communication
School of Computer and Communication
Hung X. Nguyen
Sciences, EPFL
CH-1015 Lausanne, Switzerland
hung.nguyen@epﬂ.ch
Patrick Thiran
Sciences, EPFL
CH-1015 Lausanne, Switzerland
patrick.thiran@epﬂ.ch
ABSTRACT
We address the problem of calculating link loss rates from
end-to-end measurements. Contrary to existing works that
use only the average end-to-end loss rates or strict temporal
correlations between probes, we exploit second-order mo-
ments of end-to-end ﬂows. We ﬁrst prove that the variances
of link loss rates can be uniquely calculated from the co-
variances of the measured end-to-end loss rates in any real-
istic topology. After calculating the link variances, we re-
move the un-congested links with small variances from the
ﬁrst-order moment equations to obtain a full rank linear
system of equations, from which we can calculate precisely
the loss rates of the remaining congested links. This op-
eration is possible because losses due to congestion occur
in bursts and hence the loss rates of congested links have
high variances. On the contrary, most links on the Internet
are un-congested, and hence the averages and variances of
their loss rates are virtually zero. Our proposed solution
uses only regular unicast probes and thus is applicable in
today’s Internet.
It is accurate and scalable, as shown in
our simulations and experiments on PlanetLab.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations—Management, Monitoring
General Terms
Management, Measurement, Performance
Keywords
Network Tomography, Identiﬁability, Inference
1.
INTRODUCTION
Many IP network inference problems are ill-posed: the
number of measurements are not suﬃcient to determine uniquely
their solution. For example, the traﬃc matrix estimation
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’07, October 24-26, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-908-1/07/0010 ...$5.00.
problem is ﬁnding the Origin-Destination (OD) pairs of traf-
ﬁc ﬂows from the link counts. As the number of OD pairs far
exceeds the number of links, the resulting system of equa-
tions is under-determined. Various heuristics, such as the
gravity model, can then be used to reduce the set of possi-
ble solutions.
Another tomography problem, which we address in this
paper, is to compute the loss rates of IP links from end-
to-end path measurements. This problem, unfortunately,
is also under-determined. Diﬀerent methods to overcome
the identiﬁability issue have been proposed in the literature,
which we review in Section 2. For several practical reasons
these approaches have not been widely used in large scale
applications. For those methods that use multicast, the mul-
ticast routers are not widely deployed. Methods based on
active unicast packet trains present development and admin-
istrative costs associated with deploying probing and data
collection. Other approaches that achieve more pragmatic
targets, such as detecting shared congestion, locating con-
gested links or calculating loss rates of groups of links, can-
not provide ﬁne grain information required by many appli-
cations.
We are interested in inferring IP link loss rates using only
the readily available end-to-end measurements. We argue in
this paper that the regular unicast probes provide enough
information to do so. More precisely, instead of artiﬁcially
introducing temporal correlations among the probe packets
with multicast, we seek to exploit the spatial correlations
that already exist among the regular unicast probes. Our
approach is based on two key properties of network losses.
First, losses due to congestion occur in bursts and thus loss
rates of congested links have high variances. Second, loss
rates of most un-congested links in the Internet have virtu-
ally zero ﬁrst- and second-order moments.
After having described the performance model and as-
sumptions in detail in Section 3, we ﬁrst prove our main
result in Section 4: the link variances are statistically iden-
tiﬁable from end-to-end measurements of regular unicast
probes in general mesh topologies. This is in sharp contrast
with what can be achieved with the mean loss rates of net-
work links, which are not statistically identiﬁable from end-
to-end measurements. This result shows that these variances
can be correctly learnt from a suﬃcient large set of end-to-
end measurements if we have a proper learning method. We
provide such an algorithm in Section 5.1. It combines the
end-to-end loss rates to provide a linear system of equa-
tions relating the link variances with the covariances of the
end-to-end loss rates. This system of equations can then be
solved in an accurate and scalable way using standard linear
algebra methods.
At the end of this learning phase, we can use the link
variances as additional information together with the most
recent measurement to calculate the link loss rates. The
method boils down to exploiting a monotonic dependence
between the mean and variance of a link loss rate. Specif-
ically, we ﬁrst sort the links according to their congestion
levels using their variances. By eliminating the least con-
gested links (with smallest variances) from the ﬁrst-order
moment equations we can then obtain a reduced system of
full column rank and can solve it in order to ﬁnd the loss
rates of network links as showed in Section 5.2. Because
most links on the Internet are un-congested, this operation
is very accurate.
Finally, the simulations in Section 6 and Internet experi-
ments in Section 7 show that our algorithm is both accurate
and scalable.
2. RELATED WORK
The inference of internal link properties from end-to-end
measurements is called Network Performance Tomography [14].
In this paper we are interested in a particular network per-
formance tomography branch that infers link loss rates from
end-to-end measurements. This problem, unfortunately, is
under-determined when we use only the average end-to-end
loss rates (i.e., the ﬁrst moment) as shown in Figure 1.
Figure 1: A network with three end-to-end paths
from the beacon B1 to each destination D1, D2 and
D3. The number next to each link denotes the trans-
mission rate of the link. Both sets of link trans-
mission rates give the same end-to-end transmis-
sion rates. Link transmission rates therefore cannot
be uniquely calculated from the average end-to-end
transmission rates.
To overcome the un-identiﬁability problem, we need to ei-
ther bring in additional constraints or to look for more prag-
matic goals. Most existing end-to-end tomography methods
fall into one of two classes: methods that use strong tempo-
ral correlation between probing packets in a multicast-like
environment [1, 6, 7, 9, 12, 13] (see [11] for a review of these
methods), and methods that exploit the distribution of con-
gested links in the Internet [14, 22, 24] for a more pragmatic
goal of identifying the congested links.
The initial methods in the ﬁrst class [1, 6, 7] infer the loss
rate of network links using multicast probing packets. Most
recently, [9] shows that all moments of link delays, except
the means, are statistically identiﬁable in a multicast tree.
However, statistical identiﬁability in general topologies re-
mains to be a conjecture in [9]. As multicast is not widely
deployed in the Internet, subsequent methods [12,13,29] em-
ulate this approach using clusters of unicast packets. These
methods are less accurate than their multicast counterparts
and also require substantial development and administra-
tive costs. Furthermore, the iterative algorithms used to
compute link loss rates in these approaches are expensive
for real-time applications in large networks.
Methods in the second class [14,22,24,36] use only unicast
end-to-end ﬂows for a simpler goal of identifying the con-
gested links. As diﬀerent loss rate assignments are possible,
these methods use additional information or assumptions.
For example, the methods in [14, 24] identify the congested
links by ﬁnding the smallest set of links whose congestion
can explain the observed measurements. These methods es-
sentially use two assumptions: (i) network links are equally
likely to be congested, and (ii) the number of congested links
is small. In [22], we propose to use multiple network mea-
surements in order to learn the probabilities of network links
being congested. Using these probabilities, instead of the
assumptions in [14, 24], the CLINK algorithm of [22] can
identify the congested links with higher accuracy. Using
again unicast probes, [36] by passes the un-identiﬁability of
ﬁrst order moment equations by ﬁnding the smallest set of
consecutive links whose loss rates can be determined. This
method, however, cannot be used to calculate link loss rates
at the granularity of each individual link, even if some of
these links belong to diﬀerent end-to-end probing paths. In
contrast, we will see in Theorem 1 that the loss variances of
these links are identiﬁable.
A large body of research has also been devoted to detect-
ing shared congestion of network ﬂows. All of these studies
use the correlations between diﬀerent ﬂows to identify the
shared bottlenecks.
In [26], cross-correlation between two
ﬂows is compared against the autocorrelation of each ﬂow.
If the former is greater than the latter, then a shared con-
gestion is concluded, otherwise there is none. Harfoush [17]
et al. use back-to-back packet pairs to calculate the correla-
tions from which they infer shared congested links. Kim et
al. [19] use a wavelet denoising technique to improve upon
the work of [26]. Most recently, Ariﬂer [4] used the spatial
correlations of TCP ﬂow throughput to infer shared conges-
tion with a factor analysis model.
We summarize the existing end-to-end tomographic tech-
niques in Table 1. We classify them according to their in-
ference methodologies and objectives.
Our solution in this paper is also based on end-to-end to-
mography. We use unicast probes in a totally diﬀerent way
than [12, 13, 29]: we exploit the spatial correlations between
ﬂows rather than the temporal correlations between packets
as in [12, 13, 29]. The ﬂow-level approach oﬀers two advan-
tages over its packet counterpart. First, the spatial correla-
tions exist even among the ﬂows of regular and weakly cor-
related unicast packets. A ﬂow level approach can therefore
be used with existing application traﬃc from a single end-
host as it does not require access to both ends of a ﬂow as
shown in [24]. Second, our inference technique does not rely
on the strong assumption that back-to-back unicast pack-
ets are strongly correlated. Hence it is more tolerant to
cross traﬃc, contrary to the the approach of [29] that also
works with passive unicast probes. Compared to the ap-
proach in [22], our solution in this paper also uses multiple
measurements of unicast ﬂows but can provide more infor-
mation about network links, i.e., link loss rates rather than
just their congestion statuses.
Table 1: A Summary of Techniques in Network Loss Tomography.
Temporal Correlations
First Order Moments
Higher Order Moments
Multicast Packet Trains
Prior Knowledge Link Groups One Snapshot Multiple Snapshots
Shared Congestion
Congested Links
Link Loss Rates
[6, 9]
Yes
Yes
Yes
[12, 13]
Yes
Yes
Yes
[14, 24]
Yes
Yes
No
[36]
Yes
Yes
Yes
[4, 26]
Yes
No
No
[22]
Yes
Yes
No
There are also non-tomographic techniques for calculat-
ing link loss rates such as [3, 20]. Instead of using end-to-
end measurements, these approaches use router supports
to calculate link loss rates and therefore depend heavily
on the cooperation of routers. For example, Tulip [20] re-
quires routers to support continuous IP-ID generated ICMP
packets. Unfortunately for security and performance issues,
many routers do not respond or limit the response rates to
ICMP requests. Tulip is also dependent on the cross traﬃc
and consistently underestimates link loss rates [20]. Further-
more, these approaches require that each end-to-end path is
measured separately and therefore do not scale well.
Techniques that use higher order moments for traﬃc ma-
trix estimations are proposed in [8, 30].
In both of these
works, the unknown variables are assumed to follow a cer-
tain parametric distribution. Our approach diﬀers in two
aspects. First, we are interested in the “dual” problem of in-
ferring link properties given the end-to-end measurements,
whereas in [8, 30] link measurements are known, but end-
to-end traﬃc counts need to be inferred. The structures of
the linear equations relating measurements with unknowns
in the two problems are therefore diﬀerent. Second, we do
not assume any parametric distribution for the inferred vari-
ables. A more detailed comparison between the two ap-
proaches will be given in Section 5.
3. THE NETWORK MODEL
We consider an overlay inference system that consists of
regular users who can send and receive UDP packets and
perform traceroute [18]. End-hosts periodically measure
the network topology and end-to-end loss rates by sending
probes to a set of destinations D. They also periodically
report the measurement results to a central server. We call
these end-hosts that send probes beacons, their set is denoted
by VB.
In this section, we describe the input data that can be
gathered from direct measurements: network topology and
end-to-end loss rates. We also describe the performance
model used by the central server to infer link loss rates.
Even though the framework we present in this paper can be
used to infer link delays and with passive measurements, we
only consider loss rates with active probes in this paper.
3.1 Network Topology
The network topology is measured using tools such as
traceroute [18]. There are several practical issues concerning
the use of traceroute to obtain network topologies, which we
will elaborate in the experiments of Section 7.
We model the network as a directed graph G(V, E ), where
the set V of nodes denotes the network routers/hosts and
the set E of edges represents the communication links con-
necting them. The numbers of nodes and edges are denoted
by nv = |V|, and ne = |E|, respectively. Furthermore, we
use Ps,d to denote the path traversed by an IP packet from
a source node s to a destination node d. Let P be the set of
all paths between the beacons and the destinations. Any se-
quence of consecutive links without a branching point cannot
be distinguished from each other using end-to-end measure-
ments: These links, which we call “alias” links, are therefore
grouped into a single virtual link. We call this operation the
alias reduction of the topology.
In the rest of this paper,
we use the term “link” to refer to both physical and virtual
links.
For a known topology G = (V, E ) and a set of paths P, we
compute the reduced routing matrix R of dimension np ×
ne, where np = |P|, as follows. The entry Ri,j = 1 if the
path Ps,d ≡ Pi, with i = (s, d), contains the link ej and
Ri,j = 0 otherwise. A row of R therefore corresponds to
a path, whereas a column corresponds to a link. Clearly,
if a column contains only zero entries, the quality of the
corresponding link cannot be inferred from the paths in P.
Hence we drop these columns from the routing matrix. After
the alias reduction step and this removal step, the columns
of the resulting reduced routing matrix R are therefore all
distinct and nonzero. The dimensions of R are np × nc,
where nc ≤ ne is the number of links that are covered by at
least one path in P. We denote this set of covered links by
Ec, |Ec| = nc.
Figure 2: Two end-hosts B1 and B2 are beacons,
D1, D2 and D3 are the probing destinations.
In the example of Figure 2, each of the two beacons B1 and
B2 sends probes to three destinations D1, D2 and D3. The
aggregated routing topology contains 6 end-to-end paths
and 8 directed links with the reduced routing matrix R as
shown in the ﬁgure.
We make the following assumptions on the reduced rout-
ing matrix R:
• T.1 Time-invariant routing: R remains unchanged
throughout the measurement period.
• T.2 No route ﬂuttering: There is no pair of paths
Pi and Pi′ that share two links ej and ej ′ without also
sharing all the links located in between ej and ej ′ .
That is, the two paths never meet at one link, diverge,
and meet again further away at another link.
These two assumptions are needed to guarantee the identi-
ﬁability of the variances of link loss rates from end-to-end
measurements as we will show later.
The ﬁrst assumption (T.1) can be violated in the Internet
where routing changes can happen at any timescale. As a
consequence, we will have some noise in our estimation of
the routing matrix. To overcome routing changes, we could
remeasure the network topologies frequently. However, we
avoid this approach as it could require a signiﬁcant amount
of repeated traceroute measurements, which is prohibitive
in many networks. We show in our experiments in Section 7
that despite the potential errors in network topology, our
algorithm is still very accurate.
Assumption T.2 can be violated if multiple paths are used
to route packets from one host to another. These paths do
not occur frequently and this phenomenon is called route
ﬂuttering [32]. We call these paths ﬂuttering paths. Flutter-
ing paths can occur for several reasons. The most common
cause is load balancing at routers. However, most routers
perform load balancing deterministically using ﬂow identi-
ﬁcations (source IP, source port, destination IP, and des-
tination ports) [5]. Therefore, an end-to-end ﬂow between
two end-hosts, with ﬁxed source and destination ports, does
not observe these route ﬂutterings. Another cause of route
ﬂutterings is routing failure. Signiﬁcant route ﬂutterings
can occur during the convergent period when a new route is
searched. These ﬂutterings are usually transient and come
with high packet loss rates. To handle these ﬂutterings,
measurements with high frequency are needed and we do not
consider them in this paper. In the rare cases that long-term
ﬂutterings indeed appear, we keep only the measurements
on one path and ignore the measurements on the others. To
infer the performances of the links on the diﬀerent ﬂutter-
ing paths, we repeat the inference multiple times, each time
including one of the alternative paths in the routing matrix.
Under assumption T.2, the paths from a beacon B form
a tree rooted at B. For example, the paths from beacon B1