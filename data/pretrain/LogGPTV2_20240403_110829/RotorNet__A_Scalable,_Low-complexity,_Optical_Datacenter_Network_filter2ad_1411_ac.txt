0 [0]
0 [0]
128 [262 k]
128 [262 k]
Agg.
BW
100%
33%
50-100%
50-100%
Table 3: Components and relative bandwidths of 65,536-end-
host networks built with Fat Tree and hybrid RotorNet archi-
tectures, assuming k = 64 port packet switches (and ToRs).
RotorNet is cost-comparable with a 3:1 Fat Tree, but as we
show in Section 7, delivers higher throughput.
a logically separated 100-Gb/s channel. We emphasize that because
the optical signal loss of a Rotor switch is low (2 dB), RotorNet can
use the same or similar (cost) optical transceivers used in a Fat Tree
network. The fibers from each ToR are routed to a central location
and broken out to connect to the Nsw = 128 Rotor switches. Each
Rotor switch provides 2,048 ports, one for each rack in the network,
and implements Nm = 16 matchings.
An explicit cost comparison between RotorNet (or any optical
network proposal) and a Fat Tree depends on volume pricing of
packet switches and transceivers, installation expenses, and the
manufacturing cost of OCSes. These cost numbers are not publicly
available, so we use the number of components as a proxy for cost
when comparing networks. Table 3 shows the component counts—
including electrical packet switches (EPS), transceivers (TRX), and
Rotor switches—for this 65,536-end-host network built with fully
provisioned (1:1) and over-subscribed (3:1) Fat Tree topologies,
as well as hybrid RotorNet topologies with 10% and 20% packet
switching bandwidth. RotorNet requires fewer packet switches and
transceivers than both the Fat Trees, but does require optical switch-
ing hardware that the electronic networks do not. Because Rotor
switches can be mass-produced as described below, the per-port
cost of a Rotor switch can be less than that of a packet switch. Con-
sidering that optical transceivers are the dominant cost in today’s
datacenter network fabrics [29], we estimate that for between 10–
20% packet switching, a hybrid RotorNet will be cost-competitive
with a 3:1 over-subscribed Fat Tree.
4.3 Manufacturing and deployment
While the matching patterns in Figure 4 provide complete connec-
tivity, for pragmatic reasons we choose a different set of (NR − 1)
matchings which consist of only bidirectional connections (i.e. their
adjacency matrix is symmetric).
To reduce manufacturing cost, instead of configuring each Rotor
switch with unique matching patterns, we can instead build all
Rotor switches with the same set of internal bidirectional matchings
and simply permute the input wiring pattern to each switch in
order to realize disjoint matchings among Rotor switches. This
approach dramatically reduces cost because it requires only one
unique optical element to perform the matchings internal to each
switch, rather than having to build Nsw unique switches. Space
does not permit a full discussion, but the main limitation of such
an approach is that the number of Rotor switches must be a power
of two. We leave to future work the investigation of other methods
for arranging the internal connectivity of Rotor switches.
RotorNet also offers a path for incremental deployment to reduce
the upfront cost. Consider an eventual deployment that will support
NR racks with Nsw Rotor switches. By choosing the appropriate
input port wirings to Rotor switches (or equivalently the matching
patterns contained within each switch), NR/X racks can be de-
ployed with each ToR switch having 1/X of its upward-facing ports
populated, connecting to Nsw/X Rotor switches. This configura-
tion provides a network with 1/X
2 the bisection bandwidth. Similar
to the matching patterns described above, the only constraint on
this approach is that X be a power of two.
5 DISTRIBUTED INDIRECTION
Top-of-rack switches in RotorNet implement RotorLB (RotorNet
Load Balancing), a lossless, fully distributed protocol based on the
principle of Valiant load balancing [30]. When indirecting traffic,
RotorLB injects traffic into the network fabric exactly two times:
traffic is first sent to an intermediate rack, where it is temporarily
stored, and then forwarded to its final destination. RotorLB stitches
together two-hop paths over time; i.e., the source is connected to
an intermediate rack during one matching, but the intermediate
rack is connected to the destination in a subsequent matching.
Unlike traditional VLB, which always sends traffic over random
two-hop paths, RotorLB (1) prioritizes sending traffic to the desti-
nation directly (over one-hop paths) when possible, and (2) only
injects new indirect traffic when that traffic will not subsequently
interfere with the intermediate rack’s ability to send traffic directly.
These two policies improve network throughput by up to 2× (for
uniform traffic) compared to traditional VLB.
5.1 ToR and end-host responsibilities
In RotorLB, each ToR switch is responsible for keeping an up-to-
date picture of the demand of each end host within the rack and for
exchanging in-band control information with other ToRs. There are
two types of traffic the ToR must track: local traffic generated by
hosts within the rack, and non-local traffic that is being indirected
through the rack. Each ToR is responsible for pulling traffic from
end hosts at the appropriate times and managing the storage of
non-local traffic. Indirect traffic can be stored in off-chip memory
at the ToR or in DRAM at the end hosts. We analyze the amount of
memory required for buffering indirect traffic later in Section 7.5.
RDMA can achieve microsecond latencies when pulling/pushing
data directly from/to end-host memory.
Each end host, in turn, must send to the ToR a message when
it has traffic to send, including the quantity and destination of the
traffic. This information can be generated by inspecting transmit
queues or relaying application send calls. Following prior work
on inter-rack datacenter networks [13, 15, 17], we abstract away
intra-host and intra-rack bottlenecks (which also exist in packet-
switched networks) and focus on RotorNet’s ability to handle inter-
rack traffic.
5.2 RotorLB algorithm and example
The RotorLB algorithm runs on each ToR switch. At the start of each
matching slot (the time period for which a Rotor switch implements
RotorNet: A Scalable, Low-complexity, Optical Datacenter Network
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Algorithm 1 RotorLB Algorithm
function Phase 1(Enqueued data, slot length)
alloc ← maximum possible direct data
capacity ← slot length minus alloc
offer ← remaining local data
send offer, capacity to connected nodes
send allocated direct data
remain ← size of unallocated direct data
return remain
function Phase 2(remain, LB length)
recv offer and capacity from connected nodes
indir ← no allocated data
avail ← LB length minus remain
offeri ← offeri if availi (cid:44) 0
offerscl ← fairshare of capacity over offer
while offerscl has nonzero columns do
for all nonzero columns i in offerscl do
tmpfs ← fairshare of availi over offerscli
availi ← availi − sum(tmpfs)
indir ← tmpfs
offerscl ← offer − indir
tmplc ← capacity − sum(indir)
offerscl ← fairshare of tmplc over offerscl
◃ offer
◃ accept
send indir to connected nodes
function Phase 3(Enqueued local data)
recv indir from connected nodes
locali ← enqueued local data for host i
indiri ← min(indiri , locali)
send indiri indirect local data for host i
each matching), RotorLB determines the quantities of direct and
indirect traffic to send during that slot. Before sending new indirect
traffic into the network, RotorLB prioritizes the delivery of stored
non-local traffic (which is on its second hop) as well as local traffic
that can be sent directly to the destination in the current slot. With
any remaining link capacity in that slot, RotorLB indirects traffic on
a per-destination granularity. To limit buffering and bound delivery
time of indirected traffic, RotorLB only indirects as much traffic as
can be delivered within the next matching cycle (the full cycle of
matching slots). We use a pairwise “offer/accept” protocol between
ToRs to exchange current traffic conditions, and then adaptively
determine the amount of indirect traffic to be sent based on those
conditions. The fraction of link bandwidth used to support indirect
traffic varies between 0% (if the rack’s locally-generated traffic can
saturate the link) and 100% (if the rack has no locally-generated
traffic to send during a matching slot, and would otherwise be idle).
To most effectively balance load, we allow traffic from the same
flow to be sent over RotorNet’s single one-hop path and also to be
indirected over multiple two-hop paths. This multipathing can lead
to out-of-order delivery at the receiver. Ordered delivery can be
ensured using a reorder buffer at the receiver, and we evaluate this
approach in more detail in Section 7.3.
Below, we describe the basic operation of the RotorLB algorithm,
moving through a simple example from the perspective of a single
Figure 5: RotorLB example. Matrix rows represent sources
and columns represent destinations; L and N represent local
and non-local traffic queues, respectively; matrix elements
show normalized traffic demand. In the current matching
between racks 1 and 2, traffic which can be sent directly
is bounded by black rectangles, stored indirect traffic is
marked by a red triangle, one-hop direct traffic is marked
by a green circle, and new indirect traffic is indicated by a
blue oval.
Rotor switch connection over the course of one matching slot. The
RotorLB algorithm is outlined in Algorithm 1.
Consider the ToRs of two racks, R1 and R2, which have current
demand information for the hosts within each rack stored in non-
local (N ) and local (L) queues, as shown in Figure 5. In this example,
demands are normalized so that one unit of demand can be sent
over the ToR uplink in one matching slot. Note that, as described
below, there is no central collection of demand–each host simply
shares its demand with its ToR switch, and ToR switches share
aggregated demand information in a pairwise fashion.
Phase 1: Send stored non-local and local traffic directly. Connectiv-
ity in RotorNet is predictable, and each ToR switch anticipates the
start of the upcoming matching slot as well as to which rack it will
be connected. After taking a snapshot of the N and L queues, the
ToR computes the amount of traffic destined for the upcoming rack.
Delivery of stored non-local traffic on its second (and final) hop is
prioritized to ensure data is not queued at the intermediate rack for
long periods of time. Delivery of local traffic has the next priority
level. In Figure 5, R1 has 0.25 units of stored non-local traffic (red
triangle) and 0.75 units of local traffic (green circle) destined for
R2, so it allocates the entire ToR uplink capacity for the matching
slot duration to send this traffic. R2 has no stored non-local or local
traffic for R1, so no allocation is made.
The ToR then forms a RotorLB protocol offer packet which con-
tains the amount of local traffic and the ToR uplink capacity which
will remain after the allocated data is sent directly. The smaller
of the two quantities constitutes the amount of indirect traffic the
ToR can offer to other racks. Once the matching slot starts, the ToR
sends the offer packet to the connected rack. As an optimization,
rather than waiting for the entire offer/accept process to complete,
(cid:152) .75.25(cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) .25 .5 (cid:152) (cid:152) 0 .25 (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) .25 .75 (cid:152) (cid:152) (cid:152) 0 .5 (cid:152) L: N: Rack 1 L: N: (cid:152) 11(cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) .75 .25 (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) (cid:152) Time Rack 2 L: N: L: N: R1 R2 Send traffic Recon-figure R1 R3 Start of slot End of slot … Offer / Accept SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
W. M. Mellette et al.
the ToR can also begin sending the stored non-local and local traffic
which was been allocated for direct delivery to the destination.
Phase 2: Allocate buffer space for new non-local traffic. Shortly
after the start of the slot, the ToR switch receives the protocol packet
containing the remote rack’s offer of indirect traffic. At this point, it
computes how much non-local traffic it can accept from the remote
rack. To do this, the ToR examines how much local and non-local
traffic remain from Phase 1. The amount of non-local traffic it can
accept per destination is equal to the difference between amount
of traffic that can be sent during one matching slot and the total
queued local and non-local traffic. Because the amount of accepted
indirect traffic is limited to the amount that can be delivered in
the next matching slot (accounting for any previously-enqueued
traffic), the maximum delivery time of indirect traffic is bounded
to Nm + 1 matching slots, or approximately one matching cycle
(see Section 7.3). The algorithm handles multiple simultaneous
connections by fair-sharing capacity across them.
In Figure 5, R1 sees via the offer packet that R2 would like to
forward 1 unit of traffic destined for each R3 and R4 (blue oval),
and that R2 has a full-capacity link to forward that data. R1 already
has 0.25 units of local traffic for R3 and 0.5 units of stored non-local
traffic for R3. Therefore, it allocates space to receive 1− 0.75 = 0.25
units destined for R3 and 0.75 units for R4 from R2, which fully
utilizes the remaining link capacity from R2 and ensures that all
queued traffic at R1 will be admissible.
Once the allocation is made to receive non-local traffic, the ToR
switch responds with a protocol accept packet informing the remote
rack how much traffic it can forward on a per-destination basis.
Phase 3: Forward local traffic indirectly. Finally, the ToR switch
receives the protocol accept packet from the remote rack. After it
finishes sending direct traffic determined in Phase 1, it forwards
new non-local traffic to the remote rack per the allocation specified
by the accept packet.
In Figure 5, R2 receives an accept packet informing it that 0.25
units of traffic destined for R3 and 0.75 units destined for R4 may
be sent. It forwards this traffic, which is stored as non-local traffic
at R1. Finally, the Rotor switch reconfigures and establishes a new
connection, and the RotorLB algorithm runs again.
5.3 Fault tolerance
A simple extension to RotorLB, called RotorLB-FT, ensures that the
network is tolerant to failures. The key idea is to rely on indirect
paths to “route around” any such failures. Failures of a link, ToR,
or Rotor switch are discovered at the beginning of a particular
matching slot, because a rack will not receive RotorLB protocol
messages over the link corresponding to the failure.
The primary modification to the algorithm is to give traffic which
would traverse a failed network element priority over all other
traffic, so that it is ensured indirect bandwidth regardless of the
traffic pattern. Each ToR switch first determines the number of
candidate fault flows. This is the number of locally generated flows
which both see a fault on a one-hop path out of the local rack and
do not see a fault on a one-hop path out of the currently connected
rack (i.e. a clear two-hop path exists). The ToR switch then allocates
at most 1/(2(NR −1)−1) of the uplink bandwidth to each candidate
Figure 6: We installed 7 Rotor matchings into a prototype
OCS to create two parallel Rotor switches. An FPGA sets the
switches to cycle through the matchings in open loop.
fault flow. This factor is chosen conservatively so that if there is