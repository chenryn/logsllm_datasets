c
a
r
F
 0
 20
 40
 60
[weeks]
 80
 100
 5
 0
 120
Fig. 6. The failure rate with respect to VM age.
F. Age Matters
The bathtub curve of age versus failures is well known for
hardware component [16]. We try to ﬁnd if such a curve also
holds for virtual machines, i.e., old and young VMs fail more
often than middle-age VMs. Due to the limitation of the data
available to us, we are only able to trace VMs age dated
back to January 2011. This accounts for roughly 75% of VMs
under observation. In Fig. 6, we present the CDF and PDF
of the number of failures with respect to different ages, that
is deﬁned as the time difference between the failure moment
and the VM creation date. One can see that the CDF curve is
very close to the diagonal line, indicating a close relationship
with the uniform distribution. As for PDF, it shows a weak
increasing trend with quite some ﬂuctuation. The ﬂuctuation
of failure counts can be due to the fact that VMs are created
in a batch manner and the underlying population of each age
group is not evenly distributed. Consequently, we conclude
that the relationship between VM failures and their age does
not follow bathtub like function. Moreover, VM failures show
a weak positive trend with age. This may lead us to suggest
that periodically taking snapshots of existing VM images and
creating new VM instances can reduce VM failures.
V. IMPACT OF RESOURCES ON SERVER FAILURE
In this section, we study the impact of the server resources
on server’s failure rate in terms of resource capacity and
usage. The objective is to ﬁnd out whether bigger and heavily
loaded servers tend to fail more often. To this end, we
start by considering the capacity of multiple resources, i.e.,
the number of CPUs, the memory and disk size, and the
number of attached disks, for both PMs and VMs. Second,
we analyze the impact of the CPU, memory, disk and network
utilization collected from weekly averages. Our objective is to
quantitatively differentiate the impact of the resource capacity
8888
and utilization on VM failure rates from that on PM failure
rates. We note that the following analysis is based on the
weekly failure rate of servers with certain resource attributes,
i.e.,
the number of failure events divided by the number
of servers, relative to the same range of resource attributes.
Following the convention in Fig. 2, we present the average
values as well as the 25th and 75th percentiles with respect
to different resource capacity ranges and utilization attributes.
A. Resource Capacity
1) CPU Counts: To understand the impact of the number
of CPU units on both PM and VM failures, we compute the
weekly failure rate relative to the number of CPUs ranging
from 1 to 64 in Fig. 7(a). For PMs, we note that across different
numbers of processors, the average failure rate increases from
around 0.002 to 0.011 as the CPU count increases to 24 cores,
which translates into a factor of 5.5X. However, it decreases
to below 0.005 for 32 and 64 CPUs, probably because of the
higher reliability of such high-performance systems. Another
observation worth noting is that the range between the 25th
and 75th percentiles increases with the number of CPUs,
owing to the uneven distribution of the number of servers,
i.e., 72% of servers have at most 4 processors. For VMs,
the average failure rate increases from 0.002 to 0.005 as the
number of vCPUs goes from 1 to 8, showing an increment
factor of 2.5X. In particular, most crash incidents occur on
VMs with at most 2 logical CPUs, which is the most popular
conﬁguration. Overall, the number of CPUs has a positive
impact on the failure rate for both PMs and VMs.
Comparing the failure rates of PMs and VMs, one can see
that the number of CPUs has a more signiﬁcant impact on PMs
than on VMs, evidenced by a higher average failure rate. This
could be explained as follows: in the case of PMs, a percentage
of the failures can be caused by the actual processor failing,
whereas VMs do not have access to the hardware and the
impact of CPU counts translates only into shared CPU time.
2) Memory Size: Looking at the memory capacity shown in
Fig. 7(b), we see similar trends for both PMs and VMs, i.e.,
failure rates have a high - low - high trend with increasing
memory size – a kind of bathtub shape. For PMs, the average
failure rates are roughly 0.006 for a memory capacity up
to 4 GB, stabilizing at around 0.002 for memory capacities
ranging between 4 and 32 GB, and ﬁnally increasing up to
0.01 as the memory size reaches 128 GB. The impact of
the memory size on the PM failure rates is up to a factor
of 5X. In the case of VMs, the average failure rates are
relatively ﬂat at 0.002 when the memory capacity is between
256 MB and 4 GB, suddenly drop below 0.001 for memory
sizes between 4 GB and 8 GB, and increase to around 0.003 for
VMs with up to 32 GB capacity. Comparing the lowest and
highest average failure rates across different memory sizes,
one can see an impact of 3X from the virtual memory size.
We note that most crash incidents happen on VMs equipped
with 1-2 GB memory, which constitute the majority of VM
conﬁgurations. We provide the following rationales to explain
the higher average failure rates corresponding to (1): for low
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
 1.8
 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
PM
VM
PM
VM
 1
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
 0.8
 0.6
 0.4
 0.2
1
2
4
8
12
16
24
32
64
(a) CPU counts
VM
8-16
16-32
32-64
128-256
512-1K
2K-4K
64-128
256-512
1K-2K
(c) Disk capacity (GB)
 0
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
0.5-1
1-2
2-4
4-8
8-16
32-64
128-256
16-32
64-128
(b) Memory size (GB)
VM
1
2
3
4
5
6
7
(d) Number of disks
Fig. 7. Weekly failure rate across PMs and VMs relative to the CPU counts, memory size, disk capacity and number of disks.
memory capacities (256 MB to 4 GB) , failures are mostly
due to software crashes because applications and services
run out of memory and (2): for high memory capacities
(256 GB for PMs and 32 GB for VMs), failures are mostly
caused by hardware faults because a higher number of memory
modules increases the probability of faulty RAM. Overall, the
decreasing and increasing trends of the average failure rates
are more pronounced for VMs than for PMs.
3) Disk Capacity: Next, we consider the impact of the disk
capacity and the number of disks on the weekly failure rate,
illustrated in Fig. 7 (c) and (d). In contrast to the CPU and
memory capacity, the data available to us does not contain
any disk information for PMs, and thus we can only present
results relative to VMs.
On the one hand, one can see that the average failure rate
increases steadily with the disk capacity from 0.00029 for
8 GB disks to 0.0025 for all bigger sizes, i.e., greater than
32 GB. Actually, this increasing trend is seen for 15% of VMs,
whereas the remaining VMs have disk capacity greater than
32 GB. Such an increasing trend of failure rates can possibly
be attributed to the use of constant-size physical disk platters to
store different virtual data volumes. Currently, large-capacity
disks use about the same size platters as the small-capacity
ones do, making the data density higher on the bigger disks.
On the other hand, the average failure rates remain slightly
above 0.0025 for VMs with disk sizes between 32 GB to 4 TB,
i.e., roughly 85% of VMs. Combining with the observation of
the distribution of disk conﬁgurations, we conclude that failure
rates of VMs are quite steady around 0.0025, with respect to
different disk capacities.
However, in contrast to the disk capacity, a different trend
is observed for the number of disks, i.e., the failure rates
increase visibly with the number of disks. In particular, the
average failure rate increases from 0.0005 for 1 virtual disk to
0.005 for six virtual disks. This is an almost 10X increment
in failure rates. In contrast to invariant failure rates across disk
capacity range, this different observation is due to the fact that
most VMs have 2 disks covering a wide range of capacities.
Actually, 83% of failure events happen on the VMs with at
most 2 disks. Moreover, the higher failure rates corresponding
to more disks can be explained by the fact that adding more
disks to a server increases the chance of an independent disk
failure. Similar ﬁndings are also reported in [3]. Comparing
the two disk features, we note that the number of disks has a
stronger impact on the VM failure rate than the disk capacity.
Finally, we also want to compare the impact of different
resource capacities on the average failure rates of VMs and
PMs. To this end, we look at the relative range of average
failure rates between low provisioning and high provisioning
of a certain resource. In particular, we consider the increment
factor of failure rates described earlier. We observe that the
average failure rates of VMs increase by 2.5X, 3X, and 10X
9999
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
with respect to CPU count, memory size, and disk count, The
average failure rates of PMs increase by 5.5X and 5X with
respect to CPU count and memory size. This indicates that the
number of disk has the highest impact on VM failures, whereas
CPU counts and memory size are equivalently inﬂuential for
PM failures.
B. Resource Usage
Next, we try to answer the question whether increasing
workloads on speciﬁc resources, i.e., CPU, memory, disk, and
network, also increases the failure rates of both PMs and VMs.
1) CPU and Memory Usage: We look into the weekly
failure rates with respect
to CPU utilization and memory
utilization, summarized in Fig. 8(a) and (b). Prior to analyzing
the failure trend, we note that
the numbers of VMs and
PMs decrease with CPU utilization, i.e., more than half of
the VMs and PMs are used at most 10%. When looking at
CPU utilizations ranging from 0 to 30%, accounting for the
majority of both VMs and PMs, one can see that the average
failure rates of VMs increase with CPU utilization, whereas
the average failure rate of PMs decrease with CPU utilization
by roughly an order of magnitude. A possible explanation for
the increasing failure rate of VMs compared to PMs is the
combination of the failure rate of the underlying hypervisor
(e.g., for overprovisioning reasons) and that of the actual VM
(e.g., due to the workload). When considering the entire range
of CPU utilization of PMs, the trend of the average failure rates
follows bathtub curve, meaning moderately loaded PMs are
more reliable. Particularly, in terms of the maximum relative
range of failure rates, the impact factor of CPU utilization for
PMs and VMs are roughly 13X ( 0.0015 v.s. 0.019 ) and 12X
(0.002 v.s. 0.025 ) respectively.
In the case of memory, the average failure rates for both
VMs and PMs follow an inverted bathtub curve, i.e., they ﬁrst
increase with memory utilization and then decrease. We note
that the majority of VMs is used at most 10%, whereas surpris-
ingly the number of PMs increases with memory utilization.
To achieve average failure rates below 0.005, the memory
utilization of PMs should be less than 20% or above 70%.
As for VM, to achieve achieve failure rates below 0.0025, the
memory utilization of VMs should be below 10% or above
50%. When comparing the relative differences between the
max and min failure rate across all memory utilization, one
can see that memory utilization has a stronger impact on PMs
than on VMs, i.e., 7X (0.001 v.s. 0.0075) against 3X (0.001
v.s. 0.003). Moreover, combining with our previous ﬁnding
that PMs with memory size between this 4 GB and 128 GB
experience lower failure rates, we conclude that a reliable PM
should be equipped with a moderate-size memory and keep
its utilization sufﬁciently high.
2) Disk and Network Usage: Finally, we look at the impact
of disk space and network usage on failure rates. As our dataset
does not contain any PM-related disk and network information,
we can only present results for VMs as shown in Fig. 8(c) and
(d). The average failure rates increase with the disk utilization,
from 0.001 (below 10% usage) to 0.003 (above 70% usage).
The impact of the disk usage on the VM failure rate is roughly
3.5X (0.001 v.s. 0.0035), which is lower than the CPU usage.
Such an observation can be explained by the fact that disk-
space issues do not usually result in failures and can easily
be solved by erasing old ﬁles to provide more free space.
Such a ﬁnding resonates well with the fact that the allocated
disk space has a rather insigniﬁcant impact on the average
VM failure rates. In the case of network, we quantify the
usage as the network trafﬁc in Kbps sent and received by
the VM. Roughly 45% of all VMs have an average bandwidth
between 2 Kbps and 64 Kbps, and 34% transfer rates between
128 Kbps and 512 Kbps, whereas the remaining 21% use
between 1024 Kbps and 8192 Kbps. The average failure rates
increase from 0.001 to 0.005 for VMs with up to 64 Kbps
transfer rates. After that, the average failure rates decrease with
network volume. Overall, judging from the range of absolute
values of the average failure rates for the majority of VMs, we
obtain a inﬂuential factor of network usage as 3.5X (0.0015
v.s. 0.005). Overall, our results show a weaker correlation
between the Kbps transferred and the VM failure rates, than
for the CPU utilization
When comparing all resources by the relative differences be-
tween their max and min average failure rates, i.e., increment
factors, we conclude that CPU usage is the predominant usage
factor for PMs as well as for VMs. Moreover, as all average
failure rates are observed below 0.005, memory utilization,
disk usage, and network transfers all appear to have similar
degrees of inﬂuence on the failure rates of VMs, i.e., roughly
by a factor of 3X.
VI. IMPACT OF VM MANAGEMENT
In this section, we speciﬁcally discuss how two particular
aspects of VM management, consolidation and turning on/off,
affect the failure rates of VMs.
A. VM Consolidation