queue utilization.5 and see the same behavior. This gives us con-
fidence that low Swift loss rates are sustained even when all the
traffic on the link is from Swift.
We plot aggregate cluster throughput versus the loss rate in
Figure 9. We pick 25 clusters at Google and plot the loss rate for
Swift/GCN separately against the total Swift/GCN throughput in
the cluster. We report edge links only for brevity. We see that Swift
consistently delivers low loss even at extreme tails at scale, while
GCN is much more variable. In our experience, the low loss rates of
Swift are a direct outcome of its prompt reaction to congestion as
detected by its target delay, as well as scaling to large incasts. Given
the extremely small loss rate both at the edge and in the fabric, the
end-to-end retransmission rate for Swift is also very low, which is
consistent with our choice not to invest heavily in loss recovery.
Swift’s performance improvements hold for a range of link
speeds. We show results for commodity NIC link speeds from 40
to 100Gbps. Figure 10 compares the average and 99.9p loss rate for
edge (ToR to host) links at high utilization (>90%). At near line-rate
utilization, even the 99.9th-p loss rate for Swift is much smaller
than GCN.
Takeaway: Swift achieves low latency near the target
We now turn to latency. Figure 11 shows NIC-to-NIC round trip
time (or fabric RTT) across our datacenters as measured by NIC
5Note that there is a spike in the [80-90%] loss rate bucket for GCN. We verified this
spike accurately measures our production traffic, and we are investigating plausible
causes.
521
0102030405060708090100(cid:30)eueUtilization(%)0.000.250.500.751.00LossRate(normalized)AverageGCNSwi(cid:28)0102030405060708090100(cid:30)eueUtilization(%)0.000.250.500.751.00LossRate(normalized)99.9thpercentileGCNSwi(cid:28)0x2x4x6x8x10xClusterThroughput(normalized)0.000.250.500.751.00LossRate(normalized)AverageGCNSwi(cid:28)0x2x4x6x8x10xClusterThroughput(normalized)0.000.250.500.751.00LossRate(normalized)99.9thpercentileGCNSwi(cid:28)40G50G100GPortspeed(bps)0.000.250.500.751.00LossRate(normalized)0.380.101.002e-30.021e-7AverageGCNSwi(cid:28)40G50G100GPortspeed(bps)0.000.250.500.751.00LossRate(normalized)0.360.331.007e-40.012e-999.9thpercentileGCNSwi(cid:28)0x5x10x15x20x25xFabricRTT(normalized)0.000.250.500.751.00CDFBasetargetdelayGCNSwi(cid:28)0x2x4x6x8x10xClusterThroughput(normalized)0x2x4x6x8x10xRTT(normalized)BasetargetdelayAverageGCNSwi(cid:28)0x2x4x6x8x10xClusterThroughput(normalized)0x20x40x60x80x100x120xRTT(normalized)99.9thpercentileGCNSwi(cid:28)Swift: Delay is Simple and Effective for Congestion Control in the Datacenter
Figure 13: Average loss rate vs. port utilization for GCN traffic at strict sched-
uling priority and Swift at lower scheduling weight for ToR-to-host links. The
highest GCN loss rate is normalized to 1.0.
Figure 14: CDF of end-to-end packet RTT and NIC-Rx-queuing delay for the
throughput-intensive cluster (left) and IOPS-intensive cluster (right).
using weighted-fair queuing. In our experience, this arrangement
lets Swift achieve low latency even in cases when a good portion
of the traffic is controlled by loss or via GCN. We see in Figure 11
that Swift latency is substantially smaller than the GCN latencies
when both queues have the same scheduling priority. Similarly, the
cluster-scale measurements in Figure 12 show an order of magni-
tude lower latency for Swift than GCN.
To stress the isolation mechanism, we compare the packet loss
versus port utilization for unequal scheduling priority: GCN run-
ning with the advantage of strict priority scheduling and Swift
running at its lowest weight. Figure 13 shows that Swift controls
the queueing much better than GCN even though it has less pre-
ferred access to link bandwidth.
4.4 Fabric and Host Congestion
The response of Swift to host as well as fabric congestion has been
key to its success in production. In a shared environment, we often
have a mix of IOPS-intensive applications that stress hosts, and
throughput-intensive applications that stress the fabric. While most
cluster congestion is in the fabric, host congestion is not rare, and
if we did not respond to it separately then IOPS-intensive flows
could unfairly degrade co-located throughput-intensive flows.
We choose two clusters to show how the fabric and host both
contribute to end-to-end packet RTT. One cluster is dominated by
IOPS-intensive tasks that are end-host queuing dominated, while
the other carries predominantly large storage RPCs, which are
typically network queuing dominated. For each cluster, we split the
end-to-end packet RTT as measured in Swift (t6 - t1 in Figure 2) to
obtain fabric and host components. We show these components for
the two clusters in Figures 14. There is a clear distinction. The host
delays are small and tight for the throughput-intensive cluster, but
can contribute as much as the fabric delays in the IOPS-intensive
cluster.
Splitting the RTT into fabric and host components has also been
invaluable for debugging. When a network problem is reported
in production, the first step is typically to determine whether the
culprit is the fabric or the host. By looking at log data for fabric
522
Figure 15: Small Op latency in an
in-memory filesystem.
Figure 16: Op latency in an SSD
storage system.
versus endpoint congestion in aggregate also tells us whether a
cluster is congested mostly by the fabric or end host engines.
4.5 Application Performance
It is important that Swift support IOPS-intensive and throughput-
intensive applications, latency sensitive workloads with short Ops,
and allow them to run well together; coexistence facilitates service
deployment in a shared network environment.
In-memory BigQuery Shuffle. Swift supports a disaggregated,
in-memory filesystem for BigQuery shuffle [11] built atop Snap [36].
What ultimately matters to a memory-based file system is how
quickly the workload completes, as measured by the IOPS, and the
completion time of small Ops. Swift’s ability to simultaneously con-
trol network delays and host congestion at scale is thus invaluable
for this application. Figure 15 shows that the Op completion time
closely follows the Swift’s target delay.6 The separate treatment of
fabric and host congestion in Swift was key in enabling this applica-
tion to meet its access latency SLOs in all clusters. In addition, the
team informed us that the ability to keep network latencies small
provided meaningful backpressure to them. They recently rolled
out a change to handle application-level queuing better, which im-
proved the tail-latency by 7×; a change which would not have been
possible with GCN.
Storage. Parts of Storage traffic are also served over Swift, with
throughput as the primary metric of goodness. We provide results
from a load test provided to us by the SSD-storage team that does
16kB reads. Swift achieves 4× lower 99.9th-p application latency
vs. GCN as shown in Figure 16. Additionally, Swift achieves ∼7%
higher IOPS with a 100% success rate in Op-completions, while the
losses in GCN resulted in 1.7% of its Ops failing due to deadlines
being exceeded.
4.6 Production Experience
We briefly summarize production experiences that may be of inter-
est.
Swift’s ability to operate at near line-rate with near zero loss
has caused confusion at times, since other teams are not used to
this level of performance at scale. On two occasions, bugs were
incorrectly filed for monitoring failures because highly-utilized
links reported zero loss. Both cases were quickly attributed to Swift;
the use of QoS classes separation made this attribution easy.
Swift’s extremely low delays has also met with skepticism that
it may be unnecessarily sacrificing throughput, thereby reducing
application performance. It is relatively easy to measure how well
congestion-control is mitigating congestion, but it is much harder
to measure how well it is utilizing available bandwidth given that
6High IOPS is not shown here; Reference [36] shows that in some intervals a single
Snap instance is serving upwards of 5M IOPS.
0102030405060708090100ToR-to-HostLinkUtilization(%)10−410−2100LossRate(normalized)GCNstrictprioritySwi(cid:28)lowestpriority0x1x2x3x4x5x6xTime(normalized)0.000.250.500.751.00CDFRTTNICdelay0x1x2x3x4x5x6xTime(normalized)0.000.250.500.751.00CDFRTTNICdelay0481216202428Time(day)0x1x2x3x4x5xLatency(normalized)MintargetMaxtarget50th-p99th-p99.9th-pPercentiles0x2x4x6x8x10xLatency(normalized)1.0x3.4x9.8x1.0x1.7x2.4xGCNSwi(cid:28)Kumar et al.
Figure 18: T2: Achieved RTT and throughput vs. per-machine offered load.
Total load varies from 500Gbps to ∼50Tbps.
Metric
Throughput
Loss rate
Average RTT
Swift w/o cwnd < 1
Swift
8.7Gbps
28.7%
2027.4µs
49.5Gbps
0.0003%
110.2µs
Table 3: T1: Throughput, loss rate and average RTT for 5000-to-1 incast with
and without cwnd < 1 support.
Figure 19: T1: Fabric (f cwnd) and Endpoint (ecwnd) congestion windows for
a 100-to-1 incast with 1kB and 64kB writes.
throttles throughput. After a narrow transition region, through-
put remains saturated above a target of ~25µs. This clean behavior
makes it easy to find a good target in practice. In our initial deploy-
ments, we used a liberal target to first ensure throughput. We then
trimmed it to achieve lower latency, while ensuring no reduction
in throughput.
Note that needing a 25µs target in this experiment is not a lim-
itation of Swift but of the networking stack. Appendix D shows
an experiment (from a prototype that reduces host CPU interfer-
ence) where we lowered the target delay to 15µs and still sustained
line-rate throughput of 100Gbps.
5.2 Throughput/Latency Curves
Given a target delay, we sweep the offered load to characterize
the operating points of throughput vs. latency. To do this we use
T2 with a uniform random traffic pattern: each client selects one
machine from the remote racks at random for a 64kB write RMA
operation. We set the target delay to 25µs. We vary the offered
load by varying the interval over which we issue operations, and
measure the fabric RTT with NIC timestamps.
We see in Figure 18 that throughput increases with little rise in
RTT until we exceed 80% of the line-rate. Even then, the rise of
median RTT is modest and the tail RTT diverges slowly from the
median all the way through full load. At a load close to 100%, Swift
is able to maintain the 99.9th-p RTT to be <50µs at an aggregate
load close to 50Tbps. We note that the tail RTT is at most 3× higher
than the baseline unloaded RTT.
5.3 Large-scale Incast
Swift supports cwnd < 1 with pacing to handle large-scale incast,
which is an important datacenter workload. In T1, we start 100 flows
Figure 17: T1: Achieved RTT and throughput vs. target delay, 100-flow incast.
traffic may be application-limited or bottlenecked at end-hosts. To
address this concern we rolled out experimental and more aggres-
sive versions of Swift to subsets of the fleet. These versions priori-
tized throughput at the cost of increased queuing, including raising
the target delay, disabling pacing and cwnd < 1 mode, raising the
window floor and shortening timeouts. We observed increased RTT
and loss, but with no increase in throughput or improvement in
application performance. This result corroborates that Swift is not
throttling traffic to keep latency and loss low.
To see that the Swift implementation is efficient, we checked its
CPU usage in a diverse, heavy workload. It accounted for ∼2.6%
of Pony Express CPU. Of this, 1.4% is for ACK-processing (which
includes code not specific to Swift) and 0.31% is spent to support
timestamping and delay measurement.
We also found it simple to tune Swift’s target delay in production,
in contrast with challenges in tuning ECN thresholds. ECN thresh-
olds requires switch configuration updates, which is an ongoing
task due to evolving line-rates and topologies, and complicated by
heterogeneity, e.g., switches with a mix of line rates. In comparison,
target delay is controlled at the hosts and has a clear end-to-end
interpretation.
5 EXPERIMENTAL RESULTS
We present results from controlled experiments to evaluate the
mechanisms in Swift. Our experiments are from two testbeds with
benchmarking data (and no production traffic) running on them:
• T1 has 60 machines with 50Gbps NICs. We use it for incast
scenarios and for experiments relating to fairness, target delay
scaling and fabric vs. endpoint congestion.
• T2 is a larger testbed with ∼500 machines and 100Gbps NICs.
We use it for larger experiments with all-to-all traffic patterns.
5.1 Effect of Target Delay
Target delay is the key control parameter in Swift; we report on
how it affects performance. First, we look at how well the Swift
protocol can match measured RTT to the specified base target delay.
In T1, we set up 10 sender machines with 10 flows per sender, each
pushing 64kB write RMA operations to a single receiver machine as
quickly as possible. We vary the base target delay from 15µs to 70µs.
We disable flow and topology scaling (§3.5) for this experiment
to highlight the impact of base target delay. Figure 17 plots the
achieved RTT. We see it closely tracks the configured base target
delay.
Next, we look at how target delay affects throughput. The base
delay must at a minimum cover the propagation and NIC/switch
serialization delays, along with measurement inaccuracies. Beyond
this minimum, a higher target allows for more queuing. We want
the target delay to be low to reduce latency but high enough to maxi-
mize network throughput. Figure 17 also shows how the throughput
varies with the target delay. Initially the target is too low and Swift
523
01020304050607080TargetDelay(µs)0306090120150AchievedRTT(µs)Throughput99th-pRTTAverageRTTTargetdelay01020304050Throughput(Gbps)0255075100O(cid:27)eredLoad(Gbps)010203040AchievedRTT(µs)50th-pRTT99.9th-pRTTThroughput0255075100Throughput(Gbps)1kBop(endpoint-congested)64kBop(fabric-congested)0255075100125Congestwindow(pkts)84.121st-pfcwnd121.2450th-pfcwnd0.751st-pecwnd2.1950th-pecwnd0.901st-pfcwnd1.9150th-pfcwnd70.401st-pecwnd120.5550th-pecwndSwift: Delay is Simple and Effective for Congestion Control in the Datacenter
Figure 20: T1: Throughput of four
flows shows fairness achieved by
Swift.
Figure 21: T1: Throughput with and without flow-based scaling (FBS) for
a 5000-to-1 incast. Jain’s fairness index (J) shown is measured amongst
all 5000 flows using a snapshot of flow rates.
Figure 22: T1: Throughput of two flows
with different path lengths, with and
without topology-base scaling (TBS).
from each of 50 machines to a single destination machine, repre-
senting a 5000-to-1 incast. We measure the achieved throughput,
loss rate and average RTT for runs with and without the support
for cwnd < 1.
We see in Table 3 that Swift achieves line-rate throughput with
low latency and almost zero loss—excellent performance for a 5000-
to-1 incast. Conversely, without support for cwnd < 1 the protocol
degrades to high latency and loss that drives down throughput. In
comparison, GCN running in Pony Express achieves a 5.1% loss-rate
and 8.67× average RTT even for a smaller scale 1000-to-1 incast.
5.4 Endpoint Congestion
As discussed before, endpoint congestion has become increasingly
important with rising link rates and advent of IOPS-intensive appli-
cations. To show how Swift differentiates and handles both fabric
and endpoint congestion, we use 20 machines inT1 each with 5 flows
incast to a single destination machine. We use 1kB writes for an
IOPS-intensive workload and 64kB writes for a byte-intensive work-
load. We plot the fabric and endpoint congestion windows, f cwnd
and ecwnd, showing the median and tail values in Figure 19. For
the tail, we use the 1st-percentile since lower congestion windows
represent smaller sending rates and, hence, larger RPC latencies.
We see a clear distinction as the workload shifts: the IOPS-intensive
case is limited by the endpoint window, while the byte-intensive
case is limited by the fabric window.
Decomposing the end-to-end RTT into fabric and endpoint com-
ponents lets Swift to craft different responses to congestion at
network and hosts. To show this strategy has a performance ben-
efit, we compare Swift to a version (Swift-v0) that uses a single
target for end-to-end delay. We setup two concurrent incasts, one
for the fabric and one for endpoint. Three machines in T1, each
with 5 flows, send traffic to another receiver machine in T1. Two
of the three senders perform large (64kB) writes to create fabric
congestion, and the remaining one performs small (64B) writes to
stress the endpoint. We show the results in Table 4. We observe
that Swift (fabric base target of 50µs and engine target of 100µs)
achieves throughput close to line rate. Swift-v0 with an end-to-end
target of 100µs achieves much lower throughput. This happens be-
cause the machines performing large writes inappropriately reduce
their congestion windows due to congestion at the receiver. We can
increase the target for Swift-v0 to increase throughput, but it comes
at the cost of increased RTT. Swift-v0 with 200µs target achieves
line-rate throughput but observes higher average and 99th-p RTT.
5.5 Flow Fairness
Flow and topology scaling help Swift to achieve a fair allocation
of bandwidth across flows, regardless of whether the flows have
different path lengths.
Configuration
Swift
Swift-v0, 100µs target