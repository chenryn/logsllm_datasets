### Queue Utilization and Loss Rates

At a queue utilization of 0.5, we observe the same behavior, which gives us confidence that Swift can maintain low loss rates even when all traffic on the link is from Swift.

### Aggregate Cluster Throughput vs. Loss Rate

In Figure 9, we plot the aggregate cluster throughput against the loss rate for 25 clusters at Google. We report the loss rate for Swift and GCN separately, along with the total Swift/GCN throughput in the cluster. For brevity, we only include edge links. The results show that Swift consistently delivers low loss rates, even at extreme tails, while GCN exhibits much more variability. This low loss rate is attributed to Swift's prompt reaction to congestion, as detected by its target delay, and its ability to handle large incasts. Given the extremely small loss rate both at the edge and in the fabric, the end-to-end retransmission rate for Swift is also very low, consistent with our decision not to invest heavily in loss recovery.

### Performance Across Link Speeds

Swift’s performance improvements are evident across a range of link speeds. In Figure 10, we compare the average and 99.9th percentile loss rates for edge (ToR to host) links at high utilization (>90%). Even at near line-rate utilization, the 99.9th percentile loss rate for Swift is significantly lower than that of GCN.

### Latency Analysis

Next, we examine latency. Figure 11 shows the NIC-to-NIC round trip time (or fabric RTT) across our datacenters, as measured by NIC timestamps. Note that there is a spike in the [80-90%] loss rate bucket for GCN, which we have verified accurately reflects our production traffic, and we are investigating the possible causes.

### Weighted-Fair Queuing

Using weighted-fair queuing, Swift achieves low latency even when a significant portion of the traffic is controlled by loss or via GCN. As shown in Figure 11, Swift latency is substantially lower than GCN latencies when both queues have the same scheduling priority. Similarly, the cluster-scale measurements in Figure 12 demonstrate an order of magnitude lower latency for Swift compared to GCN.

### Isolation Mechanism

To stress the isolation mechanism, we compare packet loss versus port utilization for unequal scheduling priorities: GCN with strict priority scheduling and Swift at its lowest weight. Figure 13 shows that Swift controls queueing much better than GCN, even with less preferred access to link bandwidth.

### Fabric and Host Congestion

Swift's response to both host and fabric congestion has been crucial for its success in production. In a shared environment, we often have a mix of IOPS-intensive applications that stress hosts and throughput-intensive applications that stress the fabric. While most cluster congestion is in the fabric, host congestion is not uncommon, and if not addressed, it can unfairly degrade co-located throughput-intensive flows.

We selected two clusters to illustrate how the fabric and host contribute to end-to-end packet RTT. One cluster is dominated by IOPS-intensive tasks, while the other carries predominantly large storage RPCs. For each cluster, we split the end-to-end packet RTT as measured in Swift to obtain fabric and host components, as shown in Figure 14. There is a clear distinction: host delays are small and tight for the throughput-intensive cluster but can contribute as much as the fabric delays in the IOPS-intensive cluster.

### Application Performance

Swift supports both IOPS-intensive and throughput-intensive applications, ensuring they coexist well in a shared network environment.

#### In-Memory BigQuery Shuffle

Swift supports a disaggregated, in-memory filesystem for BigQuery shuffle, built atop Snap. The key metric for a memory-based file system is the IOPS and the completion time of small operations. Swift's ability to control network delays and host congestion at scale is invaluable for this application. Figure 15 shows that the operation completion time closely follows Swift’s target delay. The separate treatment of fabric and host congestion in Swift was essential for meeting access latency SLOs in all clusters. Additionally, the team informed us that keeping network latencies small provided meaningful backpressure, leading to a 7× improvement in tail latency.

#### Storage

Parts of the storage traffic are served over Swift, with throughput as the primary metric. Results from a load test provided by the SSD-storage team show that Swift achieves 4× lower 99.9th percentile application latency compared to GCN, as shown in Figure 16. Additionally, Swift achieves ~7% higher IOPS with a 100% success rate in operation completions, while GCN experienced 1.7% operation failures due to exceeded deadlines.

### Production Experience

In production, Swift's near-zero loss at near line-rate has sometimes caused confusion, as other teams are not accustomed to such performance at scale. On two occasions, bugs were incorrectly filed for monitoring failures because highly-utilized links reported zero loss. These cases were quickly attributed to Swift, facilitated by the use of QoS class separation.

Swift's extremely low delays have also met with skepticism, with concerns that it may be sacrificing throughput. To address this, we rolled out experimental versions of Swift that prioritized throughput at the cost of increased queuing. However, these versions showed no increase in throughput or improvement in application performance, confirming that Swift is not throttling traffic to keep latency and loss low.

### Experimental Results

We present results from controlled experiments to evaluate Swift's mechanisms using two testbeds: T1 with 60 machines and 50Gbps NICs, and T2 with ~500 machines and 100Gbps NICs.

#### Effect of Target Delay

Target delay is a key control parameter in Swift. In T1, we set up 10 sender machines with 10 flows per sender, pushing 64kB write RMA operations to a single receiver machine. We vary the base target delay from 15µs to 70µs and disable flow and topology scaling to highlight the impact of the base target delay. Figure 17 shows that the achieved RTT closely tracks the configured base target delay.

#### Throughput/Latency Curves

Given a target delay, we sweep the offered load to characterize the operating points of throughput vs. latency. Using T2 with a uniform random traffic pattern, we set the target delay to 25µs and vary the offered load by adjusting the interval over which operations are issued. Figure 18 shows that throughput increases with little rise in RTT until we exceed 80% of the line-rate. At full load, Swift maintains the 99.9th percentile RTT to be <50µs at an aggregate load close to 50Tbps.

#### Large-Scale Incast

Swift supports cwnd < 1 with pacing to handle large-scale incast, an important datacenter workload. In T1, we start 100 flows from each of 50 machines to a single destination machine, representing a 5000-to-1 incast. Table 3 shows that Swift achieves line-rate throughput with low latency and almost zero loss, while without cwnd < 1 support, the protocol degrades to high latency and loss, reducing throughput.

#### Endpoint Congestion

Endpoint congestion has become increasingly important with rising link rates and the advent of IOPS-intensive applications. To show how Swift differentiates and handles both fabric and endpoint congestion, we use 20 machines in T1, each with 5 flows incast to a single destination machine. We use 1kB writes for an IOPS-intensive workload and 64kB writes for a byte-intensive workload. Figure 19 shows the fabric and endpoint congestion windows, f_cwnd and e_cwnd, demonstrating a clear distinction as the workload shifts.

#### Flow Fairness

Flow and topology scaling help Swift achieve a fair allocation of bandwidth across flows, regardless of path lengths. Configuration comparisons in Table 4 show that Swift with a fabric base target of 50µs and engine target of 100µs achieves throughput close to line rate, while Swift-v0 with an end-to-end target of 100µs achieves much lower throughput. Increasing the target for Swift-v0 to 200µs improves throughput but at the cost of increased RTT.