0
37.1
0
61.4
0
31.4
0
41.4
0
42.9
0
64.3
1.4
77.1
1.4
54.3
2.86
62.9
1.4
64.7
Note: (1) “Song & TTS” is used for the abbreviation of “Original song + TTS”. (2) “Once-recognize” and “Twice-recognize” represent that the users can
recognize over half of the hidden command when they listen to the AEs for once and twice, respectively.
more likely the source of the distortion – a hidden command
can be perceived by human. This is largely in line with the
ﬁndings from our user study as below. However, the metric
will be less accurate, for example, when the distortion ﬁts
well in other background noise, becoming less easy to notice,
even when the SNR is low. In general, human perception of
hidden commands is complicated, depending on individuals’
experience, the context of a conversation, etc. Finding an ac-
curate measurement is still an open question. Therefore, we
conducted a survey15 on Amazon Mechanical Turk to eval-
uate human perception of the AEs generated by the Devil’s
Whisper attack, and compare the result with that of “Original
song + TTS”. Speciﬁcally, we used the audio clips in Group
1 since they have the similar SRoA as Devil’s Whisper when
attacking the target models.
The results of this user study are shown in Table 4. Here,
the column “Normal” shows the percentage of the users who
consider a sample to be normal music, and the column “Noise”
gives the percentage of the users who ﬁnd noise in songs. The
column “Once-recognize” and the column “Twice-recognize”
describe the percentages of the users able to recognize over
half of the hidden command words16 after listening to the
audio once or twice, respectively. As we can see from the
table, 16.1% participants think that somebody is talking in the
background when they listen to Devil’s Whisper, but nobody
could recognize any command when an AE was played to
them. By comparison, over 93% of the participants think
that someone is talking when listening to the audio clips in
“Original song + TTS”, and nearly 42.9% of them recognizes
over half of the command words ﬁrst time when they listened.
Even if the participants were exposed to the same AEs for the
second time, only 1.4% of them could tell over 50% words
in the target commands in the Devil’s Whisper attack, while
the ratio goes up to 64.7% in “Original song + TTS”. This
15This survey will not cause any potential risks to the participants, such
as psychological, social, legal, physical, etc. We do not ask any conﬁdential
information about the participants in the questionnaires. The IRB Exempt
certiﬁcates were obtained from our institutes.
16We assume that 50% of the words in the command would be enough to
raise user’s attention.
indicates that, the samples from “Original song + TTS” are
much more perceptive to users. Furthermore, by analyzing the
SNR in Table 3 and human perception results, we found that
SNR was largely in line with human perception but not always
(see the exception described in Section 6.2). The details of
the survey study are presented in Appendix E.
7 Discussion
7.1 Selection of Songs
In order to ﬁnd what types of songs are good candidates for
our attack in terms of both effectiveness and stealthiness, we
conducted a preliminary evaluation using all the 20 songs
from CommanderSong, including the 5 rock and 5 rap songs
that we did not use in our attack (see Section 6.1). The target
commands were the same as those used in the previous ex-
periments for Google and Microsoft Bing. In the evaluation,
a song is considered to be suitable for AE generation if it
helped produce effective AEs (for both commands) in the
ﬁrst epoch (based model -> substitute model). Note that an
effective AE is stealthy, as determined by humans (authors
and other group members in our research) who listened to it.
Through the evaluation, we classiﬁed the 20 songs into three
categories: (1) easy to generate successful AEs but noticeable
to human (2) easy to generate successful AEs and unnotice-
able to human (3) hard to generate successful AEs. Obviously,
the songs in the second category are good candidates for our
attack. These songs are characterized by the similarity in the
energy distributions of their spectra, as discovered in our re-
search. We here present an example to show its spectral power
distribution in Figure 2.
Further we looked into the Top 100 Billboard songs in the
week of 11/04/2018, embedding the commands “Hey Cortana,
what is the weather?” (Command A) and “Hey Cortana, make
it warmer” (Command B) into each of them, in an attempt
to attack the Microsoft Bing Speech Service API, and “Ok
Google, turn off the light” (Command C) and “Ok Google,
navigate to my home” (Command D) to attack the Google
Cloud Speech-to-Text API. During the attack, we selected the
segment between the 60th second to the 63th second (roughly
2678    29th USENIX Security Symposium
USENIX Association
piece of audio x, we can replace the sample xi with the more
smooth value according to its local reference sequence, i.e.
the average value of the k samples before and after xi. Hence,
the added perturbations may be mitigated by this method.
Audio source identiﬁcation. Audio source identiﬁcation
aims to identify the source of the audio, e.g., from an elec-
tronic speaker or human. Such defence is based on the as-
sumption that the legitimate voice commands should only
come from human rather than an electronic speaker. There-
fore, if the audio is detected not from human, the audio signal
will be simply ignored. Previous works [21,24] show that they
can identify the audio source by either examining the electro-
magnetic wave from the audio or training a model to label the
audio. Such defence mechanism could work for most of the
existing speech AEs that require a speaker to play. However,
the attacker could play the samples over a long range, which
might evade the detection.
7.3 Limitations
It is known that AEs are rather sensitive to the change made
on the deep neural network models behind ASRs: even a small
update could cause a successful AE to stop working. This is
also true for our approach. For instance, the previous work-
able AEs (in January 2019) cannot work effectively towards
Apple Siri since July 2019 (See Section 6.3)17. A potential
solution is to ﬁne-tune the existing model in the hope of cap-
turing the impact of the change, which will be studied in the
future research. In addition, the practical attack against IVC
devices is sensitive to various environmental factors, such
as the volume when playing AEs, the distance between the
speaker and the IVC device, even the brand of the speakers,
etc., which may signiﬁcantly affect the SRoA. Hence, how to
improve the robustness of the AEs in diverse environments is
still an open research question. Finally, although user study
shows that none of the participants can identify any command
from our AEs if they only listen to them once, a few partici-
pants felt our AEs noisy/abnormal. Therefore, improving the
stealthiness of AEs is on demand.
8 Conclusion
We present Devil’s Whisper, a general adversarial attack on
commercial black-box ASR systems and IVC devices, and
the AEs are stealthy enough to be recognized by humans.
The key idea is to enhance a simple substitute model roughly
approximating the target black-box platform with a white-box
model that is more advanced yet unrelated to the target. The
two models are found to effectively complement each other
for generating highly transferable and generic AEs on the
target, which only requires around 1500 queries on remote
services to ensure a nearly 100% success rate of command on
attacking most popular commercial ASR systems.
17We further used eight samples of the case “Original song + TTS” from
Table 3 to attack Siri and only 1 out of 8 samples can work. So, we consider
that Siri may have updated the system to ignore the speech with music
background.
Figure 2: Representative original song spectrum (a) Type
1: easy to be generated as successful AEs and perceived by
human (b) Type 2: easy to be generated as successful AEs
but difﬁcult to be perceived by human (c) Type 3: hard to be
generated as successful AEs.
the middle of the songs) for each song as the carrier for the
commands. For Command A, B, C, D, we successfully gener-
ated AEs based on 59, 56, 58 and 60 songs, respectively. Then
we asked 20 students to listen to the successful AEs generated
and reported the commands that could be recognized. In the
end, again, we classiﬁed all the 100 songs into these three
categories. Most of their frequencies and energy distributions
were found to be in consistent with those discovered in the 20
songs (see the example in Figure 2). This indicates that indeed
a more systematic way to select ideal carriers for the attack is
possible, which will be explored in the future research.
7.2 Discussion on Possible Defense
We discuss three potential defense mechanisms to mitigate
our Devil’s Whisper attack.
Audio downsampling. Audio downsampling was proposed
in CommanderSong [40] to effectively mitigate the AEs. Even
though the audio can be recorded in different formats (such
as m4a, mp3, wav) at different sampling rates (e.g. 8000Hz,
16000Hz, 48000Hz), we can always ﬁrst downsample it to
a lower sampling rate and upsample it to the sampling rate
that is accepted by the target black-box model. During such
downsampling/upsampling process, the added adversarial per-
turbations may be mitigated, which makes the AEs fail to
be recognized by the target black-box model. For instance,
we choose the recorded audios, which can succeed in WAA
attack on IBM Speech to Text API. Then they are downsam-
pled to 5600Hz, and upsampled to 8000Hz, which are sent
to IBM Speech to Text API. Only 20% of them can be rec-
ognized as the target commands. When ﬁrst downsampled to
5200Hz and then upsampled to 8000Hz, none of them can
succeed. In contrast, the regular recorded human voice and
TTS audio clips can still be recognized correctly even after
such downsampling/upsampling. Hence, audio downsampling
could be one effective way in detecting speech AEs. However,
if an attacker know the dawnsampling/upsampling rates of
the defense, he could train an AE robust against it.
Signal smoothing. Since the effectiveness of our AEs is
highly dependent on the carefully added perturbations by
gradient algorithm, we can conduct local signal smoothing
towards AEs to weaken the perturbations. Speciﬁcally, for a
USENIX Association
29th USENIX Security Symposium    2679
012Time [s]43.63.22.82.421.61.20.80.40Frequency [KHz]-100-500012Time [s]43.63.22.82.421.61.20.80.40012Time [s]43.63.22.82.421.61.20.80.40Power [dB](a)(b)(c)9 Acknowledgments
We greatly appreciate our reviewers’ insightful comments,
which helped us to improve our work. Speciﬁcally, we thank
our shepherd, Professor Yongdae Kim, for his constructive
feedback on this paper. We also thank Dohyun Kim and
Taekkyung Oh in Professor Kim’s group, for their efforts
to reproduce our results. IIE authors are supported in part by
Beijing Natural Science Foundation (No.JQ18011), NSFC
U1836211, 61728209, National Top-notch Youth Talents Pro-
gram of China, Youth Innovation Promotion Association CAS,
Beijing Nova Program, National Frontier Science and Tech-
nology Innovation Project (No. YJKYYQ20170070) and a
research grant from Huawei. Indiana University author is
supported in part by NSF CNS-1527141, 1618493, 1801432,
1838083 and ARO W911NF1610127. For Florida Tech au-
thor, part of his work is using the blueshark server supported
by NSF CNS 09-23050.
References
[1] Adobe Audition. https://www.adobe.com/il_en/produc
ts/audition.html.
[2] Alexa Auto SDK. https://developer.amazon.com/zh/al
exa-voice-service/alexa-auto-sdk.
[3] Alexa Polly. https://aws.amazon.com/polly/.
[4] Amazon Transcribe. https://aws.amazon.com/transcrib
e/.
[5] Audio Adversarial Examples: Targeted Attacks on Speech-to-
Text. https://github.com/carlini/audio_adversarial
_examples/blob/master/attack.py.
[6] Bing Text to Speech. https://azure.microsoft.com/en-u
s/services/cognitive-services/text-to-speech/.
[7] Comparitive
examples
of noise
levels.
http:
//www.industrialnoisecontrol.com/comparative
-noise-examples.htm.
[8] Demo of CommanderSong. https://sites.google.com/v
iew/commandersong/.
[9] From Text to Speech. http://www.fromtexttospeech.com.
[10] Google Cloud Speech-to-Text. https://cloud.google.com
/speech-to-text/.
[11] Google Cloud Text-to-speech. https://cloud.google.com
/text-to-speech/.
[12] IBM Speech to Text. https://www.ibm.com/watson/servi
ces/speech-to-text/.
[13] IBM Text to Speech. https://www.ibm.com/watson/servi
ces/text-to-speech/.
[14] Kaldi ASR. http://kaldi-asr.org.
[15] Kaldi ASR: Extending the ASpIRE model. https://chrise
arch.wordpress.com/2017/03/11/.
[16] Microsoft Bing Speech Service. https://azure.microsoft.
com/en-us/services/cognitive-services/bing-web
-search-api/.
[17] PSU Noisequest. https://www.noisequest.psu.edu/noi
sebasics-basics.html.
[18] Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick
Traynor, Kevin R. B. Butler, and Joseph Wilson. Practical
hidden voice attacks against speech and speaker recognition
systems. In NDSS’19, pages 1369–1378, 2019.
[19] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai
Zhang, Micah Sherr, Clay Shields, David Wagner, and Wen-
chao Zhou. Hidden voice commands. In 25th USENIX Security
Symposium (USENIX Security 16), Austin, TX, 2016.
[20] Nicholas Carlini and David Wagner. Audio adversarial exam-
ples: Targeted attacks on speech-to-text. In 2018 IEEE Security
and Privacy Workshops (SPW), pages 1–7. IEEE, 2018.
[21] Si Chen, Kui Ren, Sixu Piao, Cong Wang, Qian Wang, Jian
Weng, Lu Su, and Aziz Mohaisen. You can hear but you
cannot steal: Defending against voice impersonation attacks
on smartphones. In Distributed Computing Systems (ICDCS),
2017 IEEE 37th International Conference on, pages 183–195.
IEEE, 2017.
[22] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann
Dauphin, and Nicolas Usunier. Parseval networks: Improv-
ing robustness to adversarial examples. In Proceedings of the
34th International Conference on Machine Learning-Volume
70, pages 854–863. JMLR. org, 2017.
[23] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun
Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks
with momentum. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 9185–9193,
2018.
[24] Yuan Gong and Christian Poellabauer. Protecting voice con-
trolled systems using sound source identiﬁcation based on
In 2018 27th International Conference on
acoustic cues.
Computer Communication and Networks (ICCCN), pages 1–9.
IEEE, 2018.
[25] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg
Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho
Sengupta, Adam Coates, et al. Deep speech: Scaling up end-
to-end speech recognition. arXiv preprint arXiv:1412.5567,
2014.
[26] Hynek Hermansky. Perceptual linear predictive (plp) analysis
of speech. The Journal of the Acoustical Society of America,
87(4):1738–1752, 1990.
[27] Fumitada Itakura. Line spectrum representation of linear pre-
dictor coefﬁcients of speech signals. The Journal of the Acous-
tical Society of America, 57(S1):S35–S35, 1975.
[28] Chaouki Kasmi and Jose Lopes Esteves. Iemi threats for in-
formation security: Remote command injection on modern
smartphones. IEEE Transactions on Electromagnetic Compat-
ibility, 57(6):1752–1755, 2015.
[29] Deepak Kumar, Riccardo Paccagnella, Paul Murley, Eric Hen-
nenfent, Joshua Mason, Adam Bates, and Michael Bailey. Skill
squatting attacks on amazon alexa. In 27th {USENIX} Security
Symposium ({USENIX} Security 18), pages 33–47, 2018.
[30] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delv-
ing into transferable adversarial examples and black-box at-
tacks. arXiv preprint arXiv:1611.02770, 2016.
2680    29th USENIX Security Symposium
USENIX Association
[31] Lindasalwa Muda, Begam KM, and I Elamvazuthi. Voice
recognition algorithms using mel frequency cepstral coefﬁcient
(mfcc) and dynamic time warping (dtw) techniques. Journal
of Computing, 2(3):138–143, 2010.
[32] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh
Jha, Z Berkay Celik, and Ananthram Swami. Practical black-
box attacks against machine learning. In Proceedings of the
2017 ACM on Asia Conference on Computer and Communica-
tions Security, pages 506–519. ACM, 2017.
[33] Yao Qin, Nicholas Carlini, Garrison Cottrell, Ian Goodfellow,
and Colin Raffel. Imperceptible, robust, and targeted adversar-
ial examples for automatic speech recognition. In International
Conference on Machine Learning, pages 5231–5240, 2019.
[34] Lea Schönherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz,
and Dorothea Kolossa. Adversarial attacks against automatic
speech recognition systems via psychoacoustic hiding.
In
accepted for Publication, NDSS, 2019.
[35] Charles Stephenson. Tracing those who left: Mobility studies
and the soundex indexes to the us census. Journal of Urban
History, 1(1):73–84, 1974.
[36] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.
arXiv preprint
Intriguing properties of neural networks.
arXiv:1312.6199, 2013.
[37] Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita Vemuri.
Targeted adversarial examples for black box audio systems. In
2019 IEEE Security and Privacy Workshops (SPW), pages 15–
20. IEEE, 2019.
[38] Tavish Vaidya, Yuankai Zhang, Micah Sherr, and Clay Shields.
Cocaine noodles: exploiting the gap between human and ma-
chine speech recognition. Presented at WOOT, 15:10–11, 2015.
[39] Xuejing Yuan, Yuxuan Chen, Aohui Wang, Kai Chen, Shengzhi