solution. In the end assign selects the solution with the lowest
score as the best one. Algorithm 1 gives a high-level overview of
assign’s operation.
8.2 assign+
In designing assign+ our main insight was to use expert knowl-
edge of network testbed architecture to identify allocation strategies
that lead to minimizing interswitch bandwidth. These strategies are
deployed deterministically to generate candidate solutions, instead
of exploring the entire space of possible allocations via simulated
annealing, which signiﬁcantly shortens the run time. We also recog-
nized that allocating strongly connected node clusters in experiment
topologies together leads to preservation of interswitch bandwidth
and shortens the run time. In the long run these strategies also lead
assign one node from unassigned to a pclass
Algorithm 1 assign pseudocode.
1: generate pclasses; put all virtual nodes into unassigned set
2: map each virtual node to candidate pclasses
3: repeat
4:
5: until unassigned = ∅
6: repeat
7:
8:
9: until sufﬁcient iterations or average score low
10: select the lowest scored solution as best
solution = remap one virtual node to a different pclass
score solution; solutions+ = solution
to better distribution of instances over heterogeneous testbed re-
sources.
Like assign, assign+ generates pclasses and precomputes
for each virtual node a list of pclasses that are acceptable candi-
dates. It then generates candidate lists, aggregating virtual nodes
that can be satisﬁed by the same candidate pclasses. For example,
in Figure 3 n1 can be satisﬁed by pclass1 or pclass2, n2 can be sat-
isﬁed only by pclass2 because it requires three network interfaces,
and n3, n4 and n5 can be satisﬁed by pclass2, pclass3 or pclass4.
Each pclass has a size which equals the number of currently avail-
able testbed nodes that belong to it. Next, the program calls its
allocate function ﬁve times, each time exploring a separate al-
location strategy.
The main idea of the allocate function is to divide the virtual
topology into several connected subgraphs and attempt to map each
subgraph or its portion in one step, if possible. Only if this fails, the
function attempts to map individual virtual nodes. This reduces the
number of allocation steps, while minimizing the interswitch band-
width, because connected nodes are mapped in one step whenever
possible.
The allocate function ﬁrst breaks the virtual topology into
several connected partitions attempting to minimize the number of
cut edges. Our partitioning goal is to create a large number of pos-
sible partitions, where smaller partitions can be subsets of larger
ones. This allows us ﬂexibility to map these partitions to different-
sized pclasses. We achieve this goal by traversing the topology
from edges to the center and forming parent-child relationships, so
that nodes closer to the center become parents of the farther nodes.
Graph partitioning problem has many well-known solutions (e.g.
[12]), but these either require the number of partitions to be known
in advance – whereas we want to keep this number ﬂexible – or they
are too complex for our needs. We employ the following heuristic
to generate the partitions we need. We start from virtual nodes with
the smallest degree and score them with number 1, also initializ-
ing round counter to 1. In each consecutive round, links that are
directly connected to the scored nodes are marked, if the peer on
the other side of the link is either not scored yet or is scored with
the higher number. The peer becomes a “parent” of the scored node
if it does not already have one. The process stops when all nodes
in the virtual topology have been scored. We illustrate the scores
for nodes in the virtual topology in Figure 3. Black nodes belong
to one partition and white ones to the second one. Node n2 is the
parent of nodes n4 and n5 and node n1 is the parent of the node n3.
Next, the allocate function traverses the candidate list from
the most to the least restricted, attempting to map each virtual node
and, if possible, its children. Let us call the virtual node that is
currently being allocated the allocating node. The most restricted
candidate list has the smallest number of pclasses. In our example
this is the list for node n2. The function calculates the number of
virtual nodes that must be allocated to this list and the number of
502physical nodes available in the list. If the ﬁrst is larger than the sec-
ond the entire mapping fails. Otherwise, we calculate for each par-
ent node in the candidate list two types of children pools: minimum
pool and maximum pool. Both calculations only include those chil-
dren that have not yet been allocated. The minimum pool relates to
the candidate list and contains all the children of the node that must
be allocated to this list. The maximum pool relates to each pclass
in the candidate list and contains all the children of the given par-
ent node that can be allocated to this pclass. In our example, when
we allocate n1 its minimum pool would be empty because neither
n4 nor n5 must be allocated to pclass2, while the maximum pool
would contain n4 and n5 for pclass2. The allocate function tra-
verses each pclass in the current candidate list in an order particular
to each allocation strategy we explore. This order is always from
the most to the least desirable candidate. It ﬁrst tries to allocate the
allocating node and its maximum pool. If there are no resources in
any of the pclasses of the candidate list it tries to allocate the allo-
cating node and its minimum pool. If this also fails, it tries only to
allocate the allocating node. If this fails the entire mapping fails.
Algorithm 2 assign+ pseudocode.
1: generate pclasses
2: map each virtual node to candidate pclasses
3: generate candidate lists
4: for strategy = (PART, SCORE, ISW, PREF, FRAG) do
5:
6:
7: end for
8: select the solution with the lowest interswitch bw as best
9: break ties by selecting solution with the lowest score
solution = allocate(strategy)
score solution; solutions+ = solution
There are ﬁve allocation strategies we pursue in the calls to the
allocate function: PART, SCORE, ISW, PREF and FRAG. Each
strategy uses expert knowledge of possible network testbed archi-
tectures to generate candidate solutions that are supposed to mini-
mize interswitch bandwidth use. The success of each strategy de-
pends on the available resources and the size and user-speciﬁed
constraints in a given virtual topology. The ﬁrst strategy – PART –
minimizes partitions in the virtual topology by allocating pclasses
from largest to smallest size. This improves packing of future in-
stances and also reduces number of interswitch links. The second
– SCORE – minimizes the score of the allocation by allocating
pclasses from those with the smallest to those with the largest score.
We explore different ways to score a pclass, e.g., based on how
many features it supports, based on how often it is requested, or a
combination of both. When we score by features we do not use fea-
ture weights, but instead just add up counts of supported features.
The next three allocation strategies prefer those pclasses that al-
ready host parents or children of the allocating node, thus minimiz-
ing interswitch bandwidth demand. In addition to parent/child host
preference, the ISW strategy also prefers those pclasses that have
high-bandwidth interswitch links to pclasses, which host neigh-
bors of the allocating node. This only makes a difference when
interswitch links have different capacities, in which case ISW will
minimize the risk of mapping failure due to interswitch bandwidth
oversubscription. In addition to parent/child host preference, the
PREF strategy also prefers those pclasses that share a switch with
pclasses, which host neighbors of the allocating node. This mini-
mizes use of interswitch bandwidth because communication is con-
tained within one switch, even though it occurs among different
pclasses. The PREF strategy tries to both minimize the interswitch
bandwidth and to minimize partitions in the virtual topology by al-
locating from pclasses with the largest to those with the smallest
product of their preference and size. The FRAG strategy only de-
ploys parent/child preference and tries to use the smallest number
of pclasses by allocating from pclasses with the largest to those
with the smallest product of their preference and size.
At the end, the allocate function records the candidate so-
lution and then tries to further reduce interswitch bandwidth cost
by running Kernighan-Lin graph partitioning algorithm [12] to ex-
change some nodes between pclasses if possible. Each exchange
generates a new candidate solution. The algorithm stops when no
further reduction is possible in the interswitch bandwidth. Each
solution’s score is the sum of scores of all the physical nodes in it.
After all calls to the allocate function return, assign+ choo-
ses the best solution. This solution has the smallest interswitch
bandwidth. If multiple such solutions exist, the one with the small-
est score is selected. Algorithm 2 gives a high-level overview of
assign+’s operation.
8.3 Evaluation
Algorithm
assign
assign+.1
assign+.1m
assign+.2m
assign+.at
assign+.mig
assign+.atmig
assign+.tb
assign+.borrow
Failed allocations % Baseline
(out of 19,258)
1,176 (mean)
905
983
917
801
701
674
298
301
100%
77%
83.6%
78%
68.1%
59.6%
57.3%
25.3%
25.6%
Table 4: Evaluation summary for allocation failure rates
To compare the quality of found solutions and the runtime of
assign and assign+, we needed a testbed state (hardware types
and count, node types, features, weights and operating systems sup-
ported by each hardware type, node and switch connectivity), and
a set of resource allocation requests. We reconstruct the state of
the DeterLab testbed on January 1, 2011 using virtual topology and
testbed state snapshot data from the ﬁlesystem. To make the al-
location challenging, we permanently remove 91 PC nodes from
the available pool, leaving 255. While this may seem extreme, our
analysis of testbed state over time indicates that often this many or
more PCs are unavailable due to either reserved but not yet used
nodes or to internal testbed development. We seed the set of re-
source allocation requests with all successful and failed allocations
on DeterLab in 2011. Each request contains the start and end time
of the instance and its virtual topology ﬁle. For failed allocations,
we generate their desired duration according to the duration dis-
tribution of successful allocations. Finally, we check that there
are no overlapping instances belonging to the same experiment. If
found, we keep the ﬁrst instance and remove the following over-
lapping instances from the workload. We test this workload both
with assign and assign+ on empty testbed and remove 1.3%
of instances that fail with both algorithms, because the reduced-size
testbed does not meet experimenter’s constraints. We will label this
ﬁnal simulation setup “2011 synthetic setup”. We then attempt to
allocate all workload’s instances, and release them in order dictated
by their creation and end times, evolving the testbed state for each
allocation and each release. Evaluation results are summarized in
Table 4.
Figure 6 shows the allocation failure rate over time on this setup
for both algorithms. Since assign deploys randomized search,
503)
1
-
0
(
e
t
a
r
e
r
u
l
i
a
F
 0.08
 0.07
 0.06
 0.05
 0.04
 0.03
 0.02
 0.01
 0
 0
assign+ is consistently around 10 times faster than assign,
thanks to its deterministic exploration of the search space.
assign
assign+.1
assign+.1m
assign+.2m
)
s
p
b
G
(
w
b
h
c
t
i
w
s
r
e
t
n
I
 16
 14
 12
 10
 8
 6
 4
 2
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
Instances (thousands)
Figure 6: Allocation failure rates
assign
assign+
0 1
2
3
4
0
0
0
0
5
6
0
0
7
8
0
0
9
1
0
0
1
1
1
2
1
1
3
4
1
1
5
6
1
1
7
8
0
0
0
0
0
0
0
0
0
we show the mean and the standard deviation of its 10 runs. We
test several approaches to score calculation: assign+.1 deploys
assign-like approach, where node types with more unwanted fea-
tures are penalized higher, (2) assign+.1m, same as assign+.1
but with memory, so node types that are requested more often re-
ceive a higher penalty when allocated, and (3) assign+.2m scores
nodes only by how often they are requested. In case of assign,