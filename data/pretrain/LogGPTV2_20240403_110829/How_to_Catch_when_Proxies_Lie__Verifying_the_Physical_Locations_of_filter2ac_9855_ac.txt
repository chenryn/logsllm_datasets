anchors, the majority are in Europe and North America, but we
have enough contributors elsewhere for statistics.
Our priority was to find an algorithm that would always include
the true location of each host in its predicted region, even if this
meant the region was fairly imprecise. To put it another way, when
investigating the locations of commercial proxies, we want to be
certain that the proxy is where we say it is, even if that means we
cannot assure that it is not where the provider says it is.
In figure 9, panel A, we plot an empirical CDF of how far outside
the predicted region each true location is, for each of the four
algorithms. This is a direct measure of each algorithm’s failure to
live up to the above requirement. None of the algorithms are perfect,
but CBG does better than the other three, producing predictions
that do include the true location for 90% of the test hosts, and are off
by less than 5000 km for 97% of them. Hybrid and Quasi-Octant’s
predictions miss the mark for roughly 50% of the test hosts, but they
are off by less than 5000 km for roughly 90%. Fully half of Spotter’s
predictions are off by more than 10 000 km.
In panels B and C of Figure 9, we look into why the predictions
miss the true region. Panel B shows that the distances from the
centroid of each algorithm’s predictions, to the true locations, are
about the same for all four algorithms, and panel C shows that CBG
produces predictions that are much larger than the other three.
We conclude that none of the algorithms can reliably center their
predicted region on the true location, but CBG’s predictions are
VolunteersMTurk workers0.000.250.500.751.0005 00010 00015 00020 000Distance from edge to location (km)Empirical CDFA05 00010 00015 00020 000Distance from centroid to location (km)B0.000.250.500.751.00Area of region / Earth land areaCBGasi-OctantSpoerHybridCHow to Catch when Proxies Lie
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
Figure 10: CBG bestline and baseline estimates compared to
the true distance.
usually big enough to cover the true location anyway, whereas the
other three algorithms’ predictions are not big enough.
Why should CBG be so much more effective? Looking again at
the calibration data in Figure 2, we observe that most of the data
points are well above CBG’s bestline. Quasi-Octant and Spotter
draw more information from these points than CBG does. If most of
those points are dominated by queueing and circuitousness delays,
rather than the great-circle distance between pairs of landmarks,
that would lead Quasi-Octant and especially Spotter to underesti-
mate the speed packets can travel, therefore predicting regions that
are too small. Large queueing delays also invalidate the assumption,
shared by both Quasi-Octant and Spotter, that there is a minimum
speed packets can travel [32].
Most of our crowdsourced contributors used the web application
under Windows. As we described in Section 4.3, this introduces
extra noise and “high outliers” into the measurements. CBG has
an inherent advantage in dealing with measurements biased up-
ward, since it always discards all but the quickest observation for
each landmark, its bestlines are the fastest travel time consistent
with the data, and it does not assume any minimum travel speed
when multilaterating. Crowdsourced measurements using only the
command-line tool might have allowed Quasi-Octant and Spotter
to do better. However, measurements taken through proxies are
liable to suffer extra noise and queuing delays as well. We could
thus argue that the web application’s limitations make the crowd-
sourced test a better simulation of the challenges faced by active
geolocation of proxies.
5.1 Eliminating Underestimation: CBG++
Regardless of the reasons, CBG clearly is the most effective al-
gorithm in our testing, but it still doesn’t always cover the true
209
Figure 11: Proportion of measurements that had an effect
on the final prediction region, as a function of distance be-
tween landmark and target; for effective measurements, the
amount by which they reduced the size of the final region.
The total land area of Earth is roughly 150 square megame-
ters (Mm2), and the land area of Egypt is roughly 1 Mm2.
location with its predictions. We made two modifications in order
to eliminate this flaw, producing a new algorithm we call CBG++.
CBG’s disks can only fail to cover the true location of the target
if some of them are too small. A disk being too small means the
corresponding bestline underestimates the distance that packets
could travel. This can easily happen, for instance, when the network
near a landmark was congested during calibration [28]. Not only
can an underestimate make the prediction miss the target, it can
make the intersection of all the disks be empty, meaning that the
algorithm fails to predict any location for the target.
To reduce the incidence of underestimation, we first introduced
another physical plausibility constraint. CBG’s bestlines are con-
strained to make travel-speed estimates no faster than 200 km/ms
as packets can travel no faster than this in undersea cables. We
also constrain them to make travel-speed estimates no slower than
84.5 km/ms; this is the “slowline” in the CBG panel of Figure 2. The
logic behind this number is: No landmark can be farther than half
the equatorial circumference of the Earth, 20 037.508 km, from the
target. One-way travel times greater than 237 ms could have in-
volved a geostationary communications satellite, and one such hop
can bridge any two points on the same hemisphere, so they provide
no useful information. 20 037.508 km/237 ms = 84.5 km/ms.
The slowline constraint is not enough by itself. Figure 10 shows
the distribution of ratios of bestline and baseline distance estimates
to the true distances, for all pairs of landmarks, with the slowline
BaselineBestline012345+05 00010 00015 00020 00005 00010 00015 00020 000Estimated/true distance ratioReal distance0200400600MeasurementsIneﬀectiveEﬀective0.0010.010.111010005 00010 00015 000Landmark–target distance (km)Reduction in area (Mm2)IMC ’18, October 31–November 2, 2018, Boston, MA, USA
Zachary Weinberg et al.
Figure 12: The RTT from a proxy to a landmark, A, must be
derived from the RTT through a proxy to a landmark, B, and
the RTT through a proxy back to the client, C: A = B − ηC.
constraint applied. We use the landmarks themselves for this anal-
ysis, rather than the crowdsourced test hosts, because we know
their positions more precisely and the ping-time measurements
they make themselves are also more accurate. A small fraction of
all bestline estimates are still too short, and for very short distances
this can happen for baseline estimates as well.
We weed out the remaining underestimates with a more sophis-
ticated multilateration process. For each landmark, we compute
both the bestline disk, and a larger disk using the baseline. We find
the largest subset of all the baseline disks whose intersection is
nonempty; this is called the “baseline region.” Any bestline disk
that does not overlap this region is discarded. Finally we find the
largest subset of the remaining bestline disks whose intersection
is nonempty; this is the “bestline region.” These subsets can be
found efficiently by depth-first search on the powerset of the disks,
organized into a suffix tree. Retesting on the crowdsourced test
hosts, we found that this algorithm eliminated all of the remaining
cases where the predicted region did not cover the true location.
5.2 Effectiveness of Landmarks
To check the observations of Khan et al. [26] and others, that land-
marks closer to the target are more useful, we measured the round-
trip time between all 250 RIPE Atlas anchors and the target for all
of the crowdsourced test hosts. A large majority of all measure-
ments lead to disks that radically overestimate the possible distance
between landmark and target. Multilateration produces the same
final prediction region even if these overestimates are discarded.
We call these measurements ineffective. As shown in Figure 11,
effective measurements are more likely to come from landmarks
close to the target, but among the effective measurements, there is
no correlation between distance and the amount by which the mea-
surement reduced the size of the final prediction. This is because a
distant landmark may still have only a small overlap with the final
prediction region, if it is distant in just the right direction.
5.3 Adaptations for Proxies
When taking measurements through a network proxy, each mea-
sured round-trip time is the sum of the RTT from the client to the
proxy, and the RTT from the proxy to the landmark. To locate the
proxy, we need to measure and subtract the RTT from the client
Figure 13: The relationship between direct and indirect
round-trip times, η, is almost exactly 1/2.
to the proxy. We cannot measure this directly, because the proxy
services usually configure their hosts not to respond to ICMP ping
packets, and aggressively rate-limit incoming TCP connections.
Instead, we take inspiration from Castelluccia et al. [5] and have
the client ping itself, through the VPN, as illustrated in Figure 12.
This should take slightly more than twice as long as a direct ping.
Figure 13 shows the relationship between direct and indirect pings
for all of the proxies in the study that can be pinged both ways. The
blue line is a robust linear regression, whose slope η is the inverse
of the RTT_factor described by Castelluccia et al.. In our case, the
slope is 0.49 with R2 > 0.99.
6 LOCATING VPN PROXIES
We used the two-phase, proxy-adapted CBG++ to test the locations
of proxies from seven VPN providers. This paper’s purpose is not
to call out any specific provider for false advertising, so we are
not naming the seven providers that we tested; however, figure 14
shows their rankings by number of countries and dependencies
claimed, with 150 of their competitors for comparison. Providers A
through E are among the 20 that make the broadest claims, while F
Figure 14: The countries where 157 VPN providers claim to
have proxies. Providers included in this study are colored
and labeled. Data provided by VPN.com [17].
210
ABCLandmark(Manchester)Proxy(Lyon)Client(Frankfurt)01002003000200400600Indirect RTTDirect RTTProviderCountryBEADCFGHow to Catch when Proxies Lie
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
Figure 15: Disambiguation by data center locations: the only
data centers in this region are in Chile, not Argentina.
Figure 16: Disambiguation by metadata: all these hosts be-
long to the same provider, the same AS, and the same /24, so
they are likely to be in the same physical location.
and G make more modest and typical claims. Notice that providers
who claim only a few locations, tend to claim more or less the same
locations; this is what one would expect if it were much easier to
lease space in a data center in some countries than others.
All of the VPN providers we tested use round-robin DNS for load
balancing; to avoid the possibility of unstable measurements, we
looked up all of the server hostnames in advance, from the same
host that would run the command-line measurement tool, and
tested each IP address separately. We used a single client host for
all of the measurements, located in Frankfurt, Germany. Because of
this, we cannot say whether the VPN providers might be using DNS
geotargeting or anycast routing to direct clients in different parts
of the world to different servers. In total, we tested 2269 unique
server IP addresses, allegedly distributed over 222 countries and
territories.
None of the providers advertise exact locations for their prox-
ies. At best they name a city, but often they only name a country.
City claims sometimes contradict themselves; for instance, we ob-
served a config file named “usa.new-york-city.cfg” directing
the VPN client to contact a server named “chicago.vpn-provi-
der.example.” Therefore, we only evaluate country-level claims.
CBG++ tells us only that a proxy is within some region. If that
region is big enough to cover more than one country, we can’t be
certain where the server really is. However, we might still be certain
that it isn’t where the proxy provider said it was; for instance, a
predicted region that covers Canada and the USA still rules out
the entire rest of the world. We say that the provider’s claim for a
proxy is false if the predicted region does not cover any part of the
claimed country. We say that it is credible if the predicted region is
entirely within the claimed country, and we say that it is uncertain
if the predicted region covers both the claimed country and others.
For false and uncertain claims, we also checked whether any of
the countries covered by the prediction region were on the same
continent as the claimed country.
Some uncertain predictions can be resolved by referring to a list
of known locations of data centers, such as the one maintained by
the University of Wisconsin [43]. For example, the prediction shown
in Figure 15 is uncertain because it covers Argentina as well as Chile.
211
However, the only data centers within the region are in Chile, so we
can conclude that this server is in Chile. When data center locations
are not enough, cross-checking with network metadata may help.
For example, in Figure 16, the largest of the 20 predicted regions
cover data centers on both sides of the USA-Canada border, but all
of the hosts share a provider, an autonomous system (AS), and a
24-bit network address, which means they are practically certain
to be in the same data center. Since all of the regions cover part
of Canada, but only some of them cross into the USA, we ascribe
all of these hosts to Canada. Overall, these techniques allow us to
reclassify 353 uncertain predictions as credible or not-credible.
Putting it all together, we find that the claimed location is credible
for 989 of the 2269 IP addresses, uncertain for 642, and false for
638. For 401 of the false addresses, the true location is not even
on the same continent as the claimed location; however, for 462
of the uncertain addresses, the true location is somewhere on the
same continent as the claimed location. (See Appendix A for how
we defined continental boundaries, and some discussion of which
countries and continents are most likely to be confused.)
Figure 17 shows which countries, overall, are more likely to host
credibly-advertised proxies, and where the servers for the false
claims actually are. The ten countries with the largest number of
claimed proxies account for 84% of the credible cases, and only 11%
of the false cases. (Uncertain cases are nearly evenly split between
the top ten and the remainder.) False claims are spread over the
“long tail” of countries, with only a few advertised servers each.
Figure 17 also shows the overall effect of using data center and AS
information to disambiguate predictions. It is particularly effective
when the prediction region crosses continents; 55% of those cases
were completely resolved for our purposes. Only 23% of the regions
covering multiple countries within the same continent could be
disambiguated.
Figure 18 shows another perspective on the same observation,
by relating credibility to the country ranking in Figure 14. The