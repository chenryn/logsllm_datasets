additional approval or multiple days before completing to
enable detection and remediation by targets. Combined, these
strategies reﬂect a concrete need for both platforms and the
security community to re-evaluate their existing threat models
when balancing utility versus safety. For new features or
platforms, threat modeling for potential abuse scenarios can
be just as important as modeling potential security risks.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
258
E. Policies, education, and awareness
Apart from technical and design solutions, tackling hate and
harassment also requires investing in better social structures,
including policies, education resources, training, and support
infrastructure [40], [64], [72], [111]. In 2016, Pater et al. found
that 13 of 15 prominent social networks forbade hate and ha-
rassment, but none provided an actual deﬁnition [116]. Instead,
the platforms listed potential types of abusive activities such
as attacks, bullying, defamation, harm, hate, impersonation,
racism, stalking, and threats [116]. The lack of well-crafted
policies or deﬁnitions in turn can demoralize targets of hate
and harassment [13].
VI. TENSIONS AND CHALLENGES
When considering the expansion of threat models and
technical enforcement to address hate and harassment, there
remain signiﬁcant tensions around how best to balance the
competing social equities at stake. Challenges also remain for
how our community can safely conduct research in this space.
A. Tensions balancing social equities
Empowering vs. burdening targets. Strategies like nudges,
indicators, and moderation can empower users to control their
online environment. At the same time, they place much of
the burden of staying safe on targets, which might exacerbate
emotional tolls. Although studies have explored outsourcing
this burden to family and peers [103], in general, platforms
must balance between “paternalism” and overloading targets
with safety decisions, while working to place burdens on
attackers, not targets.
Moderation vs. ﬁlter bubbles and free speech. In providing
users, communities, and platforms with technical mechanisms
to ﬁlter abusive content, there is a risk that moderation turns
into a form of censorship—either intentionally or uninten-
tionally. A real example includes a prominent social network
“suppress[ing] the reach of content created by users assumed
to be ‘vulnerable to cyberbullying”’,
including “disabled,
queer, and fat creators” [17]. Even with well-deﬁned policies,
moderators may step beyond the bounds of their expected role
to suppress content they dislike [148].
Manual review vs. well-being. Given the subjective nature
of hate and harassment, manual review remains a crucial task
in arriving at accurate decisions. However,
this can place
signiﬁcant emotional burdens on reviewers and result in mental
health issues without proper safeguards in place [73], [112].
Restricting content vs. research access. As platforms face
pressure to remove or restrict hate and harassment in a timely
fashion, researchers face an added challenge of identifying
or retroactively studying attacks. A similar challenge exists
presently for transparency around disinformation campaigns
and preventing attacks from causing futher damage [70].
Privacy vs. accountability.
Increasing user expectations
around privacy add new challenges to combating abuse.
Privacy-preserving technologies like end-to-end encryption
provide many capabilities including secrecy, deniability, and
untraceability. Secrecy precludes platforms from performing
content-based analysis and ﬁltering. Deniability compounds
the difﬁculty of targets being able to collect and provide
evidence of hate and harassment. Untraceability masks the
source of hate and harassment, especially in the presence
of viral distribution. Research has explored the possibility of
providing weaker—but still meaningful—privacy guarantees;
and anonymous tools for reporting abuse [96], [121], [139].
B. Challenges for researchers
Researcher safety and ethics. Currently, there are no best
practices for how researchers can safely and ethically study
online hate and harassment. Risks facing researchers include
becoming a target of coordinated, hostile groups, as well as
emotional harm stemming from reviewing toxic content (sim-
ilar to risks for manual reviewers) [2]. Likewise, researchers
must ensure they respect at-risk subjects and do not further
endanger targets as they study hate and harassment.
Risks of greater harm. As platforms prevent and mitigate
hate and harassment, there is a risk that the subsequent arms
race escalates the severity of attacks. In particular, attackers
may migrate to private, virulent communities that glorify hate
and harassment. Other risks may include attackers resorting to
physical or sexual violence against a target [63], [65], [66].
Deﬁning success. When expanding threat models to include
hate and harassment, it is natural to wonder what success
would look like. Progress can be measured quantitatively for
some areas, like measuring toxic content by the volume of
people exposed to abusive messages, or qualitatively as a
whole by the decrease in people reporting negative experiences
online. At the same time, there are multiple other factors that
underpin such metrics: the overhead and friction of solutions
on platforms and users; the cost of maintaining defenses in
an adversarial setting; and balancing false positives (e.g., in-
correctly penalizing legitimate people) against false negatives
(e.g., people exposed to hate and harassment).
VII. CONCLUSION
In this work, we argued that security, privacy, and anti-abuse
protections are failing to address the growing threat of online
hate and harassment. We proposed a taxonomy, built from over
150 research articles, to reason about these new threats. We
also provided longitudinal evidence that hate and harassment
has grown 4% over the last three years and now affects 48%
of people globally. Young adults, LGBTQ+ individuals, and
frequent social media users remain the communities most at
risk of attack. We believe the computer security community
must play a role in addressing this threat. To this end, we
outlined ﬁve potential directions for improving protections that
span technical, design, and policy changes to ultimately assist
in identifying, preventing, mitigating, and recovering from hate
and harassment attacks.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
259
ACKNOWLEDGEMENTS
This work was supported in part by the National Sci-
ence Foundation under grants 1704527, 1748903, 1916096,
1916126, 1942610, 2016061, the Simons Foundation, and by
a gift from Google. Any opinions, ﬁndings, and conclusions
or recommendations expressed are those of the authors and do
not necessarily reﬂect the views of the sponsors.
REFERENCES
[1] Sarah A. Aghazadeh, Alison Burns, Jun Chu, Hazel Feigenblatt, Eliz-
abeth Laribee, Lucy Maynard, Amy L. M. Meyers, Jessica L. O’Brien,
and Leah Rufus. GamerGate: A Case Study in Online Harassment,
pages 179–207. Springer International Publishing, Cham, 2018.
[2] Hannah Allam.
‘it gets to you.’ extremism researchers confront the
unseen toll of their work. https://www.npr.org/2019/09/20/762430305/
it- gets- to- you- extremism- researchers- confront- the- unseen- toll- of-
their-work, 2019.
[3] Ross Anderson, Chris Barton, Rainer Boehme, Richard Clayton,
Michel J.G. van Eeten, Michael Levi, Tyler Moore, and Stefan Savage.
Measuring the cost of cybercrime. In Proceedings of the Workshop on
Economics of Information Security, 2012.
[4] Anti-Defamation League. Online hate and harassment: The american
experience. https://www.adl.org/onlineharassment, 2019.
[5] Dennys Antonialli. Drag queen vs. david duke: Whose tweets are more
‘toxic’? https://www.wired.com/story/drag-queens-vs-far-right-toxic-
tweets/, 2019.
[6] Elle Armageddon. When technology takes hostages: The rise of
https://www.vice.com/en us/article/nejmnz/when-
’stalkerware’.
technology-takes-hostages-the-rise-of-stalkerware, 2017.
[7] Zahra Ashktorab and Jessica Vitak. Designing cyberbullying mitigation
and prevention solutions through participatory design with teenagers.
In Proceedings of the 2016 CHI Conference on Human Factors in
Computing Systems, 2016.
[8] BBC News.
India lynchings: Whatsapp sets new rules after mob
killings. https://www.bbc.com/news/world-asia-india-44897714, 2018.
[9] Zack Beauchamp. Isis captured and executed james foley and steven
sotloff, two american journalists. https://www.vox.com/2018/11/20/
17996042/isis-captured-and-executed-james-foley-and-steven-sotloff-
two-american-journalists, 2015.
[10] Carmen Best. Protect yourself from swatting. https://www.seattle.gov/
police/need-help/swatting, 2020.
[11] Jeremy Blackburn and Haewoon Kwak. Stfu noob!: predicting crowd-
sourced decisions on toxic behavior in online games. In Proceedings of
the 23rd international conference on World wide web, pages 877–888.
ACM, 2014.
[12] Lindsay Blackwell, Tianying Chen, Sarita Schoenebeck, and Cliff
In The
Lampe. When online harassment is perceived as justiﬁed.
International AAAI Conference on Web and Social Media, 2018.
[13] Lindsay Blackwell, Jill Dimond, Sarita Schoenebeck, and Cliff Lampe.
Classiﬁcation and its consequences for online harassment: Design
In Proceedings of the ACM on Human-
insights from heartmob.
Computer Interaction, 2017.
[14] Block Together. A web app intended to help cope with harassment
and abuse on twitter. https://blocktogether.org/, 2019.
[15] Sam Blum. Google warns Nest users to update security settings
after uptick of hacked cameras. https://www.popularmechanics.com/
technology/security/a26214078/google-nest-hack-warning/, 2019.
[16] Elizabeth Bodine-Baron, Todd C Helmus, Madeline Magnuson, and
Zev Winkelman. Examining ISIS support and opposition networks on
Twitter. Technical report, RAND Corporation Santa Monica United
States, 2016.
[17] Elena Botella. TikTok admits it suppressed videos by disabled, queer,
and fat creators. https://slate.com/technology/2019/12/tiktok-disabled-
users-videos-suppressed.html, 2019.
[18] Leanne Bowler, Cory Knobel, and Eleanor Mattern. From cyber-
bullying to well-being: A narrative-based participatory approach to
values-oriented design for social media. Journal of the Association
for Information Science and Technology, 2015.
[19] Leanne Bowler, Eleanor Mattern, and Cory Knobel. Developing
design interventions for cyberbullying: A narrative-based participatory
approach. In Proceedings of the iConference, 2014.
[20] Nellie Bowles. Thermostats, locks and lights: Digital tools of domestic
abuse. https://www.nytimes.com/2018/06/23/technology/smart-home-
devices-domestic-abuse.html, 2018.
[21] Jarret M Brachman. High-tech terror: Al-qaeda’s use of new technol-
ogy. Fletcher F. World Aff., 2006.
[22] Les Carpenter. Armed man charged after ’self-investigating’ pizza-
gate conspiracy. https://www.theguardian.com/us-news/2016/dec/05/
washington-pizza-child-sex-ring-fake-news-man-charged, 2016.
[23] Eshwar Chandrasekharan, Umashanthi Pavalanathan, Anirudh Srini-
vasan, Adam Glynn, Jacob Eisenstein, and Eric Gilbert. You can’t
stay here: The efﬁcacy of reddit’s 2015 ban examined through hate
speech. Proceedings of the ACM on Human-Computer Interaction,
(CSCW), 2017.
[24] Eshwar Chandrasekharan, Mattia Samory, Shagun Jhaver, Hunter Char-
vat, Amy Bruckman, Cliff Lampe, Jacob Eisenstein, and Eric Gilbert.
The internet’s hidden rules: An empirical study of reddit norm viola-
tions at micro, meso, and macro scales. Proceedings of the ACM on
Human-Computer Interaction, 2(CSCW):32, 2018.
[25] Eshwar Chandrasekharan, Mattia Samory, Anirudh Srinivasan, and
Eric Gilbert. The bag of communities: identifying abusive behavior
online with preexisting internet data. In Proceedings of the 2017 CHI
Conference on Human Factors in Computing Systems, 2017.
[26] Jonathan Chang and Cristian Danescu-Niculescu-Mizil. Trajectories of
blocked community members: Redemption, recidivism and departure.
In Proceedings of the The World Wide Web Conference, 2019.
[27] Rahul Chatterjee, Periwinkle Doerﬂer, Hadas Orgad, Sam Havron,
Jackeline Palmer, Diana Freed, Karen Levy, Nicola Dell, Damon
McCoy, and Thomas Ristenpart. The spyware used in intimate partner
In 2018 IEEE Symposium on Security and Privacy (SP),
violence.
pages 441–458. IEEE, 2018.
[28] Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano
De Cristofaro, Gianluca Stringhini, and Athena Vakali. Hate is not
binary: Studying abusive behavior of #gamergate on Twitter. In ACM
Hypertext Conference, 2017.
[29] Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano
De Cristofaro, Gianluca Stringhini, and Athena Vakali. Mean birds:
In ACM Web Science
Detecting aggression and bullying on Twitter.
Conference, 2017.
[30] Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano
De Cristofaro, Gianluca Stringhini, and Athena Vakali. Measuring
In International
#GamerGate: A tale of hate, sexism, and bullying.
Conference on World Wide Web Companion, 2017.
[31] Charalampos Chelmis and Mengfan Yao. Minority Report: Cyberbul-
lying Prediction on Instagram. In ACM Web Science Conference, 2019.
[32] Hao Chen, Susan McKeever, and Sarah Jane Delany. The use of deep
learning distributed representations in the identiﬁcation of abusive text.
In AAAI International Conference On Web and Social Media, 2019.
[33] Justin Cheng, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec.
In Eighth Interna-
How community feedback shapes user behavior.
tional AAAI Conference on Weblogs and Social Media, 2014.
[34] Justin Cheng, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec.
In Ninth
Antisocial behavior in online discussion communities.
International AAAI Conference on Web and Social Media, 2015.
[35] Shira Chess and Adrienne Shaw. A conspiracy of ﬁshes, or, how we
learned to stop worrying about #GamerGate and embrace hegemonic
masculinity. Journal of Broadcasting & Electronic Media, 2015.
[36] Danielle Keats Citron. Addressing cyber harassment: An overview of
hate crimes in cyberspace. Journal of Law, Technology & the Internet,
2014.
[37] Courtney Copenhagen and Katie Kim. Homeowner’s blood ’ran
cold’ as smart cameras,
https : / /
www.nbcchicago.com/investigations/My-Blood-Ran-Cold-as-Smart-
Cameras - Thermostat - Hacked - Homeowner - Says - 505113061.html,
2019.
thermostat hacked, he says.
[38] Marco Cova, Christopher Kruegel, and Giovanni Vigna. There is no
free phish: an analysis of “free” and live phishing kits. In Proceedings
of the Workshop on Offensive Technologies, 2008.
[39] Tiago Oliveira Cunha, Ingmar Weber, Hamed Haddadi, and Gisele L
Pappa. The effect of social feedback in a reddit weight loss community.
In Proceedings of the 6th International Conference on Digital Health
Conference, 2016.
[40] Dana Cuomo and Natalie Dolci.
Gender-Based Violence and
Technology-Enabled Coercive Control in Seattle: Challenges & Op-
portunities. https://teccworkinggroup.org/research-2, 2019.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
260
[41] Maral Dadvar, Dolf Trieschnigg, and Franciska de Jong. Experts and
machines against bullies: A hybrid approach to detect cyberbullies. In
Canadian Conference on Artiﬁcial Intelligence, 2014.
[42] Veronica Dagher. Financial abuse in the age of smartphones. https:
//www.wsj.com/articles/ﬁnancial- abuse- in-the-age-of-smartphones-
11575727200?mod=hp lead pos11, 2019.
[43] Data & Society. Nonconsensual image sharing: One in 25 americans
has been a victim of “revenge porn”. https://datasociety.net/pubs/oh/
Nonconsensual Image Sharing 2016.pdf, 2016.
[44] Data & Society. Online harassment, digital abuse, and cyberstalking
in america. https://datasociety.net/output/online-harassment-digital-
abuse-cyberstalking/, 2016.
[45] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber.
Automated hate speech detection and the problem of offensive lan-
guage. In AAAI International Conference On Web and Social Media,
2017.
[46] Matthew Der, Lawrence K. Saul, Stefan Savage, and Geoffrey M.
Voelker. Knock it off: Proﬁling the online storefronts of counterfeit
merchandise. In Proceedings of the SIGKDD International Conference
on Knowledge Discovery and Data Mining, 2014.
[47] Dominic DiFranzo, Samuel Hardman Taylor, Franccesca Kazerooni,
Olivia D Wherry, and Natalya N Bazarova. Upstanding by design:
Bystander intervention in cyberbullying. In Proceedings of the 2018
CHI Conference on Human Factors in Computing Systems, 2018.
[48] Kelly P Dillon and Brad J Bushman. Unresponsive or un-noticed?:
Cyberbystander intervention in an experimental cyberbullying context.
Computers in Human Behavior, 2015.
[49] Karthik Dinakar, Roi Reichart, and Henry Lieberman. Modeling the
detection of textual cyberbullying. In AAAI International Conference
On Web and Social Media, 2011.
[50] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy
Vasserman. Measuring and mitigating unintended bias in text clas-
siﬁcation. In Proceedings of the 2018 AAAI/ACM Conference on AI,
Ethics, and Society, 2018.
[51] Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan
Radosavljevic, and Narayan Bhamidipati. Hate speech detection with
comment embeddings. In The Web Conference, 2015.
[52] Periwinkle Doerﬂer, Kurt Thomas, Maija Marincenko, Juri Ranieri,
Yu Jiang, Angelika Moscicki, and Damon McCoy. Evaluating login
challenges as adefense against account takeover. In Proceedings of the
The World Wide Web Conference, 2019.
[53] David M. Douglas. Doxing: a conceptual analysis.
Ethics and
Information Technology, 18(3):199–210, Sep 2016.
[54] Yelena Dzhanova and Dan Mangan. Rep. katie hill’s husband claimed
his computer was ‘hacked’ before her private photos appeared online,
report says.
https://www.cnbc.com/2019/10/31/rep- katies- hills-
husband-claimed-his-computer-was-hacked.html, 2019.
[55] Charlie Edwards and Luke Gribbon. Pathways to violent extremism in
the digital era. The RUSI Journal, 2013.
[56] Mai ElSherief, Vivek Kulkarni, Dana Nguyen, William Yang Wang,
and Elizabeth Belding. Hate lingo: A target-based linguistic analysis
of hate speech in social media. In The International AAAI Conference
on Web and Social Media, 2018.
[57] Mai ElSherief, Shirin Nilizadeh, Dana Nguyen, Giovanni Vigna, and
Elizabeth Belding. Peer to peer hate: Hate speech instigators and their
targets. In AAAI International Conference On Web and Social Media,
2018.
[58] Facebook.
Community standards enforcement
https : / /
transparency.facebook.com/community-standards-enforcement, 2019.
[59] Brown Farinholt, Mohammad Rezaeirad, Paul Pearce, Hitesh Dhar-
mdasani, Haikuo Yin, Stevens Le Blond, Damon McCoy, and Kirill
Levchenko. To catch a ratter: Monitoring the behavior of amateur
In Proceedings of the IEEE
Darkcomet rat operators in the wild.
Symposium on Security and Privacy, 2017.
report.
[60] Joel Finkelstein, Savvas Zannettou, Barry Bradlyn, and Jeremy Black-
burn. A quantitative approach to understanding online antisemitism. In
Proceedings of the AAAI International Conference on Web and Social
Media, 2020.
[61] Claudia I Flores-Saviaga, Brian C Keegan, and Saiph Savage. Mo-
bilizing the trump train: Understanding collective action in a political
trolling community. In Twelfth International AAAI Conference on Web
and Social Media, 2018.
[62] Antigoni-Maria Founta, Constantinos Djouvas, Despoina Chatzakou,
Ilias Leontiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali,
Michael Sirivianos, and Nicolas Kourtellis. Large Scale Crowdsourcing
In AAAI Interna-
and Characterization of Twitter Abusive Behavior.
tional Conference On Web and Social Media, 2018.
[63] Cynthia Fraser, Erica Olsen, Kaofeng Lee, Cindy Southworth, and
Sarah Tucker. The new age of stalking: Technological implications
for stalking. Juvenile and Family Court Journal, 61:39 – 55, 11 2010.
[64] Diana Freed, Sam Havron, Emily Tseng, Andrea Gallardo, Rahul
Chatterjee, Thomas Ristenpart, and Nicola Dell. “is my phone hacked?”
analyzing clinical computer security interventions with survivors of in-
timate partner violence. Proc. ACM Hum.-Comput. Interact., 3(CSCW),
November 2019.
[65] Diana Freed, Jackeline Palmer, Diana Minchala, Karen Levy, Thomas
Ristenpart, and Nicola Dell. Digital technologies and intimate partner
violence: A qualitative analysis with multiple stakeholders. PACM:
Human-Computer Interaction: Computer-Supported Cooperative Work
and Social Computing (CSCW), Vol. 1(No. 2):Article 46, 2017.
[66] Diana Freed, Jackeline Palmer, Diana Minchala, Karen Levy, Thomas
“A Stalker’s Paradise”: How intimate
In Proceedings of the 2018 CHI
Ristenpart, and Nicola Dell.
partner abusers exploit technology.
Conference on Human Factors in Computing Systems. ACM, 2018.
[67] Jennifer Golbeck, Zahra Ashktorab, Rashad O Banjo, Alexandra
Berlinger, Siddharth Bhagwan, Cody Buntain, Paul Cheakalos, Alicia A
Geller, Quint Gergory, Rajesh Kumar Gnanasekaran, et al. A large
labeled corpus for online harassment research. In Proceedings of the
2017 ACM on Web Science Conference, 2017.
[68] Google. Update on the Global Internet Forum to Counter Terrorism.
https://www.blog.google/around- the- globe/google- europe/update-
global-internet-forum-counter-terrorism/, 2017.
[69] Andy Greenberg. Spoofed Grindr Accounts Turned One Man’s Life
Into a ’Living Hell’. https://www.wired.com/2017/01/grinder-lawsuit-
spoofed-accounts/, 2017.
[70] Sara Harrison. Twitter’s disinformation data dumps are helpful—to
a point. https://www.wired.com/story/twitters-disinformation-data-
dumps-helpful/, 2019.
[71] Hatebase. The world’s largest structured repository of regionalized,
multilingual hate speech. https://hatebase.org/, 2019.
[72] Sam Havron, Diana Freed, Rahul Chatterjee, Damon McCoy, Nicola
Dell, and Thomas Ristenpart. Clinical computer security for victims
of intimate partner violence. In Proceedings of the USENIX Security
Symposium, 2019.
[73] Alex Hern. Revealed: catastrophic effects of working as a Face-
book moderator.
https : / /
www.theguardian.com/technology/2019/sep/17/revealed-catastrophic-
effects-working-facebook-moderator.
The Guardian, September 2019.
[74] Donald Hicks and David Gasca. A healthier twitter: Progress and more
https://blog.twitter.com/en us/topics/company/2019/health-
to do.
update.html, 2019.
[75] Gabriel Emile Hine, Jeremiah Onaolapo, Emiliano De Cristofaro, Nico-
las Kourtellis, Ilias Leontiadis, Riginos Samaras, Gianluca Stringhini,
and Jeremy Blackburn. Kek, Cucks, and God Emperor Trump:
A Measurement Study of 4chan’s Politically Incorrect Forum and
In Proceedings of the AAAI International
Its Effects on the Web.
Conference On Web and Social Media, 2017.
[76] Thorsten Holz, Markus Engelberth, and Felix Freiling. Learning more
about
the underground economy: A case-study of keyloggers and
dropzones. In Proceedings of the European Symposium on Research
in Computer Security, 2009.
[77] Homa Hosseinmardi, Sabrina Arredondo Mattson, Rahat Ibn Raﬁq,
Richard Han, Qin Lv, and Shivakant Mishra. Detection of cyberbullying
In AAAI International
incidents on the instagram social network.
Conference On Web and Social Media, 2015.
[78] Caroline Jack. Lexicon of lies: Terms for problematic information.
https://datasociety.net/output/lexicon-of-lies/, 2017.
[79] Charlotte Jee. Germany’s synagogue shooting was live-streamed on
Twitch–but almost no one saw it. https://www.technologyreview.com/s/
614529/germanys-synagogue-shooting-was-live-streamed-on-twitch-
but-almost-no-one-saw-it/, 2019.
[80] Adrianne Jeffries. Meet ‘swatting,’ the dangerous prank that could
get someone killed. https://www.theverge.com/2013/4/23/4253014/
swatting-911-prank-wont-stop-hackers-celebrities, 2013.
[81] Sarah Jeong. The history of Twitter’s rules. https://www.vice.com/en
us/article/z43xw3/the-history-of-twitters-rules, 2016.