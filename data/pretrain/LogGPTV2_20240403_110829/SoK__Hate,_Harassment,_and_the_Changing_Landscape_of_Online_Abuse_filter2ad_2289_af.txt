### Additional Approval or Multiple Days for Completion

To enable detection and remediation by targets, additional approval steps or extended processing times may be necessary. These strategies highlight the need for both platforms and the security community to reassess their existing threat models, balancing utility with safety. For new features or platforms, threat modeling for potential abuse scenarios is as crucial as modeling potential security risks.

---

### Policies, Education, and Awareness

In addition to technical and design solutions, addressing hate and harassment requires investing in better social structures, including policies, educational resources, training, and support infrastructure [40], [64], [72], [111]. In 2016, Pater et al. found that 13 out of 15 prominent social networks prohibited hate and harassment but did not provide clear definitions [116]. Instead, these platforms listed potential types of abusive activities such as attacks, bullying, defamation, harm, hate, impersonation, racism, stalking, and threats [116]. The lack of well-crafted policies or definitions can demoralize targets of hate and harassment [13].

---

### Tensions and Challenges

When expanding threat models and technical enforcement to address hate and harassment, significant tensions arise around balancing competing social equities. Additionally, challenges remain for safely conducting research in this space.

#### A. Tensions Balancing Social Equities

**Empowering vs. Burdening Targets:**
Strategies like nudges, indicators, and moderation can empower users to control their online environment. However, they also place much of the burden of staying safe on the targets, which might exacerbate emotional tolls. Although some studies have explored outsourcing this burden to family and peers [103], platforms must balance between "paternalism" and overloading targets with safety decisions while working to place burdens on attackers, not targets.

**Moderation vs. Filter Bubbles and Free Speech:**
Providing users, communities, and platforms with technical mechanisms to filter abusive content carries the risk that moderation could turn into a form of censorship—intentionally or unintentionally. For example, a prominent social network suppressed the reach of content created by users assumed to be "vulnerable to cyberbullying," including "disabled, queer, and fat creators" [17]. Even with well-defined policies, moderators may exceed their roles and suppress content they dislike [148].

**Manual Review vs. Well-Being:**
Given the subjective nature of hate and harassment, manual review remains essential for accurate decisions. However, this can place significant emotional burdens on reviewers, leading to mental health issues without proper safeguards [73], [112].

**Restricting Content vs. Research Access:**
As platforms face pressure to remove or restrict hate and harassment promptly, researchers face the added challenge of identifying or retroactively studying attacks. This challenge is similar to the current issue of transparency around disinformation campaigns and preventing further damage from attacks [70].

**Privacy vs. Accountability:**
Increasing user expectations around privacy add new challenges to combating abuse. Privacy-preserving technologies like end-to-end encryption provide capabilities such as secrecy, deniability, and untraceability. Secrecy precludes platforms from performing content-based analysis and filtering. Deniability complicates the collection and provision of evidence by targets. Untraceability masks the source of hate and harassment, especially in viral distribution. Research has explored providing weaker—but still meaningful—privacy guarantees and anonymous tools for reporting abuse [96], [121], [139].

#### B. Challenges for Researchers

**Researcher Safety and Ethics:**
Currently, there are no best practices for how researchers can safely and ethically study online hate and harassment. Risks include becoming a target of coordinated, hostile groups and emotional harm from reviewing toxic content, similar to risks faced by manual reviewers [2]. Researchers must ensure they respect at-risk subjects and do not further endanger targets.

**Risks of Greater Harm:**
As platforms prevent and mitigate hate and harassment, there is a risk that the subsequent arms race escalates the severity of attacks. Attackers may migrate to private, virulent communities that glorify hate and harassment. Other risks include attackers resorting to physical or sexual violence against targets [63], [65], [66].

**Defining Success:**
When expanding threat models to include hate and harassment, it is natural to wonder what success would look like. Progress can be measured quantitatively, such as by the volume of people exposed to abusive messages, or qualitatively, such as a decrease in people reporting negative online experiences. Other factors underpin these metrics, including the overhead and friction of solutions on platforms and users, the cost of maintaining defenses in an adversarial setting, and balancing false positives (e.g., incorrectly penalizing legitimate people) against false negatives (e.g., people exposed to hate and harassment).

---

### Conclusion

In this work, we argued that security, privacy, and anti-abuse protections are failing to address the growing threat of online hate and harassment. We proposed a taxonomy, built from over 150 research articles, to reason about these new threats. We also provided longitudinal evidence that hate and harassment have grown 4% over the last three years and now affect 48% of people globally. Young adults, LGBTQ+ individuals, and frequent social media users remain the communities most at risk of attack. We believe the computer security community must play a role in addressing this threat. To this end, we outlined five potential directions for improving protections that span technical, design, and policy changes to ultimately assist in identifying, preventing, mitigating, and recovering from hate and harassment attacks.

---

### Acknowledgements

This work was supported in part by the National Science Foundation under grants 1704527, 1748903, 1916096, 1916126, 1942610, 2016061, the Simons Foundation, and by a gift from Google. Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.

---

### References

[References are listed as provided in the original text.]

---

This optimized version aims to enhance clarity, coherence, and professionalism while maintaining the original content's integrity.