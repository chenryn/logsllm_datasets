structure and its implementation on top of OpenDHT in de-
tail in Section 4.
3.4 Deployability, Robustness and Availability
OpenDHT provides the routing, storage, and robustness
substrate for Place Lab. Individual mapping servers connect
1Details are described in [21].
directly to the DHT. They rely on the DHT to provide much
of the robustness and availability. The servers store the cur-
rent estimates of each radio beacon’s location in the DHT,
while the DHT handles replication and recovery. If a map-
ping server fails, the DHT routing mechanisms automatically
ensure that the failed server’s successor in the routing overlay
takes over responsibility for the failed server’s key space. The
mapping server’s administrator still must handle restarting
of the failed server, but the DHT provides automatic graceful
fail-over in the meanwhile.
Mapping servers periodically refresh their data in the DHT.
This ensures that even in the event of catastrophic failure of
the DHT where all replicas of a beacon’s data are lost, the
mapping servers will eventually recover them. Additionally,
a temporary loss of data does not a(cid:11)ect application perfor-
mance. This resilience is due to the temporal and spatial
redundancy in the data. The e(cid:11)ect of lost information for
a beacon is reduced by the likelihood that a new war driver
will submit fresh information for the beacon eventually. Spa-
tially, the impact of lost beacons is reduced by readings for
other nearby beacons that map to di(cid:11)erent servers. As we
will show in section 5.6, a loss of even 50% of the beacon data
results in no noticeable reduction in positioning accuracy.
4. PREFIX HASH TREES
We now look at the PHT data structure in detail. Unlike
the various recent proposals for incorporating range query
support into DHTs [8, 15, 19], Pre(cid:12)x Hash Trees are built
entirely on top of a simple put/get/remove interface, and thus
run over any DHT, and speci(cid:12)cally on a third-party DHT ser-
vice like OpenDHT. Range queries use only the get(key) op-
eration and do not assume knowledge of nor require changes
to the DHT topology or routing behavior.
PHTs are e(cid:14)cient in that updates are doubly logarith-
mic in the size of the domain being indexed. They are self-
organizing and load-balanced. They tolerate failures well;
while they cannot by themselves protect against data loss
when nodes go down, the failure of any given node in the Pre-
(cid:12)x Hash Tree does not a(cid:11)ect the availability of data stored
at other nodes. Moreover, PHTs can take advantage of any
replication or other data-preserving technique employed by
the DHT.
4.1 The Data Structure
A Pre(cid:12)x Hash Tree assumes that keys in the data domain
can be expressed as binary strings of length D. It is fairly
straightforward to extend this to other alphabets through
multiway indexing, or by encoding them in binary. A PHT is
essentially a binary trie in which every node corresponds to a
distinct pre(cid:12)x of the data domain being indexed. Each node
of the trie is labeled with a pre(cid:12)x that is de(cid:12)ned recursively:
given a node with label L, its left and right child nodes are
labeled L0 and L1 respectively. The root is labeled with the
attribute being indexed, and downstream nodes are labeled
as described above. Each node in the PHT has either zero
or two children. Keys are stored only at leaf nodes. Unlike a
binary search tree, all keys that are stored in the same leaf
node share the leaf node’s label as a common pre(cid:12)x.
The PHT imposes a limit B on the number of keys that
a single leaf node can store. When a leaf node (cid:12)lls to ca-
pacity, it must split into two descendants. Similarly, if keys
are deleted from the PHT, two sibling leaf nodes may merge
into a single parent node. As a result, the shape of the PHT
depends on the distribution of keys; it is \deep" in regions
…
7
6
5
4
3
2
1
0
t
i
g
n
o
e
d
u
l
(5,6)
(2,4)
0
1
2 3
4
latitude
5
6
7
…
Figure 1: Recursive shape of a z-curve linearization to
map a two-dimensional coordinate space into a one-
dimensional sequence. The shaded region represents
a two-dimensional range query for data points in the
space (2,4){(5,6). The bold line represents the cor-
responding one-dimensional range in the z-curve be-
tween the lowest and highest linearization points of
the original query.
of the domain that are densely populated, and conversely,
\shallow" in regions of the domain that are sparsely popu-
lated.
As described this far, the PHT structure is a fairly routine
binary trie. What makes the PHT interesting lies in how
this logical trie is distributed among the servers that form
the underlying DHT. This is achieved by hashing the pre(cid:12)x
labels of PHT nodes over the DHT identi(cid:12)er space. A node
with label L is thus assigned to the DHT server to which
L is mapped by the DHT hashing algorithm. This hash-
based assignment implies that given a PHT node with label
L, it is possible to locate it in the DHT via a single get().
This \direct access" property is unlike the successive link
traversal associated with typical tree-based data structures
and results in the PHT having several desirable features that
are discussed later in Section 4.7.
4.2 Adapting PHTs for Place Lab
Queries in Place Lab are performed over a two-dimensional
latitude-longitude coordinate domain ((cid:0)180:0 < longitude <
180:0, (cid:0)90:0 < latitude < 90:0). To index this domain us-
ing PHTs, we rely on a technique known as linearization
or space-(cid:12)lling curves to map multi-dimensional data into a
single dimension. Well-known examples include the Hilbert,
Gray code, and Z-order curves [18]. First, we normalize all
latitudes and longitudes into unsigned 40-bit integer values,
which in turn can be represented using a simple binary for-
mat. We then use the z-curve linearization technique to
map each two-dimensional data point into an 80-bit one-
dimensional key space. Z-curve linearization is performed
by interleaving the bits of the binary representation of the
latitude and longitude. For example, the normalized point
(2,3) would be represented on the z-curve using the 80-bit key
000...001101. Figure 1 shows the zig-zag shape that such a z-
curve mapping takes across the two-dimensional coordinate
space. We chose z-curves because they are simple to under-
stand and easy to implement. In Section 5.4, we will compare
the performance of the various linearization techniques.
The PHT for two-dimensional queries uses z-curve keys
and their pre(cid:12)xes as node labels. Due to the interleaving of
the latitude and longitude bits in a z-curve key, each suc-
cessive level in the PHT represents a splitting of the geo-
P(=R000…00)
P0
P01
P10
P1
P11
P010
P011 P100
P110
P111
P00
(1,0)
P0100
(0,4)
(1,5)
P0101 P0110 P0111
(0,7)
(3,6)
(1,6)
(3,7)
(1,7)
(2,4)
(2,5)
(3,5)
PHT node 
(Lat,lon)
label
P00
P0100
P0101
(1,0)
(0,4)
(1,5)
(0,7)
(1,6)
(1,7)
P101
P1100
P1101
(Lat,lon)
binary
Z-curve 
key
(001,000)
000010
(000,100)
(001,101)
(000,111)
(001,110)
(001,111)
010000
010011
010101
010110
010111
Figure 2: A portion of a sample two-dimensional
PHT for Place Lab. The table shows the data items
(and their z-curve keys) that are stored at some of the
leaf nodes in the PHT. As shown, each data item is
stored at the unique leaf node whose label is a pre(cid:12)x
of the item’s z-curve key.
graphic space into two, alternately along the latitude axis
and then along the longitude axis. Data items (tuples of
the form flatitude, longitude, beacon-idg) are inserted into
the leaf node whose label is a pre(cid:12)x of the z-curve key as-
sociated with that latitude-longitude coordinate. Figure 2
shows a sample PHT along with an example assignment of
data items to PHT leaf nodes assuming three-bit normalized
latitude and longitude values.
4.3 PHT Operations
Now that we have described what the PHT data structure
looks like, let us focus on the various operations needed to
build and query this data structure using a DHT.
4.3.1 Lookup
Lookup is the primitive used to implement the other PHT
operations. Given a key K, it returns the unique leaf node
leaf(K) whose label is a pre(cid:12)x of K. A lookup can be im-
plemented e(cid:14)ciently by performing a binary search over the
D+1 possible pre(cid:12)xes corresponding to a D-bit key. An im-
portant feature of this lookup is that unlike traditional tree
lookups, it does not require each operation to originate at
the root, thereby reducing the load on the root (as well as
nodes close to the root). Minor modi(cid:12)cations to this algo-
rithm can be used to perform a lookup of a pre(cid:12)x P instead
of a full-length key K.
Binary search requires blog (D + 1)c+1 (cid:25) log D DHT gets,
which is doubly logarithmic in the size of the data domain
being indexed. This ensures that the lookup operation is
extremely e(cid:14)cient. However, binary search has the drawback
that it can fail as a result of the failure of an internal PHT
node. The search may not be able to distinguish between
a failed internal node, in which case search should proceed
downwards, and the absence of a PHT node, in which case
the search should proceed upwards. In such a situation, the
PHT client can either restart the binary search in the hope
that a refresh operation has repaired the data structure (see
section 4.4), or perform parallel gets of all pre(cid:12)xes of the key
K. The parallel search is guaranteed to succeed as long as
the leaf node is alive and the DHT is able to route to it.
This suggests two alternative modes of operation, namely,
low-overhead lookups using binary search, and low-latency
fail-over lookups using parallel search.
4.3.2 Range Query
For a one-dimensional PHT, given two keys L and H (L (cid:20)
H), a range query returns all keys K contained in the PHT
satisfying L (cid:20) K (cid:20) H. Such a range query can be performed
by locating the PHT node corresponding to the longest com-
mon pre(cid:12)x of L and H and then performing a parallel traver-
sal of its subtree to retrieve all the desired items.
Multi-dimensional range queries such as those required for
Place Lab are slightly more complicated. A query for all
matching data within a rectangular region de(cid:12)ned by (lat-
Min, lonMin) and (latMax, lonMax) is performed as follows.
We determine the linearized pre(cid:12)x that minimally encom-
passes the entire query region. This is done by computing
the z-curve keys zMin and zMax for the two end-points of the
query, and the longest common pre(cid:12)x of these keys: zPre(cid:12)x.
We then look up the PHT node corresponding to zPre(cid:12)x and
perform a parallel traversal of its sub-tree.
Unlike the simpler case of one-dimensional queries, not all
nodes between the leaf for the minimum key and the leaf
for the maximum key contribute to the query result. This
is illustrated in Figures 1 and 2 which show a query for
the rectangular region (2,4){(5,6). As shown in Figure 1,
the linearized range between these two points (shown by
the bold line) passes through points (and correspondingly
PHT nodes) that are not within the rectangular region of
the search. This is also depicted in the PHT representation
in Figure 2. The leaves for the end-points of the query are
P0110 and P1101. However, the entire subtree rooted at P10
does not contain any data items that fall within the query
range.2
Hence, the query algorithm works as follows: Starting
at the PHT node corresponding to zPre(cid:12)x, we determine
whether this node is a leaf node. If so, we apply the range
query to all items within the node and report the result. If
the node is an interior node, we evaluate whether its left
subtree (with a pre(cid:12)x of zPre(cid:12)x+\0") can contribute any
results to the query. This is done by determining whether
there is any overlap in the rectangular region de(cid:12)ned by the
subtree’s pre(cid:12)x and the range of the original query. This
check can be performed with no additional gets, so incurs
almost no penalty if it fails. If an overlap exists, the query is
propagated recursively down the left subtree. In parallel, we
perform a similar test for the right subtree (with a pre(cid:12)x of
zPre(cid:12)x+\1") and if the test succeeds, propagate the query
down that sub-tree as well. Thus the query algorithm re-
quires no more than d sequential steps, where d is the depth
of the tree.
4.3.3 Insert/Delete
Insertion and deletion of a key K require a PHT lookup
to (cid:12)rst locate the leaf node leaf(K). During insertion, if the
leaf node is already full to its limit of B values, it must be
2P10 contains items whose latitude coordinates are of the
form 000...1XX and longitudes are of the form 000...0XX,
that is, items in the range (4,0){(7,3). This range does not
overlap the query range and hence the entire subtree can be
discounted.
split into two children. In most cases, the (B+1) keys are
distributed among the two children such that each of them
stores at most B. However it is possible that all (B+1) keys
will be distributed to the same child, thus necessitating a
further split. To avoid this, the split operation determines
the longest common pre(cid:12)x of all of the (B+1) keys and cre-
ates two new leaf nodes one level deeper than that common
pre(cid:12)x, thereby ensuring that neither of the new leaves has
more than B keys. The keys are distributed across these two
new leaves and all nodes in between the original node being
split and the new leaves are marked as interior nodes. All of
these operations can be parallelized for e(cid:14)ciency.
Similarly, when a key is deleted from the PHT, it may be
possible to coalesce two sibling leaf nodes into a single parent
node. The merge operation is essentially the reverse of splits
and can be performed lazily in the background.
4.4 Refreshing and recovering from failure
PHTs inherit all of the resilience and failure recovery prop-
erties of the underlying DHT. However, in the event of catas-
trophic failure of all replicas in the DHT, the PHT can lose
data. Although our algorithms are fairly resilient even in
the face of loss of interior PHT nodes, one must eventually
restore the lost data. To achieve this, we rely on soft state
updates. Each PHT entry (leaf node keys and interior node
markers) has associated with it a time-to-live (TTL). When
the TTL expires, the entry is automatically deleted from the
DHT.
Each mapping server periodically refreshes the values that
it has inserted into the PHT. All keys are inserted into the
DHT with a TTL of T seconds. Every T=2 seconds, a map-
ping server refreshes its keys by resetting their TTL to T.
At the same time, it checks the parent of the leaf node. If
the parent’s TTL has dropped to less than T=2 seconds, it
refreshes the parent as well. This continues recursively until
it reaches the root or a parent whose TTL is greater than
T=2. Thus, interior nodes are refreshed only as needed. If
an interior node is lost due to failure, it will eventually be
refreshed as a consequence of the refresh of a value in one of
its descendant leaf nodes.
4.5 Dealing with concurrency
The PHT as described above has potential race conditions
that can result in (temporary) loss of data as well as du-
plication of work. For example, if two mapping servers at-
tempt to insert keys K1 and K2 into the same leaf node, and
that leaf node is full, both servers will attempt to split the
leaf node resulting in duplicate work. A worse race condi-
tion can cause one server’s insert operation to get lost while
a di(cid:11)erent server has begun the process of splitting a leaf
node. This however is a temporary problem since the refresh
mechanisms described in the previous section will eventually
recover the lost data.
These ine(cid:14)ciencies occur because the PHT is implemented
entirely outside the DHT by the independent mapping servers.
In the absence of concurrency primitives in the DHT, they
cannot be eliminated. Hence, we added a localized atomic
test-and-set mechanism to the OpenDHT API. Note that this
extension is not PHT- or Place Lab-speci(cid:12)c and can poten-
tially bene(cid:12)t many distributed applications. The test-and-