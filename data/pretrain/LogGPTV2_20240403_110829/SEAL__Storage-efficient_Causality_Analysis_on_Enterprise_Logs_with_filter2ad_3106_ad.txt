Our evaluation focuses on the following aspects. In
Section 5.2, we study the data compression ratio and the
number of reduced events for different hosts and differ-
ent accessright operations (read, write, and execute). We
demonstrate the impact of the assigned chunk size (for
caching events) on the reduction factor. We compare our
method to relaxed FD on compression rate. In Section 5.3, we
compare the processing time of running back-tracking queries
on the compressed and uncompressed databases. For the com-
pressed case, the time for the database to return the potential
merged events and the time for SEAL to post-process them are
investigated. We show the accuracy advantages of lossless
compression under queries with time constraints. Finally in
Appendix B, we evaluate the accuracy of the average degree
estimator and compare it with direct uniform sampling.
5.2 Compression Evaluation
Compression ratio. We measure the compression ratio as
the original data system over the compressed data system
using the above two chunk sizes. For DSind, when the chunk
size (number of cached events) is 106, the compressed data is
reduced to 7.6 GB from 20 GB, resulting in a compression
ratio of 2.63x. For DSt pc, the chunk size equals one ﬁle size
and contains around 5× 106 events. The compressed size is
18 GB reduced from 233GB, resulting in a compression ratio
of 12.94x.
Reduction factor for different operations and hosts. To
further understand the compression results, we investigate
the reduction factor, deﬁned as the number of original events
divided by the number of compressed events. We focus on
DSind, and some examples of the hosts and the average reduc-
tion factors from 95 hosts are illustrated in Table 3. In the
table, we list results for the chunk size of 106 as well as 105.
It can be observed that the types of events (read, write, and
execute) differ by the hosts. We observed that in DSind, read
Figure 6: The cumulative distribution of the reduction factors
for the 95 hosts in DSind. The reduction factor is calculated
for all three types of operations, read, write, and execute, and
the overall events in each host.
is the most popular operation among most of the hosts, where
72 hosts have more than 50% read events. Write is much less
prevalent in general, where 67 hosts have between 10% to
30% write events. Finally, execution varies by the host, and
69 hosts have between 10% to 60% executions.
On average, the reduction factor of execute events is higher
than reads, and writes have the lowest reduction factor, as
can be seen from the last row of Table 3. However, for each
host, this ordering changes depending on the structure of the
dependency graph, e.g., if there exist many repeated events
between two nodes. Host 5 is an example that has reduction
factors similar to the average case. Hosts 23, 52, 3 see higher
reduction factors of read, write, and execute events, respec-
tively. Host 94 is an example of high reduction factors for all
events. In Figure 6 we plot the cumulative distribution of the
reduction factors among the 95 hosts.
The number of events of a host affects the reduction factor
to some extent. In particular, if the number of events is less
than the chunk size, as occurred for a few hosts when the
chunk size is 106, the cache is not fully utilized, and fewer
merge patterns may be found. However, some hosts with a
small number of events still outperform the overall case as
the last row in Table 3, due to their high average degree.
Previous works like NodeMerge focus on one type of oper-
ation, such as read [77], and show a high data reduction ratio
on their dataset. Our result suggests such an approach is not
always effective, when compressing data from different types
of machines (e.g., Host 94). As such, SEAL is more versatile
to different enterprise settings.
Chunk size. When the chunk size is increased from 105 to
106, the overall reduction factor is increased by 1.7 as in the
last row of Table 3. Correspondingly, the consumed memory
size is increased from 134 MB to 866 MB. The cumulative
distribution of the reduction improvement, which is the re-
duction factor of chunk size 106 divided by that of chunk
size 105, is plotted in Figure 7. The improvement is due to
the fact that when more events are considered in one chunk,
2996    30th USENIX Security Symposium
USENIX Association
0%20%40%60%80%100%1101001000cumulative percentagereduction factorTotalReadWriteExecuteHost ID Event Count / Reduction Read % / Reduction Write /Reduction
11% / 9.2x / 8.4x
8% / 5.3x / 4.5x
Execute / Reduction
28% / 65.3x / 33.0x
1% / 35.8x / 13.2x
8% / 36.6x / 14.7x
34%/ 125.8x / 52.3x
72% / 346.0x / 209.1x
15% / 76.3x / 38.7x
22% / 54.9x / 47.5x
29% / 8.8x / 8.4x
8% / 200.6x / 96.3x
19% / 5.3x / 3.7x
5
23
52
3
94
All
278913 / 9.25x / 5.85x
880162 / 25.45x / 19.14x
523671 / 41.45x / 17.36x
312392 / 15.37x / 13.31x
517978 / 78.82x / 26.9x
53172439 / 9.81x / 5.71x
61% / 6.6x / 4.1x
91% / 37.7x / 26.9x
70% / 39.1x / 14.8x
36% / 12.6x / 10.8x
20% / 19.8x / 6.1x
65% / 10.3x / 5.5x
Table 3: Example hosts and the reduction factors. The reduction factors are measured for two chunk sizes: 106 and 105. The last
row shows the overall result for the 95 hosts.
Figure 7: The cumulative distribution of the improvement
over the 95 hosts when the chunk size is increased from 105
to 106. The improvement for Read, write, execute, and overall
events in each host is calculated.
more edges exist in the dependency graph, but the number of
nodes does not increase as fast. A larger average degree and
hence a larger reduction factor is achieved. It can be seen that
the execute events change the most with a larger chunk size,
while the write events change the least with the chunk size.
This also is consistent with the fact that executions have more
repeated edges between processes while write events operate
on different ﬁles over time.
Comparison to FD. We use DSdtc to compare SEAL and FD,
as the DARPA data is also used by Hossain et al. [37]. Figure
8 shows the compression ratio of four methods: 1) “opti-
mal” – keeping only one random edge between any pair of
nodes, which violates causality dependency but gives an up-
per bound on the highest possible compression ratio when
repeated edges are reduced, 2) “FD” – removing repeated
edges under relaxed full dependency preservation, 3) “SEAL
repeat edge” – our method that only compresses all repeated
edges, and 4) “SEAL” – our method that compresses all in-
coming edges of any node.
Figure 8 shows that if we only compress the repeated edges
by SEAL, we can get almost the same compression ratio
as FD. Both methods are close to the minimum possible
compressed size under repeated edge compression. Besides,
if we compress all the possible edges using SEAL, we get
a compression ratio of 12.94x compared to 8.96x for FD
preservation.
Figure 8: Comparison between our methods and FD.
5.3 Query Evaluation
We measured the querying and decoding time cost of SEAL
as well as the querying time of the uncompressed data. We
use a dataset with 830,235 events under DSind and run back-
tracking through breadth-ﬁrst search (BFS) to perform the
causality analysis for every node. We use BFS here as it
can be seen as a generalization of causality analysis: if no
additional constraints are assumed, causality analysis is BFS
under causality dependency. In particular, starting from any
POI node x, we query for all incoming edges e1,e2, . . . ,ed
and the corresponding parent nodes y1,y2, . . . ,yd, where d is
the incoming degree of x. Then for each node yi, 1 ≤ i ≤ d,
we query for its incoming events whose starttime is earlier
than that of ei. The process continues until no more incoming
edge is found.
Figure 9 shows the performance of this evaluation. The
querying and the decoding time on the compressed data nor-
malized by the querying time on the uncompressed data are
plotted. We obtain 133 start nodes each of which returns
more than 2,000 querying results. We observe that 89% starts
nodes (118 out of 133) use less time than the uncompressed
data, and 30 start nodes use less than half the time of the un-
compressed data. Moreover, on average decompression only
takes 18.66% of the overall time, because only potentially
valid answers are decompressed. It is also observed that the
USENIX Association
30th USENIX Security Symposium    2997
0%20%40%60%80%100%1.02.03.04.05.0cumulative percentageimprovementTotalReadWriteExecute020406080100120140Chunk Index51015202530354045Compression RatiooptimalFDSEAL repeat edgeSEALNID
1
2
3
4
Number of Reachable Nodes/Edges
Uncmp
SEAL
1093/4302
1093/4302
9496/37944 9496/37944 1457/5999 1457/5999
178/616
45/3739
178/616
45/3739
116/358
11/2113
Cnstrnd
Uncmp
293/779
Cnstrnd
SEAL
293/779
116/358
11/2113
Table 4: The results of back-tracking starting from 4 nodes.
“NID”, “Uncmp” and “Cnstrnd” are short for “Node ID”, “Un-
compressed” and “Constrained”.
querying time of SEAL is only 63.87% of the querying time
for uncompressed data. For DSdtc, SEAL runs on about 5.27M
nodes, 15.47% nodes use less time than the uncompressed
data, and on average takes 1.36x time of the uncompressed
data.
Note that queries usually have a restrictive latency require-
ment while compression of collected logs can be performed at
the background of a minoring server. Our method tradeoff the
computation during compression for better storage efﬁciency
and query speed.
Evaluation of attack provenance. Here we use the simu-
lated attacks of DSdtc to evaluate whether SEAL preserves
the accuracy for data provenance. We use four processes on
two hosts (two for each) which are labeled as attack targets
(ta1-cadets-2 and ta1-cadets-1) as the starting nodes.
Then we run the BFS queries, and count 1) the number of
nodes reachable from a starting node (reachable is deﬁned
in Section 5.1) and 2) the number of edges from a starting
node to all its reachable nodes. Table 4 (Columns 2 and 3)
shows the number of reachable nodes and edges in the BFS
graph. It turns out SEAL returns the exact same number of
reachable nodes and edges as the uncompressed data, indicat-
ing it preserves provenance accuracy. Next, we demonstrate
the versatility of our lossless method for queries with time
constraints, for example, when the analyst knows that the at-
tack occurred in an approximate time period [t1,t2]. Since
our lossless compression can restore all the time information,
we can add arbitrary constraints to our analysis without any
concerns, which is veriﬁed by the last two columns (Column
4 and 5) of Table 4. Lossy reduction methods, such as FD,
even though preserve certain dependency, still lose time in-
formation once edges are removed, and thus might introduce
false connectivity under time constraints (see Figure 2 for an
example).
6 Discussion
Limitations and future works. DSind is collected for a small
number of days and a subset of all hosts from our industrial
partner. Therefore, a larger dataset may provide a more com-
prehensive understanding of the performance for SEAL. The
Figure 9: Querying and decompression time of back tracking
with 133 start nodes that return the largest result sizes, nor-
malized by the querying time of the uncompressed data. The
nodes index are sorted by the query time.
compression ratio can be further improved through two possi-
ble methods. First, the proposed algorithms reduce the number
of events, but the properties of all merged events are loss-
lessly compressed together. Even though such compression
produces a hundred percent accuracy for log analytics and the
merge patterns can be easily found, dependency-preserving
timestamp lossy compression may improve the storage size.
Second, domain-speciﬁc knowledge can be explored such
as removing temporary ﬁles [45]. Another limitation is the
memory overhead to store the node map as in Table 2, which
is the only extra data other than the events. Our experiment
results show that the node map takes 114 MB on disk, but
consumes 1.4 GB when loaded into memory. The memory
cost can be reduced by replacing generic hash maps of Java
with user-deﬁned ones.
Potential attacks. When the adversary compromises
end-hosts and back-end servers, she can pro-actively in-
ject/change/delete events to impact the outcome of SEAL. Log
integrity needs to be ensured against such attacks, and the ex-
isting approaches based on cryptography or trusted execution
environment [9, 40, 60, 64, 71] can be integrated to this end.
One potential attack against SEAL is denial-of-service at-
tack. Though delta coding and Golumb coding are applied to
compress edges, all timestamps have to be “remembered” by
the new edge. The adversary can trigger a large number of
events to consume the storage. This issue is less of a concern
for approaches based on data reduction, as those edges will
be considered as repeated and get pruned. Moreover, knowing
the algorithm of compression ratio estimation, the adversary
can add/delete edges and nodes to mislead the estimation
process to consider each block incompressible. On the other
hand, such denial-of-service attack will make the performance
of casualty analysis fall back to the situation when no com-
pression is applied at most. The analysis accuracy will not be
2998    30th USENIX Security Symposium
USENIX Association
020406080100120Start Node Index20%40%60%80%100%120%Querying and Decompression TimeQueryingDecodingimpacted. Besides, by adding/deleting an abnormal number of
events, the attacker might expose herself to anomaly detection
methods.
Out-of-order logs. Due to reasons like network congestion,
logs occasionally arrive out of order at the back-end analysis
server [84]. Since the dependency graph possesses temporal
locality, such “out-of-order” logs result in potential impact
on the compression ratio. This issue can be addressed by the
method described as follows. Assuming the probability of out-
of-order logs is p, the server can reserve pN temporary storage
to hold all out-of-order logs in a day, where N is the daily
uncompressed log size. During off-peak hours, the server can
process each out-of-order log. For log from Node u to Node
v, we 1) retrieve in the compressed data the merged edges
to v and decompress the timestamps, and 2) merge the edge
(u,v) with the retrieved edges and compress the timestamps.
Since the probability p is typically small and off-peak hours
are utilized, out-of-order logs can be handled with smoothly.
Generalizing SEAL. Though SEAL is designed for causal-
ity analysis in the log setting, it can be extended to other
graphs/applications as well. Generally, SEAL assumes the
edges of a graph have attributes of timestamp, and the appli-
cation uses time range as a constraint to ﬁnd time-dependent
nodes/edges. Therefore, the data with timestamp and entity
relations, like network logs, social network activities, and rec-
ommendations, could beneﬁt from SEAL. Besides forensic
analysis, other applications relying on data provenance, like
fault localization, could be a good ﬁt. We leave the explo-
ration of the aforementioned data/applications as future work.
In terms of the execution environment of SEAL, we assume
SQL database stores the logs on a centralized server, like
prior works [37, 45, 77, 83]. It is possible that the company
deploying SEAL in a distributed environment (e.g., Apache
Spark) with non-SQL-based storage. How to adjust SEAL to
this new environment worth further research as well.
7 Related Works
Attack Investigation. Our work focuses on reducing the stor-
age overhead of system logs while maintaining the same
accuracy for attack investigation, in particular causality analy-
sis. Nowadays, causality analysis is mainly achieved through
back-tracking, which was proposed by King et al. [42]. This