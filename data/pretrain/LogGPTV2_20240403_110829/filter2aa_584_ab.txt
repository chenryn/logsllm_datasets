we’ve also argued that focusing too hard on privacy over usability can hurt
privacy itself. What happens when these principles conﬂict?
We encountered such a situation when designing how the Mixminion anony-
mous email network [6] should handle MIME-encoded data. MIME (Multipur-
pose Internet Mail Extensions) is the way a mail client tells the receiving mail
client about attachments, which character set was used, and so on. As a stan-
dard, MIME is so permissive and ﬂexible that diﬀerent email programs are al-
most always distinguishable by which subsets of the format, and which types of
encodings, they choose to generate. Trying to “normalize” MIME by convert-
ing all mail to a standard only works up to a point: it’s trivial to convert all
encodings to quoted-printable, for example, or to impose a standard order for
multipart/alternative parts; but demanding a uniform list of formats for multi-
part/alternative messages, normalizing HTML, stripping identifying information
from Microsoft Oﬃce documents, or imposing a single character encoding on each
language would likely be an impossible task.
Other possible solutions to this problem could include limiting users to a
single email client, or simply banning email formats other than plain 7-bit ASCII.
But these procrustean approaches would limit usability, and turn users away
from the Mixminion network. Since fewer users mean less anonymity, we must
ask whether users would be better oﬀ in a larger network where their messages
are likelier to be distinguishable based on email client, or in a smaller network
where everyone’s email formats look the same.
Some distinguishability is inevitable anyway, since users diﬀer in their inter-
ests, languages, and writing styles: if Alice writes about astronomy in Amharic,
her messages are unlikely to be mistaken for Bob’s, who writes about botany in
Basque. Also, any attempt to restrict formats is likely to backﬁre. If we limited
Mixminion to 7-bit ASCII, users wouldn’t stop sending each other images, PDF
ﬁles, and messages in Chinese: they would instead follow the same evolutionary
path that led to MIME in the ﬁrst place, and encode their messages in a variety
of distinguishable formats, with each client software implementation having its
own ad hoc favorites. So imposing uniformity in this place would not only drive
away users, but would probably fail in the long run, and lead to fragmentation
at least as dangerous as we were trying to avoid.
We also had to consider threat models. To take advantage of format dis-
tinguishability, an attacker needs to observe messages leaving the network, and
either exploit prior knowledge of suspected senders (“Alice is the only user who
owns a 1995 copy of Eudora”), or feed message format information into traﬃc
analysis approaches (“Since half of the messages to Alice are written in English,
I’ll assume they mostly come from diﬀerent senders than the ones in Amharic.”).
Neither attack is certain or easy for all attackers; even if we can’t defeat them
in the worst possible case (where the attacker knows, for example, that only
one copy of LeetMailPro was ever sold), we can provide vulnerable users with
protection against weaker attackers.
In the end, we compromised: we perform as much normalization as we can,
and warn the user about document types such as MS Word that are likely to re-
veal identifying information, but we do not forbid any particular format or client
software. This way, users are informed about how to blend with the largest pos-
sible anonymity set, but users who prefer to use distinguishable formats rather
than nothing at all still receive and contribute protection against certain attack-
ers.
6
Case study: Tor Installation
Usability and marketing have also proved important in the development of Tor,
a low-latency anonymizing network for TCP traﬃc. The technical challenges Tor
has solved, and the ones it still needs to address, are described in its design paper
[8], but at this point many of the most crucial challenges are in adoption and
usability.
While Tor was in it earliest stages, its user base was a small number of fairly
sophisticated privacy enthusiasts with experience running Unix services, who
wanted to experiment with the network (or so they say; by design, we don’t track
our users). As the project gained more attention from venues including security
conferences, articles on Slashdot.org and Wired News, and more mainstream
media like the New York Times, Forbes, and the Wall Street Journal, we added
more users with less technical expertise. These users can now provide a broader
base of anonymity for high-needs users, but only when they receive good support
themselves.
For example, it has proven diﬃcult to educate less sophisticated users about
DNS issues. Anonymizing TCP streams (as Tor does) does no good if appli-
cations reveal where they are about to connect by ﬁrst performing a non-
anonymized hostname lookup. To stay anonymous, users need either to conﬁgure
their applications to pass hostnames to Tor directly by using SOCKS4a or the
hostname-based variant of SOCKS5; to manually resolve hostnames with Tor
and pass the resulting IPs to their applications; or to direct their applications to
application-speciﬁc proxies which handle each protocol’s needs independently.
None of these is easy for an unsophisticated user, and when they misconﬁgure
their systems, they not only compromise their own privacy, but also provide no
cover for the users who are conﬁgured correctly: if Bob leaks a DNS request
whenever he is about to connect to a website, an observer can tell that anybody
connecting to Alice’s website anonymously must not be Bob. Thus, experienced
users have an interest in making sure inexperienced users can use the system
correctly. Tor being hard to conﬁgure is a weakness for everybody.
We’ve tried a few solutions that didn’t work as well as we hoped. Improving
documentation only helped the users who read it. We changed Tor to warn users
who provided an IP address rather than a hostname, but this warning usually
resulted in several email exchanges to explain DNS to the casual user, who had
typically no idea how to solve his problem.
At the time of this writing, the most important solutions for these users have
been to improve Tor’s documentation for how to conﬁgure various applications
to use Tor; to change the warning messages to refer users to a description of the
solution (“You are insecure. See this webpage.”) instead of a description of the
problem (“Your application is sending IPs instead of hostnames, which may leak
information. Consider using SOCKS4a instead.”); and to bundle Tor with the
support tools that it needs, rather than relying on users to ﬁnd and conﬁgure
them on their own.
7
Case study: JAP and its anonym-o-meter
The Java Anon Proxy (JAP) is a low-latency anonymizing network for web
browsing developed and deployed by the Technical University of Dresden in
Germany [3]. Unlike Tor, which uses a free-route topology where each user can
choose where to enter the network and where to exit, JAP has ﬁxed-route cas-
cades that aggregate user traﬃc into a single entry point and a single exit point.
The JAP client includes a GUI:
Notice the ‘anonymity meter’ giving the user an impression of the level of
protection for his current traﬃc.
How do we decide the value that the anonym-o-meter should report? In JAP’s
case, it’s based on the number of other users traveling through the cascade at
the same time. But alas, since JAP aims for quick transmission of bytes from
one end of the cascade to the other, it falls prey to the same end-to-end timing
correlation attacks as we described above. That is, an attacker who can watch
both ends of the cascade won’t actually be distracted by the other users [5,
10]. The JAP team has plans to implement full-scale padding from every user
(sending and receiving packets all the time even when they have nothing to
send), but—for usability reasons—they haven’t gone forward with these plans.
As the system is now, anonymity sets don’t provide a real measure of security
for JAP, since any attacker who can watch both ends of the cascade wins, and
the number of users on the network is no real obstacle to this attack. However,
we think the anonym-o-meter is a great way to present security information to
the user, and we hope to see a variant of it deployed one day for a high-latency
system like Mixminion, where the amount of current traﬃc in the system is more
directly related to the protection it oﬀers.
8
Bootstrapping, conﬁdence, and reputability
Another area where human factors are critical in privacy is in bootstrapping
new systems. Since new systems start out with few users, they initially provide
only small anonymity sets. This starting state creates a dilemma: a new system
with improved privacy properties will only attract users once they believe it is
popular and therefore has high anonymity sets; but a system cannot be popular
without attracting users. New systems need users for privacy, but need privacy
for users.
Low-needs users can break the deadlock [1]. The earliest stages of an anonymiz-
ing network’s lifetime tend to involve users who need only to resist weak attack-
ers who can’t know which users are using the network and thus can’t learn the
contents of the small anonymity set. This solution reverses the early adopter
trends of many security systems: rather than attracting ﬁrst the most security-
conscious users, privacy applications must begin by attracting low-needs users
and hobbyists.
But this analysis relies on users’ accurate perceptions of present and future
anonymity set size. As in market economics, expectations themselves can bring
about trends: a privacy system which people believe to be secure and popular
will gain users, thus becoming (all things equal) more secure and popular. Thus,
security depends not only on usability, but also on perceived usability by others,
and hence on the quality of the provider’s marketing and public relations. Per-
versely, over-hyped systems (if they are not too broken) may be a better choice
than modestly promoted ones, if the hype attracts more users.
Yet another factor in the safety of a given network is its reputability: the
perception of its social value based on its current users. If I’m the only user of a
system, it might be socially accepted, but I’m not getting any anonymity. Add a
thousand Communists, and I’m anonymous, but everyone thinks I’m a Commie.
Add a thousand random citizens (cancer survivors, privacy enthusiasts, and so
on) and now I’m hard to proﬁle.
The more cancer survivors on Tor, the better for the human rights activists.
The more script kiddies, the worse for the normal users. Thus, reputability is
an anonymity issue for two reasons. First, it impacts the sustainability of the
network: a network that’s always about to be shut down has diﬃculty attracting
and keeping users, so its anonymity set suﬀers. Second, a disreputable network
attracts the attention of powerful attackers who may not mind revealing the
identities of all the users to uncover the few bad ones.
While people therefore have an incentive for the network to be used for “more
reputable” activities than their own, there are still tradeoﬀs involved when it
comes to anonymity. To follow the above example, a network used entirely by
cancer survivors might welcome some Communists onto the network, though of
course they’d prefer a wider variety of users.
The impact of public perception on security is especially important during
the bootstrapping phase of the network, where the ﬁrst few widely publicized
uses of the network can dictate the types of users it attracts next.
9
Technical challenges to guessing the number of users in
a network
In addition to the social problems we describe above that make it diﬃcult for a
typical user to guess which anonymizing network will be most popular, there are
some technical challenges as well. These stem from the fact that anonymizing
networks are good at hiding what’s going on—even from their users. For example,
one of the toughest attacks to solve is that an attacker might sign up many users
to artiﬁcially inﬂate the apparent size of the network. Not only does this Sybil
attack increase the odds that the attacker will be able to successfully compromise
a given user transaction [9], but it might also trick users into thinking a given
network is safer than it actually is.
And ﬁnally, as we saw when discussing JAP above, the feasibility of end-to-
end attacks makes it hard to guess how much a given other user is contributing
to your anonymity. Even if he’s not actively trying to trick you, he can still
fail to provide cover for you, either because his behavior is suﬃciently diﬀerent
from yours (he’s active during the day, and you’re active at night), because
his transactions are diﬀerent (he talks about physics, you talk about AIDS), or
because network design parameters (such as low delay for messages) mean the
attacker is able to track transactions more easily.
10
Bringing it all together
Users’ safety relies on them behaving like other users. But how can they predict
other users’ behavior? If they need to behave in a way that’s diﬀerent from the
rest of the users, how do they compute the tradeoﬀ and risks?
There are several lessons we might take away from researching anonymity
and usability. On the one hand, we might remark that anonymity is already
tricky from a technical standpoint, and if we’re required to get usability right as
well before anybody can be safe, it will be hard indeed to come up with a good
design: if lack of anonymity means lack of users, then we’re stuck in a depressing
loop. On the other hand, the loop has an optimistic side too. Good anonymity
can mean more users: if we can make good headway on usability, then as long
as the technical designs are adequate, we’ll end up with enough users to make
everything work out.
In any case, declining to design a good solution means leaving most users to
a less secure network or no anonymizing network at all. Cancer survivors and
abuse victims would continue communications and research over the Internet,
risking social or employment problems; and human rights workers in oppressive
countries would continue publishing their stories.
The temptation to focus on designing a perfectly usable system before build-
ing it can be self-defeating, since obstacles to usability are often unforeseen. We
believe that the security community needs to focus on continuing experimental
deployment.
References
1. Alessandro Acquisti, Roger Dingledine, and Paul Syverson.
On the Economics
of Anonymity. In Rebecca N. Wright, editor, Financial Cryptography. Springer-
Verlag, LNCS 2742, January 2003.
2. Adam Back, Ulf M¨oller, and Anton Stiglic. Traﬃc Analysis Attacks and Trade-Oﬀs
in Anonymity Providing Systems. In Ira S. Moskowitz, editor, Information Hiding
(IH 2001), pages 245–257. Springer-Verlag, LNCS 2137, 2001.
3. Oliver Berthold, Hannes Federrath, and Stefan K¨opsell. Web MIXes: A system for
anonymous and unobservable Internet access. In H. Federrath, editor, Designing
Privacy Enhancing Technologies: Workshop on Design Issue in Anonymity and
Unobservability. Springer-Verlag, LNCS 2009, July 2000.
4. Lorrie Cranor and Mary Ellen Zurko, editors. Proceedings of the Symposium on
Usability Privacy and Security (SOUPS 2005), Pittsburgh, PA, July 2005.
5. George Danezis. The traﬃc analysis of continuous-time mixes. In David Martin and
Andrei Serjantov, editors, Privacy Enhancing Technologies (PET 2004), LNCS,
May 2004. http://www.cl.cam.ac.uk/users/gd216/cmm2.pdf.
6. George Danezis, Roger Dingledine, and Nick Mathewson. Mixminion: Design of a
type III anonymous remailer protocol. In 2003 IEEE Symposium on Security and
Privacy, pages 2–15. IEEE CS, May 2003.
7. Claudia Diaz, Stefaan Seys, Joris Claessens, and Bart Preneel. Towards measuring
anonymity. In Paul Syverson and Roger Dingledine, editors, Privacy Enhancing
Technologies, LNCS, April 2002.
8. Roger Dingledine, Nick Mathewson, and Paul Syverson.
Tor: The Second-
Generation Onion Router. In Proceedings of the 13th USENIX Security Symposium,
August 2004.
9. John Douceur. The Sybil Attack. In Proceedings of the 1st International Peer To
Peer Systems Workshop (IPTPS), March 2002.
10. Brian N. Levine, Michael K. Reiter, Chenxi Wang, and Matthew K. Wright. Timing
attacks in low-latency mix-based systems.
In Ari Juels, editor, Proceedings of
Financial Cryptography (FC ’04). Springer-Verlag, LNCS 3110, February 2004.
11. Nick Mathewson and Roger Dingledine. Practical Traﬃc Analysis: Extending and
Resisting Statistical Disclosure. In Proceedings of Privacy Enhancing Technologies
workshop (PET 2004), volume 3424 of LNCS, May 2004.
12. Ulf M¨oller, Lance Cottrell, Peter Palfrader, and Len Sassaman. Mixmaster Protocol
— Version 2. Draft, July 2003. http://www.abditum.com/mixmaster-spec.txt.
13. Michael Reiter and Aviel Rubin. Crowds: Anonymity for web transactions. ACM
Transactions on Information and System Security, 1(1), June 1998.
14. Andrei Serjantov and George Danezis. Towards an information theoretic metric for
anonymity. In Paul Syverson and Roger Dingledine, editors, Privacy Enhancing
Technologies, LNCS, San Francisco, CA, April 2002.