title:Understanding website complexity: measurements, metrics, and implications
author:Michael Butkiewicz and
Harsha V. Madhyastha and
Vyas Sekar
Understanding Website Complexity:
Measurements, Metrics, and Implications
Michael Butkiewicz
UC Riverside
PI:EMAIL
Harsha V. Madhyastha
UC Riverside
PI:EMAIL
Vyas Sekar
Intel Labs
PI:EMAIL
ABSTRACT
Over the years, the web has evolved from simple text content from
one server to a complex ecosystem with different types of content
from servers spread across several administrative domains. There
is anecdotal evidence of users being frustrated with high page load
times or when obscure scripts cause their browser windows to freeze.
Because page load times are known to directly impact user satisfac-
tion, providers would like to understand if and how the complexity
of their websites affects the user experience.
While there is an extensive literature on measuring web graphs,
website popularity, and the nature of web trafﬁc, there has been
little work in understanding how complex individual websites are,
and how this complexity impacts the clients’ experience. This pa-
per is a ﬁrst step to address this gap. To this end, we identify a
set of metrics to characterize the complexity of websites both at a
content-level (e.g., number and size of images) and service-level
(e.g., number of servers/origins).
We ﬁnd that the distributions of these metrics are largely inde-
pendent of a website’s popularity rank. However, some categories
(e.g., News) are more complex than others. More than 60% of web-
sites have content from at least 5 non-origin sources and these con-
tribute more than 35% of the bytes downloaded. In addition, we
analyze which metrics are most critical for predicting page render
and load times and ﬁnd that the number of objects requested is the
most important factor. With respect to variability in load times,
however, we ﬁnd that the number of servers is the best indicator.
Categories and Subject Descriptors
D.2.8 [Metrics]: [Complexity measures]; D.4.8 [Performance]:
[Modeling and prediction]
General Terms
Measurement, Human Factors, Performance
Keywords
Web page complexity, Page load times
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
1.
INTRODUCTION
Over the last decade, web pages have become signiﬁcantly more
complex. Originally used to host text and images, web pages now
include several content types, ranging from videos to scripts exe-
cuted on the client’s device to “rich" media such as Flash and Sil-
verlight. Further, a website today fetches content not only from
servers hosted by its providers but also from a range of third party
services such as advertising agencies, content distribution networks
(CDNs), and analytics services. In combination, rendering a sin-
gle web page today involves fetching several objects with varying
characteristics from multiple servers under different administrative
domains.
On the other hand, the ill-effects of slow websites are well-documented.
Recent surveys suggest two thirds of users encounter slow websites
every week [15] and that 49% of users will abandon a site or switch
to a competitor after experiencing performance issues [9]. While
there is plenty of anecdotal evidence that the increase in web page
complexity is a key factor in slowing down websites, formal studies
on this topic have been limited. Most prior work on web measure-
ment focuses on characterizing the Web graph [19, 16], analyzing
the network footprint of Web trafﬁc [34, 35, 41, 39, 42, 30], or
studying the rate of change of content on the Web [29]. While
these have contributed to a better understanding of web usage, they
do not analyze the websites themselves.
In this paper, we present a comprehensive measurement-driven
study of the complexity of web pages today and its impact on per-
formance. We measure roughly 1700 websites from four geograph-
ically distributed locations over a 7 week period. These websites
are spread across both a wide range of popularity ranks and genre
of website categories. In analyzing website complexity, we focus
on a client-side view of the landing pages of these sites and not on
the dependencies in the back-end server infrastructure—an inter-
esting topic complementary to our efforts.
Understanding the complexity of web pages and its implications
is vital on several fronts. With the increasing diversity of client plat-
forms for accessing the Web, it is important for browser developers
to identify the aspects of web page complexity that impact user-
perceived performance. On the other hand, as website providers
increasingly incorporate third-party services such as advertising,
analytics, and CDNs into their webpages, they need tools and tech-
niques to evaluate the impact of these services on users. Further-
more, beyond the perspective of any given user or web provider,
understanding website complexity is a ﬁrst step toward solutions
for automatically customizing web pages for varying client plat-
forms to achieve the right balance between performance, usability,
and business interests.
Our study focuses on two broad questions. First, we quantify
the complexity of a web page with a broad spectrum of metrics.
313We characterize a web page by the content fetched in rendering
it—the number of objects fetched, the sizes of these objects, and
the types of content. While these features remain largely the same
across different rank ranges of websites, we see a marked difference
across different website categories. For example, News websites
load a signiﬁcantly higher number of objects than others, whereas
Kids and Teens websites host a higher fraction of Flash content.
In addition to characterizing this content-level complexity, we
study the complexity of web pages with respect to the services they
build upon. We ﬁnd that non-origin content accounts for a signiﬁ-
cant fraction of the number of objects and number of bytes fetched,
an observation that holds even on low ranked websites. However,
the impact on download time of non-origin content is low—the me-
dian contribution to download time is only 15%. Though the most
popular third-party services are unsurprisingly analytics and adver-
tising providers, emerging services such as social networking plug-
ins and programming frameworks also appear on a sizeable fraction
of websites. A signiﬁcant difference that we observe in the types of
content served from non-origins in comparison to that from website
providers themselves is that Javascripts account for a much higher
fraction of non-origin objects.
The second focus of our study is to identify the critical complex-
ity metrics that have the most impact on the time to download and
render a web page. We ﬁnd that rather than the total number of
bytes fetched to render a website, the number of objects fetched is
the most dominant indicator of client-perceived load times. We cor-
roborate this with a linear regression model that predicts page load
times with a normalized mean squared error less than 0.1. We also
determine that, in contrast to actual load times, variability in load
times is better correlated with the number of servers from which
content is fetched.
2. RELATED WORK
There have been many efforts to analyze different aspects of the
Web ecosystem. This includes work to understand web structure,
tools to improve web performance, and measurements of emerging
web applications. We describe these next. Note that most of these
efforts focus either on web trafﬁc or web protocols. There has been
surprisingly little work on quantifying and understanding website
complexity.
Structure and evolution: The literature on modeling the Web
graph and its evolution focus on the interconnecting links between
websites [19, 16] rather than the structure and content of individual
websites. Related efforts have studied how the content of individual
web pages evolves over time [29]. Recent efforts have also tried to
“map” the hosting sites from which content is served [20].
Performance and optimization: As the usage scenarios for the
Web have changed, researchers have analyzed inefﬁciencies in web
protocols and suggested improvements [38, 21, 11].
In parallel,
there are efforts toward developing better browsers [37], tools to
optimize webpages [2, 1, 13], benchmarking tools [27, 12, 24], ser-
vices for customizing web pages for different platforms [40, 10, 8],
and techniques to diagnose performance bottlenecks in backend in-
frastructures [43] and to debug client performance in the wild [32].
Web trafﬁc measurement: This includes work on measuring
CDNs [34], understanding emerging Web 2.0 and AJAX-based ap-
plications [35, 41], measuring the network impact of social network
applications [39, 42], and characterizing end-user behavior within
enterprises [30], and longitudinal studies [31] among many others.
These focus on web trafﬁc as observed at the network-level, and
not on understanding the structure and performance of individual
websites.
Impact of load time on users: Several user experience studies
evaluate how page load times impact user satisfaction [18, 26].
There are also commercial services that measure page load times
in the wild [5]. These highlight the importance of optimizing page
load times. However, there have been few attempts to understand
how different aspects of website complexity impact the time to load
web pages.
Privacy leakage: Krishnamurthy et al. [33] report the prolifera-
tion of third-party services. Our measurement setup is similar to
theirs and we quantify the use of third-party services as well. How-
ever, our end goals are very different. In particular, they focus on
the privacy implications and observe that a small number of admin-
istrative entities (e.g., Google, Microsoft) have broad insights into
web access patterns. Our focus, instead is on using the presence of
third-party services as a metric to characterize website complexity
and on studying their impact on page load times.
Complexity metrics in other domains: Other efforts present met-
rics to quantify the complexity of network protocols [25], network
management [22], and systems more broadly [23]. In a Web con-
text, Zhang et al. [44] present metrics to capture ease of web page
navigation and Levering et al. [36] analyze the document layout
structure of web pages. The high-level motivation in these efforts
is the need for quantitative metrics to measure system complexity
and understand its impact on performance and usability. Our study
follows in the spirit of these prior efforts to quantify website com-
plexity to understand its impact on page load times.
Characterizing webpages: The closest related work appears in
recent industry efforts: HTTP Archive [3] and at Google [6]. While
the data collection steps are similar, we extend their analysis in two
signiﬁcant ways. First, we consider a more comprehensive set of
complexity metrics and present a breakdown across different rank
ranges and categories. Second, and more importantly, we go a step
further and construct models for correlating and predicting perfor-
mance and variability in performance with respect to the measured
complexity metrics. Furthermore, we view the presence and timing
of these parallel industry efforts as further conﬁrmation that there is
a key gap in understanding website complexity and its performance
implications. Our work is a step toward addressing this gap.
3. MEASUREMENT SETUP
We begin by describing the measurements we gathered that serve
as input for all of our analysis. All our datasets can be downloaded
at http://www.cs.ucr.edu/~harsha/web_complexity/.
We start with around 2000 websites at random from the top-20000
sites in Quantcast’s list of most popular websites.1 We annotate
these sites with content categories obtained from Alexa.2
To faithfully replicate the actions of a web browser when a user
visits a website, we use a browser (Firefox) based measure-
ment infrastructure. We use a “clean” Firefox instance (version
3.6.15) without any ad or pop-up blockers. (We install suitable plu-
gins such as the Adobe Flash player to ensure that websites render
properly.) We use the Firebug extension (version 1.7X.0b1) with
the Net:Export (version 0.8b10) and Firestarter (version
0.1.a5) add-ons to automatically export a log of all the requests and
responses involved in rendering a web page. This extension gen-
erates a report in the HTTP archive record (HAR) format [4] that
provides a detailed record of the actions performed by the browser
in loading the page.
1http://www.quantcast.com/top-sites
2http://www.alexa.com/topsites/category
314"log":{
"version":"1.1",
"browser":{
"name":"Firefox",
"version":"3.6.11"
},
"pages":[{
"startedDateTime":"18:12:59.702-04:00",
"title":"Wired News",
"pageTimings":{
"onContentLoad":2130,
"onLoad":4630}]
"entries":[{
"startedDateTime":"18:12:59.702-04:00",
"time":9,
"request":{
...
"headers":[{
"name":"Host",
"value":"www.wired.com" },
...}
"response":{
...
"content":{
"size":186013,
"mimeType":"text/html",}},
}]
]
Figure 1: Example snippet showing the different ﬁelds in a
HAR ﬁle. From this report, we can reconstruct the exact se-
quence of requests and responses, the number and size of re-
quests for objects of different content types, and identify the
server/origin for each request.
Rank range
1-400
400-1000
2000-2500
5000-10000
10000-20000
All
Number of websites
277
298
330
443
400
1748
Table 1: Summary of spread across rank ranges of websites in
our measurement dataset.
Figure 1 shows a snippet of a HAR report. It reports two page
load metrics—onContentLoad, which is the time taken to start