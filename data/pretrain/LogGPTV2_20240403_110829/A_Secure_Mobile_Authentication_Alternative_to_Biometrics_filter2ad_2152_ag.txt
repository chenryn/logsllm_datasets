[36] H. Krawczyk and P. Eronen. 2010. HMAC-based Extract-and-Expand Key Deriva-
tion Function (HKDF). RFC 5869. IETF. hŠps://tools.ietf.org/html/rfc5869
[37] N. Lalithamani and M. Sabrigiriraj. 2015. Palm and hand vein-based fuzzy vault
generation scheme for multibiometric cryptosystem. Še Imaging Science Jour-
nal 63, 2 (2015), 111–118. DOI:hŠps://doi.org/10.1179/1743131x14Y.0000000090
arXiv:hŠp://dx.doi.org/10.1179/1743131x14Y.0000000090
[38] Moritz Lipp, Daniel Gruss, Raphael Spreitzer, Cl´ementine Maurice, and Ste-
fan Mangard. 2016. ARMageddon: Cache AŠacks on Mobile Devices. In 25th
USENIX Security Symposium. 549–564.
[39] David G. Lowe. 2004. Distinctive Image Features from Scale-Invariant Key-
International Journal of Computer Vision 60, 2 (2004), 91–110. DOI:
points.
hŠps://doi.org/10.1023/B:VISI.0000029664.99615.94
[40] Emanuele Maiorana, Patrizio Campisi, Julian Fierrez, Javier Ortega-Garcia, and
Alessandro Neri. 2010. Cancelable Templates for Sequence-Based Biometrics
with Application to On-line Signature Recognition.
IEEE Transactions on Sys-
tems, Man, and Cybernetics - Part A: Systems and Humans 40, 3 (May 2010), 525–
538. DOI:hŠps://doi.org/10.1109/TSMCA.2010.2041653
[41] J. M. McCune, A. Perrig, and M. K. Reiter. 2005. Seeing-is-believing: using cam-
era phones for human-veriﬁable authentication. In 2005 IEEE Symposium on Se-
curity and Privacy (S P’05). 110–124. DOI:hŠps://doi.org/10.1109/SP.2005.19
[42] D. MenoŠi, G. Chiachia, A. Pinto, W. R. Schwartz, H. Pedrini, A. X. Falco, and
A. Rocha. 2015. Deep Representations for Iris, Face, and Fingerprint Spooﬁng
Detection. IEEE Transactions on Information Forensics and Security 10, 4 (April
2015), 864–879. DOI:hŠps://doi.org/10.1109/TIFS.2015.2398817
[43] Sanjeev Miglani and Manoj Kumar. 2016. India’s billion-member biometric data-
hŠp://www.reuters.com/article/us-
(March 2016).
base raises privacy fears.
india-biometrics-idUSKCN0WI14E.
[44] Karthik Nandakumar, Abhishek Nagar, and Anil K. Jain. 2007. Hardening Fin-
gerprint Fuzzy Vault Using Password. Springer Berlin Heidelberg, Berlin, Heidel-
berg, 927–937. DOI:hŠps://doi.org/10.1007/978-3-540-74549-5 97
[45] Douglas L Nelson, Valerie S Reed, and John R Walling. 1976. Pictorial superiority
eﬀect. Journal of Experimental Psychology: Human Learning and Memory 2, 5
(1976), 523.
[46] Lawrence O’Gorman. 2003. Comparing passwords, tokens, and biometrics
DOI:
Proc. IEEE 91, 12 (Dec 2003), 2021–2040.
for user authentication.
hŠps://doi.org/10.1109/JPROC.2003.819611
[47] Œian Song Ong, Andrew Teoh Beng Jin, and David Chek Ling Ngo. 2008.
Application-Speciﬁc Key Release Scheme from Biometrics. IJ Network Security
6, 2 (2008), 127–133.
[48] K. Patel, H. Han, and A. K. Jain. 2016. Secure Face Unlock: Spoof Detection
on Smartphones. IEEE Transactions on Information Forensics and Security 11, 10
(Oct 2016), 2268–2283. DOI:hŠps://doi.org/10.1109/TIFS.2016.2578288
[49] Andrea Peterson. 2015. OPM says 5.6 million ﬁngerprints stolen in cy-
beraŠack, ﬁve times as many as previously thought.
(September 2015).
hŠps://www.washingtonpost.com/news/the-switch/wp/2015/09/23/opm-now-
says-more-than-ﬁve-million-ﬁngerprints-compromised-in-breaches/.
[50] Giuseppe Petracca, Yuqiong Sun, Trent Jaeger, and Ahmad Atamli. 2015. Au-
Droid: Preventing AŠacks on Audio Channels in Mobile Devices. In Proceedings
of the 31st Annual Computer Security Applications Conference. 181–190.
[51] Iasonas Polakis, Marco Lancini, Georgios Kontaxis, Federico Maggi, Sotiris Ioan-
nidis, Angelos D. Keromytis, and Stefano Zanero. 2012. All Your Face Are Belong
to Us: Breaking Facebook’s Social Authentication. In Proceedings of the 28th An-
nual Computer Security Applications Conference (ACSAC ’12). ACM, New York,
NY, USA, 399–408. DOI:hŠps://doi.org/10.1145/2420950.2421008
[52] S. Pouyanfar, S. C. Chen, and M. L. Shyu. 2017.
An eﬃcient deep
residual-inception network for multimedia classiﬁcation. In 2017 IEEE In-
ternational Conference on Multimedia and Expo (ICME). 373–378.
DOI:
hŠps://doi.org/10.1109/ICME.2017.8019447
[53] Zhan Qin,
Jingbo Yan, Kui Ren, Chang Wen Chen, and Cong Wang.
Towards Eﬃcient Privacy-preserving Image Feature Extraction in
2014.
Cloud Computing. In Proceedings of the 22Nd ACM International Confer-
ence on Multimedia (MM ’14). ACM, New York, NY, USA, 497–506. DOI:
hŠps://doi.org/10.1145/2647868.2654941
12
[54] Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised represen-
tation learning with deep convolutional generative adversarial networks. arXiv
preprint arXiv:1511.06434 (2015).
[55] M. Rahman, M. Azimpourkivi, U. Topkara, and B. Carbunar. 2017. Video Live-
ness for Citizen Journalism: AŠacks and Defenses. IEEE Transactions on Mobile
Computing PP, 99 (2017), 1–1. DOI:hŠps://doi.org/10.1109/TMC.2017.2687922
[56] Nalini K. Ratha, Sharat Chikkerur,
Bolle. 2007.
Trans. Paˆern Anal. Mach.
hŠps://doi.org/10.1109/TPAMI.2007.1004
[57] RSA SecurID 2017. RSA SecurID Suite.
Generating Cancelable Fingerprint Templates.
Jonathan H. Connell, and Ruud M.
IEEE
DOI:
Intell. 29, 4 (April 2007), 561–572.
(2017).
hŠps://www.rsa.com/en-
us/products-services/identity-access-management/securid.
[58] Schlage Keypad Locks 2017.
Schlage Keypad Locks User Guide.
(2017).
hŠp://www.schlage.com/content/dam/sch-us/documents/pdf/installation-
manuals/23780042.pdf.
[59] SecuriCode
2017.
Keypad.
hŠp://owner.ford.com/how-tos/vehicle-features/locks-and-
SecuriCode Keyless
Entry
Ford
(2017).
security/securicode-keyless-entry-keypad.html.
[60] Claude Elwood Shannon. 2001. A mathematical theory of communication. ACM
SIGMOBILE Mobile Computing and Communications Review 5, 1 (2001), 3–55.
[61] Ling Shao, Fan Zhu, and Xuelong Li. 2015. Transfer Learning for Visual Catego-
rization: A Survey. IEEE Transactions on Neural Networks and Learning Systems
26, 5 (May 2015), 1019–1034. DOI:hŠps://doi.org/10.1109/TNNLS.2014.2330900
[62] Ivo Sluganovic, Marc Roeschlin, Kasper B. Rasmussen, and Ivan Martinovic.
2016. Using Reﬂexive Eye Movements for Fast Challenge-Response Authen-
tication. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security (CCS ’16). ACM, New York, NY, USA, 1056–1067. DOI:
hŠps://doi.org/10.1145/2976749.2978311
[63] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, ScoŠ Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.
Going Deeper With Convolutions. In Še IEEE Conference on Computer Vision
and Paˆern Recognition (CVPR).
[64] C. Szegedy, V. Vanhoucke, S. Ioﬀe, J. Shlens, and Z. Wojna. 2016. Rethink-
ing the Inception Architecture for Computer Vision. In 2016 IEEE Confer-
ence on Computer Vision and Paˆern Recognition (CVPR). 2818–2826.
DOI:
hŠps://doi.org/10.1109/CVPR.2016.308
[65] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. 2014. DeepFace: Clos-
ing the Gap to Human-Level Performance in Face Veriﬁcation. In 2014 IEEE
Conference on Computer Vision and Paˆern Recognition. 1701–1708.
DOI:
hŠps://doi.org/10.1109/CVPR.2014.220
[66] Robert Templeman, Zahid Rahman, David J. Crandall, and Apu Kapadia. 2013.
PlaceRaider: Virtual Œe‰ in Physical Spaces with Smartphones. In Annual Net-
work and Distributed System Security Symposium.
[67] tensorﬂow. 2017. Inception in TensorFlow. (2017). Retrieved May 5, 2017 from
hŠps://github.com/tensorﬂow/models/tree/master/inception
[68] Andrew B.
J. Teoh, Alwyn Goh, and David C. L. Ngo. 2006.
Ran-
dom Multispace ‹antization As an Analytic Mechanism for BioHashing
of Biometric and Random Identity Inputs.
IEEE Transactions on Paˆern
Analysis and Machine Intelligence 28, 12 (Dec. 2006), 1892–1901.
DOI:
hŠps://doi.org/10.1109/TPAMI.2006.250
[69] Bart Œomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl
Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2016. YFCC100M: Œe New
Data in Multimedia Research. Commun. ACM 59, 2 (Jan. 2016), 64–73. DOI:
hŠps://doi.org/10.1145/2812802
[70] Q. Wang, S. Hu, J. Wang, and K. Ren. 2016.
Secure Surﬁng: Privacy-
Preserving Speeded-Up Robust Feature Extractor. In 2016 IEEE 36th Interna-
tional Conference on Distributed Computing Systems (ICDCS). 700–710. DOI:
hŠps://doi.org/10.1109/ICDCS.2016.84
[71] Yi Xu, True Price, Jan-Michael Frahm, and Fabian Monrose. 2016. Virtual U:
Defeating Face Liveness Detection by Building Virtual Models from Your Pub-
lic Photos. In 25th USENIX Security Symposium (USENIX Security 16). USENIX
Association, Austin, TX, 497–512.
[72] Jason Yosinski, Jeﬀ Clune, Yoshua Bengio, and Hod Lipson. 2014. How
In Proceedings
Information Process-
(NIPS’14). MIT Press, Cambridge, MA, USA, 3320–3328.
Transferable Are Features in Deep Neural Networks?.
of
ing Systems
hŠp://dl.acm.org/citation.cfm?id=2969033.2969197
27th International Conference
on Neural
the
[73] MaŠhew D. Zeiler and Rob Fergus. 2014. Visualizing and Understanding Con-
volutional Networks. Springer International Publishing, Cham, 818–833. DOI:
hŠps://doi.org/10.1007/978-3-319-10590-1 53
# of words in image search query
1
2
3
4
Dataset size (ds i ze )
Avg # of trials before FA (random order)
Avg # of trials before FA (guessing a‚ack)
12, 413
12, 078
12, 034
24, 882
23, 205
22, 755
26, 418
24, 641
23, 921
26, 766
25, 028
24, 488
Portion of broken references (%)
5.0
9.0
10.9
9.0
Table 9: ai.lock under the object guessing attack. ‡e aver-
age number of trials before the ﬁrst false accept (FA) drops
only slightly in the object guessing attack scenario when
compared to a random ordering of attack images. ‡us,
knowledge of the authentication object type provides the ad-
versary only nominal guessing advantage.
A MOTIVATION FOR FEATURE SELECTION
USING PCA
Figure 8: PCA motivation: FRR vs. FAR of (i) ai.lock when
using PCA (with features ranked 200-400), (ii) ai.lock with
no feature selection (“Raw”), and (iii) 250 independent in-
stances of ai.lock when using a feature selection approach
that randomly selects 200 features. ai.lock with PCA consis-
tently achieves the lowest FRR and o…en the lowest FAR.
We now justify the need for the PCA step of ai.lock. For this,
we compare the best version of ai.lock running PCA (i.e., features
ranked 200-400), with two other versions. First, we consider a base-
line version (which we call “Raw”), that uses no feature selection
component. Speciﬁcally, Raw applies LSH to the raw embedding
vectors, then, identiﬁes the best threshold τ using the 5-fold cross
validation experiment described in § 7.1 for ai.lock. Second, we
compare against an ai.lock variant where we replace the PCA com-
ponent with a random choice of 200 features (of the embedding
vectors) produced by the last hidden layer of Inception.v3. Figure 8
shows the results of this comparison for λ values of 150, 250, 350
and 500, and 250 diﬀerent instances of ai.lock with random feature
selection. We observe that ai.lock with PCs of rank 200-400 consis-
tently achieves the signiﬁcantly lower FRR, and o‰en the lowest
FAR. In addition, we observe that randomly choosing the features
is not ideal, as it o‰en performs worse than when no feature selec-
tion is used at all.
B OBJECT/SCENE GUESSING ATTACK
Data. We have asked a graduate student to tag each of the 55
unique object images in the Nexus holdout set with 1 to 4 words.
For each value of the number of tags per image (i.e., 1 to 4), and
13
λ
P1
P2
150
350
500
0.799
0.500
0.797
0.500
0.796
0.500
Table 10: Average probability of collision, for valid (P1) and
invalid (P2) samples in the ai.lock holdout set per imageprint
bit basis. In all cases, P1 > P2, thus conclude that ai.lock with
single bit hash value is an LSIM function.
each object image, we collected 300-500 images provided by Google’s
image search engine. Œus, we generated 4 Google image datasets,
one for images found when searching with 1 tag, another when
searching with 2 tags, etc. In total, we have collected 90,479 im-
ages.
ai.lock performance under object guessing attack. We use
the 4 collected image datasets from Google to generate a total of
19, 905, 380 “guessing aŠack” authentication samples, and use them
to evaluate the guessing entropy [15] of ai.lock under an object/scene
guessing aŠack (see § 2).
Speciﬁcally, using each of the 4 Google image datasets we per-
form the following two brute force aŠacks. Œe ﬁrst aŠack emu-
lates an object guessing aŠack: re-order the images in the Google
dataset to start the brute force aŠack with the images of the same
object type, then continue with images of other object categories
in a random order. Finally, count the number of trials before the
ﬁrst match (false accept) occurs. Œe second aŠack is a standard
brute force aŠack: randomly shuﬄe the images in the Google im-
age dataset and use them to brute force each image in the Nexus
holdout set. We use the second aŠack as a baseline, to determine if
knowledge of the object type impacts the trial count to success. In
both aŠacks, we count each of the unbreakable reference images
as “success” at dsize trials, where dsize is the number of images in
the corresponding Google image dataset (see Table 9).
Table 9 summarizes the ai.lock performance under the object/scene
guessing aŠack scenario. We observe an increase in the portion of
the Nexus images that are broken when the simulated adversary
uses more words to describe the authentication objects for collect-
ing the aŠack image dataset. However, for all experiments, the
average number of trials before success drops only slightly in the
object guessing aŠack scenario compared to the baseline. Œis is
due to the fact that the reference images were mostly broken with
images of diﬀerent object categories. We conclude that knowledge
of the secret object type does not provide the adversary with a sig-
niﬁcant guessing advantage.
C IS AI.LOCK δ -LSIM FOR AI.LOCK WITH
SINGLE BIT HASH VALUE?
We now show that ai.lock with a single bit hash value is a δ -LSIM
(see Deﬁnition 3.1).
Figure 9: Histograms of normalized Hamming similarity be-
tween imageprints of valid and invalid authentication sam-
ples in the ai.lock holdout set. ‡e red rectangles pinpoint
the focus areas: valid samples with Hamming similarity be-
low 0.6 and invalid samples with Hamming similarity above
0.6. Higher values of λ provide more eﬀective separation be-
tween valid and invalid samples: when λ = 500, no invalid
samples have similarity above 0.6.
ai.lock uses Charikar’s random projection LSH [12]. Œerefore,
for any embedding vector (the input to LSH function) u and v,
Pr [1 bit collision] = 1 − θ (u,v )
, where θ(u, v) denotes the angle
between u and v. We use the angle between the feature vectors of
images in the ai.lock holdout set to compute the average probabil-
ity of collision: 0.79 for valid samples and 0.50 for invalid authen-
tication samples.
π
Figure 9 shows the histogram of normalized Hamming similar-
ity between imageprints in the valid and invalid samples of the
ai.lock holdout set. Unsurprisingly, most invalid samples have a
Hamming similarity between 0.4 and 0.6: diﬀerent images have
imageprints that are similar in around half of their bits (see also
Table 10). We observe that the overlap between the Hamming simi-
larities of valid and invalid samples signiﬁcantly reduces for higher
values of λ.
In addition, we compute these probabilities empirically by count-
ing the number of times when the hash values collide for valid
and invalid samples, a‰er the LSH transformation. We then use
this count to compute the average probability of collision for a
valid (P1) and invalid (P2) authentication samples (see Table 10).
We observe the remarkable similarity of these values, to the ones
above, computed analytically. As λ increases, the empirical P1 ap-
proaches the analytic lower bound (0.79). We perform a Mann-
Whitney one-sided test with alternative hypothesis P1 > P2. Œis
test suggests that there is a signiﬁcant gap between P1 and P2
(p − value = 0.00, α = 0.05) for all cases, hence, ai.lock is a δ -LSIM
on the Nexus holdout dataset.
In addition, comparing the values for P1 and P2 with the ones
reported in § 7.4 for ai.lock with multi-bit hash value, we observe
that concatenating multiple hashes enlarges the gap between P1
and P2 values.
14