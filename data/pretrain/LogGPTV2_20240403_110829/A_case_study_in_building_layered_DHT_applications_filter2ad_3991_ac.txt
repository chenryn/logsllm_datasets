set works as follows: get(key) returns a generation number
for the key. This generation number is updated whenever the
DHT key is modi(cid:12)ed. We use the modi(cid:12)cation timestamp
as the generation number.
In addition, we implemented a
put conditional(key, value, gen); the put succeeds only if the
key has not been modi(cid:12)ed since the generation number gen.
To implement this concurrency primitive correctly in the
presence of replication and failures, the DHT must provide
strong guarantees for atomic writes. Etna [24] is a con-
sensus protocol based on Paxos [22] that can provide such
guarantees. However, the protocol is fairly involved and will
signi(cid:12)cantly complicate the DHT implementation. Instead,
our extension uses a much simpler mechanism that works in
practice for the common case: serialize all put conditional op-
erations through the master replica for each key. In the event
of churn, if multiple DHT nodes think they are the master
replica for a key, this mechanism will fail. Such events will
hopefully be rare for a DHT service and as mentioned ear-
lier will only result in ine(cid:14)ciency in the PHT, not loss of
correctness.
With these primitives, the insert operation is modi(cid:12)ed
as follows. When inserting key K into leaf(K), we use the
put conditional() primitive to ensure that the leaf has not been
modi(cid:12)ed or split since we performed the lookup. When a leaf
node needs to be split, we (cid:12)rst mark it as being in transi-
tion using the put conditional primitive.
If multiple servers
attempt to split the same node, only one of them will suc-
ceed. All of the other PHT nodes that are involved in this
split operation are then marked in transition. Only then is
the split operation performed. The in transition markers are
removed after the split operation has completed.
4.6 Caching to improve performance
The lookup primitive is central to all PHT operations. It
can be optimized by using a client-side hint cache that keeps
track of the shape of the PHT based on previous lookup
operations. When a lookup for key K returns the leaf node
L (a pre(cid:12)x of K), the cache records L as a leaf node and all
entries from the root to the parent of L as interior nodes.
A new lookup for a di(cid:11)erent key K 0 is (cid:12)rst checked against
this cached information.
If the cache returns a leaf node
L0, the client performs get(L0) to verify that the PHT has
not been recon(cid:12)gured and that the node is indeed still a leaf
node. A cache hit thus generates a single DHT operation.
Upon a cache miss, however, the lookup must revert to the
binary search algorithm. For query operations, we can use a
similar caching scheme that (cid:12)nds relevant leaf nodes directly
by querying the cache.
We can use a number of other heuristics to optimize the
performance of PHTs. For example, for certain queries (such
as a small range containing the midpoint of the key-space),
it may be desirable to break the search query into two, and
treat these sub-queries independently. This would ensure
that searches can start at a level in the PHT that is appro-
priate for the query, that is, smaller queries start lower down
in the PHT. Another optimization is to use an n-ary tree in-
stead of binary trees to reduce the number of internal nodes
that must be queried.
4.7 PHTs versus linked data structures
This section compares the merits of the PHT with balanced-
tree-based indexes, such as the B-tree, with particular em-
phasis on implementation in a distributed setting. While
tree-based indexes may be better in traditional indexing ap-
plications like databases, we argue the reverse is true for
implementation over a DHT.
E(cid:14)ciency: A balanced tree has a height of log N , where N
is the number of elements in the tree; so a key lookup requires
log N DHT lookups. For PHTs, the binary search lookup
algorithm requires only log D DHT operations, D being the
number of bits in the PHT key.
Load Balancing: Every lookup in a tree-based index
must go through the root, creating a potential bottleneck. In
the case of PHTs, binary search allows the load to be spread
over 2
2 nodes (in the case of uniform lookups), eliminating
any bottleneck.
D
Fault Resilience: In a typical tree-based structure, the
loss of an internal node results in the loss of the entire subtree
rooted at the failed node. PHTs however do not require top-
down traversals; instead one can directly \jump" to any node
in the PHT. Thus the failure of any given node in the PHT
does not a(cid:11)ect the availability of data stored at other nodes.
5. EVALUATION
We now measure the performance of the DHT-based im-
plementation of Place Lab’s mapping service. The two main
operations that Place Lab performs are: routing of beacon
records from war drivers to mapping servers for updating
beacon position estimates, and routing of beacon position es-
timates both to and from the PHT. The former is a straight-
forward use of a DHT. Records are hashed based on each
beacon identi(cid:12)er and this hash is used to redirect through
the DHT to a mapping server. Accordingly, we focus our
measurement e(cid:11)ort on the pre(cid:12)x hash tree mechanism and
the way it behaves both under insert loads from the mappings
servers and under query loads from downloading clients.
5.1 Setup
We implemented PHTs and the rest of the Place Lab in-
frastructure on top of OpenDHT. The implementation e(cid:11)ort
required to build the glue between Place Lab’s application
code and the underlying DHT and to build a robust PHT
implementation was small. The code consists of 2100 lines
of Java. In comparison, the underlying OpenDHT codebase
is over 14000 lines of code.
We have deployed and run the Place Lab mapping ser-
vice and the PHT on top of the public Planet Lab-based [27]
OpenDHT deployment. However, for our experimental eval-
uation, we chose to use our own independent OpenDHT de-
ployment. This was for two reasons: to understand the ef-
fects of concurrent operations, we needed to use enhanced
APIs (put conditional()); and, to evaluate the e(cid:11)ect of churn,
we wished to kill and restart OpenDHT nodes as needed.
Our deployment consisted of 24{30 nodes spread across ma-
chines on the US West Coast, US East Coast and England.
We also conducted experiments on a larger deployment using
PlanetLab. However, due to the vagaries of load on Planet-
Lab, the results from those experiments were erratic and are
left out in this discussion.
As input, we used a data set composed of known loca-
tions of 1.4 million 802.11 access points gathered from a
war-driving community web service, Wigle (www.wigle.net).
This data set consists of estimated AP positions based on
war drives submitted by users in the United States to the
Wigle service. Figure 3 shows the distribution of the input
data. We conducted experiments with di(cid:11)erent data set sizes
picked uniformly at random from this larger set.
We constructed a query workload composed of 1000 queries
to represent a set of typical Place Lab queries; the workload
was proportional to the distribution of access points in the
input data. We made this choice under the assumption that
Figure 3: Distribution of the input data set. The in-
tensity of the dots on the map corresponds to the den-
sity of data points in that region.
Figure 4: The structure of the PHT for the 1.4 million
input data set and a block size of 1000.
high access point density corresponds to higher population
density and thus there is a higher likelihood of queries in
those regions. Each query was generated by picking an ac-
cess point at random from the input data set and building a
rectangular region around the location of that access point
with a size that was picked uniformly at random from [0{1.0]
latitude/longitude units (approximately 0{100km). Such a
query corresponds to requests of the form: \I can hear access
point X, (cid:12)nd all APs within distance Y of this AP."
5.2 Structural Properties
In the (cid:12)rst set of experiments, we constructed PHTs with
progressively larger data sets and measured the structure of
the resulting trees. Figure 4 shows a depiction of the PHT for
the entire data set with a block size of 1000 overlaid on top
of a map of the US. Each rectangle on the map represents
a leaf node in the PHT. Comparing to the input data set
shown in Figure 3, we note that areas with high AP density
get sub-divided into smaller rectangular blocks than sparse
areas. This is because we use a constant block size across the
PHT. This organization ensures that queries for dense areas
can be spread across a larger number of DHT nodes thereby
reducing the bottleneck that popular queries may cause.
We measured the tree characteristics using two metrics:
depth of the tree and block utilization (number of elements
per PHT leaf node as a percentage of the block size).
Tree Depth: Figure 5 shows a CDF of the depth of leaf
nodes in a PHT with 1.4 million elements and a block size
of 1000. Between the 20th and 80th percentiles, the tree
depth varies between 18 and 26. Some nodes in the densest
part of the data set have higher depth (as deep as 33) while
a small fraction of nodes in the sparse parts of the coun-
try are shallower. Figure 6 shows the variation in average
depth of the PHT for varying block sizes and di(cid:11)erent in-
put data set sizes. We can see that the tree depth decreases
s
e
d
o
n
f
a
e
l
f
o
%
e
v
i
t
l
a
u
m
u
C
100
80
60
40
20
0
0
5
10
15
20
25
30
35
40
Node depth in PHT
Figure 5: A cumulative distribu-
tion function (CDF) of leaf node
depth for a PHT with an input data
set of 1.4 million and a block size
of 1000.
e
d
o
n
f
a
e
l
f
o
h
t
p
e
d
.
g
v
A
35
30
25
20
15
10
5
0
1.4m
500k
100k
50k
10k
10
100
1000
10000
Block size
Figure 6: Variation in tree depth
as a function of block size for dif-
ferent input data sizes
n
o
i
t
a
z
i
l
i
t
u
k
c
o
b
%
l
70
60
50
40
30
20
10
0
1.4m
500k
100k
50k
10
100
Block size
1000
10000
Figure 7: Block utilization (num-
ber of items in a leaf node as
a percentage of block size) versus
block size for varying input data
set sizes.
logarithmically with the block size, that is, larger block sizes
result in shallower trees. With larger blocks, fewer accesses
are needed to retrieve a portion of the data space, however,
there is greater contention for nodes within the PHT. The
(cid:12)gure also shows that (as one would expect) the tree depth
increases with increasing data set sizes. Although not obvi-
ous from the (cid:12)gure, this increase is logarithmic as well.
Block Utilization: This experiment looks at how full
the leaf nodes are as a percentage of the block size. Figure 7
shows the utilization as a function of block size for varying in-
put data sizes. The plots for input data size of 50k and 100k
show that the block utilization is high for small block sizes.
It drops as the block size is increased, and eventually begins
to grow again once the block size begins to approach the
total input data size. The non-uniformity of the input data
results in a skewed distribution of data across leaf nodes, and
causes the average leaf utilization to be lower than if the data
were uniformly distributed. Even with non-uniform data, at
small block sizes, most blocks (cid:12)ll to capacity and thus the
utilization in those cases is high. At very large block sizes
(comparable to the input data set size), the tree becomes
shallow and the non-uniformity of the data is averaged out,
thus resulting in better block utilization.
5.3 Performance of the PHT
One critical advantage o(cid:11)ered by a PHT over simpler data
structures like a traditional pointer-based binary tree is that
because of its structured key-space-based layout, PHT look-
ups can bypass the root and begin looking for data at lower
levels in the tree. This o(cid:11)ers PHTs the potential to avoid
having the upper levels of the tree be hotspots that limit
throughput. Figures 8 and 9 show the spread of DHT ac-
cesses across PHT levels for PHT insert (for 500,000 items)
and query operations (for the entire query workload) respec-
tively. These graphs show that the levels of the tree close
to the root are accessed very seldom, with the bulk of the
activity in the depth range of 16 to 30. For sparse regions,
and for queries for large-sized areas, the query starts higher
up in the tree. Yet, the dominant accesses are for leaf nodes
deep within the tree.
The previous charts only show the distribution of DHT
operations across PHT nodes. The critical test of the viabil-
ity of PHTs is the actual latencies required to perform insert
and query operations. The next set of experiments evaluate
this performance.
Insert Operations: For this experiment, we pre-loaded
a PHT with 100,000 elements. We then started an insert
workload composed of 1000 new randomly chosen elements
and measured the performance of the insert operations. Fig-
ure 10 shows a CDF of the insert operations as a function of
insert latency for a PHT block size of 500.
The graph also shows the e(cid:11)ect of the lookup cache (Sec-
tion 4.6). After the PHT had been pre-loaded, we started
the client with an empty cache. Gradually as the client in-
serted more and more elements into the PHT, it discovered
the shape of the tree and was able to skip the binary lookups
by hitting directly in the cache.
We notice that the median insert operation takes about
1.45 seconds. When there is a cache miss, inserts take a me-
dian of 3.26 seconds, whereas on a cache hit, the median is
765ms. Part of the performance de(cid:12)ciency is due to a lack
of optimization in the OpenDHT implementation. During
a DHT get() operation, if a key matches a number of val-
ues, the current OpenDHT implementation returns only the
(cid:12)rst 1kBytes of those values and requires clients to perform
additional get() operations to retrieve the remaining values.
Hence, fetches of large leaf nodes can result in a cascade of
a number of DHT-level operations. We have communicated
this issue to the OpenDHT developers and a future version
is expected to (cid:12)x this by allowing bulk gets. With this and