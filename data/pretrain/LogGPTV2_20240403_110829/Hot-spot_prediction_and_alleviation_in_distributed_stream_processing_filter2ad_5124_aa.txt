title:Hot-spot prediction and alleviation in distributed stream processing
applications
author:Thomas Repantis and
Vana Kalogeraki
Hot-Spot Prediction and Alleviation in
Distributed Stream Processing Applications
Thomas Repantis
Vana Kalogeraki
Department of Computer Science and Engineering
University of California, Riverside, CA 92521
{trep,vana}@cs.ucr.edu
Abstract
Many emerging distributed applications require the real-
time processing of large amounts of data that are being up-
dated continuously. Distributed stream processing systems
offer a scalable and efﬁcient means of in-network process-
ing of such data streams. However, the large scale and
the distributed nature of such systems, as well as the ﬂuc-
tuation of their load render it difﬁcult to ensure that dis-
tributed stream processing applications meet their Quality
of Service demands. We describe a decentralized frame-
work for proactively predicting and alleviating hot-spots in
distributed stream processing applications in real-time. We
base our hot-spot prediction techniques on statistical fore-
casting methods, while for hot-spot alleviation we employ
a non-disruptive component migration protocol. The ex-
perimental evaluation of our techniques, implemented in
our Synergy distributed stream processing middleware over
PlanetLab, using a real stream processing application oper-
ating on real streaming data, demonstrates high prediction
accuracy and substantial performance beneﬁts.
1. Introduction
A variety of emerging applications require real-time pro-
cessing of high-volume, high-rate data that are updated con-
tinuously. Examples include analyzing the input provided
by website visitors to provide relevant advertising, moni-
toring network trafﬁc to detect intrusions or update router
conﬁguration, customizing news feeds to user interests, or
processing ﬁnancial trading data for recommendations or
alerts. This type of applications has given rise to a new
class of systems, called Distributed Stream Processing Sys-
tems (DSPSs) [1, 8, 10, 13, 14]. In DSPSs, reusable compo-
nents, located on geographically distributed nodes, process
continuous data streams in real-time. These components are
composed dynamically to form distributed applications.
Distributed stream processing applications have Quality
of Service (QoS) requirements, expressed in terms such as
end-to-end delay, throughput, or miss rate. For example,
an alert needs to be raised within a certain time frame af-
ter an intrusion, or a trading recommendation needs to be
made while processing ﬁnancial data at certain rates. Ad-
hering to such QoS requirements is crucial for the depend-
able operation of a DSPS. The ﬁrst step towards satisfying
the QoS requirements of stream processing application is
taking them into account during the application composi-
tion [8, 14]. However, as the incoming data rates may in-
crease at run-time, due to external events such as a network
attack or a rapid popularity growth of some news event, an
application execution may cease to adhere to the requested
QoS. In fact, the distinct characteristic of stream processing
applications is that the data to be processed arrive in high
rates, and often in bursts [24, 25]. Under such dynamically
changing conditions, providing application QoS is a chal-
lenging task. The problem is complicated further by the
large scale and the distributed nature of a DSPS. Accurate
centralized decisions are infeasible, due to the fact that the
global state of a large-scale DSPS is changing much faster
than it can be communicated to a single host.
In this paper we study the problem of predicting and alle-
viating application hot-spots in a DSPS. Current approaches
for addressing load ﬂuctuations in DSPSs [3, 18, 24, 25], in-
cluding our previous work [15], focus on avoiding or re-
solving hot-spots in the system resources, in other words
overloaded nodes. We refer to this kind of hot-spot de-
tection and alleviation as node-oriented. The focus of this
work, on the other hand, is on detecting and alleviating
hot-spots in the application execution, in other words ap-
plications that persistently fail to meet the QoS required by
the user. We call this kind of hot-spot detection and alle-
viation application-oriented. We believe that application-
oriented hot-spot detection and alleviation are as impor-
tant as their node-oriented counterparts for the following
key reasons: i) Application-oriented hot-spot detection is
more sensitive and can be triggered even when a node is
underloaded. Even when running on a moderately loaded
node, an application may not be meeting its QoS require-
ments (e.g., if they are stringent), thus experiencing a hot-
spot. On the other hand, with node-oriented hot-spot detec-
tion, by the time a node is overloaded many of the applica-
tions using that node will already have violated their QoS
requirements. ii) Application-oriented hot-spot alleviation
allows more ﬁne-grained hot-spot alleviation. Depending
on the individual applications’ QoS demands, only some
instead of all the applications that are using a node may be
suffering, and thus only these applications may need to be
migrated. On the contrary, node-oriented hot-spot allevia-
tion aims at reducing a node’s load, irrespective of which
of the applications experience overload.
iii) Most impor-
tantly, application-oriented hot-spot detection enables tak-
ing proactive measures with regards to application perfor-
mance, to prevent severe degradation of application QoS.
Speciﬁcally, this paper makes the three following main
contributions: i) We propose a framework built on statistical
forecasting methods, to accurately predict QoS violations at
run-time and proactively identify application hot-spots. In
order to achieve this, our prediction framework binds work-
load forecasting with execution time forecasting. To accom-
plish workload forecasting we predict rate ﬂuctuations, ex-
ploiting auto-correlation in the rate of each component, and
cross-correlation between the rates of different components
of a distributed application. To accomplish execution time
forecasting we use linear regression, an established statisti-
cal method, to accurately model the relationship of the ap-
plication execution time and the entire workload of a node,
while dynamically adapting to workload ﬂuctuations.
ii)
To react to predicted QoS violations and alleviate hot-spots
we enable nodes to autonomously migrate the execution of
stream processing components using a non-disruptive mi-
gration protocol. Candidate selection for migration is based
on preserving QoS. We employ prediction again to ensure
that migration decisions do not result to QoS violations of
other executing applications. To drive migration decisions
in a decentralized manner we build a load monitoring archi-
tecture on top of a Distributed Hash Table (DHT) [16]. iii)
We have implemented our techniques in Synergy [14], our
distributed stream processing middleware1. To validate our
approach we have deployed our middleware on the Planet-
Lab [5] wide-area network testbed and we have run exper-
iments of a real network monitoring application [20] oper-
ating on traces of real TCP trafﬁc [22]. Our experimental
evaluation demonstrates high prediction accuracy, with an
average prediction error of 3.7016%, and substantial ben-
eﬁts in application QoS, achieved by migrations that are
completed in approximately 1s.
1Synergy is implemented as a multi-threaded system of about
35,000 lines of Java code and more information is available at
http://synergy.cs.ucr.edu/
Figure 1. The basic blocks of Synergy.
2. The Synergy Middleware
In this section we present a brief overview of our Syn-
ergy distributed stream processing middleware. Synergy
is a middleware designed to provide QoS support for dis-
tributed stream processing applications.
In Synergy, data
streams, consisting of independent data tuples, arrive con-
tinuously from external sources (such as web users, mon-
itoring devices, or a sensor network) and need to be pro-
cessed by stream processing components in real-time. Each
component is a self-contained software module, that offers
a predeﬁned operator. The operators can be as simple as a
ﬁlter or a join, or as complex as transcoding or encryption.
Components are deployed in the distributed nodes of the
Synergy middleware according to their individual software
capabilities or following criteria for the optimization of the
performance of the whole system [1, 13].
The nodes of our distributed stream processing middle-
ware are connected via overlay links on top of the existing
IP network. The application component graph is built on
top of the middleware, as shown in Figure 1. The basic
blocks of the Synergy middleware running on each node
are shown in Figure 1. Synergy offers several beneﬁts: i) It
enables efﬁcient component composition that meets end-to-
end QoS demands by sharing resources, components, and
streams [14]. ii) It provides a low overhead resource moni-
toring facility. iii) It allows fast stream and component dis-
covery by utilizing the underlying DHT infrastructure [16].
The user executes a distributed stream processing appli-
cation by submitting a request at one of the nodes of the
middleware, specifying the required operators and their de-
pendencies. Then, the system runs a composition algorithm
to select the components on the nodes to accomplish the
application execution. These will constitute the application
component graph, that represents the sequence of compo-
nent execution and the corresponding hosting nodes.
3.1 End-to-End to Local Execution Time
We have extended Synergy’s architecture to enable de-
centralized load monitoring [15], built on top of the DHT
we use for component discovery. We have implemented
a distributed inverted index on top of the DHT. This way
we associate operator names with handlers to nodes host-
ing components offering these operators, together with the
current load values of these nodes. For example, in Fig-
ure 1 on the left of each node are listed the components this
node is offering, while on the right of each node are listed
the handlers and loads this node is responsible for maintain-
ing. Node B is responsible for keeping the handlers for the
components that offer an aggregator operator. Therefore it
keeps the handlers of nodes B and C, as well as the loads of
nodes B and C. Whenever a node’s load changes, it consults
the DHT to determine the nodes responsible for holding the
handlers for all the components it offers. It then sends load
update messages to them. For example, in Figure 1 node
B that offers a ﬁlter, an aggregator, and a transcoder, will
send its load update messages to the responsible nodes, C,
B, and A, respectively. To avoid the communication over-
head caused by updating, we enable the nodes to inform the
monitoring nodes only when a signiﬁcant change in their
load occurs. Conﬁguration changes such as node arrivals,
departures, failures, or balancing of operator keys among
nodes are handled by the DHT [16].
We use our decentralized load monitoring architecture to
cope with application hot-spots. We deﬁne an application
hot-spot as a node in the application component graph in
which the application execution persistently fails to meet
the QoS required by the user. The end-to-end QoS require-
ments, which are speciﬁed when requesting an applica-
tion, may among others include end-to-end execution time,
throughput, or miss rate. Although our schemes are generic
to additive QoS metrics linearly related to rate, we focus on
the end-to-end execution time metric denoted by qt.
3. Application Hot-Spot Prediction
The goal of proactive application hot-spot detection is
In
to predict end-to-end execution time QoS violations.
order to achieve this goal we employ: i) Computation of
the application “slack time” ts (Section 3.1), to determine
the maximum local execution time allowed by the applica-
tion QoS, before missing its end-to-end execution time re-
quirement. ii) Local execution time prediction based on an
application’s incoming rate and using linear regression, to
determine whether the maximum local execution time will
be reached or exceeded (Section 3.2). iii) Rate prediction
based on auto- and cross-correlation between stream pro-
cessing components, to determine the future workload that
deﬁnes the future execution time (Section 3.3).
Translation
We predict an application hot-spot by examining the
“slack time” of the application on every component of the
application component graph. The slack time represents
how close we are to violating the end-to-end execution time
requirement of the application. Let qt represent the end-
to-end execution time requirement of the application. qt
includes the execution and communication times spent for
a tuple to traverse the entire application component graph.
Thus, we deﬁne the slack time ts of an application as the
difference between the required end-to-end execution time
qt and the predicted end-to-end execution time. As the
application executes, its slack time is computed for every
tuple, on every component of the application component
graph, based on the local prediction of the end-to-end ex-
ecution time. The predicted end-to-end execution time in-
cludes the execution and communication times spent for a
tuple to reach the current component, te and tc respectively,
the predicted execution times ˆte needed for the current and
its downstream components to process the data tuple, as
well as estimated average communication times ¯tc needed
for the data tuple to traverse the rest of the application com-
ponent graph. For example, in Figure 3 the predicted end-
to-end execution time as it is calculated in component B is
ˆte(D). In
the sum of te(A), tc(A→B),
order to avoid a QoS violation, the predicted end-to-end ex-
ecution time needs to be less than the required end-to-end
execution time qt, in other words, the slack time ts needs to
be positive, for every component i of the v components of
the application component graph:
tc(B→D), and
ˆte(B),
¯
ts(i) = qt − ( Xj∈1...i−1
Xj∈i...v−1
tc(j→j+1) + Xj∈1...i−1
tc(j→j+1) + Xj∈i...v
¯
te(j)+
(1)
ˆte(j)) > 0
The above single-path computation will identify a hot-spot
in the path where it exists. For example, if in Figure 3 com-
ponent C is overloaded, the path A → B → D will not
detect a hot-spot, while path A → C → D will. In order
for the above hot-spot prediction to take place, the estimated
average communication times, and the predicted execution
times must be computed. The estimates for the communi-
cation times are available from the application composition
phase [14] and can be updated periodically. The predicted
execution times are derived locally on every node hosting
a component of the application component graph, as ex-
plained in the following Section 3.2. They are then propa-
gated to all nodes participating in the application execution
using a feedback loop passing through the source. The feed-
back loop allows us to piggyback the predicted execution
times on the data tuples, to minimize the communication
overhead. For example, in the application component graph
shown in Figure 3 when the node hosting component D cal-
culates the component’s next predicted execution time for
this application, it propagates it to the node hosting compo-
nent A, which forwards it to the nodes hosting components
B and C. Similarly, the rest of the nodes propagate their pre-
dicted execution times. Using the predicted execution times
to compute the slack time on every component enables us
to predict locally whether the end-to-end execution time re-
quirement of the application will be violated.
3.2 Local Execution Time Prediction
In this section we explain how we predict the local ex-
ecution time ˆte needed to process a data tuple of an appli-
cation. The prediction takes place at each node hosting a
component of the application. ˆte is used to compute the
next slack time ts of the application using Equation 1. The
local execution time for a data tuple (the time elapsed be-
tween the arrival and the departure of the tuple) is the sum
of the processing time to process the tuple, and the queue-
ing time the tuple has to wait in the scheduler’s queue while
other tuples are being processed. While the processing time
is constant for a given tuple size, the queueing time de-
pends on the load of the processing node, in other words
on the rates (incoming tuples to be processed per time unit)
and processing times of the applications currently being ex-
ecuted on the node. Using queueing theory, one can derive
average values for the queueing time, assuming an M/M/1
queueing model [14], or a more general M/G/1 model that
makes no assumptions regarding the service rate, in which
case the queueing time is given by the Pollaczek-Khinchin
mean value formula [9]. However, we chose not to predict
the execution time using queueing theory for the following
reason: The arrivals of data tuples may not always be accu-
rately approximated with a Poisson distribution if rate ﬂuc-
tuations or bursts occur. Such rate variations are quite com-
mon in distributed stream processing applications [24]. Ac-
curate prediction during such ﬂuctuations is however cru-
cial. We use linear regression to predict the execution time
of an application [12]. Since data tuples arrive in high rates,
prediction is more ﬁne-grained than node load changes.
To predict the local execution time ˆte of an application
using a component on a node, we need to derive the re-
rl of
lationship between ˆte and the total rates rt = Pl∈1...a
all a applications currently using components on that node.
While for increasing rt one expects ˆte to increase, the trend
of the increase is not clear without making any assumptions
regarding the arrival pattern of the data tuples. We approx-
imate the relationship using linear regression and our ex-
perimental results show good ﬁtting for increasing rates.
Figures 14, 15 show the relationships between the execu-
tion times of different components of a stream processing
application and the rates of the applications currently run-
ning on the nodes hosting them, obtained from our imple-
mentation over Planetlab. Linear relationship of execution
time and rate is also consistent with earlier works [23, 24].
Each node maintains a
series of (te, rt) pairs, for
each application a compo-
nent of which the node is
hosting. The series is main-
tained as a sliding window
of the k most recent val-
ues. The execution time is
measured every time a data
tuple for an application is
processed, while the total
rate is measured as the sum
of rates of all applications,
data tuples of which were
processed since the last time a data tuple of that application
was processed. If the rate of any application increases, it
affects the execution time of other applications on the same
node due to queueing delays. We estimate the conditional
expected value of te, given a predicted value for rt. We
use linear regression, and assuming we have k pairs so far,
the linear function is te = a + b · rt and the least square
estimators a and b are:
Figure 2. Linear
regression.
a = ¯te − b · ¯rt
b = Pj∈1...k
(rt(j) − ¯rt) · (te(j) − ¯te)
Pj∈1...k
where the average values ¯te and ¯rt are:
(rt(j) − ¯rt)2
te(j)
¯te = Pj∈1...k
k
rt(j)
¯rt = Pj∈1...k
k
(3)
(2)
In order to enable proactive hot-spot detection, we base the
prediction of the execution time ˆte of an application on the
predicted rates of the applications running on components
ˆrl. (We explain how ˆrl for an appli-
of the node, ˆrt = Pl∈1...a
cation l is derived in the following Section 3.3.) Assuming
an estimated value for the next ˆrt, we predict ˆte using the
above equations. Speciﬁcally, as shown in Figure 2, we use
the k pairs of (te, rt) values to calculate a and b and then
given an estimated ˆrt we predict ˆte using the following for-
mula:
(4)
ˆte = a + b · ˆrt
To evaluate the accuracy of our execution time prediction
we calculate the estimated standard error of the slope b:
se(b) =
vuuuut
Pj∈1...k
(te(j) − ¯te)2 − b Pj∈1...k
(rt(j) − ¯rt)(te(j) − ¯te)
(k − 2) Pj∈1...k
(rt(j) − ¯rt)2
(5)
If the estimated standard error se(b) is above a heuristically
set conﬁdence level C, we do not employ execution time
prediction. Instead we report the last measured application
execution time value rather than a predicted future one. In
general however the last measured value is not an accurate
predictor, as it ignores the current rate.