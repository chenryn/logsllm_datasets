vector is used to calculate performability and mean rate to
degraded states. The third reward vector is used to
calculate annual service cost and mean service call rate.
The interval system availability and performability are the
expected accumulated reward during interval (0, T) using
the first two reward vectors, respectively. The interval
system service cost, SSC(T), for (0, T) is calculated by
M
S S C 	T 	  
i 1
A V F i 	T 	GS C i
	8 	
where AVFi(T) denotes the average annual visit frequency
to state i during interval (0, T), SCi the third reward value
for state i, and M the number of states in the model. SCi
could be an actual cost in terms of dollar amount. If it is
set to 1 for the states where a service action is taken, then
the annual service cost is just the annual service call rate.
Fig. 2 is a model with deferred repair, cold swap, and
non-transparent recovery. When a CPU fails, the system
does an automatic recovery by a reboot during which the
failed CPU is deconfigured from the system and a short
downtime is experienced. When the second CPU fails, a
repair action is scheduled at an off-peak time, which is
modeled by a waiting time (Twaiting). The states Ok, 1
Dead and 2 Dead are working states, so the first reward
value is set to 1 for these states and to 0 for the other
states. Assume there are initially 10 CPUs in the system
and the full performance is thus defined as 10 (the second
reward value). In the degraded states 1 Dead and 2 Dead,
the performance number is set to 9 and 8, respectively.
The only service state is Repair for which the third reward
value is set to 1.
Fig. 3 is a model with deferred repair, hot swap, and
transparent recovery. When a fault occurs on a memory
component (e.g., DRAM), the system does an on-line
automatic recovery by replacing the faulty component
with a hot spare and reconstructing data on the spare. A
successful recovery should not incur a system downtime
(transparent). If the recovery is not successful, modeled by
Prf (probability of recovery failure), a system reboot is
needed to do a boot-up reconfiguration which incurs a
short system downtime. In any case, there is no repair
action associated with the first fault until the second fault
occurs.
swap is
The repair can be performed concurrently with the
supported.
system operation because hot
However, an imperfect repair, either due to diagnostic
problems or human error, would bring the system down,
incurring a downtime (Trestore). The imperfect repair is
modeled by the parameter Pre (probability of repair error).
Notice in the working states Ok, 1 Dead, 2 Dead, and
Repair, there is no performance degradation (the second
reward value is 10) because the memory size is not
reduced. Also, there are two service states in the model:
Repair and RepairError. One (Repair) is working state and
another is failure state.
For each of the above submodels, interval metrics
associated with the three reward vectors can be evaluated.
How these metrics are related to or integrated in the
parent system model? Fig. 4 shows the System Model.
The model has only three states: Ok (working state),
CPU_Fail and Mem_Fail (both are failure states). The two
gray color rectangle boxes are the interface to submodels
in which parameter bindings are defined. As shown in the
first box, the overall CPU failure rate, La_cpu, and repair
rate, Mu_cpu, are bound to the CPU Submodel output
Lambda1 and Mu1 which are the submodel equivalent
failure rate and repair rate evaluated based on the first
reward vector. The same binding rule also applies to
La_mem and Mu_mem, as shown in the second box. This
approach of binding output measures of submodel
to
is called explicit
parameters
parameter passing.
in the current model
Figure 4. The System Model
Explicit parameter passing is not sufficient
for
supporting hierarchical modeling when multiple reward
vectors are used to generate multiple metrics from the
same model, as typically required in the system design. To
allow integration of performability measures
from
submodels with the parent model, we define performance
loss to be maximum performance (the largest value of the
third reward vector in the model) minus performability
(expected performance evaluated from the model). If all
submodels are independent, the performance loss as well
as
service cost generated from the submodels are
integratable at the parent model. In RAScad, the following
formulas are used to calculate the interval
system
performance loss (SPL) and system service cost (SSC) for
a parent model:
SPL	T 	  SPLcur 	T 	U
i 
S
SSC 	T 	  SSCcur 	T 	U
i 
S
C2iGSPLi 	T 	
	9	
C3iGSSC i 	T 	
	10	
where SPLcur(T) and SSCcur(T) are the interval system
performance loss and service cost evaluated from the
current Markov diagram (Fig. 4), respectively, SPLi(T)
and SSCi(T) are the interval system performance loss and
service cost evaluated from submodel i, respectively, and
S is the set of submodels of the current model. C2i and C3i
are the coefficients defined by the user in specifying
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:55 UTC from IEEE Xplore.  Restrictions apply. 
i, which allow to count contributions from
submodel
multiple instances of a submodel. If these coefficients are
set to 0, the performance loss and service cost will not be
rolled up and integrated with the parent model. The way
of passing parameters shown in Eq. (9) and (10) is called
implicit parameter passing.
Looking at
the reward vector assignments in the
System Model, we can determine that both SPLcur and
SSCcur are 0. Assume we have only one CPU Submodel
and one Memory Submodel, i.e., C2 and C3 are set to 1
for both models. According to Eq. (9) and (10),
the
System Model performance loss is just
the sum of
performance loss for the two submodels, and the System
Model service cost is just the sum of service cost for the
two submodels. These metrics can continue to roll up if
there is another parent model above the System Model.
4. Analysis of Results
Having discussed the example model and how
parameters are passed from submodels to the parent
model, we analyze results generated from the model to
show the necessity of interval availability evaluation and
advantages of hierarchical modeling in this section. First,
we look at the increment interval failure rate generated
from the CPU submodel and memory submodel, as shown
in Fig. 5 and 6.
Figure 5. Increment Interval Failure Rate 
for CPU Submodel
Figure 6. Increment Interval Failure Rate 
for Memory Submodel
In the figures, the increment interval, T, equals ¼
year or a quarter. The failure rate unit used is FIT which
represents a failure in 109 hours. The increment interval
failure rate for both submodels changes over time. It does
not reach the steady-state failure rate in 10 years. This
quantity is used as an input parameter to the parent model
for constructing the System Model (Fig. 4). To conciliate
the time variance of input parameters and the requirement
for constant transition rates to satisfy the homogeneous
Markov property, it is necessary to apply the “divide and
conquer” approach described in Fig. 1.
Table
1
and
compares
steady-state
interval
availability metrics evaluated from the system level
model. It is obvious that the steady-state results are much
higher than the transient results for the interval of the first
5 years of life time. Table 2 further shows the speed of
convergence from interval results to steady-state results is
slow (not identical in 100 years). These results indicate
the necessity of interval availability evaluation for models
with deferred repair.
Measure
Steady State
Interval (5 years)
Expected Yearly Downtime
3 min. 55 sec.
2 min. 1 sec.
Mean Failure Rate
15,690 FITs
12,602 FITs
Expected Performance
9.47 (CPUs)
9.83 (CPUs)
Mean Service Call Rate
14,607 FITs
6,693 FITs
Table 1. Comparison of Steady-State and
Interval Measures for System Model
Time Interval
Yearly Downtime
Service Call Rate
1 year
10 years
50 years
100 years
Steady State
1 min. 14 sec.
2 min. 35 sec.
3 min. 36 sec.
3 min. 46 sec.
3 min. 55 sec.
1,890 FITs
9,534 FITs
13,467 FITs
14,037 FITs
14,607 FITs
Table 2. Convergence Speed of Interval 
Measures for System Model
Why is there such a large difference between the
steady-state and interval results for the meaningful time
period? A closer look at the state probabilities for the
steady state and a 5-year
the Memory
Submodel in Table 3 can help understand the issue.
interval
for
State
Steady State
Interval (5 years)
Ok
1 Dead
2 Dead
Repair
Reboot1
Reboot2
RepairError
0.494
0.506
2.369E-4
4.936E-6
8.226E-8
8.226E-8
9.871E-7
0.734
0.266
1.242E-4
2.587E-6
1.224E-7
4.315E-8
5.173E-7
Table 3. Comparison of Steady-State and Interval 
State Probabilities for Memory Submodel
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:55 UTC from IEEE Xplore.  Restrictions apply. 
For the interval of the first 5 years of life time, most
systems (73%) in the field would be staying in the Ok
state (no fault) and only 27% percentage of the population
would be in the "1 Dead" state (one fault). But in the
steady state, less than 50% of all systems would be staying
in the Ok state and over 50% of all systems would be in
the "1 Dead" state. However, it would be a very long
process (over 100 years) for these systems to accumulate
faults and to transition to the "1 Dead" state. In reality,
this accumulation process will never reach the steady state
in the system life time.
Table 4 shows the interval (5 years) yearly downtime
and service call rate distribution by submodel. The CPU
Submodel dominates the downtime while the Memory
Submodel dominates the service call rate. This table
shows that
the
identification of RAS bottlenecks in terms of subsystems.
the hierarchical modeling facilitates
Submodel
Yearly Downtime
Service Call Rate
CPU
Memory
1.66 min. (82.2%)
1,520 FITs (22.7%)
21.5 sec. (17.8%)
5,173 FITs (77.3%)
Table 4. Distribution of 5-Year Interval Yearly
Downtime and Service Call Rate by Submodel
5. Conclusions
two contributions
In this paper, we analyzed an example model which
reveals that the traditional steady-state availability metrics
are no longer appropriate for quantifying dependability
for systems incorporating deferred repair. Instead, interval
availability metrics need to be used for such systems to
quantify their RAS behavior in the useful life time. This is
the first study to show the importance of
interval
availability to solving real engineering problems. We
made
in addressing hierarchical
computation of interval availability and related metrics:
1. Proposed and implemented a “divide and conquer”
approach to hierarchical
interval
availability metrics. The approach allows not only
passing interval availability metrics from submodels to
the parent model, but also general time dependent
failure rates as input parameters to the model.
Identified a method to explicitly and implicitly pass up
output quantities from submodels to the parent model
so that multiple interval metrics can be generated from
the same model in one evaluation procedure. This
method is important for developing highly productive
RAS modeling tools.
calculation of
2.
Acknowledgments
The authors thank William Bryson and Robert
Cypher for raising the issue of validity of stead-state
availability in evaluating RAS architectures incorporating
deferred repair and testing interval availability solution
methods implemented in RAScad. The authors also thank
Dazhi Wang for
implementing the example model
discussed in this paper using the SHARPE textual
language which produces same results as those generated
by RAScad.
References
Systems
on Dependable
[1] S. Bose, V. Kumar, and K. S. Trivedi, “Effect of Deferring
the Repair on Availability,” Supplement Volume of the 2003
International Conference
and
Networks, June 2003, pp. B32-B33.
[2] D. C. Bossen, A. Kitamorn, K. F. Reick and M. S. Floyd,
“Fault-Tolerant Design of the IBM pSeries 690 System Using
POWER4 Processor Technology,” IBM J. Res. & Dev. Vol. 46
No. 1, January 2002.
[3] J. B. Dugan, “Automated Analysis of Phased-Mission
Reliability,” IEEE Trans. On Reliability, April 1991, pp. 45-51.
[4] A. Goyal, S. S. Lavenberg and K. S. Trivedi, “Probabilistic
Modeling of Computer System Availability,” Annals of
Operations Research, No. 8, March 1987, pp. 285-306.
[5] A. Goyal and A. N. Tantawi, “A Measure of Guaranteed
Availability and its Numerical Evaluation,” IEEE Trans. on
Computers, January 1988, pp. 25-32.
[6] M. Lanus, L. Yin, and K. S. Trivedi, "Hierarchical
Composition and Aggregation of State-Based Availabilityand
Performability Models," IEEE Trans. on Reliability, March
2003, pp. 44-52.
[7] S. Ramani, S. Gokhale and K. S. Trivedi, “SREPT: Software
Reliability Estimation and Prediction Tool,” Performance
Evaluation, Vol. 39, 2000, pp. 37-60.
[8] A. Reibman, R. Smith and K. S. Trivedi, “Markov and
Markov Reward Model Transient Analysis: An Overview of
Numerical Approaches,” European Journal of Operational
Research, Vol. 40, 1989, pp. 257--267.
[9]
"Relex Markov,"
http://www.relexsoftware.com/products/markov.asp, Sept. 2002.
[10] G. Rubino and B. Sericola, “Interval Availability Analysis
Using Denumerable Markov Processes: Application
to
to Breakdowns and Repair,” IEEE
Multiprocessor Subject
Trans. on Computers, February 1995, pp. 286-291.
[11] R. A. Sahner, K. S. Trivedi and A. Puliafito, Performance
and Reliability Analysis of Computer Systems - An Example-
Based Approach using the SHARPE Software Package, Kluwer
Academic Publishers, 1996.
[12] W. H. Sanders, W. D. Obal II, M. A. Qureshi and F. K.
Widjanarko,
Environment,”
Performance Evaluation, Oct./Nov. 1995, pp. 89-115.
[13] B. Sericola, “Availability Analysis of Repairable Computer
Systems
on
Computers, November 1999, pp. 1166-1172.
[14] D. Tang, M. Hecht, J. Miller and J. Handal, “MEADEP - A
Dependability Evaluation Tool for Engineers,” IEEE Trans. on
Reliability, Dec. 1998, pp. 443-450.
[15] D. Tang, J. Zhu and R. Andrada, “Automatic Generation of
Availability Models in RAScad,” Proceedings of International
Conference on Dependable Systems and Networks, 2002, pp.
488-492.
[16] K. S. Trivedi, Probability and Statistics with Reliability,
Queuing, and Computer Science Applications, 2nd Edition,
John Wiley & Sons, 2002.
and Stationarity Detection,”
“The UltraSAN Modeling
IEEE Trans.
Relex
Software
Corporation,
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:55 UTC from IEEE Xplore.  Restrictions apply.