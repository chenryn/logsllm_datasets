m
r
o
N
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
Baseline
ByzCast
36.8
(msgs/sec)
38.8
(msgs/sec)
34.6
(msgs/sec)
38.3
(msgs/sec)
CA
EU
JP
VA
Fig. 9: Normalized throughput with mixed workload in a WAN.
protocols that do not implement all the properties of atomic
multicast (e.g., [29], [39], [40]). We focus next on atomic
multicast algorithms that tolerate benign failures, since no
atomic multicast algorithm exists for Byzantine failures.
Existing atomic multicast algorithms fall into one of three
categories:
timestamp-based, round-based, and ring-based.
Algorithms based on timestamps (i.e., [8], [9], [15], [36])
are genuine and variations of an early atomic multicast
algorithm [41], designed for failure-free systems. In these
algorithms, processes assign timestamps to messages, ensure
that destinations agree on the ﬁnal timestamp assigned to
each message, and deliver messages following this timestamp
order. The algorithm in [9] ensures another property besides
genuineness called message-minimality. This property states
that the messages of the algorithm have a size proportional
to the number of destination groups of the multicast message,
and not to the total number of processes. Although ByzCast
is not genuine with respect to global messages, it satisﬁes this
48
 100
 80
 60
]
%
[
 40
 20
 0
 0
 100
 80
 60
]
%
[
 40
 20
 0
 0
ByzCast
 200
 400
 600
 800
 1000
Latency [msec]
Baseline
 200
 400
 600
 800
 1000
Latency [msec]
(a) Local messages.
CA - Local
EU - Local
JP - Local
VA - Local
 1400
 1200
CA - Local
EU - Local
JP - Local
VA - Local
 1400
 1200
 100
 80
 60
]
%
[
 40
 20
ByzCast
 1600
 0
 0
 200
 400
 600
 800
 1000
Latency [msec]
 100
 80
 60
]
%
[
 40
 20
Baseline
 1600
 0
 0
 200
 400
 600
 800
 1000
Latency [msec]
(b) Global messages.
CA - Global
EU - Global
JP - Global
VA - Global
 1400
 1200
 1600
CA - Global
EU - Global
JP - Global
VA - Global
 1400
 1200
 1600
Fig. 10: Latency CDF with 40 clients per group and 10% of global messages.
property for local messages which can be delivered as fast as
the underlying atomic broadcast algorithm.
In round-based algorithms, processes execute an unbounded
sequence of rounds and agree on messages delivered at the end
of each round. A round-based atomic multicast algorithm that
can deliver messages in 4δ is presented in [36]. Differently
from ByzCast, this algorithm may penalize local messages, as
they may be slowed down by global messages.
Ring-based algorithms propagate messages along a prede-
ﬁned ring overlay and ensure atomic multicast properties by
relying on this topology. An atomic multicast algorithm in this
category is proposed in [10], where consensus is run among the
members of each group. The time complexity of this algorithm
is proportional to the number of destination groups. Multi-
Ring Paxos [12], Spread [11], [14], and Ridge [13] are ring-
based non-genuine atomic multicast protocols. On the one
hand, to deliver a message m, they require communication
with processes outside of the destination groups of m and
local messages may also suffer from convoy effect. On the
other hand, these protocols do not require disjoint groups.
B. Scalable BFT
Despite the large amount of work on BFT replication in
the last two decades (e.g., [17], [18], [19], [20], [21], [42],
[43], [44], [45], [46]), the scalability of BFT protocols is still
a relatively unexplored topic, which we discuss in this section.
A common observation of BFT protocols is that their perfor-
mance degrades signiﬁcantly as the number of faults tolerated
increase [43]. This lack of fault-scalability comes mostly from
the all-to-all communication used in these protocols, which
implies in a quadratic amount of messages. This limitation
can be mitigated either by using protocols with linear message
pattern [42], [43], [44], by using protocols with a smaller ratio
between n and f [45], [46], or by exploring erasure codes and
large message batches [21]. Independently on the trade-offs
explored by these protocols, all of them lose performance as
the number of replicas increase, contrary to ByzCast.
There are few BFT protocols that target wide-area net-
works [19], [20]. These protocols tend to use more replicas
to decrease the relative quorum size or the distance between
replicas in the quorums. Similarly to the scalable protocols
described before, the performance of these protocols tends to
decrease with the number of replicas.
The natural way of scaling replicated systems is sharding the
state in multiple replica groups and running ordering protocols
only in these groups. To the best of our knowledge, there
are only three works that consider partitionable replication for
BFT systems. Augustus [47] and Callinicos [48] introduces
protocols for executing transactions in multiple shards of a
key-value store implemented on top of multiple BFT groups.
A recent work by Nogueira et al. [49] introduces protocols for
splitting and merging replica groups in BFT-SMaRt, without
discussing ways to disseminate messages to more than one of
these groups with Byzantine failures. ByzCast complements
these works by providing a protocol for disseminating requests
on multiple partitions, enabling thus the efﬁcient support for
services that require multi-partition operations.
VII. CONCLUSION
Atomic multicast is a fundamental communication abstrac-
tion in the design of scalable and highly available strongly
consistent distributed systems. This paper proposes ByzCast,
49
the ﬁrst Byzantine Fault-Tolerant atomic multicast, designed
to build on top of existing BFT abstractions. ByzCast
is
partially genuine, i.e., it scales linearly with the number of
groups, for messages adressed to a single group. In addition
to introducing a novel atomic multicast algorithm, we also
assessed its performance in two different environments. The
results show that ByzCast outperforms BFT-SMaRt in most
cases, as well as a non-genuine BFT atomic multicast protocol.
ACKNOWLEDGEMENTS
Scholarships, Hasler
We thank the reviewers for the constructive suggestions.
This work is supported in part by the Swiss Government
Excellence
CNPq
204558/2014-0), CAPES (PVE Project
(GDE Project
88887.124751/2014-00),
projects
LaSIGE (UID/CEC/00408/2013) and IRCoC (PTDC/EEI-
SCR/6970/2014).
Foundation,
and
FCT
through
REFERENCES
[1] J. Baker, C. Bond, J. Corbett, J. J. Furman, A. Khorlin, J. Larson, J.-
M. Leon, Y. Li, A. Lloyd, and V. Yushprakh, “Megastore: Providing
scalable, highly available storage for interactive services,” in CIDR,
2011.
[2] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Bur-
rows, T. Chandra, A. Fikes, and R. E. Gruber, “Bigtable: A distributed
storage system for structured data,” ACM Trans. on Computer Systems,
vol. 26, no. 2, 2008.
[3] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman,
A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels, “Dynamo:
Amazon’s highly available key-value store,” in SOSP, 2007.
[4] Y. Sovran, R. Power, M. K. Aguilera, and J. Li, “Transactional storage
for geo-replicated systems,” in SOSP, 2011.
[20] Y. Amir, C. Danilov, D. Dolev, J. Kirsch, J. Lane, C. Nita-Rotaru,
and D. Z. Josh Olsen, “STEWARD: Scaling Byzantine fault-tolerant
replication to wide area networks,” IEEE Trans. on Dependable and
Secure Computing, vol. 7, no. 1, 2010.
[21] A. Miller, Y. Xia, K. Croman, E. Shi, and D. Song, “The honey badger
of BFT protocols,” in CCS, 2016.
[22] R. Guerraoui and A. Schiper, “Genuine atomic multicast
in asyn-
chronous distributed systems,” Theoretical Computer Science, vol. 254,
no. 1-2, pp. 297–316, 2001.
[23] I. Moraru, D. G. Andersen, and M. Kaminsky, “There is more consensus
in egalitarian parliaments,” in SOSP, 2013.
[24] D. Ongaro and J. K. Ousterhout, “In search of an understandable
consensus algorithm.,” in USENIX ATC, 2014.
[25] A. Turcu, S. Peluso, R. Palmieri, and B. Ravindran, “Be general and
don’t give up consistency in geo-replicated transactional systems,” in
OPODIS, 2014.
[26] L. Lamport, R. Shostak, and M. Pease, “The Byzantine generals prob-
lem,” ACM Trans. on Programming Languages and Systems (TOPLAS),
vol. 4, no. 3, pp. 382–401, 1982.
[27] M. Castro and B. Liskov, “Practical byzantine fault
tolerance and
proactive recovery,” ACM Trans. on Computer Systems (TOCS), vol. 20,
no. 4, pp. 398–461, 2002.
[28] V. Hadzilacos and S. Toueg, “A modular approach to fault-tolerant
broadcasts and related problems,” tech. rep., Cornell University, 1994.
[29] J. C. C. et al., “Spanner: Google’s globally distributed database,” in
OSDI, 2012.
[30] C. E. Bezerra, F. Pedone, and R. van Renesse, “Scalable state-machine
replication,” in DSN, 2014.
[31] B. Li, W. Xu, M. Z. Abid, T. Distler, and R. Kapitza, “SAREK:
optimistic parallel ordering in Byzantine fault tolerance,” in EDCC,
2016.
[32] L. L. Hoang, C. E. B. Bezerra, and F. Pedone, “Dynamic scalable state
machine replication,” in DSN, 2016.
[33] J. Sousa, A. Bessani, and M. Vukolic, “A Byzantine fault-tolerant
ordering service for the hyperledger fabric blockchain platform,” in DSN,
2018.
[34] J. Sousa and A. Bessani, “From Byzantine consensus to BFT state
machine replication: A latency-optimal transformation,” in EDCC, 2012.
[35] C. Cachin, “Yet another visit to Paxos,” Tech. Rep. RZ 3754, IBM
Research Zurich, 2009.
[5] M. P. Herlihy and J. M. Wing, “Linearizability: A correctness condition
for concurrent objects,” Trans. on Programming Languages and Systems,
vol. 12, pp. 463–492, July 1990.
[36] N. Schiper and F. Pedone, “On the inherent cost of atomic broadcast
and multicast in wide area networks,” in ICDCN, 2008.
[37] N. Schiper, P. Sutra, and F. Pedone, “P-Store: Genuine partial replication
[6] L. Lamport, “Time, clocks, and the ordering of events in a distributed
in wide area networks,” in SRDS, 2010.
system,” CACM, vol. 21, pp. 558–565, July 1978.
[7] F. Schneider, “Implementing fault-tolerant services using the state ma-
chine approach: A tutorial,” ACM Computing Surveys, vol. 22, pp. 299–
319, Dec. 1990.
[38] X. D´efago, A. Schiper, and P. Urb´an, “Total order broadcast and
multicast algorithms: Taxonomy and survey,” ACM Computing Surveys,
vol. 36, no. 4, pp. 372–421, 2004.
[39] J. Cowling and B. Liskov, “Granola: Low-overhead distributed transac-
[8] U. Fritzke, P. Ingels, A. Most´efaoui, and M. Raynal, “Fault-tolerant total
tion coordination,” in USENIX ATC, 2012.
order multicast to asynchronous groups,” in SRDS, 1998.
[40] D. Sciascia, F. Pedone, and F. Junqueira, “Scalable deferred update
[9] L. Rodrigues, R. Guerraoui, and A. Schiper, “Scalable atomic multicast,”
replication,” in DSN, 2012.
in IC3N, 1998.
[10] C. Delporte-Gallet and H. Fauconnier, “Fault-tolerant genuine atomic
multicast to multiple groups,” in OPODIS, 2000.
[11] D. A. Agarwal, L. E. Moser, P. M. Melliar-Smith, and R. K. Budhia,
“The totem multiple-ring ordering and topology maintenance protocol,”
ACM Trans. on Computer Systems, vol. 16, pp. 93–132, May 1998.
[12] P. J. Marandi, M. Primi, and F. Pedone, “Multi-ring paxos,” in DSN,
2012.
[13] E. Bezerra, D. Cason, and F. Pedone, “Ridge: high-throughput, low-
latency atomic multicast,” in SRDS, 2015.
[14] A. Babay and Y. Amir, “Fast total ordering for modern data centers,” in
ICDCS, 2016.
[15] P. Coelho, N. Schiper, and F. Pedone, “Fast atomic multicast,” in DSN,
2017.
[16] C. Cachin and M. Vukolic, “Blockchain consensus protocol in the wild
(invited paper),” in DISC, 2017.
[17] M. Castro and B. Liskov, “Practical Byzantine fault-tolerance and
proactive recovery,” ACM Trans. on Computer Systems, vol. 20, no. 4,
pp. 398–461, 2002.
[18] A. Bessani, J. Sousa, and E. Alchieri, “State machine replication for the
masses with BFT-SMaRt,” in DSN, 2014.
[19] J. Sousa and A. Bessani, “Separating the WHEAT from the chaff: An
empirical design for geo-replicated state machines,” in SRDS, 2015.
[41] K. Birman and T. Joseph, “Reliable communication in the presence of
failures,” Trans. on Computer Systems, vol. 5, pp. 47–76, Feb. 1987.
[42] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong, “Zyzzyva:
Speculative Byzantine fault tolerance,” ACM Trans. on Computer Sys-
tems, vol. 27, no. 4, 2009.
[43] M. Abd-El-Malek, G. Ganger, G. Goodson, M. Reiter, and J. Wylie,
“Fault-scalable Byzantine fault-tolerant services,” in SOSP, 2005.
[44] P.-L. Aublin, R. Guerraoui, N. Kneˇzevi´c, V. Qu´ema, and M. Vukoli´c,
“The next 700 BFT protocols,” ACM Trans. on Computer Systems,
vol. 32, no. 4, pp. 12:1–12:45, 2015.
[45] S. Liu, P. Viotti, C. Cachin, V. Qu´ema, and M. Vukolic, “XFT: practical
fault tolerance beyond crashes,” in OSDI, 2016.
[46] G. S. Veronese, M. Correia, A. Bessani, L. C. Lung, and P. Veris-
simo, “Efﬁcient Byzantine fault-tolerance,” IEEE Trans. on Computers,
vol. 62, no. 1, 2013.
[47] R. Padilha and F. Pedone, “Augustus: Scalable and robust storage for
cloud applications,” in EuroSys, 2013.
[48] R. Padilha, E. Fynn, R. Soul´e, and F. Pedone, “Callinicos: Robust
transactional storage for distributed data structures,” in USENIX ATC,
2016.
[49] A. Nogueira, A. Casimiro, and A. Bessani, “Elastic state machine
replication,” IEEE Trans. on Parallel and Distributed Systems, vol. 28,
no. 9, 2017.
50