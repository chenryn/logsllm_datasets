immediate operands can be safely modiﬁed.
First, depending on the instruction type, immediates can be
modiﬁed by splitting up their parent instructions. For instance,
an addition can be split into two additions or subtractions, where
the ﬁrst takes an arbitrary operand, and the second compensates
as required. Similarly, immediate operands of mov instructions
can be modiﬁed to encode a gadget, and this modiﬁcation
can then be compensated for using bitwise operations on the
destination operand. As an example of this rule, Listing 3 shows
how the immediate operand of a mov instruction is modiﬁed
and combined with an xor instruction to compensate for the
modiﬁcations.
Instruction splitting induces a small performance overhead
on the protected code. Additionally, it may require the insertion
of code to save and restore the CPU status register.
Second, it is generally possible to freely modify immediates
which set eax before a return, or push the status of the exit
function. This is because return value and exit status semantics
commonly distinguish only between zero and non-zero. This
rule can be disabled for conﬂicting semantics.
3) Rearranged code and data: Parallax also attempts
to encode missing parts of gadgets in addresses and jump
offsets by strategically aligning functions and global variables.
For instance, in the example shown in Listing 1, we have
forced the creation of a ret instruction by aligning the
cleanup_and_exit function such that the jump offset at
address n+79 is equal to 0xc3 (the ret opcode).
4) Spurious instructions: Spurious instructions which con-
tain (parts of) gadgets can be inserted at any place in the code,
as long as care is taken to ensure that their side-effects do not
inﬂuence the semantics of the original code. This can be ensured
by saving and restoring the program state at each location
where spurious instructions are inserted. Alternatively, side-
effect analysis can be performed on the inserted instructions
using frameworks such as BAP [9].
The main beneﬁt of spurious instructions is that they
can always be inserted to encode missing parts of gadgets.
However, because spurious instructions induce a slowdown on
the protected code, it is best to avoid them if possible.
5) Far-return gadgets: Far returns (retf) are quite rare
in compiler-generated x86 code. Nevertheless, gadgets ending
in far returns can sometimes be used to protect code bytes, as
was done at address n+66 in Listing 1. Parallax searches for
existing far-return gadgets in the same way as for near-return
gadgets.
6) Using add for memory operations: One of the most
useful instruction families for gadgets is the add family. This
is because the opcodes of add range from 0x00 to 0x05,
which are very common values in immediate operands. Listing 1
contains several gadgets which use add instructions, such as
the gadget protecting the call to ptrace.
Next to implementing additions, add instructions with
memory operands can also be used as loads and stores. For
instance, add [ecx],eax is a store of the value in eax to
the address in ecx, if this memory is initially zero.
V. VERIFYING CODE INTEGRITY
In Section IV, we discussed the creation of overlapping
gadgets for code protection. To protect their parent instructions,
the integrity of these gadgets must be veriﬁed by one or more
ROP chains. In this section, we discuss the translation of
existing (source or binary) code from the protected program
into ROP chains which act as veriﬁcation code. These ROP
chains use the gadgets contained in the protected code regions.
We stress that the veriﬁcation code does not perform any
active veriﬁcation, checksumming or otherwise. Instead, it
detects and responds to tampering in a completely passive way,
by malfunctioning if the gadgets in protected code regions
are damaged. We implement veriﬁcation code at function
granularity, meaning that whole functions from the original
program are translated to ROP code. For brevity, we refer to a
function-level veriﬁcation ROP chain as a function chain. In
Section V-C, we also brieﬂy report on our experiences with
instruction-level veriﬁcation.
A. Implementation of Function Chains
Function chains were already brieﬂy discussed in Section III.
Figure 3a illustrates a binary protected using function chains.
As discussed in Section III, the protected binary contains several
gadgets g1, g2, g3, which are crafted such that they overlap with
instructions which must be protected. Furthermore, a selected
function f1 from the protected binary’s code section is translated
into equivalent ROP veriﬁcation code, denoted as ROP (f1).
Additionally, a small amount of loader code is inserted, which
is responsible for starting the execution of the veriﬁcation code.
The minimum operations required for this are (1) pointing the
stack pointer to the beginning of ROP (f1), and (2) executing
a return instruction to transfer control to the ﬁrst gadget in the
veriﬁcation code.
In our Parallax prototype, we implemented function-level
veriﬁcation on top of a modiﬁed version of the open source ROP
compiler ROPC [2], which is based on Q [32]. Our prototype
loader code, which bootstraps the execution of the function
chains, is slightly more extensive than shown in Figure 3a.
Particularly, in addition to pointing the stack pointer to the start
129129
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:51:18 UTC from IEEE Xplore.  Restrictions apply. 
(cid:74)(cid:20)
(cid:74)(cid:21)
(cid:80)(cid:82)(cid:89)(cid:3)(cid:72)(cid:86)(cid:83)(cid:15)(cid:53)(cid:50)(cid:51)(cid:11)(cid:73)(cid:20)(cid:12)
(cid:85)(cid:72)(cid:87)
(cid:53)(cid:50)(cid:51)(cid:11)(cid:73)(cid:20)(cid:12)
(cid:74)(cid:22)
(cid:83)(cid:85)(cid:82)(cid:87)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)
(cid:11)(cid:53)(cid:50)(cid:51)(cid:11)(cid:73)(cid:20)(cid:12)(cid:12)
(cid:74)(cid:20)
(cid:74)(cid:21)
(cid:83)(cid:88)(cid:86)(cid:75)(cid:3)(cid:85)(cid:20)
(cid:83)(cid:88)(cid:86)(cid:75)(cid:3)(cid:74)(cid:20)
(cid:85)(cid:72)(cid:87)
(cid:83)(cid:88)(cid:86)(cid:75)(cid:3)(cid:85)(cid:21)
(cid:83)(cid:88)(cid:86)(cid:75)(cid:3)(cid:74)(cid:21)
(cid:85)(cid:72)(cid:87)
(cid:74)(cid:22)
(cid:83)(cid:85)(cid:82)(cid:87)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)
(cid:11)(cid:151)(cid:16)(cid:70)(cid:75)(cid:68)(cid:76)(cid:81)(cid:86)(cid:12)
(cid:85)(cid:20)(cid:29)
(cid:85)(cid:21)(cid:29)
(a) A function chain.
(b) Scattered μ-chains.
Fig. 3: Veriﬁcation at function and instruction level.
of a function chain and executing a return, we also ensure that
execution continues cleanly after the function chain is complete.
To achieve this, the loader code appends an epilogue to each
function chain before transferring control to it. The epilogue
consists of the address of a pop esp gadget, followed by a
stack address in the original stack frame of the calling function.
At this stack address, the return address for the function chain
is stored. Before the epilogue’s pop esp gadget is executed,
the stack pointer points inside the function chain. The pop
esp points it back into the calling function frame, to the stack
location containing the function chain’s return address. Now,
when the function chain returns, this transfers control back to
the calling function, and program execution continues normally.
In addition to the epilogue, we perform a pushad directly
before, and a popad directly after each function chain. These
instructions save and restore the register state, preventing
problems due to registers clobbered by the function chain.
B. Dynamically Generated Function Chains
Function chains can reside in data memory, which is
writable even with W⊕X protection on. This means that it
is possible to generate function chains at runtime. Parallax
implements optional support for this. Dynamic function chain
generation has several advantages. (1) It allows for encrypted
and self-modifying function chains, which are more resistant
to analysis than their non-dynamic counterparts. We evaluate
the performance of RC4-encrypted and xor-encrypted function
chains in Section VII-B. (2) Multiple instances of the same
function chain can be generated probabilistically, with each
instance using a different set of semantically equivalent gadgets.
This allows a small function chain to verify a large set of
gadgets, checking a subset each time it is executed. The tradeoff
of this approach is that the protection of each gadget becomes
probabilistic, rather than deterministic.
Speciﬁcally, let T := {t1, . . . , tn} be the set of used gadget
types in the function chain, and for 1 ≤ i ≤ n, deﬁne Gi :=
{g | g implements ti}. Thus, each Gi is the set of all gadgets
which implement gadget type ti. For probabilistically generated
function chains, we use an extended notion of the gadget types
mentioned in Section III, which deﬁnes not only the operation
implemented by a gadget, but also its operand registers and
memory locations. Then, for every operation, the function
(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)
(cid:36)(cid:21)
(cid:53)(cid:50)(cid:51)(cid:11)(cid:73)(cid:12)
(cid:77)(cid:20)(cid:15)(cid:17)(cid:17)(cid:17)(cid:15)(cid:77)(cid:78)
(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)
(cid:36)(cid:20)
(cid:69)(cid:20)
(cid:69)(cid:21)
(cid:69)(cid:90)
(cid:37)
Fig. 4: Generating a function chain by combining vectors from a basis B,
indexed by arrays A1 and A2.
i=1
(cid:2)n
chain can probabilistically choose a gadget g ∈ Gi. In total,
|Gi| possible distinct gadget subsets which
this yields
can be checked by the same function chain. Because the used
subset of gadgets is probabilistic, it is hard for an adversary
to be sure that his code modiﬁcations will work for every
execution of the program. This is an especially useful property
to protect against software cracking, where adversaries aim to
widely distribute modiﬁed applications.
Parallax implements probabilistically generated function
chains by considering each function chain as a series of vectors
v1, . . . , vn, where vi ∈ {0, 1}w
for 1 ≤ i ≤ n. Here, w is
the native memory word length in bits (typically 32 or 64).
Intuitively, each vector in a function chain corresponds to a
gadget address or constant used by the chain (all constants
in our function chains are word-sized). Each vector can be
generated using a linear combination of vectors from a basis
B = {b1, . . . , bw} which spans the vector space {0, 1}w
.
To support dynamic generation of multiple variants of the
same function chain, we deﬁne a series of N index arrays
A1, . . . , AN , such that each Ai for 1 ≤ i ≤ N is a two-
dimensional array of vector indices. The number of index arrays
N can be arbitrarily chosen. If the function chain contains l
vectors, then each Ai stores l lists of vector indices. If the
l-th vector from the function chain is of gadget type t, then
the l-th index list in each Ai contains indices j1, . . . , jk which
∈
index vectors bj1 , . . . , bjk from B such that bj1
{g | g implements t}. This means we can form N semantic
equivalents for each vector in a function chain by choosing
randomly between A1, . . . , AN and combining the basis vectors
speciﬁed there. Figure 4 illustrates this approach to generating
function chains. For a function chain of length l, there exist
at most N l
variants (assuming that no two Ai store the same
index list at any position).
⊕, . . . , ⊕bjk
We generate the index arrays by repeatedly compiling the
function chain, each time feeding a different gadget mapping
to the ROP compiler. By varying the set of gadget addresses
used in each mapping, we obtain different compiled variants
of the function chain. We then split each vector from every
compiled variant into basis vectors, and store the indices of
these in the index arrays. Note that the compiled function
chains themselves are not stored in the binary, as shown in
Figure 4. Instead, we use the index arrays to probabilistically
generate a function chain variant at runtime, just before the
function chain is called. Since we randomly choose between
the index arrays at vector granularity, the possible number of
130130
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:51:18 UTC from IEEE Xplore.  Restrictions apply. 
function chain variants generated at runtime is greater than the
number of compiled variants.
In Section VII-B, we discuss our performance experiments
on dynamic function chain generation. We report results
for function chains encrypted with RC4 and xor, and for
probabilistically generated function chains.
C. Instruction-Level Veriﬁcation
In addition to function-level veriﬁcation, we also experi-
mented with instruction-level veriﬁcation. Instead of translating
a whole function, this approach translates many single instruc-
tions into short ROP chains, which we refer to as μ-chains.
Figure 3b compares μ-chains to function chains.
We ﬁnd μ-chains to be suboptimal for several reasons.
(1) To minimize control transfer overhead, μ-chains are best
implemented inline in the code section, as shown in Figure 3b.
This means that, unlike function chains, μ-chains cannot beneﬁt
from additional protection by checksumming (due to the attack
of Wurster et al. [36]) or self-modiﬁcation. (2) The inline
gadget setup instructions used by μ-chains can be detected
through static analysis, and can be exploited by an adversary
to pinpoint gadgets used for protection. (3) The overhead of
μ-chains exceeds that of function chains by a factor of 2×
on average, because each μ-chain contains its own prologue
and epilogue. For these reasons, we focus on function-level
veriﬁcation in this paper.
VI. ATTACK RESISTANCE
This section discusses the resistance of our technique to
attacks which attempt to disable, circumvent, or tamper with
the veriﬁcation code. As mentioned, the veriﬁcation code is a
translation to ROP of code from the original program, which
is required for the program to correctly execute. The challenge
for an adversary is thus to tamper with the protected program
in such a way that this is not detected by the veriﬁcation code,
without modifying the veriﬁcation code functionality. The rest
of this section discusses three attack classes.
A. Code Restoration
An adversary may attempt to evade detection by restoring
modiﬁed code after it has executed. Such code restore attacks
are only relevant in dynamic (runtime) tampering. For static
attack scenarios, as in software cracking, adversaries cannot use
code restore attacks. It is well-recognized in the literature that no
self-sufﬁcient tamperprooﬁng algorithm can completely prevent
code restore attacks [14]. However, Parallax complicates such
attacks in several ways. (1) It is critical to use veriﬁcation
functions which are executed repeatedly through the runtime
of the protected application. As we show in Section VII-B,
Parallax achieves this while keeping performance overhead low
(up to 4%). (2) By decoupling veriﬁcation code from protected
code, Parallax maximizes the difﬁculty for an adversary to