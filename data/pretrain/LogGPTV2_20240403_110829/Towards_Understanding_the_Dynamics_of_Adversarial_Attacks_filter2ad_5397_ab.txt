### An Empirical Study on the Dynamic Properties of Adversarial Inputs

Our study aims to provide a new perspective on key questions regarding adversarial inputs.

#### 3.1 Experimental Setting

We primarily use the CIFAR-10 dataset [7] and a DNN model from [12], which achieves an accuracy of 90.24% on CIFAR-10. Our focus is on untargeted attacks. For targeted attack models (e.g., JSMA), we select the class \(\hat{y}\) (different from the ground-truth class \(y\)) that requires the minimum perturbation as the target class. To ensure a fair comparison, we require different attack models to have similar perturbation magnitudes.

In our study, given a DNN model \(f\) and an input \(x\), we collect \(x\)'s feature maps, measure its source and target mutual information (MI) matrices \(\{S(k), T(k)\}_k\), and compute \(x\)'s information profiles (IIP, OIP, and IOC) within \(f\).

#### 3.2 Experimental Results

We present the results of our empirical study and report our findings.

**Q1: How are adversarial inputs crafted to trigger DNNs to misbehave?**

Figure 1 illustrates the process of a JSMA attack on a randomly selected input by visualizing the information flows of each intermediate adversarial input \(x_i\). We observe that the information flows of adversarial inputs deviate from those of benign ones, with the attack process essentially shifting the information flows away from benign inputs towards adversarial ones.

![Figure 1: Top: IPs of a randomly selected input. The black line represents the benign input \(\hat{x}_0\); lighter colors indicate larger \(i\); the line of the lightest color represents the adversarial input \(\hat{x}_n\). Bottom: the overall difference between IPs of \(\hat{x}_i\) and \(\hat{x}_0\).](figure1.png)

**Q2: How do various attack mechanisms differ in generating adversarial inputs?**

Figure 2 (left) compares the aggregated IIPs, OIPs, and IOCs of benign inputs and adversarial inputs generated by different attacks, including FGSM [5], IGSM [8], DeepFool [13], and C&W [4]. The results show that adversarial inputs generated by different attack models lead to significantly different information flows, suggesting that multiple defense or detection methods may be necessary to mitigate different types of attacks.

**Q3: Why are existing defense mechanisms often vulnerable to adaptive attacks?**

We compute the aggregated IPs of adversarial inputs (both successful and failed) generated by regular and adaptive JSMA [2] on a defensively distilled DNN model [15]. Figure 2 (right) shows that from the IP perspective, adversarial inputs generated by adaptive JSMA deviate much further from benign inputs than those generated by regular JSMA, explaining why defensive distillation fails to defend against adaptive attacks.

**Q4: Are more complex DNN models more vulnerable to adversarial inputs?**

Figure 3 shows the aggregated IPs of benign inputs and adversarial inputs generated by JSMA. For comparison, we also compute the IPs of adversarial inputs targeting both the original and compressed DNN models. We observe that it is easier to cause information flows to shift from benign inputs in more complex DNNs, indicating that adversarial inputs demonstrate higher transferability on the original model (i.e., more complex models tend to be more vulnerable to adversarial inputs).

**Q5: How do transferable adversarial inputs differ from non-transferable ones?**

Figure 4 shows the aggregated IPs of adversarial inputs targeting three DNNs: "good_init" [12], "deep_conv", and "vgg_like". We observe that the OIPs of adversarial inputs targeting the inference model deviate further from benign inputs compared to those targeting a different model. Additionally, the OIPs of transferable adversarial inputs [18] deviate much further than non-transferable ones. Therefore, to create transferable adversarial inputs, it is sensible to attack ensemble models [10] (i.e., training on the three DNNs simultaneously).

![Figure 2: Aggregated IPs of benign and adversarial inputs generated by (left) FGSM, IGSM, DeepFool, and C&W attacks (right) by regular and adaptive JSMA attacks.](figure2.png)
![Figure 3: Aggregated IPs of benign and adversarial inputs by JSMA. The solid and dashed blue lines respectively represent adversarial inputs targeting the inference model and the other model.](figure3.png)
![Figure 4: Aggregated IPs of adversarial inputs targeting three DNN models by JSMA. Note that JSMA fails to generate transferable adversarial samples targeting "deep_conv".](figure4.png)

#### 5 Conclusion

In this paper, we present an empirical study on the dynamic properties of adversarial inputs against DNN models. Using a data-driven approach, we measure the information flows of adversarial inputs within various DNN models and conduct an in-depth comparative study on their discriminative patterns. Our study sheds light on key questions surrounding adversarial inputs and points to several promising directions for designing more effective defense mechanisms. We hope that our visualization tool can help researchers better understand the behavior of adversarial samples during DNN model classification.

#### Acknowledgments

This work was supported by the National Science Foundation under Grant Nos. 1566526 and 1718787.

#### References

[1] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the International Conference on Learning Representations (ICLR).

[2] N. Carlini and D. Wagner. 2016. Defensive distillation is not robust to adversarial examples. ArXiv e-prints (2016).

[3] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of ACM Workshop on Artificial Intelligence and Security (AISec).

[4] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[5] I. J. Goodfellow, J. Shlens, and C. Szegedy. 2015. Explaining and harnessing adversarial examples. In Proceedings of the International Conference on Learning Representations (ICLR).

[6] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. 2016. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. ArXiv e-prints (2016).

[7] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning Multiple Layers of Features from Tiny Images. Technical report, University of Toronto (2009).

[8] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533 (2016).

[9] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature 521, 7553 (2015), 436–444.

[10] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2016. Delving into transferable adversarial examples and black-box attacks. ArXiv e-prints (2016).

[11] Dongyu Meng and Hao Chen. 2017. Magnet: a two-pronged defense against adversarial examples. In Proceedings of ACM SAC Conference on Computer and Communications (CCS).

[12] Dmytro Mishkin and Jiri Matas. 2015. All you need is a good init. ArXiv e-prints (2015).

[13] Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016. DeepFool: a simple and accurate method to fool deep neural networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[14] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. 2016. The limitations of deep learning in adversarial settings. In Proceedings of IEEE European Symposium on Security and Privacy (Euro S&P).

[15] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[16] Carl Edward Rasmussen. 2004. Gaussian processes in machine learning. Advanced lectures on machine learning. 63–71.

[17] R. Shwartz-Ziv and N. Tishby. 2017. Opening the black box of deep neural networks via information. ArXiv e-prints (2017).

[18] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. 2014. Intriguing properties of neural networks. In Proceedings of the International Conference on Learning Representations (ICLR).

[19] Naftali Tishby, Fernando C. Pereira, and William Bialek. 1999. The information bottleneck method. In Proceedings of Annual Allerton Conference on Communication, Control and Computing (Allerton).

[20] Naftali Tishby and Noga Zaslavsky. 2015. Deep learning and the information bottleneck principle. In Proceedings of IEEE Information Theory Workshop (ITW).