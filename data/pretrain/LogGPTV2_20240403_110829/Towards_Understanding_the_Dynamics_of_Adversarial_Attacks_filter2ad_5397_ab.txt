an empirical study on the dynamic properties of adversarial inputs.
Our study is designed to provide a new perspective on a set of key
questions about adversarial inputs.
3.1 Experimental Setting
We mainly use the Cifar10 dataset [7] and the DNN model in [12]
which attains the accuracy of 90.24% on Cifar10.
We focus on untargeted attacks. For targeted attack models (e.g.,
Jsma), we select the class ˆy (different from x’s ground-truth class
y) that requires the minimum perturbation as its targeted class.
Meanwhile, we require different attack models to have similar per-
turbation magnitude for fair comparison.
In our study, given a DNN model f and an input x, we collect x’s
feature maps, measure its source and target MI matrices {S (k ),T (k )}k,
and compute x’s IPs (IIP, OIP, and IOC) within f .
3.2 Experimental Results
We present our empirical study results and report our findings.
Q1: How are adversarial inputs crafted to trigger DNNs to
misbehave? Figure 1 shows the process of Jsma attack on a ran-
domly selected input, by visualizing each intermediate adversarial
input xi’s information flows. We observe that the information flows
of adversarial inputs deviate from that of benign ones, while the
attack process essentially corresponds to shifting the information
flows away from benign inputs towards adversarial ones.
2
Figure 1: Top: IPs of a randomly selected input. The black line rep-
resents the benign input ˆx0; lighter colors indicate larger i; the line
of the lightest color represents the adversarial input ˆxn. Bottom: the
overall difference between IPs of ˆxi and ˆx0.
Figure 2: Aggregated IPs of benign and adversarial inputs gener-
ated by (left) Fgsm, Igsm, DeepFool, and C&W attacks (right) by regu-
lar and adaptive Jsma attacks.
Q2: How are adversarial inputs generated by various attacks
different in their underlying mechanisms? Figure 2 (left) com-
pares the aggregated IIPs, OIPs, and IOCs of benign inputs and
adversarial inputs generated by different attacks including Fgsm
[5], Igsm [8], DeepFool [13], and C&W [4], where adversarial
inputs generated by varied attack models lead to drastically differ-
ent information flows, implying that multiple defense or detection
methods might be necessary to mitigate different attacks.
Q3: How are existing defense mechanisms often vulnerable
to adaptive attacks? We compute the aggregated IPs of adver-
sarial inputs (including successful and failed ones) generated by
regular and adaptive Jsma [2] on the defensively distilled DNN
model [15]. Figure 2 (right) shows that from the IP perspective, ad-
versarial inputs generated by adaptive Jsma deviate much further
from benign inputs than those by regular Jsma, which explains why
defensive distillation fails to defend against adaptive attacks.
Q4: How are complicated DNN models more vulnerable to
adversarial inputs? Figure 3 shows the aggregated IPs of benign
inputs and adversarial inputs generated by Jsma. For comparison,
with respect to a given DNN f (original or compressed by [6]),
besides the IPs of adversarial inputs targeting f , we also compute
the IPs of adversarial inputs targeting the other DNN (compressed
or original). We observe that it is easier to cause information flows
to shift from benign inputs on more complicated DNN, indicating
that adversarial inputs demonstrate higher transferability on the
original model (i.e., more complicated models tend to be more
vulnerable to adversarial inputs).
Q5: How are transferable adversarial inputs different from
non-transferable ones? Figure 4 shows the aggregated IPs of ad-
versarial inputs targeting three DNNs, “good_init” [12], “deep_conv”,
attack_iteration_numattack_iteration_num5 CONCLUSION
In this paper, we present an empirical study on the dynamic prop-
erties of adversarial input attacks against DNN models. Using a
data-driven approach, we measure the information flows of ad-
versarial inputs within various DNN models and conduct the in-
depth comparative study on their discriminative patterns. Our study
sheds light on a set of key questions surrounding adversarial inputs,
points to several promising directions for designing more effective
defense mechanisms. We hope that our visualization tool can help
researchers learn more about adversarial samples behavior during
DNN model classification.
ACKNOWLEDGMENTS
This material is based upon work supported by the National Science
Foundation under Grant No. 1566526 and 1718787.
[7] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning Multiple Layers of Features
[8] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial examples
REFERENCES
[1] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
In Proceedings of the International Conference on Learning Representations (ICLR).
[2] N. Carlini and D. Wagner. 2016. Defensive distillation is not robust to adversarial
examples. ArXiv e-prints (2016).
[3] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not easily
detected: Bypassing ten detection methods. In Proceedings of ACM Workshop on
Artificial Intelligence and Security (AISec).
[4] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In Proceedings of IEEE Symposium on Security and Privacy
(S&P).
[5] I. J. Goodfellow, J. Shlens, and C. Szegedy. 2015. Explaining and harnessing
adversarial examples. In Proceedings of the International Conference on Learning
Representations (ICLR).
[6] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. 2016. Network
trimming: A data-driven neuron pruning approach towards efficient deep archi-
tectures. ArXiv e-prints (2016).
from Tiny Images. Technical report, University of Toronto (2009).
in the physical world. arXiv preprint arXiv:1607.02533 (2016).
521, 7553 (2015), 436–444.
[9] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature
[10] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2016. Delving into
transferable adversarial examples and black-box attacks. ArXiv e-prints (2016).
[11] Dongyu Meng and Hao Chen. 2017. Magnet: a two-pronged defense against
adversarial examples. In Proceedings of ACM SAC Conference on Computer and
Communications (CCS).
[12] Dmytro Mishkin and Jiri Matas. 2015. All you need is a good init. ArXiv e-prints
(2015).
[13] Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
Deepfool: a simple and accurate method to fool deep neural networks. In Pro-
ceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
[14] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay
Celik, and Ananthram Swamil. 2016. The limitations of deep Learning in ad-
versarial settings. In Proceedings of IEEE European Symposium on Security and
Privacy (Euro S&P).
[15] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillation as a defense to adversarial perturbations against deep neural
networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).
[16] Carl Edward Rasmussen. 2004. Gaussian processes in machine learning.
Advanced lectures on machine learning. 63–71.
networks via information. ArXiv e-prints (2017).
[18] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R.
Fergus. 2014. Intriguing properties of neural networks. In Proceedings of the
International Conference on Learning Representations (ICLR).
[19] Naftali Tishby, Fernando C. Pereira, and William Bialek. 1999. The information
bottleneck method. In Proceedings of Annual Allerton Conference on Communica-
tion, Control and Computing (Allerton).
[20] Naftali Tishby and Noga Zaslavsky. 2015. Deep learning and the information
bottleneck principle. In Proceedings of IEEE Information Theory Workshop (ITW).
[17] R. Shwartz-Ziv and N. Tishby. 2017. Opening the black box of deep neural
In
Figure 3: Aggregated IPs of benign and adversarial inputs by Jsma.
The solid and dashed blue lines respectively represent adversarial
inputs targeting the inference model and the other model.
and “vgg_like”, by Jsma. Observe that the OIPs of adversarial inputs
targeting the inference model deviate further away from benign
inputs, compared with that of adversarial inputs targeting a model
different from this inference model. Moreover, the OIPs of transfer-
able adversarial inputs [18] deviate much further compared with
non-transferable ones. Therefore, in order to create transferable
adversarial inputs, it is sensible to attack ensemble models [10] (i.e.,
training on the three DNNs simultaneously).
Figure 4: Aggregated IPs of adversarial inputs targeting three DNN
models by Jsma. Note that Jsma fails to generate transferable adver-
sarial samples targeting “deep_conv”.
4 ADDITIONAL RELATED WORK
Adversarial Deep Learning. The phenomena of adversarial inputs
have attracted intensive research. One line of work focuses on
developing new attacks [2, 4, 5, 8, 13, 14]. Another line of work
attempts to defend against such attacks [5, 11, 15, 18]. However, the
defense-enhanced models, once deployed, can often be fooled by
adaptively engineered inputs or by new attack variants [1–3].
Information Flow Theory. Recently the information flow theory
has been used to study the underlying mechanisms of DNN mod-
els. Tishby and Zaslavsky [20] suggested the use of Information
Bottleneck (IB) [19] to study the representation learning process.
Shwartz-Ziv and Tishby [17] then applied the IB theory to evalu-
ate the DNN training process. Different from the previous studies,
this work focuses on measuring and comparing the information
flows caused by benign and adversarial inputs, and extracting their
discriminative patterns.
3