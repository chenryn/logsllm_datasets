0.8 ['singletask', 'lstm']
0.68%
0.6 0.56% ycarucca
0.52%
Fig. 4. Trace execution path with concurrent events, where the numbers 0.4
representtheindicesofthelabelsli∈L 0.27%
0.2 0.19%
In summary, the core approach from the described method- 0.05%
0.0 0.0% 0.0%
ologyisthemultimodalLSTMnetwork.Asmentioned,ituses 1 k3 5
different data modalities, forms a joint representation, and
Fig. 5. Comparison of the overall accuracies of the models evaluated for
utilizes the possible highly non-linear relationships between
threevaluesofk∈{1,3,5}.
the data modalities in addition to the features extracted from
the long-term dependencies using the LSTMs. This helps to
achieve a higher accuracy than that for the single-modality 1.0
architecture. 0.9
On the other hand, the detection of dependent and concur- 0.8
rent events helps to perform a better root-cause analysis and 0.7
ycarucca
providesmoreinsightsfromtheobserveddata.Ingeneral,the
0.6
multimodal approach is generic and can be used for anomaly
0.5
detection in sequential data when multiple data modalities are ['multimodal', 'lstm'] k=1
0.4 ['multimodal', 'lstm'] k=3
available. ['multimodal', 'lstm'] k=5
0.3 ['singletask', 'lstm'] k=1
['singletask', 'lstm'] k=3
0.2 ['singletask', 'lstm'] k=5
IV. EVALUATION
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
number of non-zero events
The deep learning methods were implemented in Python
Fig.6. Comparisonoftheaccuraciesoftwobestmodels,evaluatedforeach
using Keras [18]. The experiments on the collected dataset
tracelength4−20andk∈{1,3,5}.
were carried out on regular personal computer using GPU-
NVIDIA GTX 1060. For all models we used batch size of AnomalyInjectionsandMeasurementofAccuracyforSAD.
512, learning rate of 0.001, and 400 epochs. The accuracy evaluation for detected anomalies is carried
In practice, the anomaly may not appear only in a single
1.0
event.Weassumethatatraceisanomalousifatleastoneevent
0.9
insidedeviatesfromthenormalbehavior,makingourapproach
0.8 applicablewhentheanomalyisspreadacrossmultipleevents.
0.7 Baselines. We have two baseline models for comparison, ycarucca
0.6 single- and multimodal deep learning architectures composed
0.5 ['multimodal', 'lstm'] k=1 of simple feed-forward neural networks. The architecture for
['multimodal', 'lstm'] k=3 the single-modality network is input, dense(50), dense(20),
0.4 ['multimodal', 'lstm'] k=5
['singletask', 'lstm'] k=1 output, while for the multimodal is [D -input, D -input],
0.3 ['singletask', 'lstm'] k=3 1 2
['singletask', 'lstm'] k=5 dense(50) for D , dense(50) for D , concatenation, dense(20)
1 2
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
position of the injected anomaly for each, output for each. The decision for anomaly is done
in the same way as previously described.
Fig. 7. Comparison of the accuracy of the two best models, evaluated for
differentpositions0,20oftheinjectedanomalyandk∈{1,3,5}.
B. Results and Discussion
As shown in Figure 5, the best results in terms of accuracy
out by artificially injecting anomalies in the data. For SAD, forthetaskofstructuralanomalydetectionareachievedusing
we performed anomaly injections as follows: given a trace the multimodal LSTM predictions. The bar plot shows the
T = {e ,e ,...,e }, with N non-zero elements and accuracy of all models when different values of k are used.
test 0 1 Tl z
top-k predictions for each position i in the trace, we select We observe that that the single-modality LSTM achieves a
a label, which is not in the top-k predictions, and inject comparable accuracy, while the other two single- and mul-
it at the position of the normal observed label. We then timodal dense architectures have low accuracies. The dense
run the prediction with the model and determine a decision models can not take into account the temporal information.
anomaly/normal by comparing the non-corrupted sample with Compared to that of the single-modality LSTM architecture,
the prediction. In case of output true and the label in the themultimodalLSTMachievesabetteraccuracyowingtothe
corrupted event position is not in the top-k predictions, the additional response time information. The results for k = 3
injected anomaly is successfully detected. and k = 5 are comparable, while the results for k = 1 show
that the multimodal approach outperforms the single-modality
by a large margin of 16%. Because of the low percentages
obtainedfromthetraditionalmodels,wedonotcomparethem
1.0 below.
We evaluated the accuracy when the anomaly is injected in
0.8 traces with different sizes, as shown in Figure 6, while ignor-
ing the position of injection of the anomaly. Both proposed
0.6 ycarucca
architecturesachievehighaccuraciesfork ∈{3,5}.Themul-
timodalslightlyoutperformsthesingle-modalityapproachin9
0.4
out of 15 trace lengths for both k. Significantly better results
0.2 are achieved for k = 1 for almost all of the trace lengths.
['multimodal', 'dense'] Theplotdemonstratesthatbothapproachesarestable,without
['multimodal', 'lstm']
0.0 performance reduction when the trace length is increased.
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
number of non-zero events
Further, for SAD, we compared the two best models when
the anomaly is injected in different positions in the trace for
Fig. 8. Response time anomaly detection accuracy comparison of the
k ∈ 1,3,5, while ignoring the trace length. Similarly, Figure
multimodal LSTM and the baseline deep learning architecture evaluate for
differenttracelengths. 7 shows a significantly better performance of the multimodal
approach for k = 1. Furthermore, the accuracy is also stable
Anomaly Injections and Measurement of Accuracy for at all of the different positions of the trace. This implies that
RTAD. We perform the anomaly injections by selecting the themodelsuccessfullydetectsinjectedanomaliesfordifferent
event response time rt i at a position i of the trace. The events in different positions of the trace.
anomaly is injected by increasing the response time of the RTAD. The single-task models for sequential response time
eventbyarandomvaluer ∈(2∗rt i,5∗rt i).Oncetheanomaly modelling have quite low performances than those of both
is injected, we compute the mean squared error between the multimodal models, and thus we discuss only those results.
input and the predicted output. If the error is not in the 95% Figures 9 and 8 show comparisons of the two multimodal
confidence interval computed from the Gaussian fitted on the approachesfordifferentpositionsoftheinjectedanomalyand
training set, then the anomaly is detected successfully. different trace lengths. In both figures, the multimodal ap-
The accuracy is computed as the ratio of the number of proach achieves a higher accuracy for response time anomaly
successfully detected anomalies and the number of injected detection. Figure 8 shows that the accuracy slightly decreases
anomalies. when the trace length is larger. In Figure 9, the reason why
complex microservice systems. These achievements are fun-
damental for the development of zero-touch AIOps solutions
1.0
for the automated anomaly detection, root-cause analysis, and
0.9
remediation.
0.8
0.7 REFERENCES
ycarucca
['multimodal', 'dense']
0.6 ['multimodal', 'lstm'] [1] F.Schmidt,A.Gulenko,M.Wallschlger,A.Acker,V.Hennig,F.Liu,
0.5 and O. Kao, “Iftm - unsupervised anomaly detection for virtualized
networkfunctionservices,”in2018IEEEInternationalConferenceon
0.4 WebServices(ICWS),July2018,pp.187–194.
0.3 [2] A. Gulenko, F. Schmidt, A. Acker, M. Wallschlager, O. Kao, and
F. Liu, “Detecting anomalous behavior of black-box services modeled
0.2
withdistance-basedonlineclustering,”in2018IEEE11thInternational
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
position of the injected anomaly ConferenceonCloudComputing(CLOUD),Jul2018,pp.912–915.
[3] M.Du,F.Li,G.Zheng,andV.Srikumar,“Deeplog:Anomalydetection
anddiagnosisfromsystemlogsthroughdeeplearning,”inProceedings
Fig.9. Responsetimeanomalydetectionaccuracycomparisonbetweenthe
of the 2017 ACM SIGSAC Conference on Computer and Communica-
multimodalLSTMandthebaselinedeeplearningarchitecture,evaluatedfor
tionsSecurity. ACM,2017,pp.1285–1298.
differentpositionsoftheinjectedanomaly.
[4] F.Schmidt, F.Suri-Payer, A.Gulenko, M. Wallschlger, A.Acker, and
O. Kao, “Unsupervised anomaly event detection for cloud monitoring
using online arima,” in 2018 IEEE/ACM International Conference on
the accuracy drops at position 7 is probably because of the UtilityandCloudComputingCompanion(UCCCompanion),Dec2018,
small signal-to-noise ratio and the existence of outliers. In pp.71–76.
our approaches, we have not applied any preprocessing or [5] D. Battre, N. Frejnik, S. Goel, O. Kao, and D. Warneke, “Evaluation
ofnetworktopologyinferenceinopaquecomputecloudsthroughend-
outlier removal techniques, but we assumed that the recorded to-end measurements,” in 2011 IEEE 4th International Conference on
data represent the normal behavior. The use of preprocessing CloudComputing,July2011,pp.17–24.
techniquesfortheresponsetimemayeventuallyhelpimprove [6] B.H.Sigelman,L.A.Barroso,M.Burrows,P.Stephenson,M.Plakal,
D.Beaver,S.Jaspan,andC.Shanbhag,“Dapper,alarge-scaledistributed
the accuracy for the response time in critical positions.
systemstracinginfrastructure,”Google,Inc.,Tech.Rep.,2010.[Online].
The models are consistent and have high accuracy even Available:https://research.google.com/archive/papers/dapper-2010-1.pdf
when the length of the trace increases. This is because of [7] Q. Fu, J.-G. Lou, Y. Wang, and J. Li, “Execution anomaly detection
indistributedsystemsthroughunstructuredloganalysis,”in2009IEEE
theLSTMswhichareabletolearnlong-termdependenciesin
InternationalConferenceonDataMining(ICDM),2009,pp.149–158.
sequential tasks. [8] N.SrivastavaandR.R.Salakhutdinov,“Multimodallearningwithdeep
Performance. The time needed to train the multimodal boltzmann machines,” in Advances in neural information processing
systems,2012,pp.2222–2230.
LSTM on the 1% representative sub-sample of over one mil-
[9] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
liontraceswasapproximately30min.Themodelisapplicable computation,vol.9,no.8,pp.1735–1780,1997.
for real-time anomaly detection with a prediction time per [10] I.Sutskever,O.Vinyals,andQ.V.Le,“Sequencetosequencelearning
with neural networks,” in Advances in neural information processing
trace below 50 ms.
systems,2014,pp.3104–3112.
[11] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
V. CONCLUSIONANDFUTUREWORK
deeprecurrentneuralnetworks,”in2013IEEEInternationalConference
This paper addressed an important and growing challenge onAcoustics,speechandsignalprocessing(ICASSP),2013,pp.6645–
6649.
from the field of AIOps: the anomaly detection in large-scale
[12] P. Malhotra, L. Vig, G. Shroff, and P. Agarwal, “Long short term
cloud infrastructures using tracing data that contains detailed memory networks for anomaly detection in time series,” in ESANN,
information about inter-service calls. 2015.
[13] A.Taylor,S.Leblanc,andN.Japkowicz,“Anomalydetectioninauto-
We addressed the problem using sequential deep networks
mobile control network data with long short-term memory networks,”
for structural anomaly detection and presented approaches to in2016IEEEInternationalConferenceonDataScienceandAdvanced
recognizedependentorconcurrentlyinvokedservices.Wefur- Analytics(DSAA),2016,pp.130–139.
[14] A. Brown, A.Tuor, B. Hutchinson, and N.Nichols, “Recurrent neural
ther extended the approach by an architecture for multimodal
network attention mechanisms for interpretable system log anomaly
anomaly detection. This approach enables to detect structural detection,”inProceedingsoftheFirstWorkshoponMachineLearning
and response time anomalies by simultaneously considering for Computing Systems, ser. MLCS’18. New York, NY, USA: ACM,
2018,pp.1:1–1:8.
the trace structure and the latency of the services.
[15] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,
Ourevaluationwithdatafromareal-worldproductioncloud “Multimodal deep learning,” in Proceedings of the 28th International
showed that our multimodal LSTM approach achieved over ConferenceonMachineLearning(ICML-11),2011,pp.689–696.
[16] OpenZipkin, “openzipkin/zipkin,” 2018. [Online]. Available:
90% accuracy in multiple experiments, outperforming the
https://github.com/openzipkin/zipkin
single-modality and the baseline dense neural networks, al- [17] N. M. Nasrabadi, “Pattern recognition and machine learning,” Journal
though the single-modality LSTM yielded comparable results ofelectronicimaging,vol.16,no.4,p.049901,2007.
[18] F.Cholletetal.,“Keras,”https://keras.io,2018.
in structural anomaly detection.
[19] A.Shrivastwa,S.Sarat,K.Jackson,C.Bunch,E.Sigler,andT.Camp-
Our approach paves the way for development of new tech- bell, OpenStack: Building a Cloud Environment. Packt Publishing,
niques that simultaneously consider application logs, resource 2016.
[20] R. Ricci, E. Eide, and C. Team, “Introducing cloudlab: Scientific
metrics, or other observability data to create a joint represen-
infrastructure for advancing cloud architectures and applications,” The
tation of states to enable the anomaly detection in large-scale magazineofUSENIX&SAGE,vol.39,no.6,pp.36–38,2014.