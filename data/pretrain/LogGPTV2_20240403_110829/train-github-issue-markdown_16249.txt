The e2e framework `deleteNs` needs to be smarter to better help triage issues
in this space, as they all typically tend to be unique snowflakes particular
to a problem with the node, pod, container runtime itself and rarely the
controller that fundamentally drives deletes.
Ideas:
  * deleteNS caches the set of Events in the namespace prior to invoking deletion, in cases of failure, logs all events that occurred in that namespace prior to the delete to know if there was an unexpected error event
  * log all pods in YAML or JSON in internal API version to see their state as observed from the API server to give clues as to why they are hung. @lavalamp @smarterclayton \- is there a good one liner that I can reference that takes a pod and outputs to YAML?
  * log all information about nodes in YAML or JSON format that have pods actively scheduled against them that are not terminating (would help us know if there was a node problem encountered).
Input from others would be appreciated, but this seems like the general
purpose issue that is often really hard to diagnose and debug during each
release since we don't capture any information to help localize why the error
happened.
@kubernetes/sig-api-machinery @kubernetes/rh-cluster-infra