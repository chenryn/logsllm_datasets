AM
Component
N e tw ork
Com ponents
Crypto-Processor
Com ponent
FT-Cluster Processor
Com ponent
Single-Hop
Communication
Service
HW
Sensors
CPU
Wireless
Memory
Card
Crypto-Processor
(signature creation and
verification)
FT-Cluster
Processor
(masking of faulty/malicious data)
Figure 1. Inner-circle consistency node ar-
chitecture.
Cluster Processor has also been developed.
The operating system (Linux for ad hoc nodes and TinyOS
for sensor nodes) is augmented with the Inner-circle Intercep-
tor. The interceptor is implemented in Linux as a loadable
kernel module and in TinyOS as a TinyOS component that ex-
ports a send/receive TinyOS interface, which is directly used
by the sensor application. Other inner-circle services (voting
service, secure topology service, and suspicions manager) are
implemented as user-level daemons in Linux and as TinyOS
components in TinyOS. Applications access inner-circle ser-
vices via an API that allows them (1) to initiate and conﬁgure
the services (e.g., selection of deterministic versus statistical
voting, selection of dependability level L), (2) to specify mes-
sage templates that describe the application messages to be
checked (the architecture enables selective use of the inner-
circle approach, as not all application messages are necessar-
ily checked by the inner-circle services), and (3) to specify a
set of Inner-circle Callbacks (whose code resides in a shared
library in Linux and in a TinyOS component in TinyOS).
The following sections discuss the main inner-circle ser-
vices in greater detail. Due to space limitations, formal ser-
vice speciﬁcations are relegated to [16].
4.1 Secure Topology Service
A Secure Topology Service (STS) discovers and authenti-
cates bidirectional links up to two hops away and provides
each node with a local topology view. (Two hops are neces-
sary, since a node needs to authenticate its neighbors’ neigh-
bors in order to securely participate in its neighbors’ inner-
circles.)
Informally, we assume that there is a known time
interval ∆STS such that an STS implementation satisﬁes a
Completeness property, which formalizes the ability to ex-
clude untimely (e.g., broken) links, and One-Hop Accuracy
(b) Sensor Node Embodiment.
Figure 2. Inner-circle consistency embodi-
ments.
and Two-Hop Accuracy properties, which capture the abil-
ity to include one- and two-hop timely links. The proposed
STS implementation assumes that local clocks at neighbor-
ing nodes are kept (approximately) synchronized, and it oper-
ates by periodic broadcasting of STS messages (with period
τ Kc
)>Kc
check
value v
)>Kp
(a) Deterministic Voting Algorithm.
Kc
c
fuse L
values
into v
Kp}
p
)>Kp
>Kc
p∈PL
Kp
fuse L values
and check v
)>Kp
(b) Statistical Voting Algorithm.
c
q
p
Deterministic Voting. Node c proposes value v by sending a propose
message to its inner-circle nodes (p and q). Receiving node p checks the
validity of v by evaluating f (v) and, in this case, sends an ack message to
c, including a partial signature σKLp
obtained by using p’s share of secret
KL. On receiving L ack messages, node c generates its partial signature
, creates a total signature σKL , and assembles an agreed message to
σKLc
be sent to the proper destination(s).
Statistical Voting. Node c proposes value vc by sending a solicit
message to its inner-circle nodes. Receiving node p replies with a
value message that includes its proposed value vp. On receiving L
value messages, c computes the set PL of L nodes that have sent their
values, computes a fused value v = f (vc, vp, . . .), and sends a propose
message that includes value v and the value messages sent by each node
p ∈ PL. On receiving the propose message, p veriﬁes that the included
signatures are valid and that v was computed by applying f on the values
{vc, vp, . . .}. In this case, p replies with an ack message that includes p’s
partial signature σKLp
. On receiving L ack messages, node c assembles
an agreed message to be sent to the proper destination(s).
In both algorithms, the actual forwarding in the wireless network oc-
curs via the node’s routing and forwarding services (see § 4). Eventually,
a destination node receives the agreed message, veriﬁes the included sig-
natures, and delivers the included value to the local application.
Figure 3. Inner-circle Voting Service algorithms.
are crashes, and FL are due to broken links2—and the num-
ber N of nodes in an inner-circle (including the center node),
node c sets dependability level L so as to guarantee a min-
imum number T of non-Byzantine participants in each IVS
execution that completes successfully. It can be shown that
setting L = N − F − 1 guarantees Agreement, Integrity, and
Termination properties of IVS protocols (introduced below)
for T = L − FB. As a special case, ﬁxing L + 1 = 2N/3 and
ignoring FC and FL provides tolerance to N/3 − 1 Byzantine
failures and guarantees that a majority of correct nodes must
agree for the protocol to terminate; this scenario corresponds
to standard Byzantine agreement algorithms.
Informally, the Agreement property states that a sender
node c needs approval from at least T non-Byzantine inner-
circle nodes in order to assemble a valid agreed message m
that indicates a dependability level L. The Integrity property
states that a (remote) node y receiving an agreed message m
that indicates a dependability level L can rely on the informa-
tion contained in m. The Termination property states that an
IVS algorithm execution initiated by a correct node c is guar-
anteed to terminate. Note that IVS algorithms are not required
to terminate if the initiator node is faulty.
4.3. Fault-Tolerant Value Fusion
This section discusses techniques to implement the fault-
tolerant fusion function f used in § 4.2 to support reliable
and secure in-network processing. The mathematical problem
considered is the (fault-tolerant) estimation of an unknown
(vector) parameter Θ from a set of L (vector) observations
P = {p1, . . . , pL} that are corrupted by random noise such
2If c is not Byzantine, then FB accounts only for Byzantine nodes in c’s
inner-circle; otherwise, FB accounts also for Byzantine nodes that an agreed
message may encounter during its traversal of the wireless network. FC and
FL always account only for failures in c’s inner-circle.
that pi = Θ + Ni, where Ni are i.i.d. zero-mean random vari-
ables. In contrast with classical estimation theory, we allow
up to a number F of these observations to be arbitrarily cor-
rupted (beyond noise Ni), owing to faults/attacks.
A simple implementation of function f is given by a fault-
tolerant mean algorithm originally proposed in the context of
approximate agreement [18] and then applied to fault-tolerant
in-network processing [19]. Fault-tolerant mean is just one of
the many algorithms proposed for approximate agreement and
clock synchronization [20, 21]. The limitation of these tech-
niques, when applied to in-network processing, is that they
always discard a number of input observations, which results
in limited accuracy even in the common case of no faulty data.
High accuracy is important for the inner-circle approach be-
cause local value fusion is done on the data collected by a
limited number of nodes—an inner-circle is not expected to
have more than 10–15 members [22].
Fault-Tolerant Cluster Algorithm. This paper con-
tributes with the proposal of a Fault-Tolerant Cluster algo-
rithm to generate, from a set P of L observations, an esti-
mate (cid:1)ΘFT that is highly accurate yet robust to faulty/malicious
data (in P). To achieve this goal, we exclude from the esti-
mation process only those observations that are likely to be
faulty/malicious, i.e., that are inconsistent with the distribu-
tion indicated by the remaining observations. Before present-
ing the algorithm, however, a few deﬁnitions are required.
Given a cluster (or set of data points) C and a point p ∈ C,
we deﬁne the distance d(p, C) of point p from cluster C:
d(p, C) = (cid:4)p − centroid(C \ p)(cid:4)
(1)
(cid:2)
where centroid({p1, . . . , pn}) =
n
i=1 pi/n. The justiﬁcation
for the above deﬁnition is that we want distance d(p, C) to be
proportional to the information lost when excluding p from C.
(cid:2) = C \ p of i.i.d. observations having
Indeed, consider a set C
a unimodal p.d.f., symmetric w.r.t. the mean µ. The farther
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Require: L data points {pi, . . . , pL} and a threshold η
Ensure: bΘFT is the fault-tolerant centroid
1: // First compute the fault-tolerant cluster C
2: C := {p1, . . . , pL}
3: change := | C |> 2
4: while change do
5:
change := false
for all pi ∈ C do
6:
bΘi := centroid(C \ pi)
7:
di := (cid:3)pi − bΘi(cid:3)
8:
9:
end for
if ∃ pi ∈ C : (di > η) ∧ (∀ pj ∈ C : dj ≤ di) then
10:
11:
12:
13:
14: end while
15: // Then compute bΘFT as the centroid of the fault-tolerant cluster C
16: bΘFT := centroid(C)
C := C \ pi
change := | C |> 2
end if
6
5
4
3
2
1
0
−1
−2
−3
−3
p4
p1
^
Θ
4
^
Θ
p3
Θ
p2
−2
−1
0
1
2
3
4
5
6
Figure 4. Fault-Tolerant Cluster algorithm.
Figure 5. Fault-Tolerant Cluster example.
(cid:2)) is an estimator—
point p is from µ—for which centroid(C
the lower the probability of observing point p, and thus, the
larger the information3 carried by such an observation.
size subset of P such that each point p ∈ C
∗
P less than a user-speciﬁed threshold η:
C
∗
P is deﬁned as the maximum-
∗
P has distance from
The fault-tolerant cluster C
∗
P = arg max
C
C∈2P
| {p ∈ C : d(p, C) ≤ η} | .
(2)
Parameter η must be chosen so that two correct observations
are at distance greater than η only with negligible probabil-
ity. The justiﬁcation for the above deﬁnition is that we want
∗
P all observations p that best ﬁt the underlying
to include in C
distribution. These observations should be such that, when
considered together, the removal of one of them causes the
∗
P by dis-
loss of only a little information. Thus, we form C
carding all those observations that are unlikely to be correct.
Finally, the fault-tolerant estimate (cid:1)ΘFT is deﬁned as the
centroid of the fault tolerant cluster C
(cid:1)ΘFT = centroid(C
∗
P:
∗
P).
(3)
Based on the above deﬁnitions, Fig. 4 provides a pseudocode
of the proposed fault-tolerant cluster algorithm.
Figure 5 depicts an example in which the current cluster C
comprises four points p1, . . . , p4, which are observations of a
common (unknown) value Θ independently produced by four
sensor nodes n1, . . . , n4. The current estimate (cid:1)Θ is computed
considering all available data and has poor accuracy due to
p4—in the ﬁgure, we suppose that point p4 is due to an error in
node n4’s sensor, e.g., n4’s sensor was physically damaged by
humidity and reports readings stuck at a high value. To decide
whether to exclude point p4, the fault-tolerant cluster algo-
rithm ﬁrst computes (cid:1)Θ4 as the centroid of points {p1, p2, p3}
and then computes the distance d4 between (cid:1)Θ4 and p4. If dis-
is going to be (cid:1)Θ4, which is much more accurate than (cid:1)Θ.
tance d4 is greater than a user-supplied threshold η, then p4 is
excluded from cluster C. In this case, the new estimate of Θ
The fault-tolerant cluster algorithm cannot guarantee re-
3In information theory, the information of an event e is deﬁned in terms
of its probability of occurrence Pr{e}: I(e) = log2(1/Pr{e}).
moval of (and only of) faulty/malicious data if these data are
very similar to the correct data; however, the negative effect
of such a case should be negligible. It can be shown that [16]:
(1) If the number F of faulty/malicious points is less than half
of the total points N, then condition δF > δC
1−2F/N guarantees
that only faulty/malicious points are removed, where δC and
computed with only the correct points—in the correct and
faulty/malicious points, respectively. (2) The worst-case sce-
nario corresponds to all faulty/malicious observations cluster-
ing in a point p
δF represent the maximum distance from (cid:1)ΘC—the estimate
1−2F/N from (cid:1)ΘC. Thus,
the maximum estimation error E = (cid:4)(cid:1)ΘC − (cid:1)ΘFT(cid:4) added by
F = δC
δ∗
∗ = F
F. For instance, the
faulty/malicious observations is E
case in which one third of the points are erroneous (F = N/3)
estimate (cid:1)ΘFT is in the range of the correct observations.
∗ = δC, which indicates that
corresponds to δ∗
at a distance δ∗
F = 3δC and E
∗
N
5. Application Examples
The next sections demonstrate the inner-circle approach in
two signiﬁcant wireless application scenarios: (1) the neutral-
ization of black hole attacks in AODV networks and (2) the
neutralization of sensor errors in a wireless sensor network.
5.1. Reliable and Secure AODV Networks: Black
Hole Attack Case Study
This section presents an application of the inner-circle ap-
proach to neutralize black hole attacks in AODV [2] wireless
networks.
In a black hole attack, a malicious node M ad-
vertises itself as having the shortest (or the most recent) path
to a node D, whose packets it wants to intercept. In AODV
this can be achieved by replying to a received RREQ message
with a malicious RREP message that has a large destination
sequence number.4 Once node M has been able to insert itself
into an active routing path, it can drop packets to D. Black
hole attacks are very difﬁcult to detect and protect against be-
cause the mere use of user authentication and signed routing
4Both RREQ and RREP messages include a destination sequence number
that is used to distinguish fresher routes from older ones.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
(a)
RREQ(D, 3)
S
RREQ(D, 3)
RREQ(D, 3)
D
N2
N3
RREQ(D, 3)
RREQ(D, 3)
(b)
S
(c)
S
N1
M
N2
N3
N1
M
RREP(D, 5)